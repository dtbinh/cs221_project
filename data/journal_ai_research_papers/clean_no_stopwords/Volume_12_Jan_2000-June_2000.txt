Journal Articial Intelligence Research 12 (2000) 105-147Submitted 10/99; published 3/00Robust Agent Teams via Socially-Attentive MonitoringGal A. KaminkaMilind Tambegalk@isi.edutambe@isi.eduInformation Sciences Institute Computer Science DepartmentUniversity Southern California4676 Admiralty WayLos Angeles, CA 90292, USAAbstractAgents dynamic multi-agent environments must monitor peers execute individual group plans. key open question much monitoring agents'states required eective: Monitoring Selectivity Problem. investigatequestion context detecting failures teams cooperating agents, via SociallyAttentive Monitoring, focuses monitoring failures social relationshipsagents. empirically analytically explore family socially-attentiveteamwork monitoring algorithms two dynamic, complex, multi-agent domains,varying conditions task distribution uncertainty. show centralized schemeusing complex algorithm trades correctness completeness requires monitoringteammates. contrast, simple distributed teamwork monitoring algorithm resultscorrect complete detection teamwork failures, despite relying limited, uncertainknowledge, monitoring key agents team. addition, report designsocially-attentive monitoring system demonstrate generality monitoring several coordination relationships, diagnosing detected failures, on-line o-lineapplications.1. IntroductionAgents complex, dynamic, multi-agent environments must able detect, diagnose,recover failures run-time (Toyama & Hager, 1997).instance, robot'sgrip may slippery, opponents' behavior may intentionally dicult predict, communications may fail, etc. Examples environments include virtual environmentstraining (Johnson & Rickel, 1997; Calder, Smith, Courtemanche, Mar, & Ceranowicz, 1993),high-delity distributed simulations (Tambe, Johnson, Jones, Koss, Laird, Rosenbloom, &Schwamb, 1995; Kitano, Tambe, Stone, Veloso, Coradeschi, Osawa, Matsubara, Noda, &Asada, 1997), multi-agent robotics (Parker, 1993; Balch, 1998). rst key stepprocess execution-monitoring (Doyle, Atkinson, & Doshi, 1986; Ambros-Ingerson &Steel, 1988; Cohen, Amant, & Hart, 1992; Reece & Tate, 1994; Atkins, Durfee, & Shin,1997; Veloso, Pollack, & Cox, 1998).Monitoring execution multi-agent settings requires agent monitor peers,since correct execution depends also state peers (Cohen & Levesque,1991; Jennings, 1993; Parker, 1993; Jennings, 1995; Grosz & Kraus, 1996; Tambe, 1997).Monitoring peers particular importance teams, since team-members relywork closely together related tasks:c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiKaminka TambeMonitoring allows team-members coordinate actions plans team-mates, help teammates cooperate without interference. example, driverscars convoy cannot drive without monitoring cars convoy,disband convoy, help drivers cars break down.Monitoring allows team-members use peers dynamic information sources,learning new information. instance, driver convoy sees carsfront suddenly turn left, infer existence obstaclemilestone despite directly seeing herself.Previous work investigated dierent ways monitoring context teams cooperating agents. example, theoretical work SharedPlans (Grosz & Kraus, 1999)passive monitoring, agent notied propositionchanges (e.g., via communications), active monitoring, agent actively seeksdistinguishednd proposition changes (e.g., via observations inference unobservableattributes). Practical implementations investigated use passive monitoring viacommunications (Jennings, 1995), active monitoring via plan-recognition (Huber & Durfee, 1995), active implicit monitoring via environment (Fenster, Kraus, & Rosenschein,1995), dierent combinations methods (Parker, 1993; Jennings, 1993; Tambe,1997; Lesh, Rich, & Sidner, 1999).approach clearly superior another:Passivemonitoring generally perceived less costly active monitoring, also lessreliable (Grosz & Kraus, 1999; Huber & Durfee, 1995; Kaminka & Tambe, 1998).Regardless monitoring method, bandwidth computational limitations prohibitmonitoring agent monitoring agents full extent, time (Jennings,1995; Durfee, 1995; Grosz & Kraus, 1996). Thus key open question much monitoring agents required eective (in teams) (Jennings, 1993; Grosz & Kraus,1996, 1999). call challenging problemMonitoring Selectivity Problem,i.e.,problem selectivity observing others inferring state (based observations) monitoring.Although raised past, frameworkminimal constraints answers provided (Jennings, 1993; Grosz & Kraus, 1996).instance, theory SharedPlans requires agents verify intentionsconict teammates (Grosz & Kraus, 1996). However, methodsverication take place left investigation (Grosz & Kraus, 1996, p.308). Section 8 provides details related work.paper begins address monitoring selectivity problem teams, investigating monitoring requirements eective failure detection.focus investigationdetecting failures social relationships ideally hold agents monitored team. call monitoring social relationshipssocially-attentive monitoring,dierentiate types monitoring, monitoring failuresprogress agents towards goals. Here, term social relationship used denoterelation attributes multiple agents' states. Socially-attentive monitoring convoyexample involves verifying agents common destination heading,beliefs driving convoy mutual, etc. instance, agents observedhead dierent directions, clearly common heading. dierentmonitoring whether chosen (common) heading leads towards (agreed upon)destination.106fiRobust Agent Teams via Socially-Attentive MonitoringMonitoring relationships team (socially-attentive monitoring) critical taskmonitoring team-members.Failures maintain team's relationships often leadcatastrophic failures part team, lack cooperative behavior lackcoordination. failures often result individual agent failures, failuresagent's sensors actuators. Thus socially-attentive monitoring covers large classfailures, promotes robust individual operation.explore socially-attentive monitoring algorithms detecting teamwork failures various conditions uncertainty.analytically show despite presenceuncertainty actual state monitored agents, centralizedactivemonitoringscheme guarantee failure detection either sound incomplete, completeunsound. However, requires reasoning multiple hypotheses actualstate monitored agents, monitoring agents team.show activedistributed teamwork monitoring results sound complete detection capabilities,despite using much simpler algorithm. distributed algorithm: (a) uses single,possibly incorrect hypothesis actual state monitored agents, (b) involves monitoring key agents team, necessarily team-members. Using transformationanalytical constructs, show analogous results centralized failure-detectionmutual-exclusion coordination relationships.also conduct empirical investigation socially-attentive monitoring teams.present implemented general socially-attentive monitoring frameworkexpected ideal social relationships maintained agents comparedactual social relationships. Discrepancies detected possible failures diagnosed. apply framework two dierent complex, dynamic, multi-agent domains,service monitoring various social relationships, on-line o-line.domains involve multiple interacting agents collaborative adversarial settings,uncertainties perception action.one domain, provide empirical resultsactive monitoring conrm analytical results. another domain showo-line socially-attentive monitoring provide quantitative teamwork quality feedbackdesigner. also provide initial diagnosis procedures detected failures.focus explorations practical algorithms guarantees performance real-world applications. algorithms present seek complement usepassive communications-based monitoring (which unreliable many domains) exploreuse unintrusive key-hole plan-recognition alternative. However, ruleuse communicationswe simply seek provide techniques work evencommunications fail.analytical guarantees failure-detection soundnesscompleteness hold whether monitoring done communications plan-recognition.paper organized follows: Section 2 presents motivating examples background. Section 3 presents socially-attentive monitoring framework. Section 4 exploresmonitoring selectivity centralized teamwork monitoring.Section 5 explores monitoringselectivity distributed teamwork monitoring. Section 6 demonstrates generalityframework applying o-line conguration. Section 7 presents investigations additional relationship models. Section 8 presents related work, Section 9 concludes.two appendices contain proofs theorems presented (Appendix A), pseudo-codesocially-attentive monitoring algorithms (Appendix B).107fiKaminka Tambe2. Motivation Backgroundmonitoring selectivity problem paper addresseshow much monitoring requiredfailure-detection teamsrose growing frustration signicant softwaremaintenance eorts two application domains.ModSAF domain, high-delity battleeld virtual environment (Calder et al., 1993), involveddevelopment synthetic helicopter pilots (Tambe et al., 1995).RoboCup soccersimulation domain (Kitano et al., 1997) involved developing synthetic soccer players (Marsella, Adibi, Al-Onaizan, Kaminka, Muslea, Tallis, & Tambe, 1999).environments domains dynamic complex, many uncertainties:behavior agents (some adversarial, cooperative), unreliable communications sensors, actions may execute intended, etc.Agentsenvironments therefore presented countless opportunities failure despitedesigners' best eorts.examples may serve illustrate. following two examples actual failuresoccurred ModSAF domain.use two illustrate exploresocially-attentive monitoring throughout paper:Example 1.Here, team three helicopter pilot agents specied way-point (a given position), one team-members,attackers )towards enemy, teammates (scout,forwardland wait signal.agents monitored way-point. However, due unexpected sensor failure, oneattackers failed sense way-point.attacker correctly landed,failing attacker continued forward scout (see Figure 1 screen shotillustrating failure).Example 2.dierent run, three agents reached way-point detected it,scout gone forward identied enemy. sent message waitingattackers join attack enemy. One attackers receive message,remained behind indenitely scout attacker continuedmission alone.collected dozens similar reports ModSAF RoboCup domain.general, failures dicult anticipate design time, due huge numberpossible states. agents therefore easily nd novel statesforeseen developer, monitoring conditions communications placeproved insucient: none failure cases reported agents involved detect, letalone correct, erroneous behavior. agent believed agents actingcoordination it, since communication received agents indicateotherwise. However, agents violating collaboration relationships them,agents came disagree plan executeda collaboration relationshipfailure occurred.Preliminary empirical results show upwards 30% failuresreported involved relationship violations (relationship failures).Human observers, however, typically quick notice failures,clear social misbehavior agents cases. able infer failureoccurred despite knowing exactly happened. instance, seeing attackercontinuing ahead despite teammates' switching dierent plan (which human108fiRobust Agent Teams via Socially-Attentive MonitoringEnemyScout (ahead) failing attacker (trailing)Landing attackerFigure 1: plan-view display (the ModSAF domain) illustrating failure Example 1.thick wavy lines contour lines.observers inferred fact one teammates, attacker, landed)sucient observer detect something gone amisswithout knowingdierent plan was.analysis showed agents monitoring suciently. However, naive solution continuous communications agents clearly impractical since: (i) agents operating hostile environment; (ii) communicationsoverheads would prohibitive; (iii) fact, communications equipment broke cases. therefore sought practical ways achievequick detection failure, based limited ambiguous knowledge availablemonitoring agent.3. Socially-Attentive Monitoringbegin overview general structure socially-attentive monitoring system, shown Figure 2. consists of: (1) social relationship knowledge-base containingmodels relationships hold among monitored agents, enabling generationexpected ideal behaviorterms relationships (Section 3.1); (2) agentteam modeling component, responsible collecting representing knowledgemonitored agents'actual behavior(Section 3.2); (3) relationship failure-detection compo-nent monitors violations relationships among monitored agents contrastingexpected actual behavior (Section 3.3); (4) relationship diagnosis componentveries failures, provides explanation (Section 3.4). resulting109fiKaminka Tambeexplanation (diagnosis) used recovery, e.g., negotiations system (Kraus,Sycara, & Evenchik, 1998), general (re)planner (Ambros-Ingerson & Steel, 1988).ExpectedAttributeValuesagents monitor,agent attributesSocial Relationships Knowledge-BaseExpected BehaviorRelationshipDiagnosisRelationshipFailureDetectionsDetectedFailureObservations,CommunicationsActualBehaviorActual ValuesMonitored AgentAgent/Team Modeling ComponentDiagnosisSocially-Attentive Monitoring SystemMonitored AgentFigure 2: general structure socially-attentive monitoring system.3.1 Knowledge-Base Relationship Modelstake relationship among agents relation state attributes. relationshipmodel thus species dierent attributes agent's state relatedagents multi-agent system. attributes include beliefs heldagents, goals, plans, actions, etc. example, many teamwork relationship modelsrequire team-members mutual belief joint goal (Cohen & Levesque, 1991;Jennings, 1995).spatial formation relationship (Parker, 1993; Balch, 1998) speciesrelative distances, velocities maintained group agents (indomain, helicopter pilots).Coordination relationships may specify temporal relationshipshold among actions two agents, e.g., business contractors (Malone &Crowston, 1991).relationshipssocialexplicitly specify multipleagents act believe maintain relationshipsthem.relationship knowledge-base contains models relationships supposedhold system, species agents participating relationships.knowledge-base guides agent-modeling component selecting agents monitored,attributes state need represented (for detection diagnosis).used failure detection component generate expectations contrastedactual relationships maintained agents. provides diagnosis componentdetailed information agents' states' attributes related, drive diagnosisprocess.implementation socially-attentive monitoring teams uses four typesrelationships:formations, role-similarity, mutual exclusion, teamwork.teamworkmonitoringuseSTEAM(Tambe,1997)generaldomain-independent model teamwork, based Cohen Levesque's Joint IntentionsFramework (Levesque, Cohen, & Nunes, 1990; Cohen & Levesque, 1991) Grosz, Sidner,Kraus's SharedPlans (Grosz & Sidner, 1990; Grosz & Kraus, 1996, 1999).110However,fiRobust Agent Teams via Socially-Attentive Monitoringteamwork models may used instead STEAM. Although STEAM usedpilot soccer agents generate collaborative behavior, reused independentlyservice monitoring, i.e., monitored agents assumed team, STEAMused monitoring teamwork. STEAM teamwork models (e.g.,Cohen &Levesque, 1991; Jennings, 1995; Rich & Sidner, 1997) require mutual belief team members joint goals plans. characteristic used monitor teamworksystem. relationship models used secondary monitoring role.discussed greater length Section 7.3.2 Knowledge Monitored Agents Teamagent modeling component responsible acquiring maintaining knowledgeactual relations existideal expected relations.monitored agents. knowledge used constructagents' states' attributes, comparedsection, describe plan-recognition capabilities agent-modeling componentimplementation experiments, i.e., extent knowledge could maintainedmonitored agents' plans necessary. Later sections show fact limited, possiblyinaccurate, knowledge sucient eective failure detection. Thus implementations mayuse optimized agent-modeling algorithms rather full capabilities. Section 3.4discuss additional agent-modeling capabilities, necessary diagnosis.3.2.1 Representationmonitoring teamwork relationships, found representing agents termsselected hierarchical reactive plans enables quick monitoring state, alsofacilitates inference monitored agents' beliefs, goals, unobservable actions,since capture agents' decision processes.representation, reactive plans (Firby, 1987; Newell, 1990) form single decomposition hierarchy (a tree) represents alternative controlling processes agent.reactive plan hierarchy (hereafter referred simply plan) selectionconditions (also referred preconditions) applicable, termination conditions used terminate suspend plans. given moment, agentexecuting single path (root leaf ) hierarchy.path composedplans dierent levels.Figure 3 presents small portion hierarchy, created ModSAF domain.case Example 1, prior way-point,agentsexecuting path be-execute-mission highest-level plan, fly-flight-plan, fly-route,traveling low-level. Upon reaching way-point, supposed switchfly-flight-plan descendents wait-at-point. attackers wouldselect just-wait child wait-at-point, scout would select scout-forwardginningdescendents. course, failing attacker detect way-pointfly-flight-plan selection conditions wait-at-pointfailing attacker continued execute fly-flight-plantermination conditionssatiseddescendents.111fiKaminka TambeExecute-MissionFly Flight Plan (F)Fly RouteWait Point (W)Join Scout (J)Ordered Halt (H)Low LevelWaitScout ForwardTravelingNap EarthContourFigure 3: Portion Hierarchical Reactive Plan Library ModSAF Domain (Team plansboxed. explained Section 3.3).3.2.2 Acquisitionpractical perspective, agents may cooperatively report monitoringagent state using communications, requires communication channelssuciently fast, reliable secure.unfortunately possible many realisticdomains, examples demonstrate (Section 2).Alternatively, monitor may use plan-recognition infer agents' unobservable stateobservable behavior. approach unintrusive robust face communication failures. course, monitor may still benet focused communicationsagents, would critically dependent them.enable plan-recognition using reactive plans (our chosen representation),employed reactive plan-recognition algorithm called RESL (REal-time Situated Leastcommitments). key capability required allow explicit maintenance hierarchicalplan hypotheses matching agent's observed behavior, pruning hypothesesdeemed incorrect useless monitoring purposes.RESL works expandingentire plan-library hierarchy modeled agent, tagging paths matchingobserved behavior agent modeled (see Appendix B pseudo-codealgorithm). Heuristics external knowledge may used eliminate paths (hypotheses)deemed inappropriateindeed heuristics explored shortly. RESL'sbasic approach similar previous work reactive plan recognition (Rao, 1994)team-tracking (Tambe, 1996), used successfully ModSAF domain,share many RESL's properties.However, RESL adds belief-inference capabilitiesused diagnosis process, discussed (Section 3.4).Figure 4 gives simplied presentation plan hierarchies variation Example1, agents correctly detected way-point, i.e., failure occurred (noteplans intermediate levels abstracted gure). scout(Figure 4a) two attackers (Figures 4b, 4c) switched(denotedfly-flight-plan planF) wait-at-point plan (denoted W). outside observer using RESLinfers explanations agent's behavior observing agents. scout continues112fiRobust Agent Teams via Socially-Attentive Monitoringlow-level, one possible ight-methodswait-at-point (W) plans. Thusahead, speed altitude matchingfly-flight-plan (F)tagged possible hypotheses scout's executing plan hierarchy.Similarly,just-waitordered-halt (H)attackers land, RESL recognizes executingplan used service eitherWplan.plana planhelicopters ordered headquarters land immediately.HWThustagged explanations attackers' states (at second levelhierarchies). agents, RESL identies planplan.However,execute-missiontop-levelillustration, actual executing paths agents marked lledarrows.individual modelinghypotheses match observed behavior markedusing dashed arrows. outside observer, course, way knowingpossible hypotheses correct.Execute MissionWait-at-Point (W)Execute MissionExecute MissionFly-Flight-Plan (F)Wait-at-Point (W)Ordered-Halt (H)Wait-at-Point (W)Low-LevelJust-WaitJust-WaitJust-WaitLow-Level(b)(a)Ordered-Halt (H)Just-Wait(c)Figure 4: Scout (a) Attackers' (b, c) actual recognizedabbreviatedreactive planhierarchies.individual modeling hypotheses acquired individual agent (using planrecognition implementation, potentially also communications), monitoringagent must combine create team-modeling hypotheses state teamwhole. monitoring agent selects single individual modeling hypothesisindividual agent combines single team-modeling hypothesis. Severalteam-modeling hypotheses possible given multiple hypotheses individual agents.instance, Figure 4, team-hypothesesexecution-missiontop-level plan, eight dierent team-hypotheses dierentiatedsecond-level plan:(W,W,W), (W,W,H), (W,H,W), (W,H,H), (F,W,W), (F,W,H), (F,H,W), (F,H,H).observer member team, knows executing itself, would stillmultiple-hypotheses teammates' states. instance, attacker Figure 4bmonitoring teammates, hypotheses second level would (W,W,W), (W,W,H),(F,W,W), (F,W,H).avoid explicitly representing combinatorial number hypotheses, RESL explicitlymaintains candidate hypotheses agent individually, combinationsindividual models team hypotheses. Instead, combinations implicitly represented. Thus number hypotheses explicitly maintained grows linearly numberagents.113fiKaminka Tambe3.3 Relationship Violation Detectionfailure-detection component detects violations social relationshipshold among agents.done comparing ideal expected relationshipsactual maintenance agents. teamwork specically, relationship model requiresteam-members always agreeteam plan jointly executed team, similarlyJoint Responsibility (Jennings, 1995), SharedPlans (Grosz & Kraus, 1996).requirement fails actuality (i.e., agents executing dierent team plans)teamwork failure occurred.basic teamwork failure detection algorithm follows.plan-hierarchies processed top-down manner.monitored agents'detection component usesteamwork model tag specic plans team plans, explicitly representing joint activityteam (these plans boxed Figures 3, 5 4).team-plans equal depthshierarchies used create team-modeling hypotheses. hypothesis,plans dierent agents compared detect disagreements. dierence foundindication failure.dierences found, comparison reaches individualplans (non-team, therefore non-boxed gures) failure detected. Individual plans,may chosen agent individually service team plans boxedgures, handled using relationships discussed Section 7instance, suppose failing attacker Example 1 monitoring attacker. Figure 5 shows view hierarchical plan left. pathright represents state attacker (who landed). state inferred example observations made monitoring attacker (here,assuming plan-recognition process resulted one correct hypothesisagent. discuss realistic settings below). Figure 5, dierence woulddetected marked arrow two plans second level top.fly-flight-plan team-plan (onwait-at-point team-plan (on right).failing attacker executingleft),attacker executingdisagreementteam-plan executed failure teamwork.Execute MissionExecute MissionFly-Flight-PlanWait-at-PointFly-RouteTravelingLow-LevelJust-WaitFigure 5: Comparing two hierarchical plans. top-most dierence level 2.114fiRobust Agent Teams via Socially-Attentive MonitoringDetecting disagreements dicult multiple team-modeling hypotheses, sincemay imply contradictory results respect failure detection: hypotheses may imply failure occurred team, others may not. Unfortunately,expected realistic applications. instance, Figure 4 (Section 3.2) shows several hypotheses possible based observations. However, one hypotheses,(W,W,W), implies failure occurredall agents agreement team-planexecutingwhile another hypothesis, (F,W,H), implies failures occurred.limit reasoning small number team hypotheses, restrictingfailure-detection capabilities, use disambiguation heuristic ranks team-modelinghypotheses levelcoherencerepresent. heuristic provided initialsolution. Later sections examine additional heuristics.Denition 1.coherencelevel multi-agent modeling hypothesis denedratio number agents modeled number plans contained hypothesis.denition results partial ordering hypotheses set, least coherenthypothesis (one assigns agent dierent plan team-mates),coherent hypothesis (that assigns plan team members).instance,hypothesis (F,W,H) would lowest level coherence, 1, since implies completebreakdown teamworkevery agent executing dierent plan. hypothesis (W,W,W)would coherence level 3, highest level coherence group three agents,since assigned plan. Ranked would hypothesis(W,W,H), single teamwork failure (disagreement W H) coherence level3/2.detection component selects single maximally-coherent team-modeling hypothesis(ties broken randomly).intuition using coherence failures agree occurdespite agents' attempts teamwork. Thus expect agreements disagreements team.coherence level team-hypothesis inversely relatednumber teamwork failures implied hypothesis. Selecting maximally-coherent hypothesis therefore corresponds minimum-number-of-failures heuristic commonly useddiagnosis (Hamscher, Console, & de Kleer, 1992).case depicted Figure 4, complete detection process may conceptu-1alized follows .Suppose one attackers, whose hierarchy describedFigure 4b, monitoring team.First, collects plan hypotheses tophierarchy agent (including ). case, {execute-mission},{execute-mission}, {execute-mission}. one team-modeling hypothesis builtthese: (execute-mission,execute-mission, execute-mission).Since hypoth-esis shows disagreement occurs level, process continues second level.Here, hypotheses rst agent left {F,W}, monitoring second agent(since knows state) one possibility {W}, third agent {W,H}.saw above, maximally team-coherent hypothesis (W,W,W) selected. Sinceindicate failure, process continues third level.agentsexecuting individual plans, comparison process stops. Algorithm 2 AppendixB provides greater details process.1. implementations may make use optimized algorithms heuristics integratedagent-modeling algorithm.115fiKaminka Tambesub-teams introduced, dierence team-plans may explainedagents question part dierent sub-teams.Sub-team members stillagree joint sub-team plans, may dier onesub-team next. now, let us assume teams considerationteams, dened Denition 2.simplemake denition service later analytical resultsappear condition. return issue sub-teams Section 7.1.Denition 2.say teamsimple,plan-hierarchy involves dierentteam plans executed dierent sub-teams.Intuitively, idea simple team, members team jointly executeteam plans hierarchy. denition somewhat similar denitionground team(Kinny, Ljungberg, Rao, Sonenberg, Tidhar, & Werner, 1992),allow sub-team members team joint plan dierentmembers.3.4 Relationship Diagnosisdiagnosis component constructs explanation detected failure, identifyingfailure state facilitating recovery.diagnosis given terms set agentbelief dierences (inconsistencies) explains failure maintain relationship.starting point process detected failure (e.g., dierence team-plans).diagnosis process compares beliefs agents involved produce setinconsistent beliefs explain failure.Two problems exist practical applications procedure.First, monitoringagent likely access beliefs held monitored agents, sincefeasible practice communicate agents' beliefs other. Second,agent real-world domain may many beliefs, many vary amongagents, though irrelevant diagnosis. Thus relevant knowledgemay simply accessible, may hidden mountains irrelevant facts.gain knowledge beliefs monitored agents without relying communications,diagnosis process uses process belief ascription. agent-modeling component (using RESL implementation) maintains knowledge selection terminationconditions recognized plans (hypotheses). recognized plan hypothesis, modeling component infers termination conditions plan believed falsemonitored agent (since terminated plan). also found usefuluse additional heuristic, infer selection conditions (preconditions)planbegun executiontrue. idea plan selectedexecution, preconditions likely hold, least short period time.heuristic involves explicit assumption part system new planrecognized soon begins execution.Designers domains need verifyassumption holds.agent i, inferred termination selection conditions make set beliefsBi agent. instance, suppose agent hypothesized switchedexecutingfly-flight-planwait-at-point.RESL infers agent believesway-point detected (a selection condition116wait-at-point).addition,fiRobust Agent Teams via Socially-Attentive MonitoringRESL infers agent believes enemy seen, orderwait-at-point).received base halt mission (negated termination conditionsdetermine facts relevant failure, diagnosis component usesteamwork model. teamwork model dictates beliefs agents hold mustmutually believed agents team.dierence detectedbeliefs certain failure, team members agree issues agreementmandatory participation team.teamwork model thus speciesbeliefs contained Bi sets mutual, therefore consistent:[Bi 6`?inconsistency detected, diagnosis procedure looks contradictions (disagreements) would cause dierence team-plan selection. dierence beliefs servesdiagnosis, allowing monitoring agent initiate process recovery, e.g.,negotiating conicting beliefs (Kraus et al., 1998).example, shown Section 3.3, two attackers Example 1 (Section 2) dierchoice team-plan: One attacker continuing executionfly-flight-planplan, helicopters formation. attacker detected waypoint, terminatedfly-flight-planwait-at-point,switchedlanding immedi-ately (Figure 5). failing attacker monitors team-mate, detects dierenceteam-plans (Section 3.3), detected dierence passed diagnosis. failingattacker makes following inferences:1.Fly-flight-plan three termination conditions:(a) seeing enemy, (b) detectingway-point, (c) receiving order halt. failing attacker (left hierarchyFigure 5) knows belief none conditions hold, thusB12.Wait-at-point=f:W ayP oint; :E nemy; :H altOrdergone selection condition:way-point detected.termination condition scout sent message join it, identiedenemy's position. diagnosis component case therefore infersattacker (right hierarchy Figure 5)B2=fW ayP oint; :S coutM essageReceivedgThen,B1 [ B2=f:W ayP oint; W ayP oint; :E nemy; :S coutM essageReceived; :H altOrderginconsistent.inconsistency(disagreementattackers)f:W ayP oint; W ayP ointg, i.e., contradictory beliefs W aypoint. Thus failingattacker knows team-mate seen way-point. choose quietly adaptbelief, thereby terminatingfly-flight-plan117selectingwait-at-point,fiKaminka Tambemay choose recovery actions, negotiating attacker whetherway-point reached.found diagnosis procedures useful many failures detectedsocially-attentive monitoring (see Section 4 evaluation discussion).since paper focuses monitoring selectivity problemHowever,detection, leaveinvestigation diagnosis procedures future work.4. Monitoring Selectivity Centralized Teamwork MonitoringUsing socially-attentive framework Section 3 systematically examine failurepermutations Examples 1 2 (Section 2) centralized teamwork monitoringconguration, single team-member monitoring team.vary agentsfailing (attacker, attacker scout, etc.) role monitoring agent (attackerscout). report empirical results detecting diagnosing failures cases.Using empirical results guide, explore centralized teamwork monitoring analytically. show even monitoring uncertainty, centralized teamwork monitoringprovide either sound complete detection results (but both).starting point exploration, monitoring agent uses single maximallycoherent team-modeling hypothesis discussed Section 3.3. begin Example 2.normal order executionW,executionenemy's position.wait-at-point (W),followedjoin-scout (J).two attackers land wait scout visually identifyUpon identication, scout sends message join it,triggers selectionJplan, terminationWplan. executingJ,scout hovers low altitude, waiting attackers join it. failurespart attackers (they cannot receive message) part scout (itcannot send it). failures arise, instance, radio broken team-membersrange. agent fails, continues executeWinstead switchingJ.Table 1 summarizes permutations Example 2. permutation number appearrst column.next three columns show actual plans selected threeagents A1, A2 A3 permutation.second-to-last column shows whetherrelationship failure occurred case, i.e., whether disagreement existsagents. Finally, last column details physical conditions case.possible failure permutations: case 1, none agents failed.attacker failed.cases 2 3 onecase 4 scout failed send message attackers failedreceive it. case 5 scout identify enemy's position (so message sent,three agents continue executesince attacker switchJWplan). permutations possible,plan without scout.instance, case 2 Table 1 corresponds Example 2. scout (A3) detectedenemy, switched planJ,sent message attackers join it. One attacker(A2) received message, switched planJ, began ying towards scout.However,remaining attacker (A1) failed receive message, maintains position,continuing executeW failing switch J. Since agents longer agreementteam plan jointly executed, teamwork failure occurred. Conditionmonitors used original failure case monitor scout's message. Howeverfailures communications resulted monitoring conditions rendered useless.118fiRobust Agent Teams via Socially-Attentive MonitoringCase#Actual Executing PlansRelationship FailurePhysical?Attacker A1Attacker A2Scout A3OccurredFailure1JJJ--2WJJ+A1 fails receive3JWJ+A2 fails receive4WWJ+A3's message lost5WWW-Enemy identiedTable 1: possible failure permutations broken radio-link scenario (Example 2).One key issue raised case 5 Table 1. Here, due scout's inability identifyenemy's position (perhaps due failure scout's part, perhaps enemysimply there), three helicopter pilots remain agreement enemyidentied. Here, even though clearly pilots failing make progress towardstask goals (the scout continues search enemy indenitely), relationship failuretaking place, since agents maintaining teamwork relationship failingmake progress. clearly demonstrates failures necessarily relationshipfailures.Table 2 presents results scout monitoring teammates Example 2, usingmaximally team-coherent hypothesis basis detection. rst column showscase number, reference Table 1. next three columns show scout's (A3's)hypothesis plan agent executing according maximal coherenceheuristic. next two columns show whether failure detected, whetherdiagnosed correctly. last column shows detection class (discussed below).Case#A3's Hypothesized Executing PlansRelationship FailureDiagnosisDetectionDetected?Success?ClassJ-n/aTrue NegativeJ++True PositiveWJ++True PositiveHJ+-True PositiveWW-n/aTrue NegativeAttacker A1Attacker A2Scout A31JJ2WJ3J4H5WTable 2: Scout's (A3) monitoring results permutations Example 2.example, case 2 Table 2 shows results scout monitoring originalfailure Example 2 (Section 2). Using RESL, selecting maximally-coherent hypothesis, scout hypothesizes non-moving attacker executingmoving attacker executingselected planJJ (case 2, column 3).W (case2, column 2),scout course knows(case 2, column 4). violation teamwork relationship thusdetected (case 2, column 5), since A1'sWagreement rest team'sJ.Furthermore, diagnosis successful identifying cause failure, i.e.,fact enemy's position identied scout, knowledgepassed failing attacker (case 2, column 6).119fiKaminka Tambelast column Table 2 shows detection class failure. detection classcase one of: true positive, true negative, false positive, false negative.correspond following possible monitoring outcomes: true positive outcomerelationship failure actually occurred, detected. true negativefailure occurred, system correctly reports none detected.false positive failure occurred, system nevertheless incorrectlydetects one, false negative failure occurred, system failsdetect it. Table 2 shows permutations Example 2 teamwork monitoringtechniques encounter problematic false positive false negative cases.closer look results hints key contribution paper addressingmonitoring selectivity problem: Eective failure detection take place despite useuncertain, limited, knowledge monitored agents. case 4 Table 2, monitoring agent able detect failureinvolved.despite wrong state agentsscout believes two attackers executingW. Hplan, actually executing,H (ordered-halt)selected command receivedheadquarters halt execution hover place.scout's perspective, hov-ering attacker therefore inferred executingHW.Thus two equally-rankedmaximally-coherent hypotheses exist: two attackers either executingexecutingH.Wrandom selection made, case resulted wrong hypothe-sis selected. Nevertheless, violation teamwork relationships detected,neitherHWagrees scout'sJ.However, last column case 4 shows (in Table 2), diagnosis proceduressensitive selection team-modeling hypothesis. hypothesis used casecorrectly reect true state agents, despite scout's successdetect failure case, diagnosis procedures fail provide correct diagnosis (thediagnosis successful two failure cases). phenomenon repeatsempirical results provide below:diagnosis failed whenever hypothesis chosenincorrect, although sucient detection.intend explore ways improveaccuracy hypotheses future work, therefore address issuepaper anymore. failure detection capabilities signicant improvementthemselves, since agents know certainty failure occurred, evendiagnosis incorrect.Many social physical failures successfully captured using team-coherenceheuristic monitoring selectivity. fact, permutations Example 2, matterone agents monitor, failures maintain relationship (i.e., physicalfailures except one team remains agreement) detected reliably,although sometimes diagnosis failed.result especially surprising consideringsingle agent monitoring. Previous monitoring methods (condition monitorscommunications) unable detect failures, despite used threeagents .Tables 3 4 present empirical results, basis Example 1.Table 3presents failure permutations Example 1 format Table 1. normal or-fly-flight-planwait-at-point (W) plan,der execution plans follows: agents jointly execute(F) plan detect way-point. switchtwo attackers land scout continues ahead identify enemy.120fiRobust Agent Teams via Socially-Attentive Monitoringfailures part agents detect way-point, thus switchWplan.Case#Actual Executing PlansRelationship FailurePhysical?Attacker A1Attacker A2Scout A3OccurredFailure1WWW--2FWW+A1 vision fails3WFW+A2 vision fails4FFW+A1, A2 vision fails5WWF+A3 vision fails6FWF+A1, A3 vision fails7WFF+A2, A3 vision fails8FFF-A1,2,3 vision failsTable 3: failure permutations undetected way-point scenario (Example 1).Case#A1's Hypothesized Executing PlansRelationship FailureDetectionAttacker A1Attacker A2Scout A3Detected?ClassWWW-True Negative2FWF+True Positive3WFW+-True PositiveFalse NegativeFalse Negative145FWFWFW6FWF+True Positive7WFF+True Positive8FFF-True NegativeTable 4: Attacker's (A1) monitoring results permutations Example 1.Table 4 present monitoring results permutations Example 1.attacker A1 monitoring team using maximally team-coherent hypothesisdetecting failures. results show A1 successful detecting teamwork failurestwo (cases 4-5, highlighted bold face).two false outcomes false negatives. cases, monitoringattacker A1 picked incorrect hypothesis scout, since scout's actions leadambiguous interpretations. scout forward (to scout enemy) detectedway-point (planW), also (then would ying formationplan F).use maximal team-coherence heuristic causes A1 prefer hypothesisscout agreement attackers fact not. example, case 4, twoattackers failed detect way-point executingF. Observing scout,F W. However, believingmonitoring attacker A1 sure whether scout executingscout executingF results maximally-coherentteam-modeling hypothesis (allagents agreement), believing scout executing121Wresults lessfiKaminka Tambecoherent hypothesis. Thus A1 selects wrong hypothesis, case fails detectteamwork failure.maximal team-coherence heuristic detect failures despite using incorrect hypotheses. Unfortunately, hypotheses also lead false-negatives seenTable 4. However, none experiments resulted false-positive result, i.e., resultsystem detected failure reality none occurred. Thus heuristicprovided sound results cases. able formally prove property holdsgeneral maximal team-coherence heuristic used.First, address matter notation. Let agent monitor agent B ,executing plan P .denote (A; B =P ) set agent-modeling hypothesesA's agent-modeling component constructs based B 's observable behaviorexecution P . words, (A; B =P ) A's set plans match B 's observablebehavior.Note monitors itself, direct access state(A; A=P )=fP g. Using modeling notation, make following denitionsground assumptions underlying knowledge used monitoring:Denition 3.agent-modelingGiven monitoring agent A, monitored agent B , say A'sagent Bcompleteplan P may executed B, P 2(A; B=P ).set (A; B=P ) typically include matching hypotheses besides correcthypothesis P, guaranteed include P. Following denitionindividualagent-modeling completeness, dene group-wide team-modeling completeness:Denition 4.A'sLet agent monitoring team agents B1 ; ; Bn . sayteam-modelingteamcompleteA's agent-modeling B1 ; ; Bncomplete.Denition 4 critical guarantee capabilities explore analyticallysection next. generally holds use RESL ModSAF RoboCupdomains, make explicit service applications techniquesdomains.Armed denitions, formalize failure detection capabilities suggested empirical evidence Theorem 1.Theorem 1. Let monitoring agentmonitor simple team . A's team-modelingcomplete, uses maximally team-coherent hypothesis detection,teamwork failure detection results sound.Proof.show failure occurred, none detected, thusfailure detected fact failure. Let a1 ; : : : ; agent members .agent ai executing plan Pi (1 n). Thus collectively, group executing(P1 ; : : : ; Pn ). failure occurred, agents executing planP0 ,i.e., 8i; Pi = P0 . Since A's team-modeling complete, correct hypothesis (P0 ; : : : ; P0 )going set team-modeling hypotheses H .Since maximally team-coherent hypothesis, either selected, dierent hypothesiscoherence levelselected. hypothesis coherence level correctone implies failure detected. Thus detection procedure sound.122fiRobust Agent Teams via Socially-Attentive MonitoringDespite uncertainty knowledge used, sound failure-detection guaranteed usingmaximal team-coherence heuristic.one answer monitoring selectivityproblem. However, seen Table 4, failures may pass undetected usingheuristic (i.e., may result false-negatives).may therefore unfortunatelycomplete guaranteedincomplete.Detection using maximal team-coherencemay prefer monitoring systemdetect teamwork failures.incoherencetherefore experimented maximal team-heuristic, inversemaximal team-coherence heuristic. heuristic prefers hypotheses suggestfailures, rather less.Table 5 gives monitoring attacker A1's view team,incoherentsimilar Table 4, using maximally team-hypothesis. shows indeedusing maximally team-incoherent hypothesis lead false-negative detectionscases 4 5 (in contrast cases Table 4).CaseRelationship FailureDetectionAttacker A1Scout A3Detected?+Class1WAttacker A2False Positive2FHW+True Positive3WFF+True PositiveFFW+True Positive#A1's Hypothesized Executing Plans45HFWHF+True Positive6FHW+True Positive7W8FFFF+W+True PositiveFalse PositiveTable 5: Attacker's (A1) monitoring results permutations Example 1, using team-incoherence.Guided results, formally show team-incoherence heuristic leadsdetection procedurecomplete.Theorem 2. Let monitoring agent monitor simple team . A's team-modelingcomplete, uses maximally team-incoherent hypothesis detection,teamwork failure detection results complete.Proof. Analogous Theorem 1, proof provided appendix A.However, successes oset false positive outcomes cases 1 8 Table 5.cases, failures occurred, monitoring system falsely reported detectedfailures. practice, may lead costly processing many false alarms.Ideally, detection capabilities soundcomplete. Unfortunately,show coherence-based disambiguation scheme exists results soundcomplete detection. show Theorem 3 provide sound complete detection,disambiguation method inconsistent: Given set possible matchinghypotheses, sometimes rank one hypothesis top, sometimes another.Theorem 3. LetH complete team-modeling hypotheses set, modeling simple team.exists disambiguation scheme (1) uses coherence alone basis123fiKaminka Tambedisambiguation H , (2) deterministic selection, (3) results soundcomplete failure detection.Proof.Let disambiguation scheme leads complete sound detectionuses knowledge coherence hypotheses selecting disambiguated hypothesis. Suppose contradiction deterministic, thus consistent, selectionhypothesis H , i.e., given H , set candidate hypotheses, appliesdeterministic procedure choose one hypothesis based coherence.Sinceuse knowledge outside coherence candidate hypotheses, givenset candidates, always choose hypothesis.Letmonitoring agent using . Let B monitored agent, whose actions identicalexecuting team plans P1 ; P2 .P2 , (Am ; B =P1 )=Thus, cannot determine whether B executing P1(Am ; B =P2 )=fP1 ; P2 g. B executing P1 , 'sH=f(P1 ; P1 );hypotheses set(P1 ; P2 )gSince leads complete sound detection, choose (P1 ; P1 ). Now,B executing P1 P2 , respectively, matching hypothesis set H denedabove. must select (P1 ; P2 ). Since set candidate hypothesis Hused case, information supplied, must non-deterministicselection disambiguated hypothesis, contradicting assumption.empirical analytical results show use single disambiguated hypothesis leads improved, imperfect, failure-detection results, compared monitoringconditions communications previously used. empirical results Tables 2, 4,5 establish benets teamwork monitoring technique: physical failuresdetected. However, analytical results (Theorems 1, 2, 3) show results lessperfect. algorithms either sound complete, both. complete monitoring, would require additional procedures dierentiate true positivesfalse ones, e.g., focused communication. procedures often expensive.reduce need costly verication letting go insistence singlehypothesis, focusing instead maintaining two hypotheses: maximally-coherent hypothesis maximally-incoherent hypothesis.Table 6 shows portion full setteam-hypotheses available attacker A1 monitoring team. total numberhypotheses presented table 24, many 4 co-existing single case,thus maintaining full set hypotheses would expensive. However, two inverseheuristicsteam-coherence incoherencerepresent two extremes spacehypotheses.agree failure exists, failure actually occurred, sinceteam-coherent hypothesis guarantees soundness (Theorem 1). agree failureexists, failure took place, since team-incoherent hypothesis guarantees completeness (Theorem 2). disagree (i.e., team-coherent hypothesis implyfailure, team-incoherent hypothesis does), monitoring system cannot sureeither way, must revert back verication.revised detection algorithm oers signicant computational savings comparedsingle team-incoherent hypothesis approach. complete unsound, signicantly124fiRobust Agent Teams via Socially-Attentive MonitoringCase#12345678A1's Hypothesized Executing PlansRelationship FailureDetectionScout A3Detected?ClassHF+False PositiveHW+False PositiveWWF+False PositiveWWW-True NegativeFHF+True PositiveFHW+True PositiveFWF+True PositiveFWW+True PositiveWFF+True PositiveWFW+True PositiveFFW+True PositiveFFF-False NegativeWHF+True PositiveWHW+True PositiveWWF+True PositiveWWW-False NegativeFHW+True PositiveAttacker A1Attacker A2WWFHF+True PositiveFWW+True PositiveFWF+True PositiveWFF+True PositiveWFW+True PositiveFFW+False PositiveFFF-True NegativeTable 6: portion attacker's (A1) monitoring hypotheses implied resultsranking used select single hypothesis case.reduces need verication, since least team-coherent hypothesis impliesfailures, verication necessary. requires representing two hypotheses,thus still computationally cheaper maintaining exponential number hypotheses.example, using maximally team-incoherent hypothesis permutations Example1 results need verify eight cases seen (5). However, combinehypothesis maximally team-coherent hypothesis (e.g., Table 4),need verify four (50% ) cases. cases 2, 3, 6, 7 agreementtwo hypotheses failure occurred, thus verication required.monitoring agent therefore address monitoring selectivity problem balancingresource usage guaranteed performance monitoring algorithm used.Either simpler single-hypothesis algorithms would utilize one hypothesiscase, detection capabilities guaranteed sound complete, both.complex algorithm, two hypotheses would reasoned case,125fiKaminka Tambealgorithm would complete require verication fewer cases comparedsimple-hypothesis complete algorithm.5. Monitoring Selectivity Distributed Teamwork Monitoringsection focuses monitoring selectivity exploiting key opportunity execution monitoring multi-agent environmentsit monitored agentsdistributed, monitoring agents distributed well. beginsimple scheme selecting single maximally team-coherent hypothesis. Since centralizedteamwork monitoring successful addressing permutations Example 2, focuspermutations Example 1 (Table 3), centralized teamwork monitoringattacker resulted false-negative detections (cases 4-5 Table 4).distributed teamwork monitoring scheme, single attacker monitorteammates, scout (and attacker) also engage monitoring. Table7 presents monitoring results failure permutations, scoutmonitoring agent.nd scout successfully detects two failure casesattacker failed detect, compensating attackers' monitoring mistakes. Furthermore, since scout used maximal-coherence heuristic, detection soundverication required. reason scout's success attackers' actionscase, although ambiguous, support hypothesis matchedscout's plan. words, regardless plan attackers executingtwo cases, dierent plan executed scout.CaseRelationship FailureDetectionDetected?ClassW-True NegativeF+True PositiveFW+True PositiveFW+True PositiveHHF+True PositiveFHF+True Positive7HFF+True Positive8FFF-True Negative#A3's Hypothesized Executing PlansAttacker A1Attacker A2Scout A31WW2FW3W4F56Table 7: Scout's (A3) monitoring results permutations Example 1, using teamcoherence.Thus agents engaged monitoring permutations Example 1, detection wouldsound complete. actual failure cases (and those) would least oneteam-member detects failure. attempt formally dene general conditionsphenomenon holds.Denition 5.say two team-plans P1 ; P2 ,observably-dierent rolesR1 ; R2given agent B fullls roles R1 ; R2 two plans, resp., monitoring agent(dierent B ) (A; B =P1 ) \ (A; B =P2 )observably-dierent roles P1 P2 , call B126=key agent.;.say BfiRobust Agent Teams via Socially-Attentive MonitoringIntuitively, B key agent observably dierent roles two plansmonitoring agent dierentiate B 's behavior executing P1 executingP2 . instance, attackers observably dierent rolesW(in land).F(in y)However, observably dierent rolesrequire land. scout observably dierent rolesHW H,W (ying)(landing).key-agent basis conditions self-monitoring teamdetect failure agent using team-coherence. rst prove lemmaconditions single given agent detect failure. use lemmaprove conditions least one agent given team detect failure.Lemma 1. Suppose simple teamself-monitoring (all members team monitorother) using maximally team-coherent heuristic (and assumptionagent, team-modeling complete). Let A1 ; A2 monitoring agents membersexecuting P1 ; P2 , respectively. A1 would detect failure maintaining teamworkrelationships agent A2 , A2 key-agent P1 ; P2 .Proof.See appendix A.A1 knows executing P1 .A2 executing P2 , key-agent P1 P2 ,A1 guaranteed notice dierence exists A2 , since A2acting observably dierent would executing P1 . Note, however,A2 may may detect dierence, since A2 's perspective, A1 's behavior maymay explained P2 . A2 detect dierence A1 's roles P1 P2also observably-dierent. However, since A1 detected failure, alertteammates, diagnose failure, choose corrective action.want guarantee teamwork failure always detected leastone agent, must make sure possible combination plans,least one key-agent whose roles observably dierent.lemma showsagents monitoring agent notice failure one occurs. aim, deneobservably-partitioned set plans employed team.Denition 6.set P team-plans saidobservably-partitionedtwo plansPi ; Pj 2 P exists key-agent Aij . set Aij agents calledset P .key agentsinstance, set team-plans helicopter pilots team usingexamples (Fly-Flight-Plan (F),Wait-at-Point (W), Ordered-Halt (H), Join-Scout(J)) observably-partitioned. attackers land W H, F J. scoutlands J H, ies W F. Table 8 shows agents observably dierentroles two plans set. instance, nding cell intersectionHrowWcolumn, nd scout observably dierent roles twoplans. Indeed, scout lands command received halt execution (H), iesscout enemy's position executingW.Here, since agents observably-dierent roles least two plans, key agents set {teamattackers scout.127W , F , H, J }includes membersfiKaminka TambeFly-Flight-Plan (F)Wait-at-Point (W)Ordered-Halt (H)-AttackersAttackersScoutAttackers-ScoutScout AttackersFWHJJoin-Scout (J)AttackersScout-AttackersScoutScout AttackersAttackers-Table 8: Observable partitioning helicopter pilot team ModSAF.Theorem 4. simple team (1) employs observably-partitioned set team-plans O,team-members monitor members key agents set O, (2) using complete teammodeling (3) maximally team-coherent hypotheses, teamwork failure detectionresults sound complete.Proof.theorem 1 know detection would sound. show complete,prove least one agent detect dierence others wheneverteam-members executing plan (i.e., failure occurring). Supposeteam currently divided team-plans must executed, i.e., agentsai ; aj team executing team plans Pi ; Pj , respectively, Pi 6= Pj .Thus failure occurred. Let K key agents set . Since team observablypartitioned, Pi ; Pj exists least one key agent a1 2 K . three cases:case (i).a1 executing Pi . case agent executing Pj would detect dierencea1 would therefore detect failure (lemma 1).case (ii).a1 executing Pj . case agent executing Pi would detect dierencea1 would therefore detect failure (lemma 1).case (iii).a1 executing plan Q. roles must observably dierent QPi , Q Pj (or both), thus agent executing Pj and/or Pi would detectfailure. case a1 's roles observably dierent Q PiQPj impossible, since monitoring agent(Am ; a1 =Pi ) \ (Am ; a1 =P j ) fQg 6= ;Contradicting a1 key agent Pi ; Pj .Since three cases, least one agent would detect failure one occurred.Therefore, failure detection complete. Since also sound seen, detectionsound complete.theorem shows distributed teamwork monitoring resultsound completekeyfailure-detection, using simple algorithm. team-member monitorsagents 2 , using maximally team-coherenthypothesis. detects failure, certainlyone occurred. agent detects failure, indeed failure occurred.simple distributed algorithm, attention-focusing features guaranteedsoundness completeness contrasts complex centralized algorithmdiscussed previous section (Section 4). algorithm's eectiveness relies2. monitoring team-member know key agents are, knows exist,monitor team-members. increases monitoring, sound complete failure detectionstill guaranteed.128fiRobust Agent Teams via Socially-Attentive Monitoringcondition observably-partitioned set plans, distribution monitoring.corollary Theorems 3 4 key agents available distributedcase, failure detection either sound complete, both. even key agentsavailable, centralized teamwork monitoring still complete sound.Fortunately, observable-partitioning dicult property design: Teamsoften composed agents role plan, general,roles observable dierences them. instance, helicopter pilot teamModSAF domain typically executes set plans property, Table 8demonstrates.team, however, observably-partitioned, may case two agentsexecuting dierent plan, agent able detect using teamcoherence heuristic. minimal case occurs two agents, A1 A2executing plans P1 P2 , respectively, P1 P2 observably dierent,(A2 ; A1 =P1 ) \ (A1 ; A2 =P 2)=fP1 ; P2 gresult A1 A2 believing agreement them.check situation made part plan design process, markingpointsriskyexecution detection either sound complete (Theorem 3),verication (e.g., communications) prescribed pro-actively. Or, check couldinserted protocol run-time analysisthe agent would simulate other'shypotheses matching actions, detect risky points dynamically.6. Using Socially-Attentive Monitoring O-Line Congurationdemonstrate generality socially-attentive monitoring framework,section examines re-use teamwork monitoring domains diagnosis recoveryevery failure infeasible execution. Examples domains include teamsports, military human team training (Volpe, Cannon-Bowers, & Salas, 1996),multi-agent domains.dynamic nature domain, hard real-time deadlines,complexity agents involved (e.g., human team members) make diagnosis recoverydicult.Even failure diagnosed, often late eective recovery.environments, monitoring agent often concerned trends performance.information important long-term design evaluation analysis, neednecessarily calculated on-line. results analysis meant feedbackagents' designer (coach supervisor, humans).end, developing o-line socially-attentive monitoring system calledTeamore (TEAmwork MOnitoring REview). Teamore currently uses execution tracesmonitored agents perform monitoring, rather using plan recognition. Thusneed worry uncertainty plan-recognition, real-time performance. Instead, knows certainty agent's plans execution. Teamoreaccumulates several quantitative measures related teamwork, including Average-Timeto-Agreement measure (ATA, short), measure level agreement team.build failure detection algorithm, aggregate failures quantitatively.focus ATA measure.129fiKaminka TambeTeamore denesswitchtime interval beginning point team-member (at least one) selects new team plan execution team, endingpoint team agreement team-plan executed.perfect teamwork, team-members select new team-plan jointly, always remainagreement.realistic scenario, agents take longer switch,initially teamwork failure occur. rst team-member select new plandisagreement teammates, either rejoins executingoriginal plan, join selecting new plan. switch begins detectedfailure ends failures detected.Figure 6 shows illustration switch. three agents begin initial stateagreement joint execution Plan 1 (lled line). Agent 1 rst agent switchPlan 2 (dotted line), followed Agent 3, nally Agent 2.switchinterval begins instance Agents 1 selected Plan 2, time three agentsregained agreement (but time Plan 2).SwitchLegend:Plan 1Agent 3Plan 2Agent 2Agent 1TimeFigure 6: illustration switch. three agents switch plan 1 plan 2.Teamore keeps track lengths time failures detectedresolved. ATA measure average switch length (in time ticks) per completeteam run (e.g., mission ModSAF, game RoboCup). perfect team wouldswitches length zero, therefore ATA 0. worst team would onebeginning task execution end it, would agreeteam plan executed. instance, RoboCup game lasts 6000 ticks.worst possible team would one switch game, length 6000. ThusATA scale RoboCup goes 0 (perfect) 6000 (worst).used ATA measure analyze series games two RoboCup simulationleague teams, ISIS'97 ISIS'98 (Marsella et al., 1999) xed opponent, Andhill'97(Andou, 1998).games, varied use communications teamsevaluate design decisions use communications. approximately half games,players allowed use communications service teamwork. half,communications agents disabled. ISIS'97 played approximately 15 gamessettings, ISIS'98 played 30 games communication settings.Table 9 shows mean ATA values games, two sub-teams (eachthree members) ISIS'97 ISIS'98 (ATA values calculated separately subteam). rst column shows sub-team results refer row. second130fiRobust Agent Teams via Socially-Attentive Monitoringcolumns shows mean ATA sub-team, communications used.third column shows mean ATA communications used. next column showssize ATA reductionthe drop mean ATA values communicationsintroduced.last column shows probability null hypothesis two-tailedt-test dierence ATA means. probability dierence duechance, thus smaller numbers indicate greater signicance.ISISMean ATAMean ATAATAt-test prob.sub-teamcomm.Comm.Reductionnull-hypothesis32.805.7927.017.13e-13'97 Goalies'97 Defenders57.56.8150.69.45e-10'98 Goalies13.283.659.639.26e-16'98 Defenders12.993.989.017.13e-5Table 9: Average-Time-to-Agreement (ATA) games Andhill'97.Clearly, signicant dierence emerges communicating noncommunicating versions sub-team.ATA values indicate sharing infor-mation way communications signicantly decreases time takes team-memberscome agreement selected plan. result agrees intuitionsrole communications, sense, may surprising.However, ATA reduction magnitudes indicate ISIS'98 may much less sensitive loss communications ISIS'97. dierences ATA values ISIS'97approximately triple, nearly four times, great ISIS'98.explanationphenomenon ISIS'98 composed players improved capabilities monitoring environment (such better knowledge environment).ISIS'98therefore dependent communications teams, ISIS'97, composedplayers lesser environment monitoring capabilities. ISIS'98 players better ableselect correct plan without relying teammates.Thus, would ablemaintain level performance communications used. contrast,ISIS'97 players rely passing information (monitoring other)communications, took much longer establish agreement communications available.validate hypothesis suggested ATA measurements looking overallteam-performance games, measured score dierence end game.Table 10 shows mean score dierence series games Andhill'97.rst column lists communications settings (with without).third columns showmeansecondscore-dierence games ISIS'97 ISIS'98.bottom row summarizes results t-tests run set games, determinesignicance level dierence mean score-dierences. score-dierenceresults corroborate ATA results. dierence mean score-dierence indeedstatistically signicant ISIS'97 games, signicant ISIS'98 games. supportsexplanation situationally aware ISIS'98 indeed better able handleloss communications ISIS'97.131fiKaminka TambeISIS'97ISIS'98Communication Used-3.38-1.53Communication Used-4.36-2.13t-test p/null hypothesisp=0.032p=0.13Table 10: ISIS'97 ISIS'98 mean score dierence Andhill'97, changing communications settingsgeneral lesson emerging experiments trade-o exists addressing monitoring selectivity problem. knowledge maintained teammates(here, via communications) traded, extent, knowledge maintainedenvironment.designer therefore range alternative capabilitieschoose agents. Dierent domains may better facilitate implicit coordination monitoring environment, others require agents rely communications explicitknowledge team-members handle coordination.ATA results support additional conclusions, especially combined generalperformance measure score-dierence.illustrate, consider plotsactual data games. Figure 7 plots ATA values four variants,Goalies sub-team.graph plots approximately 60 data-points.see Figure7 communications used, ISIS'97's ATA values still generally betterISIS'98's ATA without communications. Thus, despite importance, individual situationalawareness able fully compensate lack communications.Average Time Agreement (ATA)9080706050403020100ISIS98/Comm.ISIS97/Comm.ISIS98/No-Comm. ISIS97/No-Comm.Goalies Sub-Team(a) ATA Values Goalies subteamFigure 7: ATA values Goalies sub-teams games Andhill'97.132fiRobust Agent Teams via Socially-Attentive MonitoringTeamore demonstrates reuse teamwork monitoring techniques developedearlier sections o-line conguration. designer ISIS'97 set agentsuse communications, since signicant improvement score-dierence.contrast, without communications, ISIS'98 players able maintaincollaboration. Thus communications takes precious resources, relatively safelyeliminated ISIS'98 agents' design, development eorts directedcomponents agents.7. Beyond Teamworkpresented general socially-attentive monitoring framework detect failuresmaintaining agreement joint team plans.However, eective operation teams oftenrelies additional relationships, briey address section.7.1 Richer Agreement Model: Agreeing Disagreeteamwork model requires joint execution team plans. service agreed-uponjoint plans, agents may sometimes agree execute dierent sub-plans individually, splitsub-teams execute dierent sub-team plans. Two examples may serve illustrate.Example 3.ModSAF domain, helicopters engage enemy repeatedly followingmasking ), popping (unmask-following three steps: hiding behind hill trees (ing ),shooting missiles enemy, back hiding. variationsplan, required make sure two helicopters shooting time.course, due limits communications, helicopters fail unmask time.Example 4.RoboCup domain, 11 players ISIS'97 ISIS'98 (Marsellaet al., 1999) divided four sub-teams: mid-elders, attackers, defenders, goalies(the goalie two close defenders). division sub-teams modeled agentsplay team plan (see Figure 8). Mid-eldersmust select midfield plan, goalies must select defend-goal plan, etc. Again, ideallyattacker would never select plan attack, defender would selectplan defend, etc. However, due communication failures, players may sometimesselecting one four team plans serviceaccidently abandon intended sub-team, execute team-plan another sub-team.[WinGame][Play][Attack][SimpleAdvance].....Scoregoal[FlankAttack]...Pass[Interrupt]...[Defend]......[Midfield]...........[DefendGoal][Carefuldefense]Interceptkickout[Simplegoaldefense].....RepositionFigure 8: Portion plan-hierarchy used ISIS RoboCup agents.examples, certain dierences agents agreed uponsign correct execution, failure. Indeed, lack dierence selected plans133fiKaminka Tambewould indicate failure cases. use termmutual-exclusion coordinationrefer relationships. Example 3, ideally two pilots executingplan time.shootingExample 4, two members dierent sub-teams (e.g.,attacker defender) executing plan serviceplay (e.g., defend).examples demonstrate, clear need monitoring mutual-exclusion coordination.results previous sections re-used service socially-attentive monitoringmutual-exclusion relationships. require transformation implementationtheory. hierarchies compared usual manner, except failures signiedequalities, rather dierences. instance, attacker staying team'shalf eld, teammates may come suspect mistakenly defectedattackers' sub-team believes defender.analytical results inverted well. maximal team-coherence heuristiclead completeness, since prefers hypotheses contain equalities among agents,failures mutual-exclusion coordination. maximal team-incoherence heuristic lead sound detection, prefers hypotheses imply equalitiesoccurred. properties proven formally.Theorem 5. Let monitoring agentmonitor mutual-exclusion relationships groupagents G. A's modeling G complete, uses maximally team-incoherenthypothesis detection, failure detection results sound.Proof.Provided appendix A.Theorem 6. Let monitoring agentmonitor mutual-exclusion relationships groupagents G. A's modeling G complete, uses maximally team-coherenthypothesis detection, failure detection results complete.Proof.Provided appendix A.Thus mutual-exclusion relationships, teamwork relationships, guaranteed failuredetection results may still provided despite use limited, uncertain knowledgemonitored agents. centralized teamwork monitoring algorithms easily transformed monitoring mutual-exclusion relationships. Unfortunately, resultsdistributed case (Theorem 4) cannot easily transformed, since relyproperty observable-partitioning, associated dierences, equalities.leave issue future work.7.2 Monitoring Using Role-Similarity Relationshipssection applies socially-attentive monitoring role-similarity relationships, monitoring individual performance within teams. particular, service team-plans agentsmay select individual sub-plans, necessitate agreement team-members,constrained agents' roles.fly-flight-planinstance, service executing team-plan(Figure 3) pilots individually select individual plans setvelocity heading within constraints formation ight method speciedmission.Role similarity relationships specify ways given individual plans similar,extent.Two agents role executing dissimilar plans134fiRobust Agent Teams via Socially-Attentive Monitoringconsidered violation role-similarity relationships. enables sociallyattentive monitoring system detect failure role-execution. monitor individual plansagent executing, compares selection agentsrole,similarly method used teamwork. plans considered similarrole-similarity relationship model, failure detected.Otherwise, failure mayoccurred, diagnosis component called verify provide explanation.Let us illustrate failure ModSAF domain system abledetect using role-similarity relationship:Example 5.team three helicopters take base headmission. However, one pilot agents failed correctly process mission statement.therefore kept helicopter hovering base, teammates left executemission themselves.failures detected using role-similarity relationship monitoring. agreed-uponteam-plan selected agents, problem teamwork relationshipdetected. team-plan involved agent selecting individual methods ight,determine altitude velocity.agents diered.failing helicopterremained hovering, teammates moved forward. Using role similarity relationship,failing helicopter compared selected plan teammate (who sharedrole subordinate formation), realized plans dissimilar enoughannounce possible failure.Unfortunately, actual similarity metrics seem domain- task-specic,thus easy re-use across domains. Furthermore, detected failures necessarily real failures, detected failures weight.currentlyinvestigating ways address challenging issues.8. Related Workinvestigation socially-attentive monitoring, relationship knowledgemaintained agents' states monitoring eectiveness builds research dierent subelds multi-agent systems. address sub-elds section, explaininvestigation related existing literature.8.1 Related Work TeamworkPrevious work teamwork recognized monitoring agents critical teams.Past investigations raised monitoring selectivity problem, addresseddepth. Building upon investigations, paper begins provide in-depthanswers problem.theory SharedPlans (Grosz & Kraus, 1996, 1999) touches teamwork monitoring selectivity problem several ways, provides initial answers. First,theory requires agents know teammates capable carrying tasksteam.authors note agents must communicate enough plansconvince teammates ability carry actions (Grosz & Kraus, 1996,p. 314). Second, theory requires agents mutual-belief shared recipe,135fiKaminka Tambestate requires agents reason innite recursion agent's beliefs.Un-fortunately, attainment mutual belief undecidable theory (Halpern & Moses, 1990)hence must approximated practice (Jennings, 1995; Rich & Sidner, 1997).approximations may still impose strong monitoring requirements. Third, theory introducesintention-that construct service coordination helpful behavior, implying monitoring others' progress assess need behavior (Grosz & Kraus, 1996,Axiom A5-A7).Fourth, SharedPlans requires intentions agent must con-ict (Grosz & Kraus, 1996, Axiom A1), since intentions (in particular,intentions-that) may involve attitudes agents, monitoring othersdetect avoid conicts implied. authors point theoreticallyconicts detected, infeasible practice (Grosz & Kraus, 1996, p. 307).suggest conict detection prevention investigated problem-specic mannerwithin minimal constraints (i.e., monitoring capabilities, mutual-belief, progress, lackconicts) provided SharedPlans framework (p. 308 314).Joint-Intentions (Levesque et al., 1990; Cohen & Levesque, 1991) requires agentprivately comes believe joint-goal either achieved, unachievable, irrelevant,must commit entire team mutually believe case. theorySharedPlans, Joint-Intentions' use mutual belief approximated practice,imposes strong monitoring requirements. Thus, monitoring selectivity problemraised practical implementations Joint-Intentions.Jennings hypothesized two central constructs cooperative multi-agent coordinationcommitmentsmade agents,conventions, rules used monitorcommitments (Jennings, 1993). conventions used decide information needsmonitored agents, monitored. instance, convention mayrequire agent report teammates changes privately detects respectattainability team goal.Jennings raises monitoring selectivity problemprovides example specic conventions high- low-bandwidth situationsknowledge communicated agents bandwidth available.However, Jennings explore in-depth question conventions selected, trade-os guarantees associated selection particularconventions. instance, guarantees eects using low-bandwidthconvention example.theoretical investigations described raise monitoring selectivity problem (implicitly explicitly). work builds upon address problem depth,context socially-attentive monitoring teams. paper reports soundnessand/or completeness properties teamwork relationship failure-detection analytically guaranteed, despite uncertainty knowledge acquired monitored agents.analytical guarantees applicable plan-recognition communications,corroborated empirical results.Building theoretical work, practical teamwork systems include (Jennings, 1995; Rich& Sidner, 1997) (Tambe, 1997).Jennings' investigation Joint-Responsibilityteamwork model GRATE* (Jennings, 1995) builds Joint-Intentions, similarlyimplementation, requires agents agree team-plans execute.However, GRATE* used industrial settings foolproof communicationsassumed (Jennings, 1995, p.211), thus passive monitoring (via communica-136fiRobust Agent Teams via Socially-Attentive Monitoringtions) used. Although Jennings provides evaluation GRATE*'s performancerespect communication delays, guarantees provided respect failure detection. GRATE* maintains knowledge agents acquaintances models,used keep track team-members' capabilities (in service formingteams). However, question much knowledge used modelsleft unaddressed.Rich Sidner investigate COLLAGEN collaborative user-interface system,communications reliable (Rich & Sidner, 1997). However, human-usabilityperspective, limiting amount communications still desirable.address is-sue, recent empirical work Lesh, Sidner Rich (1999) utilizes plan recognitionCOLLAGEN; focus work using collaborative settings makeplan-recognition tractable.instance, ambiguities plan-recognition may resolvedasking user clarication. Work COLLAGEN investigate muchknowledge maintained eective collaborative dialogue user. contrast,able provide guarantees failure-detection results algorithms. Also,analysizing dialogue plansrisky pointsmay allow systems COLLAGENdecide whether use communications clarication regardless plan-recognition ambiguity.STEAM (Tambe, 1997) maintains limited information ability team-memberscarry roles. STEAM also allows team-members reason explicitlycost communication deciding whether communicate not. work signicantlyextends capabilities via plan-recognition, provides analytically-guaranteed faultdetection results. Furthermore, teamwork failure-detection capabilities usefultrigger STEAM's re-planning capabilities.8.2 Related Work CoordinationHuber (1995) investigated use probabilistic plan-recognition service active teamwork monitoring, motivated unreliability costs passive communications-basedmonitoring military applications. Washington explores observation-based coordination using Markov Models (Washington, 1998), focusing making computations tractable.contrast Huber Washington, work focuses monitoring selectivity problem.showed strengths limitations centralized distributed approaches guaranteed failure-detection results using coherence-based disambiguation plan-recognitionhypotheses.Durfee (1995) discusses various methods reducing amount knowledge agentsneed consider coordinating others. methods discussed involve pruning partsnested models, using communications, using hierarchies abstractions, etc.focus work methods modeling limited, focuswork question much modeling required guaranteed performancethemonitoring selectivity problem. provide analytical guarantees trade-os involvedusing limited knowledge agents failure-detection purposes.Sugawara Lesser (1998) report use comparative reasoning/analysis techniques service learning specializing coordination rules system distributed agents coordinate diagnosing faulty network. investigation focused137fiKaminka Tambeoptimizing coordination rules minimize ineciency redundancy agent's coordinating messages. Upon detecting sub-optimal coordination (via fault model), agentsexchange information local views system problem solving activity,construct global view. compare local view global view ndcritical values/attributes missing local view therefore gave risesub-optimal performance problem. values attributes used constructingsituation-specic rules optimize coordination particular situations.example,network diagnosis agents may learn rule guides choose coordination strategy one agent performs diagnosis shares result restdiagnosis agents. work socially-attentive monitoring similarly uses comparisonagents views drive monitoring process. However, use comparisonproduct relationship monitoring.Sugawara Lesser's workviewed letting agents incrementally optimize monitoring requirements,results analytically explore level monitoring required eective failure-detection,dierent congurations. teamwork monitoring technique addresses uncertaintyacquired information, construct global view attributes systemaswould extremely expensive. Instead, technique focuses triggering failure detection via contrasting plans, incrementally expanding search dierencesdiagnosis process.Robotics literature also raised monitoring selectivity problem.Parker (1993)investigated monitoring selectivity problem dierent perspective, formationmaintenance task. empirically examined eects combining socially-attentive information (which referred local) knowledge team's goals, concludesfault-tolerant strategy one agents monitor wellprogress towards goals.Kuniyoshi et al.(Kuniyoshi, Rougeaux, Ishii, Kita, Sakane,& Kakikura, 1994) present framework cooperation observations, robotsvisually attend others prerequisite coordination. framework presents severalstandard attentional templates, i.e., monitors whom. dene team attentionalstructure one agents monitor other.work focuses mon-itoring selectivity problem within socially-attentive monitoring teamwork relationships,provides analytical well empirical results. treat attentional templatesproduct relationships hold system. results show monitoringteams may necessarily require monitoring team-members.8.3 Related WorkHorling et al.(Horling, Lesser, Vincent, Bazzan, & Xuan, 1999) present distributeddiagnosis system multi-agent intelligent home environment.The system uses faultmodels identify failures ineciencies components, guide recovery. SchroederWagner (1997) proposed distributed diagnosis technique cooperating agentsreceive requests tests diagnoses, send responses agents.construct global diagnosis based local ones produce receivewithassumption conicts occur.Frohlich Nejdl (1996) investigates schememultiple diagnosis agents cooperate via blackboard architecture diagnosingphysical system. agents may use dierent diagnosis models systems, centralized138fiRobust Agent Teams via Socially-Attentive Monitoringconict-resolution agent employed handle conicts diagnoses found. threeapproaches address monitoring selectivity problem.social measures related ATA. Goldberg Mataric (1997) in-interference amount time robotssocial entropy (Bailey, 1990) measure be-vestigate multi-robot foraging task measurespend avoiding other. Balch (1998) useshavioral diversitymulti-agent tasks soccer, foraging, formation-maintenance.investigations focus characterizing heterogeneity multi-agent systems relationperformance. contrast, focus work providing useful feedbackdesigner.Possible correlation task performance ATA values remainsinvestigated.9. Conclusions Future Workwork presented paper motivated practical concerns. beguninvestigation monitoring selectivity problem result observation failurescontinue occur despite agents' use monitoring conditions communications.Analysis failures revealed agents suciently informed other'sstate. need monitor one's teammates recognized repeatedly past(Jennings, 1993; Grosz & Kraus, 1996; Tambe, 1997), monitoring selectivity problemquestion much monitoring requiredremained largely unaddressed (Jennings,1993; Grosz & Kraus, 1996).provide key answers monitoring selectivity problem.Within contextsocially-attentive monitoring teams, demonstrate teamwork relationship failuresdetected eectively even uncertain, limited, knowledge team-members' states.show analytically centralized active teamwork monitoring provides failure-detectioneither complete unsound, sound incomplete. However, centralized teamwork monitoring requires multiple hypotheses monitoring team-members.contrast, distributed active teamwork monitoring results complete sound failuredetection, despite using simpler algorithm monitoring key agents team.Using implemented general framework socially-attentive monitoring, empirically validate results ModSAF domain. also provide initial results monitoring mutual-exclusion role-similarity relationships, initial diagnosis procedures.demonstrate generality framework applying RoboCupdomain, show useful quantitative analysis generated o-line.ModSAF RoboCup dynamic, complex, multi-agent domains involve many uncertainties perception action.attempted demonstrate results techniques applieddomains.explicitly pointed necessary conditions theorems hold,observable-partitioning team-modeling completeness. presented diagnosisalgorithm sensitive accuracy knowledge used, may require assumingplans recognized soon selected. conditions verieddesigner target application domain. Reactive plans (our chosen representation)commonly used many dynamic multi-agent domains. focus monitoring agreementsjoint plans stems centrality similar notions agreement agent humanteamwork literature (Jennings, 1995; Grosz & Kraus, 1996; Volpe et al., 1996; Tambe, 1997).139fiKaminka Tambemade several references additional areas would like conductinvestigations.One important topic plan investigate depth strongrequirements distributed teamwork monitoring algorithm terms observability.order provide soundness completeness guarantees, distributed algorithm reliesability team-members monitor key agents. investigating waysrelax requirement still providing guaranteed results. addition, diagnosisprocedures extended formalized, would like investigate waysalleviate sensitivity procedures choice team-modeling hypothesis.Acknowledgmentsarticle partially based AAAI-98 paper (Kaminka & Tambe, 1998),Agents-99 paper (Kaminka & Tambe, 1999) authors. research supported part NSF Grant ISI-9711665, part AFOSR contract #F49620-97-10501. thank Je Rickel, George Bekey, Victor Lesser, Dan O'Leary, David Pynadathmany useful comments. anonymous reviewers thanks helping us crystallize ideas contributions revisions paper.Appendix A. ProofsTheorem. (# 2, page 123). Let monitoring agent monitor simple team . A'steam-modeling complete, uses maximally team-coherent hypothesis detection, teamwork failure detection results sound.Proof.show failure occurs detected, thus failuresdetected. Let a1 ; : : : ; agent members . agent ai executingplan Pi (1 n).Thus collectively, group executing (P1 ; : : : ; Pn ).failureoccurred, two agents ak ; aj ; 1 j; k n aj executing planPj ak executing plan Pk Pj 6= Pk .Since A's team-modeling complete,correct hypothesis (P1 ; : : : ; Pj ; : : : ; Pk ; : : : Pn ) set team-modeling hypotheses.Since choose maximally team-incoherent hypothesis, either choose correcthypothesis, incoherent hypothesis implying failure occurred,select hypothesis greater incoherence hypothesis (or equivalent level).case, failure would detected, detection procedure complete.Lemma. (#1, page 127). Suppose simple team self-monitoring (all membersteam monitor other) using maximally team-coherent heuristic (andassumption agent, team-modeling complete). monitoring agent A1member executing P1 would detect failure maintaining teamwork relationshipsagent A2 (also member ) executing dierent plan P2 , A2 observablydierent role P1 P2 .Proof.A1 knows executing P1 .Since members monitorthemselves, A1 monitoring A2 , observably dierent role P1 P2 . Since A2executing P2 , following observably dierent role, P1 2= (A1 ; A2 =P2 ). Thereforeperspective A1 , cannot case assigns P1hypothesis, thereforeteam-modelingagent-modelinghypothesis A1 A1 executing140fiRobust Agent Teams via Socially-Attentive MonitoringP1 , A2 executing plan P1 . words, A1 's perspectiveteam-coherent hypothesis, dierence would detected A1 A2 .Theorem. (# 5, page 134). Let monitoring agent monitor mutual-exclusion relation-ships group agents G. A's modeling G complete, uses maximallyteam-incoherent hypothesis detection, failure detection results sound.Proof.show failure occurred, none detected, thusfailure detected fact failure.G.Let a1 ; : : : ; agent membersagent ai executing plan Pi (1 n). Thus collectively, groupexecuting (P1 ; : : : ; Pn ). failure occurred, agent executing dierentplan (i 6= j ) Pi 6= Pj ). Since A's group-modeling complete, correct hypothesisgoing set group-modeling hypotheses H . Since maximally incoherenthypothesis, either selected, dierent hypothesislevelselected.coherencehypothesis coherence level correct oneimplies failure detected. Thus detection procedure sound.Theorem. (# 6, page 134). Let monitoring agent monitor mutual-exclusion relation-ships group agents G. A's modeling G complete, uses maximallyteam-coherent hypothesis detection, failure detection results complete.Proof.show failure occurs detected, thus procedurecomplete. Let a1 ; : : : ; agent members G. agent ai executingplan Pi (1 n).Thus collectively, group executing (P1 ; : : : ; Pn ).failureoccurred, two agents ak ; aj ; 1 j; k n aj executing planPj ak executing plan Pk Pj=Pk .Since A's group-modeling complete,correct hypothesis (P1 ; : : : ; Pj ; : : : ; Pk ; : : : Pn ) set group-modeling hypotheses.Since choose maximally team-coherent hypothesis, either choose correcthypothesis, coherent hypothesis implying failure occurred,select hypothesis greater coherence hypothesis (or equivalent level).case, failure would detected. Therefore, detection procedure complete.Appendix B. Socially-Attentive Monitoring Algorithmsbring algorithms (in pseudo-code) RESL plan-recognition algorithm,comparison test supporting detection simple non-simple teams,monitoring algorithms centralized distributed cases.B.1 RESLRESL works rst expanding complete operator hierarchy agents modeled, tagging plans non-matching. plans' preconditions termination conditionsagged non-matching well. plans' actions set used expectationsbehavior. initializing plan-recognition hierarchy monitored agent, observations agent continuously matched actions expected plans.Plans whose expectations match observations tagged matching, agspropagated along hierarchy, down, complete paths hierarchy141fiKaminka Tambeagged matching not. paths specify possible matching interpretationsobservations. addition, precondition termination conditions agged truenot, signifying inferred appropriate belief modeled agents.processdescribed Algorithm 1.Algorithm 1 RESL's main loop, matchingobservation making inferences givenplan-recognition hierarchy (a single agent).1.Get observations agent2.plan set expected observations:(a) Compare observations expectations(b) succeed, ag plan matching successfully, otherwise ag plan failing match3.plan agged matching successfully(a) Flag parents matching successfully // propagate matching4.plan whose children (all them) agged failing match(a) Flag failing match // propagate non-matchingB.2 Detection Failure, Centralized Distributed Teamwork MonitoringAlgorithm 2 shows comparison hierarchical plans carried out. limitsimple-teams. algorithm accepts input two sets hierarchical plan hypotheses, two associated agents (for clarity, algorithms assume two agents.generalization n agents straightforward). algorithm also accepts policy ag,Policy.OPTIMISTICpolicy causes algorithm use maximal team-coherenceprovide sound, incomplete detection.PESSIMISTIC policy causes algorithm usemaximal team-incoherence provide complete, unsound detection.hierarchy_1 hierarchy_2. two agentsagent_2. algorithm makes use predicate Sub-team,true two agents (Agent1, Agent2) belong dierent sub-teams givenlevel hierarchy (Depth).set hierarchical plans markedmarkedagent_1aid Algorithm 2, dene centralized distributed failure detection algorithms.centralized teamwork monitoring algorithm (Algorithm 3)utilizes Algorithm 2 twice, checking failuresPESSIMISTICOPTIMISTICpolicies. results policies agree, certain. results agree,(i.e.,PESSIMISTICpolicy causes failure detected,OPTIMISTICpolicycauses failure detected), monitoring agent cannot certain failuretaken place, therefore needs verify failure.Algorithm 3 therefore returnsFAILURE, NO_FAILURE, POSSIBLE_FAILURE.distributed monitoring algorithm given pseudo-code form,nothing call Algorithm 2142OPTIMISTICpolicy parameter. powerfiRobust Agent Teams via Socially-Attentive MonitoringAlgorithm 2 Hierarchical comparison two agents, allowing sub-teams.1.Set Depth 0 //2.plans depth Depth team-plans(a)look top-most dierence rstPolicy == OPTIMISTICi. Let Plan_1, Plan_2 maximally team-coherent plans level Depthhierarchy_1 hierarchy_2, respectively.ii. else Let Plan_1, Plan_2 maximally team-incoherent plans level Depthhierarchy_1 hierarchy_2, respectively.(b)Plan_1 equal Plan_2i. return FAILUREii. else bottom hierarchies reached, return NO_FAILURE, otherwise increase Depth go 2.3.one plan team-plan, return FAILURE, else return NO_FAILURE.Algorithm 3 Centralized Teamwork Monitoring, applying optimistic pessimisticviews.1.2.Let Optimistic_Result = Detect(agent_1, agent_2, hierarchies_1,hierarchies_2, OPTIMISTIC)/* algorithm 2 */Let Pessimistic_Result = Detect(agent_1, agent_2, hierarchies_1,hierarchies_2, PESSIMISTIC)/* algorithm 2 */3.Optimistic_Result == Pessimistic_Result4.return Optimistic_Result /*5.else return POSSIBLE_FAILUREeither143FAILURE,NO_FAILURE */fiKaminka Tambederived fact members team using monitor key agentsteam.ReferencesAmbros-Ingerson, J. A., & Steel, S. (1988). Intergrating planning, execution monitoring.Proceedings Seventh National Conference Articial Intelligence (AAAI-88)Minneapolis/St. Paul, MN. AAAI Press.Andou, T. (1998). Renement soccer agents' positions using reinforcement learning.Kitano, H. (Ed.),RoboCup-97: Robot soccer world cup 1, Vol. LNAI 1395, pp. 373388.Springer-verlag.Atkins, E. M., Durfee, E. H., & Shin, K. G. (1997). Detecting reacting unplanned-Proceedings Fourteenth National Conference ArticialIntelligence (AAAI-97), pp. 571576 Providence, RI. AAAI Press.world states.Bailey, K. D. (1990).Balch, T. (1998).Social Entropy Theory.State University New York Press.Behavioral Diversity Learning Robot Teams.Ph.D. thesis, GeorgiaInstitute Technology.Calder, R. B., Smith, J. E., Courtemanche, A. J., Mar, J. M. F., & Ceranowicz, A. Z. (1993).Modsaf behavior simulation control. Proceedings Third ConferenceComputer Generated Forces Behavioral Reresentation Orlando, Florida. InstituteSimulation Training, University Central Florida.Cohen, P. R., Amant, R. S., & Hart, D. M. (1992).Early warnings plan failure, falsepositives, envelopes: Experiments model.Tech. rep. CMPSCI TechnicalReport 92-20, University Massachusetts.Cohen, P. R., & Levesque, H. J. (1991). Teamwork.Nous, 35.Doyle, R. J., Atkinson, D. J., & Doshi, R. S. (1986). Generating perception requestsexpectations verify execution plans.Conference Articial Intelligence (AAAI-86).Durfee, E. H. (1995).Blissful ignorance:Proceedings Fifth NationalKnowing enough coordinate well.Proceedings First International Conference Multiagent Systems (ICMAS-95),pp. 406413.Fenster, M., Kraus, S., & Rosenschein, J. S. (1995).Coordination without communica-Proceedings FirstInternational Conference Multiagent Systems (ICMAS-95), pp. 102108 California,tion: Experimental validation focal point techniques.USA.Firby, R. J. (1987). investigation reactive planning complex domains.ceedings Sixth National Conference Articial Intelligence (AAAI-87).144Pro-fiRobust Agent Teams via Socially-Attentive MonitoringFrohlich, P., & Nejdl, W. (1996). Resolving conicts distributed diagnosis. Wahlster,W. (Ed.),12th Europeach Conference Articial Intelligence (ECAI-96).JohnWiley & Sons, Inc.Goldberg, D., & Mataric, M. J. (1997).Interference tool designing evaluat-Proceedings Fourteenth National ConferenceArticial Intelligence (AAAI-97), pp. 637642 Providence, RI. AAAI Press.ing multi-robot controllers.Grosz, B. J., & Kraus, S. (1999). evolution sharedplans. Wooldridge, M., & Rao,A. (Eds.),Foundations Theories Rational Agency, pp. 227262.Grosz, B. J., & Kraus, S. (1996). Collaborative plans complex group actions.Intelligence, 86, 269358.ArticialGrosz, B. J., & Sidner, C. L. (1990). Plans discourse. Cohen, P. R., Morgan, J., &Pollack, M. (Eds.),Intentions Communication, pp. 417445. MIT Press, Cambridge,MA.Halpern, J. Y., & Moses, Y. (1990). Knowledge common knowledge distributedenvironment.distributed computing, 37 (3), 549587.Hamscher, W., Console, L., & de Kleer, J. (Eds.). (1992).nosis.Readings Model-Based Diag-Morgan Kaufmann Publishers, San Mateo, CA.Horling, B., Lesser, V. R., Vincent, R., Bazzan, A., & Xuan, P. (1999).Diagnosisintegral part multi-agent adaptability. Tech. rep. CMPSCI Technical Report 199903, University Massachusetts/Amherst.Huber, M. J., & Durfee, E. H. (1995).acting together: Without communication.Working Notes AAAI Spring Symposium Representing Mental StatesMechanisms, pp. 6071 Stanford, CA.Jennings, N. R. (1993). Commitments conventions: foundations coordinationmulti-agent systems.Knowledge Engineering Review, 8 (3), 223250.Jennings, N. R. (1995). Controlling cooperative problem solving industrial multi-agentsystems using joint intentions.Articial Intelligence, 75 (2), 195240.Johnson, W. L., & Rickel, J. (1997). STEVE: animated pedagogical agent proceduraltraining virtual environments.SIGART Bulletin, 8 (1-4), 1621.Kaminka, G. A., & Tambe, M. (1998). What's wrong us? Improving robustnessProceedings Fifteenth National Conference ArticialIntelligence (AAAI-98), pp. 97104 Madison, WI. AAAI Press.social diagnosis.Kaminka, G. A., & Tambe, M. (1999).I'm OK, You're OK, We're OK: Experimentsdistributed centralized social monitoring diagnosis.ProceedingsThird International Conference Autonomous Agents (Agents-99) Seattle, WA. ACMPress.145fiKaminka TambeKinny, D., Ljungberg, M., Rao, A., Sonenberg, E., Tidhar, G., & Werner, E. (1992). Plannedteam activity.Castelfranchi, C., & Werner, E. (Eds.),Articial Social Systems,Lecture notes AI 830, pp. 227256. Springer Verlag, New York.Kitano, H., Tambe, M., Stone, P., Veloso, M., Coradeschi, S., Osawa, E., Matsubara, H.,Proceedings International Joint Conference Articial Intelligence (IJCAI-97)Noda, I., & Asada, M. (1997). RoboCup synthetic agent challenge '97.Nagoya, Japan.Kraus, S., Sycara, K., & Evenchik, A. (1998). Reacing agreements negotiations:logical model implementation.articial intelligence, 104 (1-2), 169.Kuniyoshi, Y., Rougeaux, S., Ishii, M., Kita, N., Sakane, S., & Kakikura, M. (1994). Cooperation observation framework basic task patterns.International Conference Robotics Automation,IEEEpp. 767773 San-Diego, CA.IEEE Computer Society Press.Lesh, N., Rich, C., & Sidner, C. L. (1999). Using plan recognition human-computer col-Proceedings Seventh International Conference User Modelling(UM-99) Ban, Canada.laboration.Levesque, H. J., Cohen, P. R., & Nunes, J. H. T. (1990). acting together.Eigth National Conference Articial Intelligence (AAAI-90)ProceedingsMenlo-Park,CA. AAAI Press.Malone, T. W., & Crowston, K. (1991).tion.Toward interdisciplinary theory coordina-Tech. rep. CCS TR#120 SS WP# 3294-91-MSA, Massachusetts InstituteTechnology.Marsella, S. C., Adibi, J., Al-Onaizan, Y., Kaminka, G. A., Muslea, I., Tallis, M., & Tambe,M. (1999).teammate:Experiences acquired design robocupProceedings Third International Conference Autonomous Agents(Agents-99) Seattle, WA. ACM Press.teams..Newell, A. (1990).Unied Theories Cognition.Harvard University Press, Cambridge,Massachusetts.ProceedingsIEEE Robotics Automation Conference, pp. 582587 Atlanta, GA.Parker, L. E. (1993). Designing control laws cooperative agent teams.Rao, A. S. (1994). Means-end plan recognition towards theory reactive recognition.Proceedings International Conference Knowledge Representation Reasoning (KR-94), pp. 497508.Reece, G. A., & Tate, A. (1994). Synthesizing protection monitors causal structure.Proceedings Articial Intelligence Planning Systems (AIPS-94) Chicago, IL.Rich, C., & Sidner, C. L. (1997).COLLAGEN: agents collaborate people.Johnson, W. L. (Ed.), Proceedings First International Conference Autonomous Agents (Agents-97), pp. 284291 Marina del Rey, CA. ACM Press.146fiRobust Agent Teams via Socially-Attentive MonitoringProceedingsFirst International Conference Autonomous Agents (Agents-97), pp. 268275Schroeder, M., & Wagner, G. (1997). Distributed diagnosis vivid agents.Marina del Rey, CA. ACM Press.Sugawara, T., & Lesser, V. R. (1998). Learning improve coordinated actions cooperativedistributed problem-solving environments.Machine Learning, 33 (2/3), 129153.Tambe, M. (1996). Tracking dynamic team activity.ence Articial Intelligence (AAAI).Tambe, M. (1997). Towards exible teamwork.7, 83124.Proceedings National Confer-Journal Articial Intelligence Research,Tambe, M., Johnson, W. L., Jones, R., Koss, F., Laird, J. E., Rosenbloom, P. S., & Schwamb,K. (1995).16 (1).Intelligent agents interactive simulation environments.AI Magazine,ProceedingsFourteenth National Conference Articial Intelligence (AAAI-97), pp. 39 Provi-Toyama, K., & Hager, G. D. (1997). rst don't succeed....dence, RI.Veloso, M., Pollack, M. E., & Cox, M. T. (1998). Rationale-based monitoring planningdynamic environments.(AIPS-98) Pittsburgh, PA.Proceedings Articial Intelligence Planning SystemsVolpe, C. E., Cannon-Bowers, J. A., & Salas, E. (1996). impact cross-trainingteam functioning: empirical investigation.human factors, 38 (1), 87100.Markov tracking agent coordination. ProceedingsSecond International Conference Autonomous Agents (Agents-98), pp. 7077 Min-Washington, R. (1998).neapolis/St. Paul, MN. ACM Press.147fiJournal Artificial Intelligence Research 12 (2000) 317337Submitted 6/99; published 5/00Axiomatizing Causal ReasoningJoseph Y. Halpernhalpern@cs.cornell.eduCornell University, Computer Science DepartmentIthaca, NY 14853http://www.cs.cornell.edu/home/halpernAbstractCausal models defined terms collection equations, defined Pearl,axiomatized here. Axiomatizations provided three successively general classescausal models: (1) class recursive theories (those without feedback), (2) classtheories solutions equations unique, (3) arbitrary theories (whereequations may solutions and, do, necessarily unique).shown reason causality general third class, must extendlanguage used Galles Pearl (1997, 1998). addition, complexitydecision procedures characterized languages classes models considered.1. Introductionimportant role causal reasoningin prediction, explanation, counterfactualreasoninghas argued eloquently number recent papers books (Chajewska& Halpern, 1997; Heckerman & Shachter, 1995; Henrion & Druzdzel, 1990; Druzdzel &Simon, 1993; Pearl, 1995; Pearl & Verma, 1991; Spirtes, Glymour, & Scheines, 1993).reason causality, certainly useful find axioms characterizereasoning. way go axiomatizing causal reasoning depends two criticalfactors:model causality,language use reason it.paper, consider one approach modeling causality, using structural equations.use structural equations model causality standard social sciences,seems go back work Sewall Wright 1920s (see (Goldberger, 1972)discussion); particular framework use due Pearl (1995). Galles Pearl(1997) introduce axioms causal reasoning framework; also providecomplete axiomatic characterization reasoning causality framework,strong assumption fixed, given causal ordering equations (Galles& Pearl, 1998). Roughly speaking, means way ordering variablesappear equations explicit axioms say Xj influence XiXi Xj causal ordering.paper, extend results Galles Pearl providing complete axiomaticcharacterization three increasingly general classes causal models (defined structuralequations):c2000AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiHalpern1. class recursive theories (those without feedbackthis generalizes situationconsidered Galles Pearl (1998), since every fixed causal ordering variablesgives rise recursive theory),2. class theories solutions equations unique,3. arbitrary theories (where equations may solutions and, do,necessarily unique).process, clarify problems Galles-Pearl completeness proof ariselack propositional connectives (particularly disjunction) languageconsider and, generally, highlight role language reasoning causality.also characterize complexity decision problem languages classesmodels.rest paper organized follows. Section 2, give syntax semanticslanguages considering review definition modifiable causal models.Section 3, present complete axiomatizations. Section 4 consider complexitydecision procedure. conclude Section 5.2. Syntax Semanticsaxiomatization given respect particular language class models,need make precise. language models use basedconsidered Galles Pearl (1997, 1998). make comparisons easier, usenotation much possible. start semantic model, since motivateschoices syntax, give syntax, finally define semantics formulas.2.1 Causal Modelsbasic picture interested values random variables,causal effect others. effect modeled set structural equations.practice, seems useful split random variables two sets, exogenousvariables, whose values determined factors outside model, endogenousvariables. endogenous variables whose values described structuralequations.formally, signature tuple (U, V, R}, U finite set exogenousvariables, V finite set endogenous variables, R associates every variableU V nonempty set R(Y ) possible values (the range possible values ).Unless explicitly noted otherwise, assume R(Y ) finite set U V|R(Y )| 2. assumption U V finite relatively innocuous; shall see,assumption R(Y ) finite impact axioms. assumption|R(Y )| 2 allows us ignore trivial situation |R(Y )| = 1. |R(Y )| = 1,remove variable signature without loss expressiveness.causal model signature tuple = (S, F ) F associatesvariable X V function denoted FX FX : (U U R(U )) (Y V{X}R(Y ))R(X). FX tells us value X given values variables U V.think functions FX defining set (modifiable) structural equations, relating318fiAxiomatizing Causal Structuresvalues variables. FX function, unique value Xset variables. Notice functions endogenous variables.exogenous variables taken given; effect endogenous variables(and effect endogenous variables other) modelingstructural equations.~Given causal model = (S, F ) signature S, (possibly empty) vector X~ U, respectively,variables V, vectors ~x ~u values variables X~ R| ~ ).1u) signature SX~ = (, VX,define new causal model denoted TX~~ x (~VX~ set ~xIntuitively, causal model results variables X~~u) = (SX~ , F X~x,~u }), FYX~x,~u obtainedvariables U set ~u. Formally, TX~~ x (~~ ~x values variables UFY setting values variables Xu) called submodel Pearl (1999). describe~u. causal model TX~~ x (~possible counterfactual situation; is, even though, normal circumstances, setting~ values ~exogenous variables ~u may result variables Xx0 6= ~x,submodel describes happens set ~x due external action,cause modeled explicitly. example, determine manufacturerfault accident involved poorly maintained car, may want considerwould happened car well maintained. random variablesignature describes well maintained car is, means examiningsubmodel random variable set 1 (the car well maintained).ability examine counterfactual situations makes causal structures useful toolreasoning causality.Notice that, general, may unique vector values simultaneouslysatisfies equations TX~u); indeed, may solution all. One special~ x (~case guaranteed unique solution total orderingvariables V X , FX independent value ;i.e., FX (. . . , y, . . .) = FX (. . . , 0 , . . .) y, 0 R(Y ). case, causal modelsaid recursive acyclic. Intuitively, theory recursive, feedback.X , value X may affect value , value effectvalue X.clear recursive theory, always unique solution~ ~equations TX~u), X,x, ~u. (We simply solve variables~ x (~order given .) hand, following example shows, hardconstruct nonrecursive theories always unique solution equationsarise.Example 2.1: Let = (, {X, }, R}), R(X) = R(Y ) = {1, 0, 1}, let =(S, F ), FX characterized equation X = FY characterizedequation = X (that is, FX (y) = FY (x) = x). Clearly recursive;value X depends value value depends X. Nevertheless,easy see unique solution X = 0, = 0, TXx unique solution= x, TY unique solution X = y.~ subset V consisting variables X.~1. implicitly identifying vector X~restrictionRvariablesVX.throughout paper. R|VX~319fiHalpernpaper, consider three successively general classes causal modelsgiven signature = (U, V, R).1. Trec(S): class recursive causal models signature S,~ V, ~x, ~u,2. Tuniq(S): class causal models Xu) unique solution,equations TX~~ x (~3. (S): class causal models S.often omit signature clear context irrelevant, readerbear mind important role.interested causal models possess unique solutions?real causal systems possess unique solutions? issue whethernonrecursive system given causal interpretation discussed lengthStrotz Wold (1960). argue reasonable ways interpreting causalinterpretations answer yes. interpretations, may wellone solution equations. Perhaps best way view equationsthink variables V mutually interdependent; changing one maycause change others. (Think demand supply economics populationsrabbits wolves.) solutions equations represent equilibrium situations.one equilibrium, one solution equations.course, equilibria, solutions equations.related way thinking equations represent atemporal versionstemporal causal equations. is, suppose replace every variable U Vfamily variables Y0 , Y1 , Y2, . . ., where, intuitively, Yt represents value time t.equation fX F replaced family equations fXt , fXt dependsexogenous variables Ut0 t0 endogenous variables Yt0 t0 < t. givesus recursive system. values Xt setting variables subscript 0represents evolution X setting variables. Xt eventually stabilizes,might expect equilibrium value value X solutionoriginal set equations. Xt stabilizes, would solution originalset equations.2.2 Syntaxfocus two languages. languages parameterized signature S.first language, L+ (S), borrows ideas dynamic logic (Harel, 1979). Again, often writeL+ rather L+ (S) (and similarly languages defined below) simplifynotation. basic causal formula one form [Y1 y1 , . . . , Yk yk ],Boolean combination formulas form X(~u) = x, Y1 , . . . , Yk , X variables V,Y1 , . . . , Yk distinct, x R(X), ~u vector values variables U.~ ~typically abbreviate formula [Y]. special case k = 0 (which~allowed) abbreviated [true]. [Y ~]X(~u) = x interpreted possiblesolutions structural equations obtained setting Yi yi , = 1, . . ., k,exogenous variables ~u, random variable X value x. shall see, formulatrue causal model solutions equations TY~ ~y (~u), random320fiAxiomatizing Causal Structuresvariable X value x. Note formula trivially true solutionsstructural equations. causal formula Boolean combination basic causalformulas.~ ~y i(X(~u) = x)dynamic logic, also define formula hY~ ~y](X(~u) = x). hY~ ~~ ~abbreviation [Yi(X(~u) = x) dual [Y](X(~u) =x); true if, solution structural equations obtained setting Yiyi , = 1, . . ., k, exogenous variables ~u, random variable X value x. Takingtrue(~u) abbreviation X(~u) = x X(~u) 6= x variable X x R(X),~ ~taking false(~u) abbreviation true(~u), hYitrue(~u) truesolution equations obtained setting Yi yi , = 1, . . . , k,~ ~variables U ~u (since [Y]false(~u) says every solution equationsobtained setting Yi yi U ~u, formula false(~u) true, thus holds exactlyequations solution).Let Luniq(S) sublanguage L+ (S) consists Boolean combinations~ ~y]X(~u) = x. Thus, difference Luniq L+formulas form [Y~ ~], L+ , arbitrary BooleanLuniq, X(~u) = x allowed [Ycombinations formulas form X(~u) = x allowed. shall see, reasoningcausality Tuniq, language Luniq adequate, since equivalent expressivepower L+ . However, longer case reasoning causality .~ ~y ]X(~u) = x X ~ (~u) = x.Following Galles Pearls notation, often write [Y~~ clear context irrelevant, abbreviate X~y (~u) = x. (Thisactually notation used Galles Pearl.) Let LGP (S) sublanguage Luniq(S)consisting conjunctions formulas form X~y (~u) = x. particular,contain disjunctions negations formulas. Although Galles Pearl (1998)explicit language using, seems LGP .22.3 Semanticsformula L+ (S) true false causal model (S). usual, write |=causal formula true causal model . basic causal formula,~ ~y](X(~u) = x) solutions ~ (~u) (i.e., vectors values|= [Y~~~ simultaneously satisfy equations F ~y , Z V Y~ ),variables VZvariable X value x. define truth value arbitrary causal formulas,Boolean combinations basic causal formulas, obvious way:|= 1 2 |= 1 |= 2|= 6|= .usual, say formula valid respect class 0 causal models|= 0 .make precise earlier claim Tuniq (and hence Trec), languageLuniq expressive full language L+ .Lemma 2.2: following formulas valid Tuniq:2. confirmed Judea Pearl [private communication, 1997].321fiHalpern~ ~y]( ) [Y~ ~~ ~(a) Tuniq |= [Y] [Y],~ ~y]( ) [Y~ ~~ ~(b) Tuniq |= [Y] [Y],~ ~y] [Y~ ~(c) Tuniq |= [Y].Hence, Tuniq, every formula L+ equivalent formula Luniq.Proof: Straightforward; left reader.~ ~Note follows equivalences Tuniq, [Y] equivalent~hY ~yi. also worth noting Lemma 2.2(b) holds arbitrary causal models, Tuniq. However, parts (a) (c) not, following example shows.Example 2.3: Let = (, {X, }, R), R(X) = R(Y ) = {0, 1}; let = (S, F ),FX characterized equation X = FY characterized equation= X. Clearly/ Tuniq; (0, 0) (1, 1) solutions . easy see|= [true](X = 0 X = 1) [true](X = 0) [true](X = 1), showing part (a)Lemma 2.2 hold , |= [true](X = 1) [true](X = 1), showingpart (c) hold either.3. Complete Axiomatizationsbriefly recall standard definitions logic. axiom system AX consistscollection axioms inference rules. axiom formula (in predetermined language L), inference rule form 1, . . . , k infer , 1, . . . , k ,formulas L. proof AX consists sequence formulas L,either axiom AX follows application inference rule. proof saidproof formula last formula proof . say provable AX,write AX ` , proof AX; similarly, say consistentAX provable AX.axiom system AX said sound language L respect class 0causal models every formula L provable AX valid respect 0 . AXcomplete L respect 0 every formula L valid respect 0provable AX.want find axioms characterize classes causal modelsinterested, namely Trec, Tuniq, . deal Trec, helpful defineZ,read affects Z, abbreviation formula;XV,~~xuU U R(U ),z6=zXV R(X),yR(y),~0 R(Z)(Z~xy (~u) = z 0 Z~x (~u) = z).Thus, affects Z setting exogenous variables endogenous variables changing value changes value Z. definitionused axiom C6 below, characterizes recursiveness.Consider following axioms:C0. instances propositional tautologies.C1. X~y (~u) = x X~y (~u) 6= x0 x, x0 R(X), x 6= x0322(equality)fiAxiomatizing Causal StructuresC2. xR(X)X~y (~u) = x(definiteness)C3. (W~x(~u) = w Y~x (~u) = y) Y~xw (~u) =(composition)C4. Xxw~ (~u) = x(effectiveness)C5. (Y~xw (~u) = W~xy (~u) = w) Y~x (~u) =(reversibility)C6. (X0;X1. . . Xk1; X ) (X ; X )kk(recursiveness)0one rule inference:MP. , infer(modus ponens)C1 states obvious property equality: X = x every solutionequations T~x (~u), cannot X = x0 , x0 6= x.3 richer language, couldexpressed (X~y (~u) = x X~y(~u) = x0 ) (x = x0 ), formula L+(since L+ include expressions x0 = x). C2 states valuex R(X) value X solutions equations T~x(~u). C2 valid, valid Tuniq. Note stating C2, making use factR(X) finite (otherwise C2 would involve infinite disjunction, would longerformula Luniq). fact, shown allow signatures sets R(X)infinite, include C2 random variables X R(X) finite.4C3C5 introduced Galles Pearl (1997, 1998), names. Roughlyspeaking, C3 says value W w solutions equations T~x(~u),solutions equations T~xw (~u) solutions equationsT~x (~u). C3 valid well Tuniq. shall see, variant C3 (obtainedreplacing some) also valid . C4 simply says solutions obtainedsetting X x, value X x. C5 perhaps least obvious axioms;~ ~x Wproof soundness straightforward. says setting X~w results value setting X ~x results W value~ x (and W must already valuew, must already value set Xw).Finally, easy see C6 holds recursive models.Z, mustX1 . . . Xk1Xk , X0 mustprecede Z causal ordering. Thus, X0precede Xk causal ordering, Xk cannot affect X0. Thus, (XkX0 ) holds.shall see, precise sense, C6 characterizes recursive models.C6 viewed collection axioms (actually, axiom schemes), one k.case k = 1 already gives us (YZ) (Z) variables Z.;;;;;;3. earlier draft paper, C1 C2 introduced, C1 called uniqueness. GallesPearl (1998) adopted name well. retrospect, axiom really say anythinguniqueness. axiom D10, discussed later.used C64. assumption R(X) V finite also necessary abbreviation XLuniq ; however, replace C6 axiom scheme;ui ) = zi (Xi+1 )~yi = zi0 ) (X0 )~yk xk (~uk ) = zk (X0 )~yk = zk0 ),(k1yi xi (~i=0 (Xi+1 )~xi R(Xi) = 1, . . . , k. is, essentially replace C6 instances. axiomequivalent C6 (although transparent) expressed even |V| infinite |R(X)|infinite X V.323fiHalpernis, tells us that, pair variables, one affects other. However,restricting C6 case k = 1 suffice characterize Trec, followingexample shows.Example 3.1: Let = (, {X0, X1, X2}, R), R(X0) = R(X1) = R(X2) = {0, 1, 2},let = (S, F ), FXi characterized equation(Xi =2 Xi1 = 10 otherwiseaddition mod 3. easy see Tuniq: variables set,equations completely determine values variables. hand,none variables set, easy see (0, 0, 0) solution satisfiesequations. Moreover, TX~~ x , variable Xi 0 unless set value0 Xi1 set 1. easily follows Xi affected Xi1 . straightforwardverification (or appeal Theorem 3.2 below) shows satisfies axiomsC6. C6 hold , since |= X0X1 X1X2 X2X0. alsoshows recursive. However, restricted version C6 (where k = 1)hold . generalization example (with k random variables rather 2)used show cannot bound k C6; need C6 hold finitevalues k.;;;Let AXuniq(S) consist C0C5 MP; let AXrec (S) consist C0C4, C6, MP.could include C5 AXrec(S); because, Galles Pearl (1998) pointout, follows C3 C6. Note signature parameter axiomsystem, language set models. because, example,set R(X) (which determined S) appears explicitly C1 C2.Theorem 3.2: AXuniq(S) (resp., AXrec(S)) sound complete axiomatizationLuniq(S) respect Tuniq(S) (resp., Trec(S)).Proof: See appendix.said introduction, Galles Pearl (1998) prove similar completeness resultcausal models whose variables satisfy fixed causal ordering. Given total orderingvariables V, consider following axiom:Ord. Y~xw (~u) = Y~x (~u) W;Since ~x, w, ~u implicitly universally quantified Ord, axiom says (W) holds W . follows W, W . facttotal order, easy see Ord implies C6.Galles Pearl show C1C4 Ord sound complete axiomatizationrespect class causal models satisfying Ord LGP . precisely, GallesPearl take AC consist axioms C1C4 Ord (but C0 MP), show,notation, |= implies `AC , {} set formulas LGP .important subtle point worth stressing result: C1 C2,;324fiAxiomatizing Causal Structuresaxioms AC , expressible LGP (since statement involves disjunctionnegation).exactly Galles Pearls result saying? interpret |= , usual,meaning causal models satisfying S, true.5 interpret `ACmeaning provable axioms axioms AC togetherrules logic, presumably means C0 MP. follows easily Theorem 3.2result correct (see below), unlike typical soundness completenessproofs, since proof general involve formulas LuniqLGP . (In particular, happen whenever C1C3 used proof.)see Galles Pearls result follows Theorem 3.2, defineformula Luniq(S) conjunction formulas (there finitelymany, since LGP (S) finitely many distinct formulas), together conjunction instances axiom Ord (again, finitely many). Note|= holds iff Tuniq(S) |= (since formulas Ord guaranteecausal models satisfy recursive, hence Tuniq(S)). Thus, Theorem 3.2, |= iff AXuniq(S) ` . latter statement equivalent `AC ,defined Galles Pearl. fact, Theorem 3.2 shows AXuniq(S) + Ord givessound complete axiomatization respect causal models satisfying Ordlanguage Luniq(S), allows Boolean connectives. (Of course, Theorem 3.2 shows more,since extends Galles Pearls result Trec(S) Tuniq(S).) suggests Luniqappropriate language reasoning causality LGP , least causalmodels Tuniq. LGP cannot express number properties causal reasoning interest(for example, ones captured axioms C1C3). use Luniq , everyformula Luniq valid Tuniq provable axioms AXuniq, proof involvesformulas Luniq.? able find complete axiomatization languageLuniq respect . However, think finding complete axiomatizationLuniq respect great interest, Luniq simply languageappropriate reasoning causality . necessarily uniquesolution equations arise causal model , useful able sayexists solution certain properties solutions certainproperties. precisely language L+ lets us do.6 show,fact elegant sound complete axiomatization L+ respect .Consider following axioms:D0. instances propositional tautologies.~ ~y](X(~u) = x X(~u) 6= x0 ) x, x0 R(X), x 6= x0D1. [Y(functionality)~ ~y](xR(X)X(~u) = x)D2. [Y(definiteness)~ ~xi(W (~u) = w~ (~u) = ~y) hX~ ~~ (~u) = ~D3. hXx; W wi(Yy)(composition)5. Although say explicitly, clear intend restrict casual modelssatisfying Ord, fixed order . Without restriction, result true.6. Note L+ allows us say unique solution random variable X setting~ ~yitrue(~~ ~variables. example, hYu) [Y](X(~u) = x) says solutions~ set ~y U set ~equationsu and, them, X uniquely determined x.325fiHalpern~ w;D4. [W~ X x](X(~u) = x)(effectiveness)~ ~x; yi(W (~u) = w Z(~~ u) = ~~ ~~ u) = ~z))D5. (hXz ) hXx; W wi(Y (~u) = Z(~~~~~hX ~xiW (~u) = w (~u) = Z(~u) = ~z)), Z = V (X {W, })(reversibility)D6. (X0;X1. . . Xk1; X ) (X ; X )kk0~ ~x] [X~ ~x]( )) [X~ ~x]D7. ([X~ ~x] propositional tautologyD8. [X(recursiveness)(distribution)(generalization)~ ~yitrue(~u) xR(X)[Y~ ~D9. hY](X(~u) = x) = V {X}(unique solutions V {X})~ ~yitrue(~u) xR(X)[Y~ ~D10. hY](X(~u) = x)(unique solutions)~ ~~ ~y ik (~uk ), (~ui)~ ~yi(1(~u1 ) . . . k (~uk )) (hYi1(~u1) . . . hYD11. hYBoolean combination formulas form X(~ui) = x ~ui 6= ~uj 6= j(separability)D1D6 analogues C1C6 L+ . D4 D6 C4 C6,changes all. axioms quite though. example, C1~ ~y](X(~u) = x) [Y~ ~actually [Y](X(~u) = x0 ) x 6= x0 . Lemma 2.2,equivalent D1 Tuniq; however, two formulas equivalent general. Similarly,~ ~y ](X(~u) = x), closer D10 D2 (since disjunctionC2 xR(X)[Y~ ~y]). Again, D10 D2 equivalent Tuniq (bothoutside scope [Yequivalent C2 case) but, general, D10 stronger D2. D2 D9,weaker D10, hold . exact analogue C3 would use [ ] instead h~ (~u) = ~say (~u) = instead. completeness, necessary vectorvariables here. Using [ ] instead h also results valid formula (and would require~ ). two variants equivalent Tuniq, different general,vectorone given useful. (More precisely, get completeness,version [ ] suffice completeness.) Similarly, D5, use h instead~ u) = ~z. turn necessary soundness.[ ], add extra clause Z(~sense, think D1D6 capturing true content C1C6, dropassumption structural equations unique solution. D7 D8 standardproperties modal operators. D10 need capture fact structuralequations unique solutions. D11 essentially says solutions equationsarise exogenous variables set ~u independent solutionsarise exogenous variables set ~u0 6= ~u.+Let AX consist D0D5, D7D9, D11, MP (modus ponens); let AX+uniq+++result adding D10 AX ; let AXrec result adding D6 AXuniq.+Theorem 3.3: AX+ (S) (resp., AX+uniq(S), AXrec (S)) sound complete axiomati+zation L (S) respect (S) (resp., Tuniq(S), Trec(S)).Proof: See appendix.326fiAxiomatizing Causal Structures4. Decision Proceduressection consider complexity deciding formula satisfiable (or valid).This, course, depends language (L+ , Luniq, LGP ) class models (Trec,Tuniq, ) consider. also depends formulate problem.One version problem consider fixed signature = (U, V, R), askhard decide formula L+ (S) (resp., Luniq(S), LGP (S)) satisfiable Trec(S)(resp., Tuniq(S), (S)). finite (that is, V U finite R(Y ) finiteU V), turns quite easy, trivial reasons.Theorem 4.1: fixed finite signature, problem deciding formulaL+ (S) (resp., Luniq(S), LGP (S)) satisfiable Trec(S) (resp., Tuniq(S), (S))solved time linear || (the length viewed string symbols).Proof: finite, finitely many causal models (S), independent. Given , explicitly check satisfied (or all) them.done time linear ||. Since parameter problem, huge numberpossible causal models check affects constant.even better Theorem 4.1 suggests fixed finite signature. SupposeV consists 100 variables mentions 3 them. causal model must specifyequations 100 variables. really necessary consider happens97 variables mentioned decide satisfiable valid? following resultshows, restrict models Tuniq, need check variables appearS. Given signature = (U, V, R), let = ({U }, V, R), V consistsvariables V appear , U fresh exogenous variable, mentioned V U,R (X) = R(X) X V , R(U ) consists tuples U U R(U )mentioned .Theorem 4.2: formula L+ (S) satisfiable Trec(S) (resp., Tuniq(S)) iffsatisfiable Trec(S) (resp., Tuniq(S)).Proof: See appendix.analogue Theorem 4.2 hold . example, suppose =(, {X, Y, Z}, R), R(X) = R(Y ) = R(Z) = {0, 1}, formula hX0i(Y = 0) hX 0i(Y = 1). easy see causal model (S)satisfying . example, = (S, F ), FX (y, z) = z, FY (x, z) = x zFZ (x, y) = x y, represents addition mod 2, easy check |= .hand, causal model 0 (S) 0 |= . suppose0 |= 0 = (S, F 0). Since 0 |= hX 0i(Y = 0), must FY0 (0) = 0; since0 |= hX 0i(Y = 1), must FY0 (0) = 1. cannot FY0 (0) = 0FY0 (1) = 1, since FY0 function.variant Theorem 4.2 hold give us boundnumber variables need consider. Given signature = (U, V, R), define||S|| = XV |R(X)| (where take ||S|| = either V infinite |R(X)| =+X V). ||S|| > ||S||2 + ||S||, let S+ = ({U }, V+, R+), V V327fiHalperndefined together one fresh endogenous variable X , R+(X ) = XV R(X),+2+0R (U ) = R(U ). ||S|| ||S|| + ||S||, let = ({U }, V, R ), R0 (X) = R(X)X V R0 (U ) = R (U ).Theorem 4.3: formula L+ (S) satisfiable (S) iff satisfiable (S+ ).Proof: See appendix.Note ||S|| ||S||2 + ||S||, then, since assumed (without loss generality) |R(X)| 2 variable X, must case2 log2 (||S||) + 1 variables signature S.Since Theorems 4.2 4.3 apply formulas L+ (S), apply fortioriformulas Luniq(S) LGP (S). Although stated terms satisfiability,immediate also hold validity. Thus, tell us that, without loss generality,considering satisfiability validity, need consider finitely many variables(essentially, ones appear , perhaps more). sense,restrict signatures finitely many variables without loss generality. Noteresults tell us restrict finite sets values variableswithout loss generality.Returning complexity decision problem, note Theorem 4.1 analogue observation propositional logic, satisfiability problem lineartime restrict fixed set primitive propositions. proof satisfiability problem propositional logic NP-complete implicitly assumesunbounded number primitive propositions disposal.two ways get analogous result here. first allow signatureinfinite second make signature part input problem.results cases similar, consider case signature partinput here.Theorem 4.4: Given input pair (, S), L+ (S) (resp., Luniq(S))finite signature, problem deciding satisfiable Trec(S) (resp., Tuniq(S), (S))NP-complete (resp., NP-hard) ||; LGP (S), problem decidingsatisfiable Trec(S) (resp., Tuniq(S)) NP-complete (resp., NP-hard).Proof: See appendix.believe problem deciding formula Luniq(S) L+ (S) satisfiableTuniq(S) (S) NP-complete, case deciding LGP (S) satisfiableTuniq(S). However, able show this. satisfiability problemformulas LGP (S)? may well constant time! Indeed, infinitesignature (that is, = (U, V, R) |V| = ), provably constant time.point formula LGP (S) trivially satisfiable structure LGP (S)~ ~x, equations ~settings XX~x solutions, alwaysmodel structure infinitely many variables. finitely many variables,trivial models, may still possible show trivial enoughmodel exists satisfies formula. emphasizes LGP (S) simplyweak language reason models (S).328fiAxiomatizing Causal Structures5. Conclusionprovided complete axiomatizations decision procedures propositional languages reasoning causality. tried stress important role choicelanguage (and signature) axiomatizations and, generally,reasoning process.models languages considered somewhat limited. example,general approach modeling causality would allow one valueX set variables. would appropriate model thingssomewhat coarser level granularity, values variablesX suffice completely determine value X. believe results paperextended straightforward way deal generalization, althoughchecked details. general causal reasoning, believe need richer language,includes first-order features. hope return issue finding appropriatericher languages causal reasoning future work.AcknowledgmentsId like thank Judea Pearl comments previous version paper, wellgenerous help providing pointers literature. work supported partNSF grant IRI-96-25901 Air Force Office Scientific Researchgrant F49620-96-1-0323. preliminary version paper appears Proc. FourteenthConference Uncertainty AI, pp. 202210, 1998.Appendix A. ProofsTheorem 3.2: AXuniq (resp., AXrec) sound complete axiomatization Luniq(S)respect Tuniq(S) (resp., Trec(S)).Proof: Soundness proved Galles Pearl. make paper self-contained,reprove non-obvious casethe validity C5 Tuniq.Let Tuniq suppose |= Y~xw (~u) = W~xy (~u) = w. want show|= Y~x (~u) = y. Since Tuniq, unique vector ~v1 satisfiesequations T~xw (~u) unique vector ~v2 satisfies equations T~xy (~u). claim~ , W components vectors~v1 = ~v2 . assumption, X,(~x, y, w, respectively). consider T~xyw (~u). claim ~v1 ~v2solutions equations causal theory. Note variable Z~ {W, }, equation F ~xw,~u Z T~xw (~u) equations F ~xy,~uXZZFZ~xyw,~u Z T~xy (~u) T~xyw (~u), respectively, except first case, wplugged value W , second case plugged value, third case, w plugged in. However, since wvalues W , respectively, ~v1 ~v2, since vectors satisfyequation FZ~xw FZ~xy , must also satisfy FZ~xwy . Since equations T~xyw (~u)unique solution, ~v1 = ~v2 , desired.329fiHalpernNext, claim ~v1 satisfies equations T~x (~u). Again, above, clear~ {W, }. similar argument shows satisfiessatisfies equation Z/Xequation T~x (~u), since ~v1 satisfies equation T~xw (~u). Finally, similarargument shows satisfies equation W T~x (~u), since ~v2 = ~v1 satisfiesequation W T~xy (~u). Since component ~v1 y, follows Y~x (~u) = y.much soundness. completeness, usual, suffices prove formulaLuniq consistent AXuniq (resp., AXrec), satisfied causal modelTuniq (resp., Trec). (Heres argument: want show every valid formulaprovable. Suppose shown every consistent formula satisfiablevalid. provable, consistent. assumption, meanssatisfiable, contradicting assumption valid.)give argument case AXuniq.Suppose formula Luniq(S), = (U, V, V ), consistent AXuniq.Consider maximal consistent set C formulas includes . (A maximal consistent setset formulas whose conjunction consistent larger set formulas wouldinconsistent.) follows easily standard propositional reasoning (i.e., using C0MP only) maximal consistent set exists. Moreover, C1 C2, followsrandom variable X V vector ~values, exists exactly one elementx R(X) X~y = x C. construct causal model = (S, F ) Tuniq(S)satisfies every formula C (and, particular, satisfies ).~ consists variables V {X}. Thus,term XY~ ~y (~u) complete (for X)XY~ ~y (~u) complete term every random variable X determined. usecomplete terms define structural equations. variable X V, defineFX (~u, ~y) = x X~y (~u) = x, X~y (~u) complete term. gives us causal model. show model Tuniq formulas Csatisfied .~ |.show XY~ ~y (~u) = x C iff |= XY~ ~y (~u) = x induction |V| |Y~ | = 0 follows immediately C4, since X~ . |V| |Y~| =case |V| |Y6 0,assume without loss generality X Y~ , otherwise result~ | = 1, result follows definitionfollows C4. Given assumption, |V| |Yequations FX .~ | = k > 1. want showgeneral case, suppose |V| |Yunique solution equations TY~ ~y (~u) that, solution, X value x.see solution, define vector ~v show fact solution.W Y~ W w assignment W Y~ ~, set W component ~vw. W Y~ , set W component ~v unique value wWY~ ~y (~u) = w C. (By C1 C2 unique value w.) claim ~vsolution equations TY~ ~y (~u).~ W . C3 C4, every~0 =Ysee this, let W variable V Y~ . Let0~~variable Z V , Z~yw (~u) = z . Since |V| |Y 0 | = k 1, inductive~ 0 ,hypothesis, ~v fact unique solution T~yw (~u). every variable Z Vw ,~~u~y,~uequation FZZ T~yw (~u) equation FZ Z T~y (~u), except,~~uW set w . Thus, every equation T~y (~u) except possibly equation FW~y,~usatisfied ~v . see FW also satisfied ~v , simply repeat argument330fiAxiomatizing Causal Structures~ . (Such variable must exist since |V| |Y~ |starting another variable W 0 Vassumed least 2.)remains show ~v unique solution equations T~y (~u). Supposeanother solution, say ~v 0 , equations. Suppose variable WV Y~ , W component ~v 0 w. variable Z, must z 6= z .Since Z~y (~u) = z , assumption, follows C1 Z~y (~u) 6= z C (since Cmaximal consistent set). also easy see W V Y~ , vector ~v 0~.also solution equations T~yw (~u). Let W variable Z Vinduction hypothesis, follows W~yz (~u) = w Z~yw (~u) = zC. C5 (reversibility), Z~y (~u) = z C. contradicts consistency C.completes proof case Tuniq(S). Essentially proof worksTrec. need observe C6 guarantees theory construct takenrecursive. see this, given formula consistent Trec, consider maximal setC formulas consistent Trec contains . Let TC causal model determinedC, above. set C also determines relation exogenous variables: defineZZ C. easily follows C6 transitive closurepartial order: X X, X = . total order variablesconsistent gives ordering TC recursive.;+Theorem 3.3: AX+ (resp., AX+uniq, AXrec ) sound complete axiomatizationL+ (S) respect (S) (resp., Tuniq(S), Trec(S)).Proof: Soundness proceeds much Theorem 3.2; leave details reader.completeness, proceed much proof Theorem 3.2. proofssimilar spirit, sketch proof AX+ ; modifications AX+uniqleftreader.AX+recAgain, given formula consist AX+ , consider maximal consistent setformulas containing consistent AX+ , use construct causal model. Note D9 suffices this, defining FX (~u, ~y), needed know~ ~y ](X(~u) = x) Y~ = V X, D9 (together D1) assuresunique x [Yus unique x. Again, want show formulas Csatisfied .~ ~y ,this, clearly suffices show every formula form hYC iff |= . reduce considering even simpler formulas, namely,~ u) = ~x, applying axioms. see this, firstones form X(~observe standard arguments modal logic (using D0, D7, D8, MP) show~ ~y i(1 2) provably equivalent hY~ ~~ ~hYi1 hYi2. meansassume without loss generality conjunction formulas form X(~u) x~ ~yi( X(~u) 6= x) equivalentnegations. D2 follows hY0~hY ~y i( (x0R(X){x}X(~u) = x ). Thus, assume without loss generalitynegations. applying D11, assume without loss generalitysetting ~u exogenous variables used conjuncts. Thus, suffices~ ~yi(X(~~ u) = ~x) C iff |= hY~ ~~ u) = ~x) X~ = V~.show hYi(X(~~ | again. base case dealt usingthis, proceed induction |V| |Y~~ ~~ u) =D4, before. assume k 1 |V| |Y | = k + 1. Suppose hYi(X(~~ Suppose X1 x1 X2 x2 assignments~x) C. Let X1 , X2 X.331fiHalpern~ ~x. Let X~ 0 ~x0 X~ 00 ~x00 result removing X1 x1X1 X2 X~ ~~ ~~ 00(~u) = ~X2 x2, respectively, Xx. D3, hY; X1 x1 i(Xx00 )~ 0(~u) = ~x0 ) C. induction hypothesis,~ ~y; X2 x2 i(XhY~ ~~ u) = ~x0 ),formulas true . soundness D5, follows |= hYi(X(~desired.~ ~~ u) = ~x0 ). Then, since D3 sound,Conversely, suppose |= hYi(X(~~ 00(~u) = ~~ ~y ; X2 x2 i(X~ 0(~u) = ~x0 ).~ ~y; X1 x1i(Xx00 ) |= hY|= hY00~~induction hypothesis, hY ~; X1 x1i(X (~u) = ~x00 )00~ (~u) = ~x ) C. apply D5 complete proof.~ ~y ; X2 x2 i(XhYTheorem 4.2: formula L+ (S) satisfiable Trec(S) (resp., Tuniq(S)) iffsatisfiable Trec(S) (resp., Tuniq(S)).Proof: Clearly, formula satisfiable Trec(S) (resp., Tuniq(S )), satisfiableTrec(S) (resp., Tuniq(S)). easily convert causal model = (S, F ) Trec(S)satisfying causal model 0 = (S, F 0) Trec(S) satisfying simply defining0FXconstant, independent arguments, X V V ; X V , define0x V {X}R(Y ) ~VV R(Y );FX (~u, ~x, ~y) = FX (~u, ~x), ~u R(U ), ~0~u/ R(U ), define FX(~u, ~x, ~y) arbitrary constant. identical transformationworks Tuniq(S).converse, suppose satisfiable causal model = (S, F ) Trec(S).Thus, ordering variables V X , FX independent value . means view FX function exogenous variablesU variables V X. Let Pre(X) = {Y V : X}.convenience, allow FX take arguments values variables U Pre(X),rather requiring arguments include values variables U V {X}.0define functions FX: (U U R(U )) (Y V{X} R(Y )) R(X) X Vinduction (that is, start -minimal element, whose value independentvariables, work chains). Suppose X V ~x vector0 (~u, ~x) = FX (~u).values variables V {X}. X -minimal, define FX0general, define FX (~u, ~x) = FX (~u, ~z), ~z vector values variablesPre(X) defined follows. Pre(X) V , value component ~zvalue component ~y ; Pre(X) V, value component~z FY0 (~u, ~x). (By induction hypothesis, FY0 (~u, ~x) already defined.)define causal model 0 = (S , F 0). easy check 0 Trec(S ) (the orderingvariables restricted V ). Moreover, construction guarantees~ V , solutions equations 0u) TX~u) same,X~ x (~~ x (~X~0restricted variables V . follows satisfies .argument case Tuniq(S) similar spirit. X V , ~u0(~u, ~x) value X(U U R(U )), ~x (Y V {X}R(Y )), define FX7unique solution equations TV{X}~x (~u). straightforward check0 = (S , F 0) Tuniq(S ) satisfies .07. definition easily seen agree earlier definition FXTrec .332fiAxiomatizing Causal StructuresTheorem 4.3: formula L+ (S) satisfiable (S) iff satisfiable (S+ ).Proof: ||S|| ||S||2 +||S|| proof immediate, suppose ||S|| > ||S||2 +||S|| satisfied causal model = (S, F ) (S). goingproof, useful define notation. Let V = {X1, . . . , Xm}, V = {X1, . . . , Xk }V V = {Xk+1 , . . . , Xm}. Given vector ~x R(X ) = XV R(X) Xi V ,let ~xi denote vector excluding value Xi . Xi V , choose two values0xi0 xi1 R(Xi). Define 0 = (S , F 0) defining FX(~u, ~xi , ~yi , yi) = x,x = yi ~xi = ~yi X = yi solution equations TV {Xi }~xi (~u);x = xi0 yi 6= xi0 either ~xi 6= ~yi solution equationsTV{X}~xi (~u) X = yi ;x = xi1 otherwise.Finally, define FX (~u, ~x) = ~x.~ V, solutionsshow construction guarantees X0equations TX~u) TX~u) same, restricted variables~ x (~~ x (~u), ~R(X )V . First suppose (~y, ~z) solution equations TX~~ x (~~x ~y agree variables X,~z VV R(Y ). must case ~~(~y, ~z) also solution equations TV {Xi }~yi (~u) Xi V X. Thus,00FX(~u, ~yi , ~y) = yi . follows (~y, ~y ) solution equations TX~u).~ x (~00u).Conversely, suppose (~y, ~y ) solution equations TX~~ x (~0~. Moreover, since ~x ~agree variables X,definition FX guarantees ~y = ~00(~y, ~y) must also solution equations TV{X1 }~y1 (~u). Thus, FX1 (~u, ~y1, ~y) =z values variables V Vy1 , means must vector ~(~y, ~z) solution equations TV {X1 }~y1 (~u). easy check(~y, ~z) must fact solution equations TV {Xi }~yi (~u) = 1, . . ., k.u), desired. sufficesfollows (~y, ~z) solution equations TX~~ x (~prove direction theorem.suppose satisfied causal model = (S+ , F ) (S+ ). Since ||S|| >||S||2 + ||S||, must injective function f : R(X ) VV R(Y ) twodistinct vectors ~y0 = (y01, . . . , y0k ), ~y1 = (y11, . . . , y1k ) range f .Choose two distinct vectors ~x0 = (x10, . . . , xk0), ~x1 = (x11, . . . , xk1) R(X ). Define0 = (S, F 0) (S) follows. Xi V, ~xi V {Xi } R(Y ), ~z R(X ),~y VV R(Y ), let0FX(~xi , ~y) =xi , ~z) f (~z) = ~y,FXi (~x0ix1i~y range f , ~6= ~y1 ,otherwise.Xj V V , ~x R(X ) ~yj VV{Xj } R(Y ), let0(~x, ~yj )FXj=0j1jf (FX (~x)) = (~yj , y),f (FX (~x)) 6= (~yj , 0) 0 R(Xj ), ~x 6= x~0 ,otherwise.333fiHalpern~ V , solutionsAgain, show construction guarantees X0equations TX~u) TX~u) same, restricted variables V . First~ x (~~ x (~u), ~, ~z R(X ). easysuppose (~y, ~z) solution equations TX~~ x (~0check (~y, f (~z)) solution equations TX~u). Conversely, suppose~ x (~0u), ~R(X ) ~z VV R(Y ).(~y, ~z) solution equations TX~~ x (~claim must ~z = f (FX (~)). If, fact, case, easyu). hand,check (~y, FX (~y) solution equations TX~~ x (~0 X V V guarantees ~z=~y0 unless~z 6= f (FX (~y)), definition FXjj~y = ~x0; ~y = ~x0 , ~z = ~y1 . definition FXi Xi V guarantees, ~z) solution iff ~z = f (FX (~y)).~z = ~y0 , ~y = ~x0: otherwise, ~y = ~x1 . Thus, (~suffices prove result.Theorem 4.4: Given input pair (, S), L+ (S) (resp., Luniq(S))finite signature, problem deciding satisfiable respect Trec(S) (resp.,Tuniq(S), (S)) NP-complete (resp., NP-hard) ||; LGP (S), problemdeciding satisfiable Trec(S) (resp., Tuniq(S)) NP-complete (resp., NP-hard).Proof: NP-lower bound easy L+ (S) Luniq(S), since obvious wayencode satisfiability problem propositional logic satisfiability problemL+ Luniq. Given propositional formula primitive propositions p1, . . . , pk , let= (, {X1, . . . , Xk }, R), R(Xi) = {0, 1} = 1, . . . , k. Replace occurrenceprimitive proposition pi formula Xi = 1. gives us formula 0Luniq(S). easy see 0 satisfiable causal model (S) (and, fortiori0 satisfiable causal model either Trec(S) Tuniq(S)) solutionequations defines satisfying assignment . Conversely, satisfiable, saytruth assignment v, trivially construct causal model Trec(S)FXi = v(pi). (For simplicity, assume valuations assign values 0 1 ratherfalse true.)trivial construction 0 work LGP (S), since disjunctionsnegations available. lack negations cause problem. assumewithout loss generality negations occur front primitive propositions,capture pi formula Xi = 0. idea dealing disjunctionsformula p1 p2 p3 translated [X1 0; X2 1; X3 1](Y = 0),fresh variable. Essentially, viewing p1 p2 p3 (p1 p2 p3 ) false,write, example, X1 0 even though p1 appears positively disjunction.make matters simpler, assume formula 3-CNF. suffices NPhardness, since satisfiability problem 3-CNF formulas also NP-hard (Garey &Johnson, 1979). Suppose form c1 . . .cm , cl clause consistingdisjunction three primitive propositions negations. Suppose primitivepropositions appear p1, . . . , pk . Let = (, {X1, . . . , Xk , Y1 , . . ., Ym }, R),R(Xi) = R(Yj ) = {0, 1} i, j. Suppose cj , jth clause ,form qj1 qj2 qj3 , qji either pji pji ji . Let ctj LGP formula[Xj1 xj1 ; Xj2 xj2 ; Xj3 xj3 ](Yj = 0),334fiAxiomatizing Causal Structuresxjh 0 qjh pjh xjh 1 qjh pjh h = 1, 2, 3. Let 0[true](Y1 = 1 . . . Ym = 1) ct1 . . . ctm.claim satisfiable propositional formula iff LGP formula 0 satisfiableTrec(S) (resp. Tuniq(S)). First suppose 0 satisfiable, say model Tuniq(S).(If direction holds Tuniq(S), clearly holds fortiori Trec(S).) Let ~zunique solution equations . construction, Yj component ~z 1j = 1, . . ., m. Let xi value Xi component ~z. Consider valuation vv(pi) = xi . claim v() = 1. see this, suppose clause cj qj1 qj2 qj3 .v makes qj1 , qj2 , qj3 false, must xjh = xjh h = 1, 2, 3. Since|= [Xj1 xj1 ; Xj2 xj2 ; Xj3 = xj3 )](Yj = 0) value Xjh component ~zxjh h = 1, 2, 3, follows ~z solution equations TXj1 xj1 ;Xj2 xj2 ;Xj3 xj3 .contradicts fact |= [Xj1 xj1; Xj2 xj2 ; Xj3 xj3](Yj = 0) (sinceYj component ~z 1). follows v(cj ) = v(qj1 qj2 qj3 ) = 1. Since trueclauses cj , must v() = 1.converse, suppose satisfiable, say valuation v. show 0satisfiable Trec(S). Order variables Xj1 , Xj2 , Xj3 Yj . (There manyorderings variables satisfy constraints; one do.) Define FXi = v(pi)(so FXi constant, independent arguments); define FYj (xj1 , xj2 , xj3 ) = 1(xj1 , xj2 , xj3 ) = (v(pj1 ), v(pj2 ), v(pj3 )) 0 otherwise. easy check |= 0 ,desired.NP upper bound case Trec(S), clearly suffices deal L+ .Suppose given (, S) L+ . want check satisfiable Trec(S).basic idea guess causal model verify indeed satisfies .problem though. completely describe model , need describefunctions FX . However, may many variables X manypossible inputs. describing functions may take time much longer polynomial. Part solution problem provided Theorem 4.2, tells ussuffices check whether satisfiable Trec(S ). light this, remainderpart proof, assume without loss generality = . limitsnumber variables must consider O(||). even solveproblem completely. Since given bounds |R(Y )| variables S,even describing functions FY variables appear possibleinput vectors could take time much polynomial . solution giveshort partial description model show suffices.~ ~y , ~u) subformula form [Y~ ~Consider pairs (Y]2~u appears . Let R set pairs. Note |R| < || . say~ ~, ~u) R,two causal models 0 Trec(S) agree R if, pairs (Y0(unique) solutions equations TY~ ~y (~u) ~ (~u) same. easy see~0 agree R, either 0 satisfy neither do. is,need know causal model deals relevant equationsthosecorresponding pairs R.~ ~y , ~u) R, guess vector ~v (Y~ ~pair (Y, ~u) values endogenousvariables; intuitively, unique solutions relevant equations modelsatisfying . Given guesses, easy check satisfied model335fiHalpernguesses indeed represent solutions relevant equations. remains showexists causal model Trec(S) relevant equations solutions.this, first guess ordering variables. verify,~ ~fixed ~u appears , whether solution vectors ~v (Y, ~u) guessed relevantequations compatible , sense case twosolutions (~u, ~x) (~u, ~x0 ) variable X takes different values ~x ~x0 ,variables X take values ~x ~x0 . easysee solutions compatible , define functions FX X Vequations hold FX independent values XX, V. (Note never actually write functions FX , may takelong; know exist.) summarize, long guesssolutions relevant equations causal model solutions satisfies, ordering solutions compatible , satisfiableTrec(S). Conversely, satisfiable Trec(S), clearly solutionsrelevant equations satisfy ordering solutionscompatible . (We take solutions ordering .) showssatisfiability problem Trec NP, desired.ReferencesChajewska, U., & Halpern, J. Y. (1997). Defining explanation probabilistic systems.Proc. Thirteenth Conference Uncertainty Artificial Intelligence (UAI 97), pp.6271.Druzdzel, M. J., & Simon, H. A. (1993). Causality bayesian belief networks. Uncertainty Artificial Intelligence 9, pp. 311.Galles, D., & Pearl, J. (1997). Axioms causal relevance. Artificial Intelligence, 97 (12),943.Galles, D., & Pearl, J. (1998). axiomatic characterization causal counterfactuals.Foundation Science, 3 (1), 151182.Garey, M., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryNP-completeness. W. Freeman Co., San Francisco, Calif.Goldberger, A. S. (1972). Structural equation methods social sciences. Econometrica,40 (6), 9791001.Harel, D. (1979). First-Order Dynamic Logic. Lecture Notes Computer Science, Vol. 68.Springer-Verlag, Berlin/New York.Heckerman, D., & Shachter, R. (1995). Decision-theoretic foundations causal reasoning.Journal Artificial Intelligence Research, 3, 405430.Henrion, M., & Druzdzel, M. J. (1990). Qualitative propagation scenario-based approaches explanation probabilistic reasoning. Uncertainty Artificial Intelligence 6, pp. 1732.336fiAxiomatizing Causal StructuresPearl, J. (1995). Causal diagrams empirical research. Biometrika, 82 (4), 669710.Pearl, J. (1999). Causality. Cambridge University Press, New York. Forthcoming.Pearl, J., & Verma, T. (1991). theory inferred causation. Principles KnowledgeRepresentation Reasoning: Proc. Second International Conference (KR 91), pp.441452.Spirtes, P., Glymour, C., & Scheines, R. (1993).Springer-Verlag, New York.Causation, Prediction, Search.Strotz, R. H., & Wold, H. O. A. (1960). Recursive vs. nonrecursive systems: attemptsynthesis. Econometrica, 28 (2), 417427.337fiJournal Artificial Intelligence Research 12 (2000) 271-315Submitted 2/00; published 5/00Compilability Expressive PowerPropositional Planning FormalismsBernhard NebelNEBEL @ INFORMATIK . UNI - FREIBURG . DEInstitut fur Informatik, Albert-Ludwigs-Universitat, Georges-Kohler-Allee, D-79110 Freiburg, GermanyAbstractrecent approaches extending GRAPHPLAN algorithm handle expressiveplanning formalisms raise question formal meaning expressive power is.formalize intuition expressive power measure concisely planning domainsplans expressed particular formalism introducing notion compilationschemes planning formalisms. Using notion, analyze expressivenesslarge family propositional planning formalisms, ranging basic STRIPS formalismconditional effects, partial state specifications, propositional formulae preconditions.One results conditional effects cannot compiled away plan size growlinearly compiled away allow polynomial growth resulting plans.result confirms recently proposed extensions GRAPHPLAN algorithm concerningconditional effects optimal respect compilability framework. Another resultgeneral propositional formulae cannot compiled conditional effects plan sizepreserved linearly. implies allowing general propositional formulae preconditionseffect conditions adds another level difficulty generating plan.1. IntroductionG RAPHPLAN (Blum & Furst, 1997) SATPLAN (Kautz & Selman, 1996) amongefficient planning systems nowadays. However, generally felt planning formalismsupported systems, namely, propositional basic STRIPS (Fikes & Nilsson, 1971),expressive enough. reason, much research effort (Anderson, Smith, & Weld, 1998; Gazen& Knoblock, 1997; Kambhampati, Parker, & Lambrecht, 1997; Koehler, Nebel, Hoffmann, & Dimopoulos, 1997) devoted extending GRAPHPLAN order handle powerfulplanning formalisms ADL (Pednault, 1989).appears consensus much expressive power added particular language feature. example, everybody seems agree adding negative preconditionsadd much expressive power basic STRIPS, whereas conditional effects consideredsignificant increase expressive power (Anderson et al., 1998; Gazen & Knoblock, 1997;Kambhampati et al., 1997; Koehler et al., 1997). However, unclear measure expressive power formal way. Related problem question whether compilationapproaches extend expressiveness planning formalism optimal. example, GazenKnoblock (1997) propose particular method compiling operators conditional effectsbasic STRIPS operators. method, however, results exponentially larger operator sets.people (Anderson et al., 1998; Kambhampati et al., 1997; Koehler et al., 1997) agreecannot better that, nobody proven yet space-efficient methodimpossible.c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiN EBELorder address problem measuring relative expressive power planning formalisms, start intuition formalism least expressive another formalism planning domains corresponding plans formalism concisely expressedformalism . This, least, seems underlying intuition expressive powerdiscussed planning literature.Backstrom (1995) proposed measure expressiveness planning formalisms usingESP-reductions. reductions are, roughly speaking, polynomial many-one reductionsplanning instances change plan length. Using notion, showedpropositional variants basic STRIPS containing conditional effects arbitrary logicalformulae considered expressively equivalent. However, taking point view, ESPreductions restrictive two reasons. Firstly, plans must identical size, mightwant allow moderate growth. Secondly, requiring transformation computedpolynomial time overly restrictive. ask concisely something expressed,necessarily imply exists polynomial-time transformation. fact, oneformalism might expressive another one, mapping formalisms mightcomputable all. This, least, seems usual assumption made termexpressive power discussed (Baader, 1990; Cadoli, Donini, Liberatore, & Schaerf, 1996; Erol,Hendler, & Nau, 1996; Gogic, Kautz, Papadimitriou, & Selman, 1995).Inspired recent approaches measure expressiveness knowledge representation formalisms (Cadoli et al., 1996; Gogic et al., 1995), propose address questionsexpressive planning formalism using notion compiling one planning formalismanother one. compilation scheme one planning formalism another differs polynomial many-one reduction required compilation carried polynomialtime. However, result expressible polynomial space. Furthermore, requiredoperators planning instance translated without considering initial stategoal. restriction might sound unnecessarily restrictive, turns existingpractical approaches compilation (Gazen & Knoblock, 1997) well theoretical approaches(Backstrom, 1995) consider structured transformations operators transformedindependently initial state goal description. technical point viewrestriction guarantees compilations non-trivial. entire instance could transformed,compilation scheme could decide existence plan source instance generatesmall solution-preserving instance target formalism, would lead unintuitiveconclusion planning formalisms expressive power.mentioned beginning, space taken domain structure important,also space used plans. reason, distinguish compilation schemeswhether preserve plan size exactly, linearly, polynomially.Using notion compilability, analyze wide range propositional planning formalisms, ranging basic STRIPS planning formalism containing conditional effects, arbitrary boolean formulae, partial state specifications. one results, identify twoequivalence classes planning formalisms respect polynomial-time compilability preserving plan size exactly. means adding language feature formalism without leavingclass increase expressive power affect principal efficiency1. assume reader basic knowledge complexity theory (Garey & Johnson, 1979; Papadimitriou,1994), familiar notion polynomial many-one reductions complexity classes P, NP, coNP,PSPACE. notions introduced paper needed.272fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSplanning method. However, also provide results separate planning formalisms usingresults computational complexity theory circuit complexity non-uniform complexityclasses. separation results indicate adding particular language feature adds expressive power difficulty integrating feature existing planning algorithm.example, prove conditional effects cannot compiled away boolean formulaecannot compiled conditional effectsprovided plans target formalism allowedgrow linearly.answers question posed beginning. compilation approach proposed GazenKnoblock (1997) cannot space efficient, even allow linear growth planstarget formalism. Allowing polynomial growth plans, however, compilationscheme space efficient. Interestingly, seems case compilation schemeallows polynomially larger plans similar implementation conditional effectsIPP system (Koehler et al., 1997), Kambhampati colleagues' (1997) planning system,Anderson colleagues' (1998) planning system.rest paper structured follows. Section 2, introduce range propositional planning formalisms analyzed paper together general terminology definitions.Based that, introduce notion compilability planning formalisms Section 3.Section 4 present polynomial-time compilation schemes different formalismspreserve plan size exactly, demonstrating formalisms identical expressiveness.remaining cases, prove Section 5 cannot compilation schemepreserving plan size linearly, even bounds computational resourcescompilation process. Section 6 reconsider question identical expressiveness using compilation schemes allow polynomial growth plans. Finally, Section 7summarize discuss results.2. Propositional Planning FormalismsFirst, define general propositional planning formalism, appears almostexpressive propositional variant ADL (Pednault, 1989). formalism allows arbitraryboolean formulae preconditions, conditional effects partial state specifications. Subsequently,specialize formalism imposing different syntactic restrictions.2.1 General Propositional Planning FormalismLet countably infinite set propositional atoms propositional variables. Finitesubsets denoted . Further, defined set consisting constants(denoting truth) (denoting falsity) well atoms negated atoms, i.e., literals,. language propositional logic logical connectivesfiff ,propositional atoms denoted fi . clause disjunction literals. Further, sayformula conjunctive normal form (CNF) conjunction clauses.disjunctive normal form (DNF) disjunction conjunctions literals.Given set literals , fi refer positive literals , "!$#%& refernegative literals , '(& atoms used , i.e., '($fi*),+.-//102-33 4-52. Note Gazen Knoblock's (1997) translation scheme also generates planning operators dependinitial state goal description. However, operators simply code initial state goal descriptionnothing else. reason, ignore here.273fiN EBEL76. Further, define 8 element-wise negation , i.e.,8),+.-90:4-35;6&<=+>4-?0@-=376BAstate C truth-assignment atoms . following, also identify stateset atoms true state. state specification subset , i.e., logicaltheory consisting literals only. called consistent iff contain complementary literals. general, state specification describes many states, namely satisfy ,denoted EF:GH$D( . case complete, i.e., -3/ either -=/D4-IJD , precisely one model, namely K>$D( . abusing notation, referinconsistent state specification , illegal state specification.Operators pairs LM)ON pre ff post P . use notation pre LB post LQ refer firstsecond part operator L , respectively. precondition pre element RKSKT , i.e.,set propositional formulae. set post, set postconditions, consistsconditional effects, formUWV XffUZYcalled effectV conditions elementscalledelementsUeffects. singleton sets, e.g., +.-H6+\[]6 , often omit curly brackets write- V [.Example 1 order illustrate various notions, use running example planningproblems connected production camera-ready manuscripts LATEX source filessomewhat simplified, course. set atoms , choose following set:^),+`_bacffed]fgcffihQj]klffgm no4ffgp\qgffgrsklrsffgr]rsmlffgrsm o4fftklu>hvfftk hQcfftkwmhQj]k klu>h nxQffihBjsk y>k _za nx6BApropositional atoms following intended meaning. atoms first line representpresence corresponding files, atoms second line signify indexcitations correct dvi-file. Based that, define following operators: rBkwr\_bac , d{_zac ,| d]xea>kwu{h]ac . first operators simple. precondition executionrBklr - d]fgc -file exist. successful execution, r]rsm - r]m -fileproduced:rBklr{_zacM)~}`d]fgcffgrBkwr4ff{v V +Brsrsmlffgrsm ov6"vA| d]xea>kwu{hsacoperator similar:| d]xea>kwu{h]acM)}4`k hc`4ff{v V +Qkwu{h`fftklm ov6"vAFinally, d{_zac operator bit complicated. precondition needs presence_bac -file produces effect d]fgc -, k hc -, hBjsk , -files unconditionally. addition,know citations correct rsr]m -file present index correct274fiC OMPILABILITYklu>h-file present:E XPRESSIVE P OWER P LANNING F ORMALISMSd{_bac)~}_zac`4ffVVrsr]mfir]rsVklu>h+{d]f:cfftk hQcffihBjsklffgm no`6BffV hBjs8k hQy>j]k _bk y>k n_zaxff nxQffhBjsk klu>h nxQffVkwu{h 8hQj]k kwu{h nx 4semantics operators given state-transition functions, i.e., mappings statesstates. Given state C set postconditions post, CBff post denotes active effects C :MCsff post()+\0 UWV &post ffeC0 )U 6BAstate-transition function { induced operator L defined follows:>\{C>)R 3 RC7fi"!$#%$Csff post LB..<$MCsff post LB.. C0 ) pre LBCBff post LQ.0)undefinedotherwisewords, precondition operator satisfied state C active effects consistent,state C mapped state C: differs C truth values active effectsforced become true positive effects forced become false negative effects.precondition satisfied set active effects inconsistent, result functionundefined.planning formalism itself, work states state specifications. general,lead semantic problems. restricting state specifications setsliterals, however, syntactic manipulations state specifications defined waysound Lifschitz' (1986) sense.Similarly active effects respect states, define corresponding functionrespect state specifications:$Dff post *)+\I0 UWV fifipost ffD^0 )U 6BAFurther, define potentially active effects follows:$Dff post () CBff postAe %$Dff post ,state specification operator L5)N pre ff post P , $Dff post )means state specification resulting application state-transition functionsmight representable theory consisting literals only. reason, consideroperator application illegal, resulting illegal state specification . couldliberal point consider operator application state specification illegalset states resulting applying state-transition functions could definitely represented3. Note happen state specification incomplete.275fiN EBELtheory consisting literals only. Alternatively, could consider atoms mentioned$Dff post ,M$D8ff post unsafe application operator delete literals7 $D8ff post $Dff post . state specification, consider resulting state specificationstill legal $Dff post consistent. Since seem exist standard modelexecution conditional effects presence partial state specifications, adopt firstalternative one arbitrary choice. noted, however, decision influencesresults present below.$Dff post leads illegal state specification,Similarly rule $Dff post )require precondition satisfied states EF:GH$D( state specificationalready inconsistent, result applying L results . leads definitionfunction , defines outcome applying operator L set operatorsstate specification:R 1 RD/($Dff post LQ.<5M$D8ff post LB. D0)0 ) pre LBJ$D8ff post LB.0)$Dand$D8ffiLQ)$8ffBL.)ff post LB.postotherwiseExample 2 Using propositional atoms operators Example 1, assume followingtwo state specifications)+:_zacKfftklu>hK6 , )+:_bacKfftklu{h`ffgrsr]mffgrsm ov6 . try applyoperator d{_bac , notice resultsff post zm d\_bacB.) +{d]fgcfftk hcffihQj]klffgm no4ffihQj]k klu{h nx6Bffff post zm d\_bacB.) ff post zm d{_bacB.H<=+\hBjsk y>k _za nxQff8hQj]k y>k _ba nx6Bffff post zm d\_bacB.A hand, apply rBkwr\_baci.e., ff post zm d{_bacB.)successfully : ffgrBkwr\_bacB()easily verified syntactic operation state specification using functioncorresponds state transitions states described specification.Proposition 1 Let state specification, L operator, { induced state-transitionfunction. $D8ffiLQ0) ,EF:GH$$DffiLB.),+{C 0gC ){C>ffeC0 )Dfi6BA$DffiLB;0 ) , either1.EF:GH$D(),2. two states C3.MC ff post LB. ,ffeC /E:G$D( MC ff post LB.)exists state CWE:GH$D {>C> undefined.276fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSwords, whenever $DffiLB results legal specification, specification describesstates result application state-transition function > states satisfyoriginal state specification . Further, $D8ffiLQ illegal, good reasons it.planning instance tuple)~N&ffifft5Pff)~N$fftP domain structure consisting finite set propositional atomsfinite set operators ,initial state specification,goal specification.talk size instance, symbolically 00 00 , following, mean size(reasonable) encoding instance.following, use notation refer set finite sequences operators.Elements called plans. 00 300 denotes size plan, i.e., numberoperators . say -step plan 00 3008 . result applying statespecification recursively defined follows:!B;R R;!t$Dff\N$P.);!t]$Dff\NL iff L ffgAgAgA:ffiL>KP.) ;!]$$DffiL ff\NL ffgAgAgA{ffiL{KP.sequence operators said plan solution iff1. X!]ffe0)2. X!]ffe;0 ) .Example 3 Let propositionalatoms operators introduced Example 1consider following planning instance:)N.N$fftPfft+:_bacKffgrBklr]ffklu>hK6Bfft+\hBjskffihQjsk y>k _za nx6>PAwords, given latex source file (_bac ) bibliography database ( rBklr ), want generate dvifile (hBjsk ) citations file correct (hBjsk y>k _ba nx ). Furthermore, knowknow index file yetanything existence bbl-file aux-file etc.,( kwu{h ). plan )~Nzrsklr\_bacffgm d{_zacsP solution plan result illegalstate specification resulting state specification entails hQj]k hBjsk y>k _ba nx .Plans satisfying (1) (2) sound. order state precisely, extendnotion state transition functions operators state transition functions plans. Let {state transition function corresponding composition primitive state-transition functionsinduced operators )~NL ffgAgAgA\ffiL>P , i.e.,B e. b> ) >eAgAgAQfi{bvff4. could liberal requiring . done order allow faircomparison restricted planning formalisms.277fiN EBELe. C> defined iff e. b C defined every , 9 . Usingnotion, one easily proveusing induction plan lengththat plan instancesound Lifschitz' (1986) sense, i.e., corresponds application state transition functionsinitial states.Proposition 2 Let )N&ffifft5P planning instance. X!]ffe consistent,)NL gff AgAgA\ffiL>PelementE:GH$X!ffe.),+{C 0tC ){C>ffeC0 )>6BA;!]ffe inconsistent, either1.EF:GH\(),2. exists (possibly empty) prefixX!]ff\NL ffgAgAgA:ffiL P. either(a) two states CNL gff AgAgA\ffiL>$P (3~ )eff C WEF:GH$D(MC(b) exists state CWE:GH$D {bwC>D)C ff post L> . ,ff post L>2 .)undefined.2.2 Family Propositional Planning Formalismspropositional variant standard STRIPS (Fikes & Nilsson, 1971), also callfollows, planning formalism requires complete state specifications, unconditionaleffects, propositional atoms formulae precondition lists. Less restrictive planningformalisms following additional features:Incomplete state specifications ( ): state specifications may complete.Conditional Effects ( ): Effects conditional.Literals formulae ( ): formulae preconditions effect conditions literals.Boolean formulae ( ): formulae preconditions effect conditions arbitraryboolean formulae.extensions also combined. use combinations letters refer multipleextensions. instance, refers formalism extended literals precondition lists,`s refers formalism allowing incomplete state specifications conditional effects,%QB , finally, refers general planning formalism introduced Section 2.1.Example 4 consider planning instance Example 3, becomes quickly obviousinstance expressed using . initial state specification incomplete,operator d{_zac contains conditional effects negative literals effect conditions. However,need general Boolean formulae express instance.278fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMS`{>>>{`Q``Figure 1: Planning formalisms partially ordered syntactic restrictionsFigure 1 displays partial order propositional planning formalisms defined way.sequel say specialization , written , iff identicaldiagram depicting partial order.Comparing set planning formalisms one Backstrom (1995) analyzed, one notices despite small differences presentation planning formalisms:common propositional strips (CPS),propositional strips negative goals (PSN),ground Tweak (GT).2.3 Computational Complexity Planning -Familyone would expect planning much easier planning %s , turnscase, provided one takes computational complexity perspective.analyzing computational complexity planning different formalisms, consider,usual, problem deciding whether exists plan given instancethe plan existenceproblem (PLANEX). use prefix referring planning formalism considerexistence problem particular planning formalism.Theorem 3-PLANEX PSPACE-complete%QB .5. consider planning formalisms identical SAS formalism (Backstrom & Nebel, 1995), sinceallow multi-valued state variables.279fiN EBELProof. PSPACE-hardness -PLANEX follows result Bylander (1994, Corollary 3.2).Membership %Qs -PLANEX PSPACE follows could, step step, guesssequence operators, verifying step operator application leads legal followstate specification last operator application leads state specification entailsgoal specification. step, verification carried polynomial space. reasonconditions definition verified polynomially many callsNP-oracle. Therefore, %Qs decided non-deterministic machine polynomial space,hence member PSPACE.follows plan existence problem formalisms expressiveness%s including formalismsis PSPACE-complete.3. Expressiveness Compilability Planning FormalismsAlthough difference computational complexity formalismsQB -family, might nevertheless difference concisely planning domains plansexpressed. order investigate question, introduce notion compiling planning formalisms.3.1 Compiling Planning Formalismsmentioned Introduction, consider planning formalism expressive anotherformalism planning domains plans formulated formalism concisely expressible. formalize intuition making use call compilation schemes,solution preserving mappings polynomially sized results domain structuresdomain structures. restrict size result compilation scheme,require bounds computational resources compilation. fact, measuringexpressibility, irrelevant whether mapping polynomial-time computable, exponential-timecomputable, even non-recursive. least, seems idea notion expressivepower discussed similar contexts (Baader, 1990; Erol et al., 1996; Gogic et al., 1995; Cadoliet al., 1996). want use compilation schemes practice, reasonablyefficient, course. However, want prove one formalism strictly expressiveanother one, prove compilation scheme regardless manycomputational resources compilation scheme might use.far, compilation schemes restrict size domain structures. However, measuring expressive power, size generated plans also play role. Backstrom'sESP-reductions (1995), plan size must identical. Similarly, translationproposed Gazen Knoblock (1997) seems implicit prerequisite planlength target formalism almost same. comparing expressivenessdifferent planning formalisms, might, however, prepared accept growth planstarget formalism. instance, may accept additional constant number operators,may even satisfied plan target formalism linearly polynomially larger.leads schematic picture compilation schemes displayed Figure 2.Although Figure 2 gives good picture compilation framework, completelyaccurate. First all, compilation scheme may introduce auxiliary propositional atomsused control execution newly introduced operators. atoms likelyinitial value may appear goal specification planning instances target280fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSPlanningcompilationGBPlanningFigure 2: compilation frameworkformalism. assume compilation scheme takes care adds literalsinitial state goal specifications.Additionally, translations initial state goal specifications may necessary.want compile formalism permits literals preconditions goals one requires atoms, trivial translations necessary. Similarly, want compile formalismpermits us use partial state specification formalism requires complete state specifications, translation initial state specification necessary. However, state translationfunctions limited. depend set symbols sourceformalism, context-independent, i.e., translation literal state specificationdepend whole specification, efficiently computable.compilation framework theoretical tool measure expressiveness, has,course, practical relevance. Let us assume reasonably fast planning systemplanning formalism want add new feature resulting formalism .come efficient compilation scheme , means easily integratenew featureeither using compilation scheme modifying planning algorithmminimally. compilation scheme exists, probably would problems integrating feature. Finally, computationally expensive compilation schemes exist, interestingsituation. case, off-line compilation costs may high. However, since compileddomain structure used different initial goal state specifications, high off-line costsmay compensated efficiency gain resulting using planning algorithm.turns, however, situation arise analyzing compilability %Qs formalisms. Either identify polynomial-time compilation scheme able provecompilation scheme exists.6. means compilation schemes planning formalisms similar knowledge compilations (Cadoli &Donini, 1997), fixed part computational problem domain structure variable part consistsinitial state goal specifications. main difference knowledge compilation frameworkalso take (size the) result account. words compile function problems instead decisionproblems.281fiN EBEL3.2 Compilation SchemesAssume tuple functions M)N>>ffe ffe>QffN&ffifft5P -instances follows:ffz>Pinduce function-instances)()N@>>ffe H< $ffi\ffe>s%<zs$fft9.PgAfollowing three conditions satisfied, call compilation schemeiff exists plan1. exists plan;2. state-translation functions z modular, i.e.,D0) , functions (for ?)eff ) satisfy:~) </ ,,$ffD(()$ ffDfiff H< $ ffDff ffpolynomial-time computable;3. size results >\ffe , > polynomial size arguments.Condition (1) states function induced compilation scheme solutionpreserving. Condition (2) states requirements on-line state-translation functions. resultfunctions computable element-wise, provided state specification consistent. Considering fact functions depend original set symbolsstate specification, requirement seem restrictive. Since state-translationfunctions on-line functions, also require result efficiently computable.Finally, condition (3) formalizes idea compilation. compilation muchimportant result concisely represented, i.e., polynomial space, compilation process fast. Nevertheless, also interested efficient compilation schemes. saypolynomial-time compilation scheme >\ffe , > polynomial-time computablefunctions.addition resource requirements compilation process, distinguish different compilation schemes according effects size plans solvingproperty every planinstance target formalism. compilation schemesolving instance exists plan solving 00 @0000 300positive integer constant , compilation scheme preserving plan size exactly (up additive00 300 positive integer constants, compilationconstants). 00 @00Kscheme preserving plan size linearly, 00 00-i00 300lff{00 00 polynomial - ,compilation scheme preserving plan size polynomially. generally, say planning formalism compilable formalism (in polynomial time, preserving plan size exactly,linearly, polynomially), exists compilation scheme appropriate properties.writecase compilable compilation done polynomial time. super-script , , - depending whether scheme preserves plan sizeexactly, linearly plan, polynomially, respectively.easy see, notions compilability introduced reflexive transitive.7. Although hard imagine modular state-translation function polynomial time computable,pathological function could, e.g., output translations exponential size encoding symbols.282fiC OMPILABILITYProposition 4 relationsE XPRESSIVE P OWER P LANNING F ORMALISMStransitive reflexive.Furthermore, obvious moving upwards diagram displayed Figure 1,always polynomial-time compilation scheme preserving plan size exactly. v denotesprojection -th argument function returns always empty set, genericcompilation scheme moving upwards partial order )ON ffeffeff ff P .Proposition 5 J ,.4. Compilability Preserving Plan Size ExactlyProposition 5 leads question whether exist compilation schemesimplied specialization relation. Proposition 5 Proposition 4,find compilation schemes every pair formalisms. suffices prove compilable, order arrive conclusion formalisms compilableformalisms .preview results section given Figure 3. establish two equivalenceclasses members class compilable preserving plan size exactly.two equivalence classes called - -class, symbols ,naming respective largest elements.{>>{Q>`Figure 3: Equivalence classes planning formalisms created polynomial-time compilationschemes preserving plan size exactly283fiN EBEL4.1 Planning Formalisms without Conditional Effects Boolean FormulaeFirst, show formalisms analyzed Backstrom (1995), namely, , ,polynomial-time compilable preserving plan size exactly. fact, fourth classadded set, namely, ` , lies .words, using notion compilability, get equivalence classBackstrom's ESP-reductions. closer look proofs Backstrom's (1995) paper reveals surprising ESP-reductions used could reformulatedcompilation schemes. Since used quite different notation, nevertheless prove claimfirst principles.key idea compiling planning formalisms literals formalisms allow atomsconsider - 4- different atoms new formalism. purpose, introduce!!, "? set negativecopy . Further,J),+ -0 -=/6 , i.e., disjoint!literal 4- replaced - , i.e.,#?)"!!+.-//I0 -3=76fi<=+ -3J0:4-53;60) ffotherwise.!Using < new set atoms, one translate state specifications preconditions easily. postconditions make sure intended semantics taken care of, i.e.,!whenever - added, - must deleted vice versa.Finally, deal problem partial state specifications. However,problem effects unconditional preconditions contain atoms.case, safely assume atoms unknown truth-value false without changingoutcome application operator. Let $&%*fi denote completion respect, i.e.,fi*),+>4-90 -3/ff-376fi<?XA$&%Using function, transform partial state specification complete specificationwithout changing outcome, i.e., get plans.Theorem 6exactly., ` ,, polynomial-time compilable preserving plan sizeProof. Since ^`H Z, follows Propositions 4 5showorder prove claim.Let )N&ffifft5P -instance )N$fftP . translate operator Loperator!L)~N'" pre LBff(" post LQ<5)"5 post LQ.PAset operators denotedN>>ffe ffe>Qff ffz{P follows:>))>s)$ffi\)zs$fft9)!. define compilation scheme!!$N < ffPffffff$&% +* ! ' "?\ff"==A284W)fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSsatisfies conditions (2) (3), functions computed polyThe scheme obviouslynomial time, -instance.. obviousLet!$ff$DffiLB.(), $ffD(ff LBA!) .N -L ffgAgAgA:ff/L- P denote sequence operators corresponding sequence operators)~NL ffgAgAgA:iff L>vP . Using induction plan length, easy show!plan iff plan ,i.e., condition (1) compilation schemes also satisfied. means, fact compilationLetscheme. Further, since plan size change, compilation scheme preserves plan sizeexactly. Finally, functions computed time polynomial arguments,polynomial-time compilation scheme.One view result matter whether, expressivity point view,allow atoms literals matter whether complete partial statespecificationprovided propositional formulae conditional effects allowed.4.2 Planning Formalisms Conditional Effects without Boolean FormulaeInterestingly, view spelled generalizes case conditional effects allowed. Also case matter whether atoms also literals allowedwhether partial complete state specifications. proving that, however, twoadditional complications. Firstly, one must compile conditional effects partial state specifications conditional effects complete state specifications. problem$D8ff post LB. definition function must tested. Seccondition $Dff post LQ.)ondly, compiling formalism literals formalism allows atoms only,condition M$Dff post LB.0) definition must taken care of. reason,prove result two steps.first step, show compiled . problem specifyingcompilation scheme execution operator L partial state specification leads$Dff post LB. .illegal state $Dff post LB.)considering running example (Ex. 1), things quite obvious. state specification contain literal negation literal mentioned effectcondition, illegal state specification results. example, state specification neither contain r]rsm &rsrsm , result executing d{_bac . general case, however,things less straightforward effect literals produced one conditionalrule effect condition consist one literal.Assuming without loss generality (using polynomial transformation) effectssingleton sets, check following condition. Either one conditional effectseffect literal activatedi.e., effect condition entailed partial stateconditional effects effect literal blocked, i.e., effect conditioncontains literal inconsistent state specification. true, original operator$Dff post LQ. , otherwise resulting state specification inconsistent.satisfies $Dff post LB.)example, consider following operator:L )N$fft+Q+.-ff8[]6 V +>4-H6Bfft+10ff2v6 V +>4-H6Q6>PA285fiN EBELapplication operator satisfies $Dff post LB.)1.2.-08[ true state specification,$Dff post LB.iff either2 true state specification,3. one - *[ false one 0 2 false.cases, get M$D8ff post LB.)$Dff post LB. result illegal state. ordertest condition formalism complete states introduce four new sets atoms:))+.- 0 -3=6Bff.+ - 0@-3/6Bff43 ) .+ -53=0@-3/6Bff6) +1 7X0 8 th conditional effect L 6BAatom - true either - 4- part original partial state specification. atom- set true operator one conditional effects adds - - appeareffect operator. atom 5- 3 set true operator one conditional effects deletes- 4- appear effect operator. Finally, atoms form 7 addedaction 8 th conditional effect th operator blocked effect condition. Usingnew atoms, could translate operatorN$ff8+H+.-`ffi[>ff-ff8[]6 V V +.-4 ff-53*ff4-6Bff+ 0 ff 2 ff 0ff V 2`6 +.- ff- 3 ff4-H6Bff1+.-`ff4-6 V +1 6Bff+\[ ffi[]6 V 1+ 6Bff+ 04ff9 0H6 V 1+ 6Bff1+ 2]V ff9 2v6 1+ ` 6Bff1V <5 3 6 I+.- 3 6Bff+ ;:X 7 (0 < )Z1.6Q6>PALet = eff>8] function returns - 5- 3 , - 4- , respectively, effect 8 thconditional effect th operator. Assuming atoms fi set according6intended semantics previous operator deleted atoms 7=<W435< ,>L ! )following test operator checks whether original operator would led inconsistent result:test)ffA@+>9 7ff9=8 iff>8]e6 V 0( 7?6BDCWhenever 9 7 , means 8 th conditional effect th operator (which mustpreviously executed operator) blocked. addition effect conditional$Dff post LB.effect activated, i.e., 9=8 iff>8] true, would $Dff post LB.)original formalism. reason, force illegal state. Conversely, either 7 true8 false one 8 , = eff>8] true, would $Dff post LQ.) $D8ff post LB.original formalism need force illegal state.!could force, using extra literals, operator L{ test operatorapplied. would result compilation scheme preserves plan size linearly. However,possible better that. key idea merge test operator th stepoperator step E .286fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSpolynomial-time compilable preserving plan size exactly.)N&ffifft9P -instance ~)N$fftP . Without loss generality,Proof. Letassume postconditions operators L 1 following form:V F ffgAgAgA:ffi : HGV F :z6Bffpost L ),+\F77I .Lemma 7First, introduce number new sets symbols pairwise disjoint disjoint:)JI )))I3366+.-.+ -KI+.-+.- I3+.- 3)))0@-=/6Bff0z-3/6Bff0z-3/6Bff0z-3/6Bff0z-3/6Bff+1KI 7 0 8+1 7 0 8th conditional effect L6Bffth conditional effect L 6BA, denotes set primed literals, i.e., )+.- 0i-^Z76<given set literals2, i.e., C]Mv=)+>4-`*0"$4- W76 function CLl denotes successor function modulo6MN\DOQPSRUT . Further, functions =WV )ff: shall functions V JV <9JV3#V [Bfi post L ffV / V[7= V eff>8])[ V3 / V3 ( 7 V 8[B post L>Alet postVLpostVlet block VL)ff: defined followsL *)+\/<?'(2 VV .+ -ff-`ff- V6 0@-3/ff\ V V %- post L e6&<+\/<?'(2 >+ 4-ff-`ff- V36 0 -3Wff\" - post L>$e6Bff)ff: definedblock VL ()+Q+\[ffi[{ 6 V V V 7 0Q 7 VXVXF F 7{ post L ff\$*[Q75 7>6&<+Q+>8[ffi[ 6 WV 7 0 77\& post L ffi[3 7>6Bfflet testV definedZZZ6 ZV\[ 6BA),+Q+>D 7 V\[ ff9= V\[ iff>8]e6 V 0] 7 V\[6V .Further, let , , fresh symbols appearing <3 <=JV <3JV 3 <define pair compiled operators L V ( )ff: ) corresponding original operator L W :L V )ON pre L H<=+\ V 6Bff postV V L %< blockV ZL %< testV <+> V +>*/ V]ff^ `ffi V\[b6Q67<+> V V <9 V3 6 J+1= V iff>8]e6Q6X<+>+ K:;1V 7.6Q6&<Z V\[ V]0 < Z V\[ )6V+><5( 3 <5 Z V\[ 6BA{C1 V287fiN EBELpair compiled operators achieves intended effects keeps track fully knownatoms using postV , checks conditional effects blocked using block V , tests whether$Dff post LB. usingexecution previous operator satisfied condition $Dff post LB.)testV , setup bookkeeping atoms next step. Using atoms /V , enforcedexecuting testing merged parallelizing test step execution step 9 .order check execution last step, need extra checking step:L V )ON +\ V B6 ff test V <=+> V +\`ff8 V Q6 6>PAspecify compilation scheme> )follows:66N\9<fi9<<=+\`ffi ffi{g6Bff<5 <5 3 <93 <<_+\L ffiL 0tL 1?6fi<=+\L ffiL 6>Pff66+>^4ffi/IBff8 6&<9(JI 5< ( <5JI3 <3( 3 <5 I(<9 ff+\v6Bff$&% \%< $&% +.- 0 -3=fft+.-ff"-H6Jff?)]6>ff/A)>B )$ffi\ )$fft5)scheme obviously satisfies conditions (2), i.e., state-translation functions modular,polynomially sized results. Further, functions(3), i.e., compilation functionscomputed polynomial time, -instance.. obviousAssume< *)X!],z.$ffD(H<9>bff\NL P.5ff1< ff.$ff;!t]$Dff\NL> P..5ff1provided ;!t]$Dff\NL P.0) . case ;!t]$Dff\NL P.F0 ) , either ;!t], $ffD(&< ff\NL P.0 )M$Dff post L .) $Dff post L . . latter case, application operator;!t], $ffD(ff\NL P. leads inconsistent state conditional effects test ,part postconditions operators applicable state. Additionally, truerelation zb$ffX!]$Dff\NL>.ffiL 7 . ;!],zi$ffD(H<?>.ff\NL ffiL]7 P. .Let )~NL ffgAgAgA\ffiL P denote sequence operators corresponding sequence operators)~NL ffgAgAgA:ffiL>v P . Using induction plan length, easily showniffplanFurther, since plan solving instanceexists plana` LAVplan.must LAV last operator, followsiff exists plan.follows immediately polynomial-time compilation schemepreserving plan size exactly, proves claim.proved compiled preserving plan size exactly, seems worthnoting result depends semantics chosen executing conditional operators partial state specifications. example, use alternative semantics deletes literals7 $Dff post LB.ZM$Dff post LB. provided $Dff post LB. consistent, exists probably compilation scheme preserves plan size linearly. use semantics288fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSresulting state specification legal application state-transformation functions leadstheory represented set literals, seems unlikely exists schemepreserves plan size polynomially. reason pessimistic conjecturesemantics appears coNP-hard determine whether state specification resultingapplying B -operator legal.second step showing partial state specifications literals compiled away,show compile . key idea proof proof!Theorem 6. replace negative literal 4- new atom - . order detect inconsistenciesintroduced conditional effects, add postcondition conditional effects form!+.-ff -%6 V . Further, check last operator plan introduce inconsistencies,force application checking operator contains conditional effects.Lemma 8polynomial-time compilablepreserving plan size exactly.Proof. Let) N&ffifft9P -instance ) N$fftP . Sincepostconditions operators L1 following form:bpost LB()+\VXb gff AgAgA\ffi:VXb:-instance,6Bff^7Bff 7.!proof Theorem 6, shall disjoint copy , "9 set atoms!c LQ following setnegative literal 4- replaced atom - . let postpostc LB()+?^7 V '"b"fi<5)"57b\(097 VXb {7 fi7post LBe6BAFurther, let cons set conditional effectscons),+Q+.-ff -H! 6 V 0@-3/6Bff!let atom appearing , let LL! )~N'"let!O)+L! 0tL1?6pre LBffpostc LBH< cons <=+>V 9v6>Pff, let operator L>L>)~N$ff cons <=+> V ` 6>PAspecify compilation scheme>Q ))>s )$ffi\ )zs$fft5)follows:!!N$< <3+\v6Bff =< +\L>Q6>Pff+>^ `6Bff+\v6Bff"?(<5d"58ff"=/Asatisfies conditions (2) (3), functions computed polyThe scheme obviouslynomial time, -instance.289fiN EBELAssume. obvious!$ff$DffiLB.), $ffD(ff LBffprovided $DffiLB0)!!!case $DffiLB;0 ) , either , $ffD(ff LB;0 ) +.-ff -%6, $ffD(ff LB -=/ .!latter case, application operator , $ffDff LB leads inconsistent stateconditionaleffects cons, part postconditions.!Let )N.-L ffgAgAgA:ff/L- P denote sequence operators corresponding sequence operators)~NL ffgAgAgA:ffiL>vP . Using induction plan length, easily showniffplanFurther, since plan solving instanceexists plan!`L>plan.must L> last operator, followsiff exists planfollows polynomial-time compilation schemeproves claim..preserving plan size exactly,result is, course, dependent semantics formalisms deal$Dff post LB. .complete state specifications, hence always M$Dff post LB.()Theorem 9size exactly., , `,polynomial-time compilable preserving planProof. B follows Lemma 8, Lemma 7 Proposition 4. Using Propositions 45 fact `B` , claim follows.5. Limits Compilation Preserving Plan Size Linearlyinteresting question is, course, whether compilation schemes preserving plansize exactly identified far. turns out, case. provepairs formalisms identified compilation scheme preservingplan size exactly, compilation scheme impossible even allow linear increaseplan size. pairs formalisms even able prove polynomial increaseplan size would help establishing compilation scheme. results are, however,fhe assumption. previewconditional based assumption slightly stronger eF)gresults section given Table 1. symbol means exists compilationscheme first formalism specialization second one. cases,specify separation give theorem number result.5.1 Conditional Effects Cannot Compiled AwayFirst all, prove conditional effects cannot compiled away. deeper reasonconditional effects, one independently number things parallel,impossible formalisms without conditional effects. consider, example,operator d{_zac Example 1, clear `propagates' truth value rsr]m klu{hhQj]k y>k _ba nx hBjsk klu>h nx , respectivelyprovided state specification satisfies precondition.290fiC OMPILABILITY%sQs%s%]%Qj%Cor. 15Cor. 12Cor. 15Cor. 15jiji4iTheo. 11Cor. 12Cor. 12jiji4iCor. 12Cor. 12Cor. 19)ji%Cor. 15Theo. 14)Cor. 15Theo. 18)Cor. 19%QCor. 15)kE XPRESSIVE P OWER P LANNING F ORMALISMSCor. 15)Cor. 19)Table 1: Separation Resultsobviously possible come set exponentially many operatorsthing one step. However, unclear less exponentially many operators.fact, show impossible.order illustrate point, let us generalize example. start setpropositional atoms &) +.- ffgAgAgA:ff-``6 disjoint copy set: ml ) +.- l 0- ,fiv6 .Further,fi, Dnl shall denote corresponding set literalsConsider following)l, i.e.,)+.- l 0@- =Dfi6&<=+>4- l 0g4- /D6BAldomain structure:fi<5 l ffB) @*N$fft+.- V - l ff"- V 4- l 0@- Wfiv6>P ff) N$ ff8 PAconstruction follows pairs fft5 consistent complete set1l , instance )N `ffifft5P one-step plan. Conversely, pairsfifft9 gff l 1 l , exist solution.Trying define domain structure polynomially sized 00 00 propertyseems impossible, even allow -step plans. However, trying prove this, turnsadditional condition state-translation function needed.say state-translation functions local iff state specificationsff5 )$ ff Dpff qff s$ ffDff ()A291fiN EBELlocality additional condition state-translation functions could easily proveconditional effects cannot compiled away. Instead show, however,possible derive weaker condition definition compilation schemesenough prove impossibility result. weaker condition quasi-locality state-translationfunctions relative given set symbols , turn based notion universalFliterals. literal called universal literal given state-translation functions iff onefollowing conditions satisfied:1. -3= :F2. -3= :F3. -3= :F4. -3= :F5. -3= :F6. -3= :Fp +.-6Bfft+.-H6> ,p +.-6Bfft+>4-H6> ,p +.-6BffeB ,pzs +.-H6Bfft+.-H6> ,pzs +.-H6Bfft+>4-H6> ,pzs +.-H6BffeB .Let r denote set universal literals. define quasi-locality state-translation functions relative set propositional atoms induced set universal literals r follows.D0) pairs ff ff3 ) ,Wr$ ff Dff sffzs$ ffDfiffwords, non-local literals quasi-local state-translation functions universal literals.Lemma 10 given compilation scheme F)N>{ffe00ut^exists set atomsffe>Qff ffz>P natural numberquasi-local .,R;v function result union results possibleProof. Lettranslations literal returned state-translation functions, i.e.,gl- *) +.-H6Bfft+.-H6>< +.-H6Bfft+>"-H6>< +.-H6BffeB.<+.-H6Bfft+.-6>< zB +.-H6Bfft+>4-H6>< zs +.-H6BffeBASet5)wr) . choose infinite subsetwweither1. -3xw , finitely many atoms [xw" ,glor infinite subset w exist,2.wuniversal literalFfirset r?`)r<=+F6Aff4g[Q.sUr ),.Fr mustNote infinite subset w must exist. reason literaloccur infinitely many atoms w could find infinite subsetsatisfying condition (1). single atom six possible ways generateF, must exist infinite subset literal occurs either v +.-H6Bfft+.-H6> ,Fv +.-H6Bfft+>4-H6> , zv +.-H6BffeB (for ?)iff ) subset universal literal.292fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSpick subset satisfying first condition, choose finite subsetdesired cardinality state-translation functions quasi-local respectr .Otherwise repeat selection process w r condition (1) satisfied.selection process repeated finitely often otherwise atoms tl- infinite result, impossible state-translation functionspolynomial-time computable therefore finite results.demonstrates always exists set propositional atoms statetranslation functions quasi-local. However, might able effectively determineset.Using result, finally able prove non-existence compilation schemescompiling conditional effects away preserving plan size linearly.Theorem 11cannot compiled % preserving plan size linearly.Proof. Assume contradiction exists compilation scheme %Q preSserving plan size linearly, compiles domain structure defined %domain structure> ) )ON$ tff PALemma 10 assume set atoms chosen translationfunctions quasi-local set.Let us consider initial state specifications consistent completecontain positive negative literals:Obviously,following form{TK/R+>fi4ff(fiK6BAstate specifications. assumption,%QinstanceN ff5 .$ ffi\H<?>b ff5 $ ffi l H<9 .P-step plan. Since |i0 0 different -step plans, number polynomial size , plan used different initial statesprovided sufficientlylarge.Suppose plan used pairs fft ff\t fft , result :) $fi4ffi H<9 K) $ ffi l %<9) $fi4ffi H<9 K) zs$fi`ffi l %<9>s KSince ), must differ least one atom, say - . Without loss generalityassume -33 4-35 . Since successful plan modular,followsX!] ffeh}zs +.- l B6 fft+.- l >6}~293fiN EBELliterals zs +.-Kl;6Bfft+.-KlX6> may added operators none literals+.- l 6Bfft+.- l 6> deleted operator without reestablishing literal anotheroperator deletion. contains operators unconditional effects, addsdeletes literals regardless initial state.FLet us assume exists literal zs +.-Kl;6Bfft+.-KlX6> added .Fimplies 3t distinguish three cases:= K , conclude F 3t .F2. p +.K- lX6BffeB , also implies F 3t .F3.state z. +\[s6Bffi& [^) K- l ,+Q+\[s6BfftF +>*[s6Bffe]6 . assumedFtranslation functions quasi-local , must universal literal. universalFcontain positive negative, possible initial statesFliterals well literal elements . universal , presentFreason. Further, added validplan , must also part .Fwords, literals+.K- lX6Bfft+.K- lX6> added already .conclude;!] ffe }y +.- l 6Bfft+.- l 6>A1.Flet))$ +.- l 6Bffi l J+>4- l 6>H<9 ff< +.- l 6Bfft+.- l 6>) zs$ 4ffi l +>"- l 67<=+.- l 6>H<9>B vAz modular, clear } therefore ;!t]t ffefi} .achieves well +.-KlX6Bfft+.-KlX6> , follows (again modular), achievesalso .Since N ffi ffi l Z+>4K- l;67</+.E- l76>P plan, planN ffi fft P . fact plan instance implies cannot compilationscheme, desired contradiction.Using Propositions 4 5 well Theorem 9, result generalized follows (seealso Table 1).Corollary 12 %Qs , %] ,preserving plan size linearly.cannot compiled %Q formalism specializing %answers question whether space efficient compilation schemesone proposed Gazen Knoblock (1997) possible. Even assuming unboundedcomputational resources compilation process, space efficient compilation schemeimpossibleprovided compilation preserve plan size linearly. allow polynomially larger plans, efficient compilation schemes possible (see Section 6).8. result demonstrates choice semantics important. interpret conditional effectssequentially Brewka Hertzberg (1993) do, exists straightforward compilation scheme preservingplan size exactly.294fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMS5.2 Non-Uniform Complexity Classesnext section make use so-called non-uniform complexity classes, definedusing advice-taking machines, order prove impossibility compilation scheme.advice-taking Turing machine Turing machine advice oracle, (not necessarily recursive) function positive integers bit strings. input , machine loadsbit string i00 "00 continues usual. Note oracle derives bit stringlength input contents input. advice said polynomialoracle string polynomially bounded instance size. Further, complexity class defined terms resource-bounded machines, e.g., P NP, J]pBnm (also called non-uniformX) class problems decided machines resource boundspolynomial advice.advice oracle, class P/poly appears much powerful P. HowY es]pBnmever, seems unlikely P/poly contains NP. fact, one prove fJeimplies certain relationships uniform complexity classes believed unlikely. stating result, first introduce polynomial hierarchy.Let X class decision problems. e+ denotes class decision problemsdecided polynomial time deterministic Turing machine allowed useprocedurea so-called oraclefor deciding problem , whereby executing procedurecost constant time. Similarly, fJe denotes class decision problemspolynomial time usingnondeterministic Turing-machine solvesinstancesdefined follows:oracle ~p . Based notions, sets , ,)V )V ))VVVV))effe fffffJeygn fJeThus, )gfhe)ygnfJe . set classes defined way called polynomialhierarchy, denoted PH. NoteF)Ve^V)VV)VV"Ae sekDhave,ffffV . classes, unknown whetherVVVVVinclusions classes proper. However, strongly believed case,i.e., hierarchy truly infinite.Based firm belief polynomial hierarchy proper, mentioned questioneE]pBnm answered. shown fJe es]pBnm would implywhether fJepolynomial hierarchy collapses second level (Karp & Lipton, 1982), i.e., )yg nfhes] pB.nThis,however, considered quite unlikely. Further, shown fJeygnfJe fJ es ]pBnm implies polynomial hierarchy collapses third level (Yap, 1983),i.e.,) , considered unlikely. use result provingpairs formalisms unlikely one formalism compiledone.9. super-script used distinguish sets analogous sets Kleene hierarchy.295fiN EBEL5.3 Expressive Power Partial State Specifications Boolean Formulaecases considered far, operators partial state specifications could compiledoperators complete state specifications, i.e., partial state specifications add expressiveness. longer true, however, also allow arbitrary boolean formulaepreconditions effect conditions. case, decide coNP-complete problemwhether formula tautology deciding whether one-step plan exists. Asking, example,Q -instance N$fft+]Nff KPe6Bffefft+\v6>P plan equivalent asking whether tautology.Let one-step plan existence problem (1-PLANEX) PLANEX problem restrictedplans size one. evident %QB -1-PLANEX %Q -1-PLANEXcoNP-hard. Let - fixed polynomial, polynomial step plan-existence problem(- -PLANEX) PLANEX problem restricted plans length bounded - ,size planning instance. easy see, problem NP formalisms except%QB %Q . reason guessing sequence operators state specificationspolynomial size, one verify step polynomial time precondition satisfiedcurrent state specification produces next state specification. Sincepolynomially many steps, overall verification takes polynomial time.Proposition 13 -- -PLANEX solved polynomial time nondeterministic Turing machine formalisms different %s %Q .fact % -1-PLANEX coNP-hard and, e.g., %s -p-PLANEX NP, followsalmost immediately polynomial-time compilation scheme Q ]ygnfJe ). However, even allow unboundedpreserves plan length polynomially (if fhe)computational resources compilation process, proof technique first used Kautz Sel).man (1992) used show compilation scheme cannot exist (provided )Theorem 14%Qcannot compiled %] preserving plan size polynomially, unless).Proof. Let propositional formula size conjunctive normal form three literals perclause. first step, construct % domain structure * size polynomialfollowing properties. Unsatisfiability arbitrary 3CNF formula size equivalent-step plan existence % - -PLANEX instance N8`ffi1 fft+\`6>P , 1 computedpolynomial time .Given set atoms, denoted , define set clauses set containingclauses three literals built using atoms. size | , i.e.,polynomial . Let ? set new atoms -E . corresponding one-to-one clauses. Further, letB)@9 F F F M- . (0\+ F ff F ff F 6Mconstruct % domain structure 8)~N$fivfftKP formulae size follows:&))<?<=+\v6Bff+]N +> v6Bfft+\v6>Pe6BA296fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSLet function determines 3CNF formulae , atoms ? correspondclauses formula , i.e.,*()+.- . 0\+ F ff F ff F 63(6BANow, initial state particular formula size computed follows:)h *H<1 ?M~*.H<=+>^v6BA1construction, follows exists one-step plan N$&`fft`ffi1"fft+\v6>Piffunsatisfiable.Let us assume exists compilation scheme %Q %] preserving plansize polynomially. Further, let us assume % domain structure 8 compiled %]domain structure * )N$fi fft P . Using compiled domain structure, constructfollowing advice-taking Turing machine.input formula size , load advice N8 ffe $&`fftffe>]$&`fft.P .advice polynomial 8 polynomial size compilation scheme generatespolynomially larger domain structures. polynomial-time function 1computed polynomial time, compute).$ ffi % <9>b$ fftpolynomial time. Also goal specification) $ fft+\v6><9 $ fftcomputed polynomial time. Finally, decide - -PLANEX problem resulting%] -instance N ffi fft P . Proposition 13 know done polynomial timenondeterministic Turing machine.deciding - -PLANEX N ffi fft P equivalent deciding -PLANEXN84ffi1"fft+\v6>P , turn equivalent deciding unsatisfiability , followsdecide coNP-complete problem nondeterministic, polynomial advice-taking Turing machinefhe5]pBnm . Using Yap's (1983) result,polynomial time. follows ygnfJeclaim follows.Using Proposition 4 Proposition 5, result generalizes follows (see also Table 1).Corollary 15 %Qs % cannotcompiled planning formalisms preserving.plan size polynomially, unless )restrict form formulae, however, may able devise compilation schemes%Q to, e.g., % . Reconsidering proof last theorem, turns essentialuse negation CNF formula precondition. restrict CNF formulaepreconditions, seems possible move partial complete state descriptions using ideassimilar ones used proof Lemma 7.However, compilation scheme work %s . reason condition$Dff post LB.) $Dff post LB. definition function . condition satisfied, result operator inconsistent. condition could easily employed reduceunsatisfiability CNF formulae 1-step plan existence, enables us use technique proof theorem.297fiN EBEL5.4 Circuit Complexitynext impossibility result need notions boolean circuits families circuits.boolean circuit directed, acyclic graph )'*ff , nodes called gates.gate 2type K!,2K+>fiff fiff&ff:ffi6<F+1 ff ffgAgAgA26 . gates K!,2K+sffiff ff ffgAgAgA26 in-degree zero, gates K!,2K+>6 in-degree one,gates K!,2K+>fiff 6 in-degree two. gates except one least one outgoingedge. gate outgoing edge called output gate. gates incoming edgescalled input gates. depth circuit length longest path input gateoutput gate. size circuit number gates circuit.Given value assignment variables +1output gate obvious way. example,gate circuit shown Figure 4.ff ffgAgAgA6 , circuit computes value)~ ) get value 1 outputFigure 4: Example boolean circuitInstead using circuits computing boolean functions, also use acceptingwords length +\ff:>6 . word )H AgAgA4+\ff:>6 interpreted valueassignment input variables ffgAgAgA:ff4 circuit. word accepted iff output gatevalue 1 word. order deal words different length, need one circuitpossible length. family circuits infinite sequence )O' ff( ffgAgAgAw ,input variables. language accepted family circuits theI set words8 accepts .Usually, one considers so-called uniform families circuits, i.e., circuits generatedTuring machine Pk( -space bound. Sometimes, however, also non-uniform familiesinteresting. example, class languages accepted non-uniform families polynomiallysized circuits class P/poly introduced Section 5.2.Using restrictions size depth circuits, define new complexityclasses, uniform variants subsets P. One class importantfollowing class languages accepted uniform families circuits polynomial sizelogarithmic depth, named NC . Another class proves important us definedterms non-standard circuits, namely circuits gates unbounded fan-in. Insteadrestricting in-degree gate two maximum, allow unbounded in-degree.class languages accepted families polynomially sized circuits unbounded fan-inconstant depth called ACI .298fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSdefinition, follows almost immediately ACNC . Moreover,shown languages NC non-uniform variant ACI ,implies AC ) NC (Furst, Saxe, & Sipser, 1984).5.5 Boolean Formulae Cannot Compiled Conditional Effectsseen Section 5.3, Boolean formulae quite expressive used combination partial state specifications. However, state specifications complete?case, seems possible simulate evaluation CNF formulae using conditionaleffects. fact, possible compile polynomial-time, example, % preserving plansize linearly, provided formulae conjunctive normal form. operator wouldsplit two operators, one evaluates clauses formulae original operatorone combines evaluations takes appropriate actions, e.g., assertingprecondition satisfied. Sequencing pairs operators achieved introducingextra literals.say general case, however? trying simulate evaluationarbitrary logical formula using conditional effects, seems case need manyoperators nesting depth formula, means would need plans cannotbounded linearly longer original plans.use results sketched Section 5.4 separate % . order so, let usview domain structures fixed size plans machines accept languages. wordsconsisting bits, let8)ON$fi<=+\`6BfftvPAAssume atoms fi numbered 1 . wordconsisting bits couldencoded set literals,) +.- 0th bit6fi<=+>4- 0 th bit K6BAConversely, consistent state specification fi , let word th bit 1 iff- /D .say -bit word accepted one-step -step plan 8 iffexists one-step -step plan, respectively, instance)~N.N$fi<=\+ v6BfftKPffi <=+>^ v6Bfft\+ `6>PASimilarly families circuits, also define families domain structures, ) ffe ffgAgAgA .language accepted family one-step (or -step) plan set words acceptedusing domain structure 8 words length . Borrowing notion uniformity well,say family domain structures uniform generated Pk -space Turingmachine.Papadimitriou pointed languages accepted uniform polynomially-sizedboolean expressions identical NC (Papadimitriou, 1994, p. 386). easy see, family % domain structures nothing family boolean expressions, provided useone-step plans acceptance.Proposition 16 class languages accepted uniform families % domain structures usingone-step plan acceptance identical NC .299fiN EBELcloser look power -step plan acceptance familiesdomain structures is, turns less powerful NC . order show that, firstprove following lemma relates -step plans circuits gates unbounded fan-in.Lemma 17 Let F)~N$fftP domain structure, let, let -step plan. exists polynomially sized boolean circuit unbounded fan-in depth >;fiTplan N&ffifft5P iff circuit value 1 input .Proof. general structure circuit -step...plan displayed Figure 5.11..............11......... . .Figure 5: Circuit structure goal testing -stepplan7plan step (or level) 8 atom - , connection - . connections levelinput gates, i.e., - ) . goal test performed-gate checks goalstrue level , case )+.- ff4- ff- 6 . Further, using -gate, checkedinconsistency generated executing plan.plan step 8 , must computed whether precondition satisfiedresult conditional effects are. Figure 6 (a) displays precondition test precondition+.- ff- ff4- 6 . conjunction precondition literals true, V becomes true,connected -gate Figure 5.Without loss generality (using polynomial transformation), assume conditionalVGF . Whether effect F activated level 8 computed circuiteffects formV 4- .displayed Figure 6 (b), shows circuit +.- ff4- 6Finally, activated effects combined circuit shown Figure 6 (c). atoms - ,check whether - 4- activated, would set true. oneinputs -gate Figure 5. neither - 4- activated, value -level 8N determined value - level 8 . Otherwise value - level 87determined value -W , i.e., activation value positive effect - level 8 .depths circuits Figure 6 (b) (c) dominate depth circuit necessaryrepresent one plan step leading conclusion plan step represented using circuitdepth 7. Adding depth goal testing circuit, claim follows.lemma implies -step plan acceptance indeed less powerful % 1-step planacceptance, means compilation scheme % preserving plan size linearlyimpossible.300fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMS1z...(((a)z(b)z((c)Figure 6: Circuit structure precondition testing (a), conditional effects (b), computationeffects (c) operatorsTheorem 18, members-class.Proof. show %1 , Theorem 9 Proposition 4 claim follows.Assume contradiction . Let ) ffe ffgAgAgA uniform familydomain structures `)8 ffe8 ffgAgAgAw domain structures generated compilationscheme preserves plan size linearly. Lemma 17 know domainstructure )~N$ fft P given goal generate polynomially sized, unbounded fanin circuit depth >T tests whether particular -step plan achieves goal. orderdecide -step plan existence, must test |i0 0 ie different plans, polynomial size8 compilation scheme. plan, generate one test circuit,adding another -gate decide -step plan existence using circuit depth >W sizepolynomial size 8 . Further, since state-translation functions modular, resultsfixed computed using additional level gates. Since Proposition 16languages NC accepted uniform families % domain structures using one-step planacceptance, assumption % implies accept language NC (possiblynon-uniform) ACI circuits, impossible result Furst colleagues (1984).Using Propositions 4 5 again, generalize theorem follows.Corollary 19]cannot compiledBpreserving plan size linearly.6. Compilability Preserving Plan Size Polynomiallyshown previous section, compilation schemes induced Propositions 45 ones identified Section 4 allow compilation schemes preserving plan size exactly. pairs formalisms able rule compilation schemeseven301fiN EBELallow linear growth resulting plans. Nevertheless, might still chancecompilation schemes preserving plan size polynomially. shown %Qs %Q cannotcompiled formalisms even plan grow polynomially, may still ablefind compilation schemes preserving plan size polynomially %Qs /% pairremaining formalisms.preview results section given Figure 7. seen, able`{>>>{B>Figure 7: Equivalence classes planning formalisms created polynomial-time compilationschemes preserving plan size polynomially. Compilation schemes constructedsection indicated dashed linesestablish compilation schemes preserving plan size polynomially pairs formalismsproved impossibility compilation schemes.6.1 Compiling Conditional Effects Away Partial State Specificationsfirst compilation scheme develop one Qs Q . before, assumeconditional effects singleton effect sets. Further, since use arbitrary booleanformulae effect conditions Qs , assume one rule effect literal.Using simple polynomial transformation, arbitrary sets operators brought form.$Dff post LB. considerably,simplifies checking condition $Dff post LB.)one rule activate particular literal.302fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSorder simulate parallel behavior conditional effects, breakindividual operators executed sequentially. means conditional effectoperator introduce two new operators. One simulates successful application rule,one simulates blocking situation rule. least one operators mustexecuted conditional effect original operator. something forceadditional literals added control execution operators. leadssequence operators length bounded number conditional effects originaloperator.want simulate parallel behavior sequence unconditional operators, effectsunconditional operators directly influence state description, effectdeferred operators corresponding set conditional effectsexecuted. reason, use sequence copying operators copy activatedeffects state description conditional operators executed. copyingoperators also used check set activated effects consistent.Theorem 20QscompiledQpolynomial time preserving plan size polynomially.Proof. Assume ) N$fftP %QB source domain structure assume further, withoutloss generality (using polynomial transformation), operators formL )~N pre L fft+{ VXF ffgAgAgA\ffe : VGF :fiz6>PffFFF73 , 7 , 7) X) .VLet 3 disjoint copies , used record active effects conditionalanother disjoint copy, used record active effecteffects, letlcopied yet. Further, letJ),+.-4*0zLW?6 new set atoms corresponding one-to-one6operators let set symbols corresponding one-to-one conditional effects, i.e.,6V F 7 & post L>@ffiL{81?6BA),+14 7 0 7 XFinally, let fresh atom appearing <= </ 3 </<= signals copyinglactive effects state specification progress. set symbols compileddomain structure)F<9 <53<5l<9m<6</+\\6BAoperator L , compilation scheme introduces number new operators.first operator introduce one checks whether conditional effects previousoperators executed, copying progress precondition satisfied.case, execution conditional effects operator started:L pre)~N pre L{$H<56<=+>8{6Bff(+.- b 6&<3(7?<5(43<5<3(lPAoperator enables conditional effect operators. activated effects, introducefollowing operators:Lk 7 ~) N +.-4b45 7Q6Bff*+1 7>6&</+.- ffl3030z-?) F 7>67<=+.- 3 ff l0:4-) F 76>PAfiN EBELwords, effect condition entailed, activated positive negative effect wellfact rule tried recorded.Since one effect literal conditional effect, conditional effectblocked negation effect condition entailed state specification.blocked conditional effects introduce following operators:L 7 ~) N +.- b5( 7 6Bff(+1` 7 6>PAorder check conditional effects tried (activating corresponding effectactivating conditional effect blocked), following operator used:L) N +.-4bb6fi<=+1 760 7 VGF 7\post Le6Bff(+\{6&<=+>"-4bb6>PAoperator enables copying activated effects state specification, achievedfollowing set operators atom -3/ :LLL)3))N +\>ff- ff4- 3 ff- l B6 fft+.-ff"- l >6 PffN +\>ff4-"ff-53(ff- l B6 fft+>4-ff4- l >6 PffN +\>ff- ff- 3 ff- l 6BffPAFinally, need operator checks possible effects copied. operatoralso starts execution cycle enabling execution another precondition operator:Li ~) N +\{67<3(lff8+>*\6>PAUsing definitions, specify set compiled operators:pre)+\L ffiL 0L 1?6&< VHF+\L 7 0L ?ff\ 7 VXF 7\fi post L e67<+\L 7 0L 19ff\ 77\& post L e6&<+\L ffiL 3 ffiL 0 -3W6fi<+\LAit6BABased that, specify compilation scheme )ON>{ffe>zffe ff zff P follows:>> ) N$ fft P6>b ) (7?<5(3<9 l <3( <5 </+>8{6Bff>s ) (m<3+>*\6Bff$ffi\ ) ffzs$fft9) =Ascheme obviously satisfies conditions (2) (3)compilation schemesfunctions computed polynomial time. Further, Q -instance -instance.Let legal %s state specification let ) $DffiL operatorclear D0) , exists sequenceL>X . discussion,7 L 7 followedpreoperators consisting L , followed operators form Loperator L , followed turn operators L , followed finally operator L ,);!t]$D5<9 ffe ffF304fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSConversely, D8%0 ) , exist plan transforms$D5<9>bffiL prelegal state specification contains 8 4-4b .Usinginduction plan length, follows arguments exists planiff exists plan every plan 00 @00v00 300<= , <maximum number conditional effects operators . Hencepolynomial-time compilation scheme preserving plan size polynomially.immediate consequence theorem %s %Q form equivalence classrespect compilability preserving plan size polynomially.Corollary 21polynomially.%Qs%polynomial-time compilable preserving plan sizeFurther, know Corollary 15 class cannot become larger.case compiling , however, result depends semantics chosenexecuting conditional effects partial state specifications. use alternative semantics resulting state specification legal application state-transformationfunctions leads theory represented set literals, seems likely existsanother scheme preserves plan size polynomially. However, use alternative semanticsdeletes literals ; $Dff post LB.(I$Dff post LB. $Dff post LB consistent,appears unlikely able identify compilation scheme preserves plansize polynomially.6.2 Compiling Conditional Effects Away Complete State Specificationsnext compilation scheme compiles %] % . Since deal complete state$D8ff post LB. ,specification, take care condition M$D8ff post LB.M)always true complete states. makes compilation scheme somewhat simpler. Sinceallow general boolean formulae, scheme becomes little bit difficult.general, however, compilation scheme specify similar one givenproof Theorem 20.Theorem 22 %] compiledserving plan size polynomially.%compiledpolynomial time pre-Proof. proof Theorem 20, assume ) N$fftP (%]domain structure. Further, assume operators formV F :b6>PffL )~N pre L fft+ U VXF ffgAgAgA\ff U : XFU %] structure U 77M 7) sourcestructure. meansassume effects unique conditional effect.addition, assume set symbols compiled domain structure proofTheorem 20:)F<9 <53<5305l<9m<6</+\\6BAfiN EBELpreoperator L , introduce operators L , L , LTheorem 20.addition, following operators needed:Lk 7 )L 7e : ),L3,LN +.-4bb6&< U 7>ff*+1 7Q6fi<=+.- ff - l 0@-?) F 76&<=+.- 3 ff N +.-4bb6&</+>* 7e :0t 7e : U 76Bff*+1 7>6>PA, Llproof0:4-) F 76>Pffcompiled set operators contains operators compilation schemeidentical scheme presented proof Theorem 20. means significantdifference compilation scheme presented proof Theorem 20 operator schemeL 7e : tests rule whether contains effect condition blocks rule. Sincecomplete state specifications, every conditional effect either activated blocked,7 's used record execution conditional effect tried.Using similar arguments proof Theorem 20, follows compilationscheme indeed scheme leads claim made theorem.follows equivalent respect formalismsequivalent respect . two sets could merged one equivalence class,provided able prove that, e.g., % compiled .6.3 Compiling Boolean Formulae AwaySection 5.5 showed impossible compile boolean formulae conditional effectsplans allowed grow linearly. However, also sketched already idea compilationscheme preserves plan size polynomially. show compile booleanformulae , expressively equivalent basic STRIPS, i.e., compile booleanformulae away completely.Theorem 23%polynomial-time compilablepreserving plan size polynomially.Proof. Assume F)ON$fftP domain structure. assume without loss generality= (i.e.,operators L 1 form L )ON ffi P ,one formula precondition instead set formulae).Let two new sets atoms corresponding one-to-one sub-formulae occurring preconditions operators . new atoms denoted [\ [>sub-formula . Atoms form [{ used record truth-value sub-formulacomputed atoms form [\ used store computed truth-value.operator L )~N ffi P , target operator set following operator:L )ON +\[Q.ffi[ B 6Bffi <5( PAset operators generated way denoted .Further, atom -3/ , introduce following two operators:LL3))N +.-6Bff*+\[ ffi[ 6>PffN +>"-H6Bff*+\[ ff8[ 6>PAset operators generated way denoted306.fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSsub-formula occurring preconditionsoperators introduced:LL )L 3 )sub-formulae J)LLL 3form )pfollowingN +\[ ffi[ ffi[\ ffi[\ B6 fft+\[ ffi[\6>PffN +\[ ff*[\ 6Bfft+\[ ff8\[ H6>PffN +\[ ff *[\ 6Bfft+\[ ff8\[ H6>PA)3, following operators introduced:) N +\[ ffi\[ 6Bfft+\[ ffi\[ 6>Pff) N +\[ ffi\[ 6Bfft+\[ ffi\[ 6>Pff) N +\[ ffi[ ff8\[ ff81[ 6Bfft+\[ ff*\[ H6>PAFinally, J)^ , following operators:LL3))N +\[ ff 8[ 6Bfft+\[ iff [\6>PffN +\[ iff [ 6Bfft+\[ ff8\[ 6>PAset operators generated sub-formulae denotedspecify compilation scheme :>))>B)$ffi\)s$fft9).$N <9m9<5 ff8 </ </U*Pffffffff/Aconstruction obvious functions polynomial-time computable,induced function reduction,state-translation functions modular,every plan source planning instance exists plan 00 @0000 300 ,<^\ , < maximum number sub-formulae preconditions .that, claim follows.might question whether compiling boolean formulae away could doneefficiently. Using result boolean expressions evaluated circuits logarithmicdepth, indeed possible. However, satisfied resultcompilation scheme preserving plan size polynomially all. result together Theorem 22 settles question compilation schemes preserving plan size polynomially pairsformalisms.Corollary 24 formalismspreserving plan size polynomially.307%]polynomial-time compilablefiN EBEL6.4 Parallel Execution Models Feasibility Compilation Schemes Preserving PlanSize Polynomiallycompilation schemes preserve plan size exactly linearly seem immediate use,polynomial growth plan appears little practical interest. Considering practicalexperience planning algorithms roughly characterized property manysteps plan without getting caught combinatorial explosion factnumber significantly smaller 100, polynomial growth seem make much sense.take GRAPHPLAN (Blum & Furst, 1997) consideration againthe planning systemmotivated investigation first placeit turns system allows parallelexecution actions. Although parallel execution might seem add power planningsystem considerably, affect results all. sequential plan solve planninginstance steps, parallel plan also need least actions. Nevertheless, although sizeplan (measured number operations) might same, number time steps mayconsiderably smallerwhich might allow efficient generation plan.look compilation scheme compiles conditional effects away, seems caselarge number generated actions could executed parallelin particular actionssimulate conditional effects.However, semantics parallel execution GRAPHPLAN quite restrictive. one actionadds deletes atom second action adds deletes one action deletes atomsecond action precondition, two actions cannot executed parallelGRAPHPLAN . restriction, seems impossible compile conditional effects awaypreserving number time steps plan. However, compilation scheme preservesnumber time steps linearly seems possible. Instead compilation scheme,approaches far either used exponential translation (Gazen & Knoblock, 1997) modifiedGRAPHPLAN -algorithm order handle conditional effects (Anderson et al., 1998; Koehler et al.,1997; Kambhampati et al., 1997). modifications involve changes semantics parallelexecution well changes search procedure. implementations comparedstraightforward translation Gazen Knoblock (1997) used, would also interestingcompare compilation scheme based ideas spelled Theorem 22base line.7. Summary DiscussionMotivated recent approaches extend GRAPHPLAN algorithm (Blum & Furst, 1997)deal expressive planning formalisms (Anderson et al., 1998; Gazen & Knoblock, 1997;Kambhampati et al., 1997; Koehler et al., 1997), asked term expressive power couldmean context. One reasonable intuition seems term expressive power refersconcisely domain structures corresponding plans expressed. Basedintuition inspired recent approaches area knowledge compilation (Gogic et al., 1995;Cadoli et al., 1996; Cadoli & Donini, 1997), introduced notion compilability ordermeasure relative expressiveness planning formalisms. basic idea compilationscheme transform domain structure, i.e., symbol set operators,initial state goal specification transformedmodulo small changes necessarytechnical reasons. Further, distinguish compilation schemes according whether plantarget formalism size (up additive constant), size bounded linearly308fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSsize plan source formalism, size bounded polynomially original planninginstance original plan.Although compilability framework appears straightforward intuitive toolmeasuring expressiveness planning formalisms, possible come alternativemeasures. Backstrom (1995), instance, proposed use ESP-reductions, polynomialmany-one reductions planning problems preserve plan size exactly. However, requiringtransformation polynomial-time computable seems overly restrictive.particular, want prove one formalism expressive another one, betterproven exists compilation scheme regardless much computational resourcescompilation process may need. Furthermore, appear severe technical problemsusing Backstrom's (1995) framework proving negative results. hand,positive results reported Backstrom achievable compilation frameworktransformations used fact compilation schemes. Taking together, appearscase compilation framework superior intuitive technical point view.Another approach judging expressiveness planning formalisms proposedErol colleagues (1994, 1996). measure expressiveness planning formalisms according set plans planning instance have. approach contrasts hierarchical tasknetwork planning nicely STRIPS-planning, help us making distinctionsformalisms -family.compilability framework mainly theoretical tool measure concisely domainstructures plans expressed. However, also appears good measuredifficult planning becomes new language feature added. Polynomial-time compilationschemes preserve plan size linearly indicate easy integrate featurecompiled away. One either use compilation scheme mimic compilation schemeextending planning algorithm. polynomial-time compilation scheme leadingpolynomial growth plan possible, indication adding new featurerequires probably significant extension planning algorithm. even compilationscheme preserving plan size polynomially ruled out, probably seriousproblem integrating new feature.Using framework, analyzed large family planning formalisms ranging basicSTRIPS formalisms conditional effects, boolean formulae, incomplete state specifications. surprising result analysis able come completeclassification. pair formalisms, either able construct polynomial-timecompilation scheme required size bound resulting plans could prove compilation schemes impossibleeven computational resources compilation processunbounded.particular, showed formalisms considered paper:incomplete state specifications literals preconditions compiled basic STRIPSpreserving plan size exactly,incomplete state specifications literals preconditions effect conditions compiled away preserving plan size exactly, already conditional effects,compilation schemes preserving plan size linearly except impliedspecialization relationship described above.309fiN EBELallow polynomial growth plans target formalism, formalismscontaining incomplete state specifications boolean formulae compilable other. Incomplete state specifications together boolean formulae, however, seem add significantlyexpressiveness planning formalism, since cannot compiled away evenallowing polynomial growth plan unbounded resources compilation process.noted, however, results hold use semanticsconditional effects partial state specifications spelled Section 2.1. semantics,may get slightly different results concerning compilability conditional effects partialstates.One question one may ask happens consider formalisms boolean formulaesyntactically restricted. indicated various places paper, restricted formulae,CNF DNF formulae, sometimes easily compiled away. However, alsocases impossible. example, shown CNF formulae cannot compiledbasic STRIPS preserving plan size linearly (Nebel, 1999), confirms Backstrom's (1995)conjecture CNF-formulae preconditions add expressive power basic STRIPS.Another question reasonable restrictions compilation scheme are. particular,one may want know whether non-modular state-translation functions could lead powerfulcompilation schemes. First all, requiring state-translation functions modular seemsquite weak considering fact compilation scheme concerneddomain structure initial state goal specification transformedall. Secondly, considering fact state-translation functions depend operatorset, complicated functions seem useless. technical point view, needmodularity order prove conditional effects boolean formulae cannot compiled awaypreserving plan size linearly. conditional effects, modularity similar condition seemscrucial. case boolean formulae, could weaken condition pointrequire state-translation functions computable circuits constant depthorsomething similar. case, additional freedom one gets non-modular state-translationfunctions seem help functions take operatorsaccount. Nevertheless, seems interesting theoretical problem prove powerfulstate-translation functions add power compilation schemes.Although paper mainly theoretical, inspired recent approaches extendGRAPHPLAN algorithm handle powerful planning formalisms containing conditionaleffects. So, answers give open problems field planning algorithmdesign? First all, Gazen Knoblock's (1997) approach compiling conditional effects awayoptimal want allow plan growth constant factor. Secondly,approaches (Anderson et al., 1998; Kambhampati et al., 1997; Koehler et al., 1997)modify GRAPHPLAN algorithm using strategy similar polynomial-time compilationscheme preserving plan size polynomially. reason, approaches comparedpure compilation approach using ideas compilation scheme developedproof Theorem 22 base line. Thirdly, allowing unrestricted boolean formulae addslevel expressivity cannot compiled away linear growth plansize. fact, approaches one Anderson colleagues (1998) simply expandformulae DNF accepting exponential blow-up. Again, cannot better plansize preserved linearly. Fourthly, want add partial state specifications topgeneral boolean formulae, would amount increase expressivity much larger310fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSadding conditional effects general formulae basic STRIPS, case waycompile away even allow polynomial plan growth.Finally, one may wonder results apply planning approaches based translating (bounded) planning problems propositional logic SATPLAN (Kautz & Selman, 1996)BLACKBOX (Kautz & Selman, 1998). Since entire analysis relative expressivenessplanning formalisms uses assumption compile one planning formalism anotherplanning formalism, results tell us anything size representations switchanother formalism. particular, seems possible find encoding (bounded) planningproblems conditional operators propositional logic concise encodingunconditional operators. advice results give concise encodingfound first translating conditional actions unconditional actions using standard encoding unconditional actions (Kautz, McAllester, & Selman, 1996) generate booleanformulae. However, addressing problem determining conciseness representationcontext appears interesting relevant topic future research.Acknowledgmentsresearch reported paper started partly carried author enjoyedvisitor AI department University New South Wales. Many thanks go NormanFoo, Maurice Pagnucco, Abhaya Nayak rest AI department discussionscappuccinos. would also like thank Birgitt Jenner Jacobo Toran clarificationsconcerning circuit complexity.Appendix A: Symbol IndexSymbolExplanationcardinality setsize instancesymbol used conditional effectssyntactic specialization relationcompilability relation restrictionboolean constant denoting falsity, also denotingillegal state specification273 boolean constant denoting truth295 advice functionLff\Ll275, 275active effects operator state state specificationAC298 complexity class298 boolean circuit298 family boolean circuitscoNP272 complexity classcoNP/poly 295 non-uniform coNPclosing set literals w.r.t.$&%;(Ll 284277 plan, i.e., sequence operators295 complexity class polynomial hierarchy295 instance problem0L0V 00 L]00Page292277274279282273311fiN EBEL>\ffe ffe>UF ffeff;ff bE\GHLlNC"!@#%NPNP/polyL- ffi[ff0ff24ffL\ffPP/polyPH277282282282277274273287273273274298273272295274276277274275272295295273>LlPLANEX279post274pre274PSPACE272277295276Lff\Ll;!t]Lff\Ll 277C274274278278%278278`278;278283283'(273273initial state descriptioncompilation scheme ( )~N>{ffe ffe>Qff ffz>P )transformation induced compilation schemecomponents compilation schemegoal planning taskset boolean formulaeboolean formulaeliteralsets literalsboolean formulae use atomsset models theorycomplexity classnegative literals set literalscomplexity classnon-uniform NPoperator ( )~N pre ff post P )set operatorsset finite sequences operatorspropositional atomspotentially active effects operatorgiven state specificationcomplexity classnon-uniform Ppolynomial hierarchypositive literals set literalsplan existence problempostconditions operatorpreconditions operatorcomplexity classplanning instance ( )ON&ffifft9P )complexity class polynomial hierarchymaps state specification operator new stateextension Lff\Ll plansstate (or truth assignment)state specificationSTRIPS planning formalismSTRIPS literals preconditionsSTRIPS boolean formulae preconditionsSTRIPS incomplete state descriptionsSTRIPS conditional effectsSTRIPS combinations extensionsequivalence classes inducedequivalence classes inducedpropositional atoms used set literalscountably infinite set propositional atoms312fiC OMPILABILITYrffzX,273273295282292298295272277299E XPRESSIVE P OWER P LANNING F ORMALISMSfinite subsetset literals overscomplexity class polynomial hierarchystate-translation functions compilation schemeuniversal literalsword +\ff:>6tcomplexity classplanning formalismsdomain structure ( )ON$fftP )family domain structuresReferencesAnderson, C. R., Smith, D. E., & Weld, D. S. (1998). Conditional effects Graphplan. Proceedings 4th International Conference Artificial Intelligence Planning Systems (AIPS98), pp. 4453. AAAI Press, Menlo Park.Baader, F. (1990). formal definition expressive power knowledge representation languages.Proceedings 9th European Conference Artificial Intelligence (ECAI-90) Stockholm, Sweden. Pitman.Backstrom, C. (1995). Expressive equivalence planning formalisms. Artificial Intelligence, 76(12), 1734.Backstrom, C., & Nebel, B. (1995). Complexity results SAS planning. Computational Intelligence, 11(4), 625655.Blum, A. L., & Furst, M. L. (1997). Fast planning planning graph analysis. ArtificialIntelligence, 90(1-2), 279298.Brewka, G., & Hertzberg, J. (1993). things worlds: formalizing actionsplans.. Journal Logic Computation, 3(5), 517532.Bylander, T. (1994). computational complexity propositional STRIPS planning. ArtificialIntelligence, 69(12), 165204.Cadoli, M., & Donini, F. M. (1997). survey knowledge compilation. AI Communications,10(3,4), 137150.Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1996). Comparing space efficiencypropositional knowledge representation formalism. Aiello, L. C., Doyle, J., & Shapiro,S. (Eds.), Principles Knowledge Representation Reasoning: Proceedings 5thInternational Conference (KR-96), pp. 364373 Cambridge, MA. Morgan Kaufmann.Erol, K., Hendler, J. A., & Nau, D. S. (1994). HTN planning: Complexity expressivity.Proceedings 12th National Conference American Association Artificial Intelligence (AAAI-94), pp. 11231129 Seattle, WA. MIT Press.313fiN EBELErol, K., Hendler, J. A., & Nau, D. S. (1996). Complexity results hierarchical task-networkplanning. Annals Mathematics Artificial Intelligence, 18, 6993.Fikes, R. E., & Nilsson, N. (1971). STRIPS: new approach application theorem provingproblem solving. Artificial Intelligence, 2, 189208.Furst, M., Saxe, J. B., & Sipser, M. (1984). Parity, circuits, polynomial-time hierarchy.Mathematical Systems Theory, 17(1), 1327.Garey, M. R., & Johnson, D. S. (1979). Computers IntractabilityA Guide TheoryNP-Completeness. Freeman, San Francisco, CA.Gazen, B. C., & Knoblock, C. (1997). Combining expressiveness UCPOP efficiencyGraphplan. Steel, S., & Alami, R. (Eds.), Recent Advances AI Planning. 4th EuropeanConference Planning (ECP'97), Vol. 1348 Lecture Notes Artificial Intelligence, pp.221233 Toulouse, France. Springer-Verlag.Gogic, G., Kautz, H. A., Papadimitriou, C. H., & Selman, B. (1995). comparative linguisticsknowledge representation. Proceedings 14th International Joint ConferenceArtificial Intelligence (IJCAI-95), pp. 862869 Montreal, Canada. Morgan Kaufmann.Kambhampati, S., Parker, E., & Lambrecht, E. (1997). Understanding extending Graphplan.Steel, S., & Alami, R. (Eds.), Recent Advances AI Planning. 4th European ConferencePlanning (ECP'97), Vol. 1348 Lecture Notes Artificial Intelligence, pp. 260272Toulouse, France. Springer-Verlag.Karp, R. M., & Lipton, R. J. (1982).Mathematique, 28, 191210.Turing machines take advice.L' EnsignementKautz, H. A., McAllester, D. A., & Selman, B. (1996). Encoding plans propositional logic.Aiello, L. C., Doyle, J., & Shapiro, S. (Eds.), Principles Knowledge RepresentationReasoning: Proceedings 5th International Conference (KR-96), pp. 374385 Cambridge, MA. Morgan Kaufmann.Kautz, H. A., & Selman, B. (1992). Forming concepts fast inference.. Proceedings10th National Conference American Association Artificial Intelligence (AAAI-92),pp. 786793 San Jose, CA. MIT Press.Kautz, H. A., & Selman, B. (1996). Pushing envelope: Planning, propositional logic,stochastic search. Proceedings 13th National Conference American Association Artificial Intelligence (AAAI-96), pp. 11941201. MIT Press.Kautz, H. A., & Selman, B. (1998). BLACKBOX: new approach application theoremproving problem solving. Working notes AIPS'98 Workshop PlanningCombinatorial Search Pittsburgh, PA.Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphsADL subset. Steel, S., & Alami, R. (Eds.), Recent Advances AI Planning. 4th EuropeanConference Planning (ECP'97), Vol. 1348 Lecture Notes Artificial Intelligence, pp.273285 Toulouse, France. Springer-Verlag.314fiC OMPILABILITYE XPRESSIVE P OWER P LANNING F ORMALISMSLifschitz, V. (1986). semantics STRIPS. Georgeff, M. P., & Lansky, A. (Eds.), Reasoning Actions Plans: Proceedings 1986 Workshop, pp. 19 Timberline, OR.Morgan Kaufmann.Nebel, B. (1999). expressive power disjunctive preconditions?. Biundo, S., & Fox,M. (Eds.), Recent Advances AI Planning. 5th European Conference Planning (ECP'99)Durham, UK. Springer-Verlag. appear.Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley, Reading, MA.Pednault, E. P. (1989). ADL: Exploring middle ground STRIPS situationcalculus. Brachman, R., Levesque, H. J., & Reiter, R. (Eds.), Principles KnowledgeRepresentation Reasoning: Proceedings 1st International Conference (KR-89),pp. 324331 Toronto, ON. Morgan Kaufmann.Yap, C. K. (1983). consequences non-uniform conditions uniform classes. TheoreticalComputer Science, 26, 287300.315fiJournal Artificial Intelligence Research 12 (2000) 199-217Submitted 12/1999; published 5/2000Complexity Reasoning Cardinality RestrictionsNominals Expressive Description LogicsStephan Tobiestobies@informatik.rwth-aachen.deLuFG Theoretical Computer Science, RWTH AachenAhornstr. 55, 52074 Aachen, GermanyAbstractstudy complexity combination Description Logics ALCQ ALCQIterminological formalism based cardinality restrictions concepts. combinations naturally embedded C 2 , two variable fragment predicate logiccounting quantifiers, yields decidability NExpTime. show approach leads optimal solution ALCQI , ALCQI cardinality restrictionscomplexity C 2 (NExpTime-complete). contrast, show ALCQ,problem solved ExpTime. result obtained reduction reasoning cardinality restrictions reasoning (in general weaker) terminologicalformalism general axioms ALCQ extended nominals . Using reduction,show that, extension ALCQI nominals, reasoning general axiomsNExpTime-complete problem. Finally, sharpen result show pureconcept satisfiability ALCQI nominals NExpTime-complete. Without nominals,problem known PSpace-complete.1. IntroductionDescription Logics (DLs) used knowledge based systems represent reason taxonomical knowledge application domain semantically well-definedmanner (Woods & Schmolze, 1992). allow definition complex concepts (i.e.,classes, unary predicates) roles (binary predicates) built atomic onesapplication given set constructors. example, following concept describesparents least two daughters:Human u (Male Female) u (> 2 hasChild Female) u 8hasChild:Humanconcept example DL ALCQ. ALCQ extends \standard" DL ALC(Schmidt-Schau & Smolka, 1991) qualifying number restrictions, i.e., concepts restricting number individuals related via given role (here hasChild), insteadallowing existential universal restrictions like ALC. ALCQ syntactic variant(multi-)modal logic K graded modalities (Fine, 1972). paper studyproblems DLs ALCQ ALCQI . latter extends ALCQ possibilityrefer inverse role relations. Additionally, paper encounter nominals,i.e., concepts referring single elements domain. extensions ALCQ ALCQInominals denoted ALCQO ALCQIO. example concept ALCQIOdescribes common children individuals ALICE BOB living ALICE BOB9hasChild 1 :ALICE u 9hasChild 1:BOB u 9livesWith:(ALICE BOB):c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiTobiesConcept SatisfiabilityGCIsCardinality Restr.ALCQ-c.-c.ExpTime-c.PSpaceExpTimeALCQOopen-c.ExpTime-c.ExpTimeALCQI-c.-c.NExpTime-c.PSpaceExpTimeALCQIONExpTime-c.NExpTime-c.NExpTime-c.Figure 1: Complexity results established paper (shown bold face)Here, parent relationship expressed inverse hasChild relationship.terminological component (TBox) allows organisation defined conceptsroles forms knowledge base DL system. TBoxes studied DLs range weakones allowing acyclic introduction abbreviations complex concepts,TBoxes capable expressing various forms general axioms, cardinality restrictionsexpress restrictions number elements extension concept mayhave. following, give examples three types assertions.following TBox introduces parent abbreviation human leastone child whose children human, toddler young human, busy parentparent least two children toddlers.Parent = Human u (> 1 hasChild) u 8hasChild:HumanToddler = Human u VeryYoungBusyParent = Parent u (> 2 hasChild Toddler)next expressions general axioms stating males females disjoint (?denotes empty concept) males females coincide humansexactly two human parents.Female u Male = ?Female Male = Human u (= 2 hasChild 1 Human)Finally, following expression cardinality restriction expressingtwo earliest ancestors:( 2 (Human u (6 0 hasChild 1 Human)))Cardinality restriction first introduced Baader et al. (1996) terminologicalformalism DL ALCQ; see, express general axioms henceexpressive terminological formalisms considered paper.key component DL system reasoning component provides services likesubsumption consistency tests knowledge stored TBox. subsumptiontest, example, could infer previous definitions Male Femalesubsumed Human BusyParent subsumed Parent busy parent mustleast one child. exist sound complete algorithms reasoning largenumber DLs different TBox formalisms optimal respect knownworst-case complexity problems (see Donini et al., 1996, overview).200fiThe Complexity Cardinality Restrictions Nominalspaper establish number new complexity results DLs cardinalityrestrictions nominals. Figure 1 summarises new complexity bounds establishedpaper. problems complete respective complexity class. paperorganised follows.giving basic definitions Section 2, show consistency TBoxescardinality restrictions ALCQI NExpTime-complete problem (Section 3). Membership NExpTime shown translation satisfiability problem C 2 (Pacholski et al., 1997)1 , two variable fragment first order predicate logic augmentedcounting quantifiers. matching lower bound established reductionNExpTime-complete bounded domino problem.Section 4, show reasoning cardinality restrictions reducedreasoning (weaker) formalism general axioms presence nominals.yields interesting complexity results reasoning cardinality restrictionsnominals. Using result (De Giacomo, 1995), reduction shows consistency TBoxes cardinality restrictions ALCQ ExpTime. improvesresult (Baader et al., 1996), shown problem solvedNExpTime. Moreover, show DL number restrictions, inverse roles,nominals reasoning problems become NExpTime-hard, solves open problem(De Giacomo, 1995). combination particular interest application DLsarea reasoning database schemata (Calvanese et al., 1998a, 1998b).2. LogicALCQIDefinition 2.1 Let NC set atomic concept names NR set atomic rolenames. Concepts ALCQI built inductively using following rules:2 NC concepts, and, C , C1 , C2 concepts, also:C; C1 u C2;(> n C );concepts, n 2 N = R = R 1 R 2 NR .cardinality restriction ALCQI expression form (> n C ) (6 n C )C concept n 2 N ; ALCQI -TCBox 2 finite set cardinality restrictions.semantics concepts defined relative interpretation = (I ; ),consists domain valuation (I ) maps concept name subsetAI role name R subset RI . valuation inductivelyextended arbitrary concepts using following rules, ]M denotes cardinalityset :(:C )I := n C ;(C1 u C2 )I := C1I \ C2I ;(> n R C )I := fa 2 j ]fb 2 j (a; b) 2 RI ^ b 2 C g ng;(> n R 1 C )I := fa 2 j ]fb 2 j (b; a) 2 RI ^ b 2 C g ng:1. NExpTime-result valid assume unary coding numbers counting quantifiers.standard assumption made results concerning complexity DLs.2. subscripted \C" indicates TBox consists cardinality restrictions201fiTobiesx(A):= Ax2 NCx(:C ):= : x(C )x(C1 u C2) := x(C1 ) ^ x(C2 )x(> n R C ) := 9ny:(Rxy ^ (C ))x(> n R 1 C ) := 9ny:(Ryx ^ (C ))(C ):= x(C )[xny; ynx](./ n C ):= 9./nx: x(C ) ./ 2 f>; 6g(T ):= Vf (./ n C ) j (./ n C ) 2 gFigure 2: translation ALCQI C 2interpretation satisfies cardinality restriction (> n C ) iff ](C ) n, satisfies (6 n C ) iff ](C ) n. satisfies TCBox iff satisfies cardinality restrictions; case, called model denote fact j= . TCBoxmodel called consistent.ALCQ denote fragment ALCQI contain inverse rolesR 1.Using constructors Definition 2.1, use (8 C ) abbreviationcardinality restriction (6 0 :C ) introduce following abbreviations concepts:C1 C2 = :(:C1 u :C2 )(6 n C ) = :(> (n + 1) C )C1 ! C2 = :C1 C2(= n C ) = (6 n C ) u (> n C )9S:C = (> 1 C )> = :A 2 NC8S:C = (6 0 :C )TBoxes consisting cardinality restrictions first studied (Baader et al.,1996) DL ALCQ. Obviously, two concepts C; extension interpretation iff satisfies cardinality restriction (6 0 (C u :D) (:C u D)). Hence,cardinality restrictions express terminological axioms form C = D,satisfied interpretation iff C = DI . General axioms expressive TBoxformalisms usually studied DL context (De Giacomo & Lenzerini, 1996). One standard inference service DL systems satisfiability concept C respect TCBox, i.e., interpretation j= C 6= ;. TBox formalismbased cardinality restrictions easily reduced TBox consistency, obviously C satisfiable respect iff [ f(> 1 C )g consistent TCBox.reason, restrict attention TCBox consistency; standard inferencesconcept subsumption reduced consistency well.exist direct decision procedure ALCQI TCBox consistency.Nevertheless problem decided help well-known translationALCQI -TCBoxes C 2 (Borgida, 1996), given Figure 2. logic C 2 fragmentpredicate logic formulae may contain two variables, enrichedcounting quantifiers form 9`. translation yields satisfiable sentenceC 2 translated TCBox consistent. Since translation ALCQI C 2202fiThe Complexity Cardinality Restrictions Nominalsperformed linear time, NExpTime upper bound (Gradel et al., 1997; Pacholskiet al., 1997) satisfiability C 2 directly carries ALCQI -TCBox consistency:Lemma 2.2 Consistency ALCQI -TCBox decided NExpTime.Please note NExpTime-completeness result (Pacholski et al., 1997)valid assume unary coding numbers input; implies number n maystored logarithmic space k-ary representation consumes n unitsstorage. standard assumption made results concerning complexityDLs. come back issue Section 3.3.3. ConsistencyALCQI -TCBoxes NExpTime-completeshow NExpTime also lower bound complexity TCBox consistency,use bounded version domino problem. Domino problems (Wang, 1963; Berger,1966) successfully employed establish undecidability complexity resultsvarious description modal logics (Spaan, 1993; Baader & Sattler, 1999).3.1 Domino SystemsDefinition 3.1 n 2 N , let Zn denote set f0; : : : ; n 1g n denote additionmodulo n. domino system triple = (D; H; V ), finite set (of tiles)H; V relations expressing horizontal vertical compatibility constraintstiles. s; 2 N , let U (s; t) torus Zs Zt, let w = w0 : : : wn 1word length n (with n s). say tiles U (s; t) initial conditionw iff exists mapping : U (s; t) ! that, (x; ) 2 U (s; t),(x; y) = (x 1; y) = d0, (d; d0 ) 2 H (horizontal constraint);(x; y) = (x; 1) = d0 , (d; d0 ) 2 V (vertical constraint);(i; 0) = wi 0 < n (initial condition).Bounded domino systems capable expressing computational behaviourrestricted, so-called simple, Turing Machines (TM). restriction non-essentialfollowing sense: Every language accepted time (n) space (n) one-tape TMaccepted within time space bounds simple TM, long (n); (n)2n (Borger et al., 1997).Theorem 3.2 ((Borger et al., 1997), Theorem 6.1.2)Let simple TM input alphabet . exists domino system =(D; H; V ) linear time reduction takes input x 2 word w 2jxj = jwjaccepts x time t0 space s0, tiles U (s; t) initial condition ws0 + 2; t0 + 2;accept x, tile U (s; t) initial condition ws; 2.203fiTobiesCorollary 3.3domino system following NExpTime-hard problem:Given initial condition w = w0 : : : wn 1 length n. tile torusU (2n+1 ; 2n+1 ) initial condition w?Proof. Let (w.l.o.g. simple) non-deterministic TM time- (and hence space-)bound 2n deciding arbitrary NExpTime-complete language L(M ) alphabet .Let according domino system trans reduction Theorem 3.2.function trans linear reduction L(M ) problem above: v 2jvj = n, holds v 2 L(M ) iff accepts v time space 2jvj iff tilesU (2n+1 ; 2n+1 ) initial condition trans(v ).3.2 Defining Torus Exponential SizeSimilar proving undecidability reduction unbounded domino problems, defining infinite grids key problem, defining torus exponential size keyobtaining NExpTime-completeness proof reduction bounded domino problems.able apply Corollary 3.3 TCBox consistency ALCQI , must characterisetorus Z2 Z2 TCBox polynomial size. characterise torus, use2n concepts X0 ; : : : ; Xn 1 Y0 ; : : : ; Yn 1 , Xi (resp., Yi) codes ith bitbinary representation X-coordinate (resp., Y-coordinate) element a.interpretation element 2 , define pos(a)nnpos(a) := (xpos(a); ypos(a)) :=(nX1xi1nX2 ;=0 (=0yi2;62 XiI0; 62 YiI= 01;; ifotherwiseyi =1; otherwise :use well-known characterisation binary addition (e.g. (Borger et al., 1997))relate positions elements torus:Lemma 3.4 Let x; x0 natural numbers binary representationsxix=x0 x + 11nX=01xi2i1n^ k^(mod 2 ) iffnk^(=0 j =01 1n^ k_k(=0 j =0x0=1nX=0x0i2i :xj= 1) ! (xk = 1 $ x0k = 0)xj= 0) ! (xk = x0k ) ;empty conjunction disjunction interpreted true false, respectively.204fiThe Complexity Cardinality Restrictions NominalsDeast=Xkunn(=0 j =01 kG1knXj1=01k=0k=01k 1kn(8 9north:>);(8 (= 1 north 1 >));(> 1 C(2 1;2 1) );(8 Deast u Dnorth )nn:Xk uG G=nn=01knG1;2 1)1GC(2n=nGC(0;0)= (8 9east:>);(8 (= 1 east 1 >));(> 1 C(0;0) );(6 1 C(2 1;2 1) );GTnn:YkYk) ! ((Xk ! 8east::Xk ) u (:Xk ! 8east:Xk )) u( :Xj ) ! ((Xk ! 8east:Xk ) u (:Xk ! 8east::Xk )) u=0 j =01Gkn==0GDnorthk((Yk ! 8east:Yk ) u (:Yk ! 8east::Yk )):::Figure 3: TCBox defining torus exponential sizeTCBox Tn defined Figure 3. concept C(0;0) satisfied elementsdomain pos(a) = (0; 0) holds. C(2 1;2 1) similar concept, whoseinstances satisfy pos(a) = (2n 1; 2n 1).concept Dnorth similar Deast role north substituted eastvariables Xi Yi swapped. concept Deast (resp. Dnorth ) enforces that,along role east (resp. north), value xpos (resp. ypos) increases onevalue ypos (resp. xpos) unchanged. analogous formula Lemma 3.4.following lemma consequence definition pos Lemma 3.4.Lemma 3.5 Let = (I ; ) interpretation, Deast ; Dnorth defined Figure 3,a; b 2 .nimplies:(a; b) 2 eastI 2 Deastimplies:(a; b) 2 northI 2 Dnorthnxpos(b) xpos(a) + 1ypos(b) = ypos(a)xpos(b) = xpos(a)ypos(b) ypos(a) + 1(mod 2n)(mod 2n)TCBox Tn defines torus exponential size following sense:Lemma 3.6 Let Tn TCBox defined Figure 3. Let = (I ; ) modelTn .(I ; eastI ; northI ) = (U (2n ; 2n ); S1 ; S2 ) ;205fiTobiesU (2n; 2n ) torus Z2 Z2 S1; S2 horizontal vertical successorrelations torus.Proof. show function pos isomorphism U (2n ; 2n ). Injectivitypos shown induction \Manhattan distance" d(a) pos-value elementpos-value upper right corner.element 2 define d(a)d(a) = (2n 1 xpos(a)) + (2n 1 ypos(a)):Note pos(a) = pos(b) implies d(a) = d(b). Since j= (6 1 C(2 1;2 1) ),one element 2 d(a) = 0. Hence, exactly one elementpos(a) = (2n 1; 2n 1). assume elements a; b 2pos(a) = pos(b) d(a) = d(b) > 0. either xpos(a) < 2n 1 ypos(a) < 2n 1.W.l.o.g., assume xpos(a) < 2n 1. j= Tn, follows a; b 2 (9east:>)I . Leta1 ; b1 elements (a; a1 ) 2 eastI (b; b1 ) 2 eastI . Since d(a1 ) = d(b1 ) < d(a)pos(a1 ) = pos(b1), induction hypothesis yields a1 = b1 . Lemma 3.5 followsxpos(a1 ) xpos(b1 ) xpos(a) + 1 (mod 2n )ypos(a1 ) = ypos(b1 ) = ypos(a)also implies = b a1 2 (= 1 east 1:>)I f(a; a1 ); (b; a1 )g eastI . Hencepos injective.prove pos also surjective use similar technique. time, useinduction distance lower left corner. element (x; y) 2 U (2n ; 2n),define:d0 (x; ) = x + y:show induction that, (x; y) 2 U (2n ; 2n ), element 2pos(a) = (x; y). d0 (x; y) = 0, x = = 0. Since j= (> 1 C(0;0) ),element 2 pos(a) = (0; 0). consider (x; y) 2 U (2n ; 2n )d0 (x; ) > 0. Without loss generality assume x > 0 (if x = 0 > 0 must hold).Hence (x 1; y) 2 U (2n ; 2n ) d0 (x 1; y) < d0 (x; y). induction hypothesis,follows element 2 pos(a) = (x 1; y). mustelement a1 (a; a1 ) 2 eastI Lemma 3.5 implies pos(a1 ) = (x; y). Hencepos also surjective.Finally, pos indeed homomorphism immediate consequence Lemma 3.5.nnnninteresting note need inverse roles guarantee pos injective. achieved adding cardinality restriction (6 (2n 2n) >)Tn , injectivity pos follows surjectivity simple cardinalityconsiderations. course size cardinality restriction would polynomialn assume binary coding numbers. Also note made explicit usespecial expressive power cardinality restrictions stating that, model Tn,extension C(2 1;2 1) must one element. cannot expressedALCQI -TBox consisting terminological axioms.nn206fiThe Complexity Cardinality Restrictions Nominals3.3 Reducing Domino Problems TCBox ConsistencyLemma 3.6 proved, easy reduce bounded domino problemTCBox consistency. use standard reduction applied DL context,e.g., (Baader & Sattler, 1999).Lemma 3.7 Let = (D; V; H ) domino system. Let w = w0 : : : wn 1 2 .TCBox (n; D; w) that:(n; D; w) consistent iff tiles U (2n; 2n ) initial condition w.(n; D; w) computed time polynomial n.Proof. define (n; D; w) := Tn [ TD [ Tw , Tn defined Figure 3, TD capturesvertical horizontal compatibility constraints domino system D, Tw enforces initial condition. use atomic concept Cd tile 2 D. TD consistsfollowing cardinality restrictions:G:(Cd u Cd0 ));(8 Cd ); (8d2D d0 2DnfdgCd0))); (8G(d;d0 )2HGGd2D(Cd ! (8east:GG(8d2Dd2DG(Cd ! (8north:(d;d0 )2VCd0))):Tw consists cardinality restrictions(8 (C(0;0) ! Cw0 )); : : : ; (8 (C(n 1;0) ! Cw 1 );nwhere, x; y, C(x;y) concept satisfied element iff pos(a) = (x; y),defined similarly C(0;0) C(2 1;2 1) .definition (n; D; w) Lemma 3.6, follows model (n; D; w)immediately induces tiling U (2n ; 2n ) vice versa. Also, fixed domino systemD, (n; D; w) obviously polynomially computable.main result section immediate consequence Lemma 2.2, Lemma 3.7, Corollary 3.3:nnTheorem 3.8Consistency ALCQI -TCBoxes NExpTime-complete, even unary coding numbersused input.Recalling note proof Lemma 3.6, see argument alsoapplies ALCQ allow binary coding numbers.Corollary 3.9Consistency ALCQ-TCBoxes NExpTime-hard, binary coding used representnumbers cardinality restrictions.noted open problem decided NExpTime, binarycoding numbers used, since reduction C 2 yields decidability 2-NExpTime.207fiTobiesfollowing section, see that, unary coding numbers, deciding consistency ALCQ-TCBoxes done ExpTime (Corollary 4.8). showscoding numbers indeed uence complexity reasoning problem.worth noting complexity pure concept satisfiability ALCQ depend coding; problem PSpace-complete binary unary codingnumbers (Tobies, 2000).unary coding, needed inverse roles cardinality restrictionsreduction. consistent fact satisfiability ALCQI concepts respectTBoxes consisting terminological axioms still ExpTime, shownreduction ExpTime-complete logics CIN (De Giacomo, 1995) CPDL (Pratt,1979). shows cardinality restrictions concepts additional sourcecomplexity. One reason might ALCQI cardinality restrictions longertree-model property.4. Reasoning NominalsNominals, i.e., atomic concepts referring single individualsof domain, studiedarea DLs (Borgida & Patel-Schneider, 1994; Donini et al., 1996) modal logics(Gargov & Goranko, 1993; Blackburn & Seligman, 1996; Areces et al., 1999). sectionshow how, presence nominals, consistency TCBoxes polynomiallyreduced consistency TBoxes consisting general inclusion axioms, which, general,easier problem. correspondence used obtain two novel results: (i) showthat, unary coding, consistency ALCQ-TBoxes consisting cardinality restrictionsdecided ExpTime; (ii) show that, presence inverse rolesnumber restrictions, reasoning nominals strictly harder without nominals:complexity determining consistency TBoxes general axioms rises ExpTimeNExpTime, complexity concept satisfiability rises PSpace NExpTime.Definition 4.1 Let NI set individual names (also called nominals) disjointNC NR . Concepts ALCQIO defined ALCQI -concepts additional rulethat, every 2 NI , ALCQIO-concept.general concept inclusion axiom ALCQIO expression C v D,C ALCQIO-concepts. TIBox ALCQIO finite set generalinclusion axioms ALCQIO, subscript \I" stands \Inclusion".semantics ALCQIO concepts defined similar ALCQI , additionalrequirement every interpretation maps nominal 2 NI singleton set oI ;seen name element oI . Please note uniquename assumption, i.e., assume o1 6= o2 implies oI1 6= oI2 .interpretation satisfies axiom C v iff C DI . satisfies TIBox Tgci iffsatisfies axioms Tgci ; case called model Tgci, denotefact j= Tgci . TIBox model called consistent.Cardinality restrictions, TCBoxes, interpretation ALCQIO defined analogously ALCQI .208fiThe Complexity Cardinality Restrictions NominalsRALCQO denote fragment ALCQIO contain inverse roles1.Consistency TCBoxes TIBoxes ALCQO ALCQIO Exp-hard decided NExpTime, unary coding numbers used.Proof. Consistency TIBoxes (and hence TCBoxes) ExpTime-hard already (asyntactical variant of) ALC (Halpern & Moses, 1992). Assuming unary coding numbers,reduce problems satisfiability C 2, yields NExpTime upperbound.Lemma 4.2Time4.1 Expressing Cardinality Restrictions Using Nominalsfollowing show how, assumption unary coding numbers, consistencyALCQI -TCBoxes polynomially reduced consistency ALCQIO-TIBoxes.noted that, conversely, also possible polynomially reduce consistencyALCQIO-TIBoxes consistency ALCQI -TCBoxes: arbitrary concept C ,cardinality restrictions f(6 1 C ); (> 1 C )g force interpretation C singleton.Since gain insight reduction, formally proveresult.Definition 4.3 Let = f(./1 n1 C1 ); : : : (./k nk Ck )g ALCQI -TCBox. W.l.o.g.,assume contains cardinality restriction form (> 0 C ) triviallysatisfied interpretation. translation , denoted (T ), ALCQIO-TIBoxdefined follows:[(T ) = f(./i ni Ci) j 1 kg;(./i ni Ci ) defined depending whether ./i =6 ./i =>.(n1(./i ni Ci ) = ffoCji vv Coi tj 1 tj oi ng g [ foj v :o` j 1 j < ` n g=6 ;./i =>./io1i ; : : : ; oni fresh distinct nominals use convention emptydisjunction interpreted :> deal case ni = 0.Assuming unary coding numbers, translation TCBox obviously computable polynomial time.Lemma 4.4 Let ALCQI -TCBox. consistent iff (T ) consistent.Proof. Let = f(./1 n1 C1 ); : : : (./k nk Ck )g consistent TCBox. Hence,model , j= (./i ni Ci ) 1 k. show construct model0 (T ) fromj . 0 identical every respect except interpretationnominals oi (which appear )../i =6, j= implies ]CiI ni. ni = 0, have0 introducednew nominals, (T ) contains Ci v :>. Otherwise, define (oji )I CiI209fiTobiesf(oji )I 0 j 1 j nig. implies CiI 0 (o1i )I 0 [ [ (oni )I 0 hence, either case,0 j= (6 ni Ci)../i =>, ni > 0 must hold, j= implies ]CiI 0ni. Let x1; : : : xn nidistinct elements fx1 ; : : : ; xn g CiI . set (oji )I = fxj g. Sincechosen distinct individuals interpret different nominals, 0 j= oji v :o`i every1 < ` ni. Moreover, xj 2 CiI implies 0 j= oji v Ci hence 0 j= (> ni Ci).chosen distinct nominals every cardinality restrictions, hence previousconstruction well-defined and, since 0 satisfies (./i ni Ci ) every i, 0 j= (T ).converse direction, let models (T ). fact j= (and henceconsistency ) shown follows: let (./i ni Ci) arbitrary cardinalityrestriction . ./i =6 ni = 0, (6 0 Ci) = fCi v :>g and,since j= (T ),nwe CiI = ; hence j= (6 0 Ci). ./i =6ni > 0,n11fCi v oi oi g (T ). j= (T ) follows ]Ci ](oi oi ) ni. ./i =>,foji v Ci j 1 j nig[foji v :o`i j 1 j < ` nig (T ). first setaxioms get f(oji )I j 1 j nig CiI . secondSset axioms get that,every 1 j < ` ni, (oji )I 6= (o`i )I . implies ni = ] f(oji )I j 1 j nig ]CiI .Theorem 4.5Assuming unary coding numbers, consistency ALCQI -TCBoxes polynomiallyreduced consistency ALCQIO-TIBoxes. Similarly, consistency ALCQ-TCBoxespolynomially reduced consistency ALCQO-TIBoxes.Proof. first proposition follows fact (T ) polynomially computableassume unary coding numbers Lemma 4.4. second propositionfollows fact translation introduce additional inverse roles.contain inverse roles, neither (T ), hence result translatingALCQ-TCBox ALCQO-TIBox.4.2 Complexity Resultsuse Theorem 4.5 obtain new complexity results DLs cardinalityrestrictions nominals.4.2.1ALCQ ALCQODe Giacomo (1995) obtains complexity results various DLs sophisticated polynomialreduction propositional dynamic logic. author establishes many complexity results,one special interest purposes.Theorem 4.6 ((De Giacomo, 1995), Section 7.3)Satisfiability logical implication CNO knowledge bases (TBox ABox) Exp-complete.DL CNO studied author strict extension ALCQO; TBoxes thesiscorrespond call TIBoxes paper. Unary coding numbers assumedTime210fiThe Complexity Cardinality Restrictions Nominalsthroughout thesis. Although unique name assumption made, inherentutilised reduction since explicitly enforced. thus possible eliminatepropositions require unique interpretation individuals reduction. Hence,together Lemma 4.2, get following corollary.Corollary 4.7Consistency ALCQO-TIBoxes ExpTime-complete unary coding number assumed.Together Theorem 4.5, solves open problem concerning lower boundcomplexity ALCQ cardinality restrictions; moreover, shows NExpTime-algorithm presented (Baader et al., 1996) optimal respect worst casecomplexity.Corollary 4.8Consistency ALCQ-TCBoxes ExpTime-complete, unary coding numbers cardinality number restrictions used.4.2.2ALCQIOConversely, using Theorem 4.5 enables us transfer NExpTime-completeness resultTheorem 3.8 ALCQIO.Corollary 4.9Consistency ALCQIO-TIBoxes TCBoxes NExpTime-complete.Proof. TIBoxes, immediate corollary Theorem 4.5 Theorem 3.8.Reasoning TCBoxes hard TIBoxes presences nominals.results explains gap (De Giacomo, 1995). author establishescomplexity satisfiability knowledge bases consisting TIBoxes ABoxesCNO, allows qualifying number restrictions, CIO, allows inverseroles, reduction ExpTime-complete PDL. results given combination CINO, strict extension ALCQIO. Corollary 4.8 shows that, assumingExpTime 6= NExpTime, cannot polynomial reduction satisfiabilityproblem CINO knowledge bases ExpTime-complete PDL. Again, possible explanation leap complexity loss tree model property. CIOCNO, consistency decided searching tree-like pseudo-models even presencenominals, seems longer possible case knowledge bases CINO.Unique Name Assumption noted definition nominals nonstandard Description Logics sense impose unique name assumption widely made, i.e., two individual names o1 ; o2 2 NI , oI1 6= oI2required. Even without unique name assumption, possible enforce distinct interpretation nominals adding axioms form o1 v :o2. Moreover, imposing uniquename assumption presence inverse roles number restriction leads peculiareffects. Consider following TIBox:= fo v (6 k R >); > v 9R :og211fiTobiesunique name assumption, consistent iff NI contains k individualnames, individual name must interpreted unique element domain, every element domain must reachable oI via role R, oI mayk R-successors. believe dependency consistency TIBoxconstraints explicit TIBox counter-intuitive henceimposed unique name assumption.Nevertheless, possible obtain tight complexity bound consistency ALCQIOTIBoxes unique name assumption without using Theorem 4.5, immediateadaption proof Theorem 3.8.Corollary 4.10Consistency ALCQIO-TIBoxes unique name assumption NExpTime-completeunary coding numbers assumed.Proof. simple inspection reduction used prove Theorem 3.8, especiallyproof Lemma 3.6 shows single nominal, marks upper rightcorner torus, necessary perform reduction. individual namecreate role name, following TIBox defines torus exponential size:Tn = > v 9east:>;> v 9north:>;> v (= 1 east 1 >); > v (= 1 north 1 >);> v 9create:C(0;0) ; v C(2 1;2 1) ;C(2 1;2 1) v o;> v Deast u DnorthnnnnSince reduction uses single individual name, unique name assumptionirrelevant.Internalisation Axioms presence inverse roles nominals, possibleinternalise general inclusion axioms concepts using spy-point technique used,e.g., (Blackburn & Seligman, 1996; Areces et al., 1999). main idea techniqueenforce elements model concept reachable distinct point(the spy-point) marked individual name single step.Definition 4.11 Let ALCQIO-TIBox. W.l.o.g., assume formf> v C1 ; : : : ; > v Ck g. Let spy denote fresh role name fresh individual name.define function spy inductively structure concepts setting Aspy =2 NC , ospy = 2 NI , (:C )spy = :C spy, (C1 u C2 )spy = C1spy u C2spy ,(> n R C )spy = (> n R (9spy :i) u C spy).internalisation CT defined follows:G=iu>vC 2TC spyuGCT>vC 2T8spy:C spyLemma 4.12 Let ALCQIO-TIBox. consistent iff CT satisfiable.Proof. -direction let model CT 2 (CT )I . implies iI = fag.Let 0 defined0 = fag [ fx 2 j (a; x) 2 spyI g212fiThe Complexity Cardinality Restrictions Nominals0 = jI0 .00Claim 1: every x 2 every ALCQIO-concept C , x 2 (C spy )I iff x 2 C .proof claim induction structure C . interesting caseC = (> n R ). case C spy = (> n R (9spy :i) u spy ).x 2 (> n R (9spy :i) u spy )Iiff ]fy 2 j (x; y) 2 RI 2 (9spy :i)I \ (Dspy)I g > n() iff ]fy 2 0 j (x; y) 2 RI 0 2 DI 0 g > niff x 2 (> n R D)I 0 ;equivalence () holds set equality 0 definition 0.construction,0 every > v C 2 every x 2 , x 2 (C spy )I . Due Claim 1,implies x 2 C hence 0 j= > v C .only-if -direction, let interpretation I0 j= . pickarbitrary00element 2 define extension setting = fag spy = f(a; x) jx 2 . Since spy occur , still 0 j= .0Claim 2: every x 2 every ALCQIO-concept C contain spy,00x 2 C iff x 2 (C spy )I .Again, claim proved induction structure conceptsinteresting case C = (> n R D).0x 2 (> n R )Iiff ]fy 2 0 j (x; y) 2 RI 0 2 DI 0 g > n() iff ]fy 2 0 j (x; y) 2 RI 0 ; (a; y) 2 spyI 0 ; 2 (Dspy)I 0 g > niff x 2 (> n R (9spy :i) u Dspy)I 0 :Again, equivalence () holds due set equality definition 0.Since, every > v C0 2 , 0 j= > v C , Claim 2 yields ( >vC 2T C spy)I 0 =0hence 2 (CT )Iconsequence, obtain sharper result already pure concept satisfiabilityALCQIO NExpTime-complete problem.FCorollary 4.13Concept satisfiability ALCQIO NExpTime-complete.Proof. Lemma 4.12, get function mapping ALCQIO-TIBox CTreduction consistency ALCQIO-TIBoxes ALCQIO-concept satisfiability.Corollary 4.9 know former problem NExpTime-complete. Obviously, CTcomputed polynomial time. Hence, lower complexity bound transfers.213fiTobiesConcept SatisfiabilityGCIsCardinality Restr.ALCQ-c.-c.ExpTime-c.PSpaceExpTimeALCQOopen-c.ExpTime-c.ExpTimeALCQI-c.-c.NExpTime-c.ALCQIO-c.-c.NExpTime-c.PSpaceNExpTimeExpTimeNExpTimeFigure 4: Complexity reasoning problems5. ConclusionCombining results (De Giacomo, 1995) (Tobies, 2000) resultspaper, obtain classification complexity concept satisfiability TBoxconsistency various DLs TBoxes consisting either cardinality restrictionsgeneral concept inclusion axioms shown Figure 4, assume unary codingnumbers.result ALCQIO shows current efforts extending expressive DLslogic SHIQ (Horrocks et al., 1999) DLR(Calvanese et al., 1998c) propositionaldynamic logics CPDLg (De Giacomo & Lenzerini, 1996) nominals dicult tasks,one wants obtain practical decision procedure, since already concept satisfiabilitylogics NExpTime-hard problem.shown that, complexity C 2, ALCQI reachexpressive power (Tobies, 1999). Cardinality restrictions, although interesting knowledgerepresentation, inherently hard handle algorithmically. applies nominalspresence inverse roles number restrictions. explanation offerlack tree model property, identified Vardi (1997) explanationgood algorithmic behaviour many modal logics.first glance, results make ALCQI cardinality restrictions conceptsALCQIO general axioms obsolete knowledge representation C 2 deliversexpressive power computational price. Yet, likely dedicatedalgorithm ALCQI may better average complexity C 2 algorithm;algorithm yet developed. highly desirable would also applicablereasoning problems expressive DLs nominals, applications areareasoning database schemata (Calvanese et al., 1998a, 1998b).interesting question lies coding numbers: allow binary codingnumbers, translation approach together result (Pacholski et al., 1997)leads 2-NExpTime algorithm. C 2, open question whether additionalexponential blow-up necessary. positive answer would settle question C 2proof negative answer might give hints result C 2 mightimproved. ALCQ cardinality restrictions, partially solved problem:unary coding, problem ExpTime-complete whereas, binary coding,NExpTime-hard.214fiThe Complexity Cardinality Restrictions NominalsAcknowledgmentswould like thank Franz Baader, Ulrike Sattler, anonymous referees valuablecomments suggestions. work supported DFG, Project No. GR 1324/3{1.ReferencesAiello, L. C., Doyle, J., & Shapiro, S. C. (Eds.)., Proceedings KR'96 (1996). PrinciplesKnowledge Representation Reasoning: Proceedings Fifth InternationalConference (KR'96). Morgan Kaufmann Publishers, San Francisco, California.Areces, C., Blackburn, P., & Marx, M. (1999). road-map complexity hybrid logics.Proceedings Annual Conference European Association ComputerScience Logic (CSL-99), LNCS 1683, pp. 307{321. Springer-Verlag.Baader, F., Buchheit, M., & Hollunder, B. (1996). Cardinality restrictions concepts.Artificial Intelligence, 88 (1{2), 195{213.Baader, F., & Sattler, U. (1999). Expressive number restrictions description logics.Journal Logic Computation, 9 (3), 319{350.Berger, R. (1966). undecidability dominoe problem. Memoirs AmericanMathematical Society, 66.Blackburn, P., & Seligman, J. (1996). Hybrid languages. Journal Logic, LanguageInformation, 3 (4), 251{272.Borger, E., Gradel, E., & Gurevich, Y. (1997). Classical Decision Problem. PerspectivesMathematical Logic. Springer-Verlag, Berlin.Borgida, A. (1996). relative expressiveness description logics first order logics.Artificial Intelligence, 82, 353{367.Borgida, A., & Patel-Schneider, P. F. (1994). semantic complete algorithmsubsumption classic description logic. Journal Artificial Intelligence Research,1, 277{308.Calvanese, D., De Giacomo, G., Lenzerini, M., Nardi, D., & Rosati, R. (1998a). Source integration data warehousing. Proceedings Ninth International WorkshopDatabase Expert Systems Applications (DEXA-98), pp. 192{197. IEEE ComputerSociety Press.Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998b). decidability querycontainment constraints. Proceedings Seventeenth ACM SIGACTSIGMOD SIGART Symposium Principles Database Systems (PODS-98).Calvanese, D., De Giacomo, G., Lenzerini, M., Nardi, D., & Rosati, R. (1998c). Descriptionlogic framework information integration. Proc. 6th Int. Conf.Principles Knowledge Representation Reasoning (KR'98), pp. 2{13.215fiTobiesDe Giacomo, G. (1995). Decidability Class-Based Knowledge Representation Formalisms. Ph.D. thesis, Dipartimento di Informatica e Sistemistica, Univ. di Roma\La Sapienza".De Giacomo, G., & Lenzerini, M. (1996). TBox ABox reasoning expressive descriptionlogics.. Aiello et al. (Aiello et al., 1996), pp. 316{327.Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1996). Reasoning descriptionlogics. Brewka, G. (Ed.), Foundation Knowledge Representation, pp. 191{236.CSLI-Publications.Fine, K. (1972). many possible worlds. Notre Dame Journal Formal Logic, 13,516{520.Gargov, G., & Goranko, V. (1993). Modal logic names. J. Philosophical Logic, 22,607{636.Gradel, E., Otto, M., & Rosen, E. (1997). Two-variable logic counting decidable.Proceedings, Twelth Annual IEEE Symposium Logic Computer Science, pp.306{317 Warsaw, Poland. IEEE Computer Society Press.Halpern, J. Y., & Moses, Y. (1992). guide completeness complexity modellogics knowledge belief. Artificial Intelligence, 54 (3), 319{379.Horrocks, I., Sattler, U., & Tobies, S. (1999). Practical reasoning expressive descriptionlogics. Proceedings 6th International Conference Logic ProgrammingAutomated Reasoning (LPAR'99), pp. 161{180.Pacholski, L., Szwast, W., & Tendera, L. (1997). Complexity two-variable logiccounting. Proceedings, Twelth Annual IEEE Symposium Logic ComputerScience, pp. 318{327 Warsaw, Poland. IEEE Computer Society Press.Pratt, V. R. (1979). Models program logics. Proceedings Twentieth IEEESymposium Foundations Computer Science, pp. 115{122. IEEE.Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions complements. Artificial Intelligence, 48, 1{26.Spaan, E. (1993). Complexity Modal Logics. Ph.D. thesis, University Amsterdam.Tobies, S. (1999). NExpTime-complete description logic strictly contained C 2.Flum, J., & Rodriguez-Artalejo, M. (Eds.), Proceedings Annual ConferenceEuropean Association Computer Science Logic (CSL-99), LNCS 1683, pp.292{306. Springer-Verlag.Tobies, S. (2000). PSPACE reasoning graded modal logics. Journal Logic Computation. appear.216fiThe Complexity Cardinality Restrictions NominalsVardi, M. Y. (1997). modal logic robustly decidable?. Descriptive ComplexityFinite Models: Proceedings DIMACS Workshop, January 14-17, 1996, No. 31DIMACS Series Discrete Mathematics Theoretical Computer Science, pp.149{184. American Math. Society.Wang, H. (1963). Dominoes AEA case Decision Problem. Bell Syst. Tech.J., 40, 1{41.Woods, W. A., & Schmolze, J. G. (1992). Kl-One family. Computers MathematicsApplications { Special Issue Artificial Intelligence, 23 (2{5), 133{177.217fiJournal Artificial Intelligence Research 12 (2000) 387-416Submitted 12/99; published 6/00Application Reinforcement Learning DialogueStrategy Selection Spoken Dialogue System EmailMarilyn A. Walkerwalker@research.att.comAT&T Shannon Laboratory180 Park Ave., Bldg 103, Room E103Florham Park, NJ 07932Abstractpaper describes novel method spoken dialogue system learnchoose optimal dialogue strategy experience interacting human users.method based combination reinforcement learning performance modeling spoken dialogue systems. reinforcement learning component applies Q-learning(Watkins, 1989), performance modeling component applies PARADISE evaluation framework (Walker et al., 1997) learn performance function (reward) usedreinforcement learning. illustrate method spoken dialogue system namedelvis (EmaiL Voice Interactive System), supports access email phone.conduct set experiments training optimal dialogue strategy corpus 219dialogues human users interact elvis phone. teststrategy corpus 18 dialogues. show elvis learn optimize strategyselection agent initiative, reading messages, summarizing email folders.1. Introductionpast several years, become possible build spoken dialogue systemscommunicate humans telephone real time. Systems exist tasksfinding good restaurant nearby, reading email, perusing classified advertisementscars sale, making travel arrangements (Seneff, Zue, Polifroni, Pao, Hetherington, Goddeau, & Glass, 1995; Baggia, Castagneri, & Danieli, 1998; Sanderman, Sturm,den Os, Boves, & Cremers, 1998; Walker, Fromer, & Narayanan, 1998). systemsrealized examples real time, goal-oriented interactions humanscomputers. Yet spite 30 years research algorithms dialogue management task-oriented dialogue systems, (Carbonell, 1971; Winograd, 1972; Simmons &Slocum, 1975; Bruce, 1975; Power, 1974; Walker, 1978; Allen, 1979; Cohen, 1978; Pollack,Hirschberg, & Webber, 1982; Grosz, 1983; Woods, 1984; Finin, Joshi, & Webber, 1986;Carberry, 1989; Moore & Paris, 1989; Smith & Hipp, 1994; Kamm, 1995) inter alia,design dialogue manager real-time, implemented systems still artscience (Sparck-Jones & Galliers, 1996). paper describes novel method,experiments validate method, spoken dialogue system learnexperience human users optimize choice dialogue strategy.dialogue manager spoken dialogue system processes user's utterancechooses real time information communicate human usercommunicate it. choice makes called strategy. dialogue managernaturally formulated state machine, state dialogue defined setc 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiWalkerstate variables representing observations user's conversational behavior, resultsaccessing various information databases, aspects dialogue history. Transitionsstates driven system's dialogue strategy. typical system,large number potential strategy choices state dialogue.example, consider one choice faced elvis (EmaiL Voice Interactive System)spoken dialogue system supports access user's email phone. elvis providesverbal summaries user's email folders, many ways summarize folder(Sparck-Jones, 1993, 1999). summary could consist simple statementnumber messages different folders, e.g., 5 new messages, could providemuch detail messages particular folder, e.g., messagesKim, one message meeting, second interviewing Antonio.elvis must decide many properties message mention, message'sstatus, sender, subject message.1Decision theoretic planning applied problem choosing among dialoguestrategies, associating utility U strategy (action) choice positingspoken dialogue systems adhere Maximum Expected Utility Principle(Keeney & Raiffa, 1976; Russell & Norvig, 1995),Maximum Expected Utility Principle: optimal action one maximizes expected utility outcome states.Thus, elvis act optimally choosing strategy state Si maximizes U (Si ).formulation however simply leaves us problem derive utilityvalues U (Si ) dialogue state Si . Several reinforcement learning algorithms baseddynamic programming specify way calculate U (Si ) terms utility successorstate Sj (Bellman, 1957; Watkins, 1989; Sutton, 1991; Barto, Bradtke, & Singh, 1995),utility final state dialogue known, would possible calculateutilities earlier states, thus determine policy selects optimaldialogue strategies.Previous work suggested possible treat dialogue strategy selectionstochastic optimization problem way (Walker, 1993; Biermann & Long, 1996; Levin,Pieraccini, & Eckert, 1997; Mellish, Knott, Oberlander, & O'Donnell, 1998). However(Walker, 1993), argued lack performance function assigning utilityfinal state dialogue critical methodological limitation. seemedthree main possibilities simple reward function: user satisfaction, task completion,measure user effort elapsed time dialogue numberuser turns. appeared simple reward functions failcapture essential aspects system's performance. example, level usereffort complete dialogue task system, domain task dependent. Moreover, highlevels effort, e.g., requirement users confirm system's understandingutterance, necessarily lead concomitant increases task completion,1. strategies implemented elvis summarized Figure 1. Note due practicalconstraints, implemented strategy choices subset states, elvis uses fixedstrategy states. Section 2, describe detail strategy choices elvis exploresaddition choices summarization, namely choices among strategies controlling dialogueinitiative reading multiple messages.388fiReinforcement Learning ELVIS Systemlead significant decreases user satisfaction (Shriberg, Wade, & Price, 1992; Danieli &Gerbino, 1995; Kamm, 1995; Baggia et al., 1998). Furthermore, user satisfaction alone failsect fact system successful unless helps user completetask. concluded relationship measures interestingcomplex method deriving appropriate performance functionnecessary precursor applying stochastic optimization algorithms spoken dialoguesystems. (Walker, Litman, Kamm, & Abella, 1997a), proposed paradise methodlearning performance function corpus human-computer dialogues.work, apply paradise model learn performance function elvis,use calculating utility final state dialogue experimentsapplying reinforcement learning elvis's selection dialogue strategies. Section 2 describes implementation version elvis randomly explores alternate strategiesinitiative, reading messages, summarizing email folders. Section 3 describesexperimental design first use exploratory version elvis collecttraining corpus conversations 73 human users carrying set three emailtasks. Section 4 describes apply reinforcement learning corpus 219 dialogues optimize elvis's dialogue strategy decisions. test optimized policyexperiment six new users interact elvis complete settasks, show learned policy performs significantly better exploratorypolicy used training phase.2. ELVIS Spoken Dialogue Systemstarted process designing elvis conducting Wizard-of-Oz experimentrecorded dialogues six people accessing email remotely talkinghuman playing part spoken dialogue system. purposeexperiment identify basic functionality implemented elvis.analysis resulting dialogues suggested elvis needed support contentbased access email messages specification subject sender field, verbalsummaries email folders, reading body email message, requests helprepetition messages (Walker et al., 1997b, 1998).Given requirements, implemented elvis using general-purpose platformspoken dialogue systems (Kamm et al., 1997). platform consists dialoguemanager (described detail Section 2.2), speech recognizer, audio servervoice recordings text-to-speech (TTS), interface computer runningElvis telephone network, modules specifying rules spoken languageunderstanding application specific functions.speech recognizer speaker-independent Hidden Markov Model (HMM) system,context-dependent phone models telephone speech, constrained grammarsdefining vocabulary permitted point dialogue (Rabiner, Juang, &Lee, 1996). platform supports barge-in, user interrupt system;barge-in important application user interrupt systemreading long email message.audio server switch voice recordings text-to-speech (TTS)integrate voice recordings TTS. TTS technology concatenative diphone synthe389fiWalkersis (Sproat & Olive, 1995). Elvis uses TTS since would possible pre-record,concatenate, words necessary realizing content email messages.spoken language understanding (SLU) module consists set rules specifyingvocabulary allowable utterances, associated set rules translatinguser's utterance domain-specific semantic representation meaning. syntacticrules converted FSM network used directly speech recognizer(Mohri, Pereira, & Riley, 1998). semantic rule associated syntacticrule maps user's utterance directly application specific template consistingapplication function name arguments. templates converted directlyapplication specific function calls specified application module. understandingmodule also supports dynamic grammar generation loading recognizervocabulary must change interaction, e.g., support selection email messagescontent fields sender subject.application module provides application specific functions, e.g., functions accessing message attributes subject sender, functions making realizablespeech used instantiate variables spoken language generation.2.1 ELVIS's Dialogue Manager Strategies's dialogue manager based state machine one dialogue strategiesexplored state. state dialogue manager defined setstate variables representing various items information dialogue manager usesdeciding next. state variables encode various observations user'sconversational behavior, results processing user's speech spokenlanguage understanding (SLU) module, results accessing information databasesrelevant application, well certain aspects dialogue history. dialoguestrategy specification system say; Elvis representedtemplate variables must instantiated current context. statessystem always executes dialogue strategy states alternate strategiesexplored. strategies implemented Elvis summarized Figure 1.complete specification dialogue strategy executed state calledpolicy dialogue system.develop version Elvis supported exploring number possible policies,implemented several different choices particular states system. goalimplement strategy choices states optimal strategy obvious priori.purpose illustrating dialogue strategies explored, consider situationuser attempting execute following task (one tasks usedexperimental data collection described Section 3):ElvisTask 1.1: working home morning plan go directly meetinggo work. Kim said would send message tellingmeeting is. Find Meeting Time Meeting Place.complete task, user needs find message Kim meetinginbox listen it. many possible strategies Elvis could use helpuser accomplish task. Below, first describe dialogue strategies Figure 1390fiReinforcement Learning ELVIS SystemStrategy TypeInitiativeSummarizationChoicesExplored?yesyesReadingyesRequest-InfoProvide-InfoHelpTimeoutRejectionStrategy ChoicesSystem-Initiative (SI), Mixed-Initiative (MI)SummarizeBoth (SB), SummarizeSystem (SS),SummarizeChoicePrompt (SCP)Read-First (RF), Read-Summary-Only (RSO),Read-Choice-Prompt (RCP)AskUserName, Ask-Which-Selection (AskWS),Ask-Selection-Criteria (AskSC),Read-MessageAskUserName-Help, SI-Top-Help, MI-Top-Help,Read-Message-Help, AskWS-Help, AskSC-HelpAskUserName-Timeout,Read-Timeout,SI-Top-Timeout, MI-Top-Timeout, Read-MessageTimeout, AskWS-Timeout, AskSC-TimeoutAskUserName-Reject,SITop-Reject, MI-Top-Reject, AskWS-Reject, AskSCReject, Read-Message-RejectFigure 1: ELVIS's Dialogue Strategies. ELVIS explores choices Initiative, SummarizationRead Strategies uses fixed strategies elsewhere.Elvis makes choices among, describe detail complete state machine,dialogue strategies Figure 1 used states choice amongdialogue strategies, space policies Elvis execute. provide severaldetailed examples dialogues generated Elvis's dialogue manager's statemachine.2.1.1 Strategy Choices ELVISOne strategy choice experiments involved exploration two different strategiesmanaging dialogue initiative: system-initiative mixed-initiative. implementation Elvis tested here, choice initiative made early dialoguestyle initiative used throughout dialogue whenever couldchoice two order avoid confusing user.Previous research suggested system-initiative strategy Figure 2Elvis directive user might best strategy, given current stateart speech recognition (Kamm, 1995). Figure 2, Elvis's utterances labelledname dialogue strategy Elvis using produce utterance, e.g.,utterance E1 labelled SI-Top, name system-initiative top level strategy.choice system-initiative strategy also affects utterances E2 E3; utterancesElvis continues dialogue requesting particular items information. general,system-initiative strategy constrains user say point dialogue,requesting particular items information one time, user constrainedproviding information asked for. reason strategyrecommended given current state art speech recognition guides391fiWalkerE1: Hi, ELVIS here...You 5 new unread messagesinbox. Say Read, Summarize, say Help options.U1: Read.E2: Select Sender, Subject order arrival?U2: Sender.E3: Sender?U3: Kim.E4: messages Kim, there's 1 message \InterviewingAntonio" 1 message \Meeting Today." first messagetitled, \Interviewing Antonio." says, \I'd like interview him.could also go along lunch. Kim."U4: Next.E5: message meeting today. says meetingmorning 10:00 2d516. See there, Kim.U5: I'm done here.SI-TopAsk-Which-SelectionAsk-Selection-CriterionRead-First (RF)Read-MessageFigure 2: System-Initiative (SI) Dialogue Strategy: ELVIS explicitly prompts usersay point dialogue. ELVIS's utterances labelledstrategy names.user saying things system likely understand, recognizervocabulary state constrained responses question asked.strategy might dispreferred users however, could tedious userconstrained providing one item information time. dialogue Figure 2,takes four user utterances access message Kim needed complete Task 1.1.research suggested better strategy mixed-initiative strategy allowsuser take initiative except user fails say anything systemtrouble understanding user (Smith & Hipp, 1994; Levin, Pieraccini, Eckert,Fabbrizio, & Narayanan, 1999; Webber & Joshi, 1982; Walker & Whittaker, 1990).dialogue Figure 3 illustrates mixed-initiative dialogue strategy. Elvis'sutterances Figure 3 labelled name dialogue strategy Elvis usingproduce utterance, e.g., utterance E1 labelled MI-Top, name mixedinitiative top level strategy. Note unlike SI-Top strategy, MI-Top strategyvolunteer information tell user say. Instead,mixed-initiative strategy allows user take control dialogue specify exactlywants single utterance, utterance U1 Figure 3. However,evidence user trouble confused, user saysnothing utterance U2, mixed-initiative strategy take initiative askuser information tell user say. utterance E3 Figure 3Read-Timeout strategy triggered fact user's response delayed longerpre-set timeout threshhold, system tells user actions availablepoint dialogue. general, mixed-initiative strategy supportecient dialogues; dialogue Figure 2, takes two user utterances accessmessage Kim needed complete Task 1.1. However eciency gain may offsetincreased likelihood misunderstandings user confusion.392fiReinforcement Learning ELVIS SystemE1: Hi, ELVIS here. I've got mail.U1: Read messages Kim.E2: messages Kim, there's 1 message \InterviewingAntonio" 1 message \Meeting Today." first messagetitled, \Interviewing Antonio." says, \I'd like interview him. couldalso go along lunch. Kim."U2: (User says nothing within 5 seconds system takes initiative)E3: say repeat, repeat last message, say next,read next message folder. say, I'm doneexit folder.U3: Next message.E4: message meeting today. says meetingmorning 10:00 2d516. See there, Kim.U4: I'm done here.MI-TopRead-First (RF)Read-TimeoutReadMessageFigure 3: Mixed-Initiative (MI) Dialogue Strategy: ELVIS leaves user takeinitiative, unless user seems trouble systemSummarize StrategySummarize-Both (SB)Summarize-System (SS)Summarize-Choice-Prompt(SCP)Example Prompttop level inbox, Kim, there's 1 message\Lunch." Michael, there's 1 message \Evaluationgroup meeting." Noah, there's 1 message \CallTomorrow" 1 message \Interviewing Antonio."Owen, there's 1 message \Agent Personality."top level inbox, there's 1 message Kim, 2 messagesNoah, 1 message Michael, 1 message Owen.E: Summarize subject, sender, both?U: Subject.E: top level inbox, there's 1 message \Lunch," 1message \Interviewing Antonio," 1 message \CallTomorrow," 1 message \Evaluation Group Meeting,"1 message \Agent Personality."Figure 4: Alternate Summarization Strategies response request \Summarizemessages"different type strategy choice involves Elvis's decisions present information user. mentioned many different ways summarizeset items user wants information about. Elvis explores set alternate summarization strategies illustrated Figure 4; strategies vary message attributesincluded summary messages current folder. Summarize-Bothstrategy (SB) uses sender subject attributes summary. employing Summarize-System strategy (SS), Elvis summarizes subject senderbased current context. instance, user top level inbox, Elvissummarize sender, user situated folder containing messages par393fiWalkerticular sender, Elvis summarize subject, summary sender would providenew information. Summarize-Choice-Prompt (SCP) strategy asks user specifyrelevant attributes summarize by. See Figure 4.Another type information presentation choice occurs request userread subset messages, e.g., Read messages Kim, results multiplematching messages. strategies explored Elvis summarized Figure 5. Onechoice Read-First strategy (RF) involves summarizing messagesKim, taking initiative read first one. Elvis used read strategydialogues Figures 2 3. alternate strategy reading multiple matchingmessages Read-Summary-Only (RSO) strategy, Elvis provides informationallows users refine selection criteria. Another strategy reading multiplemessages Read-Choice-Prompt (RCP) strategy, Elvis explicitly tells usersay order refine message selection criteria. See Figure 5.Read StrategyRead-First (RF)Read-Summary-Only(RSO)Read-Choice-Prompt(RCP)Example Promptmessages Kim, there's 1 message \Interviewing Antonio" 1 message \Meeting Today." first messagetitled, \Interviewing Antonio." says, \I'd like interview him.could also go along lunch. Kim."messages Kim, there's 1 message \Interviewing Antonio" 1 message \Meeting Today."messages Kim, there's 1 message \Interviewing Antonio" 1 message \Meeting Today." hear messages,say, \Interviewing Antonio" \Meeting."Figure 5: Alternate Read Strategies response request \Read messagesKim"remainder Elvis's dialogue strategies, summarized Figure 1, fixed, i.e.multiple versions strategies explored experiments presented here.2.1.2 ELVIS's Dialogue State Machinementioned above, dialogue strategy choice system makes, particularstate, say say it. policy dialogue system completespecification strategy execute system state. state defined setstate variables. Ideally, state representation corresponds dialogue modelsummarizes dialogue history compactly, retains relevant informationdialogue interaction far. notion dialogue model retaining relevantinformation formally known reinforcement learning state representationsatisfies Markov Property. state representation satisfying Markov Property oneprobability particular state particular reward raction prior state estimated function actionprior state, function complete dialogue history (Sutton & Barto, 1998).precisely,Pr(st+1 = s0; rt+1 = rjst ; ) = Pr(st+1 = s0 ; rt+1 = rjst; ; rt ; st,1 ; at,1 ; rt,1 ; : : : R1; s0 ; a0 )394fiReinforcement Learning ELVIS Systems0 ; r; st .Markov Property guaranteed state representation encodes everythingsystem able observe everything happened dialogue far.However, representation would complex estimate model probabilityvarious state transitions, systems complex spoken dialogue system mustgeneral utilize state representations compact possible.2 However staterepresentation impoverished, system lose much relevant informationwork well.Operations VariableKnowUserNameInitStratSummStratReadStratTaskProgressCurrentUserGoalNumMatchesWhichSelectionKnowSelectionCriteriaConfidenceTimeoutHelpCancelAbbrev(U)(I)(S)(R)(P)(G)(M)(W)(SC)(C)(T)(H)(L)Possible Values0,10,SI,MI0,SS,SCP,SB0,RF,RSO,RCP0,1,20,Read,Summarize0,1,N>10,Sender (Snd),Subject (Sub),InOrder (InO)0,10,10,10,10,1Figure 6: Operations variables possible values define operations vectorcontrolling aspects ELVIS's behavior. abbreviations variablenames values used column headers Operations VariablesFigures 7, 8 9.'s state space representation must obviously discriminate among statesvarious strategy choices explored, addition, must state variablescapture distinctions number states Elvis always executesstrategy. state variables Elvis keeps track possible values givenFigure 6. KnowUserName (U) variable keeps track whether Elvis knows user'sname not. InitStrat (I), SummStrat (S) ReadStrat (R) variables keep trackwhether Elvis already employed particular initiative strategy, summarize strategyreading strategy current dialogue, so, strategy selected.variable needed Elvis employs one strategies, strategyused consistently throughout rest dialogue order avoid confusing user.TaskProgress (P) variable tracks much progress user made completingexperimental task. CurrentUserGoal (G) variable corresponds system's beliefElvis2. respects driven implementation requirements since system development maintenance impossible without compact state representations.395fiWalkerU01111111111111111Operations VariablesAction ChoicesR P GW SC C H L0 00 0 000 0 0 0 0 0 AskUserName0 00 0 000 0 1 0 0 0 SI-Top, MI-TopSI 00 0 000 0 1 0 1 0 SI-Top-HelpSI 00 0 000 0 0 0 0 0 SI-Top-RejectSI 00 000 0 1 0 0 0 SS,SB,SCPSI 00 0 R00 0 1 0 0 0 AskWSSI 00 0 R00 0 0 0 0 0 AskWS-RejectSI 00 0 R0 Snd 0 1 0 0 0 AskSCSI 00 0 R0 Snd 0 1 1 0 0 AskSC-TimeOutSI 00 0 R N>1 Snd 1 1 0 0 0 RF,RSO,RCPSI 0 RCP 0 R1 Snd 1 1 0 0 0 ReadMessageSI 0 RCP 1 000 1 0 0 0 0 SI-TopMI 00 0 000 0 1 0 1 0 MI-Top-HelpMI 00 0 000 0 0 0 0 0 MI-Top-RejectMI 00 000 0 1 0 0 0 SS,SB,SCPMI SS0 0 R N>1 Snd 1 1 0 0 0 RF,RSO,RCPMI SS RF 0 R1 Snd 1 1 0 0 0 ReadMessageFigure 7: portion ELVIS's operations state machine using full operations vectorcontrol ELVIS's behavioruser's current goal is. WhichSelection (W) variable tracks whethersystem knows type selection criteria user would like use readmessages. KnowSelectionCriteria (SC) variable tracks whether system believesunderstood either sender name subject name use select messages.NumMatches (M) variable keeps track many messages match user's selectioncriteria. Confidence (C) variable threshholded variable indicating whetherspeech recognizer's confidence understood user said pre-setthreshhold. Timeout (T) variable represents system's belief user didn'tsay anything allotted time. Help (H) variable represents system's beliefuser said Help, leads system providing context-specific help messages.Cancel (L) variable represents system's belief user said Cancel, leadssystem resetting state state last user utterance processed.Thus 110,592 possible states used control operation system, althoughstates occur.3order reader achieve better understanding range Elvis's capabilities way operations vector used, Figure 7 shows portion Elvis's statemachine generate sample system mixed-initiative dialogue interactionsFigures 8 9. figures provides state representation strategy choices made state sample dialogues. example, row two Figure7 shows system acquires user's name (KnowUserName (U) = 1)high confidence (Confidence (C) = 1), explore system-initiative (SI-Top)3. example system knows user name, none variable values changeinitial value.396fiReinforcement Learning ELVIS Systemmixed-initiative (MI-Top) strategies. Figure 8 illustrates dialogue SI strategychosen Figure 9 illustrates dialogue MI-Top strategy chosen.discuss detail dialogue Figure 8 generated state machineFigure 7.Figure 8, first row shows Elvis's strategy AskUserName executedinitial state dialogue operations variables set 0. Elvis'sutterance E1 surface realization strategy's execution. Note accordingstate machine Figure 7, strategy choices initial statedialogue. user responds name SLU module returns user'sname dialogue manager high confidence (Confidence (C) = 1). dialoguemanager updates operations variables KnowUserName(U) = 1 Confidence (C)= 1, shown row two Figure 8. Now, according state machine Figure 7,two choices strategy, system-initiative strategy whose initial action SI-Topmixed-initiative strategy whose initial action MI-Top. Figure 8 illustrates onepotential path SI-Top strategy chosen; Elvis's utterance E2 realizationSI-Top strategy. user responds utterance Help processedSLU, dialogue manager receives input information SLU believesuser said Help (Help (H) = 1) high confidence (Confidence (C) = 1). dialoguemanager updates operations variables ect information SLU wellfact executed system-initiative strategy (InitStrat (I) = SI). resultsoperations vector shown adjacent Elvis's utterance E3. third row statemachine Figure 7 shows state, Elvis choice strategies, Elvissimply executes SI-Top-Help strategy, realized utterance E3. userresponds saying Read (utterance U3) dialogue manager updates operationsvariables results SLU module saying believes user said Read(Goal (G) = R) high confidence (Confidence (C) = 1). state machine Figure 7specifies state Elvis execute AskWhichSelection (AskWS) strategy,corresponds Elvis's utterance E4 Figure 8. time, however, userresponds system's query word Sender (utterance U4), SLU moduleconfident understanding (Confidence (C) = 0) operations variableupdated confidence value. According state machine Figure 7, strategyElvis executes state AskWS-Reject strategy, specific rejection strategydesigned executed context goal acquire value WhichSelectionoperations variable (see list operations variables Figure 6). realizationAskWS-Reject strategy utterance E5. user responds saying Sender(utterance U5) time SLU module understands user's utterance highconfidence. dialogue manager updates WhichSelection (W) variable Sndconfidence variable, resulting operations vector shown adjacent utteranceE6. state machine Figure 7 specifies state Elvis executeAskSelectionCriterion (AskSC) strategy realized utterance E6. user saysnothing (utterance U6) SLU module detects silence pre-set timeoutthreshhold high confidence. dialogue manager updates operations variablesect observation (Timeout (T) = 1, Confidence (C) = 1). state machineFigure 7 specifies AskSC-Timeout strategy executed state;context specific strategy re-attempts system's initial goal providing information397fiWalkerOperations VariablesUtteranceU0 0R0 0P0G0W0 0SC C00 0100000000101SI0000000101SI000R000101SI000R000001SI000R0Snd0101SI000R0Snd0111SI000RNSnd1101SI0RCP0R1Snd1101SI0RCP1000010H L0 0 E1: Hi, ELVIS here,that?U1: Susan.0 0 E2: Hi Susan, Please wait.I'll get mail (10 seconds).Okay Susan, 5 newunread messages inbox. Say Read, Summarizesay Help options.U2: Help.1 0 E3: access messages,say read summarize.switch active messages,say new messages, messages. want repeatmessage summary, say repeat. always say cancelhelp. Say Goodbye Elvis,hang up.U3: Read.0 0 E4: Select Sender, Subjectorder Arrival?U4: Sender0 0 E5: didn't understand you.want select messagessender, subject orderarrival?U5: Sender0 0 E6: Sender?U6: (user says nothing)0 0 E7: senders are, Kim,Noah, Michael, Owen.sender want?U7: Kim.0 0 E8: messages Kim,there's 1 message \Interviewing Antonio" 1 message \Meeting Today."hear messages, say, \Interviewing Antonio" \Meeting."U8: Meeting.0 0 E9: messagemeeting today. says meeting morning 10:002d516. See there, Kim.U9: I'm done here.0 0 E10: backtoplevel inbox. Say Read, Summarize say Helpoptions.StrategyNameAskUserNameSI-TopSI-Top-HelpAskWSAskWSRejectAskSCAskSCTimeOutRCPReadMessageSI-TopFigure 8: System-Initiative Dialogue, completing Task 1.1 Figure 11, illustratingELVIS's ability provide help, use timeout confidence informationintended help user re-asking original query, realizedutterance E7. user responds name sender (utterance U7)understood SLU high confidence (KnowSelectionCriteria (SC) = 1, Confidence =398fiReinforcement Learning ELVIS System1). Elvis retrieves messages mail server matching selection criteria,multiple matches found (NumMatches = N, per list operations variablesFigure 6). time row ten state machine Figure 7 specifies statechoice dialogue strategies, namely choice Read-First (RF), ReadSummary-Only (RSO) Read-Choice-Prompt (RCP) strategies illustrated Figure 5.Elvis randomly chooses explore RCP strategy, realized utterance E8.information user needs complete Task 1.1 provided utterance E9user responds utterance U8 saying Meeting (and SLU understands highconfidence). row utterance E9 Figure 8 shows updated operations vectorecting fact system executed RCP strategy; ReadStrat (R) variableused enforce fact implementation Elvis, particular reading,strategy selected, used consistently throughout dialogue avoid confusinguser. last exchange Figure 8, SLU module's confident understandinguser's utterance U9, I'm done here, results resetting G,M,W, SC variablesdialogue manager also updates variable TaskProg (P) 1 ect progressexperimental task. Figure 7 shows that, state, system one strategy;since InitStrat variable set SI, system executes SI-Top strategy,realized context utterance E10.dialogue Figure 9 illustrates potential dialogue Elvis MI-Topstrategy selected rather SI-Top strategy user name acquired.reader may also track path dialogue utilizing state machine Figure 7.Note operations vector Elvis utilizes needed make Elvis fully operational system provides functionality equired support users. dialoguesFigures 8 9 also show Elvis provides:Context-Specific Help strategies: illustrated strategies SI-Top-Help MITop-Help, supported Help variable.Timeout strategies: system uses taking initiative context, trig-gered delay user's response going time threshhold, illustratedstrategy AskSC-TimeOut, supported Timeout variable.Rejection strategies: backoff dialogue actions become directive userASR returns confidence value lower confidence threshhold,illustrated strategies AskWS-Reject MI-Top-Reject, supportedConfidence variable.However, operations vector state representation needed control operationsystem necessarily required even optimal applying reinforcement learning(Barto et al., 1995; Sutton & Barto, 1998). Sometimes may advantageous aggregrate states purpose applying reinforcement learning, even possibleguarantee state representation obeys Markov property (Sutton & Barto,1998). Note many states defined Figure 7 alternate strategiesexplored. example, Confidence variable used system determine whetherapply backoff rejection strategy, vary rejection strategies.incentive aggregrating states ignoring state variables irrelevant learning399fiWalkerOperations VariablesUtteranceU0 00R0P G0 0W0 0SC C000H L0 01000000000010E1: Hi, ELVIS here, AskUserNamethat?U1: Susan.E2: Hi Susan, Please wait. I'llget mail (delay 10seconds).1MI 000000010101MI 000000000001MI 00000010001MI SS00RNSnd110001MI SSRF1R1Snd11000StrategyNameOkay Susan, I've got mail.U2: Help.E3: read summarize messages using valuessender subject field.example, say, readmessage Leemeeting, or, read messagesKim, or, summarizemessages. access messagescurrent folder, alsosay next, previous, first, last.want repeat message summary, say repeat.always say cancelhelp. exit current foldersay, I'm done here. hangup,say, Goodbye Elvis .U3: Summarize messages.E4: didn't understand you.wantmessages?U4: Summarize them.E5: top level inbox,there's 2 messages Kim,1 message Noah, 1 message Michael, 1 message Owen.U5: Read messagesKimE6: messages Kim,there's one message Interviewing Antonio one message Meeting Today.first message says (user bargesin)U6: Next messageE7: Next message.message meeting today.says meeting morning10:00 2d516. See there,Kim.MI-TopMI-Top-HelpMI-TopRejectSSRFReadMessageFigure 9: Mixed Initiative Dialogue completing Task 1.1 Figure 11, illustrating ELVIS'sability provide help, use timeout confidence informationreduction state space size; means fewer dialogue samples neededcollect large enough sample state/action pairs purpose applying reinforcementlearning. perspective, goal aggregrate state space waydistinguish states different dialogue strategies explored.400fiReinforcement Learning ELVIS SystemHowever, additional constraint state aggregration. Reinforcement learning4backs rewards received final states dialogue sf earlier states sistrategy choices explored. However algorithm distinguish strategy choicestrajectory si sf distinct strategy choice. words,two actions point lead state, without local reward, Q-valuestwo actions equal.UserName (U) Init (I) TaskProg (P) UserGoal (G)0,10,SI,MI 0,1,2,0,R,SFigure 10: Reinforcement Learning State Variables ValuesFigure 10 specifies subset state variables given Figure 6 developed represent state space purpose applying reinforcement learning.combination state variables compact, provides distinct trajectoriesdifferent strategy choices. reduced state space 18 states, supports dialogueoptimization policy space 2 312 = 1062882 different policies. policiesprima facie candidates optimal policies support human userscompleting set experimental email tasks.3. Experimental DesignExperimental dialogues training testing phase collected via experiments human users interacted Elvis complete three representative application tasks required access email messages three different email inboxes.collected data 73 users performing three tasks (219 dialogues) training Elvis,tested learned policy corpus six users performingthree tasks (18 dialogues).Instructions users given set web pages, one page experimental dialogue. web page dialogue also contained brief general descriptionfunctionality system, list hints talking system, descriptiontasks user supposed complete, information call Elvis.page also contained form specifying information acquired Elvisdialogue, survey, filled task completion, designed probe user'ssatisfaction Elvis. Users read instructions oces calling Elvisoce phone.three calls Elvis made sequence, conversation consistedtwo task scenarios system user exchanged information criteriaselecting messages information within message. tasks given Figure11, where, e.g., Task 1.1 Task 1.2 done conversation Elvis.motivation asking caller complete multiple tasks call create subdialoguestructure experimental dialogues (Litman, 1985; Grosz & Sidner, 1986).4. applied without local rewards.401fiWalkerTask 1.1: working home morning plan go directly meetinggo work. Kim said would send message tellingmeeting is. Find Meeting Time Meeting Place.Task 1.2: second task involves finding information different message. Yesterdayevening, told Lee might want call morning. Lee said would sendmessage telling reach him. Find Lee's Phone Number.Task 2.1: got work, went directly meeting. Since peoplelate, you've decided call ELVIS check mail see meetings mayscheduled. Find day, place time scheduled meetings.Task 2.2: second task involves finding information different message. Findneed call anyone. so, find number call.Task 3.1: expecting message telling Discourse Discussion Groupmeet. Find place time meeting.Task 3.2: second task involves finding information different message. secretarytaken phone call left message. Find calledreach them.Figure 11: Sample Task ScenariosDialogue Eciency Metrics: elapsed time, system turns, user turnsDialogue Quality Metrics mean recognition score, timeouts, rejections, helps, cancels,bargeins, timeout%, rejection%, help%, cancel%, bargein%Task Success Metrics: task completion per surveyUser Satisfaction: sum TTS Performance, ASR Performance, Task Ease, InteractionPace, User Expertise, System Response, Expected Behavior, Comparable Interface, FutureUse.Figure 12: Metrics collected spoken dialogues.collect number different measures dialogue via four different methods:(1) dialogues recorded; (2) dialogue manager logs statesystem enters dialogue strategy Elvis selects state; (3) dialoguemanager logs information calculating number dialogue quality dialogue eciencymetrics summarized Figure 12 described detail below; (4) enddialogue, users fill web page forms support calculation task successuser satisfaction measures. explain use measures paradiseframework reinforcement learning.402fiReinforcement Learning ELVIS Systemdialogue eciency metrics calculated dialogue recordingssystem logs. length recording used calculate elapsed time seconds(ET) beginning end interaction. Measures number SystemTurns, number User Turns, calculated basis system loggingeverything said everything heard user say.dialogue quality measures derived recordings, system logshand-labeling. number system behaviors affect quality resulting dialogueautomatically logged. included number timeout prompts (timeouts)played user didn't respond quickly expected, number recognizerrejections (rejects) system's confidence understanding low saidsomething like I'm sorry didn't understand you. User behaviors system perceivedmight affect dialogue quality also logged: included number timessystem played one context specific help messages believeduser said Help (helps), number times system reset contextreturned earlier state believed user said Cancel (cancels).recordings used check whether users barged system utterances,labeled per-state basis (bargeins).Another measure dialogue quality recognizer performance whole dialogue,calculated terms concept accuracy. recording user's utterance comparedlogged recognition result calculate concept accuracy measure utterancehand. Concept accuracy measure semantic understanding system, ratherword word understanding. example, utterance Read messagesKim contains two concepts, read function, sender:kim selection criterion.system understood user said Read, concept accuracy would 0.5. Meanconcept accuracy calculated whole dialogue used, conjunctionASR rejections, compute Mean Recognition Score (MRS) dialogue.goal generate models performance generalize across systemstasks, also thought important introduce metrics likely generalize.eciency metrics seemed unlikely generalize since, e.g., elapsed timecomplete task depends dicult task is. research suggesteddialogue quality metrics likely generalize (Litman, Walker, & Kearns, 1999),thought raw counts likely task specific. Thus normalizeddialogue quality metrics dividing raw counts total number utterancesdialogue. resulted timeout%, rejection%, help%, cancel%, bargein%metrics.web page forms basis calculating Task Success User Satisfactionmeasures. Users reported perceptions whether completed task(Comp).5 also provide objective evidence fact completedtask filling form information acquired Elvis.65. Yes,No responses converted 1,0.6. supports alternative way calculating Task Success objectively using Kappa statisticcompare information users filled key task (Walker et al., 1997a). Howeverearlier results indicated user's perception task success better predictoroverall satisfaction, simply use perceived task success measured Comp.403fiWalkerELVIS easy understand conversation? (TTS Performance)conversation, ELVIS understand said? (ASR Performance)conversation, easy find message wanted? (Task Ease)pace interaction ELVIS appropriate conversation? (Interaction Pace)conversation, know could say point dialogue? (UserExpertise)often ELVIS sluggish slow reply conversation? (SystemResponse)ELVIS work way expected conversation? (Expected Behavior)conversation, ELVIS's voice interface compare touch-tone interfacevoice mail? (Comparable Interface)current experience using ELVIS get email, think you'd useELVIS regularly access mail away desk? (Future Use)Figure 13: User Satisfaction Surveyorder calculate User Satisfaction, users asked evaluate system's performance user satisfaction survey Figure 13. question responsesfive point Likert scale simply required yes, yes, no, maybe responses.survey questions probed number different aspects users' perceptionsinteraction Elvis order focus user task rating system,(Shriberg et al., 1992; Jack, Foster, & Stentiford, 1992; Love, Dutton, Foster, Jack, &Stentiford, 1994). multiple choice survey response mapped range 15. values responses summed, resulting User Satisfactionmeasure dialogue possible range 8 40.4. Training Testing Optimized Dialogue StrategyGiven experimental training data, first apply paradise estimate performancefunction Elvis linear combination metrics described above. applyperformance function dialogue training corpus estimate utilityfinal state dialogue apply Q-learning using utility. Finally testlearned policy new population users.4.1paradisePerformance Modelingfirst step developing performance model spoken dialogue systems specification causal model performance illustrated Figure 14 (Walker et al., 1997a).According model, system's primary objective maximize user satisfaction.404fiReinforcement Learning ELVIS SystemMAXIMIZE USER SATISFACTIONMAXIMIZE TASKSUCCESSMINIMIZE COSTSEFFICIENCYMEASURESFigure 14:QUALITATIVEMEASURES's structure objectives spoken dialogue performance.paradiseTask success various costs associated interaction contributors user satisfaction. Task success measured quantitatively numberways: could represented continuous variable representing quality solutionboolean variable representing binary task completion. Dialogue costs two types:dialogue eciency quality. Eciency costs measures system's eciencyhelping user complete task, number utterances completiondialogue. Dialogue quality costs intended capture aspects systemmay strong effects user's perception system, number timesuser repeat utterance order make system understand utterance.Given model, performance metric dialogue system estimatedexperimental data applying multivariate linear regression user satisfactiondependent variable task success, dialogue quality, dialogue eciency measuresindependent variables.7 stepwise linear regression training data measuresdiscussed above, showed Comp, MRS, BargeIn% Rejection% significantcontributors User Satisfaction, accounting 39% variance R-Squared (F(4,192)=30.7, p <.0001).8Performance = :27 Comp + :54 MRS , :09 BargeIn% + :15 Rejection%tested well performance function generalize unseen test dialoguestenfold cross-validation, randomly sampling 90% training dialoguestesting goodness fit performance model remaining 10% dialogues7. One advantage approach performance function derived, longer necessarycollect user satisfaction reports users, opens possibility estimating rewardfunction fully automatic measures. latter possibility might also useful online calculationreward function calculating local reward.8. normalize metrics regression magnitude coecients directlyindicates contribution factor User Satisfaction (Cohen, 1995; Walker et al., 1997a).405fiWalkertraining set. average R2 training set 37% standard error.005, average R2 held-out 10% dialogues 38% standarderror .06. Since average R2 test set statistically indistinguishabletraining set, assume performance model generalize new Elvis dialogues.4.2 Training Optimized PolicyGiven learned performance function described above, apply functionmeasures logged dialogue Di , thereby replacing range measures single performance value Pi , used utility (reward) final statedialogue.9 apply reinforcement learning Pi utility final statedialogue Di (Bellman, 1957; Sutton, 1991; Tesauro, 1992; Russell & Norvig, 1995;Watkins, 1989). utility action state Si , U (a; Si ) (its Q-value),calculated terms utility successor state Sj , obeying recursive equation:U (a; Si ) = R(a; Si ) + Mija maxU (a0 ; Sj )X0jR(a; Si ) immediate reward received action Si , strategyfinite set strategies admissable state Si, Mija probabilityreaching state Sj strategy selected state Si . experiments reported here,reward associated state, R(Si ), zero. addition, since reliable prioriprediction user action particular state possible (for example user may sayHelp speech recognizer may fail understand user), state transition modelMija estimated logged state-strategy history dialogue.utility values estimated within desired threshold using value iteration,updates estimate U (a; Si ), based updated utility estimates neighboringstates, equation becomes:Un+1 (a; Si ) = R(Si) +XMjijmaxUn(a0 ; Sj )0Un (a; Si ) utility estimate state Si n iterations (Sutton &Barto, 1998) pp. 101. Value iteration stops difference Un (a; Si )Un+1 (a; Si ) threshold, utility values associated statesstrategy selections made.10 value iteration completed optimal policyobtained selecting action maximal Q-value dialogue state.Figure 15 enumerates subset states aggregrated state space usedreinforcement learning potential actions defining policy space. strategygreatest Q-value state training indicated boldface Figure 15.optimized policy tested fixed policy operation Elvis.states task, System-Initiative strategy Figure 2 predictedoptimal initiative strategy, Read-First strategy Figure 5 predictedbest performance Read strategies. Figure 15 shows, learned strategy9. dialogue treated unique final state.10. experimenting various threshholds, used threshold 5% performance rangedialogues.406fiReinforcement Learning ELVIS SystemState VariablesU P G0 0 0 01 0 0 01 SI 01 SI 0 R1 SI 1 01 SI 11 SI 1 R1 SI 2 01 SI 21 SI 2 R1 MI 01 MI 0 R1 MI 1 01 MI 11 MI 1 R1 MI 2 01 MI 21 MI 2 RStrategy ChoicesAskUserNameSI-Top, MI-TopSS,SB,SCPRF,RSO,RCPSI-TopSS,SB,SCPRF,RSO,RCPSI-TopSS,SB,SCPRF,RSO,RCPSS,SB,SCPRF,RSO,RCPMI-TopSS,SB,SCPRF,RSO,RCPMI-TopSS,SB,SCPRF,RSO,RCPFigure 15: subset state space defines policy class explored experiments. learned policy indicated boldface.summarization varies according state task. different summarizationstrategies illustrated Figure 4. policy learned use SummarizeBoth strategy beginning dialogue (when TaskProg = 0), switchusing Summarize-System strategy later phases dialogue. strategy makessense terms giving user complete information messages inboxbeginning dialogue.4.3 Testing Optimized Policyfirst constructed deterministic version Elvis implemented learned policydiscussed above, one variation. variation based fact decisionwhether use Summarize-Both Summarize-System summarization strategyconditioned value TaskProg variable. However, intended utilize optimized version system situations would access TaskProgvariable, namely situations task user attempting performcontrol experimenter. examined Q-values summarization strategies course dialogue, found Summarize-System strategygreatest average Q-value, strongly preferred Summarize-Both strategyexcept initial phase dialogue, Q-value Summarize-Both407fiWalkerslightly greater. Thus implemented learned policy (see Figure 15),exception Summarize-System strategy used throughout dialogue.11terms operations state machine Figure 7, implementation learnedpolicy means choices SI-Top MI-Top strategies replacedSI-Top strategy, choices different read strategies different statesreplaced Read-First (RF) strategy choices different summarizationstrategies different states replaced Summarize-System (SS) strategy.tested policy six new users never used Elvis before.users conversed Elvis perform set six email tasks usedtraining phase, described Figure 10 above. addition, identical performancemeasures collected testing dialogue training dialogue. Overall performancemeasures training test dialogues given Table 1, training data splitterms System-Initiative, Mixed-Initiative overall means. table showsversions Elvis high levels task completion, important testingutility reinforcement learning. Statistical analysis results indicated statisticallysignificant increase User Satisfaction training test (F= = 4.07 p = .047).5. Discussion Future Workpaper proposes novel method dialogue system learn chooseoptimal dialogue strategy tests experiments Elvis, dialogue systemsupports access email phone, strategies initiative, reading summarizing messages. reported experiments Elvis learned System-Initiativestrategy higher utility Mixed-Initiative strategy, Read-First bestread strategy, Summarize-System generally best summary strategy.tested policy Elvis learned new set users performing settasks showed learned policy resulted statistically significant increaseuser satisfaction test set dialogues.Previous work also treated system's choice dialogue strategy stochasticoptimization problem (Walker, 1993; Biermann & Long, 1996; Levin & Pieraccini, 1997;Levin et al., 1997). knowledge, Walker (1993) first proposed reinforcementlearning algorithms could applied dialogue strategy selection. simulation experiments reported Walker (1993, 1996), dialogues two agents artificial worldused test dialogue strategies optimal various conditions.experiments varied: (1) dialogue agent's resource bounds; (2) performance function used assess agent's performance. experiments showed strategiesoptimal one set assumptions performance function couldhighly ecacious performance function ected fact dialogue agentresource bounded. Walker (1993) suggested optimal dialogue strategy could11. Obviously choice strategy test risked testing non-optimal policy. alternativewould like try future work utilize SummStrat state variable operations vectorstate representation reinforcement learning simply distinguish states summarizestrategy selected (no summary produced) states least one summaryproduced. analysis dialogue phase carries through, policylearned use Summarize-Both strategy first summary dialogue afterwardsuse Summarize-System strategy.408fiReinforcement Learning ELVIS SystemMeasure Train SI Train MI Overall Train TestComp.87.80.85.94User Turns21.517.020.0 25.8System Turns24.221.223.1 29.2Elapsed time (sec) 339.14296.18311.56 368.5Mean recognition score.88.72.82.81TimeOuts2.74.23.03.3TimeOut%.11.19.13.11Cancs.34.02.26.00Canc%.02.00.01.00Help Requests.67.920.66 1.11Help%.03.05.03.04BargeIns3.63.63.77.8BargeIn%.08.09.18.30Rejects.91.61.11.4Reject%.04.08.05.05User satisfaction28.925.027.5 31.7Table 1: Performance measure means per dialogue Training Testing Dialogues. SI= System-Initiative, MI = Mixed-Initiativelearned via reinforcement learning, appropriate performance function could determined, described experiment using genetic algorithms learn optimal dialoguestrategy. subsequent work, utilized here, paradise model proposed waylearn appropriate performance function (Walker et al., 1997a). addition, related workutilizing Elvis, varied reward function, applied reinforcement learningalgorithms, carried Fromer (Fromer, 1998).Biermann Long (1996), proposed use similar techniques contextlearning optimal dialogue strategies multi-modal dialogue tutor. goal tutorinstruct students taking first programming class tutor interactedstudents highlighting parts code printing text screen tellingwrong program. Biermann Long describe planned experimentsystem would vary instructional style, system's reward wouldamount time system's instructions student's response. rewardfunction based assumption delayed response suggested greater cognitiveload student, cognitive load minimized instructional setting.Levin colleagues also proposed treating dialogue systems Markov Decision Processes suggested system designers could determine appropriate objectivefunction might (Levin et al., 1997; Levin & Pieraccini, 1997). carried seriesexperiments simulated user interacted implemented spoken dialoguesystem travel planning exchanging messages semantic meaning level.showed system could learn strategy choices level database interaction,409fiWalkere.g., system query database determined manyconstraints necessary order find ights matched user's goals.Stochastic optimization techniques also applied similar problems textbased dialogue interaction graphical user interfaces. Mellish colleagues appliedstochastic optimization problem determining content structuresystem's utterances ILEX system, interactive museum tour guide (Mellish et al.,1998). work tested user population performance (reward)measure based heuristics good text plans formulated experts. Christensencolleagues applied genetic algorithms design graphical user interfaceautomated teller machine. goal automatically learn best layout sequenceinteraction screens intracting user (Christensen, Marks, & Shieber, 1994).work, Levin colleagues, user population simulated.Here, method optimizing dialogue strategy selection illustrated evaluating strategies managing initiative information presentation interactionhuman callers. applied paradise performance model derive empirically motivated performance function, combines subjective user preferences objectivesystem performance measures single function. would impossible predict priori dialogue factors uence usability dialogue system,degree. performance equation shows task success dialogue quality measuresprimary contributors system performance. Furthermore, contrast assuming priori model, use dialogues real user-system interactions providerealistic estimates Mija , state transition model used learning algorithm.impossible predict priori transition frequencies, given imperfect naturespoken language understanding, unpredictability user behavior.use method introduces several open issues possible areas futurework. First, results learning algorithm dependent representationstate space. spoken dialogue systems, system designers construct state spacedecide state variables system needs monitor, whereas applications reinforcement learning (e.g. backgammon), state space pre-defined.experiments reported here, fixed state representation carried experimentsparticular state representation. However future work hope able learnaspects state history represented using similar techniquesdescribed (Langkilde, Walker, Wright, Gorin, & Litman, 1999). example, maybeneficial system represent additional state variables representingdialogue history, order Elvis able learn dialogue strategies ectaspects dialogue history.Second, advance actually running experiments, clear much experiencesystem need determine strategy better. experiments reported here,able show improvement policy converged initiativeread strategies yet converged appropriate summarization strategy.possible local rewards nonzero optimal policy couldlearned less training data. future work, hope explore interactiontraining set size use local reward.Third, experimental data based fixing particular experimental parameters.experiments based short-term interactions novice users, might410fiReinforcement Learning ELVIS Systemexpect users email system would engage many interactionssystem, preferences system interaction strategies could change timeuser expertise. means performance function might change time. alsoused fixed set tasks representative domain, possibleaspects policies learned might sensitive experimental tasks. Anotherlimitation experiments carried scenario email foldersmall number messages: strategies tested might optimalemail folder contains hundreds messages.Fourth, optimal strategy potentially dependent various system parameters.example, ReadFirst strategy takes initiative read message, might resultmessages read user wasn't interested in, since user barge-insystem utterances, little overhead taking decision. systemsupport barge-in, results might different.Fifth, learned policy depends reward function. example, since Elvisfully functional system, users complete experimental task versionsystem using strategies explored. means usedtask completion reward function, reinforcement learning would predicteddifferences different strategies. hand, usingparadise performance function, utilized reward function fit dataElvis's performance, evidence reward function may generalizesystems (Walker, Kamm, & Litman, 2000).Sixth, experiments report limited way demonstrateutility reinforcement learning dialogue strategy optimization. traditionalway selecting best dialogue strategies would experiments treateddialogue strategy selection factor, standard statistical hypothesis testing wouldused compare performance different strategies. scale experimentsmall enough imaginable space policies could possiblytested traditional way. However, primary goal experiments reportedsimply test feasibility methods, required workingdetail many issues state strategy representation discussed above.many details worked out, methods presented appliedmuch complex dialogue strategy optimization problems, varying initiativedepending dialogue state (Chu-Carroll & Brown, 1997; Webber & Joshi, 1982),exploring combinations strategies information presentation, summarization (SparckJones, 1999), error recovery (Hirschman & Pao, 1993), database query (Levin et al., 1997),cooperative responses (Joshi, Webber, & Weischedel, 1986; Finin et al., 1986; Chu-Carroll &Carberry, 1994), content selection generation (McKeown, 1985; Kittredge, Korelsky,& Rambow, 1991), inter alia.Finally, learning algorithm report off-line algorithm, i.e. Elviscollects set dialogues decides optimal strategy result. contrast,possible Elvis learn on-line, course dialogue, methodsdeveloped performance function automatically calculated approximated.primary goal experiments reported explore applicationreinforcement learning spoken dialogue systems identify open issuesdiscussed above. current work, exploring issues several ways.411fiWalkercodified notion state estimator systematically vary staterepresentation order explore effect state representation value functionoptimal policy (Singh, Kearns, Litman, & Walker, 1999). also processusing reinforcement learning conduct set experiments spoken dialogue systemaccessing information activities New Jersey. experiments explorenumber different reward functions also explore much broader range strategiesuser initiative, reprompting user, confirming system's understanding.6. Acknowledgementsreceived many useful questions comments research presentedinitial results invited talk given AAAI 1997 Providence, R.I. designimplementation basic functionality Elvis done collaboration J. Fromer,G. DiFabbrizio, D. Hindle C. Mestel. Initial experiments reinforcement learningElvis done collaboration J. Fromer S. Narayanan. work alsobenefited discussions W. Eckert, C. Kamm, M. Kearns, E. Levin, D. Litman, D.McAllester, R. Pieraccini, R. Sutton, S. Singh. Special thanks S. Whittaker, J.Wiebe four reviewers detailed comments earlier versions manuscript.vReferencesAllen, J. F. (1979). Plan-Based Approach Speech Act Recognition. Tech. rep., University Toronto.Baggia, P., Castagneri, G., & Danieli, M. (1998). Field trials italian arise traintimetable system. Interactive Voice Technology Telecommunications Applications, IVTTA, pp. 97{102.Barto, A., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamicprogramming. Artificial Intelligence Journal, 72(1-2), 81{138.Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton, N.J.Biermann, A. W., & Long, P. M. (1996). composition messages speech-graphicsinteractive systems. Proceedings 1996 International Symposium SpokenDialogue, pp. 97{100.Bruce, B. (1975). Belief systems language understanding. Tech. rep. AI-21, Bolt,Berenak Newman.Carberry, S. (1989). Plan recognition use understanding dialogue. Kobsa,A., & Wahlster, W. (Eds.), User Models Dialogue Systems, pp. 133{162. SpringerVerlag, Berlin.Carbonell, J. R. (1971). Mixed-initiative man-computer dialogues. Tech. rep. 1970, BoltBeranek Newman, Cambridge, MA.412fiReinforcement Learning ELVIS SystemChristensen, J., Marks, J., & Shieber, S. (1994). Placing text labels maps diagrams.Heckbert, P. (Ed.), Graphics Gems IV. Academic Press.Chu-Carroll, J., & Brown, M. K. (1997). Tracking initiative collaborative dialogue interactions. Proceedings 35th Annual Meeting Association ComputationalLinguistics, pp. 262{270.Chu-Carroll, J., & Carberry, S. (1994). plan-based model response generationcollaborative task-oriented dialogue. AAAI 94, pp. 799{805.Cohen, P. R. (1995). Empirical Methods Artificial Intelligence. MIT Press, Boston.Cohen, P. R. (1978). knowing say: Planning speech acts. Tech. rep. 118,University Toronto; Department Computer Science.Danieli, M., & Gerbino, E. (1995). Metrics evaluating dialogue strategies spokenlanguage system. Proceedings 1995 AAAI Spring Symposium EmpiricalMethods Discourse Interpretation Generation, pp. 34{39.Finin, T. W., Joshi, A. K., & Webber, B. L. (1986). Natural language interactionsartificial experts. Proceedings IEEE, 74(7), 921{938.Fromer, J. C. (1998). Learning optimal discourse strategies spoken dialogue system.Tech. rep., MIT AI Lab M.S. Thesis.Grosz, B. J. (1983). Team: transportable natural language interface system. Proc. 1stApplied ACL, Association Computational Linguistics, Santa Monica, Ca.Grosz, B. J., & Sidner, C. L. (1986). Attention, intentions structure discourse.Computational Linguistics, 12, 175{204.Hirschman, L., & Pao, C. (1993). cost errors spoken language system. Proceedings Third European Conference Speech Communication Technology,pp. 1419{1422.Jack, M., Foster, J. C., & Stentiford, F. W. (1992). Intelligent dialogues automated telephone services. International Conference Spoken Language Processing, ICSLP,pp. 715 { 718.Joshi, A. K., Webber, B., & Weischedel, R. M. (1986). aspects default reasoninginteractive discourse. Tech. rep. MS-CIS-86-27, University Pennsylvania.Kamm, C., Narayanan, S., Dutton, D., & Ritenour, R. (1997). Evaluating spoken dialog systems telecommunication services. 5th European Conference SpeechTechnology Communication, EUROSPEECH 97, pp. 2203{2206.Kamm, C. (1995). User interfaces voice applications. Roe, D., & Wilpon, J.(Eds.), Voice Communication Humans Machines, pp. 422{442. NationalAcademy Press.413fiWalkerKeeney, R., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences ValueTradeoffs. John Wiley Sons.Kittredge, R., Korelsky, T., & Rambow, O. (1991). need domain communicationknowledge. Computational Intelligence, 7 (4), 305{314.Langkilde, I., Walker, M., Wright, J., Gorin, A., & Litman, D. (1999). Automatic predictionproblematic human-computer dialogues May Help You?. ProceedingsIEEE Workshop Automatic Speech Recognition Understanding, ASRUU99.Levin, E., & Pieraccini, R. (1997). stochastic model computer-human interactionlearning dialogue strategies. EUROSPEECH 97.Levin, E., Pieraccini, R., & Eckert, W. (1997). Learning dialogue strategies withinMarkov Decision Process framework. Proc. IEEE Workshop Automatic SpeechRecognition Understanding.Levin, E., Pieraccini, R., Eckert, W., Fabbrizio, G. D., & Narayanan, S. (1999). Spokenlanguage dialogue: theory practice. Proc. IEEE Workshop AutomaticSpeech Recognition Understanding, ASRUU99.Litman, D. (1985). Plan recognition discourse analysis: integrated approachunderstanding dialogues. Tech. rep. 170, University Rochester.Litman, D. J., Walker, M. A., & Kearns, M. J. (1999). Automatic detection poor speechrecognition dialogue level. Proceedings Thirty Seventh Annual MeetingAssociation Computational Linguistics, pp. 309{316.Love, S., Dutton, R. T., Foster, J. C., Jack, M. A., & Stentiford, F. W. M. (1994). Identifying salient usability attributes automated telephone services. InternationalConference Spoken Language Processing, ICSLP, pp. 1307{1310.McKeown, K. R. (1985). Discourse strategies generating natural language text. ArtificialIntelligence, 27 (1), 1{42.Mellish, C., Knott, A., Oberlander, J., & O'Donnell, M. (1998). Experiments using stochastic search text planning. Proceedings International Conference NaturalLanguage Generation, pp. 97{108.Mohri, M., Pereira, F. C. N., & Riley, M. D. (1998). Fsm library { general purpose finitestate machine software tools..Moore, J. D., & Paris, C. L. (1989). Planning text advisory dialogues. Proc. 27thAnnual Meeting Association Computational Linguistics.Pollack, M., Hirschberg, J., & Webber, B. (1982). User participation reasoning processexpert systems. Proceedings First National Conference Artificial Intelligence,pp. pp. 358{361.Power, R. (1974). Computer Model Conversation. Ph.D. thesis, University Edinburgh.414fiReinforcement Learning ELVIS SystemRabiner, L. R., Juang, B. H., & Lee, C. H. (1996). overview automatic speechrecognition. Lee, C. H., Soong, F. K., & Paliwal, K. K. (Eds.), Automatic SpeechSpeaker Recognition, Advanced Topics, pp. 1{30. Kluwer Academic Publishers.Russell, S., & Norvig, P. (1995). Artificial Intelligence: Modern Approach. Prentiss Hall,Englewood Cliffs, N.J.Sanderman, A., Sturm, J., den Os, E., Boves, L., & Cremers, A. (1998). Evaluationdutchtrain timetable information system developed ARISE project.Interactive Voice Technology Telecommunications Applications, IVTTA, pp. 91{96.Seneff, S., Zue, V., Polifroni, J., Pao, C., Hetherington, L., Goddeau, D., & Glass, J. (1995).preliminary development displayless PEGASUS system. ARPA SpokenLanguage Technology Workshop.Shriberg, E., Wade, E., & Price, P. (1992). Human-machine problem solving using spoken language systems (SLS): Factors affecting performance user satisfaction.Proceedings DARPA Speech NL Workshop, pp. 49{54.Simmons, R., & Slocum, J. (1975). Generating english discourse semantic networks.CACM, 15 (10), 891{905.Singh, S., Kearns, M. S., Litman, D. J., & Walker, M. A. (1999). Reinforcement learningspoken dialogue systems. Proc. NIPS99.Smith, R. W., & Hipp, D. R. (1994). Spoken Natural Language Dialog Systems: PracticalApproach. Oxford University Press.Sparck-Jones, K. (1993). might summary?. Proceedings InformationRetrieval 93: Von der Modellierung zur Anwendung, pp. 9{26 Universitatsverlag Knstanz.Sparck-Jones, K. (1999). Automatic summarizing; factors directions. Mani, I., &Maybury, M. (Eds.), Advances Automatic Text Summarization. MIT Press.Sparck-Jones, K., & Galliers, J. R. (1996). Evaluating Natural Language Processing Systems.Springer.Sproat, R., & Olive, J. (1995). approach text-to-speech synthesis. Kleijn, W. B.,& Paliwal, K. K. (Eds.), Speech Coding Synthesis, pp. 611{633. Elsevier.Sutton, R. S. (1991). Planning incremental dynamic programming. Proceedings NinthConference Machine Learning, pp. 353{357. Morgan-Kaufmann.Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning. MIT Press.Tesauro, G. (1992). Practical Issues Temporal Difference Learning. Machine Learning,8 (3{4), 257{277.Walker, D. (1978). Understanding Spoken Language. Elsevier, North-Holland, New York.415fiWalkerWalker, M., Fromer, J., Fabbrizio, G. D., Mestel, C., & Hindle, D. (1998). say:Evaluating spoken language interface email. Proceedings ConferenceComputer Human Interaction (CHI 98), pp. 582{589.Walker, M. A., Litman, D., Kamm, C. A., & Abella, A. (1997a). PARADISE: generalframework evaluating spoken dialogue agents. Proceedings 35th AnnualMeeting Association Computational Linguistics, ACL/EACL 97, pp. 271{280.Walker, M., Hindle, D., Fromer, J., Fabbrizio, G. D., & Mestel, C. (1997b). Evaluatingcompeting agent strategies voice email agent. Proceedings EuropeanConference Speech Communication Technology, EUROSPEECH97.Walker, M. A. (1993). Informational Redundancy Resource Bounds Dialogue. Ph.D.thesis, University Pennsylvania.Walker, M. A. (1996). Effect Resource Limits Task Complexity CollaborativePlanning Dialogue. Artificial Intelligence Journal, 85 (1{2), 181{243.Walker, M. A., Fromer, J. C., & Narayanan, S. (1998). Learning optimal dialogue strategies: case study spoken dialogue agent email. Proceedings 36thAnnual Meeting Association Computational Linguistics, COLING/ACL 98,pp. 1345{1352.Walker, M. A., Kamm, C. A., & Litman, D. J. (2000). Towards developing general modelsusability PARADISE. Natural Language Engineering: Special Issue BestPractice Spoken Dialogue Systems.Walker, M. A., & Whittaker, S. (1990). Mixed initiative dialogue: investigationdiscourse segmentation. Proc. 28th Annual Meeting ACL, pp. 70{79.Watkins, C. J. (1989). Models Delayed Reinforcement Learning. Ph.D. thesis, CambridgeUniversity.Webber, B., & Joshi, A. (1982). Taking initiative natural language database interaction: Justifying why. Coling 82, pp. 413{419.Winograd, T. (1972). Understanding Natural Language. Academic Press, New York, N.Y.Woods, W. A. (1984). Natural language communication machines: ongoing goal.Reitman, W. (Ed.), Artificial Intelligence Applications Business, pp. 195{209.Ablex Publishing Corp, Norwood, N.J.416fiJournal Artificial Intelligence Research 12 (2000) 134Submitted 7/99; published 2/00Planning Graph (Dynamic) CSP:Exploiting EBL, DDB CSP Search Techniques GraphplanSubbarao KambhampatiRAO @ ASU . EDUDepartment Computer Science EngineeringArizona State University, Tempe AZ 85287-5406Abstractpaper reviews connections Graphplans planning-graph dynamicconstraint satisfaction problem motivates need adapting CSP search techniquesGraphplan algorithm. describes explanation based learning, dependency directed backtracking, dynamic variable ordering, forward checking, sticky values random-restart searchstrategies adapted Graphplan. Empirical results provided demonstrateaugmentations improve Graphplans performance significantly (up 1000x speedups)on severalbenchmark problems. Special attention paid explanation-based learning dependencydirected backtracking techniques empirically found useful improvingperformance Graphplan.1. IntroductionGraphplan (Blum & Furst, 1997) currently one efficient algorithms solving classical planning problems. Four five competing systems recent AIPS-98 planning competition based Graphplan algorithm (McDermott, 1998). Extending efficiencyGraphplan algorithm thus seems worth-while activity. (Kambhampati, Parker, &Lambrecht, 1997), provided reconstruction Graphplan algorithm explicate linksprevious work classical planning constraint satisfaction. One specific link discussedconnection process searching Graphplans planning graph, solving dynamic constraint satisfaction problem (DCSP) (Mittal & Falkenhainer, 1990). Seen DCSPperspective, standard backward search proposed Blum Furst (1997) lacks variety ingredients thought make efficient CSP search mechanisms (Frost & Dechter, 1994;Bayardo & Schrag, 1997). include forward checking, dynamic variable ordering, dependency directed backtracking explanation-based learning (Tsang, 1993; Kambhampati, 1998).(Kambhampati et al., 1997), suggested would beneficial study impactextensions effectiveness Graphplans backward search.paper, describe experiences adding variety CSP search techniques improve Graphplan backward searchincluding explanation-based learning (EBL) dependencydirected backtracking capabilities (DDB), Dynamic variable ordering, Forward checking, stickyvalues, random-restart search strategies. these, addition EBL DDB capabilitiesturned empirically useful. EBL DDB based explaining failuresleaf-nodes search tree, propagating explanations upwards searchtree (Kambhampati, 1998). DDB involves using propagation failure explanations supportintelligent backtracking, EBL involves storing interior-node failure explanations, pruningfuture search nodes. Graphplan use weak form failure-driven learning calls mem-c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiK AMBHAMPATIoization. shall see paper, Graphplans brand learning quite limitedexplicit analysis reasons failure. Instead explanation failure search nodetaken constraints search node. explained (Kambhampati, 1998),eliminates opportunities dependency directed backtracking, also adversely effectsutility stored memos.Adding full-fledged EBL DDB capabilities effect gives Graphplan abilityintelligent backtracking, ability learn generalized memos likelyapplicable situations. Technically, involves generalizing conflict-directed backjumping(Prosser, 1993), specialized version EBL/DDB strategy applicable binary CSP problems1work context dynamic constraint satisfaction problems (as discussed (Kambhampati, 1998)). Empirically, EBL/DDB capabilities improve Graphplans search efficiency quitedramaticallygiving rise 1000x speedups, allowing Graphplan easily solve severalproblems hither-to hard unsolvable. particular, report experimentsbench-mark problems described Kautz Selman (1996), well 4 domains,used recent AIPS planning competition (McDermott, 1998).discuss utility issues involved storing using memos, point Graphplanmemoization strategy seen conservative form CSP no-good learning.conservative strategy keeps storage retrieval costs no-goods usual bane no-goodlearning strategiesunder control, also loses learning opportunities. present usesticky values way recouping losses. Empirical studies show stickyvalues lead 2-4x improvement EBL.addition EBL DDB, also investigated utility forward checking dynamicvariable ordering, isolation concert EBL DDB. empirical studies showcapabilities typically lead additional 2-4x speedup EBL/DDB,competitive EBL/DDB.Finally, consider utility EBL/DDB strategies context random-restart searchstrategies (Gomes, Selman, & Kautz, 1998) recently shown good solving hard combinatorial problems, including planning problems. results show EBL/DDBstrategies retain advantages even context random-restart strategies. Specifically,EBL/DDB strategies enable Graphplan use backtrack limits effectivelyallowingachieve higher solvability rates, optimal plans significantly smaller backtrackrestart limits.paper organized follows. next section, provide background viewingGraphplans backward search (dynamic) constraint satisfaction problem, reviewopportunities view presents. Section 3, discuss inefficiencies backtrackinglearning methods used normal Graphplan motivate need EBL/DDB capabilities.Section 4 describes EBL DDB added Graphplan. Section 5 presents empirical studiesdemonstrating usefulness augmentations. Section 7 investigates utility forwardchecking dynamic variable ordering strategies Graphplan. Section 8 investigates utilityEBL/DDB strategies context random-restart search. Section 9 discusses related workSection 10 presents conclusions directions work.1. Binary CSP problems problems initial constraints pairs variables.2fiP LANNING G RAPHAction listLevel k-1Proposition listLevel k-1Action listLevel kProposition ListLevel kG1 ; ; G4 ; P1 P6Domains: G1 : fA1 g; G2 : fA2 gG3 : fA3 gG4 : fA4 gP1 : fA5 gP2 : fA6 ; A11 gP3 : fA7 gP4 : fA8 ; A9 gP5 : fA10 gP6 : fA10 gConstraints (normal):P1 = A5 ) P4 6= A9P2 = A6 ) P4 6= A8P2 = A11 ) P3 6= A7Constraints (Activity): G1 = A1 ) ActivefP1 ; P2 ; P3 gG2 = A2 ) ActivefP4 gG3 = A3 ) ActivefP5 gG4 = A4 ) ActivefP1 ; P6 gInit State: ActivefG1 ; G2 ; G3 ; G4 gP1A6A1XA7A8P311A2G2G3P4P510G1P2A9XCSPVariables:A5XA3G4P6A4(a) Planning Graph(b) DCSPFigure 1: planning graph DCSP corresponding2. Review Graphplan Algorithm Connections DCSP2.1 Review Graphplan AlgorithmGraphplan algorithm (Blum & Furst, 1997) seen disjunctive version forwardstate space planners (Kambhampati et al., 1997; Kambhampati, 1997). consists two interleavedphases forward phase, data structure called planning-graph incrementally extended,backward phase planning-graph searched extract valid plan. planninggraph consists two alternating structures, called proposition lists action lists. Figure 1 showspartial planning-graph structure. start initial state zeroth level proposition list.Given k level planning graph, extension structure level k + 1 involves introducingactions whose preconditions present k th level proposition list. addition actionsgiven domain model, consider set dummy persist actions, one conditionk th level proposition list. persist-C action C precondition C effect.actions introduced, proposition list level k + 1 constructed unioneffects introduced actions. Planning-graph maintains dependency linksactions level k + 1 preconditions level k proposition list effects level k + 1proposition list. planning-graph construction also involves computation propagationmutex constraints. propagation starts level 1, actions statically interfering(i.e., preconditions effects inconsistent) labeled mutex. Mutexespropagated level forward using two simple rules: two propositions level kmarked mutex actions level k support one proposition mutex actionssupport second proposition. Two actions level k + 1 mutex statically interferingone propositions (preconditions) supporting first action mutually exclusiveone propositions supporting second action.search phase k level planning-graph involves checking see sub-graphplanning-graph corresponds valid solution problem. involves startingpropositions corresponding goals level k (if goals present,present pair marked mutually exclusive, search abandoned right away,planning-grap grown another level). goal propositions, select action3fiK AMBHAMPATIG1 ; ; G4 ; P1 P6G1 : fA1 g; G2 : fA2 gG3 : fA3 gG4 : fA4 gP1 : fA5 gP2 : fA6 ; A11 gP3 : fA7 gP4 : fA8 ; A9 gP5 : fA10 gP6 : fA10 gConstraints (normal):P1 = A5 ) P4 6= A9P2 = A6 ) P4 6= A8P2 = A11 ) P3 6= A7Constraints (Activity): G1 = A1 ) ActivefP1 ; P2 ; P3 gG2 = A2 ) ActivefP4 gG3 = A3 ) ActivefP5 gG4 = A4 ) ActivefP1 ; P6 gInit State: ActivefG1 ; G2 ; G3 ; G4 gG1 ; ; G4 ; P1 P6G1 : fA1 ; ?g; G2 : fA2 ; ?gG3 : fA3 ; ?gG4 : fA4 ; ?gP1 : fA5 ; ?gP2 : fA6 ; A11 ; ?gP3 : fA7 ; ?gP4 : fA8 ; A9 ; ?gP5 : fA10 ; ?gP6 : fA10 ; ?gConstraints (normal):P1 = A5 ) P4 6= A9P2 = A6 ) P4 6= A8P2 = A11 ) P3 6= A7Constraints (Activity): G1 = A1 ) P1 6=? ^P2 6=? ^P3 6=?G2 = A2 ) P4 6=?G3 = A3 ) P5 6=?G4 = A4 ) P1 6=? ^P6 6=?Init State: G1 6=? ^G2 6=? ^G3 6=? ^G4 6=?Variables:Variables:Domains:Domains:(a) DCSP(b) CSPFigure 2: Compiling DCSP standard CSPlevel k action list supports it, two actions selected supporting twodifferent goals mutually exclusive (if are, backtrack try change selectionactions). point, recursively call search process k , 1 level planning-graph,preconditions actions selected level k goals k , 1 level search.search succeeds reach level 0 (corresponding initial state).Consider (partial) planning graph shown Figure 3 Graphplan may generatedsearch solution. G1 G4 top level goals want satisfy,A1 A4 actions support goals planning graph. specific actionprecondition dependencies shown straight line connections. actions A5 A11left-most level support conditions P1 P6 planning-graph. Notice conditions P2P4 level k , 1 supported two actions each. x-marked connectionsactions A5 ; A9 , A6 ; A8 A7 ; A11 denote action pairs mutually exclusive. (Noticegiven mutually exclusive relations alone, Graphplan cannot derive mutual exclusionrelations proposition level P1 P6 .)2.2 Connections Graphplan CSPGraphplan algorithm described bears little resemblance previous classical planningalgorithms. (Kambhampati et al., 1997), explicate number important linksGraphplan algorithm previous work planning constraint satisfaction communities.Specifically, show planning-graph length k thought (to first approximation)disjunctive (unioned) version k -level search tree generated forward state-space refinement,action lists corresponding union actions appearing k th level, propositionlists corresponding union states appearing k th level. mutex constraintsseen providing (partial) information subsets proposition list actuallycorrespond legal states corresponding forward state-space search. process searchingplanning graph extract valid plan seen dynamic constraint satisfactionproblem. Since last link relevant work described paper, reviewbelow.dynamic constraint satisfaction problem (DCSP) (Mittal & Falkenhainer, 1990) generalization constraint satisfaction problem (Tsang, 1993), specified set variables,4fiP LANNING G RAPHCSPactivity flags variables, domains variables, constraints legal variablevalue combinations. DCSP, initially subset variables active, objectivefind assignments active variables consistent constraints among variables. addition, DCSP specification also contains set activity constraints. activityconstraint form: variable x takes value vx , variables y; z; w::: becomeactive.correspondence planning-graph DCSP clear. Specifically, propositions various levels correspond DCSP variables2 , actions supporting correspond variable domains. three types constraints: action mutexconstraints, fact (proposition) mutex constraints subgoal activation constraints.Since actions modeled values rather variables, action mutex constraintsmodeled indirectly constraints propositions. two actions a1 a2 marked mutexplanning graph, every pair propositions p11 p12 a1one possible supporting actions p11 a2 one possible supporting actionsp12 , constraint:: (p11 = a1 ^ p12 = a2 )Fact mutex constraints modeled constraints prohibit simultaneous activationtwo facts. Specifically, two propositions p11 p12 marked mutex planning graph,constraint:: (Active(p11 ) ^ Active(p12 ))Subgoal activation constraints implicitly specified action preconditions: supportingactive proposition p action makes propositions previous level correspondingpreconditions active.Finally, propositions corresponding goals problem active beginning. Figure 1 shows dynamic constraint satisfaction problem corresponding exampleplanning-graph discussed.2.2.1 OLVINGDCSPtwo ways solving DCSP problem. first, direct, approach (Mittal & Falkenhainer,1990) involves starting initially active variables, finding satisfying assignmentthem. assignment may activate new variables, newly activated variablesassigned second epoch. process continues reach epoch newvariables activated (which implies success), unable give satisfying assignmentactivated variables given epoch. latter case, backtrack previous epochtry find alternative satisfying assignment variables (backtracking further,assignment possible). backward search process used Graphplan algorithm (Blum &Furst, 1997) seen solving DCSP corresponding planning graph directfashion.second approach solving DCSP first compile standard CSP, usestandard CSP algorithms. compilation process quite straightforward illustrated2. Note literal appearing different levels corresponds different DCSP variables. Thus, strictly speaking, literal p proposition list level converted DCSP variable pi . keep matters simple,example Figure 1 contains syntactically different literals different levels graph.5fiK AMBHAMPATIFigure 2. main idea introduce new null value (denoted ?) domainsDCSP variables. model inactive DCSP variable CSP variabletakes value ?. constraint particular variable P active modeled P 6=?. Thus,activity constraint formG1 = A1 ) ActivefP1 ; P2 ; P3 gcompiled standard CSP constraintG1 = A1 ) P1 6=? ^P2 6=? ^P3 6=?worth noting activation constraints concerned ensuringpropositions preconditions selected action take non-? values. thus allowpossibility propositions become active (take non-? values) even thoughstrictly supporting preconditions selected action. Although lead inoptimalplans, mutex constraints ensure unsound plans produced (Kautz & Selman,1999). avoid unnecessary activation variables, need add constraints effectunless one actions needing variable precondition selected valuevariable earlier (higher) level, variable must ? value. constraintstypically going high arity (as wind mentioning large number variablesprevious level), may thus harder handle search.Finally, mutex constraint two propositions: (Active(p11 ) ^ Active(p12 ))compiled: (p11 6=? ^p12 6=?) :Since action mutex constraints already standard CSP form, compilation,activity constraints converted standard constraints thus entire CSPstandard CSP. solved standard CSP search techniques (Tsang, 1993).3direct method advantage closely mirrors Graphplans planning graphstructure backward search. this, possible implement approachplan graph structure without explicitly representing constraints. Furthermore, discuss Section 6, distinct advantages adopting DCSP view implementingEBL/DDB Graphplan. compilation CSP requires plan graph first convertedextensional CSP. however allow use standard algorithms, well supports nondirectional search (in one follow epoch-by-epoch approach assigningvariables).4 Since main aim illustrate utility CSP search techniques contextGraphplan algorithm, adopt direct solution method DCSP. studytradeoffs offered technique compiling planning graph CSP, reader referred(Do & Kambhampati, 2000).3. also possible compile CSP problem propositional satisfiability problem (i.e., CSP problemboolean variables). accomplished compiling every CSP variable P domain fv1 ; v2 ; ; vn gn boolean variables form P-is-v1 P-is-vn . Every constraint form Pvj ^ ) compiledP-is-vj ^ ) . essentially done BLACKBOX system (Kautz & Selman, 1999).4. Compilation CSP strict requirement non-directional search. (Zimmerman & Kambhampati,1999), describe technique allows backward search Graphplan non-directional, see discussionSection 10.=6fiP LANNING G RAPHCSP2.3 Interpreting Mutex Propagation CSP ViewViewing planning graph constraint satisfaction problem helps put mutex propagationclearer perspective (see (Kambhampati et al., 1997)). Specifically, way Graphplan constructs planning graph, winds enforcing partial directed 1-consistency partial directed2-consistency (Tsang, 1993). partial 1-consistency ensured graph building procedureintroduces action level l actions preconditions present propositionlist level l , 1 mutually exclusive. Partial 2-consistency ensured mutualexclusion propagation procedure.particular, Graphplan planning graph construction implicitly derives no-good5 constraints form::Active(Pmi ) (or Pmi 6=?)simply removed (or put into) level i, mutexcase Pmconstraints form:: Active(Pmi ) ^ Active(Pni )Pmi 6=? ^Pni 6=?)(P marked mutually exclusive.case Pmnprocedures directed use reachability analysis enforcing consistency, partial enforce either full 1-consistency full 2-consistency.Lack full 1-consistency verified fact appearance goal level knecessarily mean goal actually achievable level k (i.e., solution CSPassigns non- ? value goal level). Similarly, lack full 2-consistency verified fact appearance pair goals level k imply planachieving goals level.another, somewhat less obvious, way consistency enforcement usedGraphplan partial (and conservative)it concentrates whether single goal variablepair goal variables simultaneously non- ? values (be active) solution. maygoal non- ? value, non- ? values feasible. Similarly, maypair goals achievable, necessarily achievable every possible pair actionsrespective domains.interpretation mutex propagation procedure Graphplan brings fore several possibleextensions worth considering Graphplan:1. Explore utility directional consistency enforcement procedures based solelyreachability analysis. Kambhampati et. al. (1997) argue extending analysis usingrelevance information, et. al. (2000) provide empirical analysis effectivenessconsistency enforcement relevance information.2. Explore utility enforcing higher level consistency. pointed (Kambhampatiet al., 1997; Kambhampati, 1998), memoization strategies seen failure-drivenprocedures incrementally enforce partial higher level consistency.5. Normally, CSP literature, no-good seen compound assignment part feasible6 ? ^Pni 6 ? correspond conjunction nogoodssolution. view, mutex constraints form PmPni .form Pm au ^ Pn av au av values domains Pm===7=fiK AMBHAMPATI3. Consider relaxing focus non- ? values alone, allow derivation no-goodsformPmi = au ^ Pni = avguaranteed winning idea number derived no-goods increasequite dramatically. particular, assuming l levels planning graph,average goals per level, average actions supporting goal, maximumnumber Graphplan style pair-wise mutexes (l m2 ) 2-size no-goodstype discussed (l (m (d + 1))2 ). consider similar issue contextGraphplan memoization strategy Section 6.3. Inefficiencies Graphplans Backward Searchmotivate need EBL DDB, shall first review details Graphplans backwardsearch, pinpoint inefficiencies. shall base discussion example planninggraph Figure 3 (which reproduced convenience Figure 1). Assuming G1 G4top level goals problem interested solving, start level k , selectactions support goals G1 G4 . keep matters simple, shall assume searchassigns conditions (variables) level top bottom (i.e., G1 first, G2on). Further, choice actions (values) support condition,consider top actions first. Since one choice conditions level,none actions mutually exclusive other, select actions A1 ; A2 ; A3A4 supporting conditions level k . make sure preconditionsA1 ; A2 ; A3 ; A4 satisfied level k , 1. thus subgoal conditions P1 P6 levelk , 1, recursively start action selection them. select action A5 P1 . P2 ,two supporting actions, using convention, select A6 first. P3 , A7choice. get selecting support P4 , choice. Supposeselect A8 first. find choice infeasible A8 mutually exclusive A6already chosen. So, backtrack choose A9 , find mutually exclusivepreviously selected action, A5 . stymied choices P4 . So,backtrack undo choices previous conditions. Graphplan uses chronologicalbacktracking approach, whereby, first tries see P3 re-assigned, P2 on.Notice first indication inefficiency failure assign P4 nothingassignment P3 , yet, chronological backtracking try re-assign P3 vain hopeaverting failure. lead large amount wasted effort case P3indeed choices.turns out, find P3 choices backtrack it. P2 anotherchoice A11 . try continue search forward value P2 , hit impasse P3since value P3 , A7 mutex A11 . point, backtrack P3 , continuebacktracking P2 P1 , remaining choices. backtrackP1 , need go back level k try re-assign goals level. done,Graphplan search algorithm makes memo signifying fact failed satisfy goalsP1 P6 level, hope search ever subgoals set goalsfuture, scuttle right away help remembered memo. secondindication inefficiency remembering subgoals P1 P6 even though seeproblem lies trying assign P1 ; P2 ; P3 P4 simultaneously, nothing8fiP LANNING G RAPHAction listLevel k-1Proposition listLevel k-1CSPAction listLevel kProposition ListLevel kA5P1A6XA1XA7A8P2A2P3G3A3P510G2P4A9XG1G4P6A411Figure 3: running example used illustrate EBL/DDB Graphplansubgoals. remember fP1 ; P2 ; P3 ; P4 g memo fP1 P6 g,remembered memo would general, would much better chance usefulfuture.memo stored, backtracking continues level k chronologicalfashion, trying reassign G4 ; G3 ; G2 G1 order. see third indication inefficiency caused chronological backtracking G3 really role failure encounteredassigning P3 P4 since spawns condition P5 level k , 1. Yet, backtrackingscheme Graphplan considers reassigning G3 . somewhat subtle point reassigningG4 going avert failure either. Although G4 requires P1 one conditions takingpart failure, P1 also required G1 unless G1 gets reassigned, consideringassignments G4 going avert failure.example, continue backtracking G2 G1 too, since alternative supports, finally memoize fG1 ; G2 ; G3 ; G4 g level. point backward searchfails, Graphplan extends planning graph another level re-initiating backwardsearch extended graph.4. Improving Backward Search EBL DDBdescribe Graphplans backward search augmented full fledged EBLDDB capabilities eliminate inefficiencies pointed previous section. Informally,EBL/DDB strategies involve explanation failures leaf nodes, regression propagationleaf node failure explanations compute interior node failure explanations, along lines described (Kambhampati, 1998). specific extensions propose backward search9fiK AMBHAMPATIessentially seen adapting conflict-directed backjumping strategy (Prosser, 1993), generalizing work dynamic constraint satisfaction problems.algorithm shown pseudo-code form Figure 4. contains two mutually recursiveprocedures find-plan assign-goals. former called levelplanning-graph. calls assign-goals assign values required conditionslevel. assign-goals picks condition, selects value it, recursively callsremaining conditions. invoked empty set conditions assigned, callsfind-plan initiate search next (previous) level.order illustrate EBL/DDB capabilities added, lets retrace previous example,pick point assign P4 level k , 1, assigned P1 ; P2P3 . try assign value A8 P4, violate mutex constraint A6 A8.explanation failure search node set constraints False derived.complete explanation failure thus stated as:P2 = A6 ^ P4 = A8 ^ (P2 = A6 ) P4 6= A8 )this, part P2 = A6 ) P4 6= A8 stripped explanation since mutualexclusion relation hold long solving particular problem particularactions. Further, take cue conflict directed backjumping algorithm (Prosser,1993), represent remaining explanation compactly terms conflict sets. Specifically,whenever search reaches condition c (and find assignment it), conflictset initialized fcg. Whenever one possible assignments c inconsistent (mutuallyexclusive) current assignment previous variable c0 , add c0 conflict set c.current example, start fP4 g conflict set P4 , expand adding P2find A8 cannot assigned P4 choice A6 support P2 . Informally,conflict set representation seen incrementally maintained (partial) explanationfailure, indicating conflict current value P2 one possiblevalues P4 (Kambhampati, 1998).consider second possible value P4 , viz., A9 , find mutually exclusiveA5 currently supporting P1 . Following practice, add P1 conflict setP4 . point, choices P4 , backtrack P4 , passingconflict set P4 , viz., fP1 ; P2 ; P4 g reason failure. essence, conflict setshorthand notation following complete failure explanation (Kambhampati, 1998):6[(P4 = A8 ) _ (P4 = A9 )] ^ (P1 = A5 ) P4 6= A9 ) ^ (P2 = A6 ) P4 6= A8 ) ^ P1 = A5 ^ P2 = A6worth noting point P4 revisited future different assignmentspreceding variables, conflict set re-initialized fP4 g considering assignments it.first advantage conflict set allows transparent way supporting dependency directed backtracking (Kambhampati, 1998). current example, failed assignP4 , start backtracking. need chronological fashion however.6. strip first (disjunctive) clause since present graph structure, next two implicative clausessince part mutual exclusion relations change problem. conflict set representation keeps condition (variable) names last two clauses denoting, essence, currentassignments variables P1 P2 causing failure assign P4 .10fiP LANNING G RAPHCSPFind-Plan(G:goals, pg : plan graph, k : level)k = 0, Return empty subplan P success.memo G,Fail, return conflict setCall Assign-goals(G; pg; k; ;).Assign-goals fails returns conflict set ,Store memoRegress actions selected level k + 1 get RFail return R conflict setAssign-goals succeeds, returns k -level subplan P ,Return P successAssign-goals(G:goals, pg : plan graph, k : level, A: actions)G = ;Let U union preconditions actionsCall Find-plan(U; pg; k , 1)Find-plan fails returns conflict set R,Fail return RFind-plan succeeds returns subplan P length k , 1Succeed return k length subplan PElse ;;(G 6= ;)Select goal g 2 GLet csfgg, Ag set actions level k pg support gL1:Ag = ;, Fail return cs conflict setAg ,Else, pick action 2 Ag , set Agmutually exclusive action b 2Let l goal b selected supportcs [ flgSet csGoto L1Else (a mutually exclusive action A)Call Assign-goals(G , fg g; pg; k; [ fag)call fails returns conflict set Cg 2 CSet cs = cs [ C ;conflict set absorptionGoto L1Else ;(g 62 C )Fail return C conflict set;dependency directed backjumpingFigure 4: pseudo-code description Graphplan backward search enhanced EBL/DDB capabilities. backward search level k planning-graph pg initiated callFind-Plan(G; pg; k), G set top level goals problem.11fiK AMBHAMPATIInstead, jump back recent variable (condition) taking part conflict set P4case P2 . so, avoiding considering alternatives P3 , thus avoidingone inefficiencies standard backward search. easy see backjumpingsound since P3 causing failure P4 thus re-assigning wont avert failure.Continuing along, whenever search backtracks condition c, backtrack conflictabsorbed current conflict set c. example, absorb fP1 ; P2 ; P4 g conflictset P2 , currently fP2 g (making fP1 ; P2 ; P4 g new conflict set P2 ). assignA11 , remaining value, P2 . Next try assign P3 find value A7mutex A11 . Thus, set conflict set P3 fP3 ; P2 g backtrack conflictset. backtracking reaches P2 , conflict set absorbed current conflict setP2 (as described earlier), giving rise fP1 ; P2 ; P3 ; P4 g current combined failure reasonP2 . step illustrates conflict set condition incrementally expanded collectreasons failure various possible values condition.point, P2 choices, backtrack P2 current conflict set,fP1 ; P2 ; P3 ; P4 g. P1 , first absorb conflict set fP1 ; P2 ; P3 ; P4 g P1 current conflictset, re-initiate backtracking since P1 choices.Now, reached end current level (k , 1). backtracking P1 mustinvolve undoing assignments conditions k th level. however,two steps: memoization regression.4.1 Memoizationbacktrack first assigned variable given level, store conflict setvariable memo level. store conflict set fP1 ; P2 ; P3 ; P4 g P1 memolevel. Notice memo store shorter (and thus general) one storednormal Graphplan, include P5 P6 , anythingfailure74.2 Regressionbacktrack level k , 1 level k , need convert conflict set (the firstassigned variable in) level k , 1 refers conditions level k . conversionprocess involves regressing conflict set actions selected k th level (Kambhampati,1998). essence, regression step computes (smallest) set conditions (variables)kth level whose supporting actions spawned (activated, DCSP terms) conditions (variables)conflict set level k , 1. current case, conflict set fP1 ; P2 ; P3 ; P4 g.see P2 , P3 required condition G1 level k , condition P4 requiredcondition G2 .case condition P1 , G1 G4 responsible it, supportingactions needed P1 . cases two heuristics computing regression: (1) Preferchoices help conflict set regress smaller set conditions (2) still choicemultiple conditions level k , pick one assigned earlier. motivation first rule keep failure explanations compact (and thus general) possible,7. current example, memo includes conditions P4 (which farthest gonelevel), even always necessary. verify P3 would memo set A11one supporters P2 .12fiP LANNING G RAPHCSPmotivation second rule support deeper dependency directed backtracking.important note heuristics aimed improving performance EBL/DDBaffect soundness completeness approach.current example, first rules applies, since P1 already required G1 ,also requiring P2 P3 . Even case (i.e., G1 required P1 ), still wouldselected G1 G4 regression P1 , since G1 assigned earlier search.result regressing fP1 ; P2 ; P3 ; P4 g actions k th level thus fG1 ; G2 g. startbacktracking level k conflict set. jump back G2 right away, sincerecent variable named conflict set. avoids inefficiency re-consideringchoices G3 G4 , done normal backward search. G2 , backtrack conflict setabsorbed, backtracking continues since choices. procedurerepeated G1 . point, end leveland memoize fG1 ; G2 gmemo level k . Since levels backtrack to, Graphplan calledextend planning-graph one level.Notice memos based EBL analysis capture failures may require significantamount search rediscover. example, able discover fG1 ; G2 g failinggoal set despite fact mutex relations choices goals G1G2 .4.3 Using Memosend section, couple observations regarding use stored memos.standard Graphplan, memos level stored level-specific hash table. Wheneverbackward search reaches level k set conditions satisfied, consults hash tablesee exact set conditions stored memo. Search terminated exact hitoccurs. Since EBL analysis allows us store compact memos, likely complete goalset level k going exactly match stored memo. likely storedmemo subset goal set level k (which sufficient declare goal set failure).words, memo checking routine Graphplan needs modified checkssee subset current goal set stored memo. naive wayinvolves enumerating subsets current goal set checkinghash table, turns costly. One needs efficient data structures, setenumeration trees (Rymon, 1992). Indeed, Koehler co-workers (Koehler, Nebel, Hoffman,& Dimopoulos, 1997) developed data structure called UB-Trees storing memos.UB-Tree structures seen specialized version set-enumeration trees,efficiently check subset current goal set stored memo.second observation regarding memos often serve failure explanationthemselves. Suppose level k , find goal set level subsumesstored memo . use failure explanation level, regressback previous level. process provide us valuable opportunitiesback jumping levels k . also allows us learn new compact memos levels. Notenone would possible normal memos stored Graphplan,way memo declare goal set level k failing memo exactly equal goalset. case regression get us goals level k + 1, buy usbackjumping learning power (Kambhampati, 1998).13fiK AMBHAMPATI5. Empirical Evaluation Effectiveness EBL/DDBseen way EBL DDB capabilities added backward search maintaining updating conflict-sets. also noted EBL DDB capabilities avoid varietyinefficiencies standard Graphplan backward search. augmentations soundness completeness preserving follows corresponding properties conflict-directedbackjumping (Kambhampati, 1998). remaining (million-dollar) question whether capabilities make difference practice. present set empirical results answerquestion.implemented EBL/DDB approach described previous section top Graphplanimplementation Lisp.8 changes needed code add EBL/DDB capability relatively minor two functions needed non-trivial changes9 . also added UB-Tree subsetmemo checking code described (Koehler et al., 1997). ran several comparative experimentsbenchmark problems (Kautz & Selman, 1996), well four domains.specific domains included blocks world, rocket world, logistics domain, gripper domain, ferrydomain, traveling salesperson domain, towers hanoi. domains, includingblocks world, logistics domain gripper domain used recent AI PlanningSystems competition. specifications problems well domains publicly available.Table 1 shows statistics times taken number backtracks made normal Graphplan, Graphplan EBL/DDB capabilities.105.1 Run-Time Improvementfirst thing note EBL/DDB techniques offer quite dramatic speedups 1.6xblocks world way 120x logistics domain (the Att-log-a problem unsolvablenormal Graphplan 40 hours cpu time!). also note number backtracksreduces significantly consistently EBL/DDB. Given lengh runs, timeLisp spends garbage collection becomes important issue. thus report cumulative time(including cpu time garbage collection time) Graphplan EBL/DDB, separatecpu time cumulative time plain Graphplan (in cases total time spentlarge enough garbage collection time significant fraction). Specifically, twoentrys column corresponding total time normal Graphplan. first entrycpu time spent, second entry parenthesis cumulative time (cpu time garbagecollection time) spent. speedup computed respect cumulative time GraphplanEBL/DDB cpu time plain Graphplan. 11 reported speedups thus seenconservative estimates.8. original lisp implementation Graphplan done Mark Peot. implementation subsequentlyimproved David Smith.9. Assign-goals find-plan10. earlier versions paper, including paper presented IJCAI (Kambhampati, 1999) reportedexperiments Sun SPARC Ultra 1 running Allegro Common Lisp 4.3. Linux machine run-time statisticsseem approximately 2.7x faster Sparc machine.11. interesting note percentage time spent garbage collection highly problem dependent.example, case Att-log-a, 30 minutes 41 hours (or 1% cumulative time) spentgarbage collection, case Tower-6, 3.1 hours 4.8 hours (or 65% cumulativetime) spent garbage collection!14fiProblemSpeedup1.7x1.8x24x17x>1215x11x90x>10x42x>40x50x37x>25x90x>58xTable 1: Empirical performance EBL/DDB. Unless otherwise noted, times cpu minutes Pentium-III 500 MHZ machine256meg RA running Linux allegro common lisp 5, compiled speed. Tt total time, Mt time used checkingmemos Btks number backtracks done search. times Graphplan EBL/DDB include cpugarbage collection time, cpu time separated total time case normal Graphplan. numbersparentheses next problem names list number time steps number actions respectively solution. AvLnAvFM denote average memo length average number failures detected per stored memo respectively.CSPAvFM1.261.133.23.224.92.22.32.45-Normal GraphplanTt. Mt.# Btks AvLn5.3 0.225181K11.34.15 0.052823K 11.8319.43 11.78128K23.914.17.7 10434K23.8>40.5hr (>41hr)321.1.392802K14.9215(272)17.8>8.2hr(>16hr)7.23 1.27 19070K20.9>1.7hr (>4.8hr)22.322(29)11 33357K24.542(144)24 53233K25>5hr(>18.4hr)89(93) 56.7 68648K13>12hr (>14.5hr)-P LANNING G RAPH15Huge-Fact (18/18)BW-Large-B (18/18)Rocket-ext-a (7/36)Rocket-ext-b (7/36)Att-log-a(11/79)Gripper-6 (11/17)Gripper-8 (15/23)Gripper-10(19/29)Tower-5 (31/31)Tower-6 (63/63)Ferry-41 (27/27)Ferry-5 (31/31)Ferry-6(39/39)Tsp-10 (10/10)Tsp-12(12/12)Graphplan EBL/DDBTtMt# Btks AvLn AvFM3.08 0.282004K9.522.522.27 0.11798K 10.153.32.8.34764K8.582.8.43569K7.51011.97.892186K8.21 46.180.1 0.03201K6.96.22.4.934426K97.6447.9 18.2 61373K 11.058.3.17 0.02277K6.72.72.53 0.224098K7.92.8.44 0.13723K7.92.541.13.411993K8.82.5311.625.3 18318K10.92.6.99 0.232232K6.91212.4 2.65 21482K7.915.2fiK AMBHAMPATI5.2 Reduction Memo Lengthresults also highlight fact speedups offered EBL/DDB problem/domaindependent quite meager blocks world problems, quite dramatic manydomains including rocket world, logistics, ferry, gripper, TSP Hanoi domains. statisticsmemos, shown Table 1 shed light reasons variation. particular interestaverage length stored memos (given columns labeled AvLn). general,expect EBL analysis reduces length stored memos, conditions partfailure explanation stored memo. However, advantage dependslikelihood small subset goals given level actually taking part failure.likelihood turn depends amount inter-dependencies goals givenlevel. table, note average length reduces quite dramatically rocket worldlogistics12 , reduction much less pronounced blocks world. variationtraced back larger degree inter-dependency goals given level blocksworld problems.reduction average memo length correlated perfectly speedups offered EBLcorresponding problems. Let put perspective. fact average lengthmemos Rocket-ext-a problem 8.5 EBL 24 withoutEBL, shows essence,normal Graphplan re-discovering 8-sized failure embedded 248 possible ways worstcase 24 sized goal set storing new memo time (incurring increased backtrackingmatching costs)! thus wonder normal Graphplan performs badly comparedGraphplan EBL/DDB.5.3 Utility Stored Memosstatistics Table 1 also show increased utility memos stored GraphplanEBL/DDB. Since EBL/DDB store general (smaller) memos normal Graphplan,should, theory, generate fewer memos use often. columns labeled AvFMgive ratio number failures discovered use memos number memosgenerated first place. seen measure average utility storedmemos. note utility consistently higher EBL/DDB. example, Rocketext-b, see average EBL/DDB generated memo used discover failures 101times, number 3.2 memos generated normal Graphplan.135.4 Relative Utility EBL vs. DDBstatistics Table 1, see even though EBL make significant improvementsrun-time, significant fraction run time EBL (as well normal Graphplan) spentmemo checking. raises possibility overall savings mostly DDB partEBL part (i.e, part involving storing checking memos) fact net drain(Kambhampati, Katukam, & Qu, 1997). see true, ran problems EBL (i.e.,memo-checking) disabled. DDB capability well standard Graphplan memoization12. case Att-log-a, took memo statistics interrupting search 6 hours13. statistics Att-log-aseem suggest memo usage bad normal Graphplan. However,noted Att-log-a solved normal Graphplan begin with. improved usage factor may duemostly fact search went considerably longer time, giving Graphplan opportunity usememos.16fiP LANNING G RAPHProblemAtt-log-aTower-6Rocket-ext-aGripper-8TSP-10Huge-FctEBL+DDBBtksTime2186K 1.954098K 2.37764K.834426K 2.432238K1.12004K 3.21CSPDDBBtksTime115421K 23597395K1213764K17.185426K4.714308K2.32465K3.83Speedup120x51x21x1.94x2.09x1.19xTable 2: Utility storing using EBL memos DDBstrategies left in.14 results shown Table 2, demonstrate ability storesmaller memos (as afforded EBL) quite helpfulgiving rise 120x speedup DDB aloneAtt-log-a problem, 50x speedup Tower-6 problem. course, results also showDDB important capability itself. Indeed, Att-log-aand tower-6 could even solvedstandard Graphplan, DDB, problems become solvable. summary, resultsshow EBL DDB net positive utility.5.5 Utility MemoizationAnother minor, well-recognized, point brought statistics Table 1memo checking sometimes significant fraction run-time standard Graphplan.example, case Rocket-ext-a, standard Graphplan takes 19.4 minutes 11.7 minutes,half time, spent memo checking (in hash tables)! raises possibilitydisable memoization, perhaps well version EBL/DDB.see case, ran problems memoization disabled. results showgeneral disabling memo-checking leads worsened performance. came acrosscases disablement reduces overall run-time, run-time still much higherget EBL/DDB. example, case Rocket-ext-a, disable memochecking completely, Graphplan takes 16.5 minutes, lower 19.4 minutes takenstandard Graphplan, still much higher .8 minutes taken version GraphplanEBL/DDB capabilities added. add DDB capability, still disabling memochecking, run time becomes 2.4 minutes, still 3 times higher affordedEBL capability.5.6 C vs. Lisp QuestionGiven existing implementations Graphplan done C many optimizations,one nagging doubt whether dramatic speedups due EBL/DDB somehow dependentmoderately optimized Lisp implementation used experiments. Thankfully,EBL/DDB techniques described paper also (re)implemented Maria FoxDerek Long STAN system. STAN highly optimized implementation Graphplanfared well recent AIPS planning competition. found EBL/DDB resultedsimilar dramatic speedups system (Fox, 1998; Fox & Long, 1999). example,14. also considered removing memoization completely, results even poorer.17fiK AMBHAMPATIunable solve Att-log-a plain Graphplan, could solve easily EBL/DDBadded.Finally, worth pointing even EBL/DDB capabilities, unable solvelarger problems AT&T benchmarks, bw-large-c Att-log-b. howeverindictment EBL/DDB since knowledge planners solvedproblems used either local search strategies GSAT, randomized re-start strategies,used additional domain-specific knowledge pre-processing. least,aware existing implementations Graphplan solve problems.6. Utility Graphplan MemosOne important issue using EBL managing costs storage matching. Indeed, discussed (Kambhampati, 1998), naive implementations EBL/DDB known lose gainsmade pruning power matching storage costs. Consequently, several techniquesinvented reduce costs selective learning well selective forgetting.interesting see costs prominent issue EBL/DDB Graphplan.think mostly two characteristics Graphplan memoization strategy:1. Graphplans memoization strategy provides compact representation no-goods,well selective strategy remembering no-goods. Seen DCSP, rememberssubsets activated variables satisfying assignment. Seen CSP (c.f.Figure 2), Graphplan remembers no-goods formP1i 6=? ^P2i 6=? Pmi 6=?(where superscripts correspond level planning graph propositionbelongs), normal EBL implementations learn no-goods formP1i = a1 ^ P2j = a2 Pmk =Suppose planning graph contains n propositions divided l levels, propositionP level j actions supporting it. CSP compilation planning graphn variables, + 1 values (the extra one ?). normal EBL implementationCSP learn, worst case, (d + 2)n no-goods.15 contrast, Graphplannremembers l 2 l memos16 dramatic reduction. reduction result twofactors:(a) individual memo stored Graphplan corresponds exponentially large setnormal no-goods (the memoP1i 6=? ^P2i 6=? Pmi 6=?shorthand notation conjunction dm no-goods corresponding possiblei)non- ? assignments P1i Pm15. variable v may either present no-good, present onepossibilities n variables.16. level, nl propositions either occurs memo occur+118+ 1 possible assignmentsgivingfiP LANNING G RAPHCSP(b) Memos subsume no-goods made proposition variables planning graph level.2. matching cost reduced fact considerably fewer no-goods everlearned, fact Graphplan stores no-goods (memos) separately level,consults memos stored level j , backwards search level j ,discussion throws light so-called EBL utility problemcritical Graphplan EBL done normal CSPs.6.1 Scenarios Memoization Conservative Avoid RediscoveryFailuresdiscussion also raises possibility Graphplan (even EBL/DDB) memoizationconservative may losing useful learning opportunitiesrequired syntactic form. Specifically, Graphplan learn memo formP1i 6=? ^P2i 6=? Pmi 6=?;must case dm possible assignments propositional variables mustno-good. Even one no-good, Graphplan avoids learning memo, thuspotentially repeating failing searches later time (although loss made extentlearning several memos lower level).PConsider example following scenario: set variables P1i Pmnlevel assigned backward search. Suppose search found legal partial asi , domain P contains k values fv1 vk g.signment variables P1i Pm,1trying assign variables Pm Pni , suppose repeatedly fail backtrack variablePmi , re-assigning eventually settling value v7. point backtrackinghigher level variables (P P ) re-assigningoccurs, time backtrack Pm1them. point, would useful remember no-goods effect nonegoing work backtracking repeated.first 6 values Pmno-goods take form:Pmi = vj ^ Pmi +1 6=? ^Pmi +2 6=? Pni 6=?tried found lead failurej ranges 1 6, values Pmassigning later variables. Unfortunately, no-goods syntactic form memosmemoization procedure cannot remember them. search thus forced rediscoverfailures.6.2 Sticky Values Partial AntidoteOne way staying standard memoization, avoiding rediscovery failing searchpaths, case example above, use sticky values heuristic (Frost& Dechter, 1994; Kambhampati, 1998). involves remembering current value variableskipping DDB, trying value first search comes backvariable. heuristic motivated fact skip variable DDB,means variable current assignment contributed failure caused19fiK AMBHAMPATIbacktrackingso makes sense restore value upon re-visit. example above,backtracked it, triesheuristic remember v7 current value Pmfirst value re-visited. variation technique re-arrange folddomain variable values precede current value sent backdomain, values tried previously untried values foundfail. makes assumption values led failure likely again.becomes fv7 ; v8 vk ; v1 ; v2 v6 g.example above, heuristic folds domain PmNotice heuristics make sense employ DDB, otherwise neverskip variable backtracking.implemented sticky value heuristics top EBL/DDB Graphplan. statisticsTable 3 show results experiments extension. seen, sticky valuesapproach able give 4.6x additional speedup EBL/DDB depending problem.Further, folding heuristic dominates simple version terms number backtracks,difference quite small terms run-time.7. Forward Checking & Dynamic Variable OrderingDDB EBL considered look-back techniques analyze failures lookingback past variables may played part failures. different classtechniques known look-forward techniques improving search. Prominent amonglatter forward checking dynamic variable ordering. Supporting forward checking involvesfiltering conflicting actions domains remaining goals, soon particulargoal assigned. example Figure 1, forward checking filter A9 domain P4soon P1 assigned A5 . Dynamic variable ordering (DVO) involves selecting assignmentgoal least number remaining establishers.17 DVO combined forward checking, variables ordered according live domain sizes (where live domaincomprised values domain yet pruned forward checking). experiments18 show techniques bring reasonable, albeit non-dramatic, improvementsGraphplans performance. Table 4 shows statistics benchmark problems, dynamic variable ordering alone, forward checking dynamic variable ordering. notebacktracks reduce 3.6x case dynamic variable ordering, 5xcase dynamic variable ordering forward checking, speedups time somewhat smaller,ranging 1.1x 4.8x. Times perhaps improved efficient implementation forward checking.19 results also seem suggest amount optimizationgoing make dynamic variable ordering forward checking competitive EBL/DDBproblems. one thing, several problems, including Att-log-a, Tsp-12, Ferry-6 etc.could solved even forward checking dynamic variable ordering. Second,even problems could solved, reduction backtracks provided EBL/DDB fargreater provided FC/DVO strategies. example, Tsp-10, FC/DVO strategies17. also experimented variation heuristic, known Brelaz heuristic (Gomes et al., 1998),ties among variables sized live-domains broken picking variables take partnumber constraints. variation however lead appreciable improvement performance.18. study forward checking dynamic variable ordering initiated Dan Weld.19. current implementation physically removes pruned values variable forward checking phase,restores values backtracks. better implementations, including use in/out flags values welluse indexed arrays (c.f. (Bacchus & van Run, 1995))20fi21EBL/DDB+StickyBtksSpeedup372K2.2x(2.05x)172K4.6x(3.3x)56212K 1.29x(1.09x)18151K .99x(1.01x)20948K 1.26x(1.02x)1144K2x(1.91x)EBL/DDB+Sticky+FoldTimeBtksSpeedup.33347K2.4x (2.2x).177169K4.5x(3.36x)40.8 54975K 1.17x(1.12x)11.87 18151K .97x(1.01x)10.18 20948K 1.22x(1.02x).67781K2.9x(2.8x)Table 3: Utility using sticky values along EBL/DDB.CSPTime.37.1836.911.759.86.95Rocket-ext-a(7/36)Rocket-ext-b(7/36)Gripper-10(39/39)Ferry-6TSP-12(12/12)Att-log-a(11/79)Plain EBL/DDBTimeBtks.8764K.8569K47.95 61373K11.62 18318K12.44 21482K1.952186KP LANNING G RAPHProblemfiK AMBHAMPATIProblemHuge-fact (18/18)BW-Large-B (18/18)Rocket-ext-a (7/36)Rocket-ext-b (7/36)Att-log-a(11/79)Gripper-6(11/17)Tsp-10(10/10)Tower-6(63/63)GP5.3(5181K)4.15(2823K)19.43(8128K)14.1(10434K)>10hr1.1(2802K)89(69974K)>10hrGP+DVO2.26 (1411K)3.14(1416K)14.9(5252K)7.91(4382K)>10hr.65(1107K)78(37654K)>10hrSpeedup2.3x(3.6x)1.3x(2x)1.3x(1.5x)1.8x(2.4x)1.7x(2.5x)1.14x(1.9x)-GP+DVO+FC3.59 (1024K)4.78(949K)14.5(1877K)6(1490K)>10hr..73 (740K)81(14697K)>10hr.Speedup1.47x(5x).86(3x)1.3x(4.3x)2.4x(7x)1.5x(3.7x)1.09x(4.8x)Table 4: Impact forward checking dynamic variable ordering routines Graphplan. Timescpu minutes measured 500 MHZ Pentium-III running Linux FranzAllegro Common Lisp 5. numbers parentheses next times numberbacktracks. speedup columns report two factorsthe first speedup time,second speedup terms number backtracks. FC DVO tendreduce number backtracks, reduction always seem showtime savings.reduce number backtracks 69974K 14697K, 4.8x improvement. However, palescomparison 2232K backtracks (or 31x improvement) given EBL/DDB (see entryTable 1). Notice results say variable ordering strategies make dramaticdifference Graphplans backward search (or DCSP compilation planning graph);make claims utility FC DVO CSP compilation planning graph.7.1 Complementing EBL/DDB Forward Checking Dynamic Variable OrderingAlthough forward checking dynamic variable ordering approaches found particularly effective isolation Graphplans backward search, thought would interestingrevisit context Graphplan enhanced EBL/DDB strategies. Part original reasoning underlying expectation goal (variable) ordering significanteffect Graphplan performance based fact failing goal sets stored in-totomemos (Blum & Furst, 1997, pp. 290). reason longer holds use EBL/DDB.more, exists difference opinion whether forward checkingDDB fruitfully co-exist. results (Prosser, 1993) suggest domain-filteringsuchone afforded forward checking, degrades intelligent backtracking. recent work(Frost & Dechter, 1994; Bayardo & Schrag, 1997) however seems suggest however best CSPalgorithms capabilities.adding plain DVO capability top EBL/DDB presents difficulties, adding forwardchecking require changes algorithm Figure 4. difficulty arisesfailure may occurred combined effect forward checking backtracking.example, suppose four variables v1 v4 considered assignmentorder. Suppose v3 domain f1; 2; 3g, v3 cannot 1 v1 a, cannot 2 v2b. Suppose v4s domain contains d, constraint saying v4 cant22fiP LANNING G RAPHProblemHuge-fctBW-Large-BRocket-ext-aRocket-ext-bAtt-log-aTower-6TSP-10EBLTime(btks)3.08(2004K)2.27(798K).8(764K).8(569K)1.97(2186K)2.53(4098K).99(2232K)CSPEBL+DVOTime(btks)Speedup1.51(745K)2x(2.68x)1.81(514K)1.25x(1.6x).4(242K)2x(3.2x).29(151K)2.75x(3.76x)2.59(1109K) .76x(1.97x)3.78(3396K).67x(1.2x)1.27(1793K) .77x(1.24x)EBL+FC+DVOTime(Btks)Speedup2.57(404K)1.2x(5x)2.98(333K) .76x(2.39x).73(273K)1.09x(2.8x).72(195K)1.1x(2.9x)3.98(1134K) .5x(1.92x)2.09(636K)1.2x(6.4x)1.34(828K).73x(2.7x)Table 5: Effect complementing EBL/DDB dynamic variable ordering forward checkingstrategies. speedup columns report two factorsthe first speedup time,second speedup terms number backtracks. FC DVO tendreduce number backtracks, reduction always seem showtime savings.v1 v3 3. Suppose using forward checking, assigned v1 ; v2values b. Forward checking prunes 1 2 v3 domain, leaving value 3.point, try assign v4 fail. use algorithm Figure 4, conflict set v4would fv4 ; v3 ; v1 g, constraint violated v1 = ^ v3 = 3 ^ v4 = d. Howeversufficient since failure v4 may occurred forward checking strippedvalue 2 domain v3 . problem handled pushing v1 v2 , variableswhose assignment stripped values v3 , v3 conflict set.20 Specifically, conflictset every variable v initialized fv g begin with, whenever v loses valueforward checking respect assignment v 0 , v 0 added conflict set v . Wheneverfuture variable (such v4 ) conflicts v3 , add conflict set v3 (rather v3 )conflict set v4 . Specifically lineSet cs = cs [ f l gprocedure Figure 4 replaced lineSet cs = cs [ Conflict-set(l)incorporated changes implementation, support support forward checking, dynamic variable ordering well EBL Graphplan. Table 5 shows performance version experimental test suite. seen numbers, numberbacktracks reduced 3.7x case EBL+DVO, 5x caseEBL+FC+DVO. cpu time improvements somewhat lower. got 2.7x speedup20. Notice possible values stripped v3 domain may impactfailure assign v4 . example, perhaps another constraint says v4 cant v3 b,case, strictly speaking, assignment v2 cannot really blamed failure v4 . leadsnon-minimal explanations, reason expect strict minimization explanations pre-requisiteeffectiveness EBL/DDB; see (Kambhampati, 1998)23fiK AMBHAMPATIEBL+DVO, 1.2x speedup EBL+FC+DVO, several cases, cpu times increase FC DVO. again, attribute overheads forward checking (andlesser extent, dynamic variable ordering). importantly, comparing resultsTables 4 5, see EBL/DDB capabilities able bring significant speedupseven Graphplan implementation using FC DVO.8. EBL/DDB & Randomized SearchRecent years seen increased use randomized search strategies planning. includepurely local search strategies (Gerevini, 1999; Selman, Levesque, & Mitchell, 1992) wellhybrid strategies introduce random restart scheme top systematic search strategy(Gomes et al., 1998). BLACKBOX planning system (Kautz & Selman, 1999) supports varietyrandom restart strategies top SAT compilation planning graph, empiricalstudies show strategies can, probabilistically speaking, scale much better purelysystematic search strategies.wanted investigate (and much) EBL & DDB techniques help Graphplaneven presence newer search strategies. EBL DDB techniques littleapplicability purely local search strategies, could theory help random restart systematicsearch strategies. Random restart strategies motivated attempt exploit heavytail distribution (Gomes et al., 1998) solution nodes search trees many problems.Intuitively, problems non-trivial percentage easy find solutionswell hard find solutions, makes sense restart search findspending much effort solution. restarting way, hope (probabilistically) hiteasier-to-find solutions.implemented random-restart strategy top Graphplan making following simplemodifications backward search:1. keep track number times backward search backtracks one levelplan graph previous level (a level closer goal state), whenever numberexceeds given limit (called backtrack limit), search restarted (by going back lastlevel plan graph), assuming number restarts also exceeded givenlimit. search process two restarts referred epoch.2. supporting actions (values) proposition variable considered randomizedorder. randomization ensures search restarted, lookvalues variable different order.21Notice random-restart strategy still allows application EBL DDB strategies, sincegiven epoch, behavior search identical standard backwardsearch algorithm. Indeed, backtrack limit number restarts made largerlarger, whole search becomes identical standard backward search.21. Reordering values variable doesnt make whole lot sense BLACKBOX based SAT encodingsthus boolean variables. Thus, randomization BLACKBOX done order goalsconsidered assignment. typically tends clash built-in goal ordering strategies (such DVOSAT-Z (Li & Anbulagan, 1997)), get around conflict breaking ties among variables randomly.avoid clashes, decided randomize Graphplan reordering values variable. also picked inter-levelbacktracks natural parameter characterizing difficulty problem Graphplans backward search.24fiProblem%sol2%11%54%13%94%0%0%3%2%2%58%90%100%Normal GraphplanLengthTime Av. MFSL19(103).21.3K(3.7K)17.6(100.5) 1.293.7K(41K)25.6(136)34K(78K)18(97.5)331K(361K)22.1(119.3)3133K(489K).2K(4K)2.6K(53K)28(156)45K(111K)26.5(135).75.4K(8K)29(152)43.7K(111K)21.24(87.3)2.2K(4K)21.3(85)8.12.3K(43K)15.3(62.5)4535K(403K)Table 6: Effect EBL/DDB random-restart Graphplan. Time measured cpu minutes Allegro Common Lisp 5.0 runningLinux 500MHZ Pentium machine. numbers next problem names number steps actions shortestplans reported problems literature. R/B/L parameters second column refer limits numberrestarts, number backtracks number levels plan graph expanded. statistics averagedmultiple runs (typically 100 50). MFSL column gives average number memo-based failures per searched levelplan graph. numbers parentheses total number memo-based failures averaged runs. Plan lengthsaveraged successful runs.CSPGraphplan EBL/DDBLengthTime Av. MFSL14(82).414.6K(28K)11.3(69.5).7217.8K(59K)11.3(69.5).7217.8K(59K)11(68.5)2.38 73K(220K)11(68.5)2.38 73K(220K)18.1(101)1.628K(93K)17.3(98)11.4 69K(717K)20.1(109)15.3 74K(896K)22.85(124) 2.778K(145K)19.9(110)1471K(848K)7.76(35.8)1.329K(109K)7(34.1)1.32 38K(115K)7(34.2)1.21 35K(105K)%sol99%100%100%100%100%17%60%100%55%100%100%100%100%P LANNING G RAPH25Att-log-a(11/54)Att-log-a(11/54)Att-log-a(11/54)Att-log-a(11/54)Att-log-a(11/54)Att-log-b(13/47)Att-log-b(13/47)Att-log-b(13/47)Att-log-c(13/65)Att-log-c(13/65)Rocket-ext-a(7/34)Rocket-ext-a(7/34)Rocket-ext-a(7/34)ParametersR/B/L5/50/2010/100/2010/100/3020/200/2020/200/305/50/2010/100/2010/100/305/50/3010/100/3010/100/3020/200/3040/400/30fiK AMBHAMPATIcheck intuitions effectiveness EBL/DDB randomized search indeed correct, conducted empirical investigation comparing performance random searchstandard Graphplan well Graphplan EBL/DDB capabilities. Since search randomized, problem solved multiple number times (100 times cases), runtime, plan length statistics averaged runs. experiments conductedgiven backtrack limit, given restart limit, well limit number levelsplanning graph extended. last one needed randomized search, solution maymissed first level appears, leading prolonged extension planning graph(inoptimal) solution found later level. limit number levels expanded,probability finding solution increases, time, cpu time spent searchinggraph also increases.implemented random restart search, first thing noticed improvementsolvability horizon (as expected, given results (Gomes et al., 1998)). Table 6 showsresults. One important point note results table talk average planlengths cpu times. needed due randomization potentially run producedifferent outcome (plan). Secondly, Graphplan systematic search guarantees shortestplans (measured number steps), randomized search guarantee.particular, randomized version might consider particular planning graph barrensolutions, based simply fact solution could found within confines givenbacktrack limit number restarts.Graphplan, without EBL/DDB, likely solve larger problems randomizedsearch strategies. example, logistics domain, Att-log-a problem solvable(within 24 hours real time) EBL systematic search. randomization added,implementation able solve Att-log-b Att-log-c quite frequently. limitsnumber restarts, backtracks levels increased, likelihood finding solution wellaverage length solution found improves. example, Graphplan EBL/DDB ablesolve Att-log-b every trial 10 restarts, 100 backtracks 30 levels limits (althoughplans quite inoptimal).next, perhaps interesting, question wanted investigate whether EBLDDB continue useful Graphplan uses randomized search. first blush,seems importantafter even Graphplan standard search mayluck able find solutions quickly presence randomization. thoughthowever suggests EBL DDB may still able help Graphplan. Specifically,help Graphplan using given backtrack limit judicious fashion. elaborate, supposerandom restart search conducted 100 backtracks 10 restarts. EBLDDB, Graphplan able pinpoint cause failure accurately without EBLDDB. means search backtracks, chance backtrack(or similar) reasons reduced. turn gives search chancecatching success one number epochs allowed. additiondirect benefit able use stored memos across epochs cut search.seen data Table 6, given set limits number restarts, numberbacktracks, number levels expanded, Graphplan EBL/DDB able get higherpercentage solvability well significantly shorter length solutions (both terms levelsterms actions). get comparable results standard Graphplan, significantlyincrease input parameters (restarts, backtracks levels expanded), turn led dra26fiP LANNING G RAPHCSPmatic increases average run time. example, Att-log-a problem, 5 restarts50 backtracks, 20 levels limit, Graphplan able solve problem 99% time,average plan length 14 steps 82 actions. contrast, without EBL/DDB, Graphplanable solve problem 2% cases, average plan length 19 steps 103actions. double restarts backtracks, EBL/DDB version goes 100% solvabilityaverage plan length 11.33 steps 69.53 actions. standard Graphplan goes 11%solvability plan length 17.6 steps 100 actions. increase number levels 30,standard Graphplan solves 54% problems average plan length 25.6 steps136 actions. takes 20 restarts 200 backtracks, well 30-level limit standardGraphplan able cross 90% solvability. time, average run time 31 minutes,average plan length 22 steps 119 actions. contrast 99% solvability 0.4 minutes 14 step 82 action plans provided Graphplan EBL 5 restarts50 backtracks significant! Similar results observed problems, logistics(Att-log-b, Att-log-c) domains (Rocket-ext-a, Rocket-ext-b).results also show Graphplan EBL/DDB able generate reuse memos effectively across different restart epochs. Specifically, numbers columns titled Av. MFSLgive average number memo-based failures per search level.22 note cases,average number memo-based failures significantly higher Graphplan EBLnormal Graphplan. shows EBL/DDB analysis helping Graphplan reduce wasted effortsignificantly, thus reap better benefits given backtrack restart limits.9. Related Workoriginal implementation Graphplan, Blum Furst experimented variationmemoization strategy called subset memoization. strategy, keep memo generationtechniques same, change way memos used, declaring failure stored memofound subset current goal set. Since complete subset checking costly,experimented partial subset memoization subsets length n n , 1considered n sized goal set.mentioned earlier, Koehler co-workers (Koehler et al., 1997) re-visitedsubset memoization strategy, developed effective solution complete subset checkinginvolves storing memos data structure called UB-Tree, instead hash tables.results experiments subset memoization mixed, indicating subset memoization seem improve cpu time performance significantly. reason quiteeasy understand improved memo checking time UB-Tree data structure,still generating storing old long memos. contrast, EBL/DDB extensiondescribed supports dependency directed backtracking, reducing average lengthstored memos, increases utility significantly, thus offering dramatic speedups.verify main source power EBL/DDB-Graphplan EBL/DDB partUB-Tree based memo checking, re-ran experiments EBL/DDB turned off,22. Notice number search levels may different (and smaller than) number planning graph levels,Graphplan initiates search none goals pair-wise mutex other. Att-log-a,Att-log-b Att-log-c, happens starting level 9. Rocket-ext-a happens starting level 5. numbersparentheses total number memo based failures. divide number average number levelssearch conducted get Av. MFSL statistic.27fiK AMBHAMPATIProblemHuge-FactBW-Large-bRocket-ext-aRocket-ext-bAtt-log-aTt3.202.7419.27.36> 12hrsMt10.1816.74.77-#Btks2497K1309K6188K7546K-EBL x"1.04x1.21x24x9.2x>120x#Gen24243117086241961666-#Fail3362815011269499265579-AvFM1.381.284.34.3-AvLn11.0711.4824.3224.28-Table 7: Performance subset memoization UB-Tree data structure (without EBL/DDB).Tt total cpu time Mt time taken checking memos. #Btksnumber backtracks. EBLx amount speedup offered EBL/DDB subsetmemoization #Gen lists number memos generated (and stored), #Fail listsnumber memo-based failures, AvFM average number failures identified pergenerated memo AvLn average length stored memos.subset memo checking UB-Tree data structure still enabled. results shownTable 7. columns labeled AvFM show expected subset memoization improveutility stored memos normal Graphplan (since uses memo scenariosnormal Graphplan can). However, also note subset memoizationdramatic impact performance Graphplan, EBL/DDB capability significantlyenhance savings offered subset memoization.(Kambhampati, 1998), describe general principles underlying EBL/DDB techniquessketch extended dynamic constraint satisfaction problems. developmentpaper seen application ideas there. Readers needing backgroundEBL/DDB thus encouraged review paper. related work includes previous attempts applying EBL/DDB planning algorithms, work UCPOP+EBL system(Kambhampati et al., 1997). One interesting contrast ease EBL/DDB addedGraphplan compared UCPOP system. Part difference comes factsearch Graphplan ultimately propositional dynamic CSP, UCPOPs searchvariablized problem-solving search.mentioned Section 2, Graphplan planning graph also compiled normal CSPrepresentation, rather dynamic CSP representation. used dynamic CSP representation corresponds quite directly backward search used Graphplan. sawmodel provides clearer picture mutex propagation memoization strategies, helps usunearth sources strength Graphplan memoization strategyincluding factmemos conservative form no-good learning obviate need no-goodmanagement strategies large extent.dynamic CSP model may also account peculiarities resultsempirical studies. example, widely believed CSP literature forward checkingdynamic variable ordering either critical as, perhaps even critical than, EBL/DDBstrategies (Bacchus & van Run, 1995; Frost & Dechter, 1994). results however showGraphplan, uses dynamic CSP model search, DVO FC largely ineffectivecompared EBL/DDB standard Graphplan. extent, may due fact28fiP LANNING G RAPHCSPGraphplan already primitive form EBL built memoization strategy. fact, Blum& Furst (1997) argue memoization minimal action set selection (an action setconsidered minimal possible remove action set still supportgoals actions selected), ordering goals little effect (especiallyearlier levels contain solution).Another reason ineffectiveness dynamic variable ordering heuristic maydifferences CSP DCSP problems. DCSP, main aimquickly find assignment current level variables, rather find assignmentcurrent level likely activate fewer easier assign variables, whose assignmentturn leads fewer easier assign variables on. general heuristic pickingvariable smallest (live) domain necessarily make sense DCSP, since variabletwo actions supporting may actually much harder handle another manyactions supporting it, actions supporting first one eventually lead activationmany harder assign new variables. may thus worth considering ordering strategiescustomized dynamic CSP modelse.g. orderings based number(and difficulty) variables get activated given variable (or value) choice.recently experimented value-ordering heuristic picks value assigned variable using distance estimates variables activated choice(Kambhampati & Nigenda, 2000). planning graph provides variety ways obtainingdistance estimates. simplest idea would say distance proposition p levelp enters planning graph first time. distance estimate usedrank variables values. Variables ranked simply terms distancesthevariables highest distance chosen first (akin fail-first principle). Value orderingbit trickierfor given variable, need pick action whose precondition set lowestdistance. distance precondition set computed distance individualpreconditions several ways:Maximum distances individual propositions making preconditions.Sum distances individual propositions making preconditions.first level set propositions making preconditions presentnon-mutex.(Kambhampati & Nigenda, 2000), evaluate goal value ordering strategies basedideas, show lead quite impressive (upto 4 orders magnitudetests) speedups solution-bearing planning graphs. also relate distances computedplanning graph distance transforms computed planners like HSP (Bonet, Loerincs, &Geffner, 1999) UNPOP (McDermott, 1999). idea using planning graph basiscomputing heuristic distance metrics investigated context state-space search(Nguyen & Kambhampati, 2000). interesting finding paper even oneusing state-space instead CSP-style solution extraction, EBL still useful lazy demanddriven approach discovering n-ary mutexes improve informedness heuristic.Specifically, Long & Kambhampati describe method limited run Graphplans backward search, armed EBL/DDB used pre-processing stage explicate memos (n-arymutexes) used significantly improve effectiveness heuristicstate-search.29fiK AMBHAMPATIgeneral importance EBL & DDB CSP SAT problems well recognized. Indeed,one best systematic solvers propositional satisfiability problems RELSAT (Bayardo &Schrag, 1997), uses EBL, DDB, forward checking. randomized version RELSATone solvers supported BLACKBOX system (Kautz & Selman, 1999), compilesplanning graph SAT encoding, ships various solvers. BLACKBOX thus offersway indirectly comparing Dynamic CSP static CSP models solving planninggraph. discussed Section 2.2, main differences BLACKBOX needs compileplanning graph extensional SAT representation. makes harder BLACKBOXexploit results searches previous levels (as Graphplan stored memos),also leads memory blowups. latter particularly problematic techniquescondensing planning graphs, bi-level representation discussed (Fox & Long, 1999;Smith & Weld, 1999) effective compile planning graph SAT.flip side, BLACKBOX allows non-directional search, opportunity exploit existing SATsolvers, rather develop customized solvers planning graph. present, clearwhether either approaches dominates other. informal experiments, foundcertain problems, Att-log-x, easier solve non-directional search offeredBLACKBOX, others, Gripper-x, easier solve Graphplan backwardsearch. results recent AIPS planning competition also inconclusive respect(McDermott, 1998).main rationale focusing dynamic CSP model planning graph duecloseness Graphplans backward search, Gelle (1998) argues keeping activity constraintsdistinct value constraints several advantages terms modularity representation.Graphplan, advantage becomes apparent activation constraints knownpriori, posted dynamically search,. case several extensionsGraphplan algorithm handle conditional effects (Kambhampati et al., 1997; Anderson, Smith,& Weld, 1998; Koehler et al., 1997), incomplete initial states (Weld, Anderson, & Smith, 1998).Although EBL DDB strategies try exploit symmetry search space improvesearch performance, go far enough many cases. example, Gripper domain,real difficulty search gets lost combinatorics deciding hand usedpick ball transfer next rooma decision completely irrelevantquality solution (or search failures, matter). EBL/DDB allow Graphplancut search bit, allowing transfer 10 balls one room another,come beyond 10 balls. two possible ways scaling further. firstvariablize memos, realize certain types failures would occurred irrespectiveactual identity hand used. Variablization, also called generalization partEBL methods (Kambhampati, 1998; Kambhampati et al., 1997). Another way scalingsituations would recognize symmetry inherent problem abstractresources search. (Srivastava & Kambhampati, 1999), describe type resourceabstraction approach Graphplan.10. Conclusion Future workpaper, traced connections Graphplan planning graph CSP, motivated need exploiting CSP techniques improve performance Graphplan backward search. adapted evaluated several CSP search techniques contest Graph-30fiP LANNING G RAPHCSPplan. included EBL, DDB, forward checking, dynamic variable ordering, sticky values,random-restart search. empirical studies show EBL/DDB particularly useful dramatically speeding Graphplans backward search (by tp 1000x instances). speedupsimproved (by 8x) addition forward checking, dynamic variable ordering sticky values top EBL/DDB. also showed EBL/DDB techniques equallyeffective helping Graphplan, even random-restart search strategies used.secondary contribution paper clear description connectionsGraphplan planning graph, (dynamic) constraint satisfaction problem. connectionshelp us understand unique properties Graphplan memoization strategy, viewedCSP standpoint (see Section 9).several possible ways extending work. first would supportuse learned memos across problems (or specification problem changes,case replanning). Blum & Furst (1997) suggest promising future direction,EBL framework described makes extension feasible. discussed (Kambhampati,1998; Schiex & Verfaillie, 1993), supporting inter-problem usage involves contextualizinglearned no-goods. particular, since soundness memos depends initial stateproblem (given operators change problem problem), inter-problem usagememos supported tagging learned memo specific initial state literalssupported memo. Memos used corresponding level new problemlong initial state justification holds new problem. initial state justificationmemos computed incrementally procedure first justifies propagated mutexrelations terms initial state, justifies individual memos terms justificationsmutexes memos derived.success EBL/DDB approaches Graphplan part due high degree redundancy planning graph structure. example, propositions (actions) level lplanning graph superset propositions (actions) level l , 1, mutexes (memos)level l subset mutexes (memos) level l , 1). EBL/DDB techniques helpGraphplan exploit redundancy avoiding previous failures, exploitation redundancy pushed further. Indeed, search Graphplan planning graph size lalmost re-play search planning graph size l , 1 (with additionalchoices). (Zimmerman & Kambhampati, 1999), present complementary technique calledexplanation-guided backward search attempts exploit deja vu property Graphplans backward search. technique involves keeping track elaborate trace searchlevel l (along failure information), termed pilot explanation level l, usingpilot explanation guide search level l , 1. way EBL/DDB help processsignificantly reduce size pilot explanations need maintained. Preliminaryresults technique shows complements EBL/DDB provides significantsavings search.Acknowledgementsresearch supported part NSF young investigator award (NYI) IRI-9457634, ARPA/RomeLaboratory planning initiative grant F30602-95-C-0247, Army AASERT grant DAAH04-96-10247, AFOSR grant F20602-98-1-0182 NSF grant IRI-9801676. thank Maria Fox DerekLong taking time implement experiment EBL/DDB STAN system.31fiK AMBHAMPATIwould also like thank them, well Terry Zimmerman, Biplav Srivastava, Dan Weld, AvrimBlum Steve Minton comments previous drafts paper. Special thanks dueDan Weld, hosted University Washington Summer 1997, spent time discussingconnections CSP Graphplan. Finally, thank Mark Peot David Smithclean Lisp implementation Graphplan algorithm, served basis extensions.ReferencesAnderson, C., Smith, D., & Weld, D. (1998). Conditional effects graphplan. Proc. AI PlanningSystems Conference.Bacchus, F., & van Run, P. (1995). Dynamic variable ordering CSPs. Proc. PrinciplesPractice Constraint Programming (CP-95). Published Lecture Notes Artificial Intelligence, No. 976. Springer Verlag.Bayardo, R., & Schrag, R. (1997). Using CSP look-back techniques solve real-world sat instances. Proc. AAAI-97.Blum, A., & Furst, M. (1997). Fast planning planning graph analysis. Artificial Intelligence,90(1-2).Bonet, B., Loerincs, G., & Geffner, H. (1999). robust fast action selection mechanismplanning. Proc. AAAI-97.Do, B., & Kambhampati, S. (2000). Solving planning graph compiling CSP. Proc. 5thInternational Conference AI Planning Scheduling.Do, B., Srivastava, B., & Kambhampati, S. (2000). Investigating effect relevance reachability constraints sat encodings planning. Proc. 5th International ConferenceAI Planning Scheduling.Fox, M. (1998). Private correspondence..Fox, M., & Long, D. (1999). Efficient implementation plan graph. Journal Artificial Intelligence Research, 10.Frost, D., & Dechter, R. (1994). search best constraint satisfactions earch. Proc. AAAI94.Gelle, E. (1998). generation locally consistent solution spaces mixed dynamic constraint problems. Ph.D. thesis, Ingenieure informaticienne EPFL de nationalite Suisse, Lausanne, Switzerland.Gerevini, A. (1999). Fast planning greedy planning graphs. Proc. AAAI-99.Gomes, C., Selman, B., & Kautz, H. (1998). Boosting combinatorial search randomization.Proc. AAAI-98, pp. 431437.Kambhampati, S. (1997). Challenges bridging plan synthesis paradigms. Proc. IJCAI-97.32fiP LANNING G RAPHCSPKambhampati, S. (1998). relations intelligent backtracking explanation-basedlearning planning constraint satisfaction. Artifical Intelligence, 105(1-2).Kambhampati, S. (1999). Improving graphplans search ebl & ddb techniques. Proc. IJCAI99.Kambhampati, S., Katukam, S., & Qu, Y. (1997). Failure driven dynamic search control partialorder planners: explanation-based approach. Artificial Intelligence, 88(1-2), 253215.Kambhampati, S., & Nigenda, R. (2000). Distance-based goal ordering heuristics graphplan.Proc. 5th International Conference AI Planning Scheduling.Kambhampati, S., Parker, E., & Lambrecht, E. (1997). Understanding extending graphplan.Proceedings 4th European Conference Planning. URL: rakaposhi.eas.asu.edu/ewspgraphplan.ps.Kautz, H., & Selman, B. (1996). Pushing envelope: Plannng, propositional logic stochasticsearch. Proc. AAAI-96.Kautz, H., & Selman, B. (1999). Blackbox: Unifying sat-based graph-based planning. Proc.IJCAI-99.Koehler, J., Nebel, B., Hoffman, J., & Dimopoulos, Y. (1997). Extending planning graphs adlsubset. Tech. rep. 88, Albert Ludwigs University.Li, C., & Anbulagan (1997). Heuristics based unit propagation satisfiability problems.Proc. IJCAI-97.McDermott,D.(1998).Aips-98planningftp.cs.yale.edu/pub/mcdermott/aipscomp-results.html.competitionresults.McDermott, D. (1999). Using regression graphs control search planning. Aritificial Intelligence, 109(1-2), 111160.Mittal, S., & Falkenhainer, B. (1990). Dynamic constraint satisfaction problems. Proc. AAAI-90.Nguyen, X., & Kambhampati, S. (2000). Extracting effective admissible state-space heuristicsplanning graph. Tech. rep. ASU CSE TR 00-03, Arizona State University.Prosser, P. (1993). Domain filtering degrade intelligent backtracking search. Proc. IJCAI-93.Rymon, R. (1992). Set enumeration trees. Proc. KRR-92.Schiex, T., & Verfaillie, G. (1993). Nogood recording static dynamic constraint satisfactionproblems. Proc. 5th intl. conference tools artificial intelligence.Selman, B., Levesque, H., & Mitchell, D. (1992). GSAT: new method solving hard satisfiabilityproblems. Proc. AAAI-92.Smith, D., & Weld, D. (1999). Temporal planning mutual exclusion reasoning. Proc.IJCAI-99.33fiK AMBHAMPATISrivastava, B., & Kambhampati, S. (1999). Scaling planning teasing resource scheduling.Proc. European Conference Planning.Tsang, E. (1993). Foundations Constraint Satisfaction. Academic Press, San Diego, California.Weld, D., Anderson, C., & Smith, D. (1998). Extending graphplan handle uncertainty & sensingactions. Proc. AAAI-98.Zimmerman, T., & Kambhampati, S. (1999). Exploiting symmetry plan-graph viaexplanation-guided search. Proc. AAAI-99.34fiJournal Artificial Intelligence Research 12 (2000) 235-270Submitted 12/99; published 5/00Backbone Fragility Local Search Cost PeakJosh Singerjoshuas@dai.ed.ac.ukDivision Informatics, University Edinburgh80 South Bridge, Edinburgh EH1 1HN, United KingdomIan P. Gentipg@dcs.st-and.ac.ukSchool Computer Science, University St. AndrewsNorth Haugh, St. Andrews, Fife KY16 9SS, United KingdomAlan SmaillA.Smaill@ed.ac.ukDivision Informatics, University Edinburgh80 South Bridge, Edinburgh EH1 1HN, United KingdomAbstractlocal search algorithm WSat one successful algorithms solvingsatisfiability (SAT) problem. notably effective solving hard Random 3-SATinstances near so-called satisfiability threshold, still shows peak search costnear threshold large variations cost different instances. make numbersignificant contributions analysis WSat high-cost random instances, usingrecently-introduced concept backbone SAT instance. backbone setliterals entailed instance. find number solutions predictscost well small-backbone instances much less relevant large-backboneinstances appear near threshold dominate overconstrained region.show strong correlation search cost Hamming distancenearest solution early WSats search. pattern leads us introduce measurebackbone fragility instance, indicates persistent backbone clausesremoved. propose high-cost random instances local searchlarge backbones also backbone-fragile. suggest decay costbeyond satisfiability threshold due increasing backbone robustness (the oppositebackbone fragility). hypothesis makes three correct predictions. First,backbone robustness instance negatively correlated local search costfactors controlled for. Second, backbone-minimal instances (which 3-SATinstances altered backbone-fragile) unusually hard WSat. Third,clauses often unsatisfied search whose deletioneffect backbone. understanding pathologies local search methods, hopecontribute development new better techniques.1. Introductionproblem instances require high computational cost algorithmssolve? Answering question help us understand interaction searchalgorithms problem instance structure potentially suggest principled improvements, example Minimise-Kappa heuristic (Gent, MacIntyre, Prosser, & Walsh,1996; Walsh, 1998).paper study propositional satisfiability problem (SAT). SAT importantfirst perhaps archetypal NP-complete problem. Furthermore, manyc2000AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiSinger, Gent & SmaillAI tasks practical interest constraint satisfaction, planning timetablingnaturally encoded SAT instances.SAT instance C propositional formula conjunctive normal form. C bagclauses represents conjunction. clause disjunction literals,Boolean variables negations. variables constitute set n symbols V .assignment mapping V {true, false}. decision question SAT asks whetherexists assignment makes C true standard logical interpretationconnectives. assignment solution instance. solution,SAT instance said satisfiable. study, assignments clausesunsatisfied also important. refer quasi-solutions. k-SAT SATproblem restricted clauses containing k literals. Notably, k-SAT decision problemNP-hard k 3 (Cook, 1971). several NP-hard decision problems, 3SAT, certain probabilistic distributions instances parameterised control parameterexhibit sharp threshold phase transition probability solution(Cheeseman, Kanefsky, & Taylor, 1991; Gent et al., 1996; Mitchell, Selman, & Levesque,1992). critical value control parameter instances generatedparameter region lower critical value (the underconstrained region)almost always solutions. generated overconstrained regioncontrol parameter higher critical value almost always solutions.many problem distributions, threshold associated peak search costwide range algorithms. Instances generated distribution controlparameter near critical value hardest cost decays move valuelower higher values. behaviour interest basic AI research. devoidregularities, random instances represent challenge faced algorithmabsence assumptions problem domain, knowledgeexploited design algorithm transformation problem instance.Random k-SAT parameterised distribution k-SAT instances. Random k-SAT,n fixed control parameter m/n. Varying m/n produces sharp thresholdprobability satisfiability associated cost peak range complete algorithms(Crawford & Auton, 1996; Larrabee & Tsuji, 1992). cost peak pattern Randomk-SAT conjectured extend reasonable complete methods CookMitchell, (1997) also give overview analytic experimental resultsaverage-case hardness SAT distributions.paper study behaviour local search Random k-SAT. term localsearch encompasses class procedures series assignments examinedobjective finding solution. first assignment typically selected random.Local search proceeds moving one assignment another flipping (i.e.inverting) truth value single variable. variable flip chosen using heuristicmay include randomness, element hill-climbing (for example numbersatisfied clauses) memory. Usually, local search incomplete SAT decisionproblem: guarantee solution exists, found within timebound. Unlike complete procedures, local search cannot usually prove certainsolution exists.relatively recent discovery (e.g. Selman, Levesque Mitchell, 1992)average cost local search procedures scales much better best complete236fiBackbone Fragility Local Search Cost Peakprocedures critical value m/n Random 3-SAT. recent studies, (e.g. ParkesWalser, 1996) confirmed detail. Therefore system completenessmay sacrificed, local search procedures important role play,generated much interest recent years.restrict instances distribution satisfiableincrease control parameter, peak cost local search proceduressolve instances near critical value several constraint-like problems (Clark, Frank,Gent, MacIntyre, Tomov, & Walsh, 1996; Hogg & Williams, 1994). underconstrainedregion, average cost increases m/n due decreasing number solutions perinstance (Clark et al., 1996). However, overconstrained region, cost decreasesm/n although number solutions per instance continues fall. Several researchersnoted fact surprise (Clark et al., 1996; Parkes, 1997; Yokoo, 1997) sincenumber solutions change special way near critical value. Why, then,cost satisfiable instances peak near critical value, decay?Parkes (1997) provided appealing answer first part question studybackbone satisfiable Random 3-SAT instances. satisfiable SAT instances,backbone set literals logically entailed clauses instance1 .Variables appear entailed literals forced take particular truthvalue solutions. Parkes study demonstrated instances underconstrained region, small fraction variables, any, appear backbone.However, control parameter increased towards critical value, subclassinstances large backbones, mentioning around 75-95% variables, rapidlyemerges. Soon control parameter increased overconstrained regionlarge-backbone instances account satisfiable instances. Parkes alsoshowed fixed value control parameter, cost local search procedure WSat strongly influenced size backbone. suggests peakaverage WSat cost near critical value control parameter increased may dueemergence large-backbone instances point. Parkes noted givensize backbone, cost actually higher instances underconstrained regionfalls control parameter increased. also identified fall indicativeanother factor produces overall peak cost. main aim paperidentify factor responsible pattern; instances certain sizebackbone costly solve others?remainder paper organised follows. Section 2 review detailsWSat algorithm Random k-SAT distribution discuss experimentalconditions used. also elucidate patterns cost intendexplain show number solutions backbone size interact. Section3 identify remarkable pattern WSats search behaviour clearly distinguisheshigh cost lower cost instances certain backbone size. WSat usually drawnearly search quasi-solutions clauses unsatisfied. high costinstances, quasi-solutions distant nearest solution, lower costinstances equal backbone size, less distant. Section 4 develop causalhypothesis, postulating structural property instances induces search space1. Here, use term backbone follows Monasson, Zecchina, Kirkpatrick, Selman Troyansky(1999a, 1999b) whose definition backbone equivalent satisfiable instances.237fiSinger, Gent & Smaillstructure turn causes observed search behaviour thus cost pattern.suggest instances certain backbone size high cost backbonefragile, i.e. removal clauses random results instancegreatly reduced backbone size. discuss property may measured showcontrol parameter increased, instances certain backbone size become lessbackbone-fragile.hypothesis true scientific merit makes correct predictions. hypothesis made three correct predictions provide experimental evidence.Section 5 show degree instance backbone-fragile accountsvariance cost control parameter backbone-size fixed.Section 6 consider generation instances backbone-fragile.clauses removed backbone unaffected, found resulting instances became progressively backbone-fragile. Eventually, clausesremoved without affecting backbone instance said backbone minimal.hypothesis correctly predicts clauses removed way Random3-SAT instances, cost becomes considerably higher. Section 7 show hypothesis makes correct prediction relating search behaviour: clausesoften unsatisfied search whose removal affects backbone.Section 8 relate study previous research give suggestions work.Finally, Section 9 concludes.2. Backgroundsection discuss local search algorithm WSat, measurement computational cost representativeness local search algorithms general. alsoreview Random k-SAT distribution overall cost pattern WSat Randomk-SAT. Finally look backbone size number solutions interact affectcost.2.1 WSat Algorithmterm WSat first introduced Selman et al. (1994). refers local searcharchitecture also subject number subsequent empirical studies(Hoos, 1999a; McAllester, Selman, & Kautz, 1997; Parkes & Walser, 1996; Parkes, 1997).pseudocode outline WSat algorithm given Figure 1. important featureWSat that, unlike earlier local search algorithms, chooses unsatisfied clauseflips variable appearing clause: Select-variable-from-clause mustreturn variable mentioned clause. architecture first seen random walkalgorithm due Papadimitriou (1991). WSat may use different strategies Selectvariable-from-clause. study, used SKC strategy introduced Selman,Kautz Cohen (1994); refer combination simply WSat. PseudocodeSKC strategy given Figure 2.follow Hoos (1998) approach measuring computational cost SATinstances local search algorithm WSat. Rather run-times, measure runlengths : number flips taken find solution. set noise level p 0.55,Hoos found approximately optimal Random 3-SAT. Hoos Stutzle (1998) showed238fiBackbone Fragility Local Search Cost PeakWSat(C, Max-tries, Max-flips, p)= 1 Max-tries:= random assignmentj = 1 Max-flipsclause := unsatisfied clause C, selected randomv := Select-variable-from-clause(clause, C, p):= vs value flippedsatisfyingreturnendendendreturn satisfying assignment foundFigure 1: WSat local search algorithmSelect-variable-from-clause(clause, C, p)variable x mentioned clausebreaks[x] := number clauses C wouldbecome unsatisfied x flippedendvariable clause breaks[y] = 0return variable, breaking ties randomlyelseprobability 1 preturn variable z clauseminimises breaks[z], breaking ties randomlyprobability preturn variable z clausechosen randomlyendFigure 2: SKC variable selection strategyrun lengths easiest instances exponentially distributed many localsearch variants. implies random restart mechanism (the re-randomisationMax-flips flips) significantly worthwhile.239fiSinger, Gent & Smaillknown date whether, without using restart, WSat almost surely (i.e.probability approaching 1) find solution satisfiable 3-SAT instances given unlimitedflips. local search algorithm eventually find solution conditions,said probabilistically approximately complete (PAC). Hoos (1999a) proved whetherseveral local search algorithms PAC Culberson Gent (1999a) provedWSat PAC 2-SAT case. Hoos (1998) observed data suggested WSatcould PAC. set Max-tries 1 Max-flips infinite runs reportedpaper. solution found every run, evidence WSat mayPAC.Another implication exponential distribution run lengths large numbersamples must taken obtain good estimate mean. Following Hoos, usemedian 1000 WSat runs instance descriptive statistic representingWSats search cost instance. appears give stable estimate cost(as less sensitive long tail mean) moderate amountcomputational effort.One objection studying single algorithm local search class mayrepresentative: results obtained algorithm may generalise membersclass. accept objection, evidence certain conditions,one local search algorithm actually large extent representative whole class.example Hoos (1998) found high correlation computational costsrandom instances pairs different local search algorithms, including WSat. alsosuggests algorithm-independent property instances resultshigh cost class algorithms.2.2 Random k-SATuse well-studied Random k-SAT distribution (Franco & Paull, 1983; Mitchell et al.,1992) k = 3. Random k-SAT distribution k-SAT instances, parameterisedratio clauses variables m/n. Let V fixed set Boolean variable symbolssize n. generate instance Random k-SAT clauses n variables,clause C independently chosen randomly selecting literals k distinct variablesV independently negating probability 12 . guaranteevariables mentioned instance contain duplicate clauses.local search cannot solve unsatisfiable instances, filter using completeSAT procedure. order control effects backbone size, also needisolate portion satisfiable part distribution backbone sizecertain value. obtained calculating backbone size satisfiableinstance discarding whose backbone required size. termcontrolling backbone size. Satisfiable instances certain backbone sizes rarecertain values m/n. example m/n 4.49, found 1 20,000generated instances satisfiable backbone size 10. Hence generation instancesway somewhat costly computational terms. therefore onelimits value n data could collected.240fiBackbone Fragility Local Search Cost Peakprimarily interested threshold region control parameter,cost peak occurs: region near point 50% instances satisfiable.looked region 90% 20% satisfiability.2.3 Pattern WSat Cost Random 3-SATFigure 3 show peak WSat cost mentioned e.g. Parkes(1997). peak slightly 50% point (4.29) median appears shifthigher percentiles. similar pattern noticed Hogg Williams (1994)local search cost graph colouring.9000800095th7000cost6000500090th4000300075th200050th1000025th44.14.24.3m/n4.44.5Figure 3: cost peak WSat m/n varied. level m/n, generated5000 satisfiable instances. measured per-instance WSat cost these.line plot gives different point cost distribution, e.g. 90thpercentile difficulty 500th costly instance WSat.Parkes (1997) Yokoo (1997) suggest local search cost peak shownWSat Figure 3 result two competing factors. m/n increased numbersolutions per instance falls causes onset high cost. However, numbersolutions continues fall overconstrained region cost decreases.must therefore second factor whose effect outweighs number solutionsoverconstrained region cause fall cost. main aim paperidentify factor. pattern WSat cost Random 3-SAT identified Parkes (1997)241fiSinger, Gent & Smaillstarting point. Parkes observed given backbone size n, averagecost falls m/n increased.Figure 4 shows fall WSat cost n = 100 Random 3-SAT instances.point plot median cost 1000 instances2 length barsinterquartile range instance cost. fall cost approximately exponential decayrange m/n near threshold range backbone sizes. rate decayaffected backbone size, cost large-backbone instances decaying fastest.length error bars Figure 4 along log scale cost axis indicatesdistribution per-instance cost also positively skewed even backbone sizecontrolled. example point m/n 4.11 backbone size 0.9ndifference 75th percentile median 4000 whereasmedian 25th percentile half that. spread cost large, particularlyrelative effect control parameter. think significant portionvariance cost among instances due errors estimates costinstance.backbone size = 0.9 nbackbone size = 0.5 nbackbone size = 0.1 n4cost1031044.054.14.154.24.25m/n4.34.354.44.454.5Figure 4: effect varying m/n cost backbone size controlled.2. cost instance defined median run length 1000 runs point Figure 4median medians.242fiBackbone Fragility Local Search Cost Peak2.4 Number Solutions Backbone Size Controlledstudied effect number solutions WSat cost. number solutionsdetermined using modified complete procedure. small-backbone instances,evidence number solutions actually increases m/n, leastoverconstrained region. Figure 5 shows plot number solutions, backbonesize controlled 0.1n. point median 1000 instances bars showinterquartile range. possible increase number solutions may help explainfall cost small-backbone instances, appears weak effectaccount full.610number solutionsbackbone size = 0.1 n51041044.054.14.154.24.25m/n4.34.354.44.454.5Figure 5: Number solutions n = 100, m/n varied, backbone size controlled0.1n.studied relationship number solutions WSat costbackbone size controlled different values. Figure 6 shows log-log plot numbersolutions cost, m/n 4.29 backbone size 0.1n. linear least squaresregression (lsr) fit superimposed. Table 1 gives summary data log-log scatter plotdifferent backbone sizes transition : gradient intercept lsr fits,product-moment correlation r rank correlation.number solutions strongly negatively related cost smaller backbone sizes transition strength relationship fairly constantm/n varied. speculate strong relationship instances arises243fiSinger, Gent & Smaillm/n4.034.114.184.234.294.354.414.49Backbonesize0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9nInterceptlsr fit3.89934.14104.20703.87274.15514.13873.78674.05334.02023.77713.98903.98913.73093.91693.78363.69813.89333.81733.60833.84453.77723.54833.75773.6228Gradientlsr fit0.19670.21230.13720.19890.23040.13360.19110.21800.11460.19320.21400.12700.19100.20760.06100.18960.21330.10180.17820.20940.11200.17480.20430.0842rRank corr.0.78080.67610.13070.76960.68340.12750.76640.69320.11590.78290.67290.13170.77870.69210.06120.80070.68720.10440.77840.70240.11790.79720.69540.0992-0.7731-0.6699-0.1365-0.7669-0.6855-0.1291-0.7760-0.6974-0.1217-0.7873-0.6867-0.1329-0.7844-0.6941-0.0534-0.7994-0.6967-0.0903-0.7628-0.7085-0.1045-0.7932-0.6991-0.0783Table 1: Data log-log correlations number solutions cost n = 100,m/n varied backbone size fixed different values.244fiBackbone Fragility Local Search Cost Peakm/n = 4.29, backbone size = 0.1 n4cost10310210210310410510number solutions610710810Figure 6: Scatter plot number solutions cost n = 100, m/n = 4.29backbone size fixed 0.1n.finding backbone straightforward main difficulty encountering solutionbackbone satisfied. density solutions region satisfyingbackbone important. larger backbone sizes, number solutions lessrelevant cost. significant change number solutions large backboneinstances observed m/n varied. number solutions coststrongly related instances unsurprising, large backbone size impliessolutions lie compact cluster local searchs main difficulty findingcluster (i.e. satisfying backbone). Therefore expect density solutionswithin cluster important. Hoos (1998) observed correlationnumber solutions local search cost becomes small overconstrained region.explained simply fact large-backbone instances dominateregion.3. Search Behaviour: Hamming Distance Nearest Solutionorder suggest cause cost decay large-backbone instancesobserved Section 2.3, made detailed study WSats search behaviour, i.e.assignments visited search. report exploratory part research245fiSinger, Gent & Smaillsection. explain somewhat novel search behaviour metrics usedgiving results discussion them.3.1 Definitions MethodsAssuming local search algorithm PAC, given run unlimited length, fb ,number flips taken find first assignment b clauses unsatisfied,well-defined b 0. f0 equal run length.particular run local search algorithm consists series assignmentsT0 , T1 , ..., Tf0 , Ti assignment visited flips made. foundRandom 3-SAT n = 100, assignment satisfying clauses quicklyfound remainder search, clauses (1 - 10) unsatisfied.shown Gent Walsh (1993) GSat, rapid hill-climbing phase,also suggested Hoos (1998), followed long plateau-like phase numberunsatisfied clauses low constantly changing. experiments used f5arbitrary indicator length hill-climbing phase. Unlike GSat, WSatwell-defined end point hill-climbing phase, since short bursts hill-climbingcontinue occur rest search. think using fb indicatorvalue b 1 10 would give similar results.Local search proceeds flipping variable values might expect Hamming distance current assignment nearest solution may alsointerest. Hamming distance two assignments hd(T1 , T2 ) simply numbervariables T1 T2 assign differently. studied Hamming distancecurrent assignment solution Tsol C hd(T, Tsol ) minimised.abbreviate hdns(T, C) (Hamming distance nearest solution). assignment, hdns(T, C) may calculated using complete SAT procedure modifiedevery solution C visited Hamming distance calculated.3.2 Resultssection, data based Random 3-SAT instances n = 100 backbone sizecontrolled various values 0.1n 0.9n. Recall control backbonecertain value, generate satisfiable Random 3-SAT instances usual discardwhose backbone required size. varied m/n point 90%satisfiability (4.03) point 20% satisfiability (4.49). hdns(Tf5 , C) Hammingdistance first assignment 5 clauses unsatisfiednearest solution. instance calculated median value f5 meanvalue hdns(Tf5 , C) based 1000 runs WSat. plots Figures 7 8,point median 1000 instances.Figure 7 shows effect varying m/n f5 backbone size controlled.values f5 low compared cost range small. althoughcost find solution varies considerably instance instance, quasi-solutionsquickly found matter overall cost. However, notable effectsbackbone size m/n f5 . might expected, larger backbone instances,overall cost generally higher, WSat takes slightly longer find quasi-solution.effect m/n unexpected. backbone size controlled 0.5n more, m/n246fiBackbone Fragility Local Search Cost Peakincreased WSat takes slightly longer find quasi-solution, although simultaneously costdecreasing seen Figure 4.170backbone size = 0.9 nbackbone size = 0.7 nbackbone size = 0.5 nbackbone size = 0.3 nbackbone size = 0.1 n160150140f5130120110100908044.054.14.154.24.25m/n4.34.354.44.454.5Figure 7: effect varying m/n f5 backbone size controlled.Figure 8 shows effect varying m/n hdns(Tf5 , C) effects backbonesize controlled for. plot, bars give interquartile range. spreadvalues mean hdns(Tf5 , C) point also small relative effect varyingm/n. positive effect backbone size hdns(Tf5 , C) one might expectsince backbone size affects cost.backbone size controlled, m/n increased satisfiability threshold,mean hdns(Tf5 , C) decreases linearly wide range backbone values. Hence, althoughquasi-solution (Tf5 ) usually quickly found, instances lower m/n quasisolution considerably Hamming-distant nearest solution. m/n increased,backbone size controlled, effect gradually lessened.also looked relationship search behaviour costm/n fixed backbone size controlled. found case variancehdns(Tf5 , C) accounts cost variance. Figure 9 shows plot meanhdns(Tf5 , C) cost backbone size controlled 0.5n m/n fixed 4.29.lsr fit superimposed. plot suggests hdns(Tf5 , C) linearly related log cost.Table 2 gives intercept gradient lsr fits r values backbone sizecontrolled three values m/n varied. Variance hdns(Tf5 , C) accountsvariance cost three different backbone sizes consistent247fiSinger, Gent & Smaill45backbone size = 0.9 nbackbone size = 0.5 nbackbone size = 0.1 n405hdns(Tf ,C)353025201544.054.14.154.24.25m/n4.34.354.44.454.5Figure 8: effect varying m/n hdns(Tf5 , C) backbone size controlled.threshold. scatter plots (not shown) linear lsr fits data similarshape Figure 9 consistent linear relationship. r valuesgreatest small-backbone instances reasons unclear. Possibly, sincesearch shorter small-backbone instances, success follows quickly f5hdns(Tf5 , C) better indicator likelihood finding solution.Figure 8 showed backbone size controlled, hdns(Tf5 , C) falls linearly m/nincreased. gradient fall 14. Table 2 showed backbone sizecontrolled m/n fixed, hdns(Tf5 , C) linearly related log cost, gradientfit around 0.08. Given linear relationship continues holdconstant gradient m/n varied (in fact gradient decreases slightly) assumingincreasing m/n affecting cost means, would expect lineardecrease log mean cost gradient 1.12, slightly steeperobserved decrease log median cost shown Figure 4.results consistent idea whatever factor causes cost decayexponentially m/n varied largely causing hdns(Tf5 , C) fall linearly.3.3 Discussionidentified pattern search behaviour strongly related patterncost discussed Section 2.3. interpretation pattern follows.248fiBackbone Fragility Local Search Cost Peakm/nBackbone size4.030.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n4.114.184.234.294.354.414.49Interceptlsr fit1.05280.69280.70651.01660.63150.81581.08580.80900.81091.12900.83430.74801.12891.00320.83821.16640.98350.98351.20291.02741.10701.24581.14721.1930Gradientlsr fit0.08440.09250.08950.08680.09550.08670.08390.08950.08640.08210.08870.08780.08260.08280.08560.08110.08420.08080.07950.08300.07680.07770.07870.0742r0.94450.87690.73080.95110.88520.71960.95560.87990.71950.95810.89740.76910.95500.89350.75790.96280.89960.77280.95650.91350.78160.96610.91970.8086Table 2: Data correlations hdns(Tf5 , C) log10 cost n = 100 m/nbackbone size fixed different values.249fiSinger, Gent & Smaill5104cost10310210161820222426hdns(Tf ,C)28303234365Figure 9: relationship hdns(Tf5 , C) log cost backbone size controlled0.5n m/n fixed 4.29.instance quasi-solutions WSat visits form interconnected areas search spacelocal search always move solution them, without often movingassignment many clauses unsatisfied. evidence simply WSatruns apparently always successful visit assignments clausesunsatisfied infrequently. Frank, Cheeseman Stutz (1997) also mentionedanalysis GSat search spaces Random 3-SAT, local minima clausesunsatisfied usually escaped unsatisfying one clause.believe instances higher cost quasi-solution area extends partssearch space Hamming-distant solutions, whereas instances lower costarea less extensive. mean Hamming distance early quasi-solutionTf5 nearest solution accurate indicator extensive quasi-solutionarea is. interpretation suggests hdns(Tf5 , C) strongly correlated cost:extensiveness quasi-solution area determines costly search. alsosuggests why, instances higher cost, quasi-solutions found slightly quickly:quasi-solution area extensive, random starting point shorter serieshill-climbing flips required find quasi-solution.250fiBackbone Fragility Local Search Cost Peakmean hdns(Tf5 , C) decreases linearly m/n increased backbone sizecontrolled. time, cost decays exponentially. think m/nincreased, quasi-solution area becomes progressively less extensive.4. Causal Hypothesispattern search behaviour Section 3 interpretation suggestedcausal hypothesis account decay cost discussed Section 2.3 henceoverall peak. key hypothesis property SAT instances: backbone fragility.property qualitatively consistent observations. importantly,although backbone fragility implications instances search space topology,property based logical structure SAT instance. section motivatedefine backbone fragility, discuss may measured show relatespatterns reported Sections 2.3 3.4.1 Backbone Fragility : MotivationSuppose B small sub-bag clauses satisfiable SAT instance C,exists set quasi-solutions QB clauses B unsatisfied.structural property C would cause quasi-solutions QB attractive WSat?already know backbone Random 3-SAT instance small, solutionsfound little search (Parkes, 1997). solutions C B (C B denotes Cone copy member B removed) either solutions C members QB .assume assignments attractive WSat C approximatelyassignments attractive C B, members QB (whichsolutions C B) attractive C backbone C B small, particularlyCs backbone large. Furthermore TB QB , number variablesappear backbone C B upper bound hdns(TB , C), large reductionbackbone size allows high hdns(TB , C). summarise, removal certainsmall sub-bag clauses causes backbone size greatly reduced, expectquasi-solutions clauses unsatisfied attractive WSatpossibly Hamming-distant nearest solution.interested quasi-solutions general rather QB . removingrandom small set clauses average causes large reduction backbone size,say instance backbone-fragile. effect backbone smalleraverage, instance backbone-robust. large-backbone instance backbone-fragile,extension argument expect general quasi-solutions attractivemay Hamming-distant nearest solution. Hence idea consistentobservations interpretation Section 3: backbone fragility approximatelycorresponds extensive quasi-solution area is.idea backbone fragility underlying factor causing search behaviourpattern appealing reasons. entailed literal l C, mustsub-bag clauses C whose conjunction entails l. given backbone size, clausesadded, given entailed literal l expect extra clauses allow alternativecombinations clauses entail l. Hence adding clauses whilst controllingbackbone size, random removal clauses less effect backbone since251fiSinger, Gent & Smaillfact literal entailed depends less presence particular sub-bags.clauses added, expect instances become less backbone-fragile. Givenhypothetical relationship backbone fragility search behaviour, wouldexplain qualitatively search behaviour changes m/n varied.think backbone fragility property instances logical structure,study may also lead results complexity issues, postpone discussionSection 8.4.2 Measurement Backbone Robustnessdefine measure backbone robustness instance allow ustest predictions hypothesis. take instance C delete clauses random,halting process backbone size reduced least half. pointrecord result number deleted clauses. constitutes one robustness trial.metric backbone robustness mean result possible trials, i.e.average number random deletions clauses must made reducebackbone size half.infeasible compute results possible robustness trials. Therefore,measuring backbone robustness instance estimated computing averagerandom sample trials. used least 100 robustness trials case orderensure reasonably accurate estimate, continued sample robustness trialsstandard error less 0.05 sample mean (in case estimatemean accurate within 10% 95% confidence level). n = 100,using satisfiable instances near satisfiability threshold whose backbone sizecontrolled 50, usually less 250 robustness trials required estimateconverge way. Even then, backbone robustness costly compute.different possible metrics backbone fragility/robustness, foundmetric described gave clearest results purposes without unnecessarily complicated definition. metrics, reduction backbone sizerandom fixed fraction clauses removed, may suitable contexts.4.3 Change Backbone Robustness Control Parameter Varieddiscussed Section 4.1 expect backbone size controlled, backbone robustnessincreases m/n increased. Since measure backbone robustness defined termssize backbone, useful comparing instances equal backbonesize.found increasing control parameter made instances backbone-robust,expected. Figure 10 shows effect backbone robustness increasing m/nsatisfiability threshold n = 100 backbone size controlled. pointmedian 1000 instances.note backbone robustness defined measure generally higherinstances larger backbones. think large-backboneinstances, backbone must reduced larger number literals fragility trialrequires clauses removed.252fiBackbone Fragility Local Search Cost Peak222018backbone robustness161412108backbone size = 0.9 nbackbone size = 0.5 nbackbone size = 0.1 n6444.054.14.154.24.25m/n4.34.354.44.454.5Figure 10: Backbone robustness satisfiability transition, backbone sizefixed 0.1n 0.5n 0.9n.5. Correct Prediction Cost Variancemay assert fall cost observed increase control parameterdue change factor F , example Yokoo (1997) has.assertion makes important testable prediction: variation Fcontrol parameter fixed accounts variation cost. However mayfactors whose influence cost great obscure effect Fcontrol parameter fixed. best reveal effect F , any, effectsfactors may controlled for.Backbone robustness proposed factor F . backbone size another factorstrongly influences cost. result section effectsm/n backbone size controlled for, i.e. fixed, effects backbonerobustness seen quite clearly large-backbone instances.5.1 Correlation DataFigure 11 shows plot log cost measure backbone robustnessRandom 3-SAT instances n = 100, m/n 4.29 backbone size controlled 0.1n,0.5n 0.9n. linear lsr fit superimposed case. Table 3 gives intercept,253fiSinger, Gent & Smaillm/nBackbone size4.030.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n0.1n0.5n0.9n4.114.184.234.294.354.414.49Interceptlsr fit3.03383.70754.28462.96393.66754.22872.93653.60674.18112.92573.51424.13132.87663.49034.09342.82613.43253.99392.79253.37723.92842.71643.35063.8720Gradientlsr fit0.02040.03700.04190.01340.03510.03700.01460.03020.03380.01550.02390.03120.01360.02250.02900.01090.01990.02370.01000.01720.02110.00730.01700.0198rr 95%r +95%0.19280.37300.47110.14900.38910.45350.17450.38400.53060.21070.36430.52530.18940.38630.53250.16710.37340.49840.17630.34520.51520.13920.40340.55490.25060.41910.51650.20880.43550.50010.23560.42720.56870.26590.41050.56450.24830.43500.57210.22500.42220.53940.23210.39540.55820.19260.45850.59490.14000.32350.42510.08730.34170.40650.11490.33890.49210.15530.31540.48180.13000.33950.49310.11050.32440.45550.11750.29190.46920.08410.35160.5125Rank corr.coefficient0.19340.37130.46990.14020.37700.46620.16630.37380.54660.21160.34360.54570.20530.39960.54670.17240.37820.52430.16830.35820.52700.13550.40270.5604Table 3: Data correlation backbone robustness log10 cost n = 100m/n backbone size fixed different values.gradient r values lsr fits backbone size controlled three valuesm/n varied threshold.r values suggest effect backbone robustness cost, particularly largebackbone sizes. smaller backbone sizes, imagine finding backbone lessissue backbone fragility, hinders this, less effect. largerbackbone sizes, think main difficulty WSat satisfying backbone; backbonefragility important. However, given somewhat unclear shape scatter plots,several concerns significance correlation, addressusing simple statistical methods.254fiBackbone Fragility Local Search Cost Peakm/n = 4.29, backbone size = 0.1 n3cost102100510152025backbone robustness30354035403540m/n = 4.29, backbone size = 0.5 n4cost103102100510152025backbone robustness30m/n = 4.29, backbone size = 0.9 n510cost4103102100510152025backbone robustness30Figure 11: Scatter plot backbone robustness cost n = 100, m/n = 4.29backbone size fixed 0.1n, 0.5n 0.9n.255fiSinger, Gent & Smaill5.2 Artifact Distributions Variables?One concern observed r could also arisen simply distributionstwo variables rather relationship them. seriousconcern distributions unknown.null hypothesis, H0 value r results distributionstwo variables equal observed r. randomisation method used test H0 .See Appendix details method. data set presented, 1000 randomisedpairings data constructed. case, found observed rfall within range sampling distribution r randomised pairings. H0therefore rejected 99.9% confidence level.r coefficient, given above, greatly affected outliers. Therefore rankcorrelation coefficient, less affected, also calculated. rank correlationalso given Table 3. found case rank correlation coefficientconsiderably different r coefficient. demonstrates observed rgreatly affected outliers.5.3 Confidence Intervals CorrelationGiven relationship two variables merely artifactdistributions outliers, accurate measurement r? bootstrapmethod used obtain bounds confidence interval statistic. Again,reader referred Appendix details method. Using method 1000pseudo-samples obtained lower upper bounds 95% confidence interval r,also given Table 3 r 95% r +95% respectively. data implies 95%confidence, upper bounds amount error estimates r 20.02 0.05.6. Correct Prediction Backbone-Fragile Instanceshypothesis proposes high backbone fragility instances quite accurately representsfactor (via search behaviour patterns uncovered Section 3) causes highWSat cost instances. However, plausible high backbone fragilityby-product unmeasured latent factor causally relatedcost.help establish causal link backbone fragility cost, therefore created sets random SAT instances higher backbone fragility usual Random3-SAT instances. degree following methodological precedent BayardoSchrag (1996), created random instances contained small unsatisfiable subinstances constraints overall. often found exceptionallyhard complete procedure Ntab. experiments thereby helped establishfeature instance structure cause exceptionally high cost completeprocedures.cannot easily set backbone fragility directly, since generation parameter.One manipulation experiment possible use instance generation procedure results instances higher backbone fragility. hypothesis predicts256fiBackbone Fragility Local Search Cost Peakinstances generated using procedure harder Random 3-SAT instances.section define procedure test prediction. may procedure also manipulating latent factor. However, since procedure specificallydesigned increase backbone fragility, correct prediction still lends credibilityhypothesis.6.1 Backbone-minimal Sub-instancesSuppose SAT instance C remove clause backboneaffected removal clause. clauses repeatedly removed, eventuallyinstance clause removed without disturbing backbone.case backbone-minimal sub-instance (BMS) C. formally,following definition:Definition SAT instance C 0 BMS C iffC 0 sub-instance C (i.e. sub-bag clauses) C 0backbone C.clause c C 0 exists literal l that:1. C 0 l2. (C 0 {c}) l satisfiablei.e. every strict sub-instance C 0 strictly smaller backbone backbone C 0 2BMSs seen satisfiable analogues minimal unsatisfiable sub-instances(MUSs) unsatisfiable instances studied amongst others Culberson Gent (1999b)context graph colouring Gent Walsh (1996) Bayardo Schrag(1996) satisfiability. MUS instance C sub-instance unsatisfiable,removal one clause sub-instance renders satisfiable.unsatisfiable instances must MUS, satisfiable SAT instances mustBMS. BMS depend non-empty backbonebackbone instance empty, BMS empty sub-instance. instanceone BMS. Different BMSs instance may share clauses. One BMSinstance cannot strict sub-instance another.Suppose backbone satisfiable instance C set literals {l1 , l2 , . . . , lk }. Letclause l1 l2 . . . lk . following useful fact:Theorem C 0 BMS C iff C 0 MUS C 2simple proof given Appendix B. Due fact, methods studyingMUSs applied study BMSs. study BMSs satisfiableinstance C finding backbone C studying MUSs C d:corresponds BMS C since must present every MUS C d.find BMS C determine backbone, find random MUS C usingMUS-finding method Gent Walsh (1996) remove result.257fiSinger, Gent & SmaillInstancesPreserve-backbone(C, 0, C 0 )Preserve-backbone(C, 5, C 0 )Preserve-backbone(C, 10, C 0 )Preserve-backbone(C, 20, C 0 )Preserve-backbone(C, 40, C 0 )Preserve-backbone(C, 80, C 0 )BMSBackbone robustness10th percentile Median 90th percentile8.5845 12.970020.63007.7374 12.031719.17007.1977 11.085117.35006.06909.391314.53514.26226.48999.99002.07452.86613.98511.02001.06001.1600Table 4: effect Preserve-backbone backbone robustness.6.2 Interpolating Instance one BMSsBMS C 0 established, also study effects interpolationC C 0 removing random C clauses appearC 0 . equivalent removing clauses random backbone preserved.Preserve-backbone(C, mr , C 0 ) denote C mr clauses, appearBMS C 0 , removed random. resulting instance backboneC.increasing m/n controlling backbone size causes backbone robustnessincrease, found deleting clauses backbone unaffected causesbackbone robustness (as measured above) decrease, one might expect.used 500 Random 3-SAT instances n = 100 m/n = 4.29. instancefound one BMS. used Preserve-backbone interpolate mr setvarious values. Table 4 shows effect increasing mr backbone robustness.BMSs threshold instances backbone-fragile removal one clauselikely reduce backbone half more.hypothesis predicts interpolation C C 0 proceeds, costlocal search increases backbone robustness decreases. conceivable,although would surprising, removing clauses random instancesnear threshold generally makes cost local search increase.case, increase cost interpolation towards BMS could merely dueremoval clauses per se rather removal clauses whilst preserving backbone.control possibility also removed clauses according two procedures.procedure Random(C, mr ) removes mr clauses C random. procedureReduce-backbone(C, mr ) removes mr clauses time clause removed,size backbone reduced. clause removed chosen randomlyclauses. procedure therefore uses opposite removal criterion Preservebackbone. backbone becomes empty, clauses removed.Figure 12 shows effect per-instance cost applying three clause removalprocedures set 500 Random 3-SAT threshold instances. plotmedian per-instance cost.258fiBackbone Fragility Local Search Cost Peak410costPreservebackboneReducebackboneRandom310210010203040mr50607080Figure 12: effect three clause removal procedures median per-instance cost.observe removing clauses randomly backbone strictly reduced, causes cost reduced, removal clauses cause highercost. Reduce-backbone procedure causes greater initial fall cost, backbone size reduced quickly Random. However, cost stabilisesReduce-backbone backbone becomes empty thereafter clausesremoved.Removing clauses according Preserve-backbone causes local search costincrease amount approximately exponential number clauses removed. Table5 gives data effect also cost data BMSs. interpolation shiftswhole distribution up, median. median cost BMSs,backbone-fragile instances, three times 90th costpercentile Random 3-SAT instances.BMSs instances 254 318 clauses. resultstherefore demonstrate existence instances underconstrained regionmuch harder typical instances near satisfiability threshold. However sinceobtained sampling Random 3-SAT directly, knowoften occur. far know, vanishingly rare therefore, contrastexceptionally hard instances complete algorithms, seems unlikely affectmean cost. Also, Gent Walsh (1996) showed exceptionally hard259fiSinger, Gent & SmaillInstancesPreserve-backbone(C, 0, C 0 )Preserve-backbone(C, 5, C 0 )Preserve-backbone(C, 10, C 0 )Preserve-backbone(C, 20, C 0 )Preserve-backbone(C, 40, C 0 )Preserve-backbone(C, 80, C 0 )BMSPer-instance cost10th percentile Median 90th percentile51714505175537151556575571608600957018037037643229510683816415424313155616945135883Table 5: effect Preserve-backbone per-instance cost.instances complete algorithms hard different reason thresholdinstances, BMSs apparently hard reason backbonefragile.One useful by-product section means generating harder test instanceslocal search variants without increasing n. However instances require O(m + n)complete searches generate: O(n) determine satisfiability backbone O(m)reduce BMS.7. Correct Prediction Search BehaviourRecall motivating discussion Section 4.1 suggested quasisolutions QB would attractive backbone C B small. sayclauses B likely set unsatisfied clauses removalclauses B large effect backbone. part hypothesis also makesprediction search behaviour clauses often unsatisfied WSatwhose removal reduces backbone size most. section show predictioncorrect.looked individual instances cost percentiles set 5000 Randomk-SAT instances n = 100 m/n = 4.29. Per-instance cost determinedprevious sections. clause instance, calculated number backboneliterals longer entailed clause removed. simple measurebackbone contribution (bc) clause much backbone size dependspresence clauses. clauses backbone contribution high, termed backbonecritical clause. made 1000 runs WSat instance conditionsprevious sections. search, time current assignment changed recordedwhether clause unsatisfied. result averaging number times clauseunsatisfied runs gives unsatisfaction frequency (uf ) clause.Figure 13 shows plot two quantities clauses instance whose costmedian 5000 threshold instances. note figure clauses whosepresence contributes backbone often unsatisfied averageWSat search.260fiBackbone Fragility Local Search Cost Peak310unsatisfaction frequency210110010051015backbone contribution2025Figure 13: Scatter plot unsatisfaction frequency backbone contributionclauses cost median 5000 instances, m/n = 4.29, n = 100.Table 6 confirms pattern. row table gives data one instance.selected cost percentiles; individual instances varying degrees difficulty. examplerow labelled 30th corresponds instance whose cost 1500th rankeasiest difficult 5000 instances, 50th percentile instanceone used produce Figure 13. third fourth columns give meanstandard deviation unsatisfaction frequency clauses instancelast two columns give statistics sub-bag clausesbackbone-critical (their backbone contribution top 10%).Table 7 shows converse effect also present: clauses oftenunsatisfied (their unsatisfaction frequency top 10%) backbone-criticalaverage. Although effect quite clear means, sometimes particularlylarge standard deviations bc values frequently unsatisfied clauses.because, seen Figure 13, clauses often unsatisfied eventhough removing affect backbone size all.found experiments removal clauses along small randombags clauses average reduce backbone size considerably. large standarddeviations therefore arise true backbone contribution clausesapparent using simple measure.261fiSinger, Gent & SmaillCostPercentileBackbonesize10th20th30th40th50th60th70th80th90th161113364825637093clausesuf mean11.343013.107921.020722.982529.561536.294052.419892.2623108.3124uf std. dev.8.070410.959616.368021.311825.927535.632748.107887.7827127.1968backbonecritical clausesuf mean uf std. dev.20.87038.873030.181716.873041.466021.114256.684127.466072.070438.777996.166454.3081119.769166.8187167.3428149.8058306.7200198.6933Table 6: Unsatisfaction frequencies clauses different cost percentile instances.CostPercentile10th20th30th40th50th60th70th80th90thBackbonesize161113364825637093clausesbc mean0.59210.48480.39631.80891.06291.38003.39160.69463.0653bc std. dev.1.23581.03801.24054.24113.27813.49208.86303.457710.0376oftenunsatisfied clausesbc mean bc std. dev.2.09091.85291.77271.96321.84092.44908.31826.46206.31826.60437.75005.779414.909115.81262.59097.860216.931820.6436Table 7: Backbone contributions clauses different cost percentile instances.262fiBackbone Fragility Local Search Cost Peakinstances different costs satisfiability threshold, clauseslikely unsatisfied search higher backbone contribution average.Conversely, clauses largest backbone contribution likelyunsatisfied search. section therefore demonstrates well explainingdifferences cost instances, backbone fragility hypothesis also explaindifferences difficulty satisfying particular clauses search.8. Related WorkClark et al. (1996) showed number solutions correlated search costnumber local search algorithms random instances different constraint problems,including Random 3-SAT. pattern confirmed Hoos (1998) using improvedmethodology. Clark et al.s work first step towards understanding variancecost number constraints fixed. followed approachlooking number solutions using linear regression estimate strengthsrelationships factors.Schrag Crawford (1996) made early empirical study clauses (includingliterals) entailed Random 3-SAT instances. Parkes (1997), whose studyalso discussed Section 1, looked detail backbone size Random 3-SAT effectlocal search cost. also linked position cost peak satisfiabilitythreshold emergence large-backbone instances occurs point. Parkesalso identified fall WSat cost instances given backbone size.therefore basis study. Parkes conjectured presence failed clustermay cause high WSat cost large-backbone Random 3-SAT instances.According hypothesis, addition single clause could remove group solutionsHamming distant remaining solutions, reducing size backbonedramatically. clause would large backbone contribution. Thereforeexplanation general high cost threshold region certain features commonParkes conjecture. particular agree presence clauseslarge backbone contribution causes high cost. especially demonstratedresults Section 7.Frank et al. (1997) studied detail topology GSat search space induceddifferent classes random SAT instances. study discussed implications searchspace structure future algorithms, well effects structures algorithmsGSat. also noted local search algorithms WSat mayblind structures studied search different ways GSat.Yokoo (1997) also addressed question cost peak local searchm/n increased. approach analyse whole search space small satisfiable random instances. study, examined SAT, Yokoo alsoshowed results generalised colourability problem. Yokoo used deterministichill-climbing algorithm. studied number assignments solutionreachable (solution-reachable assignments) via algorithms deterministic moves,largely determines cost algorithm.followed Yokoo looking factor competing number solutions whoseeffect cost changes m/n increased. factor Yokoo proposed cause263fiSinger, Gent & Smailloverall fall cost decrease number local minima assignmentslocal move decreased number unsatisfied clauses. decreasenumber demonstrated m/n increased. decrease attributeddecreasing size basins (interconnected regions local minima numberunsatisfied clauses). Yokoo claimed (p. 363) that:adding constraints [...] makes [instance] easier decreasing numberlocal minima.However, think clear priori relationship numberlocal minima cost given instance Yokoo study sufficientlyconvince us explanation. contrast Yokoo, studied detailrelationship backbone fragility instances WSats cost instancesconfirmed testing predictions hypothesis. Also, studied instance properties related logical structure clauses rather search space topologyinduced think potential generalise across algorithmseven address complexity issues, explain towards end section.Hoos (1998) also analysed search spaces SAT instances relation local searchcost looking two new measures induced objective function defined,including one based local minima. Although via measures, Hoos ableaccount Random 3-SAT cost peak, found features correlatedcost SAT encodings problems also shown (Hoos, 1999b)measures help distinguish alternative encodings problem.pattern uncovered fit work makes instancesrequire high cost solve? Gent Walsh (1996) looked probability unsatisfiable SAT instance became satisfiable fixed number clauses removed random.unsatisfiable instances highest computational cost complete procedure found unsatisfiability-fragile unsatisfiabilitysensitive random removal clauses. may therefore fragility instances unsatisfiability backbone size cause high computational costcontext complete procedures incomplete local search, would interestinglink two algorithm classes. link may form basis possible explanation reasons threshold Random 3-SAT instances may universally hardaverage case, opposed merely costly class algorithms. Recent workMonasson et al. (1999a, 1999b) suggested parameterised distributions instanceshard average case, e.g. Random 3-SAT, exhibit discontinuitybackbone size3 control parameter varied, whereas polynomial time average-casedistributions, Random 2-SAT, backbone size changes smoothly. proposecomplexity distribution linked presence discontinuity.conjecture may asymptotic limit, instances backboneor unsatisfiability-fragile persist n increased discontinuity.line research may therefore establish testable causal mechanism pattern,showing properties instance distributions affect algorithm performance.would interesting compare backbone fragility different random distributions3-SAT instances, introduced Bayardo Schrag (1996) Iwama,3. Monasson et al.s definition backbone also extends unsatisfiable instances.264fiBackbone Fragility Local Search Cost PeakMiyano Asahiro (1996) see whether differences local search cost could explained.method generates satisfiable instances quickly solved local searchanalysed Koutsoupias Papadimitriou (1992) Gent (1998). Random clausesadded formula Random 3-SAT conflict certainsolution set advance. conjecture overconstrained examplesquickly solved local search backbone-robust.interesting possibility mentioned Hoos Stutzle (1998) suggested exponential run length distribution, local search equivalent random generate-andtest drastically reduced search space. conjecture reduced search spacecorresponds quasi-solution area. Measurements hdns(TB , C) quasi-solutionsTB may therefore indicative extensiveness reduced search space, especiallysince metric linearly correlated log cost. experimentation veinmay therefore reveal topology reduced search space couldturn lead better local search algorithms designed exploit knowledge.Finally, emphasise notions backbone backbone-fragilityequally applicable non-random SAT instances. future may able confirmresults shown random SAT instances apply equally benchmark realworld SAT instances. However, one caveat entailed literals may uncommoninstances may need study fragility sets entailed formulas.9. Conclusionreconsidered question cost local search peaks near Random3-SAT satisfiability threshold. overall pattern one two competing factors.cause onset high cost control parameter increased previouslyestablished decreasing number solutions. proposed causesubsequent fall cost falling backbone fragility.found striking pattern search behaviour local search algorithm WSat.instances given backbone size, underconstrained region control parameter, WSat attracted early quasi-solutions Hamming-distantnearest solution. distance also strongly related search cost. controlparameter increased, distance decreases. suggested backbone fragilitycause pattern.defined measure backbone robustness. Backbone-fragile instances lowrobustness. able test predictions hypothesis fall backbonefragility cause overall decay cost control parameter increased.found hypothesis made three correct predictions. Firstly degreeinstance backbone-fragile correlated cost effects factorscontrolled for. Secondly, Random 3-SAT instances alteredbackbone-fragile (by removing clauses without disrupting backbone) costincreases. Thirdly, clauses often unsatisfied search whosedeletion effect backbone.summarise interpretation evidence. underconstrained region,instances small backbones predominant. region, rapid hill-climbingphase typically results assignment close nearest solution (and probably265fiSinger, Gent & Smaillsatisfies backbone). Since finding small backbone largely accomplished hillclimbing, typical cost WSat low region variance cost due variancedensity solutions region search space backbone satisfied.threshold region, large-backbone instances quickly appear large quantities.large-backbone instances, main difficulty local search identify backbonerather find solution backbone identified. identificationlarge backbone may accomplished rapid hill-climbing phase greater lesserextent. think extent determined backbone fragility instance.large-backbone instance backbone-fragile hill-climbing phase ineffectiveresults assignment Hamming-distant nearest solution (probablyimplying much backbone identified). costly plateau searchrequired find solution. Hence rare large-backbone instances occurunderconstrained region, extremely costly solve high backbonefragility.large-backbone instance backbone robust, rapid hill-climbing phaseeffective determining backbone plateau phase shorter. overallinstance less costly WSat solve. Hence large-backbone instances, sincebackbone fragility increases add clauses, cost decreases. overconstrained region,large backbone instances dominant backbone fragility becomes main factordetermining cost. Hence cost decreases region. hypothesis proposes followingexplanation cost peak: Typical cost peaks threshold regionappearance many large-backbone instances still moderately backbone-fragile,followed increasing backbone robustness instances.Acknowledgmentsresearch supported UK Engineering Physical Sciences Research Councilstudentship 97305799 first author. first two authors members crossuniversity Apes research group (http://www.cs.strath.ac.uk/~apes/). would likethank members Apes group, anonymous reviewersearlier paper Andrew Tuson invaluable comments discussions.266fiBackbone Fragility Local Search Cost PeakAppendix A: Randomisation Bootstrap Testssummarise methods used context. explanation methodsgiven Cohen (1995).A.1 Randomisation Estimating Correlation Coefficient dueDistributions VariablesRandomisation used estimate correlation coefficient two variablesresults simply distributions rather relationship. starttwo vectors data x = hx1 , x2 , . . . , xN = hy1 , y2 , . . . , yN i. correlationcoefficient merely due distributions x y, dependentparticular xi paired yi . Therefore calculate correlation coefficient resultingmerely distributions pair x data randomly.construct K randomisations. randomisation consists vector 0 ,simply random permutation y. randomisation, calculate correlationcoefficient x 0 note value xi paired random valuey. randomised correlation coefficients give us estimate correlationcoefficients resulting distributions variables. K large enough,accurate estimate compared correlation coefficientobserved data.A.2 Bootstrap Estimation Confidence Intervals Correlation Coefficientsoriginal sample h(x1 , y1 ), (x2 , y2 ), . . . (xN , yN )i N pairs. pseudo-sampleoriginal also consists N pairs. j th pair pseudo-sample (xbj , yjb ) = (xq , yq )q random number 1 N . pair pseudo-sample chosenindependently i.e. pairs sampled original replacement. assumeoriginal sample pairs data representative whole population pairs.Given this, composing pseudo-samples like sampling whole population.Therefore measuring correlation coefficient many pseudo-samples, studycorrelation coefficient would looked like taken many sets sampleswhole population. distribution correlation coefficient among manypseudo-samples (the bootstrap sampling distribution) infer bounds confidenceinterval observed correlation coefficients.Many pseudo-samples taken, correlation coefficient calculatedpseudo-samples. gives bootstrap sampling distribution correlation coefficient. 97.5th percentile distribution upper bound 95% confidenceinterval correlation coefficient, 2.5th percentile lower bound.Appendix B: Relationship BMSs MUSsLet C satisfiable SAT instance {l1 , l2 , . . . , lk } set literals entailedC. Let clause l1 l2 . . . lk .Theorem C 0 BMS C iff C 0 MUS C 2267fiSinger, Gent & SmaillProof Suppose C 0 BMS C. C 0 d, sub-instance C d, mustunsatisfiable, violates every literal backbone C 0 . removed C 0 d,result C 0 satisfiable. clause c removed C 0 d, mustliteral backbone C 0 , li say, (C 0 {c}) li satisfiable. Therefore,since li also literal d, (C 0 {c}) satisfiable. Therefore C 0 MUSC d.Conversely, suppose C 0 MUS C d. Since C 0 minimally unsatisfiable,0C satisfiable. Since C 0 sub-instance C, backbone C 0 must subsetbackbone C. Suppose literal lj backbone Cbackbone C 0 . would solution C 0 lj . would alsosolution C 0 d, since lj one literal d. contradicts C 0 unsatisfiablelj i.e. C 0 C must backbone.C 0 minimally unsatisfiable. Therefore clause c C 0 , (C 0 {c})satisfiable. solution (C 0 {c}) must make literal lk true, musttherefore also solution (C 0 {c}) lk . Therefore lk , backboneC 0 , backbone (C 0 {c}). Hence C 0 BMS C 2ReferencesBayardo, R. J., & Schrag, R. (1996). Using CSP Look-Back Techniques Solve Exceptionally Hard SAT Instances. Proceedings Second International ConferencePrinciples Practice Constraint Programming, pp. 4660. Springer.Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). Really Hard Problems Are.Proceedings IJCAI-91, pp. 331340. Morgan Kaufmann.Clark, D., Frank, J., Gent, I. P., MacIntyre, E., Tomov, N., & Walsh, T. (1996). Local SearchNumber Solutions. Proceedings Second International ConferencePrinciples Practice Constraint Programming, pp. 119133. Springer.Cohen, P. (1995). Empirical Methods Artificial Intelligence. MIT Press.Cook, S. (1971). Complexity Theorem-Proving Procedures. Proc. 3rd Ann. ACMSymp. Theory Computing, pp. 151158.Cook, S., & Mitchell, D. (1997). Finding Hard Instances Satisfiability Problem:Survey. Satisfiability Problem: Theory Applications, Vol. 35 DIMACS SeriesDiscrete Mathematics Theoretical Computer Science, pp. 1 18. AmericanMathematical Society.Crawford, J. M., & Auton, L. D. (1996). Experimental Results Crossover PointRandom 3SAT. Artificial Intelligence, 81, 3157.Culberson, J., & Gent, I. P. (1999a). Completeness WalkSAT 2-SAT. Tech.rep. APES-15-1999, APES Research Group.Available http://apes.cs.strath.ac.uk/apesreports.html.268fiBackbone Fragility Local Search Cost PeakCulberson, J., & Gent, I. P. (1999b). Well reach: hard problems hard. Tech.rep. APES-13-1999, APES Research Group.Available http://apes.cs.strath.ac.uk/apesreports.html.Franco, J., & Paull, M. (1983). Probabilistic analysis Davis Putnam proceduresolving satisfiability problem. Discrete Applied Math., 5, 7787.Frank, J., Cheeseman, P., & Stutz, J. (1997). Gravity Fails: Local Search Topology.J. Artificial Intelligence Research, 7, 249281.Gent, I. P. (1998). Stupid Algorithm Satisfiability. Tech. rep. APES-02-1998,APES Research Group.Available http://apes.cs.strath.ac.uk/apesreports.html.Gent, I. P., MacIntyre, E., Prosser, P., & Walsh, T. (1996). Constrainedness Search.Proceedings AAAI-96, pp. 246252. AAAI Press / MIT Press.Gent, I. P., & Walsh, T. (1993). Empirical Analysis Search GSAT. J. ArtificialIntelligence Research, 1, 4759.Gent, I. P., & Walsh, T. (1996). satisfiability constraint gap. Artificial Intelligence,81, 5980.Hogg, T., & Williams, C. P. (1994). hardest constraint problems: double phasetransition. Artificial Intelligence, 69, 359377.Hoos, H. (1998). Stochastic Local Search - Methods, Models, Applications. Ph.D. thesis,Darmstadt University Technology.Hoos, H. (1999a). Run-time Behaviour Stochastic Local Search AlgorithmsSAT. Proceedings AAAI-99, pp. 661666. AAAI Press / MIT Press.Hoos, H. (1999b). SAT-Encodings, Search Space Structure, Local Search Performance.Proceedings IJCAI-99, pp. 296302. Morgan Kaufmann.Hoos, H., & Stutzle, T. (1998). Characterising Run-time Behaviour Stochastic LocalSearch. Tech. rep. AIDA-98-01, Darmstadt University Technology.Iwama, K., Miyano, E., & Asahiro, Y. (1996). Random generation test instances controlled attributes. Cliques, Coloring, Satisfiability, Vol. 26 DIMACS SeriesDiscrete Mathematics Theoretical Computer Science, pp. 377394. AmericanMathematical Society.Koutsoupias, E., & Papadimitriou, C. H. (1992). greedy algorithm satisfiability.Information Processing Letters, 43 (1), 53 55.Larrabee, T., & Tsuji, Y. (1992). Evidence Satisfiability Threshold Random 3CNFFormulas. Tech. rep. UCSC-CRL-92-42, Jack Baskin School Engineering, UniversityCalifornia, Santa Cruz.269fiSinger, Gent & SmaillMcAllester, D., Selman, B., & Kautz, H. (1997). Evidence Invariants Local Search.Proceedings AAAI-97, pp. 321326. AAAI Press / MIT Press.Mitchell, D., Selman, B., & Levesque, H. (1992). Hard Easy Distributions SATProblems. Proceedings AAAI-92, pp. 459465. AAAI Press / MIT Press.Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999a). 2+PSAT: Relation Typical-Case Complexity Nature Phase Transition.Random Structures Algorithms, 15, 414 440.Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999b). Determining computational complexity characteristic phase transitions. Nature, 400,133137.Papadimitriou, C. H. (1991). selecting satisfying truth assignment. Proc. 32ndIEEE Symp. Foundations Comp. Sci., pp. 163169.Parkes, A. (1997). Clustering Phase Transition. Proceedings AAAI-97, pp.340345. AAAI Press / MIT Press.Parkes, A., & Walser, J. (1996). Tuning Local Search Satisfiability Testing. Proceedings AAAI-96, pp. 356362. AAAI Press / MIT Press.Schrag, R., & Crawford, J. (1996). Implicates prime implicates Random 3-SAT.Artificial Intelligence, 81, 199222.Selman, B., Kautz, H., & Cohen, B. (1994). Noise Strategies Improving Local Search.Proceedings AAAI-94, pp. 337343. AAAI Press / MIT Press.Walsh, T. (1998). Constrainedness Knife-Edge. Proceedings AAAI-98, pp. 406411. AAAI Press / MIT Press.Yokoo, M. (1997). Adding Constraints Makes Problem Easier Hill-ClimbingAlgorithms: Analysing Landscapes CSPs. Proceedings Third InternationalConference Principles Practice Constraint Programming, pp. 356370.Springer.270fifffi! #"$ %'&)( *,+-*...0/21035476!(89:;< =)( .0>66!?A@: %&=CB0>..DFE<GHEJILKJM<NPORQLSTEUWVYX[Z]\_^a`]b9VacedgfhVa\_bijb0fkValnmpo$Va\qZHrsZHVa\qZ,Va`<ZutAvjfkwxzy${|ZHral}vjl~ijb0fkVbVdgln^alnmvjlnrqaWbiZHZ]\_al}`s$#52F<2222Tjh$T2T$#h'LC'A-nkhA0z LjA]'C| 5A~5AH]ff5ffAffff'AffJA,ffffffff~j!A$Aq,9k 9zL9H5) 9Aa55957~05$0H|ff-<A0zffj5)!j5)0zjA$-555 5-ff9j5A#fi799ff!5ff-9|5A]55-ff!A5n 59a0L!|50!<AL}5A]5HHA]n 9!#"ffhn5$#5%99zz99!&'#, 590~Ak9ffA ffAL-AA599!|5h5n5()* +-,.* /10&23465 7.89,;:<* /=,>5?890&54@0A* ,CB*D 2ff,.E=* 0F0&23'4G5 7.89,C5 7@HI2ff/JEK2LM0&23465 78N,POQ*7.2%*RL!5 7SR* /=E=,.SLT5 7U7.2V-7.2ff,>2ff0W3.E=0&XA3.Y(2[Z>5$E=0\3](E=,>3>7PEKH-+&3.EK5$0^5 LQ*F_fi5$/=/K2ff_fi3.E=5$0`5 LQ7* 0-]&5$Sba*7E=*H-/=2ff,%E=0c3>27S?,U5 LQ3.Y&2_fi5$0(](E=3.EK5$0(* /[](E=,.3>7EKH-+&3.E=5$0(,?LT5 7d2ff* _Yea*7E=*H-/K2^X$EKa 2ff0fa* /=+(2ff,gL!5 7hEK3.,jiV-*7.2ff0W3.kla*7E=*H-/=2ff,monQY&2,>3>7+-_fi3.+&7.2 5 L93.Y&2p]-E=,>3>7EKHq+&3.EK5$0rE=,s7.2V-7.2ff,>2ff0W3>2ff]tX 7*V-Y-E=_* /=/KD@HWD[*C0(23'4G5 7.8tE=0[4CY-E=_Yt0(5N]&2ff,17.2V(7.2ff,>2ff0W3a*7E=*H-/K2ff,r* 0(]^*7.754C,[*7.2]&7P*ff4C0^LT7.5$SuV-*7.2ff0W3[0&5N](2ff,@3>5d_PY(E=/=]^0&59]&2ff,mtnCY&2ff,>2*7.7.5v4C,r3'DNV-E=_* /=/=D_fi5 7.7.2ff,.VI5$0-]R3>5;_* +(,.* /I7.2ff/=*3.E=5$0(,.Y(EKVq,mxw0R3.Y(2y,>3.* 0-](*7]zL!5 7S+(/=*3.E=5$0s{\3.Y&2r0&23465 7.8|E=,G0&5 3G* /J/K54G2ff]3>5RY(*a 2U](E=7.2ff_fi3>2ff]g_fiDN_/=2ff,mY&2ff0R*%](E=,>3>7E=H-+&3.EK5$0E=,#,>V~2ff_E-2ff];HND}3>5g]&23>27S?EJ0&2%3.Y(*3r5$0&2,.+(_PY?*t0&23465 7.8I{N3.Y&2C ',.2V-*7*3.EK5$0z_fi7EK3>27EK5$0R* /=/K54@,#5$0&2,>23r5 L 7P* 0(]&5$Sua*7PE=*H-/K2ff,{~U{~EJ,@_fi5$0(]-EK3.EK5$0(* /=/=DEJ0(]&2V~2ff0(]&2ff0W3Q5 L6* 0(5 3.Y&27,>23#5 Lq7* 0(]&5$Sa*7E=*H-/K2ff,ff{|{ X$EKa 2ff0;a* /=+&2ff,LT5 7 *y3.Y(EK7],>23#5 L-7P* 0(]&5$Sa*7PE=*H-/K2ff, ;mnQY(EJ,M_fi7E=3>27EK5$0E=0Wa 5$/Ka 2ff,@5$0(/KD|3.Y&2%V(72ff,>2ff0(_fi2t5 7r*H-,>2ff0(_fi2U5 L#*7.7.54@,@E=0d3.Y&2U0&23465 78q{I0&5 3C3.Y&2]&23.* E=/K2ff]A0W+(S?27E=_* /,>V~2ff_Eq_*3.E=5$05 L3.Y(2t_fi5$0(](EK3.EK5$0-* /s](E=,>3>7E=H-+&3.EK5$0(,ffmN22%2ff*7/6:ff $OLT5 7C*?]&23.* EJ/K2ff]g](E=,._+-,.,.EK5$0sm2ff*7/* 0(]@2ff_PY\3>27F:ff $OY(*ffa 2g*3>3>2ffSV(3>2ff]3>5F2fiN3>2ff0-]3.Y-E=,tLT7*24G5 7.8`3>5^0&23'4G5 7.89,3.Y(*3S?*DF_fi5$0W3.* E=0c]-EK7.2ff_fi3>2ff]_fiD9_/K2ff,{14CY-E=_Y_fi5 7.72ff,>V~5$0(]F3>5dL!22ff](H-* _.8^7.2ff/=*3.E=5$0(,.Y(EKVq,@*5$0(Xga*7EJ*H-/K2ff,mY&2ff0`_fiD9_/K2ff,t2fi&E=,>3{M3.Y&2rZ>5$E=0\3t]-E=,>3>7EKHq+&3.EK5$0FE=,[0&5A/K5$0(X 27%,>V~2ff_E-2ff]^E=03>27PS?,[5 L3.Y&2}V(7.59](+(_fi3[5 L_fi5$0(](E=3.EK5$0(* /~](E=,>3>7PEKH-+&3.EK5$0-, LT5 7C_Y(EJ/=]&7.2ff0|X$EKa 2ff0gVq*7.2ff0\3.,ff{&H-+&37*3.Y&27CHWD|,.*ffD9E=0&X?Y&5v4f3.Y&2[a* /=+&2ff,5 L3.Y&2U5 H-,>27.a*H-/K2ta*7E=*Hq/K2ff,{((ffff I{q*72U]&23>27SRE=0&2ff]dHND3.Y&2%a* /=+&2ff,CL!5 7r*R,.23C5 L +(0&5 Hq,>27.a 2ff]7* 0(](5$S](EJ,>3.+&7.H-* 0-_fi2ff,{s (ff MI{q4CY-E=_YF*7.2;* ,.,.+(S 2ff]A3>5HI2E=0(]&2V~2ff0(](2ff0\35 L 2ff* _PY^5 3.Y&27[* 0(]3>5UY-*ffa 2@,.VI2ff_EK-2ff]?](E=,.3>7EKH-+&3.E=5$0(,mx&5 7G2ff* _Yza*7E=*H-/K2 {\E=,2ffN+(* /s3>5|,>5$S{N* 0?2ffN+(*3.EK5$0zE=,#X$EKa 2ff0z,>V~2ff_EKL!D9E=0&X[3.Y-*3pEK32tL<+(0(_fi3.E=5$0g5 L3.Y&2%_fi5 7.7.2ff,.VI5$0-](E=0&X| * 0(]d5 L,.5$S2%,>23@5 LVq*7.2ff0\3Ca*7EJ*H-/K2ff,LT7.5$S* 5$0&XR3.Y&2[%[4CE=3.Y?dmp@,QHI2LT5 7.2 {&V-*72ff0\3'_PY(E=/=]g72ff/=*3.EK5$0(,.Y-EKV-,G*7.2t7.2V(72ff,>2ff0\3>2ff]dX 7*V-Y(E=_* /J/KDHND|]&7P*ff4CE=0(X2ff](X 2ff,Q4CEK3.Yg*77.54C,CL!75$SoV-*7.2ff0W3Q0&59]&2ff,3>5R_PY(E=/=]g0&59]&2ff,mw0?5 7](27 3>5US?*8 2y3.Y(E=,p,._Y&2ffS?2C4G2ff/=/']&2fiq0(2ff]s{$2ff*7/-* 0(]zy2ff_YW3>27G72ffW+(E=7.23.Y(*3pLT5 76* 0WDa* /=+&2ff,5 LM ff* /=/93.Y&22ffN+(*3.EK5$0-, *7.2( ff U3.Y&272QE=,2fi9* _fi3./=D5$0(2Q,>23 5 LIa* /=+(2ff,L!5 7 (ff ULT5 7 4CY(EJ_Y,.*3.E=,>-2ff]smpwL3.Y(E=,Q+-0(E=N+&2ff0&2ff,.,_fi5$0(]-EK3.EK5$0gE=,@,.*3.E=,-2ff]1{(*R](E=,.3>7EKH-+&3.E=5$0R5va 27U( M|4CE=/J/s]&2fiq0&2*...z %%q!=n=C0Gj!fi;]p:! %&%' &0%2%J0 =vfiy -*t](EJ,>3>7EKH-+(3.EK5$0t5va 276 ( Impr0&2_* 0?3.Y(2ff0?* ,>84CY(*3 _fi5$0(](E=3.EK5$0(* /&E=0(](2VI2ff0-]&2ff0(_fi2GV(7.5 V~27.3.EK2ff,3.Y(E=,]-E=,>3>7EKHq+&3.EK5$0|S?EKX$YW3VI5$,,>2ff,.,my__fi5 7](E=0(X3>5cnQY&25 7.2ffS^5 LrM2ff*7P/C* 0(]ly2ff_YW3>27`:ff $OP{CEKL@3.Y&2d[*7.2d* /=/Q]-E=,._fi7.23>2 {G3.Y&2a*7E=*H-/K2ff,Uo*7.2z_fi5$0-](EK3.EK5$0(* /J/KDhE=0-]&2V~2ff0(]&2ff0W3[5 L3.Y&2Ra*7E=*H-/=2ff,%X$EKa 2ff03.Y&2Ra*7E=*Hq/K2ff,EKL3.Y&2a*7E=*H-/K2ff,C ',>2Vq*7*3>2%3.Y&2[a*7E=*Hq/K2ff,* 0(]g|m nQY&2t ',.2V-*7*3.EK5$0h_fi7EK3>27EK5$0g_* 0AH~2y2fi9V(7.2ff,,>2ff]E=0^3>27SR,[5 L3.Y&2L!5$/J/K54CEJ0&XgS?* 0(EKV-+-/=*3.EK5$0(,@5 L3.Y&2X 7*V-Y4@EK3.Y0&59]&2ff,t_fi5 7.7.2ff,.VI5$0-](E=0&X|3>5d3.Y&2?* 0(]g4@EK3.Yg*7.7.5v4C,LT7.5$SoV-*7.2ff0W3.,Q3>5z_Y(E=/J]&7.2ff0sOy2ff/K23>2* /=/s0&59]&2ff,LT7.5$So3.Y&2[X 7P*V-Yg2fi&_fi2V(3Q3.Y&5$,>2UE=0g%{qz{&5 7r* 0(]g3.Y(2ffEK7Q* 0(_fi2ff,>3>5 7,ffm$Od)5$0(0&2ff_fi3QHND|* 0d2ff]&X 2t2a 27.DgV-* E=75 L0&59]&2ff,3.Y-*3C,.Y(*7.2t*R_fi5$S?S5$0d_PY(E=/=]smOC2ffS 5a 2t*77.54C,GLT7.5$S* /=/-3.Y(2@2ff]&X 2ff,pEm2 mK{97.2V-/J* _fi2C2ff* _Yg]-EK7.2ff_fi3>2ff]|2ff]&X 2yHWDR* 0+(0-](EK7.2ff_fi3>2ff]2ff](X 2wL{~E=0h3.Y&2U72ff,.+(/K3.E=0(X?X 7*V-Ys{~* /=/MV-*3.Y(,@L!75$Su*|0&59]&2UEJ0h3>5*0&59]&2UE=0hV-* ,,C3.Y&75$+&X$Yh*0&59]&2E=0A{&3.Y&2ff0h$',>2V-*7*3>2ff,yeL!7.5$SzmEKX$+&72^,.Y&5v4C,* 02fi9*_fi5$+(0W3>27.2fi&*Vq/K2h5 L*]-E=,>3>7EKHq+&3.EK5$0j]&2fiI0&2ff]EJ03.Y(EJ,z4*ffD {y4CY(EJ_Y,>27.a 2ff,g* ,g*V-/K2d3>5c3.Y&2A_/=* E=S3.Y-*3z ',.2V-*7*3.EK5$0jEJSV-/=EK2ff,;_fi5$0(]-EK3.EK5$0(* /E=0-]&2V~2ff0(]&2ff0(_fi2|L!5 7|* 0\D0&23465 7.8F,.*3.E=,>LTD9E=0&X?3.Y&2;+(0(EJW+&2ff0(2ff,.,Q_fi5$0(](E=3.EK5$0sm@nCY&2%a*7E=*H-/=2ff,CE=0h3.Y(E=,C2fi9*5 L6|5 7m%nQY(2zp*7.2Vq/K2* /J/13.*8 2E=0(](2VI2ff0-]&2ff0\3y* 0(]^*7.2;2ffW+-* /=/KDd/=EK8 2ff/=Dg3>5gHI2;|5 7?mtnQY&2;a* /=+&2ff,6,*3.E=,>LTDd3.Y&22ffN+(*3.EK5$0(, ,Y&54C01{\E=04CY(E=_PY* ](](EK3.E=5$0* 0-]?SU+-/K3.EKV-/=EJ_*3.EK5$0U*72Q]&5$0&2QS 59](+(/K5t :<Em2 mK{NE=0R * OPm @5 3>23.Y(*3r * {~ B {~{(* 0(] 3 ]&5?0&5 3Q*V(V~2ff*7QE=0|3.Y&2%2ffW+(*3.E=5$0(,{&* 0(]dY&2ff0-_fi2[V-/=*ffD|0&5 7.5$/K2[E=0g](2fiq0(E=0&X(3.Y&2%]-E=,>3>7EKHq+&3.EK5$0?LT5 7Q (v 3nQY(2%0&23'4G5 7.8d* 0(]d3.Y&2%2ffN+(*3.EK5$0(,@_/K2ff*7/KDY(*a 2%3.Y&2U7.2ffW+-EK7.2ff]g,>D90\3.* _fi3.EJ_tLT5 7Sgmnx5R,Y&54e3.Y(*33.Y(E=,yE=,y*Ra* /=E=]h2fi&*LT5 7G3.Y(2@V-/K2 {~EK3rE=,@* /J,>5|0(2ff_fi2ff,.,.*7.Dd3>5g,.Y(543.Y(*3r3.Y&2z +(0-E=W+(2ff/KD|](23>27S?E=0&2a* /=+&2ff,m6r0&2y_* 02ff* ,.E=/KD?_fi5$0&-7S3.Y(*3GL!5 7* 0WDRa* /=+&2ff,65 Lx3.Y&2UM13.Y(2yLT5$/=/K5v4CE=0&X%a* /=+(2ff,6LT5 73.Y&24CE=/=/~,.*3.EJ,>L!D|* /=/s3.Y&2[2ffN+(*3.EK5$0-,(6(*B??6p (3nM5R,>22[3.Y(*3Q3.Y-E=,E=,3.Y&2[5$0-/KDz,>23C5 LMa* /=+&2ff,LT5 7Q3.Y&2[3.Y(*3@,.*3.E=,>LTDz* /=/~3.Y(2t2ffW+-*3.EK5$0(,{(0&5 3>2t-7,>3* SU+(,.3tH~2R9{#,.E=0(_fi2RE=LGEK3UE=,%E=0-,>3>2ff* ]{3.Y&2ff04CY(EJ_Y^E=,rE=S V~5$,.,.E=H-/K2 m[C2ff0-_fi2 ? 3 9m;&E=0(_fi2; * 0(]F?3.Y(*3UR* 0(]c 3 {3{s462 * /=,.5d,>22;3.Y(*3q~pvPvp[>fi %v>[CvrPv@~P[C#Px~K\1>fi fivxP'1>A1vp#MvPv @P[#qvrvP'. !~P s#>>yQPfix#vPv'.p<NQGv#<'#PPvv<'.pGffG<'y&Py<' <'.#pPtJI~v vCff[&->yr #P>~>rv'# T~rr9@ T~ %!1y1v#.ff6$xC#Pv1fffi F 22N-`~$--&7T-;$ $\ $ 2U1B(*(?:T*:T*p?O?X3X4BU5X2*X1U4((3O\:T3X5X6X7EKX$+&7.2?ry7*V-Y(EJ_* /C,>3>7+-_fi3.+&7.2A* 0(]2ffN+(*3.EK5$0(,RL!5 7|3.Y&2h_fi5$+-0\3>27.2fi&*V-/=2 m@/J/Qa*7E=*H-/K2ff,?3.*8 2a* /=+(2ff,tE=0 $m|nQY(2g *7.2|E=0(]&2V~2ff0(]&2ff0W3{s4CEK3.Y`2ffW+(* /pV(7.5 H-*HqE=/=EK3.EK2ff,@LT5 7UA* 0-]jm@](]-EK3.EK5$0R* 0-]zS+(/K3.EKVq/=E=_*3.EK5$0?EJ,6]&5$0(2@S 59](+(/K5;Nm nQY&2%Mx* 0(]z3.Y&2r]&5 3>3>2ff]g*7.7.5v4C,*7.20&5 3QLT5 7S?* /=/KDRV-*7.3Q5 L3.Y&2tX 7*VqYs{&H-+&3Q*7.2t,Y&54C0dL!5 7C_/J*7EK3'D* %E=S Vq/=EK2ff,3.Y(*3p * {\LT7.5$S4@Y(E=_YREK3 LT5$/=/K54@,#3.Y(*3 B ( {,.E=0-_fi2[ ( (23+(,U0&54 _fi5$0(,.EJ]&27%4CY(23.Y&27%5 7;0&5 3 z=E ,U_fi5$0(](EK3.EK5$0-* /=/KDhEJ0(]&2V~2ff0(]&2ff0W3[5 L??X$EKa 2ff0 *} 2 _* 0^,>22U3.Y-*3@ ;* 0(]h?%*7.2; ',>2Vq*7*3>2ff]^HWDg * {~,.E=0-_fi2UE=0F,>3>2Vl:OC*H~5va 2 {s4624CE=/=/x]&2ff/K23>2?C* 0-]?3 * 0-]z* /=/-3.Y&2C2ff](X 2ff,G_fi5$0-0&2ff_fi3.E=0&XU3>5U3.Y&2ffSd{N/K2ff*aNEJ0&XU0&5UV-*3.YzL!7.5$S Q3>5?m C5v462a 27{X$EKa 2ff0l* 0WDca* /=+&2|L!5 7; * { 3.Y(2za*7E=*H-/=2ff,U * 0(] *7.2gE=0cL<* _fi3?]&2V~2ff0(]&2ff0W3m@, ,.22ff0*H~5va 2 {* 0WD,.23Q5 La* /=+(2ff,L!5 7C3.Y&2t#,.*3.E=,>LTD9E=0&X;3.Y&2t2ffN+(*3.EK5$0(,CSU+(,.3QH~2t,.+(_Yd3.Y(*3Q@2ff0(_fi2 {xEKL6G? 9m*{s3.Y&2ff0cEK3yS+(,>3[H~23.Y(*3t?E=,[]&23>27S?EJ0&2ff]^HWD* 9{x3.Y&2ff0EK3[SU+-,>3rHI2;3.Y(*3r {x* 0(]E=LpE=0(,>3>2ff* ]F *?emz)G5$0-](EK3.EK5$0(* /5$0`*a* /=+&2 LT5 7[ * {x4G2 3.YN+(,t,>22?3.Y-*3[?{(,.Y&5v4CE=0&X 3.Y-*3Q3.Y&2D*7.2%0&5 3C_fi5$0-](EK3.EK5$0(* /J/KDRE=0(](2VI2ff0-]&2ff0\3mnQY(2V(7.5 H-/K2ffS*V(V~2ff*7,g3>5*7EJ,>2`H~2ff_* +(,>2`2a 2ff03.Y&5$+&X$Ye,>V~2ff_EKL!D9E=0&Xa* /=+&2ff,gLT5 7h* /=/t3.Y&2lM{,.VI2ff_E=L!D9E=0&Xg*ha* /=+(2?L!5 7R ( * /=5$0&2z/K2ff*a 2ff,^,>23.,5 La* /J+&2ff,[LT5 7z:T ( * B O@3.Y(*3,.*3.E=,>LTDh3.Y(2?2ffW+-*3.EK5$0(,t* ,.,.5N_E=*3>2ff]c4CEK3.Y^3.Y&2ff,.2?a*7PE=*H-/K2ff,{x2a 2ff03.Y&5$+&X$Y5$0(/KD5$0&2A,.23R5 L[a* /=+&2ff, 4CEJ/=/,.*3.E=,>LTD3.Y&2d2ff0W3.EK7.2F,>23R5 Lr2ffN+(*3.EK5$0(,m } Y(E=_PYla* /J+&2ff,?LT5 7+(0(EJW+&2ff/=Dh]&23>27SRE=0&2ff,ta* /=+&2ff,%L!5 7* /=/ 3.Y(2R:T( * B *72dV-*7.3?5 Lr3.Y&2A5a 27* /=/@,>5$/=+&3.E=5$0l]&2V~2ff0(](, 5$0l3.Y(2da* /J+&2d5 LM[v{* 0(]j3.Y(E=,ffE=0(]-+(_fi2ff, *`]&2V~2ff0(]&2ff0-_fi2|H~234622ff0Md* 0(]?4CY&2ff03.Y&2da* /=+&2g5 Lr * E=,;890&5v4C0smnQY&2[72ffS5a* /s5 LV-*7.35 L3.Y&2[X 7*V-YAE=0,>3>2V:OG5 L3.Y&2tV-7.5N_fi2ff]-+&7.2rL!5 7Q](23>27S?E=0(EJ0&X',>2V-*7P*3.EK5$02ff/=E=SRE=0(*3>2ff,* 0WD|V~5$,.,EKH-E=/=E=3'D;5 L#* __fi5$+(0\3.EJ0&X?LT5 7Q3.Y(E=,]&2V~2ff0(](2ff0(_fi2nQY(2[V(7.5 H-/K2ffS4@EK3.Yg2ff*7/1* 0(]dy2ff_PY\3>27,@V(7.5N5 L*V(V~2ff*7,Q_fi5$0(0&2ff_fi3>2ff]A4CEK3.Y3.Y(EJ,m nQY&2D,.*ffD {3C3.Y(E=,V~5$E=0W3G4G2%E=0Wa 5 8 2t3.Y&2[L<* _fi3Q3.Y(*3C3.Y&2t_fi5$0(,>3>7* EJ0\3.,C5 L#)*72t0&5 3Q*7.H-EK3>7P*7.DRH-+(3*72LT+(0-_fi3.EK5$0(* /{10(* S?2ff/KD {sL!5 7%2a 27.Dha* /=+&2ff,r5 LfiffI3.Y&2V-*72ff0\3.,[5 L -p* 0(]-G3.Y&27.2?EJ,fiy -*A,>5$/=+&3.EK5$0^LT5 7 qmRnCY(E=,rE=SV-/JEK2ff,y3.Y(*3{xLT5 7%* 0WD^,.23* ,,>5N_EJ*3>2ff]l4CEK3.Y0&5$09'* 0-_fi2ff,>3>5 7,;5 L5 La*7E=*H-/=2ff,{s3.Y&2?2ffN+(*3.EK5$0(,]&50&5 3?_fi5$0(,.3>7* E=03.Y&2V~27S?EK3>3>2ff]a* /=+&2ff,5 LmmffmnQY&2U2fi9* S?V-/K2UY(27.2U,.Y(54C,@3.Y(*3C3.Y(2U2ffN+(*3.EK5$0(,yE=0Wa 5$/Ka9E=0&X?0&5$09'* 0-_fi2ff,>3>5 7,y5 L,>3>7* EJ03.Y&2tV~27S?EK3>3>2ff]a* /=+&2ff,QL!5 7_* 0^E=0(]&22ff]A_fi5$09+(]&2ff*M2ff*7P/;:TV~27,.5$0(* /@_fi5$SRSU+(0-E=_*3.EK5$0qORY(* ,,.+&X X 2ff,>3>2ff]3.Y-*3z3.Y&2$',>2V-*7*3.EK5$0_fi7E=3>27EK5$0_* 0dH~2%,* /Ka*X 2ff]AHWD7.2ffN+(EK7PE=0&X0&5 3C5$0-/KDz3.Y-*3t( v M+(0(E=N+&2ff/KD|]&23>27S?E=0(2[ (ffff q{qH-+&3g_* 0hH~2t5 H(3.* E=0&2ff]dHWDd* V-7.5N_fi2ff]-+&7.2%E=0d4CY(E=_PYg3.Y&2(ffff* /=,>5R3.Y(*3C3.Y(EJ,C+(0(EJW+&2[,>5$/J+&3.EK5$0gLT5 7C*7.2?+&Vs](*3>2ff]E=0* __fi5 7P](* 0(_fi2R4@EK3.YF3.Y&2R_* +(,.* /p,>3>7+(_fi3.+&72]&D90(* S?E=_* /qV(7.59_fi2ff](+&72 {W2ff* _PYg_fi5 7.7.2ff,.VI5$0-](E=0&Xg6* 0(]F3.Y&25 L63.Y&2?0(23'4G5 7.8qmRw0`,+(_Y`*g_* ,+(* /ME=,67.2V~2ff*3>2ff](/KDR7.2V-/=* _fi2ff]|HND?3.Y&2ra* /J+&2y_fi5$S?V-+&3>2ff]|L!5 7QE=36LT7.5$S3.Y&2_+&77.2ff0\3ya* /=+&2ff,y5 LpEK3.,yV-*7.2ff0W3.,{1* __fi5 7]-E=0&X|3>5|3.Y(22ffN+(*3.EK5$0FLT5 7r3.Y(*3{-+(0W3.E=/s*?,>3.*H-/=2t,>3.*3>2%E=,72ff* _Y&2ff]sm6nQY&2-5v45 LE=0&LT5 7S?*3.EK5$0gE=0g,+(_Yd*?V(7.59_fi2ff](+&7.2rLT5$/=/K54@,3.Y&2](EK72ff_fi3.EK5$0A5 Lp3.Y&2*77.54C,rE=0A3.Y&2;0&23'4G5 7.8Imt)5$0(,>2ffN+&2ff0\3./=D {~0(5N]&2ff,@3.Y(*3r*7.20&5 3r* 0(_fi2ff,>3>7* /3>5g* 0\D0&59]&2;5 LE=0\3>272ff,>3t_* 0`Y(*a 2R0&5dE=0I+&2ff0(_fi2;5$03.Y&2ff,>2?0(5N]&2ff,ff{9Z>+-,>3.EKLTDNE=0(Xz3.Y&2ffE=7[2ff/=E=S?E=0-*3.EK5$0hEJ0,>3>2V:OG5 L3.Y&2tV-7.5N_fi2ff]-+&7.2rL!5 7@]&23>27S?E=0-E=0&X; ',>2V-*7P*3.EK5$0smw0|X 2ff0&27* /{&4CY(23.Y&275 7Q0&5 3,.+(_PYg*](DN0(* SRE=_* /IV(7.59_fi2ff](+&7.2y2a 2ff0\3.+-* /=/KD?I0(](,63.Y&2t,>5$/=+&3.E=5$0RLT5 7( ff S?*DR]&2V~2ff0(];5$0z4CY&23.Y&2763.Y&2Q 1*7.2y+&Vs](*3>2ff]?,.E=S+(/K3.* 0(25$+(,./KD5 76,>2ffN+&2ff0W3.E=* /=/KD {N* 0(]ffEKL3.Y&2Dg*7.2+&Vs](*3>2ff]d,>2ffN+&2ff0W3.E=* /=/KD {q5$0d3.Y&2U5 7]&27C5 L3.Y&2ff,.2U+&Vs](*3>2ff,m&5 7yV(7.2ff,.2ff0\3@V-+&7.V~5$,>2ff,{(E=3CE=,,.+|_EK2ff0\363.Y(*3!t+&Vs](*3.E=0&X%,_Y&2ffS2r2fi&E=,>363.Y(*3E=,pX$+-*7* 0\3>22ff]1{NLT5 7* 0\DRa* /=+(2ff,p5 L# ff( ff MI{3>5l/K2ff* ]3>53.Y(2F+(0-E=W+(2A,>5$/=+&3.E=5$0s{Q,>3.*7.3.E=0(Xc4CEK3.Y* 0WDE=0(EK3.EJ* /Qa* /=+&2ff,zL!5 7| (ff ImewLt,.+(_PY* 0+&Vs](*3>25 7]&27r2fi&E=,>3.,{1$',>2V-*7*3.EK5$0^4@E=/=/xEJS*H~5a 2 {s* 0WDA+&Vs](*3.E=0&Xa* /=+&2ff,?5 L[?h* 0(]j_fi5$0(](E=3.EK5$0sm5 7P]&27@4@E=/=/sLT5 7y,.5$S3V-/KDg_fi5$0(](E=3.EK5$0(* /E=0(]&2V~2ff0(]&2ff0-_fi2 myw0F3.Y&2;2fi9* S?V-/K22EJ0(EK3.E=* /s,>3.*3>2/K2ff* ]h3>5_fiDN_/=EJ_tH~2ffY(*ffa9EK5$+&7yE=0d4@Y(E=_YA3.Y&2qEKVjHq* _.8j* 0(]L!5 73.Ys{,>5c3.Y&2h2fi9* S?V-/K2h](5W2ff,|0&5 3z,*3.E=,>LTD3.Y-E=,R,>3>75$0&X 27r0(2%,.Y&5$+(/J]g0&5 3>2%3.Y(*3@3.Y&2%2fi&*V-/=2%E=0g3.Y(EJ,Q0&5 3>2U](5W2ff,@0&5 3CE=0Wa* /JE=](*3>2[3.Y&2U7.2ff,.+(/K35 L69V-EK7.3>2ff,:ff #"$O@3.Y(*3y ',>2V-*7P*3.EK5$0^_* 0AH~2U+-,>2ff]d3>5]&23>27S?EJ0&2%_fi5$0(](E=3.EK5$0(* /1E=0-]&2V~2ff0(]&2ff0(_fi2tE=0h/=E=0&2ff*7@0&234G5 7.8N,R5 Ly0(5 7S?* /=/KD](E=,>3>7PEKH-+&3>2ff]a*7E=*H-/K2ff,;2a 2ff0EKL@3.Y&2Dl_fi5$0\3.* E=0_fiDN_/=2ff,mw0l3.Y&2A,*NV-E=7.3>2ff,[* /=,>5gX$*a 2z*g_fi5$+(0W3>27.2fi&*V-/K2R,Y&54CEJ0&X3.Y(*3% ',.2V-*7*3.EK5$00&22ff]0&5 3%E=S?V-/KDA_fi5$0(]-EK3.EK5$0(* /E=0(](2VI2ff0-]&2ff0(_fi2RE=00&5$09'/JE=0&2ff*7U0(23'4G5 7.8N,;5 L@_fi5$0\3.E=0N+&5$+(,U7* 0(](5$SnQY(EJ,V(7.5 H-/=2ffS2dV-*V~27ff{_* 0(0&5 3zHI2h*ffa 5$E=]&2ff]HND,E=Sa*7E=*Hq/K2ff,t3.Y(*3_fi5$0W3.* E=0_fiDN_/=2ff,mV-/KD](E=,._fi723.E%$ffE=0&X3.Y&2F_fi5$0\3.EJ0W+&5$+-,?a*7E=*H-/K2ff,ff{G* ,z3.Y&2V(7.5 Hq/K2ffS72ff*V(V~2ff*7,rE=0A3.Y(2LT5 7Su5 L60(5$092fi9EJ,>3>2ff0(_fi25 7t0&5$09'+(0(EJW+&2ff0(2ff,.,Q5 LG,>5$/=+&3.EK5$0-,m[nQY(E=,@0&5 3>2,.Y&5v4C,#3.Y(*36E=00&5$09'/=EJ0&2ff*7 0&23'4G5 7.89, 5 L~](E=,._fi723>2Qa*7PE=*H-/K2ff,#*[,>3>75$0&X 276_fi5$0(]-EK3.EK5$0;3.Y(* 0?+-0(E=N+&2ff0&2ff,.,E=,7.2ffN+(EK72ff];L!5 7p$',>2V-*7*3.E=5$0?3>5tH~2a* /JE=]sm@/K3.Y(5$+&X$Y,.+(_PY?*t,>3>7.5$0(X 27p_fi5$0(](EK3.E=5$0]&D90(* S?E=_,;_* 0lH~2,>22ff0j* ,?0-*3.+&7* /{ 3.Y(20(22ff]3>5a 27EKLTDc3.Y(E=,3.Y&2*3>3>7* _fi3.EKa 2ff0&2ff,,E=0\a 5$/=aNE=0(Xr_* +-,.* /,.3>7.5$0&X 27?_fi5$0(](E=3.EK5$0]&5W2ff,;72ff](+(_fi25 LC0&23465 7.89,;4CEK3.Y_fiD9_/K2ff,;* ,*F4*ffD5 LQL!5 7PS?* /=E%$ffE=0(XA_* +(,.* /,.EK3.+-*3.EK5$0(,U4CEK3.YLT22ff]&H-* _.8Im'&)(*,+.-0/,132546/7(JwC3.Y-* 0&8h23>27NVqEK7.3>2ff,r* 0(]^nQY&5$S?* ,[@E=_Y-*7](,>5$0FLT5 7tY&2ff/KV(L<+(/x_fi5$SRS2ff0\3.,ffm;nCY(E=,@465 7.8h4G* ,%,.+&V&V~5 7.3>2ff]HND^3.Y&2zy*3.+&7* /9_EK2ff0(_fi2ff,U* 0(]98p0(X$E=0&227E=0(XC2ff,>2ff*7_PYl)5$+(0(_E=/5 LC)* 0(* ]-*d* 0(]`HND^3.Y&2w0(,>3.EK3.+&3>2[LT5 7CC5 HI5 3.EJ_,Q* 0(]gw0\3>2ff/J/=EKX 2ff0W3Q9DN,>3>2ffSR,m;:fi F 22N-`~$--&7T-;$ $\ $ 2< />=?/$'/,(,/22ff*7/{ mt:ff $OPmA@CBDFEHGFEJILKMILNIPOQRHGJS>ITSVU6ILSXWYS,0KTKZI%UVJS,\[,]0^`_aBcbde?fg@CK%GhiYIjEKkWYSfBHS7ONfimG&* 0mlh*3>25&{s)* /=EKLT5 70(EJ*9nlh5 7.X$* 0.ot* +&L<S?* 0(0sm2ff*7/{ m* 0-]y2ff_PY\3>27ff{Ntm(:ff $OPm w]&2ff0W3.EKL!D9E=0&X@E=0(]&2V~2ff0(]&2ff0-_EK2ff,xE=0_* +(,.* /9X 7P*V-Y(,4CEK3.YUL!22ff](H-* _.8Imw0!8QmNC5 7aNEK3D$r* 0(]z6m 2ff0-,>2ff0h:T2ff](E=3>5 7,PO\pqS7ONBjGILS7]rILStsBI uOJIPGK>WYS,0KTKZI%UVJS7OY;^@vB?;OYccwITSVU?fxTyz|{5K fNTye})SfBHS~OYfimlA5 7X$* 0got* +&L<S?* 0(01{NV-V1mz\j\ NmNV-E=7.3>2ff,{mW:ff #"$OPm rEK7.2ff_fi3>2ff]%_fiDN_/=EJ_X 7*V-Y(E=_* /72V(7.2ff,>2ff0W3.*3.EK5$0(,s5 L9LT22ff]&H-* _8rS?5N]&2ff/J,m1w0[MmvB62ff,0(*7]Imvy* 0&89,:T2ff](EK3>5 7,POfip5S~OYBjGITS,]ITSsBI uOJIPGKFWYS,0KTKZI%UVJS7OY;^@CBDONcHw#ILSVU#R?fvTyvK%JJS,Ty})SfBHS~OYfimlA5 7X$* 0got* +&L<S?* 0(01{NV-V1m\9j\ Nm* 0(]fiJournal Artificial Intelligence Research 12 (2000) 149198Submitted 11/99; published 3/00Model Inductive Bias LearningJonathan BaxterJ ONATHAN .BAXTER @ ANU . EDU . AUResearch School Information Sciences EngineeringAustralian National University, Canberra 0200, AustraliaAbstractmajor problem machine learning inductive bias: choose learners hypothesis space large enough contain solution problem learnt, yet smallenough ensure reliable generalization reasonably-sized training sets. Typically biassupplied hand skill insights experts. paper model automaticallylearning bias investigated. central assumption model learner embeddedwithin environment related learning tasks. Within environment learner samplemultiple tasks, hence search hypothesis space contains good solutionsmany problems environment. certain restrictions set hypothesisspaces available learner, show hypothesis space performs well sufficientlylarge number training tasks also perform well learning novel tasks environment. Explicit bounds also derived demonstrating learning multiple tasks withinenvironment related tasks potentially give much better generalization learning singletask.1. IntroductionOften hardest problem machine learning task initial choice hypothesis space;large enough contain solution problem hand, yet small enough ensuregood generalization small number examples (Mitchell, 1991). suitable biasfound, actual learning task often straightforward. Existing methods bias generallyrequire input human expert form heuristics domain knowledge (for example,selection appropriate set features). Despite successes, methodsclearly limited accuracy reliability experts knowledge also extentknowledge transferred learner. Thus natural search methodsautomatically learning bias.paper introduce analyze formal model bias learning builds uponPAC model machine learning variants (Vapnik, 1982; Valiant, 1984; Blumer,Ehrenfeucht, Haussler, & Warmuth, 1989; Haussler, 1992). models typically taketraining datafollowing general form: learner supplied hypothesis spacedrawn independently according underlying distribution. Based information contained , learners goal select hypothesisminimizing measureexpected loss respect (for example, case squared loss). models learnersbias represented choice ; contain good solution problem, then,regardless much data learner receives, cannot learn.course, best way bias learner supply containing single optimal hypothesis. finding hypothesis precisely original learning problem,fffififi fi!"#%$ '&("@#)*+#$,/.1032 46587 + # 9;:<=?>) *+ -c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiBAXTERPAC model distinction bias learning ordinary learning. put differently,PAC model model process inductive bias, simply takes hypothesis spacegiven proceeds there. overcome problem, paper assume insteadfaced single learning task, learner embedded within environmentrelated learning tasks. learner supplied family hypothesis spaces,) appropriate entire environment.goal find bias (i.e. hypothesis spacesimple example problem handwritten character recognition. preprocessing stageidentifies removes (small) rotations, dilations translations image characteradvantageous recognizing characters. set individual character recognitionproblems viewed environment learning problems (that is, set problemsform distinguish characters, distinguish B characters,on), preprocessor represents bias appropriate problems environment.likely many currently unknown biases also appropriateenvironment. would like able learn automatically.DCBmany examples learning problems viewed belonging environments related problems. example, individual face recognition problem belongs(essentially infinite) set related learning problems (all individual face recognition problems); set individual spoken word recognition problems forms another large environment,set fingerprint recognition problems, printed Chinese Japanese character recognition problems, stock price prediction problems on. Even medical diagnostic prognosticproblems, multitude diseases predicted pathology tests, constituteenvironment related learning problems.many cases environments normally modeled such; instead treatedsingle, multiple category learning problems. example, recognizing group faces wouldnormally viewed single learning problem multiple class labels (one facegroup), multiple individual learning problems. However, reliable classifierindividual face group constructed easily combined produceclassifier whole group. Furthermore, viewing faces environment relatedlearning problems, results presented show bias learnt goodlearning novel faces, claim cannot made traditional approach.point goes heart model: concerned adjusting learnersbias performs better fixed set learning problems. process factordinary learning richer hypothesis space components labelled biasalso able varied. Instead, suppose learner faced (potentially infinite) streamtasks, adjusting bias subset tasks improves learning performancefuture, yet unseen tasks.Bias appropriate problems environment must learnt samplingmany tasks. single task learnt bias extracted likely specifictask. rest paper, general theory bias learning developed based upon idealearning multiple related tasks. Loosely speaking (formal results stated Section 2),two main conclusions theory presented here:ELearning multiple related tasks reduces sampling burden required good generalization,least number-of-examples-required-per-task basis.150fiA ODEL NDUCTIVE B IAS L EARNINGEBias learnt sufficiently many training tasks likely good learning noveltasks drawn environment.second point shows form meta-generalization possible bias learning. Ordinarily, say learner generalizes well if, seeing sufficiently many training examples,produces hypothesis high probability perform well future examplestask. However, bias learner generalizes well if, seeing sufficiently many training tasks produces hypothesis space high probability contains good solutions novel tasks. Anotherterm used process Learning Learn (Thrun & Pratt, 1997).main theorems stated agnostic setting (that is,necessarily containhypothesis space solutions problems environment), also give improvedbounds realizable case. sample complexity bounds appearing results statedterms combinatorial parameters related complexity set hypothesis spacesavailable bias learner. Boolean learning problems (pattern classification) parametersbias learning analogue Vapnik-Chervonenkis dimension (Vapnik, 1982; Blumer et al.,1989).application general theory, problem learning appropriate set neuralnetwork features environment related tasks formulated bias learning problem.case continuous neural-network features able prove upper bounds numbertraining tasks number examples training task required ensure set featuresworks well training tasks will, high probability, work well novel tasks drawnenvironment. upper bound number tasks scalesmeasure complexity possible feature sets available learner, uppernumberbound number examples task scalesexamples required learn task true set features (that is, correct bias) alreadyknown, number tasks. Thus, case see number related taskslearnt increases, number examples required task good generalization decaysminimum possible. Boolean neural-network feature maps able show matchinglower bound number examples required per task form.F JILKMGONPQPF HGF JIRG1.1 Related Worklarge body previous algorithmic experimental work machine learningstatistics literature addressing problems inductive bias learning improving generalizationmultiple task learning. approaches seen special cases of, leastclosely aligned with, model described here, others orthogonal. Withoutcompletely exhaustive, section present overview main contributions. See ThrunPratt (1997, chapter 1) comprehensive treatment.EHierarchical Bayes. earliest approaches bias learning come Hierarchical Bayesianmethods statistics (Berger, 1985; Good, 1980; Gelman, Carlin, Stern, & Rubim, 1995).contrast Bayesian methodology, present paper takes essentially empiricalprocess approach modeling problem bias learning. However, model using mixturehierarchical Bayesian information-theoretic ideas presented Baxter (1997a),similar conclusions found here. empirical study showing utilityhierarchical Bayes approach domain containing large number related tasks givenHeskes (1998).151fiEBAXTEREEarly machine learning work. Rendell, Seshu, Tcheng (1987) VBMS Variable BiasManagement System introduced mechanism selecting amongst different learningalgorithms tackling new learning problem. STABB Shift Better Bias (Utgoff, 1986) another early scheme adjusting bias, unlike VBMS, STABBprimarily focussed searching bias applicable large problem domains. useenvironment related tasks paper may also interpreted environmentanalogous tasks sense conclusions one task arrived analogy(sufficiently many of) tasks. early discussion analogy context, see Russell (1989, S4.3), particular observation analogous problemssampling burden per task reduced.Metric-based approaches. metric used nearest-neighbour classification, vectorquantization determine nearest code-book vector, represents form inductive bias.Using model present paper, extra assumptions tasksenvironment (specifically, marginal input-space distributions identicaldiffer conditional probabilities assign class labels), shownoptimal metric distance measure use vector quantization onenearest-neighbour classification (Baxter, 1995a, 1997b; Baxter & Bartlett, 1998). metriclearnt sampling subset tasks environment, useddistance measure learning novel tasks drawn environment. Boundsnumber tasks examples task required ensure good performance noveltasks given Baxter Bartlett (1998), along experiment metricsuccessfully trained examples subset 400 Japanese characters usedfixed distance measure learning 2600 yet unseen characters.similar approach described Thrun Mitchell (1995), Thrun (1996),neural networks output trained match labels novel task, simultaneouslyforced match gradient derivative information generated distance metrictrained previous, related tasks. Performance novel tasks improved substantiallyuse derivative information.EENote many adaptive metric techniques used machine learning,focus exclusively adjusting metric fixed set problems rather learningmetric suitable learning novel, related tasks (bias learning).Feature learning learning internal representations. adaptive metric techniques,many approaches feature learning focus adapting features fixed taskrather learning features used novel tasks. One cases featureslearnt subset tasks explicit aim using novel tasksIntrator Edelman (1996) low-dimensional representation learnt setmultiple related image-recognition tasks used successfully learn novel taskskind. experiments reported Baxter (1995a, chapter 4) Baxter (1995b),Baxter Bartlett (1998) also nature.Bias learning Inductive Logic Programming (ILP). Predicate invention refers process ILP whereby new predicates thought useful classification task handadded learners domain knowledge. using new predicates background domain knowledge learning novel tasks, predicate invention may viewed form152fiA ODEL NDUCTIVE B IAS L EARNINGinductive bias learning. Preliminary results approach chess domain reportedKhan, Muggleton, Parson (1998).EEImproving performance fixed reference task. Multi-task learning (Caruana, 1997)trains extra neural network outputs match related tasks order improve generalizationperformance fixed reference task. Although approach explicitly identifyextra bias generated related tasks way used learn novel tasks,example exploiting bias provided set related tasks improve generalizationperformance. similar approaches include Suddarth Kergosien (1990), SuddarthHolden (1991), Abu-Mostafa (1993).EBias computational complexity. paper consider inductive bias samplecomplexity perspective: learnt bias decrease number examples requirednovel tasks good generalization? natural alternative line enquiry runningtime computational complexity learning algorithm may improved trainingrelated tasks. early algorithms neural networks vein contained SharkeySharkey (1993), Pratt (1992).Reinforcement Learning. Many control tasks appropriately viewed elements setsrelated tasks, learning navigate different goal states, learning setcomplex motor control tasks. number papers reinforcement learning literatureproposed algorithms sharing information related tasks improve averagegeneralization performance across tasks Singh (1992), Ring (1995), learning biasset tasks improve performance future tasks Sutton (1992), Thrun Schwartz(1995).1.2 Overview PaperSection 2 bias learning model formally defined, main sample complexity resultsgiven showing utility learning multiple related tasks feasibility bias learning.results show sample complexity controlled size certain covering numbersassociated set hypothesis spaces available bias learner, much waysample complexity learning Boolean functions controlled Vapnik-Chervonenkisdimension (Vapnik, 1982; Blumer et al., 1989). results Section 2 upper boundssample complexity required good generalization learning multiple tasks learninginductive bias.general results Section 2 specialized case feature learning neural networks Section 3, algorithm training features gradient descent also presented.special case able show matching lower bounds sample complexitymultiple task learning. Section 4 present concluding remarks directions futureresearch. Many proofs quite lengthy moved appendicesinterrupt flow main text.following tables contain glossary mathematical symbols used paper.153fiBAXTERSymbolU"DescriptionInput SpaceOutput SpaceDistribution(learning task)Loss functionHypothesis SpaceHypothesisError hypothesis distributionTraining setLearning AlgorithmEmpirical error training setSet learning tasksDistribution learning tasksFamily hypothesis spacesLoss hypothesis space environment-sampleEmpirical lossBias learning algorithmFunction inducedSetAverageSetSetFunction probability distributionsSetPseudo-metricPseudo-metricCovering numberCapacityCovering numberCapacitySequence hypothesesSequence distributionsAverage lossAverage lossSet feature mapsOutput class composed feature mapsHypothesis space associatedLoss function class associatedCovering numberCapacityPseudo-metric feature mapsCovering numberST"##) * + #V#)Y W *X #Z)\ *[P]fi^_W)*a`\V#b#b#b#2 b fifi # c 2 b#fifi #Rc b#fifi #Rc bdQb cbc# cfifi #=c bbbTeAeecbfA gf[eh Jijfi e fi f [ek Jijfi e ceAcbh Jijfi c b fi flgcJijfi bAP bnPg)*n\W)* `ppsr qpbh Jijfi p b fi f +pbk Hijfi p bpbft + 2 uvxw q fi qzyh Jijfi fi f=t + 2 uv{wU#fifi #=cfifi c154qpq fi qzyqZFirst Referenced155155155155155155156156156156157157157158158158159159159159159159159160160160160160160160160163163164164166166166166166166166166fiA ODEL NDUCTIVE B IAS L EARNINGhSymbolJijfi fi f + 2 uv{w8 DescriptionCovering numberk uv JijfiCapacity}|Neural network hypothesis space~ 0restricted vector^_Growth functionVapnik-Chervonenkis dimension~restricted matrix~ P]fi^_restricted matrixGrowth functionf PQDimension functionfUpper dimension functionf cLower dimension function cofn= gOptimal performancef#/ #=c Metric##RcAverage, ,c#=#c3Set/. > 2c 5Permutations integer pairs\jPermuted \f ` fiUEmpiricalmetric functionsW)* gnOptimal average errorFirst Referenced1661661671721721721731731731731731731751791791801821821821852. Bias Learning Modelsection bias learning model formally introduced. motivate definitions, firstdescribe main features ordinary (single-task) supervised learning models.2.1 Single-Task LearningComputational learning theory models supervised learning usually include following ingredients:Einput spaceESs"E loss function U$ "s"&D ,probability distributionEhypothesis space"output space,,set hypotheses functions#$ &".example, problem learn recognize images Marys face using neural network,would set images (typically represented subsetcomponentpixel intensity), would set, distribution would peaked imagesdifferent faces correct class labels. learners hypothesis space would classneural networks mapping input space. loss case would discrete loss:"fifiU zfi $ L155](1)fiBAXTERfi UU fi ] L: >""Using loss function allows us present unified treatment pattern recognition (, above), real-valued function learning (e.g. regression)usually.goal learner select hypothesisminimum expected loss:# CT(2))* + # $ B U # fi= f QfiR#minimizingcourse, learner know cannot search)* + # . practice, learner samples repeatedly " according distributiongenerate training set$fififi fi j(3)# CT . Hence, generalBased information contained learner produces hypothesisVlearner simply map set training samples hypothesis space :V $ DT" &V(stochastic learners treated assuming distribution-valued .)#Many algorithms seek minimize empirical loss , defined by:W)* X # $ ^ U # fi(4)course, intelligent things data simply minimizing empiricalerrorfor example one add regularisation terms avoid over-fitting.However learner chooses hypothesis , uniform bound (over)probability large deviation, bound learners genas function empirical loss training set. Whethereralization errorbound holds depends upon richness . conditions ensuring convergencewell understood; Boolean function learning (, discreteloss), convergence controlled VC-dimension1 :) W * X #)*+ #) * + ##W) *X ## C) *+ #) W *X #" Bfifi supposeprobability distributionfffi6fifi fibe isanygenerated^times fi according . Letf$ = . probabilityby atsamplingleast : (over choice training set ),# C satisfy>^##fW(5))* + )* X ffKO^ f KTheorem 1. LetProofs result may found Vapnik (1982), Blumer et al. (1989),reproduced here.aJ31. VC dimension class Boolean functionslargest integer exists subsetrestriction contains Boolean functions .156fiA ODEL NDUCTIVE B IAS L EARNING)*+ #) *+ #) W *X #Theorem 1 provides conditions deviationactually small.likely small, guarantee true errorgoverned choice . contains solution small error learner minimizeserror training set, high probabilitysmall. However, bad choicemean hope achieving small error. Thus, bias learner model2represented choice hypothesis space .)*+ #2.2 Bias Learning Modelmain extra assumption bias learning model introduced learner embedded environment related tasks, sample environment generate multipletraining sets belonging multiple different tasks. model ordinary (single-task). bias learninglearning, learning task represented distributionmodel, environment learning problems represented pairset(i.e., set possible learning problems),probability distributionsdistribution . controls learning problems learner likely see3 . example,learner face recognition environment, highly peaked face-recognition-typeproblems, whereas learner character recognition environment peakedcharacter-recognition-type problems (here, introduction, view environmentssets individual classification problems, rather single, multiple class classification problems).Recall last paragraph previous section learners bias representedchoice hypothesis space . enable learner learn bias, supply family.set hypothesis spacesPutting together, formally learning learn bias learning problem consists of:"Z" fi ZZZZ$Einput spaceEloss functionoutput spaceU$ "s"&D ,E environment fi Zdistribution ,Ehypothesis space family"(both separable metric spaces),set probability distributionsUUCassume loss function rangeassume bounded.D"set functions#%$ &"Z.fi 6 , equivalently, rescaling,2. bias also governed learner uses hypothesis space. example, circumstanceslearner may choose use full power (a neural network example early-stopping). simplicitypaper abstract away features algorithm assume uses entire hypothesis space.3. domain -algebra subsets . suitable one purposes Borel -algebrageneratedtopology weak convergence . assume separable metric spaces, alsoseparable metric space Prohorov metric (which metrizes topology weak convergence) (Parthasarathy,1967), problem existence measures. See Appendix discussion,particularly proof part 5 Lemma 32.157QfiBAXTERdefine goal bias learner find hypothesis spacefollowing loss:Cminimizing)*6[ $ = )*+ # f Z(6)= U # fi= f fi= f ZZ#way )*[ small if, high -probability, contains good solutionZproblem drawn random according . sense )*[ measures appropriateZbias embodied environment fi .Zgeneral learner know , able find minimizing )*[Ztimes according yield:cfifi .E Sample ^ times S" according yield:Bfifi fi .E resulting P training setshenceforth called P]fi^_ -sample generatedprocessare supplied learner. sequel, P]fi^_ -sample denoted\ written matrix:fffifffij........\ $(7)...cfi cc fi c !. ccP]fi^_ -sample simply P training setsfifia sampled P different learning taskscfi fi , task selected according environmental probability distribution Z .size training set kept primarily facilitate analysis.'C .Based information contained \ , learner must choose hypothesis space\directly. However, learner sample environment following way:ESamplePOne way would learner finddefined by:minimizing empirical loss ,) W *a `[)*cfi fic) W * ` $ P3 R )W * X? #(8))*6[Notesimply average best possible empirical error achievabletraining set , using function . biased estimate. unbiased estimatewould require choosing minimal average error distributions, defined.ordinary learning, likely intelligent things training dataminimizing (8). Denoting set-samples, general biaslearner map takes-samples input produces hypothesis spacesoutput:\VP]fi^_c cR 3 #) *+P]fi^_. c 2 5V $s"&'c158Ss" . c 2 5PSC(9)fiA ODEL NDUCTIVE B IAS L EARNINGV(as stated, deterministic bias learner, however trivial extend results stochasticlearners).Note paper concerned sample complexity properties biaslearner ; discuss issues computability .Since searching entire hypothesis spaces within family hypothesis spaces, extra representational question model bias learning presentrepresented searched . deferordinary learning, familydiscussion Section 2.5, main sample complexity results model bias learningintroduced. specific case learning set features suitable environmentrelated learning problems, see Section 3.Regardless learner chooses hypothesis space , uniform bound (over) probability large deviation, computeupper bound, bound bias learners generalization error.view, question generalization within bias learning model becomes: manytasks ( ) many examples task ( ) required ensureclose high probability, uniformly? Or, informally, many tasksmany examples task required ensure hypothesis space good solutionstraining tasks contain good solutions novel tasks drawn environment?turns kind uniform convergence bias learning controlled sizecertain function classes derived hypothesis space family , much wayVC-dimension hypothesis spacecontrols uniform convergence case Booleanfunction learning (Theorem 1). size measures auxiliary definitions neededstate main theorem introduced following subsection.VVVVCP) W *a`) W *`^)*6[) W *`C)*[)*[#%$ &D" , define #=b$ "&( fi 6#b fi= $ U # 9fiR(10)hypothesis space hypothesis space family , definebff$ B #=b$# CT j(11)c## c , define #fifi # c b$ DT" &( fi 6sequence P hypothesesfific#fifi #Rc bfififi c fi c $ P U # fi(12)db##=c b . hypothesis space family , definealso use denotefificb $ B #fifi # c b$#fifi # c CT j(13)2.3 Covering NumbersDefinition 1. hypothesisDefinecb $ cb159(14)fiBAXTER#$ & "bbP"&( fi 6#bfirst part definition above, hypothesesturned functionsmappingcomposition loss function.collectionfunctions original hypotheses come .often called loss-function class.case interested average loss across tasks, hypotheseschosen fixed hypothesis space . motivates definition. Finally,collection, restrictionbelong singlehypothesis space.cbCDefinition 2.#fi fi # c bC, definehypothesis space familyTe $ (& fi 6e $ 3 R )* + #, definedb#fifi # cP cb(15)e $Be $ C j(16)cbe controls large P]fi^_ -sample \ must ensuresize)W *a` )*6[ close uniformly C . size defined termscertain coveringcb numbers, neede define measure distanceelementsalso elements .nfifi c sequence P probability distributions DT" .Definition 3. Letcdb b C b , definefiflg dQb fi yb $ . 5 dbfififi c fi c ;: ybfififi c fi c(17)ffif c c fi cZe e C e , defineSimilarly, distributionfi >f [ efi e $ e;: e f Z(18)>>fg f [ pseudo-metrics cb e respectively.easily verifiede fTe TeeDC e ,Definition 4. -cover fi [ setfifif [ Te fi e Ti ; . Note require Te containedhe fe , measurable functions. Let Jijfi fi [ denote size smallestecover. Define capacityk Jilfi e $h Jilfi e fi f [(19)[. h Jijfi cb fi f g defined similarsupremum probability measurescfg place f [ . Define capacity b by:way, usingk Jijfi cb $ g h Jilfi cb fi fg(20)supremum sequences P probability measures S" .R ff.4. pseudo-metric metric without condition4160fiA ODEL NDUCTIVE B IAS L EARNING2.4 Uniform Convergence Bias Learnersenough machinery state main theorem. theorem hypothesis spacefamily required permissible. Permissibility discussed detail Appendix D, noteweak measure-theoretic condition satisfied almost real-world hypothesis spacefamilies. logarithms base ."Z\P]fi^_"ZcP^fi 3fififiPfifififiPk eP fi> > fi fi > finumber examples ^ task satisfiesk fi cb^!fiPffi > > fi"i $> # fiC satisfyprobability least : (over P]fi^_ -sample \ ),)*[ % )W *a` 9KTheorem 2. Supposeseparable metric spaces let probability distribution , set distributions. Suppose-sample generatedsampling times according give, sampling timesgenerate,. Letpermissiblehypothesis space family. number tasks satisfies(21)(22)(23)Proof. See Appendix A.k Jijfi cbP]fi^_several important points note Theorem 2:k J ijfi e)* [# Cs)*O[1. Provided capacitiesfinite, theorem shows biasbound generalisation errorlearner selects hypothesis spacestermssufficiently large-samples . bias learners findexact valueinvolves finding smallest error hypothesistraining sets . upper bound(found, examplegradient descent error function) still give upper bound. SeeSection 3.3.1 brief discussion achieved feature learning setting.) W *a` W)*aP `) W *`\)*[PC\) W *a`^close uniformly2. order learn bias (in sense), number tasks number examples taskmustsufficiently large. intuitively reasonable bias learner must seesufficiently many tasks confident nature environment, sufficientlymany examples task confident nature task.CZ^) W * `3. learner foundsmall value, uselearn novel tasks drawn according . One following theorem boundingsample complexity required good generalisation learning (the proofsimilar proof bound Theorem 2).161fiBAXTERfififi fiilfi " ijfi^k ' b^ fi >) ( fi fi >!(24)# CT satisfyprobability least %: ,)*+ # % )W *X # K ijkcapacity Jilfi appearing equation (24) defined analogous#b fito=the:f #b # b $ * fashioncapacities Definition 4 (we use pseudo-metric + fi +# yb QfiR f fi= ). important thing note Theorem 3 numberex-Theorem 3. Lettraining set generated samplingaccording distribution . Let permissible hypothesis space.&%%, number training examples satisfiesamples required good generalisation learning novel tasks proportional logarithm capacity learnt hypothesis space . contrast, learnerbias learning, reason select one hypothesis spaceconsequently would view candidate solution hypothesishypothesis spaces. Thus, sample complexity proportionalcapacity ,, general considerably larger capacity. learning learner learnt learn environmentindividualsense needs far smaller training sets learn novel tasks.fi ZCCb bC:_W) *a` ff K)*[0* $ 3 R ) * + # 1fiW *a`)= )*+ #4. learnt hypothesis spacesmall value, Theorem 2 tells usprobability least, expected valuenovel taskless. course, rule really bad performance tasks. However, probability generating bad tasks bounded. particular,noteexpected value function, Markovsinequality, -/. ,e- #0 *2[3)6*[W)*`e$ e-9 Kifi-(with probability%: ).ijfi5. Keeping accuracy confidence parametersfixed, note number examplesrequired task good generalisation obeys^ F P k ijfi cb(25)ck Jilfi b increases sublinearly P , upper bound numberprovidedexamples required task decrease number tasks increases. showssuitably constructed hypothesis space families possible share informationtasks. discussed Theorem 4 below.162fiA ODEL NDUCTIVE B IAS L EARNING2.5 Choosing Hypothesis Space Family) *[P]fi^_\.) W * `)* [CP ^Theorem 2 provides conditionsclose, guaranteeactually small. governed choice . contains hypothesisspace small valuelearner able findminimizing errorsample (i.e., minimizing), then, sufficiently large , Theorem 2 enthesures high probabilitysmall. However, bad choice meanhope finding small error. sense choice represents hyper-biaslearner.Note sample complexity point view, optimal hypothesis space family choosecontains good solutionsone containing single, minimal hypothesis spaceproblems environment (or least set problems high -probability), more.bias learning (because choice made hypothesisspaces), output bias learning algorithm guaranteed good hypothesis spaceenvironment, since hypothesis space minimal, learning problem within environment using require smallest possible number examples. However, scenarioanalagous trivial scenario ordinary learning learning algorithm containssingle, optimal hypothesis problem learnt. case learning done,bias learning done correct hypothesis space already known.extreme, contains single hypothesis space consisting possible functionsbias learning impossible bias learner cannot producerestricted hypothesis space output, hence cannot produce hypothesis space improvedsample complexity requirements yet unseen tasks.Focussing two extremes highlights minimal requirements successful biasmust strictly smaller spacelearning occur: hypothesis spacesfunctions, small skewed none contain good solutionslarge majority problems environment.may seem simply replaced problem selecting right bias (i.e., selectingright hypothesis space ) equally difficult problem selecting right hyper-bias (i.e.,right hypothesis space family ). However, many cases selecting right hyper-bias fareasier selecting right bias. example, Section 3 see feature selectionproblem may viewed bias selection problem. Selecting right features extremelydifficult one knows little environment, intelligent trial-and-error typically bestone do. However, bias learning scenario, one specify set featuresexist, find loosely parameterised set features (for example neural networks), learnfeatures sampling multiple related tasks.)*[)*[W *a`)Z& "C'&"2.6 Learning Multiple TasksPfi Zmay learner interested learning learn, wants learn fixed set. previous section, assume learner startstasks environmenthypothesis space family , also receives-sample generateddistributions. time, however, learner simply looking hypotheses, contained hypothesis space , average generalizationerror hypotheses minimal. Denotingwriting,P#fifi #RcPfi fi cP]fi^_#fifi #=c163\Pnfi fi cfiBAXTERc) * g $ P) *+ #(26)cP U # fi= f fi=fiempirical loss \cW)*a` $ P )W *X? #(27)cP ^ U # 4 fi 44##=c , prove uniform boundbefore, regardless learner choosesfifig## c performWprobability large deviation )* ` )*fifiwell training sets \ high probability perform well future exampleserror given by:tasks.nfifi c P^"\P]fi^_Theorem 4. Letprobability distributionsletaccording . Letsample generated sampling timespermissible hypothesis space family. number examples task satisfies"^k) ( fi cbffP >5^!fi(28)fi > #C c satisfyprobability least : (over choice \ ),(29))* g % )W *a` ffKck b(recall Definition 4 meaning Jilfi ).Proof. Omitted (follow proof bound ^ Theorem 2).bound ^ Theorem 4 virtually identical bound ^ Theorem 2, notedepends inversely number tasks P (assuming first part maxk cbexpression dominate one). Whether helps depends rate growth )( fifunction P . following Lemma shows growth always small enough ensurenever worse learning multiple tasks (at least terms upper bound numberexamples required per task).,k li fi b k H ilfi cb k li fi bcLemma 5. hypothesis space family164(30)fiA ODEL NDUCTIVE B IAS L EARNING##R#cbProof. Let 6 denote set functionsfifi ck Hijfi cb memberk Jijfi 6 .!C (recall Definition 1).87bc6hypothesis spacekkilfibHjfiLemma 29 Appendix B,6right hand inequality follows.n meaFor first inequality,let probability measure " letcsure !<" obtained using first copy <"cb flg product,#andb C ignoringbelements product. Let-cover fi. Pickc b C fg # fi # fifi # b fi:9fifi;9 c b . construction,let :9fifi;9flg # fi # fi3fi # b fi: 9fi;fi 9 c b f + # fi: 9b , establishes first inequality.k ji fi bk Jijfi cb P k ijfi b(31)keeping accuracy parameters fixed, plugging (31) (28), see upperbound number examples required task never increases number tasks,best decreases F NPQ . Although upper bound, provides strong hintLemma 5learning multiple related tasks advantageous number examples required per taskbasis. Section 3 shown feature learning types behavior possible,decrease.advantage2.7 DependenceF NPQNi >NiTheorems 2, 3 4 bounds sample complexity scale. behaviorimprovedempirical loss always guaranteed zero (i.e., realizablecase). behavior results interested relative deviation empiricaltrue loss, rather absolute deviation. Formal theorems along lines stated AppendixA.3.3. Feature Learninguse restricted feature sets nearly ubiquitous method encoding bias many areasmachine learning statistics, including classification, regression density estimation.section show problem choosing set features environmentrelated tasks recast bias learning problem. Explicit boundscalculated general feature classes Section 3.2. bounds applied problemlearning neural network feature set Section 3.3.k e fiai3k cb fiai33.1 Feature Learning ModelConsider following quote Vapnik (1996):classical approach estimating multidimensional functional dependenciesbased following belief:Real-life problems exists small number strong features, simplefunctions (say linear combinations) approximate well unknown function.Therefore, necessary carefully choose low-dimensional feature spaceuse regular statistical techniques construct approximation.165fiBAXTERq$ &qfifiqqqqp"q C_opTr q $ B r q $ C ppr$$C(32)q q jproblem carefully choosing right features q equivalent bias learningC . Hence, providedproblem find right hypothesis spacek ek cb learner embedded withinenvironment related tasks, capacities fiai fiai finite, Theorem 2 tellsus feature set q learnt rather carefully chosen. represents importantsimplification, choosing set features often difficult part machine learningproblem.k ek cbSection 3.2 give theorem bounding fiai fiai3 general feature classes.theorem specialized neural network classes Section 3.3.pNote forced function class feature maps q , althoughpnecessary. Indeed variants results follow obtained allowed varyq .general set strong features may viewed function+< mapping input(typically lower) dimensional space < . Letset featurespace=maps (each may viewed set features). must>= <carefully chosen quote. general, simple functions features mayrepresented class functions mapping < .define hypothesis?99, hypothesis space familyspace3.2 Capacity Bounds General Feature Classesfi= &qs"s"pUC pq fi=Ufi 6 fiR & fi= Ub "pbbp b r $ r q $ C p b fi q Copbk Jilfi p b $E h Jilfi p b fi f ++f$ 9zCBfiR:supremum probability measures <" , + :9fi;9 *GF9 CBfi= fCBfi= .define capacity p ofb first define pseudo-metric f + 2 uvxwpulling back H metric follows:ft + 2 uv w q fi q $ u v 9 r q fi=;/: 9 r q QfiR f fi=(33)f=tfteasily verified + 2 uvxw pseudo-metric. Note + 2 uv w well defined suprepbmum integrand must measurable. guaranteed theh hypothesis space familyftp ib r q $ q C permissible (Lemma32, part 4). define Jilfi fi + 2 uv{w8pbfsmallest -cover pseudo-metric space fi + 2 u v w -capacity (with respect )k uv Jilfi $h Jilfi fi f + 2 uv{w+supremum probability measures -" . state main theoremA@<Notationally easier view feature maps mapping, also absorb loss function definition viewing 9A@:9 CBvia CB. Previously latter function wouldmap <denoted 9 follows drop subscript cause confusion.class 9 belongs still denoted .?99definitions let. Define capacityusual way,section.166fiA ODEL NDUCTIVE B IAS L EARNINGiiK >Theorem 6. Let,hypothesis space family equation (32).k J ilfi cbk Jijfi ek Hfi p b c k u v Ji > fik uv Hilfiijfiaifiai > .(34)(35)Proof. See Appendix B.3.3 Learning Neural Network Featuresfgeneral, set features may viewed map (typically high-dimensional) input=much smaller dimensional space( JLK). section consider approximatspaceing feature map one-hidden-layer neural network input nodes J output nodesQP RN=(Figure 1). denote set feature mapsRbounded subset TS ( U number weights (parameters) first two layers).set previous section.feature N,J definedf|| 2fifi | 2 $ C| 2 $ & fi 6 3fifib|N 2 9 $ VWX B 4 # 4 KYB b[Z(36)4#boutput#4 output\^] node first hidden layer, CBfifi_B$node parameters th feature V sigmoid squashing function V & fi 6 .# $ &S , 3fifi U , computesfirst layer hidden node# 9 $ V WXZ[4 4 K(37)``4hidden nodes parameters. assume V Lipschitz. weightfifi``vector entire feature map thusPfifififi bfifi b_fi Bfi_fi Bbfi _fi B =fi _fi B = b````UfUtotal number feature parameters U KffK J K .parguments sake, assume simple functions features (the class previous5section) squashed affine maps using sigmoid function V (in keepingPneural network flavor features). Thus, setting feature weights generateshypothesis space:| $=N | 2 Kf $fi fiC R fi==edR=bounded subset . set hypothesis spaces,$ B | Q$ P C R> k ; h?lmgnh > k h Hkporq .5. Lipschitz exists constant g h jVcb167(38)(39)fiBAXTERMultiple Output ClassesnklFeatureMapInputPPP]fi^_Figure 1: Neural network feature learning. feature map implemented first twohidden layers. output nodes correspond different taskssample . node network computes squashed linear function nodesprevious layer.\fifi=featurehypothesis space family. restrictions output layer weightsPweights , restriction Lipschitz squashing function needed obtain finite upperbounds covering numbers Theorem 2.Finding good set features environmentequivalent finding good hyP, turn means finding good set feature map parameters .pothesis spaceTheorem 2, correct set features may learnt finding hypothesis spacesmall error sufficiently large-sample . Specializing squared loss, presentframework empirical loss(equation (8)) givenfi Z| CP]fi^_\\c>=bb(40))W *` | P.tsu62 2w vwRvwv 2 s^x65 >y k ^ 4{z VYb bN | 2 4 9K f :< 4}|Since sigmoid function V range fi 6 , also restrict outputs " range.|3.3.1 LGORITHMSF INDINGG OOD ETF EATURESProvided squashing function V differentiable, gradient descent (with small variationPbackpropagation compute derivatives) used find feature weights minimizing (40)(or least local minimum (40)). extra difficulty ordinary gradientdescent appearance definition. solution perform gradientP= node feature weights .descent output parametersdetails see Baxter (1995b) Baxter (1995a, chapter 4), empirical results supportingtheoretical results presented also given.RW * ` |)fifi168fiA ODEL NDUCTIVE B IAS L EARNING3.3.2 AMPLE C OMPLEXITY B OUNDS\k J ijfi cbN EURAL -N ETWORK F EATURE L EARNINGsize ensuring resulting features good learning novel tasksenvironment given Theorem 2. compute logarithm coveringnumbers.k J ijfi e| $aP C ff S5 hypothesis space family }| formTheorem 7. Let 3~=| $ V bN | 2 Kf $fi fi = C = fi| N | 2fi fi N | 2 = neural network U weights mapping = .Pfeature weights output weights 3fififi = bounded, squashing function VULipschitz, squared loss, output space " fi 6 (any bounded subset do),exist constants fi =y (independent ilfi U J ) . ,k Jijfi cb J KP}K U(41)k Jijfi e U Ri(42)(recall specialized squared loss here).Proof. See Appendix B.Noting neural network hypothesis space familyTheorem 2 gives following theorem.permissible, plugging (41) (42)||Theorem 8. Lethypothesis space family hypothesis spaceset squashed linear maps composed neural network feature map, above. Supposenumber features J , total number feature weights W. Assume feature weights-sampleoutput weights bounded, squashing function V Lipschitz. Letgenerated environment.\fi ZP fi F > U K = R fiP]fi^_(43)^!fi F > J K K U P K P R| C satisfyprobability least :)* [ }| )W * ` }| 9 K il169(44)(45)fiBAXTER3.3.3 ISCUSSIONF KNPQ1. Keeping accuracy confidence parameters fixed, upper bound numberexamples required task behaves like JU. learner simply learningfixed tasks (rather learning learn), upper bound also applies (recallTheorem 4).P^FPupper bound2. Note away feature map altogether Ubecomes J , independent (apart less important term). termsupper bound, learning tasks becomes hard learning one task. extreme,fix output weights effectively Jnumber examples requiredtask decreases U. Thus range behavior number examples requireddecrease numbertask possible: improvementtasks increases (recall discussion end Section 2.6).PF NPQPF NPQ3. feature map learnt (which achieved using techniques outlined Baxter,1995b; Baxter & Bartlett, 1998; Baxter, 1995a, chapter 4), output weightsestimated learn novel task. keeping accuracy parameters fixed, requiresJ examples. Thus, number tasks learnt increases, upper boundnumber examples required task decays minimum possible, J .FF4. small number strong features assumption correct, J small. However,typically little idea features are, confident neuralnetwork capable implementing good feature set need large, implyingUJ .JUdecreases rapidly increasing UJ , leastterms upper bound number examples required per task, learning small featuresets ideal application bias learning. However, upper bound numbertasks fare well scales U .F KNPQPFspecial case multi-task framework one marginal distribution input~ task 3fifiP , varies tasks conditionalspacedistribution output space " . example would multi-class problem facel3fifiP; P number faces recognizedrecognition, "marginal distribution simply natural distribution images faces.case, every example 4 havein addition sample 4 th tasks conditionaldistribution " samples remaining P: conditional distributions " ,view P training sets containing ^ examples one large training set multi-classproblem ^TP examples altogether. bound ^ Theorem 8 states ^TPF P J K U , proportional total number parameters network, result would3.3.4 C OMPARISONRADITIONAL ULTIPLE -C LASS C LASSIFICATIONexpect from6 (Haussler, 1992).specialized traditional multiple-class, single task framework, Theorem 8 consistent bounds already known. However, already argued, problems facerecognition really single-task, multiple-class problems. appropriately viewed6. example classified large margin naive parameter counting improved upon (Bartlett,1998).170fiA ODEL NDUCTIVE B IAS L EARNINGPP(potentially infinite) collection distinct binary classification problems. case, goalbias learning find single -output network classify subset faceswell. learn set features reliably used fixed preprocessing distinguishing single face faces. new thing provided Theorem 8: tells usprovided trained -output neural network sufficiently many examples sufficientlymany tasks, confident common feature map learnt tasks goodlearning new, yet unseen task, provided new task drawn distributiongenerated training tasks. addition, learning new task requires estimating Joutput node parameters task, vastly easier problem estimating parametersentire network, sample computational complexity perspective. Also, sincehigh confidence learnt features good learning novel tasks drawnenvironment, features candidate study learnnature environment. claim could made features learntsmall set tasks guarantee generalization novel tasks, likely featureswould implement idiosyncrasies specific tasks, rather invariances apply acrosstasks.PP^Pviewed bias (or feature) learning perspective, rather traditional -classclassification perspective, bound number examples required task takessomewhat different meaning. tells us provided large (i.e., collecting exampleslarge number tasks), really need collect examples wouldexamples vs. J examples).otherwise collect feature map already known ( J Utells us burden imposed feature learning made negligibly small, leastviewed perspective sampling burden required task.PK NP3.4 Learning Multiple Tasks Boolean Feature MapsPIgnoring accuracy confidence parameters , Theorem 8 shows numberexamples required task learning tasks common neural-network feature mapJUbounded, J number features U numberadjustable parameters feature map. SinceJ examples required learn single tasktrue features known, shows upper bound number examplesrequired task decays (in order) minimum possible number tasks increases.suggests learning multiple tasks advantageous, truly convincing need)prove lower bound form. Proving lower bounds real-valued setting (complicated fact single example convey infinite amount information,one typically make extra assumptions, targetscorruptednoise process. Rather concern complications, section restrictattention Boolean hypothesis space families (meaning hypothesismapsmeasure error discrete lossotherwise).F KNPQFPC ""# CU # 9fiR # } U # 9fiR"show sample complexity learning P tasks Boolean hypothesis space familyf= PQ (that is, give nearly matching uppertype parametercontrolled VC dimensionf PQ ). derive bounds f PQ hypothesis spacelower bounds involvingfamily considered previous section Lipschitz sigmoid function V replaced hardthreshold (linear threshold networks).171fiBAXTERFwell bound number examples required per task good generalization acrosstasks, Theorem 8 also shows features performing well Utasks generalize wellnovel tasks, U number parameters feature map. Given many featurelearning problems U likely quite large (recall Note 4 Section 3.3.3), would usefulknowUtasks fact necessary without restrictions environmentaldistributions generating tasks. Unfortunately, yet able show lowerbound.empirical evidence suggesting practice upper bound numbertasks may weak. example, Baxter Bartlett (1998) reported experimentsset neural network features learnt subset 400 Japanese characters turnedgood enough classifying 2600 unseen characters, even though features containedseveral hundred thousand parameters. Similar results may found Intrator Edelman (1996)experiments reported Thrun (1996) Thrun Pratt (1997, chapter 8).gap experiment theory may another example looseness inherentgeneral bounds, may also analysis tightened. particular, boundnumber tasks insensitive size class output functions (the class Section 3.1),may looseness arisen.ZFp3.4.1 U PPER L OWER B OUNDSPACE FAMILIESL EARNING TASKSB OOLEAN H YPOTHESISfifi C ~ 0~ 0 $ B #fifi # $# CT j~~Clearly 0 . 0 say shatters . growth function defined^_ $ 0 L / ~ 0size largest set shattered :Vapnik-Chervonenkis dimension= $ ^ $ ^_ jFirst recall concepts theory Boolean function learning. Letclass.set binary vectors obtainableBoolean functionsapplying functions :important result theory learning Boolean functions Sauers Lemma (Sauer, 1972),also make use.Lemma 9 (Sauers Lemma). Boolean function classpositive integers^f ,^ fi^_ ^ f.generalize concepts learningP172tasks Boolean hypothesis space family.fiA ODEL NDUCTIVE B IAS L EARNINGDefinition 5. Letinput spacematrices,^ matricesDenote P. bec 2 a5 Boolean hypothesis. c 2 space5 family.CC~, define set (binary).##$z#fi fi # c CT_~ $.........#=c c#Rc c~ }$ ~P]fi^_P . fi^ . , defineP]fi^_ $ L ~c ~ c]Pfi_^%Notematrix. say shatterscf PQ $ ^ $ P]fi^_ jDefineDefineLemma 10..P.letf $ =f $ =f fi ff PQ fi f P fi f#f K fPProof. first inequality trivial definitions. get second term maximumC = f construct matrixsecondinequality, choosec.25fCwhose rows length shattered . clearlyshatters .first term maximum take sequencefifi . 5 shattered (the hypothesisspace consisting union hypothesis spaces ), distribute elements equallyamong rows (throw away leftovers). set matrices# ff# ff$#C.........#c#ccf~ size .^ NP subsetc .c 5Lemma 11.^P]fi^_ f= PQ ?173fifiBAXTERP P]fi^_ P9^_fifi c##>^f PQ = P f QPP#fifi #RcProof. Observe ,collection Booleanobtained first choosing functionsfunctions sequences, applyingfirst examples,second examples on.definition,, hence result follows Lemma 9 applied.C^k cb fiai3P]fi ^_fione follows proof Theorem 4 (in particular proof Theorem 18 AppendixA) clear .,may replacedBooleanEcase. Making replacement Theorem 18, using choicesdiscussionfollowing Theorem 26, obtain following bound probability large deviationempirical true performance Boolean setting.nfifi c Plet \P]fi^_^. Let B0* \ $=d C c $ )* g fi )W a* ` ffK ij P]fi ^_ ) : > P9^_N(46)Corollary 13. conditions Theorem 12, number examples ^ taskprobability distributionsTheorem 12. Let-sample generated sampling timesaccordingpermissible Boolean hypothesis space family. %,satisfies^ fi > f PQ K P!probability least : (over choice \ ),)* g % )W *a` ffKC c(47)satisfy(48)Proof. Applying Theorem 12, requireP]fi ^_ ) : > P9^_N fisatisfied^!fi > f PQ f ^ PQ K P fifiM ,used Lemma 11. Now,^ K K Ifif PQNi > , (49) satisfied^!fiI ^ . setting IL^!fi > f PQ K P174(49)fiA ODEL NDUCTIVE B IAS L EARNINGCorollary 13 shows algorithm learningrequiresPtasks using hypothesis space family^ F > =f PQ K P Rc(50)\Pexamples task ensure high probability average true error hypothesesselectswithin average empirical error sample . givetheorem showing learning algorithm required produce hypotheses whose averagetrue error within best possible error (achievable using) arbitrary sequencedistributions, withinfactor number examples equation (50) alsonecessary.sequenceprobability distributions, definefifi c= g cnfi fi c Pg c $ R )* gc Pcontains least twoPbe3afi Booleanhypothesis space familyfifi let V c learning algorithm taking input P]fi^_c -samplesc.25C\ ( producing output P hypotheses #fi fi #=c C .%i%MN % %MN ,^ % > f PQ KM:i > P : 3!cnfifi probability least (overexist distributions\random choice ),g)* V c J\ . = g c ffKTheorem 14. Letfunctions.Proof. See Appendix CNi3f PQ3.4.2 L INEAR HRESHOLD N ETWORKSPf PQfactor, sample complexityTheorems 13 14 show within constantslearning tasks using Boolean hypothesis space family controlled complexity parameter. section derive boundshypothesis space families constructedthresholded linear combinations Boolean feature maps. Specifically, assumeform given (39), (38), (37) (36), squashing function V replaced hardthreshold:fiVotherwise$ :Rfififi Rydont restrict range feature output layer weights. Note caseproof Theorem 8 carry constants Theorem 7 dependLipschitz bound V .f UTheorem 15. Let hypothesis space family form given (39), (38), (37) (36),hard threshold sigmoid function V . Recall parameters , J input dimension,number hidden nodes feature map number features (output nodes feature map)175fiBAXTER$ U f K]K J U K (the number adjustable parameters featuref PQ U K J K J K U KzP>Proof. Recall. c 2 for5 eachM | P ~ C TS , | $ &( = denotes feature map parameters P .C, letdenote matrix|6|...|M .. c. . | .. c~ set binary P ^ matrices obtainable composing thresholded linearNote| ~respectively. Let Umap). Then,functions elements, restriction function must appliedelement row (but functions may differ rows). slight abuse notation,defineP]fi^_ $~| ~ $aP C. c 2 5CFix. SauersLemma, node first hidden layer feature map computesf 5 functions P9^ input vectors . Thus,^TPQNb . K^TPQN f Kdistinct functions input output first hidden layerP9^ points . Fixing first hidden layerU bparameters, node second layer of. b the5feature map computes ^TPQN K functions image produced outputU =first hidden layer. Thus second hidden layer computes ^TPQN Kfunctions output first hidden layer P9^ points . So, total,b .5 ^TP = . b5^PP]fi^_ f KR U K| ~ , number functions computable row | ~Now, possible matrix=thresholded linear combination output feature map ^_N J K . Hence,c . =5 obtainable applying linear threshold functionsnumber binary sign assignments. Thus,rows ^_N J Kb.5c .5.b5P]fi^_ f ^TKP U ^TKP = P ^TKP =J$q convex function, hence IfiGfi . ,IK U GYKJq J K U K J K U K JRq JIffK U q HGffK qbbU==}KKJfiJ IK U G]KcGU K , G f K P J K showsSubstitutingc .=5U^PKKP]fi^_ K J P K(51)UJ176fiA ODEL NDUCTIVE B IAS L EARNINGHence,^TP J K U KKK(52). PJ> U K P J KP]fi^_% c definition f PQ^ . . , observe . >U KN U K P J K J K U K shows> . Setting ^TP J KU K .(52) satisfied ^ U NP}K J K > J KfUTheorem 16. Let Theorem 15 following extra restrictions: fi , fi Jf.Jf PQ fi U P K J KffProof. bound apply Lemma 10. present setting containsfUthree-layer linear-threshold networks input nodes, hidden nodes first hidden layer, J^Uhidden nodes second hidden layer one output node. Theorem 13 Bartlett (1993),=fi lf U K U J : K3fifrestrictions stated greater U N . Hence fif UfiUN.f :JJ choose feature weight assignment feature mapJidentity J components input vector insensitive setting reminaingcomponents. Hence generate Jpointswhose image feature mapJshattered linear threshold output node,.Kf ] KCombining Theorem 15 Corrolary 13 shows^!fi F > U P K J K K Pexamples task suffice learning P tasks using linear threshold hypothesis space family,combining Theorem 16 Theorem 14 shows^ > U P K J K K Plearning algorithm fail set P tasks.4. Conclusionproblem inductive bias one broad significance machine learning. paperintroduced formal model inductive bias learning applies learner ablesample multiple related tasks. proved provided certain covering numbers computedset hypothesis spaces available bias learner finite, hypothesis spacecontains good solutions sufficiently many training tasks likely contain good solutionsnovel tasks drawn environment.specific case learning set features, showed number examplesJUrequired task -task training set obeys, J numberP^ F K177NPQ^fiBAXTERfeatures U measure complexity feature class. showed boundessentially tight Boolean feature maps constructed linear threshold networks. addition,proved number tasks required ensure good performance features noveltasks U . also showed good set features may found gradientdescent.model paper represents first step towards formal model hierarchical approacheslearning. modelling learners uncertainty concerning environment probabilistic terms,shown learning occur simultaneously base levellearn taskshandand meta-levellearn bias transferred novel tasks. technicalperspective, assumption tasks distributed probabilstically allows performance guarantees proved. practical perspective, many problem domainsviewed probabilistically distributed sets related tasks. example, speech recognitionmay decomposed along many different axes: words, speakers, accents, etc. Face recognitionrepresents potentially infinite domain related tasks. Medical diagnosis prognosis problemsusing pathology tests yet another example. domains benefittackled bias learning approach.Natural avenues enquiry include:EFAlternative constructions . Although widely applicable, specific example featurelearning via gradient descent represents one possible way generating searchinghypothesis space family . would interesting investigate alternative methods,including decision tree approaches, approaches Inductive Logic Programming (Khanet al., 1998), whether general learning techniques boosting appliedbias learning setting.EAlgorithms automatically determining hypothesis space family . modelstructurefixed apriori represents hyper-bias bias learner. wouldinteresting see extent structure also learnt.EEAlgorithms automatically determining task relatedness. ordinary learning usually little doubt whether individual example belongs learning task not.analogous question bias learning whether individual learning task belongsgiven set related tasks, contrast ordinary learning, alwaysclear-cut answer. examples discussed here, speechface recognition, task-relatedness question, cases medicalproblems clear. Grouping large subset tasks together related tasks couldclearly detrimental impact bias-learning multi-task learning, emprical evidence support (Caruana, 1997). Thus, algorithms automatically determiningtask-relatedness potentially useful avenue research. context, see SilverMercer (1996), Thrun OSullivan (1996). Note question task relatednessclearly meaningful relative particular hypothesis space family (for example,possible collections tasks related contains every possible hypothesis space).Extended hierarchies. extension two-level approach arbitrarily deep hierarchies,see Langford (1999). interesting question extent hierarchyinferred data. somewhat related question automatic inductionstructure graphical models.178fiA ODEL NDUCTIVE B IAS L EARNINGAcknowledgementswork supported various times Australian Postgraduate Award, Shell Australia Postgraduate Fellowship, U.K Engineering Physical Sciences Research Council grantsK70366 K70373, Australian Postdoctoral Fellowship. Along way, many peoplecontributed helpful comments suggestions improvement including Martin Anthony,Peter Bartlett, Rich Caruana, John Langford, Stuart Russell, John Shawe-Taylor, Sebastian Thrunseveral anonymous referees.Appendix A. Uniform Convergence ResultsTheorem 2 provides bound (uniform ) probability large deviation?1?p. obtain general result, follow Haussler (1992) introducefollowing parameterized class metrics :8c" _"e. main theorem uniform bound probability large values"1 ?1?p?E"?1. Theorem 2 follow corollary,, rather?E"better bounds realizable case(Appendix A.3).pLemma 17. following three properties3.easily established:E1.2.nLp E,"1 E?"1 _,E," E ?}" ^_p _.ease exposition dealing explicitly hypothesis spacesQ!3 j}containing functions, constructing loss functions Q mapping__A3 j}Q. However, general viewloss function+j}Q function abstract set ()ignore particular constructionterms loss function . remainder section, unless otherwise stated,j}. also considerablyhypothesis spaces sets functions mapping_convenient transpose notation C-samples, writingtraining sets columnsinstead rows:... . .. ...(Equation 9 prior discussion),fiff. Recalling definitiontransposition lives. following definition generalizes quantitieslike,new setting.Definition 6. Letfunctions mapping., let orsetssimplymapdenote??179j}fiBAXTER. Let !"# denote set functions. Given$ % & elements, (or equivalently elementwritingrows), define'(recall equation, define(8)). Similarly, product probability measure ( *) +)-,#.0/ ((recall equation (26)). 21(not necessarily form 345 ),define1 %, . / 1 (, define(recall equation (17)). class functions mapping6 57 98;:=?< > @77supremum product probability measures>size smallest 7 -cover (recall Definition 4).5?"?j}j}following theorem main result rest uniform convergence resultspaper derived.BA CEDF @GJI G( 9) H)GQP+R 6 @G ST VU < G W (53)permissible class functions mappingTheorem 18. Letj}. Letgeneratedindependent trialsaccordingproductprobabilitymeasure.,,K MLN8;:#<p ??''following immediate corollary also use later.YX[Z U]\ G H^ _&` R 6bac gd fe G ihCorollary 19. conditions Theorem 18,!K Lj8;:#<p ?"?'''(54)GP g(55)A.1 Proof Theorem 18proof via double symmetrization argument kind given chapter 2 Pollard (1984).also borrowed ideas proof Theorem 3 Haussler (1992).180fiA ODEL NDUCTIVE B IAS L EARNINGA.1.1 F IRST YMMETRIZATION, let........k .extra piece notation:bottom half, viz:top halfl........l .following lemma first symmetrization trick. relate probability large deviationempirical estimate loss true loss probability large deviationtwo independent empirical estimates loss.mI GLemma 20. Let permissible set functionsj!.probability measureK LN8;:#<poGPN8;:#< rqcFn,p ?? 1'YD K Lp??j}let)ts G P(56)qrq ts G uo Gq G zD0wq uo Gq uo GProof. Note first permissibility guarantees measurability supremap" ?? 1(Lemma 32 part 5). triangle inequality," ?"???,. Thus,]qK &vVo ts GxwK vQ"??""?"?? 1? 1(57)Chebyshevsinequality, fixed ,K vQ ??{I GK L-DF @G? 1gives result.?rG|R??"? 1GD P_. Substituting last expression right hand side (57)181fiBAXTERA.1.2 ECOND YMMETRIZATIONsecond symmetrization trick bounds probability large deviation two empiricalestimates loss (i.e. right hand side (56)) computing probability large deviation elements randomly permuted first second sample. followingdefinition introduces appropriate permutation group purpose.}~~ ~ ~~~ H} , letF l.. . . . ... l ." permissible set functionsLemma 21. Letmappinglet 3W(as statement Theorem 18). FixST -cover , 1 +'1rowsG. Then,K L ~ H} j8:#O < ]q ; ; ts G PK v ~ H} qts GR (58)'~ H} chosen uniformly random.q ; ; ts G (ifProof. Fix ~ } letG ST . Without loss~ already done). Choose3 . Now,generality assume form' ffu fffi ff fffiff' ffu ffff ffff' ffp q ffff ffff' ffu q ffff ffff' ffp q ffff ffff ts' ffu q ffff ffff tsq qDefinition 7. integers, letdenote set permutations>>&sequence pairs integers,, either.j}p"??Qp>j"??>182cp?'?j?j?8'??fiA ODEL NDUCTIVE B IAS L EARNINGpHence, triangle inequality,qq tsq ; ;]qq G | R constructionG |R . Thus,(59) impliesrq tsv ~ H} xwv ~ H} xwp>">?????>c ??c ??'p ??'" ???>>"qq ts(59)ts G assumption,GDq GRy??'"??gives (58).,be function written form{3K Mv ~ b} ]q ts GR YD VU < G(60)~ H} chosen uniformly random.Proof. ~ f} ,q ffff ffff tspffq ts(61)fffiffffu, , letsimplify notation denote fffifffiff . pair ,lff independent random variablefifffffiffprobabilitylfffffiff probability . (61),K Mv ~ H} q GRlfiffYlK ~ H} q ffff ffff ts GR' ffp' ffpfiff +Kfiff GR' ffuffu0 bounded ranges, HoFor zero-mean independent random variableseffdings inequality (Devroye, Gyorfi, & Lugosi, 1996)K \h YD VU < D3'#bound probability term right hand side (58).Lemma 22. Let.>j??j?>?j}Q""??j183'fiBAXTERfiff fifffffiffff ,fiff+ YD VU < GffufiffVK lfiff GRffpfiffff' ffpffuLetffpfiff .fiff ,ffufifffiff . Hence,VU < G' ffu'lff ffulff ff YD VU < GR . Hencegiving valueminimized settingK v ~ f} q ts GR YD VU < GNoting rangej?jeDjjejj"j?required.21 22 give:K L ~ H} j8:#O < ]q ; ; ts G PYD > @G STVU <W G) ;) )empirical distributionNotesimply ((recall Definition 3). Hence,puts point massffN8;:=< q G PK L ~ H}YD 6 @G ST VU <W GNow, random choice ,fiff independently (but identically) distributed ~)ever swapsfiffff (so ~ swapsfiff drawn according ff another componentdrawn according distribution). Thus integrate respect choice~ writej8;:#< q G PK LYD 6 @G ST VU <W GA.1.3 P UTTINGfixedOGETHER, Lemmas"??'G>eee_j"??jp??Applying Lemma 20 expression gives Theorem 18.184jfiA ODEL NDUCTIVE B IAS L EARNINGA.2 Proof Theorem 2) ;)(Another piece notation required proof. hypothesis spacemeasures, let?& o&' ]'=?) ;)K ((probability?another empiricalNote used ? rather ? indicate?1estimate._Calso generated se -sampling process, additionsamplealthough supplied learner.quence probability measures,meansnotion usedfollowing Lemma,nprobability generating sequence measures environmentC _-sample accordingholds.((Lemma 23.K L (1! f N8:#<K L ( 8;:# <! 8;:#<K L" ?! xp"1 ?"??}1G P Dg"1 ?p?1Proof. Follows directly triangle inequalityG P Dg(62)(63)G P g.treat two inequalities Lemma 23 separately.A.2.1 NEQUALITY (62)following Lemma replace supremum8 .Lemma 24.K ML (inequality (62) supremumf j8;:#<QG Pf N8;:#/ <K \ (p ?"?185p ?Ep?''Gh(64)fiBAXTER(-&8;:#<pGjI 7n7?E"?Proof. Suppose. Let satisfy??"?". definition,equality. Suppose first"?"?E"exists 5. Hence property (3)" ?"?E"metric,, exists. Pick arbitrary?????E"satisfying inequality. definition,,' .??(by assumption), compatibilityorderingp1 ?"?'p'reals,, say. triangle inequality ,7uG7G G gG gThus7 g G g 7 7 satisfying inequalityGfound. Choosingshows exists.instead,, identical argument run role (interchanged. Thus cases,8:# <G wGp1 ?p?p1 ?"?'?''Qc" ?p?E"p1 ?"?"1 ?p?'"1 ?"??"8Qp1 ?"?'''completes proof Lemma._nature Csampling process,;8 :#/ <j8;:=/ <, / K \3AYK \ (p ?"?''Gh'p G h ( (65)H permissible assumedpermissibility (Lemma 32, Appendix D). Hencesatisfies conditions Corollary 19G g g Corollarycombining Lemma 24, Equation (65) substituting GGQ??19 gives following Lemma sample size required ensure (62) holds.+X[Z U L G f^ _&` 6 @G g G Pf j8;:#<Lemma 25.K L ()p ?"?'))gGG4+X[Z U L G ^ _&` 6 @G g 4 G PA.2.2 NEQUALITY (63)!G P Dgf )?Note ?, i.e expectationdistributed according . bound left-hand-side (63) apply Corollary19, replaced , replaced , replacedrespectively,replaced replaced . Notepermissible whenever (Lemma 32).Thus,)?186g(66)fiA ODEL NDUCTIVE B IAS L EARNINGinequality (63) satisfied.Now, putting together Lemma 23, Lemma 25 Equation 66, proved followinggeneral version Theorem 2.[I G gYX[Z U L G ^ _&` 6 @G g G P6 @G 0 P[XZULgG ^ _&`G! j8;:#<K LGP g7mget Theorem 2, observe77 |RSetting Gmaximizing Ggives. Substituting Gp_-sample genTheorem 26. Let permissible hypothesis space family let Cnerated environment.,?p1 ?"?1'!??1?7 ED{5.Theorem 26 gives Theorem 2.7GA.3 Realizable Case77G GGGG 0 G 7GJI G g ,Corollary 27. conditions Theorem 26, 7X[Z U L G G 7 ^ _&` 6 G g 7 G G 7 P6 G 7 P[XZULgG G 7 ^ _&`G G 7G! N8;:#<K L7P gGbounds particularly useful know, set GG ).(which maximizes G7Theorem 2 sample complexityscales. improved?"??Epinstead requiring, require ??. see this, observe ?"1 ?"?}1, settingTheorem 26 treating constantgives:_$$_?13 z bcomposition two function classes notewriterform given (32), writtenrRecalling Definition 6,$?pAppendix B. Proof Theorem 6?p$___187__definefiBAXTERbt z . Thus, setting b 0(67)6 .following two Lemmas enable us bound 57Lemma 28. Letform. 7 7,6 57 76 57 6 57=)7Proof. Fix measurelet minimum size -cover.6$)$)57definitionb letmeasuredefined) set in.is6 measurable).~ -algebra( measurable7F57 . LetLet. definition again,669 andsize -cover. Noteminimum57 57 Lemma77shown-cover. So, givenchooseproved1 H 1 7 1 V 1 7 . Now,1 1 1 1 1 11 17 7line followsfirst line follows triangle inequality secondFfacts:11 11 . Thus7 7 -cover 1 1result follows.(Definition 6), following Lemma.Recalling definition {Lemma 29.6 57 M3 6 57. Let) )Proof. Fix product probability measure(/7 -covers . let % . Given %{& , choose 93 o&7 . Now,/.,93''(' o&7-'result follows.Thus 7 -cover 93,j}j}1Lc'CCE:_1j1:!'E188>fiA ODEL NDUCTIVE B IAS L EARNINGB.1 Bounding6 @7j6 7 7 e 6 @7 6 / 7 eLemma 29,6 @7 6 57/ 576Using similar techniques used prove Lemmas 28 29,satisfy6 / 57 6 7Lemma 28,:(68)(69)shown(70)Equations (67), (68), (69) (70) together imply inequality (34).66@7 hypothesis spacewish prove 57xfamily form|f4 . Note f corresponds,) '#, definedprobability measure induces probability measurezz -, ) )~ -algebra. Note also1 bounded, positive functionsarbitrary set ,3 3 1 +8 :#3< 1(71)##. Let fLet probability measure space probability measures. Then,two elements corresponding hypothesis spaces-, '= '# ), 8; :#< ) (by (71) above)) ), , 8:#<8;:#< guaranteedpermissibility (Lemma 32 part 4, ApThe measurability|have,pendix D).f(72)> 57> 7 = M| e^B.2 Bounding/? 1j1? 1? 11? 1__gives inequality (35).189? 1_fiBAXTERB.3 Proof Theorem 7order prove bounds Theorem 7 apply Theorem 6 neural networkhypothesis space family equation (39). case structureGG @G G G0~'boundedsubsetLipschitz squashing function ~ . feature classset one hidden layer neural networks inputs, hidden nodes, outputs, ~squashing function weightsfiff ff bounded subset . Lipschitzrestriction ~ bounded restrictions weights ensure Lipschitzbclasses. Hence exists1 , 11 W 11 1 norm,case. loss function squared loss., hence 1 probability measures) onNow, (recall assumedoutput space),1 -, l1)YD, 1 )(73)))1 -marginal distributionderived . Similarly,)probability measures,1 YD&, 1 )(74)Define6 7 e 98;:#o < > 7 ) esupremum probability measures (the Borel subsets of),))77esizesmallest-covermetric.Similarlyset,>6 7 e 98;:#o < > 7 ) esupremum probability measures . Equations (73) (74) imply6 57 6 7D(75)6 @7 6 7D&(76)Applying Theorem 11 Haussler (1992), find76 7!6 D&7 7 "w_j}m/__j}>___j}m/j}Substituting two expressions (75) (76) applying Theorem 6 yields Theorem7.190fiA ODEL NDUCTIVE B IAS L EARNINGAppendix C. Proof Theorem 14proof follows similar argument one presented Anthony Bartlett (1999)ordinary Boolean function learning.First need technical Lemma.,G# #K #K # # 0 # # %$ G 'R & '( ) +/ *-. , n 1n 0,# denote number occurences random sequence # # # .Proof. Letfunction viewed decision rule, i.e. based observations # , tries guesswhether probabilityBayesDis # WD , and.# optimalrule isD otherwise.# decisionestimator: # #Hence,K # 2$ G K # GK # GKD # Ghalf probability binomialrandom variable least WD .Sluds inequality (Slud, 1977),K # 3$ G KGLemma 30. Let random variable uniformly distributed>. Leti.i.d.-valued random variables G>{. function mapping,1&^"$&"^^"&"nj. Tates inequality (Tate, 1953) statesnormal,K'465 n)1! shattered , . row 7 letsetLet 7)) )distributionscontained98))fifffiffth;: row ofD 7 , f ,8. Let.);)<< achieved sequenceNote (, optimal error _fiff )fiff,always contains sequence shatters 7 . optimal error<_ 6< )3$ ='' ffuCombining last two inequalities completes proof.+>C_>3C1C3>'C_>C_?G51911>C1fiBAXTER,p _ 6< <(77)fiff 3$fiff-sample , let elementfiff array?=....>.....=equal number occurrencesfiff .) ;) uniformly random , generate Now, select (@ (the output learning algorithm) have:sample using ( ,fiff 2$lff C B ) />fiff 2$lff >B ) /> = )fiff 2$fifflff' ffu) probability generating configuration >fiff -sampling/>process sum possible configurations. Lemma 30,)lff 2$fifffiff ER DF 'G ) * /. H , nn JI,?C _'C _fiff 3$fiffKCj}j}1B )CR'&/>()1?@_? = R' ffpM/ L /n . LnKH) * /. , n n IJ,(78), (78)G192DFG1=K *-K, , 0_ <6< G(79). Plugging (77) showsK (C _-valued random variable , Glff 3$fiff GO/ L /n . L()G NR &= K *-K, , n 0Jensens inequality. Sinceimplies:Chence5CGfiA ODEL NDUCTIVE B IAS L EARNING((@(Since inequality holds random choice , must also hold specific choicealgorithmsequence distributions. Hence learningK?@SettingAssuming equality (80), getG 7Ge(80)_ <6< 7 g_g@?eKG gensures_ << G_(81)7gSolving (79) , substituting expressions G shows (81) satisfiedprovided& g7 0 ^ _&` g g(82)gR ( R since G |R G g ), assumingSettingg7 someforYD , (82) becomes(83)^'_&`R right hand side (83) approximately maximizedSubject constraintQPSR & , point value, exceedsCDF D&D 7 . Thus, , 7 gRD&D e(84)7G!C1ej!C1C!CQ>>^<6< 7 g_contains least twogobtain -dependence Theorem 14 observe assumption)UT two distributionsfunctions, hence exists3$. Let))T78concentrated;: 7 . Let ( 9)b) ( -) H) product distributionsV)98 generated ,T @ . Note. ( one (learning algorithmchooses wrong hypothesis ,_ 6< < 7K?@{?_'_193_fiBAXTER((( ( generate/ n_ << 7 NR & ( ) /*X. W W n 0Now, choose uniformly randomcording , Lemma 30 showsK (gleast?@_c1-sampleac-r_JI g |R . Combining two constraintsX[Z U finishes proof.C _7 7 f^ _&` g gprovided(85): (84) (withP ) (85), usingAppendix D. Measurabilityorder Theorems 2 18 hold full generality impose constraint calledpermissibility hypothesis space family . Permissibility introduced Pollard (1984)ordinary hypothesis classes . definition similar Dudleys image admissibleSuslin (Dudley, 1984). extending definition cover hypothesis space families.Throughout section assume functions map (the complete separable metricj}. Let denote Borel -algebra topological space . Sectionspace)2.2, view , set probability measures , topological space equippingtopology weak convergence. -algebra generated topology.following two definitions taken (with minor modifications) Pollard (1984).ff~j}ff~-valued functions indexed setDefinition 8. setrj}ffDefinition 9. set1.ff_ZffQpermissible indexed setffffffexists functionanalytic subset Polish7 space ,~2. function-algebraffj}indexing\[]Y ff .analytic subset ff Polish space ffffmeasurable respect productsimply continuous image Borel subsetanother Polish space . analytic subsets Polish space include Borel sets.important projections analytic sets analytic, measured completemeasure space whereas projections Borel sets necessarily Borel, hence cannotmeasured Borel measure. details see Dudley (1989), section 13.2.Lemma 31.2|j}permissiblepermissible.Proof. Omitted.define permissibility hypothesis space families.7. topological space called Polish metrizable complete separable metric space.194fiA ODEL NDUCTIVE B IAS L EARNINGWDefinition 10. hypothesis space familypermissible exist setsanalytic subsets Polish spaces respectively, functionmeasurable respect,^_[`Y ff \[`Ybafe hgff_}EffZffffdcff,j}@Letanalytic subset Polish space. Letmeasure spacedenote analytic subsets . following three facts analytic sets takenPollard (1984), appendix C.fe hg(a)@(b)ffcomplete@e.e ][ ff .ff , projection onto~contains product -algebra(c) set@4@).Recall Definition 2 definition . following Lemma assume_n_ completecompleted respect probability measure , alsorespect environmental measure .Lemma 32. permissible hypothesis space family ,1. permissible.f permissible.3.permissible.8#:<'# measurable .4.5. measurable.6. permissible.simply setProof. absorbed loss function hypotheses ,"-fold products. Thus (1) follows Lemma 31. (2) (3)2. G///immediate definitions. permissible , (4) provedidentical argument used Measurable Suprema section Pollard (1984), appendixC.j}j}, functiondefined(5), note Borel-measurableBorel measurable Kechris (1995, chapter 17). Now, permissibility,measurableautomatically implies permissibility/(4).rj}appropriate way. prove (6),let indexedj}1_}E_E. Fubinis theoremdefinej}E-measurable function. Letdefined1_}Eindexesappropriate waypermissible, provided.-measurable. analyticity becomes important. Letshown1_}E _E. property (b) analytic sets,.contains1E1Esetprojectiononto, property (c)nalso analytic.assumed complete,measurable, property (a). Thusmeasurable function permissibilityfollows.)kj .0);[m ff ;[fiffqp )'#on ) ) r[fiYGc c ))p)GffWlj .)4195ccf '# f@ff)cfiBAXTERReferencesAbu-Mostafa, Y. (1993). method learning hints. Hanson, S. J., Cowan, J. D., & Giles,C. L. (Eds.), Advances Neural Information Processing Systems 5, pp. 7380 San Mateo,CA. Morgan Kaufmann.Anthony, M., & Bartlett, P. L. (1999). Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge, UK.Bartlett, P. L. (1993). Lower bounds VC-dimension multi-layer threshold networks.Proccedings Sixth ACM Conference Computational Learning Theory, pp. 44150New York. ACM Press. Summary appeared Neural Computation, 5, no. 3.Bartlett, P. L. (1998). sample complexity pattern classification neural networks:size weights important size network. IEEE TransactionsInformation Theory, 44(2), 525536.Baxter, J. (1995a). Learning Internal Representations. Ph.D. thesis, Department Mathematics Statistics, Flinders University South Australia. Copy availablehttp://wwwsyseng.anu.edu.au/ jon/papers/thesis.ps.gz.Baxter, J. (1995b). Learning internal representations. Proceedings Eighth InternationalConference Computational Learning Theory, pp. 311320. ACM Press. Copy availablehttp://wwwsyseng.anu.edu.au/ jon/papers/colt95.ps.gz.Baxter, J. (1997a). Bayesian/information theoretic model learning learn via multiple tasksampling. Machine Learning, 28, 740.Baxter, J. (1997b). canonical distortion measure vector quantization function approximation. Proceedings Fourteenth International Conference Machine Learning,pp. 3947. Morgan Kaufmann.Baxter, J., & Bartlett, P. L. (1998). canonical distortion measure feature space 1-NNclassification. Advances Neural Information Processing Systems 10, pp. 245251. MITPress.Berger, J. O. (1985). Statistical Decision Theory Bayesian Analysis. Springer-Verlag, NewYork.Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1989). Learnability vapnikchervonenkis dimension. Journal ACM, 36, 929965.Caruana, R. (1997). Multitask learning. Machine Learning, 28, 4170.Devroye, L., Gyorfi, L., & Lugosi, G. (1996). Probabilistic Theory Pattern Recognition.Springer, New York.Dudley, R. M. (1984). Course Empirical Processes, Vol. 1097 Lecture Notes Mathematics, pp. 2142. Springer-Verlag.Dudley, R. M. (1989). Real Analysis Probability. Wadsworth & Brooks/Cole, California.196fiA ODEL NDUCTIVE B IAS L EARNINGGelman, A., Carlin, J. B., Stern, H. S., & Rubim, D. B. (Eds.). (1995). Bayesian Data Analysis.Chapman Hall.Good, I. J. (1980). history hierarchical Bayesian methodology. Bernardo, J. M.,Groot, M. H. D., Lindley, D. V., & Smith, A. F. M. (Eds.), Bayesian Statistics II. UniversityPress, Valencia.Haussler, D. (1992). Decision theoretic generalizations pac model neural netlearning applications. Information Computation, 100, 78150.Heskes, T. (1998). Solving huge number similar tasks: combination multi-task learninghierarchical Bayesian approach. Shavlik, J. (Ed.), Proceedings 15th InternationalConference Machine Learning (ICML 98), pp. 233241. Morgan Kaufmann.Intrator, N., & Edelman, S. (1996). make low-dimensional representation suitablediverse tasks. Connection Science, 8.Kechris, A. S. (1995). Classical Descriptive Set Theory. Springer-Verlag, New York.Khan, K., Muggleton, S., & Parson, R. (1998). Repeat learning using predicate invention. Page,C. D. (Ed.), Proceedings 8th International Workshop Inductive Logic Programming(ILP-98), LNAI 1446, pp. 65174. Springer-Verlag.Langford, J. C. (1999). Staged learning. Tech. rep., CMU, School Computer Science.http://www.cs.cmu.edu/ jcl/research/ltol/staged latest.ps.Mitchell, T. M. (1991). need biases learning generalisations. Dietterich, T. G., &Shavlik, J. (Eds.), Readings Machine Learning. Morgan Kaufmann.Parthasarathy, K. R. (1967). Probabiliity Measures Metric Spaces. Academic Press, London.Pollard, D. (1984). Convergence Stochastic Processes. Springer-Verlag, New York.Pratt, L. Y. (1992). Discriminability-based transfer neural networks. Hanson, S. J.,Cowan, J. D., & Giles, C. L. (Eds.), Advances Neural Information Processing Systems 5,pp. 204211. Morgan Kaufmann.Rendell, L., Seshu, R., & Tcheng, D. (1987). Layered concept learning dynamically-variablebias management. Proceedings Tenth International Joint Conference ArtificialIntelligence (IJCAI 87), pp. 308314. IJCAI , Inc.Ring, M. B. (1995). Continual Learning Reinforcement Environments. R. Oldenbourg Verlag.Russell, S. (1989). Use Knowledge Analogy Induction. Morgan Kaufmann.Sauer, N. (1972). density families sets. Journal Combinatorial Theory A, 13,145168.Sharkey, N. E., & Sharkey, A. J. C. (1993). Adaptive generalisation transfer knowledge.Artificial Intelligence Review, 7, 313328.197fiBAXTERSilver, D. L., & Mercer, R. E. (1996). parallel transfer task knowledge using dynamiclearning rates based measure relatedness. Connection Science, 8, 277294.Singh, S. (1992). Transfer learning composing solutions elemental sequential tasks. Machine Learning, 8, 323339.Slud, E. (1977). Distribution inequalities binomial law. Annals Probability, 4, 404412.Suddarth, S. C., & Holden, A. D. C. (1991). Symolic-neural systems use hints developing complex systems. International Journal Man-Machine Studies, 35, 291311.Suddarth, S. C., & Kergosien, Y. L. (1990). Rule-injection hints means improving network performance learning time. Proceedings EURASIP Workshop NeuralNetworks Portugal. EURASIP.Sutton, R. (1992). Adapting bias gradient descent: incremental version delta-bar-delta.Proceedings Tenth National Conference Artificial Intelligence, pp. 171176. MITPress.Tate, R. F. (1953). double inequality normal distribution. Annals MathematicalStatistics, 24, 132134.Thrun, S. (1996). learning n-th thing easier learning first?. Advances NeuralInformation Processing Systems 8, pp. 640646. MIT Press.Thrun, S., & Mitchell, T. M. (1995). Learning one thing. Proceedings InternationalJoint Conference Artificial Intelligence, pp. 12171223. Morgan Kaufmann.Thrun, S., & OSullivan, J. (1996). Discovering structure multiple learning tasks: TC algorithm. Saitta, L. (Ed.), Proceedings 13th International Conference MachineLearning (ICML 96), pp. 489497. Morgen Kaufmann.Thrun, S., & Pratt, L. (Eds.). (1997). Learning Learn. Kluwer Academic.Thrun, S., & Schwartz, A. (1995). Finding structure reinforcement learning. Tesauro, G.,Touretzky, D., & Leen, T. (Eds.), Advances Neural Information Processing Systems, Vol. 7,pp. 385392. MIT Press.Utgoff, P. E. (1986). Shift bias inductive concept learning. Machine Learning: ArtificialIntelligence Approach, pp. 107147. Morgan Kaufmann.Valiant, L. G. (1984). theory learnable. Comm. ACM, 27, 11341142.Vapnik, V. N. (1982). Estimation Dependences Based Empirical Data. Springer-Verlag, NewYork.Vapnik, V. N. (1996). Nature Statistical Learning Theory. Springer Verlag, New York.198fiJournal Artificial Intelligence Research 12 (2000) 339{386Submitted 12/99; published 6/00Reasonable Forced Goal Orderings UseAgenda-Driven Planning Algorithmjana koehler@ch.schindler.comJana KoehlerSchindler Lifts, Ltd.R & Technology Management6031 Ebikon, SwitzerlandJorg HoffmannInstitute Computer ScienceAlbert Ludwigs UniversityGeorges-Kohler-Allee, Geb. 5279110 Freiburg, Germanyhoffmann@informatik.uni-freiburg.deAbstractpaper addresses problem computing goal orderings, onelongstanding issues AI planning. makes two new contributions. First, formallydefines discusses two different goal orderings, called reasonableforced ordering. orderings defined simple STRIPS operators wellcomplex ADL operators supporting negation conditional effects. complexityorderings investigated practical relevance discussed. Secondly, twodifferent methods compute reasonable goal orderings developed. Onebased planning graphs, investigates set actions directly. Finally,shown ordering relations, derived given set goalsG , used compute so-called goal agenda divides G ordered setsubgoals. planner then, principle, use goal agenda plan increasingsets subgoals. lead exponential complexity reduction, solutioncomplex planning problem found solving easier subproblems. Since polynomialoverhead caused goal agenda computation, potential exists dramatically speedplanning algorithms demonstrate empirical evaluation, usemethod IPP planner.1. Introductioneffectively plan interdependent subgoals focus AI planningresearch long time. Starting early work ABSTRIPS (Sacerdoti, 1974)conjunctive-goal planning problems (Chapman, 1987), quite number approachespresented complexity problems studied. today,planners made progress solving bigger planning instances scalabilityclassical planning systems still problem.paper, focus following problem: Given set conjunctive goals,define detect ordering relation subsets original goal set? arriveordering relation subsets, first focus atomic facts containedgoal set. formally define two closely related ordering relations atomic goals,c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiKoehler & Hoffmanncall reasonable forced ordering, study complexity. turnshard decide.Consequently, introduce two ecient methods used approximatereasonable goal orderings. definitions first given simple STRIPS domains,desired theoretical properties easily proven. Afterwards, extend definitionsADL operators (Pednault, 1989) handling conditional effects negative preconditions,discuss invest effort trying find forced orderings.show set ordering relations atomic goals used dividegoal set disjunct subsets, subsets ordered respectother. resulting sequence subsets comprises so-called goal agenda,used control agenda-driven planning algorithm.method, called Goal Agenda Manager, implemented context IPPplanning system, show potential exponentially reducing computation timescertain planning domains.paper organized follows: Section 2 introduces motivates reasonableforced goal orderings. Starting simple STRIPS operators, formally defined,complexity investigated. Section 3, present two methods, compute approximation reasonable ordering discuss orderingspractical point view. section concludes extension definitions ADLoperators conditional effects. Section 4 shows planning system benefitordering information computing goal agenda guides planner. definesubsets goals ordered respect discuss goalagenda affect theoretical properties, particular completeness planningalgorithm. Section 5 contains empirical evaluation work, showing resultsobtained using goal agenda IPP. Section 6 summarize approach lightrelated work. paper concludes outlook possible future research directionsSection 7.2. Ordering Relations Atomic Goalsstart, investigate simple STRIPS domains allowing sets atomsdescribe states, preconditions, add delete lists operators.Definition 1 (State) set ground atoms denoted P . state 2 2Psubset ground atoms.Note states assumed complete, i.e., always know atom p whetherp 2 p 62 holds. also assume operator schemata ground, i.e.,talk actions.Definition 2 (Strips Action) STRIPS action usual formpre(o) ! ADD add(o) DEL del(o)pre(o) preconditions o, add(o) Add list del(o) Deletelist action, set ground atoms. also assume del(o) \ add(o) = ;.result applying STRIPS action state defined usual:340fiOn Reasonable Forced Goal OrderingsResult(s; o) :=(s [ add(o)) n del(o) pre(o)otherwisepre(o) holds, action said applicable s. result applyingsequence one action state recursively definedResult(s; ho1 ; : : : ;i) := Result(Result(s; ho1 ; : : : ; 1 i); ):Definition 3 (Planning Problem) planning problem (O; ; G ) tripleset actions, (the initial state) G (the goals) sets ground atoms.plan P ordered sequence actions. actions plan taken certainaction set O, denote writing P .nnnNote define plan sequence actions, sequence parallel steps,done graphplan (Blum & Furst, 1997), example. makes subsequenttheoretical investigation readable. results directly carry parallel plans.Given two atomic goals B , various ways define ordering relationimagined. First, one distinguish domain-specific domainindependent goal ordering relations. although domain-specific orderingseffective, need redeveloped single domain. Therefore, one particularinterested domain-independent ordering relations broader range applicability.Secondly, following Hullem et al. (1999), one distinguish goal selection goalachievement order. first ordering determines order planner worksvarious atomic goals, second one determines order, solutionplan achieves goals. paper, compute ordering latter type.agenda-driven planning approach propose later paper, orderingscoincide anyway. goals achieved first plan plannerworks first.following scenario motivates achievement order goals possiblydefined. Given two atomic goals B , solution plan exists, let us assumeplanner achieved goal A, i.e., arrived state s( : ) ,holds, B hold yet. Now, exists plan executable s( : )achieves B without ever deleting A, solution found. planfound, two possible reasons exist:1. problem unsolvable|achieving first leads planner deadlock situation. Thus, planner forced achieve B simultaneously A.2. existing solution plans destroy temporarily order achieve B .then, achieved first. Instead, seems reasonable achieveB simultaneously sake shorter solution plans.first situation, ordering \B simultaneously A" forced inherent properties planning domain. second situation, ordering \Bsimultaneously A" appears reasonable order avoid non-optimal plans. Consequently, define two goal orderings, called forced reasonable ordering.sake clarity, first give basic definitions.A;BA;341BfiKoehler & HoffmannDefinition 4 (Reachable State) Let (O; ; G ) planning problem let Pset ground atoms occur problem. say state P reachable, iffexists sequence ho1 ; : : : ; actions = Result(I ; ho1 ; : : : ; i)holds.nnDefinition 5 (Generic State s( : ) ) Let (O; ; G ) planning problem. s( :A;BA;B)denote reachable state achieved, B false,i.e., B 62 s( : ) sequence actions ho1 ; : : : ; s( : ) =Result(I ; ho1 ; : : : ; i), 2 add(o ).A;nBnA;BnOne imagine s( : ) state incomplete information.states represents satisfy j= A; :B , atoms p 2 P p 6= A; Badopt arbitrary truth values.Definition 6 (Reduced Action Set ) Let (O; ; G ) planning problem, letA;B2 G atomic goal. denote set actions delete A,i.e., = fo 2 j 62 del(o)g.prepared define exactly mean forced reasonable goal orderings.Definition 7 (Forced Ordering ) Let (O; ; G ) planning problem, let A; B 2G two atomic goals. say forced ordering B A, writtenfB A,f: :9 P : B 2 Result(s( : ) ; P )Definition 7 satisfied, plan achieving B must achieve Bsimultaneously A, otherwise encounter deadlock, renderingproblem unsolvable.Definition 8 (Reasonable Ordering ) Let (O; ; G ) planning problem, letA; B 2 G two atomic goals. say reasonable ordering B8 s(:A;B)A;BrA, written B A,r8 s( : ) : :9 P OA : B 2 Result(s( : ) ; P OA )Definition 8 gives B meaning if, goal achieved,A;BA;Brplan anymore achieves B without|at least temporarily|destroying A, Bgoal prior A.remark obviously B implies B A, vice versa. also makeslightly less obvious observation point: formulae Definitions 7 8 useuniversal quantification states s( : ) . planning problemstate all, formulae satisfied goals B get ordered, i.e., BB follow, respectively. case, however, much information gainedgoal ordering B , sequence actions achieve B priorsimultaneously A|A cannot achieved B still false. Thuscase, ordering relations B B trivial sense reasonableplanner would invest much effort considering goals B ordered wayround anyway.rfA;Bfrfr342fiOn Reasonable Forced Goal OrderingsDefinition 9 (Trivial Ordering Relation) Let (O; ; G ) planning problem, letA; B 2 G two atomic goals. ordering relation Bstate s( : ) .A;fB calledrtrivial iffBpaper, usually consider forced reasonable goal orderings non-trivialorderings make distinction explicit so.Definitions 7 8 seem deliver promising candidates achievement order.Unfortunately, hard test: turns corresponding decisionproblems PSPACE hard.Theorem 1 Let F ORDER denote following problem:Given two atomic facts B , well action set initial state ,B hold ?fDeciding F ORDER PSPACE-hard.Proof: proof proceeds polynomially reducingPLANSAT (Bylander, 1994)|thedecision problem whether exists solution plan given arbitrary STRIPSplanning instance|to problem deciding F ORDER.Let , G , denote initial state, goal state, action set arbitrarySTRIPS instance. Let A, B , C new atomic facts contained instancefar. build new action set initial state F ORDER instance setting8<O0 := [ :o1o2G9! ADD fAg DEL fC g; =! ADD DEL fAg; ;! ADD fB g DEL ;= fC g= fAg=G0 := fC gdefinitions, reaching B equivalent solving original problem.way round, unreachability B A|forced ordering B A|is equivalentunsolvability original problem. order prove this, consider following:way achieving applying 1 0 . Consequently, state s( : )fAg, cf. Definition 5. Thus starting assumption B valid, applyfollowing equivalences:fA;BfBf,,,,8 s( : ) : :9 P O0 : B 2 Result(s( : ) ; P O0 )cf. Definition 700:9 P : B 2 Result(fAg; P )fAg reachable state s( : ):9 P : G Result(I ; P )definition 0solution plan exists ; G givenA;BA;BA;343BfiKoehler & HoffmannThus, complement PLANSAT polynomially reduced F ORDER. PSPACE= co-PSPACE, done.Theorem 2 Let R ORDER denote following problem:Given two atomic facts B , well action set initial state ,B hold ?Deciding R ORDER PSPACE-hard.rProof: proof proceeds polynomially reducing PLANSAT R ORDER.Let , G , initial state, goal state, action set arbitrarySTRIPS planning instance. Let A, B , C , new atomic facts containedinstance far. define new action set O0 setting891 = fC g! ADD fA; Dg DEL fC g; =<O0 := [ : 2 = fA; Dg ! ADD DEL fDg;;=G! ADD fB g DEL ;Gnew initial state0 := fC gproof Theorem 1, intention behind definitions make solvabilityoriginal problem equivalent reachability B A. reasonable orderings,reachability concerned actions delete A, need safetycondition D.Precisely, way achieve applying 1 0, i.e., per Definition 5state s( : ) fA; Dg. action new operator set O0 deletes A,following sequence equivalences.A;BB8 s( : ) :9 P OA0 : B 2 Result(s( : ); P OA0 ):9 P OA0 : B 2 Result(fA; Dg; P OA0 )r,,, :9 P O0 : B 2 Result(fA; Dg; P O0 ), :9 P G Result(I ; P ), solution plan exists ; G ;A;BA;Bcf. DefinitionfA; Dg reachable state s( : )action 0 deletesdefinition 0A;BThus, complement PLANSAT polynomially reduced R ORDER.PSPACE = co-PSPACE, done.Consequently, finding reasonable forced ordering relations atomic goalsalready hard original planning problem appears unlikely plannergain advantage that. possible way dilemma define newordering relations, decided polynomial time are, ideally, sucientexistence reasonable forced goal orderings. following, introduce twoorderings.3448fiOn Reasonable Forced Goal Orderings3. Computation Goal Orderingssection,1. define goal ordering , computed using graphplan's exclusivityinformation facts. prove ordering sucientdecided polynomial time (the subscript \e" stands \ecient").2. define goal ordering , computed based heuristic methodmuch faster computation based graphplan, also delivers powerfulgoal ordering information (the subscript \h" stands \heuristic").3. discuss currently available benchmark planning domains contain forced orderings, i.e., fail providing problem decomposition them.4. show orderings extended handle expressive ADL operators.erhf3.1 Reasonable Goal Orderings based graphplangoal ordering always computed specific planning problem involving initialstate , goal set G fA; B g, set ground actions. order developecient computational method, proceed two steps now:1. compute knowledge generic state s( : ) .2. define relation investigate theoretical properties. particular,prove implies .A;Beerstate s( : ) represents states reachable ,achieved, B hold. Given information s( : ) , one deriveadditional knowledge it. particular, possible determine subset atoms F,one definitely knows F \ s( : ) = ; must hold. One method determine Fobtained via computation invariants, i.e., logical formulae hold reachablestates, cf. (Fox & Long, 1998). determined invariants, one assumesholds, B not, computes logical implications. Another possibilitysimply use graphplan (Blum & Furst, 1997). Starting O, planning graphbuilt graph leveled time step. proposition level timestep represents set states, superset states reachableapplying actions O. atoms, marked mutually exclusive (Blum& Furst, 1997) level never hold state satisfying A. Thus, cannothold s( : ) . denote set F |the False set respect returned1graphplan.F:= fp j p exclusive graph leveled offg(1)Note planning graph grown given O, useddetermine F sets atomic goals 2 G .A;BA;A;A;BBGPBGPGP1. assume reader familiar graphplan, planning system well knownplanning research community. Otherwise, (Blum & Furst, 1997) provide necessary background.345fiKoehler & Hoffmann\ = ; holds statesusing actions O.Lemma 1FGPsatisfying 2 reachableproof follows immediately definitions \level-off" \two propositionsmutual exclusive" given (Blum & Furst, 1997).provide simple test sucient existence reasonable orderingB two atomic goals B .Definition 10 (Ecient Ordering ) Let (O; ; G fA; B g) planning problem.reLet FGPFalse set A. ordering B holdse8 2 : B 2 add(o) ) pre(o) \ F 6= ;GPmeans, B ordered reduced action set contains actions,either B add lists do, require preconditioncontained False set. preconditions never hold state satisfyingthus, actions never applicable.Theorem 3B A)BerProof: Assume B 6 A, i.e., B 2 Result(s( : ) ; P OA ) reachable state s( :rA;BA;B)2 s( : ) , B 62 s( : ) , Plan P OA = ho1 ; : : : ; 2 1 n.62 del(o ) (Definition 6),2 Result(s( : ) ; ho1 ; : : : ; i) 0 nand, Lemma 1,F\ Result(s( : ) ; ho1 ; : : : ; i) = ; 0 n(2)Furthermore, B 62 s( : ) , B 2 Result(s( : ) ; ho1 ; : : : ; i), muststep makes B true, i.e.,91 k n : B 62 Result(s( : ); ho1 ; : : : ; 1i) ^ B 2 Result(s( : ); ho1 ; : : : ; i)step, obviously B 2 add(o ) consequently, definitionB A, pre(o ) \ F 6= ;. Now, must applicable stateexecuted (otherwise would add anything state), preconditionsmust hold, i.e., pre(o ) Result(s( : ) ; ho1 ; : : : ; 1 i). immediately leads F \Result(s( : ) ; ho1 ; : : : ; 1 i) 6= ;, contradiction Equation (2).Quite obviously, ordering decided polynomial time.A;BA;nBA;BA;GPA;BBA;A;nBkBA;BkkeGPkkkkA;BA;GPkBkeTheorem 4 Let E ORDER denote following problem:Given two atomic facts B , well initial state action set O,B hold ?eThen, E ORDER decided polynomial time:346E ORDER2 P.fiOn Reasonable Forced Goal OrderingsProof: begin with, need show computing Ftakes polynomial time.results (Blum & Furst, 1997), follows directly building planning graphpolynomial jIj, jOj, l t, l maximal length precondition, adddelete list action, number time steps built. Taking l parameterinput size, remains show planning graph levels polynomialnumber time steps. Now, planning graph leveled time steps+ 1 neither set facts number exclusion relations change.two subsequent time steps, set facts increase|facts already occuringgraph remain there|and number exclusions decrease|non-exclusive factsnon-exclusive subsequent layers. Thus, maximal number time stepsbuilt graph leveled dominated maximal number changesoccur two subsequent layers, dominated maximal numberfacts plus maximal number exclusion relations. maximal number factsO(jIj + jOj l), maximal number exclusions O((jIj + jOj l)2 ), squaremaximal number facts.computed F polynomial time, testing B involves looking actionsO, rejecting eitherdelete A, decidable time O(l),precondition, element F , decidable time O(l (jIj + jOj l)).Thus additional runtime test, O(jOj l (jIj + jOj l)).GPGPeGPLet us consider following example, illustrates computation usingcommon representational variant blocks world actions stack, unstack,pickup, putdown blocks:epickup(?ob)clear(?ob) on-table(?ob) arm-empty()! ADD holding(?ob)DEL clear(?ob) on-table(?ob) arm-empty().putdown(?ob)holding(?ob)! ADD clear(?ob) arm-empty() on-table(?ob)DEL holding(?ob).stack(?ob,?underob)clear(?underob) holding(?ob)unstack(?ob,?underob)! ADD arm-empty() clear(?ob) on(?ob,?underob)DEL clear(?underob) holding(?ob).on(?ob,?underob) clear(?ob) arm-empty()! ADD holding(?ob) clear(?underob)DEL on(?ob,?underob) clear(?ob) arm-empty().Given simple task stacking three blocks:initial state: on-table(a) on-table(b) on-table(c)goal state: on(a,b) on(b,c)347fiKoehler & Hoffmannreasonable ordering two atomic goals? Intuitively, blocks worlddomain possesses natural goal ordering, namely planner start buildingtower bottom top way round.2Let us first investigate whether relation on(a; b) on(b; c) holds. Vividly speaking,asks whether still possible stack block b on(b; c) achieved.first step, run graphplan find atoms exclusive on(b; c)planning graph, corresponds problem, leveled off. resulte()b;cFGP= fclear(c), on-table(b), holding(c), holding(b), on(a,c), on(c,b), on(b,a)gOne observes immediately atoms never true state satisfieson(b; c).Secondly, remove ground actions delete on(b; c) (in case, action( ).ready test on(a; b) on(b; c) holds. action, addon(a; b) stack(a,b). preconditions holding(a) clear(b), neithermember F ( ) . test fails get on(a; b) 6 on(b; c).next step, test whether on(b; c) on(a; b) holds. graphplan returnsfollowing False set:unstack(b,c) satisfies condition) obtain reduced action setb;ceb;ceGPe()a;bFGP= fclear(b), on-table(a), holding(b), holding(a), on(a,c), on(c,b), on(b,a)gaction unstack(a,b) contained ( ) deletes on(a; b).action adds on(b; c) stack(b,c). needs preconditions clear(c)holding(b). second precondition holding(b) contained set false facts,i.e., holding(b) 2 F ( ) thus, conclude on(b; c) on(a; b). Altogether,on(a; b) 6 on(b; c) on(b; c) on(a; b), correctly ects intuition bneeds stacked onto c stacked onto b.Although appears impose strict conditions domain order derivereasonable goal ordering, succeeds finding reasonable goal orderings available testdomains orderings exists. example, tyreworld, bulldozer problems,shopping problem (Russel & Norvig, 1995), fridgeworld, glass domain,tower hanoi domain, link-world, woodshop. disadvantagecomputational resources requires, since building planning graphs, theoreticallypolynomial, quite time- memory-consuming thing do.3Therefore, next section presents fast heuristic computation goal orderings,analyzes domain actions directly need build planning graphs anymore.a;ba;beGPeee2. Note goals specify block c go, leave planner.3. recent implementations planning graphs, example developed STAN (Fox &Long, 1999) IPP 4.0 (Koehler, 1999) build graphs explicitly anymore ordersmagnitude faster original graphplan implementation, still computation planninggraph takes almost time needed determine e relations.348fiOn Reasonable Forced Goal Orderings3.2 Reasonable Goal Orderings derived Fast Heuristic MethodOne analyze available actions directly using method call Direct Analysis(DA). determines initial value F computing intersection delete listsactions contain add list, defined following equation.FDA\:=2O 2;( )del(o)(3)addatoms set false state achieved:deleted state description independently action used add A.short example, let us consider two actions! ADD fAg DEL fC; Dg! ADD fA; C g DEL fDgatom deleted actions, thus element initiallycontained F .However, Equation (3) says added atoms Fdeleted. say anything whether might possible reestablish atomsF . One easily imagine actions exist, leave true,time add atoms. case, reachable states atomsF hold.Now, goal derive ordering relation easily computed,ideally, like relation, sucient relation. Therefore, want makesure atoms F really false state achieved.arrive approximation atoms remain false performing fixpoint reductionF set, removing atoms achievable following sense.DADADADAerDADADefinition 11 (Achievable Atoms) atom p achievable state givenaction set (written A(s; p; O))p2s_ 9 2 : p 2 add(o) ^ 8 p0 2 pre(o) : A(s; p0; O)definition says atom p achievable state holds s,exists action domain, adds p whose preconditions achievables. necessary condition existence plan P statep holds.Lemma 2 9 P : p 2 Result(s; P ) ) A(s; p; O)Proof: atom p must either already contained state s, addedstep P . second case, preconditions need establishedP way. Thus p preconditions step, adds it, achievablesense Definition 11.349fiKoehler & Hoffmanntwo obvious diculties Definition 11: First, p 2 must tested.complete knowledge state s, cause problems. case,however, generic state s( : ) cannot decide whether arbitraryatom contained not. Secondly, observe infinite regression preconditions,must tested achievability.first problem, turns good heuristic simply assume p 62 s,i.e., test performed all. second problem, order avoid infinitelooping \achievable"-test, one needs terminate regression preconditionsparticular level. point question far regress? quick approximationsimply decides \achievable" first recursive call.A;BDefinition 12 (Possibly Achievable Atoms) atom p possibly achievable givenaction set (written pA(p; O))9 2 : p 2 add(o) ^ 8 p0 2 pre(o) :9 o0 2 : p0 2 add(o0 )holds, i.e., action adds p preconditions add effectsactions O.assumption justified none atoms p contained state s,possibly achievable necessary condition achievable.Lemma 3 Let state p 62 also 8o 2 : p 2 add(o) ) pre(o) \ = ;holds.A(s; p; O) ) pA(p; O)Proof: A(s; p; O) p 62 s, know step 2 O, p 2 add(o),8 p0 2 pre(o) A(s; p0; O). also know pre(o) \ = ;, p0 2 pre(o)must achiever o0 2 : p0 2 add(o0 ).condition facts p must contained state seemsrather rigid. Nevertheless, condition possibly achievable delivers good resultsbenchmark domains easy decide. use testperform fixpoint reduction set Fdecide whether atomic goal B ordered A.DAfixpoint reduction, depicted Figure 1 below, uses approximative test pA(f; )remove facts F achieved. finds facts certainrestrictions, see below. side effect fixpoint algorithm, obtain setactions method assumes applicable state s( : ) . order Biff cannot possibly achieved using actions.DAA;350BfiOn Reasonable Forced Goal Orderings:= F:= n fo j F \ pre(o) 6= ;gfixpoint reached := false:fixpoint reachedfixpoint reached := truef 2 FpA(f; )F := F n ff g:= n fo j F \ pre(o) 6= ;gfixpoint reached := falseFDAendifendforendwhilereturn F ,Figure 1: Quick, heuristic fixpoint reduction set F .DAcomputation checks whether atoms F , initially set F , possiblyachievable using actions, delete require atomsF precondition. Achievable atoms removed F , gets updatedaccordingly. one iteration, F change, fixpoint reached, i.e., Fdecrease increase|the final sets F false factsapplicable actions returned.Let us illustrate fixpoint computation short example consisting emptyinitial state, goals fA; B g, following set actionsDAop1:op2:op3: f C gop4: f g!!!!ADD f gADD fA, C gADD f gADD f B gDEL f C, gDEL f gassuming achieved, obtain F = F = fDg initialvalue False set, since atom op1 op2 delete adding A.Figure 2 illustrates hypothetical planning process. Starting empty initial statetrying achieve first, get two different states s( : ) holds.atom hold thus states, action applicablerequires precondition. excludes op4 , yielding initial action set= fop1; op2; op3g. Now, op4 action add B . Therefore, usedaction set see B still achieved, would find case.Consequently, without performing fixpoint computation, would order B A.seen Figure 2, would reasonable ordering: planhop3 ;op4i achieves B state s( : ) = Result(I ;op2) without destroying A.fixpoint computation works us around problem follows: action op3, add precondition op4 without deleting A. checkingpA(D; ) first iteration, fixpoint procedure finds action. checksDAA;A;351BBfiKoehler & Hoffmannwhether preconditions op3 achievable sense added another action. case since precondition C added op2. Thus,removed F , becomes empty now. action op4 put back set ,becomes identical action set . set, turn, identicaloriginal action set action deletes A. fixpoint process terminates Bordered achieved using action op4. correctly ectsfact exists plan state s( : ) = Result(I ; hop2i) = fC; Agstate satisfies B without destroying A.A;B0/Deadlockop1op2C,op3C, A,holds state satisfyingop4C, A, D, Bplan BFigure 2: example illustrating need fixpoint computation.already pointed out, intention behind fixpoint procedure following:Starting state s( : ) , want know facts become true withoutdestroying A, consequently, actions become applicable. first step,actions use facts F applicable, factsdeleted state description added. However, actions may make factsF true, want remove facts F . manage find factsmade true without destroying A, final set F containfacts hold state reachable s( : ) without destroying A. case,final action set contain actions applied s( : ) ,safely use action set determine whether another goal B still achievednot.However, use approximative test pA(f; O) f 2 F findfact current F set achievable, may facts achievable withoutdestroying A, remain set F . could exclude actions setsafely applied s( : ). certain restrictions, however,prove happen. order so, need impose restrictionparticular state s( : ) , achieved goal A: none preconditionsactions, add facts contained F , occur state s( : ) , fixpointprocedure remove facts F achievable without destroying A.use property fixpoint procedure later show heuristic ordering relationapproximates reasonable orderings.A;BDADADAA;BA;A;A;BBDAA;DA352BBfiOn Reasonable Forced Goal OrderingsLemma 4 Let (O; ; G ) planning problem, let 2 G atomic goal. Lets( : ) reachable state achieved. Let P OA = ho1 ; : : : ;sequence actions destroying A. Let F set facts returnedfixpoint computation depicted Figure 1.A;nB8f 2 FDA: 8o 2 : f 2 add(o) ) pre(o) \ s( : ) = ;A;()Bfact F holds state reached applying P OA , i.e.,Result(s( : ) ; P OA ) \ F = ;A;BProof:Let F denote state fact action sets, respectively, j iterationsalgorithm depicted Figure 1. F decreases computation,F F j . Let s0 ; : : : ; denote sequence states encounteredexecuting P OA = ho1 ; : : : ; s( : ) , i.e., s0 = s( : ) = Result(s 1 ; ho i)0 n. assume action applicable state 1 , i.e., pre(o ) 1 .Otherwise, cause state transition, skip P OA . Obviously,= Result(s( : ) ; P OA ), need show \ F = ;. proof proceedsinduction length n P OA .n = 0: P OA = hi = s0 = s( : ) . facts F deleted statedescription added, \ F = ;. F = F0 F F0 ,proposition follows immediately.n ! n + 1: P OA = ho1 ; : : : ; ; +1 i. induction hypothesis, know\ F = ; 0 n. need show +1 \SF = ;.Let j step fixpoint iteration F \ =0 becomes empty, i.e., jdenotes iteration intersection states ; n F emptyfirst time. iteration exists, intersections \ F nempty.action ; 1 n + 1 applicable state 1 , i.e., pre(o ) 1 ,thus pre(o ) \ F = ; actions P OA . Therefore, actions contained, set contains actions whose intersection F empty.Let us focus facts state +1 . facts achieved executing P OAs( : ) . words, plan s( : ) facts.seen, plan consists actions . Applying Lemma 2 facts p 2 +1using s( : ) P OA (= P Oj ), know facts p achievable using actions.jjnjnA;BA;BnA;nBnA;DABnnDADAnnj;:::;njjjjnA;BA;BnjA;Bj: A(s( : ) ; p; )show facts f 2 +1 interested in, namely F factsadded +1 still contained F , also possibly achievable using actions. Let f fact f 2 +1 , f 2 F . apply Lemma 3 using s( : ) , f ,8p 2+1nA;Bjnnjjnj353A;BfiKoehler & HoffmannO. apply Lemma 3 obviously f 62 s( : ) , 8o 2 : f 2 add(o) )pre(o) \ s( : ) = ; prerequisite (). A(s( : ) ; p; ), arrive8f 2 +1 \ F : pA(f; O)A;jA;BA;nBjBjjjremains proven facts f removed Ffixpoint computation. argumentation above, sucient showfacts f 2 +1 \ F get tested pA(f; ) iteration j +1 fixpoint computation.tests succeed lead +1 \ F+1 = ;, yielding, desired, +1 \ F = ;.Remember F+1 F . two cases, need consider:1. j = 0: intersections \ F0 initially empty, i.e., \ F = ; 0 n.case, facts f 2 +1 \ F tested pA(f; O0 ) iteration j + 1 = 1fixpoint computation.2. j > 0: case, least one intersections \ F became empty iterationj definition j , i.e., least one fact removed F iteration.Therefore, fixpoint reached yet, computation performsleast one iteration, namely iteration j + 1. facts F testediteration, particular facts f 2 +1 \ F .observations, induction complete proposition proven.already said, simply order B A, possibly achievableusing action set resulted fixpoint computation. ordering relation(where h stands \heuristic") obtained way approximates reasonable goalordering .Definition 13 (Heuristic Ordering ) Let (O; ; G fA; B g) planning problem.njjnnjjnDADAjjnjhrLet set actions obtained performing fixpoint computationshown Figure 1.ordering B holds:pA(B; O)hhreached particular state s( : ) assumptions madefixpoint computation test pA(B; ) justified, possibly achievable sucient condition non-existence plan Btemporarily destroy A.Theorem 5 Let (O; ; G ) planning problem, let A; B 2 G two atomic goals. LetA;Bs( : ) reachable state achieved, B still false, i.e., B 62s( : ) . Let F sets facts actions, respectively, derivedfixpoint computation shown Figure 1.A;BA;B8f 2 F [ fB g : 8o 2 : f 2 add(o) ) pre(o) \ s(DA:pA(B; O) ) :9P OA ::A;BB 2 Result(s( : ) ; P OA )354A;B)=;()fiOn Reasonable Forced Goal OrderingsProof: Assume plan P OA = ho1 ; : : : ; destroy A,nachieves B , i.e., B 2 Result(s( : ) ; ho1 ; : : : ; i). restriction ()facts F , Lemma 4 applied action sequence ho1 ; : : : ; 1 yieldingResult(s( : ) ; ho1 ; : : : ; 1 i) \ F = ;. Consequently, eitherapplicable Result(s( : ) ; ho1 ; : : : ; 1i),preconditions contained Result(s( : ); ho1 ; : : : ; 1 i), yielding pre(o ) \F = ;.first case, simply skip effects. second case,2 follows. Thus, plan constructed actions achieves Bs( : ) . Applying Lemma 2 leads us A(s( : ) ; B; ). B 62 s( : ) .also know, () respect B , , 8o 2 : B 2 add(o) )pre(o) \ s( : ) = ; holds. Therefore, apply Lemma 3 arrive pA(B; ),contradiction.return blocks world example show computation proceeds.Let us first investigate whether on(a; b) on(b; c) holds. initial value F ( )obtained delete list stack(b,c) action, one addsgoal.A;nBDAA;BA;BA;BA;BA;BA;BA;Bhb;chDA= fclear(c); holding(b)gIntuitively, immediately clear neither facts ever hold stateon(b; c) true: b c, c clear gripper cannot hold b.turns fixpoint computation respects intuition leaves set F ( )unchanged, yielding F = fclear(c); holding(b)g. repeat fixpoint processdetail here, reconstructed Figure 1 details necessaryunderstanding correct ordering relations derived. short, factsachievers reduced action set, need preconditionsachiever available. example, holding(b) achieved either unstackpickup action. either need b stand another block stand table.actions achieve facts need holding(b) true thus excludedreduced action set.finishing fixpoint computation, planner tests pA(on(a; b); ),contains actions except delete on(b; c) use clear(c) holding(b)precondition. finds action stack(a,b) adds on(a; b). preconditionsaction holding(a) clear(b). conditions added actionspickup(a) unstack(a,b), respectively, contained : neitherneeds c clear b gripper. Thus, test finds fact, on(a; b)possibly achievable using actions , ordering derived, i.e., on(a; b) 6on(b; c) follows.Now, way round, on(b; c) on(a; b) tested. initial value F ( )obtained single action stack(a,b)( )= fclear(b); holding(a)gF()b;cFDAb;cDAha;bhDAa;bDA355fiKoehler & HoffmannAgain, fixpoint computation cause changes, resulting F = fclear(b);holding(a)g. process tests whether pA(on(b; c); ) holds, containsactions except delete on(a; b) use clear(b) holding(a)precondition. action add on(b; c) stack(b,c). action needspreconditions facts holding(b) clear(c). process finds crucialcondition achieving first fact violated: action achieve holding(b)clear(b) precondition, b must clear first gripper hold it.Since clear(b) element F , none actions achieving holding(b) containedO. Consequently, test pA(on(b; c); ) fails obtain ordering on(b; c)on(a; b). makes sense gripper cannot grasp b stack onto c anymore,on(a; b) achieved.h3.3 Forced Goal Orderings Invertible Planning Problemsfar, introduced two easily computable ordering relationsapproximate reasonable goal ordering . One might wonder investeffort trying find forced goal orderings. two reasons that:her1. already seen Section 2, forced goal ordering also reasonablegoal ordering, i.e., method approximates latter also used crudeapproximation former.2. Many benchmark planning problems invertible certain sense. problemscontain forced orderings anyway.section, elaborate detail second argument. results bitgeneral necessary point. want make use later showAgenda-Driven planning algorithm propose complete respect certain classplanning problems. proceed formally defining class planning problems, showproblems contain forced orderings, identify sucient criterionmembership problem class. Finally, demonstrate many benchmarkplanning problems fact satisfy criterion. start, introduce notiondeadlock planning problem.Definition 14 (Deadlock) Let (O; ; G ) planning problem. reachable statecalled deadlock iff sequence actions leads goal, i.e., iff00= Result(I ; P ) :9 P : G Result(s; P ).class planning problems interested class problemsdeadlock-free. Naturally, problem called deadlock-free none reachable statesdeadlock sense Definition 14.Non-trivial forced goal orderings imply existence deadlocks (rememberordering B B called trivial iff state s( : ) all).frA;BLemma 5 Let (O; ; G ) planning problem, let A; B 2 G two atomic goals.non-trivial forced ordering B B , exists deadlockstate problem.f356fiOn Reasonable Forced Goal OrderingsProof: Recalling Definition 9 assuming non-triviality , knowleast one state s( : ) made true, B still false. Definition 7,know plan state achieves B . particular,possible achieve goals starting s( : ) . Thus, state := s( : ) mustdeadlock.fA;BA;BA;Binvestigate deadlocks detail discuss commonlyused benchmark problems contain them, i.e., deadlock-free. Lemma 5,also know domains contain non-trivial forced goal orderingseither|so much point trying find them. care trivial goalorderings. orderings force reasonable planning algorithm consider goalscorrect order.existence deadlocks depends structural properties planning problem:must action sequences, which, executed, lead states goalscannot reached anymore. sequences must undesired effects, cannotinverted sequence actions O. Changing perspective, one obtains hintsucient condition non-existence deadlocks might defined. Assumeplanning problem effects action sequence domaininverted executing certain sequence actions. invertible planningproblem, particular possible get back initial state reachable state.Therefore, problem solvable, contain deadlocks: state,one reach goals going back initial state first, execute arbitrarysolution thereafter. formally define notion invertible planning problems,turn argumentation proof.Definition 15 (Invertible Planning Problem) Let (O; ; G ) planning problem,let denote states reachable actions O. problem calledinvertible8 : 8 PO : 9 PO :Result(Result(s; P ); PO) =Theorem 6 Let (O; ; G ) invertible planning problem, solution exists.(O; ; G ) contain deadlocks.Proof: Let = Result(I ; P ) arbitrary reachable state. problem invert-ible, know sequence actions P Result(s; P ) = holds.problem solvable, solution plan P starting achievingG Result(I ; P ). Together, obtain G Result(Result(s; P ); P ). Therefore,concatenation P P solution plan executable consequently,deadlock.know invertible planning problems, solvable, contain deadlocksconsequently, contain (non-trivial) forced goal orderings. see nextthat, matter fact, benchmark planning problems invertible. arrivesucient condition invertibility notion inverse actions.357fiKoehler & HoffmannDefinition 16 (Inverse Action) Given action set containing actionform pre(o) ! add(o) del(o). action 2 called inverseform pre(o) ! add(o) del(o) satisfies following conditions1. pre(o) pre(o) [ add(o) n del(o)2. add(o) = del(o)3. del(o) = add(o)certain conditions, applying inverse action leads back state one startedfrom.Lemma 6 Let state action, applicable s. del(o) pre(o)\ add(o) = ; hold, action inverse sense Definition 16applicable Result(s; hoi) Result(Result(s; hoi); hoi) = follows.Proof: applicable s, pre(o) s. atoms add(o) added,atoms del(o) removed s, altogetherResult(s; hoi) (pre(o) [ add(o)) n del(o) pre(o)Thus, applicable Result(s; hoi).Furthermore, Result(s; hoi) = [ add(o) n del(o)Result(Result(s; hoi); hoi)= Result(s [ add(o) n del(o); hoi)= (s [ add(o) n del(o)) [ add(o) n del(o)= (s [ add(o) n del(o)) [ del(o) n add(o)(cf. Definition 16)= [ add(o) n add(o)(because del(o) pre(o) s)=(because \ add(o) = ;)Lemma 6 states two prerequisites: (1) inclusion operator's delete list preconditions (2) empty intersection operator's add list stateapplicable. planning problem called invertible meets prerequisitesinverse action.Theorem 7 Given planning problem (O; ; G ) set ground actions satisfyingdel(o) pre(o) pre(o) ) add(o) \ = ; actions reachable states s.inverse action 2 action 2 O, problem invertible.Proof: Let reachable state, let P = ho1 ; : : : sequence actions.need show existence sequence PnResult(Result(s; P ); P ) =358( )fiOn Reasonable Forced Goal Orderingsholds. define P := ho ; : : : ; o1 i, prove ( ) induction n.n = 0: Here, P = P = hi, Result(Result(s; hi); hi) = obvious.n ! n + 1: P = ho1 ; : : : ; ; +1 i. induction hypothesis knowResult(Result(s; ho1 ; : : : ; i); ho ; : : : ; o1 i) = s. make following bit readable,let s0 denote s0 := Result(s; ho1 ; : : : ; i).nnnnnnResult(Result(s; ho1 ; : : : ; +1 i); ho +1 ; : : : ; o1 i)Result(Result(s0 ; ho +1 i); ho +1 ; : : : ; o1 i)Result(Result(Result(s0 ; ho +1 i); ho +1 i); ho ; : : : ; o1 i)Result(s0 ; ho ; : : : ; o1 i)nn====nnnnnn(cf. Lemma 6 s0 +1 )(per induction)nAltogether, know invertible problems, solvable, contain forcedorderings. also know problems, inverse action actionO, invertible following Theorem 7. Theorem 7 requires del(o) pre(o) holdaction o, pre(o) ) add(o) \ = ; hold actions reachable states s.see conditions, (a) inclusion delete list precondition list, (b)empty intersection action's add list reachable states applicable,(c) existence inverse actions, hold currently used benchmark domains.4Concerning condition (a) actions delete facts require preconditions, one finds phenomenon domains commonly used planningcommunity, least known authors. something seemshold reasonable logical problem formulation. authors even postulateassumption algorithms work, cf. (Fox & Long, 1998).Similarly case conditions (b) (c): One usually finds inverse actionsbenchmark domains. Also, action's preconditions usually imply|by state invariants|add effects false. example blocks world, stack unstackactions invert other, action's add effects exclusive preconditions|former contained union False constructed preconditions, seeSection 3.1. Similarly domains deal logistics problems, example logistics,trains, ferry, gripper etc., one often find inverse pairs actions preconditionsalways excluding add effects. Sometimes, two different ground instancesoperator schema yield inverse pair. example, gripper, two ground instancesmove(roomA, roomB)at-robby(roomA)! ADD at-robby(roomB) DEL at-robby(roomA).4. order avoid reasoning reachable states condition (b), one could also postulateaction add effects negative preconditions, cf. (Jonsson, Haslum, & Backstrom, 2000).is, however, commonly used typical planning benchmark problems.359fiKoehler & Hoffmannmove(roomB, roomA)at-robby(roomB)! ADD at-robby(roomA) DEL at-robby(roomB).move(?from,?to) operator schema invert other. Similarly, towers hanoi,single move operator schema, inverse instance foundground instance schema, add effects always falsepreconditions true.rarely, non-invertible actions found benchmark domains.occur, role domain often quite limited example operators cussate Russel's Tyreworld.cuss! DEL annoyed().ate(?x:wheel)have(pump) not-in ated(?x) intact(?x)! ADD ated(?x) DEL not-in ated(?x).Obviously, much point defining something like decuss de ateoperator. formally speaking, none ground actions operators destroysgoal precondition action domain. Therefore, mattereffects cannot inverted. particular, forced goal ordering derivedwrt. actions. 5importance inverse actions real-world domains also discussedNayak Williams (1997), describe planner BURTON controlling Cassinispacecraft. contrast domains, problems example usedBarrett et al. (1994) almost never contain inverse actions. Consequently, domainsplenty forced goal orderings could discovered used planner avoid deadlocksituations. widespread, although perhaps unconscious use invertible problemsbenchmarking current phenomenon related STRIPS descending planning systems.one anonymous reviewers pointed us, quite number non-invertible planningproblems also proposed planning literature, e.g., register assignmentproblem (Nilsson, 1980), robot crossing road problem (Sanborn & Hendler, 1988),instances manufacturing problems (Regli, Gupta, & Nau, 1995), Yale Shootingproblem (McDermott & Hanks, 1987). problems, i.e., problemsinvertible, one could|in spirit argument 1 beginning section|simply use approximate forced orderings one interested finding leastthose. precisely, methods might detect forced orderings|asalso reasonable|but might also find more, necessarily forced, orderings.one interested finding forced orderings, possible way go.example, simple blocks world modification blocks cannot unstacked anymorestacked|which forces planner build stacks bottom up|bothstill capable finding correct goal orderings.eheheh5. cuss operator, way, one known authors deletes fact usingprecondition. also one know could removed domain descriptionwithout changing anything.360fiOn Reasonable Forced Goal Orderings3.4 Extension Goal Orderings ADL Actionsorderings, introduced far, easily extended dealground ADL actions conditional effects using negation instead delete lists.actions following syntactic structure:: 0 (o) = pre0 (o) ! eff+0 (o); eff0 (o)1 (o) = pre1 (o) ! eff+1 (o); eff1 (o)...(o) = pre (o) ! eff+ (o); eff (o)unconditional elements action summarized 0 (o): preconditionaction denoted pre0 (o), unconditional positive negative effectseff+0 (o) eff0 (o), respectively. conditional effect (o) consists effectcondition (antecedent) pre (o), positive negative effects eff+ (o) eff (o).Additionally, denote (o) set unconditional conditional effects,i.e., (o) = f0 (o); 1 (o); : : : ; (o)g.computation immediately carries ADL actions extensionplanning graphs used, handle conditional effects, e.g., IPP (Koehler, Nebel,Hoffmann, & Dimopoulos, 1997) SGP (Anderson & Weld, 1998). One simply takesset exclusive facts returned systems determine set F . testDefinition 10, decides whether ordering B two atomic goalsB , extended ADL follows.nnnnneGPeDefinition 17 (Ordering ADL) Let (O; ; G fA; B g) planning problem.eFalse set A. ordering B holdsLet FeGP8 2 O; (o) 2 (o) : B 2 eff+(o) ^ 62 (o) ) (pre (o) [ pre0(o)) \ F 6= ;GPHere, (o) denotes negative effects implied conditions (o).(o) :=eff0 (o) [eff0 (o)prej (o) prei (o) effj (o) 6= 0i=0Thus, B ordered (unconditional conditional) effects add B eitherimply effect deletes A, need conditions cannot made true togetherA. Note effect requires conditions pre (o) [ pre0 (o) satisfied,impossible state holds non-empty intersectionF.computation requires little adaptation effort. order obtainset F , need investigate conditional effects well. actionconditional unconditional effect, determine atoms negatedit, matter effect used achieve A. obtain atoms intersectingappropriate sets (o).\(o)D(o) :=GPhDA+2 effi (o)361fiKoehler & Hoffmannexactly facts always deleted achieving A, mattereffect use.intersection sets D(o) actions yields desired set F . Let usconsider following small example clarify computation.0 (o) = fU g! fW g f:X g;1 (o) = fV; W g ! fAg f:X g;2 (o) = fW g! fU g f:Y gDAobtain D1 (o) = f:X g [ f:Y g = f:X; :Y g, precondition 2 (o)implied first conditional effect 1 (o). 1 (o) effect achieve A,get D(o) = D1 (o) = f:X; :Y g.obtain smaller set D(o), add unconditional positive effectaction.0 (o) = fU g! fW; Ag f:X g;1 (o) = fV; W g ! fAg f:X g;2 (o) = fW g! fU g f:Y gcase, need intersect sets D0 (o) = f:X g D1 (o) = f:X; :Y g,yielding D(o) = f:X g. ects fact that, achieving via unconditionaleffect o, X gets removed state.fixpoint computation requires adapt computation . First, repeatsteps case simple STRIPS actions consider unconditional negativeeffects intersection preconditions False set::= n fo j 2 eff0 (o) _ F \ pre0(o) 6= ;gDAThen, additionally remove action conditional effects either implydeletion impossible effect condition.:= red(O ) = fred(o)jo 2 OgHere, red function red(o) : 7! o0(o0 ) = (o) n f (o) j 2 (o) _ pre (o) \ FkkDAk6= ;gFinally, need redefine Definition 12, expresses conditionsfact believed possibly achievable given certain set operators O.Definition 18 (Possibly Achievable Atoms ADL) atom p possibly achievable given action set (written pA(p; O))9 2 O; 2 (o) : p 2 eff+(o) ^8 p0 2 (pre (o) [ pre0(o)) : 9 o0 2 O; 0 2 (o0) : p0 2 eff+0 (o0 )holds, i.e., positive effect p conditions preconditionsmade true effects reduced action set.362fiOn Reasonable Forced Goal Orderingsprocess, decides whether atomic goal B heuristically ordered anothergoal (i.e., whether B holds) proceeds exactly way describedSection 3.2: False set F reduced fixpoint computation, remainsunchanged, employs updated routines computing deciding pA(f; O).result, B ordered (B A) possibly achievablepA(B; ) using action set results fixpoint.hDAh4. Use Goal Orderings Planningdetermined ordering relations hold pairs atomic goalsgiven goal set, question make use planning. Severalproposals made literature, see Section 6 detailed discussion.paper, propose novel approach extracts explicit ordering subsetsgoal set|called goal agenda. planner, case IPP, run successivelyplanning subproblems represented agenda.4.1 Goal Agendafirst step one take computing goal agenda perform so-called goal2 G atomic goals must examinedorder find whether ordering relation B , B A, both, none holdsthem. ordering relation , arbitrary definition used.experiments, relation always either .determined ordering relations hold atomic goals, wantsplit goal set smaller sets based relations, want ordersmaller sets, also based relations. precisely, goal sequencegoal sets G1 ; : : : ; G[G =Ganalysis. goal analysis, pair A; Behnn=1G\G =;j6= j; 1 i; j n. also want sequence goal sets respect orderingrelations derived atomic goals. make explicit, firstintroduce simple representation detected atomic orderings: goal graph G.G := (V; E )V := GE := f(A; B ) 2 G G j B gNow, desired properties, sequence goal sets possess, easilystated:363fiKoehler & HoffmannGoals A; B lie cycle G belong set, i.e., A; B 2 G .G contains path goal goal B , vice versa, orderedB , i.e., 2 G B 2 G < j .jproperties appear reasonable goal-set sequence respectingatomic orderings. introduce simple algorithmic method producesequence goal sets meets requirements.First all, transitive closure G computed. done cubictime size goal set (Warshall, 1962). Then, node transitiveclosure, ingoing edges outgoing edges counted. disconnected nodes= = 0 moved separate set goals G-sep containingatomic goals, participate relation. nodes A, degreed(A) =determined difference number ingoing edgesnumber outgoing edges. Nodes identical degree merged one set.sets ordered increasing degree yield desired sequence goal sets.problem remaining set G-sep. non-empty, clear placeput it.Let us consider small example process. Figure 3 depicts left goalgraph, results goal set G = fA; B; C; D; E g ordering relationsB; B C B D, transitive closure right.BBCCEEFigure 3: left, goal graph depicting relations atomic subgoals.right, transitive closure graph.Figure 4, number in- outgoing edges goal, corresponding degrees,resulting goal-set sequence shown.00E0312B2020-3{A}C-1{B}2{C,D}G-sepEFigure 4: left, number in- outgoing edges node. right,degree nodes merged sets goals degree.node E becomes member G-sep set remains unordered.dicult verify resulting goal sequence respects atomic goal orderings:364fiOn Reasonable Forced Goal OrderingsNodes occurring cycle graph isomorphic in- outgoing edgestransitive closure graph. particular, degree getmerged set G .Say graph, path B , vice versa. Then,transitive closure graph, edge nodeB path to, additionally edge B , i.e., > Bfollows. Similarly, ingoing edge B node pathA, additionally, edge B , gives us B > . Altogether,d(A) =<B<BB = d(B ) thus, degreesmaller degree B required, gets ordered B .Note nothing said argumentation set unordered goals, Gsep. set could, principle, inserted anywhere sequence resultingsequence still respecting atomic orderings. possible heuristic may use goal setfirst sequence, apparently problem reach goalsgoals set achieved. Another heuristic could put set endneither problem reach goal set goals. decideddeal problem sophisticated way trying derive ordering relationG-sep goal sets G already derived. orderso, need extend definitions goal orderings sets goals.4.2 Extension Goal Orderings Goal SetsGiven set atomic goals, always problem exponentially manysubsets compared order derive reasonable goal orderinggoal sets. consideration possible subsets question,result exponential overhead. partial goal agenda obtained faroffers one possible answer. suggests taking set G-sep trying orderrespect goal sets emerging goal graph.Given planning problem (O; I; G ) two subsets atomic goals fA1 ; : : : ; g GfB1 ; : : : ; B g G , definition sets atomic goals straightforward.sake simplicity, consider STRIPS actions here. definitionsdirectly extended ADL.define ordering , extends sets, begin defining set F f 1 n gatoms, exclusive least one atomic goal planning graphgenerated (O; I; G ):F f 1 n g := fp j p exclusive least one graph leveled gset 1 n g obtained accordingly removing actions deleteleast one , i.e., 1 n g = fo 2 j 8 2 f1; : : : ; ng : 62 del(o)g.Definition 19 (Ordering Goal Sets) Let (O; I; G ) planning problemnekh;:::;AEeGP;:::;AGP;:::;A;:::;AfA1 ; : : : ; g G fB1 ; : : : ; B g G . Let Ff 1 ng False set fA1 ; : : : ; g.ordering fB1 ; : : : ; B g fA1 ; : : : g holds9 j 2 f1; : : : ; kg : 8 2 1 ng : B 2 add(o) ) pre(o) \ Ff 1 ng 6= ;:E;:::;AnkkEnGPn;:::;A;:::;Aj365GPfiKoehler & Hoffmannsimilar way, extended . , sets F determinedbased Equation (3). set Ff 1 n g simply union individual sets:f 1 n g := [ FF(4)HhDA;:::;ADA;:::;ADADAfixpoint computation entered:= n fo 2 j 9 2 f1; : : : ; ng : 2 del(o) _ Ff 1 ng \ pre(o) 6= ;g (5)recomputation iteration fixpoint algorithm Figure 1 done;:::;ADAaccordingly. Apart this, algorithm remains unchanged.Definition 20 (Ordering ) Let (O; I; G ) planning problem fA1 ; : : : ; gfB1 ; : : : ; B g G . Let set actions obtained performingfixpoint computation shown Figure 1, modified handle sets facts definedEquations (4) (5). ordering fB1 ; : : : ; B g fA1 ; : : : ; g holds9 j 2 f1; : : : ; kg : :pA(B ; O)GHnkHknjgiven goal sets undergo goal analysis, i.e., pair sets checkedordering relation . derived relation defines edge graphsubgoal sets nodes. transitive closure determined before, degreenode computed. graph contains disconnected nodes, total orderingsubsets goals results ordering nodes based degree. orderingdefines goal agenda. case disconnected nodes, default heuristicadding corresponding goals last goal set agenda.EH4.3 Agenda-Driven Planning AlgorithmGiven planning problem (O; ; G ), let us assume goal agenda G1 ; G2 ; : : : ; Gk entries returned analysis. entry contains subset G G .basic idea agenda-driven planning algorithm first feed planneroriginal initial state I1 := goals G1 := G1 , execute solution plan P, yielding new initial state I2 = Result(I1; P ). Then, new planning probleminitialized (O; I2 ; G2 ). solving problem, want goals G2 true,also want goals G1 remain true, set G2 := G1 [ G2 . continuousmerging successive entries agenda yields sequence incrementally growinggoal sets planner, namely[G := Gkjj=1little detail, agenda-driven planning algorithm implemented IPP worksfollows. First, IPP called problem (O; ; G1 ) returns plan P1 ,achieves subgoal set G1 . P1 sequence parallel sets actions, returnedIPP similarly graphplan. Given plan, resulting state R(I ; P1 ) = I2366fiOn Reasonable Forced Goal Orderingscomputed based operational semantics planning actions.6 case setSTRIPS actions, one simply adds ADD effects deletes DEL effectsstate description order obtain resulting state, following Result functionDefinition 2. STRIPS, Result function coincides directly R function.case set parallel ADL actions, one needs consider possible linearizationsparallel action set deal conditional effects separately.linearization, different resulting state obtained, satisfygoals. obtain new initial state I2 , one takes intersection resulting statespossible linearization actions parallel set. means compute n!linearizations parallel action set n actions time step. Since n usuallysmall (more 5 6 ADL actions per time step rare), practical costscomputation neglectible.way, given solution subproblem (O; ; G ), one calculates new initialstate +1 runs planner subsequent planning problem (O; +1 ; G +1 )planning problem (O; ; G ) solved.plan solving original planning problem (O; ; G ) obtained takingsequence subplans P1 ; P2 ; : : : ; P . One could argue planning increasing goalsets lead highly non-optimal plans. IPP still uses \no-ops first" strategyachieve goals, originally introduced graphplan system (Blum & Furst,1997). Employing strategy, graphplan algorithm, short, first tries achievegoals simply keeping true, possible. Since goals G1 ; G2 ; : : : ; G alreadysatisfied initial state +1 , starting planner tries achieve G +1 ,strategy ensures goals destroyed re-established solutionfound otherwise. no-ops first strategy merely graphplan feature,reasonable planning strategy preserve goals already true initial statewhenever possible.soundness agenda-driven planning algorithm obvious G = Gsequence sound subplans yielding state transition initial statestate satisfying G .completeness approach less obvious holds planner cannotmake wrong decisions finally reaching goals. precisely, approachcomplete problems contain deadlocks introduced Definition 14.kkkkTheorem 8 Given solvable planning problem (O; I; G ), goal agenda G1 ; G2 ; : : : GkG G +1 G = G . Running complete planner agenda-driven mannerdescribed yield solution problem deadlock-free.kProof: Let us assume planner find solution step agenda-drivenalgorithm, i.e., solution found subproblem (O; ; G ). planner assumedcomplete subproblem, implies unsolvability (O; ; G ). problemsolvable, neither problem (O; ; G ) solvable, since G G holds. Therefore,goals cannot reached . Furthermore, reachable state|it reachedexecuting partial solution plans P1 ; : : : ; P 1 initial state. Consequently,must deadlock state sense Definition 14, contradiction.6. See (Koehler et al., 1997) exact definition R, want repeat here.367fiKoehler & Hoffmannresult states feasibility approach: shown, benchmarkproblems currently investigated contain inverse actions, therefore invertible(Theorem 7), also deadlock-free (Theorem 6). Thus, Theorem 8,approach preserves completeness domains.However general case, completeness cannot guaranteed. following exampleillustrates situation assumption s( : ) 6j= p (assuming preconditionsachieving actions contained state reached, cf. derivationordering Section 3) wrong yields goal ordering planfound anymore although problem solvable.Given initial state fC; Dg goals fA; B g, planner following setground STRIPS actions :A;Bhop1:op2:op3:op4:fC gfDgfE gfF g!!!!ADD fB g DEL fDgADD fE gADD fF gADD fAganalysis return ordering B B added op1,precondition C effect actions. Thus concludes Creachable state holds. example, C holds reachablestates. assumption s( : ) 6j= C made test pA(B; ) wrong. Thus, Breached A. hand, B holds, even forced orderingB . testing B , ordering remains undetected,method discover precondition F op4 achievable stateB holds: obtain F = fDg, excludes op2 , op3 op4remain set usable actions. Thus, op4 considered legal achiever A, op3considered legal achiever precondition F . could detect right orderingregressed action chain op4, op3, op2 found that,F set B , actions must excluded .Consequently, goal agenda fB g; fAg fed planner, solves firstsubproblem using op1, fails achieving state fB; C g sinceinverse action op1 cannot re-established way.hA;BrfhBDA5. Empirical Resultsimplemented methods approximate so-called Goal Agenda Manager(GAM) IPP planning system (Koehler et al., 1997). GAM activatedset ground actions determined either uses approximatereasonable goal ordering. calls IPP planning algorithm entrygoal agenda outputs solution plan concatenation solution plansfound entry agenda.7reh7. source code GAM, based IPP 3.3, collection domainsdraw subsequent examples downloaded http://www.informatik.uni-freiburg.de/~koehler/ipp/gam.html. experiments performed SPARC 1/170.368fiOn Reasonable Forced Goal Orderingsempirical evaluation performed uses IPP domain collection, contains 48 domains 500 planning problems. domains,able derive goal ordering information 10 domains. domains indeed pose constraints ordering planner achieve set goals.domains, goal orderings could derived, found either single goalachieved, example manhattan, movie, molgen, montlake domainsgoals achieved order, example logistics, gripper, ferrydomains. found benchmark domain, natural goal ordering existed,method failed detect it. matter fact, looking goal ordering seemsnatural, one usually finds ordering reasonable sense Definition 8, seeexample blocks world, woodshop, tyreworld domains. method finds almostreasonable orderings, indicates approximation techniquesappropriate detecting ordering information.ehfollowing, first compare techniques terms runtimenumber goal agenda entries generated. take closer look agendasgenerated selected domains investigate uence performanceIPP planning system. exact definition domains downloadedIPP webpage, give name domain name particularplanning problem well number (ground) actions domain contains,parameter nicely characterizes size domain usually dicultyhandle it.ehexamples, times shown compute goal agenda contain effortparse instantiate operators, i.e., compute set actions. Times parsinginstantiation listed explicitly, are, test examples used here,usually close zero uence performance planner significantway.5.1 Comparisonhebegin comparison summary results obtained different representational variants blocks world. bw large bw large examples originateSATPLAN test suite (Kautz & Selman, 1996) added larger examplesbw large e bw large g. parcplan example comes (El-Kholy & Richards, 1996)uses multiple grippers limited space table. stack n examples usegraphplan blocks world representation simply require stack n blocks other,table initial state.two methods return exactly ordering relations across blocks worldproblems. Figure 5 confirms, computation based planning graphsmuch time-consuming. hits computational border domain contains10000 actions. computation much faster also scales larger actionsets.eh369fiKoehler & Hoffmannproblembw largebw large bbw large cbw largebw large ebw large fbw large gparcplanstack 20stack 40stack 60stack 80#actions #agenda entries CPU( ) CPU( )16210.690.0724251.450.1145074.850.227221114.180.357221112.950.351250644.930.581800997.110.881960425.841.47800196.910.36320039160.001.74720059840.424.85128007911.38ehFigure 5: Comparison blocks world problems. #actions shows numberactions set O, planner tries construct plan. #agendaentries says many goal subsets detected ordered GAM.Column 4 5 display CPU time required methodscompute agenda provided set O. dash always meanIPP ran memory 1 Gbyte machine.ehFigure 6 Figure 7 show results domains, methodable detect reasonable orderings. Figure 6 lists domains, methodsreturn goal agendas. tyreworld, hanoi, fridgeworld domains originateUCPOP (Penberthy & Weld, 1992), link-repeat domain found (Veloso& Blythe, 1994). performance results coincide shown Figure 5. Figure 7shows picture terms runtime performance, domains differentagendas returned .woodshop scheduling domains contain actions conditional effects,domains use STRIPS operators. computation fails derive goalorderings scheduling world problems (of display largest problemsched6) wood1 problem. explanation behavior founddifferent treatment conditional effects methods. IPP find limitedform mutex relations conditional effects building planning graph.goal, achieved conditional effect, often exclusive largenumber facts graph. Thus, F sets small sometimes even emptyconsequently, actions excluded performing reachabilityanalysis thus, reasonable orderings may remain undetected. Direct analysis investigatesconditional effects detail therefore able derive much larger F sets.behavior method STRIPS domains bulldozer, glassworld,shopping world caused phenomenon. domains, one derive muchlarger F sets using planning graphs turn sets exclude actions. Since directanalysis finds smaller empty F sets, also finds less relations. woodshop domainehehh370fiOn Reasonable Forced Goal Orderings#actions #agenda entries CPU( ) CPU( )2660.050.015960.200.0310860.450.0617360.840.1025461.560.15899616.290.644830.050.029040.100.0415050.190.0823160.350.1233670.630.1977920.770.553120.190.013120.210.01domaintyreworldproblemfixit1fixit2fixit3fixit4fixit5fixit10hanoihanoi3hanoi4hanoi5hanoi6hanoi7fridgeworld fridgelink-repeat link10link30ehFigure 6: Comparison benchmark domains, returnidentical agendas.ehdomainbulldozerglassworldproblembullglass1glass2glass3shoppingworld shopschedulingsched6woodshopwood1wood2wood3#actions #agenda entries CPU( ) CPU( )612/10.090.03262/10.020.011142/10.190.091222/10.220.09812/10.070.021041/401.00.12151/30.030.01156/50.030.01436/50.140.06ehFigure 7: Domains return different goal agendas, giveform n1 =n2 . number slash says many entries containedagenda computed , number following slash says manyentries contained agenda computed . #agenda entries=1 meansagenda contains single entry, namely original goal set,ordering derived.ehehshows results differ within domain, depending specificplanning problem. problem wood2 varies problem wood1 sense onegoal slightly different|an object needs put different shape|and twogoals present. goal orderings derived pairs old371fiKoehler & Hoffmanngoals wood1, lots relations derived mixed pairs old new goalswood2, yielding detailed goal agenda. problem wood3 contains additional objectsmany goals, also successfully ordered.subsequent experiments, decided solely use heuristic orderingcomputation less costly computation cases, yieldingcomparable agendas cases. three domains investigate closely,namely blocks world, tyreworld hanoi domains, agendas derived methodsare, fact, exactly same.eheh5.2 uence Goal Orderings Performance IPP InteractionRIFOsection, analyze uence goal agenda performance IPPcombine another domain analysis method, called RIFO (Nebel, Dimopoulos, &Koehler, 1997). RIFO family heuristics enables IPP exclude irrelevant actionsinitial facts planning problem. effectively combined GAM,IPP plans subset goals original goal set, likelyalso subset relevant actions needed find plan. precisely,obtain one subproblem entry agenda, and, subproblem,use RIFO preprocessing planning IPP. configuration, GAM reducessearch space IPP decreasing number subgoals planner achievemoment, RIFO reduces search space dramatically selectingactions relevant goal subset.5.2.1 Blocks WorldFigure 8 illustrates parcplan problem (El-Kholy & Richards, 1996) detail. Sevenrobot arms used order 10 blocks 3 stacks 5 possible positions table.111142423132312223211213112332123122241413212345Figure 8: parcplan problem limited space table, seven robot arms,several stacks.goal agenda derived IPP orders blocks horizontal layers:1:2:3:4:on-table(21, t2) ^ on-table(11, t1)on-table(31, t3) ^ on(22, 21) ^ on(12, 11)on(32, 31) ^ on(13, 12) ^ on(23, 22)on(14, 13) ^ on(24, 23)372fiOn Reasonable Forced Goal Orderingsoptimal plan 20 actions solving problem found IPP using GAM 14 s,spends one second computing goal agenda, almost 13 seconds buildplanning graphs, 0.01 second search plan. 70 actions triedfind solution. Without goal analysis, IPP needs approx. 47 searches 52893actions 26 seconds.RIFO (Nebel et al., 1997) fails detecting subset relevant actions originalgoal set considered, succeeds selecting relevant actions subproblemsstated agenda. reduces runtime less 8 1 spentgoal agenda, almost 6 spent removal irrelevant actions initial facts, less1 spent building planning graphs. previously, almost time spentplanning.Figure 9 shows IPP SATPLAN blocks world examples (Kautz & Selman,1996), bw large.e example taken (Dimopoulos, Nebel, & Koehler, 1997), twolarge examples bw large.f (containing 25 blocks requiring build 6 stacksgoal state) bw large.g 30 blocks/8 stacks.SATPLANbw large.abw large.bbw large.cbw large.dbw large.ebw large.fbw large.g# actions plan length IPP +G +G+R +G+R+L16212 (12)0.70 0.740.580.3424222 (18) 26.71 0.860.550.5245048- 7.342.422.5872254- 11.623.743.8172252- 11.143.993.9712509016.01180084- 117.5628.71Figure 9: Performance extended SATPLAN blocks world test suite. secondcolumn shows number ground actions domain, third columnshows plan length, i.e., number actions contained plan, generatedGAM parentheses plan length generated IPP without GAM givenIPP without GAM able solve corresponding problem. +G meansIPP using GAM, +G+R means IPP uses GAM RIFO, +G+R+Lmeans subgoals set agenda arbitrarily linearized.runtimes cover whole planning process starting parsing operatordomain file, performing GAM RIFO analysis (if active),searching graph plan found.IPP 3.3 without GAM solve bw large.a bw large.b problems. Usinggoal agenda, plans become slightly longer, performance increasing dramatically.Plan length growing blocks accidentally put positions cutgoals still ahead agenda thus, additional actions need addedplan remove blocks wrong positions. speed-up possibleRIFO additionally used, reduces size planning graphs dramatically.Finally, goals belong subset agenda linearized based373fiKoehler & Hoffmannheuristic assumption analysis found reasonable goal orderings, goalsachievable order. option, problems solved almost instantly.reader may wonder point use linearization agenda entriesextra option investigate further. two reasons that. First,linearization negative side effects domains investigated.example, yields much longer plans logistics domain variants.linearizing single entry agenda logistics problem contains, packages gettransported goal position one one. course, takes much planningsteps simultaneously transporting packages coinciding destinations.Secondly, effects linearization somewhat unpredictible, even domainsusually tends yield good results. GAM recognise interactions goals. Consider blocks world problem four blocks A, B , C D.Say B positioned C initially, blocks table, goalon(A; B ) on(C; D). agenda problem comprise single entrycontaining goals. fact, reasonable goal ordering here. Nevertheless,stacking onto B immedeatly bad idea, planner needs move C achieveon(C; D). aware this, GAM might linearize single agenda entryon(A; B ) front, makes problem harder actually is. Thus, runtimeadvantages linearization sometimes yields blocks world less seencases \good luck".Figure 10 shows IPP stack n problems. IPP without domain analysishandle 12 blocks less 5 minutes, 13 blocks 15 minutesneeded. Using GAM, 40 blocks stacked less 5 minutes. Using GAMRIFO, 5 minutes limit extended 80 blocks, stack100 solved 11.5 min11.3 min spent analysis methods 0.2 min needed buildingplanning graphs extracting plan.time600450IPPIPP+GIPP+G+R3001501020Figure 10:IPP30405060708090100 blocks3.3 simple, huge stacking problem.Figure 11 shows sharing overall problem-solving time GAM, RIFOIPP search algorithm blocks world problems. Similar results obtainedtyreworld. GAM takes 3 16 %, RIFO takes 75 96 %,search effort reduced approx. 1 %. overall problem solving time clearlydetermined RIFO, search effort becomes marginal factor determinationperformance. indicates speed-up possible improving374fiOn Reasonable Forced Goal Orderingsperformance GAM RIFO. also indicates even hardest planning problemsbecome easy structured decomposed right way.problemstack 20stack 40stack 60stack 80parcplan# actions80032007200128001960GAMRIFO0.31 = 16 % 1.44 = 75 %1.57 = 7 % 18.77 = 90 %4.40 = 4 % 93.10 = 94 %9.60 = 3 % 283.60 = 96 %0.86 = 12 % 5.52 = 76 %search algorithm0.13 = 7 %0.51 = 2 %1.15 = 1 %2.33 = 1 %0.83 = 11 %Figure 11: Distribution problem-solving time blocks world examples GAM,RIFO, search algorithm, comprises time build searchplanning graph. remaining fraction total problem-solving time,shown table, spent parsing instantiating operators.5.2.2 Tyreworldtyreworld problem, originally formulated Stuart Russell, asks planner findreplace tire. easily solved IPP within milliseconds. problembecomes much harder number tires increasing, cf. Figure 12.Tires12345678910Figure 12:# actionsIPP+G+R+G+R+LSearch Space260.10 (12/19) 0.15 (14/19) 0.16 (17/19)1298/885917.47 (18/30) 0.41 (24/32) 0.32 (30/34) 1290182/2101082.87 (32/44) 0.63 (41/46)-/3661731.12 (52/60)-/5652541.93 (63/73)-/8073533.42 (73/85)-/10924644.81 (84/98)-/14205938.07 (95/121)-/179173811.27 (106/124)-/ 220589916.89 (118/136)-/2662Tyreworld. numbers parentheses show time steps, followednumber actions generated plan. last column comparessearch spaces. number slash shows \number actions tried"parameter plain IPP planning algorithm, number followingslash shows \number actions tried" IPP using GAM, RIFO,linearization entries agenda. dash means \numberactions tried" unknown IPP failed solving correspondingplanning problem.IPP375fiKoehler & HoffmannIPP able solve problem 1 2 tires. Using GAM RIFO, 3tires handled. Solution length GAM slightly increasing, causedsuper uous jack-up jack-down actions. short, explained follows.wheel needs mounted hub, expressed on(?r, ?h) goal. mountwheel, hub must jacked up. mounting, nuts done up. Then, hubneeds jacked again, order tighten nuts achieving tight(?n, ?h) goal.Now, GAM puts goals one entry preceeding tight goals. Thus, solvingentry containing goals, hub jacked up, wheel put on, hubimmediatly jacked order replace next wheel. Afterwards, solvingtight goals, hub must jacked up|and down|one timenuts. Solving problem manner, planner inserts one super uous jack-up,one super uous jack-down action wheel. precisely, super uous actionsinserted one wheel, namely wheel last mounted solvinggoals. mounting wheel, goals achieved, planner proceedsnext agenda entry wheel still jacked up. Then, trying achievetight goals, IPP recognizes shortest plan (in terms number parallel steps)results nuts first done hub already jacked up. Thus, hubjacked one time, achieving corresponding goal, jacked onetime, achieving tight goal.case 3 tires, following goal subsets identified ordered:1:2:3:4:5:6:7:ated(r3), ated(r2), ated(r1)on(r3, hub3), on(r1, hub1), on(r2, hub2)tight(n2, hub2), tight(n3, hub3), tight(n1, hub1)in(w3, boot), in(pump, boot), in(w1, boot), in(w2, boot)in(jack, boot)in(wrench, boot)closed(boot)hardest subproblem agenda achieve on(r ; hub ) goals entry 2,i.e., mount ated spare wheels various hubs. Trying generate maximum parallelized plan impossible IPP 3 tires. since goals completelyindependent other, linearization perfectly work. resultingplans become slightly longer due way tight goals achieved using-L option. noticed earlier one wheel (the one last mountedsolving goals) super uous jack-up jack-down actions need insertedplan. Linearizing agenda entries, super uous jack-up jack-down actions mustlikely inserted wheels, yielding plans two steps longer. reasontight goal might first linearization. likely,tight goal corresponding hub still jacked up, planner needsinsert one super uous jack-down action here. Later, must jack hub again, yieldinganother super uous action. Using +G+R+L case 10 tires, 2662 actions needtried plan 136 actions found, takes 0.08 s. GAM requires 0.55 s,RIFO requires 14.42 s, 1.74 consumed generate planning graphs, 0.08spent compute initial states subproblems. remaining 0.02 consumedparsing instantiating.376fiOn Reasonable Forced Goal Orderings5.2.3 Tower Hanoisurprising result obtained tower hanoi domain. domain, stack discsmoved one peg third peg auxiliary second peg them,never larger disc put onto smaller disc. case three discs d1, d2, d3increasing size, goals stated on(d3,peg3), on(d2,d3), on(d1,d2). GAM returnsfollowing agenda, correctly ects ordering largest disc needsput goal position first.1: on(d3,peg3)2: on(d2,d3)3: on(d1,d2)goal agenda leads partition subproblems corresponds recursiveformulation problem solving algorithm, i.e., solve problem n discs,planner first solve problem n 1 discs, etc. first entry, plan 4actions (time steps 0 3 below) generated, achieves goal on(d3,peg3).8plan 2 actions (time steps 4 5) achieves goals on(d3,peg3) on(d2,d3)on(d3,peg3) holding already initial state. Finally, one-step plan (time step 6)generated moves third disc two discs already goalposition.timetimetimetimestepstepstepstep0:1:2:3:move(d1,d2,peg3)move(d2,d3,peg2)move(d1,peg3,d2)move(d3,peg1,peg3)time step 4: move(d1,d2,peg1)time step 5: move(d2,peg2,d3)time step 6: move(d1,peg1,d2)Surprisingly, IPP able benefit information, runtime IPP usingGAM exploding dramatically increasing numbers discs, see Figure 13.discs #actions IPP IPP +GUCPOPUCPOP subproblems221 0.020.02 0.12 (27)0.06 (17) + 0.02 (6)348 0.080.07 8.00 (2291) 0.18 (48) + 0.06 (13) + 0.01 (6)490 0.330.255150 1.573.106231 9.7188.457336 69.44 2339.94Figure 13: Runtimes IPP without goal agenda hanoi problems compared UCPOP without agenda UCPOP agenda subproblems usingZLIFO ibf control strategy.8. move action takes first argument disc moved, second disc moved,third argument disc peg moved.377fiKoehler & Hoffmannable provide explanation phenomenon, divisionsubproblems causes much larger search space planner although solutionplans result. RIFO cannot improve situation selects actions relevant.tower hanoi domain one found IPP's performance deteriorated GAM. currently see way one tell advance whether IPPgain advantage using GAM not. overhead caused goal analysissmall, \inadequate" split goals subgoal sets leadsearch, see also Section 6.However case, phenomenon seems specific IPP. simulatedinformation provided GAM UCPOP obtained quite different picture.fifth column Figure 13 shows runtime UCPOP using ZLIFO (Pollack, Joslin,& Paolucci, 1997) ibf control strategy number explored partial plansparentheses. UCPOP solve problem 2 3 discs. last columnfigure, show runtime number explored partial plans, resultUCPOP run subproblems result agenda. exactlysubproblems IPP solve, performance UCPOP improvessignificantly. Instead taking 8 exploring 2291 partial plans, UCPOP takes0.18+0.06+0.01=0.25 explores 48+13+6=67 plans. Unfortunately, problemssubproblems 3 discs remain beyond performance UCPOP.performance improvement independent search strategies used UCPOP.example, ibf control used without ZLIFO, number explored partial plansreduced 78606 2209 case problem 3 discs. Runtime improves65 seconds 2 seconds. Similarly, using bf control without ZLIFOnumber explored partial plans reduces 1554 873.Knoblock (1994) also reports improvement performance Prodigy planner(Fink & Veloso, 1994) using abstraction hierarchy generated domainalpine module, provides essence information goal agenda.96. Summary Comparison Related WorkMany related approaches developed provide planner abilitydecompose planning problem giving kind goal ordering information. Subsequently, discuss important review work lightapproaches.method introduces preprocessing approach, derives total orderingsubsets goals performing static, heuristic analysis planning problem hand.approach works domains described STRIPS ADL operators basedpolynomial-time algorithms. purpose method provide plannersearch control, i.e., opt deriving goal achievement order successively callplanner totally ordered subsets goals.method preserves soundness planning system, completenesscase planning domain contain deadlocks. argue9. However, find goal ordering information, alpine requires represent tower hanoi domaininvolving several operators, cf. (Knoblock, 1991).378fiOn Reasonable Forced Goal Orderingsbenchmark domains quite often possess property, also supportedauthors (Williams & Nayak, 1997).computation requires polynomial time, methodsincomplete sense detect reasonable goal orderings generalcase. complexity deciding existence forced reasonable goal orderingsproven PSPACE-hard Section 2 therefore, trading completenesseciency seems acceptable solution. complexity results relate foundBylander (1992) proves PSPACE-completeness serial decomposability (Korf,1987). Given set subgoals, serial decomposability means previously satisfied subgoals need violated later solution path, i.e., subgoalachieved, remains valid goal reached. purpose method deriveconstraints make orderings explicit serial decomposability setgoals found, i.e., consider complementary problem, also ectedcomplexity proofs.many cases, found goal agenda manager significantly improveperformance IPP planning system, found least one domain, namelytower hanoi, dramatic decrease performance observed although IPPstill generates optimal plan processing ordered goals agenda.far, complexity results Backstrom Jonsson (1995) predicted planningabstraction hierarchies exponentially less ecient, exponentially longerplans generated.idea analyze effects preconditions operators derive orderingconstraints based interaction operators also found variety approaches.analyze harmful interactions operators method studying deleteeffects, approaches described (Dawsson & Siklossy, 1977; Korf, 1985; Knoblock,1994) concentrate positive interactions operators. successful matchingeffects preconditions forms basis learn macro-operators, see (Dawsson & Siklossy,1977; Korf, 1985).alpine system (Knoblock, 1994) learns abstraction hierarchies Prodigyplanner (Fink & Veloso, 1994). approach based ordering preconditionseffects operator, i.e., effects operator must abstractionhierarchy preconditions must placed lower level effects.introduces ordering possible subgoals domain, orthogonalordering compute: alpine, subgoal ordered subgoal Benables B , i.e., must possibly achieved first order achieve B . methodorders B cannot achieved without necessarily destroying B . resultalpine GAM set binary constraints. case alpine, constraintscomputed atoms domain, GAM restricts analysisgoals only. approaches represent binary constraints graph structure. alpinemerges atomic goals together belong strongly connected component graph.GAM merges sets goals together identical degree. computetopological sorting sets consistent constraints. resulting goalorderings quite similar examples Knoblock (1994) demonstrate, GAMapproximates reasonable goal orderings domains alpine fails finding abstractionhierarchies. Two examples (Knoblock, 1991) tower hanoi domain usinghe379fiKoehler & Hoffmannone move operator blocks world. domains, alpine cannot detectorderings investigates operator schemata, set ground actions,therefore cannot distinguish orderings different instantiationsliteral. Although alpine could modified handle ground actions, significantlyincrease amount computation requires. GAM hand, handles large setsground actions ecient way, particular direct analysis used.10analysis, quite similar alpine, performed frameworkHTN planning, described Tsuneto et al. (1998). approach analyzes externalconditions methods, cannot achieved decomposing method further.means, conditions established decomposition methods,precede method using external condition. Two strategies determinedecomposition order methods defined empirically compared. lies maindifference approaches described far: Instead trying automaticallyconstruct decomposition orderings, predefined fixed domainsproblems.Harmful interactions among operators studied Smith Peot (1993) Etzioni(1993). threat operator precondition p occurs instantiationeffects inconsistent p (Smith & Peot, 1993). knowledgethreats used control plan-space planner. contrast state-space plannerIPP, computing explicit ordering goals prevent presence threatspartial plan order goals processed determine orderactions occur plan. notion forced reasonable goal orderingscomparable threat threat still potential resolvedadding binding ordering constraints plans. contrast this, forcedreasonable goal ordering persists bindings enforces specific orderingsubgoals.Given planning problem, static (Etzioni, 1993) computes backchaining treegoals form AND/OR graph, subsequently analyzes occurrencegoal interactions necessarily occur. analysis much complicatedours, static deal uninstantiated operators axioms,describe properties legal states. result analysis goal ordering rules,order goals certain conditions satisfied state. main difference GAM,generates explicit goal orderings independently specific state. needextract conditions specific state satisfy considers generic states( : ) analysis, represents states satisfying A, B . GAM, staticincomplete sense cannot detect existing goal interactions. problemGAM deciding reasonable orderings PSPACE-hard, provenpaper. problem static compute necessary effects operatorgiven state. Etzioni (1993) conjectures Nebel Backstrom (1994) prove,A;B10. Abstraction hierarchies general goal orderings compute. cannot servepurpose providing planner goal ordering information, also allow generate plansdifferent levels refinement, see also (Bacchus & Yang, 1994). Two approaches generatingabstraction hierarchies based numerical criticality values found (Sacerdoti, 1974; Bundy,Giunchiglia, Sebastiani, & Walsh, 1996).380fiOn Reasonable Forced Goal Orderingsproblem computationally intractable therefore, polynomial-time analysis methodmust incomplete.Last, least quite number approaches late Eighties,focused directly subgoal orderings. fall two categories: approachesdescribed (Drummond & Currie, 1989; Hertzberg & Horz, 1989) focus detectioncon icts caused goal interdependencies guide partial-order planner search.investigate approaches detail extract explicitgoal orderings preprocess planning do. works described (Irani & Cheng,1987; Cheng & Irani, 1989; Joslin & Roach, 1990) implement preprocessing approaches,perform structural analysis planning task determine appropriate goalordering planning starts. Irani Cheng (1987) compute relationpairs goals, which|roughly speaking|orders goal goal B B mustachieved achieved. formalism rather complicated theoreticalproperties relation investigated. (Cheng & Irani, 1989), approachextended sets goals ordered respect other. exactproperties formalism remain unclear. (Joslin & Roach, 1990), graph-theoreticalapproach described generates graph atoms given domain descriptionnodes draws arc node node B operator exists takesprecondition B effect. assuming operators inversecounterparts, identifying connected components graph proposed meansorder goals. approach unlikely scale size problem spaces today's plannersconsider also completely outdated terms terminology.Finally, one wonder reasonable forced goal orderings relate othersdefined literature. one attempt know orderingrelation explicitly defined properties studied, see (Hullem et al., 1999).paper, notion necessary goal orderings introduced, must trueminimal solution plans (Kambhampati, 1995).11 approach extends operator graphs(Smith & Peot, 1993) orders goal based three criteria called goal subsumption, goalclobbering, precondition violation. Goal subsumption < B holds every solution planachieving goal B state also achieves goal state s0 preceding s, planachieving one goals G n fAg deletes A. Goal clobbering holds solution plandeletes B thus, < B . Precondition violation holds solution B resultsdeadlock cannot reached anymore, i.e., < B . compositecriterion defined tests three criteria simultaneously.12 goal necessarilyordered B satisfies composite criterion.remark precondition violation seems equivalent forced orderingsintroduced, goal clobbering appears similar reasonable orderings.possible us verify conjecture authors (Hullem et al., 1999) giveexact formal definitions. nothing similar goal subsumption arguecriterion rarely satisfied natural problems: goal achieved every11. plan minimal contains subplan also solution plan. remark minimalitymean shortest plans least number actions considered. fact, minimalplans highly non-optimal long action truly super uous.12. Here, authors precise mean this. argue meanstwo goals ordered satisfy least one criteria.381fiKoehler & Hoffmannsolution goal B anyway, goal removed goal set withoutchanging planning task.authors report able detect necessary orderings artificialdomains , cf. (Barrett & Weld, 1994), fail typical benchmark domainsblocks world tyreworld. reason seems operator graphsrepresent possible instantiations operator schemes. authors claim,makes operator graph analysis ecient. However, heuristic orderingintroduced paper also takes almost computation time, succeeds findinggoal orderings domains.h7. OutlookThree promising avenues future research following:First, one imagine goal ordering information also used searchprocess, i.e., ordering original goal set, also goals emergesearch. major challenge seems balance effort computing goalordering information savings result search process. Oneeasily imagine ordering goal sets ever generated become quite costlyinvestment without yielding major benefit planner.Secondly, refinement goal agenda additional subgoals another interesting future line work. first investigation using so-called intermediate goals (thesefacts planner must make true achieve original goal)explored inside GAM results reported (Koehler & Hoffmann, 1998). Earlierwork addressing task learning intermediate goals found (Ruby & Kibler,1989), problem focus AI planning research since then.third line work addresses interaction GAM forward-searching planning system. seen GAM preserves correctness planner,preserves completeness least deadlock-free planning domains. alsoseen, however, solution plans using GAM get longer, i.e., GAM preserve optimality planner. Recently, planning systems deliver plansguaranteed optimality demonstrated impressive performance terms runtimeplan length, e.g., HSP, first mentioned (Bonet, Loerincs, & Geffner, 1997),GRT (Refanidis & Vlahavas, 1999), particular ff (Hoffmann, 2000). systemsheuristic-search planners searching forward state space non-admissible,informative heuristics.ff planning system developed one authors awarded \GroupDistinguished Performance Planning System" also Schindler Awardbest performing planning system Miconic 10 Elevator domain (ADL track)AIPS 2000 planning competition. integration goal agenda techniquesplanner one factors enabled excellent behavior ff competition:crucial scaling blocks world problems 50 blocks, helped factor 2schedule Miconic 10, never slowed algorithm.Forward state-space search quite natural framework driven goal agenda:Simply let planner solve subproblem, start next search statelast search ended. Even appealing, heuristic forward-search planners deeper382fiOn Reasonable Forced Goal Orderingskind interaction GAM example graphplan-style planners. additionsmaller problems facing using goal agenda, heuristicsuenced employ techniques estimating goal distance state.using goal agenda, different goal sets result stage planning processtherefore, goal-distance estimate different, too. Currently heuristic deviceinside ff search algorithm developed, knows drivengoal agenda, access complete set goals. informationused prune unpromising branches search space discoverscurrently achieved goals probably destroyed reachieved later on.ReferencesAllen, J. (Ed.), AIPS-98 (1998). Proceedings 4th International Conference Artificial Intelligence Planning Systems. AAAI Press, Menlo Park.Anderson, C., & Weld, D. (1998). Conditional effects Graphplan. Allen (Allen, 1998),pp. 44{53.Bacchus, F., & Yang, Q. (1994). Downward refinement eciency hierarchicalproblem solving. Artificial Intelligence, 71, 43{100.Backstrom, C., & Jonsson, P. (1995). Planning abstraction hierarchies exponentially less ecient. Mellish (Mellish, 1995), pp. 1599{1604.Barrett, A., & Weld, D. (1994). Partial-order planning: Evaluating possible eciency gains.Artificial Intelligence, 67, 71{112.Blum, A., & Furst, M. (1997). Fast planning planning graph analysis. ArtificialIntelligence, 90 (1{2), 279{298.Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanism planning. Proceedings 14th National Conference AmericanAssociation Artificial Intelligence, pp. 714{719.Bundy, A., Giunchiglia, F., Sebastiani, R., & Walsh, T. (1996). Computing abstractionhierarchies numerical simulation. Weld, & Clancey (Weld & Clancey, 1996), pp.523{529.Bylander, T. (1992). Complexity results serial decomposability. Proceedings10th National Conference American Association Artificial Intelligence, pp.729{734 San Jose, CA. MIT Press.Bylander, T. (1994). computational complexity propositional STRIPS planning.Artificial Intelligence, 69, 165{204.Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32 (3), 333{377.Cheng, J., & Irani, K. (1989). Ordering problem subgoals. Sridharan (Sridharan, 1989),pp. 931{936.383fiKoehler & HoffmannDawsson, C., & Siklossy, L. (1977). role preprocessing problem solving systems.Proceedings 5th International Joint Conference Artificial Intelligence, pp.465{471 Cambridge, MA.Dimopoulos, Y., Nebel, B., & Koehler, J. (1997). Encoding planning problems nonmonotonic logic programs. Steel (Steel, 1997), pp. 169{181.Drummond, M., & Currie, K. (1989). Goal ordering partially ordered plans. Sridharan(Sridharan, 1989), pp. 960{965.El-Kholy, A., & Richards, B. (1996). Temporal resource reasoning planning:parcPLAN approch. Wahlster, W. (Ed.), Proceedings 12th European Conference Artificial Intelligence, pp. 614{618. John Wiley & Sons, Chichester, NewYork.Etzioni, O. (1993). Acquiring search-control knowledge via static analysis. Artificial Intelligence, 62, 255{301.Fink, E., & Veloso, M. (1994). Prodigy planning algorithm. Technical report CMU-94-123,Carnegie Mellon University.Fox, M., & Long, D. (1998). automatic inference state invariants TIM. JournalArtificial Intelligence Research, 9, 367{421.Fox, M., & Long, D. (1999). Ecient implementation plan graph STAN. JournalArtificial Intelligence Research, 10, 87{115.Hertzberg, J., & Horz, A. (1989). Towards theory con ict detection resolutionnonlinear plans. Sridharan (Sridharan, 1989), pp. 937{942.Hoffmann, J. (2000). heuristic domain independent planning use enforcedhill-climbing algorithm. 12th International Symposium Methods IntelligentSystems.Hullem, J., Munoz-Avila, H., & Weberskirch, F. (1999). Extracting goal orderingsimprove partial-order Graphplan-based planning. Technical report, UniversityKaiserslautern.Irani, K., & Cheng, J. (1987). Subgoal ordering goal augmentation heuristic problem solving. McDermott, D. (Ed.), Proceedings 10th International JointConference Artificial Intelligence, pp. 1018{1024 Milan, Italy. Morgan Kaufmann.Jonsson, P., Haslum, P., & Backstrom, C. (2000). Towards ecient universal planning:randomized approach. Artificial Intelligence, 117 (1), 1{29.Joslin, D., & Roach, J. (1990). theoretical analysis conjunctive-goal problems. ArtificialIntelligence, 41, 97{106.Kambhampati, S. (1995). Admissible pruning strategies based plan minimality planspace planning. Mellish (Mellish, 1995), pp. 1627{1633.384fiOn Reasonable Forced Goal OrderingsKautz, H., & Selman, B. (1996). Pushing envelope: Planning, propositional logic,stochastic search. Weld, & Clancey (Weld & Clancey, 1996), pp. 1194{1201.Knoblock, C. (1991). Automatically Generating Abstractions Problem Solving. Ph.D.thesis, Carnegie Mellon University.Knoblock, C. (1994). Automatically generating abstractions planning. Artificial Intelligence, 68 (2), 243{302.Koehler, J. (1999). Handling conditional effects negative goals IPP. Technical report 128, University Freiburg, Institute Computer Science. availablehttp://www.informatik.uni-freiburg.de/~ koehler/ipp.html.Koehler, J., & Hoffmann, J. (1998). Planning goal agendas. Technical report110, University Freiburg. available http://www.informatik.uni-freiburg.de/~koehler/ipp.html.Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphsADL subset. Steel (Steel, 1997), pp. 273{285.Korf, R. (1985). Macro-operators: weak method learning. Artificial Intelligence, 26,35{77.Korf, R. (1987). Planning search: quantitative approach. Artificial Intelligence, 33,65{88.McDermott, D., & Hanks, S. (1987). Nonmonotonic logic temporal projection. ArtificialIntelligence, 33, 379{412.Mellish, C. (Ed.), IJCAI-95 (1995). Proceedings 14th International Joint ConferenceArtificial Intelligence. Morgan Kaufmann, San Francisco, CA.Nebel, B., & Backstrom, C. (1994). computational complexity temporal projection, planning, plan validation. Journal Artificial Intelligence, 66 (1), 125{160.Nebel, B., Dimopoulos, Y., & Koehler, J. (1997). Ignoring irrelevant facts operatorsplan generation. Steel (Steel, 1997), pp. 338{350.Nilsson, N. (1980). Principles Artificial Intelligence. Tioga Publishing Company, PaloAlto.Pednault, E. (1989). ADL: Exploring middle ground STRIPS SituationCalculus. Brachman, R., Levesque, H., & Reiter, R. (Eds.), Proceedings 1stInternational Conference Principles Knowledge Representation Reasoning,pp. 324{332 Toronto, Canada. Morgan Kaufmann.Penberthy, J., & Weld, D. (1992). UCPOP: sound, complete, partial order plannerADL. Nebel, B., Swartout, W., & Rich, C. (Eds.), Proceedings 3rdInternational Conference Principles Knowledge Representation Reasoning,pp. 103{113. Morgan Kaufmann, San Mateo.385fiKoehler & HoffmannPollack, M., Joslin, D., & Paolucci, M. (1997). Selection strategies partial-order planning.Journal Artificial Intelligence Research, 6, 223{262.Refanidis, I., & Vlahavas, I. (1999). GRT: domain independent heuristic STRIPSworlds based greedy regression tables. Proceedings 5th European Conference Planning, pp. 346{358.Regli, W., Gupta, S., & Nau, D. (1995). AI planning versus manufactoring-operationplanning: case study. Mellish (Mellish, 1995), pp. 1670{1676.Ruby, D., & Kibler, D. (1989). Learning subgoal sequences planning. Sridharan(Sridharan, 1989), pp. 609{615.Russel, S., & Norvig, P. (1995). Artificial Intelligence - modern Approach. Prentice Hall.Sacerdoti, E. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence,5, 115{135.Sanborn, J., & Hendler, J. (1988). Near-term event projection dynamic simulationrobot cross road? Proceedings 2nd Conference AISimulation.Smith, D., & Peot, M. (1993). Postponing threats partial-order planning. Proceedings11th National Conference American Association Artificial Intelligence,pp. 500{506. AAAI Press, MIT Press.Sridharan, N. (Ed.), IJCAI-89 (1989). Proceedings 11th International Joint Conference Artificial Intelligence, Detroit, MI. Morgan Kaufmann.Steel, S. (Ed.), ECP-97 (1997). Proceedings 4th European Conference Planning,Vol. 1348 LNAI. Springer.Tsuneto, R., Hendler, J., & Nau, D. S. (1998). Analyzing external conditions improveeciency HTN planning. Allen (Allen, 1998), pp. 913{920.Veloso, M., & Blythe, J. (1994). Linkability: Examining causal link commitments partialorder planning. Hammond, K. (Ed.), Proceedings 2nd International Conference Artificial Intelligence Planning Systems, pp. 170{175. AAAI Press, MenloPark.Warshall, J. (1962). theorem boolean matrices. Journal ACM, 9 (1), 11{12.Weld, D., & Clancey, B. (Eds.)., AAAI-96 (1996). Proceedings 14th National Conference American Association Artificial Intelligence. AAAI Press.Williams, B., & Nayak, R. (1997). reactive planner model-based executive.Proceedings 15th International Joint Conference Artificial Intelligence, pp.1178{1185. Morgan Kaufmann, San Francisco, CA.386fiJournal Artificial Intelligence Research 12 (2000) 219-234Submitted 5/99; published 5/00Randomized Algorithms Loop Cutset ProblemAnn BeckerReuven Bar-YehudaDan Geigeranyuta@cs.technion.ac.ilreuven@cs.technion.ac.ildang@cs.technion.ac.ilComputer Science DepartmentTechnion, Haifa, 32000, IsraelAbstractshow find minimum weight loop cutset Bayesian network highprobability. Finding loop cutset first step method conditioninginference. randomized algorithm finding loop cutset outputsminimum loopcutset O(c 6k kn) steps probability least 1 ; (1 ; 61k )c6k , c > 1constant specified user, k minimal size minimum weight loop cutset,n number vertices. also show empirically variant algorithm oftenfinds loop cutset closer minimum weight loop cutset ones foundbest deterministic algorithms known.1. Introductionmethod conditioning well known inference method computation posterior probabilities general Bayesian networks (Pearl, 1986, 1988; Suermondt & Cooper,1990; Peot & Shachter, 1991) well finding MAP values solving constraint satisfaction problems (Dechter, 1999). method two conceptual phases. First findoptimal close optimal loop cutset perform likelihood computationinstance variables loop cutset. method routinely usedgeneticists via several genetic linkage programs (Ott, 1991; Lang, 1997; Becker, Geiger, &Schaffer, 1998). variant method developed Lange Elston (1975).Finding minimum weight loop cutset NP-complete thus heuristic methodsoften applied find reasonable loop cutset (Suermondt & Cooper, 1990).methods past guarantee performance performed badlypresented appropriate example. Becker Geiger (1994, 1996) offered algorithmfinds loop cutset logarithm state space guaranteedconstant factor optimal value. adaptation approximation algorithmsincluded version 4.0 FASTLINK, popular software analyzing largepedigrees small number genetic markers (Becker et al., 1998). Similar algorithmscontext undirected graphs described Bafna, Berman, Fujito (1995)Fujito (1996).approximation algorithms loop cutset problem quite useful, stillworthwhile invest finding minimum loop cutset rather approximation cost finding loop cutset amortized many iterationsconditioning method. fact, one may invest effort complexity exponential sizeloop cutset finding minimum weight loop cutset second phaseconditioning algorithm, repeated many iterations, uses procedurec 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiBecker, Bar-Yehuda, & Geigercomplexity. considerations apply also constraint satisfaction problems wellproblems method conditioning useful (Dechter, 1990, 1999).paper describe several randomized algorithms compute loop cutset.done Bar-Yehuda, Geiger, Naor, Roth (1994), solution based reductionweighted feedback vertex set problem. feedback vertex set (FVS) F setvertices undirected graph G = (V; E ) removing F G, alongedges incident F , set trees obtained. Weighted Feedback Vertex Set(WFVS) problem find feedbackvertex set F vertex-weighted graph weightPfunction w : V ! IR+ , v2F w(v ) minimized. w(v ) 1, problemcalled FVS problem. decision version associated FVS problem knownNP-Complete (Garey & Johnson, 1979, pp. 191{192).randomized algorithm finding WFVS, called RepeatedWGuessI, outputsk1kc6minimum weight FVS O(c 6 kn) steps probability least 1 ; (1 ; 6k ) ,c > 1 constant specified user, k minimal size minimum weight FVS,n number vertices. unweighted graphs present algorithm findskminimum FVS graph G O(c 4kkn) steps probability least 1 ; (1 ; 41k )c4 .comparison, several deterministic algorithms finding minimum FVS describedliterature. One complexity O((2k + 1)k n2 ) (Downey & Fellows, 1995b)others complexity O((17k4)!n) (Bodlaender, 1990; Downey & Fellows, 1995a).final variant randomized algorithms, called WRA, best performanceutilizes information previous runs. algorithm harder analyzeinvestigation mostly experimental. show empirically actual run timeWRA comparable Modified Greedy Algorithm (MGA), described BeckerGeiger (1996), best available deterministic algorithm finding close optimalloop cutsets, yet, output WRA often closer minimum weight loop cutestoutput MGA.rest paper organized follows. Section 2 outline methodconditioning, explain related loop cutset problem describe reductionloop cutset problem WFVS Problem. Section 3 present three randomized algorithms WFVS problem analysis. Section 4 compare experimentallyWRA MGA respect output quality run time.2. Background: Loop Cutset Problemshort overview method conditioning definitions related Bayesian networksgiven below. See book Pearl (1988) details. define loopcutset problem.Let P (u1 ; : : :; un) probability distribution variable ui finite setpossible values called domain ui. directed graph directed cyclescalled Bayesian network P 1{1 mapping fu1; : : :; ung verticesD, ui associated vertex P written follows:P (u1; : : :; un) =ni=1P (ui j ui ; : : :; uij )1( )i1 ; : : :; ij (i) source vertices incoming edges vertex D.220(1)fiRandomized Algorithms Loop Cutset ProblemSuppose variables fv1; : : :; vl g among fu1; : : :; ung assigned specificvalues fv1; : : :; vlg respectively. updating problem compute probability P (ui jv1 = v1; : : :; vl = vl ) = 1; : : :; n.trail Bayesian network subgraph whose underlying graph simple path.vertex b called sink respect trail exist two consecutive edges ! bb c t. trail active set vertices Z (1) every sink respecteither Z descendant Z (2) every vertex along outside Z .Otherwise, trail said blocked (d-separated) Z .Verma Pearl proved Bayesian network P (u1 ; : : :; un) trailsvertex fr1; : : :; rl g vertex fs1 ; : : :; sk g blocked ft1 ; : : :; tm g,corresponding sets variables fur ; : : :; url g fus ; : : :; usk g independentconditioned fut ; : : :; utm g (Verma & Pearl, 1988). Furthermore, Geiger Pearl provedresult cannot enhanced (Geiger & Pearl, 1990). results presentedextended Geiger, Verma, Pearl (1990).Using close relationship blocked trails conditional independence, KimPearl developed algorithm update-tree solves updating problemBayesian networks every two vertices connected one trail (Kim& Pearl, 1983). Pearl solved updating problem Bayesian network follows (Pearl, 1986). First, set vertices selected two verticesnetwork connected one active trail [ Z , Z subset vertices. Then, update-tree applied combination value assignmentsvariables corresponding , and, finally, results combined. algorithmcalled method conditioning complexity grows exponentially size. set called loop cutset. Note domain size variables varies,update-tree called number times equal product domain sizesvariables whose corresponding vertices participate loop cutset. takelogarithm domain size (number values) weight vertex, findingloop cutset sum vertices weights minimum optimizes Pearl's updatingalgorithm case domain sizes may vary.give alternative definition loop cutset provide probabilisticalgorithm finding it. definition borrowed paper Bar-Yehuda et al.(1994). underlying graph G directed graph undirected graph formedignoring directions edges D. cycle G path whose two terminalvertices coincide. loop subgraph whose underlying graph cycle.vertex v sink respect loop ; two edges adjacent v ; directedv . Every loop must contain least one vertex sink respectloop. vertex sink respect loop ; called allowed vertexrespect ;. loop cutset directed graph set vertices containsleast one allowed vertex respectPto loop D. weight set vertices Xdenoted w(X ) equal v2X w(v ) w(x) = log(jxj) jxj sizedomain associated vertex x. minimum weight loop cutset weighted directedgraph loop cutset F w(F ) minimum loop cutsets G.Loop Cutset Problem defined finding minimum weight loop cutset givenweighted directed graph D.112211fiBecker, Bar-Yehuda, & Geigerapproach take reduce loop cutset problem weighted feedbackvertex set problem, done Bar-Yehuda et al. (1994). define weightedfeedback vertex set problem reduction.Let G = (V; E ) undirected graph, let w : V ! IR+ weight functionvertices G. feedback vertex set G subset vertices F Vcycle G passes least one vertex F . words, feedback vertexset F set vertices G removing F G, along edgesincident F , obtain set trees (i.e., aPforest). weight set vertices Xdenoted (as before) w(X ) equal v2X w(v ). minimum feedback vertex setweighted graph G weight function w feedback vertex set F Gw(F ) minimum feedback vertex sets G. Weighted Feedback Vertex Set(WFVS) Problem defined finding minimum feedback vertex set given weightedgraph G weight function w.reduction follows. Given weighted directed graph (D; w) (e.g., Bayesiannetwork), define splitting weighted undirected graph Ds weight function wsfollows. Split vertex v two vertices v v Ds incomingedges v become undirected incident edges v Ds , outgoing edgesv become undirected incident edges v Ds. addition, connect vv Ds undirected edge. set ws (v ) = 1 ws (v ) = w(v ). setvertices X Ds , define (X ) set obtained replacing vertex v vX respective vertex v vertices originated. NoteXcycle Ds , (X ) loop D, loop D, ;1(Y ) = v2Y ;1(v )cycle Ds8>v sink< v;1(v ) = vv source>: fv ; v g otherwise(A vertex v source respect loop two edges adjacent v originatev ). mapping loops cycles Ds one-to-one onto.algorithm easily stated.ALGORITHM LoopCutsetInput: Bayesian networkOutput: loop cutset1. Construct splitting graph Dsweight function ws2. Find feedback vertex set F (Ds; ws)using Weighted Randomized Algorithm (WRA)3. Output (F ).immediately seen WRA (developed later sections) outputs feedbackvertex set F Ds whose weight minimum high probability, (F ) loopcutset minimum weight probability. observation holds dueone-to-one onto correspondence loops cycles DsWRA never chooses vertex infinite weight.222fiRandomized Algorithms Loop Cutset Problem3. Algorithms WFVS ProblemRecall feedback vertex set G subset vertices F V cycleG passes least one vertex F . Section 3.1 address problemfinding FVS minimum number vertices Sections 3.2 3.3 addressproblem finding FVS minimum weight. Throughout, allow Gparallel edges. two vertices u v parallel edges them, every FVSG includes either u, v , both.3.1 Basic Algorithmssection present randomized algorithm FVS problem. First introduceadditional terminology notation. Let G = (V; E ) undirected graph.degree vertex v G, denoted d(v ), number vertices adjacent v .self-loop edge two endpoints vertex. leaf vertex degreeless equal 1, linkpoint vertex degree 2 branchpoint vertexdegree strictly higher 2. cardinality set X denoted jX j.graph called rich every vertex branchpoint self-loops. Givengraph G, repeatedly removing leaves, bypassing edge every linkpoint,graph G0 obtained size minimum FVS G0 G equalevery minimum FVS G0 minimum FVS G. Since every vertex involvedself-loop belongs every FVS, transform G0 rich graph Gr addingvertices involved self loops output algorithm.algorithm based observation pick edge random richgraph probability least 1=2 least one endpoint edge belongsgiven FVS F . precise formulation claim given Lemma 1 whose proofgiven implicitly Voss (1968, Lemma 4).Lemma 1 Let G = (V; E ) rich graph, F feedback vertex set G X = V n F .Let EX denote set edges E whose endpoints vertices X EF;X denoteset edges G connect vertices F vertices X . Then, jEX j jEF;X j.Proof. graph obtained deleting feedback vertex set F graph G(V; E )forest vertices X = V n F . Hence, jEX j < jX j. However, vertex Xbranchpoint G, so,3 jX jXv 2Xd(v ) = jEF;X j + 2 jEX j:Thus, jEX j jEF;X j. 2Lemma 1 implies picking edge random rich graph, leastlikely pick edge EF;X edge EX . Consequently, selecting vertexrandom randomly selected edge probability least 1=4 belongminimum FVS. idea yields simple algorithm find FVS.223fiBecker, Bar-Yehuda, & GeigerALGORITHM SingleGuess(G,j)Input: undirected graph G0 integer j > 0.Output: feedback vertex set F size j , "Fail" otherwise.= 1; : : :; j1. Reduce Gi;1 rich graph Giplacing self loop vertices F .2. Gi empty graph Return F3. Pick edge e = (u; v ) random Ei4. Pick vertex vi random (u; v )5. F F [ fvig6. V V n fvi gReturn "Fail"Due Lemma 1, SingleGuess(G; j ) terminates FVS size j ,probability least 1=4j output minimum FVS.Note steps 3 4 SingleGuess determine vertex v first selectingarbitrary edge selecting arbitrary endpoint edge. equivalent wayachieving selection rule choose vertex probability proportionaldegree:p(v) = P d(vd)(u) = 2d(jvE) ju2Vsee equivalence two selection methods, define ;(v ) set edges whoseone endpoint v , note graphs without self-loops,XXp(v) =p(v je) p(e) = 12p(e) = 2d(jvE) je2;(v)e2;(v)equivalent phrasing selection criterion easier extend weighted caseused following sections.algorithm finding minimum FVS high probability, call RepeatedGuess, described follows: Start j = 1. Repeat SingleGuess c 4jtimes c > 1 parameter defined user. one iterations FVSsize j found, output FVS, otherwise, increase j one continue.ALGORITHM RepeatedGuess(G,c)Input: undirected graph Gconstant c > 1.Output: feedback vertex set F .j = 1; : : :; jV jRepeat c 4j times1. F SingleGuess(G; j )2. F "Fail" Return FEnd fRepeatgEnd fForg224fiRandomized Algorithms Loop Cutset Problemmain claims algorithms given following theorem.Theorem 2 Let G undirected graph c 1 constant. Then, SingleGuess(G; k)outputs FVS whose expected size 4k, RepeatedGuess(G;c) outputs,O(c 4kkn) steps, minimum FVS probability least 1 ; (1 ; 41k )c4k , ksize minimum FVS n number vertices.claims probability success number steps follow immediatelyfact probability success SingleGuess(G; j ) least (1=4)jthat, case success, O(cP4j ) iterations performed taking O(jn) steps. resultfollows fact kj=1 j 4j order O(k4k ). proof expected sizesingle guess presented next section.Theorem 2 shows guess produces FVS which, average,far minimum, enough iterations, algorithm convergesminimum high probability. weighted case, discussed next, managedachieve two guarantees separate algorithms, unable achieveguarantees single algorithm.3.2 Weighted Algorithmsturn weighted FVS problem (WFVS) size k find feedbackvertex set F vertex-weighted graph (G; w), w : V ! IR+ , size less equal kw(F ) minimized.Note weighted FVS problem cannot replace linkpoint vedge v weight lighter branchpoint neighbors v participateminimum weight FVS size k.graph called branchy endpoints, self loops, and, addition,linkpoint connected branchpoints (Bar-Yehuda, Geiger, Naor, & Roth, 1994).Given graph G, repeatedly removing leaves, bypassing edge everylinkpoint neighbor equal lighter weight, graph G0 obtainedweight minimum weight FVS (of size k) G0 G equal everyminimum WFVS G0 minimum WFVS G. Since every vertex self-loopbelongs every FVS, transform G0 branchy graph without self-loops addingvertices involved self loops output algorithm.address WFVS problem offer two slight modifications algorithm SingleGuess presented previous section. first algorithm, call SingleWGuessI, identical SingleGuess except iteration make reduction branchy graph instead reduction rich graph.chooses vertexPprobability proportional degree using p(v ) = d(v )= u2V d(u). Noteprobability take weight vertex account. second algorithm,call SingleWGuessII, chooses vertex probability proportional ratiodegree weight,X(2)p(v ) = wd((vv)) = wd((uu)) :u2V225fiBecker, Bar-Yehuda, & GeigerALGORITHM SingleWGuessI(G,j)Input: undirected weighted graph G0integer j > 0.Output: feedback vertex set F size j ,"Fail" otherwise.= 1; : : :; j1. Reduce Gi;1 branchy graph Gi (Vi; Ei)placing self loop vertices F .2. Gi empty graph Return F3. Pick vertex vi 2 Vi randomPprobability pi(v ) = di(v )= u2Vi di(u)4. F F [ fvig5. V V n fvi gReturn "Fail"second algorithm uses Eq. 2 computing p(v ) Line 3. two algorithmsremarkably different guarantees performance. Version guarantees choosingvertex belongs given FVS larger 1=6, however, expected weightFVS produced version cannot bounded constant times weight minimumWFVS. Version II guarantees expected weight output bounded 6 timesweight minimum WFVS, however, probability converging minimumfixed number iterations arbitrarily small. first demonstrate viaexample negative claims. positive claims phrased precisely Theorem 3proven thereafter.Consider graph shown Figure 1 three vertices a,b c, correspondingweights w(a) = 6, w(b) = 3 w(c) = 3m, three parallel edges b,three parallel edges c. minimum WFVS F size 1 consistsvertex a. According Version II, probability choosing vertex (Eq. 2):p(a) = (1 + 1=m ) + 1arbitrarily small suciently large, probability choosing vertexarbitrarily small. Thus, probability choosing vertex Fcriterion d(v )=w(v ), done Version II, arbitrarily small. If, hand,Version used, probability choosing a; b, c 1=2; 1=4; 1=4, respectively.Thus, expected weight first vertex chosen 3=4 ( + + 4),weight minimum WFVS 6. Consequently, suciently large, expectedweight WFVS found Version arbitrarily larger minimum WFVS.algorithm repeated guesses, call RepeatedWGuessI(G; c; j )follows: repeat SingleWGuessI(G; j ) c 6j times, j minimal number verticesminimum weight FVS seek. FVS found size j , algorithm outputssize minimum WFVS larger j high probability, otherwise,outputs lightest FVS size less equal j among explored. followingtheorem summarizes main claims.226fiRandomized Algorithms Loop Cutset Problemw(c) = 3cw(a) = 6w(b) = 3mbFigure 1: minimum WFVS F = fag.Theorem 3 Let G weighted undirected graph c 1 constant.a) algorithm RepeatedWGuessI(G; c; kk ) outputs O(c 6kkn) steps minimumWFVS probability least 1 ; (1 ; 61k )c6 , k minimal size minimumweight FVS G n number vertices.b) algorithm SingleWGuessII(G,k) outputs feedback vertex set whose expectedweight six times weight minimum weight FVS.proof part requires preliminary lemma.Lemma 4 Let G = (V; E ) branchy graph, F feedback vertex set G X =V n F . Let EX denote set edges E whose endpoints vertices XEF;X denote set edges G connect vertices F vertices X . Then,jEX j 2 jEF;X j.Proof. Let X b set branchpoints X . replace every linkpoint Xedge neighbors, denote resulting set edges vertices X bb b . proof Lemma 1 showsEXb b vertices X b F EF;Xb b j:jEXb b j jEF;XSince every linkpoint X neighbors set X b [ F , following holds:b b j:jEX j 2 jEXb b j jEF;X j = jEF;XHence, jEX j 2 jEF;X j. 2immediate consequence Lemma 4 probability randomly choosingedge least one endpoint belongs FVS greater equal 1=3. Thus,selecting vertex random randomly selected edge probability least1=6 belong FVS. Consequently, algorithm terminatesc 6k iterations,k1WFVS size k, probability least 1 ; (1 ; 6k )c6 outputminimum WFVS size k. proves part (a) Theorem 3. Note since kknown advance, use RepeatedWGuessI(G; c; j ) increasing values jFVS found, say j=J. set found still possibleexists WFVS J vertices smaller weight one found.happens k > J . However, among WFVSs size J , algorithmfinds one minimum weight high probability.second part requires following lemma.227fiBecker, Bar-Yehuda, & GeigerLemma 5 Let G branchy graph F FVS G. Then,XXd(v ) 6 d(v):v2Vv 2FProof. Denote dY (v) number edges vertex v set vertices .Then,Xv2VDue Lemma 4,Consequently,d(v) =XXd(v ) =v 2FXXXdX (v) + dF (v) + d(v ):v2Xv 2Xv2FXv2Xv2Xd(v ) +dX (v) = 2jEX j 4jEF;X j = 4Xv2V(v ) 4Xv2XXv2XdF (v) +Xv2XdF (v ):(3)dF (v)+Xv2Fd(v) 6Xv 2Fd(v )claimed. 2prove part (b) Theorem 3 analyzing SingleWGuessII(G,k). RecallVi set vertices graph Gi iteration i, di (v ) degree vertex v Gi,vi vertex chosen iteration i. Furthermore, recall pi(v ) probabilitychoose vertex v iteration i.Pexpected weight Ei (w(v )) = v2Vi w(v ) pi (v ) chosen vertex iterationPkdenoted ai . Thus, due linearity expectation operator, E (w(F )) = i=1 ai,assuming jF j = k. define normalization constant iteration follows:="#X di (u) ;1u2Viw(u)Then, pi (v ) = dwi((vv))ai =Xv2ViXw(v) dwi((vv)) = di(v )v 2VLet F minimum FVS G Fi minimum weight FVS graph Gi .expected weight Ei(w(v )jv 2 Fi )) vertex chosen Fi iteration denotedbi. have,XXbi =w(v ) pi(v ) = di (v)v2Fiv2FiLemma 5, ai =bi 6 every i.228fiRandomized Algorithms Loop Cutset ProblemRecall definition F2 minimum FVS branchy graph G2 obtainedG1 n fv1g. get,E (w(F )) E1(w(v)jv 2 F1 )) + E (w(F2))right hand side expected weight output F assuming algorithmfinds minimum FVS G2 needs select one additional vertex, lefthand side unrestricted expectation. repeating argument get,E (w(F )) b1 + E (w(F2))UsingPai =Pbi maxi ai =bi 6,kXi=1biobtainE (w(F )) 6 E (w(F )):Hence, E (w(F )) 6 w(F ) claimed. 2proof SingleGuess(G; k) outputs FVS whose expected size4k (Theorem 2) k size minimum FVS analogous proofTheorem 3 following sense. assign weight 1 vertices replacereferenceLemma P5 reference following claim: F FVS rich graph G,Pv2V d(v ) 4 v2F d(v ). proof claim identical proof Lemma 5except instead using Lemma 4 use Lemma 1.3.3 Practical Algorithmprevious sections presented several algorithms finding minimum FVS highprobability. description algorithms geared towards analysis, ratherprescription programmer. particular, number iterations used withinRepeatedWGuessI(G; c; k) changed algorithm run j < k. featureallowed us regard call SingleWGuessI(G; j ) made RepeatedWGuessIindependent process. Furthermore, small probability long run evensize minimum FVS small.slightly modify RepeatedWGuessI obtain algorithm, termed WRA,suffer deficiencies. new algorithm works follows. RepeatSingleWGuessI(G; jV j) min(Max; c 6w(F )) iterations, w(F ) weightlightest WFVS found far Max specified constant determining maximumnumber iterations SingleWGuessI.ALGORITHM WRA(G; c; Max)Input: undirected weighted graph G(V; E ) constants Max c > 1Output: feedback vertex set FF SingleWGuessI (G; jV j)min(Max; c 6w(F )); 1;1. F 0 SingleWGuessI(G; jV j)2. w(F 0 ) w(F )229fiBecker, Bar-Yehuda, & GeigerjV j jE j values15 2515 2515 2525 5525 5525 5555 1252{62{82{102{62{82{102{10size MGA WRA Eq.3{6128173{678943{669047{1239527{1239707{120100 017{220100 031652 17Figure 2: Number graphs MGA WRA yield smaller loop cutset. lastcolumn records number graphs two algorithms produced loopcutsets weight. line table based 100 graphs.F F 0;3. + 1;End fWhilegReturn Fmin(Max; c 6w(F ))Theorem 6 Max c6k, k minimal size minimum WFVS undi-rected weighted graph G, thenk WRA(G; c; Max) outputs minimum WFVS G probability least 1 ; (1 ; 61k )c6 .proof immediate corollary Theorem 3.choice Max c depend application. decision-theoretic approachselecting values any-time algorithms discussed Breese Horvitz (1990).4. Experimental Resultsexperiments compared outputs WRA vis-a-vis greedy algorithm GAmodified greedy algorithm MGA (Becker & Geiger, 1996) based randomly generatedgraphs real graphs contributed Hugin group (www.hugin.com).random graphs divided three sets. Graphs 15 vertices 25 edgesnumber values associated vertex randomly chosen 26, 2 8, 2 10. Graphs 25 vertices 55 edges numbervalues associated vertex randomly chosen 2 6, 2 8,2 10. Graphs 55 vertices 125 edges number valuesassociated vertex randomly chosen 2 10. instancethree classes based 100 random graphs generated described SuermondtCooper (1990). total number random graphs used 700.results summarized table Figure 2. WRA run Max = 300c = 1. two algorithms, MGA WRA, output loop cutsets size230fiRandomized Algorithms Loop Cutset ProblemName jV jWater 32Mildew 35Barley 48Munin1 189jEj jF j1238012636616142059GA MGA WRA40.7 42.7 29.548.1 40.5 39.372.1 76.3 57.3159.4 167.5 122.6Figure 3: Log size (base 2) loop cutsets found GA, MGA, WRA.17 graphs algorithms disagree, 95% graphs WRA performedbetter MGA.actual run time WRA(G; 1; 300) 300 times slower GA (or MGA)G. largest random graph used, took 4.5 minutes. time spendlast improvement WRA. Considerable run time saved letting Max = 5.700 graphs, WRA(G,1,5) already obtained better loop cutset MGA.largest improvement, Max = 300, weight 58.0 (log2 scale) weight35.9. improvements case obtained iterations 1, 2, 36, 83, 189respective weights 46.7, 38.8, 37.5, 37.3, 35.9 respective sizes 22, 18, 17, 18,17 nodes. average, 300 iterations, improvement larger 100 graphsweight 52 39 size 22 20. improvement smaller 600graphs weight 15 12.2 size 9 6.7.second experiment compared GA, MGA WRA four real Bayesiannetworks showing WRA outperformed GA MGA single call SingleWGuessI. weight output continued decrease logarithmicallynumber iterations. report results Max = 1000 c = 1. Run time3 minutes Water 15 minutes Munin1 Pentium 133 32M RAM.5. Discussionrandomized algorithm, WRA, incorporated popular genetic softwareFASTLINK 4.1 Alejandro Schaffer develops maintains softwareNational Institute Health. WRA replaced previous approximation algorithms findingFVS small Max value already matched improved FASTLINK 4.0datasets examined. datasets used comparison described Becker etal. (1998). main characteristics datasets collectedgeneticists, small number loops, large number values node(tens hundreds depending genetic analysis). networks methodconditioning widely used geneticists.leading inference algorithm, however, Bayesian networks clique-tree algorithm (Lauritzen & Spiegelhalter, 1988) developed several papers(Jensen, Lauritzen, & Olsen, 1990a; Jensen, Olsen, & Andersen, 1990b). networkspresented Table 3 conditioning feasible method clique tree algorithmused compute posterior probabilities networks. Furthermore,shown weight largest clique bounded weightloop cutset union largest parent set vertex Bayesian network implying231fiBecker, Bar-Yehuda, & Geigerclique tree algorithm always superior time performance conditioning algorithm(Shachter, Andersen, & Szolovits, 1994). two methods, however, combinedstrike balance time space requirements done within bucket eliminationframework (Dechter, 1999).algorithmic ideas behind randomized algorithms presented herein alsoapplied constructing good clique trees initial experiments confirm improvement deterministic algorithms often obtained. idea instead greedilyselecting smallest clique constructing clique tree, one would randomly selectnext clique according relative weights candidate cliques. remains developtheory behind random choices clique trees solid assessment presented.Currently, algorithm finding clique tree size guaranteedclose optimal high probability.Horvitz et al. (1989) show method conditioning useful approximateinference. particular, show rank instances loop cutset accordingprior probabilities assuming variables cutset marginally independent.conditioning algorithm run according ranking answerquery given interval shrinks towards exact solution instancesloop cutset considered (Horvitz, Suermondt, & Cooper, 1989; Horvitz, 1990).Applying idea without making independence assumptions described Darwiche(1994). maximal clique large store one still perform approximateinferences using conditioning algorithm.Acknowledgmentthank Se Naor fruitful discussions. Part work done thirdauthor sabbatical Microsoft Research. variant work presentedfifteenth conference uncertainty artificial intelligence, July 1999, Sweden.ReferencesBafna, V., Berman, P., & Fujito, T. (1995). Constant ratio approximations weightedfeedback vertex set problem undirected graphs. Proceedings Sixth AnnualSymposium Algorithms Computation (ISAAC95), pp. 142{151.Bar-Yehuda, R., Geiger, D., Naor, J., & Roth, R. (1994). Approximation algorithmsfeedback vertex set problems applications constraint satisfaction Bayesianinference. Proceedings 5th Annual ACM-Siam Symposium Discrete Algorithms, pp. 344{354.Becker, A., & Geiger, D. (1994). Approximation algorithms loop cutset problem.Proceedings 10th conference Uncertainty Artificial Intelligence, pp. 60{68.Becker, A., & Geiger, D. (1996). Optimization pearl's method conditioning greedylike approximation algorithms feedback vertex set problem. Artificial Intelligence, 83, 167{188.232fiRandomized Algorithms Loop Cutset ProblemBecker, A., Geiger, D., & Schaffer, A. (1998). Automatic selection loop breakersgenetic linkage analysis. Human Heredity, 48, 47{60.Bodlaender, H. (1990). disjoint cycles. International Journal Foundations Computer Science (IJFCS), 5, 59{68.Breese, J., & Horvitz, E. (1990). Ideal reformulation belief netwroks. Proceedings6th conference Uncertainty Artificial Intelligence, pp. 64{72.Darwiche, A. (1994). -bounded conditioning: method approximate updatingcausal networks. Research note, Rockwell Science Center.Dechter, R. (1990). Enhancement schemes constraint processing: backjumping, learning,cutset decomposition. Artificial Intelligence, 41, 273{312.Dechter, R. (1999). Bucket elimination: unifying framework structure-driven inference. Artificial Intelligence, appear.Downey, R., & Fellows, M. (1995a). Fixed-parameter tractability completeness I: Basicresults. SIAM Journal Computing, 24 (4), 873{921.Downey, R., & Fellows, M. (1995b). Parameterized computational feasibility. P. Clote, .J. R. (Ed.), Feasible Mathematics II, pp. 219{244. Birkhauser, Boston.Fujito, T. (1996). note approximation vertex cover feedback vertex setproblems - unified approach. Information Processing Letters, 59, 59{63.Garey, M., & Johnson, D. (1979). Computers Intractability: Guide TheoryNP-completeness. W. H. Freeman, San Francisco, California.Geiger, D., & Pearl, J. (1990). logic causal models. Uncertainty ArtificialIntelligence 4, pp. 3{14 New York. North-Holland.Geiger, D., Verma, T., & Pearl, J. (1990). Identifying independence bayesian networks.Networks, 20, 507{534.Horvitz, E. J. (1990). Computation action bounded resources. Ph.D dissertation,Stanford university.Horvitz, E. J., Suermondt, H. J., & Cooper, G. H. (1989). Bounded conditioning: Flexibleinference decisions scarce resources. Proceedings 5th conferenceUncertainty Artificial Intelligence, pp. 182{193. Morgan Kaufmann.Jensen, F., Lauritzen, S. L., & Olsen, K. (1990a). Bayesian updating causal probabilisiticnetworks local computations. Computational Statistics Quarterly, 4, 269{282.Jensen, F., Olsen, K., & Andersen, S. (1990b). algebra bayesian belief universesknowledge-based systems. Networks, 20, 637{659.Kim, H., & Pearl, J. (1983). computational model combined causal diagnostic reasoning inference systems. Proceedings Eighth International JointConference Artificial Intelligence (IJCAI83), pp. 190{193.233fiBecker, Bar-Yehuda, & GeigerLang, K. (1997). Mathematical statistical methods genetic analysis. Springer.Lange, K., & Elston, R. (1975). Extensions pedigree analysis. I. likelihood calculationsimple complex pedigrees. Human Heredity, 25, 95{105.Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphicalstructures application expert systems (with discussion). Journal RoyalStatistical Society, B, 50, 157{224.Ott, J. (1991). Analysis human genetic linkage (revised edition). Johns HopkinsUniversity Press.Pearl, J. (1986). Fusion, propagation structuring belief networks. Artificial Intelligence, 29, 241{288.Pearl, J. (1988). Probabilistic reasoning intelligent systems: Networks plausible inference. Morgan Kaufmann, San Mateo, California.Peot, M., & Shachter, R. (1991). Fusion propagation multiple observationsbelief networks. Artificial Intelligence, 48, 299{318.Shachter, R., Andersen, S., & Szolovits, P. (1994). Global conditioning probabilisticinference belief networks. Proceedings tenth conference UncertaintyArtificial Intelligence, pp. 514{522. Morgan Kaufmann.Suermondt, H., & Cooper, G. (1990). Probabilistic inference multiply connected beliefnetworks using loop cutsets. International Journal Approximate Reasoning, 4, 283{306.Verma, T., & Pearl, J. (1988). Causal networks: Semantics expressiveness. Proceedings 4th Workshop Uncertainty Artificial Intelligence, pp. 352{359.Voss, H. (1968). properties graphs containing k independent circuits. ProceedingsColloquium Tihany, pp. 321{334 New York. Academic Press.234fiJournal Artificial Intelligence Research 12 (2000) 93-103Submitted 11/99; published 3/00Exact Phase TransitionsRandom Constraint Satisfaction Problemskexu@nlsde.buaa.edu.cnliwei@nlsde.buaa.edu.cnKe XuWei LiNational Laboratory Software Development Environment,Department Computer Science Engineering,Beijing University Aeronautics Astronautics,Beijing, 100083, P.R. ChinaAbstractpaper propose new type random CSP model, called Model RB,revision standard Model B. proved phase transitions regionalmost problems satisfiable region almost problems unsatisfiableexist Model RB number variables approaches infinity. Moreover, criticalvalues phase transitions occur also known exactly. relating hardnessModel RB Model B, shown exist lot hard instances Model RB.1. IntroductionSince seminal paper Cheeseman, Kanefsky Taylor (1991) appeared,great amount interest study phase transitions NP-complete problems.However, seems dicult prove existence phenomenon obtainexact location transition points problems. example, random 3SAT, known experiments phase transition occur ratioclauses variables approximately 4:3 (Mitchell, Selman, & Levesque, 1992). Anotherexperimental estimate transition point suggested Kirkpatrick Selman (1994)4:17. used finite-size scaling methods statistical physics derive result.contrast experimental studies, theoretical work given loosehard bounds location transition point. Currently, best known lowerbound upper bound 3:003 (Frieze & Suen, 1996) 4:602 (Kirousis et al., 1998)respectively. Recently, Friedgut (1999) made tremendous progress towards establishingexistence threshold random k-SAT proving width transition regionnarrows number variables increases. still obtain exact locationphase transition point approach.fact, SAT special case constraint satisfaction problem (CSP). CSPimportant theoretical value artificial intelligence, also many immediate applications areas ranging vision, language comprehension scheduling diagnosis(Dechter, 1998). general, CSP tasks computationally intractable (NP-hard) (Dechter,1998). recent years random constraint satisfaction problems also received great attention, experimental theoretical point view (Achlioptas et al., 1999;Cheeseman et al., 1991; Frost & Dechter, 1994; Gent et al., 1999; Hogg, 1996; Larrosa &Meseguer, 1996; Prosser, 1996; Purdom, 1997; Smith & Dyer, 1996; Smith, 1999; Williamsc 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiXu & Li& Hogg, 1994). Williams Hogg (1994) developed technique predicthardest problems found uctuations diculty greatestspace problem instances. also shown predictions critical valueagree well experimental data. Smith Dyer (1996) studied locationphase transition binary constraint satisfaction problems discussed accuracyprediction based expected number solutions. results show variancenumber solutions used set bounds phase transition indicateaccuracy prediction. Recently, theoretical result Achlioptas et al. (1999)shows many models commonly used generating random CSP instancesasymptotic threshold due presence awed variables. recently, Gent et al.(1999) shown introduce structure con ict matrix eliminate aws.paper propose new type random CSP model, called Model RB,revision standard Model B (Gent et al., 1999; Smith & Dyer, 1996).proved phase transition phenomenon exist Model RB numbervariables approaches infinity. precisely, exist two control parameters r, pcorresponding critical values rcr , pcr fixed value r < rcr p < pcr ,random CSP instance generated following Model RB satisfiable probability tending1 number variables approaches infinity, r > rcr p > pcr , unsatisfiableprobability tending 1. Moreover, critical values rcr pcr also knownexactly. relating hardness Model RB Model B, shown Model RBactually lot hard instances.2. Definitions Notationsconstraint satisfaction problem (CSP) consists finite set U = fu1 ; ; un g nvariables set constraints. variable ui , domain Di di elementsspecified; variable assigned value domain. 2 k nconstraint Ci1;i2;;ik consists subset fui1 ; ui2 ; ; uik g U relation Ri1;i2;;ikDi1 Dik , i1; i2; ; ik distinct. Ci1;i2;;ik called k-ary constraintbounds variables ui1 ; ; uik . Ri1;i2;;ik specifies allowed tuples valuesvariables ui1 ; ; uik compatible other. solution CSPassignment value variable domain constraintssatisfied. constraint Ci1;i2;;ik satisfied tuple values assigned variablesui1 ; ; uik relation Ri1;i2;;ik . CSP solution called satisfiable;otherwise unsatisfiable. paper, probability random CSP instancesatisfiable denoted P r(Sat).assume k 2 variable domains contain number values= nff Model RB (where ff constant). generation random CSP instanceModel RB done following two steps:Step 1. select repetition = rn ln n random constraints. random constraintformed selecting without repetition k n variables.Step 2. constraint uniformly select without repetition q = p dk incompatibletuples values, i.e., constraint relation contains exactly (1 p) dk compatible tuplesvalues.94fiExact Phase Transitions Random Constraint Satisfaction Problemsparameter r determines many constraints CSP instance, pdetermines restrictive constraints are.following definitions needed section 4 derive expectationsecond moment number solutions.Definition 1 assignment pair ordered pair hti ; tj assignments variablesU , ti = (ai1 ; ai2 ; ; ) tj = (aj 1 ; aj 2 ; ; ajn ) ail ; ajl 2 Dl . assignment pair hti ; tj satisfies CSP ti tj satisfy CSP. setconsists assignment pairs denoted Apair .Definition 2 Similarity number f : Apair 7! f0; 1; 2; g,f(ht; tj i) =nX( il ; ajl )(1)ail = ajlail 6= ajl(2)l=1function Sam defined follows:( il ; ajl ) =10similarity number assignment pair equal number variablestwo assignments assignment pair take identical values. Definition 2easy see 0 f (hti ; tj i) n.3. Main Resultspaper, following theorems proved.Theorem 1 Let rcr =inequality k 1 1 p ,ffln(1 p) .ff > k1 , 0 < p < 1 two constants k, p satisfylimP r() = 1r < rlimP r() = 0r > rn!1n!1cr(3)cr(4)ffTheorem 2 Let pcr = 1 e r . ff > k1 , r > 0 two constants k, ff r satisfyffinequality ke r 1,limP rlimP rn!1n!1() = 1p < pcr(5)() = 0p > pcr(6)4. Proof Theorem 1 Theorem 2expected number solutions E (N ) model RB given( ) = dn (1E N)rn ln n = nffn (1p)rn ln np(7)Markov inequality P r(Sat) E (N ) hard show limn!1 P r(Sat) = 0r > rcr p > pcr . Hence relations (4), (6) proved. also easy see95fiXu & LiE (N ) eqal 1 r = rcr p = pcr , E (N ) grows exponentially nr < rcr p < pcr .key point proof relations (3), (5) derive expectation secondmoment E (N 2 ) give asymptotic estimate it. Let random CSP instancegenerated following Model RB. P (hti ; tj i) stands probability hti ; tj satisfying .start derive expression P (hti ; tj i). Since constraint generatedindependently, need consider probability hti ; tj satisfying random constraint. Assuming similarity number hti ; tj equal , followingtwo cases:(1) variable constraint assigned value ti tj .k1 = dk .case, probability hti ; tj satisfying constraintq(2) Otherwise, probability hti ; tj satisfying constraintprobability random constraint falls first case0BP(hti ; tj i) = B@k1qk= n . Thus getkkprobability second case 1kn+kqkkq2(1knqdkdk .=qq= n . Hencekk21rn ln nCC)A(8)kqLet set assignment pairs whose similarity number equal . easyshow cardinality givenj Sj =nn1)n(d(9)definition E (N 2 ),E (N 2 ) =nX=0= dnjAS jP (ti ; tj )0n (d 1)nBSBB@dk1qdkqknk+dk2qdkq(1rn ln nk C)CC (10)nk1dicult analyze expression directly. First, give asymptoticestimate P (hti ; tj i). Let = Sn . obvious 0 1. asymptotic analysis,getknk(= n n(11n )( n1n )(12n) (nn ) (1296k 1)1k g (s)nk 1 ) = + n + ( n2 )nfiExact Phase Transitions Random Constraint Satisfaction Problemskk1)(11)11)(sk2(k k( )=gq=kkkq=1(12)pqk2qk(dk=)(dk qk k 1)(d1)q= (11)2 + ( k )(13)pqNote = nff ,P(hti ; tj i) = (1) (sk +p( )gn)2 (1) + (1kp( )gn) + O(1 rn ln n)+O()2kffnn1(14)use condition ff > k1 , getP(hti ; tj i) = (1p2rn ln n)1+p1prn ln ng (s)1k(s +)(1 + O( ))n(15)nevery 0 < < 1 (where = Sn ), asymptotic estimate jAS jjAS j11en( ln (1 s) ln(1 s)) (1 + O( ))n2ns(1 s)111 n ns 1 ns)( ff) pen( ln (1 s) ln(1 s)) (1 + O( )) (16)nffnn2ns(1 s)= nffn (nff1)n ns p= n2ffn (1Notice E (N ) = nffn(1j j (h j i) =P;tE2p)rn ln n ,(N ) 1 +p1pg (s)(sk +)rn ln nn(11 n ns 1 ns( ff)ff)nnnns1(1 + O( )) (17)nn suciently large, except first term E 2 (N ), jAS jP (hti ; tj i) mainly determined following terms:f (s) = 1 +rewriteLetf (s) = e1pph00 (s) =rn ln n1( ff )nsnr ln(1+ 1 p p sk ) ffs n ln nh(s) = r ln(1 +second derivative h(s)sk1ppsk ) ffsrkpsk 2[(k 1)(1 p) psk ](1 p + psk )297(18)(19)(20)(21)fiXu & LiApplying condition k 1 1 p Theorem 1 equation easily proveffh00 (s) 0 interval 0 1. Theorem 2, condition ke r 1follows inequality k 1 1 p still holds p < pcr . also hard showh(0) = 0, h(1) = r ln(1 p) ff < 0 r < rcr p < pcr . Hence easilyprove unique maximum point h(s) = 0 r < rcr p < pcr . Thusterms 0 < 1 negligible r < rcr p < pcr . need considerterms near = 0 . process divided following three cases:Case 1: ff > 1. = 0 (s = 0), definition g(s) Equation (11)p k g(s) rn ln n1+(s +)=1(22)1 pnThus Equation (17) getjAS jP (hti ; tj i) E 2 (N )(1 n1ff )n E 2(N )(23)= 1 (s = n1 ), also hard provelim 1 +n!11Hence obtainSimilary,pp(sk +g(s))nrn ln njAS jP (hti ; tj i) E 2 (N )n1ff2(1 ff)jAS jP (hti ; tj i) E 2 (N ) n2!3(1 ff)jAS jP (hti; tj i) E 2 (N ) nSumming terms together, obtainE (N 2 ) =nX=03!= e0 = 1S=1(24)(25)= 2= 3;(26)ff(27)jAS jP (hti ; tj i) E 2 (N )en1E 2 (N )Case 2: ff = 1. use method Case 1, easily shownjAS jP (hti; tj i) E 2 (N )(1 n1 )n E 2(N ) 1e = 0jAS jP (hti ; tj i) E 2 (N ) 1e = 1jAS jP (hti ; tj i) E 2 (N ) e 12! = 2jAS jP (hti ; tj i) E 2 (N ) e 13! = 3;98(28)fiExact Phase Transitions Random Constraint Satisfaction ProblemsSumming terms together, obtainE (N 2 ) =nX=0jAS jP (hti ; tj i) E 2 (N ) 1e e = E 2(N )(29)< ff < 1. Let S0 = nfi (where fi constant satisfies 1 ff < fi < 1 k1 ).1hard show 0 S0 (0 nfi 1 < n k ), following limitCase 3:1kholds:limn!1 1ppg(s)) n ln n = 0n(sk +(30)Thus 0 S0 , asymptotic estimate second term right Equation(17)p k g(s) rn ln n 01+(s +)e = 1 n ! 1(31)1 pn0 S0 , asymptotic estimate jAS jP (hti ; tj i)jAS jP (hti ; tj i) E 2 (N ) Sn(11 n 1) ( ff)nffn(32)n (1 1 )n ( 1 )S binomial term whose maximum pointnffnffaround = n1 ff , S0 = nfi > n1 ff . asymptotic analysis, obtainnotedS0X=0n (11 n 1) ( ff)nffnThus getE (N 2 ) =nX=0nX=0n (11 n 1) ( ff) = 1nffnjAS jP (hti ; tj i) E 2(N )(33)(34)Combining three cases givesHenceE (N 2 ) E 2 (N ) r < rcr p < pcr(35)E 2 (N )lim= 1 r < rcr p < pcrn!1 E (N 2 )(36)Cauchy inequality P r(Sat) EE (N(N2 )) (Bollobas, 1985), easily provedlimn!1 P r(Sat) = 1 r < rcr p < pcr . Hence Theorem 1 Theorem 2proved.299fiXu & Li5. Relation Model B Model RBsection explain detail Model RB revises Model B showhardness Model RB relating Model B. previous papers (Gent et al.,1999; Smith & Dyer, 1996) know generation random CSP instancestandard Model B (which often written hn; d; p1 ; p2 i) done following two steps:Step 1. select repetition = p1 n(n2 1) random constraints. random constraintformed selecting without repetition 2 n variables.Step 2. constraint uniformly select without repetition q = p2 d2 incompatibletuples values, i.e., constraint relation contains exactly (1 p2 ) d2 compatible tuplesvalues.Since standard Model B binary CSP model, consider binarycase Model RB section. previous papers Model B used testCSP algorithms following way. Given values n, p1 , constrainttightness p2 varied 0 1 steps d12 . setting hn; d; p1 ; p2 fixednumber instances (e.g. 100) generated. search algorithm appliedinstance. Finally numerous statistics search cost probabilitysatisfiable gathered. fact, two steps forming constraint selectingincompatible tuples values Model RB exactly Model B.significant difference Model B Model RB Model RB restricts fastdomain size number constraints increase number variablesModel B not, may lead result many instances Model B sufferasymptotically trivially insoluble (Achlioptas et al., 1999) Model RB avoidsproblem. easy see given values n, p1 , setting hn; d; p1 ; p2Model B equivalent setting Model RB number variablesln r = p1 (n 1) (Let nff = rn ln n = 1 p n(n 1)).hn; d; p1 ; p2 i, ff = lnn2ff ln n2 1Theorem 2 shows ff > 12 2e r 1, exists exact phase transitionbinary case Model RB. Given values n, p1 Model B, settinghn; d; p1 ; p2 i, conditions equivalent setting Model RB satisfies Theorem 2ln 1ff=> ) d2 > n(37)ln n 2ln2 ln nff2 ln2e r 1 ) 2e ln n p1 (n 1) 1 ) p1(38)(n 1) ln 2proof Theorem 2 reveals conditions (37), (38) satisfied, ModelRB exhibit exact phase transition E (N ) = 1. noted WilliamsHogg (Williams & Hogg, 1994), independently Smith (1996) already developedtheory predict phase transition point Model B basis E (N ) = 1. Prosser(1996) found theory close agreement empirical results, exceptp1 small. Inequality (38) shows order make equivalent setting Model RBsatisfy conditions Theorem 2, parameter p1 Model B lesscertain value, consistent Prosser's experimental finding.ln 10consider typical setting h20; 10; 0:5; p2 Model B. Let n = 20, ff = ln201)1:5856ModelRB.settingModelRB0:7686 r = 0:5(202 ln 20values equivalent setting h20; 10; 0:5; p2 Model B. Inequalities (37)100fiExact Phase Transitions Random Constraint Satisfaction Problems(38) also hard show equivalent setting Model RB correspondingsetting h20; 10; 0:5; p2 satisfies conditions Theorem 2, i.e., 102 > 20 p1 =0:5 (202 ln1)10ln 2 0:35. experiments done Prosser (1996) show instancesgenerated p2 = 0:38 hard solve. maximum cost pointalso agreeswellff0:7686r1:5856asymptotic phase transition point Model RB p = 1 e 1 e0:38.settings Model B previous work, also find equivalentsettings Model RB using method. Thus hardness solving settingsModel B equal solving equivalent settings Model RB. manyprevious studies (Gent et al., 1999; Smith & Dyer, 1996; Prosser, 1996) knowinstances generated phase transition many settings Model B hardsolve various kinds CSP algorithms. exist lot hard instances solveModel RB.6. Conclusions Future Worklot experimental theoretical studies indicate phase transition solvabilityimportant feature many decision problems computer science. shownproblems characterized control parameter wayspace problem instances divided two regions: under-constrained regionalmost problems many solutions, over-constrained region almostproblems solutions, sharp transition them. Another interestingfeature associated phase transition peak hardness solvingproblem instances occurs transition region. Since instances generatedtransition region appear hardest solve, commonly used benchmarkalgorithms many NP-complete problems. unfortunately, except Hamiltoniancycle problem (which NP-complete), decision problems exact resultsexistence location phase transition P class (Parkes, 1997),e.g. random 2-SAT. problems interesting NP-complete problemscomplexity theoretic point view solved polynomial time.Hamiltonian cycle problem, using improved backtrack algorithm sophisticatedpruning techniques, Vandegriend Culberson (1998) recently found probleminstances phase transition region hard solve.paper proposed new type random CSP model, Model RB,revision standard Model B, asymptotic analysis model alsopresented. results quite surprising. prove existence phasetransitions model also know location transition points exactly.shown exist lot hard instances Model RB relating hardnessstandard Model B. Since still lack studies exact derivationphase transitions NP-complete problems, paper may provide new insightfield. However, discuss scaling behaviour Model RBrelated issues paper. order get better understanding Model RB,suggest future work include determining either empirically theoreticallywhether hard instances persist reasonably high frequency numbervariables increases. 11. Two anonymous referees suggest point.101fiXu & LiAcknowledgmentsresearch supported National 973 Project China Grant No. G1999032701.would like thank Ian P. Gent, Barbara M. Smith, Peter van Beek anonymousreferees helpful comments suggestions.ReferencesAchlioptas, D., Kirousis, L., Kranakis, E., Krizanc, D., Molloy, M., & Stamatiou, Y. (1999).Random constraint satisfaction: accurate picture. Constraints. submitted.Also Proc. Third International Conference Principles Practice ConstraintProgramming (CP 97), Springer-Verlag, pp. 107{120, 1997.Bollobas, B. (1985). Random Graphs. Academic Press, New York.Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). really hard problems are.Proceedings IJCAI-91, pp. 331{337.Dechter, R. (1998). Constraint satisfaction. MIT Encyclopedia Cognitive Sciences(MITECS). Online \ ftp://ftp.ics.uci.edu/pub/CSP-repository/papers/R68.ps".Friedgut, E. (1999). Sharp thresholds graph properties k-sat problem. JournalAmerican Mathematical Society, 12, 1017{1054.Frieze, A., & Suen, S. (1996). Analysis two simlpe heuristics random instancek-sat. Journal Algorithms, 53, 469{486.Frost, D., & Dechter, R. (1994). search best constraint satisfaction search.Proceedings AAAI-94, pp. 301{306.Gent, I., MacIntyre, E., Prosser, P., Smith, B., & Walsh, T. (1999). Random constraint satisfaction: Flaws structure. Constraints. submitted. Online \http://www.cs.strath.edu.uk/~apes/apesreports.html".Hogg, T. (1996). Refining phase transition combinatorial search. Artificial Intelligence, 81, 127{154.Kirkpatrick, S., & Selman, B. (1994). Critical behavior satisfiability randomboolean expressions. Science, 264, 1297{1301.Kirousis, L., Kranakis, E., Krizanc, D., & Stamatiou, Y. (1998). Approximating unsatisfiability threshold random formulas. Random Structures Algorithms, 12,253{269.Larrosa, J., & Meseguer, P. (1996). Phase transition max-csp. Proceedings ECAI-96,pp. 190{194.Mitchell, D., Selman, B., & Levesque, H. (1992). Hard easy distributions sat problems. Proceedings AAAI-92, pp. 459{465.102fiExact Phase Transitions Random Constraint Satisfaction ProblemsParkes, A. (1997). Clustering phase transition. Proceedings AAAI-97, pp.340{345.Prosser, P. (1996). empirical study phase transitions binary constraint satisfactionproblems. Artificial Intelligence, 81, 81{109.Purdom, P. (1997). Backtracking random constraint satisfaction. Annals Mathematics Artificial Intelligence, 20, 393{410.Smith, B. (1999). Constructing Asymptotic Phase Transition Random Binary Constraint Satisfaction Problems. Extended Abstract.Smith, B., & Dyer, M. (1996). Locating phase transition binary constraint satisfactionproblems. Artificial Intelligence, 81, 155{181.Vandegriend, B., & Culberson, J. (1998). Gn;m phase transition hardhamiltonian cycle problem. Journal Artificial Intelligence Research, 9, 219{245.Williams, C., & Hogg, T. (1994). Exploiting deep structure constraint problems.Artificial Intelligence, 70, 73{117.103fiJournal Artificial Intelligence Research 12 (2000) 35-86Submitted 5/99; published 2/00Reasoning Interval Point-basedDisjunctive Metric Constraints Temporal ContextsFederico BarberFBARBER@DSIC.UPV.ESDpto. de Sistemas Informticos ComputacinUniversidad Politcnica de ValenciaCamino de Vera s/n, 46022 Valencia, SpainAbstractintroduce temporal model reasoning disjunctive metric constraints intervalstime points temporal contexts. temporal model composed labeled temporal algebrareasoning algorithms. labeled temporal algebra defines labeled disjunctive metric pointbased constraints, disjunct input disjunctive constraint univocally associatedlabel. Reasoning algorithms manage labeled constraints, associated label lists, setsmutually inconsistent disjuncts. algorithms guarantee consistency obtain minimalnetwork. Additionally, constraints organized hierarchy alternative temporal contexts.Therefore, reason context-dependent disjunctive metric constraints intervalspoints. Moreover, model able represent non-binary constraints, logicaldependencies disjuncts constraints handled. computational cost reasoningalgorithms exponential accordance underlying problem complexity, althoughimprovements proposed.1. IntroductionTwo main lines research commonly recognized temporal reasoning area. firstapproach deals reasoning temporal constraints time-dependent entities. goaldetermine consequences (T) follow set temporal constraints, "{TemporalConstraints}|=T?", determine whether set temporal constraints consistent,assumptions properties temporal facts. second approach deals reasoningchange, events, actions causality. Here, goal obtain consequent state setactions events performed initial state: "[Si, {A1, A2, ..., }]|= Sj?".approaches constitute active fields research applications several artificial intelligence areasreasoning change, scheduling, temporal planning, knowledge-based systems, naturallanguage understanding, etc. areas, time plays crucial role, problems dynamicbehavior, necessary represent reason temporal dimension information.paper, deal first approaches. goal reasoning qualitativequantitative constraints intervals time-points temporal contexts. Moreover, specialcases non-binary constraints also managed. tasks pending issues temporalreasoning area, well important features facilitate modeling relevant problems area(including planning, scheduling, causal hypothetical reasoning, etc.).Several temporal reasoning models defined literature, clear trade-offrepresentation expressiveness complexity reasoning algorithms. Qualitative PointAlgebra (PA) (Vilain, Kautz & Van Beek, 1986) limited subset interval-based models. Interval2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSAlgebra (IA) introduced Allen (1983) represents symbolic (qualitative) constraintsintervals metric information, 'interval1 starts 2 seconds interval2 ', cannotincluded. Metric (quantitative) point-based models (Dechter, Meiri & Pearl, 1991) include 'timeline' (metric) constraints, represent limited subset disjunctiveconstraints intervals. Thus, constraints like 'interval1 {bef, aft} interval2 ' cannotrepresented (Gerevini & Schubert, 1995).efforts made integrate qualitative quantitative temporal informationpoints intervals (Kautz & Ladkin, 1991; Drakengren & Jonsson, 1997; etc.). Particularly, Meiri(1996) introduces Qualitative Algebra (QA), interval represented three nodes (onerepresenting interval two representing extreme points) QArepresent qualitative metric constraints points intervals. Badaloni Berati (1996) defineInterval Distance Sub Algebra (IDSA), nodes intervals. intervals relateddisjunctive 4-tuple-metric constraints ending time points {(I-i, I-j ), (I+i , I-j ), (I-i , I+j ), (I+i ,I+j )}. Staab Hahn (1998) propose model reasoning qualitative metric boundariesintervals. However, models cannot handle constraints interval durations,identified earlier Allen (1983). Constraints 'interval1 lasts 2 seconds interval2'require high-order expression (Dechter et al., 1991), duration primitiveintegrated interval point constraints (Allen, 1983; Barber, 1993). Particularly, Barber (1993)proposes two orthogonal networks relate constraints durations time points. Navarrete(1997) Wetprasit Sattar (1998) relate disjunctive constraints durations time points,limited subset interval constraints managed. recently, Pujari Sattar (1999)propose framework reasoning points, intervals durations (PIDN). Here, variablesrepresent points intervals, constraints ordered set three intervals representing (Start,End, Duration) subdomains. However, specialized algorithms management PIDNconstraints proposed.relation complexity reasoning algorithms, consistency problem polynomialPA (Vilain, Kautz & Van Beek, 1986) non-disjunctive metric networks (Dechter et al., 1991).However, Vilain, Kautz Van Beek (1986) also showed determining consistencygeneral-case temporal network (i.e.: disjunctive qualitative metric constraints points,intervals durations) NP-hard. Thus, previous qualitative quantitative models,consistency problem tractable properties constraints, relationshipsvariable domains constraints, using restricted subsets constraints (Dechter et al., 1991;Dechter, 1992; van Beek & Detcher, 1995; Wetprasit & Sattar, 1998; Jeavons et al., 1998; etc.).instance, tractable subclasses IA identified Vilain, Kautz Van Beek (1986),Nebel Burckert (1995), Drakengren Jonsson (1997), etc. Moreover, interesting resultsobtained identification tractable subclasses QA. Specifically, Jonsson et al. (1999)identified five maximal tractable subclasses qualitative point-interval algebra. However,knowledge maximal tractable subclass PIDN model (maximal tractable subclassqualitative quantitative point, interval duration constraints) still identified. case,restricted tractable subclasses able obtain expressiveness full models,problem reasoning disjunctive constraints points intervals remains NP-complete.hand, qualitative metric temporal models manage certain typesnon-binary constraints, important modeling problems (scheduling, causalreasoning, etc.). instance, disjunctive assertions like (interval1 {bef, meets} interval2 ) (time36fiBARBERpoint3 [10 20] time-point4 ), temporal-causal relations like (interval1 {bef, meets}interval2 ) (time-point3 [10 20] time-point4 ) incorporated models(Meiri, 1996). Moreover, global consistency property introduced Dechter (1992)important property temporal networks, since allows us obtain solutions backtrack-freesearch (Dechter, 1992; Freuder, 1982). particular, global consistent network would allow ushandle conjunctive queries like (interval1 {bef, meets} interval2 ) (time-point3 [10 20]time-point4 ) hold? without propagation query, required (van Beek, 1991).Stergiou Koubarakis (1996), Jonsson Bckstrm (1996) dealt representationtemporal constraints means disjunctions linear constraints (linear inequalitiesinequations) also named Disjunctive Linear Relations (DLRs). expressions unifyingapproach manage disjunctive constraints points, intervals durations,expressions subsume formalism temporal constraint reasoning (Jonsson & Bckstrm,1998). Moreover, DLRs able represent disjunctions non-disjunctive metric constraints (x1 y1 c1 x2 -y2 c2 .... xn -yn cn ), xi yi time points, ci real numbers n1 (Stergiou& Koubarakis, 1998). Obviously, satisfiability problem arbitrary set disjunctionslinear constraints NP-complete. Interesting tractable subclasses DLRs conditionstractability identified (Cohen et al., 1996; Jonsson & Bckstrm, 1996; Stergiou &Koubarakis, 1996). two main tractable subclasses Horn linear Ord-Horn linearconstraints (Stergiou & Koubarakis, 1996; Jonsson & Bckstrm, 1998). However, subclassessubsume temporal algebras whose management also polynomial.management set disjunctions linear constraints mainly based general methodslinear programming, although specific methods defined tractable subclasses(Stergiou & Koubarakis, 1998; Cohen et al., 1996; etc.). Pujari Sattar outline (1999),linear programming approach, though expressive, take advantage underlyingstructures (e.g., domain constraints) temporal constraints. addition, usual concepts temporalreasoning, composition intersection operations constraints, minimal constraints, kconsistency (Freuder, 1982), decomposability (Montanari , 1974), globally consistency (Dechter,1992), etc., consequences adapted reasoning disjunctive linear constraints,trivial issue.spite expressive power previous models, problems (including planning,scheduling, hypothetical reasoning, etc.) also need reason alternative contexts (situations,intentions causal projections) know holds one (Dousson et al., 1993;Gerevini & Schubert, 1995; Garcia & Laborie, 1996; Srivastava & Kambhampati, 1999). givesrise need reason context-dependent constraints. feature supported usualtemporal models general way, described usual expressive power constraints(Jeavons et al., 1999). Therefore, ad-hoc methods used reasoning temporalcontexts required.issues addressed paper. describe temporal model, integratesqualitative metric disjunctive constraints time-points intervals. temporal modelbased time-points primitive, intervals represented means end timepoints. However, representation interval constraints seems imply kind relationamong endpoint constraints (Gerevini & Schubert, 1995). proposed temporal model introduceslabeled constraints, elemental constraint (disjunct) disjunctive point-based metricconstraint associated one unique label. way, point-based constraints related among37fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSwithout using hyper-arcs. Therefore, metric symbolic constraints among intervals timepoints fully integrated, represented managed means labeled metric point-basedTemporal Constraint Network (TCN). Particularly, model proposed handles constraintsproposed QA (Meiri, 1996), IDSA (Badaloni & Berati, 1996), Distance Constraint Arraysmodel (Staab & Hahn, 1998). Moreover, several added functionalities also provided:Management alternative temporal contexts. input constraint associatedgiven context. hierarchy alternative temporal contexts defined,constraints points intervals dependent context. knowledge,features improve existing temporal models, contexts managed.Reasoning algorithms labeled constraints based closure process. processesguarantee consistency obtain minimal disjunctive context-dependent TCN. Additionally,special type globally labeled-consistent TCN obtained. property allows us obtainsolutions backtrack-free search (Freuder, 1982).Management special type non-binary constraints. Reasoning algorithms ablemanage disjunctions disjunctive constraints. supposes extension disjunctionsnon-disjunctive metric constraints proposed Stergiou Koubarakis (1998). Moreover,given set disjunctive constraints, model handle logical relations amongdisjunctions different constraints. Thus, express set atomic disjunctsdisjunctive constraints mutually disjunctive among them. Therefore, special typeand/or TCN managed conjunctive (and) TCN. Likewise, model also handlespecial non-binary constraints representing implications among temporal constraintsidentified Meiri (1996).features, proposed temporal model suitable modeling problemsrequirements appear. computational cost reasoning methods non-polynomial, givencomplexity underlying problem. However, several improvements also proposed.brief revision main temporal reasoning concepts presented Section 2. Section 3,temporal algebra labeled point-based disjunctive metric constraints described. temporalalgebra introduces concept labeled constraints temporal operations. Reasoningalgorithms guaranteeing minimal (and consistent) TCN specified Section 4. usingmodel, integration interval point-based constraints management non-binaryconstraints respectively described Sections 5 6. Association constraints temporalcontexts management context-dependent constraints detailed Section 7. Finally, Section8 concludes.2. Basic Temporal ConceptsTemporal reasoning deals reasoning temporal constraints. syntax semanticsconstraints defined underlying temporal algebra, basis performingreasoning processes. temporal algebra defined according following elements:Temporal primitive (or variable) 'x ', usually time-points (ti ) intervals (Ii ).Interpretation domain primitives xi . interpretation domain represents time line.38fiBARBERTime points instantiated (ti D), temporal intervals modelled pairsending time points instantiated D: Ii = (Ii -, Ii +), Ii DxD, Ii -Ii +.Temporal constraints primitives, constraint relates n primitives: c1,2..n (x1 ,x2 , ..., xn ). particular cases, 'empty constraint' {} named Inconsistent-Constraint'U' Universal-Constraint. Unary-constraints restrict interpretation domainvariables. usually used symbolic algebras, infinite domainassumed. Binary-constraints temporal constraints two variables (xi cij xj ), nary-constraints represent temporal constraints among n variables. default, binary constraintsassumed paper. also qualitative (relative relation) quantitative(metric relation) constraints, well disjunctive (cij set disjunctive basic constraints,|cij |1) non-disjunctive constraints.Operations constraints. Mainly, Temporal Composition (), Temporal Intersection(), Temporal Union (), Temporal Inclusion ().temporal problem specified set n variables X= {xi }, interpretation domainfinite set temporal constraints variables {(xi cij xj )}. temporal problem gives riseTemporal Constraint Network (TCN) represented directed graph nodesrepresent temporal primitives (xi ) labeled-directed edges represent binary constraints(cij). Universal Constraint U usually represented graph, direct edge(representing cij ) xi xj implies inverse one (representing cji ) xj xi .According underlying Temporal Algebra, mainly IA-TCNs based IntervalAlgebra (Allen, 1983), PA-TCNs based Point Algebra (Vilain et al., 1986), Metric-TCNsbased Metric Point Algebra (Dechter et al., 1991; Dean & McDermott, 1987). latercase, disjunctive metric point-based constraints give rise Temporal Constraint SatisfactionProblem (TCSP) (Dechter et al., 1991).Reasoning temporal constraints seen Constraint Satisfaction Problem (CSP).instantiation variables X n-tuple (v1 , v2 , v3 , ...,vn ) / vi represents assignmentsvalues {vi } variables {x }: (x1 =v1 , x2 =v2 , ...,xn =vn). (global) solution TCN consistentinstantiation variables X domains TCN constraints satisfied. valuev consistent (or feasible) value xi exists TCN solution xi =v. setfeasible values variable xi minimal domain variable. constraint (xi cij xj )consistent exists solution (xi cij xj ) holds. constraint cij minimal iff consistsconsistent elements (or feasible values) is, satisfiedinterpretation TCN constraints. TCN minimal iff constraints minimal.TCN consistent (or satisfiable ) iff least one solution. Freuder (1982) generalizesnotion consistency as: 'a network k-consistent iff (given instantiation k-1 variablessatisfying direct constraints among variables) exists least one instantiationkth variable k values taken together satisfy constraints among k variables'.consequences: (i) (k-1)-length paths network consistent, (ii) pair nodes,exists interpretation satisfies (k-1)-length path them, (iii) sub-TCNk-nodes consistent. particular cases, 1-consistency, 2-consistency 3-consistency callednode-consistency, arc-consistency path-consistency, respectively (Mackworth, 1977; Montanari,1974).39fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSPath-consistency common concept constraint networks. Montanari (1974)Mackworth (1977), path k-length nodes (x1 , x2 , ..., xk , xj ) path-consistent iffvalue v1 d1 vj dj (x1=v1 c1j xj=vj ) holds, exists sequence values v2 d2 , v3d3 ,..., vk dk (v1 cl2 v2 ), (v2 c23 v3 ),...., (vk ck,j vj ) hold. TCN path-consistent iffpaths consistent. Moreover, Montanari (1974) proves ensure path-consistency sufficescheck every 2-length path. Thus, path-consistency 3-consistency equivalent concepts.Alternatively, Meiri (1996) outlines path k-length (xi , x1 , x2 , ...,xk , xj ) path-consistent iff cij(ci1 c12 ... ckj ). However, definition disregards domain constraints, equivalentformer definition variable domains infinite TCN also node arc-consistent,usual case symbolic algebras. metric algebras, path-consistency usually assumes nodearc-consistency. Therefore, taking account necessary test 2-length pathsassure path-consistency, TCN path-consistent iff cij ,cik ,ckjTCN, cij (cik ckj ).condition gives rise usual path-consistent algorithm: Transitive Closure Algorithm(TCA) imposes local 3-consistency sub-TCN 3 nodes, 2-length pathsbecome consistent paths (Mackworth, 1977; Montanari , 1974). TCA algorithm obtainequivalent path-consistent TCN exists. Otherwise, fails.cij ,cik ,ckj TCN: cij cij (cik ckj )network strong k-consistent iff network j-consistent jk (Freuder, 1982). nconsistent TCN consistent TCN, strong n-consistent TCN minimal TCN. Alternatively,Dechter (1992) introduces concepts local global consistency: partial instantiationvariables (x1 =v1 , x2 =v2 , ...,xk =vk ) / 1k<n locally consistent satisfies constraints amongvariables. subTCN globally consistent locally consistent instantiationvariables subTCN extended consistent instantiation TCN. globallyconsistent TCN one subTCNs globally consistent. Thus, TCN strong nconsistent iff globally consistent (Dechter, 1992).first reasoning task TCN determine whether TCN consistent. TCNconsistent, obtain minimal-TCN, TCN solutions (by assuming discrete finitemodel time), one solution, partial solution (consistent instantiation subset TCNvariables, part global solution), etc.Deductive closure, propagation, one basic reasoning algorithms. closure processdeductive process TCN, new derived constraints deduced explicitlyasserted ones means composition () intersection () operations. Thus, processdetermining consistency minimality TCN related sound complete closureprocess (Vilain et al., 1986). Alternatively, CSP-based methods (with several heuristic search criteria)also used guaranteeing consistency obtaining TCN solutions. paper, mainlyinterested TCN closure processes.Determining consistency general-case TCN NP-hard, Minimal TCNsobtained polynomial number consistency processes (Vilain et al., 1986). Particularly, Dechter,Meiri Pearl (1991) showed determining consistency obtaining minimal disjunctivemetric TCN achieved O(n3 le), n number TCN nodes, e numberexplicitly asserted (input) constraints, l maximum number intervals inputconstraint. However, specific levels k-consistency guarantee consistency obtain minimalTCN, depending TCN topology underlying temporal algebra. example, path40fiBARBERconsistency guarantees consistency obtains minimal non-disjunctive metric TCN (Dechter etal., 1991). path-consistency TCA Algorithm O(n3) cost (Allen, 1983; Vilain, Kautz & VanBeek, 1986). However, assuring path-consistency become complex task disjunctive metricTCNs variable domain large continuous. stated Dechter, Meiri Pearl(1991), number intervals |cij cjk | upper bounded |cij |x|cjk |. Thus, total numberdisjuncts (subintervals) path-consistent TCN might exponential number disjuncts perconstraints initial (input) TCN. Schwalb Dechter (1997) call fragmentationproblem, appear non-disjunctive metric TCNs. Thus, TCA algorithm O(n3R3 ) disjunctive metric-TCNs time dense (Dechter et al., 1991), range Rmaximum difference lowest highest number specified input constraints.3. Labeled Temporal Algebramain elements point-based disjunctive metric temporal algebra (Dechter et al., 1991):Time-point (ti ) primitive variable. continuous variable domain (like Q ) usuallyassumed.temporal constraint cij U finite set l mutually exclusive subdomains (orsubintervals) D.cij {[d-1 d+1 ], [d-2 d+2 ], ...., [d-k d+k ], ....., [d-l d+l ]} ,d-k d+k d-k ,d+k D,disjunctively restricts temporal distance two time-points, ti tj :tj - ti {[d-1 d+1 ], [d-2 d+2 ], ....., [d-l d+l ]},meaning (d-1 tj -ti d+1 ) .... (d-l tj -ti d+l ). Similar conditions applied open(d-k d+k ) semi-open intervals (d-k d+k ], [d-k d+k ). Universal-Constraint U {(- +)}.Unary constraints restrict associated subdomain time-point ti {[d-1 d+1 ], [d-2 d+2], .....,[d-l d+l ]}. special time-point T0 usually included, represents 'the beginningworld' (usually, T0 =0). Thus, unary constraint ti represented binary oneti T0 :ti - T0 {[d-1 d+1 ], [d-2 d+2 ], ..... ,[d-l d+l ]} ti[d-1 , d+1 ] ti [d-2 , d+2 ] , ..., ti[d-l , d+l ]and, default: ti , (T0 {[0 )} ti ).algebra operations, mainly , , . (Meiri, 1996), given two temporalconstraints S={[dS-i , dS+i ]} T={[dT-j , dT +j ]},= {dk / diS djT / dk = di +dj }.is, [dS-i , dS+i ]S, [dT-j , dT +j ]T, T{[dS-i +d T-j , dS+i +d T+j ]}. Here, resulting subdomainsmay pairwise disjoint. Therefore, additional processing may requiredcompute disjoint subdomain set.= {dk / dkS dkT}. is, set-intersection subdomains.= {dk / dkS dk T}, set-union subdomains.ST = iff dk S, dk T.41fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSbasis point-based disjunctive metric temporal algebra operations,introduce labeled point-based disjunctive metric temporal algebra, gives rise labeledTCN.3.1 Labeled Constraints Inconsistent Label Setselemental constraint (ec) one disjunct disjunctive constraint. Similar terms atomic,basic canonical constraints. However, lets use term due special structure labeledelemental constraints introduced on. Thus, disjunctive constraint cijconsidered disjunctive set l mutually exclusive elemental constraints {ecij.k }.ecij.k = [d-ij.k d+ij.k ] / i,j,k d-ij.k d+ij.kcij {ec ij.1 , ec ij.2 , ..., ec ij.l } U / k,p(1,..,l), kp, (ecij.k ec ij.p )=Definition 1 (Labeled constraints). labeled elemental constraint lecij.k elemental constraintecij.k associated set labels {labelij.k}, labelij.k symbol. labeled constrain lc ijdisjunctive set labeled elemental constraints {lecij.k }. is,lc ij {lecij.1 , lecij.2 , ..., lecij.l },lecij.k (ec ij.k {labelij.k }), {labelij.k }{label1 , label2 , ..., labels } set symbols.label labeled-TCN considered unique symbol. following casesoccur:i)input (or explicitly asserted) constraint lc ij one elemental constraint, is,one disjunct, elemental constraint label 'R0 '. labeled UniversalConstraint {U{R0}}. given TCN, set elemental constraints labeled 'R0 'common context. Thus, label R0 represents set elemental constraintsalternatives (disjuncts). elemental constraints labeled R0hold since alternative disjuncts.ii)input constraint lc ij one elemental constraint, elemental constraintlecij.k lc ij single exclusive label associated (|{labelij.k }|=1). Thus, labelTCN represents bi-univocally elemental constraint explicitly assertedconstraint.iii) derived elemental constraint (obtained combining (lc) intersecting ( lc) twolabeled elemental constraints) set labels associated it. set labels obtainedlabel sets associated combined (or intersected) labeled elemental constraints.detailed later specification operations (lc, lc) Section 3.2.consequence, label set associated derived elemental constraint representsconjunctive support-set explicitly asserted elemental constraints imply derivedelemental constraint.Let's see simple example labeled constraints, introduced Dechter, MeiriPearl (1991).42fiBARBER{([60 70]{R0})}t4{([40 50]{R3}) ([20 30]{R4})}{([10 20]{R0})}t3T0t2{([60 ){R1}) ([30 40]{R2})}T0{([10 20]{R0} )}t1Figure 1: labeled point-based disjunctive metric TCN Example 1Example 1: "John goes work either car [30'-40'], bus (at least 60'). Fred goes workeither car [20'-30'], carpool [40'-50']. Today John left home (t1)7:10 7:20, Fred arrived (t4) work 8:00 8:10. also knowJohn arrived (t2) work 10'-20' Fred left home (t3)."example, disjunctive labeled constraints Figure 1, T0 representsinitial time (7:00) granularity minutes. label 'R0 ' associated elementalconstraints belonging constraints one disjunct. constraints one,mutually exclusive disjuncts, disjunct labeled exclusive label Rn (n>0). Thus,label R0 associated "John left home 7:10 7:20", "Fred arrived work8:00 8:10", "John arrived work 10'-20' Fred left home".common context.label R1 associated "John goes bus", R2 "John goes car".label R3 associated "Fred goes carpool", R4 "Fred goes car".Definition 2 (Inconsistent-Label-Sets). Inconsistent-Label-Set (I-L-Set) set labels {labeli }represents set overall inconsistent elemental constraints. is, cannotsimultaneously hold.Theorem 1. label set superset I-L-Set also I-L-Set. proof obvious.set elemental constraints inconsistent, superset also inconsistent.Definition 3. Elemental constraints {lecij.k} input disjunctive constraint lc ij pairwise disjoint.Thus, 2-length set labels pair {lecij.k } added set I-L-Sets. is,input constraint lc ij {lecij.1 , lecij.2 , ..., lecij.l }, lecij.k (ecij.k {labelij.k }) |{labelij.k }|=1:k,p(1,..,l) / kp, I-L-Sets I-L-Sets ({labelij.k }{labelij.p })example Figure 1, {R1 R2 } {R3 R4 } I-L-Sets. I-L-Sets existing labeledTCN detected reasoning processes later detailed Section 4.43fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTS3.2 Operations Labeled Constraintsfollowing points define main operations labeled constraints.3.2.1 TEMPORAL INCLUSION LCtemporal inclusion operation lc take account inclusion temporal intervalsinclusion associated label sets:lecij.k lc lecij.p = (ecij.k {labelij.k }) lc (ecij.p {labelij.p }) =def ecij.k ecij.p {labelij.k } {labelij.p }.3.2.2 TEMPORAL UNION LCOperation lc performs disjunctive temporal union labeled constraints set-unionelemental constraints. However, labeled elemental constraints whose associated labels I-L-Setsrejected.lc ij lc lcij =def lecij.k lc ij , lc [{lecij.k } lcij] ,lc [{lecij.k } lcij ] = (ec ij.k {labelij.k }) lc lcij =defInconsistent({labelij.k }) : lcijlecij.p lcij / lecij.p lc lecij.k : lcij: ({lcij } {lecij.k }) - ({lecij.p }, lecij.plcij lecij.k lclecij.p )(s1 )(s2 ).function Inconsistent({labelij.k}) returns true set {labelij.k } I-L-Set supersetexisting I-L-Set (Theorem 1). Otherwise, returns false:Inconsistent({labelij.k }) =def{labels }Inconsistent-Label-Sets / {labels }{labelij.k } True Else False.operation lc simplifies resulting constraint. Equal less-restricted elemental constraintsequal bigger associated label sets removed. instance:{([10 30] {R1 R3 R5 R9}), ([40 40] {R6 R7})} lc {([10 20] {R1 R3}), ([40 40] {R6 R7 R8})} ={([10 20] {R1 R3}), ([40 40] {R6 R7})}.resulting constraint, ([10 30] {R1 R3 R5 R9}) ([40 40] {R6 R7 R8}) eliminated, examplescases s1 s2 , respectively. is, ([10 20] {R1 R3}) lc ([10 30]{R1 R3 R5 R9}) ([40 40] {R6 R7})lc ([40 40] {R6 R7 R8}). simplifications seem counter-intuitive. However, note labelset associated derived-labeled elemental constraint represents support set (composedinput elemental constraints) derived-labeled elemental constraint obtained. Thus,minimal associated label set represented, reason efficiency. Moreover,labels associated label set {labelij.k }, elemental constraint (ecij.k ) equalrestricted.3.2.3 TEMPORAL COMPOSITION LCOperation lc performs temporal composition labeled constraints. based operationunderlying disjunctive metric point-based algebra.44fiBARBERlc ij lc lc jk =def lecij.plc ij , lecjk.q lc jk lc [ (ecij.p ecjk.q {labelij.p }{labeljk.q })].instance: {([0 10] {R1}), ([20 30] {R2})} lc {([100 200] {R3}), ([300 400] {R4})} ={([320 430] {R4 R2}), ([300 410] {R4 R1}), ([100 210] {R3 R1}), ([120 230] {R3 R2})}.Note elemental constraints labeled derived constraint may pairwise disjoint.However, labeled derived elemental constraints cannot simplified. relatedfragmentation problem disjunctive metric algebra (Schwalb & Dechter, 1997).derived-labeled elemental constraint associated label set. example,(([320 430] {R4 R2}), ([300 410] {R4 R1})) cannot simplified ([300 430] {R4 R2 R1}) sincesubinterval depends different set labels (that is, different support-set elementalconstraints). label set {R4 R2 } becomes I-L-Set, ([320 430] {R4 R2}) removed.hand, [300 410] becomes inconsistent interval implied time points,{R4 R1 } asserted I-L-Set.3.2.4 TEMPORAL INTERSECTION LCOperation lc performs temporal intersection labeled constraints based operation.lc ij lc lcij =def lecij.k lc ij , lecij.p lcij , lc [lecij.k lc lecij.p ]where, lecij.k lc lecij.p =defec ij.k ecij.p = {};The Inconsistent-Constraint returned.Else [(ecij.k ecij.p ) ({labelij.k }{labelij.p })]example:{([0 10] {R1}), ([20 25] {R2})} lc {([0 30] {R3}), ([40 50] {R4})} = {([20 25] {R3 R2}), ([0 10] {R3 R1})}operations lc lc, label set {labelij.r} associated derived labeled-elementalconstraint (ecij.r) obtained set-union labels associated combined (lc) intersected( lc) labeled-elemental constraints. Therefore, {labelij.r} represents support set (composed inputelemental constraints) implies derived elemental constraint (ecij.r).Definition 4. set I-L-Sets complete represents inconsistent sets TCN elementalconstraints. set I-L-Sets sound I-L-Set represents inconsistent set elementalconstraints.Theorem 2. Assuming complete sound set I-L-Sets, labeled elemental constraintconsistent iff associated label set I-L-Set. proof trivial, since labelset associated labeled elemental constraint represents support-set.Theorem 3. Assuming complete sound set I-L-Sets, inconsistent labeled elementalconstraint obtained operations lc lc.Proof: operations lc lc use operation lc obtain results. operation lcrejects labeled elemental constraints whose associated labels I-L-Sets. Thus, elementalconstraints derived operations lc lc consistent (Theorem 2).45fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTS3.3 Distributive Property lc lc Disjunctive Labeled ConstraintsOperations distributive (i.e.: distributes ) non-disjunctive metric TCN,property hold disjunctive metric constraints. Dechter, Meiri Pearl (1991) showfollowing example. Given disjunctive metric constraints:a= {[0 1], [10 20]},have:b= {[25 50]},c= {[0 30], [40 50]},(a (b c) = {[25 31], [35 70]}(a b) (a c) = {[25 70]}.Thus, clearly (a (b c) (a b) (a c). However, distributive property holdsoperations lc lc labeled TCN.Theorem 4. using labeled constraints I-L-Sets, lc distributes lc.Proof: Lets consider labeled constraints lc , lc j lc k . Thus,(lc lc lc j ) lc (lc lc lc k )expressed, according definition operation lc, as:(lecp lc , lecq lc j , lc[(lecp lc lecq )]) lc (lecrlc , lecs lc k , lc[(lecr lc lecs)]) =lecp lc , lecq lc j , lecrlc , lecs lc k (lc[(lecp lc lecq)] lc lc[(lecr lc lecs )])which, according definition lc, expressed as:lecp lc , lecq lc j , lecrlc , lecs lc k (lc[(lecp lc lecq ) lc (lecr lc lecs )])(e1)expression, lecp lecr elemental constraints same-labeled constraint lc .However, set-union label sets associated pair elemental constraints (inputderived) labeled constraint I-L-Set (Definition 3). is, lecp lecr, {labelp }{labelr}I-L-Set. Thus, lecp lecr, label set associated (lecp lc lecq ) lc (lecr lc lecs ) I-LSet. consequence, (lecp lc lecq ) lc (lecr lc lecs ) rejected operation lc. is,lecp lc , lecq lc j , lecrlc , lecs lc k / lecplecr (lc[(lecp lc lecq ) lc (lecr lc lecs )]) = .Thus, expression (e1) results:lecp lc , lecq lc j , lecslc k (lc [(lecp lc lecq ) lc (lecp lc lecs )]).expression, lc clearly distributes lc elemental constraints (i.e.: non-disjunctiveconstraints). Therefore:lecp lc , lecq lc j , lecslc k (lc [(lecp lc (lecq lc lecs ))]) =lecp lc , lc [lecp lc (lecqlc j , lecs lc k , lc [lecq lc lecs ])] = lc lc (lc j lc lc k ).is, lc distributes lc labeled constraints.instance, following previous example:a= {[0 1] {R1}, [10 20] {R2}},b= {[25 50] {R0}},c= {[0 30] {R3}, [40 50] {R4}}{R1 R2 }, {R3 R4 } I-L-Sets. Thus, have:(a lc (b lc c) = {[0 1] {R1}, [10 20] {R2}} lc ({[25 50] {R0}} lc {[0 30] {R3}, [40 50] {R4}}) ={[0 1] {R1}, [10 20] {R2}}lc {[25 30] {R3 R0}, [40 50] {R4 R0}} =46fiBARBER{[25 31] {R1 R3 R0}, [40 51] {R1 R4 R0}, [35 50] {R3 R2 R0}, [50 70] {R4 R2 R0}}.Also,(a lc b) lc (a lc c) =({[0 1] {R1}, [10 20] {R2}} lc {[25 50] {R0}}) lc({[0 1] {R1}, [10 20] {R2}} lc {[0 30] {R3}, [40 50] {R4}}) ={[25 51] {R1 R0}, [35 70] {R2 R0}} lc {[0 31] {R1 R3}, [40 51] {R1 R4} [10 50] {R2 R3}, [50 70] {R2 R4}} =lc ([25 31] {R1 R3 R0}, [40 51] {R1 R4 R0}, [25 50] {R1 R2 R3 R0},[50 51] {R1 R2 R4 R0}, [40 51] {R1 R2 R4 R0}, [35 50] {R3 R2 R0}, [50 70] {R4 R2 R0}).However, {R1 R2 }, {R3 R4 } I-L-Sets. Thus, ([25 50] {R1 R2 R3 R0}, [50 51] {R1 R2 R4 R0}, [40 51] {R1R2 R4 R0}) removed operation lc. Therefore,(a lc b) lc (a lc c) = {[25 31] {R1 R3 R0}, [40 51] {R1 R4 R0}, [35 50] {R3 R2 R0}, [50 70] {R4 R2 R0}}.is, (a lc (b lc c) = (a lc b) lc (a lc c).4. Reasoning Algorithms Labeled ConstraintsSeveral algorithms reasoning disjunctive constraints applied managementlabeled temporal constraints, using lc, lc, lc lc operations. instance, wellknown Transitive Closure Algorithm, general closure algorithms (Dechter, 1992; Dechter et al.,1991; van Beek & Dechter, 1997), CSP-based approaches, etc. However, Montanari (1974) showscomposition operation distributes intersection, path-consistent TCN alsominimal TCN. Theorem 4, lc distributes lc. Thus, application pathconsistent algorithm proposed-labeled TCN obtain minimal TCN. Thus, TCAalgorithm could used closure process labeled constraints, similar way Allen(1983) uses it. However, incremental reasoning process proposed basis incrementalpath-consistent algorithm non-disjunctive metric constraints described Barber (1993).incremental reasoning process useful temporal constraints initially knownsuccessively deduced independent process; instance, integrated planningscheduling system (Garrido et al., 1999). proposed reasoning algorithm similar TCAalgorithm. However, updating closure processes performed new input constraint.Thus, new input constraint updated closured previously minimal TCN (Figure 9).Therefore, propagation modified constraints closure process needed. Moreover,proposed reasoning algorithms obtain complete sound set I-L-Sets.specification reasoning processes described Section 4.1. propertiesprocesses described later Section 4.2.4.1 Updating ProcessGiven previous labeled-TCN, composed set nodes {ni }, set labeled constraints {lc ij }among them, set I-L-Sets, updating process new c ij nodes ni njconstraint detailed Figure 2.47fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSUpdating (ni cij nj);cij {ecij., ecij.2 , ..., ecij.l }, disjunctive metric constraint.lc'ij Put-Labels (cij ), ;An exclusive label associated elemental constraintecij.k cijConsistency-Test (lc ij , lc'ij ) ;Consistency test lc'ij. previously existingconstraint n nj lcij . Moreover, new I-L-Setsdetected.(*Inconsistent Constraint*)Return (false)Else (*Consistent Constraint*)lc ij lc ij lc lc'ij ,lc ji Inverselc (lc ij ),Closure (ni lc ij nj ), ;Closure algorithm updated constraint.Return (true)End-IfEnd-UpdatingFigure 2: Updating process labeled constraintsfunction Put-Labels(cij ) returns labeled-constraint lcij {lecij.1 , lecij.2 , ..., lecij.l },associating exclusive label elemental constraint cij. one disjunct cij ,label unique elemental constraint {R0 }. Otherwise, pair labels lcij addedset I-L-Sets, since elemental constraints cij pairwise disjoint (Definition 3). usingInverse function non-labeled constraints, Inverse lc function is:Inverselc ({(ec ij.k {labelij.k })}) =def {(Inverse (ec ij.k ) {labelij.k })}described updating process performed time one new input constraint cijasserted previous TCN. Thus, initial TCN nodes, constraints, I-L-Setsassumed (Figure 9). new input constraint (cij ), TCN incrementally updatedclosured. is, cij consistent (Consistency-Test function), constraint cij addedTCN, closure process (Closure function) propagates effects TCN, new TCNobtained. new updating process performed new TCN, successively.4.1.1. CONSISTENCY-TEST FUNCTIONConsistency-Test function (Figure 3) based operation lc. new input constraint lc'ijnodes ni nj consistent temporally intersects previously existing constraintlc ij nodes. Moreover, Consistency-Test function detect new I-L-Sets:i)new constraint lc'ij consistent existing constraint lc ij , two elementalconstraints ecij.p lc'ij , ecij.klc ij intersect (ecij.k ecij.p =), label set{labelij.k }{labelij.p } I-L-Set added current set I-L-Sets.ii)existing elemental constraint nodes ni nj (lecij.klc ij) intersectnew constraint lc'ij , {labelij.k } I-L-Set added current setI-L-Sets.48fiBARBERConsistency-Test (lc ij , lcij ) =(lc ij lc lcij ) = {}Return (False)Elselecij.k lc ij , lecij.p lc'ij / lecij.k lc lecij.p ={}I-L-Sets I-L-Sets ({labelij.k }{labelij.p }),lecij.k lc ij / lecij.k lc lcij = {}I-L-Sets I-L-Sets {labelij.k },End-IfReturn (True)End- Consistency-TestFigure 3: Consistency-Test functionexample,Consistency-Test ({([0 10] {R1}), ([20 25] {R2}), ([100 110] {Ra})},{([0 30] {R3}), ([40 50] {R4}), ([-50 -40] {Rb})}) = Truesince{{([0 10] {R1}), ([20 25] {R2}), ([100 110] {Ra})} lc {([0 30] {R3}), ([40 50] {R4}), ([-50 -40] {Rb})} ={([20 25] {R3 R2}), ([0 10] {R3 R1})} {}.function, label sets {R4 R2 }, {R4 R1 } {Ra} detected I-L-Setsadded current set I-L-Sets, since:{[20 25] {R2}} lc {[40 50] {R4}}={},{[0 10] {R1})} lc {[40 50] {R4})}={},{([100 110] {Ra})} lc {([0 30] {R3}), ([40 50] {R4}), ([-50 -40] {Rb})}={}.Note {Rb } need detected I-L-Set, since label Rb includedfinal constraint {([20 25] {R3 R2}), ([0 10] {R3 R1 })} added TCN.superset I-L-Set also I-L-Set (Theorem 1). Moreover, note {R4 R2}, {R4 R1 }need added set I-L-Sets, since label R4 included final constraint.Therefore, following simplifications also performed time new I-L-Set addedcurrent set I-L-Sets. simplifications modify results reasoning processes,minimize size set I-L-Sets improve management efficiency.i)new I-L-Set superset existing I-L-Set added set I-L-Sets.ii)existing I-L-Set superset new I-L-Set, existing I-L-Set removed.iii) new I-L-Set contains label lc'ij , appear labeled constraint(lc ij lc lc'ij ) added TCN, added set I-L-Sets.Lets see example updating consistency-test processes. Lets take labeled-TCNresults Example 1 following constraints updated closured:49fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSSet Inconsistent-Label-Sets: {{R1 R 2}, {R3 R 4}}{([60 70] {R0})}t4{([40 50] {R3})]([20 30] {R4})}{([-10 20] {R3 R0})([10 40] {R4 R0})}t3t1{([-10 20] {R3 R0})([10 40] {R4 R0})}T0{([40 60] {R2 R0})([70 ) {R1 R0})}{([40 ) {R1 R3 R0})([20 ) {R1 R4 R0})([-10 30] {R2 R4 R0})([10 50] {R2 R3 R0})}{([10 30] {R3 R0})([30 50] {R4 R0})]}t2{((- 0] {R1 R0})([0 30] {R2 R0})}{([60 ) {R1})([30 40] {R2})}T0{([10 20] {R0})]}t4t1Figure 4: resulting labeled-TCN Figure 1 updating (t3 {[10 20]} t2 )(t1 {[60 )R1, [30 40] R2 } t2 ), (t3 {[40 50] R3 , [20 30] R4 } t4), (T0 {[10 20]R0 } t1 ), (T0 {[60 70]R0 } t4 ).resulting labeled-TCN shown Figure 4 set I-L-Set {{R1 R2 }, {R3 R4 }}.Now, update (t3 {[10 20]R0 } t2 ). previously existing constraint t3 t2 (Figure 4):{([40 ){R1 R3 R0 }) ([20 ){R1 R4 R0}), ([-10 30] {R2 R4 R0}) ([10 50] {R2 R3 R0})}Consistency-Test function performs:{[10 20] {R0}} lc {([40 ){R1 R3 R0 }) ([20 ){R1 R4 R0}), ([-10 30] {R2 R4 R0}) ([10 50] {R2 R3 R0})} ={[20 20] {R1 R4 R0}, [10 20] {R2 R0} []{R1 R3 R0}} {}(e1)Thus, (t2-t3{[10 20] {R0}}) consistent. Moreover, {R1 R3 R0 } detected I-L-Set.elemental constraints associated {R1 R3 R0 } inconsistent set disjuncts cannot holdsimultaneously. is:"If today John left home 7:10 7:20 (R0 ), Fred arrived work 8:008:10 (R0 ) John arrived work 10'-20' Fred left home (R0 ),impossible John gone bus (R 1 ) Fred gone carpool (R 3 )."set I-L-Sets obtained reasoning process considered special derivedconstraints, express inconsistency set input elemental constraints. instance,I-L-Set {R0 R1 R3 } represents (Figure 1):( (T0 [10 20] T1 ) (T3 [10 20] T2 ) (T0 [60 70] T4 ) (T3 [40 50] T4 ) (T1 [60 ) T2 )).expression non-binary constraint. type constraints could representeddisjunctive linear constraint, Jonsson Bckstrm (1996), Stergiou Koubarakis (1996)show. However, input elemental constraints represented derived constraints ablederive inconsistent sets input elemental constraints. model, done meanslabel sets associated labeled elemental constraints.50fiBARBER4.2 Closure Processclosure process (Figure 5) applied time new input constraint (lc'i j ) updated,effects lc'ij propagated TCN.Closure (ni lc ij nj)(* First loop: Closure n n j n k *)nk TCN / lc jk {U{R0}}:lc ik lc ik lc (lc ij lc lc jk ), lc ki Inverse(lc ik)(* Second loop: Closure n j ni nl *)nl TCN / lc il {U{R0}}:lc jl lc jl lc (Inverse(lc ij ) lc lc il ), lc lj Inverse(lc jl )(* Third loop: Closure nl ni nj nk *)1nl , nk TCN / lc li {U{R0}}, lc jk {U{R0}}:lc lk lc lk lc (lc li lc lc ij lc lc jk ), lc kl Inverse(lc lk )End-ClosureFigure 5: closure process labeled constraints(3)nl.1nl.ilc il.i(2)nink.1(1)lcijnjnk.ilcjk.ink.tnl.sFigure 6: Loops Closure Processclosure process three loops (Figure 6). loops process obtains:1i)Derived constraints lc ik ni node nk , nk previously connected nj(edge 1 Figure 6).ii)Derived constraints lc lj nj node nl , nl previously connected ni(edge 2 Figure 6).loop could simplified as:(*n l n n k*): n l, n k TCN / lc li {U{R0}}, lc j k {U{R0}}:lc lk lc lk lc (lc li lc lc ik),(*n l nj n k*): n l, n k TCN / lc li {U{R0}}, lc j k {U{R0}}: lc lk lc lk lc (lc lj lc lcj k)since lc ik (or lc lj ) already closured first (or second loop). Moreover, efficiency third loopimproved modified constraints first (or second loop) considered.51fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSSet Inconsistent-Label-Sets: {{R1 R2 }, {R3 R4}, {R1 R3 R0}}{([60 70]{R2 R0}) ([70 70]{R4 R1 R0})}t4{([60 60]{R4 R1 R0})([40 60]{R4 R2 R0})([50 60]{R3 R2 R0})}t1 {([10 30]{R4 R2 R0})([10 20]{R3 R2 R0})([40 40]{R4 R1 R0})}{([40 50]{R3 R2 R0})([20 30]{R4 R2 R0})([20 20]{R4 R1 R0})}t3T0{([40 50]{R2 R3 R0})([40 60]{R2 R4 R0})([70 70]{R1 R4 R0})}{([10 20]{R2 R0}) ([20 20] {R4 R1 R0})}{([50 50]{R4 R1 R0})([20 30]{R3 R2 R0})([30 50]{R4 R2 R0})}t2{([0 0] {R1 R4 R0})([20 30]{R2 R3 R0})([0 20]{R2 R4 R0})}{([30 40]{R2 R0})([60 60]{R1 R4 R0})}T0{([10 20]{ R2 R0}) ([10 10] {R4 R1 R0})}t4t1Figure 7: Labeled-Minimal TCN Example 1iii) Derived constraints lc lk pair nodes nl nk , nl nk previouslyconnected ni nj respectively (edge 3 Figure 6).Lets see previous Example 1 represented Figure 1 Figure 4, consistentconstraint (expression e1):(t3 {[20 20] {R1 R4 R0}, [10 20] {R2 R0}} t2 )closured. first loop closure process, have:lc 30 lc 30 lc ({[20 20] {R1 R4 R0}, [10 20] {R2 R0}} lc lc 20 ={[-30 -10] {R3 R0 } [-50 -30] {R4 R 0 }} lc({[20 20] {R1 R4 R0}, [10 20] {R2 R0}} lc {[-60 40] {R2 R0} (- -70] {R1 R0}}) ={[-30 -10] {R3 R0}} [-50 -30] {R4 R0}} lc{[-40 -20] {R1 R2 R4 R0}, (- -50] {R1 R4 R0} [-50 20] {R2 R0} (- -50] {R1 R2 R0}}.However, {{R1 R2 }, {R3 R4 } {R0 R1 R3 }} I-L-Sets. labeled elemental constraints whoseassociated label set superset I-L-Sets derived (Theorem 3). Thus:lc 30 {[-30 -10] {R3 R0}} [-50 -30] {R4 R0}} lc {(- -50] {R1 R4 R0} [-50 20] {R2 R0} }={(-30 -20] {R2 R3 R0} [-50 50] {R4 R1 R0} [-50 -30] {R4 R2 R0}}.Similarly,lc 31 lc 31 lc ({[20 20] {R1 R4 R0} [10 20] {R2 R0}} lc lc 21 ={[-20 -10] {R3 R2 R0 } [-40 -40] {R4 R1 R0} [-30 -10] {R4 R2 R0}}lc 34 lc 34 lc ({[20 20] {R1 R4 R0}, [10 20] {R2 R0}} lc lc 24 ={[40 50] {R3 R2 R0 } [20 30] {R4 R2 R0} [20 20] {R4 R1 R0}}.second third loops, final labeled-TCN obtained (Figure 7). final set I-LSets {{R1 R2 }, {R3 R4 } {R0 R1 R3 }}. sets represent sets mutually inconsistent inputelemental constraints exist TCN Figure 1.52fiBARBER4.3 Properties Reasoning Algorithmssection, main properties proposed reasoning algorithms described.Theorem 5. proposed updating closure processes (Sections 4.1 4.2) guaranteeconsistent TCN applied previous minimal (and consistent) TCN.Proof: updating constraint lcij asserted TCN consistent previous minimalconstraint lc ij (Consistency-Test function).Theorem 6. proposed closure algorithm obtains path-consistent TCN, appliedprevious minimal TCN.Proof: detailed Barber (1993) non-disjunctive TCNs applied labeledTCNs. have:i)derived constraint exist pair nodes path combinesasserted constraint lc ij .ii)closure process computes derived constraint pair nodes (nl , nk )become connected path across closured constraint lc ij . Lets assume existing pathnodes nx1 , ny1 includes lc ij :nx1 , nx2 , nx3 , ........, nx, (nj lc ij nj ), ny ......, ny2 , ny1derived constraint nx1 ny1 computed. However, minimalconstraint (nx1 , ni ) (nj , ny1 ) already exist previous minimalTCN. consequence, derived constraint (nx1 , ny1 ) computed third loopprocess.iii) previous TCN minimal, possible derived constraints existpair nodes (nl, nk ) already computed constraint lclk derived nodesproposed closure process. third loop, process obtains:lclk = lc lk lc (lc li lc lc ij lc lc jk ).Lets suppose exists another path (nl , nk ) across updated lc ij constraint: (nl ,np , ni , nj , nq , nk ). path computes another derived constraint (nl , nk ):lc''lk = lc lk lc (lc lp lc lc pi lc lc ij lc lc jq lc lc qk ).However, since previous TCN minimal, previously existing minimal constraintslc li lc jk imply (lc lp lc lc pi ) (lc jq lc lc qk), respectively. is, lc li lc(lc lp lc lc pi )lc jk lc(lc jq lc lc qk ) Thus, lc''lk also implicitly implied lclk (lclk lclc''lk ). Here,assumed associative property lc, obvious definition.iv) Derived constraints obtained closure process need closuredprevious TCN minimal. is, constraint TCN would become restrictedderived constraints also closured. Let suppose lc lk modified third loopclosure process:lclk = lc lk lc (lc li lc lc ij lc lc jk )propagated (nl , nk , np ) subTCN (Figure 8). Thus, followingderived constraints obtained:53fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSlclp = lc lp lc (lclk lc lc kp )lcpq = lc pq lc (lc pl lc lclk ).constraint lclp , have,lclp = lc lp lc (lclk lc lc kp ) = lc lp lc ((lc lk lc (lc li lc lc ij lc lc jk )) lc lc kp ).However, since lc distributes lc,lclp = lc lp lc ((lc lk lc lc kp ) lc (lc li lc lc ij lc lc jk lc lc kp )).Since previous TCN minimal, minimal constraints lc pi lc pj previouslyexist, lclp lc(lc lk lc lc kp ) lc jp lc(lc jk lc lc kp ). Thus,lclp lc lc lp lc (lc li lc lc ij lc lc jp ).However, third loop closure process, following derived constraintcomputed:lc''lp = lc lp lc (lc li lc lc ij lc lc jp ).Thus, lclp already represented obtained constraint lc''lp (that is, lc''lp lc lc'lp ).similar way,lc''pq = lc pq lc (lc pi lc lc ij lc lc jq )also obtained proposed closure process, lc''pq lc lc'pq .Therefore, derived constraint (any combinable path across lc ij ) pair nodesTCN computed, closure process obtains path-consistent TCN.nplclplcpklclknllcljlc ijninknjlcjkFigure 8: lc lk also propagated lc lp lc pqTheorem 7. proposed reasoning processes obtain minimal TCN, previous TCNminimal TCN.Proof: Montanari (1974) shows composition distributes intersection (i.e.:distributes ), path-consistent TCN also minimal TCN). case nondisjunctive metric TCNs (Dechter et al., 1991). case, lc distributes lc (Theorem 4)closure process obtains path consistent TCN (Theorem 6). Therefore, proposed reasoningprocesses also obtain minimal TCN.54fiBARBERNew inputconstraintINITIALTCNnodes,constraints, I-L-SetsInput Constraint( ni lcij nj )(ni lcij nj ) consistentReasoning Process: Updating + Closure processesConsistency-Test: Consistent TCNClosure Process: Path-Consistent TCN.Distributive Property ( lc lc): MinimalNew consistent minimal TCNNew complete sound set I-L-SetsFigure 9: incremental reasoning processTheorem 8. updating process, reasoning algorithms obtain complete sound new setI-L-Sets (Definition 4), applied previous minimal TCN previous soundcomplete set I-L-Sets.Proof:i) new set I-L-Sets complete. consistency test updated constraint lc'ij obtainspossible new I-L-Sets appear lc'ij added TCN, except I-L-Setsrelated mutual exclusion disjuncts lc'ij (which determinedPut-Label function):a) new I-L-Sets appear label lc'ij participate. Otherwise,would detected previous updating process, since previous setI-L-Sets assumed complete. Thus, label lcij always participatenew I-L-Set appears lcij updated.b) new I-L-Sets (in label lcij participates) detected consistencytest lcij. Let's assume new undetected I-L-Set exists {Rk , R1 , R2 , ....., Rp }new elemental constraint eck{Rk}lc'ij takes part. Thus, elementalconstraints associated {R1 , R2 , ....., Rp } compute derived elemental constraint ecxnodes ni nj :(ecx {R1, R2, ....., Rp})/(ecx {R1, R2, ....., Rp}) lc (eck{Rk}) =elemental constraint ecx already represented previously existing constraint lc ij55fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSni nj since previous TCN minimal2 . Thus, eck ecx=, I-LSet {Rk , R1 , R2 , ....., Rp } detected consistency test lcij . conclusion, newinconsistent sets elemental constraints lc'ij participates detectednew I-L-Sets exist. Therefore, new set I-L-Sets complete previous setI-L-Sets complete.ii) new set I-L-Sets sound. new I-L-Sets obtained represent inconsistent setselemental constraints. trivial, given consistency test function.conclusion, proposed reasoning algorithms obtain minimal (and consistent) TCNapplied previous minimal-TCN (Figure 9). Therefore, reasoning algorithms guaranteeTCN consistency obtain minimal TCN complete sound set I-L-Sets newinput assertion.4.4 Global Labeled-Consistencyminimal (binary) disjunctive network, every subnetwork size two globally consistent(Dechter, 1992). Therefore, local consistent instantiation subset two variablesextended full consistent instantiation. However, assure local consistent instantiationsubset two variables overall consistent, partial instantiation propagatedwhole TCN (van Beek, 1991). Thus, assembling TCN solution become costlypropagation process disjunctive TCNs, even though minimal TCN used. proposedreasoning processes maintain complete sound set I-L-Sets (Theorem 8). Thus, deducelocally consistent set elemental constraints overall consistent means label setsassociated labeled elemental constraints set I-L-Sets. Specifically, deducewhether locally consistent instantiation k variables (1<k<n) overall consistent. Lets seefollowing example, based previous one proposed Dechter, Meiri Pearl (1991):Example 2: "Dave goes walking work [25 50]. John goes work either car[10 30'], bus [45 60]. Fred goes work either car [15' 20'],carpool [35' 40'], walking [55 60]. Today, lefthome 6:50 7:50 (at t1, t2 t3 time-points), arrivedwork time (t4 ) 8:00."Here, following labeled disjunctive constraints where, T0 represents initial time(6:50) granularity minutes:t1 - T0 {[0 60]R0 },t4 - 1 {[25 50]R0 },2 - T0 {[0 60]R0 },3 - T0 {[0 60]R0 },4 2 {[10 30]R1 , [45 60]R2 },4 - T0 {[0 70]R0 },4 3 {[15 20]R3 , [35 40]R4 , [55 60]R5 }.minimal TCN Example 2 represented Figure 10. Here, binary constraintstime-point T0 represent unary constraints restrict interpretation domains variables(t1 , t2 , t3 , t4 ). Obviously, minimal TCN globally consistent TCN. instance,2elemental constraint ec x already represented explicit way, means another elemental constraint ecy(ecy Tec x, {labely }{R1, R2, ....., Rp}) due simplification process performed operation lc . cases,ec kec x=, ec kecy =.56fiBARBERinstantiations {(t1 =0), (t2=0), (t3=0)} consistent existing constraints involved among (T0 ,t1 , t2 , t3 ), partial solution cannot extended overall TCN.{[0 45]}t1{[25 50]}{[-35 35]}{[0 55]}T0{[-35 40}]t3{[15 20] [35 40] [55 60]}t4{[-50 45]}{[10 30] [45 60]}{[0 60]}t2{[25 70]}Figure 10: Minimal TCN Example 2Lets consider TCN labeled constraints. reasons simplicity, denotelabeled constraints among (T0 , t1 , t2 , t3 ):(T0 {[5 45]{R0 R5}, [0 45]{R0 R4}, [0 45]{R0 R3}} 1 ),(T0 {[0 25]{R2 R0}, [5 60]{R1 R0 R4}, [25 60]{R1 R0 R5}, [0 60]{R1 R0 R3}} 2 ),(T0 {[25 55]{R0 R2 R3}, [0 15] {R0 R5}, [0 35]{R0 R1 R4}, [5 55]{R0 R1 R3}, [5 35]{R0 R2 R4}} 3 ),(t 1 {([-5 35]{R0 R2}, [-40 5] {R0 R1}} 2 ),(t 1 {[-15 15]{R0 R4}, [-35 -5]{R0 R3}, [5 35]{R0 R5}} 3 ),(t 2 {[5 30] {R1 R0 R4}, [-45 -25]{R2 R0 R3}, [25 50]{R1 R0 R5}, [-15 10]{R1 R0 R3}, [-25 -5]{R2 R0 R4}, [-5 15]{R2 R0 R5}} t3 ).set I-L-Sets {{R1 R2 } {R3 R4 } {R3 R5 } {R4 R5}}. labeled TCN setI-L-Sets, deduce instantiations {(t1 =0), (t2 =0), (t3 =0)} overall consistent.instantiations locally consistent labeled constraints subTCN (T0 , t1 , t2 , t3 ):label sets associated possible simultaneous fulfillment(T0 {[0 0]} 1 ), (T0 {[0 0]} 2 ) (T0 {[0 0]} 3 )I-L-Sets. is, label sets Cartesian product{{R0 R4 } {R0 R3 }} {{R2 R0 } {R1 R0 R3 }} {{R0 R5 } {R0 R1 R4 }}I-L-Sets. Thus, set I-L-Sets used deduce consistency set labeled elementalconstraints obtain globally consistent labeled-TCN.Theorem 9. Lets assume labeled-TCN n nodes (and corresponding complete sound setI-L-Sets) local set k (1k( n2 )) labeled elemental constraints TCN, onepair nodes:{lec1 , lec2 ,....., leck } {(ec 1 {label1 }), (ec2 {label2 }), ..., (ec k {labelk })}.57fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSlocal set labeled elemental constraints {lec1 , lec2 , ... , leck }is overall consistent iff setunion associated label sets (i=1,k {labeli }) I-L-Set.Proof: label set (i=1,k {labeli }) support-set simultaneous fulfillment {lec1 , lec2 ,--- , leck }. Moreover, set I-L-Sets complete sound respect overall TCN (Theorem8), label set set I-L-Set overall consistent. Therefore (Theorem 2),(i=1,k {labeli }) {lec1 , lec2 , ... , leck } overall consistent iff i=1,k {labeli } I-L-Set.Definition 5 (Labeled-consistency3 ): Lets assume labeled-TCN n nodes (and correspondingcomplete set I-L-Sets) set k (1k(n2 )) constraints, one pairnodes TCN:{c ij } / 1in, 1jn, ij.set constraints {c ij } labeled-consistent respect nodes involvedconstraints, iff:i)constraint cij , exists elemental labeled constraint elc ij.x (ni , nj )TCN elc ij.x satisfies cij . is: cij , elc ij.xlc ij / c ij ecij.x .ii)resulting set union label sets associated elemental labeled constraints(which satisfy {c ij }) I-L-Set: U cij{label ij.x} I-L-Set. Notecondition Theorem 9.Theorem 10. Lets assume labeled-TCN n nodes (and corresponding complete set I-LSets) set k (1k( n2 )) constraints, one pair nodesTCN:{c ij } / 1in, 1jn, ij.set constraints {c ij } overall consistent iff {c ij } labeled-consistent respectnodes involved constraints {c ij }.Proof: proof trivial according Definition 5 Theorem 9. setconstraints {c ij } consistent iff exists local set elemental constraints TCN {elc ij.x}makes {c ij} labeled-consistent (Definition 5). Thus, local set {elc ij.x} consistent (Theorem9), {c ij } also consistent.instance, determine whether pair constraints c' ij c'kl hold simultaneously(that is, overall consistent) if:elc ij.xlc ij / c' ij ecij.x elc kl.yckl / c'kl eckl.y {labelij.x}{labelkl.y }I-L-Set.Moreover, local instantiation k-1 (1<kn) variables {t1 =v1 , t2 =v2 , ..., t(k-1)=v(k-1)}extended global solution if:elc 10.xlc 10 / v1ec10.x,...... , elc (k-1)0.ylc (k-1)0 / v(k-1)ec10.x,lc i0 constraint ni T0 , {label10.x}{label20.y } .... {label(k-1)0.y }isI-L-Set.3need introduce concept labeled-consistency since different concept consistency concept.58fiBARBERinstance, Example 2 Figure 10, partial instantiation {(t1 =0), (t2 =5), (t3 =5)}consistent. have:([0 45]{R0 R3})lc10 / 0[0 45],([0 60]{R1 R0 R3})lc20 / 5[0 60],([5 55]{R0 R1 R3})lc30 / 5[5 55],{R0 R3 }{R0 R1 R3}{R0 R1 R3 }={R0 R1 R3 } IL-Set. Thus, partial solutionextended global solution. instance, {(t1 =0), (t2 =5), (t3 =5), (t4 =25)}.Therefore, labeled-TCN considered globally labeled-consistent TCN. is,basis concepts introduced Dechter (1992):Definition 6. (Local Labeled-consistency): partial instantiation variables (1k<n) {t1 =v1 , t2 =v2 ,..., tk =vk } local labeled-consistent labeled-consistent respect (T0 , t1 , t2 , ..., tk ) nodes.also holds k=n.Definition 7. (Global Labeled-consistency): labeled sub-TCN (with global set I-L-Sets)global labeled-consistent partial instantiation variables sub-TCN, locallabeled-consistent, extended overall TCN. globally labeled-consistent TCN onesub-TCNs globally labeled-consistent.Theorem 11. new assertion, proposed reasoning processes obtain globally labeledconsistent TCN, applied previous minimal TCN previous sound completeset I-L-Sets.Proof: proof trivial according previous definitions (Definition 6 Definition 7)properties reasoning processes (Theorem 7 Theorem 8). partial instantiationsubTCN, labeled-consistent respect nodes involved partialinstantiation, overall consistent (Theorem 10).Similar expressions made k-labeled-consistency strong k-labeled-consistencybasis concepts provided Freuder (1982). Therefore, set I-L-Sets labeled-TCNprovides useful way assure whether local instantiation variables part globalsolution. Moreover, Freuder (1982) shows strong k-consistent TCN, consistent instantiationsvariables subnetwork size k found backtrack-free manner variableordering. also consequence decomposability (Montanari, 1974; Dechter et al., 1991)globally consistency (Dechter, 1992) properties. Obviously, feature also holds labeledTCNs.4.5 Analysis Temporal ComplexityLets analyze computational cost proposed reasoning processes. processes are,basically, incremental path-consistent algorithm (Barber, 1993). updating processnew input constraint TCN n nodes, computational cost updating closureprocesses bounded 'n2 (O(lc) + O(lc))'. proposed reasoning process, path-consistentalgorithm obtains minimal disjunctive metric TCN. possible due managementlabeled constraints, associated label sets, I-L-Sets. Thus, complexity reasoning processesmainly due (instead complex closure process) management complex data structures(labeled constraints, associated label sets, I-L-Sets). is, complexity proposed59fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSreasoning processes mainly due complexity operations lc lc.computational cost lc lc depends number elemental constraints labeledconstraints, size associated label sets, size I-L-Sets previous minimal labeledTCN. Let 'n' number nodes, 'l' maximum number disjuncts (or labels) inputconstraints, 'e' number updated input constraints previous TCN. maximumnumber labels TCN l*e, since disjunct updated input labeled constraintown, unequivocal label. Moreover, I-L-Set maximum one label inputlabeled constraint lc ij , since: (i) elemental constraints lc ij pairwise disjoint, pairlabels lc ij added set I-L-Sets, (ii) superset existing I-L-Set alsoI-L-Set. Thus, maximum number labels I-L-Set e. Furthermore, label I-LSet different input labeled constraint. e input labeled constraints,input labeled constraint maximum l labels. Thus, maximum number I-L-Sets q-length(1qe) (( qe ) lq ).Therefore, number i-length (1ie) I-L-Sets i=1,e (( ei ) li ) = O(2e le). However,superset I-L-Set already known inconsistent, supersets stored setI-L-Sets. Thus, number I-L-Set bounded O(le). Additionally, also e*( l2 ) I-LSets 2-length, since l disjuncts updated constraint mutually exclusive among them.Similarly, maximum number associated label sets also bounded O(le), onemaximum e labels. Thus, number elemental constraints (or labeled subintervals)labeled constraint bound O(le), since elemental constraint labeled constraintassociated label set.According parameters, computational cost updating process boundedO(n2 l3e). recovery process constraints constant cost, since minimal-TCN alwaysmaintained. computational cost proposed algorithms agreed computational costinherent problem management disjunctive metric constraints (Dechter, 1991). fact,closure process could considered integrated management le alternative nondisjunctive TCNs disjunctive TCN split, shown Dechter, Meiri Pearl(1991). noted l bounded typical problems like scheduling,usually l 2 (Garrido et al., 1999), restricting domain size (range granularity) metricalgebras. hand, several improvements made described processes.example, efficient management label sets direct influence efficiencyreasoning processes. Thus, label set (for instance, {R3 R5 R8 }) consideredunidimensional array bits, binary representation integer number (for instance(23 +25 +28 )). Therefore, associated label set represented number set I-L-Setsbecomes set numbers. Matching set-union processes label sets operations lc lcefficiently performed means operations integer numbers constant cost.Therefore, computational cost bounded O(n2 l2e).alternative implementations study. Two different approaches exist temporalconstraint management (Brusoni et al., 1997; Yampratoom, Allen, 1993; Barber, 1993). firstapproach maintain closured TCN recomputing TCN new input constraintmaking derived constraints explicit. Here, queries answered constant time, althoughimplies high spatial cost. second approach explicitly represent input constraints,spatial requirements minimum. However, computation needed query timeconsistency new input constraint tested. proposed reasoning methods hold60fiBARBERfirst approach, seems appropriate problems queries TCNusual tasks updating processes.addition, proposed reasoning algorithms obtain sound complete set I-L-Setsglobally labeled-consistent TCN. Regrettably, assembling solution labeled TCN, althoughbacktrack free, also costly due exponential number I-L-Sets. However, features offercapability representing managing special types non-binary disjunctive constraints (laterdetailed Section 6).reasoning algorithms query processes non-closured TCN, well CSPapproaches defined basis labeled temporal algebra described. Less expensivealgorithms applied labeled constraints using specified operations lc, lc, lclc. instance, TCA algorithm applied Allen (1983), k-consistency algorithmslike described (Cooper, 1990; Freuder, 1978). Moreover, minimal TCN labeledconstraints obtained without enforcing global consistency; example, applying naivebacktracking algorithm described Dechter, Meiri Pearl (1991), O(n3 le).5. Interval-Based Constraints Labeled Point-Based Constraintsintegration quantitative qualitative information goal several temporalmodels, described Section 1. intervals represented means endingpoints Ii + Ii -, integration constraints intervals points seems require kind nonbinary constraints time-points (Gerevini & Schubert, 1995; Schwalb & Dechter, 1997;Drakengren & Jonsson, 1997). section, proposed temporal model applied orderintegrate interval point-based constraints. Constraints intervals managed meansconstraints ending points intervals I-L-Sets. Likewise, metric information also addedinterval constraints expressive way integrating qualitative quantitativeconstraints obtained.5.1 Symbolic Interval-Based ConstraintsSymbolic constraints intervals express qualitative temporal relation two intervals.symbolic constraint disjunctive subset 13 elemental constraints, mutuallyexclusive among (Allen, 1983). example, following constraintI1 {ec 1 , ec2 } I2 ,ec1 , ec2 {b, m, o, d, s, f, e, bi, mi, oi, di, si, fi},really means 'I1 [ (ec1 ec2 ) (ec1 ec2 ) ] I2', since ec 1 ec 2 mutually exclusive, oneone elemental constraint hold. reasons simplicity, consider two disjunctssymbolic constraint. However, expressions easily extended managing2 13 disjuncts. expression expressed as:I1 [ (ec1 ec2 ) (ec1 ec2 ) ] I2I1 [ (ec1 ec1 ) (ec 2 ec2 ) (ec1 ec2 ) (ec1 ec2 ) ] I2way, have:61(e2).fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSi)constraints [I1 (ec1 ec1 ) I2 ] [I1 (ec2 ec2 ) 2 ] expressed disjunctivemetric constraints pairs time-points,ii)constraints [I1 (ec 1 ec2 ) I2 ] [I1 (ec1 ec2 ) I2 ] expressed mutualexclusion among associated labels point-based constraints. is, setI-L-Sets.present simple example illustrate conclusions. instance, (I1 {before after} I2 )expressed means constraints among time points I1 -, I1 +, I2 - I2 +, as:[I1 {b a} I2 ] (I1 + {(0 ){Rb1}} I2 -) (I1 - {(- 0){Ra1}} I2 +).Thus, intervals represented means ending points Ii + Ii -, interval-basedconstraint gives rise disjunctive constraints different pairs time points (i.e.: non-binaryconstraints). non-binary constraints represented I-L-Sets. Thus, accordingexpression (e2),[I1 {b a} I2] [I1 (b b) I2 ] [I1 (a a) I2 ] [I1 (b a) I2 ] [I1 (b a) I2 ],have:I1 I2 I1 + {(0 ){Rb1}} I2 -,I1 I2I1 + {(- 0]{Rb2}} I2 -,I1 I2 I1 - {(- 0){Ra1}} I2 +,I1 I2 I1 - {[0 ){Ra2}} I2 +.Therefore, [I1 {b a} 2 ] expressed as:[I1 + {(0 ){Rb1} (- 0]{Rb2}} I2 -] [I1 - {(- 0){Ra1} [0 ){Ra2}} I2 +][ (I1 + {(0 ){Rb1}} I2 -) (I1 - {(- 0){Ra1}} I2 +) ][ (I1 + {(- 0]{Rb2}} I2 -) (I1 - {[0 ){Ra2}} I2 +) ],equivalent (by using labels associated elemental constraint):[I1 + {(0 ){Rb1} (- 0]{Rb2}} I2 -] [I1 - {(- 0){Ra1} [0 ){Ra2}} I2 +]{Rb1 Ra1},{Rb2 Ra2} I-L-Sets, one one disjunctive symbolic constraintholds.Thus, symbolic constraints intervals represented means of: (i) setdisjunctive metric constraints time-points, (ii) set I-L-Sets. Table 1,equivalent metric constraints interval ending time points elemental interval-basedconstraint detailed. According table, following steps allow us represent disjunctivesymbolic constraints intervals means disjunctive metric constraints intervalending points I-L-Sets:i)interval represented means ending points Ii +, Ii-. default, (I - {(0, ){R0}}Ii +) holds.ii)symbolic constraint two intervals (Ii cij Ij) composed disjunctive set(from 1 13) elemental symbolic constraints cij ={ecij.k }{b, m, o, d, s, f, e, bi, mi, oi, di,si, fi}.iii) elemental symbolic constraint ec{b, m, o, d, s, f, e, bi, mi, oi, di, si, fi} represented62fiBARBERconjunctive set disjunctive point-based metric constraints (fourth column Table1). conjunctive set point-based constraints expresses fulfillment nonfulfillment (ec ec) elemental symbolic constraint ec.iv) disjunctive set cij ={ecij.k } elemental symbolic constraints Ii Ij representedby:conjunctive set disjunctive point-based metric constraints time-pointsIi +, Ii -, Ij+ I-j . conjunctive set composed constraints fourth columnTable 1 elemental constraint {ecij.k }.set I-L-Sets expresses logical relation among elemental symbolicconstraints {ec ij.k }. is, 'one one elemental symbolic constraint {ecij.k}hold':iv.a) one elemental constraint {ecij.k } hold. conditionneed represented since different sets point-based constraintscorrespond fulfillment different elemental symbolic constraints (secondcolumn Table 1) already mutually exclusive.iv.b)One elemental symbolic constraints {ecij.k } hold. Letlabel sets, label set corresponds point-based constraintsrelated non-fulfillment elemental symbolic constraint {ec ij.k }(third column Table 1). Thus, Cartesian product among label setsset I-L-Sets.instance, I1 {b di} I2 represented as:(I1 - { (0 ){R0}} I1 +), (I2 - { (0 ) {R0}} I2 +),I1 {b b} I2 (I1 + {(0 ){Rb1} (- 0]{Rb2}} I2 -),I1 {m m} I2 (I1 + {[0 0] {Rm1} (0 ){Rm2} (- 0){Rm3}} I2 -),I1 {s s} I2 (I1 - {[0 0] {Rs1} (0 ){Rs3} (- 0){Rs4}} I2 -) (I1 + {(0 ){Rs2} (- 0]{Rs5}} I2 +),I1 {di di} I2 I2 {d d} I1 (I2 - {(- 0){Rd1} [0 ){Rd3}} I1 -) (I2 + {(0 ){Rd2} (- 0]{Rd4}} I1 +).Moreover, one symbolic constraints {b, m, s, di} hold. Thus (according Pointiv.b method), Cartesian product associated labels related non-fulfillmentelemental symbolic constraints {b, m, s, di}. is:{{Rb2 }{Rm2 , Rm3 }{Rs3 , Rs4 , Rs5 }{Rd3 , Rd4 }explicitly included set I-L-Sets.applying method, qualitative interval-based constraints fully integratedproposed labeled point-based constraints. case, interpretation domain time-points {Ii Ii +} restricted three values ({D}={(-, 0), [0 0], (0 )}), that, l=3. Therefore,computational cost reasoning algorithms bounded O(n2 3 2e).illustrate proposed method, lets show typical example symbolic interval-basedconstraints (Figure 11.a), given Allen (1983). example shows interval-basedconstraints represented managed means disjunctive metric point-based constraintsminimal IA-TCN obtained.63fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSIi ecij.k IjIi ecij.k IjIi ecij.k IjIi (ecij.k ecij.k) IjIi IjIi + {(0 ){Rb1}} Ij -Ii + {(- 0]{Rb2}} Ij -Ii + {(0 ){Rb1} (- 0]{Rb2}} Ij -Ii meets IjIi + {[0 0]{Rm1}} Ij -Ii + {(0 ){Rm2} (- 0){Rm3}} Ij -Ii + {[0 0]{Rm1} (0 ){Rm2} (- 0){Rm3}} Ij -Ii IjIi - {(- 0){Rd1}} Ij -(Ii - {[0 ){Rd3}} Ij -)Ii - {(- 0){Rd1} [0 ){Rd3}} Ij -Ii + {(0 ){Rd2}} Ij +Ii starts IjIi + {[0 0]{Rf1}} Ij +-Ii {(- 0){Rf2}} Ij-Ii + {(0 ){Rd2} (- 0]{Rd4}} Ij +Ii - {[0 0]{Rs1} (0 ){Rs3} (- 0){Rs4}} Ij Ii + {(0 ){Rs2} (- 0]{Rs5}} Ij +(Ii + {(- 0]{Rs5}} Ij +)(Ii + {(0 ){Rf3} (- 0){Rf4}} Ij +)Ii + {[0 0]{Rf1} (0 ){Rf3} (- 0){Rf4}} Ij +(Ii {[0 ){Rf5}} Ij )-Ii - {(- 0){Rf2} [0 ) {Rf5}} Ij --(Ii + {[0 ){Ro4}} Ij -)+Ii overlaps Ij Ii {(- 0){Ro1}} IjIi equal Ij(Ii + {(- 0]{Rd4}} Ij +)(Ii - {(0 ){Rs3} (- 0){Rs4}} Ij -)Ii - {[0 0]{Rs1}} Ij Ii + {(0 ){Rs2}} Ij +Ii finishes IjIi + {(- 0){Ro1} [0 ){Ro4}} Ij -Ii + {(0 ){Ro2}} Ij +(Ii + {(- 0]{Ro5}} Ij +)Ii + {(0 ){Ro2} (- 0]{Ro5}} Ij +Ii - {(0 ){Ro3}} Ij -(Ii - {(- 0]{Ro6}} Ij -)Ii - {(0 ){Ro3} (- 0]{Ro6}} Ij -Ii + {[0 0]{Re1}} Ij +(Ii + {(0 ){Re3} (- 0){Re4}} Ij +)Ii + {(0 ){Re3} [0 0]{Re1} (- 0){Re4}} Ij +Ii - {[0 0]{Re2}} Ij -(Ii - {(0 ){Re5} (- 0){Re6}} Ij -)Ii - {(0 ){Re5} [0 0]{Re2} (- 0){Re6}} Ij -Table 1: Interval-based constraints equivalent disjunctive metric constraintsinterval ending points (Cells second fourth columns conjunctive set constraints)SymbolicConstraint(IA {d di} IB)(IB {d di} IC)(ID {m s} IA)(ID {o} IB)(ID {m s} IC)Disjunctive Metric Constraint I+ I-Inconsistent-Label-SetsIA - {(- 0){R1} [0 ){R3}} IBIA + {(0 ){R2} (- 0]{R4}} IB+IB- {(- 0){R5} [0 ){R7}} IA IB+ {(0 ) {R6} (- 0]{R8}} IA +{R4 R8 } {R3 R8 }{R4 R7 } {R3 R7 }IB- {(- 0){R9} [0 ){R11}} ICIB+ {(0 ) {R10} (- 0]{R12}} IC+IC- {(- 0){R13} [0 ){R15}} IBIC+ {(0 ) {R14} (- 0]{R16}} IB++ID {[0 0]{R17} (0 ){R18} (- 0){R19}} IA ID- {[0 0]{R20} (0 ){R22} (- 0){R23}} IA ID+ {(0 ){R21} (- 0]{R24}} IA +ID+ {(- 0){R0}} IBID+ {(0 ){R0}} IB+ID- {(0 ){R0}} IBID+ {[0 0]{R25} (0 ){R26 (- 0){R27}} ICID- {[0 0]{R28} (0 ){R30} (- 0){R31}} ICID+ {(0 ){R29} (- 0]{R32}} IC+{R12 R16 } {R11 R16 }{R12 R15 } {R11 R15 }{R19 R24 } {R18 R24 } {R19 R23 }{R18 R23 } {R19 R22 } {R18 R22 }{R27 R32 } {R26 R32 } {R27 R31 }{R26 R31 } {R27 R30 } {R26 R30 }Table 2: Symbolic constraints Figure 11.a means disjunctive metricconstraints I+, I-64fiBARBERFigure 11.a represents path-consistent IA-TCN, inconsistent values constraints(Allen, 1983). Table 2, interval-based symbolic constraints example,corresponding disjunctive metric constraints ending time -points (Ii +, Ii -)corresponding set I-L-Sets (according Table 1). Moreover, also have:(IA -{(0 ){R0}}IA +), (IB-{(0 ){R0}}IB+), (IC-{(0 ){R0}}IC+ ) (ID- {(0 ){R0}}ID+).metric constraints among ending time-points intervals updated accordingproposed methods Section 4, labeled minimal TCN Table 3 obtained. associatedlabels elemental constraint (disjunct) constraints included reasons brevity.{s,m}{s,m}{o}{d, di}B{d,di}{d, oi, f, e, fi,si, s, , di}{o}{d, di}B{d,di}{s,m}{s,m}{d, di, s, si, e}Ca) Path-Consistent IA-TCNCb) Minimal IA-TCNFigure 11: Path-Consistent equivalent Minimal IA-TCNIA+IA+IA-IB+{(- 0)}{(0 ),(- 0)}{(0 )}IA-{(0 )}IB+{(- 0), {(- 0)}(0 )}IB-{(0 )}{(0 ),(- 0)}IB-{(0 )}{[0 0],(0 )}{(- 0),(0 )}{(- 0)}{(0 )}{(0 )}IC-ID+ID-{(- 0)} {(- )} {(- 0)} {(- 0)} {(- 0)}{(0 )}{(- 0), {[0 0], {(- 0),[0 0],(0 )}[0 0]}(0 )}{(- 0), {(- 0)} {(- 0)} {(- 0)}(0 )}{(0 )}IC+ {(- )} {(- 0)} {(- 0), {(- 0)}(0 )}IC{(0 )} {(- 0), {(0 )} {(- 0),[0 0],(0 )}(0 )}ID+ {(0 )} {(- 0), {(0 )} {(- 0)}[0 0]}ID-IC+{(0 )}{(- 0),(0 )}{(0 )}{(- 0)}{(- 0)} {(- 0)} {(- 0)}{(0 )}{(0 ),[0 0]}{(0 )}{(- 0),[0 0]}{(0 )}{[0 0],(0 )}{(- 0),[0 0]}{(- 0)}{(0 )}Table 3: minimal metric point-based TCN IA-TCN Figure 11.a65fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSAllen (1983) remarks symbolic constraint (IA {f fi} IC) cannot hold given existingconstraints IA, IB, IC ID. labeled point-based TCN, (IA {f fi} IC) representedset constraints among ending points IA IC. Moreover, labels associatedlabeled elemental constraint allow us determine whether set elemental constraintsdifferent pairs time-points part global solution (Theorem 10). Thus, deducewhether (IA {f fi} IC) hold point-based TCN.existing constraints ending time-points IC IA, associated labelsets are:IC+ {(- ){R25 R30 R29 R17 R22 R21 R0){R27 R28 R29 R19 R20 R21 R0},(- 0){R27 R28 R29 R17 R22 R21 R9 R10 R15 R16 R1 R2 R7 R0 R8},(0 ){R25 R30 R29 R19 R20 R21 R11 R12 R13 R14 R3 R4 R5 R0 R6}} IA+IC-{(0 ){R27 R28 R29 R17 R22 R21 R9 R10 R15 R16 R1 R2 R7 R8 R0},[0 0] {R25 R30 R29 R17 R22 R21 R0}{R27 R28 R29 R19 R20 R21 R0},(- 0){R25 R30 R29 R19 R20 R21 R11 R12 R13 R14 R3 R4 R5 R6 R0}} IA-Let's ask disjunct (IA {f fi} IC):i) constraint (IA {f} IC) implies (IC+ {[0 0]} IA+) (IC- {(- 0)} IA-). AccordingTheorem 10, constraints hold iff set-union label sets associated (IC+ [0 0]IA+) (IC- (- 0) IA-) I-L-Set. two possibilities:i.1) {R25 R30 R29 R17 R22 R21 R0 } {R25 R30 R29 R19 R20 R21 R11 R12 R13 R14 R3 R4 R5 R6 R0 } ={R6 R5 R4 R3 R20 R19 R25 R30 R29 R17 R22 R21 R11 R12 R13 R14 R0 },i.2){R27 R28 R29 R19 R20 R21 R0 } {R25 R30 R29 R19 R20 R21 R11 R12 R13 R14 R3 R4 R5 R6 R0 } ={R14 R13 R12 R11 R30 R25 R27 R28 R29 R19 R20 R21 R3 R4 R5 R0 R6 }.However, label sets (i.1, i.2) I-L-Sets: instance, {R19 R22 } {R27 R30 } I-LSets (Table 2) subsets i.1 i.2, respectively. Thus, (IA {f} IC) hold.ii) constraint (IA {fi} IC) implies (IC+ {[0 0]} IA+) (IC- { (0 )} IA-). Similarly:ii.1) {R25 R30 R29 R17 R22 R21 R0 } {R27 R28 R29 R17 R22 R21 R9 R10 R15 R16 R1 R2 R7 R8 R0 } ={R16 R15 R10 R9 R28 R27 R25 R30 R29 R17 R22 R21 R1 R2 R7 R0 R8 }.label set I-L-Set. instance, {R30 R27 } I-L-Set. Also,ii.2) {R27 R28 R29 R19 R20 R21 R0 } {R27 R28 R29 R17 R22 R21 R9 R10 R15 R16 R1 R2 R7 R8 R0 } ={R8 R7 R2 R1 R22 R17 R27 R28 R29 R19 R20 R21 R9 R10 R15 R16 R0 }.label sets (ii.1, ii.2) also I-L-Sets. instance, {R 30 R27} {R19 R22 } I-LSets. Thus, (IA {fi} IC) hold either.conclusion, symbolic constraint (IA {f fi} IC) cannot hold globally labeled-consistentpoint-based TCN. conclusion could also obtained minimal IA-TCN (Figure 11.b).Additionally, (IA {f fi} IC) implies (IA+ [0 0] IC+). is, constraint (IA+ [0 0]IC+) holds, associated constraints label sets {R25 R30 R29 R17 R22 R21 R0 }{R27 R28 R29 R19 R20 R21 R0 } also hold. one label sets implies (IC - {[0 0]} IA-).is: (IA+ [0 0] IC+) (IC- {[0 0]} IA-). Thus, way (IA+ [0 0] IC+) hold (IA{e} IC) holds. relations detailed Section 6.66fiBARBER5.2 Metric Constraints IntervalsMetric constraints intervals also managed described temporal model.general point view, metric information added elemental interval-based constraintstandard way (Table 4). metric constraints interval boundaries (Table 4) similarones proposed Staab Hahn (1998).IA SymbolicElementalConstraintsIA Metric Elemental Constraintscij {[dm1 dM1 ], [dm2 dM2 ], ..... [dmn dMn ]}c'ij {[dm 1 dM1 ], [dm 2 dM2 ], ..... [dm n dMn ]}IiIi IjIi (before cij ) IjIi meets IjIi (meets c ij ) IjIi IjIi (cij c'ij ) IjIi starts IjIi (starts c ij ) IjIi finishes IjIi (finishes cij ) jCijIjIiCijIjIiC' ijCijIjIiCijIjIiCijIjIiIi overlaps IjIi (overlaps cij ) IjCijIjIiIi equal IjIi (cij equal c'ij ) IjCijC'ijIjTable 4: Metric interval constraints interval boundariesObviously, metric constraints Table 4 managed proposed model, meansmetric constraints interval ending points. Thus, symbolic constraints Interval Algebraextended way metric domain. However, since interval represented meansending time-points, flexible metric constraints intervals represented meansmetric constraints ending time-points. way, described model also subsumesInterval Distance Sub Algebra model proposed Badaloni Berati (1996). Moreover, endingpoints intervals also related initial time-point T0 , unary metric constraintsinterval durations expressed means metric constraints two ending pointsinterval:dur (Ii ) = {[dm1 dM1 ], [dm2 dM2 ], ..... [dmn dMn ]}(Ii - {[dm1 dM1 ], [dm2 dM2 ], ..... [dmn dMn ]} Ii +).67fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTS-[ I1 ]-[ I1 ]3060T0I1I1+[]20I2[50]{[140 150], [200 210]}Figure 12: Metric constraints intervalsThus, following constraints (Figure 12):(I1 {b, o} I2) (I1- [[20 30], [50 60]} I2-) (I2- {[140 150], [200 210]} T0 )represented (Table 1):default: (I1 - { (0 ){R0}} I1+), (I2- { (0 ){R0}} I2+),(I1 {b, o} I2)(I1+ {(0 ){Rb1} (- 0]{Rb2}} I2-),(I1+ {(- 0){Ro1} [0 ){Ro4}} I2-),(I1+ {(0 ){Ro2} (- 0]{Ro5}} I2+),(I1- {(0 ){Ro3} (- 0]{Ro6}} I2-),(I1- [[20 30], [50 60]} left I2-)(I2- {[140 150], [200 210]} T0 )(I1- {[50 60]{R1} [20 30]{R2}} I2-),(T0 {[140 150]{R3} [200 210]{R4}} I2- ),{Rb2 Ro4 }, {Rb2 Ro5 }, {Rb2 Ro6 }, {R1 R2 } {R3 R4 } I-L-Sets.6. Reasoning Logical Expressions Constraintsdescribed model, disjunct input constraint univocally associated label.Moreover, label set associated derived elemental constraint represents support-setinput elemental constraints elemental constraint derived. I-L-Sets representinconsistent sets input elemental constraints. reasoning labeled disjunctive constraints,associated label lists I-L-Sets, temporal model offers capability reasoning logicalexpressions elemental constraints belonging disjunctive constraints different pairstime points. Let's assume following labeled input constraints:(ni lc ij nj ) (ni {(lecij.1 ){Rij.1} (lecij.2 ) {Rij.2} .....(lecij.p ) {Rij.p}} nj ),(nk lc kl nl ) (nk {(leckl.1 ) {Rkl.1} (leckl.2 ) {Rkl.2} .....(leckll.q ) {Rkl.q}} nl )i) represent two elemental constraints 4 (elc ij.x lc ij , elckl.ylc kl ) cannot hold simultaneously(that (elc ij.x elc kl.y )) label set {Rij.x Rkl.y } added set I-L-Sets.ii) represent logical dependency two elemental constraints, 'If lec ij.xleckl.y' (where lecij.xcij , leckl.yckl ), Cartesian product {Rij.x} {{Rkl.1 , Rkl.2 , ....., Rkl.q }{Rkl.y }} added set I-L-Sets.iii) represent two elemental constraints (elc ij.xlcij , elckl.y lc kl ) hold simultaneously(bi-directional logical dependency), Cartesian products {Rij.x} {{Rkl.1 , Rkl.2 , ....., Rkl.q }4reasons simplicity, two elemental constraints shown. However, two disjunctions managedsimilar way. Likewise, features also applied labeled derived constraints.68fiBARBER{Rkl.y }} {Rkl.y } {{Rij.1 , Rij.2 , ....., Rij.p }-{Rij.x}} added set I-L-Sets.instance, lets see Example 2 Section 4.4 (Figure 10):represent John goes work car Fred goes work walking possible,{R1 R5 } asserted I-L-Set.represent John goes work car Fred goes work walking, {R1 R3 }{R1 R4 } asserted I-L-Sets.represent John goes work car Fred goes work walking, vice versa,{R1 R3 }, {R1 R4 } {R5 R2 } asserted I-L-Sets.similar way, logical relations among point-based interval-based elemental constraintsalso represented. instance, logical dependence "the duration I1 [5 8] I2 I3duration 1 [12 15] I2 I3" represented as:(I2 {b, bi} I3 )(I2 + {(0 ){Rb9} (- 0]{Rb10}} I3 -),{Rb10 Rb12 } I-L-Set,(I3 + {(0 ){Rb11} (- 0]{Rb12}} I2 -),(I1 - {[5 8] {R1} [12 15] {R2}} I1 +),{R1 Rb11 }, {R2 Rb9 } I-L-Sets, since Rb11 associated I2 I3 Rb9 associatedI2 I3 . Likewise, "I1 starts time 2 t1 occurs t2" represented(see Table 1):I1 {s, s} I2 (I1 - {[0 0] {Rs1} (0 ){Rs3} (- 0){Rs4}} I2 -) , (I1 + {(0 ){Rs2} (- 0]{Rs5}} I2 +) ,(t1 {(- -1] {R1}, [0 0] {R2}, [1 ){R3}} t2 ),{R3 Rs3 }, {R3 Rs4 }, {R3 Rs5 } I-L-Sets, since R3 associated 't1 occurs t2 ' Rs3 ,Rs4 Rs5 associated 'I1 start time I2 '.6.1 Disjunctions Point Interval-Based ConstraintsDisjunctions constraints different pairs points intervals representedproposed model means labeled constraints points set I-L-Sets. subsumesrelated expressiveness subset disjunctive linear constraints proposed StergiouKoubarakis (1998), disjunctions constraints different pairs pointsmanaged.represent disjunctive set disjunctive constraints points, have5 :(ni lc ij nj ) (nk lc kl nl ) represented as: (ni {lc ij lc ij } nj ) (nk {lc kl lc kl } nl ),logical relation among lc ij , lc ij , lc kl lc kl. Thus, disjunctive set constraints:{(ni lc ij nj ) (nk lc kl nl )}{(ni {(lecij.1 ){Rij.1}, (lecij.2){Rij.2}, ...., (lecij.p ){Rij.p}} nj )(nk {(leckl.1 ){Rkl.1}, (leckl.2 ){Rkl.2}, ...., (leckj.q ){Rkl.q}} nl )}5reasons simplicity, two constraints shown. However, two disjunctive constraints managedsimilar way.69fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSrepresented as:i) conjunctive set constraints (ni , nj ) (nk , nl ), where, (lecx)represented means complementary domain (lecx):(ni {(lecij.1 ){Rij.1}, (lecij.2 ){Rij.2}, ...., (lecij.p ){Rij.p}, {(lecij.1 ){Rij.1}, (lecij.2 ){Rij.2}, ...., (lecij.p ){Rij.p}}} nj )(nk {(leckl.1 ){Rkl.1}, (leckl.2 ){Rkl.2}, ..., (leckj.q ){Rkl.q}, {(leckl.1 ){Rkl.1}, (leckl.2 ){Rkl.2}, ..., (leckj.q ){Rkl.q}}} nl ){(ni {(lecij.1 ){Rij.1}, (lecij.2 ){Rij.2}, ..., (lecij.p ){Rij.p}, (lecij.1 ){R'ij.1}, (lecij.2 ){R'ij.2}, ..., (lecij.p){R'ij.p}} nj )(nk {(leckl.1){Rkl.1}, (leckl.2 ){Rkl.2}, .., (leckj.q ){Rkl.q}, (leckl.1 ){R'kl.1}, (leckl.2){R'kl.2}, ..., ( leckl.q ){R'kl.q} } nl )}ii) set I-L-Sets represent mutually exclusive disjunction lc ij lc kl (they cannotsimultaneously hold):ii.a) One constraints lc ij lc kl hold: Cartesian product label setscomplementary domains lc ij lc kl , {R'ij.1 , R'ij.2 , ...., R'ij.p }{R'kl.1 , R'kl.2 , ...., R'kl.q },I-L-Sets.ii.b) one constraints lc ij lc kl hold: Cartesian product label setslc ij lc kl , {Rij.1 , Rij.2 , ...., Rij.p }{Rkl.1 , Rkl.2 , ...., Rkl.q } I-L-Sets.Thus, disjunctive conjunctive sets disjunctive constraints points representedmanaged means conjunctive set disjunctive constraints set I-L-Sets.example:(ti {[5 5] {R1} [10 10] {R2}} tj ) (tk {[0 0] {R3} [20 20] {R4}} tl )(ti {[5 5] {R1} [10 10] {R2} (- 5){R5} (5 10) {R6} (10 ){R7}} tj )(tk {[0 0] {R3} [20 20] {R4} (- 0){R8} (0 20) {R9} (20 ){R10}} tl ),(ii.a) since (ti {[5 5] {R1}, [10 10] {R2}} tj ] [tk {[0 0] {R3}, [20 20] {R4}} tl ] hold:{R5 R6 R7 }{R8 R9 R10 } I-L-Sets,(ii.b) since one constraint (t {[5 5]{R1} [10 10] {R2}} tj ) (tk {[0 0] {R3} [20 20]{R4}} tl )hold:{R1 R2 }{R3 R4 } = {R1 R3 }, {R1 R4 }, {R2 R3 }, {R2 R4 } I-L-Sets.Ii ecij IjIi ecij IjIi ecij IjI1 I2I1 {(0 ){Rb1}} I2I3 I4I3 + {(0 ){Rb3}} I4 -+-Ii (ecij ecij) Ij-I1 {(0 ){Rb1} (- 0]{Rb2}} I2 -I3 + {(- 0]{Rb4}} I4 -I3 + {(0 ){Rb3} (- 0]{Rb4}} I4 -+I1 {(- 0]{Rb2}} I2+Table 5: Point-based constraints (I1 I2 ) (I3 4 )Similarly, disjunctions interval-based constraints different pairs intervals alsorepresented. instance, Table 1 Table 5, {(I1 I2 ) (I3 I4)}represented as:(I1 + {(0 ){Rb1} (- 0]{Rb2}} I2 -), (I3 + {(0 ){Rb3} (- 0]{Rb4}} I4 -),70fiBARBERa)one constraints (I1 I2) (I3 I4 ) hold. Thus, Cartesian productlabel sets associated disjunctive constraints (Ii ecij Ij ) set I-L-Sets: {Rb2 ,Rb4 } I-L-Set,b)one constraints (I1 I2 ) (I3 I4 ) hold. Thus, label setassociated mutual fulfillment constraints (Ii ecij Ij ) I-L-Set: {Rb1 , Rb3 }I-L-Set.Thus:{(I1 I2 ) (I3 I4 )}(I1 + {(0 ){Rb1} (- 0]{Rb2}} I2 -), (I3 + {(0 ){Rb3} (- 0]{Rb4}} I4 -),{Rb2 , Rb4 }, {Rb1 , Rb3 } I-L-Sets.Ii ecij IjIi ecij IjIi ecij IjIi (ecij ecij) Ij(I1 I2 )I1 - {(- 0){Rd1}} I2 -(I1 - {[0 ){Rd3}} I2 -)I1 - {(- 0){Rd1} [0 ){Rd3}} I2 -I1 + {(0 ){Rd2}} I2 +(I3 starts I4 )I3 - {[0 0]{Rs1}} I4 I3 + {(0 ){Rs2}} I4 +(I1 + {(- 0]{Rd4}} I2 +)I1 + {(0 ){Rd2} (- 0]{Rd4}} I2 +(I3 - {(0 ){Rs3} (- 0){Rs4}} I4 -) I3 - {[0 0]{Rs1} (0 ){Rs3} (- 0){Rs4}} I4(I3 + {(- 0]{Rs5}} I4 +)I3 + {(0 ){Rs2} (- 0]{Rs5}} I4 +Table 6: Point-based constraints (I1 I2 ) (I3 starts I4 )similar way (Table 6), (I1 I2 ) (I3 starts 4 )(I1 - {(- 0){Rd1} [0 ){Rd3}} I2 -),(I1 + {(0 ){Rd2} (- 0]{Rd4}} I2 +),(I3 - {[0 0] {Rs1} (0 ){Rs3} (- 0){Rs4}} I4 -), (I3 + {(0 ){Rs2} (- 0]{Rs5}} I4 +),{Rd1 Rd2 Rs1 Rs2 } Cartesian product {Rd3 Rd4 } X {Rs3 Rs4 Rs5 } I-L-Sets.Therefore, logical relations elemental constraints represented set I-L-Sets. Thus,labeled TCN (and set I-L-Sets) represent special type and/or TCN. typesnon-binary constraints enrich expressiveness language allow modelingcomplex problems (Meiri, 1996). Stergiou Koubarakis (1996) Jonsson Bckstrm (1998)show Disjunctions Linear Constraints (DLR) also able represent non-binaryconstraints. However, Pujari Sattar (1999) remark general methods linear programmingapplied DLR management, specific temporal concepts (like onesdetailed Section 2) considered general methods. proposed model,management non-binary constraints performed proposed reasoning methodswithout increasing computational complexity. added functionality interest severaltemporal reasoning problems, including planning, scheduling temporal constraint databases(Barber et al., 1994; Gerevini & Schubert, 1995; Brusoni et al., 1997; Stergiou & Koubarakis, 1998;etc.) general solutions provided specific temporal reasoning area.addition, proposed reasoning algorithms obtain globally labeled-consistent TCN(Theorem 11). feature allows us manage hypothetical queries, importantrequirement query processes temporal constraint databases (Brusoni et al., 1997). Thus, queries71fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSc'ij hold, c'kl? answered without TCN propagation. label set associatedderived elemental constraint represents set input elemental constraints holdfulfillment elemental constraint. Therefore,(xk c'kl xl)(xi c'ij xj )holds, elc kl.y lc kl / eckl.yc'kl elc ij.xlc ij / ec ij.xc'ij labels(elc ij.x)labels(elc kl.y ) hold.example, labeled minimal TCN Figure 7, have:(T1 {[40 40]} T3 ) (T2 { [0 0] } T4 ),(T3 { [20 20] } T2 ) (T3 { [20 20] } T4 ).However, (T3 {[10 20]} T2 ) imply (T1 {[70 70]} T4 ). Similarly, questionsc'ij hold, c'kl? also easily answered applying Theorem 9 Theorem 10.7. Alternative Temporal Contextsreason temporal facts, simultaneously work different alternative temporalcontexts, situations, trends, plans, intentions possible worlds (Dousson et al., 1993; Garcia &Laborie, 1996). usual branching (backward forward) model time. Here,alternative past contexts (i.e.: different lines facts may occurred) alternative futurecontexts (i.e.: different lines facts may occur). Thus, temporal context management alsorequired hypothetical causal reasoning. Also, different contexts permits partitionwhole TCN set independent chains order decrease complexity problem size(Gerevini & Schubert, 1995). section, deal hypothetical reasoning issues.goal temporal management context-dependent constraints. Thus, general, hierarchyalternative temporal contexts established, constraints associated differenttemporal contexts. instance, Figure 13 represents hierarchy alternative contexts, W0represents root context different disjunctive constraints (n1 , n2 )context. Temporal reasoning algorithms detailed paper able manage contextdependent constraints:Input disjunctive constraints asserted different temporal contexts. this, labelsassociated input elemental constraints also used represent contextdisjunctive asserted. instance (Figure 13), constraint:(n1 {[0 50] {R1}, [200 210] {R2}} n2 )asserted context W1 , following input context-dependent labeled constraint:(n1 {[0 25] {R1, W1}, [260 280] {R2, W1}} n2 ).Here, context-dependent label set associated elemental constraint representsalternative temporal disjunct (i.e.: R1 R2 ) context elementalconstraint asserted (W1 ).Label sets associated context-dependent derived elemental constraints representtemporal contexts derived elemental constraints hold.Definition 8. context-dependent disjunctive constraint disjunctive constraintelemental constraint (i.e.: disjunct) associated alternative temporal context. universallabeled constraint {(- ){W0 R0}}, W0 root context.72fiBARBERproposed reasoning processes manage context-dependent disjunctive constraints waysimilar previously defined labeled disjunctive constraints (Section 3). instance, accordingconstraints contexts Figure 13, following input labeled constraints nodes n1n2 updated:(n1 {[0 100] {R1 W0}, [200 300] {R2 W0}} n2 ),(n1 {[0 50] {R3 W1}, [200 210] {R4 W1}} n2 ),(n1 {[60 100] {R5 W 2} , [290 300] {R6 W2}} n2 ),(n1 {[0 25] {R7 W3}, [260 280] {R8 W3}} n2 ),(n1 { [0 25] {R0 W11}} n2 ),(n1 { [30 50] {R9 W12}, [200 205] {R10 W12}} n2 ),(n1 {[0 20] {R0 W31}, [210 215] {R0 W32}} n2 ),(n1 {[260 280] {R0 W33}} n2 ).restricted constraintsContext W11n1 {[0 25]} n 2Context W1n 1 {[0 50], [200 210]} n 2Context W12n 1 {[0 100], [200 300]} n 2Context W2n 1{[30 50], [200 205]} n 2n1 {[60 100], [290 300]} n 2Root-Context W0Context W31n 1{[0 20]} n 2Context W3n 1{[0 25], [260 280]} n 2Context W32n1{[210 215]} n2Context W33n1{[260 280]} n2Assertion Context kDownward Propagation:Propagation contextksuccessor contextsUpward Consistency:Consistency contextkpredecessor contextsFigure 13: hierarchy alternative contextsupdating process new constraint cij given context Wp assureconsistency c ij context Wp , well predecessor contexts (Figure 13). consistencycij successor contexts Wp detailed Section 7.2, since several optionsidentified. However, necessary assure consistency among constraints belonging contextsdifferent hierarchies. Successor contexts given context represent different alternatives,mutually exclusive. Thus, constraints belonging contexts different hierarchiesmutually inconsistent. However, imply constraints contextsnecessarily mutually disjoint. instance (Figure 13), constraints (n1 {[0 50] {R3 W1}, [200210]{R4 W1}} n2 ) context W1 (n1 {[0 25] {R7 W3}, [260 280] {R8 W3}} n2 ) context W3mutually disjoint. However, W1 , W2 W3 assumed three mutually exclusive alternativesW0 .73fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSclosure process new constraint cij context Wp downward propagate newconstraint cij successor contexts (Figure 13). Moreover, propagation performedpredecessor contexts contextk , among contexts different hierarchies. Elementalconstraints belonging contexts different hierarchies cannot simultaneously considered,is, combined intersected.7.1 Context-Dependent Updating Closure Processesupdate closure processes defined Section 4 adapted order manage contextdependent disjunctive constraints. Context-Update process (Figure 14) asserts constraintcij {ec1 , ec2, ..., ecn } context contextk. way similar updated process describedSection 4, Context-Update performed time new context-dependent constraintasserted.Context-Update (ni cij nj contextk)lc'ij Put-label-context (cij , contextk );Labelling mutual inconsistency.Consistency-Test (get-upward (ni , nj , contextk ), lc'ij );Upwards Consistency test(*Inconsistent Constraint*)Return (false)Else (*Consistent Constraint*);lc'ij asserted contextklc ij (lc ij - get (ni , nj , contextk )) lc (lc ij lc lc'ij ),;successor contexts.lc ji Inverselc (lc ij ),Context-Closure (ni lc ij nj contextk );Downwards Closure algorithm contextk..Return (true)End-IfEnd-Context-UpdateFigure 14: Context-Update process context-dependent labeled constraintsWhere:Put-label-context (cij , contextk ) associates exclusive label set elemental constraintecij.p cij . label set two labels {Rij.p contextk }. label set, first labellabel associated temporal disjunct. way similar Put-labels function, labelsmutually exclusive (Definition 3). second label represents context cijupdated. Moreover, pair labels associated successor contexts parent contextcontextk added I-L-Sets, since successor contexts given contextmutually exclusive:contextp / contextp Succesor-Contexts(Parent-Context(Contextk )),I-L-Sets I-L-Sets ({contextk }{contextp }).Parent-Context(contextk ) Successor-Contexts(contextk ) return parent-contextset successor-contexts contextk , respectively. Thus, Figure 13, {{W1 , W2 },{W1 , W3 }, {W2 , W3 }, {W11 , W12 }, {W31 , W32 }, {W31 , W33 }, {W32 , W33 }} I-L-Sets.74fiBARBERget (ni , nj , contextk ) returns set labeled elemental constraints ni njcontextk (and successor contexts). is:get (ni , nj , contextk )::= {(ecij.p {labelij.p })lc ij / contextk {labelij.p }}.Note get(ni , nj, contextk ) subset lc ij . Thus, (lc ij - get (ni , nj , context k )) means setdifference lc ij get (ni , nj , contextk ). is, set elemental constraintscontext-dependent constraint lc ij , contextk , successor contexts.get-upward (ni, nj , contextk ), similarly previous get function, returns existingconstraints ni nj contextk (and successor contexts). However,constraint ni nj contextk , function returnsconstraints ni nj exist predecesor context contextk:get-upward (ni, nj , contextk ) ::=get (ni , nj , contextk ) return (get (ni , nj , contextk ))ElseContextk Parent-Context (Contextk )get (ni , nj , contextk ) Contextk =W0get (ni , nj , contextk ) return (get (ni , nj , contextk ))Else return({(- +)}{W0 R0}})End-get-upwardcontext-dependent closure (Figure 15) process similar closure process describedSection 4 also performed updating process. closure process updatedconstraint contextk downwards performed contextk successor contexts.Context-Closure (ni lc ij nj contextk)(* First loop: Closure n n j n k *)nk TCN / lc jk {U{R0 W0}}:lc'ik lc ik lc (lc ij lc lc jk ),lc ik (lc ik - get (ni , nk , contextk )) lc lcij ,lc ki Inverse(lc ik)(* Second loop: Closure n j ni nl *)nl TCN / lc il {U{R0 W0}}:lc'jl lc jl lc (Inverse(lc ij ) lc lc il ),lc jl (lc jl - get (nj , nl , contextk )) lc lc'jl ,lc lj Inverse(lc jl )(* Third loop: Closure nl ni nj nk *)nl , nk TCN / lc lj {U{R0 W0}}, lc jk {U{R0 W0}}:lc'lk lc lk lc (lc li lc lc ij lc lc jk )lc lk (lc lk - get (nl , nk , contextk )) lc lc'lk ,lc kl Inverse(lc lk)End-Context-ClosureFigure 15: Context-Closure process context-dependent labeled constraints75fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSresulting label set associated context-dependent derived elemental constraint representscontexts elemental constraint holds, well hierarchy predecessor contextselemental constraint. instance, Figure 16 shows contextual labeling exampleFigure 13. Moreover, successively performing updating closure processesconstraints example, following constraint nodes n1 n2 :(n1 lc 12 n2 ): (n1 {[0 100] {R1 W0}, [200 300] {R2 W0}, [0 50] {R3 R1 W1 W0}, [200 210] {R4 R2 W1 W0},(e3)[60 100] {R5 R1 W2 W0}, [290 300] {R6 R2 W2 W0}, [0 25] {R7 R1 W3 W0}, [260 280] {R8 R2 W3 W0},[0 25] {R0 R3 R1 W11 W1 W0}, [30 50] {R9 R3 R1 W12 W1 W0}, [200 205] {R10 R2 R4 W12 W1 W0},[0 20] {R0 R7 R1 W31 W3 W0}, [210 215]{R0 R2 R8 W32 W3 W0}, [260 280]{R0 R2 R8 W33 W3 W0}} n2 ){W0}{W0 W1}{W0 W1 W11}{W0 W3}{W0 W2}{W0 W1 W12}{W0 W 3 W 31}{W0 W3 W32}{W0 W 3 W33}Figure 16: Labels contextsclosure process performed among constraints belonging contexts different hierarchies.According Put-label-context function, pair labels related successor contextscontext I-L-Set. Thus, I-L-Sets prevent deriving elemental constraints contextsdifferent hierarchies. is, derived elemental constraint obtained (combining intersecting)two elemental constraints contexts different hierarchy inconsistent associatedlabel set. Therefore, derived elemental constraints rejected operation lc.instance, example Figure 13, {{W1 , W2 }, {W1 , W3 }, {W2 , W3 }, {W11 , W12 }, {W31 , W32 },{W31 , W33 }, {W32 , W33 }} I-L-Sets. Thus, constraint asserted context W1 :i) propagation performed using constraints contexts W11 W12 simultaneously,since {W11 , W12 } I-L-Set.ii) propagation performed context W2 , W3 , successors, since {W1 ,W2 } {W1 W3 } I-L-Sets.Let's see example Context-Update Context-Closure processes. Lets assumecontext-dependent constraints Figure 13 already updated closured, previousconstraint lc 12 (expression e3) exists n1 n2 . Now, update (n1 {[20 40]} n2 ) contextW1 . call Consistency-Test function Context-Update function is:Consistency-Test (get-upward (n1 , n2 , W1 ), {[20 40] {R0 W1}}).Given previous constraint lc 12 n1 n2 (expression e3), function performs:{[0 50] {R3 R1 W1 W0}, [200 210] {R4 R2 W1 W0}, [0 25] {R0 R3 R1 W11 W1 W0},76fiBARBER[30 50] {R9 R3 R1 W12 W1 W0}, [200 205] {R10 R2 R4 W12 W1 W0}} lc {[20 40]{R0 W1}}={[20 40] {R3 R1 R0 W1 W0}, [20 25] {R0 R3 R1 W11 W1 W0}, [30 40] {R9 R3 R1 R0 W12 W1 W0}}Thus, new constraint (n1 {[20 40]} n2 ) consistent context W1 . Therefore, constraintn1 n2 results:lc12 (lc 12 - get (n 1 , n 2 , W 1 )) lc (lc 12 lc {[20 40]{R0 W1}}) ={[0 100]{R1 W0}, [200 300] {R2 W0}, [60 100]{R5 R1 W2 W0}, [290 300]{R6 R2 W2 W0},[0 25]{R7 R1 W3 W0}, [260 280]{R8 R2 W3 W0}, [0 20]{R0 R7 R1 W31 W3 W0},[210 215]{R0 R2 R8 W32 W3 W0}, [260 280]{R0 R2 R8 W33 W3 W0}} lc{[20 40]{R1 R0 W1 W0}, [20 40]{R3 R1 R0 W1 W0}, [20 25]{R0 R3 R1 W11 W1 W0}, [30 40]{R9 R3 R1 R0 W12 W1 W0}}={[0 100]{R1 W0}, [200 300] {R2 W0}, [60 100]{R5 R1 W2 W0}, [290 300]{R6 R2 W2 W0}, [0 25]{R7 R1 W3 W0},(e4)[260 280]{R8 R2 W3 W0}, [0 20]{R0 R7 R1 W31 W3 W0}, [210 215]{R0 R2 R8 W32 W3 W0}, [260 280]{R0 R2 R8 W33 W3 W0},[20 40]{R1 R0 W1 W0}, [20 25]{R0 R3 R1 W11 W1 W0}, [30 40]{R9 R3 R1 R0 W12 W1 W0}}.Note new updated constraint asserted context W1 propagated successorcontexts (W11 W12 ). However, new constraint context W1 affect existingconstraints predecessor contexts W1 (W0) constraints belonging contexts differenthierarchies (W2 , W3 successors).update process, closure process performed, since node related n1 n2 .Now, lets update (n3 {[10 20]} n1 ) context W1 . have:Consistency-Test (get-upward (n3 , n1 , W1 ), {[10 20] {R0 W1}}),performs:{(- +)}{W0 R0} lc {[20 40] {R0 W1}} = {[20 40] {R0 W0 W1}} ,since previous constraint exists (n3 n1 ) context W1 . constraint (n3 {[10 20]} n1 )consistent, asserted TCN:lc 31 {(- +)}{W0 R0}, [20 40] {R0 W0 W1}}.(e5)Afterwards, constraint closured. call Context-Closure process is:Context-Closure (n3 , {(- +)}{W0 R0}, [20 40] {R0 W0 W1}}, n1 , W1 ).closure process, first loop performed since node related n3 . Moreover,previous constraint lc 12 (expression e4) exists current TCN n1 n2 . Thus,first loop performs:lc'32 lc 32 lc ({(- +)}{W0 R0}, [20 40] {R0 W0 W1}} lc lc 12 ) ={(- ){W0 R0}} lc ({(- +)}{W0 R0}, [20 40] {R0 W0 W1}} lc lc 12 ) ={(- +)}{W0 R0}, [220 340] {R2 R0 W0 W1}, [40 80] {R1 R0 W1 W0},[40 65] {R0 R3 R1 W11 W1 W0}, [50 80] {R9 R3 R1 R0 W12 W1 W0}},that,lc 32 (lc 32 - get (n3 , n2 , W1 )) lc lc'32 = ({(- ){W0 R0}} - {}) lc lc'32 ={(- ){W0 R0}, [220 340] {R2 R0 W0 W1}, [40 80] {R1 R0 W1 W0},[40 65] {R0 R3 R1 W11 W1 W0}, [50 80] {R9 R3 R1 R0 W12 W1 W0}}.77(e6)fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSThus, asserted constraint (n3 , n2 ) context W1 closured context W1successor contexts (W11 W12 ). Likewise, closure process performpropagation simultaneously using constraints contexts W11 W 22 , context W2 ,W3 , successors.7.2 Complete Versus Incomplete Partition Contextsupdating process, consistency new constraint lcij given context assuredcontext parent contexts. Lets deal consistency issues contextsuccessor contexts. Here, constraints given context Wi either completelycovered partially covered existing constraints successor contexts Wi . is,successor contexts Wi either complete partition partial partition Wi .instance, let's assert constraint (n1 {[210 210] {R0 W1}} n2 ) context W1 exampleFigure 13. Consistency-test function, (where constraint lc 12 previousexpression e2):get-upward (n 1 , n 2 , W 1 ) lc {[210 210]{R0 W1}} ={[0 50]{R3 R1 W1 W0}, [200 210]{R4 R2 W1 W0}, [0 25]{R0 R3 R1 W11 W1 W0}, [30 50]{R9 R3 R1 W12 W1 W0},[200 205]{R10 R2 R4 W12 W1 W0}} lc {[210 210]{R0 W1}} = {[210 210]{R0 W1 R4 R2 W0}}.is, asserted constraint consistent existing constraints context W1 . However,resulting elemental constraint associated context W11 W12 . means assertedconstraint (n1 {[210 210] {R0 W1}} n2 ) consistent W1 , inconsistent W11 W12 . Here,two alternatives appear:i) assume existing successor contexts complete partition parent context.Therefore, new constraint cij context Wi rejected, cij inconsistentsuccessor contexts Wi. instance, assume W11 W12 Figure 13complete partition W1 . Thus, (n1 {[210 210] {R0 W1}} n2 ) rejected.ii) assume successor contexts complete partition parent context. Therefore,successor contexts become inconsistent removed. example,assume contexts W11 W12 complete partition context W1 ,another possible new successor context W 1 would able match future assertedconstraint (n1 {[210 210] {R0 W1}} n2 ). case, constraint (n1 {[210 210] {R0 W1}} n2 )assumed correct, asserted TCN. Therefore, contexts W11W12 become inconsistent. {W11 } {W12 } added set I-L-Sets,contexts (and successor contexts constraints) become inconsistentremoved TCN. is, elemental constraints associated label set containing{W11 } {W12 } removed.cases, context always consistent successor contexts. optionadopted depend problem type solve (Garrido et al., 1999). optionseasily introduced described reasoning processes, since function Consistency Testdetermine successor contexts (Ws ) become inconsistent new constraint (lcij )context (Wk ):78fiBARBERWs Successor-Contexts(Wk ) / elc ij.p get-upward (ni , nj , Wk ), Ws{labelij.p }elc ij.r(get-upward (ni , nj , Wk ) lc lcij ), Ws{labelij.r}.hand, when: (i) successor contexts (Wk1 , Wk2 , ..., Wkp ) context Wkcomplete partition it, (ii) constraints (Wk1 , Wk2 , ..., Wkp ) asserted,constraints Wk restricted according final existing constraints (W k1 , Wk2 , ..., Wkp ).this, context Wk constrained temporal union constraintssuccessor contexts.7.3 Minimal Consistent Context-Dependent TCNDefinition 9. context-dependent TCN minimal (and consistent) constraints contextconsistent (with respect constraints context, predecessor contexts,successor contexts) minimal (with respect constraints context predecessorcontexts).Theorem 12. updating process, context-dependent reasoning processes obtain minimal(and consistent) context-dependent TCN previous context-dependent TCN minimal.Proof: previous context-dependent TCN minimal, Consistency-Test function guaranteesconsistency new context-dependent input constraint:i)context parent contexts (get-upward function Theorem 5),ii)successor contexts (depending two identified cases Section 7.2).closure process new constraint given context (Wk ) propagates effectscontext successor contexts. Therefore (Theorem 7), process obtains new minimalconstraints context (Wk ) successor contexts.Moreover, obtained context-dependent TCN globally labeled-consistent. Thus,deduce whether set elemental constraints (between different pairs time points) consistent(Theorem 10). is, set elemental constraints holds context. instance, givenprevious constraints lc 12 , lc 31 lc 32 (previous expressions e4, e5 e6), deduce that:(n1 {[40 40]} n2 ) (n3 {[40 40]} n1 ) (n3 {[40 40]} n2 )full consistent since:elc 12.xlc 12 , elc 31.y lc 31 , elc 32.zlc 32 / ({label12.x} {label12.x} {label12.x}) I-L-Set.Specifically, instantiations hold {R1 R0 W1 W0 } {R1 R0 W0 }. Thus, setelemental constraints holds context W1 (and, obviously, predecessor contexts).Likewise, minimal context-dependent TCN, user retrieve constraints holdcontext constraints simultaneously hold set given contexts. this,Context-Constraints function retrieves constraints hold pair nodes (ni , nj )given context (contextk ). is, result Get-upwards(ni , nj , contextk ) except elementalconstraints belonging successor contexts contextk :79fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSContext-Constraints (ni, nj, contextk )::= Get-upwards (ni , nj , contextk ){lecij.p lc ij / contextqSuccesor-Contexts(contextk ), {contextq }{labelij.p }}.instance, given context-dependent constraint lc 12 Figure 13 (expression e3),following constraint would hold (n1 , n2 ) contexts W1 W3 :Context-Constraints(n 1 , n 2 , W1 ) lc Context-Constraint(n 1 , n 2 , W 3 ) ={[0 50]{R3 R1 W1 W0}, [200 210]{R4 R2 W1 W0}} lc {[0 25]{R7 R1 W3 W0}, [260 280]{R8 R2 W3 W0}}={[0 25]{R7 R3 R1 W3 W1 W0}}6 .addition, obtain constraints, simultaneously hold contextsuccessor ones. instance, context W1 successor contexts (W11 , W12),following constraint holds:Context-Constrains(n1 , n2 , W1) lc [Context-Constraints(n1, n 2 , W11 ) lc Context-Constraints(n 1 , n2 , W12 )]={[0 50]{R3 R1 W1 W0}, [200 210]{R4 R2 W1 W0}} lc{[0 25]{R0 R3 R1 W11 W1 W0}}lc {[30 50] {R9 R3 R1 W12 W1 W0}, [200 205]{R10 R2 R4 W12 W1 W0}}={[200 205]{W12 R10 R4 R2 W1 W0}, [0 25]{W11 R0 R3 R1 W1 W0}, [30 50]{W12 R9 R3 R1 W1 W0}}.hand, alternative context (Wi ) associated alternative hypothesis(Hi ). hypothesis Hi gives rise set constraints, asserted associatedcontext Wi . Thus, proposed reasoning processes assure minimal constraints hierarchyhypotheses. Moreover, hypothesis (Hi ) becomes unavailable, label set {Wi }added set I-L-Sets. Thus, constraints context Wi (and successor contexts)removed. is, constraints depend unavailable hypothesis Hi removed.7.4 Computational Complexity Temporal Context Managementmanagement temporal context increase complexity reasoning processesdetailed Section 4. fact, consider label associated disjunct (Ri ) labeleddisjunctive constraints also associated context (Wi ). Thus, computational costupdating process also bounded O(n2 l2e), 'l' maximum number input disjunctspair nodes contexts.temporal labeled algebra proposed paper (Section 3) applied pointbased disjunctive metric constraints (Dechter, Meiri & Pearl, 1991). However, labeled algebraalso applied temporal constraints. case, operations lc, lc, lc lcspecified (Section 3) basis operations , , underlyingalgebra. way, management temporal contexts also applied typesconstraints.Theorem 13. computational complexity proposed reasoning process applied contextdependent non-disjunctive metric constraints polynomial (O(n2 W2)) number W managedcontexts.6However, note impossible situation, since W 1 W 3 mutually exclusive contexts. is, {W 3, W 1}I-L-Set.80fiBARBERProof: Disjunctions constraints related contexts input constraintsasserted, non-disjunctive constraints managed. is, constraints pair nodesform:(ni {(ec ij.0 {W0 R0 }), (ec ij.1 {W1 R0 }), ...... , (ecij.k {Wk R0 })} nj ) ,0kW / W=|{Wi }|Thus, maximum number disjuncts constraints bounded maximum numbermanaged contexts W. Moreover, maximum length associated label sets maximum depthhierarchy contexts, set I-L-Sets 2-length sets (i.e.: pairs labelsassociated pair successor contexts context). Therefore, computational costoperations lc lc bounded O(W2 ).methods proposed Section 7.1 management temporal contexts also appliedtemporal reasoning algorithms, instead reasoning methods detailed Section 4.requires reasoning algorithms based operations compositionintersection temporal constraints. Thus,i) elemental constraint associated context (W ) asserted7 .Thus, label sets associated elemental constraints one contextual label {Wi }.ii) methods management temporal contexts described Section 7.1integrated new reasoning algorithms. algorithms use operationslc, lc, get get-upwards. computational cost operations lc lc relatedmanagement temporal contexts polynomial (O(W2 )) number (W) managedcontexts. Therefore, computational cost reasoning algorithms increased factorW2 temporal contexts managed.instance, interval-based constraints managed, TCA algorithm usedobtain path-consistent context-dependent IA-TCN, O(n3 W2 ) cost. Similarly, contextdependent reasoning applied PIDN networks (Pujari & Sattar, 1999), computational costspecific reasoning algorithms PIDN constraints increased factor W2 . proposedtemporal algebra Section 3 applied tractable classes constraints, specific reasoningalgorithms management classes constraints also applied. computationalcost reasoning algorithms (which based combination intersectionoperations constraints) increased polynomial factor W2 . instance, nondisjunctive metric constraints managed, TCA algorithm used closure algorithmSection 7.1. algorithm obtain minimal context-dependent TCN computationalcost O(n 3 W2 ).8. ConclusionsSeveral problems remain pending representation reasoning problems temporal constraints.relation this, dealt reasoning complex qualitative quantitative constraintstime-points intervals, organized hierarchy alternative temporal7is, labels (Ri) associated disjunctions disjunctive constraints. Thus, Definition 3 appliedPut-Label-Context function. Therefore, distributive property lc lc hold disjunctiveconstraints. However, relevant since reasoning processes applied.81fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTScontexts. described new-labeled temporal algebra, whose main elements labeleddisjunctive metric constraints, label sets associated elemental constraints, sets inconsistentelemental constraints (I-L-Sets). temporal model presented able integrate qualitativemetric constraints time-points intervals. fact, symbolic metric constraintintervals represented means disjunctive metric constraints time points setI-L-Sets. model also able manage (non-binary) logical relations among elementalconstraints. reasoning algorithms described model based distributive propertycomposition intersection labeled constraints, guarantee consistency obtainminimal TCN disjunctive metric point-based constraints. addition, special type globallabeled-consistent TCN also obtained.Labeled constraints organized hierarchy alternative temporal contexts,temporal reasoning processes performed contexts. Reasoning algorithms guaranteeconsistency hierarchy contexts, maintain minimal context-dependent TCN, allow usdetermine constraints hold context set alternative contexts. Thus,reason hierarchy context-dependent constraints intervals, points unary durations(Figure 17).described features useful functionalities modeling important problemstemporal reasoning area. However, identified previous models. Therefore,temporal model presented represents flexible framework reasoning complex, contextdependent, metric qualitative constraints time-points, intervals unary durations.Dur(I 1) { [ 2 0 20], [50 60]}I1 {b} 21 {[10 20], [100 130]} -2Context W 1 1-1 {b m} 2Dur(I 1 ) {[20 30], [50 100]}1 {[100 100], [200 300]} I+ 2Dur(I 1) {[20 30], [60 100]}Context W 1Context W 1 21 {[10 20], [100 200]} - 2I- 1 {[0 100], [200 300]} + 2Root-Context W 01 {[10 15], [120 200]}1{ } 2Context W 221{d} 21 {[10 10]} -2Context W 2 1Dur(I 1 ) = 501 { } I2Context W 2 2Figure 17: Context-dependent constraints intervals, time points unary durationspath-consistent algorithm used closure process labeled TCNs, like typicalTCA algorithm applied Allen (1983). path-consistent algorithm would obtain minimalcontext-dependent TCN disjunctive metric constraints. proposed incrementalreasoning process. Thus, minimal (and consistent) context-dependent TCN assured newassertion. incremental reasoning allows us detect whether new input constraintinconsistent previously existing ones. useful problem constraints82fiBARBERinitially known successively deduced incremental independent process (Garrido etal., 1999).prototype proposed reasoning algorithms implemented Common-Lispavailable author. reasoning algorithms applied integrated architectureplanning scheduling processes (Garrido et al., 1999). Here, scheduling processguarantee consistency alternative partial plan (i.e.: temporal constraints availabilityresources operations) simultaneously planner generating partial plan (Srivastava& Kambhampati, 1999). Thus, following main features needed:Management disjunctive metric constraints. Particularly, planning schedulingproblems number disjuncts input constraints generally bounded l2 (i.e.: nonsimultaneous use resources). However, temporal dependencies constraints (i.e.:non-binary constraints) appear. instance, operation durations dependentorder scheduled.Incremental reasoning. process interactively guarantee consistency newinput temporal constraint (about resources, plans, ordering, objects) new stepdeduced partial plan.Management temporal contexts, context associated alternative plan(action state). Reasoning algorithms simultaneously work different alternativepartial plans.globally labeled-consistent (and minimal) TCN allows us determine consistent alternativechoices obtain optimal solutions plan. Additionally, proposed modeluseful framework apply problems features also appear (Dousson et al., 1993;Garcia & Laborie, 1996; Srivastava & Kambhampati, 1999; etc.).computational cost reasoning algorithms exponential, due inherent complexitymanagement disjunctive constraints. However, management temporal contextsincrease complexity reasoning processes disjunctive constraints.improvements decrease empirical cost reasoning algorithms proposedpaper. application algorithms handle explicit TCN (without makingderived constraints explicit) empirical evaluations several test cases study.Moreover, reasoning algorithms applied temporal algebra presented, proposedSection 4. hand, interesting identify subclasses labeled temporal algebrasize label sets bounded, identify tractable subclasses IA proposedmodel. could also interesting identify expressive power I-L-Sets (and labeledconstraints) basis method described Jeavons, Cohen Cooper (1999). Here, I-LSet represents special derived constraint, expresses inconsistency set inputelemental constraints; is, special type disjunctive linear constraint (Jonsson & Bckstrm,1996; Stergiou & Koubarakis, 1996).proposed-labeled algebra (labeled constraints operations them) appliedtemporal models (i.e.: classes temporal constraints, operations, reasoningalgorithms). this, operations labeled algebra (lc, lc, lc lc) definedbasis respective operations (, , ) models, reasoningalgorithms use operations defined labeled constraints ( lc, lc, lc lc).83fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSrequires reasoning algorithms based composition intersection operations.Specifically, application proposed model tractable temporal constraints -asidentified Section 1 (Jonsson et al., 1999; Drakengren & Jonsson, 1997; Vilain, Kautz VanBeek, 1986; etc.)- allows tractable reasoning process hierarchy temporal constraintcontexts.Acknowledgementswork partially supported Generalitat Valenciana (Research Project #GV-1112/93)Spanish Government (Research Project #CYCIT-TAP-98-0345). author wouldsincerely like thank JAIR reviewers helpful comments suggestions previousversions paper.ReferencesAllen, J. (1983). Maintaining knowledge temporal intervals. Comm ACM, 26, 11, 832843.Badaloni, S., & Berati, M. (1996). Hybrid Temporal Reasoning Planning Scheduling.Proceedings 3 Int. Workshop Temporal Representation Reasoning (TIME96).Barber, F. (1993). metric time-point duration-based temporal model. ACM Sigart Bulletin,4 (3), 30-40.Barber, F., Botti, V., Onaindia, E., & Crespo, A. (1994). Temporal reasoning Reakt:environment real-time knowledge-based systems. AICommunications, 7 (3), 175-202.Brusoni, V., Console, L., & Terenziani, P. (1997). Later: Managing temporal information efficiently,IEEE Expert, 12 (4), 56-64.Cohen, D., Jeavons, P., & Koubarakis, M. (1996). Tractable disjunctive constraints. Proceedings.3rd Int. Conf. Principles Practice Constraint Programming (CP96). Freuder,E.C. (Ed.). Lecture Notes Computer Science, 1118, 297-307.Cooper, M.C. (1990). optimal k-consistency algorithm. Artificial Intelligence, 41, 89-95.Dean, T.L., & McDermott, D.V. (1987). Temporal data base management. Artificial Intelligence, 38,1-55.Dechter. R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49,61-95.Dechter, R. (1992). local global consistency. Artificial Intelligence, 55, 87-107.Dousson, C., Gaborit, P., & Ghallab M. (1993). Situation Recognition: RepresentationAlgorithms. Proceedings 13th International Joint Conference Artificial Intelligence(IJCAI93).Drakengren, T., & Jonsson, P. (1997). Eight maximal tractable subclasses Allen's algebrametric time. Journal A.I. Research, 7, 25-45.84fiBARBERFreuder, E. C. (1978). Synthesizing constraint expressions. Comm. ACM, 21 (11), 958-965.Freuder, E. C. (1982). sufficient condition backtrack-free search. Journal ACM, 29 (1),24-32.Garcia, F., & Laborie, P. (1996). Hierarchisation Seach Space Temporal Planning. NewDirections AI Planning, 217-232, IOS Press.Garrido, A., Marzal, E., Sebasti, L., & Barber F. (1999). model planning schedulingintegration. Proceedings 8 th. Conference Spanish Association A.I.(CAEPIA99).Gerevini, A., & Schubert, L. (1995). Efficient algorithms qualitative reasoning time.Artificial Intelligence, 74, 207-248.Jeavons, P., Cohen, D., & Cooper M. (1998). Constraints, consistency closure. ArtificialIntelligence, 101, 251-268.Jeavons, P., Cohen, D., Gyssens, M. (1999). determine expressive power constraints.Constraints: Int. Journal, 4, 113-131.Jonsson, P., & Bckstrm, C. (1996). linear-programming approach temporal reasoning.Proceedings 13 th. National Conference Artificial Intelligence (AAAI96).AAAIPress.Jonsson, P., & Bckstrm, C. (1998). unifying approach temporal constraint reasoning. ArtificialIntelligence, 102, 143-155.Jonsson, P., Drakengren, T., & Bckstrm, C. (1999). Computational complexity relating timepoints intervals. Artificial Intelligence, 109, 273-295.Kautz, H., & Ladkin, P. (1991). Integrating metric qualitative temporal reasoning. Proceedings9th. National Conference Artificial Intelligence (AAAI91).AAAI Press.Mackworth, A. K. (1977). Consistency networks relations, Artificial Intelligence, 8, 121-118,.Meiri, I. (1996). Combining qualitative quantitative constraints temporal reasoning. ArtificialIntelligence, 87, 343-385.Montanari, U. (1974). Networks constraints: fundamental properties applications pictureprocessing. Information Science, 7, 95-132.Navarrete, I., & Marin, R. (1997). Qualitative temporal reasoning points durations.Proceedings 15 th. International Joint Conference Artificial Intelligence (IJCAI-97).Nebel, B., & Burckert, H.J. (1995). Reasoning temporal relations: maximal tractable subclassAllen's interval algebra. Journal ACM, 42 (1), 43-66.Pujari, A., & Sattar, A. (1999). new framework reasoning Points,. IntervalsDurations. Proceedings Int. Joint Conference Artificial Intelligence (IJCAI'99).Schwalb, E., & Dechter, R. (1997). Processing disjunctions temporal constraints networks.Artificial Intelligence, 93, 29-61.85fiREASONING INTERVAL POINT -BASED DISJUNCTIVE ETRIC CONSTRAINTS TEMPORAL CONTEXTSStaab, S., & Hahn, U. (1998). Distance constraint arrays: model reasoning intervalsqualitative quantitative distances. Proceedings 12th Biennial ConferenceCanadian Society Computational Studies Intelligence Advances ArtificialIntelligence (AI-98), Lecture Notes Artificial Intelligence, 1418, 334-348.Srivastava, B., & Kambhampati, S. (1999). Efficient planning separate resource scheduling.Proceedings AAAI Spring Symp. search strategy uncertainty incompleteinformation. AAAI Press.Stergiou, K., & Koubarakis, M. (1996). Tractable disjunctions Linear Constraints. Proceedings2nd Int. Conf. Principles Practice Constraints Programming (CP96).Freuder, E.C. (Ed.). Lecture Notes Computer Science, 1118, 297-307.Stergiou, K., & Koubarakis, M. (1998). Bactracking algorithms disjunctions temporalconstraints. Proceedings 15 th. National Conference Artificial Intelligence (AAAI98). AAAI Press.Van Beek, P. (1991).Temporal query processing indefinite information. Artificial IntelligenceMedicine, 3 (6), 325-339.Van Beek, P., & Detcher R. (1995). minimality global consistency row convexnetworks. Journal ACM, 42 (3), 543-561.Van Beek, P., & Dechter, R. (1997). Constraint tightness looseness versus local globalconsistency. Journal ACM, 44 (4), 549-566.Vilain, M., Kautz, H., & Van Beek P. (1986). Constraint propagation algorithm temporalreasoning. Proceedings 5Th. National Conference Artificial Intelligence (AAAI86).AAAI Press.Wetprasit, R., Sattar A. (1998). Temporal representation qualitative quantitative informationpoints durations. Proceedings 15 th. National Conference ArtificialIntelligence (AAAI98). AAAI Press.Yampratoom, E., & Allen, J. (1993). Performance temporal reasoning systems, ACM SigartBulletin, 4, (3), 26-29.86fi