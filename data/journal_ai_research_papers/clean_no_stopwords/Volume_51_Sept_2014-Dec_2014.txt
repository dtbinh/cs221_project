Journal Artificial Intelligence Research 51 (2014) 533-554

Submitted 02/14; published 10/14

Optimal Scheduling Contract Algorithms
Anytime Problem-Solving
Alejandro Lopez-Ortiz

alopez-o@uwaterloo.ca

Cheriton School Computer Science
University Waterloo
Waterloo, Ontario, Canada, N2L 3G1

Spyros Angelopoulos

spyros.angelopoulos@upmc.fr

CNRS Laboratoire dInformatique
Universite Pierre et Marie Curie
4 Place Jussieu 75252, France

Angele M. Hamel

ahamel@wlu.ca

Department Physics Computer Science
Wilfrid Laurier University
Waterloo, Ontario, Canada, N2L 3C5

Abstract
contract algorithm algorithm given, part input, specified
amount allowable computation time. algorithm must complete execution
within allotted time. interruptible algorithm, contrast, interrupted
arbitrary point time, point must report currently best solution.
known contract algorithms simulate interruptible algorithms using iterative
deepening techniques. simulation done penalty performance
solution, measured so-called acceleration ratio.
paper give matching (i.e., optimal) upper lower bounds acceleration ratio simulation. assume general setting n problem
instances must solved means scheduling executions contract algorithms
identical parallel processors. resolves open conjecture Bernstein, Finkelstein,
Zilberstein gave optimal schedule restricted setting round robin
length-increasing schedules, whose optimality general unrestricted case remained
open.
Lastly, show evaluate average acceleration ratio class exponential
strategies setting n problem instances parallel processors. broad
class schedules tend either optimal near-optimal, several variants
basic problem.

1. Introduction
Anytime algorithms algorithms whose quality output improves gradually
amount available computation time increases. class algorithms introduced
first Dean Boddy (1988) context time-depending planning, well
Horvitz (1987, 1998) context flexible computation. Anytime algorithms occur
c
2014
AI Access Foundation. rights reserved.

fiLopez-Ortiz, Angelopoulos, & Hamel

naturally settings computationally intensive problem addressed uncertainty respect available computation time. example problem
answer must provided determined external input
control. instance, consider automated trading program stock market.
programs run time-intensive simulations price various financial instruments.
change bid price given stock occurs algorithm must produce decision
(buy/sell/hold) instant, using whatever information garnished
course simulations, take advantage newly posted price. Another example given real-time applications. instance, consider motion planning algorithm
robot solution must produced within certain, varying, amount
time: example robot collide move needed momentarily even
algorithm produce suboptimal move, others, sufficient time
carefully compute next step. situation, amount allotted time given
algorithm beforehand.
According Russell Zilberstein (1991), distinction made
two different types anytime algorithms. one hand, interruptible algorithms
algorithms whose allowable running time known advance, thus interrupted (queried) given point throughout execution. algorithms typically
include versions local search, e.g., simulated annealing hill climbing.
hand, stringent class contract algorithms consists algorithms given
amount allowable computation time (i.e, intended query time) part
input. However, algorithm interrupted point contract time expires, algorithm may return meaningful results. algorithms thus less
flexible interruptible algorithms, however tend simpler construct,
easier prove strict guarantees performance. typical example contract
algorithms polynomial-time approximation schemes (PTAS) based Dynamic Programming (DP): bigger amount computation time, better approximation
achieved. However, algorithm may require available computation time order fill important entries DP table, otherwise solution returned may
useless.
Consider following general scenario: given set P n different problem
instances, want design efficient interruptible algorithm applied,
concurrent fashion, n problem instances. amount computation time
known advance; instead, expected unknown point time,
interruption occur, point algorithm stopped queried report
(partial) solutions one among problem instances. Clearly, algorithm
must make judicious use resources ensure produce reasonably good
solution, despite knowledge exact time interruptions may occur.
indicated earlier, hardly surprising setting arises frequently
design AI systems, applications game-playing programs (Althofer, 1997; Kao,
Reif, & Tate, 1996; Kao, Ma, Sipser, & Yin, 1998), e-trading agents, medical diagnosis
systems. Essentially problem captures fundamental trade-off quality
solution returned algorithm amount available computation time.
refer reader survey Zilberstein (1996) importance anytime algorithms
design intelligent systems.
534

fiOptimal Scheduling Contract Algorithms Anytime Problem-Solving

PTAS (Polynomial Time Approximation Schemes) provide rich source contract
algorithms guaranteed performance. motivates study general constructions
creating interruptible versions given contract algorithm. provides
flexibility focusing design contract algorithms, converting
interruptible algorithms applying standard, black-box transformation. Indeed,
standard method simulating interruptible algorithm repeatedly executing
contract algorithm using increasing execution times. Consider general setting
set processors identical speed available execution simulation,
problem instance corresponding (single-thread) contract algorithm.
problem face schedule executions various contract algorithms
processors way guarantees efficient interruptible algorithm. setting,
query time, algorithm report problem instance solution
corresponding contract longest length (i.e., contract time) completed
query time.
becomes obvious formal measure quality simulation required.
one may think several possible ways evaluating quality simulation,
work adopt standard measure area, namely acceleration ratio (Russell
& Zilberstein, 1991). formal definition provided Section 2; informally, acceleration ratio indicates much faster processors order guarantee
solution good one ideal algorithm foreknowledge
query time also problem instance p interest. ideal algorithm would
simply utilize single processor run solely contract instance p, time t.
sense, acceleration ratio reflects loss optimality due lack future knowledge
query times problem instance question, motivated similar
considerations competitive ratio context analysis online algorithms.
case one problem instance single processor, Russell Zilberstein (1991)
showed iterative doubling contract lengths gives rise interruptible algorithm
acceleration ratio four. Zilberstein, Charpillet, Chassaing (2003) showed
optimal acceleration ratio, sense scheduling strategy defined
set contracts acceleration ratio least four.
Zilberstein et al. (2003) also studied generalization problem multiple
problem instances considered (assuming |M | = 1), Bernstein, Perkins, Zilberstein,
Finkelstein (2002) studied generalization contracts single problem
instance must scheduled set multiple processors. versions, algorithms
optimal acceleration ratios derived. problem, full generality, involves set
processors set problems, cardinality greater one. Bernstein et al.
n m+n m+n
(2003) showed upper bound
( n ) acceleration ratio; addition, using
elegant techniques, showed bound optimal restricted, though natural
intuitive, class schedules use round robin length-increasing strategy.
strategies known cyclic strategies. Bernstein et al. leave open question whether
bound tight among possible schedules. paper answer question
affirmative.
general, observed theoretical analysis geometric searches
robot motion planning closely linked scheduling heuristics algorithms
problem solving. connection first established Kao et al. (1996, 1998)
535

fiLopez-Ortiz, Angelopoulos, & Hamel

context randomized algorithms. work Bernstein et al. (2003) drew similar connection scheduling contract algorithms robot searching set rays (Alpern
& Gal, 2003). latter problem, p robots search target located one
concurrent rays. seek search strategy minimizes competitive ratio, namely
worst-case ratio search cost distance starting position
target.
turns interesting parallels drawn two problems: informally,
rays correspond problem instances, robots processors, (unknown) location
target corresponds (also unknown) query time. general problem
p robots rays Lopez-Ortiz Schuierer (2004) showed optimal strategy


p
achieves competitive ratio 1 + 2 mp
p ( mp ) . Bernstein et al. (2003) independently
derived bound directly translating approach context contract
scheduling solution robot searching parallel rays. noted
upper bounds work Lopez-Ortiz Schuierer Bernstein et al. based
exponential strategies. Informally, cyclic strategies lengths
rays searched (respectively, lengths contracts schedule) form geometric
sequence (see Section 2 precise definition).
intuitive level, problem scheduling contract algorithms multiple processors involved problem multi-robot searching concurrent rays.
difficulty stems fact multiple searchers ray benefit,
whereas multiple contract algorithms (of different lengths) problem could
well improve performance. However, paper show ideas behind
solution Lopez-Ortiz Schuierer (2004) adapted, non-trivial manner,
show optimality schedule Bernstein et al. (2003) without restrictions
(such cyclicality) scheduling strategy.
problem removing assumption cyclicality long history searchrelated problems. instance, Jaillet Stafford (2001) argue rigorously cyclicality
waived single-searcher problems rays (whereas original work BaezaYates, Culberson, & Rawlins, 1993, cyclicality implicit). Along lines, LopezOrtiz Schuierer (2004) remove cyclicality assumptions context multi-searcher
problems. Similar conclusions harder establish concerning randomized algorithms
(for instance, see Schuierer, 2003). sense, cyclic strategies simple analyze
provide relatively easy upper bounds; however, difficulty lies establishing optimality
within space possible strategies.
remainder paper organized follows. Section 2 present formal
discussion problem setting. main result, namely Theorem 3, presented Section 3 show optimality exponential schedule Bernstein et al. (2003)
without restrictions. Section 4 present average-case analysis acceleration ratio exponential strategies multi-processor setting, assuming uniform
distribution interruption times.

2. Preliminaries
Let P denote set n problem instances simply problems. contract c pair
(p, d), p P denotes problem instance c assigned (also called
536

fiOptimal Scheduling Contract Algorithms Anytime Problem-Solving

problem tag c) duration length contract, specifies processing
time required complete c. denote set identical processors. Let C
denote (potentially infinite) set contracts. Define schedule X set contracts C
feasible assignment contracts C set processors. particular,
X described set {(ci , mi , si ) : ci C}, mi {0, . . . , 1} denotes
processor ci scheduled, si denotes time processing begins.
schedule X must feasible, sense two contracts ci = (pi , di ), cj = (pj , dj )
C assigned processor X, cj scheduled immediately
ci , si + di sj . words, cj start ci
completed. Note assume non-preemptive schedules, sense cannot
interrupt later resume contract.
Observation 1 Without loss generality consider schedules processors never idle, i.e., start time contract always finish time another
contract.
n problem instances identical processors, Bernstein et al. (2003) define
class cyclic schedules schedules following natural properties:
1. Property 1 (Problem Round Robin) ci = (pi , di ) ith contract cyclic
schedule order, problem instance pi pi = mod n.
2. Property 2 (Length Increasing) ci = (pi , di ) cj = (pj , dj ) pi = pj
< j, di < dj .
3. Property 3 (Processor Round Robin) mi = mod i.
exponential schedule cyclic schedule lengths contracts roundrobin order increase exponentially. formally, i-th contract order length
bi fixed number b.
use acceleration ratio standard measure evaluating quality
schedule. Following Bernstein et al. (2003), assume contract completed
time solution available interruption occurs time after, including
time t. also limit interruptions occur least one contract
problem P completed, otherwise problem vacuous (this canonical
assumption). Denote lX (p, t) length longest contract problem p
completed time X (if source confusion omit subscript.)
Definition 1 Given set P n problem instances set processors identical
speed, acceleration ratio schedule X P , denoted Rm,n (X) defined
smallest value r, r 1 allowable interruption time t,
problem p P , lX (p, t) t/r. acceleration ratio P set
processors identical speed defined

Rm,n
= inf Rm,n (X).
X

.
schedule X optimal Rm,n (X) = Rm,n

537

fiLopez-Ortiz, Angelopoulos, & Hamel

argue given schedule X, acceleration ratio Rm,n (X) determined
looking discrete subset timeline, instead possible interruption times t.
Let denote infinitesimally small positive value, let F denote set finish times
contracts X. easy see suffices consider interruptions
occur times , F . see this, consider certain interruption
conform rule, let t0 earliest time contract
finishes X t0 > t. problems p, l(p, t0 ) = l(p, t); words
algorithm made progress problem time interval [t, t0 ], thus
t/l(p, t) < (t0 )/l(p, t0 ).
following essentially alternative definition acceleration ratio, based
observation.
Observation 2 (Bernstein et al., 2003) Let F denote set finish times contracts schedule X. acceleration ratio schedule X P






Rm,n (X) = sup
= sup
.
l(p, t)
p,t
p,tF,0 l(p, )
given interruption time t, let p (t) denote function t/l(p, t). words,
acceleration ratio X simply maximum value p (t), possible interruption
times problem instances p. Figure 1 illustrates example schedule
2 problem instances 4 processors. Note value p peaks
contract completed, problem instance.
Given schedule X associated set contracts C, two problem instances
p1 , p2 P , let C1 , C2 denote two (potentially infinite) subsets C, contracts
C1 problem tag p1 , contracts C2 problem tag p2 . Consider new
set contracts C 0 identical C, exception every contract C1
acquires problem tag p2 instead p1 every contract C2 acquires problem tag p1
instead p2 . Consider also schedule X 0 otherwise identical X (except
problem tag swaps described above). say X 0 obtained X swap
sets C1 C2 .
Following convention Lopez-Ortiz Schuierer (2004), given two schedules X
X 0 , say X contained X 0 time , denoted X X 0 two
schedules identical time . Given sequence schedules V = (X1 , X2 , . . .)
say V converges limit schedule X > 0 exists N
N , Xm Xm+1 . limit schedule X defined obvious way.

3. Matching Lower Bound Acceleration Ratio
section prove lower bound acceleration ratio applies schedules. proceed proof main result, present intuition behind
approach illustrated Figure 2. Given arbitrary schedule, implement
transformations successively transform schedules complying certain regularization conditions (see Lemma 1, Lemma 2, Theorem 1 below). transformations
either preserve reduce acceleration ratio, purpose infuse certain
amount structure schedule. important observe transformation
538

fiOptimal Scheduling Contract Algorithms Anytime Problem-Solving

Processor 0
Processor 1
Processor 2
Processor 3

6
5

p (t)

4
3
2
1

1

2

3

4

5

6

7

8

9

10

Time

Figure 1: top figure depicts schedule contracts case 4 processors
2 problems, first ten time units. Here, white hatched rectangles
correspond executions contracts two problems, respectively,
idle time schedule. bottom figure depicts plots function
p (t) vs time two problems (p {1, 2}). hatched (white) area
top corresponds solid (hashed) line plot bottom (respectively).
acceleration ratio maximum value, axis, attained either curve,
example equal 4.

Theorem 1 might actually result object non-feasible schedule,
well defined acceleration ratio is, again, shown greater
original schedule. lower-bound acceleration ratio obtained (i.e.,
normalized) schedules, show matches upper bound Bernstein et al. (2003).
similar approach showing optimality cyclic strategies applied Lopez-Ortiz
Schuierer (2004), context parallel robot searching concurrent rays. need
emphasize, however, series transformations problem consider
paper differ significantly ones Lopez-Ortiz Schuierer. due
fact ray-searching problem easier argue structural properties
optimal algorithm. Consider following example. Suppose time t, given
ray r explored distance d. optimal algorithm (and indeed every
reasonable search algorithm) must time t0 > t, robot explores ray
r, proceed least distance origin (otherwise, algorithm
gains nothing exploration). contrast, scheduling problem consider
paper, may case certain processor M1 , contract length l
scheduled start time finish time + l, whereas different processor M2 ,
different contract problem scheduled start time bigger finish
time smaller + l (i.e., length smaller l). first sight, latter contract
539

fiLopez-Ortiz, Angelopoulos, & Hamel

appears redundant; however l large, interruption occurs right
+ l, schedule may gain smaller-length contract. fact, schedules
indeed possible optimal algorithm, ruled transformation
techniques. particular, example illustrates difficult give blackbox transformation proof Lopez-Ortiz Schuierer problem interest
(although would interesting obtain explicit reduction).
(worst)
Space schedules
Lemma 1
acceleration
ratio

Lemma 2
Theorem 1

(best)

Figure 2: Illustration proof technique. shaded region corresponds artificial,
possibly infeasible, strategies by-products proof could
principle acceleration ratio strictly smaller optimal acceleration
ratio; however, show never happens, thus shaded region collapses
line strategies non-strategies acceleration ratio exactly equal
optimal value.
Note assume, without loss generality, schedule start
contract smaller equal one already completed given
problem.
Let X given schedule contracts. follow convention denoting
lower-case upper-case lengths pair consecutively completed contracts,
given problem p, respectively. precisely, (pi , di ) denotes contract length
di problem pi , earliest contract X pi completed contract
(pi , di ) finishes denoted (pi , Di ).
Definition 2 schedule X contracts, given contract c length Dc define
acceleration ratio Dc (that is, immediately completion c X) r(c) =
(Tc + Dc )/dc (assuming dc 6= 0), Tc denotes time processor start
working contract (pc , Dc ).
particular, let C 0 denote set contracts schedule, excluding first
completed contract problem. Observation 2,
Rm,n (X) =

sup
cC 0 ,0

converges supcC 0 r(c).
540

Tc + Dc
dc

fiOptimal Scheduling Contract Algorithms Anytime Problem-Solving

following lemma establishes quasi-cyclic property optimal strategy schedule.
states problem shorter-length completed contract generally given
precedence another problem longer completed contract; one possible
exception: smaller-length contract passed processor assignment
next contract schedule must shorter one assigned problem
went ahead instead.
Lemma 2 establishes another facet quasi-cyclicality property: problem
shorter-length contract, whether favored next processor assignment, must
complete next contract problem longer completed contract, thus
re-establishing quasi-cyclic property.
Lemma 1 Consider schedule X, suppose time Ti processor start
working contract (pi , Di ) another problem pj contracts (pj , dj )
(pj , Dj ) X. Let Tj denote time processor start working
contract (pj , Dj ).
Given two problems pi pj described dj < di Tj > Ti ,
either Dj < Di define new schedule Dj < Di , whose acceleration
ratio worse original schedule.
Proof. Suppose X dj < di Tj > Ti , suppose Dj > Di . Execute
swap program tags contracts pi complete (pi , di ) contracts
pj complete (pj , dj ), obtain new schedule X 0 (recall definition
problem swapping, given Section 2).
Observe contracts swapping remain untouched. Likewise, contracts
problems involved swap also unaffected. Therefore, contribution
acceleration ratio unchanged.
Consider contracts swapped problems first swapped
contract completed. is, let cr r > contract problem pi original
schedule. schedule contribution acceleration ratio (Tr +Dr )/dr .
swap expression still denotes acceleration ratio corresponding problem
pj , since contracts dr Dr run problem pj , implies schedule
remains unaffected swapping contracts well.
Thus place acceleration ratio changes right time
swapping previously longest completed contract (the denominator) comes
old schedule new contract completed comes new schedule
(the numerator). show acceleration ratio new schedule X 0
contracts worse original schedule.
acceleration ratio original schedule Di , Dj


Ti + Di Tj + Dj
max
,
di
dj
whereas acceleration ratio X 0 Di , Dj


Ti + Di Tj + Dj
max
,
.
dj
di
541

fiLopez-Ortiz, Angelopoulos, & Hamel

since Tj > Ti Dj > Di ,
Tj +Dj
dj

Tj +Dj
dj



Ti +Di
dj ;

moreover since dj < di ,

Tj +Dj
di .



Therefore, acceleration ratio X greater equal
acceleration ratio alternative schedule.

Corollary 1 Given schedule X schedule X 0 worse acceleration ratio
property two problems pi pj , dj < di X, Tj > Ti ,
always case Dj < Di X 0 .
Proof. X already satisfies condition set X 0 := X nothing show.
Otherwise apply process, i.e., appropriate problem swapping, argued proof
Lemma 1 following way: Let F = {f1 , f2 , . . . , } sorted sequence contract
finish times problems X. given f` define p(f` ) problem associated
finishing time f` X. Let pj = p(f` ) let dj length contract associated
f` . starting f1 letting f` range ` = 1, 2 . . . check
dj < di Tj > Ti Dj < Di not, swap contracts described
proof Lemma 1.

introduce notation needed statement proof Lemma 2.
schedule X, let ST denote set contracts completed time inclusive.
Also let complement ST , namely contracts X ST . Fix contract
C0 = (p0 , D0 ), scheduled start time T0 . problem pj observe
Lemma 1 Dj = min{D : (pj , D) T0 +D0 }. Observe context
definitions, dj = max{d : (pj , d) ST0 +D0 }.
Lemma 2 Let Ci = (pi , Di ) contract scheduled X time Ti Cj = (pj , Dj )
contract Ti . exists schedule X 0 worse acceleration ratio
di dj problem pj 6= pi Ti + Di Tj + Dj .
Proof. X satisfies conditions set X 0 := X lemma holds.
Otherwise case, schedule X di dj least one
problem pj 6= pi Ti + Di < Tj + Dj . Consider swap contracts
problems pi pj Ti +Di swap problem tags defined Section 2. argue
swap gives rise new schedule X 0 worse acceleration ratio. One
see suffices look acceleration ratio affected points right
Ci Cj completed. First, note swap, acceleration ratio
two points equal quantity


Ti + Di Tj + Dj
= max
,
di
dj
swap, acceleration ratio two points becomes


Ti + Di Tj + Dj
= max
,
.
dj
di
+D

Since dj di Ti + Di Tj + Dj j dj j thus , means
acceleration ratio worsen two specific points.

542

fiOptimal Scheduling Contract Algorithms Anytime Problem-Solving

Lemma 1 apply process repeatedly, starting dj length
contract associated smallest finish time f` (with ` = 1, 2 . . .) checking
larger contract length di dj Ti +Di Tj +Dj . one
contracts, select Di smallest completion time Ti + Di swap tags
(pj , Dj ). proceed next finishing time f`+1 . produces schedule
contracts di dj (and pj 6= pi ) Ti + Di Tj + Dj .
Definition 3 schedule X said normalized satisfies conditions Corollary 1 Lemma 2.
Lemma 3 exists optimal normalized schedule.
Proof. Since Lemma 2 stronger Lemma 1, applying Lemma 2 cannot violate
conditions Lemma 1 pair contracts swapped. indeed possible,
however, swap contract conflict earliest
recently modified contracts. precise, consider configuration Figure 3(a)
original schedule next scheduled contracts completion
contracts di dj , Tj +Dj originally larger Ti +Di . swap
get new completion times Tj0 + Dj0 := Ti + Di Ti0 + Di0 := Tj + Dj . Observe, however
new completion time Ti0 + Di0 create new conflict another schedule dk
illustrated Figure 3. hard verify figure represents general case
type conflict created.
dj

di

dk

Ti + Di

Tk + Dk

Tj + Dj

dj

di

dk

Tj0 + Dj0

Tk + Dk

Ti0 + Di0

Figure 3: Situation single application Lemma 2.
key observe original setting pairs hdj , di hdj , dk formed
inversion, i.e. constituted violation Lemma 2. However application
lemma inversions disappeared new inversion hdi , dk created
net decrease one number inversions schedule. Hence process must
eventually resolve inversions involving contracts index N fixed value
N process converges well defined strategy.

Theorem 1 acceleration ratio Rm,n (X) optimal normalized schedule X n
problems processors least
k+n
P

Rm,n (X) sup
k0

i=0
k
P

xsi

i=km+1

(1)
xsi

X = (xs0 , xs1 , . . .) sorted sequence contract lengths (in increasing order, ties
broken arbitrarily) schedule Xand define xsi = 0 < 0.
543

fiLopez-Ortiz, Angelopoulos, & Hamel

Proof. Let X optimal normalized schedule. Consider time T0 processor
M0 begin new contract. Since X normalized schedule, M0 choose
problem p0 way satisfies conditions Corollary 1. Let D0 alloted
processing time M0 devote p0 starting time T0 . Let longest completed
contract problem p0 time T0 + D0 d0 .
observe that, Lemmas 1 2, every contract length strictly smaller
d0 must complete within open interval (0, T0 + D0 ), hence end
interval every processor engaged contract length least d0 every problem
completed contract length d0 previous step.
lengths contracts d` d0 0 ` n 1 elements sequence
X . Similarly let Mj denote processor denote Ij set indices X
contracts executed processor Mj time T0 + D0 , inclusively, 0 j 1.
Note d0 = xsk0 , k0 0.
Furthermore, let Dj last completed contract processor Mj , say problem
p` , previous completed contract dj p` , less d0 . acceleration
ratio problem p` Dj given
P

iIj xi
xskj
according Observation 2, 0 j 1. Hence, worst case acceleration ratio
occurred time contracts first exceeding d0 completed
least
(P
)
Pm1 P


iIj xi
j=0
iIj xi
.
(2)
Rm,n (X) max

Pm1
0jm1
xskj
j=0 xkj
make use ofPthe fact
a, b, c, > 0. Note
P smax {a/c, b/d} (a + b)/(c + d),
completed
x
contains

summands

x
sum = m1

j=0
iIj
time T0 + D0 . particular know includes xs` smaller xsk0 ,
Lemma 2 guarantees problem completed contract dj complete another
contract Dj problem completed contract d0 dj T0 + D0 hence
summation given time T0 + D0 contains xs` (i.e. dj s) smaller xsk0
(i.e. d0 ). words, every element xsk0 sorted schedule X appears A.
observe n 1 problems p1 , . . . , pn1 must completed
durations exceeding xsk0 otherwise current contract length D0 would
assigned problem instead p0 . contains sorted values
X xsk0 plus least n 1 larger values corresponding finished contracts
n 1 problems. smallest choices n 1 values together
D0 xsk0 +1 , . . . , xsk0 +n . Hence, obtain
m1
XX

xsi



j=0 iIj

kX
0 +n

xsi .

(3)

i=0

Consider values xkj = dj , 1 j 1. Recall value Dj time
problem pj completed time T0 + D0 processor Mj dj longest
544

fiOptimal Scheduling Contract Algorithms Anytime Problem-Solving

completed contract pj time Tj + Dj . Lemma 2 d0 largest time
among di s. 1 largest di values xsk0 m+1 , . . . , xsk0 1
m1
X

k0
X

dj

xsi .

(4)

i=k0 m+1

j=0

Combining (2),(3) (4)
kP
0 +n

Pm1 P
Rm,n (X)


iIj xi

j=0
Pm1
j=0 xkj



i=0
k0
P

xsi

i=k0 m+1

,
xsi

k0 n.

order prove lower bound right hand side Inequality (1) make use
results Gal (1980) Schuierer (2001) state without proof
simplified form completeness; particular, follow work Schuierer (2001, Thm.
1)1 . Define Ga = (1, a, a2 , . . .) geometric sequence X +i = (xi , xi+1 , . . .)
suffix sequence X starting xi .
Theorem 2 (Schuierer, 2001) Let X = (x0 , x1 , . . .) sequence positive numbers,
r integer, = limn (xn )1/n , R {+}. Fk , k 0, sequence
functionals satisfy
1. Fk (X) depends x0 , x1 , . . . , xk+r ,
2. Fk (X) continuous, xi > 0, 0 k + r,
3. Fk (X) = Fk (X), > 0,
4. Fk (X + ) max{Fk (X), Fk (Y )},
5. Fk+i (X) Fk (X +i ), 1,

sup Fk (X) sup Fk (Ga ).
0k<

0k<

case, sequence X represents lengths contracts schedule.
acceleration ratio completion contract forms sequence functionals.
value functional depends prefix contracts whose start time
current time. sequence Ga represents geometric sequence,
wish show describes optimal schedule.
1. Theorem 1 proven Schuierer (2001) applies broad setting, purposes proof
suffices consider case p = 1.

545

fiLopez-Ortiz, Angelopoulos, & Hamel

Proposition 1 Let Fk (X ) sequence functionals defined follows:
k+n
X



Fk (X ) =

xsi

k
X



i=0

xsi ,

i=km+1

Fk (X ) satisfies conditions Theorem 2.
Proof. straightforward see Fk (X ) satisfies conditions (1)-(3). verify
k+n
k
P
P
xsi define YT YB analogously (i.e.,
condition (4), let XT =
xi , XB =
i=0

i=km+1

km
P

substituting xsi yis ). Observe YT = YB + Q, Q =

i=0

yis +

k+n
P
i=k+1

yis . Now,

wish show Fk (X + ) max{Fk (X), Fk (Y )} equivalently
XT + YT
XT + YB + Q
=
max
XB + YB
XB + YB



XT YT
,
XB YB


(5)

follows previously noted inequality max {a/c, b/d} (a + b)/(c + d),
a, b, c, > 0.
verify Condition (5) Theorem 2, first note definition Fk (X )

Pk+n+j
xi
Fk+j (X) = Pk+ji=0

i=km+1+j xi

Pk+n
Fk (X

+j

) = Pk

i=0

xsi+j


i=km+1 xi+j

Pk+n+j
i=j

= Pk+j

xsi

i=km+1+j

xsi

.

Lastly observe since terms X positive hence
j1
X
i=0

xsi

0

=

j1
X

k+n+j
X
xsi +
xsi
i=0
i=j



k+n+j
X

Pj1
xsi

i=0

=

Pk+n+j

xsi +

i=j

Pk+j

i=j

i=km+1+j

xsi

xsi

Pk+n+j
i=j

Pk+j

i=km+1+j

required.
Therefore, combining Theorem 1, Proposition 1 Theorem 2 have:


Rm,n (X) sup Fk (X ) sup Fk (Ga ) = sup
0k<

0k<
k+n
P

Note 6= 1,

ai

i=0
k
P

=
ai

0k<

ak+n+1 1
ak+1 akm+1

=

(k+n
X







i=0

1
k+1


1a

k
X



)




.

i=km+1

. Therefore, < 1,

i=km+1

deduce Rm,n (X) tends infinity k . Moreover, = 1, obtain
546

xsi
xsi

fiOptimal Scheduling Contract Algorithms Anytime Problem-Solving

Rm,n (X) = k+n+1
, which, likewise, tends infinity k . Hence, assume
> 1 obtain


(ak+n+1 1)/(a 1)
Rm,n (X)
sup
k+1 akm+1 )/(a 1)
0k< (a
k+n+1


1

an+m
(a>1)
=
sup
=
=
.
k+1 akm+1
1
1
0k<
expression minimized = ((m + n)/n)1/m , implies acceleration ratio X bounded

m+n (m+n)/m
n + n m+n

n
Rm,n (X)
.
=
m+n

n
n 1
thus shown following theorem:
Theorem 3 Given n problem instances processors, every schedule simulates
interruptible algorithm using executions contract algorithms acceleration ratio
less
n + n m+n

.

n
worth pointing round-robin schedule contract lengths 1, a, a2 , ...
1
m+n m+n
n

acceleration ratio precisely
value = m+n
, shown
n

n
Bernstein et al. (2003), thus matches lower bound. words, round robin
length-increasing schedule proposed Bernstein et al. optimal among possible
schedules whether round robin length-increasing, not. say
optimal schedules round robin length-increasing, fact one easily construct
optimal non round-robin schedules case n multiple m. formally,
obtain following theorem.
Theorem 4 optimal schedule n problems processors simulates interruptible algorithm using executions contract algorithms acceleration ratio
n + n m+n

.

n
Theorem 4 provides tight bound worst-case acceleration ratio. precisely,
assume interruptions issued malicious adversary, typically right
contract algorithm terminates execution. sense, measure extremely
pessimistic, reflects performance schedule adversarial setting.
next section show upper-bound average-case acceleration ratio exponential
schedules. issue stochastic deadlines addressed Zilberstein et al. (2003)
case single processor n problem instances. setting, uncertainty
interruption quality output contract algorithm. Similar
types analysis applied Kao Littman (1997) ray-search problem
two rays, assuming probabilistic knowledge placement target.
547

fiLopez-Ortiz, Angelopoulos, & Hamel

G3

G0

b3
b0

b1

b2

Figure 4: Example = 2, n = 3. line corresponds acceleration ratio
problem interrupted time t.

4. Average-Case Analysis Exponential Strategies
section present average-case analysis acceleration ratio exponential
strategies (recall formal definition given Section 2). scenario, interruption
occurs time chosen adversarially, rather chosen uniformly random
interval [0, U ], U > 0. Likewise, problem instance queried
also chosen uniformly random among n problems.
similar problem considered context robots searching target
rays. natural scenario hiker becomes injured woods. searchers must
efficiently explore forest trails find injured person soon possible.
setting reason expect hiker would locate adversarially.
Hence worst case bound provides exact upper bound worst possible
delay reaching target, average case analysis gives realistic estimate
expected amount time target found. Kao et al. (1996) showed
average target found nearly twice fast worst case scenario.
consider analogous setting interruption time chosen independently random, rather adversarially. Hence, expect randomly-chosen
interruption likely coincide interruptions yield high values
acceleration ratio worst-case scenario.
remainder section, consider exponential schedule X base b;
is, length k-th contract cyclic ordering bk . Let Gk denote finish
time contract. also assume problems numbered 0, . . . , n 1;
ordering makes presentation easier follow.
formalizing concept average acceleration ratio illustrate intuition
using example. Refer Figure 4, depicts example n = 3 problems
= 2 processors. Here, three jagged line segments corresponds
acceleration ratio problem function interruption time. precisely,
line plot function p (t), formally defined Section 2. Consider example
problem 0 shown solid line Figure 4. first contract completed
problem contract 0, length b0 , completion time G0 . contract
completed, available processors occupied computing contracts length b1 b2
548

fiOptimal Scheduling Contract Algorithms Anytime Problem-Solving

problems 1 2 respectively. Eventually, processor becomes available computes
contract length b3 problem 0, completed time G3 . Note
acceleration ratio problem 0 degrades linearly (i.e., increases) time interval
G0 G3 . Similar observations made remaining linear segments plot,
well two problems.
average acceleration ratio specific problem described area
respective acceleration ratio curve, normalized length U sampled space
[0, U ]. overall average acceleration ratio given average individual
acceleration ratio averages n problems2 . Since acceleration ratio
problem piece-wise linear function, order obtain average compute
integral line segment, sum segments. normalize, need
divide length interval well number n problems.
Consider interruption interval [Gk , Gn+k ), index k cyclic
order, let pk denote problem corresponds contract finish time Gk .
Since schedule exponential, follows pk (T ) = /bk . Since random
variable, pk also random variable. Thus, compute average acceleration
ratio problem pk within interval [Gk , Gn+k )

E[Gk ,Gk+n ) [pk ] =

1
Gk+n Gk

=

1
Gk+n Gk

=
=
=

Z

Gk+n

pk (x) dx
Gk
Z Gk+n
Gk

x
dx
bk

Z Gk+n Gk
Gk + x
1
dx
Gk+n Gk 0
bk


1
Gk (Gk+n Gk ) (Gk+n Gk )2

+
Gk+n Gk
bk
2 bk
Gk+n + Gk
.
2bk

compute average acceleration ratio problem pk entire span [0, U ] need
add area entire jagged line corresponds problem. Observe
area single sawtooth given quantity (Gk+n Gk ) E[Gk ,Gk+n ) [pk ].
allows us give expressions acceleration ratio one n problems.

2. Normally, standard assumption interruptions occur least one contract problem
completed, namely time Gn1 . make simplifying assumption interruptions occur
earlier Gn1 , case acceleration ratio 0. refinement interruptions
Gn1 follows along lines discussion section. case average sampled
space [Gn1 , U ) net zero effect asymptotic acceleration ratio (the extra term goes
zero U goes infinity), results much complicated expressions.

549

fiLopez-Ortiz, Angelopoulos, & Hamel

particular, average acceleration ratio problem 0 given by:


br/nc
X

1
G(k+1)n Gkn E[Gkn ,G(k+1)n ) [0 ] + (U Gr+n )E[Gr+n ,U ) [0 ]
E[0,U ) [0 ] =
U
k=0


br/nc 2
2
2
2
X
G

G
U Gr+n
1
kn
(k+1)n

=
+
(6)
U
2br+n
2bkn
k=0

r U [Gr+n , Gr+n+1 ). Note last term corresponds
truncated sawtooth interval [Gr+n , U ].
expression average acceleration ratio problem {1, . . . n 1} similar; however, instead endpoints G0 , Gn , G2n , . . . need consider endpoints
Gi , Gn+i , G2n+i , . . .; precisely, obtain expression


ri
bX
2
2
n c
G(w+1)n+i Gwn+i U 2 G2r+i
1
E[0,U ) [i ] =
+
(7)
.
U
2bwn+i
2br+i
w=0

overall acceleration ratio random variable defined =
(7) obtain

1
n

Pn1
i=0

. Using (6)

n1

1X
E[0,U ] [i ]
n

E[0,U ] [] =

i=0

1
nU

=

1
nU

=

r
X
k=0
r
X
k=0

(Gk+n Gk ) E[Gk ,Gk+n ) [0 ] +

n1
X

!
(U Gr+i ) E[Gr+i ,U ] [i ]

i=1

X U 2 G2r+i
G2k+n G2k n1
+
2br+i
2bk

!
.

(8)

i=1

easy compute Gk (Bernstein et al., 2003, Proof Theorem 1). Namely,
bk/mc

Gk =

X

bk/mc
mi+(k mod m)

b

k mod

= b

i=0

X

bmi =

i=0

bk+m bk mod
.
bm 1

simplicity presentation consider case U = Gr+n . general case
analogous, though contains slightly complicated expressions.
!
r
X
X G2r+n G2r+i
G2k+n G2k n1
1
E[0,Gr+n ] [] =
+
nGr+n
2br+i
2bk
k=0

=

1/(bm 1)
2n(br+n+m bJr+nKm )

i=1
r
X (bk+n+m

bJk+nKm )2 (bk+m bJkKm )2
bk
k=0
!
n1
X (br+n+m bJr+nKm )2 (br+i+m bJr+iKm )2
+
.
(9)
br+i
i=1

550

fiOptimal Scheduling Contract Algorithms Anytime Problem-Solving

JxKm denotes x mod evaluate separately two summations involved
(9). First,
r
X
(bk+n+m bJk+nKm )2 (bk+m bJkKm )2
k=0

=
=

=

bk

=

r
X

!
2Jk+nKm
2JkKm
b
b
b2(n+m)+k 2bn+m+Jk+nKm +
b2m+k + 2bm+JkKm
bk
bk
k=0
!
r
2Jk+nKm
2JkKm
X
b
b
(b2(n+m) b2m )bk 2bn+m+Jk+nKm +
+ 2bm+JkKm
bk
bk
k=0

r
X
b2m
b2m
2(n+m)
2m k
n+2m
2m
(b
b )b + 2b
+ k + 2b + k
b
b
k=0



r+1 1

b2m+1
2(n+m)
2m b
n+2m
b
b
+ rb
+O
,
(10)
b1
b1

penultimate step used JxKm < m. Second, get
n1
X

(br+n+m bJr+nKm )2 (br+i+m bJr+iKm )2
=
br+i
i=1

n1
X b2(n+m)+r 2bn+m+Jr+nKm
b2Jr+nKm
b2Jr+iKm
2m++i
m+Jr+iKm
=
+
b
+ 2b

bi
br+i
br+i
i=1
2m+1 n1

n1 1
n1 1
2b
(b
1)
2m+r+n+1 b
2m+r+1 b
= b
+O

b
+
b1
br (b 1)
b1

2m+1n n1

b
(b
1)
2m
.
(11)
+ nb
+O
br (b 1)
Substituting (10) (11) (9) using simple algebraic expansion simplification
obtain


b
n1 1
(bm 1)1
2(n+m)
2m
2m+n+1
2m+1 b
E[0,Gr+n ] [] =
b
b
+ b
b
b1
b1
2n(bn+m bJr+nKm r )


2m+1 n1


1
(b 1)
1
b
(b
1) 1
+
r
rnbn+2m r +
. (12)
Jr+nK
r
n+m

b
b1
b
2n(b
b
)
Note (12) O-terms tend asymptotically 0 r approaches infinity. Hence,
terms negligible impact acceleration ratio interruption occurs
far ahead time, sense considered error terms. particular,
obtain following simplified expression asymptotic acceleration ratio:


b

1
n1 1
r (b 1)
2(n+m)
2m
2m+n+1
2m+1 b
E[0,Gr+n ] [] =
b
b
+ b
b
2nbn+m
b1
b1

n
b (b 1)(b + 1)
=
.
2n(bm 1)(b 1)
551

fiLopez-Ortiz, Angelopoulos, & Hamel

Theorem 5 asymptotic average acceleration ratio exponential schedule bi ,
= 0, 1, . . ., interruption chosen uniformly random [0, U ] bounded
(bm+n bm ) (b + 1)
.
2n (bm 1) (b 1)
Proof. Follows discussion above.



Corollary 2 asymptotic average acceleration ratio optimal schedule Theorem
4 bounded



n
1
m+n


1
+
1
(m + n) m+n
n
n


.
1


2mn m+n

1
n
Corollary 3 schedule optimal acceleration ratio non-optimal average acceleration ratio.
Proof. readily verified computing derivative expression
Theorem 5 evaluating around value b = ((m+n)/n)1/m used optimal worst
case strategy. One observe algebraic manipulation derivative
always negative point, implies larger value b0 = b + results lower
(i.e. better) average acceleration ratio hence worst-case optimal schedule
optimal average sense.


5. Conclusion
paper resolved open question posed Bernstein et al. (2003) concerning
optimal acceleration ratio schedule contract algorithms. well-studied
problem artificial intelligence, several applications design real-time systems.
main result shows optimal schedules found class cyclic schedules,
or, alternatively, cannot improve quality simulation designing
complicated schedules. also performed average-case analysis exponential schedules,
assuming uniform distribution interruption times.
recent work, Angelopoulos, Lopez-Ortiz, Hamel (2008) able apply
similar techniques design optimal schedules interruptible algorithms
presence soft deadlines. setting, interruption hard deadline, sense
interruption occurs, algorithm allowed additional window time
complete execution (which seen, intuitively, grace period). algorithm
must report solution queried problem within additional time window.
different example work, Angelopoulos Lopez-Ortiz (2009) addressed
refinements acceleration ratio reflect better performance schedules
number problems larger number available processors. cases,
resorted use normalization techniques along lines Section 3.
provides evidence techniques tied specific variant problem,
instead applicable much wider settings.
interesting open problem find tight bounds randomized strategies, namely
schedules length contracts, well processors contracts
552

fiOptimal Scheduling Contract Algorithms Anytime Problem-Solving

assigned random variables. Note randomization known help
context ray searching problem (Kao et al., 1996). related (and challenging) problem find schedules achieve optimal average-case acceleration ratio.
exponential schedules optimal measure? Based analysis Section 4,
expect answer question fairly technical involved. even
ambitious problem find optimal schedules assuming certain known probability
distribution interruption times problems queried.
Exponential schedules seen example doubling algorithm,
lengths contracts increase geometrically. challenging part work (as Angelopoulos et al., 2008; Angelopoulos & Lopez-Ortiz, 2009) lower-bound performance arbitrary strategies: accomplished lower-bounding supremum
sequence functions functionals geometric sequences (Theorem 2). believe
similar techniques applied many optimization problems doubling
algorithms known perform well (see, e.g., survey Chrobak & Mathieu, 2006).
recent example found work Langetepe (2010), concerning optimality
spiral search locating target plane.
Last, least, optimal schedule presented work parallels
search strategies problem searching rays using p robots, exact correspondence remains shown. correspondence would extend one established
Bernstein et al. (2003) concerning cyclic schedules strategies.

6. Acknowledgments
preliminary version paper (Lopez-Ortiz, Angelopoulos, & Hamel, 2006) appeared
Proceedings Twenty-First National Conference Artificial Intelligence (AAAI).

References
Alpern, S., & Gal, S. (2003). Theory Search Games Rendezvous. Kluwer
Academic Publishers.
Althofer, I. (1997). symbiosis man machine beats grandmaster Timoshchenko.
Journal International Computer Chess Association., 20 (1), 4047.
Angelopoulos, S., & Lopez-Ortiz, A. (2009). Interruptible algorithms multi-problem
solving. Proceedings 21st International Joint Conference Artificial Intelligence (IJCAI), pp. 380386.
Angelopoulos, S., Lopez-Ortiz, A., & Hamel, A. (2008). Optimal scheduling contract
algorithms soft deadlines. Proceedings 23rd National Conference
Artificial Intelligence (AAAI), pp. 868873.
Baeza-Yates, R., Culberson, J., & Rawlins, G. (1993). Searching plane. Information
Computation, 106, 234252.
Bernstein, D. S., Finkelstein, L., & Zilberstein, S. (2003). Contract algorithms robots
rays: Unifying two scheduling problems.. Proceedings Eighteenth International Joint Conference Artificial Intelligence (IJCAI), pp. 12111217.
553

fiLopez-Ortiz, Angelopoulos, & Hamel

Bernstein, D., Perkins, T. J., Zilberstein, S., & Finkelstein, L. (2002). Scheduling contract algorithms multiple processors. Proceedings Eighteenth National
Conference Artificial Intelligence (AAAI), pp. 702706.
Chrobak, M., & Mathieu, C. (2006). Competitiveness via doubling. SIGACT News, 37 (4),
115126.
Dean, T., & Boddy, M. S. (1988). analysis time-dependent planning. Proceedings
7th National Conference Artificial Intelligence, pp. 4954.
Gal, S. (1980). Search Games. Academic Press.
Horvitz, E. (1987). Reasoning beliefs actions computational resource constraints. Proceedings Third Annual Conference Uncertainty Artificial
Intelligence (UAI), pp. 301324.
Horvitz, E. (1998). Reasoning varying uncertain resource constraints. Proceedings 7th National Conference Artificial Intelligence (AAAI), pp. 111116.
Jaillet, P., & Stafford, M. (2001). Online searching. Operations Research, 49, 501515.
Kao, M.-Y., & Littman, M. L. (1997). Algorithms informed cows. AAAI workshop
Online Search.
Kao, M.-Y., Ma, Y., Sipser, M., & Yin, Y. (1998). Optimal constructions hybrid algorithms. Journal Algorithms, 29 (1), 142164.
Kao, M.-Y., Reif, J. H., & Tate, S. R. (1996). Searching unknown environment: optimal randomized algorithm cow-path problem. Information Computation,
131 (1), 6379.
Langetepe, E. (2010). optimality spiral search. Proceedings 21st Annual
ACM-SIAM Symposium Discrete Algorithms (SODA).
Lopez-Ortiz, A., Angelopoulos, S., & Hamel, A. (2006). Optimal scheduling contract
algorithms anytime problems. Proceedings 21st National Conference
Artificial Intelligence (AAAI).
Lopez-Ortiz, A., & Schuierer, S. (2004). Online parallel heuristics, processor scheduling
robot searching competitive framework. Theoretical Computer Science,
310, 527537.
Russell, S. J., & Zilberstein, S. (1991). Composing real-time systems. Proceedings
12th International Joint Conference Artificial Intelligence (IJCAI), pp. 212217.
Schuierer, S. (2001). Lower bounds on-line geometric searching. Computational Geometry:
Theory Applications, 18 (1), 3753.
Schuierer, S. (2003). lower bound randomized searching rays. Computer
Science Perspective, pp. 264277.
Zilberstein, S. (1996). Using anytime algorithms intelligent systems. AI Magazine, 17 (3),
7383.
Zilberstein, S., Charpillet, F., & Chassaing, P. (2003). Real-time problem-solving
contract algorithms. Annals Mathematics Artificial Intelligence, 39 (12), 1
18.

554

fiJournal Artificial Intelligence Research 51 (2014)

Submitted 02/14; published 11/14

Using Meta-mining Support Data Mining Workflow
Planning Optimization
Phong Nguyen
Melanie Hilario

Phong.Nguyen@unige.ch
Melanie.Hilario@unige.ch

Department Computer Science
University Geneva
Switzerland

Alexandros Kalousis

Alexandros.Kalousis@hesge.ch

Department Business Informatics
University Applied Sciences
Western Switzerland,
Department Computer Science
University Geneva
Switzerland

Abstract
Knowledge Discovery Databases complex process involves many different
data processing learning operators. Todays Knowledge Discovery Support Systems
contain several hundred operators. major challenge assist user designing
workflows valid also ideally optimize performance measure
associated user goal. paper present system. system relies
meta-mining module analyses past data mining experiments extracts metamining models associate dataset characteristics workflow descriptors view
workflow performance optimization. meta-mining model used within data mining
workflow planner, guide planner workflow planning. learn metamining models using similarity learning approach, extract workflow descriptors
mining workflows generalized relational patterns accounting also domain
knowledge provided data mining ontology. evaluate quality data mining
workflows system produces collection real world datasets coming
biology show produces workflows significantly better alternative
methods workflow selection planning.

1. Introduction
Learning models extracting knowledge data using data mining extremely
complex process requires combining number Data Mining (DM) operators, selected large pools available operators, combined data mining workflow. DM
workflow assembly individual data transformations analysis steps, implemented
DM operators, composing DM process data analyst chooses address
his/her DM task. Workflows recently emerged new paradigm representing
managing complex computations accelerating pace scientific progress. (meta-)
analysis becoming increasingly challenging growing number complexity
available operators (Gil et al., 2007).
c
2014
AI Access Foundation. rights reserved.

fiNguyen, Hilario & Kalousis

Todays second generation knowledge discovery support systems (KDSS) allow complex
modeling workflows contain several hundreds operators; RapidMiner platform (Klinkenberg, Mierswa, & Fischer, 2007), extended version Weka (Hall
et al., 2009) R (R Core Team, 2013), proposes actually 500 operators,
complex data control flows, e.g. bagging boosting operators,
several sub-workflows interleaved. consequence, possible number
workflows modeled within systems order several millions,
ranging simple elaborated workflows several hundred operators. Therefore data analyst carefully select among operators ones
meaningfully combined address his/her knowledge discovery problem. However, even
sophisticated data miner overwhelmed complexity modeling,
rely his/her experience biases well thorough experimentation
hope finding best operator combination. advance new generation
KDSS provide even advanced functionalities, becomes important provide
automated support user workflow modeling process, issue
identified one top-ten challenges data mining (Yang & Wu, 2006).

2. State Art DM Workflow Design Support
last decade, rather limited number systems proposed provide
automated user support design DM workflows. Bernstein, Provost, Hill (2005)
propose ontology-based Intelligent Discovery Assistant (ida) plans valid DM workflows valid sense executed without failure according basic
descriptions input dataset attribute types, presence missing values, number
classes, etc. describing DM ontology input conditions output effects
DM operators, according three main steps KD process, pre-processing, modeling post-processing (Fayyad, Piatetsky-Shapiro, & Smyth, 1996), ida systematically
enumerates workflow planner possible valid operator combinations, workflows,
fulfill data input request. ranking workflows computed according
user defined criteria speed memory consumption measured
past experiments.
Zakova, Kremen, Zelezny, Lavrac (2011) propose kd ontology support automatic design DM workflows relational DM. ontology, DM relational algorithms
datasets modeled semantic web language OWL-DL, providing semantic
reasoning inference querying DM workflow repository. Similar ida,
kd ontology describes DM algorithms data input/output specifications.
authors developed translator ontology representation Planning Domain Definition Language (PDDL) (McDermott et al., 1998), produce
abstract directed-acyclic graph workflows using FF-style planning algorithm (Hoffmann,
2001). demonstrate approach genomic product engineering (CAD) usecases complex workflows produced make use relational data structure
background knowledge.
recently, e-LICO project1 featured another ida built upon planner
constructs DM plans following hierarchical task networks (HTN) planning approach.
1. http://www.e-lico.eu

606

fiUsing Meta-mining Support DM Workflow Planning Optimization

specification HTN given Data Mining Workflow (dmwf) ontology (Kietz,
Serban, Bernstein, & Fischer, 2009). predecessors, e-LICO ida designed
identify operators whose preconditions met given planning step order plan
valid DM workflows exhaustive search space possible DM plans.
None three DM support systems discussed consider eventual
performance workflows plan respect DM task supposed
address. example, goal plan/design workflows solve classification
problem, would like consider measure classification performance, accuracy,
deliver workflows optimize it. discussed DM support systems deliver
extremely large number plans, DM workflows, typically ranked simple
heuristics, workflow complexity expected execution time, leaving user
loss best workflow terms expected performance DM task
he/she needs address. Even worse, planning search space large
systems even fail complete planning process, see example discussion
Kietz, Serban, Bernstein, Fischer (2012).
considerable work tries support user, view performance
maximization, specific part DM process, modeling learning.
number approaches proposed, collectively identified meta-learning (Brazdil,
Giraud-Carrier, Soares, & Vilalta, 2008; Kalousis, 2002; Kalousis & Theoharis, 1999; Hilario, 2002; Soares & Brazdil, 2000). main idea meta-learning given new
dataset system able rank pool learning algorithms respect
expected performance dataset. so, one builds meta-learning model
analysis past learning experiments, searching associations algorithms performances dataset characteristics. However, already mentioned, meta-learning
approaches address learning/modeling part applicable complete
process level.
work Hilario, Nguyen, Do, Woznica, Kalousis (2011), first effort
lift meta-learning ideas level complete DM workflows. proposed novel metalearning framework called meta-mining process-oriented meta-learning applied
complete DM process. associate workflow descriptors dataset descriptors,
applying decision tree algorithms past experiments, order learn couplings
workflows datasets lead high predictive performance. workflow descriptors
extracted using frequent pattern mining accommodating also background knowledge,
given Data Mining Optimization (dmop) ontology, DM tasks, operators, workflows, performance measures relationships. However predictive performance
system rather low, due limited capacity decision trees capturing
relations dataset workflow characteristics essential
performance prediction.
address limitation, presented work Nguyen, Wang, Hilario,
Kalousis (2012b) approach learns called heterogeneous similarity measures,
associating dataset workflow characteristics. learn similarity measures
dataset space, workflow space, dataset-workflow space. similarity
measures reflect respectively: similarity datasets given similarity
relative workflow performance vectors workflows applied them;
similarity workflows given performance based similarity different datasets;
607

fiNguyen, Hilario & Kalousis

dataset-workflow similarity based expected performance latter applied
former. However two meta-mining methods described limited select
from, rank, set given workflows according expected performance, i.e.
cannot plan new workflows given input dataset.
Retrospectively, presented work Nguyen, Kalousis, Hilario (2011)
initial blueprint approach DM workflow planning view workflow performance optimization. suggested planner guided metamining model ranks partial candidate workflows planning step. also gave
preliminary evaluation proposed approach interesting results (Nguyen, Kalousis,
& Hilario, 2012a). However, meta-mining module rather trivial, uses dataset
pattern-based workflow descriptors nearest-neighbor search dataset descriptors identify similar datasets dataset want plan
workflows. Within neighborhood, ranks partial workflows using support
workflow patterns workflows perform best datasets neighborhood. pattern-based ranking workflows cumbersome heuristic;
system learning associations dataset workflow characteristics explicitly optimize expected workflow performance, must guide workflow
planning. approach deployed Kietz et al. (2012).
paper, follow line work first sketched work Nguyen et al.
(2011). couple tightly together workflow planning meta-mining module
develop DM workflow planning system given input dataset designs workflows
expected optimize performance given dataset. meta-mining
module applies heterogeneous similarity learning method presented Nguyen et al.
(2012b) learn associations dataset workflow descriptors lead optimal
performance. exploits learned associations guide planner workflow
construction planning. best knowledge, first system
kind, i.e. able design DM workflows specifically tailored
characteristics input dataset view optimizing DM task performance measure.
evaluate system number real world datasets show workflows
plans significantly better workflows delivered number baseline methods.
rest paper structured follows. section 3, present global architecture system, together brief description planner. section 4
describe detail meta-mining module, including dataset workflow characteristics uses, learning model, learned model used workflow planning.
section 5, provide detailed experimental results evaluate approach different
settings. Finally, conclude section 6.

3. System Description
section, provide general description system. start defining
notations use throughout paper give brief overview
different components system. two important components, planner
meta-miner described subsequent sections (3.4 4 respectively).
close section providing formal representation DM workflows
use planner meta-miner.
608

fiUsing Meta-mining Support DM Workflow Planning Optimization

Symbol

e
wl = [o1 , . . . , ol ]
wcl = (I(p1 wl ), . . . , I(p|P | wl ))T
xu = (x1 , . . . , xd )T
r(x, w)
g


= {o1 , . . . , }
Cl
Sl

Meaning
workflow operator.
workflow data type.
ground DM workflow sequence l operators.
fixed length |P |-dimensional vector description
workflow wl
d-dimensional vector description dataset.
relative performance rank w workflow x
dataset
DM goal.
HTN task.
HTN method.
HTN abstract operator n possible operators.
set candidate workflows abstract operator O.
set candidate workflows selected Cl .

Table 1: Summary notations used.
3.1 Notations
provide basic notations subsequently introduce additional notations needed. use term DM experiment designate execution
DM workflow w W dataset x X . describe dataset x d-dimensional
column vector xu = (x1 , . . . xd )T ; describe detail section 4.1.1 dataset descriptions use. experiment characterized performance measure;
example mining problem addressing classification problem one
performance measure accuracy. performance measures workflows
applied given dataset x extract relative performance rank r(x, w) R+
workflow w x dataset. statistically comparing performance
differences different workflows given dataset (more section 4.1.3).
matrix X : n contains descriptions n datasets used training system. given workflow w two representations. wl denote
l-length operator sequence constitutes workflow. Note different workflows
different lengths, fixed length representation. wcl denote
fixed-length |P |-dimensional binary vector representation w workflow; feature
wcl indicates presence absence relational feature/pattern workflow.
Essentially wcl propositional representation workflow; describe
detail section 4.1.2 extract propositional representation. Finally W denotes
collection workflows, used training system, W
corresponding |P | matrix contains propositional representations. Depending context different workflow notations used interchangeably. Table 1
summarizes important notations.
3.2 System Architecture Operational Pipeline
provide Figure 1 high level architectural description system. three blue
shaded boxes are: (i) Data Mining Experiment Repository (dmer) stores
609

fiNguyen, Hilario & Kalousis

input data

3. User Interface
goal
input MD

1.

input MD

DMER

optimal plans 5.

training MD

MetaMiner

Intelligent Discovery Assistant (IDA)

(partial) candidate workflows
AI
Planner

DM Workflow
Ontology (DMWF)

workflow ranking
4.

online
mode

metamined
model

offline
mode

2.

DM Optimization
Ontology (DMOP)

software
data flow

Figure 1: meta-mining systems components pipeline.
base-level resources, i.e. training datasets, workflows, well performance results
application latter former; essentially dmer contains training data
used derive models necessary workflow planning
design; (ii) user interface user interacts system, specifying
data mining tasks input datasets (iii) Intelligent Discovery Assistant (ida)
component actually plans data mining workflows optimize
performance measure given dataset data mining task user provided
input. ida constitutes core system contains planning component
meta-mining component interact closely order deliver optimal workflows
given input problem; describe two components detail sections 3.4 4
respectively. system operates two modes, offline online mode. offline
mode meta-mining component analyses past base-level data mining experiments,
stored dmer, learn meta-mining model associates dataset workflow
characteristics view performance optimization. online mode meta-miner
interacts planner guide planning workflows using meta-mining
model.
go briefly different steps systems life cycle. first
need collect dmer sufficient number base-level data mining experiments,
i.e. applications different data mining workflows different datasets (step 1).
experiments used step 2 meta-miner generate meta-mining model.
precisely extract base level experiments number characteristics
describe datasets workflows performance latter achieved
applied former. meta-data meta-miner learns associations dataset
workflow characteristics lead high performance; learning heterogeneous similarity measure outputs high similarity workflow dataset
former expected achieve high performance applied latter
(more details section 4.2).
model learned, offline phase completed online phase
start. online phase system directly interacts user. given user
select data mining task g G, classification, regression, clustering, etc, well
610

fiUsing Meta-mining Support DM Workflow Planning Optimization

input dataset task applied; might also specify
number top-k optimal workflows planned (step 3). Given new task
input dataset action transfered IDA ai-planner starts
planning process. step planning process ai-planner generates list
valid actions, partial candidate workflows, passes ranking meta-miner
according expected performance given dataset, step 4. meta-miner
ranks using learned meta-mining model planning continues topranked partial candidate workflows data mining task resolved. end
planning process, top-k workflows presented user order given
expected performance input dataset. greedy planning approach
step select top-k current solutions. principle let aiplanner first generate possible workflows meta-miner rank them.
resulting plans would ranked according local greedy approach,
would ranked globally thus optimally respect meta-mining model.
However general feasible due complexity planning process
combinatorial explosion number plans.
3.3 DM Workflow Representation
give formal definition DM workflow represent it. DM
workflows directed acyclic typed graphs (DAGs), nodes correspond operators
edges nodes data input/output objects. fact hierarchical DAGs
since dominating nodes/operators contain sub-workflows. typical
example cross-validation operator whose control flow given parallel execution
training sub-workflows, complex operator boosting. formally, let:
set available operators appear DM workflow, e.g. classification operators, J48, SVMs, etc. also includes dominating operators
defined one sub-workflows dominate. operator
defined name labelling function (o), data types e E
inputs outputs, direct sub-workflows dominating operator.
E set available data types appear DM workflow, namely
data types various I/O objects appear DM workflow
models, datasets, attributes, etc.
graph structure DM workflow pair (O , E ), also contains subworkflows any. set vertices correspond operators used
DM workflow sub-workflow(s), E E set pairs nodes, (oi , oj ),
directed edges, correspond data types output/input objects,
passed operator oi operator oj . Following graph structure, topological sort
DM workflow permutation vertices graph edge (oi , oj )
implies oi appears oj , i.e. complete ordering nodes directed
acyclic graph given node sequence:
wl = [o1 , .., ol ]
611

(1)

fiNguyen, Hilario & Kalousis

Legend
input / output edges
sub input / output edges
X
X

End

Retrieve

basic nodes
output

composite nodes

result
X-Validation
Split

training set

Join

test set

Weight Information Gain

weights
example set
Apply Model
performance
Select Weights
labelled data
training set
Performance
Naive Bayes
model

Figure 2: Example DM workflow performance estimation combination
feature selection classification algorithms.

subscript wl denotes length l (i.e. number operators) topological
sort. topological sort DM workflow structurally represented rooted,
labelled ordered tree (Bringmann, 2004; Zaki, 2005), depth-first search
graph structure maximum depth given expanding recursively subworkflows dominating operators. Thus topological sort workflow tree
representation reduction original directed acyclic graph nodes
edges fully ordered.
example hierarchical DAG representing RapidMiner DM workflow given
Figure 2. graph corresponds DM workflow cross-validates feature selection
method followed classification model building step NaiveBayes classifier. XValidation typical example dominating operator workflow
two basic blocks, training block arbitrary workflow receives
input dataset outputs model, testing block receives input model
dataset outputs performance measure. particular, training subworkflow, feature weights computed InformationGain operator,
number features selected SelectByWeights operator, followed
final model building NaiveBayes operator. testing block, typical
sub-workflow consists application learned model testing set
ApplyModel operator, followed performance estimation given Performance
operator. topological sort graph given ordered tree given Figure 3.
612

fiUsing Meta-mining Support DM Workflow Planning Optimization

Retrieve
1

Weight
Information Gain
3

Select
Weights
4

X-Validation
2/8

Naive
Bayes
5

End
9

Apply
Model
6

Performance
7

Figure 3: topological order DM workflow given Figure 2.

3.4 Workflow Planning
workflow planner use based work Kietz et al. (2009, 2012), designs
DM workflows using hierarchical task network (HTN) decomposition crisp-dm process model (Chapman et al., 2000). However planner workflow enumeration
generating possible plans, i.e. consider expected workflow performance
planning process scale large set operators explode
search space. order address limitations, presented preliminary results
planner coupled frequent pattern meta-mining algorithm
nearest-neighbor algorithm rank partial workflows step workflow planning (Nguyen et al., 2011, 2012a). system also deployed Kietz et al. (2012).
approach followed prioritize partial workflows according support
frequent patterns contained achieved set workflows performed
well set datasets similar input dataset want plan
workflow. However learning associations dataset workflow
characteristics, approach follow here.
section 3.4.1 briefly describe HTN planner Kietz et al. (2009, 2012);
section 3.4.2 describe use prediction expected performance
partial-workflow applied given dataset guide HTN planner. give
complete description meta-mining module learns associations dataset
workflow characteristics expected achieve high performance section 4.
3.4.1 HTN Planning
Given goal g G, AI-planner decompose top-down manner goal
elements two sets, tasks methods . task achieve g,
subset associated methods share data input/output
(I/O) object specification address it. turn, method
defines sequence operators, and/or abstract operators (see below), and/or sub-tasks,
executed order achieve m. recursively expanding tasks, methods
operators given goal g, AI-planner sequentially construct HTN plan
terminal nodes correspond operators, non-terminal nodes HTN
task method decompositions, dominating operators (X-Validation instance
dominate training testing sub-workflows). example HTN plan given
613

fiNguyen, Hilario & Kalousis

EvaluateAttributeSelectionClassification
X-Validation
In: Dataset
Out: Performance

AttributeSelection
ClassificationTraining

Classification
Training

AttributeSelection
Training

Attribute
Weighting
Operator
In: Dataset
Out: AttributeWeights

Select
Weights
In: Dataset
In: AttributeWeights
Out: Dataset

Predictive
Supervised
Learner
In: Dataset
Out: Predictive
Model

AttributeSelection
ClassificationTesting

Model
Application
Apply
Model
In: Dataset
In: Predictive
Model
Out: Labelled
Dataset

Model
Evaluation
In: Labelled
Dataset
Out: Performance

Figure 4: HTN plan DM workflow given Figure 2. Non-terminal nodes
HTN tasks/methods, except dominating operator X-Validation. Abstract
operators bold simple operators italic, annotated
I/O specification.

Figure 4. plan corresponds feature selection classification workflow
Figure 2 exactly topological sort (of operators) given Figure 3.
sets goals G, tasks , methods , operators O, relations,
described dmwf ontology (Kietz et al., 2009). There, methods operators
annotated pre- post-conditions used AI-planner.
Additionally, set operators enriched shallow taxonomic view
operators share I/O object specification grouped common
ancestor:
= {o1 , . . . , }

(2)

defines abstract operator, i.e. operator choice point alternative among set
n syntactically similar operators. example, abstract AttributeWeighting operator
given Figure 4 include feature weighting algorithms InformationGain
ReliefF, similarly abstract Predictive Supervised Learner operator contain
classification algorithms NaiveBayes linear SVM.
HTN planner also plan operators applied attribute, e.g.
continuous attribute normalization operator, discretization operator. uses cyclic
planning structures apply subsets attributes. case use attributegrouping functionality require use cyclic planning structures.
precisely operator selected application applied appropriate
attributes. Overall HTN grammar contains descriptions 16 different tasks
614

fiUsing Meta-mining Support DM Workflow Planning Optimization

100 operators, planner plan. numbers limited
modeling effort required describe tasks operators, inherent
limitation HTN grammar/planner. Kietz et al. (2012) developed module
open-source ontology editor Protege called E-ProPlan2 facilitate modelling
new operators describe task-method decomposition grammars DM problems
dmwf ontology.
3.4.2 Workflow Selection Task
AI-planner designs construction HTN plan several partial workflows,
derived substituting abstract operator one n
operators. number planned workflows order several thousands
leave user loss workflow choose her/his problem;
even worse, possible planning process never succeeds find valid plan
terminate due complexity search space.
support user selection DM workflows, use post-planning approach designed workflows evaluated according evaluation
measure order find k ones globally best given mining problem. However, global approach computational expensive planning
phase mentioned above. Instead, follow planning approach
locally guide AI-planner towards design DM workflows optimized
given problem, avoiding thus need explore whole planning search space.
Clearly, following local approach cost one pay potential reduction
performance since workflows explored evaluated, potentially missing good
ones due greedy nature plan construction.
adopt heuristic hill climbing approach guide planner. abstract
operator need determine n candidate operators expected
achieve best performance given dataset. formally, define by:
Cl := {wlo = [wl1 Sl1 |o O]}kn

(3)

set k n partial candidate workflows length l generated
expansion abstract operator adding one n candidate operators
one k candidate workflows length l 1 constitute set Sl1 workflows
selected previous planning step (S0 empty set see below). let xu vector
description input dataset want plan workflows address
data mining goal g optimize performance measure. Moreover let wclo binary
vector provides propositional representation wlo workflow respect
set generalized relational frequent workflow patterns contains3 . construct
set Sl selected workflows current planning step according to:
Sl := {arg max r(xu , wclo |g)}k

(4)

{wlo Cl }

2. Available http://www.e-lico.eu/eproplan.html
3. provide detailed description extract wclo descriptors section 4.1.2,
moment note propositional representations fixed length representations,
depend l.

615

fiNguyen, Hilario & Kalousis

r(xu , wclo |g) estimated performance workflow propositional description wclo applied dataset description xu , i.e. planning step
select best current partial workflows according estimated expected performance. meta-mining model, learn past experiments, delivers
estimations expected performance. section 4.2 describe derive
meta-mining models use get estimates expected performance.
stress producing performance estimate r(xu , wclo |g)
meta-mining model uses workflow description wclo , candidate operator descriptions. pattern-based descriptions capture dependencies interactions
different operators workflows (again wclo representation
section 4.1.2). rather crucial point since well known fact construct data mining workflows need consider relations biases different
algorithms use within them, bias combinations better others.
pattern based descriptions use provide precisely type information, i.e.
information operator combinations appearing within workflows.
next section provide complete description meta-miner module,
including characterize datasets, workflows performance latter applied
former, course learn meta-mining models use planning.

4. Meta-Miner
meta-miner component operates two modes. offline mode learns
past experimental data meta-mining model provides expected performance
estimates r(xu , wclo |g). on-line mode interacts AI-planner step
planning process, delivering r(xu , wclo |g) estimates used planner
select workflows planning step according Eq.(4).
rest section organised follows. subsection 4.1.1 explain
describe datasets, i.e. derive xu dataset descriptors; subsection 4.1.2
derive propositional representation wclo data mining workflows subsection 4.1.3 rank workflows according performance given dataset, i.e.
r(xu , wclo |g). Finally subsection 4.2 explain build models past mining
experiments provide expected performance estimations r(xu , wclo |g)
use models within planner.
4.1 Meta-Data Performance Measures
section describe meta-data, namely dataset workflow descriptions,
performance measures used meta-miner learn meta-mining models
associate dataset workflow characteristics view planning DM workflows
optimize performance measure.
4.1.1 Dataset Characteristics
idea characterize datasets full-fledged research problem early
inception meta-learning (Michie, Spiegelhalter, Taylor, & Campbell, 1994;
Kopf, Taylor, & Keller, 2000; Pfahringer, Bensusan, & Giraud-Carrier., 2000; Soares &
616

fiUsing Meta-mining Support DM Workflow Planning Optimization

Brazdil, 2000; Hilario & Kalousis, 2001; Peng, Flach, Soares, & Brazdil, 2002; Kalousis,
Gama, & Hilario, 2004). Following state-of-art dataset characteristics, characterize
dataset x X three following groups characteristics.
Statistical information-theoretic measures: group refers data characteristics
defined STATLOG (Michie et al., 1994; King, Feng, & Sutherland, 1995)
METAL projects4 (Soares & Brazdil, 2000), includes number instances,
number classes, proportion missing values, proportion continuous / categorical
features, noise signal ratio, class entropy, mutual information. mainly describe
attribute statistics class distributions given dataset sample.
Geometrical topological measures: group concerns new measures try
capture geometrical topological complexity class boundaries (Ho & Basu,
2002, 2006), includes non-linearity, volume overlap region, maximum Fishers
discriminant ratio, fraction instance class boundary, ratio average intra/inter
class nearest neighbour distance.
Landmarking model-based measures: group related measures asserted
fast machine learning algorithms, called landmarkers (Pfahringer et al., 2000),
derivative based learned models (Peng et al., 2002), includes error
rates pairwise 1p values obtained landmarkers 1NN DecisionStump,
histogram weights learned Relief SVM. extended last group
new landmarking methods based weight distribution feature weighting
algorithms Relief SVM build twenty different histogram
representations discretized feature weights.
Overall, system makes use total = 150 numeric characteristics describe
dataset. denote vectorial representation dataset x X xu .
far exhaustive dataset characteristics used, including
characteristics subsampling landmarks (Leite & Brazdil, 2010). main goal
work produce comprehensive set dataset descriptors design DM
workflow planning system given set dataset characteristics, coupled workflow
descriptors, plan DM workflows optimize performance measure.
4.1.2 Workflow Characteristics
seen section 3.3 workflows graph structures quite complex
containing several nested sub-structures. often difficult analyze
spaghetti-like structure also information
subtask addressed workflow component (Van der Aalst & Giinther,
2007). Process mining addresses problem mining generalized patterns
workflow structures (Bose & der Aalst, 2009).
characterize DM workflows follow process mining like approach;
extract generalized, relational, frequent patterns tree representations,
use derive propositional representations them. possible generalizations
4. http://www.metal-kdd.org/

617

fiNguyen, Hilario & Kalousis

DM-Algorithm
DataProcessing
Algorithm

PredictiveModelling
Algorithm

FeatureWeighting
Algorithm

ClassificationModelling
Algorithm

LearnerFreeFW
Algorithm

UnivariateFW
Algorithm

MultivariateFW
Algorithm

MissingValues
Tolerant
Algorithm

Irrelevant
Tolerant
Algorithm

ExactCOS
Based
Algorithm

C4.5

Naive
Bayes

SVM

EntropyBasedFW
Algorithm

IG

ReliefF
is-followed-by

is-implemented-by

Figure 5: part dmops algorithm taxonomies. Short dashed arrows represent
is-followed-by relation DM algorithms, long dashed arrows represent is-implemented-by relation DM operators DM algorithms.

(a)

(b)

X-Validation

(c)

X-Validation

Feature
Weighting
Algorithm

Classification
Modeling
Algorithm

Feature
Weighting
Algorithm

Classification
Modeling
Algorithm

UnivariateFW
Algorithm

Irrelevant
Tolerant
Algorithm

LearnerFreeFW
Algorithm

MissingValues
Tolerant
Algorithm

EntropyBasedFW
Algorithm

X-Validation
Feature
Weighting
Algorithm

Classification
w
Algorithm
ExactCOS
Based
Algorithm

Figure 6: Three workflow patterns cross-level concepts. Thin edges depict workflow
decomposition; double lines depict dmops concept subsumption.

described domain knowledge which, among knowledge, given data
mining ontology. use Data Mining Optimization (dmop) ontology (Hilario
et al., 2009, 2011). Briefly, ontology provides formal conceptualization DM
domain describing DM algorithms defining relations terms DM tasks,
models workflows. describes learning algorithms C4.5, NaiveBayes SVM,
according learning behavior bias/variance profile, sensitivity
type attributes, etc. instance, algorithms cited tolerant irrelevant attributes, C4.5 NaiveBayes algorithms tolerant missing values,
whereas SVM NaiveBayes algorithms exact cost function. Algorithm characteristics families classified taxonomies dmop primitive
618

fiUsing Meta-mining Support DM Workflow Planning Optimization

concept DM-Algorithm. Moreover, dmop specifies workflow relations, algorithm order,
is-followed-by relation relates workflow operators DM algorithms
is-implemented-by relation. Figure 5 shows snapshot dmops algorithm taxonomies ground operators bottom related DM algorithms implement.
mine generalized relational patterns DM workflows, follow method
presented Hilario et al. (2011). First, use dmop ontology annotate set W
workflows. Then, extract set generalized patterns using frequent pattern
mining algorithm. Concretely, operator contained parse tree training DM
workflow wl W , insert tree branch operator taxonomic concepts,
ordered top bottom, implemented operator, given
dmop. result new parse tree additional nodes dmops
concepts. call parse tree augmented parse tree. reorder nodes
augmented parse tree satisfy dmops algorithm taxonomies relations.
example feature selection algorithm typically composed feature weighting algorithm
followed decision rule selects features according heuristics. result
set augmented reordered workflow parse trees. representation,
apply tree mining algorithm (Zaki, 2005) extracts set P frequent patterns.
pattern corresponds tree appears frequently within augmented parse trees;
mine patterns support higher equal five. principle go
low support one, exploding dimensionality description workflows,
probably features poor discriminatory power. Nevertheless since metamining models rely metric learning, able learn importance different
meta-features, would able cope also scenario. Note extract
workflow characteristics could used different techniques graph
mining directly graph structures defined workflows ontology,
main reason computational cost latter approaches, well
fact frequent pattern mining propositionalization known work well.
Figure 6 give examples mined patterns. Note extracted patterns
generalized, sense contain entities defined different abstraction levels,
provided dmop ontology. relational describe
relations, order relations, structures appear within workflow,
also contain properties entities described dmop ontology.
example pattern (c) Figure 6 states feature weighting algorithm (abstract
concept) followed (relation) classification algorithm exact cost function
(property), within cross-validation.
use set P frequent workflow patterns describe DM workflow wl W
patterns p P wl workflow contains. propositional description
workflow wl given |P |-length binary vector:
wcl = (I(p1 wl ), . . . , I(p|P | wl ))T {0, 1}|P |

(5)

denotes induced tree relation (Zaki, 2005) I(pi wl ) returns one
frequent pattern, pi , appears within workflow zero otherwise.
use propositional workflow representation together tabular representation datasets characteristics learn meta-mining models describe
next section. Although could used tree even graph properties represent
619

fiNguyen, Hilario & Kalousis

workflows, propositionalization standard approach used extensively successfully
learning problems learning instances complex structures (Kramer, Lavrac,
& Flach, 2000).
Also, propositional workflow representation easily deal parameter values
different operators appear within workflows. so, discretize
range values continuous parameter ranges low, medium, high,
ranges depending nature parameter, treat discretized values
simply property operators. resulting patterns parameter-aware;
include information parameter range mined operators
used support also parameter setting planning DM workflows.
However within paper explore possibility.
4.1.3 Performance-Based Ranking DM Workflow
characterize performance number workflows applied given dataset
use relative performance rank schema derive using statistical significance
tests. Given estimations performance measure different workflows
given dataset use statistical significance test compare estimated performances
every pair workflows. within given pair one workflows significantly
better gets one point gets zero points.
significance difference workflows get half point. final performance rank
workflow given dataset simply sum points pairwise
performance comparisons, higher better. denote relative performance
rank workflow wc applied dataset xu r(xu , wc ). Note workflow
applicable, executed, dataset x, set rank score default value zero
means workflow appropriate (if yet executed) given dataset.
planning goal g classification task, use evaluation measure
experiments classification accuracy, estimated ten-fold cross-validation,
significance testing using McNemars test, significance level 0.05.
next section describe build meta-mining models
past data-mining experiments using meta-data performance measures
described far use models support DM workflow planning.
4.2 Learning Meta-mining Models Workflow Planning
starting describe detail build meta-mining models let us take
step back give abstract picture type meta-mining setting
address. previous sections, described two types learning instances: datasets
x X workflows w W. Given set datasets set workflows stored
dmer, meta-miner build these, two training matrices X W.
X : n dataset matrix, ith row description xui ith dataset.
W : |P | workflow matrix, j th row description wcj jth workflow.
also preference matrix R : n m, Rij = r(xui , wcj ), i.e. gives
relative performance rank workflow wj applied dataset xi respect
workflows. see Rij measure appropriateness match
wj workflow xi dataset. ith line R matrix contains vector
620

fiUsing Meta-mining Support DM Workflow Planning Optimization

relative performance ranks workflows applied xui dataset.
meta-miner take input X, W R matrices output model
predicts expected performance, r(xu , wc ), workflow w applied dataset x.
construct meta-mining model using similarity learning, exploiting two basic
strategies initially presented context DM workflow selection (Nguyen et al., 2012b).
give high level presentation them, details interested user
refer original paper. first strategy learn homogeneous similarity
measures, measuring similarity datasets similarity workflows, use
derive r(xu , wc |g) estimates. second learn heterogeneous similarity measures
directly estimate appropriateness workflow dataset, i.e. produce
direct estimates r(xu , wc |g).
4.2.1 Learning Homogeneous Similarity Measures
goal provide meta-mining models good predictors performance
workflow applied dataset. simplest approach want learn good
similarity measure dataset space deem two datasets similar set
workflows applied result similar relative performance, i.e.
order workflows according performance achieve dataset
two datasets similar workflow orders similar. Thus learned similarity
measure dataset space good predictor similarity datasets
determined relative performance order workflows. completely
symmetrical manner consider two workflows similar achieve similar
relative performance scores set datasets. Thus case workflows learn
similarity measure workflow space good predictor similarity
relative performance scores set datasets.
Briefly, learn two Mahalanobis metric matrices, MX , MW , datasets
workflows respectively, optimizing two following convex metric learning optimization
problems:
min F1 = ||RRT XMX XT ||2F + tr(MX )
MX

s.t.

(6)

MX 0


min F2 = ||RT R WMW WT ||2F + tr(MW )
MW

s.t.

(7)

MW 0

||.||F Frobenius matrix norm, tr() matrix trace, 0 parameter
controlling trade-off empirical error metric complexity control overfitting. RRT : n n matrix reflects similarity relative workflow performance
vectors different dataset pairs learned dataset similarity metric
reflect. RT R : matrix gives respective similarities workflows.
details learning problem solve it, see work Nguyen et al.
(2012b).
621

fiNguyen, Hilario & Kalousis

Note far model computes expected relative performance
r(xu , wclo ). case homogeneous metric learning compute on-line
mode planning phase; describe right away following
paragraph.
Planning homogeneous similarity metrics (P1) use two
learned Mahalanobis matrices, MX , MW , compute dataset similarity workflow similarity finally compute estimates r(xu , wclo ) planning
step.
Concretely, prior planning determine similarity input dataset xu (for
want plan optimal DM workflows) training datasets xui X using
MX dataset metric measure dataset similarities. Mahalanobis similarity
two datasets, xu , xui , given
sX (xu , xui ) = xTu MX xui

(8)

Then, planning planning step determine similarity candidate
workflow wlo Cl training workflows wcj W,
sW (wclo , wcj ) = wcTlo MW wcj .

(9)

Finally derive r(xu , wclo ) estimate weighted average elements
R matrix. weights given similarity input dataset xu
training datasets, similarities candidate workflow wclo training
workflows. formally expected rank given by:
P
P
wcj W xui wcj r(xui , wcj |g)
xui X
P
P
(10)
r(xu , wclo |g) =
wc W xui wcj
xu X
j



xui wcj Gaussian weights given xui = exp(sX (xu , xui )/x ) wcj =
exp(sW (wclo , wcj )/w ); x w kernel widths control size neighbors
data workflow spaces respectively (Smart & Kaelbling, 2000; Forbes & Andre,
2000).
Using rank performance estimates delivered Eq.(10), select planning step best candidate workflows set, Sl , according Eq.(4). call resulting
planning strategy P1. P1 expected performance set selected candidate
workflows Sl greedily increases deliver k DM complete workflows
expected achieve best performance given dataset.
4.2.2 Learning Heterogeneous Similarity Measure
P1 planning strategy makes use two similarity measures learned independently other, one defined feature space. simplistic
assumption model interactions workflows datasets,
know certain types DM workflows appropriate datasets certain
types characteristics. order address limitation, define heterogeneous
metric learning problem directly estimate similarity/appropriateness
622

fiUsing Meta-mining Support DM Workflow Planning Optimization

workflow given dataset given r(xu , wc ) relative performance
measure.
Since learning Mahalanobis metric equivalent learning linear transformation
rewrite two Mahalanobis metric matrices described previously MX = UUT
MW = VVT . U : V : |P | respective linear transformation matrices
dimensionality = min(rank(X), rank(W)).
learn heterogeneous similarity
measure datasets workflows using two linear transformations solve
following optimization problem:
min F4 = ||R XUVT WT ||2F + ||RRT XUUT XT ||2F
U,V

+ ||RT R WVVT WT ||2F +

(11)


(||U||2F + ||V||2F )
2

using alternating gradient descent algorithm, first optimize U keeping
V fixed vice versa. optimization problem non-convex algorithm
converge local minimum. first term similar low-rank matrix factorization
Srebro, Rennie, Jaakkola (2005). However factorization learn
function dataset workflow feature spaces result address
samples training instances, also known cold start problem
recommendation systems. case DM workflow planning problem strong
requirement need able plan workflows datasets never
seen training, also able qualify workflows also seen
training. second third terms define metrics reflect performancebased similarities datasets workflows respectively (along lines homogeneous
metrics given previously), together give directly similarity/appropriateness
DM workflow dataset estimating expected relative predictive performance
as:
r(xu , wclo |g) = xu UVT wcTlo

(12)

see heterogeneous similarity metric performing projection dataset
workflow spaces common latent space compute standard similarity
projections. details, see work Nguyen et al. (2012b).
Planning heterogeneous similarity measure (P2) Planning heterogeneous similarity measure, strategy denote P2, much simpler planning homogeneous similarity measures. Given input dataset described xu
step planning make use relative performance estimate r(xu , wclo |g)
delivered Eq.(12) select set best workflows Sl set partial workflows Cl using selection process described Eq.(4). Unlike planning strategy P1
computes r(xu , wclo |g) weighted average help two independently learned similarity metrics, P2 relies heterogeneous metric directly computes
r(xu , wclo |g), modeling thus explicitly interactions dataset workflow characteristics.
note P1 P2 planning strategies able construct
workflows even pools ground operators include operators
never experimented baseline experiments, provided operators well
623

fiNguyen, Hilario & Kalousis

described within dmop. meta-mining models planner uses
prioritize workflows rely wclo descriptions workflow generalized
descriptions workflows operators.
next section evaluate ability two planning strategies
introduced plan DM workflows optimize predictive performance compare
number baseline strategies different scenarios.

5. Experimental Evaluation
evaluate approach data mining task classification. reasons
rather practical. Classification supervised task means
ground truth compare results produced classification
algorithm, using different evaluation measures accuracy, error, precision etc;
mining tasks clustering, performance evaluation comparison bit
problematic due lack ground truth. extensively studied,
extensively used many application fields, resulting plethora benchmark datasets,
easily reuse construct base-level experiments well evaluate
system. Moreover, extensively addressed context meta-learning,
providing baseline approaches compare approach. Finally approach
requires different algorithms operators use well described
dmop ontology. Due historical predominance classification task algorithms
well extensive use real world problems, started developing dmop them;
result task classification corresponding algorithms well described.
said this, emphasize approach limited task
classification. applied mining task define evaluation
measure, collect set benchmark datasets perform base-level experiments, provide descriptions task respective algorithms dmop.
train evaluate approach, collected set benchmark classification
datasets. applied set classification data mining workflows.
base-level experiments learn meta-mining models used
planner plan data mining workflows. challenge system new datasets
used training meta-mining models, datasets
plan new classification workflows achieve high level predictive performance.
explore two distinct evaluation scenarios. first one, constrain
system plans DM workflows selecting operators restricted operator
pool, namely operators experimented base-level experiments.
Thus operators characterized dmop ontology tested
base-level experiments; call tested operators. second scenario
allow system also choose operators never experimented
characterized dmop ontology; call operators untested
operators. goal second scenario evaluate extend system
effectively use untested operators workflows designs.
624

fiUsing Meta-mining Support DM Workflow Planning Optimization

Type
FS/tested
FS/tested
FS/tested

Abbr.
IG
CHI
RF

Parameters
-#features selected k = 10
-#features selected k = 10
-#features selected k = 10

SVMRFE

-#features selected k = 10

FS/untested
CL/tested

Operator
Information Gain
Chi-Square
ReliefF
Recursive feature
elimination SVM
Information Gain Ratio
One-nearest-neighbor

IGR
1NN

-#features selected k = 10

CL/tested

C4.5

C4.5

CL/tested

CART

CART

FS/tested

CL/tested
CL/tested

NaiveBayes normal
density estimation
Logistic regression
Linear kernel SVM

CL/tested

Gaussian kernel SVM

SVMr

CL/untested
CL/untested
CL/untested
CL/untested

Linear discriminant analysis
Rule induction
Random decision tree
Perceptron neural network

LDA
Ripper
RDT
NNet

CL/tested

-pruning confidence C
-min. inst. per leaf
-pruning confidence C
-min. inst. per leaf

= 0.25
=2
= 0.25
=2

NB
LR
SVMl

-complexity C = 1
-complexity C = 1
-gamma = 0.1

Table 2: Table operators used design DM workflows 65 datasets. type
corresponds feature selection (FS) classification (CL) operators. operators experimented marked tested, otherwise untested.

5.1 Base-Level Datasets DM Workflows
construct base-level experiments, collected 65 real world datasets genomic
microarray proteomic data related cancer diagnosis prognosis, mostly
National Center Biotechnology Information5 . typical datasets,
datasets use characterized high-dimensionality small sample size,
relatively low number classes, often two. average 79.26 instances,
15268.57 attributes, 2.33 classes.
build base-level experiments, applied datasets workflows consisted either single classification algorithm, combination feature selection
classification algorithm. Although HTN planner use (Kietz et al., 2009, 2012)
able generate much complex workflows, 16 different tasks, 100
operators, limit planning classification, feature selection
classification, workflows simply respective tasks, algorithms operators
well annotated dmop. annotation important characterization
workflows construction good meta-mining models used guide
planning. Nevertheless, system directly usable planning scenarios com5. http://www.ncbi.nlm.nih.gov/

625

fiNguyen, Hilario & Kalousis

plexity, describe HTN grammar, provided appropriate tasks,
algorithms operators annotated dmop ontology.
used four feature selection algorithms together seven classification algorithms
build set base-level training experiments. given Table 2, noted
tested. mentioned previously, also plan operators parameters
discretizing range values parameters treating properties
operators. Another alternative use inner cross-validation automatically select
set parameter values; strictly speaking, case, would selecting
standard operator cross-validated variant. Nevertheless, would incur significant
computational cost.
Overall, seven workflows contained classification algorithm,
28 workflows combination feature selection classification algorithm,
resulting total 35 workflows applied 65 datasets corresponds 2275 baselevel DM experiments. performance measure use accuracy estimate
using ten-fold cross-validation. algorithms, used implementations provided
RapidMiner DM suite (Klinkenberg et al., 2007).
already said, two evaluation settings. first, Scenario 1, constrain
system plan workflows using tested operators. second, Scenario 2,
allow system select also untested operators. additional operators
also given Table 2, denoted untested. total number possible workflows
setting 62.
5.2 Meta-learning & Default Methods
compare performance system two baseline methods default
strategy. two baseline methods simple approaches fall classic metalearning stream instead selecting individual algorithms select
workflows. Thus cannot plan DM workflows used setting
workflows choose seen model construction phase.
first meta-learning method use, call Eucl, standard
approach meta-learning (Kalousis & Theoharis, 1999; Soares & Brazdil, 2000),
makes use Euclidean based similarity dataset characteristics select N
similar datasets input dataset xu want select workflows
averages workflow rank vectors produce average rank vector:
N
1 X
rxui , xui {arg max xTu xui }N
N
xui X

(13)



uses order different workflows. Thus method simply ranks workflows
according average performance achieve neighborhood input
dataset. second meta-learning method call Metric makes use learned
dataset similarity measure given Eq.(8) select N similar datasets
input dataset averages well respective workflow rank vectors:
N
1 X
rxui , xui {arg max xTu MX xui }N
N
xui X


626

(14)

fiUsing Meta-mining Support DM Workflow Planning Optimization

0.8

0.6

0.4

0.2

CHI+C45
RF+NBN
SVMRFE+C45
CHI+CART
RF+C45
SVMr
1NN
IG+C45
RF+1NN
SVMRFE+CART
C45
CHI+SVMr
RF+CART
CHI+SVMl
IG+CART
RF+SVMr
RF+SVMl
SVMRFE+1NN
CHI+1NN
IG+1NN
SVMRFE+SVMr
CART
CHI+NBN
IG+SVMr
SVMRFE+LR
CHI+LR
RF+LR
SVMRFE+NBN
IG+SVMl
SVMl
IG+LR
SVMRFE+SVMl
NBN
IG+NBN
LR

0.0

Figure 7: Percentage times workflow among top-5 workflows different datasets.

default recommendation strategy, simply use average rxui workflow
rank vectors collection training datasets:
1X
rxui , xui X
n
n

(15)



rank select workflows. note rather difficult baseline
beat. see case plot Figure 7 percentage times
35 DM workflows appears among top-5 worfklows 65 datasets. top
workflow, consisting LR algorithm, among top-5 80%
datasets. next two workflows, NBN IG NBN, among top-5
almost 60% datasets. words select top-5 workflows using default
strategy roughly 80% datasets LR correctly them,
NBN IG NBN percentage around 60%. Thus set dataset
quite similar respect workflows perform better them, making
default strategy rather difficult one beat.
5.3 Evaluation Methodology
estimate performance planned workflows evaluation scenarios
use leave-one-dataset-out, using time 64 datasets build meta-mining
models one dataset plan.
evaluate method measuring well list, L, top-k ranked workflows, delivers given dataset, correlates true list, , top-k ranked
627

fiNguyen, Hilario & Kalousis

workflows dataset using rank correlation measure. place true quotes
general case, i.e. restrict choice operators specific
set, cannot know true best workflows unless exhaustively examine
exponential number them, however since select restricted list operators
set best. precisely, measure rank correlation
two lists L , use Kendall distance p penalty, denote
K (p) (L, ) (Fagin, Kumar, & Sivakumar, 2003). Kendall distance gives number
exchanges needed bubble sort convert one list other. assigns penalty
p pair workflows one workflow one list other;
set p = 1/2. K (1/2) (L, ) normalized, propose define normalized
Kendall similarity Ks(L,T) as:
1

K ( 2 ) (L, )
Ks(L, ) = 1
u

(16)

1

(2)

Pktakes values [0, 1]. u upper bound K (L, ) given u = 0.5k(5k + 1)
2 i=1 i, derived direct application lemma 3.2 work Fagin et al. (2003),
assume two lists share element. qualify method,
m, including two baselines, Kendall similarity gain, Kg(m), i.e. gain (or loss)
achieves respect default strategy given datasets, compute as:

Kg(m)(L, ) =

Ks(m)(L, )
1
Ks(def )(L, )

(17)

method, report average Kendall similarity gain overall datasets,
Kg(m). Note that, Scenario 1, default strategy based average ranks
35 workflows. Scenario 1, default strategy based average ranks 62
workflows, also experiment order set baseline.
addition see well top-k ranked list workflows, given method
suggests given dataset, correlates true list, also compute average accuracy
top-k workflows suggests achieve given dataset, report average
overall datasets.
5.4 Meta-mining Model Selection
iteration leave-one-dataset-out evaluation planning performance,
rebuild meta-mining model tune parameter Mahalanobis metric
learning using inner ten-fold cross-validation; select value maximizes
Spearman rank correlation coefficient predicted workflow rank vectors
real rank vectors. heterogenous metric, used parameter setting defined
Nguyen et al. (2012b). two meta-learning methods, fixed number N
nearest neighbors five, reflecting prior belief appropriate neighborhood size.
planning, set manually dataset kernel width parameter kx = 0.04
workflow kernel width parameter kw = 0.08 result small dataset workflow
neighborhoods respectively. Again, two parameters tuned simply set
prior belief respective neighborhood size.
628

fiUsing Meta-mining Support DM Workflow Planning Optimization

(b) Scenario 2, tested untested operators.

0.10

10

15

20

25

30

Kg

0.00

0.05

5

0.10
0.15

0.15

0

P2
P1
def62

0.05

0.00
0.05

P2
P1
Metric
Eucl
def32

0.10

Kg

0.05

0.10

(a) Scenario 1, tested operators.

35

0

k

5

10

15

20

25

30

35

k

Figure 8: Average correlation gain Kg different methods baseline
65 bio-datasets. x-axis, k = 2 . . . 35, number top-k workflows
suggested user. P1 P2 two planning strategies. Metric
Eucl baseline methods defX default strategy computed set
X workflows.

5.5 Experimental Results
following sections give results experimental evaluation different
methods presented far two evaluation scenarios described above.
5.5.1 Scenario 1, Building DM workflows Pool Tested
Operators
scenario, evaluate quality DM workflows constructed two
planning strategies P1 P2 compare two baseline methods well
default strategy. leave-one-dataset-out evaluate workflow
recommendations given method. Figure 8(a) give average Kendall gain
Kg method default strategy compute top-k lists
k = 2, . . . , 35. Clearly P2 strategy one gives largest improvements
respect default strategy, 5% 10% gain, compared method.
establish statistical significance results k, counting number
datasets method better/worse default, using McNemars
test. summarize Figure 9 statistical significance results given p-values
different ks give detailed results Table 4 appendix methods
k = 2 . . . 35. see P 2 far best method significantly better
default 16 34 values k, close significant (0.05 < p-value 0.1)
ten 34 times never significantly worse. methods, P2
managed beat default 2 34 cases k.
629

fiNguyen, Hilario & Kalousis

0.5
5

10

15

20

25

30

35

5

10

15

20

25

30

Metric (0 Wins/0 Losses)

Eucl (0 Wins/0 Losses)

35

0.5
0.0
1.0

1.0

0.5

0.0

pvalue

0.5

1.0

k

1.0

k

0.5

pvalue

0.0

pvalue

1.0

0.5

0.0
1.0

0.5

pvalue

0.5

1.0

P1 (2 Wins/0 Losses)

1.0

P2 (16 Wins/0 Losses)

5

10

15

20

25

30

35

5

k

10

15

20

25

30

35

k

Figure 9: P-values McNemars test number times Kendal similarity
method better default given k, Scenario 1. positive pvalue means wins losses, negative opposite. solid lines
p = +/ 0.05, dash-dotted p = +/ 0.1. X Wins/ Losses
header indicates number times k = 3..35 method
significantly better/worse default.

examine average accuracy top-k workflows suggested
method achieve, advantage P2 even striking. average accuracy 1.25%
1.43% higher default strategy, k = 3 k = 5 respectively, see
Table 3(a). k = 3, P2 achieves higher average accuracy default 39
65 datasets, under-performs compared default 20. Using
McNemars test statistical significance 0.02, i.e. P2 significantly better
default strategy comes average accuracy top k = 3 workflows
plans; results similar k = 5. fact eight top-k lists, k = 3 . . . 10, P2
significantly better default five values k, close significantly better once,
never significantly worse. higher values k, k = 11 . . . 35, significantly
better 11 times, close significantly better three times, never significantly worse.
stops significantly better k > 30. large k values, average taken
almost workflows, thus expect important differences lists
produced different methods. Figure 10, visualize statistical significance
results different values k = 3 . . . 35 give detailed results Table 5
Appendix.
630

fiUsing Meta-mining Support DM Workflow Planning Optimization

0.5
5

10

15

20

25

30

35

5

10

15

20

25

30

Metric (4 Wins/0 Losses)

Eucl (0 Wins/0 Losses)

35

0.5
0.0
1.0

1.0

0.5

0.0

pvalue

0.5

1.0

k

1.0

k

0.5

pvalue

0.0

pvalue

1.0

0.5

0.0
1.0

0.5

pvalue

0.5

1.0

P1 (6 Wins/0 Losses)

1.0

P2 (16 Wins/0 Losses)

5

10

15

20

25

30

35

5

k

10

15

20

25

30

35

k

Figure 10: P-values McNemars test number times Average Accuracy method better default given k, Scenario 1.
figure interpretation Figure 9

P1 never significantly better default k = 3 . . . 10, k = 11 . . . 35,
significantly better nine values k, close significantly better three times,
close significantly worse once. Metric baseline never significantly better
default k = 3 . . . 10, k = 11 . . . 35, significantly better four values
k,and close significantly better four times. results EC quite poor. terms
average accuracy, similar default, terms number times
performs better default, cases, less number
times performs worse default. Figure 10 give results
statistical significance results different values k detailed results
Table 5 appendix. P2 method directly learns exploits
workflow planning associations dataset workflow characteristics
experimental results clearly demonstrate strategy best pays off.
5.5.2 Scenario 2: Building DM workflows Pool Tested
Non-Tested Operators
second scenario, evaluate performance two planning strategies, P1
P2, pool available operators planning limited
operators already experimented base-level experimented with,
extended include additional operators described dmop ontology.
631

fiNguyen, Hilario & Kalousis

(a) Scenario 1, tested operators

P2
P1
Metric
Eucl
def35

Avg. Acc
0.7988
0.7886
0.7861
0.7829
0.7863

k=3
W/L
39/20
26/38
25/38
30/32

p value
+0.02
0.17
0.13
0.90

Avg. Acc
0.7925
0.7855
0.7830
0.7782
0.7787

k=5
W/L
41/21
35/28
32/33
32/33

p value
+0.02
0.45
1.00
1.00

(b) Scenario 2, tested untested operators

P2
P1
def62

Avg. Acc
0.7974
0.7890
0.7867

k=3
W/L
39/24
29/34

p value
0.08
0.61

Avg. Acc
0.7907
0.7853
0.7842

k=5
W/L
34/29
31/34

p value
0.61
0.80

Table 3: Average accuracy top-k workflows suggested method. W indicates
number datasets method achieved top-k average accuracy larger
respective default, L number datasets smaller
default. p value result McNemars statistical significance
test; + indicates method statistically better default.

already described exact setting section 5.1; reminder number
possible workflows 62. before, estimate performances using leave-onedataset-out. Note two baseline methods, Metric Eucl, applicable
setting, since deployed workflows already
experimented baseline experiments. Here, already explained section 5.3,
default strategy correspond average rank 62 possible workflows
denote def62. Note highly optimistically-biased default method
since relies execution 62 possible workflows base-level datasets, unlike
P1 P2 get see 35 workflows model building, operators
therein, plan larger pool.
Figure 8(b), give average Kendall gain Kg P1 P2 def62
baseline. Similarly first evaluation scenario, P2 advantage P1 since
demonstrates higher gains default. Note though performance gains
smaller previously. terms number k values P2
(close be) significantly better default, six eight,
different k = 2 . . . 35. def62 significantly better P2 close
significantly better. concerns P1, significant difference
performance def62, value k. values k > 30, P2 systematically
under-performs compared def62, due advantage latter comes
seeing performance 62 workflows base-level dataset. visualize
statistical significance results top row Figure 11, give detailed results
Table 6 Appendix values k = 2 . . . 35.
632

fiUsing Meta-mining Support DM Workflow Planning Optimization

0.5
10

15

20

25

30

35

5

10

15

20

25

30

k

P2 (1 Wins/13 Losses)

P2 (0 Wins/3 Losses)

35

0.5
0.0
0.5
1.0

1.0

0.5

0.0

pvalue

0.5

1.0

k

1.0

5

pvalue

0.0

pvalue

1.0

0.5

0.0
1.0

0.5

pvalue

0.5

1.0

P1 (0 Wins/0 Losses)

1.0

P2 (4 Wins/1 Loss)

5

10

15

20

25

30

35

5

k

10

15

20

25

30

35

k

Figure 11: Top row p-values McNemars test number times Kendall
similarity method better default given k, Scenario 2. Bottom
row Average Accuracy. figure interpretation Figure 9.

concerns performance P2 respect average accuracy
top-k workflows suggests, slight advantage def62 small
values k, four. significantly better compared def62 once, k = 4.
k = 5 17, two methods significant difference, k = 18 . . . 35 P2
worse, 13 times significant manner. P1 picture slightly different, average
accuracy significantly different def62, exception three k values
significantly worse. visualize statistical significance results bottom
row Figure 11, give detailed results Table 7. seems fact P2
learns directly associations datasets workflow characteristics puts
disadvantage want plan operators tested training
phase. scenario, P1 strategy weights preferences dataset similarity
workflow similarity seems cope better untested operators. Nevertheless
still possible outperform default strategy significant manner, keeping
however mind def62 optimistic default strategy based
experimentation possible workflows training dataset.
5.6 Discussion
previous sections, evaluated two workflow planning strategies two settings:
planning tested operators, planning tested untested operators.
633

fiNguyen, Hilario & Kalousis

first scenario, P2 planning strategy makes use heterogeneous metric
learning model, directly connects dataset workflow characteristics, clearly stands
out. outperforms default strategy terms Kendall Similarity gain,
statistically significant, close statistically significant, manner 24 values k
[2, . . . , 35]; terms average accuracy top-k workflows, outperforms 20
values k statistically significant, close statistically significant, manner.
methods, including P1, follow large performance difference P2.
allow planners include workflows operators
used baseline experiments, annotated dmop ontology, P2s
performance advantage smaller. terms Kendall similarity gain, statistically
significant, close to, k [10, . . . , 20]. respect average accuracy top-k
lists, better default small lists, k = 3, 4; k > 23,
fact significantly worse. P1 fairs better second scenario, however performance
different default method. Keep mind though baseline used
second scenario quite optimistic one.
fact, see able generalize plan well datasets,
evidenced good performance P2 first setting. However, comes
generalizing datasets operators case second scenario
performance planned workflows good, exception top
workflows. take look new operators added second scenario,
feature selection algorithm, Information Gain Ratio, four classification algorithms,
namely Linear Discriminant Algorithm, Ripper rule induction algorithm, Neural Net
algorithm, Random Tree. them, Information Gain Ratio seen
base level set experiments algorithm, Information Gain, rather
similar learning bias it. Ripper rule induction sequential covering algorithm,
closest operators set training operators two decision tree
algorithms recursive partitioning algorithms. respect dmop ontology,
Ripper shares certain number common characteristics decision trees, however
meta-mining model contains information set-covering learning bias performs
different datasets. might lead selected given dataset based
common features decision trees, learning bias fact appropriate
dataset. Similar observations hold algorithms, example LDA
shares number properties SVMl LR, however learning bias, maximizing
between-to-within class distances ratio, different learning biases
two, meta-mining model contains information bias associates
dataset characteristics.
Overall, extent system able plan, tested untested
operators, workflows achieve good performance, depends extend
properties latter seen training meta-mining models
within operators experimented with, well whether
unseen properties affect critically final performance. case operators
well characterized experimentally, Scenario 1, performance
workflows designed P2 strategy good. Note necessary
operators workflows applied datasets, enough sufficient set
experiments operator. heterogeneous metric learning algorithm handle
634

fiUsing Meta-mining Support DM Workflow Planning Optimization

(b)

(a)

X-Validation

X-Validation

DataProcessing
Algorithm
FWAlgorithm

ClassificationModeling DataProcessing
Algorithm
Algorithm
FWAlgorithm

HighBiasCMA

MultivariateFW
Algorithm

ClassificationModeling
Algorithm
HighVarianceCMA

UnivariateFW
Algorithm

Figure 12: Top-ranked workflow patterns according average absolute weights given
matrix V.

incomplete preference matrices, using available information. course clear
available information, whether form complete preference matrices
form extensive base-level experiments large number datasets, better
quality learned meta-mining model be. interesting explore
sensitivity heterogeneous metric learning method different levels completeness
preference matrix; however outside scope present paper.
quantify importance different workflow patterns
operators properties analyzing linear transformation workflow patterns
contained heterogeneous metric. precisely, establish learned importance
workflow pattern averaging absolute values weights assigned
different factors (rows) V linear transformation matrix Eq.(11). Note
approach, establish importance patterns, whether
associated good bad predictive performance. Figure 12, give two
important patterns determined basis averaged absolute
weights. describe relations workflow operators, first one
indicates multivariate feature weighting algorithm followed high bias
classification algorithm, second describes univariate feature weighting algorithm
followed high bias classification algorithm. systematic analysis learned model
could provide hints one focus ontology building effort, looking
important patterns well patterns used. addition,
reveal parts ontology might need refinement order distinguish
different workflows respect expected performance.

6. Conclusions Future Work
paper, presented is, best knowledge, first system
able plan data mining workflows, given task given input dataset,
expected optimize given performance measure. system relies tight
interaction hierarchical task network planner learned meta-mining model
plan workflows. meta-mining model, heterogeneous learned metric, associates
datasets characteristics workflow characteristics expected lead good
635

fiNguyen, Hilario & Kalousis

performance. workflow characteristics describe relations different components workflows, capturing global interactions various operators appear
within them, incorporate domain knowledge latter given data mining ontology (dmop). learn meta-mining model collection past base-level mining
experiments, data mining workflows applied different datasets. carefully evaluated
system task classification showed outperforms significant
manner number baselines default strategy plan operators
experimented base-level experiments. performance
advantage less pronounced consider also planning operators
experimented base-level experiments, especially
properties operators present within operators
experimented base-level experiments.
system directly applicable mining tasks e.g. regression, clustering.
reasons focused classification mainly practical: extensive
annotation classification task related concepts data mining ontology,
large availability classification datasets, extensive relevant work meta-learning
dataset characterization classification. main hurdle experimenting
different mining task annotation necessary operators dmop ontology
set base-level collection mining experiments specific task. Although
annotation new algorithms operators quite labor intensive task, many
concepts currently available dmop directly usable mining tasks, e.g. cost
functions, optimization problems, feature properties etc. addition, small active
community, DMO-foundry6 , maintaining augmenting collaboratively ontology
new tasks operators, significantly reducing deployment barrier new task.
DMO-foundry web site, one find number tools templates facilitate
addition new concepts operators ontology well annotate
existing ones. said note use dmop sine-qua-non
system function. well perform workflow characterization task
mining ground operators, without using ontology. downside
would extracted patterns generalized contain operator
properties. Instead defined ground operators. Everything else remains
is.
number issues still need explore finer detail. would
like gain deeper understanding better characterization reduced performance planning untested operators; example, conditions
relatively confident suitability untested operator within workflow. want
experiment strategy suggested parameter tuning, treat
parameters yet another property operators, order see whether gives better
results; expect will. want study detail level missing information
preference matrix affects performance system, well whether using
ranking based loss functions metric learning problem instead sum squares would
lead even better performance.

6. http://www.dmo-foundry.org/

636

fiUsing Meta-mining Support DM Workflow Planning Optimization

ambitious level want bring ideas reinforcement learning (Sutton
& Barto, 1998); let system design workflows systematic way
applied collection available datasets order derive even better characterizations
workflow space relate dataset space, exploring example areas
meta-mining model less confident.

Acknowledgments
work partially supported European Community 7th framework program ICT-2007.4.4 grant number 231519 e- Lico: e-Laboratory Interdisciplinary Collaborative Research Data Mining Data-Intensive Science. Alexandros
Kalousis partially supported RSCO ISNET NFT project. basic HTN planner result collaborative work within e-LICO project Jorg-Uwe Kietz,
Floarea Serban, Simon Fischer. would like thank Jun Wang important contribution developing metric learning part paper. addition would like
thank members AI lab, Adam Woznica, Huyen Do, Jun Wang,
significant effort placed providing content DMOP. Finally, would like
thank reviewers suggestions helped improve paper.

637

fiNguyen, Hilario & Kalousis

Appendix A. Detailed Results
k
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Total

P2
W L p-value
8
7
1
17 11 0.344
24 18 0.440
35 21 0.082
39 23 0.056
41 19 0.006
43 20 0.005
43 22 0.013
47 18 0.000
40 24 0.060
42 21 0.011
40 25 0.082
40 25 0.082
43 21 0.008
40 25 0.082
40 25 0.082
40 25 0.082
40 25 0.082
38 27 0.214
38 27 0.214
39 25 0.104
38 27 0.214
41 24 0.047
40 24 0.060
39 26 0.136
41 23 0.033
42 22 0.017
41 24 0.047
41 24 0.047
44 21 0.006
44 21 0.006
43 21 0.008
42 21 0.011
42 21 0.011
Wins 16/Losses 0

P1
W L p-value
13 13 1
17 20 0.742
18 27 0.233
24 29 0.582
29 31 0.897
33 31 0.900
36 27 0.313
33 31 0.900
35 30 0.619
30 35 0.619
34 29 0.614
33 28 0.608
38 26 0.169
38 26 0.169
37 27 0.260
35 29 0.531
35 29 0.531
34 30 0.707
37 26 0.207
35 30 0.619
37 27 0.260
35 29 0.531
36 28 0.381
36 28 0.381
36 29 0.456
38 27 0.214
38 26 0.169
39 26 0.136
39 25 0.104
40 24 0.060
42 23 0.025
41 24 0.047
39 25 0.104
40 24 0.060
Wins 2/Losses 0

Metric
W L p-value
4
11 0.121
12 16 0.570
19 27 0.302
23 27 0.671
26 35 0.305
27 37 0.260
30 34 0.707
30 33 0.801
29 35 0.531
26 38 0.169
33 31 0.900
32 33 1.000
34 31 0.804
34 30 0.707
34 31 0.804
32 32 1.000
34 31 0.804
32 33 1.000
31 32 1.000
30 35 0.619
31 34 0.804
32 33 1.000
33 31 0.900
33 32 1.000
31 34 0.804
32 32 1.000
35 29 0.531
35 30 0.619
37 28 0.321
39 26 0.136
39 26 0.136
38 27 0.214
37 27 0.260
36 28 0.381
Wins 0/Losses 0

Eucl
W L p-value
9
11 0.823
16 15 1.000
25 19 0.450
29 25 0.683
32 27 0.602
35 25 0.245
31 30 1.000
32 32 1.000
33 31 0.900
30 35 0.619
27 37 0.260
28 36 0.381
33 31 0.900
32 33 1.000
30 35 0.619
31 34 0.804
33 32 1.000
32 33 1.000
32 33 1.000
28 37 0.321
30 35 0.619
30 35 0.619
32 33 1.000
31 34 0.804
30 35 0.619
29 35 0.531
29 35 0.531
29 36 0.456
30 35 0.619
31 34 0.804
32 33 1.000
32 33 1.000
32 33 1.000
31 34 0.804
Wins 0/Losses 0

Table 4: Wins/Losses respective P-values McNemars test number
times Kendal similarity method better Kendal similarity
default, Scenario 1. bold, winning p-value lower 0.05.

638

fiUsing Meta-mining Support DM Workflow Planning Optimization

k
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Total

P2
Avg.Acc W L p-value
0.798
39 20 0.019
0.793
41 21 0.015
0.792
41 21 0.015
0.789
43 21 0.008
0.786
39 24 0.077
0.784
38 25 0.130
0.782
41 24 0.047
0.780
36 26 0.253
0.778
41 20 0.010
0.777
36 27 0.313
0.777
44 20 0.004
0.774
38 23 0.073
0.773
38 25 0.130
0.772
40 22 0.030
0.771
43 18 0.002
0.770
40 22 0.030
0.769
40 22 0.030
0.767
36 26 0.253
0.766
42 21 0.011
0.765
34 29 0.614
0.763
39 24 0.077
0.762
38 26 0.169
0.761
37 25 0.162
0.760
37 24 0.124
0.759
39 19 0.012
0.757
33 20 0.099
0.755
32 14 0.012
0.753
33 15 0.014
0.751
32 10 0.001
0.749
21 15 0.404
0.747
8
5
0.579
0.742
18 10 0.185
0.737
2
3
1
Wins 16/Losses 0

P1
Avg.Acc W L p-value
0.788
26 38 0.169
0.786
28 35 0.449
0.785
35 28 0.449
0.785
38 25 0.130
0.786
33 30 0.801
0.783
33 29 0.703
0.782
40 25 0.082
0.781
38 27 0.214
0.778
38 25 0.130
0.777
30 33 0.801
0.777
40 22 0.030
0.775
40 23 0.043
0.774
37 26 0.207
0.772
40 24 0.060
0.771
41 21 0.015
0.770
40 22 0.030
0.769
39 23 0.056
0.768
35 27 0.374
0.767
41 18 0.004
0.765
33 24 0.289
0.763
45 19 0.001
0.762
33 27 0.518
0.761
34 30 0.707
0.760
43 18 0.002
0.758
43 20 0.005
0.757
39 23 0.056
0.754
34 17 0.025
0.751
32 27 0.602
0.748
28 30 0.895
0.746
22 31 0.271
0.744
16 30 0.055
0.741
26 36 0.253
0.737
2
2
1
Wins 6/0 Losses

Metric
Avg.Acc W L p-value
0.786
25 38 0.130
0.784
26 37 0.207
0.783
32 33 1
0.782
33 32 1
0.780
32 31 1
0.779
28 33 0.608
0.779
35 27 0.374
0.778
34 30 0.707
0.776
34 28 0.525
0.775
29 33 0.703
0.774
40 24 0.060
0.773
39 25 0.104
0.771
32 33 1
0.770
33 29 0.703
0.770
40 25 0.082
0.769
38 27 0.214
0.767
36 26 0.253
0.767
29 35 0.531
0.766
38 25 0.130
0.765
36 25 0.200
0.764
42 22 0.017
0.763
35 27 0.374
0.762
36 29 0.456
0.761
45 11 0.001
0.759
41 20 0.010
0.758
38 24 0.098
0.755
35 10 0.000
0.752
33 19 0.071
0.750
34 17 0.025
0.748
22 18 0.635
0.745
13 17 0.583
0.742
19 21 0.874
0.737
2
5
0.449
Wins 4/Losses 0

EC
Avg.Acc W L p-value
0.782
30 32 0.898
0.780
32 32 1
0.778
32 33 1
0.777
34 30 0.707
0.776
30 33 0.801
0.774
30 35 0.619
0.773
30 34 0.707
0.773
31 33 0.900
0.773
31 34 0.804
0.772
26 37 0.207
0.772
31 33 0.900
0.772
33 30 0.801
0.771
31 34 0.804
0.770
30 33 0.801
0.769
34 30 0.707
0.767
33 31 0.900
0.767
32 31 1
0.766
30 35 0.619
0.765
37 28 0.321
0.764
33 30 0.801
0.762
32 30 0.898
0.761
30 32 0.898
0.760
30 32 0.898
0.758
37 26 0.207
0.757
37 22 0.068
0.756
33 26 0.434
0.754
31 19 0.119
0.751
32 24 0.349
0.749
25 28 0.783
0.747
20 28 0.312
0.745
12 25 0.048
0.742
13 18 0.472
0.737
2
3
1
Wins 0/ Losses 0

Def
Avg.Acc
0.786
0.785
0.778
0.777
0.780
0.778
0.775
0.775
0.774
0.774
0.771
0.771
0.771
0.769
0.766
0.765
0.765
0.766
0.763
0.763
0.761
0.761
0.760
0.756
0.756
0.756
0.753
0.751
0.749
0.748
0.747
0.742
0.737

Table 5: Average Accuracy, Wins/Losses, respective P-values McNemars test
number times Average Accuracy method better Average
Accuracy default, Scenario 1. bold, winning p-value lower
0.05.

639

fiNguyen, Hilario & Kalousis

k
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Total

P2
W L p-value
9
24 0.014
15 28 0.067
25 32 0.426
28 34 0.525
33 30 0.801
36 29 0.456
38 27 0.214
40 25 0.082
43 22 0.013
43 22 0.013
40 25 0.082
41 24 0.047
43 22 0.013
43 22 0.013
40 25 0.082
42 23 0.025
40 25 0.082
40 25 0.082
40 25 0.082
39 26 0.136
39 26 0.136
38 27 0.214
38 27 0.214
40 25 0.082
40 25 0.082
38 27 0.214
38 27 0.214
38 27 0.214
37 28 0.321
35 30 0.619
35 30 0.619
30 35 0.619
29 36 0.456
29 36 0.456
Wins 4/Losses 0

P1
W L p-value
8
11 0.646
13 13 1.000
17 18 1.000
21 25 0.658
24 29 0.582
32 28 0.698
34 26 0.366
34 29 0.614
35 30 0.619
33 32 1.000
34 31 0.804
32 32 1.000
34 31 0.804
36 29 0.456
35 30 0.619
36 29 0.456
35 30 0.619
35 30 0.619
36 29 0.456
36 28 0.381
37 28 0.321
35 30 0.619
35 30 0.619
34 31 0.804
34 31 0.804
35 30 0.619
34 30 0.707
34 31 0.804
33 32 1.000
33 32 1.000
33 32 1.000
33 32 1.000
34 31 0.804
32 33 1.000
Wins 0/Losses 0

Table 6: Wins/Losses P-values McNemars test number times Kendal
similarity method better Kendal similarity default, Scenario
2. bold, winning p-value lower 0.05.

640

fiUsing Meta-mining Support DM Workflow Planning Optimization

k
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Total

P2
Avg.Acc W L
0.797
39 24
0.792
44 20
0.790
34 29
0.787
37 27
0.785
37 27
0.783
36 28
0.782
35 28
0.781
38 26
0.779
38 25
0.777
34 30
0.775
34 30
0.774
35 28
0.773
35 29
0.773
33 32
0.772
32 33
0.770
19 44
0.769
26 39
0.768
24 37
0.768
25 39
0.767
24 39
0.767
26 38
0.766
23 40
0.765
23 42
0.763
20 45
0.762
14 51
0.762
15 50
0.761
16 48
0.760
18 47
0.759
18 47
0.757
18 47
0.756
17 48
0.756
16 49
0.755
13 51
Wins 1/Losses

p-value
0.077
0.004
0.614
0.260
0.260
0.381
0.449
0.169
0.130
0.707
0.707
0.449
0.531
1.000
1.000
0.002
0.136
0.124
0.104
0.077
0.169
0.043
0.025
0.002
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
13

P1
Avg.Acc W L p-value
0.789
29 34 0.614
0.784
33 31 0.900
0.785
31 34 0.804
0.782
32 32 1.000
0.783
33 32 1.000
0.783
32 33 1.000
0.783
30 34 0.707
0.784
31 33 0.900
0.782
35 29 0.531
0.780
36 29 0.456
0.779
38 27 0.214
0.779
38 26 0.169
0.777
37 27 0.260
0.776
34 31 0.804
0.774
31 33 0.900
0.773
26 38 0.169
0.772
27 37 0.260
0.772
28 37 0.321
0.770
23 42 0.025
0.770
24 40 0.060
0.769
29 36 0.456
0.768
26 38 0.169
0.768
29 36 0.456
0.767
24 40 0.060
0.768
22 42 0.017
0.767
24 41 0.047
0.767
32 32 1.000
0.766
33 31 0.900
0.766
39 26 0.136
0.765
35 29 0.531
0.765
33 33 1.000
0.764
31 34 0.804
0.763
27 38 0.214
Wins 0/Losses 3

Def
Avg.Acc
0.786
0.780
0.784
0.778
0.778
0.779
0.778
0.776
0.773
0.772
0.770
0.770
0.770
0.770
0.771
0.773
0.772
0.771
0.770
0.771
0.769
0.768
0.767
0.768
0.769
0.768
0.766
0.764
0.763
0.764
0.764
0.764
0.764

Table 7: Avg.Acc., Wins/Losses, respective P-values McNemars test
number times Average Accuracy method better Average
Accuracy default, Scenario 2. bold, winning p-value lower
0.05.

641

fiNguyen, Hilario & Kalousis

References
Bernstein, A., Provost, F., & Hill, S. (2005). Toward intelligent assistance data mining
process: ontology-based approach cost-sensitive classification. Knowledge
Data Engineering, IEEE Transactions on, 17 (4), 503518.
Bose, R. J. C., & der Aalst, W. M. V. (2009). Abstractions process mining: taxonomy
patterns. Proceedings 7th International Conference Bussiness Process
Management.
Brazdil, P., Giraud-Carrier, C., Soares, C., & Vilalta, R. (2008). Metalearning: Applications
Data Mining (1 edition). Springer Publishing Company, Incorporated.
Bringmann, B. (2004). Matching frequent tree discovery. Proceedings Fourth
IEEE International Conference Data Mining (ICDM04, pp. 335338.
Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., & Wirth, R.
(2000). Crisp-dm 1.0 step-by-step data mining guide. Tech. rep., CRISP-DM
consortium.
Fagin, R., Kumar, R., & Sivakumar, D. (2003). Comparing top k lists. Proceedings
fourteenth annual ACM-SIAM symposium Discrete algorithms, SODA 03, pp.
2836, Philadelphia, PA, USA. Society Industrial Applied Mathematics.
Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996). data mining knowledge
discovery databases. AI magazine, 17 (3), 37.
Forbes, J., & Andre, D. (2000). Practical reinforcement learning continuous domains.
Tech. rep., Berkeley, CA, USA.
Gil, Y., Deelman, E., Ellisman, M., Fahringer, T., Fox, G., Gannon, D., Goble, C., Livny,
M., Moreau, L., & Myers, J. (2007). Examining challenges scientific workflows.
Computer, 40 (12), 2432.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. H. (2009).
weka data mining software: update. SIGKDD Explor. Newsl., 11 (1), 1018.
Hilario, M. (2002). Model complexity algorithm selection classification. Proceedings 5th International Conference Discovery Science, DS 02, pp. 113126,
London, UK, UK. Springer-Verlag.
Hilario, M., & Kalousis, A. (2001). Fusion meta-knowledge meta-data casebased model selection. Proceedings 5th European Conference Principles
Data Mining Knowledge Discovery, PKDD 01, pp. 180191, London, UK, UK.
Springer-Verlag.
Hilario, M., Kalousis, A., Nguyen, P., & Woznica, A. (2009). data mining ontology
algorithm selection meta-learning. Proc ECML/PKDD09 Workshop
Third Generation Data Mining: Towards Service-oriented Knowledge Discovery.
Hilario, M., Nguyen, P., Do, H., Woznica, A., & Kalousis, A. (2011). Ontology-based metamining knowledge discovery workflows. Jankowski, N., Duch, W., & Grabczewski,
K. (Eds.), Meta-Learning Computational Intelligence. Springer.
642

fiUsing Meta-mining Support DM Workflow Planning Optimization

Ho, T. K., & Basu, M. (2002). Complexity measures supervised classification problems.
IEEE Trans. Pattern Anal. Mach. Intell., 24 (3), 289300.
Ho, T. K., & Basu, M. (2006). Data complexity pattern recognition. Springer.
Hoffmann, J. (2001). Ff: fast-forward planning system. AI magazine, 22 (3), 57.
Kalousis, A. (2002). Algorithm Selection via Metalearning. Ph.D. thesis, University
Geneva.
Kalousis, A., Gama, J., & Hilario, M. (2004). data algorithms: Understanding
inductive performance. Machine Learning, 54 (3), 275312.
Kalousis, A., & Theoharis, T. (1999). Noemon: Design, implementation performance
results intelligent assistant classifier selection. Intell. Data Anal., 3 (5), 319
337.
Kietz, J.-U., Serban, F., Bernstein, A., & Fischer, S. (2009). Towards Cooperative Planning Data Mining Workflows. Proc ECML/PKDD09 Workshop Third
Generation Data Mining: Towards Service-oriented Knowledge Discovery (SoKD-09).
Kietz, J.-U., Serban, F., Bernstein, A., & Fischer, S. (2012). Designing kdd-workflows via
htn-planning intelligent discovery assistance. 5th PLANNING LEARN
WORKSHOP WS28 ECAI 2012, p. 10.
King, R. D., Feng, C., & Sutherland, A. (1995). Statlog: Comparison classification
algorithms large real-world problems. Applied Artificial Intelligence, 9 (3), 289
333.
Klinkenberg, R., Mierswa, I., & Fischer, S. (2007). Free data mining software: Rapidminer
4.0 (formerly yale). http://www.rapid-i.com/.
Kopf, C., Taylor, C., & Keller, J. (2000). Meta-analysis: data characterisation
meta-learning meta-regression. Proceedings PKDD-00 Workshop Data
Mining, Decision Support,Meta-Learning ILP.
Kramer, S., Lavrac, N., & Flach, P. (2000). Relational data mining.. chap. Propositionalization Approaches Relational Data Mining, pp. 262286. Springer-Verlag New
York, Inc., New York, NY, USA.
Leite, R., & Brazdil, P. (2010). Active testing strategy predict best classification algorithm via sampling metalearning. Proceedings 2010 conference ECAI
2010: 19th European Conference Artificial Intelligence, pp. 309314, Amsterdam,
Netherlands, Netherlands. IOS Press.
McDermott, D., Ghallab, M., Howe, A., Knoblock, C., Ram, A., Veloso, M., Weld, D., &
Wilkins, D. (1998). Pddl-the planning domain definition language..
Michie, D., Spiegelhalter, D. J., Taylor, C. C., & Campbell, J. (1994). Machine learning,
neural statistical classification..
Nguyen, P., Kalousis, A., & Hilario, M. (2011). meta-mining infrastructure support kd
workflow optimization. Proc ECML/PKDD11 Workshop Planning Learn
Service-Oriented Knowledge Discovery, 1.
643

fiNguyen, Hilario & Kalousis

Nguyen, P., Kalousis, A., & Hilario, M. (2012a). Experimental evaluation e-lico
meta-miner. 5th PLANNING LEARN WORKSHOP WS28 ECAI 2012,
p. 18.
Nguyen, P., Wang, J., Hilario, M., & Kalousis, A. (2012b). Learning heterogeneous similarity
measures hybrid-recommendations meta-mining. IEEE 12th International
Conference Data Mining (ICDM), pp. 1026 1031.
Peng, Y., Flach, P. A., Soares, C., & Brazdil, P. (2002). Improved dataset characterisation
meta-learning. Discovery Science, pp. 141152. Springer.
Pfahringer, B., Bensusan, H., & Giraud-Carrier., C. (2000). Meta-learning landmarking various learning algorithms.. Proc. 17th International Conference Machine
Learning, 743750.
R Core Team (2013). R: language environment statistical computing. http:
//www.R-project.org/.
Smart, W. D., & Kaelbling, L. P. (2000). Practical reinforcement learning continuous
spaces. Proceedings Seventeenth International Conference Machine Learning, ICML 00, pp. 903910, San Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Soares, C., & Brazdil, P. (2000). Zoomed ranking: Selection classification algorithms based
relevant performance information. Proceedings 4th European Conference
Principles Data Mining Knowledge Discovery, PKDD 00, pp. 126135,
London, UK. Springer-Verlag.
Srebro, N., Rennie, J. D. M., & Jaakkola, T. S. (2005). Maximum-margin matrix factorization. Saul, L. K., Weiss, Y., & Bottou, L. (Eds.), Advances Neural Information
Processing Systems 17, pp. 13291336. MIT Press, Cambridge, MA.
Sutton, R., & Barto, A. (1998). Reinforcement learning: introduction. Neural Networks,
IEEE Transactions on, 9 (5), 1054.
Van der Aalst, W. M., & Giinther, C. (2007). Finding structure unstructured processes:
case process mining. Application Concurrency System Design, 2007.
ACSD 2007. Seventh International Conference on, pp. 312. IEEE.
Yang, Q., & Wu, X. (2006). 10 challenging problems data mining research. International
Journal Information Technology & Decision Making, 5 (04), 597604.
Zaki, M. J. (2005). Efficiently mining frequent trees forest: Algorithms applications.
IEEE Transactions Knowledge Data Engineering, 17 (8), 10211035. special
issue Mining Biological Data.
Zakova, M., Kremen, P., Zelezny, F., & Lavrac, N. (2011). Automating knowledge discovery workflow composition ontology-based planning. Automation Science
Engineering, IEEE Transactions on, 8 (2), 253264.

644

fiJournal Artificial Intelligence Research 51 (2014) 207-226

Submitted 1/14; published 09/14

Sensitivity Diffusion Dynamics Network Uncertainty
Abhijin Adiga
Chris J. Kuhlman

abhijin@vbi.vt.edu
ckuhlman@vbi.vt.edu

Network Dynamics Simulation Science Laboratory,
Virginia Bioinformatics Institute,
Virginia Tech, VA 24061

Henning S. Mortveit

hmortvei@vbi.vt.edu

Network Dynamics Simulation Science Laboratory,
Virginia Bioinformatics Institute,
Department Mathematics,
Virginia Tech, VA 24061

Anil Kumar S. Vullikanti

akumar@vbi.vt.edu

Network Dynamics Simulation Science Laboratory,
Virginia Bioinformatics Institute,
Department Computer Science,
Virginia Tech, VA 24061

Abstract
Simple diffusion processes networks used model, analyze predict
diverse phenomena spread diseases, information memes. often
not, underlying network data noisy sampled. prompts following natural
question: sensitive diffusion dynamics subsequent conclusions uncertainty
network structure?
paper, consider two popular diffusion models: Independent cascade (IC) model
Linear threshold (LT) model. study expected number vertices
influenced/infected, particular initial conditions, affected network perturbations.
rigorous analysis assumption reasonable perturbation model
establish following main results. (1) IC model, characterize sensitivity
network perturbation terms critical probability phase transition network.
find expected number infections quite stable, unless transmission
probability close critical probability. (2) show standard LT model
uniform edge weights relatively stable network perturbations. (3) study
sensitivity questions using extensive simulations diverse real world networks
find theoretical predictions models match observations quite closely.
(4) Experimentally, transient behavior, i.e., time series number infections,
models appears sensitive network perturbations.

1. Introduction
number diverse phenomena modeled simple diffusion processes graphs,
spread epidemics (Newman, 2003), viral marketing (Kempe, Kleinberg, & Tardos,
2005; Goldenberg, Libai, & Muller, 2001) memes online social media (Romero, Meeder,
& Kleinberg, 2011; Bakshy, Hofman, Mason, & Watts, 2011). common associate
vertex state 0 (denoting infected influenced) state 1 (denoting
c
2014
AI Access Foundation. rights reserved.

fiAdiga, Kuhlman, Mortveit & Vullikanti

infected influenced) models; node state 0 switches state 1 based
probabilistic rule nodes state 1 remain state 1. focus two
models, referred independent cascade (IC) model (which special case SIR
process), linear threshold (LT) model. applications, however, underlying
networks inherently noisy incomplete, since often inferred indirect
measurements, instance: (i) networks based Twitter data (Gonzlez-Bailn, BorgeHolthoefer, Rivero, & Moreno, 2011; Bakshy et al., 2011; Galuba, 2010) constructed
limited samples available public APIs, (ii) biological networks inferred
experimental correlations (Hagmann, 2008; Schwab, Bruinsma, Feldman, & Levine, 2010),
might incomplete, (iii) Internet router/AS level graphs constructed
using traceroutes (Faloutsos, Faloutsos, & Faloutsos, 1999), known give biased
incomplete structure (Achlioptas, Clauset, Kempe, & Moore, 2005).
raises fundamental issue diffusion processes networks:
uncertainty network affect conclusions drawn study diffusion
dynamics network? instance, robust inference
large outbreak network, face noise/uncertainty network? Recent
statistical simulation-based studies involving perturbation network rewiring
pairs edges (which preserves degree sequence) show changes network
structure significantly alter dynamics even aggregate structural properties
degree distribution assortativity preserved (Eubank, 2010; Chen, 2010).
Surprisingly, limited mathematically rigorous work explain empirical findings
systematic manner, despite large body research diffusion models.
work motivated considerations sensitivity dynamics noise
adequacy sampling network G = (V, E). Since limited understanding
noise modeled, consider two simple random edge perturbation models
noise: uniform degree-assortative. uniform perturbation, pair u, v vertices
selected edge addition deletion (or both) probability n , > 0
parameter,
n number vertices; thus, average, perturbed graph differs

n
n
n 2 2 edges. model used quite extensively social network analysis
computer science understanding sensitivity graph properties (Costenbader
& Valente, 2003; Borgatti, Carley, & Krackhardt, 2006; Flaxman & Frieze, 2004; Flaxman,
2007). study expected number infections, given initial conditions,
affected magnitude perturbation parameter, . degree-assortative
perturbation, probability edge modification proportional product
degrees end points G.

1.1 Contributions
results obtain assumption uniform edge addition model, i.e.,
edges absent network added probability n obtain perturbed network.
Later, Section 5, compare addition model addition/deletion model
also discuss results degree-assortative perturbation. describe independent
cascade linear threshold models Section 2.2.
208

fiSensitivity Diffusion Dynamics Network Uncertainty

1.1.1 Independent Cascade Model
consider networks G exhibit phase transition infection sizes, critical
transition probability pc (see Section 2 definitions). Theorem 1, characterize
expected number infections perturbed graph terms transmission probability p
pc single random seed node. p < pc , show exists threshold
positive constant c < 1: (i) < (1 c)t , addition, pc
average degree davg satisfy technical condition, then, expected number infections
perturbed graph remains close G and, (ii) > (1 + c)t , phase
transition, expected number infections perturbation much larger
G. main implication dynamics quite robust perturbations, unless
transmission probability close pc , critical value. find
consistent extensive simulations large number real networksthe sensitivity
perturbations maximized point approximately matches experimentally
determined threshold many networks. also examine transient behavior (i.e.,
time series number infections) studying particular time magnitude
peak number new infections affected uncertainty. find measures
sensitive expected total number infections.
1.1.2 Linear Threshold Model
Theorem 2, show formally network G maximum degree =
O(n/ log n), expected number infections perturbation, starting random
initial infections, bounded O(s( + + log n) log n). implies dynamics
quite stable low . result based analysis random graph
model node selects random in-edge. shown correspond
LT model (Kempe, Kleinberg, & Tardos, 2003). first show diameter bounded
O(0 log n), 0 maximum degree perturbed graph, prove
expected number infections, starting random source, bounded
diameter. theoretical bounds corroborate well experimental observations
large set real networks, show gradual variation . find expected
number infections grows sharply , number sources increased.
1.1.3 Discussion Implications
point view dynamical system theory, work may regarded study
stability dynamics network respect edge structure. existence
critical value parameter IC model thought bifurcation point.
Admittedly, results hold specific random edge perturbation model noise;
uncertainty networks much complex process, might involve dependencies
arising network evolution. Although focus specific dynamical properties
random edge perturbation model, results give first rigorous theoretical
empirical analysis noise susceptibility diffusion models. Further,
analytical empirical techniques, based random graph characterization, likely
help analysis complex noise models, take dependencies
account.
209

fiAdiga, Kuhlman, Mortveit & Vullikanti

1.2 Related Work
Noise issues sampling well recognized fundamental challenges complex
networks, work characterizing sensitivity different
parameters, especially network properties. works (Costenbader & Valente, 2003;
Borgatti et al., 2006), certain centrality measures shown robust random edge
node perturbations, another (Achlioptas et al., 2005), shown
inherent bias traceroute-based inference Internet router network, might
give incorrect degree distributions. Flaxman Frieze (Flaxman & Frieze, 2004; Flaxman,
2007) formally characterize conditions graph expansion diameter
highly sensitive random edge additions; among analytical results
type. approaches address noise include: (i) prediction missing links
using clustering properties (Clauset, Moore, & Newman, 2008), (ii) property testing
algorithms (Ron, 2010) smoothed analysis (Spielman, 2009) efficient computation
graph properties.
knowledge, work sensitivity graph dynamical systems noise
network empirical. However, regular networks rings, topics
synchronization bifurcations studied (Kaneko, 1985; Wu, 2005). discussed
earlier, effects changes network edge rewirings epidemic properties
investigated (Eubank, 2010; Chen, 2010). effect stochastic changes network
influence maximization problems studied (Lahiri, Maiya, Caceres, Habiba, & Berger-Wolf,
2008). Using simulations, find LT model, spread size quite robust;
techniques help explain observations.
1.2.1 Organization
Section 2, introduce notation describe noise models diffusion models
detail. Sections 3 4, analyze sensitivity IC LT models, respectively.
Experimental results presented Section 5, conclude Section 6.

2. Preliminaries
consider undirected, simple networks. network G = (V, E), let denote
maximum degree davg , average degree. vertex v, deg(v, G) N (v) denote
degree set neighbors respectively. Let correspond largest eigenvalue
adjacency matrix G. say event A(n) occurs asymptotically almost surely
(a.a.s.) P (A(n)) 1 n .
2.1 Noise Models
Since consensus best way model uncertainty noise, consider
two simple models random edge modifications: (i) uniform (ii) degree-assortative
perturbations. Uniform perturbation studied quite extensively social network
analysis (Costenbader & Valente, 2003; Borgatti et al., 2006); problems also
studied analytically model (Flaxman & Frieze, 2004; Flaxman, 2007). Let
G = (V, E) unperturbed graph; graphs work undirected simple.
Let Ru () = (V, E()) random graph V pair u, v V connected
210

fiSensitivity Diffusion Dynamics Network Uncertainty

probability n . analysis, consider perturbations involving addition
edges: denoted G + Ru (), consists edges (u, v) E E(Ru ()).
experimental studies, also considered addition/deletion edges. case,
perturbation graph G0 = G Ru () graph constructed following manner:
pair u, v V connected G0 (u, v) Ru () E (u, v) E Ru (). words,
pair u, v selected addition/deletion probability n . degree-assortative
perturbation
random graph Rd () u, v V adjacent

model, consider
deg(u,G) deg(v,G)
probability
n , i.e., edge probability proportional product
d2avg
degrees end points G. models, expected number edge modifications
approximately n
2 .
2.2 Network Diffusion Models
Let G = (V, E) denote undirected network. models study, vertex v V
state xv {0, 1}, state 0 denoting inactive/uninfected/uninfluenced
state 1 denoting active/infected/influenced, depending application. restrict
monotone progressive processes, i.e., infected node stays infected.
node associated activation function whose inputs include states neighbors.
function computes next state node. diffusion process starts
vertices set active/infected; refer set initial set seed set.
initial set active nodes S, let (S) denote expected number active nodes
termination. models always reach fixed points. consider following models:
(1) Independent Cascade (IC) Model (Kempe et al., 2003): model special case
SIR model epidemics. infected node v infects neighbor w probability
p (referred transmission probability). Equivalently, edge (v, w)
live probability p, independently edges. nodes
connected initial set live path considered infected. graph,
let (v, x) edge. Suppose v gets infected time t, x state 0. v
tries infect x probability p time + 1. Irrespective whether x gets infected
v, v remains state 1 subsequent times, never tries infect x.
(2) Linear Threshold (LT) Model (Kempe et al., 2005): node v threshold
v [0, 1], chosen uniformly random. Node v influenced neighbor w
P
according weight bv,w wN (v) bv,w 1. Node v becomes infected
P
wA(v) bv,w v , A(v) N (v) set neighbors v currently
infected. analysis experiments, assume bv,w = 1/ deg(v, G)
w N (v), deg(v, G) degree v G. means v influenced
equally neighbors. model considered (Kempe et al., 2003).
perturbed graph G0 = G + Ru (), bv,w = 1/ deg(v, G0 ), deg(v, G0 ) new
degree v.

3. Analyzing Sensitivity IC Model
section, rigorously analyze sensitivity IC model edge perturbations.
mentioned earlier, restrict attention uniform edge addition model, i.e.,
211

fiAdiga, Kuhlman, Mortveit & Vullikanti

perturbed graph obtained adding edge every pair vertices probability
/n.
stating main result, discuss aspects IC model develop
notation used analysis. transmission probability p, let G(p)
denote random subgraph G obtained retaining edges G probability p.
relationship infection spread IC model structure G(p) well-known;
question whether large spread occurs G equivalent asking
exists giant component G(p). Another interesting aspect IC model exhibits
threshold phenomenon. many graph families exists critical transmission
probability pc abrupt phase transition occurs small spread p < pc
large one p > pc high probability. formal definition pc follows (see (Bollobs,
1985) details).
Definition 3.1. graph G n nodes, pc critical transmission probability
starting single random initial infection, transmission probability p pc
total number infections a.a.s. o(n) equivalently, components G(p)
size o(n), p pc , number infections a.a.s. (n) exists giant
component G(p).
Recall Section 2 notion asymptotically almost surely formally defined
sequence graphs. However, state explicitly, order reduce notational
overload. state main result section analyze sensitivity
IC model graph operating probability p < pc satisfying pdavg 1 where,
davg average degree G. conclude discussion implications
theorem.
Theorem 1. Consider graph G n nodes average degree davg critical transmis
sion probability pc . Let transmission probability p satisfy pdavg = o(1) p = 1/n1
constant . Let G + Ru () perturbed graph obtained adding edges uniformly
random factor . Then, single seed node chosen uniformly random,
following hold:
(a) number infections G o(n) a.a.s. therefore, p < pc .
(b) pdavg = 1/ log2 n , then, exists threshold perturbation factor = p1
positive constant c < 1, (1 c)t , number infections G + Ru ()
transmission probability p a.a.s. o(n) (1 + c)t , number
infections (n).


(c) positive constant c,
(n).

1+c
p ,

number infections G + Ru () a.a.s.

Proof. Let G0 = G + Ru () let G0 (p) random subgraph G0 obtained choosing
edges probability p. Since, edge e
/ E(G), Pr(e G0 (p)) = p Pr(e Ru ()) =
p
0
n , follows G (p) obtained adding edges every pair nodes
G(p) probability p
n . Now, prove Statement (a).
p



Claim 1. number components G(p) one node n pdavg a.a.s.
therefore, G(p) giant component.
212

fiSensitivity Diffusion Dynamics Network Uncertainty

Proof. use Chebychevs inequality.


x> davg , number nodes degree
davg

greater x G n x+1 < n avg
. Let niso denote number isolated
x
nodes G(p). Then,
E[niso ] =

X

X

Pr(v isolated)

vV (G)

Pr(v isolated)

vV (G),deg(v,G)x

davg
(1 p)x n 1
x




q

davg
> (1 px)n 1
.
x




2

p

p



Choosing x davg /p, E[niso ] 1 pdavg n 1 2 pdavg n. Since assumption, davg = o(1/p), E[niso ] = (1 o(1))n. node v V (G), let Iv = 1 v
neighbors G(p) 0 otherwise. Let Var[] Cov[] denote variance covariance,
respectively. Using bounds Var[Iv ] E[Iv ] Cov[Ia Ib ] = P (Ia Ib ) 1,
Var[niso ] =

X

X

Var[Iv ] + 2

vV (G)

Cov[Ia Ib ]

(a,b)E(G)

X

E[niso ] + 2

Cov[Ia Ib ] E[niso ] + ndavg = O(ndavg ) .

(a,b)E(G)

Since assumed p = 1/n1 , follows Var[niso ] = n2 pdavg .
Applying Chebychevs inequality,


q









P |niso E[niso ]| > n pdavg P |niso E[niso ]| >

q



n Var[niso ]

p

Therefore, a.a.s., n niso 3n davg p.

1
.
n


Let {Ci | N } set connected components G(p), N number
components let ni denote size Ci . probability components Ci Cj
n n p
connected least one edge Ru () G0 (p) nj . Let Siso denote
set isolated nodes G(p) iso set remaining nodes.
Let H graph obtained adding special vertex v G0 (p) making adjacent
nodes iso . Clearly, iso belongs component H component G0 (p)
contained component H. implies G0 (p) giant component H
one. Now, prove first part Statement (b).
Claim 2. H components size o(n) pdavg = 1/ log2 n


(1c)
p .

Proof. Let H[Siso ] H[S iso ] graphs induced Siso iso , respectively, H. Note
since Siso set isolated nodes G(p), H[Siso ] Erds-Rnyi graph |Siso |
p
nodes edge probability |Sp
= n(1o(1))
. Since p < 1 c, follows H[Siso ]
iso |
components size O(log n) (see, e.g., Bollobs, 1985). show component
containing iso size o(n) a.a.s., thus completing proof.
Let N (S iso ) denote size neighborhood iso H. probability
node Siso neighbor iso
h

|S iso |p
.
n



E N (S iso ) |Siso |
213

Therefore,

|S iso |p
|S iso |p.
n

fiAdiga, Kuhlman, Mortveit & Vullikanti

Applying version Chernoff bound (Chung & Lu, 2002), following:



h



2






.
P N (S iso ) E N (S iso ) > exp h
2 E N (S iso ) + /3

two regimes consider: (i) |S iso |p = (1) (ii) |S iso |p = O(1). |S iso |p =
(1), setting = |S iso |p, follows N (S iso ) 2|S iso |p a.a.s. Since H[Siso ]
components size O(log n), size component containing iso




q



|S iso | + N (S iso ) log n = |S iso | + |S iso |p log n = n pdavg (1 + p log n) .
last expression
follows p
Claim 1. Since (1 c)t pdavg = 1/ log2 n ,
p
follows n pdavg p log n n pdavg log n(1 c) = o(n). consider regime (ii):
|S iso |p = O(1). setting = log n, component size log2 n = o(n). Hence,
proved claim.



proofs second part Statement (b) Statement (c) straightforward.
Let Rp denote random subgraph perturbation network Ru () obtained sampling
edges probability p. easy see Rp Erds-Rnyi graph edge
1+c
probability p
n = n implies giant component (Bollobs, 1985).
turn implies G0 (p) giant component. Hence, proved theorem.
Theorem 1 indicates large class networks, closer operate pc ,
sensitive dynamics structural perturbation. indeed true
conditions Statement (b) met: p1 < p2 < pc , (p1 ) > (p2 ). implies
greater perturbation required case p1 (compared p2 ) observe significant
difference expected infection size perturbation. observed
experiments; see last three columns Table 1. AstroPh graph, example,
p = 0.03, phase transition occurs = 8 p = 0.02, greater
20 transition occur.

4. Analyzing Sensitivity LT Model
analyze impact edge perturbations LT model graph G = (V, E).
previous section, restrict attention uniform edge addition model.
Recall specific version LT model consider here, set bv,w = 1/ deg(v)
node v V w N (v).
fixed points number infected nodes studied elegant
random graph model (Kempe et al., 2003) describe here. Construct random
directed graph HLT = (V, E 0 ) following manner: node v V , neighbor w
chosen probability bv,w directed edge added w v. Figure 1 illustrates
graph G instance HLT . Note even though G undirected, HLT directed
graph. set V , let (S, HLT ) denote number nodes reachable HLT
(including S). Then, (S), expected number infections starting set S,
P
satisfies (S) = HLT Pr[HLT ](S, HLT ) (Kempe et al., 2003). use characterization
analyze impact edge perturbations.
214

fiSensitivity Diffusion Dynamics Network Uncertainty

1

1

2

3

2

3

4

5

4

5

6

7

6

7

Figure 1: graph (on left) instance random graph HLT (on right)
corresponding LT model. component induced {1, 2, 3, 4, 5}, 1 chosen
root result, T0 = {1}, T1 = {3}, T2 = {2, 5} T3 = {4}.

random graph HLT constructed process following structure:
connected component HLT , every vertex one incoming edge therefore,
exists exactly one directed cycle. choose vertex cycle root r
remove incoming edge, then, corresponds tree rooted r edges oriented
away r. partitioned sets T0 , . . . , Tk > 0, vertex
v Ti incoming edge vertex u Ti1 . set T0 singleton consisting
root vertex r. incoming edge r neighbor ki=1 Ti .
illustrated Figure 1. First, show following:
Lemma 4.1. LT model, let = minvV,wN (v) bv,w . component random
subgraph HLT depth



1




log n probability least 1

1
.
n3

Proof. Consider component HLT , partitioned sets T0 , . . . , Tk , mentioned above. = 1, . . . , k 1, vertex v Ti would become root chooses
incoming edge one descendants. probability event least
minwN (v) bv,w . Therefore, probability none vertices Ti becomes
root 1 , turn implies probability none vertices
Ti , = 1, . . . , k 1 becomes root (1 )k1 . Hence, probability
P
depth k = 4 1 log n + 1 nk4 1 log n+1 (1 )k1 n14 . Since


n
components HLT , probability depth


1 log n n13 .
Consider vertex v contained component . Let n(v, ) denote number
vertices reachable v . Then, number infections resulting v expected
value n(v, ), averaged random subgraphs HLT components containing v. Let
P
A(T ) = |T1 | vT n(v, ). Conditioned random subgraph HLT , average number
infections starting random source
starting random source

P

HLT

P

A(T ) |Tn | . average number infections

HLT

Pr[HLT ]

P

HLT

A(T ) |Tn | .

Lemma 4.2. component random subgraph HLT , A(T ) 2d,
depth .
215

fiAdiga, Kuhlman, Mortveit & Vullikanti

Proof. Define tree obtained removing incoming edge root .
described above, out-tree. v , define n(v, ) number
vertices reachable v corresponds size subtree rooted v
P
. define A(T ) = |T1 | vT n(v, ), prove A(T ) induction depth
out-tree. base case leaf node u, A(u) = 1.
Let r root . Suppose children v1 , . . . , va . Let Ti subtree rooted
P
vi , let ni number vertices Ti . induction, A(Ti ) = n1i vTi n(v, Ti ) d1.
A(T ) =

1 X
n(v, )
|T |
vT

=


X
1
1 X
n(r, ) +
n(v, Ti )
|T |
i=1 |T |
vTi

= 1+


X
ni
i=1

|T |

A(Ti ) 1 +


X
ni
i=1

|T |

(d 1)

|T | 1
1+
(d 1)
|T |
third equality follows n(r, ) = |T |, definition, A(Ti ) = n1i vTi n(v, Ti ).
first inequality follows induction hypothesis, since depth Ti 1.
P
second inequality follows ai=1 ni = |T | 1.
Next, consider A(T ). recall tree cycle length d. Let
cycle consist vertices u0 = r, u1 , . . . , ub , b 1. ui , n(ui , ) = |T |,
since path ui r. every vertex u 6= ui , n(u, ) = n(u, ).
|
implies A(T ) d|T
|T | + A(T ) 2d.
P

Finally, bound number infections perturbed graph below.
Theorem 2. Let G(V, E) graph maximum degree G + Ru () perturbed
graph obtained adding edges uniformly random factor . LT model
bv,w = 1/ deg(v) node v V w N (v), expected number infected
vertices starting initial random seed set size perturbed graph G + Ru ()
O(s( + + log n) log n).
Proof. direct application Chernoff bound, shown probability
least 1 n13 , maximum degree G0 = G + Ru () + + c log n
constant c remaining probability n13 , maximum degree O(n). consider
random graph process generate subgraph HLT G0 . Since bv,w = 1/ deg(v)
node v V w N (v), model, value Lemma 4.1 1/(G0 )
therefore, component HLT depth O(( + + log n) log n),
probability least 1 n13 . Conditioned HLT satisfying bound depth,
A(T ) = O(( + + log n) log n) HLT . HLT satisfy depth bound,
A(T ) = O(n) HLT . Therefore, expected number infections single
random seed O(( + + log n) log n) + O( nn3 ) = O(( + + log n) log n). result
extends > 1 submodularity expected number infections.
216

fiSensitivity Diffusion Dynamics Network Uncertainty

Theorem 2 implies LT model uniform edge weights general robust
perturbation. Note final part proof Theorem 2 essentially based
maximum degree G0 . Replacing G0 G retracing steps, one show
expected spread unperturbed graph G O(s log n), maximum
degree. Hence, see reasonably high value (say (log n)), difference
total number infections G G0 linear function
therefore abrupt change outcome. Moreover, bound suggests
higher , lower effect perturbation. However, making formal would
require obtaining good lower bounds expected number infections, leave
interesting research question.

5. Experimental Results
study sensitivity edge perturbations twenty diverse real-world networks (Leskovec,
2011) varying degrees perturbation factors IC LT models.
listed Table 1 along properties, first four
number nodes, average degree, maximum eigenvalue, maximum degree.
properties discussed subsequently. present representative results selected
networks, networks exhibiting behavior unless stated otherwise.
focus primarily sensitivity expected number infections transient behavior
edge perturbation. course, observations restricted conditions
experiments performed.
5.1 Experimental Setup Methodology
network G Table 1 perturbed values ranging 0 100,
= 0 corresponds unperturbed network. , generated ten graph instances
G0 = G + R G R. Here, R may Ru Rd , depending whether perturbation
uniform edge approach degree-assortative approach. graph instance,
performed simulation run, consists 100 separate diffusion instances. diffusion
instance process setting node states initially zero, assigning relevant properties
graph entities (e.g., transmission probability edges IC model edge weights
node thresholds LT model), selecting seed node set whose element states
changed 1 (i.e., initially infected), marching time forward discrete units,
continuing simulation fixed point reached. record nodes time
infection diffusion instance. Thus, example, experimental data displayed
average variance quantities based 1000 values.
5.2 Edge Additions vs. Deletions
find perturbations involving edge additions deletions alter
results much, compared perturbations involving edge additions. due
sparsity graphs considered. example, uniform perturbations, expected
number edges deleted |E|/n = davg /2 expected number edges modified
n/2. Therefore, remainder paper focuses perturbation addition edges
only; i.e., graphs form G0 = G + R.
217

fiAdiga, Kuhlman, Mortveit & Vullikanti

Network

n = |V |

davg





pc

(p; ) pairs experiments

Synthetic graphs
Random-Regular-20

10000

20.0

20.0

20

0.05

0.04;9

0.03;>10

0.02;>20

0.2
0.2
0.2

0.15;2
0.15;<2
0.15;<1

0.1;5
0.1;5
0.1;5

0.05;>10
0.05;10
0.05;10

0.04
0.12
0.2
0.1
0.2

0.03;8
0.1;2
0.15;<2
0.07;<2
0.15;<1

0.02;>20
0.08;4
0.1;5
0.04;8
0.1;5

0.01;>40
0.06;8
0.05;10
0.01;>20
0.05;10

0.04
0.05

0.03;6
0.04;<2

0.02;>20
0.03;8

0.01;>40
0.02;>40

0.1
0.3

0.07;4
0.2;<2

0.04;10
0.1;6

0.01;>20
0.05;10

0.15
0.1
0.1
0.2
0.04

0.12;<1
0.07;3
0.07;3
0.15;<2
0.03;3

0.1;2
0.04;9
0.04;9
0.1;4
0.02;10

0.08;4
0.01;>20
0.01;>20
0.05;10
0.01;>20

0.1;2
0.15;1

0.07;7
0.1;6

0.04;>10
0.05;>10

Autonomous Systems
AS-2000-01-02
Oregon1-01-03-31
Oregon2-01-03-31

6474
10670
10900

3.88
4.12
5.72

46.31
58.72
70.74

AstroPh
CondMat
Grqc
HepPh
HepTh

17903
21363
4158
11204
8638

22.0
8.54
6.45
20.99
5.74

94.42
37.88
45.61
244.93
31.03

HepPh
HepTh

34546
27770

24.46
25.37

76.58
111.25

1458
2312
2343

Co-authorship
504
279
81
491
65

Citation
846
2468

Communication
Email-Enron
Email-EuAll

33696
265214

10.02
2.74

118.41
102.53

1383
7636

Social
Epinion
Slashdot0811
Slashdot0902
Twitter
Wiki-Vote

75877
77360
82168
22405
7066

10.69
12.12
12.27
5.34
28.51

184.17
131.34
134.62
54.08
138.15

3044
2539
2552
888
1065

Internet peer-to-peer
Gnutella04
Gnutella24

10876
26518

7.35
4.93

15.7
19.06

103
355

0.125
0.2

Table 1: relevant properties networks used simulations results
experiments.
5.3 Independent Cascade Model
experimental results uniform degree-assortative perturbation,
make observations relating theoretical results behavior
networks general.
5.3.1 Uniform Perturbation
Effect p final infection size. Figure 2 consists plots variation average
variance fraction infected nodes (i) transmission probability p
various levels perturbation (ii) perturbation level various p values three
networks. plots remaining networks full version (Adiga, Kuhlman,
Mortveit, & Vullikanti, 2014). note p > 1 (and p pc ), average infection
size generally high, agreeing Statement (c) Theorem 1. observe
plots final fraction infections least 0.5 p > 1.

218

fiSensitivity Diffusion Dynamics Network Uncertainty

Average Final Size
Variance Final Size

Average Final Size
Variance Final Size

Average Final Size

1.0 Random-Regular-20
0.8
0.6
=0
0.4
=0.5
=1
0.2
=5
=10
0.0
0.00 0.05 0.10 0.15 0.20
Probability, p
Random-Regular-20
0.12
=0
=0.5
0.10
=1
0.08
=5
=10
0.06
0.04
0.02
0.000.0 0.2 0.4 0.6 0.8 1.0
Probability, p
Random-Regular-20
1.0
0.8
0.6
p =0.02
0.4
p =0.03
p =0.04
0.2
p =0.05
p =0.06
0.00 20 40 60 80
100
Perturbation,

Average Final Size

Average Final Size
Variance Final Size

Wiki-Vote
1.0
0.8
0.6
=0
0.4
=0.5
=1
0.2
=5
=10
0.0
0.00 0.05 0.10 0.15 0.20
Probability, p
Wiki-Vote
0.12
=0
=0.5
0.10
=1
0.08
=5
=10
0.06
0.04
0.02
0.000.0 0.2 0.4 0.6 0.8 1.0
Probabilty, p
Wiki-Vote
1.0
0.8
0.6
p =0.01
0.4
p =0.02
p =0.03
0.2
p =0.04
p =0.05
0.00 20 40 60 80
100
Perturbation,

Average Final Size

AS-2000-01-02
1.0
0.8
0.6
=0
0.4
=0.5
=1
0.2
=5
0.00.0 0.2 0.4 0.6 =100.8
Probability, p
AS-2000-01-02
0.12
=0
=0.5
0.10
=1
0.08
=5
=10
0.06
0.04
0.02
0.000.0 0.2 0.4 0.6 0.8 1.0
Probability, p
AS-2000-01-02
1.0
0.8
0.6
p =0.05
0.4
p =0.10
p =0.15
0.2
p =0.20
p =0.25
0.00 20 40 60 80
100
Perturbation,

Figure 2: Uniform perturbation (IC model): Average variance fraction infections
single random seed (i) vs. transmission probability p various values (ii) vs.
various values p. plots first third rows average final fraction
infected nodes second row average variance final fraction infected
nodes. remaining plots full version (Adiga et al., 2014).

sensitivity final infection size modulated davg . first consider
relationship pc davg unperturbed networks. Since finite networks,
clear definition pc , chose value p average total
number infections 10% number nodes network. Table 1 contains pc
network. dependence pc davg shown Figure 3 data point
corresponds one graph colored according type graph. Clearly, plot
indicating inverse relationship two values. implies higher
davg , lower p spread small. Note Theorem 1 shows
minimum large spread inversely proportional p. strongly
suggests greater edge density, greater perturbation required achieve
significant change dynamics. supported experiments, too. example, see
plots AS-2000-01-02 Wiki-Vote first row Figure 2. Consider p = 0.2
note change spread going = 0 10. Wiki-Vote, goes 0.38 0.89,
219

fiAdiga, Kuhlman, Mortveit & Vullikanti

ratio 2.3. AS-2000-01-02, changes 0.08 0.82, ratio 10.3. Thus,
AS-2000-01-02, factor 7 smaller davg Wiki-Vote, much greater sensitivity
spread changes . Note numbers nodes two graphs comparable.
see behavior (high davg , low davg ) network pairs: (AstroPh, CondMat),
(HepPh, HepTh), (Email-Enron, Email-EuAll).

Critical Probability

0.5
0.4
0.3
0.2
0.1
0.00

auton.
co-auth.
citation
comm.
social
internet

10 20 30
Average Degree

40

Figure 3: Dependence pc davg . Data graphs Table 1, colors correspond
graph types table.

Variance final infection size. Figure 2 (and plots full version Adiga
et al., 2014), middle row plots contains variance final infection size function
transmission probability. several observations. First, graphs whose
dynamics exhibit phase transition larger infection sizes increasing fixed p
(and increasing p fixed ), variance infection size qualitatively increases
p, peaks region phase transition decreases. peaks variance
correspond regimes change final infected fractions changing
p, expected (cf., first row figures). Second, greater perturbation,
lesser range p values variance high. greater
value , faster contagion spreads, thus driving variance. last observation
non-intuitive. is, peak variance seem vary combinations
p 0 10 0 p 0.6. ranges, value p
peak occurs decreases increasing . fact, peak variance around 0.1
graphs conditions.
Effect regular network structure numbers infected nodes.
several reasons investigate random networks uniform degree. First, investigation
voter model dynamics (Kuhlman, Kumar, & Ravi, 2013), shown uniform degree
networks generated results near realistic graphs whose degree distribution
exponential decay, far graphs scale free degree distributions.
question arises close behavior uniform degree networks
realistic networks. Second, since perturbing subgraph R study random
graph, perturbed graph G0 sense maintains random structure compared G,
G random 20-regular graph (a random graph node degree 20).
Figure 2 shows latter consideration dominates. is, upper right plot shows
curves basically shifted left increases. So, too, plot right
220

fiSensitivity Diffusion Dynamics Network Uncertainty

middle row, variance curves shift slightly left increases, effect much
smaller Wiki-Vote AS-2000-01-02.
Effect average time-to-peak number new infections. average
time histories 1000 diffusion instances comprise curve one graph
type provided top row Figure 4. transmission probabilities used
plot p > pc . average time maximum number new infections
occurs decrease, stay same, increase function , depending graph,
moving left right. graphs Table 1, seems fairly even split
nine graphs show increase average time-to-peak increasing eight
show decrease average time-to-peak. remainder show change time-to-peak.

20

10 15
Time
Twitter

20

Average Max New Inf

1.0
=0
=0.5
0.8
=1
=5
0.6
=10
0.4
0.2
0.00.0 0.2 0.4 0.6 0.8 1.0
Probability, p
Twitter
0.12
=0
=0.5
0.10
=1
0.08
=5
=10
0.06
0.04
0.02
0.000.0 0.2 0.4 0.6 0.8 1.0
Probability, p

Variance Max New Inf

Variance Max New Inf

Average Max New Inf

1.0
=0
=0.5
0.8
=1
=5
0.6
=10
0.4
0.2
0.00.0 0.2 0.4 0.6 0.8 1.0
Probability, p
CondMat
0.12
=0
=0.5
0.10
=1
0.08
=5
=10
0.06
0.04
0.02
0.000.0 0.2 0.4 0.6 0.8 1.0
Probability, p

5

=0
=0.5
=1
=5
=10

0.5 Wiki-Vote (p =0.12)=0
=0.5
0.4
=1
=5
0.3
=10
0.2
0.1
0.00
5 10 15 20
Time
Wiki-Vote
1.0
=0
=0.5
0.8
=1
=5
0.6
=10
0.4
0.2
0.00.0 0.2 0.4 0.6 0.8 1.0
Probability, p
Wiki-Vote
0.12
=0
=0.5
0.10
=1
0.08
=5
=10
0.06
0.04
0.02
0.000.0 0.2 0.4 0.6 0.8 1.0
Probability, p

Average Max New Inf

10 15
Time
CondMat

Twitter (p =0.5)

Variance Max New Inf

5

0.5
0.4
0.3
0.2
0.1
0.00

Average New Inf

=0
=0.5
=1
=5
=10

Average New Inf

CondMat (p =0.3)

Average New Inf

0.5
0.4
0.3
0.2
0.1
0.00

Figure 4: Sensitivity temporal characteristics uniform perturbation (IC model): Plots
(i) average number new infections time step selected p values, (ii) average
maximum number new infections time vs. p. (iii) variance maximum number
new infections time vs. p. remaining plots full version (Adiga et al.,
2014).

221

fiAdiga, Kuhlman, Mortveit & Vullikanti

Effect average peak number new infections. second row plots
Figure 4 depicts average maximum number new infections one time
function transmission probability, different . increases, maximum number
new infections increase.
Variance average maximum number new infections discrete time.
last row plots Figure 4 provides variance 1000 experimentally determined
values used compute average maximum number new infections; is, peak
value curve plots top row. particular graphs, variances
roughly order 0.01, is, interestingly, much smaller
final number infections Figure 2.
5.3.2 Degree-Assortative Perturbation
Here, focus results different uniform perturbation.
Effect degree-assortativity average final number infections
variance. first row plots Figure 5 shows average final number infections,
1000 measurements, function p . = 0 curves
Figure 2. comparison top row plots figure, clear
degree-assortativity perturbations significantly reduce effect changes
average final number infections. find, networks, AS-2000-01-02
Wiki-Vote provide two bounding cases, i.e., least effect effect degree-assortative
perturbations, respectively. comparison second row plots Figure 5
Figure 2, also clear degree-assortative perturbations correspondingly collapse
variances across values Wiki-Vote, AS-2000-01-02 less affected.
Effect perturbation method average final number infections. Since
= 0 curves Figure 2 Figure 5, factors same,
uniform perturbations generate greater numbers infections degree-assortative
perturbations (compare top rows plots two figures).
Effect network structure degree-assortative perturbations. effects
degree assortative perturbations appear network-specific uniform
perturbations. expected since perturbation instances inherit network
properties.
Effect degree-assortative perturbations random regular graph.
effects two perturbation methods random regular graphs
uniform node degrees.
5.4 Linear Threshold Model
results effect uniform perturbation LT model. Figure 6 shows
plots average number infections vs. three representative networks different
seed probabilities s. remaining plots full version (Adiga et al., 2014).
diffusion instance, seed set constructed sampling vertex set uniformly
222

fiSensitivity Diffusion Dynamics Network Uncertainty

Average Final Size

Average Final Size
Variance Final Size

1.0 Random-Regular-20
=0
=0.5
0.8
=1
=5
0.6
=10
0.4
0.2
0.00.0 0.1 0.2 0.3 0.4
Probability, p
Random-Regular-20
0.12
=0
=0.5
0.10
=1
0.08
=5
=10
0.06
0.04
0.02
0.000.0 0.2 0.4 0.6 0.8 1.0
Probability, p

Variance Final Size

Average Final Size

Wiki-Vote
1.0
=0
=0.5
0.8
=1
=5
0.6
=10
0.4
0.2
0.00.0 0.2 0.4 0.6 0.8 1.0
Probability, p
Wiki-Vote
0.12
=0
=0.5
0.10
=1
0.08
=5
=10
0.06
0.04
0.02
0.000.0 0.2 0.4 0.6 0.8 1.0
Probability, p

Variance Final Size

AS-2000-01-02
1.0
0.8
0.6
=0
0.4
=0.5
=1
0.2
=5
0.00.0 0.2 0.4 0.6 =100.8
Probability, p
AS-2000-01-02
0.12
=0
=0.5
0.10
=1
0.08
=5
=10
0.06
0.04
0.02
0.000.0 0.2 0.4 0.6 0.8 1.0
Probability, p

Figure 5: Degree-assortative perturbation (IC model): Average variance fraction
infections single random seed vs. transmission probability p various values.
remaining plots full version (Adiga et al., 2014).

probability s, every node assigned threshold chosen uniformly random
interval [0, 1].
Recall Theorem 2 spread bounded linear function .
Figure 6, observe model generally robust perturbation spread
linear sublinear function depending seed probability s. particular,
seed probability low (s = 0.0001 example), hardly change average
spread. Note plots, ranges 0 way 100 extreme
value perturbation considering sizes networks. However, larger values
s, especially comparable maximum degree , effect
pronounced, even low values . observation worth investigating could lead
better bounds spread Theorem 2.

6. Conclusions Open Problems
give first rigorous results stability independent cascade linear
threshold models respect edge perturbations. considered two popular noise
models namely, uniform degree-assortative perturbations, studied sensitivity
final outbreak size temporal characteristics perturbations. analysis
supported experimental observations 20 diverse real networks. showed
sensitivity independent cascade model depends transmission probability
perturbation lead abrupt changes outcome, linear threshold
model uniform edge weights general stable network perturbations. Also,
experiments suggest dynamics sensitive uniform perturbation
223

fiAdiga, Kuhlman, Mortveit & Vullikanti

0.6

=0.0001
=0.001
=0.01

0.4
0.2

0.8

0.00 20 40 60 80 100
Perturbation,

CIT-HepPh

0.6

0.4

=0.0001
=0.001
=0.01

0.8

Average Final Size

CA-AstroPh

Average Final Size

Average Final Size

0.8

0.6

Slashdot-09-02

=0.0001
=0.001
=0.01

0.4

0.2

0.00 20 40 60 80 100
Perturbation,

0.2

0.00 20 40 60 80 100
Perturbation,

Figure 6: Uniform perturbation (LT model): Average fraction infected nodes vs. perturbation various seed probabilities s. remaining plots full version (Adiga
et al., 2014).

degree-assortative. Extending results models noise, especially involving
dependencies, sensitivity number sources, examining sensitivity
dynamical properties general diffusion models (including IC LT models
heterogeneous probabilities weights) natural open problems future research.

7. Acknowledgments
work partially supported following grants: DTRA Grant HDTRA111-1-0016, DTRA CNIMS Contract HDTRA1-11-D-0016-0010, NSF Career CNS 0845700,
NSF ICES CCF-1216000, NSF NETSE Grant CNS-1011769 DOE DE-SC0003957. Also
supported Intelligence Advanced Research Projects Activity (IARPA) via Department
Interior National Business Center (DoI/NBC) contract number D12PC000337, US
Government authorized reproduce distribute reprints Governmental purposes
notwithstanding copyright annotation thereon.
Disclaimer: views conclusions contained herein authors
interpreted necessarily representing official policies endorsements, either
expressed implied, IARPA, DoI/NBC, US Government.
preliminary version paper appeared Twenty-Seventh AAAI Conference
Artificial Intelligence (2013).

References
Achlioptas, D., Clauset, A., Kempe, D., & Moore, C. (2005). bias traceroute
sampling: or, power-law degree distributions regular graphs. Proceedings
thirty-seventh annual ACM Symposium Theory Computing, pp. 694703. ACM.
Adiga, A., Kuhlman, C., Mortveit, H. S., & Vullikanti, A. K. S. (2014). Sensitivity diffusion dynamics network uncertainty. Technical report, available
http://ndssl.vbi.vt.edu/supplementary-info/vskumar/sensitivity-jair.pdf.
Bakshy, E., Hofman, J. M., Mason, W. A., & Watts, D. J. (2011). Everyones influencer:
quantifying influence Twitter. Proceedings fourth ACM international
224

fiSensitivity Diffusion Dynamics Network Uncertainty

conference Web search data mining, pp. 6574. ACM.
Bollobs, B. (1985). Random graphs. Academic Press.
Borgatti, S., Carley, K., & Krackhardt, D. (2006). robustness centrality measures
conditions imperfect data. Social Networks, 28, 124136.
Chen, J. (2010). effects demographic spatial variability epidemics: comparison
Beijing, Delhi Los Angeles. Conf. Crit. Inf.
Chung, F., & Lu, L. (2002). Connected components random graphs given expected
degree sequences.. Annals Combinatorics, 6, 125145.
Clauset, A., Moore, C., & Newman, M. (2008). Hierarchical structure prediction
missing links networks. Nature, 453, 98101.
Costenbader, E., & Valente, T. (2003). stability centrality measures networks
sampled. Social Networks, 25, 283307.
Eubank, S. (2010). Detail network models epidemiology: yet?. Journal
Biological Dynamics, 4(5), 446455.
Faloutsos, M., Faloutsos, P., & Faloutsos, C. (1999). power-law relationships
internet topology. SIGCOMM, Vol. 29, pp. 251262.
Flaxman, A., & Frieze, A. M. (2004). diameter randomly perturbed digraphs
applications. APPROX-RANDOM, pp. 345356.
Flaxman, A. (2007). Expansion lack thereof randomly perturbed graphs. Internet
Mathematics, 4 (2-3), 131147.
Galuba, W. (2010). Outtweeting twitterers - predicting information cascades microblogs. WOSN.
Goldenberg, J., Libai, B., & Muller, E. (2001). Talk network: complex systems look
underlying process word-of-mouth. Marketing Letters.
Gonzlez-Bailn, S., Borge-Holthoefer, J., Rivero, A., & Moreno, Y. (2011). dynamics
protest recruitment online network. Scientific Reports, 1.
Hagmann, P. (2008). Mapping structural core human cerebral cortex. PLoS Biol,
6(7).
Kaneko, K. (1985). Spatiotemporal intermittency coupled map lattices. Progress
Theoretical Physics, 74 (5), 10331044.
Kempe, D., Kleinberg, J., & Tardos, . (2003). Maximizing spread influence
social network. Proceedings ninth ACM SIGKDD international conference
Knowledge discovery data mining, pp. 137146. ACM.
Kempe, D., Kleinberg, J., & Tardos, . (2005). Influential nodes diffusion model
social networks. Automata, languages programming, pp. 11271138. Springer.
Kuhlman, C., Kumar, V., & Ravi, S. (2013). Controlling opinion propagation online
networks. Journal Computer Networks, 57, 21212132.
Lahiri, M., Maiya, A. S., Caceres, R. S., Habiba, & Berger-Wolf, T. Y. (2008). impact
structural changes predictions diffusion networks. ICDM, pp. 939948.
225

fiAdiga, Kuhlman, Mortveit & Vullikanti

Leskovec, J. (2011). Stanford network analysis project. http://snap.stanford.edu/index.html.
Newman, M. (2003). structure function complex networks. SIAM Review, 45 (2),
167256.
Romero, D., Meeder, B., & Kleinberg, J. (2011). Differences mechanics information
diffusion across topics: idioms, political hashtags, complex contagion twitter.
Proc. WWW, pp. 695704. ACM.
Ron, D. (2010). Algorithmic analysis techniques property testing. Foundations
Trends TCS, 5 (2), 73205.
Schwab, D. J., Bruinsma, R. F., Feldman, J. L., & Levine, A. J. (2010). Rhythmogenic
neuronal networks, emergent leaders, k-cores. Phys. Rev. E, 82, 051911.
Spielman, D. (2009). Smoothed analysis: attempt explain behavior algorithms
practice. Communications ACM, 7684.
Wu, C. W. (2005). Synchronization networks nonlinear dynamical systems coupled via
directed graph. Nonlinearity, 18, 10571064.

226

fiJournal Artificial Intelligence Research 51 (2014) 413-441

Submitted 06/14; published 10/14

Scoring Functions Based Second Level Score
k-SAT Long Clauses
Shaowei Cai

SHAOWEICAI . CS @ GMAIL . COM

State Key Laboratory Computer Science,
Institute Software, Chinese Academy Sciences, Beijing, China
Queensland Research Lab, NICTA, Brisbane, Australia

Chuan Luo

CHUANLUOSABER @ GMAIL . COM

Key Laboratory High Confidence Software Technologies,
Peking University, Beijing, China

Kaile Su

K . SU @ GRIFFITH . EDU . AU

Institute Integrated Intelligent Systems,
Griffith University, Brisbane, Australia

Abstract
widely acknowledged stochastic local search (SLS) algorithms efficiently find
models satisfiable instances satisfiability (SAT) problem, especially random k-SAT
instances. However, compared random 3-SAT instances SLS algorithms shown great
success, random k-SAT instances long clauses remain difficult. Recently, notion
second level score, denoted score2 , proposed improving SLS algorithms long-clause
SAT instances, first used powerful CCASat solver tie breaker.
paper, propose three new scoring functions based score2 . Despite simplicity,
functions effective solving random k-SAT long clauses. first function
combines score score2 , second one additionally integrates diversification property
age. two functions used developing new SLS algorithm called CScoreSAT.
Experimental results large random 5-SAT 7-SAT instances near phase transition show
CScoreSAT significantly outperforms previous SLS solvers. However, CScoreSAT cannot
rival competitors random k-SAT instances phase transition. improve CScoreSAT
instances another scoring function combines score2 age. resulting
algorithm HScoreSAT exhibits state-of-the-art performance random k-SAT (k > 3) instances
phase transition. also study computation score2 , including implementation
computational complexity.

1. Introduction
Boolean Satisfiability (SAT) problem prototypical NP-complete problem whose task
decide whether variables given Boolean formula assigned way make
formula evaluate TRUE. problem plays prominent role various areas computer
science artificial intelligence, widely studied due significant importance
theory applications.
Two popular approaches solving SAT conflict driven clause learning (CDCL)
stochastic local search (SLS). latter operates complete assignments tries find model
iteratively flipping variable. Although SLS algorithms typically incomplete sense
2014 AI Access Foundation. rights reserved.

fiC AI , L UO & U

cannot prove instance unsatisfiable, often find models satisfiable formulas
surprisingly effectively.
SLS algorithms SAT switch two different modes, i.e., greedy
(intensification) mode diversification mode. greedy mode, prefer flip variables
whose flips decrease number falsified clauses; diversification mode, tend
better explore search space avoid local optima, usually using randomized strategies
exploiting diversification properties variables pick variable aim.
SLS well known effective approach solving random satisfiable instances,
SLS algorithms often evaluated uniform random k-SAT benchmarks. benchmarks
large variety instances test robustness algorithms, controlling
instance size clause-to-variable ratio, provide adjustable hardness levels assess
solving capabilities. Moreover, performance algorithms usually stable random kSAT instances, either good bad. Thus, easily recognize good heuristics testing SLS
algorithms random k-SAT instances, heuristics may beneficial solving realistic
problems. Numerous works devoted designing SLS algorithms random k-SAT
instances clause-to-variable ratio near solubility phase transition,
difficult among random k-SAT instances (Kirkpatrick & Selman, 1994).
Among random k-SAT instances, random 3-SAT ones exhibit particular statistical
properties easy solve, example, SLS algorithms statistical physics approach
called Survey Propagation (Braunstein, Mzard, & Zecchina, 2005). shown
famous SLS algorithm WalkSAT (Selman, Kautz, & Cohen, 1994), proposed two
decades ago, scales linearly number variables random 3-SAT instances near phase
transition solve instances one million variables (Kroc, Sabharwal, & Selman,
2010). latest state art direction SLS algorithm called FrwCB, solves
random 3-SAT instances near phase transition (at ratio 4.2) millions variables within 2-3
hours (Luo, Cai, Wu, & Su, 2013).
contrast, random k-SAT instances long clauses remain difficult,
performance SLS algorithms instances stagnated long time. Indeed,
instances challenging kinds algorithms, including Survey Propagation algorithm,
solves random 3-SAT instances extremely fast (Mzard, 2003) also adapted solving
MaxSAT (Chieu & Lee, 2009). Recently, progresses Sattime (Li & Li, 2012), probSAT
(Balint & Schning, 2012) CCASat (Cai & Su, 2013b), made direction.
particular, solving random instances near phase transition, Sattime algorithm good
solving random 6-SAT 7-SAT instances, probSAT good solving random 4-SAT
5-SAT instances. Comparatively, CCASat shows good performance random k-SAT instances
k {4, 5, 6, 7} random track SAT Challenge 2012. Note second
third solvers track variants portfolio solver SATzilla (Xu, Hutter, Hoos, & LeytonBrown, 2008). hand, probSAT Sattime show better performance CCASat
random k-SAT instances threshold ratio phase transition.
key notion CCASat score2 property1 , shares spirit
commonly used score property regarded second level score. considers
transformations clauses one true literal two true literals. breaking
ties using score2 , performance CCASat significantly improved random k-SAT instances
1. score2 property denoted subscore CCASat (Cai & Su, 2013b).

414

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

k > 3 (Cai & Su, 2013b). leads us question improve SLS
algorithms instances making better use score2 property? paper, give
positive answer question proposing three new scoring functions based score2 ,
using develop two SLS algorithms outperform state-of-the-art solvers random
k-SAT k > 3 near phase transition.
first scoring function proposed paper called cscore, linear combination
score score2 . cscore function differs previous hybrid scoring functions
considers two score properties different levels. Based scoring function, also define
new type decreasing variables namely comprehensively decreasing variables. cscore
function enhances intensification greedy mode integrating current greediness
look-ahead greediness. Further, combining cscore diversification property age (the
definition age found Section 2.1), propose second scoring function dubbed
hscore, used improve diversification mode. two scoring functions used
develop SLS algorithms called CScoreSAT.
conduct extensive experiments compare CScoreSAT state-of-the-art SLS solvers
including winners recent SAT competitions. experiments large random 5SAT 7-SAT instances near phase transition show CScoreSAT significantly outperforms
competitors terms success rate run time. particular, CScoreSAT able solve random
5-SAT instances 5000 variables random 7-SAT instances 300 variables,
whereas competitors fail solve instances size.
However, performance CScoreSAT random k-SAT instances threshold ratio
phase transition good state-of-the-art solvers probSAT Sattime,
top two solvers random SAT category SAT Competition 2013. Note major
part random SAT benchmark SAT Competition 2013 consists random k-SAT instances
phase transition.
second contribution paper improve CScoreSAT random k-SAT instances
threshold ratio phase transition. idea reduce intensification greedy
mode, instances fewer models (if satisfiable). considerations give rise
third scoring function dubbed hscore2 , combines score2 age. function used
improve greedy mode CScoreSAT, leading new algorithm called HScoreSAT.
greedy mode, HScoreSAT utilizes score property pick flipping variable, breaks ties
hscore2 function. evaluate HScoreSAT random k-SAT (k > 3) instances threshold
ratio phase transition, including SAT Competition 2013, experimental results
show HScoreSAT significantly improves CScoreSAT instances.
note first two functions CScoreSAT algorithm (Section 3),
presented conference paper (Cai & Su, 2013a), third scoring function
HScoreSAT algorithm (Section 4), well experimental analyses (including Section 3.5
whole Section 5) new contributions paper.
paper proceeds follows. Section 2 introduces preliminary concepts. Section 3
presents cscore hscore functions describes CScoreSAT algorithm, along
experimental evaluations analyses CScoreSAT random k-SAT (k > 3) instances near
phase transition. Section 4 presents hscore2 function HScoreSAT algorithm,
well evaluations HScoreSAT random k-SAT (k > 3) instances phase transition
related experimental analyses. Section 5, study computation score2 , including
415

fiC AI , L UO & U

implementation, complexity computational overhead. Finally, give concluding
remarks future directions Section 6.

2. Preliminaries
section, first introduce basic definitions notation problem. Then,
briefly review notion second level properties related works. Finally, introduce
configuration checking strategy, also important component algorithms.
2.1 Basic Definitions Notation
Given set n Boolean variables {x1 , x2 , ..., xn }, literal either variable x (which called
positive literal) negation x (which called negative literal), clause disjunction
literals. conjunctive normal form (CNF) formula F = c1 c2 ... cm conjunction
clauses. satisfying assignment formula assignment variables formula
evaluates true. Given CNF formula F , Boolean Satisfiability problem find satisfying
assignment prove none exists.
well-known generation model SAT uniform random k-SAT model (Achlioptas,
2009). random k-SAT instance, clause contains exactly k distinct non-complementary

literals, picked uniform probability distribution set 2k nk possible clauses.
clause-to-variable ratio CNF formula F defined r = m/n, n number
variables number clauses.
CNF formula F , use V (F ) denote set variables appear F . say
variable appears clause, clause contains either x x. Two variables neighbors
appear simultaneously least one clause. neighbourhood variable x
N (x) = {y|y occurs least one clause x}, set neighboring variables
variable x. subset X V (F ) assignment , [X] projection
variables X.
say literal true current value variable phase. E.g.,
x1 = false, negative literal x1 true, positive literal x1 true. clause
satisfied least one true literal, falsified otherwise.
SLS algorithms SAT usually select variable flip step guidance
scoring f unctions. SLS algorithms one scoring function, adopt one
current search step according conditions, whether local optimum
reached. scoring function simple variable property mathematical expression
one properties.
Perhaps popular variable property used SLS algorithms SAT score,
measures increase number satisfied clauses flipping variable. score property
also defined score(x) = make(x) break(x), make break number
clauses would become satisfied falsified, respectively, x flipped. Note
two definitions score equivalent. dynamic local search algorithms use clause
weighting techniques, score measures increase total weight satisfied clauses flipping
variable, make break measures total weight clauses would become satisfied
falsified, respectively, flipping x. variable decreasing score positive,
increasing score negative. age variable defined number search steps
occurred since variable last flipped.
416

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

2.2 Second Level Properties
subsection, introduce second level properties, especially second level score,
important concept proposed scoring functions work.
second level properties take account satisfaction degree clauses, defined
number true literals clause (Cai & Su, 2013a). clause satisfaction degree
said -satisfied clause. variable x, score2 (x) defined make2 (x) minus
break2 (x), make2 (x) number 1-satisfied clauses would become 2-satisfied
flipping x, break2 (x) number 2-satisfied clauses would become 1-satisfied
flipping x. One easily define properties levels weighted version
properties.
first SLS solver using second level properties CCASat (Cai & Su, 2013b), simply
uses score2 tie breaker achieves surprising improvements random k-SAT long
clauses. Then, conference version paper, combine score score2 develop
CScoreSAT algorithm (Cai & Su, 2013a). also propose notion multi-level properties
use make2 improve famous WalkSAT/SKC algorithm (Cai, Su, & Luo, 2013a). Afterwards,
multi-level break used improve probSAT solver (Balint, Biere, Frhlich, & Schning,
2014). work, exploit score2 property using design scoring functions
directly guide algorithm pick flipping variable.
note algorithms work utilize unweighted version score2 (although
use weighted version score), CCASat does. algorithms, unweighted
score2 found much effective weighted one, yet time could
figure reason find effective way using weighted score2 algorithms.
2.3 Configuration Checking SAT
subsection, briefly introduce configuration checking (CC) strategy SAT,
important component proposed algorithms work.
Initially introduced improving local search Minimum Vertex Cover (MVC) problem
(Cai, Su, & Sattar, 2011), CC strategy aims avoiding cycling local search, i.e., revisiting
already visited candidate solutions early. successfully used MVC (Cai et al.,
2011; Cai, Su, Luo, & Sattar, 2013b), well SAT (Cai & Su, 2012; Luo et al., 2013; Abram,
Habet, & Toumi, 2014; Luo, Cai, Wu, & Su, 2014; Li, Huang, & Xu, 2014) MaxSAT (Luo, Cai,
Wu, Jie, & Su, 2014).
CC strategy based concept configuration. One define configuration
different ways design different CC strategies accordingly. context SAT,
configuration variable typically refers truth values neighboring variables (Cai &
Su, 2013b). Formally, given assignment , CC strategy SAT defines configuration
C(xi ) variable xi subset restricted variables N (xi ), i.e., C(xi ) = [N (xi )].
variable C(xi ) flipped since last flip xi C(xi ) said changed. CC
strategy SAT forbids flip variable xi configuration C(xi ) changed since
last flip xi .
CC strategy used decrease blind unreasonable greedy search. strategy
successfully applied SAT solving, resulting several efficient SLS algorithms SAT,
CCASat (Cai & Su, 2013b), Ncca+ (the bronze medal winner random SAT track SAT
Competition 2013) (Abram et al., 2014), BalancedZ (Li et al., 2014) CSCCSat (Luo et al.,
417

fiC AI , L UO & U

2014) (the silver bronze medal winner random SAT track SAT Competition 2014),
CCAnr+glucose (Cai & Su, 2012) (the silver medal winner hard combinatorial SAT track SAT
Competition 2014), etc.

3. Two New Scoring Functions CScoreSAT Algorithm
section, design two new scoring functions, namely cscore hscore. use
develop new SLS algorithm called CScoreSAT, shows excellent performance random
k-SAT k > 3 near phase transition.
3.1 cscore Function
subsection, introduce cscore (short comprehensive score) function,
linear combination score score2 properties.
score property characterizes greediness flipping variable current search step,
tends decrease number falsified clauses, indeed aim SAT problem.
hand, score2 property regarded measurement look-ahead greediness,
tends reduce 1-satisfied clauses transforming 2-satisfied clauses, noting
1-satisfied clauses may become falsified next step 2-satisfied ones not.
seems short sighted simply take score property scoring function, especially
formulas long clauses, number true literals varies considerably among satisfied
clauses. address issue, propose scoring function incorporates score
score2 . deciding candidate variables priorities selected, although score
important score2 , cases score2 allowed overwrite priorities.
example, two variables relatively small score difference significant score2
difference, advisable prefer flip one greater score2 .
considerations suggest two principles designing desired scoring functions.
First, score property plays important role;
Second, score2 property allowed overwrite variables priorities (of
selected).
result, notion comprehensive score, formally defined follows.
Definition 1. CNF formula F , comprehensive score function, denoted cscore,
function V (F )
cscore(x) = score(x) + score2 (x)/d,
positive integer parameter.
Note cscore defined integer function, thus value cscore
rounded integer not.
cscore function linear combination score score2 bias towards score,
thus embodies two principles well. function simple computed little
overhead parameter easily tuned. Moreover, simplicity allows potential usage
solving structured SAT instances perhaps combinatorial search problems.
Recall variable decreasing positive score. following,
define new type deceasing variables based cscore function.
418

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

Definition 2. Given CNF formula F cscore function, variable x comprehensively
decreasing cscore(x) > 0 score(x) 0.
condition cscore(x) > 0 straightforward, condition score(x) 0
requires variable non-increasing. necessary, flipping increasing variable leads
local search away objective, accepted without controlling
mechanism Metroplis probability Simulated Annealing (Kirkpatrick, Gelatt, &
Vecchi, 1983), unless algorithm gets stuck local optimum.
SLS algorithms SAT prefer flip decreasing variables greedy search mode.
respect, notion comprehensively decreasing variables extension decreasing
variables, good alternative considered flip candidates greedy search phases.
3.2 hscore Function
combine cscore diversification property age, resulting hybrid scoring function
dubbed hscore, used improve diversification mode.
One commonly used variable property diversification mode SLS algorithms
SAT age. Previous SLS algorithms usually use age pick oldest variable candidate
variable set (Gent & Walsh, 1993; Li & Huang, 2005; Cai & Su, 2012; Abram et al., 2014)
break ties (Prestwich, 2005; Pham, Thornton, Gretton, & Sattar, 2007; Luo, Su, & Cai, 2012).
opinion, however, oldest strategies strict always prefer oldest one,
regardless important information score cscore. Thus, oldest strategies
may miss better variables quite often.
example, suppose SLS algorithm gets stuck local optimum, would like pick
one variable flip two variables x1 x2 : two variables similar ages x1
older x2 , cscore(x2 ) significantly greater cscore(x1 ). case, believe
x2 right choice rather older variable x1 , flipping two variables leads
similar diversification flipping x2 less harm object function.
Based considerations, design hybrid scoring function taking account
greediness information cscore diversification information age. resulting scoring
function dubbed hscore given follows.
Definition 3. CNF formula F , hscore function function V (F )
hsocre(x) = cscore(x) + age(x)/ = score(x) + score2 (x)/d + age(x)/,
positive integer parameters.
algorithms, reaching local optimum, algorithms make use hybrid
function. show hscore function better choice oldest strategy
diversification mode.
3.3 CScoreSAT Algorithm
section presents CScoreSAT algorithm, adopts cscore function guide
search greedy mode, makes use hscore function meets local optima.
getting details CScoreSAT algorithm, first introduce two techniques
employed algorithm.
419

fiC AI , L UO & U

1. PAWS weighting scheme. sake diversification, CScoreSAT employs PAWS
clause weighting scheme (Thornton, Pham, Bain, & Ferreira Jr., 2004). clause
associated positive integer weight, initiated 1. local optimum
reached, clause weights updated follows. probability sp (the so-called
smooth probability), satisfied clause whose weight larger one, weight
decreased one; probability (1 sp), weights falsified clauses increased
one.
2. Configuration checking. order reduce blind greedy search, utilize configuration
checking strategy SAT (Cai & Su, 2012). Recall variable said configuration
changed last flip least one neighboring variables
flipped. According configuration checking strategy, configuration changed
variables allowed flipped greedy mode.
Algorithm 1: CScoreSAT
Input: CNF-formula F , maxSteps
Output: satisfying assignment F , unknown
1 begin
2
:= randomly generated truth assignment;
3
step := 1 maxSteps
4
satisfies F return ;
5
= {x|x comprehensively decreasing configuration changed } =
6
6
v := variable greatest cscore, breaking ties favor oldest
one;
7
else
8
update clause weights according PAWS;
9
pick random falsified clause C;
10
v := variable C greatest hscore;
11
12
13

:= v flipped;
return unknown;
end

CScoreSAT algorithm outlined Algorithm 1, described below. beginning,
CScoreSAT generates random complete assignment, initiates clause weights 1 computes
score score2 variables accordingly. initialization, CScoreSAT executes loop
finds satisfying assignment reaches limited number steps denoted maxSteps (or
given cutoff time).
Like SLS algorithms SAT, CScoreSAT switches two modes. search
step, works either greedy mode diversification mode, depending whether
exist comprehensively decreasing variables configuration changed. exist
variables, CScoreSAT works greedy mode. picks variable greatest cscore
value flip, breaking ties preferring oldest one.
variables comprehensively decreasing configuration changed,
CScoreSAT switches diversification mode. first updates clause weights according
420

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

PAWS scheme. randomly selects falsified clause C, picks variable C
greatest hscore value flip.
3.4 Evaluations CScoreSAT
subsection, carry extensive experiments evaluate CScoreSAT random k-SAT
instances k {4, 5, 6, 7} near phase transition. First, compare CScoreSAT state-ofthe-art SLS solvers random 5-SAT 7-SAT instances. Then, compare CScoreSAT
state-of-the-art SLS solvers random k-SAT instances k {4, 5, 6, 7} SAT Challenge
2012. Finally, study effectiveness cscore hscore functions empirical
analysis random 5-SAT 7-SAT instances.
3.4.1 B ENCHMARKS



E XPERIMENT P RELIMINARIES

instances used experiments generated according random k-SAT model
near solubility phase transition. Specifically, adopt following five benchmarks. first
two benchmarks random 5-SAT, third fourth benchmarks random 7-SAT,
last one consists random k-SAT instances k = 4, 5, 6, 7 various ratios.
1. 5-SAT Comp11: large random 5-SAT instances SAT Competition 2011 (r = 20,
750 n 2000, 50 instances, 10 size).
2. 5-SAT Huge: 5-SAT instances generated randomly according random k-SAT model
(r = 20, 3000 n 5000, 500 instances, 100 size).
3. 7-SAT Comp11: large random 7-SAT instances SAT Competition 2011 (r = 85,
150 n 400, 50 instances, 10 size).
4. 7-SAT Random: 7-SAT instances generated randomly according random k-SAT model
(r = 85, 220 n 300, 500 instances, 100 size).
5. SAT Challenge 2012: random k-SAT instances k > 3 SAT Challenge 2012
(480 instances, 120 k-SAT, k = 4, 5, 6, 7), vary size ratio.
random instances occupy 80% random benchmark SAT Challenge 2012, indicating
importance random k-SAT instances k > 3 highly recognized
SAT community. instances vary 800 variables r = 9.931 10000 variables
r = 9.0 4-SAT, 300 variables r = 21.117 1600 variables r = 20 5-SAT,
200 variables r = 43.37 400 variables = 40 6-SAT, 100 variables
r = 87.79 200 variables r = 85 7-SAT.

parameter
sp (for PAWS)



4-SAT
0.62
9
2000

5-SAT
0.62
8
2000

6-SAT
0.9
7
2000

7-SAT
0.9
6
2000

Table 1: Parameter setting CScoreSAT
421

fiC AI , L UO & U

CScoreSAT implemented C++ compiled g++ -O2 option. parameter
setting CScoreSAT reported Table 1. compare CScoreSAT four state-of-the-art
SLS solvers, including Sparrow2011 (Balint & Frhlich, 2010), CCASat (Cai & Su, 2013b),
probSAT (Balint & Schning, 2012), Sattime2012 (Li & Li, 2012). Sparrow2011
probSAT gold medal random SAT track SAT competitions 2011 2013
respectively. CCASat winner category SAT Challenge 2012. Sattime regularly
medals SAT competitions track.
experiments carried parallel workstation 32-bit Ubuntu Linux Operation
System, using 2 cores Intel(R) Core(TM) 2.6 GHz CPU 8 GB RAM. experiments
conducted EDACC, experimental platform testing SAT solvers, used
SAT Challenge 2012 SAT Competition 2013. run terminates upon either finding
satisfying assignment reaching given cutoff time set 5000 seconds (as SAT
Competition 2011) 5-SAT 7-SAT benchmarks, 1000 seconds SAT Challenge
2012 benchmark (close cutoff SAT Challenge 2012, i.e., 900 seconds).
5-SAT Comp11 7-SAT Comp11 benchmarks (where instance class 10
instances), run solver 10 times instance thus 100 runs instance
class. 5-SAT Huge 7-SAT Random benchmarks (where instance class contains 100
instances) SAT Challenge 2012 benchmark (120 k-SAT instances k), run
solver one time instance, instances class enough test performance
solvers.
solver instance class, report number successful runs
satisfying assignment found (suc runs) solved instances (#solved), well
PAR10 (par10), penalized average run time timeout solver penalized
10(cutoff time). Note PAR10 adopted SAT competitions widely used
literature prominent performance measure SLS-based SAT solvers (KhudaBukhsh, Xu,
Hoos, & Leyton-Brown, 2009; Tompkins & Hoos, 2010; Tompkins, Balint, & Hoos, 2011; Balint
& Schning, 2012). results bold indicate best performance instance class.
solver successful run instance class, corresponding par10 marked n/a.
3.4.2 E XPERIMENTAL R ESULTS



CS CORE SAT

following, present comparative experimental results CScoreSAT competitors
benchmark.
Results 5-SAT Comp11 Benchmark:
Table 2 shows comparative results 5-SAT Comp11 benchmark. clear Table
2, CScoreSAT shows significantly better performance solvers whole benchmark.
CScoreSAT solver solves 5-SAT instances runs. Also, CScoreSAT
significantly outperforms competitors terms run time, obvious instance
size increases. particular, 5-SAT-v2000 instances, largest size SAT
competitions, runtime CScoreSAT 15 times less CCASat, 2 orders
magnitudes less state-of-the-art SLS solvers.
Results 5-SAT Huge Benchmark:
experimental results 5-SAT Huge benchmark presented Table 3.
encouraging see performance CScoreSAT remains surprisingly good large 5SAT instances, state-of-the-art solvers show poor performance. CScoreSAT solves
422

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

Instance
Class
5-SAT-v750
5-SAT-v1000
5-SAT-v1250
5-SAT-v1500
5-SAT-v2000

Sattime2012
suc runs
par10
100
754
100
1254
95
5288
56
24101
14
43249

Sparrow2011
suc runs
par10
100
51
100
159
100
174
99
1231
72
15288

probSAT
suc runs
par10
100
88
100
185
100
237
98
1753
71
15635

CCASat
suc runs
par10
100
47
100
81
100
128
100
443
93
4386

CScoreSAT
suc runs
par10
100
35
100
38
100
47
100
145
100
289

Table 2: Experimental results 5-SAT Comp11 benchmark. 10 instances
class solver executed 10 times instance cutoff time
5000 seconds.

Instance
Class
5-SAT-v3000
5-SAT-v3500
5-SAT-v4000
5-SAT-v4500
5-SAT-v5000

Sattime2012
suc runs
par10
0
n/a
0
n/a
0
n/a
0
n/a
0
n/a

Sparrow2011
suc runs
par10
31
35360
8
46147
4
48080
0
n/a
0
n/a

probSAT
suc runs
par10
40
30867
6
47188
3
48591
0
n/a
0
n/a

CCASat
suc runs
par10
64
19403
35
33540
10
45287
0
n/a
0
n/a

CScoreSAT
suc runs
par10
100
694
100
1431
87
8167
62
21513
38
32005

Table 3: Experimental results 5-SAT Huge benchmark. 100 instances
class solver executed one time instance cutoff time 5000
seconds.

5-SAT instances (at least) 3500 variables consistently (i.e., 100% success rate),
30 times faster solvers 5-SAT-v3500 instances. Furthermore, CScoreSAT
succeeds 62 38 runs 5-SAT-v4500 5-SAT-v5000 instances respectively, whereas
competitors fail find solution instances. Indeed, best
knowledge, large random 5-SAT instances (at r = 20) solved first time. Given
good performance CScoreSAT 5-SAT instances 5000 variables, confident
could able solve larger 5-SAT instances.
423

fiC AI , L UO & U

Instance
Class
7-SAT-v150
7-SAT-v200
7-SAT-v250
7-SAT-v300
7-SAT-v400

Sattime2012
suc runs
par10
100
498
49
26998
2
49095
0
n/a
0
n/a

Sparrow2011
suc runs
par10
100
642
17
41912
0
n/a
0
n/a
0
n/a

probSAT
suc runs
par10
88
6980
11
44806
0
n/a
0
n/a
0
n/a

CCASat
suc runs
par10
100
232
72
14912
7
46731
0
n/a
0
n/a

CScoreSAT
suc runs
par10
100
131
90
5853
35
34070
11
44776
0
n/a

Table 4: Experimental results 7-SAT Comp11 benchmark. 10 instances
class solver executed 10 times instance cutoff time
5000 seconds.

Instance
Class
7-SAT-v220
7-SAT-v240
7-SAT-v260
7-SAT-v280
7-SAT-v300

Sattime2012
suc runs
par10
39
31868
13
43935
4
48113
0
n/a
0
n/a

Sparrow2011
suc runs
par10
13
43407
2
49051
0
n/a
0
n/a
0
n/a

probSAT
suc runs
par10
10
45253
2
49052
0
n/a
0
n/a
0
n/a

CCASat
suc runs
par10
68
17189
33
34158
9
45736
5
47605
0
n/a

CScoreSAT
suc runs
par10
83
10639
66
17901
53
24825
24
39283
11
44889

Table 5: Experimental results 7-SAT Random benchmark. 100 instances
class solver executed one time instance cutoff time
5000 seconds.

Results 7-SAT Comp11 Benchmark:
Table 4 summarizes experimental results 7-SAT Comp11 benchmark. None
solvers solve 7-SAT instance 400 variables, indicating random 7-SAT instances
near phase transition difficult even relatively small size. Nevertheless, CScoreSAT
significantly outperforms competitors 7-SAT benchmark, solver
solve 7-SAT instances 300 variables. Actually, competitors become ineffective
424

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

Instance
Class
4-SAT
5-SAT
6-SAT
7-SAT


Sattime2012
#solved
par10
49
6031
32
7407
84
3187
81
3422
246
5011

Sparrow2011
#solved
par10
79
3514
52
5812
72
4193
65
4714
268
4558

probSAT
#solved
par10
111
778
54
5657
76
3877
57
5380
298
3923

CCASat
#solved
par10
112
751
71
4264
99
1887
77
3734
359
2659

CScoreSAT
#solved
par10
119
174
84
3146
110
935
91
2559
404
1703

Table 6: Experimental results SAT Challenge 2012 benchmark. instance class contains
120 instances, solver executed instance cutoff time 1000
seconds.

(among CCASat highest success rate 7%) 7-SAT-v250 instances,
CScoreSAT still achieves success rate 35% instance class.
Results 7-SAT Random Benchmark:
sizes random 7-SAT instances SAT Competition 2011 continuous enough
provide good spectrum instances SLS solvers. order investigate detailed
performance CScoreSAT state-of-the-art SLS solvers random 7-SAT instances,
evaluate 7-SAT Random benchmark, instance size increases slowly.
again, Table 5 suggests difficulty 7-SAT instances increases significantly
relatively small increment size. reported Table 5, results show CScoreSAT
dramatically outperforms competitors. Compared competitors whose performance
descends steeply instance size increases, CScoreSAT shows good scalability. example,
7-SAT-v220 7-SAT-v260, success rates competitors decline eight times
more, whereas CScoreSAT drops thirty percents. coming 7-SAT-v260
instances, probSAT Sparrow2011 fail runs, competitors succeed less
10 runs, CScoreSAT succeeds 53 runs. Finally, CScoreSAT solver survives
throughout whole benchmark.
Results SAT Challenge 2012 Benchmark:
investigate performance CScoreSAT random k-SAT instances various k (k >
3), compare state-of-the-art solvers random k-SAT instances k > 3 SAT
Challenge 2012. Table 6 reports number solved instances PAR10 solver
k-SAT instance class. results show CScoreSAT significantly outperforms competitors
terms metrics. Overall, CScoreSAT solves 404 instances. observations show
CScoreSAT solves 365 instances within half cutoff time, whereas none competitors solves
360 instances within cutoff time. encouragingly, Table 6 shows CScoreSAT
solves k-SAT instances k, illustrates robustness. good performance
425

fiC AI , L UO & U

1000
Sattime2012
Sparrow2011
probSAT
CCASat
CScoreSAT

900
800

run time (s)

700
600
500
400
300
200
100
0

0

50

100

150

200
#solved

250

300

350

400

Figure 1: Comparison run time distributions SAT Challenge 2012 benchmark, cutoff
time 1000 seconds.

CScoreSAT SAT Challenge 2012 benchmark also clearly illustrated Figure 1,
summarizes run time distributions solvers benchmark.
3.5 Experimental Analyses cscore hscore Functions
order demonstrate effectiveness cscore hscore functions, also test two
alternative versions CScoreSAT, namely CScoreSAT1 CScoreSAT2 . two algorithms
modified CScoreSAT follows.
CScoreSAT1 : greedy mode, CScoreSAT1 uses score scoring function instead
cscore; also, CScoreSAT1 utilize concept comprehensively decreasing,
variable allowed flip decreasing configuration changed.
CScoreSAT2 : diversification mode, CScoreSAT2 uses age property instead
hscore scoring fucntion, i.e., picks oldest variable selected falsified
clause.
carry experiments compare CScoreSAT two degraded versions random 5SAT 7-SAT instances. experimental results reported Table 7. obvious observation
performance CScoreSAT1 essentially worse CScoreSAT. example,
cannot solve 5-SAT instance 2000 variables 7-SAT instance 250 instance.
indicates cscore function critical good performance CScoreSAT. Compared
cscore, hscore function used random mode show much contribution.
Nevertheless, usage hscore improve CScoreSATs performance 5-SAT 7-SAT
instances. careful comparison CScoreSAT CScoreSAT2 shows hscore
important solving 7-SAT instances 5-SAT ones.
426

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

Instance
Class
5-SAT-v1500
5-SAT-v2000
5-SAT-v4000
7-SAT-v150
7-SAT-v200
7-SAT-v250

CScoreSAT1
suc runs
par10
41
31452
0
n/a
0
n/a
89
5359
22
40406
0
n/a

CScoreSAT2
suc runs
par10
100
152
100
330
78
12118
100
569
75
13669
10
45329

CScoreSAT
suc runs
par10
100
145
100
289
87
8167
100
131
90
5853
35
34070

Table 7: Comparison CScoreSAT two alternative algorithms random 5-SAT
7-SAT instances.

4. Improving CScoreSAT Random k-SAT Phase Transition
section shows excellent performance CScoreSAT random k-SAT (k > 3) near
phase transition. However, performance CScoreSAT degrades instances phase
transition. CScoreSAT participated satisfiable random category SAT Competition 2013,
major part benchmark consists instances generated threshold ratio phase
transition. Although ranked 4th category, performance good enough kind
instances, worse state-of-the-art SLS solvers probSAT Sattime2013,
top two solvers satisfiable random category SAT Competition 2013.
section improves CScoreSAT random k-SAT (k > 3) phase transition. end,
propose another scoring function combining score2 age, utilize improve greedy
mode CScoreSAT, resulting new algorithm called HScoreSAT. experiments show
HScoreSAT significantly improves CScoreSAT gives state-of-the-art performance random kSAT (k > 3) threshold ratio phase transition. also compare CScoreSAT HScoreSAT
instances various ratios find boundary ratios beyond HScoreSAT outperforms
CScoreSAT.
4.1 hscore2 Function HScoreSAT Algorithm
important issue SLS algorithms SAT balance intensification
diversification. Indeed, improvements SLS algorithms SAT due proper regulation
intensification diversification local search. random k-SAT instances solubility
phase transition, search regions contain model (if instance satisfiable).
Therefore, inadvisable strong intensification instances, might waste
search much time unpromising regions search explore enough regions
discovering model.
427

fiC AI , L UO & U

order improve CScoreSAT random k-SAT instances phase transition, propose
reduce intensification greedy mode. CScoreSAT, use cscore scoring function,
break ties age. mentioned before, cscore function quite greedy scoring function
combines score score2 , represent greediness look-ahead greediness respectively.
Therefore, opinion, cscore suitable random k-SAT instances phase transition.
Recalling object SLS algorithms SAT minimize number total weight
falsified clauses, score property primary criterion greedy mode. Also,
believe score2 property important information solving long-clause instances,
considers satisfaction degree clauses. However, score score2 combined
together primary scoring function, intensifying solving (satisfiable) random kSAT instances phase transition.
Based considerations, move score2 primary scoring function
tie-breaking function, combined diversification property age. leads
new scoring function refer hscore2 hybrid function score2 age.
Definition 4. CNF formula F , hscore2 function function V (F )
hscore2 (x) = score2 (x) + age(x)/,
positive integer parameter.
Accordingly, modify greedy mode CScoreSAT, obtain new algorithm
refer HScoreSAT. pseudo-codes HScoreSAT given Algorithm 2.
HScoreSAT differs CScoreSAT following two aspects. First, although
algorithms utilize CC strategy, HScoreSAT allows decreasing variables flipped
greedy mode, CScoreSAT allows comprehensively decreasing variables (which super-set
decreasing variables) flipped. importantly, HScoreSAT uses hscore2 break ties
greedy mode, CScoreSAT breaks ties age.
Since hscore2 -based tie-breaking important component HScoreSAT,
interested question: exist configuration changed decreasing variables, often
tie-breaking executed pick one them? conducted experiment
threshold benchmark random satisfiable category SAT Competition 2013 calculate
frequency, ratio following two statistics.
#stepsccd : number steps configuration changed decreasing (CCD) variables
exist.
#stepsbt : number steps configuration changed decreasing (CCD) variables
exist, best CCD variable picked via hscore2 -based tie-breaking.
experimental results summarized Table 8, averaged instances
run per instance. demonstrated Table 8, frequency hscore2 -based
tie-breaking CCD steps significant, high 4-SAT 5-SAT (68% 60%
respectively). indicates hscore2 -based tie-breaking mechanism plays critical role
HScoreSAT. Another interesting observation frequency decreases length
clauses instance.
428

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

Algorithm 2: HScoreSAT
Input: CNF-formula F , maxSteps
Output: satisfying assignment F , unknown
1 begin
2
:= randomly generated truth assignment;
3
step := 1 maxSteps
4
satisfies F return ;
5
= {x|x decreasing configuration changed } =
6
6
v := variable greatest score, breaking ties preferring one
greatest hscore2 ;
7
else
8
update clause weights according PAWS;
9
pick random falsified clause C;
10
v := variable C greatest hscore;
11
12
13

:= v flipped;
return unknown;
end

#stepsbt
#stepsccd
#stepsbt
#stepsccd

4-SAT
218465135
317682739
68%

5-SAT
514417498
849513823
60%

6-SAT
198497368
404629074
49%

7-SAT
17260095
43678889
38%

Table 8: Averaged number CCD steps hscore2 -based tie-breaking steps, well
averaged ratio k-SAT k {4, 5, 6, 7} threshold benchmark SAT
Competition 2013.

4.2 Evaluations HScoreSAT Threshold Instances
subsection, carry extensive experiments evaluate HScoreSAT random k-SAT
instances k {4, 5, 6, 7} phase transition. First, compare HScoreSAT CScoreSAT
well state-of-the-art SLS solvers random benchmark threshold phase transition
SAT Competition 2013. Then, compare HScoreSAT state-of-the-art SLS solvers
large-sized random k-SAT (k {4, 5, 6, 7}) instances generated randomly threshold phase
transition.
4.2.1 B ENCHMARK



E XPERIMENT P RELIMINARIES

experiments section, benchmark instances generated according random
k-SAT model threshold ratio solubility phase transition. instances clauseto-variable ratio equal conjectured threshold ratio solubility phase transition2 (Mertens,
Mzard, & Zecchina, 2006). Specifically, adopt following two benchmarks.
2. clause-to-variable ratio 50% uniform random formulas satisfiable. algorithms,
closer formula generated near threshold ratio, harder solve it.

429

fiC AI , L UO & U

1. Threshold Comp13: threshold benchmark random satisfiable category SAT
Competition 2013. k-SAT, instances various sizes. also note
filtering applied construct competition suite. consequence, significant
fraction (approximately 50%) generated threshold instances unsatisfiable. details
benchmark given Table 9.
2. Large-sized Threshold: random k-SAT instances threshold ratio phase transition,
generated randomly random k-SAT generator3 used SAT Competition 2013.
benchmark contains 400 instances, 100 k-SAT class k {4, 5, 6, 7}. sizes
instances benchmark (n = 2000, 550, 250, 150 k = 4, 5, 6, 7, respectively)
relatively large compared Threshold Comp13 benchmark. instances
evenly divided two categories: training set test set,
50 instances k-SAT class.
Note training set used tune parameters HScoreSAT,
HScoreSAT tuned parameter setting evaluated Threshold Comp13 benchmark
test set Large-sized Threshold benchmark.
4-SAT
5-SAT
6-SAT
7-SAT
#inst.
50
50
50
50
ratio
9.931
21.117
43.37
87.79
size n {830, 860, ..., 2300} n {305, 310, ..., 550} n {191, 192, ..., 240} n {91, 92, ..., 140}

Table 9: instance numbers, ratios sizes k-SAT k {4, 5, 6, 7}
Threshold Comp13 benchmark.

HScoreSAT implemented basis CScoreSAT source code complied g++
-O2 option. parameter setting HScoreSAT presented Table 10, tuned
based training set Large-sized Threshold benchmark. compare HScoreSAT
CScoreSAT, well three state-of-the-art SLS solvers, including CCASat, probSAT
(version 2013) Sattime2013. Especially, note probSAT Sattime2013 top
two solvers random SAT track SAT Competition 2013.
parameter
sp




4-SAT
0.75
9
50
500

5-SAT
0.75
8
100
500

6-SAT
0.92
7
500
500

7-SAT
0.9
6
500
500

Table 10: Parameter setting HScoreSAT
computing environments experiments used experiments
Section 3. Following experiment setup SAT Competition 2013, perform solver
one run instance, run terminates upon either finding satisfying assignment
reaching given cutoff time set 5000 seconds. report number solved instances
3. http://sourceforge.net/projects/ksatgenerator/

430

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

Instance
Class
4-SAT
(r = 9.931)
5-SAT
(r = 21.117)
6-SAT
(r = 43.37)
7-SAT
(r = 87.79)


CCASat
#solved
par10
10
40168
9
41117
15
35435
23
27308
57
36007

Sattime2013
#solved
par10
8
42174
9
41153
18
31445
26
24351
61
35006

probSAT
#solved
par10
13
37160
10
40185
15
35440
23
27363
61
35037

CScoreSAT
#solved
par10
9
41193
8
42176
13
37211
21
29189
51
37442

HScoreSAT
#solved
par10
14
36176
11
39296
19
31538
25
25275
69
33071

Table 11: Experimental results Threshold Comp13 benchmark. instance class contains
50 instances solver executed instance cutoff time 5000
seconds.

(#solved) PAR10 k-SAT class whole benchmark (as competition).
rules SAT competitions establish winner solver solves instances,
ties broken selecting solver minimum PAR10.
4.2.2 E XPERIMENTAL R ESULTS



HRESHOLD B ENCHMARK

following, present comparative experimental results HScoreSAT competitors
benchmark.
Results Threshold Comp13 Benchmark:
Table 11 presents experimental results HScoreSAT competitors random kSAT instances phase transition SAT Competition 20134 . Since HScoreSAT based
CScoreSAT, first compare two solvers. shown Table 11, HSocreSAT solves
instances CScoreSAT instance classes. Overall, CScoreSAT solves 51 instances,
HScoreSAT solves 69 instances, 1.35 times many CScoreSAT does.
HScoreSAT solves instances probSAT Sattime2013. Overall, HScoreSAT
solves 69 instances, compared 61 probSAT Sattime2013 57 CCASat.
observation shows that, HScoreSAT similar performance probSAT random 4-SAT
5-SAT instances, similar performance Sattime2013 6-SAT 7-SAT instances.
Results Large-sized Threshold Benchmark:
mesure performance HScoreSAT random phase-transition k-SAT instances
accurately, additionally test HScoreSAT test set Large-sized Threshold
benchmark, compared Sattime2013 probSAT, top two solvers random
SAT track SAT Competition 2013.
4. seems machine slightly slower ones used SAT Competition 2013, Sattime2013, probSAT
CScoreSAT solved slightly fewer instances experiment competition. CCASat
participate SAT Competition 2013.

431

fiC AI , L UO & U

results presented Table 12. 4-SAT class, HScoreSAT probSAT solve
number instances, HScoreSAT less accumulative run time. 5-SAT 6SAT classes, HScoreSAT solves instances. Particularly, HScoreSAT shows significantly
superior performance solvers 6-SAT class, solves 9 instances,
Sattime2013 probSAT solve 4 instances. instance class HScoreSAT
give best performance 7-SAT. Nevertheless, instance class, HScreSAT
similar performance best solver Sattime2013, solving one less instance. whole
benchmark, HScoreSAT solves 40 instances, compared 26 28 instances Sattime2013
probSAT.
Instance
Class
4-SAT-v2000
(r = 9.931)
5-SAT-v550
(r = 21.117)
6-SAT-v300
(r = 43.37)
7-SAT-v150
(r = 87.79)


Sattime2013
#solved
par10
0
n/a
8
42147
4
46120
14
36433
26
43675

probSAT
#solved
par10
8
42197
9
41262
4
46132
7
43248
28
43209

HScoreSAT
#solved
par10
8
42181
10
40130
9
41523
13
37091
40
40231

Table 12: Experimental results Large-size Threshold benchmark. instance class
contains 50 instances solver executed instance cutoff
time 5000 seconds.

4.3 Experimental Analyses hscore2 Function
demonstrate effectiveness hscore2 function, test two alternative versions
HScoreSAT. two algorithms different HScoreSAT tie-breaking
mechanism greedy mode.
HScoreSAT1 breaks ties greedy mode preferring variable greatest
score2 ;
HScoreSAT2 breaks ties greedy mode preferring variable greatest age.
comparative results HScoreSAT two alternative versions displayed Table
13. clear table HScoreSAT superior performance alternatives
instance classes. careful observations show 4-SAT 5-SAT instances,
performance HScoreSAT2 HScoreSAT similar, significantly better
HScoreSAT1 . indicates age property suitable score2 tie-breaker 4SAT 5-SAT, almost good hscore2 tie-breaking. contrast, performance
432

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

Instance
Class
4-SAT
(r = 9.931)
5-SAT
(r = 21.117)
6-SAT
(r = 43.37)
7-SAT
(r = 87.79)


HScoreSAT1
#solved
par10
5
45062
7
43136
16
34255
24
26328
52
37195

HScoreSAT2
#solved
par10
13
37242
10
40293
16
34371
22
28128
61
35008

HScoreSAT
#solved
par10
14
36176
11
39296
19
31538
25
25275
69
33071

Table 13: Comparative results HScoreSAT two alternative solvers Threshold
Comp13 benchmark. solver executed one time instance, cutoff
time 5000 seconds.

HScoreSAT1 HScoreSAT 7-SAT similar, better HScoreSAT2 .
indicates score2 suitable age tie-breaker 7-SAT. Indeed,
experimental analysis gain intuition set hscore2 function relatively
small value 4-SAT 5-SAT instances, relatively large value 7-SAT. However,
6-SAT, alternatives cannot achieve performance close HScoreSAT.
4.4 Evaluation Huge Random k-SAT SAT Competition 2013
random SAT category SAT Competition 2013, two kinds instances. Besides
instances phase-transition threshold, also instances whose ratios close
phase transition time huge sizes. subsection, conduct
experiments evaluate performance solvers huge instances, compared
state-of-the-art solvers.
experimental results presented Table 14, show CScoreSAT clearly
best solver benchmark huge instances. CScoreSAT gives best performance
k-SAT instance classes except 4-SAT, especially solves 6-SAT 7-SAT instances
solvers. 4-SAT, CScoreSAT solves many instances probSAT PAR10
little probSATs. experimental results confirm good performance
CScoreSAT huge benchmark SAT Competition 2013, also solved huge
instances probSAT Sattime2013.
4.5 Boundary Ratios Performance CScoreSAT HScoreSAT
mentioned before, CScoreSAT good performance solving random k-SAT (k > 3)
instances ratios near phase-transition threshold, ratios large random
instances SAT Competition 2011. hand, HScoreSAT improved CScoreSAT
solving random k-SAT (k > 3) instances phase-transition threshold. Thus, conjecture
433

fiC AI , L UO & U

Instance
Class
4-SAT-v500000
(r [7.0, 9.5])
5-SAT-v250000
(r [15.0, 20.0])
6-SAT-v100000
(r [30.0, 40.0])
7-SAT-v50000
(r [60.0, 85.0])


CCASat
#solved
par10
4
17017
3
25106
2
33386
1
41693
10
29300

Sattime2013
#solved
par10
3
25205
2
33502
2
33710
1
41963
8
33595

probSAT
#solved
par10
5
8385
4
16710
2
33338
1
41669
12
25026

CScoreSAT
#solved
par10
5
9096
4
16535
3
25200
2
34167
14
21249

HScoreSAT
#solved
par10
5
9418
3
25116
2
33376
1
41683
11
27398

Table 14: Experimental results huge random k-SAT (k > 3) instances SAT
Competition 2013. instance class contains 6 instances solver executed
instance cutoff time 5000 seconds.

exists boundary ratio k-SAT, beyond HScoreSAT outperforms CScoreSAT.
subsection dedicated finding boundary ratios experiments.
experiment carried SAT Challenge 2012 benchmark, k-SAT 10
different ratios 12 instances ratio. Details bechmark found
benchmark description. run CScoreSAT HScoreSAT one time instance
cutoff time 1000 seconds, compare performance two solvers ratio.
comparison results presented Table 15. results suggest exists
boundary ratio r , beyond HScoreSAT gives better performance CScoreSAT.
especially clear 4-, 5- 7-SAT, clear 6-SAT HScoreSAT solves
instances CScoreSAT rations smaller 42.359 except one ratio
r = 42.696, CScoreSAT solves one instance. check whether result
outlier due single instance, conduct additional experiment execute solvers 10
times instance r = 42.696. experimental results show two solvers
close performance HScoreSAT succeeds 84 runs CScoreSAT succeeds 83 runs.

, r
give conjectured interval (rmin
max ) boundary ratio r k-SAT Table 16.
results suggest that, hybrid solver combining CScoreSAT HScoreSAT would
good performance k-SAT instances long clauses different ratios. However,
CScoreSAT HScoreSAT poor performance random 3-SAT instances. Hence,
two solvers hybrid solver participate SAT Competition 2014, competition
requires participating solver core solver two different solvers.
Instead, develop solver called CSCCSat, combines two solvers namely FrwCB (for
large sized instances) (Luo et al., 2014) DCCASat (for threshold instances) (Luo et al., 2014).
Note DCCASat improved HScoreSAT using double configuration checking
heuristic, also uses hscore hscore2 functions random k-SAT k > 3. SAT
Competition 2014, CSCCSat bronze medal random SAT track, especially gives
best performance threshold instances, indicating effectiveness scoring functions.
434

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

4-SAT
r=9
4-SAT
r=9.121
4-SAT
r=9.223
4-SAT
r=9.324
4-SAT
r=9.425
4-SAT
r=9.526
4-SAT
r=9.627
4-SAT
r=9.729
4-SAT
r=9.83
4-SAT
r=9.931

CScoreSAT
HScoreSAT
4.8(12)
6.5(12)
9.7(12)
10.8(12)
11.6(12)
18.6(12)
19.1(12)
29.1(12)
42.7(12)
63.8(12)
66.5(12)
72.2(12)
133(12)
192(12)
243(12)
212(12)
1252(11)
344(12)
149(12)
118(12)

5-SAT
r=20
5-SAT
r=20.155
5-SAT
r=20.275
5-SAT
r=20.395
5-SAT
r=20.516
5-SAT
r=20.636
5-SAT
r=20.756
5-SAT
r=20.876
5-SAT
r=20.997
5-SAT
r=21.117

CScoreSAT
HScoreSAT
222(12)
1060(11)
271(12)
2022(10)
1918(10)
3471(8)
2935(9)
6025(5)
6108(5)
7647(3)
4363(7)
6003(5)
3468(8)
4452(7)
5118(6)
3051(8)
5100(6)
4436(7)
2522(9)
1772(10)

6-SAT
r=40
6-SAT
r=40.674
6-SAT
r=41.011
6-SAT
r=41.348
6-SAT
r=41.685
6-SAT
r=42.022
6-SAT
r=42.359
6-SAT
r=42.696
6-SAT
r=43.033
6-SAT
r=43.37

CScoreSAT
HScoreSAT
4.1(12)
11.5(12)
8.6(12)
36.3(12)
29.1(12)
52.3(12)
76.6(12)
98.4(12)
83(12)
1827(10)
31(12)
1928(10)
1003(11)
227(12)
2757(9)
3528(8)
4418(7)
3647(8)
939(11)
165(12)

7-SAT
r=85
7-SAT
r=85.558
7-SAT
r=85.837
7-SAT
r=86.116
7-SAT
r=86.395
7-SAT
r=86.674
7-SAT
r=86.953
7-SAT
r=87.232
7-SAT
r=87.511
7-SAT
r=87.79

CScoreSAT
HScoreSAT
4345(7)
8346(2)
2767(9)
6773(4)
2666(9)
5065(6)
3435(8)
4296(7)
1902(10)
2797(9)
3499(8)
4340(7)
3415(8)
2646(9)
150(12)
135(12)
2589(9)
1054(11)
899(11)
74(12)

Table 15: Comparing CScoreSAT HScoreSAT ratio random k-SAT (k > 3) SAT
Challenge 2012 benchmark. solver executed one time instance, cutoff
time 1000 seconds. cell reports result CScoreSAT upper row
HScoreSAT lower row, form par10(#solved). color ratios gray
HScoreSAT performs better CScoreSAT. Note 6-SAT r = 42.696 results
seem little odd, conduct additional experiment executing solvers 10 times
instance, HScoreSAT succeeds 84 runs CScoreSAT succeeds 83 runs.

, r
(rmin
max )

4-SAT
(9.627,9.729)

5-SAT
(20.756,20.876)

6-SAT
(42.022,42.359)

7-SAT
(86.674,86.953)

Table 16: conjectured interval boundary ratio r . HScoreSAT worse performance
, better (or least competitive) performance
CScoreSAT ratios r rmin

ratios r rmax
, based experiments SAT Challenge 2012 benchmark.

5. Computation score2
algorithms employing scoring functions based score2 , CScoreSAT HScoreSAT,
computation score2 considerable impact efficiency. section,
investigate computation issues score2 . Particularly, propose cache-based
implementation analyze time complexity flip. also measure overhead
two algorithms experiments.
435

fiC AI , L UO & U

5.1 Implementation Complexity Computing score2
propose caching implementation computing variables score2 values, inspired
caching implementation computing variable scores. Typically, variable scores (or
score2 values) change search step; suggests rather recomputing variable
scores (or score2 values) step, efficient compute scores (or score2
values) search initialized, subsequently update scores (or score2 values)
affected variable flipped (Hoos & Sttzle, 2004).
caching implementation, score2 values variables computed search
initialized, subsequently updated flip. initialized computation score2
straightforward according definition score2 discussed. Comparatively,
procedure updating score2 values much interest.
facilitate describing procedure updating score2 values analyzing time
complexity, first introduce notations definitions below.
Given CNF formula F ,
variable x V (F ), CL(x) = {c|c clause F x appears c};
clause c F , c.num_true_lit number true literals c;
clause c F exactly two true literals, use true_lit_var(c) true_lit_var2(c)
record two corresponding variables two true literals c (these two notations
used pseudo-code sake formalization);
use x denote variable flipped current step;
n, m, k, r number variables, number clauses, maximum clause length,
clause-to-variable ratio.
Definition 5. Given clause c variable x, say contribution clause c score2 (x)
+1 flipping x would cause c transform 1-satisfied 2-satisfied, -1 flipping x would cause
c transform 2-satisfied 1-satisfied, 0 otherwise.
useful observation variable x V (F ), score2 (x) equals sum contributions
clauses it. Also, obvious clauses x appear always contribute 0
score2 (x).
describe detail procedure updating score2 values flip, whose pseudocode shown Algorithm 3. flipping x , according definition score2 , score2 (x )
changes opposite number (lines 1 21). essential part updating score2 values
variables share clauses x (since variables would change score2 values),
accomplished loop (lines 2-20). iteration loop, clause c CL(x )
considered necessary updates performed, according two different cases: either literal
x c becomes true literal, not. explain updates first case,
case understood similarly.
first case (i.e., literal x c becomes true literal), along flip x ,
c.num_true_lit increased 1. Suppose c.num_true_lit changes 1 t. neither 1
1 2, flipping x causes change variables score2 (easy see
definition score2 ). So, need consider following three cases.
436

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

Algorithm 3: updating score2 values flip step
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

org_score2 (x ) := score2 (x );
c CL(x )
literal x c becomes true literal
c.num_true_lit+=1;
c.num_true_lit=3
score2 (true_lit_var(c))+=1;
score2 (true_lit_var2(c))+=1;
else c.num_true_lit=2
xi c score2 (xi )=1;
else c.num_true_lit=1
xi c score2 (xi )+=1;
else
c.num_true_lit=1;
c.num_true_lit=2
score2 (true_lit_var(c))=1;
score2 (true_lit_var2(c))=1;
else c.num_true_lit=1
xi c score2 (xi )+=1;
else c.num_true_lit=0
xi c score2 (xi )=1;
score2 (x ) := org_score2 (x );

c.num_true_lit changes 2 3: flipping x , c two true literals, let us
denote corresponding variables y1 y2 . contributions c score2 (y1 )
score2 (y2 ) -1 flipping x . flipping x , c becomes 3-satisfied clause,
contributions c score2 (y1 ) score2 (y2 ) become 0. Hence, along
flipping x , changes score2 (y1 ) score2 (y2 ) 0-(-1)=+1 (lines 6-7).
variables c, either flip x , contributions c score2
values 0.
c.num_true_lit changes 1 2: flipping x , c one true literal, let
us denote corresponding variable y1 . contribution c score2 (y1 ) 0
flipping x , becomes -1 flipping x , indicating change -1 score2 (y1 ).
variables c (except x ), contributions c score2 values +1
flipping x , become 0 flipping x , indicating change -1 score2 values.
Therefore, variables c (except x ), along flipping x , changes
score2 values -1 (lines 8-9).
Note include x loop (line 9) order save computational consumption.
special update score2 (x ), change score2 (x ) line 1
line 21 impact effect.
437

fiC AI , L UO & U

c.num_true_lit changes 0 1: flipping x , variables c, contributions
c score2 values 0. flipping x , c becomes 1-satisfied (the true literals
corresponding variable x ), variables c (except x ), contributions c
score2 values become +1. Therefore, variables c (except x ), along
flipping x , changes score2 values +1 (lines 10-11).
complexity score2 updating procedure flip determined main loop
(lines 2-20). flipping variable x, |CL(x)| items main loop,
iteration three possible cases, first case requires 2 operations5 latter
two require (k) operations. Therefore, worst-case time complexity score2 updating
procedure flip (maxxV (F ) |CL(x)| max{2, k, k})=(maxxV (F ) |CL(x)| k).
uniform random k-SAT formulas clause-to-variable ratio r, totally k
literals variable expected mk/n = kr literals. is, variable x V (F )
expected appear kr clauses (when n approaches +, true probability almost
1), i.e., |CL(x)| kr. Therefore, complexity score2 updating procedure flip
becomes (kr k) = (k 2 r).
Fortunately, uniform random k-SAT formulas constant ratio r, k r
constants. Therefore, independent instance size, implementation score2 computation
achieves time complexity (1) search step, caching implementation
score computation (referring pages 272-273 (Hoos & Sttzle, 2004)).
5.2 Computational Overhead Computing score2
subsection, study computational overhead computing score2 CScoreSAT
HScoreSAT. carry experiments Threshold Comp13 benchmark figure
CPU time per 107 steps computing score2 percentage total CPU time solver
per 107 steps.

CScoreSAT
computing score2
percentage
HScoreSAT
computing score2
percentage

4-SAT
13.9
1.6
11.5%
14.3
1.6
11.2%

5-SAT
28.5
8.0
28.1%
28.7
8.4
29.3%

6-SAT
52.4
17.9
34.2%
53.2
17.8
33.5%

7-SAT
94.6
37.1
39.2%
95.1
36.9
38.8%

Table 17: CPU time consumption (in seconds) per 107 steps CScoreSAT HscoreSAT,
computing score2 , well ratios. results averaged instances
Threshold Comp13 benchmark.
investigation shows overhead computing score2 occupies considerable
percentage solvers whole run time, ranging 11% 40%. Nevertheless, since score2
critical solvers, price indeed pays off. observation reveals 90%
5. note true_lit_var(c) true_lit_var2(c) recorded initially accelerating updating variable scores, thus
need extra price maintain score2 updates.

438

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

solvers run time due flip function, two costly parts score score2
updates. Another interesting phenomenon percentage overhead caused score2
computation rises clause length increases, although score2 less used (for tie-breaking)
longer clauses (see Table 8). reason might clause length increases, portion
variables whose score2 values need updated increasing compared variables whose
scores need updated.

6. Summary Future Work
paper, proposed three new scoring functions based score2 improving SLS
algorithms random SAT instances long clauses. Despite simplicity, proposed
scoring functions effective, resulting SLS algorithms namely CScoreSAT
HscoreSAT show excellent performance random k-SAT instances long clauses.
First, combined score score2 properties design scoring function named cscore
(comprehensive score), aims improve greedy mode combining greediness lookahead greediness. also defined comprehensively decreasing variables accordingly.
proposed hscore function combining cscore diversification age, devoted
improving diversification mode. two scoring functions used develop
CScoreSAT algorithm. experiments show performance CScoreSAT exceeds
state-of-the-art SLS solvers orders magnitudes large random 5-SAT 7-SAT instances
near phase transition. Moreover, CScoreSAT significantly outperforms competitors random
k-SAT instances various ratios k {4, 5, 6, 7} SAT Challenge 2012.
improve CScoreSAT solving random k-SAT instances threshold ratio phase
transition, propose another scoring function called hscore2 , combines score2 age.
using hscore2 break ties adjust greedy mode accordingly, obtain HScoreSAT
algorithm. Experiments random k-SAT instances phase-transition threshold show
HScoreSAT significantly improves CScoreSAT outperforms state-of-the-art SLS algorithms.
Finally, score2 property key notion algorithms, also present
implementation details score2 computation, analyze complexity per flip
computational overhead.
future work, significant research issue improve SLS algorithms structured
instances score2 -based scoring functions. Furthermore, notions work simple
easily applied problems, constrained satisfaction graph search
problems.

Acknowledgement
work supported China National 973 Program 2010CB328103 2014CB340301,
National Natural Science Foundation China 61370072 61472369, ARC Grant
FT0991785. would like thank anonymous referees helpful comments
earlier versions paper, significantly improve quality paper.
439

fiC AI , L UO & U

References
Abram, A., Habet, D., & Toumi, D. (2014). Improving configuration checking satisfiable
random k-SAT instances. Proc. ISAIM-14.
Achlioptas, D. (2009). Random satisfiability. Handbook Satisfiability, pp. 245270.
Balint, A., Biere, A., Frhlich, A., & Schning, U. (2014). Improving implementation SLS
solvers SAT new heuristics k-sat long clauses. Proc. SAT-14, pp. 302
316.
Balint, A., & Frhlich, A. (2010). Improving stochastic local search SAT new probability
distribution. Proc. SAT-10, pp. 1015.
Balint, A., & Schning, U. (2012). Choosing probability distributions stochastic local search
role make versus break. Proc. SAT-12, pp. 1629.
Braunstein, A., Mzard, M., & Zecchina, R. (2005). Survey propagation: algorithm
satisfiability. Random Struct. Algorithms, 27(2), 201226.
Cai, S., & Su, K. (2012). Configuration checking aspiration local search SAT. Proc.
AAAI-12, pp. 334340.
Cai, S., & Su, K. (2013a). Comprehensive score: Towards efficient local search SAT long
clauses. Proc. IJCAI-13, pp. 489495.
Cai, S., & Su, K. (2013b). Local search Boolean Satisfiability configuration checking
subscore. Artif. Intell., 204, 7598.
Cai, S., Su, K., & Luo, C. (2013a). Improving walksat random k-satisfiability problem k >
3. Proc. AAAI-13, pp. 145151.
Cai, S., Su, K., Luo, C., & Sattar, A. (2013b). NuMVC: efficient local search algorithm
minimum vertex cover. J. Artif. Intell. Res. (JAIR), 46, 687716.
Cai, S., Su, K., & Sattar, A. (2011). Local search edge weighting configuration checking
heuristics minimum vertex cover. Artif. Intell., 175(9-10), 16721696.
Chieu, H. L., & Lee, W. S. (2009). Relaxed survey propagation weighted maximum
satisfiability problem. J. Artif. Intell. Res. (JAIR), 36, 229266.
Gent, I. P., & Walsh, T. (1993). Towards understanding hill-climbing procedures SAT.
Proc. AAAI-93, pp. 2833.
Hoos, H. H., & Sttzle, T. (2004). Stochastic Local Search: Foundations & Applications. Elsevier
/ Morgan Kaufmann.
KhudaBukhsh, A. R., Xu, L., Hoos, H. H., & Leyton-Brown, K. (2009). Satenstein: Automatically
building local search SAT solvers components. Proc. IJCAI-09, pp. 517524.
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization simulated annealing. Science,
220, 671680.
Kirkpatrick, S., & Selman, B. (1994). Critical behavior satisfiability random boolean
formulae. Science, 264, 12971301.
Kroc, L., Sabharwal, A., & Selman, B. (2010). empirical study optimal noise runtime
distributions local search. Proc. SAT-10, pp. 346351.
440

fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES

Li, C. M., Huang, C., & Xu, R. (2014). Balance intensification diversification: unity
opposites. Proc. SAT Competition 2014: Solver Benchmark Descriptions, pp.
1011.
Li, C. M., & Huang, W. Q. (2005). Diversification determinism local search satisfiability.
Proc. SAT-05, pp. 158172.
Li, C. M., & Li, Y. (2012). Satisfying versus falsifying local search satisfiability - (poster
presentation). Proc. SAT-12, pp. 477478.
Luo, C., Cai, S., Wu, W., Jie, Z., & Su, K. (2014). CCLS: efficient local search algorithm
weighted maximum satisfiability. IEEE Transactions Computers, press.
Luo, C., Cai, S., Wu, W., & Su, K. (2013). Focused random walk configuration checking
break minimum satisfiability. Proc. CP-13, pp. 481496.
Luo, C., Cai, S., Wu, W., & Su, K. (2014). Double configuration checking stochastic local search
satisfiability. Proc. AAAI-14, pp. 27032709.
Luo, C., Su, K., & Cai, S. (2012). Improving local search random 3-SAT using quantitative
configuration checking. Proc. ECAI-2012, pp. 570575.
Mertens, S., Mzard, M., & Zecchina, R. (2006). Threshold values random k-SAT cavity
method. Random Struct. Algorithms, 28(3), 340373.
Mzard, M. (2003). Passing messages disciplines. Science, 301, 16851686.
Pham, D. N., Thornton, J., Gretton, C., & Sattar, A. (2007). Advances local search
satisfiability. Australian Conference Artificial Intelligence, pp. 213222.
Prestwich, S. D. (2005). Random walk continuously smoothed variable weights. Proc.
SAT-05, pp. 203215.
Selman, B., Kautz, H. A., & Cohen, B. (1994). Noise strategies improving local search. Proc.
AAAI-94, pp. 337343.
Thornton, J., Pham, D. N., Bain, S., & Ferreira Jr., V. (2004). Additive versus multiplicative clause
weighting SAT. Proc. AAAI-04, pp. 191196.
Tompkins, D. A. D., Balint, A., & Hoos, H. H. (2011). Captain jack: New variable selection
heuristics local search SAT. Proc. SAT-11, pp. 302316.
Tompkins, D. A. D., & Hoos, H. H. (2010). Dynamic scoring functions variable expressions:
New SLS methods solving SAT. Proc. SAT-10, pp. 278292.
Xu, L., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2008). SATzilla: Portfolio-based algorithm
selection SAT. J. Artif. Intell. Res. (JAIR), 32, 565606.

441

fiJournal Artificial Intelligence Research 51 (2014) 555-577

Submitted 05/14; published 11/14

Iterative Plan Construction Workflow Satisfiability Problem
David Cohen
Jason Crampton
Andrei Gagarin
Gregory Gutin
Mark Jones

D.C OHEN @ RHUL . AC . UK
JASON .C RAMPTON @ RHUL . AC . UK
NDREI .G AGARIN @ RHUL . AC . UK
G.G UTIN @ RHUL . AC . UK
ARK .J ONES @ RHUL . AC . UK

Royal Holloway, University London, UK

Abstract
Workflow Satisfiability Problem (WSP) problem practical interest arises whenever tasks need performed authorized users, subject constraints defined business
rules. required decide whether exists plan assignment tasks authorized
users constraints satisfied. natural see WSP subclass
Constraint Satisfaction Problem (CSP) variables tasks domain set
users. makes WSP distinctive number tasks usually small compared number users, appropriate ask constraint languages WSP
fixed-parameter tractable (FPT), parameterized number tasks.
novel approach WSP, using techniques CSP, enabled us design generic
algorithm FPT several families workflow constraints considered literature.
Furthermore, prove union FPT languages remains FPT satisfy simple compatibility condition. Lastly, identify new FPT constraint language, user-independent constraints, includes many constraints interest business processing systems.
demonstrate generic algorithm provably optimal running time (2k log k ),
language, k number tasks.

1. Introduction
workflow formalises business process. collection interrelated tasks performed
users order achieve objective. many situations, wish restrict users
perform certain tasks. particular, may wish specify lists users authorized
perform workflow tasks. Additionally, may wish either particular
requirements business logic security requirements prevent certain combinations
users performing particular combinations tasks (Crampton, 2005). constraints include
separation-of-duty (also known two-man rule), may used prevent sensitive
combinations tasks performed single user, binding-of-duty, requires
particular combination tasks executed user. use constraints workflow
management systems enforce security policies studied extensively last fifteen
years; example see work Bertino, Ferrari, Atluri (1999), Crampton (2005) Wang
Li (2010).
1.1 Workflow Satisfiability Problem
possible combination constraints authorization lists unsatisfiable,
sense exist assignment users tasks contraints satisfied
c
2014
AI Access Foundation. rights reserved.

fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES

every task performed authorized user; perhaps minimal example requirement
two tasks performed user intersection authorization lists
tasks empty. plan satisfies constraints allocates authorized user task said
valid. Workflow Satisfiability Problem (WSP) takes workflow specification input
returns valid plan one exists null value otherwise. important determine whether
business process satisfiable not, since unsatisfiable one never completed without
violating security policy encoded constraints authorization lists. Wang Li (2010)
shown, reduction G RAPH C OLORING, WSP NP-hard subclass
CSP, even consider binary separation-of-duty constraints. Nevertheless, practical
applications WSP, require solving algorithm efficient possible (Crampton &
Gutin, 2013, 2.2).
Many hard problems become less complex natural parameter instance bounded.
Hence, say problem input size n parameter k fixed-parameter tractable (FPT)
admits algorithm running time O(f (k)nd ), constant independent n k,
f computable function depending k.1
Wang Li (2010) first observe fixed-parameter algorithmics appropriate
way study WSP, number tasks usually small often much smaller
number users. (The literature directly support assumption, although widely-cited
study, Schaad, Moffett, & Jacob, 2001, found number users exceeds job functions,
roles, multiplicative factor around 25; finding confirmed recent followup study, Jayaraman, Ganesh, Tripunitara, Rinard, & Chapin, 2011. workflow specification
usually concerned particular business objective involve small number roles.
Taking roles proxy tasks, seems reasonable assume number users
order magnitude greater number tasks.) believe, therefore, appropriate
extend work initiated Wang Li use fixed parameter algorithms solving
WSP parameterized number tasks, and, particular, ask constraint languages
fixed parameter tractable.
Wang Li (2010) proved that, general, WSP W[1]-hard thus highly unlikely
admit fixed-parameter algorithm. also showed WSP FPT consider
separation-of-duty binding-of-duty constraints. Crampton, Gutin, Yeo (2013) obtained significantly faster fixed-parameter algorithms applicable regular constraints, thereby
including cases shown FPT Wang Li. work, recent research,
demonstrated existence fixed-parameter algorithms WSP presence constraint types (Crampton, Crowston, Gutin, Jones, & Ramanujan, 2013; Crampton & Gutin, 2013).
define WSP formally introduce number different constraint types, including regular
constraints, Section 2.
use notation, suppresses polynomial factors.
is,
g(n, k, m) = (h(n, k, m)) exists polynomial q(n, k, m) g(n, k, m) =
O(q(n, k, m)h(n, k, m)). particular, FPT algorithm one runs time (f (k))
computable function f depending k.

1. introduction fixed-parameter algorithms complexity found in, example, books Downey
Fellows (2013), Niedermeier (Niedermeier, 2006).

556

fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM

1.2 Relation WSP CSP
Constraint Satisfaction Problem (CSP) general paradigm expressing, declarative
format, problems variables assigned values domain. assignments
constrained restricting allowed simultaneous assignments sets variables.
model useful many application areas including planning, scheduling, frequency assignment
circuit verification (Rossi, van Beek, & Walsh, 2006). CSP community well-established
research community dedicated finding effective solution techniques CSP (Dechter, 2003).
CSP NP-hard, even binary not-equals constraints allowed domain
three elements, reduce G RAPH 3-C OLORING CSP. 2 Hence, considerable
effort made understand effect restricting type allowed constraints. Recently significant progress towards completion research program
strong evidence support algebraic dichotomy conjecture Bulatov, Jeavons
Krokhin (2005), characterising precisely kinds constraint language lead polynomial
solvability.
worth noting WSP subclass CSP variable (called task
WSP terminology) arbitrary unary constraint (called authorization) assigns
possible values (called users) s; called conservative CSP. Note, however,
usually CSP number variables much larger number values, WSP
number tasks usually much smaller number users. important remember
WSP use term constraint authorizations define special
types constraints, extend types authorizations, remain arbitrary.
1.3 Outline Paper
novel approach WSP using techniques CSP, characterising types constraints
constraint languages particular characteristics, enables us generalise unify existing
algorithms. So, paper, first time, rather considering algorithms specific
constraints, design generic algorithm fixed-parameter algorithm several families
workflow constraints considered literature. particular introduce notions userindependent constraints, subsume number well-studied constraint types WSP
literature, including regular constraints studied Crampton et al. (2013).
generic algorithm builds plans incrementally, discarding partial plans never satisfy
constraints. based naive algorithm, presented Section 2.2. naive algorithm
stores far information required solve WSP, running time better
exhaustively searching valid plan.
generic algorithm uses general classic paradigm: retain little information possible every step algorithm. paradigm used classical polynomial-time algorithms Gaussian elimination solving systems linear equations constraint propagation
algorithms (used, example, solve 2SAT polynomial time). generic algorithm uses
paradigm problem-specific way, based concepts extension-equivalence, planindistinguishability patterns, enabling us retain single pattern equivalence class
indistinguishable plans. Extension-equivalence plan encodings described Section 3.
way solution constructed algorithm quite unusual accumulation
2. Wang Lis NP-hardness result WSP thus restatement well-known result CSP.

557

fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES

(representatives of) set solutions goes along users (i.e., values CSP), along tasks
(i.e., variables CSP).
analyze running time algorithm introduce notion diversity (see Definition 6). notion reminiscent pathwidth (measures taken prefixes largest
outcome diversity) difference diversity based number equivalence classes, hiding actual structure behind scenes. approach might also useful
structural analysis hypergraphs.
Section 4, describe pattern-based algorithm demonstrate fixedparameter algorithm WSP user-independent constraints. show running time
algorithm (2k log k ) WSP user-independent constraints
algorithm running time (2o(k log k) ) WSP user-independent constraints unless
Exponential Time Hypothesis3 (ETH) fails. Thus, unlike WSP regular constraints
(and problems studied Bodlaender, Cygan, Kratsch, & Nederlof, 2013; Fomin, Lokshtanov, &
Saurabh, 2014), WSP user-independent constraints highly unlikely admit algorithm
running time (2O(k) ). show generic algorithm interest constraints
user-independent, prove generic algorithm single-exponential algorithm
constraint language obtained equivalence relation set users.
Section 5 show generic algorithm deal unions constraint languages.
leads generalisation result user-independent constraints. Section 6 discuss
results computational experiments using implementation algorithm (discussed
full detail work Cohen, Crampton, Gagarin, Gutin, & Jones, 2014). brief conclusion
given Section 7.

2. Background
define workflow schema tuple (S, U, A, C), set tasks workflow,
U set users, = {A(s) : S}, A(s) U authorization list task s,
C set workflow constraints. workflow constraint pair c = (L, ), L
set functions L U : L scope constraint; specifies assignments
elements U elements L satisfy constraint c.
Given X U , plan function : X. Given workflow constraint (L, ),
X U , plan : X satisfies (L, ) either L \ 6= |L =
. plan : X eligible satisfies every constraint C. plan : X
authorized (s) A(s) . plan valid authorized eligible. plan
: U called complete plan. algorithm solve WSP takes workflow schema
(S, U, A, C) input outputs valid, complete plan, one exists (and null, otherwise).
running example, consider following instance WSP.
Instance 1. task set = {s1 , . . . , s4 } user set U = {u1 , . . . , u6 }. authorization
lists follows (where tick indicates given user authorized given task):
3. Exponential Time Hypothesis claims algorithm running time (2o(n) ) 3SAT n variables (Impagliazzo, Paturi, & Zane, 2001).

558

fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM

s1
s2
s3
s4

u1
4

u2
4
4
4
4

u3

u4

u5

u6

4
4

4
4

4
4

4

constraints follows: s1 s2 must assigned user; s2 s3 must
assigned different users; s3 s4 must assigned different users; s1 s4 must
assigned different users.
Example 1 illustrates meanings eligible, complete authorised plans context
Instance 1.
Example 1. following table gives assignments four plans, 1 , 2 , 3 , 4 :

1
2
3
4

s1
u1
u1
u1
u2

s2
u2
u1
u2

s3
u4
u4
u4
u4

s4
u5
u5
u5
u5

Authorized
4
4
4

Eligible
4
4
4

Complete
4
4
4

1 complete plan authorized eligible, s1 s2 assigned
different users.
2 complete plan eligible authorized, u1 authorized s2 .
3 plan authorized eligible, therefore valid. However, 3
complete plan assignment s2 .
4 complete plan eligible authorized. Thus 4 valid complete plan,
therefore solution.
algorithm runs instance (S, U, A, C) WSP, measure running
time terms n = |U |, k = |S|, = |C|. (The set authorization lists consists k
lists size n, need consider size separately measuring
running time.) say algorithm runs polynomial time running time
p(n, k, m), p(n, k, m) polynomial n, k m.
2.1 WSP Constraints
paper interested complexity WSP workflow constraint language
(the set permissible workflow constraints) restricted. section introduce constraint
types interest. practical applications real world workflows.
assume constraints authorizations checked polynomial time.
means takes polynomial time check whether plan authorized, eligible valid.
correctness algorithm unaffected assumption, choosing constraints
checkable polynomial time would naturally affect running time.
559

fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES

2.1.1 C ONSTRAINTS EFINED B INARY R ELATION
Constraints two tasks, s0 , represented form (s, s0 , ), binary
relation U (Crampton, 2005). plan satisfies constraint (s) (s0 ). Writing
= denote relation {(u, u) : u U } 6= denote relation {(u, v) : u, v U, u 6= v},
separation-of-duty binding-of-duty constraints may represented form (s, s0 , 6=)
(s, s0 , =), respectively. Crampton et al. (2013) considered constraints ,
equivalence relation defined U . practical example workflow constraints
equivalence relation partitions users different departments: constraints could
enforce two tasks performed members department. Constraints
restricted singleton tasks also considered (Crampton et al., 2013; Wang & Li, 2010):
plan satisfies constraint form (S 0 , 00 , ) tasks s0 0 s00 00
(s0 ) (s00 ).
2.1.2 C ARDINALITY C ONSTRAINTS
tasks-per-user counting constraint form (t` , tr , ), 1 6 t` 6 tr 6 k S.
plan satisfies (t` , tr , ) user performs either tasks t` tr tasks. Tasksper-user counting constraints generalize cardinality constraints widely adopted
WSP community (American National Standards Institute, 2004; Bertino, Bonatti, & Ferrari,
2001; Joshi, Bertino, Latif, & Ghafoor, 2005; Sandhu, Coyne, Feinstein, & Youman, 1996).
2.1.3 R EGULAR C ONSTRAINTS
say C regular satisfies following condition: partition S1 , . . . , Sp
1
every
Sp [p] exists eligible complete plan user u (u) = Si ,
plan i=1 (Si ui ), ui distinct, eligible. Regular constraints extend set
constraints considered Wang Li (2010). Crampton et al. (2013) show following
constraints regular: (S 0 , 00 , 6=); (S 0 , 00 , =), least one sets 0 , 00 singleton;
tasks-per-user counting constraints form (t` , tr , ), t` = 1.
2.1.4 U SER -I NDEPENDENT C ONSTRAINTS
Many business rules concerned identities users complete set tasks;
concerned relationships users. Accordingly, say constraint (L, ) user-independent whenever : U U permutation,
. obvious example user-independent constraint requirement two
tasks performed different users (separation-of-duty). complex example suppose
most/at least/exactly p users required complete sensitive set tasks (cardinality
constraints), p usually small, i.e., 1, 2, 3 so. substantial literature constraints method specifying enforcing business rules (for example, Gligor, Gavrila, &
Ferraiolo, 1998; Simon & Zurko, 1997), including work researchers SAP IBM (for example, Basin, Burri, & Karjoth, 2014; Wolter & Schaad, 2007). widely studied constraints
cardinality constraints separation-of-duty, form part ANSI standard rolebased access control (American National Standards Institute, 2004), developed US National
Institute Standards Technology (NIST). short, literature relevant standards suggest user-independent constraints interest business processes workflow
560

fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM

management systems. definition user-independent includes constraints defined
ANSI RBAC standard many more.
Every regular constraint user-independent, many user-independent constraints
regular. Indeed, constraints type (S 0 , 00 , =) user-independent, necessarily
regular (Crampton et al., 2013). Many counting constraints Global Constraint Catalogue (Beldiceanu, Carlsson, & Rampon, 2012) user-independent, regular. particular, constraint NVALUE, bounds number users performing set
tasks, user-independent regular. Note, however, constraints form (s0 , s00 , )
(s0 , s00 , 6) user-independent general.
important note authorization lists, fundamental access control
system, viewed unary constraints, certainly user-independent. presence
user-independent constraints authorization lists workflow specification makes
WSP challenging.
2.2 Naive Algorithm
main aim section present simple algorithm (Algorithm 1) solve
instance WSP. running time algorithm slightly worse brute-force algorithm, algorithms basic structure provides starting point develop
efficient algorithm.
need introduce additional notation terminology.
Let : X plan S, X U . let TASK() = U SER() = X.
important generic algorithm TASK() U SER() given explicit parts .
particular, set U SER() may different set users assigned task . is,
user u U SER() without task (s) = u. worth observing
TASK() may empty (because may allocate tasks users X).
u U , (T u) denotes plan : {u} (s) = u .
Two functions f1 : D1 E1 f2 : D2 E2 disjoint D1 D2 = E1 E2 = .
union two disjoint functions f1 : D1 E1 , f2 : D2 E2 function f = f1 f2
f : D1 D2 E1 E2 f (d) = fi (d) Di , {1, 2}. Let g : E
h : E F functions. h g denotes composite function F
h g(d) = h(g(d)) D. integer p > 0, set [p] = {1, 2, . . . , p}.
Proposition 1. Let (S, U, A, C) instance WSP, n = |U |, k = |S| = |C|.
(S, U, A, C) solved time ((n + 1)k ) Algorithm 1.
Proof. Let u1 , . . . , un ordering U , let Ui = {u1 , . . . , ui } [n].
[n] turn, construct set plans U SER() = Ui valid.
set n contains plan TASK() = S, (S, U, A, C) solution; otherwise,
plan solution (S, U, A, C).
Algorithm 1 shows construct sets . hard verify contains exactly
every valid plan U SER() = Ui , i. implies correctness algorithm.
remains analyse running time.
[n] S, i|T | valid plans U SER() =
Ui , TASK() = . construct 1 , need consider plans U SER() = U1 ,
exactly 2k plans. plan decide polynomial time whether add 1 .
561

fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES

Algorithm 1: Naive solution procedure WSP
input : instance (S, U, A, C) WSP
1 Construct ordering u1 , . . . , un U ;
2 Set 1 = ;
3 foreach
4
Set = (T u1 );
5
eligible u1 A(s)
6
Set 1 = 1 {};
7
end
8 end
9 Set = 1;
10 < n
11
Set i+1 = ;
12
foreach 0
13
foreach \ TASK( 0 )
14
ui+1 A(s)
15
Set = 0 (T ui+1 );
16
eligible
17
Set i+1 = i+1 {};
18
end
19
end
20
end
21
end
22
Set = + 1;
23 end
24 foreach n
25
TASK() =
26
return ;
27
end
28 end
29 return NULL ;

construct i+1 [n 1], need consider every pair ( 0 , ) 0
\ TASK( 0 ). Consider pair ( 0 , ), 0 (S 0 , Ui )-plan 0 S,
0
0
\ 0 . Thus i|S | possibilities
0 ,
2|S||S | choices . Thus,
P
P
0
0
k
k
total number pairs given 0 i|S | 2|S||S | = j=0 j ij 2kj = (i + 2)k .
pair ( 0 , ) decide whether
add 0 (T ui+1 ) i+1 polynomial time. Thus,
P
k

k

k
construct takes time ( n1
i=0 (i + 2) ) = (n(n + 1) ) = ((n + 1) ).

Algorithm 1 inefficient even small k, due fact contains valid plans
U SER( 0 ) = {u1 , . . . , ui }. show next section necessary store
much information solve WSP.

0

562

fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM

3. Plan-Indistinguishability Relations
first introduce notion extension-equivalence, defined equivalence relation set
plans. Informally, relation enables us keep single member equivalence class
building plans incrementally.
Definition 1. Given instance (S, U, A, C) WSP, two eligible plans 1 2 , define
1 2 following conditions hold:
1. U SER(1 ) = U SER(2 ) TASK(1 ) = TASK(2 );
2. 1 0 eligible 2 0 eligible, plan 0 disjoint 1 2 .
equivalence relation set eligible plans, say 1 2 extensionequivalent 1 2 .
Example 2. Consider Instance 1.
Let 1 : {s3 , s4 } {u2 , u4 } function 1 (s3 ) = u2 1 (s4 ) = u4 . Let
2 : {s3 , s4 } {u2 , u4 } function 2 (s3 ) = u4 2 (s4 ) = u2 .
plans 1 2 eligible, U SER(1 ) = U SER(2 ) TASK(1 ) =
TASK(2 ). plan 0 disjoint 1 2 , plan 1 0 satisfy constraints
(s2 , s3 , 6=), (s1 , s4 , 6=). Thus 1 0 eligible 0 eligible. Similarly, 2 0
eligible 0 eligible. Thus 1 0 eligible 2 0 eligible,
1 2 extension-equivalent.
Suppose polynomial time algorithm check whether two eligible plans
extension-equivalent. Algorithm 1, could keep track one plan equivalence class: constructing , add 2 1 extension-equivalent
2 already ; construct i+1 , may use 1 proxy 2 . number
extension-equivalent classes small compared number plans, worst-case running
time algorithm may substantially lower Algorithm 1.
Unfortunately, necessarily easy decide two eligible plans extension-equivalent,
approach practical. However, always refine4 extension-equivalence equivalence relation equivalence easy determine. example, identity equivalence
relation plan equivalent refinement.
refined equivalence relation may equivalence classes extensionequivalence, substantially fewer identity relation, may obtain better running
time naive algorithm.
Definition 2. Given instance (S, U, A, C) WSP, let set eligible plans
let equivalence relation refining extension-equivalence . say planindistinguishability relation (with respect C) if, eligible 1 , 2 1 2 ,
plan 0 disjoint 1 2 1 0 eligible, 1 0 2 0 .
Example 3. Let identity relation plans. is, 1 2 U SER(1 ) =
U SER(2 ), TASK(1 ) = TASK(2 ), 1 (s) = 2 (s) U SER(1 ).
4. equivalence relation 2 refinement equivalence relation 1 every equivalence class 2 subset
equivalence class 1 .

563

fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES

plan-indistinguishability relation. shows every plan-indistinguishability relation
extension-equivalence relation. Indeed, plans given Example 2 extension-equivalent
identical.
Recall refined extension-equivalence since may hard determine whether two
eligible plans extension-equivalent. therefore natural assume following:
Assumption 1. Given plan-indistinguishability relation , takes polynomial time check
whether two eligible plans equivalent .
correctness algorithms depend assumption. However, poor choice
plan-indistinguishability relation could affect running times.
describe appropriate plan-indistinguishability relations constraints
using. case determining two eligible plans equivalent take polynomial
time.
3.1 Plan-Indistinguishability Relation User-Independent Constraints
Lemma 1. Suppose constraints user-independent, let ui relation 1 ui
2
1. U SER(1 ) = U SER(2 ) TASK(1 ) = TASK(2 );
2. s, TASK(1 ), 1 (s) = 1 (t) 2 (s) = 2 (t).
ui plan-indistinguishability relation set eligible plans.
Proof. definition user-independent constraints, eligible plan : U U
permutation, also eligible. Suppose 1 ui 2 , let = TASK(1 )
X = U SER(1 ). Let 0 : 1 (T ) 2 (T ) function 0 (1 (t)) = 2 (t)
task t. Let 00 : X \ 1 (T ) X \ 2 (T ) arbitrary bijection (note |1 (T )| = |2 (T )|
Condition 2 ui ). Let = 0 00 . permutation 2 = 1 . Thus 1
eligible 2 eligible.
consider two eligible plans 1 , 2 1 ui 2 , plan 0 disjoint 1
2 . First show 1 0 ui 2 0 . clear U SER(1 0 ) = U SER(2 0 )
TASK(1 0 ) = TASK(2 0 ). s, U SER(1 0 ), (1 0 )(s) = (1 0 )(t),
either s, TASK( 0 ), case (2 0 )(s) = (2 0 )(t) trivially, s,
TASK(1 ), case 2 (s) = 2 (t) since 1 ui 2 , hence (2 0 )(s) =
(2 0 )(t). Thus (1 0 )(s) = (1 0 )(t) (2 0 )(s) = (2 0 )(t) and, similar
argument, converse holds. Thus 1 0 ui 2 0 . Furthermore, follows argument
first paragraph 1 0 eligible 2 0 eligible. Thus, condition
Definition 2 second condition Definition 1 hold.
first condition ui trivially satisfies first condition Definition 1. Thus, ui satisfies
conditions plan-indistinguishability relation.
Example 4. Consider instance WSP users u1 , . . . u6 tasks s1 , . . . , s6
constraints user-independent. Let ui plan-indistinguishability relation given
Lemma 1. Let c1 constraint scope {s2 , s3 , s4 , s5 } c1 satisfied
even number users assigned tasks {s2 , s3 , s4 , s5 }. Let c2 constraint scope
564

fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM

{s1 , s3 , s4 , s6 } c2 satisfied either s1 s3 assigned different users,
s4 s6 assigned different users. Suppose c1 c2 constraints whose
scope contains tasks {s1 , s2 , s3 } {s4 , s5 , s6 }.
consider plans 1 , 2 : {s1 , s2 , s3 } {u1 , u2 , u3 , u4 } 1 (s1 ) =
u1 , 1 (s2 ) = u2 , 1 (s3 ) = u1 , 2 (s1 ) = u3 , 2 (s2 ) = u4 , 2 (s3 ) = u3 , suppose
1 , 2 eligible. 1 2 equivalent ui .
Observe plan 0 disjoint 1 2 , 1 0 eligible 2 0
eligible. 1 2 assign two users {s2 , s3 }, 0 must assign two users {s4 , s5 }
order satisfy c1 . 1 2 assign s1 s3 user, 0 must assign s4 s5
different users order satisfy c2 . long conditions satisfied, 0 satisfies
constraints scope {s4 , s5 , s6 }, 1 0 2 0 eligible.
3.2 Plan-Indistinguishability Relation Equivalence Relation Constraints
Recall given binary relation U , constraint form (si , sj , ) satisfied plan
(si ) (sj ). Recall constraints user-independent general.
Lemma 2. Suppose equivalence relation U . Let V1 , . . . , Vl equivalence classes
U . Suppose constraints form (si , sj , ) (si , sj , 6). Let e relation
1 e 2
1. U SER(1 ) = U SER(2 ) TASK(1 ) = TASK(2 );
2. equivalence classes Vj Vj U SER(1 ) 6= Vj \ U SER(1 ) 6= ,
TASK(1 ), 1 (s) Vj 2 (s) Vj .
e plan-indistinguishability relation.
Proof. clear e satisfies first condition Definition 1. suppose 1 , 2 eligible
plans 1 e 2 , let 0 plan disjoint 1 2 . first show 1 0
eligible 2 0 eligible.
Suppose 1 0 eligible. Consider two tasks t, t0 TASK(2 0 ). {t, t0 } TASK( 0 ),
2 0 falsify constraint t0 since equal 1 0 restricted
{t, t0 } 1 0 eligible. {t, t0 } TASK(2 ), 2 0 break constraints
since 2 eligible.
may assume TASK(2 ), t0 TASK( 0 ). definition, (2 0 )(t) (2 0 )(t0 )
exists j [l] 2 (t), 0 (t0 ) Vj . Vj U SER(2 ) 6=
Vj \ U SER(2 ) 6= . Therefore, definition e , 1 (s) Vj 2 (s) Vj ,
TASK(1 ). particular, 1 (t) Vj , (1 0 )(t) (1 0 )(t0 ). similar argument,
(1 0 )(t) (1 0 )(t0 ) (2 0 )(t) (2 0 )(t0 ). Therefore, every constraint
satisfied (1 0 ) satisfied (2 0 ). Therefore 1 0 eligible
2 0 , similar argument converse holds.
remains show 1 0 e 2 0 . clear user task sets same.
user set, sets {Vj : Vj U SER(1 0 ) 6= , Vj \ U SER(1 0 ) 6= }
{Vj : Vj U SER(2 0 ) 6= , Vj \ U SER(2 0 ) 6= } same. Furthermore,
Vj set TASK(1 0 ), (1 0 )(s) Vj (2 0 )(s) Vj , either
TASK(1 ), case Vj U SER(1 ) 6= , Vj \ U SER(1 ) 6= 2 (s) Vj ,
565

fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES

TASK( 0 ), case (2 0 )(s) = 0 (s) = (1 0 )(s). similar argument,
(2 0 )(s) Vj , (1 0 )(s) Vj . Thus 1 0 e 2 0 .
Example 5. Let equivalence relation users equivalence classes
{u1 }, {u2 }, {u3 , u4 , u5 }, {u6 , u7 , u8 }. Consider instance WSP users u1 , . . . , u8
tasks s1 , . . . , s6 constraints form (si , sj , ) (si , sj , 6). Let e
plan-indistinguishability relation given Lemma 2. Suppose constraints whose scope
contains tasks {s1 , s2 , s3 } {s4 , s5 , s6 } constraints (s1 , s5 , 6), (s2 , s5 , )
(s2 , s6 , 6).
consider plans 1 , 2 : {s1 , s2 , s3 } {u1 , u2 , u3 , u4 } 1 (s1 ) =
u1 , 1 (s2 ) = u3 , 1 (s3 ) = u3 , 2 (s1 ) = u2 , 2 (s2 ) = u3 , 2 (s3 ) = u4 , suppose
1 , 2 eligible. 1 2 equivalent e .
Observe plan 0 disjoint 1 2 , 1 0 eligible 2 0
eligible. -equivalence class members {u1 , u2 , u3 , u4 } members
{u1 , u2 , u3 , u4 } class {u3 , u4 , u5 }. 1 2 assign members {u3 , u4 } exactly
set {s2 , s3 }. Thus plan 0 disjoint 1 2 , 1 0 2 0 satisfy
constraint (s1 , s5 , 6) whatever 0 assigns s5 . satisfy (s2 , s5 , ) 0 assigns
s5 u5 , satisfy (s2 , s6 , 6) 0 assign s6 u5 . long
conditions satisfied, 0 satisfies constraints scope {s4 , s5 , s6 }, 1 0
2 0 eligible.

4. Generic Algorithm WSP
follows, X U, S, let [X, ] denote set eligible plans
U SER() = X TASK() = . section introduce algorithm works
similar way Algorithm 1, except instead storing valid plans particular set users
tasks, construct [X, ]-representative sets task set certain user sets X.
definition, equivalence classes plan-indistinguishability relation necessarily partition
[X, ]. Hence equivalence class representation form (X, T, ),
dependent constraint language. remainder section describe algorithm
give examples representations.
4.1 Encodings Patterns
generic algorithm, construct plans iteratively, using one plan
equivalence class plan-indistinguishability relation. running time algorithm
depend number equivalence classes relation, certain sets plans.
ensure sets equivalence classes ordered therefore searched sorted efficiently,
introduce notion encodings patterns. Loosely speaking, encoding function
maps plans -equivalence class element (the pattern plans).
encodings ensure logarithmic-time access insertion operations representative set plans,
rather linear time naive method would allow.
Note use encodings patterns necessary fixed-parameter
tractability results; problems could solved without use patterns encodings
fixed-parameter time, function k would grow quickly.

566

fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM

Definition 3. Given instance (S, U, A, C) WSP plan-indistinguishability relation
, let set plans. Let PAT set consider function ENC : PAT.
X U, S, let PAT[X, ] = ENC([X, ]). say ENC -encoding (or
encoding ) if, X U, 1 , 2 [X, ],
1. ENC(1 ) = ENC(2 ) 1 2 ;
2. ENC(1 ) calculated time polynomial n, k, m;
3. exists linear ordering PAT[X, ] that, p, p0 PAT[X, ], decide
whether p p0 time polynomial n, k, m.
elements PAT called -patterns. ENC() = p say p -pattern .
second third conditions Definition 3 ensure may use encodings organise
plans reasonable time. clear context, refer -encoding
encoding -patterns patterns.
note complexity consequences Definition 3 following:
Proposition 2. encoding plan-indistinguishability relation set patterns PAT ,
assigning patterns PAT nodes balanced binary tree, perform following
two operations time (log(|PAT |)): (i) check whether p PAT , (ii) insert pattern
p
/ PAT PAT .
Proof. Recall comparisons polynomial n, k, m. result follows wellknown properties balanced binary trees (e.g., see (Cormen, Stein, Rivest, & Leiserson, 2001)).
show plan-indistinguishability relations given previous section
encodings. first need define lexicographic order.
Definition 4. Given totally ordered set (A, ), (total) lexicographic order d-tuples
Ad defined follows. say (x1 , . . . , xd ) (y1 , . . . , yd ) either xj = yj j [d]
xi < yi xj = yj j < i.
Taking = N = k obtain natural lexicographic order Nk0 .
also lexicographically order sets disjoint subsets ordered set =
{t1 , . . . , tk }, t1 < < tk .
Definition 5. associate k-tuple (x1 , . . . , xk ) Nk0 set disjoint subsets
{S1 , . . . , Sr } {t1 , . . . , tk } follows. xi = 0 ti
/ rm=1 Sm . ti rm=1 Sm ,
j < {ti , tj } Sm xi = xj ,
otherwise xi = max{x1 , . . . , xi1 } + 1, max = 0.
write VEC(S) = (x1 , . . . , xk ). Note VEC(S) computed time O(k 2 ).
Thus, tasks subset assigned value; assignment integers
tasks performed iteratively. example, = {1, . . . , 8} sets =
{{2, 4}, {3}, {5, 7}} B = {{2, 3, 4}, {5, 7}}, VEC(A) = (0, 1, 2, 1, 3, 0, 3, 0)
VEC (B) = (0, 1, 1, 1, 2, 0, 2, 0). lexicographically bigger B.
567

fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES

Corollary 1. Let ui plan-indistinguishability relation given set user-independent
constraints Lemma 1. exists encoding ui .
Proof. Let s1 , . . . , sk ordering plan. Let = { 1 (u) : u U SER()}
let VEC() = VEC(S ). plan , let ENC() tuple (U SER(), TASK(), VEC()).
clear ENC(1 ) = ENC(2 ) 1 ui 2 , r (si ) = r (sj )
yi = yj VEC(r ) = (y1 , . . . , yk ), r {1, 2}. Furthermore clear ENC()
determined polynomial time .
remains define linear ordering PAT[X, ] given X U, S. two
patterns p = (X, T, (x1 , . . . , xk )), p0 = (X, T, (y1 , . . . , yk )) PAT[X, ], define p p0
(x1 , . . . , xk ) (y1 , . . . , yk ).
Example 6. Let ENC encoding given proof Corollary 1. Let 1 , 2 plans
given Example 4. ENC(1 ) = ENC(2 ) = {{u1 , u2 , u3 , u4 }, {s1 , s2 , s3 }, (1, 2, 1, 0, 0, 0)}.
Corollary 2. Let e plan-indistinguishability relation given set constraints equivalence relations Lemma 2. exists encoding e .
Proof. Suppose equivalence relation users, let V1 , . . . , Vp equivalence classes
U . Suppose constraints form (si , sj , ) (si , sj , 6).
plan , define ENC() (U SER(), TASK(), ),


= 1 (Vj U SER()) : Vj U SER() 6= , Vj \ U SER() 6= , 1 j p .
clear ENC(1 ) = ENC(2 ) 1 e 2 , (s) Vj
i1 (Vj ), {1, 2}. Furthermore clear ENC() determined polynomial
time .
remains define linear ordering PAT[X, ] given X U, S. Let : X
plan. set disjoint subsets TASK(), natural order, order
patterns PAT[X, ] according lexicographic order .

Example 7. Let ENC encoding given proof Corollary 2. Let 1 , 2 plans
given Example 5. ENC(1 ) = ENC(2 ) = {{u1 , u2 , u3 , u4 }, {s1 , s2 , s3 }, {{s2 , s3 }}}.
4.2 Generic Algorithm
use notion diversity introduced next definition analyse running time
generic algorithm.
Definition 6. Let (S, U, A, C) instance WSP, n = |U |, k = |S| = |C|,
suppose plan-indistinguishability relation respect C. Given ordering u1 , . . . , un
U , let Ui = {u1 , . . . , ui } [n]. Let wi number equivalence classes
set [Ui , ] eligible plans. define diversity respect u1 , . . . , un
w = maxi[n] wi .
Since generic algorithm stores one plan equivalence class , need
notion representative set.
568

fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM

Definition 7. Given instance (S, U, A, C) WSP, let 0 set eligible plans let
plan-indistinguishability relation. set 00 said 0 -representative set respect
following properties hold:
1. 00 0 ; every plan 00 valid;
2. every valid 0 0 , exists 00 00 0 00 .
clear context, say 00 0 -representative set representative set
0 . generic algorithm based finding plan-indistinguishability relations
exist small representative sets.
Theorem 1. Let (S, U, A, C) instance WSP, n = |U |, k = |S| = |C|. Let
u1 , . . . , un ordering U , let Ui = {u1 , . . . , ui } [n], U0 = . Suppose
diversity w respect u1 , . . . , un . Furthermore suppose exists -encoding
ENC . (S, U, A, C) solved time (3k w log w).
Proof. proof proceeds demonstrating correctness bounding running time
Algorithm 2, solves WSP. begin proof, give overview Algorithm 2.
[n] turn S, construct representative set [Ui , ],
denoted [Ui , ] .
well constructing set [Ui , ] , also maintain companion set PAT[Ui , ] =
ENC ([Ui , ] ). provides efficient way representing equivalence classes
[Ui , ] . particular, allows us check whether given valid plan added
[Ui , ] , faster searching [Ui , ] linearly.
[Un , S] constructed, remains check whether [Un , S] non-empty,
exists valid complete plan , exists valid complete plan 0 [Un , S]
0 .
Algorithm 2 gives details construct [Ui , ] .
proof correctness Algorithm 2 proceeds induction. Observe first case
[U0 , ] , 6= possible plan [U0 , ], set [U0 , ] = .
= possible plan empty plan ). plan added [U0 , ] ,
trivially valid. Thus [U0 , ] [U0 , ]-representative set .
assume set [Ui , ] constructed [Ui , ]representative set. consider construction [Ui+1 , ] S. clear
added [Ui+1 , ] , [Ui+1 , ], eligible. Furthermore authorized,
union authorized plans 0 [Ui , 0 ] (T 00 ui+1 ). Thus every plan
[Ui+1 , ] valid plan [Ui+1 , ]. hand, suppose valid plan
[Ui+1 , ]. let 00 = 1 ({ui+1 }) 0 = \ 00 , let 0 = |Ui , =
0 (T 00 ui+1 ). assumption, exists 0 [Ui , ] 0 0 . Consider
plan = 0 (T 00 ui+1 ). clear considered algorithm.
Furthermore, 0 0 = 0 (T 00 ui+1 ), . Therefore
eligible (as eligible) also authorized (as union two authorized plans). Therefore
569

fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES

Algorithm 2: Generic algorithm WSP
input : instance (S, U, A, C) WSP, ordering u1 , . . . , un U ,
plan-indistinguishability relation
1 Set [U0 , ] = {( )};
2 foreach =
6
3
Set [U0 , ] = ;
4 end
5 Set = 0;
6 < n
7
foreach
8
Set [Ui+1 , ] = ;
9
Set PAT[Ui+1 , ] = ;
10
foreach 0
11
Set 00 = \ 0 ;
12
ui+1 A(s) 00
13
foreach 0 [Ui , 0 ]
14
Set = 0 (T 00 ui+1 );
15
eligible
16
Set p = ENC();
17
p
/ PAT[Ui+1 , ]
18
Insert p PAT[Ui+1 , ] ;
19
Set [Ui+1 , ] = [Ui+1 , ] {};
20
end
21
end
22
end
23
end
24
end
25
end
26
Set = + 1;
27 end
28 [Un , S] 6=
29
return [Un , S] ;
30 else
31
return NULL;
32 end

valid added [Ui+1 , ] unless [Ui+1 , ] already contains another plan equivalent . Thus, [Ui+1 , ] contains plan -equivalent , follows
[Ui+1 , ] [Ui+1 , ]-representative set, required.
remains analyse running time algorithm. Proposition 2, testing whether
pattern p PAT[Ui , ] inserting p PAT[Ui , ] takes (log(|PAT[Ui , ] |)) time.
Since Assumption 1 assumption time check constraints authorizations
takes polynomial time check eligibility, authorization -equivalence plans, running
570

fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM

P
P
P
P

time algorithm ( n1
i=0

0
[Ui ,T 0 ] log(|[Ui+1 , ] |)). clear
construction [Ui , 0 ] contains one plan -equivalence class [Ui , 0 ],
soP
definition
|[Ui , 0 ] | w i, 0 . follows running time algorithm
P
n1 P

( i=0 0 w log w) = (3k w log w).
Remark 1. Rather checking whether [Un , S] non-empty end algorithm,
could instead check whether [Ui , S] non-empty construction [Ui , S] i.
is, stop search soon valid plan task set S. likely lead
saving running time implementation algorithm. paper concerned
worst-case running time, would unaffected change, perform check
end algorithm interest clarity.
4.3 Application User-Independent Constraints Optimality
subsection, show WSP user-independent constraints FPT. Let Bk denote
kth Bell number, number partitions set k elements.
Lemma 3. Let u1 , . . . , un ordering U , let ui plan-indistinguishability relation
given Lemma 1. ui diversity Bk respect u1 , . . . , un .
Proof. plan , set { 1 (u) : u U SER()} partition tasks TASK().
Furthermore, two plans generate partition equivalent ui . Therefore
number equivalence classes ui [Ui , ] exactly number possible partitions
, B|T | . Thus, Bk required diversity.
Theorem 2. constraints user-independent, WSP solved time
(2k log k ).
Proof. Let u1 , . . . , un ordering U , let ui plan-indistinguishability relation
given Lemma 1.
Lemma 3, ui diversity Bk respect u1 , . . . , un . Furthermore, Corollary 1,
exists encoding ui . Therefore, may apply Theorem 1 w = Bk , get
algorithm running time (3k Bk log Bk ) = (2k log k ) Bk < (0.792k/ ln(k + 1))k
every k (Berend & Tassa, 2010).
running time (2k log k ) obtained optimal sense algorithm running time
exists, unless ETH fails. proof following theorem, use result
Lokshtanov, Marx, & Saurabh, 2011 (Theorem 2.2).

(2o(k log k) )

Theorem 3. algorithm WSP user-independent constraints running time
(2o(k log k) ), unless ETH fails.
Proof. give reduction problem kk NDEPENDENT ET: Given integer parameter
k graph G vertex set V = {(i, j) : i, j [k]}, decide whether G independent set
|I| = k r [k], exists (r, i) I.
Informally, k k NDEPENDENT ET gives us graph k k grid vertices, asks
whether independent set one vertex row. Lokshtanov et al. (2011) proved
algorithm solve k k NDEPENDENT ET time 2o(k log k) , unless ETH fails.
571

fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES

Consider instance k k NDEPENDENT ET graph G. first produce
equivalent instance WSP constraints user-independent.
refine instance one user-independent constraints.
Let U = {u1 , . . . , uk } set k users = {s1 , . . . , sk } set k tasks. Let
authorization lists A(si ) = U [k]. i, j, h, l [k], let c((i, j), (h, l)) denote
constraint scope {si , sh }, satisfied plan unless (si ) = uj
(sh ) = ul . every pair vertices (i, j), (h, l) adjacent G, add constraint
c((i, j), (h, l)) C.
show (S, U, A, C) ES-instance WSP G independent set one vertex row. Suppose (S, U, A, C) ES-instance WSP
let valid complete plan. [k], let f (i) unique j (si ) = uj .
= {(i, f (i)) : [k]} set one vertex row G; furthermore, satisfies every constraint, edge G contains one element I, independent
set.
Conversely, suppose G ES-instance k k NDEPENDENT
ET. [k], let

f (i) integer (i, f (i)) I. observe ki=1 ({si } uf (i) ) valid complete
plan.
show reduce (S, U, A, C) instance WSP constraints
user-independent. main idea introduce new tasks representing users,
constraints, replace mention particular user mention user performs
particular task.
Create k new tasks t1 , . . . , tk let 0 = {t1 , , . . . , tk }. Let authorization lists
0
(s) = U A0 (ti ) = {ui } [k]. constraint c((i, j), (h, l))
C, let d((i, j), (h, l)) constraint scope {si , sh , tj , tl }, satisfied plan
unless (si ) = (tj ) (sh ) = (tl ). Let initially C 0 = C. replace, C 0 , every constraint
c((i, j), (h, l)) d((i, j, ), (h, l)).
Since defined equalities, users mentioned, constraints C 0 userindependent. show (S 0 , U, A0 , C 0 ) equivalent (S, U, A, C). First, suppose
valid complete plan (S, U, A, C). let 0 : 0 U plan 0 (si ) = (si )
[k], 0 (tj ) = uj j [k]. easy check satisfies every constraint
C 0 satisfies every constraint C 0 . Since 0 authorized eligible plan, 0 valid
complete plan (S 0 , U, A0 , C 0 ).
Conversely, suppose 0 valid complete plan (S 0 , U, A0 , C 0 ). Since A0 (ti ) = {ui }
[k], 0 (ti ) = ui every [k]. [k], let f (i) unique integer
0 (si ) = uf (i) . define : U (si ) = uf (i) , observe constraints C
satisfied . So, valid complete plan (S, U, A, C).
4.4 Application Equivalence Relation Constraints
known restricting WSP equivalence relation constraints enough ensure
problem FPT (Crampton et al., 2013). However, derive result applying
algorithm directly shown appropriate properties language equivalence relation
constraints. serves demonstrate wide applicability approach.

572

fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM

Lemma 4. Let e plan-indistinguishability relation given set equivalence relation
constraints Lemma 2. exists ordering u1 , . . . , un U e diversity
2k respect U .
Proof. Suppose equivalence relation users, let V1 , . . . , Vp equivalence classes
U . Suppose constraints form (si , sj , ) (si , sj , 6).
Let u1 , . . . , un ordering U elements Vj appear elements
Vj 0 , j < j 0 . Thus, plan U SER() = Ui = {u1 , . . . , ui },
one integer ji Vji U SER() 6= , Vji \ U SER() 6= .
follows two plans 1 , 2 [Ui , ] e -equivalent, [n], S,
provided 1 (t) Vji 2 (t) Vji . Therefore e 2k
equivalence classes [Ui , ], required.
Theorem 4. Suppose equivalence relation U . Suppose constraints form
(si , sj , ) (si , sj , 6). WSP solved time (6k ).
Proof. Let u1 , . . . , un ordering U given Lemma 4, let e planindistinguishability relation given Lemma 2.
Lemma 4, e diversity 2k respect u1 , . . . , un . Furthermore Corollary 2,
exists encoding e . Therefore, may apply Theorem 1 w = 2k , get algorithm
running time (3k 2k log(2k )) = (6k ).

5. Unions Constraint Languages
section show approach allows us easily combine constraint languages shown
FPT WSP. need build bespoke algorithms new constraint language
obtained, show two languages sense compatible.
highlights advantages approach previous methods, required development new algorithms different constraint languages combined instance
WSP (e.g., see Crampton et al., 2013).
Theorem 5. Let (S, U, A, C1 C2 ) instance WSP, suppose 1 planindistinguishability relation respect C1 2 plan-indistinguishability relation
respect C2 . Given ordering u1 , . . . , un U , let W1 diversity 1 respect
u1 , . . . , un W2 diversity 2 respect u1 , . . . , un .
Let equivalence relation 0 1 0 2 0 .
plan-indistinguishability relation respect C1 C2 , diversity W1 W2
respect u1 , . . . , un .
Proof. first show plan-indistinguishability relation respect C1 C2 . Let
0 eligible plans (with respect C1 C2 ). 0 implies 1 0 1 satisfies
conditions plan-indistinguishability relation, clear 0 U SER() =
U SER( 0 ) TASK() = TASK( 0 ). consider plan 00 disjoint 0 . 1
plan-indistinguishability relation respect C1 1 0 , 00 C1 -eligible
0 00 is. Similarly 00 C2 -eligible 0 00 is. Observing
plan C1 C2 -eligible C1 -eligible C2 -eligible, implies 00
C1 C2 -eligible 0 00 is. Thus 0 extension equivalent.
573

fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES

1 2 plan-indistinguishability relations, 00 1 0 00
00 2 0 00 , therefore 00 0 00 . Thus, satisfies conditions
plan-indistinguishability relation.
bound diversity respect u1 , . . . , un , consider Ui =
{u1 , . . . , ui }. enough note -equivalent plans [Ui , ] must
1 2 -equivalence classes. W1 choices 1 -equivalence class
W2 choices 2 equivalence class, W1 W2 equivalence classes
[Ui , ].
Remark 2. Given encoding ENC1 1 encoding ENC2 2 , may construct
encoding . Given plan , let ENC() ordered pair (ENC1 (), ENC2 ()). clear
ENC() = ENC( 0 ) 0 .
Given sets Ui = {u1 , . . . , ui }, fix linear orderings ENC1 ([Ui , ])
ENC 2 ([Ui , ]). let lexicographic ordering ENC ([Ui , ]) = ENC 1 ([Ui , ])
ENC 2 ([Ui , ]).
nothing stop us applying Theorem 5 multiple times, order get planindistinguishability relation bounded diversity union several constraint languages.
Note diversity expected grow exponentially number languages
union. Thus, makes sense apply Theorem 5 union small number languages.
However, long fixed number languages, plan-indistinguishability
relation fixed-parameter diversity, resulting union languages also planindistinguishability relation fixed-parameter diversity.
use result directly show constraints either user independent
equivalence relation constraints, WSP still FPT.
Theorem 6. Suppose equivalence relation U . Let (S, U, A, C) instance
WSP, suppose constraints either form, (s1 , s2 , ), (s1 , s2 , 6) userindependent constraints. WSP solved time (2k log k+k ).
Proof. Let Ce C set constraints form (s1 , s2 , ), (s1 , s2 , 6), let Cui
remaining (user-independent) constraints.
Let u1 , . . . , un ordering U given Lemma 4. Lemmas 2 4, exists planindistinguishability relation e Ce diversity 2k respect u1 , . . . , un . Furthermore
Corollary 2, e encoding. Lemmas 1 3, exists plan-indistinguishability
relation ui Cui diversity Bk respect u1 , . . . , un . Furthermore Corollary 1,
ui encoding.
Therefore Theorem 5, may find plan-indistinguishability relation C,
diversity Bk 2k respect u1 , . . . , un encoding. Thus
may apply Theorem 1 w = Bk 2k , get running time (3k Bk 2k log(Bk 2k )) =
(3k 2k log k(1o(1))+k log(2k log k(1o(1))+k )) = (2k log k+k ).

6. Computational Experiments WSP Algorithms
Apart conducting theoretical research WSP, Wang Li (2010) carried experimental study problem. Due difficulty acquiring real-world workflow instances,
574

fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM

Wang Li used synthetic data experimental study. encoded instances WSP
pseudo-Boolean SAT order use pseudo-Boolean SAT solver SAT4J.
implemented algorithm compared performance SAT4J another set
synthetic instances WSP (Cohen et al., 2014). instances use k = 16, 20
24, n = 10k user-independent (cardinality) constraints three different types: vary
number constraints proportions different constraint types; user authorized
1 8 tasks k = 16, 1 10 tasks k = 20, 1 12
tasks k = 24. algorithm implemented C++ enhanced inclusion
techniques employed CSP solving, propagation. also converted WSP instances
pseudo-Boolean problems processing SAT4J. experiments performed MacBook
Pro computer 2.6 GHz Intel Core i5 processor 8 GB 1600 MHz DDR3 RAM (running
Mac OS X 10.9.2).
lightly-constrained instances, SAT4J often faster algorithm, largely
number patterns considered algorithm large instances. However, highlyconstrained instances, SAT4J unable compute decision number instances (because
ran memory), sharp contrast algorithm solved instances. Overall,
average, algorithm faster SAT4J and, particular, two orders magnitude faster
k = 16. Moreover, time taken algorithm varies much less SAT4J, even
unsatisfiable instances, time taken proportional product number
patterns number users. (In particular, tested instances, much less dependent
number constraints, parameter cause significant fluctuations time taken SAT4J
leads sharp increase number variables pseudo-Boolean encoding.) Full
details results published (Cohen et al., 2014).

7. Conclusion
paper introduced algorithm based notion plan-indistinguishability, applicable
wide range WSP instances. showed algorithm powerful enough optimal,
sense, wide class user-independent constraints. generic algorithm also
fixed-parameter algorithm equivalence relation constraints, user-independent.
showed deal unions different types constraints using generic algorithm.
particular, proved generic algorithm fixed-parameter algorithm union
user-independent equivalence relation constraints.

Acknowledgments
research supported EPSRC grant EP/K005162/1. grateful referees
useful comments suggestions.

References
American National Standards Institute (2004).
CITS RBAC 359-2012).
575

Role Based Access Control (ANSI IN-

fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES

Basin, D. A., Burri, S. J., & Karjoth, G. (2014). Obstruction-free authorization enforcement: Aligning security business objectives. Journal Computer Security, 22(5), 661698.
Beldiceanu, N., Carlsson, M., & Rampon, J.-X. (2012). Global constraint catalog, 2nd edition
(revision a). working copy 5195, Swedish Institute Computer Science, Kista, Sweden.
Berend, D., & Tassa, T. (2010). Improved bounds bell numbers moments sums
random variables. Probability Mathematical Statistics, 30(2), 185205.
Bertino, E., Bonatti, P. A., & Ferrari, E. (2001). TRBAC: temporal role-based access control
model. ACM Trans. Inf. Syst. Secur., 4(3), 191233.
Bertino, E., Ferrari, E., & Atluri, V. (1999). specification enforcement authorization
constraints workflow management systems. ACM Trans. Inf. Syst. Secur., 2(1), 65104.
Bodlaender, H. L., Cygan, M., Kratsch, S., & Nederlof, J. (2013). Deterministic single exponential
time algorithms connectivity problems parameterized treewidth. Proceedings
40th International Conference Automata, Languages, Programming - Volume Part I,
ICALP13, pp. 196207, Berlin, Heidelberg. Springer-Verlag.
Bulatov, A., Jeavons, P., & Krokhin, A. (2005). Classifying complexity constraints using
finite algebras. SIAM Journal Computing, 34, 720742.
Cohen, D., Crampton, J., Gagarin, A., Gutin, G., & Jones, M. (2014). Engineering algorithms
workflow satisfiability problem user-independent constraints. Chen, J., Hopcroft,
J. E., & Wang, J. (Eds.), Frontiers Algorithmics - 8th International Workshop, FAW 2014,
Zhangjiajie, China, June 28-30, 2014. Proceedings, Vol. 8497 Lecture Notes Computer
Science, pp. 4859. Springer.
Cormen, T. H., Stein, C., Rivest, R. L., & Leiserson, C. E. (2001). Introduction Algorithms (2nd
edition). McGraw-Hill Higher Education.
Crampton, J., Crowston, R., Gutin, G., Jones, M., & Ramanujan, M. (2013). Fixed-parameter
tractability workflow satisfiability presence seniority constraints. Fellows, M.,
Tan, X., & Zhu, B. (Eds.), Frontiers Algorithmics Algorithmic Aspects Information
Management, Vol. 7924 Lecture Notes Computer Science, pp. 198209. Springer
Berlin Heidelberg.
Crampton, J. (2005). reference monitor workflow systems constrained task execution.
Proceedings Tenth ACM Symposium Access Control Models Technologies,
SACMAT 05, pp. 3847, New York, NY, USA. ACM.
Crampton, J., & Gutin, G. (2013). Constraint expressions workflow satisfiability. Proceedings 18th ACM Symposium Access Control Models Technologies, SACMAT 13,
pp. 7384, New York, NY, USA. ACM.
Crampton, J., Gutin, G., & Yeo, A. (2013). parameterized complexity kernelization
workflow satisfiability problem. ACM Trans. Inf. Syst. Secur., 16(1), 4:14:31.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann Publishers, 340 Pine Street, Sixth
Floor, San Francisco, CA 94104-3205.
Downey, R. G., & Fellows, M. R. (2013). Fundamentals Parameterized Complexity. Texts
Computer Science. Springer.
576

fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM

Fomin, F. V., Lokshtanov, D., & Saurabh, S. (2014). Efficient computation representative sets
applications parameterized exact algorithms. Proceedings Twenty-Fifth
Annual ACM-SIAM Symposium Discrete Algorithms, SODA 14, pp. 142151. SIAM.
Gligor, V., Gavrila, S., & Ferraiolo, D. (1998). formal definition separation-of-duty policies composition. 1998 IEEE Symposium Security Privacy, 1998. Proceedings., pp. 172183.
Impagliazzo, R., Paturi, R., & Zane, F. (2001). problems strongly exponential complexity?. J. Comput. Syst. Sci., 63(4), 512530.
Jayaraman, K., Ganesh, V., Tripunitara, M. V., Rinard, M. C., & Chapin, S. J. (2011). ARBAC
policy large multi-national bank. CoRR, abs/1110.2849.
Joshi, J. B. D., Bertino, E., Latif, U., & Ghafoor, A. (2005). generalized temporal role-based
access control model. IEEE Transactions Knowledge Data Engineering,, 17(1), 423.
Lokshtanov, D., Marx, D., & Saurabh, S. (2011). Slightly superexponential parameterized problems.
Proceedings Twenty-second Annual ACM-SIAM Symposium Discrete Algorithms,
SODA 11, pp. 760776. SIAM.
Niedermeier, R. (2006). Invitation Fixed-Parameter Algorithms. Oxford University Press.
Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006). Handbook Constraint Programming.
Elsevier.
Sandhu, R. S., Coyne, E. J., Feinstein, H. L., & Youman, C. E. (1996). Role-based access control
models. Computer, 29(2), 3847.
Schaad, A., Moffett, J., & Jacob, J. (2001). role-based access control system european
bank: case study discussion. Proceedings Sixth ACM Symposium Access
Control Models Technologies, SACMAT 01, pp. 39, New York, NY, USA. ACM.
Simon, R., & Zurko, M. (1997). Separation duty role-based environments. Computer
Security Foundations Workshop, 1997. Proceedings., 10th, pp. 183194.
Wang, Q., & Li, N. (2010). Satisfiability resiliency workflow authorization systems. ACM
Trans. Inf. Syst. Secur., 13(4), 40:140:35.
Wolter, C., & Schaad, A. (2007). Modeling task-based authorization constraints bpmn.
Proceedings 5th International Conference Business Process Management, BPM07,
pp. 6479, Berlin, Heidelberg. Springer-Verlag.

577

fiJournal Artificial Intelligence Research 51 (2014) 377-411

Submitted 7/14; published 10/14

Novel SAT-Based Approach Model Based Diagnosis
Amit Metodi

AMITMET @ CS . BGU . AC . IL

Department Computer Science,
Ben Gurion University Negev, Beer-Sheva, Israel

Roni Stern
Meir Kalech

RONI . STERN @ GMAIL . COM
KALECH @ BGU . AC . IL

Department Information Systems Engineering,
Ben Gurion University Negev, Beer-Sheva, Israel

Michael Codish

MCODISH @ CS . BGU . AC . IL

Department Computer Science,
Ben Gurion University Negev, Beer-Sheva, Israel

Abstract
paper introduces novel encoding Model Based Diagnosis (MBD) Boolean Satisfaction (SAT) focusing minimal cardinality diagnosis. encoding based combination
sophisticated MBD preprocessing algorithms application SAT compiler optimizes encoding provide succinct CNF representations obtained previous
works. Experimental evidence indicates approach superior published algorithms
minimal cardinality MBD. particular, determine, first time, minimal cardinality diagnoses entire standard ISCAS-85 74XXX benchmarks. results open way
improve state-of-the-art range similar MBD problems.

1. Introduction
Automated diagnosis concerned reasoning health systems, including identification abnormal behavior, isolation faulty components prediction system behavior
normal abnormal conditions. systems become large-scale complex,
automated diagnosis becomes challenging. Model Based Diagnosis (MBD) artificial
intelligence based approach aims cope diagnosis problem (Reiter, 1987; de Kleer
& Williams, 1987). MBD, model system first built. diagnoser observes
system predict behavior model. Discrepancies observation prediction used input diagnosis algorithm produces set possible faults
explain observation. MBD deployed several real-world applications, including
spacecrafts (Williams & Nayak, 1996), satellite decision support systems (Feldman, de Castro, van
Gemund, & Provan, 2013), automotive industry (Struss & Price, 2003) spreadsheets (Jannach & Schmitz, 2014). Also, exist several commercial MBD tools (Feldman, 2012; Dressler
& Struss, 1995).
MBD known hard problem algorithms exponential runtime (exponential
number components diagnosed system). Moreover, number potential diagnoses given observation huge. Therefore, MBD algorithms typically focus minimal
c
2014
AI Access Foundation. rights reserved.

fiM ETODI , TERN , K ALECH & C ODISH

diagnoses: minimal subset contain diagnoses, minimal cardinality
smallest size. Computing first minimal diagnosis P , computing next one
NP-hard (Bylander, Allemang, Tanner, & Josephson, 1991). Computing minimal cardinality
NP-hard, even first diagnosis (Selman & Levesque, 1990). work focus
hard task finding minimal cardinality diagnoses.
study Model-Based Diagnosis resulted variety computational modeling
challenges. paper focus one challenge received much attention
years. Originally defined Reiter (1987) de Kleer Williams (1987), problem
aims diagnose multiple faulty components so-called weak fault model, ignores
mode abnormal behavior components. problem extensively researched
25 years wide range papers propose different algorithms solve it, including range
papers recent years (Feldman & van Gemund, 2006; Williams & Ragno, 2007; Feldman,
Provan, & van Gemund, 2010a; Siddiqi & Huang, 2007, 2011). addressing challenge,
common practice focus diagnosis Combinational Logic Circuits. Namely, Boolean
circuits single output component determined logical function
current input state (independent time feedback).
basic setting diagnosis considers single observation inputs outputs
system. Variations consider additional information probabilities component failure,
multiple observations inputs outputs system, observations internal positions
system (probes). paper focus basic setting. Extensions variations
discussed Section 8.
Even basic setting, solving MBD problem often impractical, especially highcardinality faults. instance, system 1000 components, find minimal cardinality
diagnosis size 5, diagnosis engine must verify absence diagnosis consisting 4 components (there 1010 combinations). overcome complexity problem
consider novel encoding SAT.
recent years, Boolean SAT solving techniques improved dramatically. Todays SAT
solvers considerably faster able manage larger instances yesterdays. Moreover, encoding modeling techniques better understood increasingly innovative. SAT currently
applied solve wide variety hard practical combinatorial problems, often outperforming
dedicated algorithms. survey state-of-the-art SAT solving see work Biere,
Heule, van Maaren, Walsh (2009) draft forthcoming volume Art Computer Programming (Knuth, 2014).
general idea encode (typically, NP) hard problem instance, , Boolean formula,
, solutions correspond satisfying assignments . Given encoding,
SAT solver applied solve .
SAT-based solutions MBD already proposed. Smith et al. (2005) encode circuit,
representing component clauses add constraints cardinality. basis
SAT-based encodings, including one contribute paper. Bauer (2005)
introduces tailored SAT solver specifically designed return many diagnoses. Stein et al. (2006)
address diagnosis qualitative models physical systems multiple fault modes. recently, Feldman et al. (2010) propose encoding MAX-SAT demonstrate off-the-shelf
solvers require calls SAT solver stochastic diagnosis algorithm SAFARI (Feldman
et al., 2010a).
378

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

previous applications SAT MBD appear indicate SAT MAX-SAT solvers
doomed perform poorly standard benchmarks (Feldman et al., 2010). paper proves
contrary. SAT-based approach differs previous SAT encodings several key aspects.
First, sophisticated MBD preprocessing techniques applied facilitate construction
carefully designed constraint model, includes constraints exploit unique substructures
diagnosed system. Second, constraint model compiled corresponding CNF using
constraint compiler called BEE (Metodi & Codish, 2012), simplifies constraints generates
encoding CNF significantly improves subsequent runtime underlying SAT
solver. Lastly, structural abstraction inspired Siddiqi Huang (2007) used decompose
diagnosis problem, SAT solver used find top-level diagnoses,
show simple poly-time algorithm expand top-level diagnoses find minimal
cardinality diagnoses. approach requires preprocessing diagnosed system,
complexity preprocessing low-order polynomial, negligible (both theoretically
empirically) compared cost actual SAT solving.
evaluated SAT-based approach using two standard benchmarks: ISCAS-85 (Brglez,
Bryan, & Kozminski, 1989) 74XXX. standard benchmarks MBD literature, used extensively time made available today (Feldman & van Gemund,
2006; Feldman et al., 2010a; Siddiqi & Huang, 2007, 2011; Stern, Kalech, Feldman, & Provan,
2012; Nica, Pill, Quaritsch, & Wotawa, 2013). Finding minimal cardinality diagnoses hard sets
observations ISCAS-85 long standing challenge MBD community
used diagnosis competitions (DXC, 2009). ISCAS-85 systems also used standard
automatic benchmark generation (Wang & Provan, 2010).
consider three known sets observations minimal cardinalities 131,
first time succeed compute minimal cardinality diagnosis observations
benchmark. compare approach wide collection state-of-the-art algorithms MBD,
including: HA* (Feldman & van Gemund, 2006), CDA* (Williams & Ragno, 2007), SAFARI (Feldman et al., 2010a), HDIAG (Siddiqi & Huang, 2007) DCAS (Siddiqi & Huang, 2011). Results
unequivocal. approach outperforms others, often orders magnitude, terms
runtime. result even significant, SAFARI stochastic algorithm, known fast,
even aim guarantee minimal cardinality. approach, hand, guarantees minimal cardinality diagnosis runs faster SAFARI.
paper goes beyond preliminary version work (Metodi, Stern, Kalech, & Codish,
2012a). provide detailed description components approach, present
detailed algorithms, prove correctness, provide additional examples present elaborate
experimental evaluation. next section discuss additional related work. Section 3 presents
required background MBD. Section 4 present standard approach model MBD
SAT. Section 5 main part paper describe building blocks tool
find minimal cardinality diagnosis. Section 6 describes building blocks combined
diagnosis algorithm. Comprehensive evaluation approach given Section 7. Section
8 discusses applicability approach general setting Section 9 concludes.

2. Related Work
Since late 80s, Model Based Diagnosis problem weak fault model widely
researched wide range papers propose different algorithms solve (Reiter, 1987; de Kleer
379

fiM ETODI , TERN , K ALECH & C ODISH

& Williams, 1987; Feldman & van Gemund, 2006; Williams & Ragno, 2007; Feldman et al., 2010a;
Siddiqi & Huang, 2007, 2011). Till today considered challenge reflected synthetic
track annual DXC diagnosis competition (DXC, 2009).
Many existing diagnosis techniques propose apply combination deterministic reasoning search algorithms. One classic approach involves two stage process. First, identifies
conflict sets, includes least one fault. Then, applies hitting set algorithm
compute sets multiple faults explain observation (de Kleer & Williams, 1987; Williams
& Ragno, 2007). methods guarantee sound diagnoses, even complete.
However, tend fail large systems due infeasible runtime space requirements.
alternative method directly search diagnoses trying different assumptions
components faulty. example, DRUM-II diagnosis engine finds minimal diagnosis performing iterative deepening search, limiting every iteration, number
components assumed faulty (Frohlich & Nejdl, 1997). DRUM-II also analyzes
dependencies components prune irrelevant diagnoses. Recent work presents empirical
evidence suggesting direct search diagnoses often better conflict-directed diagnosis algorithms (Nica et al., 2013). Nica et al. compare SAT-based approach,
uses pre-processing. work show proposed pre-processing efficient
computationally results huge speedups search diagnosis. form preprocessing key ingredient enables us find large minimal cardinality diagnoses,
even sizes 31.
Another approach considers diagnosis problem terms inductive learning. Here, one
tries learn relations symptoms faults (Murray, Hughes, & Kreutz-Delgado,
2006). One disadvantage works approach learn single fault rather
multiple faults (Balakrishnan & Honavar, 1998). addition, inductive learning methods
guarantee sound diagnoses completeness. We, hand, propose method
addresses multiple faults guarantees sound complete minimal cardinality diagnoses.
Feldman et al. (2010a) propose stochastic diagnosis algorithm, called SAFARI. Although
method guaranteed return diagnoses minimal cardinality, presents solutions
close minimal cardinality low runtime. Section 7, demonstrate approach
outperforms SAFARI terms runtime, also guarantees minimal cardinality diagnoses
returned.
Compilation-based methods also proposed MBD context. Torasso Torta
(2006) proposed compile system description Binary Decision Diagrams (BDDs). Darwiche (2001) proposed compile system description Decomposable Negation Normal
Form (DNNF). cases, compiled model allowed finding minimal cardinality diagnosis
time polynomial size compiled model. However, works, size
compiled model (BDD DNNF) may grow exponentially shown become bottleneck (Siddiqi & Huang, 2007).
Siddiqi Huang (2007) suggest optimize MBD identifying components dominate
others. adopt idea apply SAT-based approach. Another compilation-based
diagnosis algorithm HA* algorithm (Feldman & van Gemund, 2006). HA* designed
exploit given hierarchy diagnosed system. done converting given system
hierarchy DNF hierarchy. element DNF hierarchy solved simple
best-first search using heuristic function given prior probability health system
components. Section 7, demonstrate approach substantially outperforms HA*.
380

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

= {X1 , X2 , A1 , A2 , O1 }
OBS = {A, B, C, D, E}
COMPS

Figure 1: MBD problem: (faulty) full adder.
Another previously proposed approach imposes tree structure given system description.
system tree structure diagnosed joining diagnoses constituent subsystems.
El Fattah Dechter (1995) obtained tree structure converting diagnosed system
chordal graph decomposed tree maximal cliques. TREE* algorithm another
tree-decomposition algorithm, initially proposed tree-structured systems (Stumptner
& Wotawa, 2001). TREE* later generalized perform system embedding hyper
tree specific representation diagnosed system (Stumptner & Wotawa, 2003). Follow
work generalized TREE* support various forms diagnosis optimization tasks,
finding minimal cardinality diagnoses, finding subset minimal diagnoses, finding probable diagnoses (Sachenbacher & Williams, 2004). Note complexity TREE* exponential
width hyper-tree embedded system description defined works.

3. Model-Based Diagnosis: Preliminaries
section introduces background Model Based Diagnosis. addition basic definitions, review several concepts literature build paper.
Model Based Diagnosis problems arise normal behavior system violated due
faulty components indicated certain observations. focus weak fault models,
ignore mode abnormal behavior components. MBD problem specified triplet
hSD, COMPS , OBS where: SD system description, COMPS set components, OBS
observation. system description takes account components might abnormal
(faulty). specified unary predicate h() components h(c) true
component c healthy false c faulty. Denoting correct behavior c propositional formula, c , SD given formally
^
SD =
h(c) c
cCOMPS

Namely, component faulty follows correct behavior. diagnosis problem
arises when, assumption none components faulty, inconsistency
system description observations (de Kleer & Williams, 1987; Reiter, 1987).
Definition 1 [Diagnosis Problem]. Given MBD problem, hSD, COMPS , OBS i, diagnosis problem
arises
^
SD
h(c) OBS `
cCOMPS

381

fiM ETODI , TERN , K ALECH & C ODISH

example, diagnosis problem arises MBD Figure 1 normal behavior would give
output E = 1. inconsistency, diagnosis algorithm tries find subset COMPS
which, assumed faulty, explains observation.
Definition 2 [Diagnosis] Given MBD problem, hSD, COMPS , OBS i, set components
COMPS diagnosis
^
^
h(c) OBS 0
SD
h(c)
c

c
/

say minimal diagnosis proper subset 0 diagnosis,
minimal cardinality diagnosis diagnosis 0 COMPS exists |0 | < ||.
MBD Figure 1, 1 ={X1 , X2 }, 2 ={O1 }, 3 ={A2 } minimal diagnoses, 2 ,
3 minimal cardinality diagnoses, smaller diagnosis.
important concept make use paper gate domination, used
automatic test pattern generation (ATPG) (Kirkland & Mercer, 1987; Fujiwara, Member, Shimono, & Member, 1983) modern SAT solvers (Marques-Silva, Lynce, & Malik, 2009),
sometimes name unique sensitization. Siddiqi Huang (2007) applied gate domination model-based diagnosis, introducing notion cone. following wording
taken Siddiqi Huangs paper setting system Boolean circuit
components gates.
Definition 3 (Dominator Cone) gate X fan-in region gate G dominated G
conversely G dominator X path X output circuit contains G. cone
corresponding gate G set gates dominated G. maximal cone one either
contained cone contained exactly one cone entire circuit.
example, circuit depicted Figure 1, components {A1 , A2 , O1 } form cone, since
path A1 A2 system output contains O1 . O1 dominator A1 A2
dominated gates.
Although Definition 3 stated terms Boolean circuits logical gates, notions
dominators cones generalized many systems, components correspond gates,
component C1 dominates component C2 paths passing C2 also pass
C1 . example, system illustrated Figure 3 components C1 C2 form cone,
C2 dominates C1 .
importance cones MBD algorithms rooted two observations presented Siddiqi
Huang. Firstly, cones single-output sub-systems such, minimal cardinality diagnosis always, independent observation, indicate one unhealthy component per cone.
Secondly, C cone SD, without loss generality, may assume dominated
components C healthy. correct X unhealthy minimal cardinality
diagnosis dominated G, G must healthy. So, exists another minimal cardinality
diagnosis X healthy G not. example, circuit depicted Figure 1, 3
minimal cardinality diagnosis signifies dominated A2 unhealthy, exists another
minimal cardinality diagnosis, 2 , A2 healthy O1 , dominates A2 , unhealthy.
Based observations restrict search minimal cardinality diagnoses,
so-called top-level minimal cardinality diagnoses. notion top-level diagnoses introduced Siddiqi Huang.
382

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

Definition 4 (top-level diagnosis (TLD)) say minimal cardinality diagnosis top-level
contain dominated components.
formally justify focus top-level diagnoses make explicit following Propositions 1 2, left implicit previous work.
Proposition 1 Let 0 minimal cardinality diagnosis given MBD problem.
top-level diagnosis , cardinality.
Proof: Straightforward. obtain , replace dominated component 0 corresponding dominator.

note set minimal cardinality diagnoses obtained expanding set top-level minimal cardinality diagnoses following sense: Given minimal
cardinality top-level diagnosis, = {c1 , . . . , c` } consisting ` dominators corresponding
cones {C1 . . . , C` }, denote
fi
n

fi
= c0i Ci fi \ {ci } {c0i } diagnosis
(1)
say expands set minimal cardinality diagnoses defined terms crossproduct by: () = 1 ` . example, consider system Figure 1
observation OBS = {A, B, C, D, E}. cones system C1 = {X1 }, C2 = {X2 }, C3 =
{A1 , A2 , O1 }. corresponding MBD problem two top-level minimal cardinality diagnoses,
1 = {X1 , O1 } 2 = {X2 , O1 } (1 ) = {X1 } {O1 } = {{X1 , O1 }} (2 ) =
{X2 } {A1 , O1 } = {{X2 , A1 }, {X2 , O1 }}.
Proposition 2 0 minimal cardinality diagnosis top-level minimal
cardinality diagnosis expands include 0 .
Proof: proof straightforward construction.

Finally, comment sets specify expansion top-level diagnosis
Equation (1) easy compute: component c0i Ci checking \ {ci } {c0i }
diagnosis means propagating observed inputs system, flipping outputs
propagating component \ {ci } {c0i } checking conflict
observed outputs. observation explicit previous work essential justify
focus top-level diagnoses. Proposition 2 important applies diagnosis algorithm.
such, diagnosis algorithms general focus, compared finding TLDs, instead
finding minimal cardinality diagnoses.

4. Standard Approach SAT-Based MBD
standard encoding MBD problem hSD, COMPS , OBS Boolean Satisfiability (as introduced Smith et al., 2005) associates component c COMPS propositional formula,
c , denoting correct behavior, Boolean variable, Hc , signifying c healthy.
Viewing observation propositional statement, encoding obtained specifying
^
= OBS
Hc c
(2)
cCOMPS

383

fiM ETODI , TERN , K ALECH & C ODISH

satisfying assignment , health variables assigned value false determine (not
necessarily minimal) diagnosis .
example, consider MBD problem Figure 1, let comp(A, B, C)
comp {and, or, xor} denote propositional formula describing behavior component and00 , or00 xor00 gate inputs A, B output C. So, Equation (2) takes
form:


B C E HX1 xor(A, B, Z1 )


(3)
= HA1 and(A, B, Z2 )
HX2 xor(Z1 , C, D)
HA2 and(Z1 , C, Z3 ) HO1 or(Z2 , Z3 , E)
formula satisfied assignment variables {A, C, HA1 , HA2 , HO1 } true variables
{B, D, E, Z1 , Z2 , Z3 , HX1 , HX2 } false. assignment indicates = {X1 , X2 } diagnosis.
obtain minimal cardinality diagnosis seek satisfying assignment minimal number health variables taking value false. example assignment variables
{A, C, Z1 , Z3 , HX1 , HX2 , HA1 , HA2 } true variables {B, D, E, Z2 , HO1 } false also satisfies Equation (3) indicates one faulty component. achieved using MAX-SAT
solver (Feldman et al., 2010), using SAT solver done implementation underlying
paper, cardinality constraint (encoded CNF) introduced constrain number
faulty components detailed next section.
matter cardinality constraint encoded CNF, setting |COMPS | = n
constant k, formula
fi
n

fi
k = sum leq( Hc fi c COMPS , k)
(4)
satisfied k n health variables take value false. specifically, seek
minimal value k (the CNF corresponding to) k satisfiable. involves iterating
calls SAT solver formulae k decreasing values k k satisfiable
k1 not. approach takes advantage fact SAT solvers typically incremental:
adding clauses satisfiable instance allows solve maintaining derived
information search space previous call.

5. Approach SAT-Based MBD
approach encoding MBD problem hSD, COMPS , OBS SAT proceeds follows: First,
adopt finite domain constraint based representation express basic model. Second, analyze
structure substructures SD introduce additional (redundant) constraints
later boost search minimal cardinality analysis. Third, introduce constraints model
given observation OBS additional constraint imposes bound cardinality
diagnosis (the number unhealthy components). additional constraint reduces
subsequent number iterations search minimal cardinality diagnosis. iteration
involves call underlying SAT solver hence worst-time exponential complexity. So,
reducing number important. Given constraints, apply finite domain constraint
compiler (Metodi & Codish, 2012; Metodi, Codish, & Stuckey, 2013) simplify encode
corresponding CNF. Finally apply SAT solver seek suitable satisfying assignment
solve problem. rest section describe phases detail.
384

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

Figure 2: Modeling component c composition xor
experimental evaluation illustrating impact various constraints model presented
Section 7.
5.1 Basic Model MBD
build standard approach, Equation (2). However, observe model based
diagnosis weak fault model single observation searching minimal
diagnosis, behavior faulty component assumed produce output opposite
normal behavior. diagnosis assumes component c faulty
still produces normal output replaced smaller diagnosis contain c. Thus,
minimal diagnosis (i.e, subset diagnosis), means components
assumed produce opposite normal output. paper focus minimal
cardinality diagnoses, particular also minimal subset, modify Equation (2)
follows, replacing implication bi-implication.
= OBS

^

Hc c

(5)

cCOMPS

model behavior (Hc c ) possibly faulty component c encapsulated together
xor gate illustrated Figure 2. Here, output encapsulated component
xor usual output c negated health variable Hc . One observe Hc
true composition equivalent normal behavior c, otherwise equivalent
component c negated output.
decision model relation component c health variable Hc introducing additional xor gate (instead introducing CNF clauses directly encode Hc c )
two motivations: (1) improve CNF encodings provide tools reason about, simplify
system components advantage uniform representation logic
expressed system model itself; (2) underlying SAT solver apply, CryptoMiniSat (Soos, 2010), offers direct support xor clauses. (1) MBD problem
amenable simplification, (2) underlying SAT solver optimize search
satisfying assignment. comment straightforward apply technique
SAT solvers, support xor clauses, adding CNF encodings model.
finite domain constraint compiler, BEE (Metodi & Codish, 2012), apply, configurable
work types solvers.
Equation (3), write comp(A, B, C) comp {and, or, xor} represent component and00 , or00 xor00 gate inputs A, B output C. also write
compH (A, B, C) represent corresponding encapsulated component health variable H.
So,
compH (A, B, C) = comp(A, B, C0 ) xor(H, C0 , C)
385

fiM ETODI , TERN , K ALECH & C ODISH

view
compH (A, B, C)

(Constraints 1)

constraint Boolean variables A, B, C H. Given notation, system depicted
Figure 1 modeled following constraints:
xorHX1 (A, B, Z1 ) andHA1 (A, B, Z2 ) xorHX2 (Z1 , C, D)
andHA2 (Z1 , C, Z3 ) orHO1 (Z2 , Z3 , E)
Finally, add constraints representing system components additional cardinality
constraint:
fi

n
fi
sum leq( Hc fi c COMPS , k)
(Constraint 2)
specify integer constant k number faulty components must k.
example, system depicted Figure 1 constant k, introduce constraint
sum leq({HX1 , HX2 , HA1 , HA2 , HO1 }, k). Later require satisfy constraints
model also minimize value k.
summarize presentation basic model, show complete constraint model
minimal cardinality diagnosis MBD problem Figure 1. integer value k,
solution constraints diagnosis cardinality k:
xorHX1 (A, B, Z1 ) andHA1 (A, B, Z2 ) xorHX2 (Z1 , C, D)
andHA2 (Z1 , C, Z3 ) orHO1 (Z2 , Z3 , E)
sum leq({HX1 , HX2 , HA1 , HA2 , HO1 }, k)
A=1 B=0 C=1 D=0 E=0
type constraint model solved encoding CNF formula applying
SAT solver. repeatedly seeking solution decreasing values k find minimal
cardinality diagnosis. However, apply basic modeling. Instead refine
described rest section.
5.2 Encoding Cardinality Constraints
encoding cardinality constraints CNF topic large body research papers. Many
these, described Een Sorensson (2006), based use Batchers oddeven sorting network (Batcher, 1968). sorting network Boolean circuit n inputs n
outputs. Given Boolean values inputs, output consists values sorted: say,
zeroes ones. context apply sorting network n inputs health
variables n components given system, n outputs sorted values. Now,
encode k health variables take value false assert (k + 1)th output
sorting network one. outputs sorted implies last n k outputs
also ones thus imposing k remaining outputs zero. Looking backwards
sorting network implies k inputs take value false.
Sorting networks, like Boolean circuits straightforward encode CNF formula.
Batchers odd-even construction results CNF O(n log2 (n)) clauses.
improvements enable encoding O(n log2 (k)) clauses constrain sum n Boolean
inputs less k (Asn, Nieuwenhuis, Oliveras, & Rodrguez-Carbonell, 2009, 2011; Codish
386

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

& Zazon-Ivry, 2010). paper encode cardinality constraints CNF using BEE (Metodi &
Codish, 2012; Metodi et al., 2013) takes improved approach.
5.3 Cones Sections
Reasoning relations components system description SD enables infer additional constraints number unhealthy components certain subsystems SD.
constraints compiled CNF, help boost search, SAT solver, minimal
cardinality diagnosis. Proposition 2 enables diagnosis algorithm focus top-level diagnoses
based partitioning system cones: cone contains one unhealthy component, without loss generality, assumed dominator cone.
restrict SAT-based search top-level minimal cardinality diagnoses simply add
following constraints denotes set dominated components.
V

(Constraint 3)

cD Hc

Introducing constraints indicate healthy components reduces number (unassigned) health
variables hence boosts search minimal cardinality diagnosis. Section 7 show
reasoning cones restrict search top-level diagnoses improves considerably search
minimal cardinality diagnosis.
Motivated utility partitioning system cones, seek general partitioning,
enables apply similar cardinality constraints larger subsystems components.
end introduce notion section. denote sysout(c) set system outputs
occur end path component c. example, system depicted Figure 3
sysout(C1 ) = {O2 , O3 } sysout(C5 ) = {O1 , O2 }.
Definition 5 (Section) Given system description SD components COMPS define disjoint
partitioning COMPS = S1 S2 Sn every c1 , c2 COMPS , c1 c2
section Si sysout(c1 ) = sysout(c2 ).

C5
C1
C3

C2
C4

S1

S2

C6

S3

C7

C8

C9

C10

O1
S4

S5

O2

O3

Figure 3: Partitioning system cones sections.
Figure 3 shows partitioning system maximal cones sections. cones depicted dotted lines, sections dashed. example, components {C1 , C2 } form
cone, section S1 consists three cones. observe partitioning system sections
done polynomial time demonstrated Algorithm 1 presented below. Given partitioning
387

fiM ETODI , TERN , K ALECH & C ODISH

{S1 , . . . , Sn } sections, introduce constraint model following constraints improve encoding hence subsequent search minimal cardinality diagnosis.
section Si , constraint
fi

n
fi
sum leq( Hc fi c Si , bi )
(Constraints 4)
expresses sum negated health variables Si bounded constant bi
smaller following two bounds number unhealthy components section
Si : (a) number outputs Si ; (b) value |sysout(c)| component
c Si . Note Definition 5, value c Si . justify statement
Proposition 3.
illustrate utility sections, consider system given Figure 3 partition
5 sections. Observe section labeled S1 3 outputs, component c S1
2 corresponding system outputs (|sysout(c)| = 2). So, b1 = min{3, 2} hence 2
upper bound number unhealthy components S1 . also improvement
reasoning cones bound number unhealthy components S1 3
(since three cones). Similarly, b2 = min{1, 2}, b3 = min{1, 1}, b4 = min{1, 1}
b5 = min{1, 1}.
Reasoning constraints number faulty components per section facilitates
MBD encoding another way. Constraints 4 already encoded number faulty
components per section. numbers partial sums context Constraint 2
specifies total number faulty components system reused encoding.
following justifies Constraints 4 .
Proposition 3 Let hSD, COMPS , OBS MBD problem, SD section, c component minimal cardinality diagnosis. Then, (a) number outputs S,
(b) value |sysout(c)| bounds number unhealthy components (from ) S.
Proof: statement regarding number outputs follows directly assertion
de Kleer (2008) number outputs (sub-) system bound number
unhealthy components. So, remains prove statement regarding |sysout(c)|.
Assume premise proposition, denote || = k |S | = (so k). Assume
contradiction > |sysout(c)|. construct diagnosis 0 less k unhealthy
components. First note obvious: given propagate observed system inputs
system outputs step choose component known inputs produce
normal output component healthy, opposite normal output otherwise.
diagnosis process result contradictions propagated outputs
observed outputs.
Now, take 0 = \ S. (yet) diagnosis. 0 , propagate observed system
inputs way . Now, 0 diagnosis
flipped system outputs (those contradict observed outputs). flipped output
must due one unhealthy components marked healthy 0
sysout(c). consider component g outputs o. g 0 , remove it;
g 6 0 , add it. So, 0 diagnosis k 0 = |0 | k + |sysout(c)| < k.
Algorithm 1 describes partition system sections. Denoting components
outputs system COMPS = {c1 , . . . , cn } OUTS = {o1 , . . . , om }, n Boolean
388

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

matrix b computed bij = true oj sysout(ci ) false otherwise. Figure 4 shows
example matrix b system Figure 3. So, Definition 5, pair components ci , cj
section row row j matrix b identical. instance, section
S5 includes components C9 C10 since system output O3 identical. computational
complexity partitioning process complexity running graph search algorithm
every system output worst case O(n2 m). algorithm returns mapping
components bit vectors seen section identifiers. So, algorithm returns
mapping components sections.
Algorithm 1 partitioning system sections
input: system (view graph)
output: partitioning system sections
= {c1 , . . . , cn }
system components
OUTS
= {o1 , . . . , om } system outputs
b
= (bij )
n Boolean matrix
(oj OUTS )
apply DFS reverse edges system, source = oj
(ci COMPS )
bij
fi oj )

= (ci reachable
return ci 7 hbi1 , . . . , bim fi 1 n

1: Denote:

2:
3:
4:
5:
6:

COMPS

Example 1 (partition sections) Consider
(abstract) system depicted Figure 3 COMPS = {c1 , . . . , c10 }
OUTS = {o1 , . . . , o3 }. Boolean matrix evaluated application Algorithm 1 illustrated
Figure 4.

Figure 4: Partitioning system
Figure 3 sections
another benefit partitioning sections: identification cones may performed per section efficient. works because, component X dominated component G sysout(X ) = sysout(G) implying components cone
always section. example, Figure 3 component C1 dominated C2
sysout(C1 ) = sysout(C2 ) = {O1 , O2 }.
recursively defined Algorithm 2 shows compute cones given partition sections.
computes set dominators component c section system. denote
succ(c) set components c feeds directly. c feeds component
dominated itself. Otherwise, c dominated c0 c0
dominated elements succ(c). instance, given section S1 component C1 Figure 3
succ(C1 ) = {C2 }. next recursive call succ(C2 ) include component S1
389

fiM ETODI , TERN , K ALECH & C ODISH

(condition line 2) thus C2 returned (line 5). union calls (C1 , C2 ) returned
cone (line 3).
straightforward implement Algorithm 2 efficiently using memoization table avoid
recomputing dominators components already encountered. Since system directed acyclic
graph, recursion Algorithm 2 halt leaf node reached. Thus complexity
calculating dominators every component c section O(|S|2 ). Given sets dominators per component, straightforward specify set maximal cones. component c
dominator maximal cone, dominated itself, maximal cone corresponding
c set components c dominator.
find cones system, Algorithm 2 applied per component per section,
cost depends size largest section. contrast, without partition sections,
algorithm applied, considering components system instead
components section. Practice shows partition sections benefits computation
cones.
Algorithm 2 dominators (component c, section S, system C)
input: component c section system C
output: set dominators c
fi


1: Denote: succ(c) = c0 C fi output c input c0
2: (succ(c) S) \
3:
return {c}
dominators(c0 , S, C)
c0 succ(c)

4: else
5:
return {c}

5.4 Modeling Observation Boosting Search
Let OBS + OBS denote sets variables assigned true false OBS , respectively. Then,
model observation add obvious constraints.
V
V
(Constraint 5)
xOBS x
xOBS + x
improve search minimal cardinality diagnosis one introduce upper bound
minimal cardinality: number outputs system upper bound minimal
cardinality (de Kleer, 2008). bound, well section-specific constraint given
(Constraints 4 ) ignores observed inputs outputs. Siddiqi Huang (2011) propose
obtain tighter upper bound minimal cardinality given observation propagating
input values system, taking upper bound number contradictions
observed propagated outputs. example, considering MBD problem
Figure 1, k = 2 upper bound size minimal cardinality diagnosis
system 2 outputs. Siddiqi Huangs proposal states also 1 upper bound
propagating inputs system one contradiction observed
outputs.
Siddiqi Huangs (2011) proposal intuitively appealing, correct case
observed output also input another component, fact results restricted
systems case. work example Figure 5. Propagating
390

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

Figure 5: Minimal cardinality diagnosis size 2, propagating observed inputs leads 1
contradiction observed outputs.
observed inputs system assigns 0 outputs indicating single contradiction
observation (on O1 ). However, smallest diagnosis example cardinality 2.
example contrived: 83 350 observations system 74181 74XXX benchmark,
exhibit minimal cardinality diagnosis larger (erroneous) bound obtained counting
conflicts propagated observed outputs.
Algorithm 3 computing upper bound number faulty components propagating observed inputs counting conflicts. algorithm computes diagnosis , ||
thus upper bound minimal cardinality diagnosis. basic idea propagate inputs
long contradict observed, already computed, outputs. components
system processed one time. line 3 select component c whose inputs
already determined (initially system inputs determined). c consider
already determined output, oobs , denote oobs = output yet determined. also
consider propagated output oprop obtained propagating inputs c assuming c healthy. three cases (lines 610): c already determined output
fix output oprop mark c healthy. c already determined output
consistent oprop also mark c healthy. Otherwise mark c healthy,
propagate output (which already determined).
Algorithm 3 Find diagnosis (and upper bound || min. card.)
input: system components, COMPS , observation, OBS
output: diagnosis
1: C COMPS , =
2: (C 6= )
3:
select c C inputs c determined
4:
oobs value output c (N/A undefined)
5:
oprop value propagating inputs c (assume c healthy)
6:
(oobs = N/A
7:
set output c oprop mark c healthy
8:
else (oobs = oprop )
9:
mark c healthy
10:
else
11:
mark c faulty = {c}
12:
C C \ {c}
13: Return

Algorithm 3 terminates marked components healthy faulty
fact determined correct diagnosis. applying Algorithm 3 provides upper bound
minimal cardinality diagnosis number components marked faulty returned
diagnosis. Note Algorithm 3 correct also given probes (observed values outputs
391

fiM ETODI , TERN , K ALECH & C ODISH

internal components). Assuming components maintained data-structure
components sorted (topologic) according depth, Algorithm 3 performed single
linear traversal data-structure complexity O(|COMPS |).
example application Algorithm 3, consider circuit Figure 5. Propagating
inputs gate A1 gives output 0 contradiction observation O1 . Hence, mark A1
unhealthy propagate observation O1 = 1 input A2 together I3 = 1. results
additional contradiction observation O2 = 0 mark A2 unhealthy too,
report = {A1 , A2 } hence value 2 upper bound minimal cardinality.
Let kUB bound found application Algorithm 3. refine Constraint 2 introduce
instead:
fi
n

fi
sum leq( Hc fi c COMPS , kUB )
(Constraint 20 )
appreciate impact Algorithm 3 note that, benchmark considered
paper, Algorithm 3 determines upper bound equal actual minimal cardinality 81%
28,527 observations considered. course, even given precise upper bound, MBD
algorithm still needs validate minimality. SAT-based approach requires one single
iteration underlying SAT solver. Typically, hardest iteration involves call
unsatisfiable largest cardinality instance unsatisfiable.
5.5 Compiling Constraints CNF
Metodi Codish (2012) introduced compiler called BEE encodes finite domain constraints
CNF. Besides facilitating encoding process, compiler also applies partial evaluation
optimizations simplify constraints encoding CNF. particular, applies
equi-propagation (Metodi, Codish, Lagoon, & Stuckey, 2011) process identifying
equalities literals (and constants) implied equations given constraint.
X=L implied constraint (where X variable L literal Boolean constant),
occurrences X replaced L, reducing number variables subsequent CNF
encoding. illustrate constraint simplification diagnosis circuit Figure 1. Consider
following constraints (we omitted constraints contribute
example):
(1)
(2)
(3)
(4)
(5)

xorHX1 (A, B, Z1 ) andHA1 (A, B, Z2 ) xorHX2 (Z1 , C, D)
andHA2 (Z1 , C, Z3 ) orHO1 (Z2 , Z3 , E)
sum leq({HX1 , HX2 , HA1 , HA2 , HO1 }, k)
A=1 B=0 C=1 D=0 E=0
HA1 = 1 HA2 = 1

constraints lines (1) (3) comprise basic constraint model described Section 5.1;
constraints line (4) model observation; constraints line (5) express without
loss generality dominated components {A1 , A2 } cone {A1 , A2 , O1 } healthy.
observe following equi-propagation steps:
1. (A = 1) (B = 0) xorHX1 (A, B, Z1 ) |= (Z1 = HX1 )
2. (A = 1) (B = 0) (HA1 = 1) andHA1 (A, B, Z2 ) |= (Z2 = 0)
392

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

3. (C = 1) (HA2 = 1) andHA2 (Z1 , C, Z3 ) |= (Z1 = Z3 )
4. (C = 1) (D = 0) xorHX2 (Z1 , C, D) |= (Z1 = HX2 )
5. (E = 1) (Z2 = 0) orHO1 (Z2 , Z3 , E) |= (Z3 = HO1 )
(and given) equalities literals obtain substitution:
)
(
7 1, B 7 0, C 7 1, 7 0, E 7 0, Z1 7 HX1 , Z2 7 0,
=
Z3 7 HX1 , HX2 7 HX1 , HA1 7 1, HA2 7 1, HO1 7 HX1
Applying specialize constraint system get:
(1)
(2)
(3)
(4)
(5)

xorHX1 (1, 0, HX1 ) and1 (1, 0, 0)
xorHX1 (HX1 , 1, 0)
and1 (HX1 , 1, HX1 ) or(HX1 ) (0, HX1 , 0)
sum leq({HX1 , HX1 , 0, 0, HX1 }, k)
1=1 0=0 1=1 0=0 0=0
1=1 1=1

Now, constraints tautologies remove them. remains single
constraint:
(3) sum leq({HX1 , HX1 , HX1 }, k)
satisfied k = 1 HX1 = 1, implied
HA1 = 1, HA2 = 1, HX2 = 1, HO1 = 0. example illustrates equi-propagation partial evaluation applied simplify constraints prior encoding CNF.
summarize following observations:
1. constraint model MBD polynomial size system: component
contributes constraint fresh health variable, cone contributes assignment
health variables dominated components, section contributes cardinality constraint, finally observation contributes assignment input variables
system.
2. Constraint simplification using BEE polynomial size constraint model.
because: (a) simplification step reduces number Boolean variables model
least one linear number steps, (b) step checks applicability
fixed number simplification patterns constraint.
3. CNF encoding constraint model polynomial size, constraints
introduces polynomial number clauses CNF (all constraints supported
BEE property).

6. Process Implementation
section summarize different phases diagnosis process approach.
Section 6.1 focus case seek single minimal cardinality diagnosis,
Section 6.2, case seek minimal cardinality diagnoses.
393

fiM ETODI , TERN , K ALECH & C ODISH

6.1 Single Minimal Cardinality Diagnosis
process single minimal cardinality diagnosis consists four phases. Let =
hSD, COMPS , OBS MBD problem. first two phases construct constraint model.
First, focusing SD, introduce constraints independent observation,
per observation introduce constraints. third phase encode constraint model
CNF, k k upper bound size minimal cardinality diagnosis.
fourth phase, solving k using SAT solver results diagnosis cardinality k.
detail four phases.
Phase 1. Modeling system (offline): system SD first preprocessed partition
sections (Algorithm 1) cones (Algorithm 2). Then, introduce Constraints 1 model SD
terms components behavior introduce Constraints 4 bound number unhealthy
components per section. Finally, using information cones, add Constraint 3 asserts
that, without loss generality, dominated components healthy. system preprocessing performed offline, per system.
Phase 2. Modeling observation (online): Constraint 5 added model observation
Constraint 20 added bound total number unhealthy components upper
bound kUB obtained application Algorithm 3.
OBS ,

Phase 3. Encoding: constraint system simplified online, observation
encoded CNF k , applying optimizing CNF compiler (Metodi et al., 2011). parameter k
reflects bound set Constraint 20 bound number unhealthy components diagnosis.
Initially, k computed Algorithm 3.
Phase 4. Solving: compute diagnosis, , seek satisfying assignment encoding, k ,
applying CryptoMiniSat solver (Soos, 2010). set health variables assigned
false assignment. Denoting || = k 0 , seek satisfying assignment, time
0
formula k 1 . satisfying assignment found, indicates smaller diagnosis, 0 .
Otherwise, minimal cardinality. process invoked repeatedly, time finding
0
smaller diagnosis, k 0 formula k 1 satisfiable. Then, diagnosis found
previous iteration minimal cardinality.
facilitate search minimal cardinality diagnosis, apply SAT solver wrapper, SCryptoMiniSat (Metodi, 2012b). SCryptoMiniSat takes input CNF formula (k )
Boolean variables representing number k. provides satisfying assignment minimizes k. SCryptoMiniSat takes advantage incrementality underlying SAT solver
maintains learned clauses consecutive calls add clauses.
Algorithm 4 illustrates process finding single minimal cardinality diagnosis (identified
line 17), returning set minimal cardinality diagnoses (line 23).
6.2 Minimal Cardinality Diagnoses
find minimal cardinality diagnoses first apply process described Section 6.1 find
single minimal cardinality diagnosis. provides us value k 0 indicating number
faulty components minimal cardinality diagnosis.
Then, enumerate set top-level minimal cardinality diagnoses (each size k 0 )
apply additional functionality SCryptoMiniSat allows enumerate (possibly
394

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

Algorithm 4 SATbD
input: system SD components COMPS observation OBS
output: , set minimal cardinality diagnoses
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

// Phase 1: Offline pre-processing

Sections partition SD sections
Constraints
(S Sections)
(c S)
Add Constraints 1 Constraints
Dominators dominators(c, S, SD)
|Dominators| > 1
Add Constraint 3 component c Constraints

// Algorithm 1

// describes normal behavior component c
// Algorithm 2
// sets dominated gates healthy

// Phase 2: Modeling observation

Add Constraint 5 Constraints
Find initial diagnosis
Add Constraint 20 Constraints kUB = ||

// add constraints representing OBS
// Algorithm 3

// Phase 3: Encoding

BEE(Constraints)

// run constraint compiler obtain CNF

// Phase 4: Solving

MC SCryptoMiniSatMinimize(k,)

// find min. card. diagnosis (assignment minimizes k = ||MC )

// Finding diagnoses minimal cardinality

cnf(kUB = |M C |)
, LD SCryptoMiniSatAllSolutions()
( LD )
Add diagnoses expanded
return

395

// restrict Constraint 20 use kUB = |M C |
// find top-level diagnoses
// Proposition 2

fiM ETODI , TERN , K ALECH & C ODISH

Name
74181
74182
74283
c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

|COMP|
65
19
36
160
202
383
546
880
1193
1669
2307
2416
3512

System Details


offline
14
8
0.02
9
5
0.01
9
5
0.01
36
7
0.03
41
32
0.08
60
26
0.06
41
32
0.24
33
25
0.37
233 140
0.29
50
22
0.71
178 123
1.50
32
32
1.48
207 108
1.73

Feldman
#obs.
350
250
202
301
835
1182
836
846
1162
756
2038
404
1557

DXC-09

1-10

11-20

21-30

100
100
100
100
93
44
87
93
66
98
47
100
49

0
0
0
0
7
44
13
7
34
2
45
0
46

0
0
0
0
0
12
0
0
0
0
8
0
5

#obs.
54
50
30
45
141
198
98
127
168
36
248
1
176

1-10

11-20

21-30

100
100
100
100
73
41
79
74
52
100
43
100
45

0
0
0
0
27
40
21
26
43
0
40
0
41

0
0
0
0
0
19
0
0
5
0
17
0
14

Sidd.
#obs.
700
400
800
800
800
800
40
40
40

Table 1: Benchmark suite: systems 74XXX ISCAS-85, observations: Feldman, DXC-09
Siddiqi.

specified time-out) all, specified number of, satisfying assignments given CNF. apply
option enumerate satisfying assignments formula, k described Section 6.1
k = k0 .
Finally, based Proposition 2 expand obtained top-level diagnoses provide minimal cardinality diagnoses. Note observation TLD expanded easily
minimal cardinality diagnoses applies diagnosis algorithm. such, diagnosis algorithms
general focus, compared finding TLDs, instead finding minimal cardinality
diagnoses.

7. Experimental Results
section presents experimental evaluation proposed SAT-based encoding MBD.
Section 7.1, consider search single minimal cardinality diagnosis compare
performance SATbD algorithms: HA* (Feldman & van Gemund, 2006), CDA* (Williams
& Ragno, 2007) SAFARI (Feldman et al., 2010a). Section 7.2, consider search
minimal cardinality diagnoses compare SATbD algorithms: HDIAG (Siddiqi & Huang,
2007) DCAS (Siddiqi & Huang, 2011). Finally, Section 7.3, evaluate impact
various components SAT-based encoding MBD. experiments run Intel Core
2 Duo (E8400 3.00GHz CPU, 4GB memory) Linux (Ubuntu lucid, kernel 2.6.32-24-generic)
unless stated otherwise. entire set tools, range benchmarks, well detailed
report results found online (Metodi, 2012a; Metodi, Stern, Kalech, & Codish, 2012b).
Table 1 provides basic details concerning systems three observation sets benchmark suite. systems 74XXX (Hansen, Yalcin, & Hayes, 1999), described first 3 rows,
ISCAS-85 (Brglez et al., 1989), described following 10 rows. left column
table specifies system name. next four columns (from left) describe systems:
numbers components, inputs outputs systems, also preprocessing time
per system SAT-based approach. includes actions performed per system
described Section 6.1: decomposing system sections cones computing bounds
per section.
396

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

rest columns divided three groups describing experiments three observation sets. first two describe observation set generated Feldman et al. (2010a)
DXC-09 observation set used diagnosis competition (DXC) 2009. applied
experimentation evaluate search single minimal cardinality diagnosis. minimal
cardinality diagnoses observations 1 30. columns
two groups indicate number observations distribution observations according
size minimal cardinality diagnoses. observations sets considered hard
many them, high minimal cardinality diagnoses. third group (the rightmost column
table) presents observation set generated Siddiqi Huang (2011) minimal
cardinality bounded 8 observations distributed uniformly according size
minimal cardinality diagnoses. set used evaluation search minimal
cardinality diagnoses.
Table 1 illustrates comprehensive experimental benchmark involving total 28,527 observations varied minimal cardinality diagnosis size. Observe also (offline) preprocessing
time per system negligible. instance, reprocessing largest system, c7552, takes less
two seconds.
7.1 SATbD vs. MBD Algorithms: Single Minimal Cardinality Diagnosis
compare SATbD algorithms: HA* (Feldman & van Gemund, 2006), CDA* (Williams
& Ragno, 2007) SAFARI (Feldman et al., 2010a) application search single minimal cardinality diagnosis. HA* CDA* based complete algorithm find minimal
subset diagnoses contain diagnoses. configured
first minimal subset diagnosis guaranteed also minimal cardinality configuration apply comparison SATbD. SAFARI applies algorithm based stochastic
search guarantee minimal cardinality even minimal subset diagnosis. Feldman et al. (2010a) report even single double fault cardinalities, SAFARI always
find minimal cardinality. So, expense minimality, SAFARI often faster, comparing
HA* CDA*.
Name

HA*
Succ.
Time
rate%
Sec.

74181
68.3
74182
100.0
74283
100.0
c432
78.1
c499
24.1
c880
11.9
c1355
11.4
c1908
6.4
c2670
12.3
c3540
3.7
c5315
2.7
c6288
13.6
c7552
4.2
80 sec timeout
c7552
7.3

CDA*
Succ.
Time
rate%
Sec.

Succ.
rate%

SAFARI
Min Diag.
card.%
ratio

3.15
0.00
0.04
3.63
5.45
3.76
3.90
1.75
4.83
4.30
11.94
7.87
1.06

46.3
100.0
100.0
38.2
10.1
6.3
0.0
0.0
0.0
0.0
0.0
0.0
0.0

4.51
0.01
1.45
5.15
1.22
6.66
-

100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
53.5
0.0

44.0
91.0
57.0
28.0
7.0
48.0
5.0
17.0
14.0
9.0
9.0
25.0
-

20.77

0.0

0.0

99.5

13.0

Time
Sec.

SATbD
Succ.
Time
rate%
Sec.

1.33
1.04
1.28
1.68
2.00
1.09
1.96
1.92
1.52
2.06
1.96
7.88
-

0.00
0.00
0.00
0.03
0.05
0.18
0.37
1.08
2.71
5.25
13.34
16.18
-

100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
99.3

0.02
0.01
0.02
0.03
0.04
0.05
0.07
0.14
0.15
0.27
0.42
0.56
1.07

1.68

43.50

100.0

1.49

Table 2: Single minimal cardinality diagnosis, Feldmans observations (30 sec. timeout).
397

fiM ETODI , TERN , K ALECH & C ODISH

Algorithm
System
74181
74182
74283
c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

Min
0.00
0.00
0.00
0.00
0.02
0.01
0.04
0.44
0.05
0.22
0.19
0.21
0.47

HA*
Max
29.40
0.00
0.76
29.53
21.43
29.43
22.12
14.78
27.91
29.42
29.68
28.32
9.47

St. dev.
6.04
0.00
0.08
6.33
5.03
6.70
3.88
2.51
6.88
6.26
11.98
8.38
1.86

Min
0.00
0.00
0.00
0.04
0.88
0.24
-

CDA*
Max
St. dev.
29.01
7.30
0.08
0.02
28.14
2.79
26.26
7.00
1.55
0.21
29.87
8.18
-

Min
0.00
0.00
0.00
0.02
0.04
0.14
0.32
0.95
2.44
4.78
12.24
13.27
30.00

SAFARI
Max
St. dev.
0.01
0.00
0.00
0.00
0.00
0.00
0.03
0.00
0.06
0.00
0.21
0.01
0.43
0.02
1.19
0.04
3.03
0.09
6.20
0.16
14.68
0.36
30.96
4.78
30.22
0.01

Min
0.00
0.00
0.00
0.00
0.01
0.01
0.02
0.03
0.04
0.06
0.09
0.10
0.14

SATbD
Max
St. dev.
0.03
0.01
0.02
0.00
0.03
0.01
0.05
0.01
0.07
0.01
0.14
0.01
0.10
0.02
0.52
0.04
0.22
0.04
0.84
0.10
5.93
0.31
1.27
0.22
24.30
2.03

Table 3: Single minimal cardinality diagnosis, Feldmans observations, additional statistics.
Table 2 presents evaluation focusing Feldmans observations imposing 30 second timeout (except bottom line). columns indicate algorithm, percentage observations solved within prescribed timeout (Succ. rate %) average search time (Time
Sec.) average computed set observations excluding timeouts. SATbD,
search time: (1) includes actions performed per observation described Section
6.1 (Modeling observation (online)); (2) Excludes cost actions performed per system described Table 1 Section 6.1 (column offline); (3) includes times adding
observation cardinality constraints, encoding CNF solving SCryptoMiniSat.
results Table 2 show clearly SATbD outperforms evaluated algorithms,
terms success rate well terms average runtime. SATbD also outperforms
SAFARI succeeds compute diagnosis almost systems. However,
small percentage minimal cardinality indicated column Min card%
shows percentage (excluding timeouts) observations diagnosis found
SAFARI actually minimal cardinality. also show, column titled Diag. ratio,
ratio average cardinality diagnoses found SAFARI average minimal
cardinality. example looking data system 74181, 44% diagnoses found
SAFARI minimal, average diagnosis size found 1.33 times larger average
size minimal cardinality diagnosis. observe SATbD computes verifies minimal
cardinality diagnoses even observations minimal cardinality 30. best
knowledge, algorithm succeeded compute minimal cardinality diagnosis hard
observations.
Table 3 details additional statistics running times presented Table 2, showing
minimum, maximum, standard deviation runtime solved observations.
seen, standard deviation SATbD small compared algorithms.
displayed statistics cases solved within 30 second timeout. Table entries
- mark cases corresponding algorithms could find minimal cardinality diagnosis
(within timeout) even single observation.
Observe Table 2 99.3% 1,557 observations system c7552 solved SATbD
within 30 second timeout (only 11 observations solved). observations are,
however, solved within 80 seconds each, indicated last row table. One may observe
SATbD solves observations given 80 seconds, algorithms still able
398

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

solve them, exception SAFARI. Given extended 80 seconds timeout, SAFARI
also able solve almost observations system, returns minimal cardinality
diagnoses 13% cases, average, size diagnosis found SAFARI
1.68 times larger actual minimal cardinality.
Name

HA*
Succ.
Time
rate%
Sec.

CDA*
Succ.
Time
rate%
Sec.

Succ.
rate%

SAFARI
Min
card.%

Time
Sec.

SATbD
Succ. Time
rate%
Sec.

74181
74182
74283
c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

94.4
100.0
100.0
75.6
12.8
10.1
9.2
7.1
11.3
11.1
1.2
100
2.3

57.4
100.0
100.0
40.0
5.7
4.5
0.0
0.0
0.0
0.0
0.0
0.0
0.0

100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
0.0

65
90
80
22
3
48
4
14
18
31
7
100
-

0.00
0.00
0.00
0.03
0.05
0.18
0.38
1.12
2.85
5.83
13.12
25.28
-

100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0

3.86
0.00
0.01
3.70
4.81
5.51
3.33
3.10
5.86
1.23
19.77
0.24
0.47

5.19
0.01
0.32
7.17
1.20
9.66
-

0.02
0.00
0.01
0.03
0.05
0.05
0.07
0.20
0.14
0.29
0.62
0.1
1.01

Table 4: Single minimal cardinality diagnosis, DXC-09 observations (30 sec. timeout).
Table 4 shows evaluation DXC-09 benchmark format Table 2
(except omit details regarding size diagnoses found SAFARI). results
exhibit trend: SAT-based method substantially faster previous algorithms. Note benchmark even contains observations minimal cardinality diagnoses
31 (!), also solved SATbD 30 seconds timeout. Table 5 provides additional
statistics runtimes format Table 3. too, see small standard
deviation SATbD compared algorithms. Note data c6288 system
given, single observation system DXC-09 observation set.
Figure 6 details evaluation single minimal cardinality diagnosis Feldmans observations four systems (from smaller larger). system plot average runtime
find single minimal cardinality diagnosis (including timeouts) function value
minimal cardinality. black diamond labeled Timeout marks time limit,
algorithms halted.
Typically, diagnosis problem becomes harder minimal cardinality increases. First
consider three plots Figures 6a, 6b 6c. two upper curves correspond systems
HA* CDA* quickly converge 30 seconds timeout. curves SAFARI
less constant minority diagnoses actually minimal cardinality (7%
c499, 48% c880 9% c5315). performance SAFARI affected
cardinality since first finds arbitrary diagnosis proceeds stochastically minimize
diagnosis using pre-defined number attempts. process involves consistency checks
affected size system cardinality diagnosis.
consider plot Figure 6d omit curves HA* CDA* depict
constant 80 second timeout. performance SAFARI less constant (around
average 43.5 seconds) 13% diagnoses found actually minimal cardinality.
contrast, SATbD considerably faster scales solve even hardest observations.
399

fiM ETODI , TERN , K ALECH & C ODISH

Algorithm
System
System
74181
74182
74283
c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c7552

Min
Min
0.00
0.00
0.00
0.00
0.01
0.01
2.74
0.85
0.05
0.31
9.86
0.47

HA*
Max
Max
27.26
0.00
0.02
23.06
11.34
29.60
6.52
15.34
26.04
3.44
28.39
0.47

St. dev.
St. dev.
6.70
0.00
0.01
5.98
4.99
8.34
1.20
4.63
8.06
1.49
9.34
0.00

CDA*
Max
St. dev.
Max
St. dev.
20.12
7.44
0.08
0.02
1.22
0.40
26.14
8.75
1.42
0.16
20.33
8.97
-

Min
Min
0.00
0.00
0.01
0.05
0.93
0.27
-

SAFARI
Max
St. dev.
Max
St. dev.
0.01
0.00
0.00
0.00
0.00
0.00
0.04
0.00
0.07
0.01
0.24
0.01
0.42
0.02
1.25
0.05
3.07
0.11
6.18
0.16
14.11
0.32
-

Min
Min
0.00
0.00
0.00
0.03
0.04
0.16
0.34
1.00
2.56
5.53
12.10
-

Min
Min
0.00
0.00
0.00
0.01
0.01
0.02
0.02
0.03
0.05
0.06
0.10
0.17

SATbD
Max
St. dev.
Max
St. dev.
0.03
0.01
0.02
0.00
0.03
0.01
0.04
0.01
0.07
0.01
0.07
0.01
0.09
0.02
1.36
0.16
0.21
0.03
0.84
0.20
10.06
0.90
11.54
1.54

100 Timeout

100

Time (sec.), log scale

Time (sec.), log scale

Table 5: Single minimal cardinality diagnosis, DXC-09 observations (30 sec. timeout), additional
statistics.

10
CDA*

HA*

Safari

SATbD

1
0.1

10
CDA*

HA*

Safari

SATbD

1
0.1

0.01

0.01
1

3

5
7
9
11
Minimal Cardinality

13

15

1

(a) System c499 (30 sec. timeout).
100

Time (sec.), log scale

10
CDA*

HA*

Safari

3

5

7

9 11 13 15 17 19 21 23 25
Minimal Cardinality

(b) System c880 (30 sec. timeout).

100 Timeout

Time (sec.), log scale

Timeout

SATbD

1

Timeout

10

Safari

SATbD

1

0.1

0.1

0.01

0.01
1

3

5

7

9 11 13 15 17 19 21 23 >24
Minimal Cardinality

1

(c) System c5315 (30 sec. timeout).

3

5

7

9 11 13 15 17 19 21 >22
Minimal Cardinality

(d) System c7552 (80 sec. timeout).

Figure 6: Single minimal cardinality diagnosis, Feldmans observations, average search time per
size minimal cardinality diagnosis.

results section clearly indicate SAT based approach outperforms
three algorithms search single minimal cardinality diagnosis.
400

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

Name
c432
c499
c880
c1355
c1908
c2670
c5315
c6288
c7552

IntelXeon X3220, 2.4GHz, 2Gb RAM
HDIAG
DCAS
Succ.
Time
Succ.
Time
rate%
Sec.
rate%
Sec.
100.0
0.21
100.0
0.31
100.0
0.12
100.0
0.20
99.0
0.07
99.0
0.12
99.5
0.16
99.5
0.15
90.5
368.13
76.5
82.25
90.0
176.17
100.0
3.15
0.0
97.5
52.34
0.0
27.5
305.10
0.0
87.5
260.93

Intel Core 2-Duo E8400, 3.00GHz, 4GB RAM
SATbD
Succ.
TLD
Time
TLD

rate%
Sec.
Sec.
count
count
100.0
0.07
0.09
7.9
72
100.0
0.08
0.10
2.4
345
100.0
0.08
0.11
10.3
963342
100.0
0.13
0.16
2.8
331927
100.0
0.25
0.30
19.9
1894733
100.0
0.23
0.29
6.7
8492
100.0
0.58
0.67
26.4
356949
50.0
104.58
105.14
7231.8
37499
100.0
1.01
1.12
64.9
68396

Table 6: Siddiqis observation set: search minimal cardinality diagnoses (1800 sec. timeout).
7.2 SATbD vs. MBD Algorithms: Minimal Cardinality Diagnosis
compare SATbD algorithms HDIAG (Siddiqi & Huang, 2007) DCAS (Siddiqi
& Huang, 2011) application search minimal cardinality diagnosis. algorithms search complete set minimal cardinality diagnoses. consider observations
generated Siddiqi Huang (2011) minimal cardinality bounded 8.
Table 6 presents results evaluation. results HDIAG DCAS quoted
Siddiqi Huangs work (2011) experiments reported IntelXeon X3220
2.4GHz, 2Gb RAM. present results systems Siddiqi Huang reported
results. Although machines differ (with advantage SATbD), results show clear
advantage SATbD faster orders magnitude larger systems.
three algorithms table reports on:
Succ. rate% indicating percentage observations algorithm finds
minimal cardinality diagnoses within 1800 second timeout;
Time sec. indicating average computation times find minimal cardinality diagnoses
(taking average set observations time out).
SATbD report also
TLD sec. average runtime compute top level diagnoses;
TLD count number top level diagnoses;
count total number minimal cardinality diagnoses found.
Table 6 illustrates SATbD clearly outperforms HDIAG DCAS. succeeds compute
minimal cardinality diagnoses observations systems except c6288
succeeds 50% 40 observations compared 26.5% DCAS. Note
higher success rate, average runtimes SATbD involve harder observations solved
DCAS.
Observe diagnosis time SATbD spent find top level diagnoses indicated column TLD sec. cost compute minimal cardinality diagnoses indicated
column Time sec difference two columns negligible. reflects
401

fiM ETODI , TERN , K ALECH & C ODISH

fact set minimal cardinality diagnoses derived cross product representation
set minimal cardinality diagnoses. Observe also number TLDs (column TLD
count) small comparison huge number minimal cardinality diagnoses (column
count). focus TLDs essential additional solution invokes additional call
SAT solver. Using SAT solver find minimal cardinality diagnoses directly would
hopeless due sheer number.
7.3 Impact Components SATbD
proceed illustrate impact various components SAT-based encoding MBD.
SATbD designed using variety techniques distinguish simple vanilla encoding MBD problem SAT problem described Section 5.1. present evaluation
impact techniques based several experiments using following five configurations SAT based system. configurations incremental: starting basic
model, one adds another component, ending final model applied SATbD.
1. Vanilla. minimal basic SAT encoding MBD described Section 5.1.
assume naive upper bound minimal cardinality determined number
outputs given system.
2. Improved Cardinality Bound. assume vanilla setting consider
improved bound minimal cardinality diagnosis using Algorithm 3.
3. E.P. setting previous applies Equi-Propagation constraint compiler Metodi Codish (2011, 2012) optimize encoding described Section 5.5.
4. Cones. setting previous also partitions system cones
adds constraints restrict search find top-level diagnoses (TLDs), described
Section 3.
5. Sections. setting previous also partitions system
sections introduces corresponding redundant cardinality constraints described Section 5.3.
Figure 7 illustrates impact five settings search single minimal
cardinality diagnosis 1182 observations Feldmans observation set system c880.
horizontal axis, consider observations according size minimal cardinality
diagnosis. vertical axis, illustrate average runtime seconds. runs apply 300
second timeout. choose system c880 present results as: (a) midsize system
observation sets contain observations minimal cardinality diagnosis larger
20; (b) largest system exhibits interesting behavior five configurations
without many timeouts shadow results. larger systems, c3515
c7552 considered below, last two five configurations exhibit interesting curves.
three configurations timeout observations.
upper curve Figure 7 describes Vanilla setting one may observe cardinality increases curve converges 300 second timeout. second curve describes
Improved Bound setting illustrates impact Algorithm 3, especially observations minimal cardinality diagnoses size 1 10. Here, using improved bound reduces
402

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

Figure 7: Impact different settings search single minimal cardinality diagnosis
system c880.

number iterations SAT solver source improvement. explain
see improvement minimal cardinality diagnosis involves larger number
faults, consider first large minimal cardinalities, results meaningless
upper curves converge timeout. medium sized minimal cardinalities, consider
techniques overwhelming part runtime spent last UNSAT iteration. Moreover, number iterations SAT solver techniques
less same. system c880 26 outputs naive bound used
vanilla setting jumps iteration smaller bound (but using
one-by-one decrement). Given evaluation might consider omitting constraints
improved lower bound depending parameters instance. However, cost running
Algorithm 3 negligible always impose corresponding constraints.
third curve (E.P.) shows additional impact applying Equi-Propagation
constraint compiler. results substantial speedups first two settings. example,
finding diagnosis minimal cardinality 20 requires average 18.5 seconds setting
comparison 214.9 seconds without it. two lower curves graph coincide,
fourth setting (cones) makes dramatic impact performance system c880.
average runtime required find first minimal cardinality using setting 0.1
seconds observations. includes finding diagnoses minimal cardinality 26 (!)
58 milliseconds, average. runtimes setting small cannot observe
additional added value applying fifth setting (sections). end consider
next experiment comparison using two larger systems, namely c5315 c7552.
Figure 8 illustrates impact fifth setting involves partitioning system
sections. left graph illustrates impact sections seeking single minimal cardinality
diagnosis observations system c5315 right graph illustrates impact
seeking top-level minimal cardinality diagnoses (TLDs). lower curve (in graphs), summarizes results using sections upper curve without. example, observations
minimal cardinality 24, sections finding first minimal cardinality diagnosis requires
403

fiM ETODI , TERN , K ALECH & C ODISH

Figure 8: Impact sections cones search single minimal cardinality diagnosis (left)
search TLDs (right) using system c5315.

Figure 9: Impact sections cones search single minimal cardinality diagnosis (left)
search TLDs (right) using system c7552.
average 1.13 seconds, without requires 2.43 seconds. observations,
sections, find TLDs average 6 seconds, without requires 15 seconds.
Similar trends illustrated Figure 9 depicts information system c7552.
apply timeout 300 seconds, timeout encountered 48 1,557 observations searching minimal cardinality TLDs (timeout observations considered
average runtimes). However, even 48 observations, using sections provides average
50% TLDs without (15,785 vs. 9,585 TLDs) reaching timeout.
Table 7 details, ISCAS-85 benchmark Feldmans observations, average sizes
SAT encodings using full SATbD algorithm. table indicates number
System
# Components
# Variables
# Clauses

c432
160
190
556

c499
202
240
1193

c880
383
227
758

c1908
880
1026
4063

c2670
1193
636
2055

c3540
1669
2051
7456

c5315
2307
2407
11277

c6288
2416
7161
22061

c7552
3512
3525
12731

Table 7: Average SAT encoding sizes ISCAS-85 benchmark Feldmans observations.
404

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

components system number variables clauses resulting CNFs, taking
average observations Feldman benchmark. Notably, indicated table,
SAT encodings extremely small (in worst case) less three CNF variables ten
CNF clauses per system component.

8. Discussion
paper focuses MBD problem considering possibly multiple faulty components weak
fault model. presentation restricted assume that: every component single output,
observation includes single input/output, queries observations specific components (probes), information regarding probability component failures. Even
restrictions, problem addressed paper computationally hard
focus many prior works model-based diagnosis literature (Feldman & van Gemund, 2006;
Williams & Ragno, 2007; Feldman et al., 2010a; Siddiqi & Huang, 2007, 2011).
discuss briefly applicability approach general setting. Deeper
analysis adapt approach general setting research topic own.
believe success applying SAT solvers MBD problems simplified setting paves
way application general settings.
8.1 Boolean Extensions
approach applies directly setting components described propositional
formulae mode fault ignored (i.e., weak fault model setting). Boolean
circuits one straightforward example obvious community
focused attention. examples similar assumptions SAT-based approach
expected apply directly include works by: Abreu et al. (2009) authors model
software components propositional formulae apply MBD algorithm find bugs; Kalech
Kaminka (2005, 2006) authors model robots multi-robot system diagnose
violation coordination constraints among robots; Felfernig et al. (2012) authors
model finite domain constraints diagnose inconsistent constraint sets.
Note restricting components single output really restriction straightforward represent component multiple outputs conjunction single output components. way partition cones sections fully compatible multiple-output
components.
8.2 Probabilities
Real world applications MBD typically come information regarding probability
component faulty many MBD algorithms exploit information prioritize diagnoses
respect likelihood (interalia, see de Kleer & Williams, 1989; Williams & Ragno,
2007). Sachenbacher Williams (2004) showed incorporate fault probabilities treedecomposition diagnosis algorithm. Extending approach tonconsider
fi probabilities
straightfi
forward. essential difference Constraint 2, sum leq( Hc fi c COMPS , k),
specifies objective function n
aim minimize.fiIt replaced
constraint takes
fi
probabilities account: sum leq( Hc (1 pc ) fi c COMPS , k) pc probabil405

fiM ETODI , TERN , K ALECH & C ODISH

ity component c faulty. So, constraints likely faulty contribute less
objective function. Note straightforward normalize constraint coefficients
integers. Constraints form called Pseudo-Boolean constraints encoding
CNF well studied (Een & Sorensson, 2006).
8.3 Testing Probes
Another extension straightforward model SAT concerns testing probing (de Kleer
& Williams, 1987). Here, diagnosis algorithm given multiple observations input/output
behavior system (in testing) additional observations internal wires system (in
probing). assumption faulty components consistently faulty, invalidate
diagnoses inconsistent multiple observations. Similarly, probes invalidate diagnoses consistent new internal observation. methods run iteratively
single consistent diagnosis. techniques straightforward encode SAT.
testing, improve diagnosis simply take conjunction encodings respect
different observations, using health variables. probing also take conjunction
internal observations. main challenge methods reduce number
probes (or tests) required find actual diagnosis. common, greedy, approach address
challenge choose probe (test) maximizes information gain described Feldman
et al. (2010b).
8.4 Strong Fault Model
consider extension approach setting components associated
wider range possible faulty behavior modes. called Strong Fault Model.
instance, setting circuit component may stuck 0 (always returns output 0), stuck
1(always returns output 1), flip (always flips output) (Struss & Dressier, 1989; de Kleer
& Williams, 1989). context example, naive SAT model may obtained follows.
Instead considering single propositional health variable Hc , consider one additional varif
s1
able per fault mode: Hs0
c (stuck zero), Hc (stuck one), Hc (flip). Now, introduce clauses
f
s1
s0
(a) express fault mode fault (Hc Hc Hc Hc ); (b) express
s1
f
one fault component sum leq({Hs0
c , Hc , Hc }, 1). way propositional health
variable Hc , before, indicates component healthy diagnosis assignment
fault types component. However, extension requires reconsider definitions
minimal diagnosis cardinality presents new challenge identifying useful partitions
system solving problem SAT. consider future work.
8.5 Diagnosis Physical Systems
challenging problem. Physical systems typically dynamic, involve components
time dependent behavior, described terms continuous variables. One common approach apply MBD physical systems use qualitative models behavior
system modeled set constraints non-numerical (discrete) descriptions (Subramanian
& Mooney, 1996). well-known example MBD engine makes relaxations
Livingstone Model-Based Diagnosis System (Williams & Nayak, 1996). Livingstone successfully applied Deep Space One, first spacecraft NASAs New Millennium program.
406

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

first important step SAT based approach diagnosis physical systems
provide setting MBD strong fault model able capture probabilities. course,
many additional challenges modeling systems using SAT important
feasible challenge.

9. Conclusion
paper addresses MBD challenge extensively researched 25
years wide range papers propose different algorithms. present novel SATbased solution problem determine first time, minimal cardinality diagnoses
entire standard benchmarks. present extensive experimental evaluation comparing
algorithm HA*, CDA*, SAFARI, HDIAG DCAS. Results unequivocal. algorithm outperforms others, often orders magnitude, search single minimal cardinality
diagnosis well search minimal cardinality diagnoses. succeed find
verify minimal cardinality diagnosis 11 28,257 observations benchmark
30 seconds per observation, remaining 11 80 seconds each.
best knowledge, SATbD algorithm first algorithm find minimal cardinality
standard benchmarks discussed above. details regarding experimental evaluation
well prototype implementation SAT-based MBD tool found online (Metodi,
2012a; Metodi et al., 2012b).
major contribution success approach range preprocessing techniques
presented Section 5. impact demonstrated five configurations system
Section 7.3 full combination fifth configuration. Even SAT,
related solvers, improve conjecture careful modeling choices involving combinations
techniques invaluable success future MBD algorithms. belief
results paper pave way develop apply SAT-based methodologies
MBD problems. particular, extensions diagnosis probabilities components
faulty, sequential diagnosis testing probes, and, challenging, diagnosis
physical systems qualitative models. expect methodology, combines
domain dependent preprocessing, clever modeling SAT, application tools optimize
CNF encodings, relevant hard problems AI SAT-based techniques applicable.
Acknowledgments
research supported Israel Science Foundation, grant 182/13.

References
Abreu, R., Zoeteweij, P., Golsteijn, R., & van Gemund, A. J. C. (2009). practical evaluation
spectrum-based fault localization. Journal Systems Software, 82(11), 17801792.
Asn, R., Nieuwenhuis, R., Oliveras, A., & Rodrguez-Carbonell, E. (2009). Cardinality networks
applications. Kullmann, O. (Ed.), SAT, Vol. 5584 Lecture Notes Computer
Science, pp. 167180. Springer.
Asn, R., Nieuwenhuis, R., Oliveras, A., & Rodrguez-Carbonell, E. (2011). Cardinality networks:
theoretical empirical study. Constraints, 16(2), 195221.
407

fiM ETODI , TERN , K ALECH & C ODISH

Balakrishnan, K., & Honavar, V. (1998). Intelligent diagnosis systems. Journal Intelligent Systems, 8(3/4), 239290.
Batcher, K. E. (1968). Sorting networks applications. AFIPS Spring Joint Computing
Conference, Vol. 32 AFIPS Conference Proceedings, pp. 307314.
Bauer, A. (2005). Simplifying diagnosis using LSAT: propositional approach reasoning
first principles. Bartak, R., & Milano, M. (Eds.), International Conference Integration
AI Techniques Constraint Programming Combinatorial Optimization Problems
(CP-AI-OR), Vol. 3524 Lecture Notes Computer Science, pp. 4963, Berlin, Heidelberg.
Springer-Verlag.
Biere, A., Heule, M., van Maaren, H., & Walsh, T. (Eds.). (2009). Handbook Satisfiability, Vol.
185 Frontiers Artificial Intelligence Applications. IOS Press.
Brglez, F., Bryan, D., & Kozminski, K. (1989). Combinatorial profiles sequential benchmark
circuits. IEEE International Symposium Circuits Systems, pp. 19291934.
Bylander, T., Allemang, D., Tanner, M. C., & Josephson, J. R. (1991). computational complexity abduction. Artificial Intelligence, 49(1-3), 2560.
Codish, M., & Zazon-Ivry, M. (2010). Pairwise cardinality networks. Logic Programming,
Artificial Intelligence, Reasoning (LPAR), pp. 154172.
Darwiche, A. (2001). Decomposable negation normal form. Journal ACM, 48(4), 608647.
de Kleer, J., & Williams, B. C. (1987). Diagnosing multiple faults. Artificial Intelligence, 32(1),
97130.
de Kleer, J. (2008). improved approach generating max-fault min-cardinality diagnoses.
International Workshop Principles Diagnosis (DX).
de Kleer, J., & Williams, B. C. (1989). Diagnosis behavioral modes. International Joint
Conference Artificial Intelligence (IJCAI), pp. 13241330.
Dressler, O., & Struss, P. (1995). Occm. http://www.occm.de.
DXC

(2009).
International diagnostic
https://sites.google.com/site/dxcompetition/.

competition

series.

Website.

Een, N., & Sorensson, N. (2006). Translating pseudo-Boolean constraints SAT. Journal
Satisfiability (JSAT), 2(1-4), 126.
El Fattah, Y., & Dechter, R. (1995). Diagnosing tree-decomposable circuits. International Joint
Conference Artificial Intelligence (IJCAI), 95, 17421749.
Feldman, A., Provan, G., de Kleer, J., Robert, S., & van Gemund, A. (2010). Solving model-based
diagnosis problems Max-SAT solvers vice versa. International Workshop
Principles Diagnosis (DX), pp. 185192.
Feldman, A. (2012). Lydia-ng. http://www.general-diagnostics.com/products.
php.
Feldman, A., de Castro, H. V., van Gemund, A., & Provan, G. (2013). Model-based diagnostic
decision-support system satellites. IEEE Aerospace Conference, pp. 114. IEEE.
Feldman, A., Provan, G., & van Gemund, A. (2010a). Approximate model-based diagnosis using
greedy stochastic search. Journal Artificial Intelligence Research (JAIR), 38, 371413.
408

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

Feldman, A., Provan, G., & van Gemund, A. (2010b). model-based active testing approach
sequential diagnosis. Journal Artificial Intelligence Research (JAIR), 39, 301334.
Feldman, A., & van Gemund, A. J. C. (2006). two-step hierarchical algorithm model-based
diagnosis. Conference Artificial Intelligence (AAAI), pp. 827833.
Felfernig, A., Schubert, M., & Zehentner, C. (2012). efficient diagnosis algorithm inconsistent constraint sets. Artificial Intelligence Engineering Design, Analysis Manufacturing, 26(1), 5362.
Frohlich, P., & Nejdl, W. (1997). static model-based engine model-based reasoning.
International Joint Conference Artificial Intelligence (IJCAI), pp. 466473.
Fujiwara, H., Member, S., Shimono, T., & Member, S. (1983). acceleration test generation
algorithms. IEEE Transactions Computers, 32, 11371144.
Hansen, M. C., Yalcin, H., & Hayes, J. P. (1999). Unveiling ISCAS-85 benchmarks: case
study reverse engineering. IEEE Des. Test, 16, 7280.
Jannach, D., & Schmitz, T. (2014). Model-based diagnosis spreadsheet programs: constraintbased debugging approach. Automated Software Engineering, 1, 140.
Kalech, M., & Kaminka, G. A. (2005). Towards model-based diagnosis coordination failures.
Conference Artificial Intelligence (AAAI), pp. 102107.
Kalech, M., Kaminka, G. A., Meisels, A., & Elmaliach, Y. (2006). Diagnosis multi-robot coordination failures using distributed CSP algorithms. Conference Artificial Intelligence
(AAAI), pp. 970975.
Kirkland, T., & Mercer, M. R. (1987). topological search algorithm ATPG. ACM/IEEE
Design Automation Conference, DAC, pp. 502508.
Knuth, D. E. (2014). Art Computer Programming: Volume 4B, Pre-fascicle 6A, Section
7.2.2.2: Satisfiability. Unpublished. Draft available from: http://www-cs-faculty.
stanford.edu/knuth/fasc6a.ps.gz.
Marques-Silva, J., Lynce, I., & Malik, S. (2009). Conflict-driven clause learning SAT solvers.
Handbook satisfiability, 185, 131153.
Metodi, A. (2012a). SCryptodiagnoser: SAT based MBD solver. http://amit.metodi.
me/research/mbdsolver.
Metodi, A. (2012b). SCryptominisat. http://amit.metodi.me/research/scrypto.
Metodi, A., & Codish, M. (2012). Compiling finite domain constraints SAT BEE. Theory
Practice Logic Programming (TPLP), 12(4-5), 465483.
Metodi, A., Codish, M., Lagoon, V., & Stuckey, P. J. (2011). Boolean equi-propagation optimized SAT encoding. CP, pp. 621636.
Metodi, A., Codish, M., & Stuckey, P. J. (2013). Boolean equi-propagation concise efficient
SAT encodings combinatorial problems. Journal Artificial Intelligence Research (JAIR),
46, 303341.
Metodi, A., Stern, R., Kalech, M., & Codish, M. (2012a). Compiling model-based diagnosis
Boolean satisfaction. Conference Artificial Intelligence (AAAI).
409

fiM ETODI , TERN , K ALECH & C ODISH

Metodi, A., Stern, R., Kalech, M., & Codish, M. (2012b). Compiling model-based diagnosis
Boolean satisfaction: Detailed experimental results prototype implementation. http:
//www.cs.bgu.ac.il/mcodish/Papers/Pages/aaai-2012.html.
Murray, J., Hughes, G., & Kreutz-Delgado, K. (2006). Machine learning methods predicting failures hard drives: multiple-instance application. Journal Machine Learning Research
(JMLR), 6(1), 783.
Nica, I., Pill, I., Quaritsch, T., & Wotawa, F. (2013). route success - performance comparison diagnosis algorithms. International Joint Conference Artificial Intelligence
(IJCAI), pp. 10391045.
Reiter, R. (1987). theory diagnosis first principles. Artificial Intelligence, 32(1), 5795.
Sachenbacher, M., & Williams, B. (2004). Diagnosis semiring-based constraint optimization.
Eureopean Conference Artificial Intelligence (ECAI), pp. 873877.
Selman, B., & Levesque, H. J. (1990). Abductive default reasoning: computational core.
National Conference Artificial Intelligence (AAAI), pp. 343348.
Siddiqi, S. A., & Huang, J. (2007). Hierarchical diagnosis multiple faults. International Joint
Conference Artificial Intelligence (IJCAI), pp. 581586.
Siddiqi, S. A., & Huang, J. (2011). Sequential diagnosis abstraction. Journal Artificial Intelligence Research (JAIR), 41, 329365.
Smith, A., Veneris, A. G., Ali, M. F., & Viglas, A. (2005). Fault diagnosis logic debugging
using Boolean satisfiability. IEEE Trans. CAD Integrated Circuits Systems, 24(10),
16061621.
Soos, M. (2010). Cryptominisat, v2.5.1. http://www.msoos.org/cryptominisat2.
Stein, B., Niggemann, O., & Lettmann, T. (2006). Speeding model-based diagnosis heuristic
approach solving SAT. IASTED international conference Artificial intelligence
applications, pp. 273278.
Stern, R. T., Kalech, M., Feldman, A., & Provan, G. M. (2012). Exploring duality conflictdirected model-based diagnosis. AAAI.
Struss, P., & Dressier, O. (1989). Physical negation: Integrating fault models general
diagnostic engine. International Joint Conference Artificial Intelligence (IJCAI), pp.
13181323.
Struss, P., & Price, C. (2003). Model-based systems automotive industry. AI magazine, 24(4),
1734.
Stumptner, M., & Wotawa, F. (2001). Diagnosing tree-structured systems. Artificial Intelligence,
127(1), 129.
Stumptner, M., & Wotawa, F. (2003). Coupling CSP decomposition methods diagnosis algorithms tree-structured systems. International Joint Conference Artificial Intelligence
(IJCAI), pp. 388393.
Subramanian, S., & Mooney, R. J. (1996). Qualitative multiple-fault diagnosis continuous dynamic systems using behavioral modes. National Conference Artificial Intelligence
(AAAI), pp. 965970.
410

fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS

Torasso, P., & Torta, G. (2006). Model-based diagnosis OBDD compilation: complexity
analysis. Reasoning, Action Interaction AI Theories Systems, pp. 287305.
Wang, J., & Provan, G. (2010). benchmark diagnostic model generation system. Part A: Systems
Humans, IEEE Transactions Systems, Man Cybernetics, 40(5), 959981.
Williams, B. C., & Nayak, P. P. (1996). model-based approach reactive self-configuring systems. National Conference Artificial Intelligence (AAAI), pp. 971978.
Williams, B. C., & Ragno, R. J. (2007). Conflict-directed A* role model-based embedded
systems. Discrete Applied Mathematics, 155(12), 15621595.

411

fiJournal Artificial Intelligence Research 51 (2014) 255-291

Submitted 03/14; published 09/14

Automaton Plans
Christer Backstrom

CHRISTER . BACKSTROM @ LIU . SE

Department Computer Science
Linkoping University
SE-581 83 Linkoping, Sweden

Anders Jonsson

ANDERS . JONSSON @ UPF. EDU

Dept. Information Communication Tecnologies
Universitat Pompeu Fabra
Roc Boronat 138
08018 Barcelona, Spain

Peter Jonsson

PETER . JONSSON @ LIU . SE

Department Computer Science
Linkoping University
SE-581 83 Linkoping, Sweden

Abstract
Macros long used planning represent subsequences operators. Macros
used place individual operators search, sometimes reducing effort required
find plan goal. Another use macros compactly represent long plans. paper
introduce novel solution concept called automaton plans plans represented using
hierarchies automata. Automaton plans viewed extension macros enables
parameterization branching. provide several examples illustrate automaton plans
useful, compact representation exponentially long plans alternative
sequential solutions benchmark domains L OGISTICS G RID. also compare
automaton plans compact plan representations literature, find automaton
plans strictly expressive macros, strictly less expressive HTNs certain
representations allowing efficient sequential access operators plan.

1. Introduction
paper introduce novel solution concept planning call automaton plans.
ease presentation divide introduction two parts. first part discuss
existing concepts plan representation literature. second part describe
novel representation propose.
1.1 Plan Representations
Following introduction TRIPS planning (Fikes & Nilsson, 1971), take researchers
long discover utility storing sequences planning operators, macros (Fikes, Hart, &
Nilsson, 1972). Macros first used tool plan execution analysis. However,
macros turned several useful properties exploited researchers
planning community ever since.
c
2014
AI Access Foundation. rights reserved.

fiB ACKSTR OM , J ONSSON , & J ONSSON

One property possibility compute cumulative preconditions effects, effectively
making macros indistinguishable individual operators. planning instance augmented
set macros, potentially speeding search solution since macros reach
state space individual operators. extreme, search space macros
exponentially smaller search space original planning operators (Korf, 1987).
Moreover, subsequences operators repeated, hierarchy macros represent plan
compactly simple operator sequence, replacing occurrence repeating subsequence single operator (i.e. macro). extreme, one represent exponentially long
plan using polynomially many macros polynomial length (Gimenez & Jonsson, 2008; Jonsson,
2009). Sometimes even possible generate compact macro plan polynomial time,
case macros viewed tool complexity analysis, reducing complexity
solving particular class planning instances.
Macros clearly show advantages associated plan representations
simply store plan sequence actions. Apart obvious purpose saving space,
reasons considering alternative representations. One important reason highlight
properties plan might apparent sequential representation
exploited increased planning efficiency. One prominent example partially ordered plans
represent plans sets actions associated partial orders. Partially ordered plans often
used planning speed search (McAllester & Rosenblitt, 1991). general, fact
exists compact representation plan implies planning instance exhibits form
structure might possible exploit simpler efficient reasoning.
Potentially, exist many alternative plan representations store plans compactly.
compact representations broadly divided two categories. first type plan
representation stores single plan, second type stores set plans. Macros
illustrative example first type, already seen macro plans exponentially
smaller sequential representation. example second type reactive plans, also
known universal plans reactive systems, represent one plan state
goal reachable.
usefulness compact representation depends several factors.
1.1.1 C OMPRESSION P ROPERTIES
Clearly, one important property compact plan representation size. However,
information-theoretic bound compressibility plans: representation containing n bits
distinguish 2n different plans, limiting applicability highly compact represenn
tations. exist TRIPS instances n variables 22 1 different plans (Backstrom &
Jonsson, 2012, Construction 10), implying least 2n 1 bits needed distinguish one
particular solution rest. However, may often suffice represent single solution
arbitrarily chosen. extreme, represent solution compactly storing planning
instance together algorithm solving it.
1.1.2 ACCESS P ROPERTIES
Another important property compact plan representation ability access particular
action plan. Two alternative concepts proposed (Backstrom & Jonsson, 2012):
sequential random access. Sequential access implies actions plan retrieved
256

fiAUTOMATON P LANS

order, random access implies retrieve action given position
plan. forms access, ideally able retrieve actions polynomial time,
something always possible.
1.1.3 V ERIFICATION
third property compact plan representation able verify plan actually constitutes solution given planning instance. complexity plan verification directly related
complexity plan existence, i.e. determining whether instance solution. Assume problem plan verification compact plan representation R complexity class
C. Let X set TRIPS instances p satisfying following condition: p solvable
exists solution p represented R using O(p(||p||)) bits, p polynomial
function ||p|| number bits representation p. assumptions,
problem plan existence X complexity class NPC : non-deterministically guess compact
plan R verify plan solves p. many choices C, plan existence X bounded
away PSPACE, i.e. easier general TRIPS planning. conclude simple verification
comes price: decreased expressiveness corresponding planning instances.
obviously difficult identify representations satisfy three properties
able express reasonably complex plans. reasonable approach look balanced representations expressive computationally efficient. Let us evaluate macros according
three properties above. paper consider grounded macros totally ordered
allow nesting, i.e. macro involve macros long create cyclic dependencies among macros. know exist examples macros provide powerful
compact representation mechanism. Macro plans excellent access properties: sequential
random access performed polynomial time (Backstrom & Jonsson, 2012).
also verifiable polynomial time (Backstrom, Jonsson, & Jonsson, 2012b), implying planning
instances whose solutions represented using polynomial-size macro plans easier
general TRIPS planning, also less expressive.
1.2 Automaton Plans
paper introduce novel solution concept planning, inspired macros, call
automaton plans. automaton plan consists hierarchy automata, endowed
ability call automata. bottom level hierarchy individual plan operators.
Automaton plans viewed extension macro plans along two dimensions. First,
automaton parameterized, enabling compactly represent single operator sequence
whole family sequences. Second, automaton branch input, making possible
store different subsequences operators distinguish providing different input
automaton.
main motivation automaton plans express plans compactly macro plans cannot, maintaining access properties verification reasonable level. present several
examples automaton plans, show useful variety ways. domains
Towers Hanoi, automaton plans represent exponentially long plans even compactly macro plans. Even plans necessarily long, ability parameterize
plans makes possible store repeating families action subsequences common benchmark
domains.
257

fiB ACKSTR OM , J ONSSON , & J ONSSON

test usefulness automaton plans, formally compare automaton plans compact plan representations literature along three dimensions discussed Section 1.1:
compression, access, verification. macro plan also automaton plan, implying
automaton plans least compressed macro plans. like macro plans, automaton plans
sequentially accessed polynomial time. also show subclass automaton plans
admit polynomial-time random access, although still unknown whether result generalizes
automaton plans. Finally, verification automaton plans p2 -complete, causing automaton
plans strictly expressive macros.
Hierarchical Task Networks (Erol, Hendler, & Nau, 1996), HTNs short, also
viewed type compact plan representation. addition planning operators, HTN defines
set tasks, set associated methods expanding task. name suggests,
tasks organized hierarchy planning operators bottom. hierarchy may
considerably compact actual sequence operators plan. general, plans
represented HTNs unique search may required find valid plan.
Instead comparing automaton plans general HTNs, consider totally ordered HTNs
unique plans, i.e. methods associated task mutually exclusive specify
totally ordered expansion. show automaton plan efficiently translated
HTN, causing HTNs least compressed automaton plans. HTNs unique plans
sequentially accessed polynomial time, true random access. Finally,
plan existence totally ordered HTNs known PSPACE-hard (Erol et al., 1996), implying
verification HTNs harder automaton plans, turn causing HTNs strictly
expressive.
Combining results, automaton plan appears offer reasonable tradeoff
compression, access, verification, making interesting candidate type balanced
plan representation discussed earlier. Since automaton plans strictly expressive
macros, use represent plans compactly wider range planning instances.
However, come expense prohibitively expensive computational properties,
since verification easier automaton plans HTNs well general TRIPS planning.
Automaton plans first introduced conference paper (Backstrom, Jonsson, & Jonsson,
2012a). present paper makes following additional contributions:
formalization automaton plans using Mealy machines, providing stronger theoretical
foundation automaton plans automaton theory.
proof plan verification automaton plans p2 -complete, result used
compare expressive power automaton plans compact plan representations.
reduction automaton plans HTNs, proving HTNs strictly expressive
automaton plans, comes price expensive computational properties.
rest paper organized follows. Section 2 describes notation basic concepts,
Section 3 introduces automaton plans. Section 4 illustrates automaton plans using several practical
examples. Section 5 compares computational properties automaton plans
compact plan representations literature. Section 6 describes related work, Section 7
concludes discussion.
258

fiAUTOMATON P LANS

2. Notation
section describe notation used throughout paper. first introduce formal
definition TRIPS planning domains based function symbols, show TRIPS planning
instances induced associating sets objects planning domains. idea used
PDDL language compactly express planning domains planning instances,
definition viewed mathematical adaptation PDDL.
Given set symbols , let n denote set strings length n composed symbols
. Let x n string. 1 k n, use xk denote k-th symbol
x. customary, use denote empty string, satisfies x = x = x. Given
set elements S, let + denote sequences non-empty sequences elements S,
respectively. Given sequence , let || denote length. construct X, let ||X||
denote size, i.e. number bits representation.
2.1 Function Symbols
planning domain abstract description instantiated given set objects
form planning instance. section introduce function symbols facilitate description
planning domains. Formally, function symbol f fixed arity ar(f ) applied
vector objects x ar(f ) produce new object f [x]. Let F set function symbols
let set objects. define F = {f [x] : f F, x ar(f ) } F set
new objects obtained applying function symbol F vector objects
appropriate arity.
Let f g two function symbols F . argument map f g function :
ar(f
) ar(g) mapping arguments f arguments g. Intuitively, result applying

argument x ar(f ) f , argument g either component x constant object
independent x. Formally, 1 k ar(g), either k (x) = xj fixed index j
satisfying 1 j ar(f ), k (x) = fixed object . argument map f g
enables us map object f [x] F object g[(x)] F .
Since argument maps restricted form, characterize argument map f
g using index string f g, i.e. string ({1, . . . , ar(f )} )ar(g) containing indices
f and/or objects . index string f g induces argument map f g
1 k ar(g), k (x) = x(k) (k) {1, . . . , ar(f )} k (x) = (k)
otherwise.
illustrate idea, let F = {f, g} ar(f ) = ar(g) = 2 let = {1 , 2 }.
example index string f g given = 21 , induces argument map f
g input x 2 , first component (x) always equals second component
x, second component (x) always equals 1 . Given , object f [1 2 ] F maps
object g[(1 2 )] = g[2 1 ] F .
2.2 Planning
Let V set propositional variables fluents. literal l non-negated negated fluent,
i.e. l = v l = v v V . Given set literals L, let L+ = {v V : v L}
L = {v V : v L} set fluents appear non-negated negated L,
/ L v V ,
respectively. say set literals L consistent v
/ L v
259

fiB ACKSTR OM , J ONSSON , & J ONSSON

equivalent L+ L = . state V set fluents currently true; fluents
assumed false. set literals L holds state L+ L = .
define update operation state set literals L L = (s \ L ) L+ .
paper focus TRIPS planning negative preconditions. Formally, TRIPS
planning domain tuple = hP, Ai, P set function symbols called predicates
set function symbols called actions. action associated precondition
pre(a) = {(p1 , 1 , b1 ), . . . , (pn , n , bn )} where, 1 k n, pk predicate P ,
k argument map pk , bk Boolean. well-defined, pre(a)
simultaneously contain (p, , true) (p, , false) predicate p argument map
p. postcondition post(a) similarly defined.
TRIPS planning instance tuple p = hP, A, , I, Gi, hP, Ai planning domain,
set objects, initial state, G goal state, i.e. set literals implicitly defining set
states G holds. P implicitly define set fluents P = {p[x] : p P, x ar(p) }
applying predicate vector objects . Likewise, implicitly define
set operators . Thus fluents correspond grounded predicates, operators correspond
grounded actions, reason distinguish action operator text.
initial state P goal state G P subsets (non-negated) fluents.
action x ar(a) , precondition operator a[x] given
pre(a[x]) = {b1 p1 [1 (x)], . . . , bn pn [n (x)]}, pre(a) = {(p1 , 1 , b1 ), . . . , (pn , n , bn )},
bp[y] = p[y] b false, bp[y] = p[y] b true. words, pre(a[x]) result
applying argument map k precondition argument x obtain fluent
pk [k (x)] P , appropriately negated. postcondition post(a[x]) a[x]
similarly defined. Note pre(a) post(a) well-defined, pre(a[x]) post(a[x])
consistent sets literals P .
operator applicable state pre(o) holds s, result
applying post(o). plan p sequence operators = ho1 , . . . ,
pre(o1 ) holds and, 1 < k n, pre(ok ) holds post(o1 ) post(ok1 ).
say solves p G holds post(o1 ) post(on ). Given two operator sequences
, let ; denote concatenation.
P
P
Note p pP ||ar(p) fluents aA ||ar(a) operators, exponential ||p||, description length p. avoid errors due discrepancies instance description
length actual instance size, consider P that, p P A,
ar(p) ar(a) constants independent ||p||. sometimes describe planning instances directly form p = hP , , , I, Gi defining predicates actions arity 0,
implying predicate fluent action operator.

3. Automaton Plans
section define concept automaton plans, similar macro plans.
like macro plan consists hierarchy macros, automaton plan consists hierarchy
automata. Unlike macros, output automaton depends input, making possible
single automaton represent family similar plans. either use automaton plan
represent single plan explicitly specifying input string root automaton, allow
parameterized plans leaving input string root automaton undefined.
260

fiAUTOMATON P LANS

3.1 Mealy Machines
represent individual automata use variant deterministic finite state automata called Mealy
machines (Mealy, 1955), defined tuple = hS, s0 , , , T,
finite set states,
s0 initial state,
input alphabet,
output alphabet,
: transition function,
: output function.
Mealy machine transducer whose purpose generate sequence output
given input string. contrast acceptors generate binary output (accept reject). Executing Mealy machine input x = x1 x2 xn n generates output
(s0 , x1 )(s1 , x2 ) (sn1 , xn ) n sk = (sk1 , xk ) 1 k < n.
extend Mealy machines allow -transitions (i.e. transitions consume input
symbols ). may general cause Mealy machines non-deterministic, include
several restrictions preserve determinism:
redefine partial function : ( {}) S, either
(s, ) defined (s, ) defined , both. still, sense,
total function since always exactly one possible transition state S,
transition may may consume input symbol.
allow -cycles, i.e. must exist subset {s1 , . . . , sn } states
(sk1 , ) = sk 1 < k n (sn , ) = s1 .
require -transitions must always fire, order make behavior Mealy
machines well defined also input symbols consumed.
also allow output, i.e. transition may may generate output symbol . allow
-transitions -output redefine partial function : ( {}) {}.
definition consistent , i.e. state S, (s, ) defined
(s, ) defined, else (s, ) defined . define extended output function
: state S, input symbol input string x ,

(s, ) (T (s, ), ), (s, ) defined,

(s, ) =
,
otherwise,


(s, ) (T (s, ), x), (s, ) defined,
(s, x) =
(s, ) (T (s, ), x), otherwise.
deterministic output Mealy machine input x given (s0 , x) .
customary use graphs represent automata. graph associated Mealy
machine one node per state S. edge states label i/o,
( {}) ( {}), implies (s, i) = (s, i) = o. simplify graphs
adopt following conventions:
261

fiB ACKSTR OM , J ONSSON , & J ONSSON

1/b
/ha, bi
0/c

/

1/c
0/a

Figure 1: example Mealy machine.
edge states label i/h1 , . . . , n ( {}) n used shorthand describe series intermediate states s2 , . . . , sn (s, i) = s2 , (sk1 , ) =
sk 2 < k n, (sn , ) = t, (s, i) = 1 , (sk , ) = k 2 k n.
edge states label n / used shorthand describe series
intermediate states s2 , . . . , sn , (s, ) = s2 , (sk1 , ) = sk
2 < k n, (sn , ) = t, (s, ) = , (sk , ) = 2 k n.
Figure 1 shows example Mealy machine = hS, s0 , , , T, |S| = 4, = {0, 1},
= {a, b, c}. initial state s0 identified incoming edge without origin. Two example
outputs (s0 , 01) = aab (s0 , 1011) = cccbab.
3.2 Automaton Hierarchies
section explain construct hierarchies automata order represent plans.
automaton hierarchy associated TRIPS planning instance p = hP, A, , I, Gi. define
set Au function symbols called automata, i.e. automaton Au fixed arity ar(M ),
something unusual automaton theory. motivation automata used represent
plans viewed abstract actions, input automaton serves dual purpose:
determine fire transitions automaton order generate output, copy
input symbols onto actions automata.
automaton Au corresponds Mealy machine hSM , s0 , , , TM , i,
set objects TRIPS instance p = (A Au) ({1, . . . , ar(M )} ) .
output symbol (u, ) pair consisting action u automaton u Au
index string u. input string x ar(M ) , require output automaton
input x non-empty, i.e. (s0 , x) = h(u1 , 1 ), . . . , (un , n )i +
M.
Given Au, define expansion graph denotes dependencies among automata Au:
Definition 1. Given set Au automata, expansion graph GAu = hAu, directed graph
automata where, pair M, Au, exists state-input
pair (s, ) SM (s, ) = (M , ) index string .
Thus edge automata appears output .
next define automaton hierarchy tuple H = h, A, Au, ri
, A, Au defined above,
GAu acyclic weakly connected,
262

fiAUTOMATON P LANS

r Au root automaton,
exists directed path GAu r automaton Au.
Au, let Succ(M ) = {M Au : } set successors .
height h(M ) length longest directed path node GAu , i.e.

0,
Succ(M ) = ,
h(M ) =

1 + maxM Succ(M ) h(M ), otherwise.
Given automaton hierarchy H, let = maxM Au |SM | maximum number states
Mealy machine representation automaton, let Ar = 1 + maxuAAu ar(u)
maximum arity actions automata, plus one.
aim automaton Au generate output (s0 , x) input x,
define decomposition strategy. requires us process output concrete way
described below. alternative would integrate processing step automata,
would longer correspond well-established definition Mealy machines.
first define notion grounded automata, analogous notion grounded actions
(i.e. operators) planning instance. automaton call [x] automaton associated
input string x ar(M ) , representing called input x. sets Au define set
automaton calls Au = {M [x] : Au, x ar(M ) }, i.e. automata paired input strings
appropriate arity. next define function Apply acts decomposition strategy:
Definition 2. Let Apply : Au (A Au )+ function [x] Au ,
Apply(M [x]) = hu1 [1 (x)], . . . , un [n (x)]i, (s0 , x) = h(u1 , 1 ), . . . , (un , n )i and,
1 k n, k argument map uk induced index string k .
purpose Apply replace automaton call sequence operators
automaton calls. Recursively applying decomposition strategy eventually result
sequence consisting exclusively operators. show Apply always computed
polynomial time size automaton.
Lemma 3. automaton call [x] Au , complexity computing Apply(M [x])
bounded Ar2 .
Proof. definition Mealy machines requires cycle consume least one input symbol.
worst case, fire |SM | 1 -transitions followed transition consumes input
symbol. Since input string x exactly ar(M ) symbols, total number transitions
bounded (|SM | 1)(1 + ar(M )) + ar(M ) |SM | (1 + ar(M )) Ar.
Let h(u1 , 1 ), . . . , (un , n )i output input x. 1 k n, uk
single symbol, k contains Ar 1 symbols. Applying argument map k
induced k input string x linear |k | Ar. Thus computing Apply(M [x]) =
hu1 [1 (x)], . . . , un [n (x)]i requires Ar time space element, since n
Ar, total complexity bounded Ar2 .
represent result recursively applying decomposition strategy, define expansion function Exp automaton calls operators:
Definition 4. Let Exp function (A Au )+ defined follows:
263

fiB ACKSTR OM , J ONSSON , & J ONSSON

1. Exp(a[x]) = ha[x]i a[x] ,
2. Exp(M [x]) = Exp(Apply(M [x])) [x] Au ,
3. Exp(hu1 [y1 ], . . . , un [yn ]i) = Exp(u1 [y1 ]); . . . ; Exp(un [yn ]).
following lemma prove expansion automaton call sequence operators.
Lemma 5. automaton call [x] Au , Exp(M [x]) A+
.
Proof. prove lemma induction h(M ). h(M ) = 0, Apply(M [x]) sequence
operators ha1 [x1 ], . . . , [xn ]i A+
, implying
Exp(M [x]) = Exp(Apply(M [x])) = Exp(ha1 [x1 ], . . . , [xn ]i) =
= Exp(a1 [x1 ]); . . . ; Exp(an [xn ]) = ha1 [x1 ]i; . . . ; han [xn ]i =
= ha1 [x1 ], . . . , [xn ]i A+
.
next prove inductive step h(M ) > 0. case, Apply(M [x]) sequence operators
automaton calls hu1 [y1 ], . . . , un [yn ]i (A Au )+ , implying
Exp(M [x]) = Exp(Apply(M [x])) = Exp(hu1 [x1 ], . . . , un [xn ]i) =
= Exp(u1 [x1 ]); ; Exp(un [xn ]).
1 k n, uk [xk ] operator Exp(uk [xk ]) = huk [xk ]i A+
.
hand, uk [xk ] automaton call, Exp(uk [xk ]) A+

hypothesis

induction
since

h(uk ) < h(M ). Thus Exp(M [x]) concatenation several operator sequences A+
,


operator sequence A+
.

Note proof depends fact expansion graph GAu acyclic, since otherwise
height h(M ) automaton ill-defined. also prove upper bound length
operator sequence Exp(M [x]).
Lemma 6. automaton call [x] Au , |Exp(M [x])| (S Ar)1+h(M ) .
Proof. induction h(M ). h(M ) = 0, Apply(M [x]) = ha1 [x1 ], . . . , [xn ]i sequence
operators , implying Exp(M [x]) = Apply(M [x]) = ha1 [x1 ], . . . , [xn ]i. follows
|Exp(M [x])| (S Ar)1+0 since n Ar.
h(M ) > 0, Apply(M [x]) = hu1 [x1 ], . . . , un [xn ]i sequence operators automaton
calls, implying Exp(M [x]) = Exp(u1 [x1 ]); ; Exp(un [xn ]). 1 k n, uk
|Exp(uk [xk ])| = 1, else |Exp(uk [xk ])| (S Ar)h(M ) hypothesis induction since
h(uk ) < h(M ). follows |Exp(M [x])| (S Ar)1+h(M ) since n Ar.
automaton plan tuple = h, A, Au, r, xi h, A, Au, ri automaton hierarchy x ar(r) input string root automaton r. automaton hierarchies represent
families plans, automaton plan represents unique plan given = Exp(r[x]) A+
.
subsequent sections exploit notion uniform expansion, defined follows:
Definition 7. automaton hierarchy h, A, Au, ri uniform expansion
automaton Au exists number |Exp(M [x])| = x ar(M ) .
words, expanding automaton call [x] always results operator sequence length
exactly , regardless input x.
264

fiAUTOMATON P LANS

/

1/M3 [1]
0/M3 [c]

1/M2 [b]

/

1/M3 [1]

0/a1 [0]

/
1/a1 [1]

0/a2 [1]

0/M2 [1]
M1 [abc]

M2 [a]

M3 [a]

Figure 2: three automata simple example.

4. Examples Automaton Plans
section present several examples automaton plans. aim first foremost
illustrate concept automaton plans. However, also use examples illuminate
several interesting properties automaton plans. example, use small automaton plans
represent exponentially long plans.
begin showing example relatively simple automaton plan two symbols, two
actions, three automata, defined = h{0, 1}, {a1 , a2 }, {M1 , M2 , M3 }, M1 , 100i. Actions a1
a2 arity 1, Figure 2 shows three automata M1 , M2 , M3 (with arity 3, 1,
1, respectively). figure, edge without origin points initial state automaton,
label edge contains name input string automaton.
simplify description index strings argument maps assign explicit names (a,
b, c) symbol input string automaton. argument map described
string input symbol names symbols = {0, 1}. example, label M2 [b]
automaton M1 corresponds output symbol (M2 , 2), i.e. index string M1 M2
assigns second input symbol (b) M1 lone input symbol M2 . Recall symbols
input string two separate functions: decide edges automaton transition
along, propagate information copying symbols onto actions automata.
plan represented given
= Exp(M1 [100]) = Exp(Apply(M1 [100])) = Exp(hM2 [0], M3 [0], M2 [1]i) =
= Exp(M2 [0]); Exp(M3 [0]); Exp(M2 [1]) =
= Exp(Apply(M2 [0])); Exp(Apply(M3 [0])); Exp(Apply(M2 [1])) =
= Exp(ha1 [0]i); Exp(ha2 [1]i); Exp(hM3 [1]i) = Exp(a1 [0]); Exp(a2 [1]); Exp(M3 [1]) =
= ha1 [0]i; ha2 [1]i; Exp(Apply(M3 [1])) = ha1 [0]i; ha2 [1]i; Exp(ha1 [1]i) =
= ha1 [0]i; ha2 [1]i; Exp(a1 [1]) = ha1 [0]i; ha2 [1]i; ha1 [1]i = ha1 [0], a2 [1], a1 [1]i.
Selecting another root automaton call would result different operator sequence. example,
root M1 [000] would result sequence Exp(M1 [000]) = ha1 [1]i, root M1 [101] would
result Exp(M1 [101]) = ha1 [0], a1 [1], a1 [0]i.
next show like macro plans, automaton plans compactly represent plans
exponentially long. Figure 3 shows automaton Mn moving n discs peg peg
b via peg c Towers Hanoi. figure, [ab] action moving disc n
b. n = 1 edge label /hA1 [ab]i. hard show automaton plan
= h{1, 2, 3}, {A1 , . . . , }, {M1 , . . . , MN }, MN , 132i plan Towers Hanoi instance
265

fiB ACKSTR OM , J ONSSON , & J ONSSON

/hMn1 [acb], [ab], Mn1 [cba]i

/

Mn [abc]
Figure 3: automaton Mn automaton plan Towers Hanoi.
li = qi /

/A[zqf qt ap]

qt = lt /
/

D[li qi qt lt ci ct ti xtt yazp]

li 6= qi /T[xli qi ci ti p]
x = y/

qt 6= lt /T[yqt lt ct tt p]
/hLT[tpy], DT[tyzc], UT[tpz]i
/

T[xyzctp]

x 6= y/DT[txyc]
x = y/

/hLA[apy], FA[ayz], UA[apz]i
/

A[xyzap]

x 6= y/FA[axy]

Figure 4: automata delivering package T, moving package using
truck/airplane.

N discs. Unlike macro solutions Towers Hanoi (Jonsson, 2009), automaton plan
single automaton per disc, possible parameterization.
ability parameterize automata also makes possible represent types plans
compactly. Figure 4 shows three automata D, T, combined construct
automaton plan instance L OGISTICS domain. set symbols contains
objects instance: packages, airplanes, trucks, cities, locations. automaton moves
package using truck, input string xyzctp consists three locations x, y, z, city c,
truck t, package p. Initially, truck x, package p y, destination package p
z. actions DT, LT, UT stand DriveTruck, LoadTruck, UnloadTruck, respectively.
Automaton assumes locations z different, else nothing done
automaton outputs empty string, violating definition automaton plans.
hand, locations x may same, automaton checks whether equal.
x different necessary first drive truck x y. use notation
x = x 6= shorthand denote |L| intermediate notes, one per location,
automaton transitions distinct intermediate node assignment location x.
intermediate node |L| edges next node: |L| 1 edges correspond
x 6= one edge corresponds x = y.
truck location y, operator sequence output loads package p t, drives
destination, unloads p t. Automaton moving package using airplane
similarly defined actions FA (FlyAirplane), LA (LoadAirplane), UA (UnloadAirplane).
Automaton delivers package current location destination, input string
li qi qt lt ci ct ti xtt yazp consists initial location li package, intermediate airports qi
266

fiAUTOMATON P LANS

/hU[rk1 l1 ], . . . , U[ln1 kn ln ], D[ln k2 d1 ], . . . , D[dn1 k2 dn ]i
M[rk1 l1 d1 kn ln dn ]
U[l1 l2 l3 ]

/hN[l1 l2 ], pickup-and-loose[l2 ], N[l2 l3 ], unlock[l3 ]i
/hN[l1 l2 ], pickup[l2 ], N[l2 l3 ], putdown[l3 ]i

/
/
/

D[l1 l2 l3 ]
Figure 5: automata automaton plan G RID domain.

qt , target location lt , initial target cities ci ct , truck ti city ci initially x,
truck tt city ct initially y, airplane initially z, package p itself. Automaton
assumes cities ci ct different, else could use automaton transport package
using truck within city. However, locations li qi may equal, well qt lt ,
automaton moves package using truck whenever necessary.
also show example automaton plan G RID domain, robot deliver
keys locations, locked. keys distributed initial locations,
robot carry one key time. actions move robot neighboring location,
pick key (possibly loosing key currently held), put key, unlock location.
Figure 5 shows automaton plan instance G RID. root automaton takes
input current location robot (r) three locations ki , li , di key, ki
current location key, li associated locked location, di destination.
plan works first unlocking locations prespecified order (which must exist problem
solvable) delivering keys destination.
automaton U takes three locations: location robot (l1 ), location key
(l2 ), location unlocked (l3 ). decomposition navigates key, picks up,
navigates location, unlocks it. Delivering key works similar way. simplicity
parameters actions omitted, modifications necessary: first time
U applied key loose, first time applied key loose.
automaton N (not shown) navigates pairs locations l1 l2 . Since automaton
plans cannot keep track state, N include one automaton state possible input
(l1 , l2 ) (alternatively define separate automaton destination l2 ). Note
automaton used represent solutions different instances set locations.

5. Relationship Compact Plan Representations
section compare contrast automaton plans compact plan representations
literature: macro plans, HTNs (Erol et al., 1996), well C RARs C SARs (Backstrom &
Jonsson, 2012). Informally, C RARs C SARs theoretical concepts describing compact
representation plan admits efficient random access (C RAR) sequential access (C SAR)
operators plan. compare plan representations use following subsumption
relation (Backstrom et al., 2012b):
267

fiB ACKSTR OM , J ONSSON , & J ONSSON

p

p

ACR

C RAR

C SAR

p
6p

p

p
p

AUTRUE

AUTR

p

HTN

Figure 6: Summary subsumption results, dashed edges marking previously known results.
Definition 8. Let X two plan representations. least expressive X,
denote X p , polynomial-time function g TRIPS planning
instance p plan p, X representation , g() representation .
Figure 6 summarizes subsumption results different plan representations considered
paper. ACR AUTR refer macro plans automaton plans, respectively,
AUTRUE refer automaton plans uniform expansion. Previously known results shown
using dashed edges; remaining results proven section. use notation X p
indicate X p 6p X. figure see automaton plans strictly
expressive macro plans, strictly less expressive C SARs HTNs. case
C RARs, prove partial result: automaton plans uniform expansion
translated C RARs.
rest section, first show automaton plans uniform expansion
efficiently transformed C RARs. prove plan verification automaton plans p2 complete. use latter result prove separation macro plans automaton plans,
automaton plans C SARs/HTNs. proving X p holds two representations X , assume size X representation polynomial ||p||,
description size planning instance. Trivially, automaton plans uniform expansion
also automaton plans, unknown whether general automaton plans efficiently transformed uniform expansion.
5.1 Automaton Plans C RARs
section show automaton plans uniform expansion efficiently translated
C RARs, i.e. compact plan representations admit efficient random access. first define C RARs
describe algorithm transforming automaton plan uniform expansion
corresponding C RAR.
Definition 9. Let p polynomial function. Given TRIPS planning instance p = hP, A, , I, Gi
associated plan , p-C RAR representation |||| p(||p||) outputs
k-th element time space p(||p||) 1 k ||.
Note p-C RAR, polynomial function p independent planning
instance p, else always find constant individual instance p size
representation p bounded constant.
268

fiAUTOMATON P LANS

1
2
3
4
5
6
7
8

function Find(k, u[x])
u[x] return u[x]
else
hu1 [x1 ], . . . , un [xn ]i Apply(u[x])
0, j 1
+ (uj ) < k
+ (uj ), j j + 1
return Find(k s, uj [xj ])
Figure 7: Algorithm using automaton plan C RAR.

Theorem 1. AUTRUE p C RAR.
Proof. prove theorem show TRIPS planning instance p automaton
plan = h, A, Au, r, xi uniform expansion representing plan p, efficiently
construct corresponding p-C RAR p(||p||) = (Ar + (log + log Ar)|Au|) Ar |Au|.
Since uniform expansion exist numbers , Au, |Exp(M [x])| =
x ar(M ) . numbers computed bottom follows. Traverse
automata expansion graph GAu reverse topological order. Au, pick input
string x ar(M ) random compute Apply(M [x]) = hu1 [y1 ], . . . , un [yn ]i. number
given = u1 + . . . + un where, 1 k n, uk = 1 uk uk already
computed uk Au since definition, uk comes topological ordering.
Lemma 3, total complexity computing Apply(M [x]) Au
Ar2 |Au|. Due Lemma 6, (S Ar)1+h(M ) (S Ar)|Au| Au. Since
(S Ar)|Au| = 2(log S+log Ar)|Au| , need (log + log Ar)|Au| bits represent ,
computing requires (log +log Ar)S Ar|Au| operations. Repeating computation
Au gives us complexity bound (Ar + (log + log Ar)|Au|) Ar |Au|.
prove recursive algorithm Find Figure 7 following properties, induction
number recursive calls:
1. [x] Au Exp(M [x]) = ha1 [x1 ], . . . , [xn ]i, Find(k, [x]) returns
operator ak [xk ] 1 k n,
2. a[x] , Find(k, a[x]) returns a[x].
Basis: Find(k, u[x]) call recursively, u[x] must operator. definition, Exp(u[x]) = u[x] since u[x] .
Induction step: Suppose claim holds Find makes recursive calls
0. Assume Find(k, u[x]) makes m+1 recursive calls. Let hu1 [x1 ], . . . , un [xn ]i = Apply(u[x])
and, 1 k n, (uk ) = 1 uk [xk ] (uk ) = uk uk [xk ] Au . Lines 57
computes j either
1. j = 1, = 0 k (u1 )
2. j > 1, = (u1 ) + . . . + (uj1 ) < k (u1 ) + . . . + (uj ).
definition, Exp(u[x]) = Exp(u1 [x1 ]); . . . ; Exp(un [xn ]), implying operator k Exp(u[x])
operator k Exp(uj [xj ]). follows induction hypothesis recursive call
Find(k s, uj [xj ]) returns operator.
269

fiB ACKSTR OM , J ONSSON , & J ONSSON

prove complexity Find, note Find calls recursively
Au since GAu acyclic. Moreover, complexity computing Apply(M [x]) bounded
Ar2 , loop lines 67 runs n Ar times, time performing
(log + log Ar)|Au| operations update value s. thus showed
automaton plan together procedure Find values , Au, constitute pC RAR p(||p||) = (Ar + (log + log Ar)|Au|) Ar |Au|.
5.2 Verification Automaton Plans
section show problem plan verification automaton plans p2 -complete.
first prove membership reducing plan verification automaton plans plan verification
C RARs, known p2 -complete (Backstrom et al., 2012b). prove hardness
reducing -SAT, also p2 -complete, plan verification automaton plans uniform
expansion. complexity result plan verification later used separate automaton plans
macro plans, C SARs, HTNs, obtain similar separation result automaton
plans C RARs since complexity plan verification same.
prove membership first define alternative expansion function Exp pads original plan dummy operators expansion automaton length
accepting input. Intuitively, even though original automaton plan need uniform expansion, alternative expansion function Exp emulates automaton plan does. Note
sufficient prove transform automaton plan p-C RAR, since operators
different indices plans represented two expansion functions Exp Exp .
Let p planning instance, let = h, A, Au, r, xi automaton plan representing
solution p. automaton Au, let IM = (S Ar)1+h(M ) upper bound
|Exp(M [x])| Lemma 6. Let = h, parameter-free dummy operator empty preand postcondition, add . Define k , k > 0, sequence containing k copies .
define alternative expansion function Exp (A Au )+ follows:
1. Exp (a[x]) = ha[x]i a[x] ,
2. Exp (M [x]) = Exp (Apply(M [x])); L [x] Au , length L L =
IM |Exp (Apply(M [x]))|,
3. Exp (hu1 [y1 ], . . . , un [yn ]i) = Exp (u1 [y1 ]); . . . ; Exp (un [yn ]).
difference respect original expansion function Exp alternative expansion function Exp appends sequence L dummy operators result Exp (Apply(M [x])),
causing Exp (M [x]) length exactly IM .
following lemma prove operator sequence output alternative expansion
function Exp equivalent operator sequence output original expansion function Exp.
Lemma 10. automaton call [x] Au , Exp (M [x]) AIM , applying Exp(M [x])
Exp (M [x]) state either possible results state.
Proof. prove lemma induction |Au|. base case given |Au| = 1.
case, since GAu acyclic, Apply(M [x]) sequence operators ha1 [x1 ], . . . , [xn ]i A+
,
270

fiAUTOMATON P LANS

implying
Exp(M [x]) = ha1 [x1 ], . . . , [xn ]i,
Exp (M [x]) = ha1 [x1 ], . . . , [xn ]i; L ,
L = IM n. Thus Exp (M [x]) AIM , applying Exp (M [x]) state
effect applying Exp(M [x]) since dummy operator always applicable effect.
next prove inductive step |Au| > 1. case, Apply(M [x]) sequence operators
automaton calls hu1 [y1 ], . . . , un [yn ]i (A Au )+ , implying
Exp(M [x]) = Exp(u1 [x1 ]); ; Exp(un [xn ]),
Exp (M [x]) = Exp (u1 [x1 ]); ; Exp (un [xn ]); L ,
L contains enough copies make |Exp (M [x])| = IM . 1 k n, uk [xk ]
operator Exp (uk [xk ]) = Exp(uk [xk ]) = huk [xk ]i, clearly identical
effects. hand, uk [xk ] automaton call, since GAu acyclic uk [xk ]
(Au \ {M }) , implying Exp (uk [xk ]) effect Exp(uk [xk ]) hypothesis
induction since |Au \ {M }| < |Au|. Thus Exp (M [x]) AIM , applying Exp (M [x])
state effect applying Exp(M [x]) since dummy operator always applicable
effect.
ready prove membership p2 . Lemma 10, instead verifying
plan Exp(r[x]), verify plan Exp (r[x]) given alternative expansion function Exp .
Lemma 11. Plan verification AUTR p2 .
Proof. prove lemma reducing plan verification automaton plans plan verification
C RARs, known p2 -complete (Backstrom et al., 2012b). Consider automaton
plan = h, A, Au, r, xi associated TRIPS planning instance p. Instead constructing
p-C RAR operator sequence Exp(r[x]) represented , construct p-C RAR
operator sequence Exp (r[x]). Due Lemma 10, Exp(r[x]) plan p Exp (r[x])
plan p.
p-C RAR Exp (r[x]) constructed modifying algorithm Find Figure 7.
Instead using numbers associated automaton plan uniform expansion,
use upper bounds IM length operator sequence output automaton.
modification need make add condition j n loop,
loop terminates j = n + 1, return , since means Iu1 + + Iun < k Iu .
complexity resulting p-C RAR identical proof Theorem 1 since
numbers IM within bounds used proof, i.e. IM (S Ar)|Au| Au.
example, consider automaton plan = h{0, 1}, {a1 , a2 }, {M1 , M2 , M3 }, M1 , 100i
M1 , M2 , M3 defined Figure 2. Applying definitions obtain = 3, Ar = 4,
h(M1 ) = 2, h(M2 ) = 1, h(M3 ) = 0, yields
IM1 = (S Ar)1+h(M1 ) = 123 = 1728,
IM2 = (S Ar)1+h(M2 ) = 122 = 144,
IM3 = (S Ar)1+h(M3 ) = 121 = 12.
271

fiB ACKSTR OM , J ONSSON , & J ONSSON

Although IM1 = 1728 gross overestimate length operator sequence output
M1 , number bits needed represent IM1 polynomial ||||. Applying alternative
expansion function Exp yields Exp (M1 [100]) = ha1 [0]i; 143 ; ha2 [1]i; 11 ; ha1 [1]i; 1571 .
prove p2 -completeness, remains show plan verification automaton plans
p
2 -hard. proof following lemma quite lengthy, defer Appendix A.
Lemma 12. Plan verification AUTRUE p2 -hard.
main theorem section follows immediately Lemmas 11 12.
Theorem 2. Plan verification AUTRUE AUTR p2 -complete.
Proof. Since AUTRUE AUTR, Lemma 11 implies plan verification AUTRUE p2 ,
Lemma 12 implies plan verification AUTR p2 -hard. Thus plan verification
AUTRUE AUTR p2 -complete.
5.3 Automaton Plans Macro Plans
section show automaton plans strictly expressive macros. this,
first define macro plans show macro plan trivially converted equivalent
automaton plan uniform expansion. show automaton plans cannot
efficiently translated macro plans.
macro plan = hM, mr TRIPS instance p consists set macros root
macro mr M. macro consists sequence = hu1 , . . . , un where,
1 k n, uk either operator another macro M. expansion
sequence operators obtained recursively replacing macro hu1 , . . . , un
expansion. process well-defined long macro appears expansion. plan
represented given expansion root macro mr .
Lemma 13. ACR p AUTRUE.
Proof. prove lemma show exists polynomial p TRIPS
planning instance p macro plan representing solution p, exists automaton
plan uniform expansion |||| = O(p(||||)).
macro parameter-free sequence = hu1 , . . . , ul operators macros.
construct automaton plan replacing macro automaton Mm
ar(Mm ) = 0. automaton Mm two states s0 s, two edges: one s0
label /h(w1 , 1 ), . . . , (wl , l )i, one label /. 1 j l,
uj operator, wj associated action index string j ar(uj ) contains
arguments uj sequence m, explicitly stated since parameter-free.
uj macro, wj = Muj j = since ar(Mm ) = ar(Muj ) = 0. root given
r = Mmr [], mr root macro .
show induction that, macro = hu1 , . . . , ul , Exp(Mm []) equals
expansion m. base case given |Au| = 1. sequence operators
A+
, Apply(Mm []) returns m, implying Exp(Mm []) = Apply(Mm []) = m. |Au| > 1,
Apply(Mm []) contains operators m, macro uj m, 1 j l,
replaced automaton call Muj []. hypothesis induction, Exp(Muj []) equals
expansion uj . Exp(Mm []) equals expansion since concatenations
272

fiAUTOMATON P LANS

identical sequences. easy see size automaton Mm polynomial m.
shown macro plan transformed equivalent automaton plan whose
size polynomial ||||, implying existence polynomial p |||| = O(p(||||)).
automaton plan trivially uniform expansion since automaton always called
empty input string.
next show automaton plans uniform expansion strictly expressive
macro plans.
Theorem 3. ACR p AUTRUE unless P = p2 .
Proof. Due Lemma 13 remains show AUTRUE 6p ACR, i.e. cannot efficiently
translate arbitrary automaton plans uniform expansion equivalent macro plans. Backstrom
et al. (2012b) showed plan verification macro plans P. Assume exists
polynomial-time algorithm translates automaton plan uniform expansion equivalent macro plan. could verify automaton plans uniform expansion polynomial
time, first applying given algorithm produce equivalent macro plan verifying
macro plan polynomial time. However, due Theorem 2, algorithm exist unless
P = p2 .
5.4 Automaton Plans C SARs
section show automaton plans strictly less expressive C SARs, defined
follows:
Definition 14. Let p polynomial function. Given TRIPS planning instance p = hP, A, , I, Gi
associated plan , p-C SAR representation |||| p(||p||) outputs
elements sequentially, time needed output element bounded p(||p||).
p-C RARs, polynomial function p p-C SAR independent planning
instance p. first show automaton plan transformed equivalent p-C SAR
polynomial time. show p-C SARs cannot efficiently translated
automaton plans.
Lemma 15. AUTR p C SAR.
Proof. prove lemma show TRIPS planning instance p automaton
plan representing solution p, efficiently construct corresponding p-C SAR
p(||p||) = Ar2 |Au|. claim algorithm Next Figure 8 always outputs next
operator polynomial time. algorithm maintains following global variables:
call stack = [M1 [x1 ], . . . , Mk [xk ]] M1 [x1 ] = r[x] root and,
1 < k, Mi [xi ] automaton call appears Apply(Mi1 [xi1 ]).
integer k representing current number elements S.
1 k, sequence stores result Apply(Mi [xi ]).
1 k, integer zi index .
273

fiB ACKSTR OM , J ONSSON , & J ONSSON

1 function Next()
2
zk = |k |
3
k = 1 return
4
else
5
pop Mk [xk ]
6
k k1
7
repeat
8
zk zk + 1
9
u[x] k [zk ]
10
u[x] return u[x]
11
else
12
push u[x] onto
13
k k+1
14
k Apply(u[x])
15
zk 0
Figure 8: Algorithm finding next operator automaton plan.
Prior first call Next, global variables initialized = [r[x]], k = 1, 1 =
Apply(r[x]), z1 = 0.
algorithm Next works follows. long elements Apply(Mk [xk ]),
automaton call Mk [xk ] popped stack k decremented. If, result, k = 1
Apply(M1 [x1 ]) contains elements, Next returns , correctly indicating plan
operators.
found automaton call Mk [xk ] stack Apply(Mk [xk ]) contains
elements, increment zk retrieve element u[x] index zk k . u[x] , u[x]
next operator plan therefore returned Next. Otherwise u[x] pushed onto
stack S, k incremented, k set Apply(u[x]), zk initialized 0, process repeated
new automaton call Mk [xk ] = u[x].
Since expansion graph GAu acyclic, number elements k stack bounded
|Au|. Thus complexity loop bounded |Au| since operations loop
constant complexity. Since Exp(M1 [x1 ]) = Exp(r[x]) A+
, repeat loop guaranteed find
k zk u[x] = k [zk ] operator, proving correctness algorithm.
operation repeat loop constant complexity Apply(u[x]); Lemma 3
know complexity bounded Ar2 , might repeat operation
|Au| times. space required store global variables bounded Ar |Au|.
shown global variables together algorithm Next constitute p-C SAR
p(||p||) = O(S Ar2 |Au|).
next show automaton plans strictly less expressive p-C SARs. Let P1
subclass TRIPS planning instances one operator applicable reachable
state. following lemma due Bylander (1994):
Lemma 16. Plan existence P1 PSPACE-hard.
274

fiAUTOMATON P LANS

Proof. Bylander presented polynomial-time reduction polynomial-space deterministic Turing machine (DTM) acceptance, PSPACE-complete problem, TRIPS plan existence. Given
DTM, planning instance p constructed Bylander belongs P1 solution
DTM accepts.
Theorem 4. AUTR p C SAR unless PSPACE = p3 .
Proof. Due Lemma 15 remains show C SAR 6p AUTR. first show plan verification C SARs PSPACE-hard. Given planning instance p P1 , let unique plan
obtained always selecting applicable operator, starting initial state. Without
loss generality, assume operators applicable goal state. Hence either solves
p, terminates dead-end state, enters cycle. trivial construct p-C SAR :
state, loop operators select one whose precondition satisfied. Critically,
construction independent . Due Lemma 16, PSPACE-hard determine whether
p solution, i.e. whether plan represented solves p.
hand, assume exists automaton plan |||| = O(p(||p||))
p
fixed polynomial p. solve plan existence p NP2 = p non3

deterministically guessing automaton plan verifying represents solution p.
implies PSPACE = p3 .
5.5 Automaton Plans HTNs
section show automaton plans strictly less expressive HTNs. begin
defining class HTNs compare automaton plans. show
efficiently transform automaton plans HTNs, way around.
like planning instances, HTN involves set fluents, set operators, initial
state. Unlike planning instances, however, aim reach goal state, aim
HTN produce sequence operators perform given set tasks. task
one associated methods specify decompose task subtasks,
either operators tasks. Planning proceeds recursively decomposing task using
associated method primitive operators remain. planning, HTN keep
track current state, operators methods applicable preconditions
satisfied current state.
general, solution HTN unique: may one applicable method
decomposing task, method may allow subtasks decomposition appear
different order. contrast, subsumption relation p defined compact representations
unique solutions. reason, consider restricted class HTNs methods
associated task mutually exclusive subtasks decomposition method
totally ordered. class HTNs indeed unique solutions, since task
decomposed one way. Since class HTNs strict subclass HTNs general,
results hold general HTNs remove requirement uniqueness solution.
definition HTNs largely based SHOP2 (Nau, Ilghami, Kuter, Murdock, Wu, &
Yaman, 2003), state-of-the-art algorithm solving HTNs. formally define HTN domain tuple H = hP, A, T, hP, Ai planning domain, set function symbols called tasks, set function symbols called methods. method
form ht, pre(), i, associated task, pre() precondition, =
275

fiB ACKSTR OM , J ONSSON , & J ONSSON

h(t1 , 1 ), . . . , (tk , k )i task list where, 1 k, ti action task
argument map ti . arity satisfies ar() ar(t), arguments
always copied onto . ar() > ar(t), arguments indices ar(t) + 1, . . . , ar()
free parameters take value. precondition pre() form
precondition action A, i.e. pre() = {(p1 , 1 , b1 ), . . . , (pl , l , bl )} where,
1 j l, pl P predicate, l argument map pl , bl Boolean. task
may multiple associated methods.
HTN instance tuple h = hP, A, T, , , I, Li hP, A, T, HTN domain,
set objects, P initial state, L task list. HTN instance implicitly defines set
grounded tasks set grounded methods , task list L T+ sequence
grounded tasks. precondition pre([xy]) grounded method [xy] , x
parameters copied assignment free parameters , derived pre()
way precondition pre(a[x]) operator a[x] derived pre(a).
Unlike TRIPS planning, aim HTN instance recursively expand grounded
task t[x] L applying associated grounded method [xy] primitive operators
remain. grounded method [xy] applicable precondition pre([xy]) satisfied,
applying [xy] replaces task t[x] sequence grounded operators tasks obtained
applying sequence argument maps [xy]. problem plan existence HTNs
determine expansion possible.
Lemma 17. AUTR p HTN.
proof Lemma 17 appears Appendix B. Intuitively, idea construct HTN
tasks associated states graphs automata, methods edges
graphs. HTN emulates execution model : grounded task corresponds
automaton , current state graph , input string x ar(M ) , index k
x. associated grounded methods indicate possible ways transition another
state. Given edge label /u, corresponding method applicable xk = ,
applying method recursively applies operators tasks sequence u, followed
task associated next state, incrementing k necessary.
Theorem 5. AUTR p HTN unless PSPACE = p3 .
Proof. Due Lemma 17 remains show HTN 6p AUTR. Erol et al. (1996) showed
problem plan existence propositional HTNs totally ordered task lists PSPACE-hard.
proof reduction propositional TRIPS planning, number applicable
methods task equals number applicable TRIPS operators original planning
instance, general larger one. However, due Lemma 16, instead
reduce class P1 , resulting HTNs one applicable method task.
exists polynomial-time algorithm translates HTNs equivalent automaton plans,
p
solve plan existence HTNs NP2 = p non-deterministically guessing automa3

ton plan verifying automaton plan solution. implies PSPACE = p3 .

reasoning used proof Theorem 5 also used show HTN 6p C RAR,
implying random access bounded away polynomial HTNs. However, unknown
whether C RARs efficiently translated HTNs.
276

fiAUTOMATON P LANS

1
2
3
4
5
6

function Lowest(hP, A, , I, Gi)
sI
G 6
{oi : oi applicable s}
mini oi
(s \ pre(om ) ) pre(om )+
Figure 9: Algorithm always selects applicable operator lowest index.

One important difference automaton plans HTNs latter keeps track
state. conjecture state-based compact representations hard verify
general. Consider algorithm Figure 9 always selects applicable operator
lowest index. algorithm compact representation well-defined operator sequence. Plan
verification compact representation PSPACE-hard due Lemma 16, since planning
instances P1 , algorithm always choose applicable operator. Arguably,
algorithm simplest state-based compact representation one think of, plan verification
still harder automaton plans.

6. Related Work
three main sources inspiration automaton plans macro planning, finite-state automata,
string compression. Below, briefly discuss three topics connections
automaton plans.
6.1 Macro Planning
connection macros automaton plans clear point: basic mechanism automaton plans recursively defining plans direct generalization macros.
context automated planning, macros first proposed Fikes et al. (1972) tool plan
execution analysis. idea immediately become widespread even though
used planners, mainly viewed interesting idea obvious applications.
Despite this, advantageous ways exploiting macros identified by, instance, Minton
Korf: Minton (1985) proposed storing useful macros adding set operators order
speed search Korf (1987) showed search space macros exponentially
smaller search space original planning operators.
last decade, popularity macros increased significantly. is, example, witnessed fact several planners exploit macros participated International Planning Competition. ARVIN (Coles & Smith, 2007) generates macros online
escape search plateaus, offline reduced version planning instance. ACRO -FF
(Botea, Enzenberger, Muller, & Schaeffer, 2005) extracts macros domain description
well solutions previous instances solved. W IZARD (Newton, Levine, Fox, & Long, 2007) uses
genetic algorithm generate macros. Researchers also studied macros influence
computational complexity solving different classes planning instances. Gimenez Jonsson
(2008) showed plan generation provably polynomial class 3S planning instances
solution expressed using macros. Jonsson (2009) presented similar algorithm
277

fiB ACKSTR OM , J ONSSON , & J ONSSON

optimally solving subclass planning instances tree-reducible causal graphs. cases,
planning instances respective class exponentially long optimal solutions, making
impossible generate solution polynomial time without use macros.
6.2 Finite State Automata
started working new plan representations, soon become evident automata
convenient way organizing computations needed inside compactly represented plan.
thought particularly original since automata automaton-like representations quite
common planning. order avoid confusion, want emphasize automaton hierarchies equivalent concept hierarchical automata. term hierarchical automata
used literature somewhat loose collective term large number different approaches
automaton-based hierarchical modelling systems; notable examples found control
theory (Zhong & Wonham, 1990) model checking (Alur & Yannakakis, 1998).
many examples solutions planning problems represented automata
examples are, unlike automaton plans, typically context non-deterministic planning.
Cimatti, Roveri, Traverso (1998) presented algorithm that, successful, returns strong
cyclic solutions non-deterministic planning instances. Winner Veloso (2003) used examples
learn generalized plans. Bonet, Palacios, Geffner (2010) used transformation classical
planning generate finite-state controllers non-deterministic planning. Finally, Hu De
Giacomo (2013) presented general algorithm synthesizing finite state controllers based
behavior specification domain.
Automata also used representing objects planning. Hickmott, Rintanen,
Thiebaux, White (2007) LaValle (2006) used automata represent entire planning instance. contrast, Toropila Bartak (2010) used automata represent domains individual
variables instance. Baier McIlraith (2006) showed convert LTL representation
temporally extended goals, i.e. conditions must hold intermediate states plan,
non-deterministic finite automaton.
6.3 String Compression
ideas behind automaton plans macro plans closely related string compression.
algorithms string compression variants pioneering work Lempel Ziv (1976).
Normally, compressed representation string straight line program (SLP)
context free grammar generate one single string. One might say precisely
hierarchical macro plan is. Although widely used, also attempts
use automaton representations strings order achieve compact representations (cf., see
Zipstein, 1992). One might say approaches generalize SLPs way similar
way automaton plans generalize macro plans. important note string compression
algorithms per se limited interest comes representing plans. basic reason
complete plan first needs generated compressed compression algorithm,
excludes utilization compact plans planning process. instance, previously
mentioned polynomial-time macro planning algorithms (Gimenez & Jonsson, 2008; Jonsson, 2009)
cannot replaced (with preserved computational properties) planner combined string
compression algorithm since planner may need produce exponentially long plan.
278

fiAUTOMATON P LANS

String compression usually makes assumptions content string represent. makes methods general although often optimal particular application.
examples, though, specialised representations. instance, Subramanian
Shankar (2005) present method compressing XML documents using automata
based XML syntax. automaton plans, like macro plans, make particular
assumptions either sequence (or string) represented. However, evident
examples primarily intend automata plan representation
functional correspondence plan structure.

7. Discussion
introduced novel concept automaton plans, i.e. plans represented hierarchies
finite state automata. Automaton plans extend macro plans allowing parameterization
branching, used represent solutions variety planning instances. showed
automaton plans strictly expressive macro plans, strictly less expressive
HTNs C SARs, i.e. compact representations allowing polynomial-time sequential access.
precise relationship automaton plans C RARs still open question,
presented partial result: automaton plans uniform expansion transformed
C RARs.
definition automaton plans restricted TRIPS planning,
possible extension would consider general planning formalisms. describe
extension would affect complexity results Section 5. transformations
valid string, hence independent planning formalism. particular, macro
plans always translated automaton plans uniform expansion, latter always
transformed C RARs, automaton plans always transformed C SARs. However,
aware HTN formalism extends action definition allow complex
actions. Hence transformation automaton plans HTNs involve actions TRIPSstyle preconditions effects. separation results TRIPS also carry general
planning formalisms since cannot exist polynomial function translating instances.
Although mainly studied computational properties automaton plans context
compact plan representation, believe automaton plans may find uses. Probably
interesting question practical perspective construct automaton plans.
definite answer question, least two ideas believe worth
exploring. One possible way generate automaton plans first construct HTN given
domain, use HTN solve instances domain. Instead flattening solution
typically done, idea would keep hierarchical structure transform
automaton plan. matter HTN representation consider long verify
whether solution instance valid; solution verified transformation
automaton plan might become tractable, least practical cases. approach would likely
require sophisticated techniques generating HTNs currently available, unless
HTN already provided.
Another interesting extension automaton plans allow recursive automata include
calls themselves. Consider automaton Mn Section 4 moving n discs Towers
Hanoi. introduced symbols j1 , . . . , jN representing number discs, could define
single recursive automaton includes fourth parameter jn representing number discs
279

fiB ACKSTR OM , J ONSSON , & J ONSSON











G



















Figure 10: example contingent planning instance.
moved. recursive calls input jn would include symbol jn1 , effectively
decrementing number discs. recursive mechanism work would need exist
base case recursive calls made (in case Towers Hanoi, base
case typically consists moving 0 1 discs). remark computational properties
automaton plans paper would longer apply since expansion graph would longer
acyclic.
Finally, modified automaton plans could used represent contingent plans compactly. Solutions contingent planning instances typically form directed graphs
node belief state, i.e. subset states. Edges correspond actions applicable
belief state, well observations current state. outcome observation single bit indicating whether current value given fluent true false. Since outcome
observation uncertain, observation splits belief state one belief state containing states
fluent true, one containing states fluent false. solution
represents policy, indicating action applied belief state.
Figure 10 shows example contingent planning instance. location described
(x, y)-coordinate. location arrow indicating way go, flag
indicating whether reached target destination G. Two fluents sufficient represent
direction arrow pointing:
00:
01:
10: left
11: right
four actions U, D, L, R moving up, down, left, right, respectively. (x, y)coordinate observable, initial state unknown. state observe
whether reached destination direction arrow pointing.
Figure 11 shows automaton represents solution example contingent planning
instance. set symbols = {0, 1}, i.e. symbols outcomes observations
used branch edges automaton. state make three consecutive observations:
whether reached goal (0 1), direction arrow pointing (two
bits). reach goal move state simply consume remaining observations
(if any). not, use direction arrow decide action apply next.
automaton solution independent size contingent planning instance long
arrows point right direction. contrast, number belief states doubly exponential
|P |
number fluents, i.e. 22 . add locations example contingent planning problem,
number belief states increases exponentially, size solution form
280

fiAUTOMATON P LANS

0/
1/D

1/
0/

0/L
1/R

0/U
1/

/

Figure 11: automaton representing contingent plan.
directed graph. Although example automaton plan hierarchical, difficult
imagine navigation subtask larger problem, case could call automaton
automata.
Although semantics contingent planning different classical planning,
automata Figure 11 used construct perfectly valid automaton plan. However,
definition imposes two restrictions contingent plans. First, since automata automaton plans
fixed arity, represent contingent plans constant number observations.
Second, entire sequence observations passed input automaton beforehand.
may therefore make sense consider relaxation definition: allowing input passed
automaton online, thus allowing arity automaton vary.

Acknowledgments
authors would like thank anonymous reviewers helpful comments suggestions.

Appendix A. Proof Lemma 12.
section prove Lemma 12, states plan verification automaton plans
uniform expansion p2 -hard. prove lemma reduction -SAT plan verification
automaton plans uniform expansion. proof proceeds three steps. first show
construct TRIPS planning instance pF given -SAT formula F . prove
exists operator sequence F pF unique solution F F
satisfiable. Finally, construct automaton plan F uniform expansion represents
sequence F , i.e. F represents valid plan pF F satisfiable.
Construction 18. Let F = x1 xm y1 yn -SAT formula 3SAT
formula, let LF = {1 , . . . , 2(m+n) } set literals, 2i1 = xi 2i = xi
1 2(m+j)1 = yj 2(m+j) = yj 1 j n. Also define
total order < LF < j < j. formula = (c1 ch )
conjunction 3-literal clauses ck = 1k 2k 3k 1k , 2k , 3k LF . Assume without loss
generality 1k 2k 3k .
Given formula F , construct TRIPS planning instance pF = hPF , AF , F , , GF
PF = {f x, f y, f s, sat, x1 , . . . , xm , y1 , . . . , yn , v0 , . . . , vh }, F = {0, 1}, = , GF =
281

fiB ACKSTR OM , J ONSSON , & J ONSSON

{f x, sat, x1 , . . . , xm }, AF contains following operators, described form
pre(a) post(a):
os :
olk1 :
olk2 :
olk3 :
onk :
ot :
:
oyj :
od :
oxi :

{f x, f y, v0 } {v0 , f s}
{vk1 , vk , 1k } {vk }
{vk1 , vk , 1k , 2k } {vk }
{vk1 , vk , 1k , 2k , 3k } {vk }
{vk1 , vk , 1k , 2k , 3k } {vk , f s}
{vh , f s} {f y, v0 , . . . , vh , sat}
{vh , f s} {f y, v0 , . . . , vh }
{f y, yj , yj+1 , . . . , yn } {f y, yj , yj+1 , . . . , yn }
{f y, y1 , . . . , yn } {f x, f y, y1 , . . . , yn }
{f x, sat, xi , xi+1 , . . . , xm } {f x, sat, xi , xi+1 , . . . , xm }

explain intuition behind planning instance pF . First note predicates actions
parameter-free, set fluents equals set predicates set operators equals
set actions. function map thus necessary describe pre- postcondition
operator. indices used operators ranges 1 m, 1 j n, 1 k h.
plan pF takes form three nested loops. outer loop uses operators type
oxi iterate assignments x1 , . . . , xm , universal variables formula F .
middle loop uses operators type oyj iterate assignments y1 , . . . , yn , existential
variables F . inner loop uses operators type olk1 , olk2 , olk3 , onk iterate clauses
3SAT formula , time verifying whether satisfied given current assignment
x1 , . . . , xm , y1 , . . . , yn . remaining fluents following functions:
f x: control applicability operators oxi used iterate assignments x1 , . . . , xm .
f y: control applicability operators oyj used iterate assignments y1 , . . . , yn .
v0 : control applicability operators olk1 , olk2 , olk3 , onk used iterate clauses.
f s: remember whether satisfied current assignment x1 , . . . , xm , y1 , . . . , yn .
sat: remember whether y1 , . . . , yn satisfied current assignment x1 , . . . , xm .
inner loop, first apply operator os add fluent v0 . clause ck ,
apply one operators olk1 , olk2 , olk3 , onk add vk . Finally, apply one
ot delete v0 , . . . , vh . process, fluent f added os deleted onk
applied clause ck .
Operators ot also add f y, causing operator type oyj become applicable. f
true, operator ot also adds sat. Applying operator type oyj effect moving next
assignment y1 , . . . , yn . y1 , . . . , yn true, operator od applicable instead, adding
fluent f x resetting y1 , . . . , yn false. f x true, apply operator type oxi
move next assignment x1 , . . . , xm . operators require sat precondition.
x1 , . . . , xm true, goal state GF ensures iterate one last time middle
loop make f x sat true.
282

fiAUTOMATON P LANS

Lemma 19. -SAT formula F satisfiable planning instance pF
unique solution F form
F

= E0 , ox, E1 , ox, . . . , ox, E2m 1 ,
n 1

Ei = Vi0 , oy, Vi1 , oy, . . . , oy, Vi2
Vij

, od,

= os, oz1 , oz2 , . . . , ozh , ow,

ox operator among ox1 , . . . , oxm , oy operator among oy1 , . . . , oyn ,
ozk , 1 k h, operator among olk1 , olk2 , olk3 , onk , ow operator among ot,
.
Proof. prove lemma showing following:
1. state reachable initial state , one operator applicable.
2. Repeatedly selecting applicable operator results sequence F given above.
3. sequence F applicable initial state goal state GF holds resulting state F satisfiable.
first show reachable state, exists 0 k h + 1 a) k = 0,
variables among v0 , . . . , vh false; b) 1 k h, v0 , . . . , vk1 true vk , . . . , vh
false; c) k = h + 1 variables among v0 , . . . , vh true. ignore
operators oyj , od, oxi since effect v0 , . . . , vh . initial state fluents
false statement holds k = 0. k = 0, applicable operator os sets v0
true, effectively incrementing current value k. 1 k h, possible operators
olk1 , olk2 , olk3 , onk . However, preconditions operators mutually exclusive
exactly one applicable. operators sets vk true, incrementing value
k. Finally, k = h + 1, possible operators ot . preconditions operators
also mutually exclusive, operators set v0 , . . . , vh false, effectively resetting value
k 0.
operators affecting fluents y1 , . . . , yn oy1 , . . . , oyn . requires f
precondition sets f false. operators setting f true ot ,
reset k 0. implies time want apply operator type oyj , fluents v0 , . . . , vh
need go complete cycle k = 0 k = h + 1 finish ot . cycle
corresponds exactly sequence Vij lemma.
next show y1 , . . . , yn act binary counter 0 2n 1, enumerating assignments existential variables formula F . Note preconditions oy1 , . . . , oyn
mutually exclusive, since oyn requires yn , oyn1 requires yn1 , yn , on. Specifically,
applicable operator oyj , 1 j n largest index yj false. Repeatedly
283

fiB ACKSTR OM , J ONSSON , & J ONSSON

applying available operator results following series values y1 , . . . , yn :
0 000
0 001
0 010
0 011
0 100
0 101
..
.
y1 , . . . , yn true, exists applicable operator type oyj , operator od applicable
instead.
operators affecting fluents x1 , . . . , xm ox1 , . . . , oxm . requires f x
precondition sets f x false. operator setting f x true od, also resets
y1 , . . . , yn false. implies time want apply operator type oxi , fluents
y1 , . . . , yn need go complete cycle 0 2n 1 finish od. cycle
corresponds exactly sequence Ei lemma.
Since operators ox1 , . . . , oxm form oy1 , . . . , oyn , one
applicable, repeatedly applying available operator among ox1 , . . . , oxm causes
fluents x1 , . . . , xm act binary counter 0 2m 1, enumerating assignments
universal variables formula F . precondition {f x, f y} operator os ensures os
applicable whenever apply operator type oyj oxi . achieve goal
state GF = {f x, sat, x1 , . . . , xm }, fluents x1 , . . . , xm complete full cycle 0
2m 1, set f x true fluents y1 , . . . , yn complete one last cycle setting
x1 , . . . , xm true. corresponds exactly sequence F lemma.
Finally, need show sequence F applicable initial state
goal state GF holds resulting state formula F satisfiable. initial
state using operator type oxi iterate x1 , . . . , xm , fluent sat false.
time apply os, fluent f added. iterating clauses, f deleted
apply operator type onk , i.e. exists clause ck satisfied current
assignment x1 , . . . , xm , y1 , . . . , yn . f true end loop, operator ot adds sat.
formula F satisfied, exists assignment x1 , . . . , xm unsatisfied assignment y1 , . . . , yn . Consequently, sat false applying operator od
making f x true assignment x1 , . . . , xm . either operator type oxi applicable
(if least one fluent among x1 , . . . , xm false) goal state GF hold resulting
state (if x1 , . . . , xm true). Conversely, F satisfied, sat always true applying od,
causing F applicable GF hold resulting state.
proceed construct automaton plan represents operator sequence F described Lemma 19.
Construction 20. Let pF = hPF , AF , F , , GF planning instance defined Construction 18. Construct automaton plan F = hF , AF , AuF , r, xi following properties:
AuF = {X1 , . . . , Xm , Y1 , . . . , Yn , S1 , . . . , Sh+1 , U1 , . . . , Uh+1 },
284

fiAUTOMATON P LANS

/
Xi [x]

/hXi+1 [x0], oxi , Xi+1 [x1]i

Xm [x]

/hY1 [x0], od, oxm , Y1 [x1], odi

Yj [x]

/hYj+1 [x0], oyj , Yj+1 [x1]i

Yn [x]

/hos, S1 [x0], oyn , os, S1 [x1]i

/

/

/

/Sk+1 [x]
1k /olk1
Sk [x]

/

2k /olk2
1k /

b /

3k /olk3

2k /

c /

/
3k /honk , Uk+1 [x]i
/Uk+1 [x]

1k /olk1
Uk [x]

/

2k /olk2
1k /

b /

3k /olk3

2k /

c /

/
3k /honk , Uk+1 [x]i
/

Sh+1 [x]

/hoti

Uh+1 [x]

/hof

/

Figure 12: Graphs automata defined Construction 20.

285

fiB ACKSTR OM , J ONSSON , & J ONSSON

1 m, ar(Xi ) = 1,
1 j n, ar(Yj ) = + j 1,
1 k h + 1, ar(Sk ) = ar(Uk ) = + n,
r = X1 x = .
graphs associated automata Aun shown Figure 12. indices used
describe automata ranges 1 < m, 1 j < n, 1 k h. automaton
[x], argument maps described u[x], u[x0], u[x1], indicating input string x
[x] copied onto u, possibly appending 0 1 end.
Intuitively, input string x automata Sk Uk represents assignment fluents
x1 , . . . , xm , y1 , . . . , yn . edge label consumes input symbols, number
variables precede variable corresponding literal 1k . Likewise, b number
variables 1k 2k , c number variables 2k 3k . assignment
1k determines whether output operator olk1 continue checking whether ck satisfied.
Note automata defined Construction 20 uniform expansion. 1 k h
x m+n , Apply(Sk [x]) contains exactly one operator among olk1 , olk2 , olk3 , onk , followed
either Sk+1 [x] Uk+1 [x]. true Uk . show automaton plan F defined
Construction 20 indeed represents operator sequence F Lemma 19.
Lemma 21. automaton plan F represents operator sequence F .
Proof. 1 m, input string x automaton Xi represents assignment
values fluents x1 , . . . , xi1 planning instance pF . 1 j n, input string
Yj represents assignment values x1 , . . . , xm , y1 , . . . , yj1 , 1 k h + 1,
input string Sk Uk represents complete assignment values x1 , . . . , xm , y1 , . . . , yn .
Consequently, symbol Xi appends x represents current value xi , symbol
Yj appends represents current value yj .
Since automaton Xi first sets xi 0 1, series assignments x1 , . . . , xm
becomes
0 000
0 001
0 010
0 011
0 100
0 101
..
.
identical binary counter x1 , . . . , xm induced plan F . Likewise,
assignment x1 , . . . , xm , assignments y1 , . . . , yn describe binary counter F .
changing value fluent xi 0 1, automaton Xi inserts operator oxi .
last assignment xi+1 , . . . , xm appending oxi 1, . . . , 1, first assignment
xi+1 , . . . , xm appending oxi 0, . . . , 0. Consequently, automata perfectly emulate
286

fiAUTOMATON P LANS

pre- postcondition oxi . true operator oyj inserted Yj . Automaton Xm
inserts operator od cycle assignments y1 , . . . , yn , automaton Yn inserts operator
os beginning iteration fluents v0 , . . . , vh .
1 k h, purpose automata Sk Uk decide operator append
among olk1 , olk2 , olk3 , onk . this, first access value variable associated
literal 1k . 1k satisfied, operator olk1 appended, else literal 2k checked, 3k necessary.
three literals unsatisfied current variable assignment operator onk appended.
difference automata Sk Uk Sh+1 appends operator ot, Uh+1
appends . automaton Sk indicates first k 1 clauses current 3SAT instance
satisfied current variable assignment. clause ck unsatisfied Sk call Uk+1 ,
case subsequent calls automata type U. words, another purpose
automata Sk Uk remember current value variable f s. way, correct
operator among ot appended end iteration fluents v0 , . . . , vh ,
concludes proof.
show plan verification p2 -hard automaton plans uniform expansion, given
-SAT formula F , construct (in polynomial time) planning instance pF Construction
18 automaton plan F Construction 20. Lemma 21 states F represents operator
sequence F defined Lemma 19. Due Lemma 19, F plan pF F
satisfiable. thus reduced -satisfiability (a p2 -complete problem) plan verification
automaton plans uniform expansion.

Appendix B. Proof Lemma 17
section prove Lemma 17, states automaton plan efficiently
transformed equivalent HTN instance h. Let p = hP, A, , I, Gi TRIPS instance let
= h, A, Au, ri automaton plan representing solution p. define HTN instance
h = hP , , T, , , , Li follows:
P = P {consec} {precedes} {isset-M }M Au ar(consec) = ar(precedes) =
ar(isset-M ) = 2 Au,
= {set-M, unset-M }M Au ar(set-M ) = ar(unset-M ) = 2 Au,
= J, J = {j0 , . . . , jK } set indices K = maxM Au ar(M ),
= {consec[ji1 ji ] : 1 K} {precedes[ji jk ] : 0 < k K}.
induced set fluents P , static fluent consec[jk] true j k consecutive indices J, precedes[jk] true j precedes k J. automaton
Au, symbol , index 1 ar(M ), fluent isset-M [ji ] indicates
whether i-th symbol input string x ar(M ) equals . induced set operators , operators set-M [j] unset-M [j] add delete fluent isset-M [j], respectively.
automaton Au, also add following tasks methods sets :
task setall-M arity ar(M ),
method dosetall-M arity ar(M ) associated task setall-M ,
287

fiB ACKSTR OM , J ONSSON , & J ONSSON

state , task visit-M -s arity ar(M ) + 1,
edge (s, t) label /u, method traverse-M -s-t arity ar(M ) + 1
associated task visit-M -s,
edge (s, t) label /u, , method consume-M -s-t arity ar(M ) + 2
associated task visit-M -s,
state || outgoing edges, method finish-M -s arity ar(M ) + 1
associated task visit-M -s.
Formally, precondition task list method contain pairs (u, )
action task u associated argument map u. However, simplify notation
instead describe grounded preconditions task lists grounded methods [xy]. induced
set grounded tasks , grounded task setall-M [x] sets current input string x.
lone associated grounded method dosetall-M [x] empty precondition following
task list:
= hunset-M [1 j1 ], . . . , unset-M [n j1 ], set-M [x1 j1 ],
..
.
unset-M [1 jar(M ) ], . . . , unset-M [n jar(M ) ], set-M [xar(M ) jar(M ) ]i,
1 , . . . , n symbols set . words, dosetall-M [x] first unsets symbols
index input string, sets symbol according x.
grounded task visit-M -s[xjk ] indicates currently state automaton ,
input string x current index x k. single outgoing edge
(s, t) label /(u, ), associated grounded method traverse-M -s-t[xjk ] empty
precondition. Let u[(x)] result applying argument map induced index
string input string x . u A, grounded task list traverse-M -s-t[xjk ]
equals = hu[(x)], visit-M -t[xjk ]i, effectively applying operator u[(x)]. hand,
u Au, task list = hsetall-u[(x)], visit-u-s0 [(x)j0 ], visit-M -t[xjk ]i, first setting
input string u (x) visiting initial state s0 u index j0 . either case,
grounded task visit-M -t[xjk ] end ensures next visit state without
incrementing k.
If, instead, multiple outgoing edges, outgoing edge (s, t) label /(u, )
, associated grounded method consume-M -s-t[xjk jk+1 ] precondition {consec[jk jk+1 ], precedes[jk jar(M ) ], isset-M [jk+1 ]}. index jk+1 free parameter
consume-M -s-t, precondition consec[jk jk+1 ] ensures jk jk+1 consecutive
indices J. precondition precedes[jk jar(M ) ] ensures k < ar(M ), i.e. input
symbols left x process. Note indices start j0 , symbol index jk+1
input string x set . task list identical traverse-M -s-t[xjk ], except
last task visit-M -t[xjk+1 ] associated next index jk+1 , indicating
consumed symbol input string.
|| outgoing edges, grounded method finish-M -s[xjk ] precondition
jk = jar(M ) , i.e. method applicable consumed symbols input string.
assume jk = jar(M ) checked without introducing additional predicate P .
288

fiAUTOMATON P LANS

task list empty, indicating finished traversing states . method
applicable states single outgoing edge since fire applicable -transitions
terminating.
task list HTN instance h given L = hsetall-r[x], visit-r-s0 [xj0 ]i r[x]
root automaton plan . way tasks methods defined, expanding
visit-r-s0 [xj0 ] corresponds exactly executing automaton r input string x starting
s0 . Thus expansion L corresponds exactly solution p represented
remove instances operators set-M unset-M . Since solution p, operator
sequence guaranteed applicable.
type task multiple associated methods visit-M -s. methods consumeM -s-t associated visit-M -s mutually exclusive since moment, isset-M [jk ] true
one symbol index 1 k ar(M ) input string x .
jk = jar(M ) , method finish-M -s applicable instead. task list method totally
ordered, implying instance h belongs restricted class HTNs mutually exclusive
methods totally ordered task lists.

References
Alur, R., & Yannakakis, M. (1998). Model Checking Hierarchical State Machines. Proceedings ACM SIGSOFT International Symposium Foundations Software Engineering,
pp. 175188.
Backstrom, C., Jonsson, A., & Jonsson, P. (2012a). Macro Plans Automata Plans.
Proceedings 20th European Conference Artificial Intelligence (ECAI), pp. 9196.
Backstrom, C., Jonsson, A., & Jonsson, P. (2012b). Macros, Reactive Plans Compact Representations. Proceedings 20th European Conference Artificial Intelligence (ECAI),
pp. 8590.
Backstrom, C., & Jonsson, P. (2012). Algorithms Limits Compact Plan Representation.
Journal Artificial Intelligence Research, 44, 141177.
Baier, J., & McIlraith, S. (2006). Planning Temporally Extended Goals Using Heuristic Search.
Proceedings 16th International Conference Automated Planning Scheduling
(ICAPS), pp. 342345.
Bonet, B., Palacios, H., & Geffner, H. (2010). Automatic Derivation Finite-State Machines
Behavior Control. Proceedings 24th National Conference Artificial Intelligence
(AAAI).
Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI Planning
Automatically Learned Macro-Operators. Journal Artificial Intelligence Research,
24, 581621.
Bylander, T. (1994). Computational Complexity Propositional STRIPS Planning. Artificial
Intelligence, 69, 165204.
Cimatti, A., Roveri, M., & Traverso, P. (1998). Automatic OBDD-based Generation Universal
Plans Non-Deterministic Domains. Proceedings 15th National Conference
Artificial Intelligence (AAAI), pp. 875881.
289

fiB ACKSTR OM , J ONSSON , & J ONSSON

Coles, A., & Smith, A. (2007). MARVIN: Heuristic Search Planner Online Macro-Action
Learning. Journal Artificial Intelligence Research, 28, 119156.
Erol, K., Hendler, J., & Nau, D. (1996). Complexity Results HTN Planning. Annals Mathematics Artificial Intelligence, 18, 6993.
Fikes, R., Hart, P., & Nilsson, N. (1972). Learning executing generalized robot plans. Artificial
Intelligence, 3(4), 251288.
Fikes, R., & Nilsson, N. (1971). STRIPS: New Approach Application Theorem Proving
Problem Solving. Artificial Intelligence, 2(3/4), 189208.
Gimenez, O., & Jonsson, A. (2008). Complexity Planning Problems Simple Causal
Graphs. Journal Artificial Intelligence Research, 31, 319351.
Hickmott, S., Rintanen, J., Thiebaux, S., & White, L. (2007). Planning via Petri Net Unfolding.
Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI), pp.
19041911.
Hu, Y., & De Giacomo, G. (2013). Generic Technique Synthesizing Bounded Finite-State
Controllers. Proceedings 23rd International Conference Automated Planning
Scheduling (ICAPS).
Jonsson, A. (2009). Role Macros Tractable Planning. Journal Artificial Intelligence
Research, 36, 471511.
Korf, R. (1987). Planning Search: Quantitative Approach. Artificial Intelligence, 33(1), 6588.
LaValle, S. (2006). Planning Algorithms. Cambridge Press.
Lempel, A., & Ziv, J. (1976). Complexity Finite Sequences. IEEE Transactions
Information Theory, 22(1), 7581.
McAllester, D., & Rosenblitt, D. (1991). Systematic Nonlinear Planning. Proceedings 9th
National Conference Artificial Intelligence (AAAI), pp. 634639.
Mealy, G. (1955). Method Synthesizing Sequential Circuits. Bell System Technical Journal,
34, 10451079.
Minton, S. (1985). Selectively Generalizing Plans Problem-Solving. Proceedings 9th
International Joint Conference Artificial Intelligence (IJCAI), pp. 596599.
Nau, D., Ilghami, O., Kuter, U., Murdock, J., Wu, D., & Yaman, F. (2003). SHOP2: HTN
Planning System. Journal Artificial Intelligence Research, 20, 379404.
Newton, M., Levine, J., Fox, M., & Long, D. (2007). Learning Macro-Actions Arbitrary Planners
Domains. Proceedings 17th International Conference Automated Planning
Scheduling (ICAPS), pp. 256263.
Subramanian, H., & Shankar, P. (2005). Compressing XML Documents Using Recursive Finite
State Automata. Proceedings 10th International Conference Implementation
Application Automata (CIAA), pp. 282293.
Toropila, D., & Bartak, R. (2010). Using Finite-State Automata Model Solve Planning
Problems. Proceedings 11th Italian AI Symposium Artificial Intelligence (AI*IA),
pp. 183189.
290

fiAUTOMATON P LANS

Winner, E., & Veloso, M. (2003). DISTILL: Towards Learning Domain-Specific Planners Example. Proceedings 20th International Conference Machine Learning (ICML),
pp. 800807.
Zhong, H., & Wonham, M. (1990). Consistency Hierarchical Supervision DiscreteEvent Systems. IEEE Transactions Automatic Control, 35(10), 11251134.
Zipstein, M. (1992). Data Compression Factor Automata. Theoretical Computer Science,
92(1), 213221.

291

fiJournal Artificial Intelligence Research 51 (2014) 493-532

Submitted 07/14; published 10 /14

Reasoning Topological Cardinal Direction Relations
2-Dimensional Spatial Objects
Anthony G. Cohn

A.G.C OHN @ LEEDS . AC . UK

School Computing, University Leeds, UK
Faculty Engineering Information Technology,
University Technology Sydney, Australia

Sanjiang Li

ANJIANG .L @ UTS . EDU . AU

AMSS-UTS Joint Research Lab,
Centre Quantum Computation & Intelligent Systems,
University Technology Sydney, Australia
College Computer Science, Shaanxi Normal University, China

Weiming Liu

L IU W EIMING @ BAIDU . COM

Baidu (China) Co., Ltd., Shanghai, China

Jochen Renz

J OCHEN .R ENZ @ ANU . EDU . AU

Research School Computer Science,
Australian National University, Australia

Abstract
Increasing expressiveness qualitative spatial calculi essential step towards meeting
requirements applications. achieved combining existing calculi way
express spatial information using relations multiple calculi. great challenge
develop reasoning algorithms correct complete reasoning combined
information. Previous work mainly studied cases interaction combined
calculi small, one two calculi simple. paper tackle
important combination topological directional information extended spatial objects.
combine best known calculi qualitative spatial reasoning, RCC8 algebra
representing topological information, Rectangle Algebra (RA) Cardinal Direction
Calculus (CDC) directional information. consider two different interpretations RCC8
algebra, one uses weak connectedness relation, uses strong connectedness relation.
interpretations, show reasoning topological directional information decidable remains NP. computational complexity results unveil significant differences
RA CDC, weak strong RCC8 models. Take combination
basic RCC8 basic CDC constraints example: show consistency problem
P use strong RCC8 algebra explicitly know corresponding basic RA
constraints.

1. Introduction
Qualitative Spatial Reasoning (QSR) multi-disciplinary research field aims establishing
expressive representation formalisms qualitative spatial knowledge providing effective reasoning mechanisms. Originating Allens work (1983) temporal interval relations, QSR
widely acknowledged AI approach spatial knowledge representation reasoning,
applications ranging natural language understanding (Davis, 2013), robot navigation (Shi,
Jian, & Krieg-Bruckner, 2010; Falomir, 2012), geographic information systems (GISs) (Egenhofer
c
2014
AI Access Foundation. rights reserved.

fiC OHN , L , L IU , & R ENZ

& Mark, 1995), sea navigation (Wolter et al., 2008), high level interpretation video data (Sridhar, Cohn, & Hogg, 2011; Cohn, Renz, & Sridhar, 2012). refer reader work Cohn
Renz (2008), Wolter Wallgrun (2012) information.
qualitative approach usually represents spatial information introducing relation model
domain spatial entities, could points, line segments, rectangles, arbitrary
regions. literature, relation model often called qualitative calculus (Ligozat &
Renz, 2004), contains finite set jointly exhaustive pairwise disjoint (JEPD) relations
defined domain. past three decades, dozens spatial relation models
proposed literature (Cohn & Renz, 2008; Chen, Cohn, Liu, Wang, Ouyang, & Yu, 2013).
Many qualitative calculi approximate spatial entities points. convenient
representing spatial direction, distance positions (providing extent objects
small compared distance apart), inappropriate far shapes and/or topology
spatial objects concerned. paper, represent spatial entities 2-dimensional
bounded regions real plane, may holes multiple connected components.
literature, spatial calculi focus one single aspect space, e.g. topology, direction, distance, position, shape. Topological relations relations invariant
homeomorphisms scale, rotation, translation. widely acknowledged topological relations crucial importance. One influential formalism topological relations
region connection calculus (RCC) (Randell, Cui, & Cohn, 1992). Based one primitive binary
connectedness relation, set eight JEPD topological relations defined RCC.
calculus known RCC8 algebra. According different interpretations connectedness,
calculus may different variants. paper, say two (closed) regions weakly connected share least common point, say strongly connected intersection
least one-dimensional. Accordingly, address two resulting RCC8 algebras weak
strong RCC8 algebras respectively. convenience, denote weak RCC8 algebra
RCC8, strong one RCC80 .
importance distinction strong weak RCC8 becomes clear analysing
different ways defining neighbourhood pixels commonly used Computer Vision. 4connectedness refers pixels horizontally vertically connected pixel,
8-connectedness includes diagonally neighbouring pixels well. distinction corresponds
nicely distinction strong weak RCC8 8-connectedness considers connections point, 4-connectedness considers connections along line (which onedimensional). Therefore, use strong weak RCC8 similar way use 4- 8connectedness, depending requirements application hand.
RCC8 algebra represents topological information spatial objects. many
practical applications, however, kinds relations often used together topological
relations. example, recommending restaurant dined common give
descriptions restaurant city centre, west central station, nearby
McDonalds.
Among aspects spatial information topology, directional relations
perhaps important. two well-known formalisms cope directional
relations extended spatial objects. One Rectangle Algebra (RA) (Balbiani, Condotta,
& Farinas del Cerro, 1999), Cardinal Direction Calculus (CDC) (Goyal & Egenhofer,
2001; Skiadopoulos & Koubarakis, 2005). representing direction primary object
reference object, RA approximates reference object primary object
494

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

minimum bounding rectangles (MBRs), relates two objects interval relations
projected intervals. hand, CDC approximates reference object MBR,
leaving primary object unchanged. CDC 511 basic relations, RA 169
basic relations. (487 511) basic CDC relations intersect one one basic
RA relation and, hence, contained unique basic RA relation. Therefore, CDC sense
expressive RA.
central reasoning problem QSR consistency problem. instance consistency
problem set constraints like (xy), x, spatial variables, qualitative
relation qualitative calculus. say consistent satisfiable exists instantiation spatial variables constraints satisfied. Without loss generality,
assume unique constraint two variables. Note x
related, add (x ? y) without changing consistency, ? universal relation
calculus. Unlike classical CSPs, domain spatial variable usually infinite, may
undecidable determine consistency binary CSPs infinite domains (Hirsch, 1999).
past three decades, QSR made significant progress solving consistency problems
variety qualitative calculi (Renz & Nebel, 1999; Renz, 1999; Balbiani et al., 1999; Zhang,
Liu, Li, & Ying, 2008; Skiadopoulos & Koubarakis, 2005; Liu, Zhang, Li, & Ying, 2010; Liu & Li,
2011).
order bring spatial reasoning theory closer practical applications, necessary
combine multiple aspects spatial information. growing number works devoted
combining topological RCC relations aspects spatial information, e.g. qualitative size
(Gerevini & Renz, 2002), cardinal directions (Sistla & Yu, 2000; Li, 2006a, 2007; Liu, Li, & Renz,
2009; Li & Cohn, 2012), connectivity (Kontchakov, Nenov, Pratt-Hartmann, & Zakharyaschev,
2011), convexity (Davis, Gotts, & Cohn, 1999; Schockaert & Li, 2012), betweenness (Schockaert
& Li, 2013), gravity (Ge & Renz, 2013). Recently, Wolfl Westphal (2009) also empirically
compared two approaches combination binary qualitative constraint calculi general.
also interesting works combining spatial temporal formalisms (Gerevini & Nebel,
2002; Gabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev, 2005). Moreover, subareas
formalisms constraint research, combination formalisms discussed long time
strong results, see e.g. work Bodirsky Kara (2010), Jonsson
Krokhin (2004).
current paper considers full combination RCC8 RCC80 two directional
relation models RA CDC. identify joint satisfaction problem (JSP) main reasoning
task. Given network topological (RCC8 RCC80 ) constraints network directional
(RA CDC) constraints , assuming involve set variables, JSP
decide joint network ] satisfiable. Note use ], instead , indicate
variables.
Since topological directional information independent, possible joint
network ] unsatisfiable satisfiable. Solving joint satisfaction
problem general harder solving independently. paper, interpret directional relations terms RA CDC, interpret topological relations terms weak
strong RCC8 algebras. basic constraints involved, show JSP
basic (weak strong) RCC8 basic RA networks solved polynomial time,
JSP basic (weak strong) RCC8 basic CDC networks NP-complete. Furthermore,
show that, three calculi (viz. RCC8, RA, CDC) combined together, JSP
495

fiC OHN , L , L IU , & R ENZ

basic RCC80 networks basic RA CDC networks tractable. Since non-basic constraints
always backtracked basic constraints, results show JSP (weak strong)
RCC8 RA CDC NP.
paper significant extension conference paper (Liu et al., 2009), combination basic weak RCC8 RA CDC constraints considered. paper also considers
combination RCC80 RA and/or CDC constraints. addition, extend tractable
results two maximal tractable subsets RCC8 one large tractable subset RA. paper
also closely related work Li (2007), Li Cohn (2012), combination
weak RCC8 algebra two subalgebras (viz. DIR9 DIR49) RA considered.
1.1 Application Scenario
example demonstrating usefulness results, use Angry Birds domain.
Similar representation reasoning tasks applied whenever use computer vision
detect objects image video. Angry Birds popular computer game gained increasing
attention within AI community, see e.g. work Zhang Renz (2014). Angry Birds
AI competition AI challenge problem, goal build intelligent agent
play Angry Birds better best human players (see http://aibirds.org).

Figure 1: screenshot Angry Birds game.
Angry Birds domain includes number building blocks different materials, sizes
shapes, even holes. building blocks form complicated spatial structures
protect pigs attacking birds (see Figure 1). AI agents able play game
like humans do, get visual information game form screenshots.
competition organisers provide basic computer vision software detects minimum
bounding boxes objects screenshot well object category. given
set rectangles form minimum bounding boxes actual objects (see Figure 1).
object solid physical object cannot overlap another object (only RCC8 relations DC
EC possible objects), bounding boxes related relation
Rectangle Algebra relation CDC. Instead considering spatial relations
single objects, also take account sets objects, example, set objects
directly indirectly supported particular object, set objects provide
cover particular pig, set wooden blocks.
496

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

Notations
, , , , ,
D, R, S,

w
x, y, z,vi , vj
, ,
a, b, c,
b8 , Q8 , C8
H
P, Q
H
Ix (a), Iy (a)
M(a)

= (mi )ni=1
(, )
x (, ), (, )
(, )
]
JSP(S, )
RA(T )
RCC8(D)
CCP(vi , vj )

Meanings
relations, usually basic relations (page 498)
relations, usually non-basic relations (page 498)
converse relation (page 498)
weak composition (page 498)
spatial variable interval variable (page 498)
network constraints (page 498)
bounded regions (page 500)
three maximal tractable subclasses RCC8 (page 500)
points (page 501)
unique maximal tractable subclass IA (page 502)
x- y- projective intervals region (page 503)
minimal bounding rectangle (MBR) region (page 503)
RA relation induced two IA relations , (page 503)
n-tuple regions mi form solution network (page 504)
consistent pair basic CDC relations (page 505)
x- y- projective interval relations (, ) (page 505)
RA relation x (, ) (, ) induced (, ) (page 506)
combination two networks set variables (page 507)
joint satisfaction problem subclasses (page 507)
RA relation induced RCC8 relation (page 509)
RCC8 relation induced RA relation (page 509)
two variables vi , vj common conflict point relation (page 510)
Table 1: Notations.

sets building blocks form spatial regions general sense used RCC8
BRCC8 (Wolter & Zakharyaschev, 2000), also include regions multiple disconnected
pieces regions holes. particular, means RCC8 relation possible two
sets objects, DC EC.
Given spatial configurations Angry Birds domain, use RCC8 relations well
RA CDC relations represent spatial information (sets of) objects minimum
bounding boxes extracted screenshots. results paper allow us accurately reason combined information represented using RCC8, RA, CDC. Important
reasoning tasks benefit results include, example, inferring configuration
changes hit bird inferring whether given representation consistent whether
stable gravity (Zhang & Renz, 2014). algorithm predicting configuration
blocks shot might work envisaging individual possible block positions might
mutually globally inconsistent. algorithm reasoning consistency
predictions therefore desirable.
remainder paper proceeds follows. Section 2 introduces basic notions, important
examples, essential results qualitative calculi. Section 3 describes joint satisfaction
problem considers simple example combination RA CDC constraints. Sections
4 5 consider computational complexity combination weak and, respectively, strong
497

fiC OHN , L , L IU , & R ENZ

RCC8 RA. Section 6 discusses computational complexity combination weak
strong RCC8 CDC. conclude paper Section 7 give proofs major computational complexity results appendices. convenience reader, Table 1 summarises
notations used paper.

2. Qualitative Calculi
establishment proper qualitative calculus key success qualitative approach temporal spatial reasoning. section introduces basic notions qualitative calculi recalls RCC8 algebra, Rectangle Algebra, Cardinal Direction Calculus.
addition, also summarise essential results used main part
paper.
2.1 Basic Notions
Let U domain temporal spatial entities, Rel(U) set binary relations U.
usual relational operations intersection, union, complement, Rel(U) Boolean
algebra. finite set B nonempty binary relations U jointly exhaustive pairwise disjoint
(JEPD short) two entities U related one one relation B. Write hBi
subalgebra Rel(U) generated B. Clearly, relations B atoms algebra hBi.
call hBi qualitative calculus U, call relations B basic relations calculus.
Notation. Note relation hBi union set basic relations. paper,
write R = {1 , 2 , ..., k } R union basic relations 1 , 2 , ..., k . convenience,
regard basic relation singleton {}.
two relations R, qualitative calculus = hBi, write R converse R,
defined
R = {(x, y) U U : (y, x) R},

(1)

write R w smallest relation contains R S, usual composition R
S, defined
R = {(x, y) U : (z U)(x, z) R (z, y) S}.
call R w weak composition R (Duntsch, Wang, & McCloskey, 2001).
constraint hBi form (xRy), R relation hBi. call (xRy) basic
constraint R basic relation B. important reasoning problem qualitative calculus
determine satisfiability consistency network = {vi Rij vj }ni,j=1 constraints hBi,
satisfiable (or consistent) instantiation (ai )ni=1 U (ai , aj ) Rij
holds 1 i, j n.
Given two constraint networks = {vi Rij vj }ni,j=1 = {vi Tij vj }ni,j=1 , say refines
Tij subset Rij 1 i, j n. consistent scenario consistent basic
network refines . clear consistent iff consistent scenario.
hand, given n-tuple entities (ai )ni=1 U, write ij basic relation fixed qualitative
calculus relates ai aj . = {vi ij vj }ni,j=1 consistent scenario call
scenario (or basic constraint network) induced (ai )ni=1 .
498

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

consistency constraint network partially determined path-consistency algorithms. say network = {vi Rij vj }ni,j=1 path-consistent

Rji = Rij
,

6= Rij Rik w Rkj

(2)

i, j k 6= i, j. case basic network, equivalent saying every
subnetwork involving three variables consistent.
Path-consistency enforced cubic time (Vilain & Kautz, 1986). is, apply
path-consistency algorithm constraint network , cubic time algorithm
terminate either get empty constraint (and hence know inconsistent) transform
equivalent path-consistent network. basic networks, easy see consistency
implies path-consistency, opposite proposition always hold.
following subsections recall qualitative topological directional calculi
discussed paper.
2.2 Region Connection Calculus RCC8
region connection calculus (RCC) (Randell et al., 1992) first-order theory based binary
connectedness relation. Standard RCC models arise topological spaces. paper,
concerned interpretations RCC real plane, provides arguably
important model RCC. Another reason lies directional calculi considered paper
also defined real plane. plane region (or region) nonempty regular closed subset
real plane. consider bounded regions, cardinal directions involve bounded
regions. regions could multi-pieces and/or holes.1
One standard interpretation RCC based Whiteheadean connectedness (Whitehead,
1929) plane regions, two regions connected common point. connectedness may considered weak many cases. example, worm cannot pass
interior one apple another, touch point, without becoming visible exterior worms point view might well say apples sufficiently
connected. (Borgo, Guarino, & Masolo, 1996, p. 223) paper, also consider stronger
connectedness, two regions regarded connected intersection least onedimensional (Li, Liu, & Wang, 2013). case rectangular grid spatial primitive entities,
already noted, strong weak connectedness correspond to, respectively, important notion
4- 8-neighbourhood pixels commonly used Computer Vision.
interpretations, relations Table 2 converses TPP NTPP form
JEPD set. Write Brcc8 Brcc80 two sets. call Boolean algebras generated
Brcc8 Brcc80 , respectively, weak strong RCC8 models, written RCC8 RCC80 .
Strong connectedness considered Borgo et al., (1996), Cohn Varzi (1999),
Li et al., (2013). easy see that, relations, strong connectedness contained weak
connectedness. Table 2 illustrates configuration (the 2nd left) instance EC
RCC8 instance DC RCC80 , configuration (the 2nd right)
instance TPP RCC8 instance NTPP RCC80 .
1. stress restriction RCC bounded plane regions affect complexity reasoning
RCC8, every consistent RCC8 network solution RCC model (Li, 2006b).

499

fiC OHN , L , L IU , & R ENZ

Relation
equals
disconnected
externally connected

Symb.
EQ
DC
EC

partially overlap

PO

tangential proper part
non-tangential proper part

TPP
NTPP

Definition (weak)
a=b
ab=
b 6= b =
b 6=
6 b 6 b
b 6 b
b

Definition (strong)
a=b
dim(a b) 0
dim(a b) = 1
b 6=
6 b 6 b
b dim(a b) = 1
b dim(a b) 0

Table 2: set basic RCC8 RCC80 relations, a, b two plane regions x , x, dim(x)
denote, respectively, interior, boundary, dimension x. Note notational convenience set dim() = 1.

Remark 1. far consistency realisations concerned, Li (2006b) shown
consistent RCC8 network solution RCC model. cubic realisation algorithm described used construct solution weak strong RCC8 models.
implies particular RCC8 network solution weak RCC8 model iff
solution strong RCC8 model. show paper, is, however, case
cardinal directions combined topological relations.
following, recall important properties three maximal tractable subclasses
b
H8 , C8 , Q8 RCC8 identified Renz (1999). complete list relations subclasses
found Appendix work Renz (2002).
Lemma 2. Suppose R non-basic RCC8 relation R {DC, EC, PO} = .
(1) R Q8 iff R either {TPP, NTPP} {TPP , NTPP }.
b8 iff R Q8 one following relations
(2) R H
{TPP, EQ}, {TPP, NTPP, EQ}, {TPP , EQ}, {TPP , NTPP , EQ}.
b8 , either {NTPP, EQ} {NTPP , EQ}.
(3) R C8 iff R H
note lemma define subclasses. particular, subclasses
include RCC8 relations R R {DC, EC, PO} =
6 .
Renz also shows consistent scenario constructed O(n2 ) time pathconsistent network one three maximal tractable subclasses.
500

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

Theorem 3 (Renz, 1999). consistent scenario path-consistent network constraints
b8 , C8 , Q8 computed O(n2 ) time, replacing every constraint (vi Rvj )
H
(vi base(R) vj ) , base(R) basic relation obtained follows:
(1) R B, base(R) = R;
(2) else {DC} R, base(R) = {DC};
b8 , base(R) = {EC};
(3) else {EC} R = Q8 = H
(4) else {PO} R, base(R) = {PO};
(5) else {NTPP} R = C8 , base(R) = {NTPP};
(6) else {TPP} R, base(R) = {TPP};
(7) else base(R) = base(R ).
follows, call canonical consistent scenario .
2.2.1 R EALISATION BASIC RCC8 N ETWORKS
known that, basic RCC8 networks, path-consistency implies consistency (Nebel, 1995).
next give short description cubic realisation algorithm proposed Li (2006b), need
devise similar algorithm later combination cases.
Given basic RCC8 network = {vi ij vj }ni,j=1 , suppose path-consistent. ntpp-chain
defined series variables vi1 , vi2 , , vik vis NTPPvis+1
= 1, , k 1. ntpp-level l(i) variable vi defined maximum length
ntpp-chains contained ends vi .
realisation constructed follows, variable may interpreted bounded
region multiple pieces. Without loss generality, assume (vi EQvj )
= j. first define variable vi finite set Xi control points follows. i,
introduce point Pi vi ; vi ECvj vi POvj , introduce point Pij vi ; vi TPPvj
vi NTPPvj , put Xi points Xj . expand point P Xi little obtain
square s(P ). squares pairwise disjoint. Then, taking union squares, obtain
instantiation bounded regions vi . works EC NTPP constraints.
modifications needed cope constraints (cf. Li, 2006b Appendix C
paper).
2.3 Interval Algebra Rectangle Algebra
subsection, recall Interval Algebra (IA) (Allen, 1983) Rectangle Algebra (RA) (Balbiani et al., 1999). IA qualitative calculus generated 13 basic relations closed
intervals real line shown Table 3. write
Bint = {b, m, o, s, d, f, eq, fi, di, si, oi, mi, bi}

(3)

set basic IA relations. Ligozat (1994) defines dimension2 basic interval relation
2 minus number equalities appearing definition relation (see Table 3). is,
2. stress notion dimension different topological dimension.

501

fiC OHN , L , L IU , & R ENZ

basic relations
dim(eq) = 0, dim(m) = dim(s) = dim(f) = 1, dim(b) = dim(o) = dim(d) = 2.

(4)

non-basic relation R define
dim(R) = max{dim() : basic relation R}.

Relation

meets
overlaps
starts

finishes
equals

Symb.
b




f
eq

Conv.
bi
mi
oi
si
di
fi
eq

Dim.
2
1
2
1
2
1
0

(5)

Definition
x+ <
x+ =
x < < x+ < +
x = < x+ < +
< x < x+ < +
< x < x+ = +
x = < x+ = +

(i)

(ii)

Table 3: IA basic relations (i) definitions (ii) conceptual neighbourhood graph, x =
[x , x+ ], = [y , + ] two intervals.
Nebel Burckert (1995) shown unique maximal tractable subclass IA
contains basic relations. subclass, written H, known ORD-Horn class.
Using conceptual neighbourhood graph (CNG) IA (Freksa, 1992), Ligozat (1994) gives
geometrical characterisation ORD-Horn relations. Consider CNG IA (shown Table 3
(ii)) partially ordered set (Bint , ) (by interpreting relation smaller right upper
neighbours). 1 , 2 Bint 1 2 , write [1 , 2 ] set basic interval relations
1 2 , call relation convex interval relation. IA relation R called
pre-convex obtained convex relation removing one basic relations
dimension lower R. example, [o, eq] = {o, s, fi, eq} convex relation {o, eq}
pre-convex relation. Ligozat shown ORD-Horn relations precisely pre-convex relations.
Nebel Burckert also show every path-consistent IA network H consistent. Furthermore, construct consistent scenario every path-consistent IA network H
quadratic time.
Suppose R IA relation let
Rcore = { Bint : dim() = dim(R), R}.

(6)

(6) clear Rcore = {eq} iff R = {eq}, Rcore = R {b, o, d, di, oi, bi} R
{b, o, d, di, oi, bi} nonempty, Rcore = R \ {eq} otherwise.
Lemma 4. (Renz, 1999) Suppose = {vi Rij vj }ni,j=1 path-consistent IA network H. Let
core
core = {vi Rij
vj }ni,j=1 .

core also path-consistent.
502

(7)

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

Table 3 (ii) easy see pre-convex relation R dimension 1 iff either R
1-dim basic relation R contained {s, eq, si} {fi, eq, f}. consequence, know
Corollary 5. Suppose = {vi Rij vj }ni,j=1 path-consistent network H.
v }n
consistent scenario = {vi Rij
j i,j=1 following property:
) = dim(R ),
dim(Rij
ij
= {eq} R = {eq}, R = {m} R = {m}, R = {mi} R = {mi}.
Rij
ij
ij
ij
ij
ij

result shows that, path-consistent network H, construct quadratic
time consistent scenario .

(i)

(ii)

Figure 2: (i) minimum bounding rectangle M(a) region a; (ii) RA relation b
o.
IA naturally extended regions plane. assume orthogonal basis Euclidean plane. bounded region a, minimum bounding rectangle (MBR), denoted M(a),
smallest rectangle contains whose sides parallel axes basis.
write Ix (a) Iy (a) as, respectively, x- y-projections M(a). basic rectangle
relation two bounded regions a, b iff (Ix (a), Ix (b)) (Iy (a), Iy (b)) ,
, two basic IA relations (see Figure 2 illustration). write Brec set
basic rectangle relations, i.e.,
Brec = { : , Bint }.

(8)

169 different basic rectangle relations Brec . Rectangle Algebra (RA) algebra
generated relations Brec (Balbiani et al., 1999).
following definitions used later.
Definition 6. Suppose = 1 2 basic RA relation. say 0-meet relation
1 , 2 {m, mi}, corner relation 1 , 2 {m, mi, s, si, f, fi, eq}. general, say
non-basic RA relation R = {1 , ..., k } (k 2) corner relation (1 k)
corner relation.
definition, 0-meet relation corner relation. Furthermore, easy see basic
RA relation 0-meet relation iff, every two rectangles r, r0 (r, r0 ) , r r0
singleton plane; corner relation iff every two rectangles r, r0 (r, r0 ) have,
least, corner point common.
following lemma straightforward.
503

fiC OHN , L , L IU , & R ENZ

Lemma 7. Let = {vi (Rij Sij )vj }ni,j=1 RA network, Rij Sij arbitrary IA
relations. satisfiable iff projections x = {xi Rij xj }ni,j=1 = {yi Sij yj }ni,j=1
satisfiable IA networks.
Corollary 5 lemma
Lemma 8. Suppose = {vi Rij vj } path-consistent RA network H H.
consistent scenario = {vi ij vj }
ij 0-meet relation iff Rij 0-meet basic relation,
ij corner relation iff Rij consists basic corner relations.
consequence, know H H tractable subclass RA. maximal tractable subclass
identified RA, larger tractable subclass RA identified (Balbiani et al.,
1999).
next show path-consistent basic IA RA network canonical solution
following sense.
+ n
Definition 9 (canonical tuple intervals (rectangles)). Suppose = ([m
, mi ])i=1 n-tuple
intervals. Let E(m) set values end points intervals m. say
canonical iff E(m) = {0, 1, , }. tuple rectangles (mi )ni=1 canonical iff x-
y-projections, (Ix (mi ))ni=1 (Iy (mi ))ni=1 , canonical tuples intervals. solution
IA (RA, respectively) network called canonical solution canonical tuple intervals
(rectangles, respectively).

basic satisfiable IA network, compute total order end points. Hence
obtain canonical solution (by assigning 0 first end point, 1 second, etc.).
gives us following proposition.
Proposition 10. Suppose satisfiable basic IA (RA) constraint network. unique
canonical solution.
2.4 Cardinal Direction Calculus
cardinal direction calculus (CDC) proposed Goyal Egenhofer (1997). Given
bounded region b real plane, extending four edges M(b), partition plane
nine tiles, denoted bij (1 i, j 3), see Figure 3 (i) illustration.
primary region reference region b, CDC relation b, denoted ab ,
encoded 3 3 Boolean matrix (dij )1i,j3 , dij = 1 iff bij 6= (where
interior a). example, basic CDC relations ab ba regions a, b
Figure 3(ii) represented following matrices.




0 0 0
0 0 1
= ab = 1 0 0 , = ba = 0 0 1 .
(9)
0 0 0
0 0 1
CDC relation zero Boolean matrix, 29 1 = 511 basic relations
CDC. denote set Bcdc . pair basic CDC relations (, ) called consistent pair
504

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

(i)

(ii)

(iii)

Figure 3: Illustrations (i) nine tiles reference region; (ii) (iii): two solutions
CDC basic constraint network {v1 v2 , v2 v1 }, defined Eq. (9).

constraint network {v1 v2 , v2 v1 } solution. also call weak converse
(, ) consistent pair. Figure 4 shows basic CDC relation may one weak
converse. Therefore, need relation b relation b give complete
description (in terms CDC calculus) directional information two regions a, b.

(i)

(ii)

Figure 4: Illustration two consistent CDC pairs (i) (ab , ba ) (ii) (a0 b0 , b0 ), ab =
a0 b0 ba 6= b0 a0 . Also note rectangle relation a, b
a0 , b0 o.
following show strong connection CDC RA relations.
Definition 11. (Zhang et al., 2008; Liu et al., 2010) pair basic CDC relations (, ),
define x-projective interval relation (, ), written x (, ), disjunction basic
IA relations instance x-projection solution {v1 v2 , v2 v1 },
i.e.
x (, ) = { Bint : (m1 , m2 )[(m1 , m2 ) (m2 , m1 ) (Ix (m1 ), Ix (m2 )) ]}.
similar definition applies y-direction.
Note (, ) consistent pair, x (, ) (, ) empty relation.
(, ) consistent pair, prove (Liu et al., 2010) x- (or y-) projective interval
505

fiC OHN , L , L IU , & R ENZ

relation IA relation R following property
R = {b, m} R = {bi, mi}, R basic IA relation {o, s, d, f, eq, oi, si, di, fi}.

(10)

two projective interval relations combined RA relation.
Definition 12. (Zhang et al., 2008; Liu et al., 2010) pair basic CDC relations (, ),
call (, ) = x (, ) (, ) RA relation induced (, ). general, basic CDC
constraint network = {vi ij vj }ni,j=1 , call () = {vi Rij vj }ni,j=1 RA constraint network
induced , Rij = (ij , ji ).
Note (, ) necessarily basic RA relation. (, ) consistent, know
RA relation (, ) form , , IA relations satisfy (10). Furthermore,
solution {v1 v2 , v2 v1 } always solution {v1 (, )v2 }. note solution
{v1 (, )v2 } necessarily solution {v1 v2 , v2 v1 }.
Take consistent pair ( , ) defined (9) example. Figure 3 (ii) (iii) show two
solutions (a, b) (a0 , b0 ) basic CDC constraint network {v1 v2 , v2 v1 }. implies
definition x ( , ) contains {b, m}. easy see definition x ( , )
contains basic IA relations x ( , ) = {b, m}. Similarly, show ( , ) =
{d}. shows consistent pair ( , ) corresponds basic RA relations, viz.
b d.
2.4.1 C ANONICAL OLUTIONS BASIC CDC N ETWORKS
like IA RA, consistent CDC networks also canonical solutions.
Definition 13 (regular solution, Zhang et al., 2008; Liu et al., 2010). Suppose = (mi )ni=1
solution basic CDC constraint network . say maximal m0i mi holds
solution (m0i )ni=1 M(mi ) = M(m0i ); say regular maximal
(M(mi ))ni=1 canonical tuple rectangles.
basic CDC network general many regular solutions, following result.
Proposition 14. Let basic CDC network. Suppose basic RA network refines
(), induced RA network . determine cubic time whether solution
also satisfies . Moreover, solution, unique regular solution also
satisfies . Furthermore, unique regular solution constructed cubic time.
Proof. proof similar Proposition 12 work Liu et al., (2010). sketch
given Appendix A.
proof result, see region mi regular solution (mi )ni=1
consists unit cells (i.e. rectangles form [i, i+1][j, j +1], i, j Z) canonical
solution , i.e. region mi cell c, either c mi c mi = .
basic CDC network , may exist exponentially many different basic RA networks
refine (). Hence, may exponentially many different regular solutions (see Figure 11(a) example network). However, verify solution, need
prove solution special basic RA network refines () (Liu et al.,
2010, Proposition 12).3 Therefore, consistency determined cubic time, and,
consistent, regular solution constructed cubic time (Liu et al., 2010).
3. special network called meet-free basic RA network work Liu et al., (2010).

506

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

3. Joint Satisfaction Problem
preparatory introduction basic notions essential results qualitative calculi,
ready describe joint satisfaction problem.
Let M1 M2 two qualitative calculi domain U. Suppose Si subclass
Mi (i = 1, 2). write JSP(S1 , S2 ) joint satisfaction problem (Gerevini & Renz, 2002; Li,
2007) S1 S2 .
Suppose = {vi Tij vj }ni,j=1 constraint network S1 , = {vi Dij vj }ni,j=1
constraint network S2 involving variables. say ] instance
JSP(S1 , S2 ). joint satisfaction problem first considered RCC8 qualitative size
calculus (identical Point Algebra Vilain & Kautz, 1986) Gerevini Renz (2002).
Moreover, shown consistency joint network approximated polynomial bipath-consistency algorithm. Li Cohn (2012) recently showed bipath-consistency
equivalently expressed below.
Definition 15. Let ] joint constraint network M1 M2 , = {vi Tij vj }ni,j=1
= {vi Dij vj }ni,j=1 . say ] bi-closed Dij Tij nonempty
basic relation Tij , basic relation Dij , 1 i, j n (here regard
relation subset U U). bi-closed joint network ] bipath-consistent
path-consistent.
Informally speaking, joint constraint network bi-closed basic relation given
relation one calculi consistent corresponding relation calculus.
simple example joint satisfaction problem, consider combination RA
CDC next subsection.
3.1 Combination RA CDC
Let R basic CDC relation. R , set-theoretic converse (or inverse) relation R
(cf. (1)), may representable relation algebra CDC (Cicerone & Di Felice, 2004; Liu
et al., 2010). is, R cannot represented union several basic CDC relations.
sense, say CDC closed converse. Recently, Schneider et al. (2012) proposed
variant CDC, called Object Interaction Model (OIM), closed converse.
two bounded regions a, b, OIM divides plane (l1 + 2) (l2 + 2) tiles
extending edges M(a) M(b), l1 + 1 l2 + 1 numbers horizontal
and, respectively, vertical lines. clear 1 l1 , l2 3 since edges M(a) M(b) may
coincide. OIM relation ab represented l1 l2 matrix (also written ab ) considering
existence interior points and/or b corresponding bounded tiles. Let bounded
tile. set entry corresponding matrix ab 0 interior point
either b; set 1 (2, respectively) interior point (b, respectively)
interior point b (a, respectively); set 3 otherwise. converse relation
basic OIM relation also basic OIM relation. particular, basic OIM relation ba b
obtained swapping occurrences 1 2 ab .
example, OIM relations regions Figure 3 (ii) (iii) respectively




0 0 2
0 2
ab = 1 0 2 , a0 b0 = 1 2 ,
0 0 2
0 2
507

fiC OHN , L , L IU , & R ENZ

OIM relations regions Figure 4 respectively

ab



0 2 2
= 1 3 2 ,
1 1 0

a0 b0



0 2 2
= 1 1 2 .
1 1 0

note regions a, b, a0 , b0 figures ab = a0 b0 . suggests OIM
finer grained CDC sense splits one basic CDC relation several OIM relations.
Nevertheless, since CDC closed converse, need consider consistent pairs basic
CDC relations order evaluate expressivity. comparing expressivity two
calculi way, see (ab , ba ) 6= (a0 b0 , b0 a0 ) Figure 4, (ab , ba ) = (a0 b0 , b0 a0 )
Figure 3. shows OIM makes finer distinctions CDC describing scenarios
given Figure 3(ii) (iii): saying west b, CDC differentiate east
boundary meets precedes west boundary b. following result shows OIM
finer CDC describing cardinal relations, essence combination
CDC RA.
Theorem 16. (Li & Liu, 2014) two regions b, compute RA relation
b, CDC relation b, CDC relation b OIM relation b,
vice versa.
words, basic OIM relation , exist two basic CDC relations , 0
basic RA relation = 0 , i.e. two regions b, relation
OIM relation b iff , 0 are, respectively, CDC relation b, CDC relation
b a, RA relation b. basic CDC RA relations JEPD,
choices , 0 , unique. following, call CDC relation induced
call RA relation induced . Note case 0 (as relation b a) CDC
relation induced , OIM relation b a.
consequence, following result.

Proposition 17. Suppose = {vi ij vj }ni,j=1 basic OIM network ji = ij
n
n
i, j. Let = {vi ij vj }i,j=1 = {vi ij vj }i,j=1 , ij ij are, respectively, CDC
relation RA relation induced ij . consistent iff joint network ]
consistent.

Proof. Recall converse basic OIM relation also basic OIM relation.
, straightforward show = . Therefore, solutions
ji = ij
ij
ij
ij
ji
exactly solutions ] .
consequence Propositions 14 17
Corollary 18. Let , given Proposition 17. basic RA network
refines (), consistency determined cubic time. Moreover, consistent,
unique regular solution also solution .
far, described example JSP. next three sections,
consider main task paper: JSP topological directional constraints.
508

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

4. Combination Weak RCC8 RA Networks
section represent topological information weak RCC8 relations directional information RA relations. first consider interaction weak RCC8 RA relations,
consider JSP basic constraints, and, lastly, consider JSP general.
4.1 Interaction Weak RCC8 RA Relations
Relations different calculi may interact sense relation one calculus may intersect
several relations second calculus. recall related definitions preliminary
results obtained Li Cohn (2012).
Definition 19. Let RCC8 relation RA relation. RA relation induced
RCC8 relation induced defined
RA(T ) = { : basic RA relation 6= }

(11)

RCC8(D) = { : basic RCC8 relation 6= }.

(12)

Note joint network ] bi-closed ij RA(ij ) ij RCC8(ij ) i, j.

easy see (cf. Li Cohn, 2012) RA(T ) = {RA({}) : }
RA({DC}) RA({EC}) RA({PO}) RA({TPP}) RA({NTPP, EQ}),




RA({PO}) RA({TPP }) RA({NTPP , EQ}),

(13)
(14)

where, example, RA({EC}) RA({PO}) holds because, basic RA relation ,
RA({EC}) M(a) M(b) 6= (a, b) , RA({PO}) M(a) M(b)
non-degenerate rectangle (a, b) .
Lemma 20. Let RCC8 relation RA relation. RCC8(D) relation
b8 , Q8 , C8 ; RA(T ) relation H H relation H
b8 Q8 .
intersection H
Proof. follows definitions RCC8(D) RA(T ) simple table look-up
Appendix work Renz (2002).
second statement apply relations C8 . example, consider = {NTPP,
EQ}. relation C8 , RA(T ) = {d d, eq eq} outside H H.
4.2 Combination Basic Networks
consider combination RCC8 RA. First show bipath-consistency
sufficient consistency JSP(Brcc8 , Brec ) (Li & Cohn, 2012). Let = {vi ij vj }4i,j=1
basic RA network induced four rectangles mi (i = 1, 2, 3, 4) illustrated below.
Let = {vi ij vj }4i,j=1 basic RCC8 constraint network 12 = 34 = {EC}
others {DC}. Clearly, satisfiable. Although ] bipath-consistent,
satisfiable. because, otherwise, exists solution = (mi )4i=1 M(m1 )M(m2 ) =
M(m3 ) M(m4 ) = {P } singleton. 12 = 34 = {EC} know P mi (i = 1, 2, 3, 4).
contradicts 13 = {DC}.
call point P configuration conflict point. general, following
definition.
509

fiC OHN , L , L IU , & R ENZ

Definition 21 (conflict point). Let = {vi ij vj }ni,j=1 basic RCC8 network = {vi
ij vj }ni,j=1 basic RA network. Suppose canonical solution . point Q called
conflict point mi exists j mi mj = {Q} ij = {EC}. write Ci
set conflict points mi .
Clearly, conflict point mi also corner point mi . implies mi mj
may one common conflict point. Moreover, suppose = (mi )ni=1 solution
] M(mi ) = mi 1 n. conflict point mi contained
mi . means Ci mi . consequence,
Ci Cj 6= ij 6= {DC} (1 i, j n)

(15)

following theorem shows also sufficient.
Theorem 22. Let = {vi ij vj }ni,j=1 basic RCC8 network = {vi ij vj }ni,j=1 basic RA
network. Suppose ] bipath-consistent. ] satisfiable iff (15) holds.
Proof. necessity part clear. defer proof sufficiency part Appendix B.
corollary, JSP(Brcc8 , Brec ) P.
Corollary 23. basic RCC8 network basic RA network , consistency ]
decided cubic time.
Proof. Bipath-consistency ] checked cubic time. construct unique
canonical rectangle solution quadratic time. conflict point set Ci also computed
quadratic time. is, condition Theorem 22 checked cubic time.
4.3 Large Tractable Subsets
b8 , C8 , Q8 , IA one maximal
Recall RCC8 three maximal tractable subclasses H
tractable subclass H, containing basic relations. subsection, aim extend
b8 , C8 RCC8, large tractable subset H H
result maximal tractable subsets H
RA.
end, need extend notion conflict points basic networks arbitrary
networks. Recall 0-meet relations corner relations basic RA relations defined Definition 6.
Definition 24 (common conflict point). Let = {vi Tij vj }ni,j=1 RCC8 network =
{vi Dij vj }ni,j=1 RA network. say two variables vi , vj CCP (common conflict point)
relation, written CCP(vi , vj ), Dij 0-meet (basic) relation Tij = {EC},
Dij (possibly disjunctive) corner relation;
510

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

exist i0 , j 0 Dii0 Djj 0 0-meet (basic) relations, Dij 0 , Di0 j Di0 j 0
(possibly disjunctive) corner relations, Tii0 = Tjj 0 = {EC}.
CCP(vi , vj ) vi0 , vj0 variables satisfy conditions, also write CCP(i, j :
i0 , j 0 ) stress roles vi0 vj 0 .

(i)

(ii)

Figure 5: Two joint constraint networks JSP(RCC8, RA), (i) (ii)
Dij basic RA relation vi vj illustrated picture, T14 = T23 = {EC} unspecified RCC8 constraints
non-basic RCC8 relation {DC, EC, PO}.
(i) (ii)
CCP(1, 2), CCP(1, 3), CCP(1, 4), CCP(2, 3), CCP(2, 4), CCP(3, 4).

Examples shown Figure 5. Note basic networks, vi vj
CCP relation iff Ci Cj nonempty, i.e. vi vj common conflict point.
Definition 25. Let = {vi Tij vj }ni,j=1 RCC8 network = {vi Dij vj }ni,j=1 RA
network. say ] CCP-consistent
CCP(vi , vj ) DC 6 Tij

(16)

holds 6= j. say joint network ] BC-consistent bipath-consistent
CCP-consistent.
general, vi vj CCP relation, (in realisation) vi vj share least
one corner point (of MBRs) common. Therefore, weak RCC8 algebra, cannot
disconnected, neither contained another non-tangential proper part. Note
latter statement also follows bi-closedness ] .
Similar bipath-consistency algorithm (Gerevini & Renz, 2002), devise algorithm
(Algorithm 1) enforcing BC-consistency. following theorem shows algorithm
sound.
Theorem 26. Suppose ] joint network RCC8 RA constraints, = {vi Tij
vj }ni,j=1 = {vi Dij vj }ni,j=1 . O(n4 ) time, algorithm BC-C ONSISTENCY either
finds inconsistency transforms ] equivalent joint network 0 ] 0
BC-consistent.
511

fiC OHN , L , L IU , & R ENZ

Input: joint network ] , = {vi Tij vj }ni,j=1 = {vi Dij vj }ni,j=1 .
Output: false, empty constraint generated; BC-consistent joint network equivalent
] , otherwise.
Q {(i, k, j) | 6= j, k 6= i, k 6= j};
(i indicates i-th variable ] .
Analogously j k)
Q 6=
select delete path (i, k, j) Q;
BC-R EVISION(i, k, j)
Tij = Dij =
return false;
end
Q Q {(i, j, k), (k, i, j) | k 6= i, k 6= j};
end
end
Function: BC-R EVISION(i, k, j)
Input: three variables i, k j
Output: true, Tij Dij revised; false otherwise.
Side effects: Tij Dji revised using operations w .
Tij
(Tij RCC8(Dij )) (Tji RCC8(Dji )) (Tik RCC8(Dik )) w (Tkj RCC8(Dkj ));
Dij (Dij RA(Tij )) (Dji RA(Tji )) (Dik RA(Tik )) w (Dkj RA(Tkj ));
CCP(i, j : k)
Tij Tij \ {DC};
end
neither Tij Dij revised
return false;
end
;
Dji Dij
Tji Tij ;
return true.
Algorithm 1: BC-C ONSISTENCY, write CCP(i, j : k) represent situation
exists another variable vl vl vk together evidence CCP(i, j).

512

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

Proof. because, iteratively use following updating rules either empty constraint occurs network becomes stable.
Tij (Tij RCC8(Dij )) (Tji RCC8(Dji ))
(Tik RCC8(Dik )) w (Tkj RCC8(Dkj ))


(17)

Dij (Dij RA(Tij )) (Dji RA(Tji )) (Dik RA(Tik )) w (Dkj RA(Tkj ))

(18)

Tij Tij \ {DC} CCP(i, j : k),

(19)

i, j, k represent variables vi , vj vk CCP(i, j : k) represents situation
exists another variable vl vl vk together evidence CCP(i, j).
triple, CCP(i, j : k) determined O(n) time subroutine BC-R EVISION(i, k, j)
carried O(n) time. Since Tij set basic RCC8 relations Dij set
basic RA relations, (Tij , Dij ) revised constant number times. Therefore number
loops remains cubic, BC-C ONSISTENCY terminate O(n4 ) time.
algorithm general complete. following lemma useful prove main
result (Theorem 28), guarantee completeness algorithm RCC8 networks
b8 RA networks H H.
H
Lemma 27. Let = {vi Tij vj }ni,j=1 RCC8 network = {vi Dij vj }ni,j=1 RA network.
b8 Q8 ] bipath-consistent. Assume canonical
Suppose H
consistent scenario (cf. Theorem 3), consistent scenario . ]
bipath-consistent.
Proof. path-consistent basic networks, need show ]
RA( ) RCC8( ) 6= j. Since basic
bi-closed, i.e. ij
ij
ij
ij
ij
ij
nonempty 6= j. (13) (14)
relations, equivalent showing ij
ij
). Therefore RA(T ) = RA( ),
straightforward show RA(Tij ) = RA(ij
ij
ij
ij
ij


i.e. ij ij nonempty.
note result apply C8 . example, let = {NTPP, EQ}, =
{d d, eq eq}. RCC8 relation NTPP inconsistent RA relation eq eq.
Theorem 28. Let = {vi Tij vj }ni,j=1 RCC8 network = {vi Dij vj }ni,j=1 RA netb8 , H H. ] consistent BC-consistent.
work. Suppose H
Proof. Recall RA network H H essence pair IA networks H.
0-meet relation iff is;
Lemma 8 know consistent scenario (i) ij
ij

(ii) ij corner relation iff Dij consists corner relations. Let canonical consistent
scenario . show ] consistent.
Lemma 27 know ] bipath-consistent. next show satisfies (15),
equivalent (16) basic constraints concerned. end, show CCP(i, j :
i0 , j 0 ) holds ] holds ] . choice , know Tii0 =
Tjj 0 = {EC} RA relations either 0-meet relations consist corner relations.
Therefore, CCP(i, j : i0 , j 0 ) also holds ] . ] BC-consistent, know DC
6= {DC} ] satisfies (15). Theorem 22, know ] ,
ij . implies ij
hence ] , consistent.
513

fiC OHN , L , L IU , & R ENZ

b8 Q8 H H
consequence, know joint consistency problem H
solved polynomial time.
b8 , H H) JSP(Q8 , H H) P.
Theorem 29. joint satisfaction problems JSP(H
b8 Q8 , H H.
Proof. Suppose ] joint network H
first apply algorithm BC-C ONSISTENCY ] . empty relation occurs
process, ] inconsistent. Otherwise, suppose 0 ] 0 BC-consistent joint network
b8 Q8 0 H H. note that,
equivalent ] . assert 0 still H
b8 (or Q8 ), RA relation H H, Lemma 31
RCC8 relation H
b8 Q8 ;
RCC8(D) relation H
RA(T ) relation H H;
b8 (or Q8 ).
\ {DC} = {EC, PO, TPP, NTPP, TPP , NTPP , EQ} H
BC-C ONSISTENCY uses rules (17)-(19) update relations, RCC8 relation
b8 (or Q8 ), RA relation 0 remains HH. consistency 0 ]0
0 remains H
follows Theorem 28.
property proof theorem hold C8 . remains open
JSP(C8 , H H) tractable (though important practical purposes since either
b8 Q8 used backtrack find solution required).
H

5. Combination RCC80 RA Networks
section, represent topological information RCC80 relations directional information
RA relations. previous section shown that, certain tractable subclasses RCC8
RA, JSP determined polynomial time, also show bipath-consistency
incomplete subclasses. reason lies two regions constrained DC
may common conflict point. RCC80 , situation exist anymore two
disjoint regions may still 0-dimensional intersection. section show that, RCC80 ,
bipath-consistency alone sufficient show consistency joint network ]
H8 C8 H H.
case weak RCC8, first consider interaction RCC80 RA relations,
consider consistency joint basic networks, and, lastly, consider general case.
Similar Definition 19, following definition.
Definition 30. Let RCC80 relation RA relation. RA relation induced
RCC80 relation induced defined
RA(T ) = { : basic RA relation 6= }
0

RCC8 (D) = { : basic RCC80 relation 6= }.

easy see RA(T ) = {RA({}) : }

(20)
(21)

RA({DC}) RA({EC}) RA({PO}) RA({TPP}) = RA({NTPP}) RA({EQ}).
RA({PO}) RA({TPP }) = RA({NTPP }) RA({EQ}).
514

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

Note RA({TPP}) = RA({NTPP})= {s, d, f, eq} {s, d, f, eq}.
RCC80 non-tangential proper part region may MBR a. example,
star region Figure 6 non-tangential proper part MBR RCC80 .
Lemma 31. Let RCC80 relation RA relation. RCC80 (D) relation
b8 , Q8 , C8 ; RA(T ) relation H H relation H
b8
intersection H
Q8 C8 .
particular, unlike case weak RCC8, RA({NTPP, EQ}) = {s, d, f, eq}
{s, d, f, eq} relation H H.

Figure 6: Basic regions control point P combination RCC80 RA.
Theorem 32. Suppose basic RCC80 network basic RA network. ]
consistent bipath-consistent.
Proof. proof follows pattern combination weak RCC8 RA (Theorem 22), need replace basic regions around control point P star regions shown Figure 6, show three regions b, r, g around P , bNTPPr
rNTPPg.
following result RCC80 RA.
b8 Q8 RCC80 , RA network. ]
Theorem 33. Suppose network H
consistent ] bi-closed, path-consistent, consistent.
Proof. Assume canonical consistent scenario , consistent scenario
) = RA( ) hence
. Then, completely similar Lemma 27, show RA(ij
ij




bi-closeness ] . consistent, know ] bipath-consistent,
hence consistent Theorem 32.
b8 , RA) polynoThe result shows consistency joint network JSP(H
b8 RA network.
mially reduced determining consistency RCC8 network H
b8 , RA) separable problem. particular,
sense, JSP(H
515

fiC OHN , L , L IU , & R ENZ

Theorem 34. RCC8 relations interpreted using strong connectedness, joint satisb8 , H H) JSP(Q8 , H H) P.
faction problems JSP(H
Again, remains open whether result holds networks C8 RCC80 , even
though case RA({NTPP, EQ}) = RA(TPP) relation H H.
following section, consider combination RCC8 CDC constraints.

6. Combination RCC8 CDC Constraints
Although basic RCC8 networks basic CDC networks solved cubic time independently, interaction RCC8 CDC constraints makes joint satisfaction problem
hard solve. section, first show joint satisfaction problem NP designing polynomial non-deterministic algorithm show NP-hard even basic constraints. shows JSP(Brcc8 , Bcdc ) NP-complete. consider three variants
JSP(Brcc8 , Bcdc ) obtained replacing RCC8 RCC80 and/or CDC OIM. Write Boim
set basic OIM relations. show JSP(Brcc8 , Boim ) JSP(Brcc80 , Bcdc ) NP-complete,
JSP(Brcc80 , Boim ) P.
6.1 Algorithms
Let instance joint basic RCC8 RCC80 network basic CDC OIM network
set variables. provide subsection algorithms determining consistency ] . key idea first showing ] consistent iff regular solution
RA consistent (see below) giving algorithms determining whether
regular solution.
Suppose = (mi )ni=1 solution . Recall say = (mi )ni=1 regular solution
maximal solution {M(mi )}ni=1 canonical tuple rectangles (cf. Dfn. 13). Note
region regular solution union set cells introduced canonical
tuple rectangles.
Definition 35. Let = {vi ij vj }ni,j=1 basic RCC8 network = {vi ij vj }ni,j=1 basic
CDC network. Suppose = (mi )ni=1 regular solution . Write RA network
induced m. say regular solution RA consistent exists solution
] also satisfies .
following lemma gives characterisation consistent joint basic networks.
Lemma 36. Let = {vi ij vj }ni,j=1 basic RCC8 network = {vi ij vj }ni,j=1 basic
CDC network. ] consistent iff regular solution RA consistent .
Proof. sufficiency part clear definition. prove necessity part. Suppose =
(ai )ni=1 solution ]. Write RA network induced a. also solution
] . Hence unique regular solution also satisfies . Write = (mi )ni=1
regular solution. clear regular solution RA consistent .
lemma, determine consistency ] , need determine existence
regular solutions RA consistent . Suppose regular solution .
next give necessary sufficient condition RA consistent .
516

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

end, first fix notation terminology. region mi m, say corner
point P mi potential conflict point (in m) exactly one four cells incident P
contained mi . example, grey region shown Figure 7 five potential conflict points
Pi (i = 1, ..., 5). Later show points may introduce conflicts hard resolve
RCC8 constraints involved. Furthermore, denote Gi set cells contained
mi , Ei set edges cells lie boundary mi , Ni set potential conflict
points vi .

Figure 7: Illustration potential conflict points.
Lemma 37. Let = {vi ij vj }ni,j=1 basic RCC8 network = {vi ij vj }ni,j=1 basic
CDC network. regular solution = (mi )ni=1 RA consistent , have:
ij = TPP NTPP Gi Gj .

(22)

Proof. prove contradiction. Assume (vi TPPvj ) (vi NTPPvj ) constraint
Gi * Gj .
Suppose (vs vt ) constraint , basic CDC relation represented 3 3
Boolean matrix (dpq )1p,q3 . solution , 1 p, q 3
dpq = 1 iff ms mpq
6= ,

(23)

mpq
denotes one nine tiles generated MBR mt (cf. Fig. 3). Since
regular solution Gs set cells contained ms , equivalent saying
dpq = 1 iff

Gs mpq
common cell.

(24)

let g cell Gi \ Gj . g Gj , construction procedure regular
solutions (see Appendix A), exists constraint (vj 0 vk ) 0 = (d0uv ) g
0pq = 0 hence,
cell contained mpq
k p, q. (24) g Gj know
pq

00
(23), mj mk = . Let (vi vk ) CDC constraint vi vk suppose
00pq = 1 (24).
00 = (d00uv ). g cell Gi mpq
k ,
RA consistent , exists solution = (ai )ni=1 ]
M(ai ) = M(mi ). Since (vi TPPvj ) (vi NTPPvj ) , know ai aj . Furthermore,
pq
aj mj maximal solution . Therefore, ai mj . mpq
k = ak
pq
pq


00
00pq
mj mk = , ai ak empty. shows (ai , ak ) since
= 1.
contradiction.
NTPP constraints may furthermore exclude edges Ei nodes Ni
valuation vi . Suppose vi NTPPvj constraint = (mi )ni=1 RA consistent
. solution = (ai )ni=1 ] M(ai ) = M(mi ), ai NTPPaj aj mj
517

fiC OHN , L , L IU , & R ENZ

aj mj = . say, ai cannot touch edges nodes Ej Nj .
characterise this, define
[
Ei Ei \ {Ej : vi NTPPvj },
(25)
[
Ni Ni \ {Nj : vi NTPPvj }.
(26)
Since every region represented Boolean matrix, Gi , Ei , Ni calculated
polynomial time. following proposition gives necessary sufficient condition
RA consistent .
Lemma 38. Let = {vi ij vj }ni,j=1 basic RCC8 network = {vi ij vj }ni,j=1 basic
CDC network. regular solution = (mi )ni=1 RA consistent iff
vi TPPvj vi NTPPvj implies Gi Gj ,

M( Ei ) = M(mi ) i,
vi POvj implies Gi Gj 6= ,

exists resolving function f , defined function V P( {N1 , , Nn })
satisfying (27)-(29).
f (vi ) Ni ,

(27)

vi ECvj Gi Gj 6= Ei Ej 6= f (vi ) f (vj ) 6= ,

(28)

vi DCvj f (vi ) f (vj ) = .

(29)

Proof. begin necessity part. Suppose RA consistent . definition
exists solution = (ai )ni=1 ] M(ai ) = M(mi ). first condition
proven Lemma 37. second condition, M(ai ) = M(mi ), ai mi , know
ai nonempty intersection one unit edge cell lies top (bottom, leftmost,
rightmost) edge M(mi ). unit
edge clearly Ei . Furthermore, proven
edge Ei , thus M( Ei ) = M(mi ). following two conditions guarantee
PO, EC constraints
satisfied violating DC constraints. third condition
follows directly ai Gi ai POaj . last condition, define resolving function
f f (vi ) = {P Ni : P ai }. straightforward prove f satisfies (27)-(29).
sufficiency part, construct solution ] . procedure quite similar
given Theorem 22 Appendix B. vi , choose control point cell Gi
control point edge Ei . vi POvj , choose control point
common cell Gi Gj . vi ECvj , choose control point common cell
Gi Gj 6= , common edge Ei Ej 6= , f (vi ) f (vj ) resolving
function f . proven control points lead solution . Moreover,
choice control points ensures regions also solution .
Since conditions Lemma 38 verified nondeterministic polynomial algorithm,
following theorem.
Theorem 39. JSP(Brcc8 , Bcdc ) NP.
518

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

Proof. Suppose ] instance JSP(Brcc8 , Bcdc ). devise nondeterministic polynomial
algorithm follows. first guess basic RA network consistent basic CDC
network , compute regular solution satisfies cubic time, guess
resolving function f satisfies conditions (27)-(29), check whether consistent
basic RCC8 network Lemma 38 via f . case, algorithm returns true.
Otherwise, algorithm returns false. shows JSP(Brcc8 , Bcdc ) NP.
Since OIM network one regular solution, Lemma 38
Corollary 40. JSP(Brcc8 , Boim ) NP.
interpret topological constraints RCC80 , following simplified condition
determining whether regular solution RA consistent .
Proposition 41. Suppose basic RCC80 network, basic CDC network,
set variables V = {v1 , , vn }. regular solution RA consistent
iff
vi TPPvj vi NTPPvj implies Gi Gj ,
vi POvj implies Gi Gj 6= ,
vi ECvj implies either Gi Gj Ei Ej nonempty.
Moreover, conditions checked polynomial time.
Proof. proof similar Lemma 38. resolving function irrelevant,
RCC80 conflict points longer evidence EC relations, point P (a set X, respectively) regarded evidence
relation (aECb) P 0 b (X b), respectively.
Note require M( Ei ) = M(mi ), RCC8 possible aNTPPb
M(a) = M(b), see Figure 6 illustration.
directly leads following two results.
Theorem 42. JSP(Brcc80 , Bcdc ) NP.
Proof. proof similar Theorem 39. Suppose ] instance JSP(Brcc8 , Bcdc ).
first guess basic RA network consistent construct regular solution
satisfies check whether RA consistent Proposition 41.
Recall OIM essence combination CDC RA, basic OIM network
consistent iff two component CDC RA networks consistent (see Proposition 17).
case RCC80 combined OIM, following tractability result.
Theorem 43. JSP(Brcc80 , Boim ) P.
Proof. algorithm JSP(Brcc80 , Boim ) contains three steps. Suppose ] instance
n variables. first step decide whether independently consistent.
so, return false; otherwise, construct unique regular solution . achieved
O(n3 ) time. calculate Gi Ni , done O(n4 ) time. third step
decide whether RA consistent according Proposition 41, done
O(n4 ) time. Therefore, consistency ] determined O(n4 ) time and, hence,
JSP(Brcc80 , Boim ) P.
519

fiC OHN , L , L IU , & R ENZ

next subsection, show JSP(Brcc8 , Bcdc ), JSP(Brcc80 , Bcdc ), JSP(Brcc8 , Boim )
NP-hard.
6.2 NP-Hardness Results
Recall proof Theorem 39, guess twice determining consistency
instance ] JSP(Brcc8 , Bcdc ), basic RA network consistent ,
resolving function f satisfies (27)-(29) (see Proposition 38). subsection
devise two polynomial reductions known NP-hard problems JSP(Brcc8 , Bcdc ) exploiting
two facts.
Theorem 44. JSP(Brcc8 , Bcdc ) NP-hard.
Proof. first reduction 3-SAT JSP(Brcc8 , Bcdc ). quite complicated,
defer construction Appendix C. explain problem NP-hard.
3-SAT instance , construct instance ] JSP(Brcc8 , Bcdc )
RCC8 constraint either DC EC constraint. Furthermore, show
unique regular solution RA consistent consistent.
intractability caused potential conflict points regular solution, which, together EC DC constraints, may introduce conflicts hard resolve.
Lemma 38, satisfy EC constraint vi ECvj , need check whether mi mj share
cell, else edge, else corner point. last case, proven without much difficulty
points shared mi mj exactly points Ni Nj . Therefore, mi mj share
cell edge, evidence point constraint vi ECvj chosen Ni Nj .
turns choosing evidence points EC constraints violating
DC constraints NP-hard.
second reduction Graph 3-colouring problem JSP(Brcc8 , Bcdc ). defer construction Appendix D. graph G, construct instance G ] G JSP(Brcc8 , Bcdc ).
reduction differs first one exploit intractability finding
resolving function. fact, vi ECvj constraint, regular solution G ,
either mi mj share cell edge, mi mj disjoint (in case RA
consistent ). say, resolving functions effect RA consistency m.
reduction based fact G exponentially many regular solutions,
general way test polynomial time (unless P = NP).
Note first reduction shown unique regular solution RA
consistent consistent, 3-SAT instance ] instance
JSP(Brcc8 , Bcdc ) defined reduction. Write basic RA network induced
particular regular solution . easy see ] consistent iff ] ]
consistent. words, reduction 3-SAT also reduction JSP(Brcc8 , Boim ).
Corollary 45. JSP(Brcc8 , Boim ) NP-hard.
Similarly, second reduction also reduction JSP(Brcc80 , Bcdc ).
JSP(Brcc8 , Bcdc ) instance graph G uses DC EC constraints, two variables required EC, MBRs 0-meet, MBRs may overlap.
Corollary 46. JSP(Brcc80 , Bcdc ) NP-hard.
520

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

NP-hardness results Theorem 39, Corollary 40, Theorem 42, know
Theorem 47. joint satisfaction problems JSP(Brcc8 , Bcdc ), JSP(Brcc80 , Bcdc ), JSP(Brcc8 ,
Boim ) NP-complete.

7. Conclusion
paper, investigated computational complexity reasoning topological
relations cardinal directions extended spatial objects. used two different interpretations well-known RCC8 algebra representing topological information, use
Rectangle Algebra (RA) cardinal direction calculus (CDC) describe directional information. shown joint satisfaction problems decidable remain NP
interpretations topological directional information. importantly, shown
consistency problem P basic (weak strong) RCC8 basic RA constraints
involved, topological constraints basic strong RCC8 constraints directional
constraints jointly represented basic RA CDC constraints.
related work reported work Sistla Yu (2000), Li (2007), Li
Cohn (2012), small fragments RA used express directional information.
results represent large step towards applicability qualitative spatial reasoning techniques
real-world problems. particular tractable results promising enable efficient
reasoning important calculi. also means efficient reasoning important
potential application, developers aim representing directional information using RA
(or together CDC) instead CDC alone and/or representing topological information using
RCC80 instead RCC8. results combining RCC8 CDC/OIM important
theoretical point view first formal results combination. demonstrated using concrete application scenario, results also practical significance,
combined information consider easily extracted cases computer vision used
obtaining spatial information.

Acknowledgments
thank anonymous reviewers invaluable comments detailed suggestions.
first author also thanks University Technology Sydney funding visit Sydney
Adjunct Professor. work partially supported Australian Research Council (Grant
No.s DP120104159, DP120103758 FT0991917), National Natural Science Foundation
China (Grant No. 61228305), EU funded projects RACE (FP7-ICT-287752) STRANDS
(FP7-ICT-600623).

Appendix A. Realisation Basic CDC Networks
describe cubic algorithm given work Zhang et al., (2008), Liu et al., (2010).
Given basic CDC network, first, compute canonical solution induced (possibly nonbasic) RA network. Next, remove cells violate constraints rectangle.
Third, check whether obtained valid solution. following, give
detailed description running example illustrated Table 4 Figure 8.
521

fiC OHN , L , L IU , & R ENZ

(1, 2)

(1, 3)

(2, 3)


0
1
1
0
1
1
1
1
0

ij
0
1
0
0
0
0
1
0
0


0
0
0
0
0
0
0
0
0


0
0
0
0
0
0
0
0
0

ji
1
1
0
0
1
0
0
0
1

xij yij

xij yij

oo

oo

{m, b} fi

b fi

oi

oi



1
1
0
0
0
0
0
1
1

Table 4: Example solving basic CDC network.

Step 1. Compute induced RA network 0 .
Step 2. Refine 0 basic RA network = {vi (xij yij )vj }ni,j=1 setting xij = xij \{m, mi}
yij = yij \{m, mi}. unsatisfiable, neither . Suppose satisfiable construct
canonical solution = (mi )ni=1 (cf. Figure 8).

Figure 8: Illustration Step 3: Deriving solution canonical solution .
Step 3. step tries find solution = (mi )ni=1 basic CDC network
M(mi ) = M(mi ). Recall basic CDC relation ij represented 3 3 Boolean matrix
((ij )xy ). solution, mi (mj )xy = holds every (ij )xy = 0, (mj )xy one
nine tiles generated M(mj ) (cf. Figure 3).
make solution ,
means,


need exclude impossible cells mi . Set Ti = {(mj )xy : (ij )xy = 0}nj=1 . Let mi
closure mi \ Ti (cf. Figure 8 left).
Step 4. last step checks whether = (mi )ni=1 solution . solution,
must regular solution; not, assert solution all.
note regular solutions may exist (cf. Figure 8 right). get
repeating Steps 2 4 using every possible refinement 0 .

Appendix B. Proof Theorem 22
need show sufficiency part. Similar cubic construction method basic RCC8 constraints (cf. Li, 2006b Section 2.2.1 paper), construct solution
= (mi )ni=1 additional requirement M(mi ) = mi 1 n,
{m1 , ..., mn } canonical solution . Recall coordinates corner point
rectangle mi integral. Assuming ntpp-level l(i) computed 1 n,
next describe construction detail.
522

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

Step 1. Selection Control Points
vi select set control points Xi . First all, corner point Ci control point
vi . select one (non-integral) point edge mi put four points
Xi . Then, j > ij = EC PO select point Pij mi mj (which nonempty
bipath-consistency ] ), put Xi Xj . Note mi mj
could single point, line segment, rectangle. choosing Pij mi mj ,
require Pij fresh point chosen corner point
rectangle (unless mj singleton set). write P set control points.
Step 2. Basic Regions Associated Control Points
control point Q, construct series sectors {q i,k : k = 1, , 4}ni=1 series
squares {q (i) }ni=1 (see Figure 9). call basic regions associated Q. Note
use upper case letter denote control point, use corresponding lower case letter (with
indices) denote basic regions. sectors chosen way allows us distinguish
four connecting regions cases Q corner point (such point P Figure3).
sectors completely fill disks.

Figure 9: Basic regions control point Q.
two different control points, require outermost squares disjoint. Furthermore, basic region must small enough crossed border mi
Q boundary point.
Step 3. Region Construction
control point Q, set q =
a1i
a2i
a3i
a4i

S4

k=1 q

i,k .

Let
[
= mi {q : Q Xi }
[
= a1i (mi {q j : ij = PO, Q Xi Xj })
[
= a2i {a2j : ji = TPP ji = NTPP}
[
= a3i {q (l(i)) : ji = NTPP, Q a3j }

Set mi = a4i = (mi )ni=1 . easy prove satisfies RCC8 constraints .
example, suppose (vi DC vj ) constraint . (15) holds, know vi vj share
common conflict point, i.e. Ci Cj = . Due choice control points vi vj , know
523

fiC OHN , L , L IU , & R ENZ

Xi Xj also empty. easy show mi mj empty hence DC constraint
satisfied.
show also satisfies , need prove M(mi ) = mi i. clear a1i
a2i subsets mi . choice Xi , know mi = M(a1i ) = M(a2i ). ji = TPP
NTPP, mj mi bipath-consistency. implies M(a3i ) = mi . Furthermore,
ji = NTPP, (mj , mi ) bipath-consistency. control point Q
a3j mj , Q also interior mi . Therefore, choice basic regions, know
outmost square q (n) Q, hence q (l(i)) , contained mi . Therefore, M(a4i ) = mi . proves
solution ] .

Appendix C. Reduction 3SAT JSP(Brcc8 , Bcdc )
V
n
Let =
i=1 cj 3SAT instance involving n propositional variables {pk }k=1 clauses.
Assume j-th clause cj qi1 qi2 qi3 , qij literal {pk }nk=1 {pk }nk=1 .
construct JSP(Brcc8 , Bcdc ) instance ] choose particular regular solution
satisfiable iff RA consistent .
three types spatial variables ] : auxiliary variables (called grid variables)
used fix relative locations variables, variables simulate propositional
variables, variables simulate propositional clauses.
C.1 Grid Variables
introduce 10 n grid variables Gij (1 2n, 1 j 5). CDC constraints
variables specified Figure 10 (left). RCC8 relation two grid variables
Gij , Gi0 j 0 EC 4-neighbours, i.e. {|i i0 |, |j j 0 |} = {0, 1}. EC constraints
make sure gap MBRs two neighbouring grid variables. implies
one regular solution .
Grid variables mainly used locate spatial variables. new variable v grid
variable Gij , say v occupies Gij v M(Gij ) nonempty, MBR M(Gij ), i.e.
M(v M(Gij )) = M(Gij ).

Figure 10: Grid spatial variables propositional variables.

C.2 Spatial Variables Propositional Variables
propositional variable pi , four spatial variables Ai , Bi , Ci Di introduced
occupies two grid cells, empty intersection interiors MBRs
524

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

grid variables. corresponding grid cells illustrated Figure 10 (right). Take Ai
example. assigning CDC constraints Ai grid variables, require Ai
occupies G2i1,1 G2i1,4 , disjoint MBRs grid variables. easy
see Ai Bi contains two points, viz. Pi+ Pi , Ci Di .
topological constraints, require Ai ECBi Ci ECDi , constraints
DC. EC constraints imply Ai Bi Ci Di nonempty. hand,
since Ai DCCi , conclude Ai Bi must share one Pi+ Pi , Ci
Di share one.
C.3 Spatial Variables Propositional Clauses
clause (qj1 qj2 qj3 ) , introduce two new spatial variables Ej Fj ,
occupy three grid cells. precise occupied grid cells set according variables signs
qjk . Figure 11 gives example illustrate construction, assume qj1 = pi1 , qj2 =
pi2 , qj3 = pi3 . topological constraints, set constraint Ej Fj
EC, set constraints DC. implies Ej Fj contains least one point
Pi1 , Pi+2 , Pi+3 . claim case Ai1 Bi1 = {Pi1 }, Ai2 Bi2 = {Pi+2 }
Ai3 Bi3 = {Pi+3 }. Otherwise, DC constraint, e.g. Ai1 Ej , violated.
C.4 Regular Solution May RA Consistent

Figure 11: Spatial variables clauses.
finished construction. Note always satisfiable exponentially many regular solutions (as may may gap 4-neighbouring
grid variables). However, EC constraints 4-neighbouring grid variables imply
regular solution gap 4-neighbouring grid variables RA
consistent . denote regular solution m.
next show consistent iff ] consistent. Suppose ] solution a.
define assignment : {pi }ni=1 {true, f alse} (pi ) = true iff Ai Bi = {Pi+ }
a. verify satisfies . hard, suppose assignment satisfies
. prove ] solution. idea introduce instance JSP(Brcc8 , Brec ),

two spatial variables A+
Ai instead Ai (also Bi , Ci , Di ), three
k
variables Ej (1 k 3) instead Ej (also Fj ). RA constraints set according
Figure 10 Figure 11, RCC8 constraints set . proven
new joint network satisfies (15), solution obtained cubic time. solution

] obtained merging related regions (e.g. merging A+
Ai Ai ).
525

fiC OHN , L , L IU , & R ENZ

verification straightforward. Therefore, satisfiable iff ] satisfiable, thus
satisfiable iff RA consistent .

Appendix D. Reduction Graph 3-Colouring JSP(Brcc8 , Bcdc )
Suppose G = (V, E) graph. construct instance G ] G JSP(Brcc8 , Bcdc ) follows.
node vi V , construct gadget 10 spatial variables: uki (k = 1, 2, , 8), xi
0
yi . first describe CDC constraints. basic CDC constraints uki uki
specified Figure 12 (i). example, G contains following basic CDC constraints








0 0 0
0 0 0
1 1 0
0 0 0
u1i 1 0 0 u2i , u2i 0 0 1 u1i , u2i 1 1 0 u7i , u7i 0 1 1 u2i .
0 0 0
0 0 0
0 0 0
0 1 1
Note that, induced RA constraint u2l+1
u2l+2
l = 0, 1, 2 (b m) eq.


basic CDC constraints xi yi specified Figure 12 (ii), i.e.




0 0 0
0 0 0
xi 1 1 0 yi , yi 0 1 1 xi .
0 0 0
0 0 0
CDC constraints concerning xi , yi uki specified follows.

0
xi 0
0

1
xi 0
0

0
k
ui 0
0

0

yi 0
0

1
yi 0
0

0
uki 0
0




0 0 0
0 0
1 0 u1i , xi 1 0 0 u2i ,
0 0 1
0 1



0 0
1 0 0
1 0 u5i , xi 1 0 0 u6i ,
0 0
0 0 0

0 0
1 0 xi (k 6= 6),
0 0



0 0
0 0 0
0 1 u1i , yi 0 1 0 u2i ,
0 1
0 0 1



0 0
1 0 0
0 1 u5i , yi 0 1 0 u6i ,
0 0
0 0 0

0 0
1 0 yi (2 k 8),
0 0


1
xi 0
0

1
xi 0
0

0
6
ui 0
0

1

yi 0
0

1
yi 0
0

0
u1i 1
0




1 0 0
0 0
1 0 u3i , xi 1 0 0 u4i ,
0 0 1
0 1

0 0
0 0 uki (k = 7, 8),
0 1

0 0
0 1 xi ;
0 0



0 0
1 0 0
0 1 u3i , yi 0 1 0 u4i ,
0 1
0 0 1

0 0
0 0 uki (k = 7, 8),
0 1

0 0
0 0 yi .
0 0

Figure 12 (ii) illustrates regular solution uki , xi yi , u1i meets u2i
gap u3i u4i , u5i u6i . note total eight different
regular solutions network restricted gadget vi .
526

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

(i)

(ii)
Figure 12: Illustrations CDC constraints: (i) constraints uki (k = 1, 2, , 8); (ii)
constraints relating xi , yi . Note use dashed squares denote variables u7i u8i ,
connect two variables. Note xi yi three disjoint parts.

527

fiC OHN , L , L IU , & R ENZ

RCC8 constraint two variables either EC DC. require xi ECyi .
realisable u2l+1
meets u2l+2
x-direction least one l {0, 1, 2}.4 use fact


mimic node vi V coloured one three colours. RCC8 constraints
remaining pairs variables specified DC. 5
gadgets nodes V horizontally aligned, illustrated Figure 13.

Figure 13: Illustrations gadgets nodes V

Figure 14: Illustrations gadget edge ek = (vi , vj ), dashed lines suggest corresponding edges aligned according proper CDC constraints.
devise gadgets edges graph G. Let ek = (vi , vj ) edge E.
0 , w 1 , w 2 w 3 well constraints
colour l {0, 1, 2}, introduce four variables wk,l
k,l
k,l
k,l
2l+2
2l+1
2l+2
guarantee u2l+1
cannot
meet
u

u
meets
u
,

corresponds
nodes vi


j
j
vj cannot colour l (because ek edge G). CDC constraints specified
1 meets w 3 iff u2l+1 meets u2l+2 , w 3 meets w 2 iff u2l+1
Figure 14. note wk,l


j
k,l
k,l
k,l
3 ) contained
meets u2l+2
,


x-direction.


CDC
constraints


show

M(w
j
k,l
0 ). implies either gap w 1 w 3 gap w 3
M(wk,l
k,l
k,l
k,l
2 . words, w 1 meets w 3 w 3 meets w 2 cannot happen simultaneously.
wk,l
k,l
k,l
k,l
k,l
4. one l u2l+1
meets u2l+2
x-direction, always choose smallest l


colour node vi .
5. Note u1i DCu2i together CDC relations u1i u2i necessarily imply u1i
precede u2i x-direction. is, u1i could still meet u2i x-direction case.

528

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

constraints enforcing dashed lines know u2l+1
cannot meet u2l+2
u2l+1
meets u2l+2
,


j
j
vice versa.
Note need complete . unspecified CDC constraints easily
deduced figures, unspecified RCC8 constraints DC.
hard verify graph G 3-colourable iff joint network G ] G consistent.
idea that, : V {0, 1, 2} 3-colouring G, may construct solution
G ] G RA relation u2l+1
u2l+2
eq (i) = l, b eq


otherwise. guarantees xi yi realisable. fact incident nodes G
r realisable. hand, ] satisfiable,
colour implies wk,l
G
G
least one pair {(u1i , u2i ), (u3i , u4i ), (u5i , u6i )} RA relation eq (otherwise,
xi ECyi violated). Define : V {0, 1, 2} (vi ) = min{l : u2l+1
eq u2l+2
}.


r realisable.
verified 3-colouring G due fact wk,l
completed reduction Graph 3-Colouring JSP(Brcc8 , Bcdc ). note
EC constraints also interpreted terms RCC80 . means G ] G
also regarded instance JSP(Brcc8 , Bcdc ). Therefore, also provided reduction
Graph 3-Colouring JSP(Brcc80 , Bcdc ).

References
Allen, J. F. (1983). Maintaining knowledge temporal intervals. Communications ACM,
26(11), 832843.
Balbiani, P., Condotta, J.-F., & Farinas del Cerro, L. (1999). new tractable subclass rectangle algebra. Dean, D. (Ed.), Proceedings Sixteenth International Joint Conference
Artificial Intelligence (IJCAI-99), pp. 442447. Morgan Kaufmann.
Bodirsky, M., & Kara, J. (2010). complexity temporal constraint satisfaction problems. J.
ACM, 57(2).
Borgo, S., Guarino, N., & Masolo, C. (1996). pointless theory space based strong connection
congruence. Aiello, L. C., Doyle, J., & Shapiro, S. C. (Eds.), KR, pp. 220229. Morgan
Kaufmann.
Chen, J., Cohn, A. G., Liu, D., Wang, S., Ouyang, J., & Yu, Q. (2013). survey qualitative
spatial representations. Knowledge Engineering Review, FirstView, 131.
Cicerone, S., & Di Felice, P. (2004). Cardinal directions spatial objects: pairwiseconsistency problem. Information Sciences, 164(1-4), 165188.
Cohn, A. G., & Renz, J. (2008). Qualitative spatial reasoning. van Harmelen, F., Lifschitz, V., &
Porter, B. (Eds.), Handbook Knowledge Representation. Elsevier.
Cohn, A. G., Renz, J., & Sridhar, M. (2012). Thinking inside box: comprehensive spatial
representation video analysis. KR, pp. 588592.
Cohn, A. G., & Varzi, A. C. (1999). Modes connection. Freksa, C., & Mark, D. M. (Eds.),
COSIT, Vol. 1661 Lecture Notes Computer Science, pp. 299314. Springer.
Davis, E. (2013). Qualitative spatial reasoning interpreting text narrative. Spatial Cognition
& Computation, 13(4), 264294.
529

fiC OHN , L , L IU , & R ENZ

Davis, E., Gotts, N. M., & Cohn, A. G. (1999). Constraint networks topological relations
convexity. Constraints, 4(3), 241280.
Duntsch, I., Wang, H., & McCloskey, S. (2001). relation-algebraic approach Region
Connection Calculus. Theoretical Computer Science, 255, 6383.
Egenhofer, M. J., & Mark, D. M. (1995). Naive geography. Frank, A., & Kuhn, W. (Eds.),
COSIT-95, pp. 115. Springer.
Falomir, Z. (2012). Qualitative distances qualitative description images indoor scene
description recognition robotics. AI Communications, 25(4), 387389.
Freksa, C. (1992). Temporal reasoning based semi-intervals. Artificial Intelligence, 54(1), 199
227.
Gabelaia, D., Kontchakov, R., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2005). Combining
spatial temporal logics: Expressiveness vs. complexity. Journal Artificial Intelligence
Research, 23, 167243.
Ge, X., & Renz, J. (2013). Representation reasoning general solid rectangles. Rossi,
F. (Ed.), IJCAI. IJCAI/AAAI.
Gerevini, A., & Renz, J. (2002). Combining topological size information spatial reasoning.
Artificial Intelligence, 137(1), 142.
Gerevini, A., & Nebel, B. (2002). Qualitative spatio-temporal reasoning rcc-8 allens
interval calculus: Computational complexity. ECAI, pp. 312316.
Goyal, R., & Egenhofer, M. (1997). direction-relation matrix: representation directions
relations extended spatial objects. Annual Assembly Summer Retreat
University Consortium Geographic Information Systems Science.
Goyal, R., & Egenhofer, M. (2001). Similarity cardinal directions. Jensen, C., Schneider, M.,
Seeger, B., & Tsotras, V. (Eds.), Proceedings 7th International Symposium Advances
Spatial Temporal Databases (SSTD-01), pp. 3658. Springer.
Hirsch, R. (1999). finite relation algebra undecidable network satisfaction problem. Logic
Journal IGPL, 7(4), 547554.
Jonsson, P., & Krokhin, A. A. (2004). Complexity classification qualitative temporal constraint
reasoning. Artificial Intelligence, 160(1-2), 3551.
Kontchakov, R., Nenov, Y., Pratt-Hartmann, I., & Zakharyaschev, M. (2011). decidability
connectedness constraints 2D 3D Euclidean spaces. IJCAI, pp. 957962.
Li, S. (2006a). Combining topological directional information: First results. Lang, J., Lin, F.,
& Wang, J. (Eds.), Proceedings First International Conference Knowledge Science,
Engineering Management (KSEM-06), pp. 252264. Springer.
Li, S. (2006b). topological consistency realization. Constraints, 11(1), 3151.
Li, S. (2007). Combining topological directional information spatial reasoning. Veloso,
M. (Ed.), Proceedings 20th International Joint Conference Artificial Intelligence
(IJCAI-07), pp. 435440. AAAI.
Li, S., & Cohn, A. G. (2012). Reasoning topological directional spatial information.
Computational Intelligence, 28(4), 579616.
530

fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS

Li, S., & Liu, W. (2014). Cardinal directions: comparison direction relation matrix objects
interaction matrix. International Journal Geographical Information Science, accepted
publication, doi: http://dx.doi.org/10.1080/13658816.2014.954580.
Li, S., Liu, W., & Wang, S. (2013). Qualitative constraint satisfaction problems: extended
framework landmarks. Artificial Intelligence, 201, 3258.
Ligozat, G. (1994). Tractable relations temporal reasoning: pre-convex relations. ECAI-94.
Workshop Spatial Temporal Reasoning, pp. 99108.
Ligozat, G., & Renz, J. (2004). qualitative calculus? general framework. Zhang, C.,
Guesgen, H., & Yeap, W.-K. (Eds.), PRICAI-04, pp. 5364. Springer.
Liu, W., & Li, S. (2011). Reasoning cardinal directions extended objects: NPhardness result. Artificial Intelligence, 175(18), 21552169.
Liu, W., Li, S., & Renz, J. (2009). Combining RCC-8 qualitative direction calculi: Algorithms
complexity. Boutilier, C. (Ed.), Proceedings Twenty-first International Joint
Conference Artificial Intelligence (IJCAI-09), pp. 854859.
Liu, W., Zhang, X., Li, S., & Ying, M. (2010). Reasoning cardinal directions extended objects. Artificial Intelligence, 174(12-13), 951983.
Nebel, B. (1995). Computational properties qualitative spatial reasoning: First results. KI-95,
pp. 233244, Berlin, Germany. Springer-Verlag.
Nebel, B., & Burckert, H.-J. (1995). Reasoning temporal relations: maximal tractable
subclass Allens interval algebra. Journal ACM, 42(1), 4366.
Randell, D. A., Cui, Z., & Cohn, A. G. (1992). spatial logic based regions connection.
KR-92, pp. 165176.
Renz, J. (1999). Maximal tractable fragments Region Connection Calculus: complete
analysis. Dean, D. (Ed.), Proceedings Sixteenth International Joint Conference
Artificial Intelligence (IJCAI-99), pp. 448454. Morgan Kaufmann.
Renz, J. (2002). Qualitative spatial reasoning topological information, Vol. 2293 Lecture
Notes Artificial Intelligence. Springer-Verlag, Berlin, Germany.
Renz, J., & Nebel, B. (1999). complexity qualitative spatial reasoning: maximal
tractable fragment Region Connection Calculus. Artificial Intelligence, 108, 69123.
Schneider, M., Chen, T., Viswanathan, G., & Yuan, W. (2012). Cardinal directions complex
regions. ACM Transactions Database Systems, 37(2), 8:18:40.
Schockaert, S., & Li, S. (2012). Convex solutions RCC8 networks. ECAI, pp. 726731.
Schockaert, S., & Li, S. (2013). Combining RCC5 relations betweenness information.
IJCAI, pp. 10831089.
Shi, H., Jian, C., & Krieg-Bruckner, B. (2010). Qualitative spatial modelling human route instructions mobile robots. ACHI, pp. 16.
Sistla, A., & Yu, C. (2000). Reasoning qualitative spatial relationships. Journal Automated
Reasoning, 25(4), 291328.
Skiadopoulos, S., & Koubarakis, M. (2005). consistency cardinal direction constraints.
Artificial Intelligence, 163(1), 91135.
531

fiC OHN , L , L IU , & R ENZ

Sridhar, M., Cohn, A. G., & Hogg, D. C. (2011). video RCC8: Exploiting distance
based semantics stabilise interpretation mereotopological relations. COSIT, pp.
110125.
Vilain, M. B., & Kautz, H. A. (1986). Constraint propagation algorithms temporal reasoning.
AAAI, pp. 377382.
Whitehead, A. (1929). Process Reality: Essay Cosmology. Cambridge University Press,
Cambridge.
Wolfl, S., & Westphal, M. (2009). combinations binary qualitative constraint calculi.
Boutilier, C. (Ed.), Proceedings Twenty-first International Joint Conference Artificial
Intelligence (IJCAI-09), pp. 967972.
Wolter, D., & Wallgrun, J. (2012). Qualitative spatial reasoning applications: New challenges
sparq toolbox. Hazarika, M. (Ed.), Qualitative Spatio-Temporal Representation
Reasoning: Trends Future Directions, pp. 336362.
Wolter, D., Dylla, F., Wolfl, S., Wallgrun, J. O., Frommberger, L., Nebel, B., & Freksa, C. (2008).
Sailaway: Spatial cognition sea navigation. KI, 22(1), 2830.
Wolter, F., & Zakharyaschev, M. (2000). Spatial reasoning RCC-8 boolean region terms.
ECAI, pp. 244250.
Zhang, P., & Renz, J. (2014). Qualitative spatial representation reasoning angry birds:
extended rectangle algebra. KR.
Zhang, X., Liu, W., Li, S., & Ying, M. (2008). Reasoning cardinal directions: efficient
algorithm. Fox, D., & Gomes, C. (Eds.), Proceedings Twenty-Third AAAI Conference
Artificial Intelligence (AAAI-08), pp. 435440. AAAI.

532

fiJournal Artificial Intelligence Research 51 (2014) 1-70

Submitted 2/14; published 9/14

Cooperative Monitoring Diagnose Multiagent Plans
Roberto Micalizio
Pietro Torasso

micalizio@di.unito.it
torasso@di.unito.it

Dipartimento di Informatica,
Universita di Torino
corso Svizzera 185, 10149 - Torino, Italy

Abstract
Diagnosing execution Multiagent Plan (MAP) means identifying explaining
action failures (i.e., actions reach expected effects). Current approaches
MAP diagnosis substantially centralized, assume action failures independent other.
paper, diagnosis MAPs, executed dynamic partially observable
environment, addressed fully distributed asynchronous way; addition, action
failures longer assumed independent other.
paper presents novel methodology, named Cooperative Weak-Committed Monitoring (CWCM), enabling agents cooperate monitoring actions. Cooperation helps agents cope scarcely observable environments: agent
cannot observe directly acquired agents. CWCM exploits nondeterministic action models carry two main tasks: detecting action failures building
trajectory-sets (i.e., structures representing knowledge agent environment recent past). Relying trajectory-sets, agent able explain
action failures terms exogenous events occurred execution
actions themselves. cope dependent failures, CWCM coupled diagnostic
engine distinguishes primary secondary action failures.
experimental analysis demonstrates CWCM methodology, together
proposed diagnostic inferences, effective identifying explaining action failures
even scenarios system observability significantly reduced.

1. Introduction
Multiagent Plans (MAPs) adopted many applicative domains, Web services
service robots, whenever interactions among cooperative agents organized
advance (i.e., planned), order reach acceptable efficiency level execution;
consider instance, orchestrated Web services (Yan, Dague, Pencole, & Cordier, 2009),
assembling tasks (Heger, Hiatt, Sellner, Simmons, & Singh, 2005; Sellner, Heger, Hiatt,
Simmons, & Singh, 2006), service robotics (Micalizio, Torasso, & Torta, 2006). MAPs
therefore characterized cooperative team agents perform actions concurrently
order achieve common global goal.
use MAPs real-world scenarios, however, cope critical issue:
plan actions deviate expected nominal behaviors due occurrence
(unpredictable) exogenous events. deviations typically considered plan failures
since prevent agents reach goals. Indeed, although MAPs versatile
c
2014
AI Access Foundation. rights reserved.

fiMicalizio & Torasso

systems, also particularly fragile: failure action easily propagate
system causing failures actions, even assigned different agents.
order make execution MAP robust (i.e., tolerant least exogenous
events), therefore important detect isolate action failures, provide human
user (or plan repair module) set possible explanations detected failures.
recent works (see e.g., Mi & Scacchi, 1993; Gupta, Roos, Witteveen, Price, & de Kleer,
2012; Micalizio, 2013) argued plan repair procedure effective
causes plan failure known (e.g., identified via diagnosis).
last decade, problem diagnosing execution MAP faced
different perspectives. Since seminal work, Kalech Kaminka (2003) focus
coordination failures introduce notion social diagnosis. Social diagnosis relies
abstract representation MAP hand, given terms behaviors, aims
explaining two agents selected conflicting behaviors. Kalech Kaminka
(2011) Kalech (2012) present alternative algorithms inferring social diagnosis.
approaches (see e.g., de Jonge, Roos, & Witteveen, 2009; Roos & Witteveen,
2009; Micalizio & Torasso, 2008, 2009) adopt explicit representation MAP terms
agents actions shared resources. particular, diagnostic framework, Roos
Witteveen (2009) de Jonge et al. (2009) consider action failures (i.e., actions
reach expected effects), introduce notion plan diagnosis. plan
diagnosis subset (already performed) actions that, assumed abnormal, make
plan execution consistent observations received far. Since set
observations possibly explained many plan diagnoses, Roos Witteveen (2009)
present criterion identifying preferred diagnoses based predictive power
diagnoses.
proposals, however, rely assumptions might limit applicability real-world scenarios. First all, assume form synchronization
among agents (e.g., synchronized selection behaviors, execution actions).
importantly, action failures assumed mutually independent. Furthermore,
particular case social diagnosis, agents cooperate exchanging
belief states, might critical issue keep information private. hand, framework proposed Roos Witteveen (2009),
diagnostic inferences substantially centralized.
paper aim relaxing assumptions extending relational-based
framework introduced Micalizio Torasso (2008, 2009). Similarly Roos Witteveen, adopt explicit representation MAP hand term agents actions
shared resources. differently them, action models include nominal well
faulty evolutions. argue rest paper, kind extended action
models subsumes action models proposed Roos Witteveen.
addition, aim fully distributed solution rely synchronized
execution actions (i.e., global clock available). distributed solution social
diagnosis also proposed Kalech, Kaminka, Meisels, Elmaliach (2006).
work, however, form synchronization among agents required agents select
next behavior simultaneously. Moreover, agents cooperate sharing
belief states. proposal, coordination among agents achieved means
exchange direct observations agents. idea observation
2

fiCooperative Monitoring Diagnose Multiagent Plans

acquired agent used agents reasoning. understand
difference exchanging belief states direct observations, note
belief state interpretation observations made specific agent according
local knowledge. Since agent might partial knowledge environment,
belief states could ambiguous even erroneous. Therefore, agents exchange
beliefs, may also propagate errors. Conversely,
coordination consists exchange direct observations, agents infer
beliefs without risk conditioned errors made others.
propose framework that, relying notion Partial-Order, Causal-Link
(POCL) Plans (see Cox, Durfee, & Bartold, 2005; Weld, 1994; Boutilier & Brafman, 2001),
limits number messages exchanged agents number causal links
existing actions assigned different agents.
proposal, communication plays central role assuring consistent
execution MAP, also easing diagnostic task. consider environment agents operate scarcely observable, agents directly acquire
information small portion surroundings. Dealing scarce observations challenging solving diagnostic task situation might
prevent detection action failures. cope issue propose paper
strategy named Cooperative Weak-Committed Monitoring (CWCM), extends
weak-committed monitoring introduced Micalizio Torasso (2009). CWCM allows
agents cooperate agent infer outcome
actions (i.e., failure detection) pieces information provided agents.
soon failures detected, must diagnosed order identify
root causes. paper, propose local diagnostic process agent
diagnose failure actions without need interacting agents.
particular, diagnostic inferences take account failures different actions
may dependent. words, action failure indirect consequence (i.e.,
secondary failure) failure preceding action (i.e., primary failure). Identifying
primary secondary failures essential point view plan repair primary
failures root causes anomalous observed execution. principle, plan repair
recovers primary failures also recover secondary failures. interesting property methodology process inferring primary secondary
failures performed autonomously agent, relying trajectory-sets built
cooperative monitoring.
1.1 Contributions
paper contributes diagnosis MAP execution many ways. First all,
paper shows extended action models proposed monitoring purpose
obtained compositionally nominal models actions, models
exogenous events affect actions. Thus, knowledge engineer take
advantage focusing models elementary components systems
(e.g., actions exogenous events), creating complex, extended, action models
composing elementary components.
3

fiMicalizio & Torasso

addition, proposed CWCM framework fully distributed: agent monitors
actions, central agent traces actions progress. Thus,
CWCM applied domains agents operate physically distributed
environments, hence centralized solution could impractical. Another important
feature CWCM asynchronous: neither assumption synchronized execution
actions, assumptions long actions last made. words, agents
share global clock. course, form synchronization still necessary
mutual exclusion required accessing critical resources. cases, however,
prefer use term coordination.
CWCM also represents valid solution whenever diagnosis MAP performed
environments characterized scarce observability levels. fact, significant contribution
CWCM cooperative monitoring protocol enables agents acquire information system resources other. paper argue number
messages exchanged via cooperative protocol linear number inter-agent
causal links (i.e., causal dependencies existing pair agents).
last important contribution paper ability distinguishing primary secondary failures. Previous approaches (see e.g., Micalizio & Torasso, 2008; Roos
& Witteveen, 2009), fact, assume action failures independent other.
1.2 Outline
paper organized follows. Section 2 introduce basic multiagent planning
framework use starting point work. Section 3 basic framework
extended relaxing important assumptions. Section 4 formally presents Cooperative Weak-Committed Monitoring (CWCM) strategy, local diagnostic inferences
discussed Section 5. paper closes detailed experimental analysis Section
6, critical discussion related works Section 7.
paper also includes Appendix briefly discuss CWCM
implemented means Ordered Binary Decision Diagrams (OBDDs) (Bryant, 1992,
1986), formally analyze computational complexity implementation.

2. Multiagent Planning: Basic Framework
section organized three parts. First, introduce basic notions Multiagent Planning terminology use throughout paper. Then, discuss
propositional planning language translated state-variable representation.
Finally, present basic strategy plan execution multiagent settings
highlight importance cooperation among agents even strong assumption exogenous event occurs. assumption relaxed Section 3.
2.1 Preliminary Concepts Multiagent Planning
Since interested diagnosing systems modeled multiagent plans,
begin discussion presenting framework developed within planning community
represent synthesize kind plans. worth noting planning problem
typically approached propositional terms: preconditions effects actions literals
4

fiCooperative Monitoring Diagnose Multiagent Plans

must true, respectively, application actions themselves.
Thus intuitively introduce section planning notions propositional terms,
however, Section 2.2, argue addressing problem plan execution,
convenient handle representation system terms state variables,
hence translate propositional framework state variables one.
important assumption holding throughout paper that, although observations gathered agents execution time partial, always correct;
elaborate point Section 3.1.
2.1.1 Multiagent Plans
Multiagent Plan (MAP) systems take account paper modeled
tuple S=hT , RES , P i, where:
team cooperating agents; agents denoted letters j;
RES set shared resources objects available environment;
assume resources reusable (i.e., consumable),
accessed mutual exclusion; note agents exchange pieces
information resources, space resource names common
language agents communicate;
P completely instantiated Partial-Order Causal-Link Plan (POCL) (Weld, 1994),
resulting planning phase ones Boutilier Brafman (2001)
Cox et al. (2005). sake simplicity, MAP P simplified structure
since involve concurrency non-concurrency constraints. precisely,
MAP P tuple hI, G, A, R, Ci where:
- set propositional atoms representing initial state system
planning time.
- G another set propositional atoms representing global goal achieved.
Note G conjunction sub-goals Gi agent charge of.
- set action instances agents execute; action
assigned specific agent . planning time, assume action
modeled terms preconditions pre(a) effects eff (a),
conjunctions grounded atoms (see PDDL level 1, Fox & Long, 2003).
rest paper, denote ail l-th action performed agent i.
- R set precedence links action instances; precedence link ha, a0
R means action a0 start completion action a.
- C set causal links form lk : ha, q, a0 i; link lk states action
provides action a0 service q, q atom occurring
effects preconditions a0 .
assume MAP P :
flaw-free: nominal execution P achieves G;
5

fiMicalizio & Torasso

safe respect use resources. Intuitively, say resource
res RES used safely iff execution step, res either assigned,
assigned exactly one agent. similar concurrency requirement (Roos
& Witteveen, 2009): two actions executed concurrently require
subset resources;
redundant actions: even though P necessarily optimal, contains
actions directly indirectly provide services help achievement goal.
means always exists chain causal links action
plan least one atom goal G.
guarantee resource safeness, introduce notion working sessions associated resources agents:
Definition 1 Let res resource RES , agent , working session
wshres,ii using res pair haio , aic actions Ai that:
aio precedes aic (i.e., aio aic , transitive closure precedence
relations R).
aio action Ai agent acquires res, modeled specifying
atom available(res) preconditions aio . Moreover, exists C
incoming causal link form hajk , available(res), aio i, ajk action assigned
agent j (possibly ajk a0 i.e., pseudo action whose effects determine
initial state system). Action aio opens working session. action
aih Ai aio aic (i.e., aio aih aic ), incoming causal link labeled
service available(res) coming action another agent j 6= i.
aic action Ai agent relinquishes resource res favor another
agent j 6= i. modeled means causal link haic , available(res), ajk C,
meaning action aic releases res one effects, available(res)
one preconditions ajk . Action aic closes working session. course,
agent release resource res one agent; i.e., outgoing link
haic , available(res), ajk unique. addition, action aih Ai aio aic ,
outgoing link labeled service available(res) directed towards action
another agent j.
action aih aio aic use res; i.e., res mentioned preconditions effects aih . precisely, causal link mentioning res two
actions aih aik Ai allowed aih aik belong working
session, namely, aio aih aik aic .
Given working session wshres,ii , denote opening closing actions
opening(wshres,ii ) closing(wshres,ii ), respectively. Two working sessions wshres,ii
ws0hres,ji consecutive, wshres,ii / ws0hres,ji , closing(wshres,ii ) provides opening(ws0hres,ji )
service available(res).
Proposition 1 MAP P satisfies resource safeness requirement resource
res RES, working sessions res totally ordered sequence
ws1hres,i1 / ws2hres,i2 . . . / wsnhres,in , agent ij (with j : 1..n).
6

fiCooperative Monitoring Diagnose Multiagent Plans

means that, independently agents accessing resource, two working sessions
resource res never overlap other. Possibly, agent
one session sequence, meaning agent acquires relinquishes resource
res many times along plan.
resource safeness requirement extension concurrency requirement introduced Roos Witteveen (2009). fact, concurrency requirement implicitly
imposes ordering two actions cannot performed simultaneously,
resource safeness imposes ordering blocks actions identified working session. necessary order model situations agent uses set resources
number consecutive actions cannot interleaved actions
agents. working session sort critical section cannot interrupted. worth
noting that, since working session associated single resource, since
constraint working sessions two resources, possible action
aic Ai closes two different working sessions. example, let wshres,ii ws0hres0 ,ii
two working sessions agent using resources res res0 , respectively; possible
action aic closing(wshres,ii ) closing(ws0hres0 ,ii ). addition, resource
res could released favor agent j, resource res0 could released favor
another agent k. modeled two causal links haic , available(res), ajx
haic , available(res0 ), aky i.
2.1.2 Local Plans
Since interested fully distributed framework plan execution plan
diagnosis, impose every agent knows portion P perform;
thus introduce notion local plan. Intuitively, local plan P projection P
actions assigned agent i; P therefore partitioned many local plans
agents . formally, given agent , local plan P assigned

, C i, represents portion
tuple P =hI , Gi , Ai , Ri , Clocal
, Cin

initial state known agent plan execution starts; Gi sub-goal assigned

agent i; Ai , Ri Clocal
meaning A, R C, respectively, restricted
, highlight causal

Cin
actions assigned agent i. remaining sets, Cout
dependencies existing actions agent actions assigned agents
maintains outgoing causal links modeling services agent provides
: Cout
maintains incoming causal links modeling services
agents with; whereas, Cin
agent receives agents. simplify plan structure, precedence links
actions different agents allowed R. This, however, real limitation
precedence links actions different agents could expressed causal links
exchanged service refers dummy resource.
rest paper consider local plan P partially ordered set
actions. However, assume agent perform one action per time,
rest paper index actions according execution step. words,
P executed sequence hai0 , ai1 , . . . , , ai i, ai0 ai two pseudoactions similar ones introduced Weld (1994). Action ai0 preconditions
effects coincide initial state known agent i; particular, pseudo-action
also used determine initial set resources assigned agent i: link leaving
7

fiMicalizio & Torasso

ai0 labeled service available(resk ) denotes resk assigned agent i.
Action ai , hand, effects preconditions correspond sub-goal
Gi assigned i.
2.2 Translating Propositional Framework State-Variable
Representation
Although planning approaches literature relies propositional language
represent planning problems, adopt paper representation based multivalued state variables similar SAS+ approach introduced Jonsson
Backstrom (1998). reason choice stems fact multi-valued
variable implicitly encode mutual exclusion constraints among values domain:
variable assume one value given time. Thus, easier represent
evolution system state time sequence assignments values state
variables. solution also effectively adopted diagnosis plans (Roos &
Witteveen, 2009), Helmert (2009) proven, restrictive since always
possible translate propositional representation set multi-valued state variables.
rest section briefly describe propositional planning language (see
e.g., Nebel, 2000) mapped state variables representation. Three main aspects
addressed: (1) represent agents states terms state variables rather
sets propositional fluents; (2) represent exchange services among agents;
(3) model actions terms state variables.
2.2.1 Mapping Atoms Variables
point view, action models system states represented terms
finite set multi-valued state variables, finite discrete domain.
Given MAP system S=hT , RES, P i, associate agents resources
RES set state variables; variables maps subset atoms
corresponding propositional representation.
follows current system state given values currently assigned
state variables agents resources. global view, however, inadequate
achieve fully distributed framework. thus introduce notion agent state,
captures portion system state relevant specific agent team.
agent associated set VARi variables. variable v VARi
finite discrete domain dom(v). state Sli agent given execution step l
therefore assignment values variables VARi . precisely, Sli (v) dom(v)
value assumed variable v VARi agent state Sli . partial agent state li
assignment values subset variables VARi .
Variables VARi partitioned two subsets: END ENV . Set END includes
endogenous state variables (e.g., agent position); whereas ENV includes variables
shared resources. partitioning global system state agent states,
state variables resources duplicated many copies agents
. Therefore, resource resk RES , exists private copy resik belonging
ENV . maintain consistency among private copies, rely two assumptions:
(1) MAP P flaw-free, (2) P satisfies resource safeness requirement. two
8

fiCooperative Monitoring Diagnose Multiagent Plans

assumptions induce variables ENV implicit constraint: execution step
one agent knows actual state given resource resk . consequence,
private copy resik keeps correct value. agents, resource resk
sight; modeled assigning value unknown local variables resk ;
namely, j 6= i, resjk =unknown. Thus consistency among different private
copies implicitly maintained.
course, agent j acquires resource resk agent i, also comes know
actual state resource. values variables ENV fact exchanged
among agents. Section 2.3 present basic cooperative protocol enables
agents share knowledge resources preserving resource safeness
requirement; basic protocol later extended Section 4.5. Variables END ,
hand, refer agent i, framework cannot shared
agents.
2.2.2 Mapping Services Variables Assignments
Since adopt representation based state variables, also service exchanged
two agents conveniently modeled value assignment resource variable. instance, causal link lk : hai , available(resk ), aj used propositional representation
model action ai Ai provides action aj Aj resource resk . link
rewritten state variables representation hai , resk = available, aj i, resk
name agents j refer specific resource RES . words,
resk sort meta-variable known agent agent j. course,
agent maps meta-variable resk private copy. precisely, means
link hai , resk = available, aj i, agent able communicate j change state
resource resk : agent j knows that, execution ai , private variable resjk
value available, meaning j use resk . worth noting available
special value, used agents exchanging resources. general, agents communicate
domain-dependent values state resource (e.g., position
block blocks world domain).
Relying state variables representation, identify set resources
available given agent l-th plan execution step availResi (l) = {resk
RES |Sli (resik ) = available}. next subsection focus coordination protocol
allows agents j exchange information shared resources.
2.2.3 Mapping Propositional Action Models Function-Like Models
Let Q subset state variables VARi , rest paper denote (Q)
space possible assignments variables Q, Q denote specific
assignment (Q); is, specific partial state agent i. rest paper
use premises(ali ) effects(ail ) denote subset status variables VARi
preconditions effects, respectively, action ail defined. Thus premises(ail )
represents specific assignment values space (premises(ail )). Note set
premises includes also services must provided agents,
therefore correspond incoming causal links action ail . Similarly, set effects
9

fiMicalizio & Torasso

includes also services agent provides agents with, encoded causal links
outgoing ail .
Given action instance ail Ai , deterministic (nominal) model ail mapping:
: premises(ail ) effects(ail )
fanom

l

premises(ail ) assignment variables premises(ail ) VAR representing
preconditions ail , effects(ail ) assignment variables effects(ail ) VARi
modeling effects action. also assume effects(ail ) premises(ail );
substantial limitation, however. variable v, principle would
mentioned effects action model, also mentioned premises
action allowing v assume possible value domain. reason
assumption clear Section 3.2 formalize notion exogenous events.
Since interested execution plans, reformulate applicability
action state (see Nebel, 2000) terms executability. Given agent state Sli ,
action instance ail Ai executable Sli (VARi ) iff Sli |= premises(ail ); indeed,
strong condition relaxed next section. Using terminology
Roos Witteveen (2009), say action ail executable Sli ail fully


enabled Sli . result execution ail enabled Sli new agent state Sl+1

(VAR ), called successor state:


(Sli \ effects(ail )) effects(ail ) Sli |= premises(ail )




Sli 6|= effects(ail ) 6|= ,

Sl+1
=




,
otherwise.

precisely, Sli \ effects(ail ) partial agent state obtained removing Sli
variables mentioned effects(ail ). partial state subsequently completed
. state
new assignments effects(ail ), yielding new (complete) agent state Sl+1
transformation, however, legal when: (1) action ail fully enabled Sli (i.e., Sli 6|=
Sli |= premises(ail )), (2) action effects consistent (i.e., effects(ail ) 6|= ).
Otherwise, new agent state undefined.

2.3 Plan Execution Nominal Conditions
actual execution MAP requires form coordination among agents.
decomposition global plan local plans, fact, allows agents execute actions fully distributed way without intervention global manager:
agents execute actions concurrently asynchronously (i.e., global clock required).
addition, differently previous approaches (see e.g., de Jonge et al., 2009; Micalizio
& Torasso, 2008), actions take single time slot completed execution globally synchronized, make assumption duration
action. Agent coordination therefore essential order violate precedence
causal constraints introduced planning phase. sake clarity, first
present basic coordination strategy assuming that:
10

fiCooperative Monitoring Diagnose Multiagent Plans




BaDE(P =hI , Gi , Ai , E , Clocal
, Cin
, Cout
i)
1. l 1
2. Sli
3. ail 6= ai
4.
Sli consume-inter-agent-messages(inbox)
5.
ail executable Sli
6.
execute ail

7.
Sl+1
fanom
(Sli )

l

8.
obsil+1 gather observations

9.
Sl+1
obsil+1 |=
10.
stop execution propagate failure
11.
end

12.
causal link lk Cout
lk : hail , v = d, ajm
13.
notify agent j achievement service v =
14.
v RES = available

15.
Sl+1
(v) unknown
16.
end
17.
end
18.
l l+1
19.
end
20. end

Figure 1: Basic Distributed Plan Execution (BaDE) strategy.
- action-based observability: even though agents complete view
environment, agent always able observe preconditions effects
actions performs. addition, observations assumed reliable correct.
denote obsil+1 set observations received agent l-th plus 1
execution step execution l-th action. observations obsil+1
thought sets value assignments variables effects(ail ). is,
variable v effects(ail ), assignment v = belongs obsil+1 ; dom(v);
- deterministic actions: actions never fail models precisely define
agent state changes;
- static environment: environment change consequence execution
agents actions.
three strong assumptions relaxed Section 3, present
complex coordination strategy guarantees consistent access resources even
actions fail.
2.3.1 Basic Coordination Protocol
coordination protocol adopt simple effective, exploits causal links
models, fact, exchange service/resource
P . outgoing link Cout
agent - service provider - another agent j - service client. order support
communication among agents, assume agent inbox, i.e., folder
messages coming agents stored. Whenever executes action
, sends message outgoing link ai notifying
ail outgoing links Cout
l

receiver needed service/resource available. Likewise, link Cin
11

fiMicalizio & Torasso

models exchange service/resource receiver another agent j
, waits
provider. Whenever execute action incoming links Cin
message links since action becomes executable required
services/resources provided. assumptions made MAP P ,
protocol guarantees resources always accessed consistently. fact, assumption
P satisfies resource safeness requirement, assures working sessions
resource totally serialized; particular, two working sessions wshres,ii /ws0hres,ji
serialized means causal link hclosing(wshres,ii ), res = available, opening(ws0hres,ji )i.
Therefore, wshres,ii closes, agent notifies agent j resource res available.
2.4 Basic Distributed Plan Execution (BaDE) Strategy
high-level plan execution strategy performed agent team outlined
Figure 1. strategy consists loop iterates long actions
P executed. first step loop acquire new information
agents shared resources. accomplish task, agent plays role client
coordination protocol gathers notification messages, any, sent
agents; notification messages (i.e., value assignments convey) asserted
within current agent state Sli updates local view system status
acquires (if available) new resources. step, next action ail selected:
ail executable yet, agent keeps waiting notification messages (i.e.,
services/resources still missing). Otherwise, agent executes ail real world,

. (Note
exploiting nominal model fanom
estimates successor state Sl+1

l

actual execution ail real world may take time necessarily
known advance.)
action ail completed, agent gathers observations efi .
fects action matches observations estimated successor state Sl+1
Since assuming actions cannot fail, discrepancy observations
estimations discovered. include control line 9 compatibility
extension described sections 3 4, actions may fail.
execution ail , agent plays role provider coordination protocol
propagates (positive) effects action ail towards agents sending notificai (see lines 13 16 algorithm
tion message outgoing link ail Cout
Figure 1). particular, case agent released resource (v RES), agent
sets private copy resource unknown, way resource becomes
unreachable mutual exclusive access guaranteed.
last remark basic strategy regards increment counter l. Note
l incremented action executed; thus l correspond metric
time, pointer next action performed. Adhering BaDE strategy,
agent conclude local plan within finite amount time.
Proposition 2 Let = hT , RES, P MAP system P flaw-free satisfies
resource safeness requirement. agents follow BaDE strategy Figure 1,
P successfully completed finite amount time.
12

fiCooperative Monitoring Diagnose Multiagent Plans

Proof: sufficient show least one action P always execution,
step, achievement goal. contradiction, let us assume goal
reached, action progress. Since assume failure
occur, situation happen agents deadlock, waiting services
one provide with. deadlock may arise either erroneous planning
phase, P flaw-free initially assumed, BaDE coordination
strategy erroneous. Let us show agents adopt BaDE strategy,
failure occurs, agents never wait indefinitely accessing resource. Since P satisfies
assumption resource safeness requirement, must hold wshres,ji / wshres,ii ; i.e.,
closing(wshres,ji ) provides opening(wshres,ii ) (i.e., ail ) service res = available. Agent
waits service res = available two situations: (1) agent j performed
action closing(wshres,ji ) yet (correct behavior), (2) agent j already performed action
closing(wshres,ji ), sent appropriate message i. second case contradicts hypotheses agents adopt BaDE coordination protocol. fact,
required protocol, whenever working session wshres,ji closes, agent j send
message next agent accessing resource.
Therefore, agents never deadlock, least one action always execution,
since P finite number actions, goal G must achieved within finite amount
time.

Example 1. conclude section briefly exemplifying concepts introduced
far. particular, present office-like domain used test scenario
experiments (Section 6), domain similar ones adopted Micalizio (2013)
Steinbauer Wotawa (2008). domain, robotic agents deliver parcels
desks number clerks. robot carry one two parcels depending whether
heavy light, respectively. Figure 2 shows office-like environment used
tests; includes: 9 desks, distributed 5 office-rooms, connected
means 8 doors. Moreover, two repositories contain 12 parcels delivered
(8 light 4 heavy). Parcels, repositories, doors, desks critical resources
used accessed one agent per time. domain also includes three
parking areas, locations agents positioned simultaneously
critical resources. (The term location used identify either parking areas
resources agent physically positioned; e.g., parcels locations.)
Agents perform following actions: move location another, load/unload
parcels within resources locations (i.e., parking areas), addition,
impose one parcel positioned desk, repositories
unlimited capacity. Finally, agents carry one heavy parcel two light parcels
location another.
Figure 3 shows simple example MAP office domain. team involves three
agents A1, A2, A3, whose plans given three columns, respectively.
bottom picture effects pseudoaction a0 represent initial states three
agents. top picture, premises pseudoaction represent desired
final state. objective MAP figure deliver parcel1 desk3 (i.e.,
agent unload parcel1 positioned desk3), bring parcel back
initial position repository repos1. Similarly, parcel2 first delivered desk6,
13

fiMicalizio & Torasso

Figure 2: office-like environment used experiments: five rooms R1-R5, two
repositories: repos1 repos2, eight doors, nine desks, three parking areas.

brought back repository repos2; parcel3, already delivered
desk3, delivered desk4. ease readability picture, show
inter-agent causal links. use two different graphical notations distinguish
causal links giving access resources (diamond-headed), causal links model
kinds services (black-circle-headed). instance, link actions a13
a24 diamond-headed, means action a13 provides a24 service desk3 = available
(i.e., a13 , agent A1 longer access desk3). three dashed rectangles
picture represent working sessions associated resource desk3, used
three agents different execution steps. (The working sessions resources
highlighted avoid picture becoming confused.)
Black-circle-headed links used represent services. instance,
1 encodes service desk3.content=empty,
link actions a12 a25 (labeled )
required action a25 since one parcel located desktop. link labeled
2 (from a25 a37 ) encodes two services: desk3.content=parcel1 parcel1.pos=desk3.

3 refer desk6 parcel2.
Similar services encoded link ,


3. Extending Framework
previous section described simple coordination strategy guarantees
consistent execution MAP P three strong assumptions hold: (1) agent
action-based observability: precisely observe preconditions effects
actions performs; (2) environment static (no exogenous events permitted);
(3) actions deterministic (no deviation nominal behavior possible).
Henceforth extend basic framework relaxing three assumptions and,
consequence, increasing complexity strategy controlling distributed
plan execution.
14

fiCooperative Monitoring Diagnose Multiagent Plans

parcel1.pos=repos1
parcel1.delivered=desk6
A3.pos=P1


parcel3.pos=desk4
parecel3.delivered=desk4
A1.pos=P2

parcel2.pos=repos2
parcel2.delivered=desk3
A2.pos=P3

a212

move(repos2, P3)

a211

a311

unload(repos2, parcel2)

a210

a17

move(repos1, P1)

repos2

a310

carry(desk6, repos2)

move(door2, P2)

a29

a16

unload(repos1, parcel1)
repos1

3


a39

load(desk6, parcel2)

move(desk4, door2)

carry(door1, repos1)

desk6

a28

a15

a38

move(door4, desk6)

unload(desk4, parcel3)

a27

a14

door4

move(P2, door4)

carry(door2, desk4)

1


a12

move(P2, desk3)

desk3

move(door4, desk3)

a35

unload(desk3, parcel1)

move(desk6. door4)

a24

a34

carry(door1, desk3)

unload(desk6, parcel2)

a23

a33

carry(repos1, door1)

A1.pos=P2
desk3=avail
desk4=avail
parcel3.pos=desk3
parcel3.delivered=desk3
door2=avail

load(desk3, parcel1)

a25

load(desk3, parcel3)
desk3

a37
a36

move(desk3, P2)

a11

parcel1

a26

a13
carry(desk3, door2)

carry(desk3, door1)

2


carry(repos2, desk6)

a22

a32

load(repos1, parcel1)

load(repos2, parcel2)

a21

a31

move(P1, repos1)

move(P3, repos2)

A2.pos=P1
repos1=avail
parcel1.pos=repos1
parcel1.delivered=no
door1=avail

A3.pos=P3
repos2=avail
parcel2.pos=repos2
parcel2.delivered=no
desk6=avail
door4=avail

a0

Figure 3: simple example MAP office-like domain used testing.

15

fiMicalizio & Torasso

3.1 Partial Observability
first assumption relax action-based observability. basic framework,
observations obsil+1 agent receives (l + 1)-th step execution cover
variables effects(ail ). extended framework, obsil+1 becomes partial since
subset variables effects(ail ) covered general, possibly obsil even empty.
Also case assume observations correct, meaning actual
state agent cannot inconsistent observations received agent itself.
However, observations ambiguous sense given variable, agent
receives disjunction possible values. addition, guarantee termination
plan execution phase, assume agent observes least achievement
atoms local goal Gi G.
3.2 Plan Threats
second extension dynamics system: move static system
dynamic one. means that, besides changes due agents actions,
system also change consequence exogenous, unpredictable events, typically represent plan threats (Birnbaum, Collins, Freed, & Krulwich, 1990) nominal
execution plan. Intuitively, plan threat seen abrupt change happened
environment (i.e., resources), state agent.
paper associate occurrence exogenous event execution
action. words, exogenous event occur execution
action affect active variables action; namely, variables
mentioned within premises effects action. Thus, exogenous event
cannot affect simultaneously two actions, indirect effects many
actions, even different agents, means shared resources.
principle, given exogenous event , one could define model predict
affect execution action. real-world domains, however, always possible
precisely know advance actual impact exogenous event: one hand,
may non-deterministic effects; hand, effects may
known. take account possibly non-deterministic effects exogenous events,
model exogenous event relation happens defined follows:
happens (affectedby ) {} (affectedby )

(1)

affectedby VARi , (affectedby ) space partial agent states defined
affectedby . worth noting (affectedby ) empty affectedby
empty, too. enables us state that, exogenous event occurs, variable
affectedby must necessarily evolve unexpectedly. Thus, tuple relation happens
represents non-deterministic effect ; namely, tuple represents possible abrupt
change agents status variables.
deal known effects exogenous event , extend domain dom(v)
variable v VARi special value unknown leaves open possible
evolution v.
denote X set exogenous events might occur plan
execution. Note X also includes pseudo event modeling absence abrupt
16

fiCooperative Monitoring Diagnose Multiagent Plans

changes. special event must hold affectedby = . Since exogenous
event defined state transition agents state variables, affect
action ail iff
affectedby effects(ail ).
(2)
Namely, affects subset variables action ail defined.
Given action ail , X (ail ) denotes subset exogenous events X satisfy
relation (2). Note that, pseudo event always included X (ail ), action
ail Ai , since affectedby (i.e., empty set) trivially satisfies relation (2).
3.3 Extended Action Models
last extension propose action models. Since plan threats occur
execution actions, effects combine actions effects. estimate
system evolves time, essential extend nominal action model order
encode, single piece knowledge, nominal well anomalous evolutions
action. Intuitively, extended model describe agent state Sli evolves

agent carried action instance ail , when,
new agent state Sl+1
execution action, exogenous event X occurred, possibly .
Moreover, basic framework give granted action performed
fully enabled. extended framework condition necessarily satisfied. Due
partial observability, fact, agent may unable precisely determine whether
next action fully enabled not. cope situation, introduce Section 3.4
concept possibly enabled action. time being, anticipate agent
may decide perform action even action fully enabled, hence
extended action model must rich estimate state agent evolves even
situation.
, given
extended model M(ail ) action ail derived nominal model fanom

l

terms premises effects, set exogenous events X (ail ); formally
defined as:
, X (ail ), (ail ), (ail )i,
M(ail ) : hfanom

l

X (ail ) already introduced; whereas (ail ) (ail ) two
fanom

l

transition relations partial states (premises(a)) (effects(a)),
possible predict execution action ail changes state
environment (i.e., resources held i) agent itself.
Relation (ail ) estimates next agents states action ail fully enabled
state Sli . relation results combination nominal action model
models exogenous events X (ail ):
(ail ) =

[

fi happens }
{fanom

l

X (ail )

(3)

Intuitively, fanom
fi happens set tuples form hpre, , eff i, pre equals

l

premises(ail ), eff (effects(ail )) models abrupt changes caused event
17

fiMicalizio & Torasso

nominal effects action ail . Formally, happening h, , 0 happens holds:


premises(ail ) {} {(effects(ail ) \ affectedby ) 0 }




[
Sli |= premises(ail ) premises(ail ) |= ;
fanom
fihappens =

l


h,,0 ihappens

otherwise.

(4)
important note that, since always part X (ail ), nominal model hpremises(ail ),
, effects(ail )i always included (ail ). particular, state transition, variable assume value unknown. follows directly by: (1) nominal model fanom

l

cannot mention unknown value definition, (2) exogenous event cannot
fi happens
affect variable since affectedby empty. Thus, operation fanom

l
reproduces nominal behavior.
addition, note X (ail ) also include special exogenous event ? . symbol
denotes indefinite exogenous event model given, hence variables
effects(ail ) mapped unknown: occurrence ? prediction possible.
Relation (ail ) structure (ail ) terms preconditions, effects
exogenous events, represents dual version (ail ) since defined ail
executable Sli . fact, (ail ) defined states action ail enabled.
Let (premises(ail )) space assignments values variables premises(ail ),
(ail ) defined space states (premises(ail ))=(premises(ail )) \ premises(ail )
as:
(ail ) (premises(ail )) {? } hunknown, . . . , unknowni,
(5)
? denotes indefinite exogenous event . Note (ail ) weaker model
(ail ) since invariably assigns unknown value variable effects(ail ).
say, whenever action performed wrong configuration, impact
effects(ail ) variables becomes unpredictable. Although use symbol ? denote
indefinite events occurring (ail ) , slightly different meanings
diagnostic point view discussed detail Section 5.
Remark 1. relational action models propose sufficiently flexible deal
incomplete imprecise knowledge. many cases, fact, may costly (or even
impossible) determine exogenous events impact variables effects(ali ).
extended framework copes problem allowing three forms incompleteness:
- unknown value included domain variable allows represent that,
effect exogenous event, value variable becomes predictable.
extreme case, variables effects action set unknown
(see weak model exogenous event ? ).
- Non-deterministic action evolutions defined: exogenous event may
non-deterministic effects states agents.
- weak relation allows us model status agent execution
action wrong conditions.
18

fiCooperative Monitoring Diagnose Multiagent Plans

Remark 2. Since actions performed even though fully enabled,
guarantee execution plan violate resource safeness
requirement? answer question coordination protocol part
Cooperative Weak-Committed Monitoring (CWCM) strategy discussed Section 4.
useful anticipate, however, coordination protocol guarantees agent
uses resource actions violate resource safeness requirement.
Example 2. Let us consider simple example office domain, assume
agent A1 charge performing action carry(A1, Parc2, desk1, desk2);
action requires A1 move current position desk1 position desk2
loaded parcel Parc2. nominal model action expressed
state transition:
hpos = desk1, cObj=Parc2, Parc2pos1=A1, , pos = desk2, cObj=Parc2, Parc2pos1=A1i;
pos cObj two endogenous variables A1 representing current position
agent carried object, respectively. state shared resource Parc2
encoded variable Parc2pos1, private variable agent A1 keeps maintain
position parcel Parc2. agents, local copy variable Parc2pos
unknown.
actual execution carry action affected number exogenous
events; instance, wheelsblocked prevents agent moving all, wrongstep
allows agent move, wrong direction. Another event affect
carry action lostparcel : agent moving, carried object(s) lost; finally,
? denotes unpredictable event occurring carry action attempted state
preconditions satisfied. alternative situations summarized
within extended model showed Table 1. first entry table nominal
state transition, one labeled . Entries 2 5 describe action
behaves known exogenous event occurs. Note that, although exogenous
event one foreseen possibilities, effects may precisely known;
instance, effect wrongstep lostparcel variables assume value
unknown. first five entries table represent relation extended model.
last entry table, instead, relation allows us make weak
predictions. tuple hpos=*, cObj=*, Parc2-place=*i shortcut represent
possible assignment preconditions satisfied. Note that, practical
point view, necessary compute (potentially huge) set explicitly,
discuss Appendix implementation.

3.4 Extending Basic Concepts
Since relaxed three assumptions basic framework, review
three important concepts: state agent, executability action,
outcome action.
19

fiMicalizio & Torasso

END

t1
t2
t3
t4
t5
t6

ENV

END

ENV

pos

cObj

Parc2pos1

event

pos

cObj

Parc2pos1

desk1
desk1
desk1
desk1
desk1
*

Parc2
Parc2
Parc2
Parc2
Parc2
*

A1
A1
A1
A1
A1
*



desk2
desk1
unknown
desk2
desk2
unknown

Parc2
Parc2
Parc2
empty
empty
unknown

A1
A1
A1
desk1
unknown
unknown

wheelblocked
wrongstep
lostparcel
lostparcel
?

Table 1: extended model action instance carry(A1, B2, desk1, desk2)
office domain.

3.4.1 Agents Belief States
First all, agent team must able deal form uncertainty.
Since actions may evolve non-deterministically since agent cannot observe
effects actions, agent must able deal belief states rather agent
states. Like agent state Sli , agent belief state Bli encodes knowledge agent
l-th execution step. Sli precise state assumed step
l, Bli set possible agent states consistent observations received i.
rest paper use lowercase indicate agent state among others within given
belief state, use uppercase indicate actual agent state given execution
step. important note that, exactly agent state Sli , belief state Bli defined
state variables agent i; two states s1 s2 Bli differ least one
variable. words, must exists least one variable v VARil s1 (v),
value assumed v s1 , different s2 (v). course, ambiguity represents
issue understanding whether next action ail executable.
3.4.2 Possibly Enabled Actions
Since agent belief state Bli , also notion action executability needs
revised. conservative policy would require action ail fully enabled every
state Bli ; due partial observability condition might satisfied,
execution MAP could stopped action enabled even though plan
threat occurred.
avoid situation, propose optimistic policy introduce notion
possibly enabled action.
Definition 2 Optimistic Policy Action ail possibly enabled Bli iff Bli
ail fully enabled s; namely, |= premises(ail ).
worth noting value unknown cannot used qualify action fully
enabled. value, fact, used explicitly state agent know
value variable. Therefore, variable v value unknown state s, v also
mentioned premises(ail ), ail fully enabled s.
possibly enabled action therefore sort conjecture: since action premises
satisfied least one state belief state, action assumed executable.
20

fiCooperative Monitoring Diagnose Multiagent Plans

course, may case s, although possible, real state agent,
hence action performed preconditions satisfied real world.
3.4.3 Action Outcome
basic framework given granted outcome action always
nominal. extended framework, however, actions fail. individuate three possible action outcomes: nominal ok, anomalous failed, pending intermediate
situations.
, |= effects(ai ).
Definition 3 Action ail outcome ok iff Bl+1
l

(estimated execution ail ).
is, actions effects hold every state Bl+1

Definition 3 hold exists least one state Bl+1
nominal
effects satisfied. previous approaches (Micalizio & Torasso, 2008, 2007a),
introduced adopted strong committed policy: effects action ail
, action outcome failed (see
satisfied state belief state Bl+1
Definition 3). strong committed policy based assumption that, whenever action
ail successfully completed, agent receives amount observations sufficient
detect success. Thereby, success cannot detected, failure must
occurred.
policy, however, may unacceptable real-world domains
guarantees system observability. consequence, agent could infer failure
even action ail completed success, observations sufficient
confirm it.
paper define failure action dual case success:
, 6|= effects(ai ).
Definition 4 Action ail outcome failed iff Bl+1
l

state expected effects ail
Namely, possible find Bl+1
achieved.
situations neither success (Definition 3) failure (Definition
4) inferred, action outcome pending.
, |= effects(ai ) s0 B ,
Definition 5 Action ail outcome pending iff Bl+1
l+1
l
0

6|= effects(al ).

words, whenever agent unable determine success failure
action ail , postpones evaluation action outcome step future;
action enqueued list pActsi pending actions maintained agent i. refer
policy weak committed since agent take decisions whenever
insufficient observations sufficient support them. next section discuss
impact weak committed policy monitoring task.

4. Cooperative Weak-Committed Monitoring
section discuss fully distributed approach problem monitoring
execution MAP. consider extended framework previously discussed introduces two sources ambiguity: agent belief states, ambiguous action outcomes.
21

fiMicalizio & Torasso

cope forms uncertainty, propose monitoring methodology called
Cooperative Weak-Committed Monitoring (CWCM), relies weak-committed
policy. CWCM approach allows agent detect outcome action
time execution. idea possibly uncertain knowledge agent
environment refined time exploiting observations
agent receive future. get result, CWCM allows team members
cooperate monitoring tasks.
rest section organized follows. first formalize notion trajectoryset maintained agent, explain extended action models used
extend trajectory-set one step further. discuss trajectory-set
refined observations helps determining outcomes
pending actions (if any). Finally, redefine cooperative protocol sketched basic
framework obtain cooperative monitoring protocol. CWCM entirely formalized
terms Relational Algebra operators. (For short introduction used operators, see
Micalizio, 2013.)
4.1 Trajectory-Set
weak-committed approach requires agent able reason past.
means agent cannot maintain last belief state, keep
trajectory-set; i.e., sequence belief states traces recent history agents
state.
define trajectory-set generalization agent trajectory. agent trajectory
agent i, denoted tr (1, l), defined segment [ai1 , . . . , ail1 ] local plan P ,
consists ordered sequence agent states interleaved exogenous events X
(including ). agent trajectory represents possible evolution status agent i,
consistent observations agent received far; formally:
Definition 6 agent trajectory tr (1, l) plan segment [ai1 , . . . , ail1 ]
tr (1, l)=hs1 , e1 , s2 , . . . , el1 , sl
where:
- sk (k : 1..l) state agent k-th step obsik sk 6|= .
- eh (h : 1..l 1) event X (ah ) labeling state transition sh sh+1 .
agent trajectory therefore sequence agent states, interleaved events, traces
agent behavior along given plan segment. sake discussion, consider
plan segment starting first performed action ai1 ; practice, however, plan
segment consideration intermediate portion agents local plan.
return point Section 4.6.
Since state sk (k [1..l]) complete assignment values agent state
variables VARi , variables duplicated many times actions
plan segment consideration; following, denote VARik copies
state variables referring k-th execution step.
noticed above, however, partial system observability general sufficient
estimation unique trajectory; reason agent keeps trajectory-set r [1..l],
22

fiCooperative Monitoring Diagnose Multiagent Plans

contains possible agent trajectories tr (1, l) consistent observations received execution plan segment [ai1 , . . . , ail1 ].
Note that, given trajectory-set r [1, l], agent belief state execution step k
[1..l] easily inferred projecting r [1..l] state variables VARik :
Bki = projectVARi (T r [1..l])
k

(6)

Thus definitions 2 (possibly enabled actions), 3 (successfully completed actions), 4 (failed
actions), 5 (pending action), based belief states, still meaningful
require redefined.
rest paper, term trajectory frontier (or simply frontier) refers
last belief state maintained within trajectory-set. instance, frontier r [1, l]
belief state Bli . general rule, use l denote index last execution step
(and hence frontier); k used refer generic execution step [1, l].
4.2 Extending Trajectory-Set
extension trajectory-set corresponds predictive step basic framework
next agent state estimated. However, basic framework
step easy mapping state another, need complex procedure
extended framework. Given current trajectory-set r [1, l] extended
model M(ail ), estimation step defined Relational terms follows:
r [1, l + 1] = r [1, l] M(ail ) = r [1, l] join ((ail ) (ail )).

(7)

new trajectory-set r [1, l + 1] built contribution
relations. relations fact used estimate execution action ail changes
state system. Relation applied portion Bli action ail fully
enabled. Whereas, relation applied states Bli action ail
enabled; i.e., occurrence exogenous event already assumed.
4.3 Refining Trajectory-Set Observations
basic framework assumed that, whenever action ail completed, agent
. extended framework,
receives observation obsil+1 new agent state Sl+1
agent also receive observation obsik referring past execution step k (i.e., 1 k l).
next section present cooperative monitoring protocol basis
message passing among agents. section discuss observation
past handled given agent i. Intuitively, consuming observation obsik means
selecting Bki states consistent it; Relational terms:
refined Bki = selectobsi Bki
k

(8)

result refined belief state less ambiguous original one number
states inconsistent observations pruned off.
important note unknown value consistent concrete observed
value. Therefore, state Bki , variable v unknown s, v mentioned
obsik , v assumes observed value obsik (v) refined Bki . Note allow
observed variable obsik assume value unknown.
23

fiMicalizio & Torasso

BlA1
s1l



s1l+1
wheelblocked
wrongstep

s2l

A1
Bl+1

s2l+1
s3l+1

lostparcel
lostparcel
?

s4l+1
s5l+1
s6l+1

Figure 4: one-step trajectory-set corresponding transition step l step l + 1.

Example 3. Let us consider office domain, assume l steps,
trajectory frontier agent A1 consists following belief state BlA1 :
s1l : h pos = desk1, cObj = Parc2, Parc2pos1= A1
s2l : h pos = unknown , cObj = Parc2, Parc2pos1= A1
Namely, BlA1 consists two alternative agent states s1l s2l . Let us assume
next l-th action performed A1 carry action, whose model previously
presented Table 1. According equation (7), easy see s1l matches
state transitions t1 t5 carry action model ( portion model), whereas
state s2l matches transition t6 ( portion model). Figure 4 gives idea
two relations used infer new frontier:
s1l+1 : h pos = desk2, cObj = Parc2, Parc2pos1 = A1
s2l+1 : h pos = desk1, cObj = Parc2, Parc2pos1 = A1
s3l+1 : h pos = unknown, cObj = Parc2, Parc2pos1= A1
s4l+1 : h pos = desk2, cObj = empty, Parc2pos1= desk1
s5l+1 : h pos = desk2, cObj = empty, Parc2pos1= unknown
s6l+1 : h pos = unknown , cObj =unknown, Parc2pos1=unknown
Now, let us assume agent A1 receives observation obsA1
l+1 = {hpos = desk2i},
used refine new frontier. easy see obsA1
l+1 consistent
states except s2l+1 , pos assigned different value. States s3l+1 s6l+1
consistent obsA1
l+1 unknown value consistent precise value.
new refined frontier therefore
s1l+1 : h pos = desk2, cObj = Parc2, Parc2pos1 = A1
s3l+1 : h pos = desk2 , cObj = Parc2, Parc2pos1 = A1
24

fiCooperative Monitoring Diagnose Multiagent Plans

s4l+1 : h pos = desk2, cObj = empty, Parc2pos1= desk1
s5l+1 : h pos = desk2, cObj = empty, Parc2pos1= unknown
s6l+1 : h pos = desk2 , cObj =unknown, Parc2pos1=unknown
seems s1l+1 s3l+1 identical, indeed consider single
belief states, trajectories; two states differ way achieved: s1l+1
inferred assuming everything goes smoothly; s3l+1 inferred assuming something
wrong occurred (i.e., wrongstep ). course, second hypothesis plausible
discuss next section pruned trajectory-set.

example shows consuming set observations obsik reduces ambiguity
within agent belief state Bki . addition, consumption messages also
beneficial effect reducing ambiguity trajectory-set r [1, l]. fact, refined
belief state turn used filter trajectory-set follows:
refined r [1, l] = selectrefinedBi r [1, l].
k

(9)

refined r [1, l] maintains trajectories k-th step
state refined Bki . important result since agent take advantage
observations whenever available, even though refer past execution step.
may happen, fact, even though obsik enough determine outcome
action aik1 , another belief state Bhi refined r [1, l] becomes sufficiently precise
determine outcome pending action aih1 . next section, exploit
characteristic determine outcomes pending actions.
4.4 Inferring Propagating Action Outcomes
Whenever current trajectory-set refined observations, useful scan
pending action list pActsi , assess, action aik pActsi , whether either Definition
3 4 applies.
outcome action important piece information exploit,
well observations, refine current trajectory-set. outcome action aik , either
positive negative, fact used infer outcome actions pActsi .
reach result exploit notions causal predecessors aik (predecessors(aik )),
causal successors aik (successors(aih )). First all, say action aih indirectly
provides action aik service, aik indirectly receives service aih , iff
exists sequence actions aiv1 , . . . , aivn that:
1. aiv1 coincides aih
2. aivn coincides aik

.
3. action aivx , x : 1..n 1, exists causal link haivx , q, aivx+1 Clocal

words, must exist chain causal links starts aih , passes
actions sequence aiv1 , . . . , aivn , ends aik . Indirect causal dependencies pass plans agents considered definition.
example, two causal links haih , q, ajv hajv , q 0 , aik i, cannot say aih
25

fiMicalizio & Torasso

indirectly provides aik service since action ajv belongs agent j. limitation, advantage otherwise agents interact heavily order compute
indirect causal relations. notion indirect dependency actions basis
locality principle allows agent consider portion local plan
monitoring diagnosis.
set predecessors (aik ) therefore subset Ai including actions
directly indirectly provide aik service. side, successors(aik )
subset Ai including actions which, directly indirectly, receive
service aik .

Given action aik , denote chains to(aik ) subset causal links Clocal
defined


actions predecessors(ak ). Similarly, denote chains f rom(ak ) subset

defined actions successors(aki ).
causal links Clocal
Proposition 3 Let aik action whose outcome ok, causal links
chains to(aik ) represent services satisfied.
fact, aik outcome ok, services required aik satisfied,
recursively, services required actions predecessors(aik ) satisfied too.
Proposition 4 (Backward Propagation Success) Let aik action whose outcome ok, let us mark satisfied causal links chains to(aik ), action
pActsi predecessors (aik ) outgoing links marked satisfied, outcome ok,
too.
Proposition 5 Let aik action whose outcome failed, services
chains f rom(aik ) might missing.
fact, since aik outcome f ailed, least one expected effects missing;
hand, action aik could reached subset effects, services could
sufficient enable subsequent actions. forward propagation failure must
therefore take account results successfully achieved. Let us denote miss(aik )
set causal links leaving aik representing missing services.
Proposition 6 (Forward Propagation Failure) Let aik action whose outcome
failed, let us mark missing causal link cl chains f rom(aik ) reachable
one links miss(aik ) via chain missing causal links, unless cl already
marked satisfied. Then, action pActsi successors(aik ), least one
outgoing link marked missing, outcome failed, too.
Intuitively, properties 4 6 assume action performed fully
enabled produce correct results. hand, action achieves
effects must performed fully enabled, hence services
mentioned premises must provided.
26

fiCooperative Monitoring Diagnose Multiagent Plans

a2

2

a3

1
...

7

a1

a6

8

4

a4

3

a5

a7

...

6

5

.
Figure 5: portion local plan restricted causal links Clocal

Example 4. example show outcome action actually exploited
determine outcomes actions. course, agent able determine
outcome relying observations messages agents. cooperative
protocol discussed details following subsection; time being, important
observe that:
positive messages (formalized confirm messages following) received
given step processed negative message (i.e., disconfirm message)
received step;
agent receiving least one negative message stop execution plan,
start diagnostic phase.

shown,
Let us consider plan segment Figure 5, links Clocal
let us assume agent performs actions order a1 , a2 , a3 , a4 , a5 , a6 ,
actions pending. execution a5 , agent discovers a5
outcome ok. outcome propagated backwards: predecessors(a5 ) = {a1 , a4 }, thereby
links chains to(a5 ) = {4, 5} marked satisfied; course, link 6 also marked
satisfied nominal outcome a5 . enables conclude action a4
outcome ok; whereas nothing concluded action a1 since links 1 7 neither
marked satisfied, missing. Let us assume receives observations
service link 1, consequence concludes a1 outcome failed.
case outcome propagated forwardly: successors(a1 ) = {a2 , a3 , a4 , a5 , a6 , a7 },
thereby chains f rom(a1 ) = {1, 2, 3, 4, 5, 6, 7, 8}; however, links 4, 5, 6 already
marked satisfied; addition, links 7 8 reachable via chain missing
causal links link 1; thus, links 2, 3 marked missing. Agent hence
concludes actions a2 a3 outcome f ailed. outcome inferred
action a6 , remains pending, outcome inferred action a7
performed yet. outcome pending action a6 inferred means diagnosis
inferences discussed Section 5.


Relying properties 4 6, determine outcome pending actions
exploiting causal dependencies existing among actions, even though
current trajectory-set still ambiguous apply either Definition 3 (outcome ok)
Definition 4 (outcome failed).
27

fiMicalizio & Torasso

Note discover action aih outcome ok, exogenous event occurred
action necessarily ; thus, also filter r [1, l] follows:
refined r [1, l] = selecteh = r [1, l]

(10)

eh refers h-th exogenous event labeling transition state sh state
sh+1 r [1, l]. refinement (10) keep r [1, l] trajectories
h-th exogenous event . Thus keep transitions obtained
relation , prune spurious trajectories contributed relation .
hand, outcome action aih f ailed, cannot refine
trajectory-set via eh since know eh cannot , already implicitly
obtained thanks refinement equation (9).
Summing up, weak-committed methodology able deal scarce observations using two essential mechanisms. First, build trajectory-set maintaining
history agent state, keep list pending action outcomes. Second,
take advantage observations whenever available revising knowledge
agent itself; favorable case, revision process empty set
pending actions.
4.5 Cooperative Monitoring Protocol
last element CWCM methodology cooperative monitoring protocol
allows agent exploit information provided others. idea agent
take advantage direct observations, also observations
agents environment, particular shared resources.
cooperative protocol plays central role preserving resource safeness requirement even actions fully enabled performed.
4.5.1 Interaction Scenarios
BaDE strategy, CWCM two agents, j, need interact
share causal link lk : hail , v = d, ajm v RES, dom(v), v =
value assignment representing change state resource requested agent
); whereas,
j. Contextually lk, agent plays service provider role (with lk Cout
j
agent j plays role service client (with lk Cin ). following first present
three interaction scenarios CWCM. shortly report messages
exchanged two agents. Then, present client provider roles detail
means high-level algorithms.
Notify-ready interaction interaction provider sure provided
client requested service. Thus, provider sends message habout lk notify
v = readyi client j; answer client provider required.
Notify-not-accomplished interaction scenario, agent sure requested
service missing; therefore sends agent j message habout lk notify v = notaccomplishedi client j; answer j foreseen.
Ask-if interaction case, provider unable determine whether
service v = achieved; thus asks j info sending j message
28

fiCooperative Monitoring Diagnose Multiagent Plans

cooperative-protocol::client(inbox, r [1, l], ail )
1. message m: h lk notify v = ready inbox s.t. lk incoming link ail
2.
remove inbox
3.
assert v = frontier r [1, l]
4. end
5. message m: h lk ask-if v = accomplished? inbox s.t. lk incoming link ail

6.
remove inbox
7.
unable observe v
8.
reply h lk no-info
9.
else
10.
obs observe v
11.
obs equals
12.
reply h lk confirm v =
13.
else obs equal
14.
reply h lk disconfirm v =
15.
end
16.
end
17. end
18. message m:habout lk notify v = not-accomplishedi inbox s.t. lk incoming message
ail
19.
remove inbox
20.
stop plan execution
21. end

Figure 6: pseudo-code cooperative protocol, client behavior.
habout lk ask-if v = accomplished?i. client reply message three
different ways:
1. habout lk confirm v = di, message confirms provider expected
service v = actually achieved;
2. habout lk disconfirm v = di expected service missing;
3. habout lk no-infoi client unable determine whether assignment
v = holds environment not.
case receives no-info message j, eventually reply either ready
message not-accomplished one.
4.5.2 Client Role
algorithm Figure 6 outlines behavior agent behaving client.
algorithm takes inputs inbox (i.e., collector messages coming agents),
current trajectory-set r [1, l], next action performed ail .
Agent consumes message inbox service required
premise execution ail . incoming message type ready (lines 1
4), agent uses information provided another agent observation, use
term assert (line 3) shortcut relational operations presented equations (8)
(9).
incoming message ask-if interaction (lines 5 17), agent determines whether capable observing v (e.g., equipped right sensor v?).
29

fiMicalizio & Torasso

case cannot observe v, replies provider no-info message. Otherwise,
agent acquires observation v, replies provider accordingly.
Finally, whenever agent receives not-accomplished message (lines 18 21),
stops execution plan service required performing ail missing.1
important note agent playing client consumes message
relevant next action performed. Thereby, ask-if message could
answered certain amount delay.
4.5.3 Provider Role
provider behavior outlined Figure 7. algorithm takes inputs inbox,
current trajectory-set r [1, l], list pending actions pActsi , last performed
action ail . precisely, last argument either null, action
performed recently, actual action instance whose outcome still assessed.
refine concept recently performed action next section present
main CWCM plan execution loop.
algorithm starts checking inbox order consume answers (if any)
previous ask-if interactions. algorithm specifies behavior agent according
type received message. case confirm messages (lines 1 5), agent uses
v = observation refine trajectory-set. term assert used
shortcut relational operations equations (8) (9); belief state
actually refined k-th +1; is, one contains effects action aik .
case disconfirm message (lines 6 10), agent prunes k-th +1 belief
state r [1, l] state v = holds. case incoming message
type no-info (line 11 18), agent checks whether outgoing links action

marked ans-no-info, meaning none services provided
aik Cout

ak agents achieved sure. case, agent marks aik
not-enough-info.
preliminary steps, agent possibly acquired information
others. Thus, assess outcome pending actions pActsi , including ail
null (line 19). algorithm Figure 8 outlines steps assessing outcomes
actions pActsi , discussed later on. sufficient say assess-pendingactions returns two lists actions, ok-list f ailed-list, empty, contain
actions whose outcome ok f ailed, respectively. course, whenever action pActsi
found either ok f ailed, removed pActsi , added corresponding
list. process also involves actions previously marked not-enough-info.
action ail null (an action performed recently), first time
outcome ail assessed. Thus, case ail outcome pending (line 20), agent
starts ask-if interaction (lines 21-24) asking information agents
requires one services produced ail . Otherwise, ail null ail outcome
pending, ask-if interaction skipped.
line 25 line 34, agent sends ready not-accomplished messages
according actions ok-list f ailed-list, respectively. addition, agent sends
1. impact action failure estimated means failure propagation mechanism (Micalizio
& Torasso, 2007b). sake discussion, leave topic paper.

30

fiCooperative Monitoring Diagnose Multiagent Plans

cooperative-protocol::provider(inbox, r [1, l], pActsi , ail )
1. message m:habout lk confirm v = di inbox
2.
remove inbox
3.
let lk haik , v = d, ajm
4.
assert v = k-th +1 belief state within r [1, l]
5. end
6. message m:habout lk disconfirm v = di inbox
7.
remove inbox
8.
let lk haik , v = d, ajm
9.
prune k-th +1 belief state within r [1, l] state v =
10. end
11. message m:habout lk no-infoi inbox
12.
remove inbox
13.
let lk haik , v = d, ajm
14.
mark lk ans-no-info
15.
links outgoing aik marked ans-no-info
16.
mark aik not-enough-info
17.
end
18. end
19. hok-list, f ailed-listi assess-pending-actions(pActsi , r [1, l])
20. ail null outcome pending

21.
link lk:hail , v = d, ajm i, lk Cout
(i 6= j)
22.
send j message m:h ask-if v = accomplished?i
23.
end
24. end
25. action aik ok-list
26.
link lk : haik , v = d, ajm
27.
send j message m:habout lk notify v = readyi
28.
end
29. end
30. action aik s.t. (aik f ailed-list) (aik pActsi marked not-enough-info)
31.
link lk : haik , v = d, ajm
32.
send j message m:habout lk notify v = not-accomplishedi
33.
end
34. end
35. return hok-list, f ailed-listi

Figure 7: pseudo-code cooperative protocol, provider behavior.

not-accomplished message pending action aik marked not-enough-info. pending
action marked not-enough-info highlights scarcely observable environment is.
fact, neither agent i, agents waiting services provided aik , capable
determine whether least one expected services provided not.
deal ambiguity, agent prudentially considers action probably failed.
Although choice could seem strong, necessary preserve resource safeness
requirement scarcely observable environments. Agent evidence supporting
successful achievement effects expected ail , hence cannot notify
success. time, agents might waiting services provided ail ,
thus agents would stalling without even knowing it. Considering ail failed allows
get impasse notifying failure agents, may attempt
form plan repair.
31

fiMicalizio & Torasso

assess-pending-actions(pActsi , r [1, l])
1. ok-list {}
2. f ailed-list {}
3. action aik pActsi

4.
Bk+1
, |= effects(aik )
5.
ok-list ok-list {aik }
6.
r [1, l] selectek = r [1, l]
7.
oks propagateSuccess(pActsi , aik )
8.
ok-list ok-list oks
9.
pActsi pActsi \ oks

10.
else Bk+1
, 6|= effects(aik )
11.
f ailed-list f ailed-list {aik }
12.
faultypropagateFailure(pActsi , aik )
13.
f ailed-list f ailed-list faulty
14.
pActsi pActsi \ faulty
15.
end
16. end
17. remove, present, mark not-enough-info action ok-list f ailed-list
18. return hok-list, f ailed-listi

Figure 8: pseudo-code assessment pending actions.
algorithm terminates returning two lists ok-list f ailed-list calling
algorithm, shown Figure 10 discussed next section.
4.5.4 Assessing Action Outcomes
presenting main CWCM algorithm, shortly present algorithm assessing
pending actions pActsi given execution step. discussed earlier, assessment
relies properties 4 6, equation (10). algorithm shown Figure 8;
takes inputs list pending actions pActsi , current trajectory-set r [1, l].
algorithm returns two lists, ok-list f ailed-list, actions whose outcomes either
ok f ailed, respectively.
algorithm considers actions pActsi (if any), tests whether
action outcome ok f ailed. first case, success backward propagated
(line 7): oks list successfully completed actions discovered means propagation; actions removed pActsi added ok-list. second case,
failure forward propagated (line 12): faulty list faulty actions discovered means
failure propagation; actions removed pActsi added f ailed-list.
algorithm terminates returning two, possibly empty, lists ok-list f ailed-list.
mentioned above, action aik pActsi , previously marked not-enough-info,
found definitive outcome (ok f ailed). may happen because, although
agents provided information effects aik , agent could
exploit outcome propagation actions preceding following aik . course, mark
not-enough-info removed actions ok-list f ailed-list.
Proposition 7 (Protocol Correctness - Resource Usage) cooperative monitoring protocol guarantees resource safeness requirement never violated
32

fiCooperative Monitoring Diagnose Multiagent Plans

execution MAP P . words, shared resources used correctly throughout
plan execution even action failures occur.
Proof: Let us consider interaction scenarios, show
resources accessed consistently; namely, never happens two (or more) agents
access resource simultaneously.
Given causal link lk : haik , res = available, ajm i, interaction activated agent
depends outcome action aik .
notify-ready interaction equivalent interaction BaDE framework,
occurs aik outcome ok. case expected services achieved
sure. Thus, notifies j res available, already released res:
resource passed j consistently.
ask-if scenario occurs aik outcome pending, splits three cases.
1. Agent j (i.e., client) directly observes resource available. therefore
access resource safely mutual exclusion.
2. Agent j directly observes resource still occupied i. case j
attempt access res. resource used single agent resource
safeness requirement violated.
3. Agent j unable say whether resource res available. point view
j, state res unknown, hence, since preconditions ajm
satisfied, j keeps waiting information i. Also case res used
one agent.
last interaction scenario occurs aik outcome f ailed. case, notifies
j resource available: j try use res preconditions ajm
satisfied.

Proposition 8 (Protocol Correctness - Provided Services) Let j two agents
playing roles provider client, respectively, given causal link lk : haik , v =
d, ajm i. cooperative monitoring protocol enables two agents determine actual
value variable v least determine whether v different expected value d.
Proof: proposition proved considering different interaction scenarios
protocol. notify-ready interaction occurs agent conclude action
aik outcome ok. (This may happen means direct observations effects
aik , means backward propagation nominal outcomes.) Since action aik
outcome ok, effects, including v = d, achieved. ask-if interaction
occurs agent cannot determine outcome aik , hence truth value
statement v = known. case agent j charge determining whether
statement v = true false. Agent j reach result means direct
observations v. possible answers j three:
j directly observes v = d, thus service provided;
j directly observes v d, thus service provided;
33

fiMicalizio & Torasso

e3 =


B1A1
r A1 [1, 5]



B2A1
s2

2
3

B4A1

obsA1
4
B5A1


B3A1



s4

6

s8

s5

7

s9

11

s6

8

s10

12

s14

s7

9

s11

13

s15

s12
10
s13

4

s1
1

s3
5

Figure 9: trajectory-set kept agent A1 execution first four actions.
j unable observe v, case agent relies answers provided agents,
any, asked link. Agent conclude v = true
least one received answers allows conclude nominal outcome aik ;
otherwise, action assumed failed, hence also service v = considered
missing.
last interaction scenario, agent directly observed, indirectly inferred means
forward failure propagation, v = false.

proposition considered sort generalization Proposition 7
applies possible services, services mentioning available value.
proposition important allows agents diagnose without
necessity interacting other, discuss Section 5.
Proposition 9 (Protocol Complexity) number messages exchanged among
agents linear number n inter-agent causal links.
Proof: provider-client interaction occurs agent, playing role
provider, performed action ail least one outgoing, inter-agent causal link.
number messages exchanged handling inter-agent causal link depends
outcome ail . ail either outcome ok f ailed, provider sends one message
client (i.e., ready not-accomplished, respectively). hand, ail
outcome pending, two agents exchange three messages: provider sends
ask-if, client answers no-info, provider either replies ready not-accomplished.
Thus worst scenario, number messages exchanged among agents 3n,
hence O(n).

Example 5. Let us assume execution first four actions, trajectoryset kept agent A1 one depicted Figure 9. trajectory-set contains 5 belief
states none sufficiently refined determine outcome action; thus,
actions currently pending. edges state another labeled
represent nominal progress plan execution; others instead, labeled
1 . . . 13 , model occurrence exogenous event (possibly ? ).
34

fiCooperative Monitoring Diagnose Multiagent Plans

show agent take advantage pieces information provided
others, let us assume agent A1 receives another agent observation obsA1
4
effects action a3 . instance, let us assume action a3 corresponds move
action, observation obsA1
4 refers position agent A1 execution
A1
a3 . Observation obs4 therefore used refine belief state B4A1 . example, s8
s9 states B4A1 consistent observation. Thanks
first refinement (see equations (8) (9)) able prune trajectories
pass either s8 s9; trajectories depicted dotted
edges. Dashed edges, hand, still possible trajectories still kept
within trajectory set.
However, refinement trajectory-set possible discover
refined B4A1 sufficiently precise determine action a3 outcome ok.
fact, A1s position conveyed observation obsA1
4 matches expected one;
means event e3 , affecting a3 , . pruning trajectory-set e3 =
(equation (10)), fortunate case B3A1 contains state s4 ,
nominal effects action a2 satisfied, too. backwards propagating success a3 ,
first a2 , a1 conclude three actions outcome ok.
fact, process, resulting trajectory-set maintains bold, solid edges;
whereas dashed edges pruned off. resulting trajectory-set, however,
allow us conclude anything a4 , still remains pending.

4.6 Cooperative Weak-Committed Monitoring: Main Algorithm
main CWCM algorithm outlined Figure 10. agent follows algorithm execute monitor local plan P .
initial steps set agent trajectory-set set pending
actions, algorithm iterates actions P far next action performed
coincides pseudo-action ai , meaning P completed. (Remind
assume actions P providing atoms premises(ai ) observable effects.)
beginning iteration, agent interacts agents (line 5) playing
client role cooperative protocol. step, agent consumes ready
not-accomplished messages (if any), acquires information resources required
perform next action ail . case agent receives not-accomplished message, stops
plan execution preconditions ail never satisfied. case
ask-if message received, agent establishes whether able observe required
service answers accordingly (see algorithms figures 6 7).
new information acquired asserted within agent trajectory-set,
agent assesses whether next action ail possibly enabled (Definition 2). positive
case, action performed real world (line 8). Subsequently, agent estimates
possible evolutions ail exploiting (ail ) (ail ) extend current
trajectory-set (line 9). completion action ail , agents direct observations
gathered obsil+1 (line 10) asserted extended trajectory-set (line 11); also
case assert shortcut relational operations described equations (8)
(9). Action ail temporarily put list pending actions (line 12).
outcome assessment fact postponed step regards current pending actions,
35

fiMicalizio & Torasso

Cooperative-Weak-Committed-Monitoring(P )
1. l 1
2. r [1, l] //The initial belief state initial state agent
3. pActsi
4. ail 6=
5.
cooperative-protocol::client(inbox, r [1, l], ail )
6.
last null
7.
ail possibly enabled frontier r [1, l]
8.
execute ail
9.
r [1, l + 1] r [1, l] M(ail )// trajectory-set extension using (ail ) (ail )
10.
obsil+1 gather direct observations
11.
assert obsil+1 frontier r [1, l + 1]
12.
pActsi pActsi {ail }
13.
last ail
14.
l l+1
15.
end
16.
hok-list, f ailed-listi cooperative-protocol::provider(inbox, r [1, l], pActsi , last)
17.
f ailed-list 6= aik pActsi marked not-enough-info
18.
stop execution
19.
diagnose(P , pActsi , ok-list, f ailed-list, r [1, l])
20.
switch safe mode
21.
end
22. end

Figure 10: Cooperative Weak-Committed Monitoring: high-level algorithm.
activated even action executed. important note
iteration loop necessarily corresponds execution action.
seen, provider behavior cooperative protocol needs know whether
action recently performed (i.e., whether action performed
current iteration). purpose use variable last, set null
beginning iteration, set action ail action actually performed
(line 13). Whenever action performed, counter l incremented (line 14);
is, l-th plan execution step completed.
loop proceeds agent behaving provider (line 16). step also
includes evaluation outcomes actions pActsi list (see algorithm
Figure 7). provider behavior returns two lists, ok-list f ailed-list, maintaining
actions outcome ok f ailed, respectively; course, lists could empty.
side effect, pActsi modified removing action whose outcome longer pending.
list f ailed-list empty, least one action pActsi marked notenough-info, agent stops plan execution, starts diagnostic process (discussed
Section 5), switches safe mode. agent safe mode perform actions,
interacts agents trying reduce impact failure. First all, agent
safe mode answers ask-if message no-info, prevents sender waiting
indefinitely answer. Moreover, agent safe mode releases many resources
possible sending appropriate ready messages; allows agents access
resources proceed plans. detailed discussion safe mode
scope paper, found works Micalizio Torasso (2007b)
Micalizio (2013).
36

fiCooperative Monitoring Diagnose Multiagent Plans

case failure discovered, actions performed far outcome
ok (i.e., pActsi gets empty), trajectory-set r simplified. fact, since
past actions nominal outcome, longer required keep whole past history
since beginning plan execution. Thus, safe convenient forget past
keep within trajectory-set frontier. implementation used
experiments adopts strategy keeping size trajectory-set manageable.
sake discussion, provide details point.
4.7 Cooperative Weak-Committed Monitoring: Correctness
conclude section discussing correctness algorithm Figure 10.
Theorem 1 [CWCM Correctness] CWCM assigns action aik outcome:
- ok iff action affected exogenous events;
- f ailed iff exogenous event, possibly ? , affected aik ;
- alternatively, CWCM marks pending action aik not-enough-info iff outcome
inferred relying observations agents, outcome propagation technique.
Proof: Part 1: action aik outcome ok iff aik affected exogenous events.
words, show aik reaches effects(aik ) iff ek = trajectory within
r [1, l], k : 1..l 1.
() contradiction, let us assume effects(aik ) reached, nominal
trajectory pruned r [1, l]. happen monitoring process
two ways: (a) observations, (b) outcome propagation. Let us
consider case (a), let us suppose monitoring phase agent receives observations obsik+1 consistent effects(aik ). effect pruning r [1, l] obsik+1 ,
nominal transition ek = pruned r 1 [1, l]; this, however, contradiction
definition extended model M(aik ), nominal transitions labeled
lead nominal effects(aik ). Thus, either obsik+1 inconsistent effects(aik ),
hence aik cannot ok, obsik+1 consistent effects(aik ) ek trajectories
within r [1, l].
Let us consider case (b), outcome propagation. two cases: backward
propagation ok, forward propagation f ailed. backward propagation ok
possibly assigns nominal outcome actions aik pActsi ; propagation, ek
equals trajectory within r , definition. forward propagation f ailed
possibly assigns nominal outcome actions aik pActsi ; propagation ek trajectory r [1, l]. two propagations cannot change
outcome action pActsi : action already assigned outcome, outcome cannot changed anymore. particular, aik outcome ok,
aih predecessors(aik ) discovered faulty, forward propagation f ailed cannot
prune ek = r [1, l]. fact, discussed Proposition 6, forward propagation
impacts causal links neither marked satisfied, missing,
along chain links starting one links miss(aih ). aik
assigned outcome ok, agent must received sufficient observations determine
premises aik satisfied. follows services required aik
37

fiMicalizio & Torasso

already marked satisfied. Thus, nominal transition ek = cannot lost
effect outcome propagation.
() aik affected exogenous event, hence eh = trajectory within
r 1 [1, l], aik outcome ok (i.e., reaches effects(aik )). construction, extended
model M(aik ) guarantees transitions labeled leads states
expected effects hold. follows that, ek = trajectories r [1, l], aik must
outcome ok necessarily.
Part 2: action aik outcome f ailed iff exogenous event, possibly ? , affected
execution. demonstrated following reasoning similar one Part 1;
omit brevity.
Part 3: action aik marked not-enough-info iff outcome inferred relying
observations agents, outcome propagation technique. easy see
CWCM marks aik not-enough-info one occasion: provider behavior,
answers gathered aik ask-if interaction no-info. exactly
means agent team provide information services provided
aik . hand, marking removed aik inserted either
ok-list f ailed-list; thus cannot happen action definite outcome
also marked not-enough-info.

Theorem 2 Given MAP system = hT , RES, P i, P plan hI, G, A, R, Ci,
global goal G achieved iff actions outcome ok.
Proof: previous theorem demonstrated action outcome ok
effects achieved, outcome cannot changed
effect refinements trajectory-set. Thereby, global goal G
reached, actions must reached effects, hence must outcome ok.
fact, since assume P redundant action (i.e., action P contributes
G), sufficient least one action fails reaching one effect have: 1) least
one action outcome f ailed, 2) least one piece G achieved.
hand, actions outcome ok, G must achieved
necessarily. absurd, actions ok, G reached.
happen P flaw, produce G even nominal conditions,
initial assumptions (see Section 2) P flaw-free actually produces G.

Example 5 used clarify proof. example shown that,
restrict B4A1 belief state state satisfies expected effects action
a3 , action a3 outcome ok. time, outcome backward propagated
edges labeled lead B4A1 . action a4 last action A1s
plan, effects action must observable, hypothesis. Now, depending
available observations, agent A1 either conclude s12 actual state a4
(thereby: (1) goal reached, (2) trajectory-set contains one trajectory
edge labeled , (3) actions outcome ok), s13
actual agents state, hence least one action (i.e., a4 itself) must outcome f ailed.
Corollary 1 global goal G achieved, agent keeps trajectory-set
r [1, l] nominal trajectory hs1 , , s2 , . . . , , sl+1 i, s1 |= sl+1 |= Gi .
38

fiCooperative Monitoring Diagnose Multiagent Plans

Proof: follows two previous theorems. G reached, actions
outcome ok (Theorem 2). hand, since aik ok iff ek =
every trajectory within r [1, l] (Theorem 1), follows agent keeps
trajectory-set nominal trajectory.

correctness monitoring process therefore summarized following
statement: execution P affected anomalous event, cooperative
monitoring able keep trace progress achievement goal G since
nominal transition never lost. hand, execution P affected
least one anomalous event, even known advance, cooperative monitoring
able detect stop execution phase. addition, Proposition 7 assures
nominal, well anomalous, situations resources always accessed consistently.

5. Plan Diagnosis: Local Strategy
Plan diagnostic inferences start soon CWCM algorithm discovered failure
least one action (i.e., f ailed-list empty), pending action marked notenough-info. section discuss mean plan diagnosis,
inferred. propose distributed approach agent infers diagnosis
local plan autonomously. fact, thanks Proposition 8, plan execution safe
respect use resources, agent never blame agents explain
action failures.
5.1 Inputs CWCM
previous section focused monitoring purpose CWCM methodology. important note, however, CWCM also produces useful pieces information
diagnostic point view. First all, actions f ailed-list could considered
plan diagnosis according definition Roos Witteveen (2009); namely, subset
actions assumed faulty explain observations. However, f ailed-list
take account action failures might indirect consequences
others. Thus, f ailed-list sufficient would like isolate primary action
failures caused secondary action failures.
addition, CWCM produces trajectory-set r [1, l], seen set
consistency-based diagnoses (Reiter, 1987). trajectory r [1, l] possible
explanation agents behavior consistent observations received agent
itself.
5.2 Event-Based Explanations
Dealing directly r [1, l], however, might awkward since encodes possible explanations, including ones mentioning indefinite exogenous event ? ,
considered unlikely. Moreover, trajectories share sequence
events, differ state variables, considered completely different explanations. Thus, r [1, l] needs processed order useful. first reduction
r [1, l] given projecting event variables e1 , . . . , el1 ; call resulting
39

fiMicalizio & Torasso

structure Event-based Explanations (EVE):
EVE = projecte1 ,...,el1 r [1, l].

(11)

EVE set sequences exogenous events (including ? ). sequence
set possible consistency-based diagnosis anomalous behavior agent.
Since EVE could still contain huge number diagnoses, EVE informative
human user decide recover plan failure. One way
reducing number diagnoses would prefer diagnoses involve
minimum number exogenous events. Unfortunately, preference criterion would lead
misleading results events dependent one another. find meaningful
explanations, one identify exogenous events caused primary action failures
exogenous events correspond secondary action failures.
5.3 Minimum Primary Action Failures
facilitate identification primary action failures, distinguish indefinite
events ? contributed portion action model, indefinite events ? contributed portion. distinction necessary CWCM, turns
useful diagnostic purpose. Intuitively, ? denotes occurrence exogenous
event affecting execution (possibly) enabled action; ? therefore unknown
abrupt change affecting nominal behavior action. hand, ?
indefinite event use label state transitions action performed
state satisfying preconditions. Relying distinction, possible
identify primary failure means following definition.
Definition 7 action ak pActsi f ailed-list primary action failure iff exists
explanation x EVE x[ek ] 6= x[ek ] 6= ? , x[ek ] k-th event
explanation x.
words, action ak considered primary failure given event-based
explanation x EVE iff occurrence exogenous event mentioned (ak )
assumed x. Note Definition 7 also examine set pending actions pActsi ,
including actions marked not-enough-info. addition, note set primary action
failures never empty. fact, agent starts diagnosis phase one
performed actions labeled failed. hand, agent stops
execution plan another agent fails providing service, first agent
exonerated diagnosing since none actions labeled failed,
root causes missing service located outside plan.
Secondary failures caused primary failure, defined follows:
Definition 8 Let x EVE possible explanation, let ak f ailed-list pActsi
primary failure x, actions ah successors(ak ) x[eh ] = ?
secondary failures caused ak according explanation x.
Note that, given primary failure ak explanation x EVE , actions
successors(ak ) necessarily secondary failures (see Proposition 4). fact, even though
ak achieved effects (i.e., outcome failed), action may reached
40

fiCooperative Monitoring Diagnose Multiagent Plans

them. consequence, actions successors(ak ) may enabled
despite failure ak . reason, Definition 8 require action ah
successors(ak ) labeled secondary failure exogenous event ? assumed
explanation x. definitions primary secondary failures proposition
follows directly.
Proposition 10 Given explanation x EVE , set primary action failures P rmx ,
set secondary action failures Sndx extracted x disjointed.
Relying proposition, define Primary Action Failure Diagnoses (PADs):
Definition 9 Let x EVE possible event-based explanation, primary action-failure
explanation (PAD) extracted x pair hP rmx , Sndx P rmx Sndx
sets primary secondary failures, respectively, extracted x.
course, since EVE general contains several explanations, since primary failures
assumed independent other, possible extract minimum cardinality
primary action-failure diagnoses (mPADs) simply selecting explanations
minimum set primary failures:
mP ADs = {P rmx x EVE |P rmx | minimum }

(12)

Minimum primary action failure diagnoses (mPADs) indeed mean plan
diagnosis: localize actions qualified failed order explain
anomalous observations.2
5.4 Refining Plan Diagnosis
inferred plan diagnosis, one refine diagnoses identifying root
causes. refined explanations expressed terms exogenous events,
extracted EVE set.
Definition 10 Let ah primary action failure, let EVE (ah ) set explanations x EVE ah P rmx , refined explanation action ah
refinedExp(ah ) =

[

x[eh ].

(13)

xEVE (ah )

words, refinedExp(ah ) consists exogenous events might
occurred execution action ah , hence might caused failure ah .
course, since ah primary failure, secondary failures caused ah also
explained occurrence one events refinedExp(ah ).
2. Note different preference criteria could adopted select explanations EVE . instance, one
could prefer minimality rather minimum cardinality.

41

fiMicalizio & Torasso

a1

a2

a4

a3

a5

a7

a6

a8

Figure 11: portion local plan assigned agent
Example 6. Let us consider simple local plan Figure 11 assigned agent i.

. Let us assume that,
simplify picture show local causal links Clocal
execution local plan, agent detects failure action a8 . diagnostic
process activated order explain failure identifying (minimum) set
primary action failures. diagnostic process receives input list failed actions
f ailed-list={a8 }, list successfully completed actions ok-list={a4 }, list
pending actions pActsi = {a1 , a2 , a3 , a5 , a6 , a7 }. addition, diagnostic process
receives also trajectory-set r [1, 9], simplicity show Table 2 set
event-based explanations (EVE ) extracted trajectory-set.
Table 2 easy see explanations, except last one, explain
failure action a8 indirect effect previous failure (i.e., a8 secondary
failure). last explanation considers a8 primary failure, unknown,
unlikely, exogenous event ? must assumed.
first step diagnostic process consists inferring set mP ADs diagnoses.
Thus, identify primary secondary failures explanation EVE :
P AD : { x1, x2 :
x3 :

h{a1 },

{a3 , a5 , a8 }i

h{a6 , a7 },

{a8 }i

x4 : h{a3 , a6 , a7 }, {a8 }i
x5 :

h{a2 },

{a7 , a8 }i

x6 :

h{a8 },

}

observe interesting consequences. First all, explanations EVE
collapsed within single explanation P ADs; see instance explanations x1 x2.
advantage reduce number alternative explanations. addition,
sets primary action failures used identify (subset-)minimal diagnoses.
instance, explanation {a6 , a7 } derived x3 minimal diagnosis, whereas explanation
{a3 , a6 , a7 } extracted x4 not. Finally, since assume primary failures
independent other, prefer subset-minimal diagnoses whose cardinality

x1
x2
x3
x4
x5
x6

a1
1
5





a2




4


a3
?
?

6



a4







a5
?
?





a6


2
2



a7


3
3
?


a8
?
?
?
?
?
?

Table 2: set EVE maintained within current trajectory-set
42

fiCooperative Monitoring Diagnose Multiagent Plans

minimal. example mP ADs = {{a1 }, {a2 }, {a8 }}. fact, thus sufficient
assume failure one actions explain observations.
step, action mP ADs, one also infer refined diagnosis.
instance, easy see primary action failure a1 two alternative refined
diagnoses: either 1 5 (see Table 2); whereas primary action failure a2 4
single possible refined diagnosis. Finally, one assume occurrence ? explain
primary action failure a8 . Relying refined diagnoses, preference criteria could
employed conclude primary failure a8 less likely a1 a2 ,
hence could disregarded.

Note that, since agent able diagnose plan autonomously, plan
diagnosis global level could inferred combining local solutions inferred
agent team, integration guaranteed globally consistent. fact,
thanks Proposition 8 agent never blame another agent failure one
actions.

6. Experimental Analysis
far addressed CWCM methodology diagnostic inferences
declarative manner means relations Relational operators relations.
Relations simple, yet powerful formalism represent nondeterministic action models
ambiguous belief states. addition, also used model complex
structures trajectory-set event-based explanations (EVE ).
comes actually implementing CWCM methodology, however, must
noticed computational complexity algorithm Figure 10 dominated
complexity (macro-)operator involved extension current trajectoryset. hand, diagnostic inferences based projection current
trajectory-set event variables (see equation 11). steps might computationally expensive, efficient implementation relations relational
operators therefore becomes essential. possible way cope issue translate
relations symbolic, hence compact, formalism, encode Relational operators operations selected symbolic formalism. Alternatively, may
possible exploit recent advancements Continuous Query Languages (CQLs) deal
data streams (see e.g., STREAM system Arasu, Babu, & Widom, 2006), implement CWCM relying primitives made available Data Stream Management
System hand.
paper, chosen method knowledge compilation, particular,
selected Ordered Binary Decision Diagram (OBDD) (Bryant, 1986, 1992) formalism encode relation Relational operators. choice justified two main
reasons: first, OBDDs nowadays well-known language made available many
mature libraries; second, theoretical results Darwiche Marquis (2002) suggest
OBDDs answer queries polynomial time provided sizes remain
tractable. in-depth description cooperative monitoring diagnosis
implemented via OBDDs reported Appendix.
rest section organized follows. First, Section 6.1, sketch
software architecture implementation; Section 6.2, present experimen43

fiMicalizio & Torasso

domain

initial
state

P

XML

XML

XML

DISPATCHER

A1

P A1

RA1

outcome assessment

CW CM

RAN

outcome assessment

...

CW CM

DIAGNOSIS

extend trj



P

r A1

DIAGNOSIS

extend trj

detected failure

r

detected failure

cooperative protocol messages

observations
observations A1

ANs next action

SIMULATOR

A1s next action

domain; initial state

XML

exogenous events

Figure 12: software architecture CWCM implementation used tests.

tal setting used carry tests, consisting simulated execution several
MAPs. Finally, discuss interesting results monitoring (Section 6.4),
diagnosis (Section 6.5).

6.1 Software Architecture Implementation
CWCM proposal implemented Java SDK 7 program. software
architecture shown Figure 12, highlighting main actors: Dispatcher, N
agents team (from agent A1 agent AN), Simulator. picture also shows
internal architecture agents. Solid edges modules represent data flows,
dashed edges represents instead control flows, whereas dotted edge CW CM
abstracts messages exchanged agents cooperative monitoring.
simulation MAP P starts submitting Dispatcher module three XML files
containing, respectively, system domain (i.e., agents objects defined
scenario hand), system initial state (e.g., initial positions agents, initial
states resources, etc.), MAP P performed. Dispatcher decomposes
P local plans agent receive portion P interest.
particular, P decomposed, Dispatcher activates agents,
implemented threads, passing initial states local plans.
44

fiCooperative Monitoring Diagnose Multiagent Plans

OBDDs made available JavaBDD library 3 , provides java, easyto-use interface Java BuDDy 4 , popular mature library manipulating
OBDDs written C.
Besides agents, Dispatcher activates also Simulator, implemented thread.
Differently agents, however, Simulator receives input plan,
initial state system. addition, Simulator reads fourth XML file
exogenous events injected plan execution. precisely,
file list agents actions, associated anomalous event
must occur execution action. course, subset actions
affected exogenous events mentioned file.
environment set-up, Dispatcher starts agents,
execute CWCM algorithm discussed Section 4. actual execution action
simulated Simulator: Whenever agent intends perform action,
sends message Simulator conveying action performed. Simulator
simulate action execution taking account possible exogenous events
injected. action associated observations, Simulator sends
corresponding agent appropriate message. worth noting also Simulator,
like agent, uses OBDDs estimate next state whole system according
actions currently progress. Differently agents, however,
Simulator always knows precise state agent resource system.
details use OBDDs handling relations given Appendix A.
discussed Section 4, whenever failure action detected agent i,
Diagnosis module agent activated. results diagnosis inferences,
discussed Section 5 saved report file Ri associated agent i.
experiments described following performed PC, Intel Core 2 Duo,
2.80 Ghz, 8 GB RAM equipped Windows 7 OS. test repeated ten times,
average values considered experimental analysis order absorb load
fluctuations CPU.
6.2 Experimental setting
domain used tests already introduced Example 1. actions
agent perform summarized Table 3 5 , reporting details
encoding action models OBDDs. precisely, # variables number state
variables OBDD defined; number includes one variable encoding
possibly anomalous event occurring action execution; remaining variables
used encode agent state transition step t, action starts, step t+1,
action ends. Columns #-nodes #-trans. report, respectively, number
nodes OBDD encoding portion action model, number state
transitions encoded . Columns #M-nodes #M-trans. refer whole extended
model M, including portion. domain, agent handles 36 variables
encode belief state environment.
3. http://javabdd.sourceforge.net/index.html
4. http://sourceforge.net/projects/buddy/
5. Examples test cases action models found
http://www.di.unito.it/micalizi/CWCM/index.html.

45

fiMicalizio & Torasso

move
carry
load
unload

# variables
15
19
17
17

#-nodes
193
346
386
386

#-trans.
34
38
40
40

#M-nodes
291
374
892
892

#M-trans.
420
1857
169
169

Table 3: details relational action models.
6.3 Objectives Experimental Analysis
least three main questions want get answered means
experiments. questions are:
CWCM scale well number agents team grows?
CWCM affected level system observability? extent?
cooperation among agents really useful monitoring purpose?
answer questions, carried tests varying three main characteristics:
team size, observability level, monitoring strategy.
6.3.1 Team Size
assess scalability CWCM, generated MAPs teams 3 8 agents.
Thus, 6 scenarios, them, synthesized 30 MAPs. main
characteristics MAPs reported Table 4. Note MAPs trivial
consist significant number actions subgoals achieved. term MAPspan refers number execution steps required complete plan
nominal conditions full observability. concurrency rate, computed number
actions divided MAP-span, indicates agents perform actions concurrently.
Finally, number inter-agent causal links shows often agents interact
achieve subgoals.
6.3.2 Observability Level
assess competence CWCM, MAPs performed different conditions
observability. particular, considered three degrees domain observability.
following, term FULL denotes complete observability effects actions
performed agents. level observability unrealistic practice,
represents benchmark compare performance CWCM observability
conditions. term HIGH denotes degree observability guarantees observe
effects 70% MAPs actions, randomly selected. Finally, term LOW denotes
degree observability 30% MAPs actions, randomly selected.
6.3.3 Monitoring Strategies
Finally, assess actual benefits achieved cooperation among agents
monitoring phase, considered three alternative monitoring strategies:
BaDE, already presented Section 2, simplest strategy, based strong
committed policy.
46

fiCooperative Monitoring Diagnose Multiagent Plans

scenario

#agents

#actions

#subgoals

MAP-span

concurrency
rate

SCN3

3

SCN4

4

SCN5

5

SCN6

6

SCN7

7

SCN8

8

67.67

47.00

30.78

14.04

10.20

9.94

69.00

52.00

27.90

5.42

3.15

2.93

91.60

77.80

34.40

7.37

3.97

3.10

128.90

73.60

27.00

40.27

4.63

8.10

180.40

73.90

30.90

36.84

18.35

5.27

156.40

48.80

20.50

6.13

7.63

2.70

2.2
2.5
2.7
4.8
5.9
7.2

#causal

#inter-agent

links

links

226.44

10.5

32.82

1.87

239.30

20.1

13.39

1.45

329.80

26.8

11.34

1.1

377.70

28

70.14

4.75

467.20

36.6

60.55

4.92

360.60

45.00

6.90

2.82

Table 4: Characteristics MAPs six scenarios nominal conditions (average
values confidence intervals).

WCM (Weak-Committed Monitoring) introduced Micalizio Torasso (2008,
2009) based weak-committed policy allows agents keep trajectorysets cope scarce observability. WCM, agent able keep pending
actions far actions provide services agents. Differently
CWCM, WCM agents cannot cooperate other; therefore,
outcome action cannot precisely determined, provides another agent
j (i.e., 6= j) service, assumed failed i, also stops execution
plan.
CWCM, discussed Section 4, extends weak-committed policy active
cooperation among agents.
6.3.4 Exogenous Events
Although exogenous events generated randomly, generation reflects
(expected) probability given exogenous event occur. instance, completely unexpected event, encoded ? , unlikely occur, hence frequency
experiments pretty low. Table 5 shows probability distribution used generate
exogenous events randomly.
6.4 Experimental Analysis: Monitoring
experimental analysis monitoring task subdivided two main parts.
first one, assess three strategies BaDE, WCM, CWCM, nominal
conditions; is, exogenous event occurs simulated execution
MAPs. goal study impact observability degree competence
47

fiMicalizio & Torasso

Exogenous event
blocked-wheel
wrong-move
lose-parcel
slip-parcel
blocked-arm
?

Probability
25 %
10 %
25 %
10 %
25 %
5%

Table 5: exogenous events frequencies experiments.
HIGH observability

WCM

CWCM

% achieved subgoals

% performed actions

HIGH observability
BaDE
100
80
60
40
20
0
3

4

5

6

7

8

BaDE

WCM

80
60
40
20
0
3

4

# agent

100
80
60
40
20
0
3

4

5

6

6

7

8

7

8

LOW observability
BaDE

CWCM

% achieved subgoals

% performed actions

WCM

5

# agents

LOW observability
BaDE

CWCM

100

7

8

WCM

CWCM

100
80
60
40
20
0
3

4

5

6

# agents

# agents

Figure 13: [Nominal Conditions] Competence:
achieved goals.

percentage performed actions

three strategies. second part, assess competence three
strategies exogenous events occur.
6.4.1 Nominal Conditions
Competence. competence estimated percentage actions performed
subgoals actually achieved agents. Since condition FULL observability
agents perform 100% actions achieve 100% subgoals
three strategies, Figure 13 report results HIGH LOW conditions.
expected, BaDE sensitive observability degree. hand, since
WCM CWCM keep trajectory-sets, tolerant partial observability,
generally behave much better BaDE. CWCM better WCM
cooperation agents allows compensate lack direct observations
messages coming others. discussed Section 4.5, however, may possible
even agents unable provide useful pieces information. Thus, also
CWCM strategy, agent decides stop execution plan when,
even asking agents observations, possible determine outcome
action. explained Section 4.5, case agent stops execution
plan marking actions not-enough-info. reason percentage
performed actions achieved goals 100% observability levels HIGH
48

fiCooperative Monitoring Diagnose Multiagent Plans

FULL observability
WCM

LOW observability

HIGH observability
BaDE

CWCM

WCM

CWCM

BaDE

300

300

250

250

250

200

200

200

150

msec

300

msec

msec

BaDE

150

100

100

50

50

50

0

0

4

5

6

# agents

7

8

CWCM

150

100

3

WCM

0
3

4

5

6

# agents

7

8

3

4

5

6

7

8

# agents

Figure 14: [Nominal Conditions] Monitoring time (average 95% confidence interval)
single execution step.

LOW. results obtained CWCM case remarkable: worst case, SCN5,
least 80% actions performed 70% subgoals achieved despite
30% actions observable.
Computational Time. Figure 14 shows average time (and 95% confidence interval) monitoring single step execution. Note BaDE strategy monitoring consists estimating next belief state; whereas, WCM CWCM
extend trajectory-sets. addition, CWCM also cooperate agents.
cooperation introduce costs consumption message another
agent corresponds operation OBDD encoding current trajectory-set. first
positive result emerging Figure 14 that, even worst scenario, CWCM takes
300 milliseconds monitoring execution action. allows us
conclude CWCM could employed effectively real-world domains agents
actions performed order seconds.
addition, easy see computational time strongly depends observability level. example, FULL observability, CWCM WCM behave similarly;
case, fact, CWCM agents need cooperate other, hence
two strategies almost same. However, observability decreases, CWCM
slightly expensive WCM BaDE. higher cost counterbalanced
competence CWCM, that, already noticed, outperforms competence
BaDE WCM.
charts Figure 14 also apparent strict dependency
number agents team computational time three strategies.
This, fact, consequence distributed approach agent maintains
point view environment, cooperation agents
based exchange messages belief states.
OBDD dimensions. relation time observability becomes clear
consider sizes OBDDs encoding trajectory-sets; see Figure 15, left.
brevity report average sizes OBDDs maintained three strategies
HIGH LOW observability conditions6 . easy see exists relation
computational time shown Figure 14 sizes OBDDs Figure
6. FULL observability case, OBDD sizes CWCM well 3000 nodes, average.
addition, CWCM WCM generate OBDDs similar sizes, expected.

49

fiMicalizio & Torasso

HIGH observability

HIGH observability
BaDE

WCM

CWCM

WCM

HIGH observability
WCM

CWCM

CWCM

12.00

12000
8000
4000
0
3

4

5

6

7

80
70
60
50
40
30
20
10
0

Length trajectory

# trajectories

# OBDD nodes

16000

3

8

4

5

LOW observability
WCM

7

8.00
6.00
4.00
2.00
0.00

8

3

4

5

# agents

# agents

BaDE

6

10.00

LOW observability

CWCM

WCM

6

7

8

7

8

#agents
LOW observability
WCM

CWCM

CWCM

12.00

12000
8000
4000
0
3

4

5

6

# agents

7

8

80
70
60
50
40
30
20
10
0

Length trajectory

# trajectories

# OBDD nodes

16000

3

4

5

6

# agents

7

8

10.00
8.00
6.00
4.00
2.00
0.00
3

4

5

6

#agents

Figure 15: [Nominal Conditions] Left: Sizes OBDDs number nodes (average
95% confidence interval); center: Average number trajectories within
trajectory-set; right: Average length one trajectory.

15, left: bigger OBDDs higher computational time. already
noted, although OBDDs may get large, computational time still acceptable.
(The biggest OBDD observed 17,707 nodes, built CWCM
SCN5 LOW observability.)
Obviously, level observability strong impact dimensions OBDDs. fact, reduced level observability makes trajectory-sets ambiguous,
hence trajectories encoded within single OBDD. made explicit Figure 15, center, show number trajectories encoded, average,
within trajectory-set time instant, length (Figure 15, right). course,
two last charts, consider WCM CWCM since BaDE strategy
build trajectory-sets. Moreover, note actual implementation CWCM,
extension trajectory-set cover whole plan performed far,
current subset pending actions.
CWCM Communication Analysis. conclude study nominal conditions
analysis communication required CWCM methodology. Figure 16
shows average number messages exchanged among agents. first interesting
result that, FULL conditions, number exchanged messages coincides
number inter-agent causal links. fact, since results taken nominal
conditions, action reaches nominal effects; therefore, cooperative protocol handles inter-agent causal link means simple ready message sent provider
client, answer required. observability level HIGH, however,
number messages tends increase, even though increase significantly except
scenario SCN8. expected, largest number messages exchanged
observation LOW, expected.
50

fiCooperative Monitoring Diagnose Multiagent Plans

CWCM messages nominal conditions
FULL

HIGH

LOW

70

# messages

60
50
40
30
20
10
0
3

4

5

6

7

8

# agents

Figure 16: number messages exchanged CWCM agents nominal conditions.
HIGH observability

CWCM

BaDE

100
80
60
40
20
0
4

5

6

7

8

60
40
20
0
3

4

40
20
0
6

7

8

# agents

% achieved subgoals

% achieved subgoals

60

5

6

7

8

WCM

60
40
20
0
3

4

BaDE

60
40
20
0
5

6

# agents

6

7

8

7

8

LOW observability

CWCM

80

4

5

# agents

100

3

CWCM

80

HIGH observability
BaDE

CWCM

80

4

5

WCM

100

# agents

100

3

BaDE

80

FULL observability
WCM

LOW observability

CWCM

100

# agents

BaDE

WCM

7

8

% achieved subgoals

3

% performed actions

% performed actions

WCM

% performed actions

FULL observability
BaDE

WCM

CWCM

100
80
60
40
20
0
3

4

5

6

# agents

Figure 17: [Faulty Conditions] Competence: percentages performed actions achieved
goals.
6.4.2 Faulty Conditions
Competence. Let us consider test set before, randomly inject
single exogenous event MAP. goal assess well three strategies
behave partial observability exogenous events combine together. Figure 17 shows
competence three strategies faulty setting three observability
levels. environment fully observable, three strategies behave exactly
same, expected. course, percentages performed actions achieved goals
depend early, late, exogenous event occurs MAP. general,
say least 70% actions performed despite injected exogenous event.
similar consideration made percentage achieved goals.
observability conditions degrade HIGH LOW, however, easy
see CWCM outperforms two strategies. means CWCM actually
tolerant strategies partial observability even faulty scenario.
51

fiMicalizio & Torasso

FULL observability
WCM

BaDE

CWCM

WCM

LOW observability

CWCM

BaDE

300

300

250

250

250

200

200

200

150
100

150
100

50

50

0

0

3

4

5

6

7

msec

300

msec

msec

BaDE

HIGH observability

8

WCM

CWCM

150
100
50
0

3

4

# agents

5

6

7

8

3

4

# agents

5

6

7

8

# agents

Figure 18: [Faulty Conditions] Monitoring time (average 95% confidence interval)
single execution step.
HIGH observability

HIGH obervability

CWCM

WCM

12000
10000
8000
6000
4000
2000

50
40
30
20
10
0

0
3

4

5

6

7

3

8

4

5

WCM

7

8

WCM

CWCM

14000

# trajectories

12000
10000
8000
6000
4000
2000
5

6

# agents

6
4
2
0
3

4

5

7

8

WCM

50
40
30
20
10
3

4

5

6

# agents

6

7

8

7

8

LOW observability

CWCM

0

0
4

8

# agents

60

3

10

LOW observability

LOW observability
BaDE

6

CWCM

12

# agents

# agents

# OBDD nodes

WCM

60

# trajectories

# OBDD nodes

14000

HIGH observability

CWCM

Length trajectory

WCM

7

8

length trajectory

BaDE

CWCM

12
10
8
6
4
2
0
3

4

5

6

# agents

Figure 19: [Faulty Conditions] Average sizes OBDDs encoding trajectory-set (left),
average number trajectories within trajectory-set (center), average length
trajectory-set (right).
particular, since difference CWCM WCM cooperative monitoring, conclude cooperation among agents actually beneficial.
Computational time. Figure 18 reports computational cost, milliseconds,
three strategies faulty conditions three levels system observability.
important note also case computational time strongly depends
observability level; whereas depend number agents team,
presence exogenous event. fact, time monitoring MAP affected
exogenous event order magnitude monitoring MAP
nominal conditions. differences observed comparing charts Figure
14 charts Figure 18 due fact execution MAP affected
fault terminates earlier MAP executed nominal conditions, independently
level observability.
course, BaDE strategy cheapest three, unable monitor
effectively execution MAP. fact, strong committed policy basis
52

fiCooperative Monitoring Diagnose Multiagent Plans

CWCM messages faulty conditions
FULL

HIGH

LOW

60

# messages

50
40
30
20
10
0
3

4

5

6

7

8

#agents

Figure 20: number messages exchanged CWCM agents faulty conditions.
strategy sensitive level observability, HIGH LOW conditions
performs poorly.
OBDD dimensions. Let us consider dimensions OBDDs maintained
three strategies. left-hand side Figure 19, report sizes, number
nodes, OBDDs representing current belief state (BaDE strategy), current
trajectory-set (WCM CWCM strategies) three conditions observability.
expected, BaDE keeps smallest OBDDs since maintains last belief state,
makes BaDE strategy unable deal low observability levels. WCM
CWCM behave similarly FULL observability conditions, CWCM tends
maintain bigger OBDDs observability level decreases. result explained
fact CWCM build longer trajectory-sets WCM (Figure 19, right),
longer trajectory-sets tend ambiguous demonstrated average
number trajectories within trajectory-set (Figure 19, right).
CWCM Communication Analysis. Figure 20 shows number messages exchanged
CWCM agents faulty conditions. trend similar nominal conditions, however, number messages slightly lower. happens occurrence failure prevents agents performing actions, consequence
messages exchanged. also reason slightly less messages SCN6 SCN5. fact, number inter-agent causal links two
scenarios almost same, faults SCN6 stronger impact SCN5,
evident looking number performed actions SCN5 SCN6 (see Figure 17).
6.5 Experimental Analysis: Diagnosis
Competence. competence diagnostic inferences evaluated percentage
cases action affected injected exogenous event included
within set preferred explanations mP ADs. Figure 21 (left-hand side) shows
diagnostic inferences behave three levels observability. Obviously, FULL
observability, diagnostic inferences always identify correct primary action failure.
HIGH LOW observability, however, impaired agent stop plan execution due lack observations (i.e., not-enough-info). cases diagnosis cannot
identify primary failure. Figure 21 (right-hand side) shows also average distance
(i.e., number actions), action affected exogenous event, action
53

fiMicalizio & Torasso

FULL

HIGH

LOW

FULL

HIGH

LOW

8

80

#actions

% diagnosed cases

100

60
40

6
4
2

20
0

0
3

4

5

6

7

8

3

4

5

#agents

6

7

8

#agents

Figure 21: [Diagnosis] Competence (left), responsiveness (right).


xplanations

EVE: inferred explanations
FULL

HIGH

LOW

FULL

# explanantions

# explanations

50
40
30
20
10
0
3

4

5

6

7

8

LOW

4
3
2
1
0
3

# agents

HIGH

5

4

5

6

7

8

# agents

Figure 22: [Diagnosis] EVE explanations (left), mPADs explanations (right).
failure actually detected. FULL observability, diagnosis highly
responsive detects action failure soon exogenous event occurs (i.e.,
distance zero). hand, observability partial, CWCM agent
take longer detect failure.
Explanations Preferred Explanations. Section 5, pointed that,
given trajectory-set, one identify two types explanations: EVE mP ADs.
set EVE represents explanations consistent observations received
agent; whereas mP ADs set primary action failures inferred EVE . Figure 22 shows cardinalities (on average) two sets inferred six scenarios
different levels observability. two charts Figure 22 draw two conclusions. First, cardinality EVE strongly depends observability level; namely,
reduction observability level causes increment number possible explanations. However, cardinality mP ADs almost independent observability
level. fact, number preferred explanations inferred LOW observability similar number preferred explanations inferred FULL observability six
scenarios. course, mP ADs sets computed LOW observability tend slightly
bigger mP ADs sets computed FULL observability, consequence
fact initial EVE set ambiguous LOW observability.
means that, regardless initial ambiguity EVE sets, preferred explanations
reduced almost subsets six scenarios.
second important conclusion mP ADs explanations substantially
useful identifying fault EVE explanations. fact, average cardinality
mP ADs sets three worst cases LOW observability; whereas, average
54

fiCooperative Monitoring Diagnose Multiagent Plans

Diagnosis: Computational Time
FULL

HIGH

LOW

1000

msec

800
600
400
200
0
3

4

5

6

7

8

# agents

Figure 23: Diagnosis: Computational Time.

number EVE explanations best case FULL observability eight,
rises 38 worst case LOW observability (see SCN3). means
mP ADs explanations may actually help human user refine her/his hypotheses
current situation system. essential consider diagnosis
first step recovery (Micalizio, 2013). Thus, human user, possibly automatic
supervisor, consider small number alternative explanations, hence better
focus plan recovery process fault(s) believed plausible.
Computational Effort. Finally, consider computational time required infer
diagnoses. inferring EVE explanations, computational cost mainly due
cost removing non-relevant variables trajectory-set provided CWCM (see
Appendix discussion theoretical point view cost variable
removal). Figure 23 reports average computational time, milliseconds, extracting
EVE explanations six scenarios three different levels observability.
noticed previous section, LOW observability, trajectory-set tends
bigger two observability levels. consequence time
inferring diagnoses LOW conditions tends higher; however, time
1 second even worst case. side, HIGH observability conditions,
worst time 150 milliseconds (see scenario SCN3). worst time falls
50 milliseconds consider FULL observability level. computational times
allow us conclude diagnostic task, monitoring one, performed
on-line number applicative domains actions performed order
seconds, even minutes.
6.6 Discussion
beginning experimental analysis posed three questions,
position answering them. First all, experimental results show CWCM
sensitive number agents team. consequence partitioning
global plan local plans. way, fact, agent keeps point
view states shared resources; namely, agent local belief state
depend number agents team. seen, consistency
among local beliefs guaranteed exchange messages whose number
linear number inter-agent causal links MAP consideration.
55

fiMicalizio & Torasso

hand, level observability system strong impact
computational effort CWCM ambiguity trajectory-sets computed.
lower observability, higher computational cost bigger
trajectory-sets. important result emerges analysis worst-case
scenarios depicted Appendix rare, never occurred experiments. Indeed, compact encoding trajectory-sets action models obtained via
OBDDs facilitates efficient implementation CWCM takes, average,
hundreds milliseconds monitor single action. allows us conclude CWCM
successfully employed on-line monitoring many real-world domains.
level observability also impact diagnostic inferences. fact,
number EVE explanations significantly grows observability level decreases.
However, number preferred mP ADs explanations strongly influenced
observability level.
Finally, direct comparison CWCM WCM demonstrates cooperation among agents essential tolerant scarce observations.
cooperation, fact, means agent CWCM keep longer
trajectory-sets WCM. longer trajectory-sets give agent chances
collect pieces information successful completion pending actions.

7. Related Works
consider four main families model-based approaches diagnosis dynamic
systems close MAPs:
Discrete-Event Systems (DESs);
Relation-oriented;
Team-oriented;
Resource-oriented.
rest section briefly review main approaches within families,
highlighting differences similarities CWCM methodology proposed here.
7.1 Discrete-Event Systems
Since seminal work Sampath, Sengupta, Lafortune, Sinnamohideen, Teneketzis
(1995) Diagnoser, huge number works addressed diagnosis dynamic
systems modeling systems DESs. Diagnoser approach compiles
diagnostic model (i.e., Diagnoser itself) whole system off-line, approaches
(see e.g., Lamperti & Zanella, 2002; Cordier & Grastien, 2007) compute possible system
behaviors, check behaviors correct. Grastien, Haslum, Thiebaux
(2012) extends DESs conflict-based approach initially proposed Reiter (1987)
static systems.
best knowledge, DES framework gets closer one
presented Grastien, Anbulagan, Rintanen, Kelareva (2007). framework,
diagnosis label either normal faulty, associated system trajectory;
trajectory sequence system states interleaved events, thus similar
56

fiCooperative Monitoring Diagnose Multiagent Plans

trajectories kept within trajectory-sets. trajectory normal contain
fault events; trajectory faulty, otherwise.
Grastien et al. propose reduce diagnosis problem SAT one. idea formulate SAT problem, order answer question observed behavior compatible
faults occurring?. course, answer yes = 0, system
assumed nominal exists least one normal path consistent observations.
principle, proposed system description could encode MAP: execution actions
could modeled subset observable events; whereas exogenous events could
mapped unobservable events directly. However, DES framework cannot directly
applied domains CWCM deal with. First all, DES approach
next state whole system inferred taking account synchronous occurrence
set events. Thus, agents event generators, follows perform
actions synchronously, CWCM restriction imposed. Moreover, SATbased methodology centralized trajectories whole system states, whereas
CWCM enables agent build local trajectory-sets distributed way.
7.2 Relation-Oriented Approaches
Relation-oriented approaches proposed Micalizio Torasso (2008, 2009).
define works relation-oriented since action models expressed terms relations
agents state variables. advantage kind model possibility
representing single piece knowledge nominal abnormal evolutions
action. CWCM methodology falls within category, extends previous
works two ways. First all, CWCM able deal completely unexpected events,
denoted ? , model exists. Indeed, occurrence ? execution
action maps variables effects(a) unknown value; meaning
variables longer predictable.
second important extension protocol allows agents cooperate
monitoring task. experimental results demonstrated,
cooperation among agents essential cope scarce observations. means
cooperation, fact, agent acquire new pieces information would
acquire directly. pieces therefore used refine trajectory-set,
possibly outcome pending actions could determined.
7.3 Teamwork-Oriented Approaches
Rather diagnosing action failures, CWCM, teamwork-oriented approaches
focused diagnosing teamwork failures; i.e., coordination failures. type failures
necessarily due erroneous actions, wrong decisions taken agents.
detection teamwork failures addressed seminal works Tambe
(1998) Kaminka Tambe (2000). Kalech Kaminka (2003) later focused
diagnosis coordination failures, introduced notion social
diagnosis. specifically, team cooperating agents represented abstract
terms means hierarchy behaviors. behavior abstraction concrete
actions agent actually takes real world. Indeed, behaviors abstract
single actions, possibly sequences actions. Thus, differently relational-
57

fiMicalizio & Torasso

resource-oriented approaches (see later), explicit model agents plans missing
teamwork-oriented solutions.
social diagnosis framework assumes agents synchronize jointly
select team behavior. disagreement arises least two agents select two behaviors
incompatible other. disagreements represent instances social
diagnosis problems. course, agents select behaviors according beliefs,
thus social diagnosis disagreement set conflicting belief states held subset
agents. Kalech Kaminka (2005, 2007, 2011) propose different methods inferring
social diagnosis. solutions, however, rely assumptions may limit
applicability real-world scenarios. First all, assumed agents team
share hierarchy behaviors belief states agents homogeneous
(i.e., defined set propositional atoms). Moreover, agents must willing
exchange beliefs. CWCM methodology propose, however,
suffer limitations. CWCM, fact, makes assumption
agents internal beliefs. addition, communication among agents exchange
agents internal beliefs, observations shared resources agents directly gather,
guarantees agents high degree privacy.
7.4 Resource-Oriented Approaches
approaches within resource-oriented family mainly proposed Roos
Witteveen. call approaches resource-oriented because, point view,
system diagnosed plan, state system given states
system resources. execution action change state(s) one
resource(s). approaches deserve particular attention similarities
CWCM methodology, also number relevant differences.
Witteveen, Roos, van der Krogt, de Weerdt (2005) present basic framework
use extend subsequent works. framework, actions modeled
atomic plan steps; precisely, action models functions deterministically map
resource states input resource states output. models therefore represent
changes normally caused actions successfully performed.
faulty behavior actions, conversely, modeled via weak abnormal function,
maps state resource input unknown value. means that,
action fails, states resources handled action become unpredictable.
diagnostic problem arises observations received execution step
inconsistent nominal predictions made action models. means
least one actions performed far behaved abnormally. Witteveen et al.
(2005) introduce notion plan diagnosis subset plan actions that, qualified
abnormal, make observations consistent predictions made assuming
actions nominal.
course, since may exist many possible plan diagnoses, important look
diagnoses preferable others. Roos Witteveen (2009) propose different preference criterion based predictive power plan diagnosis has.
therefore introduce notion maximally-informative plan diagnosis (maxi-diagnosis)
set plan diagnoses predict correctly biggest subset observations. no58

fiCooperative Monitoring Diagnose Multiagent Plans

tion diagnosis subsequently refined notion minimal maximally-informative
plan diagnosis (mini-maxi-diagnosis), subset maxi-diagnosis
number failed actions assumed minimal.
work de Jonge et al. (2009), basic framework extended: agents
seen resources, action models also includes variables agents equipment
environment events (i.e., exogenous events). extension allows distinction
primary secondary diagnoses. primary diagnosis plan diagnosis (i.e.,
expressed terms failed actions), secondary diagnosis thought second
level diagnosis tries explain given action failure occurred.
CWCM tries resolve problem one addressed Roos Witteveen
(2009): Diagnosing execution MAP. However, action models significantly different two approaches. Roos Witteveens point view, action models
deterministic functions nominal behavior only. Whereas CWCM, model actions
relations easily accommodate nominal faulty evolutions. particular,
faulty evolutions nondeterministic, partially specified support
unknown value indicate expectations possible given variable.
Another important difference two approaches execution
actions. Roos Witteveen assume actions take one time instant performed
action execution proceeds synchronously agents. CWCM
realistic since action execution asynchronous: even though actions modeled
terms preconditions effects, actual duration necessarily one time instant.
seen, fact, agents cooperate exploiting causal
precedence links explicitly defined within plan model. plan model adopted
Roos Witteveen, instead, mentions explicitly precedence links only,
include causal links.
Also process diagnosis inferred presents substantial differences.
Witteveen et al. (2005) de Jonge et al. (2009) present centralized method carry
diagnostic inferences. distributed procedure qualifying actions abnormal proposed
Roos Witteveen (2009), also case detection diagnostic problem
made centralized way. Moreover, methodology proposed Roos Witteveen
sort strong committed approach, sense whenever observations,
system infer diagnosis. hand, CWCM methodology fully
distributed detection diagnostic problem (i.e., monitoring),
solution. addition, CWCM inherently weak committed: observations necessarily
trigger diagnostic process, diagnosis inferences start interpretation
observations either lead (1) determining action failure, (2) determining
service produced favor another agents action actually missing. CWCM achieves
second point exploiting direct observations gathered agent, messages
coming agents. means observations sufficient either
reach condition (1) (2), diagnosis inferred.
said above, de Jonge et al. (2009) introduce distinction primary
secondary diagnosis. distinction also found methodology. primary
diagnosis de Jonge et al. corresponds minimum primary action failures (mPADs),
identify actions assumed faulty order make plan execution
consistent observations. secondary diagnosis, hand, corresponds
59

fiMicalizio & Torasso

refined explanations (refinedExp), associate action mPADs
set exogenous events that, consistently observations, might occurred
hence caused action failure.
paper also assess impact primary action failure mP ADs
inferring set secondary action failures; namely, subset actions fail
indirect consequence failure a. Although identification secondary failures
would possible, de Jonge et al. take account problem.
conclusion, CWCM framework considered extension frameworks de Jonge et al. (2009) Roos Witteveen (2009). fact, action models
proposed Roos Witteveen reproduced within framework including
relation-based model two entries: one deterministic nominal evolution
action, one abnormal behavior agent variables become unknown consequence unpredictable event. action models could used
CWCM usual infer plan diagnosis fully distributed way.

8. Conclusion
Plan diagnosis essential step implement robust multiagent planning systems.
shown works (Mi & Scacchi, 1993; Gupta et al., 2012; Micalizio, 2013), fact,
explanations provided plan diagnosis steer repair procedure make repair
process effective.
paper addressed problem plan diagnosis splitting two
subproblems: detection action failures, actual explanation detected
action failures terms exogenous events might occurred. detection action failures achieved means Cooperative Weak-Committed Monitoring (CWCM)
strategy, allows agents cooperate monitoring task. Cooperation among agents plays central role detection action failures,
also explanations. CWCM methodology, fact, allows agent build
structure (i.e., trajectory-set), internal representation world
point view agent itself. Relying structure, agent infer explanations
action failures without need interacting agents.
proposed framework diagnosis MAPs extends previous approaches
literature. First all, CWCM fully distributed asynchronous. Previous approaches
(see e.g., Kalech & Kaminka, 2011; Roos & Witteveen, 2009; Micalizio & Torasso, 2008),
instead, based synchronous step (e.g., agents execute actions synchronously).
framework agent perform next action soon actions preconditions
satisfied. verify condition, impose agents adhere coordination
protocol guarantees consistent access shared resources.
addition, propose extension relational language modeling nondeterministic actions (Micalizio & Torasso, 2008). previous approach, fact,
assume know advance exogenous events affect given action;
paper able deal partial knowledge exogenous events. particular, allow specify subset effects exogenous event action
(i.e., agents variables might become unknown event), also allow
specify action might affected indefinite event whose effects completely
60

fiCooperative Monitoring Diagnose Multiagent Plans

unpredictable (i.e., agents variables become unknown due event). kind
extended action model subsumes action models proposed Roos Witteveen,
consists two parts: nominal action model, abnormal model maps
agents variable unknown value.
Cooperation among agents nondeterministic action models make CWCM particularly apt deal dynamic partially observable environments. one side,
nondeterministic action models discussed capture unexpected changes
environment. side, cooperative monitoring allows agent acquire
information environment agents. important note that,
differently works agents exchange internal belief states
(see e.g., Kalech & Kaminka, 2011), CWCM agent needs communicate
observes. enables agents keep private internal beliefs; addition, agents
could adopt specific policies deciding observations forwarded
agents. Forwarding observations agents, single agent
current proposal, might help agents discover earlier outcomes
pending actions; leave opportunity future research.
must also noted CWCM assumes observations correct:
actual state agent must pruned agent belief state due erroneous
observation. assumption often made also many model-based approaches diagnosis (see e.g., Birnbaum et al., 1990; Brusoni, Console, Terenziani, & Theseider Dupre,
1998; Pencole & Cordier, 2005; Roos & Witteveen, 2009; Eiter, Erdem, Faber, & Senko,
2007, mention few). Correctness observations, however, implies
observations must precise. CWCM fact consume ambiguous messages given
disjunction values variable (i.e., var = v1 var = v2 . . . var = vn ),
negation specific value (i.e., var 6= v). point view CWCM,
consuming observations simply corresponds selection states within belief
state observations refer to. Although aspect emphasized paper,
ability dealing ambiguous observations enriches communicative capacities
agents. instance, ask-if interaction, client, rather answering
generic no-info, could give provider disjunction possible resource states among
which, however, client incapable discriminate actual one. set alternative
states is, point view provider, much informative no-info,
possibly could lead provider determine actual state resource hand.
point view diagnostic inference, shown possible
explain action failures extracting explanations trajectory-sets built CWCM.
particular, pointed assuming action failures independent
might lead spurious diagnoses. reason proposed methodology
identifying primary action failures secondary action failures, indirect
consequence primary ones. simple preference criterion, based minimality
primary action failures, proposed prefer alternative explanations.
deep experimental analysis shown cooperative monitoring diagnosis practically feasible. efficient implementation based OBDDs discussed
Appendix together computational analysis theoretical point view.
experiments highlighted CWCM scales well number agents,
affected level observability environment: trajectory-sets tend big61

fiMicalizio & Torasso

ger environment less observable. However, experiments demonstrate
cooperation effective even dealing scarcely observable environments. Competence rates noncooperative solutions, fact, comparable CWCM
environment fully observable; situations, instead, CWCM always
exhibits highest competence.
proposed framework extended different ways. mentioned above,
far adopted careful approach communication restricting agents talk
exchanged services. However, agents might willing
communicate pieces knowledge acquired. interesting possible
extension improve cooperative protocol along direction. intuition, fact,
agent acquires information, could infer outcome
pending actions earlier now. course, communication must
become bottleneck, agents able identify piece information worth
forwarded agents, avoid broadcasting every observation agents.
important extension aim at, however, relax assumption
communication among agents always reliable. Removing assumption
many consequences. First all, cooperative monitoring protocol extended
order deal messages lost. Moreover, Proposition 7, safe use
resources, might longer guaranteed CWCM; thus resources could accessed
inconsistently. diagnose situations could take point view similar Kalech
Kaminkas social diagnosis. fact, erroneous access resources, could considered
coordination failures. would impact diagnostic inferences longer
local, distributed. is, monitoring task, also diagnosis
performed means cooperation number agents.

Acknowledgments
authors wish thank anonymous reviewers insightful comments,
substantially contributed final shape work.

Appendix A. Implementation Computational Analysis
Appendix first recall basic OBDD operators complexities,
study computational cost expensive relational operations involved
CWCM diagnostic methodologies discussed above.
A.1 OBDD Operators Complexities
computational analysis discuss next subsection relies results presented
Bryant (1986, 1992). works, author discusses efficient implementation
OBDDs operators corresponding computational complexities. results
summarized Table 6, f , f1 , f2 denote Boolean functions, encoded
reduced function graphs G, G1 , G2 , respectively. size graph G corresponds
number vertices, represented |G|. primitive OBDD operators
reported upper side Table 6:
62

fiCooperative Monitoring Diagnose Multiagent Plans

- reduce builds canonical form Boolean function f ; i.e., given specific variables
ordering, reduce operator gets graph G whose size minimal.
- apply implements binary logical operations two Boolean functions f1 f2 ;
operator works graphs G1 G2 encoding two functions, respectively;
op binary logical operator (, , , ). computational complexity
worst case product sizes (i.e., number vertices) two
graphs.
- restrict substitutes constant b variable xi time almost linear number
vertices within graph G.
- rename renames set variables ~x new one ~x0 ; complexity exponential
number renamed variables.
- equiv checks equivalence two Boolean functions f1 f2 ; since operator
scans two corresponding graphs simultaneously, computational complexity
linear sizes.
lower side Table 6 report computational cost time space relational operators join, intersect, union project obtained combining
primitive OBDD operators. Observe that, among relational operators, projection
expensive; fact, exponential number (binary) variables
removed (see e.g., Torta & Torasso, 2007; Torasso & Torta, 2006 details).
A.2 CWCM: Computational Analysis
analyze computational complexity CWCM, consider high-level algorithm
presented Figure 10, focus computational cost performing single iteration
loop action ail actually performed real-world.
situation three main steps hide potentially expensive operations relations:
extension current trajectory-set (line 9);
refinement trajectory-set available observations (line 11);
detection outcomes pending actions (line 19, Figure 7).
rest section analyze computational effort steps.
operator
reduce(f )
apply(op, f1 , f2 )
restrict(xi , b, f )
rename (f , ~x, ~x0 )
equiv(f1 , f2 )

time
O(|G| log |G|)
O(|G1 | |G2 |)
O(|G|)
O(|G| 2|~x| )
O(max(|G1 |, |G2 |))

size
|G|
|G1 | |G2 |
|G|
|G| 2|~x|
N/A

join (f1 , f2 ); union(f1 , f2 );
intersect(f1 , f2 ) (i.e., select)
project(f , {x1 , . . . , xn }, {y1 , . . . , ym })

O(|G1 | |G2 |)
O(|G1 | |G2 |)
O((2(nm) |G|)2 )

|G1 | |G2 |
|G1 | |G2 |
2(nm) |G|

Table 6: OBDD operators complexity.
63

fiMicalizio & Torasso

A.2.1 Extending Trajectory-Set
According equation (7), operator , r [1, l] yields new trajectory-set
r [i, l + 1], involves two join operations: one r [1, l] (ail ), one
r [1, l] (ail ). results two operations subsequently merged new
trajectory-set r [1, l + 1] via union operation. understand computational cost
relational operations, necessary map OBDD operators. already
shown previous works (see e.g., Torta & Torasso, 2007; Micalizio, 2013), natural
join mapped two Boolean functions (and hence two
OBDDs), whereas union two relations becomes Boolean or. Let Gl , Gl+1 , G ,
G OBDDs corresponding relations r [1, l], r [1, l + 1], (ail ),
(ail ), respectively; operator mapped following expression terms
OBDDs operations:
Gl+1 = apply(, apply(, Gl , G ), apply(, Gl , G ))

(14)

Given operator complexities Table 6, computational effort infer new
trajectory-set worst case:
O(|Gl+1 |) = O(|Gl | |G |) O(|Gl | |G |)
= O(|Gl |2 |G | |G |).

(15)

A.2.2 Refinement Observations
new trajectory-set inferred, refined observations obsik
received agent. sake exposition, equations (8) (9) defined
refinement trajectory-set intersection trajectory-set
belief state Bki refined obsik . Note extraction belief state
expensive operation, thus try avoid operation whenever possible. particular
case, since agent variables V ARki included within current trajectory-set,
refinement operation carried intersection r [1, l + 1] obsik ;
terms OBDD operators:
Gref .l+1 = apply(, Gl+1 , Gobsi )
k

(16)

Gobsi OBDD encoding obsik , Gref .l+1 OBDD corresponding
k
refined trajectory-set refinedT r [1, l + 1]. follows computational cost
operation
(17)
O(|Gref .l+1 |) = O(|Gl+1 | |Gobsi |)
k

A.2.3 Detecting Pending Actions Outcomes
last step CWCM algorithm consider assessment outcome every
action currently within list pending actions pActsi . Section 4, noted
verify success given action aik pActsi , sufficient check whether nominal

effects aik satisfied every state Bk+1
(Definition 3). case condition
hold, one verify whether expected nominal effects aik missing
64

fiCooperative Monitoring Diagnose Multiagent Plans


state Bk+1
(Definition 4). checks result negative answer (i.e., belief state

Bk+1 still ambiguous), action aik remains pending.

explicitly checking conditions might parExtracting belief state Bk+1
ticularly expensive, especially trajectory-set grows time. extraction
belief state project operation, fact, would require elimination
r [1, l + 1] variables interested in; thus, would remove variables
steps 1, 2, . . . , k, k + 2, . . . , l + 1, l |VARi | variables. Table 6 shows,
complexity project exponential number variables removed, could
easily become bottleneck.
cope problem, implemented checking ok f ailed outcomes
different way. particular, Definition 3 directly follows:


.
join effects(aik ) equals Bk+1
Proposition 11 aik outcome ok iff Bk+1

Proof: proof straightforward: definition aik outcome ok iff nominal effects



satisfied every state Bk+1
; join Bk+1
effects(aik ) yields Bk+1


nominal effects already included every state Bk+1 , hence action
outcome ok.

Since Bk+1 included refinedT r [1, +l + 1], proposition extended
whole trajectory-set:
Proposition 12 aik outcome ok iff
refined r [1, l + 1] join effects(aik )equalsrefined r [1, l + 1].
say, refinement trajectory-set observations, action aik
outcome ok iff nominal effects filter trajectory refinedT r [1, l + 1].
Relying proposition, verify whether aik outcome ok two steps: first,
build temporary OBDD maintaining result join r [1, l + 1]
effects(aik ), check whether temporary OBDD equivalent original
trajectory-set; terms OBDD operators:
outcomeOK? = equiv(Gref.l+1 , apply(, Gref.l+1 , Geffects(ai ) ))
k

(18)

Since size Geffects(ai ) negligible compared size Gref.l+1 , computak
tional complexity check O(|Gref.l+1 |2 ).
f ailed outcome checked similar way; case want discover
whether nominal effects aik missing r [1, l + 1]; happens
negation effects effects(aik ) represent possible filter r [1, l + 1]
r [1, l + 1] join effects(aik ) equals r [1, l + 1]. terms OBDD operators
outcomeFailed? = equiv(Gref.l+1 , apply(, Gref.l+1 , Geffects(ai ) ))

(19)

k

important note OBDD Geffects(ai ) computed constant time directly
k
Geffects(ai ) ; fact, given Boolean function f corresponding graph Gf ,
k
sufficient exchange 0 1 nodes Gf obtain graph
representation Boolean function not(f ). Thus, also check O(|Gref.l+1 |2 ).
65

fiMicalizio & Torasso

follows cost determining outcomes actions pActsi is:
O(|pActsi | |G(ref.)l+1 |2 ).

(20)

equation (20), easy see computational cost CWCM methodology strongly depends amount available observations. worst case fact
give step l, due scarce observations, number pending actions
close l itself; is, |pActsi | l, meaning almost actions performed far
outcome pending.
A.3 Diagnosis: Computational Analysis
computational cost diagnostic process strongly dominated cost
inferring event-based explanations (EVE ). shown, fact, possible
extract set minimum cardinality primary action failures explanations (mP ADs)
structure. According Equation 11, EVE set extracted projection
current trajectory-set r [1, l] event variables e1 , . . . , el1 ; unfortunately,
case way avoid expensive operation.
estimate computational cost, first consider many binary variables
within OBDD Gl encoding r [1, l], many (binary) variables going
remove OBDD. state event variable r [i, l] fact multi-valued
variable actually implemented terms number binary variables within
OBDD Gl . number required binary variables depends size domain
original high-level variable. Let us assume size largest domain
variables VARi , estimate need b = log binary variables
variable mentioned r [1, l] (both state event variables). easy see
number binary variables required represent single belief state w = b |VARi |:
multi-valued variable VARi b binary variables OBDD level.
number binary variables encoding trajectory-set r [i, l] therefore p =
l w + (l 1) b; fact, within r [1, l], l beliefs l-1 event variables. cost
projecting r [1, l] event variables therefore:
O((2plw |Gl |)2 ).

(21)

EVE diagnoses extracted, possible infer minimum cardinality
primary failures exploiting techniques Torasso Torta (2003), proven
polynomial size OBDD.
A.4 Discussion
first important result emerges computational analysis
monitoring single execution step CWCM exponential. fact,
shown step declarative definition mapped number OBDD
operators whose complexity polynomial, provided sizes involved OBDDs
remain manageable. particular, shown exponential operation used
declarative definition, projection, avoided actual implementation.
main concern CWCM trajectory-set may grow time agent
66

fiCooperative Monitoring Diagnose Multiagent Plans

performs actions without receiving observations. Consequently, also computational cost
CWCM tends grow time since size OBDD encoding trajectoryset may increase. important note, however, growth exponential
quadratic (see equation (15)). addition, estimate computational costs
monitoring diagnosis, exploited estimations reported Table 6; these,
however, estimates worst possible cases, practice cases
common. Bryant conjectures that, although theoretical cost apply operator
two OBDDs G1 G2 O(|G1 | |G2 |) worst case, practice actual
cost cases closer O(|G1 | + |G2 | + |G3 |) G3 resulting OBDD
(Bryant, 1986). Thus also size resulting, intermediate OBDDs plays central
role determining actual computational cost.
specific case CWCM, observe common
agent perform long portion plan without receiving observations. CWCM
allows fact agents communicate other; therefore, unless agent
completely isolated others, agent likely receive observations coming
agents services provided with. means that, practice,
size OBDD encoding trajectory-set become intractable
cooperation among agents, experiments conducted far support
hypothesis.
hand, diagnostic inferences slightly expensive monitoring strategy. project operation cannot avoided order infer
diagnosis. case, however, observe plan execution already
stopped consequence detected failure. Thus, diagnosis take
time infer result since constrained on-line.

References
Arasu, A., Babu, S., & Widom, J. (2006). CQL continuous query language: semantic
foundations query execution. International Journal Large Data
Bases, 15 (2), 121142.
Birnbaum, L., Collins, G., Freed, M., & Krulwich, B. (1990). Model-based diagnosis
planning failures. Proc. Association Advancement Artificial Intelligence
(AAAI90), pp. 318323.
Boutilier, C., & Brafman, R. I. (2001). Partial-order planning concurrent interacting
actions. Journal Artificial Intelligence Research, 14, 105136.
Brusoni, V., Console, L., Terenziani, P., & Theseider Dupre, D. (1998). spectrum
definitions temporal model based diagnosis. Artificial Intelligence, 102, 3979.
Bryant, R. (1986). Graph-based algorithms boolean function manipulation. IEEE Transactions Computers, 35 (8), 677691.
Bryant, R. (1992). Symbolic boolean manipulation ordered binary-decision diagrams.
ACM Computer Surveys, 24, 293318.
67

fiMicalizio & Torasso

Cordier, M.-O., & Grastien, A. (2007). Exploiting independence decentralised
incremental approach diagnosis. Proc. International Joint Conference
Artifical Intelligence (IJCAI07), pp. 292297.
Cox, J. S., Durfee, E. H., & Bartold, T. (2005). distributed framework solving
multiagent plan coordination problem. Proc. International Conference Autonomous Agents Multiagent Systems (AAMAS05), pp. 821827.
Darwiche, A., & Marquis, P. (2002). knowledge compilation map. Journal Artificial
Intelligence Research, 17, 229264.
de Jonge, F., Roos, N., & Witteveen, C. (2009). Primary secondary diagnosis multiagent plan execution. Journal Autonomous Agent Multiagent Systems, 18 (2),
267294.
Eiter, T., Erdem, E., Faber, W., & Senko, J. (2007). logic-based approach finding
explanations discrepancies optimistic plan execution. Fundamenta Informaticae,
79 (1-2), 2569.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal
planning domains. Journal Artificial Intelligence Research, 20, 61124.
Grastien, A., Anbulagan, Rintanen, J., & Kelareva, E. (2007). Diagnosis discrete-event
systems using satisfiability algorithms. Proc. Association Advancement
Artificial Intelligence (AAAI07), pp. 305310.
Grastien, A., Haslum, P., & Thiebaux, S. (2012). Conflict-based diagnosis discrete event
systems: Theory practice. Proceedings Thirteenth International Conference Principles Knowledge Representation Reasoning (KR12), pp. 489499.
Gupta, S., Roos, N., Witteveen, C., Price, B., & de Kleer, J. (2012). Exploiting shared
resource dependencies spectrum based plan diagnosis. Proc. Association
Advancement Artificial Intelligence (AAAI12), pp. 2425 2426.
Heger, F. W., Hiatt, L. M., Sellner, B., Simmons, R., & Singh, S. (2005). Results Sliding
Autonomy Multi-Robot Spatial Assembly. Proc. International Symposium
Artificial Intelligence, Robotics Automation Space (iSAIRAS).
Helmert, M. (2009). Concise finite-domain representations PDDL planning tasks. Artificial Intelligence, 173 (5-6), 503535.
Jonsson, P., & Backstrom, C. (1998). State-variable planning structural restrictions:
Algorithms complexity. Artificial Intelligence, 100 (1-2), 125176.
Kalech, M. (2012). Diagnosis coordination failures: matrix-based approach. Journal
Autonomous Agents Multiagent Systems, 24 (1), 69103.
Kalech, M., & Kaminka, G. A. (2003). design social diagnosis algorithms
multi-agent teams. Proc. International Joint Conference Artificial Intelligence
(IJCAI03), pp. 370375.
Kalech, M., & Kaminka, G. A. (2005). Diagnosing team agents: Scaling up. Proc.
International Conference Autonomous Agents Multi-Agent Systems (AAMAS05), pp. 249255.
68

fiCooperative Monitoring Diagnose Multiagent Plans

Kalech, M., & Kaminka, G. A. (2007). design coordination diagnosis algorithms
teams situated agents. Artificial Intelligence, 171 (8-9), 491513.
Kalech, M., & Kaminka, G. A. (2011). Coordination diagnostic algorithms teams
situated agents: Scaling up. Computational Intelligence, 27 (3), 393421.
Kalech, M., Kaminka, G. A., Meisels, A., & Elmaliach, Y. (2006). Diagnosis multi-robot
coordination failures using distributed CSP algorithms. Proc. Association
Advancement Artificial Intelligence (AAAI06), pp. 970975.
Kaminka, G. A., & Tambe, M. (2000). Robust multi-agent teams via socially-attentive
monitoring. Journal Artificial Intelligence Research, 12, 105147.
Lamperti, G., & Zanella, M. (2002). Diagnosis discrete-event systems uncertain
temporal observations. Artificial Intelligence, 137 (1-2), 91163.
Mi, P., & Scacchi, W. (1993). Articulation: integrated approach diagnosis, replanning, rescheduling software process failures. Proc. Knowledge-Based
Software Engineering Conference, pp. 7784.
Micalizio, R. (2013). Action failure recovery via model-based diagnosis conformant
planning. Computational Intelligence, 29 (2), 233280.
Micalizio, R., & Torasso, P. (2007a). On-line monitoring plan execution: distributed
approach. Knowledge-Based Systems, 20 (2), 134142.
Micalizio, R., & Torasso, P. (2007b). Plan diagnosis agent diagnosis multi-agent
systems. Proc. Congress Italian Association Artificial Intelligence
(AI*IA07), Vol. 4733 LNCS, pp. 434446.
Micalizio, R., & Torasso, P. (2008). Monitoring execution multi-agent plan: Dealing
partial observability. Proc. European Conference Artificial Intelligence
(ECAI08), pp. 408412.
Micalizio, R., & Torasso, P. (2009). Agent cooperation monitoring diagnosing
MAP. Proc. Multiagent System Technologies (MATES09), Vol. 5774 LNCS,
pp. 6678.
Micalizio, R., Torasso, P., & Torta, G. (2006). On-line monitoring diagnosis team
service robots: model-based approach. AI Communications, 19 (4), 313349.
Nebel, B. (2000). compilability expressive power propositional planning
formalisms. Journal Artificial Intelligence Research, 12, 271315.
Pencole, Y., & Cordier, M. (2005). formal framework decentralized diagnosis
large scale discrete event systems application telecommunication networks.
Artificial Intelligence, 164, 121170.
Reiter, R. (1987). theory diagnosis first principles. Artificial Intelligence, 32 (1),
5796.
Roos, N., & Witteveen, C. (2009). Models methods plan diagnosis. Journal
Autonomous Agent Multiagent Systems, 19 (1), 3052.
Sampath, M., Sengupta, R., Lafortune, S., Sinnamohideen, K., & Teneketzis, D. (1995).
Diagnosability discrete event systems.. IEEE Transactions Automatic Control,
40 (9), 15551575.
69

fiMicalizio & Torasso

Sellner, B., Heger, F., Hiatt, L., Simmons, R., & Singh, S. (2006). Coordinated multiagent teams sliding autonomy large-scale assembly. IEEE - Special Issue
Multi-Robot Systems, 94 (7), 1425 1444.
Steinbauer, G., & Wotawa, F. (2008). Enhancing plan execution dynamic domains using
model-based reasoning. Intelligent Robotics Applications, First International
Conference, (ICIRA08), Vol. 5314 LNAI, pp. 510519.
Tambe, M. (1998). Implementing agent teams dynamic multi-agent environments. Applied
Artificial Intelligence, 12 (2-3), 189210.
Torasso, P., & Torta, G. (2003). Computing minimum-cardinality diagnoses using OBDDs.
German Conference AI (KI03), Vol. 2821 LNCS, pp. 224238.
Torasso, P., & Torta, G. (2006). Model-based diagnosis OBDD compilation: complexity analysis. Reasoning, Action Interaction AI Theories Systems,
Vol. 4155 LNCS, pp. 280298.
Torta, G., & Torasso, P. (2007). role modeling causal independence system
model compilation OBDDs. AI Communications, 20 (1), 1726.
Weld, D. S. (1994). introduction least commitment planning. AI Magazine, 15 (4),
2761.
Witteveen, C., Roos, N., van der Krogt, R., & de Weerdt, M. (2005). Diagnosis single
multi-agent plans. Proc. International Conference Autonomous Agents
Multiagent Systems (AAMAS05), pp. 805812.
Yan, Y., Dague, P., Pencole, Y., & Cordier, M.-O. (2009). model-based approach
diagnosing fault web service processes. Journal Web Service Research., 6 (1),
87110.

70

fiJournal Artificial Intelligence Research 51 (2014) 707-723

Submitted 07/14; published 12/14

Minimum Representations Matched Formulas
Ondrej Cepek
Stefan Gursky
Petr Kucera

ondrej.cepek@mff.cuni.cz
stevko@mail.ru
kucerap@ktiml.mff.cuni.cz

Charles University Prague
Faculty Mathematics Physics
Department Theoretical Computer Science
Malostranske nam. 25, 118 00 Praha 1, Czech Republic

Abstract
Boolean formula conjunctive normal form (CNF) called matched system
sets variables appear individual clauses system distinct representatives.
matched CNF trivially satisfiable (each clause satisfied representative
variable). Another property easy see, class matched CNFs
closed partial assignment truth values variables. latter property leads
fact (proved here) given two matched CNFs co-NP complete decide whether
logically equivalent. construction proof leads another result: much
shorter simpler proof p2 -completeness Boolean minimization matched CNFs.
main result paper deals structure clause minimum CNFs. prove
Boolean function f admits representation matched CNF every
clause minimum CNF representation f matched.

1. Introduction
paper study class matched formulas introduced Franco Van Gelder
(2003). Given formula conjunctive normal form (CNF) consider incidence
graph I() defined follows. I() bipartite graph one part consisting clauses
part containing variables . edge {C, x} clause C
variable x I() x appears C. observed Aharoni Linial (1986),
Tovey (1984) I() admits matching (i.e. set pairwise disjoint edges)
size (where number clauses ), satisfiable. Later formulas
satisfying condition called matched formulas Franco Van Gelder (2003).
Since matching maximum size given graph found polynomial time (e.g.,
Lovasz & Plummer, 1986), check whether given formula matched. Given
arbitrary CNF measure far matched considering
maximum deficiency (), number clauses remain unmatched maximum
matching I(). formula thus matched iff () = 0. weaker notion deficiency
() = n (where number clauses n number variables ) also
often considered.
Since introduction, matched formulas considered base class parameterized algorithms satisfiability (e.g., overview parameterized algorithms theory
see Flum & Grohe, 2006). particular, Fleischner, Kullmann, Szeider (2002) show
satisfiability formulas whose maximum deficiency bounded constant
c
2014
AI Access Foundation. rights reserved.

fiCepek, Gursky, & Kucera

decided polynomial time. result later improved Szeider (2003)
algorithm satisfiability parameterized maximum deficiency formula. Parameterization based backdoor sets respect matched formulas considered
Szeider (2007).
Several generalizations matched formulas considered literature, too. Kullmann (2000) generalized class matched formulas class linearly satisfiable
formulas Kullmann (2003) studied autarkies based matchings. Another generalization considered Szeider (2005) classes bi-clique satisfiable var-satisfiable
formulas. Unfortunately, bi-clique var-satisfiable formulas hard check
formula falls one classes (Szeider, 2005).
results listed previous paragraphs show matched formulas play significant role theory satisfiability solving is, without doubt, one
studied problems theoretical computer science many practical applications.
Despite fact, little known structure matched CNFs. say CNF
representing function f irredundant set-minimal representation f , i.e.
clause C 0 = \ {C} represent f . say prime
CNF representing f clauses prime implicates f , clause C
implicate f satisfied assignments satisfying f prime implicate
set-minimal implicate f (considering clause set literals). say
CNF prime without mentioning function f question implicitly consider f
function represented . hard come examples matched CNFs
logically equivalent prime irredundant CNFs matched.
quite interesting phenomenon occur classes polynomial
time satisfiability testing quadratic CNFs (also 2-CNFs, i.e. CNFs consisting
clauses two literals), Horn CNFs (a CNF Horn every clause contains
one positive literal), various generalizations, CNF
class, logically equivalent prime CNFs guaranteed class well.
brings interesting question: given (nonprime) matched CNF exist
least one equivalent prime CNF also matched? paper give affirmative
answer question. answer may seem quite intuitive usually
enough consider prime implicates function studying CNF representations,
however, proof answer easy.
Another problem study paper Boolean minimization matched CNFs.
Boolean minimization problem (BM) stated follows: given CNF find
logically equivalent CNF minimum possible number clauses. number
clauses viewed number rules implicates representation knowledge
base standard measure used context. One also consider length
formula, i.e. total number literal occurrences formula, measure
optimality, paper use number clauses measure leave
open question whether results paper could extended case
formula length well. optimization version Boolean minimization problem
turned decision version adding number k asking whether exists
logically equivalent CNF k clauses. Umans (2001) showed decision
version BM p2 -complete (e.g., related results see review paper Umans, Villa,
& Sangiovanni-Vincentelli, 2006). Buchfuhrer Umans (2011) later showed BM
708

fiOn Minimum Representations Matched Formulas

p2 -complete considering general formulas constant depth input output
Boolean minimization problem.
also long known BM NP-hard already classes CNFs
SAT solvable polynomial time. Maybe best known example class Horn
CNFs NP-hardness respect output measures proved (Ausiello,
DAtri, & Sacca, 1986; Boros & Cepek, 1994; Cepek, 1995; Hammer & Kogan, 1993; Maier,
1980). exists hierarchy tractable subclasses Horn CNFs
polynomial time minimization algorithms, namely acyclic quasi-acyclic Horn CNFs
(Hammer & Kogan, 1995), CQ Horn CNFs (Boros, Cepek, Kogan, & Kucera, 2009).
also heuristic minimization algorithms Horn CNFs (Boros, Cepek, &
Kogan, 1998).
complexity BM matched CNFs fit picture. Despite fact
SAT trivial matched CNFs, BM class p2 -complete, i.e. hard
general case. fact proved Gursky (2011), proof modifies proof
general case Umans (2001). paper give much simpler proof
fact based observation, equivalence testing co-NP-complete
matched CNFs. also study structure clause minimum CNFs. Based
mentioned result concerning prime CNFs prove Boolean function f admits
representation matched CNF every clause minimum CNF representation f
matched. main result current paper.
paper structured follows. start introducing necessary notation,
definitions basic results Section 2. Section 3 prove testing logical equivalence matched CNFs co-NP-complete use idea proof show
BM matched formulas p2 -complete. known fact, current proof
much shorter simpler. Section 4 studies prime representations functions defined
matched CNFs (possibly nonprime). prove every function least
one prime representation matched. Finally, Section 5 study structure
clause minimum representations functions defined matched CNFs. Using result
Section 4 prove Boolean function f admits representation matched
CNF every clause minimum CNF representation f matched. Section 6 concludes
paper closing remarks.

2. Definitions Results
shall start basic definitions notions need paper. shall also
recall results shall use paper.
2.1 Boolean Functions
Boolean function n variables mapping f : {0, 1}n {0, 1}. literal either variable (x, called positive literal ) negation (x x, called negative literal ). clause
disjunction literals. assume clause contains positive negative literals variable. Formula conjuntive normal form (CNF) conjuction
clauses (we also say CNF formula). shall often treat clause set
literals CNF formula set clauses. Thus || denote number
clauses . well known fact every Boolean function represented
709

fiCepek, Gursky, & Kucera

CNF formula (e.g., Genesereth & Nilsson, 1987). two CNF formulas 1 2 define
function, say equivalent denote fact 1 2 .
CNF called clause minimum every CNF || ||.
Clause C called implicate f every assignment ~x {0, 1}n satisfying f (i.e.
f (~x) = 1) also satisfies C (i.e. C(~x) = 1). say clause C1 subsumes clause C2 ,
every literal C1 occurs also C2 (i.e. C1 C2 ). C prime implicate function
f implicate f implicate C 0 f subsuming C (i.e. C
set-minimal implicate f ). say CNF formula representing function f
prime representation f contains prime implicates f (if refer prime
CNF without specifying function f consider function represented
). CNF formula irredundant sub-CNF 0 represents
function .
assignment assigns values subset (possibly all) variables
function f n called partial assignment. Formally, partial assignment
viewed mapping : 7 {0, 1} subset variables f . Given CNF ,
(t) denotes CNF applying partial assignment t. particular (t) produced
following way: Clauses contain literal satisfied
(assigned value 1) removed , occurences literals variables
satisfied removed clauses .
2.2 Resolution
say two clauses conflict variable x positive occurrence
f1 x)
x one clause negative occurrence other. Two clauses C1 = (C
f
f
f
C2 = (C2 x) resolvable x C1 C2 conflict variable.
f1 C
f2 disjunction called resolvent parent clauses
write R(C1 , C2 ) = C
C1 C2 .
Let CNF formula representing Boolean function f , say C derived
series resolutions sequence clauses C1 , . . . , Ck = C
every Ci , 1 k, either belongs , Ci = R(Cj1 , Cj2 ), j1 , j2 < i. series
resolutions also called resolution proof C . resolution proof empty
clause (denoted ) unsatisfiable formula called refutation. length
resolution proof number clauses sequence.
well known fact Boolean function resolvent two implicates
implicate (e.g., Buning & Lettmann, 1999). Another well known fact
every prime implicate f derived CNF representation f series
resolutions (e.g., Buning & Lettmann, 1999).
shall also use notions regular tree-like resolution proofs. resolution
proof tree-like resolution proof every occurrence clause proof used
premise resolution clause used premise
resolution conclusion; tree-like resolution proof represented tree,
leaves labelled input clauses, root tree conclusion
proof. depth tree-like resolution proof length longest path
leaf root . resolution proof regular path proof
input clause conclusion, variable resolved once.
710

fiOn Minimum Representations Matched Formulas

observed unsatisfiable CNF, regular tree-like
refutation (Urquhart, 2011), basically turn resolution derivation tree-like
resolution derivation repeating clauses necessary. tree-like refutation
turned regular tree-like refutation (Tseitin, 1983; Urquhart, 1995, 2011).
observed C implicate derived series resolutions
, clause C 0 C derived regular tree-like resolution
satisfying variable C 0 resolved . Indeed, C implicate ,
let partial assignment assigns false literals C. follows (t)
unsatisfiable formula, thus regular tree-like refutation 0 . put back
falsified literals C clauses satisfied t, get resolution derivation
sub-CNF C 0 . following proposition follows immediately.
Lemma 2.1 Let CNF let C prime implicate , C derived
regular tree-like resolution .
2.3 Exclusive Sets Implicates Boolean Function
section recall definition exclusive sets implicates Boolean function
state properties, shown Boros, Cepek, Kogan,
Kucera (2010).
Let us first introduce following notation. p (f ) shall denote set
prime implicates function f . I(f ) shall denote resolution closure p (f ),
i.e. implicate C f belongs I(f ) derived series resolutions
p (f ).
Definition 2.2 (Boros et al., 2010) Let f Boolean function let X I(f )
set clauses. shall say, X exclusive set clauses f every pair
resolvable clauses C1 , C2 I(f ) following implication holds:
R(C1 , C2 ) X = C1 X C2 X ,
i.e. resolvent belongs X parent clauses X . function f
clear context, shall simply say X exclusive set.
shall recall properties exclusive sets, proved Boros
et al. (2010) use paper.
Lemma 2.3 (Boros et al., 2010) Let A, B I(f ) exclusive sets implicates f ,
B B also exclusive sets implicates f .
Theorem 2.4 (Boros et al., 2010) Let f arbitrary Boolean function, let C1 , C2 I(f )
two distinct sets clauses represent f , let X I(f ) exclusive set
clauses. C1 X C2 X , i.e. represent function.
Based proposition define exclusive component Boolean function.
Definition 2.5 (Boros et al., 2010) Let f arbitrary Boolean function, X I(f )
exclusive set clauses f , C I(f ) set clauses represents f .
Boolean function fX represented set C X called X -component
function f . shall simply call function g exclusive component f , g = fX
exclusive subset X I(f ).
711

fiCepek, Gursky, & Kucera

Theorem 2.4 guarantees X -component fX well defined every exclusive set
X I(f ). Theorem 2.4 following corollary.
Corollary 2.6 (Boros et al., 2010) Let C1 , C2 I(f ) two distinct sets clauses
C1 C2 f , i.e. sets represent f , let X I(f ) exclusive set
clauses. (C1 \ X ) (C2 X ) also represents f .
2.4 Autarkies
Autark assignments introduced Monien Speckenmeyer (1985)
defined follows.
Definition 2.7 Let CNF set V variables, let V subset
variables, let L = {x | x } {x | x } corresponding set literals, let
: 7 {0, 1} partial assignment . autarky every clause
C either C L = C satisfied t.
autarky special type partial assignment satisfies every clause
substitutes value literal. shall prove two simple lemmas autarkies
needed later paper. first lemma (in different notation)
shown Kullmann (2000) Lemma 3.13, give short proof well
make paper self-contained.
Lemma 2.8 Let CNF set V variables represents function f . Let
V subset variables, let :
7 {0, 1} autarky .
autarky I(f ).
Proof : Since clauses I(f ) derived resolution, suffice
show resolution preserves autarky properties, namely parent clauses
satisfied whenever contain literal L = {x | x } {x | x },
resolvent. Let C, C1 , C2 I(f ) clauses C = R(C1 , C2 ). Let
` L literal C. ` satisfied done, let us asssume `
satisfied t. Clause C inherited ` one parent clauses let us assume without
loss generality ` C1 . autarky property, C1 must satisfied t,
must contain another literal `0 L satisfied t. two possibilities: either
`0 C (clause C inherited ` `0 C1 ) case C satisfied
done, `0 6 C means R(C1 , C2 ) resolves `0 . implies `0 C2 , i.e.
C2 contains literal L satisfied t. Thus, autarky property, C2 must
satisfied t, must contain another literal `00 L satisfied t. However, case
C inherits `00 C2 , finishes proof.
Corollary 2.9 Let CNF set V variables represents function f .
Let V subset variables, let : 7 {0, 1} autarky . Let I(f )
arbitrary representation f . autarky .
Let us mention Corollary 2.9 would hold without assumption I(f ).
Consider e.g. CNF = (x y) z CNF = (x y) (z y) (z y).
obvious , i.e. CNFs represent function f , 6 I(f ).
712

fiOn Minimum Representations Matched Formulas

assignment sets 1, autarky . hand autarky
(z y) satisfied t.
Lemma 2.10 Let CNF set V variables, let V subset variables,
let L = {x | x } {x | x } corresponding set literals, let : 7 {0, 1}
autarky . (t) represents exclusive component fX f defined
exclusive set clauses
X = {C I(f ) | C L = }.
Proof : suffices show X exclusive subset I(f ) Let C, C1 , C2 I(f )
clauses C = R(C1 , C2 ) C X . Let us assume contradiction one
parent clauses, say C1 , belong X , means exists ` C1 L.
Since ` 6 C, R(C1 , C2 ) must resolve `, implies ` C2 . However, one `, `
satisfied t, corresponding clause (one C1 , C2 ) must contain literal
`0 L satisfied autarky. get `0 C contradicting
assumption C X .
Since autarky follows (t) = X since I(f ) X
exclusive set implicates f , follows Theorem 2.4 (t) = X I(f ) X
defines exclusive compontent fX .
2.5 Matched Formulas
subsection shall define key concept paper. concept based
graph properties, end shall use standard graph terminology (e.g., see Bollobas,
1998). Given undirected graph G = (V, E), subset edges E matching
G edges pairwise disjoint. bipartite graph G = (A, B, E) undirected
graph disjoint sets vertices B, set edges E satisfying E B.
set W vertices G, let (W ) denote neighbourhood W G, i.e. set
vertices adjacent element W . shall use following well-known result
matchings bipartite graphs:
Theorem 2.11 (Halls Theorem Hall, 1935; Lovasz & Plummer, 1986) Let G = (A, B, E)
bipartite graph. matching size |M | = |A| exists every subset
|S| |(S)|.
ready define matched formulas.
Definition 2.12 Let = C1 . . . Cm CNF n variables X = {x1 , . . . , xn }.
shall associate bipartite graph I() = (, X, E) (also called incidence graph
), vertices correspond clauses CNF variables X. clause Ci
connected variable xj (i.e. {Ci , xj } E) Ci contains xj xj . CNF matched
I() matching size m, i.e. matching pairs clause
unique variable.
Note matching maximum size given graph found polynomial
time (e.g., Lovasz & Plummer, 1986) thus test polynomial time whether given
CNF matched. fact clause Ci matched variable xj matching
denoted {Ci , xj } . variable matched clause matching
713

fiCepek, Gursky, & Kucera

called matched , free otherwise. Note, matched CNF trivially
satisfiable. clause Ci matched variable xj , simply assign xj value
satisfy Ci . name matched given formulas Franco
Van Gelder (2003), although already considered Aharoni Linial (1986),
Tovey (1984).

3. Equivalence Testing Hardness Clause Minimization Matched
Formulas
Following definition given Cepek, Kucera, Savicky (2012) class CNFs X
called tractable satisfies following four properties.
Recognition: Given arbitrary CNF possible decide polynomial time
respect size whether X .
Satisfiability: Given arbitrary CNF X possible decide polynomial
time respect size whether satisfiable.
Partial assignment: Given arbitrary CNF X , produced fixing
variables 0 1 substituting values , X .
Prime representations: Given arbitrary CNF X , represents function f
prime CNF representations f belong X .
shown Cepek et al. (2012) given two CNFs tractable class,
tested polynomial time, whether two CNFs logically equivalent not.
class matched CNFs clearly satisfies first two tractability conditions, fails
satisfy remaining two. Costructing counterexample third property easy.
CNF
(x z) (x z) (x z)
clearly matched, partial assignment x 0 creates CNF
(y z) (y z) (y z)
matched. defer counterexample fourth property next
section. light findings interesting question complexity
equivalence testing matched CNFs. Despite fact satisfiability trivial
matched CNFs, equivalence testing co-NP-complete.
Matched equivalence
Instance :

Two matched CNFs

Question : ?
Theorem 3.1 problem Matched equivalence co-NP-complete.
714

fiOn Minimum Representations Matched Formulas

Proof : nondeterministic polynomial procedure checking two CNFs
equivalent simply guesses assignment checks whether (t) 6= (t). problem
Matched equivalence thus co-NP.
show co-NP-hardness reduce problem checking given CNF
unsatisfiable (this problem prominent example co-NP-complete problem
complement satisfiability problem, e.g., see Garey & Johnson, 1979). Let
arbitrary CNF n variables clauses, particular let = C1 C2 . . . Cm . Let
us define clause = (a1 a2 . . . ) new variables occuring . let
us define two CNFs:
= (C1 D) (C2 D) . . . (Cm D)
=
obviously matched, since clause Ci0 = (Ci D) matched
variable ai . iff , i.e. iff unsatisfiable. follows
directly fact . reduced co-NP-complete problem
unsatisfiability problem Matched Equivalence thus problem Matched
Equivalence co-NP-complete well.
fact, equivalence testing co-NP-hard, probably principal reason behind
fact proved Gursky (2011) clause minimization matched CNFs p2 -complete.
proof Gursky (2011) basically follows proof p2 -completeness general CNFs
presented Umans (1999), Buchfuhrer Umans (2011) quite long
complicated. present much shorter simpler proof based similar idea
proof hardness equivalence testing.
Matched minimization
Instance :

matched CNF integer k

Question : CNF equivalent k clauses?
Theorem 3.2 problem Matched minimization p2 -complete.
Proof : Since Matched Minimization special case Boolean minimization,
known Boolean minimization p2 -complete, Matched Minimization must
p2 . see problem p2 -hard, reduce p2 -complete Boolean minimization
it.
Let (, k) instance Boolean minimization = C1 C2 . . . Cm .
let us repeat construction proof Theorem 3.1. Let a1 , a2 , . . . , new
variables occur . Let matched CNF defined (a1 a2
0 C 0 = (C . . . ) 1 m.
. . . ), = C10 C20 . . . Cm

1
2


instance Matched minimization (, k).
Let (, k) positive instance Boolean minimization. exists CNF
= D1 D2 . . . Dk0 (with k 0 k) equivalent . Let CNF equivalent
(a1 a2 . . . ) let = D10 D20 . . . Dk0 0 Di0 = Di a1 a2 . . .
715

fiCepek, Gursky, & Kucera

1 k 0 . Clearly equivalent k clauses. Therefore (, k)
positive instance Matched minimization.
see direction let (, k) positive instance Matched minimization
let CNF equivalent k clauses. Let CNF originating
partial assignment sets a-variables zero sets variable.
Since equivalent equivalent (a1 a2 . . . ),
equivalent . Clearly || || k since equivalent conclude
(, k) positive instance Boolean minimization.
Remark 3.3 Since occurrences a-variables proof Theorem 3.2
positive, every resolution keeps a-variables every derived clause. assume
prime therefore every clause also contains a-variables positively
thus fact number clauses .

4. Prime Representations Matched Formulas
difficult see unlike well-behaved classes CNFs (such e.g. Horn
CNFs quadratic CNFs) prime irredundant CNFs lie inside class,
case matched CNFs. Consider CNF
(a b) (b c) (c a)
matched logically equivalent CNF
(a b) (b a) (c b) (b c)
matched despite prime irredundant. Thus legitimate question,
whether given (nonprime) matched CNF, exists least one logically equivalent
prime irredundant CNF also matched. rest section prove
affirmative answer question. Let us start simple useful observation.
Observation 4.1 Let = C1 . . .Cm matched CNF let C clause derived
regular tree-like resolution derivation . Let C = R(D1 , D2 ), D1 = (A1 z)
D2 = (A2 z), i.e. resolution z. Let T1 denote subtree rooted
D1 , let T2 denote subtree rooted D2 . Ci , {1, . . . , m} leaf
clause T1 T2 , Ci contains neither z z thus cannot matched
z matching .
Proof : Since regular, resolution z C = R(D1 , D2 ). Thus
z T1 z T2 , since would D1 D2 respectively
well. Thus Ci T1 T2 cannot contain z all.
following lemma allow us exchange one free variable matched variable
matching given matched CNF .
Lemma 4.2 Let = C1 . . . Cm matched CNF let matching . Let
clause derived regular tree-like resolution . Let x variable
free , variable matched matching 0
satisfy following property: X denotes set variables matched
, X 0 denotes set variables matched 0 , X 0 = (X \ {y}) {x}.
716

fiOn Minimum Representations Matched Formulas

Proof : shall proceed induction depth regular tree-like resolution
proof D. , set variable matched

n
n

0 := \ {D, y} {D, x} .
let us assume = R(D1 , D2 ), D1 D2 either clauses ,
derived regular tree-like resolution . Suppose resolution
variable z let us denote D1 = (A1 z), D2 = (A2 z). = 1, 2 let Ti denote
subtree rooted Di , Li set leaf clauses Ti , sub-CNF formed clauses
Li , Mi sub-matching clauses , Xi set variables matched
Mi .
First let us assume variable x A1 \ A2 . Let us without loss generality assume
x appears positively D1 (otherwise switch polarity x ), thus
D1 = (A01 x z), A01 = A1 \ {x}. use induction hypothesis
subtree T1 rooted D1 , matching M1 1 . follows matching M10
1 x matched variable variable y1 D1 matched
M1 free M10 . Moreover X10 denotes set variables matched M10 ,
X10 = (X1 \ {y1 }) {x}. extend matching M10 whole adding pairs
matching clauses L2 \L1 variables X2 \X1 , let new matching denoted
00 . y1 6= z, y1 occurs set 0 = 00 .
y1 = z, free variable 00 . Let M200 denote sub-matching 00
clauses 2 . use induction hypothesis 2 , resolvent D2 , variable
y1 = z (playing role free variable). find matching M2 2
variable D2 y1 matched M2 , free M2 matched M200 .
particular X2 denotes set matched variables M2 X200 denotes set
variables matched M200 , X2 = (X200 \ {y}) {y1 }. extend matching
M2 whole formula adding pairs matching clauses L1 \ L2 variables X100 \ X200
00 (here X100 denotes set variables matched 00 leaves T1 ).
way obtain desired matching 0 . clear 6= z thus D. Moreover
X 0 = (X \ {y}) {x}.
Now, let us assume x A1 A2 . Observation 4.1 z 6 X1 X2 . Let
{1, 2} z 6 Xi . let us use induction hypothesis variable
x. get matching Mi0 formula extended whole adding
pairs matching clauses L3i \ Li variables X3i \ Xi (the extended
matching desired matching 0 ). also get variable matched
Mi , free Mi0 . Since Xi , get 6= z thus D. Clearly
X 0 = (X \ {y}) {x}.
Lemma 4.3 Let = C1 C2 . . . Cm matched CNF let us assume
implicate derived regular tree-like resolution derivation C1
used, 0 = C2 . . . Cm matched CNF.
Proof : fact C1 used resolution derivation implies C1 leaf
clause . shall proceed induction depth . Let matching
let X denote set variables, matched . shall preserve
following invariant:
717

fiCepek, Gursky, & Kucera

(*) 0 matching 0 constructed proof X 0 denotes matched
variables 0 , X 0 = X.
Let us first assume = C1 . proposition trivially follows invariant
(*) satisfied. let us suppose = R(C1 , Cj ), j {2, . . . , m}. Let
variable matched C1 , i.e. {C1 , y} . D, set

n
n

0 = \ {C1 , y} {D, y} .
6 D, follows C1 Cj resolve y. Let us without loss generality
assume appears positively C1 let us denote C1 = (A1 y) Cj = (Aj y),
hence = A1 Aj . Cj matched another variable z Aj , thus
set

n
n

0 = \ {C1 , y}, {Cj , z} {Cj , y}, {D, z} .
0 matching 0 X 0 = X.
let us assume = R(D1 , D2 ) D1 D2 derived
resolution derivation , belong . Suppose resolution variable
z let us denote D1 = (A1 z), D2 = (A2 z). = 1, 2 let Ti denote subtree
rooted Di , Li set leaf clauses Ti , sub-CNF formed clauses Li ,
let Mi sub-matching clauses , Xi set variables matched
Mi .
Let us first assume C1 L1 L2 . Observation 4.1 get z 6 X1 X2 .
Let {1, 2} z
/ Xi . Let us use induction hypothesis Ti Di
find matching formula 0i CNF formed clauses (Li \ {C1 }) {Di }.
induction hypothesis 0i matched formula, let Mi0 matching constructed 0i
satisfies invariant (*), i.e. set matched variables Mi0 Xi . Let x
variable matched Di Mi0 . since z 6 Xi x Ai , thus x belongs
well. construct matching 0 0 extending Mi0 pairs
matching variables X3i \ Xi clauses L3i \ Li . Moreover replace
pair {Di , x} pair {D, x}. result 0 matching 0 exactly
variables X matched thus 0 satisfies invariant (*).
rest proof shall assume C1 L1 \ L2 . use induction
hypothesis T1 D1 find matching M10 formula 01 CNF
formed clauses (L1 \ {C1 }) {D1 }. induction hypothesis M10 satisfies invariant (*)
thus matched variables M10 exactly variables set X1 . Let x
variable clause D1 matched M10 . two cases consider .
1. x A1 , construct matching 0 whole formula 0 extending
M10 pairs matching clauses L2 \ L1 variables X2 \ X1 ,
replace pair {D1 , x} pair {D, x}. 0 matching matches
variables X thus also satisfies invariant (*).
2. x = z, situation complicated. case observe z X1 \X2
(z 6 X1 X2 Observation 4.1, hand z matched M1 M10 ). Let
M20 matching sub-CNF 2 formed clauses L2 constructed
718

fiOn Minimum Representations Matched Formulas

follows. clauses L1 L2 matched variables M10 ,
clauses L2 \ L1 matched variables . Note M20 formed
way really matching, particular C1 belong L2 thus
matter matched variable M10 . Moreover, X20 denotes
set variables matched M20 , variable X20 matched exactly one
clause. M10 change anything clauses L2 \ L1
match variables X2 \ X1 . Note, X20 necessarily equal
X2 , M10 allowed use variables X1 \ X2 clauses L1 L2 ,
X = X20 X1 . also z free variable M20 ,
z X1 \ X2 , thus matched clause L2 , z
matched D1 M10 , thus z matched clause L1 L2 M10 .
following situation: formula 2 , clause D2
derived regular tree-like resolution T2 2 . matching M20
2 matches variables set X20 . variable z free
M20 , thus use Lemma 4.2 find another matching M200 2 variable
D2 z matched clause M200 , free M200
matched M20 . Moreover, X200 denotes set variables matched M200 ,
X200 = (X20 \ {z}) {y}. Necessarily z 6= y, thus A2 . ready
form desired matching 0 .
(a) clause C L1 \ (L2 {C1 }) matched variable 0
{C, a} M10 .
(b) clause C L2 matched variable 0 {C, a} M200 .
(c) clause matched variable 0 .
0 defined way indeed matching, particular C L1 \(L2 {C1 }),
matched variable belongs X1 \ (X20 {z}), thus free M20
M200 , still used C. Let us also observe 0 preserves invariant
(*), particular X 0 denotes set variables matched 0 , X 0 = X. Let
variable X 0 , shall show X well, |X 0 | = |X|,
follows X 0 = X.
(a) matched clause C L1 \ (L2 {C1 }), {C, a} M10
since induction hypothesis invariant (*) preserved M10
X1 thus X.
(b) matched clause C L2 , {C, a} M200 ,
X 00 = (X20 \ {y}) {z}. know z X X20 X.
(c) matched D, = X20 X.
Together invariant (*) satisfied 0 .
case found matching 0 CNF 0 satisfies invariant (*), proof
finished.
Theorem 4.4 Let matched CNF representing function f , prime
irredundant representation f also matched.
719

fiCepek, Gursky, & Kucera

Proof : follows Lemma 4.3. Firstly, drop redundant clauses
without spoiling matched property. 0 irredundant representation f originated
dropping redundant clauses, turn prime representation
using Lemma 4.3 follows. 0 = C1 . . . Cm , C10 ( C1 prime implicate,
C10 derived resolution derivation 0 . Since 0 already irredundant,
every resolution derivation C10 0 use C1 . Thus Lemma 4.3,
formula 00 = C10 C2 . . . Cm also matched. way replace every clause
0 prime subimplicate. Thus obtain prime irredundant representation
f.

5. Minimum Representations Matched Formulas
previous sections seen matched CNF may logically
equivalent prime irredundant CNFs matched always least one
CNF matched. section shall show stronger statement holds CNFs
prime irredundant also clause minimum. shall prove
Boolean function f admits matched CNF representation, every clause minimum
CNF representation f matched CNF.
Theorem 5.1 Let matched CNF representing function f set variables V
let clause minimum CNF representation f . matched CNF.
Proof : Due Theorem 4.4 may assume prime irredundant thus
I(f ). Let us assume contradiction matched X
maximal (under inclusion) sub-CNF violating Halls condition (such subset must exist
due Theorem 2.11). Let us denote X set variables sub-CNF X , = V \ X
set remaining variables , = \ X remaining clauses (note
clauses may contain variables also X). following
holds:
violation Halls condition |X | > |X|.
maximality X exists matching clauses variables
, i.e. matched CNF even drop variables X clauses.
follows fact every subset must satisfy Halls condition even
respect variables , since otherwise violating subset could
added X contradicting maximality.
existence matching implies satisfied using variables
(each clause satisfied matched variable). let : 7 {0, 1} partial
assignment satisfying clauses (t necessarily unique). Clearly, autarky
satisfies every clause containing assigned literal.
follows Lemma 2.10 (t) = X represents exclusive component fX f
defined exclusive set X I(f ), contains clauses consisting variables
X. Since also represents f assumed I(f ), follows Corollary 2.9
autarky also . Thus, similarly (t) above, conclude
(t) sub-CNF represents exclusive component fX f , i.e. (t) (t).
720

fiOn Minimum Representations Matched Formulas

However, matched, every sub-CNF (and particular (t)) matched, thus
|(t)| |X| |(t)| = |X | > |X|. now, since (t) (t) = X represent
exclusive component f , also CNF 0 = ( \ (t)) (t) represents f Corollary 2.6.
However, get | 0 | < || contradicting assumed minimality .

6. Conclusions
paper study class matched CNFs important class formulas
theory parametrized SAT algorithms. focus clause minimum CNF representations Boolean functions represented matched CNFs. results
presented paper two types:
1. Complexity results. show testing logical equivalence two matched CNFs
co-NP-complete. use similar construction prove Boolean minimization matched CNFs p2 -complete. already known fact,
presented proof much shorter simpler proof Gursky (2011).
results appear Section 3.
2. Structural results. prove given (non-prime) matched CNF representing
function f , may prime representations f matched,
exists least one prime representation f matched. Furthermore
prove case clause minimum CNFs f guaranteed matched.
latter result course implies former, however, proof latter result
uses former one. Thus subsequently appear text main results
Sections 4 5.
interesting question future research whether structural results Sections 4 5 extended way matched CNFs, i.e. CNFs maximum
deficiency zero, CNFs maximum deficiency bounded constant (see Section 1
definition maximum deficiency).
paper use number clauses CNF measure optimality.
interesting question whether result would hold total length formula (i.e.
total number literal occurrences formula) would considered.

Acknowledgments
second author gratefully acknowledges support Charles University Grant
Agency (grant No. 1390213).

References
Aharoni, R., & Linial, N. (1986). Minimal non-two-colorable hypergraphs minimal
unsatisfiable formulas. Journal Combinatorial Theory, Series A, 43 (2), 196 204.
Ausiello, G., DAtri, A., & Sacca, D. (1986). Minimal representation directed hypergraphs. SIAM Journal Computing, 15 (2), 418431.
721

fiCepek, Gursky, & Kucera

Bollobas, B. (1998). Modern Graph Theory, Vol. 184 Graduate Texts Mathematics.
Springer.
Boros, E., & Cepek, O. (1994). complexity Horn minimization. Tech. rep. 1-94,
RUTCOR Research Report RRR, Rutgers University, New Brunswick, NJ.
Boros, E., Cepek, O., & Kogan, A. (1998). Horn minimization iterative decomposition.
Annals Mathematics Artificial Intelligence, 23, 321 343.
Boros, E., Cepek, O., Kogan, A., & Kucera, P. (2009). subclass Horn CNFs optimally
compressible polynomial time. Annals Mathematics Artificial Intelligence,
57, 249291.
Boros, E., Cepek, O., Kogan, A., & Kucera, P. (2010). Exclusive essential sets
implicates boolean functions. Discrete Applied Mathematics, 158 (2), 81 96.
Buchfuhrer, D., & Umans, C. (2011). complexity boolean formula minimization.
Journal Computer System Sciences, 77 (1), 142 153.
Buning, H. K., & Lettmann, T. (1999). Propositional Logic: Deduction Algorithms.
Cambridge University Press, New York, NY, USA.
Cepek, O. (1995). Structural Properties Minimization Horn Boolean Functions.
Ph.D. dissertation, Rutgers University, New Brunswick, NJ, October 1995.
Cepek, O., Kucera, P., & Savicky, P. (2012). Boolean functions simple certificate
CNF complexity. Discrete Applied Mathematics, 160 (4-5), 365 382.
Fleischner, H., Kullmann, O., & Szeider, S. (2002). Polynomial-time recognition minimal unsatisfiable formulas fixed clause-variable difference. Theoretical Computer
Science, 289 (1), 503 516.
Flum, J., & Grohe, M. (2006). Parameterized complexity theory, Vol. 3. Springer.
Franco, J., & Van Gelder, A. (2003). perspective certain polynomial-time solvable
classes satisfiability. Discrete Appl. Math., 125 (2-3), 177214.
Garey, M., & Johnson, D. (1979). Computers Intractability: Guide Theory
NP-Completeness. W.H. Freeman Company, San Francisco.
Genesereth, M., & Nilsson, N. (1987). Logical Foundations Artificial Intelligence. Morgan
Kaufmann, Los Altos, CA.
Gursky, S. (2011). Minimization matched formulas. Safrankova, J., & Pavlu, J. (Eds.),
WDS11 Proceedings Contributed Papers: Part Mathematics Computer
Science, pp. 101105, Prague. Matfyzpress.
Hall, P. (1935). representatives subsets. Journal London Mathematical Societysecond Series, s1-10, 2630.
Hammer, P., & Kogan, A. (1993). Optimal compression propositional Horn knowledge
bases: Complexity approximation. Artificial Intelligence, 64, 131 145.
Hammer, P., & Kogan, A. (1995). Quasi-acyclic propositional Horn knowledge bases: Optimal compression. IEEE Transactions Knowledge Data Engineering, 7 (5),
751 762.
722

fiOn Minimum Representations Matched Formulas

Kullmann, O. (2000). Investigations autark assignments. Discrete Applied Mathematics,
107 (13), 99 137.
Kullmann, O. (2003). Lean clause-sets: generalizations minimally unsatisfiable clausesets. Discrete Applied Mathematics, 130 (2), 209 249.
Lovasz, L., & Plummer, M. D. (1986). Matching Theory. North-Holland.
Maier, D. (1980). Minimal covers relational database model. Journal ACM,
27, 664 674.
Monien, B., & Speckenmeyer, E. (1985). Solving satisfiability less 2n steps. Discrete
Applied Mathematics, 10 (3), 287 295.
Szeider, S. (2003). Minimal unsatisfiable formulas bounded clause-variable difference
fixed-parameter tractable. Warnow, T., & Zhu, B. (Eds.), Computing
Combinatorics, Vol. 2697 Lecture Notes Computer Science, pp. 548558. Springer
Berlin Heidelberg.
Szeider, S. (2005). Generalizations matched CNF formulas. Annals Mathematics
Artificial Intelligence, 43 (1-4), 223238.
Szeider, S. (2007). Matched formulas backdoor sets. Marques-Silva, J., & Sakallah,
K. (Eds.), Theory Applications Satisfiability Testing SAT 2007, Vol. 4501
Lecture Notes Computer Science, pp. 9499. Springer Berlin Heidelberg.
Tovey, C. A. (1984). simplified NP-complete satisfiability problem. Discrete Applied
Mathematics, 8 (1), 85 89.
Tseitin, G. S. (1983). complexity derivation propositional calculus. Automation Reasoning, pp. 466483. Springer.
Umans, C. (2001). minimum equivalent DNF problem shortest implicants. J.
Comput. Syst. Sci., 63 (4), 597611.
Umans, C., Villa, T., & Sangiovanni-Vincentelli, A. L. (2006). Complexity two-level
logic minimization. IEEE Trans. CAD Integrated Circuits Systems, 25 (7),
12301246.
Umans, C. M. (1999). Hardness approximating p2 minimization problems. FOCS 99:
Proceedings 40th Annual Symposium Foundations Computer Science, pp.
465474, Washington, DC, USA. IEEE Computer Society.
Urquhart, A. (1995). complexity propositional proofs. Bulletin Symbolic
Logic, 1 (4), pp. 425467.
Urquhart, A. (2011). depth resolution proofs. Stud. Log., 99 (1-3), 349364.

723

fiJournal Artificial Intelligence Research 51 (2014) 579-603

Submitted 4/14; published 11/14

Agent Left Behind: Dynamic Fair Division Multiple Resources
Ian Kash

IANKASH @ MICROSOFT. COM

Microsoft Research Cambridge, UK

Ariel D. Procaccia

ARIELPRO @ CS . CMU . EDU

Carnegie Mellon University, USA

Nisarg Shah

NKSHAH @ CS . CMU . EDU

Carnegie Mellon University, USA

Abstract
Recently fair division theory emerged promising approach allocation multiple
computational resources among agents. reality agents present system
simultaneously, previous work studied static settings relevant information known
upfront. goal better understand dynamic setting. conceptual level, develop
dynamic model fair division, propose desirable axiomatic properties dynamic resource
allocation mechanisms. technical level, construct two novel mechanisms provably
satisfy properties, analyze performance using real data. believe
work informs design superior multiagent systems, time expands scope
fair division theory initiating study dynamic fair resource allocation mechanisms.

1. Introduction
question fairly divide goods resources subject intellectual curiosity
millennia. early solutions traced back ancient writings, rigorous approaches
fairness proposed late mid Twentieth Century, mathematicians social
scientists. time, fair division emerged influential subfield microeconomic theory.
last years fair division also attracted attention AI researchers (see, e.g., Chevaleyre, Endriss, Estivie, & Maudet, 2007; Procaccia, 2009; Chen, Lai, Parkes, & Procaccia, 2010;
Moulin, 2003; Brams & Taylor, 1996), envision applications fair division multiagent systems (Chevaleyre, Dunne, Endriss, Lang, Lematre, Maudet, Padget, Phelps, Rodrguez-Aguilar, &
Sousa, 2006). However, fair division theory seen relatively applications date.
recently exciting combination technological advances theoretical
innovations pointed way towards concrete applications fair division. modern data
centers, clusters, grids, multiple computational resources (such CPU, memory, network
bandwidth) must allocated among heterogeneous agents. Agents demands resources
typically highly structured, explain below. Several recent papers (Gutman & Nisan, 2012;
Ghodsi, Zaharia, Hindman, Konwinski, Shenker, & Stoica, 2011; Parkes, Procaccia, & Shah, 2014;
Dolev, Feitelson, Halpern, Kupferman, & Linial, 2012) suggest classic fair division mechanisms
possess excellent properties environments, terms fairness guarantees well
game-theoretic properties.
Nevertheless, aspects realistic computing systems beyond current scope fair
division theory. Perhaps importantly, literature capture dynamics
systems. Indeed, typically case agents present system
c
2014
AI Access Foundation. rights reserved.

fiK ASH , P ROCACCIA , & HAH

given time; agents may arrive depart, system must able adjust allocation
resources. Even conceptual level, dynamic settings challenge premises fair
division theory. example, one agent arrives another, first agent intuitively
priority; fairness mean context? introduce concepts necessary
answer question, design novel mechanisms satisfy proposed desiderata.
contribution therefore twofold: design realistic resource allocation mechanisms
multiagent systems provide theoretical guarantees, time expand scope
fair division theory capture dynamic settings.
1.1 Overview Model Results
previous papers (e.g., Ghodsi et al., 2011; Parkes et al., 2014), assume agents demand
resources fixed proportions. Leontief preferences known economics
easily justified typical settings agents must run many instances single task (e.g.,
map jobs MapReduce framework). Hence, example, agent requires twice much
CPU RAM run task prefers allocated 4 CPU units 2 RAM units 2 CPU units
1 RAM unit, indifferent former allocation 5 CPU units 2 RAM units.
consider environments agents arrive time (but depart see Section 7
additional discussion point). aim design resource allocation mechanisms make
irrevocable allocations, i.e., mechanism allocate resources agent time,
never take resources back.
adapt prominent notions fairness, efficiency, truthfulness dynamic settings.
fairness, ask envy freeness (EF), sense agents like allocation best;
sharing incentives (SI), agents prefer allocation proportional share
resources. also seek strategyproof (SP) mechanisms: agents cannot gain misreporting
demands. Finally, introduce notion dynamic Pareto optimality (DPO): k agents
entitled k/n resource, allocation dominated (in sense
formalized later) allocations divide entitlements. first result (in Section 3)
impossibility: DPO EF incompatible. proceed relaxing properties.
Section 4, relax EF property. new dynamic property, call dynamic EF
(DEF), allows agent envy another agent arrived earlier, long former agent
allocated resources latter agents arrival. construct new mechanism, DYNAMIC DRF,
prove satisfies SI, DEF, SP, DPO.
Section 5, relax DPO property. cautious DPO (CDPO) notion allows allocations
compete allocations ultimately guarantee EF, regardless demands
future agents. design mechanism called C AUTIOUS LP, show satisfies SI, EF, SP,
CDPO. sense, theoretical results tight: EF DPO incompatible, relaxing
one two properties sufficient enable mechanisms satisfy both, conjunction
SI SP.
Despite assumptions imposed theoretical model, believe new mechanisms compelling, useful guides design practical resource allocation mechanisms
realistic settings. Indeed, Section 6, test mechanisms real data obtained trace
workloads Google cluster, obtain encouraging results.
580

fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES

1.2 Related Work
Walsh (2011) proposed problem fair cake cutting agents arrive, take piece cake,
immediately depart. cake cutting setting deals allocation single, heterogeneous
divisible resource; contrast setting, deals multiple, homogeneous divisible resources. Walsh suggested several desirable properties cake cutting mechanisms setting,
showed adaptations classic mechanisms achieve properties (Walsh also pointed
allocating whole cake first agent achieves properties). particular,
notion forward envy freeness, discussed below, related notion dynamic envy
freeness.
networking community studied problem fairly allocating single homogeneous
resource queuing model agents task requires given number time units
processed. words, models tasks processed time, demands stay fixed,
dynamics agent arrivals departures. well-known fair queuing
solution (Demers, Keshav, & Shenker, 1989) allocates one unit per agent successive round-robin
fashion. solution also analyzed economists (Moulin & Stong, 2002).
Previous papers allocation multiple resources study static setting. example,
Ghodsi et al. (2011) proposed dominant resource fairness (DRF) mechanism, guarantees
number desirable theoretical properties. Li Xue (2013) presented characterizations
mechanisms satisfying various desiderata Wong et al. (2012) analyzed classic tradeoff
fairness efficiency, generic frameworks capture DRF special case.
Parkes et al. (2014) extended DRF several ways, particular studied case indivisible
tasks. Finally, DRF also extended queuing domain (Ghodsi, Sekar, Zaharia, &
Stoica, 2012) incorporate job placement considerations (Ghodsi, Zaharia, Shenker, & Stoica,
2013), generalizations also use static setting. Recently, Zahedi Lee (2014) applied
concept Competitive Equilibrium Equal Outcomes (CEEI) case Cobb-Douglas
utilities achieve properties similar DRF. empirically show utilities well
suited modeling user preferences hardware resources cache capacity memory
bandwidth. Dolev et al. (2012) defined notion fairness different one considered
DRF. also proved fair allocation according new notion always guaranteed
exist static setting. Gutman Nisan (2012) gave polynomial time algorithm find
allocation, also considered generalizations DRF general model utilities.
elaborate several results below.

2. Preliminaries
setting, agent task requires fixed amounts different resources. utility
agent depends quantity (possibly fractional) tasks execute given
allocated resources. Formally, denote set agents N = {1, . . . , n}, set resources
R, |R| = m. Let Dir denote ratio maximum amount resource r agent
use given amounts resources present system total amount resource
available system, either allocated free. words, Dir fraction resource r
required agent i. Following Ghodsi et al. (2011), dominant resource agent defined
resource r maximizes Dir , fraction dominant resource allocated agent
called dominant share. Following Parkes et al. (2014), (normalized) demand vector agent
given di = hdi1 , . . . , dim i, dir = Dir /(maxr0 Dir0 ) resource r. Let
581

fiK ASH , P ROCACCIA , & HAH

set possible normalized demand vectors. Let dk = hd1 , . . . , dk denote demand vectors
agents 1 k. Similarly, let d>k = hdk+1 , . . . , dn denote demand vectors agents
k + 1 n.
AnP
allocation allocates fraction Air resource r agent i, subject feasibility condition Air 1 r R. Throughout paper assume resources divisible
agent requires positive amount resource, i.e., dir > 0 N
r R. allocations, model preferences coincides domain Leontief
preferences, utility agent allocation vector Ai given
ui (Ai ) = max{y R+ : r R, Air dir }.
words, utility agent fraction dominant resource actually use, given
proportional demands allocation various resources. However, rely
interpersonal comparison utilities; agents utility function simply induces ordinal preferences
allocations, exact value irrelevant.
say allocation Pareto-dominated another allocation A0 ui (A0i ) ui (Ai )
every agent i, uj (A0j ) > uj (Aj ) agent j. allocations agents N
A0 agents N , say A0 extension
A0ir Air every agent every resource r. = , simply say A0
extension A. allocation called non-wasteful every agent exists R+
r R, Air = dir . non-wasteful allocation, utility agent share
dominant resource allocated agent. Also, non-wasteful allocation
N,
ui (A0i ) > ui (Ai ) r R, A0ir > Air .

(1)

3. Dynamic Resource Allocation: New Model
consider dynamic resource allocation model agents arrive different times
depart (see Section 7 discussion point). assume agent 1 arrives first, agent
2, general agent k arrives agents 1, . . . , k 1; say agent k arrives step k.
agent reports demand arrives demand change time. Thus,
step k, demand vectors dk known, demand vectors d>k unknown. dynamic resource
allocation mechanism operates follows. step k, mechanism takes input reported
demand vectors dk outputs allocation Ak agents present system. Crucially,
every step k 2, every agent
assume allocations irrevocable, i.e., Akir Ak1
ir
k 1, every resource r. also assume mechanism knows total number
agents n advance.
Irrevocability justified various settings, e.g., cases resources committed
long-term projects. One example research cluster shared faculty members
university. cluster, total number faculty members access cluster
(denoted n setting) known mechanism advance assume model.
Another important setting irrevocability becomes necessary assumption case divisible consumable resources. case, agent may consume resources receives certain
step, cannot reclaimed later on.
582

fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES

Previous work static resource allocation (e.g., Ghodsi et al., 2011; Parkes et al., 2014) focused
designing mechanisms satisfy four prominent desiderata. Three two fairness
properties one game-theoretic property immediately extend dynamic setting.
1. Sharing Incentives (SI). say dynamic allocation mechanism satisfies SI ui (Aki )
ui (h1/n, . . . , 1/ni) steps k agents k. words, agent arrives
receives allocation likes least much equal split resources.
models setting agents made equal contributions system hence
equal entitlements. cases, contributions typically recorded, allows
mechanism know total number agents n advance, assumed setting.
2. Envy Freeness (EF). dynamic allocation mechanism EF ui (Aki ) ui (Akj ) steps
k agents i, j k, is, agent present would never prefer allocation
another agent.
3. Strategyproofness (SP). dynamic allocation mechanism SP agent misreport
demand vector strictly better step k, regardless reported demands
agents. Formally, dynamic allocation mechanism SP agent N
step k, Aki allocation agent step k agent reports true demand vector
Bki allocation agent step k agent reports different demand vector
(in cases agents report true demand vectors), ui (Aki ) ui (Bki ).
avoid introducing additional notations required later.
static setting, fourth prominent axiom, Pareto optimality (PO), means mechanisms allocation Pareto dominated allocation. course, dynamic setting
unreasonable expect allocation early stages Pareto undominated, need
save resources future arrivals (recall allocations irrevocable). believe though
following definition naturally extends PO dynamic setting.
4. Dynamic Pareto Optimality (DPO). dynamic allocation mechanism DPO step k,
allocation Ak returned mechanism Pareto dominated allocation
Bk allocates (k/n)-fraction resource among k agents present
system. Put another way, step allocation Pareto dominated
allocation redistributes collective entitlements agents present
system among agents.
straightforward verify non-wasteful mechanism (a mechanism returning nonwasteful allocation step) satisfies DPO allocation returned mechanism step k uses least (k/n)-fraction least one resource (the assumption strictly
positive demands plays role here).
moving possibility impossibility results, give examples illustrate
various combinations properties constrain allocation resources.
Example 1 (Satisfying Sharing Incentives (SI) Dynamic Pareto Optimality (DPO)).
paper, consider non-wasteful allocations. Hence, described above, DPO equivalent
allocating least (k/n)-fraction least one resource every step k, allocations
proportional. hand, mechanism seeks satisfy SI, cannot allocate
583

fiK ASH , P ROCACCIA , & HAH

(k/n)-fraction resource step k. Indeed, (k/n)-fraction resource r
allocated step k, every agent arriving step k reports r dominant resource,
mechanism would enough resource r left allocate least (1/n)fraction r, required SI. Thus, non-wasteful mechanism satisfying SI DPO must
allocate, every step k, exactly (k/n)-fraction resource (k/n)-fraction
every resource. words, every step k, mechanism pool available
resources contains (k/n)-fraction resource, minus fraction already allocated
allocated k agents currently present. mechanism allocate
pool, must exhaust least one resource pool.
Example 2 (Understanding Strategyproofness (SP)). example, take mechanism
may seem SP first glance, show violates definition SP. simplicity,
allow agents possibly zero demands resources example. allows beneficial manipulations following simple mechanism, call
DYNAMIC ICTATORSHIP. (We note DYNAMIC ICTATORSHIP otherwise strategyproof
strictly positive demands see discussion following Theorem 3.) step k, mechanism allocates 1/n share resource agent k, takes back shares different resources
agent cannot use, allocates resources k present agents order
arrival using serial dictatorship, is, allocates agent many resources agent
use, proceeds next agent. mechanism keeps allocating k/n share
least one resource allocated. Note mechanism trivially satisfies SI allocates
resources valuable equal split agent soon arrives. mechanism would
satisfy DPO standard setting non-zero demands, non-wasteful every
step k allocates k/n fraction least one resource. Intuitively, seems first agent
gain reporting false demand vector round gets pick first
allowed take much use available pool resources. show
intuition incorrect. Let us denote pool resources available mechanism step
vector fraction available resource. Consider case four agents (agents 1, 2,
3, 4), three resources (R1 , R2 , R3 ). Let true demand vectors agents
follows:
d1 = h1, 0.5, 0.5i, d2 = h0, 1, 1i, d3 = h1, 0.5, 0i, d4 = h0, 1, 0.5i.
Figure 1 shows allocations returned DYNAMIC ICTATORSHIP various steps
agents report true demand vectors. Now, suppose agent 1 raises demand R3 reporting
false demand vector h1, 0.5, 1i. case allocations returned mechanism various
steps shown Figure 2. see manipulation makes agent 1 strictly worse
step 2, strictly better final step. definition SP requires agent
able benefit step process lying thus DYNAMIC ICTATORSHIP SP.
3.1 Impossibility Result
Ideally, would like design dynamic allocation mechanism SI, EF, SP, DPO.
However, show even satisfying EF DPO simultaneously impossible.
Theorem 3. Let n 3 2. dynamic resource allocation mechanism satisfies EF
DPO.
584

fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES

1

1

1

1

3
4

3
4

3
4

3
4

1/4

1
2

1
2

1
2

1
2

1/4

1
4

0

1/4

1/8
1/8

1/8
1/8

R1

R2

R3

1/4

1/4

1/2

1/4

1/4

R1

R2

R3

1
4

0

(a) Step 1

1/8
1/4
3/8

3/8

1/2

1/4

1/4

R1

R2

R3

1
4

0

1/8

0

1/4
1/8

Agent 2
Agent 3

3/8

3/8

Agent 4

1/2

1/4

1/4

Unallocated resource
available pool

R1

R2

R3

1
4

(c) Step 3

(b) Step 2

Agent 1
1/4
1/8

(d) Step 4

Figure 1: Allocations returned DYNAMIC ICTATORSHIP agent 1 reports true demand vector.
1

1

1

1

3
4

3
4

3
4

3
4

1
2

1
2

1
4

0

1
4

1/4

1/8
1/8

1/4

R1

R2

R3

(a) Step 1

0

1
2

1/8
1/4

1/4

1/4

1/4
1/8

1/4

R1

R2

R3

(b) Step 2

1/4

1/4

1/4

1/16

1/4
1/8

1/8
1/4

1/4

1/2

R1

R2

R3

(c) Step 3

0

Agent 2

Agent 4

1
4

1/2

Agent 1

Agent 3

1
2

1/4

1/4

1
4

0

1/8
1/8

1/8

5/8

5/16

5/8

R1

R2

R3

Unallocated resource
available pool

(d) Step 4

Figure 2: Allocations returned DYNAMIC ICTATORSHIP agent 1 manipulates.
Proof. Consider setting three agents two resources. Agents 1 2 demand vectors
h1, 1/9i h1/9, 1i, respectively (i.e., d11 = 1, d12 = 1/9, etc.). step 2 (after second
agent arrives), least one two agents must allocated least x = 3/5 share
dominant resource. Suppose contradiction two agents allocated x0 x00 shares
dominant resources 0 < x0 , x00 < x. Then, total fractions two resources
allocated step 2 would x0 + x00 (1/9) x00 + x0 (1/9), less x + x (1/9) = 2/3,
violating DPO. Without loss generality, assume agent 1 allocated least x = 3/5 share
dominant resource (resource 1) step 2. agent 3 reports demand vector h1, 1/9i
identical agent 1 allocated 2/5 share dominant resource
(resource 1), would envy agent 1.
easy extend argument case n > 3, adding n 3 agents demand
vectors identical demand vector agent 3. again, verified
end step 2, least one first two agents (w.l.o.g., agent 1) must allocated least 9/(5n)
share dominant resource. take remaining resources (in particular, 19/(5n)
share resource 1), divide among remaining n 2 agents demand vectors
identical agent 1, least one get (1 9/(5n))/(n 2) < 9/(5n)
share dominant resource, envy agent 1. extend case > 2, let agents
negligibly small demands additional resources. (Proof Theorem 3)
interesting note either EF DPO dropped, remaining three axioms
easily satisfied. example, trivial mechanism E QUAL PLIT gives every agent
1/n share resource arrives satisfies SI, EF SP. Achieving SI, DPO,
585

fiK ASH , P ROCACCIA , & HAH

SP also simple. Indeed, consider DYNAMIC ICTATORSHIP mechanism Example 2.
example explains DYNAMIC ICTATORSHIP satisfies SI DPO. Even though
DYNAMIC ICTATORSHIP SP possibly zero demands (as shown example),
clearly SP strictly positive demands (as assumed throughout paper). agent k arrives
step k, allocated 1/n share dominant resource (and resources proportion),
subsequently agent 1 allocated resources k/n share least one resource exhausted.
Since every agent requires exhausted resource due strictly positive demands, allocation
stops. summary, agents except agent 1 receive exactly 1/n share dominant resource
arrive, receive resources later on; hence, cannot gain reporting
false demand vector. step k, agent 1 receives much resources pool
resources remain allocating agents 2 k 1/n share dominant resource
original pool contains k/n share resource. Therefore, agent 1 also cannot
gain manipulation.
E QUAL PLIT DYNAMIC ICTATORSHIP satisfy maximal subsets proposed desiderata, neither compelling mechanism. Since mechanisms permitted
dropping EF DPO entirely, instead explore relaxations EF DPO rule mechanisms guide us towards compelling mechanisms.

4. Relaxing Envy Freeness
Recall DPO requires mechanism allocate least k/n fraction least one resource
step k, every k {1, . . . , n}. Thus mechanism sometimes needs allocate large amount
resources agents arriving early, potentially making impossible mechanism prevent
late agents envying early agents. words, agent enters system
may envy agent j arrived did; inevitable order able satisfy DPO.
However, would unfair agent agent j allocated resources since agent arrived
still envied j. distill intuition, introduce following dynamic version EF.
20 . Dynamic Envy Freeness (DEF). dynamic allocation mechanism DEF step
agent envies agent j j arrived j allocated
resources since arrived. Formally, every k {1, . . . , n}, ui (Akj ) > ui (Aki ) j <
Akj = Ai1
j .
Walsh (2011) studied dynamic cake cutting setting proposed forward EF, requires
agent envy agent arrived later. notion weaker DEF
rule case agent envies agent j arrived earlier j received
resources since arrived. setting, even trivial mechanism DYNAMIC ICTATORSHIP (see
Section 3.1) satisfies forward EF, fails satisfy stronger notion DEF.
next construct dynamic resource allocation mechanism DYNAMIC DRF achieves
relaxed fairness notion DEF, together SI, DPO, SP. mechanism given Algorithm 1.
Intuitively, step k mechanism starts current allocation among present
agents keeps allocating resources agents minimum dominant share
rate, k/n fraction least one resource allocated. Always allocating agents
minimum dominant share ensures agents allocated resources
586

fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES

ALGORITHM 1: DYNAMIC DRF
Data: Demands
Result: Allocation Ak step k
k 1;
k n
{xki }ki=1 Solution LP box below;
Akir xki dir , k;
k k + 1;
end
Maximize k
subject
xki k , k
xki xk1
, k 1
Pk ik
i=1 xi dir k/n, r R

envied. water-filling mechanism dynamic adaptation dominant resource fairness
(DRF) mechanism proposed Ghodsi et. al. (2011). See Figure 3 example.
1

1

1

2
3

2
3

2
3

2/9
1
3

0

2/9
1/3

1
3

1/3

1/6

1/4

R1

R2

R3

(a) Step 1

0

4/9

4/9

1/3

R1

R2

R3

1/6

1/6

2/9

2/9

1
3

0

(b) Step 2

1/3

Agent 1

1/3

Agent 2

4/9

4/9

1/3

Agent 3

R1

R2

R3

Dominant share

(c) Step 3

Figure 3: Allocations returned DYNAMIC DRF various steps 3 agents demands d1 =
h1, 1/2, 3/4i, d2 = h1/2, 1, 3/4i, d3 = h1/2, 1/2, 1i, three resources R1 , R2 ,
R3 . Agent 1 receives 1/3 share dominant resource step 1. step 2, water-filling drives
dominant shares agents 1 2 4/9. step 3, however, agent 3 receive
1/3 dominant share allocations agents 1 2 remain unchanged.

Theorem 4. DYNAMIC DRF satisfies SI, DEF, DPO, SP, implemented polynomial time.
Proof. First show DYNAMIC DRF satisfies SI. need prove xki 1/n
agents k every step k {1, . . . , n}. prove induction k. base case
k = 1, easy see x11 = 1/n 1 = 1/n solution LP DYNAMIC DRF
hence optimal solution satisfies x11 1 1/n (in fact, equality). Assume
true step k 1 let us prove claim step k, k {2, . . . , n}. step k, one
feasible solution LP given xki = xk1
agents k 1, xkk = 1/n k = 1/n.

see this, note trivially satisfies first two constraints LP, induction
587

fiK ASH , P ROCACCIA , & HAH

hypothesis xk1
1/n k 1. Furthermore, proposed feasible solution,

r R
k
X
i=1

xki dir =

k1
X
i=1

xk1
dir +


k1 1
k
1
dkr
+ ,
n
n
n
n

first transition follows construction feasible solution second transition holds {xk1
}k1

i=1 satisfies LP step k 1, particular third constraint
LP. Since feasible solution achieves k = 1/n, optimal solution achieves k 1/n.
Thus optimal solution xki k 1/n k, requirement SI.
Next show DYNAMIC DRF satisfies DPO. Observe step k, third constraint
LP must tight least one resource optimal solution (otherwise every xki along
k increased sufficiently small quantity, contradicting optimality k ).
Thus, step k (non-wasteful) mechanism allocates k/n fraction least one resource,
implies mechanism satisfies DPO.
prove mechanism satisfies DEF SP, first prove several useful lemmas
allocations returned mechanism. proof below, k xki refer optimal
solution LP step k. Furthermore, assume xki = 0 agents > k (i.e., agents
present system allocated resources). begin following lemma,
essentially shows agent allocated resources step using water-filling,
agents dominant share step minimum among present agents.
Lemma 5. every step k {1, . . . , n}, holds xki = max(M k , xk1
) agents k.

Proof. Consider step k {1, . . . , n}. first second constraints LP
= 0), thus xki max(M k , xik1 )
evident xki k xki xk1
(note xk1

k
k. Suppose contradiction xki > max(M k , xk1
) k. xki

reduced sufficiently small > 0 without violating constraints. makes third
constraint LP loose least dir , every resource r R. Consequently, values
xkj j 6= k increased sufficiently small > 0 without violating
third constraint LP. Finally, (and correspondingly ) chosen small enough
xki k violated. follows value k increased, contradicting
optimality k . (Proof Lemma 5)
Next show step k, dominant shares agents 1 k monotonically
non-increasing time arrival. intuitive every step k, agent k enters
zero dominant share subsequently perform water-filling, hence monotonicity preserved.
Lemma 6. agents i, j N < j, xki xkj every step k {1, . . . , n}.
Proof. Fix two agents i, j N < j. prove lemma induction k. result trivially holds k < j since xkj = 0. Assume xk1
xk1
k {j, . . . , n}. step

j
k1
k1
k
k
k
k
k, xi = max(M , xi ) max(M , xj ) = xj , first last transition
follow Lemma 5 second transition follows induction hypothesis. (Proof
Lemma 6)
588

fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES

following lemma shows agent j greater dominant share agent
step, j must arrived j must allocated resources since
arrived. Observe close requirement DEF.
Lemma 7. step k {1, . . . , n}, xkj > xki agents i, j k, j <
xkj = xi1
j .
Proof. First, note j < trivially follows Lemma 6. Suppose contradiction
xkj > xi1
(it cannot smaller allocations irrevocable). exists step
j



{i, . . . , k} xtj > xt1
j . Lemma 5 implies xj = xi , last transition follows xti satisfies second constraint LP step (note t). However, xtj xti due Lemma 6. Thus, xtj = xti . using Lemma 5, xt+1
= max(M t+1 , xtj ) =
j
t0
t0
max(M t+1 , xti ) = xt+1
. Extending argument using simple induction shows xj = xi
every step t0 t, particular, xkj = xki , contradicting assumption. (Proof Lemma 7)
proceed show DYNAMIC DRF satisfies DEF. need prove step k
{1, . . . , n} agents i, j k, agent envies agent j step k (i.e., ui (Akj ) > ui (Aki )),
k
k
k
k
j < xkj = xi1
j . First, note ui (Aj ) > ui (Ai ) trivially implies xj > xi , otherwise
dominant resource ri agent i, would Akir = xki xkj xkj djri = Akjr


agent would envy agent j. DEF follows Lemma 7.
prove DYNAMIC DRF SP, suppose contradiction agent N report
untruthful demand vector d0i agent strictly better least one step. Let k
first step. Denote xtj dominant share agent j step manipulation (for
agent i, share dominant resource untruthful demand vector) similarly,
denote value optimal solution LP step manipulation.
Lemma 8. xkj xkj every agent j k.
Proof. agent j xkj > xki ,
xkj = xji1 = xji1 xkj .
Here, first transition follows Lemma 7, second transition holds manipulation
agent affect allocation step 1, third transition follows LP.
agent j xkj xki ,
xkj xki < xki = k xkj .
second transition true xki xki agent could better true
dominant share receives manipulation would received without manipulation. justify third transition, note agent must allocated resources step k
manipulation. k = i, trivial, k > i, follows otherwise k would
first step agent strictly better would ui (Ak1
) = ui (Aki ) >

k1
k
k
ui (Ai ) ui (Ai ), Ai denotes allocation agent step k manipulation. Thus,
xki > xk1
, third transition follows Lemma 5. last transition holds

k
xj satisfies first constraint LP step k. Thus, conclude xkj xkj agents
j k. (Proof Lemma 8)
589

fiK ASH , P ROCACCIA , & HAH

Now, mechanism satisfies DPO thus allocates least k/n fraction least one
resource step k without manipulation. Let r resource. fraction resource r
allocated step k manipulation
X
X
xkj djr k/n.
xkj djr > xki dir +
xki d0ir +
jk
s.t.j6=i

jk
s.t.j6=i

justify inequality, note xki d0ir > xki dir Equation (1) (as agent strictly better
off), addition xkj xkj every j k. However, shows k/n fraction
resource r must allocated step k manipulation, impossible due third
constraint LP. Hence, successful manipulation impossible, is, DYNAMIC DRF SP.
Finally, note LP linear number variables constraints, therefore mechanism implemented polynomial time. (Proof Theorem 4)

5. Relaxing Dynamic Pareto Optimality
saw (Theorem 3) satisfying EF DPO impossible. explored intuitive
relaxation EF. Despite positive result (Theorem 4), idea achieving absolute fairness
conceptualized EF dynamic setting compelling.
straw man, consider waiting agents arrive using EF static allocation mechanism. However, scheme highly inefficient, e.g., easy see one always
allocate agent least 1/n share dominant resource (and resources proportion)
soon arrives still maintain EF every step. much allocated
step? put forward general answer question using relaxed notion DPO requires
mechanism allocate many resources possible ensuring EF achieved
future, first require following definition. Given step k {1, . . . , n}, define
allocation k present agents demands dk EF-extensible extended
EF allocation n agents demands = (dk , d>k ), possible future demand
vectors d>k Dnk .
40 . Cautious Dynamic Pareto optimality (CDPO). dynamic allocation mechanism satisfies
CDPO every step k, allocation Ak returned mechanism Pareto dominated allocation A0 k agents EF-extensible.
words, mechanism satisfies CDPO every step selects allocation
least generous allocation ultimately guarantee EF, irrespective future demands.
first glance, may obvious CDPO indeed relaxation DPO (i.e., CDPO
implied DPO). However, note DPO requires mechanism allocate least k/n fraction
least one resource r allocation Ak step k, thus allocate least 1/n
fraction resource agent i. alternative allocation Pareto dominates Ak must
also allocate least 1/n fraction r agent i. Consequently, order ensure EF extension
n agents future demands identical demand agent i, alternative
allocation must allocate k/n fraction r , future agent may also require least
1/n fraction r avoid envying agent i. follows alternative allocation cannot Pareto
dominate Ak . Thus, mechanism satisfies CDPO.
590

fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES

Recall DYNAMIC DRF extends water-filling idea static DRF mechanism (Ghodsi
et al., 2011) dynamic setting. DYNAMIC DRF unable satisfy original EF,
satisfy DPO every step k needs allocate resources k/n fraction resource
allocated. wish modify DYNAMIC DRF focus competing EF-extensible
allocations, way achieves CDPO EF (as well properties).
main technical challenge checking allocation step k violates EF-extensibility.
Indeed, uncountably many possibilities future demands d>k EF
extension needs guaranteed EF-extensible allocation! course, checking possibilities explicitly feasible. Ideally, would like check small number possibilities.
following lemma establishes sufficient verify EF extension exists
assumption future agents demand vector moreover identical
demand vector one present agents.
Lemma 9. Let k number present agents, dk demands reported present
agents, EF allocation k present agents. EF-extensible
exists EF extension n agents demands = (dk , d>k ) future
demands d>k D0 , D0 = {hd1 ink , hd2 ink , . . . , hdk ink }.
prove lemma, first introduce notion minimum EF extension. Intuitively,
minimum EF extension smallest EF extension (allocating least resources) given EF
allocation larger set agents. Formally, let EF allocation set agents N
EF extension set agents N (S ). called minimum
EF extension EF extension A0 , A0 extension
. show minimum EF extension exists exhibits simple structure.
Lemma 10. Let EF allocation set agents N let xi dominant
share agent A. Let N let allocation
xi dominant share agent . Let xi = xi S, xi = maxjS yij
\ S, yij = xj minrR djr /dir . minimum EF extension .
Proof. agent dominant share xi avoid envying agent j dominant share xj ,
must exist r R xi dir xj djr , is, xi xj djr /dir . follows xi
xj minrR djr /dir , thus minimum dominant share given yij = xj minrR djr /dir .
easy argue EF extension A0 must allocate least xi dominant
share agent , \ S, thus A0 must extension .
remains prove EF. First prove intuitive result regarding minimum
dominant share agent needs avoid envying agent j, namely yij . claim every r R,
yij dir xj djr .

(2)

Formally, r R,
djr0
djr
dir xj
dir = xj djr .
r R dir0
dir

yij dir = xj min
0

Therefore, prevent agent envying agent j, need allocate least xj djr
fraction resource r agent r R. Next show EF, i.e., agent envies
agent j . consider four cases.
591

fiK ASH , P ROCACCIA , & HAH

Case 1: j S. case trivial identical EF.
Case 2: \ j S. case also trivial receives least yij fraction
dominant resource.
Case 3: j \ S. must xj = yjt S. Agent envy
agent A, hence . Thus, exists resource r R Air Atr Ajr ,
last step follows Equation (2). Thus, agent envy agent j.
Case 4: \ j \ S. Similarly Case 3, let xj = yjt S. xi yit ,
agent envy agent . Thus, exists resource r Air Atr Ajr ,
last step follows Equation (2).
Therefore, EF extension already established EF
extension must extension . conclude minimum EF extension
. (Proof Lemma 10)
hard see construction minimum EF extension exists,
unique. ready prove Lemma 9.
Proof Lemma 9. direction proof trivial. prove part, prove
contrapositive. Assume exist future demand vectors d>k Dnk
exist EF extension N demands = (dk , d>k ). want show
exists d0>k D0 EF extension well.
Let K = {1, . . . , k} N \ K = {k + 1, . . . , n}. Denote minimum EF extension
N demands . Let dominant share agent K xi dominant
share agent j N xj .

EF extension N demands
Pn feasible, hence must infeasible too.
Therefore, exists resource r i=1 xi dir > 1. Note every agent j N \K,
exists agent K xj = xi minr0 R dir0 /djr0 , hence xj djr xi dir
Equation (2). Taking maximum K, get xj djr maxiK (xi dir ) every
agent j N \ K. Taking arg maxiK (xi dir ),
1<

n
X

xi dir =

i=1

k
X

xi dir +

i=1



k
X

n
X

xi dir

i=k+1

xi dir + (n k) xt dtr .

i=1

Consider
case
d0>k = hdt ink D0 . minimum EF extension A0 N


ff
demands d0 = dk , d0>k allocates xi dominant share every K (same A) allocates
exactly
xt dominant share every j N \ K. Thus, fraction resource r allocated A0
Pk
0
i=1 xi dir + (n k) xt dtr > 1, implying minimum EF extension d>k infeasible.
0
conclude feasible EF extension d>k , required. (Proof Lemma 9)
equivalent condition Lemma 9 provides us k linear constraints
checked determine whether allocation k agents EF-extensible. Using machinery, write small linear program (LP) begins allocation chosen
previous step (recall allocations irrevocable), gives agent k jump start
envy agents 1 k 1, uses water-filling allocate resources similarly
592

fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES

DYNAMIC DRF, subject constraint allocation stays EF-extensible. intuition
formalized via mechanism C AUTIOUS LP, given Algorithm 2.
ALGORITHM 2: C AUTIOUS LP
Data: Demands
Result: Allocation Ak step k
k 1;
k n
{xki }ki=1 Solution LP box below;
Akir xki dir , k;
k k+1
end
Maximize k
subject
xki k , k
xki xk1
, k 1




min

/d
xkk maxik1 xk1
rR ir kr

Pk
k
k
i=1 xi dir + (n k) xt dtr 1, k, r R
mechanisms third LP constraint jump-starts agent k level envy earlier
agents, fourth LP constraint derived Lemma 9. see mechanism satisfies
CDPO, observe step k EF-extensible allocation A0 Pareto dominates
allocation Ak returned mechanism, (by Lemma 9) A0 must also satisfy LP step k.
However, shown allocation feasible region LP Pareto dominate
Ak . Indeed, allocation feasible region dominate Ak , could redistribute
resources agent strictly better obtain feasible allocation value
k higher optimal solution. also easy see intuitively C AUTIOUS LP
EF: initial allocation agent k achieves EF allocation k agents, water-filling
preserves EF always allocates agents minimum dominant share. equally
straightforward show C AUTIOUS LP also satisfies SI. Establishing SP requires work,
proof mainly modification proof Theorem 4. therefore able establish
following theorem, formalizes guarantees given C AUTIOUS LP.
Theorem 11. C AUTIOUS LP satisfies SI, EF, CDPO, SP, implemented polynomial
time.
Proof. proof along lines proof Theorem 4. now, assume LP
feasible step thus mechanism return allocation step (we show
below). LP step k, let


k1
k
E = max xi min dir /dkr .
rR

ik1

Intuitively, E k jump start agent k requires beginning step k envy free
allocations agents 1 k 1 step k 1.
593

fiK ASH , P ROCACCIA , & HAH

Proof CDPO: First show C AUTIOUS LP satisfies CDPO. Assume contradiction,
step k {1, . . . , n}, alternative EF-extensible allocation A0 k present
agents Pareto dominates allocation Ak returned mechanism. Let x0i dominant
share agent A0 , k. Since A0 Pareto dominates Ak , x0i xki every
k. trivially implies A0 also satisfies first three constraints LP step k.
Moreover, since A0 EF-extensible, also satisfies fourth constraint LP step k
fourth constraint requires EF extension exist specific cases (in particular, requires
minimum EF extension thus EF extension n agents exist future demand
vectors identical demand vector present agent). Thus, A0 feasible region
LP Pareto dominates optimal solution Ak . Now, taking back extra resources
A0 allocates agents compared Ak shows fourth constraint tight Ak
value r (the assumption strictly positive demands crucial here). However, implies
allocation Ak , every xki correspondingly k increased sufficiently small
quantity still satisfying LP step k, contradicts optimality Ak . Thus,
alternative EF-extensible allocation Pareto dominate allocation given mechanism
step, i.e., C AUTIOUS LP satisfies CDPO.
Proof SI: Next, show C AUTIOUS LP satisfies SI. show induction step
k. base case k = 1, easy show setting x11 = 1/n k = 1/n satisfies
LP step 1; trivially satisfies first three constraints LP fourth constraint,
observe
1
1
dir + (n 1) dir = dir 1, r R.
n
n
Therefore, optimal solution, 1 1/n thus x11 1/n (in fact, equality holds).
consider step k {2, . . . , n}. induction hypothesis, assume xti 1/n
agents t, every step k 1. want show xki 1/n agents k.
Consider two cases.
1. E k 1/n. Observe xk1
1/n k 1 due induction hypothesis. Thus,

using second third constraints LP step k, xki 1/n k.
2. E k < 1/n. first show xki = xk1
k 1, xkk = 1/n k = 1/n

feasible region LP step k. Note assignment trivially satisfies first three
constraints LP.
fourth constraint, fix r R. Define Tr = maxik1 xik1 dir . First, show
P
k1
k1
dir 1 (n k + 1) max(Tr , 1/n). see this, note {xik1 }k1
i=1 xi
i=1
satisfies LP step k 1 and, particular, fourth constraint LP. Therefore,
k1
X

xk1
dir + (n k + 1) Tr 1 =


i=1

k1
X

xk1
dir 1 (n k + 1) Tr .


i=1

prove
k1
X

xk1
dir 1 (n k + 1) 1/n = (k 1)/n.


i=1

594

(3)

fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES

Suppose contradiction left hand side (k 1)/n. Then, pigeonhole principle, exists agent k 1 xk1
dir 1/n, thus

Tr 1/n. already shown
k1
X

xk1
dir 1 (n k + 1) Tr 1 (n k + 1) 1/n = (k 1)/n,


i=1

contradicting assumption; establishes (3). Thus,
k1
X
i=1

xk1




1
.
dir 1 (n k + 1) max Tr ,
n

Finally, show fourth constraint LP, xkt dtr max(Tr , 1/n). see this,
observe k 1, xkt dtr = xk1
dtr Tr = k, xkt dtr = 1/ndkr 1/n.

Thus, fourth constraint LP satisfied every k every r R.
established C AUTIOUS LP satisfies SI. next goal prove mechanism also satisfies EF SP. proof Theorem 4, first establish several useful lemmas
allocations returned C AUTIOUS LP. proof below, k xki refer optimal
solution LP step k.
begin following lemma (similar Lemma 5), essentially shows
agent allocated resources step k using water-filling (in addition jump-start E k
agent k), agents dominant share step would minimum among present
agents.
Lemma 12. every step k {1, . . . , n}, holds xki = max(M k , xk1
) agents

k
k
k
k 1, xk = max(M , E ).
Proof. Consider step k {1, . . . , n}. first three constraints LP, evident
)
xki k k, xki xk1
k 1 xkk E k . Thus, xki max(M k , xk1


k 1 xkk max(M k , E k ).
Suppose contradiction strict inequality holds agent k. xki
reduced sufficiently small > 0 without violating constraints. makes third
constraint LP loose least dir , every resource r R. Consequently, values
xkj j 6= k increased sufficiently small > 0 without violating
third constraint LP. Finally, (and correspondingly ) chosen small enough
xki k violated. follows value k increased, contradicting
optimality k . (Proof Lemma 12)
Next, formulate equivalent Lemma 6 two separate lemmas. First show
agent greater equal dominant share another agent step (where present),
order preserved future steps. Next show step k, dominant shares
agents 1 k monotonically non-increasing time arrival, except agents
received resources apart jump-start.
Lemma 13. agents i, j N step k max(i, j) (i.e., agents present
step k), xki xkj implies xti xtj k.
595

fiK ASH , P ROCACCIA , & HAH

Proof. Fix two agents i, j N step k max(i, j) xki xkj . use induction
t. result trivially holds = k. Consider > k assume result holds step 1.
t1



Then, since > k max(i, j) know xti = max(xt1
, ) max(xj , ) = xj ,
first last transitions follow Lemma 12 second transition follows
induction hypothesis. (Proof Lemma 13)
Lemma 14. agents i, j N < j step k j, either i)
xki xkj ii) xkj = xjj = E j .
Proof. Fix two agents i, j N < j step k j. Note xkj xjj
E j , first inequality due irrevocability resources last inequality due
Lemma 12. xkj = E j , lemma trivially holds. Assume xkj > E j . Consider first step
xtj > E j (thus j k). = j, xjj > E j . > j, xtj > xt1
j
j definition t. case, Lemma 12 implies xt = xt . Thus
since xt1
=
E
j

j
xtj xti Lemma 13 implies xkj xki . (Proof Lemma 14)
consider equivalent Lemma 7 proof Theorem 4, observe
two cases. agent j greater dominant share agent step, either j
arrived j allocated resources since arrived (as previously),
j arrived allocated resources apart jump-start.
Lemma 15. agents i, j N step k max(i, j) (i.e., agents present),
xkj > xki implies either i) j < xkj = xji1 , ii) j > xkj = xjj = E j .
Proof. Fix two agents i, j N step k max(i, j) xkj > xki . Note
j > Lemma 14 implies xkj = xjj = E j result holds trivially. assume j < i.
Suppose contradiction xkj > xji1 (it cannot smaller allocations irrevocable). exists step {i, . . . , k} xtj > xt1
j . Therefore, Lemma 12 implies



k
xj = xi . Using Lemma 13 shows xj xki , contradiction
assumption xkj > xki . Thus xkj = xi1
j , required. (Proof Lemma 15)
Finally, establish additional lemma helpful proving SP. agents i, j
j > i, jump-start E j agent j requires allocating agent j greater dominant share
agent step j 1, clearly jump-start must due agent j envying
agent l 6= i, l must greater dominant share step j 1. using
Lemma 14 extending argument, eventually trace back agent < i.
show find < jump-start original agent j actually due
agent j envying agent t.
Lemma 16. agents i, j N j > i, E j > xji implies E j = xj1


minrR dtr /djr , agent < i.
Proof. Fix agent N . use induction j {i + 1, . . . , n}. First, prove several
implications hold agent j > i. Recall E j = maxp<j (xj1
minrR dpr /djr ).
p
596

fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES

Thus, E j = xlj1 minrR dlr /djr agent l < j. follow
definition take l < i. Observe
xlj1 xlj1 min dtr /djr = E j > xji xj1
,

rR

(4)

first transition holds since minrR dtr /djr 1 (consider dominant resource
agent j), third transition assumption lemma last transition holds since
allocations irrevocable.
three cases. l < i, done. Further, l 6= since Equation (4) shows
xj1
> xj1
. assume l > i. Note case cannot appear base case

l
j = + 1 since l < j. Therefore, argument given already shows lemma holds
base case j = + 1. induction hypothesis, assume lemma holds
agent l < j. since l > xj1
> xj1
= xll = E l
, Lemma 15 implies xj1

l
l
j1
j1
l
l
l
thus E > xi
xi xi
xi l < j allocations irrevocable. Due
induction hypothesis, exists < E l = xl1
minrR dtr /dlr . prove

E j = xtj1 minrR dtr /djr . Indeed,
min dlr /djr
E j = xj1
l
rR

l

= E min dlr /djr
rR

=

xl1


min dtr /dlr min dlr /djr



xl1


min dtr /djr



xj1


min dtr /djr E j .

rR

rR

rR

rR

Here, fourth transition true r0 R,
dtr0
dtr0 dlr0
dtr
dlr
=

min
min
.
0
0
0
rR dlr rR djr
djr
dlr djr
Taking minimum r0 R, get minrR dtr /djr minrR dtr /dlr minrR dlr /djr .
last transition holds due definition E j . trivial see must equality
every step, E j = xj1
minrR dtr /djr < i, required. (Proof Lemma 16)

Proof LP Feasibility EF: use inductive argument simultaneously show
LP C AUTIOUS LP feasible every step C AUTIOUS LP satisfies EF. Consider
following induction hypothesis: LP step feasible allocation returned
mechanism step EF. base case = 1, LP trivially feasible allocation A1
also trivially EF. Assume hypothesis holds = k 1 step k {2, . . . , n}.
want show hypothesis holds step k.
feasibility, show allocation given xki = xk1
k 1 xkk = E k

k
along = 0 satisfies LP step k. Clearly, satisfies first three constraints LP.
see satisfies fourth constraint, note Ak1 EF allocation due induction
hypothesis. Moreover, satisfies LP step k 1, particular, fourth constraint
LP. Hence Lemma 9 implies Ak1 must EF-extensible allocation. Let dk denote
597

fiK ASH , P ROCACCIA , & HAH

demand reported agent k step k let d>k Dnk . EF extension Ak1
n agents future demands (dk , d>k ) EF extension n agents future
demands d>k . Since holds d>k Dnk , EF-extensible hence satisfies
fourth constraint LP. conclude LP feasible step k.
want show allocation Ak EF allocation. Intuitively, see
mechanism starts EF (it minimum EF extension), uses waterfilling allocate resources way preserves EF. Formally, note dominant shares
allocated agents Ak given Lemma 12. Take two agents i, j k. want show
agent envy agent j step k. Denote dominant share agent l xl ,
i.e., xl = xk1
l k 1 xk = E k . holds
l






djr
djr
xki = max xi , k max xj min
, k max xj , k min
rR dir
rR dir
= xkj min djr /dir ,
rR

first last transitions follow Lemma 12, second transition holds since
allocation EF, third transition holds since quantity minrR djr /dir 1.
Thus, Ak EF. induction, holds LP C AUTIOUS LP feasible every step
C AUTIOUS LP EF.
Proof SP: last task prove C AUTIOUS LP SP. Suppose contradiction
agent N report untruthful demand vector d0i agent strictly better
least one step. Let k first step. Denote xtj dominant share agent j step
manipulation (for agent i, share dominant resource untruthful demand
vector) and, similarly, denote value optimal solution LP step
manipulation.
Lemma 17. xkj xkj every agent j k.
Proof. Fix agent j k. provide case case analysis show lemma holds
case.
1. xkj xki . case,
xkj xki < xki = k xkj .
second transition holds xki xki agent could better
share dominant resource true demand vector receives manipulation
would received without manipulation. justify third transition, note
agent must allocated resources step k manipulation. k = i,
note since E depends allocation step 1 affected due
manipulation agent i, E = E xii < xii Lemma 12 implies
,
xii = . k > xki 6= k , Lemma 12 implies xki = xk1

k ) > u (Ak ) u (Ak1 ), Ak allocation agent step
ui (Ak1
)
=
u
(









k manipulation. is, agent would better manipulation step
k 1, contradiction since k first step. last transition holds
xkj satisfies first constraint LP step k manipulation.
598

fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES

2. xkj > xki . this, three sub-cases.
(a) j < i. xkj = xi1
= xi1
xkj , first transition follows
j
j
due Lemma 15, second transition holds manipulation agent
affect allocations step 1, third transition follows since allocations
irrevocable.
(b) j = i. cannot happen since assumed xkj > xki case.
(c) j > i. Since xkj > xki , Lemma 15 implies xkj = xjj = E j , E j > xki .
using Lemma 16, E j = xj1
minrR dtr /djr < i. Then, xj1



j1
j1
j
k
xt minrR dtr /djr = E > xi xi , first transition follows since
minrR dtr /djr 1 last transition follows since allocations irrevocable. Lemma 15 implies xj1
= xti1 . Putting pieces together,

xkj = E j = xtj1 min dtr /djr = xti1 min dtr /djr = xj1
min dtr /djr

rR



xj1


j

min dtr /djr E
rR

xjj



rR
xkj ,

rR

fifth transition follows since manipulation agent change
allocation step 1, sixth transition follows due definition E j (which
value E j manipulation), seventh transition follows due third
constraint LP step j manipulation, last transition follows since
allocations irrevocable.
Thus, conclude xkj xkj agents j k. (Proof Lemma 17)
Now, optimal solution LP step k without manipulation (i.e., Ak ), fourth
constraint must tight k r R (otherwise xkj every j k k
increased, contradicting optimality k ). Thus,
k
X

xkj djr + (n k) xkt dtr = 1.

j=1

consider fourth constraint LP step k manipulation values
r. simplicity notation, let d0jr = djr j 6= i. Then,
k
X
j=1

xkj d0jr + (n k) xkt d0tr >

k
X

xkj djr + (n k) xkt dtr = 1.

j=1

justify inequality, note xki d0ir > xki dir Equation (1) (as agent strictly better
off), j k j 6= i, xkj d0jr = xkj djr xkj djr Lemma 17. However,
shows allocation step k manipulation violates fourth constraint LP,
impossible. Hence, successful manipulation impossible, is, C AUTIOUS LP SP.
Finally, note every step LP O(n) variables O(n m) constraints,
n steps. Hence, mechanism implemented polynomial time. (Proof
Theorem 11)
599

fiK ASH , P ROCACCIA , & HAH

6. Experimental Results
presented two potentially useful mechanisms, DYNAMIC DRF C AUTIOUS LP,
theoretical guarantees. next goal analyze performance mechanisms
real data, two natural objectives: sum dominant shares (the maxsum objective)
minimum dominant share (the maxmin objective) agents present system.1
compare objective function values achieved two mechanisms certain lower
upper bounds. Since mechanisms satisfy SI, maxsum maxmin objective values
provably lower bounded k/n 1/n, respectively, step k.
upper bounds, consider omniscient (hence unrealistic) mechanisms maximize
objectives offline setting mechanisms complete knowledge future demands. mechanisms need guarantee EF extension real future demands rather
possible future demands. comparison C AUTIOUS LP offline mechanisms demonstrates loss C AUTIOUS LP (an online mechanism) suffers due absence
information regarding future demands, is, due cautiousness. DYNAMIC DRF
required EF extension, offline mechanisms theoretical upper bounds
DYNAMIC DRF, experiments show provide upper bounds practice.
data use traces real workloads Google compute cell, 7 hour period
2011 (Hellerstein, 2010). workload consists tasks, task ran single machine,
consumed memory one cores; demands fit model two resources.
various values n, sampled n random positive demand vectors traces analyzed
value two objective functions DYNAMIC DRF C AUTIOUS LP along
corresponding lower upper bounds. averaged 1000 simulations obtain data
points.
Figures 4(a) 4(b) show maxsum values achieved different mechanisms, 20
agents 100 agents respectively. performance two mechanisms nearly identical.
Figures 4(c) 4(d) show maxmin values achieved 20 agents 100 agents, respectively. Observe DYNAMIC DRF performs better C AUTIOUS LP lower values k,
performs worse higher values k. Intuitively, DYNAMIC DRF allocates resources
early stages satisfy DPO C AUTIOUS LP cautiously waits. results superior
performance DYNAMIC DRF initial steps fewer resources available thus lesser
flexibility optimization later steps, resulting inferior performance near end. contrast,
C AUTIOUS LP later able make loss early steps. Encouragingly, last step
C AUTIOUS LP achieves near optimal maxmin value. reason, unlike DYNAMIC DRF
maxmin objective value C AUTIOUS LP monotonically increases k increases experiments (although easy show always case).

7. Discussion
presented new model resource allocation multiple resources dynamic environments that, believe, spark study dynamic fair division generally. model
directly applicable data centers, clusters, cloud computing, allocation multiple
resources key issue, significantly extends previously studied static models. said,
1. cardinal notion utility dominant share agent utility, sum dominant shares
utilitarian social welfare minimum dominant share egalitarian social welfare.

600

fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES

Upper Bound
CautiousLP
DynamicDRF
Lower Bound

2.5
2

Upper Bound
CautiousLP
DynamicDRF
Lower Bound

2.25

1.5

1.5

1
0.75
0.5
0

0
0

5

10

15

20

0

(a) Maxsum 20 agents

0.095

50

75

100

(b) Maxsum 100 agents

0.0175

Upper Bound
CautiousLP
DynamicDRF
Lower Bound

0.08

25

Upper Bound
CautiousLP
DynamicDRF
Lower Bound

0.015

0.065

0.0125

0.05

0.01
0

5

10

15

20

0

(c) Maxmin 20 agents

25

50

75

100

(d) Maxmin 100 agents

Figure 4: maxsum maxmin objectives function time step k, n = 20
n = 100.

model also gives rise technical challenges need tackled capture realistic
settings.
First, model assumes positive demands, is, agent requires every resource. see
positive demands assumption plays role, recall achieving EF DPO impossible.
established dropping DPO leads trivial mechanism E QUAL PLIT, satisfies
remaining three properties; also true possibly zero demands. dropped EF,
observed trivial mechanism DYNAMIC ICTATORSHIP satisfies SI, DPO SP,
subsequently suggested improved mechanism DYNAMIC DRF satisfies DEF addition
SI, DPO SP. Surprisingly though, shown neither DYNAMIC ICTATORSHIP (see
Example 2) DYNAMIC DRF SP possibly zero demands.2 fact, despite significant
effort, unable settle question existence mechanism satisfies SI, DPO
SP possibly zero demands.
Second, analysis restricted setting divisible tasks, agents value fractional
quantities tasks. Parkes et. al. (2014) consider indivisible tasks setting,
2. possibly zero demands, modify DYNAMIC ICTATORSHIP DYNAMIC DRF continue allocating
even resources become saturated satisfy DPO.

601

fiK ASH , P ROCACCIA , & HAH

integral quantities agents task executed, albeit static environment. shown
even forward EF weakest EF relaxations considered paper impossible
achieve along DPO indivisible tasks. remains open determine relaxations
EF feasible dynamic resource allocation settings indivisible tasks. restrict
attention Leontief utilities, noted desiderata propose well-defined
dynamic setting utility function.
Third, model fair division extends classical model introducing dynamics,
results directly inform design practical mechanisms, make assumption
agents arrive time depart. reality, agents may arrive depart multiple times,
preferences may also change time (note changing preferences modeled
departure simultaneous re-arrival different demand vector). Departures without
re-arrivals easy handle; one allocate resources become free similar way
allocations entitlements, e.g., using DYNAMIC DRF (this scheme would clearly satisfy SI, DEF,
DPO, would interesting check whether also strategyproof). However, departures
re-arrivals immediately lead daunting impossibilities. Note though mechanisms
designed static settings performed well realistic (fully dynamic) environments (Ghodsi
et al., 2011), quite likely mechanisms provide theoretical guarantees
restricted dynamic settings would yield even better performance reality.

Acknowledgements
preliminary version paper appeared AAMAS13. Procaccia Shah partially
supported NSF grant NSF CCF-1215883, gift CMU-MSR Center
Computational Thinking.

References
Brams, S. J., & Taylor, A. D. (1996). Fair Division: Cake-Cutting Dispute Resolution.
Cambridge University Press.
Chen, Y., Lai, J. K., Parkes, D. C., & Procaccia, A. D. (2010). Truth, justice, cake cutting.
Proceedings 24th AAAI Conference Artificial Intelligence (AAAI), pp. 756761.
Chevaleyre, Y., Dunne, P. E., Endriss, U., Lang, J., Lematre, M., Maudet, N., Padget, J., Phelps,
S., Rodrguez-Aguilar, J. A., & Sousa, P. (2006). Issues multiagent resource allocation.
Informatica, 30, 331.
Chevaleyre, Y., Endriss, U., Estivie, S., & Maudet, N. (2007). Reaching envy-free states distributed negotiation settings. Proceedings 20th International Joint Conference
Artificial Intelligence (IJCAI), pp. 12391244.
Demers, A., Keshav, S., & Shenker, S. (1989). Analysis simulation fair queueing algorithm. Proceedings ACM Symposium Communications Architectures & Protocols
(SIGCOMM), pp. 112.
Dolev, D., Feitelson, D. G., Halpern, J. Y., Kupferman, R., & Linial, N. (2012). justified complaints: fair sharing multiple resources. Proceedings 3rd Innovations Theoretical Computer Science Conference (ITCS), pp. 6875.
602

fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES

Ghodsi, A., Sekar, V., Zaharia, M., & Stoica, I. (2012). Multi-resource fair queueing packet
processing. Proceedings ACM Symposium Communications Architectures &
Protocols (SIGCOMM), pp. 112.
Ghodsi, A., Zaharia, M., Hindman, B., Konwinski, A., Shenker, S., & Stoica, I. (2011). Dominant Resource Fairness: Fair allocation multiple resource types. Proceedings 8th
USENIX Conference Networked Systems Design Implementation (NSDI), pp. 2437.
Ghodsi, A., Zaharia, M., Shenker, S., & Stoica, I. (2013). Choosy: Max-min fair sharing datacenter jobs constraints. Proceedings 8th ACM European Conference Computer
Systems (EUROSYS), pp. 365378.
Gutman, A., & Nisan, N. (2012). Fair allocation without trade. Proceedings 11th International Joint Conference Autonomous Agents Multi-Agent Systems (AAMAS), pp.
719728.
Hellerstein, J. L. (2010). Google cluster data. Google research blog. Posted http://
googleresearch.blogspot.com/2010/01/google-cluster-data.html.
Joe-Wong, C., Sen, S., Lan, T., & Chiang, M. (2012). Multi-resource allocation: Fairness-efficiency
tradeoffs unifying framework. Proceedings 31st Annual IEEE International
Conference Computer Communications (INFOCOM), pp. 12061214.
Li, J., & Xue, J. (2013). Egalitarian division Leontief preferences. Economic Theory, 54(3),
597622.
Moulin, H. (2003). Fair Division Collective Welfare. MIT Press.
Moulin, H., & Stong, R. (2002). Fair queuing probabilistic allocation methods. Mathematics Operations Research, 27(1), 130.
Parkes, D. C., Procaccia, A. D., & Shah, N. (2014). Beyond Dominant Resource Fairness: Extensions, limitations, indivisibilities. ACM Transactions Economics Computation.
Forthcoming.
Procaccia, A. D. (2009). Thou shalt covet thy neighbors cake. Proceedings 21st International Joint Conference Artificial Intelligence (IJCAI), pp. 239244.
Walsh, T. (2011). Online cake cutting. Proceedings 3rd International Conference
Algorithmic Decision Theory (ADT), pp. 292305.
Zahedi, S. M., & Lee, B. C. (2014). REF: Resource elasticity fairness sharing incentives
multiprocessors. Proceedings 19th International Conference Architectural
Support Programming Languages Operating Systems (ASPLOS), pp. 145160.

603

fiJournal Artificial Intelligence Research 51 (2014) 829-866

Submitted 06/14; published 12/14

Exact Double-Oracle Algorithm Zero-Sum
Extensive-Form Games Imperfect Information
Branislav Bosansky

branislav.bosansky@agents.fel.cvut.cz

Agent Technology Center
Department Computer Science
Faculty Electrical Engineering
Czech Technical University Prague

Christopher Kiekintveld

cdkiekintveld@utep.edu

Computer Science Department
University Texas El Paso, USA

Viliam Lisy
Michal Pechoucek

viliam.lisy@agents.fel.cvut.cz
michal.pechoucek@agents.fel.cvut.cz

Agent Technology Center
Department Computer Science
Faculty Electrical Engineering
Czech Technical University Prague

Abstract
Developing scalable solution algorithms one central problems computational
game theory. present iterative algorithm computing exact Nash equilibrium
two-player zero-sum extensive-form games imperfect information. approach
combines two key elements: (1) compact sequence-form representation extensiveform games (2) algorithmic framework double-oracle methods. main idea
algorithm restrict game allowing players play selected sequences
available actions. solving restricted game, new sequences added finding
best responses current solution using fast algorithms.
experimentally evaluate algorithm set games inspired patrolling
scenarios, board, card games. results show significant runtime improvements
games admitting equilibrium small support, substantial improvement memory use even games large support. improvement memory use particularly
important allows algorithm solve much larger game instances existing
linear programming methods.
main contributions include (1) generic sequence-form double-oracle algorithm
solving zero-sum extensive-form games; (2) fast methods maintaining valid restricted
game model adding new sequences; (3) search algorithm pruning methods
computing best-response sequences; (4) theoretical guarantees convergence
algorithm Nash equilibrium; (5) experimental analysis algorithm several
games, including approximate version algorithm.

1. Introduction
Game theory widely used methodology analyzing multi-agent systems applying
formal mathematical models solution concepts. One focus computational game theory development scalable algorithms reasoning large games.
c
2014
AI Access Foundation. rights reserved.

fiBosansky, Kiekintveld, Lisy, & Pechoucek

need continued algorithmic advances driven growing number applications
game theory require solving large game instances. example, several decision
support systems recently deployed homeland security domains recommend
policies based game-theoretic models placing checkpoints airports (Pita, Jain,
Western, Portway, Tambe, Ordonez, Kraus, & Parachuri, 2008), scheduling Federal Air
Marshals (Tsai, Rathi, Kiekintveld, Ordonez, & Tambe, 2009), patrolling ports (Shieh,
An, Yang, Tambe, Baldwin, Direnzo, Meyer, Baldwin, Maule, & Meyer, 2012). capabilities systems based large amount research fast algorithms
security games (Tambe, 2011). Another notable example algorithmic progress
led game-theoretic Poker agents competitive highly skilled human
opponents (e.g., see Zinkevich, Bowling, & Burch, 2007; Sandholm, 2010).
focus developing new algorithms important general class games
includes security games Poker, well many familiar games. precisely,
study two-player zero-sum extensive-form games (EFGs) imperfect information.
class games captures sequential interactions two strictly competitive players
situations make decisions uncertainty. Uncertainty caused either
stochastic environment opponent actions directly
observable. consider general models sequential interactions uncertainty,
many fast algorithms developed Poker security domains
rely specific game structure.
propose new class algorithms finding exact (or approximate) Nash equilibrium solutions class EFGs imperfect information. leading exact
algorithm literature uses compact sequence-form representation linear programming optimization techniques solve games type (Koller, Megiddo, & von
Stengel, 1996; von Stengel, 1996). approach exploits compact representation, improve solution methods adopting algorithmic framework based
decompositions known computational game theory literature oracle algorithms
(McMahan, Gordon, & Blum, 2003). Oracle algorithms related methods constraint/column generation used solving large-scale optimization problems (Dantzig &
Wolfe, 1960; Barnhart, Johnson, Nemhauser, Savelsbergh, & Vance, 1998) exploit two
characteristics commonly found games. First, many cases finding solution
game requires using small fraction possible strategies, necessary
enumerate strategies find solution (Wilson, 1972; Koller & Megiddo, 1996).
Second, finding best response specific opponent strategy game computationally
much less expensive solving equilibrium. addition, best response algorithms
often make use domain-specific knowledge heuristics speed calculations
even further.
sequence-form double-oracle algorithm integrates decomposition ideas oracle
algorithms compact sequence-form representation EFGs imperfect information. results iterative algorithm always need generate
complete linear program game find Nash equilibrium solution. main idea
algorithm create restricted game players choose limited
space possible strategies (represented sequences actions). algorithm solves
restricted game uses fast best-response algorithm find strategies
original unrestricted game perform well current solution restricted
830

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

game. strategies added restricted game process iterates
best response found improve solution. case, current solution
equilibrium original game. Typically, solution found adding small
fraction strategies restricted game.
begin presenting related work, technical background, notation.
describe main algorithm three parts: (1) methods creating, solving, expanding valid restricted game, (2) algorithm finding best-response strategies
added restricted game, (3) variants main loop controlling iterative
process solving restricted games adding new strategies. present formal analysis
prove algorithm converges Nash equilibrium original game. Finally, provide experimental evaluation runtime performance convergence
behavior algorithm several realistic games different characteristics including
border patrolling scenario, Phantom Tic-Tac-Toe, simplified variant Poker.
compare results state-of-the-art algorithms finding exact approximate solutions: linear programming using sequence form, Counterfactual Regret
Minimization (CFR, Zinkevich, Johanson, Bowling, & Piccione, 2008; Lanctot, 2013).
experimental results confirm algorithm requires fraction possible sequences solve game practice significantly reduces memory requirements
solving large games. advances state art allows us exactly solve
much larger games compared existing algorithms. Moreover, games admitting
equilibrium small support (i.e., sequences non-zero probability
equilibrium), algorithm also achieves significant improvements computation time
finds equilibrium iterations. result hold without using
domain-specific knowledge, also show incorporating domain-specific heuristics
bounds algorithm straightforward way lead even significant
performance improvements. Analysis convergence rate shows approximative
bounds value game either similar bit worse early stages
compared CFR. However, convergence behavior CFR algorithm long
tail algorithm always finds exact solution much faster CFR.

2. Related Work
Solving imperfect-information EFGs computationally challenging task, primarily due
uncertainty actions opponent and/or stochastic environment.
leading exact algorithm (Koller et al., 1996; von Stengel, 1996) based formulating
problem finding optimal strategy play linear program. algorithm exploits
compact representation strategies sequences individual actions (called sequence
form) results linear program linear size size game tree. However,
approach limited applicability since game tree grows exponentially
number sequential actions game. common practice overcoming limited
scalability sequence-form linear programming use approximation method.
best known approximative algorithms include counterfactual regret minimization (CFR,
Zinkevich et al., 2008), improved versions CFR sampling methods (Lanctot, Waugh,
Zinkevich, & Bowling, 2009; Gibson, Lanctot, Burch, Szafron, & Bowling, 2012); Nesterovs
Excessive Gap Technique (EGT, Hoda, Gilpin, Pena, & Sandholm, 2010); variants
831

fiBosansky, Kiekintveld, Lisy, & Pechoucek

Monte Carlo Tree Search (MCTS) algorithms applied imperfect-information games (e.g.,
see Ponsen, de Jong, & Lanctot, 2011).
family counterfactual regret minimization algorithms based learning methods informally described follows. algorithm repeatedly traverses
game tree learns strategy play applying no-regret learning rule minimizes specific variant regret (counterfactual regret) information set.
no-regret learning converges optimal strategy information set. overall
regret bounded sum regret information set; hence, strategy
whole converges Nash equilibrium. main benefits approach include
simplicity robustness, adapted generic games (e.g., see Lanctot,
Gibson, Burch, Zinkevich, & Bowling, 2012, CFR applied games imperfect
recall). However, algorithm operates complete game tree therefore requires
convergence information sets, slow large games one desires
solution small error.
Another popular method Excessive Gap Technique exploits convex properties
sequence-form representation uses recent mathematical results finding extreme
points smooth functions (see Hoda et al., 2010, details). main idea approximate problem finding pair equilibrium strategies two smoothed functions
guiding find approximate solution. Although approach achieves faster
convergence comparison CFR, algorithm less robust (it known whether
similar approach used general classes games) less used practice.
Like CFR, EGT also operates complete strategy space sequences.
Monte Carlo Tree Search (MCTS) another family methods shown promise
solving large games, particular perfect information board games Go (e.g.,
Lee et al., 2009). CFR EGT algorithms guaranteed find -Nash
equilibrium, convergence equilibrium solution formally shown
variants MCTS imperfect-information games. contrary, common
version MCTS based Upper Confidence Bounds (UCB) selection function
converge incorrect solutions even simultaneous-move games (Shafiei, Sturtevant, &
Schaeffer, 2009) simplest class imperfect-information EFGs. MCTS algorithms therefore (in general) guarantee finding (approximate) optimal solution
imperfect-information games. One exception recent proof convergence MCTS
certain selection methods simultaneous-move games (Lisy, Kovarik, Lanctot, &
Bosansky, 2013). Still, using MCTS sometimes reasonable choice since produce
good strategies practice (Ponsen et al., 2011).
Contrary existing approximative approaches, algorithm aims find exact solution without explicitly considering strategy complete game tree.
work combines compact sequence-form representation double-oracle algorithmic framework. Previous work double-oracle framework focused primarily
applications normal-form games, restricted game expanded adding pure
best-response strategies iteration. One first examples solving games using
double-oracle principle McMahan et al. (2003). introduced doubleoracle algorithm, proved convergence Nash equilibrium, experimentally verified
algorithm achieves computation time improvements search game
evader trying cross environment without detected sensors placed
832

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

opponent. double-oracle algorithm reduced computation time several hours
tens seconds allowed solve much larger instances game. Similar success
domain-specific double-oracle methods demonstrated variety different domains inspired pursuit-evasion games (Halvorson, Conitzer, & Parr, 2009)
security games played graph (Jain, Korzhyk, Vanek, Conitzer, Tambe, & Pechoucek,
2011; Letchford & Vorobeychik, 2013; Jain, Conitzer, & Tambe, 2013).
works tried apply iterative framework oracle algorithms
EFGs, primarily using pure mixed strategies EFGs. first work exploited
iterative principle predecessor sequence-form linear-program formulation (Koller
& Megiddo, 1992). algorithm, authors use representation similar sequence form single player, strategies opponent iteratively
added constraints linear program (there exponential number constraints
formulation). approach seen specific variant oracle algorithms, strategy space expanded gradually single player. algorithm
generalization work, since algorithm uses sequence-form representation
players also incrementally expands strategy space players.
recent work done McMahan thesis (McMahan, 2006) followup work (McMahan & Gordon, 2007). works authors investigated extension
double-oracle algorithm normal-form games extensive-form case.
double-oracle algorithm EFGs operates similarly normal-form variant
uses pure mixed strategies defined EFGs. main disadvantage approach
basic version still requires large amount memory since pure strategy
EFG large (one action needs specified information set),
exponential number possible pure strategies. overcome disadvantage,
authors propose modification double-oracle algorithm keeps number
strategies restricted game bounded. algorithm removes restricted game
strategies least used current solution restricted game.
order guarantee convergence, algorithm adds iteration restricted
game mixed strategy representing mean removed strategies; convergence
guaranteed similarly fictitious play (see McMahan & Gordon, 2007, details).
Bounding size restricted game results low memory requirements. However,
algorithm converges extremely slowly take long time (several hours
small game) algorithm achieve small error (see experimental evaluation
McMahan, 2006; McMahan & Gordon, 2007).
similar concept using pure strategies EFGs used iterative algorithm
designed Poker work Zinkevich et al. (2007). algorithm work
expands restricted game strategies found generalized best response instead
using pure best response strategies. Generalized best response Nash equilibrium
partially restricted game player computing best response use pure
strategies original unrestricted game, opponent restricted use
strategies restricted game. However, main disadvantages using pure
mixed strategies EFGs still present result large memory requirements
exponential number iterations.
contrast, algorithm directly uses compact sequence-form representation
EFGs uses sequences building blocks (i.e., restricted game expanded
833

fiBosansky, Kiekintveld, Lisy, & Pechoucek

allowing new sequences played next iteration). Using sequences
sequence form solving restricted game reduces size restricted game
number iterations, however, also introduces new challenges constructing
maintaining restricted game, ensuring convergence Nash equilibrium,
must solve algorithm converge correct solution.

3. Technical Background
begin presenting standard game-theoretic model extensive-form games, followed discussion common solution concepts algorithms computing solutions. present sequence-form representation state-of-theart linear program computing solutions using representation. Finally, describe
oracle algorithms used solving normal-form games. summary
common notation provided Table 1 quick reference.
3.1 Extensive-Form Games
Extensive-form games (EFGs) model sequential interactions players game.
Games extensive form visually represented game trees (e.g., see Figure 2).
Nodes game tree represent states game; state game corresponds
sequence moves executed players game. node assigned player
acts game state associated node. edge game tree
node corresponds action performed player acts node.
Extensive-form games model limited observations players grouping nodes
information sets, given player cannot distinguish nodes belong
information set player choosing action. model also represents
uncertainty environment stochastic events using special Nature player.
Formally, two-player EFG defined tuple G = (N, H, Z, A, p, u, C, I): N set
two players N = {1, 2}. use refer one two players (either 1 2),
refer opponent i. H denotes finite set nodes game tree. node
corresponds unique history actions taken players Nature root
game; hence, use terms history node interchangeably. denote Z H
set terminal nodes game. denotes set actions overload
notation use A(h) represent set actions available player acting
node h H. specify ha = h0 H node h0 reached node h executing
action A(h). say h prefix h0 denote h v h0 . terminal
node z Z define utility function player (ui : Z R). study zero-sum
games, ui (z) = ui (z) holds z Z.
function p : H N {c} assigns node player takes action
node, c means Nature player selects action node based fixed
probability distribution known players. use function C : H [0, 1] denote
probability reaching node h due Nature (i.e., assuming players play
required actions reach node h). value C(h) product probabilities
assigned actions taken Nature player history h. Imperfect observation
player modeled via information sets Ii form partition nodes assigned
player {h H : p(h) = i}. Every information set contains least one node
834

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

node belongs exactly one information set. Nodes information set player
indistinguishable player. nodes h single information set Ii Ii
set possible actions A(h). Action A(h) uniquely identifies information set
Ii cannot exist node h0 H belong information set
Ii allowed played (i.e., A(h0 )). Therefore overload notation
use A(Ii ) denote set actions defined node h information set.
assume perfect recall, means players perfectly remember actions
information gained course game. result, nodes
information set Ii history actions player i.
3.2 Nash Equilibrium Extensive-Form Games
Solving game requires finding strategy profile (i.e., one strategy player)
satisfies conditions defined specific solution concept. Nash equilibrium (NE)
best known solution concept game theory describes behavior players
certain assumptions rationality. Nash equilibrium, every player plays
best response strategies players. Let set pure strategies
player i. EFGs, pure strategy assignment exactly one action played
information set. mixed strategy probability distribution set pure
strategies player. denote set mixed strategies player i.
pair strategies = (1 , 2 ) use ui () = ui (i , ) expected outcome
game player players follow strategies . best response player
opponents strategy strategy iBR , ui (iBR , ) ui (i0 , )
strategies i0 . strategy profile = (1 , 2 ) NE player
holds best response . game multiple NEs; zero-sum
setting, equilibria value (i.e., expected utility every player
same). called value game, denoted V . problem finding
NE zero-sum game polynomial computational complexity size game.
NE solution concept somewhat weak extensive-form games. Nash equilibrium
requires players act rationally. However, irrational strategies selected
parts game tree reachable players follow NE
strategies (these parts said equilibrium path). reason NE
expect part game played therefore sufficiently restrict
strategies information sets. overcome drawbacks, number refinements
NE introduced imposing restrictions intention describing
sensible strategies. Examples include subgame-perfect equilibrium (Selten, 1965) used
perfect-information EFGs. subgame-perfect equilibrium forces strategy profile
Nash equilibrium sub-game (i.e., sub-tree rooted node h)
original game. Unfortunately, sub-games particularly useful imperfectinformation EFGs; hence, refinements include strategic-from perfect equilibrium
(Selten, 1975), sequential equilibrium (Kreps & Wilson, 1982), quasi-perfect equilibrium
(van Damme, 1984; Miltersen & Srensen, 2010). first refinement avoids using weakly
dominated strategies equilibrium strategies two-player games (van Damme, 1991,
p. 29) also known undominated equilibrium. Sequential equilibrium tries
exploit mistakes opponent using notion beliefs consistent
835

fiBosansky, Kiekintveld, Lisy, & Pechoucek

strategy opponent even information sets equilibrium path. main
intuitions behind first two refinements combined quasi-perfect equilibrium.
Even though solution described NE always prescribe rational strategies
equilibrium path, still valuable compute exact NE large extensive-form
games several reasons. focus zero-sum games, NE strategy guarantees
value game even equilibrium path. words, strategy
equilibrium path optimally exploit mistakes opponent, still
guarantees outcome least value gained following equilibrium path. Moreover,
refined equilibrium still NE calculating value game often starting
point many algorithms compute refinements example used
computing undominated equilibrium (e.g., see Ganzfried & Sandholm, 2013; Cermak,
Bosansky, & Lisy, 2014) normal-form proper equilibrium (Miltersen & Srensen, 2008).
3.3 Sequence-Form Linear Program
Extensive-form games perfect recall compactly represented using sequence
form (Koller et al., 1996; von Stengel, 1996). sequence ordered list actions taken
single player history h. number actions (i.e., length sequence )
denoted |i | empty sequence (i.e., sequence actions) denoted .
set possible sequences player denoted set sequences
players = 1 2 . sequence extended single action taken
player i, denoted = i0 (we use v i0 denote prefix i0 ). games
perfect recall, nodes information set Ii share sequence actions
player use seqi (Ii ) denote sequence. overload notation use
seqi (h) denote
leading node h, seqi (H 0 ) ,
sequence of0 actions player
0
0
seqi (H ) = h0 H 0 seqi (h ) H H. Since action uniquely identifies
information set Ii nodes information set share history actions
player i, sequence uniquely identifies information set. use function infi (i0 )
denote information set last action sequence i0 taken.
empty sequence, function infi () information set root node.
Finally, define auxiliary payoff function gi : R extends utility
function nodes game tree. payoff function gi represents expected
utility nodes reachable sequentially executing actions specified pair
sequences :
X
gi (i , ) =
ui (h) C(h)
(1)
hZ : jN j =seqj (h)

value payoff function defined 0 leaf reachable sequentially executing actions sequences either actions pair sequences
executed inner node (h H \ Z) reached, sequential execution actions node h reached, current action executed
sequence (h) defined (i.e.,
/ A(h)). Formally define pair sequences
compatible exists node h H sequence every player equals
seqi (h).
compute Nash equilibrium two-player zero-sum extensive-form game
using linear program (LP) polynomial size size game tree using
836

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

sequence form (Koller et al., 1996; von Stengel, 1996). LP uses equivalent compact
representation mixed strategies players form realization plans. realization
plan sequence probability player play sequence actions
assumption opponent choose compatible sequences actions
reach information sets actions specified sequence defined.
denote realization plan player ri : R. equilibrium realization plans
computed using following LP (e.g., see Shoham & Leyton-Brown, 2009, p. 135):

vinfi (i )

X

max vinfi ()
r,v
X
0
vIi

gi (i , ) ri (i )

0 :seq (I 0 )=
Ii








ri () = 1
X

(2)
(3)

ri (i a) = ri (i )

Ii Ii , = seqi (Ii )

(4)



(5)

aA(Ii )

ri (i ) 0

Solving LP yields realization plan player using variables ri , expected values
information sets player (variables vIi ). LP works follows: player
maximizes expected utility value selecting values variables realization plan constrained Equations (35). probability playing empty
sequence defined 1 (Equation 3), probability playing sequence
equal sum probabilities playing sequences extended exactly one action
(Equation 4). Finding realization plan also constrained best responding
opponent, player i. ensured Equation (2), player selects
information set Ii action minimizes expected utility value vIi information set. one constraint defined sequence , last action
sequence determines best action played information set infi (i ) = Ii .
expected utility composed expected utilities information sets reachable
playing sequence (sum v variables left side) expected utilities
leafs sequence leads (sum g values right side constraint).
3.4 Double-Oracle Algorithm Normal-Form Games
describe concept column/constraint generation techniques applied previously
normal-form games known double-oracle algorithm (McMahan et al., 2003).
Normal-form games represented using game matrices; rows matrix correspond
pure strategies one player, columns correspond pure strategies opponent,
values matrix cells represent expected outcome game players
play corresponding pure strategies. Zero-sum normal-form games solved linear
programming polynomial time size matrix (e.g., see Shoham & LeytonBrown, 2009, p. 89).
Figure 1 shows visualization main structure double-oracle algorithm
normal-form games. algorithm consists following three steps repeat
convergence:
837

fiBosansky, Kiekintveld, Lisy, & Pechoucek

Figure 1: Schematic double-oracle algorithm normal-form game.
1. create restricted game limiting set pure strategies player
allowed play
2. compute pair Nash equilibrium strategies restricted game using LP
solving normal-form games
3. player, compute pure best response strategy equilibrium strategy
opponent found previous step; best response may pure strategy
original unrestricted game
best response strategies computed step 3 added restricted game, game
matrix expanded adding new rows columns, algorithm continues
next iteration. algorithm terminates neither players improve outcome
game adding new strategy restricted game. case players
play best response strategy opponent original unrestricted game.
algorithm maintains values expected utilities best-response strategies
throughout iterations algorithm. values provide bounds value
original unrestricted game V perspective player i, minimal value
past best-response calculations represents upper bound value
original game, ViU B , maximal value past best-response calculations
opponent represents lower bound value original game, ViLB . Note
bounds holds lower bound player equal negative value
upper bound opponent:
UB
ViLB = Vi

general, computing best responses computationally less demanding solving
game, since problem reduced single-player optimization. Due fact bestresponse algorithms operate quickly (e.g., also exploiting additional domainspecific knowledge), called oracles context. algorithm incrementally
adds strategies one player, algorithm called single-oracle algorithm,
algorithm incrementally adds strategies players, algorithm called
double-oracle algorithm. Double-oracle algorithms typically initialized arbitrary
pair strategies (one pure strategy player). However, also use larger set
initial strategies selected based domain-specific knowledge.
double-oracle algorithm zero-sum normal-form games runs polynomial time
size game matrix. Since iteration adds least one pure strategy
838

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

Figure 2: Example two-player extensive-form game visualized game tree. Circle
player aims maximize utility value, box aims minimize utility value. bold
edges represent sequences actions added restricted game.
restricted game finite pure strategies, algorithm stops
|i | + |i | iterations. iteration also polynomial, since consists solving
linear program computing best responses. relative performance doubleoracle algorithm compared solving linear program original unrestricted game
closely depends number iterations required convergence. worst case,
algorithm adds pure strategies solves original game, although rarely
case practice. Estimating expected number iterations needed double-oracle
algorithm converge, however, remains open problem.
3.4.1 Towards Extensive-Form Games
straightforward method applying double-oracle algorithm EFGs use pure
strategies defined EFGs (i.e., assignments action information set, realization
plans) apply exactly algorithm described section i.e., iteratively add
pure strategies unrestricted extensive-form game restricted game matrix.
However, result exponential number iterations exponentially large
restricted game worst case. algorithm differs significantly idea since
directly operates (more compact) sequences instead full strategies.

4. Sequence-Form Double-Oracle Algorithm Extensive-Form Games
describe sequence-form double-oracle algorithm solving extensive-form
games imperfect information. First, give informal overview algorithm.
use example game depicted Figure 2 illustrate key concepts. Afterwards, formally define restricted game describe key components
algorithm, following full example run algorithm.
overall scheme algorithm based double-oracle framework described
previous section. main difference algorithm uses sequences define
restrictions game tree. restricted game model defined allowing
players use (i.e., play non-zero probability) subset sequences
original unrestricted game. restricted subset sequences defines subsets
reachable actions, nodes, information sets original game tree. Consider example Figure 2. restricted game defined sequences , A, AC, AD circle
player, , x box player. sequences represent actions allowed game,
839

fiBosansky, Kiekintveld, Lisy, & Pechoucek

define reachable nodes (using history reference , A, Ax, AxC, AxD),
reachable information sets (I1 , I2 circle player information set
box player).
algorithm iteratively adds new sequences allowed actions restricted
game, similarly double-oracle algorithm normal-form games. restricted game
solved standard zero-sum extensive-form game using sequence-form linear program. best response algorithm searches original unrestricted game find new
sequences add restricted game. sequences added, restricted
game tree expanded adding new actions, nodes, information sets
reachable based new sets allowed sequences. process solving restricted
game adding new sequences iterates new sequences improve solution
added.
two primary complications arise use sequences instead full
strategies double-oracle algorithm, due fact sequences necessarily define actions information sets: (1) strategy computed restricted game
may complete strategy original game, define behavior
information sets restricted game, (2) may possible
play every action sequence allowed restricted game, playing
sequence depend compatible sequence actions opponent.
example game tree Figure 2, strategy circle player restricted game
specifies play information sets I3 I4 . consequence second issue
inner nodes original unrestricted game (temporarily) become leafs
restricted game. example, box player add sequence restricted
game making node Ay leaf restricted game, since actions
circle player restricted game applicable node.
algorithm solves complications using two novel ideas. first idea
concept default pure strategy (denoted iDef ). Informally speaking, algorithm
assumes player fixed implicit behavior defines player
default information set part restricted game. described
default strategy iDef , specifies action every information set. Note
default strategy need represented explicitly (which could use large amount
memory). Instead, defined implicitly using rules, selecting first action
deterministic method generating ordered set actions A(h) node h.
use default pure strategies map every strategy restricted game valid
strategy full game. Specifically, strategy original unrestricted game selects
actions according probabilities specified strategy restricted game
every information set part restricted game, information sets
plays according default pure strategy. Recall example Figure 2,
pure default strategy circle player hA, C, E, Gi (i.e., selecting leftmost
action information set). Hence, strategy original unrestricted game use
strategy restricted game information sets I1 I2 , select pure actions
E, G information sets I3 I4 respectively.
second key idea use temporary utility values cases
allowed actions played node restricted game inner
node original game (so called temporary leaf ). ensure correct convergence
840

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

H
ZH
iDef
ri : 7 R
C : H 7 R
gi : H 7 R
seqi
infi : 7 Ii

game-tree nodes / histories
leafs / terminal states
implicit default pure strategy player
realization plan player sequence
probability reaching node due Nature play
extension utility function nodes;
gi (h) = ui (h) C(h) h Z gi (h) = 0 h terminal node (h
/ Z)
sequence(s) actions player leading node / set nodes /
/ information set
information set last action sequence executed

Table 1: outline main symbols used paper.

algorithm temporary utilities must assigned provide bound
expected value gained continuing play given node. algorithm uses
value corresponds expected outcome continuing game play, assuming
player making choice temporary leaf uses default strategy,
opponent plays best response. Assume add sequence box player
restricted game example tree Figure 2. temporary utility value node Ay
would correspond value 2, since default strategy information set I3 play E
circle player. next section formally describe method prove
correctness algorithm given temporary values.
describe detail key parts method. first formally define
restricted game methods expanding restricted game, including details
key ideas introduced above. describe algorithm selecting
new sequences allowed next iteration. decision sequences add
based calculating best response original unrestricted game using game-tree
search improved additional pruning techniques. Finally, discuss different variations
main logic double-oracle algorithm determines player(s)
algorithm adds new best-response sequences current iteration.
4.1 Restricted Game
section formally defines restricted game subset original unrestricted
game. restricted game fully specified set allowed sequences. define
sets nodes, actions, information sets subsets original unrestricted sets
based allowed sequences. denote original unrestricted game tuple
G = (N, H, Z, A, p, u, C, I) restricted game G0 = (N, H 0 , Z 0 , A0 , p, u0 , C, 0 ).
sets functions associated restricted game use prime notation; set
players, functions p C remain same.
restricted game defined set allowed sequences (denoted 0 )
returned best response algorithms. indicated above, even allowed sequence
0 might playable full length due missing compatible sequences
opponent. Therefore, restricted game defined using maximal compatible set
sequences 0 0 given set allowed sequences 0 . define 0 maximal
841

fiBosansky, Kiekintveld, Lisy, & Pechoucek

subset sequences 0 that:
0i {i 0i : 0i h H j N seqj (h) = j }

N

(6)

Equation (6) means player every sequence 0i , exists
compatible sequence opponent allows sequence executed full
(i.e., sequentially executing actions sequences node h
reached seqj (h) = j players j N ).
set sequences 0 fully defines restricted game, sets
tuple G0 derived 0 . node h restricted game
sequences must played reach h set 0 players:
H 0 {h H : N seqi (h) 0 }

(7)

pair sequences 0 , nodes reachable executing pair sequences
included H 0 . Actions defined node h restricted game
playing action node leads node restricted game:
A0 (h) {a A(h) : ha H 0 }

h H 0

(8)

Nodes restricted game corresponding inner nodes original unrestricted
game may inner nodes restricted game. Therefore, set leaves
restricted game union leaf nodes original game inner nodes
original game currently valid continuation restricted game, based
allowed sequences:

Z 0 Z H 0 {h H 0 \ Z : A0 (h) = }
(9)
explicitly differentiate leaves restricted game correspond leaves
original unrestricted game (i.e., Z 0 Z) leaves restricted game correspond
inner nodes original unrestricted game (i.e., Z 0 \ Z), since algorithm assigns
temporary utility values nodes latter case.
information sets restricted game correspond information sets original
unrestricted game. node h belongs information set Ip(h) original game,
holds restricted game. define information set part
restricted game least one inner node belongs information
set included restricted game:
Ii0 {Ii Ii : h Ii h H 0 \ Z 0 }

(10)

information set restricted game Ii Ii0 consists nodes
restricted game i.e., h Ii : h H 0 .
Finally, define modified utility function u0 restricted game. primary
reason modified utility function define temporary utility values leaves
set Z 0 \Z. Consider h Z 0 \Z temporary leaf player player acting
node (i = p(h)). Moreover, let ui (h) expected outcome game starting
node assuming players playing NE strategies original unrestricted
game. modified utility function u0i leaf must return value lower bound
842

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

value ui (h). Due zero-sum assumption, value represents upper bound
value opponent i. Setting value way ensures two things: (1) player
likely use sequences leading node h optimal strategies restricted game (since
modified utility value upper bound actual value), (2) player adds new
sequences using best-response algorithms prolong sequence seqi (h) leading node h
sequences would yield better expected value u0i . Later show
counterexample setting value otherwise cause algorithm converge
incorrect solution. calculate lower bound setting utility value
corresponds outcome original game player continues playing
BR default strategy.
default strategy iDef opponent plays best response
valid lower bound since consider single strategy player acting
node h, correspond default strategy; considering strategies could allow
player improve value continuing node h. leaf nodes
h Z 0 Z set u0i (h) ui (h).
4.1.1 Solving Restricted Game
restricted game defined section valid zero-sum extensive-form game
solved using sequence-form linear programming described Section 3.
algorithm computes NE restricted game solving pair linear programs using
restricted sets 0 , H 0 , Z 0 , 0 , modified utility function u0 .
strategy restricted game translated original game using
pure default strategy extend restricted strategy defined. Formally,
ri0 mixed strategy represented realization plan player restricted
game, define extended strategy r0i strategy identical strategy
restricted game sequences included restricted game, correspond
default strategy iDef sequence included restricted game:
(
ri0 (i )
0i
r0i (i )
(11)
ri0 (i0 ) iDef (i \ i0 )
/ 0i ; i0 = arg maxi00 0i ; i00 vi |i00 |
realization plan sequence allowed restricted game (i.e.,
/ 0i )
equal realization probability longest prefix sequence allowed
restricted game (denoted i0 ), setting remaining part sequence (i.e., \ i0 )
correspond default strategy player i. computation expressed
multiplication two probabilities, overload notation use iDef (i \ i0 )
1 remaining part sequence corresponds default strategy player i,
0 otherwise.
iteration double-oracle algorithm one sequence-form LP solved
player compute pair NE strategies restricted game. denote strategies
) (r , r ) extended original unrestricted game using
(ri , ri

default strategies.
4.1.2 Expanding Restricted Game
restricted game expanded adding new sequences set 0 updating
remaining sets according definition. adding new sequences, algorithm
843

fiBosansky, Kiekintveld, Lisy, & Pechoucek

calculates stores temporary utility values leaves Z 0 \ Z used
sequence-form LP.
updating restricted game, linear programs modified correspond new restricted game. newly added information sets sequences,
new variables created linear programs constraints corresponding
information sets/sequences created (Equations 2 4). Moreover, constraints already existing linear program need updated. sequence
added set 0i immediate prefix sequence (i.e., sequence i0 v
|i0 | + 1 = |i |) already part restricted game, need update
constraint information sets Ii i0 = seqi (Ii ) ensure consistency
strategies (Equation 4), constraint corresponding sequence i0 (Equation 2).
addition, algorithm updates Equations (2) assigned sequences opponent
g(i , ) 6= 0. Finally, algorithm updates constraints previously used
utilities temporary leaf nodes longer leaf nodes restricted game
adding new sequences.
New sequences player found using best response sequence (BRS) algorithms described Section 4.2. perspective sequence-form double-oracle
algorithm, BRS algorithm calculates pure best response player fixed
strategy opponent original unrestricted game. pure best response specifies
action play information set currently reachable given opponents
extended strategy ri . best response formally defined pure realization
plan riBR assigns integer values 0 1 sequences. realization plan
necessarily pure strategy original unrestricted game may
action specified every information set. Specifically, action specified
information sets reachable (1) due choices player i, (2) due
zero probability realization plan opponent ri . Omitting actions
affect value best response information sets never reached;
hence, riBR holds r0i ui (riBR , ri ) ui (r0i , ri ) exists pure best
response strategy iBR ui (riBR , ri ) = ui (iBR , ri ). sequences
used best-response pure realization plan probability 1 returned BRS
algorithm call best-response sequences:
{i : riBR (i ) = 1}

(12)

4.1.3 Example Run Algorithm
demonstrate sequence-form double-oracle algorithm example game depicted Figure 3a. example, two players: circle box. Circle aims
maximize utility value leafs, box aims minimize utility value. assume
choosing leftmost action information set default strategy
players game.
algorithm starts empty set allowed sequences restricted game
0 ; hence, algorithm sets current pair (ri , ri ) strategies equivalent
Def ). Next, algorithm adds new sequences correspond best response
(iDef ,
default strategy opponent; example best response sequences
circle player {, A, AD}, {, y} box player. sequences added
844

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

(a) Step 0

(b) Step 1

(d) Step 3

(c) Step 2

(e) Step 4

Figure 3: Example steps sequence-form double-oracle algorithm two-player
zero-sum game, circle player aims maximize utility value, box aims minimize
utility value. Bold edges correspond sequences actions added restricted
game. dashed boxes indicate information sets.
set allowed sequences 0 . Next, set sequences restricted game 0
updated. maximal compatible set sequences set 0 cannot contain sequence
AD compatible sequence box player (i.e., x case) allowed
restricted game yet sequence AD cannot fully executed. Moreover, adding
sequences y, restricted game contain node Ay actions E F
defined original unrestricted game. However, continuation
current restricted game yet; hence, node temporary leaf, belongs Z 0 \ Z,
algorithm needs define new value modified utility function u0 node.
value u0 (Ay) equal 2 corresponds outcome game circle
player continues playing default strategy box player plays best response.
complete first step algorithm summarize nodes information sets
included restricted game; H 0 contains 3 nodes (the root, node playing
action node Ay), two information sets (the information set node Ay
added restricted game, node leaf restricted game).
Playing sequences probability 1 Nash equilibrium restricted
game. situation depicted Figure 3b, sequences 0 shown bold edges.
algorithm proceeds complete list steps algorithm summarized Table 2. second iteration, new sequences B BH added
restricted game. box player add new sequences iteration
best response extended equilibrium strategy circle player i.e., playing
sequences A, AC, AE probability 1. NE updated restricted game changes
playing sequences B, BH sequence y, probability 1. third iteration
situation changes box player adds sequence x, new sequences
845

fiBosansky, Kiekintveld, Lisy, & Pechoucek

added circle player. adding sequence x, sequence AD also becomes part
set 0 fully executed due adding compatible sequence x. NE
restricted game fully mixed, sequences starting B played
ratio 3 : 4, x ratio 4 : 3. fourth iteration, algorithm adds
sequence AF restricted game (the best response circle player), removes
assigned value u0 (Ay) since node longer belongs set Z 0 . algorithm stops
four iterations. sequences added restricted game, solution
) translated solution original unrestricted
restricted game (ri , ri


game, (ri , ri ) Nash equilibrium original game.
Iteration
1.
2.
3.
4.

BR
r
, A, AD
, B, BH
, B, BH
, A, AF

BR
r
,
,
, x
,

0
,
, A, B, BH
, A, AD, B, BH
, A, AD, AF, B, BH

0
,
,
, y, x
, y, x

Table 2: Steps sequence-form double-oracle algorithm applied example.
Consider small modification example game utility value
3 leaf following action F (i.e., node AyF ). case, algorithm
need add sequence AF (nor AE) restricted game improve
value restricted game. Note modified example game shows
algorithm needs set utility values nodes Z 0 \ Z. algorithm simply uses
unmodified utility function, node Ay treated zero utility
value. value overestimates outcome actual continuation following node
original game circle player since sequences AE AF never
part best response circle player, algorithm converge incorrect
solution.
4.2 Best-Response Sequence Algorithm
purpose best-response sequence (BRS) algorithm generate new sequences
added restricted game next iteration, prove
best response better expected value uses sequences currently allowed
restricted game. Throughout section use term searching player represent
player algorithm computes best response sequences. refer
player i.
BRS algorithm calculates expected value pure best response opponents strategy ri . algorithm returns set best-response sequences well
expected value strategy extended strategy opponent.
algorithm based depth-first search traverses original unrestricted
game tree. behavior opponent fixed strategy given extended
realization plan ri . save computation time, best-response algorithms use branch
bound search best-response sequences. algorithm uses bound
expected value inner node, denoted . bound represents minimal
utility value node currently evaluated needs gain order part
846

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

Require: - searching player, h - current node, Iik - current information set, r0i - opponents
strategy, Min/MaxUtility - bounds utility values, - lower bound node h
1: w r (seqi (h)) C(h)
2: h Z
3:
return ui (h) w
4: else h Z 0 \ Z
5:
return u0i (h) w
6: end

7: sort A(h) based probability wa r 0i seqi (ha) C(ha)
8: v h 0
9: A(h),
wa > 0

10:
0 v h + (w wa ) MaxUtility
11:
0 wa MaxUtility
12:
v 0 BRSi (ha, 0 )
13:
v 0 =
14:
return
15:
end
16:
vh vh + v0
17:
w w wa
18:
else
19:
return
20:
end
21: end
22: return v h

Figure 4: BRSi nodes players.

best-response sequence. Using bound search, algorithm able
prune branches certainly part best-response sequence. bound
set MinUtility root node.
distinguish 2 cases search algorithm: either algorithm evaluating
information set (or specifically node h) assigned searching player i,
node assigned one players (either opponent, player i,
chance node). pseudocode two cases depicted Figures 4 5.
4.2.1 Nodes Opponent
first describe case used algorithm evaluates node h assigned either
opponent searching player Nature (see Figure 4). main idea
calculate expected utility node according (fixed) strategy player.
strategy known either given extended realization plan ri ,
stochastic environment (C). Throughout algorithm, variable w represents
probability node based realization probability opponent stochastic
environment (line 1). value iteratively decreased values wa represent realization probabilities currently evaluated action A(h). Finally, vh expected
utility value node.
algorithm evaluates actions descending order according probability
played (based r0i C; lines 921). First, calculate new lower bound
847

fiBosansky, Kiekintveld, Lisy, & Pechoucek

0 successor ha (line 10). new lower bound minimal value must
returned recursive call BRSi (ha) optimistic assumption
remaining actions yield maximum possible utility. lower bound
exceed maximum possible utility game, algorithm executed recursively
successors (line 12). Note algorithm evaluate branches zero
realization probability (line 9).
3 possibilities pruning part search algorithm. first
pruning possible currently evaluated node leaf restricted game,
node inner node original node (i.e., h Z 0 \ Z; line 5). algorithm
directly use value modified utility function u0 case, since calculated
best response searching player default strategy opponent
applied successors node h since h Z 0 . Secondly, cut-off also occurs
new lower bound successor larger maximum possible utility
game, since value never obtained successor (line 19). Finally, cut-off
occurs cut-off one successors (line 14).
4.2.2 Nodes Searching Player
nodes assigned searching player, algorithm evaluates every action
state belongs current information set. algorithm traverses states
descending order according probability occurrence given strategies
opponent Nature (line 8). Similar previous case, iteration algorithm
calculates new lower bound successor node (line 17). new lower bound 0
minimal value must returned recursive call BRSi (h0 a) order
action selected best action information set optimistic
assumption action yields maximum possible utility value applying
remaining states information set. algorithm performs recursive call
(line 20) action still could best information set (i.e., lower
bound exceed maximal possible utility game). Note cut-off
occurs one successors, currently evaluated action longer best
action information set. Hence, va set action evaluated
remaining nodes. algorithm determines action selected
best one information set, evaluates action remaining nodes
information set. Finally, algorithm stores values best action
nodes information set (line 30). reused information set
visited (i.e., algorithm reaches different node h0 information set
Ii ; line 5).
cut-off occurs part search algorithm maximal possible value vah
smaller lower bound evaluating node h. means regardless
action selected best action information set, lower bound
node h reached; hence, cut-off occurs (line 27). cut-off occurs
information set, information set cannot reached sequences
searching player leading information set cannot part best response.
due propagating cut-off least one previous information set searching
player, otherwise tight lower bound set (the bound first set
848

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

Require: - searching player, h - current node, Iik - current information set, ri - opponents
strategy, Min/MaxUtility - bounds utility values, lower bound node h
1: h Z
2:
return ui (h) r0i (seqi (h)) C(h)
3: end
4: v h already calculated
5:
return v h
6: end
7: H 0 {h0 ; h0 Ii }
0
8: sort H
value ri (seqi (h0 )) C(h0 )
P descending according
0
9: w h0 H 0 r (seqi (h )) C(h0 )
10: va 0 A(h); maxAction
11: h0 H 0
12:
wh0 r0i (seqi (h0 )) C(h0 )
13:
A(h0 )
14:
maxAction empty
15:
0 wh0 MinUtility
16:
else
17:
0 (vmaxAction + w MinUtility) (va + (w wh0 ) MaxUtility)
18:
end
19:
0 wh0 MaxUtility
0
20:
vah BRSi (h0 a, 0 )
0
21:
va va + vah
22:
end
23:
end
24:
maxAction arg maxaA(h0 ) va
25:
w w wh0

26:
h evaluated maxaA(h) vah <
27:
return
28:
end
29: end
0
h0
v h h0 H 0
30: store vmaxAction
h
31: return vmaxAction

Figure 5: BRSi nodes searching player.

information sets searching player). Therefore, exists least one action
searching player never evaluated (after cut-off, value va
action set ) cannot selected best action information set. Since
assume perfect recall, nodes information set Ii share sequence actions
seqi (Ii ); hence, node h0 Ii reached again.
4.3 Main Loop Alternatives
introduce several alternative formulations main loop sequence-form
double-oracle algorithm. general approach double-oracle algorithm solve
restricted game find equilibrium strategy player, compute best responses
original game players, continue next iteration. However,
sequence-form LP formulated double-oracle scheme way
849

fiBosansky, Kiekintveld, Lisy, & Pechoucek

iteration algorithm solve restricted game perspective single
player i. words, formulate single LP described Section 3.3 computes
optimal strategy opponent restricted game (player i), compute
best response player strategy. means iteration
select specific player i, compute best response iteration. call
selection process player-selection policy.
several alternatives player-selection policy act domainindependent heuristics double-oracle algorithm. consider three possible policies:
(1) standard double-oracle player-selection policy selecting players iteration, (2) alternating policy, algorithm selects one player switches
players regularly (player selected one iteration, player selected
following iteration), finally (3) worse-player-selection policy selects player
currently worse bound solution quality. end iteration
algorithm selects player upper bound utility value away
current value restricted game. formally,
fi
fi
arg max fiViU B ViLP fi
(13)


ViLP last calculated value restricted game player i. intuition
behind choice either bound precise missing sequences
player restricted game need added, upper bound overestimated. either case, best-response sequence algorithm run player
next iteration, either add new sequences tighten bound. case tie,
alternating policy applied order guarantee regular switching players.
experimentally compare policies show impact overall performance
sequence-form double-oracle algorithm (see Section 6).

5. Theoretical Results
section prove sequence-form double-oracle algorithm always converge Nash equilibrium original unrestricted game. First, formally define
strategy computed best-response sequence (BRS) algorithm, prove lemmas
characteristics BRS strategies, finally prove main convergence
result. Note variations main loop described Section 4.3 affect
correctness algorithm long player-selection policy ensures improvement made BRS algorithm one player BRS algorithm run
opponent next iteration.
0 realization plan player restricted game G0 . BRS(r 0 )
Lemma 5.1 Let ri

returns sequences corresponding realization plan riBR unrestricted game,
riBR part pure best response strategy r0i . value returned algorithm
value executing pair strategies ui (r0i , riBR ).
0 ) searches game tree selects action maximizes value
Proof BRS(ri
game player information sets Ii assigned player reachable given
strategy opponent r0i . opponents nodes, calculates expected value

850

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

0 defined value according pure action default
according ri
Def
0 defined. chance nodes, returns expected value
strategy ri
node sum values successor nodes weighted probabilities.
node h, successors maximal possible value node h also
maximal possible value (when playing r0i ). selections nodes
belong achieves maximal value; hence, form best response strategy
r0i .
0 )) denote value returned BRS algorithm,
brevity use v(BRS(ri
equal ui (r0i , riBR ).
0 realization plan player restricted game G0 let
Lemma 5.2 Let ri

Vi value original unrestricted game G player i,
0
v(BRS(ri
)) Vi .

(14)

0 )) value best response r 0
Proof Lemma 5.1 showed v(BRS(ri

0 )) < V
valid strategy original unrestricted game G. v(BRS(ri

Vi cannot value game since player strategy r0i achieves better
utility, contradiction.
0 realization plan player returned LP
Lemma 5.3 Let ri
0
restricted game G let ViLP value restricted game returned LP,
0
v(BRS(ri
)) ViLP .

(15)

0
Proof realization plan ri
part Nash equilibrium strategy zero-sum
LP
game guarantees value Vi
G0 . best response computation original
unrestricted game G selects actions restricted game G0 , creates best
response game G0 well obtaining value ViLP . best response selects action
allowed restricted game G0 , two cases.
Case 1 : best response strategy uses action temporary leaf h Z 0 \ Z.
Player makes decision leaf, otherwise value temporary leaf
would directly returned BRS. value temporary leaf underestimated player restricted game modified utility function u0
Def .
over-estimated BRS computation best response default strategy
value best response increase including action.
Case 2 : best response strategy uses action allowed G0 internal node
restricted game H 0 \ Z 0 . occur nodes assigned player i,
actions player going G0 probability zero r0i . BRS takes action
maximum value nodes assigned player i, reason selecting action
leading outside G0 greater equal value best action G0 .
0 )) > V LP
Lemma 5.4 assumptions previous lemma, v(BRS(ri

returns sequences added restricted game G0 next iteration.

851

fiBosansky, Kiekintveld, Lisy, & Pechoucek

Proof Based proof previous Lemma, BRS player improve
value LP (ViLP ) selecting action present G0
performed node h included G0 (in makes decision). Let (i , )
pair sequences leading h. construction restricted game
next iteration, sequence sequence ensures executed full
part new restricted game.
Note, Lemmas 5.2 5.4 would hold utility values u0 temporary
leaves (h Z 0 \ Z) set arbitrarily. algorithm sets values temporary leaf h
player p(h) continues playing default strategy opponent (p(h))
playing best response. utility values temporary leaves set arbitrarily
used BRS algorithms speed-up calculation proposed (see algorithm
Figure 4, line 5), Lemma 5.2 need hold cases value
node h strictly overestimates optimal expected value player p(h). case,
best-response value opponent may lower optimal outcome,


v BRS(rp(h) ) < Vp(h)
(16)
hand, BRS algorithm use temporary values u0
node, Lemma 5.4 violated best-response value strictly higher
player p(h) even though new sequences added restricted game.
Theorem 5.5 sequence-form double-oracle algorithm extensive-form games described previous section terminates
0
v(BRS(ri
)) = v(BRS(ri0 )) = ViLP = Vi ,

(17)

always happens finite number iterations (because game finite),
strategies (r0i , r0i ) Nash equilibrium original unrestricted game.
Proof First show algorithm continues equalities (17) hold.
0 )) 6= v(BRS(r 0 )) Lemma 5.2 Lemma 5.4 know
v(BRS(ri

0 ) > V LP , restricted game following itersome player holds BRS(ri

ation larger least one action algorithm continues. worst case,
restricted game equals complete game G0 = G, cannot extended further.
case BRS cannot find better response Vi algorithm stops due
Lemma 5.4.
condition theorem holds algorithm found NE complete
BR = BRS(r 0 ) best response r 0
game, Lemma 5.1 know ri


complete game. However, value best response strategy zero-sum
game value game, strategy r0i optimal part Nash
equilibrium game.

6. Experiments
present experimental evaluation performance sequence-form
double-oracle algorithm EFGs. compare algorithm two state-of-the-art
852

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

baselines, full sequence-form LP (referred FullLP on), Counterfactual Regret Minimization (CFR). first baseline standard exact method
solving sequence-form EFG, CFR one leading approximate algorithms applied EFG. experimental results demonstrate advantages double-oracle
algorithm three different classes realistic EFGs. also test impact different
variants main loop algorithm described Section 4.3.
compare three variants sequence-form double-oracle algorithm: (1) DO-b
variant best-responses calculated players iteration;
(2) DO-sa calculates best-response single player iteration according
simple alternating policy; (3) DO-swp variant best-response
calculated single player according worse-player selection policy.
variants double-oracle algorithm use default strategy first
action applicable state played default.
Since standardized collection zero-sum extensive-form games benchmark purposes, use several specific games evaluate double-oracle algorithm
identify strengths weaknesses algorithm. games selected
evaluate performance different conditions, games differ maximal
utility players gain, causes imperfect information, structure
information sets. One key characteristics affects performance
double-oracle algorithm relative size support Nash equilibria (i.e., number sequences used NE non-zero probability). exist NE
small support, algorithm must necessarily add large fraction sequences
restricted game find solution, mitigating advantages double-oracle approach.
present results two types games double-oracle significantly outperforms FullLP instances: search game motivated border patrol Phantom
Tic-Tac-Toe. also present results simplified version poker doubleoracle algorithm always improve computation time. However, FullLP
also limited scalability due larger memory requirements cannot find solutions
larger variants poker, double-oracle algorithm able solve instances.
principal interest developing new generic methods solving extensive-form
games. Therefore, implemented algorithm generic framework modeling arbitrary extensive-form games.1 algorithms use domain-specific knowledge
implementation, rely specific ordering actions. drawbacks
generic implementation higher memory requirements additional overhead
algorithms. domain-specific implementation could improve performance
eliminating auxiliary data structures. run experiments using
single thread Intel i7 CPU running 2.8 GHz. algorithms given
maximum 10 GB memory Java heap space. used IBM CPLEX 12.5 solving
linear programs, parameter settings use single thread barrier solution
algorithm.
addition runtimes, analyze speed convergence double-oracle algorithms compare one state-of-the-art approximative algorithms, Counterfactual Regret Minimization (CFR). implemented CFR domain independent way
1. Source code available home pages authors.

853

fiBosansky, Kiekintveld, Lisy, & Pechoucek

based pseudocode work Lanctot (2013, p. 22). principle, sufficient
CFR maintain set information sets apply no-regret learning rule
information set. However, maintaining traversing set effectively
domain independent manner could affected implementation generic extensiveform games data structures (i.e., generating applicable actions states game,
applying actions, etc.). Therefore use implementation CFR traverses
complete game tree held memory maintain fairness comparison,
guarantee maximal possible speed convergence CFR algorithm. time
necessary build game tree included computation time CFR.
6.1 Test Domains
Search Games first test belongs class search (or pursuit-evasion) games,
often used experimental evaluation double-oracle algorithms (McMahan et al., 2003;
Halvorson et al., 2009). search game two players: patroller (or defender)
evader (or attacker). game played directed graph (see Figure 6),
evader aims cross safely starting node (E) destination node (D).
defender controls two units move intermediate nodes (the shaded areas)
trying capture evader occupying node evader. turn
players move units simultaneously current node adjacent node,
units stay location. exception evader cannot stay
two leftmost nodes. pre-determined number turns made without either player
winning, game draw. example win-tie-loss game utility
values set {1, 0, 1}.
Players unaware location actions player one exception
evader leaves tracks visited nodes discovered defender visits
nodes later. game also includes option evader avoid leaving tracks
using special move (a slow move) requires two turns simulate evader covering
tracks.
Figure 6 shows examples graphs used experiments. patrolling units
move shaded areas (P1,P2), start node shaded
areas. Even though graph small, concurrent movement units implies large
branching factor (up 50 one turn) thus large game trees (up 1011 nodes).
experiments used three different graphs, varied maximum number turns
game (from 3 7), altered ability attacker perform slow
moves (labeled SA slow moves allowed, SD otherwise).
Phantom Tic-Tac-Toe second game blind variant well-known game
Tic-Tac-Toe (e.g., used Lanctot et al., 2012). game played 3 3 board,
two players (cross circle) attempt place 3 identical marks horizontal, vertical,
diagonal row win game. blind variant, players unable observe
opponents moves player knows opponent made move
turn. Moreover, player tries place mark square already occupied
opponents mark, player learns information place mark
square. Again, utility values game set {1, 0, 1}.
854

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

Figure 6: Three variants graph used experiments search game; refer
G1 (left), G2 (middle), G3 (right).
uncertainty phantom Tic-Tac-Toe makes game large ( 109 nodes).
addition, since one player try several squares move successful, players
necessarily alternate making moves. rule makes structure
information sets rather complicated since opponent never learns many attempts
first player actually performed, single information set contain nodes different
depths game tree.
Poker Games Poker frequently studied literature example large
extensive-form game imperfect information. include experiments simplified
two-player poker game inspired Leduc Holdem.
version poker, player starts amount chips
players required put number chips pot (called ante). next
step, Nature player deals single card player (the opponent unaware
card) betting round begins. player either fold (the opponent wins pot),
check (let opponent make next move), bet (being first add amount
chips pot), call (add amount chips equal last bet opponent
pot), raise (match increase bet opponent). raise made
players, betting round ends, Nature player deals one card
table, second betting round rules begins. second betting
round ends, outcome game determined player wins if: (1) private card
matches table card opponents card match, (2) none players
cards matches table card private card higher private card
opponent, (3) opponent folds. utility value amount chips player
lost. player wins, game draw pot split.
experiments alter number types cards (from 3 4;
3 types cards Leduc), number cards type (from 2 3; set 2 Leduc),
maximum length sequence raises betting round (ranging 1 4; set 1
Leduc), number different sizes bets (i.e., amount chips added pot)
bet/raise actions (ranging 1 4; set 1 Leduc).
6.2 Results
Search Games results search game scenarios show sequence-form
double-oracle algorithm particularly successful applied games NEs
small support exist. Figure 7 shows comparison running times FullLP
variants double-oracle algorithm (note logarithmic y-scale). variants
855

fiBosansky, Kiekintveld, Lisy, & Pechoucek

102

101

100

103

FullLP
DO-B
DO-SA
DO-SWP
Time [s] (log scale)

Time [s] (log scale)

103

G1-SD

G2-SD

G3-SD

G1-SA

G2-SA

102

101

100

G3-SA

Search Game Scenarios - Depth 6

FullLP
DO-B
DO-SA
DO-SWP

G1-SD

G2-SD

G3-SD

G1-SA

G2-SA

G3-SA

Search Game Scenarios - Depth 7

Figure 7: Comparison running times 3 different graphs either slow moves
allowed (SA) disallowed (SD), depth set 6 (left subfigure) 7 (right subfigure).
Missing values FullLP algorithm indicate algorithm runs memory.

double-oracle algorithm several orders magnitude faster FullLP.
apparent fully-connected graph (G2) generates largest game tree.
slow moves allowed depth set 6, takes almost 100 seconds FullLP
solve instance game variants double-oracle algorithms solve
game less 3 seconds. Moreover, depth increased 7, FullLP
unable solve game due memory constraints, fastest variant DO-swp
solved game less 5 seconds. Similar results obtained graphs.
graph G1 induced game difficult double-oracle algorithm:
depth set 7, takes almost 6 minutes FullLP solve instance,
fastest variant DO-swp solved game 21 seconds. reason even though
game tree largest, complex structure information sets.
due limited compatibility among sequences players; patrolling
unit P1 observes tracks top-row node, second patrolling unit P2 capture
evader top-row node, middle-row node.
Comparing different variants sequence-form double-oracle algorithm
show consistent results. variant consistently better game since
double-oracle variants typically able compute Nash equilibrium quickly.
However, DO-swp often fastest settings difference quite significant.
speed-up variant offers apparent G1 graph. average
instances search game, DO-sa uses 92.59% computation time DO-b,
DO-swp uses 88.25%.
Table 3 shows breakdown cumulative computation time spent different components double-oracle algorithm: solving restricted game (LP), calculating best
responses (BR), creating valid restricted game selecting new sequences add
(Validity). results show due size game, computation
best-response sequences takes majority time (typically around 75% larger
instances), creating restricted game solving takes small fraction
total time. also noticeable size final restricted game small
856

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

DO-B

DO-SA

DO-SWP

CFR

Bounds Interval Size [-] (log scale)

10
1
0.1
0.01
0.001
0.0001
1e-05
0

50

100
Time [s]

150

200

Figure 8: Convergence variants double-oracle algorithm CFR search
game domain: y-axis displays current approximation error.
Algorithm
FullLP
DO-b
DO-sa
DO-swp

Overall [s]
351.98
81.51
54.32
21.15

LP [s]

6.97
5.5
1.93

BR [s]

63.39
39.11
16.28

Validity [s]

10.58
9.09
2.47

Iterations

187
344
209

|0 |

|01 |( |11 | )

252 (17.22%)
264 (18.05%)
193 (13.19%)

|0 |

|02 |( |22 | )

711 (0.26%)
649 (0.24%)
692 (0.25%)

Table 3: Cumulative running times different components double-oracle algorithm,
iterations, size restricted game terms number sequences compared
size complete game. results shown scenario G1, depth 7, allowed
slow moves.
compared original game, since number sequences second player (the
defender) less 1% (there 273,099 sequences defender).
Finally, analyze convergence rate variants double-oracle algorithm.
results depicted Figure 8, size interval given bounds
ViU B ViLB defines current error double-oracle algorithm |ViU B ViLB |.
convergence rate CFR algorithm also depicted. error CFR calculated
way, sum best-response values current mean strategies
CFR algorithm. see variants double-oracle algorithm perform
similarly error drops quickly 1 iterations later version
algorithm quickly converges exact solution. results show game
double-oracle algorithm quickly find correct sequences actions compute
exact solution, spite size game. comparison, CFR algorithm
also quickly learn correct strategies information sets, convergence
long tail. 200 seconds, error CFR equal 0.0657 dropping
slowly (0.0158 1 hour). error CFR quite significant considering value
game case (0.3333).
Phantom Tic-Tac-Toe results Phantom Tic-Tac-Toe confirm game
also suitable sequence-form double-oracle algorithm. Due size game,
baseline algorithms (the FullLP CFR) ran memory able
857

fiBosansky, Kiekintveld, Lisy, & Pechoucek

DO-SA

DO-SWP
DO-B
DO-SA
DO-SWP

1
Time [s] (log scale)

Bounds Interval Size [-] (log scale)

DO-B

0.1
0.01
0.001

104

0.0001
1e-05
0

5000

10000
15000
Time [s]

20000

25000

103

Random

Domain-dependent

Different Action Ordering Phantom Tic-Tac-Toe

Figure 9: (left) Comparison convergence rate double-oracle variants Phantom Tic-Tac-Toe; (right) Comparison performance double-oracle variants
Phantom Tic-Tac-Toe domain-specific move ordering default strategy used.
Algorithm
FullLP
DO-b
DO-sa
DO-swp

Overall [s]
N/A
21,197
17,667
17,589

LP [s]

2,635
2,206
2,143

BR [s]

17,562
14,560
14,582

Validity [s]

999
900
864

Iterations

335
671
591

|0 |

|01 |( |11 | )

7,676 (0.60%)
7,518 (0.59%)
8,242 (0.65%)

|0 |

|02 |( |22 | )

10,095 (0.23%)
9,648 (0.22%)
8,832 (0.20%)

Table 4: Cumulative running times different components double-oracle algorithm
game Phantom Tic-Tac-Toe.
solve game. Therefore, compare times different variants
double-oracle algorithm. Figure 9 (left subfigure) shows overall performance three
variants double-oracle algorithm form convergence graph. see
performance two variants similar, performance DO-sa DO-swp
almost identical. hand, results show DO-b converges significantly
slower.
time breakdown variants double-oracle algorithm shown Table 4.
Similarly previous case, majority time ( 83%) spent calculating
best responses. variants double-oracle algorithm, DO-swp variant
fastest one. converged significantly fewer iterations compared DO-sa
variant (iterations twice expensive DO-b variant).
present results demonstrate potential combining sequenceform double-oracle algorithm domain-specific knowledge. Every variant doubleoracle algorithm use move ordering based domain-specific heuristics. move
ordering determines default strategy (recall algorithm uses first action
default strategy player), direction search best response
algorithms. replacing randomly generated move ordering heuristic one
chooses better actions first, results show significant improvement performance
variants (see Figure 9, right subfigure), even though changes
rest algorithm. variant able solve game less 3 hours,
took 2 hours fastest DO-swp variant.
858

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

200

FullLP
DO-B
DO-SA
DO-SWP
103
Time [s] (log scale)

150
Time [s]

104

FullLP
DO-B
DO-SA
DO-SWP

100

102

101

50

100

0
R1

R2

R3

R4

Increasing number allowed "Raise Actions"

B1

B2

B3

B4

Increasing size possible bets

Figure 10: Comparison running times different variants simplified poker
game. left subfigure shows computation times increasing number raise
actions allowed, right subfigure shows computation times increasing number
different bet sizes raise/bet actions.
Poker Games Poker represents game double-oracle algorithms perform well sequence-form LP often faster smaller instances. One significant
difference compared previous games size NE support larger
(around 5% sequences larger instances). Secondly, game trees poker games
relatively shallow imperfect information game due Nature.
result, double-oracle algorithms require larger number iterations add
sequences restricted game (up 10% sequences player added even
largest poker scenarios) order find exact solution. However, increasing
depth and/or branching factor, size game grows exponentially FullLP
able solve largest instances due memory constraints.
Figure 10 shows selected results simplified poker variants. results
left subfigure show computation times increasing depth game allowing
players re-raise (players allowed re-raise opponent certain number
times). remaining parameters fixed 3 types cards, 2 cards type, 2
different betting sizes. size game grows exponentially, number possible
sequences increasing 210,937 player R4 scenario. computation time
FullLP directly related size tree increases exponentially
increasing depth (note standard scale). hand, increase
less dramatic variants double-oracle algorithm. DO-swp variant
fastest largest scenario FullLP solved instance 126 seconds,
took 103 seconds DO-swp. Finally, FullLP able solve games
increase length R5 due memory constraints, computation time
double-oracle algorithms increases marginally.
right subfigure Figure 10 shows increase computation time increasing number different bet sizes raise/bet actions. remaining parameters
fixed 4 types cards, 3 cards type, 2 raise actions allowed. Again,
game grows exponentially increasing branching factor. number sequences
increases 685,125 player B4 scenario, computation time
859

fiBosansky, Kiekintveld, Lisy, & Pechoucek

DO-B

DO-SA

DO-SWP

CFR

DO-B

DO-SWP

CFR

10
Bounds Interval Size [-] (log scale)

Bounds Interval Size [-] (log scale)

10

DO-SA

1
0.1
0.01
0.001
0.0001
1e-05

1
0.1
0.01
0.001
0.0001
1e-05

0

50

100

150

200
250
Time [s]

300

350

400

0

200

400

600
800
Time [s]

1000

1200

1400

Figure 11: Comparison convergence variants double-oracle algorithm
CFR two variants simplified poker 4 types cards, 3 cards
type. 4 raise actions allowed, 2 different bet sizes left subfigure;
2 raise actions allowed, 3 different bet sizes right subfigure.
Algorithm
FullLP
DO-b
DO-sa
DO-swp

Overall [s]
278.18
234.60
199.24
182.68

LP [s]

149.32
117.71
108.95

BR [s]

56.04
51.25
48.25

Validity [s]

28.61
29.59
24.8

Iterations

152
289
267

|0 |

|01 |( |11 | )

6,799 (1.81%)
6,762 (1.80%)
6,572 (1.75%)

|0 |

|02 |( |22 | )

6,854 (1.83%)
6,673 (1.78%)
6,599 (1.76%)

Table 5: Cumulative running times different components double-oracle algorithm,
iterations, sizes restricted game terms number sequences compared
size complete game. results shown poker scenario 4 raise
actions allowed, 2 different betting values, 4 types cards, 3 cards type.
algorithms increases exponentially well (note logarithmic scale). results show
even increasing branching factor, double-oracle variants tend slower
solving FullLP. However, FullLP ran memory largest
B4 setting, double-oracle variants able find exact solution using less
memory.
Comparing different variants double-oracle algorithm using convergence
graph (see Figure 11) decomposition computation times (see Table 5) shows
DO-swp fastest variant selected scenario (and nearly poker
scenarios). Decomposition overall time shows majority computation
time spent solving restricted game LP (up 65%). decomposition also shows
DO-swp typically faster due lower number iterations. addition,
final size restricted game typically smallest variant. average
instances poker games, DO-sa uses 86.57% computation time DO-b,
DO-swp uses 82.3% computation time.
Convergence poker games slower compared search games similar size (note
logarithmic scale Figure 11). Comparing double-oracle algorithm variants CFR
shows interesting result left subfigure. Due size game, speed
CFR convergence nearly double-oracle algorithms first
860

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

iterations. However, double-oracle algorithms continue converge roughly
rate able find exact solution, error CFR algorithm decreases
slowly. scenario depicted left subfigure, CFR algorithm converged
error 0.1212 (the value game case 0.09963) 400 seconds.
1 hour, error dropped 0.0268. scenarios shallow game trees
larger branching factor, convergence CFR faster beginning compared
double-oracle algorithms (right subfigure Figure 11). However, main disadvantage
CFR long tail convergence still case error 1600 seconds
still 0.0266 (the value game 0.09828).
6.3 Discussion Results
experimental results support several conclusions. results demonstrate
sequence-form double-oracle algorithm able compute exact solution much larger
games compared state-of-the-art exact algorithm based sequence-form linear
program. Moreover, experimentally shown realistic games
small fraction sequences necessary find solution game. cases,
double-oracle algorithms also significantly speed computation time. results
indicate DO-swp variant typically fastest, cases. selecting
player currently worse bound performance, DO-swp version
add important sequences, prove better sequences adjust
upper bound value faster.
Comparing speed convergence double-oracle algorithms state-ofthe-art approximative algorithm CFR showed CFR quickly approximates solution
first iterations. However, convergence CFR long tail CFR
able find exact solution larger games reasonable time. Another interesting
observation games convergence rate double-oracle algorithms
CFR similar first iterations, double-oracle algorithms continue
rate find exact solution, long tail convergence remains CFR.
despite fact implementation CFR advantage complete
game tree including states histories memory.
Unfortunately, difficult characterize exact properties games
double-oracle algorithms perform better terms computation time compared
algorithms. Certainly, double-oracle algorithm suitable games
equilibria large support due necessity large number iterations.
However, small support equilibrium sufficient condition. apparent
due two graphs shown poker experiments, either depth game tree
branching factor increased. Even though game grows exponentially
size support decreases 2.5% cases, behavior double-oracle
algorithms quite different. conjecture games longer sequences suit
double-oracle algorithms better, since several actions form best-response sequences
added single iteration. contrasts shallow game trees large
branching factors, iterations necessary add multiple actions. However,
deeper analysis identify exact properties games suitable open
question must analyzed normal-form games first.
861

fiBosansky, Kiekintveld, Lisy, & Pechoucek

7. Conclusion
present novel exact algorithm solving two player zero-sum extensive-form games
imperfect information. approach combines compact sequence-form representation extensive-form games iterative algorithmic framework double-oracle
methods. integrates two successful approaches solving large scale games
yet brought together general class games algorithm addresses.
main idea algorithm restrict game allowing players play
restricted set sequences available sequences actions, iteratively expand
restricted game time using fast best-response algorithms. Although worst
case double-oracle algorithm may need add possible sequences, experimental
results different domains prove double-oracle algorithm find exact Nash
equilibrium prior constructing full linear program complete game. Therefore,
sequence-form double-oracle algorithm reduces main limitation sequence-form
linear programmemory requirementsand able solve much larger games compared
state-of-the-art methods. Moreover, since algorithm able identify sequences
promising actions without domain-specific knowledge, also provide significant
runtime improvements.
proposed algorithm also another crucial advantage compared current state
art. double-oracle framework offers decomposition problem computing
Nash equilibrium separate sub-problems, including best-response algorithms,
choice default strategy, algorithms constructing solving restricted
game. developed solutions sub-problems domain-independent manner. However, also view algorithm general framework
specialized domain-specific components take advantage structure specific
problems improve performance sub-problems. lead substantial
improvements speed algorithm, number iterations, well reducing
final size restricted game. demonstrated potential domain-specific
approach game Phantom Tic-Tac-Toe. Another example fast best-response
algorithms operate public tree (i.e., compact representation games
publicly observable actions; see Johanson, Bowling, Waugh, & Zinkevich, 2011) exploited games like poker. Finally, formal analysis identifies key properties
domain-specific implementations need satisfy guarantee convergence
correct solution game.
algorithm opens large number directions future work. represents new
class methods solving extensive-form games imperfect information operates
differently common approaches (e.g., counterfactual regret minimization),
many possible alternatives improve performance algorithm remain
investigated. Examples include sophisticated calculation utility values
temporary leaves, alternative strategies expanding restricted game, removing
unused sequences restricted game. broader analysis using sequenceform double-oracle algorithm approximation technique performed, possibly
exploring alternative approximative best-response algorithms based sampling (e.g.,
Monte Carlo) techniques.
862

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

also several theoretical questions could investigated. First, performance double-oracle algorithm depends strongly number iterations
sequences need added. However, theoretical question regarding expected
number iterations thus speed convergence double-oracle algorithm
explored even simpler game models (e.g., games normal form).
analysis simpler models needed identify general properties games
double-oracle methods tend faster identify optimal way expanding
restricted game.

Acknowledgements
Earlier versions paper published European Conference Artificial
Intelligence (ECAI) (Bosansky, Kiekintveld, Lisy, & Pechoucek, 2012) conference
Autonomous Agents Multi Agent Systems (AAMAS) (Bosansky, Kiekintveld, Lisy,
Cermak, & Pechoucek, 2013). major additions full version include (1) novel,
detailed description parts algorithm, (2) introduction analysis
different policies player selection main loop double-oracle algorithm,
(3) new experiments phantom tic-tac-toe domain together thorough
analysis experimental results domains, including analysis convergence
algorithm, (4) experimental comparison CFR, finally (5) extended analysis
related work.
research supported Czech Science Foundation (grant no. P202/12/2054)
U.S. Army Research Office (award no. W911NF-13-1-0467).

References
Barnhart, C., Johnson, E. L., Nemhauser, G. L., Savelsbergh, M. W. P., & Vance, P. H.
(1998). Branch-And-Price: Column Generation Solving Huge Integer Programs.
Operations Research, 46, 316329.
Bosansky, B., Kiekintveld, C., Lisy, V., Cermak, J., & Pechoucek, M. (2013). Doubleoracle Algorithm Computing Exact Nash Equilibrium Zero-sum Extensiveform Games. Proceedings International Conference Autonomous Agents
Multiagent Systems (AAMAS), pp. 335342.
Bosansky, B., Kiekintveld, C., Lisy, V., & Pechoucek, M. (2012). Iterative Algorithm
Solving Two-player Zero-sum Extensive-form Games Imperfect Information.
Proceedings 20th European Conference Artificial Intelligence (ECAI), pp.
193198.
Cermak, J., Bosansky, B., & Lisy, V. (2014). Practical Performance Refinements
Nash Equilibria Extensive-Form Zero-Sum Games. Proceedings European
Conference Artificial Intelligence (ECAI), pp. 201206.
Dantzig, G., & Wolfe, P. (1960). Decomposition Principle Linear Programs. Operations
Research, 8, 101111.
Ganzfried, S., & Sandholm, T. (2013). Improving Performance Imperfect-Information
Games Large State Action Spaces Solving Endgames. Computer
863

fiBosansky, Kiekintveld, Lisy, & Pechoucek

Poker Imperfect Information Workshop National Conference Artificial
Intelligence (AAAI).
Gibson, R., Lanctot, M., Burch, N., Szafron, D., & Bowling, M. (2012). Generalized Sampling Variance Counterfactual Regret Minimization. Proceedings 26th
AAAI Conference Artificial Intelligence, pp. 13551361.
Halvorson, E., Conitzer, V., & Parr, R. (2009). Multi-step Multi-sensor Hider-seeker Games.
Proceedings Joint International Conference Artificial Intelligence (IJCAI),
pp. 159166.
Hoda, S., Gilpin, A., Pena, J., & Sandholm, T. (2010). Smoothing Techniques Computing
Nash Equilibria Sequential Games. Mathematics Operations Research, 35 (2),
494512.
Jain, M., Conitzer, V., & Tambe, M. (2013). Security Scheduling Real-world Networks.
Proceedings International Conference Autonomous Agents Multiagent
Systems (AAMAS), pp. 215222.
Jain, M., Korzhyk, D., Vanek, O., Conitzer, V., Tambe, M., & Pechoucek, M. (2011). Double
Oracle Algorithm Zero-Sum Security Games Graph. Proceedings 10th
International Conference Autonomous Agents Multiagent Systems (AAMAS),
pp. 327334.
Johanson, M., Bowling, M., Waugh, K., & Zinkevich, M. (2011). Accelerating Best Response
Calculation Large Extensive Games. Proceedings 22nd International Joint
Conference Artificial Intelligence (IJCAI), pp. 258265.
Koller, D., & Megiddo, N. (1992). Complexity Two-Person Zero-Sum Games
Extensive Form. Games Economic Behavior, 4, 528552.
Koller, D., Megiddo, N., & von Stengel, B. (1996). Efficient Computation Equilibria
Extensive Two-Person Games. Games Economic Behavior, 14 (2), 247259.
Koller, D., & Megiddo, N. (1996). Finding Mixed Strategies Small Supports Extensive Form Games. International Journal Game Theory, 25, 7392.
Kreps, D. M., & Wilson, R. (1982). Sequential Equilibria. Econometrica, 50 (4), 86394.
Lanctot, M. (2013). Monte Carlo Sampling Regret Minimization Equilibrium Computation Decision Making Large Extensive-Form Games. Ph.D. thesis, University Alberta.
Lanctot, M., Gibson, R., Burch, N., Zinkevich, M., & Bowling, M. (2012). No-Regret
Learning Extensive-Form Games Imperfect Recall. Proceedings 29th
International Conference Machine Learning (ICML 2012), pp. 121.
Lanctot, M., Waugh, K., Zinkevich, M., & Bowling, M. (2009). Monte Carlo Sampling
Regret Minimization Extensive Games. Advances Neural Information
Processing Systems (NIPS), pp. 10781086.
Lee, C.-S., Wang, M.-H., Chaslot, G., Hoock, J.-B., Rimmel, A., Teytaud, O., Tsai, S.-R.,
Hsu, S.-C., & Hong, T.-P. (2009). Computational Intelligence Mogo Revealed
Taiwans Computer Go Tournaments. IEEE Transactions Computational Intelligence AI Games, 1, 7389.
864

fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information

Letchford, J., & Vorobeychik, Y. (2013). Optimal Interdiction Attack Plans. Proceedings 12th International Conference Automonous Agents Multiagent
Systems (AAMAS), pp. 199206.
Lisy, V., Kovarik, V., Lanctot, M., & Bosansky, B. (2013). Convergence Monte Carlo Tree
Search Simultaneous Move Games. Advances Neural Information Processing
Systems (NIPS), Vol. 26, pp. 21122120.
McMahan, H. B. (2006). Robust Planning Domains Stochastic Outcomes, Adversaries, Partial Observability. Ph.D. thesis, Carnegie Mellon University.
McMahan, H. B., & Gordon, G. J. (2007). Fast Bundle-based Anytime Algorithm
Poker Convex Games. Journal Machine Learning Research - Proceedings
Track, 2, 323330.
McMahan, H. B., Gordon, G. J., & Blum, A. (2003). Planning Presence Cost
Functions Controlled Adversary. Proceedings International Conference
Machine Learning, pp. 536543.
Miltersen, P. B., & Srensen, T. B. (2008). Fast Algorithms Finding Proper Strategies
Game Trees. Proceedings Symposium Discrete Algorithms (SODA), pp.
874883.
Miltersen, P. B., & Srensen, T. B. (2010). Computing Quasi-Perfect Equilibrium
Two-Player Game. Economic Theory, 42 (1), 175192.
Pita, J., Jain, M., Western, C., Portway, C., Tambe, M., Ordonez, F., Kraus, S., & Parachuri,
P. (2008). Deployed ARMOR protection: Application Game-Theoretic Model
Security Los Angeles International Airport. Proceedings 8th International Conference Autonomous Agents Multiagent Systems (AAMAS), pp.
125132.
Ponsen, M. J. V., de Jong, S., & Lanctot, M. (2011). Computing Approximate Nash Equilibria Robust Best-Responses Using Sampling. Journal Artificial Intelligence
Research (JAIR), 42, 575605.
Sandholm, T. (2010). State Solving Large Incomplete-Information Games,
Application Poker. AI Magazine, special issue Algorithmic Game Theory, 13
32.
Selten, R. (1975). Reexamination Perfectness Concept Equilibrium Points
Extensive Games. International Journal Game Theory, 4, 2555.
Selten, R. (1965). Spieltheoretische Behandlung eines Oligopolmodells mit Nachfragetrgheit
[An oligopoly model demand inertia]. Zeitschrift fur die Gesamte Staatswissenschaft, 121, 301324.
Shafiei, M., Sturtevant, N., & Schaeffer, J. (2009). Comparing UCT versus CFR Simultaneous Games. IJCAI Workshop General Game Playing.
Shieh, E., An, B., Yang, R., Tambe, M., Baldwin, C., Direnzo, J., Meyer, G., Baldwin, C. W.,
Maule, B. J., & Meyer, G. R. (2012). PROTECT : Deployed Game Theoretic System
Protect Ports United States. International Conference Autonomous
Agents Multiagent Systems (AAMAS), pp. 1320.
865

fiBosansky, Kiekintveld, Lisy, & Pechoucek

Shoham, Y., & Leyton-Brown, K. (2009). Multiagent Systems: Algorithmic, GameTheoretic, Logical Foundations. Cambridge University Press.
Tambe, M. (2011). Security Game Theory: Algorithms, Deployed Systems, Lessons
Learned. Cambridge University Press.
Tsai, J., Rathi, S., Kiekintveld, C., Ordonez, F., & Tambe, M. (2009). IRIS - Tool
Strategic Security Allocation Transportation Networks Categories Subject
Descriptors. Proceedings 8th International Conference Autonomous Agents
Multiagent Systems (AAMAS), pp. 3744.
van Damme, E. (1984). Relation Perfect Equilibria Extensive Form Games
Proper Equilibria Normal Form Games. Game Theory, 13, 113.
van Damme, E. (1991). Stability Perfection Nash Equilibria. Springer-Verlag.
von Stengel, B. (1996). Efficient Computation Behavior Strategies. Games Economic
Behavior, 14, 220246.
Wilson, R. (1972). Computing Equilibria Two-Person Games Extensive Form.
Management Science, 18 (7), 448460.
Zinkevich, M., Johanson, M., Bowling, M., & Piccione, C. (2008). Regret Minimization
Games Incomplete Information. Advances Neural Information Processing
Systems (NIPS), 20, 17291736.
Zinkevich, M., Bowling, M., & Burch, N. (2007). New Algorithm Generating Equilibria
Massive Zero-Sum Games. Proceedings National Conference Artificial
Intelligence (AAAI), pp. 788793.

866

fiJournal Artificial Intelligence Research 51 (2014) 725778

Submitted 04/14; published 12/14

Tutorial Structured Continuous-Time Markov Processes
Christian R. Shelton

cshelton@cs.ucr.edu

University California, Riverside

Gianfranco Ciardo

ciardo@iastate.edu

Iowa State University

Abstract
continuous-time Markov process (CTMP) collection variables indexed
continuous quantity, time. obeys Markov property distribution
future variable independent past variables given state present time.
introduce continuous-time Markov process representations algorithms filtering,
smoothing, expected sufficient statistics calculations, model estimation, assuming
prior knowledge continuous-time processes basic knowledge probability
statistics. begin describing flat unstructured Markov processes move
structured Markov processes (those arising state spaces consisting assignments
variables) including Kronecker, decision-diagram, continuous-time Bayesian network
representations. provide first connection decision-diagrams continuoustime Bayesian networks.

1. Tutorial Goals
tutorial intended readers interested learning continuous-time Markov
processes, particular compact structured representations them. assumed
reader familiar general probability statistics knowledge
discrete-time Markov chains perhaps hidden Markov model algorithms.
tutorial deals Markovian systems, require variables observed. Thus, hidden variables used model long-range interactions
among observations. models, given instant assignment state variables sufficient describe future evolution system. variables
real-valued (continuous) times. consider evidence observations regularly
spaced, irregularly spaced, continuous intervals. evidence patterns change
model variable time.
deal exclusively discrete-state continuous-time systems. Real-valued variables
important many situations, keep scope manageable, treat
here. refer work Sarkka (2006) machine-learning-oriented treatment
filtering smoothing models. literature parameter estimation
scattered. constrain discussion systems finite states, although
many concepts extended countably infinite state systems.
concerned two main problems: inference learning (parameter estimation). chosen familiar applicable researchers
artificial intelligence. points also discuss computation steady-state properties, especially model research concentrates computation.
c
2014
AI Access Foundation. rights reserved.

fiShelton & Ciardo

first section (Section 2) covers basics flat (unstructured state-space) continuoustime Markov processes. remaining sections discuss compact representations. tutorials goal make mathematical foundations clear lay current research
landscape detailed papers read easily.
1.1 Related Models
many non-Markov continuous time models. Gaussian processes (Williams, 1998)
best-known model continuous-valued processes. discrete-valued processes,
models build upon Poisson processes, general marked processes. Poisson
process memoryless, make interesting model, researchers usually generalize allow
rate event function processs history.
Poisson networks (Rajaram, Graepel, & Herbrich, 2005) constrain function depend counts number events (possibly different event types)
finite time window. cascade Poisson process model (Rajaram et al., 2005) defines
rate function sum kernel applied historic event. kernel
parameters effect time passing, overall event rate, chance one type
event follows another. Piecewise-constant intensity models (PCIMs) (Gunawardana, Meek,
& Xu, 2012; Parikh, Gunamwardana, & Meek, 2012) define intensity function
decision tree, internal nodes tests drawn set pre-specified binary tests
history. Forest-based point processes (Weiss & Page, 2013) extend allowing
intensity function product set functions, PCIM-like tree. Didelez
(2008) presents generalization continuous-time Bayesian networks (see Section 5)
inhomogeneous point processes, without specific parameterizations algorithms.
1.2 Continuous Time
Contemporary computers discrete-time computation engines (or least present model
one). Therefore, would consider continuous-time model? quickest answer
analogy: build models non-temporal systems employing real-valued variables.
tools linear algebra, calculus, like allow us derive analyze algorithms
methods. Yet, end implemented discrete-valued computers
finite memory precision. However, find abstraction continuous-valued variables useful make approximations final implementation employing
fixed- floating-point precision arithmetic.
Similarly, productive treat time continuous quantity. allows us
naturally discuss reason systems
1. Events, measurements, durations irregularly spaced,
2. Rates vary orders magnitude,
3. Durations continuous measurement need expressed explicitly.
happen asynchronous systems. dynamic systems interest asynchronous: events measurements (or both) occur based global clock. Social
networks, phylogenetic trees, computer system logs examples.
Note underlying system model continuous-time, observations measurements model need continuous. directly treat discrete-time observations, regular irregular intervals.
726

fiTutorial Structured Continuous-Time Markov Processes

1.3 Discrete Time
Clearly given continuous-time system specification, discretization time values could made without introducing much approximation error. conversion
time real-valued integral makes mathematically difficult flexible
treat time algorithmically. makes development computationally
efficient algorithms difficult. instance, discrete-time model, natural
computations proceed one time step time. However, uneventful times,
computationally overly burdensome. continuous-time model,
natural time step, simpler think methods jump
uneventful time periods. Additionally, oddities Markov chains built
discretizing continuous time. Finally, full system specification may known
discretization must selected (for instance, parameters must estimated).
1.3.1 Time Discretization Markovian-ness
Consider two-state Markov chain X described stochastic matrix1


0.75 0.25
.
T1 =
0.5 0.5

(1)

elements 1 probabilities p(Xt | Xt1 ) value Xt Xt1 .
one time unit, probability moving state 1 state 2 0.25, example.
1 describes continuous-time system, sampled period 1 time unit,
matrix 1/2 describing system, sampled period 12 time unit
(or twice sampling rate). Indeed is:


0.83 0.17
.
(2)
1/2 =
0.33 0.67
verified:
P (Xt = j | Xt1 = i) =

X

P (Xt1/2 = k | Xt1 = i)P (Xt = j | Xt1/2 = k)

k

1 (i, j) =

X

1/2 (i, k)T 1/2 (k, j)

k

1 = 1/2 1/2 .
is, 1/2 matrix square root 1 .
take different two-state Markov chain transition matrix


0.1 0.9
S1 =
0.9 0.1
construct corresponding transition matrix half sampling period, 1/2 :


0.5 + .447i 0.5 .447i
1/2 =
.
0.5 + .447i 0.5 .447i

(3)

(4)

1. use row-stochastic matrices exclusively tutorial. column-stochastic matrices
often used discrete time, row-stochastic matrices common continuous time.

727

fiShelton & Ciardo

x1

x2

x3

x1

x3

y1

y2

y3

y1

y3

z1

z2

z3

z1

z3

x1

x3

y1

y3

z1

z3

(a) unrolled DBN

(b) marginalized DBN

Figure 1: Example (a) DBN unrolled, (b) DBN marginalized twice
sampling periodicity

real-valued stochastic matrix describing processes 1 , half
sampling periodicity. Put differently, two-state continuous-time Markov system
sampled rate 1 time unit produces Markov chain transition
matrix 1 .
problem generating 1/2 arises 1 negative eigenvalue (by contrast,
eigenvalues 1 positive). general, stochastic matrices positive
eigenvalues correspond continuous-time Markov process sampled given periodicity.
viewed two ways. First, means set continuous-time Markov
processes smaller set discrete-time Markov processes. Second, means
processes Markovian sampled particular periodicity
way extend time points outside periodicity would construct
non-Markovian (and non-stationary) process.
periodicity discrete-time Markov chain inherent process,
result concern. However, many systems natural sampling rate.
rate chosen computational measurement convenience. case, must
careful employ Markovian assumption. Or, directly model
underlying system continuous time.
1.3.2 Independencies Markovian-ness
similar problem arises independencies. describe problem terms
dynamic Bayesian networks (DBNs) (Dean & Kanazawa, 1989). unfamiliar DBNs,
reader may skip next section.
Consider DBN Figure 1(top,a), unrolled one time step.
marginalize middle time slice result DBN Figure 1(top,b):
model, twice sampling periodicity. However, perhaps wish go
opposite direction (to half sampling periodicity). Figure 1(bottom,b) shows DBN
728

fiTutorial Structured Continuous-Time Markov Processes

0

Test Log-Likelihood

20

40

60

80
optimal
CTBN
100

= 0.1
= 1.0
= 5.0

120
2

4

6
Number Trajectories

8

10

Figure 2: Comparison learning DBNs different time-slice widths
two time units. DBN graph structure one time unit would marginalize
graph structure. may DBN, independencies expressed
graph structure two time units expressible graph structure half
sampling periodicity. independencies would buried parameters DBN
(if parameters possible, given previous discussion). means
independencies expressed DBNs graph function underlying process
sampling rate.
1.3.3 Structure Learning
Selection sampling rate theoretical problem. Nodelman, Shelton, Koller
(2003) demonstrate problem parameter estimation. particular, considered
data drawn continuous-time Markovian process eight variables (mostly binary).
resulting trajectories discretized time according parameter DBNs
(including structure) learned setting. Figure 2 shows test log-likelihood
accuracy function number training trajectories t. also shows
result discretizing time (the CTBN line, model explained Section 5).
surprising CTBN model best (as data
generated model), instructive best depends number
observed trajectories. means that, sampling periodicity model parameter,
choice cannot made independently amount data used estimate DBN.

2. Continuous-Time Markov Processes
continuous-time Markov process (CTMP) distribution trajectories. trajectory
(or sample) CTMP right-continuous piece-wise constant function real-valued
variable, time. Figure 3 illustrates example trajectories. states natural order729

fiShelton & Ciardo

3
2
1



(a) ordered states



(b) unordered states



(c) multiple state variables

Figure 3: Example continuous-time Markov process samples (trajectories)
ing, Figure 3(a) might natural depiction. states ordered, Figure 3(b)
depicts sample three-state system. later sections considering
large factored state spaces state assignment multiple variables. Figure 3(c) depicts trajectory.
finite CTMP defines set random variables, finite sample space
(the state space), indexed real-value usually denoted time. Let X
process. Markovian property states
X(t1 ) X(t3 ) | X(t2 ), t1 < t2 < t3 .

(5)

Throughout tutorial, describe distributions continuous discrete random variables. use lowercase letters densities continuous random
variables uppercase letters probabilities discrete random variables.
2.1 Parameterization
parameterize CTMP X starting distribution time = 0, P (X(0)) (and
restrict 0) intensity matrix QX (or Q context clear). starting
distribution discrete-time Markov chain, largely ignore it.
intensity matrix analogous transition matrix discrete-time process.
2.1.1 Comparison Discrete-Time
Consider following (roughly equivalent) discrete-time transition matrix continuoustime intensity matrix Q:




0.5 0.2 0.3
0.8
0.32
0.48
0.12 .
= 0.1 0.8 0.1
Q = 0.12 0.24
0.2 0.1 0.7
0.27
0.13 0.4
interpret row two ways. first row could viewed stating
process state 1, next time step 0.5 chance
state 1, 0.2 chance state 2, 0.3 chance state 3.
Alternatively, could viewed stating process state 1, remain
number steps geometrically distributed: Pr(stay n steps) = 0.5n . And,
leaves transition state 2 probability 0.2/0.5 = 0.4 state 3
probability 0.3/0.5 = 0.6.
intensity matrix Q two similar interpretations. first row states
process state 1, short period time , approximately 1 0.8 chance
state 1, 0.32 chance state 2, 0.48 chance state
730

fiTutorial Structured Continuous-Time Markov Processes

3. approximation error O(2 ). Alternatively, states process state
1 remains duration exponentially distributed: p(dwell time = t) = 0.8e0.8t .
leaves, transition state 2 probability 0.32/0.8 = 0.4 state
3 probability 0.48/0.8 = 0.6.
2.1.2 Racing Exponentials
also view row matrix describing racing exponential distributions.
two important properties exponential distribution. First, memoryless:
pZ (t) = pZ (t + s|Z > s)

Z exponentially distributed

(6)

thus right distribution dwell times Markovian process. (The amount
time process stayed state affect remaining dwell time.)
also closed minimization: Given collection random variables Z1 , Z2 , . . . , Zn ,
pZi (t) = ri eri

(7)

= min Zi

(8)

J = arg min Zi

(9)





implies
pY (t) = rert
rj
Pr(J = j) =
r

(10)
(11)


r=

n
X

ri .

(12)

i=1

is, set exponential distributions (potentially) different rates,
minimum (the time earliest one) also exponentially distributed rate equal
sum component rates. Furthermore, component causes minimum
independent time proportional components rate. Thus, view
row matrix set racing exponential distributions: one potential
next state rates dictated off-diagonal elements. Whichever potential transition
happens first one actually taken process, rest discarded.
2.1.3 Event Decomposition
Further, use build interpretation summation two intensity
matrices. Q = Q1 + Q2 , Q1 Q2 valid intensity matrices,
process Q viewed combination processes Q1 Q2
sub-processes race produce next transition: current state, two
sub-processes rate transition possible new states. Whichever
transition happens first switches state joint process continues new
731

fiShelton & Ciardo

state. also view two different event types intensity matrix.
process Q joint process running events time, throwing
away (marginalizing out) event types associated transitions, leaving
transitions themselves.
2.1.4 Infinitesimal Rate Semantics
formally, dynamics n-state CTMP described n-by-n intensity matrix
Q. diagonal elements Q non-positive non-diagonal elements Q
non-negative. rows Q sum 0 (thus diagonal elements negative row
sums, diagonal element excluded sum). denote i, j element
Q qi,j . Further, notational simplicity, let qi = qi,i . is, qi rate
leaving state i, absolute value corresponding diagonal element.
let p(t) row-vector marginal distribution process time t,
semantics Q stated
p(t + ) = p(t)(I + Q) + o() .

(13)

implies
p(t + ) p(t) = p(t)Q + o()

(14)

lim (p(t + ) p(t))/ = p(t)Q

(15)

dp(t)
= p(t)Q
dt

(16)

0

(17)
first-order linear homogeneous differential equation solution
p(t + s) = p(t)eQs

(18)

assuming > 0 initial conditions t, p(t), known. exponential
matrix exponential, defined Taylor expansion:
eQs =

k
X

k=0

k!

Qk .

(19)

Although often practical computationally, also express matrix exponential
terms eigenvalues ({i }) corresponding right left eigenvectors ({vi }
{wi }) Q:
X
eQs =
ei vi wi> .
(20)


Q finite size irreducible (there positive-rate path state
state), process ergodic (the notion cycling behavior exist
continuous-time Markov processes) exactly one eigenvalue equal 0.
corresponding right eigenvector unique steady-state (or stationary) distribution
process. process ergodic, may multiple 0 eigenvalues
unique stationary distribution. eigenvalues less 0 correspond
transients system. Therefore, Q always negative semi-definite.
732

fi0.6

0.6

0.5

0.5

Pr(state)

Pr(state)

Tutorial Structured Continuous-Time Markov Processes

0.4
0.3
0.2

0.3
0.2

0

2

4
time

6

0.1

8

0.6

0.6

0.5

0.5

Pr(state)

Pr(state)

0.1

0.4

0.4
0.3
0.2
0.1

0

2

4
time

6

8

0

2

4
time

6

8

0.4
0.3
0.2

0

2

4
time

6

0.1

8

fixed step-size

adaptive step-size

Figure 4: Propagation marginal distribution time 0 time 8 Euler integration. Left: fixed step-size. Right: adaptive step-size. Top: 11 evaluation points.
Bottom: 5 evaluation points.

2.2 Matrix Exponential
matrix exponential plays critical role many aspects reasoning continuoustime dynamic systems. first, would seem significant downside, relative
discrete-time systems. Propagation distribution p (as vector) n time steps discretetime system requires multiplication n (if stochastic transition matrix).
contrast, operation continuous-time requires calculation matrix
exponential, infinite sum matrix powers.
Consider computing marginal distribution time integrating differential
equation Equation 18. simplest method would use Euler integration
fixed step size t. amounts propagating fixed time interval multiplying
= + tQ. essentially discretizing time approximating
stochastic matrix resulting interval. propagate time requires t/t matrix
multiplications. shown left side Figure 4.
However, time continuous, need limit time steps uniform
size. choose adaptive step size, achieve accuracy fewer
evaluation points. Figure 4 demonstrates simplest adaptive scheme (Euler steps
step size proportional derivative magnitude). Note number
steps (computational complexity), accuracy adaptive steps better.
real applications, would use advanced differential equation solver
intelligent step size selection; see work Press, Teukolsky, Vetterling, Flannery (1992) introduction. Yet, idea essentially same: take larger
steps less interesting time periods. something similar discrete time
would require computations essentially convert discrete-time system continuous733

fiShelton & Ciardo

time one. Techniques like squaring scaling multiply large matrix powers also
applied matrix exponential. full discussion matrix exponential calculations, refer excellent treatments Moler Loan (2003) Najfeld Havel
(1994, 1995).
2.3 Uniformization
Uniformization (also called randomization) method converting questions
continuous-time Markov process ones discrete-time one (Grassmann, 1977).
Given intensity matrix Q, uniformization constructs stochastic matrix
= Q/ +

(21)

maxi qi (that is, less largest rate Q). example,
following uniformization = 0.5 (the smallest possible value ).




0.5
0.1
0.4
0.0 0.2 0.8
0.1 = 0.2 0.6 0.2
Q = 0.1 0.2
(22)
0.2
0.1 0.3
0.4 0.2 0.4
resulting stochastic matrix interpreted discrete-time process. However
equivalent sampling continuous-time process uniform intervals. it,
general, equivalent embedded Markov chain (the sequence states, discarding
times transitions). former achieved matrix exponential latter
achieved setting diagonal elements Q zero normalizing row
sum one.
Rather, discrete-time process associated stochastic matrix related
continuous-time process associated intensity matrix Q following way.
Consider procedure (1) sampling event times Poisson process rate
(that is, times consecutive events independently identically distributed
exponential distribution rate ), (2) sampling state transitions
event times discrete-time process described , (3) discarding
self transitions. procedure produces samples distribution original
CTMP described Q.
transformation useful simulation (sampling), also understanding
computing matrix exponential. intensity matrix Q negative semi-definite,
Taylor expansion matrix exponential unstable, sign terms
alternates. However, fix using instead Q. reworking Equation 21,
note Q = (M I). write
eQt = e(M I)t

(23)



=e

= et

e

X
k tk

(24)

Mk

(25)

k tk
et
Mk
k!
{z }
k=0 |

(26)

k=0

=

[eA+B = eA eB AB = BA]

k!


X

k

734

fiTutorial Structured Continuous-Time Markov Processes

s0

s1

t0

t1

s2
t2

s3

s4
t4

t3

t5

Figure 5: Pictorial representation finite-length sample CTMP.
k probability exactly k events Poisson process rate
time t. series stable (M positive semi-definite) finite number
terms, sum quasi-probability vector (it non-negative sums less
1). missing probability bound error. sequence k grows
decays. Therefore, discarding tail series, also early terms speed
computations. Fox Glynn (1988) give method compute left right bounds
k ensure desired error tolerance.
Note that, Q represents ergodic continuous-time Markov process, represents ergodic discrete-time Markov process strictly greater maxi qi (a
sufficient, necessary condition). ergodic, stationary distribution
Q stationary distribution .
2.4 Likelihood
complete finite-length sample (trajectory) Tr CTMP sequence states
times transitions, plus ending time: Tr = {(s0 , t0 ), (s1 , t1 ), . . . , (sn1 , tn1 )}, tn .
Figure 5 shows pictorial representation, n = 5. use convention
process starts time 0, t0 must equal 0.
likelihood sample product conditional probabilities event
(the starting state, dwell duration, state transition):


pr last duration
density duration pr trans
init dist
}|
{
z
z
}|
{
}|
{
z
n2
z
}|
{Y

q

,s

i+1
qsn1 (tn tn1 )
qsi (ti+1 ti )


p(Tr) = Pr(X(t0 ) = s0 )
e
(27)
qs e
qsi
i=0

= P0 (s0 )

n1


qsi (ti+1 ti )

e

i=0

ln p(Tr) = ln P0 (s0 )

n2


qsi ,si+1

(28)

i=0
n1
X

qsi (ti+1 ti ) +

i=0

n2
X

ln qsi ,si+1

(29)

i=0

let P0 distribution starting state process. Note time tn
process transition. Rather, observe process remains state sn1
duration least tn tn1 .
Equation 29 rewritten
X
X
ln p(Tr) = ln P0 (s0 )
[s]qs +
N [s, s0 ] ln qs,s0
(30)
s6=s0



[s] total time spent state N [s, s0 ] total number transitions
s0 , functions Tr. demonstrates CTMP
735

fiShelton & Ciardo

member exponential family sufficient statistics [] N [, ] (plus
relevant sufficient statistics starting distribution), natural parameters
diagonal elements intensity matrix logarithm non-diagonal
elements. likelihood multiple trajectories form, [s] N [s, s0 ]
sums sufficient statistics individual trajectories.
2.4.1 Parameter Estimation
maximum likelihood
P parameters easily derived differentiating Equation 30,
replacing qs s0 6=s qs,s0 :
ln p(Tr)
N [s, s0 ]
=
[s]
qs,s0
qs,s0

s0 6=

(31)

s0 6=

(32)



(33)

implies ML parameters
qs,s0 = N [s, s0 ]/T [s]
X
qs =
N [s, s0 ]/T [s]
s0 6=s

maximum posteriori (MAP) estimate calculated place suitable prior distributions parameters. particular, put independent gamma distribution
prior independent parameters, qs,s0 , 6= s0 :


p(qs,s0 ; s,s0 , s,s0 ) =

s,ss,s
0

0 +1

(s,s0 + 1)



0

qs,ss,s0 eqs,s0 s,s0

(34)

parameters s,s0 s,s0 . posterior distribution parameters given
data summarized sufficient statistics [s] N [s, s0 ] also gamma-distributed
parameters s,s0 + N [s, s0 ] s,s0 + [s]. Thus, MAP estimates parameters
qs =

X N [s, s0 ] + s,s0
s0

qs,s0 =

[s] + s,s0

N [s, s0 ] + s,s0
.
[s] + s,s0

(35)
(36)

2.5 Inference
consider two classic problems reasoning temporal systems: filtering
smoothing. Initially, assume observations (evidence) pattern like
Figure 6: sequence times {t0 , t1 , . . . , tk } sequence evidences {e1 , e2 , . . . , ek }.
assume know prior marginal distribution X(t0 ), either previous
reasoning t0 = 0.
Filtering task computing p(X(t) | e1 , e2 , . . . , ek ) tk . evidence
point observation state system, Markovian property process
makes inference trivial. Instead assume ei probabilistically related
X(ti ) independent everything else given X(ti ). (This analogous discrete-time
736

fiTutorial Structured Continuous-Time Markov Processes

t0

e1

e2

e3

t1

t2

t3

Figure 6: Example evidence pattern, point evidence.
hidden Markov model.) Thus view observation noisy measurement
system.
hidden Markov model, define recursive filtering solution using forward
message whose components defined
(t) = Pr X(t) = i, e[t0 ,t)



(37)

denote e[s,t) = {(ti , ei ) | ti < t}: set evidence interval [s, t).
analogy also define e[s,t] e(s,t] evidence [s, t] (s, t] respectively
(which need later). Note row vector probabilities, one state
system. Recursive calculation derived
X


Pr X(t) = j, e[t0 ,t) =
Pr X(s) = i, e[t0 ,s) Pr X(t) = j, e[s,t) | X(s) = t0 <


(38)
t0 <
(39)

(t) = (s)F (s, t)

second equation vector version first equation, matrix F (s, t)
element i, j equal Pr X(t) = j, e[s,t) | X(s) = .
evidence [s, t), F (s, t) = eQ(ts) . Thus, propagate distribution
one evidence time point next matrix exponential. propagate across
evidence times, define

+ (t) = Pr X(t), e[t0 ,t]
(40)
(t), including evidence t. evidence t, two
vectors same. evidence t, + (t) (t), except
element multiplied probability evidence time point.
let (i) diagonal matrix diagonal element j Pr(ei | X(ti ) = j),
recurrence written
(t0 ) = + (t0 ) = given

(41)

+

(ti ) = (ti1 )e

Q(ti ti1 )

+ (ti ) = (ti )O (i)
+

(t) = (ti )e

Q(tti )

0<ik

(42)

0<ik

(43)

ti < ti+1 ti < t, = k

(44)

Equation 42 special case Equation 44. propagates one evidence
time next. Equation 43 propagates across evidence point.
737

fiShelton & Ciardo

Equation 44 used construct filtered estimate non-evidence time
normalizing (t) sum 1 (dividing probability evidence prior t).
Finally, note similar set recurrences derived F (, ). result allows
propagation distribution across time intervals includes evidence;
is, restricted particular initial condition, (t0 ). However, single
propagated, computing F first computationally expensive.
2.5.1 Complex Evidence
filtering equations inhomogeneous hidden Markov model (that is,
transition matrix constant) familiar employed hidden Markov
models. However, continuous time, evidence patterns
direct corresponding analogies discrete-time. evidence consists finite number
observations, convert similar form, breaking time intervals
constant evidence.
instance, might observe system subset states duration
time: time interval, system leave subset,
observe whether transitions within subset. augment evidence
include information. interval [ti1 , ti ), let Si denote subset states
evidence constrains system. constraints, Si full
state space. time points change Si , point evidence, (i)
identity matrix (inducing change filtering estimate). Si (i) may
non-trivial i.
propagate ti1 ti , must use modified intensity matrix. particular,
set zero rate inconsistent evidence Si : rates to, from,
within set states Si . Let Q(i) denote matrix. rows
columns permuted states Si upper left corner,
matrix form
Q

(i)



QSi
=
0


0
0

(45)

QSi submatrix Q rows columns corresponding Si . Additionally,
modify (i1) , setting 0 diagonal elements corresponding states Si .
Note Q(i) (strictly) intensity matrix: rows sum 0.
general, diagonal element greater (in absolute value) sum row
elements set zero non-diagonal rates. missing rate corresponds
probability leaving evidence set (and therefore conforming evidence).
eQ(ti ti1 ) stochastic matrix representing conditional distribution time ti
(i)
given state time ti1 , eQ (ti ti1 ) substochastic matrix (the row sums less
equal 1), sum row probability evidence
interval, given state time ti1 .
738

fiTutorial Structured Continuous-Time Markov Processes

new filtering recurrence
(t0 ) = given

(46)
(i)

(ti ) = + (ti1 )eQ
+

(ti ) = (ti )O

(ti ti1 )

(i)

(t) = + (ti )eQ

(i+1)

(tti )

0<ik

(47)

0<ik

(48)

ti < ti+1 ti < t, = k .

(49)

might also observe transition exact time point. generally, time ti
might observe transition occurred one state set Ui one state set
Ui+ (without knowing exactly states within sets). case, elements (t)
probabilities duration lasting least t, + (t) probability density duration lasting exactly t. difference probability
tail exponential density point multiplication
relevant rate q. Thus, type evidence, modify (i) . particular,
(
qj,k j Ui , k Ui+ , j 6= k
(i)
j,k =
.
(50)
0
otherwise
recurrence remains same, new definition (i) . evidence types
also possible derived types augmenting state space.
2.5.2 Smoothing

Smoothing problem calculating Pr X(t) | e[t0 ,tk ] t0 tk . common
Markov processes, note



Pr X(t) | e[t0 ,tk ] Pr X(t) | e[t0 ,t) Pr e[t,tk ] | X(t)
(51)
constant proportionality found noting sum Equation 51
value X(t) must equal 1. first term right calculated
() recurrence above. second term calculate backward message recurrence.
Define

(t) = Pr e[t,tk ] | X(t) =
(52)

+
(t) = Pr e(t,tk ] | X(t) =
(53)
let column vector, backward recurrence analogous forward one,
right multiplication instead left multiplication:
+ (tk ) = 1
(ti ) =

(i)

+

(ti )

(i)

(ti ti1 )

(i)

(ti t)

+ (ti1 ) = eQ

(t) = eQ

(ti )

(ti )

vector 1s

(54)

0<ik

(55)

0<ik

(56)

ti1 < ti .

(57)

time t, vector distribution state system given
evidence
p(X(t) | e[t0 ,tk ] ) (t) fi (t)
(58)
fi Hadamard (point-wise) product.
739

fiShelton & Ciardo

2.6 Parameter Estimation Incomplete Evidence
Section 2.4.1 demonstrated CTMP member exponential family sufficient statistics [i] (the amount time spent state i) N [i, j] (the number
transitions j). evidence trajectories fully observed continuous
interval time, sufficient statistics trivially tallied. Further,
evidence trajectory observed = 0, sufficient statistics initial distribution
also directly observed.
However, portions interval hidden, generally observations
form previous section, direct likelihood maximization feasible.
two basic approaches maximum likelihood estimation case: gradient ascent
expectation maximization (EM).
gradient ascent, replace sufficient statistics Equation 31
expected values standard argument exponential models applies: Let Tr partially
observed trajectory let h stand potential completion it.
ln p(Tr) = ln

X

eln p(Tr,h)

(59)

h

ln p(Tr)
1 X
ln p(Tr, h)
=
p(Tr, h)
qi,j
p(Tr)
qi,j
h


N [i, j]
= Eh|Tr
[i]
qi,j


N [i, j]
=
[i]
qi,j

(60)
(61)
(62)

N [i, j] [i] expected values N [i, j] [i] respect completions
Tr. EM, similarly replace N [i, j] [i] Equation 32 N [i, j] [i].
therefore left problem computing expected values N [i, j] [i].
Full derivations shown work Nodelman et al. (2003). quick version
[i]
Z

tk

[i] =
t0

=

p(X(t) = | e[t0 ,tk ] ) dt

1
p(Tr)

Z

(63)

tk

(t)i (t) dt .

(64)

t0

expected value N [i, j] similar form:
qi,j
N [i, j] =
p(Tr)

Z

tk

(t)j (t) dt +
t0

X (tl )O (l) i,j j+ (tl )
P
(l)
i0 ,j 0 i0 ,j 0
lTrans

(65)

Trans set evidence indices time transition (perhaps partially)
observed: first term handles unobserved transitions second handles (partially)
observed transitions.
740

fiTutorial Structured Continuous-Time Markov Processes

let i,j matrix zeros, except single one location (i, j),
integrals Equation 64 Equation 65 form
Z

tk

(t)j (t) dt =
t0

=

k Z
X

tl

l=1 tl1
k Z tl
X
l=1

(t)j (t) dt
(i)

+ (tl1 )eQ

(66)
(ttl1 )

(i)

i,j eQ

(tl t)

(tl ) dt .

(67)

tl1

Thus, forward backward pass calculate + (i) (i) evidence
change point i, integral relatively simple. solved standard quadrature
methods solution differential equation (Asmussen, Nerman, & Olsson, 1996).
Alternatively, calculation usually results values various time
points, interpolated full functions used directly solve integrals.

3. Kronecker Algebra Representations
number states thousand, methods computationally feasible modern computer. However, models described terms
assignments variables. Thus number states grows exponentially number
variables. tens variables, must seek compact representations.
remainder paper, consider state space process X
assignment L variables, {X1 , X2 , . . . , XL }. Q
let variable Xi ni possible
assignments. Thus, total state space size n = L
i=1 nl . let bold x stand
state (joint assignment L variables), component xi assignment
variable state x. state space often referred factored structured
variable-based.
Kronecker products sums natural basic operations build compact representations process intensities. cases, compact representations
naturally describe transition rates, naturally describe diagonal elements Q (the negative rates leaving state). Thus, define R
Q, except zeros diagonal position. diagonals reconstructed
non-diagonal elements row, information content same.
3.1 Kronecker Product
first basic operation Kronecker product. Given matrices A(1) , A(2) , . . . , A(K)
A(k) general size mk -by-nk , Kronecker product written
A=

K


A(k)

(68)

k=1

Q
Q
(the result) m-by-n matrix: = k mk n = k nk . elements
represent possible multiplications one element A(1) , A(2) , . . . , A(K) . Let
Mk = {1, 2, . . . , mk } Nk = {1, 2, . . . , nk }, valid indices matrix A(k) .
741

fiShelton & Ciardo

"
Given =

a00 a01



#

a10 a11


a00 b00

a00 b10



a00 b20
Ba01 B
B = 00
=
a10 Ba11 B
a10 b00

b
10 10
a10 b20

b00 b01 b02





B = b10 b11 b12
b20 b21 b22
a00 b01 a00 b02 a01 b00 a01 b01 a01 b02




a00 b11 a00 b12 a01 b10 a01 b11 a01 b12


a00 b21 a00 b22 a01 b20 a01 b21 a01 b22


a10 b01 a10 b02 a11 b00 a11 b01 a11 b02

a10 b11 a10 b12 a11 b10 a11 b11 a11 b12

a10 b21 a10 b22 a11 b20 a11 b21 a11 b22

Figure 7: Example Kronecker product
Then, let Ir mapping M1 M2 MK {1, 2, . . . , m} let Ic similarly
defined mapping N1 N2 NK {1, 2, . . . , n}. matter usually
mappings are, convention take lexicographic
orderings (or
P


mixed-base
numbering
index).

instance

(i
,

,
.
.
.
,

)
=
r
1
2
K
k
1:k1
1kK
Q
ma:b = akb mk .
AIr (i1 ,i2 ,...,iK ),Ic (j1 ,j2 ,...,jK ) =

K


A(k) ik ,jk .

(69)

k=1

notation makes appear complex, concept simple. Figure 7 demonstrates
simple example. terms sparsity (one measure structure), Kronecker product
number non-zero elements equal product number non-zero elements
input matrix.
Kronecker product analogous factor product (in Bayesian network terminology)
treat operand matrix factor two different variables (and matrices
share variables), result matrix factor half variables
flattened column dimension half flattened row
dimension.
terms distributions, Kronecker product represents independence. Given two
variables X1 X2 marginal distributions represented vectors v 1 v 2 ,
v 1 v 2 joint distribution X1 X2 . particular, independent
joint distribution marginals v 1 v 2 .
terms rate matrix, Kronecker product represents synchronization (Plateau,
1985). two variables, X1 X2 rate2 matrices R1 R2 , R1 R2
rate matrix state space X = X1 X2 (joint assignments X1 X2 ).
represents rate matrix changes state X1 must occur time
state X2 (both variables changed every transition).
2. hold generally intensity matrices, Kronecker product anything
sensible diagonal elements.

742

fiTutorial Structured Continuous-Time Markov Processes

B = I3 + I2 B =


a0,0
a0,1
b0,0 b0,1 b0,2

b1,0 b1,1 b1,2


0,0
0,1



b2,0 b2,1 b2,2


0,1
0,0

+
a1,0

a1,1
b0,0 b0,1 b0,2




a1,0
a1,1
b1,0 b1,1 b1,2
a1,0
a1,1
b2,0 b2,1 b2,2













=




a0,0+b0,0
b0,1
b0,2
a0,1
a0,1
b1,0
a0,0+b1,1
b1,2
b2,0
b2,1
a0,0+b2,2
a0,1
a1,0
a1,1+b0,0
b0,1
b0,2
a1,0
b1,0
a1,1+b1,1
b1,2
b2,0
b2,1
a1,1+b2,2
a1,0










Figure 8: Example Kronecker sum, given matrices B Figure 7. Zeros
omitted. Note non-zero off-diagonal entries correspond one
two indices (into B) changing.

3.2 Kronecker Sum
Kronecker operation Kronecker sum. defined square matrices.
Given square matrices A(1) , A(2) , . . . , A(K) A(k) size nk -by-nk , Kronecker
sum defined terms Kronecker product:
A=

K

k=1



(k)

=

K
X

n1 n2 . . . nk1 A(k) nk+1 nK

(70)

k=1

n identity matrix size n-by-n. Kronecker sum size
Kronecker product matrices use indexing function
reference elements sum, need one matrix square, thus
Ir = Ic = I:
PK
(k)

k=1 ik ,ik il = jl l
AI(i1 ,i2 ,...,iK ),I(j1 ,j2 ,...,jK ) = A(k) ik ,jk
(71)
il = jl l except l = k


0
otherwise.
Figure 8 demonstrates simple example.
terms CTMP, Kronecker sum represents asynchronicity. Given two variables,
X1 X2 intensity3 matrices Q1 Q2 , Q1 Q2 intensity matrix joint
state space processs events proceed irrespective others state.
is, processes independent (assuming starting distributions independent).
3. holds rate matrices, stronger Kronecker product make
statement intensity matrices too.

743

fiShelton & Ciardo

Note intensity transition involves two variables zero (at
instant, maximum one variable change).
3.3 Properties
Kronecker product obeys classic distributive property:
(A + B) C = C + B C
mixed product property provides relationship Kronecker product
matrix product. Given matrices A, B, C, assuming AC BD valid
matrix products,
(A B)(C D) = (AC) (BD) .
(72)
One consequence Kronecker product expressed
K


A(k) =

k=1

=

K

k=1
K


n1 n2 nk1 A(k) nk+1 nK

(73)

n1:k1 A(k) nk+1:K

(74)

k=1

na:b product terms na nb defined above. shows
bit relationship Kronecker products sums: Compare Equation 70
Equation 73.
reworked
K

i=k

(k)



=

K


P n1:k ,nk+1:K > (I nk A(k) ) P n1:k ,nk+1:K

(75)

i=k

nk = n1:K /nk P a,b matrix describing a,b-perfect shuffle permutation
(0, ..., ab 1): entry position (i, j) 1 j = (i mod a) b + bi/ac, 0 otherwise (in
particular, P a,b = P a,b = ab b 1). Whereas Equation 74 orders Kronecker
products outer products terms elements Ak correct places,
Equation 75 repeats Ak diagonal permutes rows columns place
elements correct locations. similar transformation used rewrite
Equation 70 sum shuffled block-diagonal matrices. permutations
often done implicitly code, versions useful deriving algorithms.
3.4 Compact Kronecker Representations
Given factored state space before, joint rate matrix R expressed sum
Kronecker products:
E
L
X
(l)
R=

(76)
e=1 l=1
(l)


L variables
rate matrix space variable l only.
particular, exponentially sized (in number variables) representation straightforward: e ranges elements resulting matrix. element corresponding
744

fiTutorial Structured Continuous-Time Markov Processes

(l)

(x1 , x2 , . . . , xL ), (x01 , x02 , . . . , x0L ), = xl ,x0l 1 < l L l = 1, except
multiplied scalar value placed location. way term
sum matrix single non-zero element. However, many processes
expect E manageable number. instance, variables independent,
E = L (and L L2 rate components identity matrices), per Kronecker
sum above.
view E terms sum separate events whose identities
marginalized produce resulting process (see Section 2.1.3). events
must couple variables synchronously (due Kronecker product). exploit type
decomposition extensively next sections.

4. Decision Diagram Representations
encoding Equation 76 efficient, better exploiting
internal structure. R viewed mapping two discrete domains (the row index
column index) real value. Decision diagrams long used computer
science compactly encode functions discrete domains. show
used CTMPs seen alternative Kronecker algebra
encodings, case MTBDDs used PRISM (Kwiatkowska, Norman, & Parker,
2011), even extension Kronecker algebra encodings, case Matrix
Diagrams used Mobius (Deavours, Clark, Courtney, Daly, Derisavi, Doyle, Sanders, &
Webster, 2002) EVMDDs used SMART (Ciardo, Jones, Miner, & Siminiceanu,
2006).
4.1 Decision Diagram Overview
Decision diagrams encode functions form f : X X0 where, before, domain
state space X structured: X = X1 XL . words, f applied (state)
tuple evaluates element range set X0 . One think f encoding
vector indexed X entries values X0 . course, idea
employed encode matrices, simply need use domain X X. (In practice,
actually use interleaved domain X10 X1 XL0 XL , unprimed state
variables refer row indices, states, primed state variable refer
column indices, states, usually leads compact decision diagrams.)
Binary decision diagrams, BDDs (Bryant, 1986), encode functions sets
forming domain X binary, multiway decision diagrams, MDDs (Kam, Villa,
Brayton, & Sangiovanni-Vincentelli, 1998), allow non-binary domain sets. However,
range X0 binary. numeric application, need extend representations
allow range X0 either integers Z (possibly augmented value
indicate undefined) reals R (possibly, again, augmented , restricted
nonnegative reals R0 ). range Z used primarily encode indexing functions
non-consecutive sets states. range R used encode rates themselves.
Informally, decision diagrams directed acyclic graphs organized layers
layer corresponding different variable domain function. outgoing
edges node correspond values variable layer take on.
745

fiShelton & Ciardo

value function determined following path root corresponding
values taken domain variables. path ends terminal node which, BDDs
MDDs, give value function.
first proposal encode non-binary function extend BDDs MDDs that,
instead terminals 0 1, element X0 terminal node. resulting multi-terminal (Clarke, Fujita, McGeer, Yang, & Zhao, 1993) BDDs (or MTMDDs)
quite general. However, see, MTMDDs sometimes unable compactly
encode even simple functions. therefore focus newer class edge-valued decision diagrams, exponentially compact, provably never larger,
MTMDDs (Roux & Siminiceanu, 2010). edge-valued decision diagrams, value associated edge tree, functions value determined values
along path terminal node. exact definition diagrams depends
operator used combine edge values. consider two cases, EV+MDDs (Ciardo &
Siminiceanu, 2002) (where X0 either Z {} R {} edge values along path
summed) EVMDDs (Wan, Ciardo, & Miner, 2011) (where X0 R0 edge
values along path multiplied).
4.2 Multiterminal Edge-Valued Decision Diagrams
Formally, EV+MDD EVMDD acyclic directed edge-labeled edgevalued graphs. node graph p level p.lvl set directed edges indexed
x. edge associated label x written p[x] = hp[x].val,p[x].chi, p[x].val
value associated edge p[x].ch target edge.
terminal node (one without outgoing edges) , level 0: .lvl = 0.
nonterminal node p level k {1, . . . , L}: p.lvl = k. xk Xk ,
outgoing edge labeled xk , associated value v X0 , pointing node
q q.lvl < k. Thus p[xk ] = hv,qi.
node p level k encodes function fp : X1 Xk X0 . EV+MDDs,
fp defined recursively fp = 0 p = , fp (x1 , . . . , xk ) = p[xk ].val +
fp[xk ].ch (x1 , . . . , xp[xk ].ch.lvl ) otherwise (that is, p nonterminal node).
EVMDDs, fp defined recursively fp = 1 p = , fp (x1 , . . . , xk ) =
p[xk ].val fp[xk ].ch (x1 , . . . , xp[xk ].ch.lvl ) otherwise.
decision diagram definitions additional restrictions ensure canonicity,
representable function unique representation. edge-valued
decision diagrams defined, achieved additionally requiring
following.
duplicate nodes: p.lvl = q.lvl = k and, xk Xk ,
p[xk ] = q[xk ], p = q.
absorbing value terminates path: EV+MDDs, p[xk ].val = implies
p[xk ].ch = ; EVMDDs, p[xk ].val = 0 implies p[xk ].ch = .
node p level k > 0 normalized : EV+MDDs, min{p[xk ].val : xk Xk } =
0; EVMDDs, max{p[xk ].val : xk Xk } = 1.
746

fiTutorial Structured Continuous-Time Markov Processes

MDD X

EV+MDD X

MTMDD X

EV+MDD

MTMDD

MDD

0
x3

x2

x1

0

1

0

0

1

0

1

1

0

0

0

1

1

1

1

0

2

0

1

3

0

4

1

5

1

0

6

1

7

0

1

0

4

0

1

0

2

0

1

0

1

0
0

x3

0

x2

x1

0

1

1



0

1

0

0

0

1

1

1

0

0

1

0



1

1

0

0

1

0

1

1

0

2

1

1

0

3

1

4

0

1

0

2

0

1

0

1

0

1

0

2

0 1
0

0

1

0

1

0 1
0



Figure 9: Encoding lexicographic index function, X , set X. left panel shows
quasi-reduced MDD encoding X followed MTMDD EV+MDD
encoding X ; right panel shows corresponding encodings set =
{100, 110, 001, 101, 011}. either case, EV+MDD isomorphic MDD.
level tree corresponds different variable. Black boxes
values variable (and traversal diagram follows edge leading
box value input). White boxes (for EV+MDD) values
corresponding edge (which summed produce functions value).
0 top EV+MDD value added path traversal
diagram.

Furthermore, require one following two reduction forms must used.
quasi-reduced form, nodes level L incoming edges (except special
case graph consisting ) children node level k
level k 1 (except absorbing-valued edges, point , stated above).
fully-reduced form, redundant nodes, node p level k
redundant p[xk ] = p[yk ] xk , yk Xk .
Strictly speaking, EV+MDD node encodes function values 0 (included) (possibly included); thus, function f range Z {} R {}
encoded h,pi = min{f (i) : X} fp = f (the special case f
encoded pair h,i). Analogously, EVMDD node encodes function
values 0 (possibly included) 1 (included); thus function f range R0
encoded h,pi = max{f (i) : X} fp = f / (the special case f 0
encoded pair h0,i). following, use term EV+MDD EVMDD also
pair h,pi, understanding parameter scales values
function encoded node p.
4.3 Lexicographic Index Example
illustrate compactness decision diagrams using lexicographic
index, also
P
called mixed-base value, state x = (x1 , . . . , xL ), defined (x) = 1kL xk n1:k1 ,
747

fiShelton & Ciardo

na:b = na nb b (as Section 3.1). discuss importance
function showing encoding.
Figure 9 (left) shows lexicographic index function (along side MDD encoding
set states). MTMDD full L-level tree n1:L leaves. contrast,
EV+MDD contains one node level, child labeled xk
node level k points node level k 1 value xk n1:k1 .
Interestingly, function retains compact encoding even modify
applies set X, (x) = |{y : (y) < (x)}| x ,
(x) = otherwise, sense MDD encoding EV+MDD encoding
isomorphic (right panel Figure 9). particular importance exact
numerical solution structured CTMPs whose reachable state space Xrch strict (and
possibly complicated) subset X, since case need frequently efficiently
map state x = (x1 , . . . , xL ) index Xrch (x) full probability vector size |Xrch |.
Compactly representing index function key efficient calculations CTMPs.
Obviously, EVMDDs also exponentially compact MTMDDs; simply
consider EVMDD encoding e also one node per level, child
label xk node level k value exk n1:k1 .
4.4 Decision Diagram Operations
addition efficiently encoding structured functions, decision diagrams also able
efficiently manipulate functions. decision diagram operations proceed recursively
root node(s) make extensive use dynamic programming. Specifically,
use operation cache retrieve result specific operation specific choice
parameters, result previously computed exploring different paths
recursion. reduces worst-case complexity operation (for example, computing
c = + b, b functions encoded two EVMDDs) exponential (i.e.,
size domain) polynomial. example, Figure 10 shows pseudocode
algorithm perform element-wise addition two EVMDDs, , 0
a, b EVMDD nodes level L (unless = 0, case = , = 0,
case b = ), Sum(L, h,ai, h,bi) returns EVMDD h,ri that,
x = (x1 , . . . , xL ) X, fr (x) = fa (x) + fb (x); course, input
EVMDDs assumed canonical form, output EVMDD guaranteed
canonical form. complexity product sizes input
EVMDDs.
practical implementation, unique table, stores nodes avoids duplicates, implemented lossless hash table which, given lookup key hlevel, r[0], . . . , r[nk
1]i, returns nodes address, cache implemented (possibly) lossy hash table. cache made effective scaling exploiting commutativity.
example, defining arbitrary order nodes (for example, b memory address
smaller b), exchange two input EVMDDs ensure
b prior cache lookup, observe fa + fb = (fa + fb ), = /,
store entries form hSU , a, , b , ri cache. Then, assuming
p q, call Sum(k, h0.5,pi, h0.2,qi) would cached hSU , p, 0.4, q , s,
748

fiTutorial Structured Continuous-Time Markov Processes

function Sum(level k, EVMDDh,ai, EVMDDh,bi)
= b return h + ,ai . includes terminal case k = 0: = b =
= 0 return h,bi
. = definition
= 0 return h,ai
. b = definition
cache contains hSU , , a, , b , ri return h,ri . Check result
cache
r NewNode(k)
. Create new temporary result node level k
xk Xk
r[xk ] Sum(k 1,h a[xk ].val,a[xk ].chi,h b[xk ].val,b[xk ].chi) . Recurse
one level
maxxk Xk {r[xk ].val}; . Maximum edge value node r normalization
xk xk
r[xk ].val r[xk ].val/ . Normalize node r maximum edge value 1
r UniqueTableInsert(r);
. node like r exists, return delete r, else
return r
Enter hSU , , a, , b , ri cache;
. Remember result operation
cache
return h,ri;
Figure 10: Pseudo-code sum quasi-reduced EVMDDs (a b either nodes
level k). fully-reduced version similar slightly involved,
needs take account levels b.

return h0.5,si, subsequent call Sum(k, h0.25,pi, h0.1,qi) Sum(k, h0.1,qi, h0.25,pi)
would find hSU , p, 0.4, q , s, cache immediately return h0.25,si.
4.5 Encoding Transition Rate Matrices EVMDDs
turn use EVMDDs compactly encode transition rate matrix R
(the intensity matrix Q, without diagonal) CTMP.
accomplished using various approaches.
4.5.1 Monolithic Encoding vs. Disjunctive Partition Encoding
Clearly, node r 2L-level EVMDD encode arbitrary function form
X X [0, 1]. Then, 0, pair h,ri encodes arbitrary function
form X X [0, ], EVMDD levels (1, . . . , 2L) correspond state variables
(x01 , x1 , . . . , x0L , xL ), is, use interleaved order describe transition rate
x x0 . monolithic approach, store R using single EVMDD h,ri
largest rate R r encodes matrix R/.
However, many practical systems exhibit asynchronous behavior, state
change due event e E occurring (asynchronously) somewhere system.
situations, employ disjunctive partition encode R, storing set
EVMDDs {he ,re : e E}, ,re encodes matrix , (x, x0 ) describes
rate system moves state x state x0 due occurrence event e.
749

fiShelton & Ciardo

P
disjunctive partition encoding, R = eE built explicitly;
rather, individual matrices directly used numerical computations used
solve CTMP. idea disjunctive partition initially suggested BDDs (Burch,
Clarke, & Long, 1991), although obviously also related Kronecker encodings: consider
Equation 76, expresses R first foremost sum.
choice monolithic disjunctive partition encoding largely modeldependent. applications, high-level language description model suggests
set asynchronous events E be. Thus, first build EVMDDs
disjuncts then, desired, explicitly build EVMDD R summing
EVMDDs disjunct corresponding event (using algorithm Figure 10,
instance).
However, disjunct EVMDDs usually quite compact, EVMDD
R obtained summing disjuncts might still compact, might grow
large. former case, monolithic approach preferable, allows us
directly use EVMDD encoding R numerical iterations. latter case,
disjunctive partition approach preferable, allows us use EVMDD
individually, without even attempting build monolithic EVMDD encoding R.
example, consider simple system four Boolean variables, X1 , X2 , X3 ,
X4 , two events. first event, c21 , changes value (X2 , X1 ), interpreted
2-bit integer, sequence 0 [1] 1 [1/2] 2 [1/4] 3 [1/8] 0 [1] ,
numbers square brackets indicate rate corresponding transition. second
event, c43 , changes value (X4 , X3 ), interpreted 2-bit integer, sequence
0 [3] 1 [1] 2 [1/3] 3 [1/9] 0 [3] . Figure 11 left shows EVMDDs
encoding matrices R21 R43 corresponding two events, well matrix
R = R21 + R43 . (for visual simplicity, edges value 0, definition point
terminal node , shown).
4.5.2 Adopting Ideas Kronecker Encodings: Identity Patterns
Neither monolithic approach disjunctive partition approach exploit locality:
fact events (synchronously) affect state variables. words,
matrix conceptually size |X| |X|, usually much smaller
support Se {x1 , . . . , xL }. Specifically, Xk Se Xk e dependent:
local state xk affects rate e occurs (including case may disable e
altogether, set rate 0) changed occurrence e. Xk 6 Se ,
Xk e independent EVMDD encoding contains identity patterns
correspondence xk . example, EVMDD encoding R21 Figure 11 left
exhibits patterns respect variables X4 X3 , one R43 exhibits
X2 X1 .
Essentially, identity patterns simply describe fact value x0k (the
new value xk occurrence e) equals old value Xk rate
affected value Xk , true possible values Xk .
(k)
happens, Kronecker encoding event e = nk , quasi-reduced
fully-reduced forms alone cannot take advantage common patterns. exploit
patterns, combination fully-reduced form, unprimed level Xk , new
750

fiTutorial Structured Continuous-Time Markov Processes

R21

R43

1

x4
x04

x03

0

1

1

1 1/9

1

0

1

1 1/3

1

0

0 1
1/3 1

1

0

1

0

1

1

1

1

1 1/3

1

1

0

1

1

1

1

1

1 1/4

0

1

1 1/2

1

1

0

1 1/3

0

1

1

1

0

1

1

1

1

0

1

0

1

1

1 1/3

1

0

1

1 1/4

1

0

1

0

1/2 1

1

1

1 1/2

1

1

0

1

0

1

1

1

1

0

1

1

1

x1

0

1

0

1

0

1

0

1

1

1

1

1

1

1

1

1

x01

1

0

0

1

0

1

1

1

1

1

1

1





0

x03

x02

1

0

0

1 1/3

0

x2

1/2 1

3

1

1 1/9

x3

1

1

0

x04

1/9 1

R

3

x4

1/3 1

0

R43

1

1 1/3

0

0

0

1

0

x2
x02

0

1

1

x3

3

0

0

R21

R

3

1

0

1/3 1

1

0

1

1

1 1/3

1

0
1

1

0

0

1

1

0

1

1

1

1

1

1

0

0

1/3 1

1

1 1/3

1

0

1 1/4

0

1

0

1 1/2

1

1/9 1

1 1/3

0

1

1

1 1/3

1

1 1/4

1

0

1/2 1

1

1 1/2

0

x1

0

1

0

1

1

1

1

1

x01

1

0

1

0

1

1

1

1







1

1/2 1



Figure 11: example EVMDDs encoding transition rate matrices using quasireduced form (left) fully-identity-reduced form (right). Omitted edges
implied value 0 (thus resulting value 0 path containing them).
right diagrams left, except identity patterns omitted implied: completely skipped pair levels
assumed identity structure (compare corresponding diagram
left).

identity-reduced form (Ciardo & Yu, 2005) primed level Xk0 , needed. allows
us encode EVMDD nodes (unprimed primed) levels
corresponding state variables Se . advantage fully-identity-reduced
form resulting decision diagrams, unlike Kronecker encoding, also recognize
exploit partial identity patterns (those arising models Xk remains unchanged
e occurs certain states others). Figure 11 right shows encoding
matrices R21 , R43 , R, using new fully-identity-reduced form.
4.5.3 Beyond Kronecker: Disjunctive-then-Conjunctive Partition Encoding
push decomposition employing disjunctive-then-conjunctive partition approach. idea first introduced logic analysis (Ciardo & Yu, 2005)
also related Equation 76, expresses R sum products. particularly appropriate globally-asynchronous locally-synchronous systems,
state change due (asynchronous) event e E, occurrence e de751

fiShelton & Ciardo

pends (synchronously) changes state variables.
Q
(c)
decomposed product matrices, = 1cm and, again, matrix
(c)

(c)

conceptually size |X| |X| but, practice, usually small support Se .
(c)
(c)
Specifically, Xk Se fully-identity-reduced EVMDD contains
0
node associated Xk Xk . restrict case supports

(c)
(c)
conjuncts event disjoint, 1cm Se = Se Se substantially smaller Se . example, Kronecker encoding exists,
N
(k)
(k)
(k)
= 1kL , Se = {Xk } Xk Se , 6= nk ;
case, disjunctive-then-conjunctive approach uses EVMDD store
(c)
compact disjunctive partition approach uses EVMDD store
, essentially compact Kronecker approach (except
save additional memory exploiting partial identity patterns).
disjunctive-then-conjunctive approach instead distinctly efficient
Kronecker approach applicable (that is, Kronecker approach would require
enormous set events correctly describe R), nevertheless seen
extension Kronecker approach. Consider using decomposition Equation 75:


(k)
(k)
=
=
P n1:k ,nk+1:L > (I nk ) P n1:k ,nk+1:L
1kL

1kL



=

(k)

P n1:k ,nk+1:L > (I nk ) P n1:k ,nk+1:L

Xk Se
(k)

last step simply stresses that, = nk , corresponding factor
n1:L skipped.
idea behind Shuffle Algorithm (Fernandes, Plateau, & Stewart, 1998),
which, observed Buchholz, Ciardo, Donatelli, Kemper (2000), efficient,
(k)
matrices sparse. (The perfect shuffle pre- postmultiplications essentially free; simply describe different state indexing.)
Then, disjunctive-then-conjunctive approach extends Kronecker expression
allow situations factors restricted support consisting
one variable, still exploits factors locality:



=
P (c) > (c) RS (c) P (c)
(77)
Se

e

1cm

e

e

P (c) > P (c) perfect shuffle permutations respectively move dee

e

(c)

pendent state variables Se end ofQthe variable order back original
position, (c) identity matrix size X 6S (c) nh (the skipped levels), RS (c)
Se
e
e
h
Q
square matrix size X (c) nh (the conjunct encoded EVMDD ignore
h

e

(c)

skipped levels corresponding state variables Se ).
(c)
Since supports Se disjoint, generalization Kronecker approach comes
additional cost essentially corresponds Kronecker approach allow
event defined different set state variables, set corresponding
752

fiTutorial Structured Continuous-Time Markov Processes

different partition basic state variables (X1 , .., XL ). case, building
EVMDD multiplying EVMDDs RS (c) involve numeric
e

(c)

multiplication, grows size diagram spans sets Se
(1)
(2)
disjoint; example, Se = {X3 , X7 } Se = {X4 , X6 }, path X7
X3 EVMDD contain copy entire EVMDD encoding Re(2) .
4.5.4 Numerical Solutions Decision Diagrams
described method storing rate intensity matrix compactly common
process models. address use data structures CTMP computations.
literature surrounding decision diagrams CTMPs primarily concerned computation unconditional distribution resulting process, either finite time
(more commonly) limit infinite time (the stationary distribution). follow
convention literature refer solution process. Model estimation, solutions conditioned evidence, computations marginals statistics
are, knowledge, unexplored representations, return later.
matrix R stored using 2L-level EVMDDs (using monolithic, disjunctive
partition, disjunctive-then-conjunctive partition encoding), traditional numerical
solution algorithms need adjusted accordingly. First all, seeking exact
solution, neither stationary vector transient vector (t) admit compact
EVMDD representation (unless modeled system contains extensive symmetries
composed completely independent subsystems, rarely case practice).
Two approaches explored. exact solution stationary distribution
(the null-space Q), hybrid approach (Kwiatkowska, Norman, & Parker, 2004)
usually best, solution stored full vector reals size equal
number reachable states (|Xrch |, equal |X| states reachable)
rate matrix R stored EVMDDs, expected holding time vector h (the inverse
absolute values diagonal Q) stored either full vector EVMDDs.
Clearly, approach scales size tractable problems eliminating main
memory obstacle (the storage R), encounter next memory obstacle (the
storage solution vector). example, Figure 12 shows pseudocode classic
Jacobi-style stationary solution ergodic CTMP transition rate matrix
monolithically encoded EVMDD h,ri, state space Xrch indexed
EV+MDD h0,pi, previously discussed, that, X, compute Xrch (i),
index 0 |Xrch | 1 included Xrch , 6 Xrch . Function
Xrch used index entries solution vector: new old . holding time
vector stored full vector h, also indexed Xrch (but could stored
using EVMDDs instead). recursive call JacobiRecur, descend
level current rate matrix EVMDD node single level
corresponding source destination EV+MDD nodes (these needed index
full vectors reals, initially set h0,pi, encoding entire Xrch function).
Note that, simple case states reachable
(that is, X = Xrch ) indexing
P
function Xrch mixed-base value (i) = 1kL ik n1:k1 discussed Section
4.3 and, such, really require EV+MDD encoding; hand,
shown Figure 9, EV+MDD single path nodes, use carry
753

fiShelton & Ciardo

. Computes Q = 0. h,ri R, h0,pi Xrch
function JacobiIteration(EVMDDh,ri,EV+MDDh0,pi)
old initial guess
. Real vector size |Xrch |, visible JacobiRecur
num iter 0
repeat
new zero vector
. Real vector size |Xrch |, visible JacobiRecur
JacobiRecur(L,h,ri,h0,pi,h0,pi)
{0, . . . , |Xrch | 1}
new [i] new [i] h[i]
. h holding time vector
old
new
swap( , )
num iter num iter + 1
num iter > AX ER Converged( old , new )
. example, using relative absolute test
. Computes new old R
function
JacobiRecur(level
k,
EVMDDh,mi,
EV+MDDhsrc ,srci,
+
EV MDDhdes ,desi)
src = des =
new [des ] new [des ] + old [src ]
return
0 nk 1 s.t. m[i].val 6= 0 src[i].val 6=
. level k
j 0 nk 1 s.t. m[i][j].val 6= 0 des[j].val 6= . level k 0
0
src
src + src[i].val
0
des des + des[j].val
0 m[i][j].val
0 ,src[i].chi, h 0 ,des[j].chi)
JacobiRecur(k1, h 0 ,m[i][j].chi, hsrc
des
Figure 12: Jacobi-style iteration stationary solution (Q =
dt = 0) R

(non-diagonal elements Q) stored monolithic EV MDD h (inverse
absolute value diagonal elements Q) stored vector. h,ri
encoding R h0,pi encoding mapping states indices
(for h).

overhead. similar hybrid approach used compute transient solution using
uniformization-style algorithm R also stored using EVMDDs but, again,
size full vectors limits scalability.
filtering smoothing operations described Section 2.5 explicitly tackled decision-diagram encodings. However, willing represent
distribution exactly (as above), necessary vector-matrix multiplications
directly decision-diagrams (without expanding them). Estimating EVMDD representation R data completely unexplored.
tackle larger problems must instead willing accept approximate solution.
However, work area mostly restricted systems exhibiting special structures.
754

fiTutorial Structured Continuous-Time Markov Processes

One exception work Wan et al. (2011), addresses stationary solution
arbitrary ergodic CTMPs whose state space encoded MDD whose transition rate
matrix encoded one EVMDDs. approach uses L different approximate
aggregations exact CTMP solves iteratively, reaching fixpoint.
approach provides exact solution certain conditions essentially,
system so-called product-form (Baskett, Chandy, Muntz, & Palacios-Gomez, 1975).
Unfortunately, similar approximation transient solution structured CTMP
proposed far.
Thus state spaces large enough single vector values cannot
maintained, literature inference estimation models limited.
However, next section describe different model viewed restricted
form disjunctive EVMDD encoding section. model many inference
estimation method believe link two may allow methods
extended general decision-diagram representations.

5. Continuous-Time Bayesian Networks
artificial intelligence machine learning literatures, continuous-time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller, 2002) developed extension
dynamic Bayesian networks (DBNs). discuss section, limited
case disjunctive EVMDD encodings above. However, approximate methods
computations conditioned evidence extensively developed CTBNs.
CTBN consists set variables, {X1 , X2 , . . . , XL }, directed graph G oneto-one mapping nodes variables, set conditional intensity matrices
variable, initial distribution. initial distribution usually described
Bayesian network (to keep description compact), many algorithms
theory hold compact distribution representations.
graph G describes instantaneous influence variables other. edge
Xi Xj denotes rates transitions Xj depend instantaneous value
Xi . Note G may cyclic.
dependent rates captured conditional intensity matrices. Let ni
number states variable Xi . denote parents variable Xi Pari
joint assignment Pari pari . set conditional intensity matrices variable Xi
consists one ni -by-ni intensity matrix possible instantiation pari : QXi |pari
denote element xi , x0i qxi ,x0i |pari , rate Xi transitioning xi x0i
Pari values pari .
Semantically, CTBN continuous-time Markov process joint state space
constituent variables. let (x, x0 ) equal set variables whose assignments
differ joint assignments x x0 . joint intensity matrix entire process
described

qx,x0

PL

i=1 qxi ,x0i |pari
= qxi ,x0 |pari



0
755

(x, x0 ) = {}
(x, x0 ) = {Xi }
otherwise

(78)

fiShelton & Ciardo

1
2

1
2



4
=
3

4
3





5
6



7
8




QX1 |X2 =0 =

X1



QX1 |X2 =1

X2
X3

QX2 |X1 =0
QX2 |X1 =1

5
=
6

7
=
8



QX3 |X2 =0

9
= 8
7


QX3 |X2 =1

12
= 8
7

0
10
4
6
13
7


9
2
11

6
5
14

Figure 13: example CTBN graph. See Figures 14, 15, 17 CTMP
representations.

pari assignment Pari x. intensity transition two states
differ one variable read appropriate conditional intensity matrix
variable. intensity transition two states (that differ
one variable) zero. diagonal elements filled negative row
sums. process allows two variables transition arbitrarily close times,
exactly time.
CTBN retains local Markov properties standard Bayesian network. particular, variable (local process) independent non-descendants, given parents.
course, cycles, parents may also descendants, pose problem
definition. Note, however, given refers conditioning entire trajectory
variable (from starting time ending time, variables
queried observed). Conditioning current value sufficient (even rendering
current values independent).
global Markov properties also hold. Markov blanket variable union
sets parents, children, childrens parents. Note sets
significant overlap, cycles permitted. Conditioned Markov blanket,
variable independent variables.
5.1 Connections Representations
CTBN related number representations. instance, Portinale
Codetta-Raiteri (2009) link CTBNs stochastic Petri nets (Ajmone Marsan, Balbo, Conte,
Donatelli, & Franceschinis, 1995). Donatelli (1994) shows translation stochastic
Petri nets Kronecker operators Ciardo, Zhao, Jin (2012) show translation
(ordinary, timed, stochastic) Petri nets various classes decision diagrams.
However, concentrate direct comparisons approaches presented
tutorial.
Figure 13 shows simple small CTBN two binary variables (X1 X2 ) one
ternary variable (X3 ). use running example convert
CTBN compact representations.
756

fiTutorial Structured Continuous-Time Markov Processes

X10

X1

X20

X2

P (X10 |X1 , X2 ) : 0
1

0, 0
11t
1t

1, 0
2t
12t

0, 1
14t
4t

1, 1
3t
13t

P (X20 |X2 , X1 ) : 0
1

0, 0
15t
5t

1, 0
6t
16t

0, 1
17t
7t

1, 1
8t
18t

0
:
1
2

0, 0
19t
0
9t

1, 0
8t
110t
2t

X30

X3
time =

time = +

P (X30 |X3 , X2 )

2, 0
7t
4t
111t

0, 1
112t
6t
6t

1, 1
8t
113t
5t

2, 1
7t
7t
114t

Figure 14: DBN whose limit 0 approaches CTBN Figure 13.
5.1.1 Connection DBN
CTBN, construct dynamic Bayesian network (DBN) whose parameters
function time slices limit time-slice width approaches
zero original CTBN. particular, DBN intra-time-slice edges (this
two variables CTBN cannot change exactly time). Xi
parents Pari CTBN, parents (at previous time slice) plus
previous value DBN. Figure 14 shows CTBN Figure 13 DBN. Xi
CTBN intensity matrix QXi |pari parent values pari , corresponding
variable DBN conditional probability distribution
pDBN (x0i |xi , pari ) = x0i ,xi + qxi ,x0i |pari

(79)

x0i value Xi next time step (and xi pari values
previous time step), x0i ,xi 1 x0i = xi 0 otherwise, time time
slices. limit process approaches 0 original CTBN.
5.1.2 Connection Kronecker Algebra
joint intensity matrix expressed Equation 78 also written sum Kronecker products. need first define conceptually simple, notationally cumbersome, term. First, let i,j matrix 0, except single 1 location i, j (same
before). Second, let QXi |pari denote Kronecker product one matrix variable
CTBN. variable Xi , matrix QXi |pari . variable parent Xi
value xk pari , matrix xk ,xk . Otherwise, matrix identity
matrix. way Kronecker product distributes elements QXi |pari
relevant entries joint intensity matrix.
P
define joint intensity matrix. Let QXi
pari QXi |pari . Then,
P
Q = QXi . Figure 15 gives example CTBN Figure 13. Figure 16 gives
another example. Kronecker product general handle diagonal
elements, expansion works intensity matrix case, since one
matrices product non-diagonal.
757

fiShelton & Ciardo

QX1 =

X

QX1 |x2 x2 ,x2

x2

QX2 =

X

x1 ,x1 QX2 |x1

x1

QX3 =

X

x2 ,x2 QX3 |x2

x2

Q = QX1 + QX2 + QX3
Figure 15: Sum Kronecker encoding rate matrix Q CTBN Figure 13.

QW = QW
X
QX =
QX|z z,z

W

z

X



QY =

w,w QY |w

w

QZ =
Z

X
X

x,x y,y QZ|x,y

x,y

Q = QW + QX + QY + QZ

Figure 16: Sum Kronecker product encoding CTBN one parent per
node.

5.1.3 Connection Decision Diagrams
decomposition CTBN sum Kronecker products helps clarify
connection edge-valued decision diagrams previous section. CTBN
particularly structured version disjunctive EVMDD encoding Section 4.5.1 paired
identity encoding Section 4.5.2. particular, CTBN describes CTMP
also described sum EVMDDs fully-identity-reduced form. two
descriptions order space complexity. decision-diagram encoding one
EVMDD variable joint value parents.
Figure 17 shows disjunction EVMDDs CTBN Figure 13. EVMDD
encodes non-identity matrices Kronecker product expression; identity
matrices implied fully-identity-reduced form. another example, CTBN
Figure 16, could construct 9 EVMDDs: 1 W , 2 X , 4 Z.
758

fiTutorial Structured Continuous-Time Markov Processes

RX1|X2=0

RX1|X2=1

2

RX2|X1=0

4

RX2|X1=1

6

RX3|X2=0

8
0

x3

x2

0

1

1

1

0

0

8
2

0

0



0

1

1 5/8

2



0

1

0

1

1

1

1



1

2



1



0

1

1
1

1

1

1

1

1 4/7

1

1

0

0

1

0

1

2

0

1

1

0

1 1/4

2

6/8 1 7/8

1

1

1



0

2
1

1

7/8 1

3/4 1

1

1/2 1

1

5/6 1

x02

x01

1

1 8/9 7/9

x03

x1

RX3|X2=1

9

0

1

1

1

Figure 17: set identity-reduced EVMDDs whose sum CTBN
Figure 13.

Note disjunction EVMDDs compactly encode structure within variables
local rate matrix, CTBN cannot. way, represent generalization
exploit context-sensitive independence.
Whether merging EVMDDs given variable merging EVMDDs multiple
variables result reduction increase representation size largely empirical
question. However, would generally expect increase size one transition
allowed time, paths levels must remember whether previous
variable changed and, so, one (for example, Figure 11).
5.2 Sampling
Sampling CTBN done straight-forward application sampling method
described Section 2.1. need construct full intensity matrix. Instead,
joint assignment x, find diagonal element summing diagonals
relevant conditional intensity matrices. gives us rate exponential sample
time next variable change. read intensities variables
potential transitions relevant row conditional intensity matrix select
variable new state variable proportion intensity. process
takes O(L) time transition (where L number variables).
better exploiting racing memoryless properties exponential
distributions (discussed Section 2.1). select variable transitions next,
race exponential distributions variable rates corresponding diagonal
759

fiShelton & Ciardo

function SampleCTBN(CTBN, initial distribution 0 , end time )
Let Tr empty trajectory
Let (x1 , x2 , . . . , xL ) joint sample 0
. per algorithm 0
representation
1 L
Add (Xi = xi @ 0) Tr
Let 0
Let E empty event priority queue time-variable pairs.
repeat
variables Xi event E
Sample exponential rate qxi |pari
Add ht + t,Xi E
Let ht,Xi earliest event E . Update get new variable change
<
Sample x0i multinomial proportional qxi ,x0i |pari
Let xi x0i
. Update local copy variable assignments
Add (Xi = xi @ t) Tr
Remove Xi children Xi E
. times must resampled

Figure 18: Algorithm sample CTBN

elements conditional intensity matrices. note variable
chosen, treat transition time two separate random draws: draw stating
transition time chosen time draw stating chosen
transition time next transition (because memoryless property exponential
distribution). means chosen transition affect rate variable
question, need resample transition time. using priority queue
transitions times (not durations), reduce running time per transition O(D ln L)
maximal out-degree graph. method made explicit Figure 18.
5.3 Inference
Inference CTBN process calculating expected value full trajectory,
given partial trajectory. basic case infer conditional probability
single variable single time point (the expectation indicator function) given partial
trajectory. many ways trajectory may partial. obvious
variable-based model like CTBN variables observed particular
times intervals. variable observation times intervals. Thus,
variable, assume evidence like Section 2.5.1: time
points variable known values time intervals
variable known values (which might include observations transitions).
Unfortunately, even evidence, problem NP-hard. particular,
deciding whether marginal probability single value single variable single
760

fiTutorial Structured Continuous-Time Markov Processes

time point greater positive threshold NP-hard. generally
accepted, although never formally demonstrated. provide proof Appendix.
Thus, known algorithms CTBN inference exponential (in number
variables) running time. simplest method treat CTBN general CTMP
single intensity matrix Q. apply forward backward passes Section 2.5.
intensity matrix stored compact form, resulting vectors require
space instantiation every variable CTBN (exponential space). need
keep values states consistent evidence. Thus, times
variables unobserved, inference tractable. But, periods
many variables unobserved, require approximate inference methods (overviewed
Section 5.5).
calculating probability variable time, common case
inference calculate expected sufficient statistics. shown Section 5.4.1,
means calculating N [xi , x0i |pari ] [xi |pari ] values i, xi , x0i , pari .
former expected number times variable Xi transitioned xi x0i
parents state pari , latter expected amount time variable Xi
state xi parents state pari .
proof marginal calculation easily adapted show deciding whether
quantities non-zero also NP-hard. Therefore, known method
treat system general CTMP single large Q matrix.
apply Equation 65 Equation 64 find expected number transitions expected
amount time joint assignments. let J(xi , pari ) set joint assignments
variables consistent Xi = xi Pari = pari , find expected
sufficient statistics CTBN
[xi |pari ] =

X

[x]

(80)

xJ(xi ,pari )

N [xi , x0i |pari ] =

X

X

N [x, x0 ]

(81)

xJ(xi ,pari ) x0 J(x0i ,pari )

5.4 Parameter Graph Estimation
initial distribution CTBN estimated separately using standard method
estimation Bayesian network (or whatever compact representation desired).
requires data value trajectorys value (or trajectories values)
time 0.
concentrate estimation rate parameters dynamics graph structure (G). exposition assume single trajectory, Tr. However, multiple
trajectories used summing sufficient statistics.
5.4.1 Parameter Estimation
set CTBNs fixed graph structure subset exponential family
CTMPs parameters fixed 0 many remaining ones tied
(share value). Thus, log-likelihood Equation 30 applies
761

fiShelton & Ciardo

too, sufficient statistics tied parameters summed:


X
X
[xi |pari ]qx |par +
ln pCTBN (Tr) = ln P0 (Tr(0)) +
N [xi , x0i |pari ] ln qxi ,x0i |pari


x0i 6=xi

i,pari ,xi

X

= ln P0 (Tr(0)) +



(82)


[xi |pari ]qxi ,x0i |pari + N [xi , x0i |pari ] ln qxi ,x0i |pari

i,pari ,xi ,x0i 6=xi

(83)
= ln P0 (Tr(0)) +

X

lXi (Tr)

(84)



ranges variables, pari ranges joint assignments parents i,
xi x0i range differing assignments Xi . [xi |pari ] denotes amount time
Xi = xi Pari = pari . Similarly, N [xi , x0i |pari ] denotes number transitions
Xi xi x0i Pari = pari . new sufficient statistics sums sufficient
statistics flat CTMP, summing assignments CTBN variables
Xi Pari remain (see Equations 80 81). Given complete trajectory,
construct directly without employing (exponentially large) sums. last
line definition lXi , local log-likelihood variable Xi . Note
function trajectories Xi parents (not Tr).
Maximizing Equation 83 straight-forward extension maximizing Equation 30:
qxi ,x0i |pari = N [xi , x0i |pari ]/T [xi |pari ] .

(85)

produce Bayesian posterior distributions parameters take independent
conjugate prior distributions qxi ,x0i |pari parameter (Nodelman et al., 2003).
flat CTMP, conjugate prior gamma distribution hyper-parameters
xi ,x0i |pari xi ,x0i |pari parameter qxi ,x0i |pari . resulting posterior also gamma
distribution corresponding hyper-parameters xi ,x0i |pari +N [xi , x0i |pari ] xi ,x0i |pari +
[xi |pari ]. Thus MAP parameter estimates
qxi ,x0i |pari =

N [xi , x0i |pari ] + xi ,x0i |pari
[xi |pari ] + xi ,x0i |pari

.

(86)

5.4.2 Structure Estimation
Estimating CTBN structure could accomplished statistical tests independence processes. Yet, unaware methods use suitable
independence tests.
Instead, CTBN structures estimated graph scoring functions. score
function decomposes likelihood (Equation 83) sum terms, one per
variable, selection variables parents affects term
variable, search maximal scoring graph simple. variables parents
chosen independently maximizing corresponding term sum.
exponentially large (in total number variables) number parent sets
762

fiTutorial Structured Continuous-Time Markov Processes

consider variable, limit cardinality parent sets D,
variables parents chosen exhaustive search total running time
O(L2D ), linear number variables, L.
contrast Bayesian networks similar strategy lead
efficient algorithm (unless variable ordering known priori). Learning CTBNs
structure efficient restrictions graph: CTBNs graph may
cyclic. similar situation arises dynamic Bayesian networks (DBNs).
allow inter-time-slice edges (those previous time point current time
point), graph structure may searched efficiently, like CTBNs. However,
allow intra-time-slice edges (those within current time point) DBN, must
enforce acyclicity constraints search longer efficient.
Bayesian information criterion (Lam & Bacchus, 1994) made score:
!
X
ln |Tr|
scoreBIC (G : Tr) =
lXi (Tr)
Dim(G)
(87)
2

fi
X
ln |Tr| fifi
fi
(88)
=
lXi (Tr)
fiQXi |pari fi
2


Dim(G)
fi finumber independent parameters network defined
fi
fi
graph G fiQXi |pari fi number independent parameters conditional intensity
matrices associated Xi . second term equal ni (ni 1) (because diagonal
elements independent) times number parent instantiations. data size,
|Tr|, number transitions trajectory Tr (or total data set consists
multiple trajectories). score consistent (Nodelman et al., 2003) term
lXi (Tr) grows linearly amount data represents likelihood second
term grows logarithmically amount data penalizes excess parameters.
Bayesian score also constructed placing prior graphs (as well
parameters) finding maximum ln P (G | Tr) = ln p(Tr | G)+ln P (G)ln p(Tr).
last term isnt
P affected choice G, drop it. assume structure modularity:
ln P (G) = ln P (Pari ). remaining data term, ln P (Tr | G), (logarithm the)
integral likelihood multiplied prior, possible parameter values. Using
independent gamma priors above, decomposes separate term
variable (dropping ln P0 (Tr(0)) term affect choice G):

+1
0
Z
h
X
(xi ,x0i |pari ) xi ,xi |pari
ln p(Tr | G) =
ln
exp (T [xi |pari ] + xi ,x0i |pari )qxi ,x0i |pari
(xi ,x0i |pari + 1)
0
i,pari ,xi 6=x0i

+(N [xi , x0i |pari ] + xi ,x0i |pari ) ln qxi ,x0i |pari dqxi ,x0i |pari
(89)
x

=

X

X



pari ,xi 6=x0i

+1
0
,xi |pari

ln

(xi ,x0i |pari )
(xi ,x0i |pari + 1)

!

(N [xi , x0i |pari ] + xi ,x0i |pari + 1)
N [xi ,x0i |pari ]+x

(T [xi |pari ] + xi ,x0i |pari )

+1
0
,xi |pari

(90)
=

X

lscoreB (pari : Tr)

(91)



763

fiShelton & Ciardo

last line definition lscoreB . derivation almost
one given Nodelman et al. (2003). difference prior consists gamma
distribution independent variable whereas prior consists gamma distribution diagonal rate parameter Dirichlet prior ratios qxi ,x0i |pari /qxi |pari .
two equivalent, parameterized differently.
Bayesian score therefore
X
scoreB (G : Tr) =
lscoreB (pari : Tr) + ln P (Pari ) .
(92)


converges BIC score limit infinite data (Nodelman et al., 2003)
therefore also consistent.
5.4.3 Incomplete Data
case trajectory Tr incomplete, back situation
Section 2.6. likelihood takes form CTBN general CTMP,
solutions maximizing likelihood incomplete trajectory form.
Namely, compute expected sufficient statistics (using inference), apply
gradient ascent expectation-maximization find maximum likelihood parameters.
gradient
!
p(Tr)
N [xi , x0i |pari ]
= p(Tr)
[xi |pari ]
(93)
qxi ,x0i |pari
qxi ,x0i |pari
expectation-maximization update equation Equation 85 except
sufficient statistics replaced expected values, given partially observed trajectory current model.
graph estimation, apply structural expectation-maximization (Friedman,
1997) (SEM) CTBNs (Nodelman, Shelton, & Koller, 2005). SEM Bayesian
networks little complex due structure search step, CTBNs, simpler
structure search step need enforce acyclicity constraints therefore
carried simply (see above). tricky point (which also holds standard
Bayesian networks) graph search scoring function must calculated using
expected sufficient statistics therefore, given current model, inference algorithm
must produce expected sufficient statistics current models parent sets,
also parent sets considered structure search. using exact
inference (by flattening CTBN general CTMP), available. However,
approximate methods (below) differ simple extract expected sufficient
statistics. inference performed, joint optimization parameters structure
performed, new model used find new expected sufficient statistics, process
repeats.
5.5 Approximate Inference
mentioned Section 5.3, exact inference intractable many concurrently
missing variables. Therefore, many approximate inference methods developed.
briefly cover section, would refer full papers complete
descriptions.
764

fiTutorial Structured Continuous-Time Markov Processes

5.5.1 Sampling-Based Inference
Sampling obvious method producing approximate inference. number
advantages. First, produces set full trajectories inference question
answered. Second, sampling methods converge correct value allowed run
long enough. Third, sampling methods usually easily parallelized, lending
multiple processors multiple cores.
Hobolth Stone (2009) description number methods
unstructured case full evidence beginning end trajectory,
evidence between. discuss work CTBNs general evidence
patterns.
Fan Shelton (2008) Fan, Xu, Shelton (2010) developed importance
sampler particle filter smoother. Forward sampling (like Figure 18)
turned importance sampler taking observed data given sampling
missing portions, marching time along. weight sample probability
sampled observed data (which sampled) given trajectory
data. problem arises variable goes observed observed.
case, sampling must agree coming observation evidence. Adding
transition exactly evidence starts correct (as almost surely
event time). samplers handle forward look ahead sample
necessary transition advance, suitable importance weight corrections.
extended particle filter smoother resampled based number
transitions, rather absolute time. method extended general
temporal models Pfeffer (2009).
El-Hay, Friedman, Kupferman (2008) developed Gibbs sampler CTBN models.
start simply developed trajectory agrees evidence. Then,
algorithm removes single variables full trajectory resamples (keeping time
periods value known). Conditioned full trajectories variables
Markov blanket (the union variables children, parents, childrens parents),
trajectory variable independent variables, sampler needs
consider variables Markov blanket. posterior distributions times
transitions longer exponential distributions. forms complex thus
Gibbs sampler must sample performing binary search.
Fan Shelton (2009) combined ideas Gibbs sampler earlier
work importance sampling produce Metropolis-Hastings sampler. importance
sampling method used instead Gibbs sampling importance weight used
find acceptance probability. faster generate samples, samples take longer
converge. balance trade-off depends typicality evidence
inference query.
Rao Teh (2011, 2013) used uniformization develop auxiliary Gibbs sampler
faster previous Gibbs sampler. auxiliary variables times
self-transitions uniformization sampler (Section 2.3). Thus resample
variable, algorithm samples auxiliary times, given old trajectory (which
done quickly). throws away transitions, keeps full set times
(old times new times). Then, using forward-backward two-pass algorithm, state
765

fiShelton & Ciardo

transitions sampled uniformized discrete-time process (conditioned
evidence). Finally, self transitions discarded. Rao Teh (2012) extended
time-varying uniformization rate speed convergence, explicitly case
unstructured process.
5.5.2 Non-Sampling Methods
number non-sampling methods also proposed. advantages
include determinism (often helpful used inside EM keep estimates consistent),
fewer parameters need set well (number samples, length burn-in,
others).
Cohn, El-Hay, Kupferman, Friedman (2009) Cohn, El-Hay, Friedman,
Kupferman (2010) derived mean-field approximation. approximate distribution
independent time-inhomogeneous Markov process variable. is, variables
independent (in approximation), intensities depend time. natural
parameterization also differs slightly. Instead transition rates, transition densities
used, idea same. resulting algorithm changes one variables distribution
time, depending Markov blanket. update involves solving system
differential equations (to get time-varying parameters inhomogeneous Markov
process). solved adaptive integration means less computation
required intervals less rapid change. result time-varying parameters
represented series time-value points (those produced adaptive integration)
linear interpolation points.
Nodelman, Koller, Shelton (2005) derived expectation-propagation method.
propagation uses piece-wise constant time-homogeneous Markov processes, piece
corresponds period constant evidence. piece-wise constant approximations
propagated instead true marginals (as marginals would intractably large).
Saria, Nodelman, Koller (2007) extended method subdivide pieces
approximations adaptively. El-Hay, Cohn, Friedman, Kupferman (2010)
produced belief propagation algorithm spirit mean-field approximation
above, employing free energy functional CTMPs. Instead propagating piece-wise
time-homogeneous Markov processes, propagate single time-inhomogeneous Markov
process use adaptive integration representation mean-field.
result adaptive mathematically cleaner.
Finally, filtering (but general inference), Celikkaya, Shelton, Lam (2011)
developed factored version uniformized Taylor expansion approximate matrix
exponential calculations. result something similar Boyen Koller (1998)
dynamic Bayesian networks, also involving truncation infinite sum
mixture propagation distributions. method current non-sampling method
accuracy bounds, although loose.
5.6 Extensions
shown above, CTBN converted sum decision diagrams. way
decision diagrams (and similarly convertible models) viewed extensions
766

fiTutorial Structured Continuous-Time Markov Processes

CTBNs. Many non-Markovian processes Section 1.1 could, restricted right
way, also viewed. However, direct extensions CTBNs.
First, El-Hay et al. (2010) introduced continuous-time Markov networks (CTMNs).
undirected graphical models Markov processes way CTBNs
directed graphical models. model subclass reversible processes, ones
detailed balance holds: exists distribution (the stationary distribution
process) (x)qx,x0 = (x0 )qx0 ,x pairs states x x0 . CTMN
converted CTBN replacing undirected edge pair directed
edges. parameterization directly reveals stationary distribution process
Markov network.
Second, Portinale Codetta-Raiteri (2009) Codetta-Raiteri Portinale (2010)
showed extension CTBNs allow simultaneous transition multiple variables.
based Petri nets encodes cascades transitions happen simultaneously.
Finally, Weiss, Natarajan, Page (2012) presented method constructing
local rate matrices variable matrix, multiplication regression
trees. akin exploiting context-specific independence (Shimony, 1991) standard
Bayesian network use trees (Boutilier, Friedman, Goldszmidt, & Koller, 1996).
multiplication trees reduction CTBN sum EVMDDs
(see Figure 17). particular, trees require tests made particular
variable order, use trees instead DAGS, multiply trees together (instead
adding them). Weiss et al. (2012) also give boosting-style algorithm learning
parameterization. similar method known learning sum EVMDDs.

6. Applications Current Directions
provide context theory algorithms above, describe methods used applications. discuss believe
promising pressing research directions.
6.1 Decision-Diagram-Based Models
Structured CTMPs arise many applications areas, performance reliability evaluation computer systems investigation biological systems. underlying
CTMPs describing dynamics analyzed usually large, software tools
used studies rely compact symbolic techniques encode them.
particular, PRISM (Kwiatkowska et al., 2011) uses hybrid form MTBDDs,
Mobius (Deavours et al., 2002) uses Matrix Diagrams (a data structure almost equivalent
EVMDDs), SMART (Ciardo et al., 2006) uses EVMDDs encode transition rate
matrix CTMP. tools compute stationary transient exact numerical
solutions compactly encoded CTMPs. (Indeed, compute much complex
stochastic temporal logic properties expressed CSL (Baier,
Haverkort, Hermanns, & Katoen, 2000), these, too, ultimately require sequence
stationary transient numerical solutions.) exact solutions place large
computational demands due exponential explosion state space, situation
often somewhat mitigated fact that, applications targeted tools,
actual state space small subset full cross-product state variable values.
767

fiShelton & Ciardo

example application, briefly summarize study done using PRISM
analysis complex biological pathway called FGF (Fibroblast Growth Factor) (Heath,
Kwiatkowska, Norman, Parker, & Tymchyshyn, 2006). state system consists
number proteins (e.g., A, B) protein complexes (e.g., A:B) present current
time. events system consist various reactions complexation (e.g.,
+ B : B) decomplexation (its reverse, A:B + B), well degradation
(e.g., ). Finally, event occurrence rate, course depend
number proteins currently present types involved particular reaction.
actual model FGF pathway, even substantial simplifications focus
key well-known aspects real cells, contains 87 different proteins protein complexes
(each corresponding local state variable) 50 different reactions (if count
complexation decomplexation separately). stress reaction concerns
proteins compounds; thus decision diagram representation isolation quite
compact.
Even smallest meaningful model zero one protein compound
type would potential state space size 287 . However, almost always
case type models, tiny fraction states reachable, thus model
used work Heath et al. (2006) merely 801,616 states 560,000 state-tostate transitions. study focused several key questions fraction
time particular proteins bound, probability particular degradation
occurred within given time bound, quantities obtainable numerical stationary
transient analysis underlying CTMP. Notwithstanding relatively small size
state space (which could likely scaled factor 1000, around 108 states,
given modern workstation sufficient memory) results predictions obtained
model using PRISM shown agree biological data, demonstrating
viability technique perform silico genetics much less costly alternative
vitro experiments traditionally performed biology.
6.2 Continuous-Time Bayesian networks
CTBNs employed number real-world datasets problems including life
event history data (Nodelman et al., 2003), user activity modeling (Nodelman & Horvitz,
2003), computer system failure modeling (Herbrich, Graepel, & Murphy, 2007), mobile
robotics (Ng, Pfeffer, & Dearden, 2005), network intrusion detection (Xu & Shelton, 2008,
2010), phylogentic trees (Cohn et al., 2009), social networks (Fan & Shelton, 2009), cardiovascular health model (Weiss et al., 2012), heart failure (Gatti, Luciani, & Stella,
2012). Many also innovated extending CTBN framework. instance, Ng
et al. (2005) allowed continuous-state variables whose dynamics dictated differential equations. form evidence limited, particle filter developed
situation. Cohn et al. (2009) applied CTBN model time-tree allow
branching (as first done Felsenstein, 1981 general CTMPs). Weiss et al. (2012) added
context-specific independence.
give idea application CTBNs, briefly review intrusion detection
work Xu Shelton (2008, 2010). work, goal build model normal
768

fiTutorial Structured Continuous-Time Markov Processes

G

H

Pin

Cinc

Pout

Cdec
N

Figure 19: CTBN model network traffic (Xu & Shelton, 2010). N number
destination ports.

computer system events, specific particular machine. model could used
detect abnormal events time windows method intrusion detection.
Two models built: one modeling network traffic machine, one
modeling system calls processes. network model Figure 19. one
global hidden variable G four states. traffic divided different destination
ports (for instance, 80 HTTP traffic 995 POP traffic). frequent
eight ports separated traffic remaining destinations grouped
together. N = 9 groups model (the plate Figure 19).
submodel one hidden variable H four completely observed binary variables, Pin ,
Pout , Cinc , Cdec , representing packets sent received connections started
stopped respectively. observed variables toggle state represent event
relevant type, state intrinsic meaning. Therefore, matrices
single independent parameter: rate transition either state other.
way, observed variables really conditional Poisson processes.
hidden variable H structured exploit domain knowledge. 8 states
grouped pairs, one pair children. pair one children
non-zero rate. Thus, H encodes type event happen substate
meta-state.
entire model 4 89 approximately 500 million hidden states (as observed
variables observed times, distribution G Hs need
tracked). Yet, submodel 8 hidden states, exact inference submodel
reasonable. Thus, adapted particle filter smoother Fan Shelton
(2008) distributional particles, producing Rao-Blackwellized particle filter G
sampled models (which independent, given full trajectory G)
reasoned exactly.
inference method allows learning specific models host using EM.
models run data computer virus worm traffic injected
(very slowly make blend background traffic). model asked likelihood 50-second window traffic (given previous traffic). likelihood thresh769

fiShelton & Ciardo

olded produce alarm (if likelihood dropped low). results out-performed
SVM-spectrum kernels, nearest-neighbor (using features computer network literature task), methods task similar tasks.
process system calls, model similar, single hidden variable coordinating behavior set observed system-call variables. dataset model
trained time stamps system call. However, due clock resolution, many
time stamps same. Yet, temporal order preserved (although exact
durations events not). paper demonstrates method use data,
without assuming event durations, employing ordering. case, results
better SVM-spectrum kernel nearest-neighbor stide
frequency thresholding (Warrender, Forrest, & Pearlmutter, 1999).
6.3 Relative Comparison
Neither two applications could currently tackled modeling
language. biological pathway example Section 6.1, transition system involved one variable (increasing number protein complexes, decreasing
number individual proteins). CTBN cannot represent simultaneous transitions
multiple variables. pathways cannot reformulated terms composite variables
prevent simultaneous transitions without placing state single variable.
simpler example, consider system three variables, x, y, z. single event
performs three variable updates time: {x0 x + y; 0 + 1; z 0 z 1}
rate r(x, y, z). assume variable natural number range [0, . . . , n]
update would move variable outside range disabled. Figure 20
demonstrates EVMDD encode transition. Neither Kronecker encoding
CTBN encode event without merging variables.
Likewise, network traffic example Section 6.2 cannot handled current
decision-diagram-based models. depends hidden unseen variables estimation
transient solutions conditioned data. critically, relies estimation model
parameters data, developed decision-diagram models.
6.4 Current Research Directions
range open modeling, algorithmic, theoretic problems. First, questions steady-state distributions efficient exact solutions addressed
CTBNs (as EVMDDs). Similarly, questions structure parameter estimation approximate inference addressed EVMDDs (as
CTBNs).
Optimal decision making formulated general continuous-time Markov decision processes (Puterman, 1994). Yet, extending general mathematical framework
structured variable-based models largely unexplored (Kan & Shelton, 2008).
CTBNs extended handle continuous-valued variables measurements
limited fashion (Ng et al., 2005), otherwise unexplored. many applications critical. underlying system discrete measurements
continuous, techniques like Section 2.5 work. But, systems continuous state
require stochastic differential equations (ksendal, 2003), least form. work
770

fiTutorial Structured Continuous-Time Markov Processes

x

z
x0
y0
z0

0
0
0




1
0
0




2
0
0




0
1
0




1
1
0




2
1
0




0
2
0




1
2
0




2
2
0




0
0
1
0
1
0

1
0
1
1
1
0

2
0
1
2
1
0

0
1
1
1
2
0

1
1
1
2
2
0

2
1
1




0
2
1




1
2
1




2
2
1




0
0
2
0
1
1

1
0
2
1
1
1

x0



y0

z

z0

0
1
2
1
2
1

1
1
2
2
2
1

2
1
2




0
2
2




1
2
2




2
2
2








x

2
0
2
2
1
1

0

1

2

0

1

2













0

1

1

2

2

0

1

1

2

2









1









1

0

1

0

1

0

0

1

1

1

1

1

1

1

1

1

2

1

2

1

1

2

1

1

1

1

1

1

1

1

2

1

2

1

2

1

2

1

2

1

2

























0

1

0

1

1

1

1

1





Figure 20: example EVMDD encoding simultaneous transitions multiple variables. Top: 10 possible transitions resulting states (dash indicates
disabled) state (x, y, z) state (x0 , 0 , z 0 ). Bottom, left: worst case
arbitrary set 10 rates r(x, y, z). Bottom, right: best case
r(x, y, z) = r1 (x, y)r2 (z). dot indicates positive value (used encode
particular rates). block dots, one must equal 1 others
must 1.

Sarkka (2006) describes filtering smoothing models. Yet, parameter estimation much difficult systems continuous discrete state variables
systematically addressed.
Finally, new approximate inference methods always interest (as probabilistic model). Recent methods auxiliary Gibbs sampler (Rao & Teh, 2013)
belief propagation (El-Hay et al., 2010) demonstrate exploiting properties
continuous time lead great benefits. hope research explores
methods.
771

fiShelton & Ciardo

7. Conclusions
Compared discrete-time models, CTMPs better suited domains data
real-valued time stamps (the time events regular well-approximated
single clock step rate). Thus, selecting value time-slice-width (t)
discrete-time model, either time width large resulting multiple events per
time window (obscuring temporal information), small resulting unnecessary
computational burdens (propagating across many time windows). Further, optimal
middle ground large small differ depending data size,
application, model component.
presented two different CTMP modeling languages. Edge-valued decision diagrams (of Section 4) general: allow multiple variables change simultaneously. contrast, CTBNs directly encode independence assumptions (see Section 5).
Either EVMDD CTBN might compact given situation, although
CTBN compactly rerepresented sum EVMDDs (see Section 5.1.3).
models forms histories given rise differences available algorithms,
distinctions greater. literature exact solution methods richer
decision diagram models. Furthermore, literature focused computing
models steady state. Approximate methods (especially transients) model estimation
notably absent (from artificial intelligence point-of-view). literature CTBNs
focused model estimation approximate inference conditioned evidence.
CTBN literature paid attention issues reachability (when much joint
state space reachable) optimization exact inference methods.
processes natural synchronization clock (such modeling daily high low
temperatures), discrete-time model best fit. processes without natural
time-slice-width recommend continuous-time model. questions interest
steady states system exact solution necessary, EVMDD probably
best choice. model must built data approximate inference (especially
conditioned data) necessary, CTBN probably best choice.
However, shown two models share much common. Thus, hope
efficient exact algorithms EVMDDs applied CTBNs approximate inference model estimation methods CTBNs applied EVMDDs.
so, choice model would depend upon model properties
existing suite algorithms. particular, CTBN makes assumption variable distinct. contrast, disjunctive EVMDD encoding decomposes system
local events. variable-level independencies easily read CTBN graph,
disallow simultaneous transition multiple variables. application domain
guide whether variable-level explicit independences simultaneous transitions
important.
Regardless model used, believe time continuous quantity best modeled
such. introduction matrix exponential would first seem complicate
matters (compared discrete time), believe makes true coupling variables
obvious opens mathematical algorithmic possibilities efficient
precise solutions.
772

fiTutorial Structured Continuous-Time Markov Processes

Acknowledgements
Shelton supported DARPA award FA8750-12-2-0010 Laura P. Leland
K. Whittier Virtual PICU Childrens Hospital Los Angeles (awards UCR-12101024
8220-SGNZZ0777-00). Ciardo supported NSF grant CCF-1442586.

Appendix A. NP-hardness CTBN Inference
theorem proof NP-hardness CTBN inference straight-forward extensions similar proof Bayesian networks (Koller & Friedman, 2009). literature
widely accepted true, proof formally presented. Thus,
straight-forward, present completeness.
Definition 1. CTBN-Inf following decision problem. Given CTBN specified
directed graph G nodes {X1 , X2 , . . . , XL }, set conditional intensity matrices
Q = {QXi |pari }, initial distribution node independent
marginals {Xi }; variable Xj ; value Xj , xj ; time > 0, decide whether
PG,Q, (Xj (t) = xj ) > 0.
Theorem 1. CTBN-Inf NP-hard.
Proof. proof polynomial time reduction 3-SAT, following lines
similar proof Bayesian networks.
Given 3-SAT problem variables z1 , z2 , . . . , zm clauses c1 , c2 , . . . , ck
zA(i,j) jth variable (j {1, 2, 3}) clause (i {1, 2, . . . , k}) sign sA(i,j)
{+, }, construct CTBN + 2k 1 binary variables (taking values F ):
Y1 , Y2 , . . . , Ym , C1 , C2 , . . . , Ck , B1 , B2 , . . . , Bk2 , S.
Variable Yi parents, uniform initial distribution Yi , intensity matrix
QYi | 0.
Variable Ci three parents: YA(i,1) , YA(i,2) , YA(i,3) . none truth value
parents (yA(i,1) , yA(i,2) , yA(i,3) ) match formulas signs (sA(i,1) , sA(i,2) , sA(i,3) ),
conditional intensity matrix 0. parent assignments
(in least one


1 1
variable matches), conditional intensity matrices
. initial distribution
0 0


1 0 .
Variable B1 parents C1 C2 . Variable Bi (for 1 < < k 1) parents Bi1
Ci+1 . Variable parents Bk2 Ck .
variables, conditional

1 1
intensity matrix two parents values
. Otherwise, conditional
0 0


intensity matrix 0. variables initial distribution 1 0 .
reduction polynomial size (all numeric values small polynomial number variables, maximum 3 parents) obviously output
polynomial time. construction, Yi selects time 0 truth value zi never
changes. Cj eventually change clause satisfied
selected truth values. Bj eventually change clauses 1
j + 1 . similarly eventually change clauses
satisfied.
773

fiShelton & Ciardo

Markov nature process, time > 0, PG,Q, (S(t) = ) > 0
formula satisfiable probability 0 formula satisfiable.
demonstrates determining whether marginal non-zero NP-hard. similar construction Bayesian networks (Koller & Friedman, 2009), extended
show absolute relative error formulations inference also NP-hard.

References
Ajmone Marsan, M., Balbo, G., Conte, G., Donatelli, S., & Franceschinis, G. (1995). Modelling Generalized Stochastic Petri Nets. John Wiley & Sons.
Asmussen, S., Nerman, O., & Olsson, M. (1996). Fitting phase-type distributions via
EM algorithm. Scandavian Journal Statistics, 23, 419441.
Baier, C., Haverkort, B. R., Hermanns, H., & Katoen, J.-P. (2000). Model checking
continuous-time Markov chains transient analysis. Proceedings Computer
Aided Verification, pp. 358372.
Baskett, F., Chandy, K. M., Muntz, R. R., & Palacios-Gomez, F. (1975). Open, closed,
mixed networks queues different classes customers. Journal ACM,
22 (2), 335381.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence Bayesian networks. Proceedings Twelfth International Conference
Uncertainty Artificial Intelligence, pp. 115123.
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.
Proceedings Fourteenth Annual Conference Uncertainty Artificial Intelligence, pp. 3342.
Bryant, R. E. (1986). Graph-based algorithms boolean function manipulation. IEEE
Transactions Computers, 35 (8), 677691.
Buchholz, P., Ciardo, G., Donatelli, S., & Kemper, P. (2000). Complexity memoryefficient Kronecker operations applications solution Markov models.
INFORMS Journal Computing, 12 (3), 203222.
Burch, J. R., Clarke, E. M., & Long, D. E. (1991). Symbolic model checking partitioned
transition relations. International Conference Large Scale Integration, pp.
4958. IFIP Transactions, North-Holland.
Celikkaya, E. B., Shelton, C. R., & Lam, W. (2011). Factored filtering continuoustime systems. Proceedings Twenty-Seventh International Conference
Uncertainty Artificial Intelligence.
Ciardo, G., Jones, R. L., Miner, A. S., & Siminiceanu, R. (2006). Logical stochastic
modeling SMART. Performance Evaluation, 63, 578608.
Ciardo, G., & Siminiceanu, R. (2002). Using edge-valued decision diagrams symbolic
generation shortest paths. Proceedings Formal Methods Computer-Aided
Design (FMCAD), LNCS 2517, pp. 256273. Springer.
774

fiTutorial Structured Continuous-Time Markov Processes

Ciardo, G., & Yu, A. J. (2005). Saturation-based symbolic reachability analysis using
conjunctive disjunctive partitioning. Proceedings Correct Hardware Design
Verification Methods (CHARME), LNCS 3725, pp. 146161. Springer.
Ciardo, G., Zhao, Y., & Jin, X. (2012). Ten years saturation: Petri net perspective.
Transactions Petri Nets Models Concurrency, V, 5195.
Clarke, E., Fujita, M., McGeer, P. C., Yang, J. C.-Y., & Zhao, X. (1993). Multi-terminal
binary decision diagrams: efficient data structure matrix representation.
IWLS 93 International Workshop Logic Synthesis.
Codetta-Raiteri, D., & Portinale, L. (2010). Generalized continuous time Bayesian networks
GSPN semantics. European Workshop Probabilistic Graphical Models,
pp. 105112.
Cohn, I., El-Hay, T., Friedman, N., & Kupferman, R. (2010). Mean field variational approximation continuous-time Bayesian networks. Journal Machine Learning
Research, 11 (Oct), 27452783.
Cohn, I., El-Hay, T., Kupferman, R., & Friedman, N. (2009). Mean field variational approximation continuous-time Bayesian networks. Proceedings Twenty-Fifth
International Conference Uncertainty Artificial Intelligence.
Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.
Computational Intelligence, 5 (3), 142150.
Deavours, D. D., Clark, G., Courtney, T., Daly, D., Derisavi, S., Doyle, J. M., Sanders,
W. H., & Webster, P. G. (2002). mobius framework implementation.
IEEE Transactions Software Engineering, 28 (10), 956969.
Didelez, V. (2008). Graphical models marked point processes based local independence. Journal Royal Statitical Society: Series B, 70 (1), 245264.
Donatelli, S. (1994). Superposed generalized stochastic Petri nets: definition efficient
solution. Proceedings International Conference Application Theory
Petri Nets (ICATPN), LNCS 815, pp. 258277. Springer.
El-Hay, T., Cohn, I., Friedman, N., & Kupferman, R. (2010). Continuous-time belief propagation. Proceedings 27th International Conference Machine Learning,
pp. 343350, Haifa, Israel.
El-Hay, T., Friedman, N., & Kupferman, R. (2008). Gibbs sampling factorized continuoustime Markov processes. Proceedings Twenty-Fourth Conference Uncertainty Artificial Intelligence, pp. 169178.
Fan, Y., & Shelton, C. R. (2008). Sampling approximate inference continuous time
Bayesian networks. Proceedings Tenth International Symposium Artificial
Intelligence Mathematics.
Fan, Y., & Shelton, C. R. (2009). Learning continuous-time social network dynamics.
Proceedings Twenty-Fifth International Conference Uncertainty Artificial
Intelligence.
Fan, Y., Xu, J., & Shelton, C. R. (2010). Importance sampling continuous time Bayesian
networks. Journal Machine Learning Research, 11 (Aug), 21152140.
775

fiShelton & Ciardo

Felsenstein, J. (1981). Evolutionary trees DNA sequences: maximum likelihood
approach. Journal Molecular Evolution, 17, 368376.
Fernandes, P., Plateau, B., & Stewart, W. J. (1998). Efficient descriptor-vector multiplication stochastic automata networks. Journal ACM, 45 (3), 381414.
Fox, B. L., & Glynn, P. W. (1988). Computing poisson probabilities. Communications
ACM, 31 (4), 440445.
Friedman, N. (1997). Learning belief networks presence missing values hidden
variables. Proceedings Fourteenth International Conference Machine
Learning, pp. 125133.
Gatti, E., Luciani, D., & Stella, F. (2012). continuous time Bayesian network model
cardiogenic heart failure. Flexible Services Manufacturing Journal, 24 (4),
496515.
Grassmann, W. K. (1977). Transient solutions Markovian queueing systems. Computers
& Operations Research, 4 (1), 4753.
Gunawardana, A., Meek, C., & Xu, P. (2012). model temporal dependencies event
streams. Advances Neural Information Processing Systems, Vol. 24.
Heath, J., Kwiatkowska, M., Norman, G., Parker, D., & Tymchyshyn, O. (2006). Probabilistic model checking complex biological pathways. Priami, C. (Ed.), Proceedings
Computational Methods Systems Biology (CMSB), Vol. 4210 Lecture Notes
Bioinformatics, pp. 3247. Springer Verlag.
Herbrich, R., Graepel, T., & Murphy, B. (2007). Structure failure. Proceedings
2nd USENIX workshop Tackling computer systems problems machine
learning techniques, pp. 16. USENIX Association.
Hobolth, A., & Stone, E. A. (2009). Simulation endpoint-conditioned continuous-time
Markov chains finite state space, applications molecular evolution.
Annals Applied Statistics, 3 (3), 12041231.
Kam, T., Villa, T., Brayton, R. K., & Sangiovanni-Vincentelli, A. (1998). Multi-valued
decision diagrams: theory applications. Multiple-Valued Logic, 4 (12), 962.
Kan, K. F., & Shelton, C. R. (2008). Solving structured continuous-time Markov decision processes. Proceedings Tenth International Symposium Artificial
Intelligence Mathematics.
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles Techniques. MIT Press.
Kwiatkowska, M., Norman, G., & Parker, D. (2011). PRISM 4.0: Verification probabilistic real-time systems. Gopalakrishnan, G., & Qadeer, S. (Eds.), Proceedings
Computer Aided Verification, Vol. 6806 LNCS, pp. 585591. Springer.
Kwiatkowska, M. Z., Norman, G., & Parker, D. (2004). Probabilistic symbolic model checking PRISM: hybrid approach. Software Tools Technology Transfer, 6 (2),
128142.
Lam, W., & Bacchus, F. (1994). Learning Bayesian belief networks: approach based
MDL principle. Computational Intelligence, 10, 269293.
776

fiTutorial Structured Continuous-Time Markov Processes

Moler, C., & Loan, C. V. (2003). Nineteen dubious ways compute exponential
matrix, twenty-five years later. SIAM Review, 45 (1), 349.
Najfeld, I., & Havel, T. F. (1994). Derivatives matrix exponential computation. Tech. rep. TR-33-94, Center Research Computing Technology, Harvard
University.
Najfeld, I., & Havel, T. F. (1995). Derivatives matrix exponential computation. Advances Applied Mathematics, 16, 321375.
Ng, B., Pfeffer, A., & Dearden, R. (2005). Continuous time particle filtering. Proceedings
Nineteenth International Joint Conference Artificial Intelligence, pp. 1360
1365.
Nodelman, U., & Horvitz, E. (2003). Continuous time Bayesian networks inferring
users presence activities extensions modeling evaluation. Tech. rep.
MSR-TR-2003-97, Microsoft Research.
Nodelman, U., Koller, D., & Shelton, C. R. (2005). Expectation propagation continuous
time Bayesian networks. Proceedings Twenty-First International Conference
Uncertainty Artificial Intelligence, pp. 431440.
Nodelman, U., Shelton, C. R., & Koller, D. (2002). Continuous time Bayesian networks.
Proceedings Eighteenth International Conference Uncertainty Artificial
Intelligence, pp. 378387.
Nodelman, U., Shelton, C. R., & Koller, D. (2003). Learning continuous time Bayesian
networks. Proceedings Nineteenth International Conference Uncertainty
Artificial Intelligence, pp. 451458.
Nodelman, U., Shelton, C. R., & Koller, D. (2005). Expectation maximization complex
duration distributions continuous time Bayesian networks. Proceedings
Twenty-First International Conference Uncertainty Artificial Intelligence, pp.
421430.
ksendal, B. (2003). Stochastic Differential Equations: Introduction Applications
(Sixth edition). Springer-Verlag.
Parikh, A. P., Gunamwardana, A., & Meek, C. (2012). Cojoint modeling temporal
dependencies event streams. UAI Bayesian Modelling Applications Workshop.
Pfeffer, A. (2009). CTPPL: continuous time probabilistic programming language.
Proceedings 21st International Joint Conference Artifical Intelligence, pp.
19431950.
Plateau, B. (1985). stochastic structure parallelism synchronisation models
distributed algorithms. Proceedings ACM SIGMETRICS, pp. 147153.
Portinale, L., & Codetta-Raiteri, D. (2009). Generalizing continuous time Bayesian networks immediate nodes. Proceedings Workshop Graph Structure
Knowledge Represetnation Reasoning, pp. 1217.
Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). Numerical
Recipes C (Second edition). Cambridge University Press.
777

fiShelton & Ciardo

Puterman, M. L. (1994). Markov Decision Processes. Wiley-Interscience.
Rajaram, S., Graepel, T., & Herbrich, R. (2005). Poisson networks: model structured
point processes. Proceedings AI STATS 2005 Workshop.
Rao, V., & Teh, Y. W. (2011). Fast MCMC sampling Markov jump processes continuous time Bayesian networks. Proceedings Twenty-Seventh International
Conference Uncertainty Artificial Intelligence.
Rao, V., & Teh, Y. W. (2012). MCMC continuous-time discrete-state systems.
Advances Neural Information Processing Systems 25, pp. 710718.
Rao, V., & Teh, Y. W. (2013). Fact MCMC sampling Markov jump processes
extensions. Journal Machine Learning Research, 1, 126.
Roux, P., & Siminiceanu, R. (2010). Model Checking Edge-valued Decision Diagrams. Proceedings Second NASA Formal Methods Symposium (NFM 2010),
NASA/CP-2010-216215, pp. 222226. NASA.
Saria, S., Nodelman, U., & Koller, D. (2007). Reasoning right time granularity.
Proceedings Twenty-third Conference Uncertainty AI, pp. 421430.
Sarkka, S. (2006). Recursive Bayesian Inference Stochastic Differential Equations. Ph.D.
thesis, Helsinki University Technology.
Shimony, S. E. (1991). Explanation, irrelevance statistical independence. Proceedings
Ninth National Conference Artificial Intelligence, pp. 482487.
Wan, M., Ciardo, G., & Miner, A. S. (2011). Approximate steady-state analysis large
Markov models based structure decision diagram encoding. Performance Evaluation, 68, 463486.
Warrender, C., Forrest, S., & Pearlmutter, B. (1999). Detecting intrusions using system
calls: Alternative data models. IEEE Symposium Security Privacy, IEEE
Computer Society.
Weiss, J. C., Natarajan, S., & Page, D. (2012). Multiplicative forests continuous-time
processes. Advanced Neural Information Processing Systems.
Weiss, J. C., & Page, D. (2013). Forest-based point processes event prediction electronic health records. Proceedings European Conference Machine Learning Principals Practice Knowledge Discovery Databases (ECMLPKDD).
Williams, C. K. I. (1998). Prediction Gaussian processes: linear regression
linear prediction beyond. Jordan, M. I. (Ed.), Learning Graphical Models,
pp. 599621.
Xu, J., & Shelton, C. R. (2008). Continuous time Bayesian networks host level network
intrusion detection. European Conference Machine Learning, pp. 613627.
Xu, J., & Shelton, C. R. (2010). Intrusion detection using continuous time Bayesian networks. Journal Artificial Intelligence Research, 39, 745774.

778

fiJournal Artificial Intelligence Research 51 (2014) 645-705

Submitted 05/14; published 12/14

Complexity Answering Conjunctive Navigational
Queries OWL 2 EL Knowledge Bases
Giorgio Stefanoni
Boris Motik

giorgio.stefanoni@cs.ox.ac.uk
boris.motik@cs.ox.ac.uk

Department Computer Science, University Oxford
Parks Road, Oxford OX1 3QD, United Kingdom

Markus Krotzsch
Sebastian Rudolph

markus.kroetzsch@tu-dresden.de
sebastian.rudolph@tu-dresden.de

Faculty Computer Science, TU Dresden
Nothnitzer Strae 46, 01062 Dresden, Germany

Abstract
OWL 2 EL popular ontology language supports role inclusionsaxioms
form S1 Sn v capture compositional properties roles. Role inclusions closely
correspond context-free grammars, used show answering conjunctive
queries (CQs) OWL 2 EL knowledge bases unrestricted role inclusions undecidable. However, OWL 2 EL inherits OWL 2 DL syntactic regularity restriction
role inclusions, ensures role chains implying particular role described
using finite automaton (FA). sufficient ensure decidability CQ answering;
however, FAs worst-case exponential size known approaches
provide tight upper complexity bound.
paper, solve open problem show answering CQs OWL
2 EL knowledge bases PSpace-complete combined complexity (i.e., complexity
measured total size input). end, use novel encoding regular role
inclusions using bounded-stack pushdown automatathat is, FAs extended stack
bounded size. Apart theoretical interest, encoding used practical tableau
algorithms avoid exponential blowup due role inclusions. addition, sharpen
lower complexity bound show problem PSpace-hard even consider
role inclusions part input (i.e., query parts knowledge
base fixed). Finally, turn attention navigational queries OWL 2 EL
knowledge bases, show answering positive, converse-free conjunctive graph
XPath queries PSpace-complete well; interesting since allowing converse
operator queries known make problem ExpTime-hard. Thus, paper
present several important contributions landscape complexity answering
expressive queries description logic knowledge bases.

1. Introduction
Description logics (DLs) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2010)
family knowledge representation formalisms logically underpin Web Ontology Language OWL 2 (Cuenca Grau, Horrocks, Motik, Parsia, Patel-Schneider, & Sattler,
2008). DL knowledge bases describe domain terms concepts (i.e., unary predicates),
roles (i.e., binary predicates), individuals (i.e., constants), describe relationships concepts, roles, individuals using logical axioms. DLs OWL

c
2014
AI Access Foundation. rights reserved.

fiStefanoni, Motik, Krotzsch, & Rudolph

2 steadily gaining popularity provide developers modern
information systems flexible graph-like data model natural countless application areas, Semantic Web (Gutierrez, Hurtado, Mendelzon, & Perez, 2011),
social network analysis (Fan, 2012), network traffic analysis (Barrett, Jacob, & Marathe,
2000). Answering queries DL/OWL knowledge bases core service applications
diverse monitoring financial products within Italian Ministry Economy
Finance (De Giacomo et al., 2012), accessing real-time diagnostic data turbines (Giese
et al., 2013), integrating configuration data air traffic control systems (Calvanese
et al., 2011). Due practical importance query answering, theoretical investigation
expressivity computational complexity query languages high
research agenda knowledge representation community past decade.
Conjunctive queries (CQs) (Chandra & Merlin, 1977) basic class queries
relational databases. Querying DL knowledge bases using CQs studied diverse range settings (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007; PerezUrbina, Motik, & Horrocks, 2010; Rudolph & Glimm, 2010; Kontchakov, Lutz, Toman,
Wolter, & Zakharyaschev, 2011; Ortiz, Rudolph, & Simkus, 2011; Gottlob & Schwentick,
2012; Venetis, Stoilos, & Stamou, 2012). However, conjunctive queries first-order definable thus cannot express certain important properties graph reachability.
Regular path queries (RPQs) (Cruz, Mendelzon, & Wood, 1987; Barcelo, 2013) alternative query language capable describing connections graph vertices using
regular expressions, allowing users navigate inside graph. example, RPQ
(isPartOf hasLocation) retrieves pairs vertices connected via zero isPartOf
edges followed one hasLocation edge. Furthermore, 2RPQs extend RPQs converse operator (i.e., backward navigation) (Calvanese, Vardi, De Giacomo, & Lenzerini,
2000); nested regular expressions allow existential quantification paths (Perez,
Arenas, & Gutierrez, 2010); C(2)RPQs extend (2)RPQs CQs conjunctions (2)RPQs (Calvanese, De Giacomo, Lenzerini, & Vardi, 2000; Bienvenu, Ortiz, &
Simkus, 2013). Finally, inspired XPath query language XML, graph XPath queries
(GXQs) recently proposed language querying graph databases (Libkin,
Martens, & Vrgoc, 2013) DL knowledge bases (Kostylev, Reutter, & Vrgoc, 2014; Bienvenu, Calvanese, Ortiz, & Simkus, 2014). GXQs extend 2RPQs negation regular
expressions, checking properties vertices using Boolean combinations node tests
is, concepts existential quantifications paths. example, graph XPath
query (isPartOf test(Cell hhasSpecialityi) hasLocation) refines aforementioned RPQ
requiring node isPartOf edges hasLocation edge instance Cell concept outgoing hasSpeciality edge. Graph XPath
queries straightforwardly extended conjunctive graph XPath queries (CGXQs).
query languages Boolean answer variables; hence, answer
query Boolean value.
1.1 Problem Setting
Although computing answers query DL knowledge base function problem,
common literature consider complexity associated decision problem
is, checking whether Boolean query entailed knowledge base. article

646

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

follow well-established practice analyse computational properties several
query languages DL knowledge bases. follow Vardi (1982) measure input
size two ways: combined complexity measures complexity terms combined
size query knowledge base, data complexity measures complexity
terms size data (i.e., query parts knowledge bases
considered fixed).
computational properties query answering DL knowledge bases depend
expressivity constructs used knowledge base query language
used. particular, conjunctive query answering expressive description logics
least exponential combined complexity (Glimm, Lutz, Horrocks, & Sattler, 2008; Lutz,
2008) intractable data complexity (Calvanese, De Giacomo, Lembo, Lenzerini, &
Rosati, 2013; Ortiz, Calvanese, & Eiter, 2008). problem becomes tractable data
complexity RL (Grosof, Horrocks, Volz, & Decker, 2003; ter Horst, 2005)
QL (Calvanese et al., 2007; Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009) profiles
OWL 2, several worst-case optimal algorithms proposed perform well
practice (Urbani, van Harmelen, Schlobach, & Bal, 2011; Rodriguez-Muro & Calvanese,
2012). paper, however, focus OWL 2 EL profile OWL 2, based
EL family DLs (Baader, Brandt, & Lutz, 2005). Basic reasoning problems
OWL 2 EL, checking concept subsumption instance checking, decided
polynomial time (Baader et al., 2005; Krotzsch, 2011), makes language
interesting practical applications. Motivated observation, paper present
several novel complexity results answering queries OWL 2 EL knowledge bases.
One important modelling constructs OWL 2 EL role inclusionsaxioms
form S1 Sn v express compositional properties roles. example,
following inclusions state role isPartOf transitive that, x located
part z, x located z.
isPartOf isPartOf v isPartOf

hasLocation isPartOf v hasLocation

Prior introduction EL family, role inclusions already identified
source undecidability expressive DLs loosely correspond context-free
grammars: inclusion S1 Sn v knowledge base seen production rule
S1 Sn , knowledge base induces context-free language L(S) role
S. Using correspondence, Wessel (2001) showed checking satisfiability ALCR
knowledge bases unrestricted role inclusions undecidable. regain decidability,
Horrocks Sattler (2004) proposed syntactic regularity restriction role inclusions
ensuring language L(S) regular thus recognised using finite
automaton (FA); Kazakov (2008) later showed that, cases, size automaton
necessarily exponential knowledge base size. OWL 2 DL profile OWL 2
extends ALCR thus incorporates regularity restriction definition.
Even unrestricted role inclusions, standard reasoning problems EL
solved polynomial time (Baader et al., 2005). Moreover, Stefanoni, Motik, Horrocks
(2013) showed answering CQs OWL 2 EL knowledge bases without role inclusions
NP-complete. However, using correspondence role inclusions contextfree grammars, Rosati (2007) Krotzsch, Rudolph, Hitzler (2007) independently
proved answering CQs EL knowledge bases unrestricted role inclusions
647

fiStefanoni, Motik, Krotzsch, & Rudolph

undecidable; furthermore, Krotzsch et al. (2007) also showed checking concept subsumptions EL knowledge bases inverse roles unrestricted role inclusions
undecidable.
OWL 2 EL inherits regularity restriction OWL 2 DL, undecidability proofs Rosati (2007) Krotzsch et al. (2007) apply OWL 2 EL.
fact, Krotzsch et al. (2007) showed answering CQs EL knowledge bases extended
regular role inclusions PSpace-hard combined complexity, proposed
CQ answering algorithm fragment OWL 2 EL regular role inclusions.
algorithm, however, runs PSpace if, role S, language L(S) represented using automaton polynomial size; due mentioned result Kazakov
(2008), approach provide us matching PSpace upper bound
problem. Ortiz et al. (2011) proposed different algorithm answering CQs OWL 2
EL knowledge bases (with regular role inclusions without restriction usage
features). Similarly algorithm Krotzsch et al. (2007), algorithm
Ortiz et al. (2011) also encodes regular role inclusions using finite automata. Hence,
algorithms run time polynomial size data thus settle
question data complexity, settle question combined complexity.
comparatively works studying complexity (conjunctive) graph
XPath queries DL knowledge bases. particular, Kostylev et al. (2014) observed
GXQs closely related propositional dynamic logic full negation (Harel, Tiuryn,
& Kozen, 2000), immediately shows answering GXQs DL knowledge bases
undecidable even respect empty knowledge base. Several GXQ fragments
proposed possible solution problem: path-positive GXQs disallow negation
role expressions, positive GXQs prohibit negation concepts well.
Kostylev et al. (2014) showed answering path-positive GXQs intractable data
complexity already queries without transitive closure operator knowledge
bases containing instance assertions. Recently, Bienvenu et al. (2014) showed
answering positive GXQs fragment OWL 2 EL tractable data complexity,
ExpTime-complete combined complexity.
1.2 Contributions
paper, present several novel complexity results answering queries OWL 2
EL knowledge bases.
First, present first CQ answering algorithm handle OWL 2 EL
(with regular role inclusions without restriction size FAs)
runs PSpace, thus settle open question combined complexity
CQ answering OWL 2 EL. result based novel encoding languages
induced regular role inclusions using pushdown automata (PDAs)that is, FAs extended
stack. show that, role S, construct polynomial time
PDA accepts language L(S) whose computations use stack size linear
number role inclusions. Bounded-stack PDAs (Anselmo, Giammarresi, & Varricchio,
2003) recognise precisely class regular languages exponentially
succinct finite automata (Geffert, Mereghetti, & Palano, 2010). obtain CQ
answering algorithm running PSpace, extend algorithm Krotzsch et al. (2007)

648

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

data
combined

ELHOdr

PTime

OWL 2 EL
PTime

Horn-SHOIQ
PTime

Horn-SROIQ
PTime

(Ortiz et al., 2011)

(Theorem 31)

(Ortiz et al., 2011)

(Ortiz et al., 2011)

NP

PSpace

ExpTime

2ExpTime

(Stefanoni et al., 2013)

(Theorem 31)

(Ortiz et al., 2011)

(Ortiz et al., 2011)

Table 1: complexity landscape CQ answering (all completeness results)
handle universal role, keys, self-restrictions, reflexive roles, thus covering
features EL profile apart datatypes, adapt handle
regular role inclusions encoded using PDAs. Apart allowing us obtain complexity
results presented paper, tableau algorithm Horrocks, Kutz, Sattler (2006)
used popular reasoners Pellet (Sirin, Parsia, Cuenca Grau, Kalyanpur, & Katz,
2007) FaCT++ (Tsarkov & Horrocks, 2006) straightforwardly modified use
bounded-stack PDAs instead FAs, could eliminate potential source inefficiency
practice. Finally, brevity simplicity deal datatypes paper;
however, set OWL 2 EL datatypes designed enable datatype
reasoning using external datatype checking procedure (Baader, Brandt, & Lutz, 2008;
Cuenca Grau et al., 2008) easily incorporated algorithm.
Second, improve PSpace lower bound Krotzsch et al. (2007) showing
answering CQs OWL 2 EL PSpace-hard already role inclusions
considered part input (i.e., conjunctive query, TBox, ABox
fixed). Furthermore, show CQs answered polynomial time query
role inclusions fixed, emphasises observation role inclusions
main source problems PSpace-hardness.
Third, show positive, converse-free CGXQsthat is, CGXQs allow
negation paths, negation concepts, converse operatorcan answered
OWL 2 EL knowledge bases using polynomial space. particular, OWL 2 EL allows
role inclusions, self-restrictions, reflexive roles, allow us polynomially reduce answering CGXQ answering CQ extended knowledge base. also
show answering positive, converse-free GXQs (i.e., CGXQs single atom)
done time polynomial input size. result interesting Bienvenu
et al. (2014) proved answering positive GXQs EL knowledge bases ExpTimecomplete; hence, adding converse operator increases complexity GXQs.
results thus show answering GXQs CGXQs difficult instance checking
answering conjunctive queries, respectively, least theoretical perspective
makes GXQs CGXQs appealing query languages OWL 2 EL knowledge bases.
1.3 Summary Complexity Landscape
Table 1 summarises complexity landscape answering CQs various DLs related
OWL 2 EL. Here, ELHOdr
fragment OWL 2 EL obtained allowing simple
role inclusions form v S, disallowing universal role, reflexive roles,
self-restrictions, datatypes, combined complexity result logic due
Stefanoni et al. (2013). Furthermore, Horn-SHOIQ extends ELHOdr
inverse roles
Horn qualified number restrictions, Horn-SROIQ extends Horn-SHOIQ role
649

fiStefanoni, Motik, Krotzsch, & Rudolph

data

positive
positive
converse-free converse-free
GXQs
CGXQs
PTime-c
PTime-c
(Theorem 34)

combined

PTime-c
(Theorem 34)

positive
GXQs

path-positive
GXQs

GXQs

PTime-h

coNP-h

coNP-h

(Theorem 34) (Bienvenu et al., 2014) (Kostylev et al., 2014) (Kostylev et al., 2014)

PSpace-c

ExpTime-h

ExpTime-h

undecidable

(Theorem 34) (Bienvenu et al., 2014) (Bienvenu et al., 2014) (Kostylev et al., 2014)

Table 2: complexity answering navigational queries OWL 2 EL knowledge bases
(c means complete, h means hard)

inclusions; results logics due Ortiz et al. (2011). CQ answering PTimecomplete data complexity cases, essentially due fact
logics Horn disjunctive reasoning needed. combined complexity,
table illustrates presence different constructs affects complexity answering
CQs. particular, extending ELHOdr
role inclusions increases complexity
NP PSpace; PSpace lower bound, increase solely due role inclusions.
Furthermore, extending ELHOdr
inverse roles increases complexity NP
ExpTime. Finally, extending OWL 2 EL inverse roles increases complexity
PSpace 2ExpTime.
Table 2 summarises complexity landscape answering navigational queries
OWL 2 EL knowledge bases. one see, adding converse operator increases
combined complexity GXQs ExpTime (Bienvenu et al., 2014). Moreover, adding
negation node tests increases data complexity GXQs coNP, whereas adding
negation path expressions leads undecidability combined complexity (Kostylev
et al., 2014). contrast, existential quantification paths increase complexity: answering positive, converse-free (C)GXQs OWL 2 EL knowledge bases
difficult answering (C)RPQs EL knowledge bases (Bienvenu et al., 2013).
1.4 Organisation Article
rest article organised follows. Section 2, present basic definitions
finite automata, pushdown automata, DL underpinning OWL 2 EL, conjunctive
queries. Section 3, introduce novel encoding regular role inclusions using PDAs
bounded stack size. Section 4, present CQ answering algorithm OWL 2
EL discuss complexity. Section 5, present improved PSpace lower-bound
answering CQs OWL 2 EL. Finally, Section 6, introduce (conjunctive) graph
XPath queries, show reduce problem answering positive, converse-free
conjunctive graph XPath queries answering ordinary conjunctive queries, present
aforementioned complexity results.

650

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

2. Preliminaries
section recapitulate basic definitions finite automata, pushdown automata,
DL ELRO+ underpinning OWL 2 EL, conjunctive queries. rest paper,
[i..j] set containing natural number k N k j.
2.1 Automata Language Theory
article, use standard notions alphabets (which must finite), strings, string
concatenation, Kleene operators, languages formal language theory (Hopcroft,
Motwani, & Ullman, 2003). assume alphabets contain special symbol
, use label transitions automata consume input symbols.
Furthermore, empty word. Finally, w w0 words, |w| number symbols
occurring w; w w0 unique word w00 w := w00 w0 w00 exists,
otherwise w w0 undefined.
2.1.1 Finite Automata
finite automaton (FA) tuple F = hQ, , , i, f Q finite set states,
input alphabet, : Q {} 7 2Q transition function, Q start state,
f Q final state. F deterministic |(s, )| = 0 |(s, c)| 1
Q c ; otherwise, F nondeterministic. size |F| F number
symbols used encode F tape Turing machine.
instantaneous description F pair hs, wi Q w .
derivation relation ` F smallest set that, states s0 Q,
symbol c , word w ,
s0 (s, c), hs, c wi ` hs0 , wi;
s0 (s, ), hs, wi ` hs0 , wi.
Let ` reflexive transitive closure `. Then, language accepted F
defined L(F) = {w | hi, wi ` hf, i}. language L regular FA
F exists L = L(F).
2.1.2 Pushdown Automata
pushdown automaton (PDA) tuple P = hQ, , , , i, I, f, F Q finite set
states; input alphabet; stack alphabet; transition function mapping
state Q, symbol c {}, stack symbol X finite subset
(s, c, X) Q ; Q start state; start stack ; f Q final
state; F final stack. size |P| P number symbols used
encode P tape Turing machine.
instantaneous description P triple hs, w, Q, w ,
. read stack content left rightthat is, leftmost symbol
top stack. derivation relation ` P smallest set that,
states s0 Q, symbol c , word w , stack symbol X ,
words 0 ,

651

fiStefanoni, Motik, Krotzsch, & Rudolph

hs0 , 0 (s, c, X) implies hs, c w, X ` hs0 , w, 0 i;
hs0 , 0 (s, , X) implies hs, w, X ` hs0 , w, 0 i.
Let ` reflexive transitive closure relation `. Then, language accepted
P defined L(P) = {w | hi, w, Ii ` hf, , F i}.
definitions PDA P language L(P) somewhat nonstandard:
literature typically considers Hopcroft PDA (Hopcroft et al., 2003) Ph differs
definition contain final stack F initial stack symbol
(rather word ); moreover, language accepted Ph defined
Lh (Ph ) = {w | : hi, w, Ii ` hf, , i}. show next definitions
equivalent standard definitions Hopcroft et al. (2003).
Proposition 1. following two properties hold.
(1) PDA P, Hopcroft PDA Ph exists L(P) = Lh (Ph ).
(2) Hopcroft PDA Ph , PDA P exists Lh (Ph ) = L(P).
Proof (Sketch). first prove property (1), prove property (2).
(1) show transform arbitrary PDA P Hopcroft PDA Ph
L(P) = Lh (Ph ). Ph uses fresh initial state i0 fresh stack symbols Z0
occurring . Symbol Z0 start stack symbol Ph ; furthermore, Ph new
-transition moves PDA state i0 initial state P replacing Z0
, start stack P. point, Ph simulates P, always leaving
bottom stack reaches final state f P. Next, Ph uses fresh
states s1 , . . . , s|F | fresh -transitions move Ph state f s|F | reading F
stack. Finally, s|F | , PDA Ph -moves fresh final state f 0 top-most
symbol stack , thus accepting input whenever P reaches f F
stack. Automata P Ph clearly accept languages.
(2) show transform arbitrary Hopcroft PDA Ph PDA P
Lh (Ph ) = L(P). PDA P uses fresh stack symbol , initial stack
initial stack symbol Ph , final stack empty word. P simulates Ph ,
always leaving bottom stack reaches final state f Ph . Next, P
-moves fresh final state f 0 pops topmost symbol stack. point,
PDA takes -transitions empty stack, eventually reaching final state
empty stack. Automata P Ph clearly accept languages.
k natural number, k-bounded language accepted P set Lk (P) containing word w derivation hs0 , w0 , 0 ` ` hsn , wn , n exists
s0 sn start final state P, respectively;
w0 = w wn = ;
0 n start final stack P, respectively;
|i | k [0..n].

652

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

Then, P k-bounded stack L(P) = Lk (P). stack P bounded
constant, PDA P simulated finite automaton encodes stack contents
using states, L(P) regular, translating P finite automaton may
require space exponential k (Geffert et al., 2010). contrast, following proposition
shows exists PDA Pk L(Pk ) = Lk (P) size Pk polynomial
size P k.
Proposition 2. PDA P natural number k, one compute polynomial
time PDA Pk L(Pk ) = Lk (P).
Proof. Let P = hQ, , , , i, I, f, F PDA let k N natural number. Let
Pk = hQk , , , k , ik , I, fk , F PDA defined
Qk = Q [0..k];
transition function k smallest function that, ` [0..k], symbol
c {}, states s, s0 Q, word hs0 , (s, c, X)
` + || 1 k, hhs0 , ` + || 1i, k (hs, `i, c, X);
ik = hi, |I|i fk = hf, |F |i.
Clearly, Pk computed time polynomial size P k. Let ` `k
derivation relations P Pk , respectively. definitions k ik ,
hhs, `i, w, `k hhs0 , ji, w0 , 0 hs, w, ` hs0 , w0 , 0 i, || = ` | 0 | = j,
max(`, j) k. Thus, Lk (P) = L(Pk ), required.
2.2 Description Logic ELRO+ Conjunctive Queries
description logic ELRO+ , underpinning OWL 2 EL, defined w.r.t. signature consisting mutually disjoint countably infinite alphabets C, R, atomic concepts,
roles, individuals, respectively. assume {>c , c } C, >c top
concept c bottom concept; similarly, assume {>r , r } R, >r
top role (universal role) r bottom role. individual I, expression
{a} nominal is, concept consisting precisely individual a. Then, N set
containing nominal {a} individual I. call B C N basic concept.
role chain word R; || = 0, call empty role chain write
. Concepts, TBox axioms, RBox axioms, ABox axioms defined specified
Table 3. ELRO+ TBox finite set concept inclusions, range restrictions,
keys; ELRO+ RBox R finite set role inclusions.
R ELRO+ RBox, let R := {>r } {S R | occurs R}; furthermore,
rewrite relation =
w.r.t. R smallest relation role chains following
holds role chains 1 2 .
1 2 =
1 2 axiom v R.
1 >r 2 =
1 2 role chain R .
=
reflexivetransitive closure
= . role, L(S) := { R | =
}
language induced RBox R. role simple R if, role chain
653

fiStefanoni, Motik, Krotzsch, & Rudolph

Concepts:
top concept
bottom concept
nominal
conjunction
self-restriction
existential restriction
Role chains:
top role
bottom role
empty role chain
nonempty role chain
TBox axioms:
concept inclusion
range restriction
key

RBox axioms:
role inclusion
ABox axioms:
concept assertion
role assertion

Syntax

Semantics

>c
c
{a}
C uD
S.Self
S.C



{aI }
C DI
{x | hx, xi }
{x | C : hx, yi }

>r
r

S1 Sn



{hx, xi | x }
S1I SnI

CvD
range(S, C)
key(C, S1 . . . Sn )

C DI
C
x, y, z1 , . . . , zn
individuals a, b, c1 , . . . , cn exist
x = aI , = bI , zi = cIi 1 n,
x = holds whenever {x, y} C
{hx, zi i, hy, zi i} SiI 1 n.

vS


bI
SI

A(b)
S(a, b)

haI , bI

Table 3: Interpreting ELRO+ concepts, roles, axioms interpretation = hI ,

S=
, || 1. ELRO+ ABox finite set concept role assertions.
Finally, ELRO+ knowledge base (KB) tuple K = hT , R, Ai ELRO+
TBox, R ELRO+ RBox, ELRO+ ABox
concept S.Self occurring , role simple R;
S1 Sn v R range(S 0 , C) 0 =
S, role Sn0 R
0

0
exists Sn =
Sn range(Sn , C) .
Let |T |, |R|, |A| numbers symbols needed encode , R, A, respectively,
tape Turing machine, let |K| = |T |+|R|+|A|. Furthermore, knowledge
base, TBox, ABox, define
:= {a | occurs }, N := {{a} | }, C := {A C | occurs }.
654

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

semantics ELRO+ defined follows. interpretation tuple = hI ,
nonempty set domain elements, called domain I, interpretation function maps individual domain element aI ,
atomic concept C \ {>c , c } set AI , atomic role R \ {>r , r }
relation . Function extended concepts role chains shown
upper part Table 3, denotes composition binary relations. interpretation model K satisfies axioms occurring K shown bottom
Table 3. Moreover, K consistent model K exists; K inconsistent model
K exists; K entails first-order sentence (resp. concept inclusion C v role
inclusion v S), written K |= (resp. K |= C v K |= v S), |= (resp. C DI
) model K. definition L(S), L(S) implies
K |= v S. Knowledge base consistency, entailment concept inclusions, entailment
role inclusions decided polynomial time (Krotzsch, 2011; Baader et al., 2005).
2.2.1 Conjunctive Queries
term individual variable. atom expression form A(t) R(t0 , t)
atomic concept, R role, t0 terms. conjunctive query (CQ)
formula q = ~y .(~x, ~y ) conjunction atoms variables ~x ~y . Variables ~x
answer variables q. ~x empty, call q = ~y .(~y ) Boolean CQ (BCQ).
substitution partial mapping variables terms; dom() rng()
domain range , respectively. conjunction atoms, ()
result applying substitution atoms . Then, (q) = ~z.(), ~z contains
(i) (y) variable ~y (y) variable, (ii) variable ~y
(y) undefined. definition (q) somewhat nonstandard quantified
variables also replaced: example, given q = y1 , y2 , y3 .R(y1 , y2 ) (y1 , y3 )
= {y2 7 a, y3 7 z}, (q) = y1 , z.R(y1 , a) (y1 , z).
Let K = hT , R, Ai ELRO+ knowledge base let q = ~y .(~x, ~y ) CQ.
q K q uses predicates individuals occurring K. substitution
candidate answer q K, dom() = ~x rng() IK , certain
answer q K K |= (q). Answering q K amounts computing
set certain answers q K. stated, CQ answering function problem;
thus article study complexity associated decision problem named BCQ
answering, problem deciding, given Boolean CQ q K, whether K |= q.
Please note BCQ answering equivalent recognition problem decides,
given CQ q K candidate answer , whether certain answer q K.
Following Vardi (1982), combined complexity assumes q K part
input, data complexity assumes ABox part input.
2.3 Ensuring Decidability BCQ Answering via Regularity
Rosati (2007) Krotzsch et al. (2007) independently showed answering Boolean
CQs ELRO+ knowledge bases undecidable. Intuitively, role inclusions simulate derivations context-free languages; thus, Boolean CQ check whether two
context-free languages non-empty intersection, known undecidable problem (Hopcroft et al., 2003).

655

fiStefanoni, Motik, Krotzsch, & Rudolph

regain decidability, next recapitulate definition so-called regular RBoxes
Horrocks Sattler (2004). Let R ELRO+ RBox let smallest
transitive relation R that, 0 v R 6= , S.
Then, RBox R regular irreflexive role inclusion v R form
(t1) v S,
(t2) v S,
(t3) S1 Sn v Si 6= [1..n],
(t4) S1 Sn v Si 6= [1..n],
(t5) S1 Sn v Si 6= [1..n].
induction define level lv(S) role R follows: lv(S) = 0
R exists S; otherwise, lv(S) = 1 + max{lv(T ) | S}. Clearly,
lv(S) computed time polynomial |R|. Section 4 show BCQ answering
ELRO+ KBs regular RBoxes PSpace.
2.4 Normalising ELRO+ Knowledge Bases
simplicity, rest paper assume ELRO+ knowledge base
K = hT , R, Ai normalised, case following properties hold.
(n1) IK 6= , K 6|= {a} v {b} {a, b} IK 6= b.
(n2) axiom one following forms, A(i) basic concepts role.
A1 u A2 v A3

A1 v S.A2

S.A1 v A2

v S.Self

S.Self v

(n3) axiom v R || 2 6= >r , role also
occurs R.
next show knowledge base K normalised polynomial time without
affecting regularity RBox component answers Boolean CQs.
Proposition 3. ELRO+ knowledge base K regular RBox Boolean
CQ q K, one compute polynomial time normalised ELRO+ knowledge base
K0 Boolean CQ q 0
RBox K0 regular,
q 0 K0 , K |= q K0 |= q 0 .
Proof. Let K ELRO+ KB regular RBox let q Boolean CQ K.
first satisfy property (n1). Let K1 obtained K extending ABox
K assertion >c (c) c fresh individual; clearly, K1 |= q K |= q.
Next, let K2 q 0 obtained K1 q, respectively, uniformly substituting
individual arbitrary, fixed, individual b K1 |= {a} v {b}. Entailment
656

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

concept inclusions decided polynomial time, K2 q 0 computed
polynomial time. Moreover, K2 q 0 obtained replacing individual
arbitrary, fixed individual b aI = bI model K1 , q 0 K2 ,
K2 |= q 0 K1 |= q.
next satisfy property (n2). Let K3 result eliminating keys K2 .
one see Table 3, keys derive axioms form {a} v {b}; moreover,
effects conclusions already captured K2 , K3 |= q 0
K2 |= q 0 . Next, eliminate polynomial time range restrictions occurring
K applying syntactic transformation Baader et al. (2008); let K4 resulting
knowledge base. Since definition ELRO+ knowledge base carefully restricts
interactions role inclusions range restrictions, K4 |= q 0
K3 |= q 0 (Baader et al., 2008). Next, following Krotzsch (2011), compute polynomial
time knowledge base K5 satisfies (n2) K5 |= q 0 K4 |= q 0 .
next satisfy property (n3). Let K6 result exhaustively decomposing
role inclusion v form (t3)(t5) || > 2 occurring K5 according
following rewrite rules, occurrence role 0 fresh.
(t3) S1 Sn v 7 {S 0 v S,
1 Sn v 0 }
0
(t4)
S1 Sn v 7 {S Sn v S, S1 Sn1 v 0 }
(t5) S1 Sn v
7
{S 0 v S,
1 Sn v 0 }
linearly many rewrite steps required satisfy (n3), resulting RBox
regular. Furthermore, model K6 also model K5 model K5
expanded model J K6 interpreting role 0 occurring K6 \ K5
(S 0 )J = (0 )J , 0 unique role chain 0 v 0 occurs K6 . Thus,
K6 |= q 0 K5 |= q 0 . Next, let K7 result removing axiom
v >r K6 ; removed axioms tautologies, K7 |= q 0 K6 |= q 0 .
Finally, let K0 result adding axiom r v S, role occurs K7
occur RBox component. axioms K0 \ K7 preserve regularity
tautologies, K0 |= q 0 K7 |= q 0 , required.

3. Encoding Regular RBoxes Succinctly Using Bounded-Stack PDAs
reasoning algorithm DL role inclusions known us uses step checks
whether L(S) holds arbitrary role chain role S. example, check
whether K |= S(a, b) holds, algorithm must ensure that, model K, role chain
L(S) exists connecting elements interpreting b. Although characterise
languages L(S), role inclusions lend well language recognition,
algorithms known us transform role inclusions another, manageable form.
analogous fact that, regular expressions characterise regular languages,
former routinely transformed FAs order facilitate language recognition.
Horrocks Sattler (2004) showed that, regular RBox R role
occurring R, one construct FA FS L(FS ) = L(S). FAs used
tableau decision procedure SROIQthe DL underpinning OWL 2 DL (Horrocks
et al., 2006). Given SROIQ knowledge base, tableau procedure tries construct
657

fiStefanoni, Motik, Krotzsch, & Rudolph

S2

iS2

start




iS1

S1

fS2

iS1

fS1



S0

fS0

fS1





iS0

S1


iS0

S0

iS0

fS0

S0

fS0

iS0

S0

fS0

Figure 1: FA FS2 constructed following Horrocks Sattler (2004)
finite graph representing model KB, edges labelled roles,
vertices labelled concepts. aforementioned FAs used ensure universal
restriction S.C obey constraints imposed role inclusions; roughly speaking,
obtained running FS graph updating current state FS along
path, labelling reachable vertex state FS final concept
C. Simanck (2012) optimised tableau procedure simulating FAs on-the-fly, rather
precomputing advance.
Horrocks Sattler (2004) observed FAs contain exponentially many
states. Kazakov (2008) proved unavoidable cases: regular RBox
Rn containing axioms (1), size FA F L(F) = L(Sn ) exponential n.
Si1 Si1 v Si

[1..n]

(1)

blowup number states caused simple model computation underlying FAs, behaviour automaton determined solely current state.
example above, L(Sn ) whenever consists Si repeated j times
[0..n] j = 2ni . Thus, parsing , FA recognising L(Sn ) must
remember number occurrences Si already seen, achieved
using different state number 0 2n . Figure 1 shows FA FS2
constructed Horrocks Sattler (2004): remember current state, FS2 contains
two copies automaton FS1 , copy FS1 contains two copies automaton FS0 .
Hence, obtain PSpace procedure, must devise succinct representation
languages induced role inclusions. Towards goal, note role inclusions
closely related context-free grammars, context-free languages efficiently
recognised using pushdown automata (Hopcroft et al., 2003)that is, FAs extended
infinite stack storing contextual information. Hence, given regular RBox R
role occurring R, construct PDA PS accepts L(S). Unlike FA shown
Figure 1 remembers contextual information using states, PDA PS uses stack
remember current status computation determine proceed. show
number states PS polynomial size R, PS recognise
L(S) using stack size linear size R; thus, PS provides us required
succinct encoding FS . Section 4, use PDAs algorithm answers
Boolean CQs ELRO+ knowledge bases using polynomial space.
658

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

start

iS2

S2 , X/X

fS2
, X/X

, X/iS1 fS2 X

iS1

S1 , X/X

i>r

fS1

R, X/X

f>r

, X/X
, X/iS0 fS1 X

iS0

S0 , X/X

fS0

Figure 2: PDA PS2 corresponding FA FS2 , X R R R
rest section, fix arbitrary regular RBox R. Proposition 3,
assume role inclusion v R || 2 6= >r .
role occurring R , next define PDA PS .
Definition 4. Let R role. Then, PS = hQR , R , R , R , , , fS , PDA
QR = {iT , fT | R } set states, R = QR {} stack alphabet,
R smallest transition function satisfying following conditions X R .
(r) R \ {>r }, hfT , Xi R (iT , T, X).
(t1) v R, hfT , Xi R (iT , , X).
(t2) v R, hiT , Xi R (fT , , X).
(t3) T1 v R, hiT1 , Xi R (iT , , X).
(t4) T1 T2 v R, hiT1 , iT2 fT Xi R (iT , , X).
(t5) T2 v R, hiT2 , fT Xi R (fT , , X).
(ur) R , hf>r , Xi R (i>r , T, X).
(u1) hf>r , Xi R (i>r , , X).
(u2) hi>r , Xi R (f>r , , X).
(p) R QR , hs, R (fT , , s).
following examples, present PDA succinctly encodes FA FS2 ,
explain different types transitions Definition 4, content
stack influences computation PDAs.
Example 5. Figure 2 shows PDA PS2 corresponding FA FS2 Figure 1.
c, X/

transition hs0 , R (s, c, X) shown s0 , X/ indicates transition replaces top-most stack symbol X word ; moreover, transitions form
(p) Definition 4 shown figure sake clarity. one see
figure, unlike FA FS2 , copying states PDA PS2 .
659

fiStefanoni, Motik, Krotzsch, & Rudolph

, X/X

start



S, X/X

, X/i>r fS X

i>r

fS

R, X/X

f>r

, X/X

, X/fS X
, X/iT X

iP



T, X/X

fT

, X/X
, X/X

P, X/X

fP
Figure 3: PDA PS RBox Example 6, X R R R
Example 6. explain different types transitions Definition 4 stack
used computation PDA, use regular RBox R containing role inclusions
(2)(6). Figure 3 shows PDA PS using notation Example 5.
vP

(2)

vT

(3)

P >r v

(4)

ST vS

(5)

P vT

(6)

role R associated states fT , moving former
latter ensures PDA reads role chain L(T ). transition type (r) allows
PDA read state . -transition type (t1) fT added
reflexive, allows PDA read empty role chain; example, axiom
(2) introduces -transition iP fP . Moreover, -transition type (t2)
fT added transitive, allows PDA read number role
chains 1 , . . . , n L(T ); example, axiom (3) introduces -transition fT
. Transitions types (ur), (u1), (u2) analogously reflect properties >r :
(ur) allows PDA read arbitrary role, (u1) (u2) reflect reflexivity
transitivity >r , respectively. None transitions affect PDAs stack.
illustrate transitions type (t4), next show how, 1 = P S, PDA PS
determines 1 L(S); latter ensured axiom (4). assume PDA PS
state stack. Due axiom (4), PS make -transition type (t4)
state iP , pushing i>r fS stack. Since new state iP , PDA next need
read P ; furthermore, stack content signals PDA that, finishes reading
P , move state i>r read >r state fS finish reading S. Indeed,
PS make transition type (r) state fP read P , followed -transition
type (p) state i>r popping i>r stack; next, PDA make transition
660

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

type (ur) state f>r reading S, followed -transition type (p) state fS popping
fS stack. point, PDA accepts input.
illustrate transitions types (t3) (t5), next show how, 2 = P , PDA
PS determines 2 L(S); latter ensured axioms (5) (6). Again, assume
PDA PS state stack. PDA PS make transition type
(r) state fS , reading leaving stack unchanged; next, due axiom (5), PS
make -transition type (t5) state , pushing fS stack. Due axiom (6),
PDA PS next make -transition type (t3) state iP , pushing stack;
point, stack contains fS . Next, PDA make transition type
(r) state fP reading P , -transition type (p) state popping
stack; furthermore, analogous way, PDA move state fT reading
leaving fS stack. Finally, PDA make -transition type (p) state
fS popping fS stack. point, PDA accepts input.
understand benefit using PDAs rather FAs, note PS reaches state iP
recognising 1 2 . Role P occurs axioms (4) (6), PS moves
state iP order read occurrence P , must remember two
axioms caused move knows continue reading P : 1 , PS must
continue reading >r , whereas 2 , must continue reading . Unlike FAs Horrocks
Sattler (2004) remember information copying states, PS remembers
information stack: 1 , reaches iP i>r fS stack, whereas 2 ,
PS reaches iP fS stack. Thus, stack PS analogous stacks
programming languages: stack symbols correspond return addresses, transitions
type (p) correspond return statements.
following proposition immediate definition PDA PS .
Proposition 7. PDA PS computed time polynomial |R|.
following theorem states PDA PS accepts L(S) PS stack bounded
size R. proof result given Section 3.1.
Theorem 8. role R role chain ,
1. L(PS ) L(S),
2. PS stack bounded 2 lv(S) + 1.
Theorem 8 gives rise following notion depth RBox R, provide us
global bound stack size PDAs encoding R.
Definition 9. depth RBox R defined dR := maxSR (2 lv(S) + 1).
Finally, outline bounded-stack encoding regular RBoxes reduce
space used tableau algorithm SROIQ. Since ELRO+ support inverse
roles, Definition 4 directly provide us encoding languages induced
SROIQ RBoxes. Nevertheless, extend construction completing
RBox R inv(Sn ) inv(S1 ) v inv(S) R role inclusion S1 Sn v
RBox, inv() maps role inverse. One check that, (inverse) role
661

fiStefanoni, Motik, Krotzsch, & Rudolph

S, PDA PS constructed using completed RBox R encodes FS . Then, modify
portion tableau algorithm responsible checking satisfaction universal
restrictions running bounded-stack PDA graph constructed tableau
procedure. Roughly speaking, universal restriction S.C labelling vertex, run
PS graph updating current state stack PS , label
reachable vertex current state stack PS final concept C. Since
PS stack size polynomial |R|, requires polynomial space, unlike
FAs Horrocks Sattler (2004) optimised encoding Simanck (2012),
may require exponential space.
3.1 Proof Correctness
section, prove Theorem 8. Towards goal, let ` derivation relation
w.r.t. transition function R ; furthermore, derivation step hs, , ` hs0 , 0 , 0 i,
write hs, , `x hs0 , 0 , 0 hs0 , 0 , 0 obtained hs, , applying
transition form (x) Definition 4 x {r, t1, . . . , t5, ur, u1, u2, p}.
3.1.1 Soundness Stack Boundedness
section, prove that, role R role chain ,
1. L(PS ) implies L(S),
2. PS stack bounded 2 lv(S) + 1.
end, first show PDA PS satisfies following liveness property:
computation PS pushes state QR stack, PS eventually pop
stack. Then, show derivation PS moving state state fS takes
one five forms; call derivations regular. Finally, show regular derivations
satisfy properties (1) (2).
start showing PDA PS satisfies following liveness property.
Lemma 10. Let hs0 , 0 , 0 ` ` hsn , n , n arbitrary derivation
s0 = , sn = fS , 0 = role R word R . Then,
role lv(T ) < lv(S) [0..n] si {iT , fT } = s0i i0
s0i QR , index j [i..n] exists
(a) sj = fT j = ;
(b) k [i..j], word k form k := k00 k00 R ; and,
(c) sj+1 = s0i , j+1 = i0 , j+1 = j .
Proof. Let hs0 , 0 , 0 ` ` hsn , n , n above, [0..n 1], let
xi {r , t1 , . . . , t5 , ur , u1 , u2 , p} form derivation step ithat is, fix xi (arbitrarily one possibility) hsi , , `xi hsi+1 , i+1 , i+1
holds. Furthermore, role lv(T ) < lv(S), let set containing
index [0..n] si {iT , fT } form := s0i i0 s0i QR .
Note that, index , due lv(T ) < lv(S), si {iT , fT }, sn = fS ,
662

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

< nthat is, hsi , , ` hsi+1 , i+1 , i+1 occurs derivation. Next,
induction N, show that, role = lv(T ) < lv(S) ,
j [i..n] exists satisfying properties (a)(c).
Base case (). Consider arbitrary role R 0 = lv(T ) < lv(S).
consider interesting case 6= ; otherwise, properties (a)(c) hold vacuously.
Since lv(T ) = 0 si {iT , fT }, xi {r , t1 , t2 , ur , u1 , u2 , p}. reverseinduction (i.e., induction starting maximal element), next show
index satisfies required properties.
Base case. Let = max . xi {r , t1 , t2 , ur , u1 , u2 }, si+1 {iT , fT }
i+1 = ; thus, + 1 , contradicts maximality i.
remaining possibility xi = p, implies si = fT , si+1 = s0i , i+1 = i0 ,
i+1 = ; then, j = satisfies properties (a)(c).
Inductive step. Consider arbitrary index properties (a)(c) hold
` ` > i. xi {r , t1 , t2 , ur , u1 , u2 }, si+1 {iT , fT } i+1 = ;
hence, ii+1 so, inductive hypothesis, index j exists satisfying properties
(a)(c). Otherwise, xi = p, si = fT , si+1 = s0i , i+1 = i0 , i+1 = , j =
satisfies properties (a)(c).
Inductive Step (). Consider arbitrary N properties (a)(c) hold
role P R lv(P ) lv(P ) < lv(S) index IP . Furthermore,
consider arbitrary role + 1 = lv(T ) < lv(S). consider interesting
case 6= ; otherwise, properties (a)(c) hold vacuously. Recall
v 0 R 0 6= >r , lv(>r ) = 0 6= >r . Thus,
xi 6 {ur , u1 , u2 }. reverse-induction , next show index satisfies
required properties.
Base case (). Let = max . xi {r , t1 , t2 }, si+1 {iT , fT } i+1 = ;
thus, + 1 , contradicts maximality i. xi {t3 , t4 , t5 },
si+1 {iP , fP } role P lv(P ) < lv(T ) lv(P ) < lv(S); furthermore,
00 {i , f } 00
i+1 form i+1 := i+1




i+1 sequence
00
zero one states. state occurring i+1
{iR , fR } role
R level less . then, inductive hypothesis (), index ` > exists
s` = sT ` = , contradicts maximality i. Finally, xi = p,
si = fT , si+1 = s0i , i+1 = i0 , i+1 = , j = satisfies properties (a)(c).
Inductive step (). Consider arbitrary index properties (a)(c) hold
index ` ` > i, consider possible forms xi .
xi {r , t1 , t2 }. Then, si+1 {iT , fT } i+1 = , + 1 . inductive
hypothesis (), index j exists satisfying properties (a)(c).
xi = t3 . Then, si+1 = iT1 i+1 = role T1 lv(T1 ) < lv(T ).
Thus, + 1 IT1 . inductive hypothesis (), index ` [i + 1..n] exists
s` = fT1 ` = i+1 ; furthermore, k [i + 1..`], k
form k := k00 i+1 word k00 R ; finally, s`+1 = `+1 = .
definition , ` + 1 . inductive hypothesis (),
index j exists satisfying properties (a)(c).
663

fiStefanoni, Motik, Krotzsch, & Rudolph

xi = t4 . Then, si+1 = iT1 i+1 = iT2 fT roles T1 T2
lv(T1 ) < lv(T ) lv(T2 ) < lv(T ). Thus, + 1 IT1 . inductive hypothesis
(), index `1 [i + 1..n] exists s`1 = fT1 `1 = i+1 ; furthermore,
k [i..`1 ], k form k := k00 i+1 word k00 R ;
finally, s`1 +1 = iT2 `1 +1 = fT . Then, `1 + 1 IT2 . Again, inductive
hypothesis (), index `2 [`1 + 1..n] exists s`2 = fT2 `2 = `1 +1 ;
furthermore, k [`1 + 1..`2 ], k form k := k00 `1 +1
word k00 R ; finally, s`2 +1 = fT `2 +1 = . definition ,
`2 + 1 . So, inductive hypothesis (), index j exists satisfying
properties (a)(c).
xi = t5 . Then, si+1 = iT2 i+1 = fT role T2 lv(T2 ) < lv(T ).
Then, + 1 IT2 . inductive hypothesis (), index ` [i + 1..n] exists
s` = fT2 ` = i+1 ; k [i..`], k form
k := k00 i+1 word k00 R ; finally, s`+1 = fT `+1 = .
definition , ` + 1 . So, inductive hypothesis (),
index j exists satisfying properties (a)(c).
xi = p. Then, si = fT , si+1 = s0i , i+1 = i0 , i+1 = . Therefore, j = satisfies
properties (a)(c).
Next, role R , define notion regular derivations PS .
Definition 11. set regular derivations P>r inductively defined follows,
role R , role chain R , R .
sequr hi>r , 0 , `ur hf>r , 0 , regular derivation P>r .
sequ1 hi>r , 0 , `u1 hf>r , 0 , regular derivation P>r .
sequ2 hi>r , 0 , ` ` hf>r , k , hi>r , k , ` ` hf>r , n , regular derivations P>r , following also regular derivation P>r .
hi>r , 0 , ` ` hf>r , k , `u2 hi>r , k , ` ` hf>r , n ,
Next, consider arbitrary natural number N assume regular derivations
PT already defined = >r role R \ {>r } lv(T ) m.
Then, role R \ {>r } lv(S) = + 1, regular derivations PS defined
follows, S(i) R , R , R .
seqr , 0 , `r hfS , 0 , regular derivation PS .
seqt1 v R, , 0 , `t1 hfS , 0 , regular derivation PS .
seqt2 v R , 0 , ` ` hfS , k , , k , ` ` hfS , n ,
regular derivations PS , following also regular derivation PS .
, 0 , ` ` hfS , k , `t2 , k , ` ` hfS , n ,

664

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

seqt3 S1 v R, hiS1 , 0 , ` ` hfS1 , k , regular derivation
PS1 , , k , ` ` hfS , n , regular derivation PS , following
also regular derivation PS .
, 0 , `t3 hiS1 , 0 , ` ` hfS1 , k , `p , k , ` ` hfS , n ,
seqt4 S1 S2 v R, hiS1 , 0 , iS2 fS ` ` hfS1 , k , iS2 fS regular derivation PS1 , hiS2 , k , fS ` ` hfS2 , n , fS regular derivation PS2 ,
following also regular derivation PS .
, 0 ,
hiS1 , 0 , iS2 fS
fS
hiS2 , k ,
hfS , n ,

`t4
` ` hfS1 , k , iS2 fS `p
fS `p
` ` hfS2 , n ,


seqt5 S2 v R, , 0 , ` ` hfS , k , regular derivation PS ,
hiS2 , k , fS ` ` hfS2 , n , fS regular derivation PS2 , following also regular derivation PS .
, 0 , ` ` hfS , k , `t5 hiS2 , k , fS ` ` hfS2 , n , fS `p hfS , n ,
left show derivation PS moves PDA start state
final state fS regular regular derivations satisfy required properties.
following lemma, show derivations leave particular word
bottom stack regular satisfy properties (1) (2). Subsequently,
show accepting derivation PS form.
Lemma 12. role R , word R , derivation form
hs0 , 0 , 0 ` ` hsn , n , n s0 = , sn = fS , 0 = ,
(i) derivation regular PS ;
(ii) [0..n], |i | 2 lv(S);
(iii)
= 0 n .
Proof. prove claim induction n N+ .
Base case. n = 1, consider arbitrary role R , word R , sequence
, 0 , 0 ` hfS , 1 , 1 i. Definition 4, transitions cases (r), (t1), (ur),
(u1) move PS state state fS . transitions leave stack untouched,
1 = = 0 property (ii) holds. properties (i) (iii), next consider
four different forms sequence may take.
, 1 , 0 `r hfS , 1 , 1 i. 6= >r , regular derivation
PS case seqr (i) holds. Finally, 0 1 = S, implies =
0 1 ,
(iii) holds.
665

fiStefanoni, Motik, Krotzsch, & Rudolph

, 0 , 0 `t1 hfS , 0 , 1 i. 6= >r , regular derivation PS
case seqt1 (i) holds. Finally, 0 1 = ; moreover, case t1 Definition 4,
v R, =
; hence, =
0 1 (iii) holds.
, 1 , 0 `ur hfS , 1 , 1 i. = >r R , regular
derivation P>r case sequr (i) holds. Finally, 0 1 = R ,
implies =
0 1 , (iii) holds.
, 0 , 0 `u1 hfS , 0 , 1 i. = >r , regular derivation P>r
case sequ1 (i) holds. Finally, 0 1 = ; hence, =
, (iii) holds.
Inductive step. Consider arbitrary n N+ assume (i)(iii) hold
role 0 R , word 0 R , derivation hs00 , 00 , 00 0 ` ` hs0c , 0c , c0 0
length n form required lemma. Furthermore, consider
arbitrary role R , arbitrary word R , arbitrary derivation
hs0 , 0 , 0 ` ` hsn+1 , n+1 , n+1

(7)

length n + 1 s0 = , 0 = , sn+1 = fS . [0..n 1], let
xi {r , t1 , . . . , t5 , ur , u1 , u2 , p} form derivation step ithat is, fix xi (arbitrarily one possibility) hsi , , `xi hsi+1 , i+1 , i+1
holds. next consider possible forms sequence might have, show
properties (i)(iii) hold case.
(Case 1) = >r . consider form hs0 , 0 , 0 `x0 hs1 , 1 , 1 i. Since
s0 = i>r , x0 {t1 , t3 , t4 , ur , u1 }. R normalised, v 0 R
0 6= >r , x0 {ur, u1} s1 = f>r 1 = = 0 . Since n > 1,
hs1 , 1 , 1 `x1 hs2 , 1 , 2 occurs sequence x1 {t2 , t5 , u2 , p}. Since
s1 = f>r R normalised, x1 {u2 , p}; furthermore, since 1 =
assumption form (7), x1 6= p. Hence, remaining possibility
x1 = u2 . case (u2) Definition 4, s2 = i>r , 2 = 1 , 2 = 1 .
next prove properties (i)(iii) hold.
(i) sequr sequ1 , hs0 , 0 , 0 `x0 hs1 , 1 , 1 regular derivation P>r .
inductive hypothesis, hs2 , 2 , 2 ` ` hsn+1 , n+1 , n+1 also regular
derivation PS . definition regular derivations, n = 2 = .
then, (7) regular derivation PS case sequ2 .
(ii) Words 0 , 1 , 2 empty. inductive hypothesis, |` | 2 lv(>r )
` [2..n + 1]. Therefore, |i | 2 lv(>r ) holds [0..n + 1].
(iii) inductive hypothesis, >r =
2 n+1 . cases (ur) (u1), either
0 2 = 0 2 = R . then, >r
= 0 n+1 holds.
(Case 2) 6= >r k [0..n] exists hsk , k , k `t2 hsk+1 , k+1 , k+1
sk = fS . Then, case (t2) Definition 4, v R, sk+1 = , k+1 = k ,
k+1 = k . next prove properties (i)(iii) hold.

666

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

(i) inductive hypothesis, hs0 , 0 , 0 ` ` hsk , k , k regular derivation
PS . definition regular derivations, k = 0 = . Since sk+1 =
k+1 = k = , hsk+1 , k+1 , k+1 ` ` hsn+1 , n+1 , n+1
form shown (7) shorter n + 1 so, inductive hypothesis,
regular derivation PS . Then, (7) regular derivation PS case seqt2 .
(ii) inductive hypothesis, |`1 | 2 lv(S) `1 [0..k], well
|`2 | 2 lv(S) `2 [k + 1..n + 1]. Therefore, |i | 2 lv(S) holds
[0..n + 1].
(iii) inductive hypothesis, =
0 k =
k+1 n+1 . then,

v R k+1 = k implies =
0 n+1 holds.
(Case 3) 6= >r ` [0..n] exists hs` , ` , ` `t2 hs`+1 , `+1 , `+1
s` = fS , k [0..n] exists hsk , k , k `t5 hsk+1 , k+1 , k+1
sk = fS . Then, let k largest indexthat is, assume > k exists
hsm , , `t5 hsm+1 , m+1 , m+1 sm = fS . Then, case (t5)
Definition 4, role S2 level less S, S2 v R, sk+1 = iS2 ,
k+1 = k , k+1 = fS k . next prove properties (i)(iii) hold.
(i) Since sk = fS , inductive hypothesis hs0 , 0 , 0 ` ` hsk , k , k
regular derivation PS . Definition 12, k = 0 . Since sk+1 = iS2
k+1 = fS 0 , Lemma 10, index j [k + 1..n] exists sj = fS2
j = k+1 ; furthermore, sj+1 = fS j+1 = 0 j+1 = j . prove
j + 1 = n + 1. sake contradiction, assume j + 1 < n + 1 consider
form transition hsj+1 , j+1 , j+1 `xj +1 hsj+2 , j+2 , j+2 i. Given
sj+1 = fS 6= >r , must xj+1 {t2 , t5 , p}. initial assumption,
xj+1 6= t2 ; furthermore, maximality k, xj+1 6= t5 ; finally,
since j+1 = 0 = , xj+1 6= p. Thus, j + 1 = n + 1, required. follows
sequence following form, k+1 = k n+1 = n .
, 0 ,
0 ` ` hfS , k , 0 `t5
hiS2 , k+1 , k+1 ` ` hfS2 , n , n `p
hfS , n+1 , 0
Lemma 10, ` [k + 1..n], ` form ` = `00 fS 0 .
00
particular, words k+1
n00 empty. Then, inductive hypothesis,
hiS2 , k+1 , k+1 ` ` hfS2 , n , n regular derivation PS2 .
case seqt5 , (7) regular derivation PS .
(ii) inductive hypothesis, `1 [0..k], |`1 | 2 lv(S). Furthermore, `2 [k + 1..n], |`002 | 2 lv(S2 ). Since lv(S2 ) < lv(S)
`2 = `002 fS , also |`2 | 2 lv(S). Given n+1 = ,
[0..n + 1], |i | 2 lv(S).
(iii) inductive hypothesis, =
0 k S2 =
k+1 n . Given

=
S2 , k+1 = k , n+1 = n , obtain =
0 n+1 .

667

fiStefanoni, Motik, Krotzsch, & Rudolph

(Case 4) 6= >r ` [0..n] exists hs` , ` , ` `x` hs`+1 , `+1 , `+1 i,
s` = fS x` {t2 , t5 }; hs0 , 0 , 0 `t3 hs1 , 1 , 1 i. Then, case (t3) Definition 4, role S1 level less S, S1 v R, s1 = iS1 ,
1 = 0 , 1 = 0 . next prove properties (i)(iii) hold.
(i) Since s1 = iS1 1 = 0 , Lemma 10, j [1..n] exists sj = fS1
j = 1 ; furthermore, sj+1 = j+1 = 0 j+1 = j . Then, sequence
following form, 1 = 0 .
, 0 ,
0 `t3
j `p
1 ` ` hfS1 , j ,
hiS1 , 1 ,
, j+1 , j+1 ` ` hfS , n+1 , n+1
Lemma 10, ` [1..j], ` form ` = `00 0 .
particular, words 100 j00 empty. inductive hypothesis,
hiS1 , 0 , 1 ` ` hfS1 , j , j regular derivation PS1 . Since j+1 = 0 ,
inductive hypothesis, , j+1 , j+1 ` ` hfS , n+1 , n+1
regular derivation PS . case seqt3 , (7) regular derivation PS .
(ii) inductive hypothesis, `2 [j + 1..n + 1], |`2 | 2 lv(S);
furthermore, `1 [1..j], |`001 | 2 lv(S1 ). Since lv(S1 ) < lv(S)
`1 = `001 , also |`1 | 2 lv(S). Finally, since 0 = ,
[0..n + 1], |i | 2 lv(S).
(iii) inductive hypothesis, S1 =
1 j =
j+1 n+1 .
Given =
S1 S, 1 = 0 , j+1 = j , =
0 n+1 .
(Case 5) 6= >r ` [0..n] exists hs` , ` , ` `x` hs`+1 , `+1 , `+1 i,
s` = fS , x` {t2 , t5 }; addition, hs0 , 0 , 0 `x0 hs1 , 1 , 1 x0 6= t3.
next consider remaining possibilities x0 . s0 = , x0 6 {t2 , t5 , u2 , p}
cases (t2), (t5), (u2), (p) Definition 4; furthermore, due 6= >r ,
x0 6 {ur , u1 } cases (ur) (u1) Definition 4. Moreover, assume x0 {r , t1 };
then, s1 = fS 1 = 0 cases (r) (t1) Definition 4; since n > 1
6= >r , possibility hs1 , 1 , 1 `p hs2 , 2 , 2 i, impossible
due 1 = assumption form (7). Hence, remaining possibility
x0 = t4 . case (t4) Definition 4, roles S1 S2 level less
S, S1 S2 v R, s1 = iS1 , 1 = 0 , 1 = iS2 fS 0 . next prove
properties (i)(iii) hold.
(i) Since s1 = iS1 1 = iS2 fS 0 , Lemma 10, j1 [1..n] exists sj1 = fS1
j1 = 1 ; furthermore, sj1 +1 = iS2 j1 +1 = fS 0 j1 +1 = j1 . Again,
Lemma 10, j2 [j1 + 1..n] exists sj2 = fS2 j2 = j1 +1 ; furthermore,
sj2 +1 = fS j2 +1 = 0 j2 +1 = j2 . Next, prove j2 + 1 = n + 1.
sake contradiction, suppose j2 + 1 < n + 1 consider form
hsj2 +1 , j2 +1 , j2 +1 `xj2 +1 hsj2 +2 , j2 +2 , j2 +2 i. Given sj2 +1 = fS ,
must xj2 +1 {t2 , t5 , u2 , p}. However, assumed xj2 +1 6 {t2 , t5 }
6= >r , xj2 +1 6= u2 ; finally, since j2 +1 = 0 = , xj2 +1 6= p.
668

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

Therefore, j2 + 1 = n + 1, required, sequence following
form, 1 = 0 , j1 +1 = j , n+1 = n .
, 0 ,
0 `t4
hiS1 , 1 ,
1 ` ` hfS1 , j1 , j1 `p
hiS2 , j1 +1 , j1 +1 ` ` hfS2 , n , n `p
hfS , n+1 , n+1
Lemma 10, `1 [1..j1 ], word `1 form `1 = `001 iS2 fS 0 .
particular, words 100 j001 empty. Then, inductive hypothesis,
hiS1 , 1 , 1 ` ` hfS1 , j1 , j1 regular derivation PS1 .
Similarly, Lemma 10, `2 [j1 + 1..n], `2 form
`2 = `002 fS 0 . particular, words j001 +1 n00 empty. Then,
inductive hypothesis, hiS2 , j1 +1 , j1 +1 ` ` hfS2 , n , n
regular derivation PS2 . case seqt4 , (7) regular derivation PS .
(ii) inductive hypothesis, `1 [1..j1 ], |`001 | 2 lv(S1 ). Since
lv(S1 ) < lv(S) `1 = `001 iS2 fS , also |`1 | 2 lv(S). Similarly,
inductive hypothesis, `2 [j1 + 1..n], |`002 | 2 lv(S2 ). Since
lv(S2 ) < lv(S) `2 = `002 fS , also |`2 | 2 lv(S). Since 0 = ,
[0..n + 1], |i | 2 lv(S).
(iii) inductive hypothesis, S1 =
1 j1 S2 =
j1 +1 n . Given

=
S1 S2 , 1 = 0 , n+1 = n , conclude =
0 n+1 .
possibilities form (7), claim lemma holds
derivation form.
finally ready show PDA PS satisfies properties (1) (2).
Lemma 13. role R role chain ,
1. L(PS ) implies L(S),
2. PS stack bounded 2 lv(S) + 1.
Proof. definition PS , transitions resulting case p Definition 4
ones popping elements stack, never pop symbol ; hence, point
accepting derivation PS , stack content form := i0 . Then,
two claims follow immediately Lemma 12.
3.1.2 Completeness
next prove encoding also complete, thus proving Theorem 8.
Lemma 14. role R role chain , L(S) implies
L(PS ).

669

fiStefanoni, Motik, Krotzsch, & Rudolph

Axiom Type
(t1)
vT
(t2)

(t3)

(t4)

(t5)

Derivation
`t1 hfT ,

00

hiT ,

,

vT

hiT ,
hiT ,

00 ,
00 ,

T1 v

hiT , T1 00 ,
hiT1 , T1 00 ,
hiT ,
00 ,

T1 T2 v

hiT , T1 T2 00 ,
hiT1 , T1 T2 00 , iT2 fT
hiT2 ,
T2 00 ,
fT
00
hfT ,
,

T2 v

hiT , T2 00 ,
hiT2 ,
T2 00 ,
hfT ,
00 ,




`r
`r

`t3
` r
`r
`t4
` r
` r


`r
fT ` r


00 ,



hfT , 00 ,
hfT ,
00 ,




`t2

hfT1 , 00 ,
hfT ,
00 ,




`p

hfT1 , T2 00 , iT2 fT `p
hfT2 ,
00 ,
fT ` p
hfT , T2 00 ,
hfT2 ,
00 ,

`t5
fT ` p

Table 4: Definition derivation (9) depending form axiom v .
Proof. Consider arbitrary role R . following, role chain , write
0

=
= S; furthermore, N+ , write =
role chains 1 , . . . ,
exist =
1 =
=
= . definition L(S),

L(S) natural number N exists =
. induction

N, next show =
implies L(PS ).
0

Base case. Let = 0. Then, =
S. consider two cases depending
form role R .
= >r . case (ur) Definition 4, hi>r , >r , `ur hf>r , , i.
R \ {>r }. case (r) Definition 4, , S, `r hfS , , i.
either case L(PS ), required.
Inductive step. Consider arbitrary N assume that, role chain 0

=
0 , 0 L(PS ); show holds + 1. Then,
consider arbitrary role chains 1 , . . . , m+1 =
1 =
=
=
m+1 .
definition relation =
, role R role chains 0 , , 00 exist role chain
form := 0 00 , role chain m+1 form m+1 := 0 00 ,

=
. Since =
0 00 , inductive hypothesis, 0 00 L(PS ),
sequence hs0 , 0 , 0 ` ` hsn , n , n PS exists s0 = sn = fS ; furthermore,
0 = n = ; finally, 0 = 0 00 n = . exists index
[0..n 1] = 00 i+1 = 00 . Furthermore, j [0..i], role chain


00
j form j :=
j role chain j R . Next, consider form
xi hsi , , `xi hsi+1 , i+1 , i+1 i. Definition 4, transitions cases (r) (ur)
read symbols input, xi {r, ur}. show lemma holds case.
670

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

(Case 1) Consider case xi = r . Then, si = si+1 = fT ,
= i+1 , R \ {>r }. Due =
6= >r , v R. Then,
following also derivation PS

00
00

hs0 ,
0 , 0 ` ` hsi , , `

(8)



(9)

[The derivation Table 4 v ] `
00

hsi+1 , , i+1 ` ` hsn , , n

(10)

derivation (9) defined Table 4 depending form axiom v R.
(Case 2) Consider case xi = ur . Then, si = i>r si+1 = f>r ,
= i+1 R . Then, following also derivation PS

00
00

hs0 ,
0 , 0 ` ` hsi , , `
00

[The derivation seq(, , ) (14)] `



00

hsi+1 , , i+1 ` ` hsn , , n
derivation seq(, 0 , ) (12) inductively defined follows.
(
hi>r , 00 , `u1 hf>r , 00 ,
= ,
00
seq(, , ) :=
00
00
00
hi>r , , `ur hf>r , , `u2 seq(, , ) = P .

(11)
(12)
(13)

(14)

Therefore, either case, 0 00 L(PS ), required.

4. Polynomial Space BCQ Answering Algorithm ELRO+
ELRO+ knowledge base K translated set first-order Horn clauses,
Boolean CQ q K answered evaluating q so-called canonical model
model homomorphically embedded model K. Canonical
models usually obtained using chase. Many different chase variants studied
literature, producing different, homomorphically equivalent, canonical
model (Johnson & Klug, 1984; Marnette, 2009; Cal, Gottlob, & Kifer, 2013; Baget, Leclere,
Mugnier, & Salvat, 2011). paper, introduce variant call consequencebased chase, (possibly infinite) set assertions IK produces K call
universal interpretation K. compute IK , consequence-based chase initialises IK
contain ABox K, well assertions {a}(a), >c (a), >r (a, b) individuals
b occurring K; then, iteratively extends IK using chase rules. slightly unusual
aspect chase variant considers axioms entailed (and contained
in) K. example, IK point contains assertion A(w) K |= v S.B holds,
IK extended assertions S(w, w0 ) B(w0 ) w0 fresh term; term w0
said auxiliary type S, B concept type B. BCQ answering algorithm
present section based checking consequences K, chase variant
makes proofs simpler. Example 15 illustrates aspects.
Example 15. Let K = hT , R, ELRO+ KB, contains axioms (15)(21),
R contains role inclusion (22).
{a} v S.A
671

(15)

fiStefanoni, Motik, Krotzsch, & Rudolph



b





1 S,

2 S,






3 T, B

R



b

6 S, B







4 S, C

oS,A







oT,B

oS,B

R




oS,C



S,

S,

5
T, B

Universal Interpretation

Compact Interpretation

Figure 4: universal interpretation compact interpretation K
v S.A

(16)

{b} v S.A

(17)

{b} v T.B

(18)

{b} v S.C

(19)

C v T.B

(20)

C v R.{b}

(21)

ST vS

(22)

Figure 4 shows universal interpretation IK K. Assertions involving >c >r
shown clarity. edges obtained via role inclusions dashed; remaining edges
solid, apart dotted edges, denote repetition solid edges. Black edges
obtained using conventional chase variants, whereas light grey subbranches IK
caused axioms entailed by, occurring in, K. Auxiliary terms labelled
using integers, terms type shown next term. Universal interpretation IK
viewed family directed trees whose roots individuals K
solid edges point parents children individuals K. Axiom (16) makes IK
infinite, decision procedure BCQ answering cannot simply materialise IK
evaluate query it; instead, finitary representation IK needed.
axioms (19), (20), (22), K |= {b} v S.B; then, since {b}(b) IK ,
consequence-based chase ensures {S(b, 6), B(6)} IK holds well. contrast,
commonly considered chase variants ensure {S(b, 6), B(6)} IK K
contain axiom {b} v S.B.
rest section, present first worst-case optimal algorithm decides
K |= q given arbitrary regular ELRO+ KB K Boolean CQ q K. Towards
goal, Section 4.1 review existing approaches answering CQs DLs discuss
672

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

techniques provide optimal procedure ELRO+ ; Section 4.2
discuss intuitions behind algorithm; Section 4.3 introduce algorithm
formally show runs polynomial space combined size K q
polynomial time size K; Section 4.4 prove algorithms correctness.
4.1 Existing Approaches Answering CQs
Techniques answering conjunctive queries DL knowledge bases developed thus far
broadly classified following three groups.
first group consists automata-based approaches DLs Horn-SHIQ
Horn-SROIQ (Ortiz et al., 2011), SH (Eiter, Ortiz, & Simkus, 2012a), fragment ELRO+ obtained disallowing universal role, reflexive roles, self restrictions (Krotzsch et al., 2007). techniques, however, require constructing automata
whose size exponential size knowledge base.
second group consists rewriting-based approaches. Roughly speaking, approaches rewrite query and/or TBox another formalism, usually union
CQs datalog program; relevant answers obtained evaluating
rewriting ABox. Rewriting-based approaches proposed members
DL-Lite family (Artale et al., 2009; Calvanese et al., 2007), DLs EL (Rosati,
2007), ELHIO (Perez-Urbina et al., 2010; Mora, Rosati, & Corcho, 2014) HornSHIQ (Eiter, Ortiz, Simkus, Tran, & Xiao, 2012b), members datalog family (Virgilio, Orsi, Tanca, & Torlone, 2012), name few. rewriting approach,
however, supports nominals role inclusions. Moreover, common shortcoming
rewritings exponential query and/or TBox size, approaches
may also use exponential space.
third group consists approaches based particular interpretation K
call compact interpretation. Figure 4 shows interpretation KB K
Example 15: finitely approximates universal interpretation using individuals
form oS,B represent auxiliary terms type S, B. compact interpretation thus
materialised space polynomial |K|, used answer instance queries
test atomic subsumptions K (Baader et al., 2005; Krotzsch, 2011). Materialising
compact interpretation lies core many reasoning algorithms EL variants,
natural try use interpretation answering CQs well. Since compact
interpretation model K, CQ maps universal interpretation maps
compact interpretation well; however, Example 16 shows, converse
necessarily hold.
Example 16. Let K Example 15, let q1 , q2 , q3 following BCQs.
q1 = x. R(x, b)

q2 = x. S(a, x) S(b, x)

q3 = x. (b, x) S(b, x)

compact interpretation K shown Figure 4; one see, obtained
universal interpretation merging terms type S, B onto individual oS,B .
query q1 mapped onto compact universal interpretation, queries
q2 q3 mapped onto compact interpretation. Thus, evaluating q2 q3
compact interpretation produces unsound answers.
673

fiStefanoni, Motik, Krotzsch, & Rudolph



b

{a}

b {b}

Ax

B

R
R,D 1



T,B
3




S,B 5


R





R,A
2






4 T,B






T,B
6

P





T,B



7


8 S,B





P,B 9






T,B 10

Universal Interpretation


11 S,B



z B

Skeleton q

Figure 5: universal interpretation K skeleton q
remedy, combined approaches developed first evaluate query
compact interpretation filter results eliminate unsound answers.
approaches developed members DL-Lite family (Kontchakov et al.,
2011; Lutz, Seylan, Toman, & Wolter, 2013) EL family (Lutz, Toman, & Wolter,
2009; Stefanoni et al., 2013) DLs, datalog family (Gottlob, Manna, & Pieris,
2014) rule-based languages. particular, Stefanoni et al. (2013) developed filtering
step applicable DL ELHOdr
, step breaks K contains role inclusions.
Query q3 Example 16 mapped onto compact interpretation mapping
atom S(b, x) dashed edge (i.e., edge obtained via role inclusions); moreover, q3
tree-shaped, filtering step Stefanoni et al. (2013) identify
match unsound. problem intuitively understood follows. unfolding
query (22), query q3 essentially asks whether role chains 1 L(S) 2 L(T )
exist label path solid edges IK starting b. compact interpretation,
satisfied 1 = 2 = x mapped individual oT,B . Individual
oT,B , however, represents distinct terms 3 5 IK ; hence, although 3 connected
b via 1 5 connected b via 2 , role chains 1 2 satisfy query q3 .
words, compact interpretation small represent relevant conditions.
4.2 Intuitions
worst-case optimal procedure BCQ answering ELRO+ shown Algorithm 1
page 681. essentially extends refines algorithm Krotzsch et al. (2007).
explain underlying intuitions using knowledge base shown Example 17.
Example 17. Let K ELRO+ knowledge base whose TBox contains axioms (23)(29)
whose RBox contains role inclusions (30)(31).
{a} v R.A
v T.{b}
674

(23)
(24)

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs



S, X/X

fS

, X/fS X



T, X/X

fT

, X/X

Figure 6: transitions R corresponding axioms (30)(31)
AvD

(25)

{b} v T.B

(26)

{b} v P.B

(27)

B v S.Self

(28)

B v T.B

(29)

vT

(30)

ST vS

(31)

Moreover, let q following Boolean CQ K.
q = x, y, z. D(x) (x, z) S(y, z)

(32)

Figure 5 shows universal interpretation IK K; notation Example 15.
solid looping edges auxiliary terms concept type B obtained axiom (28). One
see K |= q holds; example, following substitution embeds q IK .
= {x 7 2, 7 6, z 7 7}

(33)

algorithm uses PDA encoding RBox described Section 3. transition
function R axioms (30)(31) shown Figure 6; notation Example 6;
note Figure 6 contained Figure 3.
must prove existence substitution mapping q IK . substitution
map binary atoms q dashed edges Figure 5. Dashed edges introduce
shortcuts terms IK , dashed edge unfolded path consisting
solid edges using role inclusions K. solid paths IK two
types: auxiliary paths involve auxiliary terms, whereas nominal paths require moving
least one individual. instance, edge (2, 7) unfolded path
= connecting 2 b, b 6, 6 7. contrast, edge S(6, 7)
unfolded path = connecting 6 itself, 6 7. algorithm
uses PDAs transition function Figure 6 represent binary atom
q sequence binary atoms mapped corresponding solid path IK .
Interpretation IK , however, infinite, space possible substitutions also infinite.
Hence, prove existence substitution mapping q IK , cannot simply
enumerate them, use Algorithm 1 instead.
675

fiStefanoni, Motik, Krotzsch, & Rudolph

line 1 check whether K unsatisfiable; so, K |= q holds trivially. Next,
line 2 guess substitution continue checking K |= (q); thus, step takes
account variables could mapped individuals, two variables could
mapped term. example, guess identity mapping
~y . step 3, guess finite structure, called skeleton (q), represents
(possibly infinite) set substitutions mapping variables (q) distinct auxiliary
terms IK . Figure 5 shows skeleton query Example 17: skeleton vertices
individuals K variables (q), arranged forest whose
roots individuals; moreover, vertex v assigned atomic concept (v).
step, skeleton represents substitution (if any) satisfying following
two properties:
1. maps variable x term concept type (x),
2. edge hv, v 0 S, (v 0 ) descendant (v) IK .
next extend conditions prune set substitutions, goal
leaving substitutions compatible (q)that is, embed (q) IK .
establish compatibility unary atoms (q) line 4. particular, consider
atom D(x) (q). property (1), substitution represented skeleton
Figure 5 maps variable x term concept type (x) = A, implying A( (x)) IK
holds. then, since K |= (x) v holds, know D( (x)) IK holds well; thus,
atom D(x) satisfied substitution represented S.
contrast, cannot establish compatibility binary query atoms using entailment
checking only, vertex labels relative position vertices sufficiently
describe substitutions. example, substitution
1 = {x 7 2, 7 9, z 7 10}

(34)

satisfies properties (1) (2), (1 (x), 1 (z)) 6 IK .
prune substitutions, lines 516 Algorithm 1 guess binary atom
(q) unfold sequence solid steps IK . solid paths IK go
nominals auxiliary terms only, two possibilities accounted
guessing line 8. Moreover, skeleton already constrains relative positions
query terms, represent unfolding binary atom labelling edge hv, v 0
set L(v, v 0 ) bounded-stack PDAs transition function Figure 6
substitution represented also satisfies following property:
3. PDA P L(v, v 0 ), nonempty role chain L(P) exists labelling path
IK solid edges (v) (v 0 ).
next illustrate label edges substitutions satisfying properties
(1)(3) compatible binary atom (q).
(x, z), must ensure that, substitution represented S, role chain
L(T ) exists connecting (x) (z) using solid edges. Since relative positions
(x) (z) IK determined shown Figure 5, path must connect
(x) b, connect b (y), finally connect (y) (z). addition,
assume individuals occur paths b (y), (y) (z):
676

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

>r


R

{a}

R

B




S,

T, P


{b}

start

B



start

Figure 7: left-hand side, walking finite automaton wfa(A, {b}) (transitions involving >c >r shown clarity); right-hand side, stationary finite automaton sfa(B)

path (x) (z) involves individuals b visits b once,
absorb path segments subpath (x) b. Thus, check
existence setting v0 = au = b lines 78 (no guessing possible line 8
case) splitting lines 711 path three subpaths. particular,
line 10 guess states s0 , s1 , s2 = fT line 11 guess stack words 0 , 1 ,
2 = following properties:
(i) subpaths (x) b described PDA PT0 whose start state stack
, respectively, final state stack s0 0 , respectively;
(ii) subpaths b (y) described PDA PT1 whose start state stack
s0 0 , respectively, final state stack s1 1 , respectively;
(iii) subpaths (y) (z) described PDA PT2 whose start state stack
s1 1 , respectively, final state stack s2 2 , respectively.
know terms IK variables z mapped to, cannot
check existence paths (ii) (iii) independently. Therefore, add line 12
PDAs PT1 PT2 constraints edges hb, yi hy, zi S, respectively. edges
thus accumulate constraints moves auxiliary terms must satisfy; later
shall explain lines 1718 check constraints and, check passes,
know map z auxiliary terms whose concept types (y)
(z), respectively. contrast, path (i) finishes individual,
check existence path independently constraint. end,
construct walking finite automaton wfa(A, {b}) shown left-hand side Figure
7. wfa(A, {b}) describes moves IK terms concept type (x) =
individual bthat is, L(wfa(A, {b})) term w concept type
role chain connecting w b IK via solid edges; then, line 14 check
whether intersection languages wfa(A, {b}) PT0 empty. wfa(A, {b})
FA PT0 PDA, test emptiness intersection languages
polynomial time (Hopcroft et al., 2003). example, guess s0 = s1 = s2 = fT .
677

fiStefanoni, Motik, Krotzsch, & Rudolph

Thus, PT1 accepts language , b connected 1 (y) solid edge
labelled P , adding PT1 constraint edge hb, yi ensures substitution 1 (34)
satisfy property (3).
S(y, z), must ensure that, substitution represented S, role chain
L(S) exists connecting (y) (z) using solid edges. even though z
descendant S, line 8 could guess v0 = au = b, connects (y) b
then, without going individuals, connects b (z) via (y). rest
paragraph, however, consider case connects (y) (z) directly,
since possibility example, one see Figure 5. Therefore,
line 8, guess v0 = = y. then, path (y) (z) could first loop (y)
due self-restrictions; must actually move (y) (z); finally could
loop (z). reasons discuss following paragraph, absorb latter loop
constraint added edge hy, zi; however, check existence former loop
independently. Therefore, lines 711 split two subpaths. particular,
line 10 guess states s0 s1 = fS , line 11 guess stack words 0 1 =
following properties:
(i) looping (y) described PDA PS0 whose start state stack
, respectively, final state stack s0 0 , respectively;
(ii) subpaths (y) (z) start move (y), possibly involve
looping (z), described PDA PS1 whose start state stack s0 0 ,
respectively, final state stack s1 1 , respectively.
previous case, check (ii) adding PS1 constraint edge hy, zi S.
Furthermore, check existence path (i) constructing stationary finite
automaton sfa(B) shown right-hand side Figure 7. sfa(B) describes
possible loops terms concept type (y) = B; is, L(sfa(B))
term w concept type B role chain corresponding (possibly
empty) loop w; then, line 16 check whether intersection languages
sfa(B) PS0 empty. example, guess s0 = s1 = fS ; thus PS0 accepts
language , whereas PS1 accepts language .
line 17, skeleton represents substitutions compatible
atoms (q), must still show least one substitution realised
universal interpretation IK . end, apply Algorithm 2 page 681
edge hv, v 0 skeleton, thus check whether terms (v) (v 0 ) IK exist
satisfy properties (1)(3) PDAs L(v, v 0 ). Roughly speaking, solve
problem running PDAs parallel lines 715 Algorithm 2. However, cannot
materialise IK , exploit property consequence-based chase procedure: term
w concept type connected term w0 concept type B IK using solid
edge labelled K |= v S.B. Furthermore, concept type w fully
describes solid paths descendants w, need keep track actual
position IK ; instead, use variable concept keep track current terms concept
type. Thus, line 9 check existence edges IK via entailment checking; that,
PDA, line 11 guess state stack PDA, line 12 check
whether PDA perform move, line 14 actually move PDA. Due
678

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

self-restrictions reflexive roles, however, PDAs need move synchrony:
move solid edge, PDAs independently loop current term.
end, line 11 guess state s0 stack 0 PDA moves
looping, line 13 check whether PDA move state stack
state s0 stack 0 using role chain compatible concept type term
PDA moving into, given stationary finite automaton sfa(D).
PDAs required loop, FA sfa(D) accepts empty word. Algorithm 2 thus checks
loops move, line 16 Algorithm 1 necessary. Lines 25
take account PDAs nondeterministic initially make
several -transitions; note explicit check -transitions required initially
since line 13 allows possible -transitions move along solid edge. Finally,
ensure termination Algorithm 2 observing that, since stack PDA L(v, v 0 )
bounded, number current configurations PDA exponential,
number distinct tuples current PDA configurations exponential well;
hence, algorithm repeats computations exponentially many steps.
thus obtain nondeterministic decision procedure running polynomial space using
binary counter stop computation distinct configurations explored.
constraints added previous paragraphs, one check Algorithm 2
returns true edges S; hence, K |= (q) holds, thus K |= q holds well.
4.3 Formalisation
formalise intuitions previous section. Towards goal, fix
normalised ELRO+ KB K = hT , R, Ai regular RBox R, let QR , R , R
specified Definition 4, let dR depth R specified Section 3.
start formalising notion skeleton Boolean CQ.
Definition 18. skeleton Boolean CQ q = ~y . (~y ) triple = hV, E,
following components.
1. V = IK ~y set vertices.
2. E V ~y set edges directed graph hV, Ei forest whose
roots precisely elements IK .
3. : ~y 7 {>c } CK function maps existential variables q atomic
concepts. convenience, extended V (v) := {v} v IK .
path nonempty sequence (distinct) vertices v0 , . . . , vn n 0 and,
[0..n 1], hvi , vi+1 E.
Please observe that, K normal form, exists least one individual occurring
K thus V 6= . next generalise notion PDA encoding RBox R
Definition 4 allowing arbitrary start final states well arbitrary start
final stacks size dR . generalised PDA used algorithm
implement splitting operation mentioned Section 4.2.
Definition 19. states s, s0 QR words , 0 R || dR | 0 | dR ,
generalised PDA R given pda(s, , s0 , 0 ) := hQR , R , R , R , s, , s0 , 0 i.
679

fiStefanoni, Motik, Krotzsch, & Rudolph

following definition introduces automata one use succinctly represent
axioms logically follow K.
Definition 20. Let B basic concepts. walking finite automaton
B given wfa(A, B) := hQ, R , w , A, Bi Q w follows.
Q = {>c } CK NK .
w transition function containing w (C, S) role R
states C Q K |= C v S.D.
stationary finite automaton given sfa(A) := h{A}, R , , A, Ai
contains (A, S) role R K |= v S.Self K |= v S.
Boolean CQs answered nondeterministic procedure entails shown Algorithm 1, uses auxiliary procedure exist shown Algorithm 2. following
theorem states entails(K, q) decides K |= q, proof given Section 4.4.
Theorem 21. Let q Boolean CQ K. Then, K |= q nondeterministic
computation exists entails(K, q) returns true.
Finally, determine complexity algorithm entails, towards goal
first determine complexity auxiliary function exist.
Lemma 22. Function exist(A, B, {Pj = pda(sj , j , s0j , j0 ) | 0 j m}) implemented uses space polynomial |K| and, RBox R fixed, runs
time polynomial |T | + |A|.
Proof. Consider arbitrary A, B, Pj = pda(sj , j , s0j , j0 ) stated above; let
Algorithm 2; let ` derivation relation corresponding R . definition
generalised PDAs, |j | dR |j0 | dR j [1..m].
Proposition 2, using polynomial time one compute PDAs accepting languages
LdR (pda(sj , j , s, )) LdR (pda(s, , s0 , 0 )) lines 4 13; therefore, checks lines
4 13 implemented use time (and therefore space) polynomial
|K| (Hopcroft et al., 2003, ch. 7).
space usage Algorithm 2, please observe function stores following
information computation step:
(a) array state length state[j] QR j [1..m], array stack
length stack[j] R |stack[j]| dR j [1..m],
(b) generalised PDA line 4,
(c) generalised PDA stationary automaton line 14,
(d) concept concept {A, >c } CK line 1,
(e) binary counter k 1 k ,
(f) depth dR R, atomic concept {>c } CK , role R .
680

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

Algorithm 1: entails(K, q)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

K inconsistent return true
guess substitution dom() = ~y rng() ~y IK
guess skeleton = hV, E, (q)
atom A(t) (q) exists K 6|= (t) v return false
foreach hv, v 0 E let L(v, v 0 ) :=
foreach binary atom S(t, u) (q)
let au unique individual u reachable au
guess v0 {t, au } u reachable v0
let v0 , . . . , vn unique path vn = u
guess states s0 , . . . , sn QR sn = fS
guess words 0 , . . . .n R n = |i | dR [0..n]
foreach [1..n] let L(vi1 , vi ) := L(vi1 , vi ) {pda(si1 , i1 , si , )}
v0
L(wfa((t), (v0 ))) L(pda(iS , , s0 , 0 )) = return false
else
L(sfa((v0 ))) L(pda(iS , , s0 , 0 )) = return false
foreach hv, v 0 E
exist((v), (v 0 ), L(v, v 0 )) return false
return true

Algorithm 2: exist(A, B, {Pj = pda(sj , j , s0j , j0 ) | 0 j m})
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

let concept := let := (1 + |CK |) |QR |m (|R |1+dR )m
j = 1
guess state QR word R || dR
6 LdR (pda(sj , j , s, )) return false
set state[j] := stack[j] :=
guess k N 1 k
r = 1 k
guess R {>c } CK
K 6|= concept v S.D, K |= v {a} IK return false
j = 1
guess {s, s0 } QR {, 0 } R || dR | 0 | dR
hstate[j], S, stack[j]i 6` hs, , return false
L(sfa(D)) LdR (pda(s, , s0 , 0 )) = return false
set state[j] := s0 stack[j] := 0
set concept :=
concept 6= B return false
exists index j [1..m] state[j] 6= s0j stack[j] 6= j0
return false
return true

681

fiStefanoni, Motik, Krotzsch, & Rudolph

definition dR , dR linearly bounded number axioms occurring R; hence, need O(m |R|) space store two arrays. Furthermore,
need O(m |K|) space store counter k using binary encoding. Definition
20, size sfa(D) polynomial |K|; Definition 4, size pda(s, , s0 , 0 ) polynomial |R|. Overall, space needed store required information polynomial
|K|. Finally, following Krotzsch (2011), realise check step 9 polynomial
time. Thus, exist implemented uses space polynomial |K|.
Next, assume RBox R fixed. dR , QR , R , R fixed
well; moreover, bounded size R fixed, linear
size A. Thus, number alternatives nondeterministic step line 3
Algorithm 2 fixed, lines 15 require time polynomial |T | + |A|. Furthermore,
instead guessing k using nondeterministic step 6, repeat lines 715
k [1..M ], requires linear number iterations. show lines 715 also
implemented run polynomial time, first define three sets used
perform checks lines 9, 12, 13.
{hS, C, Di R (CK NK {>c })2 | K |= C v S.D IK : K 6|= v {a}} (35)
{hS, pda(s, , s0 , 0 )i | R hs, S, ` hs0 , , 0 i} (36)
{hC, pda(s, , s0 , 0 )i | C CK {>c } L(sfa(C)) LdR (pda(s, , s0 , 0 )) 6= } (37)
Given R fixed, sets computed time polynomial size
A. next show implement for-loop steps 715 use space logarithmic
size , A, sets equations (35)(37). space usage lines 715,
computation step for-loop store information points (a)(f) above.
Since R fixed, however, points (a)(c) require constant space. Furthermore,
checks lines 9, 12, 13 performed lookup sets (35)(37); storing
sets using suitable binary encoding using binary index sets,
check implemented using logarithmic space. Finally, CK linear
size A, store counter k, concepts concept, role using binary
encoding, overall space function needs store logarithmic |T | + |A|,
size sets (35)(37). Thus, steps 715 require nondeterministic logarithmic space,
well known implies steps 715 implemented run polynomial
time. Finally, steps 1619 clearly require polynomial time. Consequently, function exist
implemented runs time polynomial |T | + |A| fixed R.
ready establish complexity function entails(K, q); Section 5
shall show function worst-case optimal combined data complexities.
Theorem 23. q BCQ K, function entails(K, q) implemented
1. uses space polynomial input size,
2. RBox R fixed, runs nondeterministic polynomial time size
TBox , ABox A, query q,
3. RBox R query q fixed, runs (deterministic) polynomial time
size TBox ABox A.
682

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

Proof. Let q = ~y .(~y ) Boolean CQ K.
shown Proposition 2, one compute lines 14 16 PDA accepting language
LdR (pda(iS , , s0 , 0 )) polynomial time, checks lines 14 16 require time
(and therefore space) polynomial |K| (Hopcroft et al., 2003, ch. 7). Moreover, checks
lines 1 4 also require time polynomial |K| (Krotzsch, 2011).
(1), please observe function entails specified Algorithm 1 stores
following information computation step:
substitution dom() = ~y rng() ~y IK ;
skeleton = hV, E, (q);
path v0 , . . . , vn S, sequence states s0 , . . . , sn QR , sequence words
0 , . . . , n R |i | dR [0..n];
function L mapping edge hv, v 0 E set generalised PDA; and,
walking automaton wfa((t), (v0 )) stationary automaton sfa((v0 )).
definition skeleton (q), need space polynomial size q K
store S. Moreover, length longest path given number variables
occurring (q), store sequences vertices, states, words space
polynomial |q| |K| well. Also, set L(v, v 0 ) contains PDAs,
number binary atoms occurring (q). Then, Lemma 22, entails
implemented uses space polynomial input size.
(2), assume RBox R fixed. Lemma 22, fixed RBox R, step 18
implemented runs time polynomial |T | + |A|. Clearly, steps
Algorithm 1 implemented run nondeterministic polynomial time size
TBox , ABox A, query q. Consequently, fixed RBox R, function entails
implemented runs nondeterministic polynomial time size TBox
, ABox A, query q.
(3), assume RBox R query q fixed. dR , QR , R , R
fixed well. Given number variables occurring q fixed, number
guessing steps required steps 2 3 fixed; also, number alternatives
steps linear |T | + |A|. Thus, steps 2 3 require polynomial time. Furthermore,
maximum number iterations for-loop steps 616 fixed length
longest path fixed. Thus, number guessing steps lines 11 12 also
fixed. addition, number alternatives guessing steps lines 8, 11, 12
fixed well. Therefore, steps 616 require time polynomial |T | + |A|. Finally, since
query fixed, maximum number iterations for-loop steps 17 18
also fixed so, Lemma 22, steps 17 18 require time polynomial A.
Therefore, entails implemented runs time polynomial |T | + |A|
fixed R q.

683

fiStefanoni, Motik, Krotzsch, & Rudolph

4.4 Proof Theorem 21
prove function entails(K, q) indeed decides K |= q. Towards goal,
start proving correctness function exist, introduce universal
interpretation K, and, finally, show entails sound complete.
4.4.1 Correctness exist
following proposition proves correctness function exist Algorithm 2.
Lemma 24. Function exist(A, B, {Pj = pda(sj , j , s0j , j0 ) | 0 j m}) returns true
exist natural number k 1, roles S1 , . . . , Sk , basic concepts A0 , . . . , Ak
A0 = Ak = B, role chains {j,i | j [1..m] [1..k]}
following conditions hold [1..k] j [1..m].
1. IK , K |= Ai1 v Si .Ai K 6|= Ai v {a}.
2. role occurring j,i , K |= Ai v T.Self K |= v .
3. S1 j,1 Sk j,k LdR (Pj ).
Proof. Consider arbitrary A, B, Pj = pda(sj , j , s0j , j0 ) stated lemma. Moreover, let ` derivation relation corresponding R .
() Assume nondeterministic computation exist function
returns true. Let k N guessed step 6; show for-loop steps 715
satisfies following invariant: iteration r, exist roles S1 , . . . , Sr , basic
concepts A0 , . . . , Ar , and, j [1..m], role chains j,1 , . . . , j,r A0 = A,
Ar = concept, following holds [1..r] j [1..m].
(i) K |= Ai1 v Si .Ai and, IK , K 6|= Ai v {a}.
(ii) role occurring j,i , K |= Ai v T.Self K |= v .
(iii) S1 j,1 Sr j,r LdR (pda(sj , j , state[j], stack[j])).
Base case. first iteration loop (i.e., steps 15 r = 0),
concept = A, LdR (pda(sj , j , state[j], stack[j]) j [1..m],
properties (i)(iii) clearly hold.
Inductive step. Consider arbitrary iteration r [1..k 1] assume properties
(i)(iii) hold end iteration r; show true iteration r + 1.
inductive hypothesis, exist roles S1 , . . . , Sr , basic concepts A0 , . . . , Ar , and,
j [1..m], role chains j,1 , . . . , j,r A0 = A, Ar = concept properties
(i)(iii) hold. Let role Sr+1 = atomic concept Ar+1 = guessed step 8.
Clearly, K |= concept v Sr+1 .Ar+1 K 6|= Ar+1 v {a} IK ,
required property (i). Furthermore, consider arbitrary j [1..m], let s, s0 , ,
0 guessed step 11, let state[j] stack[j] end iteration r;
then, hstate[j], Sr+1 , stack[j]i ` hs, , due step 12; furthermore, due step 13, role
chain j,r+1 exists j,r+1 L(sfa(D)) LdR (pda(s, , s0 , 0 )). Definition 20
stationary automata, role occurring j,r+1 K |= v T.Self
684

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

K |= v , required property (ii). Finally, let state[j] stack[j] specified
step 14; S1 j,1 Sr j,r Sr+1 j,r+1 LdR (pda(sj , j , state[j], stack[j])),
property (iii) holds.
Step 16 ensures concept = B; furthermore, steps 1718 ensure state[j] = s0j
stack[j] = j0 j [1..m], PDA Pj accepts S1 j,1 Sk j,k . Thus, properties
(1)(3) lemma hold, required.
() Let S1 , . . . , Sn roles, let A0 , . . . , basic concepts A0 = = B,
let j,i role chains satisfying properties (1)(3) lemma. derivation
S1 j,1 Sn j,n PDA Pj following form, sj,n+1 = s0j j,n+1 = j0 :
hsj , S1 j,1 Sn j,n , j `
hs0j,1 ,

hsj,1 , S1 j,1 Sn j,n , j,1 `

j,1 S2 Sn j,n ,

hsj,2 , S2 j,2 Sn j,n , j,2 `

0
j,1


(39)



(40)



(41)

. . . `

(42)

`

... `
hs0j,i ,

hsj,i , Si j,i Sn j,n , j,i `

j,i Si+1 Sn j,n ,

hsj,i+1 , Si+1 j,i Sn j,n , j,i+1 `
hs0j,n ,

hsj,n , Sn j,n , j,n `
hsj,n+1 , , j,n+1

j,n ,

(38)



0
j,i

0
j,n


`



`

(43)
(44)

Transition (38) (39) special sense allows Pj make arbitrary
number -transitions; rest derivation regular consists reading Si
j,i . Thus, sj,i s0j,i states Pj after, respectively, reading Si ,
0 respective stacks. property (3) lemma, | |
j,i j,i
j,i
R
0
|j,i | dR .
Let Xi = hAi , s1,i , 1,i , . . . , sm,i , m,i i. PDA, |QR | many different
P R
|R |` many difstates, 1+|CK | different elements {>c }CK ; furthermore, d`=0
PdR
ferent stacks length dR . |R | > 0 dR > 0, `=0 |R |` |R |1+dR ;
consequently, distinct tuples. Thus, k ,
Xk = Xn+1 . then, Ak = B; furthermore, j [1..m], sj,k = sj,n+1 = s0j
j,k = j,n+1 = j0 , S1 j,1 Sk j,k LdR (Pj ).
easily construct nondeterministic computation exist follows.
step 3, j let = sj,1 = j,1 ; clearly, condition step 4 satisfied.
r for-loop lines 715, proceed follows.
step 8 let = Si = Ai , respectively; clearly, condition step 9
satisfied due property (1).
0 , 0 =
j [1..m], let = s0j,r , s0 = sj,r+1 , = j,r
j,r+1 ; clearly,
condition step 12 satisfied due form derivation; furthermore,
condition step 13 satisfied due property (2) Definition 20.

Finally, conditions steps 16 17 satisfied due way chose k.
Therefore, function exist returns true step 19.

685

fiStefanoni, Motik, Krotzsch, & Rudolph

Rule
(cr1)
(cr2)

(cr3)

(cr4)
(cr5)

(cr6)

(cr7)

Precondition
K |= A1 u A2 v B
{A1 (w), A2 (w)}
K |= v S.B
K |= B v {a} IK
A(w)
K |= v S.B
K 6|= B v {a} IK
A(w)
K |= S.A v B
{S(w, w0 ), A(w0 )}
K |= S.Self v B
S(w, w)
role simple
K |= v S.Self
A(w)
role simple
L(S)
(w, w0 )

Conclusion
B(w)
S(w, a), B(a)

S(w, fS,B (w)), B(fS,B (w))
>c (fS,B (w)), >r (fS,B (w), fS,B (w))
>r (fS,B (w), w0 ) term w0 occurring
>r (w0 , fS,B (w)) term w0 occurring
B(w)
B(w)

S(w, w)

S(w, w0 )

Table 5: Rules consequence-based chase
4.4.2 Consequence-Based Chase Universal Interpretations
prove entails(K, q) sound complete, interpret K using forest-shaped
universal interpretation described Section 4.2. Towards goal, next define
auxiliary notions, define universal interpretation, and, finally, prove two properties interpretation.
universe K set terms built individuals occurring K
unary function symbols form fS,A R CK . Since K normalised,
universe K nonempty. fact ground atom constructed using predicates
occurring K terms universe K. role chain = S1 Sn , terms
w w0 , set facts I, write (w, w0 ) (not necessarily distinct) terms
w = w0 , . . . , wn = w0 exists Si (wi1 , wi ) [1..n]. set facts
entails Boolean CQ q = ~y . (~y ), written |= q, substitution exists
dom( ) = ~y (q) I. universal interpretation IK K defined follows.
Definition 25. chase rule Table 5 applicable set facts preconditions
rule satisfied, contain conclusions rule. consequencebased chase (often chase) K sequence sets facts I0 , I1 , . . .
I0 = {{a}(a), >c (a), >r (a, b) | {a, b} IK }

(45)

and, 1, set Ii+1 obtained extending Ii conclusion one (arbitrarily
chosen) chase rule applicable Ii , Ii+1 = Ii chase rule applicable Ii .
686

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

sequence must fairthat is, derivation rule applicable Ii specific
precondition, j exists suchSthat Ij+1 obtained Ij applying rule
mentioned precondition. Set IK = Ii universal interpretation K.
Since K normal form, K 6|= {a} v {b} distinct individuals
b IK ; hence, one individual IK exists rule (cr2) K |= B v {a}.
that, straightforward see IK independent order
chase rules applied, call IK universal interpretation K. Moreover,
due fairness, derivation rule applicable IK is, chase rule
Table 5 either preconditions rule satisfied IK , IK contains
conclusions rule. Finally, well-known that, K consistent, IK
homomorphically embedded model K (Krotzsch et al., 2007). Consequently,
universal interpretation IK used answer arbitrary Boolean CQs K.
Fact 26. Boolean CQ q, K |= q K |= >c v c IK |= q.
Next, show IK relates axioms entailed K. end, let
following function mapping term w universe K basic concept:
(
{w} w
(w) :=

w form w = fS,A (w0 )
Proposition 27. universal model IK satisfies following properties.
1. A(w) IK , K |= (w) v A.
2. S(w, w0 ) IK , nonempty role chain = 0 S1 1 m1 Sm
L(S) terms w0 , . . . , wm universe K w0 = w wm = w0
exist
(a) [1..m], either wi IK , atomic concept Ai {>c } CK exists
wi = fSi ,Ai (wi1 ) K 6|= Ai v {a} individual IK ,
(b) [1..m], K |= (wi1 ) v Si .(wi ),
(c) [0..m] role occurring , K |= (wi ) v T.Self
K |= v .
Proof. Let I0 , I1 , . . . chase sequence K. show induction rule applications properties (1) (2) satisfied A(w) S(w, w0 ) ,
respectively, additionally satisfies following property:
3. term w occurring , K |= x.(w)(x).
definition I0 (cr3), terms w w0 occurring ,
clearly {>c (w), >r (w, w0 ), >r (w0 , w)} .
Base case. Consider I0 , note term w occurring I0 individual
(w) = {w}. Consider A(a) I0 ; either A(a) A, = {a}, = >c ,
K |= {a} v A, property (1) holds. Furthermore, consider S(a, b) I0 ;
687

fiStefanoni, Motik, Krotzsch, & Rudolph

S(a, b) = >r , K |= {a} v S.{b}, property (2) holds w0 = a,
w1 = b, = S. Finally, property (3) holds K |= x.{a}(x) IK .
Inductive step. Assume satisfies properties (1)(3). considering
derivation rule, assume rule applicable shown Table 5,
show properties (1)(3) hold conclusions rule. Note rule (cr3)
affect property (3), explicitly consider properties hold vacuously.
(cr1) inductive hypothesis, K |= (w) v A1 K |= (w) v A2 ,
implies K |= (w) v B, required property (1).
(cr2) inductive hypothesis, K |= (w) v K |= x.(w)(x),
clearly imply K |= (w) v S.B K |= x.B(x). Moreover, (a) = {a}, K |= (a) v B,
property (1) holds. Finally, since K |= (w) v S.(a), property (2) holds w0 = w,
w1 = a, = S.
(cr3) Let w00 = fS,B (w). inductive hypothesis, K |= (w) v
K |= x.(w)(x), K |= (w) v S.B K |= x.B(x). Moreover, (w00 ) = B,
K |= (w00 ) v B, K |= (w00 ) v >c , K |= x.(w00 )(x), required properties (1)
(3), respectively. property (2), consider role assertions derived rule.
S(w, w00 ). Note K |= (w) v S.(w00 ) K 6|= B v {a} IK ,
property (2) holds w0 = w, w1 = w00 , = S.
>r (w00 , w00 ). Clearly, property (2) holds w0 = w00 = 0 = >r .
>r (w00 , w0 ) term w0 occurring . Let individual w0 rooted
in; then, >r (a, w0 ) so, inductive hypothesis, role chain L(>r )
terms = w0 , . . . , wm = w0 exist satisfying properties (a)(c). Since K |= x.(w00 )(x),
K |= (w00 ) v >r .{a}; thus, >r w00 , w0 , . . . , wm satisfy property (2).
>r (w0 , w00 ) term w0 occurring . Then, >r (w0 , w) so, inductive
hypothesis, role chain L(>r ) terms w0 = w0 , . . . , wm = w exist satisfying
properties (a)(c). then, >r w0 , . . . , wm , w00 satisfy property (2).
(cr4) inductive hypothesis, K |= (w0 ) v A; moreover, terms w0 , . . . , wm
w0 = w wm = w0 nonempty role chain = 0 S1 1 m1 Sm
L(S) exist satisfying properties (a)(c). definition L(S), K |= v S;
together entailments properties (b) (c), K |= (w) v S.(w0 ).
then, K |= (w) v S.A, implies K |= (w) v B, required property (1).
(cr5) inductive hypothesis, nonempty role chain L(S) exist satisfying
properties (a)(c); moreover, K |= v definition L(S). Role simple,
|| = 1, therefore one following two forms.
= 0 0 = . property (c), K |= (w) v T.Self K |= v .
Furthermore, due K |= v S, K |= v S. then, K |= (w) v S.Self,
K |= (w) v B holds, required property (1).
= S1 . Terms w0 w1 satisfying property (2) equal w; moreover,
w1 form fS1 ,A1 (w), w IK . Furthermore, property (b)
688

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

K |= (w) v S1 .(w); together w IK , K |= (w) v S1 .Self. Finally,
due K |= v S, K |= S1 v S. then, K |= (w) v S.Self,
K |= (w) v B holds, required property (1).
(cr6) inductive hypothesis, K |= (w) v A, conclude
K |= (w) v S.Self, property (2) holds w0 = w = 0 = S.
(cr7) = , w = w0 K |= v S, property (2) holds w0 = w
= 0 = S. Otherwise, assume nonempty form = S1 Sk .
Thus, terms w0 , . . . , wk w0 = w wk = w0 exist Si (wi1 , wi )

[1..k]. inductive hypothesis, [1..k], terms w0i , . . . , wm






w0 = wi1 wmi = wi role chain L(Si ) exist satisfying properties
k
i1 = w
(a)(c); note w0i = w0 = w, wm
= wk = w0 , wm
0
i1
k
1
k
[1..k]. definition L(S), L(S), property (2) holds
1 , . . . , wk , . . . , wk .
role chain 1 k terms w0 , w11 , . . . , wm
mk
1
1
4.4.3 Soundness
ready show algorithm entails sound.
Lemma 28. nondeterministic computation exists entails(K, q) returns true,
K |= q.
Proof. Assume nondeterministic computation exists entails(K, q) returns
true. algorithm returns true step 1, K |= q, K inconsistent; hence,
rest proof, assume K consistent show IK |= q. end, let
substitution , skeleton = hV, E, i, function L determined entails. Graph
hV, Ei forest rooted individuals occurring K so, structural induction
forest, define mapping V universe K satisfy following:
(i) v V , (v)( (v)) IK ;
(ii) hv, v 0 E Pj L(v, v 0 ), role chain j L(Pj ) exists
j ( (v), (v 0 )) IK .
Base case. IK , let (a) = a. Since (a) = {a} {a}(a) IK , first
property clearly holds, second property vacuous.
Inductive step. Consider hv, v 0 E (v) defined, (v 0 ) not;
let L(v, v 0 ) = {P1 , . . . , Pm }. Since exist((v), (v 0 ), L(v, v 0 )) returns true, Lemma 24
roles S1 , . . . , Sn , atomic concepts A1 , . . . , , and, j [1..m], role
chain j = S1 j,1 Sn j,n exist n 1, A0 = (v) = (v 0 ),
following holds [1..n] j [1..m].
1. IK , K |= Ai1 v Si .Ai K 6|= Ai v {a}.
2. role occurring j,i , K |= Ai v T.Self K |= v .
3. j LdR (Pj ).

689

fiStefanoni, Motik, Krotzsch, & Rudolph

Let w0 = (v); let wi = fSi ,Ai (wi1 ) [1..n]; let (v 0 ) = wn . Since A0 = (v),
inductive hypothesis A0 ( (v)) IK . Furthermore, (cr3) applicable
IK so, [1..n], Si (wi1 , wi ) IK Ai (wi ) IK ; thus, ( (v 0 )) IK ,
required. Finally, role occurring j,i , K |= Ai v T.Self
K |= v ; (cr6) (cr7) applicable IK , respectively, (wi , wi ) IK ;
thus, j ( (v), (v 0 )) IK , required.
next show ((q)) IK considering independently atom (q).
prove lemma, combine obvious way.
Consider arbitrary unary atom A(t) (q). step 4 Algorithm 1,
K |= (t) v A, also implies K |= (t) u (t) v A. property (i),
(t)( (t)) IK . Since rule (cr1) applicable IK , A( (t)) IK , required.
Consider arbitrary binary atom S(t, u) (q). Let v0 , . . . , vn , s0 , . . . , sn ,
0 , . . . , n determined steps 811 Algorithm 1 considers atom S(t, u).
[1..n], pda(si1 , i1 , si , ) L(vi1 , vi ) step 12; then, property (ii)
role chain exists L(pda(si1 , i1 , si , )) ( (vi1 ), (vi )) IK .
Next, define 0 considering following two cases.
v0 I. step 14 Algorithm 1, role chain 0 = S1 Sk exists
0 L(wfa((t), (v0 ))). property (i), (t)( (t)) IK ; moreover, Definition 20, basic concepts (t) = A0 , A1 , . . . , Ak = {v0 } exist K |= Aj1 v Sj .Aj
j [1..k]. Rules (cr2) (cr3) applicable IK ,
0 ( (t), (v0 )) IK .
v0 6 I, implies v0 = t. step 16 Algorithm 1, role chain 0 = T1 Tk
exists 0 L(sfa((v0 ))). property (i), (v0 )( (v0 )) IK ;
moreover, Definition 20, K |= (v0 ) v Tj .Self K |= v Tj j [1..k].
Rules (cr6) (cr7), respectively, applicable IK , thus 0 ( (t), (v0 )) IK .
either case, steps 14 16 0 L(pda(iS , , s0 , 0 )). let 0 = 0 n ;
note n = 0, case 0 = 0 . Clearly, 0 ( (t), (vn )) IK ,
vn = u. Moreover, 0 L(pda(iS , , sn , n )) sn = fS n = . Finally,
rule (cr7) applicable IK , S( (t), (u)) IK , required.
4.4.4 Completeness
next prove encoding also complete, thus proving Theorem 21.
Lemma 29. K |= q, nondeterministic computation exists entails(K, q)
returns true.
Proof. Assume K |= q. K inconsistent, entails(K, q) returns true, required;
hence, rest proof, assume K consistent. then, IK |= q,
substitution exists (q) IK . Let defined Section 4.4.2.
substitution step 2, let (y) := (y) (y) I; otherwise, let (y)
arbitrary, fixed, variable 0 q (y) = (y 0 ). straightforward see
((q)) IK .

690

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

skeleton = hV, E, step 3, set V contains IK variables occurring
(q), (y) = ((v)) variable V . Furthermore, let smallest
irreflexive transitive relation universe K w fS,A (w) term
w universe K; then, let hv, v 0 E (v) (v 0 ) v 00 V exists
(v) (v 00 ) (v 0 ). definition , graph hV, Ei forest rooted
IK , required Definition 18.
step 4, arbitrary atom A(t) (q), A((t)) IK ; property (1)
Proposition 27, K |= ((t)) v A; hence, condition satisfied.
consider arbitrary edge hv, v 0 E; let w0 , . . . , wk terms, let A1 , . . . , Ak
atomic concepts, let S1 , . . . , Sk roles w0 = (v), wk = (v 0 ),
wi = fSi ,Ai (wi1 ) [1..k]; finally, let A0 = (v). Note
uniquely defined edge, that, construction IK , [1..k],
K |= Ai1 v Si .Ai K 6|= Ai v {a} IK . Then, role chain compatible
edge hv, v 0 role chains 1 , . . . , k exists = S1 1 Sk k and,
[1..k] role occurring , K |= Ai v T.Self K |= v .
rest proof show following property.
() PDA P L(v, v 0 ), role chain L(P) exists compatible
edge hv, v 0 i.
Lemma 24 definition compatibility, property () implies
condition step 18 satisfied edge hv, v 0 i.
loop steps 616, let S(t, u) arbitrary binary atom (q); next
determine required nondeterministic choices preserve () step 12,
satisfy conditions steps 14 16, completes proof lemma. Let au IK
unique individual connected u hV, Ei. Since S((t), (u)) IK , nonempty
role chain = 0 S1 1 m1 Sm L(S) terms w0 , . . . , wm
universe K w0 = (t) wm = (u) exist satisfying property (2) Proposition 27.
define vertex v0 step 8, consider two possibilities, also define
index `0 [0..m] w`0 = (v0 ).
j [0..m] exists wj IK , let v0 = au let `0 largest index
w`0 = au .
Otherwise, let v0 = let `0 = 0.
Let v0 , . . . , vn unique path connecting v0 u S. definition `0
form terms w`0 +1 , . . . , wm , wj 6 j [`0 + 1..m]; (v0 ) = w`0 ;
(vn ) = wm . Thus, [1..n], unique index `i exists (vi ) = w`i .
let 0 = 0 S`0 `0 , let = S`i1 +1 `i1 +1 S`i `i [1..n];
clearly, = 0 n . properties (a)(c) Proposition 27, [1..n], role chain
compatible edge hvi1 , vi i. Furthermore, L(S) Theorem 8 imply
LdR (pda(iS , , fS , )), states s0 , . . . , sn sn = fS words 0 , . . . , n
n = exist 0 LdR (pda(iS , , s0 , 0 )) LdR (pda(si1 , i1 , si , )
[1..n]. Since compatible hvi1 , vi i, step 12 preserves property (),
required. Finally, consider step 13.

691

fiStefanoni, Motik, Krotzsch, & Rudolph

v0 I. property (b) Proposition 27, K |= (wj1 ) v Sj .(wj ) j [1..`0 ].
Furthermore, property c Proposition 27, K |= (wj ) v T.Self K |= v
j [0..`0 ] role occurring j ; thus, K |= (wj ) v T.(wj ).
then, 0 L(wfa((t), (v0 ))), condition step 14 satisfied.
v0 6 I, v0 = 0 = 0 . property (c) Proposition 27, K |= (wj ) v T.Self
K |= v role occurring 0 . then, 0 L(sfa((v0 ))),
condition step 16 satisfied.

5. Lower Complexity Bound
previous section, presented BCQ answering algorithm ELRO+ uses
space polynomial total size input. algorithm worst-case optimal
combined complexity since Krotzsch et al. (2007) reduced PSpace-hard problem
checking nonemptiness intersection languages generated deterministic
finite automata F1 . . . Fm common alphabet (Kozen, 1977) BCQ answering
ELRO+ . knowledge base K encoding problem, regular RBox contains roles
S1 . . . Sm L(Si ) = L(Fi ) [1..m]; furthermore, TBox ensures
universal interpretation IK rooted tree so, , term w exists
reachable root chain
roles corresponding ; finally, Boolean CQ
contains atoms check whether L(Fi ) nonempty. next improve lower
bound showing problem hard already restricted setting query,
TBox, ABox fixed, RBox varies.
Theorem 30. K regular ELRO+ knowledge base q Boolean conjunctive query,
checking K |= q PSpace-hard even
query fixed consist two binary atoms single quantified variable,
TBox fixed contains axioms form v S.A,
ABox fixed contains single unary assertion.
Proof. reduce PSpace-hard problem deciding whether intersection
languages generated deterministic finite automata nonempty (Kozen, 1977). Let
0 deterministic finite automata alphabet 0 , let fresh symbols
F10 , . . . , Fm
1
2
occurring 0 , let = 0 {1 , 2 }. j [1..m], let Fj = hQj , , j , ij , fj
deterministic finite automaton alphabet obtained extending Fj0
transition labelled 1 final state fj0 Fj0
itself, transition labelled
0
0

2 fj fresh final state fj Fj . Then,
j ) 6= word
j L(F
0
w j L(Fj ) exist |w| odd: given w j L(Fj ), |w| odd, |w 1 2 |
odd w 1 2 L(Fj ) j [1..m], |w| even, |w 2 | odd
w 2 L(Fj ) j [1..m]. Finally, assume w.l.o.g. Qi Qj 6= Qi R
hold 1 < j m, R well.
Let w = ST1 Sn word n odd, let = Q1 . . . Qm .
Clearly, w j L(Fj ) holds word w form
n
n
w = e01 e0m S1 o1m o11 S2 e21 e2m e1n1 en1
Sn om o1

692

(46)

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

exists following conditions hold j [1..m]:

(i) [1..n] odd, oij Qj j (ei1
j , Si ) = oj ;

(ii) [1..n] even, eij Qj j (oi1
j , Si ) = ej ;

(iii) e0j = ij onj = fj .
let LO , LE , L1 , L2 following languages.
LO :={e1 em om o1 | j (ej , S) = oj , j [1..m]}

(47)

LE :={om o1 e1 em | j (oj , S) = ej , j [1..m]}

(48)



L1 :=(LO ) LO

(49)


L2 :={i1 im } ( LE ) {fm f1 }

(50)

Consider arbitrary word w corresponding word w . definition
L1 , w L1 w form (46) satisfies property (i). Similarly,
definition L2 , w L2 w theT form (46) satisfies
properties (ii) (iii). Thus, w L1 L2 w j L(Fj ). simplicity,
rest proof, use following equivalent formulations L1 L2 .
L1 =LO (LO )+ LO

(51)
+

L2 ={i1 im } {fm f1 } {i1 im } ( LE ) {fm f1 }

(52)

TWe next define knowledge base K fixed query q K |= q
j L(Fj ) 6= . present construction stages, describe
affects canonical model = hI , Kthat is, model constructed using
standard notion chase (i.e., Definition 25, semantic conditions
K replaced syntactic checks axioms K). simplicity, first present K
TBox depends , later modify encoding use fixed TBox.
TBox contains axioms (53), ABox contains axiom (54).
assume aI = ; then, word , domain element exists
connected via chain roles corresponding .
v.A

symbol

(53)

A(a )

(54)

next present RBox R consisting four parts, encoding languages LO , LE ,
S,m+1
S,m
L1 , L2 . encoding uses fresh roles LS,1
LS,0
uniquely
, . . . , LO
E , . . . , LE
0
associated role , well fresh roles LO , LE , L , L1 , L1 , L2 , L02 .
first part R contains axioms (55)(57). clear that, words
1 , 2 1 prefix 2 , ha1 , a2 LIO 2 1 LO .
v LS,m+1

ej

LS,j+1


oj v

LS,j




(55)

j [1..m] ej , oj Qj j (ej , S) = oj

(56)

LS,1
v LO

(57)
693

fiStefanoni, Motik, Krotzsch, & Rudolph

second part R contains axioms (58)(60). clear that, words
1 , 2 1 prefix 2 , ha1 , a2 LIE 2 1 LE .

oj

S,j1
LE

v LS,0
E



(58)

LS,j
E

j [1..m] ej , oj Qj j (oj , S) = ej

(59)

ej v

LS,m
v LE
E

(60)

third part R contains axioms (61)(65). clear that, words
1 , 2 1 prefix 2 , ha1 , a2 LI1 2 1 L1 .
v L



(61)

LO v L1

(62)

L01
L01

(63)

v L1

(65)

LO L v
L01 L01
L01 LO

v

(64)

fourth part R contains axioms (66)(69). clear that, words
1 , 2 1 prefix 2 , ha1 , a2 LI2 2 1 L2 .
i1 im L fm f1 v L2

(66)

L02
L02

(67)
(68)

L fm f1 v L2

(69)

L LE v
L02
i1 im

L02



L02

v

Query q given (70). Then, K |= q word exists
ha , LI1 ha , LI2 , latter clearly case L1 L2 .
RBox R regular size polynomial size automata F1 , . . . , Fm .
q = y. L1 (a , y) L2 (a , y)

(70)

next tighten reduction use fixed TBox 0 consisting axioms (71)(72),
P0 P1 fresh roles.
v P0 .A

(71)

v P1 .A

(72)

let k = dlog2 ||e, assume symbol corresponds k-digit binary
number b1 bk bi {0, 1}. Then, let R0 R extended axioms (73).
Pb 1 Pb k v

corresponding b1 bk

(73)

Finally, let K0 = hT 0 , R0 , Ai, let 0 canonical model K0 . Axioms (54), (71),
(72) ensure existence binary tree whose edges labelled roles P0 P1 .
Furthermore, axioms (73) ensure that, sequence k edges
tree corresponding binary number assigned , shortcut tree
694

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

labelled . Thus, homomorphically embedded 0 . Finally, roles P0
P1 occur R query q checks existence domain element connected ;
therefore, extra edges 0 irrelevant. Consequently, encoding languages L1
L2 works way varying TBox .
Finally, characterise complexity BCQ answering ELRO+ knowledge bases.
Theorem 31. K regular ELRO+ KB q Boolean CQ, checking K |= q
1. PTime-complete data complexity,
2. NP-complete, RBox R fixed,
3. PSpace-complete combined complexity.
Proof. Calvanese et al. (2006) proved BCQ answering PTime-hard data complexity already EL knowledge bases. Furthermore, query fixed, BCQ
answering NP-hard already relational databases (Chandra & Merlin, 1977).
theorem follows Theorems 23 30, Savitchs theorem.

6. Navigational Queries
data DL knowledge bases graph-like structure, unary assertions encode
properties graph nodes binary assertions encode graph edges. Conjunctive queries
cannot express recursive properties reachability, expressivity often insufficient applications require graph navigation. popularity graph
databases rise, number navigational languages querying graph-like data
proposed; example, regular path queries (Barcelo, 2013) use regular expressions express complex navigational patterns graph vertices, graph XPath
queries (Libkin et al., 2013) extend regular path queries converse operator, negation regular expressions, checking properties vertices using Boolean combinations
concepts existential quantifications paths. DL context, computational complexity navigational queries studied several expressive DLs
members DL-Lite family EL(H) fragment ELRO+ (Calvanese, Eiter, &
Ortiz, 2009; Bienvenu et al., 2013; Kostylev et al., 2014; Bienvenu et al., 2014). order
complete complexity landscape problem, section study problem
answering graph XPath queries ELRO+ knowledge bases.
6.1 Graph XPath Queries
Graph XPath queries consist node expressions path expressions, whose syntaxes
defined respectively following two context-free grammars B basic concept
role.
B | | 1 2 | 1 2 | hi
| | 1 2 | 1 + 2 | | | test()
Following Libkin et al. (2013), consider following expression fragments.
695

fiStefanoni, Motik, Krotzsch, & Rudolph

P,E
4

P

g

P

P,F
3

S,D
5

R,S



R

b

U
P

S,D
6



S,A
7



U

P,A
2



f

P

R,S

c
B

R



R,S

e



P,E
1

Figure 8: Interpretation
1. path-positive fragment disallows path expressions form .
2. positive fragment disallows path expressions form node expressions
form .
3. converse-free fragment disallows path expressions form .
graph XPath atom form (s) (s, t), node expression, path
expression, terms. conjunctive graph XPath query (CGXQ) g expression
g = ~y . (~x, ~y ) conjunction graph atoms variables ~x ~y ; variables ~x
called answer variables g. ~x = , g = ~y . (~y ) Boolean CGXQ.
Path-positive, positive, converse-free CGXQs obtained restricting query atoms
accordingly. Finally, graph XPath query (GXQ) CGXQ containing single atom.
define semantics CGXQs, let = hI , first-order interpretation.
interpretation node path expressions inductively defined follows.
()I
(1 2 )I
(1 2 )I
(hi)I

=
=
=
=

\ ()I
(1 )I (2 )I
(1 )I (2 )I
{x | : hx, yi }

(S )I
(1 2 )I
(1 + 2 )I
( )I
()I
(test())I

=
=
=
=
=
=

{hy, xi | hx, yi }
(1 )I (2 )I
(1 )I (2 )I
(I )
\ ()I
{hx, xi | x }

Please observe difference path expressions 1 2 corresponds (1 + 2 ),
whereas intersection 1 2 corresponds (1 + 2 ); moreover, Libkin et al.
(2013) define path expression , setting corresponds test(>c ). Satisfaction
Boolean CGXQ g CGXQ entailment defined obvious way; moreover,
Boolean CGXQ answering problem checking K |= g.
Example 32. illustrate definitions using interpretation shown Figure 8;
notation Example 15. Moreover, let 1 , 2 , 3 following path expressions.
1 =(R test(hS test(A B)i))

(74)

2 =(U test(hP test(A B)i))

(75)

696

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

Node expressions
TB = {B v CB }
T1 2 = {C1 u C1 v C1 2 } T1 T2
T1 2 = {C1 v C1 2 , C2 v C1 2 } T1 T2
Thi = {T .>c v Chi }
Path expressions
TS =
T1 2 = T1 T2
T1 +2 = T1 T2
=
Ttest() = {C v Ttest() .Self}

RB =
R1 2 = R1 R2
R1 2 = R1 R2
Rhi = R

RS = {S v TS }
R1 2 = {T1 T2 v T1 2 } R1 R2
R1 +2 = {T1 v T1 +2 , T2 v T1 +2 } R1 R2
R = { v , v , v } R
Rtest() = R

Table 6: Encoding positive, converse-free node path expressions using axioms
3 =((R S) )

(76)

Expression 1 positive, retrieves pairs individuals connected
path R-edges that, element occurring path first,
exists outgoing path S-edges reaching member concept B. example,
{haI , dI i, haI , eI i} (1 )I .
contrast, expression 2 path-positive, retrieves pairs individuals
connected U -edge P -successor exists member concept AtB.
example, haI , g (2 )I , haI , f 6 (2 )I .
Finally, expression 3 neither positive path-positive, retrieves pairs
individuals connected path consisting sequence edges described
regular expression (R S) . example, haI , dI (3 )I , haI , eI 6 (3 )I .
Let g = x, y, z.1 (x, y) 2 (x, z) 3 (x, y) conjunctive graph XPath query, let
= {x 7 a, 7 d, z 7 g} substitution. Using Figure 8, one check |= (g).
observed Kostylev et al. (2014), node expressions graph XPath queries correspond precisely formulas propositional dynamic logic negation (PDL ) (Harel
et al., 2000); satisfiability problem PDL undecidable (Harel, 1984), answering
GXQs DL constraints undecidable. Decidability results recently obtained path-positive positive queries DL-Lite knowledge bases (Kostylev et al.,
2014). addition, Kostylev et al. (2014) proved that, DLs, answering path-positive,
converse-free GXQs coNP-hard data-complexity. Finally, Bienvenu et al. (2014) proved
answering positive GXQs EL knowledge bases ExpTime-complete. Thus,
rest section focus positive, converse-free graph XPath queries.
6.2 Complexity Answering Positive, Converse-Free Graph XPath Queries
rest section, fix ELRO+ KB K = hT , R, Ai R regular.
next show that, given positive, converse-free Boolean CGXQ g, one construct
polynomial time regular ELRO+ KB K0 Boolean CQ q 0 K |= g
K0 |= q 0 . construction K0 combines various expressive features ELRO+ :
697

fiStefanoni, Motik, Krotzsch, & Rudolph

role inclusions reflexive roles encode path expressions g RBox, selfrestrictions encode node expressions g TBox.
Proposition 33. Given positive, converse-free Boolean CGXQ g K, one compute
time polynomial |K| + |g| ELRO+ KB K0 Boolean CQ q 0 RBox
K0 regular, g q 0 equally many atoms, K |= g K0 |= q 0 .
Proof. Let g = ~y . (~y ) positive, converse-free Boolean CGXQ K.
positive node expression , let C fresh atomic concept uniquely associated
and, positive, converse-free path expression , let fresh role uniquely
associated . structural induction, associate (resp. ) TBox
RBox R (resp. TBox RBox R ) shown Table 6. Then, let
K0 = hT 0 , R R0 , Ai TBox 0 RBox R0 follows.
[
[
[
[
T0=


R0 =
R
R
(s)

(s,t)

(s)

(s,t)

let q 0 = ~y . 0 (~y ) Boolean CQ 0 contains C (s) atom (s)
(s, t) atom (s, t) . Clearly, g q 0 number atoms;
moreover, since query g K, query q 0 K0 . Finally, q 0 K0
computed polynomial time input size, RBox K0 clearly regular.
next show K0 6|= q 0 K 6|= g.
() Assume K0 6|= q 0 , interpretation exists |= K0 6|= q 0 .
Since axiom K also axiom K0 , |= K. Furthermore,
positive node expression positive path expression , (C )I
(T )I . prove claim simultaneous induction structure node
path expressions.
Base case. base case, let arbitrary node expression form = B
let arbitrary path expression form = S. Since B v CB 0 , v TS R0 ,
model K0 , claim easily follows.
Inductive step. inductive step, distinguish two cases.
First, consider arbitrary node expression property holds node
path expressions occurring . let x arbitrary element assume
x ; show x CI considering various forms take.
= 1 2 . Since x , x I1 x I2 . inductive hypothesis,
x CI1 x CI2 . definition 0 , C1 u C2 v C 0 .
Since model 0 , x CI , required.
= 1 2 . proof case similar one above.
= hi. Since x , exists hx, yi . inductive
hypothesis, hx, yi TI . definition 0 , .>c v C 0 .
Since model 0 , x CI , required.
Second, consider arbitrary path expression property holds
node path expressions occurring . let x arbitrary elements
assume hx, yi ; show hx, yi TI considering various forms
take.
698

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

= 1 2 . Since hx, yi , exists z hx, zi 1I
hz, yi 2I . inductive hypothesis, hx, zi TI1 hz, yi TI2 .
Moreover, definition R0 , T1 T2 v R0 . Since model
R0 , hx, yi TI , required.
= 1 + 2 . Since hx, yi , hx, yi 1I hx, yi 2I .
inductive hypothesis, hx, yi TI1 hx, yi TI2 . definition R0 ,
{T1 v , T2 v } R0 . Since model R0 , hx, yi TI .
= 1 . First, consider case x = y. definition R0 ,
v R0 . Since model R0 , hx, yi TI , required. Otherwise,
consider case x 6= y. Since hx, yi , elements x0 , . . . , xn x0 = x
xn = exist n > 0 hxi1 , xi 1I [1..n].
inductive hypothesis, [1..n], hxi1 , xi TI1 . definition
R0 , T1 T1 v R0 . Since model R0 , hx, yi .
= test(). follows x = x . inductive hypothesis,
x CI . definition 0 , C v .Self 0 . Since model
0 , hx, yi TI , required.
then, since node path expressions g positive, 6|= q 0 implies 6|= g.
I0

() Assume K 6|= g, interpretation exists |= K 6|= g. Let
interpretation obtained extending fresh concepts roles follows.
0

(C )I =

0

0

(T )I =

0

definition K0 , straightforward see 0 |= K0 ; furthermore,
definition q 0 , straightforward see 0 6|= q 0 , required.
Next, establish complexity answering positive, converse-free (C)GXQs
ELRO+ knowledge bases.
Theorem 34. K regular ELRO+ KB g positive, converse-free Boolean CGXQ,
checking K |= g PTime-complete data complexity, PSpace-complete combined
complexity. g positive, converse-free Boolean GXQ, checking K |= g PTimecomplete combined data complexities.
Proof. hardness data complexity Boolean positive, converse-free (C)GXQs follows
PTime-hardness instance checking EL (Calvanese et al., 2006).
positive, converse-free GXQs, hardness combined complexity inherited
PTime-hardness TBox reasoning EL (Baader et al., 2005). matching upper
bounds, Proposition 33 allows us reduce Boolean GXQ answering checking entailments
form K0 |= q 0 q 0 BCQ containing one atom. next show that,
possible form q 0 , reduce latter problem checking entailment ELRO+
concept inclusions, decided PTime. following, c arbitrarily
chosen individual IK0 .
K0 |= A(a) K0 |= {a} v A.
699

fiStefanoni, Motik, Krotzsch, & Rudolph

K0 |= y.A(y) K0 |= {c} v >r .A.
K0 |= S(a, b) K0 |= {a} v S.{b}.
K0 |= y.S(y, b) K0 |= {c} v >r .S.{b}.
K0 |= y.S(a, y) K0 |= {a} v S.>c .
K0 |= y1 , y2 .S(y1 , y2 ) K0 |= {c} v >r .S.>c .
positive, converse-free CGXQs, hardness combined complexity given Theorem 31, matching upper bounds follow Theorem 23 Proposition 33.

7. Conclusions
paper, presented first CQ answering algorithm OWL 2 EL runs
PSpace, thus closing longstanding open question. algorithm based
innovative, succinct encoding regular role inclusions using bounded-stack PDAthat is,
finite automata extended stack fixed size. believe encoding interesting
right, used optimise popular OWL 2 DL reasoners. Moreover,
refined previously known PSpace lower bound CQ answering showing
problem remains PSpace-hard even query, TBox, ABox fixed (and
RBox varies); thus, identify role inclusions culprit problems
PSpace-hardness. Finally, showed positive, converse-free GXQs CGXQs
answered OWL 2 EL knowledge bases PTime PSpace, respectively;
interesting Bienvenu et al. (2014) showed adding converse operator
makes problem ExpTime-hard. Thus, least theoretical perspective, positive,
converse-free (C)GXQs seem provide adequate language querying OWL 2 EL
knowledge bases.
see two main open problems future work. First, drawing inspiration
succinct encoding role inclusions, shall extend combined approach
Stefanoni et al. (2013) OWL 2 EL thus obtain practical algorithm. Second, static
query analysis fundamental task query optimisation, shall study containment
problem graph queries ELRO+ constraints.

Acknowledgements
results article extension results published preliminary form Krotzsch et al. (2007) Proceedings 6th International Semantic
Web Conference (ISWC 2007). work supported Alcatel-Lucent; EU FP7
project OPTIQUE; EPSRC projects MASI3 , Score!, DBOnto; DFG project
DIAMOND (Emmy Noether grant KR 4381/1-1).

References
Anselmo, M., Giammarresi, D., & Varricchio, S. (2003). Finite automata non-selfembedding grammars. Proceedings 7th International Conference Im-

700

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

plementation Application Automata, CIAA02, pp. 4756, Berlin, Heidelberg.
Springer-Verlag.
Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). DL-Lite family
relations. J. Artif. Intell. Res. (JAIR), 36, 169.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Kaelbling, L. P.,
& Saffiotti, A. (Eds.), Proceedings 19th International Joint Conference Artificial Intelligence (IJCAI 2005), pp. 364369, Edinburgh, UK. Morgan Kaufmann
Publishers.
Baader, F., Brandt, S., & Lutz, C. (2008). Pushing EL envelope further. Clark, K.,
& Patel-Schneider, P. F. (Eds.), Proceedings OWLED 2008 DC Workshop
OWL: Experiences Directions.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2010). Description Logic Handbook: Theory, Implementation, Applications.
Cambridge University Press. Paperback edition.
Baget, J.-F., Leclere, M., Mugnier, M.-L., & Salvat, E. (2011). rules existential
variables: Walking decidability line. Artif. Intell., 175 (9-10), 16201654.
Barcelo, P. (2013). Querying graph databases. Hull, R., & Fan, W. (Eds.), PODS, pp.
175188. ACM.
Barrett, C., Jacob, R., & Marathe, M. (2000). Formal-language-constrained path problems.
SIAM J. Comput., 30 (3), 809837.
Bienvenu, M., Calvanese, D., Ortiz, M., & Simkus, M. (2014). Nested regular path queries
Description Logics. Proc. 14th Int. Conf. Principles Knowledge
Representation Reasoning (KR 2014). AAAI Press.
Bienvenu, M., Ortiz, M., & Simkus, M. (2013). Conjunctive regular path queries
lightweight Description Logics. Rossi, F. (Ed.), IJCAI. IJCAI/AAAI.
Cal, A., Gottlob, G., & Kifer, M. (2013). Taming infinite chase: Query answering
expressive relational constraints. J. Artif. Intell. Res. (JAIR), 48, 115174.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodriguez-Muro, M.,
Rosati, R., Ruzzi, M., & Savo, D. F. (2011). MASTRO system Ontology-Based
Data Access. Semantic Web, 2 (1), 4353.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2006). Data
complexity query answering Description Logics. Proc. 10th Int. Conf.
Principles Knowledge Representation Reasoning (KR 2006), pp. 260
270.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning efficient query answering Description Logics: DL-Lite family. J.
Autom. Reasoning, 39 (3), 385429.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2013). Data
complexity query answering Description Logics. Artificial Intelligence, 195, 335
360.

701

fiStefanoni, Motik, Krotzsch, & Rudolph

Calvanese, D., De Giacomo, G., Lenzerini, M., & Vardi, M. Y. (2000). Containment
conjunctive regular path queries inverse. Proc. 7th Int. Conf.
Principles Knowledge Representation Reasoning (KR 2000), pp. 176185.
Calvanese, D., Eiter, T., & Ortiz, M. (2009). Regular path queries expressive Description
Logics nominals. Boutilier, C. (Ed.), IJCAI 2009, Proceedings 21st
International Joint Conference Artificial Intelligence, Pasadena, California, USA,
July 11-17, 2009, pp. 714720.
Calvanese, D., Vardi, M. Y., De Giacomo, G., & Lenzerini, M. (2000). View-based query
processing regular path queries inverse. Proceedings Nineteenth ACM
SIGMOD-SIGACT-SIGART Symposium Principles Database Systems, PODS
00, pp. 5866, New York, NY, USA. ACM.
Chandra, A. K., & Merlin, P. M. (1977). Optimal implementation conjunctive queries
relational data bases. Hopcroft, J. E., Friedman, E. P., & Harrison, M. A. (Eds.),
Proc. 9th annual ACM Symposium Theory Computing (STOC 77), pp.
7790, Boulder, CO, USA. ACM Press.
Cruz, I. F., Mendelzon, A. O., & Wood, P. T. (1987). graphical query language supporting
recursion. SIGMOD Rec., 16 (3), 323330.
Cuenca Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P. F., & Sattler, U.
(2008). OWL 2: next step OWL. J. Web Sem., 6 (4), 309322.
De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rosati, R., Ruzzi, M., & Savo, D. F.
(2012). MASTRO: reasoner effective Ontology-Based Data Access. Horrocks,
I., Yatskevich, M., & Jimenez-Ruiz, E. (Eds.), ORE, Vol. 858 CEUR Workshop
Proceedings. CEUR-WS.org.
Eiter, T., Ortiz, M., & Simkus, M. (2012a). Conjunctive query answering Description
Logic SH using knots. J. Comput. Syst. Sci., 78 (1), 4785.
Eiter, T., Ortiz, M., Simkus, M., Tran, T.-K., & Xiao, G. (2012b). Query rewriting
Horn-SHIQ plus rules. Hoffmann, J., & Selman, B. (Eds.), AAAI. AAAI Press.
Fan, W. (2012). Graph pattern matching revised social network analysis. Deutsch,
A. (Ed.), ICDT, pp. 821. ACM.
Geffert, V., Mereghetti, C., & Palano, B. (2010). concise representation regular
languages automata regular expressions. Information computation, 208 (4),
385394.
Giese, M., Calvanese, D., Haase, P., Horrocks, I., Ioannidis, Y., Kllapi, H., Koubarakis, M.,
Lenzerini, M., Moller, R., Rodriguez-Muro, M., Ozcep, O., Rosati, R., Schlatte, R.,
Schmidt, M., Soylu, A., & Waaler, A. (2013). Scalable end-user access big data.
Akerkar, R. (Ed.), Big Data Computing. CRC Press.
Glimm, B., Lutz, C., Horrocks, I., & Sattler, U. (2008). Conjunctive query answering
Description Logic SHIQ. J. Artif. Intell. Res. (JAIR), 31, 157204.
Gottlob, G., Manna, M., & Pieris, A. (2014). Polynomial combined rewritings existential
rules. Proc. 14th Int. Conf. Principles Knowledge Representation
Reasoning (KR 2014). AAAI Press.
702

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

Gottlob, G., & Schwentick, T. (2012). Rewriting ontological queries small nonrecursive
datalog programs. Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), Principles
Knowledge Representation Reasoning: Proceedings Thirteenth International
Conference, KR 2012, Rome, Italy, June 10-14, 2012. AAAI Press.
Grosof, B. N., Horrocks, I., Volz, R., & Decker, S. (2003). Description Logic Programs: Combining logic programs Description Logic. Proceedings 12th international
conference World Wide Web, pp. 4857.
Gutierrez, C., Hurtado, C. A., Mendelzon, A. O., & Perez, J. (2011). Foundations
semantic web databases. J. Comput. Syst. Sci., 77 (3), 520541.
Harel, D. (1984). Dynamic logic. Gabbay, D., & Guenthner, F. (Eds.), Handbook
Philosophical Logic Vol. II, pp. 497604. Reidel Publishing Company.
Harel, D., Tiuryn, J., & Kozen, D. (2000). Dynamic Logic. MIT Press, Cambridge, MA,
USA.
Hopcroft, J. E., Motwani, R., & Ullman, J. D. (2003). Introduction Automata Theory,
Languages, Computation - international edition (2. ed). Addison-Wesley.
Horrocks, I., Kutz, O., & Sattler, U. (2006). even irresistible SROIQ. Doherty,
P., Mylopoulos, J., & Welty, C. A. (Eds.), KR, pp. 5767. AAAI Press.
Horrocks, I., & Sattler, U. (2004). Decidability SHIQ complex role inclusion axioms.
Artificial Intelligence, 160 (12), 79104.
Johnson, D. S., & Klug, A. C. (1984). Testing containment conjunctive queries
functional inclusion dependencies. J. Comput. Syst. Sci., 28 (1), 167189.
Kazakov, Y. (2008). RIQ SROIQ harder SHOIQ. Brewka, G., & Lang,
J. (Eds.), KR, pp. 274284. AAAI Press.
Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2011). combined approach Ontology-Based Data Access. Walsh, T. (Ed.), IJCAI 2011,
Proceedings 22nd International Joint Conference Artificial Intelligence,
Barcelona, Catalonia, Spain, July 16-22, 2011, pp. 26562661. IJCAI/AAAI.
Kostylev, E. V., Reutter, J. L., & Vrgoc, D. (2014). XPath DL-Lite ontologies.
Bienvenu, M., Ortiz, M., Rosati, R., & Simkus, M. (Eds.), Informal Proceedings
27th International Workshop Description Logics, Vienna, Austria, July 17-20,
2014., Vol. 1193 CEUR Workshop Proceedings, pp. 258269. CEUR-WS.org.
Kozen, D. (1977). Lower bounds natural proof systems. FOCS, pp. 254266. IEEE
Computer Society.
Krotzsch, M. (2011). Efficient rule-based inferencing OWL EL. Walsh, T. (Ed.),
Proceedings 22nd International Joint Conference Artificial Intelligence (IJCAI11). AAAI Press/IJCAI. 26682673.
Krotzsch, M., Rudolph, S., & Hitzler, P. (2007). Conjunctive queries tractable fragment
OWL 1.1. Aberer, K., Choi, K.-S., Noy, N., Allemang, D., Lee, K.-I., Nixon, L.,
Golbeck, J., Mika, P., Maynard, D., Mizoguchi, R., Schreiber, G., & Cudre-Mauroux,
P. (Eds.), Proceedings 6th International Semantic Web Conference (ISWC07),
Vol. 4825 LNCS, pp. 310323. Springer.
703

fiStefanoni, Motik, Krotzsch, & Rudolph

Libkin, L., Martens, W., & Vrgoc, D. (2013). Querying graph databases XPath.
Tan, W.-C., Guerrini, G., Catania, B., & Gounaris, A. (Eds.), ICDT, pp. 129140.
ACM.
Lutz, C. (2008). complexity conjunctive query answering expressive Description
Logics. Automated Reasoning.
Lutz, C., Seylan, I., Toman, D., & Wolter, F. (2013). combined approach OBDA:
Taming role hierarchies using filters. Alani, H., Kagal, L., Fokoue, A., Groth, P. T.,
Biemann, C., Parreira, J. X., Aroyo, L., Noy, N. F., Welty, C., & Janowicz, K. (Eds.),
International Semantic Web Conference (1), Vol. 8218 Lecture Notes Computer
Science, pp. 314330. Springer.
Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering Description Logic EL using relational database system. Boutilier, C. (Ed.), IJCAI
2009, Proceedings 21st International Joint Conference Artificial Intelligence,
Pasadena, California, USA, July 11-17, 2009, pp. 20702075.
Marnette, B. (2009). Generalized schema-mappings: termination tractability.
Paredaens, J., & Su, J. (Eds.), PODS, pp. 1322. ACM.
Mora, J., Rosati, R., & Corcho, O. (2014). kyrie2: Query rewriting extensional
constraints ELHIO. Mika, P., Tudorache, T., Bernstein, A., Welty, C., Knoblock,
C. A., Vrandecic, D., Groth, P. T., Noy, N. F., Janowicz, K., & Goble, C. A. (Eds.),
Semantic Web - ISWC 2014 - 13th International Semantic Web Conference, Riva
del Garda, Italy, October 19-23, 2014. Proceedings, Part I, Vol. 8796 Lecture Notes
Computer Science, pp. 568583. Springer.
Ortiz, M., Calvanese, D., & Eiter, T. (2008). Data complexity query answering expressive Description Logics via tableaux. J. Autom. Reasoning, 41 (1), 6198.
Ortiz, M., Rudolph, S., & Simkus, M. (2011). Query answering Horn fragments
Description Logics SHOIQ SROIQ. Walsh, T. (Ed.), IJCAI 2011, Proceedings 22nd International Joint Conference Artificial Intelligence, Barcelona,
Catalonia, Spain, July 16-22, 2011, pp. 10391044. IJCAI/AAAI.
Perez, J., Arenas, M., & Gutierrez, C. (2010). nSPARQL: navigational language RDF.
Web Semant., 8 (4), 255270.
Perez-Urbina, H., Motik, B., & Horrocks, I. (2010). Tractable query answering rewriting
Description Logic constraints. J. Applied Logic, 8 (2), 186209.
Rodriguez-Muro, M., & Calvanese, D. (2012). High performance query answering
DL-Lite ontologies. Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), Principles
Knowledge Representation Reasoning: Proceedings Thirteenth International
Conference, KR 2012, Rome, Italy, June 10-14, 2012. AAAI Press.
Rosati, R. (2007). conjunctive query answering EL. Calvanese, D., Franconi, E.,
Haarslev, V., Lembo, D., Motik, B., Turhan, A.-Y., & Tessaris, S. (Eds.), Description
Logics, Vol. 250 CEUR Workshop Proceedings. CEUR-WS.org.
Rudolph, S., & Glimm, B. (2010). Nominals, inverses, counting, conjunctive queries or:
infinity friend!. J. Artif. Intell. Res. (JAIR), 39, 429481.
704

fiThe Complexity Answering CQs GXQs OWL 2 EL KBs

Simanck, F. (2012). Elimination complex rias without automata. Kazakov, Y.,
Lembo, D., & Wolter, F. (Eds.), Proceedings 2012 International Workshop
Description Logics, DL-2012, Rome, Italy, June 7-10, 2012, Vol. 846 CEUR
Workshop Proceedings. CEUR-WS.org.
Sirin, E., Parsia, B., Cuenca Grau, B., Kalyanpur, A., & Katz, Y. (2007). Pellet: practical
OWL-DL reasoner. J. Web Sem., 5 (2), 5153.
Stefanoni, G., Motik, B., & Horrocks, I. (2013). Introducing nominals combined
query answering approaches EL. desJardins, M., & Littman, M. L. (Eds.),
AAAI. AAAI Press.
ter Horst, H. J. (2005). Completeness, decidability complexity entailment RDF
Schema semantic extension involving OWL vocabulary. Web Semantics:
Science, Services Agents World Wide Web, 3 (2-3), 79115.
Tsarkov, D., & Horrocks, I. (2006). FaCT++ Description Logic reasoner: System description. Furbach, U., & Shankar, N. (Eds.), IJCAR, Vol. 4130 Lecture Notes
Computer Science, pp. 292297. Springer.
Urbani, J., van Harmelen, F., Schlobach, S., & Bal, H. E. (2011). QueryPIE: Backward
reasoning OWL Horst large knowledge bases. Aroyo, L., Welty, C.,
Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy, N. F., & Blomqvist, E. (Eds.),
International Semantic Web Conference (1), Vol. 7031 Lecture Notes Computer
Science, pp. 730745. Springer.
Vardi, M. Y. (1982). complexity relational query languages (extended abstract).
Proceedings fourteenth annual ACM symposium Theory computing, STOC
82, pp. 137146, New York, NY, USA. ACM.
Venetis, T., Stoilos, G., & Stamou, G. B. (2012). Incremental query rewriting OWL 2 QL.
Kazakov, Y., Lembo, D., & Wolter, F. (Eds.), Proceedings 2012 International
Workshop Description Logics, DL-2012, Rome, Italy, June 7-10, 2012, Vol. 846
CEUR Workshop Proceedings. CEUR-WS.org.
Virgilio, R. D., Orsi, G., Tanca, L., & Torlone, R. (2012). NYAYA: system supporting
uniform management large sets semantic data. Kementsietsidis, A., & Salles,
M. A. V. (Eds.), IEEE 28th International Conference Data Engineering (ICDE
2012), Washington, DC, USA (Arlington, Virginia), 1-5 April, 2012, pp. 13091312.
IEEE Computer Society.
Wessel, M. (2001). Obstacles Way Qualitative Spatial Reasoning Description
Logics: Undecidability Results. Working Notes 2001 International
Description Logics Workshop (DL-2001), Vol. 49. CEUR-WS.org.

705

fiJournal Artificial Intelligence Research 51 (2014) 805-827

Submitted 5/14; published 12/14

Hidden Markov Model-Based Acoustic Cicada Detector
Crowdsourced Smartphone Biodiversity Monitoring
Davide Zilli
Oliver Parson
Geoff V Merrett
Alex Rogers

DZ 2 V 07@ ECS . SOTON . AC . UK
OSP @ ECS . SOTON . AC . UK
GVM @ ECS . SOTON . AC . UK
ACR @ ECS . SOTON . AC . UK

University Southampton
Southampton, SO17 1BJ, UK

Abstract
recent years, field computational sustainability striven apply artificial intelligence techniques solve ecological environmental problems. ecology, key issue
safeguarding planet monitoring biodiversity. Automated acoustic recognition
species aims provide cost-effective method biodiversity monitoring. particularly
appealing detecting endangered animals distinctive call, New Forest cicada.
end, pursue crowdsourcing approach, whereby millions visitors New
Forest, insect historically found, help monitor presence means
smartphone app detect mating call. Existing research field acoustic insect
detection typically focused upon classification recordings collected fixed field microphones. approaches segment lengthy audio recording individual segments insect
activity, independently classified using cepstral coefficients extracted recording
features. paper reports contrasting approach, whereby use crowdsourcing collect
recordings via smartphone app, present immediate feedback users whether
insect found. classification approach remove silent parts recording
via segmentation, instead uses temporal patterns throughout recording classify
insects present. show approach successfully discriminate call
New Forest cicada similar insects found New Forest, robust common types
environment noise. large scale trial deployment smartphone app collected 6000
reports insect activity 1000 users. Despite cicada rediscovered
New Forest, effectiveness approach confirmed detection algorithm,
successfully identified cicada app countries species
still present, crowdsourcing methodology, collected vast number recordings
involved thousands contributors.

1. Introduction
field computational sustainability, seeks apply computer science artificial intelligence issues sustainability, received great attention recent years planet
ever stronger environmental, societal economical pressure (Quinn, Frias-Martinez, & Subramanian, 2014; Gomes, 2009). Work field striven bring artificial intelligence research
real world, implementing practices promote sustainability environment
safeguard living organisms. Towards goal, first step monitoring biodiversity,
variety living species given environment. Biodiversity key measure
health ecosystem, land-use climate change impact natural environment,
c
2014
AI Access Foundation. rights reserved.

fiZ ILLI , PARSON , ERRETT & ROGERS

Figure 1: Cicadetta montana. Photograph Jaroslav Maly, reproduced permission.

many countries increasingly seeing need monitor protect it. example, UK
formalised endeavour within UK Biodiversity Action Plan established priority
species list focus work small number critically important species (Joint Nature Conservation Committee, 2010). One these, particular interest paper, New Forest cicada
(Cicadetta montana s. str., see Figure 1), native cicada known UK, first
identified New Forest, national park south coast England, 1812. Despite
well studied number sites 1960s, confirmed observation New
Forest cicada last 20 years (Pinchen & Ward, 2002). Understanding whether simply
due migration cicada yet undiscovered sites, whether cicada extinct
UK due climate change land-use change, important question UK biodiversity
research.
Today, traditional approaches searching rare species typically require trained ecologists
perform detailed manual surveys. However, obvious costs work led significant
recent research automated approaches whereby animals plants classified remotely
without requiring trained experts field. case insects, often
performed deploying fixed sensors sensitive microphones record sounds (or calls)
emitted insects (MacLeod, 2007). recordings analysed later automatically
identify insects whose calls captured. algorithms classification typically range
operate solely time domain, time domain signal coding (Chesmore,
2004; Chesmore & Ohya, 2004), inspired literature human speech recognition
(for example Potamitis, Ganchev, & Fakotakis, 2006; Pinhas, Soroker, Hetzoni, Mizrach, Teicher, &
Goldberger, 2008). latter typically use Gaussian mixture model hidden Markov model
classification (Leqing & Zhen, 2010), perform number pre-processing stages, often taken
directly human speech recognition literature, extract features raw recording.
example, Chaves, Travieso, Camacho, Alonso (2012) present state-of-the-art approach
pre-processes recorded sound remove un-sounded periods insect call detected,
mapping raw frequencies mel scale, better represents human hearing.
approach converts mel scale features back pseudo-time domain, called cepstrum,
calculating number mel frequency cepstral coefficients (MFCC), used features
806

fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE

hidden Markov model (HMM) classification. approaches shown classify
insects high levels accuracy clean recordings collected using purpose-built hardware.
use automatic acoustic recognition particularly appealing case New
Forest cicada, since insect particularly loud high-pitched mating song which,
close upper frequency limit normal adults hearing range inaudible adults
40 years age, easily detected conventional microphones. However, use
fixed sensors collect recordings later analysis less compelling. New Forest covers
600 km2 , would require tens thousands sensors exhaustively survey potential
cicada breeding sites. Furthermore, since cicada emits mating call months
June July, approach must able survey large area short space time,
decreasing applicability fixed sensors.
address challenge, pursue different approach, aims exploit 13 million day visits New Forest occur year general public crowdsource
search New Forest cicada using smartphone app. involvement general public
collection observations natural environment means recent practice,
records farmers clergymen devoted activity date back centuries (Miller-Rushing,
Primack, & Bonney, 2012; Brenna, 2011). start structured collection data, similar
know today practice citizen science, attributed beginning
20th century, events Christmas Bird Count foundation American Association Variable Star Observers 1911 (Silvertown, 2009). However, Internet made
remote communication collaboration far easier, widespread adoption smartphones
greatly facilitated cooperation amateur scientists around world collect process
large amounts data. ecology, method vehicle wide participation
citizens plethora different initiatives proliferated last decade (see example
survey paper, Dickinson, Zuckerberg, & Bonter, 2010). example iRecord Ladybirds app (Nature Locator, 2013), system allows users collect geo-located photographs
ladybirds helps identify correct species series morphological taxonomic questions (e.g. colour number spots). Records stored database presented
public page. However, app attempt automate analysis classification
process, outsourcing task entirely users. recently, automation classification process portable device attempted birds bats (Jones, Russ, Catto,
Walters, Szodoray-Paradi, Szodoray-Paradi, Pandourski, Pandourski, & Pandourski, 2009).
former, due difficulty differentiating calls, work still progress date
deployed prototype exists. latter, project called BatMobile (Nature Locator, 2012) starting implement automated detection Apple iOS devices, requirement expensive
ultrasonic microphones hinders accessibility tool general public large scale.
system propose paper therefore, best knowledge, first deployed
real-time acoustic species recognition system run entirely mobile device.
However, crowdsourcing acoustic biodiversity using smartphone app presents number
challenges. Firstly, smartphones expected collect short recordings user
waiting (30 seconds case), contrast always-on recordings collected fixed
sensors. fixed sensors would generate much longer recordings (in order hours
days) result, existing classification methods required automatically remove silent
periods recording. side effect, also remove useful time-domain information
used easily differentiate insects similar frequency calls, especially lower
807

fiZ ILLI , PARSON , ERRETT & ROGERS

quality recordings smartphone. makes existing methods unsuitable purpose.
Furthermore, smartphone app would require algorithm provides real-time feedback
user identification insect heard. allows user requested
collect recording cicada detected, conversely user required upload
unnecessary recordings cicada detected1 . However, low-end mobile devices limited
processing capabilities compared high-end servers, therefore previously proposed
complex feature extraction methods suitably efficient run real-time. addition,
essential acoustic cicada detector able discriminate call New
Forest cicada insects commonly found New Forest. Two examples insects
similar calls dark bush-cricket, whose call similar pitch New Forest cicada
instead chirps duration typically 0.1 seconds; Roesels bush-cricket,
whose call similar duration New Forest cicada covers broader frequency band.
Although scenario involves detection relatively insects compared existing work,
challenge design approach deployed field via low cost hardware
rediscovery New Forest cicada.
Therefore, paper present algorithm, call Cicada Detection Algorithm
(CDA), specifically intended real-time detection New Forest cicada computationally
constrained smartphones. Rather calculating number cepstral coefficients existing
work, use Goertzel algorithm calculate magnitude specific frequency bands,
efficient method approximating individual terms discrete Fourier transform (DFT)
(Goertzel, 1958). extract following three frequency bands: first centred 14 kHz
corresponding strongest frequency component calls New Forest cicada
dark bush-cricket, second centred 19 kHz, dark bush-cricket Roesels
bush-cricket still present, cicada not, third centred 8 kHz, far
general background noise (mostly lower 5 kHz) insects calls.
calculate following two features form input hidden Markov model: ratio
14 kHz 8 kHz distinguish New Forest cicada white noise across
frequencies, ratio 19 kHz 14 kHz distinguish New Forest
cicada dark bush-cricket. Next, use five-state hidden Markov model explicitly
represents idle period insect calls, calls New Forest cicada, dark bush-cricket
Roesels bush-cricket, also short pauses dark bush-crickets call. Hence,
rather attempting independently classify individual segments insect calls using complex
set features, exploit temporal patterns present throughout recording using HMM.
use Viterbi algorithm identify likely sequence insect calls throughout
recording.
evaluate approach using 235 recordings 30 seconds duration collected
New Forest Slovenia (where species cicada still present). Unlike standard
library recordings, data set represents range crowdsourced data likely
encounter, exhibiting significant noise (e.g. handling noise, road traffic noise, human voice
noise generated wind), insect calls varying amplitude depending proximity
recording device specimen. show approach able classify call
New Forest cicada normal environmental conditions F1 score 0.82. Since existing
approaches designed batch processing significantly longer recordings, compare
1. 30s mono recording 44,100 samples per second, 2.7MB; significant file upload areas poor
mobile phone reception connection rates may 100kbps less.

808

fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE

Amplitude

Freq (kHz)

20
15
10
5

0
1
0

-1

00:05

00:10

00:15

00:20
00:25
Time (mm:ss)

00:30

00:35

00:40

Figure 2: Spectrogram waveform New Forest cicada call (recording Jim Grant, 1971
courtesy British Library, wildlife sounds collection).

approach three variants order evaluate benefit various components pipeline.
results show feature extraction procedure robust noise, call Roesels
bush-cricket call dark bush-cricket, therefore satisfies requirements
deployment environment.
algorithm implemented mobile app, developed iOS Android,
downloaded 1500 times members public. culminated large-scale trial deployment, citizen scientists submitting 6000 reports worldwide. Although New Forest
cicada found first phase, accuracy detection algorithm wide geographical coverage achieved via crowdsourcing clearly motivate second phase deployment.
approach also applied monitoring many singing species, app
recognise British Orthoptera currently development.
preliminary version proposed method also compared state-of-the-art
approach batch classification insects proposed Chaves et al. (2012). comparison,
presented Zilli, Parson, Merrett, Rogers (2013), shows method considerably
computationally efficient, therefore better suited real-time operation. method proposed
paper improved accuracy efficiency presented Zilli et al.
remainder paper organised follows. Section 2 describe proposed
approach, highlighting different techniques used. Section 3 analyse performance using
hundreds smartphone recordings. Section 4 present first phase deployment
approach smartphone application, analyse coverage reports collected date. Finally,
conclude Section 5 along plans second phase deployment ensure
complete coverage New Forest.

2. Real-Time Insect Detection Using Hidden Markov Models
give description proposed approach real-time insect detection. first describe
efficient method individual terms DFT extracted raw audio recordings
using Goertzel algorithm. describe two features calculated three
individual terms DFT produce feature vector discriminate insects
interest also robust environment noise. Next, formalise classification ex809

fiZ ILLI , PARSON , ERRETT & ROGERS

tracted features inference problem HMM. Last, propose five-state HMM designed
specifically capture temporal patterns insects calls.
2.1 Feature Extraction Using Goertzel Algorithm
observed strong high frequency components calls insects interest.
frequencies sufficiently distant common background noise, wind noise, road
traffic people speaking, reliable indicator presence insect. Figure 2 shows
example frequency component, call New Forest cicada centred
14 kHz. efficient approximation magnitude frequencies calculated using
Goertzel algorithm; method evaluates individual terms DFT, implemented second
order infinite impulse response filter.
efficient implementation Goertzel algorithm requires two steps. first step produces
coefficient pre-computed cached reduce CPU cycles:

c = 2 cos

2f
fs


(1)

f central frequency question fs sampling rate recording.
second step consists iteratively updating values temporary sequence
incoming sample sn that:
yn = hamming(sn ) + (c yn1 ) yn2

(2)

samples passed Hamming filter, given by:

hamming(sn ) = 0.54 0.46 cos

2sn
N 1


(3)

length sequence samples N determines bandwidth B Goertzel filter,
that:
fs
B=4
(4)
N
shorter sequence length N yields larger bandwidth, cost noisier output.
practice, use multiples 128 samples match typical smartphones audio recording buffer
size. example, block size N = 128 samples gives bandwidth 1.4 kHz.
magnitude frequency band centred f bandwidth B time slice given
by:
q
2 + y2
(5)
mt,f = yN
N 1 c yN yN 1
terms computational complexity, approach shows considerable benefit compared
single-bin DFT. efficient algorithm compute latter, fast Fourier transform,
complexity O(N logN ), Goertzel algorithm order O(N ), N
number samples per window. Moreover, sample update described Equation 5 processed real-time, eliminating need independent background thread smartphone
app need store sample values.
810

fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE

z1

z2

z3

zT

x1

x2

x3

xT

Figure 3: hidden Markov model. Unshaded square nodes represent hidden discrete variables,
shaded circular nodes represent observed continuous variables. xt vector two
features xt,1 xt,2 .

2.2 Feature Combination Using Filter Ratio
magnitude frequency component 14 kHz good indicator presence
New Forest cicada, robust background noise, normally contained lower
5 kHz frequency spectrum. However, may sensitive white noise covers entire
spectrum, handling noise. Furthermore, able discriminate
calls New Forest cicada Roesels bush-cricket, exhibit prolonged
call similar frequency. Therefore, extract following three frequencies using Goertzel
algorithm: mt,8 represents 8 kHz frequency outside range cicada
call environmental noise, mt,14 represents 14 kHz frequency New Forest
cicada dark bush-cricket, mt,19 represents 19 kHz frequency dark
bush-cricket Roesels bush-cricket. take ratios frequencies produce two
features:
mt,14
mt,19
xt,1 =
, xt,2 =
(6)
mt,8
mt,14
such, point t, xt,1 high presence insects considered
tend one either sound detected cicada range sound present across
bands. addition, xt,2 high presence dark bush-cricket, tend
zero presence New Forest cicada. two features form -by-2 feature vector
used classification model. order obtain real-time computationally efficient
insect identification, adopt HMM-based approach classification described following
section.
2.3 Classification Using Hidden Markov Model
HMM consists Markov chain discrete latent variables sequence continuous observed variables, dependent upon one discrete variables state (Ghahramani, 2001).
Figure 3 shows graphical structure HMM, discrete, hidden variables (e.g. idle,
cicada singing) represented sequence z1 , . . . , zT , continuous, observed variables
(the features extracted audio recording) represented sequence x1 , . . . , xT .
value discrete variable zt corresponds one K states, continuous variable
take value real number.
behaviour hidden Markov model completely defined following three parameters. First, probability state hidden variable = 1 represented vector
811

fiZ ILLI , PARSON , ERRETT & ROGERS

that:
k = p(z1 = k)

(7)

Second, transition probabilities state 1 state j represented
matrix that:
Ai,j = p(zt = j|zt1 = i)
(8)
Third, emission probabilities describe observed feature, x, given parameters ,
case follow log-normal distribution that:
xt,f |zt , ln N (zt , z2t )

(9)

= {, 2 }, zt z2t mean variance Gaussian distribution
state zt . Figure 4 shows histogram data generated cicadas song, along log-normal
distribution fitted data. log-likelihood ratio test normal, log-normal exponential
distributions fitted data set cicada songs shows log-normal distribution matches
data better normal (F = 3512.13, p < 0.001) exponential (F = 1516.06, p < 0.001)
distributions. However, despite long tail, log-normal distribution still poor support
data unusually high magnitude, often generated handling noise. order prevent
model strongly favouring certain state data point extreme lognormal distribution, cap emission probabilities capture cases data likely
poorly represented model. outcome likelihood data
points result correct state may low model triggers state change even though
transition probability strongly discourages (by low). Therefore, cap
emission probability data points maximum ratio, initially 100,
state preferred another.
Equations 7, 8 9 used calculate joint likelihood hidden Markov
model:




p(x, z|) = p(z1 |)
p(zt |zt1 , A)
p(xt |zt , )
(10)
t=2

t=1

model parameters collectively defined = {, A, }.
use Viterbi algorithm (Viterbi, 1967) infer likely sequence hidden states
given features described. Despite fact number possible paths grows exponentially length chain, algorithm efficiently finds probable sequence
maximising Equation 10, cost grows linearly length chain.
2.4 5-State Finite State Model Insect Call
propose five-state HMM cicada detection, states consist of: idle state
insect singing (I), cicada singing state (C), state dark bush-cricket
chirping (DC ), short pause dark bush-crickets chirps (DSP ) state
Roesels bush-cricket calling (R). emission parameters, i.e. location scale b
log-normal distribution, learned empirically using:

!

2
2
= ln p
, b = ln 1 + 2
(11)

2 + 2
812

fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE

Log-normal emission
probabilities

Probability density function

Empirical data
cicada call

Feature value (mt,14 /mt,8 )

Figure 4: Log-normal distribution extracted feature cicada call


R


DC

DSP



C

Idle state


Dark bush-cricket
DC Dark bush-cricket's chirp
DSP Dark bush-cricket's short pause
R

Roesel's bush-cricket

C

New Forest cicada

(a) 5-state model used approach.

R

C

(b) 3-state model.

Figure 5: Comparison finite state machines.

represents mean 2 represents variance data. manual estimation
originally based recordings authors gathered historical archives,
therefore improved recordings obtained deployment work, described
following section.
transition matrices describing dynamics Markovian process represented
graphically using finite state machines. Figure 5a shows five states described possible transitions, non-zero probability represented arrows connecting two
states. model explicitly represents silence dark bush-crickets chirps,
essential information distinguishing calls New Forest cicada dark bushcricket. contrast existing batch classification methods remove silent periods
recording order improve computational cost operation classify sounded
periods sample file (Chaves et al., 2012). methods also employ feature extraction process whereby compute number mel-frequency cepstral coefficients species
model, making process scalable several insects, cost higher computational complex813

fiZ ILLI , PARSON , ERRETT & ROGERS

ity. contrast, method proposed Section 2.1 closely tailored requirements
scenario, producing improvement efficiency necessary mobile application. Figure 5b shows variant approach silent states removed,
compare approach following section. Furthermore, HMM could arranged
fully-connected, allowing transitions states otherwise disconnected (for example Roesels Bush-cricket Dark bush-cricket). However, confuses model
states similar emission probabilities, without providing improvement
accuracy. therefore exclude variation comparison following section.

3. Empirical Evaluation Using Smartphone Recordings
introduce three variants approach described thus far that, following practices
literature, motivate choices made construct Cicada Detection Algorithm. first variant
uses approach proposed Section 2, three raw frequencies, opposed
ratio, used directly features (CDA raw frequencies). second variant removes un-sounded
periods recording and, such, segments individual calls. applies 3-state
model shown Figure 5b classify insects (CDA silence removed). third approach
apply HMM all, instead uses ratio frequencies directly identify likely
state, given instantaneous emission probabilities features. such, method
considered mixture model, since time slice classified independently. method
considerably computationally efficient, cost losing information time
domain.
evaluate accuracy approach using collection 235 recordings taken citizen
scientists using smartphones New Forest (the known UK habitat New Forest
cicada) authors paper Slovenia (where species cicada still present)
summer 2013. recording 30 seconds long, cases contains call
either New Forest cicada, dark bush-cricket Roesels bush-cricket. recordings
contain different types noise, including people speaking, walking, calls birds, handling noise
even people mimicking sound cicada. contrast existing recording libraries,
data set represents typical quality crowdsourced data, exhibiting significant noise insect
calls varying amplitude depending proximity recording device specimen.
recording later labelled domain experts containing either one none insects
interest. Although multiple insects recordings make classification fail,
consider one singing insect per recording. one present, set ground truth
across 30-second recording longest loudest singing insect, therefore taking state
active longest period outcome model. Since emission probabilities
model purposely tuned, require training data such, hence use entire
data set test data. describe deployment smartphone app used collect data
detail Section 4.
assess accuracy approach correctly classify cicada using
standard precision, recall F1 score metrics. precision represents fraction recordings
approach detected cicada singing fact singing, recall
represents fraction recordings cicada singing correctly detected.
814

fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE

Approach
CDA
CDA raw frequencies
CDA silence removed
Mixture model

Precision
0.66
0.46
0.62
0.61

Recall
0.78
0.94
0.99
0.65

F1 -score
0.82
0.62
0.75
0.67

Table 1: Accuracy metrics cicada detection


C





R

C





R

C



R









C

C

C









R

R

R

R

(a) CDA

C



R




(b) CDA raw frequencies (c) CDA silence removed

C

(d) Mixture model

Figure 6: Confusion matrices four variants detection algorithm. y-axis,
actual class; x-axis, predicted class.
Precision recall defined as:
precision =

tp
,
tp + f p

recall =

tp
tp + f n

(12)

tp represents number correct cicada song detections, f p represents number
cicada song detections actually singing, f n represents number cicada
songs detected. work primarily concerned detection New
Forest cicada, insects modelled order avoid false positive detections
New Forest cicada. also use F1 score, represents weighted combination
precision recall, defined as:
F1 = 2

precision recall
precision + recall

(13)

Table 1 shows precision, recall F1 score metrics approach compared three
variants data set recordings New Forest Slovenia. Similarly, Figure 6
reports true false positives, real values along axis predicted class along x
axis. seen approach (CDA) achieves F1 score 0.82, outperforms
benchmark variant, visually apparent darkness along main diagonal Figure 6a.
contrast, variant approach uses raw frequency measurements HMM
feature vector (CDA raw frequencies) receives F1 score 0.62. result approachs
lack robustness noise, handling noise, shown high number false positives
Figure 6b. Furthermore, variant approach removes silent periods (CDA silence
removed) receives F1 score 0.75. Although appears positive result, Figure 6c highlights
815

fiZ ILLI , PARSON , ERRETT & ROGERS

lack ability discriminate dark bush-cricket New Forest cicada.
method, well raw frequencies approach, favour New Forest cicada, scoring good
true positive rate consequently also high false postive rate. Finally, mixture model method
receives F1 score 0.67 lack transition probabilities leaves decision
emission probabilities only, utilising information contained time domain, making
number true false positives equally distributed (Figure 6d). Insects similar
emission probabilities, Roesels bush-cricket dark bush-cricket, therefore
difficult classify method. noted however approach considerably
computationally efficient, decides likely state instantaneously without
traversing entire recording.
Figures 7, 8, 9 10 show comparison four approaches sample recording
four species recordings analysed. top plot figure shows spectrogram
time domain x-axis, frequency domain y-axis, magnitude
frequency bins varying colour plot. Subsequently, figure shows
likely state identified approach. plot, states labelled Figure 5a,
represents un-sounded idle state (if present), C represents cicadas song, R represents
Roesels bush-cricket DC DSP dark bush-crickets chirping short pause states,
respectively. gaps silence-removed variant correspond unsounded periods.
Figure 7 shows classifying cicada easier HMM-based methods, call lasts
long period without interruption clearly distinct background noise. noisy
recording would cause raw-frequency approach fail. mixture model approach struggles
distinguish cicada dark bush-cricket call, since similar features
different time domain, model capture. Figure 8 shows variants
sensitive noise CDA different reasons. raw frequencies approach doesnt
filter background noise, mixture model triggers cicada state even short
noise right frequency band. silence-removed method active short period
higher background noise, idle state, forced classify sound
sounded states. Figure 9 shows how, silence removed, Roesels bush-cricket becomes
similar dark bush-cricket, similar emission probabilities. perception
observed mixture model, doesnt perception time. Similarly, Figure 10 shows
dark bush-cricket difficult classify mixture model approach silence
removed, explained thus far. Moreover, shows trade-off quiet insect
(visible throughout recording) insect must made, insect could distance
microphone, thus limit quiet may be.
analysis 235 recordings presented detail projects web page, together parameters HMM, audio file, information recording device2 .

4. Automated Classification Smartphone App
deployed insect detection algorithm within smartphone app enables wide participation
search critically endangered species. process, often referred citizen science,
attempts leverage widespread presence users willing participate act
distributed network sensors, learning scientific process behind certain research,
2. Result http://www.newforestcicada.info/devdash. data used free charge, provided
New Forest Cicada Project attributed according Creative Commons Attribution (BY) licence.

816

fiMost likely state

Freq (kHz)

N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE

20

Spectrogram

15
10
5
0
R
C
DC
DSP


CDA

CDA
raw frequencies

R
C
DC
DSP


CDA
silence removed

R
C


Mixture model

R
C
DC
DSP


likely state

Freq (kHz)

Figure 7: Model comparison New Forest cicada recording

20

Spectrogram

15
10
5
0
R
C
DC
DSP


CDA

CDA
raw frequencies

R
C
DC
DSP


CDA
silence removed

R
C


Mixture model

R
C
DC
DSP


Figure 8: Model comparison recording singing insect

817

fiMost likely state

Freq (kHz)

Z ILLI , PARSON , ERRETT & ROGERS

20
15
10
5

Spectrogram

0
R
C
DC
DSP


CDA

R
C
DC
DSP


CDA
raw frequencies

R
C

CDA
silence removed


R
C
DC
DSP


Mixture model

likely state

Freq (kHz)

Figure 9: Model comparison Roesels bush-cricket recording

20

Spectrogram

15
10
5
0
R
C
DC
DSP


CDA

CDA
raw frequencies

R
C
DC
DSP


CDA
silence removed

R
C


Mixture model

R
C
DC
DSP


Figure 10: Model comparison dark bush-cricket recording

818

fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE

Possible cicada
detected
Save audio
recording
Live detection

Record audio

Sounds interesting
Upload report
cicada
detected

Figure 11: Flow detection classification process app.
case automated identification species biodiversity monitoring. Examples
communities app caters include tourists visitors New Forest National Park,
local residents bug enthusiasts. order maximise number citizen scientists taking
part, essential app compatible wide range hardware, addition
simple unobtrusive use. Therefore, released iPhone Android client,
ensures compatibility 80% smartphone users (Go-Gulf, 2012). Furthermore,
designed app simple use, require user consent recording audio constrain
usage battery mobile data usage.
Figure 11 shows overview flow interaction user takes recording
app. user first opens app presented live detection screen,
displays graphical representation audio signal entering microphone, form
circular spectrogram, immediate feedback presence singing cicada, obtained
output mixture-model described Section 2. Upon selecting start audio
recording, user shown current progress 30 second recording. completing
recording, CDA run user presented one three possibilities: possible
cicada detected screen, sounds interesting screen (which notes algorithm detected
insect cicada), cicada detected screen (where nothing known found).
report survey saved locally uploaded upon connecting Internet. recording
contains known insects, user asked consent upload recorded audio.
4.1 Stages Real-Time Classification
order capture sound fed automated classifier, user presented intuitive
interface, summarised Figures 12 13 detailed follows:
4.1.1 L IVE ETECTOR
Figure 12a shows detector screen, appears upon loading app. crucial difficulty
human detect New Forest cicadas call fact pitch high people
hear, since central frequency limit hearing range average 40 year old.
address issue, tab presents visualisation sound drawn circular spectrogram.
centre, cicada logo lights call detected, triggered instantaneous output
mixture model described Section 2, updated every 128 samples microphone. Twenty
819

fiZ ILLI , PARSON , ERRETT & ROGERS

(a) Live detector

(b) Audio recording

(c) Upload recording

Figure 12: Three screens Cicada Hunt Android. left, cicada singing lights
icon frequency bands around 14 kHz. middle, cicada singing survey
stopped shortly 15 seconds. right, latest survey waiting uploaded.
concentric circles around represent twenty frequency bands spectrum, centred 1
20 kHz bandpass 1.4 kHz, extracted 20 Goertzel filters, ensure rapid updating
interface. becomes brighter higher signal strength (i.e. louder sound
pitch) paler band quieter. outer bands, roughly 12 18 kHz,
triggered cicada call, producing distinctive effect shown Figure 12a. Tapping
cicada icon centre app starts 30-second survey, sound recorded
analysed algorithm described Section 2. idea core interface,
encourages users stop wait silence, thus maximising chance detecting required
sound. choice 30 seconds strikes balance length cicada call
amount time usermostly occasional visitor forestcan expected stand still
silence.
4.1.2 AUDIO R ECORDING
Figure 12b shows screen shown 30 second audio recording. recording
finished, audio analysed HMM-based algorithm described Section 2. Depending
result classification, user shown either cicada detected screen, sounds
interesting screen possible cicada detected screen.
4.1.3 U PLOAD R ECORDING
Figure 12c shows list reports saved locally. report geo-tagged
time-stamped, saves unique identifier phone well basic information
device. report also saves uncompressed 44.1 kHz 16 bit PCM WAV sound recording
820

fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE

(a) cicada detected

(b) Sounds interesting

(c) Possible cicada detected

Figure 13: Three possible outcomes CDA, known insect found, insect
cicada found, cicada found.
case either cicada another insect found, provided user granted permission
so. Since audio recording requires 2.7 MB disk, deleted smartphone soon
report sent server minimise storage space required smartphone. Last,
low-resolution spectrogram saved cases, constructed combination output
20 Goertzel filters 30 seconds survey, saved every 128 samples. constitutes easiest
way human check presence cicada avoids privacy concerns (speech
could reconstructed spectrogram). Moreover, payload image file,
saved Base64 (Josefsson, 2006), around 15 KB therefore much lighter raw sound
recording. Internet connection becomes available, report uploaded projects
servers, available research team analyse further.
4.1.4 N C ICADA ETECTED
Figure 13a shows screen shown nothing detected. fact cicada, habitat,
New Forest technology behind app shown provide informative notion, encouraging
user try again. intends support morale user receiving negative
results, provide educational content citizen scientist receives information
exchange work performed.
4.1.5 OUNDS NTERESTING
Figure 13b shows screen displayed another insect detected, whose call similar
New Forest cicada. present, app encompasses two insects present New
Forest: dark bush-cricket Roesels bush-cricket. user shown spectrogram
typical call insects, well spectrogram recorded,
821

fiZ ILLI , PARSON , ERRETT & ROGERS

Device
iPhone 4
iPhone 5
iPhone 4S
iPhone 3
HTC Desire
Xperia Mini
Moto A953
Galaxy S3
Xperia Z
HTC One
Nexus 4
HTC Desire X
Galaxy Ace 2
Galaxy S2
Nexus One
HTC One X
HTC Wildfire

OS
iOS
iOS
iOS
iOS
Android
Android
Android
Android
Android
Android
Android
Android
Android
Android
Android
Android
Android

Filtered














Yes
Yes


Silence (SEM)
1.623 (0.075)
1.897 (0.076)
1.466 (0.050)
1.469 (0.047)
0.844 (0.041)
2.480 (0.155)
2.015 (0.104)
1.374 (0.038)
0.951 (0.032)
1.466 (0.040)
0.675 (0.025)
1.243 (0.054)
1.953 (0.063)
1.916 (0.085)
1.514 (0.051)
1.933 (0.062)
2.032 (0.088)

Cicada (SEM)
13.047 (0.327)
14.793 (0.388)
10.549 (0.337)
10.539 (0.430)
4.255 (0.265)
10.190 (0.262)
5.845 (0.148)
3.279 (0.088)
1.971 (0.059)
2.915 (0.085)
1.314 (0.026)
1.817 (0.075)
2.162 (0.059)
2.101 (0.031)
1.568 (0.045)
1.732 (0.052)
1.683 (0.063)

Ratio (SEM)
8.041 (0.442)
7.800 (0.373)
7.196 (0.336)
7.173 (0.372)
5.041 (0.397)
4.109 (0.277)
2.901 (0.167)
2.387 (0.093)
2.072 (0.094)
1.988 (0.079)
1.946 (0.081)
1.462 (0.087)
1.107 (0.047)
1.097 (0.051)
1.036 (0.046)
0.896 (0.040)
0.828 (0.047)

Table 2: Comparison popular smartphone devices. Values means ratios 14
8 kHz Goertzel filters, sampled every 3 ms (128 samples 44,100 kHz). Standard error
mean (SEM) given brackets.
asked select insect recording looks similar. promotes involvement
user process, would otherwise passively observing detection performed
smartphone.
4.1.6 P OSSIBLE C ICADA ETECTED
Figure 13c shows message informing user discovery cicada. Since algorithm
tricked recording actual call, detection presented possible.
4.2 Evaluation Microphones Frequency Response
Prior deployment, noted smartphones equally capable detecting
cicada. tests reveal smartphones equipped microphone considerably
sensitive others. tested range different devices reproducing four types sound
least 2 seconds each: silence, white noise, frequency sweep 50 20,000 Hz, cicada
call. reproduced custom-built sound-proof chamber, placed quiet location,
Visaton KE 25 SC 8 Ohm tweeter producing four test sounds. phones arranged
microphone facing speaker equally distant it. experiences recording
cicada calls Slovenia, sound volume calibrated volume cicada call
equivalent likely detected wild. synthetic white noise frequency sweep
tuned match maximum amplitude cicada call.
report comparison sensitivity microphones based well detect
cicada call test environment. Table 2 summarises outcome test, reporting
822

fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE

Ampl.

(a)

Freq (kHz)

20
15
(b) 10
5
0
20
15
(c) 10
5
0
20
15
(d) 10
5
0
20
15
(e) 10
5
0
00:05

00:10

Time (s)

00:15

00:20

00:25

Figure 14: Comparison three phones. top, waveform (a) spectrogram (b)
sample calibration file. bottom, sensitive iPhone 5 (c), Google Nexus 4 (d)
hardware-filtered HTC One X (e), top-end devices iOS Android.
ratio 14 kHz 8 kHz bands extracted Goertzel filter sound
played (marked Silence), cicada call played (Cicada), ratio
two. higher value latter means clearer indication cicada call, results
clearer separation log-normal distributions representing sounded unsounded states,
therefore greater confidence detection. seen models iPhone capture
call New Forest cicada accurately, Android phones exhibit wide range
performance. due operating system itself, rather varied range
hardware specifications common Android devices. Figure 14 shows reference sound played
phone, together three examples high-end devices; Apple iPhone 5, detects
cicada call clearly, Nexus 4, detects time, HTC One X,
despite sensitive microphone, uses low-pass frequency filter, therefore incapable
detecting insects call. confirmed divergent rank devices score Table 2.
4.3 Large-Scale Trial Deployment
smartphone app launched 8th June 2013 collected data end mating
season New Forest cicada. Since launch, 1000 citizen scientists submitted
6000 reports worldwide. these, least 1777 New Forest (over 1600 submitted
GPS fix acquired); New Forest reports, 162 classified either sounds
interesting potential cicada detected, result include 30 second audio recording.
citizen scientists submitted reports, 738 used iOS version app, 346 used
Android version.
Figure 15 shows bar graph number reports uploaded top 25 contributors,
trend top 100 users displayed top-right corner. noted among these,
5 entomologists authors paper. However, users covered specific areas
forest, particular cicada historically observed. contrast, citizen
scientists submitted much fewer reports per user, reports much evenly distributed
823

fiiOS
Android

Number Reports

247

200

236

164

100

150

0

78

63

54 52

1

Ranking

100

42 40 38 38 37 37 37 36 36 34
33 33 31 30 29 29

0

50

Number Reports

300

306 301

250

300

350

Z ILLI , PARSON , ERRETT & ROGERS

1

2

3

4

5

6

7

8 9

10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Ranking

Figure 15: Reports per user operating system top 25 users (right, trend top 100).

across New Forest, shown Figure 16. shows crucial difference distributed
approach make, entomologists cannot ubiquitously present different areas forest
conditions favourable, cover limited territory, visitors, though
contributing individually less, help rediscover cicada moved different sites,
currently suspected. time, entomologists tools knowledge
recognise insects calls, general public must equipped accessible method.
space, implementation deployment automated acoustic insect detection algorithm
succeeded bring public possibility contribute distributed monitoring insect
species, shown large number downloads app submitted reports.

5. Conclusions
paper presented novel algorithm designed specifically detect mating call
New Forest cicada. shown careful analysis call, key features
extracted minimal cost, greatly simplifying identification process. compared approach
three variants approach, method exists date best knowledge
automatically classify insects calls constrained platform, mobile phone. three
variants, one uses raw frequency components HMM feature vectors, second variant removes
silent periods recording third one classifies time slices independently based upon
emission distributions. results show approach achieves accuracy F1 = 0.82
detection New Forest cicada data set recordings collected New Forest
Slovenia using iPhone Android smartphones. recordings included various forms
background noise, insects calls human voices. Rather focusing batch processing
large data sets species, approach focused upon identification small number
species real time.
824

fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE

(a) New Forest

(b) UK

Figure 16: Map submitted surveys around New Forest across UK. area
New Forest corresponds green area centre.

development robust acoustic classifier complete, integrated technology
smartphone app iOS Android. large-scale deployment resulted collection
least 1777 reports New Forest, 162 detected call insect interest,
1000 citizen scientists. Although New Forest cicada successfully detected
two month mating season 2013, use app Slovenia confirmed accuracy
acoustic detector deployment New Forest attested suitability using citizen
scientists crowdsource collection audio reports via smartphones. app also used
expert entomologists Slovenia detect presence Cicadetta montana.
future work consist second deployment smartphone app two
month mating season New Forest cicada, aim achieving greater coverage
New Forest, require mobilisation larger community citizen scientists
cover areas New Forest yet surveyed. deployment
constitute largest survey New Forest cicadas habitat date, therefore provide
unprecedented insight existence endangered species. Moreover, app classification British Orthoptera also currently development, pose new set
challenges. fact, higher number different calls, selection distinctive features
HMM becomes difficult, may require sampling higher frequency, increasing
computational complexity approach. increase accuracy encompass wider
number devices, use techniques cepstral mean normalisation account
difference sensitivity microphones.
Since learning part algorithm completed offline, algorithm remains efficient
solution classify insects calls real time mobile device, may readily extended
calls different animals, insects birds. Preliminary work extension
started, goal adaptive acoustic classifier trained different
sound-emitting wildlife species.
825

fiZ ILLI , PARSON , ERRETT & ROGERS

Acknowledgements
research supported EPSRC Doctoral Training Centre grant (EP/G03690X/1)
ORCHID Project, www.orchid.ac.uk (EP/I011587/1). Many thanks Dr Tomi Trilar
Prof Matija Gogala field guidance Slovenia Dr David Chesmore original
suggestion ongoing support.

References
Brenna, B. (2011). Clergymen Abiding Fields: Making Naturalist Observer
Eighteenth-Century Norwegian Natural History. Science Context, 24(02), 143166.
Chaves, V. A. E., Travieso, C. M., Camacho, A., & Alonso, J. B. (2012). Katydids acoustic classification verification approach based MFCC HMM. Proceedings 16th IEEE
International Conference Intelligent Engineering Systems (INES), 561566.
Chesmore, E. D. (2004). Automated bioacoustic identification species. Anais da Academia
Brasileira de Ciencias, 76(2), 436440.
Chesmore, E. D., & Ohya, E. (2004). Automated identification field-recorded songs four
British grasshoppers using bioacoustic signal recognition. Bulletin Entomological Research, 94(04), 319330.
Dickinson, J. L., Zuckerberg, B., & Bonter, D. N. (2010). Citizen Science Ecological Research
Tool: Challenges Benefits. Annual Review Ecology, Evolution, Systematics, 41(1),
149172.
Ghahramani, Z. (2001). Introduction Hidden Markov models Bayesian Networks.
Journal Pattern Recognition Artificial Intelligence, Vol. 15, pp. 942.
Go-Gulf (2012). Smartphone Users Around World Statistics Facts.
http://www.go-gulf.com/blog/smartphone, retrieved 19/07/2012.

On-line,

Goertzel, G. (1958). algorithm evaluation finite trigonometric series. American
Mathematical Monthly, 65(1), 3435.
Gomes, C. P. (2009). Computational Sustainability: Computational methods sustainable environment, economy, society. Bridge, 39(4), 513.
Joint Nature Conservation Committee (2010). UK priority species pages Cicadetta montana (New
Forest Cicada). Tech. rep..
Jones, K. E., Russ, J., Catto, C., Walters, C., Szodoray-Paradi, A., Szodoray-Paradi, F., Pandourski,
E., Pandourski, I., & Pandourski, T. (2009). Monitoring bat biodiversity: indicators sustainable development Eastern Europe Darwin Initiative Final Report. Tech. rep., Zoological
Society London.
Josefsson, S. (2006). base16, base32, base64 data encodings. RFC 4648, Standards Track.
Leqing, Z., & Zhen, Z. (2010). Insect Sound Recognition Based SBC HMM. International
Conference Intelligent Computation Technology Automation (ICICTA), Changsha,
China, Vol. 2, pp. 544 548.
MacLeod, N. (2007). Automated Taxon Identification Systematics: Theory, Approaches Applications. CRC Press.
826

fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE

Miller-Rushing, A., Primack, R., & Bonney, R. (2012). history public participation ecological research. Frontiers Ecology Environment, 10(6), 285290.
Nature Locator (2012).
10/04/2014.

BatMobile Project.

On-line, http://batmobile.blogs.ilrt.org, retrieved

Nature Locator (2013). iRecord Ladybirds Project. On-line, http://naturelocator.org/ladybird.html,
retrieved 10/04/2014.
Pinchen, B. J., & Ward, L. K. (2002). history, ecology conservation New Forest
Cicada. British Wildlife, 13(4), 258266.
Pinhas, J., Soroker, V., Hetzoni, A., Mizrach, A., Teicher, M., & Goldberger, J. (2008). Automatic
acoustic detection red palm weevil. Computer Electronics Agriculture, 63, 131
139.
Potamitis, I., Ganchev, T., & Fakotakis, N. (2006). Automatic acoustic identification insects
inspired speaker recognition paradigm.. Interspeech 2006, Pittsburgh, Pennsylvania,
pp. 21262129.
Quinn, J. A., Frias-Martinez, V., & Subramanian, L. (2014). Computational Sustainability
Artificial Intelligence Developing World. AI Magazine Special Issue Computational
Sustainability.
Silvertown, J. (2009). new dawn citizen science.. Trends ecology & evolution, 24(9),
46771.
Viterbi, A. (1967). Error bounds convolutional codes asymptotically optimum decoding
algorithm. IEEE Transactions Information Theory, 13(2), 260269.
Zilli, D., Parson, O., Merrett, G. V., & Rogers, A. (2013). Hidden Markov Model-Based Acoustic
Cicada Detector Crowdsourced Smartphone Biodiversity Monitoring. International
Joint Conference Artificial Intelligence, Beijing, China. AAAI Press.

827

fiJournal Artificial Intelligence Research 51 (2014) 779804

Submitted 09/14; published 12/14

Research Note
BDD Ordering Heuristics Classical Planning
Peter Kissmann
Jorg Hoffmann

KISSMANN @ CS . UNI - SAARLAND . DE
HOFFMANN @ CS . UNI - SAARLAND . DE

Saarland University, Saarbrucken, Germany

Abstract
Symbolic search using binary decision diagrams (BDDs) often save large amounts memory due concise representation state sets. decisive factor methods success
chosen variable ordering. Generally speaking, plausible dependent variables
brought close together order reduce BDD sizes. planning, variable dependencies typically captured means causal graphs, preceding work taken basis
finding BDD variable orderings. Starting observation two concepts dependency actually quite different, introduce framework assessing strength variable
ordering heuristics sub-classes planning. turns that, even extremely simple planning
tasks, causal graph based variable orders may exponentially worse optimal.
Experimental results wide range variable ordering variants corroborate theoretical
findings. Furthermore, show dynamic reordering much effective reducing BDD
size, cost-effective due prohibitive runtime overhead. exhibit potential
middle-ground techniques, running dynamic reordering simple stopping criteria hold.

1. Introduction
Finding good variable orderings important task many areas Artificial Intelligence,
constraint satisfaction problems (CSPs), SAT, planning (for heuristic search approaches,
especially applying symbolic search). many cases, efficient ordering determined
evaluating graphical representation underlying problem. CSPs, example,
constraint graph used determine variable ordering backtracking-based approaches.
Typical approaches take minimum width (Freuder, 1982), maximum degree, maximum cardinality (Dechter & Meiri, 1989) nodes constraint graph account. alternative
approach considers bandwidth constraint graph given ordering, maximal distance ordering two nodes adjacent graph; idea find
ordering minimizes bandwidth (Zabih, 1990).
SAT, widely used approach determine variable order conflict-driven clause learning
(CDCL) variable state independent decaying sum (VSIDS) (Moskewicz, Madigan, Zhao, Zhang,
& Malik, 2001). based weights propositional variables, i.e., often
variable occurs clauses. Recently, Rintanen (2012) noted applying SAT solvers
planning tasks, different ordering might efficient, giving better coverage typical
benchmarks international planning competition (IPC). ordering takes structure
planning tasks account, trying support (sub)goals early possible.
planning, variable dependencies typically represented causal graph (e.g., Knoblock,
1994; Jonsson & Backstrom, 1995; Brafman & Domshlak, 2003; Helmert, 2006), capturing variable
dependencies terms co-occurences action descriptions. kind graph turned
c
2014
AI Access Foundation. rights reserved.

fiK ISSMANN & H OFFMANN

useful great variety purposes, including problem decomposition (Knoblock, 1994),
system design (Williams & Nayak, 1997), complexity analysis (e.g. Jonsson & Backstrom, 1995;
Domshlak & Dinitz, 2001; Brafman & Domshlak, 2003; Katz & Domshlak, 2008; Gimenez &
Jonsson, 2008; Chen & Gimenez, 2010), derivation heuristic functions (Helmert, 2004, 2006),
search topology analysis (Hoffmann, 2011b, 2011a). purposes here, causal graphs
relevant application derivation variable orderings. done BDDs,
return detail below, well merge-and-shrink heuristics (Helmert, Haslum, &
Hoffmann, 2007; Helmert, Haslum, Hoffmann, & Nissim, 2014). merge-and-shrink, complete
variable ordering corresponds (linear) merging strategy, order variables merged
global abstraction. recent extension non-linear merging strategies (Sievers, Wehrle,
& Helmert, 2014), order merges instead given tree. merge tree bears
similarity concept vtrees, used generalization variable orderings
sentential decision diagram (SDDs) (Darwiche, 2011). Fan, Muller, Holte (2014) shown
efficient merge trees determined means causal graph. so, use MinCuts causal graph, putting two resulting sets variables two different branches
merge tree recursively continue subgraphs.
paper, concerned symbolic search based binary decision diagrams (BDDs)
(Bryant, 1986) optimal planning. variable ordering refers order variables
queried within BDDs, key ingredient practical efficiency approach.
planning, much work invested finding good variable orderings, model
checking, symbolic search originated (McMillan, 1993), many different variable ordering
schemes proposed past (e.g., Malik, Wang, Brayton, & Sangiovanni-Vincentelli,
1988; Minato, Ishiura, & Yajima, 1990). Again, many based evaluation
graphical representation problem. Often, bringing dependent variables close together
results smaller BDDs. straightforwardly applied planning, defining variable
dependencies via causal graph. exactly Gamer, state-of-the-art symbolic search
planner, determines variable ordering (Kissmann & Edelkamp, 2011).
starting point investigation feeling discomfort double use word
dependency above. causal graphs, dependency means corresponding
variables appear least one common action, changing value one variable may require
changing variable well. BDDs, hand, represent Boolean functions .
many assignments subset P variables immediately determine truth value ,
independently value variables, variables P grouped closely
together. planning, typically represents layer states sharing distance initial
state (forward search) goal (backward search). concept dependence relates
determining whether state member layer. What, anything,
causal graph dependencies?
conclusive answer question, contribute number insights
suggesting two concepts dependence much common. consider
issue theoretical practical perspective. theoretical side, introduce
simple formal framework assessing strength variable ordering heuristics sub-classes
planning. Applying framework causal graph based variable orders, show may
exponentially worse optimal orderings, even extremely simple planning tasks.
practical side, experiment wide range variable ordering schemes, several
ones based causal graph, also range techniques adapted model checking
780

fiBDD RDERING H EURISTICS C LASSICAL P LANNING

literature. get idea good ordering schemes are, grand scale things,
use upper lower delimiter. latter, use random variable orderings.
surpisingly, ordering schemes better random; surprisingly, are.
Indeed, Fast Downwards level heuristic (Helmert, 2006) turns much worse
average random BDD variable ordering.
upper delimiter, employ dynamic reordering techniques minimize BDD size
online, construction process. Compared static up-front variable ordering schemes,
reordering much better basis taking decisions, much time-consuming.
thus expected BDD size results much better. extent happens
experiments remarkable, however: Static orderings hardly ever even tiny bit better, whereas
advantage dynamic reordering easily frequently goes three orders magnitude.
successfully employed least one domain non-deterministic planning
(Cimatti, Pistore, Roveri, & Traverso, 2003), dynamic reordering usually prohibitively slow
cost-effective. Still, prowess reducing BDD size, combined pessimistic outlook
static ordering schemes, suggests may better alternative. initial experiment
indicates could, indeed, case: simple adaptive stopping criteria, running
dynamic reordering certain point, obtain better results static
ordering schemes.
remainder paper organized follows. Section 2 gives necessary background
planning framework use BDDs. Section 3 introduces theoretical framework
investigates properties causal graph based ordering schemes range well-known
planning sub-classes. Section 4 presents experiments regarding quality causal graph
based ordering schemes, Section 5 presents experiments adaptive stopping criteria
dynamic reordering. Section 6 concludes paper brief discussion outlook.
research note extension authors previous short conference paper (Kissmann
& Hoffmann, 2013). present paper contains comprehensive details regarding technical
background variable orderings implemented, includes full proofs. experiments
adaptive stopping criteria dynamic reordering, Section 5, new.

2. Background
BDD-based planning, argued e.g. Edelkamp Helmert (1999), important
small encoding given planning task. use finite-domain variable representation
basis investigation. finite-domain representation (FDR) planning task tuple
= hV, A, I, Gi, V set state variables v V associated
finite domain D(v). finite set actions pair hpre , eff partial
assignments V pre precondition eff effect action a. initial state
complete assignment V . goal G partial assignment V . V(pa), partial
assignment pa, denote variables v V pa(v) defined.
action applicable state iff pre s. resulting successor state s0
holds s0 (v) = eff (v) v V(eff ) s0 (v) = s(v) v V \ V(eff ). plan
sequence actions whose successive application starting initial state results state sg
G sg . plan optimal plan shorter length exists.
Binary decision diagrams (BDDs) introduced Bryant (1986) represent Boolean functions
. BDD directed acyclic graph one root two terminal nodes, 0-sink
781

fiK ISSMANN & H OFFMANN

x1

x1

x3

x2

x2

x3

x3

0

1

x2

x3

x3

0

(a) Full OBDD.

1

(b) Reduced OBDD.

Figure 1: Example BDDs function = ((x1 x2 ) x3 ). Dashed arrows denote low edges;
solid ones high edges.
1-sink. internal node corresponds binary variable p two successors, one following
high edge taken p true one following low edge taken p false. assignment
variables sink reached corresponds value function represented .
common practice, use reduced ordered BDDs. ordered BDD (OBDD)
BDD ordering binary variables path fixed. reduced OBDD
applies two reduction rules result canonical representation: (i) remove node identical
successor along high low edge; (ii) merge nodes variable
successor along high edge successor along low edge. Figure 1 illustrates
example BDDs function = ((x1 x2 ) x3 ) ordering hx1 , x2 , x3 i. Figure 1a
full OBDD without reduction. considering nodes x3 , note
rightmost one removed due rule (i), three merged due rule (ii).
Applying rules preceding layers well, end reduced OBDD Figure 1b.
consider BDD-based planning terms symbolic search (McMillan, 1993) implemented Gamer (Kissmann & Edelkamp, 2011). finite-domain variables V FDR task
encoded replacing v V binary counter (v) using dlog2 |D(v)|e bits. task
representable n bits need 2n BDD variables two sets, one set x representing current
state variables, another set x0 representing successor state variables. action represented transition relation BDD, Ta (x, x0 ), captures changes due application
also frame, i.e., variables change:
Ta (x, x0 ) = pre (x) eff (x0 ) frame(V \ V(eff ), x, x0 )
W
frame(V 0 , x, x0 ) = vV 0 v(x) v(x0 ) modeling
W frame. possible create monolithic transition relation actions, i.e., (x, x0 ) = aA Ta (x, x0 ). However, typically
feasible terms memory. Thus, store transition relations actions separately
(Burch, Clarke, & Long, 1991).
order calculate successors set states S, represented current state variables,
use image function
image(S) =

_

x.(S(x) Ta (x, x0 ))[x0 x].

aA

782

fiBDD RDERING H EURISTICS C LASSICAL P LANNING

conjunction makes sure applicable actions considered, sets corresponding
successor state variables. existential quantification removes current state variables.
operator [x0 x] stands swapping current successor state variables,
end successor states represented current state variables, i.e., new
current states. Finally, disjunction ensures successors based actions calculated.
case backward search, pre-image calculating predecessors set given
successor variables looks similar, successor state variables quantified instead
current state variables.
Using two functions, symbolic breadth-first search straightforward: Starting initial
state (or set goal states), iterate image (or pre-image), goal (or initial state)
reached. Storing entire set reached states ensure completeness. search,
layer L states subset states identical distance initial state (forward search)
goal (backward search) represented BDD characteristic function.
Based given variable ordering, size BDD, i.e., number nodes needed
represent corresponding function, differ exponentially, finding good orderings
crucial practice. size also influence runtime (e.g., time memory
requirements conjunction two BDDs polynomial product sizes two
BDDs), smaller size important terms memory also terms runtime. BDD
packages typically contain dynamic reordering algorithms, reduce BDD sizes based
current situation. However, previous work argued (Kissmann & Edelkamp, 2011),
experiments reconfirm, runtime overhead dynamic reordering prohibitive
planning. alternative use static variable ordering schemes instead. define
schemes functions mapping planning task non-empty set () variable orderings, i.e., orderings planning tasks finite-domain variables V . use set () here,
opposed single ordering, variable ordering schemes consider contain ambiguity, i.e., impose constraints final variable ordering opposed fixing
unique complete ordering.
first BDD created, set possible orderings determined pre-processing
step, actual ordering hv1 , . . . , vn = () chosen arbitrarily (i.e., consider
step here). calculated ordering defined set multi-valued variables. Thus,
get final BDD binary variable order replace finite-domain variable vi binary
counter (vi ). means BDD treats counters like inseparable fixed blocks. (Note
bits counters represented level planning tasks ,
impossible make informed choice separation block.) addition
blocks store current successor state variables interleaved fashion (Burch, Clarke,
Long, McMillan, & Dill, 1994).
layer L ordering planning tasks finite-domain variables, ordered BDD
unique. denote size, i.e., number nodes, BDDSize(o, L). BDDSize (L) :=
mino BDDSize(o, L) denote size BDD optimal variable ordering. Finding
optimal ordering NP-hard (Bryant, 1986).
state art ordering scheme symbolic planning based causal graph CG
planning task (Knoblock, 1994; Domshlak & Dinitz, 2001). CG directed graph
nodes V arc (v, v 0 ) iff v 6= v 0 exists action (v, v 0 ) V(eff )
V(pre ) V(eff ). words, arc v v 0 appear effect
action v appears precondition action v 0 effect.
783

fiK ISSMANN & H OFFMANN

Gamers scheme,
denoted ga , maps set orderings = hv1 , . . . , vn minimize
P
expression (vi ,vj )CG (i j)2 . idea variables vi , vj adjacent CG
dependent brought close together ordering minimizing distance
|i j|. bears similarity minimal bandwidth variable ordering CSPs (Zabih,
1990), though maximum distances minimized, minimize sum.
practice, Gamer approximates ga limited amount local search space orderings,
finding optimal solution NP-hard (Kissmann & Edelkamp, 2011). this, starts several
searches random ordering, swaps two variables checks sum decreased. did,
search continues new ordering, otherwise stick old one. end,
generated ordering smallest sum used. original hope connection
two notions dependency. supported fact new ordering
resulted improved coverage used benchmark set compared used before.
Apart ga , also consider scheme cg , defined acyclic CG .
maps set topological orderings nodes CG . consider theoretical
interest since straightforward way trust causal graph completely, i.e., take
dependencies derived causal graph order BDD variables accordingly.

3. Whats Causal Graph: Theory
pointed introduction, doubtful whether concept dependency
causal graph real relation concept dependency relevant BDD size.
frame terms classification guarantees offered, rather, guarantees offered,
ga cg restricted classes planning tasks.
first introduce theoretical framework, outline results cg ga .
3.1 Classification Framework
classify ordering schemes, relative given scalable family planning tasks, follows:
Definition 1 (Classification Ordering Schemes). Let F = {n } infinite family FDR
planning tasks parameterized n, size n bounded polynomial n. Let
{forward, backward} search direction. variable ordering scheme is:
(i) perfect F n F, d-layers L n , (n ),
BDDSize(o, L) = BDDSize (L).
(ii) safe F exists polynomial p s.t. n F, d-layers L n ,
(n ), BDDSize(o, L) p(BDDSize (L)).
(iii) viable F exists polynomial p s.t. n F d-layers L n ,
exists (n ) BDDSize(o, L) p(BDDSize (L)).
words, perfect guarantees deliver optimal orderings, safe guarantees
polynomial overhead, viable always delivers least one good ordering runs
risk super-polynomial overhead. viable, actively deceives planner,
sense variable orderings suggested super-polynomially bad task layer.
Note interpretation viability generous that, least one good ordering
must delivered, ordering may differ different search directions layers,
784

fiBDD RDERING H EURISTICS C LASSICAL P LANNING

x

x1

x2

x3

x4

g1

x5

(a) Chains

g2

g4

g3

g5

(b) Forks

G chain
x1

x2

x3

x4

x5

G

G fork

G dag

G

G ifork

g

(d) Relations (arrows mean )

(c) Inverted Forks

Figure 2: Causal graph special cases relation.
disambiguation (n ) left job determining ordering actually good
one. One could define notion strictly, results negative anyhow stick
optimistic version.
extend classification arbitrary sub-classes C FDR (whose sizes still bounded
polynomial) worst case families F contained C: C contains least one F
perfect, perfect C; C contains least one F safe,
safe C; C contains least one F viable, viable C.
interested variable orderings derived causal graph, natural consider
sub-classes FDR characterized causal graphs. set directed graphs G, FDR(G)
denote class FDR planning tasks whose causal graphs elements G. investigate
widely considered causal graph special cases, namely:
Chains (G chain ), find order x1 , . . . , xn variables
arcs xi xi+1 1 n 1 (cf. Figure 2a).
Forks (G fork ), one variable x, set variables gi , arc x
gi (cf. Figure 2b).
Inverted forks (G ifork ), set variables xi , one variable g, arc
xi g (cf. Figure 2c).
Directed acyclic graphs (DAGs, G dag ).
simple limiting cases, also consider causal graphs without arcs (G ), well arbitrary
causal graphs (G ). Figure 2d illustrates relations cases considered.
Bad cases inherited hierarchy Figure 2d: G G 0 , ordering scheme
classification within FDR(G 0 ) least bad FDR(G), simply culprit
worst-case (not-perfect/not-safe/not-viable) family F FDR planning tasks FDR(G)
contained FDR(G 0 ) well.
3.2 Classification Results
start investigation empty causal graphs, i.e., causal graphs arcs:
785

fiK ISSMANN & H OFFMANN

00

01

00

01

10

10

11

11

(a) DTG variable x.

(b) DTG variable y.

Figure 3: DTGs two variables planning task used proof Theorem 1.
Theorem 1. search directions, ordering scheme safe FDR(G ). ga cg
perfect.
Proof. causal graph arcs, variables move independently, i.e., action
may single variable precondition, variable effect.
forward/backward layer distance contains exactly states sum individual
distances (from variables initial value/to variables goal value) equals d. variable v
task, number vertices (more precisely, copies binary counter (v)) needed thus
bounded number possible individual-distance sums variables preceding v. Hence
BDD size polynomially bounded regardless variable ordering.
see ga cg perfect, consider following simple example. design
FDR task n uses 2 variables x y, domain size 4, represented values
00, 01, 10, 11. forward search, initially x = 00 = 00 holds. x variable
action setting 01 currently 00, another setting 10 00, two setting
11 01 10, respectively. variable action setting 01 currently
00, another setting 01 10 another setting 10 11. Thus, values
x variable distances 0, 1, 1, 2, respectively, initial value x,
variable distances 0, 1, 2, 3, respectively, ys initial value. Figure 3 illustrates
domain transition graphs (DTGs) variables x y. similar task distances
goal values defined backward search.
variable represented two BDD variables, x0 , x1 y0 , y1 . keep order
within x variables fixed, two possible orderings: x vice versa.
distance 1 initial (or goal) state, get BDDs illustrated Figure 4: Ordering
x results slightly larger BDD. Thus, ga cg , correspond possible
orderings, perfect, concludes proof.
Even though schemes ga cg constrain set possible orderings way,
Theorem 1 seen good case connection causal graphs BDD orderings:
Empty causal graphs entail ordering safe. connection doesnt seem carry
trivial case, though: sub-classes considered, space BDD orderings
contains exponentially bad ones. Indeed, true set BDD orderings,
786

fiBDD RDERING H EURISTICS C LASSICAL P LANNING

y0

x0

x1

x1

y0

y0

x0

x0

y1

y1

x1

x1

0

1

0

1

y1

(a) x y.

(b) x.

Figure 4: BDDs showing orderings perfect proof Theorem 1. Solid arrows
represent high edges, dashed ones low edges.
x1

x1

y1

x3

x2

x2

x3

x3

x3

y1

y1

y1

y2

y2

x3

y1

y2

y3

y3

0

x2

0

1

(a) Good variable ordering: hx1 , y1 , x2 , y2 , x3 , y3

1

(b) Bad variable ordering: hx1 , x2 , x3 , y1 , y2 , y3

Figure 5: BDDs different variable orderings Q(, ) n = 3: (x1 y1 ) (x2 y2 )
(x3 y3 ). Solid arrows denote high edges, dashed ones low edges.
also subsets delivered ga cg . classification schemes bad
almost considered cases, little bit hope chain causal graphs.
negative results employ Boolean functions quadratic form. variables
{x1 , y1 , . . . , xn , yn }, take form (x1 oplow y1 )ophi . . . ophi (xn oplow yn ), either ophi
{, } oplow = , vice versa. denote functions Q(ophi , oplow ).
functions, ordering hx1 , y1 , . . . , xn , yn (i.e., bringing pairs xi yi together) yields
BDD whose size polynomial n, ordering hx1 , . . . , xn , y1 , . . . , yn (i.e., splitting
variables two blocks, one x one variables) yields BDD exponential
size. (Wegener, 2000, proves Q(, ) depicted Figure 5; similar arguments apply
quadratic forms.)

787

fiK ISSMANN & H OFFMANN

g

x1

x1

y1

y1

x2

g

x2

0

g

y2

y2

x3

x3

y3

y3

1

1

0

(a) g front.

g

(b) g within pair.

Figure 6: BDDs representing g Q(, ) different positions g variable. Solid arrows
represent high edges, dashed ones low edges.
Theorem 2. search directions, ga cg safe FDR(G ifork ).
W
Proof. prove claim backward search, consider function Q(, ) = ni=1 (xi yi ).
design FDR task n uses 2n + 1 Boolean variables, {g, x1 , y1 , . . . , xn , yn } including
additional variable g goal requires true. n actions achieving g,
requires pair (xi yi ) true precondition. Clearly, Wn FDR(G ifork ). backward
layer distance 1 goal characterized g ni=1 (xi yi ).
optimal ordering Q(, ) consists pairs (xi , yi ) (yi , xi ). Adding g variable,
optimal ordering places either front (as depicted Figure 6a) end. cases
require exactly one node representing g variable. Placing g variable anywhere else requires
many nodes representing g nodes (different 0-sink) reached edges passing
layer. case, two g nodes g placed two pairs,
three nodes placed two nodes constituting pair (see Figure 6b latter case).
ordering following ga (n ) places g middle x variables arbitrary
order around it. ordering following cg (n ) places g end x variables
arbitrary order it. cases, x variables may placed variables, resulting
exponential overhead concludes proof backward search.
forward search, consider function Q(, ), construct n ,
variables {g, x1 , y1 , . . . , xn , yn } domains {x1 , y1 , . . . , xn , yn } ternary:
unknown, true (>), false (). x variables initially unknown, set either
true false currently unknown. n actions achieving g, exactly above.
states initial state distance 2n + 1 x W
variables either true false
states exactly satisfy g Q(, ) = g ni=1 (xi = >) (yi = >). causal
788

fiBDD RDERING H EURISTICS C LASSICAL P LANNING

d1

x1

dx1

dy1

y1

dy2

y2

dyn

yn

d2

x2

dx2

d3

dn

xn

dxn

dn+1

Figure 7: DTG variable z used proof Theorem 3. dashed edges correspond
preconditions changes value corresponding variable.
graph remains unchanged, set possible orderings following ga (n ) cg (n ) remains
backward search well, orders result exponential overhead
concludes proof forward search.
Note that, proof construction shows orders possible ga cg
super-polynomially bad, possible orders good. Hence, claimed prove ga
cg safe FDR(G ifork ), might case ga cg viable FDR(G ifork ).
leave open question.
Theorem 3. search directions, ga cg safe FDR(G fork ).
V
Proof. search directions, use function Q(, ) = ni=1 (xi yi ),
FDR task n Boolean variables {x1 , y1 , . . . , xn , yn } plus additional variable z domain
{d1 , dx1 , dy1 , d2 , dx2 , dy2 , . . . , dn , dxn , dyn , dn+1 }. actions that, 1 n,
z move di either dxi dyi , di+1 (see Figure 7). action
preconditioned dxi achieves xi , action preconditioned dyi achieves yi . Initially, z = d1
xi , yi false. goal requires z = dn+1 xi , yi true. forward search,
states initial state distance 3n exactly z = dn+1 Q(, ) true,
backward search states goal state distance 3n exactly z = d1 Q(, )
true.
ordering following ga (n ) places z middle x variables arbitrarily
around it; ordering following cg (n ) places z beginning x variables
arbitrarily it. Thus, constraint variables {x1 , y1 , . . . , xn , yn }, placing
789

fiK ISSMANN & H OFFMANN

x1

x2

y1

x3

y2

y3

g

Figure 8: Causal graph planning task used proof Theorem 4.
x variables variables ordering compatible schemes, results
exponential overhead.
Again, proof shows ga cg safe, makes statement regarding viability.
Note also task proof construction unsolvable. easy modify task
solvable without breaking proof argument forward search direction. investigate whether true backward search direction well. practice, proving
unsolvability traditionally popular objective planning, state space exhaustion
one traditional purposes BDDs deemed good for.
DAG causal graphs, prove cases orderings admitted ga
cg
super-polynomially bad:
Theorem 4. search directions, ga cg viable FDR(G dag ).
Proof. backward search claim, use combination chain causal graph
inverted fork illustrated Figure 8. design FDR task n uses 2n + 1 Boolean
variables, {g, x1 , y1 , . . . , xn , yn }, including variable g goal requires true.
n actions achieving g, requires pair (xi yi ) true precondition (this
part task proof Theorem 2). add actions ensuring two
schemes x variables placed variables (or vice versa). One action empty
precondition sets x1 true effect, another one requires xn true precondition
sets y1 true effect, rest xi1 (or yi1 ) precondition set xi (or yi )
true effect. states goal distance 1 thus characterized g Q(, ).
order induced ga places g middle, either places x variables increasing
order g variables increasing order g, places variables decreasing
order g x variables decreasing order g. cg induces order starting
x variables increasing order, followed variables increasing order, followed g.
Thus, cases, x variables placed separately variables, resulting exponential
overhead proves claim backward search direction.
forward search use approach proof Theorem 2, namely extend
domain x variables {true (>), false (), unknown}. x variables
initialized value unknown. n actions setting g true, requiring pair (xi yi )
true. additional actions follows. Two require x1 unknown set true
false, respectively. Two require xn true y1 unknown set y1 true false,
respectively. Two require xn false y1 unknown set y1 true false, respectively.
manner four actions xi yi (2 n), requiring xi1 (yi1 )
true respectively false, requiring xi (yi ) unknown, setting xi (yi ) true respectively
false. Thus, states
W initial state distance 2n + 1 characterized function
g Q(, ) = g ni=1 (xi = >) (yi = >). variable orders induced ga cg
backward search, resulting exponential overhead, concluding proof.
790

fiBDD RDERING H EURISTICS C LASSICAL P LANNING

x1

y1

y2

x2

y3

x3

Figure 9: Causal graph planning task used proof Theorem 5.
x1

x1

y1

y1

x3

x2

y2

y2

y2

y1

y1

x2

x2

x3

x3

y1

x3

y1

y2

y3

y3

0

1

0

(a) Optimal ordering

x3

y1

y1

y2

y2

y3

y3

y1

y2

1

(b) Exponential ordering

W
L
Figure 10: BDDs representing 3i=1 yi 3i=1 (xi yi ), used proof Theorem 5. Solid
arrows represent high edges, dashed ones low edges.
immediately get (recall cg defined acyclic causal graphs):
Corollary 1. ga viable FDR(G ).
close investigation somewhat positive result chain causal graphs:
Theorem 5. search directions, ga cg perfect FDR(G chain ). exists
ordering scheme viable FDR(G chain ).
Proof. first part claim inherited FDR(G ), i.e., corollary Theorem 1.
second part claim, existence non-viableW
ordering scheme, consider first
backward search direction, using function Q(, ) = ni=1 (xi yi ). design FDR
task n uses 2n Boolean variables, {x1 , y1 , . . . , xn , yn }. goal requires variables
false. action without precondition set x1 true, actions preconditions requiring
yi1 false setting xi true, actions preconditioned xi true setting yi false.
causal graph depicted Figure 9. Clearly, n FDR(G chain ).
states distance 1 goal ones except oneLyi false,
n

single
true
Ln
Wn yi xi true well. characterized formula i=1 yi Q(, ) =
i=1 yi i=1 (xi yi ). easy see exclusive part formula change
relevant properties BDDs quadratic form, i.e., still orderings polynomial
orderings exponential number nodes, e.g., placing x variables
791

fiK ISSMANN & H OFFMANN

safe?
G chain
trivially
safe
G


viable
G dag

safe
G fork


viable
G

safe
G ifork

Figure 11: Overview classification results. hold ga cg , search
direction.

variables (see Figure 10 illustration). ordering scheme including latter orderings
viable.
forward search direction case, construct planning task x variables
ternary (unknown, true (>), false ()), unknown initially. value x1 set
freely; yi set true false xi true, set true xi false; xi+1
set freely yi set
V either true false. 2n steps, reach exactly
states characterized Q(, ) = ni=1 (xi = >) (yi = >). BDD representing Q(, )
exponential size if, e.g., x variables placed variables, ordering scheme
including orderings viable.
Note that, planning task families {n } described, ga cg force
xi yi variable ordered pairs, resulting BDDs minimal size (see Figure 10a).
sense, two planning task families constitute truly positive result: Within them,
ordering information causal graph keeps us making exponentially bad mistakes.
positive message would much stronger ga cg safe families tasks
chain causal graphs. remains open question whether so.
Figure 11 gives overview results. evidence speaks rather clearly strong
connection causal graph dependencies dependencies relevant BDD size. Note
causal graph underlying Theorem 4 non-viability FDR(G dag ) simple
form combining chain inverted fork, Theorem 2 non-safety FDR(G ifork )
relies planning tasks fall known syntactically identified tractable class optimal
planning (Katz & Domshlak, 2010). Note also safe already quite bad practice,
incurring exponential risk unless clever way choosing ordering within ()
(which, moment, have).

4. Whats Causal Graph: Practice
shown poor worst-case performance causal graph based variable ordering schemes
theory, practice might another matter. assess latter, implemented comprehensive
set causal graph based variable ordering schemes, comprising 12 schemes total, ran
comparison practical good/bad delimiters. bad delimiter, used random
orderings. good delimiter, used off-the-shelf dynamic reordering algorithm
Gamers BDD package CUDD, based sifting (Rudell, 1993).
words order regarding sifting works. variable greatest number
nodes current BDD chosen. first moved towards end ordering,
792

fiBDD RDERING H EURISTICS C LASSICAL P LANNING

towards beginning ordering, iteratively swapping position next variable
corresponding direction. positions tried, variable moved position
BDD size smallest. done, next variable chosen, variables
processed. better comparability ordering schemes, restrict algorithm
keep variables representing (v) together.
previously indicated, dynamic reordering consumes much runtime cost-effective.
present experiments, interested BDD size, give dynamic reordering ample runtime. Section 5, identify simple adaptive criteria stopping dynamic
reordering automatically search, taking advantage size reduction capacity without
suffering much runtime consumption.
ran benchmarks 2011 International Planning Competition (IPC11), used
Gamer base implementation planners, running one core Intel Xeon
X5690 CPU 3.47 GHz. Unless otherwise stated, used IPC11 settings, namely timeout
30 minutes memory limit 6 GB.
4.1 Ordering Schemes
ran six schemes based directly causal graph:
Gamer Gamers original ordering scheme, approximates ga .
GamerPre like Gamer causal graph extended arcs pairs precondition
variables. idea capture dependency forward search, also
backward search, i.e., inverting actions.
WGamer like Gamer arcs weighted number relevant actions, i.e., number
actions inducing corresponding arcs.
WGamerPre like GamerPre weighted arcs.
CGLevel Fast Downwards (Helmert, 2006) level heuristic, approximates cg . orders
variables strongly connected components and, within components, considers
weighted causal graph orders variables smallest incoming weight first. Similar
WGamer, weights correspond number actions induce arc.
CGSons another approximation cg . always selects variable v whose parents already selected; least one whose parents already selected; arbitrary
variable v exists.
Additionally, used six ordering schemes adopted model checking literature,
based structure called abstract syntax tree (AST) (e.g., Maisonneuve, 2009).
directed graph containing root node overall task subtrees actions. subtree
consists nodes representing subformulas specified action (i.e., subformulas
actions precondition effect). variables task leaves AST. leaves
merged, i.e., one node variable task. Edges point node
representing function corresponding subtrees.
construct AST based PDDL input. Consider following example actions, similar Floortile domain. predicates at(r, t), denoting tile robot r
793

fiK ISSMANN & H OFFMANN



a2

a1







at(r1 , t1 )



painted (t2 )

at(r2 , t3 )

Figure 12: Example AST.
currently painted (t) denoting whether tile already painted. two actions
a1 = paint(r1 , t1 , t2 ) precondition (at(r1 , t1 ) painted (t2 )) effect (painted (t2 )) denoting robot r1 paint tile t2 currently t1 t2 painted. Similarly,
action a2 = paint(r2 , t3 , t2 ) precondition (at(r2 , t3 )painted (t2 )) effect (painted (t2 ))
denotes robot r2 paint tile t2 currently t3 t2 painted.
Figure 12 illustrates corresponding AST. root actions A, one subtree
two actions a1 a2 . actions preconditions effects encoded
retain one copy variable leaves (here relevant painted (t2 )).
Using first authors names reference, additional ordering schemes following.
Butler (Butler, Ross, Kapur, & Mercer, 1991) extension approach Fujita, Fujisawa,
Kawato (1988). latter proposed perform depth-first search (DFS) AST,
starting root node, order variables order reached
first time. Butler et al. extended setting several roots (if remove overall
root retain subtrees various actions arrive exactly setting).
approach starts DFS action containing highest number variables. Within
tree advances similar manner: always continues subtree contains
highest number variables among subtrees current node. retrieved ordering
order variables reached first time.
Chung1 (Chung, Hajj, & Patel, 1993) two-step approach. first step assigns values
nodes AST. Starting leaves, assigning value 0, assigns
inner node maximum values assigned successors plus 1. second step
performs DFS starting root, guided values nodes, visiting
successors highest value first. order variables reached first
time chosen variable ordering.
Chung2 (Chung et al., 1993) determines shortest distance pair variables,
calculated considering edges AST undirected. Additionally, total
distances, i.e., sum minimal distances variables, stored variables. variable smallest total distance chosen first. next one one closest
794

fiBDD RDERING H EURISTICS C LASSICAL P LANNING

last variable inserted ordering. case tie distance preceding
variables also taken account.
Maisonneuve (Maisonneuve, 2009) greedy approach starting empty sequence.
step temporarily extends current sequence variable yet sequence.
variable, weight determined, number variables extended
sequence appear action, summed actions. variable removed
sequence next one added. weights calculated variable
highest weight appended sequence, next iteration starts, calculating new
weights remaining variables. end, last sequence contains variables
thus corresponds variable ordering.
Malik (Malik et al., 1988) assigns level value (the maximal level predecessors plus 1)
node within AST. root assigned value 0. variables ordered
according level values, highest values coming first.
Minato (Minato et al., 1990) calculates weights nodes AST. weight root
node action set 1, successors node w/m w nodes weight
number successors node. One variables highest weight
chosen first nodes removed (along ingoing edges recursively
nodes remaining successors). reduced graph weights recalculated
procedure continues finally variables ordering.
4.2 Bad Delimiter
get bad delimiter ran 5000 random orderings, ordering corresponds
one run IPC11 benchmark tasks, using random variable ordering instance.
make feasible used time-out one minute (our backward search implementation
viable short time-out, use forward search here). comparison
data, settings (1 minute time-out, forward search) used twelve static
ordering schemes. Initially ran ordering schemes random orderings tasks;
200 random runs removed tasks benchmark set solved least
previous (random static ordering) runs, retaining 85 tasks. Figure 13 shows coverage,
i.e., number solved planning tasks, x axis, fraction random orderings
coverage axis. coverage data ordering schemes shown vertical lines.
Malik CGLevel lie middle Gaussian distribution, respectively.
words, Malik bad as, CGLevel even worse than, average random ordering.
Matters bleak ten ordering schemes, close together lie clearly
Gaussian distribution. Compared best-of random orders, however,
ordering schemes appear rather humble. Consider Table 1. particular, consider nr
+ , giving
number instances solved ordering scheme random order, consider n+r
,
giving number instances solved scheme solved random order.
+r
Table 1 shows, nr
+ strictly smaller n three ordering schemes ,
strictly larger (by single task) one schemes (namely Butler). average nr
+
2.92 n+r

9.08.

795

fiK ISSMANN & H OFFMANN

Percentage Random Orderings

16

CGLevel
Malik
Maisonneuve+WGamerPre
Minato+WGamer
CGSons
Gamer+GamerPre
Chung1+Chung2
Butler

14
12
10
8
6
4
2
0
20

30

40

50
Coverage

60

70

80

Type

Butler

CGLevel

CGSons

Chung1

Chung2

Gamer

GamerPre

Maisonneuve

Malik

Minato

WGamer

WGamerPre

Best Scheme

Figure 13: Coverage random orders vs. ordering schemes. Schemes ordered top-to-bottom
worst best coverage. X +Y means schemes, X , result coverage.

nr
+
n+r

n+r
+
nr


3
2
77
3

1
26
53
5

2
4
75
4

4
4
75
2

2
2
77
4

5
6
73
1

5
6
73
1

3
11
68
3

0
22
57
6

4
8
71
2

3
7
72
3

3
11
68
3

6
1
78
0

Table 1: Differences solved instances 85 IPC11 tasks (1 minute timeout); r means
solved random ordering, +r least one random ordering, solved corresponding ordering scheme, + solved corresponding ordering scheme.
4.3 Good Delimiter
performed bidirectional blind search, i.e., competitive setup general. Figure 14
contains one data point every pair (I, ) IPC11 benchmark instance ordering scheme
solved (a) Gamer using dynamic reordering starting arbitrary variable
order (the one returned Gamers grounding process), (b) Gamer using ordering scheme
(without dynamic reordering). time-out 6 hours (a), 30 minutes (b). x-value
data point size largest BDD constructed (a), y-value size
largest BDD constructed (b). allowed much higher time-out dynamic reordering
reordering runtime effective: question asking merely
two methods yields smaller BDDs. Figure 14 shows dynamic reordering universally
much better this, giving us sizes three orders magnitude smaller
schemes. total 1911 instances (solved ordering scheme dynamic reordering),
1431 cases BDD sizes smaller factor 10 using dynamic reordering,
796

fiBDD RDERING H EURISTICS C LASSICAL P LANNING

Peak Size Ordering Schemes

108
107
106
105
104
104

105
106
Peak Size Dynamic Reordering

107

108

Dynamic
Reordering

WGamerPre

WGamer

Minato

Malik

Maisonneuve

GamerPre

Gamer

Chung2

Chung1

CGSons

CGLevel

Butler

Figure 14: BDD size dynamic reordering vs. ordering schemes.

Domain
Barman
4
4
4
4
8
8
7
4
4
4
5
4
9(5)
Elevators 19 19 19 19 19 19 19 19 19 19 19 19
19(17)
Floortile
8
8
7
8
8
8
8
7
7
8
8
8
7(6)
14(12)
NoMystery 14 14 14 14 14 13 14 14 15 14 14 14
Openstacks 20 18 20 20 20 20 20 19 19 19 20 20
20(20)
PARC-Printer
6
6
5
6
5
6
6
6
5
6
6
7
8(7)
PegSol 17 17 17 17 17 18 18 17 17 17 18 18
18(17)
Scanalyzer
8
7
9
9
8
9
9
9
9
9
9
7
9(9)
Sokoban 17 18 18 17 19 19 19 18 18 18 19 19
19(13)
Tidybot 16
7 14 15 15
9 12 14
9 15 12
8
16(8)
Transport
8
9
9
7
8
8
8
9
7
9
7
7
10(7)
9 11 11 11 11 11 11 10 10 11 11
12(11)
VisitAll 11
Woodworking 16 13 10 14 16 16 16 12
8 16 15 16
19(16)
Total (260) 164 149 157 161 168 164 167 159 147 164 163 158 180(148)

Table 2: Coverage IPC11 tasks. dynamic reordering, numbers parentheses represent coverage 30 minute timeout.
406 cases smaller factor 10 100, 20 cases smaller
factor 100.
Table 2 shows coverage different schemes IPC11 tasks. make
similar observation one minute, forward search results, namely CGLevel
Malik clearly behind others. last column shows coverage Gamer using
dynamic reordering, provides two numbers, first coverage 6 hours timeout, second
coverage timeout schemes, i.e., 30 minutes. becomes clear
applying dynamic reordering entire search time feasible practice limiting
runtime.
797

fi600
500
400
300
200
100
0

Total Runtime
Reordering Time
Transition Relation Creation

0

2

4

6
8
Reorderings

Time (s)

Time (s)

K ISSMANN & H OFFMANN

10

12

1400
1200
1000
800
600
400
200
0

Total Runtime
Reordering Time
Transition Relation Creation

0

(a) VisitAll, task 011

2

4

6
8 10
Reorderings

12

14

16

(b) PegSol, task 015

Figure 15: Total runtime time spent reordering limited number reordering steps two
example IPC11 tasks. reorderings vertical line performed transition
relation creation.

5. Limited Dynamic Reordering
Given much memory-efficient behavior dynamic reordering, possible approach
run dynamic reordering limited time only, hoping get ordering good enough
remainder search. Reordering automatically started number allocated
BDD nodes reaches certain threshold (by default, first threshold 4000 nodes),
dynamically adapted reordering (by default, next threshold set 2 times number
nodes reordering). simple way control dynamic reordering limit number
reordering steps, turn dynamic reordering desired number reorderings
performed.
different reordering limits, total runtime task often looks similar situation
depicted Figure 15a. reorderings takes long time solve task due
bad initial ordering. Also, first reorderings sometimes hurt, performed
beginning construction transition relation, enough information good
orderings available. However, many reorderings solving takes long time due
immense overhead reordering time, grows exponentially step.
important different behavioral pattern depicted Figure 15b: domains,
PegSol Sokoban, minimum curve beginning (without reordering),
total runtime increases afterward (mainly based increase reordering time).
explanation behavior might initial ordering already pretty good,
dynamic reordering cannot improve much overhead incurred vain. also often
happens easier tasks domain, learning good setting based simpler tasks
seems impossible.
Attempting exploit observations design adaptive stopping criteria, geared finding
good point stopping dynamic reordering, given available observations (e.g., number
BDD nodes before/after reordering, reordering times, current total runtimes), experimented
following approaches.
First, noticed early reordering time increases step step small factor,
later factor increases. preliminary runs saw often area smallest
runtime coincides situation increase reordering time reaches threshold,
often 1.25 1.75 (see, e.g., Figure 16a compare runtime minimum
Figure 15a planning task). call factor criterion.
798

fiBDD RDERING H EURISTICS C LASSICAL P LANNING

2.5

Factor
Percentage

Factor

2
1.5
1
0.5
0
0

2

4

6
8
10
12
Reorderings
(a) Factor time last reordering
previous one.

70
60
50
40
30
20
10
0

Percentage

0

2

4

6
8
10
12
Reorderings
(b) Percentage time last reordering
total runtime far.

Figure 16: Factor percentage criterion limited number reordering steps task 011
IPC11 VisitAll domain.
Second, another observation preliminary runs percentage time spent
last reordering step total current runtime often follows U-like curve, minimum
curve often lies close number reorderings total runtime minimum (see,
e.g., Figure 16b compare runtime minimum Figure 15a task). employ
percentage criterion, stops reordering (possibly local) minimum reached,
i.e., compare percentage current step previous step; current
one greater stop reordering.
Finally, simple combination criteria stop reordering soon one tells
us so.
evaluate adaptive stopping criteria, ran tasks IPC11 domains different limits number reorderings, ranging 0 20.1 Based runs calculated
results would achieve adaptive stopping criteria. See Table 3. best possible
coverage, i.e., number tasks solved least one setting limited number reorderings,
175, without reordering found 167 solutions. adaptive stopping criteria yield
coverage 158 171. Performance reasonable factor criterion, quite bad
percentage criterion combination criteria. Recall percentage criterion aims stopping reordering incurring prohibitive overhead. Indeed, criterion,
reordering often stopped earlier factor criterion. cases detrimental,
particularly Woodworking domain strategy happens fall dramatic local
peak total-runtime curve, resulting 9 problem instances longer solved.
As, several cases, dynamic reordering transition relation creation counterproductive, also ran delayed reordering, dynamic reordering started BDDs
transition relation created. results Table 4. case without reordering
unchanged respect Table 3. best possible result slightly worse before,
coverage 174. adaptive stopping criteria, picture changes substantially: contrast
Table 3, percentage criterion excels, delivering coverage 1 short best possible.
Regarding factor criteria, overly small large factors bad best behavior (2 short
best possible) obtained middle.
1. cases highest number reorderings could observe clearly 20. Either planner ran
time memory, finished last reorderings could performed. cases used GamerPre
initial ordering, turned among best preliminary set experiments.

799

fiK ISSMANN & H OFFMANN

Domain
Barman
Elevators
Floortile
NoMystery
Openstacks
PARC-Printer
PegSol
Scanalyzer
Sokoban
Tidybot
Transport
VisitAll
Woodworking
Total


reord
7
19
8
14
20
6
18
9
19
12
8
11
16
167

best
possible
8
19
8
16
20
7
18
9
19
14
9
12
16
175

1.25
8
19
7
14
20
6
17
9
17
14
9
11
10
161

factor criterion
1.5 1.75 2.0
8
8
8
19
19
19
8
8
8
14
14
14
20
20
20
6
7
7
17
17
18
9
9
9
17
17
17
14
14
14
9
9
9
12
12
12
15
16
16
168 170 171

percentage
criterion
8
19
8
14
20
6
17
9
17
13
9
11
7
158

1.25
8
19
7
14
20
6
17
9
17
13
9
11
9
159

criteria
1.5 1.75
8
8
19
19
8
8
14
14
20
20
6
6
17
17
9
9
17
17
13
13
9
9
11
11
7
7
158 158

2.0
8
19
8
14
20
6
17
9
17
13
9
11
7
158

Table 3: Coverage results different stopping criteria. Immediate reordering.

Domain
Barman
Elevators
Floortile
NoMystery
Openstacks
PARC-Printer
PegSol
Scanalyzer
Sokoban
Tidybot
Transport
VisitAll
Woodworking
Total


reord
7
19
8
14
20
6
18
9
19
12
8
11
16
167

best
possible
8
19
8
16
20
7
18
9
19
13
9
12
16
174

1.25
8
19
7
16
20
6
17
9
19
13
9
12
10
165

factor criterion
1.5 1.75 2.0
8
8
7
19
19
19
8
8
8
16
14
14
20
20
20
7
7
6
17
17
17
9
9
9
19
19
19
13
13
13
9
9
9
12
12
12
15
16
16
172 171 169

percentage
criterion
8
19
8
16
20
7
17
9
19
13
9
12
16
173

1.25
8
19
7
16
20
6
17
9
19
13
9
12
10
165

criteria
1.5 1.75
8
8
19
19
8
8
16
16
20
20
7
7
17
17
9
9
19
19
13
13
9
9
12
12
15
16
172 173

2.0
8
19
8
16
20
7
17
9
19
13
9
12
16
173

Table 4: Coverage results different stopping criteria. Delayed reordering, i.e., reordering started
creation transition relation BDDs.
shed light observations, Figure 17 shows coverage function
different factor values, case immediate reordering (Figure 17a) delayed
reordering (Figure 17b). Figure 17a, see percentage criterion stops reordering
early. Without it, coverage resulting stopping reordering based solely factor criterion
get high 173. However, ascend coverage actually starts percentage
criterion stops reordering, thus cutting many solutions. Figure 17b, factor 1.6
curves identical, mainly increasing increasing factor. that, combination
criterion rises another little bit, factor criterion alone drops substantially. combined
criterion avoids drop because, point (here, factor roughly 2.0), percentage
criterion stops reordering least early factor criterion.
800

fi180
175
170
165
160
155
150

Factor

Coverage

Coverage

BDD RDERING H EURISTICS C LASSICAL P LANNING

0

0.5

1

1.5
2
Factor
(a) Immediate reordering.

2.5

3

180
175
170
165
160
155
150

Factor


0

0.5

1

1.5
2
Factor
(b) Delayed reordering.

2.5

3

Figure 17: Coverage function factor, factor criterion alone, combination
percentage criterion (denoted Both).

6. Conclusion
tempting equate variable dependencies BDD-based symbolic search
identified causal graphs, previous research done unquestioningly. Looking little
closely issue, shown causal graph based variable orderings exponentially bad
even severely restricted sub-classes planning. Empirically, Fast Downwards level heuristic
worse random, ordering schemes lag far behind off-the-shelf reordering.
One may wonder meaning theoretical results: could static ordering
scheme incur exponential overhead worst case? agree view principle,
expect happen planning tasks restricted tractable domainindependent optimal planning. remains seen extent classification framework
suitable characterize properties ordering schemes and/or planning fragments.
impression point static ordering schemes limited hopeless.
Prior actually building BDDs, appears impossible extract reliable information
form take. way forward, then, use dynamic reordering techniques
targeted manner. initial experiments direction meet immediate
breakthrough, certainly show promise, especially considering primitive nature
method stopping criteria employed. Promising future directions include flexible
on/off strategies dynamic reordering, machine learning deciding toggle switch,
planning-specific reordering techniques exploiting particular structure BDDs hand.

Acknowledgments
thank anonymous reviewers ICAPS 2013 short version previous version
article, whose comments helped tremendously improve paper.

References
Brafman, R., & Domshlak, C. (2003). Structure complexity planning unary operators.
Journal Artificial Intelligence Research, 18, 315349.
Bryant, R. E. (1986). Graph-based algorithms boolean function manipulation. IEEE Transactions Computers, 35(8), 677691.
801

fiK ISSMANN & H OFFMANN

Burch, J. R., Clarke, E. M., & Long, D. E. (1991). Symbolic model checking partitioned
transition relations. Halaas, A., & Denyer, P. B. (Eds.), Proceedings International
Conference Large Scale Integration (VLSI-91), Vol. A-1 IFIP Transactions, pp.
4958, Edinburgh, Scotland. North-Holland.
Burch, J. R., Clarke, E. M., Long, D. E., McMillan, K. L., & Dill, D. L. (1994). Symbolic model
checking sequential circuit verification. IEEE Transactions Computer-Aided Design
Integrated Circuits Systems, 13(4), 401424.
Butler, K. M., Ross, D. E., Kapur, R., & Mercer, M. R. (1991). Heuristics compute variable
orderings efficient manipulation ordered binary decision diagrams. Proceedings
28th Conference Design Automation (DAC-91), pp. 417420, San Francisco, CA,
USA. ACM.
Chen, H., & Gimenez, O. (2010). Causal graphs structurally restricted planning. Journal
Computer System Sciences, 76(7), 579592.
Chung, P.-Y., Hajj, I. N., & Patel, J. H. (1993). Efficient variable ordering heuristics shared
ROBDD. Proceedings 1993 IEEE International Symposium Circuits Systems
(ISCAS-93), pp. 16901693, Chicago, IL, USA. IEEE.
Cimatti, A., Pistore, M., Roveri, M., & Traverso, P. (2003). Weak, strong, strong cyclic planning
via symbolic model checking. Artificial Intelligence, 147(12), 3584.
Darwiche, A. (2011). SDD: new canonical representation propositional knowledge bases.
Walsh, T. (Ed.), Proceedings 22nd International Joint Conference Artificial Intelligence (IJCAI11), pp. 819826. AAAI Press/IJCAI.
Dechter, R., & Meiri, I. (1989). Experimental evaluation preprocessing techniques constraint
satisfaction problems. Sridharan, N. S. (Ed.), Proceedings 11th International Joint
Conference Artificial Intelligence (IJCAI-89), pp. 271277, Detroit, MI. Morgan Kaufmann.
Domshlak, C., & Dinitz, Y. (2001). Multi-agent offline coordination: Structure complexity.
Cesta, A., & Borrajo, D. (Eds.), Recent Advances AI Planning. 6th European Conference
Planning (ECP-01), Lecture Notes Artificial Intelligence, pp. 3443, Toledo, Spain.
Springer-Verlag.
Edelkamp, S., & Helmert, M. (1999). Exhibiting knowledge planning problems minimize
state encoding length. Biundo, S., & Fox, M. (Eds.), Recent Advances AI Planning.
5th European Conference Planning (ECP99), Lecture Notes Artificial Intelligence, pp.
135147, Durham, UK. Springer-Verlag.
Fan, G., Muller, M., & Holte, R. (2014). Non-linear merging strategies merge-and-shrink based
variable interactions. Edelkamp, S., & Bartak, R. (Eds.), Proceedings 7th Annual
Symposium Combinatorial Search (SOCS14). AAAI Press.
Freuder, E. C. (1982). sufficient condition backtrack-free search. Journal Association
Computing Machinery, 29(1), 2432.
Fujita, M., Fujisawa, H., & Kawato, N. (1988). Evaluation improvements boolean comparison method based binary decision diagrams. Proceedings 1988 International
Conference Computer-Aided Design (ICCAD-98), pp. 25. IEEE Computer Society Press.
802

fiBDD RDERING H EURISTICS C LASSICAL P LANNING

Gimenez, O., & Jonsson, A. (2008). complexity planning problems simple causal
graphs. Journal Artificial Intelligence Research, 31, 319351.
Helmert, M. (2004). planning heuristic based causal graph analysis. Koenig, S., Zilberstein,
S., & Koehler, J. (Eds.), Proceedings 14th International Conference Automated
Planning Scheduling (ICAPS04), pp. 161170, Whistler, Canada. Morgan Kaufmann.
Helmert, M. (2006). Fast Downward planning system. Journal Artificial Intelligence Research, 26, 191246.
Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics optimal sequential planning. Boddy, M., Fox, M., & Thiebaux, S. (Eds.), Proceedings 17th
International Conference Automated Planning Scheduling (ICAPS07), pp. 176183,
Providence, Rhode Island, USA. Morgan Kaufmann.
Helmert, M., Haslum, P., Hoffmann, J., & Nissim, R. (2014). Merge & shrink abstraction: method
generating lower bounds factored state spaces. Journal Association Computing Machinery, 61(3).
Hoffmann, J. (2011a). Analyzing search topology without running search: connection
causal graphs h+ . Journal Artificial Intelligence Research, 41, 155229.
Hoffmann, J. (2011b). ignoring delete lists works, part II: Causal graphs. Bacchus, F.,
Domshlak, C., Edelkamp, S., & Helmert, M. (Eds.), Proceedings 21st International
Conference Automated Planning Scheduling (ICAPS11), pp. 98105. AAAI Press.
Jonsson, P., & Backstrom, C. (1995). Incremental planning. European Workshop Planning.
Katz, M., & Domshlak, C. (2008). New islands tractability cost-optimal planning. Journal
Artificial Intelligence Research, 32, 203288.
Katz, M., & Domshlak, C. (2010). Implicit abstraction heuristics. Journal Artificial Intelligence
Research, 39, 51126.
Kissmann, P., & Edelkamp, S. (2011). Improving cost-optimal domain-independent symbolic planning. Burgard, W., & Roth, D. (Eds.), Proceedings 25th National Conference
American Association Artificial Intelligence (AAAI-11), pp. 992997, San Francisco, CA,
USA. AAAI Press.
Kissmann, P., & Hoffmann, J. (2013). Whats BDD? causal graphs variable orders planning. Borrajo, D., Fratini, S., Kambhampati, S., & Oddi, A. (Eds.), Proceedings
23rd International Conference Automated Planning Scheduling (ICAPS13), pp.
327331, Rome, Italy. AAAI Press.
Knoblock, C. (1994). Automatically generating abstractions planning. Artificial Intelligence,
68(2), 243302.
Maisonneuve, V. (2009). Automatic heuristic-based generation MTBDD variable orderings
PRISM models. Internship report, Oxford University Computing Laboratory.
Malik, S., Wang, A., Brayton, R., & Sangiovanni-Vincentelli, A. (1988). Logic verification using
binary decision diagrams logic synthesis environment. Proceedings 1988 International Conference Computer-Aided Design (ICCAD-98), pp. 69. IEEE Computer
Society Press.
803

fiK ISSMANN & H OFFMANN

McMillan, K. L. (1993). Symbolic Model Checking. Kluwer Academic Publishers.
Minato, S., Ishiura, N., & Yajima, S. (1990). Shared binary decision diagram attributed edges
efficient boolean function manipulation. Proceedings 27th ACM/IEEE Design
Automation Conference (DAC-90), pp. 5257, Orlando, FL, USA. IEEE Computer Society
Press.
Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
efficient SAT solver. Proceedings 38th Conference Design Automation (DAC01), Las Vegas, Nevada, USA. IEEE Computer Society.
Rintanen, J. (2012). Planning satisfiability: Heuristics. Artificial Intelligence, 193, 4586.
Rudell, R. (1993). Dynamic variable ordering ordered binary decision diagrams. Lightner,
M. R., & Jess, J. A. G. (Eds.), Proceedings 1993 IEEE/ACM International Conference
Computer-Aided Design (ICCAD-93), pp. 4247, Santa Clara, CA, USA. IEEE Computer
Society.
Sievers, S., Wehrle, M., & Helmert, M. (2014). Generalized label reduction merge-and-shrink
heuristics. Proceedings 28th AAAI Conference Artificial Intelligence (AAAI14),
Quebec City, Quebec, Canada. AAAI Press.
Wegener, I. (2000). Branching Programs Binary Decision Diagrams. SIAM.
Williams, B. C., & Nayak, P. P. (1997). reactive planner model-based executive. Pollack,
M. (Ed.), Proceedings 15th International Joint Conference Artificial Intelligence
(IJCAI-97), pp. 11781185, Nagoya, Japan. Morgan Kaufmann.
Zabih, R. (1990). applications graph bandwidth constraint satisfaction problems.
Proceedings 8th National Conference American Association Artificial Intelligence (AAAI-90), pp. 4651, Boston, MA. MIT Press.

804

fiJournal Artificial Intelligence Research 51 (2014)

Submitted 04/14; published 10/14

Verification Agent-Based Artifact Systems
Francesco Belardinelli

BELARDINELLI @ IBISC . FR

Laboratoire Ibisc, Universite dEvry, France

Alessio Lomuscio

. LOMUSCIO @ IMPERIAL . AC . UK

Department Computing, Imperial College London, UK

Fabio Patrizi

FABIO . PATRIZI @ DIS . UNIROMA 1.

Dipartimento di Ingegneria Informatica,
Automatica e Gestionale A. Ruberti
Universita di Roma La Sapienza, Italy

Abstract
Artifact systems novel paradigm specifying implementing business processes described terms interacting modules called artifacts. Artifacts consist data lifecycles, accounting respectively relational structure artifacts states possible evolutions
time. paper put forward artifact-centric multi-agent systems, novel formalisation
artifact systems context multi-agent systems operating them. Differently
usual process-based models services, give semantics explicitly accounts data
structures artifact systems defined.
study model checking problem artifact-centric multi-agent systems specifications expressed quantified version temporal-epistemic logic expressing knowledge
agents exchange. begin noting problem undecidable general.
identify noteworthy class systems admit bisimilar, finite abstractions. follows
verify systems investigating finite abstractions; also show corresponding model checking problem EXPSPACE-complete. introduce artifact-centric
programs, compact declarative representations programs governing artifact system agents. show that, principle generate infinite-state systems,
natural conditions verification problem solved finite abstractions effectively computed programs. exemplify theoretical results pursued
mainstream procurement scenario artifact systems literature.

1. Introduction
Much work area reasoning knowledge involves development formal
techniques representation epistemic properties rational actors, agents, multiagent system (MAS). approaches based modal logic often rooted interpreted systems (Parikh & Ramanujam, 1985), computationally grounded semantics (Wooldridge, 2000)
used interpretation several temporal-epistemic logics. line research thoroughly explored 1990s leading significant body work (Fagin, Halpern, Moses, &
Vardi, 1995; Meyer & van der Hoek, 1995). recent topic interest development
automatic techniques, including model checking (Clarke, Grumberg, & Peled, 1999),
verification temporal-epistemic specifications autonomous agents MAS (Gammie &
van der Meyden, 2004; Lomuscio, Qu, & Raimondi, 2009; Kacprzak, Nabialek, Niewiadomski,
Penczek, Polrola, Szreter, Wozna, & Zbrzezny, 2008). led developments number
areas traditionally outside artificial intelligence, knowledge representation MAS, including

c
2014
AI Access Foundation. rights reserved.

fiB ELARDINELLI , L OMUSCIO & PATRIZI

security (Dechesne & Wang, 2010; Ciobaca, Delaune, & Kremer, 2012), web-services (Lomuscio,
Penczek, Solanki, & Szreter, 2011) cache-coherence protocols hardware design (Baukus &
van der Meyden, 2004). ambition present paper offer similar change perspective
area artifact systems (Cohn & Hull, 2009), growing topic Service-Oriented Computing
(SOC).
Artifacts structures combine data process holistic manner basic building
block[s] (Cohn & Hull, 2009) systems descriptions. Artifact systems services constituted
complex workflow schemes based artifacts agents interact with. data component
given relational databases underpinning artifacts system, whereas workflows
described lifecycles associated artifact schema. standard service
paradigm services made public exposing process interfaces, artifact systems
data structures lifecycles advertised. Services composed hub operations artifacts executed. Implementations artifact systems, IBM engine
BARCELONA (Heath et al., 2013), provide hub service choreography service orchestration (Alonso, Casati, Kuno, & Machiraju, 2004) carried out.
Artifact systems beginning drive new application areas, case management systems (Marin, Hull, & Vaculn, 2013). However, identify two shortcomings present stateof-the-art. Firstly, artifact systems literature (Bhattacharya, Gerede, Hull, Liu, & Su, 2007;
Deutsch, Hull, Patrizi, & Vianu, 2009; Hull, 2008; Nooijen, Fahland, & Dongen, 2013) focuses
exclusively artifacts themselves. obviously need model implement
artifact infrastructure, order able reason comprehensively artifact systems,
also need account agents implementing services system, normally
done area reasoning services (Baresi, Bianculli, Ghezzi, Guinea, & Spoletini, 2007).
Secondly, pressing demand provide hub automatic choreography orchestration capabilities. well-known choreography techniques leveraged automatic
model checking techniques; orchestration recast synthesis problem, which, turn,
also benefit model checking technology. However, model checking applications
relatively well-understood plain process-based modelling, presence data makes
problems much harder virtually unexplored. Additionally, infinite domains underlying
databases lead infinite state-spaces undecidability model checking problem.
aim paper make concerted contribution problems above. Firstly,
provide computationally grounded semantics systems comprising artifact infrastructure
agents operating it. use semantics interpret temporal-epistemic language
first-order quantifiers reason evolution hub well knowledge
agents presence evolving, structured data. observe model checking problem
structures undecidable general analyse notable decidable fragment.
derive finite abstractions infinite-state artifact systems, thereby presenting technique
effective verification. evaluate methodology studying computational complexity
demonstrating use well-known scenario artifact systems literature.
1.1 Artifact-Centric Systems
Service-oriented computing concerned study development distributed applications automatically discovered composed means remote interfaces. point
distinction traditional distributed systems interoperability connectedness ser-

334

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

vices shared format data remote procedure calls. Two technology-independent
concepts permeate service-oriented literature: orchestration choreography (Alonso et al.,
2004; Singh & Huhns, 2005). Orchestration involves ordering actions possibly different
services, facilitated controller orchestrator, achieve certain overall goal. Choreography concerns distributed coordination different actions publicly observable events
achieve certain goal. MAS perspective (Wooldridge, 2001) known particularly helpful
service-oriented computing allows us ascribe information states private common goals various services. view agents system implement services
interact one another shared infrastructure environment.
key theoretical problem SOC devise effective mechanisms verify service composition correct specification. Techniques based model checking (Clarke et al.,
1999) synthesis (Berardi, Cheikh, Giacomo, & Patrizi, 2008) put forward solve
composition orchestration problem services described advertised interface level
finite state machines (Calvanese, De Giacomo, Lenzerini, Mecella, & Patrizi, 2008).
recently, attention turned services described languages WS-BPEL (Alves et al.,
2007), provide potentially unbounded variables description service process.
Again, model checking approaches successfully used verify complex service compositions (Bertoli, Pistore, & Traverso, 2010; Lomuscio, Qu, & Solanki, 2012).
WS-BPEL provides model services variables, data referenced
non-permanent. area data-centric workflows (Hull et al., 2009; Nigam & Caswell, 2003)
evolved attempt provide support permanent data, typically present form underlying databases. Although usually abstracted away, permanent data central importance
services, typically query data sources driven answers obtain; see, e.g.,
(Berardi, Calvanese, De Giacomo, Hull, & Mecella, 2005). Therefore, faithful model service behaviour cannot, general, disregard component. response this, proposals
made workflows service communities terms declarative specifications datacentric services advertised automatic discovery composition. artifact-centric
approach (Cohn & Hull, 2009) one leading emerging paradigms area. Artifactcentric systems presented along four dimensions (Hull, 2008; Hull et al., 2011).
Artifacts holders structured information available system. businessoriented scenario may include purchase orders, invoices, payment records, etc. Artifacts may
created, amended, destroyed run time; however, abstract artifact schemas provided
design time define structure artifacts manipulated system. Intuitively,
external events cause changes system, including value artifact attributes.
evolution artifacts governed lifecycles. capture changes artifact
may go creation deletion. Intuitively, purchase order may created, amended
operated fulfilled existence system terminated: lifecycle associated
purchase order artifact formalises transitions.
Services seen actors operating artifact system. represent human
software actors, possibly distributed, generate events artifact system. services may
artifacts, artifacts may shared several services. However, artifacts,
parts artifacts, visible services. Views windows respectively determine parts
artifacts artifact instances visible service. artifact hub system
maintains artifact system processes events generated services.

335

fiB ELARDINELLI , L OMUSCIO & PATRIZI

Services generate events artifact system according associations. Typically
declarative descriptions providing precondition post-conditions generation events.
generate changes artifact system according artifact lifecycles. Events processed well-defined semantics (Damaggio, Hull, & Vaculn, 2011; Hull et al., 2011) governs sequence changes artifact system may undertake upon consumption event.
semantics, based use Prerequisite-Antecedent-Consequent (PAC) rules, ensures acyclicity full determinism updates artifact system. GSM declarative language
used describe artifact systems. BARCELONA engine executes GSM-based artifact
systems (Heath et al., 2013).
partial incomplete description artifact paradigm. refer
literature details (Cohn & Hull, 2009; Hull, 2008; Hull et al., 2011).
clear next section, line agent-based approach services,
use agent-based concepts model services. artifact system represented environment, constituted evolving databases, upon agents operate; lifecycles associations
modelled local global transition functions. model intended incorporate
artifact-related concepts including views windows.
view paper address following questions. give
transition-based semantics artifacts agents operating them? language
use specify properties agents artifacts themselves? verify whether
artifact system satisfies certain properties? shown undecidable,
find suitable fragments question always answered? so, resulting
complexity? provide declarative specifications agent programs
verified model checking? Lastly, technique used mainstream scenarios
SOC literature?
1.2 Related Work
stated above, virtually current literature artifact-centric systems focuses properties
implementations artifact systems such. Little attention given actors
system, whether human artificial agents. formal techniques have, however,
put forward verify core, non-agent aspects system; following briefly compare
contribution.
knowledge verification artifact-centric business processes first discussed
Bhattacharya et al. (2007), reachability deadlocks phrased context artifactcentric systems complexity results verification problem given. Even disregarding
agent-related aspects investigated, present contribution differs markedly work
employing expressive specification language putting forward effective abstraction
procedures verification.
Gerede Su (2007) study verification technique artifact-centric systems variant
computation-tree logic. decidability verification problem proven language
considered assumption interpretation domain bounded. Decidability also
shown unbounded case making restrictions values quantified variables
range over. work presented also work unbounded domains, require
restrictions present therein: insist fact number distinct values
system exceed given threshold point run. importantly, interplay

336

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

quantification modalities considered allows us bind use variables different states. major difference feature expressive known lead potentially
undecidability.
related line research verification problem artifact systems two variants
first-order linear-time temporal logic considered (Deutsch et al., 2009; Damaggio, Deutsch, &
Vianu, 2012). Decidability verification problem retained imposing syntactic restrictions
system description specification check. effectively limits way
new values introduced every computational step used system. Properties
based arithmetic operators also considered (Damaggio et al., 2012). elements
similarity approaches one put forward here, including fact
concrete interpretation domain replaced abstract one, also significant differences.
Firstly, setting branching-time linear-time thereby resulting different expressive
power. Secondly importantly, differently similar contributions (Deutsch et al., 2009;
Damaggio et al., 2012), impose constraints nested quantifiers interaction
temporal modalities. contrast, Damaggio et al. admit guarded form quantification
state formulas, universal quantification outermost syntactic level formula,
free variables state formulas. two restrictions represent major, crucial difference
respect present work, former syntactical restrictions prevent representing
interaction data different states, instead expressible present work.
branching time setting also requires different abstraction technique. Indeed, approach
(Deutsch et al., 2009; Damaggio et al., 2012) based construction counterexample
formula checked, fact technically made possible two key factors: (i)
exclusive use universal quantification paths, guaranteed use linear-time logics;
(ii) syntactic restriction quantifiers values, permits universal quantifiers
include temporal modalities within scope. None features required work.
Namely, allow existential universal quantification paths present (although
CTL fashion), put restriction use first-order quantifiers. Additionally,
abstraction results present given general terms semantics declarative
programs depend particular presentation system.
Finally, following approach similar ours, Bagheri Hariri et al. (2013) give conditions
decidability model checking problem data-centric dynamic systems, i.e., dynamic
systems relational states. case specification language used first-order version
-calculus. temporal fragment subsumed -calculus, two specification
languages different expressive power, since use indexed epistemic modalities well
common knowledge operator. retain decidability, like here, authors assume constraint
size states. However, differently contribution, Bagheri Hariri et al. also
assume limited forms quantification whereby individuals persisting system evolution
quantified over. make restriction here.
Irrespective above, important feature characterises work
set-up entirely based epistemic logic multi-agent systems. use agents represent
autonomous services operating system agent-based concepts play key role
modelling, specifications, verification techniques put forward. Differently
approaches presented concerned whether artifact system meets
particular specification. Instead, also wish consider knowledge agents
system acquire interacting among artifact system system run.
337

fiB ELARDINELLI , L OMUSCIO & PATRIZI

Additionally, abstraction methodology put forward modular respect agents
system, is, first define abstract agents compose together obtain
abstract system. features enable us give constructive procedures generation
finite abstractions artifact-centric programs associated infinite models. aware
work literature tackling aspects.
paper combines expands preliminary results artifact-centric systems (Belardinelli, Lomuscio, & Patrizi, 2011a, 2011b, 2012a, 2012b). particular, technical set
artifacts agents different preliminary studies makes natural
express artifact-centric concepts views. Differently previous attempts,
incorporate operator common knowledge provide constructive methods define abstractions. also consider complexity verification problem, previously unexplored,
evaluate technique detail case study.
1.3 Scheme Paper
rest paper organised follows. Section 2 introduce artifact-centric multiagent systems (AC-MAS), semantics using throughout paper describe agents
operating artifact system. section put forward FO-CTLK, first-order logic
knowledge time reason evolution knowledge agents
artifact system. enables us propose satisfaction relation based notion bounded
quantification, define model checking problem, highlight properties isomorphic
states. immediate result explore concerns undecidability model checking
problem AC-MAS general setting.
Section 3 devoted identifying subclass AC-MAS admits decidable model checking problem full FO-CTLK specifications. key finding bounded uniform
AC-MAS, class identified studying strong bisimulation relation, admit finite, truth-preserving
abstractions FO-CTLK specification. Section 3.4 explore verification problem also investigate complexity thereby showing EXPSPACE-complete.
turn attention artifact programs Section 4 defining concept artifact-centric
programs. define natural, first-order preconditions post-conditions line
artifact-centric approach. give semantics terms AC-MAS show
generated models precisely uniform AC-MAS studied earlier paper. follows that,
boundedness conditions naturally expressed, model checking problem
artifact-centric programs decidable executed finite models.
Section 4.2 reports scenario artifact systems literature. used exemplify
technique providing finite abstractions effectively verified. conclude Section 5
consider limitations approach point work.

2. Artifact-Centric Multi-agent Systems
section formalise artifact-centric systems state verification problem. data
databases equally important constituents artifact systems, formalisation artifacts
relies underpinning concepts. However, discussed previous section,
give prominence agent-based concepts. such, define systems comprising
artifacts agents interacting it.
338

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

standard paradigm logic-based reasoning agents interpreted systems (Parikh &
Ramanujam, 1985; Fagin et al., 1995). setting agents endowed private local states
evolve performing actions according individual protocol. data play key part,
well allow us specify properties artifact system, define agents local states
evolving database instances. call formalisation artifact-centric multi-agent systems (ACMAS). AC-MAS enable us represent naturally concisely concepts much used artifact
paradigm one view discussed earlier.
specification language include temporal-epistemic logic also quantification
domain represent data. usual verification setting, formally define
model checking problem set up.
2.1 Databases First-Order Logic
discussed above, use databases basic building blocks defining states
agents artifact system. fix notation terminology used. refer
literature details databases (Abiteboul, Hull, & Vianu, 1995).
Definition 2.1 (Database Schemas) (relational) database schema set {P1 /q1 , . . . , Pn /qn }
relation symbols Pi , associated arity qi N.
Instances database schemas defined interpretation domains, i.e., sets individuals.
Definition 2.2 (Database Instances) Given countable interpretation domain U database
schema D, D-instance U mapping associating relation symbol Pi
finite qi -ary relation U , i.e., D(Pi ) U qi .
set D-instances countable interpretation domain U denoted D(U ).
simply refer instances whenever database schema clear context. active
domain instance D, denoted adom(D), set individuals U occurring
tuple predicate interpretation D(Pi ). Observe that, since contains finite number
relation symbols D(Pi ) finite, adom(D). Also, rest paper assume
interpretation domains always countable without explictly mentioning fact.
fix notation, recall syntax first-order formulas equality function
symbols. Let Var countable set individual variables Con finite set individual
constants. term element Var Con.
Definition 2.3 (FO-formulas D) Given database schema D, formulas firstorder language LD defined following BNF grammar:
::= = t0 | Pi (t1 , . . . , tqi ) | | | x
Pi D, t1 , . . . , tqi qi -tuple terms t, t0 terms.
assume = special binary predicate fixed obvious interpretation. summarise,
LD first-order language equality relational vocabulary function symbols
finitely many constant symbols Con. Observe considering finite set constants
limitation. Indeed, since working finite sets formulas, Con always
defined able express formula interest.
339

fiB ELARDINELLI , L OMUSCIO & PATRIZI

following use standard abbreviations , , , 6=. Also, free bound
variables defined standard. formula denote set variables var(),
set free variables free(), set constants con(). write (~x) list
explicitly arbitrary order free variables x1 , . . . , x` . slight abuse notation,
treat ~x set, thus write ~x = free(). sentence formula free variables.
Given interpretation domain U suchthat Con U , assignment function
: Var 7 U .

assignment , denote ux assignment that: (i) ux (x) = u; (ii)
ux (x0 ) = (x0 ), every x0 Var different x. convenience, extend assignments
constants (t) = t, Con; is, assume Herbrand interpretation constants.
define semantics LD .
Definition 2.4 (Satisfaction FO-formulas) Given D-instance D, assignment ,
FO-formula LD , inductively define whether satisfies , written (D, ) |= ,
follows:
(D, ) |= Pi (t1 , . . . , tqi )
(D, ) |= = t0
(D, ) |=
(D, ) |=
(D, ) |= x

iff
iff
iff
iff
iff

h(t1 ), . . . , (tqi )i D(Pi )
(t) = (t0 )
case (D, ) |=
(D, ) |= (D, ) |=

u adom(D), (D, ux ) |=

formula true D, written |= , iff (D, ) |= , assignments .
Observe adopt active-domain semantics, is, quantified variables range
active domain D. claim form quantification sufficient express specifications
interest (see Section 4.2) retaining decidability. Also notice constants interpreted
rigidly; so, two constants equal syntactically same. rest
paper, assume every interpretation domain includes Con. Also, usual shortcut, write
(D, ) 6|= express case (D, ) |= .
Finally, introduce operator D-instances used later paper. Let
primed version database schema schema D0 = {P10 /q1 , . . . , Pn0 /qn } obtained
syntactically replacing predicate symbol Pi primed version Pi0 arity.
Definition 2.5 ( Operator) Given two D-instances D0 , define D0 (D D0 )instance D0 (Pi ) = D(Pi ) D0 (Pi0 ) = D0 (Pi ).
Intuitively, operator defines disjunctive join two instances, relation symbols
interpreted according D, primed versions interpreted according D0 .
2.2 Artifact-Centric Multi-agent Systems
following introduce semantic structures use throughout paper.
define artifact-centric multi-agent system system comprising environment representing
artifacts finite set agents interacting environment. agents views
artifact state, i.e., projections status particular artifacts, assume building blocks
private local states also modelled database instances. line interpreted
systems semantics (Fagin et al., 1995) everything agents states needs present
environment; portion may entirely private replicated agents states. So,
start introducing notion agent.
340

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

Definition 2.6 (Agent) Given interpretation domain U , agent tuple = hD, Act, P ri,
where:
local database schema;
Act finite set action types (~
p), p~ tuple abstract parameters;
P r : D(U ) 7 2Act(U ) local protocol function, Act(U ) set ground
actions form (~u) (~
p) Act ~u U |~p| tuple ground parameters.
Intuitively, given time agent local state l D(U ) represents
information agent disposal. sense follow standard approach multiagent systems (Fagin et al., 1995), require information structured database.
Again, following standard literature assume agents autonomous proactive
perform actions Act according protocol function P r, returns set grounded
actions enabled local state. definition use term abstract parameters
denote variables, i.e., language particular action parameters given; use term
ground parameters refer concrete values.
assume agents interact among environment comprising
artifacts system. artifacts entities involving data processes, see
collections database instances paired actions governed special protocols.
Without loss generality assume environment state single database instance
including artifacts system. purely formal point view allows us represent
environment special agent. course, specific instantiation environment
agents different entities, exactly line standard propositional version interpreted
systems.
therefore define synchronous composition agents environment.
Definition 2.7 (Artifact-Centric Multi-agent Systems) Given interpretation domain U
set Ag = {A0 , . . . , } agents Ai = hDi , Acti , P ri defined U , artifact-centric multiagent system (or AC-MAS) tuple P = hAg, s0 , where:
Q
s0 Ai Ag Di (U ) initial global state;
Q
Q
Ai Ag Di (U )
:
global transition function,
Ai Ag Di (U ) Act(U ) 7 2
Act(U ) = Act0 (U ) Actn (U ) set global (ground) actions, (hl0 , . . . , ln i,
h0 (~u0 ), . . . , n (~un )i) defined whenever (~ui ) P ri (li ) every n.

see later sections, AC-MAS natural extension interpreted systems
first order account environments constituted artifact-centric systems. seen
specialisation quantified interpreted systems (Belardinelli & Lomuscio, 2012), general
extension interpreted systems first-order case.
formalisation agent A0 typically referred environment
E. environment normally includes artifacts system (notably assuming D0 0<in Dn ),
well additional information facilitate communication
agents hub, e.g.,
messages transit etc. follows consider D0 = 0<in Dn simplicity; modelling
choice impact results presented later on. given time AC-MAS described
tuple database instances, representing agents system well artifact
341

fiB ELARDINELLI , L OMUSCIO & PATRIZI

system. single interpretation domain database schemas given. Note
break generality representation always extend domain agents
environment composing single AC-MAS. global transition function defines
evolution system synchronous composition actions environment
agents system.
Much interaction interested modelling involves message exchanges payload, hence action parameters, agents environment, i.e., agents operating
artifacts. However, note formalisation preclude us modelling agent-toagent interactions, global transition function rule successors
agents change local state following actions. Also observe essential concepts
views easily expressed AC-MAS insisting local state agent includes part
environments, i.e., artifacts agent access to. AC-MAS need views
defined, also possible views empty.
artifact-based concepts lifecycles naturally expressed AC-MAS. artifacts
modelled part environment, lifecycle naturally encoded AC-MAS simply
sequence changes induced transition function fragment environment
representing lifecycle question. show example Section 2.4.
technical remarks follow. simplify notation, denote global ground action

~ (~u),
~ = h0 (p0 ), . . . , n (pnQ
)i ~u = h~u0 , . .Q
. , ~un i, ~ui appropriate size.
define transition relation Ai Ag Di (U ) Ai Ag Di (U ) s0
exists
~ (~u) Act(U ) s0 (s,
~ (~u)). s0 , say s0
successor s. run r infinite sequence s0 s1 , s0 = s.
.
n N, take r(n) = sn . state s0 reachable exists run r global
state r(0) = r(i) = s0 , 0. assume relation serial, i.e.,
every global state exists s0 s0 . easily obtained assuming
agent skip action enabled local state performing skip induces
changes local states. introduce set states reachable initial
state s0 according transition relation . Notice assuming unique initial state
hinder generality approach, finite set states encode transitions
s0 states I. plain interpreted systems (Fagin et al., 1995), say two global
states = hl0 , . . . , ln s0 = hl00 , . . . , ln0 epistemically indistinguishable agent Ai , written
s0 , li = li0 . Differently interpreted systems local equality evaluated database
instances. convenience use also concept temporal-epistemic (t.e., short) run.
Formally t.e. run r state infinite sequence s0 ; s1 ; . . . s0 =
si si+1 si k si+1 , k Ag. state s0 said temporally-epistemically
reachable (t.e. reachable, short) exists t.e. run r global state r(0) =
0 r(i) = s0 . Obviously, temporal-epistemic runs include purely
temporal runs special case. Also, notice admit U infinite, thereby allowing
possibility set states infinite. Indeed, unless specify otherwise, assume
working infinite-state AC-MAS.
Finally, technical reasons useful refer global database schema = D0 Dn
AC-MAS. Every global stateS = hl0 , . . . , ln associated (global) D-instance
Ds D(U ) Ds (Pi ) = jAg lj (Pi ), Pi D. omit subscript whenever
clear context write adom(s) adom(Ds ). justification choice
comes fact think agent partial, although truthful, view
342

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

global state. relation appears several agent database schemas, possibly different
interpretations, means agent aware subset total extension
relation. maintain modeling choice justified application artifact systems,
become apparent Section 2.4. Notice every S, Ds associated
unique, converse true general. Finally, lift disjoint union operator
.
global states s0 = hl0 l00 , . . . , ln ln0 i. seen Ds Ds0 Dss0 represent
fact D0 -instance.
2.3 Model Checking
define problem verifying artifact-centric multi-agent system specification
interest. following artifact-centric model, wish give data prominence
processes. deal data underlying database instances, specification language needs
include first-order logic. Further, require temporal logic describe system execution.
Lastly, use epistemic logic express information agents disposal. Hence,
define first-order temporal-epistemic specification language interpreted AC-MAS.
specification language used Section 4 formalise properties artifact-centric programs.
Definition 2.8 (The Logic FO-CTLK) first-order CTLK (or FO-CTLK) formulas
database schema inductively defined following BNF:
::= | | | x | AX | AU | EU | Ki | C
LD 0 < n.
notions free bound variables FO-CTLK extend straightforwardly LD , well
functions var, free, con. usual, temporal formulas AX AU 0 (resp. EU 0 )
read runs, next step runs (resp. run), 0 .
epistemic formulas Ki C intuitively mean agent Ai knows common
knowledge among agents respectively. use abbreviations EX, AF , AG,
EF , EG standard. Observe free variables occur within scope modal
operators, thus admitting unconstrained alternation quantifiers modal operators, thereby
allowing us refer elements different modal contexts.
semantics FO-CTLK formulas defined follows.
Definition 2.9 (Satisfaction FO-CTLK) Consider AC-MAS P, FO-CTLK formula ,
state P, assignment . inductively define whether P satisfies , written
(P, s, ) |= , follows:
(P, s, ) |=
(P, s, ) |=
(P, s, ) |= 0
(P, s, ) |= x
(P, s, ) |= AX
(P, s, ) |= AU 0

iff
iff
iff
iff
iff
iff

(P, s, ) |= EU 0

iff

(Ds , ) |= , FO-formula
case (P, s, ) |=
(P, s, ) |= (P, s, ) |= 0
u adom(s), (P, s, ux ) |=
runs r, r(0) = s, (P, r(1), ) |=
runs r, r(0) = s, k 0 s.t. (P, r(k), ) |= 0 ,
j, 0 j < k implies (P, r(j), ) |=
run r, r(0) = k 0 s.t. (P, r(k), ) |= 0 ,
343

fiB ELARDINELLI , L OMUSCIO & PATRIZI

j, 0 j < k implies (P, r(j), ) |=
s0 S, s0 implies (P, s0 , ) |=
s0 S, s0 implies (P, s0 , ) |=

transitive closure 1in .
(P, s, ) |= Ki
(P, s, ) |= C

iff
iff

formula said true state s, written (P, s) |= , (P, s, ) |= assignments
. Moreover, said true P, written P |= , (P, s0 ) |= .
key concern paper explore model checking AC-MAS first-order
temporal-epistemic specifications (Grohe, 2001).
Definition 2.10 (Model Checking Problem) Given AC-MAS P FO-CTLK formula
model checking problem consists finding assignment (P, s0 , ) |= .
easy see whenever U finite model checking problem decidable P finitestate system. general, however, case. see this, notice that, assuming computability, agents protocol functions P ri AC-MAS transition function ,
finitely represented (e.g., Turing machines). Since components agents AC-MAS
definitions finite, follows AC-MAS, particular infinite-state ones, admit finite
representation. Assuming fixed representation formalism, following result.
Theorem 2.11 model checking problem AC-MAS w.r.t. FO-CTLK undecidable.
Proof (sketch). proved showing every Turing machine whose tape contains
initial input simulated artifact system PT,I . problem checking whether
terminates particular input reduced checking whether PT,I |= , encodes
termination condition. detailed construction similar work Deutsch, Sui,
Vianu (2007, Thm. 4.10).
Given general setting model checking problem defined above, negative result surprising. following identify semantic restrictions problem
decidable.
2.4 Order-to-Cash Scenario
analyse business process inspired concrete IBM customer use case (Hull et al., 2011).
order-to-cash scenario describes interactions number agents e-commerce situation
relating purchase delivery product. agents artifact-centric system consist
manufacturer, customers, suppliers. process begins customer
prepares submits purchase order (PO), i.e., list products customer requires,
manufacturer. Upon receiving PO, manufacturer prepares material order (MO), i.e.,
list components needed assemble requested products. manufacturer selects
supplier forwards relevant material order. Upon receiving MO supplier either
accept reject it. former case proceeds deliver requested components
manufacturer. latter case notifies manufacturer rejection. MO rejected,
manufacturer deletes prepares submits new MO. manufacturer
receives delivered components, assembles product and, provided order paid
344

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

id

PO
prod code offer

status

id

MO
prod code price

status

Products
prod code budget
Materials
mat code cost
Figure 1: Data Model Order-to-Cash Scenario.
createPO

prepared

submitPO

pending

pay

paid

shipPO

shipped

deletePO

(a) Purchase Order lifecyle
accepted

shipMO

shipped

acceptMO
createMO

preparation

doneMO

submitted

deleteMO

rejectMO
rejected

deleteMO

(b) Material Order lifecyle

Figure 2: Lifecycles artifacts involved order-to-cash scenario.
for, delivers customer. manufacturer order directly indirectly related
PO deleted PO deleted.
encode order-to-cash business process artifact-centric multi-agent system,
artifact data models represented database schemas evolution characterised appropriate set operations. natural identify two artifact types, representing
PO MO. reserve distinguished relation artifact type. addition,
introduce static relations store product material information. result, data model
order-to-cash scenario associated attributes given Figure 1.
intended meaning relations self-explanatory. Note presence attribute status
relations corresponding artifact classes. intuitive representation artifact lifecycles,
i.e., evolution key records artifacts states, capturing dependence
actions artifact statuses, shown Figure 2. example, purchase order, initial
status prepared, created agent customer action createPO. order
submitted agent manufacturer, PO status changes pending. transitions
labelled pay, shipPO deletePO, act similarly, according semantics, status
purchase order. Note incomplete representation business process,
interaction actions artifact data content represented.

345

fiB ELARDINELLI , L OMUSCIO & PATRIZI

formally encode scenario AC-MAS. sake presentation
follows assume dealing three agents only: one customer c, one manufacturer
one supplier s. database schema Di agent {c, m, s} given as:
Customer c:
Dc = {Products(prod code, budget), PO(id , prod code, offer , status)};
Manufacturer m:
Dm = {PO(id , prod code, offer , status), MO(id , prod code, price, status)};
Supplier s:
Ds = {Materials(mat code, cost), MO(id , prod code, price, status)}.
consider infinite set Uotc alphanumeric strings interpretation domain, introduce parametric action transition lifecycles Figure 2. Also, assume
initial state non-empty relations Products Materials, contain background information, catalogue available products. define agents
order-to-cash scenario follows.
Definition 2.12 agents Ac , given
Ac = hDc , Actc , P rc i, (i) Dc above; (ii) Actc = {createPO(id , pcode),
submitPO(id ), pay(id ), deletePO(id )}; (iii) P rc respects intended meaning
customers actions. instance, createPO(id , pcode) P rc (lc ) iff interpretation
lc (Products) relation Products local state lc contains tuple hpcode, bi
budget b.
= hDm , Actm , P rm i, (i) Dm above; (ii) Actm = {createMO(id , price),
doneMO(id ), shipPO(id ), deleteMO(id )}; (iii) P rm respects intended meaning
manufacturers actions. instance, createMO(po id , price) P rm (lm ) iff
interpretation lm (MO) relation MO local state lm contain tuple
hpo id, pc, pr, preparationi PO id po id.

=
hDs , Acts , P rs i, (i) Ds above;
(ii) Acts
=
{acceptMO(id ), rejectMO(id ), shipMO(id )}; (iii) P rc respects intended
meaning suppliers actions. instance, acceptMO(mo id ) P rs (ls ) iff ls (M O)
contain tuple id mo id status accepted.
Further, define AC-MAS induced set agents Agotc = {Ac , , }
according Definition 2.7.
Definition 2.13 Given set agents Agotc
hAgotc , s0otc , otc

=

{Ac , , }, AC-MAS Potc

=

s0otc = hlc , lm , ls initial global state, non-empty relations Products
Materials lc ls respectively;
otc global transition function defined respect intended meaning
evolution order-to-cash scenario. instance, consider global action
346

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

(~u) = hcreatePO(pc), doneMO(m), acceptMO(m 0 )i enabled respective protocols global state s. definition actions createPO(pc), doneMO(m),
acceptMO(m 0 ) li (s) P ri {c, m, s} implies Products relation contains information product pc. Also, interpretation relation MO
contains tuples hm, p, pr, preparationi hm0 , p0 , pr0 , submittedi products p
p0 . Hence, s0 otc (s, (~u)) iff interpretation relation PO s0 extends
Ds (P O) tuple hid, pc, b, preparedi, id fresh id. tuples material orders m0 updated Ds0 (M O) becoming hm, p, pr, submittedi
hm0 , p0 , pr0 , acceptedi, respectively. element changed transition.
Clearly, function otc given easily completed encode artifacts lifecycles
given Figure 2. section 4.2 give succinct encoding Potc terms artifactcentric program.
investigate properties present business process using specifications
FO-CTLK. instance, following formula intuitively specifies manufacturer knows
material order MO match corresponding purchase order PO:
match = AG id, pc (pr, O(id, pc, pr, s) Km o, s0 P O(id, pc, o, s0 ))
next specification states given material order MO, customer eventually know
corresponding PO shipped.
fulfil

= AG id, pc (pr, O(id, pc, pr, s) EF Kc P O(id, pc, o, shipped))

Further, may interested checking whether budget costs always kept secret
supplier customer c respectively, whether customer (resp., supplier)
knows fact:
budget = Kc pc AG b Ks Products(pc, b)
cost = Ks mc AG c Kc Materials(mc, c)
interesting specifications describing properties artifact system agents operating similarly formalised FO-CTLK, thereby providing engineer valuable
tool assess implementation.
Observe AC-MAS order-to-cash scenario infinite number states thereby
making difficult investigate means traditional model checking techniques. return
scenario Subsection 4.2 investigate may still verified.
develop methodology associating finite abstractions infinite AC-MAS.

3. Abstraction Artifact-Centric Multi-agent Systems
previous section observed model checking AC-MAS FO-CTLK undecidable general. clearly interest isolate decidable settings. follows identify
semantic constraints resulting decidable model checking problem. investigation carried
rather natural subclass AC-MAS call bounded, defined below. goal
proceeding manner identify finite abstractions infinite-state AC-MAS verification programs admit bounded AC-MAS models conducted them, rather
original infinite-state AC-MAS. see detail Section 4.
347

fiB ELARDINELLI , L OMUSCIO & PATRIZI

key concept enables us achieve uniformity. Uniform AC-MAS
systems behaviour depend actual data present states.
means system contains possible transitions enabled according parametric
action rules, thereby resulting full transition relation. notion related genericity
databases (Abiteboul et al., 1995). use term uniformity refer transition
systems databases.
achieve finite abstractions proceed follows. first propose adaptation notion
isomorphism setting; introduce bisimulations; finally Subsection 3.2 show
notion exploited guarantee uniform AC-MAS satisfy FO-CTLK
formulas. use result show bounded, uniform systems admit finite abstractions
(Subsection 3.3). complexity model checking problem analysed Subsection 3.4.
rest section let P = hAg, s0 , P 0 = hAg 0 , s00 , 0 two AC-MAS
assume, unless stated differently, = hl0 , . . . , ln S, s0 = hl00 , . . . , ln0 0 .
3.1 Isomorphisms
investigate concept isomorphism AC-MAS. needed later sections
define finite abstractions infinite-state AC-MAS.
Definition 3.1 (Isomorphism) Two local states l, l0 D(U ) isomorphic, written l ' l0 , iff
exists bijection : adom(l) Con 7 adom(l0 ) Con that:
(i) identity Con;
(ii) every Pi D, ~u U qi , ~u l(Pi ) iff (~u) l0 (Pi ).
case, say witness l ' l0 .
Two global states s0 0 isomorphic, written ' s0 , iff exists bijection
: adom(s) Con 7 adom(s0 ) Con every j Ag, witness lj ' lj0 .
Notice isomorphisms preserve interpretation constants Con well predicates
local states renaming corresponding terms. function called
witness ' s0 . Obviously, relation ' equivalence relation. Given function f :
U 7 U 0 defined adom(s), f (s) denotes instance D(U 0 ) obtained renaming
u adom(s) f (u). f also injective (thus invertible) identity Con,
f (s) ' s.
Example example isomorphic states, consider agent local database schema =
{P1 /2, P2 /1}, let U = {a, b, c, . . .} interpretation domain, fix set Con = {b}
constants. Let l local state l(P1 ) = {ha, bi, hb, di} l(P2 ) = {a} (see Figure 3).
Then, local state l0 l0 (P1 ) = {hc, bi, hb, ei} l0 (P2 ) = {c} isomorphic l.
easily seen considering isomorphism , where: (a) = c, (b) = b, (d) = e.
However, state l00 l00 (P1 ) = {hf, di, hd, ei} l00 (P2 ) = {f } isomorphic l.
Indeed, although bijection exists maps l l00 , easy see none
0 (b) = b.
Note that, isomorphic states relational structure, two isomorphic states
necessarily satisfy FO-formulas satisfaction depends also values assigned
free variables. account this, introduce following notion.
348

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

l(P1 )
b
b

l(P2 )


l0 (P1 )
c b
b e

l0 (P2 )
c

l00 (P1 )
f
e

l00 (P2 )
f

Figure 3: Examples isomorphic non-isomorphic local states.
Definition 3.2 (Equivalent assignments) Given two states s0 0 , set variables V V ar, two assignments : V ar 7 U 0 : V ar 7 U 0 equivalent V w.r.t.
s0 iff exists bijection : adom(s) Con (V ) 7 adom(s0 ) Con 0 (V ) that:
(i) |adom(s)Con witness ' s0 ;
(ii) 0 |V = |V .
Intuitively, equivalent assignments preserve (in)equalities variables V
constants s, s0 renaming. Note that, definition, implies s, s0 isomorphic.
say two assignments equivalent FO-CTLK formula , omitting states
s0 clear context, equivalent free().
show following standard result first-order (non-modal) logic, i.e., isomorphic
states satisfy exactly FO-formulas (Abiteboul et al., 1995).
Proposition 3.3 Given two isomorphic states s0 0 , FO-formula , two
assignments 0 equivalent ,
(Ds , ) |= iff (Ds0 , 0 ) |=
Moreover, FO-sentence,
Ds |= iff Ds0 |=
Thus, isomorphic states cannot distinguished FO-sentences. enables us use
notion, defining simulations.
3.2 Bisimulations
Plain bisimulations known satisfaction preserving modal propositional setting (Blackburn, de Rijke, & Venema, 2001). following explore conditions
applies AC-MAS well. introduce notion bisimulation, based isomorphisms,
later explore properties context uniform AC-MAS.
Definition 3.4 (Simulation) relation R 0 simulation hs, s0 R implies:
1. ' s0 ;
2. every S, exists t0 0 s.t. s0 t0 , ' s0 t0 , ht, t0 R;
3. every S, every 0 < n, exists t0 0 s.t. t0 ,
' s0 t0 , ht, t0 R.

349

fiB ELARDINELLI , L OMUSCIO & PATRIZI

P
1

2

3

4

5

P0
1

2

Figure 4: Bisimilar AC-MAS satisfying FO-CTLK formulas.
Definition 3.4 many similarities standard notion simulation propositional
setting. particular, co-inductive structure definition requires similar states satisfy
local property preserve along corresponding transitions. However, differently
propositional case, insist ' s0 t0 ; ensures similar transitions
AC-MAS preserve isomorphic disjoint unions.
state s0 0 said simulate S, written s0 , iff exists simulation R
s.t. hs, s0 R. ambiguity arises, simply say s0 similar. Note
similar states isomorphic, condition (2) ensures ' s0 . similarity relation
shown largest simulation, reflexive transitive 0 . Further, say P 0
simulates P, written P P 0 , s0 s00 .
Simulations naturally extended bisimulations, follows.
Definition 3.5 (Bisimulation) relation B 0 bisimulation iff B B 1 =
{hs0 , si | hs, s0 B} simulations.
Two states s0 0 said bisimilar, written s0 , iff exists bisimulation
B hs, s0 B. shown largest bisimulation, equivalence
relation, 0 . say P P 0 bisimilar, written P P 0 iff s0 s00 .
instructive note bisimilar systems preserve FO-CTLK formulas.
markedly different modal propositional case.
Example Consider Figure 4, Con = P P 0 given follows. number n
agents equal 1, define = D0 = {P/1} U = N; s0 (P ) = s00 (P ) = {1}; = {hs, s0 |
s(P ) = {i}, s0 (P ) = {i + 1}}; 0 = {hs, s0 | s(P ) = {i}, s0 (P ) = {(i mod 2) + 1}}. Notice
D(N) 0 D(N). Clearly P P 0 . Now, consider constant-free
FO-CTLK formula = AG(x(P (x) AXAGP (x))). easily seen P |=
P 0 6|= .
shows that, differently propositional case, bisimilarity sufficient
condition guarantee preservation FO-CTLK formulas. Intuitively, consequence
fact bisimilar AC-MAS preserve value associations along runs. instance,
value 1 P 0 associated infinitely many times odd values occurring P. quantifying
across states able express fact therefore distinguish two structures.
difficulty as, intuitively, would like use bisimulations demonstrate existence finite
abstractions. However, show later, happens class uniform AC-MAS, defined
below.

350

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

Definition 3.6 (Uniformity) AC-MAS P said uniform iff every s, t, s0 S, t0
D(U ),
1. (s,
~ (~u)) ' s0 t0 witness , every constant-preserving
0
bijection extends ~u, t0 (s0 ,
~ (0 (~u)));
2. ' s0 t0 , s0 t0 .
definition captures idea actions take account operate relational
structure states action parameters, irrespective actual data contain (apart
finite set constants). Intuitively, uniformity expresses reached executing (~u)
s, replace element v v 0 s, ~u t, obtaining s0 , ~u0 t0 , t0
reached executing (~u0 ) s0 . terms underlying Kripke structures, i.e., frames
induced relations Ai Ag, means systems full ,
is, uniform AC-MAS states t0 identified indeed part system reachable
s0 . similar condition required epistemic relation. property uniform systems
latter requirement implied former, shown following result.
Proposition 3.7 AC-MAS P satisfies req. 1 Def. 3.6 adom(s0 ) Con, req. 2
also satisfied.
Proof. ' s0 t0 , witness : adom(s) adom(t) Con 7 adom(s0 )
adom(t0 ) Con identity Con (hence adom(s0 )). Assume t, thus li (s) = li (t),
li (s0 ) = (li (s)) = (li (t)) = li (t0 ). Notice guarantee s0 t0 ,
need prove t0 S. done showing t0 reachable s0 . Since
reachable s0 , exists run s0 s1 . . . sk s.t. sk = t. Extend total
injective function 0 : adom(s0 ) adom(sk ) Con 7 U . always done
|U | |adom(s0 ) adom(sk ) Con|. consider sequence 0 (s0 ), 0 (s1 ), . . . , 0 (sk ).
Since adom(s0 ) Con (s0 ) = s0 and, 0 extends , 0 (s0 ) = (s0 ) = s0 .
Further, 0 (sk ) = (t) = t0 . repeated applications req. 1 show 0 (sm+1 )
(0 (sm ),
~ (0 (~u))) whenever sm+1 (sm ,
~ (~u)), < k. Hence, sequence actually
0
0
0
0
run s0 . Thus, S, .
Thus, long adom(s0 ) Con, check whether AC-MAS uniform, sufficient
take account transition function.
distinctive feature uniform systems isomorphic states bisimilar.
Proposition 3.8 AC-MAS P uniform, every s, s0 S, ' s0 implies s0 .
Proof. prove B = {hs, s0 | ' s0 } bisimulation. Observe since '
equivalence relation, B. Thus B symmetric B = B 1 . Therefore, proving B
simulation proves also B 1 simulation; hence, B bisimulation. end, let
hs, s0 B, assume S. Then, (s, (~u)) (~u) Act(U ).
Consider witness ' s0 . cardinality considerations extended total
injective function 0 : adom(s) adom(t) {~u} Con 7 U . Consider 0 (t) = t0 ; follows 0
witness ' s0 t0 . Since P uniform, t0 (s0 , (0 (~u))), is, s0 t0 . Moreover,
0 witness ' t0 , thus ht, t0 B. Next assume hs, s0 B t, S.
351

fiB ELARDINELLI , L OMUSCIO & PATRIZI

reasoning find witness ' s0 , extension 0 s.t. t0 = 0 (t)
0 witness ' s0 t0 . Since P uniform, s0 t0 ht, t0 B.
result intuitively means submodels generated isomorphic states bisimilar.
Next prove partial results, useful proving main preservation theorem. first two guarantee appropriate cardinality constraints bisimulation preserves
equivalence assignments w.r.t. given FO-CTLK formula.
Lemma 3.9 Consider two bisimilar uniform AC-MAS P P 0 , two bisimilar states
s0 0 , FO-CTLK formula . every assignments 0 equivalent w.r.t.
s0 , that:
1. every s.t. t, |U 0 | |adom(s) adom(t) Con (free())|,
exists t0 0 s.t. s0 t0 , t0 , 0 equivalent w.r.t. t0 .
2. every s.t. t, |U 0 | |adom(s) adom(t) Con (free())|,
exists t0 0 s.t. s0 t0 , t0 , 0 equivalent w.r.t. t0 .
Proof. prove (1), let bijection witnessing 0 equivalent w.r.t. s0 .
Suppose t. Since s0 , definition bisimulation exists t00 0 s.t. s0 t00 ,
.
' s0 t00 , t00 . define Domj = adom(s) adom(t) Con, partition into:
.
Dom = adom(s) Con (adom(t) (free());
.
Dom0 = adom(t) \ Dom .
Let 0 : Dom0 7 U 0 \ Im() invertible total function. Observe |Im()| =
|adom(s0 ) Con 0 (free())| = |adom(s) Con (free())|, thus fact |U 0 |
|adom(s) adom(t) Con (free())| |U 0 \ Im()| |Dom(0 )|, guarantees
existence 0 .
Next, define j : Domj 7 U 0 follows:

(u), u Dom
j(u) =
0 (u), u Dom0
Obviously, j invertible. Thus, j witness ' s0 t0 , t0 = j(t). Since
' s0 t00 ' equivalence relation, obtain s0 t0 ' s0 t00 . Thus, s0 t0 ,
P 0 uniform. Moreover, 0 equivalent w.r.t. t0 , construction t0 .
check t0 , observe that, since t0 ' t00 P 0 uniform, Prop. 3.8 follows t0 t00 .
Thus, since t00 transitive, obtain t0 . proof (2) analogous
structure therefore omitted.
proven result tight, i.e., cardinality requirement violated,
exist cases assignment equivalence preserved along temporal epistemic transitions.
Lemma 3.9 easily generalises t.e. runs.
Lemma 3.10 Consider two bisimilar uniform AC-MAS P P 0 , two bisimilar states
s0 0 , FO-CTLK formula , two assignments 0 equivalent w.r.t.
s0 . every t.e. run r P, r(0) = 0, |U 0 | |adom(r(i)) adom(r(i + 1))
Con (free())|, exists t.e. run r0 P 0 s.t. 0:
352

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

(i) r0 (0) = s0 ;
(ii) r(i) r0 (i);
(iii) 0 equivalent w.r.t. r(i) r0 (i).
(iv) every 0, r(i) r(i + 1) r0 (i) r0 (i + 1), r(i) j r(i + 1),
j, r0 (i) j r0 (i + 1).
Proof. Let r t.e. run satisfying lemmas hypothesis. inductively build r0 show
conditions satisfied. = 0, let r0 (0) = s0 . hypothesis, r s.t. |U 0 |
|adom(r(0)) adom(r(1)) Con (free())|. Thus, since r(0) ; r(1), Lemma 3.9
exists t0 0 s.t. r0 (0) ; t0 , r(1) t0 , 0 equivalent w.r.t. r(1) t0 . Let
r0 (1) = t0 . Lemma 3.9 guarantees transitions r0 (0) ; t0 r(0) ; r(1) chosen
either temporal epistemic index.
case > 0 similar. Assume r(i) r0 (i) 0 equivalent
w.r.t. r(i) r0 (i). Since r(i) ; r(i + 1) |U 0 | |adom(r(i)) adom(r(i + 1)) Con
(free())|, Lemma 3.9 exists t0 0 s.t. r0 (i) ; t0 , 0 equivalent
w.r.t. r(i + 1) t0 , r(i + 1) t0 . Let r0 (i + 1) = t0 . clear r0 t.e. run P 0 ,
that, Lemma 3.9, transitions r0 chosen fulfil requirement (iv).
prove FO-CTLK formulas cannot distinguish bisimilar uniform AC-MAS.
marked contrast earlier example section related bisimilar
non-uniform AC-MAS.
Theorem 3.11 Consider two bisimilar uniform AC-MAS P P 0 , two bisimilar states
s0 0 , FO-CTLK formula , two assignments 0 equivalent w.r.t. s0 .

1. every t.e. run r s.t. r(0) = s, k 0 |U 0 | |adom(r(k)) adom(r(k +
1)) Con (free())| + |var() \ free()|;
2. every t.e. run r0 s.t. r0 (0) = s0 , k 0 |U | |adom(r0 (k)) adom(r0 (k +
1)) Con 0 (free())| + |var() \ free()|;

(P, s, ) |= iff (P 0 , s0 , 0 ) |= .
Proof. proof induction structure . prove (P, s, ) |=
|= . direction proved analogously. base case atomic formulas
follows Prop. 3.3. inductive cases propositional connectives straightforward.
x, assume x free() (otherwise consider , corresponding case),
variable quantified (otherwise rename variables). Let bijection witnessing 0 equivalent w.r.t. s0 . u adom(s), consider
x
assignment ux . definition, (u) adom(s0 ), 0 (u)
well-defined. Note


x
x
0
free() = free() {x}; u (u) equivalent w.r.t. s0 . Moreover,

| ux (free())| |(free())| + 1, u may occur (free()). considerations apply 0 . Further, |var() \ free()| = |var() \ free()| 1, var() = var(),
(P 0 , s0 , 0 )

353

fiB ELARDINELLI , L OMUSCIO & PATRIZI

free() = free() {x}, x
/ free(). Thus, hypotheses (1) (2) remain satisfied
x
. Therefore, induction hypothesis,
replace , ux , 0 0 (u)


x
x
0
0
0
(P, s, u ) |= (P , , (u) ) |= . Since u adom(s) generic bijection,
result follows.
AX, assume contradiction (P, s, ) |= (P 0 , s0 , 0 ) 6|= . Then,
exists run r0 s.t. r0 (0) = s0 (P 0 , r0 (1), 0 ) 6|= . Since |var() \ free()| 0, Lemma 3.10,
exists run r s.t. r(0) = s, 0, r(i) r0 (i) 0 equivalent
w.r.t. r(i) r0 (i). Since r run s.t. r(0) = s, satisfies hypothesis (1). Moreover,
hypothesis necessarily satisfied t.e. runs r00 s.t. 0, r00 (0) = r(i) (otherwise,
t.e. run r(0) ; ; r(i) ; r00 (1) ; r00 (2) ; would satisfy hypothesis r);
considerations apply w.r.t hypothesis (2) t.e. runs r000 s.t. r000 (0) = r0 (i),
0. particular, hold = 1. Thus, inductively apply lemma, replacing
r(1), s0 r0 (1), (observe var() = var() free() = free()).
obtain (P, r(1), ) 6|= , thus (P, r(0), ) 6|= AX. contradiction.
EU , assume variables common occur free formulas
(otherwise rename quantified variables). Let r run s.t. r(0) = s, exists k 0
s.t. (P, r(k), ) |= , (P, r(j), ) |= 0 j < k. Lemma 3.10 exists run
r0 s.t. r0 (0) = s0 0, r0 (i) r(i) 0 equivalent w.r.t. r0 (i)
r(i). bijection witnessing 0 equivalent w.r.t. r0 (i)
r(i), define bijections i, = |adom(r(i))Con(free()) i, = |adom(r(i))Con(free()) .
Since free() free(), free() free(), seen i, i, witness
0 equivalent respectively w.r.t. r0 (i) r(i). argument used
AX case above, hypothesis (1) holds t.e. runs r00 s.t. r00 (0) = r(i),
0, hypothesis (2) holds t.e. runs r000 s.t. r000 (0) = r0 (i). observe
|(free())|, |(free())| |(free())|. Moreover, assumption common variables
, (var() \ free()) = (var() \ free()) ] (var() \ free()), thus |var() \ free()| =
|(var() \ free()| + |(var() \ free()|, hence |(var() \ free()|, |(var() \ free()|
|var() \ free()|. Therefore hypotheses (1) (2) hold also uniformly replaced either . Then, induction hypothesis applies i, replacing r(i), s0
r0 (i), either . Thus, i, (P, r(i), ) |= iff (P 0 , r0 (i), 0 ) |= ,
(P, r(i), ) |= iff (P 0 , r0 (i), 0 ) |= . Therefore, r0 run s.t. r0 (0) = s0 , (P 0 , r0 (k), 0 ) |= ,
every j, 0 j < k implies (P 0 , r0 (j), 0 ) |= , i.e., (P 0 , s0 , 0 ) |= EU .
AU , assume contradiction (P, s, ) |= (P 0 , s0 , 0 ) 6|= . Then,
exists run r0 s.t. r0 (0) = s0 every k 0, either (P 0 , r0 (k), 0 ) 6|= exists j
s.t. 0 j < k (P 0 , r0 (j), 0 ) 6|= . Lemma 3.10 exists run r s.t. r(0) = s,
0, r(i) r0 (i) 0 equivalent w.r.t. r(i) r0 (i). Similarly
case EU , shown 0 equivalent w.r.t. r(i) r0 (i),
0. Further, assuming w.l.o.g. variables common occur free
formulas, shown, case EU , induction hypothesis holds every
pair runs obtained suffixes r r0 , starting i-th state, every 0. Thus,
(P, r(i), ) |= iff (P 0 , r0 (i), 0 ) |= , (P, r(i), ) |= iff (P 0 , r0 (i), 0 ) |= . r
s.t. r(0) = every k 0, either (P, r(k), ) 6|= exists j s.t. 0 j < k
(P, r(j), ) 6|= , is, (P, s, ) 6|= AU . contradiction.
Ki , assume contradiction (P, s, ) |= (P 0 , s0 , 0 ) 6|= . Then,
exists s00 s.t. s0 s00 (P 0 , s00 , 0 ) 6|= . Lemma 3.10 exists s000 s.t. s000 s00 , s000 ,
354

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

0 equivalent w.r.t. s00 s000 . Thus, argument analogous used
case AX, apply induction hypothesis, obtaining (P, s000 , ) 6|= .
(P, s, ) 6|= Ki , contradiction.
Finally, C, assume contradiction (P, s, ) |= (P 0 , s0 , 0 ) 6|= . Then,
exists s00 s.t. s0 s00 (P 0 , s00 , 0 ) 6|= . Lemma 3.10 exists s000 s.t. s000
s00 , s000 , 0 equivalent w.r.t. s00 s000 . Thus, argument analogous
used case Ki , apply induction hypothesis, obtaining (P, s000 , ) 6|= .
(P, s, ) 6|= C, contradiction.
easily extend result model checking problem AC-MAS.
Theorem 3.12 Consider two bisimilar uniform AC-MAS P P 0 , FO-CTLK formula
.

1. t.e. runs r s.t. r(0) = s0 , k 0, |U 0 | |adom(r(k)) adom(r(k + 1))
Con| + |var()|,
2. t.e. runs r0 s.t. r0 (0) = s00 , k 0, |U | |adom(r0 (k)) adom(r0 (k + 1))
Con| + |var()|

P |= iff P 0 |= .
Proof. Equivalently, prove (P, s0 , ) 6|= , exists 0
s.t. (P 0 , s00 , 0 ) 6|= , viceversa. end, observe hypotheses (1) (2) imply, respectively, hypotheses (1) (2) Theorem 3.11. Further, notice that, cardinality considerations,
given assignment : V ar 7 U , exists assignment 0 : V ar 7 U 0 s.t. 0
equivalent w.r.t. s0 s00 . Thus, applying Theorem 3.11 exists
assignment s.t. (P, s0 , ) 6|= , exists assignment 0 s.t. (P 0 , s00 , 0 ) 6|= .
converse proved analogously, hypotheses symmetric.
result shows uniform AC-MAS principle verified model checking bisimilar one. Note applies infinite AC-MAS P, well. case results
enable us show verification question posed corresponding, possibly finite
P 0 long U 0 , defined above, sufficiently large P 0 bisimulate P. noteworthy class
infinite systems results prove particularly powerful bounded AC-MAS,
which, discussed next subsection, always admit finite abstraction.
3.3 Finite Abstractions
define notion finite abstraction AC-MAS, prove that, uniformity, abstractions bisimilar corresponding concrete model. particularly interested finite
abstractions; operate special class infinite models call bounded.
Definition 3.13 (Bounded AC-MAS) AC-MAS P b-bounded, b N, S,
|adom(s)| b.

355

fiB ELARDINELLI , L OMUSCIO & PATRIZI

AC-MAS b-bounded none reachable states contain b distinct elements.
Observe bounded AC-MAS may defined infinite domains. Furthermore, note bbounded AC-MAS may contain infinitely many states, bounded b. b-bounded systems
infinite-state general. Notice also value b constrains number distinct
individuals state, size state itself, intended amount memory required
accommodate individuals. Indeed, infinitely many elements domain U need
unbounded number bits represented (e.g., finite strings), so, even though state
guaranteed contain b distinct elements, nothing said large actual
space required elements is. Conversely, memory-bounded AC-MAS finite-state (hence
b-bounded, b).
Since b-bounded AC-MAS general memory-unbounded, cannot verified trivially generating checking possibly infinite executions. However, show later
b-bounded uniform infinite-state AC-MAS admits finite-state abstraction
used verify it.
introduce abstractions modular manner first introducing set abstract agents
concrete AC-MAS.
Definition 3.14 (Abstract agent) Let = hD, Act, P ri agent defined interpretation
domain U . Given interpretation domain U 0 , abstract agent U 0 agent A0 =
hD0 , Act0 , P r0 that:
1. D0 = D;
2. Act0 = Act;
3. (~u0 ) P r0 (l0 ), l0 D0 (U 0 ), iff exist l D(U ) (~u) P r(l) s.t. l0 ' l,
witness , ~u0 = 0 (~u), bijection 0 extending ~u.
Given set Ag agents defined U , Ag 0 denotes set corresponding abstractions U 0
agents Ag.
remark abstract agent A0 agent line Definition 2.6. Notice protocol A0 defined basis corresponding concrete agent requires existence
bijection elements corresponding local states action parameters. Thus,
order ground action counterpart A0 , last requirement Definition 3.14
constrains U 0 contain sufficient number distinct values. become apparent later,
size U 0 determines closely abstract system simulate concrete counterpart. Notice
also that, general, agent may abstraction U , instance data may
impact agents protocol.
Next, combine notion uniformity boundedness. aim identify
conditions verification infinite AC-MAS reduced verification
finite one. main result given Corollary 3.19 guarantees that, context
bounded AC-MAS, uniformity sufficient condition bisimilar finite abstractions
satisfaction-preserving.
following assume AC-MAS P adom(s0 ) Con.
case, Con extended include (finitely many) elements adom(s0 ). start
formalising notion abstraction.
356

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

Definition 3.15 (Abstract AC-MAS) Let P = hAg, s0 , AC-MAS Ag 0 set abstract agents obtained Definition 3.14 interpretation domain U 0 . AC-MAS
P 0 = hAg 0 , s00 , 0 said abstraction P iff:
s00 = s0 ;
t0 0 (s0 ,
~ (~u0 )) iff exist s,
~ (~u) Act(U ) ' s0 t0 ,
0
0
witness , (s,
~ (~u)), ~u = (~u) bijection 0 extending ~u.
checked P 0 , defined above, indeed AC-MAS satisfies relevant
conditions protocols transitions Definition 2.7. Indeed, t0 0 (s0 ,
~ (~u0 )),
exist s, S,
~ (~u) (s,
~ (~u)), ' s0 t0 witness , ~u = 0 (~u0 )
0
bijection extending . means (~ui ) P ri (li ) n. definition P ri0
(~u0i ) P ri0 (li0 ) n.
definition requires abstractions initial states isomorphic concrete counterparts; specifically equal adom(s0 ) Con. Moreover, second constraint entails
transition concrete model exists transition, renaming
involved values, exists abstraction. So, example, copy action concrete model
corresponding copy action abstract model. Crucially, condition requires domain
U 0 contains enough elements bisimulate concrete states action effects. made
precise Lemma 3.17.
Obviously, U 0 finitely many elements, 0 finitely many states. Observe also
varying U 0 obtain different abstractions. Finally, notice AC-MAS necessarily
abstraction itself. issue addressed Lemma 3.16.
Next, investigate relationship AC-MAS abstractions. first useful
result states every finite abstraction uniform, independently properties AC-MAS
abstract.
Lemma 3.16 Every abstraction P 0 AC-MAS P uniform. Moreover, P uniform
U 0 = U , P 0 = P.
Proof. Consider s, t, s0 0 , t0 D(U 0 ),
~ (~u) Act0 (U 0 ) s.t. 0 (s,
~ (~u))
0
0
' , witness . need show P 0 admits transition s0 t0 . Since
P 0 abstraction P, given definition 0 , exist s00 , t00
~ (~u00 ) Act(U )
00
00
00
00
00
0
00
s.t. (s ,
~ (~u )), ' t, witness , ~u = (~u ), constantpreserving bijection 0 extending ~u00 . Consider ~u0 U 0|~u| ~u0 = 0 (~u),
constant-preserving bijection 0 extending ~u. Obviously, composition 0 0 constantpreserving bijection ~u0 = 0 (0 (~u00 )). Moreover, restricted witness
s00 t00 ' s0 t0 . then, since P 0 abstraction P, implies t0 0 (s0 ,
~ (~u0 )). Thus,
P 0 uniform.
Moreover, prove P abstraction every time P uniform U 0 = U ,
notice transition (s,
~ (~u)) P, also P 0 definition
0
abstraction. Also, transition 0 (s0 ,
~ (~u0 )) appears P 0 , exist s,
0
0
~ ) s.t. ' witness , (s,

~ (~u) Act(U
~ (~u)), ~u0 = 0 (~u)
constant-preserving bijection 0 extending ~u. Finally, since P uniform case
transition t0 0 (s0 , (~u0 )) P well.

357

fiB ELARDINELLI , L OMUSCIO & PATRIZI

lemma provides sufficient conditions AC-MAS abstraction itself,
namely uniform interpretation domain.
second result guarantees every uniform, b-bounded AC-MAS bisimilar
abstractions, provided areP
built sufficiently large interpretation domain.
0
following, take NAg = NAg =
x|}, i.e., NAg sum
x)Acti {|~
Ai Ag max(~
maximum number parameters contained action types agent Ag.
Lemma 3.17 Consider uniform, b-bounded AC-MAS P infinite interpretation domain U ,
interpretation domain U 0 Con U 0 . |U 0 | 2b + |Con| + NAg ,
abstraction P 0 P U 0 bisimilar P.
Proof. Let B = {hs, s0 0 | ' s0 }. prove B bisimulation
hs0 , s00 B. start proving B simulation relation. end, observe since
s0 = s00 , s0 ' s00 , hs0 , s00 B. Next, consider hs, s0 B, thus ' s0 . Assume
t, S. Then,
~ (~u) Act(U ) (s,
~ (~u)). Moreover,
Pmust exist
since |U 0 | 2b + |Con| + NAg ,S Ai Ag |~ui | NAg , |adom(s) adom(t)| 2b, witness
' s0 extended Ai Ag ~ui bijection 0 . let t0 = 0 (t). way 0
defined, seen ' s0 t0 . Further, since P 0 abstraction P,
t0 0 (s0 ,
~ (~u0 )) ~u0 = 0 (~u), is, s0 t0 P 0 . Therefore, exists t0 0
s0 t0 , ' s0 t0 , ht, t0 B. regards epistemic relation, assume
{1, . . . , n} S. definition , li (s) = li (t). Since |U 0 | 2b + |Con|,
witness ' s0 extended witness 0 ' s0 t0 , t0 = 0 (t).
Obviously, li (s0 ) = li (t0 ). Thus, prove s0 t0 , left show t0 0 , i.e., t0
reachable P 0 s00 = s0 . end, observe since S, exists purely temporal
run r r(0) = s0 r(k) = t, k 0. Thus, exist
~ 1 (~u1 ) . . . ,
~ k (~uk )
r(j + 1) (r(j),
~ j+1 (~uj+1 )), 0 j < k. Since |U 0 | 2b + |Con|, define,
0 j < k, function j witness r(j) r(j + 1) ' j (r(j)) j (r(j + 1)). particular,
done starting j = k 1, defining k1 k1 (r(k)) = k1 (t) = t0 ,
proceeding backward j = 0, that, 0 j < k, j (r(j + 1)) = j+1 (r(j + 1)).
Observe since adom(s0 ) Con, necessarily i0 (r(0)) = i0 (s0 ) = s0 = s00 . Moreover,
|U 0 | 2b + |Con| + NAg , j extended bijection 0j , elements occurring
~uj+1 . Thus, given P 0 abstraction P, 0 j < k, 0j (r(j + 1))
(0j (r(j)),
~ (0j (~uj+1 ))). Hence, sequence 00 (r(0)) 0k1 (r(k)) run P 0 , and,
since t0 = 0k1 (r(k)), t0 reachable P 0 . Therefore s0 t0 . Further, since ' t0 , definition
B, case ht, t0 B, hence B simulation.
prove B 1 simulation, given hs, s0 B (thus ' s0 ), assume s0 t0 ,
t0 0 . Obviously, exists
~ (~u0 ) Act(U 0 ) t0 0 (s0 ,
~ (~u0 )). P 0
00
00
00
00
abstraction P, exist ,
~ (~u ) Act(U ) t00 ' s0 t0 ,
witness , t00 (s00 , (~u00 )), ~u00 = 0 (~u0 ), bijection 0 extending ~u0 . Observe
s0 ' s00 , thus, transitivity ' ' s00 . fact exists
easily follows uniformity P. Thus, since t0 ' t, ht, t0 B.
epistemic relation, assume s0 t0 t0 0 0 < n. Let witness s0 ' s,
let 0 extension witness s0 t0 ' t. = 0 (t0 ), seen
li (s) = li (t). Observe t0 0 . Using argument analogous one above, exploiting
fact P uniform, P 0 certainly b-bounded, |U | > 2b + |Con| + NAg U
infinite, show constructing run r P r(k) = t, k 0.
358

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

t. Further, since t0 ' t, ht, t0 B. Therefore, B 1 simulation. So, P
P 0 bisimilar.
result allows us prove main abstraction theorem.
Theorem 3.18 Consider b-bounded uniform AC-MAS P infinite interpretation domain U , FO-CTLK formula , interpretation domain U 0 Con U 0 .
|U 0 | 2b + |Con| + max{|vars()|, NAg }, abstraction P 0 P U 0 ,
that:
P |= iff P 0 |= .
Proof. Lemma 3.16, P 0 uniform. Thus, hypothesis cardinalities U U 0 ,
Lemma 3.17 applies, P P 0 bisimilar. Obviously, also P 0 b-bounded. Thus, since P
P 0 b-bounded, cardinality hypothesis U U 0 , Theorem 3.12 applies.
particular, notice every temporal-epistemic run r s.t. r(0) = s0 , k 0,
|U 0 | |adom(r(k))adom(r(k+1))Con|+|var()|, |adom(r(k))| b b-boundedness.
Therefore, P |= iff P 0 |= .
follows using sufficiently large number abstract values U 0 , reduce
verification infinite, bounded, uniform AC-MAS verification finite one.
Corollary 3.19 Given b-bounded uniform AC-MAS P infinite interpretation domain
U , FO-CTLK formula , exists AC-MAS P 0 finite interpretation domain U 0
P |= iff P 0 |= .
also noted U 0 simply taken finite subset U (including Con)
satisfying cardinality requirement above. so, finite abstraction P 0 defined
simply restriction P U 0 . Thus, every infinite, b-bounded uniform AC-MAS
bisimilar finite subsystem, satisfies formulas.
Note concerned actual construction finite abstraction. intend construct directly artifact-centric program, Section 4.
that, explore complexity model checking problem.
3.4 Complexity Model Checking Finite AC-MAS FO-CTLK Specifications
analyse complexity model checking problem finite AC-MAS respect
FO-CTLK specifications. input problem consists AC-MAS P finite domain U
FO-CTLK formula ; output assignment (P, s0 , ) |= , whenever
property satisfied. Hereafter follow standard literature basic notions definitions (Grohe,
2001).
encode AC-MAS P use tuple EP = hU, D, s0 , i, U (finite) interpretation domain, global database schema, s0 initial state, = {~ 1 , . . . , ~ }
set FO-formulas, capturing transitions associated ground joint action
~ . Since
U finite, set ground actions, thus . ~ FO-formula alphabet
j
0 ,
DAg DAg
Ag = {Pi /qi | Pi /qi D, j n} set containing one distinct relation
~)
symbol Pij , agent j n relation symbol Pi D. take s0 (s,

359

fiB ELARDINELLI , L OMUSCIO & PATRIZI

0 |= , s, s0 D(U ), every P j n, l (P ) = (P j )
iff DAg DAg

j

Ag

~

j
0
0
lj (Pi ) = DAg (Pi ).
example, = {P } (thus DAg = {P j | j n}) action type
V
0
parameters, consider formula ~ = nj=0 xP j (x) P j (x), intuitively captures
transitions successor state predicate P contains elements U
current state P .
proved every transition relation represented discussed above, that,
.
given EP , size ||P|| = |S| + | | encoded AC-MAS P ||P|| |Act| |U |pmax
23`qmax , where: pmax largest number parameters action type Act, `
number relation symbols D, qmax largest arity symbols. corresponds

P
.
doubly exponential bound ||P|| w.r.t. ||EP || = |U | + ||D|| + | |, ||D|| = Pk qk ,
||E

||4

qk arity Pk . Specifically, ||P|| 232 P .
carry complexity analysis basis input above; clearly results
apply equally compact inputs AC programs presented Section 4.
consider combined complexity input, is, ||EP || + ||||. say
combined complexity model checking finite AC-MAS FO-CTLK specifications
EXPSPACE-complete problem EXPSPACE, i.e., polynomial p(x)
algorithm solving problem space bounded 2p(||EP ||+||||) , problem EXPSPACEhard, i.e., every EXPSPACE problem reduced model checking finite AC-MAS
FO-CTLK specifications.
Theorem 3.20 model checking problem finite AC-MAS succinctly presented
FO-CTLK specifications EXPSPACE-complete.
Proof. show problem EXPSPACE, recall ||P|| doubly exponential
w.r.t. size input, thus |S|. describe algorithm works NEXPSPACE;
combines algorithm model checking first-order fragment FO-CTLK
temporal-epistemic fragment. Since NEXPSPACE = EXPSPACE, result follows. Given ACMAS P FO-CTLK formula , guess assignment check whether (P, s0 , ) |= .
done induction according structure . atomic, check done
polynomial time w.r.t. size state evaluated on, is, exponential time w.r.t. ||EP ||.
form x, apply algorithm model checking first-order (non-modal)
logic, works PSPACE. Finally, outmost operator either temporal epistemic
modality, extend automata-based algorithm model check propositional CTL
(Kupferman, Vardi, & Wolper, 2000; Lomuscio & Raimondi, 2006), works logarithmic
space |S|. However, remarked |S| generally doubly exponential ||EP ||. Thus,
step performed space singly exponential ||EP ||. steps performed
time polynomial size . result, total combined complexity model checking
finite AC-MAS NEXPSPACE = EXPSPACE.
prove problem EXPSPACE-hard show reduction problem
EXPSPACE. assume standard definitions Turing machines reductions (Papadimitriou,
1994). problem EXPSPACE, exists deterministic Turing machine
TA = hQ, , q0 , F, i, Q finite set states, machine alphabet, q0 Q
initial state, F set accepting states, transition function, solves using
space 2p(|in|) given input in, polynomial function p. standard, assume
360

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

relation (QQD), = {L, R}, hq, c, q 0 , c0 , di representing transition
state q state q 0 , characters c c0 read written respectively , head direction
((L)eft (R)ight). Without loss generality, assume TA uses righthand half
tape.
TA in, build encoding EP = hD, U, s0 , AC-MAS P induced
single (environment) agent AE = hDE , ActE , P defined U = Q {0, 1}, where: (i)
DE = {P/p(|in|) + 1, Q/1, H/p(|in|), F/1}; (ii) ActE singleton {E }, E parameterfree; (iii) E P (lE ) every lE D(U ). Intuitively, states P correspond configurations TA , mimics . define EP , let = DE . intended meaning
predicates follows: first p(|in|) elements P -tuple encode (in binaries)
position non-blank cell, (p(|in|) + 1)-th element contains symbol appearing
cell; Q contains current state q TA ; H contains position cell head
currently on; F contains final states TA , i.e., F = F. initial state s0 represents
initial configuration TA , is, = in0 in` : s(Q) = {q0 }; s(H) = {h0, . . . , 0i};
s(P ) = {hBIN(i), ini | {0, . . . , `}}, BIN(i) stands binary encoding p(|in|)
bits integer i. Observe p(|in|) bits enough index (at most) 2p(|in|) cells used
TA .
transition relation, define = {E }, (we avoid sub- superscripts
predicate symbols, i.e., = DAg ambiguity arise one agent):
E =

_

(xF (x) F 0 (x))

(1)

hq,c,q 0 ,c0 ,di

Q(q) (xQ(x) x = q) Q0 (q 0 ) (xQ0 (x) x = q 0 )

(2)

~
p(H(~
p) (xH(x) x = p~) (P (~
p, c) (c = 2 xP (~
p, x))))
(3)
0
0
p~0 (d = R SUCC(~
p, p~0 )) (d = L SUCC(p~0 , p~)) H (p~0 ) (xH (x) x = p~0 ) (4)
(P 0 (~
p, c0 ) (c0 6= 2)) (xP 0 (~
p, x) x = c0 )
0

0

(5)
0

(~x, y(P (~x, y) (~x 6= p~) P (~x, y)) (~x, yP (~x, y) (P (~x, y) (~x = p~ = c )))) (6)

Vp(|in|)
symbol 2 represents content blank cells, SUCC(~x, x~0 ) = i=1 (x0i = 0x0i =
Vi1
V
0
1) (x0i = 1 ((x0i = 0 i1
j=1 xj = 1) (xi = 1 j=1 xj = 1))) formula capturing
x~0 successor ~x, ~x x~0 interpreted p(|in|)-bit binary encodings integers (observe
{0, 1} U ). formula obviously written polynomial time w.r.t. p(|in|), well
EP , particular s0 E . Formula E obtained disjunction subformulas,
referring transition . subformula, i.e., transition hq, c, q 0 , c0 , di: line 1 expresses
F , encodes final states machine, change along transition (this
formula could moved big disjunction); line 2 encodes machine
exactly one state, q 0 , transition takes place; line 3 expresses symbol read
head c (possibly blank); line 4 captures head moves direction d; line 5 states
head writes symbol c cell, moving; finally, line 6 states content tape
change, except cell head on.
obtained transition function (s, E ) = s0 iff, (q, c) = (q 0 , c0 , d) TA ,
that: s0 (P ) obtained s(P ) overwriting c0 (if blank) symbol position
(p(|in|) + 1) tuple s(P ) beginning p(|in|)-tuple s(H) (that is, c definition
E ); updating s(H) according d, increasing decreasing value contains;
361

fiB ELARDINELLI , L OMUSCIO & PATRIZI

setting s0 (Q) = {q 0 }. predicate F change. Observe cells occurring P
interpreted containing 2 2 written cell, cell simply removed
P .
checked that, starting = s0 , iteratively generating successor state s0
according , i.e., s0 s.t. s0 |= E , one obtains (single) P-run representation
computation TA in, pair consecutive P-states corresponds computation
step. particular, state, Q contains current state TA . clear =
EF (xQ(x) F (x)) holds P iff TA accepts in. Thus, model checking P, check
whether TA accepts in. completes proof EXPSPACE-hardness.
Note result given terms data structures model, i.e., U D,
state space itself. accounts high complexity model checking AC-MAS,
state space doubly exponential size data. analysing refined bound
size ||P|| (||P|| |Act||U |pmax 23`qmax ), seen double exponential essentially
due number parameters action types, number relation symbols occurring D,
respective arities. Thus, fixed database schema set action types, resulting
space complexity reduced singly exponential.
EXPSPACE-hardness indicates intractability, note expected given
dealing quantified structures principle prone high complexity. Recall
also Section 3.3 size interpretation domain U 0 abstraction P 0 linear
bound b, number constants Con, size , NAg . Hence, model checking
bounded uniform AC-MAS EXPSPACE-complete respect elements, whose size
generally small. Thus, believe several cases practical interest model checking
AC-MAS may entirely feasible.

4. Artifact-Centric Programs
far developed formalism used specify reason temporalepistemic properties models representing artifact-centric systems. identified notable
class models admit finite abstractions. remarked Introduction, however, artifact
systems typically implemented declarative languages GSM (Hull et al., 2011).
therefore interest investigate verification problem, Kripke semantics
AC-MAS, actual programs. discussed, GSM mainstream declarative
language artifact-centric environments, alternative declarative approaches exist. follows
sake generality ground discussion wide class declarative languages
define notion artifact-centric program. Intuitively, artifact-centric program (or AC
program) declarative description whole multi-agent system, i.e., set services,
interact artifact system (see discussion Introduction). Since artifact systems
also typically implemented declaratively (Heath et al., 2013), AC programs used encode
artifact system agents system. also enables us import
formalism previously discussed features views windows typical GSM
languages.
rest section organised follows. begin Subsection 4.1 defining AC
programs giving semantics terms AC-MAS. show AC-MAS
results AC program uniform. long generated AC-MAS bounded, using
results Section 3.3, deduce AC program admits AC-MAS finite model.
362

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

context important give constructive procedures generation finite abstraction;
provide procedure here. enables us state that, assumptions identify, AC
programs admit decidable verification means model checking finite model. Section 4.2
ground exemplify constructions Order-to-Cash Scenario introduced
Subsection 2.4.
4.1 Verifying Artifact-Centric Programs
start defining abstract syntax AC programs.
Definition 4.1 (AC Programs) artifact-centric program (or AC program) tuple ACP =
hD, U, , i, where:
programs database schema;
U programs interpretation domain;
= {0 , . . . , n } set agent programs = hDi , li0 , Acti i, where:
Di agent database schema;
li0 Di (U ) agent initial state (as database instance);
Acti set local actions (~x), action name ~x action
parameter names; without loss generality, assume two action types use
parameter names;
local action (~x) associated precondition (~x) (~y ), i.e., FO-formula
Di , ~y ~x free variables.
= {~ (~x) (~z) |
~ (~x) = h1 (~x1 ), . . . , n (~xn )i Act1 Actn , ~x = h~x1 , . . . , ~xn i, ~z
~x} represents AC programs transitions expressed set postconditions, i.e., FOformulas action parameters free variables. formulas defined
j
0 ,
alphabet DAg DAg
Ag = {Pi /qi | Pi /qi D, j n} set containing one
distinct relation symbol Pij , agent j n relation symbol Pi D.
AC programs defined modularly giving agents programs, including action
preconditions postconditions. Notice preconditions use relation symbols local
database only, programs transitions refer local relations agents unconstrained way. precisely, postconditions global, i.e., associated global
actions, rather local ones. Indeed, formula ~ (~x) (~z) describes effects execution
global action
~ (~z) (under particular assignment parameters) agent executes
(~zi ). reported below, accounts intuition choosing next action, agent
rely information locally stored, actions, result mutual interactions, may
change local state agent, i.e., affect global state system. Obviously,
prevent possibility specifying actions affect local states only. line
AC-MAS semantics literature interpreted systems.
Given tuple ~x variables tuplue ~u elements U |~x| = |~u|, (~x) = ~u
denote assignment binds i-th component
~u i-th component

~x. joint
action
~ (~x) given above, let con(~
) = con(i ) con(), var(~
) = var(i )
363

fiB ELARDINELLI , L OMUSCIO & PATRIZI

var(), free(~
) = ~x. execution
~ (~x) ground parameters ~u U |~x| ground
action
~ (~u), ~v (resp. w)
~ obtained replacing yi (resp. zi ) value occurring
~u position yi (resp. zi ) ~x. replacements make (~v ) (w)
~
ground, is, first-order sentences. Finally,

define

set
Con


constants
mentioned
ACP


AC program ACP , i.e., ConACP = ni=1 adom(li0 ) ~ Act con(~
).
semantics AC program given terms AC-MAS induced agents
program implicitly defines. Formally, captured following definition.
Definition 4.2 (Induced Agents) Given AC program ACP = hD, U, , i, agent =
hDi , Acti , P ri induced ACP interpretation domain U iff agent program
= hDi , li0 , Acti that:
every li Di (U ) ground action (~u) (~x) Acti , case
(~u) P ri (li ) iff (li , ) |= (~x) (~y ), (~x) = ~u (recall ~y ~x).
Note induced agents agents formalised Definition 2.6. Agents defined
composed give AC-MAS associated AC program.
Definition 4.3 (Induced AC-MAS) Given AC program ACP = hD, U, , set Ag =
{A0 , . . . , } agents induced ACP , AC-MAS induced ACP tuple PACP =
hAg, s0 , i, where:
s0 = hl00 , . . . , ln0 initial global state;
global transition function defined following condition: s0 (s,
~ (~u)),
0
0
0
= hl0 , . . . , ln i, = hl0 , . . . , ln i,
~ (~u) = h1 (~u0 ), . . . , n (~un )i, ~u = h~u0 , . . . , ~un i, iff
every {0, . . . , n},
(li , ) |= (~xi ) (~yi ) (~xi ) = ~ui ;
adom(s0 ) adom(s) ~u con(~ (~x) );
0 , ) |=
0
(DAg DAg
z ), assignment (~x) = ~u, DAg , DAg

~ (~
x) (~
j
DAg -instances that, every Pi j n, DAg (Pi ) = lj (Pi )
0 (P j ) = l0 (P ).
DAg

j


Given AC program ACP , induced AC-MAS represents programs execution tree
encodes data system. Intuitively, obtained iteratively executing state,
starting initial one, possible ground actions. Observe actions performed
enabled respective protocols transitions introduce bounded number
new elements active domain, i.e., bound action parameters. follows
AC programs parametric respect interpretation domain, i.e., replacing
interpretation domain obtain different AC-MAS.
assume every program induces AC-MAS whose transition relation serial, i.e.,
states always successors. basic requirement easily fulfilled, instance,
assuming agent skip action trivially true precondition
agents execute skip, global state system remains unchanged. next Subsection
present example one program.
significant feature AC programs induce uniform AC-MAS.
364

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

Lemma 4.4 Every AC-MAS P induced AC program ACP uniform.
Proof. Since definition adom(s0 ) ConACP , Prop. 3.7 sufficient consider
temporal transition relation . Consider s, s0 , s00 s000 L0 Ln
s0 ' s00 s000 witness . particular, every Aj Ag, lj00 ' lj lj000 ' lj0 , namely,
lj00 = (lj ) lj000 = (lj0 ). Also, assume exists
~ (~u) = h1 (~u1 ), . . . , n (~un )i Act(U )
s0 (s,
~ (~u)). First all, j (~uj ) P rj (lj ) implies (lj , j ) |= j (~xj ) (~yj )
j (~xj ) = ~uj . Since lj00 ' lj , Prop. 3.3 (lj00 , j0 ) |= j (~xj ) (~yj ) j0 (~xi ) = 0 (~ui )
0 extending ~ui . Thus, j (0 (~uj )) P rj (lj00 ) every j Ag. Further, assume
0 , ) |=
0 -instances obtained
(DAg DAg
z ), DAg , DAg
Ag

~ (~
x) (~
00 , 000 00 (P j ) = l00 (P ) = (l (P )),
(~x) = ~u. Consider DAg -instances DAg

j

j
Ag
Ag

000 (P j ) = l000 (P ) = (l0 (P )). Since s0 ' s00 s000 , obtain
0 ' 00 000
DAg




Ag
j
j
Ag
Ag
Ag

00 000 , 0 ) |=
0 (~
0 (~
witness . particular, (DAg
(~
z
),


x
)
=

u
)


0

~
(~
x
)
Ag
000
00
0
extending ~u. Finally, easily checked adom(s ) adom(s ) (~u) con(~ (~x) ).
result, s000 (s00 ,
~ (0 (~u))), i.e., P uniform.
define means AC program satisfy specification, referring
induced AC-MAS.
Definition 4.5 Given AC program ACP , FO-CTLK formula , assignment , say
ACP satisfies , written (ACP, ) |= , iff (PACP , s0 , ) |= .
Thus, model checking problem AC program specification defined terms
model checking problem corresponding AC-MAS PACP .
following result allows us reduce verification AC program infinite
interpretation domain U1 , induces b-bounded AC-MAS,
Pverification AC program
finite U2 . show constructed, let NACP = i{1,...,n} max(~x)i {|~x|}
maximum number different parameters occur joint action ACP .
Lemma 4.6 Consider AC program ACP1 = hD, U1 , operating infinite interpretation
domain U1 assume induced AC-MAS PACP1 = hAg1 , s10 , 1 b-bounded. Consider
finite interpretation domain U2 ConACP1 U2 |U2 | 2b + |ConACP1 | + NACP1
AC program ACP2 = hD, U2 , i. Then, AC-MAS PACP2 = hAg2 , s20 , 2 induced
ACP2 finite abstraction PACP1 .
Proof. Let Ag1 Ag2 set agents induced respectively ACP1 ACP2 , according
Def. 4.2. Firstly, prove set Ag1 Ag2 agents satisfy Def. 3.14, Ag = Ag1
Ag 0 = Ag2 . end, observe ACP1 ACP2 differ U , Def. 4.2,
= D0 , Act0 = Act. Thus, requirement 3 Def. 3.14 needs checked. this, fix
{1, . . . , n} assume (~u) P ri (li ). Def. 4.2, (li , ) |= (~xi ) (~yi )
(~xi ) = ~ui . assumption |U2 |, since con() ConACP1 U2 , |~u| NACP1 ,
|adom(li )| b, define injective function : adom(li ) ~u ConACP1 7 U2
identity ConACP1 . Thus, li0 = (li ), easily extract witness li ' li0 .
Moreover, seen (y) = ~v 0 (y) = ~v 0 = (~v ) equivalent . Then,
applying Prop. 3.3 li li0 , conclude (li0 , 0 ) |= (~xi ) (~yi ). Hence, Def. 4.2,
(~u0 ) P ri0 (li0 ) ~u0 = (~u). So, shown right-to-left part requirement 3.
left-to-right part shown similarly simply since U1 infinite.
365

fiB ELARDINELLI , L OMUSCIO & PATRIZI

Thus, proven Ag = Ag1 Ag 0 = Ag2 obtained Def. 3.14. Hence,
assumption Ag Ag 0 Def. 3.15 fulfilled. show next also remaining requirements Def. 3.15 satisfied. Obviously, since ACP1 ACP2 , Def. 4.3,
s10 = s20 , initial states PACP1 PACP2 same. remains show
requirements 1 2 satisfied. prove right-to-left part. end, take two states
0 , . . . , l0 joint action
s1 = hl10 , . . . , l1n i, s01 = hl10
~ (~u) = h0 (~u0 ), . . . , n (~un )i
1
1n
0
Act(U1 ) s1 1 (s1 ,
~ (~u)). Consider s1 s01 . assumptions U2 , exists
injective function : adom(s1 )adom(s01 )~u ConACP1 7 U2 identity ConACP1 (re0 ), . . . , (l0 )i
call |adom(s1 )|, |adom(s01 )| b). Then, s2 = h(l10 ), . . . , (l1n )i, s02 = h(l10
1n
0
0
S2 , extract witness s1 s1 ' s2 s2 . Moreover, seen every
(~xi ) ~ (~x) , assignments (~x) = ~u 0 (~x) = ~u0 = (~u) equivalent respect
s1 s01 s2 s02 . Now, consider Def. 4.3 recall PACP1 PACP2 AC-MAS induced ACP1 ACP2 , respectively. applying Prop. 3.3, that, {0, . . . , n},
0
(i) ((l1i ), 0 ) |= (~xi ) (~yi ) iff (l1i , ) |= (~xi ) (~yi ); (ii) (DAg2 DAg
, 0 ) |= ~ (~x) (~zi ) iff
2
0
(DAg1 DAg1 , ) |= ~ (~x) (~zi ), DAgi obtained si detailed Def. 4.3; (iii)
adom(s01 ) adom(s1 ) ~u con(~ (~x) ) iff adom(s02 ) adom(s2 ) (~u) con(~ (~x) ) defi~ ((~u))). proved right-to-left part
nition . then, case s02 2 (s2 ,
second requirement Def. 3.15. direction follows similarly. Therefore, PACP2
abstraction PACP1 .
Intuitively, Lemma 4.6 shows following diagram commutes, [U1 /U2 ] stands
replacement U1 U2 definition ACP1 . Observe since U2 finite, one
actually apply Def. 4.3 obtain PACP2 ; particular transition function 2 computed.
Instead, PACP1 , particular 1 , cannot directly computed ACP1 applying Def. 4.3,
U1 infinite.
ACP1


Def. 4.3

[U1 /U2 ]

ACP2

/ PACP
1


Def. 4.3

Def. 3.15

/ PACP
2

following result, direct consequence Lemma 3.17 Lemma 4.6, key conclusion
section.
Theorem 4.7 Consider FO-CTLK formula , AC program ACP1 operating infinite interpretation domain U1 assume induced AC-MAS PACP1 b-bounded. Consider finite interpretation domain U2 CACP1 U2 |U2 | 2b + |CACP1 | + max{NACP1 , |var()|},
AC program ACP2 = hD, U2 , i. that:
ACP1 |= iff ACP2 |= .
Proof. Lemma 4.6 PACP2 finite abstraction PACP1 . Moreover, |U2 | 2b +
|ConACP1 | + max{NACP1 , |var()|} implies |U2 | 2b + |ConACP1 | + |var()|. Hence,
apply Lemma 3.17 result follows.
results shows generated AC-MAS model bounded, AC program
verified model checking finite abstraction, i.e., bisimilar AC-MAS defined finite
366

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

interpretation domain. Note procedure constructive: given AC program ACP1 =
hD, U1 , infinite domain U1 FO-CTLK formula , check whether ACP1 satisfies
specification , first consider finite abstraction ACP2 = hD, U2 , defined finite
domain U2 satisfying cardinality requirement Theorem 4.7. Since U2 finite, induced
AC-MAS PACP2 also finite; hence apply standard model checking techniques verify
whether PACP2 satisfies . Finally, definition satisfaction AC programs Theorem 4.7,
transfer result obtained decide model checking problem original infinite
AC program ACP1 specification .
Also observe finite abstraction considered abstract interpretation domain
U2 depends number distinct variables specification contains. Thus, principle,
check program different specification 0 , one construct new
abstraction PACP20 using different interpretation domain U20 , check 0 it. However,
seen number distinct variables 0 exceed , abstraction
PACP2 , used check , re-used 0 . Formally, let FO-CTLKk set FO-CTLK
formulas containing k distinct variables. following corollary Theorem 4.7.
Corollary 4.8 |U2 | 2b + |ConACP1 | + max{NACP1 , k}, then, every FO-CTLKk formula
, ACP1 |= iff ACP2 |= .
result holds particular k = NACP ; thus FO-CTLKNACP formulas,
abstraction procedure specification-independent.
Theorem 4.7 requires induced AC-MAS bounded, may seem difficult condition
check priori. Note however AC programs declarative. therefore straightforward
give postconditions enforce transition generate states violating boundedness
requirement. scenario next Subsection exemplify this.
4.2 Verifying Order-to-Cash Scenario
Section 2.4 introduced order-to-cash scenario (Hull et al., 2011), business process modelled artifact-centric system. show formalised within framework
AC programs. sake simplicity assumed three agents scenario: one
customer c, one manufacturer one supplier s. Further, database schema Di agent
{c, m, s} given as:
Customer c: Dc = {Products(pcode, budget), PO(id , pcode, offer , status)};
Manufacturer m: Dm = {PO(id , pcode, offer , status), MO(id , pcode, price, status)};
Supplier s: Ds = {Materials(mcode, cost), MO(id , pcode, price, status)}.
Also, assumed initial state non-empty relations Products
Materials. Hence, artifact-centric program ACPotc corresponding order-to-cash scenario
given formally follows:
Definition 4.9 (ACPotc ) artifact-centric program ACPotc tuple hDotc , Uotc , otc , otc i,
where:
programs database schema Dotc interpretation domain Uotc defined
Sec. 2.4, i.e., Dotc = Dc Dm Ds = {PO/4, MO/4, Products/2, Materials/2}
Uotc set alphanumeric strings.
367

fiB ELARDINELLI , L OMUSCIO & PATRIZI

createPO(id,pcode)

= b.Products(b, pcode)
p, o, s.P O(id, p, o, s)

doneMO(id)

= pc, p.M O(id, pc, p, preparation)

acceptMO(id)

= pc, p.M O(id, pc, p, submitted)

requires id fresh identifier
POs, newly created PO refer
existing product
requires id refer existing MO
currently preparation
requires id refer existing
MO submitted

Table 1: Preconditions actions createPO(id , pcode), doneMO(id ), acceptMO(id )
= {c , , } set agent specifications customer c, manufacturer
supplier s. Specifically, {c, m, s}, = hDi , li0 , Acti , that:
Di agent database schema detailed above, i.e., Dc =
{Products/2, PO/4}, Dm = {PO/4, MO/4}, Ds = {MO/4, Materials/2}.
lc0 , lm0 , ls0 database instances Dc (Uotc ), Dm (Uotc ), Ds (Uotc ) respectively s.t. lc0 (Products) ls0 (Materials) non-empty, i.e., contain background information, lc0 (PO), lm0 (PO), lm0 (MO) ls0 (MO) empty.
sets actions given
Actc = {createPO(id , pcode), submitPO(id ), pay(id ), deletePO(id ), skip}.
Actm = {createMO(id , price), doneMO(id ), shipPO(id ), deleteMO(id ), skip};
Acts = {acceptMO(id ), rejectMO(id ), shipMO(id ), skip}.
action (~x) associated precondition (~x) . preconditions
actions createPO(id , pcode), doneMO(id ), acceptMO(id ) reported Table 1.
remaining preconditions omitted brevity.
= {~ (~x) | (~x)) Actc Actm Acts },
DAg = {P roductsc , P Oc , P Om , Om , aterialss , Os }.
Table 2 illustrates postcondition joint action

~ (id, pc, m1 , m2 ) = hcreatePO(id, pc), doneMO(m1 ), acceptMO(m2 )i.
others omitted.
postcondition Table 2 variables (from V ) constants (from U ) distinguished fonts
v c, respectively. first two lines impose interpretation relations Products
Materials, occurring local database agents c (customer) (supplier), respectively,
remain unchanged. third line states relation PO agents c (manufacturer)
contains new procurement order, identifier id product code pc, taken
parameters action createPO. Observe that, although executed customer, action affects
also local state manufacturer. next 3 lines express local PO relation c
m, addition newly added item, contains also all, only, items present
action execution. next conjunct (3 lines) states new identifiers must unique within
local PO relation. Notice cannot guaranteed agent c executing createPO
368

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS


~x.Products c (~x) Products c0 (~x)

~y .Materials (~y ) Materials s0 (~y )

c
c0
m0
b.Products
(pc,
b)

PO
(id,
pc,
b,
prepared)

PO
(id,
pc,
b,
prepared)


i, pc, b, s.i 6= id

(PO c0 (i, pc, b, s) PO c (i, pc, b, s))

(PO m0 (i, pc, b, s) PO c (i, pc, b, s))


i, pc, b, s, pc0 , b0 , s0 .
(PO c0 (i, pc, b, s) PO c0 (i, pc0 , b0 , s0 ) (pc = pc0 b = b0 = s0 ))

(PO m0 (i, pc, b, s) PO m0 (i, pc0 , b0 , s0 ) (pc = pc0 b = b0 = s0 ))



m1 = m2 m3 , pc, p, s.
(MO (m3 , pc, p, s) MO m0 (m3 , pc, p, s))

(MO (m3 , pc, p, s) MO s0 (m3 , pc, p, s))



m1 6= m2 pc, p, s.MO (m1 , pc, p, s) (
MO m0 (m1 , pc, p, s) MO m0 (m1 , pc, p, submitted)

MO s0 (m1 , pc, p, s) MO s0 (m1 , pc, p, submitted))
pc, p, s.MO (m2 , pc, p, s) (
MO s0 (m2 , pc, p, s) MO s0 (m2 , pc, p, accepted)

MO m0 (m2 , pc, p, s) MO m0 (m2 , pc, p, accepted))



m3 , pc, p, s.m1 6= m2 m1 6= m3
(MO m0 (m3 , pc, p, s) MO (m3 , pc, p, s))

(MO s0 (m3 , pc, p, s) MO (m3 , pc, p, s))

Table 2: postcondition ~ (id,pc,m1 ,m2 ) joint action
~ (id, pc, m1 , m2 )
hcreatePO(id, pc), doneMO(m1 ), acceptMO(m2 )i

369

=

fiB ELARDINELLI , L OMUSCIO & PATRIZI

(as cannot access relation PO m), value might actually returned automatically
system, used input agent. successive 3 lines state m1 m2 coincide,
i.e., two distinct operations executed material order m1 , action
effect local MO relation. contrary, successive 6 lines state, m1 6= m2
local MO relations agent material order id m1 changes state
submitted one id m2 accepted. Finally, last 3 lines state material
orders involved executed (joint) action propagated unchanged respective local
relations.
Notice although actions typically conceived manipulate artifacts specific class,
preconditions postconditions may depend artifact instances different classes.
example, note action createMO manipulates MO artifacts, precondition depends
PO artifacts. Also, stress action executability depends status attribute
artifact, actual data content whole database, i.e., artifacts.
Similarly, action executions affect W
status attributes. importantly, using first-order
formulas b = x1 , . . . , xb+1 i6=j (xi = xj ) postcondition , guarantee
AC program question bounded therefore amenable abstraction methodology
Section 4.
define agents induced AC program ACPotc given according Definition 4.2.
Definition 4.10 Given AC program ACPotc = hDotc , Uotc , otc i, agents Ac ,
induced ACPotc defined follows:
Ac = hDc , Actc , P rc i, (i) Dc above; (ii) Actc = c = {createPO, submitPO,
pay, deletePO}; (iii) (~u) P rc (lc ) iff (lc , ) |= (~x) (~y ) (~x) = ~u.
= hDm , Actm , P rm i, (i) Dm above; (ii) Actm = =
{createMO, doneMO, shipPO, deleteMO}; (iii) (~u) P rm (lm ) iff (lm , ) |=
(~x) (~y ) (~x) = ~u.
= hDs , Acts , P rs i, (i) Ds above; (ii) Acts = = {acceptMO, rejectMO,
shipMO}; (iii) (~u) P rs (ls ) iff (ls , ) |= (~x) (~y ) (~x) = ~u.
Note agents Ac , strictly correspond agents defined Def. 2.12.
particular, definition see createMO(id , price) P rm (lm )
interpretation lm (P O) relation PO local state lm contains tuple hid, pc, o, preparedi product pc offer o; doneMO(mo id ) P rm (lm ) iff
lm (M O) contains tuple id mo id status preparation. result, formal preconditions createMO doneMO satisfy intended meaning actions.
define AC-MAS generated set agents Ag = {Ac , , } according
Definition 4.3.
Definition 4.11 Given AC program ACPotc set Ag = {Ac , , } agents induced
ACPotc , AC-MAS induced ACPotc tuple Potc = hAg, s0otc , otc i, where:
s0otc = hlc0 , lm0 , ls0 initial global state, non-empty relations
Products Materials;

370

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

otc global transition function defined according Def. 4.3.
AC-MAS generated AC program ACPotc corresponds closely AC-MAS appearing Def. 2.13. example give snippet transition function otc considering global action (~u) = hcreatePO(id, pcode), doneMO(m1 ), acceptMO(m2 )i enabled
respective protocols global state s. definition actions createPO(id, pcode),
doneMO(m1 ), acceptMO(m2 ) li (s) P ri {c, m, s} implies
Products relation contains information product pcode. Also, interpretation relation MO contains tuples hm1 , p, pr, preparationi hm2 , p0 , pr0 , submittedi prod(~
u)

ucts p p0 . definition otc follows every s0 Sotc , s0 implies
0 , ) |=
0
0
(DAg DAg
(~
u) (id, pcode, m1 , m2 ), DAg DAg obtained
renaming relation symbols, (~u) = hcreatePO(id, pcode), doneMO(m1 ), acceptMO(m2 )i,
interpretation formal parameters id, pcode, m1 m2 Uotc . particu0
lar, interpretation relation PO DAg
extends DAg (P Oc ) DAg (P Om )
tuple hid, pc, b, preparedi, id fresh identifier. tuples material orders
0 (M ) (resp. 0 (M )) become hm , p, pr, submittedi
m1 m2 updated DAg
1
Ag
0
0
(resp. hm2 , p , pr , acceptedi). light specification (~u) action (~u), element updated transition. Finally, notice extensions indeed interpretations
PO MO s0 . Thus, semantics satisfies intended meaning actions. also
checked that, line discussion Section 2.4, full version function otc given
easily encode artifacts lifecycles given Figure 2.
proceed exploit methodology Section 4 verify AC program ACPotp .
use formula match Section 2.4 example specification; analogous results
obtained formulas. Observe according Definition 4.3 AC-MAS induced
ACPotp infinitely many states. assume two interpretations relations Products
Materials, determine initial state D0 . Consider maximum number max parameters
constants C operations c , . case analysis
max = 2. earlier remarked formulas b postcondition actions force
AC-MAS Potc corresponding ACPotc bounded. Potc b-bounded.
According Corollary 3.19, therefore consider finite domain U 0
U 0 D0 C con(match )
D0 (Products) D0 (Materials) C

|U 0 | 2b + |D0 | + |C | + |con(match )| + max
= 2b + |D0 | + |C | + 2
instance, consider subset U 0 Uotc satisfying conditions above. Given U 0
satisfies hypothesis Theorem 4.7, follows AC program ACPotc Uotc satisfies
match ACPotc U 0 does. AC-MAS induced latter finite-state
system, constructively built running AC program ACPotc elements
U 0 . Thus, ACPotc |= match decidable instance model checking therefore solved
means standard techniques.

371

fiB ELARDINELLI , L OMUSCIO & PATRIZI

manual check finite model indeed reveals match , budget cost satisfied
finite model, whereas fulfil not. Corollary 3.19 AC-MAS Potc induced ACPotp
satisfies specifications. Hence, view Definition 4.5, conclude artifactcentric program ACPotp satisfies match , budget cost satisfy fulfil .
line intuitions scenario.

5. Conclusions Future Work
paper put forward methodology verifying agent-based artifact-centric systems.
proposed AC-MAS, novel semantics incorporating first-order features, used reason multi-agent systems artifact-centric setting. observed model checking
problem structures specifications given first-order temporal-epistemic logic
undecidable proceeded identify suitable fragment decidability retained.
Specifically, showed class bounded, uniform AC-MAS identified admit finite abstractions preserve first-order specification language introduced. Previous results
literature, discussed Subsection 1.2, limit preservation fragments quantified language
allow interplay first-order quantifiers modalities.
explored complexity model checking problem context showed
EXPSPACE-complete. obviously hard problem, need consider
first-order structures normally lead problems high complexity. note
abstract interpretation domain actually linear size bound considered.
Mindful practical needs verification artifact-centric systems, explored
finite abstractions actually built. end, rather investigating one specific datacentric language, defined general class declarative artifact-centric programs. showed
systems admit uniform AC-MAS semantics. assumption bounded
systems showed model checking multi-agent system programs decidable gave
constructive procedure operating bisimilar, finite models. results general,
instantiated various artifact-centric languages. instance, Belardinelli et al. (2012b)
explore finite abstractions GSM programs using results.
exemplified methodology put forward use case consisting several agents purchasing delivering products. system infinitely many states showed admits
finite abstraction used verify variety specifications system.
question left open present paper whether uniform condition provided tight.
showed sufficient condition, explore whether necessary
finite abstractions whether general properties given. context interest
artifact-centric programs generate uniform structures. Also, worthwhile explore
whether notion related uniformity applied domains AI, example retain
decidability specific calculi. would appear case preliminary studies
Situation Calculus demonstrate (De Giacomo, Lesperance, & Patrizi, 2012).
application side, also interested exploring ways use results paper
build model checker artifact-centric MAS. Previous efforts area (Gonzalez, Griesmayer,
& Lomuscio, 2012) limited finite state systems. would therefore great interest
construct finite abstractions fly check practical e-commerce scenarios one
discussed.

372

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

References
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations Databases. Addison-Wesley.
Alonso, G., Casati, F., Kuno, H. A., & Machiraju, V. (2004). Web Services - Concepts, Architectures
Applications. Data-Centric Systems Applications. Springer.
Alves, A., Arkin, A., Askary, S., Barreto, C., Ben, Curbera, F., Ford, M., Goland, Y., Guzar, A.,
Kartha, N., Liu, C. K., Khalaf, R., Konig, D., Marin, M., Mehta, V., Thatte, S., van der Rijn,
D., Yendluri, P., & Yiu, A. (2007). Web Services Business Process Execution Language Version 2.0. Tech. rep., OASIS Web Services Business Process Execution Language (WSBPEL)
TC.
Bagheri Hariri, B., Calvanese, D., De Giacomo, G., Deutsch, A., & Montali, M. (2013). Verification
Relational Data-centric Dynamic Systems External Services. Hull, R., & Fan, W.
(Eds.), Proceedings 32nd ACM SIGMOD-SIGACT-SIGART Symposium Principles
Database Systems (PODS13), pp. 163174. ACM.
Baresi, L., Bianculli, D., Ghezzi, C., Guinea, S., & Spoletini, P. (2007). Validation Web Service
Compositions. IET Software, 1(6), 219232.
Baukus, K., & van der Meyden, R. (2004). knowledge based analysis cache coherence.
Davies, J., Schulte, W., & Barnett, M. (Eds.), Proceedings 6th International Conference Formal Engineering Methods (ICFEM04), Vol. 3308 Lecture Notes Computer
Science, pp. 99114. Springer.
Belardinelli, F., & Lomuscio, A. (2012). Interactions Knowledge Time First-Order
Logic Multi-Agent Systems: Completeness Results. Journal Artificial Intelligence Research, 45, 145.
Belardinelli, F., Lomuscio, A., & Patrizi, F. (2011a). Computationally-Grounded Semantics
Artifact-Centric Systems Abstraction Results. Walsh, T. (Ed.), Proceedings 22nd
International Joint Conference Artificial Intelligence (IJCAI12), pp. 738743. AAAI.
Belardinelli, F., Lomuscio, A., & Patrizi, F. (2011b). Verification Deployed Artifact Systems via
Data Abstraction. Kappel, G., Maamar, Z., & Nezhad, H. R. M. (Eds.), Proceedings
9th International Conference Service-Oriented Computing (ICSOC11), Vol. 7084
Lecture Notes Computer Science, pp. 142156. Springer.
Belardinelli, F., Lomuscio, A., & Patrizi, F. (2012a). Abstraction Technique Verification
Artifact-Centric Systems. Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), Proceedings
13th International Conference Principles Knowledge Representation Reasoning
(KR12). AAAI.
Belardinelli, F., Lomuscio, A., & Patrizi, F. (2012b). Verification gsm-based artifact-centric
systems finite abstraction. Liu, C., Ludwig, H., Toumani, F., & Yu, Q. (Eds.), Proceedings 10th International Conference Service-Oriented Computing (ICSOC12),
Vol. 7636 Lecture Notes Computer Science, pp. 1731. Springer.
Berardi, D., Calvanese, D., De Giacomo, G., Hull, R., & Mecella, M. (2005). Automatic Composition Transition-based Semantic Web Services Messaging. Bohm, K., Jensen,
C. S., Haas, L. M., Kersten, M. L., Larson, P.-A., & Ooi, B. C. (Eds.), Proceedings 31st
International Conference Large Data Bases (VLDB05), pp. 613624. ACM.
373

fiB ELARDINELLI , L OMUSCIO & PATRIZI

Berardi, D., Cheikh, F., Giacomo, G. D., & Patrizi, F. (2008). Automatic Service Composition via
Simulation. International Journal Foundations Computer Science, 19(2), 429451.
Bertoli, P., Pistore, M., & Traverso, P. (2010). Automated Composition Web Services via Planning Asynchronous Domains. Artificial Intelligence, 174(3-4), 316361.
Bhattacharya, K., Gerede, C. E., Hull, R., Liu, R., & Su, J. (2007). Towards Formal Analysis
Artifact-Centric Business Process Models. Alonso, G., Dadam, P., & Rosemann, M.
(Eds.), Proceedings 5th International Conference Business Process Management
(BPM07), Vol. 4714 Lecture Notes Computer Science, pp. 288304. Springer.
Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal Logic, Vol. 53 Cambridge Tracts
Theoretical Computer Science. Cambridge University Press.
Calvanese, D., De Giacomo, G., Lenzerini, M., Mecella, M., & Patrizi, F. (2008). Automatic Service
Composition Synthesis: Roman Model. IEEE Data Engineering Bulletin, 31(3), 18
22.
Ciobaca, S., Delaune, S., & Kremer, S. (2012). Computing Knowledge Security Protocols
Convergent Equational Theories. Journal Automated Reasoning, 48(2), 219262.
Clarke, E. M., Grumberg, O., & Peled, D. A. (1999). Model Checking. MIT Press.
Cohn, D., & Hull, R. (2009). Business Artifacts: Data-Centric Approach Modeling Business
Operations Processes. IEEE Data Engineering Bulletin, 32(3), 39.
Damaggio, E., Deutsch, A., & Vianu, V. (2012). Artifact Systems Data Dependencies
Arithmetic. ACM Transactions Database Systems, 37(3), 22:122:36.
Damaggio, E., Hull, R., & Vaculn, R. (2011). Equivalence Incremental Fixpoint
Semantics Business Artifacts Guard-Stage-Milestone Lifecycles. Rinderle-Ma, S.,
Toumani, F., & Wolf, K. (Eds.), Proceedings 9th International Conference Business
Process Management (BPM11), Vol. 6896 Lecture Notes Computer Science, pp. 396
412. Springer.
De Giacomo, G., Lesperance, Y., & Patrizi, F. (2012). Bounded Situation Calculus Action Theories
Decidable Verification. Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), Proceedings
13th International Conference Principles Knowledge Representation Reasoning
(KR12). AAAI.
Dechesne, F., & Wang, Y. (2010). Know Know: Epistemic Approaches Security
Protocol Verification. Synthese, 177(Supplement-1), 5176.
Deutsch, A., Hull, R., Patrizi, F., & Vianu, V. (2009). Automatic Verification Data-centric Business Processes. Fagin, R. (Ed.), Proceedings 12th International Conference
Database Theory (ICDT09), Vol. 361 ACM International Conference Proceeding Series,
pp. 252267. ACM.
Deutsch, A., Sui, L., & Vianu, V. (2007). Specification Verification Data-Driven Web Applications. Journal Computer System Sciences, 73(3), 442474.
Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning Knowledge. MIT
Press.

374

fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS

Gammie, P., & van der Meyden, R. (2004). MCK: Model Checking Logic Knowledge.
Alur, R., & Peled, D. (Eds.), Proceedings 16th International Conference Computer
Aided Verification (CAV04), Vol. 3114 Lecture Notes Computer Science, pp. 479483.
Springer.
Gerede, C. E., & Su, J. (2007). Specification Verification Artifact Behaviors Business
Process Models. Kramer, B. J., Lin, K.-J., & Narasimhan, P. (Eds.), Proceedings 5th
International Conference Service-Oriented Computing (ICSOC07), Vol. 4749 Lecture
Notes Computer Science, pp. 181192. Springer.
Gonzalez, P., Griesmayer, A., & Lomuscio, A. (2012). Verifying GSM-Based Business Artifacts.
Goble, C. A., Chen, P. P., & Zhang, J. (Eds.), Proceedings 19th IEEE International
Conference Web Services (ICWS12), pp. 2532. IEEE.
Grohe, M. (2001). Generalized Model-Checking Problems First-Order Logic. Ferreira, A.,
& Reichel, H. (Eds.), Proceedings 18th Annual Symposium Theoretical Aspects
Computer Science (STACS01), Vol. 2010 Lecture Notes Computer Science, pp. 1226.
Springer.
Heath, F. T., Boaz, D., Gupta, M., Vaculn, R., Sun, Y., Hull, R., & Limonad, L. (2013). Barcelona:
Design Runtime Environment Declarative Artifact-Centric BPM. Basu, S., Pautasso, C., Zhang, L., & Fu, X. (Eds.), Proceedings 11th International Conference
Service-Oriented Computing (ICSOC13), Vol. 8274 Lecture Notes Computer Science,
pp. 705709. Springer.
Hull, R. (2008). Artifact-Centric Business Process Models: Brief Survey Research Results
Challenges. Meersman, R., & Tari, Z. (Eds.), Proceedings (part II) Confederated International Conferences, CoopIS, DOA, GADA, IS, ODBASE 2008 (On Move
Meaningful Internet Systems: OTM08), Vol. 5332 Lecture Notes Computer Science,
pp. 11521163. Springer.
Hull, R., Damaggio, E., De Masellis, R., Fournier, F., Gupta, M., Heath, III, F. T., Hobson, S., Linehan, M., Maradugu, S., Nigam, A., Sukaviriya, P. N., & Vaculin, R. (2011). Business Artifacts
Guard-Stage-Milestone Lifecycles: Managing Artifact Interactions Conditions
Events. Eyers, D. M., Etzion, O., Gal, A., Zdonik, S. B., & Vincent, P. (Eds.), Proceedings
5th ACM International Conference Distributed Event-Based Systems (DEBS11),
pp. 5162. ACM.
Hull, R., Narendra, N. C., & Nigam, A. (2009). Facilitating Workflow Interoperation Using ArtifactCentric Hubs. Baresi, L., Chi, C.-H., & Suzuki, J. (Eds.), Proceedings 7th International Conference Service-Oriented Computing (ICSOC-ServiceWave 09), Vol. 5900
Lecture Notes Computer Science, pp. 118. Springer.
Kacprzak, M., Nabialek, W., Niewiadomski, A., Penczek, W., Polrola, A., Szreter, M., Wozna, B.,
& Zbrzezny, A. (2008). VerICS 2007 - Model Checker Knowledge Real-Time.
Fundamenta Informaticae, 85(1-4), 313328.
Kupferman, O., Vardi, M. Y., & Wolper, P. (2000). Automata-Theoretic Approach BranchingTime Model Checking. Journal ACM, 47(2), 312360.
Lomuscio, A., Penczek, W., Solanki, M., & Szreter, M. (2011). Runtime Monitoring Contract
Regulated Web Services. Fundamenta Informaticae, 111(3), 339355.
375

fiB ELARDINELLI , L OMUSCIO & PATRIZI

Lomuscio, A., Qu, H., & Raimondi, F. (2009). MCMAS: Model Checker Verification
Multi-Agent Systems. Bouajjani, A., & Maler, O. (Eds.), Proceedings 21st International Conference Computer Aided Verification (CAV09), Vol. 5643 Lecture Notes
Computer Science, pp. 682688. Springer.
Lomuscio, A., Qu, H., & Solanki, M. (2012). Towards Verifying Contract Regulated Service Composition. Autonomous Agents Multi-Agent Systems, 24(3), 345373.
Lomuscio, A., & Raimondi, F. (2006). Complexity Model Checking Concurrent Programs
CTLK Specifications. Baldoni, M., & Endriss, U. (Eds.), Proceedings 4th
International Workshop Declarative Agent Languages Technologies (DALT06), Selected, Revised Invited Papers, Vol. 4327 Lecture Notes Computer Science, pp.
2942. Springer.
Marin, M., Hull, R., & Vaculn, R. (2013). Data Centric BPM Emerging Case Management
Standard: Short Survey. La Rosa, M., & Soffer, P. (Eds.), Proceedings Business
Process Management Workshops - BPM 2012 International Workshops. Revised Papers, Vol.
132 Lecture Notes Business Information Processing, pp. 2430. Springer.
Meyer, J.-J. C., & van der Hoek, W. (1995). Epistemic Logic AI Computer Science, Vol. 41
Cambridge Tracts Theoretical Computer Science. Cambridge University Press.
Nigam, A., & Caswell, N. S. (2003). Business Artifacts: Approach Operational Specification.
IBM Systems Journal, 42(3), 428445.
Nooijen, E., Fahland, D., & Dongen, B. V. (2013). Automatic Discovery Data-Centric
Artifact-Centric Processes. La Rosa, M., & Soffer, P. (Eds.), Proceedings Business
Process Management Workshops - BPM 2012 International Workshops. Revised Papers, Vol.
132 Lecture Notes Business Information Processing, pp. 316327. Springer.
Papadimitriou, C. H. (1994). Computational complexity. Addison-Wesley.
Parikh, R., & Ramanujam, R. (1985). Distributed Processes Logic Knowledge. Parikh,
R. (Ed.), Logic Programs, Vol. 193 Lecture Notes Computer Science, pp. 256268.
Springer.
Singh, M. P., & Huhns, M. N. (2005). Service-Oriented Computing: Semantics, Processes, Agents.
John Wiley & Sons.
Wooldridge, M. (2000). Computationally Grounded Theories Agency. Proceedings 4th
International Conference Multi-Agent Systems (ICMAS00), pp. 1322. IEEE.
Wooldridge, M. (2001). Introduction Multiagent Systems. John Wiley & Sons.

376

fiJournal Artificial Intelligence Research 51 (2014) 133164

Submitted 05/14; published 09/14

Text Rewriting Improves Semantic Role Labeling
Kristian Woodsend
Mirella Lapata

k.woodsend@ed.ac.uk
mlap@inf.ed.ac.uk

Institute Language, Cognition Computation
School Informatics, University Edinburgh
10 Crichton Street, Edinburgh EH8 9AB

Abstract
Large-scale annotated corpora prerequisite developing high-performance NLP
systems. corpora expensive produce, limited size, often demanding linguistic
expertise. paper use text rewriting means increasing amount labeled
data available model training. method uses automatically extracted rewrite rules
comparable corpora bitexts generate multiple versions sentences annotated
gold standard labels. apply idea semantic role labeling show
model trained rewritten data outperforms state art CoNLL-2009
benchmark dataset.

1. Introduction
Recent years witnessed increased interest automatic identification labeling
semantic roles conveyed sentential constituents (Gildea & Jurafsky, 2002).
goal semantic role labeling task discover relations hold
predicate arguments given input sentence (e.g., whom,
when, where, how).
(1)

[Mrs. Yeargin]A0 [gave]V [the questions answers]A1 [two days
examination]TMP [two low-ability geography classes]ARG2 .

sentence (1), A0 represents Agent giver, A1 represents theme thing given,
A2 represents Recipient, TMP temporal modifier indicating action took
place, V determines boundaries predicate. semantic roles example
labeled style PropBank (Palmer, Gildea, & Kingsbury, 2005), broad-coverage
human-annotated corpus semantic roles syntactic realizations. PropBank annotation framework predicate associated set core roles (named A0,
A1, A2, on) whose interpretations specific predicate1 set adjunct
roles location time whose interpretation common across predicates (e.g., two
days examination sentence (1) above).
type semantic information shallow relatively straightforward infer automatically useful development broad coverage, domain-independent language
understanding systems. Indeed, analysis produced existing semantic role labelers
shown benefit wide spectrum applications ranging information extraction
(Surdeanu, Harabagiu, Williams, & Aarseth, 2003) question answering (Shen & Lapata,
1. precisely, A0 A1 common interpretation across predicates proto-agent protopatient sense described Dowty (1991).
c
2014
AI Access Foundation. rights reserved.

fiWoodsend & Lapata

Source
1. retreating guerrillas soon pursued government forces.
2. survey conducted Gallup Poll
last summer indicated one four
Americans takes cues stars believes ghosts.
3. examiner kind let student finish lunch.
4. didnt know rules,
died.
5. Mexico City, biggest city world,
many interesting archaeological sites.
6. arrival train unexpected.

Target
Government forces soon pursued retreating guerrillas.
survey conducted Gallup
Poll last summer. indicated one
four Americans takes cues stars
believes ghosts.
kind examiner let student finish
lunch.
died, didnt know
rules.
Mexico City many interesting archaeological sites.
trains arrival unexpected.

Table 1: Examples syntactic rewriting.

2007), machine translation (Wu & Fung, 2009) summarization (Melli, Wang, Liu,
Kashani, Shi, Gu, Sarkar, & Popowich, 2005).
SRL systems date conceptualize semantic role labeling task supervised
learning problem rely role-annotated data model training. Supervised methods
deliver reasonably good performance, F1-scores low eighties standard test
collections English. rely primarily syntactic features (such path features)
order identify classify roles. mixed blessing path
argument predicate informative also quite complicated. Many paths
parse tree likely occur relatively small number times (or all)
resulting sparse information classifier learn from. Even training
data includes examples specific predicate set arguments, unless test sentence
contains syntactic structure, far classifier concerned,
labeling items within two sentences unrelated.
idea use use rewrite rules order create several syntactic variants
sentence, thus alleviating training requirements semantic role labeling. Rewrite
rules typically synchronous grammar rules defining sequence source terminals
nonterminals rewrites sequence target terminals nonterminals. rules
often extracted monolingual corpora containing parallel translations
source text (Barzilay & McKeown, 2001; Pang, Knight, & Marcu, 2003), bilingual
corpora consisting documents translations (Bannard & Callison-Burch, 2005a;
Callison-Burch, 2007), comparable corpora Wikipedia revision histories (Coster
& Kauchak, 2011; Woodsend & Lapata, 2011). Examples rewrites given Table 1.
include transforming passive active sentences (see sentence pair (1) Table 1),
splitting long complicated sentence several shorter ones (see (2) Table 1),
removing redundant parts sentence (see (3) Table 1), reordering parts sentence
(see (4) Table 1), deleting appositives (see (5) Table 1), transforming prepositional
phrase genitive (see (6) Table 1), on.
134

fiText Rewriting Improves Semantic Role Labeling

automatically extract syntactic rewrite rules corpora use generate
multiple versions role annotated sentences whilst preserving original semantic roles.
therefore expand training data wide range syntactic variations
predicate-argument combination learn semantic role labeler expanded
dataset. approach describe essentially increases size training data
creating many different syntactic variations different predicates roles.
Rewrite rules previously deployed variety text-to-text generation applications ranging summarisation (Galley & McKeown, 2007; Yamangil & Nelken, 2008;
Cohn & Lapata, 2009; Ganitkevitch, Callison-Burch, Napoles, & Van Durme, 2011),
question answering (Wang, Smith, & Mitamura, 2007), information retrieval (Park, Croft,
& Smith, 2011), simplification (Zhu, Bernhard, & Gurevych, 2010; Woodsend & Lapata,
2011; Feblowitz & Kauchak, 2013), machine translation (Callison-Burch, 2008; Marton, Callison-Burch, & Resnik, 2009; Ganitkevitch, Cao, Weese, Post, & Callison-Burch,
2012). However, application text rewriting means increasing amount
labeled data available model training novel knowledge. show experimentally, syntactic transformations improve SRL performance beyond state art
using CoNLL-2009 benchmark dataset best scoring system (Bjorkelund,
Hafdell, & Nugues, 2009). Importantly, approach used combination
SRL learner role-annotated data. Moreover, specifically tied SRL task
employed learning model dataset. Rewrite rules could used expand
training data tasks make use syntactic features semantic parsing
(Kwiatkowski, 2012) textual entailment (Mehdad, Negri, & Federico, 2010; Wang &
Manning, 2010).
following, present overview related work (Section 2) describe
rewrite rules automatically extracted filtered correctness (Section 3). Section 4 details experiments Section 5 reports results.

2. Related Work
idea transforming sentences make amenable NLP technology dates
back Chandrasekar, Doran, Srinivas (1996) argue simpler sentences would
decrease likelihood incorrect parse. end, employ mostly hand-crafted
syntactic rules aimed splitting long complicated sentences simpler ones. Klebanov, Knight, Marcu (2004) preprocess texts Easy Access Sentences, i.e., sentences
consisting one finite verb dependents order facilitate information seeking applications summarization information retrieval accessing factual information.
similar vein, Vickrey Koller (2008) devise large number hand-written rules
order simplify sentences semantic role labeling. present log-linear model
jointly learns select best simplification (out possibly exponential space
candidates) role labeling. Kundu Roth (2011) use textual transformations
domain adaptation. Rather training new model out-of-domain data, propose
rewrite out-of-domain text similar training domain.
pilot idea semantic role labeling using hand-written rewrite rules show
compares favorably approaches retrain model target domain.
135

fiWoodsend & Lapata

work focused idea automatically expanding data available
given task without, however, applying transformations. Furstenau Lapata (2012)
combine labeled unlabeled data projecting semantic role annotations labeled
source sentence onto unlabeled target sentence. find novel instances classifier
training based lexical structural similarity manually labeled seed instances.
Zanzotto Pennacchiotti (2010) increase datasets textual entailment mining
Wikipedia revision histories.
Contrary previous work, automatically extract general rewrite rules various data sources including Wikipedia revision histories, comparable articles, bilingual
corpora. Given sentence (semantic role) annotated data, create several syntactic transformations, many may erroneous. maintain model
training transformations whose role labels preserved syntactic rewrite.
identify transformations label-preserving automatically, without requiring
SRL-specific knowledge. approach differs Vickrey Koller (2008)
three important aspects: (a) employ automatic rules simplification specific, (b) attempt select best rewrite, transformations preserve
gold standard role labels used training (c) model jointly
rewrites sentences labels semantic roles; rewrite training data
available model SRL task. work shares Kundu
Roth (2011) idea transforming sentences preserving gold standard
role labels. However, transform test data make look like training
data. unavoidably requires specialized knowledge differences two
domains, general model have.
mentioned earlier, use synchronous grammars extract set possible syntactic rewrites. Synchronous context-free grammars (SCFGs; Aho & Ullman, 1969)
generalization context-free grammar (CFG) formalism simultaneously produce
strings two languages. used extensively syntax-based statistical MT
(Wu, 1997; Yamada & Knight, 2001; Chiang, 2007; Graehl & Knight, 2004) related
generation tasks sentence compression (Galley & McKeown, 2007; Cohn & Lapata,
2009, 2013; Ganitkevitch et al., 2011), sentence simplification (Zhu et al., 2010; Feblowitz
& Kauchak, 2013; Woodsend & Lapata, 2011), summarization (Woodsend & Lapata,
2012). Rather focusing one type transformation (e.g., simplification compression), learn full spectrum rewrite operations select rules appropriate
task hand. Furthermore, results show rewrite rules improve semantic role
labeling performance across board, irrespectively specific variant synchronous
grammar corpus used. experiment conventional (weighted) SCFGs (Aho &
Ullman, 1969) tree substitution grammars (Eisner, 2003) employ transformation
rules extracted Wikipedia revision histories (Zhu et al., 2010; Woodsend & Lapata,
2011) bitexts (Ganitkevitch, Van Durme, & Callison-Burch, 2013).

3. Method
section describe general idea behind algorithm move
present specific implementation. define transformation g function
maps example sentence modified sentence s0 . Let G set known
136

fiText Rewriting Improves Semantic Role Labeling

transformation functions, G = {g1 , g2 , . . . , gn }. Suppose labels associated
example s. context paper, semantic role labels. Labels could
defined spans tokens, use CoNLL 20089 formalism
head word span labelled. transformation function therefore mapping
tokens sentence tokens t0 s0 . require mapping
involves tokens s0 , require mappings one-to-one.
label-preserving transformation transformation gi mapping (some the)
tokens example tokens t0 s0 , (correct) labels t0 identical
labels source tokens token mappings defined gi . words,
labels could preserved, preserved, others introduced.
Let G set label-preserving transformation functions, G G. problem
address paper therefore two-fold: Firstly, find automatically set
possible transformation functions G due automated nature unavoidably
error-prone process. Secondly, identify (again automatically) transformations G
actually label-preserving specifically, transformations rewrite
training instance s0 varying syntactic structure, yet preserve
semantic roles arguments appear new version s0 .
Algorithm 1 describes approach boils three steps: (a) extracting
transformations, (b) refining transformations, (c) generating labeling extended
corpus. standard gold annotated corpus used train initial semantic role labeling
model (see lines 12 Algorithm 1). Meanwhile, set candidate transformation functions G extracted suitable comparable parallel corpus (line 3). full
set transformation functions used rewrite gold corpus, creating much extended
corpus inevitably contain grammatically semantically incorrect sentences.
extended corpus next automatically labeled using original SRL model preprocessing normal SRL pipeline (whose details discuss Section 4.2), without
knowledge transformation functions involved.
could theory use extended corpus basis training SRL
model. However, contain many errors, unlikely yield useful information
guide model. One approach could manually correct rewrites
generated automatically, would time resource-intensive. Instead,
corrections automatically, create extended corpus rewrites
impair quality training data. therefore learn rules yield accurate
rewrites, i.e., rewrites preserve labels gold-standard. intuition
that, given large number possible rewrites, SRL model general label
accurate rewrites correctly mis-label erroneous sentences, due finding
confusing. thus compare semantic role labels produced model
labels corresponding predicate-argument pairs gold corpus, provide
samples train binary classifier (here SVM) learns predict rewrites
likely successful problematic (lines 1119 Algorithm 1).
rewritten sentence classed positive sample SRL model able label
transformation standard better able label original
sentence, i.e. labels SRL model predicts transformed sentence match
predicted original, corrected respect mapped
gold labels. If, however, semantic role longer predicted correctly, missed,
137

fiWoodsend & Lapata

Algorithm 1 Learn SRL model Mextended extending gold training corpus Cgold
transformation functions G.
1: Mgold SRL model trained Cgold
2: Cmodel label Cgold using Mgold
Extract transformations:
3: G transformation functions extracted pairs aligned sentences
comparable corpora
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

19:
20:
21:
22:
23:
24:
25:
26:
27:

Refine transformations:
initialize SVM training data DSVM
sentences Cgold
g applicable transformations G
s0 rewrite using g
label s0 using Mgold
SRL labels s0 match labels Cgold equivalent Cmodel
true
else
false
end
add (s0 , y) DSVM
end
end
train SVM using DSVM
G {g G : g positive SVM weight}
Generate extended corpus:
initialize refined rewrite corpus Crefined
sentences Cgold
g applicable transformations G
s0 rewrite using g
project labels s0 using g
add s0 Crefined
end
end
Cextended Cgold Crefined

Retrain SRL model:
Mextended SRL model trained Cextended
29: return Mextended

28:

138

fiText Rewriting Improves Semantic Role Labeling

erroneous role introduced, classified negative sample, sample likely
harm training new SRL model.
SVM identified refined set transformation functions G (line 20),
transformations used create extended training corpus. time, knowledge
transformation function involved project labels correspond original
gold corpus (lines 2128). case SRL, labels describe predicate
arguments. extended corpus supplements original gold standard corpus (line 29),
combination used create SRL model (line 30).
worth noting method impinge actual process learning
SRL model, concerned preparation training data. therefore believe
applied range SRL modeling approaches, gains performance
achieve largely orthogonal could made improving aspects
learning process (see Section 5.3 empirical evidence).
3.1 Learning Transformations
Conceptually wide range text-rewriting transformation functions could included
set G, paraphrasing, simplification translation another language. Here,
focus transformation functions expressed synchronous context-free
grammars (Aho & Ullman, 1969). Synchronous rules operate parse tree constituents
context-free manner, typically modify syntax. transformations consider
sub-categorized into:
1. Statement extraction. Constituents sub-tree parse tree identified, extracted context rewritten complete sentence, typically shorter
simpler, although necessarily so.
2. Compression. original sentence rewritten compressing constituents
parse tree, typically deleting nodes.
3. Insertion. New elements added parse tree. significant chunks new
text would semantic role information own, practice insertions
often additional punctuation clarify scope phrases, simple structure
. . . . aid statement extraction.
4. Substitution. using lexicalized synchronous grammar, text replaced
new text, paraphrases represented.
obtain set possible transformations G monolingual comparable corpora
drawn Wikipedia bitexts (see Section 4 details). following describe
grammar formalisms resources consider.
3.1.1 Transformations Monolingual Corpora
extract transformation rules corpora broadly comparable, using
unsupervised process. corpora constructed Wikipedia revision histories,
comparable Wikipedia articles. result, cannot guaranteed aligned source
target sentences truly related, expected source sentence
139

fiWoodsend & Lapata

fully generate target sentence. practice means addition requiring
strictly synchronous structure source target sentences, cannot assume
alignment source target root nodes, require surjective alignment
target nodes nodes source parse tree. able describe structural mismatches
non-isomorphic tree pairs (the grammar rules comprise trees arbitrary depth,
fragments mapped) represent transformation functions using synchronous
tree substitution grammar formalism (Eisner, 2003).
synchronous tree-substitution grammar (STSG) defines space valid pairs
source target parse trees. Rules specify map tree fragments source parse
tree fragments target tree, recursively free context. Following Cohn
Lapata (2009), STSG 7-tuple, G = (NS , NT , , , P, RS , RT ) N
non-terminals terminals, subscripts indicating source
target respectively. P productions RS NS RT NT distinguished
root symbols.
Typically, production rewrite rule two aligned non-terminals X NS
NT source target:
hX, h, , i,
elementary trees rooted symbols X respectively.
synchronous context free grammar would limited one level elementary
trees, STSG imposes limits elementary trees arbitrarily deep.
one-to-one alignment frontier nodes (non-terminal leaves elementary
trees) specified .
experiments, investigate two STSG variants, strictly synchronous tree
substitution grammar T3 (Cohn & Lapata, 2009), originally developed
task text compression, support full range transformation operations
quasi-synchronous tree substitution grammar (QTSG) Woodsend Lapata (2011),
used text simplification summarization (Woodsend & Lapata, 2012).
T3 tokens first aligned using probabilistic aligner initially provided
identity mappings entire vocabulary. experiments used Berkeley
aligner (Liang, Taskar, & Klein, 2006), however aligner broadly similar output
could used instead. Synchronous rules comprising trees arbitrary depth
extracted pair input CFG parse trees, consistent alignment. Across
complete corpus aligned trees, T3 filters extracted rules provide maximally
general rule set, consisting rules smallest depth, still capable
synchronously deriving original aligned tree pairs. removing identity rules,
resulting grammar forms transformation functions G.
Unlike T3, QTSG works partial alignment tokens, based identity.
Non-terminal nodes parse trees aligned consistent token
alignment. result sections source target parse trees
remain unaligned. Then, like T3, synchronous rules comprising trees minimum necessary
depth extracted pair input trees, consistent alignment,
before, identity rules removed form G.
example, Figure 1 shows two comparable parse trees aligned token level.
synchronous rules extracted alignment T3 QTSG shown Table 2.
140

fiText Rewriting Improves Semantic Role Labeling

ROOT


NNP

.

VP

NP
NNS

VP

VBP



VBN

VP
VP



NP

VB

PP

NP

NP



NN

DT

ADVP

Modern

scholars

come





scholars



question

question





existence

existence

NNS
NP



least

JJ

CD

NNS



first

nine

emperors



first

nine

emperors

DT

JJ

CD

NNS

.

.

NP
PP

NP
NNP

JJS



NN

DT







DT

NP

VBP
VP

.

ROOT

Figure 1: Example sentence alignment showing source (above) target (below) trees.

possible extract rules nodes child level, rules T3
QTSG extract parent level identical. cases sub-tree
compressed, (in example, come question compressed question), QTSG
extracts full sub-tree frontier nodes align, T3 extract several rules
smallest depth.
3.1.2 Transformations Bitexts
also obtain transformation rules ParaPhrase DataBase (PPDB, Ganitkevitch
et al., 2013), collection English (and Spanish) paraphrases derived large bilingual
parallel corpora. variety paraphrases (lexical, phrasal, syntactic) obtained
following Bannard Callison-Burchs (2005b) bilingual pivoting method.
141

fiWoodsend & Lapata

Rules extracted T3
], [S NP
VP
. ]i

hS, Si



h[S NP

hNP, NPi



h[NP [NNP Modern] NNS

hVP, VPi



h[VP VBP

hVP, VPi



h[VP VBN

hVP, VPi



h[VP

hVP, VPi



h[VP [VB question] NP

hNP, NPi



h[NP NP

hNP, NPi



h[NP DT

hPP, PPi



h[PP

hNP, NPi



h[NP ADVP

hS, Si



h[S NP

hNP, NPi



h[NP NNP

hVP, VPi



h[VP VBP

hNP, NPi



h[NP NP

hNP, NPi



h[NP DT

hPP, PPi



h[PP

hNP, NPi



h[NP ADVP

VP

1

2

VP



VP

PP

1

NP



1

.

JJ

NP


2

2

3

]i

1

]i

PP

1

NP
CD

2

2

NN

1

1

]i

]i

2

]i

2

NNS

3

1

]i

4

], [NP DT

1

1

1

JJ

2

CD

3

NNS

4

]i

3

[S [VP



], [PP
1

2

], [NP [DT Some] NNS

JJ

2

1

PP

1

], [NP DT

DT

]i

1

1

]i

1

], [VP [VBP question] NP

], [NP NP

2

3

Rules extracted QTSG
], [S NP
VP
. ]i

[VP VBN

NN

1

1

], [NP DT

1

2

]], [VP VP

], [PP

2

PP

1

2

2

1

], [NP NP

2

NNS



1

], [VP VP

DT



VP

1

1

], [NP [DT Some] NNS

], [VP VP

1

1

NN

1

3

[S VP





1

.

2

NN

1

NP
CD

3

2



1

]i

[VP VB

1

NP

2

]]]]], [VP VBP

1

NP

2

]i

]i

2

]i

]i
NNS

4

], [NP DT

1

JJ

2

CD

3

NNS

4

]i

Table 2: Synchronous tree grammar rules extracted T3 QTSG aligned
sentences Figure 1. Boxed indices short-hand notation alignment, .

intuition two English strings e1 e2 translate foreign
string f assumed meaning. method pivots f
extract he1 , e2 pair paraphrases. example shown Figure 2 (taken
Ganitkevitch et al., 2013). method extracts wide range possible paraphrases
unavoidably noisy due inaccurate word alignments. Paraphrases ranked
computing p(e1 |e2 ) shown below:
X
p(e2 |e1 )
p(e2 |f )p(f |e1 )
(2)
f

p(ei |f ) p(f |ei ) translation probabilities estimated bitext (Koehn,
Och, & Marcu, 2003).
Rewrite rules PPDB obtained using generalization method sketched
extract syntactic paraphrases (Ganitkevitch et al., 2011). Using techniques
syntactic machine translation (Koehn, 2010), SCFG rules first extracted Englishforeign sentence pairs. foreign phrase corresponding English phrase found via
142

fiText Rewriting Improves Semantic Role Labeling

... 5 farmers thrown jail

Ireland ...

... fnf Landwirte

festgenommen

, weil ...

... oder wurden

festgenommen

, gefoltert ...

...

imprisoned

, tortured ...

Figure 2: Paraphrases extracted via bilingual pivoting.
word alignments. phrase pair turned SCFG rule assigning left-hand side
nonterminal symbol, corresponding syntactic constituent dominates English
phrase. introduce nonterminals right-hand sides rule, corresponding subphrases English foreign phrases replaced nonterminal symbols.
sentence pairs bilingual parallel corpus results translation grammar
serves basis syntactic machine translation. translation grammar
converted paraphrase grammar follows. Let r1 r2 denote translation rules
left-hand side nonterminals hX, foreign language strings match:
r1 = hX, h1 , ,

(3)

r2 = hX, h2 , ,
paraphrase rule rp created pivoting f :
rp = hX, h1 , 2 ,

(4)

Although shown equations (3) (4), rules SCFG associated
set features combined log-linear model estimate derivation
probabilities.
3.1.3 Manual Transformations
experiments primarily make use automatically learned transformations since
adapted different tasks, domains languages. However, proposed approach necessary transformation functions acquired automatically
functions could also crafted hand. thus also investigated effectiveness
rewrites generated system Heilman Smith (2010) (henceforth H&S),
uses sophisticated hand-crafted rule-based algorithm extract simplified declarative sentences English syntactically complex ones. rules similar engineered Vickrey Koller (2008) deterministic generate
unique rewrite given sentence. algorithm operates standard phrase
143

fiWoodsend & Lapata

structure tree input sentence. extracts new sentence trees input tree
following: non-restrictive appositives relative clauses; subordinate clauses
subject finite verb; participial phrases modify noun phrases, verb phrases,
clauses. addition, algorithm splits conjoined S, SBAR, VP nodes, extracts new
sentence trees conjunct. output tree processed move leading
prepositional phrases quotations last children main verb phrase,
following removed: noun modifiers offset commas (non-restrictive appositives,
non-restrictive relative clauses, parenthetical phrases, participial phrases), verb modifiers
offset commas (subordinate clauses, participial phrases, prepositional phrases), leading
modifiers main clause (nodes precede subject).
Table 3 shows examples rules extracted using T3, QTSG PPDB grammar
formalisms applied sentence CoNLL dataset. final column Table 3 indicates whether transformation could classed statement extraction, compression,
insertion, substitution. reflected table, T3 captures compression transformations deleting nodes parse tree; QTSG rules range mainly syntactic
transformations; PPDB transformations substitutions words short phrases.
3.2 Refining Transformations
mentioned earlier, transformation rules obtained synchronous grammars
could used rewrite gold standard sentences. Unfortunately, due nature
corpora rules obtained automatic extraction process, many
rules contain errors impair rather improve quality
training data. idea extrapolate rules trust observing SRL
labeler handles rewritten sentences. mis-labeled them, possible
rewrite correct original labels preserved.
rewritten sentence classed positive sample SRL model predicts
labels transformed sentence predicted original, labels
corrected respect gold labels. If, however, semantic role
longer predicted correctly, missed, erroneous role introduced, classified
negative sample, sample likely harm training new SRL model.
capture full impact candidate transformation function, sentence provided
positive sample classifier labels (i.e., predicates arguments)
source sentence successfully projected onto rewrite. Table 4 shows
examples positive negative samples T3, QTSG, PPDB rewrites. Note
refining used H&S outputs.
decide transformation function include refined set, used linear
kernel SVM (Vapnik, 1995) binary classifier, classifiers indeed suitable
statistical tests contingency could used. input SVM learner set
l training samples (x1 , y1 ), . . . , (xl , yl ), xi Rn , {+1, 1}. xi n dimensional
feature vector representing ith sample, yi label sample. learning
process involves solving convex optimization problem find large-margin separation
hyperplane positive negative samples. order cope inseparable data,
misclassification allowed, amount determined parameter C,
thought penalty misclassified training sample. one
144

fiText Rewriting Improves Semantic Role Labeling

Grammar
Original

T3

Examples
Bell, based Los Angeles, makes distributes electronic, computer
building products.
Bell, based, makes distributes electronic, computer building products.
hPP, PPi h[PP NP ], [PP ]i

Comp

Bell, based Los Angeles, makes distributes.
hNP, NPi h[NP ADJP NNS ], [NP ]i

Comp

Based Los Angeles, makes distributes electronic, computer building
products.
hNP, NPi h[NP NNP ,], [NP ]i

Comp

Bell, based Angeless, makes distributes electronic, computer building products.
hNP, NPi h[NP NNP NNP ], [NP NNP
[POS s]]i

Comp

Bell makes distributes electronic, computer building products.
hNP, NPi h[NP NP
, VP ,], [NP NP ]i

Comp

makes distributes electronic, computer building products.
hS, Si
h[S NP VP
. ], [S [NP It] VP
. ]i

Ins

1

QTSG

1

1

1

1

Bell based Los Angeles.
hNP, Si
h[NP NP
, VP
1

2

1

,], [S NP

2

1

2

[VP [VBD was] VP

2

] .]i

Ext

Bell, based Los, makes distributes electronic, computer building
products.
hNP, NPi h[NP NNP
NNP ], [NP NNP ]i

Comp

Los Angeles makes distributes electronic, computer building products.
hNP, NPi h[NP NP , [VP VBN [PP NP ]] ,], [NP NP ]i

Comp

Bell, founded Los Angeles, makes distributes electronic, computer
building products.
hVP, VPi h[VP [X based] PP ], [VP [X founded] PP ]i

Sub

Bell, building Los Angeles, makes distributes electronic, computer
building products.
hVP, VPi h[VP [X based]
NP ], [VP [X building]
NP ]i

Sub

Bell, based Los Angeles, makes distributes electronic, computer
building products.
hVP, VPi h[VP VBN
NP ], [VP VBN
[X during] NP
]i

Sub

1

1

1

PPDB

Type

1

1

1

1

1

2

2

1

1

2

2

Table 3: Examples transformation rules extracted using T3, QTSG PPDB grammar
formalisms, applied sentence marked Original. final column indicates
whether rule statement extraction (Ext), compression (Comp), insertion
(Ins) substitution (Sub). before, boxed indices short-hand notation
alignment, .

view (the dual problem), result set Support Vectors, associated weights ,
constant b. another view (the primal problem), result vector w
defines separation hyperplane, dimension depends particular kernel
145

fiWoodsend & Lapata

Original Bell, based Los Angeles, makes distributes electronic, computer building products.
T3

QTSG

Bell, based, makes distributes electronic, computer building products.
Bell, based Los Angeles, makes distributes.
Based Los Angeles, makes distributes electronic, computer building products.
Bell, based Angeless, makes distributes electronic, computer building products.

+
+
+


Bell makes distributes electronic, computer building products.
makes distributes electronic, computer building products.
Bell based Los Angeles.
Bell, based Los, makes distributes electronic, computer building products.
Bell, based Angeles, makes distributes electronic, computer building products.
Los Angeles makes distributes electronic, computer building products.

+
+
+

+


PPDB

Bell, founded Los Angeles, makes distributes electronic, computer building products.
Bell, building Los Angeles, makes distributes electronic, computer building products.
Bell, based Los Angeles, makes distributes electronic, computer building products.

H&S

Bell makes. Bell distributes. Bell based Los Angeles.

Original employees sign options, college also must approve plan.
T3

it, college also must approve plan.
college also must approve plan.
employees sign options, also must approve the.
employees sign for, college also must approve plan.


+

+

QTSG

employees sign options, college also must approve.
employees sign options, college also must approve plan.
employees sign options, college also must approve plan.


+


PPDB






+

+


H&S

college must approve plan employees sign options.






employees
employees
employees
employees






sign
sign
sign
sign
















options,
options,
options,
options,






college
college
college
college

also
also
also
also

must adopt plan.
must agree plan.
must endorse plan.
needs approve plan.

Original went permissible line warm fuzzy feelings.
T3






went permissible line feelings.
went warm fuzzy feelings.
went it.
went.

+


+

QTSG

went line warm fuzzy feelings.
went permissible line feelings.
went permissible warm fuzzy feelings.

+



PPDB

went permissible line hot fuzzy feelings.
went permissible line warm fuzzy feelings.


+

H&S

went permissible line warm fuzzy feelings.

Table 4: Examples rewrites generated T3, QTSG, PPDB source sentence
(Original) CoNLL-2009 training set. Symbols +/ indicate whether
sample classified positive (i.e., argument label preserving) forms part
extended training corpus, not.

146

fiText Rewriting Improves Semantic Role Labeling

function used SVM. case linear kernel function, wPis ndimensional,
feature vectors, straight-forward relationship w = lj=1 yj j xj
primal dual variables, effectively assigning weights explicitly specified features.
kernel functions allow interaction variables. instance using
binary valued features, degree2 polynomial kernel function implies classifier
considers available pairs features well.
used identity transformation functions involved features
sample, size feature space n = kGk, features binary-valued.
features could easily incorporated setting, perhaps capturing information
structure source sentence transformation function, might achieve good
results conjunction polynomial kernel, pursue avenue further.
Instead used linear kernel, due simple structure features, SVM
assigned weight transformation function independent source sentence.
chose transformation functions form refined set based whether
corresponding weight global threshold value, set threshold value
maximizing performance resulting SRL model development set.
3.3 Labeling Extended Corpus
SVM identified refined set transformation functions G, transformations used create extended training corpus. Using alignment information
transformation functions trace position tokens original sentence
rewrite, semantic role labels gold corpus projected onto corresponding predicate-argument pairs rewritten corpus. Assuming SVM correctly
identified transformation function involved indeed label-preserving,
transformation functions applied current context, semantic role labeling
rewrite quality standard source. conditions
however unlikely true, resulting degradation quality rewrite corpus.
corpus rewrites appended original gold standard corpus create new
larger training corpus, used create SRL model.

4. Experimental Setup
section present experimental setup assessing performance approach. give details corpora grammars used create transformations,
model parameters used identify preserve labels. explain existing SRL system modified approach, evaluated effects
increasing training data transformations.
4.1 Grammar Extraction
extracted synchronous grammars two monolingual comparable corpora drawn
Wikipedia. corpus 137,362 aligned sentences created pairing Simple English
Wikipedia English Wikipedia (Kauchak, 2013). corpus 14,831 paired sentences comparing consecutive revisions articles Simple English Wikipedia (Woodsend & Lapata, 2011). corpora provide large repository monolingual, comparable
147

fiWoodsend & Lapata

Grammar
T3
QTSG

Aligned
13,562
3,875

Revisions
5,386
669

Table 5: Non-identical rules extracted Wikipedia corpus, rules appearing
one two times removed.

sentences, taken real-world writing. Advantageously, Simple English Wikipedia encourages contributors employ simpler grammar ordinary English Wikipedia;
corpora therefore naturally contain many examples syntactic variation reordering sentence splitting, well paraphrasing changes content. Table 5 lists
number non-identical rules grammar formalism extracted Wikipedia
corpora, rules instance count one two removed.
addition grammars extracted Simple English Wikipedia, worked
monolingual synchronous grammar included Paraphrase Database (Ganitkevitch et al., 2013), paraphrases extracted bilingual parallel corpora.
English portion PPDB contains 220 million paraphrase pairs, including 140
million paraphrase patterns capturing syntactic transformations varying confidence.
form synchronous grammar, used highest scoring 585,000 paraphrases
subset constituent syntactic paraphrases (where nonterminals labeled Penn
Treebank constituents).
4.2 Semantic Role Labeler
method presented paper crucially relies semantic role labeler refining
transformations performing semantic analysis general. used publicly available system Bjorkelund et al. (2009). competed
CoNLL-2009 SRL-only challenge, ranked first English language, second overall. best knowledge, system represents state-of-the-art English
SRL parsing. system architecture consists four-stage pipeline classifiers:
predicate identification (although module required evaluation), predicate
sense disambiguation, binary classifier argument identification, finally argument
classification using multiclass classifier. Beam search used identify arguments
predicate label them, according local classifiers using features relate
mainly dependency parse information linking predicates potential arguments
siblings. addition, global reranker used select best combination candidates (see Section 5 details). SRL system requires tokenized input lemma,
POS-tag dependency parse information. information already provided
gold-standard training corpus (see immediately below). create equivalent information
transformed text evaluation files, used mate-tools pipeline (Bjorkelund
et al., 2009), retrained (like SRL model itself) training partition data.
used English language benchmark datasets CoNLL-2009 shared task
train evaluate SRL models. identified labeled semantic arguments nouns
148

fiText Rewriting Improves Semantic Role Labeling

Corpus
Training
+ H&S
+ PPDB
+ T3
+ QTSG
+ T3 + QTSG
+ PPDB + T3
+ PPDB + QTSG
+ PPDB + T3 + QTSG
Development
Test in-domain
Test out-of-domain

Sentences
39,272
55,474
238,732
203,941
500,627
704,561
442,666
739,352
943,286
1,334
2,399
425

Tokens
958,174
909,358
7,071,550
4,701,688
9,623,471
14,325,166
11,773,245
16,695,028
21,396,723
33,368
57,676
7,207

Table 6: Statistics corpora used train evaluate SRL models.
verbs (Hajic, Ciaramita, Johansson, Kawahara, Mart, Marquez, Meyers, Nivre, Pado,
Stepanek, Stranak, Surdeanu, Xue, & Zhang, 2009). used training, development,
test out-of-domain test partitions provided, statistics
data sets shown Table 6. Specifically, show increase training data
effected method using transformations obtained T3, QTSG, PPDB,
combinations. comparison also use manual transformations available
Heilman Smith (2010). train SRL model (and also previous stages NLP
pipeline), used data training partition only, development partition
used identify best subset G transformations2 .
used LibLinear (Fan, Chang, Hsieh, Wang, & Lin, 2008) train SVM,
hyper-parameters SVM tuned cross-validation training set
maximise area ROC curve, using automatic grid-search utility python
package scikit-learn (Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel,
Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, & Duchesnay, 2011). assessment cross-validation accuracy (in terms F1 score area
ROC curve) SVM grammar shown Table 7. results show
PPDB rewrites accurate employ, perhaps rules
heavily lexicalized grammars. T3 grammar unpredictable use,
although SVM scores considerably higher chance.
Test sets used solely evaluation, making use indicators data
files words argument-bearing predicates. Results generated using
CoNLL-2009 evaluation script unmodified. report results semantic roles
(i.e., combination syntactic dependencies tends yield higher scores)
using in-domain out-of-domain evaluation data. evaluation script,
semantic propositions evaluated converting semantic dependencies
2. result re-training performance reported worse models
available mate website, trained partitions CoNLL-2009 data
(training, development test).

149

fiWoodsend & Lapata

Grammar
PPDB
T3
QTSG

F1
0.85
0.67
0.78

Area ROC
0.82
0.61
0.72

Table 7: Statistics SVMs performance grammar, obtained crossvalidation training set.

predicate arguments, labeling dependency labels
corresponding argument. Additionally, dependency created virtual root node
predicate labeled predicate sense. comparable published
results, general report scores combine predicate sense argument role label
predictions. Tables 12, 13 14, however, focus arguments only, remove
predicate sense scores.

5. Results
section provide empirical evidence performance approach.
experiments primarily designed answer following questions. text rewriting
generally improve SRL performance? matter transformation rules use,
i.e., rules better others? transformation rules useful out-ofdomain data? SRL labels mostly affected rewriting? performance vary
depending size original training data? results sensitive learner
employed? first examine effect different (transformation) grammars
SRL task in-domain out-of-domain test data, move assess
labels mostly affected method. Finally, present results effect
combining approach global reranker training different-sized datasets.
5.1 Transformation Rules Improve F1 Across Board
Table 8 (left half) shows SRL performance (measured terms precision, recall, F1)
in-domain CoNLL-2009 test set. training corpora rewritten H&S
system, T3, QTSG, PPDB grammars, resulting SRL models significantly
(p < 0.01) improve model trained original corpus. used stratified shuffling (Noreen, 1989) examine whether differences F1 significant (Pado, 2006).
Recall shows largest increase, particularly acquired synchronous grammars,
indicating increased training data resulting better coverage. Generally
expense precision cases apart PPDB increased well.
Significant gains also seen acquired grammars compared H&S system,
exception T3 greater variation performance.
also combined rewrites produced different grammars (see T3+QTSG,
PPDB+T3, PPDB+QTSG PPDB+T3+QTSG Table 8) significantly
improve performance individual grammars (although still significantly better
original model H&S system), suggesting grammars capturing
150

fiText Rewriting Improves Semantic Role Labeling

Original
H&S
PPDB
T3
QTSG
PPDB+T3
PPDB+QTSG
T3+QTSG
PPDB+QTSG+T3
label projection

P
86.79
87.08
86.42
86.84
87.04
86.61
86.70
86.78
86.76
80.95

In-domain
R
F1
83.58 85.15
83.73 85.37
84.64 85.52
84.25 85.52
84.34 85.67
84.45 85.51
84.81 85.75
84.62 85.69
84.69 85.71
78.75 79.83

Out-of-domain
P
R
F1
76.04 71.73 73.82
76.33 70.86 73.49
75.37 72.66 73.99
76.04 72.29 74.12
76.88 72.83 74.89
75.65 72.49 74.03
76.64 73.22 74.89
76.56 72.88 74.67
76.54 73.19 74.83
66.94 66.93 66.93

Table 8: Semantic evaluation results CoNLL-2009 in-domain out-of-domain test
sets (combining predicate word sense argument role labels). Results
models trained Original training set, baseline extension training set,
extensions due grammar combinations. label projection: results
training PPDB+QTSG+T3 training corpus, without rewriting
labels using gold corpus information. Difference Original significant
p < 0.01. Difference H&S significant p < 0.01.

Proportion sentences
produced grammar

H&S
QTSG

Also produced
PPDB
0.4
0.0

grammar
T3
QTSG
0.2
28.1
3.2

Table 9: Sentence rewrite overlap (%) refined rewrite corpora produced H&S,
PPDB, T3 QTSG.

similar information. instance, T3 QTSG extracted corpora
aligned sentence pairs. degree overlap rewrite corpora produced
grammars shown Table 9. Although degree overlap exact sentences low,
relative performance resulting models closer (discussed below). Overall,
best performing system uses transformations obtained QTSG PPDB,
surprising rules extracted grammars present minimal overlap.
Benefits also transfer out-of-domain text acquired grammars, improving
overall performance even in-domain data (see right half Table 8).
F1-score QTSG model 1% higher original model, Recall
model combining acquired grammars increased 1.5%. Meanwhile,
rewrites H&S system seem improve coverage, resulting drop Recall
F1-score.
151

fiWoodsend & Lapata

Original
PPDB
T3
QTSG

P
86.79
87.34
87.36
87.48

In-domain
R
F1
83.58 85.15
83.10 85.17
83.26 85.26
83.31 85.34

Out-of-domain
P
R
F1
76.04 71.73 73.82
76.41 71.42 73.83
76.49 71.76 74.05
76.75 72.00 74.30

Table 10: Results CoNLL-2009 in-domain out-of-domain test sets, training SRL
model rewrites labeled positive.

SVM
Thresholds Count
None

+0.001
10
-0.001
10
-0.2
10

10
+0.001
5
+0.001
3

P
80.26
80.74
80.82
80.65
80.55
80.46
80.00

Quality
R
76.28
76.99
76.87
76.86
77.08
76.54
76.13

F1
78.22
78.82
78.80
78.71
78.78
78.45
78.02

Table 11: Effect selecting transforms SVM quality resulting model
(precision, recall F1 measures labeling development set).

addition, examined whether filtering set acquired transformation functions
indeed beneficial. approach proposed, transformations applied
training corpus twice: first time input SVM identify reliable rewrite
rules, second pass reduced set rules applied whole training corpus.
alternative approach would apply transforms once, train SRL
model. thus took rewrites labeled positive steps 914 Algorithm 1
corrected labels gold-standard (step 23). SRL models subsequently trained using
extended training corpus, created concatenating original training dataset
rewrites. Table 10 shows SRL performance different grammars (PPDB, T3,
QTSG) test set. Although precision F1 increased original model,
gains much reduced compared results obtained using SVM (Table 8).
appears extra rewrites obtained applying generally-reliable transforms
whole training set increases coverage, improves performance models.
Table 11 shows altering quality threshold (and removing indicator features
number times transformation function extracted) affects performance.
Results shown QTSG grammar (in-domain) development set (we observed
similar patterns grammars grammar combinations). SVM quality
threshold varied positive (no transformations accepted) negative (all
152

fiText Rewriting Improves Semantic Role Labeling

Original
H&S
PPDB
T3
QTSG
PPDB+T3
PPDB+QTSG
T3+QTSG
PPDB+QTSG+T3

P
82.69
83.08
82.34
82.88
83.00
82.61
82.62
82.75
82.83

In-domain
R
F1
78.25 80.41
78.45 80.70
79.89 81.10
79.30 81.05
79.27 81.09
79.62 81.09
80.01 81.29
79.77 81.23
79.95 81.37

Out-of-domain
P
R
F1
71.44 65.62 68.40
71.65 64.25 67.75
70.68 67.02 68.80
71.54 66.46 68.90
72.48 66.98 69.62
71.16 66.88 68.95
72.11 67.47 69.71
72.08 67.09 69.49
72.08 67.54 69.74

Table 12: Performance labeling semantic arguments (predicate word sense information removed). Difference Original significant p < 0.01. Difference
H&S significant p < 0.01.

Original
H&S
PPDB
T3
QTSG
PPDB+T3
PPDB+QTSG
T3+QTSG
PPDB+QTSG+T3

In-domain
P
R
F1
89.56 84.75 87.09
89.71 84.71 87.14
88.99 86.34 87.65
89.57 85.70 87.59
89.53 85.50 87.47
89.10 86.28 87.66
89.28 86.05 87.63
89.33 86.10 87.68
89.33 86.23 87.75

Out-of-domain
P
R
F1
87.20 80.10 83.50
87.40 78.38 82.65
86.24 81.78 83.95
87.09 80.90 83.88
87.28 80.66 83.84
86.75 81.53 84.06
86.77 81.18 83.88
87.34 81.29 84.20
86.90 81.43 84.07

Table 13: Accuracy identification classification (labeling) semantic arguments.

transformations). findings indicate constructing G transformations
positive SVM weight (threshold +0.001) gives better results transformations,
permissive threshold.
5.2 Transformation Rules Improve Semantic Role Assignment Verbal
Nominal Predicates
results Table 8 combine accuracy predicting sense predicates accuracy
labeling arguments. Generally, models better assigning correct predicate
sense. interesting result much gain performance seen rewriting
training corpus comes improving semantic role assignment. appears
153

fiWoodsend & Lapata



RAMMNR



RAMLOC





RAMCAU





RA2

















RA1
RA0
CA1





















































































































CA0
AMTMP









AMPRD
AMPNC

Argument



RAMTMP







AMNEG





AMMOD





AMMNR







AMLOC



















































AMDIR















AMCAU



















































AMDIS

AMADV







A5



A4

























A2





















A1



























A0







JJ

NN

VBP

VBZ

A3

NNP NNS

VB

+10%

+1%






AMEXT

Change F1



VBD VBG VBN

0%
1%

10%
Occurrences




1+
10+

100+



1000+

Predicate

Figure 3: Changes F1-score PPDB+T3+QTSG model Original, measured
pairs predicate POS-tag argument.

introducing syntactic variation training data provides model wider coverage
syntactic dependency paths predicate arguments.
Table 12 shows results models data sets above, focusing
argument labels only. acquired grammars show biggest improvements,
1% improvement Recall case, gains F1-score 0.5% 1.2%.
models data sets used Table 13, results argument
identification only, classification (unlabelled arguments). improvements
154

fiText Rewriting Improves Semantic Role Labeling



RAMMNR



RAMLOC





RAMCAU





RA2

















RA1
RA0
CA1











































































+1%







0%







































CA0
AMTMP









AMPRD
AMPNC

Argument



RAMTMP







AMNEG





AMMOD





AMMNR







AMLOC






















































AMDIR















AMCAU



















































AMEXT
AMDIS

AMADV







A5



A4

























A2





















A1



























A0







JJ

NN

VBP

VBZ

A3

NNP NNS

VB



VBD VBG VBN

Change F1
+10%

1%

10%
Occurrences




1+
10+

100+



1000+

Predicate

Figure 4: Relative performance terms F1-score QTSG (red) PPDB (blue)
models, pairs predicate POS-tag argument.

Original Recall F1. large before, showing overall
gains result improvements argument identification classification.
breakdown gains F1-score predicate POS-tag argument shown
Figure 3, illustrating relative improvements model trained acquired grammars (PPDB+T3+QTSG) model trained original CoNLL training data.
analysis reveals gain came increased precision recall
predicting core arguments. additional gains modifiers nominal pred155

fiWoodsend & Lapata

Dependency path distance
Proportion test set
SRL model:
Original
PPDB
T3
QTSG
PPDB+QTSG+T3

01
75.75

2
13.67

3
5.54

4
2.62

5
1.13

6
0.56

7+
0.73

88.83
+0.49
+0.63
+0.53
+0.74

74.27
+1.43
+1.15
+0.66
+1.60

61.73
+2.65
+1.65
+1.82
+2.49

54.76
+3.26
+1.67
+2.98
+2.02

43.08
+4.78
+1.42
+3.01
+6.20

23.91
+5.53
+1.22
0.96
+1.35

12.27
0.06
+0.69
+0.43
+1.61

Table 14: F1-scores labeled arguments distance predicate argument
measured number arcs dependency graph. Results
CoNLL in-domain test set. Lower rows show change F1-score
Original SRL model.

icates. improvement losses common core arguments
(A0 A1) verbal predicates, striking gains seen
core argument labels. seems consistent models learning wider syntactic
coverage. Figure 4 shows similar breakdown gains F1-score predicate POStag argument, time comparing improvements seen QTSG corpus
resulting PPDB. differences less pronounced, PPDB improving
core arguments more, QTSG improving performance labeling modifiers.
also investigated effect label projection mechanism itself. used
rewrites produced grammars (PPDB+T3+QTSG) extend training set. However, instead using projected labels, used original model Mgold (trained
training partition CoNLL-2009) label refined corpus. retrained
extended corpus used retrained model label test corpus. words,
removed step 25 Algorithm 1. considered form self-training. Results
test out-of-domain sets show using automatically generated labels
instead projected ones seriously impairs resulting model, F1-scores decreasing
almost 6% test set 8% out-of-domain set (see last row Table 8).
5.3 Transformation Rules Improve Performance Relations Involving Long
Dependency Paths
dependency path (the sequence arcs syntactic dependency tree)
predicate argument typically short. Table 14 shows gold-labeled
test set, three-quarters arguments direct dependency heads children
predicate, case nominal predicates, argument predicate itself. Existing
SRL models highly accurate shorter pathsthe original SRL model
F1-score almost 89%but prediction accuracy drops considerably dependency path
grows. seen Table 14, adding rewrites training set improves prediction
accuracy almost combinations transformation grammar dependency path
distance, largest gains seen number arcs dependency path
156

fiText Rewriting Improves Semantic Role Labeling

Original
H&S
PPDB
T3
QTSG
T3+QTSG
PPDB+T3
PPDB+QTSG
PPDB+QTSG+T3

P
88.44
88.68
86.42
88.04
88.41
88.24
86.61
86.70
87.94

In-domain
R
F1
84.42 86.38
84.34 86.46
84.64 85.52
84.78 86.38
85.05 86.70
85.21 86.70
84.45 85.51
84.81 85.75
85.25 86.57

Out-of-domain
P
R
F1
77.89 72.73 75.22
78.11 71.76 74.80
76.73 73.36 75.01
77.07 72.97 74.97
78.34 73.70 75.95
78.00 73.53 75.70
77.30 73.51 75.35
77.41 73.82 75.57
77.67 73.73 75.64

Table 15: Results CoNLL test sets models combining extended training data
global reranker. Difference Original significant p < 0.01. Difference
H&S significant p < 0.01.

three six. Improvements F1-score observed individual grammars
combination (PPDB+QTSG+T3).
5.4 Transformation Rules Improve Performance Even Global
Reranker Used
SRL system used (Bjorkelund et al., 2009) optionally incorporate global
reranker (Toutanova, Haghighi, & Manning, 2005). reranker re-scores complete
predicate-argument structure, using features stages local pipeline additional features representing sequence core argument labels current predicate.
Table 15 presents evaluation results global reranker trained extended corpora
produced method. Compared model trained original corpus, adding
reranker provide significant improvement.3 Training extended data gives
increases performance; smaller, though still significant,
case Table 8. indicates global reranker compensating some,
all, new information contained extended training data.
5.5 Transformation Rules Improve Performance Across (Small Large)
Datasets
also investigated accuracy labeler function size original
training data. size, subsets original training data created (with replacement) used train SRL model, performance resulting model
measured using development set. training subset, applied Algorithm 1:
original SRL model trained subset; created extended corpus
3. scores reported higher official CoNLL-2009 ones (in domain P:87.46, R:83.87,
F1:85.63; domain P:76.04, R:70.76, F1:73.31) using mate-tools NLP pipeline
dependency parse, rather dependency information provided test set.

157

fiWoodsend & Lapata




80

80











75




75












70

70

Recall

Precision











Rewrites



Source


65



65



60

60






55

55
160

625

2500

10000

40000



160

Source sentences

625

2500

10000

40000

Source sentences

Figure 5: SRL model performance function size training data,
without additional rewrites. Error bars show standard error 10 experiments.

subset using grammar; SVM trained time refine transformations
preserved labels; SRL model retrained original plus refined
rewritten version corpus subset.
particular, wanted investigate rewritten text provided performance
benefit small amount training data, benefit would
subsumed labeled training data provided. learning curves Figure 5 show
contrary: increasing quantity source training data undoubtedly improves
quality SRL model, found including rewritten training data addition
consistently improves precision recall measures. learning curves Figure 5
use QTSG grammar set transformation functions; obtained similar results
PPDB T3 (and grammar combinations), however omit sake
brevity.

6. Conclusions
paper investigated potential text rewriting means increasing
amount training data available supervised NLP tasks. method automatically
extracts rewrite rules comparable corpora uses generate multiple syntactic variants sentences annotated gold standard labels. Application method
semantic role labeling reveals syntactic transformations improve SRL performance
158

fiText Rewriting Improves Semantic Role Labeling

QTSG

PPDB

hNP, NPi



h[NP DT

hNP, NPi



h[NP NP

hNP, Si



h[NP NP

hS, Si



h[S even VBZ

hADJP, ADJPi



h[ADJP JJ

hPP, PPi



h[PP past month], [PP last month]i

1
1
1

JJ



, NP
PP

NNS

], [NP DT

CC NP



2

2



1

NP

2

NNS

], [NP NP

], [S NP
1

1

1

PP

2

1

2

]i

]i

.]i

], [S even though VBZ

], [ADJP equally JJ

1

1

NP

2

]i

]i

Table 16: Examples QTSG PPDB synchronous grammar rules given high importance
refinement. Boxed indices indicate alignment, .

beyond sate art CoNLL 2009 benchmark dataset. Specifically, experimentally show (a) rewrite rules, whether automatic hand-written, consistently
improve SRL performance, although automatic variants tend perform best; (b) syntactic transformations improve SRL performance within- out-of-domain; (c)
improvements observed across learners, even using global reranker.
future would like explore better ways identifying best (i.e., performance enhancing) rewrite rules may task grammar specific. Table 16
illustrates rules deemed important (i.e., given high weight) SVM classifier
SRL task. instance, could undertake detailed feature engineering, including tree-based ngram features capture grammaticality rewritten sentences.
Throughout paper argued transformation rules used enhance
performance SRL task. Conversely, work described might
relevance NLP tasks employing rewriting. example, idea identifying
label preserving transformations, could used learn rules meaning preserving
consequently safe use tasks simplification sentence compression. Machine
translation, textual entailment, semantic parsing additional application areas
stand benefit accurate rewrite rules. Much methodology reported
could adapted machine translation either training larger datasets (CallisonBurch, Koehn, & Osborne, 2006), domain-adaptation (Irvine, Quirk, & Daume III,
2013), evaluation (Kauchak & Barzilay, 2006)
Finally, beyond supervised SRL, would like adapt method unsupervised
semantic role induction (Lang & Lapata, 2011; Titov & Klementiev, 2012), investigate alternative synchronous grammar extraction methods (e.g., based dependency information),
obtain rewrite rules larger comparable corpora.

Acknowledgments
grateful anonymous referees whose feedback helped substantially improve
present paper. acknowledge financial support EPSRC (EP/K017845/1)
framework CHIST-ERA READERS project.
159

fiWoodsend & Lapata

References
Aho, A. V., & Ullman, J. D. (1969). Syntax directed translations pushdown assembler. Journal Computer System Sciences, 3 (1), 3756.
Bannard, C., & Callison-Burch, C. (2005a). Paraphrasing Bilingual Parallel Corpora.
Proceedings 43rd Annual Meeting Association Computational Linguistics, pp. 597604, Ann Arbor.
Bannard, C., & Callison-Burch, C. (2005b). Paraphrasing Bilingual Parallel Corpora.
Proceedings 43rd ACL, pp. 255262, Ann Arbor, MI.
Barzilay, R., & McKeown, K. (2001). Extracting Paraphrases Parallel Corpus.
Proceedings ACL/EACL, pp. 5057, Toulouse, France.
Bjorkelund, A., Hafdell, L., & Nugues, P. (2009). Multilingual semantic role labeling.
Proceedings Thirteenth Conference Computational Natural Language Learning (CoNLL 2009): Shared Task, pp. 4348, Boulder, Colorado. Software retrieved
https://code.google.com/p/mate-tools/.
Callison-Burch, C. (2007). Paraphrasing Translation. Ph.D. thesis, University Edinburgh.
Callison-Burch, C. (2008). Syntactic Constraints Paraphrases Extracted Parallel
Corpora. Proceedings 2008 Conference Empirical Methods Natural
Language Processing, pp. 196205, Honolulu, Hawaii.
Callison-Burch, C., Koehn, P., & Osborne, M. (2006). Improved statistical machine translation using paraphrases. Proceedings Human Language Technology Conference
NAACL, Main Conference, pp. 1724, New York City, USA.
Chandrasekar, R., Doran, C., & Srinivas, B. (1996). Motivations Methods Text
Simplification. Proceedings 16th International Conference Computational
Linguistics, pp. 10411044, Copenhagen, Denmark.
Chiang, D. (2007). Hierarchical Phrase-Based Translation. Computational Linguistics,
33 (2), 201228.
Cohn, T., & Lapata, M. (2009). Sentence Compression Tree Transduction. Journal
Artificial Intelligence Research, 34, 637674.
Cohn, T., & Lapata, M. (2013). abstractive approach sentence compression. ACM
Trans. Intell. Syst. Technol., 4 (3), 41:141:35.
Coster, W., & Kauchak, D. (2011). Simple English Wikipedia: New Text Simplification
Task. Proceedings 49th Annual Meeting Association Computational
Linguistics: Human Language Technologies, pp. 665669, Portland, Oregon, USA.
Dowty, D. (1991). Thematic Proto Roles Argument Selection. Language, 67 (3), 547
619.
Eisner, J. (2003). Learning Non-Isomorphic Tree Mappings Machine Translation.
Proceedings ACL Interactive Poster/Demonstration Sessions, pp. 205208, Sapporo, Japan.
160

fiText Rewriting Improves Semantic Role Labeling

Fan, R. E., Chang, K. W., Hsieh, C. J., Wang, X. R., & Lin, C. J. (2008). LIBLINEAR:
Library Large Linear Classification. Journal Machine Learning Research, 9,
18711874.
Feblowitz, D., & Kauchak, D. (2013). Sentence simplification tree transduction.
Proceedings Second Workshop Predicting Improving Text Readability
Target Reader Populations, pp. 110, Sofia, Bulgaria.
Furstenau, H., & Lapata, M. (2012). Semi-supervised semantic role labeling via structural
alignment. Computational Linguistics, 38 (1), 135171.
Galley, M., & McKeown, K. (2007). Lexicalized Markov Grammars Sentence Compression. Proceedings NAACL/HLT, pp. 180187, Rochester, NY.
Ganitkevitch, J., Callison-Burch, C., Napoles, C., & Van Durme, B. (2011). Learning
Sentential Paraphrases Bilingual Parallel Corpora Text-to-Text Generation.
Proceedings 2011 Conference Empirical Methods Natural Language
Processing, pp. 11681179, Edinburgh, Scotland, UK.
Ganitkevitch, J., Cao, Y., Weese, J., Post, M., & Callison-Burch, C. (2012). Joshua 4.0:
Packing, pro, paraphrases. Proceedings Seventh Workshop Statistical
Machine Translation, pp. 283291, Montreal, Canada.
Ganitkevitch, J., Van Durme, B., & Callison-Burch, C. (2013). PPDB: Paraphrase
Database. Proceedings 2013 Conference North American Chapter
Association Computational Linguistics: Human Language Technologies, pp.
758764, Atlanta, Georgia. used prepackaged small constituent syntactic
subset PPDB, retrieved http://paraphrase.org.
Gildea, D., & Jurafsky, D. (2002). Automatic Labeling Semantic Roles. Computational
Linguistics, 28 (3), 245288.
Graehl, J., & Knight, K. (2004). Training Tree Transducers. HLT-NAACL 2004: Main
Proceedings, pp. 105112, Boston, MA.
Hajic, J., Ciaramita, M., Johansson, R., Kawahara, D., Mart, M. A., Marquez, L., Meyers,
A., Nivre, J., Pado, S., Stepanek, J., Stranak, P., Surdeanu, M., Xue, N., & Zhang, Y.
(2009). conll-2009 shared task: Syntactic semantic dependencies multiple
languages. Proceedings Thirteenth Conference Computational Natural
Language Learning (CoNLL 2009): Shared Task, pp. 118, Boulder, Colorado.
Heilman, M., & Smith, N. (2010). Extracting Simplified Statements Factual Question
Generation. Proceedings 3rd Workshop Question Generation, pp. 1120,
Carnegie Mellon University, PA. Software available http://www.ark.cs.cmu.edu/
mheilman/questions/.
Irvine, A., Quirk, C., & Daume III, H. (2013). Monolingual marginal matching translation model adaptation. Proceedings 2013 Conference Empirical Methods
Natural Language Processing, pp. 10771088, Seattle, Washington, USA.
Kauchak, D. (2013). Improving text simplification language modeling using unsimplified
text data. Proceedings 51st Annual Meeting Association Computational Linguistics (Volume 1: Long Papers), pp. 15371546, Sofia, Bulgaria. used
161

fiWoodsend & Lapata

Version 2.0 sentence-aligned corpus, retrieved http://www.cs.middlebury.
edu/~dkauchak/simplification/.
Kauchak, D., & Barzilay, R. (2006). Paraphrasing automatic evaluation. Proceedings
Human Language Technology Conference NAACL, Main Conference, pp.
455462, New York City, USA.
Klebanov, B. B., Knight, K., & Marcu, D. (2004). Text Simplification InformationSeeking Applications. Meersman, R., & Tari, Z. (Eds.), Move Meaningful
Internet Systems 2004: CoopIS, DOA, ODBASE, pp. 735747. Springer Berlin
Heidelberg.
Koehn, P. (2010). Statistical Machine Translation. Cambridge University Press.
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. Proceedings HLT/NAACL, pp. 4854, Edmonton, Canada.
Kundu, G., & Roth, D. (2011). Adapting Text instead Model: Open Domain
Approach. Proceedings Fifteenth Conference Computational Natural Language Learning, pp. 229237, Portland, Oregon, USA.
Kwiatkowski, T. (2012). Probabilistic Grammar Induction Sentences Structured
Meanings. Ph.D. thesis, University Edinburgh.
Lang, J., & Lapata, M. (2011). Unsupervised semantic role induction via split-merge clustering. Proceedings 49th Annual Meeting Association Computational
Linguistics: Human Language Technologies, pp. 11171126, Portland, Oregon, USA.
Liang, P., Taskar, B., & Klein, D. (2006). Alignment Agreement. Proceedings
HLT/NAACL, pp. 104111, New York, NY.
Marton, Y., Callison-Burch, C., & Resnik, P. (2009). Improved statistical machine translation using monolingually-derived paraphrases. Proceedings 2009 Conference
Empirical Methods Natural Language Processing, pp. 381390, Singapore.
Mehdad, Y., Negri, M., & Federico, M. (2010). Towards cross-lingual textual entailment.
Human Language Technologies: 2010 Annual Conference North American
Chapter Association Computational Linguistics, pp. 321324, Los Angeles,
California.
Melli, G., Wang, Y., Liu, Y., Kashani, M. M., Shi, Z., Gu, B., Sarkar, A., & Popowich, F.
(2005). Description SQUASH, SFU Question Answering Summary Handler
DUC-2005 Summarization Task. Proceedings Human Language Technology Conference Conference Empirical Methods Natural Language
Processing Document Understanding Workshop, Vancouver, Canada.
Noreen, E. (1989). Computer-intensive methods testing hypotheses: introduction.
Wiley.
Pado, S. (2006). Users guide sigf: Significance testing approximate randomisation.
Retrieved http://www.nlpado.de/~sebastian/software/sigf.shtml.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). Proposition Bank: Annotated
Corpus Semantic Roles. Computational Linguistics, 31 (1), 71106.
162

fiText Rewriting Improves Semantic Role Labeling

Pang, B., Knight, K., & Marcu, D. (2003). Syntax-based Alignment Multiple Translations:
Extracting Paraphrases Generating New Sentences. Proceedings NAACL,
pp. 181188, Edmonton, Canada.
Park, J. H., Croft, B., & Smith, D. A. (2011). Quasi-synchronous Dependence Model
Information Retrieval. Proceedings 20th ACM International Conference
Information Knowledge Management, pp. 1726, Glasgow, United Kingdom.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,
M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau,
D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning
Python. Journal Machine Learning Research, 12, 28252830.
Shen, D., & Lapata, M. (2007). Using Semantic Roles Improve Question Answering.
Proceedings 2007 Joint Conference Empirical Methods Natural Language
Processing Computational Natural Language Learning (EMNLP-CoNLL), pp. 12
21, Prague, Czech Republic.
Surdeanu, M., Harabagiu, S., Williams, J., & Aarseth, P. (2003). Using Predicate-Argument
Structures Information Extraction. Proceedings Annual Meeting
Association Computational Linguistics, pp. 815, Sapporo, Japan.
Titov, I., & Klementiev, A. (2012). bayesian approach unsupervised semantic role
induction. Proceedings 13th Conference European Chapter
Association Computational Linguistics, pp. 1222, Avignon, France.
Toutanova, K., Haghighi, A., & Manning, C. (2005). Joint learning improves semantic role
labeling. Proceedings 43rd Annual Meeting Association Computational Linguistics (ACL05), pp. 589596, Ann Arbor, Michigan.
Vapnik, V. (1995). Nature Statistical Learning Theory. Springer-Verlag New York,
Inc.
Vickrey, D., & Koller, D. (2008). Sentence simplification semantic role labeling.
Proceedings ACL-08: HLT, pp. 344352, Columbus, Ohio.
Wang, M., & Manning, C. (2010). Probabilistic tree-edit models structured latent
variables textual entailment question answering. Proceedings 23rd
International Conference Computational Linguistics (Coling 2010), pp. 11641172,
Beijing, China.
Wang, M., Smith, N. A., & Mitamura, T. (2007). Jeopardy model? quasisynchronous grammar QA. Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language
Learning (EMNLP-CoNLL), pp. 2232, Prague, Czech Republic.
Woodsend, K., & Lapata, M. (2011). Learning simplify sentences quasi-synchronous
grammar integer programming. Proceedings 2011 Conference Empirical Methods Natural Language Processing, pp. 409420, Edinburgh, Scotland, UK.
used Wikipedia revisions corpus, retrieved http://homepages.inf.ed.
ac.uk/kwoodsen/wiki.html.
Woodsend, K., & Lapata, M. (2012). Multiple aspect summarization using integer linear
programming. Proceedings 2012 Joint Conference Empirical Methods
163

fiWoodsend & Lapata

Natural Language Processing Computational Natural Language Learning, pp.
233243, Jeju Island, Korea.
Wu, D. (1997). Stochastic Inversion Transduction Grammars Bilingual Parsing
Parallel Corpora. Computational Linguistics, 23 (3), 377404.
Wu, D., & Fung, P. (2009). Semantic Roles SMT: Hybrid Two-Pass Model. Proceedings Human Language Technologies: Annual Conference North American
Chapter Association Computational Linguistics, Companion Volume: Short
Papers, pp. 1316, Boulder, Colorado.
Yamada, K., & Knight, K. (2001). Syntax-based Statistical Translation Model. Proceedings 39th Annual Meeting Association Computational Linguistics,
pp. 523530, Toulouse, France.
Yamangil, E., & Nelken, R. (2008). Mining Wikipedia revision histories improving
sentence compression. Proceedings ACL-08: HLT, Short Papers, pp. 137140,
Columbus, Ohio.
Zanzotto, F. M., & Pennacchiotti, M. (2010). Expanding textual entailment corpora
fromwikipedia using co-training. Proceedings 2nd Workshop Peoples Web Meets NLP: Collaboratively Constructed Semantic Resources, pp. 2836,
Beijing, China. Coling 2010 Organizing Committee.
Zhu, Z., Bernhard, D., & Gurevych, I. (2010). Monolingual Tree-based Translation Model
Sentence Simplification. Proceedings 23rd International Conference
Computational Linguistics, pp. 13531361, Beijing, China.

164

fiJournal Artificial Intelligence Research 51 (2014) 227-254

Submitted 04/14; published 09/14

Entrenchment-Based Horn Contraction
Zhiqiang Zhuang

z.zhuang@griffith.edu.au

Institute Integrated Intelligent Systems
Griffith University, QLD 4111, Australia

Maurice Pagnucco

morri@cse.unsw.edu.au

School Computer Science Engineering
University New South Wales, NSW 2052, Australia

Abstract
AGM framework benchmark approach belief change. Since framework
assumes underlying logic containing classical Propositional Logic, applied
systems logic weaker Propositional Logic. remedy limitation, several
researchers studied AGM-style contraction revision Horn fragment
Propositional Logic (i.e., Horn logic). paper, contribute line research
investigating Horn version AGM entrenchment-based contraction. study
challenging construction entrenchment-based contraction refers arbitrary disjunctions expressible Horn logic. order adapt construction
Horn logic, make use Horn approximation technique called Horn strengthening.
provide representation theorem newly constructed contraction refer
entrenchment-based Horn contraction. Ideally, contractions defined Horn logic (i.e.,
Horn contractions) rational AGM contraction. propose notion
Horn equivalence intuitively captures equivalence Horn contraction
AGM contraction. show that, notion, entrenchment-based Horn contraction
equivalent restricted form entrenchment-based contraction.

1. Introduction
Given agent set beliefs, theory belief change deals agent
changes beliefs rational manner confronted new information. Two
kinds changes mainly studied, namely contraction revision, removing old
beliefs incorporating new beliefs respectively. main strategies studying
belief change articulate principles called rationality postulates rational agents
obey contracting revising sets beliefs specify explicit change
mechanisms called construction methods contraction revision operation.
dominant theory belief change called AGM framework (Alchourron,
Gardenfors, & Makinson, 1985; Gardenfors, 1988). framework specify specific underlying logic, however, assumes logic contains classical Propositional
Logic. commonly accepted AGM framework provides best set rationality postulates capturing intuitions behind rational belief change along well
motivated construction methods characterised rationality postulates.
Regardless desirable properties, assumption underlying logic severe
limitation.
Tractable fragments Propositional Logic non-classic logics Description Logics (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003) particuc
2014
AI Access Foundation. rights reserved.

fiZhuang & Pagnucco

larly useful artificial intelligence applications allow efficient reasoning methods.
Since knowledge evolves, systems based logics subject change. However, fragments Propositional Logic Description Logics subsume Propositional Logic,
thus AGM framework applied systems. Consequently, extensive
attention paid problem belief change fragments Propositional
Logic Description Logics. recent trend focuses Horn fragment Propositional Logic (i.e., Horn Logic) found extensive usage artificial intelligence
database systems. paper contribute Horn belief change thoroughly investigating Horn version entrenchment-based contraction (Gardenfors & Makinson, 1988;
Gardenfors, 1988) refer entrenchment-based Horn contraction.
Entrenchment-based contraction based rankings formulas called epistemic entrenchments. general idea formulas entrenched ranking
preferred less entrenched and, deciding formulas give contraction; intuitive give less preferred beliefs. obvious obstacle adapting
entrenchment-based contraction Horn logic standard construction refers
disjunctions may non-Horn formulas. Therefore, apply construction method directly Horn logic. get around expressiveness problem
propose replace non-Horn formulas Horn approximations (Kautz & Selman, 1996). contraction thus constructed satisfies Horn versions characterising
postulates entrenchment-based contraction, except Recovery postulate (Alchourron
et al., 1985). Due limited expressiveness Horn logic, entrenchment-based Horn
contraction comprehensive entrenchment-based contraction. Therefore, characterising entrenchment-based Horn contraction, extra postulate needed capture
restricted (compared entrenchment-based contraction).
Ideally, Horn contraction perform rationally AGM contraction. evaluate rationality Horn contractions AGM contraction, propose notion
Horn equivalence formalises equivalence Horn contraction functions
AGM contraction functions constructive point view. Horn contractions
restricted Horn formulas, thus fair say Horn contraction function performs
rationally AGM one change Horn formulas result AGM
contraction function identical incurred Horn contraction function. Put
simply, Horn contraction function Horn equivalent AGM counterpart behave identically terms Horn formulas. words, Horn equivalence implies
change mechanism AGM contraction function exactly
Horn contraction function, thus latter preserves every property former.
able identify restricted form entrenchment-based contraction one-to-one
correspondence entrenchment-based Horn contraction means Horn equivalence.
Due contention Recovery postulate, Makinson introduced idea
Withdrawal function encompasses broader class belief contraction functions.
AGM setting, Rott Pagnucco (1999) explored severe withdrawal, operation
similar contraction defined intentionally violate Recovery postulate. propose construction severe withdrawal based epistemic entrenchment
refer entrenchment-based withdrawal. Different entrenchment-based contraction, construction refer arbitrary disjunctions. Moreover, see,
Horn contractions intrinsically incompatible Recovery, thus curious see
228

fiEntrenchment-Based Horn Contraction

entrenchment-based withdrawal seamlessly transferable Horn logic. investigating Horn version entrenchment-based withdrawal give affirmative
answer.
rest paper organised follows. first give technical preliminaries
Section 2, Section 3 recall details entrenchment-based contraction.
Section 4, introduce entrenchment-based Horn contraction present representation theorem. Section 5, compare entrenchment-based Horn contraction
entrenchment-based contraction notion Horn equivalence. Section 6
introduce entrenchment-based Horn withdrawal demonstrate close connection
entrenchment-based withdrawal. Finally, related work conclusions given Section 7 Section 8 respectively. Proofs given appendix. paper revised
extended version (Zhuang & Pagnucco, 2010).

2. Technical Preliminaries
assume propositional language L finite set atoms P closed
usual truth-functional connectives contains propositional constants > (truth)
(falsum). Atoms denoted lower case Roman letters (p, q, . . .). Formulas
denoted lower case Greek letters (, , . . .). Sets formulas denoted upper case
Roman letters (V, X, . . .).
logic generated L specified standard Tarskian consequence operator
Cn. set formulas X, Cn(X) denotes set formulas following logically
X. formula , Cn() abbreviates Cn({}). sometimes write X ` denote
Cn(X), denote Cn() = Cn(), ` denote Cn(). letter
K reserved represent theory belief set set formulas
K = Cn(K). Standard propositional semantics assumed. interpretation model
formula true . set formulas X, |X| denotes set models
X. formula , || abbreviates |{}|.
clause disjunction positive negative atoms. Horn clause clause
contains one positive atom, e.g. b c. Horn formula conjunction
Horn clauses. Horn language LH maximal subset L containing Horn
formulas. Horn logic generated LH specified consequence operator CnH
that, set Horn formulas X, CnH (X) = Cn(X) LH . letter H reserved
represent Horn theory Horn belief set set Horn formulas
H = CnH (H). Horn subset function H : 2L 2LH given set formulas
X, H(X) set Horn formulas X. Formally, H(X) = LH X.

3. Entrenchment-Based Contraction
.
AGM contraction function takes input belief set K formula returns
.
another belief set K . refer K original belief set, contracting
.
formula, K resulting belief set. Various constructions proposed
contraction AGM framework. section, review classical one called
entrenchment-based contraction (Gardenfors, 1988; Gardenfors & Makinson, 1988).
229

fiZhuang & Pagnucco

beliefs held agent equal terms epistemological importance.
work Gardenfors (1988) Gardenfors Makinson (1988), important
beliefs said entrenched others. idea behind entrenchmentbased contraction that, contraction, give less entrenched formulas
whenever possible. relative entrenchments formulas modeled epistemic
entrenchments. Given belief set K, epistemic entrenchment associated K
binary relation L means least entrenched .
strict relation < defined 6 . Importantly, satisfies following
conditions:
(EE1)
(EE2)
(EE3)
(EE4)
(EE5)

,
` ,

K 6`, 6 K iff every
every , `

(Transitivity)
(Dominance)
(Conjunctiveness)
(Minimality)
(Maximality)

Thus epistemic entrenchment transitive relation (EE1) logically stronger
formulas entrenched weaker ones (EE2), conjunction equally
entrenched one conjuncts (EE3), non-beliefs least entrenched (EE4),
tautologies entrenched (EE5).
Entrenchment-based contraction defined two conditions establish
connections epistemic entrenchments contraction functions. Condition (C )
generates epistemic entrenchment contraction function. motivation that,
contraction K , give either (or both), however,
intuitive give epistemically less important one. Thus, retracted,
must case least entrenched . limiting case
tautologies, equally important and, (EE5), required maximally
entrenched.
.
(C ) : iff 6 K ` .
.
Condition (C ) derives contraction function epistemic entrenchment. Accord.
ing (C ), retained contraction originally believed
(i.e., K) either sufficient evidence retaining (i.e., < )
possible remove (i.e., ` ).
.
.
(C ) : K iff K either < ` .
.
contraction function generated via (C ) referred entrenchment-based
contraction function.
.
Definition 1 (Gardenfors & Makinson, 1988) function entrenchment-based contraction function K iff outcome determined epistemic entrenchment K
.
via (C ).
pointed work Gardenfors Makinson (1988), rather difficult
.
motivate (C ), however, appropriateness justified representation theorem
entrenchment-based contraction.
230

fiEntrenchment-Based Horn Contraction

.
Theorem 1 (Gardenfors & Makinson, 1988) function entrenchment-based con.
traction function iff satisfies following postulates:
.
. = Cn(K )
.
(K 1)
K
(Closure)
.
.
(K 2) K K
(Inclusion)
.
. =K
(K 3)
6 K, K
(Vacuity)
.
.
(K 4) 6` , 6 K
(Success)
.
. +
(K 5)
K (K )
(Recovery)
.
. = K
.
(K 6)
, K
(Extensionality)
.
.
.
.
(K 7) K K K
(Conjunctive Overlap)
.
. K
. K
.
(K 8)
6 K
(Conjunctive Inclusion)

Theorem 1, entrenchment-based contraction functions characterised full set
.
.
.
.
AGM postulates contraction (K 1)(K 8). AGM tradition, (K 1)(K 6)
.
.
referred basic postulates (K 7) (K 8) referred supplementary postulates. According basic postulates, contraction produces belief set
.
.
(K 1) contain contracting formula unless tautology (K 4).
.
produced belief set larger original one (K 2). contracting formula
.
believed, nothing done (K 3). contraction syntax-insensitive
.
(K 6) contracted formula added back contracted belief set,
.
.
result entails every formula original belief set (K 5). (K 5), often called
Recovery, controversial subject much discussion (e.g., Makinson, 1987;
Hansson, 1991; Levi, 1991). example work Hansson (1991), argued
emerging property rather fundamental postulate contraction. supplementary
postulates concern relations contraction conjunction contractions
constituent conjuncts. Formulas surviving contraction conjunct also survive
.
contraction conjunction (K 7). conjunct removed contraction
conjunction, formulas surviving contraction conjunction also survive
.
contraction removed conjunct (K 8).
.
postulates equivalent (K 7) proposed essential proving
representation theorems AGM contractions. presence basic postulates,
.
(K 7) equivalent postulate Partial Antitony (Alchourron et al., 1985)
.
.
.
(K pa) K Cn() K ( )
postulate Conjunctive Trisection (Rott, 1992; Hansson, 1993).
.
.
.
(K ct) K ( ) K ( )
.
.
(K pa) rather technical nature, however, (K ct) well motivated. Informally
speaking, says preferred belief (i.e., ) pair (i.e., , ) least preferred
third belief (i.e., ) added pair. contraction, rational discard
less entrenched formulas whenever possible, thus retained contracting
means entrenched . Since entrenched ,
least entrenched one among , , . Therefore, retained contracting
. words, preferences cannot changed beliefs considered.
231

fiZhuang & Pagnucco

4. Entrenchment-Based Horn Contraction
Although entrenchment-based contraction defined assuming Propositional Logic,
intuition behind construction universal applied fragments Propositional Logic. section, apply intuition Horn fragment.
4.1 Horn Strengthening
Standard entrenchment-based contraction makes use disjunctive formula condi.
tion (C ). adapting construction method Horn logic, immediate problem
disjunctive formula may non-Horn. cases, propose replace
non-Horn formula Horn approximations, precisely, Horn strengthenings
(Kautz & Selman, 1996). notion Horn strengthening proposed Kautz
Selman context knowledge compilation. original definition clauses
sets clauses Horn strengthening clause C logically weakest Horn
clause entails C Horn strengthening set clauses {C1 , . . . , Cn } set
Horn clauses {C10 , . . . , Cn0 } Ci0 Horn strengthening Ci . reformulate
definition cover arbitrary formulas that, formula , Horn strengthening
logically weakest Horn formula entails .
Definition 2 Let formula. set Horn strengthenings , denoted HS(),
HS() iff
1. LH ;
2. || ||; and,
3. 0 LH || |0 | ||.
According Definition 2, Horn formula one Horn strengthening, namely itself.
limiting case tautology, assume single Horn strengthening itself.
following results provide new properties Horn strengthenings helpful
paper. Firstly, since Definition 2 model-theoretic, set Horn strengthenings
logically equivalent formulas identical.
Lemma 1 , HS() = HS().
Horn formula entails another formula necessarily Horn, must
also entail Horn strengthening .
Lemma 2 Horn formula ` , HS()
` .
Horn strengthening conjunction formed conjoining Horn strengthenings conjunct.
Lemma 3 HS( ), 1 HS() 2 HS()
1 2 .
232

fiEntrenchment-Based Horn Contraction

Horn strengthening Horn formulas, every Horn
strengthening Horn strengthening .
Lemma 4 Let Horn formulas. HS( ), HS( ) HS( ).
4.2 Construction
constructing Horn version entrenchment-based contraction function, also
need relation captures relative importance formulas. Since, Horn belief
change, formulas restricted Horn fragment Propositional Logic, relation
Horn formulas. define Horn epistemic entrenchment binary relation LH
satisfies (HEE1)(HEE5):
(HEE1)
(HEE2)
(HEE3)
(HEE4)
(HEE5)

,
` ,

H 6`, 6 H iff every
every , `

(HEE1)(HEE5) simply (EE1)(EE5) restricted Horn formulas Horn belief
sets. derive following properties Horn epistemic entrenchment.
Lemma 5 Let Horn epistemic entrenchment. satisfies (cf. (Foo, 1990)):
1.
2. ,
3. < iff <
4. ,
5. < < , <
6. ,
7.
Particularly, Horn epistemic entrenchment connected (Item 1) logically
equivalent formulas equally entrenched (Item 7).
Clearly, obtain Horn epistemic entrenchment epistemic entrenchment
simply removing entrenchment relations involving non-Horn formulas. obtained
Horn epistemic entrenchment called Horn subset epistemic entrenchment.
Definition 3 Let H Horn epistemic entrenchment P epistemic entrenchment.
H Horn subset P iff
P iff H
, LH .
233

fiZhuang & Pagnucco

Notice epistemic entrenchment one Horn subset different epistemic entrenchments may Horn subset. latter case, epistemic entrenchments
identical respect entrenchments Horn formulas different
respect involving non-Horn formulas.
.
Condition (C ) central determining outcome entrenchment-based contrac.
tion functions. According (C ), contraction K , K,
disjunction strictly entrenched sufficient condition retaining
.
. Since arbitrary disjunction, may Horn formula. Therefore, (C )
applicable Horn logic.
forming similar condition Horn contraction, replace non-Horn disjunctions
Horn strengthenings results following condition:
.
.
(HC ): H iff H either ` HS( ) < .
.
According (HC ), contraction H , H, existence one
Horn strengthening strictly entrenched sufficient condition
.
retaining . Similar (C ), another sufficient condition tautology. Since
logically weaker Horn strengthenings, epistemic entrenchment
(EE2) implies equal entrenched Horn strengthenings. Thus
(EE1) Horn strengthening strictly entrenched ,
.
. converse, however, hold general. So, informally speaking, (HC )
.
stricter condition (C ) retaining formulas contraction. refer
.
Horn contraction function generated via (HC ) entrenchment-based Horn contraction
function.
.
Definition 4 function entrenchment-based Horn contraction function H iff
.
outcome determined Horn epistemic entrenchment H via (HC ).

p q r
p q r
p r
p q

p r q
q r
H

p q r
p r p q r
p q
p r q
q p r q r
P1

p q r
p r
p q
p r q
q p r q r
P2

Figure 1: Entrenchment based Horn contraction function entrenchment based contraction functions determined respectively H , p1 P2 H
Horn subset P1 P2 .
Figure 1 demonstrates contraction H = CnH ({p q, q r}) p r
.
contraction determined Horn epistemic entrenchment H via (HC )
contraction K = Cn(H) p r contraction determined respectively
.
P1 P2 via (C ). Notice H Horn subset P1 P2 .
234

fiEntrenchment-Based Horn Contraction

rectangles illustrate formulas Horn belief set H belief set K along
entrenchments. Formulas level rectangle equally entrenched.
Formulas level higher strictly entrenched level lower. Nonbeliefs, tautologies conjunctions shown entrenchments uniquely
determined formulas shown. shaded formulas retained contraction.
.
Lets examine fate p q Horn contraction determined H via (HC ).
disjunction p q p r (i.e., p q r) two Horn strengthenings p
r p q. Since Horn strengthenings equally less entrenched p
.
r, p q discarded. contractions determined P1 P2 via (C ),
since p q r allowed epistemic entrenchments retention p q
determined entrenchment p q r compared p r. Observe
contraction determined P1 retains Horn formulas Horn contraction
determined P2 retains more. Section 5, identify entrenchmentbased contraction functions always retain Horn formulas corresponding
entrenchment-based Horn contraction function.
Condition (C ) concerns generation epistemic entrenchment contraction function. Horn version used generating Horn epistemic entrenchment
Horn contraction function. identical (C ) condition restricted Horn
formulas.
.
(HC ): iff 6 H ` .
4.3 Characterisation
giving representation theorem entrenchment-based Horn contraction, let us
consider following postulates.
.
.
.
(H 1) H = CnH (H )
.
.
(H 2) H H
.
.
(H 3) 6 H, H = H
.
.
(H 4) 6` , 6 H
.
.
.
(H de) H 6 H , HS( ), 6 H
.
.
.
(H wr) H 6 H , H 0 H H 0 , 6 CnH (H 0 )
CnH (H 0 {})
.
.
.
(H 6) , H = H
.
.
(H f ) ` , H = H
.
.
.
(H hs) H , HS( ) H
.
.
.
(H pa) (H ) CnH () H
.
.
.
(H ct) H , H
235

fiZhuang & Pagnucco

.
.
.
.
(H 7) H H H
.
.
.
.
(H 8) 6 H , H H
.
.
postulates well known belief change literature. (H 1)(H 4)
.
.
(H 6)(H 8) Horn versions AGM postulates Closure, Inclusion, Vacuity, Suc.
cess, Extensionality, Conjunctive Overlap Conjunctive Inclusion respectively. (H f )
Horn version Failure postulate (Fuhrmann & Hansson, 1994).
.
(H de) Horn version Disjunctive Elimination postulate (Ferme, Krevneris,
& Reis, 2008):1
.
.
.
(K de) K 6 K , 6 K
.
(K de) captures minimal change properties contraction; contrapositive form
.
.
K K , K
postulate condition sentence survive contraction process (Ferme
et al., 2008, p. 745). Combining existing results (Ferme et al., 2008; Hansson, 1991),
.
.
.
.
get (K de) implies (K 5) presence (K 2) (K 3). reverse
.
also true show contrapositive (K de) follows Recovery.2
.
.
.
.
Thus (K de) equivalent (K 5). (H de) obtained (K de) replacing
possibly non-Horn disjunction Horn strengthenings. deals removal
formulas regard related Horn strengthenings that, removed
.
contracting , Horn strengthenings . Interestingly, (H wr)
proposed work Delgrande Wassermann (2010) characterising partial meet
.
.
Horn contraction equivalent (H de) presence (H 2). Also, presence
.
.
.
(H 2), (H f ) follows (H de).
.
Horn Strengthening postulate (H hs) counterpart classic belief change.
entrenchment-based contraction, formula retained contracting ,
strictly entrenched means, accordance (C ),
.
K ( ). Thus property expressed as:
.
.
K , K ( ).
property however postulated explicitly deducible
.
.
.
(K 6) (K 1). Since ( ) logically equivalent , (K 6)
.
.
.
.
K ( ) = K . K ( ) follows fact K
.
K logically closed. entrenchment-based Horn contraction, Horn formula
retained contracting Horn formula , must Horn strengthening
strictly entrenched means, accordance (HC ),
.
.
H . property captured exactly (H hs). time property
.
deducible postulates thus postulate explicitly. Notice (H hs)
fact Horn adaptation retention property classic case replacing
disjunction Horn strengthenings.
1. Disjunctive Elimination originally proposed context belief base change. adapted
belief set change here.
.
.
2. K, Recovery ensures K . Hence also K ,
.
K .

236

fiEntrenchment-Based Horn Contraction

.
.
(H ct) (H pa) Horn versions Conjunctive Trisection Partial Antitony.
.
.
.
classical case, (H pa) (H ct) equivalent presence (H 6).
.
.
(K pa) (K 7) equivalent. Since proof relies Recovery available
.
Horn logic, establish equivalence Horn analogues (H pa)
.
(H 7). Proposition 1 summarises connections postulates.
.
Proposition 1 Let contraction function. Then:
.
.
.
.
1. satisfies (H 1) (H de), satisfies (H f );
.
.
.
.
2. satisfies (H 2) (H de), satisfies (H wr);
.
.
.
3. satisfies (H wr), satisfies (H de);
.
.
.
4. satisfies (H pa), satisfies (H ct); and,
.
.
.
.
5. satisfies (H 6) (H ct), satisfies (H pa).
Now, give representation theorem entrenchment-based Horn contraction.
.
Theorem 2 function entrenchment-based Horn contraction function iff satis.
.
.
.
.
.
.
fies (H 1)(H 4), (H de), (H 6), (H hs), (H ct), (H 8).
Comparing characterisation entrenchment-based contraction, Recovery
.
.
.
Conjunctive Overlap appear instead (H de), (H hs), (H ct) used.
Lets go new postulates. condition decomposability proposed
work Flouris, Plexousakis, Antoniou (2004) characterises logics admit
contraction functions satisfy Recovery. Langlois, Sloan, Szorenyi, Turan (2008)
verified Horn logic decomposable, thus Horn contraction function
.
satisfies Recovery. Here, (H de) plays similar role Recovery capturing minimal
.
change property entrenchment-based Horn contraction. mentioned, (H 7)
.
.
(H ct) equivalent Horn case. turns instead (H 7),
.
property (H ct) needed characterising entrenchment-based Horn contraction.
.
.
.
Since (HC ) is, sense, stricter (C ), need (H hs) capture extra
strictness entrenchment-based Horn contraction.
Besides characterising postulates, show entrenchment-based Horn con.
traction functions satisfy (H 7).
.
Proposition 2 entrenchment-based Horn contraction function, satisfies
.
(H 7).
.
.
.
.
.
.
.
Theorem 2, (H 1)(H 4), (H de), (H 6), (H hs), (H ct), (H 8) characterise entrenchment-based Horn contraction functions. Thus follows immediately
.
Proposition 2 (H 7) follows postulates.
.
.
.
.
.
Corollary 1 Let Horn contraction function. satisfies (H 1)(H 4), (H de),
.
.
.
.
.
(H 6), (H hs), (H ct), (H 8), satisfies (H 7).
237

fiZhuang & Pagnucco

Besides representation results, another property entrenchment-based Horn contraction deserves mentioning uniqueness. Horn epistemic entrenchment
determines unique entrenchment-based Horn contraction function. Specifically, given
two distinct Horn epistemic entrenchments 1 2 H two entrenchment.
.
based Horn contraction functions determine 1 2 , formula
.
.
H 1 6= H 2 .
.
Theorem 3 Let 1 2 two different Horn epistemic entrenchments let 1
.
2 entrenchment-based Horn contraction functions determined 1 2
.
.
.
respectively via condition (HC ). 1 2 identical.
Uniqueness property AGM contraction. shown (Alchourron et al., 1985) that,
finite case, selection function determines unique partial meet contraction function (OBSERVATION 4.6). desirable property captures intuition
two agents different preferences certain domain different contraction
outcomes. Notice uniqueness immediate property Horn contraction
revision. fact, existing Horn contractions Horn revisions enjoy
property, instance transitively relational partial meet Horn contraction (Zhuang
& Pagnucco, 2011), model-based Horn contraction (Zhuang & Pagnucco, 2012),
model-based Horn revision (Delgrande & Peppas, 2011).

5. Connections Entrenchment-Based Contraction
seen entrenchment-based contraction adapted naturally Horn logic,
results entrenchment-based Horn contraction. well entrenchment-based
Horn contraction perform comparison classic counterpart? section,
introduce notion Horn equivalence Horn contraction compared
AGM contraction.
properly defined Horn contraction rational AGM contraction. Horn
contraction restrictive AGM contraction deals Horn formulas.
reasonable consider Horn formulas comparing Horn contraction
AGM contraction. claim Horn contraction rational AGM one
perform identically terms Horn formulas. forms intuition behind Horn
equivalence.
evaluating possible equivalence two contraction functions, makes sense
start identical belief set check effect two functions contracting
formula. intuition fact Horn contraction function
permits Horn formulas, Horn equivalence defined pairs AGM contraction function
.
.
belief set K Horn contraction function H Horn belief set H H
.
Horn subset K. Consider set Horn formulas resulting belief set
contracting Horn formula, set returned Horn contraction function
.
.
.
H contracting Horn formula, say H Horn equivalent.
.
Definition 5 Let K belief set H Horn belief set H = H(K). Let
.
AGM contraction function K H Horn contraction function H.
238

fiEntrenchment-Based Horn Contraction

.
.
H Horn equivalent iff
.
.
H(K ) = H H
LH .
.
.
depicted Figure 2, Horn equivalence H stems fact that,
.
.
contracting Horn formula , Horn belief set H H returned H Horn
.
.
subset belief set K returned .
K

.


.
H(K )

H(K)

H

.
K

.
H

.
H H

.
Figure 2: Horn equivalence AGM contraction function Horn con.
traction function H .
Obviously, entrenchment-based Horn contraction function Horn equivalent
entrenchment-based contraction function, determining Horn epistemic entrenchment must Horn subset determining epistemic entrenchment. However,
sufficient guarantee Horn equivalence. shown Figure 1, although Horn
epistemic entrenchment H Horn subset epistemic entrenchment P2 ,
entrenchment-based contraction function determined P2 retains Horn formulas
entrenchment-based Horn contraction function determined H .
Figure 1, Horn formulas equally entrenched H , P1 P2 . However,
non-Horn formula p q r, allowed H , entrenched differently P1
P2 . extra preference information non-Horn formula, P1 P2
give rise two different entrenchment-based contraction functions. point
Horn epistemic entrenchment consistent several epistemic entrenchments (i.e.,
Horn subset), thus entrenchment-based Horn contraction function corresponds
several entrenchment-based contraction functions. entrenchment-based contraction functions Horn equivalent entrenchment-based Horn contraction function?
Essentially, requires us identify epistemic entrenchments determine
Horn equivalent entrenchment-based contraction functions. show, following
necessary sufficient condition purpose:
(EE6) , LH , K < HS( )
< .
condition requires that, pairs Horn formulas , K, strictly less
entrenched , also strictly less entrenched Horn strengthening
239

fiZhuang & Pagnucco

. Since formula entailed Horn strengthenings, verified
(EE1) (EE2) converse also true. Thus strictly less entrenched
strictly less entrenched Horn strengthening .
.
principal cases, decide whether retain contraction , (C )
.
compares entrenchment whereas (HC ) compares entrenchment
Horn strengthenings . (EE6) assures comparing
comparing Horn strengthenings . means, entrenchment
.
satisfies (EE6), mechanism retaining formulas (C ) essentially
.
(HC ).
call entrenchment-based contraction functions whose determining epistemic entrenchments satisfy (EE6) strict entrenchment-based contraction functions.
.
Definition 6 function strict entrenchment-based contraction function
entrenchment-based contraction function whose determining epistemic entrenchment satisfies (EE6).
characterise strict entrenchment-based contraction functions need, addition
.
.
.
(K 1)(K 8), classic case (H hs):
.
.
.
(K hs) , LH , K HS( ) K .
.
fact, (EE6) corresponds exactly (K hs).
.
.
Theorem 4 function strict entrenchment-based contraction function iff satisfies
.
.
.
(K 1)(K 8) (K hs).
main result section, entrenchment-based Horn contraction function
Horn equivalent strict entrenchment-based contraction function vice versa.
.
Theorem 5 H entrenchment-based Horn contraction function,
.
.
.
strict entrenchment-based contraction function H Horn equivalent.
.
strict entrenchment-based contraction function, entrenchment.
.
.
based Horn contraction function H H Horn equivalent.
Moreover, entrenchment-based contraction function Horn equivalent
entrenchment-based Horn contraction function strict entrenchment-based contraction
function.
.
.
Theorem 6 Let H entrenchment-based Horn contraction function.
.
.
entrenchment-based contraction function H Horn equivalent,
.
strict entrenchment-based contraction function.
Horn equivalence local feature deals equivalence specific
contraction functions. number functions constructed according particular
construction method contraction function represents possible way contracting
formula. Thus also compare Horn contraction AGM contraction terms
possible ways contracting formula. Horn contraction comprehensive
AGM contraction Horn contraction permits possible ways contracting Horn
240

fiEntrenchment-Based Horn Contraction

formula AGM does. Due allowance non-Horn formulas, ways
forming epistemic entrenchments Horn epistemic entrenchment; thus ways
generating entrenchment-based contraction functions. Figure 1 entrenchmentbased Horn contraction function corresponds (i.e., Horn equivalent to)
entrenchment-based contraction function determined P2 . Entrenchment-based Horn
contraction therefore comprehensive entrenchment-based contraction. However,
due Horn contraction defined due limited expressiveness
Horn logic compared Propositional Logic. Entrenchment-based Horn contraction
equally comprehensive strict entrenchment-based contraction.

6. Entrenchment-Based Horn Withdrawal
Due controversy Recovery postulate AGM setting, Makinson (1987)
proposed operation called withdrawal satisfies five basic AGM contraction
postulates necessarily Recovery. Since Horn contraction intrinsically incompatible
Recovery, makes sense explore Horn version withdrawal.
Since Horn contraction originates entrenchment-based contraction, focus
withdrawal version entrenchment-based contraction. withdrawal characterised Rott Pagnucco (1999) referred severe withdrawal. uniformity,
call entrenchment-based withdrawal. epistemic entrenchment also assumed
entrenchment-based withdrawal withdrawal outcome determined following
condition:
.
.
(W ) : K iff K either < ` .
.
Definition 7 (Rott & Pagnucco, 1999) function entrenchment-based withdrawal
.
function K iff outcome determined epistemic entrenchment K via (W ).
.
Notice (W ) also used generate epistemic entrenchment withdrawal
function as, reverse reading, strictly entrenched whenever
retained withdrawal .
Rott Pagnucco (1999) showed characterise entrenchment-based withdrawal
need full set AGM contraction postulates except Recovery also need
replace Conjunctive Overlap much stronger Antitony Condition.
.
Theorem 7 (Rott & Pagnucco, 1999) function entrenchment-based withdrawal
.
.
.
.
.
function iff satisfies following postulates: (K 1)(K 4), (K 6), (K 8),
.
. K
. (Antitony Condition)
(K 7a)
6` , K
Adapting entrenchment-based withdrawal Horn logic straightforward everything
Horn expressible. obviously need Horn epistemic entrenchment. restricting
.
Horn belief sets Horn formulas (W ) recast follows:
.
.
(HW ) : H iff H either < ` .
.
Horn withdrawal function generated via (HW ) referred entrenchment-based
Horn withdrawal function.
241

fiZhuang & Pagnucco

.
Definition 8 function entrenchment-based Horn withdrawal function H iff
.
outcome determined Horn epistemic entrenchment H via (HW ).
obvious constructions that, unlike entrenchment-based Horn contraction,
entrenchment-based Horn withdrawal change mechanism origin.
Therefore, entrenchment-based Horn withdrawal characterised exactly Horn versions
characterising postulates entrenchment-based withdrawal.
.
.
Theorem 8 function entrenchment-based Horn withdrawal function iff satis.
.
.
.
fies following postulates: (H 1)(H 4), (H 6), (H 8),
.
.
.
(H 7a) 6` H H .
show entrenchment-based Horn withdrawal entrenchment-based withdrawal one-to-one correspondence notion Horn equivalence.
.
Theorem 9 H entrenchment-based Horn withdrawal function,
.
.
.
.
entrenchment-based withdrawal function H Horn equivalent.
entrenchment-based withdrawal function, entrenchment-based Horn
.
.
.
withdrawal function H H Horn equivalent.

7. Related Work
existing approaches Horn contraction different Horn variants partial
meet contraction (Alchourron et al., 1985). notion remainder sets central
construction partial meet contraction outcome contraction
intersection candidate remainder sets chosen selection function. standard
definition, remainder set K respect maximal subset K fails
imply . maximal nature implies following model-theoretic behaviour remainder
sets (e.g., see Gardenfors 1998, p. 86). set models remainder set
union set models K model . Furthermore, bijection
remainder sets models . Another property often
referred convexity property states belief set subset outcome
maxichoice contraction superset outcome full meet contraction
outcome partial meet contraction. properties behaviours however
mutually exclusive remainder sets Horn logic, gives rise three Horn
variants remainder sets thus three Horn variants partial meet contraction. avoid
confusion, remainder sets Propositional Logic referred classic remainder
sets.
first work Horn contraction Delgrande (2008). seminal paper,
Delgrande studied Horn analogue orderly maxichoice contraction (Gardenfors, 1988)
term orderly maxichoice Horn contraction. remainder sets defined
standard definition means maximal, thus call maximal
remainder set. orderly maxichoice contraction, transitive antisymmetric relation
maximal remainder sets assumed selecting single best remainder set
contraction outcome.
242

fiEntrenchment-Based Horn Contraction

Booth, Meyer, Varzinczak (2009) suggested that, orderly maxichoice Horn
contractions appropriate choices Horn contraction, constitute
appropriate forms Horn contraction. authors argued Horn contraction
satisfy convexity property. demonstrated, case Horn contractions based maximal remainder sets. proposed infra Horn contraction (Booth,
Meyer, Varzinczak, & Wassermann, 2010, 2011). Infra Horn contraction based
notion infra remainder set. set infra remainder sets H respect Horn
formula consist Horn belief set subset maximal remainder set
superset intersection maximal remainder sets . notion
infra remainder set subsumes maximal remainder set maximal remainder
set infra one vice versa.
noticed Delgrande Wassermann (2010, 2013), maximal infra remainder sets exhibit model-theoretic behaviour classic remainder sets.
therefore give another Horn variant called partial meet Horn contraction. based
notion weak remainder set defined behave classic remainder sets
model-theoretically. show weak remainder sets subsume maximal remainder sets
weak remainder sets infra remainder sets subsume one another. Thus partial
meet Horn contraction infra Horn contraction comprehensive one
another. nice thing weak remainder sets one-to-one correspondence classic remainder sets implies one-to-one correspondence
partial meet Horn contraction partial meet contraction. means contractions
permitted partial meet Horn contraction correspond exactly permitted partial meet contraction. Zhuang Pagnucco (2011) studied partial meet Horn
contraction assuming preorder weak remainder sets. results construction transitively relational partial meet Horn contraction Horn analogue
transitively relational partial meet contraction (Alchourron et al., 1985).
Apart partial meet contraction, Zhuang Pagnucco (2012) studied Horn
analogue model-based contraction (Katsuno & Mendelzon, 1992) called modelbased Horn contraction. classic case (Katsuno & Mendelzon, 1992), pre-order
interpretations assumed. shown model-based Horn contraction identical
transitively relational partial meet contraction.
Partial meet Horn contraction infra Horn contraction differ entrenchmentbased Horn contraction assume explicit preference information.
Zhuang Pagnucco (2012) showed entrenchment-based Horn contraction restricted form model-based Horn contraction thus restricted form transitively relational partial meet Horn contraction partial meet Horn contraction. exact relationship established entrenchment-based Horn contraction infra Horn
.
contraction (H hs) incompatible infra Horn contraction one characterising postulate infra Horn contraction (i.e., Core-Retainment) incompatible
entrenchment-based Horn contraction. similar reasons exact relationship
established entrenchment-based Horn contraction orderly maxichoice Horn
contraction.
shown Section 5, entrenchment-based Horn contraction comprehensive
entrenchment-based contraction. respect, partial meet Horn contraction
transitively relational partial meet Horn contraction outperform entrenchment-based Horn
243

fiZhuang & Pagnucco

contraction bijections exist partial meet Horn contractions partial meet
contractions transitively relational partial meet Horn contraction transitively relational partial meet contraction. However, bijection comes price. Zhuang
Pagnucco (2011) showed Horn logic expressive enough Horn contraction
distinguish different pre-orders weak remainder sets. different pre-orders
may generate identical transitively relational partial meet Horn contraction functions.
observation termed loss uniqueness counterintuitive means although
two agents different preferences certain domain, always end
identical contraction outcomes. Entrenchment-based Horn contraction suffer
problem. Horn epistemic entrenchment determines unique entrenchment-based
Horn contraction.
Beside aforementioned works Horn contractions, Delgrande Peppas (2011)
studied model-based Horn revision manner classic model-based revision (Katsuno & Mendelzon, 1992). AGM setting, revision defined contraction
via called Levi identity (Levi, 1991). Zhuang, Pagnucco, Zhang (2013) explored
connection Horn setting showed that, proper restrictions, Horn revision defined Horn contraction via variant Levi identity. also
showed Horn revision functions generated transitively relational partial meet
Horn contraction functions model-based Horn contraction functions suffer
counter-intuitive results whereas one generated entrenchment-based Horn contraction functions guaranteed meaningful (i.e., satisfies revision postulates
proposed Delgrande & Peppas, 2011). Finally, Adaricheva, Sloan, Szorenyi (2012)
provided complexity results regarding Horn contractions.
major construction methods AGM contraction adapted
Horn logic. principle adaptations make adapted contraction
close possible AGM contraction (i.e., equally rational aspects). explained
above, due limited expressiveness Horn logic, none adapted contractions
identical AGM origin aspects. However, means declaring failure
adaptations emphasised Section 5 fair expect rationality
Horn contractions extent expressiveness Horn logic permits. Since
adaptations focus different aspects AGM contraction often exhibit
desirable undesirable properties, convincing argue one outperforms
other. best choice Horn contraction depends actual application.
explicit preference expected loss uniqueness avoided entrenchmentbased Horn contraction better choice.

8. Conclusion
paper, defined entrenchment-based Horn contraction Horn analogue entrenchment-based contraction. outcome Horn contraction determined epistemic entrenchments Horn formulas via refinement condition
.
.
(C ) proposed Gardenfors Makinson (1988). Since condition (C ) involves ar.
bitrary disjunctions may Horn formulas, refined condition (HC ) con.
.
siders Horn strengthenings disjunctions. (HC ) closely resembles (C ),
entrenchment-based Horn contraction function performs identically classic coun244

fiEntrenchment-Based Horn Contraction

terpart (i.e., strict entrenchment-based contraction function) Horn formulas
counted. Since non-Horn formulas disallowed, set epistemic entrenchments
determining entrenchment-based Horn contraction proper subset determining entrenchment-based contraction. Entrenchment-based Horn contraction therefore
comprehensive entrenchment-based contraction. able identify restricted
form entrenchment-based contraction called strict entrenchment-based contraction
one-to-one correspondence entrenchment-based Horn contraction.
Recently, Creignou, Papini, Pichler, Woltran (2014) investigated revision operators
fragments Propositional Logic limited Horn. work promotes new
direction furthering study Horn belief change. many Horn contractions
revisions, important find adaptation strategies generalisable
fragments Propositional Logic Horn. Thus future work, aim develop
generalised entrenchment-based contraction applied arbitrary fragments
Propositional Logic.

Appendix A. Proofs Results
first present notions properties Horn logic used proofs.
intersection two interpretations interpretation assigns true atoms
assigned true interpretations. denote intersection interpretations
. Given set interpretations , closure intersection
denoted Cl (M ). Formally,
Cl (M ) = { | , = }.
Horn formula, set models closed Cl called Horn closed. Conversely, Horn closed set models corresponds unique Horn formula (modulo
logical equivalence). Moreover, intersections Horn closed sets models also Horn
closed.
Lemma 1 , HS() = HS().
Proof: result immediate definition Horn strengthening syntax insensitive.
Lemma 2 Horn formula ` , HS()
` .
Proof: HS(), result trivially holds. Suppose 6 HS(),
definition Horn strengthening 1 LH || |1 | ||. Again,
1 6 HS(), must 2 LH |1 | |2 | ||. Since || finite,
eventually find n Horn strengthening . Since || |n |, ` n .
Lemma 3 HS( ), 1 HS() 2 HS()
1 2 .
Proof: Suppose HS( ), definition Horn strengthening ||
|| || implies || || || ||. follows Lemma 2
1 HS() 2 HS() || |1 | || |2 |, thus || |1 | |2 |.
245

fiZhuang & Pagnucco

Assume |1 | |2 | 6 ||. since |1 | |2 | || ||
|| ||. follows HS( ), || ||, 6 ||
|| 6 || ||. since , |1 | |2 | |1 | |2 |
implies || ||, contradiction! Therefore, |1 | |2 |
6 || implies |1 | |2 | ||.
Lemma 4 Let Horn formulas. HS( ), HS( ) HS( ).
Proof: Suppose HS() 0 HS(), need show 0 HS().
definition Horn strengthening, HS() implies || ||, 0 HS()
implies |0 | ||. follows || || || || || ||
implies |0 | | |. remains show Horn formula 00
|0 | |00 | | |.
Assume contrary Horn formula 00 |0 | |00 | | |.
follows |0 | |00 |, 0 HS( ), definition Horn strengthening
|00 | 6 | |. set theory |00 | \ |0 | || \ | |. follows
6 || HS( ) || = 6 | |.
also 6 || otherwise || =
6 Cl (||) implies ||. Assume 6 |0 |
x | | x = 6 | |. x || || =
6 Cl (||),
contradiction! x ||, || 6= Cl (||), contradiction! Thus conclude
|0 | implies |00 |. follows |00 |, = 6 | |
|00 | =
6 Cl (|00 |), contradiction! Thus 00 |0 | |00 | | |.
Lemma 5 Let Horn epistemic entrenchment. satisfies:
1.
2. ,
3. < iff < .
4. ,
5. < < , <
6. ,
7.
Proof: proofs identical propositional case.
.
Proposition 1 Let Horn contraction function.
.
.
.
.
1. satisfies (H 1) (H de), satisfies (H f )
.
.
.
.
2. satisfies (H 2) (H de), satisfies (H wr)
.
.
.
3. satisfies (H wr), satisfies (H de)
.
.
.
4. satisfies (H pa), satisfies (H ct)
246

fiEntrenchment-Based Horn Contraction

.
.
.
.
5. satisfies (H 6) (H ct), satisfies (H pa)
Proof: 1. Suppose ` . H, ` implies HS() = .
.
.
.
.
Since H = CnH (H ), tautologies H . Thus H . follows
.
.
.
contrapositive (H de), H, H H . Thus
.
H = H.
.
.
2. & 3. show (H wr) (H de) equivalent following postulate:
.
.
.
(H mc) H \ H |H | 6 | |.
.
.
.
.
first show satisfies (H de) iff satisfies (H mc). one direction, suppose
.
.
satisfies (H de). Let H \ H . two cases:
.
.
Case 1, LH : HS( ) = { }. (H de) H 6`
.
implies |H | 6 | |.
.
Case 2, 6 LH : Assume |H | | |. Lemma 2, HS( )
.
.
.
H ` . However, follows (H de) H 6` HS( ),
contradiction!
.
.
.
direction, suppose satisfies (H mc). Since |H | 6 | |,
.
|H | 6 | |. Since || | | HS( ), 6 ||
.
.
HS( ). model theory, follows |H | H 6`
HS( ).
.
.
.
show satisfies (H wr) iff satisfies (H mc).
.
.
.
.
one direction, suppose satisfies (H wr). Let H \ H . Assume |H |
.
0
| | set theory |H | || ||. set theory, H
.
.
|H 0 | |H | |H 0 | || 6= , follows |H | || ||
.
|H 0 | || || implies H 0 {} 6` . However, (H wr)
one H 0 H 0 {} ` , contradiction!
.
.
.
direction, suppose satisfies (H mc). Let H \ H .
.
.
(H mc) |H | || 6 ||. Let Horn belief set H 0
.
.
|H 0 | = {}. |H | implies |H 0 | |H | || implies |H 0 | ||.
.
.
follows |H 0 | |H | |H 0 | || H H 0 H 0 6` . follows
|H 0 | = {} 6 || |H 0 | || = . Thus H 0 {} inconsistent. formulas
follow inconsistent set .
4. & 5. proofs identical propositional case found Page 117
(Hansson, 1999) (OBSERVATION 2.59).
.
Theorem 2 function entrenchment-based Horn contraction function iff satisfies
.
.
.
.
.
.
.
(H 1)(H 4), (H de), (H 6), (H hs), (H ct), (H 8).
Proof: one direction, let H Horn belief set, Horn epistemic entrenchment
.
H, entrenchment-based Horn contraction function H determined
.
.
.
.
.
.
.
. need show satisfies (H 1)(H 4), (H de), (H hs), (H ct), (H 8).
.
.
(H 2): Follows directly (HC ).
.
.
.
(H 1): ` , follows (HC ) H = H. follows
.
.
.
H = CnH (H) H = CnH (H ). Suppose 6` CnH (H ), need show
.
.
(H ). (HC ), suffices show H HS( )
< . two cases:
247

fiZhuang & Pagnucco

.
Case 1, 6` : Since CnH (H ), compactness Horn logic, finite
.
.
subset {1 , . . . , n } H 1 n ` . Since {1 , . . . , n } H , follows
.
(H 2) {1 , . . . , n } H. follows {1 , . . . , n } H 1 n `
.
H ` . follows H ` H = CnH (H) H. follows (HC )
HS( ) < . follows Lemma 5
(Part 5) < 1 n . Since 1 n ` ( 1 ) ( n )
Lemma 2 HS(( 1 ) ( n )) 1 n ` .
Thus < . Since ( 1 ) ( n ) (1 n ), Lemma 1,
HS(( 1 ) ( n )) = HS( (1 n )). Thus HS( (1 n )).
Since (1 n ) ` , ` implies Lemma 2
0 HS( ) ` 0 implies 0 . Since < follows (HEE1)
.
.
< 0 . (HC ), follows H, < 0 H .
Case 2, ` : follows H = CnH (H) H. Also ` implies ` .
Thus (HEE2) (HEE5), . Since 6` , follows Lemma 5
(Part 1) (HEE5) < . (HEE1), follows
< < . definition Horn strengthening, `
.
implies HS( ) = { }. (HC ), follows H <
.
H .
.
.
.
.
(H 3): Suppose 6 H, need show H = H. H H follows (H 2).
.
Let H. suffices show H . (HEE4) Lemma 5 (Part 1), follows
H LH < . Since `
Lemma 2 HS( ) ` . follows ` , (HEE2),
. Since 6 H, follows (HEE4) . apply (HEE1)
.
, < , , obtain < . (HC ), follows H <
.
H .
.
.
(H 4): Suppose 6` . need show 6 H . (HEE2), ` implies .
Lemma 5 (Part 1), implies 6< . Since HS( ) = HS() = {}, follows
.
.
6< 6` , (HC ), 6 H .
.
.
.
.
(H 6): Suppose . first show H H . Let H . follows
.
(HC ) H either ` HS( ) < .
.
Case 1, ` : Since , ` implies ` . follows ` H, (HC ),
.
H .
Case 2, 6` : HS( ) < . Since ,
. Lemma 1, implies HS( ) = HS( ). Thus
HS( ) follows HS( ). intersubstitutativity , < follows
.
.
< . follows H < , (HC ), H .
.
.
.
.
Thus H H . show H H way.
.
.
.
(H de): Suppose H \ H . need show HS( ), 6 H .
.
.
follows (HC ) 6 H HS( ), 6< . Lemma 4,
HS( ), HS( ) HS( ). Thus 0 HS( ),
.
.
6< 0 . follows (HC ) HS( ), 6 H .
.
(HC ): one direction, suppose H . need show
.
.
` . Since H (HC ) H either `
HS(( ) ) < . Since ( ) , Lemma 1
248

fiEntrenchment-Based Horn Contraction

HS(( ) ) = HS() = {}. Lemma 5 (Part 6), implies .
connectivity , implies 6< . Thus must case ` .
.
direction, suppose either 6 H ` , need show .
` , ` follows (HEE2) ` .
.
.
suppose 6` 6 H . follows (HC ) either 6 H
HS(( ) ), 6< . former case, 6 H gives us
required (HEE4). latter case, since ( ) , Lemma 1,
HS(( ) ) = HS() = {}. Thus 6< . connectivity , 6<
implies . follows (HEE2) ` . follows
, (HEE1) .
.
.
(H hs): Suppose H . need show HS( )
.
.
.
H . follows H (HC ) HS( )
.
.
< . Since satisfies (HC ), follows < H .
.
.
.
(H ct): Suppose H , need show H . (HC ),
.
H implies < . follows (HEE2) ` .
follows (HEE1), , < < . (HC ), < implies
.
H .
.
.
.
.
(H 8): Suppose 6 H . need show H H . 6 H
.
.
.
.
.
(H 3) H = H. H H follows (H 2).
.
suppose H. (HC ), 6 H implies . Lemma 5 (Part 6),
.
.
implies . Let H , suffices show H . follows
.
.
H (HC ) HS(( ) ) < .
Lemma 1, ( ) ( ) ( ) implies HS(( ) ) = HS(( ) ( )).
Thus Lemma 3, 1 HS( ) 2 HS( ) 1 2 .
Lemma 5 (Part 7), < 1 2 . follows (HEE2) 1 2 ` 1
1 2 1 . follows (HEE1), , < 1 2 , 1 2 1
.
.
< 1 . follows 1 HS( ) < 1 , (HC ), H .
.
direction, let H Horn belief set Horn contraction function
.
.
.
.
.
.
H satisfies (H 1)(H 4), (H de), (H hs), (H ct), (H 8). suffices show
Horn epistemic entrenchment determined (HC ) satisfies (HEE1)(HEE5)
.
(HC ). replacing AGM contraction postulates corresponding Horn
analogues, proof satisfaction (HEE1)(HEE5) Theorem
.
2.50 (Hansson, 1999). therefore give proof (HC ).
.
.
(HC ): one direction, suppose H . need show H either `
.
.
HS( ) < . Since H , (H hs)
.
.
HS( ) H . (HC ), H implies < .
.
.
(H 2), H implies H.
direction, suppose H either ` HS( )
.
.
< . need show H . ` H = H. Thus H implies
.
.
H . suppose 6` . suffices show H \ H HS( ),
.
.
.
. (H de), H \ H implies HS( ), 6 H . Assume
.
.
.
.
.
H . (H 2), H implies 6 H . follows (H 8)
.
.
.
.
.
.
6 H H H . follows 6 H H H
.
.
.
6 H , contradiction. Thus 6 H . (HC ), 6 H
implies .
249

fiZhuang & Pagnucco

.
Proposition 2 entrenchment-based Horn contraction function, satisfies
.
(H 7).
.
Proof: Let entrenchment-based Horn contraction function H as.
.
sociated Horn epistemic entrenchment . Theorem 2 satisfies (H 1)
.
.
.
.
.
.
.
(H 4), (H de), (H hs), (H ct), (H 8). Suppose (H ) (H ). need
.
show H .
.
.
.
Case 1, ` : . follows (H 6) H = H , thus
.
H .
Case 2, ` : proof similar Case 1.
.
Case 3, 6` 6` : follows (HEE2) . follows H
.
(HC ) 1 HS() < 1 . conclude (HEE1)
.
< 1 . Similarly, follows (HEE2) H
.
(HC ) 2 HS( ) < 2 , thus < 2 .
Lemma 5 (Part 5), deduce < 1 < 2 , < 1 2 .
definition Horn strengthening 1 ` | 2 ` implies
1 2 ` ( ) ( ). Since ( ) ( ) ( ) , 1 2 ` ( ) .
follows Lemma 2 HS(( ) ) 1 2 ` .
(HEE2), 1 2 ` implies 1 2 . (HEE1), < .
.
.
Finally, follows < (HC ), H .
.
.
Theorem 4 function strict entrenchment-based contraction function iff satisfies
.
.
.
(K 1)(K 8) (K hs).
.
Proof: one direction, suppose strict entrenchment-based contraction function
K associated epistemic entrenchment . satieties (EE6). Since strict
entrenchment-based contraction function entrenchment-based contraction function,
.
.
.
.
Theorem 1 satisfies (K 1)(K 8) (C ). remains show satisfies
.
.
(K hs). Suppose , LH K . need show HS( )
.
K . ` definition Horn strengthening ` thus
.
.
result trivially holds. suppose 6` . follows K (C )
.
< . Since satisfies (EE6), HS( ) < . Since
.
satisfies (C ), < implies K .
.
.
.
.
direction, suppose function satisfies (K 1)(K 8) (K hs)
.
need show strict entrenchment-based contraction function. follows The.
orem 1 entrenchment-based contraction function. remains show
.
epistemic entrenchment generated via (C ) satisfies (EE6). Suppose , LH
.
< . need show HS( ) < . (C ),
.
.
< implies K . follows (K hs) HS( )
.
K implies (C ) < .
.
Theorem 5 H entrenchment-based Horn contraction function, strict
.
.
.
.
entrenchment-based contraction function H Horn equivalent.
strict entrenchment-based contraction function, entrenchment-based
.
.
.
Horn contraction function H H Horn equivalent.
Proof: Part 1: Let K belief set H Horn belief set H = H(K).
.
Suppose H entrenchment-based Horn contraction function H determined
250

fiEntrenchment-Based Horn Contraction

Horn epistemic entrenchment H H. Let binary relation L
, LH
(1) iff H ,
(2) < iff HS( ) <H .
expand rest non-Horn formulas form epistemic en.
trenchment. Let entrenchment-based contraction function K determined
.
. Due definition (i.e., (2)), satisfies (EE6) implies strict
entrenchment-based contraction function.
.
.
Suppose LH . need show H(K ) = H H . ` , follows
.
.
.
.
(H f ) classic version (follows (K 5)) H H = H K = K. Thus
.
.
.
H(K ) = H H follows H = H(K). suppose 6` . first show H H
.
.
.
.
H(K ). Let H H . suffices show K . (HC ), follows
.
H H H HS( ) <H .
definition (i.e., (2)) < . Also since H = H(K), H implies K.
.
.
follows K < (C ) K .
.
.
.
show H(K ) H H . Suppose LH K . need
.
.
.
show H H . (C ), follows K K < .
definition HS( ) <H . Also since
H = H(K), H follows K fact Horn formula. Finally,
.
.
follows H <H (HC ) H H .
Part 2: part proved similar manner Part 1. time need
generate Horn epistemic entrenchment epistemic entrenchment
Horn epistemic entrenchment Horn subset epistemic entrenchment.
.
.
Theorem 6 Let H entrenchment-based Horn contraction function.
.
.
.
entrenchment-based contraction function H Horn equivalent
strict entrenchment-based contraction function.
.
.
Proof: Let H entrenchment-based Horn contraction function H
.
.
entrenchment-based contraction function K H = H(K) H
Horn equivalent.
.
.
.
Theorem 4, suffices show satisfies (K hs). Let , LH . Suppose K .
.
.
.
need show HS( ) K . Since H Horn
.
.
.
equivalent, follows K H H . follows (H hs)
.
HS( ) H H . Horn equivalence
.
.
.
.
H , H H implies K H .
.
.
Theorem 8 function entrenchment-based Horn withdrawal function iff satisfies
.
.
.
.
following postulates: (H 1)(H 4), (H 6), (H 8),
.
. H
. .
(H 7a)
6` , H
Proof: proof identical propositional case found work
Rott Pagnucco (1999, pages 538540).
.
Theorem 9 H entrenchment-based Horn withdrawal function,
.
.
.
.
entrenchment-based withdrawal function H Horn equivalent.
251

fiZhuang & Pagnucco

entrenchment-based withdrawal function, entrenchment-based Horn
.
.
.
withdrawal function H H Horn equivalent.
Proof: Part 1: Let K belief set H Horn belief set H = H(K).
.
Suppose H entrenchment-based Horn withdrawal function H determined
Horn epistemic entrenchment H H. Let
iff H , LH .
expand non-Horn formulas form epistemic entrenchment.
.
Let entrenchment-based withdrawal function K determined
.
.
.
.
via (W ). ` , follows (K 3) (H 3) K = K
.
.
.
H H = H. Thus H(K ) = H H follows H = H(K). suppose 6` . first
.
.
.
.
.
show H H H(K ). Let H H . suffices show K . (HW ),
.
follows H H H <H . Since H = H(K), H implies K.
definition , <H implies < . follows K <
.
.
(W ) K .
.
.
.
show H(K ) H H . Let Horn formula K .
.
.
.
suffices show H H . (W ), follows K K < .
Since H = H(K), H follows K fact Horn formula. Since
Horn formulas < , definition that, definition
.
, < implies <H . follows H <H , (HW )
.
H H .
Part 2: part proved similar manner Part 1. time need
generate Horn epistemic entrenchment epistemic entrenchment
Horn epistemic entrenchment Horn subset epistemic entrenchment.

References
Adaricheva, K., Sloan, R. H., & Szorenyi, B. (2012). Horn belief contraction: Remainders,
envelopes complexity. Proceedings 13th International Conference
Principles Knowledge Representation Reasoning (KR-2012), pp. 107115.
Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). logic theory change:
Partial meet contraction revision functions. Journal Symbolic Logic, 50 (2),
510530.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2003).
Description Logic Handbook. Cambridge University Press, Cambridge, UK.
Booth, R., Meyer, T., & Varzinczak, I. J. (2009). Next steps propositional Horn contraction. Proceedings 21st International Joint Conference Artificial Intelligence (IJCAI-2009), pp. 702707.
Booth, R., Meyer, T., Varzinczak, I. J., & Wassermann, R. (2010). Horn belief change:
contraction core. Proceedings 20th European Conference Artificial
Intelligence (ECAI-2011), pp. 10651066.
Booth, R., Meyer, T., Varzinczak, I. J., & Wassermann, R. (2011). link
partial meet, kernel, infra contraction application horn logic. Journal
Artificial Intelligence Research, 42, 3153.
252

fiEntrenchment-Based Horn Contraction

Creignou, N., Papini, O., Pichler, R., & Woltran, S. (2014). Belief revision within fragments
propositional logic. Journal Computer System Sciences, 80 (2), 427449.
Delgrande, J. P. (2008). Horn clause belief change: Contraction functions. Proceedings
11th International Conference Principles Knowledge Representation
Reasoning (KR-2008), pp. 156165.
Delgrande, J. P., & Peppas, P. (2011). Revising Horn Theories. Proceedings 22nd
International Joint Conference Artificial Intelligence (IJCAI-2011), pp. 839844.
Delgrande, J. P., & Wassermann, R. (2010). Horn clause contraction function: Belief set
belief base approaches. Proceedings 12th International Conference
Principles Knowledge Representation Reasoning (KR-2010), pp. 143152.
Delgrande, J. P., & Wassermann, R. (2013). Horn clause contraction functions. Journal
Artificial Intelligence Research, 48, 475551.
Ferme, E., Krevneris, M., & Reis, M. (2008).
axiomatic characterization
ensconcement-based contraction. Journal Logic Computation, 18 (5), 739753.
Flouris, G., Plexousakis, D., & Antoniou, G. (2004). Generalizing AGM postulates: preliminary results applications. Proceedings 10th International Workshop
Non-Monotonic Reasoning (NMR-2004), pp. 171179.
Foo, N. Y. (1990). Observations AGM entrenchment. Tech. rep. 389, Basser Department
Computer Science, University Sydney.
Fuhrmann, A., & Hansson, S. O. (1994). survey multiple contractions. Journal
Logic, Language Information, 3 (1), 3974.
Gardenfors, P. (1988). Knowledge Flux: Modelling Dynamics Epistemic States.
MIT Press.
Gardenfors, P., & Makinson, D. (1988). Revisions knowledge systems using epistemic entrenchment. Proceedings 2nd conference Theoretical Aspects Reasoning
Knowledge (TARK-1988), pp. 8395.
Hansson, S. O. (1991). Belief Contraction Without Recovery. Studia Logica, 50 (2), 251260.
Hansson, S. O. (1993). Changes disjunctively closed bases. Journal Logic, Language
Information, 2 (4), 255284.
Hansson, S. O. (1999). Textbook Belief Dynamics Theory Change Database Updating. Kluwer.
Katsuno, H., & Mendelzon, A. O. (1992). Propositional knowledge base revision minimal change. Artificial Intelligence, 52 (3), 263294.
Kautz, H., & Selman, B. (1996). Knowledge compilation theory approximation. Journal
ACM, 43, 193224.
Langlois, M., Sloan, R. H., Szorenyi, B., & Turan, G. (2008). Horn complements: Towards
Horn-to-Horn belief revision. Proceedings 23rd National Conference Artificial Intelligence (AAAI-2008), pp. 466471.
Levi, I. (1991). Fixation Beliefs Udoing. Cambridge University Press.
253

fiZhuang & Pagnucco

Makinson, D. (1987). status postulate recovery logic theory
change. Journal Philosophical Logic, 16 (4), 383394.
Rott, H. (1992). Preferential belief change using generalised epistemic entrenchment. Journal Logic, Language Information, 1 (1), 4578.
Rott, H., & Pagnucco, M. (1999). Severe withdrawal (and recovery). Journal Philosophical
Logic, 28 (5), 501547.
Zhuang, Z., & Pagnucco, M. (2010). Horn contraction via epistemic entrenchment. Proceedings 12th European Conference Logics Artificial Intelligence (JELIA2010), pp. 339351.
Zhuang, Z., & Pagnucco, M. (2011). Transitively Relational Partial Meet Horn Contraction.
Proceedings 22nd International Joint Conference Artificial Intelligence
(IJCAI-2011), pp. 11321138.
Zhuang, Z., & Pagnucco, M. (2012). Model Based Horn Contraction. Proceedings
13th International Conference Principles Knowledge Representation
Reasoning (KR-2012), pp. 169178.
Zhuang, Z., Pagnucco, M., & Zhang, Y. (2013). Definability horn revision Horn
contraction. Proceedings 23rd International Joint Conference Artificial
Intelligence (IJCAI-2013), pp. 12051211.

254

fiJournal Artificial Intelligence Research 51 (2014) 443-492

Submitted 05/14; published 10/14

Push Rotate: Complete Multi-agent Pathfinding Algorithm
Boris de Wilde
Adriaan W. ter Mors
Cees Witteveen

BOREUS @ GMAIL . COM
. W. TERMORS @ TUDELFT. NL
C . WITTEVEEN @ TUDELFT. NL

Faculty Electrical Engineering, Mathematics, Computer Science
Mekelweg 4, 2628 CD Delft, Netherlands

Abstract
Multi-agent Pathfinding relevant problem wide range domains, example
robotics video games research. Formally, problem considers graph consisting vertices
edges, set agents occupying vertices. agent move unoccupied,
neighbouring vertex, problem finding minimal sequence moves transfer
agent start location destination NP-hard problem.
present Push Rotate, new algorithm complete Multi-agent Pathfinding
problems least two empty vertices. Push Rotate first divides graph
subgraphs within possible agents reach position subgraph, uses
simple push, swap, rotate operations find solution; post-processing algorithm
also presented eliminates redundant moves. Push Rotate seen extending Luna
Bekriss Push Swap algorithm, showed incomplete previous publication.
experiments compare approach Push Swap, MAPP, Bibox algorithms. latter algorithm restricted smaller class instances requires biconnected
graphs, nevertheless considered state art due strong performance. experiments show Push Swap suffers incompleteness, MAPP generally competitive
Push Rotate, Bibox better Push Rotate randomly generated biconnected
instances, Push Rotate performs better grids.

1. Introduction
Computer scientists roboticists long studied problem coordinating motions
multiple moving objects. general formulation problem find conflict-free trajectories
space time objects. Cast warehousemans problem, problem
proved PSPACE-complete Hopcroft et al. (1984). problems complexity reduced
NP-complete assuming agents move along graph (Goldreich, 1993). graph
(or roadmap) given, instance applications automated guided vehicles following
lines drawn factory floors (Roszkowska & Reveliotis, 2008), also learned (Kavraki,
Svestka, Latombe, & Overmars, 1996) otherwise constructed (LaValle & Kuffner, 2001).
Application domains multi-agent pathfinding include many forms robotics, instance
mobile robots (Simeon, Leroy, & Laumond, 2002) robot arms (Erdmann & Lozano-Perez,
1987); routing automated guided vehicles, instance container terminals (Vis, 2006;
Gawrilow, Kohler, Mohring, & Stenzel, 2007) warehousing manufacturing (Narasimhan,
Batta, & Karwan, 1999; Desaulniers, Langevin, Riopel, & Villeneuve, 2004); video games,
role-playing games real-time strategy games (Nieuwenhuisen, Kamphuis, & Overmars, 2007);
airport taxi routing (Trug, Hoffmann, & Nebel, 2004; Ter Mors, Zutt, & Witteveen, 2007) collision avoidance airplanes flight (Sislak, Pechoucek, Volf, Pavlcek, Samek, Mark, & Losiewicz,
c
2014
AI Access Foundation. rights reserved.

fiD E W ILDE , ER ORS & W ITTEVEEN

2008); routing multi-agent pathfinding also occurs less obvious domains planning mining carts (Beaulieu & Gamache, 2006) routing sheets paper modular
printer (Ruml, Do, Zhou, & Fromherz, 2011).
remarked Surynek (2011), applicability algorithmic approaches depends
freedom agents respective application domains. Incomplete reservation-based approaches, agents typically plan independently, minding reservations others (Lee,
Lee, & Choi, 1998), useful case enough room infrastructure guarantee (or
least make highly likely) solution found. paper, discuss approaches
also deal congested scenarios, majority locations
occupied agents. inspiration situation real-time strategy computer games,
large numbers units, friendly hostile, competing room move around
(see Figure 1).

Figure 1: Zerg attack Protoss Starcraft 2. maps original Starcraft
made available research http://movingai.com/benchmarks/.

Kornhauser (1984) gave complete algorithm multi-agent pathfinding problem, well
lower upper bounds O(n3 ) (where n number vertices graph) number
moves required solution. recent years, number approaches appeared solve
subclasses multi-agent pathfinding problem (see Section 2.1). Roger Helmert (2012)
suggested one reasons results Kornhauser widely
applied is:
. . . approach described one place, parts described
algorithmically. Therefore, underlying algorithm must derived number
proofs paper.
444

fiC OOPERATIVE ULTI - AGENT PATHFINDING

Push Swap algorithm (Luna & Bekris, 2011b) complete algorithm instances
least two unoccupied vertices, therefore general recent algorithms. However, found number problems Push Swap algorithm (De Wilde, Ter Mors, &
Witteveen, 2013), presented Push Rotate algorithm overcome shortcomings.
primary aim paper provide complete understandable specification
algorithm multi-agent pathfinding problem least two unoccupied vertices.
specifically, paper presents following contributions:
1. specification complete algorithm multi-agent pathfinding, involves reconstruction, cases refinement theoretical results Kornhauser (1984).
2. empirical evaluation algorithm, includes comparison Bibox algorithm (Surynek, 2009), algorithm assumes biconnected instances, among
best performing recently developed approaches described Section 2, terms
number moves CPU time required (judging empirical evaluations Surynek,
2009; Wang & Botea, 2011).
3. brief exploration possibilities improve solution quality reduce computation
times use heuristics, namely area selecting agent plan next,
path agent choose.
Part first contribution appeared conference paper (De Wilde et al., 2013), namely
Algorithms 1, 2, 3, 8 proofs. reconstruction Kornhausers theoretical
results, well complete listing relevant algorithms proofs new paper,
contributions two three (a different, restricted empirical evaluation conducted
conference paper).
paper organized follows. Section 2, define multi-agent pathfinding problem,
discuss complexity number algorithmic approaches. Section 3, describe
Push Rotate algorithm, Section 4 prove completeness analyze worstcase behavior. Section 5, describe heuristics path agent selection, describing
experiments Section 6. experiments comprise comparison Bibox Push
Swap algorithms different types randomly generated instances. Finally, finish
conclusions future work Section 7.

2. Background Problem Statement
consider simple1 connected2 graph G = (V, E), set agents A, |A| < |V |, assignment
function agents vertices : V , goal assignment agents vertices : V .
functions total, injective, non-surjective; total functions location
1. graph simple one edge two vertices; multiple edges two vertices would
expand solution space, since assume agent must always move empty vertex.
2. graph connected, Multi-agent Pathfinding problem considered components
separately. exists agent destination location different component start
location, instance solution.

445

fiD E W ILDE , ER ORS & W ITTEVEEN

agent must specified, injective one vertex hold single agent time,
non-surjective require always vertices agents.
move transfer agent ai current vertex v = (ai ), adjacent, unoccupied vertex w, 1 (w) = . define ULTI - AGENT PATHFINDING problem optimization problem: find sequence moves transforms initial assignment goal
assignment, || |0 | sequence moves 0 transforms .
decision variant Multi-agent Pathfinding problem (i.e., exist sequence
K moves transforms initial assignment goal assignment) shown NPcomplete (Goldreich, 1993)3 . NP-completeness problem holds case one
agent destination location, agents obstacles may moved
way (Papadimitriou, Raghavan, Sudan, & Tamaki, 1994) (i.e., goal assignment partial
function domain size 1). Push Rotate algorithm present Section 3
find move sequences Multi-agent Pathfinding problem least two unoccupied
vertices, although guarantee optimal solutions, would require exponential running
time, assuming P 6= NP.
Goraly Hassin (2010) presented variant Multi-agent Pathfinding problem
colors, p agents color. goal configuration specifies
vertex color occupying agent must (or whether vertex remain empty);
authors show feasibility decided linear time (i.e., decided whether
instance solution), also case = p. Goraly Hassin build work Auletta
et al. (1999), presented algorithm deciding feasibility Multi-agent Pathfinding
problems trees, linear time.
Calinescu et al. (2008) also distinguish different kinds chips, namely unlabeled
labeled (where every label unique), consider different type model moving
chip along empty path considered single move. prove even unlabeled chips,
problem APX-hard, still NP-hard instances infinite, rectangular grid. Finally, Wu
Grumbach (2009) considered directed graphs, interesting property wrong
move put problem unrecoverable configuration. Unrecoverable reconfigurations
also heart PSPACE-completeness proof Sokoban puzzle game (Culberson, 1999;
Hearn & Demaine, 2005); Wu Grumbach consider complexity optimization
problem, however, instead prove feasibility decided O(n2 m) time, n
number vertices, number arcs.
Early research Multi-agent Pathfinding focused feasibility reaching one assignment another. Wilson (1974) studied simple biconnected4 graphs single empty vertex,
shows assignment reached other, except graph bipartite, case two sets assignments reach assignments
set. Wilsons theorem includes two exceptions generally solvable: polygon (or cycle)
graphs, and, remarkably, single specific graph 0 (Figure 2).
Kornhauser et al. (1984) extend Wilsons result general graphs number unoccupied vertices provide polynomial-time decision procedure O(n3 ) bound number
moves required. result Kornhauser based insight graph may viewed
3. Goldreich proves NP-completeness Shortest Move Sequence problem equivalent definition
ULTI - AGENT PATHFINDING, exception Goldreich considers biconnected graphs.
4. biconnected graph connected graph articulation vertices, i.e., vertices whose removal disconnect
graph; graphs also called nonseparable graphs.

446

fiC OOPERATIVE ULTI - AGENT PATHFINDING

(a) polygon graph.

(b) Wilsons 0 graph.

Figure 2: Biconnected graphs generally solvable single empty vertex.

tree biconnected components linked chains zero vertices degree 2
(called isthmuses). turns two components linked isthmus contains
vertices number unoccupied vertices minus two, impossible agents
components different sides isthmus swap positions.
a5
a8

a2
a1

a3

a6

a4
Able cross

a7

Unable cross

Figure 3: Illustration isthmuses connecting nonseparable components.
Figure 3, note possible agents A1 = {a1 , . . . , a6 } swap positions,
also agents A2 = {a7 , a8 } exchange positions, agent A1 exchange
position agent A2 . means say that, take agents a6 a7 example,
possible reach configuration a6 occupies position a7 Figure 3,
time a7 occupies current position a6 . isthmuses impossible
cross induce decomposition graph smaller subgraphs, solved turn.
demonstrate decomposition Section 3.1.
2.1 Algorithms
Standley (2010) proposed algorithm guarantees optimal5 solutions multi-agent pathfinding agents move eight-connected grid. time step, agent either moves
adjacent grid cell stands still, cost single agent number time steps
takes reach goal; cost entire solution sum agent plan costs. Moves
allowed empty grid cells, also cycle agents agent moves cell
next agent cycle. Hence, Standleys algorithm also applied problem instances
number agents equal number vertices.
5. section, refer approach optimal returns solution consisting minimal number
moves.

447

fiD E W ILDE , ER ORS & W ITTEVEEN

Standleys approach seen improve standard A*-based approach state
n-tuple grid locations, one n agents, moves agents
considered simultaneously one timestep, state potentially 9n legal operators (nine
eight moves plus wait move). Standleys operator decomposition divides
time step one agent considered time, thereby reducing branching factor
9n 9, although search depth increases factor n. Coupled perfect heuristics
perfect tie-breaking strategy, operator decoupling would reduce number required steps
9n d, search depth d, 9n d. Hence, operator decoupling scheme requires good
heuristic save computation time compared standard approach.
work, operator decomposition technique employed create anytime algorithm, based maximum group size agents optimal solutions found (Standley
& Korf, 2011). anytime algorithm starts group size one, increases group size
one step solution found, looking efficient solutions. stand-alone operator
decomposition algorithm struggled find solutions problems larger 32 32 grid
ten agents, anytime algorithm able produce good-quality solutions
hundred agents.
Another optimal approach due Sharon et al. (2011), called Increasing Cost Trees
(ICT). ICT, high-level search tree node consists k-vector
individual path costs, represents possible solutions cost agent equals
value cost vector. create child node, unit cost added cost one
agents. low-level search performed see new node bring agents
destinations. ICT efficient scenarios low interaction agents,
agents reach destinations small number extra moves; otherwise, Standleys
operator decomposition efficient.
Another optimal approach Sharon et al. (2012) meta-agent constraint-based search.
constraint-based search, high-level search performed constraint tree, nodes constraints individual agents, low-level search find individual agent paths respect
constraint high-level node. constraint-based approach performed poorly types
problems, however, authors devised meta-agent strategy, groups agents
many internal conflicts merged one meta agent, order reduce number conflicts
high-level constraint search.
Given NP-completeness problem, many papers employ sequential approach,
agents planned one other, obtain polynomial-time algorithm
necessarily optimal. ensure planning agent n disrupt work done put
agents 1, . . . , n 1 position, existing algorithms either reserve time slots nodes, previous
agents restored positions planning agent n.
Reservation-based approaches typically complete, sense reservations made
first n agents make impossible agent n + 1 find plan (even multi-agent
plan n + 1 agents could found means), approaches
particularly suited instances ratio number agents number
empty vertices high. context-aware routing (Ter Mors, Witteveen, Zutt, & Kuipers, 2010),
agent n finds optimal route plan (i.e., path plus times arrive node
path) around reservations first n 1 agents, assumes agents enter
graph start first reservation, leave graph destination. Alternatively,
448

fiC OOPERATIVE ULTI - AGENT PATHFINDING

assumed start destination locations parking places, hold infinite
number agents (Zutt & Witteveen, 2004).
Velagapudi, Sycara, Scerri (2010) use reservation-based system create distributed
cooperative routing algorithm. problem definition general, simply assume
set robots looking trajectories within given time horizon, binary obstacle map O,
function C OLLISION C HECK takes two robot trajectories returns true collide.
distributed routing algorithm, agent found route, broadcasts route
agents. route planning, agent take account routes higher-priority
agents broadcast. addition, agent receives higher-priority route conflicts
route, re-plan.
Silver (2005) presents windowed approach agents make reservations
restricted time horizon. Silver claims following three advantages. First, agents continue
cooperating reach destination vertices (instead staying put blocking
destination vertices). second advantage sensitivity agent ordering (or prioritization)
reduced, different agent priorities assigned different time periods. Finally,
need plan long-term contingencies may occur.


Alt.path p0



v

Path p

Figure 4: LIDABLE class instances, must exist, every path p every vertex
v p, alternative path p0 connecting predecessor successor v.

MAPP algorithm (Wang & Botea, 2008) brings agents destination one one along
pre-computed path, lower-priority agents may temporarily pushed aside. MAPP requires
number restrictions instances guarantees find solution; LIDABLE
class instances requires:
1. node agents start-destination path (except start destination),
must exist alternative path connecting predecessor successor nodes (see Figure 4),
2. first node (after start node) first agents path must empty,
3. target location may intersect paths alternative paths
agents.
Khorshid et al. (2011) present Tree-based Agent Swapping Strategy (TASS): iteration
algorithm, agent selected sent goal, accomplished swapping
agents. Swapping two agents accomplished moving agents junction node
degree three ensuring least two neighbors junction empty.
449

fiD E W ILDE , ER ORS & W ITTEVEEN

swap performed, empty vertices agents moved
returned original locations, two swapping agents affected.
solve instances general graphs, authors first employ graph-to-tree decomposition
algorithm. However, decomposition complete sense solvable instance may
solution transformation tree. authors also provide conditions
instance guaranteed solvable, namely distance two
junctions may greater number empty vertices minus two, turns
identical result obtained Kornhauser (1984) restricted trees.
Suryneks Bibox algorithm (2009) requires (and complete for) biconnected graphs
least two unoccupied vertices. Bibox algorithm makes use fact biconnected
graph viewed original cycle, extended number handles (see Figure 5; Kornhauser shows biconnected graph decomposed handles). Agents destination outermost handle brought destination first, handle
need taken account more, algorithm proceeds next handle. Surynek
also implemented approach Kornhauser (1984), reported Bibox produced lower
running times shorter paths.

Figure 5: graph consisting initial cycle, white, two handles yellow blue.

Finally, Push Swap algorithm (Luna & Bekris, 2011b) presented complete
graph two unoccupied vertices. algorithm works iteratively selecting agents
unspecified priority order move respective destination locations. agent,
algorithm move destination location along shortest path. agents
encountered along path, action taken depends priority agent.
case agent lower priority, algorithm attempt move way
push operation. accomplished pushing blocking agent forward along shortest
path (not containing higher-priority agents) empty vertex. case blocking agent
higher priority, algorithm attempt exchange positions using swap operation
450

fiC OOPERATIVE ULTI - AGENT PATHFINDING

similar swap operation TASS (Khorshid et al., 2011). higher-priority agent must
returned destination, Push Swap algorithm uses resolve operation.
parallel version Push Swap developed Sajid, Luna, Bekris (2012),
aim reducing length solutions produced Push Swap, moves
sequential. idea behind Parallel Push Swap first resolve dependencies
agents require swap operation, push steps performed
parallel.
Although presented complete, showed Push Swap algorithm complete,
contains following shortcomings (De Wilde, 2012; De Wilde et al., 2013):
1. algorithm identify polygon graphs, i.e., graphs consisting single cycle.
solvable polygon instance, Push Swap fail find solution wrong agent
priority ordering chosen; agent tries move higher-priority agent way
swap, algorithm fail, since swap requires vertex degree 3,
polygon graph vertices degree 2 (see Figure 6 illustration).
2. enable swap operation, two neighboring vertices junction (in TASS terminology)
must emptied using clear operation; specification clear Luna Bekris
identifies two four cases must distinguished.
3. moving vertex back destination swap, resolve may invoke
swap operation again. Examples constructed recursive calls result
higher-priority agent two steps removed destination, Push Swap may
fail instances.
4. Push Swap algorithm take account result Kornhauser (1984)
impossible agents swap separated isthmus longer
number empty vertices minus two. Push Swap algorithm may fail case agent
ai must move way another agent j , ai assigned higher priority,
ai j cannot swap.

a1

a2

Figure 6: Regardless agent moves first, second agent try reach goal along
shortest path, blocked other, higher-priority agent. swap
attempted vertices degree 3 swap at.
first shortcoming resolved personal communication author: instead
always choosing shortest path, agent must choose shortest path destination
contain finished agents. second shortcoming rectified updated clear
operation (Algorithm 12, Appendix A). fix third point, changed way agent
451

fiD E W ILDE , ER ORS & W ITTEVEEN

returned position swap, introduce new operation rotate accomplish this.
address fourth point, studied theory problem decomposition Kornhauser (1984),
developed new algorithms decompose problem subgraphs (see Definition 4), assign
agents subgraphs, determine agent priorities extent determined
relation subgraphs. result algorithm multi-agent pathfinding
complete general graphs least two empty vertices.

3. Push Rotate
Push Rotate algorithm consists pre-processing phase, phase agents
moved destinations. pre-processing phase, first divide graph subgraphs, assign agents subgraphs. definition subgraph (Definition 4
Section 3.1) ensures agents assigned subgraph swap positions6 . third
final step pre-processing determine order (priority) agents planned for.
agents assigned subgraph, priority ordering feasible (we test ordering
heuristic Section 6.3), agents assigned different subgraphs, may necessary
complete one subgraph starting another, partial priority relation subgraphs
determined.
comes phase moving agents destination locations, algorithm works
fashion similar Push Swap (Luna & Bekris, 2011b). First, determine shortest path
agent destination, attempt move agent forward along path.
agents encountered along way, action taken depends whether
blocking agent higher priority. lower priority (meaning planned
agent yet), try push agent way along shortest path empty
vertex. work, agent higher priority (it planned for,
occupying destination location), attempt exchange position agents using
swap operation, involves moving agents vertex degree three higher, emptying
two neighbouring vertices, performing exchange operation (Figure 7), reversing
appropriate moves ensure swapping agents different position namely
others. swap operation, must return higher-priority agent destination.
Push Swap, results recursive calls swap, ultimately undefined behavior (see De
Wilde, 2012, analysis); Push Rotate algorithm, solve problem detecting
whether cycle agents want move forward. so, agents advanced
one step rotate operation.
swap operation fails, (like Luna & Bekris, 2011b) conclude instance
solution. noted, however, conclusion validly drawn particular
priority orderings. Figure 8, example, instance solvable agent a4 (or a5 , both)
higher priority agent a3 . Otherwise, a3 moved destination vertex v first, push
operation a4 a5 fail a3 higher priority; swap operation fail,
swap impossible a3 a4 . see swap impossible, note requires
6. normally use word swap indicate exchange position two agents occupying adjacent nodes.
broader meaning word swap two agents changing position assignment. latter
meaning, Kornhauser interchangeably uses terms 2-cycle, transposition, swap (Kornhauser, 1984, p. 8).
meanings word swap hold regard agents able swap iff assigned subgraph.
Note write swap, refer operation defined Algorithm 5.

452

fiC OOPERATIVE ULTI - AGENT PATHFINDING

a1
v
a1

v

v

a2

a2

a1

a2
Figure 7: Sequence states exchange operation.

agents vertex degree three more, two neighbors unoccupied. two
vertices degree three reached a1 a4 v v0 . a3 v,
one empty neighbor left v; a3 moves back start location a4 moves
v0 , one empty neighbor right v0 .

a2

a1

a3

a4

v0

v

a5

Figure 8: instance solvable iff priority a4 , a5 , higher priority a3 .

3.1 Problem Decomposition
Kornhauser (1984) proposed decomposition graph subgraphs, reconstruct
Section 3.1.1, along algorithm obtain decomposition. subgraphs defined
manner agents either confined one subgraph, confined isthmus.
Section 3.1.2, pose proposition proof come later swap operation
(Algorithm 5, Section 3.2) succeed two agents assigned
subgraph. results Section 3.1.2 refinement results Kornhauser,
explicitly state details agent assignment problem. Finally, Section 3.1.3, consider
interactions agents assigned different subgraphs, sense subgraphs
must solved others, order avoid problems explained Figure 8. Kornhauser
specify priority agents, therefore restrictions regarding moving
agents higher priority, prioritization subgraphs unique approach.
3.1.1 C ONSTRUCTING UBGRAPHS
Kornhauser (1984) remarked connected graph viewed tree biconnected components. Within biconnected components, agents exchange position, biconnected com453

fiD E W ILDE , ER ORS & W ITTEVEEN

ponents play central role construction subgraphs. Kornhausers thesis, first step
creating subgraphs divide graph biconnected components, defined Kornhauser
follows (Kornhauser, 1984, p. 30):
Definition 1 (Biconnected Components). Let G = (V, E) simple connected graph. Two edges
e1 , e2 E equivalent e1 = e2 cycle G containing e1 e2 ;
equivalence relation. equivalence classes, together incident vertices, biconnected
components G .
a9

a4
a6

a1
3

1

a2

1

a3

6

2

4

s1

s2

9

6

5

6

s3

a5

7

s4

a7

8

a8

10

s5

a10

1

(a) Ten biconnected components, vertices s1 , . . . , s5 degree 3 join biconnected components.

a9

a4
a6

a1
a2

S1

S2

a5

a7

a8

S3

a10

a3
(b) = 3, graph three subgraphs S1 , S2 , S3 .

Figure 9: Identifying biconnected components (9(a)) subgraphs (9(b)).
Biconnected components consisting one edge called trivial. graph Figure 9(a)
contains ten biconnected components, two nontrivial (components numbered 1 6).
Note vertices s1 , . . . , s5 Figure 9 vertices7 degree 3 join biconnected components,
trivial nontrivial. basis vertices Kornhauser defines equivalence relation
use construct subgraphs.
Definition 2 (Join vertices). Let G = (V, E) simple connected graph. set join vertices
V consists vertices degree 3 common least two biconnected components.
Note set join vertices cannot comprise set vertices V : first suppose
graph consists exclusively trivial biconnected components. implies graph acyclic,
exist least two vertices degree 1, therefore join vertices. case
graph also contains nontrivial biconnected component C1 , = V , vertices C1 must
connected biconnected components C2 , . . . ,C j , vertices would also
7. follow notation Kornhauser here, denote join vertices si rather denoting vertices vi
elsewhere.

454

fiC OOPERATIVE ULTI - AGENT PATHFINDING

connected different biconnected components. course, possible keep creating new
biconnected components, reusing earlier one would create cycle, meaning vertices
involved single biconnected component, connecting different ones.
following, let number empty vertices.
Definition 3 (Reachability Equivalence). Given simple connected graph G = (V, E) set
join vertices S, s1 , s2 equivalent
1. s1 = s2 ,
2. s1 s2 nontrivial biconnected component,
3. unique path s1 s2 , length 2.
transitive closure relation equivalence relation.
Figure 9(a), = 3 equivalence classes S1 = {s1 , s2 }, S2 = {s3 , s4 }, S3 =
{s5 }. Note remove one agent Figure 9, 4, condition 3 Definition 3
ensures s2 s3 equivalence class, well s4 s5 . Hence, 4 empty
vertices, graph Figure 9 contains single equivalence class S1 = {s1 , . . . , s5 }.
Definition 4 (Subgraph). Given equivalence class Si , subgraph Si = (Vi , Ei ) subgraph
G , induced set vertices Vi consisting of:
1. equivalence class Si Definition 3,
2. vertices v V v nontrivial biconnected component Si ,
3. vertices unique path two join vertices s1 , s2 Si .
Figure 9(b) shows three subgraphs example, corresponding equivalence classes S1 , S2 , S3 . algorithm obtain division subgraphs given
Algorithm 1.
Algorithm 1 find subgraphs(G , m)
1: nontrivial biconnected components G
2: {{v}| v V degree(v) 3 6 : v Vi }
3: Si , j | u, v(minvSi ,uS j distance(v, u)) 2
4:
Sk = Si j {v0 |v0 shortest path(u, v)}
5:
{S \ {Si , j }} {Sk }
6: return
line 1 Algorithm 1, first find nontrivial biconnected components, done
O(|V | + |E|) time (Hopcroft & Tarjan, 1973). Next, add vertices degree three higher
part nontrivial biconnected component set . loop starting
line 3, elements vertices distance 2 joined one subgraph.
Note shortest path Si j line 3 always two join vertices: two
nontrivial biconnected components connected isthmus, vertices connecting
nontrivial component isthmus must common one biconnected component,
455

fiD E W ILDE , ER ORS & W ITTEVEEN

namely nontrivial biconnected component, trivial biconnected component
isthmus edge. Hence, line 3 corresponds point 3 Definition 3 (either u v join vertices,
two join vertices encountered path u v), line 4 corresponds point 3
Definition 4.
note Kornhauser employs slightly different definition subgraph. includes
definition also vertices plank subgraph.
Definition 5 (Plank). Given equivalence class Si corresponding subgraph Si = (Vi , Ei ),
plank unique maximal path G vertex v (V \Vi ) vertex j Si length
1.
Figure 9, subgraph S2 two planks: plank left s2 s3 ,
right plank s4 s5 . Subgraph S3 even three planks: shares plank s5
s4 S2 , also two outgoing edges s5 planks.
result Kornhausers definition, subgraphs overlap. opted include planks
Definition 4, prefer keep separate, useful illustrations Figure 9(b).
3.1.2 SSIGNING AGENTS UBGRAPHS
section, state two important results relation agents subgraphs:
1. Algorithm 2 assigns subgraph agents confined subgraph
planks, i.e., agents reach vertices subgraph planks.
2. Proposition 3 state two adjacent agents assigned subgraph,
exchange positions using swap operation. implies agent assigned
subgraph reach vertices subgraph planks.
implication second result individual subgraphs solvable case goal positions agents assigned subgraph inside subgraph planks. importance
first result determine exactly agents belong subgraph. Note
agents assigned subgraph, agents confined isthmus.
Algorithm 2 assign agents subgraphs(G , A, , )
1: ai
2:
f (ai )
3: Si = (Vi , Ei )
4:
v Vi
5:
m00 number unoccupied vertices reachable Si graph induced V \ {v}
6:
u
/ Vi {u, v} E
0
7:
number unoccupied vertices reachable v (V, E \ {u, v})
8:
((m0 1 m0 < m) m00 1) ai 1 (v) 6=
9:
f (ai ) Si
10:
Follow path u away v assign first m0 1 agents path Si
11:
{u
/ Vi {u, v} E} = 0/ (ai 1 (v) 6=)
12:
f (ai ) Si
13: return f , assignment agents
456

fiC OOPERATIVE ULTI - AGENT PATHFINDING

Algorithm 2 iterates vertices subgraphs, decides whether agent occupying
vertex assigned subgraph. distinguish two types vertices subgraph:
vertices connections vertices inside subgraph, vertices
connected vertices subgraph vertices (which join vertices, Definition 2)
form start plank. Agents former type vertex immediately assigned
subgraph, whereas plank vertices checked whether sufficient empty vertices
move agents plank subgraph.
treatment plank vertices, algorithm viewed refinement Kornhauser (1984), remarks agent assigned subgraph enough blanks
subgraph one side P [the pebble] take P plank. . .. specify values
m0 m00 (Figure 10) encode exactly sufficient free space available agent
moved subgraph. meaning m0 m00 condition line 8 Algorithm 2
explained different cases proof Proposition 1.

a5

a3

a5

a4

a3

a4

S1
a1

u

a2

S1
a1

v

(a) m0 = = 2, meaning a2 assigned (yet) S1 , a1 assigned
S1 .

u

a2

v

(b) m00 = 1, meaning a2 assigned S1 .

Figure 10: Illustration m0 m00 . Unreachable removed parts graph grayed
out.

prove correctness Algorithm 2, let us first see agents assigned
subgraphs example. Figure 11(a) shows agents a1 , a2 , a3 , a4 assigned S1 .
Agents a1 , . . . , a3 clearly inside subgraph, assigned basis line 12. Agent
a4 plank, join vertex, assigned S1 line 10, since m0 = 3.
subgraph S2 (Figure 11(b)), a6 inner vertex, assigned S2 line 12.
Agent a5 occupies join vertex, three empty vertices reached without making use
vertex (a5 ), m00 = 3, a5 assigned S2 line 9. Agents a7 a8 plank
starts join vertex (a5 ); m0 = 3, first two agents plank, starting vertex (a7 ),
assigned S2 line 10.
subgraph S3 (Figure 11(c)), a9 a10 (non-join) plank vertex,
m00 = 3, assigned S3 line 10. Note v = (a8 ) (line 4) u = (a7 )
(line 6), m0 = m00 = 0, a8 assigned S3 .
Proposition 1. Algorithm 2 assigns agent ai subgraph j ai confined j
planks.

457

fiD E W ILDE , ER ORS & W ITTEVEEN

a9

a4
a6

a1
a2

S1

S2

a5

a7

a8

S3

a10

a3
(a) Agents a1 , a2 , a3 , a4 assigned S1 (the leftmost subgraph).

a9

a4
a6

a1
a2

S1

S2

a5

a7

a8

S3

a10

a3
(b) Agents a5 , a6 , a7 , a8 assigned S2 (the middle subgraph).

a9

a4
a6

a1
a2

S1

S2

a5

a7

a3
(c) Agents a9 a10 assigned S3 (the rightmost subgraph).

Figure 11: Assignment agents subgraphs.

458

a8

S3

a10

fiC OOPERATIVE ULTI - AGENT PATHFINDING

a3

a3

a4

S1
a1

a2

a4

S1

a5

a1

(a) agent a2 , m00 = 0.

a2

a5

(b) agent a2 , m00 = 2.

Figure 12: Figure 12(a), a2 assigned S1 ; Figure 12(b), a2 S1 , a3 not.

Proof. central idea proof agent reach inside subgraph,
sufficient empty vertices graph move beyond one planks, shown
Kornhauser (1984). consider following four cases:
1. agent ai occupies vertex v inside subgraph j , v plank vertex,
2. agent ai occupies plank vertex w 6 j (i.e., join vertex),
3. agent ai occupies join vertex v start plank (i.e., v j ).
4. agent ai occupies vertex v j planks.
Case 1: inner vertex: first note ai assigned j (line 12 Algorithm 2). reach
vertex assigned j one planks, agent ai must walk one subgraphs
planks. First, agent must take least one step join vertex start plank,
leaving behind one empty vertex. Then, walking end plank, 1 long,
requires 1 empty vertices. step plank, another empty vertex required,
empty vertices total, ai reach end plank.
Case 2: plank vertex w: recall m0 defined as: number unoccupied vertices reachable
v (V, E\{u, v}), v start plank. m0 1 empty vertices
used move m0 1 plank agents subgraph. Consider agent ai number
m0 1 plank: takes m0 1 empty vertices move start vertex plank,
since m0 empty vertices available agents moved, one empty
vertex available ai step plank subgraph. Then, apply
first case show ai cannot leave subgraph planks.
Case 3: join vertex: agent ai cannot always enter subgraph, even agents behind
plank can. Consider Figure 12(a), agent a2 start two planks, let
u = (a1 ). m0 = 2, a2 cannot enter single subgraph S1 , move
way give agents inside subgraph space move. case, m0 = m,
empty vertices behind a2 . Figure 12(b) shows case m0 = m, agent
join vertex assigned subgraph case empty vertices reached S1
without using join vertex. Hence, Figure 12(b) agent a2 assigned S1 since m00 1.
459

fiD E W ILDE , ER ORS & W ITTEVEEN

Note ai assigned j , also confined j planks: empty
vertices directly behind ai (m0 = m), move steps away j , plank
1 edges long.
case agent ai join vertex assigned subgraph, least one
empty vertex used make room ai step subgraph onto another
plank, apply reasoning first case conclude ai confined
subgraph planks.
Case 4: vertex v outside j planks: clearly, ai confined j planks. see
ai wont assigned j , note enter j via planks. planks
1 edges long (all shorter planks connected another part graph), ai would
need least steps reach start plank j . Since empty vertices
graph, empty vertices left enter j , ai therefore assigned
subgraph.

important property subgraphs agent reach vertex subgraph
assigned to. Kornhauser (1984) proved 2-transitivity: two pairs agents a1 , a2 , b1 , b2
assigned subgraph, possible send ai location bi (possibly moving
agents). Section 3.2, prove swap operation always succeed two agents
assigned subgraph, achieves 2-transitivity.
3.1.3 P RIORITIES UBGRAPHS
third stage decomposition process assign priorities agents based membership subgraphs. Algorithm 3 generates partial order subgraphs, agents inherit priority
subgraph assigned to. completeness Push Rotate approach,
necessary differentiate priorities agents assigned subgraph, although
possible order pursue better solution quality. Finally, agents assigned
subgraph receive lowest priority, therefore planned last.
Algorithm 3 generates precedence constraints two subgraphs case one subgraphs contains agent restrict movements agents subgraph. Specifically, given two subgraphs Si j , agent r assigned j , priority relation Si j
added following two cases8 :
1. goal position agent r start vertex plank9 Si ,
2. goal position agent r vertex v0 plank Si , vertices plank
v0 start plank goal positions agents assigned subgraph.
intuition behind cases agent r moved goal position,
empty vertices subgraph Si ; otherwise, first case, agent r would able
8. note notation: coming algorithms, use r denote agents (as Luna & Bekris, 2011b).
examples far used a1 , . . . , ak , agents ai j confusing combination subgraphs
Si j .
9. Recall start vertex plank vertex plank belongs subgraph.

460

fiC OOPERATIVE ULTI - AGENT PATHFINDING

a7

S3

a3

a1

a10

a2

S2

a8

S4

S1
a4

a5

a4
a9

a6

S1
a1

(a) S4 S3 due agent a5 .

S2
a2

a3

a5

(b) S2 S1 due agent a2 .

Figure 13: agents one subgraph goal location plank another,
might precedence relation subgraphs.

move Si would part Si , Proposition 1, contradiction (in second
case: agents assigned subgraph could move Si , would assigned Si ).
agents subgraph Si move goal locations, empty vertices must brought
Si . case r moved goal, possible: push operation allowed,
swap operation possible, Proposition 3. Hence, subgraph Si must higher priority
agent r, therefore subgraph Si j .
example first case shown Figure 13(a), agents start location
equals destination location, except agents a8 a9 want exchange position. a5
higher priority a8 a9 , algorithm based operations push swap
succeed: plan a8 , push may move a5 aside since a5 higher priority; swap
fail a5 a8 different subgraphs, therefore cannot swap. Hence, must add
priority relation S4 S3 . example second case shown Figure 13(b),
plank vertex S2 occupied a3 . example, solution using push swap
found case a2 higher priority a4 a5 . agents a4 a5 planned,
allowed move agent a3 assigned subgraph away push,
agent a2 behind may moved push operation. attempted swap a3
one agents S2 fail a3 S2 . Hence, priority relation S2 S1 must
added.
Proposition 2. priority relation subgraphs cyclic, instance solvable.
Proof. Suppose way contradiction two subgraphs Si j j Si ,
due agent r Si , Si j , due agent j .
definition subgraph, one connection two separate subgraphs,
goal locations r isthmus. agent r induce j Si , either
reach start vertex v (from perspective j ) isthmus, may agents
assigned j (so s) v. Hence, agent may occupy vertex
isthmus r vertex v. way induce Si j would goal location
behind (i.e., side Si ) goal location r.
461

fiD E W ILDE , ER ORS & W ITTEVEEN

Hence, assumption priority relation cyclic implies goal locations r
swapped, however Kornhauser proved agents assigned subgraph
swap positions (Kornhauser, 1984, p. 11).
example Proposition 2, consider agents a1 a2 Figure 13(a). destination
location agent a1 current location a2 , precedence constraint S3 S1 would
added. Similarly, destination a2 current location a1 , S1 S3 .
configuration, agents a1 a2 swapped location, since assigned different
subgraphs, possible, Proposition 3.
Algorithm 3 subgraph priority(G , , , f )
1: Si = (Vi , Ei )
2:
v Si
3:
u
/ Vi {u, v} E
4:
Vertex u first vertex path Si another subgraph j , otherwise
continue next u
5:
v0 v
6:
r : (T (r) = v0 ) ( f (r) 6= Si )
7:
f (r) = j
8:
Si j
9:
Continue next v (line 2)
0
10:
v next vertex path Si j
11: return priority relation
Algorithm 3 checks every subgraph Si , every join vertex v Si connected node
u 6 Vi path (i.e., isthmus) component j , whether exists agent r assigned
j restricts movements agents assigned Si . order precedence constraint
Si j added, agent r either occupy v, occupy node v0
nodes10 (not including) v0 v (including) goal locations agents assigned
subgraph. either case, agent r, planned first, would bottle agents assigned Si , hence
latter agents receive higher priority: Si j .
final note agents assigned subgraph. Although Algorithm 3 assign
priority agents, agents planned agents assigned subgraph.
3.2 Operations push, swap, rotate
Section 3.3 describe operation solve moves agents destinations,
possible, section first present main operations used algorithm: push,
swap, rotate. push swap operations conceptually similar operations presented Luna Bekris (2011b), rotate operation moves agents cycle one step
forwards. rotate operation require agents move simultaneously.
note notation: algorithms assume parameters passed reference,
algorithm subroutine called see changes made parameter
subroutine. Below, stands set generated moves, U stands set blocked
10. vertices v v0 empty , agents Si would still room maneuver,
even r goal location.

462

fiC OOPERATIVE ULTI - AGENT PATHFINDING

vertices, i.e., vertices algorithm may use (any agents vertices therefore
remain place).
Algorithm 4 push(, G , , r, v, U )
1: vertex v occupied
2:
U 0 U {A (r)}
3:
clear vertex(, G , , v, U 0 ) = false
4:
return false
5: move(, , agent r vertex v)
6: return true
push operation attempts move agent r, currently location (r), location v,
assumed adjacent (r). successful, move recorded current sequence
agent moves (line 5). case location v occupied push called, clear vertex
operation used try clear v. line 2, prior calling clear vertex, current location
agent r added set U locations agents may moved.
specification clear vertex (Algorithm 10) moved Appendix (as
well auxiliary algorithms), disrupt flow text much.
clear vertex find shortest path v unoccupied node u graph induced
V \ U ; path exists, agents path moved one place towards u.
Algorithm 5 swap(, G , , r, s)
1: {vertex x f (r) | degree(x) 3}
2: vertex v
3:
A0
4:
0 [ ]
5:
multipush(0 , G , 0 , r, s, v) = true
6:
clear(0 , G , 0 , r, s, v) = true
7:
+ 0
8:
A0
9:
exchange(, G , , r, s, v)
10:
reverse(, , 0r/s )
11:
return true
12: return false
swap operation attempts exchange locations two adjacent agents r s, moving
vertex degree three higher, performing exchange operation (see Figure 7
Algorithm 13), moving agents back swap initiated (at time
reversing moves agents involved line 10, 0r/s stands sequence new
moves roles r reversed). vertices degree three higher belong
subgraph agent r (denoted fS (r)) eligible perform swap (we evaluate vertices
closest r first). candidate swap node v, multipush operation (Algorithm 11,
Appendix A) attempts bring agents v (line 5). Since moves swap reversed
(with exception exchange operation, roles r reversed), multipush
take account set blocked agents U ; agents may moved well.
463

fiD E W ILDE , ER ORS & W ITTEVEEN

multipush operation essentially series push operations, iteratively moving agents r
step closer v, moving agents way.
v0

a3
a4

v0

a3

a2

a4

a2

v
a5

a6

v
ax

a1

ay

a5



a8

a1

a6

a7

ax



ax



a8
a7

(a) Initial configuration

(b) a1 moves

v0

a3

v0

a3

a4

a4
v

a5

a2

a6

v
a1

ax

a5



a8

a1

a6

a7

a8
a7

(d) agents rotate, starting a3

(c) a2 moves forward, swaps a1
v0

a4
a5

v0

a4
a5

a3

a3

v
a6

v
a2

a7

a2

ax

a6



a2

a7

a1

ax

ay



a1
a8

a8

(e) a2 moves back cycle

(f) Goal configuration

Figure 14: steps rotate operation.

multipush succeeds, next step Algorithm 5 try ensure v two empty
neighbors, calling clear operation. idea behind clear push agents vertices
adjacent v way towards empty vertices, exact specification quite intricate, due
possible movements r considered order allow agents
reach unoccupied vertices. specification explanation clear operation given
Appendix A. state result swap operation succeed two agents assigned
subgraph (the proof Appendix B).
Proposition 3. two agents r adjacent vertices G , operation swap(, G , , r, s)
succeed r assigned subgraph Si .
464

fiC OOPERATIVE ULTI - AGENT PATHFINDING

rotate operation assumes cycle c locations, moves agents within cycle
forward one step. case c fully occupied, performing rotate trivial operation; otherwise, one agent must first moved cycle provide room agents move.
steps rotate operation illustrated Figure 14. Figure 14(a), see agent
a1 temporarily pushed cycle make room others. matter
whether agents ax ay , others behind them, higher priority, since moves
performed move a1 cycle reversed.
Figure 14(c), agent a2 moves forward v0 , swap operation agents a1 a2
ensures a1 a2 right position rotate. Figure 14(d) depicts actual
rotation agents, a3 moving forward first, followed a4 , etc. Figure 14(e) agent
a2 moves back cycle complete rotate, Figure 14(f) goal configuration
reached returning agents (ax ay figure) previous locations.
Algorithm 6 rotate(, G , , c)
1: vertices v c
2:
v unoccupied
3:
Move agents c forward, starting agent moving v
4:
return true
5: vertices v c
6:
r (v)
7:
0 [ ]
8:
clear vertex(0 , G , , v, c \ {v}) = true
9:
+ 0
10:
v0 vertex c v
11:
r0 (v0 )
12:
move(, , agent r0 vertex v)
13:
swap(, G , , r, r0 )
14:
Move agents c forward, starting agent moving v0
15:
reverse(, , 0r/r0 )
16:
return true
17: return false
specification rotate operation given Algorithm 6. Lines 1 4 deal
trivial case cycle least one location v unoccupied: agent going v moved
first, room agents behind move turn. main body
algorithm, line 5 iterates vertices cycle, looking vertex v cleared
(Lemma 2 proves that, solvable instance, vertex always found), line 8.
clear vertex succeeded, algorithm proceeds steps illustrated
Figure 14; lines 10 12, next agent moved vacated vertex, line 13 agent
swaps agent moved cycle (Lemma 2 proves swap possible
solvable instance). Line 14 rotates agents, line 15 necessary moves reversed.
3.3 Push Rotate Algorithm
Algorithm 7 present Push Rotate algorithm. first determines division
subgraphs, determines assignment agents subgraphs initial goal
465

fiD E W ILDE , ER ORS & W ITTEVEEN

Algorithm 7 Push Rotate(G , A, , )
1: |V | |A|
2: find subgraphs(G , m)
3: f assign agents subgraphs(G , A, , )
4: f 0 assign agents subgraphs(G , A, , )
5: f = f 0
6:
subgraph priority(G , , , f )
7:
return solve(G , A, , , , f , )
8: return false

assignment agents. assignment functions f f 0 same, instance
feasible algorithm returns false. Otherwise, main algorithm solve called
returns sequence moves transforming initial agent assignment goal assignment.
idea behind solve operation move agents destinations one one,
single agent moved destination one step time, along shortest path destination.
every iteration, first push tried move agent next location path, push
fails, swap performed, succeed (as proved Theorem 1, provided
least two unoccupied vertices) solve called feasible instances.
agent moved destination, added F , set finished agents.
path traversed encoded q, shown Figure 15(b). Since push operation
allowed move agents F , agents moved swap operation (line 22)
rotate operation (line 19). agent F moved destination former
operation. refer agent resolving agent, returned destination.
resolving agents contained q, swap occurs along path agent
destination, added q. Note agent F moved rotate
operation, actually returned destination. second stage (lines 26 35) solve
operation aims return resolving agents destinations, shrinking q.
follows detailed description Algorithm 8. First, initialization done:
sequence moves initialized empty list [ ] (line 1), path q resolving agents
(line 2). set finished agents F initially empty (line 3), r, pointer agent
selected iteration, initially undefined, denoted (line 4). line 5, check whether
input graph G polygon. so, polygon gets value true, otherwise false.
outer loop line 6 iterates set finished agents equals set agents.
next agent selected yet (r =, line 7), line 8 choose next agent
joint-highest priority agents \ F . Note that, r 6= line 8, mean
agent selected line 30 part resolving agents q.
Next, must choose path along r moved destination. graph polygon
(is polygon = true, line 9), must choose path encounter finished agents
(line 10) swap possible polygon instance otherwise simply choose shortest
path (line 12).
inner loop line 14 move r closer destination, one step iteration.
next step path r also q, detected cycle resolving agents,
466

fiC OOPERATIVE ULTI - AGENT PATHFINDING

Algorithm 8 solve(G , A, , , , f , )
1: [ ]
2: q [ ]
3: F 0/
4: r
5: polygon v V : degree(v) = 2
6: F 6=
7:
r =
8:
r next agent(A \ F , )
9:
polygon
10:
p shortest path(G , (r), (r), (F ))
11:
else
/
12:
p shortest path(G , (r), (r), 0)
13:
q q + [A (r)]
14:
(r) 6= (r)
15:
v vertex (r) p
16:
v q
17:
c get cycle(v, q)
18:
q qc
19:
rotate(, G , , c)
20:
else
21:
push(, G , , r, v, (F )) = false
22:
swap(, G , , r, 1 (v))
23:
q q + [v]
24:
F F {r}
25:
r
26:
|q| > 0
27:
v last vertex q
28:
1 (v)
29:
F v 6= (s)
30:
r 1 (T (s))
31:
r =
32:
move(, , agent vertex (s))
33:
else
34:
Break inner loop, continue outer loop
35:
q q [v]
36: return

move r agents involved cycle q11 forwards performing rotate operation,
line 19. Otherwise, attempt move agent r forwards push (line 21), fails,
swap operation (line 22). either push swap succeeds, append vertex v
11. list q appended v consist cycle simple path emanating cycle, shape q,
less, get cycle method line 17 returns cyclic part q.

467

fiD E W ILDE , ER ORS & W ITTEVEEN

agent r moved to, q (line 23). loop line 14 completed, agent r
added set F finished agents (line 24), r reset (line 25).
a2

a3

a1

a4

a5

a7

a6

a8

a4

(a) shortest path a2 destination vertex
destination vertex a1 .

a4

a1

a5

a3

a6

a7

a1

a5

a3

a6

a7

a8

a2

(b) state directly a2 reached destination vertex, q highlighted.

a8

a2

a4

(c) a1 F encountered q, hence r a5
next iteration algorithm.

a1

a6

a3

a7

a8

a5

a2

(d) q continue grow a5 moves
destination vertex.

Figure 15: example q changes agents moved destination vertices.

moving agent r destination, loop starting line 26 iterates
locations q see whether agents F need returned goal location,
starting last location v q. v contains finished agent (line 29), agent
moved goal, check line 31 whether goal location occupied another
agent r. agent r, simply move agent back goal location; otherwise,
line 34, break loop line 26, thereby starting new iteration loop
line 14 agent r.

4. Analysis Push Rotate
section, first prove correctness completeness Push Rotate,
analyzing computational complexity section 4.1. correctness proved Theorem 1,
makes use Proposition 3, lemmas involving push rotate operations (the
proofs Lemma 1 Proposition 3 quite long moved Appendix B).
following lemma proves that, solvable instance, push operation fails,
two agents involved must belong subgraph (and, Proposition 3, swap
operation succeed).
Lemma 1. Suppose push (Algorithm 4) called context Algorithm 7 agent r moving
vertex v. push succeed, r = 1 (v) assigned subgraph.
next lemma shows rotate operation sound.
Lemma 2. instance (G , A, , ) least two empty vertices, rotate operation
moves agents cycle forward one step.
468

fiC OOPERATIVE ULTI - AGENT PATHFINDING

Proof. Consider Figure 14; see rotate operation always succeed given two empty
vertices, note following:
least two empty vertices G , find path p v empty
vertex. Since operations required clear v reversed end rotate
operation, always possible push agents along p.
Since agents cycle assigned subgraph, agents a1 a2 swap.

Theorem 1. Push Rotate complete class Multi-agent Pathfinding problems
least two empty vertices.
Proof. proof focus correctness Algorithm 8, solve. approach
Algorithm 7 dividing graph subgraphs solving subgraphs sequentially
sound proven Kornhauser (1984). Here, prove Algorithm 8 returns solution
one exists, false otherwise. idea behind proof is:
1. iteration solve agent selected F (line 8), agent
added F .
2. case finished agent moved goal location, current location
adjacent goal location, current location path q.
3. finite number iterations, vertices q processed, restoring outof-position agents F goal location.
prove first point, consider agent r while-loop line 6. First shortest path p
determined goal. graph polygon (is polygon = true, line 5), shortest path
found graph G \ A[F ]. iteration while-loop line 14, agent r moved v,
next vertex p:
v q, cycle C q, q constructed vertices already
visited (line 23). rotate operation move agents C one step forward (i.e.,
direction C). Lemma 2 shows rotate operation possible. result
rotate agents F 1 (C) returned goal positions (since swap
moved agents one step backwards along q) agent r moves v. Also, q
updated cycle removed.
push succeeds agents r s, agent pushed way, agent r
move v.
Otherwise swap operation executed. push succeed r s, then,
due Lemma 1, fS (r) = fS (s), turn implies swap succeed
(Proposition 3), hence agents r swapped successfully.
prove second point, note agent F moved goal location
result swap operation, moves location adjacent goal location. see
469

fiD E W ILDE , ER ORS & W ITTEVEEN

cannot moved away12 goal location subsequent operations, note
agents q moved back destination agent r, agent swapped,
reached destination. If, processing q, current location v encountered
again, rotate operation performed, returning destination.
prove third final point, consider agent moved goal position
(s). Then, line 30, assign r agent occupying goal position s.
agent, return goal position using single move. Otherwise, break loop
iterates q thus number iterations finite start new iteration loop
line 6, agent r (s). agent r added F , new processing loop
q started, occurs |A| times.
4.1 Runtime Analysis
Let k denote number agents instance n number vertices roadmap.
order solve instance, agent needs sent goal position, along shortest path
length n.
step along shortest path, rotate, push swap operation performed.
operations, runtime swap operation dominant13 , simplifies following
equation:
tsolve = O(k n tswap )
swap tries multipush clear v sufficient degree. analysis may
show limited (O(1)) number vertices need checked (indeed, behavior
observed experiments), following:
tswap = O(n (tmultipush + tclear ))
operations require O(n tclear vertex ) time. clear vertex operation find
free vertex simple breadth-first search, resulting O(|V | + |E|) = O(n2 ). leads
following runtime complexity solve operation:
tsolve = O(n5 k)
4.2 Solution Quality
k agents move along (shortest) path (of length n) towards goal position.
like runtime analysis, swap operation (Algorithm 5) dominant factor output
solve operation (Algorithm 8), push moves k agents along path length
k (as runtime analysis, worst-case performance rotate determined call
swap). yields following expression number moves solve operation
outputs:
lsolve = O(k n lswap )
swap operation moves two agents vertex v using multipush (Algorithm 11),
clears two neighbors v. many different attempts made clear (Algorithm 12)
12. Note operation swap operation, agents may temporarily moved way,
moves reversed end swap operation.
13. Actually, rotate calls swap, worst-case running time also determined call swap.

470

fiC OOPERATIVE ULTI - AGENT PATHFINDING

operation clear two neighbors v, total output moves constant times
number moves generated clear vertex (Algorithm 10) plus constant number
moves: O(lclear vertex ), O(n), clear vertex pushes agents backwards along path
empty vertex. Since multipush operation executes clear vertex step along
way, output dominant factor.
lsolve = O(k n lmultipush )
lsolve = O(k n2 lclear vertex )
lsolve = O(k n3 )
Note swap operation makes little progress (it advances agent one step) achieve
O(n3 ) bound shown possible Kornhauser (1984). Section 6, investigate extent higher number moves worst case manifests practice,
comparing algorithm Bibox algorithm (Surynek, 2009), achieve O(n3 )
bound number moves required (although Bibox works biconnected graphs).

5. Heuristics Post-Processing
Push Rotate guarantee optimal solution, try improve solution quality
heuristics, processing initial solution. Section 5.1, discuss postprocessing step detects unnecessary moves, first discuss points Push
Rotate algorithm heuristic might used affect solution quality (in order
appear text):
1. push operation (Algorithm 4), clear vertex operation (Algorithm 10)
called, must choose empty node move agents into.
2. Similarly, swap operation (Algorithm 5) must choose vertex v degree 3
perform swap.
3. Algorithm 8 (solve), order agents planned decided line 8.
4. Also Algorithm 8, p assigned shortest path destination, line 12;
multiple shortest paths, one must chosen.
far, considered heuristics third fourth points. regard
first two points, consider empty vertices possible swap locations specific order,
different plan found assuming multiple nodes viable. regards third point,
agent ordering important solution quality shall show Section 6. Note
agent ordering partially determined Algorithm 3, introduces precedence constraints
subgraphs. Within subgraph, heuristic used order agents.
agent-priority heuristic aims limit number swap operations required.
heuristic first determines diameter graph, chooses two vertices
distance exactly diameter. first move agents away one vertices14 , ensuring
empty spaces vertex vertices closest it. Agents ordered based
14. moves produced pre-processing part solution.

471

fiD E W ILDE , ER ORS & W ITTEVEEN

distance destination vertex, farthest agents receiving highest priority.
idea that, subset agents moved destinations, empty spaces
never get stuck behind finished agents. Remember push operation allowed
move agents higher priority. long empty spaces reached, push operation
performed, much cheaper otherwise required swap operation.
fourth point, implemented heuristic finds path minimum number agents F (if exist multiple paths, select shortest one). Again, aim
avoid use costly swap operation.
5.1 Removing Redundant Moves
Push Rotate algorithm capable outputting redundant moves. example shown
Figure 16: part swap agents a1 a2 , agent a3 moved goal location
(in particular, clear operation move agent a3 goal location, see Figure 16(b)). Since
moves generated clear operation executed reverse exchange
position agents a1 a2 , agent a3 moved back initial position (Figure 16(d)).
Finally, agent a3 planned moved goal location second time. Clearly,
final two moves sequence redundant.
a3

a3

a2


a1

a2

a3




a1

(a) initial state

a2

(b) a3 makes room a2

a1

(c) a1 a2 swap

a3

a3


a2

a1

a2

(d) a3 put back

a1

(e) a3 moves destination

Figure 16: Example redundancy: swap a1 a2 , a3 moved back.
sequence agent moves redundant agent visits vertex second time,
agents visited vertex meantime. redundant moves removed
solution, moves may become redundant too. approach taken Luna Bekris (2011a)
keep iterating moves solution, redundancies discovered.
order remove redundancies efficiently, Algorithm 9 uses linked-list-like structure,
encoded following functions. PA : (previous-agent) NA : (next-agent)
functions back forward pointers respectively form doubly-linked chain moves
specific agent. PV : (previous-vertex) NV : (next-vertex) functions
similar pointers form chain moves specific vertex.
Algorithm 9 starts single pass (lines 2 4) moves find redundancies. redundant moves added Q, processed loop starting line 5. First, line 6,
472

fiC OOPERATIVE ULTI - AGENT PATHFINDING

Algorithm 9 smooth()
1: Q empty queue
2: (a, v) =
3:
agent(PV ()) =
4:
Q.add(PV ())
5: |Q| > 0
6:
retrieve remove next element Q
7:
0 NA ()
8:
0 6= NV ()
9:
\0
10:
PA (NA (0 )) PA (0 ), NA (PA (0 )) NA (0 )
11:
PV (NV (0 )) PV (0 ), NV (PV (0 )) NV (0 )
12:
agent(PV (0 )) = agent(NV (0 ))
13:
Q.add(PV (0 ))
14:
0 NA (0 )
15: return

redundant move = (ai , v) retrieved Q. redundant, means agent ai plan
contains number moves vertices returning vertex v without agents
visited v between. means sequence [, NA (), NA (NA ()), , NV ()]
moves except first removed; achieved loop line 8.
Within loop line 8, first redundant move removed sequence moves
(line 9), pointers PA NA , PV NV updated lines 10 11 respectively.
Finally, checked whether moves become redundant result removal,
line 12. check done constant time, require looping moves again,
algorithm Luna Bekris (2011a).
5.2 Executing Moves Parallel
makespan multi-agent plan difference time moment agents
stopped moving moment first agent started moving. Push Rotates initial output
list agent moves, specification agent moves performed
time, makespan initially equal number moves. section, briefly discuss
condense function15 , tries reduce makespan executing many agent moves
parallel possible.
distinguish three models regard allowed degree parallelism. least
restrictive model allows agent move vertex another agent moving away.
This, however, Multi-agent Pathfinding problem defined Section 2, allows agent
movement even empty vertices. restrictive model agent
allowed move empty vertex, means time step, moves
performed parallel, number empty vertices. currently employ
restrictive model.
15. include algorithmic description paper condense. Although algorithm complicated, still somewhat lengthy write down.

473

fiD E W ILDE , ER ORS & W ITTEVEEN

intermediate model, also supported condense function, agents allowed
move vertices vacated, long head chain moving agents,
empty vertex. Note degree parallelism conflict Multi-agent Pathfinding
problem: still possible serialize moves agent always moves empty
vertex.
idea behind condense function follows: every time step starting first,
inspects empty vertices, places moves going empty vertex time
step. case intermediate model employed, empty-vertex variables updated
vertices vacated, checked whether agents moving
vertices. process continues moves emptying vertices found.

6. Experiments
section compare performance Push Rotate MAPP, Push Swap,
Bibox. previously compared MAPP Push Rotate game map16 Baldurs
Gate II (De Wilde et al., 2013), repeat experiment (on different map).
experiments game maps preliminary lend comparisons
algorithms: MAPP produces many moves, Push Swap requires lot computation
time, Bibox cannot solve instances biconnected.
Push Swap conceptually similar Push Rotate, would expect performance
similar also. addition, apart game map, experiments conducted
biconnected instances (to allow comparison Bibox), fact Push Swap
take account subgraphs affect success ratio instances. However, turned
source incompleteness Push Swap, namely fact recursive calls
swap may fail solved introduction rotate operation resulted
Push Swap solving fraction problem instances.
main focus experiments therefore Bibox. Bibox complete, requires
biconnected graph, performs well compared approaches (Surynek, 2009). Moreover,
achieves bound O(n3 ) number moves, Kornhauser (1984) showed
lowest worst-case bound achievable general graphs. Push Rotate, contrast, bound
O(kn3 ) number moves, k number agents. compared Bibox
two types instances: instances random generator part Bibox code,
grid instances.
experiments, run Push Rotate without agent-ordering
heuristics Section 5. effectiveness heuristic shown fact almost
always better, terms number moves therefore also terms CPU time, use
agent-ordering heuristic Push Rotate.
6.1 Map AR0603SR Baldurs Gate II
problem instances benchmark set characterized large set vertices, many
unoccupied. map chose experiments 13765 vertices,
100 2000 agents (step size 100).
16. set benchmark maps video game industry available http://movingai.com/benchmarks/.

474

fi
































































0












500

1000

MAPP
Push & Rotate
P&R noorder
PAS

200






400



























CPU time (s)

500000 1000000










800







600

MAPP
Push & Rotate
P&R noorder
PAS




0

number moves

2000000



1000 1200 1400

C OOPERATIVE ULTI - AGENT PATHFINDING

1500

2000













500

number agents




























1000













1500
















2000

number agents

(a) number moves

(b) CPU times

Figure 17: Comparison map AR0603SR Push Rotate, Push Swap (PAS), Push
Rotate without agent-ordering heuristic (P & R no-order) MAPP.

Figure 17 shows number moves produced Push Rotate, MAPP, Push Swap
(no Bibox, map biconnected), CPU times. Push Rotate Push
Swap produce efficient plans (which fact always within percent lower
bound sum shortest paths), MAPP requires many moves find solution,
therefore try include experiments. Note instances,
matter whether agent-ordering heuristic employed Push Rotate.
Figure 17(b) interesting note Push Swap quite bit slower type
instance, instances remainder experiments section (grids
biconnected instances) C++ implementation Push Swap (when find solution)
often little bit faster Java implementation Push Rotate.
6.2 Random Biconnected Instances
Suryneks code17 generates random instances iteratively adding handles initial cycle (see
Section 2.1) according three parameters: number handles, size initial cycle,
maximum handle length (where length next handle uniformly chosen 0
maximum handle length minus one, set 1 equals 0). addition, one choose
number empty vertices, default value 2. experiments measure number
moves, CPU time, makespan, number time steps required get
agents destination (in one time step agent perform one move, though multiple agents
move one time step).
17. used code available http://ktiml.mff.cuni.cz/surynek/research/icra2009/.

475

fi200

400

600

800

0

200



Bibox
Push & Rotate
P&R noorder
PAS

400

CPU time (s)

3e+06
2e+06
0e+00











































































































































































































































































































































































































































































































































0



600

Bibox
Push & Rotate
P&R noorder
PAS



1e+06

number moves

4e+06



800

E W ILDE , ER ORS & W ITTEVEEN

1000
































































































































































































































































































































































0

number vertices

200

400

600

800



1000

number vertices

(a) number moves

(b) CPU time

Figure 18: Comparison random instances parameters (handles, initial cycle, max handle
length): (20, 20, 20) (50, 50, 50), 2 empty vertices.

Figure 18 compares Push Rotate, Push Swap Bibox instances generated according single variable x ranged 20 50, steps two, used three
parameters (number handles, initial cycle size, maximum handle length), number
empty vertices kept two. Clearly number moves required Push Rotate rises
much quickly instance size, consequently CPU times increase well. results
Push Rotate slightly better terms makespan, still much worse Bibox.
also see figures agent-ordering heuristic useful, essential
performance Push Rotate. Push Swap managed solve 1.17% instances,
really usable hard instances; later shall see performs better
empty vertices.
Bibox works exceptionally well large handle sizes, unsurprising since algorithm
based concept filling handles: Bibox inserts vertices destination handle
right order, destination, vertices moved much. Push Rotate,
hand, bring agents place using push swap; swap two agents middle
handle, many agents moved way order bring node
swap. P & Rs rotate operation rarely used type instance, path resolving
agents rarely intersects itself.
Figure 19 shows experiments random biconnected instances 40 handles, initial cycle
size 5, maximum handle length 10, increasing number empty vertices, ranging 2
40 (step size 2), 180 instances per step. hardest instances, empty vertices, Push
Rotate still produces costly plans, higher number empty vertices, Push
Rotate produces much better plans. instances easier Push Rotate, first
476

fiBibox
Push & Rotate
P&R noorder
PAS

12000
number moves



















5000



10

20













30







Bibox
Push & Rotate
P&R noorder





40

2000 4000 6000 8000

15000



10000

number moves

20000

C OOPERATIVE ULTI - AGENT PATHFINDING









10

number empty vertices











20











30











40

number empty vertices

(a) average number moves

(b) average makespan

Figure 19: Comparison random instances 40 handles, initial cycle 5, maximum handle
size 10.

push operation succeed often, swap still necessary, easier clear
node two agents swap.
Bibox, hand, requires exactly two empty vertices current implementation,
fills remaining empty vertices dummy agents. Even though moves involving
dummy agents removed final solution, Bibox manage benefit
empty vertices extent Push Rotate does. Bibox still produces solutions quickly,
however, requiring tenths second instance; Push Rotate requires around 6
seconds solve hardest instances, around 1 second easiest ones. Figure 19 suggests
Push Swap also performs well, although noted success ratio
Push Swap still 53.9% experiments.
terms makespan (Figure 19(b)), performance advantage Push Rotate Bibox
even greater18 . something observed experiments comparing Push Rotate
Bibox. main reason seems condense powerful counterpart
Bibox algorithm. Table 1 shows parallelism, number moves divided
makespan, Push Rotate, Bibox, Bibox plus smooth (removing redundant moves)
condense functions (Bibox++). Without using post-processing algorithms, Bibox
poor parallelism, applying smooth condense parallel Push
Rotate emptier instances. However, Bibox still moves, makespan Push
Rotate Bibox++ equal average.
18. implementation Push Swap kind condense feature, makespan would
simply number moves, include figure.

477

fiD E W ILDE , ER ORS & W ITTEVEEN

Empty
P&R
Bibox
Bibox++

2
1.42
1.03
1.22

6
1.86
1.07
1.49

10
2.10
1.10
1.69

14
2.25
1.12
1.88

18
2.35
1.15
2.05

22
2.42
1.18
2.25

26
2.48
1.20
2.44

30
2.54
1.23
2.64

34
2.57
1.26
2.83

38
2.63
1.29
3.07

42
2.67
1.33
3.29

46
2.71
1.36
3.5

50
2.74
1.39
3.69

Table 1: Parallelism Push Rotate, Bibox, Bibox plus smooth condense, biconnected Figure 19.

6.2.1 NITIAL C YCLE
Bibox algorithm solves handles instance inserting agents one endpoint
handle, using lower part graph (i.e., initial cycle handles lower
number) move agents empty vertices around. initial cycle, different procedure
followed swap-like operation used exchange position agent vertex
agent whose destination i.

20

40

60

80

5e+05
4e+05
3e+05

makespan

1e+05
0e+00

1500000
1000000

makespan

500000
0

Bibox
Push & Rotate
P&R noorder
Bibox++

2e+05





















































































































































































































































































































































































100
























































































































































































































20

number vertices



Bibox
Push & Rotate
P&R noorder
Bibox++

40

60

80

100

number vertices

(a) initial cycle: {4, . . . , 100}, max handle size: 4

(b) initial cycle: 4, max handle size: {4, . . . , 100}

Figure 20: Makespan comparison instances initial cycle one handle, two empty
vertices.

Figure 20 shows, however, initial-cycle procedure efficient processing
handles, indeed Push Rotate19 . Figure 20(a) shows comparison instances
length initial cycle varied 4 100, single handle maximum
size 4. Figure 20(b) shows experiments similar types graphs, small initial
cycle 4 vertices, maximum handle length varies 4 100. number empty
vertices figures two. first setting, Push Rotate performs considerably better
19. Due scripting error, experiments handle size 80 run Bibox(++).

478

fiC OOPERATIVE ULTI - AGENT PATHFINDING

Bibox, second setting relative performances similar experiments
random instances. experiments, removing redundant moves solution Bibox
using smooth algorithm removed small percentage moves, trying reduce
makespan condense result significant reduction. However, even condensed Bibox
output far worse, large initial cycles, plans produced Push Rotate.
interesting note Bibox produces excess n3 moves instances large
initial cycle produces around two million moves 100 vertices yet CPU times less
second instances. Push Rotate, contrast, CPU times grow number
moves; around 5 seconds 50 vertices, 70 seconds instances 100 vertices.
6.3 Grid Instances

2000000

Bibox
Push & Rotate
P&R noorder

1000000

makespan

500000

1500000



0

500





1000




















0

0



1500000

Bibox
Push & Rotate
P&R noorder
PAS

2500000



500000

number moves

3500000

problem instance, Bibox requires specification handle decomposition graph.
grid instances generated, adopted decomposition suggested Surynek (2011):
initial cycle consists four vertices top left corner grid, handles added
first right initial cycle, initial cycle, finally remaining vertices
added one one, top left bottom right.

1500

2000

2500





0

number vertices

500





1000









1500










2000







2500

number vertices

(a) number moves

(b) makespan

Figure 21: Comparison grid instances sizes 4 4 50 50, 2 empty vertices.
Figure 21 shows grid instances two empty vertices, performance Push
Rotate relies heavily agent-ordering heuristic. heuristic enabled, Push
Rotate performs consistently better Bibox; without it, performance usually much worse.
set experiments, Push Swap solved 31.9% instances, beyond
grid sizes 25 25.
also conducted experiments increasing numbers empty vertices, 24 24 grid
instances. problems turn relatively easy Push Rotate, Bibox unable
leverage increased freedom movement, produces quite costly plans. Figure 22(a) shows
479

fi













































































































































































































10

20

30

40

Bibox
Push & Rotate
P&R noorder

20000



















140000
makespan







100000

Bibox
Push & Rotate
P&R noorder
PAS

60000



150000
100000
50000

number moves

200000

E W ILDE , ER ORS & W ITTEVEEN

50

10

number empty vertices

20

30

40

50

number empty vertices

(a) number moves

(b) makespan

Figure 22: Increasing number empty vertices 24 24 grid instances.

two empty vertices, Bibox produces around 10% moves; 40 empty vertices,
requires around 35% moves. terms makespan, difference algorithms
even larger: two empty vertices, span Push Rotate solution around 75%
Biboxs solution, 40 empty vertices less 25%. instances proved easy
enough Push Swap, 98.1% instances solved, performance comparable
Push Rotate without agent-ordering heuristic.
Finally, Figure 23 present CPU times algorithms grid instances; Figure 23(a)
experiments increasing grid sizes, Figure 23(b) experiments increasing
numbers empty vertices. First, interesting thing note Push Rotates good performance (heuristic enabled) grids also expressed terms lower CPU times, Biboxs
times grow steadily number moves. Second, algorithms sensitive, terms
CPU times, number empty vertices, even though algorithms produce solutions containing fewer moves.
6.4 Experiment Conclusions
experiments section apparent performance algorithms depends
considerably type problem instance. Bibox well-suited type random instances
generated Bibox program, Push Rotate performs really well grid instances,
long agent-ordering heuristic employed. Although havent able run Bibox
game maps movingai.com, suspect Push Rotate perform better
instances: plenty open spaces (essentially grids), many free vertices, too,
play strengths Push Rotate.
regard algorithms comparison, Push Swap performed reasonably well
plan found, larger instances, especially empty vertices, solution
480

fi

Bibox
Push & Rotate
P&R noorder
PAS













20

30








400

CPU time (s)

40









Bibox
Push & Rotate
P&R noorder
PAS

CPU time (s)



600

800

C OOPERATIVE ULTI - AGENT PATHFINDING

200





10









































































0

0




0

500

1000

1500

2000

2500

10

number vertices

(a) grid sizes 4 4 50 50, 2 empty vertices

20

30

40

50

number empty vertices

(b) 24 24 grids increasing number empty vertices

Figure 23: CPU times grid experiments.

rarely found, practically usable. Similarly, solutions produced MAPP contain
many moves, also guaranteed return solution instances LIDABLE
class. Finally, Push Rotate without agent-ordering heuristic often performs well,
never significantly better employing heuristic, often significantly worse,
instances weve seen reason use it.
regard CPU times, problem domain suits algorithm, CPU times low
grow rate number moves; otherwise, CPU times grow steadily
number moves. exception seems performance Bibox random biconnected
instances small single handle large initial cycle: tenths second, Bibox
manages produce millions moves. noted, however, Biboxs post-processing
algorithms quite rudimentary spend lot CPU time improving solution.
Push Rotate, hand, post-processing algorithms important solution quality,
also make 1% 5% total computation time.

7. Conclusions Future Work
presented complete polynomial-time algorithm multi-agent pathfinding problem. approach similar Luna Bekris (2011b); using simple primitive operations push, swap, and, algorithm, rotate, algorithm constructed easy
understand performs competitively. Although Push Rotate achieve (best
possible) worst-case bound O(n3 ) moves (n number vertices graph), produces
competitive solutions compared Bibox, achieve O(n3 ) bound (Surynek,
2011), restricted biconnected graphs.
481

fiD E W ILDE , ER ORS & W ITTEVEEN

argue Push Rotate currently algorithm choice multi-agent pathfinding
problems. Bibox performs comparably well, applicable biconnected graphs,
moreover algorithm yet mature, instance way handles two
empty vertices, post-processing algorithms comparatively basic. Finally,
work Kornhauser (1984), demonstrated O(n3 ) bound number moves. Surynek
reports implementation Kornhausers work performs considerably worse (in terms
number moves) Bibox algorithm. Moreover, Kornhausers algorithm difficult
understand therefore implement. accessible specification implementation
construction (Roger & Helmert, 2012), contact authors compare
work due course.
useful next step multi-agent pathfinding would develop algorithm guarantees
O(n3 ) bound, performs (at least) competitively Bibox Push Rotate, easy
understand implement. Arguably, Push Rotate ticks two boxes, would
make sense try adapt reach O(n3 ) bound. would probably entail replacing
expensive swap operation, require O(n2 ) moves one agent single step,
make use rotations, already done Bibox algorithm rotate operation.

Acknowledgements
research sponsored SUPPORT project Dutch Ministry Economic Affairs.

Appendix A. Algorithms
appendix present algorithms used defined main text.
A.1 Operation clear vertex
Algorithm 10 clear vertex(, G , , v, U )
1: unoccupied vertex u V
2:
p shortest path G \U u v
3:
p 6=
4:
x0
5:
vertices x path p (in order)
6:
x0 6=
7:
r 1 (x)
8:
move(, , agent r vertex x0 )
9:
x0 x
10:
return true
11: return false
Algorithm 10 called push, multipush, rotate operations, and, successful,
clears vertex v. Algorithm 10 iterates empty vertices u G (line 1), starts
iteration finding shortest path, avoiding blocked vertices U , u vertex v must
cleared (line 2). path found (p 6=, line 3), agents p moved one step
482

fiC OOPERATIVE ULTI - AGENT PATHFINDING

towards u (the for-loop line 5). Note choosing right u line 1 impact
running time clear vertex operation, well total number moves solution
produced Push Rotate.
A.2 Operation multipush
Algorithm 11 multipush(, G , , r0 , s0 , v)
1: r agent r0 s0 closest v
2: agent
3: p shortest path G (r) v
4: p =
5:
return false
6: vertices x path p
7:
vr (r), vs (s)
8:
vertex x occupied
9:
U {vr , vs }
10:
clear vertex(, G , , x, U ) = false
11:
return false
12:
move(, , agent r vertex x)
13:
move(, , agent vertex vr )
14: return true
Operation multipush (Algorithm 11) called swap operation, moves two agents
r0 s0 next other, node v (which node degree three more, r0
s0 swapped). First, shortest path found line 3, algorithm proceeds
move r forwards along path one step time, calling clear vertex next vertex
path non-empty.
A.3 Operation clear
clear operation (Algorithm 12) called swap, attempts clear two neighbors
potential swap node. algorithm works four stages: first stage (Figure 24(a)) simply
attempts push agents, occupying neighbors v, away v. two vertices
cleared stage, operation done. Otherwise, three stages need one unoccupied
vertex next v work. unoccupied vertex stage one, clear operation fails.
second stage (Figure 24(b)) neighbor n required path already empty
neighbor go v. agent n first moved , clear vertex
operation subsequently succeeds (with n, v, v0 blocked vertices), clear succeeds.
third stage (Figure 24(c)) checks whether agent occupying neighbor n v may able
vacate n moving vertex v0 currently holds agent s. tried moving
agent r empty vertex , agent v.
idea behind final stage (Figure 24(d)) may possible create additional space
behind already empty vertex , similar second stage. stage four, however, instead
using direct connection n , agent n tries move v, meaning
r must move backwards make room.
483

fiD E W ILDE , ER ORS & W ITTEVEEN

Algorithm 12 clear(, G , , r0 , s0 , v)
1: r agent r0 s0 v ; agent r0 s0 v ; v0 (s)
2: E {unoccupied n neighbours(v)}
3: |E | 2
4:
return true
5: n neighbours(v)\(E {v0 })
6:
clear vertex(, G , , n, E {v, v0 }) = true
7:
|E | 1
8:
return true
9:
E E {n}
10: |E | = 0
11:
return false
12: vertex E
13: n neighbours(v)\{v0 , }
14:
0 [ ] ; 0
15:
clear vertex(0 , G , 0 , n, {v, v0 }) = true
16:
clear vertex(0 , G , 0 , , {v, v0 , n}) = true
17:
0 ; + 0
18:
return true
19:
break
20: n neighbours(v)\{v0 , }
21:
0 [ ] ; 0
22:
move(0 , 0 , agent r vertex ) ; move(0 , 0 , agent vertex v)
23:
clear vertex(0 , G , 0 , n, {v, }) = true
24:
clear vertex(0 , G , 0 , v0 , {v, , n}) = true
25:
0 ; + 0
26:
return true
27:
break
28: clear vertex(, G , , v0 , {v}) = false
29:
return false
30: move(, A, agent r vertex v0 )
31: clear vertex(, G , , , {v, v0 , (s)}) = false
32:
return false
33: n vertex neighbours(v)\{v0 , } , 1 (n)
34: move(, , agent vertex v vertex )
35: move(, , agent r vertex v) ; move(, , agent vertex v0 )
36: return clear vertex(, G , , , {v, v0 , n})

484

fiC OOPERATIVE ULTI - AGENT PATHFINDING

n

v0


v
r

n




n

v0

x

(a) 1: push agents



v
r




v0

n


x

v

v0



r

(b) 2: move agent (c) 3: agent makes room
empty neighbor



v
r

n
x

(d) 4: agents r clear v

Figure 24: four stages clear operation.

Note stages clear omitted work Luna Bekris (2011b) stages
24(b) 24(c), Luna Bekris effectively considered clear operation trees.
A.4 Operation exchange
Algorithm 13 exchange(, G , , r0 , s0 , v)
1: r agent r0 s0 v
2: agent r0 s0 v
3: (v1 , v2 ) two unoccupied neighbors v
4: vs (s)
5: move(, , agent r vertex v1 )
6: move(, , agent vertex v vertex v2 )
7: move(, , agent r vertex v vertex vs )
8: move(, , agent v)
exchange algorithm illustrated Figure 7 Section 3.

Appendix B. Proofs
Lemma 1. Suppose push (Algorithm 4) called context Algorithm 7 agent r moving
vertex v. push succeed, r = 1 (v) assigned subgraph.
Proof. Note that, push called context Algorithm 7, problem instance solvable,
agents higher priority r already planned for.
prove lemma, prove equivalent statement r assigned
subgraph, push succeed.
Case 1: r assigned subgraph definition, r assigned
subgraph, and, Proposition 3, means r cannot swap agent. Note that,
result assigned subgraph, agent r trapped isthmus (see Figure 25
illustration). agents assigned subgraph higher priority r, therefore
F time call push made20 .
20. Note Algorithm 8, operations called it, make single call push, set
blocked vertices U equal set locations currently hold agent F .

485

fiD E W ILDE , ER ORS & W ITTEVEEN

Subproblem Si






r

Subproblem j
Figure 25: Agent r, trapped isthmus, move (s) using push.

way contradiction, suppose instance solvable yet push succeed. First,
suppose F ; r cannot goal location s, rs goal location must behind
(s). However, possible r reach location, would assigned subgraph
fS (s), Proposition 1, contradiction reached.
Next, suppose 6 F . Since r priority greater equal s, conclude
also assigned subgraph. instance solvable yet push(r, s) succeed,
exists empty vertex beyond s, path G \ (F ) reaches it; i.e.,
path empty vertex blocked agents F . However, would exist assignment 0
would move way r, agents F returned goal location,
swapped empty vertex within beyond subgraph. Hence, would
reach locations would assigned subgraph (S j Figure 25), contradiction
reached.
Case 2: r assigned subgraph fS (r) Since fS (r) 6= fS (s), blocking agent reach
(at most) plank vertex fS (r). Suppose start plank fS (r); then, due
Algorithm 3, fS (r) fS (s). Hence, possible s, agents subgraphs behind it,
F . Therefore, push succeed case instance solvable.
case start plank fS (r), goal location r must plank
fS (r). Due Kornhausers thesis (1984), know r reach vertex fS (r)
planks (the 2-transitivity result). Since agents cannot swap (Proposition 3), agent r
reach goal pushing agent away, push achieves without restrictions. Finally,
note 6 F ; goal position, instance would solvable: r cannot
swap, goal location r cannot beyond (s).
Proposition 3. two agents r adjacent vertices G , operation swap(, G , , r, s)
succeed r assigned subgraph Si .
Proof. First show swap r succeeds agents assigned
subgraph: fS (r) = fS (s) 6= . successful swap, change assignment
positions agents r s. consider assignment assignment 0 ,
positions agents r swapped:


(s) = r
0
(a) = (r) =


(a) otherwise
486

fiC OOPERATIVE ULTI - AGENT PATHFINDING

Hence, subgraph r prior swap must equal subgraph swap. Furthermore,
know Proposition 1 agent confined single subgraph, fS (r) = fS (s).
see swap succeeding implies fS (r) 6= , note swap occurred
vertex v degree 3. Vertex v clearly part subgraph Si , r reached v
two vs neighbors empty, r assigned Si :
1. case r plank: m0 1m0 < m, choosing right u (line 6 Algorithm 2),
see Figure 7.
2. case either r inner vertex: agent assigned Si line 12,
Algorithm 2.
show 6= fS (r) = fS (s) implies swap succeed agents r s. one
agents r occupies vertex v degree(v) 3 two empty neighbor vertices,
agent occupies neighbor vertex v, agents r exchange positions
moving empty vertices one order, exiting order (see Algorithm 13).
Hence, swap succeeds exists vertex v multipush clear also
succeed.
leaves us prove r assigned subgraph, v always
exists. assignment criteria agents subgraphs guarantee that:
1. agent inside biconnected component,
2. agent less steps away vertex within fS (r), degree 3.
case agents inside biconnected component, multipush succeeds trivially. biconnected component two paths pair vertices, always possible bring
r vertex v degree 3 biconnected component (note blocked
vertices multipush considers (r) (s), since moves put agents place
reversed later). Second, clear always succeed reason: always exists
one v degree 3 (at least two) unoccupied vertices reached v
least two paths. Hence, least one path v empty vertices
blocked r s, two neighbors v emptied (possibly r momentarily
stepping aside illustrated Figures 24(c) 24(d)).
case agents inside biconnected component, must show vertex
degree 3 reached, also clear succeed.
case exactly one agent (say r) inside biconnected component, agent not,
r already occupies (join) vertex degree 3. case neither agent inside biconnected
component, agents either two vertices degree 3, occupying
vertices assigned subgraph. Consider two agents two vertices degree
3. 2 vertices two vertices, two occupied
agents r s. Suppose l1 l2 steps required reach vertices, m1
m2 free vertices respective sides agents. leads to:
l1 + l2 4
m1 + m2 =
487

(1)
(2)

fiD E W ILDE , ER ORS & W ITTEVEEN

Assume one vertices unreachable:
l1 > 1 = 2

(3)

vertex reachable:
l2 4 l1

(4)

l2 < 4 (m m2 )

(5)

l2 < m2 4

(6)

agents outside vertices assigned subgraph, assignment criteria agents
subgraphs state must possible agent assigned subgraph enter subgraph
one additional empty vertex remains subgraph.
m1 free vertices






r



v
m2 free vertices
Figure 26: Illustration reachable free vertices.
Consider amount free vertices sides two agents (see Figure 26).
1. 1 free vertices one side two agents > 1 side sufficient
make clearing two vertices easy.
2. case free vertices one side two agents, either agents
subgraph, another vertex v00 degree 3 reachable 2 steps. Moving
agents towards v00 leave 2 free vertices behind v00 1 free vertex behind
trailing agent. means first case applies vertex v00 .
3. exactly one free vertex sides agents implies = 2. means
subgraph either single vertex, biconnected component graph, since
two components joined together Algorithm 1. single vertex case, impossible
agents belong subgraph, moving agent single-vertex
subgraph leave sufficient free vertices reachable required Algorithm 2.
agents belong biconnected component, stage three clear operation applies: possible push agents towards one free vertices,
trailing agent ends vertex v. clears path additional vertex next v
push towards free vertex original position trailing agent.
clears vertices next v.
488

fiC OOPERATIVE ULTI - AGENT PATHFINDING

Finally, see 6= fS (r), note swap requires r adjacent vertex v
degree 3. Clearly, possible r step vertex v one
unoccupied vertices, either Si , part planks Si . Hence, r
confined Si planks Proposition 1.

References
Auletta, V., Monti, A., Parente, M., & Persiano, P. (1999). linear-time algorithm feasibility
pebble motion trees. Algorithmica, 23(3), 223245.
Beaulieu, M., & Gamache, M. (2006). enumeration algorithm solving fleet management
problem underground mines. Computers & Operations Research, 33(6), 16061624.
Calinescu, G., Dumitrescu, A., & Pach, J. (2008). Reconfigurations graphs grids. SIAM
Journal Discrete Mathematics, 22(1), 124138.
Culberson, J. (1999). Sokoban PSPACE-complete. Proceedings Informatics, Vol. 4, pp.
6576. Citeseer.
De Wilde, B. (2012). Cooperative multi-agent path planning. Masters thesis, Delft University
Technology.
De Wilde, B., Ter Mors, A. W., & Witteveen, C. (2013). Push Rotate: Cooperative multi-agent
path planning. Proceedings twelfth international conference autonomous agents
multiagent systems, AAMAS, pp. 8794, Saint Paul, Minnesota, USA.
Desaulniers, G., Langevin, A., Riopel, D., & Villeneuve, B. (2004). Dispatching conflict-free
routing automated guided vehicles: exact approach. International Journal Flexible
Manufacturing Systems, 15(4), 309331.
Erdmann, M., & Lozano-Perez, T. (1987). multiple moving objects. Algorithmica, 2(1), 477
521.
Gawrilow, E., Kohler, E., Mohring, R. H., & Stenzel, B. (2007). Mathematics - Key Technology
Future, chap. Dynamic Routing Automated Guided Vehicles Real-time, pp. 165177.
Springer Berlin Heidelberg.
Goldreich, O. (1993). Finding shortest move-sequence graph-generalized 15-puzzle
NP-hard. Tech. rep. 792, Technion Israel Institute Technology. work paper
completed July 1984.
Goraly, G., & Hassin, R. (2010). Multi-color pebble motion graphs. Algorithmica, 58(3), 610
636.
Hearn, R. A., & Demaine, E. D. (2005). PSPACE-completeness sliding-block puzzles
problems nondeterministic constraint logic model computation. Theoretical
Computer Science, 343(12), 7296.
Hopcroft, J. E., & Tarjan, R. E. (1973). Algorithm 447: efficient algorithms graph manipulation.
Communications ACM, 16(6), 372378.
Hopcroft, J. E., Schwartz, J. T., & Sharir, M. (1984). complexity motion planning
multiple independent objects; PSPACE-hardness warehousemans problem. International Journal Robotics Research, 3(4), 7688.
489

fiD E W ILDE , ER ORS & W ITTEVEEN

Kavraki, L. E., Svestka, P., Latombe, J.-C., & Overmars, M. H. (1996). Probabilistic roadmaps
path planning high-dimensional configuration spaces. IEEE Transactions Robotics
Automation, 12(4), 566580.
Khorshid, M. M., Holte, R. C., & Sturtevant, N. (2011). polynomial-time algorithm nonoptimal multi-agent pathfinding. Proceedings Fourth International Symposium
Combinatorial Search, SoCS, pp. 7683.
Kornhauser, D., Miller, G., & Spirakis, P. (1984). Coordinating pebble motion graphs, diameter permutation groups, applications. Proceedings 25th Annual Symposium
Foundations Computer Science, FOCS, pp. 241250.
Kornhauser, D. M. (1984). Coordinating pebble motion graphs, diameter permutation
groups, applications. Masters thesis, Massachusetts Institute Technology.
LaValle, S. M., & Kuffner, J. J. (2001). Rapidly-exploring random trees: Progress prospects.
Donald, B. R., Lynch, K. M., & Rus, D. (Eds.), Algorithmic Computational Robotics:
New Directions, pp. 293308. K Peters, Wellesley, MA.
Lee, J. H., Lee, B. H., & Choi, M. H. (1998). real-time traffic control scheme multiple AGV
systems collision-free minimum time motion: routing table approach. IEEE Transactions
Man Cybernetics, Part A, 28(3), 347358.
Luna, R., & Bekris, K. E. (2011a). Efficient complete centralized multi-robot path planning.
Proceedings IEEE/RSJ International Conference Intelligent Robots Systems,
IROS, pp. 32683275, San Francisco, CA, USA. IEEE.
Luna, R., & Bekris, K. E. (2011b). Push Swap: fast cooperative path-finding completeness
guarantees. Proceedings Twenty-Second international joint conference Artificial
Intelligence Volume One, IJCAI, pp. 294300. AAAI Press.
Narasimhan, R., Batta, R., & Karwan, H. (1999). Routing automated guided vehicles presence
interruptions. International Journal Production Research, 37(3), 653681.
Nieuwenhuisen, D., Kamphuis, A., & Overmars, M. (2007). High quality navigation computer
games. Science Computer Programming, 67(1), 91 104. Special Issue Aspects
Game Programming.
Papadimitriou, C., Raghavan, P., Sudan, M., & Tamaki, H. (1994). Motion planning graph.
Proceedings 35th Annual Symposium Foundations Computer Science, FOCS, pp.
511 520.
Roger, G., & Helmert, M. (2012). Non-optimal multi-agent pathfinding solved (since 1984).
Proceedings Fifth Annual Symposium Combinatorial Search, SoCS, pp. 173174.
AAAI Press.
Roszkowska, E., & Reveliotis, S. A. (2008). liveness guidepath-based, zone-controlled
dynamically routed, closed traffic systems. IEEE Transactions Automatic Control, 53(7),
16891695.
Ruml, W., Do, M. B., Zhou, R., & Fromherz, M. P. (2011). On-line planning scheduling:
application controlling modular printers. Journal Artificial Intelligence Research, 40,
415468.
490

fiC OOPERATIVE ULTI - AGENT PATHFINDING

Sajid, Q., Luna, R., & Bekris, K. E. (2012). Multi-agent pathfinding simultaneous execution
single-agent primitives. Proceedings Fifth Annual Symposium Combinatorial
Search, SoSC, pp. 8896, Niagara Falls, Canada. AAAI.
Sharon, G., Stern, R., Felner, A., & Sturtevant, N. (2012). Meta-agent conflict-based search
optimal multi-agent path finding. Proceedings Fifth Annual Symposium Combinatorial Search, SoSC, pp. 97104, Niagara Falls, Canada. AAAI.
Sharon, G., Stern, R., Goldenberg, M., & Felner, A. (2011). increasing cost tree search
optimal multi-agent pathfinding. Proceedings Twenty-Second International Joint
Conference Artificial Intelligence, IJCAI, pp. 662667.
Silver, D. (2005). Cooperative pathfinding. Proceedings 1st Conference Artificial
Intelligence Interactive Digital Entertainment, AIIDE, pp. 117122.
Simeon, T., Leroy, S., & Laumond, J.-P. (2002). Path coordination multiple mobile robots:
resolution-complete algorithm. IEEE Transactions Robots Automation, 18(1), 4249.
Sislak, D., Pechoucek, M., Volf, P., Pavlcek, D., Samek, J., Mark, V., & Losiewicz, P. (2008).
AGENTFLY: Towards Multi-Agent Technology Free Flight Air Traffic Control, chap. 7, pp.
7397. Birkhauser Verlag.
Standley, T. (2010). Finding optimal solutions cooperative pathfinding problems. Proceedings
Twenty-Fourth AAAI Conference Artificial Intelligence, AAAI, pp. 173178.
Standley, T., & Korf, R. (2011). Complete algorithms cooperative pathfinding problems.
Proceedings Twenty-Second international joint conference Artificial Intelligence
Volume One, IJCAI, pp. 668673. AAAI Press.
Surynek, P. (2009). novel approach path planning multiple robots bi-connected graphs.
Proceedings 2009 IEEE International Conference Robotics Automation, ICRA,
pp. 36133619, Kobe, Japan.
Surynek, P. (2011). Multi-Robot Systems, Trends Development, chap. Multi-Robot Path Planning, pp. 267290. InTech - Open Access Publisher, Vienna, Austria.
Ter Mors, A. W., Witteveen, C., Zutt, J., & Kuipers, F. A. (2010). Context-aware route planning.
Proceedings Eighth German Conference Multi-Agent System Technologies, MATES,
Leipzig, Germany. Springer.
Ter Mors, A. W., Zutt, J., & Witteveen, C. (2007). Context-aware logistic routing scheduling. Proceedings Seventeenth International Conference Automated Planning
Scheduling, ICAPS, pp. 328335.
Trug, S., Hoffmann, J., & Nebel, B. (2004). Applying automatic planning systems airport groundtraffic control feasibility study. 27th Annual German Conference AI, KI, pp. 183197.
Velagapudi, P., Sycara, K., & Scerri, P. (2010). Decentralized prioritized planning large multirobot teams. Proceedings 2010 IEEE/RSJ International Conference Intelligent
Robots Systems, IROS, pp. 46034609, Taipei, Taiwan.
Vis, I. F. A. (2006). Survey research design control automated guided vehicle
systems. European Journal Operational Research, 170(3), 677709.
491

fiD E W ILDE , ER ORS & W ITTEVEEN

Wang, K.-H. C., & Botea, A. (2008). Fast memory-efficient multi-agent pathfinding. Proceedings Eighteenth International Conference Automated Planning Scheduling,
ICAPS, pp. 380387.
Wang, K.-H. C., & Botea, A. (2011). MAPP: scalable multi-agent path planning algorithm
tractability completeness guarantees. Journal Artificial Intelligence Research, 42, 55
90.
Wilson, R. M. (1974). Graph puzzles, homotopy, alternating group. Journal Combinatorial Theory, Series B, 16(1), 8696.
Wu, Z., & Grumbach, S. (2009). Feasibility motion planning directed graphs. Theory
Applications Models Computation, Vol. 5532 Lecture Notes Computer Science, pp.
430439. Springer.
Zutt, J., & Witteveen, C. (2004). Multi-agent transport planning. Proceedings Sixteenth
Belgium-Netherlands Artificial Intelligence Conference, BNAIC, pp. 139146, Groningen.

492

fiJournal Artificial Intelligence Research 51 (2014) 293-332

Submitted 01/14; published 10/14

Distributed Heuristic Forward Search
Multi-agent Planning
Raz Nissim
Ronen Brafman

raznis@cs.bgu.ac.il
brafman@cs.bgu.ac.il

Ben-Gurion University Negev,
Beer Sheva, Israel

Abstract
paper deals problem classical planning multiple cooperative agents
private information local state capabilities want
reveal. Two main approaches recently proposed solve type problem one based reduction distributed constraint satisfaction,
partial-order planning techniques. classical single-agent planning, constraint-based
partial-order planning techniques currently dominated heuristic forward search.
question arises whether possible formulate distributed heuristic forward search
algorithm privacy-preserving classical multi-agent planning. work provides positive answer question form general approach distributed state-space
search agent performs part state expansion relevant it.
resulting algorithms simple efficient outperforming previous algorithms orders
magnitude offering similar flexibility forward-search algorithms
single-agent planning. Furthermore, one particular variant general approach yields
distributed version a* algorithm first cost-optimal distributed algorithm
privacy-preserving planning.

1. Introduction
Interest multi-agent systems constantly rising, examples virtual real systems
abound, virtual social communities providing many instances. ability plan
systems ability systems autonomously plan
important challenge artificial intelligence, especially size systems
quite large. context, fundamental question perform planning
distributed multi-agent system efficiently, many cases, preserving
privacy.
Distributed planning interesting number reasons. Scientifically intellectually, interesting seek distributed algorithms fundamental computational tasks,
classical planning. Similarly, interesting (and likely useful long
term) seek distributed versions fundamental tools computer science, search
definitely tool. Moreover, pragmatic reasons seeking distributed algorithms. example, imagine setting different manufacturers service
providers publish capabilities collaborate provide new
products services none provide alone. providers certainly
need reveal sort public interface, describing contribute, well
require others. likely, want describe inner
c
2014
AI Access Foundation. rights reserved.

fiNissim & Brafman

workings: internal state manipulate (e.g., current stock
levels, machinery, logistics capabilities, personnel, commitments, etc.). usually confidential proprietary information agent would want reveal, although
clearly one must reason planing process.
principle, problem addressed using central trusted party running suitable planning algorithm. However, trusted party may exist
settings. Moreover, centralized planning puts entire computational burden single
agent, rather distributing across system. Thus, centralized algorithms less
robust, prone agent failures, sometimes less efficient. reasons,
distributed algorithms often sought, case particular, distributed, privacypreserving algorithms. Indeed, main motivation field distributed algorithms, particular, work distributed constraint satisfaction problems (CSP)
(Conry, Kuwabara, Lesser, & Meyer, 1991; Yokoo, Durfee, Ishida, & Kuwabara, 1998;
Meisels, 2007).
Moreover, often case good distributed algorithms formulated cooperative team provide foundation algorithms mechanisms solving similar
problems teams self-interested agents. example, work planning games
(Brafman, Domshlak, Engel, & Tennenholtz, 2009, 2010) suggests modified versions
earlier algorithm cooperative multi-agent systems (Brafman & Domshlak, 2008),
work mechanism design solving distributed CSPs self-interested agents (Petcu,
Faltings, & Parkes, 2008) based earlier work distributed CSPs cooperative
teams (Petcu & Faltings, 2005). fact, work presented paper forms basis
work mechanism design privacy-preserving planning, agents
self-interested (Nissim & Brafman, 2013).
Yet another motivation developing distributed algorithms provided planning
domains search operators correspond actions implemented using complex simulation software accessible relevant agent, agent
interested sharing (due privacy concerns commercial interests)
realistic transfer, integrate, appropriately execute software part
planning algorithm. example, consider planning group robotic agents,
different capabilities. agent simulator compute effect
actions, agents want share other. Thus, application
agents actions search done agents themselves.
long tradition work multi-agent planning cooperative noncooperative agent teams involving centralized distributed algorithms, often using involved models model uncertainty, resources, (Conry et al., 1991; Ephrati
& Rosenschein, 1997; Hansen & Zilberstein, 2001; Bernstein, Givan, Immerman, & Zilberstein, 2002; Szer, Charpillet, & Zilberstein, 2005; Witwicki, Oliehoek, & Kaelbling, 2012),
much work coordinate local plans agents allow agents plan
locally certain constraints (Cox & Durfee, 2005; Steenhuisen, Witteveen, ter Mors, &
Valk, 2006; ter Mors, Valk, & Witteveen, 2004; ter Mors & Witteveen, 2005). work
influenced motivated results area, takes starting point
basic model introduced Brafman Domshlak (BD) offers possibly
simplest model planning ma-strips (Brafman & Domshlak, 2008). ma-strips
minimally extends standard strips (or PDDL) models specifying set agent ids,
294

fiDistributed Heuristic Forward Search

associating action domain one agents. Thus, essentially, partitions
set actions among set agents.
One motivation exploring planning simple setting belief simple
models make easier study fundamental ideas, techniques developed often
generalised complex settings. Another closely related reason working
model recent influential trend field planning, solving richer, complex
models reduction simpler settings and, especially, using classical planners.
expected, given classical planners like SAT solvers, widely used black
boxes solving various problems (Kautz & Selman, 1992; Clarke, Biere, Raimi, & Zhu,
2001) reached certain performance level, makes capable (quickly)
solving large problems. planners incorporate wealth ideas techniques
usually difficult (and sometimes impossible) export non-classical models. use
classical planners shown efficient many different settings planning
uncertainty Stochastic Shortest Path (Yoon, Fern, & Givan, 2007), Conformant
Planning (Palacios & Geffner, 2009), Conformant Probabilistic Planning (Albore, Palacios,
& Geffner, 2010; Taig & Brafman, 2013) Contingent Planning (Albore, Palacios, &
Geffner, 2009) well planning models Net-Benefit Planning (Keyder
& Geffner, 2009) Generalized Planning (Srivastava, Immerman, Zilberstein, & Zhang,
2011).
Recently, number ma-strips planning algorithms respect agent privacy
utilize inherent distributed structure system emerged. first
approach based distributed CSP techniques introduced BDs original work.1
BD formulate CSP particularly suited ma-strips problems whose solution
plan. algorithm transformed fully distributed algorithm simply
using distributed CSP solver. BDs work establishes upper bound complexity
solving ma-strips problem depends exponentially two parameters
quantifying level agents coupling system. Unfortunately, distributed CSP
solvers cannot handle even smallest instances planning problems. Consequently,
dedicated algorithm, based ideas BD, developed (Nissim, Brafman, &
Domshlak, 2010). performing well domains, algorithm trouble
scaling problems agent execute small number
actions. (Indeed, BDs algorithm scales exponentially min-max number actions
per agent solution plan.) Recently, new, improved algorithm, based partialorder planning, map-pop, developed (Torreno, Onaindia, & Sapena, 2012). Yet,
algorithm, too, leaves serious gap solve using distributed planner
solved using centralized planner. Moreover, neither algorithm attempts
generate cost-optimal plan.
classical single-agent planning, constraint-based partial-order planning techniques
currently dominated heuristic forward search techniques2 . Thus, natural
ask whether possible formulate distributed heuristic forward search algorithm
privacy-preserving classical planning. paper provides positive answer question
1. idea using distributed CSP techniques planning first introduced Conry et al. (1991).
2. Winners sequential satisficing (non-optimal) tracks last three International Planning Competitions heuristic forward search planners. planner also winner optimal track
previous IPC (2011).

295

fiNissim & Brafman

form general approach distributed state-space search agent
performs part state expansion relevant it. resulting algorithms
simple efficient outperforming previous algorithms orders magnitude
offer similar flexibility forward-search based algorithms single-agent
planning. respect natural distributed structure system, thus allow us
formulate privacy-preserving versions.
One particular variant general approach yields distributed version a*
algorithm, called mad-a*, first distributed algorithm privacy-preserving
cost-optimal planning ma-strips. mad-a* solves difficult problem centralized search since privacy-preserving setting, agent abstracted (partial)
view problem. Yet, still able solve problems efficiently, outperforming centralized a* cases. show, main reason interesting optimality
preserving pruning technique naturally built search approach. insight
led new pruning technique centralized search shall describe later on.
rest paper organized follows. start providing background
describe model use. followed discussion related work.
Section 4 describes forward search algorithm, Section 5 describes mad-a*,
modified version maintains cost-optimality. Then, present planning
framework used, ma-fd, empirical results evaluating effectiveness algorithms.
Section 8 discusses pruning method built-in algorithms,
applied centralized search. Section 9 provides discussion privacy properties
algorithms open research challenges, followed conclusion.

2. Background
describe ma-strips model multi-agent planning, proceed define
privacy-preserving planning ma-strips.
2.1 MA-STRIPS
ma-strips problem (Brafman & Domshlak, 2008) set agents = {i }ki=1 ,
given 4-tuple = hP, {Ai }ki=1 , I, Gi, P finite set propositions, P
G P encode initial state goal, respectively, 1 k, Ai
set actions agent capable performing. action = hpre(a), eff(a), cost(a)i
given preconditions, effects cost. plan solution iff solution
underlying strips problem obtained ignoring identities agent associated
action. Since action associated agent, plan tells agent
it. different planning contexts, one might seek special types
solutions. example, context planning games (Brafman et al., 2009), stable
solutions (equilibria) sought. focus cooperative multi-agent systems, seeking
either standard solution cost-optimal one, minimizes sum action costs
plan.
partitioning actions agents yields natural distinction private
public propositions actions. private proposition agent required affected
actions . action private preconditions effects private.
actions classified public. is, private actions affect affected
296

fiDistributed Heuristic Forward Search

actions, public actions may require affect actions agents.
ease presentation algorithms proofs, assume actions
achieve goal condition considered public. methods easily modified
remove assumption.
get clearer picture ma-strips model, consider well known Logistics
classical planning domain, packages moved initial
target locations using given fleet vehicles trucks, airplanes, etc. packages
loaded onto unloaded vehicles, vehicle move certain
subset locations. Propositions associated package location, map
vehicle, every vehicles location map. Possible actions drive/fly,
load, unload, suitable parameters (e.g., drive(truck, origin, destination)
load(package, truck, location)). natural partitioning problem associates
vehicle agent, assigning agent drive, load unload actions
involved. ma-strips includes action assignment part problem description.
Logistics domain, since vehicle locations private propositions (affect
affected respective agents move actions), move actions certainly private,
depending affecting private location propositions. Package location propositions
either private public, depending whether required/affected
one agent. example, proposition at(package,location) private location
accessible one agent (e.g. inside vehicle, or, reached single
vehicle), public otherwise. Therefore, load (respectively unload ) actions require
(respectively affect) public package location propositions considered public.
note notion private/public natural ma-strips encoding,
easily applied models multi-valued variables. example, SAS+,
variable may multiple values, analogue (boolean) proposition
ma-strips hvariable, valuei pair. pair considered private required,
achieved destroyed actions single agent. Consequently, actions
require, achieve destroy private hvariable, valuei pairs considered private.
clarity consistency previous work use ma-strips notation discussing
theoretical aspects work. However, examples given, well practical
framework present planning, use concise multi-valued variables SAS+
encoding.
2.2 Privacy Preserving Planning
Given notion private/public propositions actions, formalize
problem focus work Privacy-preserving MA-STRIPS planning. Privacypreserving ma-strips planning seeks find solution ma-strips planning problem
without exposing information private agent. Specifically: set private
variables possible values, set private actions cost, private
preconditions effects public actions. Public actions variables considered
exposed interfaces agent, private actions variables correspond
agents local state actions manipulate local state.
formally, privacy-preserving ma-strips planning problem, agent
access description actions public projection public actions
297

fiNissim & Brafman

agents. public projection action consists lists public preconditions
public effects. weak privacy preserving planning algorithm agent need
communicate description private variables, private actions cost,
private preconditions effects public actions, another agent. strong privacy
preserving algorithm, agent deduce isomorphic model (i.e., model,
renaming) private variable, private action cost, private preconditions
effects public action cannot deduced initial information available
agent public parts plan. present distributed search approach
lies somewhere in-between weak strong privacy preserving. discuss
privacy properties Section 9.1. stage, let us illustrate meaning need
privacy properties.
Consider classical Logistics domain. Agent knows existence value
(private) location propositions, locations public packages packages
private (only load unload them). Thus, initial state known
contain initial locations vehicles, package locations reachable single
agent different . agent full knowledge actions, access
projections agents public actions only. example, another agents action
=unload(package, truck, location) preconditions at(package, in-truck) at(truck,
location) effects at(package, in-truck) at(package, location). However, since
at(package, location) public proposition (assuming location reachable
one agent), projection known contains preconditions
single effect. implication preserving strong privacy example agents
know packages handled single agent (private it).
know locations served agent, actual location different stages plan,
changes location. fact, unaware existence location
agent, private variable.
real life, may seem absurd, know trucks planes
location move among locations. Privacy guarantees take account
general knowledge things work. However, even example,
appealing agents know set locations agent serves, moves
them, cost. know packages handled
agent. believe ability provide privacy similar level
clear advantage planning algorithm, facilitating ability construct ad-hoc teams
cooperating, privacy seeking agents.
Many service-oriented domains similar abstract structure, one agent
work towards goal (getting package destination) requires collaboration multiple agents (e.g. truck one country, plane, truck another country).
example, imagine multiple part-manufacturers, together aggregator,
build complex object together none build own, e.g., laptop.
would require monitors, various cards (graphic card, mother board, wifi), casing, etc.
components, turn, requires diverse electronic parts, wiring, empty boards,
etc. mother board manufacturers interface external world consists action selling mother board, actions purchasing relevant components
public actions. public action sell-mother-board may preconditions
stock > 0, have-payment, have-address effect stock=stock-1, has-mother298

fiDistributed Heuristic Forward Search

board(customer), stock variables private variables public. would
also private action produce-mother-board various technical
preconditions related availability parts, workers, machinery. Alternatively, perhaps
producing board requires multiple steps, possibly using different machines different
workers. reflected set private actions. public variables refer
mother board orders, availability, payments received. internal variable refer
produces, workers roles take, stores
inventory, stock levels facility. private actions describe
shipping warehouse plant done, actions used actually construct board,
what, much more. privacy preserving algorithm neither expose,
require knowledge inner workings, typically considered proprietary
information manufacturers would share, even collaborators, unless required.
Section 1 discussed example application planning team robotic
agents, complex simulators generate outcome actions. Fitting
scenario ma-strips model, public variables agent could describe
elements environment affected agent, visible aspects robot (e.g.,
is, whether standing ground), whereas local variables would
refer local state: actual state various motors, charge level, perhaps various
local variables describing current mode operation. Public actions would actions
influence environment observable state robot: moving, lifting, climbing
ladder, collecting rock. Local actions would involve local computations manipulate
internal, private state, charging. Note public actions influence many
private variables: moving arm climbing ladder would change internal state
many variables internal mechanical parts. Privacy preservation serves two
purposes: requiring one robot understand model intricacies another robot,
requiring one robot disclose information robots, simplifying
interaction robots, maintaining confidentiality of, possibly proprietary
information. Thus, one robot public (and thus) abstract description move
action anther robot, whose public effect position robot changes. Yet,
need model, know about, inner working involved move (or lift, carry,
etc.) action. likely, state internal parts/motors/joints change various
ways accomplish move action. Indeed, ad-hoc robot teams ubiquitous
ad-hoc networks, ability work together without burden exposure entailed
sharing full models could crucial.

3. Related Work
work focuses classical (deterministic) planning setting, agents
collaborate specific task, prefer reveal private information local
states, private actions, cost private actions. preservation agent
privacy main difference methods (and distributed planning general)
approaches factored planning parallel planning. discuss
related work multi-agent systems related work privacy.
299

fiNissim & Brafman

3.1 Multi-agent Planning
Factored planning methods (Amir & Engelhardt, 2003; Brafman & Domshlak, 2006; Fabre
& Jezequel, 2009; Fabre, Jezequel, Haslum, & Thiebaux, 2010) seek utilize structure
planning problem, making centralized search efficient. methods
exploit structure ma-strips problems (as done Brafman Domshlak,
2008), applicable privacy-preserving settings, respect
distributed form problem, giving centralized entity access entire problem
description.
Parallel planning methods (Vrakas, Refanidis, & Vlahavas, 2001; Kishimoto, Fukunaga,
& Botea, 2009; Burns, Lemons, Ruml, & Zhou, 2010) aim speed-up solution
centralized planning problems given access distributed computing environment,
large cluster. Parallel planning performed ma-strips applying existing
approaches underlying strips problem (i.e., ignoring agent identities).
methods distribute computation required solving planning problem, ignore
privacy concerns, giving agents (essentially processors) access entire planning
problem.
Given privacy-preserving ma-strips model, natural ask search
solution. well-known example privacy-preserving search distributed CSPs
(Conry et al., 1991; Yokoo et al., 1998), various search techniques heuristics
developed (Meisels, 2007). Distributed CSPs, agents cooperate find
feasible assignment constrained variables keeping private internal constraints,
model problem similar distributed ma-strips. fact, planning problems
cast CSP problems (given bound number actions), first attempt
solve privacy-preserving ma-strips problems based reduction distributed
CSPs. specifically, Brafman Domshlak introduced Planning CSP+Planning
methodology planning system cooperative agents private information.
approach separates public aspect problem, involves finding public action
sequences satisfy certain distributed CSP, private aspect, ensures
agent actually execute public actions sequence. Solutions found
locally optimal, sense minimize , maximal number public actions
performed agent. methodology later extended first fully distributed
algorithm ma-strips planning, Planning-First (Nissim et al., 2010). Planning First
shown efficient solving problems agents loosely coupled,
low. However, scale rises, mostly due large search
space distributed CSP. Recently, distributed planner based partial order planning
introduced (Torreno et al., 2012), outperforms Planning First, effectively solving
tightly coupled problems. methods privacy preserving, guarantee
cost-optimal solutions.
work attempts solve privacy-preserving ma-strips using distributed heuristic
forward search algorithms. Heuristic forward search methods widely used,
applied similar settings. Ephrati Rosenschein (1994, 1997) showed given
priori breakdown global goal agent assigned subgoals, agents plan
towards subgoals, merge possibly conflicting plans. approach
essentially searches plan-space, using heuristic estimates states (which results
300

fiDistributed Heuristic Forward Search

merged feasible subplans) order guide search. approach guarantee
optimal solutions, relies solving exponential number plan merging problems
scale even loosely-coupled problems (Cox & Durfee, 2009). made
clear next section, approach simple, general framework forward search
state-space, easily specializes distributed version cost-optimal search
algorithm a*, depend complex plan merging algorithms.
setting planning uncertainty distributed agents, forward search
shown effective. Szer, Charpillet Zilberstein (2005) used heuristic bestfirst approach order optimally solve Decentralized POMDPs. approach performs
centralized forward search space agents policy vectors. order compute
heuristic estimate single node, solution underlying centralized MDP must
computed possible states system. search algorithm (which
centralized) heuristic computation require full knowledge system
therefore applicable privacy-preserving setting.
Another notable strategy solving decentralized POMDPs proposed Nair et
al. (2003). JESP (Joint Equilibrium Based Search Policies) computes locally optimal
solution iteratively modifying policy agent improve joint (global)
policy. algorithm proved converge Nash equilibrium. Subsequent work
(Ranjit, Varakantham, Tambe, & Yokoo, 2005) aims exploit local interactions
agents, applying distributed CSP techniques. Locality exploited computing
agents best response respect neighbors, agents affected
it, improves JESPs efficiency. Similarly Planning CSP+Planning,
methods generate locally optimal solutions guarantee globally optimal solutions.
Additionally, CSP-based methods suffer exactly scalability drawback
Planning-First, efficient agent interaction rises.
Recently, parallel work, heuristic forward search planning
uncertainty combined influence-based abstraction (Witwicki & Durfee, 2010;
Oliehoek, Witwicki, & Kaelbling, 2012), producing forward search algorithm searching
influence-space (Witwicki et al., 2012). Influence-based abstraction reformulates
joint-policy search space space influences, represent effect agent
policies one another. notion influences relates closely private/
public actions influences equated public actions, intricate policies
influences depend equated private actions. aforementioned
work shows heuristic search influence space lead significant improvement
performance, mostly due pruning search space (we arrive similar
conclusion respect work discussed Section 8). work
heuristic search influence space similar work, two main differences
sets apart. First, methods described apply transition-decoupled
POMDP models, restrict number agents affecting variable one,
whereas ma-strips limit number agents affecting variable. Second,
methods aimed preserving privacy distributing search,
making centralized search efficient.
Overall, motivation existing work planning uncertainty resembles Factored Planning utilizing (MA) structure order centrally solve
problem efficiently. research tackles problems stem
301

fiNissim & Brafman

models characteristics (e.g. MDPs stochastic actions, POMDPs partial observability etc.).
Therefore, product body research usually specialized methods
provide general solution schema applicable domains, despite solving general (and complex) models. important models representing problems
harder solve classical planning DEC-POMDPs shown NEXPcomplete general case (Bernstein et al., 2002), believe field planning
benefit general solutions simplest fundamental model classical planning,
presented work.
3.2 Privacy
Privacy wide topic research many technological, social, legal aspects.
interest lies much well defined area secure multi-party computation (Yao,
1982, 1986), subfield Cryptography. goal methods developed secure multiparty computation enable multiple agents compute function inputs,
keeping inputs private. specifically, agents 1 , . . . , n , private data
x1 , . . . , xn , would like jointly compute function f (x1 , . . . , xn ), without revealing
private information, reasonably deduced value
f (x1 , . . . , xn ).
Formal results area provide protocols computing various functions
guaranteed private various assumptions, is, information
inputs functions obtained beyond deduced output.
assumptions cover three key aspects problem. first aspect model
adversary party seeking violate privacy. example, relatively simple
assumption one honest curious, agents follows protocol
laid out, also curious try deduce information agents
information obtains execution protocol. Malicious agents,
hand, may deviate protocol order gain additional information.
Furthermore, agents may collude, various results place cap number
malicious colluding agents. aspects adversary could computational
power memory. second aspect network model. communication synchronous
asynchronous? channels secure? information get lost channel?
Finally, third aspect meaning secure. actually two sub-aspects
it. First, information exactly kept private. Second, strong
security guarantees. Here, much like cryptography, one makes various assumptions
protocol secure, e.g., factoring hard.
Privacy-preserving ma-strips planning clearly instance secure multi-party
computation problem. input function agents sets actions, initial
state, goal state, output plan. privacy requirements bit
weaker standard model, inputs public (i.e., public aspects
public actions). hand, output actually private: private
actions agents plan. natural adversary model one honest-butcurious, although requirement dictated problem definition, diverse
adversary models considered.
302

fiDistributed Heuristic Forward Search

principle, appears techniques developed area secure multiparty computation extended setting distributed planning, complexity
quickly becomes unmanageable. example, common approach secure multi-party
computation uses cryptographic circuits. solving shortest path problem (e.g.,
network routing, Gupta et al., 2012), size circuits created polynomial
size graph. setting function f computes shortest path implicit
graph induced descriptions agents actions. graph exponential
problem description size, quickly becomes infeasible construct circuits given
time memory limitations. true planning NP-hard forward search
algorithms do, general, require exponential time/memory, purpose heuristic search
reduce search space solve large problems low-polynomial time. Requiring
construction exponential-sized circuits a-priori contradicts goal efficiency
feasibility.
computational problem closely resembles privacy-preserving ma-strips
planning privacy preserving constraint satisfaction, subject whole subfield distributed CSPs, mentioned earlier. DisCSP, agent single variable,
exist binary unary constraints. Binary constraints public since
one agent knows existence, unary constraints considered private
information. meeting scheduling, agent single variable whose values possible
meeting time slots. binary constraint could equality constraint values
two variables belonging different agents, unary constraint represents slots
agent cannot hold meetings. Early work distributed CSP explicitly consider
privacy issues, except type weak privacy mentioned earlier. Thus, agents
expected solve underlying CSP without explicitly revealing unary constraints.
Later on, work distributed CSPs (Yokoo, Suzuki, & Hirayama, 2002; Silaghi & Mitra,
2004) identified fact even algorithm weakly private, private information
may leak search process. example, search, whenever agent sends
assignment variable agents, deduce value
unary constraint forbidding it. value end assigned solution,
agent revealed private information could deduced
viewing solution. Thus, followup work focused question measure
privacy loss (Franzin, Rossi, Freuder, & Wallace, 2004; Maheswaran, Pearce, Bowring,
Varakantham, & Tambe, 2006), analyzing much information specific algorithms lose
(Greenstadt, Pearce, & Tambe, 2006), question alter existing DisCSP
algorithms handle stricter privacy demands (Greenstadt, Grosz, & Smith, 2007; Leaute
& Faltings, 2009). recently, work provides formal guarantees DisCSP
algorithms emerged (Leaute & Faltings, 2009; Grinshpoun & Tassa, 2014). work
builds techniques area secure multi-party computation.

4. Multi-agent Forward Search
section describes distributed variant forward best-first search, call
mafs. begin algorithm itself, including overview pseudo-code. Then,
provide example flow mafs, discussion finer points.
303

fiNissim & Brafman

4.1 MAFS Algorithm
Algorithms 1-3 depict mafs algorithm agent . mafs, separate search space
maintained agent. agent maintains open list states candidates
expansion closed list already expanded states. expands state
minimal f value open list, initialized agents projected view
initial state. agent expands state s, uses operators only. means
two agents (that different operators) expanding state, generate different
successor states.
Since agent expands relevant search nodes, messages must sent agents,
informing one agent open search nodes relevant expanded another agent. Agent
characterizes state relevant agent j j public operator whose public
preconditions (the preconditions aware of) hold s, creating action
public. case, Agent send Agent j .
Algorithm 1 mafs agent
1: TRUE
2:
messages message queue
3:
process-message(m)
4:
extract-min(open list)
5:
expand(s)

Algorithm 2 process-message(m = hs, gj (s), hj (s)i)
open closed list gi (s) > gj (s)
add open list calculate hi (s)
3:
gi (s) gj (s)
4:
hi (s) max(hi (s), hj (s))
1:

2:

messages sent agents contain full state s, i.e. including public
private variable values (we later discuss encrypted), well cost
best plan initial state found far, sending agents heuristic
estimate s. agent receives state via message, checks whether state
exists open closed lists. appear lists, inserted
open list. copy state higher g value exists, g value updated,
closed list, reopened. Otherwise, discarded. Whenever received state
(re)inserted open list, agent computes local h value state,
choose between/combine value calculated h value received
message. heuristics known admissible, example, agent could choose
maximal two estimates, done Line 4 Algorithm 2. Otherwise, agent
free combine estimates however sees fit, depending known characteristics
heuristics. agent affect correctness algorithm,
could affect search efficiency. issue combining heuristic estimates discussed
Section 9.1.
304

fiDistributed Heuristic Forward Search

Algorithm 3 expand(s)
1: move closed list
2: goal state
3:
broadcast agents
4:
broadcasted agents
5:
return solution
6: agents j
7:
last action leading public j public action
public preconditions hold
8:
send j
9: apply successor operator
10: successors s0
11:
update gi (s0 ) calculate hi (s0 )
12:
s0 closed list fi (s0 ) smaller s0 moved
closed list
13:
move s0 open list

agent expands solution state s, sends agents awaits
confirmation, sent whenever expand, broadcast state (Line 3
Algorithm 3). simplicity, order avoid deadlocks, agent either
broadcasts confirms solution, allowed generate new solutions. solution
found one agent, one lower cost chosen, ties broken
choosing solution agent lower ID. solution confirmed
(broadcasted) agents (Line 4 Algorithm 3), agent returns solution
initiates trace-back solution plan. also distributed process,
involves agents perform action optimal plan. initiating agent
begins trace-back, arriving state received via message, sends
trace-back message sending agent. continues arriving initial state.
trace-back phase done, terminating message broadcasted solution
outputted.
see, general simple scheme apply actions/operators
send relevant generated nodes agents used distribute
search algorithms. However, various subtle points pertaining message sending
termination influence correctness efficiency distributed algorithm,
discuss later.
better demonstrate flow algorithm, consider example given Figure
4.1. example, two agents must cooperate order achieve goal.
agents actions described left-hand side, every node graph depicts
action, edge (u, v) indicates u either achieves destroys precondition
v. two public actions a5 , a8 , affect/depend public variable, v4 ,
rest actions private. central part figure depicts joint
search space, i.e., nodes generated centralized search. right-hand site depicts
local search space agents, i.e., nodes generated sent agent
running mafs. initial state, variable values zero (i.e., = 0000), goal
305

fiNissim & Brafman

Figure 1: Description actions example planning problem, reachable search
space, search space generated mafs. Actions represented <
pre, eff > states denoted values variables v1 , v2 , v3 , v4 respectively
(For example, 1122 denotes state v1 = 1, v2 = 1, v3 = 2, v4 = 2.).

G = {v4 = 2}. values private variables belonging agents (v3 agent
1, v1 , v2 agent 2) shown bold. values required known
agents, fact regarded dont cares, used identifiers.
agents begin searching using MAFS, applies actions only. Therefore, agent
2 quickly exhausts search space, since far concerned, state 0020 dead end.
Agent 1 generates search space, applies public action a5 , results state
= 2201. sent agent 2, since public preconditions a8 hold (Line
7 Algorithm 3). Upon receiving s, agent 2 continues applying actions, eventually
reaching goal state, broadcasted.
4.2 Discussion
discuss subtle points mafs.
4.2.1 Preserving Agent Privacy
goal preserve privacy, may appear mafs agents revealing
private data transmit private state messages. Yet, fact,
information used agents, altered. simply copied
future states used agent. Since private state data used ID,
agents encrypt data keep table locally, maps IDs private states.
encryption easily made generate multiple IDs private state,
306

fiDistributed Heuristic Forward Search

ID never used twice, agents cannot identify others private
states. Consequently, algorithms based distributed search paradigm described
weakly privacy preserving. issue privacy discussed Section 9.1.
compute heuristic estimates states receives, agent must assess effort
required achieve goal them. this, needs information
effort required agents construct part plan. fully cooperative
setting, agent access full description agents actions.
privacy preserving setting, two issues arise. First, agents partial information
agents capabilities access public interface. Second,
different agents may compute different heuristic estimates state
agent full information capabilities, others.
issue affect actual algorithm, agnostic agents compute
heuristic estimate, although fact agents less information lead poorer
heuristic estimates. hand, agents free use different heuristic functions,
demonstrate empirically, using public interfaces only, still able
efficiently solve planning problems.
state reached via message, includes sending agents heuristic estimate.
Therefore, receiving agent two (possibly different) estimates use.
heuristics known admissible, clearly maximal (more accurate) value
taken, line 4 Algorithm 2. not, agent free decide use
estimates, depending known qualities.
4.2.2 Relevancy Timing Messages
State considered relevant agent j public action public preconditions hold last action leading public (line 7 Algorithm 3).
means states products private actions considered irrelevant
agents. turns out, since private actions affect agents capability
perform actions, agent need send states last action performed public, order maintain completeness (and cost-optimality, proved next section).
Regarding states products private actions irrelevant decreases communication, effectively pruning large, symmetrical parts search space. fact,
show Section 8 property mafs used obtain state-space pruning
centralized planning algorithms, using method called Partition-based pruning.
hinted earlier, exists flexibility regarding relevant states
sent. Centralized search viewed essentially sending every state (i.e., inserting
open list) generated. mafs, relevant states sent
expanded (as pseudo-code) generated (changing Algorithm
3 moving for-loop line 6 inside for-loop line 10). timing
messages especially important distributed setting since agents may different
heuristic estimates. Sending messages generated increases communication,
allows states considered promising agent expanded
another agent earlier stage. Sending relevant states expanded,
hand, decreases communication, delays sending states viewed
promising. Experimenting two options, found lazy approach,
307

fiNissim & Brafman

sending messages expanded, dominates other, likely
communication costly.
4.2.3 Concurrent Solutions
Although solution plan outputted mafs sequential, i.e. requires agents execute
actions turns, quite easily parallelized. Since private actions require/
affect agents private propositions, agents perform concurrently without
hurting correctness plan, long public actions (interaction points
agents) performed correct order. Intuitively, execution order public
actions maintained, agents free execute private actions concurrently.
parallelization done time linear solution length, requires joint
computation. Another option using one many algorithms known parallelization
plans (Backstrom, 1998). Given existence private actions, plans much
potential concurrency.
4.2.4 Search Using Complex Actions
Section 1, mentioned scenario search operators corresponding real-world
actions implemented using complex simulation software. situation arise,
example, team heterogeneous robotic agents, dedicated simulator actions. approach well suited settings: First, forward search
methods capable using generative, rather declarative models agents actions, central step involves generation successor states insertion
appropriate queues. oblivious operators described implemented, long successor states generated. Second, approach respects
natural system structure, agent need apply operators. Thus,
need share generative models amongst agents.
One problem, however, fact contemporary methods generating
heuristic functions require either declarative strips-like description generative model
(in case sampling methods). Fortunately, empirical results indicate use
approximate model works quite well practice. Indeed, approach assumes
agents use public part agents action model, approximation.
Even original action model generative, declarative approximate model
constructed using learning techniques (Yang, Wu, & Jiang, 2007). Alternatively, sampling
methods could use suitably developed simplified simulator.

5. Optimal MAFS
mafs presented, cost-optimal planning algorithm. Recall cost-optimal
plan single-agent planning one achieves goal minimal cost, i.e.
minimizing sum action costs plan. notion remains case,
wish minimize sum cost agents participating plan. metric
important systems describe cooperative agents like ma-strips,
aim minimize cost entire system, specific agents. Moreover, costoptimal algorithms required applying mechanism design techniques planning
308

fiDistributed Heuristic Forward Search

systems comprising selfish agents (Nissim & Brafman, 2013). settings,
cost-optimal plan constitutes social-welfare maximizing solution. mafs slightly
modified order achieve cost-optimality. describe modifications,
result variation a* refer Multi-Agent Distributed A* (mad-a*).
a*, state chosen expansion agent must one lowest
f = g + h value open list, heuristic estimates admissible. mad-a*,
therefore, extract-min (Line 4 Algorithm 1) must return state.
5.1 Termination Detection
Unlike a*, expansion goal state mafs necessarily mean optimal
solution found. case, solution known optimal agents
prove so. Intuitively, solution state solution cost f known optimal
exists state s0 open list input channel agent,
f (s0 ) < f . words, solution state known optimal f (s) flowerbound ,
flowerbound lower bound f -value entire system (which includes
states open lists, well states messages processed, yet).
detect situation, use Chandy Lamports snapshot algorithm (Chandy
& Lamport, 1985), enables process create approximation global state
system, without freezing distributed computation. approximation
check whether state exists whose f value lower value candidate solution.
Although check conducted respect approximate global state, snapshot
algorithm guarantees answer positive iff true global state.
snapshot algorithm works using marker messages. process wants
initiate snapshot records local state sends marker outgoing channels.
processes, upon receiving marker, record local state, state
channel marker came empty, send marker messages
outgoing channels. process receives marker recorded local state,
records state incoming channel marker came carrying
messages received since first recorded local state.
Although guarantee global state computed algorithm actually
occurred point run mad-a*, approximation good enough
determine whether stable property currently holds system. property system
stable global predicate remains true becomes true. Specifically,
properties form flowerbound c fixed value c, stable h globally
consistent heuristic function. is, f values cannot decrease along path.
case, path may involve number agents, h values. local
functions h consistent, agents apply max operator receiving state via
message (known pathmax ), property holds3 . solution verification procedure
using snapshot algorithm, run whenever solution state expanded (meaning
minimal f -value), agent receives confirmation agents solution
state (In pseudocode, change Line 5 Algorithm 3 initiate verification
solution.). occurs agents expanded solution state.
3. Although recent work (Holte, 2010) shows pathmax necessarily make bona-fide consistent
heuristic, pathmax ensure f -values along path non-decreasing.

309

fiNissim & Brafman

snapshot algorithm returns true stable property states exist lower
f -value, algorithm return solution optimal (In pseudocode, change Line
1 Algorithm 1 receive true solution verification procedure.).
note simplicity pseudo-code omitted detection situation
goal state exist. done determining whether stable
property open states system holds, using snapshot algorithm.
5.2 Proof Optimality
prove optimality mad-a*. must note presented, mada* maintains completeness (and optimality) actions achieve goal
condition considered public. property assumed throughout section,
algorithm easily modified remove it.4 begin proving following lemma
(and corollary) regarding solution structure planning problem.
demonstrate always exists optimal solution structure found
mad-a*. continue proving extension well-known result a*,
required completeness mad-a*. Finally, proving mad-a*s optimality,
prove correctness termination detection procedure.
Lemma 1. Let P = (a1 , a2 . . . , ak ) legal plan planning problem . Let
ai , ai+1 two consecutive actions taken P different agents, least one
private. P 0 = (a1 , . . . , ai+1 , ai , . . . , ak ) legal plan P (I) = P 0 (I).
Proof. definition private public actions, ai , ai+1 actions belonging
different agents, varset(ai )varset(ai+1 ) = , varset(a) set variables
affect affected a. Therefore, ai achieve ai+1 preconditions,
ai+1 destroy ai preconditions. Therefore, state ai
executed P , ai+1 executable s, ai executable ai+1 (s), ai (ai+1 (s)) =
ai+1 (ai (s)). Therefore, P 0 = (a1 , . . . , ai+1 , ai , . . . , ak ) legal plan . Since suffix
(ai+2 , ai+3 , . . . , ak ) remains unchanged P 0 , P (I) = P 0 (I), completing proof.
Corollary 1. Let P = (a1 , a2 , . . . , ak ) solution planning problem . Then,
exists equal cost solution P 0 = (a01 , a02 , . . . , a0k ) satisfies following properties:
1. P 0 contains permutation actions P .
2. ai first public action P 0 , a1 , . . . , ai belong agent.
3. pair consecutive public actions ai , aj P 0 , actions al , < l j belong
agent.
Proof. Using repeated application Lemma 1, move ordered sequence private
actions performed agent , would immediately subsequent public
action maintain legality plan. Clearly, P 0 permutation P therefore
cost P P 0 identical. implies P cost optimal, P 0 .
4. order remove assumption, agent must send messages states creating
action achieved goal (which may private). agent approves solution state
private goal achieved.

310

fiDistributed Heuristic Forward Search

Next, prove following lemma, extension well known result
a*. follows, tacitly assumed liveness property conditions
every sent message eventually arrives destination agent operations take
finite amount time. Also, clarity proof, assume atomicity
expand process-message procedures.
Lemma 2. non-closed node optimal path P
properties 2 & 3 Corollary 1, exists agent either open node s0
incoming message containing s0 , s0 P g (s0 ) = g (s0 ) .
Proof. : Let P = (I = n0 , n1 , . . . , nk = s). open list agent (
finish algorithms first iteration), let s0 = lemma trivially true since
g (I) = g (I) = 0. Suppose closed agents. Let set nodes ni
P closed agent , g (ni ) = g (ni ). empty since,
assumption, . Let nj element highest index, closed agent .
Clearly, nj 6= since non-closed. Let action causing transition nj nj+1
P . Therefore, g (nj+1 ) = g (nj ) + cost(a).
agent performing a, nj+1 generated moved open list
lines 9-13 Algorithm 3, g (nj+1 ) assigned value g (nj ) + cost(a) = g (nj+1 )
claim holds.
Otherwise, performed agent 0 6= . public action, preconditions hold nj , therefore nj sent 0 line 8 Algorithm 3.
private action, definition P , next public action a0 P performed
0 . Since private actions change values public variables, public preconditions a0 must hold nj , therefore nj sent 0 line 8 Algorithm
3. Now, message containing nj processed 0 , nj added
open list 0 Algorithm 2 claim holds since g0 (nj ) = g (nj ) = g (nj ). Otherwise, 0 incoming (unprocessed) message containing nj claim holds since
g (nj ) = g (nj ).
Corollary 2. Suppose h admissible every , suppose algorithm
terminated. Then, optimal solution path P follows restrictions Lemma
1 goal node s? , exists agent either open node
incoming message containing s, P fi (s) h (I).
Proof. : Lemma 2, every restricted optimal path P , exists agent
either open node incoming message containing s, P
gi (s) = g (s) . definition f , since hi admissible,
cases:
fi (s) = gi (s) + hi (s) = g (s) + hi (s)
g (s) + h (s) = f (s)
since P optimal path, f (n) = h (I), n P , completes proof.
Another lemma must proved regarding solution verification process. assume
global consistency heuristic functions, since admissible heuristics made
consistent locally using pathmax equation (Mero, 1984), using max
311

fiNissim & Brafman

operator line 4 Algorithm 2 heuristic values different agents. required
since flowerbound must non-decreasing.
Lemma 3. Let agent either open node incoming message
containing s. Then, solution verification procedure state f (s ) > f (s)
return false.
Proof. Let agent either open node incoming message
containing s, f (s) < f (s ) solution node . solution verification procedure state verifies stable property p = f (s ) flowerbound . Since
flowerbound represents lowest f -value open unprocessed state system,
flowerbound f (s) < f (s ), contradicting p. Relying correctness
snapshot algorithm, means solution verification procedure return false,
proving claim.
prove optimality algorithm.
Theorem 1. mad-a* terminates finding cost-optimal path goal node, one exists.
Proof. : prove theorem assuming contrary - algorithm terminate
finding cost-optimal path goal node. Three cases considered:
1. algorithm terminates non-goal node. contradicts termination condition, since solution verification initiated goal state expanded.
2. algorithm terminate. Since dealing finite search space,
let () denote number possible non-goal states. Since finite
number paths node search space, reopened finite
number times. Let () maximum number times non-goal node
reopened agent. Let time point non-goal nodes
f (s) < h (I) closed forever agents . exists, since:
a) assume liveness message passing agent computations; b)
() () expansions non-goal nodes , non-goal nodes search space
must closed forever ; c) goal node f (s ) < h (I) exists5 .
Corollary 2 since optimal path goal state exists,
agent expanded state time t0 , f (s ) h (I). Since
optimal solution, t0 t, flowerbound f (s ) time t0 . Therefore, verification
procedure return true, algorithm terminates.
Otherwise, t0 < t. Let 0 last agent close non-goal state f0 (s) <
f (s ). 0 open list incoming message. true
broad-casted agents , every time closed
agent (when expands it), immediately broad-casted again, ending
agents open list message queue. Now, 0 open nodes f value lower , eventually expand , initiating solution verification
procedure return true, since flowerbound f (s ). contradicts
assumption non-termination.
5. needed since goal node expansions bounded.

312

fiDistributed Heuristic Forward Search

3. algorithm terminates goal node without achieving optimal cost. Suppose
algorithm terminates goal node f (s) > h (I). Corollary 2,
existed termination agent open node s0 ,
incoming message containing s0 , s0 optimal path f (s0 ) h (I).
Therefore, Lemma 3, solution verification procedure state return false,
contradicting assumption algorithm terminated.
concludes proof.
final note, point results used prove MAFS versions search algorithms, best-first search, complete, thanks Corollary 1
Lemma 2. Corollary 1 guarantees focus solutions certain properties.
is, solution exists, solution properties exists. Lemma 2 ensures
every path properties generated, eventually, solution found.
note cannot provide guarantees distributed versions every search
algorithm simply arbitrary search algorithm may prune, reason, solutions properties ensured MAFS schema, keeping solutions.
course, completeness guarantees MAFS require liveness properties discussed
earlier hold.

6. Planning Framework
One main goals work provide general scalable framework solving
planning problem. believe framework provide researchers
fertile ground developing new search techniques heuristics planning,
extensions richer planning formalisms.
chose Fast Downward (Helmert, 2006) (FD) basis framework
MA-FD. FD currently leading framework planning, number
algorithms heuristics provides, terms performance winners
past three international planning competitions implemented top it. FD also
well documented supported, implementing testing new ideas relatively easy.
MA-FD uses FDs translator preprocessor, minor changes support distribution operators agents. agent also receives projected version public
operators agents. information (its actions projected public
actions) provided heuristic used agent. addition PDDL files describing domain problem instance, MA-FD receives file detailing number
agents, names, IP addresses. agents shared memory,
information relayed agents using messages. Inter-agent communication
performed using TCP/IP protocol, enables running multiple MA-FD agents
processes multi-core systems, networked computers/robots, even cloud platforms like
Amazon Web Services. MA-FD therefore fit run number (networked)
processors, optimal satisficing setting.
settings currently implemented available6 , since full flexibility
regarding heuristics used agents, heuristics available FD also available MAFD, requiring preprocessing agents view problem, creating projected
6. code available http://github.com/raznis/dist-selfish-fd .

313

fiNissim & Brafman

view. New heuristics easily implementable, FD, creating new search algorithms
also done minimal effort, since MA-FD provides ground-work (parsing,
communication, etc.).

7. Empirical Results
following section describes empirical evaluation methods. begin evaluating mafs, comparing current state-of-the-art approaches. describe results
mad-a*, compared centralized cost-optimal search. also provide scalability results
methods, scaling 40 agents.
7.1 Evaluating MAFS
evaluate mafs non-optimal setting, compare state-of-the-art distributed
planner map-pop (Torreno et al., 2012), Planning-First algorithm (Nissim et al.,
2010). noted Section 1, another available algorithm distributed planning
via reduction distributed CSPs using off-the-shelf disCSP solver. found
approach incapable solving even small planning problems, therefore omitted
results tables. problems used benchmarks International
Planning Competition (IPC) tasks naturally cast problems.
Satellites Rovers domains motivated real applications used NASA.
Satellites requires planning scheduling observation tasks multiple satellites,
equipped different imaging tools. Rovers involves multiple rovers navigating
planetary surface, finding samples communicating back Lander. Logistics,
Transport Zenotravel transportation domains, multiple vehicles transport
packages destination. Transport domain generalizes Logistics, adding capacity
vehicle (i.e., limit number packages may carry) different move
action costs depending road length. consider problems Rovers Satellites
domains loosely-coupled, i.e., problems agents many private actions (e.g.,
instrument warm-up placement Rovers, affect agents),
small number public actions required solution plans. hand,
consider transportation domains tightly-coupled, private actions (only
move actions Logistics) many public actions (load/unload actions).
planning problem, ran mafs, using eager best-first search alternation open list one queue two heuristic functions ff (Hoffmann &
Nebel, 2001) preferred actions context-enhanced additive heuristic (Helmert &
Geffner, 2008). Table 7.1 depicts results mafs, map-pop Planning-First, IPC
domains supported map-pop. also include results baseline centralized planner
(denoted FD, implemented top Fast-Downward) using eager best-first search
heuristics used mafs. Recall planner solves problem complete
knowledge, unlike configurations. compare algorithms across three categories 1) solution cost reports total cost outputted plan, 2) running time,
3) number messages sent planning process. Experiments run
AMD Phenom 9550 2.2GHZ processor, time limit set 60 minutes, memory
usage limited 4GB. configurations shown Table 7.1, experiments re314

fiDistributed Heuristic Forward Search

#
problem
agents
Logistics4-0
3
Logistics5-0
3
Logistics6-0
3
Logistics7-0
4
Logistics8-0
4
Logistics9-0
4
Logistics10-0
5
Logistics11-0
5
Logistics12-0
5
Logistics13-0
7
Logistics14-0
7
Logistics15-0
7
Rovers5
2
Rovers6
2
Rovers7
3
Rovers8
4
Rovers9
4
Rovers10
4
Rovers11
4
Rovers12
4
Rovers13
4
Rovers14
4
Rovers15
4
Rovers17
6
Satellites3
2
Satellites4
2
Satellites5
3
Satellites6
3
Satellites7
4
Satellites8
4
Satellites9
5
Satellites10
5
Satellites11
5
Satellites12
5
Satellites13
5
Satellites14
6
Satellites15
8
Satellites16
10
Satellites17
12

Solution cost
fd mafs map-pop p-f
21
20
20
X
28
27
27
X
26
25
25
X
43
36
37
X
32
31
31
X
39
36
36
X
50
45
51
X
54
54
X
X
47
44
45
X
85
87
X
X
68
68
X
X
94
95
X
X
22
22
24 24
38
37
39
X
18
18
18
X
26
26
27
X
40
38
36
X
37
38
X
X
42
37
34
X
21
21
20
X
48
49
X
X
33
31
35
X
43
46
44
X
53
52
X
X
11
11
11 11
26
17
20 20
21
16
15
X
20
20
20
X
28
22
22
X
27
26
26
X
37
30
29
X
37
30
29
X
36
31
31
X
43
43
49
X
75
61
X
X
49
44
43
X
64
63
X
X
62
56
56
X
48
49
49
X

Runtime
fd mafs map-pop p-f
0.1 0.05
20.9
X
0.1
0.1
90.4
X
0.1 0.06
60.6
X
0.1
0.2
233.3
X
0.1 0.16
261
X
0.1 1.02
193.3
X
0.1 0.43
471
X
0.1
2.7
X
X
0.1
1.3
1687
X
0.1
0.9
X
X
0.1 0.67
X
X
0.1 0.74
X
X
0.1 0.13
18.7 22.4
0.1 0.07
18.2
X
0.1 0.07
44.1
X
0.1
0.2
744
X
0.1 0.82
222
X
0.1 0.41
X
X
0.1 0.34
132.5
X
0.1 0.09
34.4
X
0.1 0.15
X
X
0.1 0.42
443.8
X
0.1 0.33
164
X
0.1 0.57
X
X
0.1 0.01
4.5 6.8
0.1 0.17
6.4 35.2
0.1 0.15
15.4
X
0.1 0.02
12.2
X
0.1 0.23
28.8
X
0.1 0.21
40.7
X
0.1 0.35
93.3
X
0.1 0.41
65.9
X
0.1 0.69
51
X
0.2
1.1
76.9
X
0.57 0.88
X
X
0.3
1.8
123.4
X
0.66
3.9
X
X
0.94
6.6
481.2
X
1.12
6.7
2681
X

Messages
mafs map-pop p-f
340
375
X
450
1565
X
470
1050
X
2911
4898
X
940
4412
X
2970
3168
X
2097
14738
X
14933
X
X
4230
28932
X
5140
X
X
2971
X
X
6194
X
X
84
323 590
27
313
X
225
490
X
937
12102
X
380
4467
X
271
X
X
299
2286
X
435
410
X
472
X
X
310
7295
X
252
2625
X
628
X
X
7
78 104
36
109 144
78
250
X
30
323
X
248
543
X
133
678
X
397
1431
X
355
942
X
514
904
X
390
1240
X
639
X
X
721
1781
X
1507
X
X
2279
4942
X
2172
26288
X

Table 1: Comparison greedy best-first search, map-pop Planning-First. Solution
cost, running time (in sec.) number sent messages shown. X
denotes problems werent solved one hour, 4GB memory
limit exceeded. Best performance entries bold.

stricted run single processor (core), running time would easily comparable7 .
X signifies problem solved within time-limit, exceeded memory
constraints.
7. result tables, multiple processors used.

315

fiNissim & Brafman

clear mafs overwhelmingly dominates map-pop Planning-First (denoted p-f), respect running time communication, solving problems faster
sending less messages. problems solved least 70 faster map-pop,
several Logistics Rovers problems solved 1000 faster, largest
Satellites instance solved 400 faster. low communication complexity
mafs important, since distributed systems message passing could costly
time-consuming local computation. Moreover, messages mafs essentially
state description, message size always linear number propositions. Although
problems map-pop finds lower-cost solutions, cases mafs outputs better
solution quality. believe mafs finds lower quality solutions, mostly
message-passing takes longer local computation subset agents
ability achieve goal own, made aware other,
less costly solutions including agents. One possible way improving solution quality
would using anytime search methods, improve solution quality time.
comparison reference centralized planner, mafs takes longer compute solution, cases solutions lower equal cost. slowdown expected,
communication times partial information mafs agents have, unlike
centralized planner.
7.1.1 Scalability MAFS
order evaluate scalability mafs, conducted experiments Logistics,
Rovers Satellite domains IPC. domains, generated multiple
problem instances, increasing number agents k. Logistics, number cities
grows linearly k, number airplanes remains constant 2, number
packages always 2k. Satellites Rovers, number targets grows linearly
k, number available observations/locations 2k, instrumentation remains
constant. experiments run Intel Xeon 2.4GHZ, 48-core machine. FD
run single processor mafs agent given dedicated processor. Cutoff
time configurations set 1 hour (Wall-clock time), memory limit set
100GB, regardless number processors used.
Results scalability experiment seen Figure 7.1.1. every value
k, 5 different problem instances generated. runtime values shown averages
5 them, error bars correspond standard deviation sample8 .
loosely-coupled domains Rovers Satellites, mafss increase runtime nearly
linear, largest problem instances domains unsolved
centralized FD, solved 90 seconds. Efficiency (speedup divided number
processors) values always superlinear 16 largest Satellites problem solved
configurations > 11 largest Rovers problems. superior
speed, mafs outputs lower quality solutions (having higher total cost) domains.
average, mafs solution cost 14% higher Rovers 6% higher Satellites,
maximal increase 20% 13% respectively. cause deterioration solution
quality domains likely fact many problems, small subsets
agents reach solution without agents. may lead mafs quickly finding
8. error bars omitted cases standard deviation small shown scale.

316

fiDistributed Heuristic Forward Search

solution involving subset, solution usually costly found
centralized search, considers operators agents. Logistics, tightlycoupled domain many actions public, problems still solved much faster (5
faster 40-agent problems) mafs, efficiency < 1 average 0.12
largest instances. Here, solution cost average 0.5% higher centralized search,
maximal increase 2.5%, maximal decrease (improvement solution
quality) 2%. Overall, see w.r.t. runtime, mafs scalable performing
superlinear efficiency loosely-coupled domains, exhibiting speedup tightlycoupled ones.
7.2 Evaluating MAD-A*
evaluate mad-a* respect centralized optimal search (a*), ran algorithms
using state-of-the-art Merge&Shrink heuristic9 (Helmert, Haslum, & Hoffmann, 2007).
configurations run machine, mad-a*, agent
allocated single processor, a* run single processor. Wall-clock time limit
set 30 minutes, memory usage limited 4GB, regardless number cores
used. show results problems constitute limit either configuration.
existing IPC problems domains solvable configurations. Table
7.2 depicts runtime, efficiency (speedup divided number processors), number
expanded nodes average agents initial state h-values.
comparing mad-a* centralized a*, intuition efficiency low,
due inaccuracy agents heuristic estimates, overhead incurred
communication. fact, local estimates agents much less accurate
global heuristic, apparent lower average h values initial state,
given approximate measures heuristic quality discussing admissible heuristics
(which required optimal search), higher values necessarily accurate.
tightly-coupled domains Logistics, Transport Zenotravel, notice low
efficiency values, mostly due large number public actions, result many
messages passed agents, relatively small amount local (private)
search agents. However, loosely-coupled domains Satellites Rovers,
mad-a* exhibits nearly linear super-linear speedup, solving 2 problems solved
centralized a*. analyze reason superlinear speedup elaborate
important issue Section 8.
7.2.1 Scalability MAD-A*
results Table 7.2, clear mad-a* scale well tightly-coupled
domain Logistics, Transport Zenotravel. loosely-coupled domains
Rovers Satellites, however, mad-a* exhibited high efficiency, outperforming centralized
search cases. section examines scalability mad-a* domains
compared centralized a*.
Section 7.1.1, created new instances problems increasing agents using
problems generators. domains, number available locations goals increased
9. used exact bisimulation abstraction size limit 10K (DFP-bop) shrink strategy
Merge&Shrink (Nissim, Hoffmann, & Helmert, 2011).

317

fiNissim & Brafman

Logistics

Satellites

FD
MAFS

3,500
3,000

2,500

Runtime

Runtime

2,500
2,000
1,500
1,000

2,000
1,500
1,000

500

500

0

0
20

25

FD
MAFS

3,000

30

35

40

15

Number agents

20

25

30

35

40

Number agents

Rovers
1,400

FD
MAFS

1,200

Runtime

1,000
800
600
400
200
0
10

15

20

25

30

35

40

Number agents

Figure 2: Scalability MAFS w.r.t. centralized FD. configurations, runtime
seconds shown.

318

fiDistributed Heuristic Forward Search

problem
Logistics4-0
Logistics5-0
Logistics6-0
Logistics7-0
Logistics8-0
Logistics9-0
Logistics10-0
Logistics11-0
Rovers3
Rovers4
Rovers5
Rovers6
Rovers7
Rovers12
Satellites3
Satellites4
Satellites5
Satellites6
Satellites7
Transport1
Transport2
Transport3
Transport4
Transport5
Zenotravel3
Zenotravel4
Zenotravel5
Zenotravel6
Zenotravel7
Zenotravel8
Zenotravel9
Zenotravel10
Zenotravel11
Zenotravel12

agents
3
3
3
4
4
4
5
5
2
2
2
2
3
4
2
2
3
3
4
2
2
2
2
2
2
2
2
2
2
3
3
3
3
3

a*
0.07
0.16
0.29
1.42
1.28
2.17
132
180
0.2
0.07
8.24
X
32.4
190.8
0.3
0.6
16.83
1.93
X
0.02
0.17
1.35
54.6
X
0.33
0.32
0.3
0.47
0.55
1.22
31.7
9.3
2.99
X

Time
mad-a* Efficiency
0.03
0.78
0.13
0.41
0.15
0.64
8.26
0.04
2.89
0.11
11.6
0.05
X
0.00
X
0.00
0.11
0.91
0.04
0.88
4.4
0.94
301

6.82
1.58
27.6
1.73
0.29
0.52
0.4
0.75
3.9
1.44
0.92
0.70
18.26

0.08
0.13
0.21
0.40
20.64
0.03
335.15
0.08
X
N/A
0.42
0.39
0.43
0.37
0.42
0.36
0.61
0.39
0.8
0.34
1.71
0.24
315
0.03
338
0.01
11.7
0.09
X
N/A

Expansions
a*
mad-a*
21
2496
28
11020
547
9722
29216
520122
16771
157184
43283
687572
4560551
X
5713287
X
12
86
9
121
213079
307995
X 18274357
1172964
676750
4963979
2591428
12
1498
18
5045
236647
42557
1382
81731
X
1303910
6
285
242
1628
69500
546569
3527592
4397124
X
X
7
516
9
762
14
577
270
1280
621
5681
136
5461
775340
6745088
116872
4416461
20157
200868
X
X

a*
20
27
24
33
27
33
37
41
11
8
12
24
9
11
11
17
10
17
10
54
79
34
10
12
6
7
10
10
11
9
16
20
11
16

init-h
mad-a*
17
23
22
29
24
29
34
38
9
7
11
21
7
7
5
11
7
12
9
4
4
4
10
10
5
6
8
8
9
7
14
17
9
14

Table 2: Comparison centralized a* mad-a* running multiple processors. Running time (in sec.), average initial state h-values mad-a*s efficiency values
w.r.t. a* shown. Entries bold denote super-linear efficiency mad-a*.

linearly number agents k. value k, created 5 problem instances,
reported runtime values averages. experiments run Intel
Xeon machine used Section 7.1.1, a* run single processor,
mad-a* agent given dedicated processor. Runtime limit set 90 minutes
memory limited 100GB regardless number processors.
Figure 7.2.1 see domains, mad-a* solves problems faster
centralized a*, solve larger problems within time limit. However, efficiency
varies two domains Satellites, loosely-coupled, efficiency
superlinear cases, reaching 7.3 largest problem solved configura319

fiNissim & Brafman

Rovers

Satellites

A*
MAD-A*

2,500
2,000

4,000

Runtime

Runtime

A*
MAD-A*

5,000

1,500
1,000

3,000
2,000

500

1,000

0

0
3

4

5

6

4

Number agents

5

6

7

8

9

10

Number agents

Figure 3: Scalability MAD-A* w.r.t. centralized A* loosely-coupled domains.
configurations, runtime seconds shown.

tions. Rovers, tightly-coupled, efficiency 0.5 problems, dropping
low 0.35 largest problem solved configurations. Recall problem
solved mad-a* difficult, since agent private information unknown
others, drop efficiency expected. mentioned previous section, next
elaborate reason mad-a*s superlinear efficiency loosely-coupled problems.

8. Partition-Based Path Pruning
empirical results presented previous section raise interesting question:
MAD-A* achieve > 1 efficiency weakly-coupled environments? known
using consistent heuristic, a* optimal number nodes expands
recognize optimal solution. principle, appears mad-a* expand least
search tree, clear, a-priori, reach super-linear speedup
comparing a*. section provides explanation phenomenon partial
order reduction method inherently built-in mad-a* (and mafs well), exploits
symmetry search space prune effect-equivalent paths. describe pruning
method detail, showing empirical evidence supporting claim within method
lies much power mad-a*.
exploitation symmetry utilizes notion public private actions.
noted Corollary 1, existence private actions implies existence multiple
effect-equivalent permutations certain action sequences. a* recognize exploit
fact, mafs does. Specifically, imagine agent generated state using
one public actions, satisfies preconditions public action agent
j . Agent eventually send agent j , latter eventually apply
it. Now, imagine agent private action a0 applicable state s, resulting
320

fiDistributed Heuristic Forward Search

state s0 = a0 (s). a0 private , fact applicable deduce
applicable s0 well. Hence, a* would apply s0 . However, mafs, agent
j would apply s0 receive s0 agent . Thus, mafs
explore possible action sequences. fact also clearly seen example
given Figure 4.1 reachable search space example 31 states,
number reachable states using mafs 16.
Since mafss inherent pruning action sequences requires partitioning
actions, pertain systems, factored system internal
operators. Since difference ma-strips planning problem strips
one fact actions partitioned agents, re-factor centralized
problem artificial one? mapping actions disjoint sets
Sk
Ai = A, representing agent, distinguish private
public operators. Given distinction, pruning rule used simple:
Partition-Based (PB) Pruning Rule: Following private action Ai ,
prune actions Ai .
fact pruning rule optimality-preserving (i.e., prune optimal
solutions) follows immediately Corollary 1 as, exists optimal solution ? ,
permuted legal, optimal plan pruned. This, however,
enough maintain optimality a* search. present slight modification
a* algorithm, allows application optimality preserving pruning methods
(such PB-pruning) purpose optimal planning.
8.1 Path Pruning A*
path pruning a*, (denoted pp-a*), search algorithm receives planning
problem pruning method input, produces plan , guaranteed
optimal provided respects following properties: (i) optimality preserving,
(ii) prunes according last action. easy see, example, PB
pruning respects second condition, since fires according last action.
8.1.1 pp-a* versus a*
pp-a* identical a* except following three changes. First, different data-type
used recording open node. pp-a*, open list node pair (A, s),
state set actions, recording various possible ways reach previous
state. Second, node expansion subject pruning rules method . Namely, ppa* executes applicable action a0 state (A, s) least one action
s.t. execution a0 allowed pruning rules. Third, duplicate states
handled differently. a*, state already open reached another
search path, open list node updated action lower g value, case
tie drops competing path. contrast, ties pp-a* handled preserving
last actions led paths. Hence, action led open state
via path cost g, existing open list node (A, s) g value,
node updated (A {a}, s), thus actions leading path cost g saved. Tie
breaking also affects criterion closed nodes reopened. a*, nodes
321

fiNissim & Brafman

reopened reached via paths lower g values. pp-a*, action leading
state closed node (A, s) contained A, g values equal,
node reopens ({A {a}}, s). However, node expanded, actions
allowed previously pruned, executed. move prove
correctness pp-a*.
8.1.2 Proof Correctness Optimality
next lemma refers pp-a*, assumes optimality preserving pruning
method, prunes according last action. say node (A, s) optimal
path P , contains action leads state path P , g(s) = g (s).
notation P s0 denotes fact state precedes state s0 optimal path P .
Lemma 4. pp-a*, non-closed state sk optimal non--pruned path
P sk , exists open list node (A0 , s0 ) optimal P .
Proof. Let P optimal non--pruned path sk . open list, let
s0 = lemma trivially true since g(I) = g (I) = 0. Suppose closed. Let
set nodes (Ai , si ), optimal P , closed. empty, since
assumption, . Let nodes ordered si P sj < j, let
j highest index si .
Since closed node (Aj , sj ) optimal g value, expanded prior
closing. properties pp-a*, follows expansion (Aj , sj ),
optimal P , followed attempt generate node (Aj+1 , sj+1 ) optimal
P well. Generation (Aj+1 , sj+1 ) must allowed, since highest index
assumption closed node containing optimal P . Naturally,
sj P sj+1 .
point, note actions Aj+1 cannot removed competing path
sj+1 , since (Aj+1 , sj+1 ) optimal g value. possible, though, additional actions leading sj+1 added node. updated node represented
(A0j+1 Aj+1 , sj+1 ), property optimality P holds. Additionally, node
(A0j+1 , sj+1 ) cannot closed generation, since again, contradicts highest
index property. Hence, exists open list node (A0 , s0 ) optimal P .
concludes proof.
Corollary 3. h admissible optimality-preserving, pp-a* using optimal.
Proof. follows directly Lemma 4, optimality preserving property
properties pp-a*, allow every optimal, non--pruned path generated.
8.2 Empirical Analysis PB-Pruning
set check effect mad-a*s inherent exploitation symmetry efficiency
compared a*. hypothesis mad-a*s main advantage a* well
supported results Table 8.2, shows comparison mad-a* centralized
a* using PB pruning. Here, see problems mad-a* achieves superlinear
speedup w.r.t. a*, applying partition-based pruning partition=agent reduces runtime
expansions dramatically. cases, mad-a*s efficiency w.r.t. a* using PB pruning
322

fiDistributed Heuristic Forward Search

problem
Logistics4-0
Logistics5-0
Logistics6-0
Logistics7-0
Logistics8-0
Logistics9-0
Logistics10-0
Logistics11-0
Rovers3
Rovers4
Rovers5
Rovers6
Rovers7
Rovers12
Satellites3
Satellites4
Satellites5
Satellites6
Satellites7
Transport1
Transport2
Transport3
Transport4
Zenotravel3
Zenotravel4
Zenotravel5
Zenotravel6
Zenotravel7
Zenotravel8
Zenotravel9
Zenotravel10
Zenotravel11
Zenotravel12

agents
3
3
3
4
4
4
5
5
2
2
2
2
3
4
2
2
3
3
4
2
2
2
2
2
2
2
2
2
3
3
3
3
3

a*
0.07
0.16
0.29
1.42
1.28
2.17
132
180
0.2
0.07
8.24
X
32.4
190.8
0.3
0.6
16.83
1.93
X
0.02
0.17
1.35
54.6
0.33
0.32
0.3
0.47
0.55
1.22
31.7
9.3
2.99
X

Time
a?pb mad-a*
0.06
0.03
0.17
0.13
0.29
0.15
1.07
8.26
1.07
2.89
1.59
11.6
37
X
57.4
X
0.2
0.11
0.06
0.04
3.07
4.4
164.9
301
5.22
6.82
10.1
27.6
0.29
0.29
0.58
0.4
3.15
3.9
1.84
0.92
26.6
18.26
0.01
0.08
0.16
0.21
0.9
20.64
29.8
335.15
0.34
0.42
0.33
0.43
0.3
0.42
0.47
0.61
0.56
0.8
1.21
1.71
12.88
315
6.51
338
2.02
11.7
82.95
X

Efficiency
a*
a?pb
0.78 0.67
0.41 0.44
0.64 0.64
0.04 0.03
0.11 0.09
0.05 0.03
0
0
0
0
0.91 0.91
0.88 0.75
0.94 0.35
0.27
1.58 0.26
1.73 0.09
0.52 0.50
0.75 0.73
1.44 0.27
0.70 0.67
0.36
0.13 0.06
0.40 0.38
0.03 0.02
0.08 0.04
0.39 0.40
0.37 0.38
0.36 0.36
0.39 0.39
0.34 0.35
0.24 0.24
0.03 0.01
0.01 0.01
0.09 0.06
N/A
0

a*
21
28
547
29216
16771
43283
4560551
5713287
12
9
213079
X
1172964
4963979
12
18
236647
1382
X
6
242
69500
3527592
7
9
14
270
621
136
775340
116872
20157
X

Expansions
a?pb
21
28
527
22425
11750
29953
2132416
2980725
12
9
62672
8107327
235537
391372
12
18
23503
385
846394
6
225
49744
2589496
7
9
14
220
433
126
474180
104340
11565
2406708

mad-a*
2496
11020
9722
520122
157184
687572
X
X
86
121
307995
18274357
676750
2591428
1498
5045
42557
81731
1303910
285
1628
546569
4397124
516
762
577
1280
5681
5461
6745088
4416461
200868
X

Table 3: Comparison centralized a* without partition-based pruning, mada* running multiple processors. Running time, number expanded nodes,
mad-a*s efficiency w.r.t. centralized configurations shown.

sublinear. is, course, also due fact mad-a* solves difficult
problem incomplete information negative effect quality heuristics
computed agents.
note although structure evident benchmark planning domains
(e.g. Logistics, Rovers, Satellites, Zenotravel etc.), general isnt always obvious
way decomposing problem. work exploring PB pruning (Nissim, Apsel,
& Brafman, 2012), describe automated method decomposing general planning problem, making PB pruning applicable general classical planning setting.
note exist many partial-order reduction methods, Commutativity
Pruning (Haslum & Geffner, 2000), Stubborn Sets (Valmari, 1989; Wehrle & Helmert,
323

fiNissim & Brafman

2012; Alkhazraji, Wehrle, Mattmuller, & Helmert, 2012; Wehrle, Helmert, Alkhazraji, &
Mattmuller, 2013), Expansion Core (Chen & Yao, 2009), (Coles & Coles, 2010;
Xu, Chen, Lu, & Huang, 2011). None methods subsumes PB pruning,
interesting question combine methods maintaining optimality remains
open field classical planning.

9. Discussion
work raises number questions, research challenges, opportunities
discuss.
9.1 Privacy
noted earlier, algorithms weakly privacy preserving. is, information
private actions, cost, private variables, private preconditions effects
ever communicated agent agents. Nevertheless, case DisCSP
discussed earlier, information elements could deduced agents
run algorithm. given privacy preservation important goal
work, important try understand extent information leak.
examine this, let us consider maximal amount explicit information
available agent agents. worse case, would explore
entire search tree rooted initial state run algorithm. However,
agent see nodes tree, nodes correspond search states
obtained following public action (because states communicated). Thus,
sub-tree visible agent corresponds one obtained full search tree topdown recursive process every state obtained following private action removed,
parent removed node becomes parent children. Furthermore,
information available state agent consists value local
variables state value public variables. (Recall that, private state
encrypted using different identifier, local states agents appear different
every state, provide useful information).
agent learn projected search tree? First, try identify
different states identical. Two states whose sub-trees identical deduced
local states agents. Notice information cannot deduced
plan alone. Consequently, cannot claim search-based methods
strong privacy preserving. difficult identify fact two states
local state particular agent only. Let us, however, assume worst case
scenario states also identifiable use term projected subtree state ids refer structure. Thus, agent knows possible values,
renaming, local states agent occur following execution
public actions. also knows existence macros consisting sequence
private actions followed public action allow agent move private
state following public action next private state following execution next
public action. not, however, know structure local state
deduce monolithic name. know nature local actions comprising
macros enable transition two post-public-action states.
324

fiDistributed Heuristic Forward Search

strong privacy preservation guaranteed, possible show
present per domain basis distributed forward search algorithms offer
weak privacy preservation. example, return logistics domain. Imagine
truck various service stations rest-stops locations private (i.e.,
served truck). existence visible agents.
proven formally showing projected search tree visible external agent
identical regardless number private locations truck. projected
search tree contains information available agents, implies
information available agents cases. see projected
tree same, recall public actions truck load unload. Furthermore,
note loading unloading package location accessible
agent private action truck preconditions effects required
affected agent. Thus, states visible projected search tree
states follow load/unload action location served agents well,
airport. states, local state truck reflects location.
Consequently, local state corresponds private location part
projected search tree. is, projected search tree whether agent
no, one, number private service locations.
Another piece private information could existence private packages.
is, packages delivered private location another private location. Here,
proof work local state truck could different following
unloading (public) package public location, simply variable denoting
location private package could different values, technically part
local state (because truck influence value). fact, situation
similar case (of private service stations) packages unloaded
private service stations. Again, local states package located
station would appear projected search tree. However, cases, external agents
cannot infer larger set private states stems fact private
package private service station. may well denote something else, e.g., whether
driver hungry thirsty sleepy, etc. aware existence
larger domain private states.
Similar techniques used show inner processes used manufacturers laptop example visible manufacturers: much
like different locations truck.
discussion makes clear distributed forward search able provide important privacy guarantees. Future work seek go beyond domain-dependent
reasoning provide general, domain independent conditions strong privacy
guarantees obtained. believe doable ideas
good starting point.
context, useful keep mind following observations projected
sub-tree ids: 1. practical search algorithm explore entire search tree,
extent search space explored decreases, difficulty identifying identical local
states increases. empirical results indicate many problems solved quickly
using distributed forward search, without expanding many nodes. questionable
whether possible build reasonable models agents private states cases.
325

fiNissim & Brafman

Here, empirical theoretical work similar conducted distributed CSPs,
desirable. 2. clear whether possible identify two states share similar
local states among agents, especially given small portion search
tree. 3. number macros consisting sequence private actions followed
private one exponential. Thus, reasonable search process, small portion
visible. anecdotal evidence note actually attempted construct
macros order improve heuristic computation, number exploded quickly,
became impractical.
Another interesting alternative develop variants current algorithms
stronger privacy preserving properties. example, consider problem inferring
upper bounds minimal cost applying action public state s. general, private
actions achieve preconditions public action applied immediately
public action agent perform private actions required
public action previous public action. words, agent distribute
private cost public action different segments, parts plan
two public actions, making cost first action appear higher cost
second action lower, although potential impact optimality. case
non-optimal search, g-values disclosed, issue.
example illustrates general idea: one trade-off efficiency privacy.
similar tradeoff explored area differential privacy (Dwork, 2006). There,
noise inserted database statistical queries evaluated,
answer statistical query correct within given tolerance, , yet one cannot
infer information particular entry database (e.g., describing medical
record individual). Similarly, context, one consider algorithms
agents refrain sending certain public states probability, send
random delay, even possibly, generate bogus, intermediate public states. changes
likely impact running time solution quality, tradeoffs
would interesting explore.
believe area matures, much like area DisCSP, attention
given problem precise quantification privacy privacy loss.
work brings us closer stage. offers algorithms distributed search start
match centralized search, general methodology distributed forward search
respects natural distributed structure system, form basis
extensions, initial ideas formal privacy proofs work.
additional privacy-related question specification privacy properties.
ma-strips distinction private public variables derived certain way
domain definition. Recent work builds ma-strips suggests slightly
different treatment privacy. example, Bonisoli et al. (2014) allow finer notions
privacy, variables private subset agents, rather single
agent. Moreover, privacy requirements part problem description,
variable would private ma-strips could made public privacy
important maintain. extends earlier models, set variables visible
agent explicitly stated (Torreno, Onaindia, & Sapena, 2014), set
private variables agent derived, ma-strips rather specified (Luis
& Borrajo, 2014). One might also consider distinguishing public variables
326

fiDistributed Heuristic Forward Search

write-only public variables. example, agent may know value certain
variable, except immediately assigns value. would interesting explore
ability algorithms, well others, exploit notions privacy.
9.2 Accurate Heuristics Given Incomplete Information
empirical results presented lead us perhaps greatest practical challenge
suggested mafs mad-a* computing accurate heuristic distributed (privacypreserving) system. Despite theoretical results saying even almost perfect
heuristics lead large (exponential) search spaces (Helmert & Roger, 2008), practically,
accurate heuristic large effect search efficiency. domains,
existence private information shared leads serious deterioration
quality heuristic function, greatly increasing number nodes expanded, and/or
affecting solution quality. believe techniques used alleviate
problem. simple example, consider public action apub applied
private action apriv . example, Rovers domain, send message
applied various private actions required collect data executed.
cost apub known agents would reflect cost apriv well, heuristic
estimates would accurate. Another possibility improving heuristic estimates
using additive heuristic. case, rather taking maximum agents
heuristic estimate estimate sending agent, two could added.
maintain admissibility, would require using something like cost partitioning (Katz &
Domshlak, 2008). One obvious way would give agent full cost
actions zero cost actions. problem approach initially,
state generated estimate available generating agent,
estimate inaccurate, since assigns 0 actions. fact, agent
inclined prefer actions performed agents, appear cheap,
see especially poor results domains different agents achieve goal,
Rovers domain, resulting estimates 0 many non-goal states. Therefore,
effectively compute accurate heuristics distributed setting important
research challenge.
One first attempts address issue recent paper Maliah et al. (2014).
Building ma-strips model mafs approach discussed here, suggests
mechanism propagating information landmarks among agents without revealing
private information. Using approach agent detect landmarks
detected projected problems, resulting informative heuristic agent.

10. Summary
presented formulation heuristic forward search classical planning respects natural distributed structure system, preserving agent privacy. mafs
dominates state-of-the-art distributed planners w.r.t. runtime communication,
well solution quality cases. class privacy-preserving algorithms, mad-a*
first cost-optimal distributed planning algorithm, competitive centralized counterpart, despite partial information. studied strengths
weaknesses methods, providing empirical evidence claims. algorithms
327

fiNissim & Brafman

shown scalable, solving problems 40 agents high efficiency, especially loosely-coupled domains. distributed planning framework MA-FD, provides
researchers good starting point future research new algorithms heuristics
privacy-preserving setting.
many interesting directions future research. Recently, work presented
formed basis work mechanism design privacy-preserving planning
(Nissim & Brafman, 2013). Specifically, mad-a* used underlying cost-optimal
planner distributed implementation Vickerey-Clarke-Groves mechanism.
results distributed method cost-optimal planning self-interested agents
private information. interesting explore methods computing
accurate heuristics, inevitable trade-off privacy, accurate heuristics
communication complexity. One could also explore modify methods deal
extensions ma-strips model (e.g. include joint actions), different
solution criteria (e.g. makespan). Finally, notion private/public actions refined
distinguish read-write public actions read-only ones. distinction could
effect search methods heuristics, interesting avenue future
research.
Acknowledgments
authors grateful Associate Editor anonymous referees many
useful suggestions corrections. work supported part ISF grant 1101/07
Lynn William Frankel Center Computer Science.

References
Albore, A., Palacios, H., & Geffner, H. (2009). translation-based approach contingent
planning. Boutilier, C. (Ed.), IJCAI, pp. 16231628.
Albore, A., Palacios, H., & Geffner, H. (2010). Compiling uncertainty away nondeterministic conformant planning. ECAI, pp. 465470.
Alkhazraji, Y., Wehrle, M., Mattmuller, R., & Helmert, M. (2012). stubborn set algorithm
optimal planning. ECAI, pp. 891892.
Amir, E., & Engelhardt, B. (2003). Factored planning. IJCAI, pp. 929935.
Backstrom, C. (1998). Computational aspects reordering plans. J. Artif. Intell. Res.
(JAIR), 9, 99137.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity
decentralized control Markov Decision Processes. Math. Oper. Res., 27 (4), 819840.
Bonisoli, A., Gerevini, A., Saetti, A., & Serina, I. (2014). privacy-preserving model
multi-agent propositional planning problem. ICAPS14 Workshop Distributed
Multi-Agent Planning.
Brafman, R. I., & Domshlak, C. (2006). Factored planning: How, when, not.
AAAI.
328

fiDistributed Heuristic Forward Search

Brafman, R. I., & Domshlak, C. (2008). one many: Planning loosely coupled
multi-agent systems. ICAPS, pp. 2835.
Brafman, R. I., Domshlak, C., Engel, Y., & Tennenholtz, M. (2009). Planning games.
IJCAI, pp. 7378.
Brafman, R. I., Domshlak, C., Engel, Y., & Tennenholtz, M. (2010). Transferable utility
planning games. AAAI.
Burns, E., Lemons, S., Ruml, W., & Zhou, R. (2010). Best-first heuristic search multicore
machines. J. Artif. Intell. Res. (JAIR), 39, 689743.
Chandy, K. M., & Lamport, L. (1985). Distributed snapshots: Determining global states
distributed systems. ACM Trans. Comput. Syst., 3 (1), 6375.
Chen, Y., & Yao, G. (2009). Completeness optimality preserving reduction planning.
IJCAI, pp. 16591664.
Clarke, E. M., Biere, A., Raimi, R., & Zhu, Y. (2001). Bounded model checking using
satisfiability solving. Formal Methods System Design, 19 (1), 734.
Coles, A. J., & Coles, A. (2010). Completeness-preserving pruning optimal planning.
ECAI, pp. 965966.
Conry, S. E., Kuwabara, K., Lesser, V. R., & Meyer, R. A. (1991). Multistage negotiation
distributed constraint satisfaction. IEEE Transactions Systems, Man,
Cybernetics, 21 (6), 14621477.
Cox, J. S., & Durfee, E. H. (2005). efficient algorithm multiagent plan coordination.
AAMAS, pp. 828835. ACM.
Cox, J. S., & Durfee, E. H. (2009). Efficient distributable methods solving
multiagent plan coordination problem. Multiagent Grid Systems, 5 (4), 373408.
Dwork, C. (2006). Differential privacy. ICALP (2), pp. 112.
Ephrati, E., & Rosenschein, J. S. (1994). Divide conquer multi-agent planning.
AAAI, pp. 375380.
Ephrati, E., & Rosenschein, J. S. (1997). heuristic technique multi-agent planning.
Ann. Math. Artif. Intell., 20 (1-4), 1367.
Fabre, E., & Jezequel, L. (2009). Distributed optimal planning: approach weighted
automata calculus. CDC, pp. 211216. IEEE.
Fabre, E., Jezequel, L., Haslum, P., & Thiebaux, S. (2010). Cost-optimal factored planning:
Promises pitfalls. ICAPS, pp. 6572.
Franzin, M. S., Rossi, F., Freuder, E. C., & Wallace, R. J. (2004). Multi-agent constraint
systems preferences: Efficiency, solution quality, privacy loss. Computational
Intelligence, 20 (2), 264286.
Greenstadt, R., Grosz, B. J., & Smith, M. D. (2007). SSDPOP: improving privacy
DCOP secret sharing. AAMAS, p. 171.
Greenstadt, R., Pearce, J. P., & Tambe, M. (2006). Analysis privacy loss distributed
constraint optimization. AAAI, pp. 647653.
329

fiNissim & Brafman

Grinshpoun, T., & Tassa, T. (2014). privacy-preserving algorithm distributed constraint optimization. AAMAS14.
Gupta, D., Segal, A., Panda, A., Segev, G., Schapira, M., Feigenbaum, J., Rexford, J., &
Shenker, S. (2012). new approach interdomain routing based secure multiparty computation. HotNets, pp. 3742.
Hansen, E. A., & Zilberstein, S. (2001). LAO* : heuristic search algorithm finds
solutions loops. Artif. Intell., 129 (1-2), 3562.
Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. AIPS, pp.
140149.
Helmert, M. (2006). fast downward planning system. J. Artif. Intell. Res. (JAIR), 26,
191246.
Helmert, M., & Geffner, H. (2008). Unifying causal graph additive heuristics.
ICAPS, pp. 140147.
Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics optimal
sequential planning. ICAPS, pp. 176183.
Helmert, M., & Roger, G. (2008). good almost perfect?. AAAI, pp. 944949.
Hoffmann, J., & Nebel, B. (2001). FF planning system: fast plan generation
heuristic search. J. Artif. Int. Res., 14 (1), 253302.
Holte, R. C. (2010). Common misconceptions concerning heuristic search. SOCS.
ICAPS.
international planning competition.
ipc2011-deterministic/.

http://www.plg.inf.uc3m.es/

Katz, M., & Domshlak, C. (2008). Optimal additive composition abstraction-based
admissible heuristics. ICAPS, pp. 174181.
Kautz, H. A., & Selman, B. (1992). Planning satisfiability. ECAI, pp. 359363.
Keyder, E., & Geffner, H. (2009). Soft goals compiled away. J. Artif. Intell. Res.
(JAIR), 36, 547556.
Kishimoto, A., Fukunaga, A. S., & Botea, A. (2009). Scalable, parallel best-first search
optimal sequential planning. ICAPS.
Leaute, T., & Faltings, B. (2009). Privacy-preserving multi-agent constraint satisfaction.
CSE (3), pp. 1725.
Luis, N., & Borrajo, D. (2014). Plan merging reuse multi-agent planning. ICAPS14
Workshop Distributed Multi-Agent Planning.
Maheswaran, R. T., Pearce, J. P., Bowring, E., Varakantham, P., & Tambe, M. (2006).
Privacy loss distributed constraint reasoning: quantitative framework analysis
applications. Autonomous Agents Multi-Agent Systems, 13 (1), 2760.
Maliah, S., Shani, G., & Stern, R. (2014). Privacy preserving landmark detection.
ECAI14.
Meisels, A. (2007). Distributed Search Constrained Agents: Algorithms, Performance,
Communication (Advanced Information Knowledge Processing). Springer.
330

fiDistributed Heuristic Forward Search

Mero, L. (1984). heuristic search algorithm modifiable estimate. Artif. Intell., 23 (1),
1327.
Nair, R., Tambe, M., Yokoo, M., Pynadath, D., & Marsella, S. (2003). Taming decentralized
pomdps: Towards efficient policy computation multiagent settings. IJCAI.
Nissim, R., Apsel, U., & Brafman, R. I. (2012). Tunneling decomposition-based state
reduction optimal planning. ECAI, pp. 624629.
Nissim, R., & Brafman, R. I. (2013). Cost-optimal planning self-interested agents.
AAAI.
Nissim, R., Brafman, R. I., & Domshlak, C. (2010). general, fully distributed multi-agent
planning algorithm. AAMAS, pp. 13231330.
Nissim, R., Hoffmann, J., & Helmert, M. (2011). Computing perfect heuristics polynomial
time: bisimulation merge-and-shrink abstraction optimal planning.
IJCAI, pp. 19831990.
Oliehoek, F. A., Witwicki, S. J., & Kaelbling, L. P. (2012). Influence-based abstraction
multiagent systems. AAAI.
Palacios, H., & Geffner, H. (2009). Compiling uncertainty away conformant planning
problems bounded width. J. Artif. Intell. Res. (JAIR), 35, 623675.
Petcu, A., & Faltings, B. (2005). scalable method multiagent constraint optimization.
IJCAI, pp. 266271.
Petcu, A., Faltings, B., & Parkes, D. C. (2008). M-DPOP: Faithful distributed implementation efficient social choice problems. J. Artif. Intell. Res. (JAIR), 32, 705755.
Ranjit, N., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed
pomdps: synthesis distributed constraint optimization pomdps. AAAI.
Silaghi, M.-C., & Mitra, D. (2004). Distributed constraint satisfaction optimization
privacy enforcement. IAT, pp. 531535.
Srivastava, S., Immerman, N., Zilberstein, S., & Zhang, T. (2011). Directed search
generalized plans using classical planners. ICAPS.
Steenhuisen, J. R., Witteveen, C., ter Mors, A., & Valk, J. (2006). Framework complexity results coordinating non-cooperative planning agents. MATES, pp. 98109.
Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithm
solving decentralized POMDPs. UAI, pp. 576590.
Taig, R., & Brafman, R. I. (2013). Compiling conformant probabilistic planning problems
classical planning. ICAPS.
ter Mors, A., Valk, J., & Witteveen, C. (2004). Coordinating autonomous planners.
IC-AI, pp. 795.
ter Mors, A., & Witteveen, C. (2005). Coordinating self interested autonomous planning
agents. BNAIC, pp. 383384.
Torreno, A., Onaindia, E., & Sapena, O. (2012). approach multi-agent planning
incomplete information. ECAI, pp. 762767.
331

fiNissim & Brafman

Torreno, A., Onaindia, E., & Sapena, O. (2014). Fmap: Distributed cooperative multi-agent
planning. Applied Intelligence, 41 (2), 606626.
Valmari, A. (1989). Stubborn sets reduced state space generation. Applications
Theory Petri Nets, pp. 491515.
Vrakas, D., Refanidis, I., & Vlahavas, I. P. (2001). Parallel planning via distribution
operators. J. Exp. Theor. Artif. Intell., 13 (3), 211226.
Wehrle, M., & Helmert, M. (2012). partial order reduction planning computer
aided verification. ICAPS.
Wehrle, M., Helmert, M., Alkhazraji, Y., & Mattmuller, R. (2013). relative pruning
power strong stubborn sets expansion core. ICAPS.
Witwicki, S. J., & Durfee, E. H. (2010). Influence-based policy abstraction weaklycoupled dec-pomdps. ICAPS, pp. 185192.
Witwicki, S. J., Oliehoek, F. A., & Kaelbling, L. P. (2012). Heuristic search multiagent
influence space. AAMAS, pp. 973980.
Xu, Y., Chen, Y., Lu, Q., & Huang, R. (2011). Theory algorithms partial order
based reduction planning. CoRR, abs/1106.5427.
Yang, Q., Wu, K., & Jiang, Y. (2007). Learning action models plan examples using
weighted MAX-SAT. Artif. Intell., 171 (2-3), 107143.
Yao, A. C.-C. (1982). Protocols secure computations (extended abstract). FOCS, pp.
160164.
Yao, A. C.-C. (1986). generate exchange secrets (extended abstract). FOCS,
pp. 162167.
Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1998). distributed constraint
satisfaction problem: Formalization algorithms. IEEE Trans. Knowl. Data Eng.,
10 (5), 673685.
Yokoo, M., Suzuki, K., & Hirayama, K. (2002). Secure distributed constraint satisfaction:
Reaching agreement without revealing private information. CP, pp. 387401.
Yoon, S. W., Fern, A., & Givan, R. (2007). FF-Replan: baseline probabilistic planning.
ICAPS, pp. 352.

332

fiJournal Artificial Intelligence Research 51 (2014) 165-205

Submitted 05/14; published 09/14

Simple Regret Optimization Online Planning
Markov Decision Processes
Zohar Feldman
Carmel Domshlak

zoharf@tx.technion.ac.il
dcarmel@ie.technion.ac.il

Faculty Industrial Engineering & Management,
Technion - Israel Institute Technology,
Haifa, Israel

Abstract
consider online planning Markov decision processes (MDPs). online planning,
agent focuses current state only, deliberates set possible policies
state onwards and, interrupted, uses outcome exploratory deliberation
choose action perform next. Formally, performance algorithms online
planning assessed terms simple regret, agents expected performance loss
chosen action, rather optimal one, followed.
date, state-of-the-art algorithms online planning general MDPs either
best effort, guarantee polynomial-rate reduction simple regret time.
introduce new Monte-Carlo tree search algorithm, BRUE, guarantees exponentialrate smooth reduction simple regret. high level, BRUE based simple
yet non-standard state-space sampling scheme, MCTS2e, different parts
sample dedicated different exploratory objectives. extend BRUE
variant learning forgetting. resulting parametrized algorithm, BRUE(),
exhibits even attractive formal guarantees BRUE. empirical evaluation
shows BRUE generalization, BRUE(), also effective practice
compare favorably state-of-the-art.

1. Introduction
Markov decision processes (MDPs) offer general framework sequential decision
making uncertainty (Puterman, 1994). MDP hS, A, r, Ri defined set
possible agent states S, set agent actions A, stochastic transition function r :
SAS [0, 1] defined set |S||A| conditional probability functions P(S | s, a),
reward function R : R. current state agent fully observable.
agent performs action state s, state changes s0 probability
P(s0 | s, a), agent collects reward R(s, a, s0 ). finite horizon setting,
reward accumulated predefined number steps H.
objective agent act maximize accumulated reward,
decision problem always action perform next. state s, h steps go,
(possibly stochastic) action policy prescribes action taken situation.
policy called optimal if, expectation, following guarantees maximization
accumulated reward. key property MDP model that, MDP,
deterministic optimal policy : {1, . . . , H} (Bellman, 1957).
c
2014
AI Access Foundation. rights reserved.

fiFeldman & Domshlak

Efficiency finding optimal policies MDPs primary focus computational research around model. state space MDP large
allowed planning time, reasoning MDP narrowed state space region
considered relevant specific decision problem currently faced agent.
particular, algorithms online reasoning MDPs focus current state
s0 agent, deliberate set possible courses action s0 onwards, and,
interrupted, use outcome exploratory deliberation, planning, issue
instant recommendation action perform s0 . action applied
real environment, planning process repeated obtained state select
next action on.
Depending problem domain representation language, concise descriptions
large-scale MDPs either declarative generative (or mixed). declarative representations, transition reward functions described explicitly, generative models, given black box simulator. palette algorithms
finding good actions concisely represented MDPs already rather wide (Boutilier,
Dean, & Hanks, 1999; Guestrin, Koller, Parr, & Venkataraman, 2003; Kolobov, Mausam,
& Weld, 2012; Busoniu & Munos, 2012; Bonet & Geffner, 2012; Keller & Helmert, 2013;
Mausam & Kolobov, 2012; Geffner & Bonet, 2013), algorithms applicable
declaratively represented MDPs. One earliest best-known online planning algorithms developed generative MDP models sparse sampling algorithm
Kearns, Mansour, Ng (2002). Sparse sampling offers near-optimal action selection
discounted MDPs constructing sampled lookahead tree time exponential
discount factor sub-optimality bound, independent state space size. However, terminated action proven near-optimal, sparse sampling offers
quality guarantees action selection.
last decade, Monte-Carlo tree search (MCTS) algorithms (Browne, Powley, Whitehouse, Lucas, Cowling, Rohlfshagen, Tavener, Perez, Samothrakis, & Colton, 2012) became
extremely popular online planning MDPs, well online planning many
settings sequential decision making, including partial state observability
adversarial effects (Gelly & Silver, 2011; Sturtevant, 2008; Bjarnason, Fern, & Tadepalli,
2009; Balla & Fern, 2009; Eyerich, Keller, & Helmert, 2010; Browne et al., 2012).
capability dealing generative problem representations feature
MCTS made methods popular. First, MCTS algorithms natively exploit problem-specific heuristic functions, correctness independent heuristics
properties, well applied without heuristic information whatsoever.
Second, numerous MCTS algorithms exhibit strong anytimeness: meaningful
action recommendation provided interruption point instantly, time O(1),
quality recommendation also improved smoothly, time steps
independent size explored state space.
Formally, denoting shhi state h steps-to-go, quality action a,
recommended s0 hHi, assessed terms choice-error probability, is,
probability sub-optimal, terms (closely related) measure simple
regret [shhi, a]. latter captures performance loss results taking
following optimal policy remaining h 1 steps, instead following
166

fiSimple Regret Optimization Online Planning MDPs

beginning (Bubeck & Munos, 2010).1 is,
[shhi, a] = Q(shhi, (shhi)) Q(shhi, a),


(
Es0 [R(s, a, s0 ) + Q (s0 hh1i, (s0 hh1i))] ,
Q(shhi, a) =
0,

h > 0,
.
h=0

Numerous MCTS algorithms, particular, popular UCT (Kocsis & Szepesvari,
2006) algorithm variants (Coquelin & Munos, 2007; Tolpin & Shimony, 2012),
guarantee eventual convergence optimal choice action, providing smooth
reduction choice-error probability simple regret planning time. relative
empirical attractiveness various MCTS planning algorithms depends specifics
problem hand usually cannot predicted ahead time. However,
comes formal guarantees expected performance improvement planning
time, none online MCTS algorithms MDPs breaks barrier worst-case
polynomial-rate reduction simple regret choice-error probability time.
precisely contribution here. work motivated recently
growing understanding current MCTS algorithms MDPs optimize
reduction simple regret directly, via optimizing called cumulative regret,
performance measure suitable (very different) setting reinforcement learning
acting (Bubeck & Munos, 2010; Busoniu & Munos, 2012; Tolpin & Shimony, 2012;
Feldman & Domshlak, 2012).
Departing high-level realization, discuss certain pitfalls simple regret
minimization via Monte-Carlo sampling, identify two, somewhat competing, exploratory objectives pursued sampling mechanism.
suggest principle separation concerns, whereby different parts statespace sample devoted different exploration objectives.
introduce MCTS2e, novel sampling scheme specializes MCTS implements principle separation concerns. main result introduction
analysis BRUE, concrete instance MCTS2e guarantees smooth
exponential-rate reduction simple regret choice-error probability
time, general MDPs finite state spaces. fact, show
qualitatively similar guarantees satisfied broad class call purely
exploring MCTS2e algorithms, BRUE simple yet efficient instance
class.
Finally, discuss analyze prospects learning forgetting, principle according old samples degraded newer (and higher-quality) samples gathered. Generalizing BRUE extending ingredient forms
1. may appear reader intuitive consider loss results applying recommendations instead H steps, first one. However, Kearns et al.
(2002) show Lemma 5, two measures closely related, alternative measure
directly simple regret along execution horizon.

167

fiFeldman & Domshlak

parametrized algorithm BRUE(), parameter controlling level forgetfulness. show BRUE() exhibits even attractive formal guarantees
exhibited BRUE.
rest paper structured follows. Section 2.2 provide background
Monte-Carlo tree search, particular, UCT algorithm. Then, Section 3,
discuss simple regret minimization MDPs via multi-armed bandits perspective,
Section 4, introduce principle separation concerns establish
main algorithmic constructs along corresponding computational results. Section 5
devoted learning forgetting MCTS, particular, algorithm
BRUE(). Section 6 discuss findings empirical evaluation. proofs
formal claims relegated Appendix B, three subsections contain,
respectively, proofs three key theorems. completeness, Appendix provides
standard concentration inequalities use paper.

2. Background
Henceforth, A(s) denotes actions applicable state s, operation drawing
sample distribution set denoted D[], U denotes uniform distribution, JnK n N denotes set {1, . . . , n}. sequence tuples , [i] denotes
i-th tuple along , [i].x denotes value field x tuple. considering MDP hS, A, r, Ri, K denotes state branching factor (maximal number
actions per state), B denotes action branching factor (maximal number outcomes per
action), = mina6= (s0 ,H) [s0 hHi, a] denotes minimal possible non-zero simple
regret root.
2.1 Sparse Sampling
One earliest best-known online planning algorithms developed generative
MDP models sparse sampling (SS) algorithm Kearns et al. (2002). SS
originally developed infinite-horizon discounted MDPs, reformulation finite
horizon MDPs straightforward follows.
action A(s0 ), SS estimates value Q(s0 hHi, a) averaging C recursive samples outcome states. outcome states s0 sampled generative model transition function r(s0 , a), value sample set
R(s0 , a, s0 ) + maxa0 Q(s0 hH 1i, a0 ), Q-values actions a0 A(s0 ) estimated recursively way, hitting depth H. number outcome samples
C set guarantee quality recommendation issued upon termination
algorithm meets desired level accuracy. Alternatively, given C, formal
analysis used derive corresponding accuracy guarantee. Equivalent bounds
simple regret follows.
Proposition 2.1.1 Let SS called state s0 MDP hS, A, r, Ri rewards
[0, 1] finite horizon H. Then, simple regret action SS (s0 hHi), recommended
SS parameter C > 0, bounded
E[s0 hHi, SS (s0 hHi)] H(K min(B, C))H e
168

2 C
H4

.

fiSimple Regret Optimization Online Planning MDPs

proof Proposition 2.1.1 given Appendix C, p. 200. bound Proposition 2.1.1 suggests formal guarantees SS become meaningful
H 5 log(K min{B, C})
.
2
Assuming B < C, implies bound Proposition 2.1.1 becomes non-trivial
overall number SS calls generative model

2 H 5 log(BK)(BK)H .
(1)
C>

Notably, SS strong anytime algorithm, called contract algorithm (Zilberstein, 1993): termination SS parametrized C, interrupting SS
normal termination results meaningful action recommendation. However, knowing
overall number allowed calls generative model principle enable
knowledgeable allocation deliberation efforts (Hay, Shimony, Tolpin, & Russell, 2012).
Hence, general, deliverables contract algorithms expected better
deliverables the, de facto similarly budgeted, strong anytime algorithms (Zilberstein,
1993). Therefore, bound (1) sets good reference understanding significance
formal guarantees provided strong anytime algorithms online MDP planning.
2.2 Monte-Carlo Tree Search UCT

MCTS, canonical scheme Monte-Carlo tree search gives rise various specific
algorithms online MDP planning, depicted Figure 1, left. MCTS explores
state space radius H steps initial state s0 iteratively rolling
state-space samples s0 . rollout comprises sequence simulated steps
hs, a, s0 , ri state, action applicable s, s0 state resulting
applying s, r corresponding immediate reward. particular, [0].s = s0
[t].s0 = [t+1].s t.
generated rollout used update variables interest associated
states visited actions applied therein. variables typically include least
b
action value estimators Q(shhi,
a), well counters n(shhi, a) keep number
b (shhi, a) updated. rollout-oriented
times corresponding estimators Q
exploration MCTS allows information states deeper levels propagated
root s0 hHi low-complexity iterations O(H). allows smooth improvement
intermediate quality recommendation, probably one main reasons
MCTS seems particularly appealing context online planning.
Instances MCTS vary mostly along different implementation strategies
StopRollout, specifying stop rollout;
RolloutAction, prescribing action apply current state rollout;

Update, specifying rollout expand tree update maintained
variables stored nodes constructed search tree.2
2. Due Markovian nature MDPs, unreasonable distinguish nodes associated
state depth. Hence, actual graph constructed instances MCTS
forms directed acyclic graph nodes shhi {0, 1, . . . , H}.

169

fiFeldman & Domshlak

MCTS: [input: hS, A, r, Ri; s0 S]

procedure Update()
r 0
||, . . . , 1
hH
[d].a
n(shhi) n(shhi) + 1
n(shhi, a) n(shhi, a) + 1
r r + [d] .r
MC-backup(shhi, a, r)

time permits
Rollout // generate rollout
Update()
b 0 hHi, a)
return arg maxa Q(s
procedure Rollout
hi
s0
d0
StopRollout()
hH
RolloutAction(shhi)
s0 RolloutOutcome(shhi, a)
r R (s, a, s0 )
[t] hs, a, r, s0
s0 ; + 1
return

procedure MC-backup(shhi, a, r)
1
b
b
Q(shhi,
a) n(shhi,a)1
Q(shhi,
a) + n(shhi,a)
r
n(shhi,a)
procedure StopRollout()
||
return = H A([d].s0 ) =
procedure RolloutAction(shhi) // UCB
: n (shhi, a) = 0
return
q
h

n(shhi)
b
return argmaxa Q(shhi,
a) + c log
n(shhi,a)
procedure RolloutOutcome(shhi, a)
return s0 P(S | s, a)

Figure 1: template MCTS algorithms (left) , UCT algorithm specific set
sub-routines MCTS (right).

interrupted, MCTS uses information collected throughout exploration recommend action perform state s0 .
Numerous concrete instances MCTS proposed, UCT algorithm
(Kocsis & Szepesvari, 2006) modifications (Coquelin & Munos, 2007; Tolpin &
Shimony, 2011) popular instances days (Gelly & Silver, 2011;
Sturtevant, 2008; Bjarnason et al., 2009; Balla & Fern, 2009; Eyerich et al., 2010; Keller &
Eyerich, 2012). specification UCT algorithm instance MCTS depicted
Figure 1, right.
Different versions UCT use different rules end rollout. version depicted
here, rollouts end terminal nodes, is, either depth H states
applicable actions.3
RolloutAction policy UCT based deterministic decision rule
UCB1 (Auer, Cesa-Bianchi, & Fischer, 2002), originally proposed optimal balance
exploration exploitation cumulative regret minimization stochastic
3. popular version UCT, search tree grown incrementally, ending rollouts whenever
new node encountered. However, point extraneous exposition paper.

170

fiSimple Regret Optimization Online Planning MDPs

multi-armed bandit (MAB) problems (Robbins, 1952). node shhi, next-onthe-sample action selected follows: actions applicable
sampled shhi, is, n(shhi, a) > 0 A(s), selected
action corresponds

"
#
log
n(shhi)
b
argmax Q(shhi,
a) + c
,
(2)
n(shhi, a)


c > 0 fixed parameter balances first, exploitation-oriented,
second, exploration-oriented, summands Eq. 2. Otherwise, selected
uniformly random still unexplored actions {a A(s) | n(shhi, a) = 0}.
cases, procedure sample-outcome UCT samples next state
rollout according transition probability P(S | s, a).
b
UCT updates value estimators Q(shhi,
a) (shhi, a) pairs encountered
along rollouts. updates done via MC-backup procedure,
averages accumulated rewards rollouts shhi terminal states.
terms formal properties, UCT online algorithm that, certain point
time, provides smooth reduction simple regret time zero, is, smooth convergence optimal action choice s0 hHi (Kocsis & Szepesvari, 2006). Two aspects
convergence interest: (1) length transition period reduction simple regret guaranteed all, (2) reduction rate simple regret
time, transition period over. Considering (1), Coquelin Munos (2007)
showed number samples bounds UCT simple regret become
meaningful might high hyper-exponential H. Considering (2), Theorem 6
work Kocsis Szepesvari (2006) claims polynomial-rate reduction probability
choosing non-optimal action, implies simple regret4 .
attempts recently made improve UCT, online MCTS-based planning general, terms two aspects convergence (Tolpin & Shimony, 2012; Hay
et al., 2012; Coquelin & Munos, 2007). reported empirical results promising, none suggested MCTS instances breaks UCTs barrier worst-case
polynomial-rate reduction simple regret time. Hence, question whether
online, smoothly converging MCTS algorithm substantially outperform UCT terms
two convergence parameters remained open. comes next, answer
question affirmatively.

3. Simple Regret Minimization MDPs
high level, key property UCT exploration search space obtained
considering hierarchy forecasters (s, h), minimizing cumulative regret,
is, loss total reward incurred exploring environment (Auer et al.,
2002). respect, according Theorem 6 work Kocsis Szepesvari (2006),
UCT asymptotically achieves best possible (logarithmic) cumulative regret. However,
recently pointed numerous works (Bubeck & Munos, 2010; Busoniu & Munos, 2012;
4. Notably, claims made nontrivial assumptions

171

fiFeldman & Domshlak

Tolpin & Shimony, 2012; Feldman & Domshlak, 2012), cumulative regret seem
right objective online MDP planning, rewards collected
simulation phase fictitious. Furthermore, work Bubeck, Munos, Stoltz
(2011) multi-armed bandits shows minimizing cumulative simple
regret somewhat competing objectives, sense minimal simple regret
increase bound cumulative regret decreases.
relationship simple cumulative regret minimization MABs suggests
focusing online MDP planning directly simple regret minimization may lead algorithms are, worst-case and/or empirically, substantially effective UCT.
fact, context MABs, Bubeck et al. (2011) already showed simple round-robin
sampling MAB actions, followed recommending action highest empirical
mean, yields exponential-rate reduction simple regret, UCB1 sampling strategy
balances exploration exploitation yields polynomial-rate reduction
measure. respect, situation MDPs seemingly different. fact,
although designed slightly different setup, sparse sampling algorithm provides
evidence theoretical merits focused solely exploration online planning
MDPs.
appears, however, answer question one focus exploration, preserving onlineness smoothness convergence, less straightforward general MDPs special case MABs. motivate
discuss various exploratory concerns online Monte-Carlo planning MDPs,
separation concerns possibly buy us, begin MAB perspective MDPs, shows smooth exponential-rate reduction simple regret
MDPs indeed achievable, least theoretically.
3.1 Multi-armed Bandit Perspective MDPs
Let s0 state MDP hS, A, r, Ri rewards [0, 1], finite horizon H.
principle, general MDP viewed MAB, arm MAB
corresponding flat policy acting H steps starting current state s0 .
flat policy minimal partial mapping state/steps-to-go pairs actions
fully specifies acting strategy MDP H steps, starting s0 . Sampling
arm straightforward prescribes precisely action applied every
state possibly encountered along execution . reward
arm stochastic, withPsupport [0, H], expected value . number arms
H1
H
schematic MAB K i=0 B K B . Now, consider simple algorithm, NaiveUniform,
systematically samples flat policy loop, uses obtained reward
update empirical mean
b corresponding policy arm . stopped iteration
n, algorithm recommends policy arm n best empirical value
bn .
iteration n algorithm, arm sampled least b Bn H c times. Therefore,
K
using Hoeffdings tail inequality5 , probability chosen arm policy n sub-

5. completeness, Hoeffdings tail inequality provided Appendix A, pp. 188.

172

fiSimple Regret Optimization Online Planning MDPs

optimal MAB upper-bounded
X

6=

P {b
>
b } =

X

b

6=

P {b

b ( ) } K

BH

e

n c2
H
KB
2H 2

,

(3)

= = min6= . Denoting simple regret n rn ,
expected simple regret therefore bounded
b

Ern HK

BH

e

n c2
H
KB
2H 2

.

(4)

Note NaiveUniform uses rollout = hs0 hHi, a1 , s1 hH 1i, . . . , aH , sH h0ii
update estimation single policy . However, recalling arms MAB
problem actually compound policies, sample principle used update
estimates policies 0 consistent sense that, 0 H 1,
0 (si hH ii) defined 0 (si hH ii) = ai+1 . resulting algorithm, CraftyUniform,
generates samples choosing actions along sample uniformly random, uses
outcome sample update policies consistent it. Note
policy arms CraftyUniform cannot sampled systematically NaiveUniform
set policies updated iteration stochastic.
Since sampling uniform, probability policy updated sample
issued iteration CraftyUniform K1H . Let N n denote number samples consistent policy among first n samples issued CraftyUniform.
probability
n , best empirical arm n iterations, sub-optimal bounded
P
P
{b

b }

>
6=



P {b
>
b } P
b
2





+P
b



2



.

(5)

two terms right-hand side bounded as:




n

n
n

P N
+ P N >
,
b
P
b
2
2K H
2K H
2
fi


n
X
()
fifi
nH
N =
e 8K +
P {N = i} P
b
2 fi
n
i=

e



n
8K H

e



n
8K H

()

e



n
8K H



2e

2K H

+1

fi
X
n
fifi
n
+P
b
N
=
+
1
P {N = i}

2 fi
2K H
n
i= H +1
2K
fi


fifi
n
+P
b,n
N =
+1
2 fi
2K H




+e

n2

8K H H 2

n2

4K H H 2

,
(6)
173

fiFeldman & Domshlak

() () Hoeffdings tail inequality. turn, similarly Eq. 4, simple
regret CraftyUniform bounded
H

Ern 4HK B e



n2
8K H H 2

.

(7)

Since H trivial upper-bound Ern , bound Eq. 7 becomes effective
H

4K B e



n2
8K H H 2

< 1, is,
H

n > (KB) 4



H


2

log K.

(8)

Note cold start transition period much shorter UCT,
hyper-exponential H. time, unlike UCT, rate simple regret
reduction exponential number iterations. terms oracle calls, length
transition period CraftyUniform


2 H 3 log(K) (KB)H .
Likewise, comparing sparse sampling (Eq. 1), appears transition period
CraftyUniform smaller dependency H (H 3 vs. H 5 ), smaller dependency
B (log(K) vs. log(BK)).
sum, CraftyUniform seen theoretical feasibility test agenda:
algorithm uses Monte-Carlo sampling averaging updates, strong anytime (action
recommendation issued instantly, time, expected quality
recommendation improves every state-space sample), simple regret decreases
exponential rate time. Moreover, transition period reduction
rate guaranteed somewhat shorter (contracted) transition period SS,
much shorter transition period UCT. case, however, feasibility
H
CraftyUniform conceptual: requires explicit reasoning K B arms, thus
cannot efficiently implemented.

4. Separation Concerns Online MDP Planning
show practical algorithm achieves smooth, exponential-rate reduction
simple regret online MDP planning. so, first motivate introduce principle
separation concerns, whereby different parts state-space sample devoted
different aspects problem exploration. introduce MCTS2e, specialized
MCTS sampling scheme implements principle separation concerns via
two-phase scheme generating state-space samples. Using MCTS2e basis,
describe concrete algorithm, BRUE, achieves exponential-rate, smooth reduction
simple regret time, transition period comparable schematic
CraftyUniform non-interruptible SS. fact, show formal guarantees
satisfied entire class call purely exploring MCTS2e algorithms, one
BRUE.
tried achieve smooth, exponential-rate convergence merely replacing
UCB1 policy UCT pure exploration policy uniform action selection,
174

fiSimple Regret Optimization Online Planning MDPs

would failed miserably. fact, naive attempt would result algorithm
even converge optimal action. reason lies fundamental
difference MABs MDPs: Unlike MABs, direct sampling actual value
actions impossible requires knowledge optimal policy
subsequent states entire look-ahead space. knowledge, however, unavailable
beginning deliberation. Hence, sampling futures, non-root node
shhi actually serve two objectives:
(1) estimating actions ancestor(s) shhi ,
(2) identifying optimal action (shhi).
objectives exploratory, opposition extent.
meet first objective, shhi sample optimal action (shhi) probability
approaching 1 number samples grows. meet second objective, however,
actions shhi must selected frequently. protocol selecting actions
used, UCT, throughout entire rollout, rewards collected along rollout
used updating value estimations multiple nodes, protocol commit
addressing two objectives simultaneously. instance, UCB1 protocol employed
UCT nodes shhi chooses action seems attractive potential,
potential stems partially relatively high empirical value (complying objective (1)), partially less frequent sampling action (complying
objective (2)).
However, overloading action selection protocol unavoidable
learning acting setup reinforcement learning, case online
planning. sense, two objectives depicted resemble two tasks faced
MAB forecasters: objective (1) seen type recommendation, whereas objective
(2) viewed exploration. therefore makes perfect sense fulfill two
objectives different policies, much like exploration recommendation handled
different policies MAB online planning (Bubeck et al., 2011). specifically, different
policies used choose method node/action pairs updated
method values pairs estimated. follows,
refer separation exploratory objectives separation concerns, next
elaborate implementation concept online planning MDPs.
4.1 Two-Phase Sampling BRUE
introduce novel Monte-Carlo tree search scheme, MCTS2e, tailored towards employing principle separation concerns. MCTS2e depicted Figure 2(a)
specification MCTSs Update procedure. core difference MCTS2e
implementation Update UCT samples used update value
estimators. illustrated Figure 3, value estimators UCT updated
accumulated reward respective tail rollout, whereas MCTS2e estimators updated accumulated reward new sub-rollouts, created Estimate
procedure.
Estimate procedure parametrized two policies, namely
EstAction, prescribing action used estimation,
175

fiFeldman & Domshlak

procedure Update()
||, . . . , 1
hH
hs, a, r, s0 [d]
n(shhi) n(shhi) + 1
n(shhi, a) n(shhi, a) + 1
n(shhi, a, s0 ) n(shhi, a, s0 ) + 1
r r + Estimate(s0 hh 1i)
MC-backup(shhi, a, r)

procedure StopRollout()
||
return = H A([d].s0 ) =
procedure RolloutAction(shhi)
return U[A(s)]

// uniform

procedure RolloutOutcome(shhi, a)
return s0 P(S | s, a)

procedure Estimate(shhi)
r 0
0, . . . , h 1
EstAction(shh di)
s0 EstOutcome(shh di, a)
rd+1 R (s, a, s0 )
r r + rd+1
s0
return r

procedure EstAction(shhi) // best
b
return argmaxaA(s) Q(shhi,
a)
procedure EstOutcome(shhi, a)
s0 : n(shhi, a, s0 ) > 0
0
b = s0 | s, a) n(shhi,a,s )
P(S
n(shhi,a)
b | s, a)
return s0 P(S

(a)

(b)

Figure 2: (a) MCTS2e MCTS specific Update procedure, (b) BRUE algorithm specific set sub-routines MCTS2e (right).

EstOutcome, determining next state follow.
policies RolloutAction RolloutOutcome (used MCTSs Rollout
procedure) determine value estimators update, policies EstAction
EstOutcome used update estimators.
separation allows us introduce BRUE, is, way, exploratory
MCTS2e instance possible.6 BRUE setting MCTS2e depicted Figure 2(b).
Similarly UCT, rollouts generated BRUE end terminal nodes, and, throughout
rollout, next state sampled according r. However, unlike UCT, rollout
actions BRUE selected uniformly random applicable actions. turn,
estimation sub-rollouts,
selected actions empirically best actions, is, actions
highest value estimations,
b =
next states sampled according empirical transition probabilities P(S
0
0
| s, a), is, number times n(shhi, a, s) state followed applying
6. Short Best Recommendation Uniform Exploration; name carried first
presentation algorithm, estimation referred recommendation (Feldman &
Domshlak, 2012).

176

fiSimple Regret Optimization Online Planning MDPs

UCT

MCTS2e

b ([d].s, [d].a)
Q

H
Xd

b ([d].s, [d].a)
Q

H
X1

ri

i=1

[i].r

i=d

(a)

(b)

Figure 3: Illustration value estimator update MCTS2e (a) vs. UCT (b). Circles
represent decision nodes, solid lines represent actions taken, squares represent
chance nodes, dashed arrows represent outcomes result
subsequent decision nodes.

action node shhi, divided overall number times n(shhi, a) action
applied shhi.7
proceed formal analysis BRUE. general, considering
instance MCTS2e, Tn denote search graph obtained n iterations.
sake simplicity, assume uniqueness optimal policy : state
number h steps-to-go, assume single optimal action, denote (s, h).
nodes shhi Tn , nB (shhi) randomized strategy, uniformly choosing among actions
b
maximizing Q(shhi,
a). addition problem-specific state branching factor K,
minimal non-zero simple regret root = mina6= (s0 ,H) [s0 hHi, a], bounds
depend problem-specific action branching factor B, well horizon H.
former two parameters inherited MAB, latter two connect MAB
general MDP.
Theorem 1 Let BRUE called state s0 MDP hS, A, r, Ri rewards
[0, 1], finite horizon H. exist pairs parameters a, b > 0, dependent
7. Sampling according P(S | s, a) RolloutOutcome also valid choice, although terms
formal guarantees, EstOutcome Figure 2 appears attractive.

177

fiFeldman & Domshlak

{K, B, H, }, that, n > H iterations BRUE, simple regret bounded
E[s0 hHi, nB (s0 hHi)] Ha ebn ,
choice-error probability bounded


P nB (s0 hHi) 6= (s0 hHi) ebn .
particular, Eq. 9 10 hold = 3K

b=

2
.
9K 2 (196BK)H1 H 2



1044B 2 K 2
2

H1

(9)

(10)
1

2

(196BK) 2 (H1) (H 1)!2

proof Theorem 1 given Appendix B.1, p. 188. length transition
period implied Theorem 1 given


BK
2 5
H
H log(
)(196BK)
(11)

transition period rather comparable sparse sampling except rather
large constant appearing basis exponent Equations 9 10. Although
constant imposes significant increase transition period, things noted
regards bounds provided BRUE. First foremost, parameter b
Theorem 1 reflects worst-case terms transition function r, corresponds
uniform distribution, is, P(s0 | s, a) = B1 states actions A(s).
However, probability mass action transition functions concentrates
small set outcomes, convergence rate BRUE expected much better.
Proposition 4.1.1 formulates BRUEs bounds respect problem-dependent parameter
1 Pe B, related entropy transition function defined
Pe = max kP( | s, a)k 1 .
s,a

2

Proposition 4.1.1 Let BRUE called state s0 MDP hS, A, r, Ri rewards [0, 1], finite horizon H 4. BRUE converges exponential rate
H1
(H!)2 b =
sense Eq. 9 10 Theorem 1 = 3K 172BK
2
2
.
4
H1 H5 2
9KB (1666K)

Pe

H

formal guarantees BRUE therefore even better SS. proof
Proposition 4.1.1 (given Appendix B.2, p. 195) obtained rather minor modification
proof Theorem 1.
Relating tightness bounds, noted size scalar
constants analysis BRUE partially stems attempt avoid cumbersome
expressions, thus considerably reduced. Furthermore, particular point
analysis bound error action-value estimations different points
time, believe bound gets particularly loose. comment issue
detail within proof Theorem 1 (right Proposition B.1.1, p. 190).
read far, reader may rightfully ask extent guarantees provided BRUE unique among instances MCTS2e. general, formal properties
178

fiSimple Regret Optimization Online Planning MDPs

MCTS2e instances heavily depend specific sub-routines,
even guarantee convergence optimal action. However, BRUE still much
unique deliverables. particular, define family purely exploring MCTS2e algorithms guarantee exponential-rate reduction simple regret
time.8
Definition 1 (Purely exploring MCTS2e) instance MCTS2e called purely
exploring if, node shhi reachable s0 , A(s), exist parameters
, , , dependent {K, B, H, },
P {n(shhi, a) n(shhi)} en(shhi) ,
estimation policy EstAction selects empirically best arm.
Theorem 2 Let purely exploring instance MCTS2e. converges
exponential rate sense Eq. 9 10 Theorem 1.
Appendix B.3, p. 195 show proof Theorem 2 easily derived
proof Theorem 1. Furthermore, analysis provided proof Theorem 1
used extract convergence parameters c, c0 purely exploring algorithm,
given specific parameters , , .

5. Learning Forgetting BRUE()
BRUE, well converging instances MCTS MCTS2e, evolution
action value estimates internal nodes based biased samples stem
selection non-optimal actions descendant nodes. bias tends shrink
samples accumulated descendants. Consequently, estimates become
accurate, probability selecting optimal action increases accordingly,
bias ancestor nodes shrinks turn.
interesting question arises context whether samples obtained different stages sampling process weighed differently. high level,
intuition suggests biased samples provide us valuable information, especially still have. time, value information
decreases obtain accurate samples. Hence, principle, putting weight
samples smaller bias could increase accuracy estimates. led us
consider BRUE(), algorithm generalizes BRUE BRUE(1) basing estimates
fraction recent samples.
Technically, BRUE() differs BRUE implementation MC-backup
procedure depicted Figure 4. addition variables maintained BRUE,
node/action pair (shhi, a) BRUE() associated list L(shhi, a) rewards,
collected n(shhi, a) samples responsible current estimate
b
b
Q(shhi,
a). (shhi, a) updated MC-backup, value estimator Q(shhi,
a)
assigned average recent n(shhi, a)e samples, dxe denotes
8. We, course, make claims guarantees exclusive purely exploring instances
MCTS2e, even MCTS2e instances general.

179

fiFeldman & Domshlak

procedure MC-backup(shhi, a, r)
n n(shhi, a)e
n n(shhi, a)
L(shhi, a)[n] rP
b
Q(shhi,
a) n1 ni=nn L(shhi, a)[i]

Figure 4: BRUE() modified MC-backup procedure

smallest integer greater equal x. Theorem 3 exhibits benefits
adopting < 1 comes convergence guarantees.
Theorem 3 Let BRUE() called state s0 MDP hS, A, r, Ri rewards
[0, 1] finite horizon H. exist pairs parameters a, b > 0, dependent
{K, B, H, , }, that, n > H iterations BRUE (), simple regret
bounded
E[s, nB (s0 , H), H] Ha ebn ,
(12)
choice-error probability bounded


P nB (s0 , H) 6= (s0 , H) ebn .

(13)

1
particular, depth-dependent h (BK)
h1 , Eq. 12 13 hold

H1
2
= 3K 12BK
(H!)2 b = 9K 2 (196BK)
H1 H 2 .
2

particular choice h Theorem 3, length transition period
BRUE() terms number calls generative model




BKH
2 4
H
H log
(196BK)
.


bound BRUE() seems somewhat better BRUE, improvement attributed looseness bound BRUE less
actual improvement performance. proof Theorem 3 (given Appendix B.4,
p. 196) offer new technique address bound accuracy action-value
estimations different sampling times, reduces bound considering fewer samples. selection Theorem 3 stems attempt balance much possible
two sources inaccuracy appearing Propositions B.4.1 B.4.2 proof
Theorem 3. smaller is, lower sample inaccuracy originates
inaccuracy estimates successor nodes. time, however,
inaccuracy stems basing estimate fewer samples increases. Due
branching, nodes farther toward horizon sampled less frequently thus less
accurate. worst case, underlying graph Tn tree, node expected
1
sampled fraction (BK)
number samples taken
steps higher predecessor. precisely reason selection h
Theorem 3.
180

1
(BK)h1



fiSimple Regret Optimization Online Planning MDPs

practice, however, worst-case considerations tend underrate value
samples. Since Tn typically tree, ratio number samples
different depths tends higher aforementioned worst-case ratio. Therefore,
better adapted according observed ratios rather according
worst-case ones. Furthermore, since objective behind estimating action values
identify optimal action, bias samples may far less influence
quality planning outcome dictated formal guarantees. instance,
suppose action estimators particular node shhi equal bias.
b
case, shhi may home optimal action Q(shhi,
a) estimates still
biased, suffice shhi fulfill role value-estimating sub-rollouts issued
ancestor(s). illustrative setup clearly extreme, point
biased estimators still distinguish better actions worse ones, long
biases across actions correlated.

6. Experimental Evaluation
evaluated BRUE empirically MDP Sailing domain (Peret & Garcia, 2004),
used previous works evaluating MCTS algorithms (Peret & Garcia, 2004; Kocsis &
Szepesvari, 2006; Tolpin & Shimony, 2012), well MDP version random game
trees used original empirical evaluation UCT (Kocsis & Szepesvari, 2006).
Sailing domain, sailboat navigates destination 8-connected grid
representing marine environment, fluctuating wind conditions. goal reach
destination quickly possible, choosing grid location neighbor location
move to. duration
move depends direction move (ceteris
paribus, diagonal moves take 2 time straight moves), direction wind
relative sailing direction (the sailboat cannot sail wind moves fastest
tail wind), tack. direction wind changes time, strength
assumed fixed. sailing problem formulated goal-driven MDP
finite state space finite set actions, state capturing position
sailboat, wind direction, tack.
goal-driven MDP, lengths paths terminal state necessarily
bounded, thus entirely clear depth BRUE construct tree.
Sailing domain, set H 4 n, n grid-size problem instance,
unlikely optimal path two locations grid
longer complete encircling area.
compared BRUE two MCTS-based algorithms: UCT algorithm, recent
modification UCT, obtained UCT replacing UCB1 policy root node
uniform policy (Tolpin & Shimony, 2012). follows, denote modification
UCT uUCT. motivation behind design uUCT improve empirical
simple regret UCT, results uUCT reported Tolpin Shimony (2012)
(and confirmed experiments here) impressive. also display results
additional MCTS2e-based algorithm, baptized BRucbE, similar
BRUE except that, exploration, uses UCB1 policy instead uniform policy.
words, BRucbE seen UCT separation concerns. four algorithms
implemented within single software infrastructure. line setup underlying
181

fiFeldman & Domshlak

0.7

2

1.8

0.6

1.6

UCT
uUCT
BRUE
BRucbE

0.4

1.4
Simple Regret

Simple Regret

0.5

0.3

1.2

1

UCT
uUCT
BRUE
BRucbE

0.8

0.2
0.6

0.1

0

0.4

0

5

10

0.2

15

0

1

2

3

4

4

Iterations

x 10

3.5

4

3

3.5

UCT
uUCT
BRUE
BRucbE

2.5

2

7

8

9

10
4

x 10

UCT
uUCT
BRUE
BRucbE

3

2.5

2

1.5

1

6

10 10

Simple Regret

Simple Regret

55

5
Iterations

1.5
0

0.5

1

1.5

2
Iterations

2.5

3

3.5

4
4

x 10

20 20

0

0.5

1

1.5
Iterations

2

2.5

3
4

x 10

40 40

Figure 5: Empirical performance UCT, uUCT (denoted UUCT, short), BRUE,
BRucbE terms average error Sailing domain tasks n n grids
n {5, 10, 20, 40}.

Theorem 6 Kocsis Szepesvari (2006), exploration coefficient UCT uUCT
(parameter c Eq. 2) set difference largest possible smallest
possible values H-step rollouts root. Sailing domain, corresponds
maximal move duration, 6, multiplied number steps-to-go h.
Figure 5 shows performance four algorithms terms empirical simple
regret, is, average difference Q(s0 , a) V (s0 ) true values action
chosen algorithm optimal action (s0 ). algorithm run
1000 randomly chosen initial states s0 , target fixed one corners
grid. performance measured depicted function planning time.
four algorithms, planning time unit, iteration, corresponds H action
samples, is, length single rollout.
182

fiSimple Regret Optimization Online Planning MDPs

4

3.5

3.5

3

3
UCT
uUCT
BRUE
BRucbE

2

UCT
uUCT
BRUE
BRucbE

2.5
Simple Regret

Simple Regret

2.5

1.5

2

1.5
1

1

0.5

0
0

0.5

0.2

0.4

0.6

0.8

1
Iterations

1.2

1.4

1.6

1.8

0

2
6

x 10

B = 6/D = 8

0

0.5

1

1.5

2

2.5
Iterations

3

3.5

4

4.5

5
5

x 10

B = 2/D = 22

Figure 6: Empirical performance UCT, uUCT, BRUE, BRucbE terms average
error MDP version random game trees branching factor B tree
depth D.

Consistently results reported work Tolpin Shimony (2012),
smaller tasks, uUCT outperformed UCT large margin, latter exhibiting
little improvement time even smallest, 5 5, grid. difference
uUCT UCT larger tasks less notable. turn, BRUE BRucbE
substantially outperformed UCT, BRucbE slightly better smaller tasks,
BRUE taking larger instances, except relatively short planning deadlines.
shows value MCTS2es separation concerns lies ability
employ pure exploration policy, also ability base estimations
empirically best values, regardless employed exploration policy.
Overall, results Sailing domain clearly testify BRUE attractive terms formal guarantees, also effective practice.
also evaluated four algorithms domain random game trees whose goal simple
modeling two-person zero-sum games Go, Amazons Globber. games,
winner decided global evaluation end board, evaluation employing
another feature counting procedure; rewards thus associated
terminal states. Following Kocsis Szepesvari (2006), rewards domain
calculated first assigning values moves, summing values along
paths terminal states. Note move values used tree construction
made available players. values chosen uniformly
[0, 127] moves MAX, [127, 0] moves MIN. players act
(depending role) maximize/minimize individual payoff: aim MAX
reach terminal high R(s) possible, objective MIN similar,
mutatis mutandis. simple game tree model similar spirit many game tree
models used previous work (Kocsis & Szepesvari, 2006; Smith & Nau, 1994), two
exceptions. First, measure success/failure players via actual payoffs
183

fiFeldman & Domshlak

4

1.8

3.5

1.6

3

1.4

1.2

BRUE

Simple Regret

Simple Regret

2.5
BRUE()
2

BRUE
BRUE()
1

1.5

0.8

1

0.6

0.5

0.4

0

0

0.2

0.4

0.6

0.8

1
Iterations

1.2

1.4

1.6

1.8

0.2

2
6

x 10

Game Trees (B = 6/D = 8)

0

1

2

3

4

5
Iterations

6

7

8

9

10
4

x 10

Sailing (10 10)

Figure 7: Empirical performance BRUE BRUE() terms average error
MDP version random game trees sailing domain.

receive, rather ternary scale win/lose/draw. Moreover, comply
setting addressed work, model game MDP moves associated MAX player considered decision nodes, whereas moves MIN
modeled stochastic outcomes following distribution: optimal minimax
move chosen probability p = 0.9, complementary probability 1 p divided
uniformly rest moves.
Similarly setup Sailing domain, exploration coefficient UCT
uUCT
range
game values, 127H, since rewards bounded
set H

H
interval 127 2 , 127 2 . ran experiments two different settings branching
factor (B) tree depth (D). Sailing domain, compared empirical
simple regret obtained UCT, uUCT, BRUE, BRucbE time. Figure 6 shows
performance four algorithms two game configurations, B = 6, = 8
B = 2, = 22, configuration represented 1000 game trees. results
appear encouraging well, BRUE BRucbE overtaking UCT uUCT,
BRucbE even appearing slightly faster BRUE terms convergence.
also experimented BRUE() which, line discussion right
Theorem 3, parameter dynamically adjusted function depth
estimated node/action pair. Specifically, used = nnHh , nH denotes average
number samples leaf nodes, nh denotes average number samples nodes
depth value estimator consideration. show Figure 7,
find significant empirical benefit BRUE() BRUE (to match
superior formal guarantees former), neither Sailing domain game
trees domain.
last set experiments complements theoretical comparison sparse
sampling (SS) algorithm. Specifically, performed empirical comparison BRUE
UCT variant SS called forward-search sparse sampling (FSSS) (Walsh, Goschin,
184

fiSimple Regret Optimization Online Planning MDPs

& Littman, 2010). Like SS, FSSS estimates action values node using C samples.
However, instead estimating action values recursively encountered state,
FSSS uses MCTS-style rollouts explore state space, initializing values yet
unexplored actions predefined lower upper bounds. Ultimately, FSSS computes
precisely values SS, thus returning recommendation. However,
potentially benefits kind pruning reduce amount computation. Notably,
unlike SS, FSSS output action recommendation point time based
maintained lower upper bounds actions values. typical approach select
action maximum lower bound. However, similarly SS, FSSS cannot provide
non-trivial guarantees prior termination. therefore choose use following
experimental setup. First, run FSSS value C. take overall
number action samples performed FSSS termination, use stopping
criteria BRUE UCT. Figures 8 9 depict empirical simple regret obtained
three algorithms upon termination Sailing game tree domains.
planning task, picked values C allowed FSSS terminate within
reasonable amount time.9 Sailing domain, lower upper bounds FSSS
set 0 6h, respectively, whereas game trees domain, used lower bound
127 H2 upper bound 127 H2 .
appears, BRUE UCT outperform FSSS tasks, notably, BRUE
outperforms FSSS tasks, every value C. despite purported
advantage FSSS aware termination point. explanation result
concerns two fundamental differences MCTS-based algorithms SS. First, recall
formal discussion given Theorem 1 around entropy transition function.
Suppose FSSS (or SS) estimates certain action two outcomes, one
outcome likely other. outcomes caught C action
samples, efforts would invested estimating values two states,
regardless fact one outcome likely thus larger contribution
value action. contrast, UCT BRUE adapt structure
problem skewing rollouts towards states higher probability, yielding better
results theoretically empirically.
Another potential advantage MCTS algorithms SS pertains allocation
computational efforts estimating actions values different depths. FSSS (and SS),
estimations based number samples C. contrast, UCT
BRUE, nodes closer root sampled frequently branching
factor. illustrate potential benefit focusing efforts around root, let us
consider Sailing domain example. position boat reached taking
optimal moves first steps would probably closer target compared
position reached taking non-optimal moves fist steps. therefore likely
following random navigation policy position boat first
steps, target would reached sooner average former case latter
case. words, benefit knowing optimal policy deeper states smaller,
putting focus estimating actions nodes closer root makes much
9. time limit FSSS set 24 hours. Notably, implementation FSSS, minimize
number action sample sampling outcomes actions selected rollouts

185

fiFeldman & Domshlak

55

10 10

20 20

40 40

Figure 8: Empirical performance FSSS, UCT, BRUE terms average error
termination Sailing domain tasks n n grids n {5, 10, 20, 40}.
n = 5, empirical simple regret BRUE 0. n = 40, running FSSS
C > 1 took 24 hours.

186

fiSimple Regret Optimization Online Planning MDPs

B = 6/D = 8

B = 2/D = 20

Figure 9: Empirical performance FSSS, UCT, BRUE terms average error
termination MDP version random game trees branching factor B
tree depth D.

sense. believe property prevails many practical cases, cases
MCTS algorithms expected efficient.
also interesting see that, although UCT outperforms FSSS tasks,
gap decreasing size budget (C), smaller tasks
(Sailing domain grid 5 5 10 10), FSSS even outperforms UCT point.
find compliant theoretical merits pure-exploratory nature
SS BRUE.

7. Summary
goal improving convergence guarantees smooth Monte-Carlo tree search
algorithms online planning MDPs, introduced principle separation
concerns, well Monte-Carlo tree search scheme, MCTS2e, allows operationalizing principle. showed subclass purely exploring instances MCTS2e
guarantees smooth exponential-rate improvement performance measures interest,
improving polynomial-rate guarantees provided state-of-the-art algorithms.
examined, formally empirically, purely exploring MCTS2e algorithm called
BRUE. Finally, explored prospects time-dependent forgetting samples within
Monte-Carlo search, showed concrete merits sample ignorance parametric
BRUE() algorithm generalizes BRUE learning forgetting.
results open numerous questions investigation. First, BRUE
rather straightforward implementation pure exploration MCTS2e, necessarily efficient one. believe replacing uniform exploration BRUE
scheme makes use knowledge acquired along sampling direct
187

fiFeldman & Domshlak

exploration may result empirically efficient instance MCTS2e, possibly
even improve formal guarantees BRUE.
Another important point consider speed convergence good actions,
opposed speed convergence optimal actions. BRUE geared towards
identifying optimal action, good often best one hope dealing
large MDPs. identify optimal solution, BRUE constructs full-depth tree right
start. However, focusing nodes closer root node, e.g., utilizing
intelligent rules rollout termination, may improve quality recommendation
planning time severely limited. recently reported successful steps
direction (Feldman & Domshlak, 2013), steps far closing
interesting venue research.
Finally, core tree sampling scheme employed BRUE plausible way
implement concept separation concerns discussed paper. instance,
substituting MC-backup procedure value updates based Bellmans principle,
as, e.g., done Keller Helmert (2013), also constitutes form separation
concerns. would interesting in-depth comparison formal
empirical properties different protocols.
Acknowledgements
work partially supported carried Technion-Microsoft Electronic
Commerce Research Center, well partially supported Air Force Office Scientific Research, USAF, grant number FA8655-12-1-2096.

Appendix A. Auxiliary Propositions
analysis below, make extensive use Hoeffdings tail inequality sums
bounded independent random variables. addition, use result mathematical
programming P1 below.
Hoeffdings tail inequality. Let X1 , . . . , Xn independent bounded random P
variables
Xi falls interval [ai , bi ] probability 1, let Sn = ni=1 Xn .
Then, > 0,
2/

P {Sn ESn t} e2t

Pn

2
i=1 (bi ai )

.

particular, Xi identically distributed within [0, 1] EXi = ,
P {Sn n t} e

2t2
n

P1 h 1, solution mathematical program
maximize
p

subject

B
X

i=1
B
X

pi

h
pi B
pi = 1

i=1

0 pi 1
188

.

fiSimple Regret Optimization Online Planning MDPs

value 1. result follows concavity objective function.

Appendix B. Proofs
appendix structured four subsections, respectively dedicated proof
Theorem 1, Proposition 4.1.1, Theorems 2, 3. sake readability,
places believe create confusion, expressions form P(E |
X1 = x1 , . . . , Xk = xk ) f (x1 , . . . , xk ) Xi random variables written simply
P(E) f (X1 , . . . , Xk ).
B.1 Proof Theorem 1
follows, Vp (shhi) denote h-steps value function defined

E

"h1
X
i=0

fi
#
fi
fi
R (si , (si hh ii) , si+1 ) fi s0 = ,
fi

expectation transition function r, i.e., set conditional
probability distributions {P(S | s, a)}s,a . Vp (shhi) denotes value function optimal
policy . subscript p omitted p corresponds transition probabilities P
MDP question.
Theorem 1 follows almost immediately Lemma 4 below.

Lemma 4 node shhi
n

B
2
P V (shhi) VPb (shhi) ah ebh n(shhi)
n B

2
P VPb (shhi) V (shhi) ah ebh n(shhi) ,

h1
1
116 9B 2 K 2
2
(196BK) 2 (h1) (h 1)!2
2
1
bh =
.
2
9K (196BK)h1 h2
ah = 3K



189

fiFeldman & Domshlak

Proof: proof induction h. Starting h = 1,
n

B
P V (sh1i) VPb (sh1i)


2
P Q(sh1i, (sh1i)) Q(sh1i, B (sh1i))
3
)
(
X

b 0 | s, B (sh1i)))R(s, B (sh1i), s0 ) >
def. Q
+P
(P(s0 | s, B (sh1i)) P(s
3
s0


X

b
P Q(sh1i,

a) Q(sh1i, a)
3

a6= (sh1i)



b
+ P Q(sh1i, (sh1i)) Q(sh1i,
(sh1i))
3
)
(
X
X

b 0 | s, a))R(s, B (sh1i), s0 ) >
+
P
(P(s0 | s, a) P(s
3
s0
aA(s)


X
2 n(sh1i)
n(sh1i)

P n(sh1i, a)
+ 2Ke 9K
Hoeffding
2K
aA(s)

3Ke

2 n(sh1i)
9K 2

.

Hoeffding

Assuming claim holds h0 h, proving induction hypothesis h + 1,
encounter following deficiencies:
b unbiased estimator Q, is, EQ
b = Q. contrast,
(F1) h = 1, Q
b
estimates inside tree (at nodes h > 1) biased. bias stems Q
possibly based numerous sub-optimal choices sub-tree rooted shhi.

b independent. h > 1,
(F2) h = 1, summands accumulated Q
accumulated reward depends selection actions subsequent nodes,
turn depends previous rewards.
way circumvent deficiencies captured sequence bounding B.1.1B.1.5 below. high level, deal bias samples using straightforward
extension Hoeffding inequality. analysis, dependence samples
alleviated conditioning outcome sample state information
collected nodes sampled one. propositions made
assumption induction hypothesis.
b
Considering node shh + 1i, first show value estimations Q(shh+1i,
a)

actions A(s) sufficiently accurate. show = (shh + 1i), whereas
bounds actions derived similar way. ease presentation,
follows use abbreviations = (shh + 1i), aB = B (shh+1i), na =
n(shh + 1i, (shh + 1i)). also use following notation. {1, . . . , na }, let
b
random variables Xt capture accumulated reward samples averaged Q(shh+1i,
),
bt capture transition probatB capture policy induced BRUE sample t, P
b
bilities estimations sample t. Proposition B.1.2, bound error Q(shh+1i,
),
190

fiSimple Regret Optimization Online Planning MDPs

bt descendants shh + 1i samples sufgiven error tB P
bt
ficiently small. Proposition B.1.1 bound probability error tB P
sample large.
Proposition B.1.1 > 0, let E event which, sampling Xt , =
1, . . . , na , holds that,
1.

2.

P

s0



B
bt (s0 | s, ) V (s0 hhi) V (s0 hhi)
P
b
Pt

P
s0




2,




bt (s0 | s, ) (R(s, , s0 ) + V (s0 hh 1i))
P(s0 | s, ) P


2,

v


u
na 2 bh
u 2
4B
log
na
56B
1
.
+
=
9
bh


Then,

P {E }

112B 2 ah bh 2 na
36B
e
.
2 bh

(14)

Proof: follows P1

P

(
X
s0



tB

bt (s0 | s, ) V (s0 hhi) V
P
b

Pt




(s0 hhi)
2

)




X
B


.

P V (s0 hhi) V b (s0 hhi) q
Pt


0

b
0

2 B Pt (s | s, )

(15)

B
Pt

Indeed, states s0 summation, holds V (s0 hhi) V b (s0 hhi) <


2


,
bt (s0|s,a )
BP

X
s0

then, particular,


X
B

bt (s0 | s, ) V (s0 hhi) V (s0 hhi) <
bt (s0 | s, ) q
P
P
bt
P
bt (s0 | s, )
s0
2 BP
=

191

b (s0 | s, )
X P
q
2 0
bt (s0 | s, )

BP

.
2

P1

fiFeldman & Domshlak

Given that,


na

X
X

B
P {E }
P V (s0 hhi) V (s0 hhi) > q

bt (s0 | s, )
t=1 s0
2 BP
(
)
na

X
X

0

0

0
0
b
+
P
P(s | s, ) Pt (s | s, ) R(s, , ) + V (s hh 1i) >
2
0
t=1
na



X

Bah e

t=1
na

+



b 2
h4Bt

X

I.H.

t2

e 4h2

Hoeffding

t=1



na
X
t=1

=

2Bah e



bh t2
4B

na
X
b 2 n
112B 2
h36Ba


e
h
na 2 bh

definition

t=1

112B 2 ah bh 2 na
36B
e
2 bh

Note that, bounding probability event E Eq. 14, basically ignore
bt different sampling times, use
dependency state tB P
B
bt happen accurate sample t,
crude union bound. However, P
probability remain accurate subsequent samples higher. possible
factoring dependency bound Proposition B.1.1 improve
tightness bound.
bt samples = 1, . . . , na , given
Conditioned state tB P
sufficiently accurate defined event E above, Proposition B.1.2 bounds
b
probability value estimator Q(shh+1i,
) inaccurate.

Proposition B.1.2 definition E introduced Proposition B.1.1,
na
bt }na , event E ,
> 0, holds that, given {tB }t=1
, {P
t=1
(1) t, random variables Xt mutually independent,
h fi

fi
bt }, E Q(shh+1i, a) ,
(2) 1, E Xt fi {tB }, {P

fi
n

2 na

fi B
2


b
b
8(h+1)
(3) P Q(shh+1i, ) Q(shh+1i, ) fi {t }, {Pt }, E e
.

Proof: correctness mutual independence (1) direct definition BRUE:
dependency samples BRUE induced state information
b turn, proof
collected samples, determined solely B P.
(2) obtained definition E follows:
192

fiSimple Regret Optimization Online Planning MDPs

h fi
X
X
B
fi
bt }, E =
bt (s0 | s, )R(s, , s0 ) +
bt (s0 | s, )V (s0 hhi)
E Xt fi {tB }, {P
P
P
b
P
s0



s0

= Q(shh+1i, )

X

bt (s0 | s, ) R(s, , s0 ) + V (s0 hhi)

P(s0 | s, ) P
s0



X
s0



B
bt (s0 | s, ) V (s0 hhi) V (s0 hhi)
P
b
Pt



2
2
= Q(shh+1i, )
Q(shh+1i, )

definition E

Finally, proof (3) obtained noting
v


u
na 2 bh
u
na
na
4B
log
2 na
56B
1 X
1 X
1

+

=
na
9
bh
na

t=1
t=1
v


u
na 2 bh
u 2
4B
log
na
56B
2

+

9
bh
na
r
4 2 4 2
log x
2
+
since


x
5
9
35
3

4
Therefore,
fi

n
fi
b
bt }, E
P Q(shh+1i, ) Q(shh+1i,
) fi {tB }, {P
fi
(
)
na
fi
h

X
1
fi


B
bt }, E
b
b
= P E Q(shh+1i,
) Q(shh+1i,
)
fi {t }, {P
fi
na
t=1
fi

h

fifi B


b
b
b
P E Q(shh+1i, ) Q(shh+1i, ) fi {t }, {Pt }, E
4
e



2 na
8(h+1)2

,

b
bound error value estimator Q(shh+1i,
).
Proposition B.1.3 > 0, holds

n
113B 2
2
h bh na
b
36B
P Q(shh+1i, ) Q(shh+1i,
)
e
.
2 bh
193

(16)

fiFeldman & Domshlak

Proof:
n

b
P Q(shh+1i, ) Q(shh+1i,
)

P {E }
fi
fi
n
n
X
fi
b
bt }, E P { B }, {P
bt } fifi E
P Q(shh+1i, ) Q(shh+1i,
) fi {tB }, {P
+

bt }
{tB ,P



113B 2 ah bh 2 na
36B
e
2 bh

Props. B.1.1 & B.1.2.

b
Proposition B.1.4 employs bounds accuracy Q(shh+1i,
a) bound
B
simple regret .

Proposition B.1.4



P Q(shh+1i, (shh+1i)) Q(shh+1i, aB )

114B 2 ah bh 2 n(shh+1i)
144BK
e
2 bh

Proof:



P Q(shh+1i, (shh+1i)) Q(shh+1i, aB )


X

b

P Q(shh+1i, a) Q(shh+1i, a)
2
a6= (shh+1i)





b
+ P Q(shh+1i, (shh+1i)) Q(shh+1i, (shh+1i))
2

2
X
113B Kah bh 2 n(shh+1i)
n(shh + 1i)
144BK
+
e

P n(shh + 1i, a)
2K
2 bh

Prop. B.1.3

aA(s)



114B 2 Kah bh 2 n(shh+1i)
144BK
e
2 bh

Hoeffding

induction step concluded Proposition B.1.5.
Proposition B.1.5
n
116B 2 Ka
2
B
h bh n(shh+1i)
196BK
P V (shh+1i) VPb (shh+1i)
e
.
2 bh
Proof: Since
B

V (shh+1i) VPb (shh+1i)
= Q(shh+1i, (shh+1i)) Q(shh+1i, aB )


X
b 0 | s, aB ) V (s0 hhi) V B (s0 hhi)
+
P(s
b
P
s0

+

X
s0



b 0 | s, aB ) R(s, aB , s0 ) + V (s0 hhi) ,
P(s0 | s, aB ) P(s
194

fiSimple Regret Optimization Online Planning MDPs

holds
n

B
P V (shh+1i) VPb (shh+1i)


6
P Q(shh+1i, (shh+1i)) Q(shh+1i, aB )
7
)
(


X

B
b 0 | s, aB ) V (s0 hhi) V (s0 hhi)
+P
P(s
b
P
14
s0
(
)

X


0
B
0
B
B 0
0
b
+P
P(s | s, ) P(s | s, ) R(s, , ) + V (s hhi) >
14
0


114B 2 Kah bh 2 n(shh+1i)
144BK

Prop. B.1.4
e
2 bh
(
)


X
X

B
b 0 | s, a) V (s0 hhi) V (s0 hhi)
+
P(s
P
b
P
14
s0
aA(s)
)
(

X
X


b 0 | s, a) R(s, a, s0 ) + V (s0 hhi) >
+
P
P(s0 | s, a) P(s
14
0
aA(s)



2

n(shh+1i)
b 2 n(shh+1i)
114B 2 Kah bh 2 n(shh+1i)

h 196BK
196K(h+1)2
196BK
e
+
BKa
e
+
Ke
h
2 bh
h2 2 n(shh+1i)
116B 2 Kah bh196BK(h+1)
2

e
.
2 bh



n B

Proving bound P V b (shh+1i) V (shh+1i) completely similar.
P

Finally, proof Theorem 1 concluded


P nB (s0 hHi) 6= (s0 hHi)


P Q(s0 hHi, (s0 hHi)) Q(s0 hHi, B (s0 hHi))

H1
2 n
1
116 9B 2 K 2
(H1)2
2 9K 2 (196BK)H1 H 2
2
(H

1)!
e
3K
(196BK)
2

(17)

noting maximal loss choosing sub-optimal action s0 hHi H.
B.2 Proof Proposition 4.1.1 (BRUE bounds Pe )
Basically, proof Proposition 4.1.1 identical Theorem 1 h < 4.
h 4, note
(
)

Xq
X
p
Pe
0
0
0
b
b
P
Pt (s | s, a) > 3 Pe
P Pt (s | s, a) > P(s | s, a) + 2
B
0
0








195

2t
B4

.

Hoeffding

(18)

fiFeldman & Domshlak

bt (s0 | s, a) P(s0 | s, a) +
Indeed, P

Pe
B2

s0 ,

Xq
X
bt (s0 | s, a)
P
s0

s0

r

P(s0 | s, a) +

Pe
B2
r

"
X p
2P(s0 | s, a) +

s0

2

p
p
2Pe 3 Pe

Pe
2 2
B

#

Therefore, probability Eq. 15 Proposition B.1.1 bounded h > 4
(
)


X
B


0
0


0
bt (s | s, ) V (s hhi) V (s hhi)
P
P
bt
P
2
0



(
)

X
Xq
p
B



bt (s0 | s, a) > 3 Pe

P V (s0 hhi) V b (s0 hhi) q
+P
P
Pt


0

bt (s | s, )
s0
s0
6 Pe P
bh t2

2t

Bah e 36Pe + B4
bh t2
36P
e

2Bah e

I.H. & Eq. 18

.

Consequently,
2 na
158B 2 ah bh306P
e .
P {E }
e
2 bh

Plugging bound P {E } chain bounding Propositions B.1.2-B.1.5,
obtain result.
B.3 Proof Theorem 2
proof Theorem 2 follows proof Theorem 1 noting
effect rollout-action
policy
n
convergence rate comes play
n(shh+1i)
bounds P n(shh+1i, a) < 2K
,
condition Theorem 2 simply postulates bounds exponential-rate
convergence guaranteed.
B.4 Proof Theorem 3
general, proof Theorem 3 follows lines proof convergence rate
BRUE Theorem 1, role Proposition B.3.i corresponding role
Proposition B.1.i proof Theorem 1. Essentially, proof Theorem 3 deviates
substantially proof Theorem 1 modification Propositions B.1.1
B.1.2 partial averaging, captured Propositions B.4.1 B.4.2, respectively.
bounds rest propositions adjusted accordingly. formulate proof
196

fiSimple Regret Optimization Online Planning MDPs

arbitrary values , although derive bounds particular choice depth1
dependent h (BK)
h1 . Similarly Theorem 1, proof Theorem 3 based
Lemma 5 below.
Lemma 5 node shhi
n

B
2
P V (shhi) VPb (shhi) ah ebh n(shhi)
n B

2
P VPb (shhi) V (shhi) ah ebh n(shhi) ,



12BK h1
(h!)2 ,
2
1
bh =
.
2
9K (196BK)h1 h2
ah = 3K



Proof: proof induction h. base induction identical
Lemma 4, continue straight induction step. propositions
made assumption induction hypothesis. Considering node shh + 1i,
make use notation used proof Theorem 1, namely, = (shh + 1i),
aB = B (shh+1i), na = n(shh + 1i, (shh + 1i)), and, {1, . . . , na }, random
b
variables Xt capture accumulated reward samples averaged Q(shh+1i,
), tB capbt capture transition probabilities estiture policy induced BRUE sample t, P
mations sample t. addition, also use additional abbreviation na = b(1)na c.
Proposition B.4.1 > 0, let E event which, sampling Xt , =
na , . . . , na , holds


B
P b 0
) V (s0 hhi) V (s0 hhi) ,
P
(s
|
s,

1.
0


b
2
Pt

2.



P

Then,

s0



bt (s0 | s, ) (R(s, , s0 ) + V (s0 hh 1i))
P(s0 | s, ) P


2,

v


u
h+1 na 2
u 2
4B
log
na
1
8h2
.
=
+
3
bh

P {E }

b 2 n
8Bh2
h12Ba

e
.
h
2

Proof: follows P1
)
(


X
B


bt (s0 | s, ) V (s0 hhi) V (s0 hhi)
P
P
bt
P
2
0




X

B

P V (s0 hhi) V b (s0 hhi) q
Pt

bt (s0 | s, )
s0
2 BP
197

fiFeldman & Domshlak

Thus,


na

X
X

B
P {E }
P V (s0 hhi) V (s0 hhi) > q

bt (s0 | s, )
0
t=n
2 BP

(
)
na

X
X

0

0


0

0
bt (s | s, ) R(s, , ) + V (s hh 1i) >
+
P
P(s | s, ) P
2

0
t=na



na



X

Bah e

bh t2
4B

I.H.

t=n


+

na
X



e

t2
4h2

Hoeffding

t=n




na
X

2Bah e



bh t2
4B

t=n


=

b 2 n
8Bh2
h12Ba

e
h
2



na
X

t=n


b 2 n
8Bh2
h12Ba

e
h
h+1 na 2

definition

Prop. B.4.2 modified accordingly.

Proposition B.4.2 definition E introduced Proposition B.4.1,
na
bt }na , event E ,
, {P
> 0, holds that, given {tB }t=1
t=1
(1) t, random variables Xt mutually independent,

h fi

fi
bt }, E Q(shh+1i, a) ,
(2) na , E Xt fi {tB }, {P

fi
n

b h2 2 na
h
fi B


b
b
(3) P Q(shh+1i, ) Q(shh+1i, ) fi {t }, {Pt }, E e 8B(h+1)2 .

Proof: correctness mutual independence (1) direct definition BRUE:
dependency samples BRUE induced state information
b turn, proof
collected samples, determined solely B P.
(2) obtained definition E follows:
198

fiSimple Regret Optimization Online Planning MDPs

h fi
X
X
B
fi
bt }, E =
bt (s0 | s, )R(s, , s0 ) +
bt (s0 | s, )V (s0 hhi)
E Xt fi {tB }, {P
P
P
b
P
s0



s0

= Q(shh+1i, )

X

bt (s0 | s, ) R(s, , s0 ) + V (s0 hhi)

P(s0 | s, ) P
s0



X
s0



B
bt (s0 | s, ) V (s0 hhi) V (s0 hhi)
P
b
Pt



2
2
= Q(shh+1i, )
Q(shh+1i, )

definition E

Finally, proof (3) obtained noting
1
h+1 na

v


u

n 2
u 2
na
4B log h+18h2a
X
X
na
1
1


=
+

3
b

n

h
h+1 t=n
t=n


v


u
h+1 na 2

u 2
4B
log
2
na
2
1 1 h+1
8h

+


3
bh
na
h+1
r
4 2 4 2
log x
2
bh h2
+
since

h+1 =
x
5
B
9
35
3

4
na

Therefore,
fi

n
fi
b
bt }, E
P Q(shh+1i, ) Q(shh+1i,
) fi {tB }, {P

na
h

X
1
b
b
= P E Q(shh+1i,
) Q(shh+1i,
)


h+1 na

t=na
fi

h

fi
fi B


b
b
b
P E Q(shh+1i, ) Q(shh+1i, ) fi {t }, {Pt }, E
4
e



bh h2 2 na
8B(h+1)2

fi

fi

fi B
bt }, E
fi {t }, {P
fi

fi

.

(19)

Proposition B.4.3 > 0, holds
n
9Bh2
b h2 2 na
h
b
12B(h+1)2 .
P Q(shh+1i, ) Q(shh+1i,
)

e
h
2
199

fiFeldman & Domshlak

Proof:
n

b
P Q(shh+1i, ) Q(shh+1i,
)

P {E }
fi
fi
n
n
X
fi
b
bt }, E P {tB }, {P
bt } fifi E
P Q(shh+1i, ) Q(shh+1i,
) fi {tB }, {P
+
bt }
{tB ,P

2 2

b h na
9Bh2
h
12B(h+1)2


e
h
2

Props. B.4.1 & B.4.2.

remainder proof identical proof Theorem 1, whereas
b
bounds error estimators Q(shh+1i,
a) aligned Proposition B.4.3.
Proposition B.4.4

Proof:



P Q(shh+1i, ) Q(shh+1i, aB )

2 2

b h n(shh+1i)
10BKh2
h
96BK(h+1)2

e
h
2



P Q(shh+1i, ) Q(shh+1i, aB )



X




b
b

P Q(shh+1i, a) Q(shh+1i, a)
+ P Q(shh+1i, ) Q(shh+1i, )
2
2
a6=a


b h2 2 n(shh+1i)
X
9BKh2
n(shh + 1i)
h
96BK(h+1)2
Prop. B.4.3

+

e
P n(shh + 1i, a)
h
2K
2
aA(s)

2 2

b h n(shh+1i)
10BKh2
h
96BK(h+1)2

.

e
h
2

Hoeffding

induction step concluded Proposition B.4.5.
Proposition B.4.5
n
12BKh2
b h2 2 n(shh+1i)
B
h
196BK(h+1)2 .
P V (shh+1i) VPb (shh+1i)

e
h
2

Proof: Since

B

V (shh+1i) VPb (shh+1i)
= Q(shh+1i, (shh+1i)) Q(shh+1i, aB )


X
b 0 | s, aB ) V (s0 hhi) V B (s0 hhi)
+
P(s
b
P
s0

+

X
s0



b 0 | s, aB ) R(s, aB , s0 ) + V (s0 hhi) ,
P(s0 | s, aB ) P(s
200

fiSimple Regret Optimization Online Planning MDPs

holds
n

B
P V (shh+1i) VPb (shh+1i)


6
P Q(shh+1i, (shh+1i)) Q(shh+1i, aB )
7
(
)


X

B
b 0 | s, aB ) V (s0 hhi) V (s0 hhi)
+P
P(s
b
P
14
s0
)
(

X


b 0 | s, aB ) R(s, aB , s0 ) + V (s0 hhi) >
+P
P(s0 | s, aB ) P(s
14
0


2 2

b h n(shh+1i)
10BKh2
h
136BK(h+1)2

Prop. B.4.4

e
h
2
(
)


X
X

B
b 0 | s, a) V (s0 hhi) V (s0 hhi)
P(s
+
P
b
P
14
s0
aA(s)
)
(

X
X


b 0 | s, a) R(s, a, s0 ) + V (s0 hhi) >
+
P(s0 | s, a) P(s
P
14
0



aA(s)

2 2

2

b h n(shh+1i)
n(shh+1i)
b 2 n(shh+1i)
10BKh2
h

h 196BK
136BK(h+1)2
196K(h+1)2


e
+
BKa
e
+
Ke
h
h
2
b h2 2 n(shh+1i)
12BKh2
h

ah e 196BK(h+1)2 .
2
n B

Proving bound P V b (shh+1i) V (shh+1i) completely similar.

P

Finally, proof Theorem 3 concluded


P nB (s0 hHi) 6= (s0 hHi)


P Q(s0 hHi, (s0 hHi)) Q(s0 hHi, B (s0 hHi))


2 n
12BK H1
2 9K 2 (196BK)H1 H 2
3K
(H!)
e
2

(20)

noting maximal loss choosing sub-optimal action s0 hHi H.

Appendix C. Proof Proposition 2.1.1 (SS bound)
b
Let Q(shhi,
a) average C recursive samples value action shhi,
b 0 | s, a) empirical transition probability based C samples.
let P(s
node shhi action a, probability least 1 e

2 2 C
H2

,

fi
fi
fiX
fi

fi
b 0 | s, a) V (s0 hh 1i)fifi
P(s0 | s, a) P(s
fi
fi 0
fi


201

fiFeldman & Domshlak

Since sparse sampling encounters (min(B, C) K)H nodes, probability bad estimate bounded (min(B, C) K)H e

2 2 C
H2

. Therefore,

b
Q(shhi, a) Q(shhi,
a)

X
b 0 | s, a) V (s0 hh 1i)

P(s0 | s, a) P(s
s0

+

X



0
0
0
0
0
b
b
P(s | s, a) Q(s hh 1i, (s hh 1i)) max
Q(s hh 1i, )
0


s0

+

X
s0




b 0 | s, a) Q(s0 hh 1i, (s0 hh 1i)) Q(s
b 0 hh 1i, (s0 hh 1i)) ,
P(s

(21)

similarly,
b
Q(shhi,
a) Q(shhi, a)

X
b 0 | s, a) P(s0 | s, a) V (s0 hh 1i)

P(s
s0

+

X
s0

+



b 0 | s, a) max Q(s
b 0 hh 1i, a0 ) Q(s0 hh 1i, (s0 hh 1i))
P(s
0

(22)



X
s0

n

b 0 | s, a) max Q(s
b 0 hh 1i, a0 ) Q(s0 hh 1i, a0 ) .
P(s
0


Note two bounds Equations 21,22 result recursion h = + h1 =
h, h upper bounds respective difference. implies that, probability
(min(B, C) K)H e

setting =


2H ,

2 2 C
H2

,
fi
fi
fi
fi
b
Q(s
hHi,
a)

Q(s
hHi,
a)
fi
fi > H
0
0

obtain error probability bounded
2 C

(min(B, C) K)H e 2H 4

proof concludes noting maximal loss choosing non-optimal action
root node s0 hHi H.

202

fiSimple Regret Optimization Online Planning MDPs

References
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis multiarmed
bandit problem. Machine Learning, 47 (2-3), 235256.
Balla, R., & Fern, A. (2009). UCT tactical assault planning real-time strategy games.
Proceedings 21st International Joint Conference Artificial Intelligence
(IJCAI), pp. 4045.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bjarnason, R., Fern, A., & Tadepalli, P. (2009). Lower bounding Klondike Solitaire
Monte-Carlo planning. Proceedings 19th International Conference Automated Planning Scheduling (ICAPS).
Bonet, B., & Geffner, H. (2012). Action selection MDPs: Anytime ao vs. uct.
Proceedings 26th AAAI Conference Artificial Intelligence (AAAI).
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 194.
Browne, C., Powley, E. J., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen, P.,
Tavener, S., Perez, D., Samothrakis, S., & Colton, S. (2012). survey Monte-Carlo
tree search methods. IEEE Transactions Computational Intelligence AI
Games, 143.
Bubeck, S., & Munos, R. (2010). Open loop optimistic planning. Proceedings 23rd
Annual Conference Learning Theory (COLT), pp. 477489.
Bubeck, S., Munos, R., & Stoltz, G. (2011). Pure exploration finitely-armed
continuous-armed bandits. Theoretical Computer Science, 412 (19), 18321852.
Busoniu, L., & Munos, R. (2012). Optimistic planning Markov decision processes.
Proceedings Fifteenth International Conference Artificial Intelligence
Statistics (AISTATS), No. 22 Journal Machine Learning Research - Proceedings
Track, pp. 182189.
Coquelin, P.-A., & Munos, R. (2007). Bandit algorithms tree search. Proceedings
23rd Conference Uncertainty Artificial Intelligence (UAI), pp. 6774,
Vancouver, BC, Canada.
Eyerich, P., Keller, T., & Helmert, M. (2010). High-quality policies Canadian Travelers problem. Proceedings 24th AAAI Conference Artificial Intelligence
(AAAI).
Feldman, Z., & Domshlak, C. (2012). Simple regret optimization online planning
markov decision processes. CoRR, arXiv:1206.3382v2 [cs.AI].
Feldman, Z., & Domshlak, C. (2013). Monte-Carlo planning: Theoretically fast convergence
meets practical efficiency. Proceedings 29th Conference Uncertainty
Artificial Intelligence (UAI), pp. 212221.
Geffner, H., & Bonet, B. (2013). Concise Introduction Models Methods Automated Planning. Synthesis Lectures Artificial Intelligence Machine Learning.
Morgan & Claypool.
203

fiFeldman & Domshlak

Gelly, S., & Silver, D. (2011). Monte-Carlo tree search rapid action value estimation
computer Go. Artificial Intelligence, 175 (11), 18561875.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms
factored MDPs. Journal Artificial Intelligence Research, 19, 399468.
Hay, N., Shimony, S. E., Tolpin, D., & Russell, S. (2012). Selecting computations: Theory
applications. Proceedings Annual Conference Uncertainty Artificial
Intelligence (UAI).
Kearns, M. J., Mansour, Y., & Ng, A. Y. (2002). sparse sampling algorithm nearoptimal planning large Markov decision processes. Machine Learning, 49 (2-3),
193208.
Keller, T., & Eyerich, P. (2012). Probabilistic planning based UCT. Proceedings
22nd International Conference Automated Planning Scheduling (ICAPS).
Keller, T., & Helmert, M. (2013). Trial-based heuristic tree search finite horizon MDPs.
Proceedings 23rd International Conference Automated Planning
Scheduling (ICAPS), pp. 135143.
Kocsis, L., & Szepesvari, C. (2006). Bandit based Monte-Carlo planning. Proceedings
17th European Conference Machine Learning (ECML), pp. 282293, Berlin,
Germany.
Kolobov, A., Mausam, & Weld, D. (2012). LRTDP vs. UCT online probabilistic planning. Proceedings 26th AAAI Conference Artificial Intelligence (AAAI).
Mausam, & Kolobov, A. (2012). Planning Markov Decision Processes: AI Perspective. Synthesis Lectures Artificial Intelligence Machine Learning. Morgan &
Claypool.
Peret, L., & Garcia, F. (2004). On-line search solving Markov decision processes via
heuristic sampling. Proceedings 16th Eureopean Conference Artificial
Intelligence (ECAI), pp. 530534, Valencia, Spain.
Puterman, M. (1994). Markov Decision Processes. Wiley, New York.
Robbins, H. (1952). aspects sequential design experiments. Bulletin
American Mathematical Society, 58 (5), 527535.
Smith, S. J., & Nau, D. S. (1994). analysis forward pruning. Proceedings
AAAI Conference Artificial Intelligence (AAAI), pp. 13861391.
Sturtevant, N. (2008). analysis UCT multi-player games. Proceedings 6th
International Conference Computers Games (CCG), p. 3749.
Tolpin, D., & Shimony, S. E. (2011). better UCT: Rational Monte Carlo sampling trees. CoRR, arXiv:1108.3711v1 [cs.AI].
Tolpin, D., & Shimony, S. E. (2012). MCTS based simple regret. Proceedings
26th AAAI Conference Artificial Intelligence (AAAI).
Walsh, T. J., Goschin, S., & Littman, M. L. (2010). Integrating sample-based planning
model-based reinforcement learning. Proceedings 24th AAAI Conference
Artificial Intelligence (AAAI), pp. 612617.
204

fiSimple Regret Optimization Online Planning MDPs

Zilberstein, S. (1993). Operational Rationality Compilation Anytime Algorithms.
Ph.D. thesis, University California Berkeley.

205

fiJournal Artificial Intelligence Research 51 (2014) 71-131

Submitted 5/14; published 9/14

Testability BDI Agent Systems
Michael Winikoff
Stephen Cranefield

michael.winikoff@otago.ac.nz
stephen.cranefield@otago.ac.nz

Department Information Science
University Otago
New Zealand

Abstract
deploying software system need assure (and stakeholders)
system behave correctly. assurance usually done testing system.
However, intuitively obvious adaptive systems, including agent-based systems,
exhibit complex behaviour, thus harder test. paper examine
obvious intuition case Belief-Desire-Intention (BDI) agents. analyse
size behaviour space BDI agents show although intuition correct,
factors influence size expected be. Specifically,
found introduction failure handling much larger effect size
behaviour space expected. also discuss implications findings
testability BDI agents.

1. Introduction
Increasingly called upon develop software systems operate dynamic environments, robust face failure, required exhibit flexible behaviour, operate open environments. One approach developing systems
demonstrated effectiveness range domains use metaphor
software agents (Wooldridge, 2002). Agent-based systems increasingly finding
deployment wide range applications (e.g. Munroe, Miller, Belecheanu, Pechoucek,
McBurney, & Luck, 2006; Benfield, Hendrickson, & Galanti, 2006).
agent-based systems increasingly deployed, issue assurance rears head.
deploying system, need convince rely system (or
responsible fails) system will, fact, work. Traditionally,
assurance done testing1 . However, generally accepted adaptive systems
exhibit wide complex range behaviours, making testing hard. example:
Validation extensive tests mandatory . . . . However, task proved
challenging . . . . Agent-based systems explore realms behaviour outside peoples expectations often yield surprises. (Munroe et al., 2006, Section 3.7.2)
is, intuition agent systems exhibit complex behaviour, makes
hard test. paper explore intuition, focusing well known BeliefDesire-Intention (BDI) approach realising adaptive flexible agents (Rao & Georgeff,
1. Although considerable research formal methods context agent systems (Dastani,
Hindriks, & Meyer, 2010), yet ready real world application (see Section 7),
concerns scope work applicability (Winikoff, 2010).
c
2014
AI Access Foundation. rights reserved.

fiWinikoff & Cranefield

1991; Bratman, 1987), demonstrated practically applicable, resulting
reduced development cost increased flexibility (Benfield et al., 2006).
explore intuition agent systems hard test analysing
space possible behaviours BDI agents, is, number paths BDI
program, probability failure. focus BDI agents provide welldefined execution mechanism analysed, also seek understand
complexities (and testability implications) adaptive intelligent behaviour
absence parallelism (since implications parallelism already well known).
derive number paths BDI program function various parameters (e.g. number applicable plans per goal failure fate). naturally
leads us also consider number paths affected various parameters.
might expected, show intuition agent systems hard test
correct, i.e. agent systems large number paths. also show BDI
agents harder test procedural programs, showing number paths
BDI program much larger number paths similarly-sized
procedural program.
contribution paper threefold. Firstly, confirms intuition BDI
programs hard test. Secondly, quantifying number paths,
function parameters BDI program. Thirdly, find surprising results
parameters influence number paths.
Although recently increasing interest testing agent systems (Zhang,
Thangarajah, & Padgham, 2009; Ekinci, Tiryaki, Cetin, & Dikenelli, 2009; Gomez-Sanz,
Bota, Serrano, & Pavon, 2009; Nguyen, Perini, & Tonella, 2009b), surprisingly little work determining feasibility testing agent systems first place.
Padgham Winikoff (2004, pp. 1719) analyse number successful executions
BDI agents goal-plan tree (defined Section 3), consider failure failure
handling analysis, consider testability implications. Shaw, Farwer
Bordini (2008) analysed goal-plan trees shown checking whether goal-plan
tree execution schedule respect resource requirements NP-complete.
different problem one tackle: concerned allocation
resources amongst goals, rather behaviour space.
briefly address number possible criticisms work, considering
existing work.
1. number paths useful metric assessing testability?
consider related area software testing (Section 1.1) argue
metric well established one, appropriate use assess testability.
2. Isnt obvious corollary complexity HTN planning?
consider detail HTN planning problem (Section 1.2) argue although
BDI execution cycle certain similarities HTN planning, differences
significant, and, particular, mean problem HTN planning
simply different problem testing BDI programs.
3. use combinatorial analysis, rather complexity analysis?
combinatorial analysis precise: yields formulae exact number
72

fiOn Testability BDI Agent Systems

paths, exact probabilities failure. latter (see Section 4.5)
informative order magnitude complexity. Additionally allows
us consider issues complexity analysis would address, effect
number failures number paths.
1.1 Software Testing
trying assess hard agent systems test. concretely, given BDI
agent program, want know hard program test. reduced
directly question test set adequacy. agent program P easy test precisely
extent exists test set adequate testing P ,
infeasibly large. Conversely, agent program P hard test extent
adequate test set would infeasibly large. words, hardness
testing program directly assessed size required test set adequate
respect suitable adequacy criteria.
many criteria used assess whether given set tests adequate (for recent overview, see Mathur, 2008). Given interested assessing
difficulty testing given program, clearly looking white box testing. Furthermore, working abstract goal-plan trees rather detailed programs
(see Section 2). means need consider control-flow based metrics, rather
data-flow, since abstract goal-plan tree contain data-flow information.
Focussing white box testing criteria control-flow based, basic
long-standing criterion assessing test set adequacy paths program
covered (Miller & Maloney, 1963). example, consider program following form.
3. ...
1. Input x

2. condition ...

5. endif

6. C

4. ... else B

two paths program: (1, 2, 3, 5, 6) (1, 2, 4, 5, 6),
adequate test set must least two tests adequate: one exercise first path,
another exercise second. case test set single test
inadequate, result part program executed testing.
obvious complication covering paths program loop result
infinite number paths, since loop potentially executed number
times. standard technique dealing bound length paths,
number executions loop (Zhu, Hall, & May, 1997, p. 375). Bounding execution
loops done either calculating upper bound number iterations based
data (Mathur, 2008, p. 53), considering paths loops executed
zero times one time (Mathur, 2008, p. 408).
One question might asked consider paths, rather weaker
criterion. Agent applications typically involve environments non-episodic. is,
environments history matters. means behaviour given plan goal
is, general, sensitive agents history, hence need consider different
possible histories. Achieving goal may different done first thing
73

fiWinikoff & Cranefield

agent does, failed plan already performed number actions.
means makes sense consider path-based criterion testing.
Furthermore, although paths adequacy criterion often considered impractical, reason appears primarily existence infinite number paths
presence loops. instance, Zhu et al. (1997, p. 375) say plan coverage criterion
strong practically useful programs, infinite
number different paths program loops. setting,
loops, existence infinite number paths issue, considering number
paths possible.
therefore use number paths proxy measure testing difficulty:
paths program, adequate test set (according
paths criterion) need large. hand, number paths
large, adequate test set need large.
one issue need consider: since paths strong criterion,
possible that, even absence (or bounding) loops, criterion always results
infeasibly large numbers paths. order address issue also analysis
number paths procedural programs (of equivalent size), compare
number paths BDI programs (see Section 6).
Finally, bears noting paths criterion considers parts
program traversed testing, ignores values variables. So, example,
trivial program consisting single statement x := x x single one-step path,
trivially covered, many traces (x = 0, 1, 2 . . .).
1.2 HTN Planning
similarities Hierarchical Task Network (HTN) planning (Erol, Hendler,
& Nau, 1994) BDI execution (de Silva & Padgham, 2004): use hierarchical
representation goals (non-primitive tasks HTN terminology), plans (decomposition methods) goal-plan trees (task networks). complexity HTN planning
explored. Given similarities, simply exploit known complexity
results?
turns cannot so, simple reason complexity HTN
planning concerns plan finding problem, different BDI plan execution,
Sardina Padgham explain:
BDI agent systems HTN planners come different communities
differ many important ways. former focus execution plans,
whereas latter concerned actual generation plans.
former generally designed respond goals information; latter
designed bring goals. addition, BDI systems meant
embedded real world therefore take decisions based particular
(current) state. Planners, hand, perform hypothetical reasoning
actions interactions multiple potential states. Thus, failure
different meaning two types systems. context
planning, failure means plan potential plan suitable; within
BDI agent systems failure typically means active (sub)plan ought
74

fiOn Testability BDI Agent Systems

aborted. Whereas backtracking upon failure option planning systems,
generally BDI systems, actions taken real world. (Sardina
& Padgham, 2011, p. 45, bold emphasis added)
words, HTN systems plan ahead execution, whereas BDI systems interleave
execution planning2 .
HTN plan existence problem answers question plan exist?
complexity studied. settings correspond BDI execution (many goals,
total ordering within plans, variables) known EXPSPACE-hard
DEXPTIME (Erol, Hendler, & Nau, 1994, 1996). However, work address
question BDI execution. considering complexity plan existence HTN
planning asking computational complexity search process
result plan. hand, asking number paths
goal-plan tree asking possibilities arise executing plan.
illustrate point, consider following example. Suppose single goal
G decomposed two alternative plans, P1 P2 . Plan P1 consists
sequential execution actions a, b, c; plan P2 consists sequential execution
actions e. plan existence problem boils considering options P1
P2 , since case search space simple, offering two options.
hand, question many paths exist BDI execution considers different
ways goal-plan tree executed. Whereas HTN planning considers P1
single atomic decomposition, BDI execution needs consider sequence actions a, b, c
distinct steps. possible three actions succeed (giving trace a, b, c),
also possible action b fail, followed P2 (successfully) used (giving trace
a, b8, d, e), action c fail, followed P2 (successfully) used (giving trace
a, b, c8, d, e).
Overall, means complexity analysis Erol et al. (1994, 1996)
different problem, HTN complexity results relevant. Finally, note
that, fact, setting, plan existence problem actually trivially true: since
BDI programs constraints always expansion program
sequence actions.
remainder paper structured follows. begin briefly presenting
BDI execution model (Section 2) discussing BDI execution viewed
process transforming goal-plan trees (Section 3). Section 4 core paper
analyse number paths BDI-style goal-plan tree. consider
analysis assumptions hold real system real platform (Section 5),
analysis BDI programs compares analysis (number paths)
conventional procedural programs (Section 6). Finally, conclude discussion
implications testing future work (Section 7).
2. approaches blur difference adding look-ahead planning BDI online execution
HTNs, example planner RETSINA multi-agent system (Paolucci, Shehory, Sycara, Kalp,
& Pannu, 2000) ability interleave planning execution. However, theoretical analysis
extension reported, analysis Erol, Hendler, Nau (1994, 1996) applies
classical HTN planning.

75

fiWinikoff & Cranefield

2. BDI Execution Model
describe Belief-Desire-Intention (BDI) model explain chose
model agent execution. addition well known widely used, BDI model
well defined generic. well defined allows us analyse behaviour spaces
result using it. generic implies analysis applies wide range
platforms.
BDI model viewed philosophical (Bratman, 1987) logical (Rao
& Georgeff, 1991) perspectives, interested implementation perspective, exhibited range architectures platforms, JACK (Busetta,
Ronnquist, Hodgson, & Lucas, 1999), JAM (Huber, 1999), dMARS (dInverno, Kinny, Luck,
& Wooldridge, 1998), PRS (Georgeff & Lansky, 1986; Ingrand, Georgeff, & Rao, 1992),
UM-PRS (Lee, Huber, Kenny, & Durfee, 1994), Jason (Bordini, Hubner, & Wooldridge,
2007), SPARK (Morley & Myers, 2004), Jadex (Pokahr, Braubach, & Lamersdorf, 2005)
IRMA (Bratman, Israel, & Pollack, 1988). purposes analysis here,
formal detailed presentation unnecessary. interested formal semantics
BDI languages referred work Rao (1996), Winikoff, Padgham, Harland,
Thangarajah (2002) Bordini et al. (2007), example.
implementation BDI agent key concepts beliefs (or, generally,
data), events plans. reader may find surprising goals key concepts
BDI systems. reason goals modelled events: acquisition new goal
viewed new goal event, agent responds selecting executing plan
handle event3 . remainder section, keeping established
practice, describe BDI plans handling events (not goals).
BDI plan consists three parts: event pattern specifying event(s) relevant
for, context condition (a Boolean condition) indicates situations plan
used, plan body executed. plans event pattern context condition
may terms containing variables, matching unification process (depending
particular BDI system) used BDI interpreters find plan instances respond
given event. general plan body contain arbitrary code programming
language4 , however purposes assume5 plan body sequence steps,
step either action6 (which succeed fail) event posted.
example, consider simple plans shown Figure 1. first plan, Plan A,
relevant handling event achieve goal go-home, applicable situations
agent believes train imminent. plan body consists sequence
four steps (in case assume actions, could also modelled
events handled plans).
key feature BDI approach plan encapsulates conditions
applicable defining event pattern context condition. allows
additional plans given event added modular fashion, since invoking
3. types event typically include addition removal beliefs agents belief set.
4. example, JACK plan body written language superset Java.
5. follows abstract notations AgentSpeak(L) (Rao, 1996) (Winikoff et al., 2002)
aim capture essence range (more complex) BDI languages.
6. includes traditional actions affect agents environment, internal actions
invoke code, check whether certain condition follows agents beliefs.

76

fiOn Testability BDI Agent Systems

Plan A: handles event:
achieve goal go-home
context condition:
train imminent
plan body:
(1) walk train station
(2) check train running time
(3) catch train
(4) walk home
Plan B: handles event:
achieve goal go-home
context condition:
raining bicycle
plan body:
(1) cycle home
Plan C: handles event:
achieve goal go-home
context condition:
true (i.e. always applicable)
plan body:
(1) walk bus stop
(2) check buses running
(3) catch bus
(4) walk home
Figure 1: Three Simple Plans
context (i.e. triggering event posted) contain code selects amongst
available plans, key reason flexibility BDI programming.
typical BDI execution cycle elaboration following event-driven process
(summarised Figure 2)7 :
1. event occurs (either received outside source, triggered within
agent).
2. agent determines set instances plans plan library event patterns
match triggering event. set relevant plan instances.
3. agent evaluates context conditions relevant plan instances generate
set applicable plan instances. relevant plan instance applicable context
condition true. applicable plan instances event deemed
failed, posted plan, plan fails. Note
single relevant plan may lead applicable plan instances (if context condition
false), one applicable plan instance (if context condition,
may contain free variables, multiple solutions).
4. One applicable plan instances selected executed. selection mechanism varies platforms. generality, analysis make as7. BDI engines are, fact, complicated interleave execution multiple
active plan instances (or intentions) triggered different events.

77

fiWinikoff & Cranefield

Boolean function execute(an-event)
let relevant-plans = set plan instances resulting
matching plans event patterns an-event
let tried-plans =
true
let applicable-plans = set plan instances resulting
solving context conditions relevant-plans
applicable-plans := applicable-plans \ tried-plans
applicable-plans empty return false
select plan p applicable-plans
tried-plans := tried-plans {p}
execute(p.body) = true return true
endwhile
Boolean function execute(plan-body)
plan-body empty return true
elseif execute(first(plan-body)) = false return false
else return execute(rest(plan-body))
endif
Boolean function execute(action)
attempt perform action
action executed successfully return true else return false endif

Figure 2: BDI Execution Cycle
sumptions plan selection. plans body may create additional events
handled using process.
5. plan body fails, failure handling triggered.
brevity, remainder paper use term plan loosely mean
either plan plan instance intention clear context.
Regarding final step, approaches dealing failure. Perhaps
common approach, used many existing BDI platforms,
select alternative applicable plan, consider event failed
remaining applicable plans. determining alternative applicable plans one may
either consider existing set applicable plans, re-calculate set applicable
plans (ignoring already tried), done Figure 2. makes
sense situation may changed since applicable plans determined.
Many (but all) BDI platforms use failure-handling mechanism retrying
plans upon failure, analysis applies platforms.
One alternative failure-handling approach, used Jason (Bordini et al., 2007),
post failure event handled user-provided plan. Although
flexible, since user specify upon failure, place burden
78

fiOn Testability BDI Agent Systems

specifying failure handling user. Note Jason provides pattern allows
traditional BDI failure-handling mechanism specified succinctly (Bordini et al., 2007,
pp. 171172). Another alternative failure-handling approach used 2APL (Dastani,
2008) predecessor, 3APL: permit programmer write plan repair rules
conditionally rewrite (failed) plan another plan. approach, like Jasons,
quite flexible, possible analyse general way plan rules
quite arbitrary. Another well known BDI architecture IRMA, described
high-level prescribe specific failure-handling mechanism:
full development architecture would give account
ways resource-bounded agent would monitor prior plans
light changes belief. However developed, course
times agent give prior plan light new belief
plan longer executable. happens, new process
deliberation may triggered (Bratman et al., 1988).
Given BDI execution cycle discussed above, three example plans given earlier
(Figure 1) give rise range behaviours, including following:
Suppose event achieve goal go-home posted agent believes
train imminent. walks train station, finds train running
time, catches train, walks home.
Suppose upon arrival train station agent finds trains
delayed. Step (2) Plan fails, agent considers alternative plans.
raining present time, Plan B applicable, Plan C adopted
(to catch bus).
Suppose agent decided catch bus (because train believed
imminent, raining), attempting execute Plan C fails (e.g.
bus strike). agent reconsider plans rain stopped (and
bicycle) may use Plan B.
Note correct (respectively incorrect) behaviour distinct successful
(respectively failed) execution plan. Software testing essence process running system checking whether observed behaviour trace correct (i.e. conforms
specification, model). hand, BDI agents behaviour traces
classified successful failed. However, correctness given execution
trace independent whether trace successful failed execution. successful
execution may, fact, exhibit behaviour correct, instance, traffic controller
agent may successfully execute actions set traffic signals intersection green
achieve goal so. successful execution, incorrect behaviour.
also possible failed execution correct. instance, traffic controller agent
attempting route cars point point B, traffic accident blocked key
bridge two points, rational (and correct) behaviour agent
fail achieve goal.
79

fiWinikoff & Cranefield

3. BDI Execution Goal-Plan Tree Expansion
BDI execution, summarised Figure 2, dynamic process progressively executes
actions goals posted. order easily analyse process, present
alternative view declarative. Instead viewing BDI execution process,
view data transformation (finite) goal-plan tree sequence action
executions.
events plans visualised tree goal8 children
plan instances applicable it, plan instance children sub-goals
posts. goal-plan tree and-or tree: goal realised one plan
instances (or) plan instance needs sub-goals achieved (and).
Viewing BDI execution terms goal-plan tree action sequences makes
analysis behaviour space size9 easier. consider BDI execution process taking
goal-plan tree transforming sequence recording (failed successful)
executions actions, progressively making decisions plans use
goal executing plans.
process non-deterministic: need choose plan goal tree.
Furthermore, consider failure, need consider action whether fails
not, fail, failure recovery done.
define transformation process detail. Prolog code implementing
process found Figure 3. defines non-deterministic predicate exec first
argument (input) goal-plan tree, second argument (output) sequence
actions. goal-plan tree represented Prolog term conforming following
simple grammar (where GPT abbreviates Goal-Plan Tree, AoGL abbreviates Action
Goal List, symbol):
hGPT ::= goal([]) | goal([hPlanListi])
hPlanListi ::= hPlani | hPlani,hPlanListi
hPlani ::= plan([]) | plan([hAoGLi])
hAoGLi ::= act(A) | hGPT | act(A),hAoGLi | hGPT i,hAoGLi
example, simple goal-plan tree shown Figure 4 modelled Prolog term
goal([plan([act(a)]), plan([act(b)])]).
analysis make simplifying assumption. Instead modelling instantiation
plans plan instances, assume goal-plan tree contains applicable plan
instances. Thus, order transform goal node sequence actions (nondeterministically) select one applicable plan instances. selected plan
transformed turn, resulting action sequence (line 2 Figure 3). selecting
plan, consider possibility applicable plans could chosen,
first plan. done different points time different plan instances may
applicable. saw example earlier, Plan chosen failed,
8. order consistent existing practice shall use term goal rather event
remainder paper.
9. remainder paper use term behaviour space size, rather
cumbersome term number paths BDI program.

80

fiOn Testability BDI Agent Systems

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

exec ( goal ([]) ,[]).
exec ( goal ( Plans ) , Trace ) : - remove ( Plans , Plan , Rest ) , exec ( Plan , Trace1 ) ,
( failed ( Trace1 ) -> recover ( Rest , Trace1 , Trace ) ; Trace = Trace1 ).
exec ( plan ([]) , []).
exec ( plan ([ Step | Steps ]) , Trace ) : - exec ( Step , Trace1 ) ,
( failed ( Trace1 ) -> Trace = Trace1 ; continue ( Steps , Trace1 , Trace )).
exec ( act ( Action ) , [ Action ]).
exec ( act ( Action ) , [ Action , fail ]).
failed ( Trace ) : - append (X ,[ fail ] , Trace ).
recover ( Plans , Trace1 , Traces ) : exec ( goal ( Plans ) , Trace2 ) , append ( Trace1 , Trace2 , Traces ).
continue ( Steps , Trace1 , Trace ) : - exec ( plan ( Steps ) , Trace2 ) ,
append ( Trace1 , Trace2 , Trace ).
remove ([ X | Xs ] ,X , Xs ).
remove ([ X | Xs ] ,Y ,[ X | Z ]) : - remove ( Xs ,Y , Z ).

Figure 3: Prolog Code Expand Goal-Plan Trees
goal
plan

plan



b

Figure 4: Simple Goal-Plan Tree

Plan C selected (and also failed), finally Plan B (which applicable
Plan failed) selected.
selected plan executes successfully (i.e. action trace doesnt end fail
marker; line 9), resulting trace trace goals execution (line 3). Otherwise, perform failure recovery (line 10), done taking remaining plans
transforming goal plans options. resulting action sequence
appended action sequence failed plan obtain complete action sequence
goal.
process easily seen match described Figure 2 (with exception,
discussed above, begin applicable plans, relevant plans). Specifically,
applicable plan selected executed, successful execution stops.
successful, alternative plan selected execution continues (i.e. action
sequences appended).
order transform plan node first transform first step plan,
either sub-goal action (line 5). successful, continue transform
rest plan, append two resulting traces together (lines 6 12).
first step plan successful, trace simply trace first step
(line 6); words stop transforming plan step fails. Again, process
easily seen correspond plan body execution Figure 2.
81

fiWinikoff & Cranefield

Finally, order transform action action sequence simply take
action singleton sequence (line 7). However, need also take account
possibility action may fail, thus second possibility action followed
failure indicator (line 8). Again, process easily seen correspond action
execution Figure 2. Note model dont concern
action fails: could lack resources, environmental issues.
example applying process two example goal-plan trees found
Appendix A.

4. Behaviour Space Size BDI Agents
consider many paths goal-plan tree used
BDI agent realise goal10 using tree. use analysis previous section
basis; is, view BDI execution transforming goal-plan tree action
traces. Thus, question large behaviour space BDI agents, answered
deriving formulae allow one compute number behaviours, successful
unsuccessful (i.e. failed), given goal-plan tree.
make following uniformity assumptions allow us perform analysis.
simplifying assumptions concern form goal-plan tree.
1. assume subtrees goal plan node structure. is,
leaves goal-plan tree distance (number edges) away
root tree. therefore define depth goal-plan tree
number layers goal nodes contains. goal-plan tree depth 0 plan
sub-goals, goal-plan tree depth > 0 either plan node
children goal nodes depth goal node children plan
nodes depth 1. Note definition depth reverse usual
definition (where depth trees root defined 0). use definition
simplifies presentation derivations later section.
2. assume plan instances depth > 0 k sub-goals.
3. assume goals j applicable plan instances. case
goal j relevant plans, results exactly one applicable plan
instance, also case ways. instance, goal may 2j
relevant plans, half applicable current situation, goal may
single relevant plan j applicable instances. Note assumption rules
possibility infinite number applicable plan instances,
would case plans context condition infinite number solutions.
cannot occur context condition defined terms conjunctions
propositions refer finite belief base. However, occur agents
context conditions also make use Prolog-like knowledge base (as case
agent-oriented programming languages, Jason Goal). Nevertheless,
since deal applicable plans, dont model context conditions.
10. focus single goal analysis: multiple goals treated concurrent interleaving
individual goals. Multiple agents also treated concurrent interleaving, care
needs taken details agent waiting another agent respond.

82

fiOn Testability BDI Agent Systems

Figure 5 shows uniform goal-plan tree depth 2.
g2

d=2

@
R pj1
p11 . . . @
..
.
@

R
@
g11 . . . gk1
..
.
@

R
@
p10 . . . pj0

d=1
d=1
d=0

Figure 5: uniform goal-plan tree
assumptions made clearly unrealistic. means consider
possibility real agent programs behave quite differently, since meet
assumptions. address issue number ways. Firstly, Section 4.4
consider relaxation assumptions defining semi-uniform trees,
number available plan instances (j) vary across different levels tree. Secondly,
Section 5.2 consider example (non-uniform) goal-plan tree industrial
application. derive number paths real goal-plan tree compare
analysis similarly-sized uniform goal-plan trees see whether real (non-uniform)
tree significantly lower number paths uniform tree. Finally, Section 4.7,
consider issue infinite trees allowing trees recursive, defining
number paths (up bound path length) recursive tree.
analysis uses following terminology:
uniformity assumptions mean structure subtree rooted goal
plan node determined solely depth, therefore denote goal
plan node depth gd pd (respectively).
use n4(xd ) denote number successful execution paths goal-plan tree
depth rooted x (where x either goal g plan p). specifying
important sometimes elide it, writing n4(x).
Similarly, use n8(xd ) denote number unsuccessful execution paths
goal-plan tree depth root x (either g p).
extend notation plan body segments, i.e. sequences x1 ; . . . ; xn xi
goal action ; denotes sequential composition. abbreviate sequence
n occurrences x xn (for example, g13 = g1 ; g1 ; g1 ).
4.1 Base Case: Successful Executions
begin calculating number successful paths goal-plan tree
absence failure (and failure handling). analysis follows Padgham
Winikoff (2004, pp. 1719).
Roughly speaking, number ways goal achieved sum number
ways children achieved (since children represent alternatives,
83

fiWinikoff & Cranefield

i.e. goal represented node). hand, number ways plan
achieved product number ways children achieved
(since children must achieved, i.e. plan represented node).
precisely, n4(x1 ; x2 ) = n4(x1 ) n4(x2 ); is, sequence successful x1
x2 successful.
Given tree root g (a goal), assume j children achieved
n different ways11 ; then, select one children, number ways
g achieved jn. Similarly, tree root p (a plan), assume
k children achieved n different ways, then, execute children,
number ways p executed n n, nk . plan children
(i.e. depth 0) executed (successfully) exactly one way. yields following
definition:
n4(gd ) = j n4(pd1 )
n4(p0 ) = 1
n4(pd ) = n4(gd k ) = n4(gd )k
Expanding definition obtain
n4(g1 ) = j n4(p0 ) = j 1 = j
k

n4(g2 ) = j n4(p1 ) = j (n4(g1 ) ) = j (j k ) = j k+1
n4(g3 ) = j n4(p2 ) = j (j k+1 )k = j k
n4(g4 ) = j n4(p3 ) = j (j k

2 +k+1

2 +k+1

)k = j k

3 +k 2 +k+1

generalised to:
n4(gd ) = j

Pd1
i=0

ki

k > 1 simplified using equivalence k i1 + . . . + k 2 + k + 1 = (k 1)/(k 1)
give following closed form definition: (and k = 1 n4(gd ) = n4(pd ) = j )
n4(gd ) = j (k
4

n (pd ) = j

1)/(k1)

k (kd 1)/(k1)

(1)
(2)

Note equation n4(pd ) assumes sub-goals achieved sequentially.
executed parallel number options higher, since need consider
possible interleavings sub-goals execution. example, suppose plan pd
two sub-goals, g1d g2d , sub-goals n4(gd ) successful executions,
execution l steps (we assume ease analysis execution paths
length). number ways interleaving two parallel executions,
length l, calculated follows (Naish, 2007, Section 3):


(2 l)!
2l
=
l
(l!) (l!)
11. tree assumed uniform, children achieved number
ways, thus interchangeable analysis, allowing us write j n rather n1 + . . . + nj .

84

fiOn Testability BDI Agent Systems

hence number ways executing pd parallel execution subgoals is:
4

4

2



n (pd ) = n (gd )

2l
l



= n4(gd )2

(2 l)!
(l!) (l!)

remainder paper assume sub-goals plan achieved
sequentially, since common case, since yields lower figure which,
shall see, still large enough allow conclusions drawn.
4.2 Adding Failure
extend analysis include failure, determine number unsuccessful
executions, i.e. executions result failure top-level goal. moment
assume failure handing (we add failure handling Section 4.3).
order determine number failed executions know failure
occur. BDI systems two places failure occurs: goal
applicable plan instances, action (within applicable plan instance) fails.
However, uniformity assumption means address former caseit
assumed goal always j instances applicable plans. Note
conservative assumption: relaxing results number unsuccessful executions
even larger.
order model latter case need extend model plans encompass
actions. example, suppose plan body form a1; ga; a2; gb; a3 ai
actions, ga gb sub-goals, ; denotes sequential execution. plan
following five cases unsuccessful (i.e. failed) executions:
1. a1 fails
2. a1 succeeds, ga fails
3. a1 ga succeed, a2 fails
4. a1, ga, a2 succeed, gb fails
5. a1, ga, a2 gb succeed, a3 fails
Suppose ga executed successfully n4(ga) different ways. third
case corresponds n4(ga) different failed executions: successful execution ga,
extend adding failed execution a2 (actions executed one way,
i.e. n4(a) = 1 n8(a) = 1). Similarly, gb n4(gb) successful executions fifth
case corresponds n4(ga) n4(gb) different failed executions. ga unsuccessfully
executed n8(ga) different ways second case corresponds n8(ga) different executions. Similarly, fourth case corresponds n4(ga) n8(gb) different executions. Putting
together, total number unsuccessful executions plan p
body a1; ga; a2; gb; a3 sum five cases:
1 + n8(ga) + n4(ga) + n4(ga) n8(gb) + n4(ga) n4(gb)
85

fiWinikoff & Cranefield

formally, n8(x1 ; x2 ) = n8(x1 ) + n4(x1 ) n8(x2 ); is, sequence fail either
x1 fails, x1 succeeds x2 fails. follows n4(xk ) = n4(x)k n8(xk ) =
n8(x) (1 + + n4(x)k1 ), easily proven induction.
generally, assume ` actions before, after, sub-goals
plan, i.e. example plan corresponds ` = 1, following plan body
corresponds ` = 2: a1; a2; g3; a4; a5; g6; a7; a8. plan sub-goals (i.e. depth
0) considered consist ` actions (which quite conservative: particular,
use ` = 1 assume plans depth 0 consist single action).
number unsuccessful execution traces goal-plan tree defined,
based analysis above, follows. First calculate numbers successes
failures following repeated section plan body: gd ; a` :
n4(gd ; a` ) = n4(gd ) n4(a` )
= n4(gd ) n4(a)`
= n4(gd ) 1`
= n4(gd )
n8(gd ; a` ) = n8(gd ) + n4(gd ) n8(a` )
= n8(gd ) + n4(gd ) n8(a) (1 + + n4(a)`1 )
= n8(gd ) + n4(gd ) `
> 0:
n8(pd ) = n8(a` ; (gd ; a` )k )
= n8(a` ) + n4(a` ) n8((gd ; a` )k )
= n8(a) (1 + + n4(a)`1 )) + n4(a)` n8((gd ; a` )k )
= ` + 1 n8(gd ; a` ) (1 + + n4(gd ; a` )k1 )
= ` + (n8(gd ) + n4(gd ) `) (1 + + n4(gd )k1 ))
n4(gd )k 1
= ` + (n8(gd ) + ` n4(gd )) 4
(assuming n4(gd ) > 1)
n (gd ) 1
yields following definitions number unsuccessful executions goalplan tree, without failure handling. equation n8(gd ) derived using
reasoning previous section: single plan selected executed, j
plans.
n8(gd ) = j n8(pd1 )
n8(p0 ) = `
n4(gd )k 1
n4(gd ) 1
4
(for > 0 n (gd ) > 1)

n8(pd ) = ` + (n8(gd ) + ` n4(gd ))

Finally, note analysis number successful executions goal-plan
tree absence failure handling presented Section 4.1 unaffected addition
actions plan bodies. one way sequence actions
succeed, Equations 1 2 remain correct.
86

fiOn Testability BDI Agent Systems

4.3 Adding Failure Handling
consider introduction failure-handling mechanism affects analysis.
common means dealing failure BDI systems respond failure
plan trying alternative applicable plan event triggered plan.
example, suppose goal g (e.g. achieve goal go-home) three applicable plans pa,
pb pc, pa selected, fails. failure-handling mechanism
respond selecting pb pc executing it. Assume pc selected. pc fails,
last remaining plan (pb) used, fails, goal deemed
failed.
result that, might hope, harder fail: way goal
execution fail applicable plans tried fails12 .
number executions computed follows: goal gd j applicable plan instances, n8(pd1 ) unsuccessful executions, n8(pd1 )j
unsuccessful executions plans sequence. Since plans selected
order multiply j! yielding n8(gd ) = j! n8(pd1 )j .
number ways plan fail still defined equation
failure handling happens level goalsbut n8(g) refers new
definition:
j

n8(gd ) = j! n8(pd1 )

(3)

8

n (p0 ) = `

(4)
4

)k

n (gd 1
n4(gd ) 1
4
(for > 0 n (gd ) > 1)

n8(pd ) = ` + (n8(gd ) + ` n4(gd ))

(5)

Turning number successful executions (i.e. n4(x)) observe
effect adding failure handling convert failures successes, i.e. execution
would otherwise unsuccessful extended longer execution may succeed.
Consider simple case: depth 1 tree consisting goal g (e.g. achieve goal go-home)
three children: pa, pbandpc. Previously successful executions corresponded
pi (i.e. select pi execute it). However, failure handling,
following additional successful executions (as well additional cases corresponding
different orderings plans, e.g. pb failing pa successfully executed):
pa fails, pb executed successfully
pa fails, pb executed fails, pc executed succeeds
leads definition form
n4(g) = n4(pa) + n8(pa) n4(pb) + n8(pa) n8(pb) n4(pc)
12. fact, actually underestimate: also possible goal fail none untried
relevant plans applicable resulting situation. noted earlier, assume analysis
goals cannot fail result applicable plan instances. conservative assumption:
relaxing results number behaviours even larger.

87

fiWinikoff & Cranefield

However, need account different orderings plans. instance, case
first selected plan succeeds (corresponding first term, n4(pa)) fact applies
j plans, first term, including different orderings, j n4(p).
Similarly, second term (n8(pa) n4(pb)), corresponding case initially
selected plan fails next plan selected succeeds, fact applies j initial plans,
j 1 next plans, yielding j (j 1) n8(p) n4(p).
Continuing process (for j = 3) yields following formulae:
2

n4(g) = 3 n4(p) + 32 n8(p) n4(p) + 3! n8(p) n4(p)
generalises
j1

n4(g) = j n4(p) + j (j 1) n8(p) n4(p) + + j! n8(p)

n4(p)

resulting following equations (again, since failure handling done goal level,
equation plans Section 4.1):
4

n (gd ) =

j
X

i1

n8(pd1 )

n4(pd1 )

i=1

n4(p0 ) = 1
4

j!
(j i)!

(6)
(7)

4

n (pd ) = n (gd )

k

(for > 0 )

(8)

used standard BDI failure-handling mechanism trying alternative
applicable plans. let us briefly consider alternative failure-handling mechanism
simply re-posts event, without tracking plans already attempted.
fairly easy see this, fact, creates infinite number behaviours: suppose
goal g achieved pa pb, pa could selected, executed resulting
failure, pa could selected again, fail again, etc. suggests
standard BDI failure-handling mechanism is, fact, appropriate, avoids
infinite behaviour space, possibility infinite loop. discussed earlier (in
Section 2), failure recovery mechanism used 3APL 2APL (Dastani, 2008) cannot
analysed general way, since depends details specific agent program;
IRMA (Bratman et al., 1988) provide sufficient details allow analysis.
Tables 1 2 make various equations developed far concrete showing illustrative values n8 n4 range reasonable (and fairly low) values j, k
using ` = 1. Number columns show number goals, plans actions tree. number actions brackets many actions executed
single (successful) execution failure handling. number goals calculated
follows. depth 1 single goal (see Figure 5). depth n + 1
1 + (j k G(n)) goals, G(n) denotes number goals depth n tree.
gives G(n) = 1 + (j k) + (j k)2 + + (j k)n1 . example, j = k = 2,
G(3) = 1 + 4 + 16 = 21. Since goal exactly j plans, number plans tree
depth n j G(n). consider number actions. non-leaf plan
` (k + 1) actions (since k goals, k + 1 places ` actions).
leaf plan ` actions. tree depth n j (j k)n1 leaf plans. Let P (n)
number plans depth n tree, comprised Pn (n) non-leaf plans
88

fiOn Testability BDI Agent Systems

Parameters
j k

2 2
3
3 3
3
2 3
4
3 4
3

goals
21
91
259
157

Number
plans
42
273
518
471


actions
62 (13)
363 (25)
776 (79)
627 (41)

n4(g)
128
1,594,323

n8(g)
614
6,337,425

1,099,511,627,776

6,523,509,472,174

10,460,353,203

41,754,963,603

Table 1: Illustrative values n4(g) n8(g) without failure handling. first number actions (e.g. 62) number actions tree, second
(e.g. 13) number actions single execution failures occur.

Parameters
j k

2 2
3
3 3
3
2 3
4
3 4
3

goals
21
91
259
157

Number
plans
42
273
518
471


actions
62 (13)
363 (25)
776 (79)
627 (41)

n4(g)
6.33 1012
1.02 10107
1.82 10157
3.13 10184

n8(g)
1.82 1013
2.56 10107
7.23 10157
7.82 10184

Table 2: Illustrative values n4(g) n8(g) failure handling
Pl (n) leaf plans, i.e. P (n) = Pn (n) + Pl (n). number actions depth n tree
(` (k + 1)) Pn (n) + ` Pl (n). example, j = k = 2 ` = 1,
P (3) = 2 G(3) = 42, comprised 32 leaf plans 10 non-leaf plans.
therefore (1 3 10) + (1 32) = 62 actions.
4.4 Recurrence Relations
equations previous sections define functions n4 n8 mutual recurrence
depth goal-plan tree uniform branching structure. effect
increasing parameters k ` evident level recursion,
clear effect increasing number applicable plan instances j
given goal. aim section explore effects changing j.
relaxing uniformity assumption. Specifically, allow number plans available
vary goal nodes different depths tree, still assuming nodes
given depth structure. refer semi-uniform goal-plan trees.
derive set recurrence relations n4 n8 presence failure handling
explicitly show effect adding new plan goal root particular
sub-tree.
begin defining generalised notation n8(gj ) n4(gj ) j list13
(jd , jd1 , . . . , j0 ) element ji represents number plans available goals
depth goal-plan tree. denote empty list hi write j j represent
list head j tail j.
13. order corresponds definition depth, decreases tree.

89

fiWinikoff & Cranefield

generalise Equations 3 6 apply semi-uniform goal-plan trees,
derivation equations depended sub-nodes goal plan node
structure. assumption preserved generalised setting.
therefore rewrite equations using new notation, also express right
hand sides functions f 8 f 4 n8(pj ) (for f 4 ) n4(pj ). aim find recursive
definition f 8 f 4 recurrence j.
n8(gjj ) = f 8 (j, n8(pj ))
n4(gjj ) = f 4 (j, n8(pj ), n4(pj ))

f 8 (j, a) = j! aj
f 4 (j, a, b) =

j
X

b ai1

i=1

j!
(j i)!

(change bounds 0 . . . n, hence replace + 1)
=

j1
X

b a(i+1)1

i=0

j!
(j (i + 1))!

(simplify using (j (i + 1))! = (j i)!/(j i) )
=

j1
X
i=0

b ai

j!(j i)
(j i)!

(multiple i!/i! reorder)
=

j1
X
i=0

j!
i! ai (j i) b
i!(j i)!


j
(use definition binomial:
= j!/i!(j i)!)

j1
X
j
i! ai (j i) b
=


(9)

i=0

expression right last line corresponds following combinatorial analysis f 4 . goal gjj , successful execution involve sequence
plan executions thatfail (for i, 0 j 1) followed one plan execution
succeeds. ji ways choosing failed plans, ordered i! ways,
plan = n8(pj ) ways fail. j ways choosing final
successful plan, b = n4(pj ) ways succeed.
goal find explicit characterisation incremental effect adding
extra plan n8(gjj ) n4(gjj ) finding definitions f 8 f 4 recurrence relations
terms parameter j. Deriving recurrence relation f 8 straightforward:
f 8 (j, a) = j! aj = (j (j 1) . . . 21) (a
. . a}) = (j a) ((j 1) a) . . . (2 a) (1 a)
| .{z
j times
90

fiOn Testability BDI Agent Systems

n4(gjj ) = f 4 (j, n8(pj ), n4(pj ))
n8(gjj ) = f 8 (j, n8(pj ))
f 4 (0, a, b) = 0
f 4 (j +1, a, b) = (j +1) (b + f 4 (j, a, b))

(10)

8

f (0, a) = 1
8

f (j +1, a) = (j +1) f 8 (j, a)
n4(phi ) = 1
n8(phi ) = `
n4(pj ) = n4(gj )k , j 6= hi
n4(gj )k 1
, j 6= hi
n8(pj ) = ` + n8(gj ) + ` n4(gj )
n4(gj ) 1
Figure 6: Recurrence relations numbers failures successes goal plan tree
presence failure handling

shows f 8 (0, a) = 1 f 8 (j +1, a) = (j +1) f 8 (j, a)
However, derivation recurrence relation f 4 simple. use
technique first finding exponential generating function (e.g.f.) (Wilf, 1994)
sequence {f 4 (j, a, b)}
j=0 , using derive recurrence relation. details
given Appendix B, yield equation 10 Figure 6.
Equation 10 (copied Equation 25 Appendix B) gives us recurrence relation
14
sequence {f 4 (j, a, b)}
j=0 seeking . Figure 6 brings together
equations far failure-handling case (including previous
section defining n4(pd ) n8(pd ), generalised semi-uniform trees).
formulation gives us different way looking recurrence, allows us
easily see behaviour space grows number applicable plans, j,
goal grows. Considering meaning parameters b numbers failures
successes (respectively) plan level current goal node, equation
f 4 (j +1, a, b) seen following combinatorial interpretation. One plan
must selected try initially (there j +1 choices) either succeed (in one
b different ways), meaning plans need tried, fail (in one different
ways). fails, goal must succeed using remaining j plans,
occur f 4 (j, a, b) ways.
see growth number successful executions goal grows
rate greater j!aj , presence b term. relaxed uniformity
14. simple case = b = 1 listed sequence A007526 On-Line Encyclopedia
Integer Sequences (Sloane, 2007): number permutations nonempty subsets {1, , n}.

91

fiWinikoff & Cranefield

constraint used recurrence relations also gives us way investigate numbers
traces goal-plan trees different semi-uniform shapes. However, remainder
paper focus uniform trees using original parameter j (with exception
Section 4.7).
4.5 Probability Failing
Section 4.3 said introducing failure handling makes harder fail. However,
Tables 1 2 appear first glance contradict this, many ways
failing failure handling without failure handling.
key understanding apparent discrepancy consider probability
failing: Tables 1 2 merely count number possible execution paths, without
considering likelihood particular path taken. Working probability
failing (as below) shows although many ways failing (and also
succeeding), probability failing is, indeed, much lower.
Let us denote probability execution goal-plan tree root x depth
failing p8(xd ), probability succeeding p4(xd ) = 1 p8(xd ).
assume probability action failing 15 . probability
given plans actions succeeding simply (1 )x x number actions.
Hence probability plan failing failure (one of) actions simply
1 (1 )x , i.e. plan depth 0 probability failure is:
0 = 1 (1 )`
plan depth greater 0 probability failure due actions is:
= 1 (1 )` (k+1)
(recall plan ` actions before, after, between, k sub-goals).
Considering actions also sub-goals g1 , . . . , gk plan p,
plan succeed, sub-goals must succeed, additionally, plans
actions must succeed giving p4(pd ) = (1 ) p4(gd )k . easily derive
equation p8(pd ) (given below). Note reasoning applies plan regardless
whether failure handling, failure handling done goal level.
absence failure handling, goal g possible plans p1 , . . . , pj succeed
must select one plan execute it, probability success probability
plan succeeding, i.e. p4(gd ) = p4(pd1 ). ignore moment possibility
goal failing applicable plans. assumption relaxed later on.
Formally, then, case without failure handling:
p8(gd ) = p8(pd1 )
p8(p0 ) = 0
k

p8(pd ) = 1 [(1 ) (1 p8(gd )) ]
15. simplicity, assume failure action plan independent failure
actions plan.

92

fiOn Testability BDI Agent Systems


0.05

0.01


2
3
4
2
3
4

failure handling
30%
72%
98%
07%
22%
55%

failure handling
0.64%
0.81%
0.86%
0.006%
0.006%
0.006%

Table 3: Goal failure probabilities without failure handling
consider happens failure handling added. case, order
goal fail, plans must fail, i.e. p8(gd ) = p8(pd1 )j . Since failure handling
goal level, equation plans unchanged, giving:
p8(gd ) = p8(pd1 )j
p8(p0 ) = 0
k

p8(pd ) = 1 [(1 ) (1 p8(gd )) ]
easy see equations patterns probabilities actually are,
so, illustration purposes, Table 3 shows probability failure is,
without failure handling, two scenarios. values computed using j = k = 3
(i.e. relatively small branching factor) ` = 1. consider two cases:
= 0.05 hence 0.185 (which rather high), = 0.01 hence
0.04.
seen, without failure handling, failure magnified : larger goalplan tree is, actions involved, hence greater chance action
somewhere failing, leading failure top-level goal (since failure
handling). hand, failure handling, probability failure low,
doesnt appear grow significantly goal-plan tree grows.
relax assumption goal cannot fail applicable
plans, i.e. goal fail plans tried. Unfortunately, relaxing
assumption complicates analysis need consider possibility none
remaining plans applicable point failure handling attemped.
Let us begin reconsidering case failure handling. use g
denote probability goal failing none remaining plans applicable.
case failure handling non-zero g indicates situations
goal applicable plans, may indicate error part
programmer, certain situations goal may possible achieve.
assume, analysis purposes, probability constant, particular,
depend plans already tried number relevant
plans remaining.
probability goal failing p8(gd ) = g + (1 g ) p8(pd1 ), i.e. goal fails
either plans applicable applicable plans selected
plan fails. before, equation plans unchanged, since failure handling done
93

fiWinikoff & Cranefield

goal level. following equations case without failure handling:
p8(gd ) = g + (1 g ) p8(pd1 )
p8(p0 ) = 0
k

p8(pd ) = 1 [(1 ) (1 p8(gd )) ]
Observe setting g = 0 yields equations derived earlier, assumed
goal cannot fail due inapplicable plans.
consider probability failure failure handling. goal two
plans following cases:
goal fail plans applicable (g )
applicable plans ((1 g ) . . .) goal fail first selected
plan fails (p8(pd1 ) . . .) failure handling successful, occur
either applicable plans (g ) applicable plans ((1 g ) . . .)
selected plan fails (p8(pd1 )).
Putting together, goal two plans have:
p8(gd ) = g + (1 g ) p8(pd1 ) (g + (1 g ) p8(pd1 ))
general case j available plans, goal fail if:
A. applicable plans outset, probability g ,
B. applicable plans (1 g ), selected plan fails (p8(pd1 ))
either applicable plans (g ),
C. applicable plans (1 g ), selected plan fails (p8(pd1 ))
either applicable plans (g ),
D. on: reasoning B repeated j times.
gives definition following form:
g + (1 g ) p8(pd1 ) (g + (1 g ) p8(pd1 ) (g + . . .g ))
|{z} |
{z
}|
{z
} |{z}
B
C


defined terms auxiliary function p8(gd , i) defines probability
failure goal g depth remaining relevant plan instances may
(or may not) yield applicable plan instances:
p8(gd ) = p8(gd , j)
p8(gd , 1) = g + (1 g ) p8(pd1 )
p8(gd , + 1) = g + (1 g ) p8(pd1 ) p8(gd , i)
p8(p0 ) = 0
k

p8(pd ) = 1 [(1 ) (1 p8(gd )) ]
94

fiOn Testability BDI Agent Systems


0.05


0.01


2
3
4

2
3
4

failure handling
g = 0
g = 0.01 g = 0.05
30%
33%
43%
72%
76%
86%
98%
99%
100%
g = 0 g = 0.005 g = 0.01
7%
9%
10%
22%
27%
32%
55%
63%
70%

failure handling
g = 0
g = 0.01 g = 0.05
0.64%
2.2%
9.4%
0.81%
2.6%
12.8%
0.86%
2.8%
16.5%
g = 0 g = 0.005 g = 0.01
0.006%
0.5%
1.1%
0.006%
0.6%
1.1%
0.006%
0.6%
1.1%

Table 4: Goal failure probabilities without failure handling goals
applicable plans

Observe setting g = 0 reduces definition derived earlier, since g +(1g ) X
simplifies X, hence p8(gd , i) = p8(pd1 )i .
before, immediately clear formulae actual patterns
probability are. Considering illustrative examples, Table 4 shows (a) overall behaviour before, (b) g assumed relatively low compared
probability action failure ( 0 ), doesnt significantly affect probabilities.
4.6 Analysis Rate Failures
section briefly examine number traces goal-plan tree affected
placing bound rate action failures occur within trace. simplicity,
work uniform goal-plan trees, construction extends trivially semiuniform goal-plan trees.
Figure 6 presented equations calculating total number behaviours
goal-plan tree (with failure handling). many behaviours involve possibly
unrealistic number action failures? make assumption upper
limit rate action failures16 , i.e. number failures divided length
trace, affect number possible behaviours? large numbers
seen reduce significantly?
instance, considering j = k = 2, ` = 1 = 2, 1,922 possible executions
result failure. many involve high rate action failure many
involve small percentage failures? Figure 7 contains (cumulative) counts
generated looking possible executions (small) case, plotted
number action failures. x axis shows given value N {0, . . . , 6} many
traces N fewer action failures. instance, N = 2, 426
traces 2 fewer action failures. 426 traces, 328 successful 98
unsuccessful. Figure 8 shows equivalent graph rate action failure: trace
failure rate computed (the number failures divided length trace),
16. Bounding rate action failures allows us model assumption environment limited
unpredictability, perhaps programmer limited incompetence!

95

fiWinikoff & Cranefield

ok"

failed"

both"

3500"

Number'of'traces'(cumula0ve)'

3000"

2500"

2000"

1500"

1000"

500"

0"

0"

1"

2"

3"

4"

5"

6"

ok"

8"

80"

328"

704"

960"

1024"

1024"

failed"

0"

0"

98"

546"

1282"

1794"

1922"

both"

8"

80"

426"

1250"

2242"

2818"

2946"

Number'of'failures'

Figure 7: Number traces (cumulative) vs. number failures j = k = 2, ` = 1, = 2
3500

3000

Number traces (cumulative)

2500

2000

1500

1000

500

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Failure Rate

Figure 8: Number traces (cumulative) vs. failure rate j = k = 2, ` = 1, = 2

96

fiOn Testability BDI Agent Systems

number traces counted range failure rate. instance, first
data point graph shows 40 traces failure rate 0.1.
question generalise analysis larger execution spaces. Clearly,
counting possible executions feasible. Instead, turn generating functions.
given plan body segment17 (and particularly = gd ), interested
computing numbers successful failed traces failure rate bounded
given ratio r number failed actions total number actions,
i.e. proportion actions execution trace fail. denote n4r(s)
n8r(s). compute values, first determine integers > 0 n 0
numbers successful failed traces length contain exactly n action
failures, denoted n4r(s, m, n) n8r(s, m, n), respectively. define length trace
number actions (both successful unsuccessful) contains. Note
finite goal-plan tree, uniform semi-uniform one, maximum
possible trace length n4r(s, m, n) n8r(s, m, n) non-zero finite
number integer pairs (m, n) positive quadrant plane positive
axis (in case n = 0). values, calculate n4r(s)
n
r, similarly n8rs using n8r(s, m, n).
sum n4r(s, m, n)
begin considering ordinary 18 bivariate generating functions (Wilf, 1994)
values n4r(s, m, n) n8r(s, m, n):
4

Fr (s, x, y) =
Fr8(s, x, y) =

X

X

n4r(s, m, n) xm n

m=0 n=0
X

X

n8r(s, m, n) xm n

m=0 n=0

action one successful execution, length 1 contains action
failures, Fr4(a, x, y) = x (a power series coefficient x1 0 1
coefficients 0). Similarly, Fr8(a, x, y) = x1 1 = xy, one failed execution,
length 1 one action failure.
consider Fr4(s1 ; s2 ):
Fr4(s1 ; s2 , x, y) =
=

X

X

n4r(s1 ; s2 , m, n) xm n

m=0 n=0
X

X
m=0 n=0

X

X


nr(s1 , p, t) nr(s2 , q, u) xm n
4

4

p+q=m t+u=n

double sum parentheses considers, trace, possible ways allocating
number actions number action failures n (necessarily) successful
executions s1 s2 , sums non-negative integer values p, q, u.
17. Recall that, defined towards start Section 4 (page 83), plan body segment sequence
x1 ; . . . ; xn xi either goal action.
18. Ordinary generating functions differ exponential generating functions including denominators
factorials powers variable(s).

97

fiWinikoff & Cranefield

have:
Fr4(s1 ; s2 , x, y) =
=


X
X
X X

n4r(s1 , p, t) xp n4r(s2 , q, u) xq u

m=0 p+q=m n=0 t+u=n
X
X
X

X

n4r(s1 , p, t) xp n4r(s2 , q, u) xq u

p=0 q=0 t=0 u=0

X
X

X

X

4
p
4
q u
=
nr(s1 , p, t) x
nr(s2 , q, u) x
p=0 t=0

q=0 u=0

4

4

= Fr (s1 , x, y) Fr (s2 , x, y)
P
P P
P
second line derived using identity
q
p
m=0
p+q=m f (p, q) =
f (p, q). expressions sum non-negative integers p q, first expression
first summing non-negative values horizontal axis,
summing pairs (p, q) non-negative integers lying line slope 1
intersects horizontal axis m.
Considering Fr8(s1 ; s2 , x, y), have:
Fr8(s1 ; s2 , x, y) =
=

X

X

n8r(s1 ; s2 , m, n) xm n

m=0 n=0
X

X

n8r(s1 , m, n)

m=0 n=0

+

=

X

X



nr(s1 , p, t) nr(s2 , q, u) xm n
4

8

p+q=m t+u=n


XX

n8r(s1 , m, n) xm n

m=0 n=0
X

X

+

m=0 n=0

X

X


nr(s1 , p, t) nr(s2 , q, u) xm n
4

8

p+q=m t+u=n

8

= Fr (s1 , x, y)
X
X

X

X

+
n4r(s1 , p, t) xp
n8r(s2 , q, u) xq u
p=0 t=0

q=0 u=0

= Fr8(s1 , x, y) + Fr4(s1 , x, y) Fr8(s2 , x, y)
second line based observation failed execution s1 ; s2
length n action failures either failed execution s1 length n
action failures occurring execution, successful execution s1 length p
failures followed failed execution s2 length q u failures,
p + q = + u = n.
Now, assuming know Fr4(gd , x, y) Fr8(gd , x, y) depth d,
construct functions Fr4(pd , x, y) Fr8(pd , x, y) applying results expand
right hand sides following equations (which simply replace pd plan body):
98

fiOn Testability BDI Agent Systems

Fr4(pd , x, y) = Fr4(a` ; (gd ; a` )k , x, y)
Fr8(pd , x, y) = Fr8(a` ; (gd ; a` )k , x, y)

remains define Fr4(gd , x, y) Fr8(gd , x, y) terms Fr4(pd1 , x, y) Fr8(pd1 ,
x, y). count successful executions gd length n action failures,
must first choose one j applicable plans one ultimately succeeds.
must choose 0 j1 remaining applicable plans tried
failed, consider possible orderings plans. actions trace
n action failures must distributed across failed successful plans. leads
us following derivation procedure construct Fr4(gd , x, y):
Fr4(gd , x, y)
X

X
=
n4r(gd , m, n) xm n
m=0 n=0

=

X

X


j

m=0 n=0

= j

p=0


j1
X
j 1
p

p=0

= j


j1
X
j 1

p!

p

X

p!

X

n4r(pd1 , `0 , f0 )

p



X

X


m=0 n=0

n8r(pd1 , `i , fi ) xm n

i=1

`0 ++`p =m f0 ++fp =n

X

X



n4r(pd1 , `0 , f0 )

p



n8r(pd1 , `i , fi ) xm n

i=1

`0 ++`p =m f0 ++fp =n

j1
X
p=0

X
X
p
X

X

j 1
4
` f
8
` f
p!
nr(pd1 , `, f ) x
nr(pd1 , `, f ) x
p
`=0 f =0

`=0 f =0


j1
X
j 1
= j
p! Fr4(pd1 , x, y) Fr8(pd1 , x, y)p
p
p=0

Constructing Fr8(gd , x, y) simpler. failed execution goal involves failed attempts
execute j applicable plans. j! orderings plans must considered.
gives us following construction Fr8(gd , x, y):
Fr8(gd , x, y) =
=

X

X

n8r(gd , m, n) xm n

m=0 n=0
X

X

X

j!

m=0 n=0

= j!

X
X


X

`1 ++`j =m f1 ++fj =n
8

` f

nr(pd1 , `, f ) x

`=0 f =0

= j! Fr8(pd1 , x, y)j
99

j



nr(pd1 , `1 , f1 ) nr(pd1 , `j , fj ) xm n
8

8

fiWinikoff & Cranefield

equations define recursive procedure computing Fr4(gd , x, y) Fr8(gd ,
x, y) given values d, j, k `. discussed earlier section, given way
n
calculating n4r(s, m, n), calculate n4r(s) sum n4r(s, m, n)
r,
8
similarly nrs. used Python rmpoly GMPY2 libraries generate
polynomial representations functions Fr4(gd , x, y) Fr8(gd , x, y) specified
values d, j, k l, calculate n4r(s) n8r(s) various ratios r19 . Figure 9
shows results = j = k = 3 ` = 1.
Examining Figure 9 conclude two things. one hand, number traces
really explodes larger rates action failures. example, Figure 9 traces
failure rate greater 0.4. hand, although making assumptions
failure rate reduce number possible traces, number traces still quite
large (note scale y-axis). instance, failure rate 0.1 around
4.8 1044 failed executions 8.7 1047 successful executions. failure rate 0.2
respective numbers 1.0 1077 6.7 1077 , failure rate 0.3
1.2 1096 2.7 1096 .
shape Figure 9 explained follows. Firstly, occurrence action
failure triggers activity (alternative plans), failures result longer traces.
Secondly, longer traces shorter traces, simply
longer trace, possibilities variations (e.g. different orders trying
plans). explains increase Figure 9 starts slowly accelerates:
get failures, longer traces, longer traces
them. word, plot non-cumulative number paths
ratio action failures would see initial increase: ratio grows,
paths. doesnt explain beyond certain point get fewer traces,
cumulative graph levels out. explanation quite simple: beyond certain
ratio (which appears around 0.4) successful traces, number
failed traces also declines.
4.7 Recursive Trees
Section 4.4 developed recurrence relations allowed us relax assumption
goal-plan trees uniform, considered semi-uniform trees. section
relax assumption goal-plan trees finite, also allow trees
shape. considering arbitrary trees allowed contain labels
refer parts tree, i.e. allow trees recursive. derive generating
functions, seen extension derived previous section,
number paths (both successful unsuccessful) executing recursive goalplan trees. Obviously, infinite tree infinite number paths, define
generating functions take parameter bound lengths paths counted.

19. finite number actions attempted execution goal-plan tree,
bounds length possible traces number action failures occur within them.
Thus Fr4(gd , x, y) Fr8(gd , x, y) polynomials finite orderonly finite number coefficients
non-zero infinite sums define them.

100

fiOn Testability BDI Agent Systems

Failed executions (cumulative)

Successful executions (cumulative)

(cumulative)

4E+107

3.5E+107

Number traces (cumulative)

3E+107

2.5E+107

2E+107

1.5E+107

1E+107

5E+106

0.005
0.025
0.045
0.065
0.085
0.105
0.125
0.145
0.165
0.185
0.205
0.225
0.245
0.265
0.285
0.305
0.325
0.345
0.365
0.385
0.405
0.425
0.445
0.465
0.485
0.505
0.525
0.545
0.565
0.585
0.605
0.625
0.645
0.665
0.685
0.705
0.725
0.745
0.765
0.785
0.805
0.825
0.845
0.865
0.885
0.905
0.925
0.945
0.965
0.985

0

Failure rate

Figure 9: Number traces (cumulative) vs. failure rate j = k = = 3 ` = 1
given upper bound path length equations specify number paths
many actions20 .
begin defining notation representing recursive trees: goals, plan-body
multisets, plans, variables bindings. goal represented term form
goal(plan-body-multiset) plan-body-multiset multiset representing different
applicable plan instances used satisfy goal. multiset
combinatorial analysis, structure plans significant. Therefore
use single abstract action represent actions21 , goal may achievable
using multiple plan instances structure, must treat
distinct. need represent bodies plan instances, element
multiset (i.e. plan) sequence terms separated right-associative sequential
composition operator ;. term sequence either abstract action term a,
goal term defined (representing sub-goal), label (see below). Formally,
plan-body multiset P multiset plans, written {p1 :c1 , . . . , pj :cj }
ci number times associated plan pi appears multiset.
define following multiset operations: set(P ) set pi multiset P ,
P (pi ) characteristic function denoting number times plan pi appears
multiset (i.e. ci ); P M1 P M2 multiset subtraction, defined P M1 P M2 (x) =
max(P M1 (x) P M2 (x), 0). Finally |P | size multiset, i.e. sum ci .

20. also use equations derived section non-recursive trees, case allow
= , define 1 = F power(x) = F .
21. However, avoid confusion, use numeric subscripts (a1 , a2 , . . .) distinguish different occurrences actions.

101

fiWinikoff & Cranefield

order allow recursive trees represented, possible step plan
label (denoted , 0 ) referring term provided binding, simply
mapping labels terms (either goal plan terms). b binding write
b[] denote item mapped b, entry b.
example, consider simple tree below, consisting goal two plans, together
binding maps variable root tree. first plan (on left)
two steps: action (a1 ), recursive reference root tree ().
second plan (on right) single action (a2 ).
: goal
plan

plan
a1



a2

recursive tree represented follows. define binding b = { 7
goal({(a1 ; ):1, a2 :1})} maps whole tree, tree .
proceed defining generating functions, introduce auxiliary notation. P power series use standard notation [xp11 xpnn ]P denote
coefficient term xp11 xpnn series. define P cond denote power
xn

series containing terms P satisfy condition cond. define f g
(f g) power(x)n , i.e. f g terms power x greater n removed.
n
define f mx (f )m power(x)n , i.e. (f )m terms power x greater
n removed.22
position derive generating functions specify number
paths arbitrary, possible recursive, goal-plan tree, given bound
path length. define BDI program represented term (i.e. goal, plan,
plan multiset, action, label), b binding mapping labels terms (as defined
above). define n4(s, m, n, b) number successful paths, respect
binding b, actions, n failed actions. Similarly define
n8(s, m, n, b) number failed paths, respect b, actions,
n failed actions. want derive recurrence relations generating
functions23 :
4
F
(s, x, y, b, ) =

8
F
(s, x, y, b, ) =

X

X
m=0 n=0
X

X

n4(s, m, n, b)xm n
n8(s, m, n, b)xm n

m=0 n=0

upper bound number actions path.
22. This, previous operation, directly supported rmpoly Python library multivariate
polynomials series, used compute generating functions.
23. subscript used distinguish generating functions, allow recursive tree,
generating functions defined elsewhere paper.

102

fiOn Testability BDI Agent Systems

order simplify presentation, details complex derivations
given Appendix C. resulting equations shown Figure 10. first two
equations (Equations 11 12 Figure 10), applicable term t, capture
assumption > 0 (and remaining equations apply > 0). next
two equations simply specify labels looked provided binding. Equation 15
indicates single successful path action a, single
action unsuccessful actions (i.e. generating function 1x1 0 ). Equation 16
similarly indicates single unsuccessful path single action a, which,
unsurprisingly, single unsuccessful action (so generating function 1x1 1 ).
Equations 17 18 deal sequences: sequence s1 ; s2 succeed s1
s2 must succeed, count paths concatenating sub-paths, corresponds
multiplying power series. sequence s1 ; s2 fail either s1 fails, s1 succeeds
s2 fails (alternatives correspond addition power series). equations
special case: s1 action, divide overall path-length limit
precisely: s1 must trace length 1 (since action) s2 must therefore
maximum length 1.
dealt labels (), single actions, sequences, next turn goals (equations 19 20). cases derivation complex, covered Appendix C.
4
F
(Equation 19 Appendix C.1), intuition successful path
goals execution involves single successful plan p, number failed executions
plan selected remaining multiset plans (P {p:1}). case plan
appears multiset, select occurrences, hence
multiplication P (p). number failed paths goal (Equation 20
Appendix C.2) introduce auxiliary generating function G8(P M, x, y, z, b, ),
P multiset plans, z variable whose power z indicates exact number plans P used. words, given power series denoted
G8(P M, x, y, z, b, ), term cmno xm n z /o! indicates cmno paths involve actions, n failed, exactly plans P . generating
8
function G8 technical device allows us derive definition F
need.
8
Given power series, definition F simply selects terms |P |
power z (since plans must fail goal fail) using removes
z |P | terms dividing. G8 exponential generating function z,
means includes division factorial, need multiply factorial |P |!
remove it.
8
Equation 21 defines F
(P M, x, y, b, ), used Equation 19, terms
8
auxiliary function G. derivation given Appendix C.3. intuition
possible number plans could used (o) limit power series G8
value o, remove z dividing. o! due G8 exponential
generating function z (see Appendix C).

Finally, Equations 22 23 give definition G8(P M, x, y, z, b, ) (see Appendix C.4
derivation). Intuitively, Equation 22 creates power series plan type,
x

combines (using ). Equation 23 little complex: single way
failing (when plans used, corresponding term x0 0 z 0 = 1). Otherwise
select c plans, plans must fail (corresponding term
103

fiWinikoff & Cranefield

4
F
(t, x, y, b, ) = 0 0

(11)

8

F(t, x, y, b, ) = 0 0

(12)

4

4

(13)

8

8

(14)

F(, x, y, b, ) = F(b[], x, y, b, )
F(, x, y, b, ) = F(b[], x, y, b, )
4

(15)

8

(16)

F(a, x, y, b, ) = x
F(a, x, y, b, ) = xy
4

F(s1 ; s2 , x, y, b, )
(
4
4
F
(s1 , x, y, b, 1) F
(s2 , x, y, b, 1) s1 action
=
x
4
4
F(s1 , x, y, b, ) F
(s2 , x, y, b, ) otherwise
8
F
(s1 ; s2 , x, y, b, )
(
4
8
8
(s1 , x, y, b, 1)+F
(s1 , x, y, b, 1) F
(s2 , x, y, b, 1) s1 action
F
=
x
8
4
8
F
(s1 , x, y, b, )+F
(s1 , x, y, b, ) F
(s2 , x, y, b, ) otherwise
4
F
(goal(P ), x, y, b, )
X
x
4
8
=
P (p)F
(p, x, y, b, ) F
(P {p:1}, x, y, b, )

(17)

(18)

(19)

pset(P )
8
F
(goal(P ), x, y, b, ) = |P |!

|P |
8
F
(P M, x, y, b, ) =

X
o=0

o!

G8(P M, x, y, z, b, ) power(z)=|P |
z |P |
G8(P M, x, y, z, b, ) power(z)=o
zo

(20)
(21)

G8({p1 :c1 , . . . , pj :cj }, x, y, z, b, )
x

x

G8({p1 :c1 }, x, y, z, b, ) G8({pj :cj }, x, y, z, b, )

=

c
X
c
G({p:c}, x, y, z, b, ) = 1 +
F 8 (p, x, y, b, )ox z

8

(22)

(23)

o=1

Figure 10: Equations Recursive Goal-Plan Trees
8
F
(p, x, y, b, )), giving number failed traces across plans as:
x

x

8
8
8
F
(p, x, y, b, )ox = F
(p, x, y, b, ) F
(p, x, y, b, )
|
{z
}

times

used Python rmpoly GMPY2 libraries generate polynomial repre4
8
sentations functions F
(t, x, y, b, ) F
(t, x, y, b, ) (as defined Figure 10)
specified values t, b, . defined simple tree (the one given earlier
section example) computed number paths different values .
104

fiOn Testability BDI Agent Systems

values chosen correspond values Table 2 (which
values n4(g) n8(g) come from24 ). Table 2, values 62 363 correspond
longest path, argue comparing recursive tree uniform tree,
consider path length limit. results shown Table 5.

62
363

n4(g)
6.33 1012
1.02 10107

n8(g)
1.82 1013
2.56 10107

n4(s)
3.8 1013
1.9 1076

n8(s)
4.3 109
6.1 1054

Table 5: Comparing n4 n4 (respectively n8 n8).
Looking numbers Table 5, worth noting recursive tree
used extremely simple: two plans, single action. low number
actions (and sparseness tree) account relatively low number unsuccessful
paths. instance, modify tree adding extra actions (giving tree
binding below) = 62 around 3.9 1013 successful paths, 1.5 1011
unsuccessful paths. Unfortunately, Python unable calculate n4 n8 tree
= 363, manage = 362, 1.26 1064 successful paths,
3.281063 unsuccessful paths. shows, expected, number unsuccessful
paths higher complex tree. fewer successful paths
complex tree explained observing that, tree, traces longer (more
actions need done), traces excluded bound trace
length .
: goal
plan

plan
a1

a3



a2

a4

Overall, analysis section, application = 62 363 confirms
number paths recursive tree depends trees structure (which
unsurprising), also indicates even simple recursive tree, number
paths given upper bound path length quickly becomes extremely large.

5. Reality Check
previous section analysed abstract model BDI execution order determine
size behaviour space. analysis yielded information size
behaviour space affected various factors, probability goal
failing.
section consider two issues whether analysis faithful, whether
applicable real systems. analysis made number simplifying assumptions,
24. correspond first two rows table, respectively involve 62 363 actions.

105

fiWinikoff & Cranefield

mean results may faithful semantics real BDI platform,
may apply real systems. thus conduct two reality checks assess
whether analysis faithful (Section 5.1) whether applicable (Section 5.2).
firstly assess whether analysis faithful real BDI platforms, i.e.
omit significant features, contain errors. comparing abstract
BDI execution model results real BDI platform, namely JACK (Busetta et al.,
1999). comparison allows us assess extent analysis abstract BDI
execution model matches execution takes place real (industrial strength) BDI
platform. comparison is, essence, basic reality check: simply checking
analysis previous section indeed match execution semantics typical
BDI platform. modelling artificial goal-plan tree BDI platform.
Next, order assess extent analysis results apply real systems,
analyse goal-plan tree real industrial application. analysis allows us
determine extent conclusions analysis uniform (and semi-uniform)
goal-plan trees applies real applications, goal-plan trees likely
uniform. words, extent large numbers Tables 1 2 apply
real applications?
5.1 Real Platform
order compare real BDI platforms execution results abstract BDI
execution model implemented two goal-plan trees Appendix JACK agent
programming language25 . structure plans events26 precisely mirrors
structure tree. goal-plan tree, event two relevant plans,
always applicable, selectable either order. Actions implemented
using code printed action name, then, depending condition (described
below), either continued execution triggered failure (and printed failure indicator):
System.out.print("a"); // Action "a"
((N.i & 1)==0) {
System.out.print("x");
false; // trigger failure
}
conditions determined whether action failed succeeded, plan
selected first, controlled input (N.i, Java class variable). test harness
systematically generated inputs, thus forcing decision options explored.
results matched computed Prolog code Figure 3, giving precisely
six traces smaller tree, 162 traces larger tree.
indicates abstract BDI execution model indeed accurate description
takes place real BDI platform (specifically JACK).
Note selected JACK two reasons. One modern, well known,
industry-strength BDI platform. other, important, reason, JACK descendent line BDI platforms going back PRS, thus good representative
25. code available upon request authors.
26. JACK models goal BDIGoalEvent.

106

fiOn Testability BDI Agent Systems

Parameters
Number
j k

goals
actions
2 2
3
21
62 (13)
3 3
3
91 363 (25)
Workflow 57 goals(*)
(*) paper says 60 goals,
Figure 11 57 goals.

failure handling
(secs 4.1 4.2)
n4(g)
n8(g)
128
614
1,594,323 6,337,425
294,912 3,250,604
294,912 1,625,302
294,912
812,651

failure handling
(Section 4.3)
n4(g)
n8(g)
6.33 1012
1.82 1013
107
1.02 10
2.56 10107
2.98 1020
9.69 1020
15
6.28 10
8.96 1015
9.66 1011
6.27 1011

Table 6: Illustrative values n4(g) n8(g) (bottom part ` = 4 first row, ` = 2
second, ` = 1 last row)

larger family BDI platforms. words, showing BDI execution model
analysed matches JACKs model, also able argue matches execution
JACKs predecessors (including PRS dMARS), close relatives (e.g. UM-PRS
JAM).
5.2 Real Application
consider extent real systems deep branching goal-plan trees,
extent large numbers shown Tables 1 2 apply real applications,
rather uniform goal-plan trees. example real application consider
industrial application Daimler used BDI agents realise agile business processes
(Burmeister, Arnold, Copaciu, & Rimassa, 2008). Note finding suitable application
somewhat challenging: need application real (not toy system). However,
order able analyse it, application BDI-based, furthermore,
details applications goal-plan tree need available. Unfortunately, many
reported BDI-based industrial applications provide sufficient details
internals allow analysis carried out.
Figure 11 shows27 goal-plan tree work Burmeister et al. (2008)
60 achieve goals 7 levels. 10 maintain goals, 85 plans 100 context
variables (Burmeister et al., 2008, p. 41). Unlike typical goal-plan trees used BDI
platforms, tree Figure 11 consists layers and-refined goals,
refinements leaves (where plans are). terms analysis presented
paper treat link goal g set goals, say, g1 , g2 , g3
equivalent goal g single plan p performs g1 , g2 , g3 (and actions,
i.e. ` = 0 non-leaf plans).
last row Table 6 gives various n values goal-plan tree, ` = 4 (top
row), ` = 2 (middle row) ` = 1 (bottom row). Note figures actually
lower bounds assumed plans depth 0 simple linear combinations
` actions, whereas clear Burmeister et al. (2008) plans fact
27. details meant legible: structure matters.

107

fiWinikoff & Cranefield

model
LS/AB
differe
model
keep th

figuretree6:from
goal
ACM
prototype
Figure 11: Goal-plan
thehierarchy
work Burmeister
et al. (2008,
Figure 6) (reproduced
permission IFAAMAS)

advantage modeling approach implicitly offers
support parallel execution process parts
depend other. reduce overall time needed
complicated, contain nested decision making (e.g., see Burmeister et al., 2008,
process
execution. Moreover maintain goals good means
Figure 4).
provide
process


agent
monitors
roughthe
indication
size
aadditional
goal-plan tree isagility:
number
goals.

57 goals,
tree Figure
sizeto


firstthroughout
two rows Table
Comparing(e.g.

conditions
that11
fulfilled
the6. process
number possible behaviours uniform goal-plan trees real (and nontime
constraints)

pro-actively
activities
avoid
uniform)
goal-plan tree,
see
behaviour initiates
space somewhat
smaller

real
tree, thatbefore
stillthey
quite appear.
large, especially case failure handling. However,
problems
note following points:

development prototype support rapid
1. tree Figure 11 plans leaves, reduces complexity.
prototyping
execution process models provided
words goal-plan tree typical plans alternating
goals would

larger number
possible
behaviours.
LS/ABPM

proven
ofvery
helpful.
developed models
represent
living process models, directly executed
2. figures tree conservative estimate, since assume leaf plans
visualized.

part ofIn
interface
thatcalculated
coupled
simple
behaviour.
otherweb
words,user
number
paths

theis
actual
number directly
paths thefrom
real application.
under-estimate
workflow
generated
process model.

interface computed directly parameters
6. Comparison Procedural Programs
corresponding task: context variables, types possible
order argue BDI programs harder test non-agent programs, need
values.

approach
ofprocess
programs,
quickly
comparison.
Specifically,
need changes
analyze number
paths non-agent
compareand
tested.
agent
programs.
us address
concern
modeled
Thus
errorsThis

theallow
models
bethediscovered
paths criterion test suite adequacy always requires infeasibly large number

corrected
briefly
short
tests.
section
doestime.
this, analyzing number paths procedural
program.

stated starting point building ACM-prototype
model ACM-reference 108
process model developed
software demonstrator. underlying agent engine
demonstrator (JadeX) partially different modeling
execution semantics compared LS/ABPM tool.


model
prototy

compl
depend
challen

execut
depend
proces

Based
con

contex
manip
model
plans,

compl
possib
variab
variab
one
proces
variab
startin
goals.
create
h
model

fiOn Testability BDI Agent Systems

Number actions / statements
62
363
776
627

n(m)
6,973,568,802
5.39 1057
2.5 10123
5.23 1099

n4(g)
6.33 1012
1.02 10107
1.82 10157
3.13 10184

n8(g)
1.82 1013
2.56 10107
7.23 10157
7.82 10184

Table 7: Comparison values n(m), n4(g) n8(g).
define program composed primitive statements s, sequences statements P1 ; P2 , conditionals select two sub-programs. Since capture
conditions statements, elide condition, write conditional P1 + P2 indicating one Pi selected. Note that, BDI analysis, exclude loops.
define number paths program P n(P ). straightforward28 see
definition n(P ) is:
n(s) = 1
n(P1 ; P2 ) = n(P1 ) n(P2 )
n(P1 + P2 ) = n(P1 ) + n(P2 )
order compare BDI programs, consider size program,
compare programs size. key question is: procedural program
nodes significantly fewer paths BDI program size? define
size program P number primitive statements contains, denote
|P |. Note means count internal nodes syntax tree
(i.e. + ;). Therefore, comparing BDI programs, consider size
BDI program number actions29 .
work number paths varies size program P .
size program (and therefore natural number), define n(m) max{n(P ) :
|P | = m}. is, n(m) largest number paths possible program size m.
Appendix contains derivation n(m), resulting following definition (where
6 multiple 3):
n(1)
n(3)
n(5)
n(m + 1)

=
=
=
=

1
3
6
4
3

3m/3

n(2)
n(4)
n(m)
n(m + 2)

=
=
=
=

2
4
3m/3
2 3m/3

Table 7 shows comparison values n(m) n4(g) n8(g), same-sized
programs, based Table 2. worth emphasising n(m) highest possible value:
defined maximum possible programs. However, maximal program
highly atypical. example, considering programs seven statements,
28. path P1 ; P2 simply concatenates path P1 path P2 , hence product; path
P1 + P2 either path P1 path P2 , hence addition.
29. Using total number nodes tree yields almost identical results.

109

fiWinikoff & Cranefield

total 8,448 possible programs. 8448 programs, 32 12 paths (the
maximum). Figure 12 shows number paths (112) many programs
many paths. maximum 12 clearly typical: indeed, mean number paths
seven statement program 4.379, median 4. consider programs
9 statements, 366,080 programs, 16 maximal
number paths (which 27). average number paths across programs
5.95.
Overall, looking Table 7, conclude number paths BDI programs
much larger even (atypical) maximal number paths procedural program
size. supports conclusion BDI programs harder test
procedural programs.
2500"

Number'of'programs'

2000"

1500"

1000"

500"

0"
1"

2"

3"

4"
5"
6"
7"
8"
9"
Number'of'paths'in'a'procedural'program'

10"

11"

12"

Figure 12: Profile number paths 7-statement programs

7. Conclusion
summarise, analysis found space possible behaviours BDI agents is,
indeed, large, absolute sense, relative sense (compared procedural
programs size).
expected, number possible behaviours grows trees depth (d) breadth
(j k) grow. However, somewhat surprisingly, introduction failure handling makes
significant difference number behaviours. instance, uniform goalplan tree depth 3 j = k = 2, adding failure handling took number successful
behaviours 128 6,332,669,231,104.
consider negative consequences analysis, worth highlighting
one positive consequence: analysis provides quantitative support long-held belief
110

fiOn Testability BDI Agent Systems

BDI agents allow definition highly flexible robust agents. Flexibility
defined number possible behaviours agent, shown large.
Robustness defined ability agent recover failure. analysis
Section 4.6 showed BDI failure recovery mechanism effective achieving low
rate actual failure (< 1%), even action reasonable chance failing (5%).
analysis paper tell us testability BDI agent systems?
answer question, need consider tested. Testing
typically carried levels individual components (unit testing), collections
components (integration testing), system whole.
Consider testing whole system. behaviour space sizes depicted Tables 1, 2
6 suggest quite strongly attempting obtain assurance systems correctness
testing system whole feasible. reason (as discussed
Section 1.1), adequate test suite (using paths criterion adequacy) requires
least many tests paths program tested. program has, say,
1013 paths, even test suite tens thousands tests inadequate,
hugely inadequate, since covers tiny fraction percent number
paths.
fact, situation even worse consider number possible
executions also probability failing: space unsuccessful executions particularly hard test, since many unsuccessful executions (more successful ones),
probability unsuccessful execution low, making part behaviour
space hard reach. Furthermore, shown Section 4.6, although making assumptions
possible numbers action failures occur given execution reduces
number possible behaviours, still many many behaviours, even relatively
small trees (e.g. j = k = = 3).
system testing BDI agents seems impractical. unit testing
integration testing? Unfortunately, always clear apply usefully
agent systems interesting behaviour complex possibly emergent.
example, given ant colony optimisation system (Dorigo & Stutzle, 2004), testing single
ant doesnt provide much useful information correct functioning whole
system. Similarly, BDI agents, testing sub-goal difficult ensure
testing covers situations goal may attempted. Consequently,
difficult draw conclusions correctness goal results testing
sub-goals.
need acknowledge analysis somewhat pessimistic: real BDI systems
necessarily deep heavily branching goal-plan trees. Indeed, tree
real application described Section 5 smaller behaviour space abstract
goal-plan trees analysed Section 4. However, even though smaller, still quite large,
cause problems validation:

One big challenges test phase keep model consistent
define right context conditions result correct execution
scenarios. Therefore support dependency analysis, automated
111

fiWinikoff & Cranefield

simulation testing process models needed (Burmeister et al., 2008,
p. 42)30 .
leave us respect testing agent systems? conclusion
seems testing whole BDI system feasible. number possible
approaches dealing issue testability could recommended:
Keep BDI goal-plan trees shallow sparse. keeps number behaviours small. issue approach lose benefits BDI
approach: reasonably large number behaviours desirable provides
flexibility robustness.
Avoid failure handling. Since failure handling large contributor behaviour space, could modify agent languages disable failure handling. Again,
useful approach disabling failure handling removes benefits
approach, specifically ability recover failures.
Make testing sophisticated. Could testing coverage perhaps improved
incorporating additional information domain knowledge, detailed
model environment (which indicates possible failure modes probabilities)? answer known, potentially interesting area
work. However, large number paths encourage much optimism
approach.
Another, related, direction see whether patterns exist behaviour space.
Since failure recovery mechanism certain structure, may
results behaviour space large, but, sense, structured.
structure exists, may useful making agents testable. However,
point time, research direction may may turn fruitful;
viable testing strategy.
Finally, related direction try intelligent selection
test cases, order gain coverage given number test cases. One
approach this, recently described, evolutionary testing
(Nguyen, Miles, Perini, Tonella, Harman, & Luck, 2009a), genetic evolution
used find good (i.e. challenging) test cases.
Supplement testing alternative means assurance. Since testing
able cover large behaviour space, consider forms assurance.
promising candidate form formal method31 . Unfortunately, formal
methods techniques yet applicable industry-sized agent systems (we return
below, Section 7.1).
30. Burmeister et al. made following observation: approach changes process
quickly modeled tested. Thus errors models discovered corrected short time.
discussing advantages executable models, arguing able execute
model allowed testing, useful detecting errors model. able execute
model undoubtedly useful, evidence given (nor specific claim made) testing
sufficient assuring correctness agent system.
31. See volume edited Dastani et al. (2010) recent overview current state-of-the-art,
including chapter role formal methods assurance agents (Winikoff, 2010).

112

fiOn Testability BDI Agent Systems

Proceed caution. Accept BDI agent systems general robust (due
failure-handling mechanisms), is, present, practical way
assuring behave appropriately possible situations. worth
noting humans similar respect. Whilst train, examine
certify human certain role (e.g. pilot surgeon), way assuring
he/she behave appropriately situations. Consequently, situations
incorrect behaviour may dire consequences, surrounding system needs
safety precautions built (e.g. process double-checks information,
backup system co-pilot).
7.1 Future Work
room extending analysis Section 4. Firstly, analysis single
goal within single agent. Multiple agents collaborating achieve single highlevel goal viewed shared goal-plan tree certain goals and/or plans
allocated certain agents. course, distributed goal plan tree
concurrency. concurrency introduced, would useful consider whether
certain interleavings concurrent goals fact equivalent. Furthermore,
considered achievement goals. would interesting consider types goals (van
Riemsdijk, Dastani, & Winikoff, 2008). Secondly, analysis focused BDI agents,
one particular type agent. would interesting consider sorts
agent systems, and, broadly, sorts adaptive systems.
Another extension analysis consider criteria test suite adequacy.
paper used paths criterion, arguing appropriate.
recognize paths actually quite strong criterionit subsumes many
criteria (Zhu et al., 1997, Figure 7). alternative criterion could consider
edges, also known branch coverage decision coverage. requires
choice program, statement, tests test suite
exercise options, i.e. edges program graph covered. edges
criterion weaker paths regarded generally accepted minimum
(Jorgensen, 2002).
Another area refinement analysis make less abstract. Two specific areas
could made detailed resources environment. analysis
consider resources environment directly, instead, considers actions may
fail range reasons might include resource issues, environmental issues.
analysis could extended explicitly consider resources interaction goals
(Thangarajah, Winikoff, Padgham, & Fischer, 2002). could also extended
explicit model environment.
Whilst analysis consider real application, would desirable consider
range applications. could provide additional evidence analysis unduly
pessimistic, would also lead understanding variance goal-plan trees
characteristics across applications. key challenge finding suitable applications
BDI-based, sufficiently complex (ideally real applications), detailed design
information available (and preferably source code). Another challenge methodology:
analysed shape goal-plan tree Daimler workflow application,
113

fiWinikoff & Cranefield

access run system. alternative methodology, requires access
implemented system probably source code, run it, force generate
traces sub-goals32 (which would require modification either source code
underlying agent platform). collected data shape real-world
industrial applications, able analyse whether uniform semi-uniform goalplan trees good models types system, whether seek ways
relax uniformity assumption.
importantly, highlighted difficulties assuring BDI agent systems
testing, need find ways assuring systems.
approach promise automatic generation test cases agent
systems (Nguyen, Perini, & Tonella, 2007; Zhang, Thangarajah, & Padgham, 2009). However, size behaviour space suggests number test cases needed may
large, testing failed plan execution difficult. One interesting,
potentially promising, avenue use formal techniques help guide test generation
process (e.g. symbolic execution specification-guided testing) (Dwyer, Hatcliff, Pasareanu, Robby, & Visser, 2007).
Another approach33 attracted interest model checking agent systems
(Wooldridge, Fisher, Huget, & Parsons, 2002; Bordini, Fisher, Pardavila, & Wooldridge,
2003; Raimondi & Lomuscio, 2007). work promising model checking techniques use range abstractions cover large search space without deal
individual cases one-at-a-time (Burch, Clarke, McMillan, Dill, & Hwang, 1992; Fix,
Grumberg, Heyman, Heyman, & Schuster, 2005). Furthermore, verifying subgoal considers possibilities, possible combine verification different sub-goals.
However, work needed: Raimondi Lomuscio (2007) verify systems agents
defined abstractly, i.e. terms plans goals. MABLE agent programming
language (Wooldridge et al., 2002) actually imperative language augmented certain agent features, BDI language; work Bordini et al. (2003)
include failure handling. general, state art model checking agent system
implementations still limited quite small systems (Dennis, Fisher, Webster, & Bordini,
2012).
Acknowledgements
would like thank members Department Information Science University
Otago discussions relating paper. would also like thank Lin Padgham
comments draft paper. Finally, would like thank anonymous
reviewers insightful comments helped improve paper.
work paper done Winikoff sabbatical RMIT, visiting
University Otago.

32. Generating traces top level goal likely feasible.
33. also work deductive verification, (based research verification
concurrent systems) appears less likely result verification tools (relatively)
easy use applicable real systems.

114

fiOn Testability BDI Agent Systems

Appendix A. Example Goal-Plan Trees Expansions
Suppose following two trees: sample (left) sample2 (right). trees
correspond j = 2, k = ` = 1, = 1 sample = 2 sample2.
goal
plan


goal

plan

goal

b c

goal



plan

plan

plan

plan

plan

plan



b

e

f

g

h

trees expanded respectively following sequences actions,
letter indicates execution action, 8 indicates failure34 . predicted
formulae, four successful executions two unsuccessful executions
first tree:

b

a8b
b8a

a8b8
b8a8

second tree, expansions following 162 possibilities (consisting 64
successful 98 unsuccessful traces).





















e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e

b
b8cgd
b8cgd8
b8cg8hd
b8cg8hd8
b8cg8h8
b8chd
b8chd8
b8ch8gd
b8ch8gd8
b8ch8g8
b8c8
8fb
8fb8cgd
8fb8cgd8
8fb8cg8hd
8fb8cg8hd8
8fb8cg8h8
8fb8chd
8fb8chd8






















f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f

b8chd
b8chd8
b8ch8gd
b8ch8gd8
b8ch8g8
b8c8
8eb
8eb8cgd
8eb8cgd8
8eb8cg8hd
8eb8cg8hd8
8eb8cg8h8
8eb8chd
8eb8chd8
8eb8ch8gd
8eb8ch8gd8
8eb8ch8g8
8eb8c8
8e8cgd
8e8cgd8

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c

g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g

d8
d8
d8
d8
d8
d8
d8
d8
d8
d8
d8
8h
8h
8h
8h
8h
8h
8h
8h
8h

aeb
aeb8
ae8fb
ae8fb8
ae8f8
afb
afb8
af8eb
af8eb8
af8e8
a8

d8aeb
d8aeb8
d8ae8fb
d8ae8fb8
d8ae8f8
d8afb
d8afb8
d8af8eb

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c

h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h

d8afb8
d8af8eb
d8af8eb8
d8af8e8
d8a8
8gd
8gd8aeb
8gd8aeb8
8gd8ae8fb
8gd8ae8fb8
8gd8ae8f8
8gd8afb
8gd8afb8
8gd8af8eb
8gd8af8eb8
8gd8af8e8
8gd8a8
8g8aeb
8g8aeb8
8g8ae8fb

34. Note failure marker isnt counted considering length trace Section 4.6.

115

fiWinikoff & Cranefield























e8fb8ch8gd
e8fb8ch8gd8
e8fb8ch8g8
e8fb8c8
e8f8cgd
e8f8cgd8
e8f8cg8hd
e8f8cg8hd8
e8f8cg8h8
e8f8chd
e8f8chd8
e8f8ch8gd
e8f8ch8gd8
e8f8ch8g8
e8f8c8
fb
fb8cgd
fb8cgd8
fb8cg8hd
fb8cg8hd8
fb8cg8h8

af8e8cg8hd
af8e8cg8hd8
af8e8cg8h8
af8e8chd
af8e8chd8
af8e8ch8gd
af8e8ch8gd8
af8e8ch8g8
af8e8c8
a8cgd
a8cgd8
a8cg8hd
a8cg8hd8
a8cg8h8
a8chd
a8chd8
a8ch8gd
a8ch8gd8
a8ch8g8
a8c8
cgd

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c

g8hd8af8eb8
g8hd8af8e8
g8hd8a8
g8h8aeb
g8h8aeb8
g8h8ae8fb
g8h8ae8fb8
g8h8ae8f8
g8h8afb
g8h8afb8
g8h8af8eb
g8h8af8eb8
g8h8af8e8
g8h8a8
hd
hd8aeb
hd8aeb8
hd8ae8fb
hd8ae8fb8
hd8ae8f8
hd8afb

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c

h8g8ae8fb8
h8g8ae8f8
h8g8afb
h8g8afb8
h8g8af8eb
h8g8af8eb8
h8g8af8e8
h8g8a8
8aeb
8aeb8
8ae8fb
8ae8fb8
8ae8f8
8afb
8afb8
8af8eb
8af8eb8
8af8e8
8a8

Appendix B. Analysis Recurrence Relations
appendix contains details derivation Section 4.4.
exponential generating function F (x) sequence {f 4 (j, a, b)}
j=0 function
defined following power series:
F (x) =


X

f 4 (j, a, b)

j=0

xj
j!

(24)

(by definition f 4 )
!
!
j1



j
X
X
X
X
x
xj
j
j
=
i!ai (j i)b
=
i!ai (j i)b

j!

j!
j=0

i=0

j=0

i=0

right hand side
changed upper limit inner sum based
j
generalised definition j(j 1)(j 2) . . . (j + 1)/i!, valid

complex numbers j non-zero integers (Wilf, 1994) gives ji = 0 > j.
right hand side form product exponential generating functions (Wilf,
1994, Rule 30 , Section 2.3):



!




j
j
X
X
X
X
x
x
j
xj

(j)
(j) =
(i)(j i)
j!
j!

j!
j=0

j=0

j=0

i=0

where, case, (j) = j! aj (j) = j b. Therefore, write:





j
j
X
X
(ax)
x
F (x) =
j!
jb
j!
j!
j=0

j=0

116

fiOn Testability BDI Agent Systems

P
1
left hand sum G(ax) G(y) = n n = 1y
(Wilf, 1994, Equation 2.5.1)35 .

P
n
x
x

0
right hand sum equal bx dx
n! (Wilf, 1994, Rule 2 , Section 2.3) = bx dx e
x
(Wilf, 1994, Equation 2.5.3) = bxe . Thus have:
F (x) =

1
bxex
bxex =
1 ax
1 ax

P
xj
4
Therefore, f 4 (0, a, b) constant term power series
j=0 f (j, a, b) j! ,
F (0) = 0. find recurrence relation defining f 4 (j + 1, a, b) equate original
definition F (x) Equation 24 closed form function, differentiate
side (to give us power series f 4 (j, a, b) values shifted one position left),
multiply denominator closed form, giving us following derivation.





X 4

bxex
xj
(1 ax)
= (1 ax)
f (j, a, b)
dx
j!
dx 1 ax
j=0



X
xj1
b(x+1)ex
abxex
4
= (1 ax)
f (j, a, b)j
= (1 ax)
+
j!
1 ax
(1 ax)2
j=0

=


X
j=0



bxex
xj1 X 4
xj

= b(x+1)ex +
f (j, a, b)j
af (j, a, b)j
j!
j!
1 ax
4

j=0

4

(recall f (0, a, b) = 0)


X
X
xj
xj
4
=
f (j +1, a, b)(j +1)

ajf 4 (j, a, b)
(j +1)!
j!
j=0

j=0

= bxex + bex +


X

f 4 (j, a, b)

j=0

(recall

bxex

=

j=0

Equating coefficients

xj
j!

xj
j=0 jb j! ,

j
X

ex =

j=0

j=0

P


X
xj
=b
j +b
j!

xj
j!
xj
j=0 j! )

P



X
xj
x
+a
f 4 (j, a, b)
j!
j!

get:

f 4 (j +1, a, b) ajf 4 (j, a, b) = bj + b + af 4 (j, a, b)
= f 4 (j +1, a, b) = b(j +1) + af 4 (j, a, b) + ajf 4 (j, a, b)
= (j +1)(b + af 4 (j, a, b))

(25)

35. Note many operations performed generating functions (and used paper)
valid without concern convergence series. combinatorics, generating functions often
treated analytic functions evaluated specific variable values, rather formal (possibly
infinite) algebraic objects, well defined operations addition multiplication.
set formal power series finite set variables structure ring abstract algebra,
ring notion function convergence evaluation (Wilf, 1994, ch. 2).

117

fiWinikoff & Cranefield

Appendix C. Analysis Recursive Goal-Plan Trees
appendix contains detailed derivations relating Section 4.7.
4
C.1 Derivation F
(goal(P ), x, y, b, )
4
define F
(goal(P ), x, y, b, ) terms n4 usual way, noting upper
bound realise length bound:

4
F
(goal(P ), x, y, b, ) =

X

X

n4(goal(P ), m, n, b)xm n

m=0 n=0
4

n(goal(P ), m, n, b) defined Section 4.7. also make use nonbounded version (which four arguments):
4
F
(goal(P ), x, y, b) =

X

X

n4(goal(P ), m, n, b)xm n

m=0 n=0
4

define n counting successful traces:
n4(goal(P ), m, n, b)
X
=
P (p)

X

n4(p, m1 , n1 , b) n8(P {p:1}, m2 , n2 , b)

m1 +m2 =m
n1 +n2 =n

pset(P )

n8(P M, m, n, b) number unsuccessful paths using zero plans
plan multiset P (with respect binding b) actions, n
failed actions.
inner sum considers ways partition numbers actions (m) action
failures (n) caused single plan shape p P
P
caused


plans.


Section
4.6
(page
98)

use

identity
m=0
p+q=m f (p, q) =
P P
p=0
q=0 f (p, q) rewrite:
4
F
(goal(P ), x, y, b)
X

X
X
X
=
P (p)
n4(p, m1 , n1 , b) n8(P {p:1}, m2 , n2 , b)xm n

m1 +m2 =m
n1 +n2 =n

m=0 n=0 pset(P )

X

=

P (p)

pset(P )

X

X

X

n4(p, m1 , n1 , b) n8(P {p:1}, m2 , n2 , b)xm n

m=0 n=0 m1 +m2 =m
n1 +n2 =n

give us:
4
F
(goal(P ), x, y, b)
X
=
P (p)

pset(P )


X
X

X

X

n4(p, m1 , n1 , b)xm1 n1 n8(P {p:1}, m2 , n2 , b)xm2 n2

m1 =0 m2 =0 n1 =0 n2 =0

118

fiOn Testability BDI Agent Systems

=

X

P (p)

pset(P )
X

X

4

n(p, m1 , n1 , b)x



=

n8(P {p:1}, m2 , n2 , b)xm2 n2

m2 =0 n2 =0

m1 =0 n1 =0

X

X

X

m1 n1

4

8

P (p) F(p, x, y, b) F(P {p:1}, x, y, b)

pset(P )
8
F
(P M, x, y, b) generating function n8(P M, m, n, b). Section C.3
8
provide definition F
(P M, x, y, b) terms auxiliary function G8 (see Section C.4).
introduce bound length paths giving:
4
F
(goal(P ), x, y, b, )
X
x
4
8
=
P (p) (F
(p, x, y, b) F
(P {p : 1}, x, y, b))

pset(P )

=

X

x

4
8
P (p) (F
(p, x, y, b, ) F
(P {p : 1}, x, y, b, ))

pset(P )
8
C.2 Derivation F
(goal(P ), x, y, b, )

Similarly previous derivation, define:
8
F
(goal(P ), x, y, b, ) =

X

X

n8(goal(P ), m, n, b)xm n

m=0 n=0
8
(goal(P ), . . . ) terms plans P ,
derive recursive definition F
8
first define new function n(P M, m, n, o, b), denotes number unsuccessful
paths use plans multiset P . have:

n8(goal(P ), m, n, b) = n8(P M, m, n, |P |, b)
states goal fail, |P | plans multiset must tried.
define generating function G8(P M, x, y, z, b, ) n8(P M, m, n, o, b) ordinary x exponential z, i.e. coefficients xm n z /o! values
n8(P M, m, n, o, b).
have:
8
F
(goal(P ), x, y, b, ) =

X

X

n8(P M, m, n, |P |, b)xm n

m=0 n=0

wish rewrite terms G8 . generalising right hand side
sum possible values number plans used (o), followed restriction
select values = |P |:

119

fiWinikoff & Cranefield

8
F
(goal(P ), x, y, b, )

= |P |!

X

X
n8 (P M, m, n, |P |, b) z |P |


|P |! z |P |

xm n

m=0 n=0

X
P
8

X
o=0 n(P M, m, n, o, b)z /o! power(z)=|P |
|P |!
xm n
|P |
z
m=0 n=0

=

P
= |P |!

P P 8

n
m=0
n=0
o=0 n(P M, m, n, o, b)x z /o!



power(z)=|P |

z |P |

Since nested sum definition G8 (see Section C.4), simplify to:
8

F(goal(P ), x, y, b, ) = |P |!

G8(P M, x, y, z, b, ) power(z)=|P |
z |P |

8
Section C.4 derive definition G8(P M, . . .) terms F
(p, . . .) p
set(P ).

8
C.3 Definition F
(P M, x, y, b, )

Recall n8(P M, m, n, b) number unsuccessful paths using zero
plans plan multiset P (with respect binding b) actions, n
8
failed actions, F
(P M, x, y, b, ) ordinary generating function.
First consider case P empty. case, precisely one way
8
fail, generates trace length zero. Therefore, F
({}, x, y, b, ) = 1x0 0 = 1.
case P non-empty sum number plans used
execution, yields following definition:
|P |
8

n(P M, m, n, b) =

X

n8(P M, m, n, o, b)

o=0

n8(P M, m, n, o, b) is, before, number unsuccessful paths plan
8
multiset
, using plans. Therefore, using definition F
(P M, x, y, b, ) =
P PP

8

n
n
(P
M,
m,
n,
o,
b)x

,

have:
m=0
n=0
8
F
(P M, x, y, b, )

=

M|
X
|P
X
X

n8(P M, m, n, o, b)xm n

m=0 n=0 o=0

(replace n8 looking coefficient corresponding term G8 ,
o! accounts division o! G8 ; also reorder summations)
|P |

=

X

X X

o![xm n z ]G8(P M, x, y, z, b, )xm n

o=0 m=0 n=0

120

fiOn Testability BDI Agent Systems

(we shift o! outwards, multiply z /z )
P
|P | P
n
8
n
X
n=0 [x z ]G(P M, x, y, z, b, )x z
m=0
=
o!
zo
o=0

|P |

=

X
o=0

o!

G8(P M, x, y, z, b, ) power(z)=o
zo

C.4 Definition G8(P M, x, y, z, b, )
define G8(P M, x, y, z, b, ) generating function n8(P M, m, n, o, b) ordinary x exponential z (hence division o! below), ( 0)
maximum allowed trace length:
G8(P M, x, y, z, b, ) =

X
X

X

n8(P M, m, n, o, b)

m=0 n=0 o=0

xm n z
o!

Recall n8(P M, m, n, o, b) denotes number unsuccessful paths use
plans multiset P . empty multiset plans successful execution,
single unsuccessful execution 0 actions, uses 0 plans, hence:
(
1 = n = = 0
8
n({}, m, n, o, b) =
0 otherwise
Therefore, G8({}, x, y, z, b, ) = 1. non-empty multisets must partition actions
trace, action failures, numbers plans used, across different plan bodies
multiset, also consider ways plans various plan shapes
interleaved give overall order attempting plans:
n8({p1 :c1 , . . . , pj :cj }, m, n, o, b)
X
=
n8({p1 :c1 }, m1 , n1 , o1 , b) n8({pj :cj }, mj , nj , oj , b)
m1 ++mj =m
n1 ++nj =n
o1 ++oj =o

multinomial coefficient
Thus:


o1 ...oj



=

o!
o1 !...oj !

G8({p1 :c1 , . . . , pj :cj }, x, y, z, b, )
(by definition G8, using restriction, rather bounded sum m,
expanding n8 above)



X
X

=
n8({p1 :c1 }, m1 , n1 , o1 , b)

.
.
.

1
j
++m =m
m,n,o=0

1

j

n1 ++nj =n
o1 ++oj =o

xm n z
n({pj :cj }, mj , nj , oj , b)
o!
8

121

!
power(x)

fiWinikoff & Cranefield

=


X

X

o!
n8({p1 :c1 }, m1 , n1 , o1 , b)

!
.
.
.

!
1
j
=m

m,n,o=0 m1 ++mj
n1 ++nj =n
o1 ++oj =o

xm n z
n8({pj :cj }, mj , nj , oj , b)
o!

!
power(x)

(cancelling o! distributing oi ! xmi , ni z oi )
=


X

X

n8({p1 :c1 }, m1 , n1 , o1 , b)

m,n,o=0 m1 ++mj =m
n1 ++nj =n
o1 ++oj =o

xm1 n1 z o1
o1 !

xmj nj z oj
n({pj :cj }, mj , nj , oj , b)
oj !

!

8

(replacing


X

X



m=0 m1 +m2 =m


=


X



X
X

redistributing sums)

m1 =0 m2 =0

n8({p1 :c1 }, m1 , n1 , o1 , b)

xm1 n1 z o1

m1 ,n1 ,o1 =0




X



power(x)

o1 !




n8({pj :cj }, mj , nj , oj , b)

xmj nj z oj

mj ,nj ,oj =0

oj !


power(x)

x

(replacing restriction )




n

X
1
1
1
x z x
=
n8({p1 :c1 }, m1 , n1 , o1 , b)

o1 !
m1 ,n1 ,o1 =0




n

X
j
j
j
x
x z

n8({pj :cj }, mj , nj , oj , b)
oj !
mj ,nj ,oj =0

8

(by definition G . . . )
x

x

= G8({p1 :c1 }, x, y, z, b, ) G8({pj :cj }, x, y, z, b, )

need define G8({pi :ci }, x, y, z, b, ).
Consider n8({p:c}, m, n, o, b). simple cases = 0: use
plans, single unsuccessful
path, actions (m = n = 0).

c!
hand, > 0 oc = o!(co)!
ways selecting c available
copies plan p. selected plans executed o! different orders.
execution sum possible distributions actions (successful unsuccessful)
122

fiOn Testability BDI Agent Systems

amongst plans. gives:

X
c


o!
n8(p, m1 , n1 , b) n8(p, mo , , b)




m1 ++mo =m
n1 ++no =n
n8({p:c}, m, n, o, b) =


1
= n = = 0



0
otherwise

> 0

therefore following definition G8({p:c}, x, y, z, b, ), initial 1 abbreviates 1x0 0 z 0 /0!, i.e. base case = n = = 1, rest
definition G8 , expanding n8 using definition.
G8({p:c}, x, y, z, b, )
= 1+



X
c
o!




X

X

n8(p, m1 , n1 , b) n8(p, mo , , b)

1 ++mo =m
n1 ++no =n

m=0 n=0,o=1

xm n z
o!

(cancel o!/o!, rearrange sums replace upper bound
restriction)





X
X
X
X

c
8
8

n


= 1+
n(p, m1 , n1 , b) n(p, mo , , b)x

power(x) z


o=1
m=0 n=0
m1 ++mo =m
n1 ++no =n


X

(replacing

X

m=0 m1 +m2 =m

= 1+





X
X

redistributing sums)

m1 =0 m2 =0


X
c
o=1


X

X

!
n8(p, m1 , n1 , b)xm1 n1



m1 =0 n1 =0

X

X

!!
n8(p, mo , , b) xmo

mo =0 =0


power(x) z
!o
X
X

X
c
= 1+
n8(p, m, n, b) xm n
power(x) z

m=0 n=0
o=1
XX
8
(Replace
n8(p, m, n, b)xm n F
(p, x, y, b, ) per definition)


n

c
X
c
= 1+
F 8 (p, x, y, b, )o power(x) z

o=1
c
X
c
F 8 (p, x, y, b, )ox z
= 1+

o=1

123

fiWinikoff & Cranefield

Appendix D. Analysis Procedural Code Structures
seek derive expression largest possible number paths program
given size have, i.e. definition n(m) = max{n(P ) : |P | = m}. Recall
program either (atomic) statement single path (i.e. n(s) = 1), sequence
two programs P1 ; P2 n(P1 ; P1 ) = n(P1 ) n(P2 ), conditional P1 + P2
n(P1 + P2 ) = n(P1 ) + n(P2 ).
relatively easy see examining possible programs 3
n(m) = m. instance, largest number paths = 3 obtained program
+ + s. also easy show = 4 largest number paths possible 4.
larger values m? observe > 4 program36
largest number paths follows particular form. = 5 program
largest path written P5 = (s + + s); (s + s), n(P5 ) = 3 2.
generally, define S2 + s, S3 + + s, following
result, shows programs maximal number paths size,
considered particular form.
Theorem D.1 program size (for > 4) largest possible number
paths written Pi = Pi1 ; Pi2 ; . . . ; Pik Pij (1 j k) either S2
S3 .
Proof: establish result induction. assume holds n
4 < n m, show must also hold + 1. So, let us assume
program Pm+1 maximal number paths, form
j
1
2
k
Pm+1
; Pm+1
; . . . ; Pm+1
Pm+1
either S2 S3 . two cases, depending
structure Pm+1 . consider case turn show fact either (a)
Pm+1 rewritten desired form, preserving number paths
program size; (b) Pm+1 cannot maximal, since construct program size
+ 1 larger number paths Pm+1 .
j
1
2
k
Case 1: Pm+1 form Pm+1
; Pm+1
; . . . ; Pm+1
least one Pm+1
neither

S2 S3 . Let Pm+1 one sub-programs neither S2 S3 . convenience

define P shorthand Pm+1
. Now, since P size less + 1, induction
hypothesis applies37 , written form Pi1 ; Pi2 ; . . . Pil Pij
either S2 S3 . easy see one rewrite Pm+1 desired form
exploiting associativity ;, rewriting follows:
i1
i+1
i1
i+1
. . . Pm+1
; (Pi1 ; Pi2 ; . . . Pij ); Pm+1
; . . . = . . . Pm+1
; Pi1 ; Pi2 ; . . . Pij ; Pm+1
;...

Applying rewriting Pm+1
S2 S3 yields program size
+ 1, number paths original program, desired form:
sequence sub-programs, either S2 S3 . shows result holds
+ 1, i.e. maximal-path program written desired form.
1
2
k
Case 2: Pm+1 form Pm+1
; Pm+1
; . . . ; Pm+1
k, means
1
k
Pm+1 must consist single conditional, i.e. Pm+1 = Pm+1 + . . . + Pm+1
k > 1.

36. fact one maximal-path program, structure, modulo
swapping order arguments + ;.
37. Or, size 4, written S2 ; S2 maximal number paths program
size 4 meets desired form.

124

fiOn Testability BDI Agent Systems

1
2
Without loss generality view Pm+1 form Pm+1
+ Pm+1
(by viewing
1
k
1
k
Pm+1 + . . . + Pm+1 (Pm+1 + . . .) + Pm+1 k > 2). consider following
1
2
sub-cases, depending values n(Pm+1
) n(Pm+1
).
1
2
Case 2a: n(Pm+1
) n(Pm+1
) greater 2. show Pm+1
0
1
2
maximal. Consider program Pm+1
= Pm+1
; Pm+1
(i.e. + replaced
1
2
1
2
;). know n(Pm+1 ; Pm+1 ) = n(Pm+1 ) n(Pm+1
). Without loss gen1
2
erality, lets assume n(Pm+1 ) n(Pm+1 ). show original Pm+1
0
1
fewer paths Pm+1
. number paths Pm+1 n(Pm+1 ) = n(Pm+1
)+
2
1
2
1
2
n(Pm+1 ). Since n(Pm+1 ) n(Pm+1 ), n(Pm+1 ) = n(Pm+1 ) + n(Pm+1 )
2
2
2
2
1
n(Pm+1
) + n(Pm+1
) = 2 n(Pm+1
). Since n(Pm+1
) n(Pm+1
) greater
2
1
2
0
2, 2 n(Pm+1 ) < n(Pm+1 ) n(Pm+1 ) = n(Pm+1
), i.e.
0
Pm+1 paths Pm+1 , hence Pm+1 maximal + 1.
1
2
Case 2b: least one n(Pm+1
) n(Pm+1
) greater 2. Without loss
1
2
1
generality, assume n(Pm+1 ) n(Pm+1
). two cases: n(Pm+1
)
either 2 1.
1
) = 1.
Sub-case 2b(i): Let us consider first case n(Pm+1
program one path statement s, sequence statements s; s; . . . ; s.
Clearly latter maximal since replacing + + . . . + would
result program size paths. So, therefore Pm+1
1
2
2
maximal, Pm+1
must s, Pm+1 = + Pm+1
. Therefore Pm+1

size m. two sub-cases: either still greater 4, = 4.
second sub-case simple: 4 show, inspecting possible
2
programs size 4, n(4) = 4, therefore n(s + Pm+1
)
1 + 4 = 5. However, also know (s + + s); (s + s) size 5 6 paths,
hence sub-case Pm+1 cannot maximal number paths.
first sub-case, still greater 4, induction hypothesis applies
2
2
written desired form. abbreviate Pm+1

therefore Pm+1
j

1
2
P2 , Pm+1 = + (P2 ; P2 ; . . . ; P2 ) P2 either S2
00
S3 . Consider variant program Pm+1
= ((s + P21 ); P22 ; . . . P2j ),
00
clearly size Pm+1 . show Pm+1
paths
j
00
1
2
2
Pm+1 : n(Pm+1 ) = ((1 + n(P2 )) n(P2 ; . . . ; P2 )) = n(P2 ; . . . ; P2j ) + (n(P21 )
n(P22 ; . . . ; P2j )). Now, n(Pm+1 ) = 1 + (n(P21 ) n(P22 ; . . . ; P2j )). order show
00
n(Pm+1 ) < n(Pm+1
) need show 1 < n(P22 ; . . . ; P2j )
follows fact must least one P2i , that, since P2i
either S2 S3 , size least 2.
1
2
1
Sub-case 2b(ii): know n(Pm+1
) = 2 2 n(Pm+1
). Since n(Pm+1
)
2
2
2
n(Pm+1 ) n(Pm+1 ) 2 hence n(Pm+1 ) = 2 + n(Pm+1 )
2
0
2
2 n(Pm+1
) = n(Pm+1
). Now, n(Pm+1
) strictly greater 2
0
n(Pm+1 ) strictly less n(Pm+1 ) shown Pm+1 actually
2
maximal number paths. hand, n(Pm+1
)=2
2
n(Pm+1 ) = 2 + n(Pm+1 ) = 2 + 2 = 4. However, values
theorem applies, know n(m) > 4, therefore
shown sub-case Pm+1 maximal + 1.

125

fiWinikoff & Cranefield

shown assume Pm+1 maximal structure
specified, fact one derive another program, also size + 1, either
satisfy desired structure, larger number paths Pm+1 , contradicts assumption Pm+1 maximal. establishes desired property Pm+1 .
induction result applies > 4, desired.
previous result shows considering programs given size
largest possible number paths (denoted Pm ), limit considering
programs form P1m ; P2m ; . . . ; Pkm Pim either + + + s.
derive definition n(m). Firstly, observe that, inspecting cases:
n(m) = m, 4
n(5) = 6
n(6) = 9
two first cases discussed above. last case, two programs
appropriate structure size 6: S2 ; S2 ; S2 (with 8 paths) S3 ; S3
(with 9 paths).
consider > 6. Adding statement program (i.e. going m+1)
effect modifies Pm adding one Pim , increments n(Pim ) one.
Since multiplication commutative associative, without loss generality, assume
) n(P )
increment n(Pkm ). therefore n(Pm ) = n(P1m ; . . . ; Pk1
k



n(Pm+1 ) = n(P1 ; . . . ; Pk1 ) (n(Pk ) + 1). two cases:
)3 therefore
Case 1: Pim S3 , n(Pm ) = n(P1m ; . . . ; Pk1
) 4 = n(P ) 4 . Note case P
n(Pm+1 ) = n(P1m ; . . . ; Pk1

m+1
3


written P1 ; . . . ; Pk1 ; S2 ; S2 .

Case 2: Pim S2 S3 observe replacing 2 3 gives
greater increase number paths replacing 3 4, hence (after
)2
possibly reordering Pim Pkm = S2 ) n(Pm ) = n(P1m ; . . . ; Pk1
3


n(Pm+1 ) = n(P1 ; . . . ; Pk1 ) 3 = n(Pm ) 2 .
therefore recursive definition n(m) depending form Pm . next
observe fact form Pm follows simple cycle. know = 6, case 1
holds (as above, P6 = S3 ; S3 ). therefore P7 written S3 ; S2 ; S2 , hence
P8 written S3 ; S3 ; S2 S3 ; S2 ; S3 , hence P9 written S3 ; S3 ; S3 .
generally, prove induction Pm written P1m ; . . . ; Pkm
following holds: (a) multiple 3, Pim S3 ; (b)
one multiple 3, exactly two Pim S2 rest S3 ;
(c) two multiple 3, exactly one Pim S2 rest
S3 . gives us following recursive definition, 6 multiple 3:
n(m + 1) = n(m)

4
3

n(m + 2) = n(m + 1)
126

3
2

fiOn Testability BDI Agent Systems

n(m + 3) = n(m + 2)

3
2

simplified to:
4
3
34
n(m + 2) = n(m)
= 2 n(m)
23
334
= 3 n(m)
n(m + 3) = n(m)
223
n(m + 1) = n(m)

easily derive non-recursive definition focusing last case observing
n(6) = 9 = 32 n(m + 3) = 3 n(m) (for 6 multiple 3),
n(m) = 3m/3 . substitute definition obtain
following complete definition n(m), 6 multiple 3:
n(1) = 1
n(2) = 2
n(3) = 3
n(4) = 4
n(5) = 6
n(m) = 3m/3
4
n(m + 1) =
3m/3
3
n(m + 2) = 2 3m/3

127

fiWinikoff & Cranefield

References
Benfield, S. S., Hendrickson, J., & Galanti, D. (2006). Making strong business case
multiagent technology. Stone, P., & Weiss, G. (Eds.), Proceedings Fifth International Joint Conference Autonomous Agents Multiagent Systems (AAMAS),
pp. 1015. ACM Press.
Bordini, R. H., Fisher, M., Pardavila, C., & Wooldridge, M. (2003). Model checking AgentSpeak. Proceedings Second International Joint Conference Autonomous
Agents Multiagent Systems (AAMAS), pp. 409416. ACM Press.
Bordini, R. H., Hubner, J. F., & Wooldridge, M. (2007). Programming multi-agent systems
AgentSpeak using Jason. Wiley.
Bratman, M. E., Israel, D. J., & Pollack, M. E. (1988). Plans resource-bounded practical
reasoning. Computational Intelligence, 4, 349355.
Bratman, M. E. (1987). Intentions, Plans, Practical Reason. Harvard University Press,
Cambridge, MA.
Burch, J., Clarke, E., McMillan, K., Dill, D., & Hwang, J. (1992). Symbolic model checking:
1020 states beyond. Information Computation, 98 (2), 142170.
Burmeister, B., Arnold, M., Copaciu, F., & Rimassa, G. (2008). BDI-agents agile goaloriented business processes. Proceedings Seventh International Conference
Autonomous Agents Multiagent Systems (AAMAS) [Industry Track], pp. 3744.
IFAAMAS.
Busetta, P., Ronnquist, R., Hodgson, A., & Lucas, A. (1999). JACK Intelligent Agents Components Intelligent Agents Java. AgentLink News (2).
Dastani, M. (2008). 2APL: practical agent programming language. Autonomous Agents
Multi-Agent Systems, 16 (3), 214248.
Dastani, M., Hindriks, K. V., & Meyer, J.-J. C. (Eds.). (2010). Specification Verification
Multi-agent systems. Springer, Berlin/Heidelberg.
de Silva, L., & Padgham, L. (2004). comparison BDI based real-time reasoning
HTN based planning. Webb, G., & Yu, X. (Eds.), AI 2004: Advances Artificial
Intelligence, Vol. 3339 Lecture Notes Computer Science, pp. 11671173. Springer,
Berlin/Heidelberg.
Dennis, L. A., Fisher, M., Webster, M. P., & Bordini, R. H. (2012). Model checking agent
programming languages. Automated Software Engineering, 19 (1), 363.
dInverno, M., Kinny, D., Luck, M., & Wooldridge, M. (1998). formal specification
dMARS. Singh, M., Rao, A., & Wooldridge, M. (Eds.), Intelligent Agents IV:
Proceedings Fourth International Workshop Agent Theories, Architectures,
Languages, Vol. 1365 Lecture Notes Artificial Intelligence, pp. 155176,
Berlin/Heidelberg. Springer.
Dorigo, M., & Stutzle, T. (2004). Ant Colony Optimization. MIT Press.
Dwyer, M. B., Hatcliff, J., Pasareanu, C., Robby, & Visser, W. (2007). Formal software analysis: Emerging trends software model checking. Future Software Engineering
2007, pp. 120136, Los Alamitos, CA. IEEE Computer Society.
128

fiOn Testability BDI Agent Systems

Ekinci, E. E., Tiryaki, A. M., Cetin, O., & Dikenelli, O. (2009). Goal-oriented agent testing
revisited. Luck, M., & Gomez-Sanz, J. J. (Eds.), Agent-Oriented Software Engineering IX, Vol. 5386 Lecture Notes Computer Science, pp. 173186, Berlin/Heidelberg. Springer.
Erol, K., Hendler, J., & Nau, D. (1996). Complexity results HTN planning. Annals
Mathematics Artificial Intelligence, 18 (1), 6993.
Erol, K., Hendler, J. A., & Nau, D. S. (1994). HTN planning: Complexity expressivity.
Proceedings 12th National Conference Artificial Intelligence (AAAI), pp.
11231128. AAAI Press.
Fix, L., Grumberg, O., Heyman, A., Heyman, T., & Schuster, A. (2005). Verifying
large industrial circuits using 100 processes beyond. Peled, D., & Tsay, Y.K. (Eds.), Automated Technology Verification Analysis, Vol. 3707 Lecture
Notes Computer Science, pp. 1125, Berlin/Heidelberg. Springer.
Georgeff, M. P., & Lansky, A. L. (1986). Procedural knowledge. Proceedings IEEE,
Special Issue Knowledge Representation, 74 (10), 13831398.
Gomez-Sanz, J. J., Bota, J., Serrano, E., & Pavon, J. (2009). Testing debugging
MAS interactions INGENIAS. Luck, M., & Gomez-Sanz, J. J. (Eds.), AgentOriented Software Engineering IX, Vol. 5386 Lecture Notes Computer Science,
pp. 199212, Berlin/Heidelberg. Springer.
Huber, M. J. (1999). JAM: BDI-theoretic mobile agent architecture. Proceedings
Third International Conference Autonomous Agents (Agents99), pp. 236243.
ACM Press.
Ingrand, F. F., Georgeff, M. P., & Rao, A. S. (1992). architecture real-time reasoning
system control. IEEE Expert, 7 (6), 3344.
Jorgensen, P. (2002). Software Testing: Craftsmans Approach (Second edition). CRC
Press.
Lee, J., Huber, M. J., Kenny, P. G., & Durfee, E. H. (1994). UM-PRS: implementation procedural reasoning system multirobot applications. Proceedings Conference Intelligent Robotics Field, Factory, Service, Space
(CIRFFSS94), pp. 842849. American Institute Aeronautics Astronautics.
Mathur, A. P. (2008). Foundations Software Testing. Pearson.
Miller, J. C., & Maloney, C. J. (1963). Systematic mistake analysis digital computer
programs. Communications ACM, 6 (2), 5863.
Morley, D., & Myers, K. (2004). SPARK agent framework. Proceedings
Third International Joint Conference Autonomous Agents Multiagent Systems
(AAMAS), pp. 714721, New York. ACM.
Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., & Luck, M. (2006).
Crossing agent technology chasm: Experiences challenges commercial applications agents. Knowledge Engineering Review, 21 (4), 345392.
129

fiWinikoff & Cranefield

Naish, L. (2007). Resource-oriented deadlock analysis. Dahl, V., & Niemela, I. (Eds.),
Proceedings 23rd International Conference Logic Programming, Vol. 4670
Lecture Notes Computer Science, pp. 302316. Springer, Berlin/Heidelberg.
Nguyen, C., Miles, S., Perini, A., Tonella, P., Harman, M., & Luck, M. (2009a). Evolutionary testing autonomous software agents. Proceedings 8th International
Conference Autonomous Agents Multiagent Systems (AAMAS), pp. 521528.
IFAAMAS.
Nguyen, C. D., Perini, A., & Tonella, P. (2009b). Experimental evaluation ontology-based
test generation multi-agent systems. Luck, M., & Gomez-Sanz, J. J. (Eds.),
Agent-Oriented Software Engineering IX, Vol. 5386 Lecture Notes Computer
Science, pp. 187198, Berlin/Heidelberg. Springer.
Nguyen, C. D., Perini, A., & Tonella, P. (2007). Automated continuous testing multiagent systems. Proceedings Fifth European Workshop Multi-Agent Systems
(EUMAS).
Padgham, L., & Winikoff, M. (2004). Developing Intelligent Agent Systems: Practical
Guide. John Wiley Sons.
Paolucci, M., Shehory, O., Sycara, K. P., Kalp, D., & Pannu, A. (2000). planning component RETSINA agents. Jennings, N. R., & Lesperance, Y. (Eds.), Proceedings
6th International Workshop Agent Theories, Architectures, Languages
(ATAL), Vol. 1757 Lecture Notes Computer Science, pp. 147161, Berlin/Heidelberg. Springer.
Pokahr, A., Braubach, L., & Lamersdorf, W. (2005). Jadex: BDI reasoning engine.
Bordini, R. H., Dastani, M., Dix, J., & El Fallah Seghrouchni, A. (Eds.), Multi-Agent
Programming: Languages, Platforms Applications, chap. 6, pp. 149174. Springer.
Raimondi, F., & Lomuscio, A. (2007). Automatic verification multi-agent systems
model checking via ordered binary decision diagrams. J. Applied Logic, 5 (2), 235
251.
Rao, A. S. (1996). AgentSpeak(L): BDI agents speak logical computable language.
de Velde, W. V., & Perrame, J. (Eds.), Agents Breaking Away: Proceedings
Seventh European Workshop Modelling Autonomous Agents Multi-Agent
World (MAAMAW96), Vol. 1038 Lecture Notes Artificial Intelligence, pp. 4255,
Berlin/Heidelberg. Springer.
Rao, A. S., & Georgeff, M. P. (1991). Modeling rational agents within BDI-architecture.
Allen, J., Fikes, R., & Sandewall, E. (Eds.), Proceedings Second International
Conference Principles Knowledge Representation Reasoning, pp. 473484.
Morgan Kaufmann.
Sardina, S., & Padgham, L. (2011). BDI agent programming language failure handling, declarative goals, planning. Autonomous Agents Multi-Agent Systems,
23 (1), 1870.
Shaw, P., Farwer, B., & Bordini, R. (2008). Theoretical experimental results
goal-plan tree problem. Proceedings Seventh International Conference
Autonomous Agents Multiagent Systems (AAMAS), pp. 13791382. IFAAMAS.
130

fiOn Testability BDI Agent Systems

Sloane, N. J. A. (2007). on-line encyclopedia integer sequences. http://www.research.
att.com/njas/sequences/.
Thangarajah, J., Winikoff, M., Padgham, L., & Fischer, K. (2002). Avoiding resource
conflicts intelligent agents. van Harmelen, F. (Ed.), Proceedings 15th
European Conference Artificial Intelligence (ECAI), pp. 1822. IOS Press.
van Riemsdijk, M. B., Dastani, M., & Winikoff, M. (2008). Goals agent systems:
unifying framework. Proceedings Seventh Conference Autonomous Agents
Multiagent Systems (AAMAS), pp. 713720. IFAAMAS.
Wilf, H. S. (1994). generatingfunctionology (Second edition). Academic Press Inc., Boston,
MA. http://www.math.upenn.edu/wilf/gfology2.pdf.
Winikoff, M. (2010). Assurance Agent Systems: Role Formal Verification
play?. Dastani, M., Hindriks, K. V., & Meyer, J.-J. C. (Eds.), Specification
Verification Multi-agent systems, chap. 12, pp. 353383. Springer, Berlin/Heidelberg.
Winikoff, M., Padgham, L., Harland, J., & Thangarajah, J. (2002). Declarative & procedural goals intelligent agent systems. Proceedings Eighth International
Conference Principles Knowledge Representation Reasoning (KR2002), pp.
470481, Toulouse, France. Morgan Kaufmann.
Wooldridge, M. (2002). Introduction MultiAgent Systems. John Wiley & Sons,
Chichester, England.
Wooldridge, M., Fisher, M., Huget, M.-P., & Parsons, S. (2002). Model checking multi-agent
systems MABLE. Proceedings First International Joint Conference
Autonomous Agents Multi-Agent Systems (AAMAS), pp. 952959. ACM Press.
Zhang, Z., Thangarajah, J., & Padgham, L. (2009). Model based testing agent systems.
Filipe, J., Shishkov, B., Helfert, M., & Maciaszek, L. (Eds.), Software Data
Technologies, Vol. 22 Communications Computer Information Science, pp.
399413, Berlin/Heidelberg. Springer.
Zhu, H., Hall, P. A. V., & May, J. H. R. (1997). Software unit test coverage adequacy.
ACM Computing Surveys, 29 (4), 366427.

131

fi
