Journal Artificial Intelligence Research 51 (2014) 533-554Submitted 02/14; published 10/14Optimal Scheduling Contract AlgorithmsAnytime Problem-SolvingAlejandro Lopez-Ortizalopez-o@uwaterloo.caCheriton School Computer ScienceUniversity WaterlooWaterloo, Ontario, Canada, N2L 3G1Spyros Angelopoulosspyros.angelopoulos@upmc.frCNRS Laboratoire dInformatiqueUniversite Pierre et Marie Curie4 Place Jussieu 75252, FranceAngele M. Hamelahamel@wlu.caDepartment Physics Computer ScienceWilfrid Laurier UniversityWaterloo, Ontario, Canada, N2L 3C5Abstractcontract algorithm algorithm given, part input, specifiedamount allowable computation time. algorithm must complete executionwithin allotted time. interruptible algorithm, contrast, interruptedarbitrary point time, point must report currently best solution.known contract algorithms simulate interruptible algorithms using iterativedeepening techniques. simulation done penalty performancesolution, measured so-called acceleration ratio.paper give matching (i.e., optimal) upper lower bounds acceleration ratio simulation. assume general setting n probleminstances must solved means scheduling executions contract algorithmsidentical parallel processors. resolves open conjecture Bernstein, Finkelstein,Zilberstein gave optimal schedule restricted setting round robinlength-increasing schedules, whose optimality general unrestricted case remainedopen.Lastly, show evaluate average acceleration ratio class exponentialstrategies setting n problem instances parallel processors. broadclass schedules tend either optimal near-optimal, several variantsbasic problem.1. IntroductionAnytime algorithms algorithms whose quality output improves graduallyamount available computation time increases. class algorithms introducedfirst Dean Boddy (1988) context time-depending planning, wellHorvitz (1987, 1998) context flexible computation. Anytime algorithms occurc2014AI Access Foundation. rights reserved.fiLopez-Ortiz, Angelopoulos, & Hamelnaturally settings computationally intensive problem addressed uncertainty respect available computation time. example problemanswer must provided determined external inputcontrol. instance, consider automated trading program stock market.programs run time-intensive simulations price various financial instruments.change bid price given stock occurs algorithm must produce decision(buy/sell/hold) instant, using whatever information garnishedcourse simulations, take advantage newly posted price. Another example given real-time applications. instance, consider motion planning algorithmrobot solution must produced within certain, varying, amounttime: example robot collide move needed momentarily evenalgorithm produce suboptimal move, others, sufficient timecarefully compute next step. situation, amount allotted time givenalgorithm beforehand.According Russell Zilberstein (1991), distinction madetwo different types anytime algorithms. one hand, interruptible algorithmsalgorithms whose allowable running time known advance, thus interrupted (queried) given point throughout execution. algorithms typicallyinclude versions local search, e.g., simulated annealing hill climbing.hand, stringent class contract algorithms consists algorithms givenamount allowable computation time (i.e, intended query time) partinput. However, algorithm interrupted point contract time expires, algorithm may return meaningful results. algorithms thus lessflexible interruptible algorithms, however tend simpler construct,easier prove strict guarantees performance. typical example contractalgorithms polynomial-time approximation schemes (PTAS) based Dynamic Programming (DP): bigger amount computation time, better approximationachieved. However, algorithm may require available computation time order fill important entries DP table, otherwise solution returned mayuseless.Consider following general scenario: given set P n different probleminstances, want design efficient interruptible algorithm applied,concurrent fashion, n problem instances. amount computation timeknown advance; instead, expected unknown point time,interruption occur, point algorithm stopped queried report(partial) solutions one among problem instances. Clearly, algorithmmust make judicious use resources ensure produce reasonably goodsolution, despite knowledge exact time interruptions may occur.indicated earlier, hardly surprising setting arises frequentlydesign AI systems, applications game-playing programs (Althofer, 1997; Kao,Reif, & Tate, 1996; Kao, Ma, Sipser, & Yin, 1998), e-trading agents, medical diagnosissystems. Essentially problem captures fundamental trade-off qualitysolution returned algorithm amount available computation time.refer reader survey Zilberstein (1996) importance anytime algorithmsdesign intelligent systems.534fiOptimal Scheduling Contract Algorithms Anytime Problem-SolvingPTAS (Polynomial Time Approximation Schemes) provide rich source contractalgorithms guaranteed performance. motivates study general constructionscreating interruptible versions given contract algorithm. providesflexibility focusing design contract algorithms, convertinginterruptible algorithms applying standard, black-box transformation. Indeed,standard method simulating interruptible algorithm repeatedly executingcontract algorithm using increasing execution times. Consider general settingset processors identical speed available execution simulation,problem instance corresponding (single-thread) contract algorithm.problem face schedule executions various contract algorithmsprocessors way guarantees efficient interruptible algorithm. setting,query time, algorithm report problem instance solutioncorresponding contract longest length (i.e., contract time) completedquery time.becomes obvious formal measure quality simulation required.one may think several possible ways evaluating quality simulation,work adopt standard measure area, namely acceleration ratio (Russell& Zilberstein, 1991). formal definition provided Section 2; informally, acceleration ratio indicates much faster processors order guaranteesolution good one ideal algorithm foreknowledgequery time also problem instance p interest. ideal algorithm wouldsimply utilize single processor run solely contract instance p, time t.sense, acceleration ratio reflects loss optimality due lack future knowledgequery times problem instance question, motivated similarconsiderations competitive ratio context analysis online algorithms.case one problem instance single processor, Russell Zilberstein (1991)showed iterative doubling contract lengths gives rise interruptible algorithmacceleration ratio four. Zilberstein, Charpillet, Chassaing (2003) showedoptimal acceleration ratio, sense scheduling strategy definedset contracts acceleration ratio least four.Zilberstein et al. (2003) also studied generalization problem multipleproblem instances considered (assuming |M | = 1), Bernstein, Perkins, Zilberstein,Finkelstein (2002) studied generalization contracts single probleminstance must scheduled set multiple processors. versions, algorithmsoptimal acceleration ratios derived. problem, full generality, involves setprocessors set problems, cardinality greater one. Bernstein et al.n m+n m+n(2003) showed upper bound( n ) acceleration ratio; addition, usingelegant techniques, showed bound optimal restricted, though naturalintuitive, class schedules use round robin length-increasing strategy.strategies known cyclic strategies. Bernstein et al. leave open question whetherbound tight among possible schedules. paper answer questionaffirmative.general, observed theoretical analysis geometric searchesrobot motion planning closely linked scheduling heuristics algorithmsproblem solving. connection first established Kao et al. (1996, 1998)535fiLopez-Ortiz, Angelopoulos, & Hamelcontext randomized algorithms. work Bernstein et al. (2003) drew similar connection scheduling contract algorithms robot searching set rays (Alpern& Gal, 2003). latter problem, p robots search target located oneconcurrent rays. seek search strategy minimizes competitive ratio, namelyworst-case ratio search cost distance starting positiontarget.turns interesting parallels drawn two problems: informally,rays correspond problem instances, robots processors, (unknown) locationtarget corresponds (also unknown) query time. general problemp robots rays Lopez-Ortiz Schuierer (2004) showed optimal strategypachieves competitive ratio 1 + 2 mpp ( mp ) . Bernstein et al. (2003) independentlyderived bound directly translating approach context contractscheduling solution robot searching parallel rays. notedupper bounds work Lopez-Ortiz Schuierer Bernstein et al. basedexponential strategies. Informally, cyclic strategies lengthsrays searched (respectively, lengths contracts schedule) form geometricsequence (see Section 2 precise definition).intuitive level, problem scheduling contract algorithms multiple processors involved problem multi-robot searching concurrent rays.difficulty stems fact multiple searchers ray benefit,whereas multiple contract algorithms (of different lengths) problem couldwell improve performance. However, paper show ideas behindsolution Lopez-Ortiz Schuierer (2004) adapted, non-trivial manner,show optimality schedule Bernstein et al. (2003) without restrictions(such cyclicality) scheduling strategy.problem removing assumption cyclicality long history searchrelated problems. instance, Jaillet Stafford (2001) argue rigorously cyclicalitywaived single-searcher problems rays (whereas original work BaezaYates, Culberson, & Rawlins, 1993, cyclicality implicit). Along lines, LopezOrtiz Schuierer (2004) remove cyclicality assumptions context multi-searcherproblems. Similar conclusions harder establish concerning randomized algorithms(for instance, see Schuierer, 2003). sense, cyclic strategies simple analyzeprovide relatively easy upper bounds; however, difficulty lies establishing optimalitywithin space possible strategies.remainder paper organized follows. Section 2 present formaldiscussion problem setting. main result, namely Theorem 3, presented Section 3 show optimality exponential schedule Bernstein et al. (2003)without restrictions. Section 4 present average-case analysis acceleration ratio exponential strategies multi-processor setting, assuming uniformdistribution interruption times.2. PreliminariesLet P denote set n problem instances simply problems. contract c pair(p, d), p P denotes problem instance c assigned (also called536fiOptimal Scheduling Contract Algorithms Anytime Problem-Solvingproblem tag c) duration length contract, specifies processingtime required complete c. denote set identical processors. Let Cdenote (potentially infinite) set contracts. Define schedule X set contracts Cfeasible assignment contracts C set processors. particular,X described set {(ci , mi , si ) : ci C}, mi {0, . . . , 1} denotesprocessor ci scheduled, si denotes time processing begins.schedule X must feasible, sense two contracts ci = (pi , di ), cj = (pj , dj )C assigned processor X, cj scheduled immediatelyci , si + di sj . words, cj start cicompleted. Note assume non-preemptive schedules, sense cannotinterrupt later resume contract.Observation 1 Without loss generality consider schedules processors never idle, i.e., start time contract always finish time anothercontract.n problem instances identical processors, Bernstein et al. (2003) defineclass cyclic schedules schedules following natural properties:1. Property 1 (Problem Round Robin) ci = (pi , di ) ith contract cyclicschedule order, problem instance pi pi = mod n.2. Property 2 (Length Increasing) ci = (pi , di ) cj = (pj , dj ) pi = pj< j, di < dj .3. Property 3 (Processor Round Robin) mi = mod i.exponential schedule cyclic schedule lengths contracts roundrobin order increase exponentially. formally, i-th contract order lengthbi fixed number b.use acceleration ratio standard measure evaluating qualityschedule. Following Bernstein et al. (2003), assume contract completedtime solution available interruption occurs time after, includingtime t. also limit interruptions occur least one contractproblem P completed, otherwise problem vacuous (this canonicalassumption). Denote lX (p, t) length longest contract problem pcompleted time X (if source confusion omit subscript.)Definition 1 Given set P n problem instances set processors identicalspeed, acceleration ratio schedule X P , denoted Rm,n (X) definedsmallest value r, r 1 allowable interruption time t,problem p P , lX (p, t) t/r. acceleration ratio P setprocessors identical speed definedRm,n= inf Rm,n (X).X.schedule X optimal Rm,n (X) = Rm,n537fiLopez-Ortiz, Angelopoulos, & Hamelargue given schedule X, acceleration ratio Rm,n (X) determinedlooking discrete subset timeline, instead possible interruption times t.Let denote infinitesimally small positive value, let F denote set finish timescontracts X. easy see suffices consider interruptionsoccur times , F . see this, consider certain interruptionconform rule, let t0 earliest time contractfinishes X t0 > t. problems p, l(p, t0 ) = l(p, t); wordsalgorithm made progress problem time interval [t, t0 ], thust/l(p, t) < (t0 )/l(p, t0 ).following essentially alternative definition acceleration ratio, basedobservation.Observation 2 (Bernstein et al., 2003) Let F denote set finish times contracts schedule X. acceleration ratio schedule X PRm,n (X) = sup= sup.l(p, t)p,tp,tF,0 l(p, )given interruption time t, let p (t) denote function t/l(p, t). words,acceleration ratio X simply maximum value p (t), possible interruptiontimes problem instances p. Figure 1 illustrates example schedule2 problem instances 4 processors. Note value p peakscontract completed, problem instance.Given schedule X associated set contracts C, two problem instancesp1 , p2 P , let C1 , C2 denote two (potentially infinite) subsets C, contractsC1 problem tag p1 , contracts C2 problem tag p2 . Consider newset contracts C 0 identical C, exception every contract C1acquires problem tag p2 instead p1 every contract C2 acquires problem tag p1instead p2 . Consider also schedule X 0 otherwise identical X (exceptproblem tag swaps described above). say X 0 obtained X swapsets C1 C2 .Following convention Lopez-Ortiz Schuierer (2004), given two schedules XX 0 , say X contained X 0 time , denoted X X 0 twoschedules identical time . Given sequence schedules V = (X1 , X2 , . . .)say V converges limit schedule X > 0 exists NN , Xm Xm+1 . limit schedule X defined obvious way.3. Matching Lower Bound Acceleration Ratiosection prove lower bound acceleration ratio applies schedules. proceed proof main result, present intuition behindapproach illustrated Figure 2. Given arbitrary schedule, implementtransformations successively transform schedules complying certain regularization conditions (see Lemma 1, Lemma 2, Theorem 1 below). transformationseither preserve reduce acceleration ratio, purpose infuse certainamount structure schedule. important observe transformation538fiOptimal Scheduling Contract Algorithms Anytime Problem-SolvingProcessor 0Processor 1Processor 2Processor 365p (t)432112345678910TimeFigure 1: top figure depicts schedule contracts case 4 processors2 problems, first ten time units. Here, white hatched rectanglescorrespond executions contracts two problems, respectively,idle time schedule. bottom figure depicts plots functionp (t) vs time two problems (p {1, 2}). hatched (white) areatop corresponds solid (hashed) line plot bottom (respectively).acceleration ratio maximum value, axis, attained either curve,example equal 4.Theorem 1 might actually result object non-feasible schedule,well defined acceleration ratio is, again, shown greateroriginal schedule. lower-bound acceleration ratio obtained (i.e.,normalized) schedules, show matches upper bound Bernstein et al. (2003).similar approach showing optimality cyclic strategies applied Lopez-OrtizSchuierer (2004), context parallel robot searching concurrent rays. needemphasize, however, series transformations problem considerpaper differ significantly ones Lopez-Ortiz Schuierer. duefact ray-searching problem easier argue structural propertiesoptimal algorithm. Consider following example. Suppose time t, givenray r explored distance d. optimal algorithm (and indeed everyreasonable search algorithm) must time t0 > t, robot explores rayr, proceed least distance origin (otherwise, algorithmgains nothing exploration). contrast, scheduling problem considerpaper, may case certain processor M1 , contract length lscheduled start time finish time + l, whereas different processor M2 ,different contract problem scheduled start time bigger finishtime smaller + l (i.e., length smaller l). first sight, latter contract539fiLopez-Ortiz, Angelopoulos, & Hamelappears redundant; however l large, interruption occurs right+ l, schedule may gain smaller-length contract. fact, schedulesindeed possible optimal algorithm, ruled transformationtechniques. particular, example illustrates difficult give blackbox transformation proof Lopez-Ortiz Schuierer problem interest(although would interesting obtain explicit reduction).(worst)Space schedulesLemma 1accelerationratioLemma 2Theorem 1(best)Figure 2: Illustration proof technique. shaded region corresponds artificial,possibly infeasible, strategies by-products proof couldprinciple acceleration ratio strictly smaller optimal accelerationratio; however, show never happens, thus shaded region collapsesline strategies non-strategies acceleration ratio exactly equaloptimal value.Note assume, without loss generality, schedule startcontract smaller equal one already completed givenproblem.Let X given schedule contracts. follow convention denotinglower-case upper-case lengths pair consecutively completed contracts,given problem p, respectively. precisely, (pi , di ) denotes contract lengthdi problem pi , earliest contract X pi completed contract(pi , di ) finishes denoted (pi , Di ).Definition 2 schedule X contracts, given contract c length Dc defineacceleration ratio Dc (that is, immediately completion c X) r(c) =(Tc + Dc )/dc (assuming dc 6= 0), Tc denotes time processor startworking contract (pc , Dc ).particular, let C 0 denote set contracts schedule, excluding firstcompleted contract problem. Observation 2,Rm,n (X) =supcC 0 ,0converges supcC 0 r(c).540Tc + DcdcfiOptimal Scheduling Contract Algorithms Anytime Problem-Solvingfollowing lemma establishes quasi-cyclic property optimal strategy schedule.states problem shorter-length completed contract generally givenprecedence another problem longer completed contract; one possibleexception: smaller-length contract passed processor assignmentnext contract schedule must shorter one assigned problemwent ahead instead.Lemma 2 establishes another facet quasi-cyclicality property: problemshorter-length contract, whether favored next processor assignment, mustcomplete next contract problem longer completed contract, thusre-establishing quasi-cyclic property.Lemma 1 Consider schedule X, suppose time Ti processor startworking contract (pi , Di ) another problem pj contracts (pj , dj )(pj , Dj ) X. Let Tj denote time processor start workingcontract (pj , Dj ).Given two problems pi pj described dj < di Tj > Ti ,either Dj < Di define new schedule Dj < Di , whose accelerationratio worse original schedule.Proof. Suppose X dj < di Tj > Ti , suppose Dj > Di . Executeswap program tags contracts pi complete (pi , di ) contractspj complete (pj , dj ), obtain new schedule X 0 (recall definitionproblem swapping, given Section 2).Observe contracts swapping remain untouched. Likewise, contractsproblems involved swap also unaffected. Therefore, contributionacceleration ratio unchanged.Consider contracts swapped problems first swappedcontract completed. is, let cr r > contract problem pi originalschedule. schedule contribution acceleration ratio (Tr +Dr )/dr .swap expression still denotes acceleration ratio corresponding problempj , since contracts dr Dr run problem pj , implies scheduleremains unaffected swapping contracts well.Thus place acceleration ratio changes right timeswapping previously longest completed contract (the denominator) comesold schedule new contract completed comes new schedule(the numerator). show acceleration ratio new schedule X 0contracts worse original schedule.acceleration ratio original schedule Di , DjTi + Di Tj + Djmax,didjwhereas acceleration ratio X 0 Di , DjTi + Di Tj + Djmax,.djdi541fiLopez-Ortiz, Angelopoulos, & Hamelsince Tj > Ti Dj > Di ,Tj +DjdjTj +DjdjTi +Didj ;moreover since dj < di ,Tj +Djdi .Therefore, acceleration ratio X greater equalacceleration ratio alternative schedule.Corollary 1 Given schedule X schedule X 0 worse acceleration ratioproperty two problems pi pj , dj < di X, Tj > Ti ,always case Dj < Di X 0 .Proof. X already satisfies condition set X 0 := X nothing show.Otherwise apply process, i.e., appropriate problem swapping, argued proofLemma 1 following way: Let F = {f1 , f2 , . . . , } sorted sequence contractfinish times problems X. given f` define p(f` ) problem associatedfinishing time f` X. Let pj = p(f` ) let dj length contract associatedf` . starting f1 letting f` range ` = 1, 2 . . . checkdj < di Tj > Ti Dj < Di not, swap contracts describedproof Lemma 1.introduce notation needed statement proof Lemma 2.schedule X, let ST denote set contracts completed time inclusive.Also let complement ST , namely contracts X ST . Fix contractC0 = (p0 , D0 ), scheduled start time T0 . problem pj observeLemma 1 Dj = min{D : (pj , D) T0 +D0 }. Observe contextdefinitions, dj = max{d : (pj , d) ST0 +D0 }.Lemma 2 Let Ci = (pi , Di ) contract scheduled X time Ti Cj = (pj , Dj )contract Ti . exists schedule X 0 worse acceleration ratiodi dj problem pj 6= pi Ti + Di Tj + Dj .Proof. X satisfies conditions set X 0 := X lemma holds.Otherwise case, schedule X di dj least oneproblem pj 6= pi Ti + Di < Tj + Dj . Consider swap contractsproblems pi pj Ti +Di swap problem tags defined Section 2. argueswap gives rise new schedule X 0 worse acceleration ratio. Onesee suffices look acceleration ratio affected points rightCi Cj completed. First, note swap, acceleration ratiotwo points equal quantityTi + Di Tj + Dj= max,didjswap, acceleration ratio two points becomesTi + Di Tj + Dj= max,.djdi+DSince dj di Ti + Di Tj + Dj j dj j thus , meansacceleration ratio worsen two specific points.542fiOptimal Scheduling Contract Algorithms Anytime Problem-SolvingLemma 1 apply process repeatedly, starting dj lengthcontract associated smallest finish time f` (with ` = 1, 2 . . .) checkinglarger contract length di dj Ti +Di Tj +Dj . onecontracts, select Di smallest completion time Ti + Di swap tags(pj , Dj ). proceed next finishing time f`+1 . produces schedulecontracts di dj (and pj 6= pi ) Ti + Di Tj + Dj .Definition 3 schedule X said normalized satisfies conditions Corollary 1 Lemma 2.Lemma 3 exists optimal normalized schedule.Proof. Since Lemma 2 stronger Lemma 1, applying Lemma 2 cannot violateconditions Lemma 1 pair contracts swapped. indeed possible,however, swap contract conflict earliestrecently modified contracts. precise, consider configuration Figure 3(a)original schedule next scheduled contracts completioncontracts di dj , Tj +Dj originally larger Ti +Di . swapget new completion times Tj0 + Dj0 := Ti + Di Ti0 + Di0 := Tj + Dj . Observe, howevernew completion time Ti0 + Di0 create new conflict another schedule dkillustrated Figure 3. hard verify figure represents general casetype conflict created.djdidkTi + DiTk + DkTj + DjdjdidkTj0 + Dj0Tk + DkTi0 + Di0Figure 3: Situation single application Lemma 2.key observe original setting pairs hdj , di hdj , dk formedinversion, i.e. constituted violation Lemma 2. However applicationlemma inversions disappeared new inversion hdi , dk creatednet decrease one number inversions schedule. Hence process musteventually resolve inversions involving contracts index N fixed valueN process converges well defined strategy.Theorem 1 acceleration ratio Rm,n (X) optimal normalized schedule X nproblems processors leastk+nPRm,n (X) supk0i=0kPxsii=km+1(1)xsiX = (xs0 , xs1 , . . .) sorted sequence contract lengths (in increasing order, tiesbroken arbitrarily) schedule Xand define xsi = 0 < 0.543fiLopez-Ortiz, Angelopoulos, & HamelProof. Let X optimal normalized schedule. Consider time T0 processorM0 begin new contract. Since X normalized schedule, M0 chooseproblem p0 way satisfies conditions Corollary 1. Let D0 allotedprocessing time M0 devote p0 starting time T0 . Let longest completedcontract problem p0 time T0 + D0 d0 .observe that, Lemmas 1 2, every contract length strictly smallerd0 must complete within open interval (0, T0 + D0 ), hence endinterval every processor engaged contract length least d0 every problemcompleted contract length d0 previous step.lengths contracts d` d0 0 ` n 1 elements sequenceX . Similarly let Mj denote processor denote Ij set indices Xcontracts executed processor Mj time T0 + D0 , inclusively, 0 j 1.Note d0 = xsk0 , k0 0.Furthermore, let Dj last completed contract processor Mj , say problemp` , previous completed contract dj p` , less d0 . accelerationratio problem p` Dj givenPiIj xixskjaccording Observation 2, 0 j 1. Hence, worst case acceleration ratiooccurred time contracts first exceeding d0 completedleast(P)Pm1 PiIj xij=0iIj xi.(2)Rm,n (X) maxPm10jm1xskjj=0 xkjmake use ofPthe facta, b, c, > 0. NoteP smax {a/c, b/d} (a + b)/(c + d),completedxcontainssummandsxsum = m1j=0iIjtime T0 + D0 . particular know includes xs` smaller xsk0 ,Lemma 2 guarantees problem completed contract dj complete anothercontract Dj problem completed contract d0 dj T0 + D0 hencesummation given time T0 + D0 contains xs` (i.e. dj s) smaller xsk0(i.e. d0 ). words, every element xsk0 sorted schedule X appears A.observe n 1 problems p1 , . . . , pn1 must completeddurations exceeding xsk0 otherwise current contract length D0 wouldassigned problem instead p0 . contains sorted valuesX xsk0 plus least n 1 larger values corresponding finished contractsn 1 problems. smallest choices n 1 values togetherD0 xsk0 +1 , . . . , xsk0 +n . Hence, obtainm1XXxsij=0 iIjkX0 +nxsi .(3)i=0Consider values xkj = dj , 1 j 1. Recall value Dj timeproblem pj completed time T0 + D0 processor Mj dj longest544fiOptimal Scheduling Contract Algorithms Anytime Problem-Solvingcompleted contract pj time Tj + Dj . Lemma 2 d0 largest timeamong di s. 1 largest di values xsk0 m+1 , . . . , xsk0 1m1Xk0Xdjxsi .(4)i=k0 m+1j=0Combining (2),(3) (4)kP0 +nPm1 PRm,n (X)iIj xij=0Pm1j=0 xkji=0k0Pxsii=k0 m+1,xsik0 n.order prove lower bound right hand side Inequality (1) make useresults Gal (1980) Schuierer (2001) state without proofsimplified form completeness; particular, follow work Schuierer (2001, Thm.1)1 . Define Ga = (1, a, a2 , . . .) geometric sequence X +i = (xi , xi+1 , . . .)suffix sequence X starting xi .Theorem 2 (Schuierer, 2001) Let X = (x0 , x1 , . . .) sequence positive numbers,r integer, = limn (xn )1/n , R {+}. Fk , k 0, sequencefunctionals satisfy1. Fk (X) depends x0 , x1 , . . . , xk+r ,2. Fk (X) continuous, xi > 0, 0 k + r,3. Fk (X) = Fk (X), > 0,4. Fk (X + ) max{Fk (X), Fk (Y )},5. Fk+i (X) Fk (X +i ), 1,sup Fk (X) sup Fk (Ga ).0k<0k<case, sequence X represents lengths contracts schedule.acceleration ratio completion contract forms sequence functionals.value functional depends prefix contracts whose start timecurrent time. sequence Ga represents geometric sequence,wish show describes optimal schedule.1. Theorem 1 proven Schuierer (2001) applies broad setting, purposes proofsuffices consider case p = 1.545fiLopez-Ortiz, Angelopoulos, & HamelProposition 1 Let Fk (X ) sequence functionals defined follows:k+nXFk (X ) =xsikXi=0xsi ,i=km+1Fk (X ) satisfies conditions Theorem 2.Proof. straightforward see Fk (X ) satisfies conditions (1)-(3). verifyk+nkPPxsi define YT YB analogously (i.e.,condition (4), let XT =xi , XB =i=0i=km+1kmPsubstituting xsi yis ). Observe YT = YB + Q, Q =i=0yis +k+nPi=k+1yis . Now,wish show Fk (X + ) max{Fk (X), Fk (Y )} equivalentlyXT + YTXT + YB + Q=maxXB + YBXB + YBXT YT,XB YB(5)follows previously noted inequality max {a/c, b/d} (a + b)/(c + d),a, b, c, > 0.verify Condition (5) Theorem 2, first note definition Fk (X )Pk+n+jxiFk+j (X) = Pk+ji=0i=km+1+j xiPk+nFk (X+j) = Pki=0xsi+ji=km+1 xi+jPk+n+ji=j= Pk+jxsii=km+1+jxsi.Lastly observe since terms X positive hencej1Xi=0xsi0=j1Xk+n+jXxsi +xsii=0i=jk+n+jXPj1xsii=0=Pk+n+jxsi +i=jPk+ji=ji=km+1+jxsixsiPk+n+ji=jPk+ji=km+1+jrequired.Therefore, combining Theorem 1, Proposition 1 Theorem 2 have:Rm,n (X) sup Fk (X ) sup Fk (Ga ) = sup0k<0k<k+nPNote 6= 1,aii=0kP=ai0k<ak+n+1 1ak+1 akm+1=(k+nXi=01k+11akX).i=km+1. Therefore, < 1,i=km+1deduce Rm,n (X) tends infinity k . Moreover, = 1, obtain546xsixsifiOptimal Scheduling Contract Algorithms Anytime Problem-SolvingRm,n (X) = k+n+1, which, likewise, tends infinity k . Hence, assume> 1 obtain(ak+n+1 1)/(a 1)Rm,n (X)supk+1 akm+1 )/(a 1)0k< (ak+n+11an+m(a>1)=sup==.k+1 akm+1110k<expression minimized = ((m + n)/n)1/m , implies acceleration ratio X boundedm+n (m+n)/mn + n m+nnRm,n (X).=m+nnn 1thus shown following theorem:Theorem 3 Given n problem instances processors, every schedule simulatesinterruptible algorithm using executions contract algorithms acceleration ratiolessn + n m+n.nworth pointing round-robin schedule contract lengths 1, a, a2 , ...1m+n m+nnacceleration ratio preciselyvalue = m+n, shownnnBernstein et al. (2003), thus matches lower bound. words, round robinlength-increasing schedule proposed Bernstein et al. optimal among possibleschedules whether round robin length-increasing, not. sayoptimal schedules round robin length-increasing, fact one easily constructoptimal non round-robin schedules case n multiple m. formally,obtain following theorem.Theorem 4 optimal schedule n problems processors simulates interruptible algorithm using executions contract algorithms acceleration ration + n m+n.nTheorem 4 provides tight bound worst-case acceleration ratio. precisely,assume interruptions issued malicious adversary, typically rightcontract algorithm terminates execution. sense, measure extremelypessimistic, reflects performance schedule adversarial setting.next section show upper-bound average-case acceleration ratio exponentialschedules. issue stochastic deadlines addressed Zilberstein et al. (2003)case single processor n problem instances. setting, uncertaintyinterruption quality output contract algorithm. Similartypes analysis applied Kao Littman (1997) ray-search problemtwo rays, assuming probabilistic knowledge placement target.547fiLopez-Ortiz, Angelopoulos, & HamelG3G0b3b0b1b2Figure 4: Example = 2, n = 3. line corresponds acceleration ratioproblem interrupted time t.4. Average-Case Analysis Exponential Strategiessection present average-case analysis acceleration ratio exponentialstrategies (recall formal definition given Section 2). scenario, interruptionoccurs time chosen adversarially, rather chosen uniformly randominterval [0, U ], U > 0. Likewise, problem instance queriedalso chosen uniformly random among n problems.similar problem considered context robots searching targetrays. natural scenario hiker becomes injured woods. searchers mustefficiently explore forest trails find injured person soon possible.setting reason expect hiker would locate adversarially.Hence worst case bound provides exact upper bound worst possibledelay reaching target, average case analysis gives realistic estimateexpected amount time target found. Kao et al. (1996) showedaverage target found nearly twice fast worst case scenario.consider analogous setting interruption time chosen independently random, rather adversarially. Hence, expect randomly-choseninterruption likely coincide interruptions yield high valuesacceleration ratio worst-case scenario.remainder section, consider exponential schedule X base b;is, length k-th contract cyclic ordering bk . Let Gk denote finishtime contract. also assume problems numbered 0, . . . , n 1;ordering makes presentation easier follow.formalizing concept average acceleration ratio illustrate intuitionusing example. Refer Figure 4, depicts example n = 3 problems= 2 processors. Here, three jagged line segments correspondsacceleration ratio problem function interruption time. precisely,line plot function p (t), formally defined Section 2. Consider exampleproblem 0 shown solid line Figure 4. first contract completedproblem contract 0, length b0 , completion time G0 . contractcompleted, available processors occupied computing contracts length b1 b2548fiOptimal Scheduling Contract Algorithms Anytime Problem-Solvingproblems 1 2 respectively. Eventually, processor becomes available computescontract length b3 problem 0, completed time G3 . Noteacceleration ratio problem 0 degrades linearly (i.e., increases) time intervalG0 G3 . Similar observations made remaining linear segments plot,well two problems.average acceleration ratio specific problem described arearespective acceleration ratio curve, normalized length U sampled space[0, U ]. overall average acceleration ratio given average individualacceleration ratio averages n problems2 . Since acceleration ratioproblem piece-wise linear function, order obtain average computeintegral line segment, sum segments. normalize, needdivide length interval well number n problems.Consider interruption interval [Gk , Gn+k ), index k cyclicorder, let pk denote problem corresponds contract finish time Gk .Since schedule exponential, follows pk (T ) = /bk . Since randomvariable, pk also random variable. Thus, compute average accelerationratio problem pk within interval [Gk , Gn+k )E[Gk ,Gk+n ) [pk ] =1Gk+n Gk=1Gk+n Gk===ZGk+npk (x) dxGkZ Gk+nGkxdxbkZ Gk+n GkGk + x1dxGk+n Gk 0bk1Gk (Gk+n Gk ) (Gk+n Gk )2+Gk+n Gkbk2 bkGk+n + Gk.2bkcompute average acceleration ratio problem pk entire span [0, U ] needadd area entire jagged line corresponds problem. Observearea single sawtooth given quantity (Gk+n Gk ) E[Gk ,Gk+n ) [pk ].allows us give expressions acceleration ratio one n problems.2. Normally, standard assumption interruptions occur least one contract problemcompleted, namely time Gn1 . make simplifying assumption interruptions occurearlier Gn1 , case acceleration ratio 0. refinement interruptionsGn1 follows along lines discussion section. case average sampledspace [Gn1 , U ) net zero effect asymptotic acceleration ratio (the extra term goeszero U goes infinity), results much complicated expressions.549fiLopez-Ortiz, Angelopoulos, & Hamelparticular, average acceleration ratio problem 0 given by:br/ncX1G(k+1)n Gkn E[Gkn ,G(k+1)n ) [0 ] + (U Gr+n )E[Gr+n ,U ) [0 ]E[0,U ) [0 ] =Uk=0br/nc 2222XGGU Gr+n1kn(k+1)n=+(6)U2br+n2bknk=0r U [Gr+n , Gr+n+1 ). Note last term correspondstruncated sawtooth interval [Gr+n , U ].expression average acceleration ratio problem {1, . . . n 1} similar; however, instead endpoints G0 , Gn , G2n , . . . need consider endpointsGi , Gn+i , G2n+i , . . .; precisely, obtain expressionribX22n cG(w+1)n+i Gwn+i U 2 G2r+i1E[0,U ) [i ] =+(7).U2bwn+i2br+iw=0overall acceleration ratio random variable defined =(7) obtain1nPn1i=0. Using (6)n11XE[0,U ] [i ]nE[0,U ] [] =i=01nU=1nU=rXk=0rXk=0(Gk+n Gk ) E[Gk ,Gk+n ) [0 ] +n1X!(U Gr+i ) E[Gr+i ,U ] [i ]i=1X U 2 G2r+iG2k+n G2k n1+2br+i2bk!.(8)i=1easy compute Gk (Bernstein et al., 2003, Proof Theorem 1). Namely,bk/mcGk =Xbk/mcmi+(k mod m)bk mod= bi=0Xbmi =i=0bk+m bk mod.bm 1simplicity presentation consider case U = Gr+n . general caseanalogous, though contains slightly complicated expressions.!rXX G2r+n G2r+iG2k+n G2k n11E[0,Gr+n ] [] =+nGr+n2br+i2bkk=0=1/(bm 1)2n(br+n+m bJr+nKm )i=1rX (bk+n+mbJk+nKm )2 (bk+m bJkKm )2bkk=0!n1X (br+n+m bJr+nKm )2 (br+i+m bJr+iKm )2+.(9)br+ii=1550fiOptimal Scheduling Contract Algorithms Anytime Problem-SolvingJxKm denotes x mod evaluate separately two summations involved(9). First,rX(bk+n+m bJk+nKm )2 (bk+m bJkKm )2k=0===bk=rX!2Jk+nKm2JkKmbbb2(n+m)+k 2bn+m+Jk+nKm +b2m+k + 2bm+JkKmbkbkk=0!r2Jk+nKm2JkKmXbb(b2(n+m) b2m )bk 2bn+m+Jk+nKm ++ 2bm+JkKmbkbkk=0rXb2mb2m2(n+m)2m kn+2m2m(bb )b + 2b+ k + 2b + kbbk=0r+1 1b2m+12(n+m)2m bn+2mbb+ rb+O,(10)b1b1penultimate step used JxKm < m. Second, getn1X(br+n+m bJr+nKm )2 (br+i+m bJr+iKm )2=br+ii=1n1X b2(n+m)+r 2bn+m+Jr+nKmb2Jr+nKmb2Jr+iKm2m++im+Jr+iKm=+b+ 2bbibr+ibr+ii=12m+1 n1n1 1n1 12b(b1)2m+r+n+1 b2m+r+1 b= b+Ob+b1br (b 1)b12m+1n n1b(b1)2m.(11)+ nb+Obr (b 1)Substituting (10) (11) (9) using simple algebraic expansion simplificationobtainbn1 1(bm 1)12(n+m)2m2m+n+12m+1 bE[0,Gr+n ] [] =bb+ bbb1b12n(bn+m bJr+nKm r )2m+1 n11(b 1)1b(b1) 1+rrnbn+2m r +. (12)Jr+nKrn+mbb1b2n(bb)Note (12) O-terms tend asymptotically 0 r approaches infinity. Hence,terms negligible impact acceleration ratio interruption occursfar ahead time, sense considered error terms. particular,obtain following simplified expression asymptotic acceleration ratio:b1n1 1r (b 1)2(n+m)2m2m+n+12m+1 bE[0,Gr+n ] [] =bb+ bb2nbn+mb1b1nb (b 1)(b + 1)=.2n(bm 1)(b 1)551fiLopez-Ortiz, Angelopoulos, & HamelTheorem 5 asymptotic average acceleration ratio exponential schedule bi ,= 0, 1, . . ., interruption chosen uniformly random [0, U ] bounded(bm+n bm ) (b + 1).2n (bm 1) (b 1)Proof. Follows discussion above.Corollary 2 asymptotic average acceleration ratio optimal schedule Theorem4 boundedn1m+n1+1(m + n) m+nnn.12mn m+n1nCorollary 3 schedule optimal acceleration ratio non-optimal average acceleration ratio.Proof. readily verified computing derivative expressionTheorem 5 evaluating around value b = ((m+n)/n)1/m used optimal worstcase strategy. One observe algebraic manipulation derivativealways negative point, implies larger value b0 = b + results lower(i.e. better) average acceleration ratio hence worst-case optimal scheduleoptimal average sense.5. Conclusionpaper resolved open question posed Bernstein et al. (2003) concerningoptimal acceleration ratio schedule contract algorithms. well-studiedproblem artificial intelligence, several applications design real-time systems.main result shows optimal schedules found class cyclic schedules,or, alternatively, cannot improve quality simulation designingcomplicated schedules. also performed average-case analysis exponential schedules,assuming uniform distribution interruption times.recent work, Angelopoulos, Lopez-Ortiz, Hamel (2008) able applysimilar techniques design optimal schedules interruptible algorithmspresence soft deadlines. setting, interruption hard deadline, senseinterruption occurs, algorithm allowed additional window timecomplete execution (which seen, intuitively, grace period). algorithmmust report solution queried problem within additional time window.different example work, Angelopoulos Lopez-Ortiz (2009) addressedrefinements acceleration ratio reflect better performance schedulesnumber problems larger number available processors. cases,resorted use normalization techniques along lines Section 3.provides evidence techniques tied specific variant problem,instead applicable much wider settings.interesting open problem find tight bounds randomized strategies, namelyschedules length contracts, well processors contracts552fiOptimal Scheduling Contract Algorithms Anytime Problem-Solvingassigned random variables. Note randomization known helpcontext ray searching problem (Kao et al., 1996). related (and challenging) problem find schedules achieve optimal average-case acceleration ratio.exponential schedules optimal measure? Based analysis Section 4,expect answer question fairly technical involved. evenambitious problem find optimal schedules assuming certain known probabilitydistribution interruption times problems queried.Exponential schedules seen example doubling algorithm,lengths contracts increase geometrically. challenging part work (as Angelopoulos et al., 2008; Angelopoulos & Lopez-Ortiz, 2009) lower-bound performance arbitrary strategies: accomplished lower-bounding supremumsequence functions functionals geometric sequences (Theorem 2). believesimilar techniques applied many optimization problems doublingalgorithms known perform well (see, e.g., survey Chrobak & Mathieu, 2006).recent example found work Langetepe (2010), concerning optimalityspiral search locating target plane.Last, least, optimal schedule presented work parallelssearch strategies problem searching rays using p robots, exact correspondence remains shown. correspondence would extend one establishedBernstein et al. (2003) concerning cyclic schedules strategies.6. Acknowledgmentspreliminary version paper (Lopez-Ortiz, Angelopoulos, & Hamel, 2006) appearedProceedings Twenty-First National Conference Artificial Intelligence (AAAI).ReferencesAlpern, S., & Gal, S. (2003). Theory Search Games Rendezvous. KluwerAcademic Publishers.Althofer, I. (1997). symbiosis man machine beats grandmaster Timoshchenko.Journal International Computer Chess Association., 20 (1), 4047.Angelopoulos, S., & Lopez-Ortiz, A. (2009). Interruptible algorithms multi-problemsolving. Proceedings 21st International Joint Conference Artificial Intelligence (IJCAI), pp. 380386.Angelopoulos, S., Lopez-Ortiz, A., & Hamel, A. (2008). Optimal scheduling contractalgorithms soft deadlines. Proceedings 23rd National ConferenceArtificial Intelligence (AAAI), pp. 868873.Baeza-Yates, R., Culberson, J., & Rawlins, G. (1993). Searching plane. InformationComputation, 106, 234252.Bernstein, D. S., Finkelstein, L., & Zilberstein, S. (2003). Contract algorithms robotsrays: Unifying two scheduling problems.. Proceedings Eighteenth International Joint Conference Artificial Intelligence (IJCAI), pp. 12111217.553fiLopez-Ortiz, Angelopoulos, & HamelBernstein, D., Perkins, T. J., Zilberstein, S., & Finkelstein, L. (2002). Scheduling contract algorithms multiple processors. Proceedings Eighteenth NationalConference Artificial Intelligence (AAAI), pp. 702706.Chrobak, M., & Mathieu, C. (2006). Competitiveness via doubling. SIGACT News, 37 (4),115126.Dean, T., & Boddy, M. S. (1988). analysis time-dependent planning. Proceedings7th National Conference Artificial Intelligence, pp. 4954.Gal, S. (1980). Search Games. Academic Press.Horvitz, E. (1987). Reasoning beliefs actions computational resource constraints. Proceedings Third Annual Conference Uncertainty ArtificialIntelligence (UAI), pp. 301324.Horvitz, E. (1998). Reasoning varying uncertain resource constraints. Proceedings 7th National Conference Artificial Intelligence (AAAI), pp. 111116.Jaillet, P., & Stafford, M. (2001). Online searching. Operations Research, 49, 501515.Kao, M.-Y., & Littman, M. L. (1997). Algorithms informed cows. AAAI workshopOnline Search.Kao, M.-Y., Ma, Y., Sipser, M., & Yin, Y. (1998). Optimal constructions hybrid algorithms. Journal Algorithms, 29 (1), 142164.Kao, M.-Y., Reif, J. H., & Tate, S. R. (1996). Searching unknown environment: optimal randomized algorithm cow-path problem. Information Computation,131 (1), 6379.Langetepe, E. (2010). optimality spiral search. Proceedings 21st AnnualACM-SIAM Symposium Discrete Algorithms (SODA).Lopez-Ortiz, A., Angelopoulos, S., & Hamel, A. (2006). Optimal scheduling contractalgorithms anytime problems. Proceedings 21st National ConferenceArtificial Intelligence (AAAI).Lopez-Ortiz, A., & Schuierer, S. (2004). Online parallel heuristics, processor schedulingrobot searching competitive framework. Theoretical Computer Science,310, 527537.Russell, S. J., & Zilberstein, S. (1991). Composing real-time systems. Proceedings12th International Joint Conference Artificial Intelligence (IJCAI), pp. 212217.Schuierer, S. (2001). Lower bounds on-line geometric searching. Computational Geometry:Theory Applications, 18 (1), 3753.Schuierer, S. (2003). lower bound randomized searching rays. ComputerScience Perspective, pp. 264277.Zilberstein, S. (1996). Using anytime algorithms intelligent systems. AI Magazine, 17 (3),7383.Zilberstein, S., Charpillet, F., & Chassaing, P. (2003). Real-time problem-solvingcontract algorithms. Annals Mathematics Artificial Intelligence, 39 (12), 118.554fiJournal Artificial Intelligence Research 51 (2014)Submitted 02/14; published 11/14Using Meta-mining Support Data Mining WorkflowPlanning OptimizationPhong NguyenMelanie HilarioPhong.Nguyen@unige.chMelanie.Hilario@unige.chDepartment Computer ScienceUniversity GenevaSwitzerlandAlexandros KalousisAlexandros.Kalousis@hesge.chDepartment Business InformaticsUniversity Applied SciencesWestern Switzerland,Department Computer ScienceUniversity GenevaSwitzerlandAbstractKnowledge Discovery Databases complex process involves many differentdata processing learning operators. Todays Knowledge Discovery Support Systemscontain several hundred operators. major challenge assist user designingworkflows valid also ideally optimize performance measureassociated user goal. paper present system. system reliesmeta-mining module analyses past data mining experiments extracts metamining models associate dataset characteristics workflow descriptors viewworkflow performance optimization. meta-mining model used within data miningworkflow planner, guide planner workflow planning. learn metamining models using similarity learning approach, extract workflow descriptorsmining workflows generalized relational patterns accounting also domainknowledge provided data mining ontology. evaluate quality data miningworkflows system produces collection real world datasets comingbiology show produces workflows significantly better alternativemethods workflow selection planning.1. IntroductionLearning models extracting knowledge data using data mining extremelycomplex process requires combining number Data Mining (DM) operators, selected large pools available operators, combined data mining workflow. DMworkflow assembly individual data transformations analysis steps, implementedDM operators, composing DM process data analyst chooses addresshis/her DM task. Workflows recently emerged new paradigm representingmanaging complex computations accelerating pace scientific progress. (meta-)analysis becoming increasingly challenging growing number complexityavailable operators (Gil et al., 2007).c2014AI Access Foundation. rights reserved.fiNguyen, Hilario & KalousisTodays second generation knowledge discovery support systems (KDSS) allow complexmodeling workflows contain several hundreds operators; RapidMiner platform (Klinkenberg, Mierswa, & Fischer, 2007), extended version Weka (Hallet al., 2009) R (R Core Team, 2013), proposes actually 500 operators,complex data control flows, e.g. bagging boosting operators,several sub-workflows interleaved. consequence, possible numberworkflows modeled within systems order several millions,ranging simple elaborated workflows several hundred operators. Therefore data analyst carefully select among operators onesmeaningfully combined address his/her knowledge discovery problem. However, evensophisticated data miner overwhelmed complexity modeling,rely his/her experience biases well thorough experimentationhope finding best operator combination. advance new generationKDSS provide even advanced functionalities, becomes important provideautomated support user workflow modeling process, issueidentified one top-ten challenges data mining (Yang & Wu, 2006).2. State Art DM Workflow Design Supportlast decade, rather limited number systems proposed provideautomated user support design DM workflows. Bernstein, Provost, Hill (2005)propose ontology-based Intelligent Discovery Assistant (ida) plans valid DM workflows valid sense executed without failure according basicdescriptions input dataset attribute types, presence missing values, numberclasses, etc. describing DM ontology input conditions output effectsDM operators, according three main steps KD process, pre-processing, modeling post-processing (Fayyad, Piatetsky-Shapiro, & Smyth, 1996), ida systematicallyenumerates workflow planner possible valid operator combinations, workflows,fulfill data input request. ranking workflows computed accordinguser defined criteria speed memory consumption measuredpast experiments.Zakova, Kremen, Zelezny, Lavrac (2011) propose kd ontology support automatic design DM workflows relational DM. ontology, DM relational algorithmsdatasets modeled semantic web language OWL-DL, providing semanticreasoning inference querying DM workflow repository. Similar ida,kd ontology describes DM algorithms data input/output specifications.authors developed translator ontology representation Planning Domain Definition Language (PDDL) (McDermott et al., 1998), produceabstract directed-acyclic graph workflows using FF-style planning algorithm (Hoffmann,2001). demonstrate approach genomic product engineering (CAD) usecases complex workflows produced make use relational data structurebackground knowledge.recently, e-LICO project1 featured another ida built upon plannerconstructs DM plans following hierarchical task networks (HTN) planning approach.1. http://www.e-lico.eu606fiUsing Meta-mining Support DM Workflow Planning Optimizationspecification HTN given Data Mining Workflow (dmwf) ontology (Kietz,Serban, Bernstein, & Fischer, 2009). predecessors, e-LICO ida designedidentify operators whose preconditions met given planning step order planvalid DM workflows exhaustive search space possible DM plans.None three DM support systems discussed consider eventualperformance workflows plan respect DM task supposedaddress. example, goal plan/design workflows solve classificationproblem, would like consider measure classification performance, accuracy,deliver workflows optimize it. discussed DM support systems deliverextremely large number plans, DM workflows, typically ranked simpleheuristics, workflow complexity expected execution time, leaving userloss best workflow terms expected performance DM taskhe/she needs address. Even worse, planning search space largesystems even fail complete planning process, see example discussionKietz, Serban, Bernstein, Fischer (2012).considerable work tries support user, view performancemaximization, specific part DM process, modeling learning.number approaches proposed, collectively identified meta-learning (Brazdil,Giraud-Carrier, Soares, & Vilalta, 2008; Kalousis, 2002; Kalousis & Theoharis, 1999; Hilario, 2002; Soares & Brazdil, 2000). main idea meta-learning given newdataset system able rank pool learning algorithms respectexpected performance dataset. so, one builds meta-learning modelanalysis past learning experiments, searching associations algorithms performances dataset characteristics. However, already mentioned, meta-learningapproaches address learning/modeling part applicable completeprocess level.work Hilario, Nguyen, Do, Woznica, Kalousis (2011), first effortlift meta-learning ideas level complete DM workflows. proposed novel metalearning framework called meta-mining process-oriented meta-learning appliedcomplete DM process. associate workflow descriptors dataset descriptors,applying decision tree algorithms past experiments, order learn couplingsworkflows datasets lead high predictive performance. workflow descriptorsextracted using frequent pattern mining accommodating also background knowledge,given Data Mining Optimization (dmop) ontology, DM tasks, operators, workflows, performance measures relationships. However predictive performancesystem rather low, due limited capacity decision trees capturingrelations dataset workflow characteristics essentialperformance prediction.address limitation, presented work Nguyen, Wang, Hilario,Kalousis (2012b) approach learns called heterogeneous similarity measures,associating dataset workflow characteristics. learn similarity measuresdataset space, workflow space, dataset-workflow space. similaritymeasures reflect respectively: similarity datasets given similarityrelative workflow performance vectors workflows applied them;similarity workflows given performance based similarity different datasets;607fiNguyen, Hilario & Kalousisdataset-workflow similarity based expected performance latter appliedformer. However two meta-mining methods described limited selectfrom, rank, set given workflows according expected performance, i.e.cannot plan new workflows given input dataset.Retrospectively, presented work Nguyen, Kalousis, Hilario (2011)initial blueprint approach DM workflow planning view workflow performance optimization. suggested planner guided metamining model ranks partial candidate workflows planning step. also gavepreliminary evaluation proposed approach interesting results (Nguyen, Kalousis,& Hilario, 2012a). However, meta-mining module rather trivial, uses datasetpattern-based workflow descriptors nearest-neighbor search dataset descriptors identify similar datasets dataset want planworkflows. Within neighborhood, ranks partial workflows using supportworkflow patterns workflows perform best datasets neighborhood. pattern-based ranking workflows cumbersome heuristic;system learning associations dataset workflow characteristics explicitly optimize expected workflow performance, must guide workflowplanning. approach deployed Kietz et al. (2012).paper, follow line work first sketched work Nguyen et al.(2011). couple tightly together workflow planning meta-mining moduledevelop DM workflow planning system given input dataset designs workflowsexpected optimize performance given dataset. meta-miningmodule applies heterogeneous similarity learning method presented Nguyen et al.(2012b) learn associations dataset workflow descriptors lead optimalperformance. exploits learned associations guide planner workflowconstruction planning. best knowledge, first systemkind, i.e. able design DM workflows specifically tailoredcharacteristics input dataset view optimizing DM task performance measure.evaluate system number real world datasets show workflowsplans significantly better workflows delivered number baseline methods.rest paper structured follows. section 3, present global architecture system, together brief description planner. section 4describe detail meta-mining module, including dataset workflow characteristics uses, learning model, learned model used workflow planning.section 5, provide detailed experimental results evaluate approach differentsettings. Finally, conclude section 6.3. System Descriptionsection, provide general description system. start definingnotations use throughout paper give brief overviewdifferent components system. two important components, plannermeta-miner described subsequent sections (3.4 4 respectively).close section providing formal representation DM workflowsuse planner meta-miner.608fiUsing Meta-mining Support DM Workflow Planning OptimizationSymbolewl = [o1 , . . . , ol ]wcl = (I(p1 wl ), . . . , I(p|P | wl ))Txu = (x1 , . . . , xd )Tr(x, w)g= {o1 , . . . , }ClSlMeaningworkflow operator.workflow data type.ground DM workflow sequence l operators.fixed length |P |-dimensional vector descriptionworkflow wld-dimensional vector description dataset.relative performance rank w workflow xdatasetDM goal.HTN task.HTN method.HTN abstract operator n possible operators.set candidate workflows abstract operator O.set candidate workflows selected Cl .Table 1: Summary notations used.3.1 Notationsprovide basic notations subsequently introduce additional notations needed. use term DM experiment designate executionDM workflow w W dataset x X . describe dataset x d-dimensionalcolumn vector xu = (x1 , . . . xd )T ; describe detail section 4.1.1 dataset descriptions use. experiment characterized performance measure;example mining problem addressing classification problem oneperformance measure accuracy. performance measures workflowsapplied given dataset x extract relative performance rank r(x, w) R+workflow w x dataset. statistically comparing performancedifferences different workflows given dataset (more section 4.1.3).matrix X : n contains descriptions n datasets used training system. given workflow w two representations. wl denotel-length operator sequence constitutes workflow. Note different workflowsdifferent lengths, fixed length representation. wcl denotefixed-length |P |-dimensional binary vector representation w workflow; featurewcl indicates presence absence relational feature/pattern workflow.Essentially wcl propositional representation workflow; describedetail section 4.1.2 extract propositional representation. Finally W denotescollection workflows, used training system, Wcorresponding |P | matrix contains propositional representations. Depending context different workflow notations used interchangeably. Table 1summarizes important notations.3.2 System Architecture Operational Pipelineprovide Figure 1 high level architectural description system. three blueshaded boxes are: (i) Data Mining Experiment Repository (dmer) stores609fiNguyen, Hilario & Kalousisinput data3. User Interfacegoalinput MD1.input MDDMERoptimal plans 5.training MDMetaMinerIntelligent Discovery Assistant (IDA)(partial) candidate workflowsAIPlannerDM WorkflowOntology (DMWF)workflow ranking4.onlinemodemetaminedmodelofflinemode2.DM OptimizationOntology (DMOP)softwaredata flowFigure 1: meta-mining systems components pipeline.base-level resources, i.e. training datasets, workflows, well performance resultsapplication latter former; essentially dmer contains training dataused derive models necessary workflow planningdesign; (ii) user interface user interacts system, specifyingdata mining tasks input datasets (iii) Intelligent Discovery Assistant (ida)component actually plans data mining workflows optimizeperformance measure given dataset data mining task user providedinput. ida constitutes core system contains planning componentmeta-mining component interact closely order deliver optimal workflowsgiven input problem; describe two components detail sections 3.4 4respectively. system operates two modes, offline online mode. offlinemode meta-mining component analyses past base-level data mining experiments,stored dmer, learn meta-mining model associates dataset workflowcharacteristics view performance optimization. online mode meta-minerinteracts planner guide planning workflows using meta-miningmodel.go briefly different steps systems life cycle. firstneed collect dmer sufficient number base-level data mining experiments,i.e. applications different data mining workflows different datasets (step 1).experiments used step 2 meta-miner generate meta-mining model.precisely extract base level experiments number characteristicsdescribe datasets workflows performance latter achievedapplied former. meta-data meta-miner learns associations datasetworkflow characteristics lead high performance; learning heterogeneous similarity measure outputs high similarity workflow datasetformer expected achieve high performance applied latter(more details section 4.2).model learned, offline phase completed online phasestart. online phase system directly interacts user. given userselect data mining task g G, classification, regression, clustering, etc, well610fiUsing Meta-mining Support DM Workflow Planning Optimizationinput dataset task applied; might also specifynumber top-k optimal workflows planned (step 3). Given new taskinput dataset action transfered IDA ai-planner startsplanning process. step planning process ai-planner generates listvalid actions, partial candidate workflows, passes ranking meta-mineraccording expected performance given dataset, step 4. meta-minerranks using learned meta-mining model planning continues topranked partial candidate workflows data mining task resolved. endplanning process, top-k workflows presented user order givenexpected performance input dataset. greedy planning approachstep select top-k current solutions. principle let aiplanner first generate possible workflows meta-miner rank them.resulting plans would ranked according local greedy approach,would ranked globally thus optimally respect meta-mining model.However general feasible due complexity planning processcombinatorial explosion number plans.3.3 DM Workflow Representationgive formal definition DM workflow represent it. DMworkflows directed acyclic typed graphs (DAGs), nodes correspond operatorsedges nodes data input/output objects. fact hierarchical DAGssince dominating nodes/operators contain sub-workflows. typicalexample cross-validation operator whose control flow given parallel executiontraining sub-workflows, complex operator boosting. formally, let:set available operators appear DM workflow, e.g. classification operators, J48, SVMs, etc. also includes dominating operatorsdefined one sub-workflows dominate. operatordefined name labelling function (o), data types e Einputs outputs, direct sub-workflows dominating operator.E set available data types appear DM workflow, namelydata types various I/O objects appear DM workflowmodels, datasets, attributes, etc.graph structure DM workflow pair (O , E ), also contains subworkflows any. set vertices correspond operators usedDM workflow sub-workflow(s), E E set pairs nodes, (oi , oj ),directed edges, correspond data types output/input objects,passed operator oi operator oj . Following graph structure, topological sortDM workflow permutation vertices graph edge (oi , oj )implies oi appears oj , i.e. complete ordering nodes directedacyclic graph given node sequence:wl = [o1 , .., ol ]611(1)fiNguyen, Hilario & KalousisLegendinput / output edgessub input / output edgesXXEndRetrievebasic nodesoutputcomposite nodesresultX-ValidationSplittraining setJointest setWeight Information Gainweightsexample setApply ModelperformanceSelect Weightslabelled datatraining setPerformanceNaive BayesmodelFigure 2: Example DM workflow performance estimation combinationfeature selection classification algorithms.subscript wl denotes length l (i.e. number operators) topologicalsort. topological sort DM workflow structurally represented rooted,labelled ordered tree (Bringmann, 2004; Zaki, 2005), depth-first searchgraph structure maximum depth given expanding recursively subworkflows dominating operators. Thus topological sort workflow treerepresentation reduction original directed acyclic graph nodesedges fully ordered.example hierarchical DAG representing RapidMiner DM workflow givenFigure 2. graph corresponds DM workflow cross-validates feature selectionmethod followed classification model building step NaiveBayes classifier. XValidation typical example dominating operator workflowtwo basic blocks, training block arbitrary workflow receivesinput dataset outputs model, testing block receives input modeldataset outputs performance measure. particular, training subworkflow, feature weights computed InformationGain operator,number features selected SelectByWeights operator, followedfinal model building NaiveBayes operator. testing block, typicalsub-workflow consists application learned model testing setApplyModel operator, followed performance estimation given Performanceoperator. topological sort graph given ordered tree given Figure 3.612fiUsing Meta-mining Support DM Workflow Planning OptimizationRetrieve1WeightInformation Gain3SelectWeights4X-Validation2/8NaiveBayes5End9ApplyModel6Performance7Figure 3: topological order DM workflow given Figure 2.3.4 Workflow Planningworkflow planner use based work Kietz et al. (2009, 2012), designsDM workflows using hierarchical task network (HTN) decomposition crisp-dm process model (Chapman et al., 2000). However planner workflow enumerationgenerating possible plans, i.e. consider expected workflow performanceplanning process scale large set operators explodesearch space. order address limitations, presented preliminary resultsplanner coupled frequent pattern meta-mining algorithmnearest-neighbor algorithm rank partial workflows step workflow planning (Nguyen et al., 2011, 2012a). system also deployed Kietz et al. (2012).approach followed prioritize partial workflows according supportfrequent patterns contained achieved set workflows performedwell set datasets similar input dataset want planworkflow. However learning associations dataset workflowcharacteristics, approach follow here.section 3.4.1 briefly describe HTN planner Kietz et al. (2009, 2012);section 3.4.2 describe use prediction expected performancepartial-workflow applied given dataset guide HTN planner. givecomplete description meta-mining module learns associations datasetworkflow characteristics expected achieve high performance section 4.3.4.1 HTN PlanningGiven goal g G, AI-planner decompose top-down manner goalelements two sets, tasks methods . task achieve g,subset associated methods share data input/output(I/O) object specification address it. turn, methoddefines sequence operators, and/or abstract operators (see below), and/or sub-tasks,executed order achieve m. recursively expanding tasks, methodsoperators given goal g, AI-planner sequentially construct HTN planterminal nodes correspond operators, non-terminal nodes HTNtask method decompositions, dominating operators (X-Validation instancedominate training testing sub-workflows). example HTN plan given613fiNguyen, Hilario & KalousisEvaluateAttributeSelectionClassificationX-ValidationIn: DatasetOut: PerformanceAttributeSelectionClassificationTrainingClassificationTrainingAttributeSelectionTrainingAttributeWeightingOperatorIn: DatasetOut: AttributeWeightsSelectWeightsIn: DatasetIn: AttributeWeightsOut: DatasetPredictiveSupervisedLearnerIn: DatasetOut: PredictiveModelAttributeSelectionClassificationTestingModelApplicationApplyModelIn: DatasetIn: PredictiveModelOut: LabelledDatasetModelEvaluationIn: LabelledDatasetOut: PerformanceFigure 4: HTN plan DM workflow given Figure 2. Non-terminal nodesHTN tasks/methods, except dominating operator X-Validation. Abstractoperators bold simple operators italic, annotatedI/O specification.Figure 4. plan corresponds feature selection classification workflowFigure 2 exactly topological sort (of operators) given Figure 3.sets goals G, tasks , methods , operators O, relations,described dmwf ontology (Kietz et al., 2009). There, methods operatorsannotated pre- post-conditions used AI-planner.Additionally, set operators enriched shallow taxonomic viewoperators share I/O object specification grouped commonancestor:= {o1 , . . . , }(2)defines abstract operator, i.e. operator choice point alternative among setn syntactically similar operators. example, abstract AttributeWeighting operatorgiven Figure 4 include feature weighting algorithms InformationGainReliefF, similarly abstract Predictive Supervised Learner operator containclassification algorithms NaiveBayes linear SVM.HTN planner also plan operators applied attribute, e.g.continuous attribute normalization operator, discretization operator. uses cyclicplanning structures apply subsets attributes. case use attributegrouping functionality require use cyclic planning structures.precisely operator selected application applied appropriateattributes. Overall HTN grammar contains descriptions 16 different tasks614fiUsing Meta-mining Support DM Workflow Planning Optimization100 operators, planner plan. numbers limitedmodeling effort required describe tasks operators, inherentlimitation HTN grammar/planner. Kietz et al. (2012) developed moduleopen-source ontology editor Protege called E-ProPlan2 facilitate modellingnew operators describe task-method decomposition grammars DM problemsdmwf ontology.3.4.2 Workflow Selection TaskAI-planner designs construction HTN plan several partial workflows,derived substituting abstract operator one noperators. number planned workflows order several thousandsleave user loss workflow choose her/his problem;even worse, possible planning process never succeeds find valid planterminate due complexity search space.support user selection DM workflows, use post-planning approach designed workflows evaluated according evaluationmeasure order find k ones globally best given mining problem. However, global approach computational expensive planningphase mentioned above. Instead, follow planning approachlocally guide AI-planner towards design DM workflows optimizedgiven problem, avoiding thus need explore whole planning search space.Clearly, following local approach cost one pay potential reductionperformance since workflows explored evaluated, potentially missing goodones due greedy nature plan construction.adopt heuristic hill climbing approach guide planner. abstractoperator need determine n candidate operators expectedachieve best performance given dataset. formally, define by:Cl := {wlo = [wl1 Sl1 |o O]}kn(3)set k n partial candidate workflows length l generatedexpansion abstract operator adding one n candidate operatorsone k candidate workflows length l 1 constitute set Sl1 workflowsselected previous planning step (S0 empty set see below). let xu vectordescription input dataset want plan workflows addressdata mining goal g optimize performance measure. Moreover let wclo binaryvector provides propositional representation wlo workflow respectset generalized relational frequent workflow patterns contains3 . constructset Sl selected workflows current planning step according to:Sl := {arg max r(xu , wclo |g)}k(4){wlo Cl }2. Available http://www.e-lico.eu/eproplan.html3. provide detailed description extract wclo descriptors section 4.1.2,moment note propositional representations fixed length representations,depend l.615fiNguyen, Hilario & Kalousisr(xu , wclo |g) estimated performance workflow propositional description wclo applied dataset description xu , i.e. planning stepselect best current partial workflows according estimated expected performance. meta-mining model, learn past experiments, deliversestimations expected performance. section 4.2 describe derivemeta-mining models use get estimates expected performance.stress producing performance estimate r(xu , wclo |g)meta-mining model uses workflow description wclo , candidate operator descriptions. pattern-based descriptions capture dependencies interactionsdifferent operators workflows (again wclo representationsection 4.1.2). rather crucial point since well known fact construct data mining workflows need consider relations biases differentalgorithms use within them, bias combinations better others.pattern based descriptions use provide precisely type information, i.e.information operator combinations appearing within workflows.next section provide complete description meta-miner module,including characterize datasets, workflows performance latter appliedformer, course learn meta-mining models use planning.4. Meta-Minermeta-miner component operates two modes. offline mode learnspast experimental data meta-mining model provides expected performanceestimates r(xu , wclo |g). on-line mode interacts AI-planner stepplanning process, delivering r(xu , wclo |g) estimates used plannerselect workflows planning step according Eq.(4).rest section organised follows. subsection 4.1.1 explaindescribe datasets, i.e. derive xu dataset descriptors; subsection 4.1.2derive propositional representation wclo data mining workflows subsection 4.1.3 rank workflows according performance given dataset, i.e.r(xu , wclo |g). Finally subsection 4.2 explain build models past miningexperiments provide expected performance estimations r(xu , wclo |g)use models within planner.4.1 Meta-Data Performance Measuressection describe meta-data, namely dataset workflow descriptions,performance measures used meta-miner learn meta-mining modelsassociate dataset workflow characteristics view planning DM workflowsoptimize performance measure.4.1.1 Dataset Characteristicsidea characterize datasets full-fledged research problem earlyinception meta-learning (Michie, Spiegelhalter, Taylor, & Campbell, 1994;Kopf, Taylor, & Keller, 2000; Pfahringer, Bensusan, & Giraud-Carrier., 2000; Soares &616fiUsing Meta-mining Support DM Workflow Planning OptimizationBrazdil, 2000; Hilario & Kalousis, 2001; Peng, Flach, Soares, & Brazdil, 2002; Kalousis,Gama, & Hilario, 2004). Following state-of-art dataset characteristics, characterizedataset x X three following groups characteristics.Statistical information-theoretic measures: group refers data characteristicsdefined STATLOG (Michie et al., 1994; King, Feng, & Sutherland, 1995)METAL projects4 (Soares & Brazdil, 2000), includes number instances,number classes, proportion missing values, proportion continuous / categoricalfeatures, noise signal ratio, class entropy, mutual information. mainly describeattribute statistics class distributions given dataset sample.Geometrical topological measures: group concerns new measures trycapture geometrical topological complexity class boundaries (Ho & Basu,2002, 2006), includes non-linearity, volume overlap region, maximum Fishersdiscriminant ratio, fraction instance class boundary, ratio average intra/interclass nearest neighbour distance.Landmarking model-based measures: group related measures assertedfast machine learning algorithms, called landmarkers (Pfahringer et al., 2000),derivative based learned models (Peng et al., 2002), includes errorrates pairwise 1p values obtained landmarkers 1NN DecisionStump,histogram weights learned Relief SVM. extended last groupnew landmarking methods based weight distribution feature weightingalgorithms Relief SVM build twenty different histogramrepresentations discretized feature weights.Overall, system makes use total = 150 numeric characteristics describedataset. denote vectorial representation dataset x X xu .far exhaustive dataset characteristics used, includingcharacteristics subsampling landmarks (Leite & Brazdil, 2010). main goalwork produce comprehensive set dataset descriptors design DMworkflow planning system given set dataset characteristics, coupled workflowdescriptors, plan DM workflows optimize performance measure.4.1.2 Workflow Characteristicsseen section 3.3 workflows graph structures quite complexcontaining several nested sub-structures. often difficult analyzespaghetti-like structure also informationsubtask addressed workflow component (Van der Aalst & Giinther,2007). Process mining addresses problem mining generalized patternsworkflow structures (Bose & der Aalst, 2009).characterize DM workflows follow process mining like approach;extract generalized, relational, frequent patterns tree representations,use derive propositional representations them. possible generalizations4. http://www.metal-kdd.org/617fiNguyen, Hilario & KalousisDM-AlgorithmDataProcessingAlgorithmPredictiveModellingAlgorithmFeatureWeightingAlgorithmClassificationModellingAlgorithmLearnerFreeFWAlgorithmUnivariateFWAlgorithmMultivariateFWAlgorithmMissingValuesTolerantAlgorithmIrrelevantTolerantAlgorithmExactCOSBasedAlgorithmC4.5NaiveBayesSVMEntropyBasedFWAlgorithmIGReliefFis-followed-byis-implemented-byFigure 5: part dmops algorithm taxonomies. Short dashed arrows representis-followed-by relation DM algorithms, long dashed arrows represent is-implemented-by relation DM operators DM algorithms.(a)(b)X-Validation(c)X-ValidationFeatureWeightingAlgorithmClassificationModelingAlgorithmFeatureWeightingAlgorithmClassificationModelingAlgorithmUnivariateFWAlgorithmIrrelevantTolerantAlgorithmLearnerFreeFWAlgorithmMissingValuesTolerantAlgorithmEntropyBasedFWAlgorithmX-ValidationFeatureWeightingAlgorithmClassificationwAlgorithmExactCOSBasedAlgorithmFigure 6: Three workflow patterns cross-level concepts. Thin edges depict workflowdecomposition; double lines depict dmops concept subsumption.described domain knowledge which, among knowledge, given datamining ontology. use Data Mining Optimization (dmop) ontology (Hilarioet al., 2009, 2011). Briefly, ontology provides formal conceptualization DMdomain describing DM algorithms defining relations terms DM tasks,models workflows. describes learning algorithms C4.5, NaiveBayes SVM,according learning behavior bias/variance profile, sensitivitytype attributes, etc. instance, algorithms cited tolerant irrelevant attributes, C4.5 NaiveBayes algorithms tolerant missing values,whereas SVM NaiveBayes algorithms exact cost function. Algorithm characteristics families classified taxonomies dmop primitive618fiUsing Meta-mining Support DM Workflow Planning Optimizationconcept DM-Algorithm. Moreover, dmop specifies workflow relations, algorithm order,is-followed-by relation relates workflow operators DM algorithmsis-implemented-by relation. Figure 5 shows snapshot dmops algorithm taxonomies ground operators bottom related DM algorithms implement.mine generalized relational patterns DM workflows, follow methodpresented Hilario et al. (2011). First, use dmop ontology annotate set Wworkflows. Then, extract set generalized patterns using frequent patternmining algorithm. Concretely, operator contained parse tree training DMworkflow wl W , insert tree branch operator taxonomic concepts,ordered top bottom, implemented operator, givendmop. result new parse tree additional nodes dmopsconcepts. call parse tree augmented parse tree. reorder nodesaugmented parse tree satisfy dmops algorithm taxonomies relations.example feature selection algorithm typically composed feature weighting algorithmfollowed decision rule selects features according heuristics. resultset augmented reordered workflow parse trees. representation,apply tree mining algorithm (Zaki, 2005) extracts set P frequent patterns.pattern corresponds tree appears frequently within augmented parse trees;mine patterns support higher equal five. principle golow support one, exploding dimensionality description workflows,probably features poor discriminatory power. Nevertheless since metamining models rely metric learning, able learn importance differentmeta-features, would able cope also scenario. Note extractworkflow characteristics could used different techniques graphmining directly graph structures defined workflows ontology,main reason computational cost latter approaches, wellfact frequent pattern mining propositionalization known work well.Figure 6 give examples mined patterns. Note extracted patternsgeneralized, sense contain entities defined different abstraction levels,provided dmop ontology. relational describerelations, order relations, structures appear within workflow,also contain properties entities described dmop ontology.example pattern (c) Figure 6 states feature weighting algorithm (abstractconcept) followed (relation) classification algorithm exact cost function(property), within cross-validation.use set P frequent workflow patterns describe DM workflow wl Wpatterns p P wl workflow contains. propositional descriptionworkflow wl given |P |-length binary vector:wcl = (I(p1 wl ), . . . , I(p|P | wl ))T {0, 1}|P |(5)denotes induced tree relation (Zaki, 2005) I(pi wl ) returns onefrequent pattern, pi , appears within workflow zero otherwise.use propositional workflow representation together tabular representation datasets characteristics learn meta-mining models describenext section. Although could used tree even graph properties represent619fiNguyen, Hilario & Kalousisworkflows, propositionalization standard approach used extensively successfullylearning problems learning instances complex structures (Kramer, Lavrac,& Flach, 2000).Also, propositional workflow representation easily deal parameter valuesdifferent operators appear within workflows. so, discretizerange values continuous parameter ranges low, medium, high,ranges depending nature parameter, treat discretized valuessimply property operators. resulting patterns parameter-aware;include information parameter range mined operatorsused support also parameter setting planning DM workflows.However within paper explore possibility.4.1.3 Performance-Based Ranking DM Workflowcharacterize performance number workflows applied given datasetuse relative performance rank schema derive using statistical significancetests. Given estimations performance measure different workflowsgiven dataset use statistical significance test compare estimated performancesevery pair workflows. within given pair one workflows significantlybetter gets one point gets zero points.significance difference workflows get half point. final performance rankworkflow given dataset simply sum points pairwiseperformance comparisons, higher better. denote relative performancerank workflow wc applied dataset xu r(xu , wc ). Note workflowapplicable, executed, dataset x, set rank score default value zeromeans workflow appropriate (if yet executed) given dataset.planning goal g classification task, use evaluation measureexperiments classification accuracy, estimated ten-fold cross-validation,significance testing using McNemars test, significance level 0.05.next section describe build meta-mining modelspast data-mining experiments using meta-data performance measuresdescribed far use models support DM workflow planning.4.2 Learning Meta-mining Models Workflow Planningstarting describe detail build meta-mining models let us takestep back give abstract picture type meta-mining settingaddress. previous sections, described two types learning instances: datasetsx X workflows w W. Given set datasets set workflows storeddmer, meta-miner build these, two training matrices X W.X : n dataset matrix, ith row description xui ith dataset.W : |P | workflow matrix, j th row description wcj jth workflow.also preference matrix R : n m, Rij = r(xui , wcj ), i.e. givesrelative performance rank workflow wj applied dataset xi respectworkflows. see Rij measure appropriateness matchwj workflow xi dataset. ith line R matrix contains vector620fiUsing Meta-mining Support DM Workflow Planning Optimizationrelative performance ranks workflows applied xui dataset.meta-miner take input X, W R matrices output modelpredicts expected performance, r(xu , wc ), workflow w applied dataset x.construct meta-mining model using similarity learning, exploiting two basicstrategies initially presented context DM workflow selection (Nguyen et al., 2012b).give high level presentation them, details interested userrefer original paper. first strategy learn homogeneous similaritymeasures, measuring similarity datasets similarity workflows, usederive r(xu , wc |g) estimates. second learn heterogeneous similarity measuresdirectly estimate appropriateness workflow dataset, i.e. producedirect estimates r(xu , wc |g).4.2.1 Learning Homogeneous Similarity Measuresgoal provide meta-mining models good predictors performanceworkflow applied dataset. simplest approach want learn goodsimilarity measure dataset space deem two datasets similar setworkflows applied result similar relative performance, i.e.order workflows according performance achieve datasettwo datasets similar workflow orders similar. Thus learned similaritymeasure dataset space good predictor similarity datasetsdetermined relative performance order workflows. completelysymmetrical manner consider two workflows similar achieve similarrelative performance scores set datasets. Thus case workflows learnsimilarity measure workflow space good predictor similarityrelative performance scores set datasets.Briefly, learn two Mahalanobis metric matrices, MX , MW , datasetsworkflows respectively, optimizing two following convex metric learning optimizationproblems:min F1 = ||RRT XMX XT ||2F + tr(MX )MXs.t.(6)MX 0min F2 = ||RT R WMW WT ||2F + tr(MW )MWs.t.(7)MW 0||.||F Frobenius matrix norm, tr() matrix trace, 0 parametercontrolling trade-off empirical error metric complexity control overfitting. RRT : n n matrix reflects similarity relative workflow performancevectors different dataset pairs learned dataset similarity metricreflect. RT R : matrix gives respective similarities workflows.details learning problem solve it, see work Nguyen et al.(2012b).621fiNguyen, Hilario & KalousisNote far model computes expected relative performancer(xu , wclo ). case homogeneous metric learning compute on-linemode planning phase; describe right away followingparagraph.Planning homogeneous similarity metrics (P1) use twolearned Mahalanobis matrices, MX , MW , compute dataset similarity workflow similarity finally compute estimates r(xu , wclo ) planningstep.Concretely, prior planning determine similarity input dataset xu (forwant plan optimal DM workflows) training datasets xui X usingMX dataset metric measure dataset similarities. Mahalanobis similaritytwo datasets, xu , xui , givensX (xu , xui ) = xTu MX xui(8)Then, planning planning step determine similarity candidateworkflow wlo Cl training workflows wcj W,sW (wclo , wcj ) = wcTlo MW wcj .(9)Finally derive r(xu , wclo ) estimate weighted average elementsR matrix. weights given similarity input dataset xutraining datasets, similarities candidate workflow wclo trainingworkflows. formally expected rank given by:PPwcj W xui wcj r(xui , wcj |g)xui XPP(10)r(xu , wclo |g) =wc W xui wcjxu Xjxui wcj Gaussian weights given xui = exp(sX (xu , xui )/x ) wcj =exp(sW (wclo , wcj )/w ); x w kernel widths control size neighborsdata workflow spaces respectively (Smart & Kaelbling, 2000; Forbes & Andre,2000).Using rank performance estimates delivered Eq.(10), select planning step best candidate workflows set, Sl , according Eq.(4). call resultingplanning strategy P1. P1 expected performance set selected candidateworkflows Sl greedily increases deliver k DM complete workflowsexpected achieve best performance given dataset.4.2.2 Learning Heterogeneous Similarity MeasureP1 planning strategy makes use two similarity measures learned independently other, one defined feature space. simplisticassumption model interactions workflows datasets,know certain types DM workflows appropriate datasets certaintypes characteristics. order address limitation, define heterogeneousmetric learning problem directly estimate similarity/appropriateness622fiUsing Meta-mining Support DM Workflow Planning Optimizationworkflow given dataset given r(xu , wc ) relative performancemeasure.Since learning Mahalanobis metric equivalent learning linear transformationrewrite two Mahalanobis metric matrices described previously MX = UUTMW = VVT . U : V : |P | respective linear transformation matricesdimensionality = min(rank(X), rank(W)).learn heterogeneous similaritymeasure datasets workflows using two linear transformations solvefollowing optimization problem:min F4 = ||R XUVT WT ||2F + ||RRT XUUT XT ||2FU,V+ ||RT R WVVT WT ||2F +(11)(||U||2F + ||V||2F )2using alternating gradient descent algorithm, first optimize U keepingV fixed vice versa. optimization problem non-convex algorithmconverge local minimum. first term similar low-rank matrix factorizationSrebro, Rennie, Jaakkola (2005). However factorization learnfunction dataset workflow feature spaces result addresssamples training instances, also known cold start problemrecommendation systems. case DM workflow planning problem strongrequirement need able plan workflows datasets neverseen training, also able qualify workflows also seentraining. second third terms define metrics reflect performancebased similarities datasets workflows respectively (along lines homogeneousmetrics given previously), together give directly similarity/appropriatenessDM workflow dataset estimating expected relative predictive performanceas:r(xu , wclo |g) = xu UVT wcTlo(12)see heterogeneous similarity metric performing projection datasetworkflow spaces common latent space compute standard similarityprojections. details, see work Nguyen et al. (2012b).Planning heterogeneous similarity measure (P2) Planning heterogeneous similarity measure, strategy denote P2, much simpler planning homogeneous similarity measures. Given input dataset described xustep planning make use relative performance estimate r(xu , wclo |g)delivered Eq.(12) select set best workflows Sl set partial workflows Cl using selection process described Eq.(4). Unlike planning strategy P1computes r(xu , wclo |g) weighted average help two independently learned similarity metrics, P2 relies heterogeneous metric directly computesr(xu , wclo |g), modeling thus explicitly interactions dataset workflow characteristics.note P1 P2 planning strategies able constructworkflows even pools ground operators include operatorsnever experimented baseline experiments, provided operators well623fiNguyen, Hilario & Kalousisdescribed within dmop. meta-mining models planner usesprioritize workflows rely wclo descriptions workflow generalizeddescriptions workflows operators.next section evaluate ability two planning strategiesintroduced plan DM workflows optimize predictive performance comparenumber baseline strategies different scenarios.5. Experimental Evaluationevaluate approach data mining task classification. reasonsrather practical. Classification supervised task meansground truth compare results produced classificationalgorithm, using different evaluation measures accuracy, error, precision etc;mining tasks clustering, performance evaluation comparison bitproblematic due lack ground truth. extensively studied,extensively used many application fields, resulting plethora benchmark datasets,easily reuse construct base-level experiments well evaluatesystem. Moreover, extensively addressed context meta-learning,providing baseline approaches compare approach. Finally approachrequires different algorithms operators use well describeddmop ontology. Due historical predominance classification task algorithmswell extensive use real world problems, started developing dmop them;result task classification corresponding algorithms well described.said this, emphasize approach limited taskclassification. applied mining task define evaluationmeasure, collect set benchmark datasets perform base-level experiments, provide descriptions task respective algorithms dmop.train evaluate approach, collected set benchmark classificationdatasets. applied set classification data mining workflows.base-level experiments learn meta-mining models usedplanner plan data mining workflows. challenge system new datasetsused training meta-mining models, datasetsplan new classification workflows achieve high level predictive performance.explore two distinct evaluation scenarios. first one, constrainsystem plans DM workflows selecting operators restricted operatorpool, namely operators experimented base-level experiments.Thus operators characterized dmop ontology testedbase-level experiments; call tested operators. second scenarioallow system also choose operators never experimentedcharacterized dmop ontology; call operators untestedoperators. goal second scenario evaluate extend systemeffectively use untested operators workflows designs.624fiUsing Meta-mining Support DM Workflow Planning OptimizationTypeFS/testedFS/testedFS/testedAbbr.IGCHIRFParameters-#features selected k = 10-#features selected k = 10-#features selected k = 10SVMRFE-#features selected k = 10FS/untestedCL/testedOperatorInformation GainChi-SquareReliefFRecursive featureelimination SVMInformation Gain RatioOne-nearest-neighborIGR1NN-#features selected k = 10CL/testedC4.5C4.5CL/testedCARTCARTFS/testedCL/testedCL/testedNaiveBayes normaldensity estimationLogistic regressionLinear kernel SVMCL/testedGaussian kernel SVMSVMrCL/untestedCL/untestedCL/untestedCL/untestedLinear discriminant analysisRule inductionRandom decision treePerceptron neural networkLDARipperRDTNNetCL/tested-pruning confidence C-min. inst. per leaf-pruning confidence C-min. inst. per leaf= 0.25=2= 0.25=2NBLRSVMl-complexity C = 1-complexity C = 1-gamma = 0.1Table 2: Table operators used design DM workflows 65 datasets. typecorresponds feature selection (FS) classification (CL) operators. operators experimented marked tested, otherwise untested.5.1 Base-Level Datasets DM Workflowsconstruct base-level experiments, collected 65 real world datasets genomicmicroarray proteomic data related cancer diagnosis prognosis, mostlyNational Center Biotechnology Information5 . typical datasets,datasets use characterized high-dimensionality small sample size,relatively low number classes, often two. average 79.26 instances,15268.57 attributes, 2.33 classes.build base-level experiments, applied datasets workflows consisted either single classification algorithm, combination feature selectionclassification algorithm. Although HTN planner use (Kietz et al., 2009, 2012)able generate much complex workflows, 16 different tasks, 100operators, limit planning classification, feature selectionclassification, workflows simply respective tasks, algorithms operatorswell annotated dmop. annotation important characterizationworkflows construction good meta-mining models used guideplanning. Nevertheless, system directly usable planning scenarios com5. http://www.ncbi.nlm.nih.gov/625fiNguyen, Hilario & Kalousisplexity, describe HTN grammar, provided appropriate tasks,algorithms operators annotated dmop ontology.used four feature selection algorithms together seven classification algorithmsbuild set base-level training experiments. given Table 2, notedtested. mentioned previously, also plan operators parametersdiscretizing range values parameters treating propertiesoperators. Another alternative use inner cross-validation automatically selectset parameter values; strictly speaking, case, would selectingstandard operator cross-validated variant. Nevertheless, would incur significantcomputational cost.Overall, seven workflows contained classification algorithm,28 workflows combination feature selection classification algorithm,resulting total 35 workflows applied 65 datasets corresponds 2275 baselevel DM experiments. performance measure use accuracy estimateusing ten-fold cross-validation. algorithms, used implementations providedRapidMiner DM suite (Klinkenberg et al., 2007).already said, two evaluation settings. first, Scenario 1, constrainsystem plan workflows using tested operators. second, Scenario 2,allow system select also untested operators. additional operatorsalso given Table 2, denoted untested. total number possible workflowssetting 62.5.2 Meta-learning & Default Methodscompare performance system two baseline methods defaultstrategy. two baseline methods simple approaches fall classic metalearning stream instead selecting individual algorithms selectworkflows. Thus cannot plan DM workflows used settingworkflows choose seen model construction phase.first meta-learning method use, call Eucl, standardapproach meta-learning (Kalousis & Theoharis, 1999; Soares & Brazdil, 2000),makes use Euclidean based similarity dataset characteristics select Nsimilar datasets input dataset xu want select workflowsaverages workflow rank vectors produce average rank vector:N1 Xrxui , xui {arg max xTu xui }NNxui X(13)uses order different workflows. Thus method simply ranks workflowsaccording average performance achieve neighborhood inputdataset. second meta-learning method call Metric makes use learneddataset similarity measure given Eq.(8) select N similar datasetsinput dataset averages well respective workflow rank vectors:N1 Xrxui , xui {arg max xTu MX xui }NNxui X626(14)fiUsing Meta-mining Support DM Workflow Planning Optimization0.80.60.40.2CHI+C45RF+NBNSVMRFE+C45CHI+CARTRF+C45SVMr1NNIG+C45RF+1NNSVMRFE+CARTC45CHI+SVMrRF+CARTCHI+SVMlIG+CARTRF+SVMrRF+SVMlSVMRFE+1NNCHI+1NNIG+1NNSVMRFE+SVMrCARTCHI+NBNIG+SVMrSVMRFE+LRCHI+LRRF+LRSVMRFE+NBNIG+SVMlSVMlIG+LRSVMRFE+SVMlNBNIG+NBNLR0.0Figure 7: Percentage times workflow among top-5 workflows different datasets.default recommendation strategy, simply use average rxui workflowrank vectors collection training datasets:1Xrxui , xui Xnn(15)rank select workflows. note rather difficult baselinebeat. see case plot Figure 7 percentage times35 DM workflows appears among top-5 worfklows 65 datasets. topworkflow, consisting LR algorithm, among top-5 80%datasets. next two workflows, NBN IG NBN, among top-5almost 60% datasets. words select top-5 workflows using defaultstrategy roughly 80% datasets LR correctly them,NBN IG NBN percentage around 60%. Thus set datasetquite similar respect workflows perform better them, makingdefault strategy rather difficult one beat.5.3 Evaluation Methodologyestimate performance planned workflows evaluation scenariosuse leave-one-dataset-out, using time 64 datasets build meta-miningmodels one dataset plan.evaluate method measuring well list, L, top-k ranked workflows, delivers given dataset, correlates true list, , top-k ranked627fiNguyen, Hilario & Kalousisworkflows dataset using rank correlation measure. place true quotesgeneral case, i.e. restrict choice operators specificset, cannot know true best workflows unless exhaustively examineexponential number them, however since select restricted list operatorsset best. precisely, measure rank correlationtwo lists L , use Kendall distance p penalty, denoteK (p) (L, ) (Fagin, Kumar, & Sivakumar, 2003). Kendall distance gives numberexchanges needed bubble sort convert one list other. assigns penaltyp pair workflows one workflow one list other;set p = 1/2. K (1/2) (L, ) normalized, propose define normalizedKendall similarity Ks(L,T) as:1K ( 2 ) (L, )Ks(L, ) = 1u(16)1(2)Pktakes values [0, 1]. u upper bound K (L, ) given u = 0.5k(5k + 1)2 i=1 i, derived direct application lemma 3.2 work Fagin et al. (2003),assume two lists share element. qualify method,m, including two baselines, Kendall similarity gain, Kg(m), i.e. gain (or loss)achieves respect default strategy given datasets, compute as:Kg(m)(L, ) =Ks(m)(L, )1Ks(def )(L, )(17)method, report average Kendall similarity gain overall datasets,Kg(m). Note that, Scenario 1, default strategy based average ranks35 workflows. Scenario 1, default strategy based average ranks 62workflows, also experiment order set baseline.addition see well top-k ranked list workflows, given methodsuggests given dataset, correlates true list, also compute average accuracytop-k workflows suggests achieve given dataset, report averageoverall datasets.5.4 Meta-mining Model Selectioniteration leave-one-dataset-out evaluation planning performance,rebuild meta-mining model tune parameter Mahalanobis metriclearning using inner ten-fold cross-validation; select value maximizesSpearman rank correlation coefficient predicted workflow rank vectorsreal rank vectors. heterogenous metric, used parameter setting definedNguyen et al. (2012b). two meta-learning methods, fixed number Nnearest neighbors five, reflecting prior belief appropriate neighborhood size.planning, set manually dataset kernel width parameter kx = 0.04workflow kernel width parameter kw = 0.08 result small dataset workflowneighborhoods respectively. Again, two parameters tuned simply setprior belief respective neighborhood size.628fiUsing Meta-mining Support DM Workflow Planning Optimization(b) Scenario 2, tested untested operators.0.101015202530Kg0.000.0550.100.150.150P2P1def620.050.000.05P2P1MetricEucldef320.10Kg0.050.10(a) Scenario 1, tested operators.350k5101520253035kFigure 8: Average correlation gain Kg different methods baseline65 bio-datasets. x-axis, k = 2 . . . 35, number top-k workflowssuggested user. P1 P2 two planning strategies. MetricEucl baseline methods defX default strategy computed setX workflows.5.5 Experimental Resultsfollowing sections give results experimental evaluation differentmethods presented far two evaluation scenarios described above.5.5.1 Scenario 1, Building DM workflows Pool TestedOperatorsscenario, evaluate quality DM workflows constructed twoplanning strategies P1 P2 compare two baseline methods welldefault strategy. leave-one-dataset-out evaluate workflowrecommendations given method. Figure 8(a) give average Kendall gainKg method default strategy compute top-k listsk = 2, . . . , 35. Clearly P2 strategy one gives largest improvementsrespect default strategy, 5% 10% gain, compared method.establish statistical significance results k, counting numberdatasets method better/worse default, using McNemarstest. summarize Figure 9 statistical significance results given p-valuesdifferent ks give detailed results Table 4 appendix methodsk = 2 . . . 35. see P 2 far best method significantly betterdefault 16 34 values k, close significant (0.05 < p-value 0.1)ten 34 times never significantly worse. methods, P2managed beat default 2 34 cases k.629fiNguyen, Hilario & Kalousis0.5510152025303551015202530Metric (0 Wins/0 Losses)Eucl (0 Wins/0 Losses)350.50.01.01.00.50.0pvalue0.51.0k1.0k0.5pvalue0.0pvalue1.00.50.01.00.5pvalue0.51.0P1 (2 Wins/0 Losses)1.0P2 (16 Wins/0 Losses)51015202530355k101520253035kFigure 9: P-values McNemars test number times Kendal similaritymethod better default given k, Scenario 1. positive pvalue means wins losses, negative opposite. solid linesp = +/ 0.05, dash-dotted p = +/ 0.1. X Wins/ Lossesheader indicates number times k = 3..35 methodsignificantly better/worse default.examine average accuracy top-k workflows suggestedmethod achieve, advantage P2 even striking. average accuracy 1.25%1.43% higher default strategy, k = 3 k = 5 respectively, seeTable 3(a). k = 3, P2 achieves higher average accuracy default 3965 datasets, under-performs compared default 20. UsingMcNemars test statistical significance 0.02, i.e. P2 significantly betterdefault strategy comes average accuracy top k = 3 workflowsplans; results similar k = 5. fact eight top-k lists, k = 3 . . . 10, P2significantly better default five values k, close significantly better once,never significantly worse. higher values k, k = 11 . . . 35, significantlybetter 11 times, close significantly better three times, never significantly worse.stops significantly better k > 30. large k values, average takenalmost workflows, thus expect important differences listsproduced different methods. Figure 10, visualize statistical significanceresults different values k = 3 . . . 35 give detailed results Table 5Appendix.630fiUsing Meta-mining Support DM Workflow Planning Optimization0.5510152025303551015202530Metric (4 Wins/0 Losses)Eucl (0 Wins/0 Losses)350.50.01.01.00.50.0pvalue0.51.0k1.0k0.5pvalue0.0pvalue1.00.50.01.00.5pvalue0.51.0P1 (6 Wins/0 Losses)1.0P2 (16 Wins/0 Losses)51015202530355k101520253035kFigure 10: P-values McNemars test number times Average Accuracy method better default given k, Scenario 1.figure interpretation Figure 9P1 never significantly better default k = 3 . . . 10, k = 11 . . . 35,significantly better nine values k, close significantly better three times,close significantly worse once. Metric baseline never significantly betterdefault k = 3 . . . 10, k = 11 . . . 35, significantly better four valuesk,and close significantly better four times. results EC quite poor. termsaverage accuracy, similar default, terms number timesperforms better default, cases, less numbertimes performs worse default. Figure 10 give resultsstatistical significance results different values k detailed resultsTable 5 appendix. P2 method directly learns exploitsworkflow planning associations dataset workflow characteristicsexperimental results clearly demonstrate strategy best pays off.5.5.2 Scenario 2: Building DM workflows Pool TestedNon-Tested Operatorssecond scenario, evaluate performance two planning strategies, P1P2, pool available operators planning limitedoperators already experimented base-level experimented with,extended include additional operators described dmop ontology.631fiNguyen, Hilario & Kalousis(a) Scenario 1, tested operatorsP2P1MetricEucldef35Avg. Acc0.79880.78860.78610.78290.7863k=3W/L39/2026/3825/3830/32p value+0.020.170.130.90Avg. Acc0.79250.78550.78300.77820.7787k=5W/L41/2135/2832/3332/33p value+0.020.451.001.00(b) Scenario 2, tested untested operatorsP2P1def62Avg. Acc0.79740.78900.7867k=3W/L39/2429/34p value0.080.61Avg. Acc0.79070.78530.7842k=5W/L34/2931/34p value0.610.80Table 3: Average accuracy top-k workflows suggested method. W indicatesnumber datasets method achieved top-k average accuracy largerrespective default, L number datasets smallerdefault. p value result McNemars statistical significancetest; + indicates method statistically better default.already described exact setting section 5.1; reminder numberpossible workflows 62. before, estimate performances using leave-onedataset-out. Note two baseline methods, Metric Eucl, applicablesetting, since deployed workflows alreadyexperimented baseline experiments. Here, already explained section 5.3,default strategy correspond average rank 62 possible workflowsdenote def62. Note highly optimistically-biased default methodsince relies execution 62 possible workflows base-level datasets, unlikeP1 P2 get see 35 workflows model building, operatorstherein, plan larger pool.Figure 8(b), give average Kendall gain Kg P1 P2 def62baseline. Similarly first evaluation scenario, P2 advantage P1 sincedemonstrates higher gains default. Note though performance gainssmaller previously. terms number k values P2(close be) significantly better default, six eight,different k = 2 . . . 35. def62 significantly better P2 closesignificantly better. concerns P1, significant differenceperformance def62, value k. values k > 30, P2 systematicallyunder-performs compared def62, due advantage latter comesseeing performance 62 workflows base-level dataset. visualizestatistical significance results top row Figure 11, give detailed resultsTable 6 Appendix values k = 2 . . . 35.632fiUsing Meta-mining Support DM Workflow Planning Optimization0.510152025303551015202530kP2 (1 Wins/13 Losses)P2 (0 Wins/3 Losses)350.50.00.51.01.00.50.0pvalue0.51.0k1.05pvalue0.0pvalue1.00.50.01.00.5pvalue0.51.0P1 (0 Wins/0 Losses)1.0P2 (4 Wins/1 Loss)51015202530355k101520253035kFigure 11: Top row p-values McNemars test number times Kendallsimilarity method better default given k, Scenario 2. Bottomrow Average Accuracy. figure interpretation Figure 9.concerns performance P2 respect average accuracytop-k workflows suggests, slight advantage def62 smallvalues k, four. significantly better compared def62 once, k = 4.k = 5 17, two methods significant difference, k = 18 . . . 35 P2worse, 13 times significant manner. P1 picture slightly different, averageaccuracy significantly different def62, exception three k valuessignificantly worse. visualize statistical significance results bottomrow Figure 11, give detailed results Table 7. seems fact P2learns directly associations datasets workflow characteristics putsdisadvantage want plan operators tested trainingphase. scenario, P1 strategy weights preferences dataset similarityworkflow similarity seems cope better untested operators. Neverthelessstill possible outperform default strategy significant manner, keepinghowever mind def62 optimistic default strategy basedexperimentation possible workflows training dataset.5.6 Discussionprevious sections, evaluated two workflow planning strategies two settings:planning tested operators, planning tested untested operators.633fiNguyen, Hilario & Kalousisfirst scenario, P2 planning strategy makes use heterogeneous metriclearning model, directly connects dataset workflow characteristics, clearly standsout. outperforms default strategy terms Kendall Similarity gain,statistically significant, close statistically significant, manner 24 values k[2, . . . , 35]; terms average accuracy top-k workflows, outperforms 20values k statistically significant, close statistically significant, manner.methods, including P1, follow large performance difference P2.allow planners include workflows operatorsused baseline experiments, annotated dmop ontology, P2sperformance advantage smaller. terms Kendall similarity gain, statisticallysignificant, close to, k [10, . . . , 20]. respect average accuracy top-klists, better default small lists, k = 3, 4; k > 23,fact significantly worse. P1 fairs better second scenario, however performancedifferent default method. Keep mind though baseline usedsecond scenario quite optimistic one.fact, see able generalize plan well datasets,evidenced good performance P2 first setting. However, comesgeneralizing datasets operators case second scenarioperformance planned workflows good, exception topworkflows. take look new operators added second scenario,feature selection algorithm, Information Gain Ratio, four classification algorithms,namely Linear Discriminant Algorithm, Ripper rule induction algorithm, Neural Netalgorithm, Random Tree. them, Information Gain Ratio seenbase level set experiments algorithm, Information Gain, rathersimilar learning bias it. Ripper rule induction sequential covering algorithm,closest operators set training operators two decision treealgorithms recursive partitioning algorithms. respect dmop ontology,Ripper shares certain number common characteristics decision trees, howevermeta-mining model contains information set-covering learning bias performsdifferent datasets. might lead selected given dataset basedcommon features decision trees, learning bias fact appropriatedataset. Similar observations hold algorithms, example LDAshares number properties SVMl LR, however learning bias, maximizingbetween-to-within class distances ratio, different learning biasestwo, meta-mining model contains information bias associatesdataset characteristics.Overall, extent system able plan, tested untestedoperators, workflows achieve good performance, depends extendproperties latter seen training meta-mining modelswithin operators experimented with, well whetherunseen properties affect critically final performance. case operatorswell characterized experimentally, Scenario 1, performanceworkflows designed P2 strategy good. Note necessaryoperators workflows applied datasets, enough sufficient setexperiments operator. heterogeneous metric learning algorithm handle634fiUsing Meta-mining Support DM Workflow Planning Optimization(b)(a)X-ValidationX-ValidationDataProcessingAlgorithmFWAlgorithmClassificationModeling DataProcessingAlgorithmAlgorithmFWAlgorithmHighBiasCMAMultivariateFWAlgorithmClassificationModelingAlgorithmHighVarianceCMAUnivariateFWAlgorithmFigure 12: Top-ranked workflow patterns according average absolute weights givenmatrix V.incomplete preference matrices, using available information. course clearavailable information, whether form complete preference matricesform extensive base-level experiments large number datasets, betterquality learned meta-mining model be. interesting exploresensitivity heterogeneous metric learning method different levels completenesspreference matrix; however outside scope present paper.quantify importance different workflow patternsoperators properties analyzing linear transformation workflow patternscontained heterogeneous metric. precisely, establish learned importanceworkflow pattern averaging absolute values weights assigneddifferent factors (rows) V linear transformation matrix Eq.(11). Noteapproach, establish importance patterns, whetherassociated good bad predictive performance. Figure 12, give twoimportant patterns determined basis averaged absoluteweights. describe relations workflow operators, first oneindicates multivariate feature weighting algorithm followed high biasclassification algorithm, second describes univariate feature weighting algorithmfollowed high bias classification algorithm. systematic analysis learned modelcould provide hints one focus ontology building effort, lookingimportant patterns well patterns used. addition,reveal parts ontology might need refinement order distinguishdifferent workflows respect expected performance.6. Conclusions Future Workpaper, presented is, best knowledge, first systemable plan data mining workflows, given task given input dataset,expected optimize given performance measure. system relies tightinteraction hierarchical task network planner learned meta-mining modelplan workflows. meta-mining model, heterogeneous learned metric, associatesdatasets characteristics workflow characteristics expected lead good635fiNguyen, Hilario & Kalousisperformance. workflow characteristics describe relations different components workflows, capturing global interactions various operators appearwithin them, incorporate domain knowledge latter given data mining ontology (dmop). learn meta-mining model collection past base-level miningexperiments, data mining workflows applied different datasets. carefully evaluatedsystem task classification showed outperforms significantmanner number baselines default strategy plan operatorsexperimented base-level experiments. performanceadvantage less pronounced consider also planning operatorsexperimented base-level experiments, especiallyproperties operators present within operatorsexperimented base-level experiments.system directly applicable mining tasks e.g. regression, clustering.reasons focused classification mainly practical: extensiveannotation classification task related concepts data mining ontology,large availability classification datasets, extensive relevant work meta-learningdataset characterization classification. main hurdle experimentingdifferent mining task annotation necessary operators dmop ontologyset base-level collection mining experiments specific task. Althoughannotation new algorithms operators quite labor intensive task, manyconcepts currently available dmop directly usable mining tasks, e.g. costfunctions, optimization problems, feature properties etc. addition, small activecommunity, DMO-foundry6 , maintaining augmenting collaboratively ontologynew tasks operators, significantly reducing deployment barrier new task.DMO-foundry web site, one find number tools templates facilitateaddition new concepts operators ontology well annotateexisting ones. said note use dmop sine-qua-nonsystem function. well perform workflow characterization taskmining ground operators, without using ontology. downsidewould extracted patterns generalized contain operatorproperties. Instead defined ground operators. Everything else remainsis.number issues still need explore finer detail. wouldlike gain deeper understanding better characterization reduced performance planning untested operators; example, conditionsrelatively confident suitability untested operator within workflow. wantexperiment strategy suggested parameter tuning, treatparameters yet another property operators, order see whether gives betterresults; expect will. want study detail level missing informationpreference matrix affects performance system, well whether usingranking based loss functions metric learning problem instead sum squares wouldlead even better performance.6. http://www.dmo-foundry.org/636fiUsing Meta-mining Support DM Workflow Planning Optimizationambitious level want bring ideas reinforcement learning (Sutton& Barto, 1998); let system design workflows systematic wayapplied collection available datasets order derive even better characterizationsworkflow space relate dataset space, exploring example areasmeta-mining model less confident.Acknowledgmentswork partially supported European Community 7th framework program ICT-2007.4.4 grant number 231519 e- Lico: e-Laboratory Interdisciplinary Collaborative Research Data Mining Data-Intensive Science. AlexandrosKalousis partially supported RSCO ISNET NFT project. basic HTN planner result collaborative work within e-LICO project Jorg-Uwe Kietz,Floarea Serban, Simon Fischer. would like thank Jun Wang important contribution developing metric learning part paper. addition would likethank members AI lab, Adam Woznica, Huyen Do, Jun Wang,significant effort placed providing content DMOP. Finally, would likethank reviewers suggestions helped improve paper.637fiNguyen, Hilario & KalousisAppendix A. Detailed Resultsk234567891011121314151617181920212223242526272829303132333435TotalP2W L p-value87117 11 0.34424 18 0.44035 21 0.08239 23 0.05641 19 0.00643 20 0.00543 22 0.01347 18 0.00040 24 0.06042 21 0.01140 25 0.08240 25 0.08243 21 0.00840 25 0.08240 25 0.08240 25 0.08240 25 0.08238 27 0.21438 27 0.21439 25 0.10438 27 0.21441 24 0.04740 24 0.06039 26 0.13641 23 0.03342 22 0.01741 24 0.04741 24 0.04744 21 0.00644 21 0.00643 21 0.00842 21 0.01142 21 0.011Wins 16/Losses 0P1W L p-value13 13 117 20 0.74218 27 0.23324 29 0.58229 31 0.89733 31 0.90036 27 0.31333 31 0.90035 30 0.61930 35 0.61934 29 0.61433 28 0.60838 26 0.16938 26 0.16937 27 0.26035 29 0.53135 29 0.53134 30 0.70737 26 0.20735 30 0.61937 27 0.26035 29 0.53136 28 0.38136 28 0.38136 29 0.45638 27 0.21438 26 0.16939 26 0.13639 25 0.10440 24 0.06042 23 0.02541 24 0.04739 25 0.10440 24 0.060Wins 2/Losses 0MetricW L p-value411 0.12112 16 0.57019 27 0.30223 27 0.67126 35 0.30527 37 0.26030 34 0.70730 33 0.80129 35 0.53126 38 0.16933 31 0.90032 33 1.00034 31 0.80434 30 0.70734 31 0.80432 32 1.00034 31 0.80432 33 1.00031 32 1.00030 35 0.61931 34 0.80432 33 1.00033 31 0.90033 32 1.00031 34 0.80432 32 1.00035 29 0.53135 30 0.61937 28 0.32139 26 0.13639 26 0.13638 27 0.21437 27 0.26036 28 0.381Wins 0/Losses 0EuclW L p-value911 0.82316 15 1.00025 19 0.45029 25 0.68332 27 0.60235 25 0.24531 30 1.00032 32 1.00033 31 0.90030 35 0.61927 37 0.26028 36 0.38133 31 0.90032 33 1.00030 35 0.61931 34 0.80433 32 1.00032 33 1.00032 33 1.00028 37 0.32130 35 0.61930 35 0.61932 33 1.00031 34 0.80430 35 0.61929 35 0.53129 35 0.53129 36 0.45630 35 0.61931 34 0.80432 33 1.00032 33 1.00032 33 1.00031 34 0.804Wins 0/Losses 0Table 4: Wins/Losses respective P-values McNemars test numbertimes Kendal similarity method better Kendal similaritydefault, Scenario 1. bold, winning p-value lower 0.05.638fiUsing Meta-mining Support DM Workflow Planning Optimizationk34567891011121314151617181920212223242526272829303132333435TotalP2Avg.Acc W L p-value0.79839 20 0.0190.79341 21 0.0150.79241 21 0.0150.78943 21 0.0080.78639 24 0.0770.78438 25 0.1300.78241 24 0.0470.78036 26 0.2530.77841 20 0.0100.77736 27 0.3130.77744 20 0.0040.77438 23 0.0730.77338 25 0.1300.77240 22 0.0300.77143 18 0.0020.77040 22 0.0300.76940 22 0.0300.76736 26 0.2530.76642 21 0.0110.76534 29 0.6140.76339 24 0.0770.76238 26 0.1690.76137 25 0.1620.76037 24 0.1240.75939 19 0.0120.75733 20 0.0990.75532 14 0.0120.75333 15 0.0140.75132 10 0.0010.74921 15 0.4040.747850.5790.74218 10 0.1850.737231Wins 16/Losses 0P1Avg.Acc W L p-value0.78826 38 0.1690.78628 35 0.4490.78535 28 0.4490.78538 25 0.1300.78633 30 0.8010.78333 29 0.7030.78240 25 0.0820.78138 27 0.2140.77838 25 0.1300.77730 33 0.8010.77740 22 0.0300.77540 23 0.0430.77437 26 0.2070.77240 24 0.0600.77141 21 0.0150.77040 22 0.0300.76939 23 0.0560.76835 27 0.3740.76741 18 0.0040.76533 24 0.2890.76345 19 0.0010.76233 27 0.5180.76134 30 0.7070.76043 18 0.0020.75843 20 0.0050.75739 23 0.0560.75434 17 0.0250.75132 27 0.6020.74828 30 0.8950.74622 31 0.2710.74416 30 0.0550.74126 36 0.2530.737221Wins 6/0 LossesMetricAvg.Acc W L p-value0.78625 38 0.1300.78426 37 0.2070.78332 33 10.78233 32 10.78032 31 10.77928 33 0.6080.77935 27 0.3740.77834 30 0.7070.77634 28 0.5250.77529 33 0.7030.77440 24 0.0600.77339 25 0.1040.77132 33 10.77033 29 0.7030.77040 25 0.0820.76938 27 0.2140.76736 26 0.2530.76729 35 0.5310.76638 25 0.1300.76536 25 0.2000.76442 22 0.0170.76335 27 0.3740.76236 29 0.4560.76145 11 0.0010.75941 20 0.0100.75838 24 0.0980.75535 10 0.0000.75233 19 0.0710.75034 17 0.0250.74822 18 0.6350.74513 17 0.5830.74219 21 0.8740.737250.449Wins 4/Losses 0ECAvg.Acc W L p-value0.78230 32 0.8980.78032 32 10.77832 33 10.77734 30 0.7070.77630 33 0.8010.77430 35 0.6190.77330 34 0.7070.77331 33 0.9000.77331 34 0.8040.77226 37 0.2070.77231 33 0.9000.77233 30 0.8010.77131 34 0.8040.77030 33 0.8010.76934 30 0.7070.76733 31 0.9000.76732 31 10.76630 35 0.6190.76537 28 0.3210.76433 30 0.8010.76232 30 0.8980.76130 32 0.8980.76030 32 0.8980.75837 26 0.2070.75737 22 0.0680.75633 26 0.4340.75431 19 0.1190.75132 24 0.3490.74925 28 0.7830.74720 28 0.3120.74512 25 0.0480.74213 18 0.4720.737231Wins 0/ Losses 0DefAvg.Acc0.7860.7850.7780.7770.7800.7780.7750.7750.7740.7740.7710.7710.7710.7690.7660.7650.7650.7660.7630.7630.7610.7610.7600.7560.7560.7560.7530.7510.7490.7480.7470.7420.737Table 5: Average Accuracy, Wins/Losses, respective P-values McNemars testnumber times Average Accuracy method better AverageAccuracy default, Scenario 1. bold, winning p-value lower0.05.639fiNguyen, Hilario & Kalousisk234567891011121314151617181920212223242526272829303132333435TotalP2W L p-value924 0.01415 28 0.06725 32 0.42628 34 0.52533 30 0.80136 29 0.45638 27 0.21440 25 0.08243 22 0.01343 22 0.01340 25 0.08241 24 0.04743 22 0.01343 22 0.01340 25 0.08242 23 0.02540 25 0.08240 25 0.08240 25 0.08239 26 0.13639 26 0.13638 27 0.21438 27 0.21440 25 0.08240 25 0.08238 27 0.21438 27 0.21438 27 0.21437 28 0.32135 30 0.61935 30 0.61930 35 0.61929 36 0.45629 36 0.456Wins 4/Losses 0P1W L p-value811 0.64613 13 1.00017 18 1.00021 25 0.65824 29 0.58232 28 0.69834 26 0.36634 29 0.61435 30 0.61933 32 1.00034 31 0.80432 32 1.00034 31 0.80436 29 0.45635 30 0.61936 29 0.45635 30 0.61935 30 0.61936 29 0.45636 28 0.38137 28 0.32135 30 0.61935 30 0.61934 31 0.80434 31 0.80435 30 0.61934 30 0.70734 31 0.80433 32 1.00033 32 1.00033 32 1.00033 32 1.00034 31 0.80432 33 1.000Wins 0/Losses 0Table 6: Wins/Losses P-values McNemars test number times Kendalsimilarity method better Kendal similarity default, Scenario2. bold, winning p-value lower 0.05.640fiUsing Meta-mining Support DM Workflow Planning Optimizationk34567891011121314151617181920212223242526272829303132333435TotalP2Avg.Acc W L0.79739 240.79244 200.79034 290.78737 270.78537 270.78336 280.78235 280.78138 260.77938 250.77734 300.77534 300.77435 280.77335 290.77333 320.77232 330.77019 440.76926 390.76824 370.76825 390.76724 390.76726 380.76623 400.76523 420.76320 450.76214 510.76215 500.76116 480.76018 470.75918 470.75718 470.75617 480.75616 490.75513 51Wins 1/Lossesp-value0.0770.0040.6140.2600.2600.3810.4490.1690.1300.7070.7070.4490.5311.0001.0000.0020.1360.1240.1040.0770.1690.0430.0250.0020.0000.0000.0000.0000.0000.0000.0000.0000.00013P1Avg.Acc W L p-value0.78929 34 0.6140.78433 31 0.9000.78531 34 0.8040.78232 32 1.0000.78333 32 1.0000.78332 33 1.0000.78330 34 0.7070.78431 33 0.9000.78235 29 0.5310.78036 29 0.4560.77938 27 0.2140.77938 26 0.1690.77737 27 0.2600.77634 31 0.8040.77431 33 0.9000.77326 38 0.1690.77227 37 0.2600.77228 37 0.3210.77023 42 0.0250.77024 40 0.0600.76929 36 0.4560.76826 38 0.1690.76829 36 0.4560.76724 40 0.0600.76822 42 0.0170.76724 41 0.0470.76732 32 1.0000.76633 31 0.9000.76639 26 0.1360.76535 29 0.5310.76533 33 1.0000.76431 34 0.8040.76327 38 0.214Wins 0/Losses 3DefAvg.Acc0.7860.7800.7840.7780.7780.7790.7780.7760.7730.7720.7700.7700.7700.7700.7710.7730.7720.7710.7700.7710.7690.7680.7670.7680.7690.7680.7660.7640.7630.7640.7640.7640.764Table 7: Avg.Acc., Wins/Losses, respective P-values McNemars testnumber times Average Accuracy method better AverageAccuracy default, Scenario 2. bold, winning p-value lower0.05.641fiNguyen, Hilario & KalousisReferencesBernstein, A., Provost, F., & Hill, S. (2005). Toward intelligent assistance data miningprocess: ontology-based approach cost-sensitive classification. KnowledgeData Engineering, IEEE Transactions on, 17 (4), 503518.Bose, R. J. C., & der Aalst, W. M. V. (2009). Abstractions process mining: taxonomypatterns. Proceedings 7th International Conference Bussiness ProcessManagement.Brazdil, P., Giraud-Carrier, C., Soares, C., & Vilalta, R. (2008). Metalearning: ApplicationsData Mining (1 edition). Springer Publishing Company, Incorporated.Bringmann, B. (2004). Matching frequent tree discovery. Proceedings FourthIEEE International Conference Data Mining (ICDM04, pp. 335338.Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., & Wirth, R.(2000). Crisp-dm 1.0 step-by-step data mining guide. Tech. rep., CRISP-DMconsortium.Fagin, R., Kumar, R., & Sivakumar, D. (2003). Comparing top k lists. Proceedingsfourteenth annual ACM-SIAM symposium Discrete algorithms, SODA 03, pp.2836, Philadelphia, PA, USA. Society Industrial Applied Mathematics.Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996). data mining knowledgediscovery databases. AI magazine, 17 (3), 37.Forbes, J., & Andre, D. (2000). Practical reinforcement learning continuous domains.Tech. rep., Berkeley, CA, USA.Gil, Y., Deelman, E., Ellisman, M., Fahringer, T., Fox, G., Gannon, D., Goble, C., Livny,M., Moreau, L., & Myers, J. (2007). Examining challenges scientific workflows.Computer, 40 (12), 2432.Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. H. (2009).weka data mining software: update. SIGKDD Explor. Newsl., 11 (1), 1018.Hilario, M. (2002). Model complexity algorithm selection classification. Proceedings 5th International Conference Discovery Science, DS 02, pp. 113126,London, UK, UK. Springer-Verlag.Hilario, M., & Kalousis, A. (2001). Fusion meta-knowledge meta-data casebased model selection. Proceedings 5th European Conference PrinciplesData Mining Knowledge Discovery, PKDD 01, pp. 180191, London, UK, UK.Springer-Verlag.Hilario, M., Kalousis, A., Nguyen, P., & Woznica, A. (2009). data mining ontologyalgorithm selection meta-learning. Proc ECML/PKDD09 WorkshopThird Generation Data Mining: Towards Service-oriented Knowledge Discovery.Hilario, M., Nguyen, P., Do, H., Woznica, A., & Kalousis, A. (2011). Ontology-based metamining knowledge discovery workflows. Jankowski, N., Duch, W., & Grabczewski,K. (Eds.), Meta-Learning Computational Intelligence. Springer.642fiUsing Meta-mining Support DM Workflow Planning OptimizationHo, T. K., & Basu, M. (2002). Complexity measures supervised classification problems.IEEE Trans. Pattern Anal. Mach. Intell., 24 (3), 289300.Ho, T. K., & Basu, M. (2006). Data complexity pattern recognition. Springer.Hoffmann, J. (2001). Ff: fast-forward planning system. AI magazine, 22 (3), 57.Kalousis, A. (2002). Algorithm Selection via Metalearning. Ph.D. thesis, UniversityGeneva.Kalousis, A., Gama, J., & Hilario, M. (2004). data algorithms: Understandinginductive performance. Machine Learning, 54 (3), 275312.Kalousis, A., & Theoharis, T. (1999). Noemon: Design, implementation performanceresults intelligent assistant classifier selection. Intell. Data Anal., 3 (5), 319337.Kietz, J.-U., Serban, F., Bernstein, A., & Fischer, S. (2009). Towards Cooperative Planning Data Mining Workflows. Proc ECML/PKDD09 Workshop ThirdGeneration Data Mining: Towards Service-oriented Knowledge Discovery (SoKD-09).Kietz, J.-U., Serban, F., Bernstein, A., & Fischer, S. (2012). Designing kdd-workflows viahtn-planning intelligent discovery assistance. 5th PLANNING LEARNWORKSHOP WS28 ECAI 2012, p. 10.King, R. D., Feng, C., & Sutherland, A. (1995). Statlog: Comparison classificationalgorithms large real-world problems. Applied Artificial Intelligence, 9 (3), 289333.Klinkenberg, R., Mierswa, I., & Fischer, S. (2007). Free data mining software: Rapidminer4.0 (formerly yale). http://www.rapid-i.com/.Kopf, C., Taylor, C., & Keller, J. (2000). Meta-analysis: data characterisationmeta-learning meta-regression. Proceedings PKDD-00 Workshop DataMining, Decision Support,Meta-Learning ILP.Kramer, S., Lavrac, N., & Flach, P. (2000). Relational data mining.. chap. Propositionalization Approaches Relational Data Mining, pp. 262286. Springer-Verlag NewYork, Inc., New York, NY, USA.Leite, R., & Brazdil, P. (2010). Active testing strategy predict best classification algorithm via sampling metalearning. Proceedings 2010 conference ECAI2010: 19th European Conference Artificial Intelligence, pp. 309314, Amsterdam,Netherlands, Netherlands. IOS Press.McDermott, D., Ghallab, M., Howe, A., Knoblock, C., Ram, A., Veloso, M., Weld, D., &Wilkins, D. (1998). Pddl-the planning domain definition language..Michie, D., Spiegelhalter, D. J., Taylor, C. C., & Campbell, J. (1994). Machine learning,neural statistical classification..Nguyen, P., Kalousis, A., & Hilario, M. (2011). meta-mining infrastructure support kdworkflow optimization. Proc ECML/PKDD11 Workshop Planning LearnService-Oriented Knowledge Discovery, 1.643fiNguyen, Hilario & KalousisNguyen, P., Kalousis, A., & Hilario, M. (2012a). Experimental evaluation e-licometa-miner. 5th PLANNING LEARN WORKSHOP WS28 ECAI 2012,p. 18.Nguyen, P., Wang, J., Hilario, M., & Kalousis, A. (2012b). Learning heterogeneous similaritymeasures hybrid-recommendations meta-mining. IEEE 12th InternationalConference Data Mining (ICDM), pp. 1026 1031.Peng, Y., Flach, P. A., Soares, C., & Brazdil, P. (2002). Improved dataset characterisationmeta-learning. Discovery Science, pp. 141152. Springer.Pfahringer, B., Bensusan, H., & Giraud-Carrier., C. (2000). Meta-learning landmarking various learning algorithms.. Proc. 17th International Conference MachineLearning, 743750.R Core Team (2013). R: language environment statistical computing. http://www.R-project.org/.Smart, W. D., & Kaelbling, L. P. (2000). Practical reinforcement learning continuousspaces. Proceedings Seventeenth International Conference Machine Learning, ICML 00, pp. 903910, San Francisco, CA, USA. Morgan Kaufmann PublishersInc.Soares, C., & Brazdil, P. (2000). Zoomed ranking: Selection classification algorithms basedrelevant performance information. Proceedings 4th European ConferencePrinciples Data Mining Knowledge Discovery, PKDD 00, pp. 126135,London, UK. Springer-Verlag.Srebro, N., Rennie, J. D. M., & Jaakkola, T. S. (2005). Maximum-margin matrix factorization. Saul, L. K., Weiss, Y., & Bottou, L. (Eds.), Advances Neural InformationProcessing Systems 17, pp. 13291336. MIT Press, Cambridge, MA.Sutton, R., & Barto, A. (1998). Reinforcement learning: introduction. Neural Networks,IEEE Transactions on, 9 (5), 1054.Van der Aalst, W. M., & Giinther, C. (2007). Finding structure unstructured processes:case process mining. Application Concurrency System Design, 2007.ACSD 2007. Seventh International Conference on, pp. 312. IEEE.Yang, Q., & Wu, X. (2006). 10 challenging problems data mining research. InternationalJournal Information Technology & Decision Making, 5 (04), 597604.Zaki, M. J. (2005). Efficiently mining frequent trees forest: Algorithms applications.IEEE Transactions Knowledge Data Engineering, 17 (8), 10211035. specialissue Mining Biological Data.Zakova, M., Kremen, P., Zelezny, F., & Lavrac, N. (2011). Automating knowledge discovery workflow composition ontology-based planning. Automation ScienceEngineering, IEEE Transactions on, 8 (2), 253264.644fiJournal Artificial Intelligence Research 51 (2014) 207-226Submitted 1/14; published 09/14Sensitivity Diffusion Dynamics Network UncertaintyAbhijin AdigaChris J. Kuhlmanabhijin@vbi.vt.educkuhlman@vbi.vt.eduNetwork Dynamics Simulation Science Laboratory,Virginia Bioinformatics Institute,Virginia Tech, VA 24061Henning S. Mortveithmortvei@vbi.vt.eduNetwork Dynamics Simulation Science Laboratory,Virginia Bioinformatics Institute,Department Mathematics,Virginia Tech, VA 24061Anil Kumar S. Vullikantiakumar@vbi.vt.eduNetwork Dynamics Simulation Science Laboratory,Virginia Bioinformatics Institute,Department Computer Science,Virginia Tech, VA 24061AbstractSimple diffusion processes networks used model, analyze predictdiverse phenomena spread diseases, information memes. oftennot, underlying network data noisy sampled. prompts following naturalquestion: sensitive diffusion dynamics subsequent conclusions uncertaintynetwork structure?paper, consider two popular diffusion models: Independent cascade (IC) modelLinear threshold (LT) model. study expected number verticesinfluenced/infected, particular initial conditions, affected network perturbations.rigorous analysis assumption reasonable perturbation modelestablish following main results. (1) IC model, characterize sensitivitynetwork perturbation terms critical probability phase transition network.find expected number infections quite stable, unless transmissionprobability close critical probability. (2) show standard LT modeluniform edge weights relatively stable network perturbations. (3) studysensitivity questions using extensive simulations diverse real world networksfind theoretical predictions models match observations quite closely.(4) Experimentally, transient behavior, i.e., time series number infections,models appears sensitive network perturbations.1. Introductionnumber diverse phenomena modeled simple diffusion processes graphs,spread epidemics (Newman, 2003), viral marketing (Kempe, Kleinberg, & Tardos,2005; Goldenberg, Libai, & Muller, 2001) memes online social media (Romero, Meeder,& Kleinberg, 2011; Bakshy, Hofman, Mason, & Watts, 2011). common associatevertex state 0 (denoting infected influenced) state 1 (denotingc2014AI Access Foundation. rights reserved.fiAdiga, Kuhlman, Mortveit & Vullikantiinfected influenced) models; node state 0 switches state 1 basedprobabilistic rule nodes state 1 remain state 1. focus twomodels, referred independent cascade (IC) model (which special case SIRprocess), linear threshold (LT) model. applications, however, underlyingnetworks inherently noisy incomplete, since often inferred indirectmeasurements, instance: (i) networks based Twitter data (Gonzlez-Bailn, BorgeHolthoefer, Rivero, & Moreno, 2011; Bakshy et al., 2011; Galuba, 2010) constructedlimited samples available public APIs, (ii) biological networks inferredexperimental correlations (Hagmann, 2008; Schwab, Bruinsma, Feldman, & Levine, 2010),might incomplete, (iii) Internet router/AS level graphs constructedusing traceroutes (Faloutsos, Faloutsos, & Faloutsos, 1999), known give biasedincomplete structure (Achlioptas, Clauset, Kempe, & Moore, 2005).raises fundamental issue diffusion processes networks:uncertainty network affect conclusions drawn study diffusiondynamics network? instance, robust inferencelarge outbreak network, face noise/uncertainty network? Recentstatistical simulation-based studies involving perturbation network rewiringpairs edges (which preserves degree sequence) show changes networkstructure significantly alter dynamics even aggregate structural propertiesdegree distribution assortativity preserved (Eubank, 2010; Chen, 2010).Surprisingly, limited mathematically rigorous work explain empirical findingssystematic manner, despite large body research diffusion models.work motivated considerations sensitivity dynamics noiseadequacy sampling network G = (V, E). Since limited understandingnoise modeled, consider two simple random edge perturbation modelsnoise: uniform degree-assortative. uniform perturbation, pair u, v verticesselected edge addition deletion (or both) probability n , > 0parameter,n number vertices; thus, average, perturbed graph differsnnn 2 2 edges. model used quite extensively social network analysiscomputer science understanding sensitivity graph properties (Costenbader& Valente, 2003; Borgatti, Carley, & Krackhardt, 2006; Flaxman & Frieze, 2004; Flaxman,2007). study expected number infections, given initial conditions,affected magnitude perturbation parameter, . degree-assortativeperturbation, probability edge modification proportional productdegrees end points G.1.1 Contributionsresults obtain assumption uniform edge addition model, i.e.,edges absent network added probability n obtain perturbed network.Later, Section 5, compare addition model addition/deletion modelalso discuss results degree-assortative perturbation. describe independentcascade linear threshold models Section 2.2.208fiSensitivity Diffusion Dynamics Network Uncertainty1.1.1 Independent Cascade Modelconsider networks G exhibit phase transition infection sizes, criticaltransition probability pc (see Section 2 definitions). Theorem 1, characterizeexpected number infections perturbed graph terms transmission probability ppc single random seed node. p < pc , show exists thresholdpositive constant c < 1: (i) < (1 c)t , addition, pcaverage degree davg satisfy technical condition, then, expected number infectionsperturbed graph remains close G and, (ii) > (1 + c)t , phasetransition, expected number infections perturbation much largerG. main implication dynamics quite robust perturbations, unlesstransmission probability close pc , critical value. findconsistent extensive simulations large number real networksthe sensitivityperturbations maximized point approximately matches experimentallydetermined threshold many networks. also examine transient behavior (i.e.,time series number infections) studying particular time magnitudepeak number new infections affected uncertainty. find measuressensitive expected total number infections.1.1.2 Linear Threshold ModelTheorem 2, show formally network G maximum degree =O(n/ log n), expected number infections perturbation, starting randominitial infections, bounded O(s( + + log n) log n). implies dynamicsquite stable low . result based analysis random graphmodel node selects random in-edge. shown correspondLT model (Kempe, Kleinberg, & Tardos, 2003). first show diameter boundedO(0 log n), 0 maximum degree perturbed graph, proveexpected number infections, starting random source, boundeddiameter. theoretical bounds corroborate well experimental observationslarge set real networks, show gradual variation . find expectednumber infections grows sharply , number sources increased.1.1.3 Discussion Implicationspoint view dynamical system theory, work may regarded studystability dynamics network respect edge structure. existencecritical value parameter IC model thought bifurcation point.Admittedly, results hold specific random edge perturbation model noise;uncertainty networks much complex process, might involve dependenciesarising network evolution. Although focus specific dynamical propertiesrandom edge perturbation model, results give first rigorous theoreticalempirical analysis noise susceptibility diffusion models. Further,analytical empirical techniques, based random graph characterization, likelyhelp analysis complex noise models, take dependenciesaccount.209fiAdiga, Kuhlman, Mortveit & Vullikanti1.2 Related WorkNoise issues sampling well recognized fundamental challenges complexnetworks, work characterizing sensitivity differentparameters, especially network properties. works (Costenbader & Valente, 2003;Borgatti et al., 2006), certain centrality measures shown robust random edgenode perturbations, another (Achlioptas et al., 2005), showninherent bias traceroute-based inference Internet router network, mightgive incorrect degree distributions. Flaxman Frieze (Flaxman & Frieze, 2004; Flaxman,2007) formally characterize conditions graph expansion diameterhighly sensitive random edge additions; among analytical resultstype. approaches address noise include: (i) prediction missing linksusing clustering properties (Clauset, Moore, & Newman, 2008), (ii) property testingalgorithms (Ron, 2010) smoothed analysis (Spielman, 2009) efficient computationgraph properties.knowledge, work sensitivity graph dynamical systems noisenetwork empirical. However, regular networks rings, topicssynchronization bifurcations studied (Kaneko, 1985; Wu, 2005). discussedearlier, effects changes network edge rewirings epidemic propertiesinvestigated (Eubank, 2010; Chen, 2010). effect stochastic changes networkinfluence maximization problems studied (Lahiri, Maiya, Caceres, Habiba, & Berger-Wolf,2008). Using simulations, find LT model, spread size quite robust;techniques help explain observations.1.2.1 OrganizationSection 2, introduce notation describe noise models diffusion modelsdetail. Sections 3 4, analyze sensitivity IC LT models, respectively.Experimental results presented Section 5, conclude Section 6.2. Preliminariesconsider undirected, simple networks. network G = (V, E), let denotemaximum degree davg , average degree. vertex v, deg(v, G) N (v) denotedegree set neighbors respectively. Let correspond largest eigenvalueadjacency matrix G. say event A(n) occurs asymptotically almost surely(a.a.s.) P (A(n)) 1 n .2.1 Noise ModelsSince consensus best way model uncertainty noise, considertwo simple models random edge modifications: (i) uniform (ii) degree-assortativeperturbations. Uniform perturbation studied quite extensively social networkanalysis (Costenbader & Valente, 2003; Borgatti et al., 2006); problems alsostudied analytically model (Flaxman & Frieze, 2004; Flaxman, 2007). LetG = (V, E) unperturbed graph; graphs work undirected simple.Let Ru () = (V, E()) random graph V pair u, v V connected210fiSensitivity Diffusion Dynamics Network Uncertaintyprobability n . analysis, consider perturbations involving additionedges: denoted G + Ru (), consists edges (u, v) E E(Ru ()).experimental studies, also considered addition/deletion edges. case,perturbation graph G0 = G Ru () graph constructed following manner:pair u, v V connected G0 (u, v) Ru () E (u, v) E Ru (). words,pair u, v selected addition/deletion probability n . degree-assortativeperturbationrandom graph Rd () u, v V adjacentmodel, considerdeg(u,G) deg(v,G)probabilityn , i.e., edge probability proportional productd2avgdegrees end points G. models, expected number edge modificationsapproximately n2 .2.2 Network Diffusion ModelsLet G = (V, E) denote undirected network. models study, vertex v Vstate xv {0, 1}, state 0 denoting inactive/uninfected/uninfluencedstate 1 denoting active/infected/influenced, depending application. restrictmonotone progressive processes, i.e., infected node stays infected.node associated activation function whose inputs include states neighbors.function computes next state node. diffusion process startsvertices set active/infected; refer set initial set seed set.initial set active nodes S, let (S) denote expected number active nodestermination. models always reach fixed points. consider following models:(1) Independent Cascade (IC) Model (Kempe et al., 2003): model special caseSIR model epidemics. infected node v infects neighbor w probabilityp (referred transmission probability). Equivalently, edge (v, w)live probability p, independently edges. nodesconnected initial set live path considered infected. graph,let (v, x) edge. Suppose v gets infected time t, x state 0. vtries infect x probability p time + 1. Irrespective whether x gets infectedv, v remains state 1 subsequent times, never tries infect x.(2) Linear Threshold (LT) Model (Kempe et al., 2005): node v thresholdv [0, 1], chosen uniformly random. Node v influenced neighbor wPaccording weight bv,w wN (v) bv,w 1. Node v becomes infectedPwA(v) bv,w v , A(v) N (v) set neighbors v currentlyinfected. analysis experiments, assume bv,w = 1/ deg(v, G)w N (v), deg(v, G) degree v G. means v influencedequally neighbors. model considered (Kempe et al., 2003).perturbed graph G0 = G + Ru (), bv,w = 1/ deg(v, G0 ), deg(v, G0 ) newdegree v.3. Analyzing Sensitivity IC Modelsection, rigorously analyze sensitivity IC model edge perturbations.mentioned earlier, restrict attention uniform edge addition model, i.e.,211fiAdiga, Kuhlman, Mortveit & Vullikantiperturbed graph obtained adding edge every pair vertices probability/n.stating main result, discuss aspects IC model developnotation used analysis. transmission probability p, let G(p)denote random subgraph G obtained retaining edges G probability p.relationship infection spread IC model structure G(p) well-known;question whether large spread occurs G equivalent askingexists giant component G(p). Another interesting aspect IC model exhibitsthreshold phenomenon. many graph families exists critical transmissionprobability pc abrupt phase transition occurs small spread p < pclarge one p > pc high probability. formal definition pc follows (see (Bollobs,1985) details).Definition 3.1. graph G n nodes, pc critical transmission probabilitystarting single random initial infection, transmission probability p pctotal number infections a.a.s. o(n) equivalently, components G(p)size o(n), p pc , number infections a.a.s. (n) exists giantcomponent G(p).Recall Section 2 notion asymptotically almost surely formally definedsequence graphs. However, state explicitly, order reduce notationaloverload. state main result section analyze sensitivityIC model graph operating probability p < pc satisfying pdavg 1 where,davg average degree G. conclude discussion implicationstheorem.Theorem 1. Consider graph G n nodes average degree davg critical transmission probability pc . Let transmission probability p satisfy pdavg = o(1) p = 1/n1constant . Let G + Ru () perturbed graph obtained adding edges uniformlyrandom factor . Then, single seed node chosen uniformly random,following hold:(a) number infections G o(n) a.a.s. therefore, p < pc .(b) pdavg = 1/ log2 n , then, exists threshold perturbation factor = p1positive constant c < 1, (1 c)t , number infections G + Ru ()transmission probability p a.a.s. o(n) (1 + c)t , numberinfections (n).(c) positive constant c,(n).1+cp ,number infections G + Ru () a.a.s.Proof. Let G0 = G + Ru () let G0 (p) random subgraph G0 obtained choosingedges probability p. Since, edge e/ E(G), Pr(e G0 (p)) = p Pr(e Ru ()) =p0n , follows G (p) obtained adding edges every pair nodesG(p) probability pn . Now, prove Statement (a).pClaim 1. number components G(p) one node n pdavg a.a.s.therefore, G(p) giant component.212fiSensitivity Diffusion Dynamics Network UncertaintyProof. use Chebychevs inequality.x> davg , number nodes degreedavggreater x G n x+1 < n avg. Let niso denote number isolatedxnodes G(p). Then,E[niso ] =XXPr(v isolated)vV (G)Pr(v isolated)vV (G),deg(v,G)xdavg(1 p)x n 1xqdavg> (1 px)n 1.x2ppChoosing x davg /p, E[niso ] 1 pdavg n 1 2 pdavg n. Since assumption, davg = o(1/p), E[niso ] = (1 o(1))n. node v V (G), let Iv = 1 vneighbors G(p) 0 otherwise. Let Var[] Cov[] denote variance covariance,respectively. Using bounds Var[Iv ] E[Iv ] Cov[Ia Ib ] = P (Ia Ib ) 1,Var[niso ] =XXVar[Iv ] + 2vV (G)Cov[Ia Ib ](a,b)E(G)XE[niso ] + 2Cov[Ia Ib ] E[niso ] + ndavg = O(ndavg ) .(a,b)E(G)Since assumed p = 1/n1 , follows Var[niso ] = n2 pdavg .Applying Chebychevs inequality,qP |niso E[niso ]| > n pdavg P |niso E[niso ]| >qn Var[niso ]pTherefore, a.a.s., n niso 3n davg p.1.nLet {Ci | N } set connected components G(p), N numbercomponents let ni denote size Ci . probability components Ci Cjn n pconnected least one edge Ru () G0 (p) nj . Let Siso denoteset isolated nodes G(p) iso set remaining nodes.Let H graph obtained adding special vertex v G0 (p) making adjacentnodes iso . Clearly, iso belongs component H component G0 (p)contained component H. implies G0 (p) giant component Hone. Now, prove first part Statement (b).Claim 2. H components size o(n) pdavg = 1/ log2 n(1c)p .Proof. Let H[Siso ] H[S iso ] graphs induced Siso iso , respectively, H. Notesince Siso set isolated nodes G(p), H[Siso ] Erds-Rnyi graph |Siso |pnodes edge probability |Sp= n(1o(1)). Since p < 1 c, follows H[Siso ]iso |components size O(log n) (see, e.g., Bollobs, 1985). show componentcontaining iso size o(n) a.a.s., thus completing proof.Let N (S iso ) denote size neighborhood iso H. probabilitynode Siso neighbor isoh|S iso |p.nE N (S iso ) |Siso |213Therefore,|S iso |p|S iso |p.nfiAdiga, Kuhlman, Mortveit & VullikantiApplying version Chernoff bound (Chung & Lu, 2002), following:h2.P N (S iso ) E N (S iso ) > exp h2 E N (S iso ) + /3two regimes consider: (i) |S iso |p = (1) (ii) |S iso |p = O(1). |S iso |p =(1), setting = |S iso |p, follows N (S iso ) 2|S iso |p a.a.s. Since H[Siso ]components size O(log n), size component containing isoq|S iso | + N (S iso ) log n = |S iso | + |S iso |p log n = n pdavg (1 + p log n) .last expressionfollows pClaim 1. Since (1 c)t pdavg = 1/ log2 n ,pfollows n pdavg p log n n pdavg log n(1 c) = o(n). consider regime (ii):|S iso |p = O(1). setting = log n, component size log2 n = o(n). Hence,proved claim.proofs second part Statement (b) Statement (c) straightforward.Let Rp denote random subgraph perturbation network Ru () obtained samplingedges probability p. easy see Rp Erds-Rnyi graph edge1+cprobability pn = n implies giant component (Bollobs, 1985).turn implies G0 (p) giant component. Hence, proved theorem.Theorem 1 indicates large class networks, closer operate pc ,sensitive dynamics structural perturbation. indeed trueconditions Statement (b) met: p1 < p2 < pc , (p1 ) > (p2 ). impliesgreater perturbation required case p1 (compared p2 ) observe significantdifference expected infection size perturbation. observedexperiments; see last three columns Table 1. AstroPh graph, example,p = 0.03, phase transition occurs = 8 p = 0.02, greater20 transition occur.4. Analyzing Sensitivity LT Modelanalyze impact edge perturbations LT model graph G = (V, E).previous section, restrict attention uniform edge addition model.Recall specific version LT model consider here, set bv,w = 1/ deg(v)node v V w N (v).fixed points number infected nodes studied elegantrandom graph model (Kempe et al., 2003) describe here. Construct randomdirected graph HLT = (V, E 0 ) following manner: node v V , neighbor wchosen probability bv,w directed edge added w v. Figure 1 illustratesgraph G instance HLT . Note even though G undirected, HLT directedgraph. set V , let (S, HLT ) denote number nodes reachable HLT(including S). Then, (S), expected number infections starting set S,Psatisfies (S) = HLT Pr[HLT ](S, HLT ) (Kempe et al., 2003). use characterizationanalyze impact edge perturbations.214fiSensitivity Diffusion Dynamics Network Uncertainty11232345456767Figure 1: graph (on left) instance random graph HLT (on right)corresponding LT model. component induced {1, 2, 3, 4, 5}, 1 chosenroot result, T0 = {1}, T1 = {3}, T2 = {2, 5} T3 = {4}.random graph HLT constructed process following structure:connected component HLT , every vertex one incoming edge therefore,exists exactly one directed cycle. choose vertex cycle root rremove incoming edge, then, corresponds tree rooted r edges orientedaway r. partitioned sets T0 , . . . , Tk > 0, vertexv Ti incoming edge vertex u Ti1 . set T0 singleton consistingroot vertex r. incoming edge r neighbor ki=1 Ti .illustrated Figure 1. First, show following:Lemma 4.1. LT model, let = minvV,wN (v) bv,w . component randomsubgraph HLT depth1log n probability least 11.n3Proof. Consider component HLT , partitioned sets T0 , . . . , Tk , mentioned above. = 1, . . . , k 1, vertex v Ti would become root choosesincoming edge one descendants. probability event leastminwN (v) bv,w . Therefore, probability none vertices Ti becomesroot 1 , turn implies probability none verticesTi , = 1, . . . , k 1 becomes root (1 )k1 . Hence, probabilityPdepth k = 4 1 log n + 1 nk4 1 log n+1 (1 )k1 n14 . Sincencomponents HLT , probability depth1 log n n13 .Consider vertex v contained component . Let n(v, ) denote numbervertices reachable v . Then, number infections resulting v expectedvalue n(v, ), averaged random subgraphs HLT components containing v. LetPA(T ) = |T1 | vT n(v, ). Conditioned random subgraph HLT , average numberinfections starting random sourcestarting random sourcePHLTPA(T ) |Tn | . average number infectionsHLTPr[HLT ]PHLTA(T ) |Tn | .Lemma 4.2. component random subgraph HLT , A(T ) 2d,depth .215fiAdiga, Kuhlman, Mortveit & VullikantiProof. Define tree obtained removing incoming edge root .described above, out-tree. v , define n(v, ) numbervertices reachable v corresponds size subtree rooted vP. define A(T ) = |T1 | vT n(v, ), prove A(T ) induction depthout-tree. base case leaf node u, A(u) = 1.Let r root . Suppose children v1 , . . . , va . Let Ti subtree rootedPvi , let ni number vertices Ti . induction, A(Ti ) = n1i vTi n(v, Ti ) d1.A(T ) =1 Xn(v, )|T |vT=X11 Xn(r, ) +n(v, Ti )|T |i=1 |T |vTi= 1+Xnii=1|T |A(Ti ) 1 +Xnii=1|T |(d 1)|T | 11+(d 1)|T |third equality follows n(r, ) = |T |, definition, A(Ti ) = n1i vTi n(v, Ti ).first inequality follows induction hypothesis, since depth Ti 1.Psecond inequality follows ai=1 ni = |T | 1.Next, consider A(T ). recall tree cycle length d. Letcycle consist vertices u0 = r, u1 , . . . , ub , b 1. ui , n(ui , ) = |T |,since path ui r. every vertex u 6= ui , n(u, ) = n(u, ).|implies A(T ) d|T|T | + A(T ) 2d.PFinally, bound number infections perturbed graph below.Theorem 2. Let G(V, E) graph maximum degree G + Ru () perturbedgraph obtained adding edges uniformly random factor . LT modelbv,w = 1/ deg(v) node v V w N (v), expected number infectedvertices starting initial random seed set size perturbed graph G + Ru ()O(s( + + log n) log n).Proof. direct application Chernoff bound, shown probabilityleast 1 n13 , maximum degree G0 = G + Ru () + + c log nconstant c remaining probability n13 , maximum degree O(n). considerrandom graph process generate subgraph HLT G0 . Since bv,w = 1/ deg(v)node v V w N (v), model, value Lemma 4.1 1/(G0 )therefore, component HLT depth O(( + + log n) log n),probability least 1 n13 . Conditioned HLT satisfying bound depth,A(T ) = O(( + + log n) log n) HLT . HLT satisfy depth bound,A(T ) = O(n) HLT . Therefore, expected number infections singlerandom seed O(( + + log n) log n) + O( nn3 ) = O(( + + log n) log n). resultextends > 1 submodularity expected number infections.216fiSensitivity Diffusion Dynamics Network UncertaintyTheorem 2 implies LT model uniform edge weights general robustperturbation. Note final part proof Theorem 2 essentially basedmaximum degree G0 . Replacing G0 G retracing steps, one showexpected spread unperturbed graph G O(s log n), maximumdegree. Hence, see reasonably high value (say (log n)), differencetotal number infections G G0 linear functiontherefore abrupt change outcome. Moreover, bound suggestshigher , lower effect perturbation. However, making formal wouldrequire obtaining good lower bounds expected number infections, leaveinteresting research question.5. Experimental Resultsstudy sensitivity edge perturbations twenty diverse real-world networks (Leskovec,2011) varying degrees perturbation factors IC LT models.listed Table 1 along properties, first fournumber nodes, average degree, maximum eigenvalue, maximum degree.properties discussed subsequently. present representative results selectednetworks, networks exhibiting behavior unless stated otherwise.focus primarily sensitivity expected number infections transient behavioredge perturbation. course, observations restricted conditionsexperiments performed.5.1 Experimental Setup Methodologynetwork G Table 1 perturbed values ranging 0 100,= 0 corresponds unperturbed network. , generated ten graph instancesG0 = G + R G R. Here, R may Ru Rd , depending whether perturbationuniform edge approach degree-assortative approach. graph instance,performed simulation run, consists 100 separate diffusion instances. diffusioninstance process setting node states initially zero, assigning relevant propertiesgraph entities (e.g., transmission probability edges IC model edge weightsnode thresholds LT model), selecting seed node set whose element stateschanged 1 (i.e., initially infected), marching time forward discrete units,continuing simulation fixed point reached. record nodes timeinfection diffusion instance. Thus, example, experimental data displayedaverage variance quantities based 1000 values.5.2 Edge Additions vs. Deletionsfind perturbations involving edge additions deletions alterresults much, compared perturbations involving edge additions. duesparsity graphs considered. example, uniform perturbations, expectednumber edges deleted |E|/n = davg /2 expected number edges modifiedn/2. Therefore, remainder paper focuses perturbation addition edgesonly; i.e., graphs form G0 = G + R.217fiAdiga, Kuhlman, Mortveit & VullikantiNetworkn = |V |davgpc(p; ) pairs experimentsSynthetic graphsRandom-Regular-201000020.020.0200.050.04;90.03;>100.02;>200.20.20.20.15;20.15;<20.15;<10.1;50.1;50.1;50.05;>100.05;100.05;100.040.120.20.10.20.03;80.1;20.15;<20.07;<20.15;<10.02;>200.08;40.1;50.04;80.1;50.01;>400.06;80.05;100.01;>200.05;100.040.050.03;60.04;<20.02;>200.03;80.01;>400.02;>400.10.30.07;40.2;<20.04;100.1;60.01;>200.05;100.150.10.10.20.040.12;<10.07;30.07;30.15;<20.03;30.1;20.04;90.04;90.1;40.02;100.08;40.01;>200.01;>200.05;100.01;>200.1;20.15;10.07;70.1;60.04;>100.05;>10Autonomous SystemsAS-2000-01-02Oregon1-01-03-31Oregon2-01-03-31647410670109003.884.125.7246.3158.7270.74AstroPhCondMatGrqcHepPhHepTh1790321363415811204863822.08.546.4520.995.7494.4237.8845.61244.9331.03HepPhHepTh345462777024.4625.3776.58111.25145823122343Co-authorship5042798149165Citation8462468CommunicationEmail-EnronEmail-EuAll3369626521410.022.74118.41102.5313837636SocialEpinionSlashdot0811Slashdot0902TwitterWiki-Vote75877773608216822405706610.6912.1212.275.3428.51184.17131.34134.6254.08138.153044253925528881065Internet peer-to-peerGnutella04Gnutella2410876265187.354.9315.719.061033550.1250.2Table 1: relevant properties networks used simulations resultsexperiments.5.3 Independent Cascade Modelexperimental results uniform degree-assortative perturbation,make observations relating theoretical results behaviornetworks general.5.3.1 Uniform PerturbationEffect p final infection size. Figure 2 consists plots variation averagevariance fraction infected nodes (i) transmission probability pvarious levels perturbation (ii) perturbation level various p values threenetworks. plots remaining networks full version (Adiga, Kuhlman,Mortveit, & Vullikanti, 2014). note p > 1 (and p pc ), average infectionsize generally high, agreeing Statement (c) Theorem 1. observeplots final fraction infections least 0.5 p > 1.218fiSensitivity Diffusion Dynamics Network UncertaintyAverage Final SizeVariance Final SizeAverage Final SizeVariance Final SizeAverage Final Size1.0 Random-Regular-200.80.6=00.4=0.5=10.2=5=100.00.00 0.05 0.10 0.15 0.20Probability, pRandom-Regular-200.12=0=0.50.10=10.08=5=100.060.040.020.000.0 0.2 0.4 0.6 0.8 1.0Probability, pRandom-Regular-201.00.80.6p =0.020.4p =0.03p =0.040.2p =0.05p =0.060.00 20 40 60 80100Perturbation,Average Final SizeAverage Final SizeVariance Final SizeWiki-Vote1.00.80.6=00.4=0.5=10.2=5=100.00.00 0.05 0.10 0.15 0.20Probability, pWiki-Vote0.12=0=0.50.10=10.08=5=100.060.040.020.000.0 0.2 0.4 0.6 0.8 1.0Probabilty, pWiki-Vote1.00.80.6p =0.010.4p =0.02p =0.030.2p =0.04p =0.050.00 20 40 60 80100Perturbation,Average Final SizeAS-2000-01-021.00.80.6=00.4=0.5=10.2=50.00.0 0.2 0.4 0.6 =100.8Probability, pAS-2000-01-020.12=0=0.50.10=10.08=5=100.060.040.020.000.0 0.2 0.4 0.6 0.8 1.0Probability, pAS-2000-01-021.00.80.6p =0.050.4p =0.10p =0.150.2p =0.20p =0.250.00 20 40 60 80100Perturbation,Figure 2: Uniform perturbation (IC model): Average variance fraction infectionssingle random seed (i) vs. transmission probability p various values (ii) vs.various values p. plots first third rows average final fractioninfected nodes second row average variance final fraction infectednodes. remaining plots full version (Adiga et al., 2014).sensitivity final infection size modulated davg . first considerrelationship pc davg unperturbed networks. Since finite networks,clear definition pc , chose value p average totalnumber infections 10% number nodes network. Table 1 contains pcnetwork. dependence pc davg shown Figure 3 data pointcorresponds one graph colored according type graph. Clearly, plotindicating inverse relationship two values. implies higherdavg , lower p spread small. Note Theorem 1 showsminimum large spread inversely proportional p. stronglysuggests greater edge density, greater perturbation required achievesignificant change dynamics. supported experiments, too. example, seeplots AS-2000-01-02 Wiki-Vote first row Figure 2. Consider p = 0.2note change spread going = 0 10. Wiki-Vote, goes 0.38 0.89,219fiAdiga, Kuhlman, Mortveit & Vullikantiratio 2.3. AS-2000-01-02, changes 0.08 0.82, ratio 10.3. Thus,AS-2000-01-02, factor 7 smaller davg Wiki-Vote, much greater sensitivityspread changes . Note numbers nodes two graphs comparable.see behavior (high davg , low davg ) network pairs: (AstroPh, CondMat),(HepPh, HepTh), (Email-Enron, Email-EuAll).Critical Probability0.50.40.30.20.10.00auton.co-auth.citationcomm.socialinternet10 20 30Average Degree40Figure 3: Dependence pc davg . Data graphs Table 1, colors correspondgraph types table.Variance final infection size. Figure 2 (and plots full version Adigaet al., 2014), middle row plots contains variance final infection size functiontransmission probability. several observations. First, graphs whosedynamics exhibit phase transition larger infection sizes increasing fixed p(and increasing p fixed ), variance infection size qualitatively increasesp, peaks region phase transition decreases. peaks variancecorrespond regimes change final infected fractions changingp, expected (cf., first row figures). Second, greater perturbation,lesser range p values variance high. greatervalue , faster contagion spreads, thus driving variance. last observationnon-intuitive. is, peak variance seem vary combinationsp 0 10 0 p 0.6. ranges, value ppeak occurs decreases increasing . fact, peak variance around 0.1graphs conditions.Effect regular network structure numbers infected nodes.several reasons investigate random networks uniform degree. First, investigationvoter model dynamics (Kuhlman, Kumar, & Ravi, 2013), shown uniform degreenetworks generated results near realistic graphs whose degree distributionexponential decay, far graphs scale free degree distributions.question arises close behavior uniform degree networksrealistic networks. Second, since perturbing subgraph R study randomgraph, perturbed graph G0 sense maintains random structure compared G,G random 20-regular graph (a random graph node degree 20).Figure 2 shows latter consideration dominates. is, upper right plot showscurves basically shifted left increases. So, too, plot right220fiSensitivity Diffusion Dynamics Network Uncertaintymiddle row, variance curves shift slightly left increases, effect muchsmaller Wiki-Vote AS-2000-01-02.Effect average time-to-peak number new infections. averagetime histories 1000 diffusion instances comprise curve one graphtype provided top row Figure 4. transmission probabilities usedplot p > pc . average time maximum number new infectionsoccurs decrease, stay same, increase function , depending graph,moving left right. graphs Table 1, seems fairly even splitnine graphs show increase average time-to-peak increasing eightshow decrease average time-to-peak. remainder show change time-to-peak.2010 15TimeTwitter20Average Max New Inf1.0=0=0.50.8=1=50.6=100.40.20.00.0 0.2 0.4 0.6 0.8 1.0Probability, pTwitter0.12=0=0.50.10=10.08=5=100.060.040.020.000.0 0.2 0.4 0.6 0.8 1.0Probability, pVariance Max New InfVariance Max New InfAverage Max New Inf1.0=0=0.50.8=1=50.6=100.40.20.00.0 0.2 0.4 0.6 0.8 1.0Probability, pCondMat0.12=0=0.50.10=10.08=5=100.060.040.020.000.0 0.2 0.4 0.6 0.8 1.0Probability, p5=0=0.5=1=5=100.5 Wiki-Vote (p =0.12)=0=0.50.4=1=50.3=100.20.10.005 10 15 20TimeWiki-Vote1.0=0=0.50.8=1=50.6=100.40.20.00.0 0.2 0.4 0.6 0.8 1.0Probability, pWiki-Vote0.12=0=0.50.10=10.08=5=100.060.040.020.000.0 0.2 0.4 0.6 0.8 1.0Probability, pAverage Max New Inf10 15TimeCondMatTwitter (p =0.5)Variance Max New Inf50.50.40.30.20.10.00Average New Inf=0=0.5=1=5=10Average New InfCondMat (p =0.3)Average New Inf0.50.40.30.20.10.00Figure 4: Sensitivity temporal characteristics uniform perturbation (IC model): Plots(i) average number new infections time step selected p values, (ii) averagemaximum number new infections time vs. p. (iii) variance maximum numbernew infections time vs. p. remaining plots full version (Adiga et al.,2014).221fiAdiga, Kuhlman, Mortveit & VullikantiEffect average peak number new infections. second row plotsFigure 4 depicts average maximum number new infections one timefunction transmission probability, different . increases, maximum numbernew infections increase.Variance average maximum number new infections discrete time.last row plots Figure 4 provides variance 1000 experimentally determinedvalues used compute average maximum number new infections; is, peakvalue curve plots top row. particular graphs, variancesroughly order 0.01, is, interestingly, much smallerfinal number infections Figure 2.5.3.2 Degree-Assortative PerturbationHere, focus results different uniform perturbation.Effect degree-assortativity average final number infectionsvariance. first row plots Figure 5 shows average final number infections,1000 measurements, function p . = 0 curvesFigure 2. comparison top row plots figure, cleardegree-assortativity perturbations significantly reduce effect changesaverage final number infections. find, networks, AS-2000-01-02Wiki-Vote provide two bounding cases, i.e., least effect effect degree-assortativeperturbations, respectively. comparison second row plots Figure 5Figure 2, also clear degree-assortative perturbations correspondingly collapsevariances across values Wiki-Vote, AS-2000-01-02 less affected.Effect perturbation method average final number infections. Since= 0 curves Figure 2 Figure 5, factors same,uniform perturbations generate greater numbers infections degree-assortativeperturbations (compare top rows plots two figures).Effect network structure degree-assortative perturbations. effectsdegree assortative perturbations appear network-specific uniformperturbations. expected since perturbation instances inherit networkproperties.Effect degree-assortative perturbations random regular graph.effects two perturbation methods random regular graphsuniform node degrees.5.4 Linear Threshold Modelresults effect uniform perturbation LT model. Figure 6 showsplots average number infections vs. three representative networks differentseed probabilities s. remaining plots full version (Adiga et al., 2014).diffusion instance, seed set constructed sampling vertex set uniformly222fiSensitivity Diffusion Dynamics Network UncertaintyAverage Final SizeAverage Final SizeVariance Final Size1.0 Random-Regular-20=0=0.50.8=1=50.6=100.40.20.00.0 0.1 0.2 0.3 0.4Probability, pRandom-Regular-200.12=0=0.50.10=10.08=5=100.060.040.020.000.0 0.2 0.4 0.6 0.8 1.0Probability, pVariance Final SizeAverage Final SizeWiki-Vote1.0=0=0.50.8=1=50.6=100.40.20.00.0 0.2 0.4 0.6 0.8 1.0Probability, pWiki-Vote0.12=0=0.50.10=10.08=5=100.060.040.020.000.0 0.2 0.4 0.6 0.8 1.0Probability, pVariance Final SizeAS-2000-01-021.00.80.6=00.4=0.5=10.2=50.00.0 0.2 0.4 0.6 =100.8Probability, pAS-2000-01-020.12=0=0.50.10=10.08=5=100.060.040.020.000.0 0.2 0.4 0.6 0.8 1.0Probability, pFigure 5: Degree-assortative perturbation (IC model): Average variance fractioninfections single random seed vs. transmission probability p various values.remaining plots full version (Adiga et al., 2014).probability s, every node assigned threshold chosen uniformly randominterval [0, 1].Recall Theorem 2 spread bounded linear function .Figure 6, observe model generally robust perturbation spreadlinear sublinear function depending seed probability s. particular,seed probability low (s = 0.0001 example), hardly change averagespread. Note plots, ranges 0 way 100 extremevalue perturbation considering sizes networks. However, larger valuess, especially comparable maximum degree , effectpronounced, even low values . observation worth investigating could leadbetter bounds spread Theorem 2.6. Conclusions Open Problemsgive first rigorous results stability independent cascade linearthreshold models respect edge perturbations. considered two popular noisemodels namely, uniform degree-assortative perturbations, studied sensitivityfinal outbreak size temporal characteristics perturbations. analysissupported experimental observations 20 diverse real networks. showedsensitivity independent cascade model depends transmission probabilityperturbation lead abrupt changes outcome, linear thresholdmodel uniform edge weights general stable network perturbations. Also,experiments suggest dynamics sensitive uniform perturbation223fiAdiga, Kuhlman, Mortveit & Vullikanti0.6=0.0001=0.001=0.010.40.20.80.00 20 40 60 80 100Perturbation,CIT-HepPh0.60.4=0.0001=0.001=0.010.8Average Final SizeCA-AstroPhAverage Final SizeAverage Final Size0.80.6Slashdot-09-02=0.0001=0.001=0.010.40.20.00 20 40 60 80 100Perturbation,0.20.00 20 40 60 80 100Perturbation,Figure 6: Uniform perturbation (LT model): Average fraction infected nodes vs. perturbation various seed probabilities s. remaining plots full version (Adigaet al., 2014).degree-assortative. Extending results models noise, especially involvingdependencies, sensitivity number sources, examining sensitivitydynamical properties general diffusion models (including IC LT modelsheterogeneous probabilities weights) natural open problems future research.7. Acknowledgmentswork partially supported following grants: DTRA Grant HDTRA111-1-0016, DTRA CNIMS Contract HDTRA1-11-D-0016-0010, NSF Career CNS 0845700,NSF ICES CCF-1216000, NSF NETSE Grant CNS-1011769 DOE DE-SC0003957. Alsosupported Intelligence Advanced Research Projects Activity (IARPA) via DepartmentInterior National Business Center (DoI/NBC) contract number D12PC000337, USGovernment authorized reproduce distribute reprints Governmental purposesnotwithstanding copyright annotation thereon.Disclaimer: views conclusions contained herein authorsinterpreted necessarily representing official policies endorsements, eitherexpressed implied, IARPA, DoI/NBC, US Government.preliminary version paper appeared Twenty-Seventh AAAI ConferenceArtificial Intelligence (2013).ReferencesAchlioptas, D., Clauset, A., Kempe, D., & Moore, C. (2005). bias traceroutesampling: or, power-law degree distributions regular graphs. Proceedingsthirty-seventh annual ACM Symposium Theory Computing, pp. 694703. ACM.Adiga, A., Kuhlman, C., Mortveit, H. S., & Vullikanti, A. K. S. (2014). Sensitivity diffusion dynamics network uncertainty. Technical report, availablehttp://ndssl.vbi.vt.edu/supplementary-info/vskumar/sensitivity-jair.pdf.Bakshy, E., Hofman, J. M., Mason, W. A., & Watts, D. J. (2011). Everyones influencer:quantifying influence Twitter. Proceedings fourth ACM international224fiSensitivity Diffusion Dynamics Network Uncertaintyconference Web search data mining, pp. 6574. ACM.Bollobs, B. (1985). Random graphs. Academic Press.Borgatti, S., Carley, K., & Krackhardt, D. (2006). robustness centrality measuresconditions imperfect data. Social Networks, 28, 124136.Chen, J. (2010). effects demographic spatial variability epidemics: comparisonBeijing, Delhi Los Angeles. Conf. Crit. Inf.Chung, F., & Lu, L. (2002). Connected components random graphs given expecteddegree sequences.. Annals Combinatorics, 6, 125145.Clauset, A., Moore, C., & Newman, M. (2008). Hierarchical structure predictionmissing links networks. Nature, 453, 98101.Costenbader, E., & Valente, T. (2003). stability centrality measures networkssampled. Social Networks, 25, 283307.Eubank, S. (2010). Detail network models epidemiology: yet?. JournalBiological Dynamics, 4(5), 446455.Faloutsos, M., Faloutsos, P., & Faloutsos, C. (1999). power-law relationshipsinternet topology. SIGCOMM, Vol. 29, pp. 251262.Flaxman, A., & Frieze, A. M. (2004). diameter randomly perturbed digraphsapplications. APPROX-RANDOM, pp. 345356.Flaxman, A. (2007). Expansion lack thereof randomly perturbed graphs. InternetMathematics, 4 (2-3), 131147.Galuba, W. (2010). Outtweeting twitterers - predicting information cascades microblogs. WOSN.Goldenberg, J., Libai, B., & Muller, E. (2001). Talk network: complex systems lookunderlying process word-of-mouth. Marketing Letters.Gonzlez-Bailn, S., Borge-Holthoefer, J., Rivero, A., & Moreno, Y. (2011). dynamicsprotest recruitment online network. Scientific Reports, 1.Hagmann, P. (2008). Mapping structural core human cerebral cortex. PLoS Biol,6(7).Kaneko, K. (1985). Spatiotemporal intermittency coupled map lattices. ProgressTheoretical Physics, 74 (5), 10331044.Kempe, D., Kleinberg, J., & Tardos, . (2003). Maximizing spread influencesocial network. Proceedings ninth ACM SIGKDD international conferenceKnowledge discovery data mining, pp. 137146. ACM.Kempe, D., Kleinberg, J., & Tardos, . (2005). Influential nodes diffusion modelsocial networks. Automata, languages programming, pp. 11271138. Springer.Kuhlman, C., Kumar, V., & Ravi, S. (2013). Controlling opinion propagation onlinenetworks. Journal Computer Networks, 57, 21212132.Lahiri, M., Maiya, A. S., Caceres, R. S., Habiba, & Berger-Wolf, T. Y. (2008). impactstructural changes predictions diffusion networks. ICDM, pp. 939948.225fiAdiga, Kuhlman, Mortveit & VullikantiLeskovec, J. (2011). Stanford network analysis project. http://snap.stanford.edu/index.html.Newman, M. (2003). structure function complex networks. SIAM Review, 45 (2),167256.Romero, D., Meeder, B., & Kleinberg, J. (2011). Differences mechanics informationdiffusion across topics: idioms, political hashtags, complex contagion twitter.Proc. WWW, pp. 695704. ACM.Ron, D. (2010). Algorithmic analysis techniques property testing. FoundationsTrends TCS, 5 (2), 73205.Schwab, D. J., Bruinsma, R. F., Feldman, J. L., & Levine, A. J. (2010). Rhythmogenicneuronal networks, emergent leaders, k-cores. Phys. Rev. E, 82, 051911.Spielman, D. (2009). Smoothed analysis: attempt explain behavior algorithmspractice. Communications ACM, 7684.Wu, C. W. (2005). Synchronization networks nonlinear dynamical systems coupled viadirected graph. Nonlinearity, 18, 10571064.226fiJournal Artificial Intelligence Research 51 (2014) 413-441Submitted 06/14; published 10/14Scoring Functions Based Second Level Scorek-SAT Long ClausesShaowei CaiSHAOWEICAI . CS @ GMAIL . COMState Key Laboratory Computer Science,Institute Software, Chinese Academy Sciences, Beijing, ChinaQueensland Research Lab, NICTA, Brisbane, AustraliaChuan LuoCHUANLUOSABER @ GMAIL . COMKey Laboratory High Confidence Software Technologies,Peking University, Beijing, ChinaKaile SuK . SU @ GRIFFITH . EDU . AUInstitute Integrated Intelligent Systems,Griffith University, Brisbane, AustraliaAbstractwidely acknowledged stochastic local search (SLS) algorithms efficiently findmodels satisfiable instances satisfiability (SAT) problem, especially random k-SATinstances. However, compared random 3-SAT instances SLS algorithms shown greatsuccess, random k-SAT instances long clauses remain difficult. Recently, notionsecond level score, denoted score2 , proposed improving SLS algorithms long-clauseSAT instances, first used powerful CCASat solver tie breaker.paper, propose three new scoring functions based score2 . Despite simplicity,functions effective solving random k-SAT long clauses. first functioncombines score score2 , second one additionally integrates diversification propertyage. two functions used developing new SLS algorithm called CScoreSAT.Experimental results large random 5-SAT 7-SAT instances near phase transition showCScoreSAT significantly outperforms previous SLS solvers. However, CScoreSAT cannotrival competitors random k-SAT instances phase transition. improve CScoreSATinstances another scoring function combines score2 age. resultingalgorithm HScoreSAT exhibits state-of-the-art performance random k-SAT (k > 3) instancesphase transition. also study computation score2 , including implementationcomputational complexity.1. IntroductionBoolean Satisfiability (SAT) problem prototypical NP-complete problem whose taskdecide whether variables given Boolean formula assigned way makeformula evaluate TRUE. problem plays prominent role various areas computerscience artificial intelligence, widely studied due significant importancetheory applications.Two popular approaches solving SAT conflict driven clause learning (CDCL)stochastic local search (SLS). latter operates complete assignments tries find modeliteratively flipping variable. Although SLS algorithms typically incomplete sense2014 AI Access Foundation. rights reserved.fiC AI , L UO & Ucannot prove instance unsatisfiable, often find models satisfiable formulassurprisingly effectively.SLS algorithms SAT switch two different modes, i.e., greedy(intensification) mode diversification mode. greedy mode, prefer flip variableswhose flips decrease number falsified clauses; diversification mode, tendbetter explore search space avoid local optima, usually using randomized strategiesexploiting diversification properties variables pick variable aim.SLS well known effective approach solving random satisfiable instances,SLS algorithms often evaluated uniform random k-SAT benchmarks. benchmarkslarge variety instances test robustness algorithms, controllinginstance size clause-to-variable ratio, provide adjustable hardness levels assesssolving capabilities. Moreover, performance algorithms usually stable random kSAT instances, either good bad. Thus, easily recognize good heuristics testing SLSalgorithms random k-SAT instances, heuristics may beneficial solving realisticproblems. Numerous works devoted designing SLS algorithms random k-SATinstances clause-to-variable ratio near solubility phase transition,difficult among random k-SAT instances (Kirkpatrick & Selman, 1994).Among random k-SAT instances, random 3-SAT ones exhibit particular statisticalproperties easy solve, example, SLS algorithms statistical physics approachcalled Survey Propagation (Braunstein, Mzard, & Zecchina, 2005). shownfamous SLS algorithm WalkSAT (Selman, Kautz, & Cohen, 1994), proposed twodecades ago, scales linearly number variables random 3-SAT instances near phasetransition solve instances one million variables (Kroc, Sabharwal, & Selman,2010). latest state art direction SLS algorithm called FrwCB, solvesrandom 3-SAT instances near phase transition (at ratio 4.2) millions variables within 2-3hours (Luo, Cai, Wu, & Su, 2013).contrast, random k-SAT instances long clauses remain difficult,performance SLS algorithms instances stagnated long time. Indeed,instances challenging kinds algorithms, including Survey Propagation algorithm,solves random 3-SAT instances extremely fast (Mzard, 2003) also adapted solvingMaxSAT (Chieu & Lee, 2009). Recently, progresses Sattime (Li & Li, 2012), probSAT(Balint & Schning, 2012) CCASat (Cai & Su, 2013b), made direction.particular, solving random instances near phase transition, Sattime algorithm goodsolving random 6-SAT 7-SAT instances, probSAT good solving random 4-SAT5-SAT instances. Comparatively, CCASat shows good performance random k-SAT instancesk {4, 5, 6, 7} random track SAT Challenge 2012. Note secondthird solvers track variants portfolio solver SATzilla (Xu, Hutter, Hoos, & LeytonBrown, 2008). hand, probSAT Sattime show better performance CCASatrandom k-SAT instances threshold ratio phase transition.key notion CCASat score2 property1 , shares spiritcommonly used score property regarded second level score. considerstransformations clauses one true literal two true literals. breakingties using score2 , performance CCASat significantly improved random k-SAT instances1. score2 property denoted subscore CCASat (Cai & Su, 2013b).414fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSESk > 3 (Cai & Su, 2013b). leads us question improve SLSalgorithms instances making better use score2 property? paper, givepositive answer question proposing three new scoring functions based score2 ,using develop two SLS algorithms outperform state-of-the-art solvers randomk-SAT k > 3 near phase transition.first scoring function proposed paper called cscore, linear combinationscore score2 . cscore function differs previous hybrid scoring functionsconsiders two score properties different levels. Based scoring function, also definenew type decreasing variables namely comprehensively decreasing variables. cscorefunction enhances intensification greedy mode integrating current greedinesslook-ahead greediness. Further, combining cscore diversification property age (thedefinition age found Section 2.1), propose second scoring function dubbedhscore, used improve diversification mode. two scoring functions useddevelop SLS algorithms called CScoreSAT.conduct extensive experiments compare CScoreSAT state-of-the-art SLS solversincluding winners recent SAT competitions. experiments large random 5SAT 7-SAT instances near phase transition show CScoreSAT significantly outperformscompetitors terms success rate run time. particular, CScoreSAT able solve random5-SAT instances 5000 variables random 7-SAT instances 300 variables,whereas competitors fail solve instances size.However, performance CScoreSAT random k-SAT instances threshold ratiophase transition good state-of-the-art solvers probSAT Sattime,top two solvers random SAT category SAT Competition 2013. Note majorpart random SAT benchmark SAT Competition 2013 consists random k-SAT instancesphase transition.second contribution paper improve CScoreSAT random k-SAT instancesthreshold ratio phase transition. idea reduce intensification greedymode, instances fewer models (if satisfiable). considerations give risethird scoring function dubbed hscore2 , combines score2 age. function usedimprove greedy mode CScoreSAT, leading new algorithm called HScoreSAT.greedy mode, HScoreSAT utilizes score property pick flipping variable, breaks tieshscore2 function. evaluate HScoreSAT random k-SAT (k > 3) instances thresholdratio phase transition, including SAT Competition 2013, experimental resultsshow HScoreSAT significantly improves CScoreSAT instances.note first two functions CScoreSAT algorithm (Section 3),presented conference paper (Cai & Su, 2013a), third scoring functionHScoreSAT algorithm (Section 4), well experimental analyses (including Section 3.5whole Section 5) new contributions paper.paper proceeds follows. Section 2 introduces preliminary concepts. Section 3presents cscore hscore functions describes CScoreSAT algorithm, alongexperimental evaluations analyses CScoreSAT random k-SAT (k > 3) instances nearphase transition. Section 4 presents hscore2 function HScoreSAT algorithm,well evaluations HScoreSAT random k-SAT (k > 3) instances phase transitionrelated experimental analyses. Section 5, study computation score2 , including415fiC AI , L UO & Uimplementation, complexity computational overhead. Finally, give concludingremarks future directions Section 6.2. Preliminariessection, first introduce basic definitions notation problem. Then,briefly review notion second level properties related works. Finally, introduceconfiguration checking strategy, also important component algorithms.2.1 Basic Definitions NotationGiven set n Boolean variables {x1 , x2 , ..., xn }, literal either variable x (which calledpositive literal) negation x (which called negative literal), clause disjunctionliterals. conjunctive normal form (CNF) formula F = c1 c2 ... cm conjunctionclauses. satisfying assignment formula assignment variables formulaevaluates true. Given CNF formula F , Boolean Satisfiability problem find satisfyingassignment prove none exists.well-known generation model SAT uniform random k-SAT model (Achlioptas,2009). random k-SAT instance, clause contains exactly k distinct non-complementaryliterals, picked uniform probability distribution set 2k nk possible clauses.clause-to-variable ratio CNF formula F defined r = m/n, n numbervariables number clauses.CNF formula F , use V (F ) denote set variables appear F . sayvariable appears clause, clause contains either x x. Two variables neighborsappear simultaneously least one clause. neighbourhood variable xN (x) = {y|y occurs least one clause x}, set neighboring variablesvariable x. subset X V (F ) assignment , [X] projectionvariables X.say literal true current value variable phase. E.g.,x1 = false, negative literal x1 true, positive literal x1 true. clausesatisfied least one true literal, falsified otherwise.SLS algorithms SAT usually select variable flip step guidancescoring f unctions. SLS algorithms one scoring function, adopt onecurrent search step according conditions, whether local optimumreached. scoring function simple variable property mathematical expressionone properties.Perhaps popular variable property used SLS algorithms SAT score,measures increase number satisfied clauses flipping variable. score propertyalso defined score(x) = make(x) break(x), make break numberclauses would become satisfied falsified, respectively, x flipped. Notetwo definitions score equivalent. dynamic local search algorithms use clauseweighting techniques, score measures increase total weight satisfied clauses flippingvariable, make break measures total weight clauses would become satisfiedfalsified, respectively, flipping x. variable decreasing score positive,increasing score negative. age variable defined number search stepsoccurred since variable last flipped.416fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES2.2 Second Level Propertiessubsection, introduce second level properties, especially second level score,important concept proposed scoring functions work.second level properties take account satisfaction degree clauses, definednumber true literals clause (Cai & Su, 2013a). clause satisfaction degreesaid -satisfied clause. variable x, score2 (x) defined make2 (x) minusbreak2 (x), make2 (x) number 1-satisfied clauses would become 2-satisfiedflipping x, break2 (x) number 2-satisfied clauses would become 1-satisfiedflipping x. One easily define properties levels weighted versionproperties.first SLS solver using second level properties CCASat (Cai & Su, 2013b), simplyuses score2 tie breaker achieves surprising improvements random k-SAT longclauses. Then, conference version paper, combine score score2 developCScoreSAT algorithm (Cai & Su, 2013a). also propose notion multi-level propertiesuse make2 improve famous WalkSAT/SKC algorithm (Cai, Su, & Luo, 2013a). Afterwards,multi-level break used improve probSAT solver (Balint, Biere, Frhlich, & Schning,2014). work, exploit score2 property using design scoring functionsdirectly guide algorithm pick flipping variable.note algorithms work utilize unweighted version score2 (althoughuse weighted version score), CCASat does. algorithms, unweightedscore2 found much effective weighted one, yet time couldfigure reason find effective way using weighted score2 algorithms.2.3 Configuration Checking SATsubsection, briefly introduce configuration checking (CC) strategy SAT,important component proposed algorithms work.Initially introduced improving local search Minimum Vertex Cover (MVC) problem(Cai, Su, & Sattar, 2011), CC strategy aims avoiding cycling local search, i.e., revisitingalready visited candidate solutions early. successfully used MVC (Cai et al.,2011; Cai, Su, Luo, & Sattar, 2013b), well SAT (Cai & Su, 2012; Luo et al., 2013; Abram,Habet, & Toumi, 2014; Luo, Cai, Wu, & Su, 2014; Li, Huang, & Xu, 2014) MaxSAT (Luo, Cai,Wu, Jie, & Su, 2014).CC strategy based concept configuration. One define configurationdifferent ways design different CC strategies accordingly. context SAT,configuration variable typically refers truth values neighboring variables (Cai &Su, 2013b). Formally, given assignment , CC strategy SAT defines configurationC(xi ) variable xi subset restricted variables N (xi ), i.e., C(xi ) = [N (xi )].variable C(xi ) flipped since last flip xi C(xi ) said changed. CCstrategy SAT forbids flip variable xi configuration C(xi ) changed sincelast flip xi .CC strategy used decrease blind unreasonable greedy search. strategysuccessfully applied SAT solving, resulting several efficient SLS algorithms SAT,CCASat (Cai & Su, 2013b), Ncca+ (the bronze medal winner random SAT track SATCompetition 2013) (Abram et al., 2014), BalancedZ (Li et al., 2014) CSCCSat (Luo et al.,417fiC AI , L UO & U2014) (the silver bronze medal winner random SAT track SAT Competition 2014),CCAnr+glucose (Cai & Su, 2012) (the silver medal winner hard combinatorial SAT track SATCompetition 2014), etc.3. Two New Scoring Functions CScoreSAT Algorithmsection, design two new scoring functions, namely cscore hscore. usedevelop new SLS algorithm called CScoreSAT, shows excellent performance randomk-SAT k > 3 near phase transition.3.1 cscore Functionsubsection, introduce cscore (short comprehensive score) function,linear combination score score2 properties.score property characterizes greediness flipping variable current search step,tends decrease number falsified clauses, indeed aim SAT problem.hand, score2 property regarded measurement look-ahead greediness,tends reduce 1-satisfied clauses transforming 2-satisfied clauses, noting1-satisfied clauses may become falsified next step 2-satisfied ones not.seems short sighted simply take score property scoring function, especiallyformulas long clauses, number true literals varies considerably among satisfiedclauses. address issue, propose scoring function incorporates scorescore2 . deciding candidate variables priorities selected, although scoreimportant score2 , cases score2 allowed overwrite priorities.example, two variables relatively small score difference significant score2difference, advisable prefer flip one greater score2 .considerations suggest two principles designing desired scoring functions.First, score property plays important role;Second, score2 property allowed overwrite variables priorities (ofselected).result, notion comprehensive score, formally defined follows.Definition 1. CNF formula F , comprehensive score function, denoted cscore,function V (F )cscore(x) = score(x) + score2 (x)/d,positive integer parameter.Note cscore defined integer function, thus value cscorerounded integer not.cscore function linear combination score score2 bias towards score,thus embodies two principles well. function simple computed littleoverhead parameter easily tuned. Moreover, simplicity allows potential usagesolving structured SAT instances perhaps combinatorial search problems.Recall variable decreasing positive score. following,define new type deceasing variables based cscore function.418fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSESDefinition 2. Given CNF formula F cscore function, variable x comprehensivelydecreasing cscore(x) > 0 score(x) 0.condition cscore(x) > 0 straightforward, condition score(x) 0requires variable non-increasing. necessary, flipping increasing variable leadslocal search away objective, accepted without controllingmechanism Metroplis probability Simulated Annealing (Kirkpatrick, Gelatt, &Vecchi, 1983), unless algorithm gets stuck local optimum.SLS algorithms SAT prefer flip decreasing variables greedy search mode.respect, notion comprehensively decreasing variables extension decreasingvariables, good alternative considered flip candidates greedy search phases.3.2 hscore Functioncombine cscore diversification property age, resulting hybrid scoring functiondubbed hscore, used improve diversification mode.One commonly used variable property diversification mode SLS algorithmsSAT age. Previous SLS algorithms usually use age pick oldest variable candidatevariable set (Gent & Walsh, 1993; Li & Huang, 2005; Cai & Su, 2012; Abram et al., 2014)break ties (Prestwich, 2005; Pham, Thornton, Gretton, & Sattar, 2007; Luo, Su, & Cai, 2012).opinion, however, oldest strategies strict always prefer oldest one,regardless important information score cscore. Thus, oldest strategiesmay miss better variables quite often.example, suppose SLS algorithm gets stuck local optimum, would like pickone variable flip two variables x1 x2 : two variables similar ages x1older x2 , cscore(x2 ) significantly greater cscore(x1 ). case, believex2 right choice rather older variable x1 , flipping two variables leadssimilar diversification flipping x2 less harm object function.Based considerations, design hybrid scoring function taking accountgreediness information cscore diversification information age. resulting scoringfunction dubbed hscore given follows.Definition 3. CNF formula F , hscore function function V (F )hsocre(x) = cscore(x) + age(x)/ = score(x) + score2 (x)/d + age(x)/,positive integer parameters.algorithms, reaching local optimum, algorithms make use hybridfunction. show hscore function better choice oldest strategydiversification mode.3.3 CScoreSAT Algorithmsection presents CScoreSAT algorithm, adopts cscore function guidesearch greedy mode, makes use hscore function meets local optima.getting details CScoreSAT algorithm, first introduce two techniquesemployed algorithm.419fiC AI , L UO & U1. PAWS weighting scheme. sake diversification, CScoreSAT employs PAWSclause weighting scheme (Thornton, Pham, Bain, & Ferreira Jr., 2004). clauseassociated positive integer weight, initiated 1. local optimumreached, clause weights updated follows. probability sp (the so-calledsmooth probability), satisfied clause whose weight larger one, weightdecreased one; probability (1 sp), weights falsified clauses increasedone.2. Configuration checking. order reduce blind greedy search, utilize configurationchecking strategy SAT (Cai & Su, 2012). Recall variable said configurationchanged last flip least one neighboring variablesflipped. According configuration checking strategy, configuration changedvariables allowed flipped greedy mode.Algorithm 1: CScoreSATInput: CNF-formula F , maxStepsOutput: satisfying assignment F , unknown1 begin2:= randomly generated truth assignment;3step := 1 maxSteps4satisfies F return ;5= {x|x comprehensively decreasing configuration changed } =66v := variable greatest cscore, breaking ties favor oldestone;7else8update clause weights according PAWS;9pick random falsified clause C;10v := variable C greatest hscore;111213:= v flipped;return unknown;endCScoreSAT algorithm outlined Algorithm 1, described below. beginning,CScoreSAT generates random complete assignment, initiates clause weights 1 computesscore score2 variables accordingly. initialization, CScoreSAT executes loopfinds satisfying assignment reaches limited number steps denoted maxSteps (orgiven cutoff time).Like SLS algorithms SAT, CScoreSAT switches two modes. searchstep, works either greedy mode diversification mode, depending whetherexist comprehensively decreasing variables configuration changed. existvariables, CScoreSAT works greedy mode. picks variable greatest cscorevalue flip, breaking ties preferring oldest one.variables comprehensively decreasing configuration changed,CScoreSAT switches diversification mode. first updates clause weights according420fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSESPAWS scheme. randomly selects falsified clause C, picks variable Cgreatest hscore value flip.3.4 Evaluations CScoreSATsubsection, carry extensive experiments evaluate CScoreSAT random k-SATinstances k {4, 5, 6, 7} near phase transition. First, compare CScoreSAT state-ofthe-art SLS solvers random 5-SAT 7-SAT instances. Then, compare CScoreSATstate-of-the-art SLS solvers random k-SAT instances k {4, 5, 6, 7} SAT Challenge2012. Finally, study effectiveness cscore hscore functions empiricalanalysis random 5-SAT 7-SAT instances.3.4.1 B ENCHMARKSE XPERIMENT P RELIMINARIESinstances used experiments generated according random k-SAT modelnear solubility phase transition. Specifically, adopt following five benchmarks. firsttwo benchmarks random 5-SAT, third fourth benchmarks random 7-SAT,last one consists random k-SAT instances k = 4, 5, 6, 7 various ratios.1. 5-SAT Comp11: large random 5-SAT instances SAT Competition 2011 (r = 20,750 n 2000, 50 instances, 10 size).2. 5-SAT Huge: 5-SAT instances generated randomly according random k-SAT model(r = 20, 3000 n 5000, 500 instances, 100 size).3. 7-SAT Comp11: large random 7-SAT instances SAT Competition 2011 (r = 85,150 n 400, 50 instances, 10 size).4. 7-SAT Random: 7-SAT instances generated randomly according random k-SAT model(r = 85, 220 n 300, 500 instances, 100 size).5. SAT Challenge 2012: random k-SAT instances k > 3 SAT Challenge 2012(480 instances, 120 k-SAT, k = 4, 5, 6, 7), vary size ratio.random instances occupy 80% random benchmark SAT Challenge 2012, indicatingimportance random k-SAT instances k > 3 highly recognizedSAT community. instances vary 800 variables r = 9.931 10000 variablesr = 9.0 4-SAT, 300 variables r = 21.117 1600 variables r = 20 5-SAT,200 variables r = 43.37 400 variables = 40 6-SAT, 100 variablesr = 87.79 200 variables r = 85 7-SAT.parametersp (for PAWS)4-SAT0.62920005-SAT0.62820006-SAT0.9720007-SAT0.962000Table 1: Parameter setting CScoreSAT421fiC AI , L UO & UCScoreSAT implemented C++ compiled g++ -O2 option. parametersetting CScoreSAT reported Table 1. compare CScoreSAT four state-of-the-artSLS solvers, including Sparrow2011 (Balint & Frhlich, 2010), CCASat (Cai & Su, 2013b),probSAT (Balint & Schning, 2012), Sattime2012 (Li & Li, 2012). Sparrow2011probSAT gold medal random SAT track SAT competitions 2011 2013respectively. CCASat winner category SAT Challenge 2012. Sattime regularlymedals SAT competitions track.experiments carried parallel workstation 32-bit Ubuntu Linux OperationSystem, using 2 cores Intel(R) Core(TM) 2.6 GHz CPU 8 GB RAM. experimentsconducted EDACC, experimental platform testing SAT solvers, usedSAT Challenge 2012 SAT Competition 2013. run terminates upon either findingsatisfying assignment reaching given cutoff time set 5000 seconds (as SATCompetition 2011) 5-SAT 7-SAT benchmarks, 1000 seconds SAT Challenge2012 benchmark (close cutoff SAT Challenge 2012, i.e., 900 seconds).5-SAT Comp11 7-SAT Comp11 benchmarks (where instance class 10instances), run solver 10 times instance thus 100 runs instanceclass. 5-SAT Huge 7-SAT Random benchmarks (where instance class contains 100instances) SAT Challenge 2012 benchmark (120 k-SAT instances k), runsolver one time instance, instances class enough test performancesolvers.solver instance class, report number successful runssatisfying assignment found (suc runs) solved instances (#solved), wellPAR10 (par10), penalized average run time timeout solver penalized10(cutoff time). Note PAR10 adopted SAT competitions widely usedliterature prominent performance measure SLS-based SAT solvers (KhudaBukhsh, Xu,Hoos, & Leyton-Brown, 2009; Tompkins & Hoos, 2010; Tompkins, Balint, & Hoos, 2011; Balint& Schning, 2012). results bold indicate best performance instance class.solver successful run instance class, corresponding par10 marked n/a.3.4.2 E XPERIMENTAL R ESULTSCS CORE SATfollowing, present comparative experimental results CScoreSAT competitorsbenchmark.Results 5-SAT Comp11 Benchmark:Table 2 shows comparative results 5-SAT Comp11 benchmark. clear Table2, CScoreSAT shows significantly better performance solvers whole benchmark.CScoreSAT solver solves 5-SAT instances runs. Also, CScoreSATsignificantly outperforms competitors terms run time, obvious instancesize increases. particular, 5-SAT-v2000 instances, largest size SATcompetitions, runtime CScoreSAT 15 times less CCASat, 2 ordersmagnitudes less state-of-the-art SLS solvers.Results 5-SAT Huge Benchmark:experimental results 5-SAT Huge benchmark presented Table 3.encouraging see performance CScoreSAT remains surprisingly good large 5SAT instances, state-of-the-art solvers show poor performance. CScoreSAT solves422fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSESInstanceClass5-SAT-v7505-SAT-v10005-SAT-v12505-SAT-v15005-SAT-v2000Sattime2012suc runspar10100754100125495528856241011443249Sparrow2011suc runspar10100511001591001749912317215288probSATsuc runspar10100881001851002379817537115635CCASatsuc runspar101004710081100128100443934386CScoreSATsuc runspar10100351003810047100145100289Table 2: Experimental results 5-SAT Comp11 benchmark. 10 instancesclass solver executed 10 times instance cutoff time5000 seconds.InstanceClass5-SAT-v30005-SAT-v35005-SAT-v40005-SAT-v45005-SAT-v5000Sattime2012suc runspar100n/a0n/a0n/a0n/a0n/aSparrow2011suc runspar1031353608461474480800n/a0n/aprobSATsuc runspar1040308676471883485910n/a0n/aCCASatsuc runspar106419403353354010452870n/a0n/aCScoreSATsuc runspar10100694100143187816762215133832005Table 3: Experimental results 5-SAT Huge benchmark. 100 instancesclass solver executed one time instance cutoff time 5000seconds.5-SAT instances (at least) 3500 variables consistently (i.e., 100% success rate),30 times faster solvers 5-SAT-v3500 instances. Furthermore, CScoreSATsucceeds 62 38 runs 5-SAT-v4500 5-SAT-v5000 instances respectively, whereascompetitors fail find solution instances. Indeed, bestknowledge, large random 5-SAT instances (at r = 20) solved first time. Givengood performance CScoreSAT 5-SAT instances 5000 variables, confidentcould able solve larger 5-SAT instances.423fiC AI , L UO & UInstanceClass7-SAT-v1507-SAT-v2007-SAT-v2507-SAT-v3007-SAT-v400Sattime2012suc runspar1010049849269982490950n/a0n/aSparrow2011suc runspar1010064217419120n/a0n/a0n/aprobSATsuc runspar1088698011448060n/a0n/a0n/aCCASatsuc runspar1010023272149127467310n/a0n/aCScoreSATsuc runspar10100131905853353407011447760n/aTable 4: Experimental results 7-SAT Comp11 benchmark. 10 instancesclass solver executed 10 times instance cutoff time5000 seconds.InstanceClass7-SAT-v2207-SAT-v2407-SAT-v2607-SAT-v2807-SAT-v300Sattime2012suc runspar10393186813439354481130n/a0n/aSparrow2011suc runspar1013434072490510n/a0n/a0n/aprobSATsuc runspar1010452532490520n/a0n/a0n/aCCASatsuc runspar10681718933341589457365476050n/aCScoreSATsuc runspar1083106396617901532482524392831144889Table 5: Experimental results 7-SAT Random benchmark. 100 instancesclass solver executed one time instance cutoff time5000 seconds.Results 7-SAT Comp11 Benchmark:Table 4 summarizes experimental results 7-SAT Comp11 benchmark. Nonesolvers solve 7-SAT instance 400 variables, indicating random 7-SAT instancesnear phase transition difficult even relatively small size. Nevertheless, CScoreSATsignificantly outperforms competitors 7-SAT benchmark, solversolve 7-SAT instances 300 variables. Actually, competitors become ineffective424fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSESInstanceClass4-SAT5-SAT6-SAT7-SATSattime2012#solvedpar104960313274078431878134222465011Sparrow2011#solvedpar107935145258127241936547142684558probSAT#solvedpar101117785456577638775753802983923CCASat#solvedpar101127517142649918877737343592659CScoreSAT#solvedpar101191748431461109359125594041703Table 6: Experimental results SAT Challenge 2012 benchmark. instance class contains120 instances, solver executed instance cutoff time 1000seconds.(among CCASat highest success rate 7%) 7-SAT-v250 instances,CScoreSAT still achieves success rate 35% instance class.Results 7-SAT Random Benchmark:sizes random 7-SAT instances SAT Competition 2011 continuous enoughprovide good spectrum instances SLS solvers. order investigate detailedperformance CScoreSAT state-of-the-art SLS solvers random 7-SAT instances,evaluate 7-SAT Random benchmark, instance size increases slowly.again, Table 5 suggests difficulty 7-SAT instances increases significantlyrelatively small increment size. reported Table 5, results show CScoreSATdramatically outperforms competitors. Compared competitors whose performancedescends steeply instance size increases, CScoreSAT shows good scalability. example,7-SAT-v220 7-SAT-v260, success rates competitors decline eight timesmore, whereas CScoreSAT drops thirty percents. coming 7-SAT-v260instances, probSAT Sparrow2011 fail runs, competitors succeed less10 runs, CScoreSAT succeeds 53 runs. Finally, CScoreSAT solver survivesthroughout whole benchmark.Results SAT Challenge 2012 Benchmark:investigate performance CScoreSAT random k-SAT instances various k (k >3), compare state-of-the-art solvers random k-SAT instances k > 3 SATChallenge 2012. Table 6 reports number solved instances PAR10 solverk-SAT instance class. results show CScoreSAT significantly outperforms competitorsterms metrics. Overall, CScoreSAT solves 404 instances. observations showCScoreSAT solves 365 instances within half cutoff time, whereas none competitors solves360 instances within cutoff time. encouragingly, Table 6 shows CScoreSATsolves k-SAT instances k, illustrates robustness. good performance425fiC AI , L UO & U1000Sattime2012Sparrow2011probSATCCASatCScoreSAT900800run time (s)7006005004003002001000050100150200#solved250300350400Figure 1: Comparison run time distributions SAT Challenge 2012 benchmark, cutofftime 1000 seconds.CScoreSAT SAT Challenge 2012 benchmark also clearly illustrated Figure 1,summarizes run time distributions solvers benchmark.3.5 Experimental Analyses cscore hscore Functionsorder demonstrate effectiveness cscore hscore functions, also test twoalternative versions CScoreSAT, namely CScoreSAT1 CScoreSAT2 . two algorithmsmodified CScoreSAT follows.CScoreSAT1 : greedy mode, CScoreSAT1 uses score scoring function insteadcscore; also, CScoreSAT1 utilize concept comprehensively decreasing,variable allowed flip decreasing configuration changed.CScoreSAT2 : diversification mode, CScoreSAT2 uses age property insteadhscore scoring fucntion, i.e., picks oldest variable selected falsifiedclause.carry experiments compare CScoreSAT two degraded versions random 5SAT 7-SAT instances. experimental results reported Table 7. obvious observationperformance CScoreSAT1 essentially worse CScoreSAT. example,cannot solve 5-SAT instance 2000 variables 7-SAT instance 250 instance.indicates cscore function critical good performance CScoreSAT. Comparedcscore, hscore function used random mode show much contribution.Nevertheless, usage hscore improve CScoreSATs performance 5-SAT 7-SATinstances. careful comparison CScoreSAT CScoreSAT2 shows hscoreimportant solving 7-SAT instances 5-SAT ones.426fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSESInstanceClass5-SAT-v15005-SAT-v20005-SAT-v40007-SAT-v1507-SAT-v2007-SAT-v250CScoreSAT1suc runspar1041314520n/a0n/a89535922404060n/aCScoreSAT2suc runspar10100152100330781211810056975136691045329CScoreSATsuc runspar101001451002898781671001319058533534070Table 7: Comparison CScoreSAT two alternative algorithms random 5-SAT7-SAT instances.4. Improving CScoreSAT Random k-SAT Phase Transitionsection shows excellent performance CScoreSAT random k-SAT (k > 3) nearphase transition. However, performance CScoreSAT degrades instances phasetransition. CScoreSAT participated satisfiable random category SAT Competition 2013,major part benchmark consists instances generated threshold ratio phasetransition. Although ranked 4th category, performance good enough kindinstances, worse state-of-the-art SLS solvers probSAT Sattime2013,top two solvers satisfiable random category SAT Competition 2013.section improves CScoreSAT random k-SAT (k > 3) phase transition. end,propose another scoring function combining score2 age, utilize improve greedymode CScoreSAT, resulting new algorithm called HScoreSAT. experiments showHScoreSAT significantly improves CScoreSAT gives state-of-the-art performance random kSAT (k > 3) threshold ratio phase transition. also compare CScoreSAT HScoreSATinstances various ratios find boundary ratios beyond HScoreSAT outperformsCScoreSAT.4.1 hscore2 Function HScoreSAT Algorithmimportant issue SLS algorithms SAT balance intensificationdiversification. Indeed, improvements SLS algorithms SAT due proper regulationintensification diversification local search. random k-SAT instances solubilityphase transition, search regions contain model (if instance satisfiable).Therefore, inadvisable strong intensification instances, might wastesearch much time unpromising regions search explore enough regionsdiscovering model.427fiC AI , L UO & Uorder improve CScoreSAT random k-SAT instances phase transition, proposereduce intensification greedy mode. CScoreSAT, use cscore scoring function,break ties age. mentioned before, cscore function quite greedy scoring functioncombines score score2 , represent greediness look-ahead greediness respectively.Therefore, opinion, cscore suitable random k-SAT instances phase transition.Recalling object SLS algorithms SAT minimize number total weightfalsified clauses, score property primary criterion greedy mode. Also,believe score2 property important information solving long-clause instances,considers satisfaction degree clauses. However, score score2 combinedtogether primary scoring function, intensifying solving (satisfiable) random kSAT instances phase transition.Based considerations, move score2 primary scoring functiontie-breaking function, combined diversification property age. leadsnew scoring function refer hscore2 hybrid function score2 age.Definition 4. CNF formula F , hscore2 function function V (F )hscore2 (x) = score2 (x) + age(x)/,positive integer parameter.Accordingly, modify greedy mode CScoreSAT, obtain new algorithmrefer HScoreSAT. pseudo-codes HScoreSAT given Algorithm 2.HScoreSAT differs CScoreSAT following two aspects. First, althoughalgorithms utilize CC strategy, HScoreSAT allows decreasing variables flippedgreedy mode, CScoreSAT allows comprehensively decreasing variables (which super-setdecreasing variables) flipped. importantly, HScoreSAT uses hscore2 break tiesgreedy mode, CScoreSAT breaks ties age.Since hscore2 -based tie-breaking important component HScoreSAT,interested question: exist configuration changed decreasing variables, oftentie-breaking executed pick one them? conducted experimentthreshold benchmark random satisfiable category SAT Competition 2013 calculatefrequency, ratio following two statistics.#stepsccd : number steps configuration changed decreasing (CCD) variablesexist.#stepsbt : number steps configuration changed decreasing (CCD) variablesexist, best CCD variable picked via hscore2 -based tie-breaking.experimental results summarized Table 8, averaged instancesrun per instance. demonstrated Table 8, frequency hscore2 -basedtie-breaking CCD steps significant, high 4-SAT 5-SAT (68% 60%respectively). indicates hscore2 -based tie-breaking mechanism plays critical roleHScoreSAT. Another interesting observation frequency decreases lengthclauses instance.428fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSESAlgorithm 2: HScoreSATInput: CNF-formula F , maxStepsOutput: satisfying assignment F , unknown1 begin2:= randomly generated truth assignment;3step := 1 maxSteps4satisfies F return ;5= {x|x decreasing configuration changed } =66v := variable greatest score, breaking ties preferring onegreatest hscore2 ;7else8update clause weights according PAWS;9pick random falsified clause C;10v := variable C greatest hscore;111213:= v flipped;return unknown;end#stepsbt#stepsccd#stepsbt#stepsccd4-SAT21846513531768273968%5-SAT51441749884951382360%6-SAT19849736840462907449%7-SAT172600954367888938%Table 8: Averaged number CCD steps hscore2 -based tie-breaking steps, wellaveraged ratio k-SAT k {4, 5, 6, 7} threshold benchmark SATCompetition 2013.4.2 Evaluations HScoreSAT Threshold Instancessubsection, carry extensive experiments evaluate HScoreSAT random k-SATinstances k {4, 5, 6, 7} phase transition. First, compare HScoreSAT CScoreSATwell state-of-the-art SLS solvers random benchmark threshold phase transitionSAT Competition 2013. Then, compare HScoreSAT state-of-the-art SLS solverslarge-sized random k-SAT (k {4, 5, 6, 7}) instances generated randomly threshold phasetransition.4.2.1 B ENCHMARKE XPERIMENT P RELIMINARIESexperiments section, benchmark instances generated according randomk-SAT model threshold ratio solubility phase transition. instances clauseto-variable ratio equal conjectured threshold ratio solubility phase transition2 (Mertens,Mzard, & Zecchina, 2006). Specifically, adopt following two benchmarks.2. clause-to-variable ratio 50% uniform random formulas satisfiable. algorithms,closer formula generated near threshold ratio, harder solve it.429fiC AI , L UO & U1. Threshold Comp13: threshold benchmark random satisfiable category SATCompetition 2013. k-SAT, instances various sizes. also notefiltering applied construct competition suite. consequence, significantfraction (approximately 50%) generated threshold instances unsatisfiable. detailsbenchmark given Table 9.2. Large-sized Threshold: random k-SAT instances threshold ratio phase transition,generated randomly random k-SAT generator3 used SAT Competition 2013.benchmark contains 400 instances, 100 k-SAT class k {4, 5, 6, 7}. sizesinstances benchmark (n = 2000, 550, 250, 150 k = 4, 5, 6, 7, respectively)relatively large compared Threshold Comp13 benchmark. instancesevenly divided two categories: training set test set,50 instances k-SAT class.Note training set used tune parameters HScoreSAT,HScoreSAT tuned parameter setting evaluated Threshold Comp13 benchmarktest set Large-sized Threshold benchmark.4-SAT5-SAT6-SAT7-SAT#inst.50505050ratio9.93121.11743.3787.79size n {830, 860, ..., 2300} n {305, 310, ..., 550} n {191, 192, ..., 240} n {91, 92, ..., 140}Table 9: instance numbers, ratios sizes k-SAT k {4, 5, 6, 7}Threshold Comp13 benchmark.HScoreSAT implemented basis CScoreSAT source code complied g++-O2 option. parameter setting HScoreSAT presented Table 10, tunedbased training set Large-sized Threshold benchmark. compare HScoreSATCScoreSAT, well three state-of-the-art SLS solvers, including CCASat, probSAT(version 2013) Sattime2013. Especially, note probSAT Sattime2013 toptwo solvers random SAT track SAT Competition 2013.parametersp4-SAT0.759505005-SAT0.7581005006-SAT0.9275005007-SAT0.96500500Table 10: Parameter setting HScoreSATcomputing environments experiments used experimentsSection 3. Following experiment setup SAT Competition 2013, perform solverone run instance, run terminates upon either finding satisfying assignmentreaching given cutoff time set 5000 seconds. report number solved instances3. http://sourceforge.net/projects/ksatgenerator/430fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSESInstanceClass4-SAT(r = 9.931)5-SAT(r = 21.117)6-SAT(r = 43.37)7-SAT(r = 87.79)CCASat#solvedpar101040168941117153543523273085736007Sattime2013#solvedpar10842174941153183144526243516135006probSAT#solvedpar1013371601040185153544023273636135037CScoreSAT#solvedpar10941193842176133721121291895137442HScoreSAT#solvedpar1014361761139296193153825252756933071Table 11: Experimental results Threshold Comp13 benchmark. instance class contains50 instances solver executed instance cutoff time 5000seconds.(#solved) PAR10 k-SAT class whole benchmark (as competition).rules SAT competitions establish winner solver solves instances,ties broken selecting solver minimum PAR10.4.2.2 E XPERIMENTAL R ESULTSHRESHOLD B ENCHMARKfollowing, present comparative experimental results HScoreSAT competitorsbenchmark.Results Threshold Comp13 Benchmark:Table 11 presents experimental results HScoreSAT competitors random kSAT instances phase transition SAT Competition 20134 . Since HScoreSAT basedCScoreSAT, first compare two solvers. shown Table 11, HSocreSAT solvesinstances CScoreSAT instance classes. Overall, CScoreSAT solves 51 instances,HScoreSAT solves 69 instances, 1.35 times many CScoreSAT does.HScoreSAT solves instances probSAT Sattime2013. Overall, HScoreSATsolves 69 instances, compared 61 probSAT Sattime2013 57 CCASat.observation shows that, HScoreSAT similar performance probSAT random 4-SAT5-SAT instances, similar performance Sattime2013 6-SAT 7-SAT instances.Results Large-sized Threshold Benchmark:mesure performance HScoreSAT random phase-transition k-SAT instancesaccurately, additionally test HScoreSAT test set Large-sized Thresholdbenchmark, compared Sattime2013 probSAT, top two solvers randomSAT track SAT Competition 2013.4. seems machine slightly slower ones used SAT Competition 2013, Sattime2013, probSATCScoreSAT solved slightly fewer instances experiment competition. CCASatparticipate SAT Competition 2013.431fiC AI , L UO & Uresults presented Table 12. 4-SAT class, HScoreSAT probSAT solvenumber instances, HScoreSAT less accumulative run time. 5-SAT 6SAT classes, HScoreSAT solves instances. Particularly, HScoreSAT shows significantlysuperior performance solvers 6-SAT class, solves 9 instances,Sattime2013 probSAT solve 4 instances. instance class HScoreSATgive best performance 7-SAT. Nevertheless, instance class, HScreSATsimilar performance best solver Sattime2013, solving one less instance. wholebenchmark, HScoreSAT solves 40 instances, compared 26 28 instances Sattime2013probSAT.InstanceClass4-SAT-v2000(r = 9.931)5-SAT-v550(r = 21.117)6-SAT-v300(r = 43.37)7-SAT-v150(r = 87.79)Sattime2013#solvedpar100n/a84214744612014364332643675probSAT#solvedpar108421979412624461327432482843209HScoreSAT#solvedpar10842181104013094152313370914040231Table 12: Experimental results Large-size Threshold benchmark. instance classcontains 50 instances solver executed instance cutofftime 5000 seconds.4.3 Experimental Analyses hscore2 Functiondemonstrate effectiveness hscore2 function, test two alternative versionsHScoreSAT. two algorithms different HScoreSAT tie-breakingmechanism greedy mode.HScoreSAT1 breaks ties greedy mode preferring variable greatestscore2 ;HScoreSAT2 breaks ties greedy mode preferring variable greatest age.comparative results HScoreSAT two alternative versions displayed Table13. clear table HScoreSAT superior performance alternativesinstance classes. careful observations show 4-SAT 5-SAT instances,performance HScoreSAT2 HScoreSAT similar, significantly betterHScoreSAT1 . indicates age property suitable score2 tie-breaker 4SAT 5-SAT, almost good hscore2 tie-breaking. contrast, performance432fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSESInstanceClass4-SAT(r = 9.931)5-SAT(r = 21.117)6-SAT(r = 43.37)7-SAT(r = 87.79)HScoreSAT1#solvedpar10545062743136163425524263285237195HScoreSAT2#solvedpar1013372421040293163437122281286135008HScoreSAT#solvedpar1014361761139296193153825252756933071Table 13: Comparative results HScoreSAT two alternative solvers ThresholdComp13 benchmark. solver executed one time instance, cutofftime 5000 seconds.HScoreSAT1 HScoreSAT 7-SAT similar, better HScoreSAT2 .indicates score2 suitable age tie-breaker 7-SAT. Indeed,experimental analysis gain intuition set hscore2 function relativelysmall value 4-SAT 5-SAT instances, relatively large value 7-SAT. However,6-SAT, alternatives cannot achieve performance close HScoreSAT.4.4 Evaluation Huge Random k-SAT SAT Competition 2013random SAT category SAT Competition 2013, two kinds instances. Besidesinstances phase-transition threshold, also instances whose ratios closephase transition time huge sizes. subsection, conductexperiments evaluate performance solvers huge instances, comparedstate-of-the-art solvers.experimental results presented Table 14, show CScoreSAT clearlybest solver benchmark huge instances. CScoreSAT gives best performancek-SAT instance classes except 4-SAT, especially solves 6-SAT 7-SAT instancessolvers. 4-SAT, CScoreSAT solves many instances probSAT PAR10little probSATs. experimental results confirm good performanceCScoreSAT huge benchmark SAT Competition 2013, also solved hugeinstances probSAT Sattime2013.4.5 Boundary Ratios Performance CScoreSAT HScoreSATmentioned before, CScoreSAT good performance solving random k-SAT (k > 3)instances ratios near phase-transition threshold, ratios large randominstances SAT Competition 2011. hand, HScoreSAT improved CScoreSATsolving random k-SAT (k > 3) instances phase-transition threshold. Thus, conjecture433fiC AI , L UO & UInstanceClass4-SAT-v500000(r [7.0, 9.5])5-SAT-v250000(r [15.0, 20.0])6-SAT-v100000(r [30.0, 40.0])7-SAT-v50000(r [60.0, 85.0])CCASat#solvedpar104170173251062333861416931029300Sattime2013#solvedpar10325205233502233710141963833595probSAT#solvedpar10583854167102333381416691225026CScoreSAT#solvedpar10590964165353252002341671421249HScoreSAT#solvedpar10594183251162333761416831127398Table 14: Experimental results huge random k-SAT (k > 3) instances SATCompetition 2013. instance class contains 6 instances solver executedinstance cutoff time 5000 seconds.exists boundary ratio k-SAT, beyond HScoreSAT outperforms CScoreSAT.subsection dedicated finding boundary ratios experiments.experiment carried SAT Challenge 2012 benchmark, k-SAT 10different ratios 12 instances ratio. Details bechmark foundbenchmark description. run CScoreSAT HScoreSAT one time instancecutoff time 1000 seconds, compare performance two solvers ratio.comparison results presented Table 15. results suggest existsboundary ratio r , beyond HScoreSAT gives better performance CScoreSAT.especially clear 4-, 5- 7-SAT, clear 6-SAT HScoreSAT solvesinstances CScoreSAT rations smaller 42.359 except one ratior = 42.696, CScoreSAT solves one instance. check whether resultoutlier due single instance, conduct additional experiment execute solvers 10times instance r = 42.696. experimental results show two solversclose performance HScoreSAT succeeds 84 runs CScoreSAT succeeds 83 runs., rgive conjectured interval (rminmax ) boundary ratio r k-SAT Table 16.results suggest that, hybrid solver combining CScoreSAT HScoreSAT wouldgood performance k-SAT instances long clauses different ratios. However,CScoreSAT HScoreSAT poor performance random 3-SAT instances. Hence,two solvers hybrid solver participate SAT Competition 2014, competitionrequires participating solver core solver two different solvers.Instead, develop solver called CSCCSat, combines two solvers namely FrwCB (forlarge sized instances) (Luo et al., 2014) DCCASat (for threshold instances) (Luo et al., 2014).Note DCCASat improved HScoreSAT using double configuration checkingheuristic, also uses hscore hscore2 functions random k-SAT k > 3. SATCompetition 2014, CSCCSat bronze medal random SAT track, especially givesbest performance threshold instances, indicating effectiveness scoring functions.434fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSES4-SATr=94-SATr=9.1214-SATr=9.2234-SATr=9.3244-SATr=9.4254-SATr=9.5264-SATr=9.6274-SATr=9.7294-SATr=9.834-SATr=9.931CScoreSATHScoreSAT4.8(12)6.5(12)9.7(12)10.8(12)11.6(12)18.6(12)19.1(12)29.1(12)42.7(12)63.8(12)66.5(12)72.2(12)133(12)192(12)243(12)212(12)1252(11)344(12)149(12)118(12)5-SATr=205-SATr=20.1555-SATr=20.2755-SATr=20.3955-SATr=20.5165-SATr=20.6365-SATr=20.7565-SATr=20.8765-SATr=20.9975-SATr=21.117CScoreSATHScoreSAT222(12)1060(11)271(12)2022(10)1918(10)3471(8)2935(9)6025(5)6108(5)7647(3)4363(7)6003(5)3468(8)4452(7)5118(6)3051(8)5100(6)4436(7)2522(9)1772(10)6-SATr=406-SATr=40.6746-SATr=41.0116-SATr=41.3486-SATr=41.6856-SATr=42.0226-SATr=42.3596-SATr=42.6966-SATr=43.0336-SATr=43.37CScoreSATHScoreSAT4.1(12)11.5(12)8.6(12)36.3(12)29.1(12)52.3(12)76.6(12)98.4(12)83(12)1827(10)31(12)1928(10)1003(11)227(12)2757(9)3528(8)4418(7)3647(8)939(11)165(12)7-SATr=857-SATr=85.5587-SATr=85.8377-SATr=86.1167-SATr=86.3957-SATr=86.6747-SATr=86.9537-SATr=87.2327-SATr=87.5117-SATr=87.79CScoreSATHScoreSAT4345(7)8346(2)2767(9)6773(4)2666(9)5065(6)3435(8)4296(7)1902(10)2797(9)3499(8)4340(7)3415(8)2646(9)150(12)135(12)2589(9)1054(11)899(11)74(12)Table 15: Comparing CScoreSAT HScoreSAT ratio random k-SAT (k > 3) SATChallenge 2012 benchmark. solver executed one time instance, cutofftime 1000 seconds. cell reports result CScoreSAT upper rowHScoreSAT lower row, form par10(#solved). color ratios grayHScoreSAT performs better CScoreSAT. Note 6-SAT r = 42.696 resultsseem little odd, conduct additional experiment executing solvers 10 timesinstance, HScoreSAT succeeds 84 runs CScoreSAT succeeds 83 runs., r(rminmax )4-SAT(9.627,9.729)5-SAT(20.756,20.876)6-SAT(42.022,42.359)7-SAT(86.674,86.953)Table 16: conjectured interval boundary ratio r . HScoreSAT worse performance, better (or least competitive) performanceCScoreSAT ratios r rminratios r rmax, based experiments SAT Challenge 2012 benchmark.5. Computation score2algorithms employing scoring functions based score2 , CScoreSAT HScoreSAT,computation score2 considerable impact efficiency. section,investigate computation issues score2 . Particularly, propose cache-basedimplementation analyze time complexity flip. also measure overheadtwo algorithms experiments.435fiC AI , L UO & U5.1 Implementation Complexity Computing score2propose caching implementation computing variables score2 values, inspiredcaching implementation computing variable scores. Typically, variable scores (orscore2 values) change search step; suggests rather recomputing variablescores (or score2 values) step, efficient compute scores (or score2values) search initialized, subsequently update scores (or score2 values)affected variable flipped (Hoos & Sttzle, 2004).caching implementation, score2 values variables computed searchinitialized, subsequently updated flip. initialized computation score2straightforward according definition score2 discussed. Comparatively,procedure updating score2 values much interest.facilitate describing procedure updating score2 values analyzing timecomplexity, first introduce notations definitions below.Given CNF formula F ,variable x V (F ), CL(x) = {c|c clause F x appears c};clause c F , c.num_true_lit number true literals c;clause c F exactly two true literals, use true_lit_var(c) true_lit_var2(c)record two corresponding variables two true literals c (these two notationsused pseudo-code sake formalization);use x denote variable flipped current step;n, m, k, r number variables, number clauses, maximum clause length,clause-to-variable ratio.Definition 5. Given clause c variable x, say contribution clause c score2 (x)+1 flipping x would cause c transform 1-satisfied 2-satisfied, -1 flipping x would causec transform 2-satisfied 1-satisfied, 0 otherwise.useful observation variable x V (F ), score2 (x) equals sum contributionsclauses it. Also, obvious clauses x appear always contribute 0score2 (x).describe detail procedure updating score2 values flip, whose pseudocode shown Algorithm 3. flipping x , according definition score2 , score2 (x )changes opposite number (lines 1 21). essential part updating score2 valuesvariables share clauses x (since variables would change score2 values),accomplished loop (lines 2-20). iteration loop, clause c CL(x )considered necessary updates performed, according two different cases: either literalx c becomes true literal, not. explain updates first case,case understood similarly.first case (i.e., literal x c becomes true literal), along flip x ,c.num_true_lit increased 1. Suppose c.num_true_lit changes 1 t. neither 11 2, flipping x causes change variables score2 (easy seedefinition score2 ). So, need consider following three cases.436fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSESAlgorithm 3: updating score2 values flip step123456789101112131415161718192021org_score2 (x ) := score2 (x );c CL(x )literal x c becomes true literalc.num_true_lit+=1;c.num_true_lit=3score2 (true_lit_var(c))+=1;score2 (true_lit_var2(c))+=1;else c.num_true_lit=2xi c score2 (xi )=1;else c.num_true_lit=1xi c score2 (xi )+=1;elsec.num_true_lit=1;c.num_true_lit=2score2 (true_lit_var(c))=1;score2 (true_lit_var2(c))=1;else c.num_true_lit=1xi c score2 (xi )+=1;else c.num_true_lit=0xi c score2 (xi )=1;score2 (x ) := org_score2 (x );c.num_true_lit changes 2 3: flipping x , c two true literals, let usdenote corresponding variables y1 y2 . contributions c score2 (y1 )score2 (y2 ) -1 flipping x . flipping x , c becomes 3-satisfied clause,contributions c score2 (y1 ) score2 (y2 ) become 0. Hence, alongflipping x , changes score2 (y1 ) score2 (y2 ) 0-(-1)=+1 (lines 6-7).variables c, either flip x , contributions c score2values 0.c.num_true_lit changes 1 2: flipping x , c one true literal, letus denote corresponding variable y1 . contribution c score2 (y1 ) 0flipping x , becomes -1 flipping x , indicating change -1 score2 (y1 ).variables c (except x ), contributions c score2 values +1flipping x , become 0 flipping x , indicating change -1 score2 values.Therefore, variables c (except x ), along flipping x , changesscore2 values -1 (lines 8-9).Note include x loop (line 9) order save computational consumption.special update score2 (x ), change score2 (x ) line 1line 21 impact effect.437fiC AI , L UO & Uc.num_true_lit changes 0 1: flipping x , variables c, contributionsc score2 values 0. flipping x , c becomes 1-satisfied (the true literalscorresponding variable x ), variables c (except x ), contributions cscore2 values become +1. Therefore, variables c (except x ), alongflipping x , changes score2 values +1 (lines 10-11).complexity score2 updating procedure flip determined main loop(lines 2-20). flipping variable x, |CL(x)| items main loop,iteration three possible cases, first case requires 2 operations5 lattertwo require (k) operations. Therefore, worst-case time complexity score2 updatingprocedure flip (maxxV (F ) |CL(x)| max{2, k, k})=(maxxV (F ) |CL(x)| k).uniform random k-SAT formulas clause-to-variable ratio r, totally kliterals variable expected mk/n = kr literals. is, variable x V (F )expected appear kr clauses (when n approaches +, true probability almost1), i.e., |CL(x)| kr. Therefore, complexity score2 updating procedure flipbecomes (kr k) = (k 2 r).Fortunately, uniform random k-SAT formulas constant ratio r, k rconstants. Therefore, independent instance size, implementation score2 computationachieves time complexity (1) search step, caching implementationscore computation (referring pages 272-273 (Hoos & Sttzle, 2004)).5.2 Computational Overhead Computing score2subsection, study computational overhead computing score2 CScoreSATHScoreSAT. carry experiments Threshold Comp13 benchmark figureCPU time per 107 steps computing score2 percentage total CPU time solverper 107 steps.CScoreSATcomputing score2percentageHScoreSATcomputing score2percentage4-SAT13.91.611.5%14.31.611.2%5-SAT28.58.028.1%28.78.429.3%6-SAT52.417.934.2%53.217.833.5%7-SAT94.637.139.2%95.136.938.8%Table 17: CPU time consumption (in seconds) per 107 steps CScoreSAT HscoreSAT,computing score2 , well ratios. results averaged instancesThreshold Comp13 benchmark.investigation shows overhead computing score2 occupies considerablepercentage solvers whole run time, ranging 11% 40%. Nevertheless, since score2critical solvers, price indeed pays off. observation reveals 90%5. note true_lit_var(c) true_lit_var2(c) recorded initially accelerating updating variable scores, thusneed extra price maintain score2 updates.438fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSESsolvers run time due flip function, two costly parts score score2updates. Another interesting phenomenon percentage overhead caused score2computation rises clause length increases, although score2 less used (for tie-breaking)longer clauses (see Table 8). reason might clause length increases, portionvariables whose score2 values need updated increasing compared variables whosescores need updated.6. Summary Future Workpaper, proposed three new scoring functions based score2 improving SLSalgorithms random SAT instances long clauses. Despite simplicity, proposedscoring functions effective, resulting SLS algorithms namely CScoreSATHscoreSAT show excellent performance random k-SAT instances long clauses.First, combined score score2 properties design scoring function named cscore(comprehensive score), aims improve greedy mode combining greediness lookahead greediness. also defined comprehensively decreasing variables accordingly.proposed hscore function combining cscore diversification age, devotedimproving diversification mode. two scoring functions used developCScoreSAT algorithm. experiments show performance CScoreSAT exceedsstate-of-the-art SLS solvers orders magnitudes large random 5-SAT 7-SAT instancesnear phase transition. Moreover, CScoreSAT significantly outperforms competitors randomk-SAT instances various ratios k {4, 5, 6, 7} SAT Challenge 2012.improve CScoreSAT solving random k-SAT instances threshold ratio phasetransition, propose another scoring function called hscore2 , combines score2 age.using hscore2 break ties adjust greedy mode accordingly, obtain HScoreSATalgorithm. Experiments random k-SAT instances phase-transition threshold showHScoreSAT significantly improves CScoreSAT outperforms state-of-the-art SLS algorithms.Finally, score2 property key notion algorithms, also presentimplementation details score2 computation, analyze complexity per flipcomputational overhead.future work, significant research issue improve SLS algorithms structuredinstances score2 -based scoring functions. Furthermore, notions work simpleeasily applied problems, constrained satisfaction graph searchproblems.Acknowledgementwork supported China National 973 Program 2010CB328103 2014CB340301,National Natural Science Foundation China 61370072 61472369, ARC GrantFT0991785. would like thank anonymous referees helpful commentsearlier versions paper, significantly improve quality paper.439fiC AI , L UO & UReferencesAbram, A., Habet, D., & Toumi, D. (2014). Improving configuration checking satisfiablerandom k-SAT instances. Proc. ISAIM-14.Achlioptas, D. (2009). Random satisfiability. Handbook Satisfiability, pp. 245270.Balint, A., Biere, A., Frhlich, A., & Schning, U. (2014). Improving implementation SLSsolvers SAT new heuristics k-sat long clauses. Proc. SAT-14, pp. 302316.Balint, A., & Frhlich, A. (2010). Improving stochastic local search SAT new probabilitydistribution. Proc. SAT-10, pp. 1015.Balint, A., & Schning, U. (2012). Choosing probability distributions stochastic local searchrole make versus break. Proc. SAT-12, pp. 1629.Braunstein, A., Mzard, M., & Zecchina, R. (2005). Survey propagation: algorithmsatisfiability. Random Struct. Algorithms, 27(2), 201226.Cai, S., & Su, K. (2012). Configuration checking aspiration local search SAT. Proc.AAAI-12, pp. 334340.Cai, S., & Su, K. (2013a). Comprehensive score: Towards efficient local search SAT longclauses. Proc. IJCAI-13, pp. 489495.Cai, S., & Su, K. (2013b). Local search Boolean Satisfiability configuration checkingsubscore. Artif. Intell., 204, 7598.Cai, S., Su, K., & Luo, C. (2013a). Improving walksat random k-satisfiability problem k >3. Proc. AAAI-13, pp. 145151.Cai, S., Su, K., Luo, C., & Sattar, A. (2013b). NuMVC: efficient local search algorithmminimum vertex cover. J. Artif. Intell. Res. (JAIR), 46, 687716.Cai, S., Su, K., & Sattar, A. (2011). Local search edge weighting configuration checkingheuristics minimum vertex cover. Artif. Intell., 175(9-10), 16721696.Chieu, H. L., & Lee, W. S. (2009). Relaxed survey propagation weighted maximumsatisfiability problem. J. Artif. Intell. Res. (JAIR), 36, 229266.Gent, I. P., & Walsh, T. (1993). Towards understanding hill-climbing procedures SAT.Proc. AAAI-93, pp. 2833.Hoos, H. H., & Sttzle, T. (2004). Stochastic Local Search: Foundations & Applications. Elsevier/ Morgan Kaufmann.KhudaBukhsh, A. R., Xu, L., Hoos, H. H., & Leyton-Brown, K. (2009). Satenstein: Automaticallybuilding local search SAT solvers components. Proc. IJCAI-09, pp. 517524.Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization simulated annealing. Science,220, 671680.Kirkpatrick, S., & Selman, B. (1994). Critical behavior satisfiability random booleanformulae. Science, 264, 12971301.Kroc, L., Sabharwal, A., & Selman, B. (2010). empirical study optimal noise runtimedistributions local search. Proc. SAT-10, pp. 346351.440fiS CORING F UNCTIONS BASED ECOND L EVEL CORE k-SAT L ONG C LAUSESLi, C. M., Huang, C., & Xu, R. (2014). Balance intensification diversification: unityopposites. Proc. SAT Competition 2014: Solver Benchmark Descriptions, pp.1011.Li, C. M., & Huang, W. Q. (2005). Diversification determinism local search satisfiability.Proc. SAT-05, pp. 158172.Li, C. M., & Li, Y. (2012). Satisfying versus falsifying local search satisfiability - (posterpresentation). Proc. SAT-12, pp. 477478.Luo, C., Cai, S., Wu, W., Jie, Z., & Su, K. (2014). CCLS: efficient local search algorithmweighted maximum satisfiability. IEEE Transactions Computers, press.Luo, C., Cai, S., Wu, W., & Su, K. (2013). Focused random walk configuration checkingbreak minimum satisfiability. Proc. CP-13, pp. 481496.Luo, C., Cai, S., Wu, W., & Su, K. (2014). Double configuration checking stochastic local searchsatisfiability. Proc. AAAI-14, pp. 27032709.Luo, C., Su, K., & Cai, S. (2012). Improving local search random 3-SAT using quantitativeconfiguration checking. Proc. ECAI-2012, pp. 570575.Mertens, S., Mzard, M., & Zecchina, R. (2006). Threshold values random k-SAT cavitymethod. Random Struct. Algorithms, 28(3), 340373.Mzard, M. (2003). Passing messages disciplines. Science, 301, 16851686.Pham, D. N., Thornton, J., Gretton, C., & Sattar, A. (2007). Advances local searchsatisfiability. Australian Conference Artificial Intelligence, pp. 213222.Prestwich, S. D. (2005). Random walk continuously smoothed variable weights. Proc.SAT-05, pp. 203215.Selman, B., Kautz, H. A., & Cohen, B. (1994). Noise strategies improving local search. Proc.AAAI-94, pp. 337343.Thornton, J., Pham, D. N., Bain, S., & Ferreira Jr., V. (2004). Additive versus multiplicative clauseweighting SAT. Proc. AAAI-04, pp. 191196.Tompkins, D. A. D., Balint, A., & Hoos, H. H. (2011). Captain jack: New variable selectionheuristics local search SAT. Proc. SAT-11, pp. 302316.Tompkins, D. A. D., & Hoos, H. H. (2010). Dynamic scoring functions variable expressions:New SLS methods solving SAT. Proc. SAT-10, pp. 278292.Xu, L., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2008). SATzilla: Portfolio-based algorithmselection SAT. J. Artif. Intell. Res. (JAIR), 32, 565606.441fiJournal Artificial Intelligence Research 51 (2014) 555-577Submitted 05/14; published 11/14Iterative Plan Construction Workflow Satisfiability ProblemDavid CohenJason CramptonAndrei GagarinGregory GutinMark JonesD.C OHEN @ RHUL . AC . UKJASON .C RAMPTON @ RHUL . AC . UKNDREI .G AGARIN @ RHUL . AC . UKG.G UTIN @ RHUL . AC . UKARK .J ONES @ RHUL . AC . UKRoyal Holloway, University London, UKAbstractWorkflow Satisfiability Problem (WSP) problem practical interest arises whenever tasks need performed authorized users, subject constraints defined businessrules. required decide whether exists plan assignment tasks authorizedusers constraints satisfied. natural see WSP subclassConstraint Satisfaction Problem (CSP) variables tasks domain setusers. makes WSP distinctive number tasks usually small compared number users, appropriate ask constraint languages WSPfixed-parameter tractable (FPT), parameterized number tasks.novel approach WSP, using techniques CSP, enabled us design genericalgorithm FPT several families workflow constraints considered literature.Furthermore, prove union FPT languages remains FPT satisfy simple compatibility condition. Lastly, identify new FPT constraint language, user-independent constraints, includes many constraints interest business processing systems.demonstrate generic algorithm provably optimal running time (2k log k ),language, k number tasks.1. Introductionworkflow formalises business process. collection interrelated tasks performedusers order achieve objective. many situations, wish restrict usersperform certain tasks. particular, may wish specify lists users authorizedperform workflow tasks. Additionally, may wish either particularrequirements business logic security requirements prevent certain combinationsusers performing particular combinations tasks (Crampton, 2005). constraints includeseparation-of-duty (also known two-man rule), may used prevent sensitivecombinations tasks performed single user, binding-of-duty, requiresparticular combination tasks executed user. use constraints workflowmanagement systems enforce security policies studied extensively last fifteenyears; example see work Bertino, Ferrari, Atluri (1999), Crampton (2005) WangLi (2010).1.1 Workflow Satisfiability Problempossible combination constraints authorization lists unsatisfiable,sense exist assignment users tasks contraints satisfiedc2014AI Access Foundation. rights reserved.fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONESevery task performed authorized user; perhaps minimal example requirementtwo tasks performed user intersection authorization liststasks empty. plan satisfies constraints allocates authorized user task saidvalid. Workflow Satisfiability Problem (WSP) takes workflow specification inputreturns valid plan one exists null value otherwise. important determine whetherbusiness process satisfiable not, since unsatisfiable one never completed withoutviolating security policy encoded constraints authorization lists. Wang Li (2010)shown, reduction G RAPH C OLORING, WSP NP-hard subclassCSP, even consider binary separation-of-duty constraints. Nevertheless, practicalapplications WSP, require solving algorithm efficient possible (Crampton &Gutin, 2013, 2.2).Many hard problems become less complex natural parameter instance bounded.Hence, say problem input size n parameter k fixed-parameter tractable (FPT)admits algorithm running time O(f (k)nd ), constant independent n k,f computable function depending k.1Wang Li (2010) first observe fixed-parameter algorithmics appropriateway study WSP, number tasks usually small often much smallernumber users. (The literature directly support assumption, although widely-citedstudy, Schaad, Moffett, & Jacob, 2001, found number users exceeds job functions,roles, multiplicative factor around 25; finding confirmed recent followup study, Jayaraman, Ganesh, Tripunitara, Rinard, & Chapin, 2011. workflow specificationusually concerned particular business objective involve small number roles.Taking roles proxy tasks, seems reasonable assume number usersorder magnitude greater number tasks.) believe, therefore, appropriateextend work initiated Wang Li use fixed parameter algorithms solvingWSP parameterized number tasks, and, particular, ask constraint languagesfixed parameter tractable.Wang Li (2010) proved that, general, WSP W[1]-hard thus highly unlikelyadmit fixed-parameter algorithm. also showed WSP FPT considerseparation-of-duty binding-of-duty constraints. Crampton, Gutin, Yeo (2013) obtained significantly faster fixed-parameter algorithms applicable regular constraints, therebyincluding cases shown FPT Wang Li. work, recent research,demonstrated existence fixed-parameter algorithms WSP presence constraint types (Crampton, Crowston, Gutin, Jones, & Ramanujan, 2013; Crampton & Gutin, 2013).define WSP formally introduce number different constraint types, including regularconstraints, Section 2.use notation, suppresses polynomial factors.is,g(n, k, m) = (h(n, k, m)) exists polynomial q(n, k, m) g(n, k, m) =O(q(n, k, m)h(n, k, m)). particular, FPT algorithm one runs time (f (k))computable function f depending k.1. introduction fixed-parameter algorithms complexity found in, example, books DowneyFellows (2013), Niedermeier (Niedermeier, 2006).556fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM1.2 Relation WSP CSPConstraint Satisfaction Problem (CSP) general paradigm expressing, declarativeformat, problems variables assigned values domain. assignmentsconstrained restricting allowed simultaneous assignments sets variables.model useful many application areas including planning, scheduling, frequency assignmentcircuit verification (Rossi, van Beek, & Walsh, 2006). CSP community well-establishedresearch community dedicated finding effective solution techniques CSP (Dechter, 2003).CSP NP-hard, even binary not-equals constraints allowed domainthree elements, reduce G RAPH 3-C OLORING CSP. 2 Hence, considerableeffort made understand effect restricting type allowed constraints. Recently significant progress towards completion research programstrong evidence support algebraic dichotomy conjecture Bulatov, JeavonsKrokhin (2005), characterising precisely kinds constraint language lead polynomialsolvability.worth noting WSP subclass CSP variable (called taskWSP terminology) arbitrary unary constraint (called authorization) assignspossible values (called users) s; called conservative CSP. Note, however,usually CSP number variables much larger number values, WSPnumber tasks usually much smaller number users. important rememberWSP use term constraint authorizations define specialtypes constraints, extend types authorizations, remain arbitrary.1.3 Outline Papernovel approach WSP using techniques CSP, characterising types constraintsconstraint languages particular characteristics, enables us generalise unify existingalgorithms. So, paper, first time, rather considering algorithms specificconstraints, design generic algorithm fixed-parameter algorithm several familiesworkflow constraints considered literature. particular introduce notions userindependent constraints, subsume number well-studied constraint types WSPliterature, including regular constraints studied Crampton et al. (2013).generic algorithm builds plans incrementally, discarding partial plans never satisfyconstraints. based naive algorithm, presented Section 2.2. naive algorithmstores far information required solve WSP, running time betterexhaustively searching valid plan.generic algorithm uses general classic paradigm: retain little information possible every step algorithm. paradigm used classical polynomial-time algorithms Gaussian elimination solving systems linear equations constraint propagationalgorithms (used, example, solve 2SAT polynomial time). generic algorithm usesparadigm problem-specific way, based concepts extension-equivalence, planindistinguishability patterns, enabling us retain single pattern equivalence classindistinguishable plans. Extension-equivalence plan encodings described Section 3.way solution constructed algorithm quite unusual accumulation2. Wang Lis NP-hardness result WSP thus restatement well-known result CSP.557fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES(representatives of) set solutions goes along users (i.e., values CSP), along tasks(i.e., variables CSP).analyze running time algorithm introduce notion diversity (see Definition 6). notion reminiscent pathwidth (measures taken prefixes largestoutcome diversity) difference diversity based number equivalence classes, hiding actual structure behind scenes. approach might also usefulstructural analysis hypergraphs.Section 4, describe pattern-based algorithm demonstrate fixedparameter algorithm WSP user-independent constraints. show running timealgorithm (2k log k ) WSP user-independent constraintsalgorithm running time (2o(k log k) ) WSP user-independent constraints unlessExponential Time Hypothesis3 (ETH) fails. Thus, unlike WSP regular constraints(and problems studied Bodlaender, Cygan, Kratsch, & Nederlof, 2013; Fomin, Lokshtanov, &Saurabh, 2014), WSP user-independent constraints highly unlikely admit algorithmrunning time (2O(k) ). show generic algorithm interest constraintsuser-independent, prove generic algorithm single-exponential algorithmconstraint language obtained equivalence relation set users.Section 5 show generic algorithm deal unions constraint languages.leads generalisation result user-independent constraints. Section 6 discussresults computational experiments using implementation algorithm (discussedfull detail work Cohen, Crampton, Gagarin, Gutin, & Jones, 2014). brief conclusiongiven Section 7.2. Backgrounddefine workflow schema tuple (S, U, A, C), set tasks workflow,U set users, = {A(s) : S}, A(s) U authorization list task s,C set workflow constraints. workflow constraint pair c = (L, ), Lset functions L U : L scope constraint; specifies assignmentselements U elements L satisfy constraint c.Given X U , plan function : X. Given workflow constraint (L, ),X U , plan : X satisfies (L, ) either L \ 6= |L =. plan : X eligible satisfies every constraint C. plan : Xauthorized (s) A(s) . plan valid authorized eligible. plan: U called complete plan. algorithm solve WSP takes workflow schema(S, U, A, C) input outputs valid, complete plan, one exists (and null, otherwise).running example, consider following instance WSP.Instance 1. task set = {s1 , . . . , s4 } user set U = {u1 , . . . , u6 }. authorizationlists follows (where tick indicates given user authorized given task):3. Exponential Time Hypothesis claims algorithm running time (2o(n) ) 3SAT n variables (Impagliazzo, Paturi, & Zane, 2001).558fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEMs1s2s3s4u14u24444u3u4u5u64444444constraints follows: s1 s2 must assigned user; s2 s3 mustassigned different users; s3 s4 must assigned different users; s1 s4 mustassigned different users.Example 1 illustrates meanings eligible, complete authorised plans contextInstance 1.Example 1. following table gives assignments four plans, 1 , 2 , 3 , 4 :1234s1u1u1u1u2s2u2u1u2s3u4u4u4u4s4u5u5u5u5Authorized444Eligible444Complete4441 complete plan authorized eligible, s1 s2 assigneddifferent users.2 complete plan eligible authorized, u1 authorized s2 .3 plan authorized eligible, therefore valid. However, 3complete plan assignment s2 .4 complete plan eligible authorized. Thus 4 valid complete plan,therefore solution.algorithm runs instance (S, U, A, C) WSP, measure runningtime terms n = |U |, k = |S|, = |C|. (The set authorization lists consists klists size n, need consider size separately measuringrunning time.) say algorithm runs polynomial time running timep(n, k, m), p(n, k, m) polynomial n, k m.2.1 WSP Constraintspaper interested complexity WSP workflow constraint language(the set permissible workflow constraints) restricted. section introduce constrainttypes interest. practical applications real world workflows.assume constraints authorizations checked polynomial time.means takes polynomial time check whether plan authorized, eligible valid.correctness algorithm unaffected assumption, choosing constraintscheckable polynomial time would naturally affect running time.559fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES2.1.1 C ONSTRAINTS EFINED B INARY R ELATIONConstraints two tasks, s0 , represented form (s, s0 , ), binaryrelation U (Crampton, 2005). plan satisfies constraint (s) (s0 ). Writing= denote relation {(u, u) : u U } 6= denote relation {(u, v) : u, v U, u 6= v},separation-of-duty binding-of-duty constraints may represented form (s, s0 , 6=)(s, s0 , =), respectively. Crampton et al. (2013) considered constraints ,equivalence relation defined U . practical example workflow constraintsequivalence relation partitions users different departments: constraints couldenforce two tasks performed members department. Constraintsrestricted singleton tasks also considered (Crampton et al., 2013; Wang & Li, 2010):plan satisfies constraint form (S 0 , 00 , ) tasks s0 0 s00 00(s0 ) (s00 ).2.1.2 C ARDINALITY C ONSTRAINTStasks-per-user counting constraint form (t` , tr , ), 1 6 t` 6 tr 6 k S.plan satisfies (t` , tr , ) user performs either tasks t` tr tasks. Tasksper-user counting constraints generalize cardinality constraints widely adoptedWSP community (American National Standards Institute, 2004; Bertino, Bonatti, & Ferrari,2001; Joshi, Bertino, Latif, & Ghafoor, 2005; Sandhu, Coyne, Feinstein, & Youman, 1996).2.1.3 R EGULAR C ONSTRAINTSsay C regular satisfies following condition: partition S1 , . . . , Sp1everySp [p] exists eligible complete plan user u (u) = Si ,plan i=1 (Si ui ), ui distinct, eligible. Regular constraints extend setconstraints considered Wang Li (2010). Crampton et al. (2013) show followingconstraints regular: (S 0 , 00 , 6=); (S 0 , 00 , =), least one sets 0 , 00 singleton;tasks-per-user counting constraints form (t` , tr , ), t` = 1.2.1.4 U SER -I NDEPENDENT C ONSTRAINTSMany business rules concerned identities users complete set tasks;concerned relationships users. Accordingly, say constraint (L, ) user-independent whenever : U U permutation,. obvious example user-independent constraint requirement twotasks performed different users (separation-of-duty). complex example supposemost/at least/exactly p users required complete sensitive set tasks (cardinalityconstraints), p usually small, i.e., 1, 2, 3 so. substantial literature constraints method specifying enforcing business rules (for example, Gligor, Gavrila, &Ferraiolo, 1998; Simon & Zurko, 1997), including work researchers SAP IBM (for example, Basin, Burri, & Karjoth, 2014; Wolter & Schaad, 2007). widely studied constraintscardinality constraints separation-of-duty, form part ANSI standard rolebased access control (American National Standards Institute, 2004), developed US NationalInstitute Standards Technology (NIST). short, literature relevant standards suggest user-independent constraints interest business processes workflow560fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEMmanagement systems. definition user-independent includes constraints definedANSI RBAC standard many more.Every regular constraint user-independent, many user-independent constraintsregular. Indeed, constraints type (S 0 , 00 , =) user-independent, necessarilyregular (Crampton et al., 2013). Many counting constraints Global Constraint Catalogue (Beldiceanu, Carlsson, & Rampon, 2012) user-independent, regular. particular, constraint NVALUE, bounds number users performing settasks, user-independent regular. Note, however, constraints form (s0 , s00 , )(s0 , s00 , 6) user-independent general.important note authorization lists, fundamental access controlsystem, viewed unary constraints, certainly user-independent. presenceuser-independent constraints authorization lists workflow specification makesWSP challenging.2.2 Naive Algorithmmain aim section present simple algorithm (Algorithm 1) solveinstance WSP. running time algorithm slightly worse brute-force algorithm, algorithms basic structure provides starting point developefficient algorithm.need introduce additional notation terminology.Let : X plan S, X U . let TASK() = U SER() = X.important generic algorithm TASK() U SER() given explicit parts .particular, set U SER() may different set users assigned task . is,user u U SER() without task (s) = u. worth observingTASK() may empty (because may allocate tasks users X).u U , (T u) denotes plan : {u} (s) = u .Two functions f1 : D1 E1 f2 : D2 E2 disjoint D1 D2 = E1 E2 = .union two disjoint functions f1 : D1 E1 , f2 : D2 E2 function f = f1 f2f : D1 D2 E1 E2 f (d) = fi (d) Di , {1, 2}. Let g : Eh : E F functions. h g denotes composite function Fh g(d) = h(g(d)) D. integer p > 0, set [p] = {1, 2, . . . , p}.Proposition 1. Let (S, U, A, C) instance WSP, n = |U |, k = |S| = |C|.(S, U, A, C) solved time ((n + 1)k ) Algorithm 1.Proof. Let u1 , . . . , un ordering U , let Ui = {u1 , . . . , ui } [n].[n] turn, construct set plans U SER() = Ui valid.set n contains plan TASK() = S, (S, U, A, C) solution; otherwise,plan solution (S, U, A, C).Algorithm 1 shows construct sets . hard verify contains exactlyevery valid plan U SER() = Ui , i. implies correctness algorithm.remains analyse running time.[n] S, i|T | valid plans U SER() =Ui , TASK() = . construct 1 , need consider plans U SER() = U1 ,exactly 2k plans. plan decide polynomial time whether add 1 .561fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONESAlgorithm 1: Naive solution procedure WSPinput : instance (S, U, A, C) WSP1 Construct ordering u1 , . . . , un U ;2 Set 1 = ;3 foreach4Set = (T u1 );5eligible u1 A(s)6Set 1 = 1 {};7end8 end9 Set = 1;10 < n11Set i+1 = ;12foreach 013foreach \ TASK( 0 )14ui+1 A(s)15Set = 0 (T ui+1 );16eligible17Set i+1 = i+1 {};18end19end20end21end22Set = + 1;23 end24 foreach n25TASK() =26return ;27end28 end29 return NULL ;construct i+1 [n 1], need consider every pair ( 0 , ) 0\ TASK( 0 ). Consider pair ( 0 , ), 0 (S 0 , Ui )-plan 0 S,00\ 0 . Thus i|S | possibilities0 ,2|S||S | choices . Thus,PP00kktotal number pairs given 0 i|S | 2|S||S | = j=0 j ij 2kj = (i + 2)k .pair ( 0 , ) decide whetheradd 0 (T ui+1 ) i+1 polynomial time. Thus,Pkkkconstruct takes time ( n1i=0 (i + 2) ) = (n(n + 1) ) = ((n + 1) ).Algorithm 1 inefficient even small k, due fact contains valid plansU SER( 0 ) = {u1 , . . . , ui }. show next section necessary storemuch information solve WSP.0562fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM3. Plan-Indistinguishability Relationsfirst introduce notion extension-equivalence, defined equivalence relation setplans. Informally, relation enables us keep single member equivalence classbuilding plans incrementally.Definition 1. Given instance (S, U, A, C) WSP, two eligible plans 1 2 , define1 2 following conditions hold:1. U SER(1 ) = U SER(2 ) TASK(1 ) = TASK(2 );2. 1 0 eligible 2 0 eligible, plan 0 disjoint 1 2 .equivalence relation set eligible plans, say 1 2 extensionequivalent 1 2 .Example 2. Consider Instance 1.Let 1 : {s3 , s4 } {u2 , u4 } function 1 (s3 ) = u2 1 (s4 ) = u4 . Let2 : {s3 , s4 } {u2 , u4 } function 2 (s3 ) = u4 2 (s4 ) = u2 .plans 1 2 eligible, U SER(1 ) = U SER(2 ) TASK(1 ) =TASK(2 ). plan 0 disjoint 1 2 , plan 1 0 satisfy constraints(s2 , s3 , 6=), (s1 , s4 , 6=). Thus 1 0 eligible 0 eligible. Similarly, 2 0eligible 0 eligible. Thus 1 0 eligible 2 0 eligible,1 2 extension-equivalent.Suppose polynomial time algorithm check whether two eligible plansextension-equivalent. Algorithm 1, could keep track one plan equivalence class: constructing , add 2 1 extension-equivalent2 already ; construct i+1 , may use 1 proxy 2 . numberextension-equivalent classes small compared number plans, worst-case runningtime algorithm may substantially lower Algorithm 1.Unfortunately, necessarily easy decide two eligible plans extension-equivalent,approach practical. However, always refine4 extension-equivalence equivalence relation equivalence easy determine. example, identity equivalencerelation plan equivalent refinement.refined equivalence relation may equivalence classes extensionequivalence, substantially fewer identity relation, may obtain better runningtime naive algorithm.Definition 2. Given instance (S, U, A, C) WSP, let set eligible planslet equivalence relation refining extension-equivalence . say planindistinguishability relation (with respect C) if, eligible 1 , 2 1 2 ,plan 0 disjoint 1 2 1 0 eligible, 1 0 2 0 .Example 3. Let identity relation plans. is, 1 2 U SER(1 ) =U SER(2 ), TASK(1 ) = TASK(2 ), 1 (s) = 2 (s) U SER(1 ).4. equivalence relation 2 refinement equivalence relation 1 every equivalence class 2 subsetequivalence class 1 .563fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONESplan-indistinguishability relation. shows every plan-indistinguishability relationextension-equivalence relation. Indeed, plans given Example 2 extension-equivalentidentical.Recall refined extension-equivalence since may hard determine whether twoeligible plans extension-equivalent. therefore natural assume following:Assumption 1. Given plan-indistinguishability relation , takes polynomial time checkwhether two eligible plans equivalent .correctness algorithms depend assumption. However, poor choiceplan-indistinguishability relation could affect running times.describe appropriate plan-indistinguishability relations constraintsusing. case determining two eligible plans equivalent take polynomialtime.3.1 Plan-Indistinguishability Relation User-Independent ConstraintsLemma 1. Suppose constraints user-independent, let ui relation 1 ui21. U SER(1 ) = U SER(2 ) TASK(1 ) = TASK(2 );2. s, TASK(1 ), 1 (s) = 1 (t) 2 (s) = 2 (t).ui plan-indistinguishability relation set eligible plans.Proof. definition user-independent constraints, eligible plan : U Upermutation, also eligible. Suppose 1 ui 2 , let = TASK(1 )X = U SER(1 ). Let 0 : 1 (T ) 2 (T ) function 0 (1 (t)) = 2 (t)task t. Let 00 : X \ 1 (T ) X \ 2 (T ) arbitrary bijection (note |1 (T )| = |2 (T )|Condition 2 ui ). Let = 0 00 . permutation 2 = 1 . Thus 1eligible 2 eligible.consider two eligible plans 1 , 2 1 ui 2 , plan 0 disjoint 12 . First show 1 0 ui 2 0 . clear U SER(1 0 ) = U SER(2 0 )TASK(1 0 ) = TASK(2 0 ). s, U SER(1 0 ), (1 0 )(s) = (1 0 )(t),either s, TASK( 0 ), case (2 0 )(s) = (2 0 )(t) trivially, s,TASK(1 ), case 2 (s) = 2 (t) since 1 ui 2 , hence (2 0 )(s) =(2 0 )(t). Thus (1 0 )(s) = (1 0 )(t) (2 0 )(s) = (2 0 )(t) and, similarargument, converse holds. Thus 1 0 ui 2 0 . Furthermore, follows argumentfirst paragraph 1 0 eligible 2 0 eligible. Thus, conditionDefinition 2 second condition Definition 1 hold.first condition ui trivially satisfies first condition Definition 1. Thus, ui satisfiesconditions plan-indistinguishability relation.Example 4. Consider instance WSP users u1 , . . . u6 tasks s1 , . . . , s6constraints user-independent. Let ui plan-indistinguishability relation givenLemma 1. Let c1 constraint scope {s2 , s3 , s4 , s5 } c1 satisfiedeven number users assigned tasks {s2 , s3 , s4 , s5 }. Let c2 constraint scope564fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEM{s1 , s3 , s4 , s6 } c2 satisfied either s1 s3 assigned different users,s4 s6 assigned different users. Suppose c1 c2 constraints whosescope contains tasks {s1 , s2 , s3 } {s4 , s5 , s6 }.consider plans 1 , 2 : {s1 , s2 , s3 } {u1 , u2 , u3 , u4 } 1 (s1 ) =u1 , 1 (s2 ) = u2 , 1 (s3 ) = u1 , 2 (s1 ) = u3 , 2 (s2 ) = u4 , 2 (s3 ) = u3 , suppose1 , 2 eligible. 1 2 equivalent ui .Observe plan 0 disjoint 1 2 , 1 0 eligible 2 0eligible. 1 2 assign two users {s2 , s3 }, 0 must assign two users {s4 , s5 }order satisfy c1 . 1 2 assign s1 s3 user, 0 must assign s4 s5different users order satisfy c2 . long conditions satisfied, 0 satisfiesconstraints scope {s4 , s5 , s6 }, 1 0 2 0 eligible.3.2 Plan-Indistinguishability Relation Equivalence Relation ConstraintsRecall given binary relation U , constraint form (si , sj , ) satisfied plan(si ) (sj ). Recall constraints user-independent general.Lemma 2. Suppose equivalence relation U . Let V1 , . . . , Vl equivalence classesU . Suppose constraints form (si , sj , ) (si , sj , 6). Let e relation1 e 21. U SER(1 ) = U SER(2 ) TASK(1 ) = TASK(2 );2. equivalence classes Vj Vj U SER(1 ) 6= Vj \ U SER(1 ) 6= ,TASK(1 ), 1 (s) Vj 2 (s) Vj .e plan-indistinguishability relation.Proof. clear e satisfies first condition Definition 1. suppose 1 , 2 eligibleplans 1 e 2 , let 0 plan disjoint 1 2 . first show 1 0eligible 2 0 eligible.Suppose 1 0 eligible. Consider two tasks t, t0 TASK(2 0 ). {t, t0 } TASK( 0 ),2 0 falsify constraint t0 since equal 1 0 restricted{t, t0 } 1 0 eligible. {t, t0 } TASK(2 ), 2 0 break constraintssince 2 eligible.may assume TASK(2 ), t0 TASK( 0 ). definition, (2 0 )(t) (2 0 )(t0 )exists j [l] 2 (t), 0 (t0 ) Vj . Vj U SER(2 ) 6=Vj \ U SER(2 ) 6= . Therefore, definition e , 1 (s) Vj 2 (s) Vj ,TASK(1 ). particular, 1 (t) Vj , (1 0 )(t) (1 0 )(t0 ). similar argument,(1 0 )(t) (1 0 )(t0 ) (2 0 )(t) (2 0 )(t0 ). Therefore, every constraintsatisfied (1 0 ) satisfied (2 0 ). Therefore 1 0 eligible2 0 , similar argument converse holds.remains show 1 0 e 2 0 . clear user task sets same.user set, sets {Vj : Vj U SER(1 0 ) 6= , Vj \ U SER(1 0 ) 6= }{Vj : Vj U SER(2 0 ) 6= , Vj \ U SER(2 0 ) 6= } same. Furthermore,Vj set TASK(1 0 ), (1 0 )(s) Vj (2 0 )(s) Vj , eitherTASK(1 ), case Vj U SER(1 ) 6= , Vj \ U SER(1 ) 6= 2 (s) Vj ,565fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONESTASK( 0 ), case (2 0 )(s) = 0 (s) = (1 0 )(s). similar argument,(2 0 )(s) Vj , (1 0 )(s) Vj . Thus 1 0 e 2 0 .Example 5. Let equivalence relation users equivalence classes{u1 }, {u2 }, {u3 , u4 , u5 }, {u6 , u7 , u8 }. Consider instance WSP users u1 , . . . , u8tasks s1 , . . . , s6 constraints form (si , sj , ) (si , sj , 6). Let eplan-indistinguishability relation given Lemma 2. Suppose constraints whose scopecontains tasks {s1 , s2 , s3 } {s4 , s5 , s6 } constraints (s1 , s5 , 6), (s2 , s5 , )(s2 , s6 , 6).consider plans 1 , 2 : {s1 , s2 , s3 } {u1 , u2 , u3 , u4 } 1 (s1 ) =u1 , 1 (s2 ) = u3 , 1 (s3 ) = u3 , 2 (s1 ) = u2 , 2 (s2 ) = u3 , 2 (s3 ) = u4 , suppose1 , 2 eligible. 1 2 equivalent e .Observe plan 0 disjoint 1 2 , 1 0 eligible 2 0eligible. -equivalence class members {u1 , u2 , u3 , u4 } members{u1 , u2 , u3 , u4 } class {u3 , u4 , u5 }. 1 2 assign members {u3 , u4 } exactlyset {s2 , s3 }. Thus plan 0 disjoint 1 2 , 1 0 2 0 satisfyconstraint (s1 , s5 , 6) whatever 0 assigns s5 . satisfy (s2 , s5 , ) 0 assignss5 u5 , satisfy (s2 , s6 , 6) 0 assign s6 u5 . longconditions satisfied, 0 satisfies constraints scope {s4 , s5 , s6 }, 1 02 0 eligible.4. Generic Algorithm WSPfollows, X U, S, let [X, ] denote set eligible plansU SER() = X TASK() = . section introduce algorithm workssimilar way Algorithm 1, except instead storing valid plans particular set userstasks, construct [X, ]-representative sets task set certain user sets X.definition, equivalence classes plan-indistinguishability relation necessarily partition[X, ]. Hence equivalence class representation form (X, T, ),dependent constraint language. remainder section describe algorithmgive examples representations.4.1 Encodings Patternsgeneric algorithm, construct plans iteratively, using one planequivalence class plan-indistinguishability relation. running time algorithmdepend number equivalence classes relation, certain sets plans.ensure sets equivalence classes ordered therefore searched sorted efficiently,introduce notion encodings patterns. Loosely speaking, encoding functionmaps plans -equivalence class element (the pattern plans).encodings ensure logarithmic-time access insertion operations representative set plans,rather linear time naive method would allow.Note use encodings patterns necessary fixed-parametertractability results; problems could solved without use patterns encodingsfixed-parameter time, function k would grow quickly.566fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEMDefinition 3. Given instance (S, U, A, C) WSP plan-indistinguishability relation, let set plans. Let PAT set consider function ENC : PAT.X U, S, let PAT[X, ] = ENC([X, ]). say ENC -encoding (orencoding ) if, X U, 1 , 2 [X, ],1. ENC(1 ) = ENC(2 ) 1 2 ;2. ENC(1 ) calculated time polynomial n, k, m;3. exists linear ordering PAT[X, ] that, p, p0 PAT[X, ], decidewhether p p0 time polynomial n, k, m.elements PAT called -patterns. ENC() = p say p -pattern .second third conditions Definition 3 ensure may use encodings organiseplans reasonable time. clear context, refer -encodingencoding -patterns patterns.note complexity consequences Definition 3 following:Proposition 2. encoding plan-indistinguishability relation set patterns PAT ,assigning patterns PAT nodes balanced binary tree, perform followingtwo operations time (log(|PAT |)): (i) check whether p PAT , (ii) insert patternp/ PAT PAT .Proof. Recall comparisons polynomial n, k, m. result follows wellknown properties balanced binary trees (e.g., see (Cormen, Stein, Rivest, & Leiserson, 2001)).show plan-indistinguishability relations given previous sectionencodings. first need define lexicographic order.Definition 4. Given totally ordered set (A, ), (total) lexicographic order d-tuplesAd defined follows. say (x1 , . . . , xd ) (y1 , . . . , yd ) either xj = yj j [d]xi < yi xj = yj j < i.Taking = N = k obtain natural lexicographic order Nk0 .also lexicographically order sets disjoint subsets ordered set ={t1 , . . . , tk }, t1 < < tk .Definition 5. associate k-tuple (x1 , . . . , xk ) Nk0 set disjoint subsets{S1 , . . . , Sr } {t1 , . . . , tk } follows. xi = 0 ti/ rm=1 Sm . ti rm=1 Sm ,j < {ti , tj } Sm xi = xj ,otherwise xi = max{x1 , . . . , xi1 } + 1, max = 0.write VEC(S) = (x1 , . . . , xk ). Note VEC(S) computed time O(k 2 ).Thus, tasks subset assigned value; assignment integerstasks performed iteratively. example, = {1, . . . , 8} sets ={{2, 4}, {3}, {5, 7}} B = {{2, 3, 4}, {5, 7}}, VEC(A) = (0, 1, 2, 1, 3, 0, 3, 0)VEC (B) = (0, 1, 1, 1, 2, 0, 2, 0). lexicographically bigger B.567fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONESCorollary 1. Let ui plan-indistinguishability relation given set user-independentconstraints Lemma 1. exists encoding ui .Proof. Let s1 , . . . , sk ordering plan. Let = { 1 (u) : u U SER()}let VEC() = VEC(S ). plan , let ENC() tuple (U SER(), TASK(), VEC()).clear ENC(1 ) = ENC(2 ) 1 ui 2 , r (si ) = r (sj )yi = yj VEC(r ) = (y1 , . . . , yk ), r {1, 2}. Furthermore clear ENC()determined polynomial time .remains define linear ordering PAT[X, ] given X U, S. twopatterns p = (X, T, (x1 , . . . , xk )), p0 = (X, T, (y1 , . . . , yk )) PAT[X, ], define p p0(x1 , . . . , xk ) (y1 , . . . , yk ).Example 6. Let ENC encoding given proof Corollary 1. Let 1 , 2 plansgiven Example 4. ENC(1 ) = ENC(2 ) = {{u1 , u2 , u3 , u4 }, {s1 , s2 , s3 }, (1, 2, 1, 0, 0, 0)}.Corollary 2. Let e plan-indistinguishability relation given set constraints equivalence relations Lemma 2. exists encoding e .Proof. Suppose equivalence relation users, let V1 , . . . , Vp equivalence classesU . Suppose constraints form (si , sj , ) (si , sj , 6).plan , define ENC() (U SER(), TASK(), ),= 1 (Vj U SER()) : Vj U SER() 6= , Vj \ U SER() 6= , 1 j p .clear ENC(1 ) = ENC(2 ) 1 e 2 , (s) Vji1 (Vj ), {1, 2}. Furthermore clear ENC() determined polynomialtime .remains define linear ordering PAT[X, ] given X U, S. Let : Xplan. set disjoint subsets TASK(), natural order, orderpatterns PAT[X, ] according lexicographic order .Example 7. Let ENC encoding given proof Corollary 2. Let 1 , 2 plansgiven Example 5. ENC(1 ) = ENC(2 ) = {{u1 , u2 , u3 , u4 }, {s1 , s2 , s3 }, {{s2 , s3 }}}.4.2 Generic Algorithmuse notion diversity introduced next definition analyse running timegeneric algorithm.Definition 6. Let (S, U, A, C) instance WSP, n = |U |, k = |S| = |C|,suppose plan-indistinguishability relation respect C. Given ordering u1 , . . . , unU , let Ui = {u1 , . . . , ui } [n]. Let wi number equivalence classesset [Ui , ] eligible plans. define diversity respect u1 , . . . , unw = maxi[n] wi .Since generic algorithm stores one plan equivalence class , neednotion representative set.568fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEMDefinition 7. Given instance (S, U, A, C) WSP, let 0 set eligible plans letplan-indistinguishability relation. set 00 said 0 -representative set respectfollowing properties hold:1. 00 0 ; every plan 00 valid;2. every valid 0 0 , exists 00 00 0 00 .clear context, say 00 0 -representative set representative set0 . generic algorithm based finding plan-indistinguishability relationsexist small representative sets.Theorem 1. Let (S, U, A, C) instance WSP, n = |U |, k = |S| = |C|. Letu1 , . . . , un ordering U , let Ui = {u1 , . . . , ui } [n], U0 = . Supposediversity w respect u1 , . . . , un . Furthermore suppose exists -encodingENC . (S, U, A, C) solved time (3k w log w).Proof. proof proceeds demonstrating correctness bounding running timeAlgorithm 2, solves WSP. begin proof, give overview Algorithm 2.[n] turn S, construct representative set [Ui , ],denoted [Ui , ] .well constructing set [Ui , ] , also maintain companion set PAT[Ui , ] =ENC ([Ui , ] ). provides efficient way representing equivalence classes[Ui , ] . particular, allows us check whether given valid plan added[Ui , ] , faster searching [Ui , ] linearly.[Un , S] constructed, remains check whether [Un , S] non-empty,exists valid complete plan , exists valid complete plan 0 [Un , S]0 .Algorithm 2 gives details construct [Ui , ] .proof correctness Algorithm 2 proceeds induction. Observe first case[U0 , ] , 6= possible plan [U0 , ], set [U0 , ] = .= possible plan empty plan ). plan added [U0 , ] ,trivially valid. Thus [U0 , ] [U0 , ]-representative set .assume set [Ui , ] constructed [Ui , ]representative set. consider construction [Ui+1 , ] S. clearadded [Ui+1 , ] , [Ui+1 , ], eligible. Furthermore authorized,union authorized plans 0 [Ui , 0 ] (T 00 ui+1 ). Thus every plan[Ui+1 , ] valid plan [Ui+1 , ]. hand, suppose valid plan[Ui+1 , ]. let 00 = 1 ({ui+1 }) 0 = \ 00 , let 0 = |Ui , =0 (T 00 ui+1 ). assumption, exists 0 [Ui , ] 0 0 . Considerplan = 0 (T 00 ui+1 ). clear considered algorithm.Furthermore, 0 0 = 0 (T 00 ui+1 ), . Thereforeeligible (as eligible) also authorized (as union two authorized plans). Therefore569fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONESAlgorithm 2: Generic algorithm WSPinput : instance (S, U, A, C) WSP, ordering u1 , . . . , un U ,plan-indistinguishability relation1 Set [U0 , ] = {( )};2 foreach =63Set [U0 , ] = ;4 end5 Set = 0;6 < n7foreach8Set [Ui+1 , ] = ;9Set PAT[Ui+1 , ] = ;10foreach 011Set 00 = \ 0 ;12ui+1 A(s) 0013foreach 0 [Ui , 0 ]14Set = 0 (T 00 ui+1 );15eligible16Set p = ENC();17p/ PAT[Ui+1 , ]18Insert p PAT[Ui+1 , ] ;19Set [Ui+1 , ] = [Ui+1 , ] {};20end21end22end23end24end25end26Set = + 1;27 end28 [Un , S] 6=29return [Un , S] ;30 else31return NULL;32 endvalid added [Ui+1 , ] unless [Ui+1 , ] already contains another plan equivalent . Thus, [Ui+1 , ] contains plan -equivalent , follows[Ui+1 , ] [Ui+1 , ]-representative set, required.remains analyse running time algorithm. Proposition 2, testing whetherpattern p PAT[Ui , ] inserting p PAT[Ui , ] takes (log(|PAT[Ui , ] |)) time.Since Assumption 1 assumption time check constraints authorizationstakes polynomial time check eligibility, authorization -equivalence plans, running570fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEMPPPPtime algorithm ( n1i=00[Ui ,T 0 ] log(|[Ui+1 , ] |)). clearconstruction [Ui , 0 ] contains one plan -equivalence class [Ui , 0 ],soPdefinition|[Ui , 0 ] | w i, 0 . follows running time algorithmPn1 P( i=0 0 w log w) = (3k w log w).Remark 1. Rather checking whether [Un , S] non-empty end algorithm,could instead check whether [Ui , S] non-empty construction [Ui , S] i.is, stop search soon valid plan task set S. likely leadsaving running time implementation algorithm. paper concernedworst-case running time, would unaffected change, perform checkend algorithm interest clarity.4.3 Application User-Independent Constraints Optimalitysubsection, show WSP user-independent constraints FPT. Let Bk denotekth Bell number, number partitions set k elements.Lemma 3. Let u1 , . . . , un ordering U , let ui plan-indistinguishability relationgiven Lemma 1. ui diversity Bk respect u1 , . . . , un .Proof. plan , set { 1 (u) : u U SER()} partition tasks TASK().Furthermore, two plans generate partition equivalent ui . Thereforenumber equivalence classes ui [Ui , ] exactly number possible partitions, B|T | . Thus, Bk required diversity.Theorem 2. constraints user-independent, WSP solved time(2k log k ).Proof. Let u1 , . . . , un ordering U , let ui plan-indistinguishability relationgiven Lemma 1.Lemma 3, ui diversity Bk respect u1 , . . . , un . Furthermore, Corollary 1,exists encoding ui . Therefore, may apply Theorem 1 w = Bk , getalgorithm running time (3k Bk log Bk ) = (2k log k ) Bk < (0.792k/ ln(k + 1))kevery k (Berend & Tassa, 2010).running time (2k log k ) obtained optimal sense algorithm running timeexists, unless ETH fails. proof following theorem, use resultLokshtanov, Marx, & Saurabh, 2011 (Theorem 2.2).(2o(k log k) )Theorem 3. algorithm WSP user-independent constraints running time(2o(k log k) ), unless ETH fails.Proof. give reduction problem kk NDEPENDENT ET: Given integer parameterk graph G vertex set V = {(i, j) : i, j [k]}, decide whether G independent set|I| = k r [k], exists (r, i) I.Informally, k k NDEPENDENT ET gives us graph k k grid vertices, askswhether independent set one vertex row. Lokshtanov et al. (2011) provedalgorithm solve k k NDEPENDENT ET time 2o(k log k) , unless ETH fails.571fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONESConsider instance k k NDEPENDENT ET graph G. first produceequivalent instance WSP constraints user-independent.refine instance one user-independent constraints.Let U = {u1 , . . . , uk } set k users = {s1 , . . . , sk } set k tasks. Letauthorization lists A(si ) = U [k]. i, j, h, l [k], let c((i, j), (h, l)) denoteconstraint scope {si , sh }, satisfied plan unless (si ) = uj(sh ) = ul . every pair vertices (i, j), (h, l) adjacent G, add constraintc((i, j), (h, l)) C.show (S, U, A, C) ES-instance WSP G independent set one vertex row. Suppose (S, U, A, C) ES-instance WSPlet valid complete plan. [k], let f (i) unique j (si ) = uj .= {(i, f (i)) : [k]} set one vertex row G; furthermore, satisfies every constraint, edge G contains one element I, independentset.Conversely, suppose G ES-instance k k NDEPENDENTET. [k], letf (i) integer (i, f (i)) I. observe ki=1 ({si } uf (i) ) valid completeplan.show reduce (S, U, A, C) instance WSP constraintsuser-independent. main idea introduce new tasks representing users,constraints, replace mention particular user mention user performsparticular task.Create k new tasks t1 , . . . , tk let 0 = {t1 , , . . . , tk }. Let authorization lists0(s) = U A0 (ti ) = {ui } [k]. constraint c((i, j), (h, l))C, let d((i, j), (h, l)) constraint scope {si , sh , tj , tl }, satisfied planunless (si ) = (tj ) (sh ) = (tl ). Let initially C 0 = C. replace, C 0 , every constraintc((i, j), (h, l)) d((i, j, ), (h, l)).Since defined equalities, users mentioned, constraints C 0 userindependent. show (S 0 , U, A0 , C 0 ) equivalent (S, U, A, C). First, supposevalid complete plan (S, U, A, C). let 0 : 0 U plan 0 (si ) = (si )[k], 0 (tj ) = uj j [k]. easy check satisfies every constraintC 0 satisfies every constraint C 0 . Since 0 authorized eligible plan, 0 validcomplete plan (S 0 , U, A0 , C 0 ).Conversely, suppose 0 valid complete plan (S 0 , U, A0 , C 0 ). Since A0 (ti ) = {ui }[k], 0 (ti ) = ui every [k]. [k], let f (i) unique integer0 (si ) = uf (i) . define : U (si ) = uf (i) , observe constraints Csatisfied . So, valid complete plan (S, U, A, C).4.4 Application Equivalence Relation Constraintsknown restricting WSP equivalence relation constraints enough ensureproblem FPT (Crampton et al., 2013). However, derive result applyingalgorithm directly shown appropriate properties language equivalence relationconstraints. serves demonstrate wide applicability approach.572fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEMLemma 4. Let e plan-indistinguishability relation given set equivalence relationconstraints Lemma 2. exists ordering u1 , . . . , un U e diversity2k respect U .Proof. Suppose equivalence relation users, let V1 , . . . , Vp equivalence classesU . Suppose constraints form (si , sj , ) (si , sj , 6).Let u1 , . . . , un ordering U elements Vj appear elementsVj 0 , j < j 0 . Thus, plan U SER() = Ui = {u1 , . . . , ui },one integer ji Vji U SER() 6= , Vji \ U SER() 6= .follows two plans 1 , 2 [Ui , ] e -equivalent, [n], S,provided 1 (t) Vji 2 (t) Vji . Therefore e 2kequivalence classes [Ui , ], required.Theorem 4. Suppose equivalence relation U . Suppose constraints form(si , sj , ) (si , sj , 6). WSP solved time (6k ).Proof. Let u1 , . . . , un ordering U given Lemma 4, let e planindistinguishability relation given Lemma 2.Lemma 4, e diversity 2k respect u1 , . . . , un . Furthermore Corollary 2,exists encoding e . Therefore, may apply Theorem 1 w = 2k , get algorithmrunning time (3k 2k log(2k )) = (6k ).5. Unions Constraint Languagessection show approach allows us easily combine constraint languages shownFPT WSP. need build bespoke algorithms new constraint languageobtained, show two languages sense compatible.highlights advantages approach previous methods, required development new algorithms different constraint languages combined instanceWSP (e.g., see Crampton et al., 2013).Theorem 5. Let (S, U, A, C1 C2 ) instance WSP, suppose 1 planindistinguishability relation respect C1 2 plan-indistinguishability relationrespect C2 . Given ordering u1 , . . . , un U , let W1 diversity 1 respectu1 , . . . , un W2 diversity 2 respect u1 , . . . , un .Let equivalence relation 0 1 0 2 0 .plan-indistinguishability relation respect C1 C2 , diversity W1 W2respect u1 , . . . , un .Proof. first show plan-indistinguishability relation respect C1 C2 . Let0 eligible plans (with respect C1 C2 ). 0 implies 1 0 1 satisfiesconditions plan-indistinguishability relation, clear 0 U SER() =U SER( 0 ) TASK() = TASK( 0 ). consider plan 00 disjoint 0 . 1plan-indistinguishability relation respect C1 1 0 , 00 C1 -eligible0 00 is. Similarly 00 C2 -eligible 0 00 is. Observingplan C1 C2 -eligible C1 -eligible C2 -eligible, implies 00C1 C2 -eligible 0 00 is. Thus 0 extension equivalent.573fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONES1 2 plan-indistinguishability relations, 00 1 0 0000 2 0 00 , therefore 00 0 00 . Thus, satisfies conditionsplan-indistinguishability relation.bound diversity respect u1 , . . . , un , consider Ui ={u1 , . . . , ui }. enough note -equivalent plans [Ui , ] must1 2 -equivalence classes. W1 choices 1 -equivalence classW2 choices 2 equivalence class, W1 W2 equivalence classes[Ui , ].Remark 2. Given encoding ENC1 1 encoding ENC2 2 , may constructencoding . Given plan , let ENC() ordered pair (ENC1 (), ENC2 ()). clearENC() = ENC( 0 ) 0 .Given sets Ui = {u1 , . . . , ui }, fix linear orderings ENC1 ([Ui , ])ENC 2 ([Ui , ]). let lexicographic ordering ENC ([Ui , ]) = ENC 1 ([Ui , ])ENC 2 ([Ui , ]).nothing stop us applying Theorem 5 multiple times, order get planindistinguishability relation bounded diversity union several constraint languages.Note diversity expected grow exponentially number languagesunion. Thus, makes sense apply Theorem 5 union small number languages.However, long fixed number languages, plan-indistinguishabilityrelation fixed-parameter diversity, resulting union languages also planindistinguishability relation fixed-parameter diversity.use result directly show constraints either user independentequivalence relation constraints, WSP still FPT.Theorem 6. Suppose equivalence relation U . Let (S, U, A, C) instanceWSP, suppose constraints either form, (s1 , s2 , ), (s1 , s2 , 6) userindependent constraints. WSP solved time (2k log k+k ).Proof. Let Ce C set constraints form (s1 , s2 , ), (s1 , s2 , 6), let Cuiremaining (user-independent) constraints.Let u1 , . . . , un ordering U given Lemma 4. Lemmas 2 4, exists planindistinguishability relation e Ce diversity 2k respect u1 , . . . , un . FurthermoreCorollary 2, e encoding. Lemmas 1 3, exists plan-indistinguishabilityrelation ui Cui diversity Bk respect u1 , . . . , un . Furthermore Corollary 1,ui encoding.Therefore Theorem 5, may find plan-indistinguishability relation C,diversity Bk 2k respect u1 , . . . , un encoding. Thusmay apply Theorem 1 w = Bk 2k , get running time (3k Bk 2k log(Bk 2k )) =(3k 2k log k(1o(1))+k log(2k log k(1o(1))+k )) = (2k log k+k ).6. Computational Experiments WSP AlgorithmsApart conducting theoretical research WSP, Wang Li (2010) carried experimental study problem. Due difficulty acquiring real-world workflow instances,574fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEMWang Li used synthetic data experimental study. encoded instances WSPpseudo-Boolean SAT order use pseudo-Boolean SAT solver SAT4J.implemented algorithm compared performance SAT4J another setsynthetic instances WSP (Cohen et al., 2014). instances use k = 16, 2024, n = 10k user-independent (cardinality) constraints three different types: varynumber constraints proportions different constraint types; user authorized1 8 tasks k = 16, 1 10 tasks k = 20, 1 12tasks k = 24. algorithm implemented C++ enhanced inclusiontechniques employed CSP solving, propagation. also converted WSP instancespseudo-Boolean problems processing SAT4J. experiments performed MacBookPro computer 2.6 GHz Intel Core i5 processor 8 GB 1600 MHz DDR3 RAM (runningMac OS X 10.9.2).lightly-constrained instances, SAT4J often faster algorithm, largelynumber patterns considered algorithm large instances. However, highlyconstrained instances, SAT4J unable compute decision number instances (becauseran memory), sharp contrast algorithm solved instances. Overall,average, algorithm faster SAT4J and, particular, two orders magnitude fasterk = 16. Moreover, time taken algorithm varies much less SAT4J, evenunsatisfiable instances, time taken proportional product numberpatterns number users. (In particular, tested instances, much less dependentnumber constraints, parameter cause significant fluctuations time taken SAT4Jleads sharp increase number variables pseudo-Boolean encoding.) Fulldetails results published (Cohen et al., 2014).7. Conclusionpaper introduced algorithm based notion plan-indistinguishability, applicablewide range WSP instances. showed algorithm powerful enough optimal,sense, wide class user-independent constraints. generic algorithm alsofixed-parameter algorithm equivalence relation constraints, user-independent.showed deal unions different types constraints using generic algorithm.particular, proved generic algorithm fixed-parameter algorithm unionuser-independent equivalence relation constraints.Acknowledgmentsresearch supported EPSRC grant EP/K005162/1. grateful refereesuseful comments suggestions.ReferencesAmerican National Standards Institute (2004).CITS RBAC 359-2012).575Role Based Access Control (ANSI IN-fiC OHEN , C RAMPTON , G AGARIN , G UTIN , & J ONESBasin, D. A., Burri, S. J., & Karjoth, G. (2014). Obstruction-free authorization enforcement: Aligning security business objectives. Journal Computer Security, 22(5), 661698.Beldiceanu, N., Carlsson, M., & Rampon, J.-X. (2012). Global constraint catalog, 2nd edition(revision a). working copy 5195, Swedish Institute Computer Science, Kista, Sweden.Berend, D., & Tassa, T. (2010). Improved bounds bell numbers moments sumsrandom variables. Probability Mathematical Statistics, 30(2), 185205.Bertino, E., Bonatti, P. A., & Ferrari, E. (2001). TRBAC: temporal role-based access controlmodel. ACM Trans. Inf. Syst. Secur., 4(3), 191233.Bertino, E., Ferrari, E., & Atluri, V. (1999). specification enforcement authorizationconstraints workflow management systems. ACM Trans. Inf. Syst. Secur., 2(1), 65104.Bodlaender, H. L., Cygan, M., Kratsch, S., & Nederlof, J. (2013). Deterministic single exponentialtime algorithms connectivity problems parameterized treewidth. Proceedings40th International Conference Automata, Languages, Programming - Volume Part I,ICALP13, pp. 196207, Berlin, Heidelberg. Springer-Verlag.Bulatov, A., Jeavons, P., & Krokhin, A. (2005). Classifying complexity constraints usingfinite algebras. SIAM Journal Computing, 34, 720742.Cohen, D., Crampton, J., Gagarin, A., Gutin, G., & Jones, M. (2014). Engineering algorithmsworkflow satisfiability problem user-independent constraints. Chen, J., Hopcroft,J. E., & Wang, J. (Eds.), Frontiers Algorithmics - 8th International Workshop, FAW 2014,Zhangjiajie, China, June 28-30, 2014. Proceedings, Vol. 8497 Lecture Notes ComputerScience, pp. 4859. Springer.Cormen, T. H., Stein, C., Rivest, R. L., & Leiserson, C. E. (2001). Introduction Algorithms (2ndedition). McGraw-Hill Higher Education.Crampton, J., Crowston, R., Gutin, G., Jones, M., & Ramanujan, M. (2013). Fixed-parametertractability workflow satisfiability presence seniority constraints. Fellows, M.,Tan, X., & Zhu, B. (Eds.), Frontiers Algorithmics Algorithmic Aspects InformationManagement, Vol. 7924 Lecture Notes Computer Science, pp. 198209. SpringerBerlin Heidelberg.Crampton, J. (2005). reference monitor workflow systems constrained task execution.Proceedings Tenth ACM Symposium Access Control Models Technologies,SACMAT 05, pp. 3847, New York, NY, USA. ACM.Crampton, J., & Gutin, G. (2013). Constraint expressions workflow satisfiability. Proceedings 18th ACM Symposium Access Control Models Technologies, SACMAT 13,pp. 7384, New York, NY, USA. ACM.Crampton, J., Gutin, G., & Yeo, A. (2013). parameterized complexity kernelizationworkflow satisfiability problem. ACM Trans. Inf. Syst. Secur., 16(1), 4:14:31.Dechter, R. (2003). Constraint Processing. Morgan Kaufmann Publishers, 340 Pine Street, SixthFloor, San Francisco, CA 94104-3205.Downey, R. G., & Fellows, M. R. (2013). Fundamentals Parameterized Complexity. TextsComputer Science. Springer.576fiI TERATIVE P LAN C ONSTRUCTION W ORKFLOW ATISFIABILITY P ROBLEMFomin, F. V., Lokshtanov, D., & Saurabh, S. (2014). Efficient computation representative setsapplications parameterized exact algorithms. Proceedings Twenty-FifthAnnual ACM-SIAM Symposium Discrete Algorithms, SODA 14, pp. 142151. SIAM.Gligor, V., Gavrila, S., & Ferraiolo, D. (1998). formal definition separation-of-duty policies composition. 1998 IEEE Symposium Security Privacy, 1998. Proceedings., pp. 172183.Impagliazzo, R., Paturi, R., & Zane, F. (2001). problems strongly exponential complexity?. J. Comput. Syst. Sci., 63(4), 512530.Jayaraman, K., Ganesh, V., Tripunitara, M. V., Rinard, M. C., & Chapin, S. J. (2011). ARBACpolicy large multi-national bank. CoRR, abs/1110.2849.Joshi, J. B. D., Bertino, E., Latif, U., & Ghafoor, A. (2005). generalized temporal role-basedaccess control model. IEEE Transactions Knowledge Data Engineering,, 17(1), 423.Lokshtanov, D., Marx, D., & Saurabh, S. (2011). Slightly superexponential parameterized problems.Proceedings Twenty-second Annual ACM-SIAM Symposium Discrete Algorithms,SODA 11, pp. 760776. SIAM.Niedermeier, R. (2006). Invitation Fixed-Parameter Algorithms. Oxford University Press.Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006). Handbook Constraint Programming.Elsevier.Sandhu, R. S., Coyne, E. J., Feinstein, H. L., & Youman, C. E. (1996). Role-based access controlmodels. Computer, 29(2), 3847.Schaad, A., Moffett, J., & Jacob, J. (2001). role-based access control system europeanbank: case study discussion. Proceedings Sixth ACM Symposium AccessControl Models Technologies, SACMAT 01, pp. 39, New York, NY, USA. ACM.Simon, R., & Zurko, M. (1997). Separation duty role-based environments. ComputerSecurity Foundations Workshop, 1997. Proceedings., 10th, pp. 183194.Wang, Q., & Li, N. (2010). Satisfiability resiliency workflow authorization systems. ACMTrans. Inf. Syst. Secur., 13(4), 40:140:35.Wolter, C., & Schaad, A. (2007). Modeling task-based authorization constraints bpmn.Proceedings 5th International Conference Business Process Management, BPM07,pp. 6479, Berlin, Heidelberg. Springer-Verlag.577fiJournal Artificial Intelligence Research 51 (2014) 377-411Submitted 7/14; published 10/14Novel SAT-Based Approach Model Based DiagnosisAmit MetodiAMITMET @ CS . BGU . AC . ILDepartment Computer Science,Ben Gurion University Negev, Beer-Sheva, IsraelRoni SternMeir KalechRONI . STERN @ GMAIL . COMKALECH @ BGU . AC . ILDepartment Information Systems Engineering,Ben Gurion University Negev, Beer-Sheva, IsraelMichael CodishMCODISH @ CS . BGU . AC . ILDepartment Computer Science,Ben Gurion University Negev, Beer-Sheva, IsraelAbstractpaper introduces novel encoding Model Based Diagnosis (MBD) Boolean Satisfaction (SAT) focusing minimal cardinality diagnosis. encoding based combinationsophisticated MBD preprocessing algorithms application SAT compiler optimizes encoding provide succinct CNF representations obtained previousworks. Experimental evidence indicates approach superior published algorithmsminimal cardinality MBD. particular, determine, first time, minimal cardinality diagnoses entire standard ISCAS-85 74XXX benchmarks. results open wayimprove state-of-the-art range similar MBD problems.1. IntroductionAutomated diagnosis concerned reasoning health systems, including identification abnormal behavior, isolation faulty components prediction system behaviornormal abnormal conditions. systems become large-scale complex,automated diagnosis becomes challenging. Model Based Diagnosis (MBD) artificialintelligence based approach aims cope diagnosis problem (Reiter, 1987; de Kleer& Williams, 1987). MBD, model system first built. diagnoser observessystem predict behavior model. Discrepancies observation prediction used input diagnosis algorithm produces set possible faultsexplain observation. MBD deployed several real-world applications, includingspacecrafts (Williams & Nayak, 1996), satellite decision support systems (Feldman, de Castro, vanGemund, & Provan, 2013), automotive industry (Struss & Price, 2003) spreadsheets (Jannach & Schmitz, 2014). Also, exist several commercial MBD tools (Feldman, 2012; Dressler& Struss, 1995).MBD known hard problem algorithms exponential runtime (exponentialnumber components diagnosed system). Moreover, number potential diagnoses given observation huge. Therefore, MBD algorithms typically focus minimalc2014AI Access Foundation. rights reserved.fiM ETODI , TERN , K ALECH & C ODISHdiagnoses: minimal subset contain diagnoses, minimal cardinalitysmallest size. Computing first minimal diagnosis P , computing next oneNP-hard (Bylander, Allemang, Tanner, & Josephson, 1991). Computing minimal cardinalityNP-hard, even first diagnosis (Selman & Levesque, 1990). work focushard task finding minimal cardinality diagnoses.study Model-Based Diagnosis resulted variety computational modelingchallenges. paper focus one challenge received much attentionyears. Originally defined Reiter (1987) de Kleer Williams (1987), problemaims diagnose multiple faulty components so-called weak fault model, ignoresmode abnormal behavior components. problem extensively researched25 years wide range papers propose different algorithms solve it, including rangepapers recent years (Feldman & van Gemund, 2006; Williams & Ragno, 2007; Feldman,Provan, & van Gemund, 2010a; Siddiqi & Huang, 2007, 2011). addressing challenge,common practice focus diagnosis Combinational Logic Circuits. Namely, Booleancircuits single output component determined logical functioncurrent input state (independent time feedback).basic setting diagnosis considers single observation inputs outputssystem. Variations consider additional information probabilities component failure,multiple observations inputs outputs system, observations internal positionssystem (probes). paper focus basic setting. Extensions variationsdiscussed Section 8.Even basic setting, solving MBD problem often impractical, especially highcardinality faults. instance, system 1000 components, find minimal cardinalitydiagnosis size 5, diagnosis engine must verify absence diagnosis consisting 4 components (there 1010 combinations). overcome complexity problemconsider novel encoding SAT.recent years, Boolean SAT solving techniques improved dramatically. Todays SATsolvers considerably faster able manage larger instances yesterdays. Moreover, encoding modeling techniques better understood increasingly innovative. SAT currentlyapplied solve wide variety hard practical combinatorial problems, often outperformingdedicated algorithms. survey state-of-the-art SAT solving see work Biere,Heule, van Maaren, Walsh (2009) draft forthcoming volume Art Computer Programming (Knuth, 2014).general idea encode (typically, NP) hard problem instance, , Boolean formula,, solutions correspond satisfying assignments . Given encoding,SAT solver applied solve .SAT-based solutions MBD already proposed. Smith et al. (2005) encode circuit,representing component clauses add constraints cardinality. basisSAT-based encodings, including one contribute paper. Bauer (2005)introduces tailored SAT solver specifically designed return many diagnoses. Stein et al. (2006)address diagnosis qualitative models physical systems multiple fault modes. recently, Feldman et al. (2010) propose encoding MAX-SAT demonstrate off-the-shelfsolvers require calls SAT solver stochastic diagnosis algorithm SAFARI (Feldmanet al., 2010a).378fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISprevious applications SAT MBD appear indicate SAT MAX-SAT solversdoomed perform poorly standard benchmarks (Feldman et al., 2010). paper provescontrary. SAT-based approach differs previous SAT encodings several key aspects.First, sophisticated MBD preprocessing techniques applied facilitate constructioncarefully designed constraint model, includes constraints exploit unique substructuresdiagnosed system. Second, constraint model compiled corresponding CNF usingconstraint compiler called BEE (Metodi & Codish, 2012), simplifies constraints generatesencoding CNF significantly improves subsequent runtime underlying SATsolver. Lastly, structural abstraction inspired Siddiqi Huang (2007) used decomposediagnosis problem, SAT solver used find top-level diagnoses,show simple poly-time algorithm expand top-level diagnoses find minimalcardinality diagnoses. approach requires preprocessing diagnosed system,complexity preprocessing low-order polynomial, negligible (both theoreticallyempirically) compared cost actual SAT solving.evaluated SAT-based approach using two standard benchmarks: ISCAS-85 (Brglez,Bryan, & Kozminski, 1989) 74XXX. standard benchmarks MBD literature, used extensively time made available today (Feldman & van Gemund,2006; Feldman et al., 2010a; Siddiqi & Huang, 2007, 2011; Stern, Kalech, Feldman, & Provan,2012; Nica, Pill, Quaritsch, & Wotawa, 2013). Finding minimal cardinality diagnoses hard setsobservations ISCAS-85 long standing challenge MBD communityused diagnosis competitions (DXC, 2009). ISCAS-85 systems also used standardautomatic benchmark generation (Wang & Provan, 2010).consider three known sets observations minimal cardinalities 131,first time succeed compute minimal cardinality diagnosis observationsbenchmark. compare approach wide collection state-of-the-art algorithms MBD,including: HA* (Feldman & van Gemund, 2006), CDA* (Williams & Ragno, 2007), SAFARI (Feldman et al., 2010a), HDIAG (Siddiqi & Huang, 2007) DCAS (Siddiqi & Huang, 2011). Resultsunequivocal. approach outperforms others, often orders magnitude, termsruntime. result even significant, SAFARI stochastic algorithm, known fast,even aim guarantee minimal cardinality. approach, hand, guarantees minimal cardinality diagnosis runs faster SAFARI.paper goes beyond preliminary version work (Metodi, Stern, Kalech, & Codish,2012a). provide detailed description components approach, presentdetailed algorithms, prove correctness, provide additional examples present elaborateexperimental evaluation. next section discuss additional related work. Section 3 presentsrequired background MBD. Section 4 present standard approach model MBDSAT. Section 5 main part paper describe building blocks toolfind minimal cardinality diagnosis. Section 6 describes building blocks combineddiagnosis algorithm. Comprehensive evaluation approach given Section 7. Section8 discusses applicability approach general setting Section 9 concludes.2. Related WorkSince late 80s, Model Based Diagnosis problem weak fault model widelyresearched wide range papers propose different algorithms solve (Reiter, 1987; de Kleer379fiM ETODI , TERN , K ALECH & C ODISH& Williams, 1987; Feldman & van Gemund, 2006; Williams & Ragno, 2007; Feldman et al., 2010a;Siddiqi & Huang, 2007, 2011). Till today considered challenge reflected synthetictrack annual DXC diagnosis competition (DXC, 2009).Many existing diagnosis techniques propose apply combination deterministic reasoning search algorithms. One classic approach involves two stage process. First, identifiesconflict sets, includes least one fault. Then, applies hitting set algorithmcompute sets multiple faults explain observation (de Kleer & Williams, 1987; Williams& Ragno, 2007). methods guarantee sound diagnoses, even complete.However, tend fail large systems due infeasible runtime space requirements.alternative method directly search diagnoses trying different assumptionscomponents faulty. example, DRUM-II diagnosis engine finds minimal diagnosis performing iterative deepening search, limiting every iteration, numbercomponents assumed faulty (Frohlich & Nejdl, 1997). DRUM-II also analyzesdependencies components prune irrelevant diagnoses. Recent work presents empiricalevidence suggesting direct search diagnoses often better conflict-directed diagnosis algorithms (Nica et al., 2013). Nica et al. compare SAT-based approach,uses pre-processing. work show proposed pre-processing efficientcomputationally results huge speedups search diagnosis. form preprocessing key ingredient enables us find large minimal cardinality diagnoses,even sizes 31.Another approach considers diagnosis problem terms inductive learning. Here, onetries learn relations symptoms faults (Murray, Hughes, & Kreutz-Delgado,2006). One disadvantage works approach learn single fault rathermultiple faults (Balakrishnan & Honavar, 1998). addition, inductive learning methodsguarantee sound diagnoses completeness. We, hand, propose methodaddresses multiple faults guarantees sound complete minimal cardinality diagnoses.Feldman et al. (2010a) propose stochastic diagnosis algorithm, called SAFARI. Althoughmethod guaranteed return diagnoses minimal cardinality, presents solutionsclose minimal cardinality low runtime. Section 7, demonstrate approachoutperforms SAFARI terms runtime, also guarantees minimal cardinality diagnosesreturned.Compilation-based methods also proposed MBD context. Torasso Torta(2006) proposed compile system description Binary Decision Diagrams (BDDs). Darwiche (2001) proposed compile system description Decomposable Negation NormalForm (DNNF). cases, compiled model allowed finding minimal cardinality diagnosistime polynomial size compiled model. However, works, sizecompiled model (BDD DNNF) may grow exponentially shown become bottleneck (Siddiqi & Huang, 2007).Siddiqi Huang (2007) suggest optimize MBD identifying components dominateothers. adopt idea apply SAT-based approach. Another compilation-baseddiagnosis algorithm HA* algorithm (Feldman & van Gemund, 2006). HA* designedexploit given hierarchy diagnosed system. done converting given systemhierarchy DNF hierarchy. element DNF hierarchy solved simplebest-first search using heuristic function given prior probability health systemcomponents. Section 7, demonstrate approach substantially outperforms HA*.380fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS= {X1 , X2 , A1 , A2 , O1 }OBS = {A, B, C, D, E}COMPSFigure 1: MBD problem: (faulty) full adder.Another previously proposed approach imposes tree structure given system description.system tree structure diagnosed joining diagnoses constituent subsystems.El Fattah Dechter (1995) obtained tree structure converting diagnosed systemchordal graph decomposed tree maximal cliques. TREE* algorithm anothertree-decomposition algorithm, initially proposed tree-structured systems (Stumptner& Wotawa, 2001). TREE* later generalized perform system embedding hypertree specific representation diagnosed system (Stumptner & Wotawa, 2003). Followwork generalized TREE* support various forms diagnosis optimization tasks,finding minimal cardinality diagnoses, finding subset minimal diagnoses, finding probable diagnoses (Sachenbacher & Williams, 2004). Note complexity TREE* exponentialwidth hyper-tree embedded system description defined works.3. Model-Based Diagnosis: Preliminariessection introduces background Model Based Diagnosis. addition basic definitions, review several concepts literature build paper.Model Based Diagnosis problems arise normal behavior system violated duefaulty components indicated certain observations. focus weak fault models,ignore mode abnormal behavior components. MBD problem specified triplethSD, COMPS , OBS where: SD system description, COMPS set components, OBSobservation. system description takes account components might abnormal(faulty). specified unary predicate h() components h(c) truecomponent c healthy false c faulty. Denoting correct behavior c propositional formula, c , SD given formally^SD =h(c) ccCOMPSNamely, component faulty follows correct behavior. diagnosis problemarises when, assumption none components faulty, inconsistencysystem description observations (de Kleer & Williams, 1987; Reiter, 1987).Definition 1 [Diagnosis Problem]. Given MBD problem, hSD, COMPS , OBS i, diagnosis problemarises^SDh(c) OBS `cCOMPS381fiM ETODI , TERN , K ALECH & C ODISHexample, diagnosis problem arises MBD Figure 1 normal behavior would giveoutput E = 1. inconsistency, diagnosis algorithm tries find subset COMPSwhich, assumed faulty, explains observation.Definition 2 [Diagnosis] Given MBD problem, hSD, COMPS , OBS i, set componentsCOMPS diagnosis^^h(c) OBS 0SDh(c)cc/say minimal diagnosis proper subset 0 diagnosis,minimal cardinality diagnosis diagnosis 0 COMPS exists |0 | < ||.MBD Figure 1, 1 ={X1 , X2 }, 2 ={O1 }, 3 ={A2 } minimal diagnoses, 2 ,3 minimal cardinality diagnoses, smaller diagnosis.important concept make use paper gate domination, usedautomatic test pattern generation (ATPG) (Kirkland & Mercer, 1987; Fujiwara, Member, Shimono, & Member, 1983) modern SAT solvers (Marques-Silva, Lynce, & Malik, 2009),sometimes name unique sensitization. Siddiqi Huang (2007) applied gate domination model-based diagnosis, introducing notion cone. following wordingtaken Siddiqi Huangs paper setting system Boolean circuitcomponents gates.Definition 3 (Dominator Cone) gate X fan-in region gate G dominated Gconversely G dominator X path X output circuit contains G. conecorresponding gate G set gates dominated G. maximal cone one eithercontained cone contained exactly one cone entire circuit.example, circuit depicted Figure 1, components {A1 , A2 , O1 } form cone, sincepath A1 A2 system output contains O1 . O1 dominator A1 A2dominated gates.Although Definition 3 stated terms Boolean circuits logical gates, notionsdominators cones generalized many systems, components correspond gates,component C1 dominates component C2 paths passing C2 also passC1 . example, system illustrated Figure 3 components C1 C2 form cone,C2 dominates C1 .importance cones MBD algorithms rooted two observations presented SiddiqiHuang. Firstly, cones single-output sub-systems such, minimal cardinality diagnosis always, independent observation, indicate one unhealthy component per cone.Secondly, C cone SD, without loss generality, may assume dominatedcomponents C healthy. correct X unhealthy minimal cardinalitydiagnosis dominated G, G must healthy. So, exists another minimal cardinalitydiagnosis X healthy G not. example, circuit depicted Figure 1, 3minimal cardinality diagnosis signifies dominated A2 unhealthy, exists anotherminimal cardinality diagnosis, 2 , A2 healthy O1 , dominates A2 , unhealthy.Based observations restrict search minimal cardinality diagnoses,so-called top-level minimal cardinality diagnoses. notion top-level diagnoses introduced Siddiqi Huang.382fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISDefinition 4 (top-level diagnosis (TLD)) say minimal cardinality diagnosis top-levelcontain dominated components.formally justify focus top-level diagnoses make explicit following Propositions 1 2, left implicit previous work.Proposition 1 Let 0 minimal cardinality diagnosis given MBD problem.top-level diagnosis , cardinality.Proof: Straightforward. obtain , replace dominated component 0 corresponding dominator.note set minimal cardinality diagnoses obtained expanding set top-level minimal cardinality diagnoses following sense: Given minimalcardinality top-level diagnosis, = {c1 , . . . , c` } consisting ` dominators correspondingcones {C1 . . . , C` }, denotefinfi= c0i Ci fi \ {ci } {c0i } diagnosis(1)say expands set minimal cardinality diagnoses defined terms crossproduct by: () = 1 ` . example, consider system Figure 1observation OBS = {A, B, C, D, E}. cones system C1 = {X1 }, C2 = {X2 }, C3 ={A1 , A2 , O1 }. corresponding MBD problem two top-level minimal cardinality diagnoses,1 = {X1 , O1 } 2 = {X2 , O1 } (1 ) = {X1 } {O1 } = {{X1 , O1 }} (2 ) ={X2 } {A1 , O1 } = {{X2 , A1 }, {X2 , O1 }}.Proposition 2 0 minimal cardinality diagnosis top-level minimalcardinality diagnosis expands include 0 .Proof: proof straightforward construction.Finally, comment sets specify expansion top-level diagnosisEquation (1) easy compute: component c0i Ci checking \ {ci } {c0i }diagnosis means propagating observed inputs system, flipping outputspropagating component \ {ci } {c0i } checking conflictobserved outputs. observation explicit previous work essential justifyfocus top-level diagnoses. Proposition 2 important applies diagnosis algorithm.such, diagnosis algorithms general focus, compared finding TLDs, insteadfinding minimal cardinality diagnoses.4. Standard Approach SAT-Based MBDstandard encoding MBD problem hSD, COMPS , OBS Boolean Satisfiability (as introduced Smith et al., 2005) associates component c COMPS propositional formula,c , denoting correct behavior, Boolean variable, Hc , signifying c healthy.Viewing observation propositional statement, encoding obtained specifying^= OBSHc c(2)cCOMPS383fiM ETODI , TERN , K ALECH & C ODISHsatisfying assignment , health variables assigned value false determine (notnecessarily minimal) diagnosis .example, consider MBD problem Figure 1, let comp(A, B, C)comp {and, or, xor} denote propositional formula describing behavior component and00 , or00 xor00 gate inputs A, B output C. So, Equation (2) takesform:B C E HX1 xor(A, B, Z1 )(3)= HA1 and(A, B, Z2 )HX2 xor(Z1 , C, D)HA2 and(Z1 , C, Z3 ) HO1 or(Z2 , Z3 , E)formula satisfied assignment variables {A, C, HA1 , HA2 , HO1 } true variables{B, D, E, Z1 , Z2 , Z3 , HX1 , HX2 } false. assignment indicates = {X1 , X2 } diagnosis.obtain minimal cardinality diagnosis seek satisfying assignment minimal number health variables taking value false. example assignment variables{A, C, Z1 , Z3 , HX1 , HX2 , HA1 , HA2 } true variables {B, D, E, Z2 , HO1 } false also satisfies Equation (3) indicates one faulty component. achieved using MAX-SATsolver (Feldman et al., 2010), using SAT solver done implementation underlyingpaper, cardinality constraint (encoded CNF) introduced constrain numberfaulty components detailed next section.matter cardinality constraint encoded CNF, setting |COMPS | = nconstant k, formulafinfik = sum leq( Hc fi c COMPS , k)(4)satisfied k n health variables take value false. specifically, seekminimal value k (the CNF corresponding to) k satisfiable. involves iteratingcalls SAT solver formulae k decreasing values k k satisfiablek1 not. approach takes advantage fact SAT solvers typically incremental:adding clauses satisfiable instance allows solve maintaining derivedinformation search space previous call.5. Approach SAT-Based MBDapproach encoding MBD problem hSD, COMPS , OBS SAT proceeds follows: First,adopt finite domain constraint based representation express basic model. Second, analyzestructure substructures SD introduce additional (redundant) constraintslater boost search minimal cardinality analysis. Third, introduce constraints modelgiven observation OBS additional constraint imposes bound cardinalitydiagnosis (the number unhealthy components). additional constraint reducessubsequent number iterations search minimal cardinality diagnosis. iterationinvolves call underlying SAT solver hence worst-time exponential complexity. So,reducing number important. Given constraints, apply finite domain constraintcompiler (Metodi & Codish, 2012; Metodi, Codish, & Stuckey, 2013) simplify encodecorresponding CNF. Finally apply SAT solver seek suitable satisfying assignmentsolve problem. rest section describe phases detail.384fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISFigure 2: Modeling component c composition xorexperimental evaluation illustrating impact various constraints model presentedSection 7.5.1 Basic Model MBDbuild standard approach, Equation (2). However, observe model baseddiagnosis weak fault model single observation searching minimaldiagnosis, behavior faulty component assumed produce output oppositenormal behavior. diagnosis assumes component c faultystill produces normal output replaced smaller diagnosis contain c. Thus,minimal diagnosis (i.e, subset diagnosis), means componentsassumed produce opposite normal output. paper focus minimalcardinality diagnoses, particular also minimal subset, modify Equation (2)follows, replacing implication bi-implication.= OBS^Hc c(5)cCOMPSmodel behavior (Hc c ) possibly faulty component c encapsulated togetherxor gate illustrated Figure 2. Here, output encapsulated componentxor usual output c negated health variable Hc . One observe Hctrue composition equivalent normal behavior c, otherwise equivalentcomponent c negated output.decision model relation component c health variable Hc introducing additional xor gate (instead introducing CNF clauses directly encode Hc c )two motivations: (1) improve CNF encodings provide tools reason about, simplifysystem components advantage uniform representation logicexpressed system model itself; (2) underlying SAT solver apply, CryptoMiniSat (Soos, 2010), offers direct support xor clauses. (1) MBD problemamenable simplification, (2) underlying SAT solver optimize searchsatisfying assignment. comment straightforward apply techniqueSAT solvers, support xor clauses, adding CNF encodings model.finite domain constraint compiler, BEE (Metodi & Codish, 2012), apply, configurablework types solvers.Equation (3), write comp(A, B, C) comp {and, or, xor} represent component and00 , or00 xor00 gate inputs A, B output C. also writecompH (A, B, C) represent corresponding encapsulated component health variable H.So,compH (A, B, C) = comp(A, B, C0 ) xor(H, C0 , C)385fiM ETODI , TERN , K ALECH & C ODISHviewcompH (A, B, C)(Constraints 1)constraint Boolean variables A, B, C H. Given notation, system depictedFigure 1 modeled following constraints:xorHX1 (A, B, Z1 ) andHA1 (A, B, Z2 ) xorHX2 (Z1 , C, D)andHA2 (Z1 , C, Z3 ) orHO1 (Z2 , Z3 , E)Finally, add constraints representing system components additional cardinalityconstraint:finfisum leq( Hc fi c COMPS , k)(Constraint 2)specify integer constant k number faulty components must k.example, system depicted Figure 1 constant k, introduce constraintsum leq({HX1 , HX2 , HA1 , HA2 , HO1 }, k). Later require satisfy constraintsmodel also minimize value k.summarize presentation basic model, show complete constraint modelminimal cardinality diagnosis MBD problem Figure 1. integer value k,solution constraints diagnosis cardinality k:xorHX1 (A, B, Z1 ) andHA1 (A, B, Z2 ) xorHX2 (Z1 , C, D)andHA2 (Z1 , C, Z3 ) orHO1 (Z2 , Z3 , E)sum leq({HX1 , HX2 , HA1 , HA2 , HO1 }, k)A=1 B=0 C=1 D=0 E=0type constraint model solved encoding CNF formula applyingSAT solver. repeatedly seeking solution decreasing values k find minimalcardinality diagnosis. However, apply basic modeling. Instead refinedescribed rest section.5.2 Encoding Cardinality Constraintsencoding cardinality constraints CNF topic large body research papers. Manythese, described Een Sorensson (2006), based use Batchers oddeven sorting network (Batcher, 1968). sorting network Boolean circuit n inputs noutputs. Given Boolean values inputs, output consists values sorted: say,zeroes ones. context apply sorting network n inputs healthvariables n components given system, n outputs sorted values. Now,encode k health variables take value false assert (k + 1)th outputsorting network one. outputs sorted implies last n k outputsalso ones thus imposing k remaining outputs zero. Looking backwardssorting network implies k inputs take value false.Sorting networks, like Boolean circuits straightforward encode CNF formula.Batchers odd-even construction results CNF O(n log2 (n)) clauses.improvements enable encoding O(n log2 (k)) clauses constrain sum n Booleaninputs less k (Asn, Nieuwenhuis, Oliveras, & Rodrguez-Carbonell, 2009, 2011; Codish386fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS& Zazon-Ivry, 2010). paper encode cardinality constraints CNF using BEE (Metodi &Codish, 2012; Metodi et al., 2013) takes improved approach.5.3 Cones SectionsReasoning relations components system description SD enables infer additional constraints number unhealthy components certain subsystems SD.constraints compiled CNF, help boost search, SAT solver, minimalcardinality diagnosis. Proposition 2 enables diagnosis algorithm focus top-level diagnosesbased partitioning system cones: cone contains one unhealthy component, without loss generality, assumed dominator cone.restrict SAT-based search top-level minimal cardinality diagnoses simply addfollowing constraints denotes set dominated components.V(Constraint 3)cD HcIntroducing constraints indicate healthy components reduces number (unassigned) healthvariables hence boosts search minimal cardinality diagnosis. Section 7 showreasoning cones restrict search top-level diagnoses improves considerably searchminimal cardinality diagnosis.Motivated utility partitioning system cones, seek general partitioning,enables apply similar cardinality constraints larger subsystems components.end introduce notion section. denote sysout(c) set system outputsoccur end path component c. example, system depicted Figure 3sysout(C1 ) = {O2 , O3 } sysout(C5 ) = {O1 , O2 }.Definition 5 (Section) Given system description SD components COMPS define disjointpartitioning COMPS = S1 S2 Sn every c1 , c2 COMPS , c1 c2section Si sysout(c1 ) = sysout(c2 ).C5C1C3C2C4S1S2C6S3C7C8C9C10O1S4S5O2O3Figure 3: Partitioning system cones sections.Figure 3 shows partitioning system maximal cones sections. cones depicted dotted lines, sections dashed. example, components {C1 , C2 } formcone, section S1 consists three cones. observe partitioning system sectionsdone polynomial time demonstrated Algorithm 1 presented below. Given partitioning387fiM ETODI , TERN , K ALECH & C ODISH{S1 , . . . , Sn } sections, introduce constraint model following constraints improve encoding hence subsequent search minimal cardinality diagnosis.section Si , constraintfinfisum leq( Hc fi c Si , bi )(Constraints 4)expresses sum negated health variables Si bounded constant bismaller following two bounds number unhealthy components sectionSi : (a) number outputs Si ; (b) value |sysout(c)| componentc Si . Note Definition 5, value c Si . justify statementProposition 3.illustrate utility sections, consider system given Figure 3 partition5 sections. Observe section labeled S1 3 outputs, component c S12 corresponding system outputs (|sysout(c)| = 2). So, b1 = min{3, 2} hence 2upper bound number unhealthy components S1 . also improvementreasoning cones bound number unhealthy components S1 3(since three cones). Similarly, b2 = min{1, 2}, b3 = min{1, 1}, b4 = min{1, 1}b5 = min{1, 1}.Reasoning constraints number faulty components per section facilitatesMBD encoding another way. Constraints 4 already encoded number faultycomponents per section. numbers partial sums context Constraint 2specifies total number faulty components system reused encoding.following justifies Constraints 4 .Proposition 3 Let hSD, COMPS , OBS MBD problem, SD section, c component minimal cardinality diagnosis. Then, (a) number outputs S,(b) value |sysout(c)| bounds number unhealthy components (from ) S.Proof: statement regarding number outputs follows directly assertionde Kleer (2008) number outputs (sub-) system bound numberunhealthy components. So, remains prove statement regarding |sysout(c)|.Assume premise proposition, denote || = k |S | = (so k). Assumecontradiction > |sysout(c)|. construct diagnosis 0 less k unhealthycomponents. First note obvious: given propagate observed system inputssystem outputs step choose component known inputs producenormal output component healthy, opposite normal output otherwise.diagnosis process result contradictions propagated outputsobserved outputs.Now, take 0 = \ S. (yet) diagnosis. 0 , propagate observed systeminputs way . Now, 0 diagnosisflipped system outputs (those contradict observed outputs). flipped outputmust due one unhealthy components marked healthy 0sysout(c). consider component g outputs o. g 0 , remove it;g 6 0 , add it. So, 0 diagnosis k 0 = |0 | k + |sysout(c)| < k.Algorithm 1 describes partition system sections. Denoting componentsoutputs system COMPS = {c1 , . . . , cn } OUTS = {o1 , . . . , om }, n Boolean388fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISmatrix b computed bij = true oj sysout(ci ) false otherwise. Figure 4 showsexample matrix b system Figure 3. So, Definition 5, pair components ci , cjsection row row j matrix b identical. instance, sectionS5 includes components C9 C10 since system output O3 identical. computationalcomplexity partitioning process complexity running graph search algorithmevery system output worst case O(n2 m). algorithm returns mappingcomponents bit vectors seen section identifiers. So, algorithm returnsmapping components sections.Algorithm 1 partitioning system sectionsinput: system (view graph)output: partitioning system sections= {c1 , . . . , cn }system componentsOUTS= {o1 , . . . , om } system outputsb= (bij )n Boolean matrix(oj OUTS )apply DFS reverse edges system, source = oj(ci COMPS )bijfi oj )= (ci reachablereturn ci 7 hbi1 , . . . , bim fi 1 n1: Denote:2:3:4:5:6:COMPSExample 1 (partition sections) Consider(abstract) system depicted Figure 3 COMPS = {c1 , . . . , c10 }OUTS = {o1 , . . . , o3 }. Boolean matrix evaluated application Algorithm 1 illustratedFigure 4.Figure 4: Partitioning systemFigure 3 sectionsanother benefit partitioning sections: identification cones may performed per section efficient. works because, component X dominated component G sysout(X ) = sysout(G) implying components conealways section. example, Figure 3 component C1 dominated C2sysout(C1 ) = sysout(C2 ) = {O1 , O2 }.recursively defined Algorithm 2 shows compute cones given partition sections.computes set dominators component c section system. denotesucc(c) set components c feeds directly. c feeds componentdominated itself. Otherwise, c dominated c0 c0dominated elements succ(c). instance, given section S1 component C1 Figure 3succ(C1 ) = {C2 }. next recursive call succ(C2 ) include component S1389fiM ETODI , TERN , K ALECH & C ODISH(condition line 2) thus C2 returned (line 5). union calls (C1 , C2 ) returnedcone (line 3).straightforward implement Algorithm 2 efficiently using memoization table avoidrecomputing dominators components already encountered. Since system directed acyclicgraph, recursion Algorithm 2 halt leaf node reached. Thus complexitycalculating dominators every component c section O(|S|2 ). Given sets dominators per component, straightforward specify set maximal cones. component cdominator maximal cone, dominated itself, maximal cone correspondingc set components c dominator.find cones system, Algorithm 2 applied per component per section,cost depends size largest section. contrast, without partition sections,algorithm applied, considering components system insteadcomponents section. Practice shows partition sections benefits computationcones.Algorithm 2 dominators (component c, section S, system C)input: component c section system Coutput: set dominators cfi1: Denote: succ(c) = c0 C fi output c input c02: (succ(c) S) \3:return {c}dominators(c0 , S, C)c0 succ(c)4: else5:return {c}5.4 Modeling Observation Boosting SearchLet OBS + OBS denote sets variables assigned true false OBS , respectively. Then,model observation add obvious constraints.VV(Constraint 5)xOBS xxOBS + ximprove search minimal cardinality diagnosis one introduce upper boundminimal cardinality: number outputs system upper bound minimalcardinality (de Kleer, 2008). bound, well section-specific constraint given(Constraints 4 ) ignores observed inputs outputs. Siddiqi Huang (2011) proposeobtain tighter upper bound minimal cardinality given observation propagatinginput values system, taking upper bound number contradictionsobserved propagated outputs. example, considering MBD problemFigure 1, k = 2 upper bound size minimal cardinality diagnosissystem 2 outputs. Siddiqi Huangs proposal states also 1 upper boundpropagating inputs system one contradiction observedoutputs.Siddiqi Huangs (2011) proposal intuitively appealing, correct caseobserved output also input another component, fact results restrictedsystems case. work example Figure 5. Propagating390fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISFigure 5: Minimal cardinality diagnosis size 2, propagating observed inputs leads 1contradiction observed outputs.observed inputs system assigns 0 outputs indicating single contradictionobservation (on O1 ). However, smallest diagnosis example cardinality 2.example contrived: 83 350 observations system 74181 74XXX benchmark,exhibit minimal cardinality diagnosis larger (erroneous) bound obtained countingconflicts propagated observed outputs.Algorithm 3 computing upper bound number faulty components propagating observed inputs counting conflicts. algorithm computes diagnosis , ||thus upper bound minimal cardinality diagnosis. basic idea propagate inputslong contradict observed, already computed, outputs. componentssystem processed one time. line 3 select component c whose inputsalready determined (initially system inputs determined). c consideralready determined output, oobs , denote oobs = output yet determined. alsoconsider propagated output oprop obtained propagating inputs c assuming c healthy. three cases (lines 610): c already determined outputfix output oprop mark c healthy. c already determined outputconsistent oprop also mark c healthy. Otherwise mark c healthy,propagate output (which already determined).Algorithm 3 Find diagnosis (and upper bound || min. card.)input: system components, COMPS , observation, OBSoutput: diagnosis1: C COMPS , =2: (C 6= )3:select c C inputs c determined4:oobs value output c (N/A undefined)5:oprop value propagating inputs c (assume c healthy)6:(oobs = N/A7:set output c oprop mark c healthy8:else (oobs = oprop )9:mark c healthy10:else11:mark c faulty = {c}12:C C \ {c}13: ReturnAlgorithm 3 terminates marked components healthy faultyfact determined correct diagnosis. applying Algorithm 3 provides upper boundminimal cardinality diagnosis number components marked faulty returneddiagnosis. Note Algorithm 3 correct also given probes (observed values outputs391fiM ETODI , TERN , K ALECH & C ODISHinternal components). Assuming components maintained data-structurecomponents sorted (topologic) according depth, Algorithm 3 performed singlelinear traversal data-structure complexity O(|COMPS |).example application Algorithm 3, consider circuit Figure 5. Propagatinginputs gate A1 gives output 0 contradiction observation O1 . Hence, mark A1unhealthy propagate observation O1 = 1 input A2 together I3 = 1. resultsadditional contradiction observation O2 = 0 mark A2 unhealthy too,report = {A1 , A2 } hence value 2 upper bound minimal cardinality.Let kUB bound found application Algorithm 3. refine Constraint 2 introduceinstead:finfisum leq( Hc fi c COMPS , kUB )(Constraint 20 )appreciate impact Algorithm 3 note that, benchmark consideredpaper, Algorithm 3 determines upper bound equal actual minimal cardinality 81%28,527 observations considered. course, even given precise upper bound, MBDalgorithm still needs validate minimality. SAT-based approach requires one singleiteration underlying SAT solver. Typically, hardest iteration involves callunsatisfiable largest cardinality instance unsatisfiable.5.5 Compiling Constraints CNFMetodi Codish (2012) introduced compiler called BEE encodes finite domain constraintsCNF. Besides facilitating encoding process, compiler also applies partial evaluationoptimizations simplify constraints encoding CNF. particular, appliesequi-propagation (Metodi, Codish, Lagoon, & Stuckey, 2011) process identifyingequalities literals (and constants) implied equations given constraint.X=L implied constraint (where X variable L literal Boolean constant),occurrences X replaced L, reducing number variables subsequent CNFencoding. illustrate constraint simplification diagnosis circuit Figure 1. Considerfollowing constraints (we omitted constraints contributeexample):(1)(2)(3)(4)(5)xorHX1 (A, B, Z1 ) andHA1 (A, B, Z2 ) xorHX2 (Z1 , C, D)andHA2 (Z1 , C, Z3 ) orHO1 (Z2 , Z3 , E)sum leq({HX1 , HX2 , HA1 , HA2 , HO1 }, k)A=1 B=0 C=1 D=0 E=0HA1 = 1 HA2 = 1constraints lines (1) (3) comprise basic constraint model described Section 5.1;constraints line (4) model observation; constraints line (5) express withoutloss generality dominated components {A1 , A2 } cone {A1 , A2 , O1 } healthy.observe following equi-propagation steps:1. (A = 1) (B = 0) xorHX1 (A, B, Z1 ) |= (Z1 = HX1 )2. (A = 1) (B = 0) (HA1 = 1) andHA1 (A, B, Z2 ) |= (Z2 = 0)392fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIS3. (C = 1) (HA2 = 1) andHA2 (Z1 , C, Z3 ) |= (Z1 = Z3 )4. (C = 1) (D = 0) xorHX2 (Z1 , C, D) |= (Z1 = HX2 )5. (E = 1) (Z2 = 0) orHO1 (Z2 , Z3 , E) |= (Z3 = HO1 )(and given) equalities literals obtain substitution:)(7 1, B 7 0, C 7 1, 7 0, E 7 0, Z1 7 HX1 , Z2 7 0,=Z3 7 HX1 , HX2 7 HX1 , HA1 7 1, HA2 7 1, HO1 7 HX1Applying specialize constraint system get:(1)(2)(3)(4)(5)xorHX1 (1, 0, HX1 ) and1 (1, 0, 0)xorHX1 (HX1 , 1, 0)and1 (HX1 , 1, HX1 ) or(HX1 ) (0, HX1 , 0)sum leq({HX1 , HX1 , 0, 0, HX1 }, k)1=1 0=0 1=1 0=0 0=01=1 1=1Now, constraints tautologies remove them. remains singleconstraint:(3) sum leq({HX1 , HX1 , HX1 }, k)satisfied k = 1 HX1 = 1, impliedHA1 = 1, HA2 = 1, HX2 = 1, HO1 = 0. example illustrates equi-propagation partial evaluation applied simplify constraints prior encoding CNF.summarize following observations:1. constraint model MBD polynomial size system: componentcontributes constraint fresh health variable, cone contributes assignmenthealth variables dominated components, section contributes cardinality constraint, finally observation contributes assignment input variablessystem.2. Constraint simplification using BEE polynomial size constraint model.because: (a) simplification step reduces number Boolean variables modelleast one linear number steps, (b) step checks applicabilityfixed number simplification patterns constraint.3. CNF encoding constraint model polynomial size, constraintsintroduces polynomial number clauses CNF (all constraints supportedBEE property).6. Process Implementationsection summarize different phases diagnosis process approach.Section 6.1 focus case seek single minimal cardinality diagnosis,Section 6.2, case seek minimal cardinality diagnoses.393fiM ETODI , TERN , K ALECH & C ODISH6.1 Single Minimal Cardinality Diagnosisprocess single minimal cardinality diagnosis consists four phases. Let =hSD, COMPS , OBS MBD problem. first two phases construct constraint model.First, focusing SD, introduce constraints independent observation,per observation introduce constraints. third phase encode constraint modelCNF, k k upper bound size minimal cardinality diagnosis.fourth phase, solving k using SAT solver results diagnosis cardinality k.detail four phases.Phase 1. Modeling system (offline): system SD first preprocessed partitionsections (Algorithm 1) cones (Algorithm 2). Then, introduce Constraints 1 model SDterms components behavior introduce Constraints 4 bound number unhealthycomponents per section. Finally, using information cones, add Constraint 3 assertsthat, without loss generality, dominated components healthy. system preprocessing performed offline, per system.Phase 2. Modeling observation (online): Constraint 5 added model observationConstraint 20 added bound total number unhealthy components upperbound kUB obtained application Algorithm 3.OBS ,Phase 3. Encoding: constraint system simplified online, observationencoded CNF k , applying optimizing CNF compiler (Metodi et al., 2011). parameter kreflects bound set Constraint 20 bound number unhealthy components diagnosis.Initially, k computed Algorithm 3.Phase 4. Solving: compute diagnosis, , seek satisfying assignment encoding, k ,applying CryptoMiniSat solver (Soos, 2010). set health variables assignedfalse assignment. Denoting || = k 0 , seek satisfying assignment, time0formula k 1 . satisfying assignment found, indicates smaller diagnosis, 0 .Otherwise, minimal cardinality. process invoked repeatedly, time finding0smaller diagnosis, k 0 formula k 1 satisfiable. Then, diagnosis foundprevious iteration minimal cardinality.facilitate search minimal cardinality diagnosis, apply SAT solver wrapper, SCryptoMiniSat (Metodi, 2012b). SCryptoMiniSat takes input CNF formula (k )Boolean variables representing number k. provides satisfying assignment minimizes k. SCryptoMiniSat takes advantage incrementality underlying SAT solvermaintains learned clauses consecutive calls add clauses.Algorithm 4 illustrates process finding single minimal cardinality diagnosis (identifiedline 17), returning set minimal cardinality diagnoses (line 23).6.2 Minimal Cardinality Diagnosesfind minimal cardinality diagnoses first apply process described Section 6.1 findsingle minimal cardinality diagnosis. provides us value k 0 indicating numberfaulty components minimal cardinality diagnosis.Then, enumerate set top-level minimal cardinality diagnoses (each size k 0 )apply additional functionality SCryptoMiniSat allows enumerate (possibly394fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISAlgorithm 4 SATbDinput: system SD components COMPS observation OBSoutput: , set minimal cardinality diagnoses1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:// Phase 1: Offline pre-processingSections partition SD sectionsConstraints(S Sections)(c S)Add Constraints 1 ConstraintsDominators dominators(c, S, SD)|Dominators| > 1Add Constraint 3 component c Constraints// Algorithm 1// describes normal behavior component c// Algorithm 2// sets dominated gates healthy// Phase 2: Modeling observationAdd Constraint 5 ConstraintsFind initial diagnosisAdd Constraint 20 Constraints kUB = ||// add constraints representing OBS// Algorithm 3// Phase 3: EncodingBEE(Constraints)// run constraint compiler obtain CNF// Phase 4: SolvingMC SCryptoMiniSatMinimize(k,)// find min. card. diagnosis (assignment minimizes k = ||MC )// Finding diagnoses minimal cardinalitycnf(kUB = |M C |), LD SCryptoMiniSatAllSolutions()( LD )Add diagnoses expandedreturn395// restrict Constraint 20 use kUB = |M C |// find top-level diagnoses// Proposition 2fiM ETODI , TERN , K ALECH & C ODISHName741817418274283c432c499c880c1355c1908c2670c3540c5315c6288c7552|COMP|65193616020238354688011931669230724163512System Detailsoffline1480.02950.01950.013670.0341320.0860260.0641320.2433250.37233 1400.2950220.71178 1231.5032321.48207 1081.73Feldman#obs.3502502023018351182836846116275620384041557DXC-091-1011-2021-30100100100100934487936698471004900007441373424504600000120000805#obs.54503045141198981271683624811761-1011-2021-3010010010010073417974521004310045000027402126430400410000019005017014Sidd.#obs.700400800800800800404040Table 1: Benchmark suite: systems 74XXX ISCAS-85, observations: Feldman, DXC-09Siddiqi.specified time-out) all, specified number of, satisfying assignments given CNF. applyoption enumerate satisfying assignments formula, k described Section 6.1k = k0 .Finally, based Proposition 2 expand obtained top-level diagnoses provide minimal cardinality diagnoses. Note observation TLD expanded easilyminimal cardinality diagnoses applies diagnosis algorithm. such, diagnosis algorithmsgeneral focus, compared finding TLDs, instead finding minimal cardinalitydiagnoses.7. Experimental Resultssection presents experimental evaluation proposed SAT-based encoding MBD.Section 7.1, consider search single minimal cardinality diagnosis compareperformance SATbD algorithms: HA* (Feldman & van Gemund, 2006), CDA* (Williams& Ragno, 2007) SAFARI (Feldman et al., 2010a). Section 7.2, consider searchminimal cardinality diagnoses compare SATbD algorithms: HDIAG (Siddiqi & Huang,2007) DCAS (Siddiqi & Huang, 2011). Finally, Section 7.3, evaluate impactvarious components SAT-based encoding MBD. experiments run Intel Core2 Duo (E8400 3.00GHz CPU, 4GB memory) Linux (Ubuntu lucid, kernel 2.6.32-24-generic)unless stated otherwise. entire set tools, range benchmarks, well detailedreport results found online (Metodi, 2012a; Metodi, Stern, Kalech, & Codish, 2012b).Table 1 provides basic details concerning systems three observation sets benchmark suite. systems 74XXX (Hansen, Yalcin, & Hayes, 1999), described first 3 rows,ISCAS-85 (Brglez et al., 1989), described following 10 rows. left columntable specifies system name. next four columns (from left) describe systems:numbers components, inputs outputs systems, also preprocessing timeper system SAT-based approach. includes actions performed per systemdescribed Section 6.1: decomposing system sections cones computing boundsper section.396fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISrest columns divided three groups describing experiments three observation sets. first two describe observation set generated Feldman et al. (2010a)DXC-09 observation set used diagnosis competition (DXC) 2009. appliedexperimentation evaluate search single minimal cardinality diagnosis. minimalcardinality diagnoses observations 1 30. columnstwo groups indicate number observations distribution observations accordingsize minimal cardinality diagnoses. observations sets considered hardmany them, high minimal cardinality diagnoses. third group (the rightmost columntable) presents observation set generated Siddiqi Huang (2011) minimalcardinality bounded 8 observations distributed uniformly according sizeminimal cardinality diagnoses. set used evaluation search minimalcardinality diagnoses.Table 1 illustrates comprehensive experimental benchmark involving total 28,527 observations varied minimal cardinality diagnosis size. Observe also (offline) preprocessingtime per system negligible. instance, reprocessing largest system, c7552, takes lesstwo seconds.7.1 SATbD vs. MBD Algorithms: Single Minimal Cardinality Diagnosiscompare SATbD algorithms: HA* (Feldman & van Gemund, 2006), CDA* (Williams& Ragno, 2007) SAFARI (Feldman et al., 2010a) application search single minimal cardinality diagnosis. HA* CDA* based complete algorithm find minimalsubset diagnoses contain diagnoses. configuredfirst minimal subset diagnosis guaranteed also minimal cardinality configuration apply comparison SATbD. SAFARI applies algorithm based stochasticsearch guarantee minimal cardinality even minimal subset diagnosis. Feldman et al. (2010a) report even single double fault cardinalities, SAFARI alwaysfind minimal cardinality. So, expense minimality, SAFARI often faster, comparingHA* CDA*.NameHA*Succ.Timerate%Sec.7418168.374182100.074283100.0c43278.1c49924.1c88011.9c135511.4c19086.4c267012.3c35403.7c53152.7c628813.6c75524.280 sec timeoutc75527.3CDA*Succ.Timerate%Sec.Succ.rate%SAFARIMin Diag.card.%ratio3.150.000.043.635.453.763.901.754.834.3011.947.871.0646.3100.0100.038.210.16.30.00.00.00.00.00.00.04.510.011.455.151.226.66-100.0100.0100.0100.0100.0100.0100.0100.0100.0100.0100.053.50.044.091.057.028.07.048.05.017.014.09.09.025.0-20.770.00.099.513.0TimeSec.SATbDSucc.Timerate%Sec.1.331.041.281.682.001.091.961.921.522.061.967.88-0.000.000.000.030.050.180.371.082.715.2513.3416.18-100.0100.0100.0100.0100.0100.0100.0100.0100.0100.0100.0100.099.30.020.010.020.030.040.050.070.140.150.270.420.561.071.6843.50100.01.49Table 2: Single minimal cardinality diagnosis, Feldmans observations (30 sec. timeout).397fiM ETODI , TERN , K ALECH & C ODISHAlgorithmSystem741817418274283c432c499c880c1355c1908c2670c3540c5315c6288c7552Min0.000.000.000.000.020.010.040.440.050.220.190.210.47HA*Max29.400.000.7629.5321.4329.4322.1214.7827.9129.4229.6828.329.47St. dev.6.040.000.086.335.036.703.882.516.886.2611.988.381.86Min0.000.000.000.040.880.24-CDA*MaxSt. dev.29.017.300.080.0228.142.7926.267.001.550.2129.878.18-Min0.000.000.000.020.040.140.320.952.444.7812.2413.2730.00SAFARIMaxSt. dev.0.010.000.000.000.000.000.030.000.060.000.210.010.430.021.190.043.030.096.200.1614.680.3630.964.7830.220.01Min0.000.000.000.000.010.010.020.030.040.060.090.100.14SATbDMaxSt. dev.0.030.010.020.000.030.010.050.010.070.010.140.010.100.020.520.040.220.040.840.105.930.311.270.2224.302.03Table 3: Single minimal cardinality diagnosis, Feldmans observations, additional statistics.Table 2 presents evaluation focusing Feldmans observations imposing 30 second timeout (except bottom line). columns indicate algorithm, percentage observations solved within prescribed timeout (Succ. rate %) average search time (TimeSec.) average computed set observations excluding timeouts. SATbD,search time: (1) includes actions performed per observation described Section6.1 (Modeling observation (online)); (2) Excludes cost actions performed per system described Table 1 Section 6.1 (column offline); (3) includes times addingobservation cardinality constraints, encoding CNF solving SCryptoMiniSat.results Table 2 show clearly SATbD outperforms evaluated algorithms,terms success rate well terms average runtime. SATbD also outperformsSAFARI succeeds compute diagnosis almost systems. However,small percentage minimal cardinality indicated column Min card%shows percentage (excluding timeouts) observations diagnosis foundSAFARI actually minimal cardinality. also show, column titled Diag. ratio,ratio average cardinality diagnoses found SAFARI average minimalcardinality. example looking data system 74181, 44% diagnoses foundSAFARI minimal, average diagnosis size found 1.33 times larger averagesize minimal cardinality diagnosis. observe SATbD computes verifies minimalcardinality diagnoses even observations minimal cardinality 30. bestknowledge, algorithm succeeded compute minimal cardinality diagnosis hardobservations.Table 3 details additional statistics running times presented Table 2, showingminimum, maximum, standard deviation runtime solved observations.seen, standard deviation SATbD small compared algorithms.displayed statistics cases solved within 30 second timeout. Table entries- mark cases corresponding algorithms could find minimal cardinality diagnosis(within timeout) even single observation.Observe Table 2 99.3% 1,557 observations system c7552 solved SATbDwithin 30 second timeout (only 11 observations solved). observations are,however, solved within 80 seconds each, indicated last row table. One may observeSATbD solves observations given 80 seconds, algorithms still able398fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISsolve them, exception SAFARI. Given extended 80 seconds timeout, SAFARIalso able solve almost observations system, returns minimal cardinalitydiagnoses 13% cases, average, size diagnosis found SAFARI1.68 times larger actual minimal cardinality.NameHA*Succ.Timerate%Sec.CDA*Succ.Timerate%Sec.Succ.rate%SAFARIMincard.%TimeSec.SATbDSucc. Timerate%Sec.741817418274283c432c499c880c1355c1908c2670c3540c5315c6288c755294.4100.0100.075.612.810.19.27.111.311.11.21002.357.4100.0100.040.05.74.50.00.00.00.00.00.00.0100.0100.0100.0100.0100.0100.0100.0100.0100.0100.0100.0100.00.06590802234841418317100-0.000.000.000.030.050.180.381.122.855.8313.1225.28-100.0100.0100.0100.0100.0100.0100.0100.0100.0100.0100.0100.0100.03.860.000.013.704.815.513.333.105.861.2319.770.240.475.190.010.327.171.209.66-0.020.000.010.030.050.050.070.200.140.290.620.11.01Table 4: Single minimal cardinality diagnosis, DXC-09 observations (30 sec. timeout).Table 4 shows evaluation DXC-09 benchmark format Table 2(except omit details regarding size diagnoses found SAFARI). resultsexhibit trend: SAT-based method substantially faster previous algorithms. Note benchmark even contains observations minimal cardinality diagnoses31 (!), also solved SATbD 30 seconds timeout. Table 5 provides additionalstatistics runtimes format Table 3. too, see small standarddeviation SATbD compared algorithms. Note data c6288 systemgiven, single observation system DXC-09 observation set.Figure 6 details evaluation single minimal cardinality diagnosis Feldmans observations four systems (from smaller larger). system plot average runtimefind single minimal cardinality diagnosis (including timeouts) function valueminimal cardinality. black diamond labeled Timeout marks time limit,algorithms halted.Typically, diagnosis problem becomes harder minimal cardinality increases. Firstconsider three plots Figures 6a, 6b 6c. two upper curves correspond systemsHA* CDA* quickly converge 30 seconds timeout. curves SAFARIless constant minority diagnoses actually minimal cardinality (7%c499, 48% c880 9% c5315). performance SAFARI affectedcardinality since first finds arbitrary diagnosis proceeds stochastically minimizediagnosis using pre-defined number attempts. process involves consistency checksaffected size system cardinality diagnosis.consider plot Figure 6d omit curves HA* CDA* depictconstant 80 second timeout. performance SAFARI less constant (aroundaverage 43.5 seconds) 13% diagnoses found actually minimal cardinality.contrast, SATbD considerably faster scales solve even hardest observations.399fiM ETODI , TERN , K ALECH & C ODISHAlgorithmSystemSystem741817418274283c432c499c880c1355c1908c2670c3540c5315c7552MinMin0.000.000.000.000.010.012.740.850.050.319.860.47HA*MaxMax27.260.000.0223.0611.3429.606.5215.3426.043.4428.390.47St. dev.St. dev.6.700.000.015.984.998.341.204.638.061.499.340.00CDA*MaxSt. dev.MaxSt. dev.20.127.440.080.021.220.4026.148.751.420.1620.338.97-MinMin0.000.000.010.050.930.27-SAFARIMaxSt. dev.MaxSt. dev.0.010.000.000.000.000.000.040.000.070.010.240.010.420.021.250.053.070.116.180.1614.110.32-MinMin0.000.000.000.030.040.160.341.002.565.5312.10-MinMin0.000.000.000.010.010.020.020.030.050.060.100.17SATbDMaxSt. dev.MaxSt. dev.0.030.010.020.000.030.010.040.010.070.010.070.010.090.021.360.160.210.030.840.2010.060.9011.541.54100 Timeout100Time (sec.), log scaleTime (sec.), log scaleTable 5: Single minimal cardinality diagnosis, DXC-09 observations (30 sec. timeout), additionalstatistics.10CDA*HA*SafariSATbD10.110CDA*HA*SafariSATbD10.10.010.011357911Minimal Cardinality13151(a) System c499 (30 sec. timeout).100Time (sec.), log scale10CDA*HA*Safari3579 11 13 15 17 19 21 23 25Minimal Cardinality(b) System c880 (30 sec. timeout).100 TimeoutTime (sec.), log scaleTimeoutSATbD1Timeout10SafariSATbD10.10.10.010.0113579 11 13 15 17 19 21 23 >24Minimal Cardinality1(c) System c5315 (30 sec. timeout).3579 11 13 15 17 19 21 >22Minimal Cardinality(d) System c7552 (80 sec. timeout).Figure 6: Single minimal cardinality diagnosis, Feldmans observations, average search time persize minimal cardinality diagnosis.results section clearly indicate SAT based approach outperformsthree algorithms search single minimal cardinality diagnosis.400fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISNamec432c499c880c1355c1908c2670c5315c6288c7552IntelXeon X3220, 2.4GHz, 2Gb RAMHDIAGDCASSucc.TimeSucc.Timerate%Sec.rate%Sec.100.00.21100.00.31100.00.12100.00.2099.00.0799.00.1299.50.1699.50.1590.5368.1376.582.2590.0176.17100.03.150.097.552.340.027.5305.100.087.5260.93Intel Core 2-Duo E8400, 3.00GHz, 4GB RAMSATbDSucc.TLDTimeTLDrate%Sec.Sec.countcount100.00.070.097.972100.00.080.102.4345100.00.080.1110.3963342100.00.130.162.8331927100.00.250.3019.91894733100.00.230.296.78492100.00.580.6726.435694950.0104.58105.147231.837499100.01.011.1264.968396Table 6: Siddiqis observation set: search minimal cardinality diagnoses (1800 sec. timeout).7.2 SATbD vs. MBD Algorithms: Minimal Cardinality Diagnosiscompare SATbD algorithms HDIAG (Siddiqi & Huang, 2007) DCAS (Siddiqi& Huang, 2011) application search minimal cardinality diagnosis. algorithms search complete set minimal cardinality diagnoses. consider observationsgenerated Siddiqi Huang (2011) minimal cardinality bounded 8.Table 6 presents results evaluation. results HDIAG DCAS quotedSiddiqi Huangs work (2011) experiments reported IntelXeon X32202.4GHz, 2Gb RAM. present results systems Siddiqi Huang reportedresults. Although machines differ (with advantage SATbD), results show clearadvantage SATbD faster orders magnitude larger systems.three algorithms table reports on:Succ. rate% indicating percentage observations algorithm findsminimal cardinality diagnoses within 1800 second timeout;Time sec. indicating average computation times find minimal cardinality diagnoses(taking average set observations time out).SATbD report alsoTLD sec. average runtime compute top level diagnoses;TLD count number top level diagnoses;count total number minimal cardinality diagnoses found.Table 6 illustrates SATbD clearly outperforms HDIAG DCAS. succeeds computeminimal cardinality diagnoses observations systems except c6288succeeds 50% 40 observations compared 26.5% DCAS. Notehigher success rate, average runtimes SATbD involve harder observations solvedDCAS.Observe diagnosis time SATbD spent find top level diagnoses indicated column TLD sec. cost compute minimal cardinality diagnoses indicatedcolumn Time sec difference two columns negligible. reflects401fiM ETODI , TERN , K ALECH & C ODISHfact set minimal cardinality diagnoses derived cross product representationset minimal cardinality diagnoses. Observe also number TLDs (column TLDcount) small comparison huge number minimal cardinality diagnoses (columncount). focus TLDs essential additional solution invokes additional callSAT solver. Using SAT solver find minimal cardinality diagnoses directly wouldhopeless due sheer number.7.3 Impact Components SATbDproceed illustrate impact various components SAT-based encoding MBD.SATbD designed using variety techniques distinguish simple vanilla encoding MBD problem SAT problem described Section 5.1. present evaluationimpact techniques based several experiments using following five configurations SAT based system. configurations incremental: starting basicmodel, one adds another component, ending final model applied SATbD.1. Vanilla. minimal basic SAT encoding MBD described Section 5.1.assume naive upper bound minimal cardinality determined numberoutputs given system.2. Improved Cardinality Bound. assume vanilla setting considerimproved bound minimal cardinality diagnosis using Algorithm 3.3. E.P. setting previous applies Equi-Propagation constraint compiler Metodi Codish (2011, 2012) optimize encoding described Section 5.5.4. Cones. setting previous also partitions system conesadds constraints restrict search find top-level diagnoses (TLDs), describedSection 3.5. Sections. setting previous also partitions systemsections introduces corresponding redundant cardinality constraints described Section 5.3.Figure 7 illustrates impact five settings search single minimalcardinality diagnosis 1182 observations Feldmans observation set system c880.horizontal axis, consider observations according size minimal cardinalitydiagnosis. vertical axis, illustrate average runtime seconds. runs apply 300second timeout. choose system c880 present results as: (a) midsize systemobservation sets contain observations minimal cardinality diagnosis larger20; (b) largest system exhibits interesting behavior five configurationswithout many timeouts shadow results. larger systems, c3515c7552 considered below, last two five configurations exhibit interesting curves.three configurations timeout observations.upper curve Figure 7 describes Vanilla setting one may observe cardinality increases curve converges 300 second timeout. second curve describesImproved Bound setting illustrates impact Algorithm 3, especially observations minimal cardinality diagnoses size 1 10. Here, using improved bound reduces402fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISFigure 7: Impact different settings search single minimal cardinality diagnosissystem c880.number iterations SAT solver source improvement. explainsee improvement minimal cardinality diagnosis involves larger numberfaults, consider first large minimal cardinalities, results meaninglessupper curves converge timeout. medium sized minimal cardinalities, considertechniques overwhelming part runtime spent last UNSAT iteration. Moreover, number iterations SAT solver techniquesless same. system c880 26 outputs naive bound usedvanilla setting jumps iteration smaller bound (but usingone-by-one decrement). Given evaluation might consider omitting constraintsimproved lower bound depending parameters instance. However, cost runningAlgorithm 3 negligible always impose corresponding constraints.third curve (E.P.) shows additional impact applying Equi-Propagationconstraint compiler. results substantial speedups first two settings. example,finding diagnosis minimal cardinality 20 requires average 18.5 seconds settingcomparison 214.9 seconds without it. two lower curves graph coincide,fourth setting (cones) makes dramatic impact performance system c880.average runtime required find first minimal cardinality using setting 0.1seconds observations. includes finding diagnoses minimal cardinality 26 (!)58 milliseconds, average. runtimes setting small cannot observeadditional added value applying fifth setting (sections). end considernext experiment comparison using two larger systems, namely c5315 c7552.Figure 8 illustrates impact fifth setting involves partitioning systemsections. left graph illustrates impact sections seeking single minimal cardinalitydiagnosis observations system c5315 right graph illustrates impactseeking top-level minimal cardinality diagnoses (TLDs). lower curve (in graphs), summarizes results using sections upper curve without. example, observationsminimal cardinality 24, sections finding first minimal cardinality diagnosis requires403fiM ETODI , TERN , K ALECH & C ODISHFigure 8: Impact sections cones search single minimal cardinality diagnosis (left)search TLDs (right) using system c5315.Figure 9: Impact sections cones search single minimal cardinality diagnosis (left)search TLDs (right) using system c7552.average 1.13 seconds, without requires 2.43 seconds. observations,sections, find TLDs average 6 seconds, without requires 15 seconds.Similar trends illustrated Figure 9 depicts information system c7552.apply timeout 300 seconds, timeout encountered 48 1,557 observations searching minimal cardinality TLDs (timeout observations consideredaverage runtimes). However, even 48 observations, using sections provides average50% TLDs without (15,785 vs. 9,585 TLDs) reaching timeout.Table 7 details, ISCAS-85 benchmark Feldmans observations, average sizesSAT encodings using full SATbD algorithm. table indicates numberSystem# Components# Variables# Clausesc432160190556c4992022401193c880383227758c190888010264063c267011936362055c3540166920517456c53152307240711277c62882416716122061c75523512352512731Table 7: Average SAT encoding sizes ISCAS-85 benchmark Feldmans observations.404fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSIScomponents system number variables clauses resulting CNFs, takingaverage observations Feldman benchmark. Notably, indicated table,SAT encodings extremely small (in worst case) less three CNF variables tenCNF clauses per system component.8. Discussionpaper focuses MBD problem considering possibly multiple faulty components weakfault model. presentation restricted assume that: every component single output,observation includes single input/output, queries observations specific components (probes), information regarding probability component failures. Evenrestrictions, problem addressed paper computationally hardfocus many prior works model-based diagnosis literature (Feldman & van Gemund, 2006;Williams & Ragno, 2007; Feldman et al., 2010a; Siddiqi & Huang, 2007, 2011).discuss briefly applicability approach general setting. Deeperanalysis adapt approach general setting research topic own.believe success applying SAT solvers MBD problems simplified setting pavesway application general settings.8.1 Boolean Extensionsapproach applies directly setting components described propositionalformulae mode fault ignored (i.e., weak fault model setting). Booleancircuits one straightforward example obvious communityfocused attention. examples similar assumptions SAT-based approachexpected apply directly include works by: Abreu et al. (2009) authors modelsoftware components propositional formulae apply MBD algorithm find bugs; KalechKaminka (2005, 2006) authors model robots multi-robot system diagnoseviolation coordination constraints among robots; Felfernig et al. (2012) authorsmodel finite domain constraints diagnose inconsistent constraint sets.Note restricting components single output really restriction straightforward represent component multiple outputs conjunction single output components. way partition cones sections fully compatible multiple-outputcomponents.8.2 ProbabilitiesReal world applications MBD typically come information regarding probabilitycomponent faulty many MBD algorithms exploit information prioritize diagnosesrespect likelihood (interalia, see de Kleer & Williams, 1989; Williams & Ragno,2007). Sachenbacher Williams (2004) showed incorporate fault probabilities treedecomposition diagnosis algorithm. Extending approach tonconsiderfi probabilitiesstraightfiforward. essential difference Constraint 2, sum leq( Hc fi c COMPS , k),specifies objective function naim minimize.fiIt replacedconstraint takesfiprobabilities account: sum leq( Hc (1 pc ) fi c COMPS , k) pc probabil405fiM ETODI , TERN , K ALECH & C ODISHity component c faulty. So, constraints likely faulty contribute lessobjective function. Note straightforward normalize constraint coefficientsintegers. Constraints form called Pseudo-Boolean constraints encodingCNF well studied (Een & Sorensson, 2006).8.3 Testing ProbesAnother extension straightforward model SAT concerns testing probing (de Kleer& Williams, 1987). Here, diagnosis algorithm given multiple observations input/outputbehavior system (in testing) additional observations internal wires system (inprobing). assumption faulty components consistently faulty, invalidatediagnoses inconsistent multiple observations. Similarly, probes invalidate diagnoses consistent new internal observation. methods run iterativelysingle consistent diagnosis. techniques straightforward encode SAT.testing, improve diagnosis simply take conjunction encodings respectdifferent observations, using health variables. probing also take conjunctioninternal observations. main challenge methods reduce numberprobes (or tests) required find actual diagnosis. common, greedy, approach addresschallenge choose probe (test) maximizes information gain described Feldmanet al. (2010b).8.4 Strong Fault Modelconsider extension approach setting components associatedwider range possible faulty behavior modes. called Strong Fault Model.instance, setting circuit component may stuck 0 (always returns output 0), stuck1(always returns output 1), flip (always flips output) (Struss & Dressier, 1989; de Kleer& Williams, 1989). context example, naive SAT model may obtained follows.Instead considering single propositional health variable Hc , consider one additional varifs1able per fault mode: Hs0c (stuck zero), Hc (stuck one), Hc (flip). Now, introduce clausesfs1s0(a) express fault mode fault (Hc Hc Hc Hc ); (b) expresss1fone fault component sum leq({Hs0c , Hc , Hc }, 1). way propositional healthvariable Hc , before, indicates component healthy diagnosis assignmentfault types component. However, extension requires reconsider definitionsminimal diagnosis cardinality presents new challenge identifying useful partitionssystem solving problem SAT. consider future work.8.5 Diagnosis Physical Systemschallenging problem. Physical systems typically dynamic, involve componentstime dependent behavior, described terms continuous variables. One common approach apply MBD physical systems use qualitative models behaviorsystem modeled set constraints non-numerical (discrete) descriptions (Subramanian& Mooney, 1996). well-known example MBD engine makes relaxationsLivingstone Model-Based Diagnosis System (Williams & Nayak, 1996). Livingstone successfully applied Deep Space One, first spacecraft NASAs New Millennium program.406fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISfirst important step SAT based approach diagnosis physical systemsprovide setting MBD strong fault model able capture probabilities. course,many additional challenges modeling systems using SAT importantfeasible challenge.9. Conclusionpaper addresses MBD challenge extensively researched 25years wide range papers propose different algorithms. present novel SATbased solution problem determine first time, minimal cardinality diagnosesentire standard benchmarks. present extensive experimental evaluation comparingalgorithm HA*, CDA*, SAFARI, HDIAG DCAS. Results unequivocal. algorithm outperforms others, often orders magnitude, search single minimal cardinalitydiagnosis well search minimal cardinality diagnoses. succeed findverify minimal cardinality diagnosis 11 28,257 observations benchmark30 seconds per observation, remaining 11 80 seconds each.best knowledge, SATbD algorithm first algorithm find minimal cardinalitystandard benchmarks discussed above. details regarding experimental evaluationwell prototype implementation SAT-based MBD tool found online (Metodi,2012a; Metodi et al., 2012b).major contribution success approach range preprocessing techniquespresented Section 5. impact demonstrated five configurations systemSection 7.3 full combination fifth configuration. Even SAT,related solvers, improve conjecture careful modeling choices involving combinationstechniques invaluable success future MBD algorithms. beliefresults paper pave way develop apply SAT-based methodologiesMBD problems. particular, extensions diagnosis probabilities componentsfaulty, sequential diagnosis testing probes, and, challenging, diagnosisphysical systems qualitative models. expect methodology, combinesdomain dependent preprocessing, clever modeling SAT, application tools optimizeCNF encodings, relevant hard problems AI SAT-based techniques applicable.Acknowledgmentsresearch supported Israel Science Foundation, grant 182/13.ReferencesAbreu, R., Zoeteweij, P., Golsteijn, R., & van Gemund, A. J. C. (2009). practical evaluationspectrum-based fault localization. Journal Systems Software, 82(11), 17801792.Asn, R., Nieuwenhuis, R., Oliveras, A., & Rodrguez-Carbonell, E. (2009). Cardinality networksapplications. Kullmann, O. (Ed.), SAT, Vol. 5584 Lecture Notes ComputerScience, pp. 167180. Springer.Asn, R., Nieuwenhuis, R., Oliveras, A., & Rodrguez-Carbonell, E. (2011). Cardinality networks:theoretical empirical study. Constraints, 16(2), 195221.407fiM ETODI , TERN , K ALECH & C ODISHBalakrishnan, K., & Honavar, V. (1998). Intelligent diagnosis systems. Journal Intelligent Systems, 8(3/4), 239290.Batcher, K. E. (1968). Sorting networks applications. AFIPS Spring Joint ComputingConference, Vol. 32 AFIPS Conference Proceedings, pp. 307314.Bauer, A. (2005). Simplifying diagnosis using LSAT: propositional approach reasoningfirst principles. Bartak, R., & Milano, M. (Eds.), International Conference IntegrationAI Techniques Constraint Programming Combinatorial Optimization Problems(CP-AI-OR), Vol. 3524 Lecture Notes Computer Science, pp. 4963, Berlin, Heidelberg.Springer-Verlag.Biere, A., Heule, M., van Maaren, H., & Walsh, T. (Eds.). (2009). Handbook Satisfiability, Vol.185 Frontiers Artificial Intelligence Applications. IOS Press.Brglez, F., Bryan, D., & Kozminski, K. (1989). Combinatorial profiles sequential benchmarkcircuits. IEEE International Symposium Circuits Systems, pp. 19291934.Bylander, T., Allemang, D., Tanner, M. C., & Josephson, J. R. (1991). computational complexity abduction. Artificial Intelligence, 49(1-3), 2560.Codish, M., & Zazon-Ivry, M. (2010). Pairwise cardinality networks. Logic Programming,Artificial Intelligence, Reasoning (LPAR), pp. 154172.Darwiche, A. (2001). Decomposable negation normal form. Journal ACM, 48(4), 608647.de Kleer, J., & Williams, B. C. (1987). Diagnosing multiple faults. Artificial Intelligence, 32(1),97130.de Kleer, J. (2008). improved approach generating max-fault min-cardinality diagnoses.International Workshop Principles Diagnosis (DX).de Kleer, J., & Williams, B. C. (1989). Diagnosis behavioral modes. International JointConference Artificial Intelligence (IJCAI), pp. 13241330.Dressler, O., & Struss, P. (1995). Occm. http://www.occm.de.DXC(2009).International diagnostichttps://sites.google.com/site/dxcompetition/.competitionseries.Website.Een, N., & Sorensson, N. (2006). Translating pseudo-Boolean constraints SAT. JournalSatisfiability (JSAT), 2(1-4), 126.El Fattah, Y., & Dechter, R. (1995). Diagnosing tree-decomposable circuits. International JointConference Artificial Intelligence (IJCAI), 95, 17421749.Feldman, A., Provan, G., de Kleer, J., Robert, S., & van Gemund, A. (2010). Solving model-baseddiagnosis problems Max-SAT solvers vice versa. International WorkshopPrinciples Diagnosis (DX), pp. 185192.Feldman, A. (2012). Lydia-ng. http://www.general-diagnostics.com/products.php.Feldman, A., de Castro, H. V., van Gemund, A., & Provan, G. (2013). Model-based diagnosticdecision-support system satellites. IEEE Aerospace Conference, pp. 114. IEEE.Feldman, A., Provan, G., & van Gemund, A. (2010a). Approximate model-based diagnosis usinggreedy stochastic search. Journal Artificial Intelligence Research (JAIR), 38, 371413.408fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISFeldman, A., Provan, G., & van Gemund, A. (2010b). model-based active testing approachsequential diagnosis. Journal Artificial Intelligence Research (JAIR), 39, 301334.Feldman, A., & van Gemund, A. J. C. (2006). two-step hierarchical algorithm model-baseddiagnosis. Conference Artificial Intelligence (AAAI), pp. 827833.Felfernig, A., Schubert, M., & Zehentner, C. (2012). efficient diagnosis algorithm inconsistent constraint sets. Artificial Intelligence Engineering Design, Analysis Manufacturing, 26(1), 5362.Frohlich, P., & Nejdl, W. (1997). static model-based engine model-based reasoning.International Joint Conference Artificial Intelligence (IJCAI), pp. 466473.Fujiwara, H., Member, S., Shimono, T., & Member, S. (1983). acceleration test generationalgorithms. IEEE Transactions Computers, 32, 11371144.Hansen, M. C., Yalcin, H., & Hayes, J. P. (1999). Unveiling ISCAS-85 benchmarks: casestudy reverse engineering. IEEE Des. Test, 16, 7280.Jannach, D., & Schmitz, T. (2014). Model-based diagnosis spreadsheet programs: constraintbased debugging approach. Automated Software Engineering, 1, 140.Kalech, M., & Kaminka, G. A. (2005). Towards model-based diagnosis coordination failures.Conference Artificial Intelligence (AAAI), pp. 102107.Kalech, M., Kaminka, G. A., Meisels, A., & Elmaliach, Y. (2006). Diagnosis multi-robot coordination failures using distributed CSP algorithms. Conference Artificial Intelligence(AAAI), pp. 970975.Kirkland, T., & Mercer, M. R. (1987). topological search algorithm ATPG. ACM/IEEEDesign Automation Conference, DAC, pp. 502508.Knuth, D. E. (2014). Art Computer Programming: Volume 4B, Pre-fascicle 6A, Section7.2.2.2: Satisfiability. Unpublished. Draft available from: http://www-cs-faculty.stanford.edu/knuth/fasc6a.ps.gz.Marques-Silva, J., Lynce, I., & Malik, S. (2009). Conflict-driven clause learning SAT solvers.Handbook satisfiability, 185, 131153.Metodi, A. (2012a). SCryptodiagnoser: SAT based MBD solver. http://amit.metodi.me/research/mbdsolver.Metodi, A. (2012b). SCryptominisat. http://amit.metodi.me/research/scrypto.Metodi, A., & Codish, M. (2012). Compiling finite domain constraints SAT BEE. TheoryPractice Logic Programming (TPLP), 12(4-5), 465483.Metodi, A., Codish, M., Lagoon, V., & Stuckey, P. J. (2011). Boolean equi-propagation optimized SAT encoding. CP, pp. 621636.Metodi, A., Codish, M., & Stuckey, P. J. (2013). Boolean equi-propagation concise efficientSAT encodings combinatorial problems. Journal Artificial Intelligence Research (JAIR),46, 303341.Metodi, A., Stern, R., Kalech, M., & Codish, M. (2012a). Compiling model-based diagnosisBoolean satisfaction. Conference Artificial Intelligence (AAAI).409fiM ETODI , TERN , K ALECH & C ODISHMetodi, A., Stern, R., Kalech, M., & Codish, M. (2012b). Compiling model-based diagnosisBoolean satisfaction: Detailed experimental results prototype implementation. http://www.cs.bgu.ac.il/mcodish/Papers/Pages/aaai-2012.html.Murray, J., Hughes, G., & Kreutz-Delgado, K. (2006). Machine learning methods predicting failures hard drives: multiple-instance application. Journal Machine Learning Research(JMLR), 6(1), 783.Nica, I., Pill, I., Quaritsch, T., & Wotawa, F. (2013). route success - performance comparison diagnosis algorithms. International Joint Conference Artificial Intelligence(IJCAI), pp. 10391045.Reiter, R. (1987). theory diagnosis first principles. Artificial Intelligence, 32(1), 5795.Sachenbacher, M., & Williams, B. (2004). Diagnosis semiring-based constraint optimization.Eureopean Conference Artificial Intelligence (ECAI), pp. 873877.Selman, B., & Levesque, H. J. (1990). Abductive default reasoning: computational core.National Conference Artificial Intelligence (AAAI), pp. 343348.Siddiqi, S. A., & Huang, J. (2007). Hierarchical diagnosis multiple faults. International JointConference Artificial Intelligence (IJCAI), pp. 581586.Siddiqi, S. A., & Huang, J. (2011). Sequential diagnosis abstraction. Journal Artificial Intelligence Research (JAIR), 41, 329365.Smith, A., Veneris, A. G., Ali, M. F., & Viglas, A. (2005). Fault diagnosis logic debuggingusing Boolean satisfiability. IEEE Trans. CAD Integrated Circuits Systems, 24(10),16061621.Soos, M. (2010). Cryptominisat, v2.5.1. http://www.msoos.org/cryptominisat2.Stein, B., Niggemann, O., & Lettmann, T. (2006). Speeding model-based diagnosis heuristicapproach solving SAT. IASTED international conference Artificial intelligenceapplications, pp. 273278.Stern, R. T., Kalech, M., Feldman, A., & Provan, G. M. (2012). Exploring duality conflictdirected model-based diagnosis. AAAI.Struss, P., & Dressier, O. (1989). Physical negation: Integrating fault models generaldiagnostic engine. International Joint Conference Artificial Intelligence (IJCAI), pp.13181323.Struss, P., & Price, C. (2003). Model-based systems automotive industry. AI magazine, 24(4),1734.Stumptner, M., & Wotawa, F. (2001). Diagnosing tree-structured systems. Artificial Intelligence,127(1), 129.Stumptner, M., & Wotawa, F. (2003). Coupling CSP decomposition methods diagnosis algorithms tree-structured systems. International Joint Conference Artificial Intelligence(IJCAI), pp. 388393.Subramanian, S., & Mooney, R. J. (1996). Qualitative multiple-fault diagnosis continuous dynamic systems using behavioral modes. National Conference Artificial Intelligence(AAAI), pp. 965970.410fiA N OVEL SAT-BASED PPROACH ODEL BASED IAGNOSISTorasso, P., & Torta, G. (2006). Model-based diagnosis OBDD compilation: complexityanalysis. Reasoning, Action Interaction AI Theories Systems, pp. 287305.Wang, J., & Provan, G. (2010). benchmark diagnostic model generation system. Part A: SystemsHumans, IEEE Transactions Systems, Man Cybernetics, 40(5), 959981.Williams, B. C., & Nayak, P. P. (1996). model-based approach reactive self-configuring systems. National Conference Artificial Intelligence (AAAI), pp. 971978.Williams, B. C., & Ragno, R. J. (2007). Conflict-directed A* role model-based embeddedsystems. Discrete Applied Mathematics, 155(12), 15621595.411fiJournal Artificial Intelligence Research 51 (2014) 255-291Submitted 03/14; published 09/14Automaton PlansChrister BackstromCHRISTER . BACKSTROM @ LIU . SEDepartment Computer ScienceLinkoping UniversitySE-581 83 Linkoping, SwedenAnders JonssonANDERS . JONSSON @ UPF. EDUDept. Information Communication TecnologiesUniversitat Pompeu FabraRoc Boronat 13808018 Barcelona, SpainPeter JonssonPETER . JONSSON @ LIU . SEDepartment Computer ScienceLinkoping UniversitySE-581 83 Linkoping, SwedenAbstractMacros long used planning represent subsequences operators. Macrosused place individual operators search, sometimes reducing effort requiredfind plan goal. Another use macros compactly represent long plans. paperintroduce novel solution concept called automaton plans plans represented usinghierarchies automata. Automaton plans viewed extension macros enablesparameterization branching. provide several examples illustrate automaton plansuseful, compact representation exponentially long plans alternativesequential solutions benchmark domains L OGISTICS G RID. also compareautomaton plans compact plan representations literature, find automatonplans strictly expressive macros, strictly less expressive HTNs certainrepresentations allowing efficient sequential access operators plan.1. Introductionpaper introduce novel solution concept planning call automaton plans.ease presentation divide introduction two parts. first part discussexisting concepts plan representation literature. second part describenovel representation propose.1.1 Plan RepresentationsFollowing introduction TRIPS planning (Fikes & Nilsson, 1971), take researcherslong discover utility storing sequences planning operators, macros (Fikes, Hart, &Nilsson, 1972). Macros first used tool plan execution analysis. However,macros turned several useful properties exploited researchersplanning community ever since.c2014AI Access Foundation. rights reserved.fiB ACKSTR OM , J ONSSON , & J ONSSONOne property possibility compute cumulative preconditions effects, effectivelymaking macros indistinguishable individual operators. planning instance augmentedset macros, potentially speeding search solution since macros reachstate space individual operators. extreme, search space macrosexponentially smaller search space original planning operators (Korf, 1987).Moreover, subsequences operators repeated, hierarchy macros represent plancompactly simple operator sequence, replacing occurrence repeating subsequence single operator (i.e. macro). extreme, one represent exponentially longplan using polynomially many macros polynomial length (Gimenez & Jonsson, 2008; Jonsson,2009). Sometimes even possible generate compact macro plan polynomial time,case macros viewed tool complexity analysis, reducing complexitysolving particular class planning instances.Macros clearly show advantages associated plan representationssimply store plan sequence actions. Apart obvious purpose saving space,reasons considering alternative representations. One important reason highlightproperties plan might apparent sequential representationexploited increased planning efficiency. One prominent example partially ordered plansrepresent plans sets actions associated partial orders. Partially ordered plans oftenused planning speed search (McAllester & Rosenblitt, 1991). general, factexists compact representation plan implies planning instance exhibits formstructure might possible exploit simpler efficient reasoning.Potentially, exist many alternative plan representations store plans compactly.compact representations broadly divided two categories. first type planrepresentation stores single plan, second type stores set plans. Macrosillustrative example first type, already seen macro plans exponentiallysmaller sequential representation. example second type reactive plans, alsoknown universal plans reactive systems, represent one plan stategoal reachable.usefulness compact representation depends several factors.1.1.1 C OMPRESSION P ROPERTIESClearly, one important property compact plan representation size. However,information-theoretic bound compressibility plans: representation containing n bitsdistinguish 2n different plans, limiting applicability highly compact represenntations. exist TRIPS instances n variables 22 1 different plans (Backstrom &Jonsson, 2012, Construction 10), implying least 2n 1 bits needed distinguish oneparticular solution rest. However, may often suffice represent single solutionarbitrarily chosen. extreme, represent solution compactly storing planninginstance together algorithm solving it.1.1.2 ACCESS P ROPERTIESAnother important property compact plan representation ability access particularaction plan. Two alternative concepts proposed (Backstrom & Jonsson, 2012):sequential random access. Sequential access implies actions plan retrieved256fiAUTOMATON P LANSorder, random access implies retrieve action given positionplan. forms access, ideally able retrieve actions polynomial time,something always possible.1.1.3 V ERIFICATIONthird property compact plan representation able verify plan actually constitutes solution given planning instance. complexity plan verification directly relatedcomplexity plan existence, i.e. determining whether instance solution. Assume problem plan verification compact plan representation R complexity classC. Let X set TRIPS instances p satisfying following condition: p solvableexists solution p represented R using O(p(||p||)) bits, p polynomialfunction ||p|| number bits representation p. assumptions,problem plan existence X complexity class NPC : non-deterministically guess compactplan R verify plan solves p. many choices C, plan existence X boundedaway PSPACE, i.e. easier general TRIPS planning. conclude simple verificationcomes price: decreased expressiveness corresponding planning instances.obviously difficult identify representations satisfy three propertiesable express reasonably complex plans. reasonable approach look balanced representations expressive computationally efficient. Let us evaluate macros accordingthree properties above. paper consider grounded macros totally orderedallow nesting, i.e. macro involve macros long create cyclic dependencies among macros. know exist examples macros provide powerfulcompact representation mechanism. Macro plans excellent access properties: sequentialrandom access performed polynomial time (Backstrom & Jonsson, 2012).also verifiable polynomial time (Backstrom, Jonsson, & Jonsson, 2012b), implying planninginstances whose solutions represented using polynomial-size macro plans easiergeneral TRIPS planning, also less expressive.1.2 Automaton Planspaper introduce novel solution concept planning, inspired macros, callautomaton plans. automaton plan consists hierarchy automata, endowedability call automata. bottom level hierarchy individual plan operators.Automaton plans viewed extension macro plans along two dimensions. First,automaton parameterized, enabling compactly represent single operator sequencewhole family sequences. Second, automaton branch input, making possiblestore different subsequences operators distinguish providing different inputautomaton.main motivation automaton plans express plans compactly macro plans cannot, maintaining access properties verification reasonable level. present severalexamples automaton plans, show useful variety ways. domainsTowers Hanoi, automaton plans represent exponentially long plans even compactly macro plans. Even plans necessarily long, ability parameterizeplans makes possible store repeating families action subsequences common benchmarkdomains.257fiB ACKSTR OM , J ONSSON , & J ONSSONtest usefulness automaton plans, formally compare automaton plans compact plan representations literature along three dimensions discussed Section 1.1:compression, access, verification. macro plan also automaton plan, implyingautomaton plans least compressed macro plans. like macro plans, automaton planssequentially accessed polynomial time. also show subclass automaton plansadmit polynomial-time random access, although still unknown whether result generalizesautomaton plans. Finally, verification automaton plans p2 -complete, causing automatonplans strictly expressive macros.Hierarchical Task Networks (Erol, Hendler, & Nau, 1996), HTNs short, alsoviewed type compact plan representation. addition planning operators, HTN definesset tasks, set associated methods expanding task. name suggests,tasks organized hierarchy planning operators bottom. hierarchy mayconsiderably compact actual sequence operators plan. general, plansrepresented HTNs unique search may required find valid plan.Instead comparing automaton plans general HTNs, consider totally ordered HTNsunique plans, i.e. methods associated task mutually exclusive specifytotally ordered expansion. show automaton plan efficiently translatedHTN, causing HTNs least compressed automaton plans. HTNs unique planssequentially accessed polynomial time, true random access. Finally,plan existence totally ordered HTNs known PSPACE-hard (Erol et al., 1996), implyingverification HTNs harder automaton plans, turn causing HTNs strictlyexpressive.Combining results, automaton plan appears offer reasonable tradeoffcompression, access, verification, making interesting candidate type balancedplan representation discussed earlier. Since automaton plans strictly expressivemacros, use represent plans compactly wider range planning instances.However, come expense prohibitively expensive computational properties,since verification easier automaton plans HTNs well general TRIPS planning.Automaton plans first introduced conference paper (Backstrom, Jonsson, & Jonsson,2012a). present paper makes following additional contributions:formalization automaton plans using Mealy machines, providing stronger theoreticalfoundation automaton plans automaton theory.proof plan verification automaton plans p2 -complete, result usedcompare expressive power automaton plans compact plan representations.reduction automaton plans HTNs, proving HTNs strictly expressiveautomaton plans, comes price expensive computational properties.rest paper organized follows. Section 2 describes notation basic concepts,Section 3 introduces automaton plans. Section 4 illustrates automaton plans using several practicalexamples. Section 5 compares computational properties automaton planscompact plan representations literature. Section 6 describes related work, Section 7concludes discussion.258fiAUTOMATON P LANS2. Notationsection describe notation used throughout paper. first introduce formaldefinition TRIPS planning domains based function symbols, show TRIPS planninginstances induced associating sets objects planning domains. idea usedPDDL language compactly express planning domains planning instances,definition viewed mathematical adaptation PDDL.Given set symbols , let n denote set strings length n composed symbols. Let x n string. 1 k n, use xk denote k-th symbolx. customary, use denote empty string, satisfies x = x = x. Givenset elements S, let + denote sequences non-empty sequences elements S,respectively. Given sequence , let || denote length. construct X, let ||X||denote size, i.e. number bits representation.2.1 Function Symbolsplanning domain abstract description instantiated given set objectsform planning instance. section introduce function symbols facilitate descriptionplanning domains. Formally, function symbol f fixed arity ar(f ) appliedvector objects x ar(f ) produce new object f [x]. Let F set function symbolslet set objects. define F = {f [x] : f F, x ar(f ) } F setnew objects obtained applying function symbol F vector objectsappropriate arity.Let f g two function symbols F . argument map f g function :ar(f) ar(g) mapping arguments f arguments g. Intuitively, result applyingargument x ar(f ) f , argument g either component x constant objectindependent x. Formally, 1 k ar(g), either k (x) = xj fixed index jsatisfying 1 j ar(f ), k (x) = fixed object . argument map f genables us map object f [x] F object g[(x)] F .Since argument maps restricted form, characterize argument map fg using index string f g, i.e. string ({1, . . . , ar(f )} )ar(g) containing indicesf and/or objects . index string f g induces argument map f g1 k ar(g), k (x) = x(k) (k) {1, . . . , ar(f )} k (x) = (k)otherwise.illustrate idea, let F = {f, g} ar(f ) = ar(g) = 2 let = {1 , 2 }.example index string f g given = 21 , induces argument map fg input x 2 , first component (x) always equals second componentx, second component (x) always equals 1 . Given , object f [1 2 ] F mapsobject g[(1 2 )] = g[2 1 ] F .2.2 PlanningLet V set propositional variables fluents. literal l non-negated negated fluent,i.e. l = v l = v v V . Given set literals L, let L+ = {v V : v L}L = {v V : v L} set fluents appear non-negated negated L,/ L v V ,respectively. say set literals L consistent v/ L v259fiB ACKSTR OM , J ONSSON , & J ONSSONequivalent L+ L = . state V set fluents currently true; fluentsassumed false. set literals L holds state L+ L = .define update operation state set literals L L = (s \ L ) L+ .paper focus TRIPS planning negative preconditions. Formally, TRIPSplanning domain tuple = hP, Ai, P set function symbols called predicatesset function symbols called actions. action associated preconditionpre(a) = {(p1 , 1 , b1 ), . . . , (pn , n , bn )} where, 1 k n, pk predicate P ,k argument map pk , bk Boolean. well-defined, pre(a)simultaneously contain (p, , true) (p, , false) predicate p argument mapp. postcondition post(a) similarly defined.TRIPS planning instance tuple p = hP, A, , I, Gi, hP, Ai planning domain,set objects, initial state, G goal state, i.e. set literals implicitly defining setstates G holds. P implicitly define set fluents P = {p[x] : p P, x ar(p) }applying predicate vector objects . Likewise, implicitly defineset operators . Thus fluents correspond grounded predicates, operators correspondgrounded actions, reason distinguish action operator text.initial state P goal state G P subsets (non-negated) fluents.action x ar(a) , precondition operator a[x] givenpre(a[x]) = {b1 p1 [1 (x)], . . . , bn pn [n (x)]}, pre(a) = {(p1 , 1 , b1 ), . . . , (pn , n , bn )},bp[y] = p[y] b false, bp[y] = p[y] b true. words, pre(a[x]) resultapplying argument map k precondition argument x obtain fluentpk [k (x)] P , appropriately negated. postcondition post(a[x]) a[x]similarly defined. Note pre(a) post(a) well-defined, pre(a[x]) post(a[x])consistent sets literals P .operator applicable state pre(o) holds s, resultapplying post(o). plan p sequence operators = ho1 , . . . ,pre(o1 ) holds and, 1 < k n, pre(ok ) holds post(o1 ) post(ok1 ).say solves p G holds post(o1 ) post(on ). Given two operator sequences, let ; denote concatenation.PPNote p pP ||ar(p) fluents aA ||ar(a) operators, exponential ||p||, description length p. avoid errors due discrepancies instance descriptionlength actual instance size, consider P that, p P A,ar(p) ar(a) constants independent ||p||. sometimes describe planning instances directly form p = hP , , , I, Gi defining predicates actions arity 0,implying predicate fluent action operator.3. Automaton Planssection define concept automaton plans, similar macro plans.like macro plan consists hierarchy macros, automaton plan consists hierarchyautomata. Unlike macros, output automaton depends input, making possiblesingle automaton represent family similar plans. either use automaton planrepresent single plan explicitly specifying input string root automaton, allowparameterized plans leaving input string root automaton undefined.260fiAUTOMATON P LANS3.1 Mealy Machinesrepresent individual automata use variant deterministic finite state automata called Mealymachines (Mealy, 1955), defined tuple = hS, s0 , , , T,finite set states,s0 initial state,input alphabet,output alphabet,: transition function,: output function.Mealy machine transducer whose purpose generate sequence outputgiven input string. contrast acceptors generate binary output (accept reject). Executing Mealy machine input x = x1 x2 xn n generates output(s0 , x1 )(s1 , x2 ) (sn1 , xn ) n sk = (sk1 , xk ) 1 k < n.extend Mealy machines allow -transitions (i.e. transitions consume inputsymbols ). may general cause Mealy machines non-deterministic, includeseveral restrictions preserve determinism:redefine partial function : ( {}) S, either(s, ) defined (s, ) defined , both. still, sense,total function since always exactly one possible transition state S,transition may may consume input symbol.allow -cycles, i.e. must exist subset {s1 , . . . , sn } states(sk1 , ) = sk 1 < k n (sn , ) = s1 .require -transitions must always fire, order make behavior Mealymachines well defined also input symbols consumed.also allow output, i.e. transition may may generate output symbol . allow-transitions -output redefine partial function : ( {}) {}.definition consistent , i.e. state S, (s, ) defined(s, ) defined, else (s, ) defined . define extended output function: state S, input symbol input string x ,(s, ) (T (s, ), ), (s, ) defined,(s, ) =,otherwise,(s, ) (T (s, ), x), (s, ) defined,(s, x) =(s, ) (T (s, ), x), otherwise.deterministic output Mealy machine input x given (s0 , x) .customary use graphs represent automata. graph associated Mealymachine one node per state S. edge states label i/o,( {}) ( {}), implies (s, i) = (s, i) = o. simplify graphsadopt following conventions:261fiB ACKSTR OM , J ONSSON , & J ONSSON1/b/ha, bi0/c/1/c0/aFigure 1: example Mealy machine.edge states label i/h1 , . . . , n ( {}) n used shorthand describe series intermediate states s2 , . . . , sn (s, i) = s2 , (sk1 , ) =sk 2 < k n, (sn , ) = t, (s, i) = 1 , (sk , ) = k 2 k n.edge states label n / used shorthand describe seriesintermediate states s2 , . . . , sn , (s, ) = s2 , (sk1 , ) = sk2 < k n, (sn , ) = t, (s, ) = , (sk , ) = 2 k n.Figure 1 shows example Mealy machine = hS, s0 , , , T, |S| = 4, = {0, 1},= {a, b, c}. initial state s0 identified incoming edge without origin. Two exampleoutputs (s0 , 01) = aab (s0 , 1011) = cccbab.3.2 Automaton Hierarchiessection explain construct hierarchies automata order represent plans.automaton hierarchy associated TRIPS planning instance p = hP, A, , I, Gi. defineset Au function symbols called automata, i.e. automaton Au fixed arity ar(M ),something unusual automaton theory. motivation automata used representplans viewed abstract actions, input automaton serves dual purpose:determine fire transitions automaton order generate output, copyinput symbols onto actions automata.automaton Au corresponds Mealy machine hSM , s0 , , , TM , i,set objects TRIPS instance p = (A Au) ({1, . . . , ar(M )} ) .output symbol (u, ) pair consisting action u automaton u Auindex string u. input string x ar(M ) , require output automatoninput x non-empty, i.e. (s0 , x) = h(u1 , 1 ), . . . , (un , n )i +M.Given Au, define expansion graph denotes dependencies among automata Au:Definition 1. Given set Au automata, expansion graph GAu = hAu, directed graphautomata where, pair M, Au, exists state-inputpair (s, ) SM (s, ) = (M , ) index string .Thus edge automata appears output .next define automaton hierarchy tuple H = h, A, Au, ri, A, Au defined above,GAu acyclic weakly connected,262fiAUTOMATON P LANSr Au root automaton,exists directed path GAu r automaton Au.Au, let Succ(M ) = {M Au : } set successors .height h(M ) length longest directed path node GAu , i.e.0,Succ(M ) = ,h(M ) =1 + maxM Succ(M ) h(M ), otherwise.Given automaton hierarchy H, let = maxM Au |SM | maximum number statesMealy machine representation automaton, let Ar = 1 + maxuAAu ar(u)maximum arity actions automata, plus one.aim automaton Au generate output (s0 , x) input x,define decomposition strategy. requires us process output concrete waydescribed below. alternative would integrate processing step automata,would longer correspond well-established definition Mealy machines.first define notion grounded automata, analogous notion grounded actions(i.e. operators) planning instance. automaton call [x] automaton associatedinput string x ar(M ) , representing called input x. sets Au define setautomaton calls Au = {M [x] : Au, x ar(M ) }, i.e. automata paired input stringsappropriate arity. next define function Apply acts decomposition strategy:Definition 2. Let Apply : Au (A Au )+ function [x] Au ,Apply(M [x]) = hu1 [1 (x)], . . . , un [n (x)]i, (s0 , x) = h(u1 , 1 ), . . . , (un , n )i and,1 k n, k argument map uk induced index string k .purpose Apply replace automaton call sequence operatorsautomaton calls. Recursively applying decomposition strategy eventually resultsequence consisting exclusively operators. show Apply always computedpolynomial time size automaton.Lemma 3. automaton call [x] Au , complexity computing Apply(M [x])bounded Ar2 .Proof. definition Mealy machines requires cycle consume least one input symbol.worst case, fire |SM | 1 -transitions followed transition consumes inputsymbol. Since input string x exactly ar(M ) symbols, total number transitionsbounded (|SM | 1)(1 + ar(M )) + ar(M ) |SM | (1 + ar(M )) Ar.Let h(u1 , 1 ), . . . , (un , n )i output input x. 1 k n, uksingle symbol, k contains Ar 1 symbols. Applying argument map kinduced k input string x linear |k | Ar. Thus computing Apply(M [x]) =hu1 [1 (x)], . . . , un [n (x)]i requires Ar time space element, since nAr, total complexity bounded Ar2 .represent result recursively applying decomposition strategy, define expansion function Exp automaton calls operators:Definition 4. Let Exp function (A Au )+ defined follows:263fiB ACKSTR OM , J ONSSON , & J ONSSON1. Exp(a[x]) = ha[x]i a[x] ,2. Exp(M [x]) = Exp(Apply(M [x])) [x] Au ,3. Exp(hu1 [y1 ], . . . , un [yn ]i) = Exp(u1 [y1 ]); . . . ; Exp(un [yn ]).following lemma prove expansion automaton call sequence operators.Lemma 5. automaton call [x] Au , Exp(M [x]) A+.Proof. prove lemma induction h(M ). h(M ) = 0, Apply(M [x]) sequenceoperators ha1 [x1 ], . . . , [xn ]i A+, implyingExp(M [x]) = Exp(Apply(M [x])) = Exp(ha1 [x1 ], . . . , [xn ]i) == Exp(a1 [x1 ]); . . . ; Exp(an [xn ]) = ha1 [x1 ]i; . . . ; han [xn ]i == ha1 [x1 ], . . . , [xn ]i A+.next prove inductive step h(M ) > 0. case, Apply(M [x]) sequence operatorsautomaton calls hu1 [y1 ], . . . , un [yn ]i (A Au )+ , implyingExp(M [x]) = Exp(Apply(M [x])) = Exp(hu1 [x1 ], . . . , un [xn ]i) == Exp(u1 [x1 ]); ; Exp(un [xn ]).1 k n, uk [xk ] operator Exp(uk [xk ]) = huk [xk ]i A+.hand, uk [xk ] automaton call, Exp(uk [xk ]) A+hypothesisinductionsinceh(uk ) < h(M ). Thus Exp(M [x]) concatenation several operator sequences A+,operator sequence A+.Note proof depends fact expansion graph GAu acyclic, since otherwiseheight h(M ) automaton ill-defined. also prove upper bound lengthoperator sequence Exp(M [x]).Lemma 6. automaton call [x] Au , |Exp(M [x])| (S Ar)1+h(M ) .Proof. induction h(M ). h(M ) = 0, Apply(M [x]) = ha1 [x1 ], . . . , [xn ]i sequenceoperators , implying Exp(M [x]) = Apply(M [x]) = ha1 [x1 ], . . . , [xn ]i. follows|Exp(M [x])| (S Ar)1+0 since n Ar.h(M ) > 0, Apply(M [x]) = hu1 [x1 ], . . . , un [xn ]i sequence operators automatoncalls, implying Exp(M [x]) = Exp(u1 [x1 ]); ; Exp(un [xn ]). 1 k n, uk|Exp(uk [xk ])| = 1, else |Exp(uk [xk ])| (S Ar)h(M ) hypothesis induction sinceh(uk ) < h(M ). follows |Exp(M [x])| (S Ar)1+h(M ) since n Ar.automaton plan tuple = h, A, Au, r, xi h, A, Au, ri automaton hierarchy x ar(r) input string root automaton r. automaton hierarchies representfamilies plans, automaton plan represents unique plan given = Exp(r[x]) A+.subsequent sections exploit notion uniform expansion, defined follows:Definition 7. automaton hierarchy h, A, Au, ri uniform expansionautomaton Au exists number |Exp(M [x])| = x ar(M ) .words, expanding automaton call [x] always results operator sequence lengthexactly , regardless input x.264fiAUTOMATON P LANS/1/M3 [1]0/M3 [c]1/M2 [b]/1/M3 [1]0/a1 [0]/1/a1 [1]0/a2 [1]0/M2 [1]M1 [abc]M2 [a]M3 [a]Figure 2: three automata simple example.4. Examples Automaton Planssection present several examples automaton plans. aim first foremostillustrate concept automaton plans. However, also use examples illuminateseveral interesting properties automaton plans. example, use small automaton plansrepresent exponentially long plans.begin showing example relatively simple automaton plan two symbols, twoactions, three automata, defined = h{0, 1}, {a1 , a2 }, {M1 , M2 , M3 }, M1 , 100i. Actions a1a2 arity 1, Figure 2 shows three automata M1 , M2 , M3 (with arity 3, 1,1, respectively). figure, edge without origin points initial state automaton,label edge contains name input string automaton.simplify description index strings argument maps assign explicit names (a,b, c) symbol input string automaton. argument map describedstring input symbol names symbols = {0, 1}. example, label M2 [b]automaton M1 corresponds output symbol (M2 , 2), i.e. index string M1 M2assigns second input symbol (b) M1 lone input symbol M2 . Recall symbolsinput string two separate functions: decide edges automaton transitionalong, propagate information copying symbols onto actions automata.plan represented given= Exp(M1 [100]) = Exp(Apply(M1 [100])) = Exp(hM2 [0], M3 [0], M2 [1]i) == Exp(M2 [0]); Exp(M3 [0]); Exp(M2 [1]) == Exp(Apply(M2 [0])); Exp(Apply(M3 [0])); Exp(Apply(M2 [1])) == Exp(ha1 [0]i); Exp(ha2 [1]i); Exp(hM3 [1]i) = Exp(a1 [0]); Exp(a2 [1]); Exp(M3 [1]) == ha1 [0]i; ha2 [1]i; Exp(Apply(M3 [1])) = ha1 [0]i; ha2 [1]i; Exp(ha1 [1]i) == ha1 [0]i; ha2 [1]i; Exp(a1 [1]) = ha1 [0]i; ha2 [1]i; ha1 [1]i = ha1 [0], a2 [1], a1 [1]i.Selecting another root automaton call would result different operator sequence. example,root M1 [000] would result sequence Exp(M1 [000]) = ha1 [1]i, root M1 [101] wouldresult Exp(M1 [101]) = ha1 [0], a1 [1], a1 [0]i.next show like macro plans, automaton plans compactly represent plansexponentially long. Figure 3 shows automaton Mn moving n discs peg pegb via peg c Towers Hanoi. figure, [ab] action moving disc nb. n = 1 edge label /hA1 [ab]i. hard show automaton plan= h{1, 2, 3}, {A1 , . . . , }, {M1 , . . . , MN }, MN , 132i plan Towers Hanoi instance265fiB ACKSTR OM , J ONSSON , & J ONSSON/hMn1 [acb], [ab], Mn1 [cba]i/Mn [abc]Figure 3: automaton Mn automaton plan Towers Hanoi.li = qi //A[zqf qt ap]qt = lt //D[li qi qt lt ci ct ti xtt yazp]li 6= qi /T[xli qi ci ti p]x = y/qt 6= lt /T[yqt lt ct tt p]/hLT[tpy], DT[tyzc], UT[tpz]i/T[xyzctp]x 6= y/DT[txyc]x = y//hLA[apy], FA[ayz], UA[apz]i/A[xyzap]x 6= y/FA[axy]Figure 4: automata delivering package T, moving package usingtruck/airplane.N discs. Unlike macro solutions Towers Hanoi (Jonsson, 2009), automaton plansingle automaton per disc, possible parameterization.ability parameterize automata also makes possible represent types planscompactly. Figure 4 shows three automata D, T, combined constructautomaton plan instance L OGISTICS domain. set symbols containsobjects instance: packages, airplanes, trucks, cities, locations. automaton movespackage using truck, input string xyzctp consists three locations x, y, z, city c,truck t, package p. Initially, truck x, package p y, destination package pz. actions DT, LT, UT stand DriveTruck, LoadTruck, UnloadTruck, respectively.Automaton assumes locations z different, else nothing doneautomaton outputs empty string, violating definition automaton plans.hand, locations x may same, automaton checks whether equal.x different necessary first drive truck x y. use notationx = x 6= shorthand denote |L| intermediate notes, one per location,automaton transitions distinct intermediate node assignment location x.intermediate node |L| edges next node: |L| 1 edges correspondx 6= one edge corresponds x = y.truck location y, operator sequence output loads package p t, drivesdestination, unloads p t. Automaton moving package using airplanesimilarly defined actions FA (FlyAirplane), LA (LoadAirplane), UA (UnloadAirplane).Automaton delivers package current location destination, input stringli qi qt lt ci ct ti xtt yazp consists initial location li package, intermediate airports qi266fiAUTOMATON P LANS/hU[rk1 l1 ], . . . , U[ln1 kn ln ], D[ln k2 d1 ], . . . , D[dn1 k2 dn ]iM[rk1 l1 d1 kn ln dn ]U[l1 l2 l3 ]/hN[l1 l2 ], pickup-and-loose[l2 ], N[l2 l3 ], unlock[l3 ]i/hN[l1 l2 ], pickup[l2 ], N[l2 l3 ], putdown[l3 ]i///D[l1 l2 l3 ]Figure 5: automata automaton plan G RID domain.qt , target location lt , initial target cities ci ct , truck ti city ci initially x,truck tt city ct initially y, airplane initially z, package p itself. Automatonassumes cities ci ct different, else could use automaton transport packageusing truck within city. However, locations li qi may equal, well qt lt ,automaton moves package using truck whenever necessary.also show example automaton plan G RID domain, robot deliverkeys locations, locked. keys distributed initial locations,robot carry one key time. actions move robot neighboring location,pick key (possibly loosing key currently held), put key, unlock location.Figure 5 shows automaton plan instance G RID. root automaton takesinput current location robot (r) three locations ki , li , di key, kicurrent location key, li associated locked location, di destination.plan works first unlocking locations prespecified order (which must exist problemsolvable) delivering keys destination.automaton U takes three locations: location robot (l1 ), location key(l2 ), location unlocked (l3 ). decomposition navigates key, picks up,navigates location, unlocks it. Delivering key works similar way. simplicityparameters actions omitted, modifications necessary: first timeU applied key loose, first time applied key loose.automaton N (not shown) navigates pairs locations l1 l2 . Since automatonplans cannot keep track state, N include one automaton state possible input(l1 , l2 ) (alternatively define separate automaton destination l2 ). Noteautomaton used represent solutions different instances set locations.5. Relationship Compact Plan Representationssection compare contrast automaton plans compact plan representationsliterature: macro plans, HTNs (Erol et al., 1996), well C RARs C SARs (Backstrom &Jonsson, 2012). Informally, C RARs C SARs theoretical concepts describing compactrepresentation plan admits efficient random access (C RAR) sequential access (C SAR)operators plan. compare plan representations use following subsumptionrelation (Backstrom et al., 2012b):267fiB ACKSTR OM , J ONSSON , & J ONSSONppACRC RARC SARp6ppppAUTRUEAUTRpHTNFigure 6: Summary subsumption results, dashed edges marking previously known results.Definition 8. Let X two plan representations. least expressive X,denote X p , polynomial-time function g TRIPS planninginstance p plan p, X representation , g() representation .Figure 6 summarizes subsumption results different plan representations consideredpaper. ACR AUTR refer macro plans automaton plans, respectively,AUTRUE refer automaton plans uniform expansion. Previously known results shownusing dashed edges; remaining results proven section. use notation X pindicate X p 6p X. figure see automaton plans strictlyexpressive macro plans, strictly less expressive C SARs HTNs. caseC RARs, prove partial result: automaton plans uniform expansiontranslated C RARs.rest section, first show automaton plans uniform expansionefficiently transformed C RARs. prove plan verification automaton plans p2 complete. use latter result prove separation macro plans automaton plans,automaton plans C SARs/HTNs. proving X p holds two representations X , assume size X representation polynomial ||p||,description size planning instance. Trivially, automaton plans uniform expansionalso automaton plans, unknown whether general automaton plans efficiently transformed uniform expansion.5.1 Automaton Plans C RARssection show automaton plans uniform expansion efficiently translatedC RARs, i.e. compact plan representations admit efficient random access. first define C RARsdescribe algorithm transforming automaton plan uniform expansioncorresponding C RAR.Definition 9. Let p polynomial function. Given TRIPS planning instance p = hP, A, , I, Giassociated plan , p-C RAR representation |||| p(||p||) outputsk-th element time space p(||p||) 1 k ||.Note p-C RAR, polynomial function p independent planninginstance p, else always find constant individual instance p sizerepresentation p bounded constant.268fiAUTOMATON P LANS12345678function Find(k, u[x])u[x] return u[x]elsehu1 [x1 ], . . . , un [xn ]i Apply(u[x])0, j 1+ (uj ) < k+ (uj ), j j + 1return Find(k s, uj [xj ])Figure 7: Algorithm using automaton plan C RAR.Theorem 1. AUTRUE p C RAR.Proof. prove theorem show TRIPS planning instance p automatonplan = h, A, Au, r, xi uniform expansion representing plan p, efficientlyconstruct corresponding p-C RAR p(||p||) = (Ar + (log + log Ar)|Au|) Ar |Au|.Since uniform expansion exist numbers , Au, |Exp(M [x])| =x ar(M ) . numbers computed bottom follows. Traverseautomata expansion graph GAu reverse topological order. Au, pick inputstring x ar(M ) random compute Apply(M [x]) = hu1 [y1 ], . . . , un [yn ]i. numbergiven = u1 + . . . + un where, 1 k n, uk = 1 uk uk alreadycomputed uk Au since definition, uk comes topological ordering.Lemma 3, total complexity computing Apply(M [x]) AuAr2 |Au|. Due Lemma 6, (S Ar)1+h(M ) (S Ar)|Au| Au. Since(S Ar)|Au| = 2(log S+log Ar)|Au| , need (log + log Ar)|Au| bits represent ,computing requires (log +log Ar)S Ar|Au| operations. Repeating computationAu gives us complexity bound (Ar + (log + log Ar)|Au|) Ar |Au|.prove recursive algorithm Find Figure 7 following properties, inductionnumber recursive calls:1. [x] Au Exp(M [x]) = ha1 [x1 ], . . . , [xn ]i, Find(k, [x]) returnsoperator ak [xk ] 1 k n,2. a[x] , Find(k, a[x]) returns a[x].Basis: Find(k, u[x]) call recursively, u[x] must operator. definition, Exp(u[x]) = u[x] since u[x] .Induction step: Suppose claim holds Find makes recursive calls0. Assume Find(k, u[x]) makes m+1 recursive calls. Let hu1 [x1 ], . . . , un [xn ]i = Apply(u[x])and, 1 k n, (uk ) = 1 uk [xk ] (uk ) = uk uk [xk ] Au . Lines 57computes j either1. j = 1, = 0 k (u1 )2. j > 1, = (u1 ) + . . . + (uj1 ) < k (u1 ) + . . . + (uj ).definition, Exp(u[x]) = Exp(u1 [x1 ]); . . . ; Exp(un [xn ]), implying operator k Exp(u[x])operator k Exp(uj [xj ]). follows induction hypothesis recursive callFind(k s, uj [xj ]) returns operator.269fiB ACKSTR OM , J ONSSON , & J ONSSONprove complexity Find, note Find calls recursivelyAu since GAu acyclic. Moreover, complexity computing Apply(M [x]) boundedAr2 , loop lines 67 runs n Ar times, time performing(log + log Ar)|Au| operations update value s. thus showedautomaton plan together procedure Find values , Au, constitute pC RAR p(||p||) = (Ar + (log + log Ar)|Au|) Ar |Au|.5.2 Verification Automaton Planssection show problem plan verification automaton plans p2 -complete.first prove membership reducing plan verification automaton plans plan verificationC RARs, known p2 -complete (Backstrom et al., 2012b). prove hardnessreducing -SAT, also p2 -complete, plan verification automaton plans uniformexpansion. complexity result plan verification later used separate automaton plansmacro plans, C SARs, HTNs, obtain similar separation result automatonplans C RARs since complexity plan verification same.prove membership first define alternative expansion function Exp pads original plan dummy operators expansion automaton lengthaccepting input. Intuitively, even though original automaton plan need uniform expansion, alternative expansion function Exp emulates automaton plan does. Notesufficient prove transform automaton plan p-C RAR, since operatorsdifferent indices plans represented two expansion functions Exp Exp .Let p planning instance, let = h, A, Au, r, xi automaton plan representingsolution p. automaton Au, let IM = (S Ar)1+h(M ) upper bound|Exp(M [x])| Lemma 6. Let = h, parameter-free dummy operator empty preand postcondition, add . Define k , k > 0, sequence containing k copies .define alternative expansion function Exp (A Au )+ follows:1. Exp (a[x]) = ha[x]i a[x] ,2. Exp (M [x]) = Exp (Apply(M [x])); L [x] Au , length L L =IM |Exp (Apply(M [x]))|,3. Exp (hu1 [y1 ], . . . , un [yn ]i) = Exp (u1 [y1 ]); . . . ; Exp (un [yn ]).difference respect original expansion function Exp alternative expansion function Exp appends sequence L dummy operators result Exp (Apply(M [x])),causing Exp (M [x]) length exactly IM .following lemma prove operator sequence output alternative expansionfunction Exp equivalent operator sequence output original expansion function Exp.Lemma 10. automaton call [x] Au , Exp (M [x]) AIM , applying Exp(M [x])Exp (M [x]) state either possible results state.Proof. prove lemma induction |Au|. base case given |Au| = 1.case, since GAu acyclic, Apply(M [x]) sequence operators ha1 [x1 ], . . . , [xn ]i A+,270fiAUTOMATON P LANSimplyingExp(M [x]) = ha1 [x1 ], . . . , [xn ]i,Exp (M [x]) = ha1 [x1 ], . . . , [xn ]i; L ,L = IM n. Thus Exp (M [x]) AIM , applying Exp (M [x]) stateeffect applying Exp(M [x]) since dummy operator always applicable effect.next prove inductive step |Au| > 1. case, Apply(M [x]) sequence operatorsautomaton calls hu1 [y1 ], . . . , un [yn ]i (A Au )+ , implyingExp(M [x]) = Exp(u1 [x1 ]); ; Exp(un [xn ]),Exp (M [x]) = Exp (u1 [x1 ]); ; Exp (un [xn ]); L ,L contains enough copies make |Exp (M [x])| = IM . 1 k n, uk [xk ]operator Exp (uk [xk ]) = Exp(uk [xk ]) = huk [xk ]i, clearly identicaleffects. hand, uk [xk ] automaton call, since GAu acyclic uk [xk ](Au \ {M }) , implying Exp (uk [xk ]) effect Exp(uk [xk ]) hypothesisinduction since |Au \ {M }| < |Au|. Thus Exp (M [x]) AIM , applying Exp (M [x])state effect applying Exp(M [x]) since dummy operator always applicableeffect.ready prove membership p2 . Lemma 10, instead verifyingplan Exp(r[x]), verify plan Exp (r[x]) given alternative expansion function Exp .Lemma 11. Plan verification AUTR p2 .Proof. prove lemma reducing plan verification automaton plans plan verificationC RARs, known p2 -complete (Backstrom et al., 2012b). Consider automatonplan = h, A, Au, r, xi associated TRIPS planning instance p. Instead constructingp-C RAR operator sequence Exp(r[x]) represented , construct p-C RARoperator sequence Exp (r[x]). Due Lemma 10, Exp(r[x]) plan p Exp (r[x])plan p.p-C RAR Exp (r[x]) constructed modifying algorithm Find Figure 7.Instead using numbers associated automaton plan uniform expansion,use upper bounds IM length operator sequence output automaton.modification need make add condition j n loop,loop terminates j = n + 1, return , since means Iu1 + + Iun < k Iu .complexity resulting p-C RAR identical proof Theorem 1 sincenumbers IM within bounds used proof, i.e. IM (S Ar)|Au| Au.example, consider automaton plan = h{0, 1}, {a1 , a2 }, {M1 , M2 , M3 }, M1 , 100iM1 , M2 , M3 defined Figure 2. Applying definitions obtain = 3, Ar = 4,h(M1 ) = 2, h(M2 ) = 1, h(M3 ) = 0, yieldsIM1 = (S Ar)1+h(M1 ) = 123 = 1728,IM2 = (S Ar)1+h(M2 ) = 122 = 144,IM3 = (S Ar)1+h(M3 ) = 121 = 12.271fiB ACKSTR OM , J ONSSON , & J ONSSONAlthough IM1 = 1728 gross overestimate length operator sequence outputM1 , number bits needed represent IM1 polynomial ||||. Applying alternativeexpansion function Exp yields Exp (M1 [100]) = ha1 [0]i; 143 ; ha2 [1]i; 11 ; ha1 [1]i; 1571 .prove p2 -completeness, remains show plan verification automaton plansp2 -hard. proof following lemma quite lengthy, defer Appendix A.Lemma 12. Plan verification AUTRUE p2 -hard.main theorem section follows immediately Lemmas 11 12.Theorem 2. Plan verification AUTRUE AUTR p2 -complete.Proof. Since AUTRUE AUTR, Lemma 11 implies plan verification AUTRUE p2 ,Lemma 12 implies plan verification AUTR p2 -hard. Thus plan verificationAUTRUE AUTR p2 -complete.5.3 Automaton Plans Macro Planssection show automaton plans strictly expressive macros. this,first define macro plans show macro plan trivially converted equivalentautomaton plan uniform expansion. show automaton plans cannotefficiently translated macro plans.macro plan = hM, mr TRIPS instance p consists set macros rootmacro mr M. macro consists sequence = hu1 , . . . , un where,1 k n, uk either operator another macro M. expansionsequence operators obtained recursively replacing macro hu1 , . . . , unexpansion. process well-defined long macro appears expansion. planrepresented given expansion root macro mr .Lemma 13. ACR p AUTRUE.Proof. prove lemma show exists polynomial p TRIPSplanning instance p macro plan representing solution p, exists automatonplan uniform expansion |||| = O(p(||||)).macro parameter-free sequence = hu1 , . . . , ul operators macros.construct automaton plan replacing macro automaton Mmar(Mm ) = 0. automaton Mm two states s0 s, two edges: one s0label /h(w1 , 1 ), . . . , (wl , l )i, one label /. 1 j l,uj operator, wj associated action index string j ar(uj ) containsarguments uj sequence m, explicitly stated since parameter-free.uj macro, wj = Muj j = since ar(Mm ) = ar(Muj ) = 0. root givenr = Mmr [], mr root macro .show induction that, macro = hu1 , . . . , ul , Exp(Mm []) equalsexpansion m. base case given |Au| = 1. sequence operatorsA+, Apply(Mm []) returns m, implying Exp(Mm []) = Apply(Mm []) = m. |Au| > 1,Apply(Mm []) contains operators m, macro uj m, 1 j l,replaced automaton call Muj []. hypothesis induction, Exp(Muj []) equalsexpansion uj . Exp(Mm []) equals expansion since concatenations272fiAUTOMATON P LANSidentical sequences. easy see size automaton Mm polynomial m.shown macro plan transformed equivalent automaton plan whosesize polynomial ||||, implying existence polynomial p |||| = O(p(||||)).automaton plan trivially uniform expansion since automaton always calledempty input string.next show automaton plans uniform expansion strictly expressivemacro plans.Theorem 3. ACR p AUTRUE unless P = p2 .Proof. Due Lemma 13 remains show AUTRUE 6p ACR, i.e. cannot efficientlytranslate arbitrary automaton plans uniform expansion equivalent macro plans. Backstromet al. (2012b) showed plan verification macro plans P. Assume existspolynomial-time algorithm translates automaton plan uniform expansion equivalent macro plan. could verify automaton plans uniform expansion polynomialtime, first applying given algorithm produce equivalent macro plan verifyingmacro plan polynomial time. However, due Theorem 2, algorithm exist unlessP = p2 .5.4 Automaton Plans C SARssection show automaton plans strictly less expressive C SARs, definedfollows:Definition 14. Let p polynomial function. Given TRIPS planning instance p = hP, A, , I, Giassociated plan , p-C SAR representation |||| p(||p||) outputselements sequentially, time needed output element bounded p(||p||).p-C RARs, polynomial function p p-C SAR independent planninginstance p. first show automaton plan transformed equivalent p-C SARpolynomial time. show p-C SARs cannot efficiently translatedautomaton plans.Lemma 15. AUTR p C SAR.Proof. prove lemma show TRIPS planning instance p automatonplan representing solution p, efficiently construct corresponding p-C SARp(||p||) = Ar2 |Au|. claim algorithm Next Figure 8 always outputs nextoperator polynomial time. algorithm maintains following global variables:call stack = [M1 [x1 ], . . . , Mk [xk ]] M1 [x1 ] = r[x] root and,1 < k, Mi [xi ] automaton call appears Apply(Mi1 [xi1 ]).integer k representing current number elements S.1 k, sequence stores result Apply(Mi [xi ]).1 k, integer zi index .273fiB ACKSTR OM , J ONSSON , & J ONSSON1 function Next()2zk = |k |3k = 1 return4else5pop Mk [xk ]6k k17repeat8zk zk + 19u[x] k [zk ]10u[x] return u[x]11else12push u[x] onto13k k+114k Apply(u[x])15zk 0Figure 8: Algorithm finding next operator automaton plan.Prior first call Next, global variables initialized = [r[x]], k = 1, 1 =Apply(r[x]), z1 = 0.algorithm Next works follows. long elements Apply(Mk [xk ]),automaton call Mk [xk ] popped stack k decremented. If, result, k = 1Apply(M1 [x1 ]) contains elements, Next returns , correctly indicating planoperators.found automaton call Mk [xk ] stack Apply(Mk [xk ]) containselements, increment zk retrieve element u[x] index zk k . u[x] , u[x]next operator plan therefore returned Next. Otherwise u[x] pushed ontostack S, k incremented, k set Apply(u[x]), zk initialized 0, process repeatednew automaton call Mk [xk ] = u[x].Since expansion graph GAu acyclic, number elements k stack bounded|Au|. Thus complexity loop bounded |Au| since operations loopconstant complexity. Since Exp(M1 [x1 ]) = Exp(r[x]) A+, repeat loop guaranteed findk zk u[x] = k [zk ] operator, proving correctness algorithm.operation repeat loop constant complexity Apply(u[x]); Lemma 3know complexity bounded Ar2 , might repeat operation|Au| times. space required store global variables bounded Ar |Au|.shown global variables together algorithm Next constitute p-C SARp(||p||) = O(S Ar2 |Au|).next show automaton plans strictly less expressive p-C SARs. Let P1subclass TRIPS planning instances one operator applicable reachablestate. following lemma due Bylander (1994):Lemma 16. Plan existence P1 PSPACE-hard.274fiAUTOMATON P LANSProof. Bylander presented polynomial-time reduction polynomial-space deterministic Turing machine (DTM) acceptance, PSPACE-complete problem, TRIPS plan existence. GivenDTM, planning instance p constructed Bylander belongs P1 solutionDTM accepts.Theorem 4. AUTR p C SAR unless PSPACE = p3 .Proof. Due Lemma 15 remains show C SAR 6p AUTR. first show plan verification C SARs PSPACE-hard. Given planning instance p P1 , let unique planobtained always selecting applicable operator, starting initial state. Withoutloss generality, assume operators applicable goal state. Hence either solvesp, terminates dead-end state, enters cycle. trivial construct p-C SAR :state, loop operators select one whose precondition satisfied. Critically,construction independent . Due Lemma 16, PSPACE-hard determine whetherp solution, i.e. whether plan represented solves p.hand, assume exists automaton plan |||| = O(p(||p||))pfixed polynomial p. solve plan existence p NP2 = p non3deterministically guessing automaton plan verifying represents solution p.implies PSPACE = p3 .5.5 Automaton Plans HTNssection show automaton plans strictly less expressive HTNs. begindefining class HTNs compare automaton plans. showefficiently transform automaton plans HTNs, way around.like planning instances, HTN involves set fluents, set operators, initialstate. Unlike planning instances, however, aim reach goal state, aimHTN produce sequence operators perform given set tasks. taskone associated methods specify decompose task subtasks,either operators tasks. Planning proceeds recursively decomposing task usingassociated method primitive operators remain. planning, HTN keeptrack current state, operators methods applicable preconditionssatisfied current state.general, solution HTN unique: may one applicable methoddecomposing task, method may allow subtasks decomposition appeardifferent order. contrast, subsumption relation p defined compact representationsunique solutions. reason, consider restricted class HTNs methodsassociated task mutually exclusive subtasks decomposition methodtotally ordered. class HTNs indeed unique solutions, since taskdecomposed one way. Since class HTNs strict subclass HTNs general,results hold general HTNs remove requirement uniqueness solution.definition HTNs largely based SHOP2 (Nau, Ilghami, Kuter, Murdock, Wu, &Yaman, 2003), state-of-the-art algorithm solving HTNs. formally define HTN domain tuple H = hP, A, T, hP, Ai planning domain, set function symbols called tasks, set function symbols called methods. methodform ht, pre(), i, associated task, pre() precondition, =275fiB ACKSTR OM , J ONSSON , & J ONSSONh(t1 , 1 ), . . . , (tk , k )i task list where, 1 k, ti action taskargument map ti . arity satisfies ar() ar(t), argumentsalways copied onto . ar() > ar(t), arguments indices ar(t) + 1, . . . , ar()free parameters take value. precondition pre() formprecondition action A, i.e. pre() = {(p1 , 1 , b1 ), . . . , (pl , l , bl )} where,1 j l, pl P predicate, l argument map pl , bl Boolean. taskmay multiple associated methods.HTN instance tuple h = hP, A, T, , , I, Li hP, A, T, HTN domain,set objects, P initial state, L task list. HTN instance implicitly defines setgrounded tasks set grounded methods , task list L T+ sequencegrounded tasks. precondition pre([xy]) grounded method [xy] , xparameters copied assignment free parameters , derived pre()way precondition pre(a[x]) operator a[x] derived pre(a).Unlike TRIPS planning, aim HTN instance recursively expand groundedtask t[x] L applying associated grounded method [xy] primitive operatorsremain. grounded method [xy] applicable precondition pre([xy]) satisfied,applying [xy] replaces task t[x] sequence grounded operators tasks obtainedapplying sequence argument maps [xy]. problem plan existence HTNsdetermine expansion possible.Lemma 17. AUTR p HTN.proof Lemma 17 appears Appendix B. Intuitively, idea construct HTNtasks associated states graphs automata, methods edgesgraphs. HTN emulates execution model : grounded task correspondsautomaton , current state graph , input string x ar(M ) , index kx. associated grounded methods indicate possible ways transition anotherstate. Given edge label /u, corresponding method applicable xk = ,applying method recursively applies operators tasks sequence u, followedtask associated next state, incrementing k necessary.Theorem 5. AUTR p HTN unless PSPACE = p3 .Proof. Due Lemma 17 remains show HTN 6p AUTR. Erol et al. (1996) showedproblem plan existence propositional HTNs totally ordered task lists PSPACE-hard.proof reduction propositional TRIPS planning, number applicablemethods task equals number applicable TRIPS operators original planninginstance, general larger one. However, due Lemma 16, insteadreduce class P1 , resulting HTNs one applicable method task.exists polynomial-time algorithm translates HTNs equivalent automaton plans,psolve plan existence HTNs NP2 = p non-deterministically guessing automa3ton plan verifying automaton plan solution. implies PSPACE = p3 .reasoning used proof Theorem 5 also used show HTN 6p C RAR,implying random access bounded away polynomial HTNs. However, unknownwhether C RARs efficiently translated HTNs.276fiAUTOMATON P LANS123456function Lowest(hP, A, , I, Gi)sIG 6{oi : oi applicable s}mini oi(s \ pre(om ) ) pre(om )+Figure 9: Algorithm always selects applicable operator lowest index.One important difference automaton plans HTNs latter keeps trackstate. conjecture state-based compact representations hard verifygeneral. Consider algorithm Figure 9 always selects applicable operatorlowest index. algorithm compact representation well-defined operator sequence. Planverification compact representation PSPACE-hard due Lemma 16, since planninginstances P1 , algorithm always choose applicable operator. Arguably,algorithm simplest state-based compact representation one think of, plan verificationstill harder automaton plans.6. Related Workthree main sources inspiration automaton plans macro planning, finite-state automata,string compression. Below, briefly discuss three topics connectionsautomaton plans.6.1 Macro Planningconnection macros automaton plans clear point: basic mechanism automaton plans recursively defining plans direct generalization macros.context automated planning, macros first proposed Fikes et al. (1972) tool planexecution analysis. idea immediately become widespread even thoughused planners, mainly viewed interesting idea obvious applications.Despite this, advantageous ways exploiting macros identified by, instance, MintonKorf: Minton (1985) proposed storing useful macros adding set operators orderspeed search Korf (1987) showed search space macros exponentiallysmaller search space original planning operators.last decade, popularity macros increased significantly. is, example, witnessed fact several planners exploit macros participated International Planning Competition. ARVIN (Coles & Smith, 2007) generates macros onlineescape search plateaus, offline reduced version planning instance. ACRO -FF(Botea, Enzenberger, Muller, & Schaeffer, 2005) extracts macros domain descriptionwell solutions previous instances solved. W IZARD (Newton, Levine, Fox, & Long, 2007) usesgenetic algorithm generate macros. Researchers also studied macros influencecomputational complexity solving different classes planning instances. Gimenez Jonsson(2008) showed plan generation provably polynomial class 3S planning instancessolution expressed using macros. Jonsson (2009) presented similar algorithm277fiB ACKSTR OM , J ONSSON , & J ONSSONoptimally solving subclass planning instances tree-reducible causal graphs. cases,planning instances respective class exponentially long optimal solutions, makingimpossible generate solution polynomial time without use macros.6.2 Finite State Automatastarted working new plan representations, soon become evident automataconvenient way organizing computations needed inside compactly represented plan.thought particularly original since automata automaton-like representations quitecommon planning. order avoid confusion, want emphasize automaton hierarchies equivalent concept hierarchical automata. term hierarchical automataused literature somewhat loose collective term large number different approachesautomaton-based hierarchical modelling systems; notable examples found controltheory (Zhong & Wonham, 1990) model checking (Alur & Yannakakis, 1998).many examples solutions planning problems represented automataexamples are, unlike automaton plans, typically context non-deterministic planning.Cimatti, Roveri, Traverso (1998) presented algorithm that, successful, returns strongcyclic solutions non-deterministic planning instances. Winner Veloso (2003) used exampleslearn generalized plans. Bonet, Palacios, Geffner (2010) used transformation classicalplanning generate finite-state controllers non-deterministic planning. Finally, Hu DeGiacomo (2013) presented general algorithm synthesizing finite state controllers basedbehavior specification domain.Automata also used representing objects planning. Hickmott, Rintanen,Thiebaux, White (2007) LaValle (2006) used automata represent entire planning instance. contrast, Toropila Bartak (2010) used automata represent domains individualvariables instance. Baier McIlraith (2006) showed convert LTL representationtemporally extended goals, i.e. conditions must hold intermediate states plan,non-deterministic finite automaton.6.3 String Compressionideas behind automaton plans macro plans closely related string compression.algorithms string compression variants pioneering work Lempel Ziv (1976).Normally, compressed representation string straight line program (SLP)context free grammar generate one single string. One might say preciselyhierarchical macro plan is. Although widely used, also attemptsuse automaton representations strings order achieve compact representations (cf., seeZipstein, 1992). One might say approaches generalize SLPs way similarway automaton plans generalize macro plans. important note string compressionalgorithms per se limited interest comes representing plans. basic reasoncomplete plan first needs generated compressed compression algorithm,excludes utilization compact plans planning process. instance, previouslymentioned polynomial-time macro planning algorithms (Gimenez & Jonsson, 2008; Jonsson, 2009)cannot replaced (with preserved computational properties) planner combined stringcompression algorithm since planner may need produce exponentially long plan.278fiAUTOMATON P LANSString compression usually makes assumptions content string represent. makes methods general although often optimal particular application.examples, though, specialised representations. instance, SubramanianShankar (2005) present method compressing XML documents using automatabased XML syntax. automaton plans, like macro plans, make particularassumptions either sequence (or string) represented. However, evidentexamples primarily intend automata plan representationfunctional correspondence plan structure.7. Discussionintroduced novel concept automaton plans, i.e. plans represented hierarchiesfinite state automata. Automaton plans extend macro plans allowing parameterizationbranching, used represent solutions variety planning instances. showedautomaton plans strictly expressive macro plans, strictly less expressiveHTNs C SARs, i.e. compact representations allowing polynomial-time sequential access.precise relationship automaton plans C RARs still open question,presented partial result: automaton plans uniform expansion transformedC RARs.definition automaton plans restricted TRIPS planning,possible extension would consider general planning formalisms. describeextension would affect complexity results Section 5. transformationsvalid string, hence independent planning formalism. particular, macroplans always translated automaton plans uniform expansion, latter alwaystransformed C RARs, automaton plans always transformed C SARs. However,aware HTN formalism extends action definition allow complexactions. Hence transformation automaton plans HTNs involve actions TRIPSstyle preconditions effects. separation results TRIPS also carry generalplanning formalisms since cannot exist polynomial function translating instances.Although mainly studied computational properties automaton plans contextcompact plan representation, believe automaton plans may find uses. Probablyinteresting question practical perspective construct automaton plans.definite answer question, least two ideas believe worthexploring. One possible way generate automaton plans first construct HTN givendomain, use HTN solve instances domain. Instead flattening solutiontypically done, idea would keep hierarchical structure transformautomaton plan. matter HTN representation consider long verifywhether solution instance valid; solution verified transformationautomaton plan might become tractable, least practical cases. approach would likelyrequire sophisticated techniques generating HTNs currently available, unlessHTN already provided.Another interesting extension automaton plans allow recursive automata includecalls themselves. Consider automaton Mn Section 4 moving n discs TowersHanoi. introduced symbols j1 , . . . , jN representing number discs, could definesingle recursive automaton includes fourth parameter jn representing number discs279fiB ACKSTR OM , J ONSSON , & J ONSSONGFigure 10: example contingent planning instance.moved. recursive calls input jn would include symbol jn1 , effectivelydecrementing number discs. recursive mechanism work would need existbase case recursive calls made (in case Towers Hanoi, basecase typically consists moving 0 1 discs). remark computational propertiesautomaton plans paper would longer apply since expansion graph would longeracyclic.Finally, modified automaton plans could used represent contingent plans compactly. Solutions contingent planning instances typically form directed graphsnode belief state, i.e. subset states. Edges correspond actions applicablebelief state, well observations current state. outcome observation single bit indicating whether current value given fluent true false. Since outcomeobservation uncertain, observation splits belief state one belief state containing statesfluent true, one containing states fluent false. solutionrepresents policy, indicating action applied belief state.Figure 10 shows example contingent planning instance. location described(x, y)-coordinate. location arrow indicating way go, flagindicating whether reached target destination G. Two fluents sufficient representdirection arrow pointing:00:01:10: left11: rightfour actions U, D, L, R moving up, down, left, right, respectively. (x, y)coordinate observable, initial state unknown. state observewhether reached destination direction arrow pointing.Figure 11 shows automaton represents solution example contingent planninginstance. set symbols = {0, 1}, i.e. symbols outcomes observationsused branch edges automaton. state make three consecutive observations:whether reached goal (0 1), direction arrow pointing (twobits). reach goal move state simply consume remaining observations(if any). not, use direction arrow decide action apply next.automaton solution independent size contingent planning instance longarrows point right direction. contrast, number belief states doubly exponential|P |number fluents, i.e. 22 . add locations example contingent planning problem,number belief states increases exponentially, size solution form280fiAUTOMATON P LANS0/1/D1/0/0/L1/R0/U1//Figure 11: automaton representing contingent plan.directed graph. Although example automaton plan hierarchical, difficultimagine navigation subtask larger problem, case could call automatonautomata.Although semantics contingent planning different classical planning,automata Figure 11 used construct perfectly valid automaton plan. However,definition imposes two restrictions contingent plans. First, since automata automaton plansfixed arity, represent contingent plans constant number observations.Second, entire sequence observations passed input automaton beforehand.may therefore make sense consider relaxation definition: allowing input passedautomaton online, thus allowing arity automaton vary.Acknowledgmentsauthors would like thank anonymous reviewers helpful comments suggestions.Appendix A. Proof Lemma 12.section prove Lemma 12, states plan verification automaton plansuniform expansion p2 -hard. prove lemma reduction -SAT plan verificationautomaton plans uniform expansion. proof proceeds three steps. first showconstruct TRIPS planning instance pF given -SAT formula F . proveexists operator sequence F pF unique solution F Fsatisfiable. Finally, construct automaton plan F uniform expansion representssequence F , i.e. F represents valid plan pF F satisfiable.Construction 18. Let F = x1 xm y1 yn -SAT formula 3SATformula, let LF = {1 , . . . , 2(m+n) } set literals, 2i1 = xi 2i = xi1 2(m+j)1 = yj 2(m+j) = yj 1 j n. Also definetotal order < LF < j < j. formula = (c1 ch )conjunction 3-literal clauses ck = 1k 2k 3k 1k , 2k , 3k LF . Assume without lossgenerality 1k 2k 3k .Given formula F , construct TRIPS planning instance pF = hPF , AF , F , , GFPF = {f x, f y, f s, sat, x1 , . . . , xm , y1 , . . . , yn , v0 , . . . , vh }, F = {0, 1}, = , GF =281fiB ACKSTR OM , J ONSSON , & J ONSSON{f x, sat, x1 , . . . , xm }, AF contains following operators, described formpre(a) post(a):os :olk1 :olk2 :olk3 :onk :ot ::oyj :od :oxi :{f x, f y, v0 } {v0 , f s}{vk1 , vk , 1k } {vk }{vk1 , vk , 1k , 2k } {vk }{vk1 , vk , 1k , 2k , 3k } {vk }{vk1 , vk , 1k , 2k , 3k } {vk , f s}{vh , f s} {f y, v0 , . . . , vh , sat}{vh , f s} {f y, v0 , . . . , vh }{f y, yj , yj+1 , . . . , yn } {f y, yj , yj+1 , . . . , yn }{f y, y1 , . . . , yn } {f x, f y, y1 , . . . , yn }{f x, sat, xi , xi+1 , . . . , xm } {f x, sat, xi , xi+1 , . . . , xm }explain intuition behind planning instance pF . First note predicates actionsparameter-free, set fluents equals set predicates set operators equalsset actions. function map thus necessary describe pre- postconditionoperator. indices used operators ranges 1 m, 1 j n, 1 k h.plan pF takes form three nested loops. outer loop uses operators typeoxi iterate assignments x1 , . . . , xm , universal variables formula F .middle loop uses operators type oyj iterate assignments y1 , . . . , yn , existentialvariables F . inner loop uses operators type olk1 , olk2 , olk3 , onk iterate clauses3SAT formula , time verifying whether satisfied given current assignmentx1 , . . . , xm , y1 , . . . , yn . remaining fluents following functions:f x: control applicability operators oxi used iterate assignments x1 , . . . , xm .f y: control applicability operators oyj used iterate assignments y1 , . . . , yn .v0 : control applicability operators olk1 , olk2 , olk3 , onk used iterate clauses.f s: remember whether satisfied current assignment x1 , . . . , xm , y1 , . . . , yn .sat: remember whether y1 , . . . , yn satisfied current assignment x1 , . . . , xm .inner loop, first apply operator os add fluent v0 . clause ck ,apply one operators olk1 , olk2 , olk3 , onk add vk . Finally, apply oneot delete v0 , . . . , vh . process, fluent f added os deleted onkapplied clause ck .Operators ot also add f y, causing operator type oyj become applicable. ftrue, operator ot also adds sat. Applying operator type oyj effect moving nextassignment y1 , . . . , yn . y1 , . . . , yn true, operator od applicable instead, addingfluent f x resetting y1 , . . . , yn false. f x true, apply operator type oximove next assignment x1 , . . . , xm . operators require sat precondition.x1 , . . . , xm true, goal state GF ensures iterate one last time middleloop make f x sat true.282fiAUTOMATON P LANSLemma 19. -SAT formula F satisfiable planning instance pFunique solution F formF= E0 , ox, E1 , ox, . . . , ox, E2m 1 ,n 1Ei = Vi0 , oy, Vi1 , oy, . . . , oy, Vi2Vij, od,= os, oz1 , oz2 , . . . , ozh , ow,ox operator among ox1 , . . . , oxm , oy operator among oy1 , . . . , oyn ,ozk , 1 k h, operator among olk1 , olk2 , olk3 , onk , ow operator among ot,.Proof. prove lemma showing following:1. state reachable initial state , one operator applicable.2. Repeatedly selecting applicable operator results sequence F given above.3. sequence F applicable initial state goal state GF holds resulting state F satisfiable.first show reachable state, exists 0 k h + 1 a) k = 0,variables among v0 , . . . , vh false; b) 1 k h, v0 , . . . , vk1 true vk , . . . , vhfalse; c) k = h + 1 variables among v0 , . . . , vh true. ignoreoperators oyj , od, oxi since effect v0 , . . . , vh . initial state fluentsfalse statement holds k = 0. k = 0, applicable operator os sets v0true, effectively incrementing current value k. 1 k h, possible operatorsolk1 , olk2 , olk3 , onk . However, preconditions operators mutually exclusiveexactly one applicable. operators sets vk true, incrementing valuek. Finally, k = h + 1, possible operators ot . preconditions operatorsalso mutually exclusive, operators set v0 , . . . , vh false, effectively resetting valuek 0.operators affecting fluents y1 , . . . , yn oy1 , . . . , oyn . requires fprecondition sets f false. operators setting f true ot ,reset k 0. implies time want apply operator type oyj , fluents v0 , . . . , vhneed go complete cycle k = 0 k = h + 1 finish ot . cyclecorresponds exactly sequence Vij lemma.next show y1 , . . . , yn act binary counter 0 2n 1, enumerating assignments existential variables formula F . Note preconditions oy1 , . . . , oynmutually exclusive, since oyn requires yn , oyn1 requires yn1 , yn , on. Specifically,applicable operator oyj , 1 j n largest index yj false. Repeatedly283fiB ACKSTR OM , J ONSSON , & J ONSSONapplying available operator results following series values y1 , . . . , yn :0 0000 0010 0100 0110 1000 101...y1 , . . . , yn true, exists applicable operator type oyj , operator od applicableinstead.operators affecting fluents x1 , . . . , xm ox1 , . . . , oxm . requires f xprecondition sets f x false. operator setting f x true od, also resetsy1 , . . . , yn false. implies time want apply operator type oxi , fluentsy1 , . . . , yn need go complete cycle 0 2n 1 finish od. cyclecorresponds exactly sequence Ei lemma.Since operators ox1 , . . . , oxm form oy1 , . . . , oyn , oneapplicable, repeatedly applying available operator among ox1 , . . . , oxm causesfluents x1 , . . . , xm act binary counter 0 2m 1, enumerating assignmentsuniversal variables formula F . precondition {f x, f y} operator os ensures osapplicable whenever apply operator type oyj oxi . achieve goalstate GF = {f x, sat, x1 , . . . , xm }, fluents x1 , . . . , xm complete full cycle 02m 1, set f x true fluents y1 , . . . , yn complete one last cycle settingx1 , . . . , xm true. corresponds exactly sequence F lemma.Finally, need show sequence F applicable initial stategoal state GF holds resulting state formula F satisfiable. initialstate using operator type oxi iterate x1 , . . . , xm , fluent sat false.time apply os, fluent f added. iterating clauses, f deletedapply operator type onk , i.e. exists clause ck satisfied currentassignment x1 , . . . , xm , y1 , . . . , yn . f true end loop, operator ot adds sat.formula F satisfied, exists assignment x1 , . . . , xm unsatisfied assignment y1 , . . . , yn . Consequently, sat false applying operator odmaking f x true assignment x1 , . . . , xm . either operator type oxi applicable(if least one fluent among x1 , . . . , xm false) goal state GF hold resultingstate (if x1 , . . . , xm true). Conversely, F satisfied, sat always true applying od,causing F applicable GF hold resulting state.proceed construct automaton plan represents operator sequence F described Lemma 19.Construction 20. Let pF = hPF , AF , F , , GF planning instance defined Construction 18. Construct automaton plan F = hF , AF , AuF , r, xi following properties:AuF = {X1 , . . . , Xm , Y1 , . . . , Yn , S1 , . . . , Sh+1 , U1 , . . . , Uh+1 },284fiAUTOMATON P LANS/Xi [x]/hXi+1 [x0], oxi , Xi+1 [x1]iXm [x]/hY1 [x0], od, oxm , Y1 [x1], odiYj [x]/hYj+1 [x0], oyj , Yj+1 [x1]iYn [x]/hos, S1 [x0], oyn , os, S1 [x1]i////Sk+1 [x]1k /olk1Sk [x]/2k /olk21k /b /3k /olk32k /c //3k /honk , Uk+1 [x]i/Uk+1 [x]1k /olk1Uk [x]/2k /olk21k /b /3k /olk32k /c //3k /honk , Uk+1 [x]i/Sh+1 [x]/hotiUh+1 [x]/hof/Figure 12: Graphs automata defined Construction 20.285fiB ACKSTR OM , J ONSSON , & J ONSSON1 m, ar(Xi ) = 1,1 j n, ar(Yj ) = + j 1,1 k h + 1, ar(Sk ) = ar(Uk ) = + n,r = X1 x = .graphs associated automata Aun shown Figure 12. indices useddescribe automata ranges 1 < m, 1 j < n, 1 k h. automaton[x], argument maps described u[x], u[x0], u[x1], indicating input string x[x] copied onto u, possibly appending 0 1 end.Intuitively, input string x automata Sk Uk represents assignment fluentsx1 , . . . , xm , y1 , . . . , yn . edge label consumes input symbols, numbervariables precede variable corresponding literal 1k . Likewise, b numbervariables 1k 2k , c number variables 2k 3k . assignment1k determines whether output operator olk1 continue checking whether ck satisfied.Note automata defined Construction 20 uniform expansion. 1 k hx m+n , Apply(Sk [x]) contains exactly one operator among olk1 , olk2 , olk3 , onk , followedeither Sk+1 [x] Uk+1 [x]. true Uk . show automaton plan F definedConstruction 20 indeed represents operator sequence F Lemma 19.Lemma 21. automaton plan F represents operator sequence F .Proof. 1 m, input string x automaton Xi represents assignmentvalues fluents x1 , . . . , xi1 planning instance pF . 1 j n, input stringYj represents assignment values x1 , . . . , xm , y1 , . . . , yj1 , 1 k h + 1,input string Sk Uk represents complete assignment values x1 , . . . , xm , y1 , . . . , yn .Consequently, symbol Xi appends x represents current value xi , symbolYj appends represents current value yj .Since automaton Xi first sets xi 0 1, series assignments x1 , . . . , xmbecomes0 0000 0010 0100 0110 1000 101...identical binary counter x1 , . . . , xm induced plan F . Likewise,assignment x1 , . . . , xm , assignments y1 , . . . , yn describe binary counter F .changing value fluent xi 0 1, automaton Xi inserts operator oxi .last assignment xi+1 , . . . , xm appending oxi 1, . . . , 1, first assignmentxi+1 , . . . , xm appending oxi 0, . . . , 0. Consequently, automata perfectly emulate286fiAUTOMATON P LANSpre- postcondition oxi . true operator oyj inserted Yj . Automaton Xminserts operator od cycle assignments y1 , . . . , yn , automaton Yn inserts operatoros beginning iteration fluents v0 , . . . , vh .1 k h, purpose automata Sk Uk decide operator appendamong olk1 , olk2 , olk3 , onk . this, first access value variable associatedliteral 1k . 1k satisfied, operator olk1 appended, else literal 2k checked, 3k necessary.three literals unsatisfied current variable assignment operator onk appended.difference automata Sk Uk Sh+1 appends operator ot, Uh+1appends . automaton Sk indicates first k 1 clauses current 3SAT instancesatisfied current variable assignment. clause ck unsatisfied Sk call Uk+1 ,case subsequent calls automata type U. words, another purposeautomata Sk Uk remember current value variable f s. way, correctoperator among ot appended end iteration fluents v0 , . . . , vh ,concludes proof.show plan verification p2 -hard automaton plans uniform expansion, given-SAT formula F , construct (in polynomial time) planning instance pF Construction18 automaton plan F Construction 20. Lemma 21 states F represents operatorsequence F defined Lemma 19. Due Lemma 19, F plan pF Fsatisfiable. thus reduced -satisfiability (a p2 -complete problem) plan verificationautomaton plans uniform expansion.Appendix B. Proof Lemma 17section prove Lemma 17, states automaton plan efficientlytransformed equivalent HTN instance h. Let p = hP, A, , I, Gi TRIPS instance let= h, A, Au, ri automaton plan representing solution p. define HTN instanceh = hP , , T, , , , Li follows:P = P {consec} {precedes} {isset-M }M Au ar(consec) = ar(precedes) =ar(isset-M ) = 2 Au,= {set-M, unset-M }M Au ar(set-M ) = ar(unset-M ) = 2 Au,= J, J = {j0 , . . . , jK } set indices K = maxM Au ar(M ),= {consec[ji1 ji ] : 1 K} {precedes[ji jk ] : 0 < k K}.induced set fluents P , static fluent consec[jk] true j k consecutive indices J, precedes[jk] true j precedes k J. automatonAu, symbol , index 1 ar(M ), fluent isset-M [ji ] indicateswhether i-th symbol input string x ar(M ) equals . induced set operators , operators set-M [j] unset-M [j] add delete fluent isset-M [j], respectively.automaton Au, also add following tasks methods sets :task setall-M arity ar(M ),method dosetall-M arity ar(M ) associated task setall-M ,287fiB ACKSTR OM , J ONSSON , & J ONSSONstate , task visit-M -s arity ar(M ) + 1,edge (s, t) label /u, method traverse-M -s-t arity ar(M ) + 1associated task visit-M -s,edge (s, t) label /u, , method consume-M -s-t arity ar(M ) + 2associated task visit-M -s,state || outgoing edges, method finish-M -s arity ar(M ) + 1associated task visit-M -s.Formally, precondition task list method contain pairs (u, )action task u associated argument map u. However, simplify notationinstead describe grounded preconditions task lists grounded methods [xy]. inducedset grounded tasks , grounded task setall-M [x] sets current input string x.lone associated grounded method dosetall-M [x] empty precondition followingtask list:= hunset-M [1 j1 ], . . . , unset-M [n j1 ], set-M [x1 j1 ],...unset-M [1 jar(M ) ], . . . , unset-M [n jar(M ) ], set-M [xar(M ) jar(M ) ]i,1 , . . . , n symbols set . words, dosetall-M [x] first unsets symbolsindex input string, sets symbol according x.grounded task visit-M -s[xjk ] indicates currently state automaton ,input string x current index x k. single outgoing edge(s, t) label /(u, ), associated grounded method traverse-M -s-t[xjk ] emptyprecondition. Let u[(x)] result applying argument map induced indexstring input string x . u A, grounded task list traverse-M -s-t[xjk ]equals = hu[(x)], visit-M -t[xjk ]i, effectively applying operator u[(x)]. hand,u Au, task list = hsetall-u[(x)], visit-u-s0 [(x)j0 ], visit-M -t[xjk ]i, first settinginput string u (x) visiting initial state s0 u index j0 . either case,grounded task visit-M -t[xjk ] end ensures next visit state withoutincrementing k.If, instead, multiple outgoing edges, outgoing edge (s, t) label /(u, ), associated grounded method consume-M -s-t[xjk jk+1 ] precondition {consec[jk jk+1 ], precedes[jk jar(M ) ], isset-M [jk+1 ]}. index jk+1 free parameterconsume-M -s-t, precondition consec[jk jk+1 ] ensures jk jk+1 consecutiveindices J. precondition precedes[jk jar(M ) ] ensures k < ar(M ), i.e. inputsymbols left x process. Note indices start j0 , symbol index jk+1input string x set . task list identical traverse-M -s-t[xjk ], exceptlast task visit-M -t[xjk+1 ] associated next index jk+1 , indicatingconsumed symbol input string.|| outgoing edges, grounded method finish-M -s[xjk ] preconditionjk = jar(M ) , i.e. method applicable consumed symbols input string.assume jk = jar(M ) checked without introducing additional predicate P .288fiAUTOMATON P LANStask list empty, indicating finished traversing states . methodapplicable states single outgoing edge since fire applicable -transitionsterminating.task list HTN instance h given L = hsetall-r[x], visit-r-s0 [xj0 ]i r[x]root automaton plan . way tasks methods defined, expandingvisit-r-s0 [xj0 ] corresponds exactly executing automaton r input string x startings0 . Thus expansion L corresponds exactly solution p representedremove instances operators set-M unset-M . Since solution p, operatorsequence guaranteed applicable.type task multiple associated methods visit-M -s. methods consumeM -s-t associated visit-M -s mutually exclusive since moment, isset-M [jk ] trueone symbol index 1 k ar(M ) input string x .jk = jar(M ) , method finish-M -s applicable instead. task list method totallyordered, implying instance h belongs restricted class HTNs mutually exclusivemethods totally ordered task lists.ReferencesAlur, R., & Yannakakis, M. (1998). Model Checking Hierarchical State Machines. Proceedings ACM SIGSOFT International Symposium Foundations Software Engineering,pp. 175188.Backstrom, C., Jonsson, A., & Jonsson, P. (2012a). Macro Plans Automata Plans.Proceedings 20th European Conference Artificial Intelligence (ECAI), pp. 9196.Backstrom, C., Jonsson, A., & Jonsson, P. (2012b). Macros, Reactive Plans Compact Representations. Proceedings 20th European Conference Artificial Intelligence (ECAI),pp. 8590.Backstrom, C., & Jonsson, P. (2012). Algorithms Limits Compact Plan Representation.Journal Artificial Intelligence Research, 44, 141177.Baier, J., & McIlraith, S. (2006). Planning Temporally Extended Goals Using Heuristic Search.Proceedings 16th International Conference Automated Planning Scheduling(ICAPS), pp. 342345.Bonet, B., Palacios, H., & Geffner, H. (2010). Automatic Derivation Finite-State MachinesBehavior Control. Proceedings 24th National Conference Artificial Intelligence(AAAI).Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI PlanningAutomatically Learned Macro-Operators. Journal Artificial Intelligence Research,24, 581621.Bylander, T. (1994). Computational Complexity Propositional STRIPS Planning. ArtificialIntelligence, 69, 165204.Cimatti, A., Roveri, M., & Traverso, P. (1998). Automatic OBDD-based Generation UniversalPlans Non-Deterministic Domains. Proceedings 15th National ConferenceArtificial Intelligence (AAAI), pp. 875881.289fiB ACKSTR OM , J ONSSON , & J ONSSONColes, A., & Smith, A. (2007). MARVIN: Heuristic Search Planner Online Macro-ActionLearning. Journal Artificial Intelligence Research, 28, 119156.Erol, K., Hendler, J., & Nau, D. (1996). Complexity Results HTN Planning. Annals Mathematics Artificial Intelligence, 18, 6993.Fikes, R., Hart, P., & Nilsson, N. (1972). Learning executing generalized robot plans. ArtificialIntelligence, 3(4), 251288.Fikes, R., & Nilsson, N. (1971). STRIPS: New Approach Application Theorem ProvingProblem Solving. Artificial Intelligence, 2(3/4), 189208.Gimenez, O., & Jonsson, A. (2008). Complexity Planning Problems Simple CausalGraphs. Journal Artificial Intelligence Research, 31, 319351.Hickmott, S., Rintanen, J., Thiebaux, S., & White, L. (2007). Planning via Petri Net Unfolding.Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI), pp.19041911.Hu, Y., & De Giacomo, G. (2013). Generic Technique Synthesizing Bounded Finite-StateControllers. Proceedings 23rd International Conference Automated PlanningScheduling (ICAPS).Jonsson, A. (2009). Role Macros Tractable Planning. Journal Artificial IntelligenceResearch, 36, 471511.Korf, R. (1987). Planning Search: Quantitative Approach. Artificial Intelligence, 33(1), 6588.LaValle, S. (2006). Planning Algorithms. Cambridge Press.Lempel, A., & Ziv, J. (1976). Complexity Finite Sequences. IEEE TransactionsInformation Theory, 22(1), 7581.McAllester, D., & Rosenblitt, D. (1991). Systematic Nonlinear Planning. Proceedings 9thNational Conference Artificial Intelligence (AAAI), pp. 634639.Mealy, G. (1955). Method Synthesizing Sequential Circuits. Bell System Technical Journal,34, 10451079.Minton, S. (1985). Selectively Generalizing Plans Problem-Solving. Proceedings 9thInternational Joint Conference Artificial Intelligence (IJCAI), pp. 596599.Nau, D., Ilghami, O., Kuter, U., Murdock, J., Wu, D., & Yaman, F. (2003). SHOP2: HTNPlanning System. Journal Artificial Intelligence Research, 20, 379404.Newton, M., Levine, J., Fox, M., & Long, D. (2007). Learning Macro-Actions Arbitrary PlannersDomains. Proceedings 17th International Conference Automated PlanningScheduling (ICAPS), pp. 256263.Subramanian, H., & Shankar, P. (2005). Compressing XML Documents Using Recursive FiniteState Automata. Proceedings 10th International Conference ImplementationApplication Automata (CIAA), pp. 282293.Toropila, D., & Bartak, R. (2010). Using Finite-State Automata Model Solve PlanningProblems. Proceedings 11th Italian AI Symposium Artificial Intelligence (AI*IA),pp. 183189.290fiAUTOMATON P LANSWinner, E., & Veloso, M. (2003). DISTILL: Towards Learning Domain-Specific Planners Example. Proceedings 20th International Conference Machine Learning (ICML),pp. 800807.Zhong, H., & Wonham, M. (1990). Consistency Hierarchical Supervision DiscreteEvent Systems. IEEE Transactions Automatic Control, 35(10), 11251134.Zipstein, M. (1992). Data Compression Factor Automata. Theoretical Computer Science,92(1), 213221.291fiJournal Artificial Intelligence Research 51 (2014) 493-532Submitted 07/14; published 10 /14Reasoning Topological Cardinal Direction Relations2-Dimensional Spatial ObjectsAnthony G. CohnA.G.C OHN @ LEEDS . AC . UKSchool Computing, University Leeds, UKFaculty Engineering Information Technology,University Technology Sydney, AustraliaSanjiang LiANJIANG .L @ UTS . EDU . AUAMSS-UTS Joint Research Lab,Centre Quantum Computation & Intelligent Systems,University Technology Sydney, AustraliaCollege Computer Science, Shaanxi Normal University, ChinaWeiming LiuL IU W EIMING @ BAIDU . COMBaidu (China) Co., Ltd., Shanghai, ChinaJochen RenzJ OCHEN .R ENZ @ ANU . EDU . AUResearch School Computer Science,Australian National University, AustraliaAbstractIncreasing expressiveness qualitative spatial calculi essential step towards meetingrequirements applications. achieved combining existing calculi wayexpress spatial information using relations multiple calculi. great challengedevelop reasoning algorithms correct complete reasoning combinedinformation. Previous work mainly studied cases interaction combinedcalculi small, one two calculi simple. paper tackleimportant combination topological directional information extended spatial objects.combine best known calculi qualitative spatial reasoning, RCC8 algebrarepresenting topological information, Rectangle Algebra (RA) Cardinal DirectionCalculus (CDC) directional information. consider two different interpretations RCC8algebra, one uses weak connectedness relation, uses strong connectedness relation.interpretations, show reasoning topological directional information decidable remains NP. computational complexity results unveil significant differencesRA CDC, weak strong RCC8 models. Take combinationbasic RCC8 basic CDC constraints example: show consistency problemP use strong RCC8 algebra explicitly know corresponding basic RAconstraints.1. IntroductionQualitative Spatial Reasoning (QSR) multi-disciplinary research field aims establishingexpressive representation formalisms qualitative spatial knowledge providing effective reasoning mechanisms. Originating Allens work (1983) temporal interval relations, QSRwidely acknowledged AI approach spatial knowledge representation reasoning,applications ranging natural language understanding (Davis, 2013), robot navigation (Shi,Jian, & Krieg-Bruckner, 2010; Falomir, 2012), geographic information systems (GISs) (Egenhoferc2014AI Access Foundation. rights reserved.fiC OHN , L , L IU , & R ENZ& Mark, 1995), sea navigation (Wolter et al., 2008), high level interpretation video data (Sridhar, Cohn, & Hogg, 2011; Cohn, Renz, & Sridhar, 2012). refer reader work CohnRenz (2008), Wolter Wallgrun (2012) information.qualitative approach usually represents spatial information introducing relation modeldomain spatial entities, could points, line segments, rectangles, arbitraryregions. literature, relation model often called qualitative calculus (Ligozat &Renz, 2004), contains finite set jointly exhaustive pairwise disjoint (JEPD) relationsdefined domain. past three decades, dozens spatial relation modelsproposed literature (Cohn & Renz, 2008; Chen, Cohn, Liu, Wang, Ouyang, & Yu, 2013).Many qualitative calculi approximate spatial entities points. convenientrepresenting spatial direction, distance positions (providing extent objectssmall compared distance apart), inappropriate far shapes and/or topologyspatial objects concerned. paper, represent spatial entities 2-dimensionalbounded regions real plane, may holes multiple connected components.literature, spatial calculi focus one single aspect space, e.g. topology, direction, distance, position, shape. Topological relations relations invarianthomeomorphisms scale, rotation, translation. widely acknowledged topological relations crucial importance. One influential formalism topological relationsregion connection calculus (RCC) (Randell, Cui, & Cohn, 1992). Based one primitive binaryconnectedness relation, set eight JEPD topological relations defined RCC.calculus known RCC8 algebra. According different interpretations connectedness,calculus may different variants. paper, say two (closed) regions weakly connected share least common point, say strongly connected intersectionleast one-dimensional. Accordingly, address two resulting RCC8 algebras weakstrong RCC8 algebras respectively. convenience, denote weak RCC8 algebraRCC8, strong one RCC80 .importance distinction strong weak RCC8 becomes clear analysingdifferent ways defining neighbourhood pixels commonly used Computer Vision. 4connectedness refers pixels horizontally vertically connected pixel,8-connectedness includes diagonally neighbouring pixels well. distinction correspondsnicely distinction strong weak RCC8 8-connectedness considers connections point, 4-connectedness considers connections along line (which onedimensional). Therefore, use strong weak RCC8 similar way use 4- 8connectedness, depending requirements application hand.RCC8 algebra represents topological information spatial objects. manypractical applications, however, kinds relations often used together topologicalrelations. example, recommending restaurant dined common givedescriptions restaurant city centre, west central station, nearbyMcDonalds.Among aspects spatial information topology, directional relationsperhaps important. two well-known formalisms cope directionalrelations extended spatial objects. One Rectangle Algebra (RA) (Balbiani, Condotta,& Farinas del Cerro, 1999), Cardinal Direction Calculus (CDC) (Goyal & Egenhofer,2001; Skiadopoulos & Koubarakis, 2005). representing direction primary objectreference object, RA approximates reference object primary object494fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSminimum bounding rectangles (MBRs), relates two objects interval relationsprojected intervals. hand, CDC approximates reference object MBR,leaving primary object unchanged. CDC 511 basic relations, RA 169basic relations. (487 511) basic CDC relations intersect one one basicRA relation and, hence, contained unique basic RA relation. Therefore, CDC senseexpressive RA.central reasoning problem QSR consistency problem. instance consistencyproblem set constraints like (xy), x, spatial variables, qualitativerelation qualitative calculus. say consistent satisfiable exists instantiation spatial variables constraints satisfied. Without loss generality,assume unique constraint two variables. Note xrelated, add (x ? y) without changing consistency, ? universal relationcalculus. Unlike classical CSPs, domain spatial variable usually infinite, mayundecidable determine consistency binary CSPs infinite domains (Hirsch, 1999).past three decades, QSR made significant progress solving consistency problemsvariety qualitative calculi (Renz & Nebel, 1999; Renz, 1999; Balbiani et al., 1999; Zhang,Liu, Li, & Ying, 2008; Skiadopoulos & Koubarakis, 2005; Liu, Zhang, Li, & Ying, 2010; Liu & Li,2011).order bring spatial reasoning theory closer practical applications, necessarycombine multiple aspects spatial information. growing number works devotedcombining topological RCC relations aspects spatial information, e.g. qualitative size(Gerevini & Renz, 2002), cardinal directions (Sistla & Yu, 2000; Li, 2006a, 2007; Liu, Li, & Renz,2009; Li & Cohn, 2012), connectivity (Kontchakov, Nenov, Pratt-Hartmann, & Zakharyaschev,2011), convexity (Davis, Gotts, & Cohn, 1999; Schockaert & Li, 2012), betweenness (Schockaert& Li, 2013), gravity (Ge & Renz, 2013). Recently, Wolfl Westphal (2009) also empiricallycompared two approaches combination binary qualitative constraint calculi general.also interesting works combining spatial temporal formalisms (Gerevini & Nebel,2002; Gabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev, 2005). Moreover, subareasformalisms constraint research, combination formalisms discussed long timestrong results, see e.g. work Bodirsky Kara (2010), JonssonKrokhin (2004).current paper considers full combination RCC8 RCC80 two directionalrelation models RA CDC. identify joint satisfaction problem (JSP) main reasoningtask. Given network topological (RCC8 RCC80 ) constraints network directional(RA CDC) constraints , assuming involve set variables, JSPdecide joint network ] satisfiable. Note use ], instead , indicatevariables.Since topological directional information independent, possible jointnetwork ] unsatisfiable satisfiable. Solving joint satisfactionproblem general harder solving independently. paper, interpret directional relations terms RA CDC, interpret topological relations terms weakstrong RCC8 algebras. basic constraints involved, show JSPbasic (weak strong) RCC8 basic RA networks solved polynomial time,JSP basic (weak strong) RCC8 basic CDC networks NP-complete. Furthermore,show that, three calculi (viz. RCC8, RA, CDC) combined together, JSP495fiC OHN , L , L IU , & R ENZbasic RCC80 networks basic RA CDC networks tractable. Since non-basic constraintsalways backtracked basic constraints, results show JSP (weak strong)RCC8 RA CDC NP.paper significant extension conference paper (Liu et al., 2009), combination basic weak RCC8 RA CDC constraints considered. paper also considerscombination RCC80 RA and/or CDC constraints. addition, extend tractableresults two maximal tractable subsets RCC8 one large tractable subset RA. paperalso closely related work Li (2007), Li Cohn (2012), combinationweak RCC8 algebra two subalgebras (viz. DIR9 DIR49) RA considered.1.1 Application Scenarioexample demonstrating usefulness results, use Angry Birds domain.Similar representation reasoning tasks applied whenever use computer visiondetect objects image video. Angry Birds popular computer game gained increasingattention within AI community, see e.g. work Zhang Renz (2014). Angry BirdsAI competition AI challenge problem, goal build intelligent agentplay Angry Birds better best human players (see http://aibirds.org).Figure 1: screenshot Angry Birds game.Angry Birds domain includes number building blocks different materials, sizesshapes, even holes. building blocks form complicated spatial structuresprotect pigs attacking birds (see Figure 1). AI agents able play gamelike humans do, get visual information game form screenshots.competition organisers provide basic computer vision software detects minimumbounding boxes objects screenshot well object category. givenset rectangles form minimum bounding boxes actual objects (see Figure 1).object solid physical object cannot overlap another object (only RCC8 relations DCEC possible objects), bounding boxes related relationRectangle Algebra relation CDC. Instead considering spatial relationssingle objects, also take account sets objects, example, set objectsdirectly indirectly supported particular object, set objects providecover particular pig, set wooden blocks.496fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSNotations, , , , ,D, R, S,wx, y, z,vi , vj, ,a, b, c,b8 , Q8 , C8HP, QHIx (a), Iy (a)M(a)= (mi )ni=1(, )x (, ), (, )(, )]JSP(S, )RA(T )RCC8(D)CCP(vi , vj )Meaningsrelations, usually basic relations (page 498)relations, usually non-basic relations (page 498)converse relation (page 498)weak composition (page 498)spatial variable interval variable (page 498)network constraints (page 498)bounded regions (page 500)three maximal tractable subclasses RCC8 (page 500)points (page 501)unique maximal tractable subclass IA (page 502)x- y- projective intervals region (page 503)minimal bounding rectangle (MBR) region (page 503)RA relation induced two IA relations , (page 503)n-tuple regions mi form solution network (page 504)consistent pair basic CDC relations (page 505)x- y- projective interval relations (, ) (page 505)RA relation x (, ) (, ) induced (, ) (page 506)combination two networks set variables (page 507)joint satisfaction problem subclasses (page 507)RA relation induced RCC8 relation (page 509)RCC8 relation induced RA relation (page 509)two variables vi , vj common conflict point relation (page 510)Table 1: Notations.sets building blocks form spatial regions general sense used RCC8BRCC8 (Wolter & Zakharyaschev, 2000), also include regions multiple disconnectedpieces regions holes. particular, means RCC8 relation possible twosets objects, DC EC.Given spatial configurations Angry Birds domain, use RCC8 relations wellRA CDC relations represent spatial information (sets of) objects minimumbounding boxes extracted screenshots. results paper allow us accurately reason combined information represented using RCC8, RA, CDC. Importantreasoning tasks benefit results include, example, inferring configurationchanges hit bird inferring whether given representation consistent whetherstable gravity (Zhang & Renz, 2014). algorithm predicting configurationblocks shot might work envisaging individual possible block positions mightmutually globally inconsistent. algorithm reasoning consistencypredictions therefore desirable.remainder paper proceeds follows. Section 2 introduces basic notions, importantexamples, essential results qualitative calculi. Section 3 describes joint satisfactionproblem considers simple example combination RA CDC constraints. Sections4 5 consider computational complexity combination weak and, respectively, strong497fiC OHN , L , L IU , & R ENZRCC8 RA. Section 6 discusses computational complexity combination weakstrong RCC8 CDC. conclude paper Section 7 give proofs major computational complexity results appendices. convenience reader, Table 1 summarisesnotations used paper.2. Qualitative Calculiestablishment proper qualitative calculus key success qualitative approach temporal spatial reasoning. section introduces basic notions qualitative calculi recalls RCC8 algebra, Rectangle Algebra, Cardinal Direction Calculus.addition, also summarise essential results used main partpaper.2.1 Basic NotionsLet U domain temporal spatial entities, Rel(U) set binary relations U.usual relational operations intersection, union, complement, Rel(U) Booleanalgebra. finite set B nonempty binary relations U jointly exhaustive pairwise disjoint(JEPD short) two entities U related one one relation B. Write hBisubalgebra Rel(U) generated B. Clearly, relations B atoms algebra hBi.call hBi qualitative calculus U, call relations B basic relations calculus.Notation. Note relation hBi union set basic relations. paper,write R = {1 , 2 , ..., k } R union basic relations 1 , 2 , ..., k . convenience,regard basic relation singleton {}.two relations R, qualitative calculus = hBi, write R converse R,definedR = {(x, y) U U : (y, x) R},(1)write R w smallest relation contains R S, usual composition RS, definedR = {(x, y) U : (z U)(x, z) R (z, y) S}.call R w weak composition R (Duntsch, Wang, & McCloskey, 2001).constraint hBi form (xRy), R relation hBi. call (xRy) basicconstraint R basic relation B. important reasoning problem qualitative calculusdetermine satisfiability consistency network = {vi Rij vj }ni,j=1 constraints hBi,satisfiable (or consistent) instantiation (ai )ni=1 U (ai , aj ) Rijholds 1 i, j n.Given two constraint networks = {vi Rij vj }ni,j=1 = {vi Tij vj }ni,j=1 , say refinesTij subset Rij 1 i, j n. consistent scenario consistent basicnetwork refines . clear consistent iff consistent scenario.hand, given n-tuple entities (ai )ni=1 U, write ij basic relation fixed qualitativecalculus relates ai aj . = {vi ij vj }ni,j=1 consistent scenario callscenario (or basic constraint network) induced (ai )ni=1 .498fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSconsistency constraint network partially determined path-consistency algorithms. say network = {vi Rij vj }ni,j=1 path-consistentRji = Rij,6= Rij Rik w Rkj(2)i, j k 6= i, j. case basic network, equivalent saying everysubnetwork involving three variables consistent.Path-consistency enforced cubic time (Vilain & Kautz, 1986). is, applypath-consistency algorithm constraint network , cubic time algorithmterminate either get empty constraint (and hence know inconsistent) transformequivalent path-consistent network. basic networks, easy see consistencyimplies path-consistency, opposite proposition always hold.following subsections recall qualitative topological directional calculidiscussed paper.2.2 Region Connection Calculus RCC8region connection calculus (RCC) (Randell et al., 1992) first-order theory based binaryconnectedness relation. Standard RCC models arise topological spaces. paper,concerned interpretations RCC real plane, provides arguablyimportant model RCC. Another reason lies directional calculi considered paperalso defined real plane. plane region (or region) nonempty regular closed subsetreal plane. consider bounded regions, cardinal directions involve boundedregions. regions could multi-pieces and/or holes.1One standard interpretation RCC based Whiteheadean connectedness (Whitehead,1929) plane regions, two regions connected common point. connectedness may considered weak many cases. example, worm cannot passinterior one apple another, touch point, without becoming visible exterior worms point view might well say apples sufficientlyconnected. (Borgo, Guarino, & Masolo, 1996, p. 223) paper, also consider strongerconnectedness, two regions regarded connected intersection least onedimensional (Li, Liu, & Wang, 2013). case rectangular grid spatial primitive entities,already noted, strong weak connectedness correspond to, respectively, important notion4- 8-neighbourhood pixels commonly used Computer Vision.interpretations, relations Table 2 converses TPP NTPP formJEPD set. Write Brcc8 Brcc80 two sets. call Boolean algebras generatedBrcc8 Brcc80 , respectively, weak strong RCC8 models, written RCC8 RCC80 .Strong connectedness considered Borgo et al., (1996), Cohn Varzi (1999),Li et al., (2013). easy see that, relations, strong connectedness contained weakconnectedness. Table 2 illustrates configuration (the 2nd left) instance ECRCC8 instance DC RCC80 , configuration (the 2nd right)instance TPP RCC8 instance NTPP RCC80 .1. stress restriction RCC bounded plane regions affect complexity reasoningRCC8, every consistent RCC8 network solution RCC model (Li, 2006b).499fiC OHN , L , L IU , & R ENZRelationequalsdisconnectedexternally connectedSymb.EQDCECpartially overlapPOtangential proper partnon-tangential proper partTPPNTPPDefinition (weak)a=bab=b 6= b =b 6=6 b 6 bb 6 bbDefinition (strong)a=bdim(a b) 0dim(a b) = 1b 6=6 b 6 bb dim(a b) = 1b dim(a b) 0Table 2: set basic RCC8 RCC80 relations, a, b two plane regions x , x, dim(x)denote, respectively, interior, boundary, dimension x. Note notational convenience set dim() = 1.Remark 1. far consistency realisations concerned, Li (2006b) shownconsistent RCC8 network solution RCC model. cubic realisation algorithm described used construct solution weak strong RCC8 models.implies particular RCC8 network solution weak RCC8 model iffsolution strong RCC8 model. show paper, is, however, casecardinal directions combined topological relations.following, recall important properties three maximal tractable subclassesbH8 , C8 , Q8 RCC8 identified Renz (1999). complete list relations subclassesfound Appendix work Renz (2002).Lemma 2. Suppose R non-basic RCC8 relation R {DC, EC, PO} = .(1) R Q8 iff R either {TPP, NTPP} {TPP , NTPP }.b8 iff R Q8 one following relations(2) R H{TPP, EQ}, {TPP, NTPP, EQ}, {TPP , EQ}, {TPP , NTPP , EQ}.b8 , either {NTPP, EQ} {NTPP , EQ}.(3) R C8 iff R Hnote lemma define subclasses. particular, subclassesinclude RCC8 relations R R {DC, EC, PO} =6 .Renz also shows consistent scenario constructed O(n2 ) time pathconsistent network one three maximal tractable subclasses.500fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSTheorem 3 (Renz, 1999). consistent scenario path-consistent network constraintsb8 , C8 , Q8 computed O(n2 ) time, replacing every constraint (vi Rvj )H(vi base(R) vj ) , base(R) basic relation obtained follows:(1) R B, base(R) = R;(2) else {DC} R, base(R) = {DC};b8 , base(R) = {EC};(3) else {EC} R = Q8 = H(4) else {PO} R, base(R) = {PO};(5) else {NTPP} R = C8 , base(R) = {NTPP};(6) else {TPP} R, base(R) = {TPP};(7) else base(R) = base(R ).follows, call canonical consistent scenario .2.2.1 R EALISATION BASIC RCC8 N ETWORKSknown that, basic RCC8 networks, path-consistency implies consistency (Nebel, 1995).next give short description cubic realisation algorithm proposed Li (2006b), needdevise similar algorithm later combination cases.Given basic RCC8 network = {vi ij vj }ni,j=1 , suppose path-consistent. ntpp-chaindefined series variables vi1 , vi2 , , vik vis NTPPvis+1= 1, , k 1. ntpp-level l(i) variable vi defined maximum lengthntpp-chains contained ends vi .realisation constructed follows, variable may interpreted boundedregion multiple pieces. Without loss generality, assume (vi EQvj )= j. first define variable vi finite set Xi control points follows. i,introduce point Pi vi ; vi ECvj vi POvj , introduce point Pij vi ; vi TPPvjvi NTPPvj , put Xi points Xj . expand point P Xi little obtainsquare s(P ). squares pairwise disjoint. Then, taking union squares, obtaininstantiation bounded regions vi . works EC NTPP constraints.modifications needed cope constraints (cf. Li, 2006b Appendix Cpaper).2.3 Interval Algebra Rectangle Algebrasubsection, recall Interval Algebra (IA) (Allen, 1983) Rectangle Algebra (RA) (Balbiani et al., 1999). IA qualitative calculus generated 13 basic relations closedintervals real line shown Table 3. writeBint = {b, m, o, s, d, f, eq, fi, di, si, oi, mi, bi}(3)set basic IA relations. Ligozat (1994) defines dimension2 basic interval relation2 minus number equalities appearing definition relation (see Table 3). is,2. stress notion dimension different topological dimension.501fiC OHN , L , L IU , & R ENZbasic relationsdim(eq) = 0, dim(m) = dim(s) = dim(f) = 1, dim(b) = dim(o) = dim(d) = 2.(4)non-basic relation R definedim(R) = max{dim() : basic relation R}.RelationmeetsoverlapsstartsfinishesequalsSymb.bfeqConv.bimioisidifieqDim.2121210(5)Definitionx+ <x+ =x < < x+ < +x = < x+ < +< x < x+ < +< x < x+ = +x = < x+ = +(i)(ii)Table 3: IA basic relations (i) definitions (ii) conceptual neighbourhood graph, x =[x , x+ ], = [y , + ] two intervals.Nebel Burckert (1995) shown unique maximal tractable subclass IAcontains basic relations. subclass, written H, known ORD-Horn class.Using conceptual neighbourhood graph (CNG) IA (Freksa, 1992), Ligozat (1994) givesgeometrical characterisation ORD-Horn relations. Consider CNG IA (shown Table 3(ii)) partially ordered set (Bint , ) (by interpreting relation smaller right upperneighbours). 1 , 2 Bint 1 2 , write [1 , 2 ] set basic interval relations1 2 , call relation convex interval relation. IA relation R calledpre-convex obtained convex relation removing one basic relationsdimension lower R. example, [o, eq] = {o, s, fi, eq} convex relation {o, eq}pre-convex relation. Ligozat shown ORD-Horn relations precisely pre-convex relations.Nebel Burckert also show every path-consistent IA network H consistent. Furthermore, construct consistent scenario every path-consistent IA network Hquadratic time.Suppose R IA relation letRcore = { Bint : dim() = dim(R), R}.(6)(6) clear Rcore = {eq} iff R = {eq}, Rcore = R {b, o, d, di, oi, bi} R{b, o, d, di, oi, bi} nonempty, Rcore = R \ {eq} otherwise.Lemma 4. (Renz, 1999) Suppose = {vi Rij vj }ni,j=1 path-consistent IA network H. Letcorecore = {vi Rijvj }ni,j=1 .core also path-consistent.502(7)fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSTable 3 (ii) easy see pre-convex relation R dimension 1 iff either R1-dim basic relation R contained {s, eq, si} {fi, eq, f}. consequence, knowCorollary 5. Suppose = {vi Rij vj }ni,j=1 path-consistent network H.v }nconsistent scenario = {vi Rijj i,j=1 following property:) = dim(R ),dim(Rijij= {eq} R = {eq}, R = {m} R = {m}, R = {mi} R = {mi}.Rijijijijijijresult shows that, path-consistent network H, construct quadratictime consistent scenario .(i)(ii)Figure 2: (i) minimum bounding rectangle M(a) region a; (ii) RA relation bo.IA naturally extended regions plane. assume orthogonal basis Euclidean plane. bounded region a, minimum bounding rectangle (MBR), denoted M(a),smallest rectangle contains whose sides parallel axes basis.write Ix (a) Iy (a) as, respectively, x- y-projections M(a). basic rectanglerelation two bounded regions a, b iff (Ix (a), Ix (b)) (Iy (a), Iy (b)) ,, two basic IA relations (see Figure 2 illustration). write Brec setbasic rectangle relations, i.e.,Brec = { : , Bint }.(8)169 different basic rectangle relations Brec . Rectangle Algebra (RA) algebragenerated relations Brec (Balbiani et al., 1999).following definitions used later.Definition 6. Suppose = 1 2 basic RA relation. say 0-meet relation1 , 2 {m, mi}, corner relation 1 , 2 {m, mi, s, si, f, fi, eq}. general, saynon-basic RA relation R = {1 , ..., k } (k 2) corner relation (1 k)corner relation.definition, 0-meet relation corner relation. Furthermore, easy see basicRA relation 0-meet relation iff, every two rectangles r, r0 (r, r0 ) , r r0singleton plane; corner relation iff every two rectangles r, r0 (r, r0 ) have,least, corner point common.following lemma straightforward.503fiC OHN , L , L IU , & R ENZLemma 7. Let = {vi (Rij Sij )vj }ni,j=1 RA network, Rij Sij arbitrary IArelations. satisfiable iff projections x = {xi Rij xj }ni,j=1 = {yi Sij yj }ni,j=1satisfiable IA networks.Corollary 5 lemmaLemma 8. Suppose = {vi Rij vj } path-consistent RA network H H.consistent scenario = {vi ij vj }ij 0-meet relation iff Rij 0-meet basic relation,ij corner relation iff Rij consists basic corner relations.consequence, know H H tractable subclass RA. maximal tractable subclassidentified RA, larger tractable subclass RA identified (Balbiani et al.,1999).next show path-consistent basic IA RA network canonical solutionfollowing sense.+ nDefinition 9 (canonical tuple intervals (rectangles)). Suppose = ([m, mi ])i=1 n-tupleintervals. Let E(m) set values end points intervals m. saycanonical iff E(m) = {0, 1, , }. tuple rectangles (mi )ni=1 canonical iff x-y-projections, (Ix (mi ))ni=1 (Iy (mi ))ni=1 , canonical tuples intervals. solutionIA (RA, respectively) network called canonical solution canonical tuple intervals(rectangles, respectively).basic satisfiable IA network, compute total order end points. Henceobtain canonical solution (by assigning 0 first end point, 1 second, etc.).gives us following proposition.Proposition 10. Suppose satisfiable basic IA (RA) constraint network. uniquecanonical solution.2.4 Cardinal Direction Calculuscardinal direction calculus (CDC) proposed Goyal Egenhofer (1997). Givenbounded region b real plane, extending four edges M(b), partition planenine tiles, denoted bij (1 i, j 3), see Figure 3 (i) illustration.primary region reference region b, CDC relation b, denoted ab ,encoded 3 3 Boolean matrix (dij )1i,j3 , dij = 1 iff bij 6= (whereinterior a). example, basic CDC relations ab ba regions a, bFigure 3(ii) represented following matrices.0 0 00 0 1= ab = 1 0 0 , = ba = 0 0 1 .(9)0 0 00 0 1CDC relation zero Boolean matrix, 29 1 = 511 basic relationsCDC. denote set Bcdc . pair basic CDC relations (, ) called consistent pair504fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS(i)(ii)(iii)Figure 3: Illustrations (i) nine tiles reference region; (ii) (iii): two solutionsCDC basic constraint network {v1 v2 , v2 v1 }, defined Eq. (9).constraint network {v1 v2 , v2 v1 } solution. also call weak converse(, ) consistent pair. Figure 4 shows basic CDC relation may one weakconverse. Therefore, need relation b relation b give completedescription (in terms CDC calculus) directional information two regions a, b.(i)(ii)Figure 4: Illustration two consistent CDC pairs (i) (ab , ba ) (ii) (a0 b0 , b0 ), ab =a0 b0 ba 6= b0 a0 . Also note rectangle relation a, ba0 , b0 o.following show strong connection CDC RA relations.Definition 11. (Zhang et al., 2008; Liu et al., 2010) pair basic CDC relations (, ),define x-projective interval relation (, ), written x (, ), disjunction basicIA relations instance x-projection solution {v1 v2 , v2 v1 },i.e.x (, ) = { Bint : (m1 , m2 )[(m1 , m2 ) (m2 , m1 ) (Ix (m1 ), Ix (m2 )) ]}.similar definition applies y-direction.Note (, ) consistent pair, x (, ) (, ) empty relation.(, ) consistent pair, prove (Liu et al., 2010) x- (or y-) projective interval505fiC OHN , L , L IU , & R ENZrelation IA relation R following propertyR = {b, m} R = {bi, mi}, R basic IA relation {o, s, d, f, eq, oi, si, di, fi}.(10)two projective interval relations combined RA relation.Definition 12. (Zhang et al., 2008; Liu et al., 2010) pair basic CDC relations (, ),call (, ) = x (, ) (, ) RA relation induced (, ). general, basic CDCconstraint network = {vi ij vj }ni,j=1 , call () = {vi Rij vj }ni,j=1 RA constraint networkinduced , Rij = (ij , ji ).Note (, ) necessarily basic RA relation. (, ) consistent, knowRA relation (, ) form , , IA relations satisfy (10). Furthermore,solution {v1 v2 , v2 v1 } always solution {v1 (, )v2 }. note solution{v1 (, )v2 } necessarily solution {v1 v2 , v2 v1 }.Take consistent pair ( , ) defined (9) example. Figure 3 (ii) (iii) show twosolutions (a, b) (a0 , b0 ) basic CDC constraint network {v1 v2 , v2 v1 }. impliesdefinition x ( , ) contains {b, m}. easy see definition x ( , )contains basic IA relations x ( , ) = {b, m}. Similarly, show ( , ) ={d}. shows consistent pair ( , ) corresponds basic RA relations, viz.b d.2.4.1 C ANONICAL OLUTIONS BASIC CDC N ETWORKSlike IA RA, consistent CDC networks also canonical solutions.Definition 13 (regular solution, Zhang et al., 2008; Liu et al., 2010). Suppose = (mi )ni=1solution basic CDC constraint network . say maximal m0i mi holdssolution (m0i )ni=1 M(mi ) = M(m0i ); say regular maximal(M(mi ))ni=1 canonical tuple rectangles.basic CDC network general many regular solutions, following result.Proposition 14. Let basic CDC network. Suppose basic RA network refines(), induced RA network . determine cubic time whether solutionalso satisfies . Moreover, solution, unique regular solution alsosatisfies . Furthermore, unique regular solution constructed cubic time.Proof. proof similar Proposition 12 work Liu et al., (2010). sketchgiven Appendix A.proof result, see region mi regular solution (mi )ni=1consists unit cells (i.e. rectangles form [i, i+1][j, j +1], i, j Z) canonicalsolution , i.e. region mi cell c, either c mi c mi = .basic CDC network , may exist exponentially many different basic RA networksrefine (). Hence, may exponentially many different regular solutions (see Figure 11(a) example network). However, verify solution, needprove solution special basic RA network refines () (Liu et al.,2010, Proposition 12).3 Therefore, consistency determined cubic time, and,consistent, regular solution constructed cubic time (Liu et al., 2010).3. special network called meet-free basic RA network work Liu et al., (2010).506fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS3. Joint Satisfaction Problempreparatory introduction basic notions essential results qualitative calculi,ready describe joint satisfaction problem.Let M1 M2 two qualitative calculi domain U. Suppose Si subclassMi (i = 1, 2). write JSP(S1 , S2 ) joint satisfaction problem (Gerevini & Renz, 2002; Li,2007) S1 S2 .Suppose = {vi Tij vj }ni,j=1 constraint network S1 , = {vi Dij vj }ni,j=1constraint network S2 involving variables. say ] instanceJSP(S1 , S2 ). joint satisfaction problem first considered RCC8 qualitative sizecalculus (identical Point Algebra Vilain & Kautz, 1986) Gerevini Renz (2002).Moreover, shown consistency joint network approximated polynomial bipath-consistency algorithm. Li Cohn (2012) recently showed bipath-consistencyequivalently expressed below.Definition 15. Let ] joint constraint network M1 M2 , = {vi Tij vj }ni,j=1= {vi Dij vj }ni,j=1 . say ] bi-closed Dij Tij nonemptybasic relation Tij , basic relation Dij , 1 i, j n (here regardrelation subset U U). bi-closed joint network ] bipath-consistentpath-consistent.Informally speaking, joint constraint network bi-closed basic relation givenrelation one calculi consistent corresponding relation calculus.simple example joint satisfaction problem, consider combination RACDC next subsection.3.1 Combination RA CDCLet R basic CDC relation. R , set-theoretic converse (or inverse) relation R(cf. (1)), may representable relation algebra CDC (Cicerone & Di Felice, 2004; Liuet al., 2010). is, R cannot represented union several basic CDC relations.sense, say CDC closed converse. Recently, Schneider et al. (2012) proposedvariant CDC, called Object Interaction Model (OIM), closed converse.two bounded regions a, b, OIM divides plane (l1 + 2) (l2 + 2) tilesextending edges M(a) M(b), l1 + 1 l2 + 1 numbers horizontaland, respectively, vertical lines. clear 1 l1 , l2 3 since edges M(a) M(b) maycoincide. OIM relation ab represented l1 l2 matrix (also written ab ) consideringexistence interior points and/or b corresponding bounded tiles. Let boundedtile. set entry corresponding matrix ab 0 interior pointeither b; set 1 (2, respectively) interior point (b, respectively)interior point b (a, respectively); set 3 otherwise. converse relationbasic OIM relation also basic OIM relation. particular, basic OIM relation ba bobtained swapping occurrences 1 2 ab .example, OIM relations regions Figure 3 (ii) (iii) respectively0 0 20 2ab = 1 0 2 , a0 b0 = 1 2 ,0 0 20 2507fiC OHN , L , L IU , & R ENZOIM relations regions Figure 4 respectivelyab0 2 2= 1 3 2 ,1 1 0a0 b00 2 2= 1 1 2 .1 1 0note regions a, b, a0 , b0 figures ab = a0 b0 . suggests OIMfiner grained CDC sense splits one basic CDC relation several OIM relations.Nevertheless, since CDC closed converse, need consider consistent pairs basicCDC relations order evaluate expressivity. comparing expressivity twocalculi way, see (ab , ba ) 6= (a0 b0 , b0 a0 ) Figure 4, (ab , ba ) = (a0 b0 , b0 a0 )Figure 3. shows OIM makes finer distinctions CDC describing scenariosgiven Figure 3(ii) (iii): saying west b, CDC differentiate eastboundary meets precedes west boundary b. following result shows OIMfiner CDC describing cardinal relations, essence combinationCDC RA.Theorem 16. (Li & Liu, 2014) two regions b, compute RA relationb, CDC relation b, CDC relation b OIM relation b,vice versa.words, basic OIM relation , exist two basic CDC relations , 0basic RA relation = 0 , i.e. two regions b, relationOIM relation b iff , 0 are, respectively, CDC relation b, CDC relationb a, RA relation b. basic CDC RA relations JEPD,choices , 0 , unique. following, call CDC relation inducedcall RA relation induced . Note case 0 (as relation b a) CDCrelation induced , OIM relation b a.consequence, following result.Proposition 17. Suppose = {vi ij vj }ni,j=1 basic OIM network ji = ijnni, j. Let = {vi ij vj }i,j=1 = {vi ij vj }i,j=1 , ij ij are, respectively, CDCrelation RA relation induced ij . consistent iff joint network ]consistent.Proof. Recall converse basic OIM relation also basic OIM relation., straightforward show = . Therefore, solutionsji = ijijijijjiexactly solutions ] .consequence Propositions 14 17Corollary 18. Let , given Proposition 17. basic RA networkrefines (), consistency determined cubic time. Moreover, consistent,unique regular solution also solution .far, described example JSP. next three sections,consider main task paper: JSP topological directional constraints.508fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS4. Combination Weak RCC8 RA Networkssection represent topological information weak RCC8 relations directional information RA relations. first consider interaction weak RCC8 RA relations,consider JSP basic constraints, and, lastly, consider JSP general.4.1 Interaction Weak RCC8 RA RelationsRelations different calculi may interact sense relation one calculus may intersectseveral relations second calculus. recall related definitions preliminaryresults obtained Li Cohn (2012).Definition 19. Let RCC8 relation RA relation. RA relation inducedRCC8 relation induced definedRA(T ) = { : basic RA relation 6= }(11)RCC8(D) = { : basic RCC8 relation 6= }.(12)Note joint network ] bi-closed ij RA(ij ) ij RCC8(ij ) i, j.easy see (cf. Li Cohn, 2012) RA(T ) = {RA({}) : }RA({DC}) RA({EC}) RA({PO}) RA({TPP}) RA({NTPP, EQ}),RA({PO}) RA({TPP }) RA({NTPP , EQ}),(13)(14)where, example, RA({EC}) RA({PO}) holds because, basic RA relation ,RA({EC}) M(a) M(b) 6= (a, b) , RA({PO}) M(a) M(b)non-degenerate rectangle (a, b) .Lemma 20. Let RCC8 relation RA relation. RCC8(D) relationb8 , Q8 , C8 ; RA(T ) relation H H relation Hb8 Q8 .intersection HProof. follows definitions RCC8(D) RA(T ) simple table look-upAppendix work Renz (2002).second statement apply relations C8 . example, consider = {NTPP,EQ}. relation C8 , RA(T ) = {d d, eq eq} outside H H.4.2 Combination Basic Networksconsider combination RCC8 RA. First show bipath-consistencysufficient consistency JSP(Brcc8 , Brec ) (Li & Cohn, 2012). Let = {vi ij vj }4i,j=1basic RA network induced four rectangles mi (i = 1, 2, 3, 4) illustrated below.Let = {vi ij vj }4i,j=1 basic RCC8 constraint network 12 = 34 = {EC}others {DC}. Clearly, satisfiable. Although ] bipath-consistent,satisfiable. because, otherwise, exists solution = (mi )4i=1 M(m1 )M(m2 ) =M(m3 ) M(m4 ) = {P } singleton. 12 = 34 = {EC} know P mi (i = 1, 2, 3, 4).contradicts 13 = {DC}.call point P configuration conflict point. general, followingdefinition.509fiC OHN , L , L IU , & R ENZDefinition 21 (conflict point). Let = {vi ij vj }ni,j=1 basic RCC8 network = {viij vj }ni,j=1 basic RA network. Suppose canonical solution . point Q calledconflict point mi exists j mi mj = {Q} ij = {EC}. write Ciset conflict points mi .Clearly, conflict point mi also corner point mi . implies mi mjmay one common conflict point. Moreover, suppose = (mi )ni=1 solution] M(mi ) = mi 1 n. conflict point mi containedmi . means Ci mi . consequence,Ci Cj 6= ij 6= {DC} (1 i, j n)(15)following theorem shows also sufficient.Theorem 22. Let = {vi ij vj }ni,j=1 basic RCC8 network = {vi ij vj }ni,j=1 basic RAnetwork. Suppose ] bipath-consistent. ] satisfiable iff (15) holds.Proof. necessity part clear. defer proof sufficiency part Appendix B.corollary, JSP(Brcc8 , Brec ) P.Corollary 23. basic RCC8 network basic RA network , consistency ]decided cubic time.Proof. Bipath-consistency ] checked cubic time. construct uniquecanonical rectangle solution quadratic time. conflict point set Ci also computedquadratic time. is, condition Theorem 22 checked cubic time.4.3 Large Tractable Subsetsb8 , C8 , Q8 , IA one maximalRecall RCC8 three maximal tractable subclasses Htractable subclass H, containing basic relations. subsection, aim extendb8 , C8 RCC8, large tractable subset H Hresult maximal tractable subsets HRA.end, need extend notion conflict points basic networks arbitrarynetworks. Recall 0-meet relations corner relations basic RA relations defined Definition 6.Definition 24 (common conflict point). Let = {vi Tij vj }ni,j=1 RCC8 network ={vi Dij vj }ni,j=1 RA network. say two variables vi , vj CCP (common conflict point)relation, written CCP(vi , vj ), Dij 0-meet (basic) relation Tij = {EC},Dij (possibly disjunctive) corner relation;510fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSexist i0 , j 0 Dii0 Djj 0 0-meet (basic) relations, Dij 0 , Di0 j Di0 j 0(possibly disjunctive) corner relations, Tii0 = Tjj 0 = {EC}.CCP(vi , vj ) vi0 , vj0 variables satisfy conditions, also write CCP(i, j :i0 , j 0 ) stress roles vi0 vj 0 .(i)(ii)Figure 5: Two joint constraint networks JSP(RCC8, RA), (i) (ii)Dij basic RA relation vi vj illustrated picture, T14 = T23 = {EC} unspecified RCC8 constraintsnon-basic RCC8 relation {DC, EC, PO}.(i) (ii)CCP(1, 2), CCP(1, 3), CCP(1, 4), CCP(2, 3), CCP(2, 4), CCP(3, 4).Examples shown Figure 5. Note basic networks, vi vjCCP relation iff Ci Cj nonempty, i.e. vi vj common conflict point.Definition 25. Let = {vi Tij vj }ni,j=1 RCC8 network = {vi Dij vj }ni,j=1 RAnetwork. say ] CCP-consistentCCP(vi , vj ) DC 6 Tij(16)holds 6= j. say joint network ] BC-consistent bipath-consistentCCP-consistent.general, vi vj CCP relation, (in realisation) vi vj share leastone corner point (of MBRs) common. Therefore, weak RCC8 algebra, cannotdisconnected, neither contained another non-tangential proper part. Notelatter statement also follows bi-closedness ] .Similar bipath-consistency algorithm (Gerevini & Renz, 2002), devise algorithm(Algorithm 1) enforcing BC-consistency. following theorem shows algorithmsound.Theorem 26. Suppose ] joint network RCC8 RA constraints, = {vi Tijvj }ni,j=1 = {vi Dij vj }ni,j=1 . O(n4 ) time, algorithm BC-C ONSISTENCY eitherfinds inconsistency transforms ] equivalent joint network 0 ] 0BC-consistent.511fiC OHN , L , L IU , & R ENZInput: joint network ] , = {vi Tij vj }ni,j=1 = {vi Dij vj }ni,j=1 .Output: false, empty constraint generated; BC-consistent joint network equivalent] , otherwise.Q {(i, k, j) | 6= j, k 6= i, k 6= j};(i indicates i-th variable ] .Analogously j k)Q 6=select delete path (i, k, j) Q;BC-R EVISION(i, k, j)Tij = Dij =return false;endQ Q {(i, j, k), (k, i, j) | k 6= i, k 6= j};endendFunction: BC-R EVISION(i, k, j)Input: three variables i, k jOutput: true, Tij Dij revised; false otherwise.Side effects: Tij Dji revised using operations w .Tij(Tij RCC8(Dij )) (Tji RCC8(Dji )) (Tik RCC8(Dik )) w (Tkj RCC8(Dkj ));Dij (Dij RA(Tij )) (Dji RA(Tji )) (Dik RA(Tik )) w (Dkj RA(Tkj ));CCP(i, j : k)Tij Tij \ {DC};endneither Tij Dij revisedreturn false;end;Dji DijTji Tij ;return true.Algorithm 1: BC-C ONSISTENCY, write CCP(i, j : k) represent situationexists another variable vl vl vk together evidence CCP(i, j).512fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSProof. because, iteratively use following updating rules either empty constraint occurs network becomes stable.Tij (Tij RCC8(Dij )) (Tji RCC8(Dji ))(Tik RCC8(Dik )) w (Tkj RCC8(Dkj ))(17)Dij (Dij RA(Tij )) (Dji RA(Tji )) (Dik RA(Tik )) w (Dkj RA(Tkj ))(18)Tij Tij \ {DC} CCP(i, j : k),(19)i, j, k represent variables vi , vj vk CCP(i, j : k) represents situationexists another variable vl vl vk together evidence CCP(i, j).triple, CCP(i, j : k) determined O(n) time subroutine BC-R EVISION(i, k, j)carried O(n) time. Since Tij set basic RCC8 relations Dij setbasic RA relations, (Tij , Dij ) revised constant number times. Therefore numberloops remains cubic, BC-C ONSISTENCY terminate O(n4 ) time.algorithm general complete. following lemma useful prove mainresult (Theorem 28), guarantee completeness algorithm RCC8 networksb8 RA networks H H.HLemma 27. Let = {vi Tij vj }ni,j=1 RCC8 network = {vi Dij vj }ni,j=1 RA network.b8 Q8 ] bipath-consistent. Assume canonicalSuppose Hconsistent scenario (cf. Theorem 3), consistent scenario . ]bipath-consistent.Proof. path-consistent basic networks, need show ]RA( ) RCC8( ) 6= j. Since basicbi-closed, i.e. ijijijijijijnonempty 6= j. (13) (14)relations, equivalent showing ijij). Therefore RA(T ) = RA( ),straightforward show RA(Tij ) = RA(ijijijijiji.e. ij ij nonempty.note result apply C8 . example, let = {NTPP, EQ}, ={d d, eq eq}. RCC8 relation NTPP inconsistent RA relation eq eq.Theorem 28. Let = {vi Tij vj }ni,j=1 RCC8 network = {vi Dij vj }ni,j=1 RA netb8 , H H. ] consistent BC-consistent.work. Suppose HProof. Recall RA network H H essence pair IA networks H.0-meet relation iff is;Lemma 8 know consistent scenario (i) ijij(ii) ij corner relation iff Dij consists corner relations. Let canonical consistentscenario . show ] consistent.Lemma 27 know ] bipath-consistent. next show satisfies (15),equivalent (16) basic constraints concerned. end, show CCP(i, j :i0 , j 0 ) holds ] holds ] . choice , know Tii0 =Tjj 0 = {EC} RA relations either 0-meet relations consist corner relations.Therefore, CCP(i, j : i0 , j 0 ) also holds ] . ] BC-consistent, know DC6= {DC} ] satisfies (15). Theorem 22, know ] ,ij . implies ijhence ] , consistent.513fiC OHN , L , L IU , & R ENZb8 Q8 H Hconsequence, know joint consistency problem Hsolved polynomial time.b8 , H H) JSP(Q8 , H H) P.Theorem 29. joint satisfaction problems JSP(Hb8 Q8 , H H.Proof. Suppose ] joint network Hfirst apply algorithm BC-C ONSISTENCY ] . empty relation occursprocess, ] inconsistent. Otherwise, suppose 0 ] 0 BC-consistent joint networkb8 Q8 0 H H. note that,equivalent ] . assert 0 still Hb8 (or Q8 ), RA relation H H, Lemma 31RCC8 relation Hb8 Q8 ;RCC8(D) relation HRA(T ) relation H H;b8 (or Q8 ).\ {DC} = {EC, PO, TPP, NTPP, TPP , NTPP , EQ} HBC-C ONSISTENCY uses rules (17)-(19) update relations, RCC8 relationb8 (or Q8 ), RA relation 0 remains HH. consistency 0 ]00 remains Hfollows Theorem 28.property proof theorem hold C8 . remains openJSP(C8 , H H) tractable (though important practical purposes since eitherb8 Q8 used backtrack find solution required).H5. Combination RCC80 RA Networkssection, represent topological information RCC80 relations directional informationRA relations. previous section shown that, certain tractable subclasses RCC8RA, JSP determined polynomial time, also show bipath-consistencyincomplete subclasses. reason lies two regions constrained DCmay common conflict point. RCC80 , situation exist anymore twodisjoint regions may still 0-dimensional intersection. section show that, RCC80 ,bipath-consistency alone sufficient show consistency joint network ]H8 C8 H H.case weak RCC8, first consider interaction RCC80 RA relations,consider consistency joint basic networks, and, lastly, consider general case.Similar Definition 19, following definition.Definition 30. Let RCC80 relation RA relation. RA relation inducedRCC80 relation induced definedRA(T ) = { : basic RA relation 6= }0RCC8 (D) = { : basic RCC80 relation 6= }.easy see RA(T ) = {RA({}) : }(20)(21)RA({DC}) RA({EC}) RA({PO}) RA({TPP}) = RA({NTPP}) RA({EQ}).RA({PO}) RA({TPP }) = RA({NTPP }) RA({EQ}).514fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSNote RA({TPP}) = RA({NTPP})= {s, d, f, eq} {s, d, f, eq}.RCC80 non-tangential proper part region may MBR a. example,star region Figure 6 non-tangential proper part MBR RCC80 .Lemma 31. Let RCC80 relation RA relation. RCC80 (D) relationb8 , Q8 , C8 ; RA(T ) relation H H relation Hb8intersection HQ8 C8 .particular, unlike case weak RCC8, RA({NTPP, EQ}) = {s, d, f, eq}{s, d, f, eq} relation H H.Figure 6: Basic regions control point P combination RCC80 RA.Theorem 32. Suppose basic RCC80 network basic RA network. ]consistent bipath-consistent.Proof. proof follows pattern combination weak RCC8 RA (Theorem 22), need replace basic regions around control point P star regions shown Figure 6, show three regions b, r, g around P , bNTPPrrNTPPg.following result RCC80 RA.b8 Q8 RCC80 , RA network. ]Theorem 33. Suppose network Hconsistent ] bi-closed, path-consistent, consistent.Proof. Assume canonical consistent scenario , consistent scenario) = RA( ) hence. Then, completely similar Lemma 27, show RA(ijijbi-closeness ] . consistent, know ] bipath-consistent,hence consistent Theorem 32.b8 , RA) polynoThe result shows consistency joint network JSP(Hb8 RA network.mially reduced determining consistency RCC8 network Hb8 , RA) separable problem. particular,sense, JSP(H515fiC OHN , L , L IU , & R ENZTheorem 34. RCC8 relations interpreted using strong connectedness, joint satisb8 , H H) JSP(Q8 , H H) P.faction problems JSP(HAgain, remains open whether result holds networks C8 RCC80 , eventhough case RA({NTPP, EQ}) = RA(TPP) relation H H.following section, consider combination RCC8 CDC constraints.6. Combination RCC8 CDC ConstraintsAlthough basic RCC8 networks basic CDC networks solved cubic time independently, interaction RCC8 CDC constraints makes joint satisfaction problemhard solve. section, first show joint satisfaction problem NP designing polynomial non-deterministic algorithm show NP-hard even basic constraints. shows JSP(Brcc8 , Bcdc ) NP-complete. consider three variantsJSP(Brcc8 , Bcdc ) obtained replacing RCC8 RCC80 and/or CDC OIM. Write Boimset basic OIM relations. show JSP(Brcc8 , Boim ) JSP(Brcc80 , Bcdc ) NP-complete,JSP(Brcc80 , Boim ) P.6.1 AlgorithmsLet instance joint basic RCC8 RCC80 network basic CDC OIM networkset variables. provide subsection algorithms determining consistency ] . key idea first showing ] consistent iff regular solutionRA consistent (see below) giving algorithms determining whetherregular solution.Suppose = (mi )ni=1 solution . Recall say = (mi )ni=1 regular solutionmaximal solution {M(mi )}ni=1 canonical tuple rectangles (cf. Dfn. 13). Noteregion regular solution union set cells introduced canonicaltuple rectangles.Definition 35. Let = {vi ij vj }ni,j=1 basic RCC8 network = {vi ij vj }ni,j=1 basicCDC network. Suppose = (mi )ni=1 regular solution . Write RA networkinduced m. say regular solution RA consistent exists solution] also satisfies .following lemma gives characterisation consistent joint basic networks.Lemma 36. Let = {vi ij vj }ni,j=1 basic RCC8 network = {vi ij vj }ni,j=1 basicCDC network. ] consistent iff regular solution RA consistent .Proof. sufficiency part clear definition. prove necessity part. Suppose =(ai )ni=1 solution ]. Write RA network induced a. also solution] . Hence unique regular solution also satisfies . Write = (mi )ni=1regular solution. clear regular solution RA consistent .lemma, determine consistency ] , need determine existenceregular solutions RA consistent . Suppose regular solution .next give necessary sufficient condition RA consistent .516fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSend, first fix notation terminology. region mi m, say cornerpoint P mi potential conflict point (in m) exactly one four cells incident Pcontained mi . example, grey region shown Figure 7 five potential conflict pointsPi (i = 1, ..., 5). Later show points may introduce conflicts hard resolveRCC8 constraints involved. Furthermore, denote Gi set cells containedmi , Ei set edges cells lie boundary mi , Ni set potential conflictpoints vi .Figure 7: Illustration potential conflict points.Lemma 37. Let = {vi ij vj }ni,j=1 basic RCC8 network = {vi ij vj }ni,j=1 basicCDC network. regular solution = (mi )ni=1 RA consistent , have:ij = TPP NTPP Gi Gj .(22)Proof. prove contradiction. Assume (vi TPPvj ) (vi NTPPvj ) constraintGi * Gj .Suppose (vs vt ) constraint , basic CDC relation represented 3 3Boolean matrix (dpq )1p,q3 . solution , 1 p, q 3dpq = 1 iff ms mpq6= ,(23)mpqdenotes one nine tiles generated MBR mt (cf. Fig. 3). Sinceregular solution Gs set cells contained ms , equivalent sayingdpq = 1 iffGs mpqcommon cell.(24)let g cell Gi \ Gj . g Gj , construction procedure regularsolutions (see Appendix A), exists constraint (vj 0 vk ) 0 = (d0uv ) g0pq = 0 hence,cell contained mpqk p, q. (24) g Gj knowpq00(23), mj mk = . Let (vi vk ) CDC constraint vi vk suppose00pq = 1 (24).00 = (d00uv ). g cell Gi mpqk ,RA consistent , exists solution = (ai )ni=1 ]M(ai ) = M(mi ). Since (vi TPPvj ) (vi NTPPvj ) , know ai aj . Furthermore,pqaj mj maximal solution . Therefore, ai mj . mpqk = akpqpq0000pqmj mk = , ai ak empty. shows (ai , ak ) since= 1.contradiction.NTPP constraints may furthermore exclude edges Ei nodes Nivaluation vi . Suppose vi NTPPvj constraint = (mi )ni=1 RA consistent. solution = (ai )ni=1 ] M(ai ) = M(mi ), ai NTPPaj aj mj517fiC OHN , L , L IU , & R ENZaj mj = . say, ai cannot touch edges nodes Ej Nj .characterise this, define[Ei Ei \ {Ej : vi NTPPvj },(25)[Ni Ni \ {Nj : vi NTPPvj }.(26)Since every region represented Boolean matrix, Gi , Ei , Ni calculatedpolynomial time. following proposition gives necessary sufficient conditionRA consistent .Lemma 38. Let = {vi ij vj }ni,j=1 basic RCC8 network = {vi ij vj }ni,j=1 basicCDC network. regular solution = (mi )ni=1 RA consistent iffvi TPPvj vi NTPPvj implies Gi Gj ,M( Ei ) = M(mi ) i,vi POvj implies Gi Gj 6= ,exists resolving function f , defined function V P( {N1 , , Nn })satisfying (27)-(29).f (vi ) Ni ,(27)vi ECvj Gi Gj 6= Ei Ej 6= f (vi ) f (vj ) 6= ,(28)vi DCvj f (vi ) f (vj ) = .(29)Proof. begin necessity part. Suppose RA consistent . definitionexists solution = (ai )ni=1 ] M(ai ) = M(mi ). first conditionproven Lemma 37. second condition, M(ai ) = M(mi ), ai mi , knowai nonempty intersection one unit edge cell lies top (bottom, leftmost,rightmost) edge M(mi ). unitedge clearly Ei . Furthermore, provenedge Ei , thus M( Ei ) = M(mi ). following two conditions guaranteePO, EC constraintssatisfied violating DC constraints. third conditionfollows directly ai Gi ai POaj . last condition, define resolving functionf f (vi ) = {P Ni : P ai }. straightforward prove f satisfies (27)-(29).sufficiency part, construct solution ] . procedure quite similargiven Theorem 22 Appendix B. vi , choose control point cell Gicontrol point edge Ei . vi POvj , choose control pointcommon cell Gi Gj . vi ECvj , choose control point common cellGi Gj 6= , common edge Ei Ej 6= , f (vi ) f (vj ) resolvingfunction f . proven control points lead solution . Moreover,choice control points ensures regions also solution .Since conditions Lemma 38 verified nondeterministic polynomial algorithm,following theorem.Theorem 39. JSP(Brcc8 , Bcdc ) NP.518fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSProof. Suppose ] instance JSP(Brcc8 , Bcdc ). devise nondeterministic polynomialalgorithm follows. first guess basic RA network consistent basic CDCnetwork , compute regular solution satisfies cubic time, guessresolving function f satisfies conditions (27)-(29), check whether consistentbasic RCC8 network Lemma 38 via f . case, algorithm returns true.Otherwise, algorithm returns false. shows JSP(Brcc8 , Bcdc ) NP.Since OIM network one regular solution, Lemma 38Corollary 40. JSP(Brcc8 , Boim ) NP.interpret topological constraints RCC80 , following simplified conditiondetermining whether regular solution RA consistent .Proposition 41. Suppose basic RCC80 network, basic CDC network,set variables V = {v1 , , vn }. regular solution RA consistentiffvi TPPvj vi NTPPvj implies Gi Gj ,vi POvj implies Gi Gj 6= ,vi ECvj implies either Gi Gj Ei Ej nonempty.Moreover, conditions checked polynomial time.Proof. proof similar Lemma 38. resolving function irrelevant,RCC80 conflict points longer evidence EC relations, point P (a set X, respectively) regarded evidencerelation (aECb) P 0 b (X b), respectively.Note require M( Ei ) = M(mi ), RCC8 possible aNTPPbM(a) = M(b), see Figure 6 illustration.directly leads following two results.Theorem 42. JSP(Brcc80 , Bcdc ) NP.Proof. proof similar Theorem 39. Suppose ] instance JSP(Brcc8 , Bcdc ).first guess basic RA network consistent construct regular solutionsatisfies check whether RA consistent Proposition 41.Recall OIM essence combination CDC RA, basic OIM networkconsistent iff two component CDC RA networks consistent (see Proposition 17).case RCC80 combined OIM, following tractability result.Theorem 43. JSP(Brcc80 , Boim ) P.Proof. algorithm JSP(Brcc80 , Boim ) contains three steps. Suppose ] instancen variables. first step decide whether independently consistent.so, return false; otherwise, construct unique regular solution . achievedO(n3 ) time. calculate Gi Ni , done O(n4 ) time. third stepdecide whether RA consistent according Proposition 41, doneO(n4 ) time. Therefore, consistency ] determined O(n4 ) time and, hence,JSP(Brcc80 , Boim ) P.519fiC OHN , L , L IU , & R ENZnext subsection, show JSP(Brcc8 , Bcdc ), JSP(Brcc80 , Bcdc ), JSP(Brcc8 , Boim )NP-hard.6.2 NP-Hardness ResultsRecall proof Theorem 39, guess twice determining consistencyinstance ] JSP(Brcc8 , Bcdc ), basic RA network consistent ,resolving function f satisfies (27)-(29) (see Proposition 38). subsectiondevise two polynomial reductions known NP-hard problems JSP(Brcc8 , Bcdc ) exploitingtwo facts.Theorem 44. JSP(Brcc8 , Bcdc ) NP-hard.Proof. first reduction 3-SAT JSP(Brcc8 , Bcdc ). quite complicated,defer construction Appendix C. explain problem NP-hard.3-SAT instance , construct instance ] JSP(Brcc8 , Bcdc )RCC8 constraint either DC EC constraint. Furthermore, showunique regular solution RA consistent consistent.intractability caused potential conflict points regular solution, which, together EC DC constraints, may introduce conflicts hard resolve.Lemma 38, satisfy EC constraint vi ECvj , need check whether mi mj sharecell, else edge, else corner point. last case, proven without much difficultypoints shared mi mj exactly points Ni Nj . Therefore, mi mj sharecell edge, evidence point constraint vi ECvj chosen Ni Nj .turns choosing evidence points EC constraints violatingDC constraints NP-hard.second reduction Graph 3-colouring problem JSP(Brcc8 , Bcdc ). defer construction Appendix D. graph G, construct instance G ] G JSP(Brcc8 , Bcdc ).reduction differs first one exploit intractability findingresolving function. fact, vi ECvj constraint, regular solution G ,either mi mj share cell edge, mi mj disjoint (in case RAconsistent ). say, resolving functions effect RA consistency m.reduction based fact G exponentially many regular solutions,general way test polynomial time (unless P = NP).Note first reduction shown unique regular solution RAconsistent consistent, 3-SAT instance ] instanceJSP(Brcc8 , Bcdc ) defined reduction. Write basic RA network inducedparticular regular solution . easy see ] consistent iff ] ]consistent. words, reduction 3-SAT also reduction JSP(Brcc8 , Boim ).Corollary 45. JSP(Brcc8 , Boim ) NP-hard.Similarly, second reduction also reduction JSP(Brcc80 , Bcdc ).JSP(Brcc8 , Bcdc ) instance graph G uses DC EC constraints, two variables required EC, MBRs 0-meet, MBRs may overlap.Corollary 46. JSP(Brcc80 , Bcdc ) NP-hard.520fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSNP-hardness results Theorem 39, Corollary 40, Theorem 42, knowTheorem 47. joint satisfaction problems JSP(Brcc8 , Bcdc ), JSP(Brcc80 , Bcdc ), JSP(Brcc8 ,Boim ) NP-complete.7. Conclusionpaper, investigated computational complexity reasoning topologicalrelations cardinal directions extended spatial objects. used two different interpretations well-known RCC8 algebra representing topological information, useRectangle Algebra (RA) cardinal direction calculus (CDC) describe directional information. shown joint satisfaction problems decidable remain NPinterpretations topological directional information. importantly, shownconsistency problem P basic (weak strong) RCC8 basic RA constraintsinvolved, topological constraints basic strong RCC8 constraints directionalconstraints jointly represented basic RA CDC constraints.related work reported work Sistla Yu (2000), Li (2007), LiCohn (2012), small fragments RA used express directional information.results represent large step towards applicability qualitative spatial reasoning techniquesreal-world problems. particular tractable results promising enable efficientreasoning important calculi. also means efficient reasoning importantpotential application, developers aim representing directional information using RA(or together CDC) instead CDC alone and/or representing topological information usingRCC80 instead RCC8. results combining RCC8 CDC/OIM importanttheoretical point view first formal results combination. demonstrated using concrete application scenario, results also practical significance,combined information consider easily extracted cases computer vision usedobtaining spatial information.Acknowledgmentsthank anonymous reviewers invaluable comments detailed suggestions.first author also thanks University Technology Sydney funding visit SydneyAdjunct Professor. work partially supported Australian Research Council (GrantNo.s DP120104159, DP120103758 FT0991917), National Natural Science FoundationChina (Grant No. 61228305), EU funded projects RACE (FP7-ICT-287752) STRANDS(FP7-ICT-600623).Appendix A. Realisation Basic CDC Networksdescribe cubic algorithm given work Zhang et al., (2008), Liu et al., (2010).Given basic CDC network, first, compute canonical solution induced (possibly nonbasic) RA network. Next, remove cells violate constraints rectangle.Third, check whether obtained valid solution. following, givedetailed description running example illustrated Table 4 Figure 8.521fiC OHN , L , L IU , & R ENZ(1, 2)(1, 3)(2, 3)011011110ij010000100000000000000000000ji110010001xij yijxij yijoooo{m, b} fib fioioi110000011Table 4: Example solving basic CDC network.Step 1. Compute induced RA network 0 .Step 2. Refine 0 basic RA network = {vi (xij yij )vj }ni,j=1 setting xij = xij \{m, mi}yij = yij \{m, mi}. unsatisfiable, neither . Suppose satisfiable constructcanonical solution = (mi )ni=1 (cf. Figure 8).Figure 8: Illustration Step 3: Deriving solution canonical solution .Step 3. step tries find solution = (mi )ni=1 basic CDC networkM(mi ) = M(mi ). Recall basic CDC relation ij represented 3 3 Boolean matrix((ij )xy ). solution, mi (mj )xy = holds every (ij )xy = 0, (mj )xy onenine tiles generated M(mj ) (cf. Figure 3).make solution ,means,need exclude impossible cells mi . Set Ti = {(mj )xy : (ij )xy = 0}nj=1 . Let miclosure mi \ Ti (cf. Figure 8 left).Step 4. last step checks whether = (mi )ni=1 solution . solution,must regular solution; not, assert solution all.note regular solutions may exist (cf. Figure 8 right). getrepeating Steps 2 4 using every possible refinement 0 .Appendix B. Proof Theorem 22need show sufficiency part. Similar cubic construction method basic RCC8 constraints (cf. Li, 2006b Section 2.2.1 paper), construct solution= (mi )ni=1 additional requirement M(mi ) = mi 1 n,{m1 , ..., mn } canonical solution . Recall coordinates corner pointrectangle mi integral. Assuming ntpp-level l(i) computed 1 n,next describe construction detail.522fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSStep 1. Selection Control Pointsvi select set control points Xi . First all, corner point Ci control pointvi . select one (non-integral) point edge mi put four pointsXi . Then, j > ij = EC PO select point Pij mi mj (which nonemptybipath-consistency ] ), put Xi Xj . Note mi mjcould single point, line segment, rectangle. choosing Pij mi mj ,require Pij fresh point chosen corner pointrectangle (unless mj singleton set). write P set control points.Step 2. Basic Regions Associated Control Pointscontrol point Q, construct series sectors {q i,k : k = 1, , 4}ni=1 seriessquares {q (i) }ni=1 (see Figure 9). call basic regions associated Q. Noteuse upper case letter denote control point, use corresponding lower case letter (withindices) denote basic regions. sectors chosen way allows us distinguishfour connecting regions cases Q corner point (such point P Figure3).sectors completely fill disks.Figure 9: Basic regions control point Q.two different control points, require outermost squares disjoint. Furthermore, basic region must small enough crossed border miQ boundary point.Step 3. Region Constructioncontrol point Q, set q =a1ia2ia3ia4iS4k=1 qi,k .Let[= mi {q : Q Xi }[= a1i (mi {q j : ij = PO, Q Xi Xj })[= a2i {a2j : ji = TPP ji = NTPP}[= a3i {q (l(i)) : ji = NTPP, Q a3j }Set mi = a4i = (mi )ni=1 . easy prove satisfies RCC8 constraints .example, suppose (vi DC vj ) constraint . (15) holds, know vi vj sharecommon conflict point, i.e. Ci Cj = . Due choice control points vi vj , know523fiC OHN , L , L IU , & R ENZXi Xj also empty. easy show mi mj empty hence DC constraintsatisfied.show also satisfies , need prove M(mi ) = mi i. clear a1ia2i subsets mi . choice Xi , know mi = M(a1i ) = M(a2i ). ji = TPPNTPP, mj mi bipath-consistency. implies M(a3i ) = mi . Furthermore,ji = NTPP, (mj , mi ) bipath-consistency. control point Qa3j mj , Q also interior mi . Therefore, choice basic regions, knowoutmost square q (n) Q, hence q (l(i)) , contained mi . Therefore, M(a4i ) = mi . provessolution ] .Appendix C. Reduction 3SAT JSP(Brcc8 , Bcdc )VnLet =i=1 cj 3SAT instance involving n propositional variables {pk }k=1 clauses.Assume j-th clause cj qi1 qi2 qi3 , qij literal {pk }nk=1 {pk }nk=1 .construct JSP(Brcc8 , Bcdc ) instance ] choose particular regular solutionsatisfiable iff RA consistent .three types spatial variables ] : auxiliary variables (called grid variables)used fix relative locations variables, variables simulate propositionalvariables, variables simulate propositional clauses.C.1 Grid Variablesintroduce 10 n grid variables Gij (1 2n, 1 j 5). CDC constraintsvariables specified Figure 10 (left). RCC8 relation two grid variablesGij , Gi0 j 0 EC 4-neighbours, i.e. {|i i0 |, |j j 0 |} = {0, 1}. EC constraintsmake sure gap MBRs two neighbouring grid variables. impliesone regular solution .Grid variables mainly used locate spatial variables. new variable v gridvariable Gij , say v occupies Gij v M(Gij ) nonempty, MBR M(Gij ), i.e.M(v M(Gij )) = M(Gij ).Figure 10: Grid spatial variables propositional variables.C.2 Spatial Variables Propositional Variablespropositional variable pi , four spatial variables Ai , Bi , Ci Di introducedoccupies two grid cells, empty intersection interiors MBRs524fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSgrid variables. corresponding grid cells illustrated Figure 10 (right). Take Aiexample. assigning CDC constraints Ai grid variables, require Aioccupies G2i1,1 G2i1,4 , disjoint MBRs grid variables. easysee Ai Bi contains two points, viz. Pi+ Pi , Ci Di .topological constraints, require Ai ECBi Ci ECDi , constraintsDC. EC constraints imply Ai Bi Ci Di nonempty. hand,since Ai DCCi , conclude Ai Bi must share one Pi+ Pi , CiDi share one.C.3 Spatial Variables Propositional Clausesclause (qj1 qj2 qj3 ) , introduce two new spatial variables Ej Fj ,occupy three grid cells. precise occupied grid cells set according variables signsqjk . Figure 11 gives example illustrate construction, assume qj1 = pi1 , qj2 =pi2 , qj3 = pi3 . topological constraints, set constraint Ej FjEC, set constraints DC. implies Ej Fj contains least one pointPi1 , Pi+2 , Pi+3 . claim case Ai1 Bi1 = {Pi1 }, Ai2 Bi2 = {Pi+2 }Ai3 Bi3 = {Pi+3 }. Otherwise, DC constraint, e.g. Ai1 Ej , violated.C.4 Regular Solution May RA ConsistentFigure 11: Spatial variables clauses.finished construction. Note always satisfiable exponentially many regular solutions (as may may gap 4-neighbouringgrid variables). However, EC constraints 4-neighbouring grid variables implyregular solution gap 4-neighbouring grid variables RAconsistent . denote regular solution m.next show consistent iff ] consistent. Suppose ] solution a.define assignment : {pi }ni=1 {true, f alse} (pi ) = true iff Ai Bi = {Pi+ }a. verify satisfies . hard, suppose assignment satisfies. prove ] solution. idea introduce instance JSP(Brcc8 , Brec ),two spatial variables A+Ai instead Ai (also Bi , Ci , Di ), threekvariables Ej (1 k 3) instead Ej (also Fj ). RA constraints set accordingFigure 10 Figure 11, RCC8 constraints set . provennew joint network satisfies (15), solution obtained cubic time. solution] obtained merging related regions (e.g. merging A+Ai Ai ).525fiC OHN , L , L IU , & R ENZverification straightforward. Therefore, satisfiable iff ] satisfiable, thussatisfiable iff RA consistent .Appendix D. Reduction Graph 3-Colouring JSP(Brcc8 , Bcdc )Suppose G = (V, E) graph. construct instance G ] G JSP(Brcc8 , Bcdc ) follows.node vi V , construct gadget 10 spatial variables: uki (k = 1, 2, , 8), xi0yi . first describe CDC constraints. basic CDC constraints uki ukispecified Figure 12 (i). example, G contains following basic CDC constraints0 0 00 0 01 1 00 0 0u1i 1 0 0 u2i , u2i 0 0 1 u1i , u2i 1 1 0 u7i , u7i 0 1 1 u2i .0 0 00 0 00 0 00 1 1Note that, induced RA constraint u2l+1u2l+2l = 0, 1, 2 (b m) eq.basic CDC constraints xi yi specified Figure 12 (ii), i.e.0 0 00 0 0xi 1 1 0 yi , yi 0 1 1 xi .0 0 00 0 0CDC constraints concerning xi , yi uki specified follows.0xi 001xi 000kui 000yi 001yi 000uki 000 0 00 01 0 u1i , xi 1 0 0 u2i ,0 0 10 10 01 0 01 0 u5i , xi 1 0 0 u6i ,0 00 0 00 01 0 xi (k 6= 6),0 00 00 0 00 1 u1i , yi 0 1 0 u2i ,0 10 0 10 01 0 00 1 u5i , yi 0 1 0 u6i ,0 00 0 00 01 0 yi (2 k 8),0 01xi 001xi 0006ui 001yi 001yi 000u1i 101 0 00 01 0 u3i , xi 1 0 0 u4i ,0 0 10 10 00 0 uki (k = 7, 8),0 10 00 1 xi ;0 00 01 0 00 1 u3i , yi 0 1 0 u4i ,0 10 0 10 00 0 uki (k = 7, 8),0 10 00 0 yi .0 0Figure 12 (ii) illustrates regular solution uki , xi yi , u1i meets u2igap u3i u4i , u5i u6i . note total eight differentregular solutions network restricted gadget vi .526fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONS(i)(ii)Figure 12: Illustrations CDC constraints: (i) constraints uki (k = 1, 2, , 8); (ii)constraints relating xi , yi . Note use dashed squares denote variables u7i u8i ,connect two variables. Note xi yi three disjoint parts.527fiC OHN , L , L IU , & R ENZRCC8 constraint two variables either EC DC. require xi ECyi .realisable u2l+1meets u2l+2x-direction least one l {0, 1, 2}.4 use factmimic node vi V coloured one three colours. RCC8 constraintsremaining pairs variables specified DC. 5gadgets nodes V horizontally aligned, illustrated Figure 13.Figure 13: Illustrations gadgets nodes VFigure 14: Illustrations gadget edge ek = (vi , vj ), dashed lines suggest corresponding edges aligned according proper CDC constraints.devise gadgets edges graph G. Let ek = (vi , vj ) edge E.0 , w 1 , w 2 w 3 well constraintscolour l {0, 1, 2}, introduce four variables wk,lk,lk,lk,l2l+22l+12l+2guarantee u2l+1cannotmeetuumeetsu,correspondsnodes vijjvj cannot colour l (because ek edge G). CDC constraints specified1 meets w 3 iff u2l+1 meets u2l+2 , w 3 meets w 2 iff u2l+1Figure 14. note wk,ljk,lk,lk,l3 ) containedmeets u2l+2,x-direction.CDCconstraintsshowM(wjk,l0 ). implies either gap w 1 w 3 gap w 3M(wk,lk,lk,lk,l2 . words, w 1 meets w 3 w 3 meets w 2 cannot happen simultaneously.wk,lk,lk,lk,lk,l4. one l u2l+1meets u2l+2x-direction, always choose smallest lcolour node vi .5. Note u1i DCu2i together CDC relations u1i u2i necessarily imply u1iprecede u2i x-direction. is, u1i could still meet u2i x-direction case.528fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSconstraints enforcing dashed lines know u2l+1cannot meet u2l+2u2l+1meets u2l+2,jjvice versa.Note need complete . unspecified CDC constraints easilydeduced figures, unspecified RCC8 constraints DC.hard verify graph G 3-colourable iff joint network G ] G consistent.idea that, : V {0, 1, 2} 3-colouring G, may construct solutionG ] G RA relation u2l+1u2l+2eq (i) = l, b eqotherwise. guarantees xi yi realisable. fact incident nodes Gr realisable. hand, ] satisfiable,colour implies wk,lGGleast one pair {(u1i , u2i ), (u3i , u4i ), (u5i , u6i )} RA relation eq (otherwise,xi ECyi violated). Define : V {0, 1, 2} (vi ) = min{l : u2l+1eq u2l+2}.r realisable.verified 3-colouring G due fact wk,lcompleted reduction Graph 3-Colouring JSP(Brcc8 , Bcdc ). noteEC constraints also interpreted terms RCC80 . means G ] Galso regarded instance JSP(Brcc8 , Bcdc ). Therefore, also provided reductionGraph 3-Colouring JSP(Brcc80 , Bcdc ).ReferencesAllen, J. F. (1983). Maintaining knowledge temporal intervals. Communications ACM,26(11), 832843.Balbiani, P., Condotta, J.-F., & Farinas del Cerro, L. (1999). new tractable subclass rectangle algebra. Dean, D. (Ed.), Proceedings Sixteenth International Joint ConferenceArtificial Intelligence (IJCAI-99), pp. 442447. Morgan Kaufmann.Bodirsky, M., & Kara, J. (2010). complexity temporal constraint satisfaction problems. J.ACM, 57(2).Borgo, S., Guarino, N., & Masolo, C. (1996). pointless theory space based strong connectioncongruence. Aiello, L. C., Doyle, J., & Shapiro, S. C. (Eds.), KR, pp. 220229. MorganKaufmann.Chen, J., Cohn, A. G., Liu, D., Wang, S., Ouyang, J., & Yu, Q. (2013). survey qualitativespatial representations. Knowledge Engineering Review, FirstView, 131.Cicerone, S., & Di Felice, P. (2004). Cardinal directions spatial objects: pairwiseconsistency problem. Information Sciences, 164(1-4), 165188.Cohn, A. G., & Renz, J. (2008). Qualitative spatial reasoning. van Harmelen, F., Lifschitz, V., &Porter, B. (Eds.), Handbook Knowledge Representation. Elsevier.Cohn, A. G., Renz, J., & Sridhar, M. (2012). Thinking inside box: comprehensive spatialrepresentation video analysis. KR, pp. 588592.Cohn, A. G., & Varzi, A. C. (1999). Modes connection. Freksa, C., & Mark, D. M. (Eds.),COSIT, Vol. 1661 Lecture Notes Computer Science, pp. 299314. Springer.Davis, E. (2013). Qualitative spatial reasoning interpreting text narrative. Spatial Cognition& Computation, 13(4), 264294.529fiC OHN , L , L IU , & R ENZDavis, E., Gotts, N. M., & Cohn, A. G. (1999). Constraint networks topological relationsconvexity. Constraints, 4(3), 241280.Duntsch, I., Wang, H., & McCloskey, S. (2001). relation-algebraic approach RegionConnection Calculus. Theoretical Computer Science, 255, 6383.Egenhofer, M. J., & Mark, D. M. (1995). Naive geography. Frank, A., & Kuhn, W. (Eds.),COSIT-95, pp. 115. Springer.Falomir, Z. (2012). Qualitative distances qualitative description images indoor scenedescription recognition robotics. AI Communications, 25(4), 387389.Freksa, C. (1992). Temporal reasoning based semi-intervals. Artificial Intelligence, 54(1), 199227.Gabelaia, D., Kontchakov, R., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2005). Combiningspatial temporal logics: Expressiveness vs. complexity. Journal Artificial IntelligenceResearch, 23, 167243.Ge, X., & Renz, J. (2013). Representation reasoning general solid rectangles. Rossi,F. (Ed.), IJCAI. IJCAI/AAAI.Gerevini, A., & Renz, J. (2002). Combining topological size information spatial reasoning.Artificial Intelligence, 137(1), 142.Gerevini, A., & Nebel, B. (2002). Qualitative spatio-temporal reasoning rcc-8 allensinterval calculus: Computational complexity. ECAI, pp. 312316.Goyal, R., & Egenhofer, M. (1997). direction-relation matrix: representation directionsrelations extended spatial objects. Annual Assembly Summer RetreatUniversity Consortium Geographic Information Systems Science.Goyal, R., & Egenhofer, M. (2001). Similarity cardinal directions. Jensen, C., Schneider, M.,Seeger, B., & Tsotras, V. (Eds.), Proceedings 7th International Symposium AdvancesSpatial Temporal Databases (SSTD-01), pp. 3658. Springer.Hirsch, R. (1999). finite relation algebra undecidable network satisfaction problem. LogicJournal IGPL, 7(4), 547554.Jonsson, P., & Krokhin, A. A. (2004). Complexity classification qualitative temporal constraintreasoning. Artificial Intelligence, 160(1-2), 3551.Kontchakov, R., Nenov, Y., Pratt-Hartmann, I., & Zakharyaschev, M. (2011). decidabilityconnectedness constraints 2D 3D Euclidean spaces. IJCAI, pp. 957962.Li, S. (2006a). Combining topological directional information: First results. Lang, J., Lin, F.,& Wang, J. (Eds.), Proceedings First International Conference Knowledge Science,Engineering Management (KSEM-06), pp. 252264. Springer.Li, S. (2006b). topological consistency realization. Constraints, 11(1), 3151.Li, S. (2007). Combining topological directional information spatial reasoning. Veloso,M. (Ed.), Proceedings 20th International Joint Conference Artificial Intelligence(IJCAI-07), pp. 435440. AAAI.Li, S., & Cohn, A. G. (2012). Reasoning topological directional spatial information.Computational Intelligence, 28(4), 579616.530fiR EASONING OPOLOGICAL C ARDINAL IRECTION R ELATIONSLi, S., & Liu, W. (2014). Cardinal directions: comparison direction relation matrix objectsinteraction matrix. International Journal Geographical Information Science, acceptedpublication, doi: http://dx.doi.org/10.1080/13658816.2014.954580.Li, S., Liu, W., & Wang, S. (2013). Qualitative constraint satisfaction problems: extendedframework landmarks. Artificial Intelligence, 201, 3258.Ligozat, G. (1994). Tractable relations temporal reasoning: pre-convex relations. ECAI-94.Workshop Spatial Temporal Reasoning, pp. 99108.Ligozat, G., & Renz, J. (2004). qualitative calculus? general framework. Zhang, C.,Guesgen, H., & Yeap, W.-K. (Eds.), PRICAI-04, pp. 5364. Springer.Liu, W., & Li, S. (2011). Reasoning cardinal directions extended objects: NPhardness result. Artificial Intelligence, 175(18), 21552169.Liu, W., Li, S., & Renz, J. (2009). Combining RCC-8 qualitative direction calculi: Algorithmscomplexity. Boutilier, C. (Ed.), Proceedings Twenty-first International JointConference Artificial Intelligence (IJCAI-09), pp. 854859.Liu, W., Zhang, X., Li, S., & Ying, M. (2010). Reasoning cardinal directions extended objects. Artificial Intelligence, 174(12-13), 951983.Nebel, B. (1995). Computational properties qualitative spatial reasoning: First results. KI-95,pp. 233244, Berlin, Germany. Springer-Verlag.Nebel, B., & Burckert, H.-J. (1995). Reasoning temporal relations: maximal tractablesubclass Allens interval algebra. Journal ACM, 42(1), 4366.Randell, D. A., Cui, Z., & Cohn, A. G. (1992). spatial logic based regions connection.KR-92, pp. 165176.Renz, J. (1999). Maximal tractable fragments Region Connection Calculus: completeanalysis. Dean, D. (Ed.), Proceedings Sixteenth International Joint ConferenceArtificial Intelligence (IJCAI-99), pp. 448454. Morgan Kaufmann.Renz, J. (2002). Qualitative spatial reasoning topological information, Vol. 2293 LectureNotes Artificial Intelligence. Springer-Verlag, Berlin, Germany.Renz, J., & Nebel, B. (1999). complexity qualitative spatial reasoning: maximaltractable fragment Region Connection Calculus. Artificial Intelligence, 108, 69123.Schneider, M., Chen, T., Viswanathan, G., & Yuan, W. (2012). Cardinal directions complexregions. ACM Transactions Database Systems, 37(2), 8:18:40.Schockaert, S., & Li, S. (2012). Convex solutions RCC8 networks. ECAI, pp. 726731.Schockaert, S., & Li, S. (2013). Combining RCC5 relations betweenness information.IJCAI, pp. 10831089.Shi, H., Jian, C., & Krieg-Bruckner, B. (2010). Qualitative spatial modelling human route instructions mobile robots. ACHI, pp. 16.Sistla, A., & Yu, C. (2000). Reasoning qualitative spatial relationships. Journal AutomatedReasoning, 25(4), 291328.Skiadopoulos, S., & Koubarakis, M. (2005). consistency cardinal direction constraints.Artificial Intelligence, 163(1), 91135.531fiC OHN , L , L IU , & R ENZSridhar, M., Cohn, A. G., & Hogg, D. C. (2011). video RCC8: Exploiting distancebased semantics stabilise interpretation mereotopological relations. COSIT, pp.110125.Vilain, M. B., & Kautz, H. A. (1986). Constraint propagation algorithms temporal reasoning.AAAI, pp. 377382.Whitehead, A. (1929). Process Reality: Essay Cosmology. Cambridge University Press,Cambridge.Wolfl, S., & Westphal, M. (2009). combinations binary qualitative constraint calculi.Boutilier, C. (Ed.), Proceedings Twenty-first International Joint Conference ArtificialIntelligence (IJCAI-09), pp. 967972.Wolter, D., & Wallgrun, J. (2012). Qualitative spatial reasoning applications: New challengessparq toolbox. Hazarika, M. (Ed.), Qualitative Spatio-Temporal RepresentationReasoning: Trends Future Directions, pp. 336362.Wolter, D., Dylla, F., Wolfl, S., Wallgrun, J. O., Frommberger, L., Nebel, B., & Freksa, C. (2008).Sailaway: Spatial cognition sea navigation. KI, 22(1), 2830.Wolter, F., & Zakharyaschev, M. (2000). Spatial reasoning RCC-8 boolean region terms.ECAI, pp. 244250.Zhang, P., & Renz, J. (2014). Qualitative spatial representation reasoning angry birds:extended rectangle algebra. KR.Zhang, X., Liu, W., Li, S., & Ying, M. (2008). Reasoning cardinal directions: efficientalgorithm. Fox, D., & Gomes, C. (Eds.), Proceedings Twenty-Third AAAI ConferenceArtificial Intelligence (AAAI-08), pp. 435440. AAAI.532fiJournal Artificial Intelligence Research 51 (2014) 1-70Submitted 2/14; published 9/14Cooperative Monitoring Diagnose Multiagent PlansRoberto MicalizioPietro Torassomicalizio@di.unito.ittorasso@di.unito.itDipartimento di Informatica,Universita di Torinocorso Svizzera 185, 10149 - Torino, ItalyAbstractDiagnosing execution Multiagent Plan (MAP) means identifying explainingaction failures (i.e., actions reach expected effects). Current approachesMAP diagnosis substantially centralized, assume action failures independent other.paper, diagnosis MAPs, executed dynamic partially observableenvironment, addressed fully distributed asynchronous way; addition, actionfailures longer assumed independent other.paper presents novel methodology, named Cooperative Weak-Committed Monitoring (CWCM), enabling agents cooperate monitoring actions. Cooperation helps agents cope scarcely observable environments: agentcannot observe directly acquired agents. CWCM exploits nondeterministic action models carry two main tasks: detecting action failures buildingtrajectory-sets (i.e., structures representing knowledge agent environment recent past). Relying trajectory-sets, agent able explainaction failures terms exogenous events occurred executionactions themselves. cope dependent failures, CWCM coupled diagnosticengine distinguishes primary secondary action failures.experimental analysis demonstrates CWCM methodology, togetherproposed diagnostic inferences, effective identifying explaining action failureseven scenarios system observability significantly reduced.1. IntroductionMultiagent Plans (MAPs) adopted many applicative domains, Web servicesservice robots, whenever interactions among cooperative agents organizedadvance (i.e., planned), order reach acceptable efficiency level execution;consider instance, orchestrated Web services (Yan, Dague, Pencole, & Cordier, 2009),assembling tasks (Heger, Hiatt, Sellner, Simmons, & Singh, 2005; Sellner, Heger, Hiatt,Simmons, & Singh, 2006), service robotics (Micalizio, Torasso, & Torta, 2006). MAPstherefore characterized cooperative team agents perform actions concurrentlyorder achieve common global goal.use MAPs real-world scenarios, however, cope critical issue:plan actions deviate expected nominal behaviors due occurrence(unpredictable) exogenous events. deviations typically considered plan failuressince prevent agents reach goals. Indeed, although MAPs versatilec2014AI Access Foundation. rights reserved.fiMicalizio & Torassosystems, also particularly fragile: failure action easily propagatesystem causing failures actions, even assigned different agents.order make execution MAP robust (i.e., tolerant least exogenousevents), therefore important detect isolate action failures, provide humanuser (or plan repair module) set possible explanations detected failures.recent works (see e.g., Mi & Scacchi, 1993; Gupta, Roos, Witteveen, Price, & de Kleer,2012; Micalizio, 2013) argued plan repair procedure effectivecauses plan failure known (e.g., identified via diagnosis).last decade, problem diagnosing execution MAP faceddifferent perspectives. Since seminal work, Kalech Kaminka (2003) focuscoordination failures introduce notion social diagnosis. Social diagnosis reliesabstract representation MAP hand, given terms behaviors, aimsexplaining two agents selected conflicting behaviors. Kalech Kaminka(2011) Kalech (2012) present alternative algorithms inferring social diagnosis.approaches (see e.g., de Jonge, Roos, & Witteveen, 2009; Roos & Witteveen,2009; Micalizio & Torasso, 2008, 2009) adopt explicit representation MAP termsagents actions shared resources. particular, diagnostic framework, RoosWitteveen (2009) de Jonge et al. (2009) consider action failures (i.e., actionsreach expected effects), introduce notion plan diagnosis. plandiagnosis subset (already performed) actions that, assumed abnormal, makeplan execution consistent observations received far. Since setobservations possibly explained many plan diagnoses, Roos Witteveen (2009)present criterion identifying preferred diagnoses based predictive powerdiagnoses.proposals, however, rely assumptions might limit applicability real-world scenarios. First all, assume form synchronizationamong agents (e.g., synchronized selection behaviors, execution actions).importantly, action failures assumed mutually independent. Furthermore,particular case social diagnosis, agents cooperate exchangingbelief states, might critical issue keep information private. hand, framework proposed Roos Witteveen (2009),diagnostic inferences substantially centralized.paper aim relaxing assumptions extending relational-basedframework introduced Micalizio Torasso (2008, 2009). Similarly Roos Witteveen, adopt explicit representation MAP hand term agents actionsshared resources. differently them, action models include nominal wellfaulty evolutions. argue rest paper, kind extended actionmodels subsumes action models proposed Roos Witteveen.addition, aim fully distributed solution rely synchronizedexecution actions (i.e., global clock available). distributed solution socialdiagnosis also proposed Kalech, Kaminka, Meisels, Elmaliach (2006).work, however, form synchronization among agents required agents selectnext behavior simultaneously. Moreover, agents cooperate sharingbelief states. proposal, coordination among agents achieved meansexchange direct observations agents. idea observation2fiCooperative Monitoring Diagnose Multiagent Plansacquired agent used agents reasoning. understanddifference exchanging belief states direct observations, notebelief state interpretation observations made specific agent accordinglocal knowledge. Since agent might partial knowledge environment,belief states could ambiguous even erroneous. Therefore, agents exchangebeliefs, may also propagate errors. Conversely,coordination consists exchange direct observations, agents inferbeliefs without risk conditioned errors made others.propose framework that, relying notion Partial-Order, Causal-Link(POCL) Plans (see Cox, Durfee, & Bartold, 2005; Weld, 1994; Boutilier & Brafman, 2001),limits number messages exchanged agents number causal linksexisting actions assigned different agents.proposal, communication plays central role assuring consistentexecution MAP, also easing diagnostic task. consider environment agents operate scarcely observable, agents directly acquireinformation small portion surroundings. Dealing scarce observations challenging solving diagnostic task situation mightprevent detection action failures. cope issue propose paperstrategy named Cooperative Weak-Committed Monitoring (CWCM), extendsweak-committed monitoring introduced Micalizio Torasso (2009). CWCM allowsagents cooperate agent infer outcomeactions (i.e., failure detection) pieces information provided agents.soon failures detected, must diagnosed order identifyroot causes. paper, propose local diagnostic process agentdiagnose failure actions without need interacting agents.particular, diagnostic inferences take account failures different actionsmay dependent. words, action failure indirect consequence (i.e.,secondary failure) failure preceding action (i.e., primary failure). Identifyingprimary secondary failures essential point view plan repair primaryfailures root causes anomalous observed execution. principle, plan repairrecovers primary failures also recover secondary failures. interesting property methodology process inferring primary secondaryfailures performed autonomously agent, relying trajectory-sets builtcooperative monitoring.1.1 Contributionspaper contributes diagnosis MAP execution many ways. First all,paper shows extended action models proposed monitoring purposeobtained compositionally nominal models actions, modelsexogenous events affect actions. Thus, knowledge engineer takeadvantage focusing models elementary components systems(e.g., actions exogenous events), creating complex, extended, action modelscomposing elementary components.3fiMicalizio & Torassoaddition, proposed CWCM framework fully distributed: agent monitorsactions, central agent traces actions progress. Thus,CWCM applied domains agents operate physically distributedenvironments, hence centralized solution could impractical. Another importantfeature CWCM asynchronous: neither assumption synchronized executionactions, assumptions long actions last made. words, agentsshare global clock. course, form synchronization still necessarymutual exclusion required accessing critical resources. cases, however,prefer use term coordination.CWCM also represents valid solution whenever diagnosis MAP performedenvironments characterized scarce observability levels. fact, significant contributionCWCM cooperative monitoring protocol enables agents acquire information system resources other. paper argue numbermessages exchanged via cooperative protocol linear number inter-agentcausal links (i.e., causal dependencies existing pair agents).last important contribution paper ability distinguishing primary secondary failures. Previous approaches (see e.g., Micalizio & Torasso, 2008; Roos& Witteveen, 2009), fact, assume action failures independent other.1.2 Outlinepaper organized follows. Section 2 introduce basic multiagent planningframework use starting point work. Section 3 basic frameworkextended relaxing important assumptions. Section 4 formally presents Cooperative Weak-Committed Monitoring (CWCM) strategy, local diagnostic inferencesdiscussed Section 5. paper closes detailed experimental analysis Section6, critical discussion related works Section 7.paper also includes Appendix briefly discuss CWCMimplemented means Ordered Binary Decision Diagrams (OBDDs) (Bryant, 1992,1986), formally analyze computational complexity implementation.2. Multiagent Planning: Basic Frameworksection organized three parts. First, introduce basic notions Multiagent Planning terminology use throughout paper. Then, discusspropositional planning language translated state-variable representation.Finally, present basic strategy plan execution multiagent settingshighlight importance cooperation among agents even strong assumption exogenous event occurs. assumption relaxed Section 3.2.1 Preliminary Concepts Multiagent PlanningSince interested diagnosing systems modeled multiagent plans,begin discussion presenting framework developed within planning communityrepresent synthesize kind plans. worth noting planning problemtypically approached propositional terms: preconditions effects actions literals4fiCooperative Monitoring Diagnose Multiagent Plansmust true, respectively, application actions themselves.Thus intuitively introduce section planning notions propositional terms,however, Section 2.2, argue addressing problem plan execution,convenient handle representation system terms state variables,hence translate propositional framework state variables one.important assumption holding throughout paper that, although observations gathered agents execution time partial, always correct;elaborate point Section 3.1.2.1.1 Multiagent PlansMultiagent Plan (MAP) systems take account paper modeledtuple S=hT , RES , P i, where:team cooperating agents; agents denoted letters j;RES set shared resources objects available environment;assume resources reusable (i.e., consumable),accessed mutual exclusion; note agents exchange piecesinformation resources, space resource names commonlanguage agents communicate;P completely instantiated Partial-Order Causal-Link Plan (POCL) (Weld, 1994),resulting planning phase ones Boutilier Brafman (2001)Cox et al. (2005). sake simplicity, MAP P simplified structuresince involve concurrency non-concurrency constraints. precisely,MAP P tuple hI, G, A, R, Ci where:- set propositional atoms representing initial state systemplanning time.- G another set propositional atoms representing global goal achieved.Note G conjunction sub-goals Gi agent charge of.- set action instances agents execute; actionassigned specific agent . planning time, assume actionmodeled terms preconditions pre(a) effects eff (a),conjunctions grounded atoms (see PDDL level 1, Fox & Long, 2003).rest paper, denote ail l-th action performed agent i.- R set precedence links action instances; precedence link ha, a0R means action a0 start completion action a.- C set causal links form lk : ha, q, a0 i; link lk states actionprovides action a0 service q, q atom occurringeffects preconditions a0 .assume MAP P :flaw-free: nominal execution P achieves G;5fiMicalizio & Torassosafe respect use resources. Intuitively, say resourceres RES used safely iff execution step, res either assigned,assigned exactly one agent. similar concurrency requirement (Roos& Witteveen, 2009): two actions executed concurrently requiresubset resources;redundant actions: even though P necessarily optimal, containsactions directly indirectly provide services help achievement goal.means always exists chain causal links actionplan least one atom goal G.guarantee resource safeness, introduce notion working sessions associated resources agents:Definition 1 Let res resource RES , agent , working sessionwshres,ii using res pair haio , aic actions Ai that:aio precedes aic (i.e., aio aic , transitive closure precedencerelations R).aio action Ai agent acquires res, modeled specifyingatom available(res) preconditions aio . Moreover, exists Cincoming causal link form hajk , available(res), aio i, ajk action assignedagent j (possibly ajk a0 i.e., pseudo action whose effects determineinitial state system). Action aio opens working session. actionaih Ai aio aic (i.e., aio aih aic ), incoming causal link labeledservice available(res) coming action another agent j 6= i.aic action Ai agent relinquishes resource res favor anotheragent j 6= i. modeled means causal link haic , available(res), ajk C,meaning action aic releases res one effects, available(res)one preconditions ajk . Action aic closes working session. course,agent release resource res one agent; i.e., outgoing linkhaic , available(res), ajk unique. addition, action aih Ai aio aic ,outgoing link labeled service available(res) directed towards actionanother agent j.action aih aio aic use res; i.e., res mentioned preconditions effects aih . precisely, causal link mentioning res twoactions aih aik Ai allowed aih aik belong workingsession, namely, aio aih aik aic .Given working session wshres,ii , denote opening closing actionsopening(wshres,ii ) closing(wshres,ii ), respectively. Two working sessions wshres,iiws0hres,ji consecutive, wshres,ii / ws0hres,ji , closing(wshres,ii ) provides opening(ws0hres,ji )service available(res).Proposition 1 MAP P satisfies resource safeness requirement resourceres RES, working sessions res totally ordered sequencews1hres,i1 / ws2hres,i2 . . . / wsnhres,in , agent ij (with j : 1..n).6fiCooperative Monitoring Diagnose Multiagent Plansmeans that, independently agents accessing resource, two working sessionsresource res never overlap other. Possibly, agentone session sequence, meaning agent acquires relinquishes resourceres many times along plan.resource safeness requirement extension concurrency requirement introduced Roos Witteveen (2009). fact, concurrency requirement implicitlyimposes ordering two actions cannot performed simultaneously,resource safeness imposes ordering blocks actions identified working session. necessary order model situations agent uses set resourcesnumber consecutive actions cannot interleaved actionsagents. working session sort critical section cannot interrupted. worthnoting that, since working session associated single resource, sinceconstraint working sessions two resources, possible actionaic Ai closes two different working sessions. example, let wshres,ii ws0hres0 ,iitwo working sessions agent using resources res res0 , respectively; possibleaction aic closing(wshres,ii ) closing(ws0hres0 ,ii ). addition, resourceres could released favor agent j, resource res0 could released favoranother agent k. modeled two causal links haic , available(res), ajxhaic , available(res0 ), aky i.2.1.2 Local PlansSince interested fully distributed framework plan execution plandiagnosis, impose every agent knows portion P perform;thus introduce notion local plan. Intuitively, local plan P projection Pactions assigned agent i; P therefore partitioned many local plansagents . formally, given agent , local plan P assigned, C i, represents portiontuple P =hI , Gi , Ai , Ri , Clocal, Cininitial state known agent plan execution starts; Gi sub-goal assignedagent i; Ai , Ri Clocalmeaning A, R C, respectively, restricted, highlight causalCinactions assigned agent i. remaining sets, Coutdependencies existing actions agent actions assigned agentsmaintains outgoing causal links modeling services agent provides: Coutmaintains incoming causal links modeling servicesagents with; whereas, Cinagent receives agents. simplify plan structure, precedence linksactions different agents allowed R. This, however, real limitationprecedence links actions different agents could expressed causal linksexchanged service refers dummy resource.rest paper consider local plan P partially ordered setactions. However, assume agent perform one action per time,rest paper index actions according execution step. words,P executed sequence hai0 , ai1 , . . . , , ai i, ai0 ai two pseudoactions similar ones introduced Weld (1994). Action ai0 preconditionseffects coincide initial state known agent i; particular, pseudo-actionalso used determine initial set resources assigned agent i: link leaving7fiMicalizio & Torassoai0 labeled service available(resk ) denotes resk assigned agent i.Action ai , hand, effects preconditions correspond sub-goalGi assigned i.2.2 Translating Propositional Framework State-VariableRepresentationAlthough planning approaches literature relies propositional languagerepresent planning problems, adopt paper representation based multivalued state variables similar SAS+ approach introduced JonssonBackstrom (1998). reason choice stems fact multi-valuedvariable implicitly encode mutual exclusion constraints among values domain:variable assume one value given time. Thus, easier representevolution system state time sequence assignments values statevariables. solution also effectively adopted diagnosis plans (Roos &Witteveen, 2009), Helmert (2009) proven, restrictive since alwayspossible translate propositional representation set multi-valued state variables.rest section briefly describe propositional planning language (seee.g., Nebel, 2000) mapped state variables representation. Three main aspectsaddressed: (1) represent agents states terms state variables rathersets propositional fluents; (2) represent exchange services among agents;(3) model actions terms state variables.2.2.1 Mapping Atoms Variablespoint view, action models system states represented termsfinite set multi-valued state variables, finite discrete domain.Given MAP system S=hT , RES, P i, associate agents resourcesRES set state variables; variables maps subset atomscorresponding propositional representation.follows current system state given values currently assignedstate variables agents resources. global view, however, inadequateachieve fully distributed framework. thus introduce notion agent state,captures portion system state relevant specific agent team.agent associated set VARi variables. variable v VARifinite discrete domain dom(v). state Sli agent given execution step ltherefore assignment values variables VARi . precisely, Sli (v) dom(v)value assumed variable v VARi agent state Sli . partial agent state liassignment values subset variables VARi .Variables VARi partitioned two subsets: END ENV . Set END includesendogenous state variables (e.g., agent position); whereas ENV includes variablesshared resources. partitioning global system state agent states,state variables resources duplicated many copies agents. Therefore, resource resk RES , exists private copy resik belongingENV . maintain consistency among private copies, rely two assumptions:(1) MAP P flaw-free, (2) P satisfies resource safeness requirement. two8fiCooperative Monitoring Diagnose Multiagent Plansassumptions induce variables ENV implicit constraint: execution stepone agent knows actual state given resource resk . consequence,private copy resik keeps correct value. agents, resource resksight; modeled assigning value unknown local variables resk ;namely, j 6= i, resjk =unknown. Thus consistency among different privatecopies implicitly maintained.course, agent j acquires resource resk agent i, also comes knowactual state resource. values variables ENV fact exchangedamong agents. Section 2.3 present basic cooperative protocol enablesagents share knowledge resources preserving resource safenessrequirement; basic protocol later extended Section 4.5. Variables END ,hand, refer agent i, framework cannot sharedagents.2.2.2 Mapping Services Variables AssignmentsSince adopt representation based state variables, also service exchangedtwo agents conveniently modeled value assignment resource variable. instance, causal link lk : hai , available(resk ), aj used propositional representationmodel action ai Ai provides action aj Aj resource resk . linkrewritten state variables representation hai , resk = available, aj i, reskname agents j refer specific resource RES . words,resk sort meta-variable known agent agent j. course,agent maps meta-variable resk private copy. precisely, meanslink hai , resk = available, aj i, agent able communicate j change stateresource resk : agent j knows that, execution ai , private variable resjkvalue available, meaning j use resk . worth noting availablespecial value, used agents exchanging resources. general, agents communicatedomain-dependent values state resource (e.g., positionblock blocks world domain).Relying state variables representation, identify set resourcesavailable given agent l-th plan execution step availResi (l) = {reskRES |Sli (resik ) = available}. next subsection focus coordination protocolallows agents j exchange information shared resources.2.2.3 Mapping Propositional Action Models Function-Like ModelsLet Q subset state variables VARi , rest paper denote (Q)space possible assignments variables Q, Q denote specificassignment (Q); is, specific partial state agent i. rest paperuse premises(ali ) effects(ail ) denote subset status variables VARipreconditions effects, respectively, action ail defined. Thus premises(ail )represents specific assignment values space (premises(ail )). Note setpremises includes also services must provided agents,therefore correspond incoming causal links action ail . Similarly, set effects9fiMicalizio & Torassoincludes also services agent provides agents with, encoded causal linksoutgoing ail .Given action instance ail Ai , deterministic (nominal) model ail mapping:: premises(ail ) effects(ail )fanomlpremises(ail ) assignment variables premises(ail ) VAR representingpreconditions ail , effects(ail ) assignment variables effects(ail ) VARimodeling effects action. also assume effects(ail ) premises(ail );substantial limitation, however. variable v, principle wouldmentioned effects action model, also mentioned premisesaction allowing v assume possible value domain. reasonassumption clear Section 3.2 formalize notion exogenous events.Since interested execution plans, reformulate applicabilityaction state (see Nebel, 2000) terms executability. Given agent state Sli ,action instance ail Ai executable Sli (VARi ) iff Sli |= premises(ail ); indeed,strong condition relaxed next section. Using terminologyRoos Witteveen (2009), say action ail executable Sli ail fullyenabled Sli . result execution ail enabled Sli new agent state Sl+1(VAR ), called successor state:(Sli \ effects(ail )) effects(ail ) Sli |= premises(ail )Sli 6|= effects(ail ) 6|= ,Sl+1=,otherwise.precisely, Sli \ effects(ail ) partial agent state obtained removing Slivariables mentioned effects(ail ). partial state subsequently completed. statenew assignments effects(ail ), yielding new (complete) agent state Sl+1transformation, however, legal when: (1) action ail fully enabled Sli (i.e., Sli 6|=Sli |= premises(ail )), (2) action effects consistent (i.e., effects(ail ) 6|= ).Otherwise, new agent state undefined.2.3 Plan Execution Nominal Conditionsactual execution MAP requires form coordination among agents.decomposition global plan local plans, fact, allows agents execute actions fully distributed way without intervention global manager:agents execute actions concurrently asynchronously (i.e., global clock required).addition, differently previous approaches (see e.g., de Jonge et al., 2009; Micalizio& Torasso, 2008), actions take single time slot completed execution globally synchronized, make assumption durationaction. Agent coordination therefore essential order violate precedencecausal constraints introduced planning phase. sake clarity, firstpresent basic coordination strategy assuming that:10fiCooperative Monitoring Diagnose Multiagent PlansBaDE(P =hI , Gi , Ai , E , Clocal, Cin, Couti)1. l 12. Sli3. ail 6= ai4.Sli consume-inter-agent-messages(inbox)5.ail executable Sli6.execute ail7.Sl+1fanom(Sli )l8.obsil+1 gather observations9.Sl+1obsil+1 |=10.stop execution propagate failure11.end12.causal link lk Coutlk : hail , v = d, ajm13.notify agent j achievement service v =14.v RES = available15.Sl+1(v) unknown16.end17.end18.l l+119.end20. endFigure 1: Basic Distributed Plan Execution (BaDE) strategy.- action-based observability: even though agents complete viewenvironment, agent always able observe preconditions effectsactions performs. addition, observations assumed reliable correct.denote obsil+1 set observations received agent l-th plus 1execution step execution l-th action. observations obsil+1thought sets value assignments variables effects(ail ). is,variable v effects(ail ), assignment v = belongs obsil+1 ; dom(v);- deterministic actions: actions never fail models precisely defineagent state changes;- static environment: environment change consequence executionagents actions.three strong assumptions relaxed Section 3, presentcomplex coordination strategy guarantees consistent access resources evenactions fail.2.3.1 Basic Coordination Protocolcoordination protocol adopt simple effective, exploits causal linksmodels, fact, exchange service/resourceP . outgoing link Coutagent - service provider - another agent j - service client. order supportcommunication among agents, assume agent inbox, i.e., foldermessages coming agents stored. Whenever executes action, sends message outgoing link ai notifyingail outgoing links Coutlreceiver needed service/resource available. Likewise, link Cin11fiMicalizio & Torassomodels exchange service/resource receiver another agent j, waitsprovider. Whenever execute action incoming links Cinmessage links since action becomes executable requiredservices/resources provided. assumptions made MAP P ,protocol guarantees resources always accessed consistently. fact, assumptionP satisfies resource safeness requirement, assures working sessionsresource totally serialized; particular, two working sessions wshres,ii /ws0hres,jiserialized means causal link hclosing(wshres,ii ), res = available, opening(ws0hres,ji )i.Therefore, wshres,ii closes, agent notifies agent j resource res available.2.4 Basic Distributed Plan Execution (BaDE) Strategyhigh-level plan execution strategy performed agent team outlinedFigure 1. strategy consists loop iterates long actionsP executed. first step loop acquire new informationagents shared resources. accomplish task, agent plays role clientcoordination protocol gathers notification messages, any, sentagents; notification messages (i.e., value assignments convey) assertedwithin current agent state Sli updates local view system statusacquires (if available) new resources. step, next action ail selected:ail executable yet, agent keeps waiting notification messages (i.e.,services/resources still missing). Otherwise, agent executes ail real world,. (Noteexploiting nominal model fanomestimates successor state Sl+1lactual execution ail real world may take time necessarilyknown advance.)action ail completed, agent gathers observations efi .fects action matches observations estimated successor state Sl+1Since assuming actions cannot fail, discrepancy observationsestimations discovered. include control line 9 compatibilityextension described sections 3 4, actions may fail.execution ail , agent plays role provider coordination protocolpropagates (positive) effects action ail towards agents sending notificai (see lines 13 16 algorithmtion message outgoing link ail CoutFigure 1). particular, case agent released resource (v RES), agentsets private copy resource unknown, way resource becomesunreachable mutual exclusive access guaranteed.last remark basic strategy regards increment counter l. Notel incremented action executed; thus l correspond metrictime, pointer next action performed. Adhering BaDE strategy,agent conclude local plan within finite amount time.Proposition 2 Let = hT , RES, P MAP system P flaw-free satisfiesresource safeness requirement. agents follow BaDE strategy Figure 1,P successfully completed finite amount time.12fiCooperative Monitoring Diagnose Multiagent PlansProof: sufficient show least one action P always execution,step, achievement goal. contradiction, let us assume goalreached, action progress. Since assume failureoccur, situation happen agents deadlock, waiting servicesone provide with. deadlock may arise either erroneous planningphase, P flaw-free initially assumed, BaDE coordinationstrategy erroneous. Let us show agents adopt BaDE strategy,failure occurs, agents never wait indefinitely accessing resource. Since P satisfiesassumption resource safeness requirement, must hold wshres,ji / wshres,ii ; i.e.,closing(wshres,ji ) provides opening(wshres,ii ) (i.e., ail ) service res = available. Agentwaits service res = available two situations: (1) agent j performedaction closing(wshres,ji ) yet (correct behavior), (2) agent j already performed actionclosing(wshres,ji ), sent appropriate message i. second case contradicts hypotheses agents adopt BaDE coordination protocol. fact,required protocol, whenever working session wshres,ji closes, agent j sendmessage next agent accessing resource.Therefore, agents never deadlock, least one action always execution,since P finite number actions, goal G must achieved within finite amounttime.Example 1. conclude section briefly exemplifying concepts introducedfar. particular, present office-like domain used test scenarioexperiments (Section 6), domain similar ones adopted Micalizio (2013)Steinbauer Wotawa (2008). domain, robotic agents deliver parcelsdesks number clerks. robot carry one two parcels depending whetherheavy light, respectively. Figure 2 shows office-like environment usedtests; includes: 9 desks, distributed 5 office-rooms, connectedmeans 8 doors. Moreover, two repositories contain 12 parcels delivered(8 light 4 heavy). Parcels, repositories, doors, desks critical resourcesused accessed one agent per time. domain also includes threeparking areas, locations agents positioned simultaneouslycritical resources. (The term location used identify either parking areasresources agent physically positioned; e.g., parcels locations.)Agents perform following actions: move location another, load/unloadparcels within resources locations (i.e., parking areas), addition,impose one parcel positioned desk, repositoriesunlimited capacity. Finally, agents carry one heavy parcel two light parcelslocation another.Figure 3 shows simple example MAP office domain. team involves threeagents A1, A2, A3, whose plans given three columns, respectively.bottom picture effects pseudoaction a0 represent initial states threeagents. top picture, premises pseudoaction represent desiredfinal state. objective MAP figure deliver parcel1 desk3 (i.e.,agent unload parcel1 positioned desk3), bring parcel backinitial position repository repos1. Similarly, parcel2 first delivered desk6,13fiMicalizio & TorassoFigure 2: office-like environment used experiments: five rooms R1-R5, tworepositories: repos1 repos2, eight doors, nine desks, three parking areas.brought back repository repos2; parcel3, already delivereddesk3, delivered desk4. ease readability picture, showinter-agent causal links. use two different graphical notations distinguishcausal links giving access resources (diamond-headed), causal links modelkinds services (black-circle-headed). instance, link actions a13a24 diamond-headed, means action a13 provides a24 service desk3 = available(i.e., a13 , agent A1 longer access desk3). three dashed rectanglespicture represent working sessions associated resource desk3, usedthree agents different execution steps. (The working sessions resourceshighlighted avoid picture becoming confused.)Black-circle-headed links used represent services. instance,1 encodes service desk3.content=empty,link actions a12 a25 (labeled )required action a25 since one parcel located desktop. link labeled2 (from a25 a37 ) encodes two services: desk3.content=parcel1 parcel1.pos=desk3.3 refer desk6 parcel2.Similar services encoded link ,3. Extending Frameworkprevious section described simple coordination strategy guaranteesconsistent execution MAP P three strong assumptions hold: (1) agentaction-based observability: precisely observe preconditions effectsactions performs; (2) environment static (no exogenous events permitted);(3) actions deterministic (no deviation nominal behavior possible).Henceforth extend basic framework relaxing three assumptions and,consequence, increasing complexity strategy controlling distributedplan execution.14fiCooperative Monitoring Diagnose Multiagent Plansparcel1.pos=repos1parcel1.delivered=desk6A3.pos=P1parcel3.pos=desk4parecel3.delivered=desk4A1.pos=P2parcel2.pos=repos2parcel2.delivered=desk3A2.pos=P3a212move(repos2, P3)a211a311unload(repos2, parcel2)a210a17move(repos1, P1)repos2a310carry(desk6, repos2)move(door2, P2)a29a16unload(repos1, parcel1)repos13a39load(desk6, parcel2)move(desk4, door2)carry(door1, repos1)desk6a28a15a38move(door4, desk6)unload(desk4, parcel3)a27a14door4move(P2, door4)carry(door2, desk4)1a12move(P2, desk3)desk3move(door4, desk3)a35unload(desk3, parcel1)move(desk6. door4)a24a34carry(door1, desk3)unload(desk6, parcel2)a23a33carry(repos1, door1)A1.pos=P2desk3=availdesk4=availparcel3.pos=desk3parcel3.delivered=desk3door2=availload(desk3, parcel1)a25load(desk3, parcel3)desk3a37a36move(desk3, P2)a11parcel1a26a13carry(desk3, door2)carry(desk3, door1)2carry(repos2, desk6)a22a32load(repos1, parcel1)load(repos2, parcel2)a21a31move(P1, repos1)move(P3, repos2)A2.pos=P1repos1=availparcel1.pos=repos1parcel1.delivered=nodoor1=availA3.pos=P3repos2=availparcel2.pos=repos2parcel2.delivered=nodesk6=availdoor4=availa0Figure 3: simple example MAP office-like domain used testing.15fiMicalizio & Torasso3.1 Partial Observabilityfirst assumption relax action-based observability. basic framework,observations obsil+1 agent receives (l + 1)-th step execution covervariables effects(ail ). extended framework, obsil+1 becomes partial sincesubset variables effects(ail ) covered general, possibly obsil even empty.Also case assume observations correct, meaning actualstate agent cannot inconsistent observations received agent itself.However, observations ambiguous sense given variable, agentreceives disjunction possible values. addition, guarantee terminationplan execution phase, assume agent observes least achievementatoms local goal Gi G.3.2 Plan Threatssecond extension dynamics system: move static systemdynamic one. means that, besides changes due agents actions,system also change consequence exogenous, unpredictable events, typically represent plan threats (Birnbaum, Collins, Freed, & Krulwich, 1990) nominalexecution plan. Intuitively, plan threat seen abrupt change happenedenvironment (i.e., resources), state agent.paper associate occurrence exogenous event executionaction. words, exogenous event occur executionaction affect active variables action; namely, variablesmentioned within premises effects action. Thus, exogenous eventcannot affect simultaneously two actions, indirect effects manyactions, even different agents, means shared resources.principle, given exogenous event , one could define model predictaffect execution action. real-world domains, however, always possibleprecisely know advance actual impact exogenous event: one hand,may non-deterministic effects; hand, effects mayknown. take account possibly non-deterministic effects exogenous events,model exogenous event relation happens defined follows:happens (affectedby ) {} (affectedby )(1)affectedby VARi , (affectedby ) space partial agent states definedaffectedby . worth noting (affectedby ) empty affectedbyempty, too. enables us state that, exogenous event occurs, variableaffectedby must necessarily evolve unexpectedly. Thus, tuple relation happensrepresents non-deterministic effect ; namely, tuple represents possible abruptchange agents status variables.deal known effects exogenous event , extend domain dom(v)variable v VARi special value unknown leaves open possibleevolution v.denote X set exogenous events might occur planexecution. Note X also includes pseudo event modeling absence abrupt16fiCooperative Monitoring Diagnose Multiagent Planschanges. special event must hold affectedby = . Since exogenousevent defined state transition agents state variables, affectaction ail iffaffectedby effects(ail ).(2)Namely, affects subset variables action ail defined.Given action ail , X (ail ) denotes subset exogenous events X satisfyrelation (2). Note that, pseudo event always included X (ail ), actionail Ai , since affectedby (i.e., empty set) trivially satisfies relation (2).3.3 Extended Action Modelslast extension propose action models. Since plan threats occurexecution actions, effects combine actions effects. estimatesystem evolves time, essential extend nominal action model orderencode, single piece knowledge, nominal well anomalous evolutionsaction. Intuitively, extended model describe agent state Sli evolvesagent carried action instance ail , when,new agent state Sl+1execution action, exogenous event X occurred, possibly .Moreover, basic framework give granted action performedfully enabled. extended framework condition necessarily satisfied. Duepartial observability, fact, agent may unable precisely determine whethernext action fully enabled not. cope situation, introduce Section 3.4concept possibly enabled action. time being, anticipate agentmay decide perform action even action fully enabled, henceextended action model must rich estimate state agent evolves evensituation., givenextended model M(ail ) action ail derived nominal model fanomlterms premises effects, set exogenous events X (ail ); formallydefined as:, X (ail ), (ail ), (ail )i,M(ail ) : hfanomlX (ail ) already introduced; whereas (ail ) (ail ) twofanomltransition relations partial states (premises(a)) (effects(a)),possible predict execution action ail changes stateenvironment (i.e., resources held i) agent itself.Relation (ail ) estimates next agents states action ail fully enabledstate Sli . relation results combination nominal action modelmodels exogenous events X (ail ):(ail ) =[fi happens }{fanomlX (ail )(3)Intuitively, fanomfi happens set tuples form hpre, , eff i, pre equalslpremises(ail ), eff (effects(ail )) models abrupt changes caused event17fiMicalizio & Torassonominal effects action ail . Formally, happening h, , 0 happens holds:premises(ail ) {} {(effects(ail ) \ affectedby ) 0 }[Sli |= premises(ail ) premises(ail ) |= ;fanomfihappens =lh,,0 ihappensotherwise.(4)important note that, since always part X (ail ), nominal model hpremises(ail ),, effects(ail )i always included (ail ). particular, state transition, variable assume value unknown. follows directly by: (1) nominal model fanomlcannot mention unknown value definition, (2) exogenous event cannotfi happensaffect variable since affectedby empty. Thus, operation fanomlreproduces nominal behavior.addition, note X (ail ) also include special exogenous event ? . symboldenotes indefinite exogenous event model given, hence variableseffects(ail ) mapped unknown: occurrence ? prediction possible.Relation (ail ) structure (ail ) terms preconditions, effectsexogenous events, represents dual version (ail ) since defined ailexecutable Sli . fact, (ail ) defined states action ail enabled.Let (premises(ail )) space assignments values variables premises(ail ),(ail ) defined space states (premises(ail ))=(premises(ail )) \ premises(ail )as:(ail ) (premises(ail )) {? } hunknown, . . . , unknowni,(5)? denotes indefinite exogenous event . Note (ail ) weaker model(ail ) since invariably assigns unknown value variable effects(ail ).say, whenever action performed wrong configuration, impacteffects(ail ) variables becomes unpredictable. Although use symbol ? denoteindefinite events occurring (ail ) , slightly different meaningsdiagnostic point view discussed detail Section 5.Remark 1. relational action models propose sufficiently flexible dealincomplete imprecise knowledge. many cases, fact, may costly (or evenimpossible) determine exogenous events impact variables effects(ali ).extended framework copes problem allowing three forms incompleteness:- unknown value included domain variable allows represent that,effect exogenous event, value variable becomes predictable.extreme case, variables effects action set unknown(see weak model exogenous event ? ).- Non-deterministic action evolutions defined: exogenous event maynon-deterministic effects states agents.- weak relation allows us model status agent executionaction wrong conditions.18fiCooperative Monitoring Diagnose Multiagent PlansRemark 2. Since actions performed even though fully enabled,guarantee execution plan violate resource safenessrequirement? answer question coordination protocol partCooperative Weak-Committed Monitoring (CWCM) strategy discussed Section 4.useful anticipate, however, coordination protocol guarantees agentuses resource actions violate resource safeness requirement.Example 2. Let us consider simple example office domain, assumeagent A1 charge performing action carry(A1, Parc2, desk1, desk2);action requires A1 move current position desk1 position desk2loaded parcel Parc2. nominal model action expressedstate transition:hpos = desk1, cObj=Parc2, Parc2pos1=A1, , pos = desk2, cObj=Parc2, Parc2pos1=A1i;pos cObj two endogenous variables A1 representing current positionagent carried object, respectively. state shared resource Parc2encoded variable Parc2pos1, private variable agent A1 keeps maintainposition parcel Parc2. agents, local copy variable Parc2posunknown.actual execution carry action affected number exogenousevents; instance, wheelsblocked prevents agent moving all, wrongstepallows agent move, wrong direction. Another event affectcarry action lostparcel : agent moving, carried object(s) lost; finally,? denotes unpredictable event occurring carry action attempted statepreconditions satisfied. alternative situations summarizedwithin extended model showed Table 1. first entry table nominalstate transition, one labeled . Entries 2 5 describe actionbehaves known exogenous event occurs. Note that, although exogenousevent one foreseen possibilities, effects may precisely known;instance, effect wrongstep lostparcel variables assume valueunknown. first five entries table represent relation extended model.last entry table, instead, relation allows us make weakpredictions. tuple hpos=*, cObj=*, Parc2-place=*i shortcut representpossible assignment preconditions satisfied. Note that, practicalpoint view, necessary compute (potentially huge) set explicitly,discuss Appendix implementation.3.4 Extending Basic ConceptsSince relaxed three assumptions basic framework, reviewthree important concepts: state agent, executability action,outcome action.19fiMicalizio & TorassoENDt1t2t3t4t5t6ENVENDENVposcObjParc2pos1eventposcObjParc2pos1desk1desk1desk1desk1desk1*Parc2Parc2Parc2Parc2Parc2*A1A1A1A1A1*desk2desk1unknowndesk2desk2unknownParc2Parc2Parc2emptyemptyunknownA1A1A1desk1unknownunknownwheelblockedwrongsteplostparcellostparcel?Table 1: extended model action instance carry(A1, B2, desk1, desk2)office domain.3.4.1 Agents Belief StatesFirst all, agent team must able deal form uncertainty.Since actions may evolve non-deterministically since agent cannot observeeffects actions, agent must able deal belief states rather agentstates. Like agent state Sli , agent belief state Bli encodes knowledge agentl-th execution step. Sli precise state assumed stepl, Bli set possible agent states consistent observations received i.rest paper use lowercase indicate agent state among others within givenbelief state, use uppercase indicate actual agent state given executionstep. important note that, exactly agent state Sli , belief state Bli definedstate variables agent i; two states s1 s2 Bli differ least onevariable. words, must exists least one variable v VARil s1 (v),value assumed v s1 , different s2 (v). course, ambiguity representsissue understanding whether next action ail executable.3.4.2 Possibly Enabled ActionsSince agent belief state Bli , also notion action executability needsrevised. conservative policy would require action ail fully enabled everystate Bli ; due partial observability condition might satisfied,execution MAP could stopped action enabled even though planthreat occurred.avoid situation, propose optimistic policy introduce notionpossibly enabled action.Definition 2 Optimistic Policy Action ail possibly enabled Bli iff Bliail fully enabled s; namely, |= premises(ail ).worth noting value unknown cannot used qualify action fullyenabled. value, fact, used explicitly state agent knowvalue variable. Therefore, variable v value unknown state s, v alsomentioned premises(ail ), ail fully enabled s.possibly enabled action therefore sort conjecture: since action premisessatisfied least one state belief state, action assumed executable.20fiCooperative Monitoring Diagnose Multiagent Planscourse, may case s, although possible, real state agent,hence action performed preconditions satisfied real world.3.4.3 Action Outcomebasic framework given granted outcome action alwaysnominal. extended framework, however, actions fail. individuate three possible action outcomes: nominal ok, anomalous failed, pending intermediatesituations., |= effects(ai ).Definition 3 Action ail outcome ok iff Bl+1l(estimated execution ail ).is, actions effects hold every state Bl+1Definition 3 hold exists least one state Bl+1nominaleffects satisfied. previous approaches (Micalizio & Torasso, 2008, 2007a),introduced adopted strong committed policy: effects action ail, action outcome failed (seesatisfied state belief state Bl+1Definition 3). strong committed policy based assumption that, whenever actionail successfully completed, agent receives amount observations sufficientdetect success. Thereby, success cannot detected, failure mustoccurred.policy, however, may unacceptable real-world domainsguarantees system observability. consequence, agent could infer failureeven action ail completed success, observations sufficientconfirm it.paper define failure action dual case success:, 6|= effects(ai ).Definition 4 Action ail outcome failed iff Bl+1lstate expected effects ailNamely, possible find Bl+1achieved.situations neither success (Definition 3) failure (Definition4) inferred, action outcome pending., |= effects(ai ) s0 B ,Definition 5 Action ail outcome pending iff Bl+1l+1l06|= effects(al ).words, whenever agent unable determine success failureaction ail , postpones evaluation action outcome step future;action enqueued list pActsi pending actions maintained agent i. referpolicy weak committed since agent take decisions wheneverinsufficient observations sufficient support them. next section discussimpact weak committed policy monitoring task.4. Cooperative Weak-Committed Monitoringsection discuss fully distributed approach problem monitoringexecution MAP. consider extended framework previously discussed introduces two sources ambiguity: agent belief states, ambiguous action outcomes.21fiMicalizio & Torassocope forms uncertainty, propose monitoring methodology calledCooperative Weak-Committed Monitoring (CWCM), relies weak-committedpolicy. CWCM approach allows agent detect outcome actiontime execution. idea possibly uncertain knowledge agentenvironment refined time exploiting observationsagent receive future. get result, CWCM allows team memberscooperate monitoring tasks.rest section organized follows. first formalize notion trajectoryset maintained agent, explain extended action models usedextend trajectory-set one step further. discuss trajectory-setrefined observations helps determining outcomespending actions (if any). Finally, redefine cooperative protocol sketched basicframework obtain cooperative monitoring protocol. CWCM entirely formalizedterms Relational Algebra operators. (For short introduction used operators, seeMicalizio, 2013.)4.1 Trajectory-Setweak-committed approach requires agent able reason past.means agent cannot maintain last belief state, keeptrajectory-set; i.e., sequence belief states traces recent history agentsstate.define trajectory-set generalization agent trajectory. agent trajectoryagent i, denoted tr (1, l), defined segment [ai1 , . . . , ail1 ] local plan P ,consists ordered sequence agent states interleaved exogenous events X(including ). agent trajectory represents possible evolution status agent i,consistent observations agent received far; formally:Definition 6 agent trajectory tr (1, l) plan segment [ai1 , . . . , ail1 ]tr (1, l)=hs1 , e1 , s2 , . . . , el1 , slwhere:- sk (k : 1..l) state agent k-th step obsik sk 6|= .- eh (h : 1..l 1) event X (ah ) labeling state transition sh sh+1 .agent trajectory therefore sequence agent states, interleaved events, tracesagent behavior along given plan segment. sake discussion, considerplan segment starting first performed action ai1 ; practice, however, plansegment consideration intermediate portion agents local plan.return point Section 4.6.Since state sk (k [1..l]) complete assignment values agent statevariables VARi , variables duplicated many times actionsplan segment consideration; following, denote VARik copiesstate variables referring k-th execution step.noticed above, however, partial system observability general sufficientestimation unique trajectory; reason agent keeps trajectory-set r [1..l],22fiCooperative Monitoring Diagnose Multiagent Planscontains possible agent trajectories tr (1, l) consistent observations received execution plan segment [ai1 , . . . , ail1 ].Note that, given trajectory-set r [1, l], agent belief state execution step k[1..l] easily inferred projecting r [1..l] state variables VARik :Bki = projectVARi (T r [1..l])k(6)Thus definitions 2 (possibly enabled actions), 3 (successfully completed actions), 4 (failedactions), 5 (pending action), based belief states, still meaningfulrequire redefined.rest paper, term trajectory frontier (or simply frontier) referslast belief state maintained within trajectory-set. instance, frontier r [1, l]belief state Bli . general rule, use l denote index last execution step(and hence frontier); k used refer generic execution step [1, l].4.2 Extending Trajectory-Setextension trajectory-set corresponds predictive step basic frameworknext agent state estimated. However, basic frameworkstep easy mapping state another, need complex procedureextended framework. Given current trajectory-set r [1, l] extendedmodel M(ail ), estimation step defined Relational terms follows:r [1, l + 1] = r [1, l] M(ail ) = r [1, l] join ((ail ) (ail )).(7)new trajectory-set r [1, l + 1] built contributionrelations. relations fact used estimate execution action ail changesstate system. Relation applied portion Bli action ail fullyenabled. Whereas, relation applied states Bli action ailenabled; i.e., occurrence exogenous event already assumed.4.3 Refining Trajectory-Set Observationsbasic framework assumed that, whenever action ail completed, agent. extended framework,receives observation obsil+1 new agent state Sl+1agent also receive observation obsik referring past execution step k (i.e., 1 k l).next section present cooperative monitoring protocol basismessage passing among agents. section discuss observationpast handled given agent i. Intuitively, consuming observation obsik meansselecting Bki states consistent it; Relational terms:refined Bki = selectobsi Bkik(8)result refined belief state less ambiguous original one numberstates inconsistent observations pruned off.important note unknown value consistent concrete observedvalue. Therefore, state Bki , variable v unknown s, v mentionedobsik , v assumes observed value obsik (v) refined Bki . Note allowobserved variable obsik assume value unknown.23fiMicalizio & TorassoBlA1s1ls1l+1wheelblockedwrongsteps2lA1Bl+1s2l+1s3l+1lostparcellostparcel?s4l+1s5l+1s6l+1Figure 4: one-step trajectory-set corresponding transition step l step l + 1.Example 3. Let us consider office domain, assume l steps,trajectory frontier agent A1 consists following belief state BlA1 :s1l : h pos = desk1, cObj = Parc2, Parc2pos1= A1s2l : h pos = unknown , cObj = Parc2, Parc2pos1= A1Namely, BlA1 consists two alternative agent states s1l s2l . Let us assumenext l-th action performed A1 carry action, whose model previouslypresented Table 1. According equation (7), easy see s1l matchesstate transitions t1 t5 carry action model ( portion model), whereasstate s2l matches transition t6 ( portion model). Figure 4 gives ideatwo relations used infer new frontier:s1l+1 : h pos = desk2, cObj = Parc2, Parc2pos1 = A1s2l+1 : h pos = desk1, cObj = Parc2, Parc2pos1 = A1s3l+1 : h pos = unknown, cObj = Parc2, Parc2pos1= A1s4l+1 : h pos = desk2, cObj = empty, Parc2pos1= desk1s5l+1 : h pos = desk2, cObj = empty, Parc2pos1= unknowns6l+1 : h pos = unknown , cObj =unknown, Parc2pos1=unknownNow, let us assume agent A1 receives observation obsA1l+1 = {hpos = desk2i},used refine new frontier. easy see obsA1l+1 consistentstates except s2l+1 , pos assigned different value. States s3l+1 s6l+1consistent obsA1l+1 unknown value consistent precise value.new refined frontier therefores1l+1 : h pos = desk2, cObj = Parc2, Parc2pos1 = A1s3l+1 : h pos = desk2 , cObj = Parc2, Parc2pos1 = A124fiCooperative Monitoring Diagnose Multiagent Planss4l+1 : h pos = desk2, cObj = empty, Parc2pos1= desk1s5l+1 : h pos = desk2, cObj = empty, Parc2pos1= unknowns6l+1 : h pos = desk2 , cObj =unknown, Parc2pos1=unknownseems s1l+1 s3l+1 identical, indeed consider singlebelief states, trajectories; two states differ way achieved: s1l+1inferred assuming everything goes smoothly; s3l+1 inferred assuming somethingwrong occurred (i.e., wrongstep ). course, second hypothesis plausiblediscuss next section pruned trajectory-set.example shows consuming set observations obsik reduces ambiguitywithin agent belief state Bki . addition, consumption messages alsobeneficial effect reducing ambiguity trajectory-set r [1, l]. fact, refinedbelief state turn used filter trajectory-set follows:refined r [1, l] = selectrefinedBi r [1, l].k(9)refined r [1, l] maintains trajectories k-th stepstate refined Bki . important result since agent take advantageobservations whenever available, even though refer past execution step.may happen, fact, even though obsik enough determine outcomeaction aik1 , another belief state Bhi refined r [1, l] becomes sufficiently precisedetermine outcome pending action aih1 . next section, exploitcharacteristic determine outcomes pending actions.4.4 Inferring Propagating Action OutcomesWhenever current trajectory-set refined observations, useful scanpending action list pActsi , assess, action aik pActsi , whether either Definition3 4 applies.outcome action important piece information exploit,well observations, refine current trajectory-set. outcome action aik , eitherpositive negative, fact used infer outcome actions pActsi .reach result exploit notions causal predecessors aik (predecessors(aik )),causal successors aik (successors(aih )). First all, say action aih indirectlyprovides action aik service, aik indirectly receives service aih , iffexists sequence actions aiv1 , . . . , aivn that:1. aiv1 coincides aih2. aivn coincides aik.3. action aivx , x : 1..n 1, exists causal link haivx , q, aivx+1 Clocalwords, must exist chain causal links starts aih , passesactions sequence aiv1 , . . . , aivn , ends aik . Indirect causal dependencies pass plans agents considered definition.example, two causal links haih , q, ajv hajv , q 0 , aik i, cannot say aih25fiMicalizio & Torassoindirectly provides aik service since action ajv belongs agent j. limitation, advantage otherwise agents interact heavily order computeindirect causal relations. notion indirect dependency actions basislocality principle allows agent consider portion local planmonitoring diagnosis.set predecessors (aik ) therefore subset Ai including actionsdirectly indirectly provide aik service. side, successors(aik )subset Ai including actions which, directly indirectly, receiveservice aik .Given action aik , denote chains to(aik ) subset causal links Clocaldefinedactions predecessors(ak ). Similarly, denote chains f rom(ak ) subsetdefined actions successors(aki ).causal links ClocalProposition 3 Let aik action whose outcome ok, causal linkschains to(aik ) represent services satisfied.fact, aik outcome ok, services required aik satisfied,recursively, services required actions predecessors(aik ) satisfied too.Proposition 4 (Backward Propagation Success) Let aik action whose outcome ok, let us mark satisfied causal links chains to(aik ), actionpActsi predecessors (aik ) outgoing links marked satisfied, outcome ok,too.Proposition 5 Let aik action whose outcome failed, serviceschains f rom(aik ) might missing.fact, since aik outcome f ailed, least one expected effects missing;hand, action aik could reached subset effects, services couldsufficient enable subsequent actions. forward propagation failure musttherefore take account results successfully achieved. Let us denote miss(aik )set causal links leaving aik representing missing services.Proposition 6 (Forward Propagation Failure) Let aik action whose outcomefailed, let us mark missing causal link cl chains f rom(aik ) reachableone links miss(aik ) via chain missing causal links, unless cl alreadymarked satisfied. Then, action pActsi successors(aik ), least oneoutgoing link marked missing, outcome failed, too.Intuitively, properties 4 6 assume action performed fullyenabled produce correct results. hand, action achieveseffects must performed fully enabled, hence servicesmentioned premises must provided.26fiCooperative Monitoring Diagnose Multiagent Plansa22a31...7a1a684a43a5a7...65.Figure 5: portion local plan restricted causal links ClocalExample 4. example show outcome action actually exploiteddetermine outcomes actions. course, agent able determineoutcome relying observations messages agents. cooperativeprotocol discussed details following subsection; time being, importantobserve that:positive messages (formalized confirm messages following) receivedgiven step processed negative message (i.e., disconfirm message)received step;agent receiving least one negative message stop execution plan,start diagnostic phase.shown,Let us consider plan segment Figure 5, links Clocallet us assume agent performs actions order a1 , a2 , a3 , a4 , a5 , a6 ,actions pending. execution a5 , agent discovers a5outcome ok. outcome propagated backwards: predecessors(a5 ) = {a1 , a4 }, therebylinks chains to(a5 ) = {4, 5} marked satisfied; course, link 6 also markedsatisfied nominal outcome a5 . enables conclude action a4outcome ok; whereas nothing concluded action a1 since links 1 7 neithermarked satisfied, missing. Let us assume receives observationsservice link 1, consequence concludes a1 outcome failed.case outcome propagated forwardly: successors(a1 ) = {a2 , a3 , a4 , a5 , a6 , a7 },thereby chains f rom(a1 ) = {1, 2, 3, 4, 5, 6, 7, 8}; however, links 4, 5, 6 alreadymarked satisfied; addition, links 7 8 reachable via chain missingcausal links link 1; thus, links 2, 3 marked missing. Agent henceconcludes actions a2 a3 outcome f ailed. outcome inferredaction a6 , remains pending, outcome inferred action a7performed yet. outcome pending action a6 inferred means diagnosisinferences discussed Section 5.Relying properties 4 6, determine outcome pending actionsexploiting causal dependencies existing among actions, even thoughcurrent trajectory-set still ambiguous apply either Definition 3 (outcome ok)Definition 4 (outcome failed).27fiMicalizio & TorassoNote discover action aih outcome ok, exogenous event occurredaction necessarily ; thus, also filter r [1, l] follows:refined r [1, l] = selecteh = r [1, l](10)eh refers h-th exogenous event labeling transition state sh statesh+1 r [1, l]. refinement (10) keep r [1, l] trajectoriesh-th exogenous event . Thus keep transitions obtainedrelation , prune spurious trajectories contributed relation .hand, outcome action aih f ailed, cannot refinetrajectory-set via eh since know eh cannot , already implicitlyobtained thanks refinement equation (9).Summing up, weak-committed methodology able deal scarce observations using two essential mechanisms. First, build trajectory-set maintaininghistory agent state, keep list pending action outcomes. Second,take advantage observations whenever available revising knowledgeagent itself; favorable case, revision process empty setpending actions.4.5 Cooperative Monitoring Protocollast element CWCM methodology cooperative monitoring protocolallows agent exploit information provided others. idea agenttake advantage direct observations, also observationsagents environment, particular shared resources.cooperative protocol plays central role preserving resource safeness requirement even actions fully enabled performed.4.5.1 Interaction ScenariosBaDE strategy, CWCM two agents, j, need interactshare causal link lk : hail , v = d, ajm v RES, dom(v), v =value assignment representing change state resource requested agent); whereas,j. Contextually lk, agent plays service provider role (with lk Coutjagent j plays role service client (with lk Cin ). following first presentthree interaction scenarios CWCM. shortly report messagesexchanged two agents. Then, present client provider roles detailmeans high-level algorithms.Notify-ready interaction interaction provider sure providedclient requested service. Thus, provider sends message habout lk notifyv = readyi client j; answer client provider required.Notify-not-accomplished interaction scenario, agent sure requestedservice missing; therefore sends agent j message habout lk notify v = notaccomplishedi client j; answer j foreseen.Ask-if interaction case, provider unable determine whetherservice v = achieved; thus asks j info sending j message28fiCooperative Monitoring Diagnose Multiagent Planscooperative-protocol::client(inbox, r [1, l], ail )1. message m: h lk notify v = ready inbox s.t. lk incoming link ail2.remove inbox3.assert v = frontier r [1, l]4. end5. message m: h lk ask-if v = accomplished? inbox s.t. lk incoming link ail6.remove inbox7.unable observe v8.reply h lk no-info9.else10.obs observe v11.obs equals12.reply h lk confirm v =13.else obs equal14.reply h lk disconfirm v =15.end16.end17. end18. message m:habout lk notify v = not-accomplishedi inbox s.t. lk incoming messageail19.remove inbox20.stop plan execution21. endFigure 6: pseudo-code cooperative protocol, client behavior.habout lk ask-if v = accomplished?i. client reply message threedifferent ways:1. habout lk confirm v = di, message confirms provider expectedservice v = actually achieved;2. habout lk disconfirm v = di expected service missing;3. habout lk no-infoi client unable determine whether assignmentv = holds environment not.case receives no-info message j, eventually reply either readymessage not-accomplished one.4.5.2 Client Rolealgorithm Figure 6 outlines behavior agent behaving client.algorithm takes inputs inbox (i.e., collector messages coming agents),current trajectory-set r [1, l], next action performed ail .Agent consumes message inbox service requiredpremise execution ail . incoming message type ready (lines 14), agent uses information provided another agent observation, useterm assert (line 3) shortcut relational operations presented equations (8)(9).incoming message ask-if interaction (lines 5 17), agent determines whether capable observing v (e.g., equipped right sensor v?).29fiMicalizio & Torassocase cannot observe v, replies provider no-info message. Otherwise,agent acquires observation v, replies provider accordingly.Finally, whenever agent receives not-accomplished message (lines 18 21),stops execution plan service required performing ail missing.1important note agent playing client consumes messagerelevant next action performed. Thereby, ask-if message couldanswered certain amount delay.4.5.3 Provider Roleprovider behavior outlined Figure 7. algorithm takes inputs inbox,current trajectory-set r [1, l], list pending actions pActsi , last performedaction ail . precisely, last argument either null, actionperformed recently, actual action instance whose outcome still assessed.refine concept recently performed action next section presentmain CWCM plan execution loop.algorithm starts checking inbox order consume answers (if any)previous ask-if interactions. algorithm specifies behavior agent accordingtype received message. case confirm messages (lines 1 5), agent usesv = observation refine trajectory-set. term assert usedshortcut relational operations equations (8) (9); belief stateactually refined k-th +1; is, one contains effects action aik .case disconfirm message (lines 6 10), agent prunes k-th +1 beliefstate r [1, l] state v = holds. case incoming messagetype no-info (line 11 18), agent checks whether outgoing links actionmarked ans-no-info, meaning none services providedaik Coutak agents achieved sure. case, agent marks aiknot-enough-info.preliminary steps, agent possibly acquired informationothers. Thus, assess outcome pending actions pActsi , including ailnull (line 19). algorithm Figure 8 outlines steps assessing outcomesactions pActsi , discussed later on. sufficient say assess-pendingactions returns two lists actions, ok-list f ailed-list, empty, containactions whose outcome ok f ailed, respectively. course, whenever action pActsifound either ok f ailed, removed pActsi , added correspondinglist. process also involves actions previously marked not-enough-info.action ail null (an action performed recently), first timeoutcome ail assessed. Thus, case ail outcome pending (line 20), agentstarts ask-if interaction (lines 21-24) asking information agentsrequires one services produced ail . Otherwise, ail null ail outcomepending, ask-if interaction skipped.line 25 line 34, agent sends ready not-accomplished messagesaccording actions ok-list f ailed-list, respectively. addition, agent sends1. impact action failure estimated means failure propagation mechanism (Micalizio& Torasso, 2007b). sake discussion, leave topic paper.30fiCooperative Monitoring Diagnose Multiagent Planscooperative-protocol::provider(inbox, r [1, l], pActsi , ail )1. message m:habout lk confirm v = di inbox2.remove inbox3.let lk haik , v = d, ajm4.assert v = k-th +1 belief state within r [1, l]5. end6. message m:habout lk disconfirm v = di inbox7.remove inbox8.let lk haik , v = d, ajm9.prune k-th +1 belief state within r [1, l] state v =10. end11. message m:habout lk no-infoi inbox12.remove inbox13.let lk haik , v = d, ajm14.mark lk ans-no-info15.links outgoing aik marked ans-no-info16.mark aik not-enough-info17.end18. end19. hok-list, f ailed-listi assess-pending-actions(pActsi , r [1, l])20. ail null outcome pending21.link lk:hail , v = d, ajm i, lk Cout(i 6= j)22.send j message m:h ask-if v = accomplished?i23.end24. end25. action aik ok-list26.link lk : haik , v = d, ajm27.send j message m:habout lk notify v = readyi28.end29. end30. action aik s.t. (aik f ailed-list) (aik pActsi marked not-enough-info)31.link lk : haik , v = d, ajm32.send j message m:habout lk notify v = not-accomplishedi33.end34. end35. return hok-list, f ailed-listiFigure 7: pseudo-code cooperative protocol, provider behavior.not-accomplished message pending action aik marked not-enough-info. pendingaction marked not-enough-info highlights scarcely observable environment is.fact, neither agent i, agents waiting services provided aik , capabledetermine whether least one expected services provided not.deal ambiguity, agent prudentially considers action probably failed.Although choice could seem strong, necessary preserve resource safenessrequirement scarcely observable environments. Agent evidence supportingsuccessful achievement effects expected ail , hence cannot notifysuccess. time, agents might waiting services provided ail ,thus agents would stalling without even knowing it. Considering ail failed allowsget impasse notifying failure agents, may attemptform plan repair.31fiMicalizio & Torassoassess-pending-actions(pActsi , r [1, l])1. ok-list {}2. f ailed-list {}3. action aik pActsi4.Bk+1, |= effects(aik )5.ok-list ok-list {aik }6.r [1, l] selectek = r [1, l]7.oks propagateSuccess(pActsi , aik )8.ok-list ok-list oks9.pActsi pActsi \ oks10.else Bk+1, 6|= effects(aik )11.f ailed-list f ailed-list {aik }12.faultypropagateFailure(pActsi , aik )13.f ailed-list f ailed-list faulty14.pActsi pActsi \ faulty15.end16. end17. remove, present, mark not-enough-info action ok-list f ailed-list18. return hok-list, f ailed-listiFigure 8: pseudo-code assessment pending actions.algorithm terminates returning two lists ok-list f ailed-list callingalgorithm, shown Figure 10 discussed next section.4.5.4 Assessing Action Outcomespresenting main CWCM algorithm, shortly present algorithm assessingpending actions pActsi given execution step. discussed earlier, assessmentrelies properties 4 6, equation (10). algorithm shown Figure 8;takes inputs list pending actions pActsi , current trajectory-set r [1, l].algorithm returns two lists, ok-list f ailed-list, actions whose outcomes eitherok f ailed, respectively.algorithm considers actions pActsi (if any), tests whetheraction outcome ok f ailed. first case, success backward propagated(line 7): oks list successfully completed actions discovered means propagation; actions removed pActsi added ok-list. second case,failure forward propagated (line 12): faulty list faulty actions discovered meansfailure propagation; actions removed pActsi added f ailed-list.algorithm terminates returning two, possibly empty, lists ok-list f ailed-list.mentioned above, action aik pActsi , previously marked not-enough-info,found definitive outcome (ok f ailed). may happen because, althoughagents provided information effects aik , agent couldexploit outcome propagation actions preceding following aik . course, marknot-enough-info removed actions ok-list f ailed-list.Proposition 7 (Protocol Correctness - Resource Usage) cooperative monitoring protocol guarantees resource safeness requirement never violated32fiCooperative Monitoring Diagnose Multiagent Plansexecution MAP P . words, shared resources used correctly throughoutplan execution even action failures occur.Proof: Let us consider interaction scenarios, showresources accessed consistently; namely, never happens two (or more) agentsaccess resource simultaneously.Given causal link lk : haik , res = available, ajm i, interaction activated agentdepends outcome action aik .notify-ready interaction equivalent interaction BaDE framework,occurs aik outcome ok. case expected services achievedsure. Thus, notifies j res available, already released res:resource passed j consistently.ask-if scenario occurs aik outcome pending, splits three cases.1. Agent j (i.e., client) directly observes resource available. thereforeaccess resource safely mutual exclusion.2. Agent j directly observes resource still occupied i. case jattempt access res. resource used single agent resourcesafeness requirement violated.3. Agent j unable say whether resource res available. point viewj, state res unknown, hence, since preconditions ajmsatisfied, j keeps waiting information i. Also case res usedone agent.last interaction scenario occurs aik outcome f ailed. case, notifiesj resource available: j try use res preconditions ajmsatisfied.Proposition 8 (Protocol Correctness - Provided Services) Let j two agentsplaying roles provider client, respectively, given causal link lk : haik , v =d, ajm i. cooperative monitoring protocol enables two agents determine actualvalue variable v least determine whether v different expected value d.Proof: proposition proved considering different interaction scenariosprotocol. notify-ready interaction occurs agent conclude actionaik outcome ok. (This may happen means direct observations effectsaik , means backward propagation nominal outcomes.) Since action aikoutcome ok, effects, including v = d, achieved. ask-if interactionoccurs agent cannot determine outcome aik , hence truth valuestatement v = known. case agent j charge determining whetherstatement v = true false. Agent j reach result means directobservations v. possible answers j three:j directly observes v = d, thus service provided;j directly observes v d, thus service provided;33fiMicalizio & Torassoe3 =B1A1r A1 [1, 5]B2A1s223B4A1obsA14B5A1B3A1s46s8s57s911s68s1012s14s79s1113s15s1210s134s11s35Figure 9: trajectory-set kept agent A1 execution first four actions.j unable observe v, case agent relies answers provided agents,any, asked link. Agent conclude v = trueleast one received answers allows conclude nominal outcome aik ;otherwise, action assumed failed, hence also service v = consideredmissing.last interaction scenario, agent directly observed, indirectly inferred meansforward failure propagation, v = false.proposition considered sort generalization Proposition 7applies possible services, services mentioning available value.proposition important allows agents diagnose withoutnecessity interacting other, discuss Section 5.Proposition 9 (Protocol Complexity) number messages exchanged amongagents linear number n inter-agent causal links.Proof: provider-client interaction occurs agent, playing roleprovider, performed action ail least one outgoing, inter-agent causal link.number messages exchanged handling inter-agent causal link dependsoutcome ail . ail either outcome ok f ailed, provider sends one messageclient (i.e., ready not-accomplished, respectively). hand, ailoutcome pending, two agents exchange three messages: provider sendsask-if, client answers no-info, provider either replies ready not-accomplished.Thus worst scenario, number messages exchanged among agents 3n,hence O(n).Example 5. Let us assume execution first four actions, trajectoryset kept agent A1 one depicted Figure 9. trajectory-set contains 5 beliefstates none sufficiently refined determine outcome action; thus,actions currently pending. edges state another labeledrepresent nominal progress plan execution; others instead, labeled1 . . . 13 , model occurrence exogenous event (possibly ? ).34fiCooperative Monitoring Diagnose Multiagent Plansshow agent take advantage pieces information providedothers, let us assume agent A1 receives another agent observation obsA14effects action a3 . instance, let us assume action a3 corresponds moveaction, observation obsA14 refers position agent A1 executionA1a3 . Observation obs4 therefore used refine belief state B4A1 . example, s8s9 states B4A1 consistent observation. Thanksfirst refinement (see equations (8) (9)) able prune trajectoriespass either s8 s9; trajectories depicted dottededges. Dashed edges, hand, still possible trajectories still keptwithin trajectory set.However, refinement trajectory-set possible discoverrefined B4A1 sufficiently precise determine action a3 outcome ok.fact, A1s position conveyed observation obsA14 matches expected one;means event e3 , affecting a3 , . pruning trajectory-set e3 =(equation (10)), fortunate case B3A1 contains state s4 ,nominal effects action a2 satisfied, too. backwards propagating success a3 ,first a2 , a1 conclude three actions outcome ok.fact, process, resulting trajectory-set maintains bold, solid edges;whereas dashed edges pruned off. resulting trajectory-set, however,allow us conclude anything a4 , still remains pending.4.6 Cooperative Weak-Committed Monitoring: Main Algorithmmain CWCM algorithm outlined Figure 10. agent follows algorithm execute monitor local plan P .initial steps set agent trajectory-set set pendingactions, algorithm iterates actions P far next action performedcoincides pseudo-action ai , meaning P completed. (Remindassume actions P providing atoms premises(ai ) observable effects.)beginning iteration, agent interacts agents (line 5) playingclient role cooperative protocol. step, agent consumes readynot-accomplished messages (if any), acquires information resources requiredperform next action ail . case agent receives not-accomplished message, stopsplan execution preconditions ail never satisfied. caseask-if message received, agent establishes whether able observe requiredservice answers accordingly (see algorithms figures 6 7).new information acquired asserted within agent trajectory-set,agent assesses whether next action ail possibly enabled (Definition 2). positivecase, action performed real world (line 8). Subsequently, agent estimatespossible evolutions ail exploiting (ail ) (ail ) extend currenttrajectory-set (line 9). completion action ail , agents direct observationsgathered obsil+1 (line 10) asserted extended trajectory-set (line 11); alsocase assert shortcut relational operations described equations (8)(9). Action ail temporarily put list pending actions (line 12).outcome assessment fact postponed step regards current pending actions,35fiMicalizio & TorassoCooperative-Weak-Committed-Monitoring(P )1. l 12. r [1, l] //The initial belief state initial state agent3. pActsi4. ail 6=5.cooperative-protocol::client(inbox, r [1, l], ail )6.last null7.ail possibly enabled frontier r [1, l]8.execute ail9.r [1, l + 1] r [1, l] M(ail )// trajectory-set extension using (ail ) (ail )10.obsil+1 gather direct observations11.assert obsil+1 frontier r [1, l + 1]12.pActsi pActsi {ail }13.last ail14.l l+115.end16.hok-list, f ailed-listi cooperative-protocol::provider(inbox, r [1, l], pActsi , last)17.f ailed-list 6= aik pActsi marked not-enough-info18.stop execution19.diagnose(P , pActsi , ok-list, f ailed-list, r [1, l])20.switch safe mode21.end22. endFigure 10: Cooperative Weak-Committed Monitoring: high-level algorithm.activated even action executed. important noteiteration loop necessarily corresponds execution action.seen, provider behavior cooperative protocol needs know whetheraction recently performed (i.e., whether action performedcurrent iteration). purpose use variable last, set nullbeginning iteration, set action ail action actually performed(line 13). Whenever action performed, counter l incremented (line 14);is, l-th plan execution step completed.loop proceeds agent behaving provider (line 16). step alsoincludes evaluation outcomes actions pActsi list (see algorithmFigure 7). provider behavior returns two lists, ok-list f ailed-list, maintainingactions outcome ok f ailed, respectively; course, lists could empty.side effect, pActsi modified removing action whose outcome longer pending.list f ailed-list empty, least one action pActsi marked notenough-info, agent stops plan execution, starts diagnostic process (discussedSection 5), switches safe mode. agent safe mode perform actions,interacts agents trying reduce impact failure. First all, agentsafe mode answers ask-if message no-info, prevents sender waitingindefinitely answer. Moreover, agent safe mode releases many resourcespossible sending appropriate ready messages; allows agents accessresources proceed plans. detailed discussion safe modescope paper, found works Micalizio Torasso (2007b)Micalizio (2013).36fiCooperative Monitoring Diagnose Multiagent Planscase failure discovered, actions performed far outcomeok (i.e., pActsi gets empty), trajectory-set r simplified. fact, sincepast actions nominal outcome, longer required keep whole past historysince beginning plan execution. Thus, safe convenient forget pastkeep within trajectory-set frontier. implementation usedexperiments adopts strategy keeping size trajectory-set manageable.sake discussion, provide details point.4.7 Cooperative Weak-Committed Monitoring: Correctnessconclude section discussing correctness algorithm Figure 10.Theorem 1 [CWCM Correctness] CWCM assigns action aik outcome:- ok iff action affected exogenous events;- f ailed iff exogenous event, possibly ? , affected aik ;- alternatively, CWCM marks pending action aik not-enough-info iff outcomeinferred relying observations agents, outcome propagation technique.Proof: Part 1: action aik outcome ok iff aik affected exogenous events.words, show aik reaches effects(aik ) iff ek = trajectory withinr [1, l], k : 1..l 1.() contradiction, let us assume effects(aik ) reached, nominaltrajectory pruned r [1, l]. happen monitoring processtwo ways: (a) observations, (b) outcome propagation. Let usconsider case (a), let us suppose monitoring phase agent receives observations obsik+1 consistent effects(aik ). effect pruning r [1, l] obsik+1 ,nominal transition ek = pruned r 1 [1, l]; this, however, contradictiondefinition extended model M(aik ), nominal transitions labeledlead nominal effects(aik ). Thus, either obsik+1 inconsistent effects(aik ),hence aik cannot ok, obsik+1 consistent effects(aik ) ek trajectorieswithin r [1, l].Let us consider case (b), outcome propagation. two cases: backwardpropagation ok, forward propagation f ailed. backward propagation okpossibly assigns nominal outcome actions aik pActsi ; propagation, ekequals trajectory within r , definition. forward propagation f ailedpossibly assigns nominal outcome actions aik pActsi ; propagation ek trajectory r [1, l]. two propagations cannot changeoutcome action pActsi : action already assigned outcome, outcome cannot changed anymore. particular, aik outcome ok,aih predecessors(aik ) discovered faulty, forward propagation f ailed cannotprune ek = r [1, l]. fact, discussed Proposition 6, forward propagationimpacts causal links neither marked satisfied, missing,along chain links starting one links miss(aih ). aikassigned outcome ok, agent must received sufficient observations determinepremises aik satisfied. follows services required aik37fiMicalizio & Torassoalready marked satisfied. Thus, nominal transition ek = cannot losteffect outcome propagation.() aik affected exogenous event, hence eh = trajectory withinr 1 [1, l], aik outcome ok (i.e., reaches effects(aik )). construction, extendedmodel M(aik ) guarantees transitions labeled leads statesexpected effects hold. follows that, ek = trajectories r [1, l], aik mustoutcome ok necessarily.Part 2: action aik outcome f ailed iff exogenous event, possibly ? , affectedexecution. demonstrated following reasoning similar one Part 1;omit brevity.Part 3: action aik marked not-enough-info iff outcome inferred relyingobservations agents, outcome propagation technique. easy seeCWCM marks aik not-enough-info one occasion: provider behavior,answers gathered aik ask-if interaction no-info. exactlymeans agent team provide information services providedaik . hand, marking removed aik inserted eitherok-list f ailed-list; thus cannot happen action definite outcomealso marked not-enough-info.Theorem 2 Given MAP system = hT , RES, P i, P plan hI, G, A, R, Ci,global goal G achieved iff actions outcome ok.Proof: previous theorem demonstrated action outcome okeffects achieved, outcome cannot changedeffect refinements trajectory-set. Thereby, global goal Greached, actions must reached effects, hence must outcome ok.fact, since assume P redundant action (i.e., action P contributesG), sufficient least one action fails reaching one effect have: 1) leastone action outcome f ailed, 2) least one piece G achieved.hand, actions outcome ok, G must achievednecessarily. absurd, actions ok, G reached.happen P flaw, produce G even nominal conditions,initial assumptions (see Section 2) P flaw-free actually produces G.Example 5 used clarify proof. example shown that,restrict B4A1 belief state state satisfies expected effects actiona3 , action a3 outcome ok. time, outcome backward propagatededges labeled lead B4A1 . action a4 last action A1splan, effects action must observable, hypothesis. Now, dependingavailable observations, agent A1 either conclude s12 actual state a4(thereby: (1) goal reached, (2) trajectory-set contains one trajectoryedge labeled , (3) actions outcome ok), s13actual agents state, hence least one action (i.e., a4 itself) must outcome f ailed.Corollary 1 global goal G achieved, agent keeps trajectory-setr [1, l] nominal trajectory hs1 , , s2 , . . . , , sl+1 i, s1 |= sl+1 |= Gi .38fiCooperative Monitoring Diagnose Multiagent PlansProof: follows two previous theorems. G reached, actionsoutcome ok (Theorem 2). hand, since aik ok iff ek =every trajectory within r [1, l] (Theorem 1), follows agent keepstrajectory-set nominal trajectory.correctness monitoring process therefore summarized followingstatement: execution P affected anomalous event, cooperativemonitoring able keep trace progress achievement goal G sincenominal transition never lost. hand, execution P affectedleast one anomalous event, even known advance, cooperative monitoringable detect stop execution phase. addition, Proposition 7 assuresnominal, well anomalous, situations resources always accessed consistently.5. Plan Diagnosis: Local StrategyPlan diagnostic inferences start soon CWCM algorithm discovered failureleast one action (i.e., f ailed-list empty), pending action marked notenough-info. section discuss mean plan diagnosis,inferred. propose distributed approach agent infers diagnosislocal plan autonomously. fact, thanks Proposition 8, plan execution saferespect use resources, agent never blame agents explainaction failures.5.1 Inputs CWCMprevious section focused monitoring purpose CWCM methodology. important note, however, CWCM also produces useful pieces informationdiagnostic point view. First all, actions f ailed-list could consideredplan diagnosis according definition Roos Witteveen (2009); namely, subsetactions assumed faulty explain observations. However, f ailed-listtake account action failures might indirect consequencesothers. Thus, f ailed-list sufficient would like isolate primary actionfailures caused secondary action failures.addition, CWCM produces trajectory-set r [1, l], seen setconsistency-based diagnoses (Reiter, 1987). trajectory r [1, l] possibleexplanation agents behavior consistent observations received agentitself.5.2 Event-Based ExplanationsDealing directly r [1, l], however, might awkward since encodes possible explanations, including ones mentioning indefinite exogenous event ? ,considered unlikely. Moreover, trajectories share sequenceevents, differ state variables, considered completely different explanations. Thus, r [1, l] needs processed order useful. first reductionr [1, l] given projecting event variables e1 , . . . , el1 ; call resulting39fiMicalizio & Torassostructure Event-based Explanations (EVE):EVE = projecte1 ,...,el1 r [1, l].(11)EVE set sequences exogenous events (including ? ). sequenceset possible consistency-based diagnosis anomalous behavior agent.Since EVE could still contain huge number diagnoses, EVE informativehuman user decide recover plan failure. One wayreducing number diagnoses would prefer diagnoses involveminimum number exogenous events. Unfortunately, preference criterion would leadmisleading results events dependent one another. find meaningfulexplanations, one identify exogenous events caused primary action failuresexogenous events correspond secondary action failures.5.3 Minimum Primary Action Failuresfacilitate identification primary action failures, distinguish indefiniteevents ? contributed portion action model, indefinite events ? contributed portion. distinction necessary CWCM, turnsuseful diagnostic purpose. Intuitively, ? denotes occurrence exogenousevent affecting execution (possibly) enabled action; ? therefore unknownabrupt change affecting nominal behavior action. hand, ?indefinite event use label state transitions action performedstate satisfying preconditions. Relying distinction, possibleidentify primary failure means following definition.Definition 7 action ak pActsi f ailed-list primary action failure iff existsexplanation x EVE x[ek ] 6= x[ek ] 6= ? , x[ek ] k-th eventexplanation x.words, action ak considered primary failure given event-basedexplanation x EVE iff occurrence exogenous event mentioned (ak )assumed x. Note Definition 7 also examine set pending actions pActsi ,including actions marked not-enough-info. addition, note set primary actionfailures never empty. fact, agent starts diagnosis phase oneperformed actions labeled failed. hand, agent stopsexecution plan another agent fails providing service, first agentexonerated diagnosing since none actions labeled failed,root causes missing service located outside plan.Secondary failures caused primary failure, defined follows:Definition 8 Let x EVE possible explanation, let ak f ailed-list pActsiprimary failure x, actions ah successors(ak ) x[eh ] = ?secondary failures caused ak according explanation x.Note that, given primary failure ak explanation x EVE , actionssuccessors(ak ) necessarily secondary failures (see Proposition 4). fact, even thoughak achieved effects (i.e., outcome failed), action may reached40fiCooperative Monitoring Diagnose Multiagent Plansthem. consequence, actions successors(ak ) may enableddespite failure ak . reason, Definition 8 require action ahsuccessors(ak ) labeled secondary failure exogenous event ? assumedexplanation x. definitions primary secondary failures propositionfollows directly.Proposition 10 Given explanation x EVE , set primary action failures P rmx ,set secondary action failures Sndx extracted x disjointed.Relying proposition, define Primary Action Failure Diagnoses (PADs):Definition 9 Let x EVE possible event-based explanation, primary action-failureexplanation (PAD) extracted x pair hP rmx , Sndx P rmx Sndxsets primary secondary failures, respectively, extracted x.course, since EVE general contains several explanations, since primary failuresassumed independent other, possible extract minimum cardinalityprimary action-failure diagnoses (mPADs) simply selecting explanationsminimum set primary failures:mP ADs = {P rmx x EVE |P rmx | minimum }(12)Minimum primary action failure diagnoses (mPADs) indeed mean plandiagnosis: localize actions qualified failed order explainanomalous observations.25.4 Refining Plan Diagnosisinferred plan diagnosis, one refine diagnoses identifying rootcauses. refined explanations expressed terms exogenous events,extracted EVE set.Definition 10 Let ah primary action failure, let EVE (ah ) set explanations x EVE ah P rmx , refined explanation action ahrefinedExp(ah ) =[x[eh ].(13)xEVE (ah )words, refinedExp(ah ) consists exogenous events mightoccurred execution action ah , hence might caused failure ah .course, since ah primary failure, secondary failures caused ah alsoexplained occurrence one events refinedExp(ah ).2. Note different preference criteria could adopted select explanations EVE . instance, onecould prefer minimality rather minimum cardinality.41fiMicalizio & Torassoa1a2a4a3a5a7a6a8Figure 11: portion local plan assigned agentExample 6. Let us consider simple local plan Figure 11 assigned agent i.. Let us assume that,simplify picture show local causal links Clocalexecution local plan, agent detects failure action a8 . diagnosticprocess activated order explain failure identifying (minimum) setprimary action failures. diagnostic process receives input list failed actionsf ailed-list={a8 }, list successfully completed actions ok-list={a4 }, listpending actions pActsi = {a1 , a2 , a3 , a5 , a6 , a7 }. addition, diagnostic processreceives also trajectory-set r [1, 9], simplicity show Table 2 setevent-based explanations (EVE ) extracted trajectory-set.Table 2 easy see explanations, except last one, explainfailure action a8 indirect effect previous failure (i.e., a8 secondaryfailure). last explanation considers a8 primary failure, unknown,unlikely, exogenous event ? must assumed.first step diagnostic process consists inferring set mP ADs diagnoses.Thus, identify primary secondary failures explanation EVE :P AD : { x1, x2 :x3 :h{a1 },{a3 , a5 , a8 }ih{a6 , a7 },{a8 }ix4 : h{a3 , a6 , a7 }, {a8 }ix5 :h{a2 },{a7 , a8 }ix6 :h{a8 },}observe interesting consequences. First all, explanations EVEcollapsed within single explanation P ADs; see instance explanations x1 x2.advantage reduce number alternative explanations. addition,sets primary action failures used identify (subset-)minimal diagnoses.instance, explanation {a6 , a7 } derived x3 minimal diagnosis, whereas explanation{a3 , a6 , a7 } extracted x4 not. Finally, since assume primary failuresindependent other, prefer subset-minimal diagnoses whose cardinalityx1x2x3x4x5x6a115a24a3??6a4a5??a622a733?a8??????Table 2: set EVE maintained within current trajectory-set42fiCooperative Monitoring Diagnose Multiagent Plansminimal. example mP ADs = {{a1 }, {a2 }, {a8 }}. fact, thus sufficientassume failure one actions explain observations.step, action mP ADs, one also infer refined diagnosis.instance, easy see primary action failure a1 two alternative refineddiagnoses: either 1 5 (see Table 2); whereas primary action failure a2 4single possible refined diagnosis. Finally, one assume occurrence ? explainprimary action failure a8 . Relying refined diagnoses, preference criteria couldemployed conclude primary failure a8 less likely a1 a2 ,hence could disregarded.Note that, since agent able diagnose plan autonomously, plandiagnosis global level could inferred combining local solutions inferredagent team, integration guaranteed globally consistent. fact,thanks Proposition 8 agent never blame another agent failure oneactions.6. Experimental Analysisfar addressed CWCM methodology diagnostic inferencesdeclarative manner means relations Relational operators relations.Relations simple, yet powerful formalism represent nondeterministic action modelsambiguous belief states. addition, also used model complexstructures trajectory-set event-based explanations (EVE ).comes actually implementing CWCM methodology, however, mustnoticed computational complexity algorithm Figure 10 dominatedcomplexity (macro-)operator involved extension current trajectoryset. hand, diagnostic inferences based projection currenttrajectory-set event variables (see equation 11). steps might computationally expensive, efficient implementation relations relationaloperators therefore becomes essential. possible way cope issue translaterelations symbolic, hence compact, formalism, encode Relational operators operations selected symbolic formalism. Alternatively, maypossible exploit recent advancements Continuous Query Languages (CQLs) dealdata streams (see e.g., STREAM system Arasu, Babu, & Widom, 2006), implement CWCM relying primitives made available Data Stream ManagementSystem hand.paper, chosen method knowledge compilation, particular,selected Ordered Binary Decision Diagram (OBDD) (Bryant, 1986, 1992) formalism encode relation Relational operators. choice justified two mainreasons: first, OBDDs nowadays well-known language made available manymature libraries; second, theoretical results Darwiche Marquis (2002) suggestOBDDs answer queries polynomial time provided sizes remaintractable. in-depth description cooperative monitoring diagnosisimplemented via OBDDs reported Appendix.rest section organized follows. First, Section 6.1, sketchsoftware architecture implementation; Section 6.2, present experimen43fiMicalizio & TorassodomaininitialstatePXMLXMLXMLDISPATCHERA1P A1RA1outcome assessmentCW CMRANoutcome assessment...CW CMDIAGNOSISextend trjPr A1DIAGNOSISextend trjdetected failurerdetected failurecooperative protocol messagesobservationsobservations A1ANs next actionSIMULATORA1s next actiondomain; initial stateXMLexogenous eventsFigure 12: software architecture CWCM implementation used tests.tal setting used carry tests, consisting simulated execution severalMAPs. Finally, discuss interesting results monitoring (Section 6.4),diagnosis (Section 6.5).6.1 Software Architecture ImplementationCWCM proposal implemented Java SDK 7 program. softwarearchitecture shown Figure 12, highlighting main actors: Dispatcher, Nagents team (from agent A1 agent AN), Simulator. picture also showsinternal architecture agents. Solid edges modules represent data flows,dashed edges represents instead control flows, whereas dotted edge CW CMabstracts messages exchanged agents cooperative monitoring.simulation MAP P starts submitting Dispatcher module three XML filescontaining, respectively, system domain (i.e., agents objects definedscenario hand), system initial state (e.g., initial positions agents, initialstates resources, etc.), MAP P performed. Dispatcher decomposesP local plans agent receive portion P interest.particular, P decomposed, Dispatcher activates agents,implemented threads, passing initial states local plans.44fiCooperative Monitoring Diagnose Multiagent PlansOBDDs made available JavaBDD library 3 , provides java, easyto-use interface Java BuDDy 4 , popular mature library manipulatingOBDDs written C.Besides agents, Dispatcher activates also Simulator, implemented thread.Differently agents, however, Simulator receives input plan,initial state system. addition, Simulator reads fourth XML fileexogenous events injected plan execution. precisely,file list agents actions, associated anomalous eventmust occur execution action. course, subset actionsaffected exogenous events mentioned file.environment set-up, Dispatcher starts agents,execute CWCM algorithm discussed Section 4. actual execution actionsimulated Simulator: Whenever agent intends perform action,sends message Simulator conveying action performed. Simulatorsimulate action execution taking account possible exogenous eventsinjected. action associated observations, Simulator sendscorresponding agent appropriate message. worth noting also Simulator,like agent, uses OBDDs estimate next state whole system accordingactions currently progress. Differently agents, however,Simulator always knows precise state agent resource system.details use OBDDs handling relations given Appendix A.discussed Section 4, whenever failure action detected agent i,Diagnosis module agent activated. results diagnosis inferences,discussed Section 5 saved report file Ri associated agent i.experiments described following performed PC, Intel Core 2 Duo,2.80 Ghz, 8 GB RAM equipped Windows 7 OS. test repeated ten times,average values considered experimental analysis order absorb loadfluctuations CPU.6.2 Experimental settingdomain used tests already introduced Example 1. actionsagent perform summarized Table 3 5 , reporting detailsencoding action models OBDDs. precisely, # variables number statevariables OBDD defined; number includes one variable encodingpossibly anomalous event occurring action execution; remaining variablesused encode agent state transition step t, action starts, step t+1,action ends. Columns #-nodes #-trans. report, respectively, numbernodes OBDD encoding portion action model, number statetransitions encoded . Columns #M-nodes #M-trans. refer whole extendedmodel M, including portion. domain, agent handles 36 variablesencode belief state environment.3. http://javabdd.sourceforge.net/index.html4. http://sourceforge.net/projects/buddy/5. Examples test cases action models foundhttp://www.di.unito.it/micalizi/CWCM/index.html.45fiMicalizio & Torassomovecarryloadunload# variables15191717#-nodes193346386386#-trans.34384040#M-nodes291374892892#M-trans.4201857169169Table 3: details relational action models.6.3 Objectives Experimental Analysisleast three main questions want get answered meansexperiments. questions are:CWCM scale well number agents team grows?CWCM affected level system observability? extent?cooperation among agents really useful monitoring purpose?answer questions, carried tests varying three main characteristics:team size, observability level, monitoring strategy.6.3.1 Team Sizeassess scalability CWCM, generated MAPs teams 3 8 agents.Thus, 6 scenarios, them, synthesized 30 MAPs. maincharacteristics MAPs reported Table 4. Note MAPs trivialconsist significant number actions subgoals achieved. term MAPspan refers number execution steps required complete plannominal conditions full observability. concurrency rate, computed numberactions divided MAP-span, indicates agents perform actions concurrently.Finally, number inter-agent causal links shows often agents interactachieve subgoals.6.3.2 Observability Levelassess competence CWCM, MAPs performed different conditionsobservability. particular, considered three degrees domain observability.following, term FULL denotes complete observability effects actionsperformed agents. level observability unrealistic practice,represents benchmark compare performance CWCM observabilityconditions. term HIGH denotes degree observability guarantees observeeffects 70% MAPs actions, randomly selected. Finally, term LOW denotesdegree observability 30% MAPs actions, randomly selected.6.3.3 Monitoring StrategiesFinally, assess actual benefits achieved cooperation among agentsmonitoring phase, considered three alternative monitoring strategies:BaDE, already presented Section 2, simplest strategy, based strongcommitted policy.46fiCooperative Monitoring Diagnose Multiagent Plansscenario#agents#actions#subgoalsMAP-spanconcurrencyrateSCN33SCN44SCN55SCN66SCN77SCN8867.6747.0030.7814.0410.209.9469.0052.0027.905.423.152.9391.6077.8034.407.373.973.10128.9073.6027.0040.274.638.10180.4073.9030.9036.8418.355.27156.4048.8020.506.137.632.702.22.52.74.85.97.2#causal#inter-agentlinkslinks226.4410.532.821.87239.3020.113.391.45329.8026.811.341.1377.702870.144.75467.2036.660.554.92360.6045.006.902.82Table 4: Characteristics MAPs six scenarios nominal conditions (averagevalues confidence intervals).WCM (Weak-Committed Monitoring) introduced Micalizio Torasso (2008,2009) based weak-committed policy allows agents keep trajectorysets cope scarce observability. WCM, agent able keep pendingactions far actions provide services agents. DifferentlyCWCM, WCM agents cannot cooperate other; therefore,outcome action cannot precisely determined, provides another agentj (i.e., 6= j) service, assumed failed i, also stops executionplan.CWCM, discussed Section 4, extends weak-committed policy activecooperation among agents.6.3.4 Exogenous EventsAlthough exogenous events generated randomly, generation reflects(expected) probability given exogenous event occur. instance, completely unexpected event, encoded ? , unlikely occur, hence frequencyexperiments pretty low. Table 5 shows probability distribution used generateexogenous events randomly.6.4 Experimental Analysis: Monitoringexperimental analysis monitoring task subdivided two main parts.first one, assess three strategies BaDE, WCM, CWCM, nominalconditions; is, exogenous event occurs simulated executionMAPs. goal study impact observability degree competence47fiMicalizio & TorassoExogenous eventblocked-wheelwrong-movelose-parcelslip-parcelblocked-arm?Probability25 %10 %25 %10 %25 %5%Table 5: exogenous events frequencies experiments.HIGH observabilityWCMCWCM% achieved subgoals% performed actionsHIGH observabilityBaDE100806040200345678BaDEWCM80604020034# agent100806040200345667878LOW observabilityBaDECWCM% achieved subgoals% performed actionsWCM5# agentsLOW observabilityBaDECWCM10078WCMCWCM1008060402003456# agents# agentsFigure 13: [Nominal Conditions] Competence:achieved goals.percentage performed actionsthree strategies. second part, assess competence threestrategies exogenous events occur.6.4.1 Nominal ConditionsCompetence. competence estimated percentage actions performedsubgoals actually achieved agents. Since condition FULL observabilityagents perform 100% actions achieve 100% subgoalsthree strategies, Figure 13 report results HIGH LOW conditions.expected, BaDE sensitive observability degree. hand, sinceWCM CWCM keep trajectory-sets, tolerant partial observability,generally behave much better BaDE. CWCM better WCMcooperation agents allows compensate lack direct observationsmessages coming others. discussed Section 4.5, however, may possibleeven agents unable provide useful pieces information. Thus, alsoCWCM strategy, agent decides stop execution plan when,even asking agents observations, possible determine outcomeaction. explained Section 4.5, case agent stops executionplan marking actions not-enough-info. reason percentageperformed actions achieved goals 100% observability levels HIGH48fiCooperative Monitoring Diagnose Multiagent PlansFULL observabilityWCMLOW observabilityHIGH observabilityBaDECWCMWCMCWCMBaDE300300250250250200200200150msec300msecmsecBaDE15010010050505000456# agents78CWCM1501003WCM03456# agents78345678# agentsFigure 14: [Nominal Conditions] Monitoring time (average 95% confidence interval)single execution step.LOW. results obtained CWCM case remarkable: worst case, SCN5,least 80% actions performed 70% subgoals achieved despite30% actions observable.Computational Time. Figure 14 shows average time (and 95% confidence interval) monitoring single step execution. Note BaDE strategy monitoring consists estimating next belief state; whereas, WCM CWCMextend trajectory-sets. addition, CWCM also cooperate agents.cooperation introduce costs consumption message anotheragent corresponds operation OBDD encoding current trajectory-set. firstpositive result emerging Figure 14 that, even worst scenario, CWCM takes300 milliseconds monitoring execution action. allows usconclude CWCM could employed effectively real-world domains agentsactions performed order seconds.addition, easy see computational time strongly depends observability level. example, FULL observability, CWCM WCM behave similarly;case, fact, CWCM agents need cooperate other, hencetwo strategies almost same. However, observability decreases, CWCMslightly expensive WCM BaDE. higher cost counterbalancedcompetence CWCM, that, already noticed, outperforms competenceBaDE WCM.charts Figure 14 also apparent strict dependencynumber agents team computational time three strategies.This, fact, consequence distributed approach agent maintainspoint view environment, cooperation agentsbased exchange messages belief states.OBDD dimensions. relation time observability becomes clearconsider sizes OBDDs encoding trajectory-sets; see Figure 15, left.brevity report average sizes OBDDs maintained three strategiesHIGH LOW observability conditions6 . easy see exists relationcomputational time shown Figure 14 sizes OBDDs Figure6. FULL observability case, OBDD sizes CWCM well 3000 nodes, average.addition, CWCM WCM generate OBDDs similar sizes, expected.49fiMicalizio & TorassoHIGH observabilityHIGH observabilityBaDEWCMCWCMWCMHIGH observabilityWCMCWCMCWCM12.00120008000400003456780706050403020100Length trajectory# trajectories# OBDD nodes160003845LOW observabilityWCM78.006.004.002.000.008345# agents# agentsBaDE610.00LOW observabilityCWCMWCM67878#agentsLOW observabilityWCMCWCMCWCM12.00120008000400003456# agents7880706050403020100Length trajectory# trajectories# OBDD nodes160003456# agents7810.008.006.004.002.000.003456#agentsFigure 15: [Nominal Conditions] Left: Sizes OBDDs number nodes (average95% confidence interval); center: Average number trajectories withintrajectory-set; right: Average length one trajectory.15, left: bigger OBDDs higher computational time. alreadynoted, although OBDDs may get large, computational time still acceptable.(The biggest OBDD observed 17,707 nodes, built CWCMSCN5 LOW observability.)Obviously, level observability strong impact dimensions OBDDs. fact, reduced level observability makes trajectory-sets ambiguous,hence trajectories encoded within single OBDD. made explicit Figure 15, center, show number trajectories encoded, average,within trajectory-set time instant, length (Figure 15, right). course,two last charts, consider WCM CWCM since BaDE strategybuild trajectory-sets. Moreover, note actual implementation CWCM,extension trajectory-set cover whole plan performed far,current subset pending actions.CWCM Communication Analysis. conclude study nominal conditionsanalysis communication required CWCM methodology. Figure 16shows average number messages exchanged among agents. first interestingresult that, FULL conditions, number exchanged messages coincidesnumber inter-agent causal links. fact, since results taken nominalconditions, action reaches nominal effects; therefore, cooperative protocol handles inter-agent causal link means simple ready message sent providerclient, answer required. observability level HIGH, however,number messages tends increase, even though increase significantly exceptscenario SCN8. expected, largest number messages exchangedobservation LOW, expected.50fiCooperative Monitoring Diagnose Multiagent PlansCWCM messages nominal conditionsFULLHIGHLOW70# messages6050403020100345678# agentsFigure 16: number messages exchanged CWCM agents nominal conditions.HIGH observabilityCWCMBaDE1008060402004567860402003440200678# agents% achieved subgoals% achieved subgoals605678WCM604020034BaDE604020056# agents67878LOW observabilityCWCM8045# agents1003CWCM80HIGH observabilityBaDECWCM8045WCM100# agents1003BaDE80FULL observabilityWCMLOW observabilityCWCM100# agentsBaDEWCM78% achieved subgoals3% performed actions% performed actionsWCM% performed actionsFULL observabilityBaDEWCMCWCM1008060402003456# agentsFigure 17: [Faulty Conditions] Competence: percentages performed actions achievedgoals.6.4.2 Faulty ConditionsCompetence. Let us consider test set before, randomly injectsingle exogenous event MAP. goal assess well three strategiesbehave partial observability exogenous events combine together. Figure 17 showscompetence three strategies faulty setting three observabilitylevels. environment fully observable, three strategies behave exactlysame, expected. course, percentages performed actions achieved goalsdepend early, late, exogenous event occurs MAP. general,say least 70% actions performed despite injected exogenous event.similar consideration made percentage achieved goals.observability conditions degrade HIGH LOW, however, easysee CWCM outperforms two strategies. means CWCM actuallytolerant strategies partial observability even faulty scenario.51fiMicalizio & TorassoFULL observabilityWCMBaDECWCMWCMLOW observabilityCWCMBaDE30030025025025020020020015010015010050500034567msec300msecmsecBaDEHIGH observability8WCMCWCM15010050034# agents567834# agents5678# agentsFigure 18: [Faulty Conditions] Monitoring time (average 95% confidence interval)single execution step.HIGH observabilityHIGH obervabilityCWCMWCM12000100008000600040002000504030201000345673845WCM78WCMCWCM14000# trajectories1200010000800060004000200056# agents642034578WCM50403020103456# agents67878LOW observabilityCWCM0048# agents60310LOW observabilityLOW observabilityBaDE6CWCM12# agents# agents# OBDD nodesWCM60# trajectories# OBDD nodes14000HIGH observabilityCWCMLength trajectoryWCM78length trajectoryBaDECWCM1210864203456# agentsFigure 19: [Faulty Conditions] Average sizes OBDDs encoding trajectory-set (left),average number trajectories within trajectory-set (center), average lengthtrajectory-set (right).particular, since difference CWCM WCM cooperative monitoring, conclude cooperation among agents actually beneficial.Computational time. Figure 18 reports computational cost, milliseconds,three strategies faulty conditions three levels system observability.important note also case computational time strongly dependsobservability level; whereas depend number agents team,presence exogenous event. fact, time monitoring MAP affectedexogenous event order magnitude monitoring MAPnominal conditions. differences observed comparing charts Figure14 charts Figure 18 due fact execution MAP affectedfault terminates earlier MAP executed nominal conditions, independentlylevel observability.course, BaDE strategy cheapest three, unable monitoreffectively execution MAP. fact, strong committed policy basis52fiCooperative Monitoring Diagnose Multiagent PlansCWCM messages faulty conditionsFULLHIGHLOW60# messages50403020100345678#agentsFigure 20: number messages exchanged CWCM agents faulty conditions.strategy sensitive level observability, HIGH LOW conditionsperforms poorly.OBDD dimensions. Let us consider dimensions OBDDs maintainedthree strategies. left-hand side Figure 19, report sizes, numbernodes, OBDDs representing current belief state (BaDE strategy), currenttrajectory-set (WCM CWCM strategies) three conditions observability.expected, BaDE keeps smallest OBDDs since maintains last belief state,makes BaDE strategy unable deal low observability levels. WCMCWCM behave similarly FULL observability conditions, CWCM tendsmaintain bigger OBDDs observability level decreases. result explainedfact CWCM build longer trajectory-sets WCM (Figure 19, right),longer trajectory-sets tend ambiguous demonstrated averagenumber trajectories within trajectory-set (Figure 19, right).CWCM Communication Analysis. Figure 20 shows number messages exchangedCWCM agents faulty conditions. trend similar nominal conditions, however, number messages slightly lower. happens occurrence failure prevents agents performing actions, consequencemessages exchanged. also reason slightly less messages SCN6 SCN5. fact, number inter-agent causal links twoscenarios almost same, faults SCN6 stronger impact SCN5,evident looking number performed actions SCN5 SCN6 (see Figure 17).6.5 Experimental Analysis: DiagnosisCompetence. competence diagnostic inferences evaluated percentagecases action affected injected exogenous event includedwithin set preferred explanations mP ADs. Figure 21 (left-hand side) showsdiagnostic inferences behave three levels observability. Obviously, FULLobservability, diagnostic inferences always identify correct primary action failure.HIGH LOW observability, however, impaired agent stop plan execution due lack observations (i.e., not-enough-info). cases diagnosis cannotidentify primary failure. Figure 21 (right-hand side) shows also average distance(i.e., number actions), action affected exogenous event, action53fiMicalizio & TorassoFULLHIGHLOWFULLHIGHLOW880#actions% diagnosed cases10060406422000345678345#agents678#agentsFigure 21: [Diagnosis] Competence (left), responsiveness (right).xplanationsEVE: inferred explanationsFULLHIGHLOWFULL# explanantions# explanations50403020100345678LOW432103# agentsHIGH545678# agentsFigure 22: [Diagnosis] EVE explanations (left), mPADs explanations (right).failure actually detected. FULL observability, diagnosis highlyresponsive detects action failure soon exogenous event occurs (i.e.,distance zero). hand, observability partial, CWCM agenttake longer detect failure.Explanations Preferred Explanations. Section 5, pointed that,given trajectory-set, one identify two types explanations: EVE mP ADs.set EVE represents explanations consistent observations receivedagent; whereas mP ADs set primary action failures inferred EVE . Figure 22 shows cardinalities (on average) two sets inferred six scenariosdifferent levels observability. two charts Figure 22 draw two conclusions. First, cardinality EVE strongly depends observability level; namely,reduction observability level causes increment number possible explanations. However, cardinality mP ADs almost independent observabilitylevel. fact, number preferred explanations inferred LOW observability similar number preferred explanations inferred FULL observability sixscenarios. course, mP ADs sets computed LOW observability tend slightlybigger mP ADs sets computed FULL observability, consequencefact initial EVE set ambiguous LOW observability.means that, regardless initial ambiguity EVE sets, preferred explanationsreduced almost subsets six scenarios.second important conclusion mP ADs explanations substantiallyuseful identifying fault EVE explanations. fact, average cardinalitymP ADs sets three worst cases LOW observability; whereas, average54fiCooperative Monitoring Diagnose Multiagent PlansDiagnosis: Computational TimeFULLHIGHLOW1000msec8006004002000345678# agentsFigure 23: Diagnosis: Computational Time.number EVE explanations best case FULL observability eight,rises 38 worst case LOW observability (see SCN3). meansmP ADs explanations may actually help human user refine her/his hypothesescurrent situation system. essential consider diagnosisfirst step recovery (Micalizio, 2013). Thus, human user, possibly automaticsupervisor, consider small number alternative explanations, hence betterfocus plan recovery process fault(s) believed plausible.Computational Effort. Finally, consider computational time required inferdiagnoses. inferring EVE explanations, computational cost mainly duecost removing non-relevant variables trajectory-set provided CWCM (seeAppendix discussion theoretical point view cost variableremoval). Figure 23 reports average computational time, milliseconds, extractingEVE explanations six scenarios three different levels observability.noticed previous section, LOW observability, trajectory-set tendsbigger two observability levels. consequence timeinferring diagnoses LOW conditions tends higher; however, time1 second even worst case. side, HIGH observability conditions,worst time 150 milliseconds (see scenario SCN3). worst time falls50 milliseconds consider FULL observability level. computational timesallow us conclude diagnostic task, monitoring one, performedon-line number applicative domains actions performed orderseconds, even minutes.6.6 Discussionbeginning experimental analysis posed three questions,position answering them. First all, experimental results show CWCMsensitive number agents team. consequence partitioningglobal plan local plans. way, fact, agent keeps pointview states shared resources; namely, agent local belief statedepend number agents team. seen, consistencyamong local beliefs guaranteed exchange messages whose numberlinear number inter-agent causal links MAP consideration.55fiMicalizio & Torassohand, level observability system strong impactcomputational effort CWCM ambiguity trajectory-sets computed.lower observability, higher computational cost biggertrajectory-sets. important result emerges analysis worst-casescenarios depicted Appendix rare, never occurred experiments. Indeed, compact encoding trajectory-sets action models obtained viaOBDDs facilitates efficient implementation CWCM takes, average,hundreds milliseconds monitor single action. allows us conclude CWCMsuccessfully employed on-line monitoring many real-world domains.level observability also impact diagnostic inferences. fact,number EVE explanations significantly grows observability level decreases.However, number preferred mP ADs explanations strongly influencedobservability level.Finally, direct comparison CWCM WCM demonstrates cooperation among agents essential tolerant scarce observations.cooperation, fact, means agent CWCM keep longertrajectory-sets WCM. longer trajectory-sets give agent chancescollect pieces information successful completion pending actions.7. Related Worksconsider four main families model-based approaches diagnosis dynamicsystems close MAPs:Discrete-Event Systems (DESs);Relation-oriented;Team-oriented;Resource-oriented.rest section briefly review main approaches within families,highlighting differences similarities CWCM methodology proposed here.7.1 Discrete-Event SystemsSince seminal work Sampath, Sengupta, Lafortune, Sinnamohideen, Teneketzis(1995) Diagnoser, huge number works addressed diagnosis dynamicsystems modeling systems DESs. Diagnoser approach compilesdiagnostic model (i.e., Diagnoser itself) whole system off-line, approaches(see e.g., Lamperti & Zanella, 2002; Cordier & Grastien, 2007) compute possible systembehaviors, check behaviors correct. Grastien, Haslum, Thiebaux(2012) extends DESs conflict-based approach initially proposed Reiter (1987)static systems.best knowledge, DES framework gets closer onepresented Grastien, Anbulagan, Rintanen, Kelareva (2007). framework,diagnosis label either normal faulty, associated system trajectory;trajectory sequence system states interleaved events, thus similar56fiCooperative Monitoring Diagnose Multiagent Planstrajectories kept within trajectory-sets. trajectory normal containfault events; trajectory faulty, otherwise.Grastien et al. propose reduce diagnosis problem SAT one. idea formulate SAT problem, order answer question observed behavior compatiblefaults occurring?. course, answer yes = 0, systemassumed nominal exists least one normal path consistent observations.principle, proposed system description could encode MAP: execution actionscould modeled subset observable events; whereas exogenous events couldmapped unobservable events directly. However, DES framework cannot directlyapplied domains CWCM deal with. First all, DES approachnext state whole system inferred taking account synchronous occurrenceset events. Thus, agents event generators, follows performactions synchronously, CWCM restriction imposed. Moreover, SATbased methodology centralized trajectories whole system states, whereasCWCM enables agent build local trajectory-sets distributed way.7.2 Relation-Oriented ApproachesRelation-oriented approaches proposed Micalizio Torasso (2008, 2009).define works relation-oriented since action models expressed terms relationsagents state variables. advantage kind model possibilityrepresenting single piece knowledge nominal abnormal evolutionsaction. CWCM methodology falls within category, extends previousworks two ways. First all, CWCM able deal completely unexpected events,denoted ? , model exists. Indeed, occurrence ? executionaction maps variables effects(a) unknown value; meaningvariables longer predictable.second important extension protocol allows agents cooperatemonitoring task. experimental results demonstrated,cooperation among agents essential cope scarce observations. meanscooperation, fact, agent acquire new pieces information wouldacquire directly. pieces therefore used refine trajectory-set,possibly outcome pending actions could determined.7.3 Teamwork-Oriented ApproachesRather diagnosing action failures, CWCM, teamwork-oriented approachesfocused diagnosing teamwork failures; i.e., coordination failures. type failuresnecessarily due erroneous actions, wrong decisions taken agents.detection teamwork failures addressed seminal works Tambe(1998) Kaminka Tambe (2000). Kalech Kaminka (2003) later focuseddiagnosis coordination failures, introduced notion socialdiagnosis. specifically, team cooperating agents represented abstractterms means hierarchy behaviors. behavior abstraction concreteactions agent actually takes real world. Indeed, behaviors abstractsingle actions, possibly sequences actions. Thus, differently relational-57fiMicalizio & Torassoresource-oriented approaches (see later), explicit model agents plans missingteamwork-oriented solutions.social diagnosis framework assumes agents synchronize jointlyselect team behavior. disagreement arises least two agents select two behaviorsincompatible other. disagreements represent instances socialdiagnosis problems. course, agents select behaviors according beliefs,thus social diagnosis disagreement set conflicting belief states held subsetagents. Kalech Kaminka (2005, 2007, 2011) propose different methods inferringsocial diagnosis. solutions, however, rely assumptions may limitapplicability real-world scenarios. First all, assumed agents teamshare hierarchy behaviors belief states agents homogeneous(i.e., defined set propositional atoms). Moreover, agents must willingexchange beliefs. CWCM methodology propose, however,suffer limitations. CWCM, fact, makes assumptionagents internal beliefs. addition, communication among agents exchangeagents internal beliefs, observations shared resources agents directly gather,guarantees agents high degree privacy.7.4 Resource-Oriented Approachesapproaches within resource-oriented family mainly proposed RoosWitteveen. call approaches resource-oriented because, point view,system diagnosed plan, state system given statessystem resources. execution action change state(s) oneresource(s). approaches deserve particular attention similaritiesCWCM methodology, also number relevant differences.Witteveen, Roos, van der Krogt, de Weerdt (2005) present basic frameworkuse extend subsequent works. framework, actions modeledatomic plan steps; precisely, action models functions deterministically mapresource states input resource states output. models therefore representchanges normally caused actions successfully performed.faulty behavior actions, conversely, modeled via weak abnormal function,maps state resource input unknown value. means that,action fails, states resources handled action become unpredictable.diagnostic problem arises observations received execution stepinconsistent nominal predictions made action models. meansleast one actions performed far behaved abnormally. Witteveen et al.(2005) introduce notion plan diagnosis subset plan actions that, qualifiedabnormal, make observations consistent predictions made assumingactions nominal.course, since may exist many possible plan diagnoses, important lookdiagnoses preferable others. Roos Witteveen (2009) propose different preference criterion based predictive power plan diagnosis has.therefore introduce notion maximally-informative plan diagnosis (maxi-diagnosis)set plan diagnoses predict correctly biggest subset observations. no58fiCooperative Monitoring Diagnose Multiagent Planstion diagnosis subsequently refined notion minimal maximally-informativeplan diagnosis (mini-maxi-diagnosis), subset maxi-diagnosisnumber failed actions assumed minimal.work de Jonge et al. (2009), basic framework extended: agentsseen resources, action models also includes variables agents equipmentenvironment events (i.e., exogenous events). extension allows distinctionprimary secondary diagnoses. primary diagnosis plan diagnosis (i.e.,expressed terms failed actions), secondary diagnosis thought secondlevel diagnosis tries explain given action failure occurred.CWCM tries resolve problem one addressed Roos Witteveen(2009): Diagnosing execution MAP. However, action models significantly different two approaches. Roos Witteveens point view, action modelsdeterministic functions nominal behavior only. Whereas CWCM, model actionsrelations easily accommodate nominal faulty evolutions. particular,faulty evolutions nondeterministic, partially specified supportunknown value indicate expectations possible given variable.Another important difference two approaches executionactions. Roos Witteveen assume actions take one time instant performedaction execution proceeds synchronously agents. CWCMrealistic since action execution asynchronous: even though actions modeledterms preconditions effects, actual duration necessarily one time instant.seen, fact, agents cooperate exploiting causalprecedence links explicitly defined within plan model. plan model adoptedRoos Witteveen, instead, mentions explicitly precedence links only,include causal links.Also process diagnosis inferred presents substantial differences.Witteveen et al. (2005) de Jonge et al. (2009) present centralized method carrydiagnostic inferences. distributed procedure qualifying actions abnormal proposedRoos Witteveen (2009), also case detection diagnostic problemmade centralized way. Moreover, methodology proposed Roos Witteveensort strong committed approach, sense whenever observations,system infer diagnosis. hand, CWCM methodology fullydistributed detection diagnostic problem (i.e., monitoring),solution. addition, CWCM inherently weak committed: observations necessarilytrigger diagnostic process, diagnosis inferences start interpretationobservations either lead (1) determining action failure, (2) determiningservice produced favor another agents action actually missing. CWCM achievessecond point exploiting direct observations gathered agent, messagescoming agents. means observations sufficient eitherreach condition (1) (2), diagnosis inferred.said above, de Jonge et al. (2009) introduce distinction primarysecondary diagnosis. distinction also found methodology. primarydiagnosis de Jonge et al. corresponds minimum primary action failures (mPADs),identify actions assumed faulty order make plan executionconsistent observations. secondary diagnosis, hand, corresponds59fiMicalizio & Torassorefined explanations (refinedExp), associate action mPADsset exogenous events that, consistently observations, might occurredhence caused action failure.paper also assess impact primary action failure mP ADsinferring set secondary action failures; namely, subset actions failindirect consequence failure a. Although identification secondary failureswould possible, de Jonge et al. take account problem.conclusion, CWCM framework considered extension frameworks de Jonge et al. (2009) Roos Witteveen (2009). fact, action modelsproposed Roos Witteveen reproduced within framework includingrelation-based model two entries: one deterministic nominal evolutionaction, one abnormal behavior agent variables become unknown consequence unpredictable event. action models could usedCWCM usual infer plan diagnosis fully distributed way.8. ConclusionPlan diagnosis essential step implement robust multiagent planning systems.shown works (Mi & Scacchi, 1993; Gupta et al., 2012; Micalizio, 2013), fact,explanations provided plan diagnosis steer repair procedure make repairprocess effective.paper addressed problem plan diagnosis splitting twosubproblems: detection action failures, actual explanation detectedaction failures terms exogenous events might occurred. detection action failures achieved means Cooperative Weak-Committed Monitoring (CWCM)strategy, allows agents cooperate monitoring task. Cooperation among agents plays central role detection action failures,also explanations. CWCM methodology, fact, allows agent buildstructure (i.e., trajectory-set), internal representation worldpoint view agent itself. Relying structure, agent infer explanationsaction failures without need interacting agents.proposed framework diagnosis MAPs extends previous approachesliterature. First all, CWCM fully distributed asynchronous. Previous approaches(see e.g., Kalech & Kaminka, 2011; Roos & Witteveen, 2009; Micalizio & Torasso, 2008),instead, based synchronous step (e.g., agents execute actions synchronously).framework agent perform next action soon actions preconditionssatisfied. verify condition, impose agents adhere coordinationprotocol guarantees consistent access shared resources.addition, propose extension relational language modeling nondeterministic actions (Micalizio & Torasso, 2008). previous approach, fact,assume know advance exogenous events affect given action;paper able deal partial knowledge exogenous events. particular, allow specify subset effects exogenous event action(i.e., agents variables might become unknown event), also allowspecify action might affected indefinite event whose effects completely60fiCooperative Monitoring Diagnose Multiagent Plansunpredictable (i.e., agents variables become unknown due event). kindextended action model subsumes action models proposed Roos Witteveen,consists two parts: nominal action model, abnormal model mapsagents variable unknown value.Cooperation among agents nondeterministic action models make CWCM particularly apt deal dynamic partially observable environments. one side,nondeterministic action models discussed capture unexpected changesenvironment. side, cooperative monitoring allows agent acquireinformation environment agents. important note that,differently works agents exchange internal belief states(see e.g., Kalech & Kaminka, 2011), CWCM agent needs communicateobserves. enables agents keep private internal beliefs; addition, agentscould adopt specific policies deciding observations forwardedagents. Forwarding observations agents, single agentcurrent proposal, might help agents discover earlier outcomespending actions; leave opportunity future research.must also noted CWCM assumes observations correct:actual state agent must pruned agent belief state due erroneousobservation. assumption often made also many model-based approaches diagnosis (see e.g., Birnbaum et al., 1990; Brusoni, Console, Terenziani, & Theseider Dupre,1998; Pencole & Cordier, 2005; Roos & Witteveen, 2009; Eiter, Erdem, Faber, & Senko,2007, mention few). Correctness observations, however, impliesobservations must precise. CWCM fact consume ambiguous messages givendisjunction values variable (i.e., var = v1 var = v2 . . . var = vn ),negation specific value (i.e., var 6= v). point view CWCM,consuming observations simply corresponds selection states within beliefstate observations refer to. Although aspect emphasized paper,ability dealing ambiguous observations enriches communicative capacitiesagents. instance, ask-if interaction, client, rather answeringgeneric no-info, could give provider disjunction possible resource states amongwhich, however, client incapable discriminate actual one. set alternativestates is, point view provider, much informative no-info,possibly could lead provider determine actual state resource hand.point view diagnostic inference, shown possibleexplain action failures extracting explanations trajectory-sets built CWCM.particular, pointed assuming action failures independentmight lead spurious diagnoses. reason proposed methodologyidentifying primary action failures secondary action failures, indirectconsequence primary ones. simple preference criterion, based minimalityprimary action failures, proposed prefer alternative explanations.deep experimental analysis shown cooperative monitoring diagnosis practically feasible. efficient implementation based OBDDs discussedAppendix together computational analysis theoretical point view.experiments highlighted CWCM scales well number agents,affected level observability environment: trajectory-sets tend big61fiMicalizio & Torassoger environment less observable. However, experiments demonstratecooperation effective even dealing scarcely observable environments. Competence rates noncooperative solutions, fact, comparable CWCMenvironment fully observable; situations, instead, CWCM alwaysexhibits highest competence.proposed framework extended different ways. mentioned above,far adopted careful approach communication restricting agents talkexchanged services. However, agents might willingcommunicate pieces knowledge acquired. interesting possibleextension improve cooperative protocol along direction. intuition, fact,agent acquires information, could infer outcomepending actions earlier now. course, communication mustbecome bottleneck, agents able identify piece information worthforwarded agents, avoid broadcasting every observation agents.important extension aim at, however, relax assumptioncommunication among agents always reliable. Removing assumptionmany consequences. First all, cooperative monitoring protocol extendedorder deal messages lost. Moreover, Proposition 7, safe useresources, might longer guaranteed CWCM; thus resources could accessedinconsistently. diagnose situations could take point view similar KalechKaminkas social diagnosis. fact, erroneous access resources, could consideredcoordination failures. would impact diagnostic inferences longerlocal, distributed. is, monitoring task, also diagnosisperformed means cooperation number agents.Acknowledgmentsauthors wish thank anonymous reviewers insightful comments,substantially contributed final shape work.Appendix A. Implementation Computational AnalysisAppendix first recall basic OBDD operators complexities,study computational cost expensive relational operations involvedCWCM diagnostic methodologies discussed above.A.1 OBDD Operators Complexitiescomputational analysis discuss next subsection relies results presentedBryant (1986, 1992). works, author discusses efficient implementationOBDDs operators corresponding computational complexities. resultssummarized Table 6, f , f1 , f2 denote Boolean functions, encodedreduced function graphs G, G1 , G2 , respectively. size graph G correspondsnumber vertices, represented |G|. primitive OBDD operatorsreported upper side Table 6:62fiCooperative Monitoring Diagnose Multiagent Plans- reduce builds canonical form Boolean function f ; i.e., given specific variablesordering, reduce operator gets graph G whose size minimal.- apply implements binary logical operations two Boolean functions f1 f2 ;operator works graphs G1 G2 encoding two functions, respectively;op binary logical operator (, , , ). computational complexityworst case product sizes (i.e., number vertices) twographs.- restrict substitutes constant b variable xi time almost linear numbervertices within graph G.- rename renames set variables ~x new one ~x0 ; complexity exponentialnumber renamed variables.- equiv checks equivalence two Boolean functions f1 f2 ; since operatorscans two corresponding graphs simultaneously, computational complexitylinear sizes.lower side Table 6 report computational cost time space relational operators join, intersect, union project obtained combiningprimitive OBDD operators. Observe that, among relational operators, projectionexpensive; fact, exponential number (binary) variablesremoved (see e.g., Torta & Torasso, 2007; Torasso & Torta, 2006 details).A.2 CWCM: Computational Analysisanalyze computational complexity CWCM, consider high-level algorithmpresented Figure 10, focus computational cost performing single iterationloop action ail actually performed real-world.situation three main steps hide potentially expensive operations relations:extension current trajectory-set (line 9);refinement trajectory-set available observations (line 11);detection outcomes pending actions (line 19, Figure 7).rest section analyze computational effort steps.operatorreduce(f )apply(op, f1 , f2 )restrict(xi , b, f )rename (f , ~x, ~x0 )equiv(f1 , f2 )timeO(|G| log |G|)O(|G1 | |G2 |)O(|G|)O(|G| 2|~x| )O(max(|G1 |, |G2 |))size|G||G1 | |G2 ||G||G| 2|~x|N/Ajoin (f1 , f2 ); union(f1 , f2 );intersect(f1 , f2 ) (i.e., select)project(f , {x1 , . . . , xn }, {y1 , . . . , ym })O(|G1 | |G2 |)O(|G1 | |G2 |)O((2(nm) |G|)2 )|G1 | |G2 ||G1 | |G2 |2(nm) |G|Table 6: OBDD operators complexity.63fiMicalizio & TorassoA.2.1 Extending Trajectory-SetAccording equation (7), operator , r [1, l] yields new trajectory-setr [i, l + 1], involves two join operations: one r [1, l] (ail ), oner [1, l] (ail ). results two operations subsequently merged newtrajectory-set r [1, l + 1] via union operation. understand computational costrelational operations, necessary map OBDD operators. alreadyshown previous works (see e.g., Torta & Torasso, 2007; Micalizio, 2013), naturaljoin mapped two Boolean functions (and hence twoOBDDs), whereas union two relations becomes Boolean or. Let Gl , Gl+1 , G ,G OBDDs corresponding relations r [1, l], r [1, l + 1], (ail ),(ail ), respectively; operator mapped following expression termsOBDDs operations:Gl+1 = apply(, apply(, Gl , G ), apply(, Gl , G ))(14)Given operator complexities Table 6, computational effort infer newtrajectory-set worst case:O(|Gl+1 |) = O(|Gl | |G |) O(|Gl | |G |)= O(|Gl |2 |G | |G |).(15)A.2.2 Refinement Observationsnew trajectory-set inferred, refined observations obsikreceived agent. sake exposition, equations (8) (9) definedrefinement trajectory-set intersection trajectory-setbelief state Bki refined obsik . Note extraction belief stateexpensive operation, thus try avoid operation whenever possible. particularcase, since agent variables V ARki included within current trajectory-set,refinement operation carried intersection r [1, l + 1] obsik ;terms OBDD operators:Gref .l+1 = apply(, Gl+1 , Gobsi )k(16)Gobsi OBDD encoding obsik , Gref .l+1 OBDD correspondingkrefined trajectory-set refinedT r [1, l + 1]. follows computational costoperation(17)O(|Gref .l+1 |) = O(|Gl+1 | |Gobsi |)kA.2.3 Detecting Pending Actions Outcomeslast step CWCM algorithm consider assessment outcome everyaction currently within list pending actions pActsi . Section 4, notedverify success given action aik pActsi , sufficient check whether nominaleffects aik satisfied every state Bk+1(Definition 3). case conditionhold, one verify whether expected nominal effects aik missing64fiCooperative Monitoring Diagnose Multiagent Plansstate Bk+1(Definition 4). checks result negative answer (i.e., belief stateBk+1 still ambiguous), action aik remains pending.explicitly checking conditions might parExtracting belief state Bk+1ticularly expensive, especially trajectory-set grows time. extractionbelief state project operation, fact, would require eliminationr [1, l + 1] variables interested in; thus, would remove variablessteps 1, 2, . . . , k, k + 2, . . . , l + 1, l |VARi | variables. Table 6 shows,complexity project exponential number variables removed, couldeasily become bottleneck.cope problem, implemented checking ok f ailed outcomesdifferent way. particular, Definition 3 directly follows:.join effects(aik ) equals Bk+1Proposition 11 aik outcome ok iff Bk+1Proof: proof straightforward: definition aik outcome ok iff nominal effectssatisfied every state Bk+1; join Bk+1effects(aik ) yields Bk+1nominal effects already included every state Bk+1 , hence actionoutcome ok.Since Bk+1 included refinedT r [1, +l + 1], proposition extendedwhole trajectory-set:Proposition 12 aik outcome ok iffrefined r [1, l + 1] join effects(aik )equalsrefined r [1, l + 1].say, refinement trajectory-set observations, action aikoutcome ok iff nominal effects filter trajectory refinedT r [1, l + 1].Relying proposition, verify whether aik outcome ok two steps: first,build temporary OBDD maintaining result join r [1, l + 1]effects(aik ), check whether temporary OBDD equivalent originaltrajectory-set; terms OBDD operators:outcomeOK? = equiv(Gref.l+1 , apply(, Gref.l+1 , Geffects(ai ) ))k(18)Since size Geffects(ai ) negligible compared size Gref.l+1 , computaktional complexity check O(|Gref.l+1 |2 ).f ailed outcome checked similar way; case want discoverwhether nominal effects aik missing r [1, l + 1]; happensnegation effects effects(aik ) represent possible filter r [1, l + 1]r [1, l + 1] join effects(aik ) equals r [1, l + 1]. terms OBDD operatorsoutcomeFailed? = equiv(Gref.l+1 , apply(, Gref.l+1 , Geffects(ai ) ))(19)kimportant note OBDD Geffects(ai ) computed constant time directlykGeffects(ai ) ; fact, given Boolean function f corresponding graph Gf ,ksufficient exchange 0 1 nodes Gf obtain graphrepresentation Boolean function not(f ). Thus, also check O(|Gref.l+1 |2 ).65fiMicalizio & Torassofollows cost determining outcomes actions pActsi is:O(|pActsi | |G(ref.)l+1 |2 ).(20)equation (20), easy see computational cost CWCM methodology strongly depends amount available observations. worst case factgive step l, due scarce observations, number pending actionsclose l itself; is, |pActsi | l, meaning almost actions performed faroutcome pending.A.3 Diagnosis: Computational Analysiscomputational cost diagnostic process strongly dominated costinferring event-based explanations (EVE ). shown, fact, possibleextract set minimum cardinality primary action failures explanations (mP ADs)structure. According Equation 11, EVE set extracted projectioncurrent trajectory-set r [1, l] event variables e1 , . . . , el1 ; unfortunately,case way avoid expensive operation.estimate computational cost, first consider many binary variableswithin OBDD Gl encoding r [1, l], many (binary) variables goingremove OBDD. state event variable r [i, l] fact multi-valuedvariable actually implemented terms number binary variables withinOBDD Gl . number required binary variables depends size domainoriginal high-level variable. Let us assume size largest domainvariables VARi , estimate need b = log binary variablesvariable mentioned r [1, l] (both state event variables). easy seenumber binary variables required represent single belief state w = b |VARi |:multi-valued variable VARi b binary variables OBDD level.number binary variables encoding trajectory-set r [i, l] therefore p =l w + (l 1) b; fact, within r [1, l], l beliefs l-1 event variables. costprojecting r [1, l] event variables therefore:O((2plw |Gl |)2 ).(21)EVE diagnoses extracted, possible infer minimum cardinalityprimary failures exploiting techniques Torasso Torta (2003), provenpolynomial size OBDD.A.4 Discussionfirst important result emerges computational analysismonitoring single execution step CWCM exponential. fact,shown step declarative definition mapped number OBDDoperators whose complexity polynomial, provided sizes involved OBDDsremain manageable. particular, shown exponential operation useddeclarative definition, projection, avoided actual implementation.main concern CWCM trajectory-set may grow time agent66fiCooperative Monitoring Diagnose Multiagent Plansperforms actions without receiving observations. Consequently, also computational costCWCM tends grow time since size OBDD encoding trajectoryset may increase. important note, however, growth exponentialquadratic (see equation (15)). addition, estimate computational costsmonitoring diagnosis, exploited estimations reported Table 6; these,however, estimates worst possible cases, practice casescommon. Bryant conjectures that, although theoretical cost apply operatortwo OBDDs G1 G2 O(|G1 | |G2 |) worst case, practice actualcost cases closer O(|G1 | + |G2 | + |G3 |) G3 resulting OBDD(Bryant, 1986). Thus also size resulting, intermediate OBDDs plays centralrole determining actual computational cost.specific case CWCM, observe commonagent perform long portion plan without receiving observations. CWCMallows fact agents communicate other; therefore, unless agentcompletely isolated others, agent likely receive observations comingagents services provided with. means that, practice,size OBDD encoding trajectory-set become intractablecooperation among agents, experiments conducted far supporthypothesis.hand, diagnostic inferences slightly expensive monitoring strategy. project operation cannot avoided order inferdiagnosis. case, however, observe plan execution alreadystopped consequence detected failure. Thus, diagnosis taketime infer result since constrained on-line.ReferencesArasu, A., Babu, S., & Widom, J. (2006). CQL continuous query language: semanticfoundations query execution. International Journal Large DataBases, 15 (2), 121142.Birnbaum, L., Collins, G., Freed, M., & Krulwich, B. (1990). Model-based diagnosisplanning failures. Proc. Association Advancement Artificial Intelligence(AAAI90), pp. 318323.Boutilier, C., & Brafman, R. I. (2001). Partial-order planning concurrent interactingactions. Journal Artificial Intelligence Research, 14, 105136.Brusoni, V., Console, L., Terenziani, P., & Theseider Dupre, D. (1998). spectrumdefinitions temporal model based diagnosis. Artificial Intelligence, 102, 3979.Bryant, R. (1986). Graph-based algorithms boolean function manipulation. IEEE Transactions Computers, 35 (8), 677691.Bryant, R. (1992). Symbolic boolean manipulation ordered binary-decision diagrams.ACM Computer Surveys, 24, 293318.67fiMicalizio & TorassoCordier, M.-O., & Grastien, A. (2007). Exploiting independence decentralisedincremental approach diagnosis. Proc. International Joint ConferenceArtifical Intelligence (IJCAI07), pp. 292297.Cox, J. S., Durfee, E. H., & Bartold, T. (2005). distributed framework solvingmultiagent plan coordination problem. Proc. International Conference Autonomous Agents Multiagent Systems (AAMAS05), pp. 821827.Darwiche, A., & Marquis, P. (2002). knowledge compilation map. Journal ArtificialIntelligence Research, 17, 229264.de Jonge, F., Roos, N., & Witteveen, C. (2009). Primary secondary diagnosis multiagent plan execution. Journal Autonomous Agent Multiagent Systems, 18 (2),267294.Eiter, T., Erdem, E., Faber, W., & Senko, J. (2007). logic-based approach findingexplanations discrepancies optimistic plan execution. Fundamenta Informaticae,79 (1-2), 2569.Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporalplanning domains. Journal Artificial Intelligence Research, 20, 61124.Grastien, A., Anbulagan, Rintanen, J., & Kelareva, E. (2007). Diagnosis discrete-eventsystems using satisfiability algorithms. Proc. Association AdvancementArtificial Intelligence (AAAI07), pp. 305310.Grastien, A., Haslum, P., & Thiebaux, S. (2012). Conflict-based diagnosis discrete eventsystems: Theory practice. Proceedings Thirteenth International Conference Principles Knowledge Representation Reasoning (KR12), pp. 489499.Gupta, S., Roos, N., Witteveen, C., Price, B., & de Kleer, J. (2012). Exploiting sharedresource dependencies spectrum based plan diagnosis. Proc. AssociationAdvancement Artificial Intelligence (AAAI12), pp. 2425 2426.Heger, F. W., Hiatt, L. M., Sellner, B., Simmons, R., & Singh, S. (2005). Results SlidingAutonomy Multi-Robot Spatial Assembly. Proc. International SymposiumArtificial Intelligence, Robotics Automation Space (iSAIRAS).Helmert, M. (2009). Concise finite-domain representations PDDL planning tasks. Artificial Intelligence, 173 (5-6), 503535.Jonsson, P., & Backstrom, C. (1998). State-variable planning structural restrictions:Algorithms complexity. Artificial Intelligence, 100 (1-2), 125176.Kalech, M. (2012). Diagnosis coordination failures: matrix-based approach. JournalAutonomous Agents Multiagent Systems, 24 (1), 69103.Kalech, M., & Kaminka, G. A. (2003). design social diagnosis algorithmsmulti-agent teams. Proc. International Joint Conference Artificial Intelligence(IJCAI03), pp. 370375.Kalech, M., & Kaminka, G. A. (2005). Diagnosing team agents: Scaling up. Proc.International Conference Autonomous Agents Multi-Agent Systems (AAMAS05), pp. 249255.68fiCooperative Monitoring Diagnose Multiagent PlansKalech, M., & Kaminka, G. A. (2007). design coordination diagnosis algorithmsteams situated agents. Artificial Intelligence, 171 (8-9), 491513.Kalech, M., & Kaminka, G. A. (2011). Coordination diagnostic algorithms teamssituated agents: Scaling up. Computational Intelligence, 27 (3), 393421.Kalech, M., Kaminka, G. A., Meisels, A., & Elmaliach, Y. (2006). Diagnosis multi-robotcoordination failures using distributed CSP algorithms. Proc. AssociationAdvancement Artificial Intelligence (AAAI06), pp. 970975.Kaminka, G. A., & Tambe, M. (2000). Robust multi-agent teams via socially-attentivemonitoring. Journal Artificial Intelligence Research, 12, 105147.Lamperti, G., & Zanella, M. (2002). Diagnosis discrete-event systems uncertaintemporal observations. Artificial Intelligence, 137 (1-2), 91163.Mi, P., & Scacchi, W. (1993). Articulation: integrated approach diagnosis, replanning, rescheduling software process failures. Proc. Knowledge-BasedSoftware Engineering Conference, pp. 7784.Micalizio, R. (2013). Action failure recovery via model-based diagnosis conformantplanning. Computational Intelligence, 29 (2), 233280.Micalizio, R., & Torasso, P. (2007a). On-line monitoring plan execution: distributedapproach. Knowledge-Based Systems, 20 (2), 134142.Micalizio, R., & Torasso, P. (2007b). Plan diagnosis agent diagnosis multi-agentsystems. Proc. Congress Italian Association Artificial Intelligence(AI*IA07), Vol. 4733 LNCS, pp. 434446.Micalizio, R., & Torasso, P. (2008). Monitoring execution multi-agent plan: Dealingpartial observability. Proc. European Conference Artificial Intelligence(ECAI08), pp. 408412.Micalizio, R., & Torasso, P. (2009). Agent cooperation monitoring diagnosingMAP. Proc. Multiagent System Technologies (MATES09), Vol. 5774 LNCS,pp. 6678.Micalizio, R., Torasso, P., & Torta, G. (2006). On-line monitoring diagnosis teamservice robots: model-based approach. AI Communications, 19 (4), 313349.Nebel, B. (2000). compilability expressive power propositional planningformalisms. Journal Artificial Intelligence Research, 12, 271315.Pencole, Y., & Cordier, M. (2005). formal framework decentralized diagnosislarge scale discrete event systems application telecommunication networks.Artificial Intelligence, 164, 121170.Reiter, R. (1987). theory diagnosis first principles. Artificial Intelligence, 32 (1),5796.Roos, N., & Witteveen, C. (2009). Models methods plan diagnosis. JournalAutonomous Agent Multiagent Systems, 19 (1), 3052.Sampath, M., Sengupta, R., Lafortune, S., Sinnamohideen, K., & Teneketzis, D. (1995).Diagnosability discrete event systems.. IEEE Transactions Automatic Control,40 (9), 15551575.69fiMicalizio & TorassoSellner, B., Heger, F., Hiatt, L., Simmons, R., & Singh, S. (2006). Coordinated multiagent teams sliding autonomy large-scale assembly. IEEE - Special IssueMulti-Robot Systems, 94 (7), 1425 1444.Steinbauer, G., & Wotawa, F. (2008). Enhancing plan execution dynamic domains usingmodel-based reasoning. Intelligent Robotics Applications, First InternationalConference, (ICIRA08), Vol. 5314 LNAI, pp. 510519.Tambe, M. (1998). Implementing agent teams dynamic multi-agent environments. AppliedArtificial Intelligence, 12 (2-3), 189210.Torasso, P., & Torta, G. (2003). Computing minimum-cardinality diagnoses using OBDDs.German Conference AI (KI03), Vol. 2821 LNCS, pp. 224238.Torasso, P., & Torta, G. (2006). Model-based diagnosis OBDD compilation: complexity analysis. Reasoning, Action Interaction AI Theories Systems,Vol. 4155 LNCS, pp. 280298.Torta, G., & Torasso, P. (2007). role modeling causal independence systemmodel compilation OBDDs. AI Communications, 20 (1), 1726.Weld, D. S. (1994). introduction least commitment planning. AI Magazine, 15 (4),2761.Witteveen, C., Roos, N., van der Krogt, R., & de Weerdt, M. (2005). Diagnosis singlemulti-agent plans. Proc. International Conference Autonomous AgentsMultiagent Systems (AAMAS05), pp. 805812.Yan, Y., Dague, P., Pencole, Y., & Cordier, M.-O. (2009). model-based approachdiagnosing fault web service processes. Journal Web Service Research., 6 (1),87110.70fiJournal Artificial Intelligence Research 51 (2014) 707-723Submitted 07/14; published 12/14Minimum Representations Matched FormulasOndrej CepekStefan GurskyPetr Kuceraondrej.cepek@mff.cuni.czstevko@mail.rukucerap@ktiml.mff.cuni.czCharles University PragueFaculty Mathematics PhysicsDepartment Theoretical Computer ScienceMalostranske nam. 25, 118 00 Praha 1, Czech RepublicAbstractBoolean formula conjunctive normal form (CNF) called matched systemsets variables appear individual clauses system distinct representatives.matched CNF trivially satisfiable (each clause satisfied representativevariable). Another property easy see, class matched CNFsclosed partial assignment truth values variables. latter property leadsfact (proved here) given two matched CNFs co-NP complete decide whetherlogically equivalent. construction proof leads another result: muchshorter simpler proof p2 -completeness Boolean minimization matched CNFs.main result paper deals structure clause minimum CNFs. proveBoolean function f admits representation matched CNF everyclause minimum CNF representation f matched.1. Introductionpaper study class matched formulas introduced Franco Van Gelder(2003). Given formula conjunctive normal form (CNF) consider incidencegraph I() defined follows. I() bipartite graph one part consisting clausespart containing variables . edge {C, x} clause Cvariable x I() x appears C. observed Aharoni Linial (1986),Tovey (1984) I() admits matching (i.e. set pairwise disjoint edges)size (where number clauses ), satisfiable. Later formulassatisfying condition called matched formulas Franco Van Gelder (2003).Since matching maximum size given graph found polynomial time (e.g.,Lovasz & Plummer, 1986), check whether given formula matched. Givenarbitrary CNF measure far matched consideringmaximum deficiency (), number clauses remain unmatched maximummatching I(). formula thus matched iff () = 0. weaker notion deficiency() = n (where number clauses n number variables ) alsooften considered.Since introduction, matched formulas considered base class parameterized algorithms satisfiability (e.g., overview parameterized algorithms theorysee Flum & Grohe, 2006). particular, Fleischner, Kullmann, Szeider (2002) showsatisfiability formulas whose maximum deficiency bounded constantc2014AI Access Foundation. rights reserved.fiCepek, Gursky, & Kuceradecided polynomial time. result later improved Szeider (2003)algorithm satisfiability parameterized maximum deficiency formula. Parameterization based backdoor sets respect matched formulas consideredSzeider (2007).Several generalizations matched formulas considered literature, too. Kullmann (2000) generalized class matched formulas class linearly satisfiableformulas Kullmann (2003) studied autarkies based matchings. Another generalization considered Szeider (2005) classes bi-clique satisfiable var-satisfiableformulas. Unfortunately, bi-clique var-satisfiable formulas hard checkformula falls one classes (Szeider, 2005).results listed previous paragraphs show matched formulas play significant role theory satisfiability solving is, without doubt, onestudied problems theoretical computer science many practical applications.Despite fact, little known structure matched CNFs. say CNFrepresenting function f irredundant set-minimal representation f , i.e.clause C 0 = \ {C} represent f . say primeCNF representing f clauses prime implicates f , clause Cimplicate f satisfied assignments satisfying f prime implicateset-minimal implicate f (considering clause set literals). sayCNF prime without mentioning function f question implicitly consider ffunction represented . hard come examples matched CNFslogically equivalent prime irredundant CNFs matched.quite interesting phenomenon occur classes polynomialtime satisfiability testing quadratic CNFs (also 2-CNFs, i.e. CNFs consistingclauses two literals), Horn CNFs (a CNF Horn every clause containsone positive literal), various generalizations, CNFclass, logically equivalent prime CNFs guaranteed class well.brings interesting question: given (nonprime) matched CNF existleast one equivalent prime CNF also matched? paper give affirmativeanswer question. answer may seem quite intuitive usuallyenough consider prime implicates function studying CNF representations,however, proof answer easy.Another problem study paper Boolean minimization matched CNFs.Boolean minimization problem (BM) stated follows: given CNF findlogically equivalent CNF minimum possible number clauses. numberclauses viewed number rules implicates representation knowledgebase standard measure used context. One also consider lengthformula, i.e. total number literal occurrences formula, measureoptimality, paper use number clauses measure leaveopen question whether results paper could extended caseformula length well. optimization version Boolean minimization problemturned decision version adding number k asking whether existslogically equivalent CNF k clauses. Umans (2001) showed decisionversion BM p2 -complete (e.g., related results see review paper Umans, Villa,& Sangiovanni-Vincentelli, 2006). Buchfuhrer Umans (2011) later showed BM708fiOn Minimum Representations Matched Formulasp2 -complete considering general formulas constant depth input outputBoolean minimization problem.also long known BM NP-hard already classes CNFsSAT solvable polynomial time. Maybe best known example class HornCNFs NP-hardness respect output measures proved (Ausiello,DAtri, & Sacca, 1986; Boros & Cepek, 1994; Cepek, 1995; Hammer & Kogan, 1993; Maier,1980). exists hierarchy tractable subclasses Horn CNFspolynomial time minimization algorithms, namely acyclic quasi-acyclic Horn CNFs(Hammer & Kogan, 1995), CQ Horn CNFs (Boros, Cepek, Kogan, & Kucera, 2009).also heuristic minimization algorithms Horn CNFs (Boros, Cepek, &Kogan, 1998).complexity BM matched CNFs fit picture. Despite factSAT trivial matched CNFs, BM class p2 -complete, i.e. hardgeneral case. fact proved Gursky (2011), proof modifies proofgeneral case Umans (2001). paper give much simpler prooffact based observation, equivalence testing co-NP-completematched CNFs. also study structure clause minimum CNFs. Basedmentioned result concerning prime CNFs prove Boolean function f admitsrepresentation matched CNF every clause minimum CNF representation fmatched. main result current paper.paper structured follows. start introducing necessary notation,definitions basic results Section 2. Section 3 prove testing logical equivalence matched CNFs co-NP-complete use idea proof showBM matched formulas p2 -complete. known fact, current proofmuch shorter simpler. Section 4 studies prime representations functions definedmatched CNFs (possibly nonprime). prove every function leastone prime representation matched. Finally, Section 5 study structureclause minimum representations functions defined matched CNFs. Using resultSection 4 prove Boolean function f admits representation matchedCNF every clause minimum CNF representation f matched. Section 6 concludespaper closing remarks.2. Definitions Resultsshall start basic definitions notions need paper. shall alsorecall results shall use paper.2.1 Boolean FunctionsBoolean function n variables mapping f : {0, 1}n {0, 1}. literal either variable (x, called positive literal ) negation (x x, called negative literal ). clausedisjunction literals. assume clause contains positive negative literals variable. Formula conjuntive normal form (CNF) conjuctionclauses (we also say CNF formula). shall often treat clause setliterals CNF formula set clauses. Thus || denote numberclauses . well known fact every Boolean function represented709fiCepek, Gursky, & KuceraCNF formula (e.g., Genesereth & Nilsson, 1987). two CNF formulas 1 2 definefunction, say equivalent denote fact 1 2 .CNF called clause minimum every CNF || ||.Clause C called implicate f every assignment ~x {0, 1}n satisfying f (i.e.f (~x) = 1) also satisfies C (i.e. C(~x) = 1). say clause C1 subsumes clause C2 ,every literal C1 occurs also C2 (i.e. C1 C2 ). C prime implicate functionf implicate f implicate C 0 f subsuming C (i.e. Cset-minimal implicate f ). say CNF formula representing function fprime representation f contains prime implicates f (if refer primeCNF without specifying function f consider function represented). CNF formula irredundant sub-CNF 0 representsfunction .assignment assigns values subset (possibly all) variablesfunction f n called partial assignment. Formally, partial assignmentviewed mapping : 7 {0, 1} subset variables f . Given CNF ,(t) denotes CNF applying partial assignment t. particular (t) producedfollowing way: Clauses contain literal satisfied(assigned value 1) removed , occurences literals variablessatisfied removed clauses .2.2 Resolutionsay two clauses conflict variable x positive occurrencef1 x)x one clause negative occurrence other. Two clauses C1 = (CfffC2 = (C2 x) resolvable x C1 C2 conflict variable.f1 Cf2 disjunction called resolvent parent clauseswrite R(C1 , C2 ) = CC1 C2 .Let CNF formula representing Boolean function f , say C derivedseries resolutions sequence clauses C1 , . . . , Ck = Cevery Ci , 1 k, either belongs , Ci = R(Cj1 , Cj2 ), j1 , j2 < i. seriesresolutions also called resolution proof C . resolution proof emptyclause (denoted ) unsatisfiable formula called refutation. lengthresolution proof number clauses sequence.well known fact Boolean function resolvent two implicatesimplicate (e.g., Buning & Lettmann, 1999). Another well known factevery prime implicate f derived CNF representation f seriesresolutions (e.g., Buning & Lettmann, 1999).shall also use notions regular tree-like resolution proofs. resolutionproof tree-like resolution proof every occurrence clause proof usedpremise resolution clause used premiseresolution conclusion; tree-like resolution proof represented tree,leaves labelled input clauses, root tree conclusionproof. depth tree-like resolution proof length longest pathleaf root . resolution proof regular path proofinput clause conclusion, variable resolved once.710fiOn Minimum Representations Matched Formulasobserved unsatisfiable CNF, regular tree-likerefutation (Urquhart, 2011), basically turn resolution derivation tree-likeresolution derivation repeating clauses necessary. tree-like refutationturned regular tree-like refutation (Tseitin, 1983; Urquhart, 1995, 2011).observed C implicate derived series resolutions, clause C 0 C derived regular tree-like resolutionsatisfying variable C 0 resolved . Indeed, C implicate ,let partial assignment assigns false literals C. follows (t)unsatisfiable formula, thus regular tree-like refutation 0 . put backfalsified literals C clauses satisfied t, get resolution derivationsub-CNF C 0 . following proposition follows immediately.Lemma 2.1 Let CNF let C prime implicate , C derivedregular tree-like resolution .2.3 Exclusive Sets Implicates Boolean Functionsection recall definition exclusive sets implicates Boolean functionstate properties, shown Boros, Cepek, Kogan,Kucera (2010).Let us first introduce following notation. p (f ) shall denote setprime implicates function f . I(f ) shall denote resolution closure p (f ),i.e. implicate C f belongs I(f ) derived series resolutionsp (f ).Definition 2.2 (Boros et al., 2010) Let f Boolean function let X I(f )set clauses. shall say, X exclusive set clauses f every pairresolvable clauses C1 , C2 I(f ) following implication holds:R(C1 , C2 ) X = C1 X C2 X ,i.e. resolvent belongs X parent clauses X . function fclear context, shall simply say X exclusive set.shall recall properties exclusive sets, proved Boroset al. (2010) use paper.Lemma 2.3 (Boros et al., 2010) Let A, B I(f ) exclusive sets implicates f ,B B also exclusive sets implicates f .Theorem 2.4 (Boros et al., 2010) Let f arbitrary Boolean function, let C1 , C2 I(f )two distinct sets clauses represent f , let X I(f ) exclusive setclauses. C1 X C2 X , i.e. represent function.Based proposition define exclusive component Boolean function.Definition 2.5 (Boros et al., 2010) Let f arbitrary Boolean function, X I(f )exclusive set clauses f , C I(f ) set clauses represents f .Boolean function fX represented set C X called X -componentfunction f . shall simply call function g exclusive component f , g = fXexclusive subset X I(f ).711fiCepek, Gursky, & KuceraTheorem 2.4 guarantees X -component fX well defined every exclusive setX I(f ). Theorem 2.4 following corollary.Corollary 2.6 (Boros et al., 2010) Let C1 , C2 I(f ) two distinct sets clausesC1 C2 f , i.e. sets represent f , let X I(f ) exclusive setclauses. (C1 \ X ) (C2 X ) also represents f .2.4 AutarkiesAutark assignments introduced Monien Speckenmeyer (1985)defined follows.Definition 2.7 Let CNF set V variables, let V subsetvariables, let L = {x | x } {x | x } corresponding set literals, let: 7 {0, 1} partial assignment . autarky every clauseC either C L = C satisfied t.autarky special type partial assignment satisfies every clausesubstitutes value literal. shall prove two simple lemmas autarkiesneeded later paper. first lemma (in different notation)shown Kullmann (2000) Lemma 3.13, give short proof wellmake paper self-contained.Lemma 2.8 Let CNF set V variables represents function f . LetV subset variables, let :7 {0, 1} autarky .autarky I(f ).Proof : Since clauses I(f ) derived resolution, sufficeshow resolution preserves autarky properties, namely parent clausessatisfied whenever contain literal L = {x | x } {x | x },resolvent. Let C, C1 , C2 I(f ) clauses C = R(C1 , C2 ). Let` L literal C. ` satisfied done, let us asssume `satisfied t. Clause C inherited ` one parent clauses let us assume withoutloss generality ` C1 . autarky property, C1 must satisfied t,must contain another literal `0 L satisfied t. two possibilities: either`0 C (clause C inherited ` `0 C1 ) case C satisfieddone, `0 6 C means R(C1 , C2 ) resolves `0 . implies `0 C2 , i.e.C2 contains literal L satisfied t. Thus, autarky property, C2 mustsatisfied t, must contain another literal `00 L satisfied t. However, caseC inherits `00 C2 , finishes proof.Corollary 2.9 Let CNF set V variables represents function f .Let V subset variables, let : 7 {0, 1} autarky . Let I(f )arbitrary representation f . autarky .Let us mention Corollary 2.9 would hold without assumption I(f ).Consider e.g. CNF = (x y) z CNF = (x y) (z y) (z y).obvious , i.e. CNFs represent function f , 6 I(f ).712fiOn Minimum Representations Matched Formulasassignment sets 1, autarky . hand autarky(z y) satisfied t.Lemma 2.10 Let CNF set V variables, let V subset variables,let L = {x | x } {x | x } corresponding set literals, let : 7 {0, 1}autarky . (t) represents exclusive component fX f definedexclusive set clausesX = {C I(f ) | C L = }.Proof : suffices show X exclusive subset I(f ) Let C, C1 , C2 I(f )clauses C = R(C1 , C2 ) C X . Let us assume contradiction oneparent clauses, say C1 , belong X , means exists ` C1 L.Since ` 6 C, R(C1 , C2 ) must resolve `, implies ` C2 . However, one `, `satisfied t, corresponding clause (one C1 , C2 ) must contain literal`0 L satisfied autarky. get `0 C contradictingassumption C X .Since autarky follows (t) = X since I(f ) Xexclusive set implicates f , follows Theorem 2.4 (t) = X I(f ) Xdefines exclusive compontent fX .2.5 Matched Formulassubsection shall define key concept paper. concept basedgraph properties, end shall use standard graph terminology (e.g., see Bollobas,1998). Given undirected graph G = (V, E), subset edges E matchingG edges pairwise disjoint. bipartite graph G = (A, B, E) undirectedgraph disjoint sets vertices B, set edges E satisfying E B.set W vertices G, let (W ) denote neighbourhood W G, i.e. setvertices adjacent element W . shall use following well-known resultmatchings bipartite graphs:Theorem 2.11 (Halls Theorem Hall, 1935; Lovasz & Plummer, 1986) Let G = (A, B, E)bipartite graph. matching size |M | = |A| exists every subset|S| |(S)|.ready define matched formulas.Definition 2.12 Let = C1 . . . Cm CNF n variables X = {x1 , . . . , xn }.shall associate bipartite graph I() = (, X, E) (also called incidence graph), vertices correspond clauses CNF variables X. clause Ciconnected variable xj (i.e. {Ci , xj } E) Ci contains xj xj . CNF matchedI() matching size m, i.e. matching pairs clauseunique variable.Note matching maximum size given graph found polynomialtime (e.g., Lovasz & Plummer, 1986) thus test polynomial time whether givenCNF matched. fact clause Ci matched variable xj matchingdenoted {Ci , xj } . variable matched clause matching713fiCepek, Gursky, & Kuceracalled matched , free otherwise. Note, matched CNF triviallysatisfiable. clause Ci matched variable xj , simply assign xj valuesatisfy Ci . name matched given formulas FrancoVan Gelder (2003), although already considered Aharoni Linial (1986),Tovey (1984).3. Equivalence Testing Hardness Clause Minimization MatchedFormulasFollowing definition given Cepek, Kucera, Savicky (2012) class CNFs Xcalled tractable satisfies following four properties.Recognition: Given arbitrary CNF possible decide polynomial timerespect size whether X .Satisfiability: Given arbitrary CNF X possible decide polynomialtime respect size whether satisfiable.Partial assignment: Given arbitrary CNF X , produced fixingvariables 0 1 substituting values , X .Prime representations: Given arbitrary CNF X , represents function fprime CNF representations f belong X .shown Cepek et al. (2012) given two CNFs tractable class,tested polynomial time, whether two CNFs logically equivalent not.class matched CNFs clearly satisfies first two tractability conditions, failssatisfy remaining two. Costructing counterexample third property easy.CNF(x z) (x z) (x z)clearly matched, partial assignment x 0 creates CNF(y z) (y z) (y z)matched. defer counterexample fourth property nextsection. light findings interesting question complexityequivalence testing matched CNFs. Despite fact satisfiability trivialmatched CNFs, equivalence testing co-NP-complete.Matched equivalenceInstance :Two matched CNFsQuestion : ?Theorem 3.1 problem Matched equivalence co-NP-complete.714fiOn Minimum Representations Matched FormulasProof : nondeterministic polynomial procedure checking two CNFsequivalent simply guesses assignment checks whether (t) 6= (t). problemMatched equivalence thus co-NP.show co-NP-hardness reduce problem checking given CNFunsatisfiable (this problem prominent example co-NP-complete problemcomplement satisfiability problem, e.g., see Garey & Johnson, 1979). Letarbitrary CNF n variables clauses, particular let = C1 C2 . . . Cm . Letus define clause = (a1 a2 . . . ) new variables occuring . letus define two CNFs:= (C1 D) (C2 D) . . . (Cm D)=obviously matched, since clause Ci0 = (Ci D) matchedvariable ai . iff , i.e. iff unsatisfiable. followsdirectly fact . reduced co-NP-complete problemunsatisfiability problem Matched Equivalence thus problem MatchedEquivalence co-NP-complete well.fact, equivalence testing co-NP-hard, probably principal reason behindfact proved Gursky (2011) clause minimization matched CNFs p2 -complete.proof Gursky (2011) basically follows proof p2 -completeness general CNFspresented Umans (1999), Buchfuhrer Umans (2011) quite longcomplicated. present much shorter simpler proof based similar ideaproof hardness equivalence testing.Matched minimizationInstance :matched CNF integer kQuestion : CNF equivalent k clauses?Theorem 3.2 problem Matched minimization p2 -complete.Proof : Since Matched Minimization special case Boolean minimization,known Boolean minimization p2 -complete, Matched Minimization mustp2 . see problem p2 -hard, reduce p2 -complete Boolean minimizationit.Let (, k) instance Boolean minimization = C1 C2 . . . Cm .let us repeat construction proof Theorem 3.1. Let a1 , a2 , . . . , newvariables occur . Let matched CNF defined (a1 a20 C 0 = (C . . . ) 1 m.. . . ), = C10 C20 . . . Cm12instance Matched minimization (, k).Let (, k) positive instance Boolean minimization. exists CNF= D1 D2 . . . Dk0 (with k 0 k) equivalent . Let CNF equivalent(a1 a2 . . . ) let = D10 D20 . . . Dk0 0 Di0 = Di a1 a2 . . .715fiCepek, Gursky, & Kucera1 k 0 . Clearly equivalent k clauses. Therefore (, k)positive instance Matched minimization.see direction let (, k) positive instance Matched minimizationlet CNF equivalent k clauses. Let CNF originatingpartial assignment sets a-variables zero sets variable.Since equivalent equivalent (a1 a2 . . . ),equivalent . Clearly || || k since equivalent conclude(, k) positive instance Boolean minimization.Remark 3.3 Since occurrences a-variables proof Theorem 3.2positive, every resolution keeps a-variables every derived clause. assumeprime therefore every clause also contains a-variables positivelythus fact number clauses .4. Prime Representations Matched Formulasdifficult see unlike well-behaved classes CNFs (such e.g. HornCNFs quadratic CNFs) prime irredundant CNFs lie inside class,case matched CNFs. Consider CNF(a b) (b c) (c a)matched logically equivalent CNF(a b) (b a) (c b) (b c)matched despite prime irredundant. Thus legitimate question,whether given (nonprime) matched CNF, exists least one logically equivalentprime irredundant CNF also matched. rest section proveaffirmative answer question. Let us start simple useful observation.Observation 4.1 Let = C1 . . .Cm matched CNF let C clause derivedregular tree-like resolution derivation . Let C = R(D1 , D2 ), D1 = (A1 z)D2 = (A2 z), i.e. resolution z. Let T1 denote subtree rootedD1 , let T2 denote subtree rooted D2 . Ci , {1, . . . , m} leafclause T1 T2 , Ci contains neither z z thus cannot matchedz matching .Proof : Since regular, resolution z C = R(D1 , D2 ). Thusz T1 z T2 , since would D1 D2 respectivelywell. Thus Ci T1 T2 cannot contain z all.following lemma allow us exchange one free variable matched variablematching given matched CNF .Lemma 4.2 Let = C1 . . . Cm matched CNF let matching . Letclause derived regular tree-like resolution . Let x variablefree , variable matched matching 0satisfy following property: X denotes set variables matched, X 0 denotes set variables matched 0 , X 0 = (X \ {y}) {x}.716fiOn Minimum Representations Matched FormulasProof : shall proceed induction depth regular tree-like resolutionproof D. , set variable matchednn0 := \ {D, y} {D, x} .let us assume = R(D1 , D2 ), D1 D2 either clauses ,derived regular tree-like resolution . Suppose resolutionvariable z let us denote D1 = (A1 z), D2 = (A2 z). = 1, 2 let Ti denotesubtree rooted Di , Li set leaf clauses Ti , sub-CNF formed clausesLi , Mi sub-matching clauses , Xi set variables matchedMi .First let us assume variable x A1 \ A2 . Let us without loss generality assumex appears positively D1 (otherwise switch polarity x ), thusD1 = (A01 x z), A01 = A1 \ {x}. use induction hypothesissubtree T1 rooted D1 , matching M1 1 . follows matching M101 x matched variable variable y1 D1 matchedM1 free M10 . Moreover X10 denotes set variables matched M10 ,X10 = (X1 \ {y1 }) {x}. extend matching M10 whole adding pairsmatching clauses L2 \L1 variables X2 \X1 , let new matching denoted00 . y1 6= z, y1 occurs set 0 = 00 .y1 = z, free variable 00 . Let M200 denote sub-matching 00clauses 2 . use induction hypothesis 2 , resolvent D2 , variabley1 = z (playing role free variable). find matching M2 2variable D2 y1 matched M2 , free M2 matched M200 .particular X2 denotes set matched variables M2 X200 denotes setvariables matched M200 , X2 = (X200 \ {y}) {y1 }. extend matchingM2 whole formula adding pairs matching clauses L1 \ L2 variables X100 \ X20000 (here X100 denotes set variables matched 00 leaves T1 ).way obtain desired matching 0 . clear 6= z thus D. MoreoverX 0 = (X \ {y}) {x}.Now, let us assume x A1 A2 . Observation 4.1 z 6 X1 X2 . Let{1, 2} z 6 Xi . let us use induction hypothesis variablex. get matching Mi0 formula extended whole addingpairs matching clauses L3i \ Li variables X3i \ Xi (the extendedmatching desired matching 0 ). also get variable matchedMi , free Mi0 . Since Xi , get 6= z thus D. ClearlyX 0 = (X \ {y}) {x}.Lemma 4.3 Let = C1 C2 . . . Cm matched CNF let us assumeimplicate derived regular tree-like resolution derivation C1used, 0 = C2 . . . Cm matched CNF.Proof : fact C1 used resolution derivation implies C1 leafclause . shall proceed induction depth . Let matchinglet X denote set variables, matched . shall preservefollowing invariant:717fiCepek, Gursky, & Kucera(*) 0 matching 0 constructed proof X 0 denotes matchedvariables 0 , X 0 = X.Let us first assume = C1 . proposition trivially follows invariant(*) satisfied. let us suppose = R(C1 , Cj ), j {2, . . . , m}. Letvariable matched C1 , i.e. {C1 , y} . D, setnn0 = \ {C1 , y} {D, y} .6 D, follows C1 Cj resolve y. Let us without loss generalityassume appears positively C1 let us denote C1 = (A1 y) Cj = (Aj y),hence = A1 Aj . Cj matched another variable z Aj , thussetnn0 = \ {C1 , y}, {Cj , z} {Cj , y}, {D, z} .0 matching 0 X 0 = X.let us assume = R(D1 , D2 ) D1 D2 derivedresolution derivation , belong . Suppose resolution variablez let us denote D1 = (A1 z), D2 = (A2 z). = 1, 2 let Ti denote subtreerooted Di , Li set leaf clauses Ti , sub-CNF formed clauses Li ,let Mi sub-matching clauses , Xi set variables matchedMi .Let us first assume C1 L1 L2 . Observation 4.1 get z 6 X1 X2 .Let {1, 2} z/ Xi . Let us use induction hypothesis Ti Difind matching formula 0i CNF formed clauses (Li \ {C1 }) {Di }.induction hypothesis 0i matched formula, let Mi0 matching constructed 0isatisfies invariant (*), i.e. set matched variables Mi0 Xi . Let xvariable matched Di Mi0 . since z 6 Xi x Ai , thus x belongswell. construct matching 0 0 extending Mi0 pairsmatching variables X3i \ Xi clauses L3i \ Li . Moreover replacepair {Di , x} pair {D, x}. result 0 matching 0 exactlyvariables X matched thus 0 satisfies invariant (*).rest proof shall assume C1 L1 \ L2 . use inductionhypothesis T1 D1 find matching M10 formula 01 CNFformed clauses (L1 \ {C1 }) {D1 }. induction hypothesis M10 satisfies invariant (*)thus matched variables M10 exactly variables set X1 . Let xvariable clause D1 matched M10 . two cases consider .1. x A1 , construct matching 0 whole formula 0 extendingM10 pairs matching clauses L2 \ L1 variables X2 \ X1 ,replace pair {D1 , x} pair {D, x}. 0 matching matchesvariables X thus also satisfies invariant (*).2. x = z, situation complicated. case observe z X1 \X2(z 6 X1 X2 Observation 4.1, hand z matched M1 M10 ). LetM20 matching sub-CNF 2 formed clauses L2 constructed718fiOn Minimum Representations Matched Formulasfollows. clauses L1 L2 matched variables M10 ,clauses L2 \ L1 matched variables . Note M20 formedway really matching, particular C1 belong L2 thusmatter matched variable M10 . Moreover, X20 denotesset variables matched M20 , variable X20 matched exactly oneclause. M10 change anything clauses L2 \ L1match variables X2 \ X1 . Note, X20 necessarily equalX2 , M10 allowed use variables X1 \ X2 clauses L1 L2 ,X = X20 X1 . also z free variable M20 ,z X1 \ X2 , thus matched clause L2 , zmatched D1 M10 , thus z matched clause L1 L2 M10 .following situation: formula 2 , clause D2derived regular tree-like resolution T2 2 . matching M202 matches variables set X20 . variable z freeM20 , thus use Lemma 4.2 find another matching M200 2 variableD2 z matched clause M200 , free M200matched M20 . Moreover, X200 denotes set variables matched M200 ,X200 = (X20 \ {z}) {y}. Necessarily z 6= y, thus A2 . readyform desired matching 0 .(a) clause C L1 \ (L2 {C1 }) matched variable 0{C, a} M10 .(b) clause C L2 matched variable 0 {C, a} M200 .(c) clause matched variable 0 .0 defined way indeed matching, particular C L1 \(L2 {C1 }),matched variable belongs X1 \ (X20 {z}), thus free M20M200 , still used C. Let us also observe 0 preserves invariant(*), particular X 0 denotes set variables matched 0 , X 0 = X. Letvariable X 0 , shall show X well, |X 0 | = |X|,follows X 0 = X.(a) matched clause C L1 \ (L2 {C1 }), {C, a} M10since induction hypothesis invariant (*) preserved M10X1 thus X.(b) matched clause C L2 , {C, a} M200 ,X 00 = (X20 \ {y}) {z}. know z X X20 X.(c) matched D, = X20 X.Together invariant (*) satisfied 0 .case found matching 0 CNF 0 satisfies invariant (*), prooffinished.Theorem 4.4 Let matched CNF representing function f , primeirredundant representation f also matched.719fiCepek, Gursky, & KuceraProof : follows Lemma 4.3. Firstly, drop redundant clauseswithout spoiling matched property. 0 irredundant representation f originateddropping redundant clauses, turn prime representationusing Lemma 4.3 follows. 0 = C1 . . . Cm , C10 ( C1 prime implicate,C10 derived resolution derivation 0 . Since 0 already irredundant,every resolution derivation C10 0 use C1 . Thus Lemma 4.3,formula 00 = C10 C2 . . . Cm also matched. way replace every clause0 prime subimplicate. Thus obtain prime irredundant representationf.5. Minimum Representations Matched Formulasprevious sections seen matched CNF may logicallyequivalent prime irredundant CNFs matched always least oneCNF matched. section shall show stronger statement holds CNFsprime irredundant also clause minimum. shall proveBoolean function f admits matched CNF representation, every clause minimumCNF representation f matched CNF.Theorem 5.1 Let matched CNF representing function f set variables Vlet clause minimum CNF representation f . matched CNF.Proof : Due Theorem 4.4 may assume prime irredundant thusI(f ). Let us assume contradiction matched Xmaximal (under inclusion) sub-CNF violating Halls condition (such subset must existdue Theorem 2.11). Let us denote X set variables sub-CNF X , = V \ Xset remaining variables , = \ X remaining clauses (noteclauses may contain variables also X). followingholds:violation Halls condition |X | > |X|.maximality X exists matching clauses variables, i.e. matched CNF even drop variables X clauses.follows fact every subset must satisfy Halls condition evenrespect variables , since otherwise violating subset couldadded X contradicting maximality.existence matching implies satisfied using variables(each clause satisfied matched variable). let : 7 {0, 1} partialassignment satisfying clauses (t necessarily unique). Clearly, autarkysatisfies every clause containing assigned literal.follows Lemma 2.10 (t) = X represents exclusive component fX fdefined exclusive set X I(f ), contains clauses consisting variablesX. Since also represents f assumed I(f ), follows Corollary 2.9autarky also . Thus, similarly (t) above, conclude(t) sub-CNF represents exclusive component fX f , i.e. (t) (t).720fiOn Minimum Representations Matched FormulasHowever, matched, every sub-CNF (and particular (t)) matched, thus|(t)| |X| |(t)| = |X | > |X|. now, since (t) (t) = X representexclusive component f , also CNF 0 = ( \ (t)) (t) represents f Corollary 2.6.However, get | 0 | < || contradicting assumed minimality .6. Conclusionspaper study class matched CNFs important class formulastheory parametrized SAT algorithms. focus clause minimum CNF representations Boolean functions represented matched CNFs. resultspresented paper two types:1. Complexity results. show testing logical equivalence two matched CNFsco-NP-complete. use similar construction prove Boolean minimization matched CNFs p2 -complete. already known fact,presented proof much shorter simpler proof Gursky (2011).results appear Section 3.2. Structural results. prove given (non-prime) matched CNF representingfunction f , may prime representations f matched,exists least one prime representation f matched. Furthermoreprove case clause minimum CNFs f guaranteed matched.latter result course implies former, however, proof latter resultuses former one. Thus subsequently appear text main resultsSections 4 5.interesting question future research whether structural results Sections 4 5 extended way matched CNFs, i.e. CNFs maximumdeficiency zero, CNFs maximum deficiency bounded constant (see Section 1definition maximum deficiency).paper use number clauses CNF measure optimality.interesting question whether result would hold total length formula (i.e.total number literal occurrences formula) would considered.Acknowledgmentssecond author gratefully acknowledges support Charles University GrantAgency (grant No. 1390213).ReferencesAharoni, R., & Linial, N. (1986). Minimal non-two-colorable hypergraphs minimalunsatisfiable formulas. Journal Combinatorial Theory, Series A, 43 (2), 196 204.Ausiello, G., DAtri, A., & Sacca, D. (1986). Minimal representation directed hypergraphs. SIAM Journal Computing, 15 (2), 418431.721fiCepek, Gursky, & KuceraBollobas, B. (1998). Modern Graph Theory, Vol. 184 Graduate Texts Mathematics.Springer.Boros, E., & Cepek, O. (1994). complexity Horn minimization. Tech. rep. 1-94,RUTCOR Research Report RRR, Rutgers University, New Brunswick, NJ.Boros, E., Cepek, O., & Kogan, A. (1998). Horn minimization iterative decomposition.Annals Mathematics Artificial Intelligence, 23, 321 343.Boros, E., Cepek, O., Kogan, A., & Kucera, P. (2009). subclass Horn CNFs optimallycompressible polynomial time. Annals Mathematics Artificial Intelligence,57, 249291.Boros, E., Cepek, O., Kogan, A., & Kucera, P. (2010). Exclusive essential setsimplicates boolean functions. Discrete Applied Mathematics, 158 (2), 81 96.Buchfuhrer, D., & Umans, C. (2011). complexity boolean formula minimization.Journal Computer System Sciences, 77 (1), 142 153.Buning, H. K., & Lettmann, T. (1999). Propositional Logic: Deduction Algorithms.Cambridge University Press, New York, NY, USA.Cepek, O. (1995). Structural Properties Minimization Horn Boolean Functions.Ph.D. dissertation, Rutgers University, New Brunswick, NJ, October 1995.Cepek, O., Kucera, P., & Savicky, P. (2012). Boolean functions simple certificateCNF complexity. Discrete Applied Mathematics, 160 (4-5), 365 382.Fleischner, H., Kullmann, O., & Szeider, S. (2002). Polynomial-time recognition minimal unsatisfiable formulas fixed clause-variable difference. Theoretical ComputerScience, 289 (1), 503 516.Flum, J., & Grohe, M. (2006). Parameterized complexity theory, Vol. 3. Springer.Franco, J., & Van Gelder, A. (2003). perspective certain polynomial-time solvableclasses satisfiability. Discrete Appl. Math., 125 (2-3), 177214.Garey, M., & Johnson, D. (1979). Computers Intractability: Guide TheoryNP-Completeness. W.H. Freeman Company, San Francisco.Genesereth, M., & Nilsson, N. (1987). Logical Foundations Artificial Intelligence. MorganKaufmann, Los Altos, CA.Gursky, S. (2011). Minimization matched formulas. Safrankova, J., & Pavlu, J. (Eds.),WDS11 Proceedings Contributed Papers: Part Mathematics ComputerScience, pp. 101105, Prague. Matfyzpress.Hall, P. (1935). representatives subsets. Journal London Mathematical Societysecond Series, s1-10, 2630.Hammer, P., & Kogan, A. (1993). Optimal compression propositional Horn knowledgebases: Complexity approximation. Artificial Intelligence, 64, 131 145.Hammer, P., & Kogan, A. (1995). Quasi-acyclic propositional Horn knowledge bases: Optimal compression. IEEE Transactions Knowledge Data Engineering, 7 (5),751 762.722fiOn Minimum Representations Matched FormulasKullmann, O. (2000). Investigations autark assignments. Discrete Applied Mathematics,107 (13), 99 137.Kullmann, O. (2003). Lean clause-sets: generalizations minimally unsatisfiable clausesets. Discrete Applied Mathematics, 130 (2), 209 249.Lovasz, L., & Plummer, M. D. (1986). Matching Theory. North-Holland.Maier, D. (1980). Minimal covers relational database model. Journal ACM,27, 664 674.Monien, B., & Speckenmeyer, E. (1985). Solving satisfiability less 2n steps. DiscreteApplied Mathematics, 10 (3), 287 295.Szeider, S. (2003). Minimal unsatisfiable formulas bounded clause-variable differencefixed-parameter tractable. Warnow, T., & Zhu, B. (Eds.), ComputingCombinatorics, Vol. 2697 Lecture Notes Computer Science, pp. 548558. SpringerBerlin Heidelberg.Szeider, S. (2005). Generalizations matched CNF formulas. Annals MathematicsArtificial Intelligence, 43 (1-4), 223238.Szeider, S. (2007). Matched formulas backdoor sets. Marques-Silva, J., & Sakallah,K. (Eds.), Theory Applications Satisfiability Testing SAT 2007, Vol. 4501Lecture Notes Computer Science, pp. 9499. Springer Berlin Heidelberg.Tovey, C. A. (1984). simplified NP-complete satisfiability problem. Discrete AppliedMathematics, 8 (1), 85 89.Tseitin, G. S. (1983). complexity derivation propositional calculus. Automation Reasoning, pp. 466483. Springer.Umans, C. (2001). minimum equivalent DNF problem shortest implicants. J.Comput. Syst. Sci., 63 (4), 597611.Umans, C., Villa, T., & Sangiovanni-Vincentelli, A. L. (2006). Complexity two-levellogic minimization. IEEE Trans. CAD Integrated Circuits Systems, 25 (7),12301246.Umans, C. M. (1999). Hardness approximating p2 minimization problems. FOCS 99:Proceedings 40th Annual Symposium Foundations Computer Science, pp.465474, Washington, DC, USA. IEEE Computer Society.Urquhart, A. (1995). complexity propositional proofs. Bulletin SymbolicLogic, 1 (4), pp. 425467.Urquhart, A. (2011). depth resolution proofs. Stud. Log., 99 (1-3), 349364.723fiJournal Artificial Intelligence Research 51 (2014) 579-603Submitted 4/14; published 11/14Agent Left Behind: Dynamic Fair Division Multiple ResourcesIan KashIANKASH @ MICROSOFT. COMMicrosoft Research Cambridge, UKAriel D. ProcacciaARIELPRO @ CS . CMU . EDUCarnegie Mellon University, USANisarg ShahNKSHAH @ CS . CMU . EDUCarnegie Mellon University, USAAbstractRecently fair division theory emerged promising approach allocation multiplecomputational resources among agents. reality agents present systemsimultaneously, previous work studied static settings relevant information knownupfront. goal better understand dynamic setting. conceptual level, developdynamic model fair division, propose desirable axiomatic properties dynamic resourceallocation mechanisms. technical level, construct two novel mechanisms provablysatisfy properties, analyze performance using real data. believework informs design superior multiagent systems, time expands scopefair division theory initiating study dynamic fair resource allocation mechanisms.1. Introductionquestion fairly divide goods resources subject intellectual curiositymillennia. early solutions traced back ancient writings, rigorous approachesfairness proposed late mid Twentieth Century, mathematicians socialscientists. time, fair division emerged influential subfield microeconomic theory.last years fair division also attracted attention AI researchers (see, e.g., Chevaleyre, Endriss, Estivie, & Maudet, 2007; Procaccia, 2009; Chen, Lai, Parkes, & Procaccia, 2010;Moulin, 2003; Brams & Taylor, 1996), envision applications fair division multiagent systems (Chevaleyre, Dunne, Endriss, Lang, Lematre, Maudet, Padget, Phelps, Rodrguez-Aguilar, &Sousa, 2006). However, fair division theory seen relatively applications date.recently exciting combination technological advances theoreticalinnovations pointed way towards concrete applications fair division. modern datacenters, clusters, grids, multiple computational resources (such CPU, memory, networkbandwidth) must allocated among heterogeneous agents. Agents demands resourcestypically highly structured, explain below. Several recent papers (Gutman & Nisan, 2012;Ghodsi, Zaharia, Hindman, Konwinski, Shenker, & Stoica, 2011; Parkes, Procaccia, & Shah, 2014;Dolev, Feitelson, Halpern, Kupferman, & Linial, 2012) suggest classic fair division mechanismspossess excellent properties environments, terms fairness guarantees wellgame-theoretic properties.Nevertheless, aspects realistic computing systems beyond current scope fairdivision theory. Perhaps importantly, literature capture dynamicssystems. Indeed, typically case agents present systemc2014AI Access Foundation. rights reserved.fiK ASH , P ROCACCIA , & HAHgiven time; agents may arrive depart, system must able adjust allocationresources. Even conceptual level, dynamic settings challenge premises fairdivision theory. example, one agent arrives another, first agent intuitivelypriority; fairness mean context? introduce concepts necessaryanswer question, design novel mechanisms satisfy proposed desiderata.contribution therefore twofold: design realistic resource allocation mechanismsmultiagent systems provide theoretical guarantees, time expand scopefair division theory capture dynamic settings.1.1 Overview Model Resultsprevious papers (e.g., Ghodsi et al., 2011; Parkes et al., 2014), assume agents demandresources fixed proportions. Leontief preferences known economicseasily justified typical settings agents must run many instances single task (e.g.,map jobs MapReduce framework). Hence, example, agent requires twice muchCPU RAM run task prefers allocated 4 CPU units 2 RAM units 2 CPU units1 RAM unit, indifferent former allocation 5 CPU units 2 RAM units.consider environments agents arrive time (but depart see Section 7additional discussion point). aim design resource allocation mechanisms makeirrevocable allocations, i.e., mechanism allocate resources agent time,never take resources back.adapt prominent notions fairness, efficiency, truthfulness dynamic settings.fairness, ask envy freeness (EF), sense agents like allocation best;sharing incentives (SI), agents prefer allocation proportional shareresources. also seek strategyproof (SP) mechanisms: agents cannot gain misreportingdemands. Finally, introduce notion dynamic Pareto optimality (DPO): k agentsentitled k/n resource, allocation dominated (in senseformalized later) allocations divide entitlements. first result (in Section 3)impossibility: DPO EF incompatible. proceed relaxing properties.Section 4, relax EF property. new dynamic property, call dynamic EF(DEF), allows agent envy another agent arrived earlier, long former agentallocated resources latter agents arrival. construct new mechanism, DYNAMIC DRF,prove satisfies SI, DEF, SP, DPO.Section 5, relax DPO property. cautious DPO (CDPO) notion allows allocationscompete allocations ultimately guarantee EF, regardless demandsfuture agents. design mechanism called C AUTIOUS LP, show satisfies SI, EF, SP,CDPO. sense, theoretical results tight: EF DPO incompatible, relaxingone two properties sufficient enable mechanisms satisfy both, conjunctionSI SP.Despite assumptions imposed theoretical model, believe new mechanisms compelling, useful guides design practical resource allocation mechanismsrealistic settings. Indeed, Section 6, test mechanisms real data obtained traceworkloads Google cluster, obtain encouraging results.580fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES1.2 Related WorkWalsh (2011) proposed problem fair cake cutting agents arrive, take piece cake,immediately depart. cake cutting setting deals allocation single, heterogeneousdivisible resource; contrast setting, deals multiple, homogeneous divisible resources. Walsh suggested several desirable properties cake cutting mechanisms setting,showed adaptations classic mechanisms achieve properties (Walsh also pointedallocating whole cake first agent achieves properties). particular,notion forward envy freeness, discussed below, related notion dynamic envyfreeness.networking community studied problem fairly allocating single homogeneousresource queuing model agents task requires given number time unitsprocessed. words, models tasks processed time, demands stay fixed,dynamics agent arrivals departures. well-known fair queuingsolution (Demers, Keshav, & Shenker, 1989) allocates one unit per agent successive round-robinfashion. solution also analyzed economists (Moulin & Stong, 2002).Previous papers allocation multiple resources study static setting. example,Ghodsi et al. (2011) proposed dominant resource fairness (DRF) mechanism, guaranteesnumber desirable theoretical properties. Li Xue (2013) presented characterizationsmechanisms satisfying various desiderata Wong et al. (2012) analyzed classic tradeofffairness efficiency, generic frameworks capture DRF special case.Parkes et al. (2014) extended DRF several ways, particular studied case indivisibletasks. Finally, DRF also extended queuing domain (Ghodsi, Sekar, Zaharia, &Stoica, 2012) incorporate job placement considerations (Ghodsi, Zaharia, Shenker, & Stoica,2013), generalizations also use static setting. Recently, Zahedi Lee (2014) appliedconcept Competitive Equilibrium Equal Outcomes (CEEI) case Cobb-Douglasutilities achieve properties similar DRF. empirically show utilities wellsuited modeling user preferences hardware resources cache capacity memorybandwidth. Dolev et al. (2012) defined notion fairness different one consideredDRF. also proved fair allocation according new notion always guaranteedexist static setting. Gutman Nisan (2012) gave polynomial time algorithm findallocation, also considered generalizations DRF general model utilities.elaborate several results below.2. Preliminariessetting, agent task requires fixed amounts different resources. utilityagent depends quantity (possibly fractional) tasks execute givenallocated resources. Formally, denote set agents N = {1, . . . , n}, set resourcesR, |R| = m. Let Dir denote ratio maximum amount resource r agentuse given amounts resources present system total amount resourceavailable system, either allocated free. words, Dir fraction resource rrequired agent i. Following Ghodsi et al. (2011), dominant resource agent definedresource r maximizes Dir , fraction dominant resource allocated agentcalled dominant share. Following Parkes et al. (2014), (normalized) demand vector agentgiven di = hdi1 , . . . , dim i, dir = Dir /(maxr0 Dir0 ) resource r. Let581fiK ASH , P ROCACCIA , & HAHset possible normalized demand vectors. Let dk = hd1 , . . . , dk denote demand vectorsagents 1 k. Similarly, let d>k = hdk+1 , . . . , dn denote demand vectors agentsk + 1 n.AnPallocation allocates fraction Air resource r agent i, subject feasibility condition Air 1 r R. Throughout paper assume resources divisibleagent requires positive amount resource, i.e., dir > 0 Nr R. allocations, model preferences coincides domain Leontiefpreferences, utility agent allocation vector Ai givenui (Ai ) = max{y R+ : r R, Air dir }.words, utility agent fraction dominant resource actually use, givenproportional demands allocation various resources. However, relyinterpersonal comparison utilities; agents utility function simply induces ordinal preferencesallocations, exact value irrelevant.say allocation Pareto-dominated another allocation A0 ui (A0i ) ui (Ai )every agent i, uj (A0j ) > uj (Aj ) agent j. allocations agents NA0 agents N , say A0 extensionA0ir Air every agent every resource r. = , simply say A0extension A. allocation called non-wasteful every agent exists R+r R, Air = dir . non-wasteful allocation, utility agent sharedominant resource allocated agent. Also, non-wasteful allocationN,ui (A0i ) > ui (Ai ) r R, A0ir > Air .(1)3. Dynamic Resource Allocation: New Modelconsider dynamic resource allocation model agents arrive different timesdepart (see Section 7 discussion point). assume agent 1 arrives first, agent2, general agent k arrives agents 1, . . . , k 1; say agent k arrives step k.agent reports demand arrives demand change time. Thus,step k, demand vectors dk known, demand vectors d>k unknown. dynamic resourceallocation mechanism operates follows. step k, mechanism takes input reporteddemand vectors dk outputs allocation Ak agents present system. Crucially,every step k 2, every agentassume allocations irrevocable, i.e., Akir Ak1irk 1, every resource r. also assume mechanism knows total numberagents n advance.Irrevocability justified various settings, e.g., cases resources committedlong-term projects. One example research cluster shared faculty membersuniversity. cluster, total number faculty members access cluster(denoted n setting) known mechanism advance assume model.Another important setting irrevocability becomes necessary assumption case divisible consumable resources. case, agent may consume resources receives certainstep, cannot reclaimed later on.582fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCESPrevious work static resource allocation (e.g., Ghodsi et al., 2011; Parkes et al., 2014) focuseddesigning mechanisms satisfy four prominent desiderata. Three two fairnessproperties one game-theoretic property immediately extend dynamic setting.1. Sharing Incentives (SI). say dynamic allocation mechanism satisfies SI ui (Aki )ui (h1/n, . . . , 1/ni) steps k agents k. words, agent arrivesreceives allocation likes least much equal split resources.models setting agents made equal contributions system henceequal entitlements. cases, contributions typically recorded, allowsmechanism know total number agents n advance, assumed setting.2. Envy Freeness (EF). dynamic allocation mechanism EF ui (Aki ) ui (Akj ) stepsk agents i, j k, is, agent present would never prefer allocationanother agent.3. Strategyproofness (SP). dynamic allocation mechanism SP agent misreportdemand vector strictly better step k, regardless reported demandsagents. Formally, dynamic allocation mechanism SP agent Nstep k, Aki allocation agent step k agent reports true demand vectorBki allocation agent step k agent reports different demand vector(in cases agents report true demand vectors), ui (Aki ) ui (Bki ).avoid introducing additional notations required later.static setting, fourth prominent axiom, Pareto optimality (PO), means mechanisms allocation Pareto dominated allocation. course, dynamic settingunreasonable expect allocation early stages Pareto undominated, needsave resources future arrivals (recall allocations irrevocable). believe thoughfollowing definition naturally extends PO dynamic setting.4. Dynamic Pareto Optimality (DPO). dynamic allocation mechanism DPO step k,allocation Ak returned mechanism Pareto dominated allocationBk allocates (k/n)-fraction resource among k agents presentsystem. Put another way, step allocation Pareto dominatedallocation redistributes collective entitlements agents presentsystem among agents.straightforward verify non-wasteful mechanism (a mechanism returning nonwasteful allocation step) satisfies DPO allocation returned mechanism step k uses least (k/n)-fraction least one resource (the assumption strictlypositive demands plays role here).moving possibility impossibility results, give examples illustratevarious combinations properties constrain allocation resources.Example 1 (Satisfying Sharing Incentives (SI) Dynamic Pareto Optimality (DPO)).paper, consider non-wasteful allocations. Hence, described above, DPO equivalentallocating least (k/n)-fraction least one resource every step k, allocationsproportional. hand, mechanism seeks satisfy SI, cannot allocate583fiK ASH , P ROCACCIA , & HAH(k/n)-fraction resource step k. Indeed, (k/n)-fraction resource rallocated step k, every agent arriving step k reports r dominant resource,mechanism would enough resource r left allocate least (1/n)fraction r, required SI. Thus, non-wasteful mechanism satisfying SI DPO mustallocate, every step k, exactly (k/n)-fraction resource (k/n)-fractionevery resource. words, every step k, mechanism pool availableresources contains (k/n)-fraction resource, minus fraction already allocatedallocated k agents currently present. mechanism allocatepool, must exhaust least one resource pool.Example 2 (Understanding Strategyproofness (SP)). example, take mechanismmay seem SP first glance, show violates definition SP. simplicity,allow agents possibly zero demands resources example. allows beneficial manipulations following simple mechanism, callDYNAMIC ICTATORSHIP. (We note DYNAMIC ICTATORSHIP otherwise strategyproofstrictly positive demands see discussion following Theorem 3.) step k, mechanism allocates 1/n share resource agent k, takes back shares different resourcesagent cannot use, allocates resources k present agents orderarrival using serial dictatorship, is, allocates agent many resources agentuse, proceeds next agent. mechanism keeps allocating k/n shareleast one resource allocated. Note mechanism trivially satisfies SI allocatesresources valuable equal split agent soon arrives. mechanism wouldsatisfy DPO standard setting non-zero demands, non-wasteful everystep k allocates k/n fraction least one resource. Intuitively, seems first agentgain reporting false demand vector round gets pick firstallowed take much use available pool resources. showintuition incorrect. Let us denote pool resources available mechanism stepvector fraction available resource. Consider case four agents (agents 1, 2,3, 4), three resources (R1 , R2 , R3 ). Let true demand vectors agentsfollows:d1 = h1, 0.5, 0.5i, d2 = h0, 1, 1i, d3 = h1, 0.5, 0i, d4 = h0, 1, 0.5i.Figure 1 shows allocations returned DYNAMIC ICTATORSHIP various stepsagents report true demand vectors. Now, suppose agent 1 raises demand R3 reportingfalse demand vector h1, 0.5, 1i. case allocations returned mechanism varioussteps shown Figure 2. see manipulation makes agent 1 strictly worsestep 2, strictly better final step. definition SP requires agentable benefit step process lying thus DYNAMIC ICTATORSHIP SP.3.1 Impossibility ResultIdeally, would like design dynamic allocation mechanism SI, EF, SP, DPO.However, show even satisfying EF DPO simultaneously impossible.Theorem 3. Let n 3 2. dynamic resource allocation mechanism satisfies EFDPO.584fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES1111343434341/4121212121/41401/41/81/81/81/8R1R2R31/41/41/21/41/4R1R2R3140(a) Step 11/81/43/83/81/21/41/4R1R2R31401/801/41/8Agent 2Agent 33/83/8Agent 41/21/41/4Unallocated resourceavailable poolR1R2R314(c) Step 3(b) Step 2Agent 11/41/8(d) Step 4Figure 1: Allocations returned DYNAMIC ICTATORSHIP agent 1 reports true demand vector.1111343434341212140141/41/81/81/4R1R2R3(a) Step 10121/81/41/41/41/41/81/4R1R2R3(b) Step 21/41/41/41/161/41/81/81/41/41/2R1R2R3(c) Step 30Agent 2Agent 4141/2Agent 1Agent 3121/41/41401/81/81/85/85/165/8R1R2R3Unallocated resourceavailable pool(d) Step 4Figure 2: Allocations returned DYNAMIC ICTATORSHIP agent 1 manipulates.Proof. Consider setting three agents two resources. Agents 1 2 demand vectorsh1, 1/9i h1/9, 1i, respectively (i.e., d11 = 1, d12 = 1/9, etc.). step 2 (after secondagent arrives), least one two agents must allocated least x = 3/5 sharedominant resource. Suppose contradiction two agents allocated x0 x00 sharesdominant resources 0 < x0 , x00 < x. Then, total fractions two resourcesallocated step 2 would x0 + x00 (1/9) x00 + x0 (1/9), less x + x (1/9) = 2/3,violating DPO. Without loss generality, assume agent 1 allocated least x = 3/5 sharedominant resource (resource 1) step 2. agent 3 reports demand vector h1, 1/9iidentical agent 1 allocated 2/5 share dominant resource(resource 1), would envy agent 1.easy extend argument case n > 3, adding n 3 agents demandvectors identical demand vector agent 3. again, verifiedend step 2, least one first two agents (w.l.o.g., agent 1) must allocated least 9/(5n)share dominant resource. take remaining resources (in particular, 19/(5n)share resource 1), divide among remaining n 2 agents demand vectorsidentical agent 1, least one get (1 9/(5n))/(n 2) < 9/(5n)share dominant resource, envy agent 1. extend case > 2, let agentsnegligibly small demands additional resources. (Proof Theorem 3)interesting note either EF DPO dropped, remaining three axiomseasily satisfied. example, trivial mechanism E QUAL PLIT gives every agent1/n share resource arrives satisfies SI, EF SP. Achieving SI, DPO,585fiK ASH , P ROCACCIA , & HAHSP also simple. Indeed, consider DYNAMIC ICTATORSHIP mechanism Example 2.example explains DYNAMIC ICTATORSHIP satisfies SI DPO. Even thoughDYNAMIC ICTATORSHIP SP possibly zero demands (as shown example),clearly SP strictly positive demands (as assumed throughout paper). agent k arrivesstep k, allocated 1/n share dominant resource (and resources proportion),subsequently agent 1 allocated resources k/n share least one resource exhausted.Since every agent requires exhausted resource due strictly positive demands, allocationstops. summary, agents except agent 1 receive exactly 1/n share dominant resourcearrive, receive resources later on; hence, cannot gain reportingfalse demand vector. step k, agent 1 receives much resources poolresources remain allocating agents 2 k 1/n share dominant resourceoriginal pool contains k/n share resource. Therefore, agent 1 also cannotgain manipulation.E QUAL PLIT DYNAMIC ICTATORSHIP satisfy maximal subsets proposed desiderata, neither compelling mechanism. Since mechanisms permitteddropping EF DPO entirely, instead explore relaxations EF DPO rule mechanisms guide us towards compelling mechanisms.4. Relaxing Envy FreenessRecall DPO requires mechanism allocate least k/n fraction least one resourcestep k, every k {1, . . . , n}. Thus mechanism sometimes needs allocate large amountresources agents arriving early, potentially making impossible mechanism preventlate agents envying early agents. words, agent enters systemmay envy agent j arrived did; inevitable order able satisfy DPO.However, would unfair agent agent j allocated resources since agent arrivedstill envied j. distill intuition, introduce following dynamic version EF.20 . Dynamic Envy Freeness (DEF). dynamic allocation mechanism DEF stepagent envies agent j j arrived j allocatedresources since arrived. Formally, every k {1, . . . , n}, ui (Akj ) > ui (Aki ) j <Akj = Ai1j .Walsh (2011) studied dynamic cake cutting setting proposed forward EF, requiresagent envy agent arrived later. notion weaker DEFrule case agent envies agent j arrived earlier j receivedresources since arrived. setting, even trivial mechanism DYNAMIC ICTATORSHIP (seeSection 3.1) satisfies forward EF, fails satisfy stronger notion DEF.next construct dynamic resource allocation mechanism DYNAMIC DRF achievesrelaxed fairness notion DEF, together SI, DPO, SP. mechanism given Algorithm 1.Intuitively, step k mechanism starts current allocation among presentagents keeps allocating resources agents minimum dominant sharerate, k/n fraction least one resource allocated. Always allocating agentsminimum dominant share ensures agents allocated resources586fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCESALGORITHM 1: DYNAMIC DRFData: DemandsResult: Allocation Ak step kk 1;k n{xki }ki=1 Solution LP box below;Akir xki dir , k;k k + 1;endMaximize ksubjectxki k , kxki xk1, k 1Pk iki=1 xi dir k/n, r Renvied. water-filling mechanism dynamic adaptation dominant resource fairness(DRF) mechanism proposed Ghodsi et. al. (2011). See Figure 3 example.1112323232/91302/91/3131/31/61/4R1R2R3(a) Step 104/94/91/3R1R2R31/61/62/92/9130(b) Step 21/3Agent 11/3Agent 24/94/91/3Agent 3R1R2R3Dominant share(c) Step 3Figure 3: Allocations returned DYNAMIC DRF various steps 3 agents demands d1 =h1, 1/2, 3/4i, d2 = h1/2, 1, 3/4i, d3 = h1/2, 1/2, 1i, three resources R1 , R2 ,R3 . Agent 1 receives 1/3 share dominant resource step 1. step 2, water-filling drivesdominant shares agents 1 2 4/9. step 3, however, agent 3 receive1/3 dominant share allocations agents 1 2 remain unchanged.Theorem 4. DYNAMIC DRF satisfies SI, DEF, DPO, SP, implemented polynomial time.Proof. First show DYNAMIC DRF satisfies SI. need prove xki 1/nagents k every step k {1, . . . , n}. prove induction k. base casek = 1, easy see x11 = 1/n 1 = 1/n solution LP DYNAMIC DRFhence optimal solution satisfies x11 1 1/n (in fact, equality). Assumetrue step k 1 let us prove claim step k, k {2, . . . , n}. step k, onefeasible solution LP given xki = xk1agents k 1, xkk = 1/n k = 1/n.see this, note trivially satisfies first two constraints LP, induction587fiK ASH , P ROCACCIA , & HAHhypothesis xk11/n k 1. Furthermore, proposed feasible solution,r RkXi=1xki dir =k1Xi=1xk1dir +k1 1k1dkr+ ,nnnnfirst transition follows construction feasible solution second transition holds {xk1}k1i=1 satisfies LP step k 1, particular third constraintLP. Since feasible solution achieves k = 1/n, optimal solution achieves k 1/n.Thus optimal solution xki k 1/n k, requirement SI.Next show DYNAMIC DRF satisfies DPO. Observe step k, third constraintLP must tight least one resource optimal solution (otherwise every xki alongk increased sufficiently small quantity, contradicting optimality k ).Thus, step k (non-wasteful) mechanism allocates k/n fraction least one resource,implies mechanism satisfies DPO.prove mechanism satisfies DEF SP, first prove several useful lemmasallocations returned mechanism. proof below, k xki refer optimalsolution LP step k. Furthermore, assume xki = 0 agents > k (i.e., agentspresent system allocated resources). begin following lemma,essentially shows agent allocated resources step using water-filling,agents dominant share step minimum among present agents.Lemma 5. every step k {1, . . . , n}, holds xki = max(M k , xk1) agents k.Proof. Consider step k {1, . . . , n}. first second constraints LP= 0), thus xki max(M k , xik1 )evident xki k xki xk1(note xk1kk. Suppose contradiction xki > max(M k , xk1) k. xkireduced sufficiently small > 0 without violating constraints. makes thirdconstraint LP loose least dir , every resource r R. Consequently, valuesxkj j 6= k increased sufficiently small > 0 without violatingthird constraint LP. Finally, (and correspondingly ) chosen small enoughxki k violated. follows value k increased, contradictingoptimality k . (Proof Lemma 5)Next show step k, dominant shares agents 1 k monotonicallynon-increasing time arrival. intuitive every step k, agent k enterszero dominant share subsequently perform water-filling, hence monotonicity preserved.Lemma 6. agents i, j N < j, xki xkj every step k {1, . . . , n}.Proof. Fix two agents i, j N < j. prove lemma induction k. result trivially holds k < j since xkj = 0. Assume xk1xk1k {j, . . . , n}. stepjk1k1kkkkk, xi = max(M , xi ) max(M , xj ) = xj , first last transitionfollow Lemma 5 second transition follows induction hypothesis. (ProofLemma 6)588fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCESfollowing lemma shows agent j greater dominant share agentstep, j must arrived j must allocated resources sincearrived. Observe close requirement DEF.Lemma 7. step k {1, . . . , n}, xkj > xki agents i, j k, j <xkj = xi1j .Proof. First, note j < trivially follows Lemma 6. Suppose contradictionxkj > xi1(it cannot smaller allocations irrevocable). exists stepj{i, . . . , k} xtj > xt1j . Lemma 5 implies xj = xi , last transition follows xti satisfies second constraint LP step (note t). However, xtj xti due Lemma 6. Thus, xtj = xti . using Lemma 5, xt+1= max(M t+1 , xtj ) =jt0t0max(M t+1 , xti ) = xt+1. Extending argument using simple induction shows xj = xievery step t0 t, particular, xkj = xki , contradicting assumption. (Proof Lemma 7)proceed show DYNAMIC DRF satisfies DEF. need prove step k{1, . . . , n} agents i, j k, agent envies agent j step k (i.e., ui (Akj ) > ui (Aki )),kkkkj < xkj = xi1j . First, note ui (Aj ) > ui (Ai ) trivially implies xj > xi , otherwisedominant resource ri agent i, would Akir = xki xkj xkj djri = Akjragent would envy agent j. DEF follows Lemma 7.prove DYNAMIC DRF SP, suppose contradiction agent N reportuntruthful demand vector d0i agent strictly better least one step. Let kfirst step. Denote xtj dominant share agent j step manipulation (foragent i, share dominant resource untruthful demand vector) similarly,denote value optimal solution LP step manipulation.Lemma 8. xkj xkj every agent j k.Proof. agent j xkj > xki ,xkj = xji1 = xji1 xkj .Here, first transition follows Lemma 7, second transition holds manipulationagent affect allocation step 1, third transition follows LP.agent j xkj xki ,xkj xki < xki = k xkj .second transition true xki xki agent could better truedominant share receives manipulation would received without manipulation. justify third transition, note agent must allocated resources step kmanipulation. k = i, trivial, k > i, follows otherwise k wouldfirst step agent strictly better would ui (Ak1) = ui (Aki ) >k1kkui (Ai ) ui (Ai ), Ai denotes allocation agent step k manipulation. Thus,xki > xk1, third transition follows Lemma 5. last transition holdskxj satisfies first constraint LP step k. Thus, conclude xkj xkj agentsj k. (Proof Lemma 8)589fiK ASH , P ROCACCIA , & HAHNow, mechanism satisfies DPO thus allocates least k/n fraction least oneresource step k without manipulation. Let r resource. fraction resource rallocated step k manipulationXXxkj djr k/n.xkj djr > xki dir +xki d0ir +jks.t.j6=ijks.t.j6=ijustify inequality, note xki d0ir > xki dir Equation (1) (as agent strictly betteroff), addition xkj xkj every j k. However, shows k/n fractionresource r must allocated step k manipulation, impossible due thirdconstraint LP. Hence, successful manipulation impossible, is, DYNAMIC DRF SP.Finally, note LP linear number variables constraints, therefore mechanism implemented polynomial time. (Proof Theorem 4)5. Relaxing Dynamic Pareto Optimalitysaw (Theorem 3) satisfying EF DPO impossible. explored intuitiverelaxation EF. Despite positive result (Theorem 4), idea achieving absolute fairnessconceptualized EF dynamic setting compelling.straw man, consider waiting agents arrive using EF static allocation mechanism. However, scheme highly inefficient, e.g., easy see one alwaysallocate agent least 1/n share dominant resource (and resources proportion)soon arrives still maintain EF every step. much allocatedstep? put forward general answer question using relaxed notion DPO requiresmechanism allocate many resources possible ensuring EF achievedfuture, first require following definition. Given step k {1, . . . , n}, defineallocation k present agents demands dk EF-extensible extendedEF allocation n agents demands = (dk , d>k ), possible future demandvectors d>k Dnk .40 . Cautious Dynamic Pareto optimality (CDPO). dynamic allocation mechanism satisfiesCDPO every step k, allocation Ak returned mechanism Pareto dominated allocation A0 k agents EF-extensible.words, mechanism satisfies CDPO every step selects allocationleast generous allocation ultimately guarantee EF, irrespective future demands.first glance, may obvious CDPO indeed relaxation DPO (i.e., CDPOimplied DPO). However, note DPO requires mechanism allocate least k/n fractionleast one resource r allocation Ak step k, thus allocate least 1/nfraction resource agent i. alternative allocation Pareto dominates Ak mustalso allocate least 1/n fraction r agent i. Consequently, order ensure EF extensionn agents future demands identical demand agent i, alternativeallocation must allocate k/n fraction r , future agent may also require least1/n fraction r avoid envying agent i. follows alternative allocation cannot Paretodominate Ak . Thus, mechanism satisfies CDPO.590fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCESRecall DYNAMIC DRF extends water-filling idea static DRF mechanism (Ghodsiet al., 2011) dynamic setting. DYNAMIC DRF unable satisfy original EF,satisfy DPO every step k needs allocate resources k/n fraction resourceallocated. wish modify DYNAMIC DRF focus competing EF-extensibleallocations, way achieves CDPO EF (as well properties).main technical challenge checking allocation step k violates EF-extensibility.Indeed, uncountably many possibilities future demands d>k EFextension needs guaranteed EF-extensible allocation! course, checking possibilities explicitly feasible. Ideally, would like check small number possibilities.following lemma establishes sufficient verify EF extension existsassumption future agents demand vector moreover identicaldemand vector one present agents.Lemma 9. Let k number present agents, dk demands reported presentagents, EF allocation k present agents. EF-extensibleexists EF extension n agents demands = (dk , d>k ) futuredemands d>k D0 , D0 = {hd1 ink , hd2 ink , . . . , hdk ink }.prove lemma, first introduce notion minimum EF extension. Intuitively,minimum EF extension smallest EF extension (allocating least resources) given EFallocation larger set agents. Formally, let EF allocation set agents NEF extension set agents N (S ). called minimumEF extension EF extension A0 , A0 extension. show minimum EF extension exists exhibits simple structure.Lemma 10. Let EF allocation set agents N let xi dominantshare agent A. Let N let allocationxi dominant share agent . Let xi = xi S, xi = maxjS yij\ S, yij = xj minrR djr /dir . minimum EF extension .Proof. agent dominant share xi avoid envying agent j dominant share xj ,must exist r R xi dir xj djr , is, xi xj djr /dir . follows xixj minrR djr /dir , thus minimum dominant share given yij = xj minrR djr /dir .easy argue EF extension A0 must allocate least xi dominantshare agent , \ S, thus A0 must extension .remains prove EF. First prove intuitive result regarding minimumdominant share agent needs avoid envying agent j, namely yij . claim every r R,yij dir xj djr .(2)Formally, r R,djr0djrdir xjdir = xj djr .r R dir0diryij dir = xj min0Therefore, prevent agent envying agent j, need allocate least xj djrfraction resource r agent r R. Next show EF, i.e., agent enviesagent j . consider four cases.591fiK ASH , P ROCACCIA , & HAHCase 1: j S. case trivial identical EF.Case 2: \ j S. case also trivial receives least yij fractiondominant resource.Case 3: j \ S. must xj = yjt S. Agent envyagent A, hence . Thus, exists resource r R Air Atr Ajr ,last step follows Equation (2). Thus, agent envy agent j.Case 4: \ j \ S. Similarly Case 3, let xj = yjt S. xi yit ,agent envy agent . Thus, exists resource r Air Atr Ajr ,last step follows Equation (2).Therefore, EF extension already established EFextension must extension . conclude minimum EF extension. (Proof Lemma 10)hard see construction minimum EF extension exists,unique. ready prove Lemma 9.Proof Lemma 9. direction proof trivial. prove part, provecontrapositive. Assume exist future demand vectors d>k Dnkexist EF extension N demands = (dk , d>k ). want showexists d0>k D0 EF extension well.Let K = {1, . . . , k} N \ K = {k + 1, . . . , n}. Denote minimum EF extensionN demands . Let dominant share agent K xi dominantshare agent j N xj .EF extension N demandsPn feasible, hence must infeasible too.Therefore, exists resource r i=1 xi dir > 1. Note every agent j N \K,exists agent K xj = xi minr0 R dir0 /djr0 , hence xj djr xi dirEquation (2). Taking maximum K, get xj djr maxiK (xi dir ) everyagent j N \ K. Taking arg maxiK (xi dir ),1<nXxi dir =i=1kXxi dir +i=1kXnXxi diri=k+1xi dir + (n k) xt dtr .i=1Considercased0>k = hdt ink D0 . minimum EF extension A0 Nffdemands d0 = dk , d0>k allocates xi dominant share every K (same A) allocatesexactlyxt dominant share every j N \ K. Thus, fraction resource r allocated A0Pk0i=1 xi dir + (n k) xt dtr > 1, implying minimum EF extension d>k infeasible.0conclude feasible EF extension d>k , required. (Proof Lemma 9)equivalent condition Lemma 9 provides us k linear constraintschecked determine whether allocation k agents EF-extensible. Using machinery, write small linear program (LP) begins allocation chosenprevious step (recall allocations irrevocable), gives agent k jump startenvy agents 1 k 1, uses water-filling allocate resources similarly592fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCESDYNAMIC DRF, subject constraint allocation stays EF-extensible. intuitionformalized via mechanism C AUTIOUS LP, given Algorithm 2.ALGORITHM 2: C AUTIOUS LPData: DemandsResult: Allocation Ak step kk 1;k n{xki }ki=1 Solution LP box below;Akir xki dir , k;k k+1endMaximize ksubjectxki k , kxki xk1, k 1min/dxkk maxik1 xk1rR ir krPkkki=1 xi dir + (n k) xt dtr 1, k, r Rmechanisms third LP constraint jump-starts agent k level envy earlieragents, fourth LP constraint derived Lemma 9. see mechanism satisfiesCDPO, observe step k EF-extensible allocation A0 Pareto dominatesallocation Ak returned mechanism, (by Lemma 9) A0 must also satisfy LP step k.However, shown allocation feasible region LP Pareto dominateAk . Indeed, allocation feasible region dominate Ak , could redistributeresources agent strictly better obtain feasible allocation valuek higher optimal solution. also easy see intuitively C AUTIOUS LPEF: initial allocation agent k achieves EF allocation k agents, water-fillingpreserves EF always allocates agents minimum dominant share. equallystraightforward show C AUTIOUS LP also satisfies SI. Establishing SP requires work,proof mainly modification proof Theorem 4. therefore able establishfollowing theorem, formalizes guarantees given C AUTIOUS LP.Theorem 11. C AUTIOUS LP satisfies SI, EF, CDPO, SP, implemented polynomialtime.Proof. proof along lines proof Theorem 4. now, assume LPfeasible step thus mechanism return allocation step (we showbelow). LP step k, letk1kE = max xi min dir /dkr .rRik1Intuitively, E k jump start agent k requires beginning step k envy freeallocations agents 1 k 1 step k 1.593fiK ASH , P ROCACCIA , & HAHProof CDPO: First show C AUTIOUS LP satisfies CDPO. Assume contradiction,step k {1, . . . , n}, alternative EF-extensible allocation A0 k presentagents Pareto dominates allocation Ak returned mechanism. Let x0i dominantshare agent A0 , k. Since A0 Pareto dominates Ak , x0i xki everyk. trivially implies A0 also satisfies first three constraints LP step k.Moreover, since A0 EF-extensible, also satisfies fourth constraint LP step kfourth constraint requires EF extension exist specific cases (in particular, requiresminimum EF extension thus EF extension n agents exist future demandvectors identical demand vector present agent). Thus, A0 feasible regionLP Pareto dominates optimal solution Ak . Now, taking back extra resourcesA0 allocates agents compared Ak shows fourth constraint tight Akvalue r (the assumption strictly positive demands crucial here). However, impliesallocation Ak , every xki correspondingly k increased sufficiently smallquantity still satisfying LP step k, contradicts optimality Ak . Thus,alternative EF-extensible allocation Pareto dominate allocation given mechanismstep, i.e., C AUTIOUS LP satisfies CDPO.Proof SI: Next, show C AUTIOUS LP satisfies SI. show induction stepk. base case k = 1, easy show setting x11 = 1/n k = 1/n satisfiesLP step 1; trivially satisfies first three constraints LP fourth constraint,observe11dir + (n 1) dir = dir 1, r R.nnTherefore, optimal solution, 1 1/n thus x11 1/n (in fact, equality holds).consider step k {2, . . . , n}. induction hypothesis, assume xti 1/nagents t, every step k 1. want show xki 1/n agents k.Consider two cases.1. E k 1/n. Observe xk11/n k 1 due induction hypothesis. Thus,using second third constraints LP step k, xki 1/n k.2. E k < 1/n. first show xki = xk1k 1, xkk = 1/n k = 1/nfeasible region LP step k. Note assignment trivially satisfies first threeconstraints LP.fourth constraint, fix r R. Define Tr = maxik1 xik1 dir . First, showPk1k1dir 1 (n k + 1) max(Tr , 1/n). see this, note {xik1 }k1i=1 xii=1satisfies LP step k 1 and, particular, fourth constraint LP. Therefore,k1Xxk1dir + (n k + 1) Tr 1 =i=1k1Xxk1dir 1 (n k + 1) Tr .i=1provek1Xxk1dir 1 (n k + 1) 1/n = (k 1)/n.i=1594(3)fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCESSuppose contradiction left hand side (k 1)/n. Then, pigeonhole principle, exists agent k 1 xk1dir 1/n, thusTr 1/n. already shownk1Xxk1dir 1 (n k + 1) Tr 1 (n k + 1) 1/n = (k 1)/n,i=1contradicting assumption; establishes (3). Thus,k1Xi=1xk11.dir 1 (n k + 1) max Tr ,nFinally, show fourth constraint LP, xkt dtr max(Tr , 1/n). see this,observe k 1, xkt dtr = xk1dtr Tr = k, xkt dtr = 1/ndkr 1/n.Thus, fourth constraint LP satisfied every k every r R.established C AUTIOUS LP satisfies SI. next goal prove mechanism also satisfies EF SP. proof Theorem 4, first establish several useful lemmasallocations returned C AUTIOUS LP. proof below, k xki refer optimalsolution LP step k.begin following lemma (similar Lemma 5), essentially showsagent allocated resources step k using water-filling (in addition jump-start E kagent k), agents dominant share step would minimum among presentagents.Lemma 12. every step k {1, . . . , n}, holds xki = max(M k , xk1) agentskkkk 1, xk = max(M , E ).Proof. Consider step k {1, . . . , n}. first three constraints LP, evident)xki k k, xki xk1k 1 xkk E k . Thus, xki max(M k , xk1k 1 xkk max(M k , E k ).Suppose contradiction strict inequality holds agent k. xkireduced sufficiently small > 0 without violating constraints. makes thirdconstraint LP loose least dir , every resource r R. Consequently, valuesxkj j 6= k increased sufficiently small > 0 without violatingthird constraint LP. Finally, (and correspondingly ) chosen small enoughxki k violated. follows value k increased, contradictingoptimality k . (Proof Lemma 12)Next, formulate equivalent Lemma 6 two separate lemmas. First showagent greater equal dominant share another agent step (where present),order preserved future steps. Next show step k, dominant sharesagents 1 k monotonically non-increasing time arrival, except agentsreceived resources apart jump-start.Lemma 13. agents i, j N step k max(i, j) (i.e., agents presentstep k), xki xkj implies xti xtj k.595fiK ASH , P ROCACCIA , & HAHProof. Fix two agents i, j N step k max(i, j) xki xkj . use inductiont. result trivially holds = k. Consider > k assume result holds step 1.t1Then, since > k max(i, j) know xti = max(xt1, ) max(xj , ) = xj ,first last transitions follow Lemma 12 second transition followsinduction hypothesis. (Proof Lemma 13)Lemma 14. agents i, j N < j step k j, either i)xki xkj ii) xkj = xjj = E j .Proof. Fix two agents i, j N < j step k j. Note xkj xjjE j , first inequality due irrevocability resources last inequality dueLemma 12. xkj = E j , lemma trivially holds. Assume xkj > E j . Consider first stepxtj > E j (thus j k). = j, xjj > E j . > j, xtj > xt1jj definition t. case, Lemma 12 implies xt = xt . Thussince xt1=Ejjxtj xti Lemma 13 implies xkj xki . (Proof Lemma 14)consider equivalent Lemma 7 proof Theorem 4, observetwo cases. agent j greater dominant share agent step, either jarrived j allocated resources since arrived (as previously),j arrived allocated resources apart jump-start.Lemma 15. agents i, j N step k max(i, j) (i.e., agents present),xkj > xki implies either i) j < xkj = xji1 , ii) j > xkj = xjj = E j .Proof. Fix two agents i, j N step k max(i, j) xkj > xki . Notej > Lemma 14 implies xkj = xjj = E j result holds trivially. assume j < i.Suppose contradiction xkj > xji1 (it cannot smaller allocations irrevocable). exists step {i, . . . , k} xtj > xt1j . Therefore, Lemma 12 implieskxj = xi . Using Lemma 13 shows xj xki , contradictionassumption xkj > xki . Thus xkj = xi1j , required. (Proof Lemma 15)Finally, establish additional lemma helpful proving SP. agents i, jj > i, jump-start E j agent j requires allocating agent j greater dominant shareagent step j 1, clearly jump-start must due agent j envyingagent l 6= i, l must greater dominant share step j 1. usingLemma 14 extending argument, eventually trace back agent < i.show find < jump-start original agent j actually dueagent j envying agent t.Lemma 16. agents i, j N j > i, E j > xji implies E j = xj1minrR dtr /djr , agent < i.Proof. Fix agent N . use induction j {i + 1, . . . , n}. First, prove severalimplications hold agent j > i. Recall E j = maxp<j (xj1minrR dpr /djr ).p596fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCESThus, E j = xlj1 minrR dlr /djr agent l < j. followdefinition take l < i. Observexlj1 xlj1 min dtr /djr = E j > xji xj1,rR(4)first transition holds since minrR dtr /djr 1 (consider dominant resourceagent j), third transition assumption lemma last transition holds sinceallocations irrevocable.three cases. l < i, done. Further, l 6= since Equation (4) showsxj1> xj1. assume l > i. Note case cannot appear base caselj = + 1 since l < j. Therefore, argument given already shows lemma holdsbase case j = + 1. induction hypothesis, assume lemma holdsagent l < j. since l > xj1> xj1= xll = E l, Lemma 15 implies xj1llj1j1lllthus E > xixi xixi l < j allocations irrevocable. Dueinduction hypothesis, exists < E l = xl1minrR dtr /dlr . proveE j = xtj1 minrR dtr /djr . Indeed,min dlr /djrE j = xj1lrRl= E min dlr /djrrR=xl1min dtr /dlr min dlr /djrxl1min dtr /djrxj1min dtr /djr E j .rRrRrRrRHere, fourth transition true r0 R,dtr0dtr0 dlr0dtrdlr=minmin.000rR dlr rR djrdjrdlr djrTaking minimum r0 R, get minrR dtr /djr minrR dtr /dlr minrR dlr /djr .last transition holds due definition E j . trivial see must equalityevery step, E j = xj1minrR dtr /djr < i, required. (Proof Lemma 16)Proof LP Feasibility EF: use inductive argument simultaneously showLP C AUTIOUS LP feasible every step C AUTIOUS LP satisfies EF. Considerfollowing induction hypothesis: LP step feasible allocation returnedmechanism step EF. base case = 1, LP trivially feasible allocation A1also trivially EF. Assume hypothesis holds = k 1 step k {2, . . . , n}.want show hypothesis holds step k.feasibility, show allocation given xki = xk1k 1 xkk = E kkalong = 0 satisfies LP step k. Clearly, satisfies first three constraints LP.see satisfies fourth constraint, note Ak1 EF allocation due inductionhypothesis. Moreover, satisfies LP step k 1, particular, fourth constraintLP. Hence Lemma 9 implies Ak1 must EF-extensible allocation. Let dk denote597fiK ASH , P ROCACCIA , & HAHdemand reported agent k step k let d>k Dnk . EF extension Ak1n agents future demands (dk , d>k ) EF extension n agents futuredemands d>k . Since holds d>k Dnk , EF-extensible hence satisfiesfourth constraint LP. conclude LP feasible step k.want show allocation Ak EF allocation. Intuitively, seemechanism starts EF (it minimum EF extension), uses waterfilling allocate resources way preserves EF. Formally, note dominant sharesallocated agents Ak given Lemma 12. Take two agents i, j k. want showagent envy agent j step k. Denote dominant share agent l xl ,i.e., xl = xk1l k 1 xk = E k . holdsldjrdjrxki = max xi , k max xj min, k max xj , k minrR dirrR dir= xkj min djr /dir ,rRfirst last transitions follow Lemma 12, second transition holds sinceallocation EF, third transition holds since quantity minrR djr /dir 1.Thus, Ak EF. induction, holds LP C AUTIOUS LP feasible every stepC AUTIOUS LP EF.Proof SP: last task prove C AUTIOUS LP SP. Suppose contradictionagent N report untruthful demand vector d0i agent strictly betterleast one step. Let k first step. Denote xtj dominant share agent j stepmanipulation (for agent i, share dominant resource untruthful demandvector) and, similarly, denote value optimal solution LP stepmanipulation.Lemma 17. xkj xkj every agent j k.Proof. Fix agent j k. provide case case analysis show lemma holdscase.1. xkj xki . case,xkj xki < xki = k xkj .second transition holds xki xki agent could bettershare dominant resource true demand vector receives manipulationwould received without manipulation. justify third transition, noteagent must allocated resources step k manipulation. k = i,note since E depends allocation step 1 affected duemanipulation agent i, E = E xii < xii Lemma 12 implies,xii = . k > xki 6= k , Lemma 12 implies xki = xk1k ) > u (Ak ) u (Ak1 ), Ak allocation agent stepui (Ak1)=u(k manipulation. is, agent would better manipulation stepk 1, contradiction since k first step. last transition holdsxkj satisfies first constraint LP step k manipulation.598fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCES2. xkj > xki . this, three sub-cases.(a) j < i. xkj = xi1= xi1xkj , first transition followsjjdue Lemma 15, second transition holds manipulation agentaffect allocations step 1, third transition follows since allocationsirrevocable.(b) j = i. cannot happen since assumed xkj > xki case.(c) j > i. Since xkj > xki , Lemma 15 implies xkj = xjj = E j , E j > xki .using Lemma 16, E j = xj1minrR dtr /djr < i. Then, xj1j1j1jkxt minrR dtr /djr = E > xi xi , first transition follows sinceminrR dtr /djr 1 last transition follows since allocations irrevocable. Lemma 15 implies xj1= xti1 . Putting pieces together,xkj = E j = xtj1 min dtr /djr = xti1 min dtr /djr = xj1min dtr /djrrRxj1jmin dtr /djr ErRxjjrRxkj ,rRfifth transition follows since manipulation agent changeallocation step 1, sixth transition follows due definition E j (whichvalue E j manipulation), seventh transition follows due thirdconstraint LP step j manipulation, last transition follows sinceallocations irrevocable.Thus, conclude xkj xkj agents j k. (Proof Lemma 17)Now, optimal solution LP step k without manipulation (i.e., Ak ), fourthconstraint must tight k r R (otherwise xkj every j k kincreased, contradicting optimality k ). Thus,kXxkj djr + (n k) xkt dtr = 1.j=1consider fourth constraint LP step k manipulation valuesr. simplicity notation, let d0jr = djr j 6= i. Then,kXj=1xkj d0jr + (n k) xkt d0tr >kXxkj djr + (n k) xkt dtr = 1.j=1justify inequality, note xki d0ir > xki dir Equation (1) (as agent strictly betteroff), j k j 6= i, xkj d0jr = xkj djr xkj djr Lemma 17. However,shows allocation step k manipulation violates fourth constraint LP,impossible. Hence, successful manipulation impossible, is, C AUTIOUS LP SP.Finally, note every step LP O(n) variables O(n m) constraints,n steps. Hence, mechanism implemented polynomial time. (ProofTheorem 11)599fiK ASH , P ROCACCIA , & HAH6. Experimental Resultspresented two potentially useful mechanisms, DYNAMIC DRF C AUTIOUS LP,theoretical guarantees. next goal analyze performance mechanismsreal data, two natural objectives: sum dominant shares (the maxsum objective)minimum dominant share (the maxmin objective) agents present system.1compare objective function values achieved two mechanisms certain lowerupper bounds. Since mechanisms satisfy SI, maxsum maxmin objective valuesprovably lower bounded k/n 1/n, respectively, step k.upper bounds, consider omniscient (hence unrealistic) mechanisms maximizeobjectives offline setting mechanisms complete knowledge future demands. mechanisms need guarantee EF extension real future demands ratherpossible future demands. comparison C AUTIOUS LP offline mechanisms demonstrates loss C AUTIOUS LP (an online mechanism) suffers due absenceinformation regarding future demands, is, due cautiousness. DYNAMIC DRFrequired EF extension, offline mechanisms theoretical upper boundsDYNAMIC DRF, experiments show provide upper bounds practice.data use traces real workloads Google compute cell, 7 hour period2011 (Hellerstein, 2010). workload consists tasks, task ran single machine,consumed memory one cores; demands fit model two resources.various values n, sampled n random positive demand vectors traces analyzedvalue two objective functions DYNAMIC DRF C AUTIOUS LP alongcorresponding lower upper bounds. averaged 1000 simulations obtain datapoints.Figures 4(a) 4(b) show maxsum values achieved different mechanisms, 20agents 100 agents respectively. performance two mechanisms nearly identical.Figures 4(c) 4(d) show maxmin values achieved 20 agents 100 agents, respectively. Observe DYNAMIC DRF performs better C AUTIOUS LP lower values k,performs worse higher values k. Intuitively, DYNAMIC DRF allocates resourcesearly stages satisfy DPO C AUTIOUS LP cautiously waits. results superiorperformance DYNAMIC DRF initial steps fewer resources available thus lesserflexibility optimization later steps, resulting inferior performance near end. contrast,C AUTIOUS LP later able make loss early steps. Encouragingly, last stepC AUTIOUS LP achieves near optimal maxmin value. reason, unlike DYNAMIC DRFmaxmin objective value C AUTIOUS LP monotonically increases k increases experiments (although easy show always case).7. Discussionpresented new model resource allocation multiple resources dynamic environments that, believe, spark study dynamic fair division generally. modeldirectly applicable data centers, clusters, cloud computing, allocation multipleresources key issue, significantly extends previously studied static models. said,1. cardinal notion utility dominant share agent utility, sum dominant sharesutilitarian social welfare minimum dominant share egalitarian social welfare.600fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCESUpper BoundCautiousLPDynamicDRFLower Bound2.52Upper BoundCautiousLPDynamicDRFLower Bound2.251.51.510.750.500051015200(a) Maxsum 20 agents0.0955075100(b) Maxsum 100 agents0.0175Upper BoundCautiousLPDynamicDRFLower Bound0.0825Upper BoundCautiousLPDynamicDRFLower Bound0.0150.0650.01250.050.01051015200(c) Maxmin 20 agents255075100(d) Maxmin 100 agentsFigure 4: maxsum maxmin objectives function time step k, n = 20n = 100.model also gives rise technical challenges need tackled capture realisticsettings.First, model assumes positive demands, is, agent requires every resource. seepositive demands assumption plays role, recall achieving EF DPO impossible.established dropping DPO leads trivial mechanism E QUAL PLIT, satisfiesremaining three properties; also true possibly zero demands. dropped EF,observed trivial mechanism DYNAMIC ICTATORSHIP satisfies SI, DPO SP,subsequently suggested improved mechanism DYNAMIC DRF satisfies DEF additionSI, DPO SP. Surprisingly though, shown neither DYNAMIC ICTATORSHIP (seeExample 2) DYNAMIC DRF SP possibly zero demands.2 fact, despite significanteffort, unable settle question existence mechanism satisfies SI, DPOSP possibly zero demands.Second, analysis restricted setting divisible tasks, agents value fractionalquantities tasks. Parkes et. al. (2014) consider indivisible tasks setting,2. possibly zero demands, modify DYNAMIC ICTATORSHIP DYNAMIC DRF continue allocatingeven resources become saturated satisfy DPO.601fiK ASH , P ROCACCIA , & HAHintegral quantities agents task executed, albeit static environment. showneven forward EF weakest EF relaxations considered paper impossibleachieve along DPO indivisible tasks. remains open determine relaxationsEF feasible dynamic resource allocation settings indivisible tasks. restrictattention Leontief utilities, noted desiderata propose well-defineddynamic setting utility function.Third, model fair division extends classical model introducing dynamics,results directly inform design practical mechanisms, make assumptionagents arrive time depart. reality, agents may arrive depart multiple times,preferences may also change time (note changing preferences modeleddeparture simultaneous re-arrival different demand vector). Departures withoutre-arrivals easy handle; one allocate resources become free similar wayallocations entitlements, e.g., using DYNAMIC DRF (this scheme would clearly satisfy SI, DEF,DPO, would interesting check whether also strategyproof). However, departuresre-arrivals immediately lead daunting impossibilities. Note though mechanismsdesigned static settings performed well realistic (fully dynamic) environments (Ghodsiet al., 2011), quite likely mechanisms provide theoretical guaranteesrestricted dynamic settings would yield even better performance reality.Acknowledgementspreliminary version paper appeared AAMAS13. Procaccia Shah partiallysupported NSF grant NSF CCF-1215883, gift CMU-MSR CenterComputational Thinking.ReferencesBrams, S. J., & Taylor, A. D. (1996). Fair Division: Cake-Cutting Dispute Resolution.Cambridge University Press.Chen, Y., Lai, J. K., Parkes, D. C., & Procaccia, A. D. (2010). Truth, justice, cake cutting.Proceedings 24th AAAI Conference Artificial Intelligence (AAAI), pp. 756761.Chevaleyre, Y., Dunne, P. E., Endriss, U., Lang, J., Lematre, M., Maudet, N., Padget, J., Phelps,S., Rodrguez-Aguilar, J. A., & Sousa, P. (2006). Issues multiagent resource allocation.Informatica, 30, 331.Chevaleyre, Y., Endriss, U., Estivie, S., & Maudet, N. (2007). Reaching envy-free states distributed negotiation settings. Proceedings 20th International Joint ConferenceArtificial Intelligence (IJCAI), pp. 12391244.Demers, A., Keshav, S., & Shenker, S. (1989). Analysis simulation fair queueing algorithm. Proceedings ACM Symposium Communications Architectures & Protocols(SIGCOMM), pp. 112.Dolev, D., Feitelson, D. G., Halpern, J. Y., Kupferman, R., & Linial, N. (2012). justified complaints: fair sharing multiple resources. Proceedings 3rd Innovations Theoretical Computer Science Conference (ITCS), pp. 6875.602fiN AGENT L EFT B EHIND : DYNAMIC FAIR IVISION ULTIPLE R ESOURCESGhodsi, A., Sekar, V., Zaharia, M., & Stoica, I. (2012). Multi-resource fair queueing packetprocessing. Proceedings ACM Symposium Communications Architectures &Protocols (SIGCOMM), pp. 112.Ghodsi, A., Zaharia, M., Hindman, B., Konwinski, A., Shenker, S., & Stoica, I. (2011). Dominant Resource Fairness: Fair allocation multiple resource types. Proceedings 8thUSENIX Conference Networked Systems Design Implementation (NSDI), pp. 2437.Ghodsi, A., Zaharia, M., Shenker, S., & Stoica, I. (2013). Choosy: Max-min fair sharing datacenter jobs constraints. Proceedings 8th ACM European Conference ComputerSystems (EUROSYS), pp. 365378.Gutman, A., & Nisan, N. (2012). Fair allocation without trade. Proceedings 11th International Joint Conference Autonomous Agents Multi-Agent Systems (AAMAS), pp.719728.Hellerstein, J. L. (2010). Google cluster data. Google research blog. Posted http://googleresearch.blogspot.com/2010/01/google-cluster-data.html.Joe-Wong, C., Sen, S., Lan, T., & Chiang, M. (2012). Multi-resource allocation: Fairness-efficiencytradeoffs unifying framework. Proceedings 31st Annual IEEE InternationalConference Computer Communications (INFOCOM), pp. 12061214.Li, J., & Xue, J. (2013). Egalitarian division Leontief preferences. Economic Theory, 54(3),597622.Moulin, H. (2003). Fair Division Collective Welfare. MIT Press.Moulin, H., & Stong, R. (2002). Fair queuing probabilistic allocation methods. Mathematics Operations Research, 27(1), 130.Parkes, D. C., Procaccia, A. D., & Shah, N. (2014). Beyond Dominant Resource Fairness: Extensions, limitations, indivisibilities. ACM Transactions Economics Computation.Forthcoming.Procaccia, A. D. (2009). Thou shalt covet thy neighbors cake. Proceedings 21st International Joint Conference Artificial Intelligence (IJCAI), pp. 239244.Walsh, T. (2011). Online cake cutting. Proceedings 3rd International ConferenceAlgorithmic Decision Theory (ADT), pp. 292305.Zahedi, S. M., & Lee, B. C. (2014). REF: Resource elasticity fairness sharing incentivesmultiprocessors. Proceedings 19th International Conference ArchitecturalSupport Programming Languages Operating Systems (ASPLOS), pp. 145160.603fiJournal Artificial Intelligence Research 51 (2014) 829-866Submitted 06/14; published 12/14Exact Double-Oracle Algorithm Zero-SumExtensive-Form Games Imperfect InformationBranislav Bosanskybranislav.bosansky@agents.fel.cvut.czAgent Technology CenterDepartment Computer ScienceFaculty Electrical EngineeringCzech Technical University PragueChristopher Kiekintveldcdkiekintveld@utep.eduComputer Science DepartmentUniversity Texas El Paso, USAViliam LisyMichal Pechoucekviliam.lisy@agents.fel.cvut.czmichal.pechoucek@agents.fel.cvut.czAgent Technology CenterDepartment Computer ScienceFaculty Electrical EngineeringCzech Technical University PragueAbstractDeveloping scalable solution algorithms one central problems computationalgame theory. present iterative algorithm computing exact Nash equilibriumtwo-player zero-sum extensive-form games imperfect information. approachcombines two key elements: (1) compact sequence-form representation extensiveform games (2) algorithmic framework double-oracle methods. main ideaalgorithm restrict game allowing players play selected sequencesavailable actions. solving restricted game, new sequences added findingbest responses current solution using fast algorithms.experimentally evaluate algorithm set games inspired patrollingscenarios, board, card games. results show significant runtime improvementsgames admitting equilibrium small support, substantial improvement memory use even games large support. improvement memory use particularlyimportant allows algorithm solve much larger game instances existinglinear programming methods.main contributions include (1) generic sequence-form double-oracle algorithmsolving zero-sum extensive-form games; (2) fast methods maintaining valid restrictedgame model adding new sequences; (3) search algorithm pruning methodscomputing best-response sequences; (4) theoretical guarantees convergencealgorithm Nash equilibrium; (5) experimental analysis algorithm severalgames, including approximate version algorithm.1. IntroductionGame theory widely used methodology analyzing multi-agent systems applyingformal mathematical models solution concepts. One focus computational game theory development scalable algorithms reasoning large games.c2014AI Access Foundation. rights reserved.fiBosansky, Kiekintveld, Lisy, & Pechoucekneed continued algorithmic advances driven growing number applicationsgame theory require solving large game instances. example, several decisionsupport systems recently deployed homeland security domains recommendpolicies based game-theoretic models placing checkpoints airports (Pita, Jain,Western, Portway, Tambe, Ordonez, Kraus, & Parachuri, 2008), scheduling Federal AirMarshals (Tsai, Rathi, Kiekintveld, Ordonez, & Tambe, 2009), patrolling ports (Shieh,An, Yang, Tambe, Baldwin, Direnzo, Meyer, Baldwin, Maule, & Meyer, 2012). capabilities systems based large amount research fast algorithmssecurity games (Tambe, 2011). Another notable example algorithmic progressled game-theoretic Poker agents competitive highly skilled humanopponents (e.g., see Zinkevich, Bowling, & Burch, 2007; Sandholm, 2010).focus developing new algorithms important general class gamesincludes security games Poker, well many familiar games. precisely,study two-player zero-sum extensive-form games (EFGs) imperfect information.class games captures sequential interactions two strictly competitive playerssituations make decisions uncertainty. Uncertainty caused eitherstochastic environment opponent actions directlyobservable. consider general models sequential interactions uncertainty,many fast algorithms developed Poker security domainsrely specific game structure.propose new class algorithms finding exact (or approximate) Nash equilibrium solutions class EFGs imperfect information. leading exactalgorithm literature uses compact sequence-form representation linear programming optimization techniques solve games type (Koller, Megiddo, & vonStengel, 1996; von Stengel, 1996). approach exploits compact representation, improve solution methods adopting algorithmic framework baseddecompositions known computational game theory literature oracle algorithms(McMahan, Gordon, & Blum, 2003). Oracle algorithms related methods constraint/column generation used solving large-scale optimization problems (Dantzig &Wolfe, 1960; Barnhart, Johnson, Nemhauser, Savelsbergh, & Vance, 1998) exploit twocharacteristics commonly found games. First, many cases finding solutiongame requires using small fraction possible strategies, necessaryenumerate strategies find solution (Wilson, 1972; Koller & Megiddo, 1996).Second, finding best response specific opponent strategy game computationallymuch less expensive solving equilibrium. addition, best response algorithmsoften make use domain-specific knowledge heuristics speed calculationseven further.sequence-form double-oracle algorithm integrates decomposition ideas oraclealgorithms compact sequence-form representation EFGs imperfect information. results iterative algorithm always need generatecomplete linear program game find Nash equilibrium solution. main ideaalgorithm create restricted game players choose limitedspace possible strategies (represented sequences actions). algorithm solvesrestricted game uses fast best-response algorithm find strategiesoriginal unrestricted game perform well current solution restricted830fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Informationgame. strategies added restricted game process iteratesbest response found improve solution. case, current solutionequilibrium original game. Typically, solution found adding smallfraction strategies restricted game.begin presenting related work, technical background, notation.describe main algorithm three parts: (1) methods creating, solving, expanding valid restricted game, (2) algorithm finding best-response strategiesadded restricted game, (3) variants main loop controlling iterativeprocess solving restricted games adding new strategies. present formal analysisprove algorithm converges Nash equilibrium original game. Finally, provide experimental evaluation runtime performance convergencebehavior algorithm several realistic games different characteristics includingborder patrolling scenario, Phantom Tic-Tac-Toe, simplified variant Poker.compare results state-of-the-art algorithms finding exact approximate solutions: linear programming using sequence form, Counterfactual RegretMinimization (CFR, Zinkevich, Johanson, Bowling, & Piccione, 2008; Lanctot, 2013).experimental results confirm algorithm requires fraction possible sequences solve game practice significantly reduces memory requirementssolving large games. advances state art allows us exactly solvemuch larger games compared existing algorithms. Moreover, games admittingequilibrium small support (i.e., sequences non-zero probabilityequilibrium), algorithm also achieves significant improvements computation timefinds equilibrium iterations. result hold without usingdomain-specific knowledge, also show incorporating domain-specific heuristicsbounds algorithm straightforward way lead even significantperformance improvements. Analysis convergence rate shows approximativebounds value game either similar bit worse early stagescompared CFR. However, convergence behavior CFR algorithm longtail algorithm always finds exact solution much faster CFR.2. Related WorkSolving imperfect-information EFGs computationally challenging task, primarily dueuncertainty actions opponent and/or stochastic environment.leading exact algorithm (Koller et al., 1996; von Stengel, 1996) based formulatingproblem finding optimal strategy play linear program. algorithm exploitscompact representation strategies sequences individual actions (called sequenceform) results linear program linear size size game tree. However,approach limited applicability since game tree grows exponentiallynumber sequential actions game. common practice overcoming limitedscalability sequence-form linear programming use approximation method.best known approximative algorithms include counterfactual regret minimization (CFR,Zinkevich et al., 2008), improved versions CFR sampling methods (Lanctot, Waugh,Zinkevich, & Bowling, 2009; Gibson, Lanctot, Burch, Szafron, & Bowling, 2012); NesterovsExcessive Gap Technique (EGT, Hoda, Gilpin, Pena, & Sandholm, 2010); variants831fiBosansky, Kiekintveld, Lisy, & PechoucekMonte Carlo Tree Search (MCTS) algorithms applied imperfect-information games (e.g.,see Ponsen, de Jong, & Lanctot, 2011).family counterfactual regret minimization algorithms based learning methods informally described follows. algorithm repeatedly traversesgame tree learns strategy play applying no-regret learning rule minimizes specific variant regret (counterfactual regret) information set.no-regret learning converges optimal strategy information set. overallregret bounded sum regret information set; hence, strategywhole converges Nash equilibrium. main benefits approach includesimplicity robustness, adapted generic games (e.g., see Lanctot,Gibson, Burch, Zinkevich, & Bowling, 2012, CFR applied games imperfectrecall). However, algorithm operates complete game tree therefore requiresconvergence information sets, slow large games one desiressolution small error.Another popular method Excessive Gap Technique exploits convex propertiessequence-form representation uses recent mathematical results finding extremepoints smooth functions (see Hoda et al., 2010, details). main idea approximate problem finding pair equilibrium strategies two smoothed functionsguiding find approximate solution. Although approach achieves fasterconvergence comparison CFR, algorithm less robust (it known whethersimilar approach used general classes games) less used practice.Like CFR, EGT also operates complete strategy space sequences.Monte Carlo Tree Search (MCTS) another family methods shown promisesolving large games, particular perfect information board games Go (e.g.,Lee et al., 2009). CFR EGT algorithms guaranteed find -Nashequilibrium, convergence equilibrium solution formally shownvariants MCTS imperfect-information games. contrary, commonversion MCTS based Upper Confidence Bounds (UCB) selection functionconverge incorrect solutions even simultaneous-move games (Shafiei, Sturtevant, &Schaeffer, 2009) simplest class imperfect-information EFGs. MCTS algorithms therefore (in general) guarantee finding (approximate) optimal solutionimperfect-information games. One exception recent proof convergence MCTScertain selection methods simultaneous-move games (Lisy, Kovarik, Lanctot, &Bosansky, 2013). Still, using MCTS sometimes reasonable choice since producegood strategies practice (Ponsen et al., 2011).Contrary existing approximative approaches, algorithm aims find exact solution without explicitly considering strategy complete game tree.work combines compact sequence-form representation double-oracle algorithmic framework. Previous work double-oracle framework focused primarilyapplications normal-form games, restricted game expanded adding purebest-response strategies iteration. One first examples solving games usingdouble-oracle principle McMahan et al. (2003). introduced doubleoracle algorithm, proved convergence Nash equilibrium, experimentally verifiedalgorithm achieves computation time improvements search gameevader trying cross environment without detected sensors placed832fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Informationopponent. double-oracle algorithm reduced computation time several hourstens seconds allowed solve much larger instances game. Similar successdomain-specific double-oracle methods demonstrated variety different domains inspired pursuit-evasion games (Halvorson, Conitzer, & Parr, 2009)security games played graph (Jain, Korzhyk, Vanek, Conitzer, Tambe, & Pechoucek,2011; Letchford & Vorobeychik, 2013; Jain, Conitzer, & Tambe, 2013).works tried apply iterative framework oracle algorithmsEFGs, primarily using pure mixed strategies EFGs. first work exploitediterative principle predecessor sequence-form linear-program formulation (Koller& Megiddo, 1992). algorithm, authors use representation similar sequence form single player, strategies opponent iterativelyadded constraints linear program (there exponential number constraintsformulation). approach seen specific variant oracle algorithms, strategy space expanded gradually single player. algorithmgeneralization work, since algorithm uses sequence-form representationplayers also incrementally expands strategy space players.recent work done McMahan thesis (McMahan, 2006) followup work (McMahan & Gordon, 2007). works authors investigated extensiondouble-oracle algorithm normal-form games extensive-form case.double-oracle algorithm EFGs operates similarly normal-form variantuses pure mixed strategies defined EFGs. main disadvantage approachbasic version still requires large amount memory since pure strategyEFG large (one action needs specified information set),exponential number possible pure strategies. overcome disadvantage,authors propose modification double-oracle algorithm keeps numberstrategies restricted game bounded. algorithm removes restricted gamestrategies least used current solution restricted game.order guarantee convergence, algorithm adds iteration restrictedgame mixed strategy representing mean removed strategies; convergenceguaranteed similarly fictitious play (see McMahan & Gordon, 2007, details).Bounding size restricted game results low memory requirements. However,algorithm converges extremely slowly take long time (several hourssmall game) algorithm achieve small error (see experimental evaluationMcMahan, 2006; McMahan & Gordon, 2007).similar concept using pure strategies EFGs used iterative algorithmdesigned Poker work Zinkevich et al. (2007). algorithm workexpands restricted game strategies found generalized best response insteadusing pure best response strategies. Generalized best response Nash equilibriumpartially restricted game player computing best response use purestrategies original unrestricted game, opponent restricted usestrategies restricted game. However, main disadvantages using puremixed strategies EFGs still present result large memory requirementsexponential number iterations.contrast, algorithm directly uses compact sequence-form representationEFGs uses sequences building blocks (i.e., restricted game expanded833fiBosansky, Kiekintveld, Lisy, & Pechoucekallowing new sequences played next iteration). Using sequencessequence form solving restricted game reduces size restricted gamenumber iterations, however, also introduces new challenges constructingmaintaining restricted game, ensuring convergence Nash equilibrium,must solve algorithm converge correct solution.3. Technical Backgroundbegin presenting standard game-theoretic model extensive-form games, followed discussion common solution concepts algorithms computing solutions. present sequence-form representation state-of-theart linear program computing solutions using representation. Finally, describeoracle algorithms used solving normal-form games. summarycommon notation provided Table 1 quick reference.3.1 Extensive-Form GamesExtensive-form games (EFGs) model sequential interactions players game.Games extensive form visually represented game trees (e.g., see Figure 2).Nodes game tree represent states game; state game correspondssequence moves executed players game. node assigned playeracts game state associated node. edge game treenode corresponds action performed player acts node.Extensive-form games model limited observations players grouping nodesinformation sets, given player cannot distinguish nodes belonginformation set player choosing action. model also representsuncertainty environment stochastic events using special Nature player.Formally, two-player EFG defined tuple G = (N, H, Z, A, p, u, C, I): N settwo players N = {1, 2}. use refer one two players (either 1 2),refer opponent i. H denotes finite set nodes game tree. nodecorresponds unique history actions taken players Nature rootgame; hence, use terms history node interchangeably. denote Z Hset terminal nodes game. denotes set actions overloadnotation use A(h) represent set actions available player actingnode h H. specify ha = h0 H node h0 reached node h executingaction A(h). say h prefix h0 denote h v h0 . terminalnode z Z define utility function player (ui : Z R). study zero-sumgames, ui (z) = ui (z) holds z Z.function p : H N {c} assigns node player takes actionnode, c means Nature player selects action node based fixedprobability distribution known players. use function C : H [0, 1] denoteprobability reaching node h due Nature (i.e., assuming players playrequired actions reach node h). value C(h) product probabilitiesassigned actions taken Nature player history h. Imperfect observationplayer modeled via information sets Ii form partition nodes assignedplayer {h H : p(h) = i}. Every information set contains least one node834fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Informationnode belongs exactly one information set. Nodes information set playerindistinguishable player. nodes h single information set Ii Iiset possible actions A(h). Action A(h) uniquely identifies information setIi cannot exist node h0 H belong information setIi allowed played (i.e., A(h0 )). Therefore overload notationuse A(Ii ) denote set actions defined node h information set.assume perfect recall, means players perfectly remember actionsinformation gained course game. result, nodesinformation set Ii history actions player i.3.2 Nash Equilibrium Extensive-Form GamesSolving game requires finding strategy profile (i.e., one strategy player)satisfies conditions defined specific solution concept. Nash equilibrium (NE)best known solution concept game theory describes behavior playerscertain assumptions rationality. Nash equilibrium, every player playsbest response strategies players. Let set pure strategiesplayer i. EFGs, pure strategy assignment exactly one action playedinformation set. mixed strategy probability distribution set purestrategies player. denote set mixed strategies player i.pair strategies = (1 , 2 ) use ui () = ui (i , ) expected outcomegame player players follow strategies . best response playeropponents strategy strategy iBR , ui (iBR , ) ui (i0 , )strategies i0 . strategy profile = (1 , 2 ) NE playerholds best response . game multiple NEs; zero-sumsetting, equilibria value (i.e., expected utility every playersame). called value game, denoted V . problem findingNE zero-sum game polynomial computational complexity size game.NE solution concept somewhat weak extensive-form games. Nash equilibriumrequires players act rationally. However, irrational strategies selectedparts game tree reachable players follow NEstrategies (these parts said equilibrium path). reason NEexpect part game played therefore sufficiently restrictstrategies information sets. overcome drawbacks, number refinementsNE introduced imposing restrictions intention describingsensible strategies. Examples include subgame-perfect equilibrium (Selten, 1965) usedperfect-information EFGs. subgame-perfect equilibrium forces strategy profileNash equilibrium sub-game (i.e., sub-tree rooted node h)original game. Unfortunately, sub-games particularly useful imperfectinformation EFGs; hence, refinements include strategic-from perfect equilibrium(Selten, 1975), sequential equilibrium (Kreps & Wilson, 1982), quasi-perfect equilibrium(van Damme, 1984; Miltersen & Srensen, 2010). first refinement avoids using weaklydominated strategies equilibrium strategies two-player games (van Damme, 1991,p. 29) also known undominated equilibrium. Sequential equilibrium triesexploit mistakes opponent using notion beliefs consistent835fiBosansky, Kiekintveld, Lisy, & Pechoucekstrategy opponent even information sets equilibrium path. mainintuitions behind first two refinements combined quasi-perfect equilibrium.Even though solution described NE always prescribe rational strategiesequilibrium path, still valuable compute exact NE large extensive-formgames several reasons. focus zero-sum games, NE strategy guaranteesvalue game even equilibrium path. words, strategyequilibrium path optimally exploit mistakes opponent, stillguarantees outcome least value gained following equilibrium path. Moreover,refined equilibrium still NE calculating value game often startingpoint many algorithms compute refinements example usedcomputing undominated equilibrium (e.g., see Ganzfried & Sandholm, 2013; Cermak,Bosansky, & Lisy, 2014) normal-form proper equilibrium (Miltersen & Srensen, 2008).3.3 Sequence-Form Linear ProgramExtensive-form games perfect recall compactly represented using sequenceform (Koller et al., 1996; von Stengel, 1996). sequence ordered list actions takensingle player history h. number actions (i.e., length sequence )denoted |i | empty sequence (i.e., sequence actions) denoted .set possible sequences player denoted set sequencesplayers = 1 2 . sequence extended single action takenplayer i, denoted = i0 (we use v i0 denote prefix i0 ). gamesperfect recall, nodes information set Ii share sequence actionsplayer use seqi (Ii ) denote sequence. overload notation useseqi (h) denoteleading node h, seqi (H 0 ) ,sequence of0 actions player00seqi (H ) = h0 H 0 seqi (h ) H H. Since action uniquely identifiesinformation set Ii nodes information set share history actionsplayer i, sequence uniquely identifies information set. use function infi (i0 )denote information set last action sequence i0 taken.empty sequence, function infi () information set root node.Finally, define auxiliary payoff function gi : R extends utilityfunction nodes game tree. payoff function gi represents expectedutility nodes reachable sequentially executing actions specified pairsequences :Xgi (i , ) =ui (h) C(h)(1)hZ : jN j =seqj (h)value payoff function defined 0 leaf reachable sequentially executing actions sequences either actions pair sequencesexecuted inner node (h H \ Z) reached, sequential execution actions node h reached, current action executedsequence (h) defined (i.e.,/ A(h)). Formally define pair sequencescompatible exists node h H sequence every player equalsseqi (h).compute Nash equilibrium two-player zero-sum extensive-form gameusing linear program (LP) polynomial size size game tree using836fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Informationsequence form (Koller et al., 1996; von Stengel, 1996). LP uses equivalent compactrepresentation mixed strategies players form realization plans. realizationplan sequence probability player play sequence actionsassumption opponent choose compatible sequences actionsreach information sets actions specified sequence defined.denote realization plan player ri : R. equilibrium realization planscomputed using following LP (e.g., see Shoham & Leyton-Brown, 2009, p. 135):vinfi (i )Xmax vinfi ()r,vX0vIigi (i , ) ri (i )0 :seq (I 0 )=Iiri () = 1X(2)(3)ri (i a) = ri (i )Ii Ii , = seqi (Ii )(4)(5)aA(Ii )ri (i ) 0Solving LP yields realization plan player using variables ri , expected valuesinformation sets player (variables vIi ). LP works follows: playermaximizes expected utility value selecting values variables realization plan constrained Equations (35). probability playing emptysequence defined 1 (Equation 3), probability playing sequenceequal sum probabilities playing sequences extended exactly one action(Equation 4). Finding realization plan also constrained best respondingopponent, player i. ensured Equation (2), player selectsinformation set Ii action minimizes expected utility value vIi information set. one constraint defined sequence , last actionsequence determines best action played information set infi (i ) = Ii .expected utility composed expected utilities information sets reachableplaying sequence (sum v variables left side) expected utilitiesleafs sequence leads (sum g values right side constraint).3.4 Double-Oracle Algorithm Normal-Form Gamesdescribe concept column/constraint generation techniques applied previouslynormal-form games known double-oracle algorithm (McMahan et al., 2003).Normal-form games represented using game matrices; rows matrix correspondpure strategies one player, columns correspond pure strategies opponent,values matrix cells represent expected outcome game playersplay corresponding pure strategies. Zero-sum normal-form games solved linearprogramming polynomial time size matrix (e.g., see Shoham & LeytonBrown, 2009, p. 89).Figure 1 shows visualization main structure double-oracle algorithmnormal-form games. algorithm consists following three steps repeatconvergence:837fiBosansky, Kiekintveld, Lisy, & PechoucekFigure 1: Schematic double-oracle algorithm normal-form game.1. create restricted game limiting set pure strategies playerallowed play2. compute pair Nash equilibrium strategies restricted game using LPsolving normal-form games3. player, compute pure best response strategy equilibrium strategyopponent found previous step; best response may pure strategyoriginal unrestricted gamebest response strategies computed step 3 added restricted game, gamematrix expanded adding new rows columns, algorithm continuesnext iteration. algorithm terminates neither players improve outcomegame adding new strategy restricted game. case playersplay best response strategy opponent original unrestricted game.algorithm maintains values expected utilities best-response strategiesthroughout iterations algorithm. values provide bounds valueoriginal unrestricted game V perspective player i, minimal valuepast best-response calculations represents upper bound valueoriginal game, ViU B , maximal value past best-response calculationsopponent represents lower bound value original game, ViLB . Notebounds holds lower bound player equal negative valueupper bound opponent:UBViLB = Vigeneral, computing best responses computationally less demanding solvinggame, since problem reduced single-player optimization. Due fact bestresponse algorithms operate quickly (e.g., also exploiting additional domainspecific knowledge), called oracles context. algorithm incrementallyadds strategies one player, algorithm called single-oracle algorithm,algorithm incrementally adds strategies players, algorithm calleddouble-oracle algorithm. Double-oracle algorithms typically initialized arbitrarypair strategies (one pure strategy player). However, also use larger setinitial strategies selected based domain-specific knowledge.double-oracle algorithm zero-sum normal-form games runs polynomial timesize game matrix. Since iteration adds least one pure strategy838fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect InformationFigure 2: Example two-player extensive-form game visualized game tree. Circleplayer aims maximize utility value, box aims minimize utility value. boldedges represent sequences actions added restricted game.restricted game finite pure strategies, algorithm stops|i | + |i | iterations. iteration also polynomial, since consists solvinglinear program computing best responses. relative performance doubleoracle algorithm compared solving linear program original unrestricted gameclosely depends number iterations required convergence. worst case,algorithm adds pure strategies solves original game, although rarelycase practice. Estimating expected number iterations needed double-oraclealgorithm converge, however, remains open problem.3.4.1 Towards Extensive-Form Gamesstraightforward method applying double-oracle algorithm EFGs use purestrategies defined EFGs (i.e., assignments action information set, realizationplans) apply exactly algorithm described section i.e., iteratively addpure strategies unrestricted extensive-form game restricted game matrix.However, result exponential number iterations exponentially largerestricted game worst case. algorithm differs significantly idea sincedirectly operates (more compact) sequences instead full strategies.4. Sequence-Form Double-Oracle Algorithm Extensive-Form Gamesdescribe sequence-form double-oracle algorithm solving extensive-formgames imperfect information. First, give informal overview algorithm.use example game depicted Figure 2 illustrate key concepts. Afterwards, formally define restricted game describe key componentsalgorithm, following full example run algorithm.overall scheme algorithm based double-oracle framework describedprevious section. main difference algorithm uses sequences definerestrictions game tree. restricted game model defined allowingplayers use (i.e., play non-zero probability) subset sequencesoriginal unrestricted game. restricted subset sequences defines subsetsreachable actions, nodes, information sets original game tree. Consider example Figure 2. restricted game defined sequences , A, AC, AD circleplayer, , x box player. sequences represent actions allowed game,839fiBosansky, Kiekintveld, Lisy, & Pechoucekdefine reachable nodes (using history reference , A, Ax, AxC, AxD),reachable information sets (I1 , I2 circle player information setbox player).algorithm iteratively adds new sequences allowed actions restrictedgame, similarly double-oracle algorithm normal-form games. restricted gamesolved standard zero-sum extensive-form game using sequence-form linear program. best response algorithm searches original unrestricted game find newsequences add restricted game. sequences added, restrictedgame tree expanded adding new actions, nodes, information setsreachable based new sets allowed sequences. process solving restrictedgame adding new sequences iterates new sequences improve solutionadded.two primary complications arise use sequences instead fullstrategies double-oracle algorithm, due fact sequences necessarily define actions information sets: (1) strategy computed restricted gamemay complete strategy original game, define behaviorinformation sets restricted game, (2) may possibleplay every action sequence allowed restricted game, playingsequence depend compatible sequence actions opponent.example game tree Figure 2, strategy circle player restricted gamespecifies play information sets I3 I4 . consequence second issueinner nodes original unrestricted game (temporarily) become leafsrestricted game. example, box player add sequence restrictedgame making node Ay leaf restricted game, since actionscircle player restricted game applicable node.algorithm solves complications using two novel ideas. first ideaconcept default pure strategy (denoted iDef ). Informally speaking, algorithmassumes player fixed implicit behavior defines playerdefault information set part restricted game. describeddefault strategy iDef , specifies action every information set. Notedefault strategy need represented explicitly (which could use large amountmemory). Instead, defined implicitly using rules, selecting first actiondeterministic method generating ordered set actions A(h) node h.use default pure strategies map every strategy restricted game validstrategy full game. Specifically, strategy original unrestricted game selectsactions according probabilities specified strategy restricted gameevery information set part restricted game, information setsplays according default pure strategy. Recall example Figure 2,pure default strategy circle player hA, C, E, Gi (i.e., selecting leftmostaction information set). Hence, strategy original unrestricted game usestrategy restricted game information sets I1 I2 , select pure actionsE, G information sets I3 I4 respectively.second key idea use temporary utility values casesallowed actions played node restricted game innernode original game (so called temporary leaf ). ensure correct convergence840fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect InformationHZHiDefri : 7 RC : H 7 Rgi : H 7 Rseqiinfi : 7 Iigame-tree nodes / historiesleafs / terminal statesimplicit default pure strategy playerrealization plan player sequenceprobability reaching node due Nature playextension utility function nodes;gi (h) = ui (h) C(h) h Z gi (h) = 0 h terminal node (h/ Z)sequence(s) actions player leading node / set nodes // information setinformation set last action sequence executedTable 1: outline main symbols used paper.algorithm temporary utilities must assigned provide boundexpected value gained continuing play given node. algorithm usesvalue corresponds expected outcome continuing game play, assumingplayer making choice temporary leaf uses default strategy,opponent plays best response. Assume add sequence box playerrestricted game example tree Figure 2. temporary utility value node Aywould correspond value 2, since default strategy information set I3 play Ecircle player. next section formally describe method provecorrectness algorithm given temporary values.describe detail key parts method. first formally definerestricted game methods expanding restricted game, including detailskey ideas introduced above. describe algorithm selectingnew sequences allowed next iteration. decision sequences addbased calculating best response original unrestricted game using game-treesearch improved additional pruning techniques. Finally, discuss different variationsmain logic double-oracle algorithm determines player(s)algorithm adds new best-response sequences current iteration.4.1 Restricted Gamesection formally defines restricted game subset original unrestrictedgame. restricted game fully specified set allowed sequences. definesets nodes, actions, information sets subsets original unrestricted setsbased allowed sequences. denote original unrestricted game tupleG = (N, H, Z, A, p, u, C, I) restricted game G0 = (N, H 0 , Z 0 , A0 , p, u0 , C, 0 ).sets functions associated restricted game use prime notation; setplayers, functions p C remain same.restricted game defined set allowed sequences (denoted 0 )returned best response algorithms. indicated above, even allowed sequence0 might playable full length due missing compatible sequencesopponent. Therefore, restricted game defined using maximal compatible setsequences 0 0 given set allowed sequences 0 . define 0 maximal841fiBosansky, Kiekintveld, Lisy, & Pechouceksubset sequences 0 that:0i {i 0i : 0i h H j N seqj (h) = j }N(6)Equation (6) means player every sequence 0i , existscompatible sequence opponent allows sequence executed full(i.e., sequentially executing actions sequences node hreached seqj (h) = j players j N ).set sequences 0 fully defines restricted game, setstuple G0 derived 0 . node h restricted gamesequences must played reach h set 0 players:H 0 {h H : N seqi (h) 0 }(7)pair sequences 0 , nodes reachable executing pair sequencesincluded H 0 . Actions defined node h restricted gameplaying action node leads node restricted game:A0 (h) {a A(h) : ha H 0 }h H 0(8)Nodes restricted game corresponding inner nodes original unrestrictedgame may inner nodes restricted game. Therefore, set leavesrestricted game union leaf nodes original game inner nodesoriginal game currently valid continuation restricted game, basedallowed sequences:Z 0 Z H 0 {h H 0 \ Z : A0 (h) = }(9)explicitly differentiate leaves restricted game correspond leavesoriginal unrestricted game (i.e., Z 0 Z) leaves restricted game correspondinner nodes original unrestricted game (i.e., Z 0 \ Z), since algorithm assignstemporary utility values nodes latter case.information sets restricted game correspond information sets originalunrestricted game. node h belongs information set Ip(h) original game,holds restricted game. define information set partrestricted game least one inner node belongs informationset included restricted game:Ii0 {Ii Ii : h Ii h H 0 \ Z 0 }(10)information set restricted game Ii Ii0 consists nodesrestricted game i.e., h Ii : h H 0 .Finally, define modified utility function u0 restricted game. primaryreason modified utility function define temporary utility values leavesset Z 0 \Z. Consider h Z 0 \Z temporary leaf player player actingnode (i = p(h)). Moreover, let ui (h) expected outcome game startingnode assuming players playing NE strategies original unrestrictedgame. modified utility function u0i leaf must return value lower bound842fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Informationvalue ui (h). Due zero-sum assumption, value represents upper boundvalue opponent i. Setting value way ensures two things: (1) playerlikely use sequences leading node h optimal strategies restricted game (sincemodified utility value upper bound actual value), (2) player adds newsequences using best-response algorithms prolong sequence seqi (h) leading node hsequences would yield better expected value u0i . Later showcounterexample setting value otherwise cause algorithm convergeincorrect solution. calculate lower bound setting utility valuecorresponds outcome original game player continues playingBR default strategy.default strategy iDef opponent plays best responsevalid lower bound since consider single strategy player actingnode h, correspond default strategy; considering strategies could allowplayer improve value continuing node h. leaf nodesh Z 0 Z set u0i (h) ui (h).4.1.1 Solving Restricted Gamerestricted game defined section valid zero-sum extensive-form gamesolved using sequence-form linear programming described Section 3.algorithm computes NE restricted game solving pair linear programs usingrestricted sets 0 , H 0 , Z 0 , 0 , modified utility function u0 .strategy restricted game translated original game usingpure default strategy extend restricted strategy defined. Formally,ri0 mixed strategy represented realization plan player restrictedgame, define extended strategy r0i strategy identical strategyrestricted game sequences included restricted game, corresponddefault strategy iDef sequence included restricted game:(ri0 (i )0ir0i (i )(11)ri0 (i0 ) iDef (i \ i0 )/ 0i ; i0 = arg maxi00 0i ; i00 vi |i00 |realization plan sequence allowed restricted game (i.e.,/ 0i )equal realization probability longest prefix sequence allowedrestricted game (denoted i0 ), setting remaining part sequence (i.e., \ i0 )correspond default strategy player i. computation expressedmultiplication two probabilities, overload notation use iDef (i \ i0 )1 remaining part sequence corresponds default strategy player i,0 otherwise.iteration double-oracle algorithm one sequence-form LP solvedplayer compute pair NE strategies restricted game. denote strategies) (r , r ) extended original unrestricted game using(ri , ridefault strategies.4.1.2 Expanding Restricted Gamerestricted game expanded adding new sequences set 0 updatingremaining sets according definition. adding new sequences, algorithm843fiBosansky, Kiekintveld, Lisy, & Pechoucekcalculates stores temporary utility values leaves Z 0 \ Z usedsequence-form LP.updating restricted game, linear programs modified correspond new restricted game. newly added information sets sequences,new variables created linear programs constraints correspondinginformation sets/sequences created (Equations 2 4). Moreover, constraints already existing linear program need updated. sequenceadded set 0i immediate prefix sequence (i.e., sequence i0 v|i0 | + 1 = |i |) already part restricted game, need updateconstraint information sets Ii i0 = seqi (Ii ) ensure consistencystrategies (Equation 4), constraint corresponding sequence i0 (Equation 2).addition, algorithm updates Equations (2) assigned sequences opponentg(i , ) 6= 0. Finally, algorithm updates constraints previously usedutilities temporary leaf nodes longer leaf nodes restricted gameadding new sequences.New sequences player found using best response sequence (BRS) algorithms described Section 4.2. perspective sequence-form double-oraclealgorithm, BRS algorithm calculates pure best response player fixedstrategy opponent original unrestricted game. pure best response specifiesaction play information set currently reachable given opponentsextended strategy ri . best response formally defined pure realizationplan riBR assigns integer values 0 1 sequences. realization plannecessarily pure strategy original unrestricted game mayaction specified every information set. Specifically, action specifiedinformation sets reachable (1) due choices player i, (2) duezero probability realization plan opponent ri . Omitting actionsaffect value best response information sets never reached;hence, riBR holds r0i ui (riBR , ri ) ui (r0i , ri ) exists pure bestresponse strategy iBR ui (riBR , ri ) = ui (iBR , ri ). sequencesused best-response pure realization plan probability 1 returned BRSalgorithm call best-response sequences:{i : riBR (i ) = 1}(12)4.1.3 Example Run Algorithmdemonstrate sequence-form double-oracle algorithm example game depicted Figure 3a. example, two players: circle box. Circle aimsmaximize utility value leafs, box aims minimize utility value. assumechoosing leftmost action information set default strategyplayers game.algorithm starts empty set allowed sequences restricted game0 ; hence, algorithm sets current pair (ri , ri ) strategies equivalentDef ). Next, algorithm adds new sequences correspond best response(iDef ,default strategy opponent; example best response sequencescircle player {, A, AD}, {, y} box player. sequences added844fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information(a) Step 0(b) Step 1(d) Step 3(c) Step 2(e) Step 4Figure 3: Example steps sequence-form double-oracle algorithm two-playerzero-sum game, circle player aims maximize utility value, box aims minimizeutility value. Bold edges correspond sequences actions added restrictedgame. dashed boxes indicate information sets.set allowed sequences 0 . Next, set sequences restricted game 0updated. maximal compatible set sequences set 0 cannot contain sequenceAD compatible sequence box player (i.e., x case) allowedrestricted game yet sequence AD cannot fully executed. Moreover, addingsequences y, restricted game contain node Ay actions E Fdefined original unrestricted game. However, continuationcurrent restricted game yet; hence, node temporary leaf, belongs Z 0 \ Z,algorithm needs define new value modified utility function u0 node.value u0 (Ay) equal 2 corresponds outcome game circleplayer continues playing default strategy box player plays best response.complete first step algorithm summarize nodes information setsincluded restricted game; H 0 contains 3 nodes (the root, node playingaction node Ay), two information sets (the information set node Ayadded restricted game, node leaf restricted game).Playing sequences probability 1 Nash equilibrium restrictedgame. situation depicted Figure 3b, sequences 0 shown bold edges.algorithm proceeds complete list steps algorithm summarized Table 2. second iteration, new sequences B BH addedrestricted game. box player add new sequences iterationbest response extended equilibrium strategy circle player i.e., playingsequences A, AC, AE probability 1. NE updated restricted game changesplaying sequences B, BH sequence y, probability 1. third iterationsituation changes box player adds sequence x, new sequences845fiBosansky, Kiekintveld, Lisy, & Pechoucekadded circle player. adding sequence x, sequence AD also becomes partset 0 fully executed due adding compatible sequence x. NErestricted game fully mixed, sequences starting B playedratio 3 : 4, x ratio 4 : 3. fourth iteration, algorithm addssequence AF restricted game (the best response circle player), removesassigned value u0 (Ay) since node longer belongs set Z 0 . algorithm stopsfour iterations. sequences added restricted game, solution) translated solution original unrestrictedrestricted game (ri , rigame, (ri , ri ) Nash equilibrium original game.Iteration1.2.3.4.BRr, A, AD, B, BH, B, BH, A, AFBRr,,, x,0,, A, B, BH, A, AD, B, BH, A, AD, AF, B, BH0,,, y, x, y, xTable 2: Steps sequence-form double-oracle algorithm applied example.Consider small modification example game utility value3 leaf following action F (i.e., node AyF ). case, algorithmneed add sequence AF (nor AE) restricted game improvevalue restricted game. Note modified example game showsalgorithm needs set utility values nodes Z 0 \ Z. algorithm simply usesunmodified utility function, node Ay treated zero utilityvalue. value overestimates outcome actual continuation following nodeoriginal game circle player since sequences AE AF neverpart best response circle player, algorithm converge incorrectsolution.4.2 Best-Response Sequence Algorithmpurpose best-response sequence (BRS) algorithm generate new sequencesadded restricted game next iteration, provebest response better expected value uses sequences currently allowedrestricted game. Throughout section use term searching player representplayer algorithm computes best response sequences. referplayer i.BRS algorithm calculates expected value pure best response opponents strategy ri . algorithm returns set best-response sequences wellexpected value strategy extended strategy opponent.algorithm based depth-first search traverses original unrestrictedgame tree. behavior opponent fixed strategy given extendedrealization plan ri . save computation time, best-response algorithms use branchbound search best-response sequences. algorithm uses boundexpected value inner node, denoted . bound represents minimalutility value node currently evaluated needs gain order part846fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect InformationRequire: - searching player, h - current node, Iik - current information set, r0i - opponentsstrategy, Min/MaxUtility - bounds utility values, - lower bound node h1: w r (seqi (h)) C(h)2: h Z3:return ui (h) w4: else h Z 0 \ Z5:return u0i (h) w6: end7: sort A(h) based probability wa r 0i seqi (ha) C(ha)8: v h 09: A(h),wa > 010:0 v h + (w wa ) MaxUtility11:0 wa MaxUtility12:v 0 BRSi (ha, 0 )13:v 0 =14:return15:end16:vh vh + v017:w w wa18:else19:return20:end21: end22: return v hFigure 4: BRSi nodes players.best-response sequence. Using bound search, algorithm ableprune branches certainly part best-response sequence. boundset MinUtility root node.distinguish 2 cases search algorithm: either algorithm evaluatinginformation set (or specifically node h) assigned searching player i,node assigned one players (either opponent, player i,chance node). pseudocode two cases depicted Figures 4 5.4.2.1 Nodes Opponentfirst describe case used algorithm evaluates node h assigned eitheropponent searching player Nature (see Figure 4). main ideacalculate expected utility node according (fixed) strategy player.strategy known either given extended realization plan ri ,stochastic environment (C). Throughout algorithm, variable w representsprobability node based realization probability opponent stochasticenvironment (line 1). value iteratively decreased values wa represent realization probabilities currently evaluated action A(h). Finally, vh expectedutility value node.algorithm evaluates actions descending order according probabilityplayed (based r0i C; lines 921). First, calculate new lower bound847fiBosansky, Kiekintveld, Lisy, & Pechoucek0 successor ha (line 10). new lower bound minimal value mustreturned recursive call BRSi (ha) optimistic assumptionremaining actions yield maximum possible utility. lower boundexceed maximum possible utility game, algorithm executed recursivelysuccessors (line 12). Note algorithm evaluate branches zerorealization probability (line 9).3 possibilities pruning part search algorithm. firstpruning possible currently evaluated node leaf restricted game,node inner node original node (i.e., h Z 0 \ Z; line 5). algorithmdirectly use value modified utility function u0 case, since calculatedbest response searching player default strategy opponentapplied successors node h since h Z 0 . Secondly, cut-off also occursnew lower bound successor larger maximum possible utilitygame, since value never obtained successor (line 19). Finally, cut-offoccurs cut-off one successors (line 14).4.2.2 Nodes Searching Playernodes assigned searching player, algorithm evaluates every actionstate belongs current information set. algorithm traverses statesdescending order according probability occurrence given strategiesopponent Nature (line 8). Similar previous case, iteration algorithmcalculates new lower bound successor node (line 17). new lower bound 0minimal value must returned recursive call BRSi (h0 a) orderaction selected best action information set optimisticassumption action yields maximum possible utility value applyingremaining states information set. algorithm performs recursive call(line 20) action still could best information set (i.e., lowerbound exceed maximal possible utility game). Note cut-offoccurs one successors, currently evaluated action longer bestaction information set. Hence, va set action evaluatedremaining nodes. algorithm determines action selectedbest one information set, evaluates action remaining nodesinformation set. Finally, algorithm stores values best actionnodes information set (line 30). reused information setvisited (i.e., algorithm reaches different node h0 information setIi ; line 5).cut-off occurs part search algorithm maximal possible value vahsmaller lower bound evaluating node h. means regardlessaction selected best action information set, lower boundnode h reached; hence, cut-off occurs (line 27). cut-off occursinformation set, information set cannot reached sequencessearching player leading information set cannot part best response.due propagating cut-off least one previous information set searchingplayer, otherwise tight lower bound set (the bound first set848fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect InformationRequire: - searching player, h - current node, Iik - current information set, ri - opponentsstrategy, Min/MaxUtility - bounds utility values, lower bound node h1: h Z2:return ui (h) r0i (seqi (h)) C(h)3: end4: v h already calculated5:return v h6: end7: H 0 {h0 ; h0 Ii }08: sort Hvalue ri (seqi (h0 )) C(h0 )P descending according09: w h0 H 0 r (seqi (h )) C(h0 )10: va 0 A(h); maxAction11: h0 H 012:wh0 r0i (seqi (h0 )) C(h0 )13:A(h0 )14:maxAction empty15:0 wh0 MinUtility16:else17:0 (vmaxAction + w MinUtility) (va + (w wh0 ) MaxUtility)18:end19:0 wh0 MaxUtility020:vah BRSi (h0 a, 0 )021:va va + vah22:end23:end24:maxAction arg maxaA(h0 ) va25:w w wh026:h evaluated maxaA(h) vah <27:return28:end29: end0h0v h h0 H 030: store vmaxActionh31: return vmaxActionFigure 5: BRSi nodes searching player.information sets searching player). Therefore, exists least one actionsearching player never evaluated (after cut-off, value vaaction set ) cannot selected best action information set. Sinceassume perfect recall, nodes information set Ii share sequence actionsseqi (Ii ); hence, node h0 Ii reached again.4.3 Main Loop Alternativesintroduce several alternative formulations main loop sequence-formdouble-oracle algorithm. general approach double-oracle algorithm solverestricted game find equilibrium strategy player, compute best responsesoriginal game players, continue next iteration. However,sequence-form LP formulated double-oracle scheme way849fiBosansky, Kiekintveld, Lisy, & Pechoucekiteration algorithm solve restricted game perspective singleplayer i. words, formulate single LP described Section 3.3 computesoptimal strategy opponent restricted game (player i), computebest response player strategy. means iterationselect specific player i, compute best response iteration. callselection process player-selection policy.several alternatives player-selection policy act domainindependent heuristics double-oracle algorithm. consider three possible policies:(1) standard double-oracle player-selection policy selecting players iteration, (2) alternating policy, algorithm selects one player switchesplayers regularly (player selected one iteration, player selectedfollowing iteration), finally (3) worse-player-selection policy selects playercurrently worse bound solution quality. end iterationalgorithm selects player upper bound utility value awaycurrent value restricted game. formally,fifiarg max fiViU B ViLP fi(13)ViLP last calculated value restricted game player i. intuitionbehind choice either bound precise missing sequencesplayer restricted game need added, upper bound overestimated. either case, best-response sequence algorithm run playernext iteration, either add new sequences tighten bound. case tie,alternating policy applied order guarantee regular switching players.experimentally compare policies show impact overall performancesequence-form double-oracle algorithm (see Section 6).5. Theoretical Resultssection prove sequence-form double-oracle algorithm always converge Nash equilibrium original unrestricted game. First, formally definestrategy computed best-response sequence (BRS) algorithm, prove lemmascharacteristics BRS strategies, finally prove main convergenceresult. Note variations main loop described Section 4.3 affectcorrectness algorithm long player-selection policy ensures improvement made BRS algorithm one player BRS algorithm runopponent next iteration.0 realization plan player restricted game G0 . BRS(r 0 )Lemma 5.1 Let rireturns sequences corresponding realization plan riBR unrestricted game,riBR part pure best response strategy r0i . value returned algorithmvalue executing pair strategies ui (r0i , riBR ).0 ) searches game tree selects action maximizes valueProof BRS(rigame player information sets Ii assigned player reachable givenstrategy opponent r0i . opponents nodes, calculates expected value850fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information0 defined value according pure action defaultaccording riDef0 defined. chance nodes, returns expected valuestrategy rinode sum values successor nodes weighted probabilities.node h, successors maximal possible value node h alsomaximal possible value (when playing r0i ). selections nodesbelong achieves maximal value; hence, form best response strategyr0i .0 )) denote value returned BRS algorithm,brevity use v(BRS(riequal ui (r0i , riBR ).0 realization plan player restricted game G0 letLemma 5.2 Let riVi value original unrestricted game G player i,0v(BRS(ri)) Vi .(14)0 )) value best response r 0Proof Lemma 5.1 showed v(BRS(ri0 )) < Vvalid strategy original unrestricted game G. v(BRS(riVi cannot value game since player strategy r0i achieves betterutility, contradiction.0 realization plan player returned LPLemma 5.3 Let ri0restricted game G let ViLP value restricted game returned LP,0v(BRS(ri)) ViLP .(15)0Proof realization plan ripart Nash equilibrium strategy zero-sumLPgame guarantees value ViG0 . best response computation originalunrestricted game G selects actions restricted game G0 , creates bestresponse game G0 well obtaining value ViLP . best response selects actionallowed restricted game G0 , two cases.Case 1 : best response strategy uses action temporary leaf h Z 0 \ Z.Player makes decision leaf, otherwise value temporary leafwould directly returned BRS. value temporary leaf underestimated player restricted game modified utility function u0Def .over-estimated BRS computation best response default strategyvalue best response increase including action.Case 2 : best response strategy uses action allowed G0 internal noderestricted game H 0 \ Z 0 . occur nodes assigned player i,actions player going G0 probability zero r0i . BRS takes actionmaximum value nodes assigned player i, reason selecting actionleading outside G0 greater equal value best action G0 .0 )) > V LPLemma 5.4 assumptions previous lemma, v(BRS(rireturns sequences added restricted game G0 next iteration.851fiBosansky, Kiekintveld, Lisy, & PechoucekProof Based proof previous Lemma, BRS player improvevalue LP (ViLP ) selecting action present G0performed node h included G0 (in makes decision). Let (i , )pair sequences leading h. construction restricted gamenext iteration, sequence sequence ensures executed fullpart new restricted game.Note, Lemmas 5.2 5.4 would hold utility values u0 temporaryleaves (h Z 0 \ Z) set arbitrarily. algorithm sets values temporary leaf hplayer p(h) continues playing default strategy opponent (p(h))playing best response. utility values temporary leaves set arbitrarilyused BRS algorithms speed-up calculation proposed (see algorithmFigure 4, line 5), Lemma 5.2 need hold cases valuenode h strictly overestimates optimal expected value player p(h). case,best-response value opponent may lower optimal outcome,v BRS(rp(h) ) < Vp(h)(16)hand, BRS algorithm use temporary values u0node, Lemma 5.4 violated best-response value strictly higherplayer p(h) even though new sequences added restricted game.Theorem 5.5 sequence-form double-oracle algorithm extensive-form games described previous section terminates0v(BRS(ri)) = v(BRS(ri0 )) = ViLP = Vi ,(17)always happens finite number iterations (because game finite),strategies (r0i , r0i ) Nash equilibrium original unrestricted game.Proof First show algorithm continues equalities (17) hold.0 )) 6= v(BRS(r 0 )) Lemma 5.2 Lemma 5.4 knowv(BRS(ri0 ) > V LP , restricted game following itersome player holds BRS(riation larger least one action algorithm continues. worst case,restricted game equals complete game G0 = G, cannot extended further.case BRS cannot find better response Vi algorithm stops dueLemma 5.4.condition theorem holds algorithm found NE completeBR = BRS(r 0 ) best response r 0game, Lemma 5.1 know ricomplete game. However, value best response strategy zero-sumgame value game, strategy r0i optimal part Nashequilibrium game.6. Experimentspresent experimental evaluation performance sequence-formdouble-oracle algorithm EFGs. compare algorithm two state-of-the-art852fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Informationbaselines, full sequence-form LP (referred FullLP on), Counterfactual Regret Minimization (CFR). first baseline standard exact methodsolving sequence-form EFG, CFR one leading approximate algorithms applied EFG. experimental results demonstrate advantages double-oraclealgorithm three different classes realistic EFGs. also test impact differentvariants main loop algorithm described Section 4.3.compare three variants sequence-form double-oracle algorithm: (1) DO-bvariant best-responses calculated players iteration;(2) DO-sa calculates best-response single player iteration accordingsimple alternating policy; (3) DO-swp variant best-responsecalculated single player according worse-player selection policy.variants double-oracle algorithm use default strategy firstaction applicable state played default.Since standardized collection zero-sum extensive-form games benchmark purposes, use several specific games evaluate double-oracle algorithmidentify strengths weaknesses algorithm. games selectedevaluate performance different conditions, games differ maximalutility players gain, causes imperfect information, structureinformation sets. One key characteristics affects performancedouble-oracle algorithm relative size support Nash equilibria (i.e., number sequences used NE non-zero probability). exist NEsmall support, algorithm must necessarily add large fraction sequencesrestricted game find solution, mitigating advantages double-oracle approach.present results two types games double-oracle significantly outperforms FullLP instances: search game motivated border patrol PhantomTic-Tac-Toe. also present results simplified version poker doubleoracle algorithm always improve computation time. However, FullLPalso limited scalability due larger memory requirements cannot find solutionslarger variants poker, double-oracle algorithm able solve instances.principal interest developing new generic methods solving extensive-formgames. Therefore, implemented algorithm generic framework modeling arbitrary extensive-form games.1 algorithms use domain-specific knowledgeimplementation, rely specific ordering actions. drawbacksgeneric implementation higher memory requirements additional overheadalgorithms. domain-specific implementation could improve performanceeliminating auxiliary data structures. run experiments usingsingle thread Intel i7 CPU running 2.8 GHz. algorithms givenmaximum 10 GB memory Java heap space. used IBM CPLEX 12.5 solvinglinear programs, parameter settings use single thread barrier solutionalgorithm.addition runtimes, analyze speed convergence double-oracle algorithms compare one state-of-the-art approximative algorithms, Counterfactual Regret Minimization (CFR). implemented CFR domain independent way1. Source code available home pages authors.853fiBosansky, Kiekintveld, Lisy, & Pechoucekbased pseudocode work Lanctot (2013, p. 22). principle, sufficientCFR maintain set information sets apply no-regret learning ruleinformation set. However, maintaining traversing set effectivelydomain independent manner could affected implementation generic extensiveform games data structures (i.e., generating applicable actions states game,applying actions, etc.). Therefore use implementation CFR traversescomplete game tree held memory maintain fairness comparison,guarantee maximal possible speed convergence CFR algorithm. timenecessary build game tree included computation time CFR.6.1 Test DomainsSearch Games first test belongs class search (or pursuit-evasion) games,often used experimental evaluation double-oracle algorithms (McMahan et al., 2003;Halvorson et al., 2009). search game two players: patroller (or defender)evader (or attacker). game played directed graph (see Figure 6),evader aims cross safely starting node (E) destination node (D).defender controls two units move intermediate nodes (the shaded areas)trying capture evader occupying node evader. turnplayers move units simultaneously current node adjacent node,units stay location. exception evader cannot staytwo leftmost nodes. pre-determined number turns made without either playerwinning, game draw. example win-tie-loss game utilityvalues set {1, 0, 1}.Players unaware location actions player one exceptionevader leaves tracks visited nodes discovered defender visitsnodes later. game also includes option evader avoid leaving tracksusing special move (a slow move) requires two turns simulate evader coveringtracks.Figure 6 shows examples graphs used experiments. patrolling unitsmove shaded areas (P1,P2), start node shadedareas. Even though graph small, concurrent movement units implies largebranching factor (up 50 one turn) thus large game trees (up 1011 nodes).experiments used three different graphs, varied maximum number turnsgame (from 3 7), altered ability attacker perform slowmoves (labeled SA slow moves allowed, SD otherwise).Phantom Tic-Tac-Toe second game blind variant well-known gameTic-Tac-Toe (e.g., used Lanctot et al., 2012). game played 3 3 board,two players (cross circle) attempt place 3 identical marks horizontal, vertical,diagonal row win game. blind variant, players unable observeopponents moves player knows opponent made moveturn. Moreover, player tries place mark square already occupiedopponents mark, player learns information place marksquare. Again, utility values game set {1, 0, 1}.854fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect InformationFigure 6: Three variants graph used experiments search game; referG1 (left), G2 (middle), G3 (right).uncertainty phantom Tic-Tac-Toe makes game large ( 109 nodes).addition, since one player try several squares move successful, playersnecessarily alternate making moves. rule makes structureinformation sets rather complicated since opponent never learns many attemptsfirst player actually performed, single information set contain nodes differentdepths game tree.Poker Games Poker frequently studied literature example largeextensive-form game imperfect information. include experiments simplifiedtwo-player poker game inspired Leduc Holdem.version poker, player starts amount chipsplayers required put number chips pot (called ante). nextstep, Nature player deals single card player (the opponent unawarecard) betting round begins. player either fold (the opponent wins pot),check (let opponent make next move), bet (being first add amountchips pot), call (add amount chips equal last bet opponentpot), raise (match increase bet opponent). raise madeplayers, betting round ends, Nature player deals one cardtable, second betting round rules begins. second bettinground ends, outcome game determined player wins if: (1) private cardmatches table card opponents card match, (2) none playerscards matches table card private card higher private cardopponent, (3) opponent folds. utility value amount chips playerlost. player wins, game draw pot split.experiments alter number types cards (from 3 4;3 types cards Leduc), number cards type (from 2 3; set 2 Leduc),maximum length sequence raises betting round (ranging 1 4; set 1Leduc), number different sizes bets (i.e., amount chips added pot)bet/raise actions (ranging 1 4; set 1 Leduc).6.2 ResultsSearch Games results search game scenarios show sequence-formdouble-oracle algorithm particularly successful applied games NEssmall support exist. Figure 7 shows comparison running times FullLPvariants double-oracle algorithm (note logarithmic y-scale). variants855fiBosansky, Kiekintveld, Lisy, & Pechoucek102101100103FullLPDO-BDO-SADO-SWPTime [s] (log scale)Time [s] (log scale)103G1-SDG2-SDG3-SDG1-SAG2-SA102101100G3-SASearch Game Scenarios - Depth 6FullLPDO-BDO-SADO-SWPG1-SDG2-SDG3-SDG1-SAG2-SAG3-SASearch Game Scenarios - Depth 7Figure 7: Comparison running times 3 different graphs either slow movesallowed (SA) disallowed (SD), depth set 6 (left subfigure) 7 (right subfigure).Missing values FullLP algorithm indicate algorithm runs memory.double-oracle algorithm several orders magnitude faster FullLP.apparent fully-connected graph (G2) generates largest game tree.slow moves allowed depth set 6, takes almost 100 seconds FullLPsolve instance game variants double-oracle algorithms solvegame less 3 seconds. Moreover, depth increased 7, FullLPunable solve game due memory constraints, fastest variant DO-swpsolved game less 5 seconds. Similar results obtained graphs.graph G1 induced game difficult double-oracle algorithm:depth set 7, takes almost 6 minutes FullLP solve instance,fastest variant DO-swp solved game 21 seconds. reason even thoughgame tree largest, complex structure information sets.due limited compatibility among sequences players; patrollingunit P1 observes tracks top-row node, second patrolling unit P2 captureevader top-row node, middle-row node.Comparing different variants sequence-form double-oracle algorithmshow consistent results. variant consistently better game sincedouble-oracle variants typically able compute Nash equilibrium quickly.However, DO-swp often fastest settings difference quite significant.speed-up variant offers apparent G1 graph. averageinstances search game, DO-sa uses 92.59% computation time DO-b,DO-swp uses 88.25%.Table 3 shows breakdown cumulative computation time spent different components double-oracle algorithm: solving restricted game (LP), calculating bestresponses (BR), creating valid restricted game selecting new sequences add(Validity). results show due size game, computationbest-response sequences takes majority time (typically around 75% largerinstances), creating restricted game solving takes small fractiontotal time. also noticeable size final restricted game small856fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect InformationDO-BDO-SADO-SWPCFRBounds Interval Size [-] (log scale)1010.10.010.0010.00011e-05050100Time [s]150200Figure 8: Convergence variants double-oracle algorithm CFR searchgame domain: y-axis displays current approximation error.AlgorithmFullLPDO-bDO-saDO-swpOverall [s]351.9881.5154.3221.15LP [s]6.975.51.93BR [s]63.3939.1116.28Validity [s]10.589.092.47Iterations187344209|0 ||01 |( |11 | )252 (17.22%)264 (18.05%)193 (13.19%)|0 ||02 |( |22 | )711 (0.26%)649 (0.24%)692 (0.25%)Table 3: Cumulative running times different components double-oracle algorithm,iterations, size restricted game terms number sequences comparedsize complete game. results shown scenario G1, depth 7, allowedslow moves.compared original game, since number sequences second player (thedefender) less 1% (there 273,099 sequences defender).Finally, analyze convergence rate variants double-oracle algorithm.results depicted Figure 8, size interval given boundsViU B ViLB defines current error double-oracle algorithm |ViU B ViLB |.convergence rate CFR algorithm also depicted. error CFR calculatedway, sum best-response values current mean strategiesCFR algorithm. see variants double-oracle algorithm performsimilarly error drops quickly 1 iterations later versionalgorithm quickly converges exact solution. results show gamedouble-oracle algorithm quickly find correct sequences actions computeexact solution, spite size game. comparison, CFR algorithmalso quickly learn correct strategies information sets, convergencelong tail. 200 seconds, error CFR equal 0.0657 droppingslowly (0.0158 1 hour). error CFR quite significant considering valuegame case (0.3333).Phantom Tic-Tac-Toe results Phantom Tic-Tac-Toe confirm gamealso suitable sequence-form double-oracle algorithm. Due size game,baseline algorithms (the FullLP CFR) ran memory able857fiBosansky, Kiekintveld, Lisy, & PechoucekDO-SADO-SWPDO-BDO-SADO-SWP1Time [s] (log scale)Bounds Interval Size [-] (log scale)DO-B0.10.010.0011040.00011e-05050001000015000Time [s]2000025000103RandomDomain-dependentDifferent Action Ordering Phantom Tic-Tac-ToeFigure 9: (left) Comparison convergence rate double-oracle variants Phantom Tic-Tac-Toe; (right) Comparison performance double-oracle variantsPhantom Tic-Tac-Toe domain-specific move ordering default strategy used.AlgorithmFullLPDO-bDO-saDO-swpOverall [s]N/A21,19717,66717,589LP [s]2,6352,2062,143BR [s]17,56214,56014,582Validity [s]999900864Iterations335671591|0 ||01 |( |11 | )7,676 (0.60%)7,518 (0.59%)8,242 (0.65%)|0 ||02 |( |22 | )10,095 (0.23%)9,648 (0.22%)8,832 (0.20%)Table 4: Cumulative running times different components double-oracle algorithmgame Phantom Tic-Tac-Toe.solve game. Therefore, compare times different variantsdouble-oracle algorithm. Figure 9 (left subfigure) shows overall performance threevariants double-oracle algorithm form convergence graph. seeperformance two variants similar, performance DO-sa DO-swpalmost identical. hand, results show DO-b converges significantlyslower.time breakdown variants double-oracle algorithm shown Table 4.Similarly previous case, majority time ( 83%) spent calculatingbest responses. variants double-oracle algorithm, DO-swp variantfastest one. converged significantly fewer iterations compared DO-savariant (iterations twice expensive DO-b variant).present results demonstrate potential combining sequenceform double-oracle algorithm domain-specific knowledge. Every variant doubleoracle algorithm use move ordering based domain-specific heuristics. moveordering determines default strategy (recall algorithm uses first actiondefault strategy player), direction search best responsealgorithms. replacing randomly generated move ordering heuristic onechooses better actions first, results show significant improvement performancevariants (see Figure 9, right subfigure), even though changesrest algorithm. variant able solve game less 3 hours,took 2 hours fastest DO-swp variant.858fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Information200FullLPDO-BDO-SADO-SWP103Time [s] (log scale)150Time [s]104FullLPDO-BDO-SADO-SWP100102101501000R1R2R3R4Increasing number allowed "Raise Actions"B1B2B3B4Increasing size possible betsFigure 10: Comparison running times different variants simplified pokergame. left subfigure shows computation times increasing number raiseactions allowed, right subfigure shows computation times increasing numberdifferent bet sizes raise/bet actions.Poker Games Poker represents game double-oracle algorithms perform well sequence-form LP often faster smaller instances. One significantdifference compared previous games size NE support larger(around 5% sequences larger instances). Secondly, game trees poker gamesrelatively shallow imperfect information game due Nature.result, double-oracle algorithms require larger number iterations addsequences restricted game (up 10% sequences player added evenlargest poker scenarios) order find exact solution. However, increasingdepth and/or branching factor, size game grows exponentially FullLPable solve largest instances due memory constraints.Figure 10 shows selected results simplified poker variants. resultsleft subfigure show computation times increasing depth game allowingplayers re-raise (players allowed re-raise opponent certain numbertimes). remaining parameters fixed 3 types cards, 2 cards type, 2different betting sizes. size game grows exponentially, number possiblesequences increasing 210,937 player R4 scenario. computation timeFullLP directly related size tree increases exponentiallyincreasing depth (note standard scale). hand, increaseless dramatic variants double-oracle algorithm. DO-swp variantfastest largest scenario FullLP solved instance 126 seconds,took 103 seconds DO-swp. Finally, FullLP able solve gamesincrease length R5 due memory constraints, computation timedouble-oracle algorithms increases marginally.right subfigure Figure 10 shows increase computation time increasing number different bet sizes raise/bet actions. remaining parametersfixed 4 types cards, 3 cards type, 2 raise actions allowed. Again,game grows exponentially increasing branching factor. number sequencesincreases 685,125 player B4 scenario, computation time859fiBosansky, Kiekintveld, Lisy, & PechoucekDO-BDO-SADO-SWPCFRDO-BDO-SWPCFR10Bounds Interval Size [-] (log scale)Bounds Interval Size [-] (log scale)10DO-SA10.10.010.0010.00011e-0510.10.010.0010.00011e-05050100150200250Time [s]3003504000200400600800Time [s]100012001400Figure 11: Comparison convergence variants double-oracle algorithmCFR two variants simplified poker 4 types cards, 3 cardstype. 4 raise actions allowed, 2 different bet sizes left subfigure;2 raise actions allowed, 3 different bet sizes right subfigure.AlgorithmFullLPDO-bDO-saDO-swpOverall [s]278.18234.60199.24182.68LP [s]149.32117.71108.95BR [s]56.0451.2548.25Validity [s]28.6129.5924.8Iterations152289267|0 ||01 |( |11 | )6,799 (1.81%)6,762 (1.80%)6,572 (1.75%)|0 ||02 |( |22 | )6,854 (1.83%)6,673 (1.78%)6,599 (1.76%)Table 5: Cumulative running times different components double-oracle algorithm,iterations, sizes restricted game terms number sequences comparedsize complete game. results shown poker scenario 4 raiseactions allowed, 2 different betting values, 4 types cards, 3 cards type.algorithms increases exponentially well (note logarithmic scale). results showeven increasing branching factor, double-oracle variants tend slowersolving FullLP. However, FullLP ran memory largestB4 setting, double-oracle variants able find exact solution using lessmemory.Comparing different variants double-oracle algorithm using convergencegraph (see Figure 11) decomposition computation times (see Table 5) showsDO-swp fastest variant selected scenario (and nearly pokerscenarios). Decomposition overall time shows majority computationtime spent solving restricted game LP (up 65%). decomposition also showsDO-swp typically faster due lower number iterations. addition,final size restricted game typically smallest variant. averageinstances poker games, DO-sa uses 86.57% computation time DO-b,DO-swp uses 82.3% computation time.Convergence poker games slower compared search games similar size (notelogarithmic scale Figure 11). Comparing double-oracle algorithm variants CFRshows interesting result left subfigure. Due size game, speedCFR convergence nearly double-oracle algorithms first860fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Informationiterations. However, double-oracle algorithms continue converge roughlyrate able find exact solution, error CFR algorithm decreasesslowly. scenario depicted left subfigure, CFR algorithm convergederror 0.1212 (the value game case 0.09963) 400 seconds.1 hour, error dropped 0.0268. scenarios shallow game treeslarger branching factor, convergence CFR faster beginning compareddouble-oracle algorithms (right subfigure Figure 11). However, main disadvantageCFR long tail convergence still case error 1600 secondsstill 0.0266 (the value game 0.09828).6.3 Discussion Resultsexperimental results support several conclusions. results demonstratesequence-form double-oracle algorithm able compute exact solution much largergames compared state-of-the-art exact algorithm based sequence-form linearprogram. Moreover, experimentally shown realistic gamessmall fraction sequences necessary find solution game. cases,double-oracle algorithms also significantly speed computation time. resultsindicate DO-swp variant typically fastest, cases. selectingplayer currently worse bound performance, DO-swp versionadd important sequences, prove better sequences adjustupper bound value faster.Comparing speed convergence double-oracle algorithms state-ofthe-art approximative algorithm CFR showed CFR quickly approximates solutionfirst iterations. However, convergence CFR long tail CFRable find exact solution larger games reasonable time. Another interestingobservation games convergence rate double-oracle algorithmsCFR similar first iterations, double-oracle algorithms continuerate find exact solution, long tail convergence remains CFR.despite fact implementation CFR advantage completegame tree including states histories memory.Unfortunately, difficult characterize exact properties gamesdouble-oracle algorithms perform better terms computation time comparedalgorithms. Certainly, double-oracle algorithm suitable gamesequilibria large support due necessity large number iterations.However, small support equilibrium sufficient condition. apparentdue two graphs shown poker experiments, either depth game treebranching factor increased. Even though game grows exponentiallysize support decreases 2.5% cases, behavior double-oraclealgorithms quite different. conjecture games longer sequences suitdouble-oracle algorithms better, since several actions form best-response sequencesadded single iteration. contrasts shallow game trees largebranching factors, iterations necessary add multiple actions. However,deeper analysis identify exact properties games suitable openquestion must analyzed normal-form games first.861fiBosansky, Kiekintveld, Lisy, & Pechoucek7. Conclusionpresent novel exact algorithm solving two player zero-sum extensive-form gamesimperfect information. approach combines compact sequence-form representation extensive-form games iterative algorithmic framework double-oraclemethods. integrates two successful approaches solving large scale gamesyet brought together general class games algorithm addresses.main idea algorithm restrict game allowing players playrestricted set sequences available sequences actions, iteratively expandrestricted game time using fast best-response algorithms. Although worstcase double-oracle algorithm may need add possible sequences, experimentalresults different domains prove double-oracle algorithm find exact Nashequilibrium prior constructing full linear program complete game. Therefore,sequence-form double-oracle algorithm reduces main limitation sequence-formlinear programmemory requirementsand able solve much larger games comparedstate-of-the-art methods. Moreover, since algorithm able identify sequencespromising actions without domain-specific knowledge, also provide significantruntime improvements.proposed algorithm also another crucial advantage compared current stateart. double-oracle framework offers decomposition problem computingNash equilibrium separate sub-problems, including best-response algorithms,choice default strategy, algorithms constructing solving restrictedgame. developed solutions sub-problems domain-independent manner. However, also view algorithm general frameworkspecialized domain-specific components take advantage structure specificproblems improve performance sub-problems. lead substantialimprovements speed algorithm, number iterations, well reducingfinal size restricted game. demonstrated potential domain-specificapproach game Phantom Tic-Tac-Toe. Another example fast best-responsealgorithms operate public tree (i.e., compact representation gamespublicly observable actions; see Johanson, Bowling, Waugh, & Zinkevich, 2011) exploited games like poker. Finally, formal analysis identifies key propertiesdomain-specific implementations need satisfy guarantee convergencecorrect solution game.algorithm opens large number directions future work. represents newclass methods solving extensive-form games imperfect information operatesdifferently common approaches (e.g., counterfactual regret minimization),many possible alternatives improve performance algorithm remaininvestigated. Examples include sophisticated calculation utility valuestemporary leaves, alternative strategies expanding restricted game, removingunused sequences restricted game. broader analysis using sequenceform double-oracle algorithm approximation technique performed, possiblyexploring alternative approximative best-response algorithms based sampling (e.g.,Monte Carlo) techniques.862fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect Informationalso several theoretical questions could investigated. First, performance double-oracle algorithm depends strongly number iterationssequences need added. However, theoretical question regarding expectednumber iterations thus speed convergence double-oracle algorithmexplored even simpler game models (e.g., games normal form).analysis simpler models needed identify general properties gamesdouble-oracle methods tend faster identify optimal way expandingrestricted game.AcknowledgementsEarlier versions paper published European Conference ArtificialIntelligence (ECAI) (Bosansky, Kiekintveld, Lisy, & Pechoucek, 2012) conferenceAutonomous Agents Multi Agent Systems (AAMAS) (Bosansky, Kiekintveld, Lisy,Cermak, & Pechoucek, 2013). major additions full version include (1) novel,detailed description parts algorithm, (2) introduction analysisdifferent policies player selection main loop double-oracle algorithm,(3) new experiments phantom tic-tac-toe domain together thoroughanalysis experimental results domains, including analysis convergencealgorithm, (4) experimental comparison CFR, finally (5) extended analysisrelated work.research supported Czech Science Foundation (grant no. P202/12/2054)U.S. Army Research Office (award no. W911NF-13-1-0467).ReferencesBarnhart, C., Johnson, E. L., Nemhauser, G. L., Savelsbergh, M. W. P., & Vance, P. H.(1998). Branch-And-Price: Column Generation Solving Huge Integer Programs.Operations Research, 46, 316329.Bosansky, B., Kiekintveld, C., Lisy, V., Cermak, J., & Pechoucek, M. (2013). Doubleoracle Algorithm Computing Exact Nash Equilibrium Zero-sum Extensiveform Games. Proceedings International Conference Autonomous AgentsMultiagent Systems (AAMAS), pp. 335342.Bosansky, B., Kiekintveld, C., Lisy, V., & Pechoucek, M. (2012). Iterative AlgorithmSolving Two-player Zero-sum Extensive-form Games Imperfect Information.Proceedings 20th European Conference Artificial Intelligence (ECAI), pp.193198.Cermak, J., Bosansky, B., & Lisy, V. (2014). Practical Performance RefinementsNash Equilibria Extensive-Form Zero-Sum Games. Proceedings EuropeanConference Artificial Intelligence (ECAI), pp. 201206.Dantzig, G., & Wolfe, P. (1960). Decomposition Principle Linear Programs. OperationsResearch, 8, 101111.Ganzfried, S., & Sandholm, T. (2013). Improving Performance Imperfect-InformationGames Large State Action Spaces Solving Endgames. Computer863fiBosansky, Kiekintveld, Lisy, & PechoucekPoker Imperfect Information Workshop National Conference ArtificialIntelligence (AAAI).Gibson, R., Lanctot, M., Burch, N., Szafron, D., & Bowling, M. (2012). Generalized Sampling Variance Counterfactual Regret Minimization. Proceedings 26thAAAI Conference Artificial Intelligence, pp. 13551361.Halvorson, E., Conitzer, V., & Parr, R. (2009). Multi-step Multi-sensor Hider-seeker Games.Proceedings Joint International Conference Artificial Intelligence (IJCAI),pp. 159166.Hoda, S., Gilpin, A., Pena, J., & Sandholm, T. (2010). Smoothing Techniques ComputingNash Equilibria Sequential Games. Mathematics Operations Research, 35 (2),494512.Jain, M., Conitzer, V., & Tambe, M. (2013). Security Scheduling Real-world Networks.Proceedings International Conference Autonomous Agents MultiagentSystems (AAMAS), pp. 215222.Jain, M., Korzhyk, D., Vanek, O., Conitzer, V., Tambe, M., & Pechoucek, M. (2011). DoubleOracle Algorithm Zero-Sum Security Games Graph. Proceedings 10thInternational Conference Autonomous Agents Multiagent Systems (AAMAS),pp. 327334.Johanson, M., Bowling, M., Waugh, K., & Zinkevich, M. (2011). Accelerating Best ResponseCalculation Large Extensive Games. Proceedings 22nd International JointConference Artificial Intelligence (IJCAI), pp. 258265.Koller, D., & Megiddo, N. (1992). Complexity Two-Person Zero-Sum GamesExtensive Form. Games Economic Behavior, 4, 528552.Koller, D., Megiddo, N., & von Stengel, B. (1996). Efficient Computation EquilibriaExtensive Two-Person Games. Games Economic Behavior, 14 (2), 247259.Koller, D., & Megiddo, N. (1996). Finding Mixed Strategies Small Supports Extensive Form Games. International Journal Game Theory, 25, 7392.Kreps, D. M., & Wilson, R. (1982). Sequential Equilibria. Econometrica, 50 (4), 86394.Lanctot, M. (2013). Monte Carlo Sampling Regret Minimization Equilibrium Computation Decision Making Large Extensive-Form Games. Ph.D. thesis, University Alberta.Lanctot, M., Gibson, R., Burch, N., Zinkevich, M., & Bowling, M. (2012). No-RegretLearning Extensive-Form Games Imperfect Recall. Proceedings 29thInternational Conference Machine Learning (ICML 2012), pp. 121.Lanctot, M., Waugh, K., Zinkevich, M., & Bowling, M. (2009). Monte Carlo SamplingRegret Minimization Extensive Games. Advances Neural InformationProcessing Systems (NIPS), pp. 10781086.Lee, C.-S., Wang, M.-H., Chaslot, G., Hoock, J.-B., Rimmel, A., Teytaud, O., Tsai, S.-R.,Hsu, S.-C., & Hong, T.-P. (2009). Computational Intelligence Mogo RevealedTaiwans Computer Go Tournaments. IEEE Transactions Computational Intelligence AI Games, 1, 7389.864fiAn Exact Double-Oracle Algorithm Zero-Sum EFGs Imperfect InformationLetchford, J., & Vorobeychik, Y. (2013). Optimal Interdiction Attack Plans. Proceedings 12th International Conference Automonous Agents MultiagentSystems (AAMAS), pp. 199206.Lisy, V., Kovarik, V., Lanctot, M., & Bosansky, B. (2013). Convergence Monte Carlo TreeSearch Simultaneous Move Games. Advances Neural Information ProcessingSystems (NIPS), Vol. 26, pp. 21122120.McMahan, H. B. (2006). Robust Planning Domains Stochastic Outcomes, Adversaries, Partial Observability. Ph.D. thesis, Carnegie Mellon University.McMahan, H. B., & Gordon, G. J. (2007). Fast Bundle-based Anytime AlgorithmPoker Convex Games. Journal Machine Learning Research - ProceedingsTrack, 2, 323330.McMahan, H. B., Gordon, G. J., & Blum, A. (2003). Planning Presence CostFunctions Controlled Adversary. Proceedings International ConferenceMachine Learning, pp. 536543.Miltersen, P. B., & Srensen, T. B. (2008). Fast Algorithms Finding Proper StrategiesGame Trees. Proceedings Symposium Discrete Algorithms (SODA), pp.874883.Miltersen, P. B., & Srensen, T. B. (2010). Computing Quasi-Perfect EquilibriumTwo-Player Game. Economic Theory, 42 (1), 175192.Pita, J., Jain, M., Western, C., Portway, C., Tambe, M., Ordonez, F., Kraus, S., & Parachuri,P. (2008). Deployed ARMOR protection: Application Game-Theoretic ModelSecurity Los Angeles International Airport. Proceedings 8th International Conference Autonomous Agents Multiagent Systems (AAMAS), pp.125132.Ponsen, M. J. V., de Jong, S., & Lanctot, M. (2011). Computing Approximate Nash Equilibria Robust Best-Responses Using Sampling. Journal Artificial IntelligenceResearch (JAIR), 42, 575605.Sandholm, T. (2010). State Solving Large Incomplete-Information Games,Application Poker. AI Magazine, special issue Algorithmic Game Theory, 1332.Selten, R. (1975). Reexamination Perfectness Concept Equilibrium PointsExtensive Games. International Journal Game Theory, 4, 2555.Selten, R. (1965). Spieltheoretische Behandlung eines Oligopolmodells mit Nachfragetrgheit[An oligopoly model demand inertia]. Zeitschrift fur die Gesamte Staatswissenschaft, 121, 301324.Shafiei, M., Sturtevant, N., & Schaeffer, J. (2009). Comparing UCT versus CFR Simultaneous Games. IJCAI Workshop General Game Playing.Shieh, E., An, B., Yang, R., Tambe, M., Baldwin, C., Direnzo, J., Meyer, G., Baldwin, C. W.,Maule, B. J., & Meyer, G. R. (2012). PROTECT : Deployed Game Theoretic SystemProtect Ports United States. International Conference AutonomousAgents Multiagent Systems (AAMAS), pp. 1320.865fiBosansky, Kiekintveld, Lisy, & PechoucekShoham, Y., & Leyton-Brown, K. (2009). Multiagent Systems: Algorithmic, GameTheoretic, Logical Foundations. Cambridge University Press.Tambe, M. (2011). Security Game Theory: Algorithms, Deployed Systems, LessonsLearned. Cambridge University Press.Tsai, J., Rathi, S., Kiekintveld, C., Ordonez, F., & Tambe, M. (2009). IRIS - ToolStrategic Security Allocation Transportation Networks Categories SubjectDescriptors. Proceedings 8th International Conference Autonomous AgentsMultiagent Systems (AAMAS), pp. 3744.van Damme, E. (1984). Relation Perfect Equilibria Extensive Form GamesProper Equilibria Normal Form Games. Game Theory, 13, 113.van Damme, E. (1991). Stability Perfection Nash Equilibria. Springer-Verlag.von Stengel, B. (1996). Efficient Computation Behavior Strategies. Games EconomicBehavior, 14, 220246.Wilson, R. (1972). Computing Equilibria Two-Person Games Extensive Form.Management Science, 18 (7), 448460.Zinkevich, M., Johanson, M., Bowling, M., & Piccione, C. (2008). Regret MinimizationGames Incomplete Information. Advances Neural Information ProcessingSystems (NIPS), 20, 17291736.Zinkevich, M., Bowling, M., & Burch, N. (2007). New Algorithm Generating EquilibriaMassive Zero-Sum Games. Proceedings National Conference ArtificialIntelligence (AAAI), pp. 788793.866fiJournal Artificial Intelligence Research 51 (2014) 725778Submitted 04/14; published 12/14Tutorial Structured Continuous-Time Markov ProcessesChristian R. Sheltoncshelton@cs.ucr.eduUniversity California, RiversideGianfranco Ciardociardo@iastate.eduIowa State UniversityAbstractcontinuous-time Markov process (CTMP) collection variables indexedcontinuous quantity, time. obeys Markov property distributionfuture variable independent past variables given state present time.introduce continuous-time Markov process representations algorithms filtering,smoothing, expected sufficient statistics calculations, model estimation, assumingprior knowledge continuous-time processes basic knowledge probabilitystatistics. begin describing flat unstructured Markov processes movestructured Markov processes (those arising state spaces consisting assignmentsvariables) including Kronecker, decision-diagram, continuous-time Bayesian networkrepresentations. provide first connection decision-diagrams continuoustime Bayesian networks.1. Tutorial Goalstutorial intended readers interested learning continuous-time Markovprocesses, particular compact structured representations them. assumedreader familiar general probability statistics knowledgediscrete-time Markov chains perhaps hidden Markov model algorithms.tutorial deals Markovian systems, require variables observed. Thus, hidden variables used model long-range interactionsamong observations. models, given instant assignment state variables sufficient describe future evolution system. variablesreal-valued (continuous) times. consider evidence observations regularlyspaced, irregularly spaced, continuous intervals. evidence patterns changemodel variable time.deal exclusively discrete-state continuous-time systems. Real-valued variablesimportant many situations, keep scope manageable, treathere. refer work Sarkka (2006) machine-learning-oriented treatmentfiltering smoothing models. literature parameter estimationscattered. constrain discussion systems finite states, althoughmany concepts extended countably infinite state systems.concerned two main problems: inference learning (parameter estimation). chosen familiar applicable researchersartificial intelligence. points also discuss computation steady-state properties, especially model research concentrates computation.c2014AI Access Foundation. rights reserved.fiShelton & Ciardofirst section (Section 2) covers basics flat (unstructured state-space) continuoustime Markov processes. remaining sections discuss compact representations. tutorials goal make mathematical foundations clear lay current researchlandscape detailed papers read easily.1.1 Related Modelsmany non-Markov continuous time models. Gaussian processes (Williams, 1998)best-known model continuous-valued processes. discrete-valued processes,models build upon Poisson processes, general marked processes. Poissonprocess memoryless, make interesting model, researchers usually generalize allowrate event function processs history.Poisson networks (Rajaram, Graepel, & Herbrich, 2005) constrain function depend counts number events (possibly different event types)finite time window. cascade Poisson process model (Rajaram et al., 2005) definesrate function sum kernel applied historic event. kernelparameters effect time passing, overall event rate, chance one typeevent follows another. Piecewise-constant intensity models (PCIMs) (Gunawardana, Meek,& Xu, 2012; Parikh, Gunamwardana, & Meek, 2012) define intensity functiondecision tree, internal nodes tests drawn set pre-specified binary testshistory. Forest-based point processes (Weiss & Page, 2013) extend allowingintensity function product set functions, PCIM-like tree. Didelez(2008) presents generalization continuous-time Bayesian networks (see Section 5)inhomogeneous point processes, without specific parameterizations algorithms.1.2 Continuous TimeContemporary computers discrete-time computation engines (or least present modelone). Therefore, would consider continuous-time model? quickest answeranalogy: build models non-temporal systems employing real-valued variables.tools linear algebra, calculus, like allow us derive analyze algorithmsmethods. Yet, end implemented discrete-valued computersfinite memory precision. However, find abstraction continuous-valued variables useful make approximations final implementation employingfixed- floating-point precision arithmetic.Similarly, productive treat time continuous quantity. allows usnaturally discuss reason systems1. Events, measurements, durations irregularly spaced,2. Rates vary orders magnitude,3. Durations continuous measurement need expressed explicitly.happen asynchronous systems. dynamic systems interest asynchronous: events measurements (or both) occur based global clock. Socialnetworks, phylogenetic trees, computer system logs examples.Note underlying system model continuous-time, observations measurements model need continuous. directly treat discrete-time observations, regular irregular intervals.726fiTutorial Structured Continuous-Time Markov Processes1.3 Discrete TimeClearly given continuous-time system specification, discretization time values could made without introducing much approximation error. conversiontime real-valued integral makes mathematically difficult flexibletreat time algorithmically. makes development computationallyefficient algorithms difficult. instance, discrete-time model, naturalcomputations proceed one time step time. However, uneventful times,computationally overly burdensome. continuous-time model,natural time step, simpler think methods jumpuneventful time periods. Additionally, oddities Markov chains builtdiscretizing continuous time. Finally, full system specification may knowndiscretization must selected (for instance, parameters must estimated).1.3.1 Time Discretization Markovian-nessConsider two-state Markov chain X described stochastic matrix10.75 0.25.T1 =0.5 0.5(1)elements 1 probabilities p(Xt | Xt1 ) value Xt Xt1 .one time unit, probability moving state 1 state 2 0.25, example.1 describes continuous-time system, sampled period 1 time unit,matrix 1/2 describing system, sampled period 12 time unit(or twice sampling rate). Indeed is:0.83 0.17.(2)1/2 =0.33 0.67verified:P (Xt = j | Xt1 = i) =XP (Xt1/2 = k | Xt1 = i)P (Xt = j | Xt1/2 = k)k1 (i, j) =X1/2 (i, k)T 1/2 (k, j)k1 = 1/2 1/2 .is, 1/2 matrix square root 1 .take different two-state Markov chain transition matrix0.1 0.9S1 =0.9 0.1construct corresponding transition matrix half sampling period, 1/2 :0.5 + .447i 0.5 .447i1/2 =.0.5 + .447i 0.5 .447i(3)(4)1. use row-stochastic matrices exclusively tutorial. column-stochastic matricesoften used discrete time, row-stochastic matrices common continuous time.727fiShelton & Ciardox1x2x3x1x3y1y2y3y1y3z1z2z3z1z3x1x3y1y3z1z3(a) unrolled DBN(b) marginalized DBNFigure 1: Example (a) DBN unrolled, (b) DBN marginalized twicesampling periodicityreal-valued stochastic matrix describing processes 1 , halfsampling periodicity. Put differently, two-state continuous-time Markov systemsampled rate 1 time unit produces Markov chain transitionmatrix 1 .problem generating 1/2 arises 1 negative eigenvalue (by contrast,eigenvalues 1 positive). general, stochastic matrices positiveeigenvalues correspond continuous-time Markov process sampled given periodicity.viewed two ways. First, means set continuous-time Markovprocesses smaller set discrete-time Markov processes. Second, meansprocesses Markovian sampled particular periodicityway extend time points outside periodicity would constructnon-Markovian (and non-stationary) process.periodicity discrete-time Markov chain inherent process,result concern. However, many systems natural sampling rate.rate chosen computational measurement convenience. case, mustcareful employ Markovian assumption. Or, directly modelunderlying system continuous time.1.3.2 Independencies Markovian-nesssimilar problem arises independencies. describe problem termsdynamic Bayesian networks (DBNs) (Dean & Kanazawa, 1989). unfamiliar DBNs,reader may skip next section.Consider DBN Figure 1(top,a), unrolled one time step.marginalize middle time slice result DBN Figure 1(top,b):model, twice sampling periodicity. However, perhaps wish goopposite direction (to half sampling periodicity). Figure 1(bottom,b) shows DBN728fiTutorial Structured Continuous-Time Markov Processes0Test Log-Likelihood20406080optimalCTBN100= 0.1= 1.0= 5.0120246Number Trajectories810Figure 2: Comparison learning DBNs different time-slice widthstwo time units. DBN graph structure one time unit would marginalizegraph structure. may DBN, independencies expressedgraph structure two time units expressible graph structure halfsampling periodicity. independencies would buried parameters DBN(if parameters possible, given previous discussion). meansindependencies expressed DBNs graph function underlying processsampling rate.1.3.3 Structure LearningSelection sampling rate theoretical problem. Nodelman, Shelton, Koller(2003) demonstrate problem parameter estimation. particular, considereddata drawn continuous-time Markovian process eight variables (mostly binary).resulting trajectories discretized time according parameter DBNs(including structure) learned setting. Figure 2 shows test log-likelihoodaccuracy function number training trajectories t. also showsresult discretizing time (the CTBN line, model explained Section 5).surprising CTBN model best (as datagenerated model), instructive best depends numberobserved trajectories. means that, sampling periodicity model parameter,choice cannot made independently amount data used estimate DBN.2. Continuous-Time Markov Processescontinuous-time Markov process (CTMP) distribution trajectories. trajectory(or sample) CTMP right-continuous piece-wise constant function real-valuedvariable, time. Figure 3 illustrates example trajectories. states natural order729fiShelton & Ciardo321(a) ordered states(b) unordered states(c) multiple state variablesFigure 3: Example continuous-time Markov process samples (trajectories)ing, Figure 3(a) might natural depiction. states ordered, Figure 3(b)depicts sample three-state system. later sections consideringlarge factored state spaces state assignment multiple variables. Figure 3(c) depicts trajectory.finite CTMP defines set random variables, finite sample space(the state space), indexed real-value usually denoted time. Let Xprocess. Markovian property statesX(t1 ) X(t3 ) | X(t2 ), t1 < t2 < t3 .(5)Throughout tutorial, describe distributions continuous discrete random variables. use lowercase letters densities continuous randomvariables uppercase letters probabilities discrete random variables.2.1 Parameterizationparameterize CTMP X starting distribution time = 0, P (X(0)) (andrestrict 0) intensity matrix QX (or Q context clear). startingdistribution discrete-time Markov chain, largely ignore it.intensity matrix analogous transition matrix discrete-time process.2.1.1 Comparison Discrete-TimeConsider following (roughly equivalent) discrete-time transition matrix continuoustime intensity matrix Q:0.5 0.2 0.30.80.320.480.12 .= 0.1 0.8 0.1Q = 0.12 0.240.2 0.1 0.70.270.13 0.4interpret row two ways. first row could viewed statingprocess state 1, next time step 0.5 chancestate 1, 0.2 chance state 2, 0.3 chance state 3.Alternatively, could viewed stating process state 1, remainnumber steps geometrically distributed: Pr(stay n steps) = 0.5n . And,leaves transition state 2 probability 0.2/0.5 = 0.4 state 3probability 0.3/0.5 = 0.6.intensity matrix Q two similar interpretations. first row statesprocess state 1, short period time , approximately 1 0.8 chancestate 1, 0.32 chance state 2, 0.48 chance state730fiTutorial Structured Continuous-Time Markov Processes3. approximation error O(2 ). Alternatively, states process state1 remains duration exponentially distributed: p(dwell time = t) = 0.8e0.8t .leaves, transition state 2 probability 0.32/0.8 = 0.4 state3 probability 0.48/0.8 = 0.6.2.1.2 Racing Exponentialsalso view row matrix describing racing exponential distributions.two important properties exponential distribution. First, memoryless:pZ (t) = pZ (t + s|Z > s)Z exponentially distributed(6)thus right distribution dwell times Markovian process. (The amounttime process stayed state affect remaining dwell time.)also closed minimization: Given collection random variables Z1 , Z2 , . . . , Zn ,pZi (t) = ri eri(7)= min Zi(8)J = arg min Zi(9)impliespY (t) = rertrjPr(J = j) =r(10)(11)r=nXri .(12)i=1is, set exponential distributions (potentially) different rates,minimum (the time earliest one) also exponentially distributed rate equalsum component rates. Furthermore, component causes minimumindependent time proportional components rate. Thus, viewrow matrix set racing exponential distributions: one potentialnext state rates dictated off-diagonal elements. Whichever potential transitionhappens first one actually taken process, rest discarded.2.1.3 Event DecompositionFurther, use build interpretation summation two intensitymatrices. Q = Q1 + Q2 , Q1 Q2 valid intensity matrices,process Q viewed combination processes Q1 Q2sub-processes race produce next transition: current state, twosub-processes rate transition possible new states. Whichevertransition happens first switches state joint process continues new731fiShelton & Ciardostate. also view two different event types intensity matrix.process Q joint process running events time, throwingaway (marginalizing out) event types associated transitions, leavingtransitions themselves.2.1.4 Infinitesimal Rate Semanticsformally, dynamics n-state CTMP described n-by-n intensity matrixQ. diagonal elements Q non-positive non-diagonal elements Qnon-negative. rows Q sum 0 (thus diagonal elements negative rowsums, diagonal element excluded sum). denote i, j elementQ qi,j . Further, notational simplicity, let qi = qi,i . is, qi rateleaving state i, absolute value corresponding diagonal element.let p(t) row-vector marginal distribution process time t,semantics Q statedp(t + ) = p(t)(I + Q) + o() .(13)impliesp(t + ) p(t) = p(t)Q + o()(14)lim (p(t + ) p(t))/ = p(t)Q(15)dp(t)= p(t)Qdt(16)0(17)first-order linear homogeneous differential equation solutionp(t + s) = p(t)eQs(18)assuming > 0 initial conditions t, p(t), known. exponentialmatrix exponential, defined Taylor expansion:eQs =kXk=0k!Qk .(19)Although often practical computationally, also express matrix exponentialterms eigenvalues ({i }) corresponding right left eigenvectors ({vi }{wi }) Q:XeQs =ei vi wi> .(20)Q finite size irreducible (there positive-rate path statestate), process ergodic (the notion cycling behavior existcontinuous-time Markov processes) exactly one eigenvalue equal 0.corresponding right eigenvector unique steady-state (or stationary) distributionprocess. process ergodic, may multiple 0 eigenvaluesunique stationary distribution. eigenvalues less 0 correspondtransients system. Therefore, Q always negative semi-definite.732fi0.60.60.50.5Pr(state)Pr(state)Tutorial Structured Continuous-Time Markov Processes0.40.30.20.30.2024time60.180.60.60.50.5Pr(state)Pr(state)0.10.40.40.30.20.1024time68024time680.40.30.2024time60.18fixed step-sizeadaptive step-sizeFigure 4: Propagation marginal distribution time 0 time 8 Euler integration. Left: fixed step-size. Right: adaptive step-size. Top: 11 evaluation points.Bottom: 5 evaluation points.2.2 Matrix Exponentialmatrix exponential plays critical role many aspects reasoning continuoustime dynamic systems. first, would seem significant downside, relativediscrete-time systems. Propagation distribution p (as vector) n time steps discretetime system requires multiplication n (if stochastic transition matrix).contrast, operation continuous-time requires calculation matrixexponential, infinite sum matrix powers.Consider computing marginal distribution time integrating differentialequation Equation 18. simplest method would use Euler integrationfixed step size t. amounts propagating fixed time interval multiplying= + tQ. essentially discretizing time approximatingstochastic matrix resulting interval. propagate time requires t/t matrixmultiplications. shown left side Figure 4.However, time continuous, need limit time steps uniformsize. choose adaptive step size, achieve accuracy fewerevaluation points. Figure 4 demonstrates simplest adaptive scheme (Euler stepsstep size proportional derivative magnitude). Note numbersteps (computational complexity), accuracy adaptive steps better.real applications, would use advanced differential equation solverintelligent step size selection; see work Press, Teukolsky, Vetterling, Flannery (1992) introduction. Yet, idea essentially same: take largersteps less interesting time periods. something similar discrete timewould require computations essentially convert discrete-time system continuous733fiShelton & Ciardotime one. Techniques like squaring scaling multiply large matrix powers alsoapplied matrix exponential. full discussion matrix exponential calculations, refer excellent treatments Moler Loan (2003) Najfeld Havel(1994, 1995).2.3 UniformizationUniformization (also called randomization) method converting questionscontinuous-time Markov process ones discrete-time one (Grassmann, 1977).Given intensity matrix Q, uniformization constructs stochastic matrix= Q/ +(21)maxi qi (that is, less largest rate Q). example,following uniformization = 0.5 (the smallest possible value ).0.50.10.40.0 0.2 0.80.1 = 0.2 0.6 0.2Q = 0.1 0.2(22)0.20.1 0.30.4 0.2 0.4resulting stochastic matrix interpreted discrete-time process. Howeverequivalent sampling continuous-time process uniform intervals. it,general, equivalent embedded Markov chain (the sequence states, discardingtimes transitions). former achieved matrix exponential latterachieved setting diagonal elements Q zero normalizing rowsum one.Rather, discrete-time process associated stochastic matrix relatedcontinuous-time process associated intensity matrix Q following way.Consider procedure (1) sampling event times Poisson process rate(that is, times consecutive events independently identically distributedexponential distribution rate ), (2) sampling state transitionsevent times discrete-time process described , (3) discardingself transitions. procedure produces samples distribution originalCTMP described Q.transformation useful simulation (sampling), also understandingcomputing matrix exponential. intensity matrix Q negative semi-definite,Taylor expansion matrix exponential unstable, sign termsalternates. However, fix using instead Q. reworking Equation 21,note Q = (M I). writeeQt = e(M I)t(23)=e= eteXk tk(24)Mk(25)k tketMkk!{z }k=0 |(26)k=0=[eA+B = eA eB AB = BA]k!Xk734fiTutorial Structured Continuous-Time Markov Processess0s1t0t1s2t2s3s4t4t3t5Figure 5: Pictorial representation finite-length sample CTMP.k probability exactly k events Poisson process ratetime t. series stable (M positive semi-definite) finite numberterms, sum quasi-probability vector (it non-negative sums less1). missing probability bound error. sequence k growsdecays. Therefore, discarding tail series, also early terms speedcomputations. Fox Glynn (1988) give method compute left right boundsk ensure desired error tolerance.Note that, Q represents ergodic continuous-time Markov process, represents ergodic discrete-time Markov process strictly greater maxi qi (asufficient, necessary condition). ergodic, stationary distributionQ stationary distribution .2.4 Likelihoodcomplete finite-length sample (trajectory) Tr CTMP sequence statestimes transitions, plus ending time: Tr = {(s0 , t0 ), (s1 , t1 ), . . . , (sn1 , tn1 )}, tn .Figure 5 shows pictorial representation, n = 5. use conventionprocess starts time 0, t0 must equal 0.likelihood sample product conditional probabilities event(the starting state, dwell duration, state transition):pr last durationdensity duration pr transinit dist}|{zz}|{}|{zn2z}|{Yq,si+1qsn1 (tn tn1 )qsi (ti+1 ti )p(Tr) = Pr(X(t0 ) = s0 )e(27)qs eqsii=0= P0 (s0 )n1qsi (ti+1 ti )ei=0ln p(Tr) = ln P0 (s0 )n2qsi ,si+1(28)i=0n1Xqsi (ti+1 ti ) +i=0n2Xln qsi ,si+1(29)i=0let P0 distribution starting state process. Note time tnprocess transition. Rather, observe process remains state sn1duration least tn tn1 .Equation 29 rewrittenXXln p(Tr) = ln P0 (s0 )[s]qs +N [s, s0 ] ln qs,s0(30)s6=s0[s] total time spent state N [s, s0 ] total number transitionss0 , functions Tr. demonstrates CTMP735fiShelton & Ciardomember exponential family sufficient statistics [] N [, ] (plusrelevant sufficient statistics starting distribution), natural parametersdiagonal elements intensity matrix logarithm non-diagonalelements. likelihood multiple trajectories form, [s] N [s, s0 ]sums sufficient statistics individual trajectories.2.4.1 Parameter Estimationmaximum likelihoodP parameters easily derived differentiating Equation 30,replacing qs s0 6=s qs,s0 :ln p(Tr)N [s, s0 ]=[s]qs,s0qs,s0s0 6=(31)s0 6=(32)(33)implies ML parametersqs,s0 = N [s, s0 ]/T [s]Xqs =N [s, s0 ]/T [s]s0 6=smaximum posteriori (MAP) estimate calculated place suitable prior distributions parameters. particular, put independent gamma distributionprior independent parameters, qs,s0 , 6= s0 :p(qs,s0 ; s,s0 , s,s0 ) =s,ss,s00 +1(s,s0 + 1)0qs,ss,s0 eqs,s0 s,s0(34)parameters s,s0 s,s0 . posterior distribution parameters givendata summarized sufficient statistics [s] N [s, s0 ] also gamma-distributedparameters s,s0 + N [s, s0 ] s,s0 + [s]. Thus, MAP estimates parametersqs =X N [s, s0 ] + s,s0s0qs,s0 =[s] + s,s0N [s, s0 ] + s,s0.[s] + s,s0(35)(36)2.5 Inferenceconsider two classic problems reasoning temporal systems: filteringsmoothing. Initially, assume observations (evidence) pattern likeFigure 6: sequence times {t0 , t1 , . . . , tk } sequence evidences {e1 , e2 , . . . , ek }.assume know prior marginal distribution X(t0 ), either previousreasoning t0 = 0.Filtering task computing p(X(t) | e1 , e2 , . . . , ek ) tk . evidencepoint observation state system, Markovian property processmakes inference trivial. Instead assume ei probabilistically relatedX(ti ) independent everything else given X(ti ). (This analogous discrete-time736fiTutorial Structured Continuous-Time Markov Processest0e1e2e3t1t2t3Figure 6: Example evidence pattern, point evidence.hidden Markov model.) Thus view observation noisy measurementsystem.hidden Markov model, define recursive filtering solution using forwardmessage whose components defined(t) = Pr X(t) = i, e[t0 ,t)(37)denote e[s,t) = {(ti , ei ) | ti < t}: set evidence interval [s, t).analogy also define e[s,t] e(s,t] evidence [s, t] (s, t] respectively(which need later). Note row vector probabilities, one statesystem. Recursive calculation derivedXPr X(t) = j, e[t0 ,t) =Pr X(s) = i, e[t0 ,s) Pr X(t) = j, e[s,t) | X(s) = t0 <(38)t0 <(39)(t) = (s)F (s, t)second equation vector version first equation, matrix F (s, t)element i, j equal Pr X(t) = j, e[s,t) | X(s) = .evidence [s, t), F (s, t) = eQ(ts) . Thus, propagate distributionone evidence time point next matrix exponential. propagate acrossevidence times, define+ (t) = Pr X(t), e[t0 ,t](40)(t), including evidence t. evidence t, twovectors same. evidence t, + (t) (t), exceptelement multiplied probability evidence time point.let (i) diagonal matrix diagonal element j Pr(ei | X(ti ) = j),recurrence written(t0 ) = + (t0 ) = given(41)+(ti ) = (ti1 )eQ(ti ti1 )+ (ti ) = (ti )O (i)+(t) = (ti )eQ(tti )0<ik(42)0<ik(43)ti < ti+1 ti < t, = k(44)Equation 42 special case Equation 44. propagates one evidencetime next. Equation 43 propagates across evidence point.737fiShelton & CiardoEquation 44 used construct filtered estimate non-evidence timenormalizing (t) sum 1 (dividing probability evidence prior t).Finally, note similar set recurrences derived F (, ). result allowspropagation distribution across time intervals includes evidence;is, restricted particular initial condition, (t0 ). However, singlepropagated, computing F first computationally expensive.2.5.1 Complex Evidencefiltering equations inhomogeneous hidden Markov model (that is,transition matrix constant) familiar employed hidden Markovmodels. However, continuous time, evidence patternsdirect corresponding analogies discrete-time. evidence consists finite numberobservations, convert similar form, breaking time intervalsconstant evidence.instance, might observe system subset states durationtime: time interval, system leave subset,observe whether transitions within subset. augment evidenceinclude information. interval [ti1 , ti ), let Si denote subset statesevidence constrains system. constraints, Si fullstate space. time points change Si , point evidence, (i)identity matrix (inducing change filtering estimate). Si (i) maynon-trivial i.propagate ti1 ti , must use modified intensity matrix. particular,set zero rate inconsistent evidence Si : rates to, from,within set states Si . Let Q(i) denote matrix. rowscolumns permuted states Si upper left corner,matrix formQ(i)QSi=000(45)QSi submatrix Q rows columns corresponding Si . Additionally,modify (i1) , setting 0 diagonal elements corresponding states Si .Note Q(i) (strictly) intensity matrix: rows sum 0.general, diagonal element greater (in absolute value) sum rowelements set zero non-diagonal rates. missing rate correspondsprobability leaving evidence set (and therefore conforming evidence).eQ(ti ti1 ) stochastic matrix representing conditional distribution time ti(i)given state time ti1 , eQ (ti ti1 ) substochastic matrix (the row sums lessequal 1), sum row probability evidenceinterval, given state time ti1 .738fiTutorial Structured Continuous-Time Markov Processesnew filtering recurrence(t0 ) = given(46)(i)(ti ) = + (ti1 )eQ+(ti ) = (ti )O(ti ti1 )(i)(t) = + (ti )eQ(i+1)(tti )0<ik(47)0<ik(48)ti < ti+1 ti < t, = k .(49)might also observe transition exact time point. generally, time timight observe transition occurred one state set Ui one state setUi+ (without knowing exactly states within sets). case, elements (t)probabilities duration lasting least t, + (t) probability density duration lasting exactly t. difference probabilitytail exponential density point multiplicationrelevant rate q. Thus, type evidence, modify (i) . particular,(qj,k j Ui , k Ui+ , j 6= k(i)j,k =.(50)0otherwiserecurrence remains same, new definition (i) . evidence typesalso possible derived types augmenting state space.2.5.2 SmoothingSmoothing problem calculating Pr X(t) | e[t0 ,tk ] t0 tk . commonMarkov processes, notePr X(t) | e[t0 ,tk ] Pr X(t) | e[t0 ,t) Pr e[t,tk ] | X(t)(51)constant proportionality found noting sum Equation 51value X(t) must equal 1. first term right calculated() recurrence above. second term calculate backward message recurrence.Define(t) = Pr e[t,tk ] | X(t) =(52)+(t) = Pr e(t,tk ] | X(t) =(53)let column vector, backward recurrence analogous forward one,right multiplication instead left multiplication:+ (tk ) = 1(ti ) =(i)+(ti )(i)(ti ti1 )(i)(ti t)+ (ti1 ) = eQ(t) = eQ(ti )(ti )vector 1s(54)0<ik(55)0<ik(56)ti1 < ti .(57)time t, vector distribution state system givenevidencep(X(t) | e[t0 ,tk ] ) (t) fi (t)(58)fi Hadamard (point-wise) product.739fiShelton & Ciardo2.6 Parameter Estimation Incomplete EvidenceSection 2.4.1 demonstrated CTMP member exponential family sufficient statistics [i] (the amount time spent state i) N [i, j] (the numbertransitions j). evidence trajectories fully observed continuousinterval time, sufficient statistics trivially tallied. Further,evidence trajectory observed = 0, sufficient statistics initial distributionalso directly observed.However, portions interval hidden, generally observationsform previous section, direct likelihood maximization feasible.two basic approaches maximum likelihood estimation case: gradient ascentexpectation maximization (EM).gradient ascent, replace sufficient statistics Equation 31expected values standard argument exponential models applies: Let Tr partiallyobserved trajectory let h stand potential completion it.ln p(Tr) = lnXeln p(Tr,h)(59)hln p(Tr)1 Xln p(Tr, h)=p(Tr, h)qi,jp(Tr)qi,jhN [i, j]= Eh|Tr[i]qi,jN [i, j]=[i]qi,j(60)(61)(62)N [i, j] [i] expected values N [i, j] [i] respect completionsTr. EM, similarly replace N [i, j] [i] Equation 32 N [i, j] [i].therefore left problem computing expected values N [i, j] [i].Full derivations shown work Nodelman et al. (2003). quick version[i]Ztk[i] =t0=p(X(t) = | e[t0 ,tk ] ) dt1p(Tr)Z(63)tk(t)i (t) dt .(64)t0expected value N [i, j] similar form:qi,jN [i, j] =p(Tr)Ztk(t)j (t) dt +t0X (tl )O (l) i,j j+ (tl )P(l)i0 ,j 0 i0 ,j 0lTrans(65)Trans set evidence indices time transition (perhaps partially)observed: first term handles unobserved transitions second handles (partially)observed transitions.740fiTutorial Structured Continuous-Time Markov Processeslet i,j matrix zeros, except single one location (i, j),integrals Equation 64 Equation 65 formZtk(t)j (t) dt =t0=k ZXtll=1 tl1k Z tlXl=1(t)j (t) dt(i)+ (tl1 )eQ(66)(ttl1 )(i)i,j eQ(tl t)(tl ) dt .(67)tl1Thus, forward backward pass calculate + (i) (i) evidencechange point i, integral relatively simple. solved standard quadraturemethods solution differential equation (Asmussen, Nerman, & Olsson, 1996).Alternatively, calculation usually results values various timepoints, interpolated full functions used directly solve integrals.3. Kronecker Algebra Representationsnumber states thousand, methods computationally feasible modern computer. However, models described termsassignments variables. Thus number states grows exponentially numbervariables. tens variables, must seek compact representations.remainder paper, consider state space process Xassignment L variables, {X1 , X2 , . . . , XL }. Qlet variable Xi ni possibleassignments. Thus, total state space size n = Li=1 nl . let bold x standstate (joint assignment L variables), component xi assignmentvariable state x. state space often referred factored structuredvariable-based.Kronecker products sums natural basic operations build compact representations process intensities. cases, compact representationsnaturally describe transition rates, naturally describe diagonal elements Q (the negative rates leaving state). Thus, define RQ, except zeros diagonal position. diagonals reconstructednon-diagonal elements row, information content same.3.1 Kronecker Productfirst basic operation Kronecker product. Given matrices A(1) , A(2) , . . . , A(K)A(k) general size mk -by-nk , Kronecker product writtenA=KA(k)(68)k=1QQ(the result) m-by-n matrix: = k mk n = k nk . elementsrepresent possible multiplications one element A(1) , A(2) , . . . , A(K) . LetMk = {1, 2, . . . , mk } Nk = {1, 2, . . . , nk }, valid indices matrix A(k) .741fiShelton & Ciardo"Given =a00 a01#a10 a11a00 b00a00 b10a00 b20Ba01 BB = 00=a10 Ba11 Ba10 b00b10 10a10 b20b00 b01 b02B = b10 b11 b12b20 b21 b22a00 b01 a00 b02 a01 b00 a01 b01 a01 b02a00 b11 a00 b12 a01 b10 a01 b11 a01 b12a00 b21 a00 b22 a01 b20 a01 b21 a01 b22a10 b01 a10 b02 a11 b00 a11 b01 a11 b02a10 b11 a10 b12 a11 b10 a11 b11 a11 b12a10 b21 a10 b22 a11 b20 a11 b21 a11 b22Figure 7: Example Kronecker productThen, let Ir mapping M1 M2 MK {1, 2, . . . , m} let Ic similarlydefined mapping N1 N2 NK {1, 2, . . . , n}. matter usuallymappings are, convention take lexicographicorderings (orPmixed-basenumberingindex).instance(i,,...,)=r12Kk1:k11kKQma:b = akb mk .AIr (i1 ,i2 ,...,iK ),Ic (j1 ,j2 ,...,jK ) =KA(k) ik ,jk .(69)k=1notation makes appear complex, concept simple. Figure 7 demonstratessimple example. terms sparsity (one measure structure), Kronecker productnumber non-zero elements equal product number non-zero elementsinput matrix.Kronecker product analogous factor product (in Bayesian network terminology)treat operand matrix factor two different variables (and matricesshare variables), result matrix factor half variablesflattened column dimension half flattened rowdimension.terms distributions, Kronecker product represents independence. Given twovariables X1 X2 marginal distributions represented vectors v 1 v 2 ,v 1 v 2 joint distribution X1 X2 . particular, independentjoint distribution marginals v 1 v 2 .terms rate matrix, Kronecker product represents synchronization (Plateau,1985). two variables, X1 X2 rate2 matrices R1 R2 , R1 R2rate matrix state space X = X1 X2 (joint assignments X1 X2 ).represents rate matrix changes state X1 must occur timestate X2 (both variables changed every transition).2. hold generally intensity matrices, Kronecker product anythingsensible diagonal elements.742fiTutorial Structured Continuous-Time Markov ProcessesB = I3 + I2 B =a0,0a0,1b0,0 b0,1 b0,2b1,0 b1,1 b1,20,00,1b2,0 b2,1 b2,20,10,0+a1,0a1,1b0,0 b0,1 b0,2a1,0a1,1b1,0 b1,1 b1,2a1,0a1,1b2,0 b2,1 b2,2=a0,0+b0,0b0,1b0,2a0,1a0,1b1,0a0,0+b1,1b1,2b2,0b2,1a0,0+b2,2a0,1a1,0a1,1+b0,0b0,1b0,2a1,0b1,0a1,1+b1,1b1,2b2,0b2,1a1,1+b2,2a1,0Figure 8: Example Kronecker sum, given matrices B Figure 7. Zerosomitted. Note non-zero off-diagonal entries correspond onetwo indices (into B) changing.3.2 Kronecker SumKronecker operation Kronecker sum. defined square matrices.Given square matrices A(1) , A(2) , . . . , A(K) A(k) size nk -by-nk , Kroneckersum defined terms Kronecker product:A=Kk=1(k)=KXn1 n2 . . . nk1 A(k) nk+1 nK(70)k=1n identity matrix size n-by-n. Kronecker sum sizeKronecker product matrices use indexing functionreference elements sum, need one matrix square, thusIr = Ic = I:PK(k)k=1 ik ,ik il = jl lAI(i1 ,i2 ,...,iK ),I(j1 ,j2 ,...,jK ) = A(k) ik ,jk(71)il = jl l except l = k0otherwise.Figure 8 demonstrates simple example.terms CTMP, Kronecker sum represents asynchronicity. Given two variables,X1 X2 intensity3 matrices Q1 Q2 , Q1 Q2 intensity matrix jointstate space processs events proceed irrespective others state.is, processes independent (assuming starting distributions independent).3. holds rate matrices, stronger Kronecker product makestatement intensity matrices too.743fiShelton & CiardoNote intensity transition involves two variables zero (atinstant, maximum one variable change).3.3 PropertiesKronecker product obeys classic distributive property:(A + B) C = C + B Cmixed product property provides relationship Kronecker productmatrix product. Given matrices A, B, C, assuming AC BD validmatrix products,(A B)(C D) = (AC) (BD) .(72)One consequence Kronecker product expressedKA(k) =k=1=Kk=1Kn1 n2 nk1 A(k) nk+1 nK(73)n1:k1 A(k) nk+1:K(74)k=1na:b product terms na nb defined above. showsbit relationship Kronecker products sums: Compare Equation 70Equation 73.reworkedKi=k(k)=KP n1:k ,nk+1:K > (I nk A(k) ) P n1:k ,nk+1:K(75)i=knk = n1:K /nk P a,b matrix describing a,b-perfect shuffle permutation(0, ..., ab 1): entry position (i, j) 1 j = (i mod a) b + bi/ac, 0 otherwise (inparticular, P a,b = P a,b = ab b 1). Whereas Equation 74 orders Kroneckerproducts outer products terms elements Ak correct places,Equation 75 repeats Ak diagonal permutes rows columns placeelements correct locations. similar transformation used rewriteEquation 70 sum shuffled block-diagonal matrices. permutationsoften done implicitly code, versions useful deriving algorithms.3.4 Compact Kronecker RepresentationsGiven factored state space before, joint rate matrix R expressed sumKronecker products:ELX(l)R=(76)e=1 l=1(l)L variablesrate matrix space variable l only.particular, exponentially sized (in number variables) representation straightforward: e ranges elements resulting matrix. element corresponding744fiTutorial Structured Continuous-Time Markov Processes(l)(x1 , x2 , . . . , xL ), (x01 , x02 , . . . , x0L ), = xl ,x0l 1 < l L l = 1, exceptmultiplied scalar value placed location. way termsum matrix single non-zero element. However, many processesexpect E manageable number. instance, variables independent,E = L (and L L2 rate components identity matrices), per Kroneckersum above.view E terms sum separate events whose identitiesmarginalized produce resulting process (see Section 2.1.3). eventsmust couple variables synchronously (due Kronecker product). exploit typedecomposition extensively next sections.4. Decision Diagram Representationsencoding Equation 76 efficient, better exploitinginternal structure. R viewed mapping two discrete domains (the row indexcolumn index) real value. Decision diagrams long used computerscience compactly encode functions discrete domains. showused CTMPs seen alternative Kronecker algebraencodings, case MTBDDs used PRISM (Kwiatkowska, Norman, & Parker,2011), even extension Kronecker algebra encodings, case MatrixDiagrams used Mobius (Deavours, Clark, Courtney, Daly, Derisavi, Doyle, Sanders, &Webster, 2002) EVMDDs used SMART (Ciardo, Jones, Miner, & Siminiceanu,2006).4.1 Decision Diagram OverviewDecision diagrams encode functions form f : X X0 where, before, domainstate space X structured: X = X1 XL . words, f applied (state)tuple evaluates element range set X0 . One think f encodingvector indexed X entries values X0 . course, ideaemployed encode matrices, simply need use domain X X. (In practice,actually use interleaved domain X10 X1 XL0 XL , unprimed statevariables refer row indices, states, primed state variable refercolumn indices, states, usually leads compact decision diagrams.)Binary decision diagrams, BDDs (Bryant, 1986), encode functions setsforming domain X binary, multiway decision diagrams, MDDs (Kam, Villa,Brayton, & Sangiovanni-Vincentelli, 1998), allow non-binary domain sets. However,range X0 binary. numeric application, need extend representationsallow range X0 either integers Z (possibly augmented valueindicate undefined) reals R (possibly, again, augmented , restrictednonnegative reals R0 ). range Z used primarily encode indexing functionsnon-consecutive sets states. range R used encode rates themselves.Informally, decision diagrams directed acyclic graphs organized layerslayer corresponding different variable domain function. outgoingedges node correspond values variable layer take on.745fiShelton & Ciardovalue function determined following path root correspondingvalues taken domain variables. path ends terminal node which, BDDsMDDs, give value function.first proposal encode non-binary function extend BDDs MDDs that,instead terminals 0 1, element X0 terminal node. resulting multi-terminal (Clarke, Fujita, McGeer, Yang, & Zhao, 1993) BDDs (or MTMDDs)quite general. However, see, MTMDDs sometimes unable compactlyencode even simple functions. therefore focus newer class edge-valued decision diagrams, exponentially compact, provably never larger,MTMDDs (Roux & Siminiceanu, 2010). edge-valued decision diagrams, value associated edge tree, functions value determined valuesalong path terminal node. exact definition diagrams dependsoperator used combine edge values. consider two cases, EV+MDDs (Ciardo &Siminiceanu, 2002) (where X0 either Z {} R {} edge values along pathsummed) EVMDDs (Wan, Ciardo, & Miner, 2011) (where X0 R0 edgevalues along path multiplied).4.2 Multiterminal Edge-Valued Decision DiagramsFormally, EV+MDD EVMDD acyclic directed edge-labeled edgevalued graphs. node graph p level p.lvl set directed edges indexedx. edge associated label x written p[x] = hp[x].val,p[x].chi, p[x].valvalue associated edge p[x].ch target edge.terminal node (one without outgoing edges) , level 0: .lvl = 0.nonterminal node p level k {1, . . . , L}: p.lvl = k. xk Xk ,outgoing edge labeled xk , associated value v X0 , pointing nodeq q.lvl < k. Thus p[xk ] = hv,qi.node p level k encodes function fp : X1 Xk X0 . EV+MDDs,fp defined recursively fp = 0 p = , fp (x1 , . . . , xk ) = p[xk ].val +fp[xk ].ch (x1 , . . . , xp[xk ].ch.lvl ) otherwise (that is, p nonterminal node).EVMDDs, fp defined recursively fp = 1 p = , fp (x1 , . . . , xk ) =p[xk ].val fp[xk ].ch (x1 , . . . , xp[xk ].ch.lvl ) otherwise.decision diagram definitions additional restrictions ensure canonicity,representable function unique representation. edge-valueddecision diagrams defined, achieved additionally requiringfollowing.duplicate nodes: p.lvl = q.lvl = k and, xk Xk ,p[xk ] = q[xk ], p = q.absorbing value terminates path: EV+MDDs, p[xk ].val = impliesp[xk ].ch = ; EVMDDs, p[xk ].val = 0 implies p[xk ].ch = .node p level k > 0 normalized : EV+MDDs, min{p[xk ].val : xk Xk } =0; EVMDDs, max{p[xk ].val : xk Xk } = 1.746fiTutorial Structured Continuous-Time Markov ProcessesMDD XEV+MDD XMTMDD XEV+MDDMTMDDMDD0x3x2x10100101100011110201304151061701040102010100x30x2x101101000111001011001011021103140102010101020 1001010 10Figure 9: Encoding lexicographic index function, X , set X. left panel showsquasi-reduced MDD encoding X followed MTMDD EV+MDDencoding X ; right panel shows corresponding encodings set ={100, 110, 001, 101, 011}. either case, EV+MDD isomorphic MDD.level tree corresponds different variable. Black boxesvalues variable (and traversal diagram follows edge leadingbox value input). White boxes (for EV+MDD) valuescorresponding edge (which summed produce functions value).0 top EV+MDD value added path traversaldiagram.Furthermore, require one following two reduction forms must used.quasi-reduced form, nodes level L incoming edges (except specialcase graph consisting ) children node level klevel k 1 (except absorbing-valued edges, point , stated above).fully-reduced form, redundant nodes, node p level kredundant p[xk ] = p[yk ] xk , yk Xk .Strictly speaking, EV+MDD node encodes function values 0 (included) (possibly included); thus, function f range Z {} R {}encoded h,pi = min{f (i) : X} fp = f (the special case fencoded pair h,i). Analogously, EVMDD node encodes functionvalues 0 (possibly included) 1 (included); thus function f range R0encoded h,pi = max{f (i) : X} fp = f / (the special case f 0encoded pair h0,i). following, use term EV+MDD EVMDD alsopair h,pi, understanding parameter scales valuesfunction encoded node p.4.3 Lexicographic Index Exampleillustrate compactness decision diagrams using lexicographicindex, alsoPcalled mixed-base value, state x = (x1 , . . . , xL ), defined (x) = 1kL xk n1:k1 ,747fiShelton & Ciardona:b = na nb b (as Section 3.1). discuss importancefunction showing encoding.Figure 9 (left) shows lexicographic index function (along side MDD encodingset states). MTMDD full L-level tree n1:L leaves. contrast,EV+MDD contains one node level, child labeled xknode level k points node level k 1 value xk n1:k1 .Interestingly, function retains compact encoding even modifyapplies set X, (x) = |{y : (y) < (x)}| x ,(x) = otherwise, sense MDD encoding EV+MDD encodingisomorphic (right panel Figure 9). particular importance exactnumerical solution structured CTMPs whose reachable state space Xrch strict (andpossibly complicated) subset X, since case need frequently efficientlymap state x = (x1 , . . . , xL ) index Xrch (x) full probability vector size |Xrch |.Compactly representing index function key efficient calculations CTMPs.Obviously, EVMDDs also exponentially compact MTMDDs; simplyconsider EVMDD encoding e also one node per level, childlabel xk node level k value exk n1:k1 .4.4 Decision Diagram Operationsaddition efficiently encoding structured functions, decision diagrams also ableefficiently manipulate functions. decision diagram operations proceed recursivelyroot node(s) make extensive use dynamic programming. Specifically,use operation cache retrieve result specific operation specific choiceparameters, result previously computed exploring different pathsrecursion. reduces worst-case complexity operation (for example, computingc = + b, b functions encoded two EVMDDs) exponential (i.e.,size domain) polynomial. example, Figure 10 shows pseudocodealgorithm perform element-wise addition two EVMDDs, , 0a, b EVMDD nodes level L (unless = 0, case = , = 0,case b = ), Sum(L, h,ai, h,bi) returns EVMDD h,ri that,x = (x1 , . . . , xL ) X, fr (x) = fa (x) + fb (x); course, inputEVMDDs assumed canonical form, output EVMDD guaranteedcanonical form. complexity product sizes inputEVMDDs.practical implementation, unique table, stores nodes avoids duplicates, implemented lossless hash table which, given lookup key hlevel, r[0], . . . , r[nk1]i, returns nodes address, cache implemented (possibly) lossy hash table. cache made effective scaling exploiting commutativity.example, defining arbitrary order nodes (for example, b memory addresssmaller b), exchange two input EVMDDs ensureb prior cache lookup, observe fa + fb = (fa + fb ), = /,store entries form hSU , a, , b , ri cache. Then, assumingp q, call Sum(k, h0.5,pi, h0.2,qi) would cached hSU , p, 0.4, q , s,748fiTutorial Structured Continuous-Time Markov Processesfunction Sum(level k, EVMDDh,ai, EVMDDh,bi)= b return h + ,ai . includes terminal case k = 0: = b == 0 return h,bi. = definition= 0 return h,ai. b = definitioncache contains hSU , , a, , b , ri return h,ri . Check resultcacher NewNode(k). Create new temporary result node level kxk Xkr[xk ] Sum(k 1,h a[xk ].val,a[xk ].chi,h b[xk ].val,b[xk ].chi) . Recurseone levelmaxxk Xk {r[xk ].val}; . Maximum edge value node r normalizationxk xkr[xk ].val r[xk ].val/ . Normalize node r maximum edge value 1r UniqueTableInsert(r);. node like r exists, return delete r, elsereturn rEnter hSU , , a, , b , ri cache;. Remember result operationcachereturn h,ri;Figure 10: Pseudo-code sum quasi-reduced EVMDDs (a b either nodeslevel k). fully-reduced version similar slightly involved,needs take account levels b.return h0.5,si, subsequent call Sum(k, h0.25,pi, h0.1,qi) Sum(k, h0.1,qi, h0.25,pi)would find hSU , p, 0.4, q , s, cache immediately return h0.25,si.4.5 Encoding Transition Rate Matrices EVMDDsturn use EVMDDs compactly encode transition rate matrix R(the intensity matrix Q, without diagonal) CTMP.accomplished using various approaches.4.5.1 Monolithic Encoding vs. Disjunctive Partition EncodingClearly, node r 2L-level EVMDD encode arbitrary function formX X [0, 1]. Then, 0, pair h,ri encodes arbitrary functionform X X [0, ], EVMDD levels (1, . . . , 2L) correspond state variables(x01 , x1 , . . . , x0L , xL ), is, use interleaved order describe transition ratex x0 . monolithic approach, store R using single EVMDD h,rilargest rate R r encodes matrix R/.However, many practical systems exhibit asynchronous behavior, statechange due event e E occurring (asynchronously) somewhere system.situations, employ disjunctive partition encode R, storing setEVMDDs {he ,re : e E}, ,re encodes matrix , (x, x0 ) describesrate system moves state x state x0 due occurrence event e.749fiShelton & CiardoPdisjunctive partition encoding, R = eE built explicitly;rather, individual matrices directly used numerical computations usedsolve CTMP. idea disjunctive partition initially suggested BDDs (Burch,Clarke, & Long, 1991), although obviously also related Kronecker encodings: considerEquation 76, expresses R first foremost sum.choice monolithic disjunctive partition encoding largely modeldependent. applications, high-level language description model suggestsset asynchronous events E be. Thus, first build EVMDDsdisjuncts then, desired, explicitly build EVMDD R summingEVMDDs disjunct corresponding event (using algorithm Figure 10,instance).However, disjunct EVMDDs usually quite compact, EVMDDR obtained summing disjuncts might still compact, might growlarge. former case, monolithic approach preferable, allows usdirectly use EVMDD encoding R numerical iterations. latter case,disjunctive partition approach preferable, allows us use EVMDDindividually, without even attempting build monolithic EVMDD encoding R.example, consider simple system four Boolean variables, X1 , X2 , X3 ,X4 , two events. first event, c21 , changes value (X2 , X1 ), interpreted2-bit integer, sequence 0 [1] 1 [1/2] 2 [1/4] 3 [1/8] 0 [1] ,numbers square brackets indicate rate corresponding transition. secondevent, c43 , changes value (X4 , X3 ), interpreted 2-bit integer, sequence0 [3] 1 [1] 2 [1/3] 3 [1/9] 0 [3] . Figure 11 left shows EVMDDsencoding matrices R21 R43 corresponding two events, well matrixR = R21 + R43 . (for visual simplicity, edges value 0, definition pointterminal node , shown).4.5.2 Adopting Ideas Kronecker Encodings: Identity PatternsNeither monolithic approach disjunctive partition approach exploit locality:fact events (synchronously) affect state variables. words,matrix conceptually size |X| |X|, usually much smallersupport Se {x1 , . . . , xL }. Specifically, Xk Se Xk e dependent:local state xk affects rate e occurs (including case may disable ealtogether, set rate 0) changed occurrence e. Xk 6 Se ,Xk e independent EVMDD encoding contains identity patternscorrespondence xk . example, EVMDD encoding R21 Figure 11 leftexhibits patterns respect variables X4 X3 , one R43 exhibitsX2 X1 .Essentially, identity patterns simply describe fact value x0k (thenew value xk occurrence e) equals old value Xk rateaffected value Xk , true possible values Xk .(k)happens, Kronecker encoding event e = nk , quasi-reducedfully-reduced forms alone cannot take advantage common patterns. exploitpatterns, combination fully-reduced form, unprimed level Xk , new750fiTutorial Structured Continuous-Time Markov ProcessesR21R431x4x04x030111 1/91011 1/3100 11/3 1101011111 1/3110111111 1/4011 1/21101 1/3011101111010111 1/31011 1/410101/2 1111 1/21101011110111x10101010111111111x011001011111110x03x021001 1/30x21/2 1311 1/9x3110x041/9 1R3x41/3 10R4311 1/300010x2x02011x3300R21R3101/3 110111 1/3101100110111111001/3 111 1/3101 1/40101 1/211/9 11 1/30111 1/311 1/4101/2 111 1/20x101011111x011010111111/2 1Figure 11: example EVMDDs encoding transition rate matrices using quasireduced form (left) fully-identity-reduced form (right). Omitted edgesimplied value 0 (thus resulting value 0 path containing them).right diagrams left, except identity patterns omitted implied: completely skipped pair levelsassumed identity structure (compare corresponding diagramleft).identity-reduced form (Ciardo & Yu, 2005) primed level Xk0 , needed. allowsus encode EVMDD nodes (unprimed primed) levelscorresponding state variables Se . advantage fully-identity-reducedform resulting decision diagrams, unlike Kronecker encoding, also recognizeexploit partial identity patterns (those arising models Xk remains unchangede occurs certain states others). Figure 11 right shows encodingmatrices R21 , R43 , R, using new fully-identity-reduced form.4.5.3 Beyond Kronecker: Disjunctive-then-Conjunctive Partition Encodingpush decomposition employing disjunctive-then-conjunctive partition approach. idea first introduced logic analysis (Ciardo & Yu, 2005)also related Equation 76, expresses R sum products. particularly appropriate globally-asynchronous locally-synchronous systems,state change due (asynchronous) event e E, occurrence e de751fiShelton & Ciardopends (synchronously) changes state variables.Q(c)decomposed product matrices, = 1cm and, again, matrix(c)(c)conceptually size |X| |X| but, practice, usually small support Se .(c)(c)Specifically, Xk Se fully-identity-reduced EVMDD contains0node associated Xk Xk . restrict case supports(c)(c)conjuncts event disjoint, 1cm Se = Se Se substantially smaller Se . example, Kronecker encoding exists,N(k)(k)(k)= 1kL , Se = {Xk } Xk Se , 6= nk ;case, disjunctive-then-conjunctive approach uses EVMDD store(c)compact disjunctive partition approach uses EVMDD store, essentially compact Kronecker approach (exceptsave additional memory exploiting partial identity patterns).disjunctive-then-conjunctive approach instead distinctly efficientKronecker approach applicable (that is, Kronecker approach would requireenormous set events correctly describe R), nevertheless seenextension Kronecker approach. Consider using decomposition Equation 75:(k)(k)==P n1:k ,nk+1:L > (I nk ) P n1:k ,nk+1:L1kL1kL=(k)P n1:k ,nk+1:L > (I nk ) P n1:k ,nk+1:LXk Se(k)last step simply stresses that, = nk , corresponding factorn1:L skipped.idea behind Shuffle Algorithm (Fernandes, Plateau, & Stewart, 1998),which, observed Buchholz, Ciardo, Donatelli, Kemper (2000), efficient,(k)matrices sparse. (The perfect shuffle pre- postmultiplications essentially free; simply describe different state indexing.)Then, disjunctive-then-conjunctive approach extends Kronecker expressionallow situations factors restricted support consistingone variable, still exploits factors locality:=P (c) > (c) RS (c) P (c)(77)See1cmeeP (c) > P (c) perfect shuffle permutations respectively move deee(c)pendent state variables Se end ofQthe variable order back originalposition, (c) identity matrix size X 6S (c) nh (the skipped levels), RS (c)SeeehQsquare matrix size X (c) nh (the conjunct encoded EVMDD ignorehe(c)skipped levels corresponding state variables Se ).(c)Since supports Se disjoint, generalization Kronecker approach comesadditional cost essentially corresponds Kronecker approach allowevent defined different set state variables, set corresponding752fiTutorial Structured Continuous-Time Markov Processesdifferent partition basic state variables (X1 , .., XL ). case, buildingEVMDD multiplying EVMDDs RS (c) involve numerice(c)multiplication, grows size diagram spans sets Se(1)(2)disjoint; example, Se = {X3 , X7 } Se = {X4 , X6 }, path X7X3 EVMDD contain copy entire EVMDD encoding Re(2) .4.5.4 Numerical Solutions Decision Diagramsdescribed method storing rate intensity matrix compactly commonprocess models. address use data structures CTMP computations.literature surrounding decision diagrams CTMPs primarily concerned computation unconditional distribution resulting process, either finite time(more commonly) limit infinite time (the stationary distribution). followconvention literature refer solution process. Model estimation, solutions conditioned evidence, computations marginals statisticsare, knowledge, unexplored representations, return later.matrix R stored using 2L-level EVMDDs (using monolithic, disjunctivepartition, disjunctive-then-conjunctive partition encoding), traditional numericalsolution algorithms need adjusted accordingly. First all, seeking exactsolution, neither stationary vector transient vector (t) admit compactEVMDD representation (unless modeled system contains extensive symmetriescomposed completely independent subsystems, rarely case practice).Two approaches explored. exact solution stationary distribution(the null-space Q), hybrid approach (Kwiatkowska, Norman, & Parker, 2004)usually best, solution stored full vector reals size equalnumber reachable states (|Xrch |, equal |X| states reachable)rate matrix R stored EVMDDs, expected holding time vector h (the inverseabsolute values diagonal Q) stored either full vector EVMDDs.Clearly, approach scales size tractable problems eliminating mainmemory obstacle (the storage R), encounter next memory obstacle (thestorage solution vector). example, Figure 12 shows pseudocode classicJacobi-style stationary solution ergodic CTMP transition rate matrixmonolithically encoded EVMDD h,ri, state space Xrch indexedEV+MDD h0,pi, previously discussed, that, X, compute Xrch (i),index 0 |Xrch | 1 included Xrch , 6 Xrch . FunctionXrch used index entries solution vector: new old . holding timevector stored full vector h, also indexed Xrch (but could storedusing EVMDDs instead). recursive call JacobiRecur, descendlevel current rate matrix EVMDD node single levelcorresponding source destination EV+MDD nodes (these needed indexfull vectors reals, initially set h0,pi, encoding entire Xrch function).Note that, simple case states reachable(that is, X = Xrch ) indexingPfunction Xrch mixed-base value (i) = 1kL ik n1:k1 discussed Section4.3 and, such, really require EV+MDD encoding; hand,shown Figure 9, EV+MDD single path nodes, use carry753fiShelton & Ciardo. Computes Q = 0. h,ri R, h0,pi Xrchfunction JacobiIteration(EVMDDh,ri,EV+MDDh0,pi)old initial guess. Real vector size |Xrch |, visible JacobiRecurnum iter 0repeatnew zero vector. Real vector size |Xrch |, visible JacobiRecurJacobiRecur(L,h,ri,h0,pi,h0,pi){0, . . . , |Xrch | 1}new [i] new [i] h[i]. h holding time vectoroldnewswap( , )num iter num iter + 1num iter > AX ER Converged( old , new ). example, using relative absolute test. Computes new old RfunctionJacobiRecur(levelk,EVMDDh,mi,EV+MDDhsrc ,srci,+EV MDDhdes ,desi)src = des =new [des ] new [des ] + old [src ]return0 nk 1 s.t. m[i].val 6= 0 src[i].val 6=. level kj 0 nk 1 s.t. m[i][j].val 6= 0 des[j].val 6= . level k 00srcsrc + src[i].val0des des + des[j].val0 m[i][j].val0 ,src[i].chi, h 0 ,des[j].chi)JacobiRecur(k1, h 0 ,m[i][j].chi, hsrcdesFigure 12: Jacobi-style iteration stationary solution (Q =dt = 0) R(non-diagonal elements Q) stored monolithic EV MDD h (inverseabsolute value diagonal elements Q) stored vector. h,riencoding R h0,pi encoding mapping states indices(for h).overhead. similar hybrid approach used compute transient solution usinguniformization-style algorithm R also stored using EVMDDs but, again,size full vectors limits scalability.filtering smoothing operations described Section 2.5 explicitly tackled decision-diagram encodings. However, willing representdistribution exactly (as above), necessary vector-matrix multiplicationsdirectly decision-diagrams (without expanding them). Estimating EVMDD representation R data completely unexplored.tackle larger problems must instead willing accept approximate solution.However, work area mostly restricted systems exhibiting special structures.754fiTutorial Structured Continuous-Time Markov ProcessesOne exception work Wan et al. (2011), addresses stationary solutionarbitrary ergodic CTMPs whose state space encoded MDD whose transition ratematrix encoded one EVMDDs. approach uses L different approximateaggregations exact CTMP solves iteratively, reaching fixpoint.approach provides exact solution certain conditions essentially,system so-called product-form (Baskett, Chandy, Muntz, & Palacios-Gomez, 1975).Unfortunately, similar approximation transient solution structured CTMPproposed far.Thus state spaces large enough single vector values cannotmaintained, literature inference estimation models limited.However, next section describe different model viewed restrictedform disjunctive EVMDD encoding section. model many inferenceestimation method believe link two may allow methodsextended general decision-diagram representations.5. Continuous-Time Bayesian Networksartificial intelligence machine learning literatures, continuous-time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller, 2002) developed extensiondynamic Bayesian networks (DBNs). discuss section, limitedcase disjunctive EVMDD encodings above. However, approximate methodscomputations conditioned evidence extensively developed CTBNs.CTBN consists set variables, {X1 , X2 , . . . , XL }, directed graph G oneto-one mapping nodes variables, set conditional intensity matricesvariable, initial distribution. initial distribution usually describedBayesian network (to keep description compact), many algorithmstheory hold compact distribution representations.graph G describes instantaneous influence variables other. edgeXi Xj denotes rates transitions Xj depend instantaneous valueXi . Note G may cyclic.dependent rates captured conditional intensity matrices. Let ninumber states variable Xi . denote parents variable Xi Parijoint assignment Pari pari . set conditional intensity matrices variable Xiconsists one ni -by-ni intensity matrix possible instantiation pari : QXi |paridenote element xi , x0i qxi ,x0i |pari , rate Xi transitioning xi x0iPari values pari .Semantically, CTBN continuous-time Markov process joint state spaceconstituent variables. let (x, x0 ) equal set variables whose assignmentsdiffer joint assignments x x0 . joint intensity matrix entire processdescribedqx,x0PLi=1 qxi ,x0i |pari= qxi ,x0 |pari0755(x, x0 ) = {}(x, x0 ) = {Xi }otherwise(78)fiShelton & Ciardo12124=3435678QX1 |X2 =0 =X1QX1 |X2 =1X2X3QX2 |X1 =0QX2 |X1 =15=67=8QX3 |X2 =09= 87QX3 |X2 =112= 870104613792116514Figure 13: example CTBN graph. See Figures 14, 15, 17 CTMPrepresentations.pari assignment Pari x. intensity transition two statesdiffer one variable read appropriate conditional intensity matrixvariable. intensity transition two states (that differone variable) zero. diagonal elements filled negative rowsums. process allows two variables transition arbitrarily close times,exactly time.CTBN retains local Markov properties standard Bayesian network. particular, variable (local process) independent non-descendants, given parents.course, cycles, parents may also descendants, pose problemdefinition. Note, however, given refers conditioning entire trajectoryvariable (from starting time ending time, variablesqueried observed). Conditioning current value sufficient (even renderingcurrent values independent).global Markov properties also hold. Markov blanket variable unionsets parents, children, childrens parents. Note setssignificant overlap, cycles permitted. Conditioned Markov blanket,variable independent variables.5.1 Connections RepresentationsCTBN related number representations. instance, PortinaleCodetta-Raiteri (2009) link CTBNs stochastic Petri nets (Ajmone Marsan, Balbo, Conte,Donatelli, & Franceschinis, 1995). Donatelli (1994) shows translation stochasticPetri nets Kronecker operators Ciardo, Zhao, Jin (2012) show translation(ordinary, timed, stochastic) Petri nets various classes decision diagrams.However, concentrate direct comparisons approaches presentedtutorial.Figure 13 shows simple small CTBN two binary variables (X1 X2 ) oneternary variable (X3 ). use running example convertCTBN compact representations.756fiTutorial Structured Continuous-Time Markov ProcessesX10X1X20X2P (X10 |X1 , X2 ) : 010, 011t1t1, 02t12t0, 114t4t1, 13t13tP (X20 |X2 , X1 ) : 010, 015t5t1, 06t16t0, 117t7t1, 18t18t0:120, 019t09t1, 08t110t2tX30X3time =time = +P (X30 |X3 , X2 )2, 07t4t111t0, 1112t6t6t1, 18t113t5t2, 17t7t114tFigure 14: DBN whose limit 0 approaches CTBN Figure 13.5.1.1 Connection DBNCTBN, construct dynamic Bayesian network (DBN) whose parametersfunction time slices limit time-slice width approacheszero original CTBN. particular, DBN intra-time-slice edges (thistwo variables CTBN cannot change exactly time). Xiparents Pari CTBN, parents (at previous time slice) plusprevious value DBN. Figure 14 shows CTBN Figure 13 DBN. XiCTBN intensity matrix QXi |pari parent values pari , correspondingvariable DBN conditional probability distributionpDBN (x0i |xi , pari ) = x0i ,xi + qxi ,x0i |pari(79)x0i value Xi next time step (and xi pari valuesprevious time step), x0i ,xi 1 x0i = xi 0 otherwise, time timeslices. limit process approaches 0 original CTBN.5.1.2 Connection Kronecker Algebrajoint intensity matrix expressed Equation 78 also written sum Kronecker products. need first define conceptually simple, notationally cumbersome, term. First, let i,j matrix 0, except single 1 location i, j (samebefore). Second, let QXi |pari denote Kronecker product one matrix variableCTBN. variable Xi , matrix QXi |pari . variable parent Xivalue xk pari , matrix xk ,xk . Otherwise, matrix identitymatrix. way Kronecker product distributes elements QXi |parirelevant entries joint intensity matrix.Pdefine joint intensity matrix. Let QXipari QXi |pari . Then,PQ = QXi . Figure 15 gives example CTBN Figure 13. Figure 16 givesanother example. Kronecker product general handle diagonalelements, expansion works intensity matrix case, since onematrices product non-diagonal.757fiShelton & CiardoQX1 =XQX1 |x2 x2 ,x2x2QX2 =Xx1 ,x1 QX2 |x1x1QX3 =Xx2 ,x2 QX3 |x2x2Q = QX1 + QX2 + QX3Figure 15: Sum Kronecker encoding rate matrix Q CTBN Figure 13.QW = QWXQX =QX|z z,zWzXQY =w,w QY |wwQZ =ZXXx,x y,y QZ|x,yx,yQ = QW + QX + QY + QZFigure 16: Sum Kronecker product encoding CTBN one parent pernode.5.1.3 Connection Decision Diagramsdecomposition CTBN sum Kronecker products helps clarifyconnection edge-valued decision diagrams previous section. CTBNparticularly structured version disjunctive EVMDD encoding Section 4.5.1 pairedidentity encoding Section 4.5.2. particular, CTBN describes CTMPalso described sum EVMDDs fully-identity-reduced form. twodescriptions order space complexity. decision-diagram encoding oneEVMDD variable joint value parents.Figure 17 shows disjunction EVMDDs CTBN Figure 13. EVMDDencodes non-identity matrices Kronecker product expression; identitymatrices implied fully-identity-reduced form. another example, CTBNFigure 16, could construct 9 EVMDDs: 1 W , 2 X , 4 Z.758fiTutorial Structured Continuous-Time Markov ProcessesRX1|X2=0RX1|X2=12RX2|X1=04RX2|X1=16RX3|X2=080x3x20111008200011 5/820101111121011111111 4/71100101201101 1/426/8 1 7/811102117/8 13/4 111/2 115/6 1x02x0111 8/9 7/9x03x1RX3|X2=190111Figure 17: set identity-reduced EVMDDs whose sum CTBNFigure 13.Note disjunction EVMDDs compactly encode structure within variableslocal rate matrix, CTBN cannot. way, represent generalizationexploit context-sensitive independence.Whether merging EVMDDs given variable merging EVMDDs multiplevariables result reduction increase representation size largely empiricalquestion. However, would generally expect increase size one transitionallowed time, paths levels must remember whether previousvariable changed and, so, one (for example, Figure 11).5.2 SamplingSampling CTBN done straight-forward application sampling methoddescribed Section 2.1. need construct full intensity matrix. Instead,joint assignment x, find diagonal element summing diagonalsrelevant conditional intensity matrices. gives us rate exponential sampletime next variable change. read intensities variablespotential transitions relevant row conditional intensity matrix selectvariable new state variable proportion intensity. processtakes O(L) time transition (where L number variables).better exploiting racing memoryless properties exponentialdistributions (discussed Section 2.1). select variable transitions next,race exponential distributions variable rates corresponding diagonal759fiShelton & Ciardofunction SampleCTBN(CTBN, initial distribution 0 , end time )Let Tr empty trajectoryLet (x1 , x2 , . . . , xL ) joint sample 0. per algorithm 0representation1 LAdd (Xi = xi @ 0) TrLet 0Let E empty event priority queue time-variable pairs.repeatvariables Xi event ESample exponential rate qxi |pariAdd ht + t,Xi ELet ht,Xi earliest event E . Update get new variable change<Sample x0i multinomial proportional qxi ,x0i |pariLet xi x0i. Update local copy variable assignmentsAdd (Xi = xi @ t) TrRemove Xi children Xi E. times must resampledFigure 18: Algorithm sample CTBNelements conditional intensity matrices. note variablechosen, treat transition time two separate random draws: draw statingtransition time chosen time draw stating chosentransition time next transition (because memoryless property exponentialdistribution). means chosen transition affect rate variablequestion, need resample transition time. using priority queuetransitions times (not durations), reduce running time per transition O(D ln L)maximal out-degree graph. method made explicit Figure 18.5.3 InferenceInference CTBN process calculating expected value full trajectory,given partial trajectory. basic case infer conditional probabilitysingle variable single time point (the expectation indicator function) given partialtrajectory. many ways trajectory may partial. obviousvariable-based model like CTBN variables observed particulartimes intervals. variable observation times intervals. Thus,variable, assume evidence like Section 2.5.1: timepoints variable known values time intervalsvariable known values (which might include observations transitions).Unfortunately, even evidence, problem NP-hard. particular,deciding whether marginal probability single value single variable single760fiTutorial Structured Continuous-Time Markov Processestime point greater positive threshold NP-hard. generallyaccepted, although never formally demonstrated. provide proof Appendix.Thus, known algorithms CTBN inference exponential (in numbervariables) running time. simplest method treat CTBN general CTMPsingle intensity matrix Q. apply forward backward passes Section 2.5.intensity matrix stored compact form, resulting vectors requirespace instantiation every variable CTBN (exponential space). needkeep values states consistent evidence. Thus, timesvariables unobserved, inference tractable. But, periodsmany variables unobserved, require approximate inference methods (overviewedSection 5.5).calculating probability variable time, common caseinference calculate expected sufficient statistics. shown Section 5.4.1,means calculating N [xi , x0i |pari ] [xi |pari ] values i, xi , x0i , pari .former expected number times variable Xi transitioned xi x0iparents state pari , latter expected amount time variable Xistate xi parents state pari .proof marginal calculation easily adapted show deciding whetherquantities non-zero also NP-hard. Therefore, known methodtreat system general CTMP single large Q matrix.apply Equation 65 Equation 64 find expected number transitions expectedamount time joint assignments. let J(xi , pari ) set joint assignmentsvariables consistent Xi = xi Pari = pari , find expectedsufficient statistics CTBN[xi |pari ] =X[x](80)xJ(xi ,pari )N [xi , x0i |pari ] =XXN [x, x0 ](81)xJ(xi ,pari ) x0 J(x0i ,pari )5.4 Parameter Graph Estimationinitial distribution CTBN estimated separately using standard methodestimation Bayesian network (or whatever compact representation desired).requires data value trajectorys value (or trajectories values)time 0.concentrate estimation rate parameters dynamics graph structure (G). exposition assume single trajectory, Tr. However, multipletrajectories used summing sufficient statistics.5.4.1 Parameter Estimationset CTBNs fixed graph structure subset exponential familyCTMPs parameters fixed 0 many remaining ones tied(share value). Thus, log-likelihood Equation 30 applies761fiShelton & Ciardotoo, sufficient statistics tied parameters summed:XX[xi |pari ]qx |par +ln pCTBN (Tr) = ln P0 (Tr(0)) +N [xi , x0i |pari ] ln qxi ,x0i |parix0i 6=xii,pari ,xiX= ln P0 (Tr(0)) +(82)[xi |pari ]qxi ,x0i |pari + N [xi , x0i |pari ] ln qxi ,x0i |parii,pari ,xi ,x0i 6=xi(83)= ln P0 (Tr(0)) +XlXi (Tr)(84)ranges variables, pari ranges joint assignments parents i,xi x0i range differing assignments Xi . [xi |pari ] denotes amount timeXi = xi Pari = pari . Similarly, N [xi , x0i |pari ] denotes number transitionsXi xi x0i Pari = pari . new sufficient statistics sums sufficientstatistics flat CTMP, summing assignments CTBN variablesXi Pari remain (see Equations 80 81). Given complete trajectory,construct directly without employing (exponentially large) sums. lastline definition lXi , local log-likelihood variable Xi . Notefunction trajectories Xi parents (not Tr).Maximizing Equation 83 straight-forward extension maximizing Equation 30:qxi ,x0i |pari = N [xi , x0i |pari ]/T [xi |pari ] .(85)produce Bayesian posterior distributions parameters take independentconjugate prior distributions qxi ,x0i |pari parameter (Nodelman et al., 2003).flat CTMP, conjugate prior gamma distribution hyper-parametersxi ,x0i |pari xi ,x0i |pari parameter qxi ,x0i |pari . resulting posterior also gammadistribution corresponding hyper-parameters xi ,x0i |pari +N [xi , x0i |pari ] xi ,x0i |pari +[xi |pari ]. Thus MAP parameter estimatesqxi ,x0i |pari =N [xi , x0i |pari ] + xi ,x0i |pari[xi |pari ] + xi ,x0i |pari.(86)5.4.2 Structure EstimationEstimating CTBN structure could accomplished statistical tests independence processes. Yet, unaware methods use suitableindependence tests.Instead, CTBN structures estimated graph scoring functions. scorefunction decomposes likelihood (Equation 83) sum terms, one pervariable, selection variables parents affects termvariable, search maximal scoring graph simple. variables parentschosen independently maximizing corresponding term sum.exponentially large (in total number variables) number parent sets762fiTutorial Structured Continuous-Time Markov Processesconsider variable, limit cardinality parent sets D,variables parents chosen exhaustive search total running timeO(L2D ), linear number variables, L.contrast Bayesian networks similar strategy leadefficient algorithm (unless variable ordering known priori). Learning CTBNsstructure efficient restrictions graph: CTBNs graph maycyclic. similar situation arises dynamic Bayesian networks (DBNs).allow inter-time-slice edges (those previous time point current timepoint), graph structure may searched efficiently, like CTBNs. However,allow intra-time-slice edges (those within current time point) DBN, mustenforce acyclicity constraints search longer efficient.Bayesian information criterion (Lam & Bacchus, 1994) made score:!Xln |Tr|scoreBIC (G : Tr) =lXi (Tr)Dim(G)(87)2fiXln |Tr| fififi(88)=lXi (Tr)fiQXi |pari fi2Dim(G)fi finumber independent parameters network definedfifigraph G fiQXi |pari fi number independent parameters conditional intensitymatrices associated Xi . second term equal ni (ni 1) (because diagonalelements independent) times number parent instantiations. data size,|Tr|, number transitions trajectory Tr (or total data set consistsmultiple trajectories). score consistent (Nodelman et al., 2003) termlXi (Tr) grows linearly amount data represents likelihood secondterm grows logarithmically amount data penalizes excess parameters.Bayesian score also constructed placing prior graphs (as wellparameters) finding maximum ln P (G | Tr) = ln p(Tr | G)+ln P (G)ln p(Tr).last term isntP affected choice G, drop it. assume structure modularity:ln P (G) = ln P (Pari ). remaining data term, ln P (Tr | G), (logarithm the)integral likelihood multiplied prior, possible parameter values. Usingindependent gamma priors above, decomposes separate termvariable (dropping ln P0 (Tr(0)) term affect choice G):+10ZhX(xi ,x0i |pari ) xi ,xi |pariln p(Tr | G) =lnexp (T [xi |pari ] + xi ,x0i |pari )qxi ,x0i |pari(xi ,x0i |pari + 1)0i,pari ,xi 6=x0i+(N [xi , x0i |pari ] + xi ,x0i |pari ) ln qxi ,x0i |pari dqxi ,x0i |pari(89)x=XXpari ,xi 6=x0i+10,xi |pariln(xi ,x0i |pari )(xi ,x0i |pari + 1)!(N [xi , x0i |pari ] + xi ,x0i |pari + 1)N [xi ,x0i |pari ]+x(T [xi |pari ] + xi ,x0i |pari )+10,xi |pari(90)=XlscoreB (pari : Tr)(91)763fiShelton & Ciardolast line definition lscoreB . derivation almostone given Nodelman et al. (2003). difference prior consists gammadistribution independent variable whereas prior consists gamma distribution diagonal rate parameter Dirichlet prior ratios qxi ,x0i |pari /qxi |pari .two equivalent, parameterized differently.Bayesian score thereforeXscoreB (G : Tr) =lscoreB (pari : Tr) + ln P (Pari ) .(92)converges BIC score limit infinite data (Nodelman et al., 2003)therefore also consistent.5.4.3 Incomplete Datacase trajectory Tr incomplete, back situationSection 2.6. likelihood takes form CTBN general CTMP,solutions maximizing likelihood incomplete trajectory form.Namely, compute expected sufficient statistics (using inference), applygradient ascent expectation-maximization find maximum likelihood parameters.gradient!p(Tr)N [xi , x0i |pari ]= p(Tr)[xi |pari ](93)qxi ,x0i |pariqxi ,x0i |pariexpectation-maximization update equation Equation 85 exceptsufficient statistics replaced expected values, given partially observed trajectory current model.graph estimation, apply structural expectation-maximization (Friedman,1997) (SEM) CTBNs (Nodelman, Shelton, & Koller, 2005). SEM Bayesiannetworks little complex due structure search step, CTBNs, simplerstructure search step need enforce acyclicity constraints thereforecarried simply (see above). tricky point (which also holds standardBayesian networks) graph search scoring function must calculated usingexpected sufficient statistics therefore, given current model, inference algorithmmust produce expected sufficient statistics current models parent sets,also parent sets considered structure search. using exactinference (by flattening CTBN general CTMP), available. However,approximate methods (below) differ simple extract expected sufficientstatistics. inference performed, joint optimization parameters structureperformed, new model used find new expected sufficient statistics, processrepeats.5.5 Approximate Inferencementioned Section 5.3, exact inference intractable many concurrentlymissing variables. Therefore, many approximate inference methods developed.briefly cover section, would refer full papers completedescriptions.764fiTutorial Structured Continuous-Time Markov Processes5.5.1 Sampling-Based InferenceSampling obvious method producing approximate inference. numberadvantages. First, produces set full trajectories inference questionanswered. Second, sampling methods converge correct value allowed runlong enough. Third, sampling methods usually easily parallelized, lendingmultiple processors multiple cores.Hobolth Stone (2009) description number methodsunstructured case full evidence beginning end trajectory,evidence between. discuss work CTBNs general evidencepatterns.Fan Shelton (2008) Fan, Xu, Shelton (2010) developed importancesampler particle filter smoother. Forward sampling (like Figure 18)turned importance sampler taking observed data given samplingmissing portions, marching time along. weight sample probabilitysampled observed data (which sampled) given trajectorydata. problem arises variable goes observed observed.case, sampling must agree coming observation evidence. Addingtransition exactly evidence starts correct (as almost surelyevent time). samplers handle forward look ahead samplenecessary transition advance, suitable importance weight corrections.extended particle filter smoother resampled based numbertransitions, rather absolute time. method extended generaltemporal models Pfeffer (2009).El-Hay, Friedman, Kupferman (2008) developed Gibbs sampler CTBN models.start simply developed trajectory agrees evidence. Then,algorithm removes single variables full trajectory resamples (keeping timeperiods value known). Conditioned full trajectories variablesMarkov blanket (the union variables children, parents, childrens parents),trajectory variable independent variables, sampler needsconsider variables Markov blanket. posterior distributions timestransitions longer exponential distributions. forms complex thusGibbs sampler must sample performing binary search.Fan Shelton (2009) combined ideas Gibbs sampler earlierwork importance sampling produce Metropolis-Hastings sampler. importancesampling method used instead Gibbs sampling importance weight usedfind acceptance probability. faster generate samples, samples take longerconverge. balance trade-off depends typicality evidenceinference query.Rao Teh (2011, 2013) used uniformization develop auxiliary Gibbs samplerfaster previous Gibbs sampler. auxiliary variables timesself-transitions uniformization sampler (Section 2.3). Thus resamplevariable, algorithm samples auxiliary times, given old trajectory (whichdone quickly). throws away transitions, keeps full set times(old times new times). Then, using forward-backward two-pass algorithm, state765fiShelton & Ciardotransitions sampled uniformized discrete-time process (conditionedevidence). Finally, self transitions discarded. Rao Teh (2012) extendedtime-varying uniformization rate speed convergence, explicitly caseunstructured process.5.5.2 Non-Sampling Methodsnumber non-sampling methods also proposed. advantagesinclude determinism (often helpful used inside EM keep estimates consistent),fewer parameters need set well (number samples, length burn-in,others).Cohn, El-Hay, Kupferman, Friedman (2009) Cohn, El-Hay, Friedman,Kupferman (2010) derived mean-field approximation. approximate distributionindependent time-inhomogeneous Markov process variable. is, variablesindependent (in approximation), intensities depend time. naturalparameterization also differs slightly. Instead transition rates, transition densitiesused, idea same. resulting algorithm changes one variables distributiontime, depending Markov blanket. update involves solving systemdifferential equations (to get time-varying parameters inhomogeneous Markovprocess). solved adaptive integration means less computationrequired intervals less rapid change. result time-varying parametersrepresented series time-value points (those produced adaptive integration)linear interpolation points.Nodelman, Koller, Shelton (2005) derived expectation-propagation method.propagation uses piece-wise constant time-homogeneous Markov processes, piececorresponds period constant evidence. piece-wise constant approximationspropagated instead true marginals (as marginals would intractably large).Saria, Nodelman, Koller (2007) extended method subdivide piecesapproximations adaptively. El-Hay, Cohn, Friedman, Kupferman (2010)produced belief propagation algorithm spirit mean-field approximationabove, employing free energy functional CTMPs. Instead propagating piece-wisetime-homogeneous Markov processes, propagate single time-inhomogeneous Markovprocess use adaptive integration representation mean-field.result adaptive mathematically cleaner.Finally, filtering (but general inference), Celikkaya, Shelton, Lam (2011)developed factored version uniformized Taylor expansion approximate matrixexponential calculations. result something similar Boyen Koller (1998)dynamic Bayesian networks, also involving truncation infinite summixture propagation distributions. method current non-sampling methodaccuracy bounds, although loose.5.6 Extensionsshown above, CTBN converted sum decision diagrams. waydecision diagrams (and similarly convertible models) viewed extensions766fiTutorial Structured Continuous-Time Markov ProcessesCTBNs. Many non-Markovian processes Section 1.1 could, restricted rightway, also viewed. However, direct extensions CTBNs.First, El-Hay et al. (2010) introduced continuous-time Markov networks (CTMNs).undirected graphical models Markov processes way CTBNsdirected graphical models. model subclass reversible processes, onesdetailed balance holds: exists distribution (the stationary distributionprocess) (x)qx,x0 = (x0 )qx0 ,x pairs states x x0 . CTMNconverted CTBN replacing undirected edge pair directededges. parameterization directly reveals stationary distribution processMarkov network.Second, Portinale Codetta-Raiteri (2009) Codetta-Raiteri Portinale (2010)showed extension CTBNs allow simultaneous transition multiple variables.based Petri nets encodes cascades transitions happen simultaneously.Finally, Weiss, Natarajan, Page (2012) presented method constructinglocal rate matrices variable matrix, multiplication regressiontrees. akin exploiting context-specific independence (Shimony, 1991) standardBayesian network use trees (Boutilier, Friedman, Goldszmidt, & Koller, 1996).multiplication trees reduction CTBN sum EVMDDs(see Figure 17). particular, trees require tests made particularvariable order, use trees instead DAGS, multiply trees together (insteadadding them). Weiss et al. (2012) also give boosting-style algorithm learningparameterization. similar method known learning sum EVMDDs.6. Applications Current Directionsprovide context theory algorithms above, describe methods used applications. discuss believepromising pressing research directions.6.1 Decision-Diagram-Based ModelsStructured CTMPs arise many applications areas, performance reliability evaluation computer systems investigation biological systems. underlyingCTMPs describing dynamics analyzed usually large, software toolsused studies rely compact symbolic techniques encode them.particular, PRISM (Kwiatkowska et al., 2011) uses hybrid form MTBDDs,Mobius (Deavours et al., 2002) uses Matrix Diagrams (a data structure almost equivalentEVMDDs), SMART (Ciardo et al., 2006) uses EVMDDs encode transition ratematrix CTMP. tools compute stationary transient exact numericalsolutions compactly encoded CTMPs. (Indeed, compute much complexstochastic temporal logic properties expressed CSL (Baier,Haverkort, Hermanns, & Katoen, 2000), these, too, ultimately require sequencestationary transient numerical solutions.) exact solutions place largecomputational demands due exponential explosion state space, situationoften somewhat mitigated fact that, applications targeted tools,actual state space small subset full cross-product state variable values.767fiShelton & Ciardoexample application, briefly summarize study done using PRISManalysis complex biological pathway called FGF (Fibroblast Growth Factor) (Heath,Kwiatkowska, Norman, Parker, & Tymchyshyn, 2006). state system consistsnumber proteins (e.g., A, B) protein complexes (e.g., A:B) present currenttime. events system consist various reactions complexation (e.g.,+ B : B) decomplexation (its reverse, A:B + B), well degradation(e.g., ). Finally, event occurrence rate, course dependnumber proteins currently present types involved particular reaction.actual model FGF pathway, even substantial simplifications focuskey well-known aspects real cells, contains 87 different proteins protein complexes(each corresponding local state variable) 50 different reactions (if countcomplexation decomplexation separately). stress reaction concernsproteins compounds; thus decision diagram representation isolation quitecompact.Even smallest meaningful model zero one protein compoundtype would potential state space size 287 . However, almost alwayscase type models, tiny fraction states reachable, thus modelused work Heath et al. (2006) merely 801,616 states 560,000 state-tostate transitions. study focused several key questions fractiontime particular proteins bound, probability particular degradationoccurred within given time bound, quantities obtainable numerical stationarytransient analysis underlying CTMP. Notwithstanding relatively small sizestate space (which could likely scaled factor 1000, around 108 states,given modern workstation sufficient memory) results predictions obtainedmodel using PRISM shown agree biological data, demonstratingviability technique perform silico genetics much less costly alternativevitro experiments traditionally performed biology.6.2 Continuous-Time Bayesian networksCTBNs employed number real-world datasets problems including lifeevent history data (Nodelman et al., 2003), user activity modeling (Nodelman & Horvitz,2003), computer system failure modeling (Herbrich, Graepel, & Murphy, 2007), mobilerobotics (Ng, Pfeffer, & Dearden, 2005), network intrusion detection (Xu & Shelton, 2008,2010), phylogentic trees (Cohn et al., 2009), social networks (Fan & Shelton, 2009), cardiovascular health model (Weiss et al., 2012), heart failure (Gatti, Luciani, & Stella,2012). Many also innovated extending CTBN framework. instance, Nget al. (2005) allowed continuous-state variables whose dynamics dictated differential equations. form evidence limited, particle filter developedsituation. Cohn et al. (2009) applied CTBN model time-tree allowbranching (as first done Felsenstein, 1981 general CTMPs). Weiss et al. (2012) addedcontext-specific independence.give idea application CTBNs, briefly review intrusion detectionwork Xu Shelton (2008, 2010). work, goal build model normal768fiTutorial Structured Continuous-Time Markov ProcessesGHPinCincPoutCdecNFigure 19: CTBN model network traffic (Xu & Shelton, 2010). N numberdestination ports.computer system events, specific particular machine. model could useddetect abnormal events time windows method intrusion detection.Two models built: one modeling network traffic machine, onemodeling system calls processes. network model Figure 19. oneglobal hidden variable G four states. traffic divided different destinationports (for instance, 80 HTTP traffic 995 POP traffic). frequenteight ports separated traffic remaining destinations groupedtogether. N = 9 groups model (the plate Figure 19).submodel one hidden variable H four completely observed binary variables, Pin ,Pout , Cinc , Cdec , representing packets sent received connections startedstopped respectively. observed variables toggle state represent eventrelevant type, state intrinsic meaning. Therefore, matricessingle independent parameter: rate transition either state other.way, observed variables really conditional Poisson processes.hidden variable H structured exploit domain knowledge. 8 statesgrouped pairs, one pair children. pair one childrennon-zero rate. Thus, H encodes type event happen substatemeta-state.entire model 4 89 approximately 500 million hidden states (as observedvariables observed times, distribution G Hs needtracked). Yet, submodel 8 hidden states, exact inference submodelreasonable. Thus, adapted particle filter smoother Fan Shelton(2008) distributional particles, producing Rao-Blackwellized particle filter Gsampled models (which independent, given full trajectory G)reasoned exactly.inference method allows learning specific models host using EM.models run data computer virus worm traffic injected(very slowly make blend background traffic). model asked likelihood 50-second window traffic (given previous traffic). likelihood thresh769fiShelton & Ciardoolded produce alarm (if likelihood dropped low). results out-performedSVM-spectrum kernels, nearest-neighbor (using features computer network literature task), methods task similar tasks.process system calls, model similar, single hidden variable coordinating behavior set observed system-call variables. dataset modeltrained time stamps system call. However, due clock resolution, manytime stamps same. Yet, temporal order preserved (although exactdurations events not). paper demonstrates method use data,without assuming event durations, employing ordering. case, resultsbetter SVM-spectrum kernel nearest-neighbor stidefrequency thresholding (Warrender, Forrest, & Pearlmutter, 1999).6.3 Relative ComparisonNeither two applications could currently tackled modelinglanguage. biological pathway example Section 6.1, transition system involved one variable (increasing number protein complexes, decreasingnumber individual proteins). CTBN cannot represent simultaneous transitionsmultiple variables. pathways cannot reformulated terms composite variablesprevent simultaneous transitions without placing state single variable.simpler example, consider system three variables, x, y, z. single eventperforms three variable updates time: {x0 x + y; 0 + 1; z 0 z 1}rate r(x, y, z). assume variable natural number range [0, . . . , n]update would move variable outside range disabled. Figure 20demonstrates EVMDD encode transition. Neither Kronecker encodingCTBN encode event without merging variables.Likewise, network traffic example Section 6.2 cannot handled currentdecision-diagram-based models. depends hidden unseen variables estimationtransient solutions conditioned data. critically, relies estimation modelparameters data, developed decision-diagram models.6.4 Current Research Directionsrange open modeling, algorithmic, theoretic problems. First, questions steady-state distributions efficient exact solutions addressedCTBNs (as EVMDDs). Similarly, questions structure parameter estimation approximate inference addressed EVMDDs (asCTBNs).Optimal decision making formulated general continuous-time Markov decision processes (Puterman, 1994). Yet, extending general mathematical frameworkstructured variable-based models largely unexplored (Kan & Shelton, 2008).CTBNs extended handle continuous-valued variables measurementslimited fashion (Ng et al., 2005), otherwise unexplored. many applications critical. underlying system discrete measurementscontinuous, techniques like Section 2.5 work. But, systems continuous staterequire stochastic differential equations (ksendal, 2003), least form. work770fiTutorial Structured Continuous-Time Markov Processesxzx0y0z0000100200010110210020120220001010101110201210011120111220211021121221002011102111x0y0zz0012121112221212022122222x202211012012011220112211010100111111111212112111111112121212121201011111Figure 20: example EVMDD encoding simultaneous transitions multiple variables. Top: 10 possible transitions resulting states (dash indicatesdisabled) state (x, y, z) state (x0 , 0 , z 0 ). Bottom, left: worst casearbitrary set 10 rates r(x, y, z). Bottom, right: best caser(x, y, z) = r1 (x, y)r2 (z). dot indicates positive value (used encodeparticular rates). block dots, one must equal 1 othersmust 1.Sarkka (2006) describes filtering smoothing models. Yet, parameter estimation much difficult systems continuous discrete state variablessystematically addressed.Finally, new approximate inference methods always interest (as probabilistic model). Recent methods auxiliary Gibbs sampler (Rao & Teh, 2013)belief propagation (El-Hay et al., 2010) demonstrate exploiting propertiescontinuous time lead great benefits. hope research exploresmethods.771fiShelton & Ciardo7. ConclusionsCompared discrete-time models, CTMPs better suited domains datareal-valued time stamps (the time events regular well-approximatedsingle clock step rate). Thus, selecting value time-slice-width (t)discrete-time model, either time width large resulting multiple events pertime window (obscuring temporal information), small resulting unnecessarycomputational burdens (propagating across many time windows). Further, optimalmiddle ground large small differ depending data size,application, model component.presented two different CTMP modeling languages. Edge-valued decision diagrams (of Section 4) general: allow multiple variables change simultaneously. contrast, CTBNs directly encode independence assumptions (see Section 5).Either EVMDD CTBN might compact given situation, althoughCTBN compactly rerepresented sum EVMDDs (see Section 5.1.3).models forms histories given rise differences available algorithms,distinctions greater. literature exact solution methods richerdecision diagram models. Furthermore, literature focused computingmodels steady state. Approximate methods (especially transients) model estimationnotably absent (from artificial intelligence point-of-view). literature CTBNsfocused model estimation approximate inference conditioned evidence.CTBN literature paid attention issues reachability (when much jointstate space reachable) optimization exact inference methods.processes natural synchronization clock (such modeling daily high lowtemperatures), discrete-time model best fit. processes without naturaltime-slice-width recommend continuous-time model. questions intereststeady states system exact solution necessary, EVMDD probablybest choice. model must built data approximate inference (especiallyconditioned data) necessary, CTBN probably best choice.However, shown two models share much common. Thus, hopeefficient exact algorithms EVMDDs applied CTBNs approximate inference model estimation methods CTBNs applied EVMDDs.so, choice model would depend upon model propertiesexisting suite algorithms. particular, CTBN makes assumption variable distinct. contrast, disjunctive EVMDD encoding decomposes systemlocal events. variable-level independencies easily read CTBN graph,disallow simultaneous transition multiple variables. application domainguide whether variable-level explicit independences simultaneous transitionsimportant.Regardless model used, believe time continuous quantity best modeledsuch. introduction matrix exponential would first seem complicatematters (compared discrete time), believe makes true coupling variablesobvious opens mathematical algorithmic possibilities efficientprecise solutions.772fiTutorial Structured Continuous-Time Markov ProcessesAcknowledgementsShelton supported DARPA award FA8750-12-2-0010 Laura P. LelandK. Whittier Virtual PICU Childrens Hospital Los Angeles (awards UCR-121010248220-SGNZZ0777-00). Ciardo supported NSF grant CCF-1442586.Appendix A. NP-hardness CTBN Inferencetheorem proof NP-hardness CTBN inference straight-forward extensions similar proof Bayesian networks (Koller & Friedman, 2009). literaturewidely accepted true, proof formally presented. Thus,straight-forward, present completeness.Definition 1. CTBN-Inf following decision problem. Given CTBN specifieddirected graph G nodes {X1 , X2 , . . . , XL }, set conditional intensity matricesQ = {QXi |pari }, initial distribution node independentmarginals {Xi }; variable Xj ; value Xj , xj ; time > 0, decide whetherPG,Q, (Xj (t) = xj ) > 0.Theorem 1. CTBN-Inf NP-hard.Proof. proof polynomial time reduction 3-SAT, following linessimilar proof Bayesian networks.Given 3-SAT problem variables z1 , z2 , . . . , zm clauses c1 , c2 , . . . , ckzA(i,j) jth variable (j {1, 2, 3}) clause (i {1, 2, . . . , k}) sign sA(i,j){+, }, construct CTBN + 2k 1 binary variables (taking values F ):Y1 , Y2 , . . . , Ym , C1 , C2 , . . . , Ck , B1 , B2 , . . . , Bk2 , S.Variable Yi parents, uniform initial distribution Yi , intensity matrixQYi | 0.Variable Ci three parents: YA(i,1) , YA(i,2) , YA(i,3) . none truth valueparents (yA(i,1) , yA(i,2) , yA(i,3) ) match formulas signs (sA(i,1) , sA(i,2) , sA(i,3) ),conditional intensity matrix 0. parent assignments(in least one1 1variable matches), conditional intensity matrices. initial distribution0 01 0 .Variable B1 parents C1 C2 . Variable Bi (for 1 < < k 1) parents Bi1Ci+1 . Variable parents Bk2 Ck .variables, conditional1 1intensity matrix two parents values. Otherwise, conditional0 0intensity matrix 0. variables initial distribution 1 0 .reduction polynomial size (all numeric values small polynomial number variables, maximum 3 parents) obviously outputpolynomial time. construction, Yi selects time 0 truth value zi neverchanges. Cj eventually change clause satisfiedselected truth values. Bj eventually change clauses 1j + 1 . similarly eventually change clausessatisfied.773fiShelton & CiardoMarkov nature process, time > 0, PG,Q, (S(t) = ) > 0formula satisfiable probability 0 formula satisfiable.demonstrates determining whether marginal non-zero NP-hard. similar construction Bayesian networks (Koller & Friedman, 2009), extendedshow absolute relative error formulations inference also NP-hard.ReferencesAjmone Marsan, M., Balbo, G., Conte, G., Donatelli, S., & Franceschinis, G. (1995). Modelling Generalized Stochastic Petri Nets. John Wiley & Sons.Asmussen, S., Nerman, O., & Olsson, M. (1996). Fitting phase-type distributions viaEM algorithm. Scandavian Journal Statistics, 23, 419441.Baier, C., Haverkort, B. R., Hermanns, H., & Katoen, J.-P. (2000). Model checkingcontinuous-time Markov chains transient analysis. Proceedings ComputerAided Verification, pp. 358372.Baskett, F., Chandy, K. M., Muntz, R. R., & Palacios-Gomez, F. (1975). Open, closed,mixed networks queues different classes customers. Journal ACM,22 (2), 335381.Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence Bayesian networks. Proceedings Twelfth International ConferenceUncertainty Artificial Intelligence, pp. 115123.Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.Proceedings Fourteenth Annual Conference Uncertainty Artificial Intelligence, pp. 3342.Bryant, R. E. (1986). Graph-based algorithms boolean function manipulation. IEEETransactions Computers, 35 (8), 677691.Buchholz, P., Ciardo, G., Donatelli, S., & Kemper, P. (2000). Complexity memoryefficient Kronecker operations applications solution Markov models.INFORMS Journal Computing, 12 (3), 203222.Burch, J. R., Clarke, E. M., & Long, D. E. (1991). Symbolic model checking partitionedtransition relations. International Conference Large Scale Integration, pp.4958. IFIP Transactions, North-Holland.Celikkaya, E. B., Shelton, C. R., & Lam, W. (2011). Factored filtering continuoustime systems. Proceedings Twenty-Seventh International ConferenceUncertainty Artificial Intelligence.Ciardo, G., Jones, R. L., Miner, A. S., & Siminiceanu, R. (2006). Logical stochasticmodeling SMART. Performance Evaluation, 63, 578608.Ciardo, G., & Siminiceanu, R. (2002). Using edge-valued decision diagrams symbolicgeneration shortest paths. Proceedings Formal Methods Computer-AidedDesign (FMCAD), LNCS 2517, pp. 256273. Springer.774fiTutorial Structured Continuous-Time Markov ProcessesCiardo, G., & Yu, A. J. (2005). Saturation-based symbolic reachability analysis usingconjunctive disjunctive partitioning. Proceedings Correct Hardware DesignVerification Methods (CHARME), LNCS 3725, pp. 146161. Springer.Ciardo, G., Zhao, Y., & Jin, X. (2012). Ten years saturation: Petri net perspective.Transactions Petri Nets Models Concurrency, V, 5195.Clarke, E., Fujita, M., McGeer, P. C., Yang, J. C.-Y., & Zhao, X. (1993). Multi-terminalbinary decision diagrams: efficient data structure matrix representation.IWLS 93 International Workshop Logic Synthesis.Codetta-Raiteri, D., & Portinale, L. (2010). Generalized continuous time Bayesian networksGSPN semantics. European Workshop Probabilistic Graphical Models,pp. 105112.Cohn, I., El-Hay, T., Friedman, N., & Kupferman, R. (2010). Mean field variational approximation continuous-time Bayesian networks. Journal Machine LearningResearch, 11 (Oct), 27452783.Cohn, I., El-Hay, T., Kupferman, R., & Friedman, N. (2009). Mean field variational approximation continuous-time Bayesian networks. Proceedings Twenty-FifthInternational Conference Uncertainty Artificial Intelligence.Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.Computational Intelligence, 5 (3), 142150.Deavours, D. D., Clark, G., Courtney, T., Daly, D., Derisavi, S., Doyle, J. M., Sanders,W. H., & Webster, P. G. (2002). mobius framework implementation.IEEE Transactions Software Engineering, 28 (10), 956969.Didelez, V. (2008). Graphical models marked point processes based local independence. Journal Royal Statitical Society: Series B, 70 (1), 245264.Donatelli, S. (1994). Superposed generalized stochastic Petri nets: definition efficientsolution. Proceedings International Conference Application TheoryPetri Nets (ICATPN), LNCS 815, pp. 258277. Springer.El-Hay, T., Cohn, I., Friedman, N., & Kupferman, R. (2010). Continuous-time belief propagation. Proceedings 27th International Conference Machine Learning,pp. 343350, Haifa, Israel.El-Hay, T., Friedman, N., & Kupferman, R. (2008). Gibbs sampling factorized continuoustime Markov processes. Proceedings Twenty-Fourth Conference Uncertainty Artificial Intelligence, pp. 169178.Fan, Y., & Shelton, C. R. (2008). Sampling approximate inference continuous timeBayesian networks. Proceedings Tenth International Symposium ArtificialIntelligence Mathematics.Fan, Y., & Shelton, C. R. (2009). Learning continuous-time social network dynamics.Proceedings Twenty-Fifth International Conference Uncertainty ArtificialIntelligence.Fan, Y., Xu, J., & Shelton, C. R. (2010). Importance sampling continuous time Bayesiannetworks. Journal Machine Learning Research, 11 (Aug), 21152140.775fiShelton & CiardoFelsenstein, J. (1981). Evolutionary trees DNA sequences: maximum likelihoodapproach. Journal Molecular Evolution, 17, 368376.Fernandes, P., Plateau, B., & Stewart, W. J. (1998). Efficient descriptor-vector multiplication stochastic automata networks. Journal ACM, 45 (3), 381414.Fox, B. L., & Glynn, P. W. (1988). Computing poisson probabilities. CommunicationsACM, 31 (4), 440445.Friedman, N. (1997). Learning belief networks presence missing values hiddenvariables. Proceedings Fourteenth International Conference MachineLearning, pp. 125133.Gatti, E., Luciani, D., & Stella, F. (2012). continuous time Bayesian network modelcardiogenic heart failure. Flexible Services Manufacturing Journal, 24 (4),496515.Grassmann, W. K. (1977). Transient solutions Markovian queueing systems. Computers& Operations Research, 4 (1), 4753.Gunawardana, A., Meek, C., & Xu, P. (2012). model temporal dependencies eventstreams. Advances Neural Information Processing Systems, Vol. 24.Heath, J., Kwiatkowska, M., Norman, G., Parker, D., & Tymchyshyn, O. (2006). Probabilistic model checking complex biological pathways. Priami, C. (Ed.), ProceedingsComputational Methods Systems Biology (CMSB), Vol. 4210 Lecture NotesBioinformatics, pp. 3247. Springer Verlag.Herbrich, R., Graepel, T., & Murphy, B. (2007). Structure failure. Proceedings2nd USENIX workshop Tackling computer systems problems machinelearning techniques, pp. 16. USENIX Association.Hobolth, A., & Stone, E. A. (2009). Simulation endpoint-conditioned continuous-timeMarkov chains finite state space, applications molecular evolution.Annals Applied Statistics, 3 (3), 12041231.Kam, T., Villa, T., Brayton, R. K., & Sangiovanni-Vincentelli, A. (1998). Multi-valueddecision diagrams: theory applications. Multiple-Valued Logic, 4 (12), 962.Kan, K. F., & Shelton, C. R. (2008). Solving structured continuous-time Markov decision processes. Proceedings Tenth International Symposium ArtificialIntelligence Mathematics.Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles Techniques. MIT Press.Kwiatkowska, M., Norman, G., & Parker, D. (2011). PRISM 4.0: Verification probabilistic real-time systems. Gopalakrishnan, G., & Qadeer, S. (Eds.), ProceedingsComputer Aided Verification, Vol. 6806 LNCS, pp. 585591. Springer.Kwiatkowska, M. Z., Norman, G., & Parker, D. (2004). Probabilistic symbolic model checking PRISM: hybrid approach. Software Tools Technology Transfer, 6 (2),128142.Lam, W., & Bacchus, F. (1994). Learning Bayesian belief networks: approach basedMDL principle. Computational Intelligence, 10, 269293.776fiTutorial Structured Continuous-Time Markov ProcessesMoler, C., & Loan, C. V. (2003). Nineteen dubious ways compute exponentialmatrix, twenty-five years later. SIAM Review, 45 (1), 349.Najfeld, I., & Havel, T. F. (1994). Derivatives matrix exponential computation. Tech. rep. TR-33-94, Center Research Computing Technology, HarvardUniversity.Najfeld, I., & Havel, T. F. (1995). Derivatives matrix exponential computation. Advances Applied Mathematics, 16, 321375.Ng, B., Pfeffer, A., & Dearden, R. (2005). Continuous time particle filtering. ProceedingsNineteenth International Joint Conference Artificial Intelligence, pp. 13601365.Nodelman, U., & Horvitz, E. (2003). Continuous time Bayesian networks inferringusers presence activities extensions modeling evaluation. Tech. rep.MSR-TR-2003-97, Microsoft Research.Nodelman, U., Koller, D., & Shelton, C. R. (2005). Expectation propagation continuoustime Bayesian networks. Proceedings Twenty-First International ConferenceUncertainty Artificial Intelligence, pp. 431440.Nodelman, U., Shelton, C. R., & Koller, D. (2002). Continuous time Bayesian networks.Proceedings Eighteenth International Conference Uncertainty ArtificialIntelligence, pp. 378387.Nodelman, U., Shelton, C. R., & Koller, D. (2003). Learning continuous time Bayesiannetworks. Proceedings Nineteenth International Conference UncertaintyArtificial Intelligence, pp. 451458.Nodelman, U., Shelton, C. R., & Koller, D. (2005). Expectation maximization complexduration distributions continuous time Bayesian networks. ProceedingsTwenty-First International Conference Uncertainty Artificial Intelligence, pp.421430.ksendal, B. (2003). Stochastic Differential Equations: Introduction Applications(Sixth edition). Springer-Verlag.Parikh, A. P., Gunamwardana, A., & Meek, C. (2012). Cojoint modeling temporaldependencies event streams. UAI Bayesian Modelling Applications Workshop.Pfeffer, A. (2009). CTPPL: continuous time probabilistic programming language.Proceedings 21st International Joint Conference Artifical Intelligence, pp.19431950.Plateau, B. (1985). stochastic structure parallelism synchronisation modelsdistributed algorithms. Proceedings ACM SIGMETRICS, pp. 147153.Portinale, L., & Codetta-Raiteri, D. (2009). Generalizing continuous time Bayesian networks immediate nodes. Proceedings Workshop Graph StructureKnowledge Represetnation Reasoning, pp. 1217.Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). NumericalRecipes C (Second edition). Cambridge University Press.777fiShelton & CiardoPuterman, M. L. (1994). Markov Decision Processes. Wiley-Interscience.Rajaram, S., Graepel, T., & Herbrich, R. (2005). Poisson networks: model structuredpoint processes. Proceedings AI STATS 2005 Workshop.Rao, V., & Teh, Y. W. (2011). Fast MCMC sampling Markov jump processes continuous time Bayesian networks. Proceedings Twenty-Seventh InternationalConference Uncertainty Artificial Intelligence.Rao, V., & Teh, Y. W. (2012). MCMC continuous-time discrete-state systems.Advances Neural Information Processing Systems 25, pp. 710718.Rao, V., & Teh, Y. W. (2013). Fact MCMC sampling Markov jump processesextensions. Journal Machine Learning Research, 1, 126.Roux, P., & Siminiceanu, R. (2010). Model Checking Edge-valued Decision Diagrams. Proceedings Second NASA Formal Methods Symposium (NFM 2010),NASA/CP-2010-216215, pp. 222226. NASA.Saria, S., Nodelman, U., & Koller, D. (2007). Reasoning right time granularity.Proceedings Twenty-third Conference Uncertainty AI, pp. 421430.Sarkka, S. (2006). Recursive Bayesian Inference Stochastic Differential Equations. Ph.D.thesis, Helsinki University Technology.Shimony, S. E. (1991). Explanation, irrelevance statistical independence. ProceedingsNinth National Conference Artificial Intelligence, pp. 482487.Wan, M., Ciardo, G., & Miner, A. S. (2011). Approximate steady-state analysis largeMarkov models based structure decision diagram encoding. Performance Evaluation, 68, 463486.Warrender, C., Forrest, S., & Pearlmutter, B. (1999). Detecting intrusions using systemcalls: Alternative data models. IEEE Symposium Security Privacy, IEEEComputer Society.Weiss, J. C., Natarajan, S., & Page, D. (2012). Multiplicative forests continuous-timeprocesses. Advanced Neural Information Processing Systems.Weiss, J. C., & Page, D. (2013). Forest-based point processes event prediction electronic health records. Proceedings European Conference Machine Learning Principals Practice Knowledge Discovery Databases (ECMLPKDD).Williams, C. K. I. (1998). Prediction Gaussian processes: linear regressionlinear prediction beyond. Jordan, M. I. (Ed.), Learning Graphical Models,pp. 599621.Xu, J., & Shelton, C. R. (2008). Continuous time Bayesian networks host level networkintrusion detection. European Conference Machine Learning, pp. 613627.Xu, J., & Shelton, C. R. (2010). Intrusion detection using continuous time Bayesian networks. Journal Artificial Intelligence Research, 39, 745774.778fiJournal Artificial Intelligence Research 51 (2014) 645-705Submitted 05/14; published 12/14Complexity Answering Conjunctive NavigationalQueries OWL 2 EL Knowledge BasesGiorgio StefanoniBoris Motikgiorgio.stefanoni@cs.ox.ac.ukboris.motik@cs.ox.ac.ukDepartment Computer Science, University OxfordParks Road, Oxford OX1 3QD, United KingdomMarkus KrotzschSebastian Rudolphmarkus.kroetzsch@tu-dresden.desebastian.rudolph@tu-dresden.deFaculty Computer Science, TU DresdenNothnitzer Strae 46, 01062 Dresden, GermanyAbstractOWL 2 EL popular ontology language supports role inclusionsaxiomsform S1 Sn v capture compositional properties roles. Role inclusions closelycorrespond context-free grammars, used show answering conjunctivequeries (CQs) OWL 2 EL knowledge bases unrestricted role inclusions undecidable. However, OWL 2 EL inherits OWL 2 DL syntactic regularity restrictionrole inclusions, ensures role chains implying particular role describedusing finite automaton (FA). sufficient ensure decidability CQ answering;however, FAs worst-case exponential size known approachesprovide tight upper complexity bound.paper, solve open problem show answering CQs OWL2 EL knowledge bases PSpace-complete combined complexity (i.e., complexitymeasured total size input). end, use novel encoding regular roleinclusions using bounded-stack pushdown automatathat is, FAs extended stackbounded size. Apart theoretical interest, encoding used practical tableaualgorithms avoid exponential blowup due role inclusions. addition, sharpenlower complexity bound show problem PSpace-hard even considerrole inclusions part input (i.e., query parts knowledgebase fixed). Finally, turn attention navigational queries OWL 2 ELknowledge bases, show answering positive, converse-free conjunctive graphXPath queries PSpace-complete well; interesting since allowing converseoperator queries known make problem ExpTime-hard. Thus, paperpresent several important contributions landscape complexity answeringexpressive queries description logic knowledge bases.1. IntroductionDescription logics (DLs) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2010)family knowledge representation formalisms logically underpin Web Ontology Language OWL 2 (Cuenca Grau, Horrocks, Motik, Parsia, Patel-Schneider, & Sattler,2008). DL knowledge bases describe domain terms concepts (i.e., unary predicates),roles (i.e., binary predicates), individuals (i.e., constants), describe relationships concepts, roles, individuals using logical axioms. DLs OWLc2014AI Access Foundation. rights reserved.fiStefanoni, Motik, Krotzsch, & Rudolph2 steadily gaining popularity provide developers moderninformation systems flexible graph-like data model natural countless application areas, Semantic Web (Gutierrez, Hurtado, Mendelzon, & Perez, 2011),social network analysis (Fan, 2012), network traffic analysis (Barrett, Jacob, & Marathe,2000). Answering queries DL/OWL knowledge bases core service applicationsdiverse monitoring financial products within Italian Ministry EconomyFinance (De Giacomo et al., 2012), accessing real-time diagnostic data turbines (Gieseet al., 2013), integrating configuration data air traffic control systems (Calvaneseet al., 2011). Due practical importance query answering, theoretical investigationexpressivity computational complexity query languages highresearch agenda knowledge representation community past decade.Conjunctive queries (CQs) (Chandra & Merlin, 1977) basic class queriesrelational databases. Querying DL knowledge bases using CQs studied diverse range settings (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007; PerezUrbina, Motik, & Horrocks, 2010; Rudolph & Glimm, 2010; Kontchakov, Lutz, Toman,Wolter, & Zakharyaschev, 2011; Ortiz, Rudolph, & Simkus, 2011; Gottlob & Schwentick,2012; Venetis, Stoilos, & Stamou, 2012). However, conjunctive queries first-order definable thus cannot express certain important properties graph reachability.Regular path queries (RPQs) (Cruz, Mendelzon, & Wood, 1987; Barcelo, 2013) alternative query language capable describing connections graph vertices usingregular expressions, allowing users navigate inside graph. example, RPQ(isPartOf hasLocation) retrieves pairs vertices connected via zero isPartOfedges followed one hasLocation edge. Furthermore, 2RPQs extend RPQs converse operator (i.e., backward navigation) (Calvanese, Vardi, De Giacomo, & Lenzerini,2000); nested regular expressions allow existential quantification paths (Perez,Arenas, & Gutierrez, 2010); C(2)RPQs extend (2)RPQs CQs conjunctions (2)RPQs (Calvanese, De Giacomo, Lenzerini, & Vardi, 2000; Bienvenu, Ortiz, &Simkus, 2013). Finally, inspired XPath query language XML, graph XPath queries(GXQs) recently proposed language querying graph databases (Libkin,Martens, & Vrgoc, 2013) DL knowledge bases (Kostylev, Reutter, & Vrgoc, 2014; Bienvenu, Calvanese, Ortiz, & Simkus, 2014). GXQs extend 2RPQs negation regularexpressions, checking properties vertices using Boolean combinations node testsis, concepts existential quantifications paths. example, graph XPathquery (isPartOf test(Cell hhasSpecialityi) hasLocation) refines aforementioned RPQrequiring node isPartOf edges hasLocation edge instance Cell concept outgoing hasSpeciality edge. Graph XPathqueries straightforwardly extended conjunctive graph XPath queries (CGXQs).query languages Boolean answer variables; hence, answerquery Boolean value.1.1 Problem SettingAlthough computing answers query DL knowledge base function problem,common literature consider complexity associated decision problemis, checking whether Boolean query entailed knowledge base. article646fiThe Complexity Answering CQs GXQs OWL 2 EL KBsfollow well-established practice analyse computational properties severalquery languages DL knowledge bases. follow Vardi (1982) measure inputsize two ways: combined complexity measures complexity terms combinedsize query knowledge base, data complexity measures complexityterms size data (i.e., query parts knowledge basesconsidered fixed).computational properties query answering DL knowledge bases dependexpressivity constructs used knowledge base query languageused. particular, conjunctive query answering expressive description logicsleast exponential combined complexity (Glimm, Lutz, Horrocks, & Sattler, 2008; Lutz,2008) intractable data complexity (Calvanese, De Giacomo, Lembo, Lenzerini, &Rosati, 2013; Ortiz, Calvanese, & Eiter, 2008). problem becomes tractable datacomplexity RL (Grosof, Horrocks, Volz, & Decker, 2003; ter Horst, 2005)QL (Calvanese et al., 2007; Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009) profilesOWL 2, several worst-case optimal algorithms proposed perform wellpractice (Urbani, van Harmelen, Schlobach, & Bal, 2011; Rodriguez-Muro & Calvanese,2012). paper, however, focus OWL 2 EL profile OWL 2, basedEL family DLs (Baader, Brandt, & Lutz, 2005). Basic reasoning problemsOWL 2 EL, checking concept subsumption instance checking, decidedpolynomial time (Baader et al., 2005; Krotzsch, 2011), makes languageinteresting practical applications. Motivated observation, paper presentseveral novel complexity results answering queries OWL 2 EL knowledge bases.One important modelling constructs OWL 2 EL role inclusionsaxiomsform S1 Sn v express compositional properties roles. example,following inclusions state role isPartOf transitive that, x locatedpart z, x located z.isPartOf isPartOf v isPartOfhasLocation isPartOf v hasLocationPrior introduction EL family, role inclusions already identifiedsource undecidability expressive DLs loosely correspond context-freegrammars: inclusion S1 Sn v knowledge base seen production ruleS1 Sn , knowledge base induces context-free language L(S) roleS. Using correspondence, Wessel (2001) showed checking satisfiability ALCRknowledge bases unrestricted role inclusions undecidable. regain decidability,Horrocks Sattler (2004) proposed syntactic regularity restriction role inclusionsensuring language L(S) regular thus recognised using finiteautomaton (FA); Kazakov (2008) later showed that, cases, size automatonnecessarily exponential knowledge base size. OWL 2 DL profile OWL 2extends ALCR thus incorporates regularity restriction definition.Even unrestricted role inclusions, standard reasoning problems ELsolved polynomial time (Baader et al., 2005). Moreover, Stefanoni, Motik, Horrocks(2013) showed answering CQs OWL 2 EL knowledge bases without role inclusionsNP-complete. However, using correspondence role inclusions contextfree grammars, Rosati (2007) Krotzsch, Rudolph, Hitzler (2007) independentlyproved answering CQs EL knowledge bases unrestricted role inclusions647fiStefanoni, Motik, Krotzsch, & Rudolphundecidable; furthermore, Krotzsch et al. (2007) also showed checking concept subsumptions EL knowledge bases inverse roles unrestricted role inclusionsundecidable.OWL 2 EL inherits regularity restriction OWL 2 DL, undecidability proofs Rosati (2007) Krotzsch et al. (2007) apply OWL 2 EL.fact, Krotzsch et al. (2007) showed answering CQs EL knowledge bases extendedregular role inclusions PSpace-hard combined complexity, proposedCQ answering algorithm fragment OWL 2 EL regular role inclusions.algorithm, however, runs PSpace if, role S, language L(S) represented using automaton polynomial size; due mentioned result Kazakov(2008), approach provide us matching PSpace upper boundproblem. Ortiz et al. (2011) proposed different algorithm answering CQs OWL 2EL knowledge bases (with regular role inclusions without restriction usagefeatures). Similarly algorithm Krotzsch et al. (2007), algorithmOrtiz et al. (2011) also encodes regular role inclusions using finite automata. Hence,algorithms run time polynomial size data thus settlequestion data complexity, settle question combined complexity.comparatively works studying complexity (conjunctive) graphXPath queries DL knowledge bases. particular, Kostylev et al. (2014) observedGXQs closely related propositional dynamic logic full negation (Harel, Tiuryn,& Kozen, 2000), immediately shows answering GXQs DL knowledge basesundecidable even respect empty knowledge base. Several GXQ fragmentsproposed possible solution problem: path-positive GXQs disallow negationrole expressions, positive GXQs prohibit negation concepts well.Kostylev et al. (2014) showed answering path-positive GXQs intractable datacomplexity already queries without transitive closure operator knowledgebases containing instance assertions. Recently, Bienvenu et al. (2014) showedanswering positive GXQs fragment OWL 2 EL tractable data complexity,ExpTime-complete combined complexity.1.2 Contributionspaper, present several novel complexity results answering queries OWL 2EL knowledge bases.First, present first CQ answering algorithm handle OWL 2 EL(with regular role inclusions without restriction size FAs)runs PSpace, thus settle open question combined complexityCQ answering OWL 2 EL. result based novel encoding languagesinduced regular role inclusions using pushdown automata (PDAs)that is, FAs extendedstack. show that, role S, construct polynomial timePDA accepts language L(S) whose computations use stack size linearnumber role inclusions. Bounded-stack PDAs (Anselmo, Giammarresi, & Varricchio,2003) recognise precisely class regular languages exponentiallysuccinct finite automata (Geffert, Mereghetti, & Palano, 2010). obtain CQanswering algorithm running PSpace, extend algorithm Krotzsch et al. (2007)648fiThe Complexity Answering CQs GXQs OWL 2 EL KBsdatacombinedELHOdrPTimeOWL 2 ELPTimeHorn-SHOIQPTimeHorn-SROIQPTime(Ortiz et al., 2011)(Theorem 31)(Ortiz et al., 2011)(Ortiz et al., 2011)NPPSpaceExpTime2ExpTime(Stefanoni et al., 2013)(Theorem 31)(Ortiz et al., 2011)(Ortiz et al., 2011)Table 1: complexity landscape CQ answering (all completeness results)handle universal role, keys, self-restrictions, reflexive roles, thus coveringfeatures EL profile apart datatypes, adapt handleregular role inclusions encoded using PDAs. Apart allowing us obtain complexityresults presented paper, tableau algorithm Horrocks, Kutz, Sattler (2006)used popular reasoners Pellet (Sirin, Parsia, Cuenca Grau, Kalyanpur, & Katz,2007) FaCT++ (Tsarkov & Horrocks, 2006) straightforwardly modified usebounded-stack PDAs instead FAs, could eliminate potential source inefficiencypractice. Finally, brevity simplicity deal datatypes paper;however, set OWL 2 EL datatypes designed enable datatypereasoning using external datatype checking procedure (Baader, Brandt, & Lutz, 2008;Cuenca Grau et al., 2008) easily incorporated algorithm.Second, improve PSpace lower bound Krotzsch et al. (2007) showinganswering CQs OWL 2 EL PSpace-hard already role inclusionsconsidered part input (i.e., conjunctive query, TBox, ABoxfixed). Furthermore, show CQs answered polynomial time queryrole inclusions fixed, emphasises observation role inclusionsmain source problems PSpace-hardness.Third, show positive, converse-free CGXQsthat is, CGXQs allownegation paths, negation concepts, converse operatorcan answeredOWL 2 EL knowledge bases using polynomial space. particular, OWL 2 EL allowsrole inclusions, self-restrictions, reflexive roles, allow us polynomially reduce answering CGXQ answering CQ extended knowledge base. alsoshow answering positive, converse-free GXQs (i.e., CGXQs single atom)done time polynomial input size. result interesting Bienvenuet al. (2014) proved answering positive GXQs EL knowledge bases ExpTimecomplete; hence, adding converse operator increases complexity GXQs.results thus show answering GXQs CGXQs difficult instance checkinganswering conjunctive queries, respectively, least theoretical perspectivemakes GXQs CGXQs appealing query languages OWL 2 EL knowledge bases.1.3 Summary Complexity LandscapeTable 1 summarises complexity landscape answering CQs various DLs relatedOWL 2 EL. Here, ELHOdrfragment OWL 2 EL obtained allowing simplerole inclusions form v S, disallowing universal role, reflexive roles,self-restrictions, datatypes, combined complexity result logic dueStefanoni et al. (2013). Furthermore, Horn-SHOIQ extends ELHOdrinverse rolesHorn qualified number restrictions, Horn-SROIQ extends Horn-SHOIQ role649fiStefanoni, Motik, Krotzsch, & Rudolphdatapositivepositiveconverse-free converse-freeGXQsCGXQsPTime-cPTime-c(Theorem 34)combinedPTime-c(Theorem 34)positiveGXQspath-positiveGXQsGXQsPTime-hcoNP-hcoNP-h(Theorem 34) (Bienvenu et al., 2014) (Kostylev et al., 2014) (Kostylev et al., 2014)PSpace-cExpTime-hExpTime-hundecidable(Theorem 34) (Bienvenu et al., 2014) (Bienvenu et al., 2014) (Kostylev et al., 2014)Table 2: complexity answering navigational queries OWL 2 EL knowledge bases(c means complete, h means hard)inclusions; results logics due Ortiz et al. (2011). CQ answering PTimecomplete data complexity cases, essentially due factlogics Horn disjunctive reasoning needed. combined complexity,table illustrates presence different constructs affects complexity answeringCQs. particular, extending ELHOdrrole inclusions increases complexityNP PSpace; PSpace lower bound, increase solely due role inclusions.Furthermore, extending ELHOdrinverse roles increases complexity NPExpTime. Finally, extending OWL 2 EL inverse roles increases complexityPSpace 2ExpTime.Table 2 summarises complexity landscape answering navigational queriesOWL 2 EL knowledge bases. one see, adding converse operator increasescombined complexity GXQs ExpTime (Bienvenu et al., 2014). Moreover, addingnegation node tests increases data complexity GXQs coNP, whereas addingnegation path expressions leads undecidability combined complexity (Kostylevet al., 2014). contrast, existential quantification paths increase complexity: answering positive, converse-free (C)GXQs OWL 2 EL knowledge basesdifficult answering (C)RPQs EL knowledge bases (Bienvenu et al., 2013).1.4 Organisation Articlerest article organised follows. Section 2, present basic definitionsfinite automata, pushdown automata, DL underpinning OWL 2 EL, conjunctivequeries. Section 3, introduce novel encoding regular role inclusions using PDAsbounded stack size. Section 4, present CQ answering algorithm OWL 2EL discuss complexity. Section 5, present improved PSpace lower-boundanswering CQs OWL 2 EL. Finally, Section 6, introduce (conjunctive) graphXPath queries, show reduce problem answering positive, converse-freeconjunctive graph XPath queries answering ordinary conjunctive queries, presentaforementioned complexity results.650fiThe Complexity Answering CQs GXQs OWL 2 EL KBs2. Preliminariessection recapitulate basic definitions finite automata, pushdown automata,DL ELRO+ underpinning OWL 2 EL, conjunctive queries. rest paper,[i..j] set containing natural number k N k j.2.1 Automata Language Theoryarticle, use standard notions alphabets (which must finite), strings, stringconcatenation, Kleene operators, languages formal language theory (Hopcroft,Motwani, & Ullman, 2003). assume alphabets contain special symbol, use label transitions automata consume input symbols.Furthermore, empty word. Finally, w w0 words, |w| number symbolsoccurring w; w w0 unique word w00 w := w00 w0 w00 exists,otherwise w w0 undefined.2.1.1 Finite Automatafinite automaton (FA) tuple F = hQ, , , i, f Q finite set states,input alphabet, : Q {} 7 2Q transition function, Q start state,f Q final state. F deterministic |(s, )| = 0 |(s, c)| 1Q c ; otherwise, F nondeterministic. size |F| F numbersymbols used encode F tape Turing machine.instantaneous description F pair hs, wi Q w .derivation relation ` F smallest set that, states s0 Q,symbol c , word w ,s0 (s, c), hs, c wi ` hs0 , wi;s0 (s, ), hs, wi ` hs0 , wi.Let ` reflexive transitive closure `. Then, language accepted Fdefined L(F) = {w | hi, wi ` hf, i}. language L regular FAF exists L = L(F).2.1.2 Pushdown Automatapushdown automaton (PDA) tuple P = hQ, , , , i, I, f, F Q finite setstates; input alphabet; stack alphabet; transition function mappingstate Q, symbol c {}, stack symbol X finite subset(s, c, X) Q ; Q start state; start stack ; f Q finalstate; F final stack. size |P| P number symbols usedencode P tape Turing machine.instantaneous description P triple hs, w, Q, w ,. read stack content left rightthat is, leftmost symboltop stack. derivation relation ` P smallest set that,states s0 Q, symbol c , word w , stack symbol X ,words 0 ,651fiStefanoni, Motik, Krotzsch, & Rudolphhs0 , 0 (s, c, X) implies hs, c w, X ` hs0 , w, 0 i;hs0 , 0 (s, , X) implies hs, w, X ` hs0 , w, 0 i.Let ` reflexive transitive closure relation `. Then, language acceptedP defined L(P) = {w | hi, w, Ii ` hf, , F i}.definitions PDA P language L(P) somewhat nonstandard:literature typically considers Hopcroft PDA (Hopcroft et al., 2003) Ph differsdefinition contain final stack F initial stack symbol(rather word ); moreover, language accepted Ph definedLh (Ph ) = {w | : hi, w, Ii ` hf, , i}. show next definitionsequivalent standard definitions Hopcroft et al. (2003).Proposition 1. following two properties hold.(1) PDA P, Hopcroft PDA Ph exists L(P) = Lh (Ph ).(2) Hopcroft PDA Ph , PDA P exists Lh (Ph ) = L(P).Proof (Sketch). first prove property (1), prove property (2).(1) show transform arbitrary PDA P Hopcroft PDA PhL(P) = Lh (Ph ). Ph uses fresh initial state i0 fresh stack symbols Z0occurring . Symbol Z0 start stack symbol Ph ; furthermore, Ph new-transition moves PDA state i0 initial state P replacing Z0, start stack P. point, Ph simulates P, always leavingbottom stack reaches final state f P. Next, Ph uses freshstates s1 , . . . , s|F | fresh -transitions move Ph state f s|F | reading Fstack. Finally, s|F | , PDA Ph -moves fresh final state f 0 top-mostsymbol stack , thus accepting input whenever P reaches f Fstack. Automata P Ph clearly accept languages.(2) show transform arbitrary Hopcroft PDA Ph PDA PLh (Ph ) = L(P). PDA P uses fresh stack symbol , initial stackinitial stack symbol Ph , final stack empty word. P simulates Ph ,always leaving bottom stack reaches final state f Ph . Next, P-moves fresh final state f 0 pops topmost symbol stack. point,PDA takes -transitions empty stack, eventually reaching final stateempty stack. Automata P Ph clearly accept languages.k natural number, k-bounded language accepted P set Lk (P) containing word w derivation hs0 , w0 , 0 ` ` hsn , wn , n existss0 sn start final state P, respectively;w0 = w wn = ;0 n start final stack P, respectively;|i | k [0..n].652fiThe Complexity Answering CQs GXQs OWL 2 EL KBsThen, P k-bounded stack L(P) = Lk (P). stack P boundedconstant, PDA P simulated finite automaton encodes stack contentsusing states, L(P) regular, translating P finite automaton mayrequire space exponential k (Geffert et al., 2010). contrast, following propositionshows exists PDA Pk L(Pk ) = Lk (P) size Pk polynomialsize P k.Proposition 2. PDA P natural number k, one compute polynomialtime PDA Pk L(Pk ) = Lk (P).Proof. Let P = hQ, , , , i, I, f, F PDA let k N natural number. LetPk = hQk , , , k , ik , I, fk , F PDA definedQk = Q [0..k];transition function k smallest function that, ` [0..k], symbolc {}, states s, s0 Q, word hs0 , (s, c, X)` + || 1 k, hhs0 , ` + || 1i, k (hs, `i, c, X);ik = hi, |I|i fk = hf, |F |i.Clearly, Pk computed time polynomial size P k. Let ` `kderivation relations P Pk , respectively. definitions k ik ,hhs, `i, w, `k hhs0 , ji, w0 , 0 hs, w, ` hs0 , w0 , 0 i, || = ` | 0 | = j,max(`, j) k. Thus, Lk (P) = L(Pk ), required.2.2 Description Logic ELRO+ Conjunctive Queriesdescription logic ELRO+ , underpinning OWL 2 EL, defined w.r.t. signature consisting mutually disjoint countably infinite alphabets C, R, atomic concepts,roles, individuals, respectively. assume {>c , c } C, >c topconcept c bottom concept; similarly, assume {>r , r } R, >rtop role (universal role) r bottom role. individual I, expression{a} nominal is, concept consisting precisely individual a. Then, N setcontaining nominal {a} individual I. call B C N basic concept.role chain word R; || = 0, call empty role chain write. Concepts, TBox axioms, RBox axioms, ABox axioms defined specifiedTable 3. ELRO+ TBox finite set concept inclusions, range restrictions,keys; ELRO+ RBox R finite set role inclusions.R ELRO+ RBox, let R := {>r } {S R | occurs R}; furthermore,rewrite relation =w.r.t. R smallest relation role chains followingholds role chains 1 2 .1 2 =1 2 axiom v R.1 >r 2 =1 2 role chain R .=reflexivetransitive closure= . role, L(S) := { R | =}language induced RBox R. role simple R if, role chain653fiStefanoni, Motik, Krotzsch, & RudolphConcepts:top conceptbottom conceptnominalconjunctionself-restrictionexistential restrictionRole chains:top rolebottom roleempty role chainnonempty role chainTBox axioms:concept inclusionrange restrictionkeyRBox axioms:role inclusionABox axioms:concept assertionrole assertionSyntaxSemantics>cc{a}C uDS.SelfS.C{aI }C DI{x | hx, xi }{x | C : hx, yi }>rrS1 Sn{hx, xi | x }S1I SnICvDrange(S, C)key(C, S1 . . . Sn )C DICx, y, z1 , . . . , znindividuals a, b, c1 , . . . , cn existx = aI , = bI , zi = cIi 1 n,x = holds whenever {x, y} C{hx, zi i, hy, zi i} SiI 1 n.vSbISIA(b)S(a, b)haI , bITable 3: Interpreting ELRO+ concepts, roles, axioms interpretation = hI ,S=, || 1. ELRO+ ABox finite set concept role assertions.Finally, ELRO+ knowledge base (KB) tuple K = hT , R, Ai ELRO+TBox, R ELRO+ RBox, ELRO+ ABoxconcept S.Self occurring , role simple R;S1 Sn v R range(S 0 , C) 0 =S, role Sn0 R00exists Sn =Sn range(Sn , C) .Let |T |, |R|, |A| numbers symbols needed encode , R, A, respectively,tape Turing machine, let |K| = |T |+|R|+|A|. Furthermore, knowledgebase, TBox, ABox, define:= {a | occurs }, N := {{a} | }, C := {A C | occurs }.654fiThe Complexity Answering CQs GXQs OWL 2 EL KBssemantics ELRO+ defined follows. interpretation tuple = hI ,nonempty set domain elements, called domain I, interpretation function maps individual domain element aI ,atomic concept C \ {>c , c } set AI , atomic role R \ {>r , r }relation . Function extended concepts role chains shownupper part Table 3, denotes composition binary relations. interpretation model K satisfies axioms occurring K shown bottomTable 3. Moreover, K consistent model K exists; K inconsistent modelK exists; K entails first-order sentence (resp. concept inclusion C v roleinclusion v S), written K |= (resp. K |= C v K |= v S), |= (resp. C DI) model K. definition L(S), L(S) impliesK |= v S. Knowledge base consistency, entailment concept inclusions, entailmentrole inclusions decided polynomial time (Krotzsch, 2011; Baader et al., 2005).2.2.1 Conjunctive Queriesterm individual variable. atom expression form A(t) R(t0 , t)atomic concept, R role, t0 terms. conjunctive query (CQ)formula q = ~y .(~x, ~y ) conjunction atoms variables ~x ~y . Variables ~xanswer variables q. ~x empty, call q = ~y .(~y ) Boolean CQ (BCQ).substitution partial mapping variables terms; dom() rng()domain range , respectively. conjunction atoms, ()result applying substitution atoms . Then, (q) = ~z.(), ~z contains(i) (y) variable ~y (y) variable, (ii) variable ~y(y) undefined. definition (q) somewhat nonstandard quantifiedvariables also replaced: example, given q = y1 , y2 , y3 .R(y1 , y2 ) (y1 , y3 )= {y2 7 a, y3 7 z}, (q) = y1 , z.R(y1 , a) (y1 , z).Let K = hT , R, Ai ELRO+ knowledge base let q = ~y .(~x, ~y ) CQ.q K q uses predicates individuals occurring K. substitutioncandidate answer q K, dom() = ~x rng() IK , certainanswer q K K |= (q). Answering q K amounts computingset certain answers q K. stated, CQ answering function problem;thus article study complexity associated decision problem named BCQanswering, problem deciding, given Boolean CQ q K, whether K |= q.Please note BCQ answering equivalent recognition problem decides,given CQ q K candidate answer , whether certain answer q K.Following Vardi (1982), combined complexity assumes q K partinput, data complexity assumes ABox part input.2.3 Ensuring Decidability BCQ Answering via RegularityRosati (2007) Krotzsch et al. (2007) independently showed answering BooleanCQs ELRO+ knowledge bases undecidable. Intuitively, role inclusions simulate derivations context-free languages; thus, Boolean CQ check whether twocontext-free languages non-empty intersection, known undecidable problem (Hopcroft et al., 2003).655fiStefanoni, Motik, Krotzsch, & Rudolphregain decidability, next recapitulate definition so-called regular RBoxesHorrocks Sattler (2004). Let R ELRO+ RBox let smallesttransitive relation R that, 0 v R 6= , S.Then, RBox R regular irreflexive role inclusion v R form(t1) v S,(t2) v S,(t3) S1 Sn v Si 6= [1..n],(t4) S1 Sn v Si 6= [1..n],(t5) S1 Sn v Si 6= [1..n].induction define level lv(S) role R follows: lv(S) = 0R exists S; otherwise, lv(S) = 1 + max{lv(T ) | S}. Clearly,lv(S) computed time polynomial |R|. Section 4 show BCQ answeringELRO+ KBs regular RBoxes PSpace.2.4 Normalising ELRO+ Knowledge Basessimplicity, rest paper assume ELRO+ knowledge baseK = hT , R, Ai normalised, case following properties hold.(n1) IK 6= , K 6|= {a} v {b} {a, b} IK 6= b.(n2) axiom one following forms, A(i) basic concepts role.A1 u A2 v A3A1 v S.A2S.A1 v A2v S.SelfS.Self v(n3) axiom v R || 2 6= >r , role alsooccurs R.next show knowledge base K normalised polynomial time withoutaffecting regularity RBox component answers Boolean CQs.Proposition 3. ELRO+ knowledge base K regular RBox BooleanCQ q K, one compute polynomial time normalised ELRO+ knowledge baseK0 Boolean CQ q 0RBox K0 regular,q 0 K0 , K |= q K0 |= q 0 .Proof. Let K ELRO+ KB regular RBox let q Boolean CQ K.first satisfy property (n1). Let K1 obtained K extending ABoxK assertion >c (c) c fresh individual; clearly, K1 |= q K |= q.Next, let K2 q 0 obtained K1 q, respectively, uniformly substitutingindividual arbitrary, fixed, individual b K1 |= {a} v {b}. Entailment656fiThe Complexity Answering CQs GXQs OWL 2 EL KBsconcept inclusions decided polynomial time, K2 q 0 computedpolynomial time. Moreover, K2 q 0 obtained replacing individualarbitrary, fixed individual b aI = bI model K1 , q 0 K2 ,K2 |= q 0 K1 |= q.next satisfy property (n2). Let K3 result eliminating keys K2 .one see Table 3, keys derive axioms form {a} v {b}; moreover,effects conclusions already captured K2 , K3 |= q 0K2 |= q 0 . Next, eliminate polynomial time range restrictions occurringK applying syntactic transformation Baader et al. (2008); let K4 resultingknowledge base. Since definition ELRO+ knowledge base carefully restrictsinteractions role inclusions range restrictions, K4 |= q 0K3 |= q 0 (Baader et al., 2008). Next, following Krotzsch (2011), compute polynomialtime knowledge base K5 satisfies (n2) K5 |= q 0 K4 |= q 0 .next satisfy property (n3). Let K6 result exhaustively decomposingrole inclusion v form (t3)(t5) || > 2 occurring K5 accordingfollowing rewrite rules, occurrence role 0 fresh.(t3) S1 Sn v 7 {S 0 v S,1 Sn v 0 }0(t4)S1 Sn v 7 {S Sn v S, S1 Sn1 v 0 }(t5) S1 Sn v7{S 0 v S,1 Sn v 0 }linearly many rewrite steps required satisfy (n3), resulting RBoxregular. Furthermore, model K6 also model K5 model K5expanded model J K6 interpreting role 0 occurring K6 \ K5(S 0 )J = (0 )J , 0 unique role chain 0 v 0 occurs K6 . Thus,K6 |= q 0 K5 |= q 0 . Next, let K7 result removing axiomv >r K6 ; removed axioms tautologies, K7 |= q 0 K6 |= q 0 .Finally, let K0 result adding axiom r v S, role occurs K7occur RBox component. axioms K0 \ K7 preserve regularitytautologies, K0 |= q 0 K7 |= q 0 , required.3. Encoding Regular RBoxes Succinctly Using Bounded-Stack PDAsreasoning algorithm DL role inclusions known us uses step checkswhether L(S) holds arbitrary role chain role S. example, checkwhether K |= S(a, b) holds, algorithm must ensure that, model K, role chainL(S) exists connecting elements interpreting b. Although characteriselanguages L(S), role inclusions lend well language recognition,algorithms known us transform role inclusions another, manageable form.analogous fact that, regular expressions characterise regular languages,former routinely transformed FAs order facilitate language recognition.Horrocks Sattler (2004) showed that, regular RBox R roleoccurring R, one construct FA FS L(FS ) = L(S). FAs usedtableau decision procedure SROIQthe DL underpinning OWL 2 DL (Horrockset al., 2006). Given SROIQ knowledge base, tableau procedure tries construct657fiStefanoni, Motik, Krotzsch, & RudolphS2iS2startiS1S1fS2iS1fS1S0fS0fS1iS0S1iS0S0iS0fS0S0fS0iS0S0fS0Figure 1: FA FS2 constructed following Horrocks Sattler (2004)finite graph representing model KB, edges labelled roles,vertices labelled concepts. aforementioned FAs used ensure universalrestriction S.C obey constraints imposed role inclusions; roughly speaking,obtained running FS graph updating current state FS alongpath, labelling reachable vertex state FS final conceptC. Simanck (2012) optimised tableau procedure simulating FAs on-the-fly, ratherprecomputing advance.Horrocks Sattler (2004) observed FAs contain exponentially manystates. Kazakov (2008) proved unavoidable cases: regular RBoxRn containing axioms (1), size FA F L(F) = L(Sn ) exponential n.Si1 Si1 v Si[1..n](1)blowup number states caused simple model computation underlying FAs, behaviour automaton determined solely current state.example above, L(Sn ) whenever consists Si repeated j times[0..n] j = 2ni . Thus, parsing , FA recognising L(Sn ) mustremember number occurrences Si already seen, achievedusing different state number 0 2n . Figure 1 shows FA FS2constructed Horrocks Sattler (2004): remember current state, FS2 containstwo copies automaton FS1 , copy FS1 contains two copies automaton FS0 .Hence, obtain PSpace procedure, must devise succinct representationlanguages induced role inclusions. Towards goal, note role inclusionsclosely related context-free grammars, context-free languages efficientlyrecognised using pushdown automata (Hopcroft et al., 2003)that is, FAs extendedinfinite stack storing contextual information. Hence, given regular RBox Rrole occurring R, construct PDA PS accepts L(S). Unlike FA shownFigure 1 remembers contextual information using states, PDA PS uses stackremember current status computation determine proceed. shownumber states PS polynomial size R, PS recogniseL(S) using stack size linear size R; thus, PS provides us requiredsuccinct encoding FS . Section 4, use PDAs algorithm answersBoolean CQs ELRO+ knowledge bases using polynomial space.658fiThe Complexity Answering CQs GXQs OWL 2 EL KBsstartiS2S2 , X/XfS2, X/X, X/iS1 fS2 XiS1S1 , X/Xi>rfS1R, X/Xf>r, X/X, X/iS0 fS1 XiS0S0 , X/XfS0Figure 2: PDA PS2 corresponding FA FS2 , X R R Rrest section, fix arbitrary regular RBox R. Proposition 3,assume role inclusion v R || 2 6= >r .role occurring R , next define PDA PS .Definition 4. Let R role. Then, PS = hQR , R , R , R , , , fS , PDAQR = {iT , fT | R } set states, R = QR {} stack alphabet,R smallest transition function satisfying following conditions X R .(r) R \ {>r }, hfT , Xi R (iT , T, X).(t1) v R, hfT , Xi R (iT , , X).(t2) v R, hiT , Xi R (fT , , X).(t3) T1 v R, hiT1 , Xi R (iT , , X).(t4) T1 T2 v R, hiT1 , iT2 fT Xi R (iT , , X).(t5) T2 v R, hiT2 , fT Xi R (fT , , X).(ur) R , hf>r , Xi R (i>r , T, X).(u1) hf>r , Xi R (i>r , , X).(u2) hi>r , Xi R (f>r , , X).(p) R QR , hs, R (fT , , s).following examples, present PDA succinctly encodes FA FS2 ,explain different types transitions Definition 4, contentstack influences computation PDAs.Example 5. Figure 2 shows PDA PS2 corresponding FA FS2 Figure 1.c, X/transition hs0 , R (s, c, X) shown s0 , X/ indicates transition replaces top-most stack symbol X word ; moreover, transitions form(p) Definition 4 shown figure sake clarity. one seefigure, unlike FA FS2 , copying states PDA PS2 .659fiStefanoni, Motik, Krotzsch, & Rudolph, X/XstartS, X/X, X/i>r fS Xi>rfSR, X/Xf>r, X/X, X/fS X, X/iT XiPT, X/XfT, X/X, X/XP, X/XfPFigure 3: PDA PS RBox Example 6, X R R RExample 6. explain different types transitions Definition 4 stackused computation PDA, use regular RBox R containing role inclusions(2)(6). Figure 3 shows PDA PS using notation Example 5.vP(2)vT(3)P >r v(4)ST vS(5)P vT(6)role R associated states fT , moving formerlatter ensures PDA reads role chain L(T ). transition type (r) allowsPDA read state . -transition type (t1) fT addedreflexive, allows PDA read empty role chain; example, axiom(2) introduces -transition iP fP . Moreover, -transition type (t2)fT added transitive, allows PDA read number rolechains 1 , . . . , n L(T ); example, axiom (3) introduces -transition fT. Transitions types (ur), (u1), (u2) analogously reflect properties >r :(ur) allows PDA read arbitrary role, (u1) (u2) reflect reflexivitytransitivity >r , respectively. None transitions affect PDAs stack.illustrate transitions type (t4), next show how, 1 = P S, PDA PSdetermines 1 L(S); latter ensured axiom (4). assume PDA PSstate stack. Due axiom (4), PS make -transition type (t4)state iP , pushing i>r fS stack. Since new state iP , PDA next needread P ; furthermore, stack content signals PDA that, finishes readingP , move state i>r read >r state fS finish reading S. Indeed,PS make transition type (r) state fP read P , followed -transitiontype (p) state i>r popping i>r stack; next, PDA make transition660fiThe Complexity Answering CQs GXQs OWL 2 EL KBstype (ur) state f>r reading S, followed -transition type (p) state fS poppingfS stack. point, PDA accepts input.illustrate transitions types (t3) (t5), next show how, 2 = P , PDAPS determines 2 L(S); latter ensured axioms (5) (6). Again, assumePDA PS state stack. PDA PS make transition type(r) state fS , reading leaving stack unchanged; next, due axiom (5), PSmake -transition type (t5) state , pushing fS stack. Due axiom (6),PDA PS next make -transition type (t3) state iP , pushing stack;point, stack contains fS . Next, PDA make transition type(r) state fP reading P , -transition type (p) state poppingstack; furthermore, analogous way, PDA move state fT readingleaving fS stack. Finally, PDA make -transition type (p) statefS popping fS stack. point, PDA accepts input.understand benefit using PDAs rather FAs, note PS reaches state iPrecognising 1 2 . Role P occurs axioms (4) (6), PS movesstate iP order read occurrence P , must remember twoaxioms caused move knows continue reading P : 1 , PS mustcontinue reading >r , whereas 2 , must continue reading . Unlike FAs HorrocksSattler (2004) remember information copying states, PS remembersinformation stack: 1 , reaches iP i>r fS stack, whereas 2 ,PS reaches iP fS stack. Thus, stack PS analogous stacksprogramming languages: stack symbols correspond return addresses, transitionstype (p) correspond return statements.following proposition immediate definition PDA PS .Proposition 7. PDA PS computed time polynomial |R|.following theorem states PDA PS accepts L(S) PS stack boundedsize R. proof result given Section 3.1.Theorem 8. role R role chain ,1. L(PS ) L(S),2. PS stack bounded 2 lv(S) + 1.Theorem 8 gives rise following notion depth RBox R, provide usglobal bound stack size PDAs encoding R.Definition 9. depth RBox R defined dR := maxSR (2 lv(S) + 1).Finally, outline bounded-stack encoding regular RBoxes reducespace used tableau algorithm SROIQ. Since ELRO+ support inverseroles, Definition 4 directly provide us encoding languages inducedSROIQ RBoxes. Nevertheless, extend construction completingRBox R inv(Sn ) inv(S1 ) v inv(S) R role inclusion S1 Sn vRBox, inv() maps role inverse. One check that, (inverse) role661fiStefanoni, Motik, Krotzsch, & RudolphS, PDA PS constructed using completed RBox R encodes FS . Then, modifyportion tableau algorithm responsible checking satisfaction universalrestrictions running bounded-stack PDA graph constructed tableauprocedure. Roughly speaking, universal restriction S.C labelling vertex, runPS graph updating current state stack PS , labelreachable vertex current state stack PS final concept C. SincePS stack size polynomial |R|, requires polynomial space, unlikeFAs Horrocks Sattler (2004) optimised encoding Simanck (2012),may require exponential space.3.1 Proof Correctnesssection, prove Theorem 8. Towards goal, let ` derivation relationw.r.t. transition function R ; furthermore, derivation step hs, , ` hs0 , 0 , 0 i,write hs, , `x hs0 , 0 , 0 hs0 , 0 , 0 obtained hs, , applyingtransition form (x) Definition 4 x {r, t1, . . . , t5, ur, u1, u2, p}.3.1.1 Soundness Stack Boundednesssection, prove that, role R role chain ,1. L(PS ) implies L(S),2. PS stack bounded 2 lv(S) + 1.end, first show PDA PS satisfies following liveness property:computation PS pushes state QR stack, PS eventually popstack. Then, show derivation PS moving state state fS takesone five forms; call derivations regular. Finally, show regular derivationssatisfy properties (1) (2).start showing PDA PS satisfies following liveness property.Lemma 10. Let hs0 , 0 , 0 ` ` hsn , n , n arbitrary derivations0 = , sn = fS , 0 = role R word R . Then,role lv(T ) < lv(S) [0..n] si {iT , fT } = s0i i0s0i QR , index j [i..n] exists(a) sj = fT j = ;(b) k [i..j], word k form k := k00 k00 R ; and,(c) sj+1 = s0i , j+1 = i0 , j+1 = j .Proof. Let hs0 , 0 , 0 ` ` hsn , n , n above, [0..n 1], letxi {r , t1 , . . . , t5 , ur , u1 , u2 , p} form derivation step ithat is, fix xi (arbitrarily one possibility) hsi , , `xi hsi+1 , i+1 , i+1holds. Furthermore, role lv(T ) < lv(S), let set containingindex [0..n] si {iT , fT } form := s0i i0 s0i QR .Note that, index , due lv(T ) < lv(S), si {iT , fT }, sn = fS ,662fiThe Complexity Answering CQs GXQs OWL 2 EL KBs< nthat is, hsi , , ` hsi+1 , i+1 , i+1 occurs derivation. Next,induction N, show that, role = lv(T ) < lv(S) ,j [i..n] exists satisfying properties (a)(c).Base case (). Consider arbitrary role R 0 = lv(T ) < lv(S).consider interesting case 6= ; otherwise, properties (a)(c) hold vacuously.Since lv(T ) = 0 si {iT , fT }, xi {r , t1 , t2 , ur , u1 , u2 , p}. reverseinduction (i.e., induction starting maximal element), next showindex satisfies required properties.Base case. Let = max . xi {r , t1 , t2 , ur , u1 , u2 }, si+1 {iT , fT }i+1 = ; thus, + 1 , contradicts maximality i.remaining possibility xi = p, implies si = fT , si+1 = s0i , i+1 = i0 ,i+1 = ; then, j = satisfies properties (a)(c).Inductive step. Consider arbitrary index properties (a)(c) hold` ` > i. xi {r , t1 , t2 , ur , u1 , u2 }, si+1 {iT , fT } i+1 = ;hence, ii+1 so, inductive hypothesis, index j exists satisfying properties(a)(c). Otherwise, xi = p, si = fT , si+1 = s0i , i+1 = i0 , i+1 = , j =satisfies properties (a)(c).Inductive Step (). Consider arbitrary N properties (a)(c) holdrole P R lv(P ) lv(P ) < lv(S) index IP . Furthermore,consider arbitrary role + 1 = lv(T ) < lv(S). consider interestingcase 6= ; otherwise, properties (a)(c) hold vacuously. Recallv 0 R 0 6= >r , lv(>r ) = 0 6= >r . Thus,xi 6 {ur , u1 , u2 }. reverse-induction , next show index satisfiesrequired properties.Base case (). Let = max . xi {r , t1 , t2 }, si+1 {iT , fT } i+1 = ;thus, + 1 , contradicts maximality i. xi {t3 , t4 , t5 },si+1 {iP , fP } role P lv(P ) < lv(T ) lv(P ) < lv(S); furthermore,00 {i , f } 00i+1 form i+1 := i+1i+1 sequence00zero one states. state occurring i+1{iR , fR } roleR level less . then, inductive hypothesis (), index ` > existss` = sT ` = , contradicts maximality i. Finally, xi = p,si = fT , si+1 = s0i , i+1 = i0 , i+1 = , j = satisfies properties (a)(c).Inductive step (). Consider arbitrary index properties (a)(c) holdindex ` ` > i, consider possible forms xi .xi {r , t1 , t2 }. Then, si+1 {iT , fT } i+1 = , + 1 . inductivehypothesis (), index j exists satisfying properties (a)(c).xi = t3 . Then, si+1 = iT1 i+1 = role T1 lv(T1 ) < lv(T ).Thus, + 1 IT1 . inductive hypothesis (), index ` [i + 1..n] existss` = fT1 ` = i+1 ; furthermore, k [i + 1..`], kform k := k00 i+1 word k00 R ; finally, s`+1 = `+1 = .definition , ` + 1 . inductive hypothesis (),index j exists satisfying properties (a)(c).663fiStefanoni, Motik, Krotzsch, & Rudolphxi = t4 . Then, si+1 = iT1 i+1 = iT2 fT roles T1 T2lv(T1 ) < lv(T ) lv(T2 ) < lv(T ). Thus, + 1 IT1 . inductive hypothesis(), index `1 [i + 1..n] exists s`1 = fT1 `1 = i+1 ; furthermore,k [i..`1 ], k form k := k00 i+1 word k00 R ;finally, s`1 +1 = iT2 `1 +1 = fT . Then, `1 + 1 IT2 . Again, inductivehypothesis (), index `2 [`1 + 1..n] exists s`2 = fT2 `2 = `1 +1 ;furthermore, k [`1 + 1..`2 ], k form k := k00 `1 +1word k00 R ; finally, s`2 +1 = fT `2 +1 = . definition ,`2 + 1 . So, inductive hypothesis (), index j exists satisfyingproperties (a)(c).xi = t5 . Then, si+1 = iT2 i+1 = fT role T2 lv(T2 ) < lv(T ).Then, + 1 IT2 . inductive hypothesis (), index ` [i + 1..n] existss` = fT2 ` = i+1 ; k [i..`], k formk := k00 i+1 word k00 R ; finally, s`+1 = fT `+1 = .definition , ` + 1 . So, inductive hypothesis (),index j exists satisfying properties (a)(c).xi = p. Then, si = fT , si+1 = s0i , i+1 = i0 , i+1 = . Therefore, j = satisfiesproperties (a)(c).Next, role R , define notion regular derivations PS .Definition 11. set regular derivations P>r inductively defined follows,role R , role chain R , R .sequr hi>r , 0 , `ur hf>r , 0 , regular derivation P>r .sequ1 hi>r , 0 , `u1 hf>r , 0 , regular derivation P>r .sequ2 hi>r , 0 , ` ` hf>r , k , hi>r , k , ` ` hf>r , n , regular derivations P>r , following also regular derivation P>r .hi>r , 0 , ` ` hf>r , k , `u2 hi>r , k , ` ` hf>r , n ,Next, consider arbitrary natural number N assume regular derivationsPT already defined = >r role R \ {>r } lv(T ) m.Then, role R \ {>r } lv(S) = + 1, regular derivations PS definedfollows, S(i) R , R , R .seqr , 0 , `r hfS , 0 , regular derivation PS .seqt1 v R, , 0 , `t1 hfS , 0 , regular derivation PS .seqt2 v R , 0 , ` ` hfS , k , , k , ` ` hfS , n ,regular derivations PS , following also regular derivation PS ., 0 , ` ` hfS , k , `t2 , k , ` ` hfS , n ,664fiThe Complexity Answering CQs GXQs OWL 2 EL KBsseqt3 S1 v R, hiS1 , 0 , ` ` hfS1 , k , regular derivationPS1 , , k , ` ` hfS , n , regular derivation PS , followingalso regular derivation PS ., 0 , `t3 hiS1 , 0 , ` ` hfS1 , k , `p , k , ` ` hfS , n ,seqt4 S1 S2 v R, hiS1 , 0 , iS2 fS ` ` hfS1 , k , iS2 fS regular derivation PS1 , hiS2 , k , fS ` ` hfS2 , n , fS regular derivation PS2 ,following also regular derivation PS ., 0 ,hiS1 , 0 , iS2 fSfShiS2 , k ,hfS , n ,`t4` ` hfS1 , k , iS2 fS `pfS `p` ` hfS2 , n ,seqt5 S2 v R, , 0 , ` ` hfS , k , regular derivation PS ,hiS2 , k , fS ` ` hfS2 , n , fS regular derivation PS2 , following also regular derivation PS ., 0 , ` ` hfS , k , `t5 hiS2 , k , fS ` ` hfS2 , n , fS `p hfS , n ,left show derivation PS moves PDA start statefinal state fS regular regular derivations satisfy required properties.following lemma, show derivations leave particular wordbottom stack regular satisfy properties (1) (2). Subsequently,show accepting derivation PS form.Lemma 12. role R , word R , derivation formhs0 , 0 , 0 ` ` hsn , n , n s0 = , sn = fS , 0 = ,(i) derivation regular PS ;(ii) [0..n], |i | 2 lv(S);(iii)= 0 n .Proof. prove claim induction n N+ .Base case. n = 1, consider arbitrary role R , word R , sequence, 0 , 0 ` hfS , 1 , 1 i. Definition 4, transitions cases (r), (t1), (ur),(u1) move PS state state fS . transitions leave stack untouched,1 = = 0 property (ii) holds. properties (i) (iii), next considerfour different forms sequence may take., 1 , 0 `r hfS , 1 , 1 i. 6= >r , regular derivationPS case seqr (i) holds. Finally, 0 1 = S, implies =0 1 ,(iii) holds.665fiStefanoni, Motik, Krotzsch, & Rudolph, 0 , 0 `t1 hfS , 0 , 1 i. 6= >r , regular derivation PScase seqt1 (i) holds. Finally, 0 1 = ; moreover, case t1 Definition 4,v R, =; hence, =0 1 (iii) holds., 1 , 0 `ur hfS , 1 , 1 i. = >r R , regularderivation P>r case sequr (i) holds. Finally, 0 1 = R ,implies =0 1 , (iii) holds., 0 , 0 `u1 hfS , 0 , 1 i. = >r , regular derivation P>rcase sequ1 (i) holds. Finally, 0 1 = ; hence, =, (iii) holds.Inductive step. Consider arbitrary n N+ assume (i)(iii) holdrole 0 R , word 0 R , derivation hs00 , 00 , 00 0 ` ` hs0c , 0c , c0 0length n form required lemma. Furthermore, considerarbitrary role R , arbitrary word R , arbitrary derivationhs0 , 0 , 0 ` ` hsn+1 , n+1 , n+1(7)length n + 1 s0 = , 0 = , sn+1 = fS . [0..n 1], letxi {r , t1 , . . . , t5 , ur , u1 , u2 , p} form derivation step ithat is, fix xi (arbitrarily one possibility) hsi , , `xi hsi+1 , i+1 , i+1holds. next consider possible forms sequence might have, showproperties (i)(iii) hold case.(Case 1) = >r . consider form hs0 , 0 , 0 `x0 hs1 , 1 , 1 i. Sinces0 = i>r , x0 {t1 , t3 , t4 , ur , u1 }. R normalised, v 0 R0 6= >r , x0 {ur, u1} s1 = f>r 1 = = 0 . Since n > 1,hs1 , 1 , 1 `x1 hs2 , 1 , 2 occurs sequence x1 {t2 , t5 , u2 , p}. Sinces1 = f>r R normalised, x1 {u2 , p}; furthermore, since 1 =assumption form (7), x1 6= p. Hence, remaining possibilityx1 = u2 . case (u2) Definition 4, s2 = i>r , 2 = 1 , 2 = 1 .next prove properties (i)(iii) hold.(i) sequr sequ1 , hs0 , 0 , 0 `x0 hs1 , 1 , 1 regular derivation P>r .inductive hypothesis, hs2 , 2 , 2 ` ` hsn+1 , n+1 , n+1 also regularderivation PS . definition regular derivations, n = 2 = .then, (7) regular derivation PS case sequ2 .(ii) Words 0 , 1 , 2 empty. inductive hypothesis, |` | 2 lv(>r )` [2..n + 1]. Therefore, |i | 2 lv(>r ) holds [0..n + 1].(iii) inductive hypothesis, >r =2 n+1 . cases (ur) (u1), either0 2 = 0 2 = R . then, >r= 0 n+1 holds.(Case 2) 6= >r k [0..n] exists hsk , k , k `t2 hsk+1 , k+1 , k+1sk = fS . Then, case (t2) Definition 4, v R, sk+1 = , k+1 = k ,k+1 = k . next prove properties (i)(iii) hold.666fiThe Complexity Answering CQs GXQs OWL 2 EL KBs(i) inductive hypothesis, hs0 , 0 , 0 ` ` hsk , k , k regular derivationPS . definition regular derivations, k = 0 = . Since sk+1 =k+1 = k = , hsk+1 , k+1 , k+1 ` ` hsn+1 , n+1 , n+1form shown (7) shorter n + 1 so, inductive hypothesis,regular derivation PS . Then, (7) regular derivation PS case seqt2 .(ii) inductive hypothesis, |`1 | 2 lv(S) `1 [0..k], well|`2 | 2 lv(S) `2 [k + 1..n + 1]. Therefore, |i | 2 lv(S) holds[0..n + 1].(iii) inductive hypothesis, =0 k =k+1 n+1 . then,v R k+1 = k implies =0 n+1 holds.(Case 3) 6= >r ` [0..n] exists hs` , ` , ` `t2 hs`+1 , `+1 , `+1s` = fS , k [0..n] exists hsk , k , k `t5 hsk+1 , k+1 , k+1sk = fS . Then, let k largest indexthat is, assume > k existshsm , , `t5 hsm+1 , m+1 , m+1 sm = fS . Then, case (t5)Definition 4, role S2 level less S, S2 v R, sk+1 = iS2 ,k+1 = k , k+1 = fS k . next prove properties (i)(iii) hold.(i) Since sk = fS , inductive hypothesis hs0 , 0 , 0 ` ` hsk , k , kregular derivation PS . Definition 12, k = 0 . Since sk+1 = iS2k+1 = fS 0 , Lemma 10, index j [k + 1..n] exists sj = fS2j = k+1 ; furthermore, sj+1 = fS j+1 = 0 j+1 = j . provej + 1 = n + 1. sake contradiction, assume j + 1 < n + 1 considerform transition hsj+1 , j+1 , j+1 `xj +1 hsj+2 , j+2 , j+2 i. Givensj+1 = fS 6= >r , must xj+1 {t2 , t5 , p}. initial assumption,xj+1 6= t2 ; furthermore, maximality k, xj+1 6= t5 ; finally,since j+1 = 0 = , xj+1 6= p. Thus, j + 1 = n + 1, required. followssequence following form, k+1 = k n+1 = n ., 0 ,0 ` ` hfS , k , 0 `t5hiS2 , k+1 , k+1 ` ` hfS2 , n , n `phfS , n+1 , 0Lemma 10, ` [k + 1..n], ` form ` = `00 fS 0 .00particular, words k+1n00 empty. Then, inductive hypothesis,hiS2 , k+1 , k+1 ` ` hfS2 , n , n regular derivation PS2 .case seqt5 , (7) regular derivation PS .(ii) inductive hypothesis, `1 [0..k], |`1 | 2 lv(S). Furthermore, `2 [k + 1..n], |`002 | 2 lv(S2 ). Since lv(S2 ) < lv(S)`2 = `002 fS , also |`2 | 2 lv(S). Given n+1 = ,[0..n + 1], |i | 2 lv(S).(iii) inductive hypothesis, =0 k S2 =k+1 n . Given=S2 , k+1 = k , n+1 = n , obtain =0 n+1 .667fiStefanoni, Motik, Krotzsch, & Rudolph(Case 4) 6= >r ` [0..n] exists hs` , ` , ` `x` hs`+1 , `+1 , `+1 i,s` = fS x` {t2 , t5 }; hs0 , 0 , 0 `t3 hs1 , 1 , 1 i. Then, case (t3) Definition 4, role S1 level less S, S1 v R, s1 = iS1 ,1 = 0 , 1 = 0 . next prove properties (i)(iii) hold.(i) Since s1 = iS1 1 = 0 , Lemma 10, j [1..n] exists sj = fS1j = 1 ; furthermore, sj+1 = j+1 = 0 j+1 = j . Then, sequencefollowing form, 1 = 0 ., 0 ,0 `t3j `p1 ` ` hfS1 , j ,hiS1 , 1 ,, j+1 , j+1 ` ` hfS , n+1 , n+1Lemma 10, ` [1..j], ` form ` = `00 0 .particular, words 100 j00 empty. inductive hypothesis,hiS1 , 0 , 1 ` ` hfS1 , j , j regular derivation PS1 . Since j+1 = 0 ,inductive hypothesis, , j+1 , j+1 ` ` hfS , n+1 , n+1regular derivation PS . case seqt3 , (7) regular derivation PS .(ii) inductive hypothesis, `2 [j + 1..n + 1], |`2 | 2 lv(S);furthermore, `1 [1..j], |`001 | 2 lv(S1 ). Since lv(S1 ) < lv(S)`1 = `001 , also |`1 | 2 lv(S). Finally, since 0 = ,[0..n + 1], |i | 2 lv(S).(iii) inductive hypothesis, S1 =1 j =j+1 n+1 .Given =S1 S, 1 = 0 , j+1 = j , =0 n+1 .(Case 5) 6= >r ` [0..n] exists hs` , ` , ` `x` hs`+1 , `+1 , `+1 i,s` = fS , x` {t2 , t5 }; addition, hs0 , 0 , 0 `x0 hs1 , 1 , 1 x0 6= t3.next consider remaining possibilities x0 . s0 = , x0 6 {t2 , t5 , u2 , p}cases (t2), (t5), (u2), (p) Definition 4; furthermore, due 6= >r ,x0 6 {ur , u1 } cases (ur) (u1) Definition 4. Moreover, assume x0 {r , t1 };then, s1 = fS 1 = 0 cases (r) (t1) Definition 4; since n > 16= >r , possibility hs1 , 1 , 1 `p hs2 , 2 , 2 i, impossibledue 1 = assumption form (7). Hence, remaining possibilityx0 = t4 . case (t4) Definition 4, roles S1 S2 level lessS, S1 S2 v R, s1 = iS1 , 1 = 0 , 1 = iS2 fS 0 . next proveproperties (i)(iii) hold.(i) Since s1 = iS1 1 = iS2 fS 0 , Lemma 10, j1 [1..n] exists sj1 = fS1j1 = 1 ; furthermore, sj1 +1 = iS2 j1 +1 = fS 0 j1 +1 = j1 . Again,Lemma 10, j2 [j1 + 1..n] exists sj2 = fS2 j2 = j1 +1 ; furthermore,sj2 +1 = fS j2 +1 = 0 j2 +1 = j2 . Next, prove j2 + 1 = n + 1.sake contradiction, suppose j2 + 1 < n + 1 consider formhsj2 +1 , j2 +1 , j2 +1 `xj2 +1 hsj2 +2 , j2 +2 , j2 +2 i. Given sj2 +1 = fS ,must xj2 +1 {t2 , t5 , u2 , p}. However, assumed xj2 +1 6 {t2 , t5 }6= >r , xj2 +1 6= u2 ; finally, since j2 +1 = 0 = , xj2 +1 6= p.668fiThe Complexity Answering CQs GXQs OWL 2 EL KBsTherefore, j2 + 1 = n + 1, required, sequence followingform, 1 = 0 , j1 +1 = j , n+1 = n ., 0 ,0 `t4hiS1 , 1 ,1 ` ` hfS1 , j1 , j1 `phiS2 , j1 +1 , j1 +1 ` ` hfS2 , n , n `phfS , n+1 , n+1Lemma 10, `1 [1..j1 ], word `1 form `1 = `001 iS2 fS 0 .particular, words 100 j001 empty. Then, inductive hypothesis,hiS1 , 1 , 1 ` ` hfS1 , j1 , j1 regular derivation PS1 .Similarly, Lemma 10, `2 [j1 + 1..n], `2 form`2 = `002 fS 0 . particular, words j001 +1 n00 empty. Then,inductive hypothesis, hiS2 , j1 +1 , j1 +1 ` ` hfS2 , n , nregular derivation PS2 . case seqt4 , (7) regular derivation PS .(ii) inductive hypothesis, `1 [1..j1 ], |`001 | 2 lv(S1 ). Sincelv(S1 ) < lv(S) `1 = `001 iS2 fS , also |`1 | 2 lv(S). Similarly,inductive hypothesis, `2 [j1 + 1..n], |`002 | 2 lv(S2 ). Sincelv(S2 ) < lv(S) `2 = `002 fS , also |`2 | 2 lv(S). Since 0 = ,[0..n + 1], |i | 2 lv(S).(iii) inductive hypothesis, S1 =1 j1 S2 =j1 +1 n . Given=S1 S2 , 1 = 0 , n+1 = n , conclude =0 n+1 .possibilities form (7), claim lemma holdsderivation form.finally ready show PDA PS satisfies properties (1) (2).Lemma 13. role R role chain ,1. L(PS ) implies L(S),2. PS stack bounded 2 lv(S) + 1.Proof. definition PS , transitions resulting case p Definition 4ones popping elements stack, never pop symbol ; hence, pointaccepting derivation PS , stack content form := i0 . Then,two claims follow immediately Lemma 12.3.1.2 Completenessnext prove encoding also complete, thus proving Theorem 8.Lemma 14. role R role chain , L(S) impliesL(PS ).669fiStefanoni, Motik, Krotzsch, & RudolphAxiom Type(t1)vT(t2)(t3)(t4)(t5)Derivation`t1 hfT ,00hiT ,,vThiT ,hiT ,00 ,00 ,T1 vhiT , T1 00 ,hiT1 , T1 00 ,hiT ,00 ,T1 T2 vhiT , T1 T2 00 ,hiT1 , T1 T2 00 , iT2 fThiT2 ,T2 00 ,fT00hfT ,,T2 vhiT , T2 00 ,hiT2 ,T2 00 ,hfT ,00 ,`r`r`t3` r`r`t4` r` r`rfT ` r00 ,hfT , 00 ,hfT ,00 ,`t2hfT1 , 00 ,hfT ,00 ,`phfT1 , T2 00 , iT2 fT `phfT2 ,00 ,fT ` phfT , T2 00 ,hfT2 ,00 ,`t5fT ` pTable 4: Definition derivation (9) depending form axiom v .Proof. Consider arbitrary role R . following, role chain , write0== S; furthermore, N+ , write =role chains 1 , . . . ,exist =1 === . definition L(S),L(S) natural number N exists =. inductionN, next show =implies L(PS ).0Base case. Let = 0. Then, =S. consider two cases dependingform role R .= >r . case (ur) Definition 4, hi>r , >r , `ur hf>r , , i.R \ {>r }. case (r) Definition 4, , S, `r hfS , , i.either case L(PS ), required.Inductive step. Consider arbitrary N assume that, role chain 0=0 , 0 L(PS ); show holds + 1. Then,consider arbitrary role chains 1 , . . . , m+1 =1 ===m+1 .definition relation =, role R role chains 0 , , 00 exist role chainform := 0 00 , role chain m+1 form m+1 := 0 00 ,=. Since =0 00 , inductive hypothesis, 0 00 L(PS ),sequence hs0 , 0 , 0 ` ` hsn , n , n PS exists s0 = sn = fS ; furthermore,0 = n = ; finally, 0 = 0 00 n = . exists index[0..n 1] = 00 i+1 = 00 . Furthermore, j [0..i], role chain00j form j :=j role chain j R . Next, consider formxi hsi , , `xi hsi+1 , i+1 , i+1 i. Definition 4, transitions cases (r) (ur)read symbols input, xi {r, ur}. show lemma holds case.670fiThe Complexity Answering CQs GXQs OWL 2 EL KBs(Case 1) Consider case xi = r . Then, si = si+1 = fT ,= i+1 , R \ {>r }. Due =6= >r , v R. Then,following also derivation PS0000hs0 ,0 , 0 ` ` hsi , , `(8)(9)[The derivation Table 4 v ] `00hsi+1 , , i+1 ` ` hsn , , n(10)derivation (9) defined Table 4 depending form axiom v R.(Case 2) Consider case xi = ur . Then, si = i>r si+1 = f>r ,= i+1 R . Then, following also derivation PS0000hs0 ,0 , 0 ` ` hsi , , `00[The derivation seq(, , ) (14)] `00hsi+1 , , i+1 ` ` hsn , , nderivation seq(, 0 , ) (12) inductively defined follows.(hi>r , 00 , `u1 hf>r , 00 ,= ,00seq(, , ) :=000000hi>r , , `ur hf>r , , `u2 seq(, , ) = P .(11)(12)(13)(14)Therefore, either case, 0 00 L(PS ), required.4. Polynomial Space BCQ Answering Algorithm ELRO+ELRO+ knowledge base K translated set first-order Horn clauses,Boolean CQ q K answered evaluating q so-called canonical modelmodel homomorphically embedded model K. Canonicalmodels usually obtained using chase. Many different chase variants studiedliterature, producing different, homomorphically equivalent, canonicalmodel (Johnson & Klug, 1984; Marnette, 2009; Cal, Gottlob, & Kifer, 2013; Baget, Leclere,Mugnier, & Salvat, 2011). paper, introduce variant call consequencebased chase, (possibly infinite) set assertions IK produces K calluniversal interpretation K. compute IK , consequence-based chase initialises IKcontain ABox K, well assertions {a}(a), >c (a), >r (a, b) individualsb occurring K; then, iteratively extends IK using chase rules. slightly unusualaspect chase variant considers axioms entailed (and containedin) K. example, IK point contains assertion A(w) K |= v S.B holds,IK extended assertions S(w, w0 ) B(w0 ) w0 fresh term; term w0said auxiliary type S, B concept type B. BCQ answering algorithmpresent section based checking consequences K, chase variantmakes proofs simpler. Example 15 illustrates aspects.Example 15. Let K = hT , R, ELRO+ KB, contains axioms (15)(21),R contains role inclusion (22).{a} v S.A671(15)fiStefanoni, Motik, Krotzsch, & Rudolphb1 S,2 S,3 T, BRb6 S, B4 S, CoS,AoT,BoS,BRoS,CS,S,5T, BUniversal InterpretationCompact InterpretationFigure 4: universal interpretation compact interpretation Kv S.A(16){b} v S.A(17){b} v T.B(18){b} v S.C(19)C v T.B(20)C v R.{b}(21)ST vS(22)Figure 4 shows universal interpretation IK K. Assertions involving >c >rshown clarity. edges obtained via role inclusions dashed; remaining edgessolid, apart dotted edges, denote repetition solid edges. Black edgesobtained using conventional chase variants, whereas light grey subbranches IKcaused axioms entailed by, occurring in, K. Auxiliary terms labelledusing integers, terms type shown next term. Universal interpretation IKviewed family directed trees whose roots individuals Ksolid edges point parents children individuals K. Axiom (16) makes IKinfinite, decision procedure BCQ answering cannot simply materialise IKevaluate query it; instead, finitary representation IK needed.axioms (19), (20), (22), K |= {b} v S.B; then, since {b}(b) IK ,consequence-based chase ensures {S(b, 6), B(6)} IK holds well. contrast,commonly considered chase variants ensure {S(b, 6), B(6)} IK Kcontain axiom {b} v S.B.rest section, present first worst-case optimal algorithm decidesK |= q given arbitrary regular ELRO+ KB K Boolean CQ q K. Towardsgoal, Section 4.1 review existing approaches answering CQs DLs discuss672fiThe Complexity Answering CQs GXQs OWL 2 EL KBstechniques provide optimal procedure ELRO+ ; Section 4.2discuss intuitions behind algorithm; Section 4.3 introduce algorithmformally show runs polynomial space combined size K qpolynomial time size K; Section 4.4 prove algorithms correctness.4.1 Existing Approaches Answering CQsTechniques answering conjunctive queries DL knowledge bases developed thus farbroadly classified following three groups.first group consists automata-based approaches DLs Horn-SHIQHorn-SROIQ (Ortiz et al., 2011), SH (Eiter, Ortiz, & Simkus, 2012a), fragment ELRO+ obtained disallowing universal role, reflexive roles, self restrictions (Krotzsch et al., 2007). techniques, however, require constructing automatawhose size exponential size knowledge base.second group consists rewriting-based approaches. Roughly speaking, approaches rewrite query and/or TBox another formalism, usually unionCQs datalog program; relevant answers obtained evaluatingrewriting ABox. Rewriting-based approaches proposed membersDL-Lite family (Artale et al., 2009; Calvanese et al., 2007), DLs EL (Rosati,2007), ELHIO (Perez-Urbina et al., 2010; Mora, Rosati, & Corcho, 2014) HornSHIQ (Eiter, Ortiz, Simkus, Tran, & Xiao, 2012b), members datalog family (Virgilio, Orsi, Tanca, & Torlone, 2012), name few. rewriting approach,however, supports nominals role inclusions. Moreover, common shortcomingrewritings exponential query and/or TBox size, approachesmay also use exponential space.third group consists approaches based particular interpretation Kcall compact interpretation. Figure 4 shows interpretation KB KExample 15: finitely approximates universal interpretation using individualsform oS,B represent auxiliary terms type S, B. compact interpretation thusmaterialised space polynomial |K|, used answer instance queriestest atomic subsumptions K (Baader et al., 2005; Krotzsch, 2011). Materialisingcompact interpretation lies core many reasoning algorithms EL variants,natural try use interpretation answering CQs well. Since compactinterpretation model K, CQ maps universal interpretation mapscompact interpretation well; however, Example 16 shows, conversenecessarily hold.Example 16. Let K Example 15, let q1 , q2 , q3 following BCQs.q1 = x. R(x, b)q2 = x. S(a, x) S(b, x)q3 = x. (b, x) S(b, x)compact interpretation K shown Figure 4; one see, obtaineduniversal interpretation merging terms type S, B onto individual oS,B .query q1 mapped onto compact universal interpretation, queriesq2 q3 mapped onto compact interpretation. Thus, evaluating q2 q3compact interpretation produces unsound answers.673fiStefanoni, Motik, Krotzsch, & Rudolphb{a}b {b}AxBRR,D 1T,B3S,B 5RR,A24 T,BT,B6PT,B78 S,BP,B 9T,B 10Universal Interpretation11 S,Bz BSkeleton qFigure 5: universal interpretation K skeleton qremedy, combined approaches developed first evaluate querycompact interpretation filter results eliminate unsound answers.approaches developed members DL-Lite family (Kontchakov et al.,2011; Lutz, Seylan, Toman, & Wolter, 2013) EL family (Lutz, Toman, & Wolter,2009; Stefanoni et al., 2013) DLs, datalog family (Gottlob, Manna, & Pieris,2014) rule-based languages. particular, Stefanoni et al. (2013) developed filteringstep applicable DL ELHOdr, step breaks K contains role inclusions.Query q3 Example 16 mapped onto compact interpretation mappingatom S(b, x) dashed edge (i.e., edge obtained via role inclusions); moreover, q3tree-shaped, filtering step Stefanoni et al. (2013) identifymatch unsound. problem intuitively understood follows. unfoldingquery (22), query q3 essentially asks whether role chains 1 L(S) 2 L(T )exist label path solid edges IK starting b. compact interpretation,satisfied 1 = 2 = x mapped individual oT,B . IndividualoT,B , however, represents distinct terms 3 5 IK ; hence, although 3 connectedb via 1 5 connected b via 2 , role chains 1 2 satisfy query q3 .words, compact interpretation small represent relevant conditions.4.2 Intuitionsworst-case optimal procedure BCQ answering ELRO+ shown Algorithm 1page 681. essentially extends refines algorithm Krotzsch et al. (2007).explain underlying intuitions using knowledge base shown Example 17.Example 17. Let K ELRO+ knowledge base whose TBox contains axioms (23)(29)whose RBox contains role inclusions (30)(31).{a} v R.Av T.{b}674(23)(24)fiThe Complexity Answering CQs GXQs OWL 2 EL KBsS, X/XfS, X/fS XT, X/XfT, X/XFigure 6: transitions R corresponding axioms (30)(31)AvD(25){b} v T.B(26){b} v P.B(27)B v S.Self(28)B v T.B(29)vT(30)ST vS(31)Moreover, let q following Boolean CQ K.q = x, y, z. D(x) (x, z) S(y, z)(32)Figure 5 shows universal interpretation IK K; notation Example 15.solid looping edges auxiliary terms concept type B obtained axiom (28). Onesee K |= q holds; example, following substitution embeds q IK .= {x 7 2, 7 6, z 7 7}(33)algorithm uses PDA encoding RBox described Section 3. transitionfunction R axioms (30)(31) shown Figure 6; notation Example 6;note Figure 6 contained Figure 3.must prove existence substitution mapping q IK . substitutionmap binary atoms q dashed edges Figure 5. Dashed edges introduceshortcuts terms IK , dashed edge unfolded path consistingsolid edges using role inclusions K. solid paths IK twotypes: auxiliary paths involve auxiliary terms, whereas nominal paths require movingleast one individual. instance, edge (2, 7) unfolded path= connecting 2 b, b 6, 6 7. contrast, edge S(6, 7)unfolded path = connecting 6 itself, 6 7. algorithmuses PDAs transition function Figure 6 represent binary atomq sequence binary atoms mapped corresponding solid path IK .Interpretation IK , however, infinite, space possible substitutions also infinite.Hence, prove existence substitution mapping q IK , cannot simplyenumerate them, use Algorithm 1 instead.675fiStefanoni, Motik, Krotzsch, & Rudolphline 1 check whether K unsatisfiable; so, K |= q holds trivially. Next,line 2 guess substitution continue checking K |= (q); thus, step takesaccount variables could mapped individuals, two variables couldmapped term. example, guess identity mapping~y . step 3, guess finite structure, called skeleton (q), represents(possibly infinite) set substitutions mapping variables (q) distinct auxiliaryterms IK . Figure 5 shows skeleton query Example 17: skeleton verticesindividuals K variables (q), arranged forest whoseroots individuals; moreover, vertex v assigned atomic concept (v).step, skeleton represents substitution (if any) satisfying followingtwo properties:1. maps variable x term concept type (x),2. edge hv, v 0 S, (v 0 ) descendant (v) IK .next extend conditions prune set substitutions, goalleaving substitutions compatible (q)that is, embed (q) IK .establish compatibility unary atoms (q) line 4. particular, consideratom D(x) (q). property (1), substitution represented skeletonFigure 5 maps variable x term concept type (x) = A, implying A( (x)) IKholds. then, since K |= (x) v holds, know D( (x)) IK holds well; thus,atom D(x) satisfied substitution represented S.contrast, cannot establish compatibility binary query atoms using entailmentchecking only, vertex labels relative position vertices sufficientlydescribe substitutions. example, substitution1 = {x 7 2, 7 9, z 7 10}(34)satisfies properties (1) (2), (1 (x), 1 (z)) 6 IK .prune substitutions, lines 516 Algorithm 1 guess binary atom(q) unfold sequence solid steps IK . solid paths IK gonominals auxiliary terms only, two possibilities accountedguessing line 8. Moreover, skeleton already constrains relative positionsquery terms, represent unfolding binary atom labelling edge hv, v 0set L(v, v 0 ) bounded-stack PDAs transition function Figure 6substitution represented also satisfies following property:3. PDA P L(v, v 0 ), nonempty role chain L(P) exists labelling pathIK solid edges (v) (v 0 ).next illustrate label edges substitutions satisfying properties(1)(3) compatible binary atom (q).(x, z), must ensure that, substitution represented S, role chainL(T ) exists connecting (x) (z) using solid edges. Since relative positions(x) (z) IK determined shown Figure 5, path must connect(x) b, connect b (y), finally connect (y) (z). addition,assume individuals occur paths b (y), (y) (z):676fiThe Complexity Answering CQs GXQs OWL 2 EL KBs>rR{a}RBS,T, P{b}startBstartFigure 7: left-hand side, walking finite automaton wfa(A, {b}) (transitions involving >c >r shown clarity); right-hand side, stationary finite automaton sfa(B)path (x) (z) involves individuals b visits b once,absorb path segments subpath (x) b. Thus, checkexistence setting v0 = au = b lines 78 (no guessing possible line 8case) splitting lines 711 path three subpaths. particular,line 10 guess states s0 , s1 , s2 = fT line 11 guess stack words 0 , 1 ,2 = following properties:(i) subpaths (x) b described PDA PT0 whose start state stack, respectively, final state stack s0 0 , respectively;(ii) subpaths b (y) described PDA PT1 whose start state stacks0 0 , respectively, final state stack s1 1 , respectively;(iii) subpaths (y) (z) described PDA PT2 whose start state stacks1 1 , respectively, final state stack s2 2 , respectively.know terms IK variables z mapped to, cannotcheck existence paths (ii) (iii) independently. Therefore, add line 12PDAs PT1 PT2 constraints edges hb, yi hy, zi S, respectively. edgesthus accumulate constraints moves auxiliary terms must satisfy; latershall explain lines 1718 check constraints and, check passes,know map z auxiliary terms whose concept types (y)(z), respectively. contrast, path (i) finishes individual,check existence path independently constraint. end,construct walking finite automaton wfa(A, {b}) shown left-hand side Figure7. wfa(A, {b}) describes moves IK terms concept type (x) =individual bthat is, L(wfa(A, {b})) term w concept typerole chain connecting w b IK via solid edges; then, line 14 checkwhether intersection languages wfa(A, {b}) PT0 empty. wfa(A, {b})FA PT0 PDA, test emptiness intersection languagespolynomial time (Hopcroft et al., 2003). example, guess s0 = s1 = s2 = fT .677fiStefanoni, Motik, Krotzsch, & RudolphThus, PT1 accepts language , b connected 1 (y) solid edgelabelled P , adding PT1 constraint edge hb, yi ensures substitution 1 (34)satisfy property (3).S(y, z), must ensure that, substitution represented S, role chainL(S) exists connecting (y) (z) using solid edges. even though zdescendant S, line 8 could guess v0 = au = b, connects (y) bthen, without going individuals, connects b (z) via (y). restparagraph, however, consider case connects (y) (z) directly,since possibility example, one see Figure 5. Therefore,line 8, guess v0 = = y. then, path (y) (z) could first loop (y)due self-restrictions; must actually move (y) (z); finally couldloop (z). reasons discuss following paragraph, absorb latter loopconstraint added edge hy, zi; however, check existence former loopindependently. Therefore, lines 711 split two subpaths. particular,line 10 guess states s0 s1 = fS , line 11 guess stack words 0 1 =following properties:(i) looping (y) described PDA PS0 whose start state stack, respectively, final state stack s0 0 , respectively;(ii) subpaths (y) (z) start move (y), possibly involvelooping (z), described PDA PS1 whose start state stack s0 0 ,respectively, final state stack s1 1 , respectively.previous case, check (ii) adding PS1 constraint edge hy, zi S.Furthermore, check existence path (i) constructing stationary finiteautomaton sfa(B) shown right-hand side Figure 7. sfa(B) describespossible loops terms concept type (y) = B; is, L(sfa(B))term w concept type B role chain corresponding (possiblyempty) loop w; then, line 16 check whether intersection languagessfa(B) PS0 empty. example, guess s0 = s1 = fS ; thus PS0 acceptslanguage , whereas PS1 accepts language .line 17, skeleton represents substitutions compatibleatoms (q), must still show least one substitution realiseduniversal interpretation IK . end, apply Algorithm 2 page 681edge hv, v 0 skeleton, thus check whether terms (v) (v 0 ) IK existsatisfy properties (1)(3) PDAs L(v, v 0 ). Roughly speaking, solveproblem running PDAs parallel lines 715 Algorithm 2. However, cannotmaterialise IK , exploit property consequence-based chase procedure: termw concept type connected term w0 concept type B IK using solidedge labelled K |= v S.B. Furthermore, concept type w fullydescribes solid paths descendants w, need keep track actualposition IK ; instead, use variable concept keep track current terms concepttype. Thus, line 9 check existence edges IK via entailment checking; that,PDA, line 11 guess state stack PDA, line 12 checkwhether PDA perform move, line 14 actually move PDA. Due678fiThe Complexity Answering CQs GXQs OWL 2 EL KBsself-restrictions reflexive roles, however, PDAs need move synchrony:move solid edge, PDAs independently loop current term.end, line 11 guess state s0 stack 0 PDA moveslooping, line 13 check whether PDA move state stackstate s0 stack 0 using role chain compatible concept type termPDA moving into, given stationary finite automaton sfa(D).PDAs required loop, FA sfa(D) accepts empty word. Algorithm 2 thus checksloops move, line 16 Algorithm 1 necessary. Lines 25take account PDAs nondeterministic initially makeseveral -transitions; note explicit check -transitions required initiallysince line 13 allows possible -transitions move along solid edge. Finally,ensure termination Algorithm 2 observing that, since stack PDA L(v, v 0 )bounded, number current configurations PDA exponential,number distinct tuples current PDA configurations exponential well;hence, algorithm repeats computations exponentially many steps.thus obtain nondeterministic decision procedure running polynomial space usingbinary counter stop computation distinct configurations explored.constraints added previous paragraphs, one check Algorithm 2returns true edges S; hence, K |= (q) holds, thus K |= q holds well.4.3 Formalisationformalise intuitions previous section. Towards goal, fixnormalised ELRO+ KB K = hT , R, Ai regular RBox R, let QR , R , Rspecified Definition 4, let dR depth R specified Section 3.start formalising notion skeleton Boolean CQ.Definition 18. skeleton Boolean CQ q = ~y . (~y ) triple = hV, E,following components.1. V = IK ~y set vertices.2. E V ~y set edges directed graph hV, Ei forest whoseroots precisely elements IK .3. : ~y 7 {>c } CK function maps existential variables q atomicconcepts. convenience, extended V (v) := {v} v IK .path nonempty sequence (distinct) vertices v0 , . . . , vn n 0 and,[0..n 1], hvi , vi+1 E.Please observe that, K normal form, exists least one individual occurringK thus V 6= . next generalise notion PDA encoding RBox RDefinition 4 allowing arbitrary start final states well arbitrary startfinal stacks size dR . generalised PDA used algorithmimplement splitting operation mentioned Section 4.2.Definition 19. states s, s0 QR words , 0 R || dR | 0 | dR ,generalised PDA R given pda(s, , s0 , 0 ) := hQR , R , R , R , s, , s0 , 0 i.679fiStefanoni, Motik, Krotzsch, & Rudolphfollowing definition introduces automata one use succinctly representaxioms logically follow K.Definition 20. Let B basic concepts. walking finite automatonB given wfa(A, B) := hQ, R , w , A, Bi Q w follows.Q = {>c } CK NK .w transition function containing w (C, S) role Rstates C Q K |= C v S.D.stationary finite automaton given sfa(A) := h{A}, R , , A, Aicontains (A, S) role R K |= v S.Self K |= v S.Boolean CQs answered nondeterministic procedure entails shown Algorithm 1, uses auxiliary procedure exist shown Algorithm 2. followingtheorem states entails(K, q) decides K |= q, proof given Section 4.4.Theorem 21. Let q Boolean CQ K. Then, K |= q nondeterministiccomputation exists entails(K, q) returns true.Finally, determine complexity algorithm entails, towards goalfirst determine complexity auxiliary function exist.Lemma 22. Function exist(A, B, {Pj = pda(sj , j , s0j , j0 ) | 0 j m}) implemented uses space polynomial |K| and, RBox R fixed, runstime polynomial |T | + |A|.Proof. Consider arbitrary A, B, Pj = pda(sj , j , s0j , j0 ) stated above; letAlgorithm 2; let ` derivation relation corresponding R . definitiongeneralised PDAs, |j | dR |j0 | dR j [1..m].Proposition 2, using polynomial time one compute PDAs accepting languagesLdR (pda(sj , j , s, )) LdR (pda(s, , s0 , 0 )) lines 4 13; therefore, checks lines4 13 implemented use time (and therefore space) polynomial|K| (Hopcroft et al., 2003, ch. 7).space usage Algorithm 2, please observe function stores followinginformation computation step:(a) array state length state[j] QR j [1..m], array stacklength stack[j] R |stack[j]| dR j [1..m],(b) generalised PDA line 4,(c) generalised PDA stationary automaton line 14,(d) concept concept {A, >c } CK line 1,(e) binary counter k 1 k ,(f) depth dR R, atomic concept {>c } CK , role R .680fiThe Complexity Answering CQs GXQs OWL 2 EL KBsAlgorithm 1: entails(K, q)12345678910111213141516171819K inconsistent return trueguess substitution dom() = ~y rng() ~y IKguess skeleton = hV, E, (q)atom A(t) (q) exists K 6|= (t) v return falseforeach hv, v 0 E let L(v, v 0 ) :=foreach binary atom S(t, u) (q)let au unique individual u reachable auguess v0 {t, au } u reachable v0let v0 , . . . , vn unique path vn = uguess states s0 , . . . , sn QR sn = fSguess words 0 , . . . .n R n = |i | dR [0..n]foreach [1..n] let L(vi1 , vi ) := L(vi1 , vi ) {pda(si1 , i1 , si , )}v0L(wfa((t), (v0 ))) L(pda(iS , , s0 , 0 )) = return falseelseL(sfa((v0 ))) L(pda(iS , , s0 , 0 )) = return falseforeach hv, v 0 Eexist((v), (v 0 ), L(v, v 0 )) return falsereturn trueAlgorithm 2: exist(A, B, {Pj = pda(sj , j , s0j , j0 ) | 0 j m})12345678910111213141516171819let concept := let := (1 + |CK |) |QR |m (|R |1+dR )mj = 1guess state QR word R || dR6 LdR (pda(sj , j , s, )) return falseset state[j] := stack[j] :=guess k N 1 kr = 1 kguess R {>c } CKK 6|= concept v S.D, K |= v {a} IK return falsej = 1guess {s, s0 } QR {, 0 } R || dR | 0 | dRhstate[j], S, stack[j]i 6` hs, , return falseL(sfa(D)) LdR (pda(s, , s0 , 0 )) = return falseset state[j] := s0 stack[j] := 0set concept :=concept 6= B return falseexists index j [1..m] state[j] 6= s0j stack[j] 6= j0return falsereturn true681fiStefanoni, Motik, Krotzsch, & Rudolphdefinition dR , dR linearly bounded number axioms occurring R; hence, need O(m |R|) space store two arrays. Furthermore,need O(m |K|) space store counter k using binary encoding. Definition20, size sfa(D) polynomial |K|; Definition 4, size pda(s, , s0 , 0 ) polynomial |R|. Overall, space needed store required information polynomial|K|. Finally, following Krotzsch (2011), realise check step 9 polynomialtime. Thus, exist implemented uses space polynomial |K|.Next, assume RBox R fixed. dR , QR , R , R fixedwell; moreover, bounded size R fixed, linearsize A. Thus, number alternatives nondeterministic step line 3Algorithm 2 fixed, lines 15 require time polynomial |T | + |A|. Furthermore,instead guessing k using nondeterministic step 6, repeat lines 715k [1..M ], requires linear number iterations. show lines 715 alsoimplemented run polynomial time, first define three sets usedperform checks lines 9, 12, 13.{hS, C, Di R (CK NK {>c })2 | K |= C v S.D IK : K 6|= v {a}} (35){hS, pda(s, , s0 , 0 )i | R hs, S, ` hs0 , , 0 i} (36){hC, pda(s, , s0 , 0 )i | C CK {>c } L(sfa(C)) LdR (pda(s, , s0 , 0 )) 6= } (37)Given R fixed, sets computed time polynomial sizeA. next show implement for-loop steps 715 use space logarithmicsize , A, sets equations (35)(37). space usage lines 715,computation step for-loop store information points (a)(f) above.Since R fixed, however, points (a)(c) require constant space. Furthermore,checks lines 9, 12, 13 performed lookup sets (35)(37); storingsets using suitable binary encoding using binary index sets,check implemented using logarithmic space. Finally, CK linearsize A, store counter k, concepts concept, role using binaryencoding, overall space function needs store logarithmic |T | + |A|,size sets (35)(37). Thus, steps 715 require nondeterministic logarithmic space,well known implies steps 715 implemented run polynomialtime. Finally, steps 1619 clearly require polynomial time. Consequently, function existimplemented runs time polynomial |T | + |A| fixed R.ready establish complexity function entails(K, q); Section 5shall show function worst-case optimal combined data complexities.Theorem 23. q BCQ K, function entails(K, q) implemented1. uses space polynomial input size,2. RBox R fixed, runs nondeterministic polynomial time sizeTBox , ABox A, query q,3. RBox R query q fixed, runs (deterministic) polynomial timesize TBox ABox A.682fiThe Complexity Answering CQs GXQs OWL 2 EL KBsProof. Let q = ~y .(~y ) Boolean CQ K.shown Proposition 2, one compute lines 14 16 PDA accepting languageLdR (pda(iS , , s0 , 0 )) polynomial time, checks lines 14 16 require time(and therefore space) polynomial |K| (Hopcroft et al., 2003, ch. 7). Moreover, checkslines 1 4 also require time polynomial |K| (Krotzsch, 2011).(1), please observe function entails specified Algorithm 1 storesfollowing information computation step:substitution dom() = ~y rng() ~y IK ;skeleton = hV, E, (q);path v0 , . . . , vn S, sequence states s0 , . . . , sn QR , sequence words0 , . . . , n R |i | dR [0..n];function L mapping edge hv, v 0 E set generalised PDA; and,walking automaton wfa((t), (v0 )) stationary automaton sfa((v0 )).definition skeleton (q), need space polynomial size q Kstore S. Moreover, length longest path given number variablesoccurring (q), store sequences vertices, states, words spacepolynomial |q| |K| well. Also, set L(v, v 0 ) contains PDAs,number binary atoms occurring (q). Then, Lemma 22, entailsimplemented uses space polynomial input size.(2), assume RBox R fixed. Lemma 22, fixed RBox R, step 18implemented runs time polynomial |T | + |A|. Clearly, stepsAlgorithm 1 implemented run nondeterministic polynomial time sizeTBox , ABox A, query q. Consequently, fixed RBox R, function entailsimplemented runs nondeterministic polynomial time size TBox, ABox A, query q.(3), assume RBox R query q fixed. dR , QR , R , Rfixed well. Given number variables occurring q fixed, numberguessing steps required steps 2 3 fixed; also, number alternativessteps linear |T | + |A|. Thus, steps 2 3 require polynomial time. Furthermore,maximum number iterations for-loop steps 616 fixed lengthlongest path fixed. Thus, number guessing steps lines 11 12 alsofixed. addition, number alternatives guessing steps lines 8, 11, 12fixed well. Therefore, steps 616 require time polynomial |T | + |A|. Finally, sincequery fixed, maximum number iterations for-loop steps 17 18also fixed so, Lemma 22, steps 17 18 require time polynomial A.Therefore, entails implemented runs time polynomial |T | + |A|fixed R q.683fiStefanoni, Motik, Krotzsch, & Rudolph4.4 Proof Theorem 21prove function entails(K, q) indeed decides K |= q. Towards goal,start proving correctness function exist, introduce universalinterpretation K, and, finally, show entails sound complete.4.4.1 Correctness existfollowing proposition proves correctness function exist Algorithm 2.Lemma 24. Function exist(A, B, {Pj = pda(sj , j , s0j , j0 ) | 0 j m}) returns trueexist natural number k 1, roles S1 , . . . , Sk , basic concepts A0 , . . . , AkA0 = Ak = B, role chains {j,i | j [1..m] [1..k]}following conditions hold [1..k] j [1..m].1. IK , K |= Ai1 v Si .Ai K 6|= Ai v {a}.2. role occurring j,i , K |= Ai v T.Self K |= v .3. S1 j,1 Sk j,k LdR (Pj ).Proof. Consider arbitrary A, B, Pj = pda(sj , j , s0j , j0 ) stated lemma. Moreover, let ` derivation relation corresponding R .() Assume nondeterministic computation exist functionreturns true. Let k N guessed step 6; show for-loop steps 715satisfies following invariant: iteration r, exist roles S1 , . . . , Sr , basicconcepts A0 , . . . , Ar , and, j [1..m], role chains j,1 , . . . , j,r A0 = A,Ar = concept, following holds [1..r] j [1..m].(i) K |= Ai1 v Si .Ai and, IK , K 6|= Ai v {a}.(ii) role occurring j,i , K |= Ai v T.Self K |= v .(iii) S1 j,1 Sr j,r LdR (pda(sj , j , state[j], stack[j])).Base case. first iteration loop (i.e., steps 15 r = 0),concept = A, LdR (pda(sj , j , state[j], stack[j]) j [1..m],properties (i)(iii) clearly hold.Inductive step. Consider arbitrary iteration r [1..k 1] assume properties(i)(iii) hold end iteration r; show true iteration r + 1.inductive hypothesis, exist roles S1 , . . . , Sr , basic concepts A0 , . . . , Ar , and,j [1..m], role chains j,1 , . . . , j,r A0 = A, Ar = concept properties(i)(iii) hold. Let role Sr+1 = atomic concept Ar+1 = guessed step 8.Clearly, K |= concept v Sr+1 .Ar+1 K 6|= Ar+1 v {a} IK ,required property (i). Furthermore, consider arbitrary j [1..m], let s, s0 , ,0 guessed step 11, let state[j] stack[j] end iteration r;then, hstate[j], Sr+1 , stack[j]i ` hs, , due step 12; furthermore, due step 13, rolechain j,r+1 exists j,r+1 L(sfa(D)) LdR (pda(s, , s0 , 0 )). Definition 20stationary automata, role occurring j,r+1 K |= v T.Self684fiThe Complexity Answering CQs GXQs OWL 2 EL KBsK |= v , required property (ii). Finally, let state[j] stack[j] specifiedstep 14; S1 j,1 Sr j,r Sr+1 j,r+1 LdR (pda(sj , j , state[j], stack[j])),property (iii) holds.Step 16 ensures concept = B; furthermore, steps 1718 ensure state[j] = s0jstack[j] = j0 j [1..m], PDA Pj accepts S1 j,1 Sk j,k . Thus, properties(1)(3) lemma hold, required.() Let S1 , . . . , Sn roles, let A0 , . . . , basic concepts A0 = = B,let j,i role chains satisfying properties (1)(3) lemma. derivationS1 j,1 Sn j,n PDA Pj following form, sj,n+1 = s0j j,n+1 = j0 :hsj , S1 j,1 Sn j,n , j `hs0j,1 ,hsj,1 , S1 j,1 Sn j,n , j,1 `j,1 S2 Sn j,n ,hsj,2 , S2 j,2 Sn j,n , j,2 `0j,1(39)(40)(41). . . `(42)`... `hs0j,i ,hsj,i , Si j,i Sn j,n , j,i `j,i Si+1 Sn j,n ,hsj,i+1 , Si+1 j,i Sn j,n , j,i+1 `hs0j,n ,hsj,n , Sn j,n , j,n `hsj,n+1 , , j,n+1j,n ,(38)0j,i0j,n``(43)(44)Transition (38) (39) special sense allows Pj make arbitrarynumber -transitions; rest derivation regular consists reading Sij,i . Thus, sj,i s0j,i states Pj after, respectively, reading Si ,0 respective stacks. property (3) lemma, | |j,i j,ij,iR0|j,i | dR .Let Xi = hAi , s1,i , 1,i , . . . , sm,i , m,i i. PDA, |QR | many differentP R|R |` many difstates, 1+|CK | different elements {>c }CK ; furthermore, d`=0PdRferent stacks length dR . |R | > 0 dR > 0, `=0 |R |` |R |1+dR ;consequently, distinct tuples. Thus, k ,Xk = Xn+1 . then, Ak = B; furthermore, j [1..m], sj,k = sj,n+1 = s0jj,k = j,n+1 = j0 , S1 j,1 Sk j,k LdR (Pj ).easily construct nondeterministic computation exist follows.step 3, j let = sj,1 = j,1 ; clearly, condition step 4 satisfied.r for-loop lines 715, proceed follows.step 8 let = Si = Ai , respectively; clearly, condition step 9satisfied due property (1).0 , 0 =j [1..m], let = s0j,r , s0 = sj,r+1 , = j,rj,r+1 ; clearly,condition step 12 satisfied due form derivation; furthermore,condition step 13 satisfied due property (2) Definition 20.Finally, conditions steps 16 17 satisfied due way chose k.Therefore, function exist returns true step 19.685fiStefanoni, Motik, Krotzsch, & RudolphRule(cr1)(cr2)(cr3)(cr4)(cr5)(cr6)(cr7)PreconditionK |= A1 u A2 v B{A1 (w), A2 (w)}K |= v S.BK |= B v {a} IKA(w)K |= v S.BK 6|= B v {a} IKA(w)K |= S.A v B{S(w, w0 ), A(w0 )}K |= S.Self v BS(w, w)role simpleK |= v S.SelfA(w)role simpleL(S)(w, w0 )ConclusionB(w)S(w, a), B(a)S(w, fS,B (w)), B(fS,B (w))>c (fS,B (w)), >r (fS,B (w), fS,B (w))>r (fS,B (w), w0 ) term w0 occurring>r (w0 , fS,B (w)) term w0 occurringB(w)B(w)S(w, w)S(w, w0 )Table 5: Rules consequence-based chase4.4.2 Consequence-Based Chase Universal Interpretationsprove entails(K, q) sound complete, interpret K using forest-shapeduniversal interpretation described Section 4.2. Towards goal, next defineauxiliary notions, define universal interpretation, and, finally, prove two properties interpretation.universe K set terms built individuals occurring Kunary function symbols form fS,A R CK . Since K normalised,universe K nonempty. fact ground atom constructed using predicatesoccurring K terms universe K. role chain = S1 Sn , termsw w0 , set facts I, write (w, w0 ) (not necessarily distinct) termsw = w0 , . . . , wn = w0 exists Si (wi1 , wi ) [1..n]. set factsentails Boolean CQ q = ~y . (~y ), written |= q, substitution existsdom( ) = ~y (q) I. universal interpretation IK K defined follows.Definition 25. chase rule Table 5 applicable set facts preconditionsrule satisfied, contain conclusions rule. consequencebased chase (often chase) K sequence sets facts I0 , I1 , . . .I0 = {{a}(a), >c (a), >r (a, b) | {a, b} IK }(45)and, 1, set Ii+1 obtained extending Ii conclusion one (arbitrarilychosen) chase rule applicable Ii , Ii+1 = Ii chase rule applicable Ii .686fiThe Complexity Answering CQs GXQs OWL 2 EL KBssequence must fairthat is, derivation rule applicable Ii specificprecondition, j exists suchSthat Ij+1 obtained Ij applying rulementioned precondition. Set IK = Ii universal interpretation K.Since K normal form, K 6|= {a} v {b} distinct individualsb IK ; hence, one individual IK exists rule (cr2) K |= B v {a}.that, straightforward see IK independent orderchase rules applied, call IK universal interpretation K. Moreover,due fairness, derivation rule applicable IK is, chase ruleTable 5 either preconditions rule satisfied IK , IK containsconclusions rule. Finally, well-known that, K consistent, IKhomomorphically embedded model K (Krotzsch et al., 2007). Consequently,universal interpretation IK used answer arbitrary Boolean CQs K.Fact 26. Boolean CQ q, K |= q K |= >c v c IK |= q.Next, show IK relates axioms entailed K. end, letfollowing function mapping term w universe K basic concept:({w} w(w) :=w form w = fS,A (w0 )Proposition 27. universal model IK satisfies following properties.1. A(w) IK , K |= (w) v A.2. S(w, w0 ) IK , nonempty role chain = 0 S1 1 m1 SmL(S) terms w0 , . . . , wm universe K w0 = w wm = w0exist(a) [1..m], either wi IK , atomic concept Ai {>c } CK existswi = fSi ,Ai (wi1 ) K 6|= Ai v {a} individual IK ,(b) [1..m], K |= (wi1 ) v Si .(wi ),(c) [0..m] role occurring , K |= (wi ) v T.SelfK |= v .Proof. Let I0 , I1 , . . . chase sequence K. show induction rule applications properties (1) (2) satisfied A(w) S(w, w0 ) ,respectively, additionally satisfies following property:3. term w occurring , K |= x.(w)(x).definition I0 (cr3), terms w w0 occurring ,clearly {>c (w), >r (w, w0 ), >r (w0 , w)} .Base case. Consider I0 , note term w occurring I0 individual(w) = {w}. Consider A(a) I0 ; either A(a) A, = {a}, = >c ,K |= {a} v A, property (1) holds. Furthermore, consider S(a, b) I0 ;687fiStefanoni, Motik, Krotzsch, & RudolphS(a, b) = >r , K |= {a} v S.{b}, property (2) holds w0 = a,w1 = b, = S. Finally, property (3) holds K |= x.{a}(x) IK .Inductive step. Assume satisfies properties (1)(3). consideringderivation rule, assume rule applicable shown Table 5,show properties (1)(3) hold conclusions rule. Note rule (cr3)affect property (3), explicitly consider properties hold vacuously.(cr1) inductive hypothesis, K |= (w) v A1 K |= (w) v A2 ,implies K |= (w) v B, required property (1).(cr2) inductive hypothesis, K |= (w) v K |= x.(w)(x),clearly imply K |= (w) v S.B K |= x.B(x). Moreover, (a) = {a}, K |= (a) v B,property (1) holds. Finally, since K |= (w) v S.(a), property (2) holds w0 = w,w1 = a, = S.(cr3) Let w00 = fS,B (w). inductive hypothesis, K |= (w) vK |= x.(w)(x), K |= (w) v S.B K |= x.B(x). Moreover, (w00 ) = B,K |= (w00 ) v B, K |= (w00 ) v >c , K |= x.(w00 )(x), required properties (1)(3), respectively. property (2), consider role assertions derived rule.S(w, w00 ). Note K |= (w) v S.(w00 ) K 6|= B v {a} IK ,property (2) holds w0 = w, w1 = w00 , = S.>r (w00 , w00 ). Clearly, property (2) holds w0 = w00 = 0 = >r .>r (w00 , w0 ) term w0 occurring . Let individual w0 rootedin; then, >r (a, w0 ) so, inductive hypothesis, role chain L(>r )terms = w0 , . . . , wm = w0 exist satisfying properties (a)(c). Since K |= x.(w00 )(x),K |= (w00 ) v >r .{a}; thus, >r w00 , w0 , . . . , wm satisfy property (2).>r (w0 , w00 ) term w0 occurring . Then, >r (w0 , w) so, inductivehypothesis, role chain L(>r ) terms w0 = w0 , . . . , wm = w exist satisfyingproperties (a)(c). then, >r w0 , . . . , wm , w00 satisfy property (2).(cr4) inductive hypothesis, K |= (w0 ) v A; moreover, terms w0 , . . . , wmw0 = w wm = w0 nonempty role chain = 0 S1 1 m1 SmL(S) exist satisfying properties (a)(c). definition L(S), K |= v S;together entailments properties (b) (c), K |= (w) v S.(w0 ).then, K |= (w) v S.A, implies K |= (w) v B, required property (1).(cr5) inductive hypothesis, nonempty role chain L(S) exist satisfyingproperties (a)(c); moreover, K |= v definition L(S). Role simple,|| = 1, therefore one following two forms.= 0 0 = . property (c), K |= (w) v T.Self K |= v .Furthermore, due K |= v S, K |= v S. then, K |= (w) v S.Self,K |= (w) v B holds, required property (1).= S1 . Terms w0 w1 satisfying property (2) equal w; moreover,w1 form fS1 ,A1 (w), w IK . Furthermore, property (b)688fiThe Complexity Answering CQs GXQs OWL 2 EL KBsK |= (w) v S1 .(w); together w IK , K |= (w) v S1 .Self. Finally,due K |= v S, K |= S1 v S. then, K |= (w) v S.Self,K |= (w) v B holds, required property (1).(cr6) inductive hypothesis, K |= (w) v A, concludeK |= (w) v S.Self, property (2) holds w0 = w = 0 = S.(cr7) = , w = w0 K |= v S, property (2) holds w0 = w= 0 = S. Otherwise, assume nonempty form = S1 Sk .Thus, terms w0 , . . . , wk w0 = w wk = w0 exist Si (wi1 , wi )[1..k]. inductive hypothesis, [1..k], terms w0i , . . . , wmw0 = wi1 wmi = wi role chain L(Si ) exist satisfying propertieski1 = w(a)(c); note w0i = w0 = w, wm= wk = w0 , wm0i1k1k[1..k]. definition L(S), L(S), property (2) holds1 , . . . , wk , . . . , wk .role chain 1 k terms w0 , w11 , . . . , wmmk114.4.3 Soundnessready show algorithm entails sound.Lemma 28. nondeterministic computation exists entails(K, q) returns true,K |= q.Proof. Assume nondeterministic computation exists entails(K, q) returnstrue. algorithm returns true step 1, K |= q, K inconsistent; hence,rest proof, assume K consistent show IK |= q. end, letsubstitution , skeleton = hV, E, i, function L determined entails. GraphhV, Ei forest rooted individuals occurring K so, structural inductionforest, define mapping V universe K satisfy following:(i) v V , (v)( (v)) IK ;(ii) hv, v 0 E Pj L(v, v 0 ), role chain j L(Pj ) existsj ( (v), (v 0 )) IK .Base case. IK , let (a) = a. Since (a) = {a} {a}(a) IK , firstproperty clearly holds, second property vacuous.Inductive step. Consider hv, v 0 E (v) defined, (v 0 ) not;let L(v, v 0 ) = {P1 , . . . , Pm }. Since exist((v), (v 0 ), L(v, v 0 )) returns true, Lemma 24roles S1 , . . . , Sn , atomic concepts A1 , . . . , , and, j [1..m], rolechain j = S1 j,1 Sn j,n exist n 1, A0 = (v) = (v 0 ),following holds [1..n] j [1..m].1. IK , K |= Ai1 v Si .Ai K 6|= Ai v {a}.2. role occurring j,i , K |= Ai v T.Self K |= v .3. j LdR (Pj ).689fiStefanoni, Motik, Krotzsch, & RudolphLet w0 = (v); let wi = fSi ,Ai (wi1 ) [1..n]; let (v 0 ) = wn . Since A0 = (v),inductive hypothesis A0 ( (v)) IK . Furthermore, (cr3) applicableIK so, [1..n], Si (wi1 , wi ) IK Ai (wi ) IK ; thus, ( (v 0 )) IK ,required. Finally, role occurring j,i , K |= Ai v T.SelfK |= v ; (cr6) (cr7) applicable IK , respectively, (wi , wi ) IK ;thus, j ( (v), (v 0 )) IK , required.next show ((q)) IK considering independently atom (q).prove lemma, combine obvious way.Consider arbitrary unary atom A(t) (q). step 4 Algorithm 1,K |= (t) v A, also implies K |= (t) u (t) v A. property (i),(t)( (t)) IK . Since rule (cr1) applicable IK , A( (t)) IK , required.Consider arbitrary binary atom S(t, u) (q). Let v0 , . . . , vn , s0 , . . . , sn ,0 , . . . , n determined steps 811 Algorithm 1 considers atom S(t, u).[1..n], pda(si1 , i1 , si , ) L(vi1 , vi ) step 12; then, property (ii)role chain exists L(pda(si1 , i1 , si , )) ( (vi1 ), (vi )) IK .Next, define 0 considering following two cases.v0 I. step 14 Algorithm 1, role chain 0 = S1 Sk exists0 L(wfa((t), (v0 ))). property (i), (t)( (t)) IK ; moreover, Definition 20, basic concepts (t) = A0 , A1 , . . . , Ak = {v0 } exist K |= Aj1 v Sj .Ajj [1..k]. Rules (cr2) (cr3) applicable IK ,0 ( (t), (v0 )) IK .v0 6 I, implies v0 = t. step 16 Algorithm 1, role chain 0 = T1 Tkexists 0 L(sfa((v0 ))). property (i), (v0 )( (v0 )) IK ;moreover, Definition 20, K |= (v0 ) v Tj .Self K |= v Tj j [1..k].Rules (cr6) (cr7), respectively, applicable IK , thus 0 ( (t), (v0 )) IK .either case, steps 14 16 0 L(pda(iS , , s0 , 0 )). let 0 = 0 n ;note n = 0, case 0 = 0 . Clearly, 0 ( (t), (vn )) IK ,vn = u. Moreover, 0 L(pda(iS , , sn , n )) sn = fS n = . Finally,rule (cr7) applicable IK , S( (t), (u)) IK , required.4.4.4 Completenessnext prove encoding also complete, thus proving Theorem 21.Lemma 29. K |= q, nondeterministic computation exists entails(K, q)returns true.Proof. Assume K |= q. K inconsistent, entails(K, q) returns true, required;hence, rest proof, assume K consistent. then, IK |= q,substitution exists (q) IK . Let defined Section 4.4.2.substitution step 2, let (y) := (y) (y) I; otherwise, let (y)arbitrary, fixed, variable 0 q (y) = (y 0 ). straightforward see((q)) IK .690fiThe Complexity Answering CQs GXQs OWL 2 EL KBsskeleton = hV, E, step 3, set V contains IK variables occurring(q), (y) = ((v)) variable V . Furthermore, let smallestirreflexive transitive relation universe K w fS,A (w) termw universe K; then, let hv, v 0 E (v) (v 0 ) v 00 V exists(v) (v 00 ) (v 0 ). definition , graph hV, Ei forest rootedIK , required Definition 18.step 4, arbitrary atom A(t) (q), A((t)) IK ; property (1)Proposition 27, K |= ((t)) v A; hence, condition satisfied.consider arbitrary edge hv, v 0 E; let w0 , . . . , wk terms, let A1 , . . . , Akatomic concepts, let S1 , . . . , Sk roles w0 = (v), wk = (v 0 ),wi = fSi ,Ai (wi1 ) [1..k]; finally, let A0 = (v). Noteuniquely defined edge, that, construction IK , [1..k],K |= Ai1 v Si .Ai K 6|= Ai v {a} IK . Then, role chain compatibleedge hv, v 0 role chains 1 , . . . , k exists = S1 1 Sk k and,[1..k] role occurring , K |= Ai v T.Self K |= v .rest proof show following property.() PDA P L(v, v 0 ), role chain L(P) exists compatibleedge hv, v 0 i.Lemma 24 definition compatibility, property () impliescondition step 18 satisfied edge hv, v 0 i.loop steps 616, let S(t, u) arbitrary binary atom (q); nextdetermine required nondeterministic choices preserve () step 12,satisfy conditions steps 14 16, completes proof lemma. Let au IKunique individual connected u hV, Ei. Since S((t), (u)) IK , nonemptyrole chain = 0 S1 1 m1 Sm L(S) terms w0 , . . . , wmuniverse K w0 = (t) wm = (u) exist satisfying property (2) Proposition 27.define vertex v0 step 8, consider two possibilities, also defineindex `0 [0..m] w`0 = (v0 ).j [0..m] exists wj IK , let v0 = au let `0 largest indexw`0 = au .Otherwise, let v0 = let `0 = 0.Let v0 , . . . , vn unique path connecting v0 u S. definition `0form terms w`0 +1 , . . . , wm , wj 6 j [`0 + 1..m]; (v0 ) = w`0 ;(vn ) = wm . Thus, [1..n], unique index `i exists (vi ) = w`i .let 0 = 0 S`0 `0 , let = S`i1 +1 `i1 +1 S`i `i [1..n];clearly, = 0 n . properties (a)(c) Proposition 27, [1..n], role chaincompatible edge hvi1 , vi i. Furthermore, L(S) Theorem 8 implyLdR (pda(iS , , fS , )), states s0 , . . . , sn sn = fS words 0 , . . . , nn = exist 0 LdR (pda(iS , , s0 , 0 )) LdR (pda(si1 , i1 , si , )[1..n]. Since compatible hvi1 , vi i, step 12 preserves property (),required. Finally, consider step 13.691fiStefanoni, Motik, Krotzsch, & Rudolphv0 I. property (b) Proposition 27, K |= (wj1 ) v Sj .(wj ) j [1..`0 ].Furthermore, property c Proposition 27, K |= (wj ) v T.Self K |= vj [0..`0 ] role occurring j ; thus, K |= (wj ) v T.(wj ).then, 0 L(wfa((t), (v0 ))), condition step 14 satisfied.v0 6 I, v0 = 0 = 0 . property (c) Proposition 27, K |= (wj ) v T.SelfK |= v role occurring 0 . then, 0 L(sfa((v0 ))),condition step 16 satisfied.5. Lower Complexity Boundprevious section, presented BCQ answering algorithm ELRO+ usesspace polynomial total size input. algorithm worst-case optimalcombined complexity since Krotzsch et al. (2007) reduced PSpace-hard problemchecking nonemptiness intersection languages generated deterministicfinite automata F1 . . . Fm common alphabet (Kozen, 1977) BCQ answeringELRO+ . knowledge base K encoding problem, regular RBox contains rolesS1 . . . Sm L(Si ) = L(Fi ) [1..m]; furthermore, TBox ensuresuniversal interpretation IK rooted tree so, , term w existsreachable root chainroles corresponding ; finally, Boolean CQcontains atoms check whether L(Fi ) nonempty. next improve lowerbound showing problem hard already restricted setting query,TBox, ABox fixed, RBox varies.Theorem 30. K regular ELRO+ knowledge base q Boolean conjunctive query,checking K |= q PSpace-hard evenquery fixed consist two binary atoms single quantified variable,TBox fixed contains axioms form v S.A,ABox fixed contains single unary assertion.Proof. reduce PSpace-hard problem deciding whether intersectionlanguages generated deterministic finite automata nonempty (Kozen, 1977). Let0 deterministic finite automata alphabet 0 , let fresh symbolsF10 , . . . , Fm12occurring 0 , let = 0 {1 , 2 }. j [1..m], let Fj = hQj , , j , ij , fjdeterministic finite automaton alphabet obtained extending Fj0transition labelled 1 final state fj0 Fj0itself, transition labelled002 fj fresh final state fj Fj . Then,j ) 6= wordj L(F0w j L(Fj ) exist |w| odd: given w j L(Fj ), |w| odd, |w 1 2 |odd w 1 2 L(Fj ) j [1..m], |w| even, |w 2 | oddw 2 L(Fj ) j [1..m]. Finally, assume w.l.o.g. Qi Qj 6= Qi Rhold 1 < j m, R well.Let w = ST1 Sn word n odd, let = Q1 . . . Qm .Clearly, w j L(Fj ) holds word w formnnw = e01 e0m S1 o1m o11 S2 e21 e2m e1n1 en1Sn om o1692(46)fiThe Complexity Answering CQs GXQs OWL 2 EL KBsexists following conditions hold j [1..m]:(i) [1..n] odd, oij Qj j (ei1j , Si ) = oj ;(ii) [1..n] even, eij Qj j (oi1j , Si ) = ej ;(iii) e0j = ij onj = fj .let LO , LE , L1 , L2 following languages.LO :={e1 em om o1 | j (ej , S) = oj , j [1..m]}(47)LE :={om o1 e1 em | j (oj , S) = ej , j [1..m]}(48)L1 :=(LO ) LO(49)L2 :={i1 im } ( LE ) {fm f1 }(50)Consider arbitrary word w corresponding word w . definitionL1 , w L1 w form (46) satisfies property (i). Similarly,definition L2 , w L2 w theT form (46) satisfiesproperties (ii) (iii). Thus, w L1 L2 w j L(Fj ). simplicity,rest proof, use following equivalent formulations L1 L2 .L1 =LO (LO )+ LO(51)+L2 ={i1 im } {fm f1 } {i1 im } ( LE ) {fm f1 }(52)TWe next define knowledge base K fixed query q K |= qj L(Fj ) 6= . present construction stages, describeaffects canonical model = hI , Kthat is, model constructed usingstandard notion chase (i.e., Definition 25, semantic conditionsK replaced syntactic checks axioms K). simplicity, first present KTBox depends , later modify encoding use fixed TBox.TBox contains axioms (53), ABox contains axiom (54).assume aI = ; then, word , domain element existsconnected via chain roles corresponding .v.Asymbol(53)A(a )(54)next present RBox R consisting four parts, encoding languages LO , LE ,S,m+1S,mL1 , L2 . encoding uses fresh roles LS,1LS,0uniquely, . . . , LOE , . . . , LE0associated role , well fresh roles LO , LE , L , L1 , L1 , L2 , L02 .first part R contains axioms (55)(57). clear that, words1 , 2 1 prefix 2 , ha1 , a2 LIO 2 1 LO .v LS,m+1ejLS,j+1oj vLS,j(55)j [1..m] ej , oj Qj j (ej , S) = oj(56)LS,1v LO(57)693fiStefanoni, Motik, Krotzsch, & Rudolphsecond part R contains axioms (58)(60). clear that, words1 , 2 1 prefix 2 , ha1 , a2 LIE 2 1 LE .ojS,j1LEv LS,0E(58)LS,jEj [1..m] ej , oj Qj j (oj , S) = ej(59)ej vLS,mv LEE(60)third part R contains axioms (61)(65). clear that, words1 , 2 1 prefix 2 , ha1 , a2 LI1 2 1 L1 .v L(61)LO v L1(62)L01L01(63)v L1(65)LO L vL01 L01L01 LOv(64)fourth part R contains axioms (66)(69). clear that, words1 , 2 1 prefix 2 , ha1 , a2 LI2 2 1 L2 .i1 im L fm f1 v L2(66)L02L02(67)(68)L fm f1 v L2(69)L LE vL02i1 imL02L02vQuery q given (70). Then, K |= q word existsha , LI1 ha , LI2 , latter clearly case L1 L2 .RBox R regular size polynomial size automata F1 , . . . , Fm .q = y. L1 (a , y) L2 (a , y)(70)next tighten reduction use fixed TBox 0 consisting axioms (71)(72),P0 P1 fresh roles.v P0 .A(71)v P1 .A(72)let k = dlog2 ||e, assume symbol corresponds k-digit binarynumber b1 bk bi {0, 1}. Then, let R0 R extended axioms (73).Pb 1 Pb k vcorresponding b1 bk(73)Finally, let K0 = hT 0 , R0 , Ai, let 0 canonical model K0 . Axioms (54), (71),(72) ensure existence binary tree whose edges labelled roles P0 P1 .Furthermore, axioms (73) ensure that, sequence k edgestree corresponding binary number assigned , shortcut tree694fiThe Complexity Answering CQs GXQs OWL 2 EL KBslabelled . Thus, homomorphically embedded 0 . Finally, roles P0P1 occur R query q checks existence domain element connected ;therefore, extra edges 0 irrelevant. Consequently, encoding languages L1L2 works way varying TBox .Finally, characterise complexity BCQ answering ELRO+ knowledge bases.Theorem 31. K regular ELRO+ KB q Boolean CQ, checking K |= q1. PTime-complete data complexity,2. NP-complete, RBox R fixed,3. PSpace-complete combined complexity.Proof. Calvanese et al. (2006) proved BCQ answering PTime-hard data complexity already EL knowledge bases. Furthermore, query fixed, BCQanswering NP-hard already relational databases (Chandra & Merlin, 1977).theorem follows Theorems 23 30, Savitchs theorem.6. Navigational Queriesdata DL knowledge bases graph-like structure, unary assertions encodeproperties graph nodes binary assertions encode graph edges. Conjunctive queriescannot express recursive properties reachability, expressivity often insufficient applications require graph navigation. popularity graphdatabases rise, number navigational languages querying graph-like dataproposed; example, regular path queries (Barcelo, 2013) use regular expressions express complex navigational patterns graph vertices, graph XPathqueries (Libkin et al., 2013) extend regular path queries converse operator, negation regular expressions, checking properties vertices using Boolean combinationsconcepts existential quantifications paths. DL context, computational complexity navigational queries studied several expressive DLsmembers DL-Lite family EL(H) fragment ELRO+ (Calvanese, Eiter, &Ortiz, 2009; Bienvenu et al., 2013; Kostylev et al., 2014; Bienvenu et al., 2014). ordercomplete complexity landscape problem, section study problemanswering graph XPath queries ELRO+ knowledge bases.6.1 Graph XPath QueriesGraph XPath queries consist node expressions path expressions, whose syntaxesdefined respectively following two context-free grammars B basic conceptrole.B | | 1 2 | 1 2 | hi| | 1 2 | 1 + 2 | | | test()Following Libkin et al. (2013), consider following expression fragments.695fiStefanoni, Motik, Krotzsch, & RudolphP,E4PgPP,F3S,D5R,SRbUPS,D6S,A7UP,A2fPR,ScBRR,SeP,E1Figure 8: Interpretation1. path-positive fragment disallows path expressions form .2. positive fragment disallows path expressions form node expressionsform .3. converse-free fragment disallows path expressions form .graph XPath atom form (s) (s, t), node expression, pathexpression, terms. conjunctive graph XPath query (CGXQ) g expressiong = ~y . (~x, ~y ) conjunction graph atoms variables ~x ~y ; variables ~xcalled answer variables g. ~x = , g = ~y . (~y ) Boolean CGXQ.Path-positive, positive, converse-free CGXQs obtained restricting query atomsaccordingly. Finally, graph XPath query (GXQ) CGXQ containing single atom.define semantics CGXQs, let = hI , first-order interpretation.interpretation node path expressions inductively defined follows.()I(1 2 )I(1 2 )I(hi)I====\ ()I(1 )I (2 )I(1 )I (2 )I{x | : hx, yi }(S )I(1 2 )I(1 + 2 )I( )I()I(test())I======{hy, xi | hx, yi }(1 )I (2 )I(1 )I (2 )I(I )\ ()I{hx, xi | x }Please observe difference path expressions 1 2 corresponds (1 + 2 ),whereas intersection 1 2 corresponds (1 + 2 ); moreover, Libkin et al.(2013) define path expression , setting corresponds test(>c ). SatisfactionBoolean CGXQ g CGXQ entailment defined obvious way; moreover,Boolean CGXQ answering problem checking K |= g.Example 32. illustrate definitions using interpretation shown Figure 8;notation Example 15. Moreover, let 1 , 2 , 3 following path expressions.1 =(R test(hS test(A B)i))(74)2 =(U test(hP test(A B)i))(75)696fiThe Complexity Answering CQs GXQs OWL 2 EL KBsNode expressionsTB = {B v CB }T1 2 = {C1 u C1 v C1 2 } T1 T2T1 2 = {C1 v C1 2 , C2 v C1 2 } T1 T2Thi = {T .>c v Chi }Path expressionsTS =T1 2 = T1 T2T1 +2 = T1 T2=Ttest() = {C v Ttest() .Self}RB =R1 2 = R1 R2R1 2 = R1 R2Rhi = RRS = {S v TS }R1 2 = {T1 T2 v T1 2 } R1 R2R1 +2 = {T1 v T1 +2 , T2 v T1 +2 } R1 R2R = { v , v , v } RRtest() = RTable 6: Encoding positive, converse-free node path expressions using axioms3 =((R S) )(76)Expression 1 positive, retrieves pairs individuals connectedpath R-edges that, element occurring path first,exists outgoing path S-edges reaching member concept B. example,{haI , dI i, haI , eI i} (1 )I .contrast, expression 2 path-positive, retrieves pairs individualsconnected U -edge P -successor exists member concept AtB.example, haI , g (2 )I , haI , f 6 (2 )I .Finally, expression 3 neither positive path-positive, retrieves pairsindividuals connected path consisting sequence edges describedregular expression (R S) . example, haI , dI (3 )I , haI , eI 6 (3 )I .Let g = x, y, z.1 (x, y) 2 (x, z) 3 (x, y) conjunctive graph XPath query, let= {x 7 a, 7 d, z 7 g} substitution. Using Figure 8, one check |= (g).observed Kostylev et al. (2014), node expressions graph XPath queries correspond precisely formulas propositional dynamic logic negation (PDL ) (Harelet al., 2000); satisfiability problem PDL undecidable (Harel, 1984), answeringGXQs DL constraints undecidable. Decidability results recently obtained path-positive positive queries DL-Lite knowledge bases (Kostylev et al.,2014). addition, Kostylev et al. (2014) proved that, DLs, answering path-positive,converse-free GXQs coNP-hard data-complexity. Finally, Bienvenu et al. (2014) provedanswering positive GXQs EL knowledge bases ExpTime-complete. Thus,rest section focus positive, converse-free graph XPath queries.6.2 Complexity Answering Positive, Converse-Free Graph XPath Queriesrest section, fix ELRO+ KB K = hT , R, Ai R regular.next show that, given positive, converse-free Boolean CGXQ g, one constructpolynomial time regular ELRO+ KB K0 Boolean CQ q 0 K |= gK0 |= q 0 . construction K0 combines various expressive features ELRO+ :697fiStefanoni, Motik, Krotzsch, & Rudolphrole inclusions reflexive roles encode path expressions g RBox, selfrestrictions encode node expressions g TBox.Proposition 33. Given positive, converse-free Boolean CGXQ g K, one computetime polynomial |K| + |g| ELRO+ KB K0 Boolean CQ q 0 RBoxK0 regular, g q 0 equally many atoms, K |= g K0 |= q 0 .Proof. Let g = ~y . (~y ) positive, converse-free Boolean CGXQ K.positive node expression , let C fresh atomic concept uniquely associatedand, positive, converse-free path expression , let fresh role uniquelyassociated . structural induction, associate (resp. ) TBoxRBox R (resp. TBox RBox R ) shown Table 6. Then, letK0 = hT 0 , R R0 , Ai TBox 0 RBox R0 follows.[[[[T0=R0 =RR(s)(s,t)(s)(s,t)let q 0 = ~y . 0 (~y ) Boolean CQ 0 contains C (s) atom (s)(s, t) atom (s, t) . Clearly, g q 0 number atoms;moreover, since query g K, query q 0 K0 . Finally, q 0 K0computed polynomial time input size, RBox K0 clearly regular.next show K0 6|= q 0 K 6|= g.() Assume K0 6|= q 0 , interpretation exists |= K0 6|= q 0 .Since axiom K also axiom K0 , |= K. Furthermore,positive node expression positive path expression , (C )I(T )I . prove claim simultaneous induction structure nodepath expressions.Base case. base case, let arbitrary node expression form = Blet arbitrary path expression form = S. Since B v CB 0 , v TS R0 ,model K0 , claim easily follows.Inductive step. inductive step, distinguish two cases.First, consider arbitrary node expression property holds nodepath expressions occurring . let x arbitrary element assumex ; show x CI considering various forms take.= 1 2 . Since x , x I1 x I2 . inductive hypothesis,x CI1 x CI2 . definition 0 , C1 u C2 v C 0 .Since model 0 , x CI , required.= 1 2 . proof case similar one above.= hi. Since x , exists hx, yi . inductivehypothesis, hx, yi TI . definition 0 , .>c v C 0 .Since model 0 , x CI , required.Second, consider arbitrary path expression property holdsnode path expressions occurring . let x arbitrary elementsassume hx, yi ; show hx, yi TI considering various formstake.698fiThe Complexity Answering CQs GXQs OWL 2 EL KBs= 1 2 . Since hx, yi , exists z hx, zi 1Ihz, yi 2I . inductive hypothesis, hx, zi TI1 hz, yi TI2 .Moreover, definition R0 , T1 T2 v R0 . Since modelR0 , hx, yi TI , required.= 1 + 2 . Since hx, yi , hx, yi 1I hx, yi 2I .inductive hypothesis, hx, yi TI1 hx, yi TI2 . definition R0 ,{T1 v , T2 v } R0 . Since model R0 , hx, yi TI .= 1 . First, consider case x = y. definition R0 ,v R0 . Since model R0 , hx, yi TI , required. Otherwise,consider case x 6= y. Since hx, yi , elements x0 , . . . , xn x0 = xxn = exist n > 0 hxi1 , xi 1I [1..n].inductive hypothesis, [1..n], hxi1 , xi TI1 . definitionR0 , T1 T1 v R0 . Since model R0 , hx, yi .= test(). follows x = x . inductive hypothesis,x CI . definition 0 , C v .Self 0 . Since model0 , hx, yi TI , required.then, since node path expressions g positive, 6|= q 0 implies 6|= g.I0() Assume K 6|= g, interpretation exists |= K 6|= g. Letinterpretation obtained extending fresh concepts roles follows.0(C )I =00(T )I =0definition K0 , straightforward see 0 |= K0 ; furthermore,definition q 0 , straightforward see 0 6|= q 0 , required.Next, establish complexity answering positive, converse-free (C)GXQsELRO+ knowledge bases.Theorem 34. K regular ELRO+ KB g positive, converse-free Boolean CGXQ,checking K |= g PTime-complete data complexity, PSpace-complete combinedcomplexity. g positive, converse-free Boolean GXQ, checking K |= g PTimecomplete combined data complexities.Proof. hardness data complexity Boolean positive, converse-free (C)GXQs followsPTime-hardness instance checking EL (Calvanese et al., 2006).positive, converse-free GXQs, hardness combined complexity inheritedPTime-hardness TBox reasoning EL (Baader et al., 2005). matching upperbounds, Proposition 33 allows us reduce Boolean GXQ answering checking entailmentsform K0 |= q 0 q 0 BCQ containing one atom. next show that,possible form q 0 , reduce latter problem checking entailment ELRO+concept inclusions, decided PTime. following, c arbitrarilychosen individual IK0 .K0 |= A(a) K0 |= {a} v A.699fiStefanoni, Motik, Krotzsch, & RudolphK0 |= y.A(y) K0 |= {c} v >r .A.K0 |= S(a, b) K0 |= {a} v S.{b}.K0 |= y.S(y, b) K0 |= {c} v >r .S.{b}.K0 |= y.S(a, y) K0 |= {a} v S.>c .K0 |= y1 , y2 .S(y1 , y2 ) K0 |= {c} v >r .S.>c .positive, converse-free CGXQs, hardness combined complexity given Theorem 31, matching upper bounds follow Theorem 23 Proposition 33.7. Conclusionspaper, presented first CQ answering algorithm OWL 2 EL runsPSpace, thus closing longstanding open question. algorithm basedinnovative, succinct encoding regular role inclusions using bounded-stack PDAthat is,finite automata extended stack fixed size. believe encoding interestingright, used optimise popular OWL 2 DL reasoners. Moreover,refined previously known PSpace lower bound CQ answering showingproblem remains PSpace-hard even query, TBox, ABox fixed (andRBox varies); thus, identify role inclusions culprit problemsPSpace-hardness. Finally, showed positive, converse-free GXQs CGXQsanswered OWL 2 EL knowledge bases PTime PSpace, respectively;interesting Bienvenu et al. (2014) showed adding converse operatormakes problem ExpTime-hard. Thus, least theoretical perspective, positive,converse-free (C)GXQs seem provide adequate language querying OWL 2 ELknowledge bases.see two main open problems future work. First, drawing inspirationsuccinct encoding role inclusions, shall extend combined approachStefanoni et al. (2013) OWL 2 EL thus obtain practical algorithm. Second, staticquery analysis fundamental task query optimisation, shall study containmentproblem graph queries ELRO+ constraints.Acknowledgementsresults article extension results published preliminary form Krotzsch et al. (2007) Proceedings 6th International SemanticWeb Conference (ISWC 2007). work supported Alcatel-Lucent; EU FP7project OPTIQUE; EPSRC projects MASI3 , Score!, DBOnto; DFG projectDIAMOND (Emmy Noether grant KR 4381/1-1).ReferencesAnselmo, M., Giammarresi, D., & Varricchio, S. (2003). Finite automata non-selfembedding grammars. Proceedings 7th International Conference Im-700fiThe Complexity Answering CQs GXQs OWL 2 EL KBsplementation Application Automata, CIAA02, pp. 4756, Berlin, Heidelberg.Springer-Verlag.Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). DL-Lite familyrelations. J. Artif. Intell. Res. (JAIR), 36, 169.Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Kaelbling, L. P.,& Saffiotti, A. (Eds.), Proceedings 19th International Joint Conference Artificial Intelligence (IJCAI 2005), pp. 364369, Edinburgh, UK. Morgan KaufmannPublishers.Baader, F., Brandt, S., & Lutz, C. (2008). Pushing EL envelope further. Clark, K.,& Patel-Schneider, P. F. (Eds.), Proceedings OWLED 2008 DC WorkshopOWL: Experiences Directions.Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).(2010). Description Logic Handbook: Theory, Implementation, Applications.Cambridge University Press. Paperback edition.Baget, J.-F., Leclere, M., Mugnier, M.-L., & Salvat, E. (2011). rules existentialvariables: Walking decidability line. Artif. Intell., 175 (9-10), 16201654.Barcelo, P. (2013). Querying graph databases. Hull, R., & Fan, W. (Eds.), PODS, pp.175188. ACM.Barrett, C., Jacob, R., & Marathe, M. (2000). Formal-language-constrained path problems.SIAM J. Comput., 30 (3), 809837.Bienvenu, M., Calvanese, D., Ortiz, M., & Simkus, M. (2014). Nested regular path queriesDescription Logics. Proc. 14th Int. Conf. Principles KnowledgeRepresentation Reasoning (KR 2014). AAAI Press.Bienvenu, M., Ortiz, M., & Simkus, M. (2013). Conjunctive regular path querieslightweight Description Logics. Rossi, F. (Ed.), IJCAI. IJCAI/AAAI.Cal, A., Gottlob, G., & Kifer, M. (2013). Taming infinite chase: Query answeringexpressive relational constraints. J. Artif. Intell. Res. (JAIR), 48, 115174.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodriguez-Muro, M.,Rosati, R., Ruzzi, M., & Savo, D. F. (2011). MASTRO system Ontology-BasedData Access. Semantic Web, 2 (1), 4353.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2006). Datacomplexity query answering Description Logics. Proc. 10th Int. Conf.Principles Knowledge Representation Reasoning (KR 2006), pp. 260270.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractablereasoning efficient query answering Description Logics: DL-Lite family. J.Autom. Reasoning, 39 (3), 385429.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2013). Datacomplexity query answering Description Logics. Artificial Intelligence, 195, 335360.701fiStefanoni, Motik, Krotzsch, & RudolphCalvanese, D., De Giacomo, G., Lenzerini, M., & Vardi, M. Y. (2000). Containmentconjunctive regular path queries inverse. Proc. 7th Int. Conf.Principles Knowledge Representation Reasoning (KR 2000), pp. 176185.Calvanese, D., Eiter, T., & Ortiz, M. (2009). Regular path queries expressive DescriptionLogics nominals. Boutilier, C. (Ed.), IJCAI 2009, Proceedings 21stInternational Joint Conference Artificial Intelligence, Pasadena, California, USA,July 11-17, 2009, pp. 714720.Calvanese, D., Vardi, M. Y., De Giacomo, G., & Lenzerini, M. (2000). View-based queryprocessing regular path queries inverse. Proceedings Nineteenth ACMSIGMOD-SIGACT-SIGART Symposium Principles Database Systems, PODS00, pp. 5866, New York, NY, USA. ACM.Chandra, A. K., & Merlin, P. M. (1977). Optimal implementation conjunctive queriesrelational data bases. Hopcroft, J. E., Friedman, E. P., & Harrison, M. A. (Eds.),Proc. 9th annual ACM Symposium Theory Computing (STOC 77), pp.7790, Boulder, CO, USA. ACM Press.Cruz, I. F., Mendelzon, A. O., & Wood, P. T. (1987). graphical query language supportingrecursion. SIGMOD Rec., 16 (3), 323330.Cuenca Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P. F., & Sattler, U.(2008). OWL 2: next step OWL. J. Web Sem., 6 (4), 309322.De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rosati, R., Ruzzi, M., & Savo, D. F.(2012). MASTRO: reasoner effective Ontology-Based Data Access. Horrocks,I., Yatskevich, M., & Jimenez-Ruiz, E. (Eds.), ORE, Vol. 858 CEUR WorkshopProceedings. CEUR-WS.org.Eiter, T., Ortiz, M., & Simkus, M. (2012a). Conjunctive query answering DescriptionLogic SH using knots. J. Comput. Syst. Sci., 78 (1), 4785.Eiter, T., Ortiz, M., Simkus, M., Tran, T.-K., & Xiao, G. (2012b). Query rewritingHorn-SHIQ plus rules. Hoffmann, J., & Selman, B. (Eds.), AAAI. AAAI Press.Fan, W. (2012). Graph pattern matching revised social network analysis. Deutsch,A. (Ed.), ICDT, pp. 821. ACM.Geffert, V., Mereghetti, C., & Palano, B. (2010). concise representation regularlanguages automata regular expressions. Information computation, 208 (4),385394.Giese, M., Calvanese, D., Haase, P., Horrocks, I., Ioannidis, Y., Kllapi, H., Koubarakis, M.,Lenzerini, M., Moller, R., Rodriguez-Muro, M., Ozcep, O., Rosati, R., Schlatte, R.,Schmidt, M., Soylu, A., & Waaler, A. (2013). Scalable end-user access big data.Akerkar, R. (Ed.), Big Data Computing. CRC Press.Glimm, B., Lutz, C., Horrocks, I., & Sattler, U. (2008). Conjunctive query answeringDescription Logic SHIQ. J. Artif. Intell. Res. (JAIR), 31, 157204.Gottlob, G., Manna, M., & Pieris, A. (2014). Polynomial combined rewritings existentialrules. Proc. 14th Int. Conf. Principles Knowledge RepresentationReasoning (KR 2014). AAAI Press.702fiThe Complexity Answering CQs GXQs OWL 2 EL KBsGottlob, G., & Schwentick, T. (2012). Rewriting ontological queries small nonrecursivedatalog programs. Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), PrinciplesKnowledge Representation Reasoning: Proceedings Thirteenth InternationalConference, KR 2012, Rome, Italy, June 10-14, 2012. AAAI Press.Grosof, B. N., Horrocks, I., Volz, R., & Decker, S. (2003). Description Logic Programs: Combining logic programs Description Logic. Proceedings 12th internationalconference World Wide Web, pp. 4857.Gutierrez, C., Hurtado, C. A., Mendelzon, A. O., & Perez, J. (2011). Foundationssemantic web databases. J. Comput. Syst. Sci., 77 (3), 520541.Harel, D. (1984). Dynamic logic. Gabbay, D., & Guenthner, F. (Eds.), HandbookPhilosophical Logic Vol. II, pp. 497604. Reidel Publishing Company.Harel, D., Tiuryn, J., & Kozen, D. (2000). Dynamic Logic. MIT Press, Cambridge, MA,USA.Hopcroft, J. E., Motwani, R., & Ullman, J. D. (2003). Introduction Automata Theory,Languages, Computation - international edition (2. ed). Addison-Wesley.Horrocks, I., Kutz, O., & Sattler, U. (2006). even irresistible SROIQ. Doherty,P., Mylopoulos, J., & Welty, C. A. (Eds.), KR, pp. 5767. AAAI Press.Horrocks, I., & Sattler, U. (2004). Decidability SHIQ complex role inclusion axioms.Artificial Intelligence, 160 (12), 79104.Johnson, D. S., & Klug, A. C. (1984). Testing containment conjunctive queriesfunctional inclusion dependencies. J. Comput. Syst. Sci., 28 (1), 167189.Kazakov, Y. (2008). RIQ SROIQ harder SHOIQ. Brewka, G., & Lang,J. (Eds.), KR, pp. 274284. AAAI Press.Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2011). combined approach Ontology-Based Data Access. Walsh, T. (Ed.), IJCAI 2011,Proceedings 22nd International Joint Conference Artificial Intelligence,Barcelona, Catalonia, Spain, July 16-22, 2011, pp. 26562661. IJCAI/AAAI.Kostylev, E. V., Reutter, J. L., & Vrgoc, D. (2014). XPath DL-Lite ontologies.Bienvenu, M., Ortiz, M., Rosati, R., & Simkus, M. (Eds.), Informal Proceedings27th International Workshop Description Logics, Vienna, Austria, July 17-20,2014., Vol. 1193 CEUR Workshop Proceedings, pp. 258269. CEUR-WS.org.Kozen, D. (1977). Lower bounds natural proof systems. FOCS, pp. 254266. IEEEComputer Society.Krotzsch, M. (2011). Efficient rule-based inferencing OWL EL. Walsh, T. (Ed.),Proceedings 22nd International Joint Conference Artificial Intelligence (IJCAI11). AAAI Press/IJCAI. 26682673.Krotzsch, M., Rudolph, S., & Hitzler, P. (2007). Conjunctive queries tractable fragmentOWL 1.1. Aberer, K., Choi, K.-S., Noy, N., Allemang, D., Lee, K.-I., Nixon, L.,Golbeck, J., Mika, P., Maynard, D., Mizoguchi, R., Schreiber, G., & Cudre-Mauroux,P. (Eds.), Proceedings 6th International Semantic Web Conference (ISWC07),Vol. 4825 LNCS, pp. 310323. Springer.703fiStefanoni, Motik, Krotzsch, & RudolphLibkin, L., Martens, W., & Vrgoc, D. (2013). Querying graph databases XPath.Tan, W.-C., Guerrini, G., Catania, B., & Gounaris, A. (Eds.), ICDT, pp. 129140.ACM.Lutz, C. (2008). complexity conjunctive query answering expressive DescriptionLogics. Automated Reasoning.Lutz, C., Seylan, I., Toman, D., & Wolter, F. (2013). combined approach OBDA:Taming role hierarchies using filters. Alani, H., Kagal, L., Fokoue, A., Groth, P. T.,Biemann, C., Parreira, J. X., Aroyo, L., Noy, N. F., Welty, C., & Janowicz, K. (Eds.),International Semantic Web Conference (1), Vol. 8218 Lecture Notes ComputerScience, pp. 314330. Springer.Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering Description Logic EL using relational database system. Boutilier, C. (Ed.), IJCAI2009, Proceedings 21st International Joint Conference Artificial Intelligence,Pasadena, California, USA, July 11-17, 2009, pp. 20702075.Marnette, B. (2009). Generalized schema-mappings: termination tractability.Paredaens, J., & Su, J. (Eds.), PODS, pp. 1322. ACM.Mora, J., Rosati, R., & Corcho, O. (2014). kyrie2: Query rewriting extensionalconstraints ELHIO. Mika, P., Tudorache, T., Bernstein, A., Welty, C., Knoblock,C. A., Vrandecic, D., Groth, P. T., Noy, N. F., Janowicz, K., & Goble, C. A. (Eds.),Semantic Web - ISWC 2014 - 13th International Semantic Web Conference, Rivadel Garda, Italy, October 19-23, 2014. Proceedings, Part I, Vol. 8796 Lecture NotesComputer Science, pp. 568583. Springer.Ortiz, M., Calvanese, D., & Eiter, T. (2008). Data complexity query answering expressive Description Logics via tableaux. J. Autom. Reasoning, 41 (1), 6198.Ortiz, M., Rudolph, S., & Simkus, M. (2011). Query answering Horn fragmentsDescription Logics SHOIQ SROIQ. Walsh, T. (Ed.), IJCAI 2011, Proceedings 22nd International Joint Conference Artificial Intelligence, Barcelona,Catalonia, Spain, July 16-22, 2011, pp. 10391044. IJCAI/AAAI.Perez, J., Arenas, M., & Gutierrez, C. (2010). nSPARQL: navigational language RDF.Web Semant., 8 (4), 255270.Perez-Urbina, H., Motik, B., & Horrocks, I. (2010). Tractable query answering rewritingDescription Logic constraints. J. Applied Logic, 8 (2), 186209.Rodriguez-Muro, M., & Calvanese, D. (2012). High performance query answeringDL-Lite ontologies. Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), PrinciplesKnowledge Representation Reasoning: Proceedings Thirteenth InternationalConference, KR 2012, Rome, Italy, June 10-14, 2012. AAAI Press.Rosati, R. (2007). conjunctive query answering EL. Calvanese, D., Franconi, E.,Haarslev, V., Lembo, D., Motik, B., Turhan, A.-Y., & Tessaris, S. (Eds.), DescriptionLogics, Vol. 250 CEUR Workshop Proceedings. CEUR-WS.org.Rudolph, S., & Glimm, B. (2010). Nominals, inverses, counting, conjunctive queries or:infinity friend!. J. Artif. Intell. Res. (JAIR), 39, 429481.704fiThe Complexity Answering CQs GXQs OWL 2 EL KBsSimanck, F. (2012). Elimination complex rias without automata. Kazakov, Y.,Lembo, D., & Wolter, F. (Eds.), Proceedings 2012 International WorkshopDescription Logics, DL-2012, Rome, Italy, June 7-10, 2012, Vol. 846 CEURWorkshop Proceedings. CEUR-WS.org.Sirin, E., Parsia, B., Cuenca Grau, B., Kalyanpur, A., & Katz, Y. (2007). Pellet: practicalOWL-DL reasoner. J. Web Sem., 5 (2), 5153.Stefanoni, G., Motik, B., & Horrocks, I. (2013). Introducing nominals combinedquery answering approaches EL. desJardins, M., & Littman, M. L. (Eds.),AAAI. AAAI Press.ter Horst, H. J. (2005). Completeness, decidability complexity entailment RDFSchema semantic extension involving OWL vocabulary. Web Semantics:Science, Services Agents World Wide Web, 3 (2-3), 79115.Tsarkov, D., & Horrocks, I. (2006). FaCT++ Description Logic reasoner: System description. Furbach, U., & Shankar, N. (Eds.), IJCAR, Vol. 4130 Lecture NotesComputer Science, pp. 292297. Springer.Urbani, J., van Harmelen, F., Schlobach, S., & Bal, H. E. (2011). QueryPIE: Backwardreasoning OWL Horst large knowledge bases. Aroyo, L., Welty, C.,Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy, N. F., & Blomqvist, E. (Eds.),International Semantic Web Conference (1), Vol. 7031 Lecture Notes ComputerScience, pp. 730745. Springer.Vardi, M. Y. (1982). complexity relational query languages (extended abstract).Proceedings fourteenth annual ACM symposium Theory computing, STOC82, pp. 137146, New York, NY, USA. ACM.Venetis, T., Stoilos, G., & Stamou, G. B. (2012). Incremental query rewriting OWL 2 QL.Kazakov, Y., Lembo, D., & Wolter, F. (Eds.), Proceedings 2012 InternationalWorkshop Description Logics, DL-2012, Rome, Italy, June 7-10, 2012, Vol. 846CEUR Workshop Proceedings. CEUR-WS.org.Virgilio, R. D., Orsi, G., Tanca, L., & Torlone, R. (2012). NYAYA: system supportinguniform management large sets semantic data. Kementsietsidis, A., & Salles,M. A. V. (Eds.), IEEE 28th International Conference Data Engineering (ICDE2012), Washington, DC, USA (Arlington, Virginia), 1-5 April, 2012, pp. 13091312.IEEE Computer Society.Wessel, M. (2001). Obstacles Way Qualitative Spatial Reasoning DescriptionLogics: Undecidability Results. Working Notes 2001 InternationalDescription Logics Workshop (DL-2001), Vol. 49. CEUR-WS.org.705fiJournal Artificial Intelligence Research 51 (2014) 805-827Submitted 5/14; published 12/14Hidden Markov Model-Based Acoustic Cicada DetectorCrowdsourced Smartphone Biodiversity MonitoringDavide ZilliOliver ParsonGeoff V MerrettAlex RogersDZ 2 V 07@ ECS . SOTON . AC . UKOSP @ ECS . SOTON . AC . UKGVM @ ECS . SOTON . AC . UKACR @ ECS . SOTON . AC . UKUniversity SouthamptonSouthampton, SO17 1BJ, UKAbstractrecent years, field computational sustainability striven apply artificial intelligence techniques solve ecological environmental problems. ecology, key issuesafeguarding planet monitoring biodiversity. Automated acoustic recognitionspecies aims provide cost-effective method biodiversity monitoring. particularlyappealing detecting endangered animals distinctive call, New Forest cicada.end, pursue crowdsourcing approach, whereby millions visitors NewForest, insect historically found, help monitor presence meanssmartphone app detect mating call. Existing research field acoustic insectdetection typically focused upon classification recordings collected fixed field microphones. approaches segment lengthy audio recording individual segments insectactivity, independently classified using cepstral coefficients extracted recordingfeatures. paper reports contrasting approach, whereby use crowdsourcing collectrecordings via smartphone app, present immediate feedback users whetherinsect found. classification approach remove silent parts recordingvia segmentation, instead uses temporal patterns throughout recording classifyinsects present. show approach successfully discriminate callNew Forest cicada similar insects found New Forest, robust common typesenvironment noise. large scale trial deployment smartphone app collected 6000reports insect activity 1000 users. Despite cicada rediscoveredNew Forest, effectiveness approach confirmed detection algorithm,successfully identified cicada app countries speciesstill present, crowdsourcing methodology, collected vast number recordingsinvolved thousands contributors.1. Introductionfield computational sustainability, seeks apply computer science artificial intelligence issues sustainability, received great attention recent years planetever stronger environmental, societal economical pressure (Quinn, Frias-Martinez, & Subramanian, 2014; Gomes, 2009). Work field striven bring artificial intelligence researchreal world, implementing practices promote sustainability environmentsafeguard living organisms. Towards goal, first step monitoring biodiversity,variety living species given environment. Biodiversity key measurehealth ecosystem, land-use climate change impact natural environment,c2014AI Access Foundation. rights reserved.fiZ ILLI , PARSON , ERRETT & ROGERSFigure 1: Cicadetta montana. Photograph Jaroslav Maly, reproduced permission.many countries increasingly seeing need monitor protect it. example, UKformalised endeavour within UK Biodiversity Action Plan established priorityspecies list focus work small number critically important species (Joint Nature Conservation Committee, 2010). One these, particular interest paper, New Forest cicada(Cicadetta montana s. str., see Figure 1), native cicada known UK, firstidentified New Forest, national park south coast England, 1812. Despitewell studied number sites 1960s, confirmed observation NewForest cicada last 20 years (Pinchen & Ward, 2002). Understanding whether simplydue migration cicada yet undiscovered sites, whether cicada extinctUK due climate change land-use change, important question UK biodiversityresearch.Today, traditional approaches searching rare species typically require trained ecologistsperform detailed manual surveys. However, obvious costs work led significantrecent research automated approaches whereby animals plants classified remotelywithout requiring trained experts field. case insects, oftenperformed deploying fixed sensors sensitive microphones record sounds (or calls)emitted insects (MacLeod, 2007). recordings analysed later automaticallyidentify insects whose calls captured. algorithms classification typically rangeoperate solely time domain, time domain signal coding (Chesmore,2004; Chesmore & Ohya, 2004), inspired literature human speech recognition(for example Potamitis, Ganchev, & Fakotakis, 2006; Pinhas, Soroker, Hetzoni, Mizrach, Teicher, &Goldberger, 2008). latter typically use Gaussian mixture model hidden Markov modelclassification (Leqing & Zhen, 2010), perform number pre-processing stages, often takendirectly human speech recognition literature, extract features raw recording.example, Chaves, Travieso, Camacho, Alonso (2012) present state-of-the-art approachpre-processes recorded sound remove un-sounded periods insect call detected,mapping raw frequencies mel scale, better represents human hearing.approach converts mel scale features back pseudo-time domain, called cepstrum,calculating number mel frequency cepstral coefficients (MFCC), used features806fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCEhidden Markov model (HMM) classification. approaches shown classifyinsects high levels accuracy clean recordings collected using purpose-built hardware.use automatic acoustic recognition particularly appealing case NewForest cicada, since insect particularly loud high-pitched mating song which,close upper frequency limit normal adults hearing range inaudible adults40 years age, easily detected conventional microphones. However, usefixed sensors collect recordings later analysis less compelling. New Forest covers600 km2 , would require tens thousands sensors exhaustively survey potentialcicada breeding sites. Furthermore, since cicada emits mating call monthsJune July, approach must able survey large area short space time,decreasing applicability fixed sensors.address challenge, pursue different approach, aims exploit 13 million day visits New Forest occur year general public crowdsourcesearch New Forest cicada using smartphone app. involvement general publiccollection observations natural environment means recent practice,records farmers clergymen devoted activity date back centuries (Miller-Rushing,Primack, & Bonney, 2012; Brenna, 2011). start structured collection data, similarknow today practice citizen science, attributed beginning20th century, events Christmas Bird Count foundation American Association Variable Star Observers 1911 (Silvertown, 2009). However, Internet maderemote communication collaboration far easier, widespread adoption smartphonesgreatly facilitated cooperation amateur scientists around world collect processlarge amounts data. ecology, method vehicle wide participationcitizens plethora different initiatives proliferated last decade (see examplesurvey paper, Dickinson, Zuckerberg, & Bonter, 2010). example iRecord Ladybirds app (Nature Locator, 2013), system allows users collect geo-located photographsladybirds helps identify correct species series morphological taxonomic questions (e.g. colour number spots). Records stored database presentedpublic page. However, app attempt automate analysis classificationprocess, outsourcing task entirely users. recently, automation classification process portable device attempted birds bats (Jones, Russ, Catto,Walters, Szodoray-Paradi, Szodoray-Paradi, Pandourski, Pandourski, & Pandourski, 2009).former, due difficulty differentiating calls, work still progress datedeployed prototype exists. latter, project called BatMobile (Nature Locator, 2012) starting implement automated detection Apple iOS devices, requirement expensiveultrasonic microphones hinders accessibility tool general public large scale.system propose paper therefore, best knowledge, first deployedreal-time acoustic species recognition system run entirely mobile device.However, crowdsourcing acoustic biodiversity using smartphone app presents numberchallenges. Firstly, smartphones expected collect short recordings userwaiting (30 seconds case), contrast always-on recordings collected fixedsensors. fixed sensors would generate much longer recordings (in order hoursdays) result, existing classification methods required automatically remove silentperiods recording. side effect, also remove useful time-domain informationused easily differentiate insects similar frequency calls, especially lower807fiZ ILLI , PARSON , ERRETT & ROGERSquality recordings smartphone. makes existing methods unsuitable purpose.Furthermore, smartphone app would require algorithm provides real-time feedbackuser identification insect heard. allows user requestedcollect recording cicada detected, conversely user required uploadunnecessary recordings cicada detected1 . However, low-end mobile devices limitedprocessing capabilities compared high-end servers, therefore previously proposedcomplex feature extraction methods suitably efficient run real-time. addition,essential acoustic cicada detector able discriminate call NewForest cicada insects commonly found New Forest. Two examples insectssimilar calls dark bush-cricket, whose call similar pitch New Forest cicadainstead chirps duration typically 0.1 seconds; Roesels bush-cricket,whose call similar duration New Forest cicada covers broader frequency band.Although scenario involves detection relatively insects compared existing work,challenge design approach deployed field via low cost hardwarerediscovery New Forest cicada.Therefore, paper present algorithm, call Cicada Detection Algorithm(CDA), specifically intended real-time detection New Forest cicada computationallyconstrained smartphones. Rather calculating number cepstral coefficients existingwork, use Goertzel algorithm calculate magnitude specific frequency bands,efficient method approximating individual terms discrete Fourier transform (DFT)(Goertzel, 1958). extract following three frequency bands: first centred 14 kHzcorresponding strongest frequency component calls New Forest cicadadark bush-cricket, second centred 19 kHz, dark bush-cricket Roeselsbush-cricket still present, cicada not, third centred 8 kHz, fargeneral background noise (mostly lower 5 kHz) insects calls.calculate following two features form input hidden Markov model: ratio14 kHz 8 kHz distinguish New Forest cicada white noise acrossfrequencies, ratio 19 kHz 14 kHz distinguish New Forestcicada dark bush-cricket. Next, use five-state hidden Markov model explicitlyrepresents idle period insect calls, calls New Forest cicada, dark bush-cricketRoesels bush-cricket, also short pauses dark bush-crickets call. Hence,rather attempting independently classify individual segments insect calls using complexset features, exploit temporal patterns present throughout recording using HMM.use Viterbi algorithm identify likely sequence insect calls throughoutrecording.evaluate approach using 235 recordings 30 seconds duration collectedNew Forest Slovenia (where species cicada still present). Unlike standardlibrary recordings, data set represents range crowdsourced data likelyencounter, exhibiting significant noise (e.g. handling noise, road traffic noise, human voicenoise generated wind), insect calls varying amplitude depending proximityrecording device specimen. show approach able classify callNew Forest cicada normal environmental conditions F1 score 0.82. Since existingapproaches designed batch processing significantly longer recordings, compare1. 30s mono recording 44,100 samples per second, 2.7MB; significant file upload areas poormobile phone reception connection rates may 100kbps less.808fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCEAmplitudeFreq (kHz)2015105010-100:0500:1000:1500:2000:25Time (mm:ss)00:3000:3500:40Figure 2: Spectrogram waveform New Forest cicada call (recording Jim Grant, 1971courtesy British Library, wildlife sounds collection).approach three variants order evaluate benefit various components pipeline.results show feature extraction procedure robust noise, call Roeselsbush-cricket call dark bush-cricket, therefore satisfies requirementsdeployment environment.algorithm implemented mobile app, developed iOS Android,downloaded 1500 times members public. culminated large-scale trial deployment, citizen scientists submitting 6000 reports worldwide. Although New Forestcicada found first phase, accuracy detection algorithm wide geographical coverage achieved via crowdsourcing clearly motivate second phase deployment.approach also applied monitoring many singing species, apprecognise British Orthoptera currently development.preliminary version proposed method also compared state-of-the-artapproach batch classification insects proposed Chaves et al. (2012). comparison,presented Zilli, Parson, Merrett, Rogers (2013), shows method considerablycomputationally efficient, therefore better suited real-time operation. method proposedpaper improved accuracy efficiency presented Zilli et al.remainder paper organised follows. Section 2 describe proposedapproach, highlighting different techniques used. Section 3 analyse performance usinghundreds smartphone recordings. Section 4 present first phase deploymentapproach smartphone application, analyse coverage reports collected date. Finally,conclude Section 5 along plans second phase deployment ensurecomplete coverage New Forest.2. Real-Time Insect Detection Using Hidden Markov Modelsgive description proposed approach real-time insect detection. first describeefficient method individual terms DFT extracted raw audio recordingsusing Goertzel algorithm. describe two features calculated threeindividual terms DFT produce feature vector discriminate insectsinterest also robust environment noise. Next, formalise classification ex809fiZ ILLI , PARSON , ERRETT & ROGERStracted features inference problem HMM. Last, propose five-state HMM designedspecifically capture temporal patterns insects calls.2.1 Feature Extraction Using Goertzel Algorithmobserved strong high frequency components calls insects interest.frequencies sufficiently distant common background noise, wind noise, roadtraffic people speaking, reliable indicator presence insect. Figure 2 showsexample frequency component, call New Forest cicada centred14 kHz. efficient approximation magnitude frequencies calculated usingGoertzel algorithm; method evaluates individual terms DFT, implemented secondorder infinite impulse response filter.efficient implementation Goertzel algorithm requires two steps. first step producescoefficient pre-computed cached reduce CPU cycles:c = 2 cos2ffs(1)f central frequency question fs sampling rate recording.second step consists iteratively updating values temporary sequenceincoming sample sn that:yn = hamming(sn ) + (c yn1 ) yn2(2)samples passed Hamming filter, given by:hamming(sn ) = 0.54 0.46 cos2snN 1(3)length sequence samples N determines bandwidth B Goertzel filter,that:fsB=4(4)Nshorter sequence length N yields larger bandwidth, cost noisier output.practice, use multiples 128 samples match typical smartphones audio recording buffersize. example, block size N = 128 samples gives bandwidth 1.4 kHz.magnitude frequency band centred f bandwidth B time slice givenby:q2 + y2(5)mt,f = yNN 1 c yN yN 1terms computational complexity, approach shows considerable benefit comparedsingle-bin DFT. efficient algorithm compute latter, fast Fourier transform,complexity O(N logN ), Goertzel algorithm order O(N ), Nnumber samples per window. Moreover, sample update described Equation 5 processed real-time, eliminating need independent background thread smartphoneapp need store sample values.810fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCEz1z2z3zTx1x2x3xTFigure 3: hidden Markov model. Unshaded square nodes represent hidden discrete variables,shaded circular nodes represent observed continuous variables. xt vector twofeatures xt,1 xt,2 .2.2 Feature Combination Using Filter Ratiomagnitude frequency component 14 kHz good indicator presenceNew Forest cicada, robust background noise, normally contained lower5 kHz frequency spectrum. However, may sensitive white noise covers entirespectrum, handling noise. Furthermore, able discriminatecalls New Forest cicada Roesels bush-cricket, exhibit prolongedcall similar frequency. Therefore, extract following three frequencies using Goertzelalgorithm: mt,8 represents 8 kHz frequency outside range cicadacall environmental noise, mt,14 represents 14 kHz frequency New Forestcicada dark bush-cricket, mt,19 represents 19 kHz frequency darkbush-cricket Roesels bush-cricket. take ratios frequencies produce twofeatures:mt,14mt,19xt,1 =, xt,2 =(6)mt,8mt,14such, point t, xt,1 high presence insects consideredtend one either sound detected cicada range sound present acrossbands. addition, xt,2 high presence dark bush-cricket, tendzero presence New Forest cicada. two features form -by-2 feature vectorused classification model. order obtain real-time computationally efficientinsect identification, adopt HMM-based approach classification described followingsection.2.3 Classification Using Hidden Markov ModelHMM consists Markov chain discrete latent variables sequence continuous observed variables, dependent upon one discrete variables state (Ghahramani, 2001).Figure 3 shows graphical structure HMM, discrete, hidden variables (e.g. idle,cicada singing) represented sequence z1 , . . . , zT , continuous, observed variables(the features extracted audio recording) represented sequence x1 , . . . , xT .value discrete variable zt corresponds one K states, continuous variabletake value real number.behaviour hidden Markov model completely defined following three parameters. First, probability state hidden variable = 1 represented vector811fiZ ILLI , PARSON , ERRETT & ROGERSthat:k = p(z1 = k)(7)Second, transition probabilities state 1 state j representedmatrix that:Ai,j = p(zt = j|zt1 = i)(8)Third, emission probabilities describe observed feature, x, given parameters ,case follow log-normal distribution that:xt,f |zt , ln N (zt , z2t )(9)= {, 2 }, zt z2t mean variance Gaussian distributionstate zt . Figure 4 shows histogram data generated cicadas song, along log-normaldistribution fitted data. log-likelihood ratio test normal, log-normal exponentialdistributions fitted data set cicada songs shows log-normal distribution matchesdata better normal (F = 3512.13, p < 0.001) exponential (F = 1516.06, p < 0.001)distributions. However, despite long tail, log-normal distribution still poor supportdata unusually high magnitude, often generated handling noise. order preventmodel strongly favouring certain state data point extreme lognormal distribution, cap emission probabilities capture cases data likelypoorly represented model. outcome likelihood datapoints result correct state may low model triggers state change even thoughtransition probability strongly discourages (by low). Therefore, capemission probability data points maximum ratio, initially 100,state preferred another.Equations 7, 8 9 used calculate joint likelihood hidden Markovmodel:p(x, z|) = p(z1 |)p(zt |zt1 , A)p(xt |zt , )(10)t=2t=1model parameters collectively defined = {, A, }.use Viterbi algorithm (Viterbi, 1967) infer likely sequence hidden statesgiven features described. Despite fact number possible paths grows exponentially length chain, algorithm efficiently finds probable sequencemaximising Equation 10, cost grows linearly length chain.2.4 5-State Finite State Model Insect Callpropose five-state HMM cicada detection, states consist of: idle stateinsect singing (I), cicada singing state (C), state dark bush-cricketchirping (DC ), short pause dark bush-crickets chirps (DSP ) stateRoesels bush-cricket calling (R). emission parameters, i.e. location scale blog-normal distribution, learned empirically using:!22= ln p, b = ln 1 + 2(11)2 + 2812fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCELog-normal emissionprobabilitiesProbability density functionEmpirical datacicada callFeature value (mt,14 /mt,8 )Figure 4: Log-normal distribution extracted feature cicada callRDCDSPCIdle stateDark bush-cricketDC Dark bush-cricket's chirpDSP Dark bush-cricket's short pauseRRoesel's bush-cricketCNew Forest cicada(a) 5-state model used approach.RC(b) 3-state model.Figure 5: Comparison finite state machines.represents mean 2 represents variance data. manual estimationoriginally based recordings authors gathered historical archives,therefore improved recordings obtained deployment work, describedfollowing section.transition matrices describing dynamics Markovian process representedgraphically using finite state machines. Figure 5a shows five states described possible transitions, non-zero probability represented arrows connecting twostates. model explicitly represents silence dark bush-crickets chirps,essential information distinguishing calls New Forest cicada dark bushcricket. contrast existing batch classification methods remove silent periodsrecording order improve computational cost operation classify soundedperiods sample file (Chaves et al., 2012). methods also employ feature extraction process whereby compute number mel-frequency cepstral coefficients speciesmodel, making process scalable several insects, cost higher computational complex813fiZ ILLI , PARSON , ERRETT & ROGERSity. contrast, method proposed Section 2.1 closely tailored requirementsscenario, producing improvement efficiency necessary mobile application. Figure 5b shows variant approach silent states removed,compare approach following section. Furthermore, HMM could arrangedfully-connected, allowing transitions states otherwise disconnected (for example Roesels Bush-cricket Dark bush-cricket). However, confuses modelstates similar emission probabilities, without providing improvementaccuracy. therefore exclude variation comparison following section.3. Empirical Evaluation Using Smartphone Recordingsintroduce three variants approach described thus far that, following practicesliterature, motivate choices made construct Cicada Detection Algorithm. first variantuses approach proposed Section 2, three raw frequencies, opposedratio, used directly features (CDA raw frequencies). second variant removes un-soundedperiods recording and, such, segments individual calls. applies 3-statemodel shown Figure 5b classify insects (CDA silence removed). third approachapply HMM all, instead uses ratio frequencies directly identify likelystate, given instantaneous emission probabilities features. such, methodconsidered mixture model, since time slice classified independently. methodconsiderably computationally efficient, cost losing information timedomain.evaluate accuracy approach using collection 235 recordings taken citizenscientists using smartphones New Forest (the known UK habitat New Forestcicada) authors paper Slovenia (where species cicada still present)summer 2013. recording 30 seconds long, cases contains calleither New Forest cicada, dark bush-cricket Roesels bush-cricket. recordingscontain different types noise, including people speaking, walking, calls birds, handling noiseeven people mimicking sound cicada. contrast existing recording libraries,data set represents typical quality crowdsourced data, exhibiting significant noise insectcalls varying amplitude depending proximity recording device specimen.recording later labelled domain experts containing either one none insectsinterest. Although multiple insects recordings make classification fail,consider one singing insect per recording. one present, set ground truthacross 30-second recording longest loudest singing insect, therefore taking stateactive longest period outcome model. Since emission probabilitiesmodel purposely tuned, require training data such, hence use entiredata set test data. describe deployment smartphone app used collect datadetail Section 4.assess accuracy approach correctly classify cicada usingstandard precision, recall F1 score metrics. precision represents fraction recordingsapproach detected cicada singing fact singing, recallrepresents fraction recordings cicada singing correctly detected.814fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCEApproachCDACDA raw frequenciesCDA silence removedMixture modelPrecision0.660.460.620.61Recall0.780.940.990.65F1 -score0.820.620.750.67Table 1: Accuracy metrics cicada detectionCRCRCRCCCRRRR(a) CDACR(b) CDA raw frequencies (c) CDA silence removedC(d) Mixture modelFigure 6: Confusion matrices four variants detection algorithm. y-axis,actual class; x-axis, predicted class.Precision recall defined as:precision =tp,tp + f precall =tptp + f n(12)tp represents number correct cicada song detections, f p represents numbercicada song detections actually singing, f n represents number cicadasongs detected. work primarily concerned detection NewForest cicada, insects modelled order avoid false positive detectionsNew Forest cicada. also use F1 score, represents weighted combinationprecision recall, defined as:F1 = 2precision recallprecision + recall(13)Table 1 shows precision, recall F1 score metrics approach compared threevariants data set recordings New Forest Slovenia. Similarly, Figure 6reports true false positives, real values along axis predicted class along xaxis. seen approach (CDA) achieves F1 score 0.82, outperformsbenchmark variant, visually apparent darkness along main diagonal Figure 6a.contrast, variant approach uses raw frequency measurements HMMfeature vector (CDA raw frequencies) receives F1 score 0.62. result approachslack robustness noise, handling noise, shown high number false positivesFigure 6b. Furthermore, variant approach removes silent periods (CDA silenceremoved) receives F1 score 0.75. Although appears positive result, Figure 6c highlights815fiZ ILLI , PARSON , ERRETT & ROGERSlack ability discriminate dark bush-cricket New Forest cicada.method, well raw frequencies approach, favour New Forest cicada, scoring goodtrue positive rate consequently also high false postive rate. Finally, mixture model methodreceives F1 score 0.67 lack transition probabilities leaves decisionemission probabilities only, utilising information contained time domain, makingnumber true false positives equally distributed (Figure 6d). Insects similaremission probabilities, Roesels bush-cricket dark bush-cricket, thereforedifficult classify method. noted however approach considerablycomputationally efficient, decides likely state instantaneously withouttraversing entire recording.Figures 7, 8, 9 10 show comparison four approaches sample recordingfour species recordings analysed. top plot figure shows spectrogramtime domain x-axis, frequency domain y-axis, magnitudefrequency bins varying colour plot. Subsequently, figure showslikely state identified approach. plot, states labelled Figure 5a,represents un-sounded idle state (if present), C represents cicadas song, R representsRoesels bush-cricket DC DSP dark bush-crickets chirping short pause states,respectively. gaps silence-removed variant correspond unsounded periods.Figure 7 shows classifying cicada easier HMM-based methods, call lastslong period without interruption clearly distinct background noise. noisyrecording would cause raw-frequency approach fail. mixture model approach strugglesdistinguish cicada dark bush-cricket call, since similar featuresdifferent time domain, model capture. Figure 8 shows variantssensitive noise CDA different reasons. raw frequencies approach doesntfilter background noise, mixture model triggers cicada state even shortnoise right frequency band. silence-removed method active short periodhigher background noise, idle state, forced classify soundsounded states. Figure 9 shows how, silence removed, Roesels bush-cricket becomessimilar dark bush-cricket, similar emission probabilities. perceptionobserved mixture model, doesnt perception time. Similarly, Figure 10 showsdark bush-cricket difficult classify mixture model approach silenceremoved, explained thus far. Moreover, shows trade-off quiet insect(visible throughout recording) insect must made, insect could distancemicrophone, thus limit quiet may be.analysis 235 recordings presented detail projects web page, together parameters HMM, audio file, information recording device2 .4. Automated Classification Smartphone Appdeployed insect detection algorithm within smartphone app enables wide participationsearch critically endangered species. process, often referred citizen science,attempts leverage widespread presence users willing participate actdistributed network sensors, learning scientific process behind certain research,2. Result http://www.newforestcicada.info/devdash. data used free charge, providedNew Forest Cicada Project attributed according Creative Commons Attribution (BY) licence.816fiMost likely stateFreq (kHz)N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE20Spectrogram151050RCDCDSPCDACDAraw frequenciesRCDCDSPCDAsilence removedRCMixture modelRCDCDSPlikely stateFreq (kHz)Figure 7: Model comparison New Forest cicada recording20Spectrogram151050RCDCDSPCDACDAraw frequenciesRCDCDSPCDAsilence removedRCMixture modelRCDCDSPFigure 8: Model comparison recording singing insect817fiMost likely stateFreq (kHz)Z ILLI , PARSON , ERRETT & ROGERS2015105Spectrogram0RCDCDSPCDARCDCDSPCDAraw frequenciesRCCDAsilence removedRCDCDSPMixture modellikely stateFreq (kHz)Figure 9: Model comparison Roesels bush-cricket recording20Spectrogram151050RCDCDSPCDACDAraw frequenciesRCDCDSPCDAsilence removedRCMixture modelRCDCDSPFigure 10: Model comparison dark bush-cricket recording818fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCEPossible cicadadetectedSave audiorecordingLive detectionRecord audioSounds interestingUpload reportcicadadetectedFigure 11: Flow detection classification process app.case automated identification species biodiversity monitoring. Examplescommunities app caters include tourists visitors New Forest National Park,local residents bug enthusiasts. order maximise number citizen scientists takingpart, essential app compatible wide range hardware, additionsimple unobtrusive use. Therefore, released iPhone Android client,ensures compatibility 80% smartphone users (Go-Gulf, 2012). Furthermore,designed app simple use, require user consent recording audio constrainusage battery mobile data usage.Figure 11 shows overview flow interaction user takes recordingapp. user first opens app presented live detection screen,displays graphical representation audio signal entering microphone, formcircular spectrogram, immediate feedback presence singing cicada, obtainedoutput mixture-model described Section 2. Upon selecting start audiorecording, user shown current progress 30 second recording. completingrecording, CDA run user presented one three possibilities: possiblecicada detected screen, sounds interesting screen (which notes algorithm detectedinsect cicada), cicada detected screen (where nothing known found).report survey saved locally uploaded upon connecting Internet. recordingcontains known insects, user asked consent upload recorded audio.4.1 Stages Real-Time Classificationorder capture sound fed automated classifier, user presented intuitiveinterface, summarised Figures 12 13 detailed follows:4.1.1 L IVE ETECTORFigure 12a shows detector screen, appears upon loading app. crucial difficultyhuman detect New Forest cicadas call fact pitch high peoplehear, since central frequency limit hearing range average 40 year old.address issue, tab presents visualisation sound drawn circular spectrogram.centre, cicada logo lights call detected, triggered instantaneous outputmixture model described Section 2, updated every 128 samples microphone. Twenty819fiZ ILLI , PARSON , ERRETT & ROGERS(a) Live detector(b) Audio recording(c) Upload recordingFigure 12: Three screens Cicada Hunt Android. left, cicada singing lightsicon frequency bands around 14 kHz. middle, cicada singing surveystopped shortly 15 seconds. right, latest survey waiting uploaded.concentric circles around represent twenty frequency bands spectrum, centred 120 kHz bandpass 1.4 kHz, extracted 20 Goertzel filters, ensure rapid updatinginterface. becomes brighter higher signal strength (i.e. louder soundpitch) paler band quieter. outer bands, roughly 12 18 kHz,triggered cicada call, producing distinctive effect shown Figure 12a. Tappingcicada icon centre app starts 30-second survey, sound recordedanalysed algorithm described Section 2. idea core interface,encourages users stop wait silence, thus maximising chance detecting requiredsound. choice 30 seconds strikes balance length cicada callamount time usermostly occasional visitor forestcan expected stand stillsilence.4.1.2 AUDIO R ECORDINGFigure 12b shows screen shown 30 second audio recording. recordingfinished, audio analysed HMM-based algorithm described Section 2. Dependingresult classification, user shown either cicada detected screen, soundsinteresting screen possible cicada detected screen.4.1.3 U PLOAD R ECORDINGFigure 12c shows list reports saved locally. report geo-taggedtime-stamped, saves unique identifier phone well basic informationdevice. report also saves uncompressed 44.1 kHz 16 bit PCM WAV sound recording820fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE(a) cicada detected(b) Sounds interesting(c) Possible cicada detectedFigure 13: Three possible outcomes CDA, known insect found, insectcicada found, cicada found.case either cicada another insect found, provided user granted permissionso. Since audio recording requires 2.7 MB disk, deleted smartphone soonreport sent server minimise storage space required smartphone. Last,low-resolution spectrogram saved cases, constructed combination output20 Goertzel filters 30 seconds survey, saved every 128 samples. constitutes easiestway human check presence cicada avoids privacy concerns (speechcould reconstructed spectrogram). Moreover, payload image file,saved Base64 (Josefsson, 2006), around 15 KB therefore much lighter raw soundrecording. Internet connection becomes available, report uploaded projectsservers, available research team analyse further.4.1.4 N C ICADA ETECTEDFigure 13a shows screen shown nothing detected. fact cicada, habitat,New Forest technology behind app shown provide informative notion, encouraginguser try again. intends support morale user receiving negativeresults, provide educational content citizen scientist receives informationexchange work performed.4.1.5 OUNDS NTERESTINGFigure 13b shows screen displayed another insect detected, whose call similarNew Forest cicada. present, app encompasses two insects present NewForest: dark bush-cricket Roesels bush-cricket. user shown spectrogramtypical call insects, well spectrogram recorded,821fiZ ILLI , PARSON , ERRETT & ROGERSDeviceiPhone 4iPhone 5iPhone 4SiPhone 3HTC DesireXperia MiniMoto A953Galaxy S3Xperia ZHTC OneNexus 4HTC Desire XGalaxy Ace 2Galaxy S2Nexus OneHTC One XHTC WildfireOSiOSiOSiOSiOSAndroidAndroidAndroidAndroidAndroidAndroidAndroidAndroidAndroidAndroidAndroidAndroidAndroidFilteredYesYesSilence (SEM)1.623 (0.075)1.897 (0.076)1.466 (0.050)1.469 (0.047)0.844 (0.041)2.480 (0.155)2.015 (0.104)1.374 (0.038)0.951 (0.032)1.466 (0.040)0.675 (0.025)1.243 (0.054)1.953 (0.063)1.916 (0.085)1.514 (0.051)1.933 (0.062)2.032 (0.088)Cicada (SEM)13.047 (0.327)14.793 (0.388)10.549 (0.337)10.539 (0.430)4.255 (0.265)10.190 (0.262)5.845 (0.148)3.279 (0.088)1.971 (0.059)2.915 (0.085)1.314 (0.026)1.817 (0.075)2.162 (0.059)2.101 (0.031)1.568 (0.045)1.732 (0.052)1.683 (0.063)Ratio (SEM)8.041 (0.442)7.800 (0.373)7.196 (0.336)7.173 (0.372)5.041 (0.397)4.109 (0.277)2.901 (0.167)2.387 (0.093)2.072 (0.094)1.988 (0.079)1.946 (0.081)1.462 (0.087)1.107 (0.047)1.097 (0.051)1.036 (0.046)0.896 (0.040)0.828 (0.047)Table 2: Comparison popular smartphone devices. Values means ratios 148 kHz Goertzel filters, sampled every 3 ms (128 samples 44,100 kHz). Standard errormean (SEM) given brackets.asked select insect recording looks similar. promotes involvementuser process, would otherwise passively observing detection performedsmartphone.4.1.6 P OSSIBLE C ICADA ETECTEDFigure 13c shows message informing user discovery cicada. Since algorithmtricked recording actual call, detection presented possible.4.2 Evaluation Microphones Frequency ResponsePrior deployment, noted smartphones equally capable detectingcicada. tests reveal smartphones equipped microphone considerablysensitive others. tested range different devices reproducing four types soundleast 2 seconds each: silence, white noise, frequency sweep 50 20,000 Hz, cicadacall. reproduced custom-built sound-proof chamber, placed quiet location,Visaton KE 25 SC 8 Ohm tweeter producing four test sounds. phones arrangedmicrophone facing speaker equally distant it. experiences recordingcicada calls Slovenia, sound volume calibrated volume cicada callequivalent likely detected wild. synthetic white noise frequency sweeptuned match maximum amplitude cicada call.report comparison sensitivity microphones based well detectcicada call test environment. Table 2 summarises outcome test, reporting822fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCEAmpl.(a)Freq (kHz)2015(b) 10502015(c) 10502015(d) 10502015(e) 105000:0500:10Time (s)00:1500:2000:25Figure 14: Comparison three phones. top, waveform (a) spectrogram (b)sample calibration file. bottom, sensitive iPhone 5 (c), Google Nexus 4 (d)hardware-filtered HTC One X (e), top-end devices iOS Android.ratio 14 kHz 8 kHz bands extracted Goertzel filter soundplayed (marked Silence), cicada call played (Cicada), ratiotwo. higher value latter means clearer indication cicada call, resultsclearer separation log-normal distributions representing sounded unsounded states,therefore greater confidence detection. seen models iPhone capturecall New Forest cicada accurately, Android phones exhibit wide rangeperformance. due operating system itself, rather varied rangehardware specifications common Android devices. Figure 14 shows reference sound playedphone, together three examples high-end devices; Apple iPhone 5, detectscicada call clearly, Nexus 4, detects time, HTC One X,despite sensitive microphone, uses low-pass frequency filter, therefore incapabledetecting insects call. confirmed divergent rank devices score Table 2.4.3 Large-Scale Trial Deploymentsmartphone app launched 8th June 2013 collected data end matingseason New Forest cicada. Since launch, 1000 citizen scientists submitted6000 reports worldwide. these, least 1777 New Forest (over 1600 submittedGPS fix acquired); New Forest reports, 162 classified either soundsinteresting potential cicada detected, result include 30 second audio recording.citizen scientists submitted reports, 738 used iOS version app, 346 usedAndroid version.Figure 15 shows bar graph number reports uploaded top 25 contributors,trend top 100 users displayed top-right corner. noted among these,5 entomologists authors paper. However, users covered specific areasforest, particular cicada historically observed. contrast, citizenscientists submitted much fewer reports per user, reports much evenly distributed823fiiOSAndroidNumber Reports2472002361641001500786354 521Ranking10042 40 38 38 37 37 37 36 36 3433 33 31 30 29 29050Number Reports300306 301250300350Z ILLI , PARSON , ERRETT & ROGERS12345678 910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25RankingFigure 15: Reports per user operating system top 25 users (right, trend top 100).across New Forest, shown Figure 16. shows crucial difference distributedapproach make, entomologists cannot ubiquitously present different areas forestconditions favourable, cover limited territory, visitors, thoughcontributing individually less, help rediscover cicada moved different sites,currently suspected. time, entomologists tools knowledgerecognise insects calls, general public must equipped accessible method.space, implementation deployment automated acoustic insect detection algorithmsucceeded bring public possibility contribute distributed monitoring insectspecies, shown large number downloads app submitted reports.5. Conclusionspaper presented novel algorithm designed specifically detect mating callNew Forest cicada. shown careful analysis call, key featuresextracted minimal cost, greatly simplifying identification process. compared approachthree variants approach, method exists date best knowledgeautomatically classify insects calls constrained platform, mobile phone. threevariants, one uses raw frequency components HMM feature vectors, second variant removessilent periods recording third one classifies time slices independently based uponemission distributions. results show approach achieves accuracy F1 = 0.82detection New Forest cicada data set recordings collected New ForestSlovenia using iPhone Android smartphones. recordings included various formsbackground noise, insects calls human voices. Rather focusing batch processinglarge data sets species, approach focused upon identification small numberspecies real time.824fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCE(a) New Forest(b) UKFigure 16: Map submitted surveys around New Forest across UK. areaNew Forest corresponds green area centre.development robust acoustic classifier complete, integrated technologysmartphone app iOS Android. large-scale deployment resulted collectionleast 1777 reports New Forest, 162 detected call insect interest,1000 citizen scientists. Although New Forest cicada successfully detectedtwo month mating season 2013, use app Slovenia confirmed accuracyacoustic detector deployment New Forest attested suitability using citizenscientists crowdsource collection audio reports via smartphones. app also usedexpert entomologists Slovenia detect presence Cicadetta montana.future work consist second deployment smartphone app twomonth mating season New Forest cicada, aim achieving greater coverageNew Forest, require mobilisation larger community citizen scientistscover areas New Forest yet surveyed. deploymentconstitute largest survey New Forest cicadas habitat date, therefore provideunprecedented insight existence endangered species. Moreover, app classification British Orthoptera also currently development, pose new setchallenges. fact, higher number different calls, selection distinctive featuresHMM becomes difficult, may require sampling higher frequency, increasingcomputational complexity approach. increase accuracy encompass widernumber devices, use techniques cepstral mean normalisation accountdifference sensitivity microphones.Since learning part algorithm completed offline, algorithm remains efficientsolution classify insects calls real time mobile device, may readily extendedcalls different animals, insects birds. Preliminary work extensionstarted, goal adaptive acoustic classifier trained differentsound-emitting wildlife species.825fiZ ILLI , PARSON , ERRETT & ROGERSAcknowledgementsresearch supported EPSRC Doctoral Training Centre grant (EP/G03690X/1)ORCHID Project, www.orchid.ac.uk (EP/I011587/1). Many thanks Dr Tomi TrilarProf Matija Gogala field guidance Slovenia Dr David Chesmore originalsuggestion ongoing support.ReferencesBrenna, B. (2011). Clergymen Abiding Fields: Making Naturalist ObserverEighteenth-Century Norwegian Natural History. Science Context, 24(02), 143166.Chaves, V. A. E., Travieso, C. M., Camacho, A., & Alonso, J. B. (2012). Katydids acoustic classification verification approach based MFCC HMM. Proceedings 16th IEEEInternational Conference Intelligent Engineering Systems (INES), 561566.Chesmore, E. D. (2004). Automated bioacoustic identification species. Anais da AcademiaBrasileira de Ciencias, 76(2), 436440.Chesmore, E. D., & Ohya, E. (2004). Automated identification field-recorded songs fourBritish grasshoppers using bioacoustic signal recognition. Bulletin Entomological Research, 94(04), 319330.Dickinson, J. L., Zuckerberg, B., & Bonter, D. N. (2010). Citizen Science Ecological ResearchTool: Challenges Benefits. Annual Review Ecology, Evolution, Systematics, 41(1),149172.Ghahramani, Z. (2001). Introduction Hidden Markov models Bayesian Networks.Journal Pattern Recognition Artificial Intelligence, Vol. 15, pp. 942.Go-Gulf (2012). Smartphone Users Around World Statistics Facts.http://www.go-gulf.com/blog/smartphone, retrieved 19/07/2012.On-line,Goertzel, G. (1958). algorithm evaluation finite trigonometric series. AmericanMathematical Monthly, 65(1), 3435.Gomes, C. P. (2009). Computational Sustainability: Computational methods sustainable environment, economy, society. Bridge, 39(4), 513.Joint Nature Conservation Committee (2010). UK priority species pages Cicadetta montana (NewForest Cicada). Tech. rep..Jones, K. E., Russ, J., Catto, C., Walters, C., Szodoray-Paradi, A., Szodoray-Paradi, F., Pandourski,E., Pandourski, I., & Pandourski, T. (2009). Monitoring bat biodiversity: indicators sustainable development Eastern Europe Darwin Initiative Final Report. Tech. rep., ZoologicalSociety London.Josefsson, S. (2006). base16, base32, base64 data encodings. RFC 4648, Standards Track.Leqing, Z., & Zhen, Z. (2010). Insect Sound Recognition Based SBC HMM. InternationalConference Intelligent Computation Technology Automation (ICICTA), Changsha,China, Vol. 2, pp. 544 548.MacLeod, N. (2007). Automated Taxon Identification Systematics: Theory, Approaches Applications. CRC Press.826fiA N HMM ACOUSTIC C ICADA ETECTOR C ITIZEN CIENCEMiller-Rushing, A., Primack, R., & Bonney, R. (2012). history public participation ecological research. Frontiers Ecology Environment, 10(6), 285290.Nature Locator (2012).10/04/2014.BatMobile Project.On-line, http://batmobile.blogs.ilrt.org, retrievedNature Locator (2013). iRecord Ladybirds Project. On-line, http://naturelocator.org/ladybird.html,retrieved 10/04/2014.Pinchen, B. J., & Ward, L. K. (2002). history, ecology conservation New ForestCicada. British Wildlife, 13(4), 258266.Pinhas, J., Soroker, V., Hetzoni, A., Mizrach, A., Teicher, M., & Goldberger, J. (2008). Automaticacoustic detection red palm weevil. Computer Electronics Agriculture, 63, 131139.Potamitis, I., Ganchev, T., & Fakotakis, N. (2006). Automatic acoustic identification insectsinspired speaker recognition paradigm.. Interspeech 2006, Pittsburgh, Pennsylvania,pp. 21262129.Quinn, J. A., Frias-Martinez, V., & Subramanian, L. (2014). Computational SustainabilityArtificial Intelligence Developing World. AI Magazine Special Issue ComputationalSustainability.Silvertown, J. (2009). new dawn citizen science.. Trends ecology & evolution, 24(9),46771.Viterbi, A. (1967). Error bounds convolutional codes asymptotically optimum decodingalgorithm. IEEE Transactions Information Theory, 13(2), 260269.Zilli, D., Parson, O., Merrett, G. V., & Rogers, A. (2013). Hidden Markov Model-Based AcousticCicada Detector Crowdsourced Smartphone Biodiversity Monitoring. InternationalJoint Conference Artificial Intelligence, Beijing, China. AAAI Press.827fiJournal Artificial Intelligence Research 51 (2014) 779804Submitted 09/14; published 12/14Research NoteBDD Ordering Heuristics Classical PlanningPeter KissmannJorg HoffmannKISSMANN @ CS . UNI - SAARLAND . DEHOFFMANN @ CS . UNI - SAARLAND . DESaarland University, Saarbrucken, GermanyAbstractSymbolic search using binary decision diagrams (BDDs) often save large amounts memory due concise representation state sets. decisive factor methods successchosen variable ordering. Generally speaking, plausible dependent variablesbrought close together order reduce BDD sizes. planning, variable dependencies typically captured means causal graphs, preceding work taken basisfinding BDD variable orderings. Starting observation two concepts dependency actually quite different, introduce framework assessing strength variableordering heuristics sub-classes planning. turns that, even extremely simple planningtasks, causal graph based variable orders may exponentially worse optimal.Experimental results wide range variable ordering variants corroborate theoreticalfindings. Furthermore, show dynamic reordering much effective reducing BDDsize, cost-effective due prohibitive runtime overhead. exhibit potentialmiddle-ground techniques, running dynamic reordering simple stopping criteria hold.1. IntroductionFinding good variable orderings important task many areas Artificial Intelligence,constraint satisfaction problems (CSPs), SAT, planning (for heuristic search approaches,especially applying symbolic search). many cases, efficient ordering determinedevaluating graphical representation underlying problem. CSPs, example,constraint graph used determine variable ordering backtracking-based approaches.Typical approaches take minimum width (Freuder, 1982), maximum degree, maximum cardinality (Dechter & Meiri, 1989) nodes constraint graph account. alternativeapproach considers bandwidth constraint graph given ordering, maximal distance ordering two nodes adjacent graph; idea findordering minimizes bandwidth (Zabih, 1990).SAT, widely used approach determine variable order conflict-driven clause learning(CDCL) variable state independent decaying sum (VSIDS) (Moskewicz, Madigan, Zhao, Zhang,& Malik, 2001). based weights propositional variables, i.e., oftenvariable occurs clauses. Recently, Rintanen (2012) noted applying SAT solversplanning tasks, different ordering might efficient, giving better coverage typicalbenchmarks international planning competition (IPC). ordering takes structureplanning tasks account, trying support (sub)goals early possible.planning, variable dependencies typically represented causal graph (e.g., Knoblock,1994; Jonsson & Backstrom, 1995; Brafman & Domshlak, 2003; Helmert, 2006), capturing variabledependencies terms co-occurences action descriptions. kind graph turnedc2014AI Access Foundation. rights reserved.fiK ISSMANN & H OFFMANNuseful great variety purposes, including problem decomposition (Knoblock, 1994),system design (Williams & Nayak, 1997), complexity analysis (e.g. Jonsson & Backstrom, 1995;Domshlak & Dinitz, 2001; Brafman & Domshlak, 2003; Katz & Domshlak, 2008; Gimenez &Jonsson, 2008; Chen & Gimenez, 2010), derivation heuristic functions (Helmert, 2004, 2006),search topology analysis (Hoffmann, 2011b, 2011a). purposes here, causal graphsrelevant application derivation variable orderings. done BDDs,return detail below, well merge-and-shrink heuristics (Helmert, Haslum, &Hoffmann, 2007; Helmert, Haslum, Hoffmann, & Nissim, 2014). merge-and-shrink, completevariable ordering corresponds (linear) merging strategy, order variables mergedglobal abstraction. recent extension non-linear merging strategies (Sievers, Wehrle,& Helmert, 2014), order merges instead given tree. merge tree bearssimilarity concept vtrees, used generalization variable orderingssentential decision diagram (SDDs) (Darwiche, 2011). Fan, Muller, Holte (2014) shownefficient merge trees determined means causal graph. so, use MinCuts causal graph, putting two resulting sets variables two different branchesmerge tree recursively continue subgraphs.paper, concerned symbolic search based binary decision diagrams (BDDs)(Bryant, 1986) optimal planning. variable ordering refers order variablesqueried within BDDs, key ingredient practical efficiency approach.planning, much work invested finding good variable orderings, modelchecking, symbolic search originated (McMillan, 1993), many different variable orderingschemes proposed past (e.g., Malik, Wang, Brayton, & Sangiovanni-Vincentelli,1988; Minato, Ishiura, & Yajima, 1990). Again, many based evaluationgraphical representation problem. Often, bringing dependent variables close togetherresults smaller BDDs. straightforwardly applied planning, defining variabledependencies via causal graph. exactly Gamer, state-of-the-art symbolic searchplanner, determines variable ordering (Kissmann & Edelkamp, 2011).starting point investigation feeling discomfort double use worddependency above. causal graphs, dependency means correspondingvariables appear least one common action, changing value one variable may requirechanging variable well. BDDs, hand, represent Boolean functions .many assignments subset P variables immediately determine truth value ,independently value variables, variables P grouped closelytogether. planning, typically represents layer states sharing distance initialstate (forward search) goal (backward search). concept dependence relatesdetermining whether state member layer. What, anything,causal graph dependencies?conclusive answer question, contribute number insightssuggesting two concepts dependence much common. considerissue theoretical practical perspective. theoretical side, introducesimple formal framework assessing strength variable ordering heuristics sub-classesplanning. Applying framework causal graph based variable orders, show mayexponentially worse optimal orderings, even extremely simple planning tasks.practical side, experiment wide range variable ordering schemes, severalones based causal graph, also range techniques adapted model checking780fiBDD RDERING H EURISTICS C LASSICAL P LANNINGliterature. get idea good ordering schemes are, grand scale things,use upper lower delimiter. latter, use random variable orderings.surpisingly, ordering schemes better random; surprisingly, are.Indeed, Fast Downwards level heuristic (Helmert, 2006) turns much worseaverage random BDD variable ordering.upper delimiter, employ dynamic reordering techniques minimize BDD sizeonline, construction process. Compared static up-front variable ordering schemes,reordering much better basis taking decisions, much time-consuming.thus expected BDD size results much better. extent happensexperiments remarkable, however: Static orderings hardly ever even tiny bit better, whereasadvantage dynamic reordering easily frequently goes three orders magnitude.successfully employed least one domain non-deterministic planning(Cimatti, Pistore, Roveri, & Traverso, 2003), dynamic reordering usually prohibitively slowcost-effective. Still, prowess reducing BDD size, combined pessimistic outlookstatic ordering schemes, suggests may better alternative. initial experimentindicates could, indeed, case: simple adaptive stopping criteria, runningdynamic reordering certain point, obtain better results staticordering schemes.remainder paper organized follows. Section 2 gives necessary backgroundplanning framework use BDDs. Section 3 introduces theoretical frameworkinvestigates properties causal graph based ordering schemes range well-knownplanning sub-classes. Section 4 presents experiments regarding quality causal graphbased ordering schemes, Section 5 presents experiments adaptive stopping criteriadynamic reordering. Section 6 concludes paper brief discussion outlook.research note extension authors previous short conference paper (Kissmann& Hoffmann, 2013). present paper contains comprehensive details regarding technicalbackground variable orderings implemented, includes full proofs. experimentsadaptive stopping criteria dynamic reordering, Section 5, new.2. BackgroundBDD-based planning, argued e.g. Edelkamp Helmert (1999), importantsmall encoding given planning task. use finite-domain variable representationbasis investigation. finite-domain representation (FDR) planning task tuple= hV, A, I, Gi, V set state variables v V associatedfinite domain D(v). finite set actions pair hpre , eff partialassignments V pre precondition eff effect action a. initial statecomplete assignment V . goal G partial assignment V . V(pa), partialassignment pa, denote variables v V pa(v) defined.action applicable state iff pre s. resulting successor state s0holds s0 (v) = eff (v) v V(eff ) s0 (v) = s(v) v V \ V(eff ). plansequence actions whose successive application starting initial state results state sgG sg . plan optimal plan shorter length exists.Binary decision diagrams (BDDs) introduced Bryant (1986) represent Boolean functions. BDD directed acyclic graph one root two terminal nodes, 0-sink781fiK ISSMANN & H OFFMANNx1x1x3x2x2x3x301x2x3x30(a) Full OBDD.1(b) Reduced OBDD.Figure 1: Example BDDs function = ((x1 x2 ) x3 ). Dashed arrows denote low edges;solid ones high edges.1-sink. internal node corresponds binary variable p two successors, one followinghigh edge taken p true one following low edge taken p false. assignmentvariables sink reached corresponds value function represented .common practice, use reduced ordered BDDs. ordered BDD (OBDD)BDD ordering binary variables path fixed. reduced OBDDapplies two reduction rules result canonical representation: (i) remove node identicalsuccessor along high low edge; (ii) merge nodes variablesuccessor along high edge successor along low edge. Figure 1 illustratesexample BDDs function = ((x1 x2 ) x3 ) ordering hx1 , x2 , x3 i. Figure 1afull OBDD without reduction. considering nodes x3 , noterightmost one removed due rule (i), three merged due rule (ii).Applying rules preceding layers well, end reduced OBDD Figure 1b.consider BDD-based planning terms symbolic search (McMillan, 1993) implemented Gamer (Kissmann & Edelkamp, 2011). finite-domain variables V FDR taskencoded replacing v V binary counter (v) using dlog2 |D(v)|e bits. taskrepresentable n bits need 2n BDD variables two sets, one set x representing currentstate variables, another set x0 representing successor state variables. action represented transition relation BDD, Ta (x, x0 ), captures changes due applicationalso frame, i.e., variables change:Ta (x, x0 ) = pre (x) eff (x0 ) frame(V \ V(eff ), x, x0 )Wframe(V 0 , x, x0 ) = vV 0 v(x) v(x0 ) modelingW frame. possible create monolithic transition relation actions, i.e., (x, x0 ) = aA Ta (x, x0 ). However, typicallyfeasible terms memory. Thus, store transition relations actions separately(Burch, Clarke, & Long, 1991).order calculate successors set states S, represented current state variables,use image functionimage(S) =_x.(S(x) Ta (x, x0 ))[x0 x].aA782fiBDD RDERING H EURISTICS C LASSICAL P LANNINGconjunction makes sure applicable actions considered, sets correspondingsuccessor state variables. existential quantification removes current state variables.operator [x0 x] stands swapping current successor state variables,end successor states represented current state variables, i.e., newcurrent states. Finally, disjunction ensures successors based actions calculated.case backward search, pre-image calculating predecessors set givensuccessor variables looks similar, successor state variables quantified insteadcurrent state variables.Using two functions, symbolic breadth-first search straightforward: Starting initialstate (or set goal states), iterate image (or pre-image), goal (or initial state)reached. Storing entire set reached states ensure completeness. search,layer L states subset states identical distance initial state (forward search)goal (backward search) represented BDD characteristic function.Based given variable ordering, size BDD, i.e., number nodes neededrepresent corresponding function, differ exponentially, finding good orderingscrucial practice. size also influence runtime (e.g., time memoryrequirements conjunction two BDDs polynomial product sizes twoBDDs), smaller size important terms memory also terms runtime. BDDpackages typically contain dynamic reordering algorithms, reduce BDD sizes basedcurrent situation. However, previous work argued (Kissmann & Edelkamp, 2011),experiments reconfirm, runtime overhead dynamic reordering prohibitiveplanning. alternative use static variable ordering schemes instead. defineschemes functions mapping planning task non-empty set () variable orderings, i.e., orderings planning tasks finite-domain variables V . use set () here,opposed single ordering, variable ordering schemes consider contain ambiguity, i.e., impose constraints final variable ordering opposed fixingunique complete ordering.first BDD created, set possible orderings determined pre-processingstep, actual ordering hv1 , . . . , vn = () chosen arbitrarily (i.e., considerstep here). calculated ordering defined set multi-valued variables. Thus,get final BDD binary variable order replace finite-domain variable vi binarycounter (vi ). means BDD treats counters like inseparable fixed blocks. (Notebits counters represented level planning tasks ,impossible make informed choice separation block.) additionblocks store current successor state variables interleaved fashion (Burch, Clarke,Long, McMillan, & Dill, 1994).layer L ordering planning tasks finite-domain variables, ordered BDDunique. denote size, i.e., number nodes, BDDSize(o, L). BDDSize (L) :=mino BDDSize(o, L) denote size BDD optimal variable ordering. Findingoptimal ordering NP-hard (Bryant, 1986).state art ordering scheme symbolic planning based causal graph CGplanning task (Knoblock, 1994; Domshlak & Dinitz, 2001). CG directed graphnodes V arc (v, v 0 ) iff v 6= v 0 exists action (v, v 0 ) V(eff )V(pre ) V(eff ). words, arc v v 0 appear effectaction v appears precondition action v 0 effect.783fiK ISSMANN & H OFFMANNGamers scheme,denoted ga , maps set orderings = hv1 , . . . , vn minimizePexpression (vi ,vj )CG (i j)2 . idea variables vi , vj adjacent CGdependent brought close together ordering minimizing distance|i j|. bears similarity minimal bandwidth variable ordering CSPs (Zabih,1990), though maximum distances minimized, minimize sum.practice, Gamer approximates ga limited amount local search space orderings,finding optimal solution NP-hard (Kissmann & Edelkamp, 2011). this, starts severalsearches random ordering, swaps two variables checks sum decreased. did,search continues new ordering, otherwise stick old one. end,generated ordering smallest sum used. original hope connectiontwo notions dependency. supported fact new orderingresulted improved coverage used benchmark set compared used before.Apart ga , also consider scheme cg , defined acyclic CG .maps set topological orderings nodes CG . consider theoreticalinterest since straightforward way trust causal graph completely, i.e., takedependencies derived causal graph order BDD variables accordingly.3. Whats Causal Graph: Theorypointed introduction, doubtful whether concept dependencycausal graph real relation concept dependency relevant BDD size.frame terms classification guarantees offered, rather, guarantees offered,ga cg restricted classes planning tasks.first introduce theoretical framework, outline results cg ga .3.1 Classification Frameworkclassify ordering schemes, relative given scalable family planning tasks, follows:Definition 1 (Classification Ordering Schemes). Let F = {n } infinite family FDRplanning tasks parameterized n, size n bounded polynomial n. Let{forward, backward} search direction. variable ordering scheme is:(i) perfect F n F, d-layers L n , (n ),BDDSize(o, L) = BDDSize (L).(ii) safe F exists polynomial p s.t. n F, d-layers L n ,(n ), BDDSize(o, L) p(BDDSize (L)).(iii) viable F exists polynomial p s.t. n F d-layers L n ,exists (n ) BDDSize(o, L) p(BDDSize (L)).words, perfect guarantees deliver optimal orderings, safe guaranteespolynomial overhead, viable always delivers least one good ordering runsrisk super-polynomial overhead. viable, actively deceives planner,sense variable orderings suggested super-polynomially bad task layer.Note interpretation viability generous that, least one good orderingmust delivered, ordering may differ different search directions layers,784fiBDD RDERING H EURISTICS C LASSICAL P LANNINGxx1x2x3x4g1x5(a) Chainsg2g4g3g5(b) ForksG chainx1x2x3x4x5GG forkG dagGG iforkg(d) Relations (arrows mean )(c) Inverted ForksFigure 2: Causal graph special cases relation.disambiguation (n ) left job determining ordering actually goodone. One could define notion strictly, results negative anyhow stickoptimistic version.extend classification arbitrary sub-classes C FDR (whose sizes still boundedpolynomial) worst case families F contained C: C contains least one Fperfect, perfect C; C contains least one F safe,safe C; C contains least one F viable, viable C.interested variable orderings derived causal graph, natural considersub-classes FDR characterized causal graphs. set directed graphs G, FDR(G)denote class FDR planning tasks whose causal graphs elements G. investigatewidely considered causal graph special cases, namely:Chains (G chain ), find order x1 , . . . , xn variablesarcs xi xi+1 1 n 1 (cf. Figure 2a).Forks (G fork ), one variable x, set variables gi , arc xgi (cf. Figure 2b).Inverted forks (G ifork ), set variables xi , one variable g, arcxi g (cf. Figure 2c).Directed acyclic graphs (DAGs, G dag ).simple limiting cases, also consider causal graphs without arcs (G ), well arbitrarycausal graphs (G ). Figure 2d illustrates relations cases considered.Bad cases inherited hierarchy Figure 2d: G G 0 , ordering schemeclassification within FDR(G 0 ) least bad FDR(G), simply culpritworst-case (not-perfect/not-safe/not-viable) family F FDR planning tasks FDR(G)contained FDR(G 0 ) well.3.2 Classification Resultsstart investigation empty causal graphs, i.e., causal graphs arcs:785fiK ISSMANN & H OFFMANN0001000110101111(a) DTG variable x.(b) DTG variable y.Figure 3: DTGs two variables planning task used proof Theorem 1.Theorem 1. search directions, ordering scheme safe FDR(G ). ga cgperfect.Proof. causal graph arcs, variables move independently, i.e., actionmay single variable precondition, variable effect.forward/backward layer distance contains exactly states sum individualdistances (from variables initial value/to variables goal value) equals d. variable vtask, number vertices (more precisely, copies binary counter (v)) needed thusbounded number possible individual-distance sums variables preceding v. HenceBDD size polynomially bounded regardless variable ordering.see ga cg perfect, consider following simple example. designFDR task n uses 2 variables x y, domain size 4, represented values00, 01, 10, 11. forward search, initially x = 00 = 00 holds. x variableaction setting 01 currently 00, another setting 10 00, two setting11 01 10, respectively. variable action setting 01 currently00, another setting 01 10 another setting 10 11. Thus, valuesx variable distances 0, 1, 1, 2, respectively, initial value x,variable distances 0, 1, 2, 3, respectively, ys initial value. Figure 3 illustratesdomain transition graphs (DTGs) variables x y. similar task distancesgoal values defined backward search.variable represented two BDD variables, x0 , x1 y0 , y1 . keep orderwithin x variables fixed, two possible orderings: x vice versa.distance 1 initial (or goal) state, get BDDs illustrated Figure 4: Orderingx results slightly larger BDD. Thus, ga cg , correspond possibleorderings, perfect, concludes proof.Even though schemes ga cg constrain set possible orderings way,Theorem 1 seen good case connection causal graphs BDD orderings:Empty causal graphs entail ordering safe. connection doesnt seem carrytrivial case, though: sub-classes considered, space BDD orderingscontains exponentially bad ones. Indeed, true set BDD orderings,786fiBDD RDERING H EURISTICS C LASSICAL P LANNINGy0x0x1x1y0y0x0x0y1y1x1x10101y1(a) x y.(b) x.Figure 4: BDDs showing orderings perfect proof Theorem 1. Solid arrowsrepresent high edges, dashed ones low edges.x1x1y1x3x2x2x3x3x3y1y1y1y2y2x3y1y2y3y30x201(a) Good variable ordering: hx1 , y1 , x2 , y2 , x3 , y31(b) Bad variable ordering: hx1 , x2 , x3 , y1 , y2 , y3Figure 5: BDDs different variable orderings Q(, ) n = 3: (x1 y1 ) (x2 y2 )(x3 y3 ). Solid arrows denote high edges, dashed ones low edges.also subsets delivered ga cg . classification schemes badalmost considered cases, little bit hope chain causal graphs.negative results employ Boolean functions quadratic form. variables{x1 , y1 , . . . , xn , yn }, take form (x1 oplow y1 )ophi . . . ophi (xn oplow yn ), either ophi{, } oplow = , vice versa. denote functions Q(ophi , oplow ).functions, ordering hx1 , y1 , . . . , xn , yn (i.e., bringing pairs xi yi together) yieldsBDD whose size polynomial n, ordering hx1 , . . . , xn , y1 , . . . , yn (i.e., splittingvariables two blocks, one x one variables) yields BDD exponentialsize. (Wegener, 2000, proves Q(, ) depicted Figure 5; similar arguments applyquadratic forms.)787fiK ISSMANN & H OFFMANNgx1x1y1y1x2gx20gy2y2x3x3y3y3110(a) g front.g(b) g within pair.Figure 6: BDDs representing g Q(, ) different positions g variable. Solid arrowsrepresent high edges, dashed ones low edges.Theorem 2. search directions, ga cg safe FDR(G ifork ).WProof. prove claim backward search, consider function Q(, ) = ni=1 (xi yi ).design FDR task n uses 2n + 1 Boolean variables, {g, x1 , y1 , . . . , xn , yn } includingadditional variable g goal requires true. n actions achieving g,requires pair (xi yi ) true precondition. Clearly, Wn FDR(G ifork ). backwardlayer distance 1 goal characterized g ni=1 (xi yi ).optimal ordering Q(, ) consists pairs (xi , yi ) (yi , xi ). Adding g variable,optimal ordering places either front (as depicted Figure 6a) end. casesrequire exactly one node representing g variable. Placing g variable anywhere else requiresmany nodes representing g nodes (different 0-sink) reached edges passinglayer. case, two g nodes g placed two pairs,three nodes placed two nodes constituting pair (see Figure 6b latter case).ordering following ga (n ) places g middle x variables arbitraryorder around it. ordering following cg (n ) places g end x variablesarbitrary order it. cases, x variables may placed variables, resultingexponential overhead concludes proof backward search.forward search, consider function Q(, ), construct n ,variables {g, x1 , y1 , . . . , xn , yn } domains {x1 , y1 , . . . , xn , yn } ternary:unknown, true (>), false (). x variables initially unknown, set eithertrue false currently unknown. n actions achieving g, exactly above.states initial state distance 2n + 1 x Wvariables either true falsestates exactly satisfy g Q(, ) = g ni=1 (xi = >) (yi = >). causal788fiBDD RDERING H EURISTICS C LASSICAL P LANNINGd1x1dx1dy1y1dy2y2dynynd2x2dx2d3dnxndxndn+1Figure 7: DTG variable z used proof Theorem 3. dashed edges correspondpreconditions changes value corresponding variable.graph remains unchanged, set possible orderings following ga (n ) cg (n ) remainsbackward search well, orders result exponential overheadconcludes proof forward search.Note that, proof construction shows orders possible ga cgsuper-polynomially bad, possible orders good. Hence, claimed prove gacg safe FDR(G ifork ), might case ga cg viable FDR(G ifork ).leave open question.Theorem 3. search directions, ga cg safe FDR(G fork ).VProof. search directions, use function Q(, ) = ni=1 (xi yi ),FDR task n Boolean variables {x1 , y1 , . . . , xn , yn } plus additional variable z domain{d1 , dx1 , dy1 , d2 , dx2 , dy2 , . . . , dn , dxn , dyn , dn+1 }. actions that, 1 n,z move di either dxi dyi , di+1 (see Figure 7). actionpreconditioned dxi achieves xi , action preconditioned dyi achieves yi . Initially, z = d1xi , yi false. goal requires z = dn+1 xi , yi true. forward search,states initial state distance 3n exactly z = dn+1 Q(, ) true,backward search states goal state distance 3n exactly z = d1 Q(, )true.ordering following ga (n ) places z middle x variables arbitrarilyaround it; ordering following cg (n ) places z beginning x variablesarbitrarily it. Thus, constraint variables {x1 , y1 , . . . , xn , yn }, placing789fiK ISSMANN & H OFFMANNx1x2y1x3y2y3gFigure 8: Causal graph planning task used proof Theorem 4.x variables variables ordering compatible schemes, resultsexponential overhead.Again, proof shows ga cg safe, makes statement regarding viability.Note also task proof construction unsolvable. easy modify tasksolvable without breaking proof argument forward search direction. investigate whether true backward search direction well. practice, provingunsolvability traditionally popular objective planning, state space exhaustionone traditional purposes BDDs deemed good for.DAG causal graphs, prove cases orderings admitted gacgsuper-polynomially bad:Theorem 4. search directions, ga cg viable FDR(G dag ).Proof. backward search claim, use combination chain causal graphinverted fork illustrated Figure 8. design FDR task n uses 2n + 1 Booleanvariables, {g, x1 , y1 , . . . , xn , yn }, including variable g goal requires true.n actions achieving g, requires pair (xi yi ) true precondition (thispart task proof Theorem 2). add actions ensuring twoschemes x variables placed variables (or vice versa). One action emptyprecondition sets x1 true effect, another one requires xn true preconditionsets y1 true effect, rest xi1 (or yi1 ) precondition set xi (or yi )true effect. states goal distance 1 thus characterized g Q(, ).order induced ga places g middle, either places x variables increasingorder g variables increasing order g, places variables decreasingorder g x variables decreasing order g. cg induces order startingx variables increasing order, followed variables increasing order, followed g.Thus, cases, x variables placed separately variables, resulting exponentialoverhead proves claim backward search direction.forward search use approach proof Theorem 2, namely extenddomain x variables {true (>), false (), unknown}. x variablesinitialized value unknown. n actions setting g true, requiring pair (xi yi )true. additional actions follows. Two require x1 unknown set truefalse, respectively. Two require xn true y1 unknown set y1 true false,respectively. Two require xn false y1 unknown set y1 true false, respectively.manner four actions xi yi (2 n), requiring xi1 (yi1 )true respectively false, requiring xi (yi ) unknown, setting xi (yi ) true respectivelyfalse. Thus, statesW initial state distance 2n + 1 characterized functiong Q(, ) = g ni=1 (xi = >) (yi = >). variable orders induced ga cgbackward search, resulting exponential overhead, concluding proof.790fiBDD RDERING H EURISTICS C LASSICAL P LANNINGx1y1y2x2y3x3Figure 9: Causal graph planning task used proof Theorem 5.x1x1y1y1x3x2y2y2y2y1y1x2x2x3x3y1x3y1y2y3y3010(a) Optimal orderingx3y1y1y2y2y3y3y1y21(b) Exponential orderingWLFigure 10: BDDs representing 3i=1 yi 3i=1 (xi yi ), used proof Theorem 5. Solidarrows represent high edges, dashed ones low edges.immediately get (recall cg defined acyclic causal graphs):Corollary 1. ga viable FDR(G ).close investigation somewhat positive result chain causal graphs:Theorem 5. search directions, ga cg perfect FDR(G chain ). existsordering scheme viable FDR(G chain ).Proof. first part claim inherited FDR(G ), i.e., corollary Theorem 1.second part claim, existence non-viableWordering scheme, consider firstbackward search direction, using function Q(, ) = ni=1 (xi yi ). design FDRtask n uses 2n Boolean variables, {x1 , y1 , . . . , xn , yn }. goal requires variablesfalse. action without precondition set x1 true, actions preconditions requiringyi1 false setting xi true, actions preconditioned xi true setting yi false.causal graph depicted Figure 9. Clearly, n FDR(G chain ).states distance 1 goal ones except oneLyi false,nsingletrueLnWn yi xi true well. characterized formula i=1 yi Q(, ) =i=1 yi i=1 (xi yi ). easy see exclusive part formula changerelevant properties BDDs quadratic form, i.e., still orderings polynomialorderings exponential number nodes, e.g., placing x variables791fiK ISSMANN & H OFFMANNsafe?G chaintriviallysafeGviableG dagsafeG forkviableGsafeG iforkFigure 11: Overview classification results. hold ga cg , searchdirection.variables (see Figure 10 illustration). ordering scheme including latter orderingsviable.forward search direction case, construct planning task x variablesternary (unknown, true (>), false ()), unknown initially. value x1 setfreely; yi set true false xi true, set true xi false; xi+1set freely yi setV either true false. 2n steps, reach exactlystates characterized Q(, ) = ni=1 (xi = >) (yi = >). BDD representing Q(, )exponential size if, e.g., x variables placed variables, ordering schemeincluding orderings viable.Note that, planning task families {n } described, ga cg forcexi yi variable ordered pairs, resulting BDDs minimal size (see Figure 10a).sense, two planning task families constitute truly positive result: Within them,ordering information causal graph keeps us making exponentially bad mistakes.positive message would much stronger ga cg safe families taskschain causal graphs. remains open question whether so.Figure 11 gives overview results. evidence speaks rather clearly strongconnection causal graph dependencies dependencies relevant BDD size. Notecausal graph underlying Theorem 4 non-viability FDR(G dag ) simpleform combining chain inverted fork, Theorem 2 non-safety FDR(G ifork )relies planning tasks fall known syntactically identified tractable class optimalplanning (Katz & Domshlak, 2010). Note also safe already quite bad practice,incurring exponential risk unless clever way choosing ordering within ()(which, moment, have).4. Whats Causal Graph: Practiceshown poor worst-case performance causal graph based variable ordering schemestheory, practice might another matter. assess latter, implemented comprehensiveset causal graph based variable ordering schemes, comprising 12 schemes total, rancomparison practical good/bad delimiters. bad delimiter, used randomorderings. good delimiter, used off-the-shelf dynamic reordering algorithmGamers BDD package CUDD, based sifting (Rudell, 1993).words order regarding sifting works. variable greatest numbernodes current BDD chosen. first moved towards end ordering,792fiBDD RDERING H EURISTICS C LASSICAL P LANNINGtowards beginning ordering, iteratively swapping position next variablecorresponding direction. positions tried, variable moved positionBDD size smallest. done, next variable chosen, variablesprocessed. better comparability ordering schemes, restrict algorithmkeep variables representing (v) together.previously indicated, dynamic reordering consumes much runtime cost-effective.present experiments, interested BDD size, give dynamic reordering ample runtime. Section 5, identify simple adaptive criteria stopping dynamicreordering automatically search, taking advantage size reduction capacity withoutsuffering much runtime consumption.ran benchmarks 2011 International Planning Competition (IPC11), usedGamer base implementation planners, running one core Intel XeonX5690 CPU 3.47 GHz. Unless otherwise stated, used IPC11 settings, namely timeout30 minutes memory limit 6 GB.4.1 Ordering Schemesran six schemes based directly causal graph:Gamer Gamers original ordering scheme, approximates ga .GamerPre like Gamer causal graph extended arcs pairs preconditionvariables. idea capture dependency forward search, alsobackward search, i.e., inverting actions.WGamer like Gamer arcs weighted number relevant actions, i.e., numberactions inducing corresponding arcs.WGamerPre like GamerPre weighted arcs.CGLevel Fast Downwards (Helmert, 2006) level heuristic, approximates cg . ordersvariables strongly connected components and, within components, considersweighted causal graph orders variables smallest incoming weight first. SimilarWGamer, weights correspond number actions induce arc.CGSons another approximation cg . always selects variable v whose parents already selected; least one whose parents already selected; arbitraryvariable v exists.Additionally, used six ordering schemes adopted model checking literature,based structure called abstract syntax tree (AST) (e.g., Maisonneuve, 2009).directed graph containing root node overall task subtrees actions. subtreeconsists nodes representing subformulas specified action (i.e., subformulasactions precondition effect). variables task leaves AST. leavesmerged, i.e., one node variable task. Edges point noderepresenting function corresponding subtrees.construct AST based PDDL input. Consider following example actions, similar Floortile domain. predicates at(r, t), denoting tile robot r793fiK ISSMANN & H OFFMANNa2a1at(r1 , t1 )painted (t2 )at(r2 , t3 )Figure 12: Example AST.currently painted (t) denoting whether tile already painted. two actionsa1 = paint(r1 , t1 , t2 ) precondition (at(r1 , t1 ) painted (t2 )) effect (painted (t2 )) denoting robot r1 paint tile t2 currently t1 t2 painted. Similarly,action a2 = paint(r2 , t3 , t2 ) precondition (at(r2 , t3 )painted (t2 )) effect (painted (t2 ))denotes robot r2 paint tile t2 currently t3 t2 painted.Figure 12 illustrates corresponding AST. root actions A, one subtreetwo actions a1 a2 . actions preconditions effects encodedretain one copy variable leaves (here relevant painted (t2 )).Using first authors names reference, additional ordering schemes following.Butler (Butler, Ross, Kapur, & Mercer, 1991) extension approach Fujita, Fujisawa,Kawato (1988). latter proposed perform depth-first search (DFS) AST,starting root node, order variables order reachedfirst time. Butler et al. extended setting several roots (if remove overallroot retain subtrees various actions arrive exactly setting).approach starts DFS action containing highest number variables. Withintree advances similar manner: always continues subtree containshighest number variables among subtrees current node. retrieved orderingorder variables reached first time.Chung1 (Chung, Hajj, & Patel, 1993) two-step approach. first step assigns valuesnodes AST. Starting leaves, assigning value 0, assignsinner node maximum values assigned successors plus 1. second stepperforms DFS starting root, guided values nodes, visitingsuccessors highest value first. order variables reached firsttime chosen variable ordering.Chung2 (Chung et al., 1993) determines shortest distance pair variables,calculated considering edges AST undirected. Additionally, totaldistances, i.e., sum minimal distances variables, stored variables. variable smallest total distance chosen first. next one one closest794fiBDD RDERING H EURISTICS C LASSICAL P LANNINGlast variable inserted ordering. case tie distance precedingvariables also taken account.Maisonneuve (Maisonneuve, 2009) greedy approach starting empty sequence.step temporarily extends current sequence variable yet sequence.variable, weight determined, number variables extendedsequence appear action, summed actions. variable removedsequence next one added. weights calculated variablehighest weight appended sequence, next iteration starts, calculating newweights remaining variables. end, last sequence contains variablesthus corresponds variable ordering.Malik (Malik et al., 1988) assigns level value (the maximal level predecessors plus 1)node within AST. root assigned value 0. variables orderedaccording level values, highest values coming first.Minato (Minato et al., 1990) calculates weights nodes AST. weight rootnode action set 1, successors node w/m w nodes weightnumber successors node. One variables highest weightchosen first nodes removed (along ingoing edges recursivelynodes remaining successors). reduced graph weights recalculatedprocedure continues finally variables ordering.4.2 Bad Delimiterget bad delimiter ran 5000 random orderings, ordering correspondsone run IPC11 benchmark tasks, using random variable ordering instance.make feasible used time-out one minute (our backward search implementationviable short time-out, use forward search here). comparisondata, settings (1 minute time-out, forward search) used twelve staticordering schemes. Initially ran ordering schemes random orderings tasks;200 random runs removed tasks benchmark set solved leastprevious (random static ordering) runs, retaining 85 tasks. Figure 13 shows coverage,i.e., number solved planning tasks, x axis, fraction random orderingscoverage axis. coverage data ordering schemes shown vertical lines.Malik CGLevel lie middle Gaussian distribution, respectively.words, Malik bad as, CGLevel even worse than, average random ordering.Matters bleak ten ordering schemes, close together lie clearlyGaussian distribution. Compared best-of random orders, however,ordering schemes appear rather humble. Consider Table 1. particular, consider nr+ , givingnumber instances solved ordering scheme random order, consider n+r,giving number instances solved scheme solved random order.+rTable 1 shows, nr+ strictly smaller n three ordering schemes ,strictly larger (by single task) one schemes (namely Butler). average nr+2.92 n+r9.08.795fiK ISSMANN & H OFFMANNPercentage Random Orderings16CGLevelMalikMaisonneuve+WGamerPreMinato+WGamerCGSonsGamer+GamerPreChung1+Chung2Butler1412108642020304050Coverage607080TypeButlerCGLevelCGSonsChung1Chung2GamerGamerPreMaisonneuveMalikMinatoWGamerWGamerPreBest SchemeFigure 13: Coverage random orders vs. ordering schemes. Schemes ordered top-to-bottomworst best coverage. X +Y means schemes, X , result coverage.nr+n+rn+r+nr327731265352475444752227745673156731311683022576487123772331168361780Table 1: Differences solved instances 85 IPC11 tasks (1 minute timeout); r meanssolved random ordering, +r least one random ordering, solved corresponding ordering scheme, + solved corresponding ordering scheme.4.3 Good Delimiterperformed bidirectional blind search, i.e., competitive setup general. Figure 14contains one data point every pair (I, ) IPC11 benchmark instance ordering schemesolved (a) Gamer using dynamic reordering starting arbitrary variableorder (the one returned Gamers grounding process), (b) Gamer using ordering scheme(without dynamic reordering). time-out 6 hours (a), 30 minutes (b). x-valuedata point size largest BDD constructed (a), y-value sizelargest BDD constructed (b). allowed much higher time-out dynamic reorderingreordering runtime effective: question asking merelytwo methods yields smaller BDDs. Figure 14 shows dynamic reordering universallymuch better this, giving us sizes three orders magnitude smallerschemes. total 1911 instances (solved ordering scheme dynamic reordering),1431 cases BDD sizes smaller factor 10 using dynamic reordering,796fiBDD RDERING H EURISTICS C LASSICAL P LANNINGPeak Size Ordering Schemes108107106105104104105106Peak Size Dynamic Reordering107108DynamicReorderingWGamerPreWGamerMinatoMalikMaisonneuveGamerPreGamerChung2Chung1CGSonsCGLevelButlerFigure 14: BDD size dynamic reordering vs. ordering schemes.DomainBarman4444887444549(5)Elevators 19 19 19 19 19 19 19 19 19 19 19 1919(17)Floortile8878888778887(6)14(12)NoMystery 14 14 14 14 14 13 14 14 15 14 14 14Openstacks 20 18 20 20 20 20 20 19 19 19 20 2020(20)PARC-Printer6656566656678(7)PegSol 17 17 17 17 17 18 18 17 17 17 18 1818(17)Scanalyzer8799899999979(9)Sokoban 17 18 18 17 19 19 19 18 18 18 19 1919(13)Tidybot 167 14 15 159 12 149 15 12816(8)Transport89978889797710(7)9 11 11 11 11 11 11 10 10 11 1112(11)VisitAll 11Woodworking 16 13 10 14 16 16 16 128 16 15 1619(16)Total (260) 164 149 157 161 168 164 167 159 147 164 163 158 180(148)Table 2: Coverage IPC11 tasks. dynamic reordering, numbers parentheses represent coverage 30 minute timeout.406 cases smaller factor 10 100, 20 cases smallerfactor 100.Table 2 shows coverage different schemes IPC11 tasks. makesimilar observation one minute, forward search results, namely CGLevelMalik clearly behind others. last column shows coverage Gamer usingdynamic reordering, provides two numbers, first coverage 6 hours timeout, secondcoverage timeout schemes, i.e., 30 minutes. becomes clearapplying dynamic reordering entire search time feasible practice limitingruntime.797fi6005004003002001000Total RuntimeReordering TimeTransition Relation Creation02468ReorderingsTime (s)Time (s)K ISSMANN & H OFFMANN10121400120010008006004002000Total RuntimeReordering TimeTransition Relation Creation0(a) VisitAll, task 0112468 10Reorderings121416(b) PegSol, task 015Figure 15: Total runtime time spent reordering limited number reordering steps twoexample IPC11 tasks. reorderings vertical line performed transitionrelation creation.5. Limited Dynamic ReorderingGiven much memory-efficient behavior dynamic reordering, possible approachrun dynamic reordering limited time only, hoping get ordering good enoughremainder search. Reordering automatically started number allocatedBDD nodes reaches certain threshold (by default, first threshold 4000 nodes),dynamically adapted reordering (by default, next threshold set 2 times numbernodes reordering). simple way control dynamic reordering limit numberreordering steps, turn dynamic reordering desired number reorderingsperformed.different reordering limits, total runtime task often looks similar situationdepicted Figure 15a. reorderings takes long time solve task duebad initial ordering. Also, first reorderings sometimes hurt, performedbeginning construction transition relation, enough information goodorderings available. However, many reorderings solving takes long time dueimmense overhead reordering time, grows exponentially step.important different behavioral pattern depicted Figure 15b: domains,PegSol Sokoban, minimum curve beginning (without reordering),total runtime increases afterward (mainly based increase reordering time).explanation behavior might initial ordering already pretty good,dynamic reordering cannot improve much overhead incurred vain. also oftenhappens easier tasks domain, learning good setting based simpler tasksseems impossible.Attempting exploit observations design adaptive stopping criteria, geared findinggood point stopping dynamic reordering, given available observations (e.g., numberBDD nodes before/after reordering, reordering times, current total runtimes), experimentedfollowing approaches.First, noticed early reordering time increases step step small factor,later factor increases. preliminary runs saw often area smallestruntime coincides situation increase reordering time reaches threshold,often 1.25 1.75 (see, e.g., Figure 16a compare runtime minimumFigure 15a planning task). call factor criterion.798fiBDD RDERING H EURISTICS C LASSICAL P LANNING2.5FactorPercentageFactor21.510.50024681012Reorderings(a) Factor time last reorderingprevious one.706050403020100Percentage024681012Reorderings(b) Percentage time last reorderingtotal runtime far.Figure 16: Factor percentage criterion limited number reordering steps task 011IPC11 VisitAll domain.Second, another observation preliminary runs percentage time spentlast reordering step total current runtime often follows U-like curve, minimumcurve often lies close number reorderings total runtime minimum (see,e.g., Figure 16b compare runtime minimum Figure 15a task). employpercentage criterion, stops reordering (possibly local) minimum reached,i.e., compare percentage current step previous step; currentone greater stop reordering.Finally, simple combination criteria stop reordering soon one tellsus so.evaluate adaptive stopping criteria, ran tasks IPC11 domains different limits number reorderings, ranging 0 20.1 Based runs calculatedresults would achieve adaptive stopping criteria. See Table 3. best possiblecoverage, i.e., number tasks solved least one setting limited number reorderings,175, without reordering found 167 solutions. adaptive stopping criteria yieldcoverage 158 171. Performance reasonable factor criterion, quite badpercentage criterion combination criteria. Recall percentage criterion aims stopping reordering incurring prohibitive overhead. Indeed, criterion,reordering often stopped earlier factor criterion. cases detrimental,particularly Woodworking domain strategy happens fall dramatic localpeak total-runtime curve, resulting 9 problem instances longer solved.As, several cases, dynamic reordering transition relation creation counterproductive, also ran delayed reordering, dynamic reordering started BDDstransition relation created. results Table 4. case without reorderingunchanged respect Table 3. best possible result slightly worse before,coverage 174. adaptive stopping criteria, picture changes substantially: contrastTable 3, percentage criterion excels, delivering coverage 1 short best possible.Regarding factor criteria, overly small large factors bad best behavior (2 shortbest possible) obtained middle.1. cases highest number reorderings could observe clearly 20. Either planner rantime memory, finished last reorderings could performed. cases used GamerPreinitial ordering, turned among best preliminary set experiments.799fiK ISSMANN & H OFFMANNDomainBarmanElevatorsFloortileNoMysteryOpenstacksPARC-PrinterPegSolScanalyzerSokobanTidybotTransportVisitAllWoodworkingTotalreord719814206189191281116167bestpossible8198162071891914912161751.25819714206179171491110161factor criterion1.5 1.75 2.0888191919888141414202020677171718999171717141414999121212151616168 170 171percentagecriterion819814206179171391171581.2581971420617917139119159criteria1.5 1.758819198814142020661717991717131399111177158 1582.081981420617917139117158Table 3: Coverage results different stopping criteria. Immediate reordering.DomainBarmanElevatorsFloortileNoMysteryOpenstacksPARC-PrinterPegSolScanalyzerSokobanTidybotTransportVisitAllWoodworkingTotalreord719814206189191281116167bestpossible8198162071891913912161741.25819716206179191391210165factor criterion1.5 1.75 2.0887191919888161414202020776171717999191919131313999121212151616172 171 169percentagecriterion8198162071791913912161731.25819716206179191391210165criteria1.5 1.75881919881616202077171799191913139912121516172 1732.0819816207179191391216173Table 4: Coverage results different stopping criteria. Delayed reordering, i.e., reordering startedcreation transition relation BDDs.shed light observations, Figure 17 shows coverage functiondifferent factor values, case immediate reordering (Figure 17a) delayedreordering (Figure 17b). Figure 17a, see percentage criterion stops reorderingearly. Without it, coverage resulting stopping reordering based solely factor criterionget high 173. However, ascend coverage actually starts percentagecriterion stops reordering, thus cutting many solutions. Figure 17b, factor 1.6curves identical, mainly increasing increasing factor. that, combinationcriterion rises another little bit, factor criterion alone drops substantially. combinedcriterion avoids drop because, point (here, factor roughly 2.0), percentagecriterion stops reordering least early factor criterion.800fi180175170165160155150FactorCoverageCoverageBDD RDERING H EURISTICS C LASSICAL P LANNING00.511.52Factor(a) Immediate reordering.2.53180175170165160155150Factor00.511.52Factor(b) Delayed reordering.2.53Figure 17: Coverage function factor, factor criterion alone, combinationpercentage criterion (denoted Both).6. Conclusiontempting equate variable dependencies BDD-based symbolic searchidentified causal graphs, previous research done unquestioningly. Looking littleclosely issue, shown causal graph based variable orderings exponentially badeven severely restricted sub-classes planning. Empirically, Fast Downwards level heuristicworse random, ordering schemes lag far behind off-the-shelf reordering.One may wonder meaning theoretical results: could static orderingscheme incur exponential overhead worst case? agree view principle,expect happen planning tasks restricted tractable domainindependent optimal planning. remains seen extent classification frameworksuitable characterize properties ordering schemes and/or planning fragments.impression point static ordering schemes limited hopeless.Prior actually building BDDs, appears impossible extract reliable informationform take. way forward, then, use dynamic reordering techniquestargeted manner. initial experiments direction meet immediatebreakthrough, certainly show promise, especially considering primitive naturemethod stopping criteria employed. Promising future directions include flexibleon/off strategies dynamic reordering, machine learning deciding toggle switch,planning-specific reordering techniques exploiting particular structure BDDs hand.Acknowledgmentsthank anonymous reviewers ICAPS 2013 short version previous versionarticle, whose comments helped tremendously improve paper.ReferencesBrafman, R., & Domshlak, C. (2003). Structure complexity planning unary operators.Journal Artificial Intelligence Research, 18, 315349.Bryant, R. E. (1986). Graph-based algorithms boolean function manipulation. IEEE Transactions Computers, 35(8), 677691.801fiK ISSMANN & H OFFMANNBurch, J. R., Clarke, E. M., & Long, D. E. (1991). Symbolic model checking partitionedtransition relations. Halaas, A., & Denyer, P. B. (Eds.), Proceedings InternationalConference Large Scale Integration (VLSI-91), Vol. A-1 IFIP Transactions, pp.4958, Edinburgh, Scotland. North-Holland.Burch, J. R., Clarke, E. M., Long, D. E., McMillan, K. L., & Dill, D. L. (1994). Symbolic modelchecking sequential circuit verification. IEEE Transactions Computer-Aided DesignIntegrated Circuits Systems, 13(4), 401424.Butler, K. M., Ross, D. E., Kapur, R., & Mercer, M. R. (1991). Heuristics compute variableorderings efficient manipulation ordered binary decision diagrams. Proceedings28th Conference Design Automation (DAC-91), pp. 417420, San Francisco, CA,USA. ACM.Chen, H., & Gimenez, O. (2010). Causal graphs structurally restricted planning. JournalComputer System Sciences, 76(7), 579592.Chung, P.-Y., Hajj, I. N., & Patel, J. H. (1993). Efficient variable ordering heuristics sharedROBDD. Proceedings 1993 IEEE International Symposium Circuits Systems(ISCAS-93), pp. 16901693, Chicago, IL, USA. IEEE.Cimatti, A., Pistore, M., Roveri, M., & Traverso, P. (2003). Weak, strong, strong cyclic planningvia symbolic model checking. Artificial Intelligence, 147(12), 3584.Darwiche, A. (2011). SDD: new canonical representation propositional knowledge bases.Walsh, T. (Ed.), Proceedings 22nd International Joint Conference Artificial Intelligence (IJCAI11), pp. 819826. AAAI Press/IJCAI.Dechter, R., & Meiri, I. (1989). Experimental evaluation preprocessing techniques constraintsatisfaction problems. Sridharan, N. S. (Ed.), Proceedings 11th International JointConference Artificial Intelligence (IJCAI-89), pp. 271277, Detroit, MI. Morgan Kaufmann.Domshlak, C., & Dinitz, Y. (2001). Multi-agent offline coordination: Structure complexity.Cesta, A., & Borrajo, D. (Eds.), Recent Advances AI Planning. 6th European ConferencePlanning (ECP-01), Lecture Notes Artificial Intelligence, pp. 3443, Toledo, Spain.Springer-Verlag.Edelkamp, S., & Helmert, M. (1999). Exhibiting knowledge planning problems minimizestate encoding length. Biundo, S., & Fox, M. (Eds.), Recent Advances AI Planning.5th European Conference Planning (ECP99), Lecture Notes Artificial Intelligence, pp.135147, Durham, UK. Springer-Verlag.Fan, G., Muller, M., & Holte, R. (2014). Non-linear merging strategies merge-and-shrink basedvariable interactions. Edelkamp, S., & Bartak, R. (Eds.), Proceedings 7th AnnualSymposium Combinatorial Search (SOCS14). AAAI Press.Freuder, E. C. (1982). sufficient condition backtrack-free search. Journal AssociationComputing Machinery, 29(1), 2432.Fujita, M., Fujisawa, H., & Kawato, N. (1988). Evaluation improvements boolean comparison method based binary decision diagrams. Proceedings 1988 InternationalConference Computer-Aided Design (ICCAD-98), pp. 25. IEEE Computer Society Press.802fiBDD RDERING H EURISTICS C LASSICAL P LANNINGGimenez, O., & Jonsson, A. (2008). complexity planning problems simple causalgraphs. Journal Artificial Intelligence Research, 31, 319351.Helmert, M. (2004). planning heuristic based causal graph analysis. Koenig, S., Zilberstein,S., & Koehler, J. (Eds.), Proceedings 14th International Conference AutomatedPlanning Scheduling (ICAPS04), pp. 161170, Whistler, Canada. Morgan Kaufmann.Helmert, M. (2006). Fast Downward planning system. Journal Artificial Intelligence Research, 26, 191246.Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics optimal sequential planning. Boddy, M., Fox, M., & Thiebaux, S. (Eds.), Proceedings 17thInternational Conference Automated Planning Scheduling (ICAPS07), pp. 176183,Providence, Rhode Island, USA. Morgan Kaufmann.Helmert, M., Haslum, P., Hoffmann, J., & Nissim, R. (2014). Merge & shrink abstraction: methodgenerating lower bounds factored state spaces. Journal Association Computing Machinery, 61(3).Hoffmann, J. (2011a). Analyzing search topology without running search: connectioncausal graphs h+ . Journal Artificial Intelligence Research, 41, 155229.Hoffmann, J. (2011b). ignoring delete lists works, part II: Causal graphs. Bacchus, F.,Domshlak, C., Edelkamp, S., & Helmert, M. (Eds.), Proceedings 21st InternationalConference Automated Planning Scheduling (ICAPS11), pp. 98105. AAAI Press.Jonsson, P., & Backstrom, C. (1995). Incremental planning. European Workshop Planning.Katz, M., & Domshlak, C. (2008). New islands tractability cost-optimal planning. JournalArtificial Intelligence Research, 32, 203288.Katz, M., & Domshlak, C. (2010). Implicit abstraction heuristics. Journal Artificial IntelligenceResearch, 39, 51126.Kissmann, P., & Edelkamp, S. (2011). Improving cost-optimal domain-independent symbolic planning. Burgard, W., & Roth, D. (Eds.), Proceedings 25th National ConferenceAmerican Association Artificial Intelligence (AAAI-11), pp. 992997, San Francisco, CA,USA. AAAI Press.Kissmann, P., & Hoffmann, J. (2013). Whats BDD? causal graphs variable orders planning. Borrajo, D., Fratini, S., Kambhampati, S., & Oddi, A. (Eds.), Proceedings23rd International Conference Automated Planning Scheduling (ICAPS13), pp.327331, Rome, Italy. AAAI Press.Knoblock, C. (1994). Automatically generating abstractions planning. Artificial Intelligence,68(2), 243302.Maisonneuve, V. (2009). Automatic heuristic-based generation MTBDD variable orderingsPRISM models. Internship report, Oxford University Computing Laboratory.Malik, S., Wang, A., Brayton, R., & Sangiovanni-Vincentelli, A. (1988). Logic verification usingbinary decision diagrams logic synthesis environment. Proceedings 1988 International Conference Computer-Aided Design (ICCAD-98), pp. 69. IEEE ComputerSociety Press.803fiK ISSMANN & H OFFMANNMcMillan, K. L. (1993). Symbolic Model Checking. Kluwer Academic Publishers.Minato, S., Ishiura, N., & Yajima, S. (1990). Shared binary decision diagram attributed edgesefficient boolean function manipulation. Proceedings 27th ACM/IEEE DesignAutomation Conference (DAC-90), pp. 5257, Orlando, FL, USA. IEEE Computer SocietyPress.Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineeringefficient SAT solver. Proceedings 38th Conference Design Automation (DAC01), Las Vegas, Nevada, USA. IEEE Computer Society.Rintanen, J. (2012). Planning satisfiability: Heuristics. Artificial Intelligence, 193, 4586.Rudell, R. (1993). Dynamic variable ordering ordered binary decision diagrams. Lightner,M. R., & Jess, J. A. G. (Eds.), Proceedings 1993 IEEE/ACM International ConferenceComputer-Aided Design (ICCAD-93), pp. 4247, Santa Clara, CA, USA. IEEE ComputerSociety.Sievers, S., Wehrle, M., & Helmert, M. (2014). Generalized label reduction merge-and-shrinkheuristics. Proceedings 28th AAAI Conference Artificial Intelligence (AAAI14),Quebec City, Quebec, Canada. AAAI Press.Wegener, I. (2000). Branching Programs Binary Decision Diagrams. SIAM.Williams, B. C., & Nayak, P. P. (1997). reactive planner model-based executive. Pollack,M. (Ed.), Proceedings 15th International Joint Conference Artificial Intelligence(IJCAI-97), pp. 11781185, Nagoya, Japan. Morgan Kaufmann.Zabih, R. (1990). applications graph bandwidth constraint satisfaction problems.Proceedings 8th National Conference American Association Artificial Intelligence (AAAI-90), pp. 4651, Boston, MA. MIT Press.804fiJournal Artificial Intelligence Research 51 (2014)Submitted 04/14; published 10/14Verification Agent-Based Artifact SystemsFrancesco BelardinelliBELARDINELLI @ IBISC . FRLaboratoire Ibisc, Universite dEvry, FranceAlessio Lomuscio. LOMUSCIO @ IMPERIAL . AC . UKDepartment Computing, Imperial College London, UKFabio PatriziFABIO . PATRIZI @ DIS . UNIROMA 1.Dipartimento di Ingegneria Informatica,Automatica e Gestionale A. RubertiUniversita di Roma La Sapienza, ItalyAbstractArtifact systems novel paradigm specifying implementing business processes described terms interacting modules called artifacts. Artifacts consist data lifecycles, accounting respectively relational structure artifacts states possible evolutionstime. paper put forward artifact-centric multi-agent systems, novel formalisationartifact systems context multi-agent systems operating them. Differentlyusual process-based models services, give semantics explicitly accounts datastructures artifact systems defined.study model checking problem artifact-centric multi-agent systems specifications expressed quantified version temporal-epistemic logic expressing knowledgeagents exchange. begin noting problem undecidable general.identify noteworthy class systems admit bisimilar, finite abstractions. followsverify systems investigating finite abstractions; also show corresponding model checking problem EXPSPACE-complete. introduce artifact-centricprograms, compact declarative representations programs governing artifact system agents. show that, principle generate infinite-state systems,natural conditions verification problem solved finite abstractions effectively computed programs. exemplify theoretical results pursuedmainstream procurement scenario artifact systems literature.1. IntroductionMuch work area reasoning knowledge involves development formaltechniques representation epistemic properties rational actors, agents, multiagent system (MAS). approaches based modal logic often rooted interpreted systems (Parikh & Ramanujam, 1985), computationally grounded semantics (Wooldridge, 2000)used interpretation several temporal-epistemic logics. line research thoroughly explored 1990s leading significant body work (Fagin, Halpern, Moses, &Vardi, 1995; Meyer & van der Hoek, 1995). recent topic interest developmentautomatic techniques, including model checking (Clarke, Grumberg, & Peled, 1999),verification temporal-epistemic specifications autonomous agents MAS (Gammie &van der Meyden, 2004; Lomuscio, Qu, & Raimondi, 2009; Kacprzak, Nabialek, Niewiadomski,Penczek, Polrola, Szreter, Wozna, & Zbrzezny, 2008). led developments numberareas traditionally outside artificial intelligence, knowledge representation MAS, includingc2014AI Access Foundation. rights reserved.fiB ELARDINELLI , L OMUSCIO & PATRIZIsecurity (Dechesne & Wang, 2010; Ciobaca, Delaune, & Kremer, 2012), web-services (Lomuscio,Penczek, Solanki, & Szreter, 2011) cache-coherence protocols hardware design (Baukus &van der Meyden, 2004). ambition present paper offer similar change perspectivearea artifact systems (Cohn & Hull, 2009), growing topic Service-Oriented Computing(SOC).Artifacts structures combine data process holistic manner basic buildingblock[s] (Cohn & Hull, 2009) systems descriptions. Artifact systems services constitutedcomplex workflow schemes based artifacts agents interact with. data componentgiven relational databases underpinning artifacts system, whereas workflowsdescribed lifecycles associated artifact schema. standard serviceparadigm services made public exposing process interfaces, artifact systemsdata structures lifecycles advertised. Services composed hub operations artifacts executed. Implementations artifact systems, IBM engineBARCELONA (Heath et al., 2013), provide hub service choreography service orchestration (Alonso, Casati, Kuno, & Machiraju, 2004) carried out.Artifact systems beginning drive new application areas, case management systems (Marin, Hull, & Vaculn, 2013). However, identify two shortcomings present stateof-the-art. Firstly, artifact systems literature (Bhattacharya, Gerede, Hull, Liu, & Su, 2007;Deutsch, Hull, Patrizi, & Vianu, 2009; Hull, 2008; Nooijen, Fahland, & Dongen, 2013) focusesexclusively artifacts themselves. obviously need model implementartifact infrastructure, order able reason comprehensively artifact systems,also need account agents implementing services system, normallydone area reasoning services (Baresi, Bianculli, Ghezzi, Guinea, & Spoletini, 2007).Secondly, pressing demand provide hub automatic choreography orchestration capabilities. well-known choreography techniques leveraged automaticmodel checking techniques; orchestration recast synthesis problem, which, turn,also benefit model checking technology. However, model checking applicationsrelatively well-understood plain process-based modelling, presence data makesproblems much harder virtually unexplored. Additionally, infinite domains underlyingdatabases lead infinite state-spaces undecidability model checking problem.aim paper make concerted contribution problems above. Firstly,provide computationally grounded semantics systems comprising artifact infrastructureagents operating it. use semantics interpret temporal-epistemic languagefirst-order quantifiers reason evolution hub well knowledgeagents presence evolving, structured data. observe model checking problemstructures undecidable general analyse notable decidable fragment.derive finite abstractions infinite-state artifact systems, thereby presenting techniqueeffective verification. evaluate methodology studying computational complexitydemonstrating use well-known scenario artifact systems literature.1.1 Artifact-Centric SystemsService-oriented computing concerned study development distributed applications automatically discovered composed means remote interfaces. pointdistinction traditional distributed systems interoperability connectedness ser-334fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSvices shared format data remote procedure calls. Two technology-independentconcepts permeate service-oriented literature: orchestration choreography (Alonso et al.,2004; Singh & Huhns, 2005). Orchestration involves ordering actions possibly differentservices, facilitated controller orchestrator, achieve certain overall goal. Choreography concerns distributed coordination different actions publicly observable eventsachieve certain goal. MAS perspective (Wooldridge, 2001) known particularly helpfulservice-oriented computing allows us ascribe information states private common goals various services. view agents system implement servicesinteract one another shared infrastructure environment.key theoretical problem SOC devise effective mechanisms verify service composition correct specification. Techniques based model checking (Clarke et al.,1999) synthesis (Berardi, Cheikh, Giacomo, & Patrizi, 2008) put forward solvecomposition orchestration problem services described advertised interface levelfinite state machines (Calvanese, De Giacomo, Lenzerini, Mecella, & Patrizi, 2008).recently, attention turned services described languages WS-BPEL (Alves et al.,2007), provide potentially unbounded variables description service process.Again, model checking approaches successfully used verify complex service compositions (Bertoli, Pistore, & Traverso, 2010; Lomuscio, Qu, & Solanki, 2012).WS-BPEL provides model services variables, data referencednon-permanent. area data-centric workflows (Hull et al., 2009; Nigam & Caswell, 2003)evolved attempt provide support permanent data, typically present form underlying databases. Although usually abstracted away, permanent data central importanceservices, typically query data sources driven answers obtain; see, e.g.,(Berardi, Calvanese, De Giacomo, Hull, & Mecella, 2005). Therefore, faithful model service behaviour cannot, general, disregard component. response this, proposalsmade workflows service communities terms declarative specifications datacentric services advertised automatic discovery composition. artifact-centricapproach (Cohn & Hull, 2009) one leading emerging paradigms area. Artifactcentric systems presented along four dimensions (Hull, 2008; Hull et al., 2011).Artifacts holders structured information available system. businessoriented scenario may include purchase orders, invoices, payment records, etc. Artifacts maycreated, amended, destroyed run time; however, abstract artifact schemas provideddesign time define structure artifacts manipulated system. Intuitively,external events cause changes system, including value artifact attributes.evolution artifacts governed lifecycles. capture changes artifactmay go creation deletion. Intuitively, purchase order may created, amendedoperated fulfilled existence system terminated: lifecycle associatedpurchase order artifact formalises transitions.Services seen actors operating artifact system. represent humansoftware actors, possibly distributed, generate events artifact system. services mayartifacts, artifacts may shared several services. However, artifacts,parts artifacts, visible services. Views windows respectively determine partsartifacts artifact instances visible service. artifact hub systemmaintains artifact system processes events generated services.335fiB ELARDINELLI , L OMUSCIO & PATRIZIServices generate events artifact system according associations. Typicallydeclarative descriptions providing precondition post-conditions generation events.generate changes artifact system according artifact lifecycles. Events processed well-defined semantics (Damaggio, Hull, & Vaculn, 2011; Hull et al., 2011) governs sequence changes artifact system may undertake upon consumption event.semantics, based use Prerequisite-Antecedent-Consequent (PAC) rules, ensures acyclicity full determinism updates artifact system. GSM declarative languageused describe artifact systems. BARCELONA engine executes GSM-based artifactsystems (Heath et al., 2013).partial incomplete description artifact paradigm. referliterature details (Cohn & Hull, 2009; Hull, 2008; Hull et al., 2011).clear next section, line agent-based approach services,use agent-based concepts model services. artifact system represented environment, constituted evolving databases, upon agents operate; lifecycles associationsmodelled local global transition functions. model intended incorporateartifact-related concepts including views windows.view paper address following questions. givetransition-based semantics artifacts agents operating them? languageuse specify properties agents artifacts themselves? verify whetherartifact system satisfies certain properties? shown undecidable,find suitable fragments question always answered? so, resultingcomplexity? provide declarative specifications agent programsverified model checking? Lastly, technique used mainstream scenariosSOC literature?1.2 Related Workstated above, virtually current literature artifact-centric systems focuses propertiesimplementations artifact systems such. Little attention given actorssystem, whether human artificial agents. formal techniques have, however,put forward verify core, non-agent aspects system; following briefly comparecontribution.knowledge verification artifact-centric business processes first discussedBhattacharya et al. (2007), reachability deadlocks phrased context artifactcentric systems complexity results verification problem given. Even disregardingagent-related aspects investigated, present contribution differs markedly workemploying expressive specification language putting forward effective abstractionprocedures verification.Gerede Su (2007) study verification technique artifact-centric systems variantcomputation-tree logic. decidability verification problem proven languageconsidered assumption interpretation domain bounded. Decidability alsoshown unbounded case making restrictions values quantified variablesrange over. work presented also work unbounded domains, requirerestrictions present therein: insist fact number distinct valuessystem exceed given threshold point run. importantly, interplay336fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSquantification modalities considered allows us bind use variables different states. major difference feature expressive known lead potentiallyundecidability.related line research verification problem artifact systems two variantsfirst-order linear-time temporal logic considered (Deutsch et al., 2009; Damaggio, Deutsch, &Vianu, 2012). Decidability verification problem retained imposing syntactic restrictionssystem description specification check. effectively limits waynew values introduced every computational step used system. Propertiesbased arithmetic operators also considered (Damaggio et al., 2012). elementssimilarity approaches one put forward here, including factconcrete interpretation domain replaced abstract one, also significant differences.Firstly, setting branching-time linear-time thereby resulting different expressivepower. Secondly importantly, differently similar contributions (Deutsch et al., 2009;Damaggio et al., 2012), impose constraints nested quantifiers interactiontemporal modalities. contrast, Damaggio et al. admit guarded form quantificationstate formulas, universal quantification outermost syntactic level formula,free variables state formulas. two restrictions represent major, crucial differencerespect present work, former syntactical restrictions prevent representinginteraction data different states, instead expressible present work.branching time setting also requires different abstraction technique. Indeed, approach(Deutsch et al., 2009; Damaggio et al., 2012) based construction counterexampleformula checked, fact technically made possible two key factors: (i)exclusive use universal quantification paths, guaranteed use linear-time logics;(ii) syntactic restriction quantifiers values, permits universal quantifiersinclude temporal modalities within scope. None features required work.Namely, allow existential universal quantification paths present (althoughCTL fashion), put restriction use first-order quantifiers. Additionally,abstraction results present given general terms semantics declarativeprograms depend particular presentation system.Finally, following approach similar ours, Bagheri Hariri et al. (2013) give conditionsdecidability model checking problem data-centric dynamic systems, i.e., dynamicsystems relational states. case specification language used first-order version-calculus. temporal fragment subsumed -calculus, two specificationlanguages different expressive power, since use indexed epistemic modalities wellcommon knowledge operator. retain decidability, like here, authors assume constraintsize states. However, differently contribution, Bagheri Hariri et al. alsoassume limited forms quantification whereby individuals persisting system evolutionquantified over. make restriction here.Irrespective above, important feature characterises workset-up entirely based epistemic logic multi-agent systems. use agents representautonomous services operating system agent-based concepts play key rolemodelling, specifications, verification techniques put forward. Differentlyapproaches presented concerned whether artifact system meetsparticular specification. Instead, also wish consider knowledge agentssystem acquire interacting among artifact system system run.337fiB ELARDINELLI , L OMUSCIO & PATRIZIAdditionally, abstraction methodology put forward modular respect agentssystem, is, first define abstract agents compose together obtainabstract system. features enable us give constructive procedures generationfinite abstractions artifact-centric programs associated infinite models. awarework literature tackling aspects.paper combines expands preliminary results artifact-centric systems (Belardinelli, Lomuscio, & Patrizi, 2011a, 2011b, 2012a, 2012b). particular, technical setartifacts agents different preliminary studies makes naturalexpress artifact-centric concepts views. Differently previous attempts,incorporate operator common knowledge provide constructive methods define abstractions. also consider complexity verification problem, previously unexplored,evaluate technique detail case study.1.3 Scheme Paperrest paper organised follows. Section 2 introduce artifact-centric multiagent systems (AC-MAS), semantics using throughout paper describe agentsoperating artifact system. section put forward FO-CTLK, first-order logicknowledge time reason evolution knowledge agentsartifact system. enables us propose satisfaction relation based notion boundedquantification, define model checking problem, highlight properties isomorphicstates. immediate result explore concerns undecidability model checkingproblem AC-MAS general setting.Section 3 devoted identifying subclass AC-MAS admits decidable model checking problem full FO-CTLK specifications. key finding bounded uniformAC-MAS, class identified studying strong bisimulation relation, admit finite, truth-preservingabstractions FO-CTLK specification. Section 3.4 explore verification problem also investigate complexity thereby showing EXPSPACE-complete.turn attention artifact programs Section 4 defining concept artifact-centricprograms. define natural, first-order preconditions post-conditions lineartifact-centric approach. give semantics terms AC-MAS showgenerated models precisely uniform AC-MAS studied earlier paper. follows that,boundedness conditions naturally expressed, model checking problemartifact-centric programs decidable executed finite models.Section 4.2 reports scenario artifact systems literature. used exemplifytechnique providing finite abstractions effectively verified. conclude Section 5consider limitations approach point work.2. Artifact-Centric Multi-agent Systemssection formalise artifact-centric systems state verification problem. datadatabases equally important constituents artifact systems, formalisation artifactsrelies underpinning concepts. However, discussed previous section,give prominence agent-based concepts. such, define systems comprisingartifacts agents interacting it.338fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSstandard paradigm logic-based reasoning agents interpreted systems (Parikh &Ramanujam, 1985; Fagin et al., 1995). setting agents endowed private local statesevolve performing actions according individual protocol. data play key part,well allow us specify properties artifact system, define agents local statesevolving database instances. call formalisation artifact-centric multi-agent systems (ACMAS). AC-MAS enable us represent naturally concisely concepts much used artifactparadigm one view discussed earlier.specification language include temporal-epistemic logic also quantificationdomain represent data. usual verification setting, formally definemodel checking problem set up.2.1 Databases First-Order Logicdiscussed above, use databases basic building blocks defining statesagents artifact system. fix notation terminology used. referliterature details databases (Abiteboul, Hull, & Vianu, 1995).Definition 2.1 (Database Schemas) (relational) database schema set {P1 /q1 , . . . , Pn /qn }relation symbols Pi , associated arity qi N.Instances database schemas defined interpretation domains, i.e., sets individuals.Definition 2.2 (Database Instances) Given countable interpretation domain U databaseschema D, D-instance U mapping associating relation symbol Pifinite qi -ary relation U , i.e., D(Pi ) U qi .set D-instances countable interpretation domain U denoted D(U ).simply refer instances whenever database schema clear context. activedomain instance D, denoted adom(D), set individuals U occurringtuple predicate interpretation D(Pi ). Observe that, since contains finite numberrelation symbols D(Pi ) finite, adom(D). Also, rest paper assumeinterpretation domains always countable without explictly mentioning fact.fix notation, recall syntax first-order formulas equality functionsymbols. Let Var countable set individual variables Con finite set individualconstants. term element Var Con.Definition 2.3 (FO-formulas D) Given database schema D, formulas firstorder language LD defined following BNF grammar:::= = t0 | Pi (t1 , . . . , tqi ) | | | xPi D, t1 , . . . , tqi qi -tuple terms t, t0 terms.assume = special binary predicate fixed obvious interpretation. summarise,LD first-order language equality relational vocabulary function symbolsfinitely many constant symbols Con. Observe considering finite set constantslimitation. Indeed, since working finite sets formulas, Con alwaysdefined able express formula interest.339fiB ELARDINELLI , L OMUSCIO & PATRIZIfollowing use standard abbreviations , , , 6=. Also, free boundvariables defined standard. formula denote set variables var(),set free variables free(), set constants con(). write (~x) listexplicitly arbitrary order free variables x1 , . . . , x` . slight abuse notation,treat ~x set, thus write ~x = free(). sentence formula free variables.Given interpretation domain U suchthat Con U , assignment function: Var 7 U .assignment , denote ux assignment that: (i) ux (x) = u; (ii)ux (x0 ) = (x0 ), every x0 Var different x. convenience, extend assignmentsconstants (t) = t, Con; is, assume Herbrand interpretation constants.define semantics LD .Definition 2.4 (Satisfaction FO-formulas) Given D-instance D, assignment ,FO-formula LD , inductively define whether satisfies , written (D, ) |= ,follows:(D, ) |= Pi (t1 , . . . , tqi )(D, ) |= = t0(D, ) |=(D, ) |=(D, ) |= xiffiffiffiffiffh(t1 ), . . . , (tqi )i D(Pi )(t) = (t0 )case (D, ) |=(D, ) |= (D, ) |=u adom(D), (D, ux ) |=formula true D, written |= , iff (D, ) |= , assignments .Observe adopt active-domain semantics, is, quantified variables rangeactive domain D. claim form quantification sufficient express specificationsinterest (see Section 4.2) retaining decidability. Also notice constants interpretedrigidly; so, two constants equal syntactically same. restpaper, assume every interpretation domain includes Con. Also, usual shortcut, write(D, ) 6|= express case (D, ) |= .Finally, introduce operator D-instances used later paper. Letprimed version database schema schema D0 = {P10 /q1 , . . . , Pn0 /qn } obtainedsyntactically replacing predicate symbol Pi primed version Pi0 arity.Definition 2.5 ( Operator) Given two D-instances D0 , define D0 (D D0 )instance D0 (Pi ) = D(Pi ) D0 (Pi0 ) = D0 (Pi ).Intuitively, operator defines disjunctive join two instances, relation symbolsinterpreted according D, primed versions interpreted according D0 .2.2 Artifact-Centric Multi-agent Systemsfollowing introduce semantic structures use throughout paper.define artifact-centric multi-agent system system comprising environment representingartifacts finite set agents interacting environment. agents viewsartifact state, i.e., projections status particular artifacts, assume building blocksprivate local states also modelled database instances. line interpretedsystems semantics (Fagin et al., 1995) everything agents states needs presentenvironment; portion may entirely private replicated agents states. So,start introducing notion agent.340fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSDefinition 2.6 (Agent) Given interpretation domain U , agent tuple = hD, Act, P ri,where:local database schema;Act finite set action types (~p), p~ tuple abstract parameters;P r : D(U ) 7 2Act(U ) local protocol function, Act(U ) set groundactions form (~u) (~p) Act ~u U |~p| tuple ground parameters.Intuitively, given time agent local state l D(U ) representsinformation agent disposal. sense follow standard approach multiagent systems (Fagin et al., 1995), require information structured database.Again, following standard literature assume agents autonomous proactiveperform actions Act according protocol function P r, returns set groundedactions enabled local state. definition use term abstract parametersdenote variables, i.e., language particular action parameters given; use termground parameters refer concrete values.assume agents interact among environment comprisingartifacts system. artifacts entities involving data processes, seecollections database instances paired actions governed special protocols.Without loss generality assume environment state single database instanceincluding artifacts system. purely formal point view allows us representenvironment special agent. course, specific instantiation environmentagents different entities, exactly line standard propositional version interpretedsystems.therefore define synchronous composition agents environment.Definition 2.7 (Artifact-Centric Multi-agent Systems) Given interpretation domain Uset Ag = {A0 , . . . , } agents Ai = hDi , Acti , P ri defined U , artifact-centric multiagent system (or AC-MAS) tuple P = hAg, s0 , where:Qs0 Ai Ag Di (U ) initial global state;QQAi Ag Di (U ):global transition function,Ai Ag Di (U ) Act(U ) 7 2Act(U ) = Act0 (U ) Actn (U ) set global (ground) actions, (hl0 , . . . , ln i,h0 (~u0 ), . . . , n (~un )i) defined whenever (~ui ) P ri (li ) every n.see later sections, AC-MAS natural extension interpreted systemsfirst order account environments constituted artifact-centric systems. seenspecialisation quantified interpreted systems (Belardinelli & Lomuscio, 2012), generalextension interpreted systems first-order case.formalisation agent A0 typically referred environmentE. environment normally includes artifacts system (notably assuming D0 0<in Dn ),well additional information facilitate communicationagents hub, e.g.,messages transit etc. follows consider D0 = 0<in Dn simplicity; modellingchoice impact results presented later on. given time AC-MAS describedtuple database instances, representing agents system well artifact341fiB ELARDINELLI , L OMUSCIO & PATRIZIsystem. single interpretation domain database schemas given. Notebreak generality representation always extend domain agentsenvironment composing single AC-MAS. global transition function definesevolution system synchronous composition actions environmentagents system.Much interaction interested modelling involves message exchanges payload, hence action parameters, agents environment, i.e., agents operatingartifacts. However, note formalisation preclude us modelling agent-toagent interactions, global transition function rule successorsagents change local state following actions. Also observe essential conceptsviews easily expressed AC-MAS insisting local state agent includes partenvironments, i.e., artifacts agent access to. AC-MAS need viewsdefined, also possible views empty.artifact-based concepts lifecycles naturally expressed AC-MAS. artifactsmodelled part environment, lifecycle naturally encoded AC-MAS simplysequence changes induced transition function fragment environmentrepresenting lifecycle question. show example Section 2.4.technical remarks follow. simplify notation, denote global ground action~ (~u),~ = h0 (p0 ), . . . , n (pnQ)i ~u = h~u0 , . .Q. , ~un i, ~ui appropriate size.define transition relation Ai Ag Di (U ) Ai Ag Di (U ) s0exists~ (~u) Act(U ) s0 (s,~ (~u)). s0 , say s0successor s. run r infinite sequence s0 s1 , s0 = s..n N, take r(n) = sn . state s0 reachable exists run r globalstate r(0) = r(i) = s0 , 0. assume relation serial, i.e.,every global state exists s0 s0 . easily obtained assumingagent skip action enabled local state performing skip induceschanges local states. introduce set states reachable initialstate s0 according transition relation . Notice assuming unique initial statehinder generality approach, finite set states encode transitionss0 states I. plain interpreted systems (Fagin et al., 1995), say two globalstates = hl0 , . . . , ln s0 = hl00 , . . . , ln0 epistemically indistinguishable agent Ai , writtens0 , li = li0 . Differently interpreted systems local equality evaluated databaseinstances. convenience use also concept temporal-epistemic (t.e., short) run.Formally t.e. run r state infinite sequence s0 ; s1 ; . . . s0 =si si+1 si k si+1 , k Ag. state s0 said temporally-epistemicallyreachable (t.e. reachable, short) exists t.e. run r global state r(0) =0 r(i) = s0 . Obviously, temporal-epistemic runs include purelytemporal runs special case. Also, notice admit U infinite, thereby allowingpossibility set states infinite. Indeed, unless specify otherwise, assumeworking infinite-state AC-MAS.Finally, technical reasons useful refer global database schema = D0 DnAC-MAS. Every global stateS = hl0 , . . . , ln associated (global) D-instanceDs D(U ) Ds (Pi ) = jAg lj (Pi ), Pi D. omit subscript wheneverclear context write adom(s) adom(Ds ). justification choicecomes fact think agent partial, although truthful, view342fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSglobal state. relation appears several agent database schemas, possibly differentinterpretations, means agent aware subset total extensionrelation. maintain modeling choice justified application artifact systems,become apparent Section 2.4. Notice every S, Ds associatedunique, converse true general. Finally, lift disjoint union operator.global states s0 = hl0 l00 , . . . , ln ln0 i. seen Ds Ds0 Dss0 representfact D0 -instance.2.3 Model Checkingdefine problem verifying artifact-centric multi-agent system specificationinterest. following artifact-centric model, wish give data prominenceprocesses. deal data underlying database instances, specification language needsinclude first-order logic. Further, require temporal logic describe system execution.Lastly, use epistemic logic express information agents disposal. Hence,define first-order temporal-epistemic specification language interpreted AC-MAS.specification language used Section 4 formalise properties artifact-centric programs.Definition 2.8 (The Logic FO-CTLK) first-order CTLK (or FO-CTLK) formulasdatabase schema inductively defined following BNF:::= | | | x | AX | AU | EU | Ki | CLD 0 < n.notions free bound variables FO-CTLK extend straightforwardly LD , wellfunctions var, free, con. usual, temporal formulas AX AU 0 (resp. EU 0 )read runs, next step runs (resp. run), 0 .epistemic formulas Ki C intuitively mean agent Ai knows commonknowledge among agents respectively. use abbreviations EX, AF , AG,EF , EG standard. Observe free variables occur within scope modaloperators, thus admitting unconstrained alternation quantifiers modal operators, therebyallowing us refer elements different modal contexts.semantics FO-CTLK formulas defined follows.Definition 2.9 (Satisfaction FO-CTLK) Consider AC-MAS P, FO-CTLK formula ,state P, assignment . inductively define whether P satisfies , written(P, s, ) |= , follows:(P, s, ) |=(P, s, ) |=(P, s, ) |= 0(P, s, ) |= x(P, s, ) |= AX(P, s, ) |= AU 0iffiffiffiffiffiff(P, s, ) |= EU 0iff(Ds , ) |= , FO-formulacase (P, s, ) |=(P, s, ) |= (P, s, ) |= 0u adom(s), (P, s, ux ) |=runs r, r(0) = s, (P, r(1), ) |=runs r, r(0) = s, k 0 s.t. (P, r(k), ) |= 0 ,j, 0 j < k implies (P, r(j), ) |=run r, r(0) = k 0 s.t. (P, r(k), ) |= 0 ,343fiB ELARDINELLI , L OMUSCIO & PATRIZIj, 0 j < k implies (P, r(j), ) |=s0 S, s0 implies (P, s0 , ) |=s0 S, s0 implies (P, s0 , ) |=transitive closure 1in .(P, s, ) |= Ki(P, s, ) |= Ciffiffformula said true state s, written (P, s) |= , (P, s, ) |= assignments. Moreover, said true P, written P |= , (P, s0 ) |= .key concern paper explore model checking AC-MAS first-ordertemporal-epistemic specifications (Grohe, 2001).Definition 2.10 (Model Checking Problem) Given AC-MAS P FO-CTLK formulamodel checking problem consists finding assignment (P, s0 , ) |= .easy see whenever U finite model checking problem decidable P finitestate system. general, however, case. see this, notice that, assuming computability, agents protocol functions P ri AC-MAS transition function ,finitely represented (e.g., Turing machines). Since components agents AC-MASdefinitions finite, follows AC-MAS, particular infinite-state ones, admit finiterepresentation. Assuming fixed representation formalism, following result.Theorem 2.11 model checking problem AC-MAS w.r.t. FO-CTLK undecidable.Proof (sketch). proved showing every Turing machine whose tape containsinitial input simulated artifact system PT,I . problem checking whetherterminates particular input reduced checking whether PT,I |= , encodestermination condition. detailed construction similar work Deutsch, Sui,Vianu (2007, Thm. 4.10).Given general setting model checking problem defined above, negative result surprising. following identify semantic restrictions problemdecidable.2.4 Order-to-Cash Scenarioanalyse business process inspired concrete IBM customer use case (Hull et al., 2011).order-to-cash scenario describes interactions number agents e-commerce situationrelating purchase delivery product. agents artifact-centric system consistmanufacturer, customers, suppliers. process begins customerprepares submits purchase order (PO), i.e., list products customer requires,manufacturer. Upon receiving PO, manufacturer prepares material order (MO), i.e.,list components needed assemble requested products. manufacturer selectssupplier forwards relevant material order. Upon receiving MO supplier eitheraccept reject it. former case proceeds deliver requested componentsmanufacturer. latter case notifies manufacturer rejection. MO rejected,manufacturer deletes prepares submits new MO. manufacturerreceives delivered components, assembles product and, provided order paid344fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSidPOprod code offerstatusidMOprod code pricestatusProductsprod code budgetMaterialsmat code costFigure 1: Data Model Order-to-Cash Scenario.createPOpreparedsubmitPOpendingpaypaidshipPOshippeddeletePO(a) Purchase Order lifecyleacceptedshipMOshippedacceptMOcreateMOpreparationdoneMOsubmitteddeleteMOrejectMOrejecteddeleteMO(b) Material Order lifecyleFigure 2: Lifecycles artifacts involved order-to-cash scenario.for, delivers customer. manufacturer order directly indirectly relatedPO deleted PO deleted.encode order-to-cash business process artifact-centric multi-agent system,artifact data models represented database schemas evolution characterised appropriate set operations. natural identify two artifact types, representingPO MO. reserve distinguished relation artifact type. addition,introduce static relations store product material information. result, data modelorder-to-cash scenario associated attributes given Figure 1.intended meaning relations self-explanatory. Note presence attribute statusrelations corresponding artifact classes. intuitive representation artifact lifecycles,i.e., evolution key records artifacts states, capturing dependenceactions artifact statuses, shown Figure 2. example, purchase order, initialstatus prepared, created agent customer action createPO. ordersubmitted agent manufacturer, PO status changes pending. transitionslabelled pay, shipPO deletePO, act similarly, according semantics, statuspurchase order. Note incomplete representation business process,interaction actions artifact data content represented.345fiB ELARDINELLI , L OMUSCIO & PATRIZIformally encode scenario AC-MAS. sake presentationfollows assume dealing three agents only: one customer c, one manufacturerone supplier s. database schema Di agent {c, m, s} given as:Customer c:Dc = {Products(prod code, budget), PO(id , prod code, offer , status)};Manufacturer m:Dm = {PO(id , prod code, offer , status), MO(id , prod code, price, status)};Supplier s:Ds = {Materials(mat code, cost), MO(id , prod code, price, status)}.consider infinite set Uotc alphanumeric strings interpretation domain, introduce parametric action transition lifecycles Figure 2. Also, assumeinitial state non-empty relations Products Materials, contain background information, catalogue available products. define agentsorder-to-cash scenario follows.Definition 2.12 agents Ac , givenAc = hDc , Actc , P rc i, (i) Dc above; (ii) Actc = {createPO(id , pcode),submitPO(id ), pay(id ), deletePO(id )}; (iii) P rc respects intended meaningcustomers actions. instance, createPO(id , pcode) P rc (lc ) iff interpretationlc (Products) relation Products local state lc contains tuple hpcode, bibudget b.= hDm , Actm , P rm i, (i) Dm above; (ii) Actm = {createMO(id , price),doneMO(id ), shipPO(id ), deleteMO(id )}; (iii) P rm respects intended meaningmanufacturers actions. instance, createMO(po id , price) P rm (lm ) iffinterpretation lm (MO) relation MO local state lm contain tuplehpo id, pc, pr, preparationi PO id po id.=hDs , Acts , P rs i, (i) Ds above;(ii) Acts={acceptMO(id ), rejectMO(id ), shipMO(id )}; (iii) P rc respects intendedmeaning suppliers actions. instance, acceptMO(mo id ) P rs (ls ) iff ls (M O)contain tuple id mo id status accepted.Further, define AC-MAS induced set agents Agotc = {Ac , , }according Definition 2.7.Definition 2.13 Given set agents AgotchAgotc , s0otc , otc={Ac , , }, AC-MAS Potc=s0otc = hlc , lm , ls initial global state, non-empty relations ProductsMaterials lc ls respectively;otc global transition function defined respect intended meaningevolution order-to-cash scenario. instance, consider global action346fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS(~u) = hcreatePO(pc), doneMO(m), acceptMO(m 0 )i enabled respective protocols global state s. definition actions createPO(pc), doneMO(m),acceptMO(m 0 ) li (s) P ri {c, m, s} implies Products relation contains information product pc. Also, interpretation relation MOcontains tuples hm, p, pr, preparationi hm0 , p0 , pr0 , submittedi products pp0 . Hence, s0 otc (s, (~u)) iff interpretation relation PO s0 extendsDs (P O) tuple hid, pc, b, preparedi, id fresh id. tuples material orders m0 updated Ds0 (M O) becoming hm, p, pr, submittedihm0 , p0 , pr0 , acceptedi, respectively. element changed transition.Clearly, function otc given easily completed encode artifacts lifecyclesgiven Figure 2. section 4.2 give succinct encoding Potc terms artifactcentric program.investigate properties present business process using specificationsFO-CTLK. instance, following formula intuitively specifies manufacturer knowsmaterial order MO match corresponding purchase order PO:match = AG id, pc (pr, O(id, pc, pr, s) Km o, s0 P O(id, pc, o, s0 ))next specification states given material order MO, customer eventually knowcorresponding PO shipped.fulfil= AG id, pc (pr, O(id, pc, pr, s) EF Kc P O(id, pc, o, shipped))Further, may interested checking whether budget costs always kept secretsupplier customer c respectively, whether customer (resp., supplier)knows fact:budget = Kc pc AG b Ks Products(pc, b)cost = Ks mc AG c Kc Materials(mc, c)interesting specifications describing properties artifact system agents operating similarly formalised FO-CTLK, thereby providing engineer valuabletool assess implementation.Observe AC-MAS order-to-cash scenario infinite number states therebymaking difficult investigate means traditional model checking techniques. returnscenario Subsection 4.2 investigate may still verified.develop methodology associating finite abstractions infinite AC-MAS.3. Abstraction Artifact-Centric Multi-agent Systemsprevious section observed model checking AC-MAS FO-CTLK undecidable general. clearly interest isolate decidable settings. follows identifysemantic constraints resulting decidable model checking problem. investigation carriedrather natural subclass AC-MAS call bounded, defined below. goalproceeding manner identify finite abstractions infinite-state AC-MAS verification programs admit bounded AC-MAS models conducted them, ratheroriginal infinite-state AC-MAS. see detail Section 4.347fiB ELARDINELLI , L OMUSCIO & PATRIZIkey concept enables us achieve uniformity. Uniform AC-MASsystems behaviour depend actual data present states.means system contains possible transitions enabled according parametricaction rules, thereby resulting full transition relation. notion related genericitydatabases (Abiteboul et al., 1995). use term uniformity refer transitionsystems databases.achieve finite abstractions proceed follows. first propose adaptation notionisomorphism setting; introduce bisimulations; finally Subsection 3.2 shownotion exploited guarantee uniform AC-MAS satisfy FO-CTLKformulas. use result show bounded, uniform systems admit finite abstractions(Subsection 3.3). complexity model checking problem analysed Subsection 3.4.rest section let P = hAg, s0 , P 0 = hAg 0 , s00 , 0 two AC-MASassume, unless stated differently, = hl0 , . . . , ln S, s0 = hl00 , . . . , ln0 0 .3.1 Isomorphismsinvestigate concept isomorphism AC-MAS. needed later sectionsdefine finite abstractions infinite-state AC-MAS.Definition 3.1 (Isomorphism) Two local states l, l0 D(U ) isomorphic, written l ' l0 , iffexists bijection : adom(l) Con 7 adom(l0 ) Con that:(i) identity Con;(ii) every Pi D, ~u U qi , ~u l(Pi ) iff (~u) l0 (Pi ).case, say witness l ' l0 .Two global states s0 0 isomorphic, written ' s0 , iff exists bijection: adom(s) Con 7 adom(s0 ) Con every j Ag, witness lj ' lj0 .Notice isomorphisms preserve interpretation constants Con well predicateslocal states renaming corresponding terms. function calledwitness ' s0 . Obviously, relation ' equivalence relation. Given function f :U 7 U 0 defined adom(s), f (s) denotes instance D(U 0 ) obtained renamingu adom(s) f (u). f also injective (thus invertible) identity Con,f (s) ' s.Example example isomorphic states, consider agent local database schema ={P1 /2, P2 /1}, let U = {a, b, c, . . .} interpretation domain, fix set Con = {b}constants. Let l local state l(P1 ) = {ha, bi, hb, di} l(P2 ) = {a} (see Figure 3).Then, local state l0 l0 (P1 ) = {hc, bi, hb, ei} l0 (P2 ) = {c} isomorphic l.easily seen considering isomorphism , where: (a) = c, (b) = b, (d) = e.However, state l00 l00 (P1 ) = {hf, di, hd, ei} l00 (P2 ) = {f } isomorphic l.Indeed, although bijection exists maps l l00 , easy see none0 (b) = b.Note that, isomorphic states relational structure, two isomorphic statesnecessarily satisfy FO-formulas satisfaction depends also values assignedfree variables. account this, introduce following notion.348fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSl(P1 )bbl(P2 )l0 (P1 )c bb el0 (P2 )cl00 (P1 )fel00 (P2 )fFigure 3: Examples isomorphic non-isomorphic local states.Definition 3.2 (Equivalent assignments) Given two states s0 0 , set variables V V ar, two assignments : V ar 7 U 0 : V ar 7 U 0 equivalent V w.r.t.s0 iff exists bijection : adom(s) Con (V ) 7 adom(s0 ) Con 0 (V ) that:(i) |adom(s)Con witness ' s0 ;(ii) 0 |V = |V .Intuitively, equivalent assignments preserve (in)equalities variables Vconstants s, s0 renaming. Note that, definition, implies s, s0 isomorphic.say two assignments equivalent FO-CTLK formula , omitting statess0 clear context, equivalent free().show following standard result first-order (non-modal) logic, i.e., isomorphicstates satisfy exactly FO-formulas (Abiteboul et al., 1995).Proposition 3.3 Given two isomorphic states s0 0 , FO-formula , twoassignments 0 equivalent ,(Ds , ) |= iff (Ds0 , 0 ) |=Moreover, FO-sentence,Ds |= iff Ds0 |=Thus, isomorphic states cannot distinguished FO-sentences. enables us usenotion, defining simulations.3.2 BisimulationsPlain bisimulations known satisfaction preserving modal propositional setting (Blackburn, de Rijke, & Venema, 2001). following explore conditionsapplies AC-MAS well. introduce notion bisimulation, based isomorphisms,later explore properties context uniform AC-MAS.Definition 3.4 (Simulation) relation R 0 simulation hs, s0 R implies:1. ' s0 ;2. every S, exists t0 0 s.t. s0 t0 , ' s0 t0 , ht, t0 R;3. every S, every 0 < n, exists t0 0 s.t. t0 ,' s0 t0 , ht, t0 R.349fiB ELARDINELLI , L OMUSCIO & PATRIZIP12345P012Figure 4: Bisimilar AC-MAS satisfying FO-CTLK formulas.Definition 3.4 many similarities standard notion simulation propositionalsetting. particular, co-inductive structure definition requires similar states satisfylocal property preserve along corresponding transitions. However, differentlypropositional case, insist ' s0 t0 ; ensures similar transitionsAC-MAS preserve isomorphic disjoint unions.state s0 0 said simulate S, written s0 , iff exists simulation Rs.t. hs, s0 R. ambiguity arises, simply say s0 similar. Notesimilar states isomorphic, condition (2) ensures ' s0 . similarity relationshown largest simulation, reflexive transitive 0 . Further, say P 0simulates P, written P P 0 , s0 s00 .Simulations naturally extended bisimulations, follows.Definition 3.5 (Bisimulation) relation B 0 bisimulation iff B B 1 ={hs0 , si | hs, s0 B} simulations.Two states s0 0 said bisimilar, written s0 , iff exists bisimulationB hs, s0 B. shown largest bisimulation, equivalencerelation, 0 . say P P 0 bisimilar, written P P 0 iff s0 s00 .instructive note bisimilar systems preserve FO-CTLK formulas.markedly different modal propositional case.Example Consider Figure 4, Con = P P 0 given follows. number nagents equal 1, define = D0 = {P/1} U = N; s0 (P ) = s00 (P ) = {1}; = {hs, s0 |s(P ) = {i}, s0 (P ) = {i + 1}}; 0 = {hs, s0 | s(P ) = {i}, s0 (P ) = {(i mod 2) + 1}}. NoticeD(N) 0 D(N). Clearly P P 0 . Now, consider constant-freeFO-CTLK formula = AG(x(P (x) AXAGP (x))). easily seen P |=P 0 6|= .shows that, differently propositional case, bisimilarity sufficientcondition guarantee preservation FO-CTLK formulas. Intuitively, consequencefact bisimilar AC-MAS preserve value associations along runs. instance,value 1 P 0 associated infinitely many times odd values occurring P. quantifyingacross states able express fact therefore distinguish two structures.difficulty as, intuitively, would like use bisimulations demonstrate existence finiteabstractions. However, show later, happens class uniform AC-MAS, definedbelow.350fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSDefinition 3.6 (Uniformity) AC-MAS P said uniform iff every s, t, s0 S, t0D(U ),1. (s,~ (~u)) ' s0 t0 witness , every constant-preserving0bijection extends ~u, t0 (s0 ,~ (0 (~u)));2. ' s0 t0 , s0 t0 .definition captures idea actions take account operate relationalstructure states action parameters, irrespective actual data contain (apartfinite set constants). Intuitively, uniformity expresses reached executing (~u)s, replace element v v 0 s, ~u t, obtaining s0 , ~u0 t0 , t0reached executing (~u0 ) s0 . terms underlying Kripke structures, i.e., framesinduced relations Ai Ag, means systems full ,is, uniform AC-MAS states t0 identified indeed part system reachables0 . similar condition required epistemic relation. property uniform systemslatter requirement implied former, shown following result.Proposition 3.7 AC-MAS P satisfies req. 1 Def. 3.6 adom(s0 ) Con, req. 2also satisfied.Proof. ' s0 t0 , witness : adom(s) adom(t) Con 7 adom(s0 )adom(t0 ) Con identity Con (hence adom(s0 )). Assume t, thus li (s) = li (t),li (s0 ) = (li (s)) = (li (t)) = li (t0 ). Notice guarantee s0 t0 ,need prove t0 S. done showing t0 reachable s0 . Sincereachable s0 , exists run s0 s1 . . . sk s.t. sk = t. Extend totalinjective function 0 : adom(s0 ) adom(sk ) Con 7 U . always done|U | |adom(s0 ) adom(sk ) Con|. consider sequence 0 (s0 ), 0 (s1 ), . . . , 0 (sk ).Since adom(s0 ) Con (s0 ) = s0 and, 0 extends , 0 (s0 ) = (s0 ) = s0 .Further, 0 (sk ) = (t) = t0 . repeated applications req. 1 show 0 (sm+1 )(0 (sm ),~ (0 (~u))) whenever sm+1 (sm ,~ (~u)), < k. Hence, sequence actually0000run s0 . Thus, S, .Thus, long adom(s0 ) Con, check whether AC-MAS uniform, sufficienttake account transition function.distinctive feature uniform systems isomorphic states bisimilar.Proposition 3.8 AC-MAS P uniform, every s, s0 S, ' s0 implies s0 .Proof. prove B = {hs, s0 | ' s0 } bisimulation. Observe since 'equivalence relation, B. Thus B symmetric B = B 1 . Therefore, proving Bsimulation proves also B 1 simulation; hence, B bisimulation. end, leths, s0 B, assume S. Then, (s, (~u)) (~u) Act(U ).Consider witness ' s0 . cardinality considerations extended totalinjective function 0 : adom(s) adom(t) {~u} Con 7 U . Consider 0 (t) = t0 ; follows 0witness ' s0 t0 . Since P uniform, t0 (s0 , (0 (~u))), is, s0 t0 . Moreover,0 witness ' t0 , thus ht, t0 B. Next assume hs, s0 B t, S.351fiB ELARDINELLI , L OMUSCIO & PATRIZIreasoning find witness ' s0 , extension 0 s.t. t0 = 0 (t)0 witness ' s0 t0 . Since P uniform, s0 t0 ht, t0 B.result intuitively means submodels generated isomorphic states bisimilar.Next prove partial results, useful proving main preservation theorem. first two guarantee appropriate cardinality constraints bisimulation preservesequivalence assignments w.r.t. given FO-CTLK formula.Lemma 3.9 Consider two bisimilar uniform AC-MAS P P 0 , two bisimilar statess0 0 , FO-CTLK formula . every assignments 0 equivalent w.r.t.s0 , that:1. every s.t. t, |U 0 | |adom(s) adom(t) Con (free())|,exists t0 0 s.t. s0 t0 , t0 , 0 equivalent w.r.t. t0 .2. every s.t. t, |U 0 | |adom(s) adom(t) Con (free())|,exists t0 0 s.t. s0 t0 , t0 , 0 equivalent w.r.t. t0 .Proof. prove (1), let bijection witnessing 0 equivalent w.r.t. s0 .Suppose t. Since s0 , definition bisimulation exists t00 0 s.t. s0 t00 ,.' s0 t00 , t00 . define Domj = adom(s) adom(t) Con, partition into:.Dom = adom(s) Con (adom(t) (free());.Dom0 = adom(t) \ Dom .Let 0 : Dom0 7 U 0 \ Im() invertible total function. Observe |Im()| =|adom(s0 ) Con 0 (free())| = |adom(s) Con (free())|, thus fact |U 0 ||adom(s) adom(t) Con (free())| |U 0 \ Im()| |Dom(0 )|, guaranteesexistence 0 .Next, define j : Domj 7 U 0 follows:(u), u Domj(u) =0 (u), u Dom0Obviously, j invertible. Thus, j witness ' s0 t0 , t0 = j(t). Since' s0 t00 ' equivalence relation, obtain s0 t0 ' s0 t00 . Thus, s0 t0 ,P 0 uniform. Moreover, 0 equivalent w.r.t. t0 , construction t0 .check t0 , observe that, since t0 ' t00 P 0 uniform, Prop. 3.8 follows t0 t00 .Thus, since t00 transitive, obtain t0 . proof (2) analogousstructure therefore omitted.proven result tight, i.e., cardinality requirement violated,exist cases assignment equivalence preserved along temporal epistemic transitions.Lemma 3.9 easily generalises t.e. runs.Lemma 3.10 Consider two bisimilar uniform AC-MAS P P 0 , two bisimilar statess0 0 , FO-CTLK formula , two assignments 0 equivalent w.r.t.s0 . every t.e. run r P, r(0) = 0, |U 0 | |adom(r(i)) adom(r(i + 1))Con (free())|, exists t.e. run r0 P 0 s.t. 0:352fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS(i) r0 (0) = s0 ;(ii) r(i) r0 (i);(iii) 0 equivalent w.r.t. r(i) r0 (i).(iv) every 0, r(i) r(i + 1) r0 (i) r0 (i + 1), r(i) j r(i + 1),j, r0 (i) j r0 (i + 1).Proof. Let r t.e. run satisfying lemmas hypothesis. inductively build r0 showconditions satisfied. = 0, let r0 (0) = s0 . hypothesis, r s.t. |U 0 ||adom(r(0)) adom(r(1)) Con (free())|. Thus, since r(0) ; r(1), Lemma 3.9exists t0 0 s.t. r0 (0) ; t0 , r(1) t0 , 0 equivalent w.r.t. r(1) t0 . Letr0 (1) = t0 . Lemma 3.9 guarantees transitions r0 (0) ; t0 r(0) ; r(1) choseneither temporal epistemic index.case > 0 similar. Assume r(i) r0 (i) 0 equivalentw.r.t. r(i) r0 (i). Since r(i) ; r(i + 1) |U 0 | |adom(r(i)) adom(r(i + 1)) Con(free())|, Lemma 3.9 exists t0 0 s.t. r0 (i) ; t0 , 0 equivalentw.r.t. r(i + 1) t0 , r(i + 1) t0 . Let r0 (i + 1) = t0 . clear r0 t.e. run P 0 ,that, Lemma 3.9, transitions r0 chosen fulfil requirement (iv).prove FO-CTLK formulas cannot distinguish bisimilar uniform AC-MAS.marked contrast earlier example section related bisimilarnon-uniform AC-MAS.Theorem 3.11 Consider two bisimilar uniform AC-MAS P P 0 , two bisimilar statess0 0 , FO-CTLK formula , two assignments 0 equivalent w.r.t. s0 .1. every t.e. run r s.t. r(0) = s, k 0 |U 0 | |adom(r(k)) adom(r(k +1)) Con (free())| + |var() \ free()|;2. every t.e. run r0 s.t. r0 (0) = s0 , k 0 |U | |adom(r0 (k)) adom(r0 (k +1)) Con 0 (free())| + |var() \ free()|;(P, s, ) |= iff (P 0 , s0 , 0 ) |= .Proof. proof induction structure . prove (P, s, ) |=|= . direction proved analogously. base case atomic formulasfollows Prop. 3.3. inductive cases propositional connectives straightforward.x, assume x free() (otherwise consider , corresponding case),variable quantified (otherwise rename variables). Let bijection witnessing 0 equivalent w.r.t. s0 . u adom(s), considerxassignment ux . definition, (u) adom(s0 ), 0 (u)well-defined. Notexx0free() = free() {x}; u (u) equivalent w.r.t. s0 . Moreover,| ux (free())| |(free())| + 1, u may occur (free()). considerations apply 0 . Further, |var() \ free()| = |var() \ free()| 1, var() = var(),(P 0 , s0 , 0 )353fiB ELARDINELLI , L OMUSCIO & PATRIZIfree() = free() {x}, x/ free(). Thus, hypotheses (1) (2) remain satisfiedx. Therefore, induction hypothesis,replace , ux , 0 0 (u)xx000(P, s, u ) |= (P , , (u) ) |= . Since u adom(s) generic bijection,result follows.AX, assume contradiction (P, s, ) |= (P 0 , s0 , 0 ) 6|= . Then,exists run r0 s.t. r0 (0) = s0 (P 0 , r0 (1), 0 ) 6|= . Since |var() \ free()| 0, Lemma 3.10,exists run r s.t. r(0) = s, 0, r(i) r0 (i) 0 equivalentw.r.t. r(i) r0 (i). Since r run s.t. r(0) = s, satisfies hypothesis (1). Moreover,hypothesis necessarily satisfied t.e. runs r00 s.t. 0, r00 (0) = r(i) (otherwise,t.e. run r(0) ; ; r(i) ; r00 (1) ; r00 (2) ; would satisfy hypothesis r);considerations apply w.r.t hypothesis (2) t.e. runs r000 s.t. r000 (0) = r0 (i),0. particular, hold = 1. Thus, inductively apply lemma, replacingr(1), s0 r0 (1), (observe var() = var() free() = free()).obtain (P, r(1), ) 6|= , thus (P, r(0), ) 6|= AX. contradiction.EU , assume variables common occur free formulas(otherwise rename quantified variables). Let r run s.t. r(0) = s, exists k 0s.t. (P, r(k), ) |= , (P, r(j), ) |= 0 j < k. Lemma 3.10 exists runr0 s.t. r0 (0) = s0 0, r0 (i) r(i) 0 equivalent w.r.t. r0 (i)r(i). bijection witnessing 0 equivalent w.r.t. r0 (i)r(i), define bijections i, = |adom(r(i))Con(free()) i, = |adom(r(i))Con(free()) .Since free() free(), free() free(), seen i, i, witness0 equivalent respectively w.r.t. r0 (i) r(i). argument usedAX case above, hypothesis (1) holds t.e. runs r00 s.t. r00 (0) = r(i),0, hypothesis (2) holds t.e. runs r000 s.t. r000 (0) = r0 (i). observe|(free())|, |(free())| |(free())|. Moreover, assumption common variables, (var() \ free()) = (var() \ free()) ] (var() \ free()), thus |var() \ free()| =|(var() \ free()| + |(var() \ free()|, hence |(var() \ free()|, |(var() \ free()||var() \ free()|. Therefore hypotheses (1) (2) hold also uniformly replaced either . Then, induction hypothesis applies i, replacing r(i), s0r0 (i), either . Thus, i, (P, r(i), ) |= iff (P 0 , r0 (i), 0 ) |= ,(P, r(i), ) |= iff (P 0 , r0 (i), 0 ) |= . Therefore, r0 run s.t. r0 (0) = s0 , (P 0 , r0 (k), 0 ) |= ,every j, 0 j < k implies (P 0 , r0 (j), 0 ) |= , i.e., (P 0 , s0 , 0 ) |= EU .AU , assume contradiction (P, s, ) |= (P 0 , s0 , 0 ) 6|= . Then,exists run r0 s.t. r0 (0) = s0 every k 0, either (P 0 , r0 (k), 0 ) 6|= exists js.t. 0 j < k (P 0 , r0 (j), 0 ) 6|= . Lemma 3.10 exists run r s.t. r(0) = s,0, r(i) r0 (i) 0 equivalent w.r.t. r(i) r0 (i). Similarlycase EU , shown 0 equivalent w.r.t. r(i) r0 (i),0. Further, assuming w.l.o.g. variables common occur freeformulas, shown, case EU , induction hypothesis holds everypair runs obtained suffixes r r0 , starting i-th state, every 0. Thus,(P, r(i), ) |= iff (P 0 , r0 (i), 0 ) |= , (P, r(i), ) |= iff (P 0 , r0 (i), 0 ) |= . rs.t. r(0) = every k 0, either (P, r(k), ) 6|= exists j s.t. 0 j < k(P, r(j), ) 6|= , is, (P, s, ) 6|= AU . contradiction.Ki , assume contradiction (P, s, ) |= (P 0 , s0 , 0 ) 6|= . Then,exists s00 s.t. s0 s00 (P 0 , s00 , 0 ) 6|= . Lemma 3.10 exists s000 s.t. s000 s00 , s000 ,354fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS0 equivalent w.r.t. s00 s000 . Thus, argument analogous usedcase AX, apply induction hypothesis, obtaining (P, s000 , ) 6|= .(P, s, ) 6|= Ki , contradiction.Finally, C, assume contradiction (P, s, ) |= (P 0 , s0 , 0 ) 6|= . Then,exists s00 s.t. s0 s00 (P 0 , s00 , 0 ) 6|= . Lemma 3.10 exists s000 s.t. s000s00 , s000 , 0 equivalent w.r.t. s00 s000 . Thus, argument analogousused case Ki , apply induction hypothesis, obtaining (P, s000 , ) 6|= .(P, s, ) 6|= C, contradiction.easily extend result model checking problem AC-MAS.Theorem 3.12 Consider two bisimilar uniform AC-MAS P P 0 , FO-CTLK formula.1. t.e. runs r s.t. r(0) = s0 , k 0, |U 0 | |adom(r(k)) adom(r(k + 1))Con| + |var()|,2. t.e. runs r0 s.t. r0 (0) = s00 , k 0, |U | |adom(r0 (k)) adom(r0 (k + 1))Con| + |var()|P |= iff P 0 |= .Proof. Equivalently, prove (P, s0 , ) 6|= , exists 0s.t. (P 0 , s00 , 0 ) 6|= , viceversa. end, observe hypotheses (1) (2) imply, respectively, hypotheses (1) (2) Theorem 3.11. Further, notice that, cardinality considerations,given assignment : V ar 7 U , exists assignment 0 : V ar 7 U 0 s.t. 0equivalent w.r.t. s0 s00 . Thus, applying Theorem 3.11 existsassignment s.t. (P, s0 , ) 6|= , exists assignment 0 s.t. (P 0 , s00 , 0 ) 6|= .converse proved analogously, hypotheses symmetric.result shows uniform AC-MAS principle verified model checking bisimilar one. Note applies infinite AC-MAS P, well. case resultsenable us show verification question posed corresponding, possibly finiteP 0 long U 0 , defined above, sufficiently large P 0 bisimulate P. noteworthy classinfinite systems results prove particularly powerful bounded AC-MAS,which, discussed next subsection, always admit finite abstraction.3.3 Finite Abstractionsdefine notion finite abstraction AC-MAS, prove that, uniformity, abstractions bisimilar corresponding concrete model. particularly interested finiteabstractions; operate special class infinite models call bounded.Definition 3.13 (Bounded AC-MAS) AC-MAS P b-bounded, b N, S,|adom(s)| b.355fiB ELARDINELLI , L OMUSCIO & PATRIZIAC-MAS b-bounded none reachable states contain b distinct elements.Observe bounded AC-MAS may defined infinite domains. Furthermore, note bbounded AC-MAS may contain infinitely many states, bounded b. b-bounded systemsinfinite-state general. Notice also value b constrains number distinctindividuals state, size state itself, intended amount memory requiredaccommodate individuals. Indeed, infinitely many elements domain U needunbounded number bits represented (e.g., finite strings), so, even though stateguaranteed contain b distinct elements, nothing said large actualspace required elements is. Conversely, memory-bounded AC-MAS finite-state (henceb-bounded, b).Since b-bounded AC-MAS general memory-unbounded, cannot verified trivially generating checking possibly infinite executions. However, show laterb-bounded uniform infinite-state AC-MAS admits finite-state abstractionused verify it.introduce abstractions modular manner first introducing set abstract agentsconcrete AC-MAS.Definition 3.14 (Abstract agent) Let = hD, Act, P ri agent defined interpretationdomain U . Given interpretation domain U 0 , abstract agent U 0 agent A0 =hD0 , Act0 , P r0 that:1. D0 = D;2. Act0 = Act;3. (~u0 ) P r0 (l0 ), l0 D0 (U 0 ), iff exist l D(U ) (~u) P r(l) s.t. l0 ' l,witness , ~u0 = 0 (~u), bijection 0 extending ~u.Given set Ag agents defined U , Ag 0 denotes set corresponding abstractions U 0agents Ag.remark abstract agent A0 agent line Definition 2.6. Notice protocol A0 defined basis corresponding concrete agent requires existencebijection elements corresponding local states action parameters. Thus,order ground action counterpart A0 , last requirement Definition 3.14constrains U 0 contain sufficient number distinct values. become apparent later,size U 0 determines closely abstract system simulate concrete counterpart. Noticealso that, general, agent may abstraction U , instance data mayimpact agents protocol.Next, combine notion uniformity boundedness. aim identifyconditions verification infinite AC-MAS reduced verificationfinite one. main result given Corollary 3.19 guarantees that, contextbounded AC-MAS, uniformity sufficient condition bisimilar finite abstractionssatisfaction-preserving.following assume AC-MAS P adom(s0 ) Con.case, Con extended include (finitely many) elements adom(s0 ). startformalising notion abstraction.356fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSDefinition 3.15 (Abstract AC-MAS) Let P = hAg, s0 , AC-MAS Ag 0 set abstract agents obtained Definition 3.14 interpretation domain U 0 . AC-MASP 0 = hAg 0 , s00 , 0 said abstraction P iff:s00 = s0 ;t0 0 (s0 ,~ (~u0 )) iff exist s,~ (~u) Act(U ) ' s0 t0 ,00witness , (s,~ (~u)), ~u = (~u) bijection 0 extending ~u.checked P 0 , defined above, indeed AC-MAS satisfies relevantconditions protocols transitions Definition 2.7. Indeed, t0 0 (s0 ,~ (~u0 )),exist s, S,~ (~u) (s,~ (~u)), ' s0 t0 witness , ~u = 0 (~u0 )0bijection extending . means (~ui ) P ri (li ) n. definition P ri0(~u0i ) P ri0 (li0 ) n.definition requires abstractions initial states isomorphic concrete counterparts; specifically equal adom(s0 ) Con. Moreover, second constraint entailstransition concrete model exists transition, renaminginvolved values, exists abstraction. So, example, copy action concrete modelcorresponding copy action abstract model. Crucially, condition requires domainU 0 contains enough elements bisimulate concrete states action effects. madeprecise Lemma 3.17.Obviously, U 0 finitely many elements, 0 finitely many states. Observe alsovarying U 0 obtain different abstractions. Finally, notice AC-MAS necessarilyabstraction itself. issue addressed Lemma 3.16.Next, investigate relationship AC-MAS abstractions. first usefulresult states every finite abstraction uniform, independently properties AC-MASabstract.Lemma 3.16 Every abstraction P 0 AC-MAS P uniform. Moreover, P uniformU 0 = U , P 0 = P.Proof. Consider s, t, s0 0 , t0 D(U 0 ),~ (~u) Act0 (U 0 ) s.t. 0 (s,~ (~u))00' , witness . need show P 0 admits transition s0 t0 . SinceP 0 abstraction P, given definition 0 , exist s00 , t00~ (~u00 ) Act(U )0000000000000s.t. (s ,~ (~u )), ' t, witness , ~u = (~u ), constantpreserving bijection 0 extending ~u00 . Consider ~u0 U 0|~u| ~u0 = 0 (~u),constant-preserving bijection 0 extending ~u. Obviously, composition 0 0 constantpreserving bijection ~u0 = 0 (0 (~u00 )). Moreover, restricted witnesss00 t00 ' s0 t0 . then, since P 0 abstraction P, implies t0 0 (s0 ,~ (~u0 )). Thus,P 0 uniform.Moreover, prove P abstraction every time P uniform U 0 = U ,notice transition (s,~ (~u)) P, also P 0 definition0abstraction. Also, transition 0 (s0 ,~ (~u0 )) appears P 0 , exist s,00~ ) s.t. ' witness , (s,~ (~u) Act(U~ (~u)), ~u0 = 0 (~u)constant-preserving bijection 0 extending ~u. Finally, since P uniform casetransition t0 0 (s0 , (~u0 )) P well.357fiB ELARDINELLI , L OMUSCIO & PATRIZIlemma provides sufficient conditions AC-MAS abstraction itself,namely uniform interpretation domain.second result guarantees every uniform, b-bounded AC-MAS bisimilarabstractions, provided arePbuilt sufficiently large interpretation domain.0following, take NAg = NAg =x|}, i.e., NAg sumx)Acti {|~Ai Ag max(~maximum number parameters contained action types agent Ag.Lemma 3.17 Consider uniform, b-bounded AC-MAS P infinite interpretation domain U ,interpretation domain U 0 Con U 0 . |U 0 | 2b + |Con| + NAg ,abstraction P 0 P U 0 bisimilar P.Proof. Let B = {hs, s0 0 | ' s0 }. prove B bisimulationhs0 , s00 B. start proving B simulation relation. end, observe sinces0 = s00 , s0 ' s00 , hs0 , s00 B. Next, consider hs, s0 B, thus ' s0 . Assumet, S. Then,~ (~u) Act(U ) (s,~ (~u)). Moreover,Pmust existsince |U 0 | 2b + |Con| + NAg ,S Ai Ag |~ui | NAg , |adom(s) adom(t)| 2b, witness' s0 extended Ai Ag ~ui bijection 0 . let t0 = 0 (t). way 0defined, seen ' s0 t0 . Further, since P 0 abstraction P,t0 0 (s0 ,~ (~u0 )) ~u0 = 0 (~u), is, s0 t0 P 0 . Therefore, exists t0 0s0 t0 , ' s0 t0 , ht, t0 B. regards epistemic relation, assume{1, . . . , n} S. definition , li (s) = li (t). Since |U 0 | 2b + |Con|,witness ' s0 extended witness 0 ' s0 t0 , t0 = 0 (t).Obviously, li (s0 ) = li (t0 ). Thus, prove s0 t0 , left show t0 0 , i.e., t0reachable P 0 s00 = s0 . end, observe since S, exists purely temporalrun r r(0) = s0 r(k) = t, k 0. Thus, exist~ 1 (~u1 ) . . . ,~ k (~uk )r(j + 1) (r(j),~ j+1 (~uj+1 )), 0 j < k. Since |U 0 | 2b + |Con|, define,0 j < k, function j witness r(j) r(j + 1) ' j (r(j)) j (r(j + 1)). particular,done starting j = k 1, defining k1 k1 (r(k)) = k1 (t) = t0 ,proceeding backward j = 0, that, 0 j < k, j (r(j + 1)) = j+1 (r(j + 1)).Observe since adom(s0 ) Con, necessarily i0 (r(0)) = i0 (s0 ) = s0 = s00 . Moreover,|U 0 | 2b + |Con| + NAg , j extended bijection 0j , elements occurring~uj+1 . Thus, given P 0 abstraction P, 0 j < k, 0j (r(j + 1))(0j (r(j)),~ (0j (~uj+1 ))). Hence, sequence 00 (r(0)) 0k1 (r(k)) run P 0 , and,since t0 = 0k1 (r(k)), t0 reachable P 0 . Therefore s0 t0 . Further, since ' t0 , definitionB, case ht, t0 B, hence B simulation.prove B 1 simulation, given hs, s0 B (thus ' s0 ), assume s0 t0 ,t0 0 . Obviously, exists~ (~u0 ) Act(U 0 ) t0 0 (s0 ,~ (~u0 )). P 000000000abstraction P, exist ,~ (~u ) Act(U ) t00 ' s0 t0 ,witness , t00 (s00 , (~u00 )), ~u00 = 0 (~u0 ), bijection 0 extending ~u0 . Observes0 ' s00 , thus, transitivity ' ' s00 . fact existseasily follows uniformity P. Thus, since t0 ' t, ht, t0 B.epistemic relation, assume s0 t0 t0 0 0 < n. Let witness s0 ' s,let 0 extension witness s0 t0 ' t. = 0 (t0 ), seenli (s) = li (t). Observe t0 0 . Using argument analogous one above, exploitingfact P uniform, P 0 certainly b-bounded, |U | > 2b + |Con| + NAg Uinfinite, show constructing run r P r(k) = t, k 0.358fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSt. Further, since t0 ' t, ht, t0 B. Therefore, B 1 simulation. So, PP 0 bisimilar.result allows us prove main abstraction theorem.Theorem 3.18 Consider b-bounded uniform AC-MAS P infinite interpretation domain U , FO-CTLK formula , interpretation domain U 0 Con U 0 .|U 0 | 2b + |Con| + max{|vars()|, NAg }, abstraction P 0 P U 0 ,that:P |= iff P 0 |= .Proof. Lemma 3.16, P 0 uniform. Thus, hypothesis cardinalities U U 0 ,Lemma 3.17 applies, P P 0 bisimilar. Obviously, also P 0 b-bounded. Thus, since PP 0 b-bounded, cardinality hypothesis U U 0 , Theorem 3.12 applies.particular, notice every temporal-epistemic run r s.t. r(0) = s0 , k 0,|U 0 | |adom(r(k))adom(r(k+1))Con|+|var()|, |adom(r(k))| b b-boundedness.Therefore, P |= iff P 0 |= .follows using sufficiently large number abstract values U 0 , reduceverification infinite, bounded, uniform AC-MAS verification finite one.Corollary 3.19 Given b-bounded uniform AC-MAS P infinite interpretation domainU , FO-CTLK formula , exists AC-MAS P 0 finite interpretation domain U 0P |= iff P 0 |= .also noted U 0 simply taken finite subset U (including Con)satisfying cardinality requirement above. so, finite abstraction P 0 definedsimply restriction P U 0 . Thus, every infinite, b-bounded uniform AC-MASbisimilar finite subsystem, satisfies formulas.Note concerned actual construction finite abstraction. intend construct directly artifact-centric program, Section 4.that, explore complexity model checking problem.3.4 Complexity Model Checking Finite AC-MAS FO-CTLK Specificationsanalyse complexity model checking problem finite AC-MAS respectFO-CTLK specifications. input problem consists AC-MAS P finite domain UFO-CTLK formula ; output assignment (P, s0 , ) |= , wheneverproperty satisfied. Hereafter follow standard literature basic notions definitions (Grohe,2001).encode AC-MAS P use tuple EP = hU, D, s0 , i, U (finite) interpretation domain, global database schema, s0 initial state, = {~ 1 , . . . , ~ }set FO-formulas, capturing transitions associated ground joint action~ . SinceU finite, set ground actions, thus . ~ FO-formula alphabetj0 ,DAg DAgAg = {Pi /qi | Pi /qi D, j n} set containing one distinct relation~)symbol Pij , agent j n relation symbol Pi D. take s0 (s,359fiB ELARDINELLI , L OMUSCIO & PATRIZI0 |= , s, s0 D(U ), every P j n, l (P ) = (P j )iff DAg DAgjAg~j00lj (Pi ) = DAg (Pi ).example, = {P } (thus DAg = {P j | j n}) action typeV0parameters, consider formula ~ = nj=0 xP j (x) P j (x), intuitively capturestransitions successor state predicate P contains elements Ucurrent state P .proved every transition relation represented discussed above, that,.given EP , size ||P|| = |S| + | | encoded AC-MAS P ||P|| |Act| |U |pmax23`qmax , where: pmax largest number parameters action type Act, `number relation symbols D, qmax largest arity symbols. correspondsP.doubly exponential bound ||P|| w.r.t. ||EP || = |U | + ||D|| + | |, ||D|| = Pk qk ,||E||4qk arity Pk . Specifically, ||P|| 232 P .carry complexity analysis basis input above; clearly resultsapply equally compact inputs AC programs presented Section 4.consider combined complexity input, is, ||EP || + ||||. saycombined complexity model checking finite AC-MAS FO-CTLK specificationsEXPSPACE-complete problem EXPSPACE, i.e., polynomial p(x)algorithm solving problem space bounded 2p(||EP ||+||||) , problem EXPSPACEhard, i.e., every EXPSPACE problem reduced model checking finite AC-MASFO-CTLK specifications.Theorem 3.20 model checking problem finite AC-MAS succinctly presentedFO-CTLK specifications EXPSPACE-complete.Proof. show problem EXPSPACE, recall ||P|| doubly exponentialw.r.t. size input, thus |S|. describe algorithm works NEXPSPACE;combines algorithm model checking first-order fragment FO-CTLKtemporal-epistemic fragment. Since NEXPSPACE = EXPSPACE, result follows. Given ACMAS P FO-CTLK formula , guess assignment check whether (P, s0 , ) |= .done induction according structure . atomic, check donepolynomial time w.r.t. size state evaluated on, is, exponential time w.r.t. ||EP ||.form x, apply algorithm model checking first-order (non-modal)logic, works PSPACE. Finally, outmost operator either temporal epistemicmodality, extend automata-based algorithm model check propositional CTL(Kupferman, Vardi, & Wolper, 2000; Lomuscio & Raimondi, 2006), works logarithmicspace |S|. However, remarked |S| generally doubly exponential ||EP ||. Thus,step performed space singly exponential ||EP ||. steps performedtime polynomial size . result, total combined complexity model checkingfinite AC-MAS NEXPSPACE = EXPSPACE.prove problem EXPSPACE-hard show reduction problemEXPSPACE. assume standard definitions Turing machines reductions (Papadimitriou,1994). problem EXPSPACE, exists deterministic Turing machineTA = hQ, , q0 , F, i, Q finite set states, machine alphabet, q0 Qinitial state, F set accepting states, transition function, solves usingspace 2p(|in|) given input in, polynomial function p. standard, assume360fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSrelation (QQD), = {L, R}, hq, c, q 0 , c0 , di representing transitionstate q state q 0 , characters c c0 read written respectively , head direction((L)eft (R)ight). Without loss generality, assume TA uses righthand halftape.TA in, build encoding EP = hD, U, s0 , AC-MAS P inducedsingle (environment) agent AE = hDE , ActE , P defined U = Q {0, 1}, where: (i)DE = {P/p(|in|) + 1, Q/1, H/p(|in|), F/1}; (ii) ActE singleton {E }, E parameterfree; (iii) E P (lE ) every lE D(U ). Intuitively, states P correspond configurations TA , mimics . define EP , let = DE . intended meaningpredicates follows: first p(|in|) elements P -tuple encode (in binaries)position non-blank cell, (p(|in|) + 1)-th element contains symbol appearingcell; Q contains current state q TA ; H contains position cell headcurrently on; F contains final states TA , i.e., F = F. initial state s0 representsinitial configuration TA , is, = in0 in` : s(Q) = {q0 }; s(H) = {h0, . . . , 0i};s(P ) = {hBIN(i), ini | {0, . . . , `}}, BIN(i) stands binary encoding p(|in|)bits integer i. Observe p(|in|) bits enough index (at most) 2p(|in|) cells usedTA .transition relation, define = {E }, (we avoid sub- superscriptspredicate symbols, i.e., = DAg ambiguity arise one agent):E =_(xF (x) F 0 (x))(1)hq,c,q 0 ,c0 ,diQ(q) (xQ(x) x = q) Q0 (q 0 ) (xQ0 (x) x = q 0 )(2)~p(H(~p) (xH(x) x = p~) (P (~p, c) (c = 2 xP (~p, x))))(3)00p~0 (d = R SUCC(~p, p~0 )) (d = L SUCC(p~0 , p~)) H (p~0 ) (xH (x) x = p~0 ) (4)(P 0 (~p, c0 ) (c0 6= 2)) (xP 0 (~p, x) x = c0 )00(5)0(~x, y(P (~x, y) (~x 6= p~) P (~x, y)) (~x, yP (~x, y) (P (~x, y) (~x = p~ = c )))) (6)Vp(|in|)symbol 2 represents content blank cells, SUCC(~x, x~0 ) = i=1 (x0i = 0x0i =Vi1V01) (x0i = 1 ((x0i = 0 i1j=1 xj = 1) (xi = 1 j=1 xj = 1))) formula capturingx~0 successor ~x, ~x x~0 interpreted p(|in|)-bit binary encodings integers (observe{0, 1} U ). formula obviously written polynomial time w.r.t. p(|in|), wellEP , particular s0 E . Formula E obtained disjunction subformulas,referring transition . subformula, i.e., transition hq, c, q 0 , c0 , di: line 1 expressesF , encodes final states machine, change along transition (thisformula could moved big disjunction); line 2 encodes machineexactly one state, q 0 , transition takes place; line 3 expresses symbol readhead c (possibly blank); line 4 captures head moves direction d; line 5 stateshead writes symbol c cell, moving; finally, line 6 states content tapechange, except cell head on.obtained transition function (s, E ) = s0 iff, (q, c) = (q 0 , c0 , d) TA ,that: s0 (P ) obtained s(P ) overwriting c0 (if blank) symbol position(p(|in|) + 1) tuple s(P ) beginning p(|in|)-tuple s(H) (that is, c definitionE ); updating s(H) according d, increasing decreasing value contains;361fiB ELARDINELLI , L OMUSCIO & PATRIZIsetting s0 (Q) = {q 0 }. predicate F change. Observe cells occurring Pinterpreted containing 2 2 written cell, cell simply removedP .checked that, starting = s0 , iteratively generating successor state s0according , i.e., s0 s.t. s0 |= E , one obtains (single) P-run representationcomputation TA in, pair consecutive P-states corresponds computationstep. particular, state, Q contains current state TA . clear =EF (xQ(x) F (x)) holds P iff TA accepts in. Thus, model checking P, checkwhether TA accepts in. completes proof EXPSPACE-hardness.Note result given terms data structures model, i.e., U D,state space itself. accounts high complexity model checking AC-MAS,state space doubly exponential size data. analysing refined boundsize ||P|| (||P|| |Act||U |pmax 23`qmax ), seen double exponential essentiallydue number parameters action types, number relation symbols occurring D,respective arities. Thus, fixed database schema set action types, resultingspace complexity reduced singly exponential.EXPSPACE-hardness indicates intractability, note expected givendealing quantified structures principle prone high complexity. Recallalso Section 3.3 size interpretation domain U 0 abstraction P 0 linearbound b, number constants Con, size , NAg . Hence, model checkingbounded uniform AC-MAS EXPSPACE-complete respect elements, whose sizegenerally small. Thus, believe several cases practical interest model checkingAC-MAS may entirely feasible.4. Artifact-Centric Programsfar developed formalism used specify reason temporalepistemic properties models representing artifact-centric systems. identified notableclass models admit finite abstractions. remarked Introduction, however, artifactsystems typically implemented declarative languages GSM (Hull et al., 2011).therefore interest investigate verification problem, Kripke semanticsAC-MAS, actual programs. discussed, GSM mainstream declarativelanguage artifact-centric environments, alternative declarative approaches exist. followssake generality ground discussion wide class declarative languagesdefine notion artifact-centric program. Intuitively, artifact-centric program (or ACprogram) declarative description whole multi-agent system, i.e., set services,interact artifact system (see discussion Introduction). Since artifact systemsalso typically implemented declaratively (Heath et al., 2013), AC programs used encodeartifact system agents system. also enables us importformalism previously discussed features views windows typical GSMlanguages.rest section organised follows. begin Subsection 4.1 defining ACprograms giving semantics terms AC-MAS. show AC-MASresults AC program uniform. long generated AC-MAS bounded, usingresults Section 3.3, deduce AC program admits AC-MAS finite model.362fiV ERIFICATION AGENT-BASED RTIFACT YSTEMScontext important give constructive procedures generation finite abstraction;provide procedure here. enables us state that, assumptions identify, ACprograms admit decidable verification means model checking finite model. Section 4.2ground exemplify constructions Order-to-Cash Scenario introducedSubsection 2.4.4.1 Verifying Artifact-Centric Programsstart defining abstract syntax AC programs.Definition 4.1 (AC Programs) artifact-centric program (or AC program) tuple ACP =hD, U, , i, where:programs database schema;U programs interpretation domain;= {0 , . . . , n } set agent programs = hDi , li0 , Acti i, where:Di agent database schema;li0 Di (U ) agent initial state (as database instance);Acti set local actions (~x), action name ~x actionparameter names; without loss generality, assume two action types useparameter names;local action (~x) associated precondition (~x) (~y ), i.e., FO-formulaDi , ~y ~x free variables.= {~ (~x) (~z) |~ (~x) = h1 (~x1 ), . . . , n (~xn )i Act1 Actn , ~x = h~x1 , . . . , ~xn i, ~z~x} represents AC programs transitions expressed set postconditions, i.e., FOformulas action parameters free variables. formulas definedj0 ,alphabet DAg DAgAg = {Pi /qi | Pi /qi D, j n} set containing onedistinct relation symbol Pij , agent j n relation symbol Pi D.AC programs defined modularly giving agents programs, including actionpreconditions postconditions. Notice preconditions use relation symbols localdatabase only, programs transitions refer local relations agents unconstrained way. precisely, postconditions global, i.e., associated globalactions, rather local ones. Indeed, formula ~ (~x) (~z) describes effects executionglobal action~ (~z) (under particular assignment parameters) agent executes(~zi ). reported below, accounts intuition choosing next action, agentrely information locally stored, actions, result mutual interactions, maychange local state agent, i.e., affect global state system. Obviously,prevent possibility specifying actions affect local states only. lineAC-MAS semantics literature interpreted systems.Given tuple ~x variables tuplue ~u elements U |~x| = |~u|, (~x) = ~udenote assignment binds i-th component~u i-th component~x. jointaction~ (~x) given above, let con(~) = con(i ) con(), var(~) = var(i )363fiB ELARDINELLI , L OMUSCIO & PATRIZIvar(), free(~) = ~x. execution~ (~x) ground parameters ~u U |~x| groundaction~ (~u), ~v (resp. w)~ obtained replacing yi (resp. zi ) value occurring~u position yi (resp. zi ) ~x. replacements make (~v ) (w)~ground, is, first-order sentences. Finally,definesetConconstantsmentionedACPAC program ACP , i.e., ConACP = ni=1 adom(li0 ) ~ Act con(~).semantics AC program given terms AC-MAS induced agentsprogram implicitly defines. Formally, captured following definition.Definition 4.2 (Induced Agents) Given AC program ACP = hD, U, , i, agent =hDi , Acti , P ri induced ACP interpretation domain U iff agent program= hDi , li0 , Acti that:every li Di (U ) ground action (~u) (~x) Acti , case(~u) P ri (li ) iff (li , ) |= (~x) (~y ), (~x) = ~u (recall ~y ~x).Note induced agents agents formalised Definition 2.6. Agents definedcomposed give AC-MAS associated AC program.Definition 4.3 (Induced AC-MAS) Given AC program ACP = hD, U, , set Ag ={A0 , . . . , } agents induced ACP , AC-MAS induced ACP tuple PACP =hAg, s0 , i, where:s0 = hl00 , . . . , ln0 initial global state;global transition function defined following condition: s0 (s,~ (~u)),000= hl0 , . . . , ln i, = hl0 , . . . , ln i,~ (~u) = h1 (~u0 ), . . . , n (~un )i, ~u = h~u0 , . . . , ~un i, iffevery {0, . . . , n},(li , ) |= (~xi ) (~yi ) (~xi ) = ~ui ;adom(s0 ) adom(s) ~u con(~ (~x) );0 , ) |=0(DAg DAgz ), assignment (~x) = ~u, DAg , DAg~ (~x) (~jDAg -instances that, every Pi j n, DAg (Pi ) = lj (Pi )0 (P j ) = l0 (P ).DAgjGiven AC program ACP , induced AC-MAS represents programs execution treeencodes data system. Intuitively, obtained iteratively executing state,starting initial one, possible ground actions. Observe actions performedenabled respective protocols transitions introduce bounded numbernew elements active domain, i.e., bound action parameters. followsAC programs parametric respect interpretation domain, i.e., replacinginterpretation domain obtain different AC-MAS.assume every program induces AC-MAS whose transition relation serial, i.e.,states always successors. basic requirement easily fulfilled, instance,assuming agent skip action trivially true preconditionagents execute skip, global state system remains unchanged. next Subsectionpresent example one program.significant feature AC programs induce uniform AC-MAS.364fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSLemma 4.4 Every AC-MAS P induced AC program ACP uniform.Proof. Since definition adom(s0 ) ConACP , Prop. 3.7 sufficient considertemporal transition relation . Consider s, s0 , s00 s000 L0 Lns0 ' s00 s000 witness . particular, every Aj Ag, lj00 ' lj lj000 ' lj0 , namely,lj00 = (lj ) lj000 = (lj0 ). Also, assume exists~ (~u) = h1 (~u1 ), . . . , n (~un )i Act(U )s0 (s,~ (~u)). First all, j (~uj ) P rj (lj ) implies (lj , j ) |= j (~xj ) (~yj )j (~xj ) = ~uj . Since lj00 ' lj , Prop. 3.3 (lj00 , j0 ) |= j (~xj ) (~yj ) j0 (~xi ) = 0 (~ui )0 extending ~ui . Thus, j (0 (~uj )) P rj (lj00 ) every j Ag. Further, assume0 , ) |=0 -instances obtained(DAg DAgz ), DAg , DAgAg~ (~x) (~00 , 000 00 (P j ) = l00 (P ) = (l (P )),(~x) = ~u. Consider DAg -instances DAgjjAgAg000 (P j ) = l000 (P ) = (l0 (P )). Since s0 ' s00 s000 , obtain0 ' 00 000DAgAgjjAgAgAg00 000 , 0 ) |=0 (~0 (~witness . particular, (DAg(~z),x)=u)0~(~x)Ag000000extending ~u. Finally, easily checked adom(s ) adom(s ) (~u) con(~ (~x) ).result, s000 (s00 ,~ (0 (~u))), i.e., P uniform.define means AC program satisfy specification, referringinduced AC-MAS.Definition 4.5 Given AC program ACP , FO-CTLK formula , assignment , sayACP satisfies , written (ACP, ) |= , iff (PACP , s0 , ) |= .Thus, model checking problem AC program specification defined termsmodel checking problem corresponding AC-MAS PACP .following result allows us reduce verification AC program infiniteinterpretation domain U1 , induces b-bounded AC-MAS,Pverification AC programfinite U2 . show constructed, let NACP = i{1,...,n} max(~x)i {|~x|}maximum number different parameters occur joint action ACP .Lemma 4.6 Consider AC program ACP1 = hD, U1 , operating infinite interpretationdomain U1 assume induced AC-MAS PACP1 = hAg1 , s10 , 1 b-bounded. Considerfinite interpretation domain U2 ConACP1 U2 |U2 | 2b + |ConACP1 | + NACP1AC program ACP2 = hD, U2 , i. Then, AC-MAS PACP2 = hAg2 , s20 , 2 inducedACP2 finite abstraction PACP1 .Proof. Let Ag1 Ag2 set agents induced respectively ACP1 ACP2 , accordingDef. 4.2. Firstly, prove set Ag1 Ag2 agents satisfy Def. 3.14, Ag = Ag1Ag 0 = Ag2 . end, observe ACP1 ACP2 differ U , Def. 4.2,= D0 , Act0 = Act. Thus, requirement 3 Def. 3.14 needs checked. this, fix{1, . . . , n} assume (~u) P ri (li ). Def. 4.2, (li , ) |= (~xi ) (~yi )(~xi ) = ~ui . assumption |U2 |, since con() ConACP1 U2 , |~u| NACP1 ,|adom(li )| b, define injective function : adom(li ) ~u ConACP1 7 U2identity ConACP1 . Thus, li0 = (li ), easily extract witness li ' li0 .Moreover, seen (y) = ~v 0 (y) = ~v 0 = (~v ) equivalent . Then,applying Prop. 3.3 li li0 , conclude (li0 , 0 ) |= (~xi ) (~yi ). Hence, Def. 4.2,(~u0 ) P ri0 (li0 ) ~u0 = (~u). So, shown right-to-left part requirement 3.left-to-right part shown similarly simply since U1 infinite.365fiB ELARDINELLI , L OMUSCIO & PATRIZIThus, proven Ag = Ag1 Ag 0 = Ag2 obtained Def. 3.14. Hence,assumption Ag Ag 0 Def. 3.15 fulfilled. show next also remaining requirements Def. 3.15 satisfied. Obviously, since ACP1 ACP2 , Def. 4.3,s10 = s20 , initial states PACP1 PACP2 same. remains showrequirements 1 2 satisfied. prove right-to-left part. end, take two states0 , . . . , l0 joint actions1 = hl10 , . . . , l1n i, s01 = hl10~ (~u) = h0 (~u0 ), . . . , n (~un )i11n0Act(U1 ) s1 1 (s1 ,~ (~u)). Consider s1 s01 . assumptions U2 , existsinjective function : adom(s1 )adom(s01 )~u ConACP1 7 U2 identity ConACP1 (re0 ), . . . , (l0 )icall |adom(s1 )|, |adom(s01 )| b). Then, s2 = h(l10 ), . . . , (l1n )i, s02 = h(l101n00S2 , extract witness s1 s1 ' s2 s2 . Moreover, seen every(~xi ) ~ (~x) , assignments (~x) = ~u 0 (~x) = ~u0 = (~u) equivalent respects1 s01 s2 s02 . Now, consider Def. 4.3 recall PACP1 PACP2 AC-MAS induced ACP1 ACP2 , respectively. applying Prop. 3.3, that, {0, . . . , n},0(i) ((l1i ), 0 ) |= (~xi ) (~yi ) iff (l1i , ) |= (~xi ) (~yi ); (ii) (DAg2 DAg, 0 ) |= ~ (~x) (~zi ) iff20(DAg1 DAg1 , ) |= ~ (~x) (~zi ), DAgi obtained si detailed Def. 4.3; (iii)adom(s01 ) adom(s1 ) ~u con(~ (~x) ) iff adom(s02 ) adom(s2 ) (~u) con(~ (~x) ) defi~ ((~u))). proved right-to-left partnition . then, case s02 2 (s2 ,second requirement Def. 3.15. direction follows similarly. Therefore, PACP2abstraction PACP1 .Intuitively, Lemma 4.6 shows following diagram commutes, [U1 /U2 ] standsreplacement U1 U2 definition ACP1 . Observe since U2 finite, oneactually apply Def. 4.3 obtain PACP2 ; particular transition function 2 computed.Instead, PACP1 , particular 1 , cannot directly computed ACP1 applying Def. 4.3,U1 infinite.ACP1Def. 4.3[U1 /U2 ]ACP2/ PACP1Def. 4.3Def. 3.15/ PACP2following result, direct consequence Lemma 3.17 Lemma 4.6, key conclusionsection.Theorem 4.7 Consider FO-CTLK formula , AC program ACP1 operating infinite interpretation domain U1 assume induced AC-MAS PACP1 b-bounded. Consider finite interpretation domain U2 CACP1 U2 |U2 | 2b + |CACP1 | + max{NACP1 , |var()|},AC program ACP2 = hD, U2 , i. that:ACP1 |= iff ACP2 |= .Proof. Lemma 4.6 PACP2 finite abstraction PACP1 . Moreover, |U2 | 2b +|ConACP1 | + max{NACP1 , |var()|} implies |U2 | 2b + |ConACP1 | + |var()|. Hence,apply Lemma 3.17 result follows.results shows generated AC-MAS model bounded, AC programverified model checking finite abstraction, i.e., bisimilar AC-MAS defined finite366fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSinterpretation domain. Note procedure constructive: given AC program ACP1 =hD, U1 , infinite domain U1 FO-CTLK formula , check whether ACP1 satisfiesspecification , first consider finite abstraction ACP2 = hD, U2 , defined finitedomain U2 satisfying cardinality requirement Theorem 4.7. Since U2 finite, inducedAC-MAS PACP2 also finite; hence apply standard model checking techniques verifywhether PACP2 satisfies . Finally, definition satisfaction AC programs Theorem 4.7,transfer result obtained decide model checking problem original infiniteAC program ACP1 specification .Also observe finite abstraction considered abstract interpretation domainU2 depends number distinct variables specification contains. Thus, principle,check program different specification 0 , one construct newabstraction PACP20 using different interpretation domain U20 , check 0 it. However,seen number distinct variables 0 exceed , abstractionPACP2 , used check , re-used 0 . Formally, let FO-CTLKk set FO-CTLKformulas containing k distinct variables. following corollary Theorem 4.7.Corollary 4.8 |U2 | 2b + |ConACP1 | + max{NACP1 , k}, then, every FO-CTLKk formula, ACP1 |= iff ACP2 |= .result holds particular k = NACP ; thus FO-CTLKNACP formulas,abstraction procedure specification-independent.Theorem 4.7 requires induced AC-MAS bounded, may seem difficult conditioncheck priori. Note however AC programs declarative. therefore straightforwardgive postconditions enforce transition generate states violating boundednessrequirement. scenario next Subsection exemplify this.4.2 Verifying Order-to-Cash ScenarioSection 2.4 introduced order-to-cash scenario (Hull et al., 2011), business process modelled artifact-centric system. show formalised within frameworkAC programs. sake simplicity assumed three agents scenario: onecustomer c, one manufacturer one supplier s. Further, database schema Di agent{c, m, s} given as:Customer c: Dc = {Products(pcode, budget), PO(id , pcode, offer , status)};Manufacturer m: Dm = {PO(id , pcode, offer , status), MO(id , pcode, price, status)};Supplier s: Ds = {Materials(mcode, cost), MO(id , pcode, price, status)}.Also, assumed initial state non-empty relations ProductsMaterials. Hence, artifact-centric program ACPotc corresponding order-to-cash scenariogiven formally follows:Definition 4.9 (ACPotc ) artifact-centric program ACPotc tuple hDotc , Uotc , otc , otc i,where:programs database schema Dotc interpretation domain Uotc definedSec. 2.4, i.e., Dotc = Dc Dm Ds = {PO/4, MO/4, Products/2, Materials/2}Uotc set alphanumeric strings.367fiB ELARDINELLI , L OMUSCIO & PATRIZIcreatePO(id,pcode)= b.Products(b, pcode)p, o, s.P O(id, p, o, s)doneMO(id)= pc, p.M O(id, pc, p, preparation)acceptMO(id)= pc, p.M O(id, pc, p, submitted)requires id fresh identifierPOs, newly created PO referexisting productrequires id refer existing MOcurrently preparationrequires id refer existingMO submittedTable 1: Preconditions actions createPO(id , pcode), doneMO(id ), acceptMO(id )= {c , , } set agent specifications customer c, manufacturersupplier s. Specifically, {c, m, s}, = hDi , li0 , Acti , that:Di agent database schema detailed above, i.e., Dc ={Products/2, PO/4}, Dm = {PO/4, MO/4}, Ds = {MO/4, Materials/2}.lc0 , lm0 , ls0 database instances Dc (Uotc ), Dm (Uotc ), Ds (Uotc ) respectively s.t. lc0 (Products) ls0 (Materials) non-empty, i.e., contain background information, lc0 (PO), lm0 (PO), lm0 (MO) ls0 (MO) empty.sets actions givenActc = {createPO(id , pcode), submitPO(id ), pay(id ), deletePO(id ), skip}.Actm = {createMO(id , price), doneMO(id ), shipPO(id ), deleteMO(id ), skip};Acts = {acceptMO(id ), rejectMO(id ), shipMO(id ), skip}.action (~x) associated precondition (~x) . preconditionsactions createPO(id , pcode), doneMO(id ), acceptMO(id ) reported Table 1.remaining preconditions omitted brevity.= {~ (~x) | (~x)) Actc Actm Acts },DAg = {P roductsc , P Oc , P Om , Om , aterialss , Os }.Table 2 illustrates postcondition joint action~ (id, pc, m1 , m2 ) = hcreatePO(id, pc), doneMO(m1 ), acceptMO(m2 )i.others omitted.postcondition Table 2 variables (from V ) constants (from U ) distinguished fontsv c, respectively. first two lines impose interpretation relations ProductsMaterials, occurring local database agents c (customer) (supplier), respectively,remain unchanged. third line states relation PO agents c (manufacturer)contains new procurement order, identifier id product code pc, takenparameters action createPO. Observe that, although executed customer, action affectsalso local state manufacturer. next 3 lines express local PO relation cm, addition newly added item, contains also all, only, items presentaction execution. next conjunct (3 lines) states new identifiers must unique withinlocal PO relation. Notice cannot guaranteed agent c executing createPO368fiV ERIFICATION AGENT-BASED RTIFACT YSTEMS~x.Products c (~x) Products c0 (~x)~y .Materials (~y ) Materials s0 (~y )cc0m0b.Products(pc,b)PO(id,pc,b,prepared)PO(id,pc,b,prepared)i, pc, b, s.i 6= id(PO c0 (i, pc, b, s) PO c (i, pc, b, s))(PO m0 (i, pc, b, s) PO c (i, pc, b, s))i, pc, b, s, pc0 , b0 , s0 .(PO c0 (i, pc, b, s) PO c0 (i, pc0 , b0 , s0 ) (pc = pc0 b = b0 = s0 ))(PO m0 (i, pc, b, s) PO m0 (i, pc0 , b0 , s0 ) (pc = pc0 b = b0 = s0 ))m1 = m2 m3 , pc, p, s.(MO (m3 , pc, p, s) MO m0 (m3 , pc, p, s))(MO (m3 , pc, p, s) MO s0 (m3 , pc, p, s))m1 6= m2 pc, p, s.MO (m1 , pc, p, s) (MO m0 (m1 , pc, p, s) MO m0 (m1 , pc, p, submitted)MO s0 (m1 , pc, p, s) MO s0 (m1 , pc, p, submitted))pc, p, s.MO (m2 , pc, p, s) (MO s0 (m2 , pc, p, s) MO s0 (m2 , pc, p, accepted)MO m0 (m2 , pc, p, s) MO m0 (m2 , pc, p, accepted))m3 , pc, p, s.m1 6= m2 m1 6= m3(MO m0 (m3 , pc, p, s) MO (m3 , pc, p, s))(MO s0 (m3 , pc, p, s) MO (m3 , pc, p, s))Table 2: postcondition ~ (id,pc,m1 ,m2 ) joint action~ (id, pc, m1 , m2 )hcreatePO(id, pc), doneMO(m1 ), acceptMO(m2 )i369=fiB ELARDINELLI , L OMUSCIO & PATRIZI(as cannot access relation PO m), value might actually returned automaticallysystem, used input agent. successive 3 lines state m1 m2 coincide,i.e., two distinct operations executed material order m1 , actioneffect local MO relation. contrary, successive 6 lines state, m1 6= m2local MO relations agent material order id m1 changes statesubmitted one id m2 accepted. Finally, last 3 lines state materialorders involved executed (joint) action propagated unchanged respective localrelations.Notice although actions typically conceived manipulate artifacts specific class,preconditions postconditions may depend artifact instances different classes.example, note action createMO manipulates MO artifacts, precondition dependsPO artifacts. Also, stress action executability depends status attributeartifact, actual data content whole database, i.e., artifacts.Similarly, action executions affect Wstatus attributes. importantly, using first-orderformulas b = x1 , . . . , xb+1 i6=j (xi = xj ) postcondition , guaranteeAC program question bounded therefore amenable abstraction methodologySection 4.define agents induced AC program ACPotc given according Definition 4.2.Definition 4.10 Given AC program ACPotc = hDotc , Uotc , otc i, agents Ac ,induced ACPotc defined follows:Ac = hDc , Actc , P rc i, (i) Dc above; (ii) Actc = c = {createPO, submitPO,pay, deletePO}; (iii) (~u) P rc (lc ) iff (lc , ) |= (~x) (~y ) (~x) = ~u.= hDm , Actm , P rm i, (i) Dm above; (ii) Actm = ={createMO, doneMO, shipPO, deleteMO}; (iii) (~u) P rm (lm ) iff (lm , ) |=(~x) (~y ) (~x) = ~u.= hDs , Acts , P rs i, (i) Ds above; (ii) Acts = = {acceptMO, rejectMO,shipMO}; (iii) (~u) P rs (ls ) iff (ls , ) |= (~x) (~y ) (~x) = ~u.Note agents Ac , strictly correspond agents defined Def. 2.12.particular, definition see createMO(id , price) P rm (lm )interpretation lm (P O) relation PO local state lm contains tuple hid, pc, o, preparedi product pc offer o; doneMO(mo id ) P rm (lm ) ifflm (M O) contains tuple id mo id status preparation. result, formal preconditions createMO doneMO satisfy intended meaning actions.define AC-MAS generated set agents Ag = {Ac , , } accordingDefinition 4.3.Definition 4.11 Given AC program ACPotc set Ag = {Ac , , } agents inducedACPotc , AC-MAS induced ACPotc tuple Potc = hAg, s0otc , otc i, where:s0otc = hlc0 , lm0 , ls0 initial global state, non-empty relationsProducts Materials;370fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSotc global transition function defined according Def. 4.3.AC-MAS generated AC program ACPotc corresponds closely AC-MAS appearing Def. 2.13. example give snippet transition function otc considering global action (~u) = hcreatePO(id, pcode), doneMO(m1 ), acceptMO(m2 )i enabledrespective protocols global state s. definition actions createPO(id, pcode),doneMO(m1 ), acceptMO(m2 ) li (s) P ri {c, m, s} impliesProducts relation contains information product pcode. Also, interpretation relation MO contains tuples hm1 , p, pr, preparationi hm2 , p0 , pr0 , submittedi prod(~u)ucts p p0 . definition otc follows every s0 Sotc , s0 implies0 , ) |=00(DAg DAg(~u) (id, pcode, m1 , m2 ), DAg DAg obtainedrenaming relation symbols, (~u) = hcreatePO(id, pcode), doneMO(m1 ), acceptMO(m2 )i,interpretation formal parameters id, pcode, m1 m2 Uotc . particu0lar, interpretation relation PO DAgextends DAg (P Oc ) DAg (P Om )tuple hid, pc, b, preparedi, id fresh identifier. tuples material orders0 (M ) (resp. 0 (M )) become hm , p, pr, submittedim1 m2 updated DAg1Ag00(resp. hm2 , p , pr , acceptedi). light specification (~u) action (~u), element updated transition. Finally, notice extensions indeed interpretationsPO MO s0 . Thus, semantics satisfies intended meaning actions. alsochecked that, line discussion Section 2.4, full version function otc giveneasily encode artifacts lifecycles given Figure 2.proceed exploit methodology Section 4 verify AC program ACPotp .use formula match Section 2.4 example specification; analogous resultsobtained formulas. Observe according Definition 4.3 AC-MAS inducedACPotp infinitely many states. assume two interpretations relations ProductsMaterials, determine initial state D0 . Consider maximum number max parametersconstants C operations c , . case analysismax = 2. earlier remarked formulas b postcondition actions forceAC-MAS Potc corresponding ACPotc bounded. Potc b-bounded.According Corollary 3.19, therefore consider finite domain U 0U 0 D0 C con(match )D0 (Products) D0 (Materials) C|U 0 | 2b + |D0 | + |C | + |con(match )| + max= 2b + |D0 | + |C | + 2instance, consider subset U 0 Uotc satisfying conditions above. Given U 0satisfies hypothesis Theorem 4.7, follows AC program ACPotc Uotc satisfiesmatch ACPotc U 0 does. AC-MAS induced latter finite-statesystem, constructively built running AC program ACPotc elementsU 0 . Thus, ACPotc |= match decidable instance model checking therefore solvedmeans standard techniques.371fiB ELARDINELLI , L OMUSCIO & PATRIZImanual check finite model indeed reveals match , budget cost satisfiedfinite model, whereas fulfil not. Corollary 3.19 AC-MAS Potc induced ACPotpsatisfies specifications. Hence, view Definition 4.5, conclude artifactcentric program ACPotp satisfies match , budget cost satisfy fulfil .line intuitions scenario.5. Conclusions Future Workpaper put forward methodology verifying agent-based artifact-centric systems.proposed AC-MAS, novel semantics incorporating first-order features, used reason multi-agent systems artifact-centric setting. observed model checkingproblem structures specifications given first-order temporal-epistemic logicundecidable proceeded identify suitable fragment decidability retained.Specifically, showed class bounded, uniform AC-MAS identified admit finite abstractions preserve first-order specification language introduced. Previous resultsliterature, discussed Subsection 1.2, limit preservation fragments quantified languageallow interplay first-order quantifiers modalities.explored complexity model checking problem context showedEXPSPACE-complete. obviously hard problem, need considerfirst-order structures normally lead problems high complexity. noteabstract interpretation domain actually linear size bound considered.Mindful practical needs verification artifact-centric systems, exploredfinite abstractions actually built. end, rather investigating one specific datacentric language, defined general class declarative artifact-centric programs. showedsystems admit uniform AC-MAS semantics. assumption boundedsystems showed model checking multi-agent system programs decidable gaveconstructive procedure operating bisimilar, finite models. results general,instantiated various artifact-centric languages. instance, Belardinelli et al. (2012b)explore finite abstractions GSM programs using results.exemplified methodology put forward use case consisting several agents purchasing delivering products. system infinitely many states showed admitsfinite abstraction used verify variety specifications system.question left open present paper whether uniform condition provided tight.showed sufficient condition, explore whether necessaryfinite abstractions whether general properties given. context interestartifact-centric programs generate uniform structures. Also, worthwhile explorewhether notion related uniformity applied domains AI, example retaindecidability specific calculi. would appear case preliminary studiesSituation Calculus demonstrate (De Giacomo, Lesperance, & Patrizi, 2012).application side, also interested exploring ways use results paperbuild model checker artifact-centric MAS. Previous efforts area (Gonzalez, Griesmayer,& Lomuscio, 2012) limited finite state systems. would therefore great interestconstruct finite abstractions fly check practical e-commerce scenarios onediscussed.372fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSReferencesAbiteboul, S., Hull, R., & Vianu, V. (1995). Foundations Databases. Addison-Wesley.Alonso, G., Casati, F., Kuno, H. A., & Machiraju, V. (2004). Web Services - Concepts, ArchitecturesApplications. Data-Centric Systems Applications. Springer.Alves, A., Arkin, A., Askary, S., Barreto, C., Ben, Curbera, F., Ford, M., Goland, Y., Guzar, A.,Kartha, N., Liu, C. K., Khalaf, R., Konig, D., Marin, M., Mehta, V., Thatte, S., van der Rijn,D., Yendluri, P., & Yiu, A. (2007). Web Services Business Process Execution Language Version 2.0. Tech. rep., OASIS Web Services Business Process Execution Language (WSBPEL)TC.Bagheri Hariri, B., Calvanese, D., De Giacomo, G., Deutsch, A., & Montali, M. (2013). VerificationRelational Data-centric Dynamic Systems External Services. Hull, R., & Fan, W.(Eds.), Proceedings 32nd ACM SIGMOD-SIGACT-SIGART Symposium PrinciplesDatabase Systems (PODS13), pp. 163174. ACM.Baresi, L., Bianculli, D., Ghezzi, C., Guinea, S., & Spoletini, P. (2007). Validation Web ServiceCompositions. IET Software, 1(6), 219232.Baukus, K., & van der Meyden, R. (2004). knowledge based analysis cache coherence.Davies, J., Schulte, W., & Barnett, M. (Eds.), Proceedings 6th International Conference Formal Engineering Methods (ICFEM04), Vol. 3308 Lecture Notes ComputerScience, pp. 99114. Springer.Belardinelli, F., & Lomuscio, A. (2012). Interactions Knowledge Time First-OrderLogic Multi-Agent Systems: Completeness Results. Journal Artificial Intelligence Research, 45, 145.Belardinelli, F., Lomuscio, A., & Patrizi, F. (2011a). Computationally-Grounded SemanticsArtifact-Centric Systems Abstraction Results. Walsh, T. (Ed.), Proceedings 22ndInternational Joint Conference Artificial Intelligence (IJCAI12), pp. 738743. AAAI.Belardinelli, F., Lomuscio, A., & Patrizi, F. (2011b). Verification Deployed Artifact Systems viaData Abstraction. Kappel, G., Maamar, Z., & Nezhad, H. R. M. (Eds.), Proceedings9th International Conference Service-Oriented Computing (ICSOC11), Vol. 7084Lecture Notes Computer Science, pp. 142156. Springer.Belardinelli, F., Lomuscio, A., & Patrizi, F. (2012a). Abstraction Technique VerificationArtifact-Centric Systems. Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), Proceedings13th International Conference Principles Knowledge Representation Reasoning(KR12). AAAI.Belardinelli, F., Lomuscio, A., & Patrizi, F. (2012b). Verification gsm-based artifact-centricsystems finite abstraction. Liu, C., Ludwig, H., Toumani, F., & Yu, Q. (Eds.), Proceedings 10th International Conference Service-Oriented Computing (ICSOC12),Vol. 7636 Lecture Notes Computer Science, pp. 1731. Springer.Berardi, D., Calvanese, D., De Giacomo, G., Hull, R., & Mecella, M. (2005). Automatic Composition Transition-based Semantic Web Services Messaging. Bohm, K., Jensen,C. S., Haas, L. M., Kersten, M. L., Larson, P.-A., & Ooi, B. C. (Eds.), Proceedings 31stInternational Conference Large Data Bases (VLDB05), pp. 613624. ACM.373fiB ELARDINELLI , L OMUSCIO & PATRIZIBerardi, D., Cheikh, F., Giacomo, G. D., & Patrizi, F. (2008). Automatic Service Composition viaSimulation. International Journal Foundations Computer Science, 19(2), 429451.Bertoli, P., Pistore, M., & Traverso, P. (2010). Automated Composition Web Services via Planning Asynchronous Domains. Artificial Intelligence, 174(3-4), 316361.Bhattacharya, K., Gerede, C. E., Hull, R., Liu, R., & Su, J. (2007). Towards Formal AnalysisArtifact-Centric Business Process Models. Alonso, G., Dadam, P., & Rosemann, M.(Eds.), Proceedings 5th International Conference Business Process Management(BPM07), Vol. 4714 Lecture Notes Computer Science, pp. 288304. Springer.Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal Logic, Vol. 53 Cambridge TractsTheoretical Computer Science. Cambridge University Press.Calvanese, D., De Giacomo, G., Lenzerini, M., Mecella, M., & Patrizi, F. (2008). Automatic ServiceComposition Synthesis: Roman Model. IEEE Data Engineering Bulletin, 31(3), 1822.Ciobaca, S., Delaune, S., & Kremer, S. (2012). Computing Knowledge Security ProtocolsConvergent Equational Theories. Journal Automated Reasoning, 48(2), 219262.Clarke, E. M., Grumberg, O., & Peled, D. A. (1999). Model Checking. MIT Press.Cohn, D., & Hull, R. (2009). Business Artifacts: Data-Centric Approach Modeling BusinessOperations Processes. IEEE Data Engineering Bulletin, 32(3), 39.Damaggio, E., Deutsch, A., & Vianu, V. (2012). Artifact Systems Data DependenciesArithmetic. ACM Transactions Database Systems, 37(3), 22:122:36.Damaggio, E., Hull, R., & Vaculn, R. (2011). Equivalence Incremental FixpointSemantics Business Artifacts Guard-Stage-Milestone Lifecycles. Rinderle-Ma, S.,Toumani, F., & Wolf, K. (Eds.), Proceedings 9th International Conference BusinessProcess Management (BPM11), Vol. 6896 Lecture Notes Computer Science, pp. 396412. Springer.De Giacomo, G., Lesperance, Y., & Patrizi, F. (2012). Bounded Situation Calculus Action TheoriesDecidable Verification. Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), Proceedings13th International Conference Principles Knowledge Representation Reasoning(KR12). AAAI.Dechesne, F., & Wang, Y. (2010). Know Know: Epistemic Approaches SecurityProtocol Verification. Synthese, 177(Supplement-1), 5176.Deutsch, A., Hull, R., Patrizi, F., & Vianu, V. (2009). Automatic Verification Data-centric Business Processes. Fagin, R. (Ed.), Proceedings 12th International ConferenceDatabase Theory (ICDT09), Vol. 361 ACM International Conference Proceeding Series,pp. 252267. ACM.Deutsch, A., Sui, L., & Vianu, V. (2007). Specification Verification Data-Driven Web Applications. Journal Computer System Sciences, 73(3), 442474.Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning Knowledge. MITPress.374fiV ERIFICATION AGENT-BASED RTIFACT YSTEMSGammie, P., & van der Meyden, R. (2004). MCK: Model Checking Logic Knowledge.Alur, R., & Peled, D. (Eds.), Proceedings 16th International Conference ComputerAided Verification (CAV04), Vol. 3114 Lecture Notes Computer Science, pp. 479483.Springer.Gerede, C. E., & Su, J. (2007). Specification Verification Artifact Behaviors BusinessProcess Models. Kramer, B. J., Lin, K.-J., & Narasimhan, P. (Eds.), Proceedings 5thInternational Conference Service-Oriented Computing (ICSOC07), Vol. 4749 LectureNotes Computer Science, pp. 181192. Springer.Gonzalez, P., Griesmayer, A., & Lomuscio, A. (2012). Verifying GSM-Based Business Artifacts.Goble, C. A., Chen, P. P., & Zhang, J. (Eds.), Proceedings 19th IEEE InternationalConference Web Services (ICWS12), pp. 2532. IEEE.Grohe, M. (2001). Generalized Model-Checking Problems First-Order Logic. Ferreira, A.,& Reichel, H. (Eds.), Proceedings 18th Annual Symposium Theoretical AspectsComputer Science (STACS01), Vol. 2010 Lecture Notes Computer Science, pp. 1226.Springer.Heath, F. T., Boaz, D., Gupta, M., Vaculn, R., Sun, Y., Hull, R., & Limonad, L. (2013). Barcelona:Design Runtime Environment Declarative Artifact-Centric BPM. Basu, S., Pautasso, C., Zhang, L., & Fu, X. (Eds.), Proceedings 11th International ConferenceService-Oriented Computing (ICSOC13), Vol. 8274 Lecture Notes Computer Science,pp. 705709. Springer.Hull, R. (2008). Artifact-Centric Business Process Models: Brief Survey Research ResultsChallenges. Meersman, R., & Tari, Z. (Eds.), Proceedings (part II) Confederated International Conferences, CoopIS, DOA, GADA, IS, ODBASE 2008 (On MoveMeaningful Internet Systems: OTM08), Vol. 5332 Lecture Notes Computer Science,pp. 11521163. Springer.Hull, R., Damaggio, E., De Masellis, R., Fournier, F., Gupta, M., Heath, III, F. T., Hobson, S., Linehan, M., Maradugu, S., Nigam, A., Sukaviriya, P. N., & Vaculin, R. (2011). Business ArtifactsGuard-Stage-Milestone Lifecycles: Managing Artifact Interactions ConditionsEvents. Eyers, D. M., Etzion, O., Gal, A., Zdonik, S. B., & Vincent, P. (Eds.), Proceedings5th ACM International Conference Distributed Event-Based Systems (DEBS11),pp. 5162. ACM.Hull, R., Narendra, N. C., & Nigam, A. (2009). Facilitating Workflow Interoperation Using ArtifactCentric Hubs. Baresi, L., Chi, C.-H., & Suzuki, J. (Eds.), Proceedings 7th International Conference Service-Oriented Computing (ICSOC-ServiceWave 09), Vol. 5900Lecture Notes Computer Science, pp. 118. Springer.Kacprzak, M., Nabialek, W., Niewiadomski, A., Penczek, W., Polrola, A., Szreter, M., Wozna, B.,& Zbrzezny, A. (2008). VerICS 2007 - Model Checker Knowledge Real-Time.Fundamenta Informaticae, 85(1-4), 313328.Kupferman, O., Vardi, M. Y., & Wolper, P. (2000). Automata-Theoretic Approach BranchingTime Model Checking. Journal ACM, 47(2), 312360.Lomuscio, A., Penczek, W., Solanki, M., & Szreter, M. (2011). Runtime Monitoring ContractRegulated Web Services. Fundamenta Informaticae, 111(3), 339355.375fiB ELARDINELLI , L OMUSCIO & PATRIZILomuscio, A., Qu, H., & Raimondi, F. (2009). MCMAS: Model Checker VerificationMulti-Agent Systems. Bouajjani, A., & Maler, O. (Eds.), Proceedings 21st International Conference Computer Aided Verification (CAV09), Vol. 5643 Lecture NotesComputer Science, pp. 682688. Springer.Lomuscio, A., Qu, H., & Solanki, M. (2012). Towards Verifying Contract Regulated Service Composition. Autonomous Agents Multi-Agent Systems, 24(3), 345373.Lomuscio, A., & Raimondi, F. (2006). Complexity Model Checking Concurrent ProgramsCTLK Specifications. Baldoni, M., & Endriss, U. (Eds.), Proceedings 4thInternational Workshop Declarative Agent Languages Technologies (DALT06), Selected, Revised Invited Papers, Vol. 4327 Lecture Notes Computer Science, pp.2942. Springer.Marin, M., Hull, R., & Vaculn, R. (2013). Data Centric BPM Emerging Case ManagementStandard: Short Survey. La Rosa, M., & Soffer, P. (Eds.), Proceedings BusinessProcess Management Workshops - BPM 2012 International Workshops. Revised Papers, Vol.132 Lecture Notes Business Information Processing, pp. 2430. Springer.Meyer, J.-J. C., & van der Hoek, W. (1995). Epistemic Logic AI Computer Science, Vol. 41Cambridge Tracts Theoretical Computer Science. Cambridge University Press.Nigam, A., & Caswell, N. S. (2003). Business Artifacts: Approach Operational Specification.IBM Systems Journal, 42(3), 428445.Nooijen, E., Fahland, D., & Dongen, B. V. (2013). Automatic Discovery Data-CentricArtifact-Centric Processes. La Rosa, M., & Soffer, P. (Eds.), Proceedings BusinessProcess Management Workshops - BPM 2012 International Workshops. Revised Papers, Vol.132 Lecture Notes Business Information Processing, pp. 316327. Springer.Papadimitriou, C. H. (1994). Computational complexity. Addison-Wesley.Parikh, R., & Ramanujam, R. (1985). Distributed Processes Logic Knowledge. Parikh,R. (Ed.), Logic Programs, Vol. 193 Lecture Notes Computer Science, pp. 256268.Springer.Singh, M. P., & Huhns, M. N. (2005). Service-Oriented Computing: Semantics, Processes, Agents.John Wiley & Sons.Wooldridge, M. (2000). Computationally Grounded Theories Agency. Proceedings 4thInternational Conference Multi-Agent Systems (ICMAS00), pp. 1322. IEEE.Wooldridge, M. (2001). Introduction Multiagent Systems. John Wiley & Sons.376fiJournal Artificial Intelligence Research 51 (2014) 133164Submitted 05/14; published 09/14Text Rewriting Improves Semantic Role LabelingKristian WoodsendMirella Lapatak.woodsend@ed.ac.ukmlap@inf.ed.ac.ukInstitute Language, Cognition ComputationSchool Informatics, University Edinburgh10 Crichton Street, Edinburgh EH8 9ABAbstractLarge-scale annotated corpora prerequisite developing high-performance NLPsystems. corpora expensive produce, limited size, often demanding linguisticexpertise. paper use text rewriting means increasing amount labeleddata available model training. method uses automatically extracted rewrite rulescomparable corpora bitexts generate multiple versions sentences annotatedgold standard labels. apply idea semantic role labeling showmodel trained rewritten data outperforms state art CoNLL-2009benchmark dataset.1. IntroductionRecent years witnessed increased interest automatic identification labelingsemantic roles conveyed sentential constituents (Gildea & Jurafsky, 2002).goal semantic role labeling task discover relations holdpredicate arguments given input sentence (e.g., whom,when, where, how).(1)[Mrs. Yeargin]A0 [gave]V [the questions answers]A1 [two daysexamination]TMP [two low-ability geography classes]ARG2 .sentence (1), A0 represents Agent giver, A1 represents theme thing given,A2 represents Recipient, TMP temporal modifier indicating action tookplace, V determines boundaries predicate. semantic roles examplelabeled style PropBank (Palmer, Gildea, & Kingsbury, 2005), broad-coveragehuman-annotated corpus semantic roles syntactic realizations. PropBank annotation framework predicate associated set core roles (named A0,A1, A2, on) whose interpretations specific predicate1 set adjunctroles location time whose interpretation common across predicates (e.g., twodays examination sentence (1) above).type semantic information shallow relatively straightforward infer automatically useful development broad coverage, domain-independent languageunderstanding systems. Indeed, analysis produced existing semantic role labelersshown benefit wide spectrum applications ranging information extraction(Surdeanu, Harabagiu, Williams, & Aarseth, 2003) question answering (Shen & Lapata,1. precisely, A0 A1 common interpretation across predicates proto-agent protopatient sense described Dowty (1991).c2014AI Access Foundation. rights reserved.fiWoodsend & LapataSource1. retreating guerrillas soon pursued government forces.2. survey conducted Gallup Polllast summer indicated one fourAmericans takes cues stars believes ghosts.3. examiner kind let student finish lunch.4. didnt know rules,died.5. Mexico City, biggest city world,many interesting archaeological sites.6. arrival train unexpected.TargetGovernment forces soon pursued retreating guerrillas.survey conducted GallupPoll last summer. indicated onefour Americans takes cues starsbelieves ghosts.kind examiner let student finishlunch.died, didnt knowrules.Mexico City many interesting archaeological sites.trains arrival unexpected.Table 1: Examples syntactic rewriting.2007), machine translation (Wu & Fung, 2009) summarization (Melli, Wang, Liu,Kashani, Shi, Gu, Sarkar, & Popowich, 2005).SRL systems date conceptualize semantic role labeling task supervisedlearning problem rely role-annotated data model training. Supervised methodsdeliver reasonably good performance, F1-scores low eighties standard testcollections English. rely primarily syntactic features (such path features)order identify classify roles. mixed blessing pathargument predicate informative also quite complicated. Many pathsparse tree likely occur relatively small number times (or all)resulting sparse information classifier learn from. Even trainingdata includes examples specific predicate set arguments, unless test sentencecontains syntactic structure, far classifier concerned,labeling items within two sentences unrelated.idea use use rewrite rules order create several syntactic variantssentence, thus alleviating training requirements semantic role labeling. Rewriterules typically synchronous grammar rules defining sequence source terminalsnonterminals rewrites sequence target terminals nonterminals. rulesoften extracted monolingual corpora containing parallel translationssource text (Barzilay & McKeown, 2001; Pang, Knight, & Marcu, 2003), bilingualcorpora consisting documents translations (Bannard & Callison-Burch, 2005a;Callison-Burch, 2007), comparable corpora Wikipedia revision histories (Coster& Kauchak, 2011; Woodsend & Lapata, 2011). Examples rewrites given Table 1.include transforming passive active sentences (see sentence pair (1) Table 1),splitting long complicated sentence several shorter ones (see (2) Table 1),removing redundant parts sentence (see (3) Table 1), reordering parts sentence(see (4) Table 1), deleting appositives (see (5) Table 1), transforming prepositionalphrase genitive (see (6) Table 1), on.134fiText Rewriting Improves Semantic Role Labelingautomatically extract syntactic rewrite rules corpora use generatemultiple versions role annotated sentences whilst preserving original semantic roles.therefore expand training data wide range syntactic variationspredicate-argument combination learn semantic role labeler expandeddataset. approach describe essentially increases size training datacreating many different syntactic variations different predicates roles.Rewrite rules previously deployed variety text-to-text generation applications ranging summarisation (Galley & McKeown, 2007; Yamangil & Nelken, 2008;Cohn & Lapata, 2009; Ganitkevitch, Callison-Burch, Napoles, & Van Durme, 2011),question answering (Wang, Smith, & Mitamura, 2007), information retrieval (Park, Croft,& Smith, 2011), simplification (Zhu, Bernhard, & Gurevych, 2010; Woodsend & Lapata,2011; Feblowitz & Kauchak, 2013), machine translation (Callison-Burch, 2008; Marton, Callison-Burch, & Resnik, 2009; Ganitkevitch, Cao, Weese, Post, & Callison-Burch,2012). However, application text rewriting means increasing amountlabeled data available model training novel knowledge. show experimentally, syntactic transformations improve SRL performance beyond state artusing CoNLL-2009 benchmark dataset best scoring system (Bjorkelund,Hafdell, & Nugues, 2009). Importantly, approach used combinationSRL learner role-annotated data. Moreover, specifically tied SRL taskemployed learning model dataset. Rewrite rules could used expandtraining data tasks make use syntactic features semantic parsing(Kwiatkowski, 2012) textual entailment (Mehdad, Negri, & Federico, 2010; Wang &Manning, 2010).following, present overview related work (Section 2) describerewrite rules automatically extracted filtered correctness (Section 3). Section 4 details experiments Section 5 reports results.2. Related Workidea transforming sentences make amenable NLP technology datesback Chandrasekar, Doran, Srinivas (1996) argue simpler sentences woulddecrease likelihood incorrect parse. end, employ mostly hand-craftedsyntactic rules aimed splitting long complicated sentences simpler ones. Klebanov, Knight, Marcu (2004) preprocess texts Easy Access Sentences, i.e., sentencesconsisting one finite verb dependents order facilitate information seeking applications summarization information retrieval accessing factual information.similar vein, Vickrey Koller (2008) devise large number hand-written rulesorder simplify sentences semantic role labeling. present log-linear modeljointly learns select best simplification (out possibly exponential spacecandidates) role labeling. Kundu Roth (2011) use textual transformationsdomain adaptation. Rather training new model out-of-domain data, proposerewrite out-of-domain text similar training domain.pilot idea semantic role labeling using hand-written rewrite rules showcompares favorably approaches retrain model target domain.135fiWoodsend & Lapatawork focused idea automatically expanding data availablegiven task without, however, applying transformations. Furstenau Lapata (2012)combine labeled unlabeled data projecting semantic role annotations labeledsource sentence onto unlabeled target sentence. find novel instances classifiertraining based lexical structural similarity manually labeled seed instances.Zanzotto Pennacchiotti (2010) increase datasets textual entailment miningWikipedia revision histories.Contrary previous work, automatically extract general rewrite rules various data sources including Wikipedia revision histories, comparable articles, bilingualcorpora. Given sentence (semantic role) annotated data, create several syntactic transformations, many may erroneous. maintain modeltraining transformations whose role labels preserved syntactic rewrite.identify transformations label-preserving automatically, without requiringSRL-specific knowledge. approach differs Vickrey Koller (2008)three important aspects: (a) employ automatic rules simplification specific, (b) attempt select best rewrite, transformations preservegold standard role labels used training (c) model jointlyrewrites sentences labels semantic roles; rewrite training dataavailable model SRL task. work shares KunduRoth (2011) idea transforming sentences preserving gold standardrole labels. However, transform test data make look like trainingdata. unavoidably requires specialized knowledge differences twodomains, general model have.mentioned earlier, use synchronous grammars extract set possible syntactic rewrites. Synchronous context-free grammars (SCFGs; Aho & Ullman, 1969)generalization context-free grammar (CFG) formalism simultaneously producestrings two languages. used extensively syntax-based statistical MT(Wu, 1997; Yamada & Knight, 2001; Chiang, 2007; Graehl & Knight, 2004) relatedgeneration tasks sentence compression (Galley & McKeown, 2007; Cohn & Lapata,2009, 2013; Ganitkevitch et al., 2011), sentence simplification (Zhu et al., 2010; Feblowitz& Kauchak, 2013; Woodsend & Lapata, 2011), summarization (Woodsend & Lapata,2012). Rather focusing one type transformation (e.g., simplification compression), learn full spectrum rewrite operations select rules appropriatetask hand. Furthermore, results show rewrite rules improve semantic rolelabeling performance across board, irrespectively specific variant synchronousgrammar corpus used. experiment conventional (weighted) SCFGs (Aho &Ullman, 1969) tree substitution grammars (Eisner, 2003) employ transformationrules extracted Wikipedia revision histories (Zhu et al., 2010; Woodsend & Lapata,2011) bitexts (Ganitkevitch, Van Durme, & Callison-Burch, 2013).3. Methodsection describe general idea behind algorithm movepresent specific implementation. define transformation g functionmaps example sentence modified sentence s0 . Let G set known136fiText Rewriting Improves Semantic Role Labelingtransformation functions, G = {g1 , g2 , . . . , gn }. Suppose labels associatedexample s. context paper, semantic role labels. Labels coulddefined spans tokens, use CoNLL 20089 formalismhead word span labelled. transformation function therefore mappingtokens sentence tokens t0 s0 . require mappinginvolves tokens s0 , require mappings one-to-one.label-preserving transformation transformation gi mapping (some the)tokens example tokens t0 s0 , (correct) labels t0 identicallabels source tokens token mappings defined gi . words,labels could preserved, preserved, others introduced.Let G set label-preserving transformation functions, G G. problemaddress paper therefore two-fold: Firstly, find automatically setpossible transformation functions G due automated nature unavoidablyerror-prone process. Secondly, identify (again automatically) transformations Gactually label-preserving specifically, transformations rewritetraining instance s0 varying syntactic structure, yet preservesemantic roles arguments appear new version s0 .Algorithm 1 describes approach boils three steps: (a) extractingtransformations, (b) refining transformations, (c) generating labeling extendedcorpus. standard gold annotated corpus used train initial semantic role labelingmodel (see lines 12 Algorithm 1). Meanwhile, set candidate transformation functions G extracted suitable comparable parallel corpus (line 3). fullset transformation functions used rewrite gold corpus, creating much extendedcorpus inevitably contain grammatically semantically incorrect sentences.extended corpus next automatically labeled using original SRL model preprocessing normal SRL pipeline (whose details discuss Section 4.2), withoutknowledge transformation functions involved.could theory use extended corpus basis training SRLmodel. However, contain many errors, unlikely yield useful informationguide model. One approach could manually correct rewritesgenerated automatically, would time resource-intensive. Instead,corrections automatically, create extended corpus rewritesimpair quality training data. therefore learn rules yield accuraterewrites, i.e., rewrites preserve labels gold-standard. intuitionthat, given large number possible rewrites, SRL model general labelaccurate rewrites correctly mis-label erroneous sentences, due findingconfusing. thus compare semantic role labels produced modellabels corresponding predicate-argument pairs gold corpus, providesamples train binary classifier (here SVM) learns predict rewriteslikely successful problematic (lines 1119 Algorithm 1).rewritten sentence classed positive sample SRL model able labeltransformation standard better able label originalsentence, i.e. labels SRL model predicts transformed sentence matchpredicted original, corrected respect mappedgold labels. If, however, semantic role longer predicted correctly, missed,137fiWoodsend & LapataAlgorithm 1 Learn SRL model Mextended extending gold training corpus Cgoldtransformation functions G.1: Mgold SRL model trained Cgold2: Cmodel label Cgold using MgoldExtract transformations:3: G transformation functions extracted pairs aligned sentencescomparable corpora4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:27:Refine transformations:initialize SVM training data DSVMsentences Cgoldg applicable transformations Gs0 rewrite using glabel s0 using MgoldSRL labels s0 match labels Cgold equivalent Cmodeltrueelsefalseendadd (s0 , y) DSVMendendtrain SVM using DSVMG {g G : g positive SVM weight}Generate extended corpus:initialize refined rewrite corpus Crefinedsentences Cgoldg applicable transformations Gs0 rewrite using gproject labels s0 using gadd s0 CrefinedendendCextended Cgold CrefinedRetrain SRL model:Mextended SRL model trained Cextended29: return Mextended28:138fiText Rewriting Improves Semantic Role Labelingerroneous role introduced, classified negative sample, sample likelyharm training new SRL model.SVM identified refined set transformation functions G (line 20),transformations used create extended training corpus. time, knowledgetransformation function involved project labels correspond originalgold corpus (lines 2128). case SRL, labels describe predicatearguments. extended corpus supplements original gold standard corpus (line 29),combination used create SRL model (line 30).worth noting method impinge actual process learningSRL model, concerned preparation training data. therefore believeapplied range SRL modeling approaches, gains performanceachieve largely orthogonal could made improving aspectslearning process (see Section 5.3 empirical evidence).3.1 Learning TransformationsConceptually wide range text-rewriting transformation functions could includedset G, paraphrasing, simplification translation another language. Here,focus transformation functions expressed synchronous context-freegrammars (Aho & Ullman, 1969). Synchronous rules operate parse tree constituentscontext-free manner, typically modify syntax. transformations considersub-categorized into:1. Statement extraction. Constituents sub-tree parse tree identified, extracted context rewritten complete sentence, typically shortersimpler, although necessarily so.2. Compression. original sentence rewritten compressing constituentsparse tree, typically deleting nodes.3. Insertion. New elements added parse tree. significant chunks newtext would semantic role information own, practice insertionsoften additional punctuation clarify scope phrases, simple structure. . . . aid statement extraction.4. Substitution. using lexicalized synchronous grammar, text replacednew text, paraphrases represented.obtain set possible transformations G monolingual comparable corporadrawn Wikipedia bitexts (see Section 4 details). following describegrammar formalisms resources consider.3.1.1 Transformations Monolingual Corporaextract transformation rules corpora broadly comparable, usingunsupervised process. corpora constructed Wikipedia revision histories,comparable Wikipedia articles. result, cannot guaranteed aligned sourcetarget sentences truly related, expected source sentence139fiWoodsend & Lapatafully generate target sentence. practice means addition requiringstrictly synchronous structure source target sentences, cannot assumealignment source target root nodes, require surjective alignmenttarget nodes nodes source parse tree. able describe structural mismatchesnon-isomorphic tree pairs (the grammar rules comprise trees arbitrary depth,fragments mapped) represent transformation functions using synchronoustree substitution grammar formalism (Eisner, 2003).synchronous tree-substitution grammar (STSG) defines space valid pairssource target parse trees. Rules specify map tree fragments source parsetree fragments target tree, recursively free context. Following CohnLapata (2009), STSG 7-tuple, G = (NS , NT , , , P, RS , RT ) Nnon-terminals terminals, subscripts indicating sourcetarget respectively. P productions RS NS RT NT distinguishedroot symbols.Typically, production rewrite rule two aligned non-terminals X NSNT source target:hX, h, , i,elementary trees rooted symbols X respectively.synchronous context free grammar would limited one level elementarytrees, STSG imposes limits elementary trees arbitrarily deep.one-to-one alignment frontier nodes (non-terminal leaves elementarytrees) specified .experiments, investigate two STSG variants, strictly synchronous treesubstitution grammar T3 (Cohn & Lapata, 2009), originally developedtask text compression, support full range transformation operationsquasi-synchronous tree substitution grammar (QTSG) Woodsend Lapata (2011),used text simplification summarization (Woodsend & Lapata, 2012).T3 tokens first aligned using probabilistic aligner initially providedidentity mappings entire vocabulary. experiments used Berkeleyaligner (Liang, Taskar, & Klein, 2006), however aligner broadly similar outputcould used instead. Synchronous rules comprising trees arbitrary depthextracted pair input CFG parse trees, consistent alignment. Acrosscomplete corpus aligned trees, T3 filters extracted rules provide maximallygeneral rule set, consisting rules smallest depth, still capablesynchronously deriving original aligned tree pairs. removing identity rules,resulting grammar forms transformation functions G.Unlike T3, QTSG works partial alignment tokens, based identity.Non-terminal nodes parse trees aligned consistent tokenalignment. result sections source target parse treesremain unaligned. Then, like T3, synchronous rules comprising trees minimum necessarydepth extracted pair input trees, consistent alignment,before, identity rules removed form G.example, Figure 1 shows two comparable parse trees aligned token level.synchronous rules extracted alignment T3 QTSG shown Table 2.140fiText Rewriting Improves Semantic Role LabelingROOTNNP.VPNPNNSVPVBPVBNVPVPNPVBPPNPNPNNDTADVPModernscholarscomescholarsquestionquestionexistenceexistenceNNSNPleastJJCDNNSfirstnineemperorsfirstnineemperorsDTJJCDNNS..NPPPNPNNPJJSNNDTDTNPVBPVP.ROOTFigure 1: Example sentence alignment showing source (above) target (below) trees.possible extract rules nodes child level, rules T3QTSG extract parent level identical. cases sub-treecompressed, (in example, come question compressed question), QTSGextracts full sub-tree frontier nodes align, T3 extract several rulessmallest depth.3.1.2 Transformations Bitextsalso obtain transformation rules ParaPhrase DataBase (PPDB, Ganitkevitchet al., 2013), collection English (and Spanish) paraphrases derived large bilingualparallel corpora. variety paraphrases (lexical, phrasal, syntactic) obtainedfollowing Bannard Callison-Burchs (2005b) bilingual pivoting method.141fiWoodsend & LapataRules extracted T3], [S NPVP. ]ihS, Sih[S NPhNP, NPih[NP [NNP Modern] NNShVP, VPih[VP VBPhVP, VPih[VP VBNhVP, VPih[VPhVP, VPih[VP [VB question] NPhNP, NPih[NP NPhNP, NPih[NP DThPP, PPih[PPhNP, NPih[NP ADVPhS, Sih[S NPhNP, NPih[NP NNPhVP, VPih[VP VBPhNP, NPih[NP NPhNP, NPih[NP DThPP, PPih[PPhNP, NPih[NP ADVPVP12VPVPPP1NP1.JJNP223]i1]iPP1NPCD22NN11]i]i2]i2NNS31]i4], [NP DT111JJ2CD3NNS4]i3[S [VP], [PP12], [NP [DT Some] NNSJJ21PP1], [NP DTDT]i11]i1], [VP [VBP question] NP], [NP NP23Rules extracted QTSG], [S NPVP. ]i[VP VBNNN11], [NP DT12]], [VP VP], [PP2PP1221], [NP NP2NNS1], [VP VPDTVP11], [NP [DT Some] NNS], [VP VP11NN13[S VP1.2NN1NPCD321]i[VP VB1NP2]]]]], [VP VBP1NP2]i]i2]i]iNNS4], [NP DT1JJ2CD3NNS4]iTable 2: Synchronous tree grammar rules extracted T3 QTSG alignedsentences Figure 1. Boxed indices short-hand notation alignment, .intuition two English strings e1 e2 translate foreignstring f assumed meaning. method pivots fextract he1 , e2 pair paraphrases. example shown Figure 2 (takenGanitkevitch et al., 2013). method extracts wide range possible paraphrasesunavoidably noisy due inaccurate word alignments. Paraphrases rankedcomputing p(e1 |e2 ) shown below:Xp(e2 |e1 )p(e2 |f )p(f |e1 )(2)fp(ei |f ) p(f |ei ) translation probabilities estimated bitext (Koehn,Och, & Marcu, 2003).Rewrite rules PPDB obtained using generalization method sketchedextract syntactic paraphrases (Ganitkevitch et al., 2011). Using techniquessyntactic machine translation (Koehn, 2010), SCFG rules first extracted Englishforeign sentence pairs. foreign phrase corresponding English phrase found via142fiText Rewriting Improves Semantic Role Labeling... 5 farmers thrown jailIreland ...... fnf Landwirtefestgenommen, weil ...... oder wurdenfestgenommen, gefoltert ......imprisoned, tortured ...Figure 2: Paraphrases extracted via bilingual pivoting.word alignments. phrase pair turned SCFG rule assigning left-hand sidenonterminal symbol, corresponding syntactic constituent dominates Englishphrase. introduce nonterminals right-hand sides rule, corresponding subphrases English foreign phrases replaced nonterminal symbols.sentence pairs bilingual parallel corpus results translation grammarserves basis syntactic machine translation. translation grammarconverted paraphrase grammar follows. Let r1 r2 denote translation rulesleft-hand side nonterminals hX, foreign language strings match:r1 = hX, h1 , ,(3)r2 = hX, h2 , ,paraphrase rule rp created pivoting f :rp = hX, h1 , 2 ,(4)Although shown equations (3) (4), rules SCFG associatedset features combined log-linear model estimate derivationprobabilities.3.1.3 Manual Transformationsexperiments primarily make use automatically learned transformations sinceadapted different tasks, domains languages. However, proposed approach necessary transformation functions acquired automaticallyfunctions could also crafted hand. thus also investigated effectivenessrewrites generated system Heilman Smith (2010) (henceforth H&S),uses sophisticated hand-crafted rule-based algorithm extract simplified declarative sentences English syntactically complex ones. rules similar engineered Vickrey Koller (2008) deterministic generateunique rewrite given sentence. algorithm operates standard phrase143fiWoodsend & Lapatastructure tree input sentence. extracts new sentence trees input treefollowing: non-restrictive appositives relative clauses; subordinate clausessubject finite verb; participial phrases modify noun phrases, verb phrases,clauses. addition, algorithm splits conjoined S, SBAR, VP nodes, extracts newsentence trees conjunct. output tree processed move leadingprepositional phrases quotations last children main verb phrase,following removed: noun modifiers offset commas (non-restrictive appositives,non-restrictive relative clauses, parenthetical phrases, participial phrases), verb modifiersoffset commas (subordinate clauses, participial phrases, prepositional phrases), leadingmodifiers main clause (nodes precede subject).Table 3 shows examples rules extracted using T3, QTSG PPDB grammarformalisms applied sentence CoNLL dataset. final column Table 3 indicates whether transformation could classed statement extraction, compression,insertion, substitution. reflected table, T3 captures compression transformations deleting nodes parse tree; QTSG rules range mainly syntactictransformations; PPDB transformations substitutions words short phrases.3.2 Refining Transformationsmentioned earlier, transformation rules obtained synchronous grammarscould used rewrite gold standard sentences. Unfortunately, due naturecorpora rules obtained automatic extraction process, manyrules contain errors impair rather improve qualitytraining data. idea extrapolate rules trust observing SRLlabeler handles rewritten sentences. mis-labeled them, possiblerewrite correct original labels preserved.rewritten sentence classed positive sample SRL model predictslabels transformed sentence predicted original, labelscorrected respect gold labels. If, however, semantic rolelonger predicted correctly, missed, erroneous role introduced, classifiednegative sample, sample likely harm training new SRL model.capture full impact candidate transformation function, sentence providedpositive sample classifier labels (i.e., predicates arguments)source sentence successfully projected onto rewrite. Table 4 showsexamples positive negative samples T3, QTSG, PPDB rewrites. Noterefining used H&S outputs.decide transformation function include refined set, used linearkernel SVM (Vapnik, 1995) binary classifier, classifiers indeed suitablestatistical tests contingency could used. input SVM learner setl training samples (x1 , y1 ), . . . , (xl , yl ), xi Rn , {+1, 1}. xi n dimensionalfeature vector representing ith sample, yi label sample. learningprocess involves solving convex optimization problem find large-margin separationhyperplane positive negative samples. order cope inseparable data,misclassification allowed, amount determined parameter C,thought penalty misclassified training sample. one144fiText Rewriting Improves Semantic Role LabelingGrammarOriginalT3ExamplesBell, based Los Angeles, makes distributes electronic, computerbuilding products.Bell, based, makes distributes electronic, computer building products.hPP, PPi h[PP NP ], [PP ]iCompBell, based Los Angeles, makes distributes.hNP, NPi h[NP ADJP NNS ], [NP ]iCompBased Los Angeles, makes distributes electronic, computer buildingproducts.hNP, NPi h[NP NNP ,], [NP ]iCompBell, based Angeless, makes distributes electronic, computer building products.hNP, NPi h[NP NNP NNP ], [NP NNP[POS s]]iCompBell makes distributes electronic, computer building products.hNP, NPi h[NP NP, VP ,], [NP NP ]iCompmakes distributes electronic, computer building products.hS, Sih[S NP VP. ], [S [NP It] VP. ]iIns1QTSG1111Bell based Los Angeles.hNP, Sih[NP NP, VP121,], [S NP212[VP [VBD was] VP2] .]iExtBell, based Los, makes distributes electronic, computer buildingproducts.hNP, NPi h[NP NNPNNP ], [NP NNP ]iCompLos Angeles makes distributes electronic, computer building products.hNP, NPi h[NP NP , [VP VBN [PP NP ]] ,], [NP NP ]iCompBell, founded Los Angeles, makes distributes electronic, computerbuilding products.hVP, VPi h[VP [X based] PP ], [VP [X founded] PP ]iSubBell, building Los Angeles, makes distributes electronic, computerbuilding products.hVP, VPi h[VP [X based]NP ], [VP [X building]NP ]iSubBell, based Los Angeles, makes distributes electronic, computerbuilding products.hVP, VPi h[VP VBNNP ], [VP VBN[X during] NP]iSub111PPDBType11111221122Table 3: Examples transformation rules extracted using T3, QTSG PPDB grammarformalisms, applied sentence marked Original. final column indicateswhether rule statement extraction (Ext), compression (Comp), insertion(Ins) substitution (Sub). before, boxed indices short-hand notationalignment, .view (the dual problem), result set Support Vectors, associated weights ,constant b. another view (the primal problem), result vector wdefines separation hyperplane, dimension depends particular kernel145fiWoodsend & LapataOriginal Bell, based Los Angeles, makes distributes electronic, computer building products.T3QTSGBell, based, makes distributes electronic, computer building products.Bell, based Los Angeles, makes distributes.Based Los Angeles, makes distributes electronic, computer building products.Bell, based Angeless, makes distributes electronic, computer building products.+++Bell makes distributes electronic, computer building products.makes distributes electronic, computer building products.Bell based Los Angeles.Bell, based Los, makes distributes electronic, computer building products.Bell, based Angeles, makes distributes electronic, computer building products.Los Angeles makes distributes electronic, computer building products.++++PPDBBell, founded Los Angeles, makes distributes electronic, computer building products.Bell, building Los Angeles, makes distributes electronic, computer building products.Bell, based Los Angeles, makes distributes electronic, computer building products.H&SBell makes. Bell distributes. Bell based Los Angeles.Original employees sign options, college also must approve plan.T3it, college also must approve plan.college also must approve plan.employees sign options, also must approve the.employees sign for, college also must approve plan.++QTSGemployees sign options, college also must approve.employees sign options, college also must approve plan.employees sign options, college also must approve plan.+PPDB++H&Scollege must approve plan employees sign options.employeesemployeesemployeesemployeessignsignsignsignoptions,options,options,options,collegecollegecollegecollegealsoalsoalsoalsomust adopt plan.must agree plan.must endorse plan.needs approve plan.Original went permissible line warm fuzzy feelings.T3went permissible line feelings.went warm fuzzy feelings.went it.went.++QTSGwent line warm fuzzy feelings.went permissible line feelings.went permissible warm fuzzy feelings.+PPDBwent permissible line hot fuzzy feelings.went permissible line warm fuzzy feelings.+H&Swent permissible line warm fuzzy feelings.Table 4: Examples rewrites generated T3, QTSG, PPDB source sentence(Original) CoNLL-2009 training set. Symbols +/ indicate whethersample classified positive (i.e., argument label preserving) forms partextended training corpus, not.146fiText Rewriting Improves Semantic Role Labelingfunction used SVM. case linear kernel function, wPis ndimensional,feature vectors, straight-forward relationship w = lj=1 yj j xjprimal dual variables, effectively assigning weights explicitly specified features.kernel functions allow interaction variables. instance usingbinary valued features, degree2 polynomial kernel function implies classifierconsiders available pairs features well.used identity transformation functions involved featuressample, size feature space n = kGk, features binary-valued.features could easily incorporated setting, perhaps capturing informationstructure source sentence transformation function, might achieve goodresults conjunction polynomial kernel, pursue avenue further.Instead used linear kernel, due simple structure features, SVMassigned weight transformation function independent source sentence.chose transformation functions form refined set based whethercorresponding weight global threshold value, set threshold valuemaximizing performance resulting SRL model development set.3.3 Labeling Extended CorpusSVM identified refined set transformation functions G, transformations used create extended training corpus. Using alignment informationtransformation functions trace position tokens original sentencerewrite, semantic role labels gold corpus projected onto corresponding predicate-argument pairs rewritten corpus. Assuming SVM correctlyidentified transformation function involved indeed label-preserving,transformation functions applied current context, semantic role labelingrewrite quality standard source. conditionshowever unlikely true, resulting degradation quality rewrite corpus.corpus rewrites appended original gold standard corpus create newlarger training corpus, used create SRL model.4. Experimental Setupsection present experimental setup assessing performance approach. give details corpora grammars used create transformations,model parameters used identify preserve labels. explain existing SRL system modified approach, evaluated effectsincreasing training data transformations.4.1 Grammar Extractionextracted synchronous grammars two monolingual comparable corpora drawnWikipedia. corpus 137,362 aligned sentences created pairing Simple EnglishWikipedia English Wikipedia (Kauchak, 2013). corpus 14,831 paired sentences comparing consecutive revisions articles Simple English Wikipedia (Woodsend & Lapata, 2011). corpora provide large repository monolingual, comparable147fiWoodsend & LapataGrammarT3QTSGAligned13,5623,875Revisions5,386669Table 5: Non-identical rules extracted Wikipedia corpus, rules appearingone two times removed.sentences, taken real-world writing. Advantageously, Simple English Wikipedia encourages contributors employ simpler grammar ordinary English Wikipedia;corpora therefore naturally contain many examples syntactic variation reordering sentence splitting, well paraphrasing changes content. Table 5 listsnumber non-identical rules grammar formalism extracted Wikipediacorpora, rules instance count one two removed.addition grammars extracted Simple English Wikipedia, workedmonolingual synchronous grammar included Paraphrase Database (Ganitkevitch et al., 2013), paraphrases extracted bilingual parallel corpora.English portion PPDB contains 220 million paraphrase pairs, including 140million paraphrase patterns capturing syntactic transformations varying confidence.form synchronous grammar, used highest scoring 585,000 paraphrasessubset constituent syntactic paraphrases (where nonterminals labeled PennTreebank constituents).4.2 Semantic Role Labelermethod presented paper crucially relies semantic role labeler refiningtransformations performing semantic analysis general. used publicly available system Bjorkelund et al. (2009). competedCoNLL-2009 SRL-only challenge, ranked first English language, second overall. best knowledge, system represents state-of-the-art EnglishSRL parsing. system architecture consists four-stage pipeline classifiers:predicate identification (although module required evaluation), predicatesense disambiguation, binary classifier argument identification, finally argumentclassification using multiclass classifier. Beam search used identify argumentspredicate label them, according local classifiers using features relatemainly dependency parse information linking predicates potential argumentssiblings. addition, global reranker used select best combination candidates (see Section 5 details). SRL system requires tokenized input lemma,POS-tag dependency parse information. information already providedgold-standard training corpus (see immediately below). create equivalent informationtransformed text evaluation files, used mate-tools pipeline (Bjorkelundet al., 2009), retrained (like SRL model itself) training partition data.used English language benchmark datasets CoNLL-2009 shared tasktrain evaluate SRL models. identified labeled semantic arguments nouns148fiText Rewriting Improves Semantic Role LabelingCorpusTraining+ H&S+ PPDB+ T3+ QTSG+ T3 + QTSG+ PPDB + T3+ PPDB + QTSG+ PPDB + T3 + QTSGDevelopmentTest in-domainTest out-of-domainSentences39,27255,474238,732203,941500,627704,561442,666739,352943,2861,3342,399425Tokens958,174909,3587,071,5504,701,6889,623,47114,325,16611,773,24516,695,02821,396,72333,36857,6767,207Table 6: Statistics corpora used train evaluate SRL models.verbs (Hajic, Ciaramita, Johansson, Kawahara, Mart, Marquez, Meyers, Nivre, Pado,Stepanek, Stranak, Surdeanu, Xue, & Zhang, 2009). used training, development,test out-of-domain test partitions provided, statisticsdata sets shown Table 6. Specifically, show increase training dataeffected method using transformations obtained T3, QTSG, PPDB,combinations. comparison also use manual transformations availableHeilman Smith (2010). train SRL model (and also previous stages NLPpipeline), used data training partition only, development partitionused identify best subset G transformations2 .used LibLinear (Fan, Chang, Hsieh, Wang, & Lin, 2008) train SVM,hyper-parameters SVM tuned cross-validation training setmaximise area ROC curve, using automatic grid-search utility pythonpackage scikit-learn (Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel,Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, & Duchesnay, 2011). assessment cross-validation accuracy (in terms F1 score areaROC curve) SVM grammar shown Table 7. results showPPDB rewrites accurate employ, perhaps rulesheavily lexicalized grammars. T3 grammar unpredictable use,although SVM scores considerably higher chance.Test sets used solely evaluation, making use indicators datafiles words argument-bearing predicates. Results generated usingCoNLL-2009 evaluation script unmodified. report results semantic roles(i.e., combination syntactic dependencies tends yield higher scores)using in-domain out-of-domain evaluation data. evaluation script,semantic propositions evaluated converting semantic dependencies2. result re-training performance reported worse modelsavailable mate website, trained partitions CoNLL-2009 data(training, development test).149fiWoodsend & LapataGrammarPPDBT3QTSGF10.850.670.78Area ROC0.820.610.72Table 7: Statistics SVMs performance grammar, obtained crossvalidation training set.predicate arguments, labeling dependency labelscorresponding argument. Additionally, dependency created virtual root nodepredicate labeled predicate sense. comparable publishedresults, general report scores combine predicate sense argument role labelpredictions. Tables 12, 13 14, however, focus arguments only, removepredicate sense scores.5. Resultssection provide empirical evidence performance approach.experiments primarily designed answer following questions. text rewritinggenerally improve SRL performance? matter transformation rules use,i.e., rules better others? transformation rules useful out-ofdomain data? SRL labels mostly affected rewriting? performance varydepending size original training data? results sensitive learneremployed? first examine effect different (transformation) grammarsSRL task in-domain out-of-domain test data, move assesslabels mostly affected method. Finally, present results effectcombining approach global reranker training different-sized datasets.5.1 Transformation Rules Improve F1 Across BoardTable 8 (left half) shows SRL performance (measured terms precision, recall, F1)in-domain CoNLL-2009 test set. training corpora rewritten H&Ssystem, T3, QTSG, PPDB grammars, resulting SRL models significantly(p < 0.01) improve model trained original corpus. used stratified shuffling (Noreen, 1989) examine whether differences F1 significant (Pado, 2006).Recall shows largest increase, particularly acquired synchronous grammars,indicating increased training data resulting better coverage. Generallyexpense precision cases apart PPDB increased well.Significant gains also seen acquired grammars compared H&S system,exception T3 greater variation performance.also combined rewrites produced different grammars (see T3+QTSG,PPDB+T3, PPDB+QTSG PPDB+T3+QTSG Table 8) significantlyimprove performance individual grammars (although still significantly betteroriginal model H&S system), suggesting grammars capturing150fiText Rewriting Improves Semantic Role LabelingOriginalH&SPPDBT3QTSGPPDB+T3PPDB+QTSGT3+QTSGPPDB+QTSG+T3label projectionP86.7987.0886.4286.8487.0486.6186.7086.7886.7680.95In-domainRF183.58 85.1583.73 85.3784.64 85.5284.25 85.5284.34 85.6784.45 85.5184.81 85.7584.62 85.6984.69 85.7178.75 79.83Out-of-domainPRF176.04 71.73 73.8276.33 70.86 73.4975.37 72.66 73.9976.04 72.29 74.1276.88 72.83 74.8975.65 72.49 74.0376.64 73.22 74.8976.56 72.88 74.6776.54 73.19 74.8366.94 66.93 66.93Table 8: Semantic evaluation results CoNLL-2009 in-domain out-of-domain testsets (combining predicate word sense argument role labels). Resultsmodels trained Original training set, baseline extension training set,extensions due grammar combinations. label projection: resultstraining PPDB+QTSG+T3 training corpus, without rewritinglabels using gold corpus information. Difference Original significantp < 0.01. Difference H&S significant p < 0.01.Proportion sentencesproduced grammarH&SQTSGAlso producedPPDB0.40.0grammarT3QTSG0.228.13.2Table 9: Sentence rewrite overlap (%) refined rewrite corpora produced H&S,PPDB, T3 QTSG.similar information. instance, T3 QTSG extracted corporaaligned sentence pairs. degree overlap rewrite corpora producedgrammars shown Table 9. Although degree overlap exact sentences low,relative performance resulting models closer (discussed below). Overall,best performing system uses transformations obtained QTSG PPDB,surprising rules extracted grammars present minimal overlap.Benefits also transfer out-of-domain text acquired grammars, improvingoverall performance even in-domain data (see right half Table 8).F1-score QTSG model 1% higher original model, Recallmodel combining acquired grammars increased 1.5%. Meanwhile,rewrites H&S system seem improve coverage, resulting drop RecallF1-score.151fiWoodsend & LapataOriginalPPDBT3QTSGP86.7987.3487.3687.48In-domainRF183.58 85.1583.10 85.1783.26 85.2683.31 85.34Out-of-domainPRF176.04 71.73 73.8276.41 71.42 73.8376.49 71.76 74.0576.75 72.00 74.30Table 10: Results CoNLL-2009 in-domain out-of-domain test sets, training SRLmodel rewrites labeled positive.SVMThresholds CountNone+0.00110-0.00110-0.21010+0.0015+0.0013P80.2680.7480.8280.6580.5580.4680.00QualityR76.2876.9976.8776.8677.0876.5476.13F178.2278.8278.8078.7178.7878.4578.02Table 11: Effect selecting transforms SVM quality resulting model(precision, recall F1 measures labeling development set).addition, examined whether filtering set acquired transformation functionsindeed beneficial. approach proposed, transformations appliedtraining corpus twice: first time input SVM identify reliable rewriterules, second pass reduced set rules applied whole training corpus.alternative approach would apply transforms once, train SRLmodel. thus took rewrites labeled positive steps 914 Algorithm 1corrected labels gold-standard (step 23). SRL models subsequently trained usingextended training corpus, created concatenating original training datasetrewrites. Table 10 shows SRL performance different grammars (PPDB, T3,QTSG) test set. Although precision F1 increased original model,gains much reduced compared results obtained using SVM (Table 8).appears extra rewrites obtained applying generally-reliable transformswhole training set increases coverage, improves performance models.Table 11 shows altering quality threshold (and removing indicator featuresnumber times transformation function extracted) affects performance.Results shown QTSG grammar (in-domain) development set (we observedsimilar patterns grammars grammar combinations). SVM qualitythreshold varied positive (no transformations accepted) negative (all152fiText Rewriting Improves Semantic Role LabelingOriginalH&SPPDBT3QTSGPPDB+T3PPDB+QTSGT3+QTSGPPDB+QTSG+T3P82.6983.0882.3482.8883.0082.6182.6282.7582.83In-domainRF178.25 80.4178.45 80.7079.89 81.1079.30 81.0579.27 81.0979.62 81.0980.01 81.2979.77 81.2379.95 81.37Out-of-domainPRF171.44 65.62 68.4071.65 64.25 67.7570.68 67.02 68.8071.54 66.46 68.9072.48 66.98 69.6271.16 66.88 68.9572.11 67.47 69.7172.08 67.09 69.4972.08 67.54 69.74Table 12: Performance labeling semantic arguments (predicate word sense information removed). Difference Original significant p < 0.01. DifferenceH&S significant p < 0.01.OriginalH&SPPDBT3QTSGPPDB+T3PPDB+QTSGT3+QTSGPPDB+QTSG+T3In-domainPRF189.56 84.75 87.0989.71 84.71 87.1488.99 86.34 87.6589.57 85.70 87.5989.53 85.50 87.4789.10 86.28 87.6689.28 86.05 87.6389.33 86.10 87.6889.33 86.23 87.75Out-of-domainPRF187.20 80.10 83.5087.40 78.38 82.6586.24 81.78 83.9587.09 80.90 83.8887.28 80.66 83.8486.75 81.53 84.0686.77 81.18 83.8887.34 81.29 84.2086.90 81.43 84.07Table 13: Accuracy identification classification (labeling) semantic arguments.transformations). findings indicate constructing G transformationspositive SVM weight (threshold +0.001) gives better results transformations,permissive threshold.5.2 Transformation Rules Improve Semantic Role Assignment VerbalNominal Predicatesresults Table 8 combine accuracy predicting sense predicates accuracylabeling arguments. Generally, models better assigning correct predicatesense. interesting result much gain performance seen rewritingtraining corpus comes improving semantic role assignment. appears153fiWoodsend & LapataRAMMNRRAMLOCRAMCAURA2RA1RA0CA1CA0AMTMPAMPRDAMPNCArgumentRAMTMPAMNEGAMMODAMMNRAMLOCAMDIRAMCAUAMDISAMADVA5A4A2A1A0JJNNVBPVBZA3NNP NNSVB+10%+1%AMEXTChange F1VBD VBG VBN0%1%10%Occurrences1+10+100+1000+PredicateFigure 3: Changes F1-score PPDB+T3+QTSG model Original, measuredpairs predicate POS-tag argument.introducing syntactic variation training data provides model wider coveragesyntactic dependency paths predicate arguments.Table 12 shows results models data sets above, focusingargument labels only. acquired grammars show biggest improvements,1% improvement Recall case, gains F1-score 0.5% 1.2%.models data sets used Table 13, results argumentidentification only, classification (unlabelled arguments). improvements154fiText Rewriting Improves Semantic Role LabelingRAMMNRRAMLOCRAMCAURA2RA1RA0CA1+1%0%CA0AMTMPAMPRDAMPNCArgumentRAMTMPAMNEGAMMODAMMNRAMLOCAMDIRAMCAUAMEXTAMDISAMADVA5A4A2A1A0JJNNVBPVBZA3NNP NNSVBVBD VBG VBNChange F1+10%1%10%Occurrences1+10+100+1000+PredicateFigure 4: Relative performance terms F1-score QTSG (red) PPDB (blue)models, pairs predicate POS-tag argument.Original Recall F1. large before, showing overallgains result improvements argument identification classification.breakdown gains F1-score predicate POS-tag argument shownFigure 3, illustrating relative improvements model trained acquired grammars (PPDB+T3+QTSG) model trained original CoNLL training data.analysis reveals gain came increased precision recallpredicting core arguments. additional gains modifiers nominal pred155fiWoodsend & LapataDependency path distanceProportion test setSRL model:OriginalPPDBT3QTSGPPDB+QTSG+T30175.75213.6735.5442.6251.1360.567+0.7388.83+0.49+0.63+0.53+0.7474.27+1.43+1.15+0.66+1.6061.73+2.65+1.65+1.82+2.4954.76+3.26+1.67+2.98+2.0243.08+4.78+1.42+3.01+6.2023.91+5.53+1.220.96+1.3512.270.06+0.69+0.43+1.61Table 14: F1-scores labeled arguments distance predicate argumentmeasured number arcs dependency graph. ResultsCoNLL in-domain test set. Lower rows show change F1-scoreOriginal SRL model.icates. improvement losses common core arguments(A0 A1) verbal predicates, striking gains seencore argument labels. seems consistent models learning wider syntacticcoverage. Figure 4 shows similar breakdown gains F1-score predicate POStag argument, time comparing improvements seen QTSG corpusresulting PPDB. differences less pronounced, PPDB improvingcore arguments more, QTSG improving performance labeling modifiers.also investigated effect label projection mechanism itself. usedrewrites produced grammars (PPDB+T3+QTSG) extend training set. However, instead using projected labels, used original model Mgold (trainedtraining partition CoNLL-2009) label refined corpus. retrainedextended corpus used retrained model label test corpus. words,removed step 25 Algorithm 1. considered form self-training. Resultstest out-of-domain sets show using automatically generated labelsinstead projected ones seriously impairs resulting model, F1-scores decreasingalmost 6% test set 8% out-of-domain set (see last row Table 8).5.3 Transformation Rules Improve Performance Relations Involving LongDependency Pathsdependency path (the sequence arcs syntactic dependency tree)predicate argument typically short. Table 14 shows gold-labeledtest set, three-quarters arguments direct dependency heads childrenpredicate, case nominal predicates, argument predicate itself. ExistingSRL models highly accurate shorter pathsthe original SRL modelF1-score almost 89%but prediction accuracy drops considerably dependency pathgrows. seen Table 14, adding rewrites training set improves predictionaccuracy almost combinations transformation grammar dependency pathdistance, largest gains seen number arcs dependency path156fiText Rewriting Improves Semantic Role LabelingOriginalH&SPPDBT3QTSGT3+QTSGPPDB+T3PPDB+QTSGPPDB+QTSG+T3P88.4488.6886.4288.0488.4188.2486.6186.7087.94In-domainRF184.42 86.3884.34 86.4684.64 85.5284.78 86.3885.05 86.7085.21 86.7084.45 85.5184.81 85.7585.25 86.57Out-of-domainPRF177.89 72.73 75.2278.11 71.76 74.8076.73 73.36 75.0177.07 72.97 74.9778.34 73.70 75.9578.00 73.53 75.7077.30 73.51 75.3577.41 73.82 75.5777.67 73.73 75.64Table 15: Results CoNLL test sets models combining extended training dataglobal reranker. Difference Original significant p < 0.01. DifferenceH&S significant p < 0.01.three six. Improvements F1-score observed individual grammarscombination (PPDB+QTSG+T3).5.4 Transformation Rules Improve Performance Even GlobalReranker UsedSRL system used (Bjorkelund et al., 2009) optionally incorporate globalreranker (Toutanova, Haghighi, & Manning, 2005). reranker re-scores completepredicate-argument structure, using features stages local pipeline additional features representing sequence core argument labels current predicate.Table 15 presents evaluation results global reranker trained extended corporaproduced method. Compared model trained original corpus, addingreranker provide significant improvement.3 Training extended data givesincreases performance; smaller, though still significant,case Table 8. indicates global reranker compensating some,all, new information contained extended training data.5.5 Transformation Rules Improve Performance Across (Small Large)Datasetsalso investigated accuracy labeler function size originaltraining data. size, subsets original training data created (with replacement) used train SRL model, performance resulting modelmeasured using development set. training subset, applied Algorithm 1:original SRL model trained subset; created extended corpus3. scores reported higher official CoNLL-2009 ones (in domain P:87.46, R:83.87,F1:85.63; domain P:76.04, R:70.76, F1:73.31) using mate-tools NLP pipelinedependency parse, rather dependency information provided test set.157fiWoodsend & Lapata808075757070RecallPrecisionRewritesSource65656060555516062525001000040000160Source sentences62525001000040000Source sentencesFigure 5: SRL model performance function size training data,without additional rewrites. Error bars show standard error 10 experiments.subset using grammar; SVM trained time refine transformationspreserved labels; SRL model retrained original plus refinedrewritten version corpus subset.particular, wanted investigate rewritten text provided performancebenefit small amount training data, benefit wouldsubsumed labeled training data provided. learning curves Figure 5 showcontrary: increasing quantity source training data undoubtedly improvesquality SRL model, found including rewritten training data additionconsistently improves precision recall measures. learning curves Figure 5use QTSG grammar set transformation functions; obtained similar resultsPPDB T3 (and grammar combinations), however omit sakebrevity.6. Conclusionspaper investigated potential text rewriting means increasingamount training data available supervised NLP tasks. method automaticallyextracts rewrite rules comparable corpora uses generate multiple syntactic variants sentences annotated gold standard labels. Application methodsemantic role labeling reveals syntactic transformations improve SRL performance158fiText Rewriting Improves Semantic Role LabelingQTSGPPDBhNP, NPih[NP DThNP, NPih[NP NPhNP, Sih[NP NPhS, Sih[S even VBZhADJP, ADJPih[ADJP JJhPP, PPih[PP past month], [PP last month]i111JJ, NPPPNNS], [NP DTCC NP221NP2NNS], [NP NP], [S NP111PP212]i]i.]i], [S even though VBZ], [ADJP equally JJ11NP2]i]iTable 16: Examples QTSG PPDB synchronous grammar rules given high importancerefinement. Boxed indices indicate alignment, .beyond sate art CoNLL 2009 benchmark dataset. Specifically, experimentally show (a) rewrite rules, whether automatic hand-written, consistentlyimprove SRL performance, although automatic variants tend perform best; (b) syntactic transformations improve SRL performance within- out-of-domain; (c)improvements observed across learners, even using global reranker.future would like explore better ways identifying best (i.e., performance enhancing) rewrite rules may task grammar specific. Table 16illustrates rules deemed important (i.e., given high weight) SVM classifierSRL task. instance, could undertake detailed feature engineering, including tree-based ngram features capture grammaticality rewritten sentences.Throughout paper argued transformation rules used enhanceperformance SRL task. Conversely, work described mightrelevance NLP tasks employing rewriting. example, idea identifyinglabel preserving transformations, could used learn rules meaning preservingconsequently safe use tasks simplification sentence compression. Machinetranslation, textual entailment, semantic parsing additional application areasstand benefit accurate rewrite rules. Much methodology reportedcould adapted machine translation either training larger datasets (CallisonBurch, Koehn, & Osborne, 2006), domain-adaptation (Irvine, Quirk, & Daume III,2013), evaluation (Kauchak & Barzilay, 2006)Finally, beyond supervised SRL, would like adapt method unsupervisedsemantic role induction (Lang & Lapata, 2011; Titov & Klementiev, 2012), investigate alternative synchronous grammar extraction methods (e.g., based dependency information),obtain rewrite rules larger comparable corpora.Acknowledgmentsgrateful anonymous referees whose feedback helped substantially improvepresent paper. acknowledge financial support EPSRC (EP/K017845/1)framework CHIST-ERA READERS project.159fiWoodsend & LapataReferencesAho, A. V., & Ullman, J. D. (1969). Syntax directed translations pushdown assembler. Journal Computer System Sciences, 3 (1), 3756.Bannard, C., & Callison-Burch, C. (2005a). Paraphrasing Bilingual Parallel Corpora.Proceedings 43rd Annual Meeting Association Computational Linguistics, pp. 597604, Ann Arbor.Bannard, C., & Callison-Burch, C. (2005b). Paraphrasing Bilingual Parallel Corpora.Proceedings 43rd ACL, pp. 255262, Ann Arbor, MI.Barzilay, R., & McKeown, K. (2001). Extracting Paraphrases Parallel Corpus.Proceedings ACL/EACL, pp. 5057, Toulouse, France.Bjorkelund, A., Hafdell, L., & Nugues, P. (2009). Multilingual semantic role labeling.Proceedings Thirteenth Conference Computational Natural Language Learning (CoNLL 2009): Shared Task, pp. 4348, Boulder, Colorado. Software retrievedhttps://code.google.com/p/mate-tools/.Callison-Burch, C. (2007). Paraphrasing Translation. Ph.D. thesis, University Edinburgh.Callison-Burch, C. (2008). Syntactic Constraints Paraphrases Extracted ParallelCorpora. Proceedings 2008 Conference Empirical Methods NaturalLanguage Processing, pp. 196205, Honolulu, Hawaii.Callison-Burch, C., Koehn, P., & Osborne, M. (2006). Improved statistical machine translation using paraphrases. Proceedings Human Language Technology ConferenceNAACL, Main Conference, pp. 1724, New York City, USA.Chandrasekar, R., Doran, C., & Srinivas, B. (1996). Motivations Methods TextSimplification. Proceedings 16th International Conference ComputationalLinguistics, pp. 10411044, Copenhagen, Denmark.Chiang, D. (2007). Hierarchical Phrase-Based Translation. Computational Linguistics,33 (2), 201228.Cohn, T., & Lapata, M. (2009). Sentence Compression Tree Transduction. JournalArtificial Intelligence Research, 34, 637674.Cohn, T., & Lapata, M. (2013). abstractive approach sentence compression. ACMTrans. Intell. Syst. Technol., 4 (3), 41:141:35.Coster, W., & Kauchak, D. (2011). Simple English Wikipedia: New Text SimplificationTask. Proceedings 49th Annual Meeting Association ComputationalLinguistics: Human Language Technologies, pp. 665669, Portland, Oregon, USA.Dowty, D. (1991). Thematic Proto Roles Argument Selection. Language, 67 (3), 547619.Eisner, J. (2003). Learning Non-Isomorphic Tree Mappings Machine Translation.Proceedings ACL Interactive Poster/Demonstration Sessions, pp. 205208, Sapporo, Japan.160fiText Rewriting Improves Semantic Role LabelingFan, R. E., Chang, K. W., Hsieh, C. J., Wang, X. R., & Lin, C. J. (2008). LIBLINEAR:Library Large Linear Classification. Journal Machine Learning Research, 9,18711874.Feblowitz, D., & Kauchak, D. (2013). Sentence simplification tree transduction.Proceedings Second Workshop Predicting Improving Text ReadabilityTarget Reader Populations, pp. 110, Sofia, Bulgaria.Furstenau, H., & Lapata, M. (2012). Semi-supervised semantic role labeling via structuralalignment. Computational Linguistics, 38 (1), 135171.Galley, M., & McKeown, K. (2007). Lexicalized Markov Grammars Sentence Compression. Proceedings NAACL/HLT, pp. 180187, Rochester, NY.Ganitkevitch, J., Callison-Burch, C., Napoles, C., & Van Durme, B. (2011). LearningSentential Paraphrases Bilingual Parallel Corpora Text-to-Text Generation.Proceedings 2011 Conference Empirical Methods Natural LanguageProcessing, pp. 11681179, Edinburgh, Scotland, UK.Ganitkevitch, J., Cao, Y., Weese, J., Post, M., & Callison-Burch, C. (2012). Joshua 4.0:Packing, pro, paraphrases. Proceedings Seventh Workshop StatisticalMachine Translation, pp. 283291, Montreal, Canada.Ganitkevitch, J., Van Durme, B., & Callison-Burch, C. (2013). PPDB: ParaphraseDatabase. Proceedings 2013 Conference North American ChapterAssociation Computational Linguistics: Human Language Technologies, pp.758764, Atlanta, Georgia. used prepackaged small constituent syntacticsubset PPDB, retrieved http://paraphrase.org.Gildea, D., & Jurafsky, D. (2002). Automatic Labeling Semantic Roles. ComputationalLinguistics, 28 (3), 245288.Graehl, J., & Knight, K. (2004). Training Tree Transducers. HLT-NAACL 2004: MainProceedings, pp. 105112, Boston, MA.Hajic, J., Ciaramita, M., Johansson, R., Kawahara, D., Mart, M. A., Marquez, L., Meyers,A., Nivre, J., Pado, S., Stepanek, J., Stranak, P., Surdeanu, M., Xue, N., & Zhang, Y.(2009). conll-2009 shared task: Syntactic semantic dependencies multiplelanguages. Proceedings Thirteenth Conference Computational NaturalLanguage Learning (CoNLL 2009): Shared Task, pp. 118, Boulder, Colorado.Heilman, M., & Smith, N. (2010). Extracting Simplified Statements Factual QuestionGeneration. Proceedings 3rd Workshop Question Generation, pp. 1120,Carnegie Mellon University, PA. Software available http://www.ark.cs.cmu.edu/mheilman/questions/.Irvine, A., Quirk, C., & Daume III, H. (2013). Monolingual marginal matching translation model adaptation. Proceedings 2013 Conference Empirical MethodsNatural Language Processing, pp. 10771088, Seattle, Washington, USA.Kauchak, D. (2013). Improving text simplification language modeling using unsimplifiedtext data. Proceedings 51st Annual Meeting Association Computational Linguistics (Volume 1: Long Papers), pp. 15371546, Sofia, Bulgaria. used161fiWoodsend & LapataVersion 2.0 sentence-aligned corpus, retrieved http://www.cs.middlebury.edu/~dkauchak/simplification/.Kauchak, D., & Barzilay, R. (2006). Paraphrasing automatic evaluation. ProceedingsHuman Language Technology Conference NAACL, Main Conference, pp.455462, New York City, USA.Klebanov, B. B., Knight, K., & Marcu, D. (2004). Text Simplification InformationSeeking Applications. Meersman, R., & Tari, Z. (Eds.), Move MeaningfulInternet Systems 2004: CoopIS, DOA, ODBASE, pp. 735747. Springer BerlinHeidelberg.Koehn, P. (2010). Statistical Machine Translation. Cambridge University Press.Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. Proceedings HLT/NAACL, pp. 4854, Edmonton, Canada.Kundu, G., & Roth, D. (2011). Adapting Text instead Model: Open DomainApproach. Proceedings Fifteenth Conference Computational Natural Language Learning, pp. 229237, Portland, Oregon, USA.Kwiatkowski, T. (2012). Probabilistic Grammar Induction Sentences StructuredMeanings. Ph.D. thesis, University Edinburgh.Lang, J., & Lapata, M. (2011). Unsupervised semantic role induction via split-merge clustering. Proceedings 49th Annual Meeting Association ComputationalLinguistics: Human Language Technologies, pp. 11171126, Portland, Oregon, USA.Liang, P., Taskar, B., & Klein, D. (2006). Alignment Agreement. ProceedingsHLT/NAACL, pp. 104111, New York, NY.Marton, Y., Callison-Burch, C., & Resnik, P. (2009). Improved statistical machine translation using monolingually-derived paraphrases. Proceedings 2009 ConferenceEmpirical Methods Natural Language Processing, pp. 381390, Singapore.Mehdad, Y., Negri, M., & Federico, M. (2010). Towards cross-lingual textual entailment.Human Language Technologies: 2010 Annual Conference North AmericanChapter Association Computational Linguistics, pp. 321324, Los Angeles,California.Melli, G., Wang, Y., Liu, Y., Kashani, M. M., Shi, Z., Gu, B., Sarkar, A., & Popowich, F.(2005). Description SQUASH, SFU Question Answering Summary HandlerDUC-2005 Summarization Task. Proceedings Human Language Technology Conference Conference Empirical Methods Natural LanguageProcessing Document Understanding Workshop, Vancouver, Canada.Noreen, E. (1989). Computer-intensive methods testing hypotheses: introduction.Wiley.Pado, S. (2006). Users guide sigf: Significance testing approximate randomisation.Retrieved http://www.nlpado.de/~sebastian/software/sigf.shtml.Palmer, M., Gildea, D., & Kingsbury, P. (2005). Proposition Bank: AnnotatedCorpus Semantic Roles. Computational Linguistics, 31 (1), 71106.162fiText Rewriting Improves Semantic Role LabelingPang, B., Knight, K., & Marcu, D. (2003). Syntax-based Alignment Multiple Translations:Extracting Paraphrases Generating New Sentences. Proceedings NAACL,pp. 181188, Edmonton, Canada.Park, J. H., Croft, B., & Smith, D. A. (2011). Quasi-synchronous Dependence ModelInformation Retrieval. Proceedings 20th ACM International ConferenceInformation Knowledge Management, pp. 1726, Glasgow, United Kingdom.Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau,D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learningPython. Journal Machine Learning Research, 12, 28252830.Shen, D., & Lapata, M. (2007). Using Semantic Roles Improve Question Answering.Proceedings 2007 Joint Conference Empirical Methods Natural LanguageProcessing Computational Natural Language Learning (EMNLP-CoNLL), pp. 1221, Prague, Czech Republic.Surdeanu, M., Harabagiu, S., Williams, J., & Aarseth, P. (2003). Using Predicate-ArgumentStructures Information Extraction. Proceedings Annual MeetingAssociation Computational Linguistics, pp. 815, Sapporo, Japan.Titov, I., & Klementiev, A. (2012). bayesian approach unsupervised semantic roleinduction. Proceedings 13th Conference European ChapterAssociation Computational Linguistics, pp. 1222, Avignon, France.Toutanova, K., Haghighi, A., & Manning, C. (2005). Joint learning improves semantic rolelabeling. Proceedings 43rd Annual Meeting Association Computational Linguistics (ACL05), pp. 589596, Ann Arbor, Michigan.Vapnik, V. (1995). Nature Statistical Learning Theory. Springer-Verlag New York,Inc.Vickrey, D., & Koller, D. (2008). Sentence simplification semantic role labeling.Proceedings ACL-08: HLT, pp. 344352, Columbus, Ohio.Wang, M., & Manning, C. (2010). Probabilistic tree-edit models structured latentvariables textual entailment question answering. Proceedings 23rdInternational Conference Computational Linguistics (Coling 2010), pp. 11641172,Beijing, China.Wang, M., Smith, N. A., & Mitamura, T. (2007). Jeopardy model? quasisynchronous grammar QA. Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural LanguageLearning (EMNLP-CoNLL), pp. 2232, Prague, Czech Republic.Woodsend, K., & Lapata, M. (2011). Learning simplify sentences quasi-synchronousgrammar integer programming. Proceedings 2011 Conference Empirical Methods Natural Language Processing, pp. 409420, Edinburgh, Scotland, UK.used Wikipedia revisions corpus, retrieved http://homepages.inf.ed.ac.uk/kwoodsen/wiki.html.Woodsend, K., & Lapata, M. (2012). Multiple aspect summarization using integer linearprogramming. Proceedings 2012 Joint Conference Empirical Methods163fiWoodsend & LapataNatural Language Processing Computational Natural Language Learning, pp.233243, Jeju Island, Korea.Wu, D. (1997). Stochastic Inversion Transduction Grammars Bilingual ParsingParallel Corpora. Computational Linguistics, 23 (3), 377404.Wu, D., & Fung, P. (2009). Semantic Roles SMT: Hybrid Two-Pass Model. Proceedings Human Language Technologies: Annual Conference North AmericanChapter Association Computational Linguistics, Companion Volume: ShortPapers, pp. 1316, Boulder, Colorado.Yamada, K., & Knight, K. (2001). Syntax-based Statistical Translation Model. Proceedings 39th Annual Meeting Association Computational Linguistics,pp. 523530, Toulouse, France.Yamangil, E., & Nelken, R. (2008). Mining Wikipedia revision histories improvingsentence compression. Proceedings ACL-08: HLT, Short Papers, pp. 137140,Columbus, Ohio.Zanzotto, F. M., & Pennacchiotti, M. (2010). Expanding textual entailment corporafromwikipedia using co-training. Proceedings 2nd Workshop Peoples Web Meets NLP: Collaboratively Constructed Semantic Resources, pp. 2836,Beijing, China. Coling 2010 Organizing Committee.Zhu, Z., Bernhard, D., & Gurevych, I. (2010). Monolingual Tree-based Translation ModelSentence Simplification. Proceedings 23rd International ConferenceComputational Linguistics, pp. 13531361, Beijing, China.164fiJournal Artificial Intelligence Research 51 (2014) 227-254Submitted 04/14; published 09/14Entrenchment-Based Horn ContractionZhiqiang Zhuangz.zhuang@griffith.edu.auInstitute Integrated Intelligent SystemsGriffith University, QLD 4111, AustraliaMaurice Pagnuccomorri@cse.unsw.edu.auSchool Computer Science EngineeringUniversity New South Wales, NSW 2052, AustraliaAbstractAGM framework benchmark approach belief change. Since frameworkassumes underlying logic containing classical Propositional Logic, appliedsystems logic weaker Propositional Logic. remedy limitation, severalresearchers studied AGM-style contraction revision Horn fragmentPropositional Logic (i.e., Horn logic). paper, contribute line researchinvestigating Horn version AGM entrenchment-based contraction. studychallenging construction entrenchment-based contraction refers arbitrary disjunctions expressible Horn logic. order adapt constructionHorn logic, make use Horn approximation technique called Horn strengthening.provide representation theorem newly constructed contraction referentrenchment-based Horn contraction. Ideally, contractions defined Horn logic (i.e.,Horn contractions) rational AGM contraction. propose notionHorn equivalence intuitively captures equivalence Horn contractionAGM contraction. show that, notion, entrenchment-based Horn contractionequivalent restricted form entrenchment-based contraction.1. IntroductionGiven agent set beliefs, theory belief change deals agentchanges beliefs rational manner confronted new information. Twokinds changes mainly studied, namely contraction revision, removing oldbeliefs incorporating new beliefs respectively. main strategies studyingbelief change articulate principles called rationality postulates rational agentsobey contracting revising sets beliefs specify explicit changemechanisms called construction methods contraction revision operation.dominant theory belief change called AGM framework (Alchourron,Gardenfors, & Makinson, 1985; Gardenfors, 1988). framework specify specific underlying logic, however, assumes logic contains classical PropositionalLogic. commonly accepted AGM framework provides best set rationality postulates capturing intuitions behind rational belief change along wellmotivated construction methods characterised rationality postulates.Regardless desirable properties, assumption underlying logic severelimitation.Tractable fragments Propositional Logic non-classic logics Description Logics (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003) particuc2014AI Access Foundation. rights reserved.fiZhuang & Pagnuccolarly useful artificial intelligence applications allow efficient reasoning methods.Since knowledge evolves, systems based logics subject change. However, fragments Propositional Logic Description Logics subsume Propositional Logic,thus AGM framework applied systems. Consequently, extensiveattention paid problem belief change fragments PropositionalLogic Description Logics. recent trend focuses Horn fragment Propositional Logic (i.e., Horn Logic) found extensive usage artificial intelligencedatabase systems. paper contribute Horn belief change thoroughly investigating Horn version entrenchment-based contraction (Gardenfors & Makinson, 1988;Gardenfors, 1988) refer entrenchment-based Horn contraction.Entrenchment-based contraction based rankings formulas called epistemic entrenchments. general idea formulas entrenched rankingpreferred less entrenched and, deciding formulas give contraction; intuitive give less preferred beliefs. obvious obstacle adaptingentrenchment-based contraction Horn logic standard construction refersdisjunctions may non-Horn formulas. Therefore, apply construction method directly Horn logic. get around expressiveness problempropose replace non-Horn formulas Horn approximations (Kautz & Selman, 1996). contraction thus constructed satisfies Horn versions characterisingpostulates entrenchment-based contraction, except Recovery postulate (Alchourronet al., 1985). Due limited expressiveness Horn logic, entrenchment-based Horncontraction comprehensive entrenchment-based contraction. Therefore, characterising entrenchment-based Horn contraction, extra postulate needed capturerestricted (compared entrenchment-based contraction).Ideally, Horn contraction perform rationally AGM contraction. evaluate rationality Horn contractions AGM contraction, propose notionHorn equivalence formalises equivalence Horn contraction functionsAGM contraction functions constructive point view. Horn contractionsrestricted Horn formulas, thus fair say Horn contraction function performsrationally AGM one change Horn formulas result AGMcontraction function identical incurred Horn contraction function. Putsimply, Horn contraction function Horn equivalent AGM counterpart behave identically terms Horn formulas. words, Horn equivalence implieschange mechanism AGM contraction function exactlyHorn contraction function, thus latter preserves every property former.able identify restricted form entrenchment-based contraction one-to-onecorrespondence entrenchment-based Horn contraction means Horn equivalence.Due contention Recovery postulate, Makinson introduced ideaWithdrawal function encompasses broader class belief contraction functions.AGM setting, Rott Pagnucco (1999) explored severe withdrawal, operationsimilar contraction defined intentionally violate Recovery postulate. propose construction severe withdrawal based epistemic entrenchmentrefer entrenchment-based withdrawal. Different entrenchment-based contraction, construction refer arbitrary disjunctions. Moreover, see,Horn contractions intrinsically incompatible Recovery, thus curious see228fiEntrenchment-Based Horn Contractionentrenchment-based withdrawal seamlessly transferable Horn logic. investigating Horn version entrenchment-based withdrawal give affirmativeanswer.rest paper organised follows. first give technical preliminariesSection 2, Section 3 recall details entrenchment-based contraction.Section 4, introduce entrenchment-based Horn contraction present representation theorem. Section 5, compare entrenchment-based Horn contractionentrenchment-based contraction notion Horn equivalence. Section 6introduce entrenchment-based Horn withdrawal demonstrate close connectionentrenchment-based withdrawal. Finally, related work conclusions given Section 7 Section 8 respectively. Proofs given appendix. paper revisedextended version (Zhuang & Pagnucco, 2010).2. Technical Preliminariesassume propositional language L finite set atoms P closedusual truth-functional connectives contains propositional constants > (truth)(falsum). Atoms denoted lower case Roman letters (p, q, . . .). Formulasdenoted lower case Greek letters (, , . . .). Sets formulas denoted upper caseRoman letters (V, X, . . .).logic generated L specified standard Tarskian consequence operatorCn. set formulas X, Cn(X) denotes set formulas following logicallyX. formula , Cn() abbreviates Cn({}). sometimes write X ` denoteCn(X), denote Cn() = Cn(), ` denote Cn(). letterK reserved represent theory belief set set formulasK = Cn(K). Standard propositional semantics assumed. interpretation modelformula true . set formulas X, |X| denotes set modelsX. formula , || abbreviates |{}|.clause disjunction positive negative atoms. Horn clause clausecontains one positive atom, e.g. b c. Horn formula conjunctionHorn clauses. Horn language LH maximal subset L containing Hornformulas. Horn logic generated LH specified consequence operator CnHthat, set Horn formulas X, CnH (X) = Cn(X) LH . letter H reservedrepresent Horn theory Horn belief set set Horn formulasH = CnH (H). Horn subset function H : 2L 2LH given set formulasX, H(X) set Horn formulas X. Formally, H(X) = LH X.3. Entrenchment-Based Contraction.AGM contraction function takes input belief set K formula returns.another belief set K . refer K original belief set, contracting.formula, K resulting belief set. Various constructions proposedcontraction AGM framework. section, review classical one calledentrenchment-based contraction (Gardenfors, 1988; Gardenfors & Makinson, 1988).229fiZhuang & Pagnuccobeliefs held agent equal terms epistemological importance.work Gardenfors (1988) Gardenfors Makinson (1988), importantbeliefs said entrenched others. idea behind entrenchmentbased contraction that, contraction, give less entrenched formulaswhenever possible. relative entrenchments formulas modeled epistemicentrenchments. Given belief set K, epistemic entrenchment associated Kbinary relation L means least entrenched .strict relation < defined 6 . Importantly, satisfies followingconditions:(EE1)(EE2)(EE3)(EE4)(EE5),` ,K 6`, 6 K iff everyevery , `(Transitivity)(Dominance)(Conjunctiveness)(Minimality)(Maximality)Thus epistemic entrenchment transitive relation (EE1) logically strongerformulas entrenched weaker ones (EE2), conjunction equallyentrenched one conjuncts (EE3), non-beliefs least entrenched (EE4),tautologies entrenched (EE5).Entrenchment-based contraction defined two conditions establishconnections epistemic entrenchments contraction functions. Condition (C )generates epistemic entrenchment contraction function. motivation that,contraction K , give either (or both), however,intuitive give epistemically less important one. Thus, retracted,must case least entrenched . limiting casetautologies, equally important and, (EE5), required maximallyentrenched..(C ) : iff 6 K ` ..Condition (C ) derives contraction function epistemic entrenchment. Accord.ing (C ), retained contraction originally believed(i.e., K) either sufficient evidence retaining (i.e., < )possible remove (i.e., ` )...(C ) : K iff K either < ` ..contraction function generated via (C ) referred entrenchment-basedcontraction function..Definition 1 (Gardenfors & Makinson, 1988) function entrenchment-based contraction function K iff outcome determined epistemic entrenchment K.via (C ).pointed work Gardenfors Makinson (1988), rather difficult.motivate (C ), however, appropriateness justified representation theorementrenchment-based contraction.230fiEntrenchment-Based Horn Contraction.Theorem 1 (Gardenfors & Makinson, 1988) function entrenchment-based con.traction function iff satisfies following postulates:.. = Cn(K ).(K 1)K(Closure)..(K 2) K K(Inclusion).. =K(K 3)6 K, K(Vacuity)..(K 4) 6` , 6 K(Success).. +(K 5)K (K )(Recovery).. = K.(K 6), K(Extensionality)....(K 7) K K K(Conjunctive Overlap).. K. K.(K 8)6 K(Conjunctive Inclusion)Theorem 1, entrenchment-based contraction functions characterised full set....AGM postulates contraction (K 1)(K 8). AGM tradition, (K 1)(K 6)..referred basic postulates (K 7) (K 8) referred supplementary postulates. According basic postulates, contraction produces belief set..(K 1) contain contracting formula unless tautology (K 4)..produced belief set larger original one (K 2). contracting formula.believed, nothing done (K 3). contraction syntax-insensitive.(K 6) contracted formula added back contracted belief set,..result entails every formula original belief set (K 5). (K 5), often calledRecovery, controversial subject much discussion (e.g., Makinson, 1987;Hansson, 1991; Levi, 1991). example work Hansson (1991), arguedemerging property rather fundamental postulate contraction. supplementarypostulates concern relations contraction conjunction contractionsconstituent conjuncts. Formulas surviving contraction conjunct also survive.contraction conjunction (K 7). conjunct removed contractionconjunction, formulas surviving contraction conjunction also survive.contraction removed conjunct (K 8)..postulates equivalent (K 7) proposed essential provingrepresentation theorems AGM contractions. presence basic postulates,.(K 7) equivalent postulate Partial Antitony (Alchourron et al., 1985)...(K pa) K Cn() K ( )postulate Conjunctive Trisection (Rott, 1992; Hansson, 1993)....(K ct) K ( ) K ( )..(K pa) rather technical nature, however, (K ct) well motivated. Informallyspeaking, says preferred belief (i.e., ) pair (i.e., , ) least preferredthird belief (i.e., ) added pair. contraction, rational discardless entrenched formulas whenever possible, thus retained contractingmeans entrenched . Since entrenched ,least entrenched one among , , . Therefore, retained contracting. words, preferences cannot changed beliefs considered.231fiZhuang & Pagnucco4. Entrenchment-Based Horn ContractionAlthough entrenchment-based contraction defined assuming Propositional Logic,intuition behind construction universal applied fragments Propositional Logic. section, apply intuition Horn fragment.4.1 Horn StrengtheningStandard entrenchment-based contraction makes use disjunctive formula condi.tion (C ). adapting construction method Horn logic, immediate problemdisjunctive formula may non-Horn. cases, propose replacenon-Horn formula Horn approximations, precisely, Horn strengthenings(Kautz & Selman, 1996). notion Horn strengthening proposed KautzSelman context knowledge compilation. original definition clausessets clauses Horn strengthening clause C logically weakest Hornclause entails C Horn strengthening set clauses {C1 , . . . , Cn } setHorn clauses {C10 , . . . , Cn0 } Ci0 Horn strengthening Ci . reformulatedefinition cover arbitrary formulas that, formula , Horn strengtheninglogically weakest Horn formula entails .Definition 2 Let formula. set Horn strengthenings , denoted HS(),HS() iff1. LH ;2. || ||; and,3. 0 LH || |0 | ||.According Definition 2, Horn formula one Horn strengthening, namely itself.limiting case tautology, assume single Horn strengthening itself.following results provide new properties Horn strengthenings helpfulpaper. Firstly, since Definition 2 model-theoretic, set Horn strengtheningslogically equivalent formulas identical.Lemma 1 , HS() = HS().Horn formula entails another formula necessarily Horn, mustalso entail Horn strengthening .Lemma 2 Horn formula ` , HS()` .Horn strengthening conjunction formed conjoining Horn strengthenings conjunct.Lemma 3 HS( ), 1 HS() 2 HS()1 2 .232fiEntrenchment-Based Horn ContractionHorn strengthening Horn formulas, every Hornstrengthening Horn strengthening .Lemma 4 Let Horn formulas. HS( ), HS( ) HS( ).4.2 Constructionconstructing Horn version entrenchment-based contraction function, alsoneed relation captures relative importance formulas. Since, Horn beliefchange, formulas restricted Horn fragment Propositional Logic, relationHorn formulas. define Horn epistemic entrenchment binary relation LHsatisfies (HEE1)(HEE5):(HEE1)(HEE2)(HEE3)(HEE4)(HEE5),` ,H 6`, 6 H iff everyevery , `(HEE1)(HEE5) simply (EE1)(EE5) restricted Horn formulas Horn beliefsets. derive following properties Horn epistemic entrenchment.Lemma 5 Let Horn epistemic entrenchment. satisfies (cf. (Foo, 1990)):1.2. ,3. < iff <4. ,5. < < , <6. ,7.Particularly, Horn epistemic entrenchment connected (Item 1) logicallyequivalent formulas equally entrenched (Item 7).Clearly, obtain Horn epistemic entrenchment epistemic entrenchmentsimply removing entrenchment relations involving non-Horn formulas. obtainedHorn epistemic entrenchment called Horn subset epistemic entrenchment.Definition 3 Let H Horn epistemic entrenchment P epistemic entrenchment.H Horn subset P iffP iff H, LH .233fiZhuang & PagnuccoNotice epistemic entrenchment one Horn subset different epistemic entrenchments may Horn subset. latter case, epistemic entrenchmentsidentical respect entrenchments Horn formulas differentrespect involving non-Horn formulas..Condition (C ) central determining outcome entrenchment-based contrac.tion functions. According (C ), contraction K , K,disjunction strictly entrenched sufficient condition retaining.. Since arbitrary disjunction, may Horn formula. Therefore, (C )applicable Horn logic.forming similar condition Horn contraction, replace non-Horn disjunctionsHorn strengthenings results following condition:..(HC ): H iff H either ` HS( ) < ..According (HC ), contraction H , H, existence oneHorn strengthening strictly entrenched sufficient condition.retaining . Similar (C ), another sufficient condition tautology. Sincelogically weaker Horn strengthenings, epistemic entrenchment(EE2) implies equal entrenched Horn strengthenings. Thus(EE1) Horn strengthening strictly entrenched ,.. converse, however, hold general. So, informally speaking, (HC ).stricter condition (C ) retaining formulas contraction. refer.Horn contraction function generated via (HC ) entrenchment-based Horn contractionfunction..Definition 4 function entrenchment-based Horn contraction function H iff.outcome determined Horn epistemic entrenchment H via (HC ).p q rp q rp rp qp r qq rHp q rp r p q rp qp r qq p r q rP1p q rp rp qp r qq p r q rP2Figure 1: Entrenchment based Horn contraction function entrenchment based contraction functions determined respectively H , p1 P2 HHorn subset P1 P2 .Figure 1 demonstrates contraction H = CnH ({p q, q r}) p r.contraction determined Horn epistemic entrenchment H via (HC )contraction K = Cn(H) p r contraction determined respectively.P1 P2 via (C ). Notice H Horn subset P1 P2 .234fiEntrenchment-Based Horn Contractionrectangles illustrate formulas Horn belief set H belief set K alongentrenchments. Formulas level rectangle equally entrenched.Formulas level higher strictly entrenched level lower. Nonbeliefs, tautologies conjunctions shown entrenchments uniquelydetermined formulas shown. shaded formulas retained contraction..Lets examine fate p q Horn contraction determined H via (HC ).disjunction p q p r (i.e., p q r) two Horn strengthenings pr p q. Since Horn strengthenings equally less entrenched p.r, p q discarded. contractions determined P1 P2 via (C ),since p q r allowed epistemic entrenchments retention p qdetermined entrenchment p q r compared p r. Observecontraction determined P1 retains Horn formulas Horn contractiondetermined P2 retains more. Section 5, identify entrenchmentbased contraction functions always retain Horn formulas correspondingentrenchment-based Horn contraction function.Condition (C ) concerns generation epistemic entrenchment contraction function. Horn version used generating Horn epistemic entrenchmentHorn contraction function. identical (C ) condition restricted Hornformulas..(HC ): iff 6 H ` .4.3 Characterisationgiving representation theorem entrenchment-based Horn contraction, let usconsider following postulates....(H 1) H = CnH (H )..(H 2) H H..(H 3) 6 H, H = H..(H 4) 6` , 6 H...(H de) H 6 H , HS( ), 6 H...(H wr) H 6 H , H 0 H H 0 , 6 CnH (H 0 )CnH (H 0 {})...(H 6) , H = H..(H f ) ` , H = H...(H hs) H , HS( ) H...(H pa) (H ) CnH () H...(H ct) H , H235fiZhuang & Pagnucco....(H 7) H H H....(H 8) 6 H , H H..postulates well known belief change literature. (H 1)(H 4)..(H 6)(H 8) Horn versions AGM postulates Closure, Inclusion, Vacuity, Suc.cess, Extensionality, Conjunctive Overlap Conjunctive Inclusion respectively. (H f )Horn version Failure postulate (Fuhrmann & Hansson, 1994)..(H de) Horn version Disjunctive Elimination postulate (Ferme, Krevneris,& Reis, 2008):1...(K de) K 6 K , 6 K.(K de) captures minimal change properties contraction; contrapositive form..K K , Kpostulate condition sentence survive contraction process (Fermeet al., 2008, p. 745). Combining existing results (Ferme et al., 2008; Hansson, 1991),....get (K de) implies (K 5) presence (K 2) (K 3). reverse.also true show contrapositive (K de) follows Recovery.2....Thus (K de) equivalent (K 5). (H de) obtained (K de) replacingpossibly non-Horn disjunction Horn strengthenings. deals removalformulas regard related Horn strengthenings that, removed.contracting , Horn strengthenings . Interestingly, (H wr)proposed work Delgrande Wassermann (2010) characterising partial meet..Horn contraction equivalent (H de) presence (H 2). Also, presence...(H 2), (H f ) follows (H de)..Horn Strengthening postulate (H hs) counterpart classic belief change.entrenchment-based contraction, formula retained contracting ,strictly entrenched means, accordance (C ),.K ( ). Thus property expressed as:..K , K ( ).property however postulated explicitly deducible...(K 6) (K 1). Since ( ) logically equivalent , (K 6)....K ( ) = K . K ( ) follows fact K.K logically closed. entrenchment-based Horn contraction, Horn formularetained contracting Horn formula , must Horn strengtheningstrictly entrenched means, accordance (HC ),..H . property captured exactly (H hs). time property.deducible postulates thus postulate explicitly. Notice (H hs)fact Horn adaptation retention property classic case replacingdisjunction Horn strengthenings.1. Disjunctive Elimination originally proposed context belief base change. adaptedbelief set change here...2. K, Recovery ensures K . Hence also K ,.K .236fiEntrenchment-Based Horn Contraction..(H ct) (H pa) Horn versions Conjunctive Trisection Partial Antitony....classical case, (H pa) (H ct) equivalent presence (H 6)...(K pa) (K 7) equivalent. Since proof relies Recovery available.Horn logic, establish equivalence Horn analogues (H pa).(H 7). Proposition 1 summarises connections postulates..Proposition 1 Let contraction function. Then:....1. satisfies (H 1) (H de), satisfies (H f );....2. satisfies (H 2) (H de), satisfies (H wr);...3. satisfies (H wr), satisfies (H de);...4. satisfies (H pa), satisfies (H ct); and,....5. satisfies (H 6) (H ct), satisfies (H pa).Now, give representation theorem entrenchment-based Horn contraction..Theorem 2 function entrenchment-based Horn contraction function iff satis.......fies (H 1)(H 4), (H de), (H 6), (H hs), (H ct), (H 8).Comparing characterisation entrenchment-based contraction, Recovery...Conjunctive Overlap appear instead (H de), (H hs), (H ct) used.Lets go new postulates. condition decomposability proposedwork Flouris, Plexousakis, Antoniou (2004) characterises logics admitcontraction functions satisfy Recovery. Langlois, Sloan, Szorenyi, Turan (2008)verified Horn logic decomposable, thus Horn contraction function.satisfies Recovery. Here, (H de) plays similar role Recovery capturing minimal.change property entrenchment-based Horn contraction. mentioned, (H 7)..(H ct) equivalent Horn case. turns instead (H 7),.property (H ct) needed characterising entrenchment-based Horn contraction....Since (HC ) is, sense, stricter (C ), need (H hs) capture extrastrictness entrenchment-based Horn contraction.Besides characterising postulates, show entrenchment-based Horn con.traction functions satisfy (H 7)..Proposition 2 entrenchment-based Horn contraction function, satisfies.(H 7)........Theorem 2, (H 1)(H 4), (H de), (H 6), (H hs), (H ct), (H 8) characterise entrenchment-based Horn contraction functions. Thus follows immediately.Proposition 2 (H 7) follows postulates......Corollary 1 Let Horn contraction function. satisfies (H 1)(H 4), (H de),.....(H 6), (H hs), (H ct), (H 8), satisfies (H 7).237fiZhuang & PagnuccoBesides representation results, another property entrenchment-based Horn contraction deserves mentioning uniqueness. Horn epistemic entrenchmentdetermines unique entrenchment-based Horn contraction function. Specifically, giventwo distinct Horn epistemic entrenchments 1 2 H two entrenchment..based Horn contraction functions determine 1 2 , formula..H 1 6= H 2 ..Theorem 3 Let 1 2 two different Horn epistemic entrenchments let 1.2 entrenchment-based Horn contraction functions determined 1 2...respectively via condition (HC ). 1 2 identical.Uniqueness property AGM contraction. shown (Alchourron et al., 1985) that,finite case, selection function determines unique partial meet contraction function (OBSERVATION 4.6). desirable property captures intuitiontwo agents different preferences certain domain different contractionoutcomes. Notice uniqueness immediate property Horn contractionrevision. fact, existing Horn contractions Horn revisions enjoyproperty, instance transitively relational partial meet Horn contraction (Zhuang& Pagnucco, 2011), model-based Horn contraction (Zhuang & Pagnucco, 2012),model-based Horn revision (Delgrande & Peppas, 2011).5. Connections Entrenchment-Based Contractionseen entrenchment-based contraction adapted naturally Horn logic,results entrenchment-based Horn contraction. well entrenchment-basedHorn contraction perform comparison classic counterpart? section,introduce notion Horn equivalence Horn contraction comparedAGM contraction.properly defined Horn contraction rational AGM contraction. Horncontraction restrictive AGM contraction deals Horn formulas.reasonable consider Horn formulas comparing Horn contractionAGM contraction. claim Horn contraction rational AGM oneperform identically terms Horn formulas. forms intuition behind Hornequivalence.evaluating possible equivalence two contraction functions, makes sensestart identical belief set check effect two functions contractingformula. intuition fact Horn contraction functionpermits Horn formulas, Horn equivalence defined pairs AGM contraction function..belief set K Horn contraction function H Horn belief set H H.Horn subset K. Consider set Horn formulas resulting belief setcontracting Horn formula, set returned Horn contraction function...H contracting Horn formula, say H Horn equivalent..Definition 5 Let K belief set H Horn belief set H = H(K). Let.AGM contraction function K H Horn contraction function H.238fiEntrenchment-Based Horn Contraction..H Horn equivalent iff..H(K ) = H HLH ...depicted Figure 2, Horn equivalence H stems fact that,..contracting Horn formula , Horn belief set H H returned H Horn..subset belief set K returned .K..H(K )H(K)H.K.H.H H.Figure 2: Horn equivalence AGM contraction function Horn con.traction function H .Obviously, entrenchment-based Horn contraction function Horn equivalententrenchment-based contraction function, determining Horn epistemic entrenchment must Horn subset determining epistemic entrenchment. However,sufficient guarantee Horn equivalence. shown Figure 1, although Hornepistemic entrenchment H Horn subset epistemic entrenchment P2 ,entrenchment-based contraction function determined P2 retains Horn formulasentrenchment-based Horn contraction function determined H .Figure 1, Horn formulas equally entrenched H , P1 P2 . However,non-Horn formula p q r, allowed H , entrenched differently P1P2 . extra preference information non-Horn formula, P1 P2give rise two different entrenchment-based contraction functions. pointHorn epistemic entrenchment consistent several epistemic entrenchments (i.e.,Horn subset), thus entrenchment-based Horn contraction function correspondsseveral entrenchment-based contraction functions. entrenchment-based contraction functions Horn equivalent entrenchment-based Horn contraction function?Essentially, requires us identify epistemic entrenchments determineHorn equivalent entrenchment-based contraction functions. show, followingnecessary sufficient condition purpose:(EE6) , LH , K < HS( )< .condition requires that, pairs Horn formulas , K, strictly lessentrenched , also strictly less entrenched Horn strengthening239fiZhuang & Pagnucco. Since formula entailed Horn strengthenings, verified(EE1) (EE2) converse also true. Thus strictly less entrenchedstrictly less entrenched Horn strengthening ..principal cases, decide whether retain contraction , (C ).compares entrenchment whereas (HC ) compares entrenchmentHorn strengthenings . (EE6) assures comparingcomparing Horn strengthenings . means, entrenchment.satisfies (EE6), mechanism retaining formulas (C ) essentially.(HC ).call entrenchment-based contraction functions whose determining epistemic entrenchments satisfy (EE6) strict entrenchment-based contraction functions..Definition 6 function strict entrenchment-based contraction functionentrenchment-based contraction function whose determining epistemic entrenchment satisfies (EE6).characterise strict entrenchment-based contraction functions need, addition...(K 1)(K 8), classic case (H hs):...(K hs) , LH , K HS( ) K ..fact, (EE6) corresponds exactly (K hs)...Theorem 4 function strict entrenchment-based contraction function iff satisfies...(K 1)(K 8) (K hs).main result section, entrenchment-based Horn contraction functionHorn equivalent strict entrenchment-based contraction function vice versa..Theorem 5 H entrenchment-based Horn contraction function,...strict entrenchment-based contraction function H Horn equivalent..strict entrenchment-based contraction function, entrenchment...based Horn contraction function H H Horn equivalent.Moreover, entrenchment-based contraction function Horn equivalententrenchment-based Horn contraction function strict entrenchment-based contractionfunction...Theorem 6 Let H entrenchment-based Horn contraction function...entrenchment-based contraction function H Horn equivalent,.strict entrenchment-based contraction function.Horn equivalence local feature deals equivalence specificcontraction functions. number functions constructed according particularconstruction method contraction function represents possible way contractingformula. Thus also compare Horn contraction AGM contraction termspossible ways contracting formula. Horn contraction comprehensiveAGM contraction Horn contraction permits possible ways contracting Horn240fiEntrenchment-Based Horn Contractionformula AGM does. Due allowance non-Horn formulas, waysforming epistemic entrenchments Horn epistemic entrenchment; thus waysgenerating entrenchment-based contraction functions. Figure 1 entrenchmentbased Horn contraction function corresponds (i.e., Horn equivalent to)entrenchment-based contraction function determined P2 . Entrenchment-based Horncontraction therefore comprehensive entrenchment-based contraction. However,due Horn contraction defined due limited expressivenessHorn logic compared Propositional Logic. Entrenchment-based Horn contractionequally comprehensive strict entrenchment-based contraction.6. Entrenchment-Based Horn WithdrawalDue controversy Recovery postulate AGM setting, Makinson (1987)proposed operation called withdrawal satisfies five basic AGM contractionpostulates necessarily Recovery. Since Horn contraction intrinsically incompatibleRecovery, makes sense explore Horn version withdrawal.Since Horn contraction originates entrenchment-based contraction, focuswithdrawal version entrenchment-based contraction. withdrawal characterised Rott Pagnucco (1999) referred severe withdrawal. uniformity,call entrenchment-based withdrawal. epistemic entrenchment also assumedentrenchment-based withdrawal withdrawal outcome determined followingcondition:..(W ) : K iff K either < ` ..Definition 7 (Rott & Pagnucco, 1999) function entrenchment-based withdrawal.function K iff outcome determined epistemic entrenchment K via (W )..Notice (W ) also used generate epistemic entrenchment withdrawalfunction as, reverse reading, strictly entrenched wheneverretained withdrawal .Rott Pagnucco (1999) showed characterise entrenchment-based withdrawalneed full set AGM contraction postulates except Recovery also needreplace Conjunctive Overlap much stronger Antitony Condition..Theorem 7 (Rott & Pagnucco, 1999) function entrenchment-based withdrawal.....function iff satisfies following postulates: (K 1)(K 4), (K 6), (K 8),.. K. (Antitony Condition)(K 7a)6` , KAdapting entrenchment-based withdrawal Horn logic straightforward everythingHorn expressible. obviously need Horn epistemic entrenchment. restricting.Horn belief sets Horn formulas (W ) recast follows:..(HW ) : H iff H either < ` ..Horn withdrawal function generated via (HW ) referred entrenchment-basedHorn withdrawal function.241fiZhuang & Pagnucco.Definition 8 function entrenchment-based Horn withdrawal function H iff.outcome determined Horn epistemic entrenchment H via (HW ).obvious constructions that, unlike entrenchment-based Horn contraction,entrenchment-based Horn withdrawal change mechanism origin.Therefore, entrenchment-based Horn withdrawal characterised exactly Horn versionscharacterising postulates entrenchment-based withdrawal...Theorem 8 function entrenchment-based Horn withdrawal function iff satis....fies following postulates: (H 1)(H 4), (H 6), (H 8),...(H 7a) 6` H H .show entrenchment-based Horn withdrawal entrenchment-based withdrawal one-to-one correspondence notion Horn equivalence..Theorem 9 H entrenchment-based Horn withdrawal function,....entrenchment-based withdrawal function H Horn equivalent.entrenchment-based withdrawal function, entrenchment-based Horn...withdrawal function H H Horn equivalent.7. Related Workexisting approaches Horn contraction different Horn variants partialmeet contraction (Alchourron et al., 1985). notion remainder sets centralconstruction partial meet contraction outcome contractionintersection candidate remainder sets chosen selection function. standarddefinition, remainder set K respect maximal subset K failsimply . maximal nature implies following model-theoretic behaviour remaindersets (e.g., see Gardenfors 1998, p. 86). set models remainder setunion set models K model . Furthermore, bijectionremainder sets models . Another property oftenreferred convexity property states belief set subset outcomemaxichoice contraction superset outcome full meet contractionoutcome partial meet contraction. properties behaviours howevermutually exclusive remainder sets Horn logic, gives rise three Hornvariants remainder sets thus three Horn variants partial meet contraction. avoidconfusion, remainder sets Propositional Logic referred classic remaindersets.first work Horn contraction Delgrande (2008). seminal paper,Delgrande studied Horn analogue orderly maxichoice contraction (Gardenfors, 1988)term orderly maxichoice Horn contraction. remainder sets definedstandard definition means maximal, thus call maximalremainder set. orderly maxichoice contraction, transitive antisymmetric relationmaximal remainder sets assumed selecting single best remainder setcontraction outcome.242fiEntrenchment-Based Horn ContractionBooth, Meyer, Varzinczak (2009) suggested that, orderly maxichoice Horncontractions appropriate choices Horn contraction, constituteappropriate forms Horn contraction. authors argued Horn contractionsatisfy convexity property. demonstrated, case Horn contractions based maximal remainder sets. proposed infra Horn contraction (Booth,Meyer, Varzinczak, & Wassermann, 2010, 2011). Infra Horn contraction basednotion infra remainder set. set infra remainder sets H respect Hornformula consist Horn belief set subset maximal remainder setsuperset intersection maximal remainder sets . notioninfra remainder set subsumes maximal remainder set maximal remainderset infra one vice versa.noticed Delgrande Wassermann (2010, 2013), maximal infra remainder sets exhibit model-theoretic behaviour classic remainder sets.therefore give another Horn variant called partial meet Horn contraction. basednotion weak remainder set defined behave classic remainder setsmodel-theoretically. show weak remainder sets subsume maximal remainder setsweak remainder sets infra remainder sets subsume one another. Thus partialmeet Horn contraction infra Horn contraction comprehensive oneanother. nice thing weak remainder sets one-to-one correspondence classic remainder sets implies one-to-one correspondencepartial meet Horn contraction partial meet contraction. means contractionspermitted partial meet Horn contraction correspond exactly permitted partial meet contraction. Zhuang Pagnucco (2011) studied partial meet Horncontraction assuming preorder weak remainder sets. results construction transitively relational partial meet Horn contraction Horn analoguetransitively relational partial meet contraction (Alchourron et al., 1985).Apart partial meet contraction, Zhuang Pagnucco (2012) studied Hornanalogue model-based contraction (Katsuno & Mendelzon, 1992) called modelbased Horn contraction. classic case (Katsuno & Mendelzon, 1992), pre-orderinterpretations assumed. shown model-based Horn contraction identicaltransitively relational partial meet contraction.Partial meet Horn contraction infra Horn contraction differ entrenchmentbased Horn contraction assume explicit preference information.Zhuang Pagnucco (2012) showed entrenchment-based Horn contraction restricted form model-based Horn contraction thus restricted form transitively relational partial meet Horn contraction partial meet Horn contraction. exact relationship established entrenchment-based Horn contraction infra Horn.contraction (H hs) incompatible infra Horn contraction one characterising postulate infra Horn contraction (i.e., Core-Retainment) incompatibleentrenchment-based Horn contraction. similar reasons exact relationshipestablished entrenchment-based Horn contraction orderly maxichoice Horncontraction.shown Section 5, entrenchment-based Horn contraction comprehensiveentrenchment-based contraction. respect, partial meet Horn contractiontransitively relational partial meet Horn contraction outperform entrenchment-based Horn243fiZhuang & Pagnuccocontraction bijections exist partial meet Horn contractions partial meetcontractions transitively relational partial meet Horn contraction transitively relational partial meet contraction. However, bijection comes price. ZhuangPagnucco (2011) showed Horn logic expressive enough Horn contractiondistinguish different pre-orders weak remainder sets. different pre-ordersmay generate identical transitively relational partial meet Horn contraction functions.observation termed loss uniqueness counterintuitive means althoughtwo agents different preferences certain domain, always endidentical contraction outcomes. Entrenchment-based Horn contraction sufferproblem. Horn epistemic entrenchment determines unique entrenchment-basedHorn contraction.Beside aforementioned works Horn contractions, Delgrande Peppas (2011)studied model-based Horn revision manner classic model-based revision (Katsuno & Mendelzon, 1992). AGM setting, revision defined contractionvia called Levi identity (Levi, 1991). Zhuang, Pagnucco, Zhang (2013) exploredconnection Horn setting showed that, proper restrictions, Horn revision defined Horn contraction via variant Levi identity. alsoshowed Horn revision functions generated transitively relational partial meetHorn contraction functions model-based Horn contraction functions suffercounter-intuitive results whereas one generated entrenchment-based Horn contraction functions guaranteed meaningful (i.e., satisfies revision postulatesproposed Delgrande & Peppas, 2011). Finally, Adaricheva, Sloan, Szorenyi (2012)provided complexity results regarding Horn contractions.major construction methods AGM contraction adaptedHorn logic. principle adaptations make adapted contractionclose possible AGM contraction (i.e., equally rational aspects). explainedabove, due limited expressiveness Horn logic, none adapted contractionsidentical AGM origin aspects. However, means declaring failureadaptations emphasised Section 5 fair expect rationalityHorn contractions extent expressiveness Horn logic permits. Sinceadaptations focus different aspects AGM contraction often exhibitdesirable undesirable properties, convincing argue one outperformsother. best choice Horn contraction depends actual application.explicit preference expected loss uniqueness avoided entrenchmentbased Horn contraction better choice.8. Conclusionpaper, defined entrenchment-based Horn contraction Horn analogue entrenchment-based contraction. outcome Horn contraction determined epistemic entrenchments Horn formulas via refinement condition..(C ) proposed Gardenfors Makinson (1988). Since condition (C ) involves ar.bitrary disjunctions may Horn formulas, refined condition (HC ) con..siders Horn strengthenings disjunctions. (HC ) closely resembles (C ),entrenchment-based Horn contraction function performs identically classic coun244fiEntrenchment-Based Horn Contractionterpart (i.e., strict entrenchment-based contraction function) Horn formulascounted. Since non-Horn formulas disallowed, set epistemic entrenchmentsdetermining entrenchment-based Horn contraction proper subset determining entrenchment-based contraction. Entrenchment-based Horn contraction thereforecomprehensive entrenchment-based contraction. able identify restrictedform entrenchment-based contraction called strict entrenchment-based contractionone-to-one correspondence entrenchment-based Horn contraction.Recently, Creignou, Papini, Pichler, Woltran (2014) investigated revision operatorsfragments Propositional Logic limited Horn. work promotes newdirection furthering study Horn belief change. many Horn contractionsrevisions, important find adaptation strategies generalisablefragments Propositional Logic Horn. Thus future work, aim developgeneralised entrenchment-based contraction applied arbitrary fragmentsPropositional Logic.Appendix A. Proofs Resultsfirst present notions properties Horn logic used proofs.intersection two interpretations interpretation assigns true atomsassigned true interpretations. denote intersection interpretations. Given set interpretations , closure intersectiondenoted Cl (M ). Formally,Cl (M ) = { | , = }.Horn formula, set models closed Cl called Horn closed. Conversely, Horn closed set models corresponds unique Horn formula (modulological equivalence). Moreover, intersections Horn closed sets models also Hornclosed.Lemma 1 , HS() = HS().Proof: result immediate definition Horn strengthening syntax insensitive.Lemma 2 Horn formula ` , HS()` .Proof: HS(), result trivially holds. Suppose 6 HS(),definition Horn strengthening 1 LH || |1 | ||. Again,1 6 HS(), must 2 LH |1 | |2 | ||. Since || finite,eventually find n Horn strengthening . Since || |n |, ` n .Lemma 3 HS( ), 1 HS() 2 HS()1 2 .Proof: Suppose HS( ), definition Horn strengthening |||| || implies || || || ||. follows Lemma 21 HS() 2 HS() || |1 | || |2 |, thus || |1 | |2 |.245fiZhuang & PagnuccoAssume |1 | |2 | 6 ||. since |1 | |2 | || |||| ||. follows HS( ), || ||, 6 |||| 6 || ||. since , |1 | |2 | |1 | |2 |implies || ||, contradiction! Therefore, |1 | |2 |6 || implies |1 | |2 | ||.Lemma 4 Let Horn formulas. HS( ), HS( ) HS( ).Proof: Suppose HS() 0 HS(), need show 0 HS().definition Horn strengthening, HS() implies || ||, 0 HS()implies |0 | ||. follows || || || || || ||implies |0 | | |. remains show Horn formula 00|0 | |00 | | |.Assume contrary Horn formula 00 |0 | |00 | | |.follows |0 | |00 |, 0 HS( ), definition Horn strengthening|00 | 6 | |. set theory |00 | \ |0 | || \ | |. follows6 || HS( ) || = 6 | |.also 6 || otherwise || =6 Cl (||) implies ||. Assume 6 |0 |x | | x = 6 | |. x || || =6 Cl (||),contradiction! x ||, || 6= Cl (||), contradiction! Thus conclude|0 | implies |00 |. follows |00 |, = 6 | ||00 | =6 Cl (|00 |), contradiction! Thus 00 |0 | |00 | | |.Lemma 5 Let Horn epistemic entrenchment. satisfies:1.2. ,3. < iff < .4. ,5. < < , <6. ,7.Proof: proofs identical propositional case..Proposition 1 Let Horn contraction function.....1. satisfies (H 1) (H de), satisfies (H f )....2. satisfies (H 2) (H de), satisfies (H wr)...3. satisfies (H wr), satisfies (H de)...4. satisfies (H pa), satisfies (H ct)246fiEntrenchment-Based Horn Contraction....5. satisfies (H 6) (H ct), satisfies (H pa)Proof: 1. Suppose ` . H, ` implies HS() = .....Since H = CnH (H ), tautologies H . Thus H . follows...contrapositive (H de), H, H H . Thus.H = H...2. & 3. show (H wr) (H de) equivalent following postulate:...(H mc) H \ H |H | 6 | |.....first show satisfies (H de) iff satisfies (H mc). one direction, suppose..satisfies (H de). Let H \ H . two cases:..Case 1, LH : HS( ) = { }. (H de) H 6`.implies |H | 6 | |..Case 2, 6 LH : Assume |H | | |. Lemma 2, HS( )...H ` . However, follows (H de) H 6` HS( ),contradiction!...direction, suppose satisfies (H mc). Since |H | 6 | |,.|H | 6 | |. Since || | | HS( ), 6 ||..HS( ). model theory, follows |H | H 6`HS( )....show satisfies (H wr) iff satisfies (H mc).....one direction, suppose satisfies (H wr). Let H \ H . Assume |H |.0| | set theory |H | || ||. set theory, H..|H 0 | |H | |H 0 | || 6= , follows |H | || ||.|H 0 | || || implies H 0 {} 6` . However, (H wr)one H 0 H 0 {} ` , contradiction!...direction, suppose satisfies (H mc). Let H \ H ...(H mc) |H | || 6 ||. Let Horn belief set H 0..|H 0 | = {}. |H | implies |H 0 | |H | || implies |H 0 | ||...follows |H 0 | |H | |H 0 | || H H 0 H 0 6` . follows|H 0 | = {} 6 || |H 0 | || = . Thus H 0 {} inconsistent. formulasfollow inconsistent set .4. & 5. proofs identical propositional case found Page 117(Hansson, 1999) (OBSERVATION 2.59)..Theorem 2 function entrenchment-based Horn contraction function iff satisfies.......(H 1)(H 4), (H de), (H 6), (H hs), (H ct), (H 8).Proof: one direction, let H Horn belief set, Horn epistemic entrenchment.H, entrenchment-based Horn contraction function H determined........ need show satisfies (H 1)(H 4), (H de), (H hs), (H ct), (H 8)...(H 2): Follows directly (HC )....(H 1): ` , follows (HC ) H = H. follows...H = CnH (H) H = CnH (H ). Suppose 6` CnH (H ), need show..(H ). (HC ), suffices show H HS( )< . two cases:247fiZhuang & Pagnucco.Case 1, 6` : Since CnH (H ), compactness Horn logic, finite..subset {1 , . . . , n } H 1 n ` . Since {1 , . . . , n } H , follows.(H 2) {1 , . . . , n } H. follows {1 , . . . , n } H 1 n `.H ` . follows H ` H = CnH (H) H. follows (HC )HS( ) < . follows Lemma 5(Part 5) < 1 n . Since 1 n ` ( 1 ) ( n )Lemma 2 HS(( 1 ) ( n )) 1 n ` .Thus < . Since ( 1 ) ( n ) (1 n ), Lemma 1,HS(( 1 ) ( n )) = HS( (1 n )). Thus HS( (1 n )).Since (1 n ) ` , ` implies Lemma 20 HS( ) ` 0 implies 0 . Since < follows (HEE1)..< 0 . (HC ), follows H, < 0 H .Case 2, ` : follows H = CnH (H) H. Also ` implies ` .Thus (HEE2) (HEE5), . Since 6` , follows Lemma 5(Part 1) (HEE5) < . (HEE1), follows< < . definition Horn strengthening, `.implies HS( ) = { }. (HC ), follows H <.H .....(H 3): Suppose 6 H, need show H = H. H H follows (H 2)..Let H. suffices show H . (HEE4) Lemma 5 (Part 1), followsH LH < . Since `Lemma 2 HS( ) ` . follows ` , (HEE2),. Since 6 H, follows (HEE4) . apply (HEE1)., < , , obtain < . (HC ), follows H <.H ...(H 4): Suppose 6` . need show 6 H . (HEE2), ` implies .Lemma 5 (Part 1), implies 6< . Since HS( ) = HS() = {}, follows..6< 6` , (HC ), 6 H .....(H 6): Suppose . first show H H . Let H . follows.(HC ) H either ` HS( ) < ..Case 1, ` : Since , ` implies ` . follows ` H, (HC ),.H .Case 2, 6` : HS( ) < . Since ,. Lemma 1, implies HS( ) = HS( ). ThusHS( ) follows HS( ). intersubstitutativity , < follows..< . follows H < , (HC ), H .....Thus H H . show H H way....(H de): Suppose H \ H . need show HS( ), 6 H ...follows (HC ) 6 H HS( ), 6< . Lemma 4,HS( ), HS( ) HS( ). Thus 0 HS( ),..6< 0 . follows (HC ) HS( ), 6 H ..(HC ): one direction, suppose H . need show..` . Since H (HC ) H either `HS(( ) ) < . Since ( ) , Lemma 1248fiEntrenchment-Based Horn ContractionHS(( ) ) = HS() = {}. Lemma 5 (Part 6), implies .connectivity , implies 6< . Thus must case ` ..direction, suppose either 6 H ` , need show .` , ` follows (HEE2) ` ...suppose 6` 6 H . follows (HC ) either 6 HHS(( ) ), 6< . former case, 6 H gives usrequired (HEE4). latter case, since ( ) , Lemma 1,HS(( ) ) = HS() = {}. Thus 6< . connectivity , 6<implies . follows (HEE2) ` . follows, (HEE1) ...(H hs): Suppose H . need show HS( )...H . follows H (HC ) HS( )..< . Since satisfies (HC ), follows < H ....(H ct): Suppose H , need show H . (HC ),.H implies < . follows (HEE2) ` .follows (HEE1), , < < . (HC ), < implies.H .....(H 8): Suppose 6 H . need show H H . 6 H.....(H 3) H = H. H H follows (H 2)..suppose H. (HC ), 6 H implies . Lemma 5 (Part 6),..implies . Let H , suffices show H . follows..H (HC ) HS(( ) ) < .Lemma 1, ( ) ( ) ( ) implies HS(( ) ) = HS(( ) ( )).Thus Lemma 3, 1 HS( ) 2 HS( ) 1 2 .Lemma 5 (Part 7), < 1 2 . follows (HEE2) 1 2 ` 11 2 1 . follows (HEE1), , < 1 2 , 1 2 1..< 1 . follows 1 HS( ) < 1 , (HC ), H ..direction, let H Horn belief set Horn contraction function......H satisfies (H 1)(H 4), (H de), (H hs), (H ct), (H 8). suffices showHorn epistemic entrenchment determined (HC ) satisfies (HEE1)(HEE5).(HC ). replacing AGM contraction postulates corresponding Hornanalogues, proof satisfaction (HEE1)(HEE5) Theorem.2.50 (Hansson, 1999). therefore give proof (HC )...(HC ): one direction, suppose H . need show H either `..HS( ) < . Since H , (H hs)..HS( ) H . (HC ), H implies < ...(H 2), H implies H.direction, suppose H either ` HS( )..< . need show H . ` H = H. Thus H implies..H . suppose 6` . suffices show H \ H HS( ),.... (H de), H \ H implies HS( ), 6 H . Assume.....H . (H 2), H implies 6 H . follows (H 8)......6 H H H . follows 6 H H H...6 H , contradiction. Thus 6 H . (HC ), 6 Himplies .249fiZhuang & Pagnucco.Proposition 2 entrenchment-based Horn contraction function, satisfies.(H 7)..Proof: Let entrenchment-based Horn contraction function H as..sociated Horn epistemic entrenchment . Theorem 2 satisfies (H 1).......(H 4), (H de), (H hs), (H ct), (H 8). Suppose (H ) (H ). need.show H ....Case 1, ` : . follows (H 6) H = H , thus.H .Case 2, ` : proof similar Case 1..Case 3, 6` 6` : follows (HEE2) . follows H.(HC ) 1 HS() < 1 . conclude (HEE1).< 1 . Similarly, follows (HEE2) H.(HC ) 2 HS( ) < 2 , thus < 2 .Lemma 5 (Part 5), deduce < 1 < 2 , < 1 2 .definition Horn strengthening 1 ` | 2 ` implies1 2 ` ( ) ( ). Since ( ) ( ) ( ) , 1 2 ` ( ) .follows Lemma 2 HS(( ) ) 1 2 ` .(HEE2), 1 2 ` implies 1 2 . (HEE1), < ...Finally, follows < (HC ), H ...Theorem 4 function strict entrenchment-based contraction function iff satisfies...(K 1)(K 8) (K hs)..Proof: one direction, suppose strict entrenchment-based contraction functionK associated epistemic entrenchment . satieties (EE6). Since strictentrenchment-based contraction function entrenchment-based contraction function,....Theorem 1 satisfies (K 1)(K 8) (C ). remains show satisfies..(K hs). Suppose , LH K . need show HS( ).K . ` definition Horn strengthening ` thus..result trivially holds. suppose 6` . follows K (C ).< . Since satisfies (EE6), HS( ) < . Since.satisfies (C ), < implies K .....direction, suppose function satisfies (K 1)(K 8) (K hs).need show strict entrenchment-based contraction function. follows The.orem 1 entrenchment-based contraction function. remains show.epistemic entrenchment generated via (C ) satisfies (EE6). Suppose , LH.< . need show HS( ) < . (C ),..< implies K . follows (K hs) HS( ).K implies (C ) < ..Theorem 5 H entrenchment-based Horn contraction function, strict....entrenchment-based contraction function H Horn equivalent.strict entrenchment-based contraction function, entrenchment-based...Horn contraction function H H Horn equivalent.Proof: Part 1: Let K belief set H Horn belief set H = H(K)..Suppose H entrenchment-based Horn contraction function H determined250fiEntrenchment-Based Horn ContractionHorn epistemic entrenchment H H. Let binary relation L, LH(1) iff H ,(2) < iff HS( ) <H .expand rest non-Horn formulas form epistemic en.trenchment. Let entrenchment-based contraction function K determined.. Due definition (i.e., (2)), satisfies (EE6) implies strictentrenchment-based contraction function...Suppose LH . need show H(K ) = H H . ` , follows....(H f ) classic version (follows (K 5)) H H = H K = K. Thus...H(K ) = H H follows H = H(K). suppose 6` . first show H H....H(K ). Let H H . suffices show K . (HC ), follows.H H H HS( ) <H .definition (i.e., (2)) < . Also since H = H(K), H implies K...follows K < (C ) K ....show H(K ) H H . Suppose LH K . need...show H H . (C ), follows K K < .definition HS( ) <H . Also sinceH = H(K), H follows K fact Horn formula. Finally,..follows H <H (HC ) H H .Part 2: part proved similar manner Part 1. time needgenerate Horn epistemic entrenchment epistemic entrenchmentHorn epistemic entrenchment Horn subset epistemic entrenchment...Theorem 6 Let H entrenchment-based Horn contraction function....entrenchment-based contraction function H Horn equivalentstrict entrenchment-based contraction function...Proof: Let H entrenchment-based Horn contraction function H..entrenchment-based contraction function K H = H(K) HHorn equivalent....Theorem 4, suffices show satisfies (K hs). Let , LH . Suppose K ....need show HS( ) K . Since H Horn...equivalent, follows K H H . follows (H hs).HS( ) H H . Horn equivalence....H , H H implies K H ...Theorem 8 function entrenchment-based Horn withdrawal function iff satisfies....following postulates: (H 1)(H 4), (H 6), (H 8),.. H. .(H 7a)6` , HProof: proof identical propositional case found workRott Pagnucco (1999, pages 538540)..Theorem 9 H entrenchment-based Horn withdrawal function,....entrenchment-based withdrawal function H Horn equivalent.251fiZhuang & Pagnuccoentrenchment-based withdrawal function, entrenchment-based Horn...withdrawal function H H Horn equivalent.Proof: Part 1: Let K belief set H Horn belief set H = H(K)..Suppose H entrenchment-based Horn withdrawal function H determinedHorn epistemic entrenchment H H. Letiff H , LH .expand non-Horn formulas form epistemic entrenchment..Let entrenchment-based withdrawal function K determined....via (W ). ` , follows (K 3) (H 3) K = K...H H = H. Thus H(K ) = H H follows H = H(K). suppose 6` . first.....show H H H(K ). Let H H . suffices show K . (HW ),.follows H H H <H . Since H = H(K), H implies K.definition , <H implies < . follows K <..(W ) K ....show H(K ) H H . Let Horn formula K ....suffices show H H . (W ), follows K K < .Since H = H(K), H follows K fact Horn formula. SinceHorn formulas < , definition that, definition., < implies <H . follows H <H , (HW ).H H .Part 2: part proved similar manner Part 1. time needgenerate Horn epistemic entrenchment epistemic entrenchmentHorn epistemic entrenchment Horn subset epistemic entrenchment.ReferencesAdaricheva, K., Sloan, R. H., & Szorenyi, B. (2012). Horn belief contraction: Remainders,envelopes complexity. Proceedings 13th International ConferencePrinciples Knowledge Representation Reasoning (KR-2012), pp. 107115.Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). logic theory change:Partial meet contraction revision functions. Journal Symbolic Logic, 50 (2),510530.Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2003).Description Logic Handbook. Cambridge University Press, Cambridge, UK.Booth, R., Meyer, T., & Varzinczak, I. J. (2009). Next steps propositional Horn contraction. Proceedings 21st International Joint Conference Artificial Intelligence (IJCAI-2009), pp. 702707.Booth, R., Meyer, T., Varzinczak, I. J., & Wassermann, R. (2010). Horn belief change:contraction core. Proceedings 20th European Conference ArtificialIntelligence (ECAI-2011), pp. 10651066.Booth, R., Meyer, T., Varzinczak, I. J., & Wassermann, R. (2011). linkpartial meet, kernel, infra contraction application horn logic. JournalArtificial Intelligence Research, 42, 3153.252fiEntrenchment-Based Horn ContractionCreignou, N., Papini, O., Pichler, R., & Woltran, S. (2014). Belief revision within fragmentspropositional logic. Journal Computer System Sciences, 80 (2), 427449.Delgrande, J. P. (2008). Horn clause belief change: Contraction functions. Proceedings11th International Conference Principles Knowledge RepresentationReasoning (KR-2008), pp. 156165.Delgrande, J. P., & Peppas, P. (2011). Revising Horn Theories. Proceedings 22ndInternational Joint Conference Artificial Intelligence (IJCAI-2011), pp. 839844.Delgrande, J. P., & Wassermann, R. (2010). Horn clause contraction function: Belief setbelief base approaches. Proceedings 12th International ConferencePrinciples Knowledge Representation Reasoning (KR-2010), pp. 143152.Delgrande, J. P., & Wassermann, R. (2013). Horn clause contraction functions. JournalArtificial Intelligence Research, 48, 475551.Ferme, E., Krevneris, M., & Reis, M. (2008).axiomatic characterizationensconcement-based contraction. Journal Logic Computation, 18 (5), 739753.Flouris, G., Plexousakis, D., & Antoniou, G. (2004). Generalizing AGM postulates: preliminary results applications. Proceedings 10th International WorkshopNon-Monotonic Reasoning (NMR-2004), pp. 171179.Foo, N. Y. (1990). Observations AGM entrenchment. Tech. rep. 389, Basser DepartmentComputer Science, University Sydney.Fuhrmann, A., & Hansson, S. O. (1994). survey multiple contractions. JournalLogic, Language Information, 3 (1), 3974.Gardenfors, P. (1988). Knowledge Flux: Modelling Dynamics Epistemic States.MIT Press.Gardenfors, P., & Makinson, D. (1988). Revisions knowledge systems using epistemic entrenchment. Proceedings 2nd conference Theoretical Aspects ReasoningKnowledge (TARK-1988), pp. 8395.Hansson, S. O. (1991). Belief Contraction Without Recovery. Studia Logica, 50 (2), 251260.Hansson, S. O. (1993). Changes disjunctively closed bases. Journal Logic, LanguageInformation, 2 (4), 255284.Hansson, S. O. (1999). Textbook Belief Dynamics Theory Change Database Updating. Kluwer.Katsuno, H., & Mendelzon, A. O. (1992). Propositional knowledge base revision minimal change. Artificial Intelligence, 52 (3), 263294.Kautz, H., & Selman, B. (1996). Knowledge compilation theory approximation. JournalACM, 43, 193224.Langlois, M., Sloan, R. H., Szorenyi, B., & Turan, G. (2008). Horn complements: TowardsHorn-to-Horn belief revision. Proceedings 23rd National Conference Artificial Intelligence (AAAI-2008), pp. 466471.Levi, I. (1991). Fixation Beliefs Udoing. Cambridge University Press.253fiZhuang & PagnuccoMakinson, D. (1987). status postulate recovery logic theorychange. Journal Philosophical Logic, 16 (4), 383394.Rott, H. (1992). Preferential belief change using generalised epistemic entrenchment. Journal Logic, Language Information, 1 (1), 4578.Rott, H., & Pagnucco, M. (1999). Severe withdrawal (and recovery). Journal PhilosophicalLogic, 28 (5), 501547.Zhuang, Z., & Pagnucco, M. (2010). Horn contraction via epistemic entrenchment. Proceedings 12th European Conference Logics Artificial Intelligence (JELIA2010), pp. 339351.Zhuang, Z., & Pagnucco, M. (2011). Transitively Relational Partial Meet Horn Contraction.Proceedings 22nd International Joint Conference Artificial Intelligence(IJCAI-2011), pp. 11321138.Zhuang, Z., & Pagnucco, M. (2012). Model Based Horn Contraction. Proceedings13th International Conference Principles Knowledge RepresentationReasoning (KR-2012), pp. 169178.Zhuang, Z., Pagnucco, M., & Zhang, Y. (2013). Definability horn revision Horncontraction. Proceedings 23rd International Joint Conference ArtificialIntelligence (IJCAI-2013), pp. 12051211.254fiJournal Artificial Intelligence Research 51 (2014) 443-492Submitted 05/14; published 10/14Push Rotate: Complete Multi-agent Pathfinding AlgorithmBoris de WildeAdriaan W. ter MorsCees WitteveenBOREUS @ GMAIL . COM. W. TERMORS @ TUDELFT. NLC . WITTEVEEN @ TUDELFT. NLFaculty Electrical Engineering, Mathematics, Computer ScienceMekelweg 4, 2628 CD Delft, NetherlandsAbstractMulti-agent Pathfinding relevant problem wide range domains, examplerobotics video games research. Formally, problem considers graph consisting verticesedges, set agents occupying vertices. agent move unoccupied,neighbouring vertex, problem finding minimal sequence moves transferagent start location destination NP-hard problem.present Push Rotate, new algorithm complete Multi-agent Pathfindingproblems least two empty vertices. Push Rotate first divides graphsubgraphs within possible agents reach position subgraph, usessimple push, swap, rotate operations find solution; post-processing algorithmalso presented eliminates redundant moves. Push Rotate seen extending LunaBekriss Push Swap algorithm, showed incomplete previous publication.experiments compare approach Push Swap, MAPP, Bibox algorithms. latter algorithm restricted smaller class instances requires biconnectedgraphs, nevertheless considered state art due strong performance. experiments show Push Swap suffers incompleteness, MAPP generally competitivePush Rotate, Bibox better Push Rotate randomly generated biconnectedinstances, Push Rotate performs better grids.1. IntroductionComputer scientists roboticists long studied problem coordinating motionsmultiple moving objects. general formulation problem find conflict-free trajectoriesspace time objects. Cast warehousemans problem, problemproved PSPACE-complete Hopcroft et al. (1984). problems complexity reducedNP-complete assuming agents move along graph (Goldreich, 1993). graph(or roadmap) given, instance applications automated guided vehicles followinglines drawn factory floors (Roszkowska & Reveliotis, 2008), also learned (Kavraki,Svestka, Latombe, & Overmars, 1996) otherwise constructed (LaValle & Kuffner, 2001).Application domains multi-agent pathfinding include many forms robotics, instancemobile robots (Simeon, Leroy, & Laumond, 2002) robot arms (Erdmann & Lozano-Perez,1987); routing automated guided vehicles, instance container terminals (Vis, 2006;Gawrilow, Kohler, Mohring, & Stenzel, 2007) warehousing manufacturing (Narasimhan,Batta, & Karwan, 1999; Desaulniers, Langevin, Riopel, & Villeneuve, 2004); video games,role-playing games real-time strategy games (Nieuwenhuisen, Kamphuis, & Overmars, 2007);airport taxi routing (Trug, Hoffmann, & Nebel, 2004; Ter Mors, Zutt, & Witteveen, 2007) collision avoidance airplanes flight (Sislak, Pechoucek, Volf, Pavlcek, Samek, Mark, & Losiewicz,c2014AI Access Foundation. rights reserved.fiD E W ILDE , ER ORS & W ITTEVEEN2008); routing multi-agent pathfinding also occurs less obvious domains planning mining carts (Beaulieu & Gamache, 2006) routing sheets paper modularprinter (Ruml, Do, Zhou, & Fromherz, 2011).remarked Surynek (2011), applicability algorithmic approaches dependsfreedom agents respective application domains. Incomplete reservation-based approaches, agents typically plan independently, minding reservations others (Lee,Lee, & Choi, 1998), useful case enough room infrastructure guarantee (orleast make highly likely) solution found. paper, discuss approachesalso deal congested scenarios, majority locationsoccupied agents. inspiration situation real-time strategy computer games,large numbers units, friendly hostile, competing room move around(see Figure 1).Figure 1: Zerg attack Protoss Starcraft 2. maps original Starcraftmade available research http://movingai.com/benchmarks/.Kornhauser (1984) gave complete algorithm multi-agent pathfinding problem, welllower upper bounds O(n3 ) (where n number vertices graph) numbermoves required solution. recent years, number approaches appeared solvesubclasses multi-agent pathfinding problem (see Section 2.1). Roger Helmert (2012)suggested one reasons results Kornhauser widelyapplied is:. . . approach described one place, parts describedalgorithmically. Therefore, underlying algorithm must derived numberproofs paper.444fiC OOPERATIVE ULTI - AGENT PATHFINDINGPush Swap algorithm (Luna & Bekris, 2011b) complete algorithm instancesleast two unoccupied vertices, therefore general recent algorithms. However, found number problems Push Swap algorithm (De Wilde, Ter Mors, &Witteveen, 2013), presented Push Rotate algorithm overcome shortcomings.primary aim paper provide complete understandable specificationalgorithm multi-agent pathfinding problem least two unoccupied vertices.specifically, paper presents following contributions:1. specification complete algorithm multi-agent pathfinding, involves reconstruction, cases refinement theoretical results Kornhauser (1984).2. empirical evaluation algorithm, includes comparison Bibox algorithm (Surynek, 2009), algorithm assumes biconnected instances, amongbest performing recently developed approaches described Section 2, termsnumber moves CPU time required (judging empirical evaluations Surynek,2009; Wang & Botea, 2011).3. brief exploration possibilities improve solution quality reduce computationtimes use heuristics, namely area selecting agent plan next,path agent choose.Part first contribution appeared conference paper (De Wilde et al., 2013), namelyAlgorithms 1, 2, 3, 8 proofs. reconstruction Kornhausers theoreticalresults, well complete listing relevant algorithms proofs new paper,contributions two three (a different, restricted empirical evaluation conductedconference paper).paper organized follows. Section 2, define multi-agent pathfinding problem,discuss complexity number algorithmic approaches. Section 3, describePush Rotate algorithm, Section 4 prove completeness analyze worstcase behavior. Section 5, describe heuristics path agent selection, describingexperiments Section 6. experiments comprise comparison Bibox PushSwap algorithms different types randomly generated instances. Finally, finishconclusions future work Section 7.2. Background Problem Statementconsider simple1 connected2 graph G = (V, E), set agents A, |A| < |V |, assignmentfunction agents vertices : V , goal assignment agents vertices : V .functions total, injective, non-surjective; total functions location1. graph simple one edge two vertices; multiple edges two vertices wouldexpand solution space, since assume agent must always move empty vertex.2. graph connected, Multi-agent Pathfinding problem considered componentsseparately. exists agent destination location different component startlocation, instance solution.445fiD E W ILDE , ER ORS & W ITTEVEENagent must specified, injective one vertex hold single agent time,non-surjective require always vertices agents.move transfer agent ai current vertex v = (ai ), adjacent, unoccupied vertex w, 1 (w) = . define ULTI - AGENT PATHFINDING problem optimization problem: find sequence moves transforms initial assignment goalassignment, || |0 | sequence moves 0 transforms .decision variant Multi-agent Pathfinding problem (i.e., exist sequenceK moves transforms initial assignment goal assignment) shown NPcomplete (Goldreich, 1993)3 . NP-completeness problem holds case oneagent destination location, agents obstacles may movedway (Papadimitriou, Raghavan, Sudan, & Tamaki, 1994) (i.e., goal assignment partialfunction domain size 1). Push Rotate algorithm present Section 3find move sequences Multi-agent Pathfinding problem least two unoccupiedvertices, although guarantee optimal solutions, would require exponential runningtime, assuming P 6= NP.Goraly Hassin (2010) presented variant Multi-agent Pathfinding problemcolors, p agents color. goal configuration specifiesvertex color occupying agent must (or whether vertex remain empty);authors show feasibility decided linear time (i.e., decided whetherinstance solution), also case = p. Goraly Hassin build work Aulettaet al. (1999), presented algorithm deciding feasibility Multi-agent Pathfindingproblems trees, linear time.Calinescu et al. (2008) also distinguish different kinds chips, namely unlabeledlabeled (where every label unique), consider different type model movingchip along empty path considered single move. prove even unlabeled chips,problem APX-hard, still NP-hard instances infinite, rectangular grid. Finally, WuGrumbach (2009) considered directed graphs, interesting property wrongmove put problem unrecoverable configuration. Unrecoverable reconfigurationsalso heart PSPACE-completeness proof Sokoban puzzle game (Culberson, 1999;Hearn & Demaine, 2005); Wu Grumbach consider complexity optimizationproblem, however, instead prove feasibility decided O(n2 m) time, nnumber vertices, number arcs.Early research Multi-agent Pathfinding focused feasibility reaching one assignment another. Wilson (1974) studied simple biconnected4 graphs single empty vertex,shows assignment reached other, except graph bipartite, case two sets assignments reach assignmentsset. Wilsons theorem includes two exceptions generally solvable: polygon (or cycle)graphs, and, remarkably, single specific graph 0 (Figure 2).Kornhauser et al. (1984) extend Wilsons result general graphs number unoccupied vertices provide polynomial-time decision procedure O(n3 ) bound numbermoves required. result Kornhauser based insight graph may viewed3. Goldreich proves NP-completeness Shortest Move Sequence problem equivalent definitionULTI - AGENT PATHFINDING, exception Goldreich considers biconnected graphs.4. biconnected graph connected graph articulation vertices, i.e., vertices whose removal disconnectgraph; graphs also called nonseparable graphs.446fiC OOPERATIVE ULTI - AGENT PATHFINDING(a) polygon graph.(b) Wilsons 0 graph.Figure 2: Biconnected graphs generally solvable single empty vertex.tree biconnected components linked chains zero vertices degree 2(called isthmuses). turns two components linked isthmus containsvertices number unoccupied vertices minus two, impossible agentscomponents different sides isthmus swap positions.a5a8a2a1a3a6a4Able crossa7Unable crossFigure 3: Illustration isthmuses connecting nonseparable components.Figure 3, note possible agents A1 = {a1 , . . . , a6 } swap positions,also agents A2 = {a7 , a8 } exchange positions, agent A1 exchangeposition agent A2 . means say that, take agents a6 a7 example,possible reach configuration a6 occupies position a7 Figure 3,time a7 occupies current position a6 . isthmuses impossiblecross induce decomposition graph smaller subgraphs, solved turn.demonstrate decomposition Section 3.1.2.1 AlgorithmsStandley (2010) proposed algorithm guarantees optimal5 solutions multi-agent pathfinding agents move eight-connected grid. time step, agent either movesadjacent grid cell stands still, cost single agent number time stepstakes reach goal; cost entire solution sum agent plan costs. Movesallowed empty grid cells, also cycle agents agent moves cellnext agent cycle. Hence, Standleys algorithm also applied problem instancesnumber agents equal number vertices.5. section, refer approach optimal returns solution consisting minimal numbermoves.447fiD E W ILDE , ER ORS & W ITTEVEENStandleys approach seen improve standard A*-based approach staten-tuple grid locations, one n agents, moves agentsconsidered simultaneously one timestep, state potentially 9n legal operators (nineeight moves plus wait move). Standleys operator decomposition dividestime step one agent considered time, thereby reducing branching factor9n 9, although search depth increases factor n. Coupled perfect heuristicsperfect tie-breaking strategy, operator decoupling would reduce number required steps9n d, search depth d, 9n d. Hence, operator decoupling scheme requires goodheuristic save computation time compared standard approach.work, operator decomposition technique employed create anytime algorithm, based maximum group size agents optimal solutions found (Standley& Korf, 2011). anytime algorithm starts group size one, increases group sizeone step solution found, looking efficient solutions. stand-alone operatordecomposition algorithm struggled find solutions problems larger 32 32 gridten agents, anytime algorithm able produce good-quality solutionshundred agents.Another optimal approach due Sharon et al. (2011), called Increasing Cost Trees(ICT). ICT, high-level search tree node consists k-vectorindividual path costs, represents possible solutions cost agent equalsvalue cost vector. create child node, unit cost added cost oneagents. low-level search performed see new node bring agentsdestinations. ICT efficient scenarios low interaction agents,agents reach destinations small number extra moves; otherwise, Standleysoperator decomposition efficient.Another optimal approach Sharon et al. (2012) meta-agent constraint-based search.constraint-based search, high-level search performed constraint tree, nodes constraints individual agents, low-level search find individual agent paths respectconstraint high-level node. constraint-based approach performed poorly typesproblems, however, authors devised meta-agent strategy, groups agentsmany internal conflicts merged one meta agent, order reduce number conflictshigh-level constraint search.Given NP-completeness problem, many papers employ sequential approach,agents planned one other, obtain polynomial-time algorithmnecessarily optimal. ensure planning agent n disrupt work done putagents 1, . . . , n 1 position, existing algorithms either reserve time slots nodes, previousagents restored positions planning agent n.Reservation-based approaches typically complete, sense reservations madefirst n agents make impossible agent n + 1 find plan (even multi-agentplan n + 1 agents could found means), approachesparticularly suited instances ratio number agents numberempty vertices high. context-aware routing (Ter Mors, Witteveen, Zutt, & Kuipers, 2010),agent n finds optimal route plan (i.e., path plus times arrive nodepath) around reservations first n 1 agents, assumes agents entergraph start first reservation, leave graph destination. Alternatively,448fiC OOPERATIVE ULTI - AGENT PATHFINDINGassumed start destination locations parking places, hold infinitenumber agents (Zutt & Witteveen, 2004).Velagapudi, Sycara, Scerri (2010) use reservation-based system create distributedcooperative routing algorithm. problem definition general, simply assumeset robots looking trajectories within given time horizon, binary obstacle map O,function C OLLISION C HECK takes two robot trajectories returns true collide.distributed routing algorithm, agent found route, broadcasts routeagents. route planning, agent take account routes higher-priorityagents broadcast. addition, agent receives higher-priority route conflictsroute, re-plan.Silver (2005) presents windowed approach agents make reservationsrestricted time horizon. Silver claims following three advantages. First, agents continuecooperating reach destination vertices (instead staying put blockingdestination vertices). second advantage sensitivity agent ordering (or prioritization)reduced, different agent priorities assigned different time periods. Finally,need plan long-term contingencies may occur.Alt.path p0vPath pFigure 4: LIDABLE class instances, must exist, every path p every vertexv p, alternative path p0 connecting predecessor successor v.MAPP algorithm (Wang & Botea, 2008) brings agents destination one one alongpre-computed path, lower-priority agents may temporarily pushed aside. MAPP requiresnumber restrictions instances guarantees find solution; LIDABLEclass instances requires:1. node agents start-destination path (except start destination),must exist alternative path connecting predecessor successor nodes (see Figure 4),2. first node (after start node) first agents path must empty,3. target location may intersect paths alternative pathsagents.Khorshid et al. (2011) present Tree-based Agent Swapping Strategy (TASS): iterationalgorithm, agent selected sent goal, accomplished swappingagents. Swapping two agents accomplished moving agents junction nodedegree three ensuring least two neighbors junction empty.449fiD E W ILDE , ER ORS & W ITTEVEENswap performed, empty vertices agents movedreturned original locations, two swapping agents affected.solve instances general graphs, authors first employ graph-to-tree decompositionalgorithm. However, decomposition complete sense solvable instance maysolution transformation tree. authors also provide conditionsinstance guaranteed solvable, namely distance twojunctions may greater number empty vertices minus two, turnsidentical result obtained Kornhauser (1984) restricted trees.Suryneks Bibox algorithm (2009) requires (and complete for) biconnected graphsleast two unoccupied vertices. Bibox algorithm makes use fact biconnectedgraph viewed original cycle, extended number handles (see Figure 5; Kornhauser shows biconnected graph decomposed handles). Agents destination outermost handle brought destination first, handleneed taken account more, algorithm proceeds next handle. Surynekalso implemented approach Kornhauser (1984), reported Bibox produced lowerrunning times shorter paths.Figure 5: graph consisting initial cycle, white, two handles yellow blue.Finally, Push Swap algorithm (Luna & Bekris, 2011b) presented completegraph two unoccupied vertices. algorithm works iteratively selecting agentsunspecified priority order move respective destination locations. agent,algorithm move destination location along shortest path. agentsencountered along path, action taken depends priority agent.case agent lower priority, algorithm attempt move waypush operation. accomplished pushing blocking agent forward along shortestpath (not containing higher-priority agents) empty vertex. case blocking agenthigher priority, algorithm attempt exchange positions using swap operation450fiC OOPERATIVE ULTI - AGENT PATHFINDINGsimilar swap operation TASS (Khorshid et al., 2011). higher-priority agent mustreturned destination, Push Swap algorithm uses resolve operation.parallel version Push Swap developed Sajid, Luna, Bekris (2012),aim reducing length solutions produced Push Swap, movessequential. idea behind Parallel Push Swap first resolve dependenciesagents require swap operation, push steps performedparallel.Although presented complete, showed Push Swap algorithm complete,contains following shortcomings (De Wilde, 2012; De Wilde et al., 2013):1. algorithm identify polygon graphs, i.e., graphs consisting single cycle.solvable polygon instance, Push Swap fail find solution wrong agentpriority ordering chosen; agent tries move higher-priority agent wayswap, algorithm fail, since swap requires vertex degree 3,polygon graph vertices degree 2 (see Figure 6 illustration).2. enable swap operation, two neighboring vertices junction (in TASS terminology)must emptied using clear operation; specification clear Luna Bekrisidentifies two four cases must distinguished.3. moving vertex back destination swap, resolve may invokeswap operation again. Examples constructed recursive calls resulthigher-priority agent two steps removed destination, Push Swap mayfail instances.4. Push Swap algorithm take account result Kornhauser (1984)impossible agents swap separated isthmus longernumber empty vertices minus two. Push Swap algorithm may fail case agentai must move way another agent j , ai assigned higher priority,ai j cannot swap.a1a2Figure 6: Regardless agent moves first, second agent try reach goal alongshortest path, blocked other, higher-priority agent. swapattempted vertices degree 3 swap at.first shortcoming resolved personal communication author: insteadalways choosing shortest path, agent must choose shortest path destinationcontain finished agents. second shortcoming rectified updated clearoperation (Algorithm 12, Appendix A). fix third point, changed way agent451fiD E W ILDE , ER ORS & W ITTEVEENreturned position swap, introduce new operation rotate accomplish this.address fourth point, studied theory problem decomposition Kornhauser (1984),developed new algorithms decompose problem subgraphs (see Definition 4), assignagents subgraphs, determine agent priorities extent determinedrelation subgraphs. result algorithm multi-agent pathfindingcomplete general graphs least two empty vertices.3. Push RotatePush Rotate algorithm consists pre-processing phase, phase agentsmoved destinations. pre-processing phase, first divide graph subgraphs, assign agents subgraphs. definition subgraph (Definition 4Section 3.1) ensures agents assigned subgraph swap positions6 . thirdfinal step pre-processing determine order (priority) agents planned for.agents assigned subgraph, priority ordering feasible (we test orderingheuristic Section 6.3), agents assigned different subgraphs, may necessarycomplete one subgraph starting another, partial priority relation subgraphsdetermined.comes phase moving agents destination locations, algorithm worksfashion similar Push Swap (Luna & Bekris, 2011b). First, determine shortest pathagent destination, attempt move agent forward along path.agents encountered along way, action taken depends whetherblocking agent higher priority. lower priority (meaning plannedagent yet), try push agent way along shortest path emptyvertex. work, agent higher priority (it planned for,occupying destination location), attempt exchange position agents usingswap operation, involves moving agents vertex degree three higher, emptyingtwo neighbouring vertices, performing exchange operation (Figure 7), reversingappropriate moves ensure swapping agents different position namelyothers. swap operation, must return higher-priority agent destination.Push Swap, results recursive calls swap, ultimately undefined behavior (see DeWilde, 2012, analysis); Push Rotate algorithm, solve problem detectingwhether cycle agents want move forward. so, agents advancedone step rotate operation.swap operation fails, (like Luna & Bekris, 2011b) conclude instancesolution. noted, however, conclusion validly drawn particularpriority orderings. Figure 8, example, instance solvable agent a4 (or a5 , both)higher priority agent a3 . Otherwise, a3 moved destination vertex v first, pushoperation a4 a5 fail a3 higher priority; swap operation fail,swap impossible a3 a4 . see swap impossible, note requires6. normally use word swap indicate exchange position two agents occupying adjacent nodes.broader meaning word swap two agents changing position assignment. lattermeaning, Kornhauser interchangeably uses terms 2-cycle, transposition, swap (Kornhauser, 1984, p. 8).meanings word swap hold regard agents able swap iff assigned subgraph.Note write swap, refer operation defined Algorithm 5.452fiC OOPERATIVE ULTI - AGENT PATHFINDINGa1va1vva2a2a1a2Figure 7: Sequence states exchange operation.agents vertex degree three more, two neighbors unoccupied. twovertices degree three reached a1 a4 v v0 . a3 v,one empty neighbor left v; a3 moves back start location a4 movesv0 , one empty neighbor right v0 .a2a1a3a4v0va5Figure 8: instance solvable iff priority a4 , a5 , higher priority a3 .3.1 Problem DecompositionKornhauser (1984) proposed decomposition graph subgraphs, reconstructSection 3.1.1, along algorithm obtain decomposition. subgraphs definedmanner agents either confined one subgraph, confined isthmus.Section 3.1.2, pose proposition proof come later swap operation(Algorithm 5, Section 3.2) succeed two agents assignedsubgraph. results Section 3.1.2 refinement results Kornhauser,explicitly state details agent assignment problem. Finally, Section 3.1.3, considerinteractions agents assigned different subgraphs, sense subgraphsmust solved others, order avoid problems explained Figure 8. Kornhauserspecify priority agents, therefore restrictions regarding movingagents higher priority, prioritization subgraphs unique approach.3.1.1 C ONSTRUCTING UBGRAPHSKornhauser (1984) remarked connected graph viewed tree biconnected components. Within biconnected components, agents exchange position, biconnected com453fiD E W ILDE , ER ORS & W ITTEVEENponents play central role construction subgraphs. Kornhausers thesis, first stepcreating subgraphs divide graph biconnected components, defined Kornhauserfollows (Kornhauser, 1984, p. 30):Definition 1 (Biconnected Components). Let G = (V, E) simple connected graph. Two edgese1 , e2 E equivalent e1 = e2 cycle G containing e1 e2 ;equivalence relation. equivalence classes, together incident vertices, biconnectedcomponents G .a9a4a6a131a21a3624s1s29656s3a57s4a78a810s5a101(a) Ten biconnected components, vertices s1 , . . . , s5 degree 3 join biconnected components.a9a4a6a1a2S1S2a5a7a8S3a10a3(b) = 3, graph three subgraphs S1 , S2 , S3 .Figure 9: Identifying biconnected components (9(a)) subgraphs (9(b)).Biconnected components consisting one edge called trivial. graph Figure 9(a)contains ten biconnected components, two nontrivial (components numbered 1 6).Note vertices s1 , . . . , s5 Figure 9 vertices7 degree 3 join biconnected components,trivial nontrivial. basis vertices Kornhauser defines equivalence relationuse construct subgraphs.Definition 2 (Join vertices). Let G = (V, E) simple connected graph. set join verticesV consists vertices degree 3 common least two biconnected components.Note set join vertices cannot comprise set vertices V : first supposegraph consists exclusively trivial biconnected components. implies graph acyclic,exist least two vertices degree 1, therefore join vertices. casegraph also contains nontrivial biconnected component C1 , = V , vertices C1 mustconnected biconnected components C2 , . . . ,C j , vertices would also7. follow notation Kornhauser here, denote join vertices si rather denoting vertices vielsewhere.454fiC OOPERATIVE ULTI - AGENT PATHFINDINGconnected different biconnected components. course, possible keep creating newbiconnected components, reusing earlier one would create cycle, meaning verticesinvolved single biconnected component, connecting different ones.following, let number empty vertices.Definition 3 (Reachability Equivalence). Given simple connected graph G = (V, E) setjoin vertices S, s1 , s2 equivalent1. s1 = s2 ,2. s1 s2 nontrivial biconnected component,3. unique path s1 s2 , length 2.transitive closure relation equivalence relation.Figure 9(a), = 3 equivalence classes S1 = {s1 , s2 }, S2 = {s3 , s4 }, S3 ={s5 }. Note remove one agent Figure 9, 4, condition 3 Definition 3ensures s2 s3 equivalence class, well s4 s5 . Hence, 4 emptyvertices, graph Figure 9 contains single equivalence class S1 = {s1 , . . . , s5 }.Definition 4 (Subgraph). Given equivalence class Si , subgraph Si = (Vi , Ei ) subgraphG , induced set vertices Vi consisting of:1. equivalence class Si Definition 3,2. vertices v V v nontrivial biconnected component Si ,3. vertices unique path two join vertices s1 , s2 Si .Figure 9(b) shows three subgraphs example, corresponding equivalence classes S1 , S2 , S3 . algorithm obtain division subgraphs givenAlgorithm 1.Algorithm 1 find subgraphs(G , m)1: nontrivial biconnected components G2: {{v}| v V degree(v) 3 6 : v Vi }3: Si , j | u, v(minvSi ,uS j distance(v, u)) 24:Sk = Si j {v0 |v0 shortest path(u, v)}5:{S \ {Si , j }} {Sk }6: returnline 1 Algorithm 1, first find nontrivial biconnected components, doneO(|V | + |E|) time (Hopcroft & Tarjan, 1973). Next, add vertices degree three higherpart nontrivial biconnected component set . loop startingline 3, elements vertices distance 2 joined one subgraph.Note shortest path Si j line 3 always two join vertices: twonontrivial biconnected components connected isthmus, vertices connectingnontrivial component isthmus must common one biconnected component,455fiD E W ILDE , ER ORS & W ITTEVEENnamely nontrivial biconnected component, trivial biconnected componentisthmus edge. Hence, line 3 corresponds point 3 Definition 3 (either u v join vertices,two join vertices encountered path u v), line 4 corresponds point 3Definition 4.note Kornhauser employs slightly different definition subgraph. includesdefinition also vertices plank subgraph.Definition 5 (Plank). Given equivalence class Si corresponding subgraph Si = (Vi , Ei ),plank unique maximal path G vertex v (V \Vi ) vertex j Si length1.Figure 9, subgraph S2 two planks: plank left s2 s3 ,right plank s4 s5 . Subgraph S3 even three planks: shares plank s5s4 S2 , also two outgoing edges s5 planks.result Kornhausers definition, subgraphs overlap. opted include planksDefinition 4, prefer keep separate, useful illustrations Figure 9(b).3.1.2 SSIGNING AGENTS UBGRAPHSsection, state two important results relation agents subgraphs:1. Algorithm 2 assigns subgraph agents confined subgraphplanks, i.e., agents reach vertices subgraph planks.2. Proposition 3 state two adjacent agents assigned subgraph,exchange positions using swap operation. implies agent assignedsubgraph reach vertices subgraph planks.implication second result individual subgraphs solvable case goal positions agents assigned subgraph inside subgraph planks. importancefirst result determine exactly agents belong subgraph. Noteagents assigned subgraph, agents confined isthmus.Algorithm 2 assign agents subgraphs(G , A, , )1: ai2:f (ai )3: Si = (Vi , Ei )4:v Vi5:m00 number unoccupied vertices reachable Si graph induced V \ {v}6:u/ Vi {u, v} E07:number unoccupied vertices reachable v (V, E \ {u, v})8:((m0 1 m0 < m) m00 1) ai 1 (v) 6=9:f (ai ) Si10:Follow path u away v assign first m0 1 agents path Si11:{u/ Vi {u, v} E} = 0/ (ai 1 (v) 6=)12:f (ai ) Si13: return f , assignment agents456fiC OOPERATIVE ULTI - AGENT PATHFINDINGAlgorithm 2 iterates vertices subgraphs, decides whether agent occupyingvertex assigned subgraph. distinguish two types vertices subgraph:vertices connections vertices inside subgraph, verticesconnected vertices subgraph vertices (which join vertices, Definition 2)form start plank. Agents former type vertex immediately assignedsubgraph, whereas plank vertices checked whether sufficient empty verticesmove agents plank subgraph.treatment plank vertices, algorithm viewed refinement Kornhauser (1984), remarks agent assigned subgraph enough blankssubgraph one side P [the pebble] take P plank. . .. specify valuesm0 m00 (Figure 10) encode exactly sufficient free space available agentmoved subgraph. meaning m0 m00 condition line 8 Algorithm 2explained different cases proof Proposition 1.a5a3a5a4a3a4S1a1ua2S1a1v(a) m0 = = 2, meaning a2 assigned (yet) S1 , a1 assignedS1 .ua2v(b) m00 = 1, meaning a2 assigned S1 .Figure 10: Illustration m0 m00 . Unreachable removed parts graph grayedout.prove correctness Algorithm 2, let us first see agents assignedsubgraphs example. Figure 11(a) shows agents a1 , a2 , a3 , a4 assigned S1 .Agents a1 , . . . , a3 clearly inside subgraph, assigned basis line 12. Agenta4 plank, join vertex, assigned S1 line 10, since m0 = 3.subgraph S2 (Figure 11(b)), a6 inner vertex, assigned S2 line 12.Agent a5 occupies join vertex, three empty vertices reached without making usevertex (a5 ), m00 = 3, a5 assigned S2 line 9. Agents a7 a8 plankstarts join vertex (a5 ); m0 = 3, first two agents plank, starting vertex (a7 ),assigned S2 line 10.subgraph S3 (Figure 11(c)), a9 a10 (non-join) plank vertex,m00 = 3, assigned S3 line 10. Note v = (a8 ) (line 4) u = (a7 )(line 6), m0 = m00 = 0, a8 assigned S3 .Proposition 1. Algorithm 2 assigns agent ai subgraph j ai confined jplanks.457fiD E W ILDE , ER ORS & W ITTEVEENa9a4a6a1a2S1S2a5a7a8S3a10a3(a) Agents a1 , a2 , a3 , a4 assigned S1 (the leftmost subgraph).a9a4a6a1a2S1S2a5a7a8S3a10a3(b) Agents a5 , a6 , a7 , a8 assigned S2 (the middle subgraph).a9a4a6a1a2S1S2a5a7a3(c) Agents a9 a10 assigned S3 (the rightmost subgraph).Figure 11: Assignment agents subgraphs.458a8S3a10fiC OOPERATIVE ULTI - AGENT PATHFINDINGa3a3a4S1a1a2a4S1a5a1(a) agent a2 , m00 = 0.a2a5(b) agent a2 , m00 = 2.Figure 12: Figure 12(a), a2 assigned S1 ; Figure 12(b), a2 S1 , a3 not.Proof. central idea proof agent reach inside subgraph,sufficient empty vertices graph move beyond one planks, shownKornhauser (1984). consider following four cases:1. agent ai occupies vertex v inside subgraph j , v plank vertex,2. agent ai occupies plank vertex w 6 j (i.e., join vertex),3. agent ai occupies join vertex v start plank (i.e., v j ).4. agent ai occupies vertex v j planks.Case 1: inner vertex: first note ai assigned j (line 12 Algorithm 2). reachvertex assigned j one planks, agent ai must walk one subgraphsplanks. First, agent must take least one step join vertex start plank,leaving behind one empty vertex. Then, walking end plank, 1 long,requires 1 empty vertices. step plank, another empty vertex required,empty vertices total, ai reach end plank.Case 2: plank vertex w: recall m0 defined as: number unoccupied vertices reachablev (V, E\{u, v}), v start plank. m0 1 empty verticesused move m0 1 plank agents subgraph. Consider agent ai numberm0 1 plank: takes m0 1 empty vertices move start vertex plank,since m0 empty vertices available agents moved, one emptyvertex available ai step plank subgraph. Then, applyfirst case show ai cannot leave subgraph planks.Case 3: join vertex: agent ai cannot always enter subgraph, even agents behindplank can. Consider Figure 12(a), agent a2 start two planks, letu = (a1 ). m0 = 2, a2 cannot enter single subgraph S1 , moveway give agents inside subgraph space move. case, m0 = m,empty vertices behind a2 . Figure 12(b) shows case m0 = m, agentjoin vertex assigned subgraph case empty vertices reached S1without using join vertex. Hence, Figure 12(b) agent a2 assigned S1 since m00 1.459fiD E W ILDE , ER ORS & W ITTEVEENNote ai assigned j , also confined j planks: emptyvertices directly behind ai (m0 = m), move steps away j , plank1 edges long.case agent ai join vertex assigned subgraph, least oneempty vertex used make room ai step subgraph onto anotherplank, apply reasoning first case conclude ai confinedsubgraph planks.Case 4: vertex v outside j planks: clearly, ai confined j planks. seeai wont assigned j , note enter j via planks. planks1 edges long (all shorter planks connected another part graph), ai wouldneed least steps reach start plank j . Since empty verticesgraph, empty vertices left enter j , ai therefore assignedsubgraph.important property subgraphs agent reach vertex subgraphassigned to. Kornhauser (1984) proved 2-transitivity: two pairs agents a1 , a2 , b1 , b2assigned subgraph, possible send ai location bi (possibly movingagents). Section 3.2, prove swap operation always succeed two agentsassigned subgraph, achieves 2-transitivity.3.1.3 P RIORITIES UBGRAPHSthird stage decomposition process assign priorities agents based membership subgraphs. Algorithm 3 generates partial order subgraphs, agents inherit prioritysubgraph assigned to. completeness Push Rotate approach,necessary differentiate priorities agents assigned subgraph, althoughpossible order pursue better solution quality. Finally, agents assignedsubgraph receive lowest priority, therefore planned last.Algorithm 3 generates precedence constraints two subgraphs case one subgraphs contains agent restrict movements agents subgraph. Specifically, given two subgraphs Si j , agent r assigned j , priority relation Si jadded following two cases8 :1. goal position agent r start vertex plank9 Si ,2. goal position agent r vertex v0 plank Si , vertices plankv0 start plank goal positions agents assigned subgraph.intuition behind cases agent r moved goal position,empty vertices subgraph Si ; otherwise, first case, agent r would able8. note notation: coming algorithms, use r denote agents (as Luna & Bekris, 2011b).examples far used a1 , . . . , ak , agents ai j confusing combination subgraphsSi j .9. Recall start vertex plank vertex plank belongs subgraph.460fiC OOPERATIVE ULTI - AGENT PATHFINDINGa7S3a3a1a10a2S2a8S4S1a4a5a4a9a6S1a1(a) S4 S3 due agent a5 .S2a2a3a5(b) S2 S1 due agent a2 .Figure 13: agents one subgraph goal location plank another,might precedence relation subgraphs.move Si would part Si , Proposition 1, contradiction (in secondcase: agents assigned subgraph could move Si , would assigned Si ).agents subgraph Si move goal locations, empty vertices must broughtSi . case r moved goal, possible: push operation allowed,swap operation possible, Proposition 3. Hence, subgraph Si must higher priorityagent r, therefore subgraph Si j .example first case shown Figure 13(a), agents start locationequals destination location, except agents a8 a9 want exchange position. a5higher priority a8 a9 , algorithm based operations push swapsucceed: plan a8 , push may move a5 aside since a5 higher priority; swapfail a5 a8 different subgraphs, therefore cannot swap. Hence, must addpriority relation S4 S3 . example second case shown Figure 13(b),plank vertex S2 occupied a3 . example, solution using push swapfound case a2 higher priority a4 a5 . agents a4 a5 planned,allowed move agent a3 assigned subgraph away push,agent a2 behind may moved push operation. attempted swap a3one agents S2 fail a3 S2 . Hence, priority relation S2 S1 mustadded.Proposition 2. priority relation subgraphs cyclic, instance solvable.Proof. Suppose way contradiction two subgraphs Si j j Si ,due agent r Si , Si j , due agent j .definition subgraph, one connection two separate subgraphs,goal locations r isthmus. agent r induce j Si , eitherreach start vertex v (from perspective j ) isthmus, may agentsassigned j (so s) v. Hence, agent may occupy vertexisthmus r vertex v. way induce Si j would goal locationbehind (i.e., side Si ) goal location r.461fiD E W ILDE , ER ORS & W ITTEVEENHence, assumption priority relation cyclic implies goal locations rswapped, however Kornhauser proved agents assigned subgraphswap positions (Kornhauser, 1984, p. 11).example Proposition 2, consider agents a1 a2 Figure 13(a). destinationlocation agent a1 current location a2 , precedence constraint S3 S1 wouldadded. Similarly, destination a2 current location a1 , S1 S3 .configuration, agents a1 a2 swapped location, since assigned differentsubgraphs, possible, Proposition 3.Algorithm 3 subgraph priority(G , , , f )1: Si = (Vi , Ei )2:v Si3:u/ Vi {u, v} E4:Vertex u first vertex path Si another subgraph j , otherwisecontinue next u5:v0 v6:r : (T (r) = v0 ) ( f (r) 6= Si )7:f (r) = j8:Si j9:Continue next v (line 2)010:v next vertex path Si j11: return priority relationAlgorithm 3 checks every subgraph Si , every join vertex v Si connected nodeu 6 Vi path (i.e., isthmus) component j , whether exists agent r assignedj restricts movements agents assigned Si . order precedence constraintSi j added, agent r either occupy v, occupy node v0nodes10 (not including) v0 v (including) goal locations agents assignedsubgraph. either case, agent r, planned first, would bottle agents assigned Si , hencelatter agents receive higher priority: Si j .final note agents assigned subgraph. Although Algorithm 3 assignpriority agents, agents planned agents assigned subgraph.3.2 Operations push, swap, rotateSection 3.3 describe operation solve moves agents destinations,possible, section first present main operations used algorithm: push,swap, rotate. push swap operations conceptually similar operations presented Luna Bekris (2011b), rotate operation moves agents cycle one stepforwards. rotate operation require agents move simultaneously.note notation: algorithms assume parameters passed reference,algorithm subroutine called see changes made parametersubroutine. Below, stands set generated moves, U stands set blocked10. vertices v v0 empty , agents Si would still room maneuver,even r goal location.462fiC OOPERATIVE ULTI - AGENT PATHFINDINGvertices, i.e., vertices algorithm may use (any agents vertices thereforeremain place).Algorithm 4 push(, G , , r, v, U )1: vertex v occupied2:U 0 U {A (r)}3:clear vertex(, G , , v, U 0 ) = false4:return false5: move(, , agent r vertex v)6: return truepush operation attempts move agent r, currently location (r), location v,assumed adjacent (r). successful, move recorded current sequenceagent moves (line 5). case location v occupied push called, clear vertexoperation used try clear v. line 2, prior calling clear vertex, current locationagent r added set U locations agents may moved.specification clear vertex (Algorithm 10) moved Appendix (aswell auxiliary algorithms), disrupt flow text much.clear vertex find shortest path v unoccupied node u graph inducedV \ U ; path exists, agents path moved one place towards u.Algorithm 5 swap(, G , , r, s)1: {vertex x f (r) | degree(x) 3}2: vertex v3:A04:0 [ ]5:multipush(0 , G , 0 , r, s, v) = true6:clear(0 , G , 0 , r, s, v) = true7:+ 08:A09:exchange(, G , , r, s, v)10:reverse(, , 0r/s )11:return true12: return falseswap operation attempts exchange locations two adjacent agents r s, movingvertex degree three higher, performing exchange operation (see Figure 7Algorithm 13), moving agents back swap initiated (at timereversing moves agents involved line 10, 0r/s stands sequence newmoves roles r reversed). vertices degree three higher belongsubgraph agent r (denoted fS (r)) eligible perform swap (we evaluate verticesclosest r first). candidate swap node v, multipush operation (Algorithm 11,Appendix A) attempts bring agents v (line 5). Since moves swap reversed(with exception exchange operation, roles r reversed), multipushtake account set blocked agents U ; agents may moved well.463fiD E W ILDE , ER ORS & W ITTEVEENmultipush operation essentially series push operations, iteratively moving agents rstep closer v, moving agents way.v0a3a4v0a3a2a4a2va5a6vaxa1aya5a8a1a6a7axaxa8a7(a) Initial configuration(b) a1 movesv0a3v0a3a4a4va5a2a6va1axa5a8a1a6a7a8a7(d) agents rotate, starting a3(c) a2 moves forward, swaps a1v0a4a5v0a4a5a3a3va6va2a7a2axa6a2a7a1axaya1a8a8(e) a2 moves back cycle(f) Goal configurationFigure 14: steps rotate operation.multipush succeeds, next step Algorithm 5 try ensure v two emptyneighbors, calling clear operation. idea behind clear push agents verticesadjacent v way towards empty vertices, exact specification quite intricate, duepossible movements r considered order allow agentsreach unoccupied vertices. specification explanation clear operation givenAppendix A. state result swap operation succeed two agents assignedsubgraph (the proof Appendix B).Proposition 3. two agents r adjacent vertices G , operation swap(, G , , r, s)succeed r assigned subgraph Si .464fiC OOPERATIVE ULTI - AGENT PATHFINDINGrotate operation assumes cycle c locations, moves agents within cycleforward one step. case c fully occupied, performing rotate trivial operation; otherwise, one agent must first moved cycle provide room agents move.steps rotate operation illustrated Figure 14. Figure 14(a), see agenta1 temporarily pushed cycle make room others. matterwhether agents ax ay , others behind them, higher priority, since movesperformed move a1 cycle reversed.Figure 14(c), agent a2 moves forward v0 , swap operation agents a1 a2ensures a1 a2 right position rotate. Figure 14(d) depicts actualrotation agents, a3 moving forward first, followed a4 , etc. Figure 14(e) agenta2 moves back cycle complete rotate, Figure 14(f) goal configurationreached returning agents (ax ay figure) previous locations.Algorithm 6 rotate(, G , , c)1: vertices v c2:v unoccupied3:Move agents c forward, starting agent moving v4:return true5: vertices v c6:r (v)7:0 [ ]8:clear vertex(0 , G , , v, c \ {v}) = true9:+ 010:v0 vertex c v11:r0 (v0 )12:move(, , agent r0 vertex v)13:swap(, G , , r, r0 )14:Move agents c forward, starting agent moving v015:reverse(, , 0r/r0 )16:return true17: return falsespecification rotate operation given Algorithm 6. Lines 1 4 dealtrivial case cycle least one location v unoccupied: agent going v movedfirst, room agents behind move turn. main bodyalgorithm, line 5 iterates vertices cycle, looking vertex v cleared(Lemma 2 proves that, solvable instance, vertex always found), line 8.clear vertex succeeded, algorithm proceeds steps illustratedFigure 14; lines 10 12, next agent moved vacated vertex, line 13 agentswaps agent moved cycle (Lemma 2 proves swap possiblesolvable instance). Line 14 rotates agents, line 15 necessary moves reversed.3.3 Push Rotate AlgorithmAlgorithm 7 present Push Rotate algorithm. first determines divisionsubgraphs, determines assignment agents subgraphs initial goal465fiD E W ILDE , ER ORS & W ITTEVEENAlgorithm 7 Push Rotate(G , A, , )1: |V | |A|2: find subgraphs(G , m)3: f assign agents subgraphs(G , A, , )4: f 0 assign agents subgraphs(G , A, , )5: f = f 06:subgraph priority(G , , , f )7:return solve(G , A, , , , f , )8: return falseassignment agents. assignment functions f f 0 same, instancefeasible algorithm returns false. Otherwise, main algorithm solve calledreturns sequence moves transforming initial agent assignment goal assignment.idea behind solve operation move agents destinations one one,single agent moved destination one step time, along shortest path destination.every iteration, first push tried move agent next location path, pushfails, swap performed, succeed (as proved Theorem 1, providedleast two unoccupied vertices) solve called feasible instances.agent moved destination, added F , set finished agents.path traversed encoded q, shown Figure 15(b). Since push operationallowed move agents F , agents moved swap operation (line 22)rotate operation (line 19). agent F moved destination formeroperation. refer agent resolving agent, returned destination.resolving agents contained q, swap occurs along path agentdestination, added q. Note agent F moved rotateoperation, actually returned destination. second stage (lines 26 35) solveoperation aims return resolving agents destinations, shrinking q.follows detailed description Algorithm 8. First, initialization done:sequence moves initialized empty list [ ] (line 1), path q resolving agents(line 2). set finished agents F initially empty (line 3), r, pointer agentselected iteration, initially undefined, denoted (line 4). line 5, check whetherinput graph G polygon. so, polygon gets value true, otherwise false.outer loop line 6 iterates set finished agents equals set agents.next agent selected yet (r =, line 7), line 8 choose next agentjoint-highest priority agents \ F . Note that, r 6= line 8, meanagent selected line 30 part resolving agents q.Next, must choose path along r moved destination. graph polygon(is polygon = true, line 9), must choose path encounter finished agents(line 10) swap possible polygon instance otherwise simply choose shortestpath (line 12).inner loop line 14 move r closer destination, one step iteration.next step path r also q, detected cycle resolving agents,466fiC OOPERATIVE ULTI - AGENT PATHFINDINGAlgorithm 8 solve(G , A, , , , f , )1: [ ]2: q [ ]3: F 0/4: r5: polygon v V : degree(v) = 26: F 6=7:r =8:r next agent(A \ F , )9:polygon10:p shortest path(G , (r), (r), (F ))11:else/12:p shortest path(G , (r), (r), 0)13:q q + [A (r)]14:(r) 6= (r)15:v vertex (r) p16:v q17:c get cycle(v, q)18:q qc19:rotate(, G , , c)20:else21:push(, G , , r, v, (F )) = false22:swap(, G , , r, 1 (v))23:q q + [v]24:F F {r}25:r26:|q| > 027:v last vertex q28:1 (v)29:F v 6= (s)30:r 1 (T (s))31:r =32:move(, , agent vertex (s))33:else34:Break inner loop, continue outer loop35:q q [v]36: returnmove r agents involved cycle q11 forwards performing rotate operation,line 19. Otherwise, attempt move agent r forwards push (line 21), fails,swap operation (line 22). either push swap succeeds, append vertex v11. list q appended v consist cycle simple path emanating cycle, shape q,less, get cycle method line 17 returns cyclic part q.467fiD E W ILDE , ER ORS & W ITTEVEENagent r moved to, q (line 23). loop line 14 completed, agent radded set F finished agents (line 24), r reset (line 25).a2a3a1a4a5a7a6a8a4(a) shortest path a2 destination vertexdestination vertex a1 .a4a1a5a3a6a7a1a5a3a6a7a8a2(b) state directly a2 reached destination vertex, q highlighted.a8a2a4(c) a1 F encountered q, hence r a5next iteration algorithm.a1a6a3a7a8a5a2(d) q continue grow a5 movesdestination vertex.Figure 15: example q changes agents moved destination vertices.moving agent r destination, loop starting line 26 iterateslocations q see whether agents F need returned goal location,starting last location v q. v contains finished agent (line 29), agentmoved goal, check line 31 whether goal location occupied anotheragent r. agent r, simply move agent back goal location; otherwise,line 34, break loop line 26, thereby starting new iteration loopline 14 agent r.4. Analysis Push Rotatesection, first prove correctness completeness Push Rotate,analyzing computational complexity section 4.1. correctness proved Theorem 1,makes use Proposition 3, lemmas involving push rotate operations (theproofs Lemma 1 Proposition 3 quite long moved Appendix B).following lemma proves that, solvable instance, push operation fails,two agents involved must belong subgraph (and, Proposition 3, swapoperation succeed).Lemma 1. Suppose push (Algorithm 4) called context Algorithm 7 agent r movingvertex v. push succeed, r = 1 (v) assigned subgraph.next lemma shows rotate operation sound.Lemma 2. instance (G , A, , ) least two empty vertices, rotate operationmoves agents cycle forward one step.468fiC OOPERATIVE ULTI - AGENT PATHFINDINGProof. Consider Figure 14; see rotate operation always succeed given two emptyvertices, note following:least two empty vertices G , find path p v emptyvertex. Since operations required clear v reversed end rotateoperation, always possible push agents along p.Since agents cycle assigned subgraph, agents a1 a2 swap.Theorem 1. Push Rotate complete class Multi-agent Pathfinding problemsleast two empty vertices.Proof. proof focus correctness Algorithm 8, solve. approachAlgorithm 7 dividing graph subgraphs solving subgraphs sequentiallysound proven Kornhauser (1984). Here, prove Algorithm 8 returns solutionone exists, false otherwise. idea behind proof is:1. iteration solve agent selected F (line 8), agentadded F .2. case finished agent moved goal location, current locationadjacent goal location, current location path q.3. finite number iterations, vertices q processed, restoring outof-position agents F goal location.prove first point, consider agent r while-loop line 6. First shortest path pdetermined goal. graph polygon (is polygon = true, line 5), shortest pathfound graph G \ A[F ]. iteration while-loop line 14, agent r moved v,next vertex p:v q, cycle C q, q constructed vertices alreadyvisited (line 23). rotate operation move agents C one step forward (i.e.,direction C). Lemma 2 shows rotate operation possible. resultrotate agents F 1 (C) returned goal positions (since swapmoved agents one step backwards along q) agent r moves v. Also, qupdated cycle removed.push succeeds agents r s, agent pushed way, agent rmove v.Otherwise swap operation executed. push succeed r s, then,due Lemma 1, fS (r) = fS (s), turn implies swap succeed(Proposition 3), hence agents r swapped successfully.prove second point, note agent F moved goal locationresult swap operation, moves location adjacent goal location. see469fiD E W ILDE , ER ORS & W ITTEVEENcannot moved away12 goal location subsequent operations, noteagents q moved back destination agent r, agent swapped,reached destination. If, processing q, current location v encounteredagain, rotate operation performed, returning destination.prove third final point, consider agent moved goal position(s). Then, line 30, assign r agent occupying goal position s.agent, return goal position using single move. Otherwise, break loopiterates q thus number iterations finite start new iteration loopline 6, agent r (s). agent r added F , new processing loopq started, occurs |A| times.4.1 Runtime AnalysisLet k denote number agents instance n number vertices roadmap.order solve instance, agent needs sent goal position, along shortest pathlength n.step along shortest path, rotate, push swap operation performed.operations, runtime swap operation dominant13 , simplifies followingequation:tsolve = O(k n tswap )swap tries multipush clear v sufficient degree. analysis mayshow limited (O(1)) number vertices need checked (indeed, behaviorobserved experiments), following:tswap = O(n (tmultipush + tclear ))operations require O(n tclear vertex ) time. clear vertex operation findfree vertex simple breadth-first search, resulting O(|V | + |E|) = O(n2 ). leadsfollowing runtime complexity solve operation:tsolve = O(n5 k)4.2 Solution Qualityk agents move along (shortest) path (of length n) towards goal position.like runtime analysis, swap operation (Algorithm 5) dominant factor outputsolve operation (Algorithm 8), push moves k agents along path lengthk (as runtime analysis, worst-case performance rotate determined callswap). yields following expression number moves solve operationoutputs:lsolve = O(k n lswap )swap operation moves two agents vertex v using multipush (Algorithm 11),clears two neighbors v. many different attempts made clear (Algorithm 12)12. Note operation swap operation, agents may temporarily moved way,moves reversed end swap operation.13. Actually, rotate calls swap, worst-case running time also determined call swap.470fiC OOPERATIVE ULTI - AGENT PATHFINDINGoperation clear two neighbors v, total output moves constant timesnumber moves generated clear vertex (Algorithm 10) plus constant numbermoves: O(lclear vertex ), O(n), clear vertex pushes agents backwards along pathempty vertex. Since multipush operation executes clear vertex step alongway, output dominant factor.lsolve = O(k n lmultipush )lsolve = O(k n2 lclear vertex )lsolve = O(k n3 )Note swap operation makes little progress (it advances agent one step) achieveO(n3 ) bound shown possible Kornhauser (1984). Section 6, investigate extent higher number moves worst case manifests practice,comparing algorithm Bibox algorithm (Surynek, 2009), achieve O(n3 )bound number moves required (although Bibox works biconnected graphs).5. Heuristics Post-ProcessingPush Rotate guarantee optimal solution, try improve solution qualityheuristics, processing initial solution. Section 5.1, discuss postprocessing step detects unnecessary moves, first discuss points PushRotate algorithm heuristic might used affect solution quality (in orderappear text):1. push operation (Algorithm 4), clear vertex operation (Algorithm 10)called, must choose empty node move agents into.2. Similarly, swap operation (Algorithm 5) must choose vertex v degree 3perform swap.3. Algorithm 8 (solve), order agents planned decided line 8.4. Also Algorithm 8, p assigned shortest path destination, line 12;multiple shortest paths, one must chosen.far, considered heuristics third fourth points. regardfirst two points, consider empty vertices possible swap locations specific order,different plan found assuming multiple nodes viable. regards third point,agent ordering important solution quality shall show Section 6. Noteagent ordering partially determined Algorithm 3, introduces precedence constraintssubgraphs. Within subgraph, heuristic used order agents.agent-priority heuristic aims limit number swap operations required.heuristic first determines diameter graph, chooses two verticesdistance exactly diameter. first move agents away one vertices14 , ensuringempty spaces vertex vertices closest it. Agents ordered based14. moves produced pre-processing part solution.471fiD E W ILDE , ER ORS & W ITTEVEENdistance destination vertex, farthest agents receiving highest priority.idea that, subset agents moved destinations, empty spacesnever get stuck behind finished agents. Remember push operation allowedmove agents higher priority. long empty spaces reached, push operationperformed, much cheaper otherwise required swap operation.fourth point, implemented heuristic finds path minimum number agents F (if exist multiple paths, select shortest one). Again, aimavoid use costly swap operation.5.1 Removing Redundant MovesPush Rotate algorithm capable outputting redundant moves. example shownFigure 16: part swap agents a1 a2 , agent a3 moved goal location(in particular, clear operation move agent a3 goal location, see Figure 16(b)). Sincemoves generated clear operation executed reverse exchangeposition agents a1 a2 , agent a3 moved back initial position (Figure 16(d)).Finally, agent a3 planned moved goal location second time. Clearly,final two moves sequence redundant.a3a3a2a1a2a3a1(a) initial statea2(b) a3 makes room a2a1(c) a1 a2 swapa3a3a2a1a2(d) a3 put backa1(e) a3 moves destinationFigure 16: Example redundancy: swap a1 a2 , a3 moved back.sequence agent moves redundant agent visits vertex second time,agents visited vertex meantime. redundant moves removedsolution, moves may become redundant too. approach taken Luna Bekris (2011a)keep iterating moves solution, redundancies discovered.order remove redundancies efficiently, Algorithm 9 uses linked-list-like structure,encoded following functions. PA : (previous-agent) NA : (next-agent)functions back forward pointers respectively form doubly-linked chain movesspecific agent. PV : (previous-vertex) NV : (next-vertex) functionssimilar pointers form chain moves specific vertex.Algorithm 9 starts single pass (lines 2 4) moves find redundancies. redundant moves added Q, processed loop starting line 5. First, line 6,472fiC OOPERATIVE ULTI - AGENT PATHFINDINGAlgorithm 9 smooth()1: Q empty queue2: (a, v) =3:agent(PV ()) =4:Q.add(PV ())5: |Q| > 06:retrieve remove next element Q7:0 NA ()8:0 6= NV ()9:\010:PA (NA (0 )) PA (0 ), NA (PA (0 )) NA (0 )11:PV (NV (0 )) PV (0 ), NV (PV (0 )) NV (0 )12:agent(PV (0 )) = agent(NV (0 ))13:Q.add(PV (0 ))14:0 NA (0 )15: returnredundant move = (ai , v) retrieved Q. redundant, means agent ai plancontains number moves vertices returning vertex v without agentsvisited v between. means sequence [, NA (), NA (NA ()), , NV ()]moves except first removed; achieved loop line 8.Within loop line 8, first redundant move removed sequence moves(line 9), pointers PA NA , PV NV updated lines 10 11 respectively.Finally, checked whether moves become redundant result removal,line 12. check done constant time, require looping moves again,algorithm Luna Bekris (2011a).5.2 Executing Moves Parallelmakespan multi-agent plan difference time moment agentsstopped moving moment first agent started moving. Push Rotates initial outputlist agent moves, specification agent moves performedtime, makespan initially equal number moves. section, briefly discusscondense function15 , tries reduce makespan executing many agent movesparallel possible.distinguish three models regard allowed degree parallelism. leastrestrictive model allows agent move vertex another agent moving away.This, however, Multi-agent Pathfinding problem defined Section 2, allows agentmovement even empty vertices. restrictive model agentallowed move empty vertex, means time step, movesperformed parallel, number empty vertices. currently employrestrictive model.15. include algorithmic description paper condense. Although algorithm complicated, still somewhat lengthy write down.473fiD E W ILDE , ER ORS & W ITTEVEENintermediate model, also supported condense function, agents allowedmove vertices vacated, long head chain moving agents,empty vertex. Note degree parallelism conflict Multi-agent Pathfindingproblem: still possible serialize moves agent always moves emptyvertex.idea behind condense function follows: every time step starting first,inspects empty vertices, places moves going empty vertex timestep. case intermediate model employed, empty-vertex variables updatedvertices vacated, checked whether agents movingvertices. process continues moves emptying vertices found.6. Experimentssection compare performance Push Rotate MAPP, Push Swap,Bibox. previously compared MAPP Push Rotate game map16 BaldursGate II (De Wilde et al., 2013), repeat experiment (on different map).experiments game maps preliminary lend comparisonsalgorithms: MAPP produces many moves, Push Swap requires lot computationtime, Bibox cannot solve instances biconnected.Push Swap conceptually similar Push Rotate, would expect performancesimilar also. addition, apart game map, experiments conductedbiconnected instances (to allow comparison Bibox), fact Push Swaptake account subgraphs affect success ratio instances. However, turnedsource incompleteness Push Swap, namely fact recursive callsswap may fail solved introduction rotate operation resultedPush Swap solving fraction problem instances.main focus experiments therefore Bibox. Bibox complete, requiresbiconnected graph, performs well compared approaches (Surynek, 2009). Moreover,achieves bound O(n3 ) number moves, Kornhauser (1984) showedlowest worst-case bound achievable general graphs. Push Rotate, contrast, boundO(kn3 ) number moves, k number agents. compared Biboxtwo types instances: instances random generator part Bibox code,grid instances.experiments, run Push Rotate without agent-orderingheuristics Section 5. effectiveness heuristic shown fact almostalways better, terms number moves therefore also terms CPU time, useagent-ordering heuristic Push Rotate.6.1 Map AR0603SR Baldurs Gate IIproblem instances benchmark set characterized large set vertices, manyunoccupied. map chose experiments 13765 vertices,100 2000 agents (step size 100).16. set benchmark maps video game industry available http://movingai.com/benchmarks/.474fi05001000MAPPPush & RotateP&R noorderPAS200400CPU time (s)500000 1000000800600MAPPPush & RotateP&R noorderPAS0number moves20000001000 1200 1400C OOPERATIVE ULTI - AGENT PATHFINDING15002000500number agents100015002000number agents(a) number moves(b) CPU timesFigure 17: Comparison map AR0603SR Push Rotate, Push Swap (PAS), PushRotate without agent-ordering heuristic (P & R no-order) MAPP.Figure 17 shows number moves produced Push Rotate, MAPP, Push Swap(no Bibox, map biconnected), CPU times. Push Rotate PushSwap produce efficient plans (which fact always within percent lowerbound sum shortest paths), MAPP requires many moves find solution,therefore try include experiments. Note instances,matter whether agent-ordering heuristic employed Push Rotate.Figure 17(b) interesting note Push Swap quite bit slower typeinstance, instances remainder experiments section (gridsbiconnected instances) C++ implementation Push Swap (when find solution)often little bit faster Java implementation Push Rotate.6.2 Random Biconnected InstancesSuryneks code17 generates random instances iteratively adding handles initial cycle (seeSection 2.1) according three parameters: number handles, size initial cycle,maximum handle length (where length next handle uniformly chosen 0maximum handle length minus one, set 1 equals 0). addition, one choosenumber empty vertices, default value 2. experiments measure numbermoves, CPU time, makespan, number time steps required getagents destination (in one time step agent perform one move, though multiple agentsmove one time step).17. used code available http://ktiml.mff.cuni.cz/surynek/research/icra2009/.475fi2004006008000200BiboxPush & RotateP&R noorderPAS400CPU time (s)3e+062e+060e+000600BiboxPush & RotateP&R noorderPAS1e+06number moves4e+06800E W ILDE , ER ORS & W ITTEVEEN10000number vertices2004006008001000number vertices(a) number moves(b) CPU timeFigure 18: Comparison random instances parameters (handles, initial cycle, max handlelength): (20, 20, 20) (50, 50, 50), 2 empty vertices.Figure 18 compares Push Rotate, Push Swap Bibox instances generated according single variable x ranged 20 50, steps two, used threeparameters (number handles, initial cycle size, maximum handle length), numberempty vertices kept two. Clearly number moves required Push Rotate risesmuch quickly instance size, consequently CPU times increase well. resultsPush Rotate slightly better terms makespan, still much worse Bibox.also see figures agent-ordering heuristic useful, essentialperformance Push Rotate. Push Swap managed solve 1.17% instances,really usable hard instances; later shall see performs betterempty vertices.Bibox works exceptionally well large handle sizes, unsurprising since algorithmbased concept filling handles: Bibox inserts vertices destination handleright order, destination, vertices moved much. Push Rotate,hand, bring agents place using push swap; swap two agents middlehandle, many agents moved way order bring nodeswap. P & Rs rotate operation rarely used type instance, path resolvingagents rarely intersects itself.Figure 19 shows experiments random biconnected instances 40 handles, initial cyclesize 5, maximum handle length 10, increasing number empty vertices, ranging 240 (step size 2), 180 instances per step. hardest instances, empty vertices, PushRotate still produces costly plans, higher number empty vertices, PushRotate produces much better plans. instances easier Push Rotate, first476fiBiboxPush & RotateP&R noorderPAS12000number moves5000102030BiboxPush & RotateP&R noorder402000 4000 6000 80001500010000number moves20000C OOPERATIVE ULTI - AGENT PATHFINDING10number empty vertices203040number empty vertices(a) average number moves(b) average makespanFigure 19: Comparison random instances 40 handles, initial cycle 5, maximum handlesize 10.push operation succeed often, swap still necessary, easier clearnode two agents swap.Bibox, hand, requires exactly two empty vertices current implementation,fills remaining empty vertices dummy agents. Even though moves involvingdummy agents removed final solution, Bibox manage benefitempty vertices extent Push Rotate does. Bibox still produces solutions quickly,however, requiring tenths second instance; Push Rotate requires around 6seconds solve hardest instances, around 1 second easiest ones. Figure 19 suggestsPush Swap also performs well, although noted success ratioPush Swap still 53.9% experiments.terms makespan (Figure 19(b)), performance advantage Push Rotate Biboxeven greater18 . something observed experiments comparing Push RotateBibox. main reason seems condense powerful counterpartBibox algorithm. Table 1 shows parallelism, number moves dividedmakespan, Push Rotate, Bibox, Bibox plus smooth (removing redundant moves)condense functions (Bibox++). Without using post-processing algorithms, Biboxpoor parallelism, applying smooth condense parallel PushRotate emptier instances. However, Bibox still moves, makespan PushRotate Bibox++ equal average.18. implementation Push Swap kind condense feature, makespan wouldsimply number moves, include figure.477fiD E W ILDE , ER ORS & W ITTEVEENEmptyP&RBiboxBibox++21.421.031.2261.861.071.49102.101.101.69142.251.121.88182.351.152.05222.421.182.25262.481.202.44302.541.232.64342.571.262.83382.631.293.07422.671.333.29462.711.363.5502.741.393.69Table 1: Parallelism Push Rotate, Bibox, Bibox plus smooth condense, biconnected Figure 19.6.2.1 NITIAL C YCLEBibox algorithm solves handles instance inserting agents one endpointhandle, using lower part graph (i.e., initial cycle handles lowernumber) move agents empty vertices around. initial cycle, different procedurefollowed swap-like operation used exchange position agent vertexagent whose destination i.204060805e+054e+053e+05makespan1e+050e+0015000001000000makespan5000000BiboxPush & RotateP&R noorderBibox++2e+0510020number verticesBiboxPush & RotateP&R noorderBibox++406080100number vertices(a) initial cycle: {4, . . . , 100}, max handle size: 4(b) initial cycle: 4, max handle size: {4, . . . , 100}Figure 20: Makespan comparison instances initial cycle one handle, two emptyvertices.Figure 20 shows, however, initial-cycle procedure efficient processinghandles, indeed Push Rotate19 . Figure 20(a) shows comparison instanceslength initial cycle varied 4 100, single handle maximumsize 4. Figure 20(b) shows experiments similar types graphs, small initialcycle 4 vertices, maximum handle length varies 4 100. number emptyvertices figures two. first setting, Push Rotate performs considerably better19. Due scripting error, experiments handle size 80 run Bibox(++).478fiC OOPERATIVE ULTI - AGENT PATHFINDINGBibox, second setting relative performances similar experimentsrandom instances. experiments, removing redundant moves solution Biboxusing smooth algorithm removed small percentage moves, trying reducemakespan condense result significant reduction. However, even condensed Biboxoutput far worse, large initial cycles, plans produced Push Rotate.interesting note Bibox produces excess n3 moves instances largeinitial cycle produces around two million moves 100 vertices yet CPU times lesssecond instances. Push Rotate, contrast, CPU times grow numbermoves; around 5 seconds 50 vertices, 70 seconds instances 100 vertices.6.3 Grid Instances2000000BiboxPush & RotateP&R noorder1000000makespan500000150000005001000001500000BiboxPush & RotateP&R noorderPAS2500000500000number moves3500000problem instance, Bibox requires specification handle decomposition graph.grid instances generated, adopted decomposition suggested Surynek (2011):initial cycle consists four vertices top left corner grid, handles addedfirst right initial cycle, initial cycle, finally remaining verticesadded one one, top left bottom right.1500200025000number vertices5001000150020002500number vertices(a) number moves(b) makespanFigure 21: Comparison grid instances sizes 4 4 50 50, 2 empty vertices.Figure 21 shows grid instances two empty vertices, performance PushRotate relies heavily agent-ordering heuristic. heuristic enabled, PushRotate performs consistently better Bibox; without it, performance usually much worse.set experiments, Push Swap solved 31.9% instances, beyondgrid sizes 25 25.also conducted experiments increasing numbers empty vertices, 24 24 gridinstances. problems turn relatively easy Push Rotate, Bibox unableleverage increased freedom movement, produces quite costly plans. Figure 22(a) shows479fi10203040BiboxPush & RotateP&R noorder20000140000makespan100000BiboxPush & RotateP&R noorderPAS6000015000010000050000number moves200000E W ILDE , ER ORS & W ITTEVEEN5010number empty vertices20304050number empty vertices(a) number moves(b) makespanFigure 22: Increasing number empty vertices 24 24 grid instances.two empty vertices, Bibox produces around 10% moves; 40 empty vertices,requires around 35% moves. terms makespan, difference algorithmseven larger: two empty vertices, span Push Rotate solution around 75%Biboxs solution, 40 empty vertices less 25%. instances proved easyenough Push Swap, 98.1% instances solved, performance comparablePush Rotate without agent-ordering heuristic.Finally, Figure 23 present CPU times algorithms grid instances; Figure 23(a)experiments increasing grid sizes, Figure 23(b) experiments increasingnumbers empty vertices. First, interesting thing note Push Rotates good performance (heuristic enabled) grids also expressed terms lower CPU times, Biboxstimes grow steadily number moves. Second, algorithms sensitive, termsCPU times, number empty vertices, even though algorithms produce solutions containing fewer moves.6.4 Experiment Conclusionsexperiments section apparent performance algorithms dependsconsiderably type problem instance. Bibox well-suited type random instancesgenerated Bibox program, Push Rotate performs really well grid instances,long agent-ordering heuristic employed. Although havent able run Biboxgame maps movingai.com, suspect Push Rotate perform betterinstances: plenty open spaces (essentially grids), many free vertices, too,play strengths Push Rotate.regard algorithms comparison, Push Swap performed reasonably wellplan found, larger instances, especially empty vertices, solution480fiBiboxPush & RotateP&R noorderPAS2030400CPU time (s)40BiboxPush & RotateP&R noorderPASCPU time (s)600800C OOPERATIVE ULTI - AGENT PATHFINDING20010000500100015002000250010number vertices(a) grid sizes 4 4 50 50, 2 empty vertices20304050number empty vertices(b) 24 24 grids increasing number empty verticesFigure 23: CPU times grid experiments.rarely found, practically usable. Similarly, solutions produced MAPP containmany moves, also guaranteed return solution instances LIDABLEclass. Finally, Push Rotate without agent-ordering heuristic often performs well,never significantly better employing heuristic, often significantly worse,instances weve seen reason use it.regard CPU times, problem domain suits algorithm, CPU times lowgrow rate number moves; otherwise, CPU times grow steadilynumber moves. exception seems performance Bibox random biconnectedinstances small single handle large initial cycle: tenths second, Biboxmanages produce millions moves. noted, however, Biboxs post-processingalgorithms quite rudimentary spend lot CPU time improving solution.Push Rotate, hand, post-processing algorithms important solution quality,also make 1% 5% total computation time.7. Conclusions Future Workpresented complete polynomial-time algorithm multi-agent pathfinding problem. approach similar Luna Bekris (2011b); using simple primitive operations push, swap, and, algorithm, rotate, algorithm constructed easyunderstand performs competitively. Although Push Rotate achieve (bestpossible) worst-case bound O(n3 ) moves (n number vertices graph), producescompetitive solutions compared Bibox, achieve O(n3 ) bound (Surynek,2011), restricted biconnected graphs.481fiD E W ILDE , ER ORS & W ITTEVEENargue Push Rotate currently algorithm choice multi-agent pathfindingproblems. Bibox performs comparably well, applicable biconnected graphs,moreover algorithm yet mature, instance way handles twoempty vertices, post-processing algorithms comparatively basic. Finally,work Kornhauser (1984), demonstrated O(n3 ) bound number moves. Surynekreports implementation Kornhausers work performs considerably worse (in termsnumber moves) Bibox algorithm. Moreover, Kornhausers algorithm difficultunderstand therefore implement. accessible specification implementationconstruction (Roger & Helmert, 2012), contact authors comparework due course.useful next step multi-agent pathfinding would develop algorithm guaranteesO(n3 ) bound, performs (at least) competitively Bibox Push Rotate, easyunderstand implement. Arguably, Push Rotate ticks two boxes, wouldmake sense try adapt reach O(n3 ) bound. would probably entail replacingexpensive swap operation, require O(n2 ) moves one agent single step,make use rotations, already done Bibox algorithm rotate operation.Acknowledgementsresearch sponsored SUPPORT project Dutch Ministry Economic Affairs.Appendix A. Algorithmsappendix present algorithms used defined main text.A.1 Operation clear vertexAlgorithm 10 clear vertex(, G , , v, U )1: unoccupied vertex u V2:p shortest path G \U u v3:p 6=4:x05:vertices x path p (in order)6:x0 6=7:r 1 (x)8:move(, , agent r vertex x0 )9:x0 x10:return true11: return falseAlgorithm 10 called push, multipush, rotate operations, and, successful,clears vertex v. Algorithm 10 iterates empty vertices u G (line 1), startsiteration finding shortest path, avoiding blocked vertices U , u vertex v mustcleared (line 2). path found (p 6=, line 3), agents p moved one step482fiC OOPERATIVE ULTI - AGENT PATHFINDINGtowards u (the for-loop line 5). Note choosing right u line 1 impactrunning time clear vertex operation, well total number moves solutionproduced Push Rotate.A.2 Operation multipushAlgorithm 11 multipush(, G , , r0 , s0 , v)1: r agent r0 s0 closest v2: agent3: p shortest path G (r) v4: p =5:return false6: vertices x path p7:vr (r), vs (s)8:vertex x occupied9:U {vr , vs }10:clear vertex(, G , , x, U ) = false11:return false12:move(, , agent r vertex x)13:move(, , agent vertex vr )14: return trueOperation multipush (Algorithm 11) called swap operation, moves two agentsr0 s0 next other, node v (which node degree three more, r0s0 swapped). First, shortest path found line 3, algorithm proceedsmove r forwards along path one step time, calling clear vertex next vertexpath non-empty.A.3 Operation clearclear operation (Algorithm 12) called swap, attempts clear two neighborspotential swap node. algorithm works four stages: first stage (Figure 24(a)) simplyattempts push agents, occupying neighbors v, away v. two verticescleared stage, operation done. Otherwise, three stages need one unoccupiedvertex next v work. unoccupied vertex stage one, clear operation fails.second stage (Figure 24(b)) neighbor n required path already emptyneighbor go v. agent n first moved , clear vertexoperation subsequently succeeds (with n, v, v0 blocked vertices), clear succeeds.third stage (Figure 24(c)) checks whether agent occupying neighbor n v may ablevacate n moving vertex v0 currently holds agent s. tried movingagent r empty vertex , agent v.idea behind final stage (Figure 24(d)) may possible create additional spacebehind already empty vertex , similar second stage. stage four, however, insteadusing direct connection n , agent n tries move v, meaningr must move backwards make room.483fiD E W ILDE , ER ORS & W ITTEVEENAlgorithm 12 clear(, G , , r0 , s0 , v)1: r agent r0 s0 v ; agent r0 s0 v ; v0 (s)2: E {unoccupied n neighbours(v)}3: |E | 24:return true5: n neighbours(v)\(E {v0 })6:clear vertex(, G , , n, E {v, v0 }) = true7:|E | 18:return true9:E E {n}10: |E | = 011:return false12: vertex E13: n neighbours(v)\{v0 , }14:0 [ ] ; 015:clear vertex(0 , G , 0 , n, {v, v0 }) = true16:clear vertex(0 , G , 0 , , {v, v0 , n}) = true17:0 ; + 018:return true19:break20: n neighbours(v)\{v0 , }21:0 [ ] ; 022:move(0 , 0 , agent r vertex ) ; move(0 , 0 , agent vertex v)23:clear vertex(0 , G , 0 , n, {v, }) = true24:clear vertex(0 , G , 0 , v0 , {v, , n}) = true25:0 ; + 026:return true27:break28: clear vertex(, G , , v0 , {v}) = false29:return false30: move(, A, agent r vertex v0 )31: clear vertex(, G , , , {v, v0 , (s)}) = false32:return false33: n vertex neighbours(v)\{v0 , } , 1 (n)34: move(, , agent vertex v vertex )35: move(, , agent r vertex v) ; move(, , agent vertex v0 )36: return clear vertex(, G , , , {v, v0 , n})484fiC OOPERATIVE ULTI - AGENT PATHFINDINGnv0vrnnv0x(a) 1: push agentsvrv0nxvv0r(b) 2: move agent (c) 3: agent makes roomempty neighborvrnx(d) 4: agents r clear vFigure 24: four stages clear operation.Note stages clear omitted work Luna Bekris (2011b) stages24(b) 24(c), Luna Bekris effectively considered clear operation trees.A.4 Operation exchangeAlgorithm 13 exchange(, G , , r0 , s0 , v)1: r agent r0 s0 v2: agent r0 s0 v3: (v1 , v2 ) two unoccupied neighbors v4: vs (s)5: move(, , agent r vertex v1 )6: move(, , agent vertex v vertex v2 )7: move(, , agent r vertex v vertex vs )8: move(, , agent v)exchange algorithm illustrated Figure 7 Section 3.Appendix B. ProofsLemma 1. Suppose push (Algorithm 4) called context Algorithm 7 agent r movingvertex v. push succeed, r = 1 (v) assigned subgraph.Proof. Note that, push called context Algorithm 7, problem instance solvable,agents higher priority r already planned for.prove lemma, prove equivalent statement r assignedsubgraph, push succeed.Case 1: r assigned subgraph definition, r assignedsubgraph, and, Proposition 3, means r cannot swap agent. Note that,result assigned subgraph, agent r trapped isthmus (see Figure 25illustration). agents assigned subgraph higher priority r, thereforeF time call push made20 .20. Note Algorithm 8, operations called it, make single call push, setblocked vertices U equal set locations currently hold agent F .485fiD E W ILDE , ER ORS & W ITTEVEENSubproblem SirSubproblem jFigure 25: Agent r, trapped isthmus, move (s) using push.way contradiction, suppose instance solvable yet push succeed. First,suppose F ; r cannot goal location s, rs goal location must behind(s). However, possible r reach location, would assigned subgraphfS (s), Proposition 1, contradiction reached.Next, suppose 6 F . Since r priority greater equal s, concludealso assigned subgraph. instance solvable yet push(r, s) succeed,exists empty vertex beyond s, path G \ (F ) reaches it; i.e.,path empty vertex blocked agents F . However, would exist assignment 0would move way r, agents F returned goal location,swapped empty vertex within beyond subgraph. Hence, wouldreach locations would assigned subgraph (S j Figure 25), contradictionreached.Case 2: r assigned subgraph fS (r) Since fS (r) 6= fS (s), blocking agent reach(at most) plank vertex fS (r). Suppose start plank fS (r); then, dueAlgorithm 3, fS (r) fS (s). Hence, possible s, agents subgraphs behind it,F . Therefore, push succeed case instance solvable.case start plank fS (r), goal location r must plankfS (r). Due Kornhausers thesis (1984), know r reach vertex fS (r)planks (the 2-transitivity result). Since agents cannot swap (Proposition 3), agent rreach goal pushing agent away, push achieves without restrictions. Finally,note 6 F ; goal position, instance would solvable: r cannotswap, goal location r cannot beyond (s).Proposition 3. two agents r adjacent vertices G , operation swap(, G , , r, s)succeed r assigned subgraph Si .Proof. First show swap r succeeds agents assignedsubgraph: fS (r) = fS (s) 6= . successful swap, change assignmentpositions agents r s. consider assignment assignment 0 ,positions agents r swapped:(s) = r0(a) = (r) =(a) otherwise486fiC OOPERATIVE ULTI - AGENT PATHFINDINGHence, subgraph r prior swap must equal subgraph swap. Furthermore,know Proposition 1 agent confined single subgraph, fS (r) = fS (s).see swap succeeding implies fS (r) 6= , note swap occurredvertex v degree 3. Vertex v clearly part subgraph Si , r reached vtwo vs neighbors empty, r assigned Si :1. case r plank: m0 1m0 < m, choosing right u (line 6 Algorithm 2),see Figure 7.2. case either r inner vertex: agent assigned Si line 12,Algorithm 2.show 6= fS (r) = fS (s) implies swap succeed agents r s. oneagents r occupies vertex v degree(v) 3 two empty neighbor vertices,agent occupies neighbor vertex v, agents r exchange positionsmoving empty vertices one order, exiting order (see Algorithm 13).Hence, swap succeeds exists vertex v multipush clear alsosucceed.leaves us prove r assigned subgraph, v alwaysexists. assignment criteria agents subgraphs guarantee that:1. agent inside biconnected component,2. agent less steps away vertex within fS (r), degree 3.case agents inside biconnected component, multipush succeeds trivially. biconnected component two paths pair vertices, always possible bringr vertex v degree 3 biconnected component (note blockedvertices multipush considers (r) (s), since moves put agents placereversed later). Second, clear always succeed reason: always existsone v degree 3 (at least two) unoccupied vertices reached vleast two paths. Hence, least one path v empty verticesblocked r s, two neighbors v emptied (possibly r momentarilystepping aside illustrated Figures 24(c) 24(d)).case agents inside biconnected component, must show vertexdegree 3 reached, also clear succeed.case exactly one agent (say r) inside biconnected component, agent not,r already occupies (join) vertex degree 3. case neither agent inside biconnectedcomponent, agents either two vertices degree 3, occupyingvertices assigned subgraph. Consider two agents two vertices degree3. 2 vertices two vertices, two occupiedagents r s. Suppose l1 l2 steps required reach vertices, m1m2 free vertices respective sides agents. leads to:l1 + l2 4m1 + m2 =487(1)(2)fiD E W ILDE , ER ORS & W ITTEVEENAssume one vertices unreachable:l1 > 1 = 2(3)vertex reachable:l2 4 l1(4)l2 < 4 (m m2 )(5)l2 < m2 4(6)agents outside vertices assigned subgraph, assignment criteria agentssubgraphs state must possible agent assigned subgraph enter subgraphone additional empty vertex remains subgraph.m1 free verticesrvm2 free verticesFigure 26: Illustration reachable free vertices.Consider amount free vertices sides two agents (see Figure 26).1. 1 free vertices one side two agents > 1 side sufficientmake clearing two vertices easy.2. case free vertices one side two agents, either agentssubgraph, another vertex v00 degree 3 reachable 2 steps. Movingagents towards v00 leave 2 free vertices behind v00 1 free vertex behindtrailing agent. means first case applies vertex v00 .3. exactly one free vertex sides agents implies = 2. meanssubgraph either single vertex, biconnected component graph, sincetwo components joined together Algorithm 1. single vertex case, impossibleagents belong subgraph, moving agent single-vertexsubgraph leave sufficient free vertices reachable required Algorithm 2.agents belong biconnected component, stage three clear operation applies: possible push agents towards one free vertices,trailing agent ends vertex v. clears path additional vertex next vpush towards free vertex original position trailing agent.clears vertices next v.488fiC OOPERATIVE ULTI - AGENT PATHFINDINGFinally, see 6= fS (r), note swap requires r adjacent vertex vdegree 3. Clearly, possible r step vertex v oneunoccupied vertices, either Si , part planks Si . Hence, rconfined Si planks Proposition 1.ReferencesAuletta, V., Monti, A., Parente, M., & Persiano, P. (1999). linear-time algorithm feasibilitypebble motion trees. Algorithmica, 23(3), 223245.Beaulieu, M., & Gamache, M. (2006). enumeration algorithm solving fleet managementproblem underground mines. Computers & Operations Research, 33(6), 16061624.Calinescu, G., Dumitrescu, A., & Pach, J. (2008). Reconfigurations graphs grids. SIAMJournal Discrete Mathematics, 22(1), 124138.Culberson, J. (1999). Sokoban PSPACE-complete. Proceedings Informatics, Vol. 4, pp.6576. Citeseer.De Wilde, B. (2012). Cooperative multi-agent path planning. Masters thesis, Delft UniversityTechnology.De Wilde, B., Ter Mors, A. W., & Witteveen, C. (2013). Push Rotate: Cooperative multi-agentpath planning. Proceedings twelfth international conference autonomous agentsmultiagent systems, AAMAS, pp. 8794, Saint Paul, Minnesota, USA.Desaulniers, G., Langevin, A., Riopel, D., & Villeneuve, B. (2004). Dispatching conflict-freerouting automated guided vehicles: exact approach. International Journal FlexibleManufacturing Systems, 15(4), 309331.Erdmann, M., & Lozano-Perez, T. (1987). multiple moving objects. Algorithmica, 2(1), 477521.Gawrilow, E., Kohler, E., Mohring, R. H., & Stenzel, B. (2007). Mathematics - Key TechnologyFuture, chap. Dynamic Routing Automated Guided Vehicles Real-time, pp. 165177.Springer Berlin Heidelberg.Goldreich, O. (1993). Finding shortest move-sequence graph-generalized 15-puzzleNP-hard. Tech. rep. 792, Technion Israel Institute Technology. work papercompleted July 1984.Goraly, G., & Hassin, R. (2010). Multi-color pebble motion graphs. Algorithmica, 58(3), 610636.Hearn, R. A., & Demaine, E. D. (2005). PSPACE-completeness sliding-block puzzlesproblems nondeterministic constraint logic model computation. TheoreticalComputer Science, 343(12), 7296.Hopcroft, J. E., & Tarjan, R. E. (1973). Algorithm 447: efficient algorithms graph manipulation.Communications ACM, 16(6), 372378.Hopcroft, J. E., Schwartz, J. T., & Sharir, M. (1984). complexity motion planningmultiple independent objects; PSPACE-hardness warehousemans problem. International Journal Robotics Research, 3(4), 7688.489fiD E W ILDE , ER ORS & W ITTEVEENKavraki, L. E., Svestka, P., Latombe, J.-C., & Overmars, M. H. (1996). Probabilistic roadmapspath planning high-dimensional configuration spaces. IEEE Transactions RoboticsAutomation, 12(4), 566580.Khorshid, M. M., Holte, R. C., & Sturtevant, N. (2011). polynomial-time algorithm nonoptimal multi-agent pathfinding. Proceedings Fourth International SymposiumCombinatorial Search, SoCS, pp. 7683.Kornhauser, D., Miller, G., & Spirakis, P. (1984). Coordinating pebble motion graphs, diameter permutation groups, applications. Proceedings 25th Annual SymposiumFoundations Computer Science, FOCS, pp. 241250.Kornhauser, D. M. (1984). Coordinating pebble motion graphs, diameter permutationgroups, applications. Masters thesis, Massachusetts Institute Technology.LaValle, S. M., & Kuffner, J. J. (2001). Rapidly-exploring random trees: Progress prospects.Donald, B. R., Lynch, K. M., & Rus, D. (Eds.), Algorithmic Computational Robotics:New Directions, pp. 293308. K Peters, Wellesley, MA.Lee, J. H., Lee, B. H., & Choi, M. H. (1998). real-time traffic control scheme multiple AGVsystems collision-free minimum time motion: routing table approach. IEEE TransactionsMan Cybernetics, Part A, 28(3), 347358.Luna, R., & Bekris, K. E. (2011a). Efficient complete centralized multi-robot path planning.Proceedings IEEE/RSJ International Conference Intelligent Robots Systems,IROS, pp. 32683275, San Francisco, CA, USA. IEEE.Luna, R., & Bekris, K. E. (2011b). Push Swap: fast cooperative path-finding completenessguarantees. Proceedings Twenty-Second international joint conference ArtificialIntelligence Volume One, IJCAI, pp. 294300. AAAI Press.Narasimhan, R., Batta, R., & Karwan, H. (1999). Routing automated guided vehicles presenceinterruptions. International Journal Production Research, 37(3), 653681.Nieuwenhuisen, D., Kamphuis, A., & Overmars, M. (2007). High quality navigation computergames. Science Computer Programming, 67(1), 91 104. Special Issue AspectsGame Programming.Papadimitriou, C., Raghavan, P., Sudan, M., & Tamaki, H. (1994). Motion planning graph.Proceedings 35th Annual Symposium Foundations Computer Science, FOCS, pp.511 520.Roger, G., & Helmert, M. (2012). Non-optimal multi-agent pathfinding solved (since 1984).Proceedings Fifth Annual Symposium Combinatorial Search, SoCS, pp. 173174.AAAI Press.Roszkowska, E., & Reveliotis, S. A. (2008). liveness guidepath-based, zone-controlleddynamically routed, closed traffic systems. IEEE Transactions Automatic Control, 53(7),16891695.Ruml, W., Do, M. B., Zhou, R., & Fromherz, M. P. (2011). On-line planning scheduling:application controlling modular printers. Journal Artificial Intelligence Research, 40,415468.490fiC OOPERATIVE ULTI - AGENT PATHFINDINGSajid, Q., Luna, R., & Bekris, K. E. (2012). Multi-agent pathfinding simultaneous executionsingle-agent primitives. Proceedings Fifth Annual Symposium CombinatorialSearch, SoSC, pp. 8896, Niagara Falls, Canada. AAAI.Sharon, G., Stern, R., Felner, A., & Sturtevant, N. (2012). Meta-agent conflict-based searchoptimal multi-agent path finding. Proceedings Fifth Annual Symposium Combinatorial Search, SoSC, pp. 97104, Niagara Falls, Canada. AAAI.Sharon, G., Stern, R., Goldenberg, M., & Felner, A. (2011). increasing cost tree searchoptimal multi-agent pathfinding. Proceedings Twenty-Second International JointConference Artificial Intelligence, IJCAI, pp. 662667.Silver, D. (2005). Cooperative pathfinding. Proceedings 1st Conference ArtificialIntelligence Interactive Digital Entertainment, AIIDE, pp. 117122.Simeon, T., Leroy, S., & Laumond, J.-P. (2002). Path coordination multiple mobile robots:resolution-complete algorithm. IEEE Transactions Robots Automation, 18(1), 4249.Sislak, D., Pechoucek, M., Volf, P., Pavlcek, D., Samek, J., Mark, V., & Losiewicz, P. (2008).AGENTFLY: Towards Multi-Agent Technology Free Flight Air Traffic Control, chap. 7, pp.7397. Birkhauser Verlag.Standley, T. (2010). Finding optimal solutions cooperative pathfinding problems. ProceedingsTwenty-Fourth AAAI Conference Artificial Intelligence, AAAI, pp. 173178.Standley, T., & Korf, R. (2011). Complete algorithms cooperative pathfinding problems.Proceedings Twenty-Second international joint conference Artificial IntelligenceVolume One, IJCAI, pp. 668673. AAAI Press.Surynek, P. (2009). novel approach path planning multiple robots bi-connected graphs.Proceedings 2009 IEEE International Conference Robotics Automation, ICRA,pp. 36133619, Kobe, Japan.Surynek, P. (2011). Multi-Robot Systems, Trends Development, chap. Multi-Robot Path Planning, pp. 267290. InTech - Open Access Publisher, Vienna, Austria.Ter Mors, A. W., Witteveen, C., Zutt, J., & Kuipers, F. A. (2010). Context-aware route planning.Proceedings Eighth German Conference Multi-Agent System Technologies, MATES,Leipzig, Germany. Springer.Ter Mors, A. W., Zutt, J., & Witteveen, C. (2007). Context-aware logistic routing scheduling. Proceedings Seventeenth International Conference Automated PlanningScheduling, ICAPS, pp. 328335.Trug, S., Hoffmann, J., & Nebel, B. (2004). Applying automatic planning systems airport groundtraffic control feasibility study. 27th Annual German Conference AI, KI, pp. 183197.Velagapudi, P., Sycara, K., & Scerri, P. (2010). Decentralized prioritized planning large multirobot teams. Proceedings 2010 IEEE/RSJ International Conference IntelligentRobots Systems, IROS, pp. 46034609, Taipei, Taiwan.Vis, I. F. A. (2006). Survey research design control automated guided vehiclesystems. European Journal Operational Research, 170(3), 677709.491fiD E W ILDE , ER ORS & W ITTEVEENWang, K.-H. C., & Botea, A. (2008). Fast memory-efficient multi-agent pathfinding. Proceedings Eighteenth International Conference Automated Planning Scheduling,ICAPS, pp. 380387.Wang, K.-H. C., & Botea, A. (2011). MAPP: scalable multi-agent path planning algorithmtractability completeness guarantees. Journal Artificial Intelligence Research, 42, 5590.Wilson, R. M. (1974). Graph puzzles, homotopy, alternating group. Journal Combinatorial Theory, Series B, 16(1), 8696.Wu, Z., & Grumbach, S. (2009). Feasibility motion planning directed graphs. TheoryApplications Models Computation, Vol. 5532 Lecture Notes Computer Science, pp.430439. Springer.Zutt, J., & Witteveen, C. (2004). Multi-agent transport planning. Proceedings SixteenthBelgium-Netherlands Artificial Intelligence Conference, BNAIC, pp. 139146, Groningen.492fiJournal Artificial Intelligence Research 51 (2014) 293-332Submitted 01/14; published 10/14Distributed Heuristic Forward SearchMulti-agent PlanningRaz NissimRonen Brafmanraznis@cs.bgu.ac.ilbrafman@cs.bgu.ac.ilBen-Gurion University Negev,Beer Sheva, IsraelAbstractpaper deals problem classical planning multiple cooperative agentsprivate information local state capabilities wantreveal. Two main approaches recently proposed solve type problem one based reduction distributed constraint satisfaction,partial-order planning techniques. classical single-agent planning, constraint-basedpartial-order planning techniques currently dominated heuristic forward search.question arises whether possible formulate distributed heuristic forward searchalgorithm privacy-preserving classical multi-agent planning. work provides positive answer question form general approach distributed state-spacesearch agent performs part state expansion relevant it.resulting algorithms simple efficient outperforming previous algorithms ordersmagnitude offering similar flexibility forward-search algorithmssingle-agent planning. Furthermore, one particular variant general approach yieldsdistributed version a* algorithm first cost-optimal distributed algorithmprivacy-preserving planning.1. IntroductionInterest multi-agent systems constantly rising, examples virtual real systemsabound, virtual social communities providing many instances. ability plansystems ability systems autonomously planimportant challenge artificial intelligence, especially size systemsquite large. context, fundamental question perform planningdistributed multi-agent system efficiently, many cases, preservingprivacy.Distributed planning interesting number reasons. Scientifically intellectually, interesting seek distributed algorithms fundamental computational tasks,classical planning. Similarly, interesting (and likely useful longterm) seek distributed versions fundamental tools computer science, searchdefinitely tool. Moreover, pragmatic reasons seeking distributed algorithms. example, imagine setting different manufacturers serviceproviders publish capabilities collaborate provide newproducts services none provide alone. providers certainlyneed reveal sort public interface, describing contribute, wellrequire others. likely, want describe innerc2014AI Access Foundation. rights reserved.fiNissim & Brafmanworkings: internal state manipulate (e.g., current stocklevels, machinery, logistics capabilities, personnel, commitments, etc.). usually confidential proprietary information agent would want reveal, althoughclearly one must reason planing process.principle, problem addressed using central trusted party running suitable planning algorithm. However, trusted party may existsettings. Moreover, centralized planning puts entire computational burden singleagent, rather distributing across system. Thus, centralized algorithms lessrobust, prone agent failures, sometimes less efficient. reasons,distributed algorithms often sought, case particular, distributed, privacypreserving algorithms. Indeed, main motivation field distributed algorithms, particular, work distributed constraint satisfaction problems (CSP)(Conry, Kuwabara, Lesser, & Meyer, 1991; Yokoo, Durfee, Ishida, & Kuwabara, 1998;Meisels, 2007).Moreover, often case good distributed algorithms formulated cooperative team provide foundation algorithms mechanisms solving similarproblems teams self-interested agents. example, work planning games(Brafman, Domshlak, Engel, & Tennenholtz, 2009, 2010) suggests modified versionsearlier algorithm cooperative multi-agent systems (Brafman & Domshlak, 2008),work mechanism design solving distributed CSPs self-interested agents (Petcu,Faltings, & Parkes, 2008) based earlier work distributed CSPs cooperativeteams (Petcu & Faltings, 2005). fact, work presented paper forms basiswork mechanism design privacy-preserving planning, agentsself-interested (Nissim & Brafman, 2013).Yet another motivation developing distributed algorithms provided planningdomains search operators correspond actions implemented using complex simulation software accessible relevant agent, agentinterested sharing (due privacy concerns commercial interests)realistic transfer, integrate, appropriately execute software partplanning algorithm. example, consider planning group robotic agents,different capabilities. agent simulator compute effectactions, agents want share other. Thus, applicationagents actions search done agents themselves.long tradition work multi-agent planning cooperative noncooperative agent teams involving centralized distributed algorithms, often using involved models model uncertainty, resources, (Conry et al., 1991; Ephrati& Rosenschein, 1997; Hansen & Zilberstein, 2001; Bernstein, Givan, Immerman, & Zilberstein, 2002; Szer, Charpillet, & Zilberstein, 2005; Witwicki, Oliehoek, & Kaelbling, 2012),much work coordinate local plans agents allow agents planlocally certain constraints (Cox & Durfee, 2005; Steenhuisen, Witteveen, ter Mors, &Valk, 2006; ter Mors, Valk, & Witteveen, 2004; ter Mors & Witteveen, 2005). workinfluenced motivated results area, takes starting pointbasic model introduced Brafman Domshlak (BD) offers possiblysimplest model planning ma-strips (Brafman & Domshlak, 2008). ma-stripsminimally extends standard strips (or PDDL) models specifying set agent ids,294fiDistributed Heuristic Forward Searchassociating action domain one agents. Thus, essentially, partitionsset actions among set agents.One motivation exploring planning simple setting belief simplemodels make easier study fundamental ideas, techniques developed oftengeneralised complex settings. Another closely related reason workingmodel recent influential trend field planning, solving richer, complexmodels reduction simpler settings and, especially, using classical planners.expected, given classical planners like SAT solvers, widely used blackboxes solving various problems (Kautz & Selman, 1992; Clarke, Biere, Raimi, & Zhu,2001) reached certain performance level, makes capable (quickly)solving large problems. planners incorporate wealth ideas techniquesusually difficult (and sometimes impossible) export non-classical models. useclassical planners shown efficient many different settings planninguncertainty Stochastic Shortest Path (Yoon, Fern, & Givan, 2007), ConformantPlanning (Palacios & Geffner, 2009), Conformant Probabilistic Planning (Albore, Palacios,& Geffner, 2010; Taig & Brafman, 2013) Contingent Planning (Albore, Palacios, &Geffner, 2009) well planning models Net-Benefit Planning (Keyder& Geffner, 2009) Generalized Planning (Srivastava, Immerman, Zilberstein, & Zhang,2011).Recently, number ma-strips planning algorithms respect agent privacyutilize inherent distributed structure system emerged. firstapproach based distributed CSP techniques introduced BDs original work.1BD formulate CSP particularly suited ma-strips problems whose solutionplan. algorithm transformed fully distributed algorithm simplyusing distributed CSP solver. BDs work establishes upper bound complexitysolving ma-strips problem depends exponentially two parametersquantifying level agents coupling system. Unfortunately, distributed CSPsolvers cannot handle even smallest instances planning problems. Consequently,dedicated algorithm, based ideas BD, developed (Nissim, Brafman, &Domshlak, 2010). performing well domains, algorithm troublescaling problems agent execute small numberactions. (Indeed, BDs algorithm scales exponentially min-max number actionsper agent solution plan.) Recently, new, improved algorithm, based partialorder planning, map-pop, developed (Torreno, Onaindia, & Sapena, 2012). Yet,algorithm, too, leaves serious gap solve using distributed plannersolved using centralized planner. Moreover, neither algorithm attemptsgenerate cost-optimal plan.classical single-agent planning, constraint-based partial-order planning techniquescurrently dominated heuristic forward search techniques2 . Thus, naturalask whether possible formulate distributed heuristic forward search algorithmprivacy-preserving classical planning. paper provides positive answer question1. idea using distributed CSP techniques planning first introduced Conry et al. (1991).2. Winners sequential satisficing (non-optimal) tracks last three International Planning Competitions heuristic forward search planners. planner also winner optimal trackprevious IPC (2011).295fiNissim & Brafmanform general approach distributed state-space search agentperforms part state expansion relevant it. resulting algorithmssimple efficient outperforming previous algorithms orders magnitudeoffer similar flexibility forward-search based algorithms single-agentplanning. respect natural distributed structure system, thus allow usformulate privacy-preserving versions.One particular variant general approach yields distributed version a*algorithm, called mad-a*, first distributed algorithm privacy-preservingcost-optimal planning ma-strips. mad-a* solves difficult problem centralized search since privacy-preserving setting, agent abstracted (partial)view problem. Yet, still able solve problems efficiently, outperforming centralized a* cases. show, main reason interesting optimalitypreserving pruning technique naturally built search approach. insightled new pruning technique centralized search shall describe later on.rest paper organized follows. start providing backgrounddescribe model use. followed discussion related work.Section 4 describes forward search algorithm, Section 5 describes mad-a*,modified version maintains cost-optimality. Then, present planningframework used, ma-fd, empirical results evaluating effectiveness algorithms.Section 8 discusses pruning method built-in algorithms,applied centralized search. Section 9 provides discussion privacy propertiesalgorithms open research challenges, followed conclusion.2. Backgrounddescribe ma-strips model multi-agent planning, proceed defineprivacy-preserving planning ma-strips.2.1 MA-STRIPSma-strips problem (Brafman & Domshlak, 2008) set agents = {i }ki=1 ,given 4-tuple = hP, {Ai }ki=1 , I, Gi, P finite set propositions, PG P encode initial state goal, respectively, 1 k, Aiset actions agent capable performing. action = hpre(a), eff(a), cost(a)igiven preconditions, effects cost. plan solution iff solutionunderlying strips problem obtained ignoring identities agent associatedaction. Since action associated agent, plan tells agentit. different planning contexts, one might seek special typessolutions. example, context planning games (Brafman et al., 2009), stablesolutions (equilibria) sought. focus cooperative multi-agent systems, seekingeither standard solution cost-optimal one, minimizes sum action costsplan.partitioning actions agents yields natural distinction privatepublic propositions actions. private proposition agent required affectedactions . action private preconditions effects private.actions classified public. is, private actions affect affected296fiDistributed Heuristic Forward Searchactions, public actions may require affect actions agents.ease presentation algorithms proofs, assume actionsachieve goal condition considered public. methods easily modifiedremove assumption.get clearer picture ma-strips model, consider well known Logisticsclassical planning domain, packages moved initialtarget locations using given fleet vehicles trucks, airplanes, etc. packagesloaded onto unloaded vehicles, vehicle move certainsubset locations. Propositions associated package location, mapvehicle, every vehicles location map. Possible actions drive/fly,load, unload, suitable parameters (e.g., drive(truck, origin, destination)load(package, truck, location)). natural partitioning problem associatesvehicle agent, assigning agent drive, load unload actionsinvolved. ma-strips includes action assignment part problem description.Logistics domain, since vehicle locations private propositions (affectaffected respective agents move actions), move actions certainly private,depending affecting private location propositions. Package location propositionseither private public, depending whether required/affectedone agent. example, proposition at(package,location) private locationaccessible one agent (e.g. inside vehicle, or, reached singlevehicle), public otherwise. Therefore, load (respectively unload ) actions require(respectively affect) public package location propositions considered public.note notion private/public natural ma-strips encoding,easily applied models multi-valued variables. example, SAS+,variable may multiple values, analogue (boolean) propositionma-strips hvariable, valuei pair. pair considered private required,achieved destroyed actions single agent. Consequently, actionsrequire, achieve destroy private hvariable, valuei pairs considered private.clarity consistency previous work use ma-strips notation discussingtheoretical aspects work. However, examples given, well practicalframework present planning, use concise multi-valued variables SAS+encoding.2.2 Privacy Preserving PlanningGiven notion private/public propositions actions, formalizeproblem focus work Privacy-preserving MA-STRIPS planning. Privacypreserving ma-strips planning seeks find solution ma-strips planning problemwithout exposing information private agent. Specifically: set privatevariables possible values, set private actions cost, privatepreconditions effects public actions. Public actions variables consideredexposed interfaces agent, private actions variables correspondagents local state actions manipulate local state.formally, privacy-preserving ma-strips planning problem, agentaccess description actions public projection public actions297fiNissim & Brafmanagents. public projection action consists lists public preconditionspublic effects. weak privacy preserving planning algorithm agent needcommunicate description private variables, private actions cost,private preconditions effects public actions, another agent. strong privacypreserving algorithm, agent deduce isomorphic model (i.e., model,renaming) private variable, private action cost, private preconditionseffects public action cannot deduced initial information availableagent public parts plan. present distributed search approachlies somewhere in-between weak strong privacy preserving. discussprivacy properties Section 9.1. stage, let us illustrate meaning needprivacy properties.Consider classical Logistics domain. Agent knows existence value(private) location propositions, locations public packages packagesprivate (only load unload them). Thus, initial state knowncontain initial locations vehicles, package locations reachable singleagent different . agent full knowledge actions, accessprojections agents public actions only. example, another agents action=unload(package, truck, location) preconditions at(package, in-truck) at(truck,location) effects at(package, in-truck) at(package, location). However, sinceat(package, location) public proposition (assuming location reachableone agent), projection known contains preconditionssingle effect. implication preserving strong privacy example agentsknow packages handled single agent (private it).know locations served agent, actual location different stages plan,changes location. fact, unaware existence locationagent, private variable.real life, may seem absurd, know trucks planeslocation move among locations. Privacy guarantees take accountgeneral knowledge things work. However, even example,appealing agents know set locations agent serves, movesthem, cost. know packages handledagent. believe ability provide privacy similar levelclear advantage planning algorithm, facilitating ability construct ad-hoc teamscooperating, privacy seeking agents.Many service-oriented domains similar abstract structure, one agentwork towards goal (getting package destination) requires collaboration multiple agents (e.g. truck one country, plane, truck another country).example, imagine multiple part-manufacturers, together aggregator,build complex object together none build own, e.g., laptop.would require monitors, various cards (graphic card, mother board, wifi), casing, etc.components, turn, requires diverse electronic parts, wiring, empty boards,etc. mother board manufacturers interface external world consists action selling mother board, actions purchasing relevant componentspublic actions. public action sell-mother-board may preconditionsstock > 0, have-payment, have-address effect stock=stock-1, has-mother298fiDistributed Heuristic Forward Searchboard(customer), stock variables private variables public. wouldalso private action produce-mother-board various technicalpreconditions related availability parts, workers, machinery. Alternatively, perhapsproducing board requires multiple steps, possibly using different machines differentworkers. reflected set private actions. public variables refermother board orders, availability, payments received. internal variable referproduces, workers roles take, storesinventory, stock levels facility. private actions describeshipping warehouse plant done, actions used actually construct board,what, much more. privacy preserving algorithm neither expose,require knowledge inner workings, typically considered proprietaryinformation manufacturers would share, even collaborators, unless required.Section 1 discussed example application planning team roboticagents, complex simulators generate outcome actions. Fittingscenario ma-strips model, public variables agent could describeelements environment affected agent, visible aspects robot (e.g.,is, whether standing ground), whereas local variables wouldrefer local state: actual state various motors, charge level, perhaps variouslocal variables describing current mode operation. Public actions would actionsinfluence environment observable state robot: moving, lifting, climbingladder, collecting rock. Local actions would involve local computations manipulateinternal, private state, charging. Note public actions influence manyprivate variables: moving arm climbing ladder would change internal statemany variables internal mechanical parts. Privacy preservation serves twopurposes: requiring one robot understand model intricacies another robot,requiring one robot disclose information robots, simplifyinginteraction robots, maintaining confidentiality of, possibly proprietaryinformation. Thus, one robot public (and thus) abstract description moveaction anther robot, whose public effect position robot changes. Yet,need model, know about, inner working involved move (or lift, carry,etc.) action. likely, state internal parts/motors/joints change variousways accomplish move action. Indeed, ad-hoc robot teams ubiquitousad-hoc networks, ability work together without burden exposure entailedsharing full models could crucial.3. Related Workwork focuses classical (deterministic) planning setting, agentscollaborate specific task, prefer reveal private information localstates, private actions, cost private actions. preservation agentprivacy main difference methods (and distributed planning general)approaches factored planning parallel planning. discussrelated work multi-agent systems related work privacy.299fiNissim & Brafman3.1 Multi-agent PlanningFactored planning methods (Amir & Engelhardt, 2003; Brafman & Domshlak, 2006; Fabre& Jezequel, 2009; Fabre, Jezequel, Haslum, & Thiebaux, 2010) seek utilize structureplanning problem, making centralized search efficient. methodsexploit structure ma-strips problems (as done Brafman Domshlak,2008), applicable privacy-preserving settings, respectdistributed form problem, giving centralized entity access entire problemdescription.Parallel planning methods (Vrakas, Refanidis, & Vlahavas, 2001; Kishimoto, Fukunaga,& Botea, 2009; Burns, Lemons, Ruml, & Zhou, 2010) aim speed-up solutioncentralized planning problems given access distributed computing environment,large cluster. Parallel planning performed ma-strips applying existingapproaches underlying strips problem (i.e., ignoring agent identities).methods distribute computation required solving planning problem, ignoreprivacy concerns, giving agents (essentially processors) access entire planningproblem.Given privacy-preserving ma-strips model, natural ask searchsolution. well-known example privacy-preserving search distributed CSPs(Conry et al., 1991; Yokoo et al., 1998), various search techniques heuristicsdeveloped (Meisels, 2007). Distributed CSPs, agents cooperate findfeasible assignment constrained variables keeping private internal constraints,model problem similar distributed ma-strips. fact, planning problemscast CSP problems (given bound number actions), first attemptsolve privacy-preserving ma-strips problems based reduction distributedCSPs. specifically, Brafman Domshlak introduced Planning CSP+Planningmethodology planning system cooperative agents private information.approach separates public aspect problem, involves finding public actionsequences satisfy certain distributed CSP, private aspect, ensuresagent actually execute public actions sequence. Solutions foundlocally optimal, sense minimize , maximal number public actionsperformed agent. methodology later extended first fully distributedalgorithm ma-strips planning, Planning-First (Nissim et al., 2010). Planning Firstshown efficient solving problems agents loosely coupled,low. However, scale rises, mostly due large searchspace distributed CSP. Recently, distributed planner based partial order planningintroduced (Torreno et al., 2012), outperforms Planning First, effectively solvingtightly coupled problems. methods privacy preserving, guaranteecost-optimal solutions.work attempts solve privacy-preserving ma-strips using distributed heuristicforward search algorithms. Heuristic forward search methods widely used,applied similar settings. Ephrati Rosenschein (1994, 1997) showed givenpriori breakdown global goal agent assigned subgoals, agents plantowards subgoals, merge possibly conflicting plans. approachessentially searches plan-space, using heuristic estimates states (which results300fiDistributed Heuristic Forward Searchmerged feasible subplans) order guide search. approach guaranteeoptimal solutions, relies solving exponential number plan merging problemsscale even loosely-coupled problems (Cox & Durfee, 2009). madeclear next section, approach simple, general framework forward searchstate-space, easily specializes distributed version cost-optimal searchalgorithm a*, depend complex plan merging algorithms.setting planning uncertainty distributed agents, forward searchshown effective. Szer, Charpillet Zilberstein (2005) used heuristic bestfirst approach order optimally solve Decentralized POMDPs. approach performscentralized forward search space agents policy vectors. order computeheuristic estimate single node, solution underlying centralized MDP mustcomputed possible states system. search algorithm (whichcentralized) heuristic computation require full knowledge systemtherefore applicable privacy-preserving setting.Another notable strategy solving decentralized POMDPs proposed Nair etal. (2003). JESP (Joint Equilibrium Based Search Policies) computes locally optimalsolution iteratively modifying policy agent improve joint (global)policy. algorithm proved converge Nash equilibrium. Subsequent work(Ranjit, Varakantham, Tambe, & Yokoo, 2005) aims exploit local interactionsagents, applying distributed CSP techniques. Locality exploited computingagents best response respect neighbors, agents affectedit, improves JESPs efficiency. Similarly Planning CSP+Planning,methods generate locally optimal solutions guarantee globally optimal solutions.Additionally, CSP-based methods suffer exactly scalability drawbackPlanning-First, efficient agent interaction rises.Recently, parallel work, heuristic forward search planninguncertainty combined influence-based abstraction (Witwicki & Durfee, 2010;Oliehoek, Witwicki, & Kaelbling, 2012), producing forward search algorithm searchinginfluence-space (Witwicki et al., 2012). Influence-based abstraction reformulatesjoint-policy search space space influences, represent effect agentpolicies one another. notion influences relates closely private/public actions influences equated public actions, intricate policiesinfluences depend equated private actions. aforementionedwork shows heuristic search influence space lead significant improvementperformance, mostly due pruning search space (we arrive similarconclusion respect work discussed Section 8). workheuristic search influence space similar work, two main differencessets apart. First, methods described apply transition-decoupledPOMDP models, restrict number agents affecting variable one,whereas ma-strips limit number agents affecting variable. Second,methods aimed preserving privacy distributing search,making centralized search efficient.Overall, motivation existing work planning uncertainty resembles Factored Planning utilizing (MA) structure order centrally solveproblem efficiently. research tackles problems stem301fiNissim & Brafmanmodels characteristics (e.g. MDPs stochastic actions, POMDPs partial observability etc.).Therefore, product body research usually specialized methodsprovide general solution schema applicable domains, despite solving general (and complex) models. important models representing problemsharder solve classical planning DEC-POMDPs shown NEXPcomplete general case (Bernstein et al., 2002), believe field planningbenefit general solutions simplest fundamental model classical planning,presented work.3.2 PrivacyPrivacy wide topic research many technological, social, legal aspects.interest lies much well defined area secure multi-party computation (Yao,1982, 1986), subfield Cryptography. goal methods developed secure multiparty computation enable multiple agents compute function inputs,keeping inputs private. specifically, agents 1 , . . . , n , private datax1 , . . . , xn , would like jointly compute function f (x1 , . . . , xn ), without revealingprivate information, reasonably deduced valuef (x1 , . . . , xn ).Formal results area provide protocols computing various functionsguaranteed private various assumptions, is, informationinputs functions obtained beyond deduced output.assumptions cover three key aspects problem. first aspect modeladversary party seeking violate privacy. example, relatively simpleassumption one honest curious, agents follows protocollaid out, also curious try deduce information agentsinformation obtains execution protocol. Malicious agents,hand, may deviate protocol order gain additional information.Furthermore, agents may collude, various results place cap numbermalicious colluding agents. aspects adversary could computationalpower memory. second aspect network model. communication synchronousasynchronous? channels secure? information get lost channel?Finally, third aspect meaning secure. actually two sub-aspectsit. First, information exactly kept private. Second, strongsecurity guarantees. Here, much like cryptography, one makes various assumptionsprotocol secure, e.g., factoring hard.Privacy-preserving ma-strips planning clearly instance secure multi-partycomputation problem. input function agents sets actions, initialstate, goal state, output plan. privacy requirements bitweaker standard model, inputs public (i.e., public aspectspublic actions). hand, output actually private: privateactions agents plan. natural adversary model one honest-butcurious, although requirement dictated problem definition, diverseadversary models considered.302fiDistributed Heuristic Forward Searchprinciple, appears techniques developed area secure multiparty computation extended setting distributed planning, complexityquickly becomes unmanageable. example, common approach secure multi-partycomputation uses cryptographic circuits. solving shortest path problem (e.g.,network routing, Gupta et al., 2012), size circuits created polynomialsize graph. setting function f computes shortest path implicitgraph induced descriptions agents actions. graph exponentialproblem description size, quickly becomes infeasible construct circuits giventime memory limitations. true planning NP-hard forward searchalgorithms do, general, require exponential time/memory, purpose heuristic searchreduce search space solve large problems low-polynomial time. Requiringconstruction exponential-sized circuits a-priori contradicts goal efficiencyfeasibility.computational problem closely resembles privacy-preserving ma-stripsplanning privacy preserving constraint satisfaction, subject whole subfield distributed CSPs, mentioned earlier. DisCSP, agent single variable,exist binary unary constraints. Binary constraints public sinceone agent knows existence, unary constraints considered privateinformation. meeting scheduling, agent single variable whose values possiblemeeting time slots. binary constraint could equality constraint valuestwo variables belonging different agents, unary constraint represents slotsagent cannot hold meetings. Early work distributed CSP explicitly considerprivacy issues, except type weak privacy mentioned earlier. Thus, agentsexpected solve underlying CSP without explicitly revealing unary constraints.Later on, work distributed CSPs (Yokoo, Suzuki, & Hirayama, 2002; Silaghi & Mitra,2004) identified fact even algorithm weakly private, private informationmay leak search process. example, search, whenever agent sendsassignment variable agents, deduce valueunary constraint forbidding it. value end assigned solution,agent revealed private information could deducedviewing solution. Thus, followup work focused question measureprivacy loss (Franzin, Rossi, Freuder, & Wallace, 2004; Maheswaran, Pearce, Bowring,Varakantham, & Tambe, 2006), analyzing much information specific algorithms lose(Greenstadt, Pearce, & Tambe, 2006), question alter existing DisCSPalgorithms handle stricter privacy demands (Greenstadt, Grosz, & Smith, 2007; Leaute& Faltings, 2009). recently, work provides formal guarantees DisCSPalgorithms emerged (Leaute & Faltings, 2009; Grinshpoun & Tassa, 2014). workbuilds techniques area secure multi-party computation.4. Multi-agent Forward Searchsection describes distributed variant forward best-first search, callmafs. begin algorithm itself, including overview pseudo-code. Then,provide example flow mafs, discussion finer points.303fiNissim & Brafman4.1 MAFS AlgorithmAlgorithms 1-3 depict mafs algorithm agent . mafs, separate search spacemaintained agent. agent maintains open list states candidatesexpansion closed list already expanded states. expands stateminimal f value open list, initialized agents projected viewinitial state. agent expands state s, uses operators only. meanstwo agents (that different operators) expanding state, generate differentsuccessor states.Since agent expands relevant search nodes, messages must sent agents,informing one agent open search nodes relevant expanded another agent. Agentcharacterizes state relevant agent j j public operator whose publicpreconditions (the preconditions aware of) hold s, creating actionpublic. case, Agent send Agent j .Algorithm 1 mafs agent1: TRUE2:messages message queue3:process-message(m)4:extract-min(open list)5:expand(s)Algorithm 2 process-message(m = hs, gj (s), hj (s)i)open closed list gi (s) > gj (s)add open list calculate hi (s)3:gi (s) gj (s)4:hi (s) max(hi (s), hj (s))1:2:messages sent agents contain full state s, i.e. including publicprivate variable values (we later discuss encrypted), well costbest plan initial state found far, sending agents heuristicestimate s. agent receives state via message, checks whether stateexists open closed lists. appear lists, insertedopen list. copy state higher g value exists, g value updated,closed list, reopened. Otherwise, discarded. Whenever received state(re)inserted open list, agent computes local h value state,choose between/combine value calculated h value receivedmessage. heuristics known admissible, example, agent could choosemaximal two estimates, done Line 4 Algorithm 2. Otherwise, agentfree combine estimates however sees fit, depending known characteristicsheuristics. agent affect correctness algorithm,could affect search efficiency. issue combining heuristic estimates discussedSection 9.1.304fiDistributed Heuristic Forward SearchAlgorithm 3 expand(s)1: move closed list2: goal state3:broadcast agents4:broadcasted agents5:return solution6: agents j7:last action leading public j public actionpublic preconditions hold8:send j9: apply successor operator10: successors s011:update gi (s0 ) calculate hi (s0 )12:s0 closed list fi (s0 ) smaller s0 movedclosed list13:move s0 open listagent expands solution state s, sends agents awaitsconfirmation, sent whenever expand, broadcast state (Line 3Algorithm 3). simplicity, order avoid deadlocks, agent eitherbroadcasts confirms solution, allowed generate new solutions. solutionfound one agent, one lower cost chosen, ties brokenchoosing solution agent lower ID. solution confirmed(broadcasted) agents (Line 4 Algorithm 3), agent returns solutioninitiates trace-back solution plan. also distributed process,involves agents perform action optimal plan. initiating agentbegins trace-back, arriving state received via message, sendstrace-back message sending agent. continues arriving initial state.trace-back phase done, terminating message broadcasted solutionoutputted.see, general simple scheme apply actions/operatorssend relevant generated nodes agents used distributesearch algorithms. However, various subtle points pertaining message sendingtermination influence correctness efficiency distributed algorithm,discuss later.better demonstrate flow algorithm, consider example given Figure4.1. example, two agents must cooperate order achieve goal.agents actions described left-hand side, every node graph depictsaction, edge (u, v) indicates u either achieves destroys preconditionv. two public actions a5 , a8 , affect/depend public variable, v4 ,rest actions private. central part figure depicts jointsearch space, i.e., nodes generated centralized search. right-hand site depictslocal search space agents, i.e., nodes generated sent agentrunning mafs. initial state, variable values zero (i.e., = 0000), goal305fiNissim & BrafmanFigure 1: Description actions example planning problem, reachable searchspace, search space generated mafs. Actions represented <pre, eff > states denoted values variables v1 , v2 , v3 , v4 respectively(For example, 1122 denotes state v1 = 1, v2 = 1, v3 = 2, v4 = 2.).G = {v4 = 2}. values private variables belonging agents (v3 agent1, v1 , v2 agent 2) shown bold. values required knownagents, fact regarded dont cares, used identifiers.agents begin searching using MAFS, applies actions only. Therefore, agent2 quickly exhausts search space, since far concerned, state 0020 dead end.Agent 1 generates search space, applies public action a5 , results state= 2201. sent agent 2, since public preconditions a8 hold (Line7 Algorithm 3). Upon receiving s, agent 2 continues applying actions, eventuallyreaching goal state, broadcasted.4.2 Discussiondiscuss subtle points mafs.4.2.1 Preserving Agent Privacygoal preserve privacy, may appear mafs agents revealingprivate data transmit private state messages. Yet, fact,information used agents, altered. simply copiedfuture states used agent. Since private state data used ID,agents encrypt data keep table locally, maps IDs private states.encryption easily made generate multiple IDs private state,306fiDistributed Heuristic Forward SearchID never used twice, agents cannot identify others privatestates. Consequently, algorithms based distributed search paradigm describedweakly privacy preserving. issue privacy discussed Section 9.1.compute heuristic estimates states receives, agent must assess effortrequired achieve goal them. this, needs informationeffort required agents construct part plan. fully cooperativesetting, agent access full description agents actions.privacy preserving setting, two issues arise. First, agents partial informationagents capabilities access public interface. Second,different agents may compute different heuristic estimates stateagent full information capabilities, others.issue affect actual algorithm, agnostic agents computeheuristic estimate, although fact agents less information lead poorerheuristic estimates. hand, agents free use different heuristic functions,demonstrate empirically, using public interfaces only, still ableefficiently solve planning problems.state reached via message, includes sending agents heuristic estimate.Therefore, receiving agent two (possibly different) estimates use.heuristics known admissible, clearly maximal (more accurate) valuetaken, line 4 Algorithm 2. not, agent free decide useestimates, depending known qualities.4.2.2 Relevancy Timing MessagesState considered relevant agent j public action public preconditions hold last action leading public (line 7 Algorithm 3).means states products private actions considered irrelevantagents. turns out, since private actions affect agents capabilityperform actions, agent need send states last action performed public, order maintain completeness (and cost-optimality, proved next section).Regarding states products private actions irrelevant decreases communication, effectively pruning large, symmetrical parts search space. fact,show Section 8 property mafs used obtain state-space pruningcentralized planning algorithms, using method called Partition-based pruning.hinted earlier, exists flexibility regarding relevant statessent. Centralized search viewed essentially sending every state (i.e., insertingopen list) generated. mafs, relevant states sentexpanded (as pseudo-code) generated (changing Algorithm3 moving for-loop line 6 inside for-loop line 10). timingmessages especially important distributed setting since agents may differentheuristic estimates. Sending messages generated increases communication,allows states considered promising agent expandedanother agent earlier stage. Sending relevant states expanded,hand, decreases communication, delays sending states viewedpromising. Experimenting two options, found lazy approach,307fiNissim & Brafmansending messages expanded, dominates other, likelycommunication costly.4.2.3 Concurrent SolutionsAlthough solution plan outputted mafs sequential, i.e. requires agents executeactions turns, quite easily parallelized. Since private actions require/affect agents private propositions, agents perform concurrently withouthurting correctness plan, long public actions (interaction pointsagents) performed correct order. Intuitively, execution order publicactions maintained, agents free execute private actions concurrently.parallelization done time linear solution length, requires jointcomputation. Another option using one many algorithms known parallelizationplans (Backstrom, 1998). Given existence private actions, plans muchpotential concurrency.4.2.4 Search Using Complex ActionsSection 1, mentioned scenario search operators corresponding real-worldactions implemented using complex simulation software. situation arise,example, team heterogeneous robotic agents, dedicated simulator actions. approach well suited settings: First, forward searchmethods capable using generative, rather declarative models agents actions, central step involves generation successor states insertionappropriate queues. oblivious operators described implemented, long successor states generated. Second, approach respectsnatural system structure, agent need apply operators. Thus,need share generative models amongst agents.One problem, however, fact contemporary methods generatingheuristic functions require either declarative strips-like description generative model(in case sampling methods). Fortunately, empirical results indicate useapproximate model works quite well practice. Indeed, approach assumesagents use public part agents action model, approximation.Even original action model generative, declarative approximate modelconstructed using learning techniques (Yang, Wu, & Jiang, 2007). Alternatively, samplingmethods could use suitably developed simplified simulator.5. Optimal MAFSmafs presented, cost-optimal planning algorithm. Recall cost-optimalplan single-agent planning one achieves goal minimal cost, i.e.minimizing sum action costs plan. notion remains case,wish minimize sum cost agents participating plan. metricimportant systems describe cooperative agents like ma-strips,aim minimize cost entire system, specific agents. Moreover, costoptimal algorithms required applying mechanism design techniques planning308fiDistributed Heuristic Forward Searchsystems comprising selfish agents (Nissim & Brafman, 2013). settings,cost-optimal plan constitutes social-welfare maximizing solution. mafs slightlymodified order achieve cost-optimality. describe modifications,result variation a* refer Multi-Agent Distributed A* (mad-a*).a*, state chosen expansion agent must one lowestf = g + h value open list, heuristic estimates admissible. mad-a*,therefore, extract-min (Line 4 Algorithm 1) must return state.5.1 Termination DetectionUnlike a*, expansion goal state mafs necessarily mean optimalsolution found. case, solution known optimal agentsprove so. Intuitively, solution state solution cost f known optimalexists state s0 open list input channel agent,f (s0 ) < f . words, solution state known optimal f (s) flowerbound ,flowerbound lower bound f -value entire system (which includesstates open lists, well states messages processed, yet).detect situation, use Chandy Lamports snapshot algorithm (Chandy& Lamport, 1985), enables process create approximation global statesystem, without freezing distributed computation. approximationcheck whether state exists whose f value lower value candidate solution.Although check conducted respect approximate global state, snapshotalgorithm guarantees answer positive iff true global state.snapshot algorithm works using marker messages. process wantsinitiate snapshot records local state sends marker outgoing channels.processes, upon receiving marker, record local state, statechannel marker came empty, send marker messagesoutgoing channels. process receives marker recorded local state,records state incoming channel marker came carryingmessages received since first recorded local state.Although guarantee global state computed algorithm actuallyoccurred point run mad-a*, approximation good enoughdetermine whether stable property currently holds system. property systemstable global predicate remains true becomes true. Specifically,properties form flowerbound c fixed value c, stable h globallyconsistent heuristic function. is, f values cannot decrease along path.case, path may involve number agents, h values. localfunctions h consistent, agents apply max operator receiving state viamessage (known pathmax ), property holds3 . solution verification procedureusing snapshot algorithm, run whenever solution state expanded (meaningminimal f -value), agent receives confirmation agents solutionstate (In pseudocode, change Line 5 Algorithm 3 initiate verificationsolution.). occurs agents expanded solution state.3. Although recent work (Holte, 2010) shows pathmax necessarily make bona-fide consistentheuristic, pathmax ensure f -values along path non-decreasing.309fiNissim & Brafmansnapshot algorithm returns true stable property states exist lowerf -value, algorithm return solution optimal (In pseudocode, change Line1 Algorithm 1 receive true solution verification procedure.).note simplicity pseudo-code omitted detection situationgoal state exist. done determining whether stableproperty open states system holds, using snapshot algorithm.5.2 Proof Optimalityprove optimality mad-a*. must note presented, mada* maintains completeness (and optimality) actions achieve goalcondition considered public. property assumed throughout section,algorithm easily modified remove it.4 begin proving following lemma(and corollary) regarding solution structure planning problem.demonstrate always exists optimal solution structure foundmad-a*. continue proving extension well-known result a*,required completeness mad-a*. Finally, proving mad-a*s optimality,prove correctness termination detection procedure.Lemma 1. Let P = (a1 , a2 . . . , ak ) legal plan planning problem . Letai , ai+1 two consecutive actions taken P different agents, least oneprivate. P 0 = (a1 , . . . , ai+1 , ai , . . . , ak ) legal plan P (I) = P 0 (I).Proof. definition private public actions, ai , ai+1 actions belongingdifferent agents, varset(ai )varset(ai+1 ) = , varset(a) set variablesaffect affected a. Therefore, ai achieve ai+1 preconditions,ai+1 destroy ai preconditions. Therefore, state aiexecuted P , ai+1 executable s, ai executable ai+1 (s), ai (ai+1 (s)) =ai+1 (ai (s)). Therefore, P 0 = (a1 , . . . , ai+1 , ai , . . . , ak ) legal plan . Since suffix(ai+2 , ai+3 , . . . , ak ) remains unchanged P 0 , P (I) = P 0 (I), completing proof.Corollary 1. Let P = (a1 , a2 , . . . , ak ) solution planning problem . Then,exists equal cost solution P 0 = (a01 , a02 , . . . , a0k ) satisfies following properties:1. P 0 contains permutation actions P .2. ai first public action P 0 , a1 , . . . , ai belong agent.3. pair consecutive public actions ai , aj P 0 , actions al , < l j belongagent.Proof. Using repeated application Lemma 1, move ordered sequence privateactions performed agent , would immediately subsequent publicaction maintain legality plan. Clearly, P 0 permutation P thereforecost P P 0 identical. implies P cost optimal, P 0 .4. order remove assumption, agent must send messages states creatingaction achieved goal (which may private). agent approves solution stateprivate goal achieved.310fiDistributed Heuristic Forward SearchNext, prove following lemma, extension well known resulta*. follows, tacitly assumed liveness property conditionsevery sent message eventually arrives destination agent operations takefinite amount time. Also, clarity proof, assume atomicityexpand process-message procedures.Lemma 2. non-closed node optimal path Pproperties 2 & 3 Corollary 1, exists agent either open node s0incoming message containing s0 , s0 P g (s0 ) = g (s0 ) .Proof. : Let P = (I = n0 , n1 , . . . , nk = s). open list agent (finish algorithms first iteration), let s0 = lemma trivially true sinceg (I) = g (I) = 0. Suppose closed agents. Let set nodes niP closed agent , g (ni ) = g (ni ). empty since,assumption, . Let nj element highest index, closed agent .Clearly, nj 6= since non-closed. Let action causing transition nj nj+1P . Therefore, g (nj+1 ) = g (nj ) + cost(a).agent performing a, nj+1 generated moved open listlines 9-13 Algorithm 3, g (nj+1 ) assigned value g (nj ) + cost(a) = g (nj+1 )claim holds.Otherwise, performed agent 0 6= . public action, preconditions hold nj , therefore nj sent 0 line 8 Algorithm 3.private action, definition P , next public action a0 P performed0 . Since private actions change values public variables, public preconditions a0 must hold nj , therefore nj sent 0 line 8 Algorithm3. Now, message containing nj processed 0 , nj addedopen list 0 Algorithm 2 claim holds since g0 (nj ) = g (nj ) = g (nj ). Otherwise, 0 incoming (unprocessed) message containing nj claim holds sinceg (nj ) = g (nj ).Corollary 2. Suppose h admissible every , suppose algorithmterminated. Then, optimal solution path P follows restrictions Lemma1 goal node s? , exists agent either open nodeincoming message containing s, P fi (s) h (I).Proof. : Lemma 2, every restricted optimal path P , exists agenteither open node incoming message containing s, Pgi (s) = g (s) . definition f , since hi admissible,cases:fi (s) = gi (s) + hi (s) = g (s) + hi (s)g (s) + h (s) = f (s)since P optimal path, f (n) = h (I), n P , completes proof.Another lemma must proved regarding solution verification process. assumeglobal consistency heuristic functions, since admissible heuristics madeconsistent locally using pathmax equation (Mero, 1984), using max311fiNissim & Brafmanoperator line 4 Algorithm 2 heuristic values different agents. requiredsince flowerbound must non-decreasing.Lemma 3. Let agent either open node incoming messagecontaining s. Then, solution verification procedure state f (s ) > f (s)return false.Proof. Let agent either open node incoming messagecontaining s, f (s) < f (s ) solution node . solution verification procedure state verifies stable property p = f (s ) flowerbound . Sinceflowerbound represents lowest f -value open unprocessed state system,flowerbound f (s) < f (s ), contradicting p. Relying correctnesssnapshot algorithm, means solution verification procedure return false,proving claim.prove optimality algorithm.Theorem 1. mad-a* terminates finding cost-optimal path goal node, one exists.Proof. : prove theorem assuming contrary - algorithm terminatefinding cost-optimal path goal node. Three cases considered:1. algorithm terminates non-goal node. contradicts termination condition, since solution verification initiated goal state expanded.2. algorithm terminate. Since dealing finite search space,let () denote number possible non-goal states. Since finitenumber paths node search space, reopened finitenumber times. Let () maximum number times non-goal nodereopened agent. Let time point non-goal nodesf (s) < h (I) closed forever agents . exists, since:a) assume liveness message passing agent computations; b)() () expansions non-goal nodes , non-goal nodes search spacemust closed forever ; c) goal node f (s ) < h (I) exists5 .Corollary 2 since optimal path goal state exists,agent expanded state time t0 , f (s ) h (I). Sinceoptimal solution, t0 t, flowerbound f (s ) time t0 . Therefore, verificationprocedure return true, algorithm terminates.Otherwise, t0 < t. Let 0 last agent close non-goal state f0 (s) <f (s ). 0 open list incoming message. truebroad-casted agents , every time closedagent (when expands it), immediately broad-casted again, endingagents open list message queue. Now, 0 open nodes f value lower , eventually expand , initiating solution verificationprocedure return true, since flowerbound f (s ). contradictsassumption non-termination.5. needed since goal node expansions bounded.312fiDistributed Heuristic Forward Search3. algorithm terminates goal node without achieving optimal cost. Supposealgorithm terminates goal node f (s) > h (I). Corollary 2,existed termination agent open node s0 ,incoming message containing s0 , s0 optimal path f (s0 ) h (I).Therefore, Lemma 3, solution verification procedure state return false,contradicting assumption algorithm terminated.concludes proof.final note, point results used prove MAFS versions search algorithms, best-first search, complete, thanks Corollary 1Lemma 2. Corollary 1 guarantees focus solutions certain properties.is, solution exists, solution properties exists. Lemma 2 ensuresevery path properties generated, eventually, solution found.note cannot provide guarantees distributed versions every searchalgorithm simply arbitrary search algorithm may prune, reason, solutions properties ensured MAFS schema, keeping solutions.course, completeness guarantees MAFS require liveness properties discussedearlier hold.6. Planning FrameworkOne main goals work provide general scalable framework solvingplanning problem. believe framework provide researchersfertile ground developing new search techniques heuristics planning,extensions richer planning formalisms.chose Fast Downward (Helmert, 2006) (FD) basis frameworkMA-FD. FD currently leading framework planning, numberalgorithms heuristics provides, terms performance winnerspast three international planning competitions implemented top it. FD alsowell documented supported, implementing testing new ideas relatively easy.MA-FD uses FDs translator preprocessor, minor changes support distribution operators agents. agent also receives projected version publicoperators agents. information (its actions projected publicactions) provided heuristic used agent. addition PDDL files describing domain problem instance, MA-FD receives file detailing numberagents, names, IP addresses. agents shared memory,information relayed agents using messages. Inter-agent communicationperformed using TCP/IP protocol, enables running multiple MA-FD agentsprocesses multi-core systems, networked computers/robots, even cloud platforms likeAmazon Web Services. MA-FD therefore fit run number (networked)processors, optimal satisficing setting.settings currently implemented available6 , since full flexibilityregarding heuristics used agents, heuristics available FD also available MAFD, requiring preprocessing agents view problem, creating projected6. code available http://github.com/raznis/dist-selfish-fd .313fiNissim & Brafmanview. New heuristics easily implementable, FD, creating new search algorithmsalso done minimal effort, since MA-FD provides ground-work (parsing,communication, etc.).7. Empirical Resultsfollowing section describes empirical evaluation methods. begin evaluating mafs, comparing current state-of-the-art approaches. describe resultsmad-a*, compared centralized cost-optimal search. also provide scalability resultsmethods, scaling 40 agents.7.1 Evaluating MAFSevaluate mafs non-optimal setting, compare state-of-the-art distributedplanner map-pop (Torreno et al., 2012), Planning-First algorithm (Nissim et al.,2010). noted Section 1, another available algorithm distributed planningvia reduction distributed CSPs using off-the-shelf disCSP solver. foundapproach incapable solving even small planning problems, therefore omittedresults tables. problems used benchmarks InternationalPlanning Competition (IPC) tasks naturally cast problems.Satellites Rovers domains motivated real applications used NASA.Satellites requires planning scheduling observation tasks multiple satellites,equipped different imaging tools. Rovers involves multiple rovers navigatingplanetary surface, finding samples communicating back Lander. Logistics,Transport Zenotravel transportation domains, multiple vehicles transportpackages destination. Transport domain generalizes Logistics, adding capacityvehicle (i.e., limit number packages may carry) different moveaction costs depending road length. consider problems Rovers Satellitesdomains loosely-coupled, i.e., problems agents many private actions (e.g.,instrument warm-up placement Rovers, affect agents),small number public actions required solution plans. hand,consider transportation domains tightly-coupled, private actions (onlymove actions Logistics) many public actions (load/unload actions).planning problem, ran mafs, using eager best-first search alternation open list one queue two heuristic functions ff (Hoffmann &Nebel, 2001) preferred actions context-enhanced additive heuristic (Helmert &Geffner, 2008). Table 7.1 depicts results mafs, map-pop Planning-First, IPCdomains supported map-pop. also include results baseline centralized planner(denoted FD, implemented top Fast-Downward) using eager best-first searchheuristics used mafs. Recall planner solves problem completeknowledge, unlike configurations. compare algorithms across three categories 1) solution cost reports total cost outputted plan, 2) running time,3) number messages sent planning process. Experiments runAMD Phenom 9550 2.2GHZ processor, time limit set 60 minutes, memoryusage limited 4GB. configurations shown Table 7.1, experiments re314fiDistributed Heuristic Forward Search#problemagentsLogistics4-03Logistics5-03Logistics6-03Logistics7-04Logistics8-04Logistics9-04Logistics10-05Logistics11-05Logistics12-05Logistics13-07Logistics14-07Logistics15-07Rovers52Rovers62Rovers73Rovers84Rovers94Rovers104Rovers114Rovers124Rovers134Rovers144Rovers154Rovers176Satellites32Satellites42Satellites53Satellites63Satellites74Satellites84Satellites95Satellites105Satellites115Satellites125Satellites135Satellites146Satellites158Satellites1610Satellites1712Solution costfd mafs map-pop p-f212020X282727X262525X433637X323131X393636X504551X5454XX474445X8587XX6868XX9495XX222224 24383739X181818X262627X403836X3738XX423734X212120X4849XX333135X434644X5352XX111111 11261720 20211615X202020X282222X272626X373029X373029X363131X434349X7561XX494443X6463XX625656X484949XRuntimefd mafs map-pop p-f0.1 0.0520.9X0.10.190.4X0.1 0.0660.6X0.10.2233.3X0.1 0.16261X0.1 1.02193.3X0.1 0.43471X0.12.7XX0.11.31687X0.10.9XX0.1 0.67XX0.1 0.74XX0.1 0.1318.7 22.40.1 0.0718.2X0.1 0.0744.1X0.10.2744X0.1 0.82222X0.1 0.41XX0.1 0.34132.5X0.1 0.0934.4X0.1 0.15XX0.1 0.42443.8X0.1 0.33164X0.1 0.57XX0.1 0.014.5 6.80.1 0.176.4 35.20.1 0.1515.4X0.1 0.0212.2X0.1 0.2328.8X0.1 0.2140.7X0.1 0.3593.3X0.1 0.4165.9X0.1 0.6951X0.21.176.9X0.57 0.88XX0.31.8123.4X0.663.9XX0.946.6481.2X1.126.72681XMessagesmafs map-pop p-f340375X4501565X4701050X29114898X9404412X29703168X209714738X14933XX423028932X5140XX2971XX6194XX84323 59027313X225490X93712102X3804467X271XX2992286X435410X472XX3107295X2522625X628XX778 10436109 14478250X30323X248543X133678X3971431X355942X514904X3901240X639XX7211781X1507XX22794942X217226288XTable 1: Comparison greedy best-first search, map-pop Planning-First. Solutioncost, running time (in sec.) number sent messages shown. Xdenotes problems werent solved one hour, 4GB memorylimit exceeded. Best performance entries bold.stricted run single processor (core), running time would easily comparable7 .X signifies problem solved within time-limit, exceeded memoryconstraints.7. result tables, multiple processors used.315fiNissim & Brafmanclear mafs overwhelmingly dominates map-pop Planning-First (denoted p-f), respect running time communication, solving problems fastersending less messages. problems solved least 70 faster map-pop,several Logistics Rovers problems solved 1000 faster, largestSatellites instance solved 400 faster. low communication complexitymafs important, since distributed systems message passing could costlytime-consuming local computation. Moreover, messages mafs essentiallystate description, message size always linear number propositions. Althoughproblems map-pop finds lower-cost solutions, cases mafs outputs bettersolution quality. believe mafs finds lower quality solutions, mostlymessage-passing takes longer local computation subset agentsability achieve goal own, made aware other,less costly solutions including agents. One possible way improving solution qualitywould using anytime search methods, improve solution quality time.comparison reference centralized planner, mafs takes longer compute solution, cases solutions lower equal cost. slowdown expected,communication times partial information mafs agents have, unlikecentralized planner.7.1.1 Scalability MAFSorder evaluate scalability mafs, conducted experiments Logistics,Rovers Satellite domains IPC. domains, generated multipleproblem instances, increasing number agents k. Logistics, number citiesgrows linearly k, number airplanes remains constant 2, numberpackages always 2k. Satellites Rovers, number targets grows linearlyk, number available observations/locations 2k, instrumentation remainsconstant. experiments run Intel Xeon 2.4GHZ, 48-core machine. FDrun single processor mafs agent given dedicated processor. Cutofftime configurations set 1 hour (Wall-clock time), memory limit set100GB, regardless number processors used.Results scalability experiment seen Figure 7.1.1. every valuek, 5 different problem instances generated. runtime values shown averages5 them, error bars correspond standard deviation sample8 .loosely-coupled domains Rovers Satellites, mafss increase runtime nearlylinear, largest problem instances domains unsolvedcentralized FD, solved 90 seconds. Efficiency (speedup divided numberprocessors) values always superlinear 16 largest Satellites problem solvedconfigurations > 11 largest Rovers problems. superiorspeed, mafs outputs lower quality solutions (having higher total cost) domains.average, mafs solution cost 14% higher Rovers 6% higher Satellites,maximal increase 20% 13% respectively. cause deterioration solutionquality domains likely fact many problems, small subsetsagents reach solution without agents. may lead mafs quickly finding8. error bars omitted cases standard deviation small shown scale.316fiDistributed Heuristic Forward Searchsolution involving subset, solution usually costly foundcentralized search, considers operators agents. Logistics, tightlycoupled domain many actions public, problems still solved much faster (5faster 40-agent problems) mafs, efficiency < 1 average 0.12largest instances. Here, solution cost average 0.5% higher centralized search,maximal increase 2.5%, maximal decrease (improvement solutionquality) 2%. Overall, see w.r.t. runtime, mafs scalable performingsuperlinear efficiency loosely-coupled domains, exhibiting speedup tightlycoupled ones.7.2 Evaluating MAD-A*evaluate mad-a* respect centralized optimal search (a*), ran algorithmsusing state-of-the-art Merge&Shrink heuristic9 (Helmert, Haslum, & Hoffmann, 2007).configurations run machine, mad-a*, agentallocated single processor, a* run single processor. Wall-clock time limitset 30 minutes, memory usage limited 4GB, regardless number coresused. show results problems constitute limit either configuration.existing IPC problems domains solvable configurations. Table7.2 depicts runtime, efficiency (speedup divided number processors), numberexpanded nodes average agents initial state h-values.comparing mad-a* centralized a*, intuition efficiency low,due inaccuracy agents heuristic estimates, overhead incurredcommunication. fact, local estimates agents much less accurateglobal heuristic, apparent lower average h values initial state,given approximate measures heuristic quality discussing admissible heuristics(which required optimal search), higher values necessarily accurate.tightly-coupled domains Logistics, Transport Zenotravel, notice lowefficiency values, mostly due large number public actions, result manymessages passed agents, relatively small amount local (private)search agents. However, loosely-coupled domains Satellites Rovers,mad-a* exhibits nearly linear super-linear speedup, solving 2 problems solvedcentralized a*. analyze reason superlinear speedup elaborateimportant issue Section 8.7.2.1 Scalability MAD-A*results Table 7.2, clear mad-a* scale well tightly-coupleddomain Logistics, Transport Zenotravel. loosely-coupled domainsRovers Satellites, however, mad-a* exhibited high efficiency, outperforming centralizedsearch cases. section examines scalability mad-a* domainscompared centralized a*.Section 7.1.1, created new instances problems increasing agents usingproblems generators. domains, number available locations goals increased9. used exact bisimulation abstraction size limit 10K (DFP-bop) shrink strategyMerge&Shrink (Nissim, Hoffmann, & Helmert, 2011).317fiNissim & BrafmanLogisticsSatellitesFDMAFS3,5003,0002,500RuntimeRuntime2,5002,0001,5001,0002,0001,5001,000500500002025FDMAFS3,00030354015Number agents2025303540Number agentsRovers1,400FDMAFS1,200Runtime1,000800600400200010152025303540Number agentsFigure 2: Scalability MAFS w.r.t. centralized FD. configurations, runtimeseconds shown.318fiDistributed Heuristic Forward SearchproblemLogistics4-0Logistics5-0Logistics6-0Logistics7-0Logistics8-0Logistics9-0Logistics10-0Logistics11-0Rovers3Rovers4Rovers5Rovers6Rovers7Rovers12Satellites3Satellites4Satellites5Satellites6Satellites7Transport1Transport2Transport3Transport4Transport5Zenotravel3Zenotravel4Zenotravel5Zenotravel6Zenotravel7Zenotravel8Zenotravel9Zenotravel10Zenotravel11Zenotravel12agents3334445522223422334222222222233333a*0.070.160.291.421.282.171321800.20.078.24X32.4190.80.30.616.831.93X0.020.171.3554.6X0.330.320.30.470.551.2231.79.32.99XTimemad-a* Efficiency0.030.780.130.410.150.648.260.042.890.1111.60.05X0.00X0.000.110.910.040.884.40.943016.821.5827.61.730.290.520.40.753.91.440.920.7018.260.080.130.210.4020.640.03335.150.08XN/A0.420.390.430.370.420.360.610.390.80.341.710.243150.033380.0111.70.09XN/AExpansionsa*mad-a*212496281102054797222921652012216771157184432836875724560551X5713287X12869121213079307995X 1827435711729646767504963979259142812149818504523664742557138281731X1303910628524216286950054656935275924397124XX75169762145772701280621568113654617753406745088116872441646120157200868XXa*202724332733374111812249111117101710547934101267101011916201116init-hmad-a*172322292429343897112177511712944410105688971417914Table 2: Comparison centralized a* mad-a* running multiple processors. Running time (in sec.), average initial state h-values mad-a*s efficiency valuesw.r.t. a* shown. Entries bold denote super-linear efficiency mad-a*.linearly number agents k. value k, created 5 problem instances,reported runtime values averages. experiments run IntelXeon machine used Section 7.1.1, a* run single processor,mad-a* agent given dedicated processor. Runtime limit set 90 minutesmemory limited 100GB regardless number processors.Figure 7.2.1 see domains, mad-a* solves problems fastercentralized a*, solve larger problems within time limit. However, efficiencyvaries two domains Satellites, loosely-coupled, efficiencysuperlinear cases, reaching 7.3 largest problem solved configura319fiNissim & BrafmanRoversSatellitesA*MAD-A*2,5002,0004,000RuntimeRuntimeA*MAD-A*5,0001,5001,0003,0002,0005001,0000034564Number agents5678910Number agentsFigure 3: Scalability MAD-A* w.r.t. centralized A* loosely-coupled domains.configurations, runtime seconds shown.tions. Rovers, tightly-coupled, efficiency 0.5 problems, droppinglow 0.35 largest problem solved configurations. Recall problemsolved mad-a* difficult, since agent private information unknownothers, drop efficiency expected. mentioned previous section, nextelaborate reason mad-a*s superlinear efficiency loosely-coupled problems.8. Partition-Based Path Pruningempirical results presented previous section raise interesting question:MAD-A* achieve > 1 efficiency weakly-coupled environments? knownusing consistent heuristic, a* optimal number nodes expandsrecognize optimal solution. principle, appears mad-a* expand leastsearch tree, clear, a-priori, reach super-linear speedupcomparing a*. section provides explanation phenomenon partialorder reduction method inherently built-in mad-a* (and mafs well), exploitssymmetry search space prune effect-equivalent paths. describe pruningmethod detail, showing empirical evidence supporting claim within methodlies much power mad-a*.exploitation symmetry utilizes notion public private actions.noted Corollary 1, existence private actions implies existence multipleeffect-equivalent permutations certain action sequences. a* recognize exploitfact, mafs does. Specifically, imagine agent generated state usingone public actions, satisfies preconditions public action agentj . Agent eventually send agent j , latter eventually applyit. Now, imagine agent private action a0 applicable state s, resulting320fiDistributed Heuristic Forward Searchstate s0 = a0 (s). a0 private , fact applicable deduceapplicable s0 well. Hence, a* would apply s0 . However, mafs, agentj would apply s0 receive s0 agent . Thus, mafsexplore possible action sequences. fact also clearly seen examplegiven Figure 4.1 reachable search space example 31 states,number reachable states using mafs 16.Since mafss inherent pruning action sequences requires partitioningactions, pertain systems, factored system internaloperators. Since difference ma-strips planning problem stripsone fact actions partitioned agents, re-factor centralizedproblem artificial one? mapping actions disjoint setsSkAi = A, representing agent, distinguish privatepublic operators. Given distinction, pruning rule used simple:Partition-Based (PB) Pruning Rule: Following private action Ai ,prune actions Ai .fact pruning rule optimality-preserving (i.e., prune optimalsolutions) follows immediately Corollary 1 as, exists optimal solution ? ,permuted legal, optimal plan pruned. This, however,enough maintain optimality a* search. present slight modificationa* algorithm, allows application optimality preserving pruning methods(such PB-pruning) purpose optimal planning.8.1 Path Pruning A*path pruning a*, (denoted pp-a*), search algorithm receives planningproblem pruning method input, produces plan , guaranteedoptimal provided respects following properties: (i) optimality preserving,(ii) prunes according last action. easy see, example, PBpruning respects second condition, since fires according last action.8.1.1 pp-a* versus a*pp-a* identical a* except following three changes. First, different data-typeused recording open node. pp-a*, open list node pair (A, s),state set actions, recording various possible ways reach previousstate. Second, node expansion subject pruning rules method . Namely, ppa* executes applicable action a0 state (A, s) least one actions.t. execution a0 allowed pruning rules. Third, duplicate stateshandled differently. a*, state already open reached anothersearch path, open list node updated action lower g value, casetie drops competing path. contrast, ties pp-a* handled preservinglast actions led paths. Hence, action led open statevia path cost g, existing open list node (A, s) g value,node updated (A {a}, s), thus actions leading path cost g saved. Tiebreaking also affects criterion closed nodes reopened. a*, nodes321fiNissim & Brafmanreopened reached via paths lower g values. pp-a*, action leadingstate closed node (A, s) contained A, g values equal,node reopens ({A {a}}, s). However, node expanded, actionsallowed previously pruned, executed. move provecorrectness pp-a*.8.1.2 Proof Correctness Optimalitynext lemma refers pp-a*, assumes optimality preserving pruningmethod, prunes according last action. say node (A, s) optimalpath P , contains action leads state path P , g(s) = g (s).notation P s0 denotes fact state precedes state s0 optimal path P .Lemma 4. pp-a*, non-closed state sk optimal non--pruned pathP sk , exists open list node (A0 , s0 ) optimal P .Proof. Let P optimal non--pruned path sk . open list, lets0 = lemma trivially true since g(I) = g (I) = 0. Suppose closed. Letset nodes (Ai , si ), optimal P , closed. empty, sinceassumption, . Let nodes ordered si P sj < j, letj highest index si .Since closed node (Aj , sj ) optimal g value, expanded priorclosing. properties pp-a*, follows expansion (Aj , sj ),optimal P , followed attempt generate node (Aj+1 , sj+1 ) optimalP well. Generation (Aj+1 , sj+1 ) must allowed, since highest indexassumption closed node containing optimal P . Naturally,sj P sj+1 .point, note actions Aj+1 cannot removed competing pathsj+1 , since (Aj+1 , sj+1 ) optimal g value. possible, though, additional actions leading sj+1 added node. updated node represented(A0j+1 Aj+1 , sj+1 ), property optimality P holds. Additionally, node(A0j+1 , sj+1 ) cannot closed generation, since again, contradicts highestindex property. Hence, exists open list node (A0 , s0 ) optimal P .concludes proof.Corollary 3. h admissible optimality-preserving, pp-a* using optimal.Proof. follows directly Lemma 4, optimality preserving propertyproperties pp-a*, allow every optimal, non--pruned path generated.8.2 Empirical Analysis PB-Pruningset check effect mad-a*s inherent exploitation symmetry efficiencycompared a*. hypothesis mad-a*s main advantage a* wellsupported results Table 8.2, shows comparison mad-a* centralizeda* using PB pruning. Here, see problems mad-a* achieves superlinearspeedup w.r.t. a*, applying partition-based pruning partition=agent reduces runtimeexpansions dramatically. cases, mad-a*s efficiency w.r.t. a* using PB pruning322fiDistributed Heuristic Forward SearchproblemLogistics4-0Logistics5-0Logistics6-0Logistics7-0Logistics8-0Logistics9-0Logistics10-0Logistics11-0Rovers3Rovers4Rovers5Rovers6Rovers7Rovers12Satellites3Satellites4Satellites5Satellites6Satellites7Transport1Transport2Transport3Transport4Zenotravel3Zenotravel4Zenotravel5Zenotravel6Zenotravel7Zenotravel8Zenotravel9Zenotravel10Zenotravel11Zenotravel12agents333444552222342233422222222233333a*0.070.160.291.421.282.171321800.20.078.24X32.4190.80.30.616.831.93X0.020.171.3554.60.330.320.30.470.551.2231.79.32.99XTimea?pb mad-a*0.060.030.170.130.290.151.078.261.072.891.5911.637X57.4X0.20.110.060.043.074.4164.93015.226.8210.127.60.290.290.580.43.153.91.840.9226.618.260.010.080.160.210.920.6429.8335.150.340.420.330.430.30.420.470.610.560.81.211.7112.883156.513382.0211.782.95XEfficiencya*a?pb0.78 0.670.41 0.440.64 0.640.04 0.030.11 0.090.05 0.0300000.91 0.910.88 0.750.94 0.350.271.58 0.261.73 0.090.52 0.500.75 0.731.44 0.270.70 0.670.360.13 0.060.40 0.380.03 0.020.08 0.040.39 0.400.37 0.380.36 0.360.39 0.390.34 0.350.24 0.240.03 0.010.01 0.010.09 0.06N/A0a*212854729216167714328345605515713287129213079X1172964496397912182366471382X6242695003527592791427062113677534011687220157XExpansionsa?pb21285272242511750299532132416298072512962672810732723553739137212182350338584639462254974425894967914220433126474180104340115652406708mad-a*2496110209722520122157184687572XX8612130799518274357676750259142814985045425578173113039102851628546569439712451676257712805681546167450884416461200868XTable 3: Comparison centralized a* without partition-based pruning, mada* running multiple processors. Running time, number expanded nodes,mad-a*s efficiency w.r.t. centralized configurations shown.sublinear. is, course, also due fact mad-a* solves difficultproblem incomplete information negative effect quality heuristicscomputed agents.note although structure evident benchmark planning domains(e.g. Logistics, Rovers, Satellites, Zenotravel etc.), general isnt always obviousway decomposing problem. work exploring PB pruning (Nissim, Apsel,& Brafman, 2012), describe automated method decomposing general planning problem, making PB pruning applicable general classical planning setting.note exist many partial-order reduction methods, CommutativityPruning (Haslum & Geffner, 2000), Stubborn Sets (Valmari, 1989; Wehrle & Helmert,323fiNissim & Brafman2012; Alkhazraji, Wehrle, Mattmuller, & Helmert, 2012; Wehrle, Helmert, Alkhazraji, &Mattmuller, 2013), Expansion Core (Chen & Yao, 2009), (Coles & Coles, 2010;Xu, Chen, Lu, & Huang, 2011). None methods subsumes PB pruning,interesting question combine methods maintaining optimality remainsopen field classical planning.9. Discussionwork raises number questions, research challenges, opportunitiesdiscuss.9.1 Privacynoted earlier, algorithms weakly privacy preserving. is, informationprivate actions, cost, private variables, private preconditions effectsever communicated agent agents. Nevertheless, case DisCSPdiscussed earlier, information elements could deduced agentsrun algorithm. given privacy preservation important goalwork, important try understand extent information leak.examine this, let us consider maximal amount explicit informationavailable agent agents. worse case, would exploreentire search tree rooted initial state run algorithm. However,agent see nodes tree, nodes correspond search statesobtained following public action (because states communicated). Thus,sub-tree visible agent corresponds one obtained full search tree topdown recursive process every state obtained following private action removed,parent removed node becomes parent children. Furthermore,information available state agent consists value localvariables state value public variables. (Recall that, private stateencrypted using different identifier, local states agents appear differentevery state, provide useful information).agent learn projected search tree? First, try identifydifferent states identical. Two states whose sub-trees identical deducedlocal states agents. Notice information cannot deducedplan alone. Consequently, cannot claim search-based methodsstrong privacy preserving. difficult identify fact two stateslocal state particular agent only. Let us, however, assume worst casescenario states also identifiable use term projected subtree state ids refer structure. Thus, agent knows possible values,renaming, local states agent occur following executionpublic actions. also knows existence macros consisting sequenceprivate actions followed public action allow agent move privatestate following public action next private state following execution nextpublic action. not, however, know structure local statededuce monolithic name. know nature local actions comprisingmacros enable transition two post-public-action states.324fiDistributed Heuristic Forward Searchstrong privacy preservation guaranteed, possible showpresent per domain basis distributed forward search algorithms offerweak privacy preservation. example, return logistics domain. Imaginetruck various service stations rest-stops locations private (i.e.,served truck). existence visible agents.proven formally showing projected search tree visible external agentidentical regardless number private locations truck. projectedsearch tree contains information available agents, impliesinformation available agents cases. see projectedtree same, recall public actions truck load unload. Furthermore,note loading unloading package location accessibleagent private action truck preconditions effects requiredaffected agent. Thus, states visible projected search treestates follow load/unload action location served agents well,airport. states, local state truck reflects location.Consequently, local state corresponds private location partprojected search tree. is, projected search tree whether agentno, one, number private service locations.Another piece private information could existence private packages.is, packages delivered private location another private location. Here,proof work local state truck could different followingunloading (public) package public location, simply variable denotinglocation private package could different values, technically partlocal state (because truck influence value). fact, situationsimilar case (of private service stations) packages unloadedprivate service stations. Again, local states package locatedstation would appear projected search tree. However, cases, external agentscannot infer larger set private states stems fact privatepackage private service station. may well denote something else, e.g., whetherdriver hungry thirsty sleepy, etc. aware existencelarger domain private states.Similar techniques used show inner processes used manufacturers laptop example visible manufacturers: muchlike different locations truck.discussion makes clear distributed forward search able provide important privacy guarantees. Future work seek go beyond domain-dependentreasoning provide general, domain independent conditions strong privacyguarantees obtained. believe doable ideasgood starting point.context, useful keep mind following observations projectedsub-tree ids: 1. practical search algorithm explore entire search tree,extent search space explored decreases, difficulty identifying identical localstates increases. empirical results indicate many problems solved quicklyusing distributed forward search, without expanding many nodes. questionablewhether possible build reasonable models agents private states cases.325fiNissim & BrafmanHere, empirical theoretical work similar conducted distributed CSPs,desirable. 2. clear whether possible identify two states share similarlocal states among agents, especially given small portion searchtree. 3. number macros consisting sequence private actions followedprivate one exponential. Thus, reasonable search process, small portionvisible. anecdotal evidence note actually attempted constructmacros order improve heuristic computation, number exploded quickly,became impractical.Another interesting alternative develop variants current algorithmsstronger privacy preserving properties. example, consider problem inferringupper bounds minimal cost applying action public state s. general, privateactions achieve preconditions public action applied immediatelypublic action agent perform private actions requiredpublic action previous public action. words, agent distributeprivate cost public action different segments, parts plantwo public actions, making cost first action appear higher costsecond action lower, although potential impact optimality. casenon-optimal search, g-values disclosed, issue.example illustrates general idea: one trade-off efficiency privacy.similar tradeoff explored area differential privacy (Dwork, 2006). There,noise inserted database statistical queries evaluated,answer statistical query correct within given tolerance, , yet one cannotinfer information particular entry database (e.g., describing medicalrecord individual). Similarly, context, one consider algorithmsagents refrain sending certain public states probability, sendrandom delay, even possibly, generate bogus, intermediate public states. changeslikely impact running time solution quality, tradeoffswould interesting explore.believe area matures, much like area DisCSP, attentiongiven problem precise quantification privacy privacy loss.work brings us closer stage. offers algorithms distributed search startmatch centralized search, general methodology distributed forward searchrespects natural distributed structure system, form basisextensions, initial ideas formal privacy proofs work.additional privacy-related question specification privacy properties.ma-strips distinction private public variables derived certain waydomain definition. Recent work builds ma-strips suggests slightlydifferent treatment privacy. example, Bonisoli et al. (2014) allow finer notionsprivacy, variables private subset agents, rather singleagent. Moreover, privacy requirements part problem description,variable would private ma-strips could made public privacyimportant maintain. extends earlier models, set variables visibleagent explicitly stated (Torreno, Onaindia, & Sapena, 2014), setprivate variables agent derived, ma-strips rather specified (Luis& Borrajo, 2014). One might also consider distinguishing public variables326fiDistributed Heuristic Forward Searchwrite-only public variables. example, agent may know value certainvariable, except immediately assigns value. would interesting exploreability algorithms, well others, exploit notions privacy.9.2 Accurate Heuristics Given Incomplete Informationempirical results presented lead us perhaps greatest practical challengesuggested mafs mad-a* computing accurate heuristic distributed (privacypreserving) system. Despite theoretical results saying even almost perfectheuristics lead large (exponential) search spaces (Helmert & Roger, 2008), practically,accurate heuristic large effect search efficiency. domains,existence private information shared leads serious deteriorationquality heuristic function, greatly increasing number nodes expanded, and/oraffecting solution quality. believe techniques used alleviateproblem. simple example, consider public action apub appliedprivate action apriv . example, Rovers domain, send messageapplied various private actions required collect data executed.cost apub known agents would reflect cost apriv well, heuristicestimates would accurate. Another possibility improving heuristic estimatesusing additive heuristic. case, rather taking maximum agentsheuristic estimate estimate sending agent, two could added.maintain admissibility, would require using something like cost partitioning (Katz &Domshlak, 2008). One obvious way would give agent full costactions zero cost actions. problem approach initially,state generated estimate available generating agent,estimate inaccurate, since assigns 0 actions. fact, agentinclined prefer actions performed agents, appear cheap,see especially poor results domains different agents achieve goal,Rovers domain, resulting estimates 0 many non-goal states. Therefore,effectively compute accurate heuristics distributed setting importantresearch challenge.One first attempts address issue recent paper Maliah et al. (2014).Building ma-strips model mafs approach discussed here, suggestsmechanism propagating information landmarks among agents without revealingprivate information. Using approach agent detect landmarksdetected projected problems, resulting informative heuristic agent.10. Summarypresented formulation heuristic forward search classical planning respects natural distributed structure system, preserving agent privacy. mafsdominates state-of-the-art distributed planners w.r.t. runtime communication,well solution quality cases. class privacy-preserving algorithms, mad-a*first cost-optimal distributed planning algorithm, competitive centralized counterpart, despite partial information. studied strengthsweaknesses methods, providing empirical evidence claims. algorithms327fiNissim & Brafmanshown scalable, solving problems 40 agents high efficiency, especially loosely-coupled domains. distributed planning framework MA-FD, providesresearchers good starting point future research new algorithms heuristicsprivacy-preserving setting.many interesting directions future research. Recently, work presentedformed basis work mechanism design privacy-preserving planning(Nissim & Brafman, 2013). Specifically, mad-a* used underlying cost-optimalplanner distributed implementation Vickerey-Clarke-Groves mechanism.results distributed method cost-optimal planning self-interested agentsprivate information. interesting explore methods computingaccurate heuristics, inevitable trade-off privacy, accurate heuristicscommunication complexity. One could also explore modify methods dealextensions ma-strips model (e.g. include joint actions), differentsolution criteria (e.g. makespan). Finally, notion private/public actions refineddistinguish read-write public actions read-only ones. distinction couldeffect search methods heuristics, interesting avenue futureresearch.Acknowledgmentsauthors grateful Associate Editor anonymous referees manyuseful suggestions corrections. work supported part ISF grant 1101/07Lynn William Frankel Center Computer Science.ReferencesAlbore, A., Palacios, H., & Geffner, H. (2009). translation-based approach contingentplanning. Boutilier, C. (Ed.), IJCAI, pp. 16231628.Albore, A., Palacios, H., & Geffner, H. (2010). Compiling uncertainty away nondeterministic conformant planning. ECAI, pp. 465470.Alkhazraji, Y., Wehrle, M., Mattmuller, R., & Helmert, M. (2012). stubborn set algorithmoptimal planning. ECAI, pp. 891892.Amir, E., & Engelhardt, B. (2003). Factored planning. IJCAI, pp. 929935.Backstrom, C. (1998). Computational aspects reordering plans. J. Artif. Intell. Res.(JAIR), 9, 99137.Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexitydecentralized control Markov Decision Processes. Math. Oper. Res., 27 (4), 819840.Bonisoli, A., Gerevini, A., Saetti, A., & Serina, I. (2014). privacy-preserving modelmulti-agent propositional planning problem. ICAPS14 Workshop DistributedMulti-Agent Planning.Brafman, R. I., & Domshlak, C. (2006). Factored planning: How, when, not.AAAI.328fiDistributed Heuristic Forward SearchBrafman, R. I., & Domshlak, C. (2008). one many: Planning loosely coupledmulti-agent systems. ICAPS, pp. 2835.Brafman, R. I., Domshlak, C., Engel, Y., & Tennenholtz, M. (2009). Planning games.IJCAI, pp. 7378.Brafman, R. I., Domshlak, C., Engel, Y., & Tennenholtz, M. (2010). Transferable utilityplanning games. AAAI.Burns, E., Lemons, S., Ruml, W., & Zhou, R. (2010). Best-first heuristic search multicoremachines. J. Artif. Intell. Res. (JAIR), 39, 689743.Chandy, K. M., & Lamport, L. (1985). Distributed snapshots: Determining global statesdistributed systems. ACM Trans. Comput. Syst., 3 (1), 6375.Chen, Y., & Yao, G. (2009). Completeness optimality preserving reduction planning.IJCAI, pp. 16591664.Clarke, E. M., Biere, A., Raimi, R., & Zhu, Y. (2001). Bounded model checking usingsatisfiability solving. Formal Methods System Design, 19 (1), 734.Coles, A. J., & Coles, A. (2010). Completeness-preserving pruning optimal planning.ECAI, pp. 965966.Conry, S. E., Kuwabara, K., Lesser, V. R., & Meyer, R. A. (1991). Multistage negotiationdistributed constraint satisfaction. IEEE Transactions Systems, Man,Cybernetics, 21 (6), 14621477.Cox, J. S., & Durfee, E. H. (2005). efficient algorithm multiagent plan coordination.AAMAS, pp. 828835. ACM.Cox, J. S., & Durfee, E. H. (2009). Efficient distributable methods solvingmultiagent plan coordination problem. Multiagent Grid Systems, 5 (4), 373408.Dwork, C. (2006). Differential privacy. ICALP (2), pp. 112.Ephrati, E., & Rosenschein, J. S. (1994). Divide conquer multi-agent planning.AAAI, pp. 375380.Ephrati, E., & Rosenschein, J. S. (1997). heuristic technique multi-agent planning.Ann. Math. Artif. Intell., 20 (1-4), 1367.Fabre, E., & Jezequel, L. (2009). Distributed optimal planning: approach weightedautomata calculus. CDC, pp. 211216. IEEE.Fabre, E., Jezequel, L., Haslum, P., & Thiebaux, S. (2010). Cost-optimal factored planning:Promises pitfalls. ICAPS, pp. 6572.Franzin, M. S., Rossi, F., Freuder, E. C., & Wallace, R. J. (2004). Multi-agent constraintsystems preferences: Efficiency, solution quality, privacy loss. ComputationalIntelligence, 20 (2), 264286.Greenstadt, R., Grosz, B. J., & Smith, M. D. (2007). SSDPOP: improving privacyDCOP secret sharing. AAMAS, p. 171.Greenstadt, R., Pearce, J. P., & Tambe, M. (2006). Analysis privacy loss distributedconstraint optimization. AAAI, pp. 647653.329fiNissim & BrafmanGrinshpoun, T., & Tassa, T. (2014). privacy-preserving algorithm distributed constraint optimization. AAMAS14.Gupta, D., Segal, A., Panda, A., Segev, G., Schapira, M., Feigenbaum, J., Rexford, J., &Shenker, S. (2012). new approach interdomain routing based secure multiparty computation. HotNets, pp. 3742.Hansen, E. A., & Zilberstein, S. (2001). LAO* : heuristic search algorithm findssolutions loops. Artif. Intell., 129 (1-2), 3562.Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. AIPS, pp.140149.Helmert, M. (2006). fast downward planning system. J. Artif. Intell. Res. (JAIR), 26,191246.Helmert, M., & Geffner, H. (2008). Unifying causal graph additive heuristics.ICAPS, pp. 140147.Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics optimalsequential planning. ICAPS, pp. 176183.Helmert, M., & Roger, G. (2008). good almost perfect?. AAAI, pp. 944949.Hoffmann, J., & Nebel, B. (2001). FF planning system: fast plan generationheuristic search. J. Artif. Int. Res., 14 (1), 253302.Holte, R. C. (2010). Common misconceptions concerning heuristic search. SOCS.ICAPS.international planning competition.ipc2011-deterministic/.http://www.plg.inf.uc3m.es/Katz, M., & Domshlak, C. (2008). Optimal additive composition abstraction-basedadmissible heuristics. ICAPS, pp. 174181.Kautz, H. A., & Selman, B. (1992). Planning satisfiability. ECAI, pp. 359363.Keyder, E., & Geffner, H. (2009). Soft goals compiled away. J. Artif. Intell. Res.(JAIR), 36, 547556.Kishimoto, A., Fukunaga, A. S., & Botea, A. (2009). Scalable, parallel best-first searchoptimal sequential planning. ICAPS.Leaute, T., & Faltings, B. (2009). Privacy-preserving multi-agent constraint satisfaction.CSE (3), pp. 1725.Luis, N., & Borrajo, D. (2014). Plan merging reuse multi-agent planning. ICAPS14Workshop Distributed Multi-Agent Planning.Maheswaran, R. T., Pearce, J. P., Bowring, E., Varakantham, P., & Tambe, M. (2006).Privacy loss distributed constraint reasoning: quantitative framework analysisapplications. Autonomous Agents Multi-Agent Systems, 13 (1), 2760.Maliah, S., Shani, G., & Stern, R. (2014). Privacy preserving landmark detection.ECAI14.Meisels, A. (2007). Distributed Search Constrained Agents: Algorithms, Performance,Communication (Advanced Information Knowledge Processing). Springer.330fiDistributed Heuristic Forward SearchMero, L. (1984). heuristic search algorithm modifiable estimate. Artif. Intell., 23 (1),1327.Nair, R., Tambe, M., Yokoo, M., Pynadath, D., & Marsella, S. (2003). Taming decentralizedpomdps: Towards efficient policy computation multiagent settings. IJCAI.Nissim, R., Apsel, U., & Brafman, R. I. (2012). Tunneling decomposition-based statereduction optimal planning. ECAI, pp. 624629.Nissim, R., & Brafman, R. I. (2013). Cost-optimal planning self-interested agents.AAAI.Nissim, R., Brafman, R. I., & Domshlak, C. (2010). general, fully distributed multi-agentplanning algorithm. AAMAS, pp. 13231330.Nissim, R., Hoffmann, J., & Helmert, M. (2011). Computing perfect heuristics polynomialtime: bisimulation merge-and-shrink abstraction optimal planning.IJCAI, pp. 19831990.Oliehoek, F. A., Witwicki, S. J., & Kaelbling, L. P. (2012). Influence-based abstractionmultiagent systems. AAAI.Palacios, H., & Geffner, H. (2009). Compiling uncertainty away conformant planningproblems bounded width. J. Artif. Intell. Res. (JAIR), 35, 623675.Petcu, A., & Faltings, B. (2005). scalable method multiagent constraint optimization.IJCAI, pp. 266271.Petcu, A., Faltings, B., & Parkes, D. C. (2008). M-DPOP: Faithful distributed implementation efficient social choice problems. J. Artif. Intell. Res. (JAIR), 32, 705755.Ranjit, N., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributedpomdps: synthesis distributed constraint optimization pomdps. AAAI.Silaghi, M.-C., & Mitra, D. (2004). Distributed constraint satisfaction optimizationprivacy enforcement. IAT, pp. 531535.Srivastava, S., Immerman, N., Zilberstein, S., & Zhang, T. (2011). Directed searchgeneralized plans using classical planners. ICAPS.Steenhuisen, J. R., Witteveen, C., ter Mors, A., & Valk, J. (2006). Framework complexity results coordinating non-cooperative planning agents. MATES, pp. 98109.Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithmsolving decentralized POMDPs. UAI, pp. 576590.Taig, R., & Brafman, R. I. (2013). Compiling conformant probabilistic planning problemsclassical planning. ICAPS.ter Mors, A., Valk, J., & Witteveen, C. (2004). Coordinating autonomous planners.IC-AI, pp. 795.ter Mors, A., & Witteveen, C. (2005). Coordinating self interested autonomous planningagents. BNAIC, pp. 383384.Torreno, A., Onaindia, E., & Sapena, O. (2012). approach multi-agent planningincomplete information. ECAI, pp. 762767.331fiNissim & BrafmanTorreno, A., Onaindia, E., & Sapena, O. (2014). Fmap: Distributed cooperative multi-agentplanning. Applied Intelligence, 41 (2), 606626.Valmari, A. (1989). Stubborn sets reduced state space generation. ApplicationsTheory Petri Nets, pp. 491515.Vrakas, D., Refanidis, I., & Vlahavas, I. P. (2001). Parallel planning via distributionoperators. J. Exp. Theor. Artif. Intell., 13 (3), 211226.Wehrle, M., & Helmert, M. (2012). partial order reduction planning computeraided verification. ICAPS.Wehrle, M., Helmert, M., Alkhazraji, Y., & Mattmuller, R. (2013). relative pruningpower strong stubborn sets expansion core. ICAPS.Witwicki, S. J., & Durfee, E. H. (2010). Influence-based policy abstraction weaklycoupled dec-pomdps. ICAPS, pp. 185192.Witwicki, S. J., Oliehoek, F. A., & Kaelbling, L. P. (2012). Heuristic search multiagentinfluence space. AAMAS, pp. 973980.Xu, Y., Chen, Y., Lu, Q., & Huang, R. (2011). Theory algorithms partial orderbased reduction planning. CoRR, abs/1106.5427.Yang, Q., Wu, K., & Jiang, Y. (2007). Learning action models plan examples usingweighted MAX-SAT. Artif. Intell., 171 (2-3), 107143.Yao, A. C.-C. (1982). Protocols secure computations (extended abstract). FOCS, pp.160164.Yao, A. C.-C. (1986). generate exchange secrets (extended abstract). FOCS,pp. 162167.Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1998). distributed constraintsatisfaction problem: Formalization algorithms. IEEE Trans. Knowl. Data Eng.,10 (5), 673685.Yokoo, M., Suzuki, K., & Hirayama, K. (2002). Secure distributed constraint satisfaction:Reaching agreement without revealing private information. CP, pp. 387401.Yoon, S. W., Fern, A., & Givan, R. (2007). FF-Replan: baseline probabilistic planning.ICAPS, pp. 352.332fiJournal Artificial Intelligence Research 51 (2014) 165-205Submitted 05/14; published 09/14Simple Regret Optimization Online PlanningMarkov Decision ProcessesZohar FeldmanCarmel Domshlakzoharf@tx.technion.ac.ildcarmel@ie.technion.ac.ilFaculty Industrial Engineering & Management,Technion - Israel Institute Technology,Haifa, IsraelAbstractconsider online planning Markov decision processes (MDPs). online planning,agent focuses current state only, deliberates set possible policiesstate onwards and, interrupted, uses outcome exploratory deliberationchoose action perform next. Formally, performance algorithms onlineplanning assessed terms simple regret, agents expected performance losschosen action, rather optimal one, followed.date, state-of-the-art algorithms online planning general MDPs eitherbest effort, guarantee polynomial-rate reduction simple regret time.introduce new Monte-Carlo tree search algorithm, BRUE, guarantees exponentialrate smooth reduction simple regret. high level, BRUE based simpleyet non-standard state-space sampling scheme, MCTS2e, different partssample dedicated different exploratory objectives. extend BRUEvariant learning forgetting. resulting parametrized algorithm, BRUE(),exhibits even attractive formal guarantees BRUE. empirical evaluationshows BRUE generalization, BRUE(), also effective practicecompare favorably state-of-the-art.1. IntroductionMarkov decision processes (MDPs) offer general framework sequential decisionmaking uncertainty (Puterman, 1994). MDP hS, A, r, Ri defined setpossible agent states S, set agent actions A, stochastic transition function r :SAS [0, 1] defined set |S||A| conditional probability functions P(S | s, a),reward function R : R. current state agent fully observable.agent performs action state s, state changes s0 probabilityP(s0 | s, a), agent collects reward R(s, a, s0 ). finite horizon setting,reward accumulated predefined number steps H.objective agent act maximize accumulated reward,decision problem always action perform next. state s, h steps go,(possibly stochastic) action policy prescribes action taken situation.policy called optimal if, expectation, following guarantees maximizationaccumulated reward. key property MDP model that, MDP,deterministic optimal policy : {1, . . . , H} (Bellman, 1957).c2014AI Access Foundation. rights reserved.fiFeldman & DomshlakEfficiency finding optimal policies MDPs primary focus computational research around model. state space MDP largeallowed planning time, reasoning MDP narrowed state space regionconsidered relevant specific decision problem currently faced agent.particular, algorithms online reasoning MDPs focus current states0 agent, deliberate set possible courses action s0 onwards, and,interrupted, use outcome exploratory deliberation, planning, issueinstant recommendation action perform s0 . action appliedreal environment, planning process repeated obtained state selectnext action on.Depending problem domain representation language, concise descriptionslarge-scale MDPs either declarative generative (or mixed). declarative representations, transition reward functions described explicitly, generative models, given black box simulator. palette algorithmsfinding good actions concisely represented MDPs already rather wide (Boutilier,Dean, & Hanks, 1999; Guestrin, Koller, Parr, & Venkataraman, 2003; Kolobov, Mausam,& Weld, 2012; Busoniu & Munos, 2012; Bonet & Geffner, 2012; Keller & Helmert, 2013;Mausam & Kolobov, 2012; Geffner & Bonet, 2013), algorithms applicabledeclaratively represented MDPs. One earliest best-known online planning algorithms developed generative MDP models sparse sampling algorithmKearns, Mansour, Ng (2002). Sparse sampling offers near-optimal action selectiondiscounted MDPs constructing sampled lookahead tree time exponentialdiscount factor sub-optimality bound, independent state space size. However, terminated action proven near-optimal, sparse sampling offersquality guarantees action selection.last decade, Monte-Carlo tree search (MCTS) algorithms (Browne, Powley, Whitehouse, Lucas, Cowling, Rohlfshagen, Tavener, Perez, Samothrakis, & Colton, 2012) becameextremely popular online planning MDPs, well online planning manysettings sequential decision making, including partial state observabilityadversarial effects (Gelly & Silver, 2011; Sturtevant, 2008; Bjarnason, Fern, & Tadepalli,2009; Balla & Fern, 2009; Eyerich, Keller, & Helmert, 2010; Browne et al., 2012).capability dealing generative problem representations featureMCTS made methods popular. First, MCTS algorithms natively exploit problem-specific heuristic functions, correctness independent heuristicsproperties, well applied without heuristic information whatsoever.Second, numerous MCTS algorithms exhibit strong anytimeness: meaningfulaction recommendation provided interruption point instantly, time O(1),quality recommendation also improved smoothly, time stepsindependent size explored state space.Formally, denoting shhi state h steps-to-go, quality action a,recommended s0 hHi, assessed terms choice-error probability, is,probability sub-optimal, terms (closely related) measure simpleregret [shhi, a]. latter captures performance loss results takingfollowing optimal policy remaining h 1 steps, instead following166fiSimple Regret Optimization Online Planning MDPsbeginning (Bubeck & Munos, 2010).1 is,[shhi, a] = Q(shhi, (shhi)) Q(shhi, a),(Es0 [R(s, a, s0 ) + Q (s0 hh1i, (s0 hh1i))] ,Q(shhi, a) =0,h > 0,.h=0Numerous MCTS algorithms, particular, popular UCT (Kocsis & Szepesvari,2006) algorithm variants (Coquelin & Munos, 2007; Tolpin & Shimony, 2012),guarantee eventual convergence optimal choice action, providing smoothreduction choice-error probability simple regret planning time. relativeempirical attractiveness various MCTS planning algorithms depends specificsproblem hand usually cannot predicted ahead time. However,comes formal guarantees expected performance improvement planningtime, none online MCTS algorithms MDPs breaks barrier worst-casepolynomial-rate reduction simple regret choice-error probability time.precisely contribution here. work motivated recentlygrowing understanding current MCTS algorithms MDPs optimizereduction simple regret directly, via optimizing called cumulative regret,performance measure suitable (very different) setting reinforcement learningacting (Bubeck & Munos, 2010; Busoniu & Munos, 2012; Tolpin & Shimony, 2012;Feldman & Domshlak, 2012).Departing high-level realization, discuss certain pitfalls simple regretminimization via Monte-Carlo sampling, identify two, somewhat competing, exploratory objectives pursued sampling mechanism.suggest principle separation concerns, whereby different parts statespace sample devoted different exploration objectives.introduce MCTS2e, novel sampling scheme specializes MCTS implements principle separation concerns. main result introductionanalysis BRUE, concrete instance MCTS2e guarantees smoothexponential-rate reduction simple regret choice-error probabilitytime, general MDPs finite state spaces. fact, showqualitatively similar guarantees satisfied broad class call purelyexploring MCTS2e algorithms, BRUE simple yet efficient instanceclass.Finally, discuss analyze prospects learning forgetting, principle according old samples degraded newer (and higher-quality) samples gathered. Generalizing BRUE extending ingredient forms1. may appear reader intuitive consider loss results applying recommendations instead H steps, first one. However, Kearns et al.(2002) show Lemma 5, two measures closely related, alternative measuredirectly simple regret along execution horizon.167fiFeldman & Domshlakparametrized algorithm BRUE(), parameter controlling level forgetfulness. show BRUE() exhibits even attractive formal guaranteesexhibited BRUE.rest paper structured follows. Section 2.2 provide backgroundMonte-Carlo tree search, particular, UCT algorithm. Then, Section 3,discuss simple regret minimization MDPs via multi-armed bandits perspective,Section 4, introduce principle separation concerns establishmain algorithmic constructs along corresponding computational results. Section 5devoted learning forgetting MCTS, particular, algorithmBRUE(). Section 6 discuss findings empirical evaluation. proofsformal claims relegated Appendix B, three subsections contain,respectively, proofs three key theorems. completeness, Appendix providesstandard concentration inequalities use paper.2. BackgroundHenceforth, A(s) denotes actions applicable state s, operation drawingsample distribution set denoted D[], U denotes uniform distribution, JnK n N denotes set {1, . . . , n}. sequence tuples , [i] denotesi-th tuple along , [i].x denotes value field x tuple. considering MDP hS, A, r, Ri, K denotes state branching factor (maximal numberactions per state), B denotes action branching factor (maximal number outcomes peraction), = mina6= (s0 ,H) [s0 hHi, a] denotes minimal possible non-zero simpleregret root.2.1 Sparse SamplingOne earliest best-known online planning algorithms developed generativeMDP models sparse sampling (SS) algorithm Kearns et al. (2002). SSoriginally developed infinite-horizon discounted MDPs, reformulation finitehorizon MDPs straightforward follows.action A(s0 ), SS estimates value Q(s0 hHi, a) averaging C recursive samples outcome states. outcome states s0 sampled generative model transition function r(s0 , a), value sample setR(s0 , a, s0 ) + maxa0 Q(s0 hH 1i, a0 ), Q-values actions a0 A(s0 ) estimated recursively way, hitting depth H. number outcome samplesC set guarantee quality recommendation issued upon terminationalgorithm meets desired level accuracy. Alternatively, given C, formalanalysis used derive corresponding accuracy guarantee. Equivalent boundssimple regret follows.Proposition 2.1.1 Let SS called state s0 MDP hS, A, r, Ri rewards[0, 1] finite horizon H. Then, simple regret action SS (s0 hHi), recommendedSS parameter C > 0, boundedE[s0 hHi, SS (s0 hHi)] H(K min(B, C))H e1682 CH4.fiSimple Regret Optimization Online Planning MDPsproof Proposition 2.1.1 given Appendix C, p. 200. bound Proposition 2.1.1 suggests formal guarantees SS become meaningfulH 5 log(K min{B, C}).2Assuming B < C, implies bound Proposition 2.1.1 becomes non-trivialoverall number SS calls generative model2 H 5 log(BK)(BK)H .(1)C>Notably, SS strong anytime algorithm, called contract algorithm (Zilberstein, 1993): termination SS parametrized C, interrupting SSnormal termination results meaningful action recommendation. However, knowingoverall number allowed calls generative model principle enableknowledgeable allocation deliberation efforts (Hay, Shimony, Tolpin, & Russell, 2012).Hence, general, deliverables contract algorithms expected betterdeliverables the, de facto similarly budgeted, strong anytime algorithms (Zilberstein,1993). Therefore, bound (1) sets good reference understanding significanceformal guarantees provided strong anytime algorithms online MDP planning.2.2 Monte-Carlo Tree Search UCTMCTS, canonical scheme Monte-Carlo tree search gives rise various specificalgorithms online MDP planning, depicted Figure 1, left. MCTS exploresstate space radius H steps initial state s0 iteratively rollingstate-space samples s0 . rollout comprises sequence simulated stepshs, a, s0 , ri state, action applicable s, s0 state resultingapplying s, r corresponding immediate reward. particular, [0].s = s0[t].s0 = [t+1].s t.generated rollout used update variables interest associatedstates visited actions applied therein. variables typically include leastbaction value estimators Q(shhi,a), well counters n(shhi, a) keep numberb (shhi, a) updated. rollout-orientedtimes corresponding estimators Qexploration MCTS allows information states deeper levels propagatedroot s0 hHi low-complexity iterations O(H). allows smooth improvementintermediate quality recommendation, probably one main reasonsMCTS seems particularly appealing context online planning.Instances MCTS vary mostly along different implementation strategiesStopRollout, specifying stop rollout;RolloutAction, prescribing action apply current state rollout;Update, specifying rollout expand tree update maintainedvariables stored nodes constructed search tree.22. Due Markovian nature MDPs, unreasonable distinguish nodes associatedstate depth. Hence, actual graph constructed instances MCTSforms directed acyclic graph nodes shhi {0, 1, . . . , H}.169fiFeldman & DomshlakMCTS: [input: hS, A, r, Ri; s0 S]procedure Update()r 0||, . . . , 1hH[d].an(shhi) n(shhi) + 1n(shhi, a) n(shhi, a) + 1r r + [d] .rMC-backup(shhi, a, r)time permitsRollout // generate rolloutUpdate()b 0 hHi, a)return arg maxa Q(sprocedure Rollouthis0d0StopRollout()hHRolloutAction(shhi)s0 RolloutOutcome(shhi, a)r R (s, a, s0 )[t] hs, a, r, s0s0 ; + 1returnprocedure MC-backup(shhi, a, r)1bbQ(shhi,a) n(shhi,a)1Q(shhi,a) + n(shhi,a)rn(shhi,a)procedure StopRollout()||return = H A([d].s0 ) =procedure RolloutAction(shhi) // UCB: n (shhi, a) = 0returnqhn(shhi)breturn argmaxa Q(shhi,a) + c logn(shhi,a)procedure RolloutOutcome(shhi, a)return s0 P(S | s, a)Figure 1: template MCTS algorithms (left) , UCT algorithm specific setsub-routines MCTS (right).interrupted, MCTS uses information collected throughout exploration recommend action perform state s0 .Numerous concrete instances MCTS proposed, UCT algorithm(Kocsis & Szepesvari, 2006) modifications (Coquelin & Munos, 2007; Tolpin &Shimony, 2011) popular instances days (Gelly & Silver, 2011;Sturtevant, 2008; Bjarnason et al., 2009; Balla & Fern, 2009; Eyerich et al., 2010; Keller &Eyerich, 2012). specification UCT algorithm instance MCTS depictedFigure 1, right.Different versions UCT use different rules end rollout. version depictedhere, rollouts end terminal nodes, is, either depth H statesapplicable actions.3RolloutAction policy UCT based deterministic decision ruleUCB1 (Auer, Cesa-Bianchi, & Fischer, 2002), originally proposed optimal balanceexploration exploitation cumulative regret minimization stochastic3. popular version UCT, search tree grown incrementally, ending rollouts whenevernew node encountered. However, point extraneous exposition paper.170fiSimple Regret Optimization Online Planning MDPsmulti-armed bandit (MAB) problems (Robbins, 1952). node shhi, next-onthe-sample action selected follows: actions applicablesampled shhi, is, n(shhi, a) > 0 A(s), selectedaction corresponds"#logn(shhi)bargmax Q(shhi,a) + c,(2)n(shhi, a)c > 0 fixed parameter balances first, exploitation-oriented,second, exploration-oriented, summands Eq. 2. Otherwise, selecteduniformly random still unexplored actions {a A(s) | n(shhi, a) = 0}.cases, procedure sample-outcome UCT samples next staterollout according transition probability P(S | s, a).bUCT updates value estimators Q(shhi,a) (shhi, a) pairs encounteredalong rollouts. updates done via MC-backup procedure,averages accumulated rewards rollouts shhi terminal states.terms formal properties, UCT online algorithm that, certain pointtime, provides smooth reduction simple regret time zero, is, smooth convergence optimal action choice s0 hHi (Kocsis & Szepesvari, 2006). Two aspectsconvergence interest: (1) length transition period reduction simple regret guaranteed all, (2) reduction rate simple regrettime, transition period over. Considering (1), Coquelin Munos (2007)showed number samples bounds UCT simple regret becomemeaningful might high hyper-exponential H. Considering (2), Theorem 6work Kocsis Szepesvari (2006) claims polynomial-rate reduction probabilitychoosing non-optimal action, implies simple regret4 .attempts recently made improve UCT, online MCTS-based planning general, terms two aspects convergence (Tolpin & Shimony, 2012; Hayet al., 2012; Coquelin & Munos, 2007). reported empirical results promising, none suggested MCTS instances breaks UCTs barrier worst-casepolynomial-rate reduction simple regret time. Hence, question whetheronline, smoothly converging MCTS algorithm substantially outperform UCT termstwo convergence parameters remained open. comes next, answerquestion affirmatively.3. Simple Regret Minimization MDPshigh level, key property UCT exploration search space obtainedconsidering hierarchy forecasters (s, h), minimizing cumulative regret,is, loss total reward incurred exploring environment (Auer et al.,2002). respect, according Theorem 6 work Kocsis Szepesvari (2006),UCT asymptotically achieves best possible (logarithmic) cumulative regret. However,recently pointed numerous works (Bubeck & Munos, 2010; Busoniu & Munos, 2012;4. Notably, claims made nontrivial assumptions171fiFeldman & DomshlakTolpin & Shimony, 2012; Feldman & Domshlak, 2012), cumulative regret seemright objective online MDP planning, rewards collectedsimulation phase fictitious. Furthermore, work Bubeck, Munos, Stoltz(2011) multi-armed bandits shows minimizing cumulative simpleregret somewhat competing objectives, sense minimal simple regretincrease bound cumulative regret decreases.relationship simple cumulative regret minimization MABs suggestsfocusing online MDP planning directly simple regret minimization may lead algorithms are, worst-case and/or empirically, substantially effective UCT.fact, context MABs, Bubeck et al. (2011) already showed simple round-robinsampling MAB actions, followed recommending action highest empiricalmean, yields exponential-rate reduction simple regret, UCB1 sampling strategybalances exploration exploitation yields polynomial-rate reductionmeasure. respect, situation MDPs seemingly different. fact,although designed slightly different setup, sparse sampling algorithm providesevidence theoretical merits focused solely exploration online planningMDPs.appears, however, answer question one focus exploration, preserving onlineness smoothness convergence, less straightforward general MDPs special case MABs. motivatediscuss various exploratory concerns online Monte-Carlo planning MDPs,separation concerns possibly buy us, begin MAB perspective MDPs, shows smooth exponential-rate reduction simple regretMDPs indeed achievable, least theoretically.3.1 Multi-armed Bandit Perspective MDPsLet s0 state MDP hS, A, r, Ri rewards [0, 1], finite horizon H.principle, general MDP viewed MAB, arm MABcorresponding flat policy acting H steps starting current state s0 .flat policy minimal partial mapping state/steps-to-go pairs actionsfully specifies acting strategy MDP H steps, starting s0 . Samplingarm straightforward prescribes precisely action applied everystate possibly encountered along execution . rewardarm stochastic, withPsupport [0, H], expected value . number armsH1Hschematic MAB K i=0 B K B . Now, consider simple algorithm, NaiveUniform,systematically samples flat policy loop, uses obtained rewardupdate empirical meanb corresponding policy arm . stopped iterationn, algorithm recommends policy arm n best empirical valuebn .iteration n algorithm, arm sampled least b Bn H c times. Therefore,Kusing Hoeffdings tail inequality5 , probability chosen arm policy n sub-5. completeness, Hoeffdings tail inequality provided Appendix A, pp. 188.172fiSimple Regret Optimization Online Planning MDPsoptimal MAB upper-boundedX6=P {b>b } =Xb6=P {bb ( ) } KBHen c2HKB2H 2,(3)= = min6= . Denoting simple regret n rn ,expected simple regret therefore boundedbErn HKBHen c2HKB2H 2.(4)Note NaiveUniform uses rollout = hs0 hHi, a1 , s1 hH 1i, . . . , aH , sH h0iiupdate estimation single policy . However, recalling arms MABproblem actually compound policies, sample principle used updateestimates policies 0 consistent sense that, 0 H 1,0 (si hH ii) defined 0 (si hH ii) = ai+1 . resulting algorithm, CraftyUniform,generates samples choosing actions along sample uniformly random, usesoutcome sample update policies consistent it. Notepolicy arms CraftyUniform cannot sampled systematically NaiveUniformset policies updated iteration stochastic.Since sampling uniform, probability policy updated sampleissued iteration CraftyUniform K1H . Let N n denote number samples consistent policy among first n samples issued CraftyUniform.probabilityn , best empirical arm n iterations, sub-optimal boundedPP{bb }>6=P {b>b } Pb2+Pb2.(5)two terms right-hand side bounded as:nnnP N+ P N >,bPb22K H2K H2finX()fifinHN =e 8K +P {N = i} Pb2 fini=en8K Hen8K H()en8K H2e2K H+1fiXnfifin+PbN=+1P {N = i}2 fi2K Hni= H +12Kfififin+Pb,nN =+12 fi2K H+en28K H H 2n24K H H 2,(6)173fiFeldman & Domshlak() () Hoeffdings tail inequality. turn, similarly Eq. 4, simpleregret CraftyUniform boundedHErn 4HK B en28K H H 2.(7)Since H trivial upper-bound Ern , bound Eq. 7 becomes effectiveH4K B en28K H H 2< 1, is,Hn > (KB) 4H2log K.(8)Note cold start transition period much shorter UCT,hyper-exponential H. time, unlike UCT, rate simple regretreduction exponential number iterations. terms oracle calls, lengthtransition period CraftyUniform2 H 3 log(K) (KB)H .Likewise, comparing sparse sampling (Eq. 1), appears transition periodCraftyUniform smaller dependency H (H 3 vs. H 5 ), smaller dependencyB (log(K) vs. log(BK)).sum, CraftyUniform seen theoretical feasibility test agenda:algorithm uses Monte-Carlo sampling averaging updates, strong anytime (actionrecommendation issued instantly, time, expected qualityrecommendation improves every state-space sample), simple regret decreasesexponential rate time. Moreover, transition period reductionrate guaranteed somewhat shorter (contracted) transition period SS,much shorter transition period UCT. case, however, feasibilityHCraftyUniform conceptual: requires explicit reasoning K B arms, thuscannot efficiently implemented.4. Separation Concerns Online MDP Planningshow practical algorithm achieves smooth, exponential-rate reductionsimple regret online MDP planning. so, first motivate introduce principleseparation concerns, whereby different parts state-space sample devoteddifferent aspects problem exploration. introduce MCTS2e, specializedMCTS sampling scheme implements principle separation concerns viatwo-phase scheme generating state-space samples. Using MCTS2e basis,describe concrete algorithm, BRUE, achieves exponential-rate, smooth reductionsimple regret time, transition period comparable schematicCraftyUniform non-interruptible SS. fact, show formal guaranteessatisfied entire class call purely exploring MCTS2e algorithms, oneBRUE.tried achieve smooth, exponential-rate convergence merely replacingUCB1 policy UCT pure exploration policy uniform action selection,174fiSimple Regret Optimization Online Planning MDPswould failed miserably. fact, naive attempt would result algorithmeven converge optimal action. reason lies fundamentaldifference MABs MDPs: Unlike MABs, direct sampling actual valueactions impossible requires knowledge optimal policysubsequent states entire look-ahead space. knowledge, however, unavailablebeginning deliberation. Hence, sampling futures, non-root nodeshhi actually serve two objectives:(1) estimating actions ancestor(s) shhi ,(2) identifying optimal action (shhi).objectives exploratory, opposition extent.meet first objective, shhi sample optimal action (shhi) probabilityapproaching 1 number samples grows. meet second objective, however,actions shhi must selected frequently. protocol selecting actionsused, UCT, throughout entire rollout, rewards collected along rolloutused updating value estimations multiple nodes, protocol commitaddressing two objectives simultaneously. instance, UCB1 protocol employedUCT nodes shhi chooses action seems attractive potential,potential stems partially relatively high empirical value (complying objective (1)), partially less frequent sampling action (complyingobjective (2)).However, overloading action selection protocol unavoidablelearning acting setup reinforcement learning, case onlineplanning. sense, two objectives depicted resemble two tasks facedMAB forecasters: objective (1) seen type recommendation, whereas objective(2) viewed exploration. therefore makes perfect sense fulfill twoobjectives different policies, much like exploration recommendation handleddifferent policies MAB online planning (Bubeck et al., 2011). specifically, differentpolicies used choose method node/action pairs updatedmethod values pairs estimated. follows,refer separation exploratory objectives separation concerns, nextelaborate implementation concept online planning MDPs.4.1 Two-Phase Sampling BRUEintroduce novel Monte-Carlo tree search scheme, MCTS2e, tailored towards employing principle separation concerns. MCTS2e depicted Figure 2(a)specification MCTSs Update procedure. core difference MCTS2eimplementation Update UCT samples used update valueestimators. illustrated Figure 3, value estimators UCT updatedaccumulated reward respective tail rollout, whereas MCTS2e estimators updated accumulated reward new sub-rollouts, created Estimateprocedure.Estimate procedure parametrized two policies, namelyEstAction, prescribing action used estimation,175fiFeldman & Domshlakprocedure Update()||, . . . , 1hHhs, a, r, s0 [d]n(shhi) n(shhi) + 1n(shhi, a) n(shhi, a) + 1n(shhi, a, s0 ) n(shhi, a, s0 ) + 1r r + Estimate(s0 hh 1i)MC-backup(shhi, a, r)procedure StopRollout()||return = H A([d].s0 ) =procedure RolloutAction(shhi)return U[A(s)]// uniformprocedure RolloutOutcome(shhi, a)return s0 P(S | s, a)procedure Estimate(shhi)r 00, . . . , h 1EstAction(shh di)s0 EstOutcome(shh di, a)rd+1 R (s, a, s0 )r r + rd+1s0return rprocedure EstAction(shhi) // bestbreturn argmaxaA(s) Q(shhi,a)procedure EstOutcome(shhi, a)s0 : n(shhi, a, s0 ) > 00b = s0 | s, a) n(shhi,a,s )P(Sn(shhi,a)b | s, a)return s0 P(S(a)(b)Figure 2: (a) MCTS2e MCTS specific Update procedure, (b) BRUE algorithm specific set sub-routines MCTS2e (right).EstOutcome, determining next state follow.policies RolloutAction RolloutOutcome (used MCTSs Rolloutprocedure) determine value estimators update, policies EstActionEstOutcome used update estimators.separation allows us introduce BRUE, is, way, exploratoryMCTS2e instance possible.6 BRUE setting MCTS2e depicted Figure 2(b).Similarly UCT, rollouts generated BRUE end terminal nodes, and, throughoutrollout, next state sampled according r. However, unlike UCT, rolloutactions BRUE selected uniformly random applicable actions. turn,estimation sub-rollouts,selected actions empirically best actions, is, actionshighest value estimations,b =next states sampled according empirical transition probabilities P(S00| s, a), is, number times n(shhi, a, s) state followed applying6. Short Best Recommendation Uniform Exploration; name carried firstpresentation algorithm, estimation referred recommendation (Feldman &Domshlak, 2012).176fiSimple Regret Optimization Online Planning MDPsUCTMCTS2eb ([d].s, [d].a)QHXdb ([d].s, [d].a)QHX1rii=1[i].ri=d(a)(b)Figure 3: Illustration value estimator update MCTS2e (a) vs. UCT (b). Circlesrepresent decision nodes, solid lines represent actions taken, squares representchance nodes, dashed arrows represent outcomes resultsubsequent decision nodes.action node shhi, divided overall number times n(shhi, a) actionapplied shhi.7proceed formal analysis BRUE. general, consideringinstance MCTS2e, Tn denote search graph obtained n iterations.sake simplicity, assume uniqueness optimal policy : statenumber h steps-to-go, assume single optimal action, denote (s, h).nodes shhi Tn , nB (shhi) randomized strategy, uniformly choosing among actionsbmaximizing Q(shhi,a). addition problem-specific state branching factor K,minimal non-zero simple regret root = mina6= (s0 ,H) [s0 hHi, a], boundsdepend problem-specific action branching factor B, well horizon H.former two parameters inherited MAB, latter two connect MABgeneral MDP.Theorem 1 Let BRUE called state s0 MDP hS, A, r, Ri rewards[0, 1], finite horizon H. exist pairs parameters a, b > 0, dependent7. Sampling according P(S | s, a) RolloutOutcome also valid choice, although termsformal guarantees, EstOutcome Figure 2 appears attractive.177fiFeldman & Domshlak{K, B, H, }, that, n > H iterations BRUE, simple regret boundedE[s0 hHi, nB (s0 hHi)] Ha ebn ,choice-error probability boundedP nB (s0 hHi) 6= (s0 hHi) ebn .particular, Eq. 9 10 hold = 3Kb=2.9K 2 (196BK)H1 H 21044B 2 K 22H1(9)(10)12(196BK) 2 (H1) (H 1)!2proof Theorem 1 given Appendix B.1, p. 188. length transitionperiod implied Theorem 1 givenBK2 5HH log()(196BK)(11)transition period rather comparable sparse sampling except ratherlarge constant appearing basis exponent Equations 9 10. Althoughconstant imposes significant increase transition period, things notedregards bounds provided BRUE. First foremost, parameter bTheorem 1 reflects worst-case terms transition function r, correspondsuniform distribution, is, P(s0 | s, a) = B1 states actions A(s).However, probability mass action transition functions concentratessmall set outcomes, convergence rate BRUE expected much better.Proposition 4.1.1 formulates BRUEs bounds respect problem-dependent parameter1 Pe B, related entropy transition function definedPe = max kP( | s, a)k 1 .s,a2Proposition 4.1.1 Let BRUE called state s0 MDP hS, A, r, Ri rewards [0, 1], finite horizon H 4. BRUE converges exponential rateH1(H!)2 b =sense Eq. 9 10 Theorem 1 = 3K 172BK22.4H1 H5 29KB (1666K)PeHformal guarantees BRUE therefore even better SS. proofProposition 4.1.1 (given Appendix B.2, p. 195) obtained rather minor modificationproof Theorem 1.Relating tightness bounds, noted size scalarconstants analysis BRUE partially stems attempt avoid cumbersomeexpressions, thus considerably reduced. Furthermore, particular pointanalysis bound error action-value estimations different pointstime, believe bound gets particularly loose. comment issuedetail within proof Theorem 1 (right Proposition B.1.1, p. 190).read far, reader may rightfully ask extent guarantees provided BRUE unique among instances MCTS2e. general, formal properties178fiSimple Regret Optimization Online Planning MDPsMCTS2e instances heavily depend specific sub-routines,even guarantee convergence optimal action. However, BRUE still muchunique deliverables. particular, define family purely exploring MCTS2e algorithms guarantee exponential-rate reduction simple regrettime.8Definition 1 (Purely exploring MCTS2e) instance MCTS2e called purelyexploring if, node shhi reachable s0 , A(s), exist parameters, , , dependent {K, B, H, },P {n(shhi, a) n(shhi)} en(shhi) ,estimation policy EstAction selects empirically best arm.Theorem 2 Let purely exploring instance MCTS2e. convergesexponential rate sense Eq. 9 10 Theorem 1.Appendix B.3, p. 195 show proof Theorem 2 easily derivedproof Theorem 1. Furthermore, analysis provided proof Theorem 1used extract convergence parameters c, c0 purely exploring algorithm,given specific parameters , , .5. Learning Forgetting BRUE()BRUE, well converging instances MCTS MCTS2e, evolutionaction value estimates internal nodes based biased samples stemselection non-optimal actions descendant nodes. bias tends shrinksamples accumulated descendants. Consequently, estimates becomeaccurate, probability selecting optimal action increases accordingly,bias ancestor nodes shrinks turn.interesting question arises context whether samples obtained different stages sampling process weighed differently. high level,intuition suggests biased samples provide us valuable information, especially still have. time, value informationdecreases obtain accurate samples. Hence, principle, putting weightsamples smaller bias could increase accuracy estimates. led usconsider BRUE(), algorithm generalizes BRUE BRUE(1) basing estimatesfraction recent samples.Technically, BRUE() differs BRUE implementation MC-backupprocedure depicted Figure 4. addition variables maintained BRUE,node/action pair (shhi, a) BRUE() associated list L(shhi, a) rewards,collected n(shhi, a) samples responsible current estimatebbQ(shhi,a). (shhi, a) updated MC-backup, value estimator Q(shhi,a)assigned average recent n(shhi, a)e samples, dxe denotes8. We, course, make claims guarantees exclusive purely exploring instancesMCTS2e, even MCTS2e instances general.179fiFeldman & Domshlakprocedure MC-backup(shhi, a, r)n n(shhi, a)en n(shhi, a)L(shhi, a)[n] rPbQ(shhi,a) n1 ni=nn L(shhi, a)[i]Figure 4: BRUE() modified MC-backup proceduresmallest integer greater equal x. Theorem 3 exhibits benefitsadopting < 1 comes convergence guarantees.Theorem 3 Let BRUE() called state s0 MDP hS, A, r, Ri rewards[0, 1] finite horizon H. exist pairs parameters a, b > 0, dependent{K, B, H, , }, that, n > H iterations BRUE (), simple regretboundedE[s, nB (s0 , H), H] Ha ebn ,(12)choice-error probability boundedP nB (s0 , H) 6= (s0 , H) ebn .(13)1particular, depth-dependent h (BK)h1 , Eq. 12 13 holdH12= 3K 12BK(H!)2 b = 9K 2 (196BK)H1 H 2 .2particular choice h Theorem 3, length transition periodBRUE() terms number calls generative modelBKH2 4HH log(196BK).bound BRUE() seems somewhat better BRUE, improvement attributed looseness bound BRUE lessactual improvement performance. proof Theorem 3 (given Appendix B.4,p. 196) offer new technique address bound accuracy action-valueestimations different sampling times, reduces bound considering fewer samples. selection Theorem 3 stems attempt balance much possibletwo sources inaccuracy appearing Propositions B.4.1 B.4.2 proofTheorem 3. smaller is, lower sample inaccuracy originatesinaccuracy estimates successor nodes. time, however,inaccuracy stems basing estimate fewer samples increases. Duebranching, nodes farther toward horizon sampled less frequently thus lessaccurate. worst case, underlying graph Tn tree, node expected1sampled fraction (BK)number samples takensteps higher predecessor. precisely reason selection hTheorem 3.1801(BK)h1fiSimple Regret Optimization Online Planning MDPspractice, however, worst-case considerations tend underrate valuesamples. Since Tn typically tree, ratio number samplesdifferent depths tends higher aforementioned worst-case ratio. Therefore,better adapted according observed ratios rather accordingworst-case ones. Furthermore, since objective behind estimating action valuesidentify optimal action, bias samples may far less influencequality planning outcome dictated formal guarantees. instance,suppose action estimators particular node shhi equal bias.bcase, shhi may home optimal action Q(shhi,a) estimates stillbiased, suffice shhi fulfill role value-estimating sub-rollouts issuedancestor(s). illustrative setup clearly extreme, pointbiased estimators still distinguish better actions worse ones, longbiases across actions correlated.6. Experimental Evaluationevaluated BRUE empirically MDP Sailing domain (Peret & Garcia, 2004),used previous works evaluating MCTS algorithms (Peret & Garcia, 2004; Kocsis &Szepesvari, 2006; Tolpin & Shimony, 2012), well MDP version random gametrees used original empirical evaluation UCT (Kocsis & Szepesvari, 2006).Sailing domain, sailboat navigates destination 8-connected gridrepresenting marine environment, fluctuating wind conditions. goal reachdestination quickly possible, choosing grid location neighbor locationmove to. durationmove depends direction move (ceterisparibus, diagonal moves take 2 time straight moves), direction windrelative sailing direction (the sailboat cannot sail wind moves fastesttail wind), tack. direction wind changes time, strengthassumed fixed. sailing problem formulated goal-driven MDPfinite state space finite set actions, state capturing positionsailboat, wind direction, tack.goal-driven MDP, lengths paths terminal state necessarilybounded, thus entirely clear depth BRUE construct tree.Sailing domain, set H 4 n, n grid-size problem instance,unlikely optimal path two locations gridlonger complete encircling area.compared BRUE two MCTS-based algorithms: UCT algorithm, recentmodification UCT, obtained UCT replacing UCB1 policy root nodeuniform policy (Tolpin & Shimony, 2012). follows, denote modificationUCT uUCT. motivation behind design uUCT improve empiricalsimple regret UCT, results uUCT reported Tolpin Shimony (2012)(and confirmed experiments here) impressive. also display resultsadditional MCTS2e-based algorithm, baptized BRucbE, similarBRUE except that, exploration, uses UCB1 policy instead uniform policy.words, BRucbE seen UCT separation concerns. four algorithmsimplemented within single software infrastructure. line setup underlying181fiFeldman & Domshlak0.721.80.61.6UCTuUCTBRUEBRucbE0.41.4Simple RegretSimple Regret0.50.31.21UCTuUCTBRUEBRucbE0.80.20.60.100.405100.215012344Iterationsx 103.5433.5UCTuUCTBRUEBRucbE2.52789104x 10UCTuUCTBRUEBRucbE32.521.51610 10Simple RegretSimple Regret555Iterations1.500.511.52Iterations2.533.544x 1020 2000.511.5Iterations22.534x 1040 40Figure 5: Empirical performance UCT, uUCT (denoted UUCT, short), BRUE,BRucbE terms average error Sailing domain tasks n n gridsn {5, 10, 20, 40}.Theorem 6 Kocsis Szepesvari (2006), exploration coefficient UCT uUCT(parameter c Eq. 2) set difference largest possible smallestpossible values H-step rollouts root. Sailing domain, correspondsmaximal move duration, 6, multiplied number steps-to-go h.Figure 5 shows performance four algorithms terms empirical simpleregret, is, average difference Q(s0 , a) V (s0 ) true values actionchosen algorithm optimal action (s0 ). algorithm run1000 randomly chosen initial states s0 , target fixed one cornersgrid. performance measured depicted function planning time.four algorithms, planning time unit, iteration, corresponds H actionsamples, is, length single rollout.182fiSimple Regret Optimization Online Planning MDPs43.53.533UCTuUCTBRUEBRucbE2UCTuUCTBRUEBRucbE2.5Simple RegretSimple Regret2.51.521.5110.5000.50.20.40.60.81Iterations1.21.41.61.8026x 10B = 6/D = 800.511.522.5Iterations33.544.555x 10B = 2/D = 22Figure 6: Empirical performance UCT, uUCT, BRUE, BRucbE terms averageerror MDP version random game trees branching factor B treedepth D.Consistently results reported work Tolpin Shimony (2012),smaller tasks, uUCT outperformed UCT large margin, latter exhibitinglittle improvement time even smallest, 5 5, grid. differenceuUCT UCT larger tasks less notable. turn, BRUE BRucbEsubstantially outperformed UCT, BRucbE slightly better smaller tasks,BRUE taking larger instances, except relatively short planning deadlines.shows value MCTS2es separation concerns lies abilityemploy pure exploration policy, also ability base estimationsempirically best values, regardless employed exploration policy.Overall, results Sailing domain clearly testify BRUE attractive terms formal guarantees, also effective practice.also evaluated four algorithms domain random game trees whose goal simplemodeling two-person zero-sum games Go, Amazons Globber. games,winner decided global evaluation end board, evaluation employinganother feature counting procedure; rewards thus associatedterminal states. Following Kocsis Szepesvari (2006), rewards domaincalculated first assigning values moves, summing values alongpaths terminal states. Note move values used tree constructionmade available players. values chosen uniformly[0, 127] moves MAX, [127, 0] moves MIN. players act(depending role) maximize/minimize individual payoff: aim MAXreach terminal high R(s) possible, objective MIN similar,mutatis mutandis. simple game tree model similar spirit many game treemodels used previous work (Kocsis & Szepesvari, 2006; Smith & Nau, 1994), twoexceptions. First, measure success/failure players via actual payoffs183fiFeldman & Domshlak41.83.51.631.41.2BRUESimple RegretSimple Regret2.5BRUE()2BRUEBRUE()11.50.810.60.50.4000.20.40.60.81Iterations1.21.41.61.80.226x 10Game Trees (B = 6/D = 8)012345Iterations6789104x 10Sailing (10 10)Figure 7: Empirical performance BRUE BRUE() terms average errorMDP version random game trees sailing domain.receive, rather ternary scale win/lose/draw. Moreover, complysetting addressed work, model game MDP moves associated MAX player considered decision nodes, whereas moves MINmodeled stochastic outcomes following distribution: optimal minimaxmove chosen probability p = 0.9, complementary probability 1 p divideduniformly rest moves.Similarly setup Sailing domain, exploration coefficient UCTuUCTrangegame values, 127H, since rewards boundedset HHinterval 127 2 , 127 2 . ran experiments two different settings branchingfactor (B) tree depth (D). Sailing domain, compared empiricalsimple regret obtained UCT, uUCT, BRUE, BRucbE time. Figure 6 showsperformance four algorithms two game configurations, B = 6, = 8B = 2, = 22, configuration represented 1000 game trees. resultsappear encouraging well, BRUE BRucbE overtaking UCT uUCT,BRucbE even appearing slightly faster BRUE terms convergence.also experimented BRUE() which, line discussion rightTheorem 3, parameter dynamically adjusted function depthestimated node/action pair. Specifically, used = nnHh , nH denotes averagenumber samples leaf nodes, nh denotes average number samples nodesdepth value estimator consideration. show Figure 7,find significant empirical benefit BRUE() BRUE (to matchsuperior formal guarantees former), neither Sailing domain gametrees domain.last set experiments complements theoretical comparison sparsesampling (SS) algorithm. Specifically, performed empirical comparison BRUEUCT variant SS called forward-search sparse sampling (FSSS) (Walsh, Goschin,184fiSimple Regret Optimization Online Planning MDPs& Littman, 2010). Like SS, FSSS estimates action values node using C samples.However, instead estimating action values recursively encountered state,FSSS uses MCTS-style rollouts explore state space, initializing values yetunexplored actions predefined lower upper bounds. Ultimately, FSSS computesprecisely values SS, thus returning recommendation. However,potentially benefits kind pruning reduce amount computation. Notably,unlike SS, FSSS output action recommendation point time basedmaintained lower upper bounds actions values. typical approach selectaction maximum lower bound. However, similarly SS, FSSS cannot providenon-trivial guarantees prior termination. therefore choose use followingexperimental setup. First, run FSSS value C. take overallnumber action samples performed FSSS termination, use stoppingcriteria BRUE UCT. Figures 8 9 depict empirical simple regret obtainedthree algorithms upon termination Sailing game tree domains.planning task, picked values C allowed FSSS terminate withinreasonable amount time.9 Sailing domain, lower upper bounds FSSSset 0 6h, respectively, whereas game trees domain, used lower bound127 H2 upper bound 127 H2 .appears, BRUE UCT outperform FSSS tasks, notably, BRUEoutperforms FSSS tasks, every value C. despite purportedadvantage FSSS aware termination point. explanation resultconcerns two fundamental differences MCTS-based algorithms SS. First, recallformal discussion given Theorem 1 around entropy transition function.Suppose FSSS (or SS) estimates certain action two outcomes, oneoutcome likely other. outcomes caught C actionsamples, efforts would invested estimating values two states,regardless fact one outcome likely thus larger contributionvalue action. contrast, UCT BRUE adapt structureproblem skewing rollouts towards states higher probability, yielding betterresults theoretically empirically.Another potential advantage MCTS algorithms SS pertains allocationcomputational efforts estimating actions values different depths. FSSS (and SS),estimations based number samples C. contrast, UCTBRUE, nodes closer root sampled frequently branchingfactor. illustrate potential benefit focusing efforts around root, let usconsider Sailing domain example. position boat reached takingoptimal moves first steps would probably closer target comparedposition reached taking non-optimal moves fist steps. therefore likelyfollowing random navigation policy position boat firststeps, target would reached sooner average former case lattercase. words, benefit knowing optimal policy deeper states smaller,putting focus estimating actions nodes closer root makes much9. time limit FSSS set 24 hours. Notably, implementation FSSS, minimizenumber action sample sampling outcomes actions selected rollouts185fiFeldman & Domshlak5510 1020 2040 40Figure 8: Empirical performance FSSS, UCT, BRUE terms average errortermination Sailing domain tasks n n grids n {5, 10, 20, 40}.n = 5, empirical simple regret BRUE 0. n = 40, running FSSSC > 1 took 24 hours.186fiSimple Regret Optimization Online Planning MDPsB = 6/D = 8B = 2/D = 20Figure 9: Empirical performance FSSS, UCT, BRUE terms average errortermination MDP version random game trees branching factor Btree depth D.sense. believe property prevails many practical cases, casesMCTS algorithms expected efficient.also interesting see that, although UCT outperforms FSSS tasks,gap decreasing size budget (C), smaller tasks(Sailing domain grid 5 5 10 10), FSSS even outperforms UCT point.find compliant theoretical merits pure-exploratory natureSS BRUE.7. Summarygoal improving convergence guarantees smooth Monte-Carlo tree searchalgorithms online planning MDPs, introduced principle separationconcerns, well Monte-Carlo tree search scheme, MCTS2e, allows operationalizing principle. showed subclass purely exploring instances MCTS2eguarantees smooth exponential-rate improvement performance measures interest,improving polynomial-rate guarantees provided state-of-the-art algorithms.examined, formally empirically, purely exploring MCTS2e algorithm calledBRUE. Finally, explored prospects time-dependent forgetting samples withinMonte-Carlo search, showed concrete merits sample ignorance parametricBRUE() algorithm generalizes BRUE learning forgetting.results open numerous questions investigation. First, BRUErather straightforward implementation pure exploration MCTS2e, necessarily efficient one. believe replacing uniform exploration BRUEscheme makes use knowledge acquired along sampling direct187fiFeldman & Domshlakexploration may result empirically efficient instance MCTS2e, possiblyeven improve formal guarantees BRUE.Another important point consider speed convergence good actions,opposed speed convergence optimal actions. BRUE geared towardsidentifying optimal action, good often best one hope dealinglarge MDPs. identify optimal solution, BRUE constructs full-depth tree rightstart. However, focusing nodes closer root node, e.g., utilizingintelligent rules rollout termination, may improve quality recommendationplanning time severely limited. recently reported successful stepsdirection (Feldman & Domshlak, 2013), steps far closinginteresting venue research.Finally, core tree sampling scheme employed BRUE plausible wayimplement concept separation concerns discussed paper. instance,substituting MC-backup procedure value updates based Bellmans principle,as, e.g., done Keller Helmert (2013), also constitutes form separationconcerns. would interesting in-depth comparison formalempirical properties different protocols.Acknowledgementswork partially supported carried Technion-Microsoft ElectronicCommerce Research Center, well partially supported Air Force Office Scientific Research, USAF, grant number FA8655-12-1-2096.Appendix A. Auxiliary Propositionsanalysis below, make extensive use Hoeffdings tail inequality sumsbounded independent random variables. addition, use result mathematicalprogramming P1 below.Hoeffdings tail inequality. Let X1 , . . . , Xn independent bounded random PvariablesXi falls interval [ai , bi ] probability 1, let Sn = ni=1 Xn .Then, > 0,2/P {Sn ESn t} e2tPn2i=1 (bi ai ).particular, Xi identically distributed within [0, 1] EXi = ,P {Sn n t} e2t2nP1 h 1, solution mathematical programmaximizepsubjectBXi=1BXpihpi Bpi = 1i=10 pi 1188.fiSimple Regret Optimization Online Planning MDPsvalue 1. result follows concavity objective function.Appendix B. Proofsappendix structured four subsections, respectively dedicated proofTheorem 1, Proposition 4.1.1, Theorems 2, 3. sake readability,places believe create confusion, expressions form P(E |X1 = x1 , . . . , Xk = xk ) f (x1 , . . . , xk ) Xi random variables written simplyP(E) f (X1 , . . . , Xk ).B.1 Proof Theorem 1follows, Vp (shhi) denote h-steps value function definedE"h1Xi=0fi#fifiR (si , (si hh ii) , si+1 ) fi s0 = ,fiexpectation transition function r, i.e., set conditionalprobability distributions {P(S | s, a)}s,a . Vp (shhi) denotes value function optimalpolicy . subscript p omitted p corresponds transition probabilities PMDP question.Theorem 1 follows almost immediately Lemma 4 below.Lemma 4 node shhinB2P V (shhi) VPb (shhi) ah ebh n(shhi)n B2P VPb (shhi) V (shhi) ah ebh n(shhi) ,h11116 9B 2 K 22(196BK) 2 (h1) (h 1)!221bh =.29K (196BK)h1 h2ah = 3K189fiFeldman & DomshlakProof: proof induction h. Starting h = 1,nBP V (sh1i) VPb (sh1i)2P Q(sh1i, (sh1i)) Q(sh1i, B (sh1i))3)(Xb 0 | s, B (sh1i)))R(s, B (sh1i), s0 ) >def. Q+P(P(s0 | s, B (sh1i)) P(s3s0XbP Q(sh1i,a) Q(sh1i, a)3a6= (sh1i)b+ P Q(sh1i, (sh1i)) Q(sh1i,(sh1i))3)(XXb 0 | s, a))R(s, B (sh1i), s0 ) >+P(P(s0 | s, a) P(s3s0aA(s)X2 n(sh1i)n(sh1i)P n(sh1i, a)+ 2Ke 9KHoeffding2KaA(s)3Ke2 n(sh1i)9K 2.HoeffdingAssuming claim holds h0 h, proving induction hypothesis h + 1,encounter following deficiencies:b unbiased estimator Q, is, EQb = Q. contrast,(F1) h = 1, Qbestimates inside tree (at nodes h > 1) biased. bias stems Qpossibly based numerous sub-optimal choices sub-tree rooted shhi.b independent. h > 1,(F2) h = 1, summands accumulated Qaccumulated reward depends selection actions subsequent nodes,turn depends previous rewards.way circumvent deficiencies captured sequence bounding B.1.1B.1.5 below. high level, deal bias samples using straightforwardextension Hoeffding inequality. analysis, dependence samplesalleviated conditioning outcome sample state informationcollected nodes sampled one. propositions madeassumption induction hypothesis.bConsidering node shh + 1i, first show value estimations Q(shh+1i,a)actions A(s) sufficiently accurate. show = (shh + 1i), whereasbounds actions derived similar way. ease presentation,follows use abbreviations = (shh + 1i), aB = B (shh+1i), na =n(shh + 1i, (shh + 1i)). also use following notation. {1, . . . , na }, letbrandom variables Xt capture accumulated reward samples averaged Q(shh+1i,),bt capture transition probatB capture policy induced BRUE sample t, Pbbilities estimations sample t. Proposition B.1.2, bound error Q(shh+1i,),190fiSimple Regret Optimization Online Planning MDPsbt descendants shh + 1i samples sufgiven error tB Pbtficiently small. Proposition B.1.1 bound probability error tB Psample large.Proposition B.1.1 > 0, let E event which, sampling Xt , =1, . . . , na , holds that,1.2.Ps0Bbt (s0 | s, ) V (s0 hhi) V (s0 hhi)PbPtPs02,bt (s0 | s, ) (R(s, , s0 ) + V (s0 hh 1i))P(s0 | s, ) P2,vuna 2 bhu 24Blogna56B1.+=9bhThen,P {E }112B 2 ah bh 2 na36Be.2 bh(14)Proof: follows P1P(Xs0tBbt (s0 | s, ) V (s0 hhi) VPbPt(s0 hhi)2)XB.P V (s0 hhi) V b (s0 hhi) qPt0b02 B Pt (s | s, )(15)BPtIndeed, states s0 summation, holds V (s0 hhi) V b (s0 hhi) <2,bt (s0|s,a )BPXs0then, particular,XBbt (s0 | s, ) V (s0 hhi) V (s0 hhi) <bt (s0 | s, ) qPPbtPbt (s0 | s, )s02 BP=191b (s0 | s, )X Pq2 0bt (s0 | s, )BP.2P1fiFeldman & DomshlakGiven that,naXXBP {E }P V (s0 hhi) V (s0 hhi) > qbt (s0 | s, )t=1 s02 BP()naXX0000b+PP(s | s, ) Pt (s | s, ) R(s, , ) + V (s hh 1i) >20t=1naXBah et=1na+b 2h4BtXI.H.t2e 4h2Hoeffdingt=1naXt=1=2Bah ebh t24BnaXb 2 n112B 2h36Baehna 2 bhdefinitiont=1112B 2 ah bh 2 na36Be2 bhNote that, bounding probability event E Eq. 14, basically ignorebt different sampling times, usedependency state tB PBbt happen accurate sample t,crude union bound. However, Pprobability remain accurate subsequent samples higher. possiblefactoring dependency bound Proposition B.1.1 improvetightness bound.bt samples = 1, . . . , na , givenConditioned state tB Psufficiently accurate defined event E above, Proposition B.1.2 boundsbprobability value estimator Q(shh+1i,) inaccurate.Proposition B.1.2 definition E introduced Proposition B.1.1,nabt }na , event E ,> 0, holds that, given {tB }t=1, {Pt=1(1) t, random variables Xt mutually independent,h fifibt }, E Q(shh+1i, a) ,(2) 1, E Xt fi {tB }, {Pfin2 nafi B2bb8(h+1)(3) P Q(shh+1i, ) Q(shh+1i, ) fi {t }, {Pt }, E e.Proof: correctness mutual independence (1) direct definition BRUE:dependency samples BRUE induced state informationb turn, proofcollected samples, determined solely B P.(2) obtained definition E follows:192fiSimple Regret Optimization Online Planning MDPsh fiXXBfibt }, E =bt (s0 | s, )R(s, , s0 ) +bt (s0 | s, )V (s0 hhi)E Xt fi {tB }, {PPPbPs0s0= Q(shh+1i, )Xbt (s0 | s, ) R(s, , s0 ) + V (s0 hhi)P(s0 | s, ) Ps0Xs0Bbt (s0 | s, ) V (s0 hhi) V (s0 hhi)PbPt22= Q(shh+1i, )Q(shh+1i, )definition EFinally, proof (3) obtained notingvuna 2 bhunana4Blog2 na56B1 X1 X1+=na9bhnat=1t=1vuna 2 bhu 24Blogna56B2+9bhnar4 2 4 2log x2+sincex593534Therefore,finfibbt }, EP Q(shh+1i, ) Q(shh+1i,) fi {tB }, {Pfi()nafihX1fiBbt }, Ebb= P E Q(shh+1i,) Q(shh+1i,)fi {t }, {Pfinat=1fihfifi BbbbP E Q(shh+1i, ) Q(shh+1i, ) fi {t }, {Pt }, E4e2 na8(h+1)2,bbound error value estimator Q(shh+1i,).Proposition B.1.3 > 0, holdsn113B 22h bh nab36BP Q(shh+1i, ) Q(shh+1i,)e.2 bh193(16)fiFeldman & DomshlakProof:nbP Q(shh+1i, ) Q(shh+1i,)P {E }fifinnXfibbt }, E P { B }, {Pbt } fifi EP Q(shh+1i, ) Q(shh+1i,) fi {tB }, {P+bt }{tB ,P113B 2 ah bh 2 na36Be2 bhProps. B.1.1 & B.1.2.bProposition B.1.4 employs bounds accuracy Q(shh+1i,a) boundBsimple regret .Proposition B.1.4P Q(shh+1i, (shh+1i)) Q(shh+1i, aB )114B 2 ah bh 2 n(shh+1i)144BKe2 bhProof:P Q(shh+1i, (shh+1i)) Q(shh+1i, aB )XbP Q(shh+1i, a) Q(shh+1i, a)2a6= (shh+1i)b+ P Q(shh+1i, (shh+1i)) Q(shh+1i, (shh+1i))22X113B Kah bh 2 n(shh+1i)n(shh + 1i)144BK+eP n(shh + 1i, a)2K2 bhProp. B.1.3aA(s)114B 2 Kah bh 2 n(shh+1i)144BKe2 bhHoeffdinginduction step concluded Proposition B.1.5.Proposition B.1.5n116B 2 Ka2Bh bh n(shh+1i)196BKP V (shh+1i) VPb (shh+1i)e.2 bhProof: SinceBV (shh+1i) VPb (shh+1i)= Q(shh+1i, (shh+1i)) Q(shh+1i, aB )Xb 0 | s, aB ) V (s0 hhi) V B (s0 hhi)+P(sbPs0+Xs0b 0 | s, aB ) R(s, aB , s0 ) + V (s0 hhi) ,P(s0 | s, aB ) P(s194fiSimple Regret Optimization Online Planning MDPsholdsnBP V (shh+1i) VPb (shh+1i)6P Q(shh+1i, (shh+1i)) Q(shh+1i, aB )7)(XBb 0 | s, aB ) V (s0 hhi) V (s0 hhi)+PP(sbP14s0()X0B0BB 00b+PP(s | s, ) P(s | s, ) R(s, , ) + V (s hhi) >140114B 2 Kah bh 2 n(shh+1i)144BKProp. B.1.4e2 bh()XXBb 0 | s, a) V (s0 hhi) V (s0 hhi)+P(sPbP14s0aA(s))(XXb 0 | s, a) R(s, a, s0 ) + V (s0 hhi) >+PP(s0 | s, a) P(s140aA(s)2n(shh+1i)b 2 n(shh+1i)114B 2 Kah bh 2 n(shh+1i)h 196BK196K(h+1)2196BKe+BKae+Keh2 bhh2 2 n(shh+1i)116B 2 Kah bh196BK(h+1)2e.2 bhn BProving bound P V b (shh+1i) V (shh+1i) completely similar.PFinally, proof Theorem 1 concludedP nB (s0 hHi) 6= (s0 hHi)P Q(s0 hHi, (s0 hHi)) Q(s0 hHi, B (s0 hHi))H12 n1116 9B 2 K 2(H1)22 9K 2 (196BK)H1 H 22(H1)!e3K(196BK)2(17)noting maximal loss choosing sub-optimal action s0 hHi H.B.2 Proof Proposition 4.1.1 (BRUE bounds Pe )Basically, proof Proposition 4.1.1 identical Theorem 1 h < 4.h 4, note()XqXpPe000bbPPt (s | s, a) > 3 PeP Pt (s | s, a) > P(s | s, a) + 2B001952tB4.Hoeffding(18)fiFeldman & Domshlakbt (s0 | s, a) P(s0 | s, a) +Indeed, PPeB2s0 ,XqXbt (s0 | s, a)Ps0s0rP(s0 | s, a) +PeB2r"X p2P(s0 | s, a) +s02pp2Pe 3 PePe2 2B#Therefore, probability Eq. 15 Proposition B.1.1 bounded h > 4()XB000bt (s | s, ) V (s hhi) V (s hhi)PPbtP20()XXqpBbt (s0 | s, a) > 3 PeP V (s0 hhi) V b (s0 hhi) q+PPPt0bt (s | s, )s0s06 Pe Pbh t22tBah e 36Pe + B4bh t236Pe2Bah eI.H. & Eq. 18.Consequently,2 na158B 2 ah bh306Pe .P {E }e2 bhPlugging bound P {E } chain bounding Propositions B.1.2-B.1.5,obtain result.B.3 Proof Theorem 2proof Theorem 2 follows proof Theorem 1 notingeffect rollout-actionpolicynconvergence rate comes playn(shh+1i)bounds P n(shh+1i, a) < 2K,condition Theorem 2 simply postulates bounds exponential-rateconvergence guaranteed.B.4 Proof Theorem 3general, proof Theorem 3 follows lines proof convergence rateBRUE Theorem 1, role Proposition B.3.i corresponding roleProposition B.1.i proof Theorem 1. Essentially, proof Theorem 3 deviatessubstantially proof Theorem 1 modification Propositions B.1.1B.1.2 partial averaging, captured Propositions B.4.1 B.4.2, respectively.bounds rest propositions adjusted accordingly. formulate proof196fiSimple Regret Optimization Online Planning MDPsarbitrary values , although derive bounds particular choice depth1dependent h (BK)h1 . Similarly Theorem 1, proof Theorem 3 basedLemma 5 below.Lemma 5 node shhinB2P V (shhi) VPb (shhi) ah ebh n(shhi)n B2P VPb (shhi) V (shhi) ah ebh n(shhi) ,12BK h1(h!)2 ,21bh =.29K (196BK)h1 h2ah = 3KProof: proof induction h. base induction identicalLemma 4, continue straight induction step. propositionsmade assumption induction hypothesis. Considering node shh + 1i,make use notation used proof Theorem 1, namely, = (shh + 1i),aB = B (shh+1i), na = n(shh + 1i, (shh + 1i)), and, {1, . . . , na }, randombvariables Xt capture accumulated reward samples averaged Q(shh+1i,), tB capbt capture transition probabilities estiture policy induced BRUE sample t, Pmations sample t. addition, also use additional abbreviation na = b(1)na c.Proposition B.4.1 > 0, let E event which, sampling Xt , =na , . . . , na , holdsBP b 0) V (s0 hhi) V (s0 hhi) ,P(s|s,1.0b2Pt2.PThen,s0bt (s0 | s, ) (R(s, , s0 ) + V (s0 hh 1i))P(s0 | s, ) P2,vuh+1 na 2u 24Blogna18h2.=+3bhP {E }b 2 n8Bh2h12Bae.h2Proof: follows P1)(XBbt (s0 | s, ) V (s0 hhi) V (s0 hhi)PPbtP20XBP V (s0 hhi) V b (s0 hhi) qPtbt (s0 | s, )s02 BP197fiFeldman & DomshlakThus,naXXBP {E }P V (s0 hhi) V (s0 hhi) > qbt (s0 | s, )0t=n2 BP()naXX0000bt (s | s, ) R(s, , ) + V (s hh 1i) >+PP(s | s, ) P20t=nanaXBah ebh t24BI.H.t=n+naXet24h2Hoeffdingt=nnaX2Bah ebh t24Bt=n=b 2 n8Bh2h12Baeh2naXt=nb 2 n8Bh2h12Baehh+1 na 2definitionProp. B.4.2 modified accordingly.Proposition B.4.2 definition E introduced Proposition B.4.1,nabt }na , event E ,, {P> 0, holds that, given {tB }t=1t=1(1) t, random variables Xt mutually independent,h fifibt }, E Q(shh+1i, a) ,(2) na , E Xt fi {tB }, {Pfinb h2 2 nahfi Bbb(3) P Q(shh+1i, ) Q(shh+1i, ) fi {t }, {Pt }, E e 8B(h+1)2 .Proof: correctness mutual independence (1) direct definition BRUE:dependency samples BRUE induced state informationb turn, proofcollected samples, determined solely B P.(2) obtained definition E follows:198fiSimple Regret Optimization Online Planning MDPsh fiXXBfibt }, E =bt (s0 | s, )R(s, , s0 ) +bt (s0 | s, )V (s0 hhi)E Xt fi {tB }, {PPPbPs0s0= Q(shh+1i, )Xbt (s0 | s, ) R(s, , s0 ) + V (s0 hhi)P(s0 | s, ) Ps0Xs0Bbt (s0 | s, ) V (s0 hhi) V (s0 hhi)PbPt22= Q(shh+1i, )Q(shh+1i, )definition EFinally, proof (3) obtained noting1h+1 navun 2u 2na4B log h+18h2aXXna11=+3bnhh+1 t=nt=nvuh+1 na 2u 24Blog2na21 1 h+18h+3bhnah+1r4 2 4 2log x2bh h2+sinceh+1 =x5B93534naTherefore,finfibbt }, EP Q(shh+1i, ) Q(shh+1i,) fi {tB }, {PnahX1bb= P E Q(shh+1i,) Q(shh+1i,)h+1 nat=nafihfifi BbbbP E Q(shh+1i, ) Q(shh+1i, ) fi {t }, {Pt }, E4ebh h2 2 na8B(h+1)2fififi Bbt }, Efi {t }, {Pfifi.(19)Proposition B.4.3 > 0, holdsn9Bh2b h2 2 nahb12B(h+1)2 .P Q(shh+1i, ) Q(shh+1i,)eh2199fiFeldman & DomshlakProof:nbP Q(shh+1i, ) Q(shh+1i,)P {E }fifinnXfibbt }, E P {tB }, {Pbt } fifi EP Q(shh+1i, ) Q(shh+1i,) fi {tB }, {P+bt }{tB ,P2 2b h na9Bh2h12B(h+1)2eh2Props. B.4.1 & B.4.2.remainder proof identical proof Theorem 1, whereasbbounds error estimators Q(shh+1i,a) aligned Proposition B.4.3.Proposition B.4.4Proof:P Q(shh+1i, ) Q(shh+1i, aB )2 2b h n(shh+1i)10BKh2h96BK(h+1)2eh2P Q(shh+1i, ) Q(shh+1i, aB )XbbP Q(shh+1i, a) Q(shh+1i, a)+ P Q(shh+1i, ) Q(shh+1i, )22a6=ab h2 2 n(shh+1i)X9BKh2n(shh + 1i)h96BK(h+1)2Prop. B.4.3+eP n(shh + 1i, a)h2K2aA(s)2 2b h n(shh+1i)10BKh2h96BK(h+1)2.eh2Hoeffdinginduction step concluded Proposition B.4.5.Proposition B.4.5n12BKh2b h2 2 n(shh+1i)Bh196BK(h+1)2 .P V (shh+1i) VPb (shh+1i)eh2Proof: SinceBV (shh+1i) VPb (shh+1i)= Q(shh+1i, (shh+1i)) Q(shh+1i, aB )Xb 0 | s, aB ) V (s0 hhi) V B (s0 hhi)+P(sbPs0+Xs0b 0 | s, aB ) R(s, aB , s0 ) + V (s0 hhi) ,P(s0 | s, aB ) P(s200fiSimple Regret Optimization Online Planning MDPsholdsnBP V (shh+1i) VPb (shh+1i)6P Q(shh+1i, (shh+1i)) Q(shh+1i, aB )7()XBb 0 | s, aB ) V (s0 hhi) V (s0 hhi)+PP(sbP14s0)(Xb 0 | s, aB ) R(s, aB , s0 ) + V (s0 hhi) >+PP(s0 | s, aB ) P(s1402 2b h n(shh+1i)10BKh2h136BK(h+1)2Prop. B.4.4eh2()XXBb 0 | s, a) V (s0 hhi) V (s0 hhi)P(s+PbP14s0aA(s))(XXb 0 | s, a) R(s, a, s0 ) + V (s0 hhi) >+P(s0 | s, a) P(sP140aA(s)2 22b h n(shh+1i)n(shh+1i)b 2 n(shh+1i)10BKh2hh 196BK136BK(h+1)2196K(h+1)2e+BKae+Kehh2b h2 2 n(shh+1i)12BKh2hah e 196BK(h+1)2 .2n BProving bound P V b (shh+1i) V (shh+1i) completely similar.PFinally, proof Theorem 3 concludedP nB (s0 hHi) 6= (s0 hHi)P Q(s0 hHi, (s0 hHi)) Q(s0 hHi, B (s0 hHi))2 n12BK H12 9K 2 (196BK)H1 H 23K(H!)e2(20)noting maximal loss choosing sub-optimal action s0 hHi H.Appendix C. Proof Proposition 2.1.1 (SS bound)bLet Q(shhi,a) average C recursive samples value action shhi,b 0 | s, a) empirical transition probability based C samples.let P(snode shhi action a, probability least 1 e2 2 CH2,fififiXfifib 0 | s, a) V (s0 hh 1i)fifiP(s0 | s, a) P(sfifi 0fi201fiFeldman & DomshlakSince sparse sampling encounters (min(B, C) K)H nodes, probability bad estimate bounded (min(B, C) K)H e2 2 CH2. Therefore,bQ(shhi, a) Q(shhi,a)Xb 0 | s, a) V (s0 hh 1i)P(s0 | s, a) P(ss0+X00000bbP(s | s, a) Q(s hh 1i, (s hh 1i)) maxQ(s hh 1i, )0s0+Xs0b 0 | s, a) Q(s0 hh 1i, (s0 hh 1i)) Q(sb 0 hh 1i, (s0 hh 1i)) ,P(s(21)similarly,bQ(shhi,a) Q(shhi, a)Xb 0 | s, a) P(s0 | s, a) V (s0 hh 1i)P(ss0+Xs0+b 0 | s, a) max Q(sb 0 hh 1i, a0 ) Q(s0 hh 1i, (s0 hh 1i))P(s0(22)Xs0nb 0 | s, a) max Q(sb 0 hh 1i, a0 ) Q(s0 hh 1i, a0 ) .P(s0Note two bounds Equations 21,22 result recursion h = + h1 =h, h upper bounds respective difference. implies that, probability(min(B, C) K)H esetting =2H ,2 2 CH2,fifififibQ(shHi,a)Q(shHi,a)fifi > H00obtain error probability bounded2 C(min(B, C) K)H e 2H 4proof concludes noting maximal loss choosing non-optimal actionroot node s0 hHi H.202fiSimple Regret Optimization Online Planning MDPsReferencesAuer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis multiarmedbandit problem. Machine Learning, 47 (2-3), 235256.Balla, R., & Fern, A. (2009). UCT tactical assault planning real-time strategy games.Proceedings 21st International Joint Conference Artificial Intelligence(IJCAI), pp. 4045.Bellman, R. (1957). Dynamic Programming. Princeton University Press.Bjarnason, R., Fern, A., & Tadepalli, P. (2009). Lower bounding Klondike SolitaireMonte-Carlo planning. Proceedings 19th International Conference Automated Planning Scheduling (ICAPS).Bonet, B., & Geffner, H. (2012). Action selection MDPs: Anytime ao vs. uct.Proceedings 26th AAAI Conference Artificial Intelligence (AAAI).Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,11, 194.Browne, C., Powley, E. J., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen, P.,Tavener, S., Perez, D., Samothrakis, S., & Colton, S. (2012). survey Monte-Carlotree search methods. IEEE Transactions Computational Intelligence AIGames, 143.Bubeck, S., & Munos, R. (2010). Open loop optimistic planning. Proceedings 23rdAnnual Conference Learning Theory (COLT), pp. 477489.Bubeck, S., Munos, R., & Stoltz, G. (2011). Pure exploration finitely-armedcontinuous-armed bandits. Theoretical Computer Science, 412 (19), 18321852.Busoniu, L., & Munos, R. (2012). Optimistic planning Markov decision processes.Proceedings Fifteenth International Conference Artificial IntelligenceStatistics (AISTATS), No. 22 Journal Machine Learning Research - ProceedingsTrack, pp. 182189.Coquelin, P.-A., & Munos, R. (2007). Bandit algorithms tree search. Proceedings23rd Conference Uncertainty Artificial Intelligence (UAI), pp. 6774,Vancouver, BC, Canada.Eyerich, P., Keller, T., & Helmert, M. (2010). High-quality policies Canadian Travelers problem. Proceedings 24th AAAI Conference Artificial Intelligence(AAAI).Feldman, Z., & Domshlak, C. (2012). Simple regret optimization online planningmarkov decision processes. CoRR, arXiv:1206.3382v2 [cs.AI].Feldman, Z., & Domshlak, C. (2013). Monte-Carlo planning: Theoretically fast convergencemeets practical efficiency. Proceedings 29th Conference UncertaintyArtificial Intelligence (UAI), pp. 212221.Geffner, H., & Bonet, B. (2013). Concise Introduction Models Methods Automated Planning. Synthesis Lectures Artificial Intelligence Machine Learning.Morgan & Claypool.203fiFeldman & DomshlakGelly, S., & Silver, D. (2011). Monte-Carlo tree search rapid action value estimationcomputer Go. Artificial Intelligence, 175 (11), 18561875.Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithmsfactored MDPs. Journal Artificial Intelligence Research, 19, 399468.Hay, N., Shimony, S. E., Tolpin, D., & Russell, S. (2012). Selecting computations: Theoryapplications. Proceedings Annual Conference Uncertainty ArtificialIntelligence (UAI).Kearns, M. J., Mansour, Y., & Ng, A. Y. (2002). sparse sampling algorithm nearoptimal planning large Markov decision processes. Machine Learning, 49 (2-3),193208.Keller, T., & Eyerich, P. (2012). Probabilistic planning based UCT. Proceedings22nd International Conference Automated Planning Scheduling (ICAPS).Keller, T., & Helmert, M. (2013). Trial-based heuristic tree search finite horizon MDPs.Proceedings 23rd International Conference Automated PlanningScheduling (ICAPS), pp. 135143.Kocsis, L., & Szepesvari, C. (2006). Bandit based Monte-Carlo planning. Proceedings17th European Conference Machine Learning (ECML), pp. 282293, Berlin,Germany.Kolobov, A., Mausam, & Weld, D. (2012). LRTDP vs. UCT online probabilistic planning. Proceedings 26th AAAI Conference Artificial Intelligence (AAAI).Mausam, & Kolobov, A. (2012). Planning Markov Decision Processes: AI Perspective. Synthesis Lectures Artificial Intelligence Machine Learning. Morgan &Claypool.Peret, L., & Garcia, F. (2004). On-line search solving Markov decision processes viaheuristic sampling. Proceedings 16th Eureopean Conference ArtificialIntelligence (ECAI), pp. 530534, Valencia, Spain.Puterman, M. (1994). Markov Decision Processes. Wiley, New York.Robbins, H. (1952). aspects sequential design experiments. BulletinAmerican Mathematical Society, 58 (5), 527535.Smith, S. J., & Nau, D. S. (1994). analysis forward pruning. ProceedingsAAAI Conference Artificial Intelligence (AAAI), pp. 13861391.Sturtevant, N. (2008). analysis UCT multi-player games. Proceedings 6thInternational Conference Computers Games (CCG), p. 3749.Tolpin, D., & Shimony, S. E. (2011). better UCT: Rational Monte Carlo sampling trees. CoRR, arXiv:1108.3711v1 [cs.AI].Tolpin, D., & Shimony, S. E. (2012). MCTS based simple regret. Proceedings26th AAAI Conference Artificial Intelligence (AAAI).Walsh, T. J., Goschin, S., & Littman, M. L. (2010). Integrating sample-based planningmodel-based reinforcement learning. Proceedings 24th AAAI ConferenceArtificial Intelligence (AAAI), pp. 612617.204fiSimple Regret Optimization Online Planning MDPsZilberstein, S. (1993). Operational Rationality Compilation Anytime Algorithms.Ph.D. thesis, University California Berkeley.205fiJournal Artificial Intelligence Research 51 (2014) 71-131Submitted 5/14; published 9/14Testability BDI Agent SystemsMichael WinikoffStephen Cranefieldmichael.winikoff@otago.ac.nzstephen.cranefield@otago.ac.nzDepartment Information ScienceUniversity OtagoNew ZealandAbstractdeploying software system need assure (and stakeholders)system behave correctly. assurance usually done testing system.However, intuitively obvious adaptive systems, including agent-based systems,exhibit complex behaviour, thus harder test. paper examineobvious intuition case Belief-Desire-Intention (BDI) agents. analysesize behaviour space BDI agents show although intuition correct,factors influence size expected be. Specifically,found introduction failure handling much larger effect sizebehaviour space expected. also discuss implications findingstestability BDI agents.1. IntroductionIncreasingly called upon develop software systems operate dynamic environments, robust face failure, required exhibit flexible behaviour, operate open environments. One approach developing systemsdemonstrated effectiveness range domains use metaphorsoftware agents (Wooldridge, 2002). Agent-based systems increasingly findingdeployment wide range applications (e.g. Munroe, Miller, Belecheanu, Pechoucek,McBurney, & Luck, 2006; Benfield, Hendrickson, & Galanti, 2006).agent-based systems increasingly deployed, issue assurance rears head.deploying system, need convince rely system (orresponsible fails) system will, fact, work. Traditionally,assurance done testing1 . However, generally accepted adaptive systemsexhibit wide complex range behaviours, making testing hard. example:Validation extensive tests mandatory . . . . However, task provedchallenging . . . . Agent-based systems explore realms behaviour outside peoples expectations often yield surprises. (Munroe et al., 2006, Section 3.7.2)is, intuition agent systems exhibit complex behaviour, makeshard test. paper explore intuition, focusing well known BeliefDesire-Intention (BDI) approach realising adaptive flexible agents (Rao & Georgeff,1. Although considerable research formal methods context agent systems (Dastani,Hindriks, & Meyer, 2010), yet ready real world application (see Section 7),concerns scope work applicability (Winikoff, 2010).c2014AI Access Foundation. rights reserved.fiWinikoff & Cranefield1991; Bratman, 1987), demonstrated practically applicable, resultingreduced development cost increased flexibility (Benfield et al., 2006).explore intuition agent systems hard test analysingspace possible behaviours BDI agents, is, number paths BDIprogram, probability failure. focus BDI agents provide welldefined execution mechanism analysed, also seek understandcomplexities (and testability implications) adaptive intelligent behaviourabsence parallelism (since implications parallelism already well known).derive number paths BDI program function various parameters (e.g. number applicable plans per goal failure fate). naturallyleads us also consider number paths affected various parameters.might expected, show intuition agent systems hard testcorrect, i.e. agent systems large number paths. also show BDIagents harder test procedural programs, showing number pathsBDI program much larger number paths similarly-sizedprocedural program.contribution paper threefold. Firstly, confirms intuition BDIprograms hard test. Secondly, quantifying number paths,function parameters BDI program. Thirdly, find surprising resultsparameters influence number paths.Although recently increasing interest testing agent systems (Zhang,Thangarajah, & Padgham, 2009; Ekinci, Tiryaki, Cetin, & Dikenelli, 2009; Gomez-Sanz,Bota, Serrano, & Pavon, 2009; Nguyen, Perini, & Tonella, 2009b), surprisingly little work determining feasibility testing agent systems first place.Padgham Winikoff (2004, pp. 1719) analyse number successful executionsBDI agents goal-plan tree (defined Section 3), consider failure failurehandling analysis, consider testability implications. Shaw, FarwerBordini (2008) analysed goal-plan trees shown checking whether goal-plantree execution schedule respect resource requirements NP-complete.different problem one tackle: concerned allocationresources amongst goals, rather behaviour space.briefly address number possible criticisms work, consideringexisting work.1. number paths useful metric assessing testability?consider related area software testing (Section 1.1) arguemetric well established one, appropriate use assess testability.2. Isnt obvious corollary complexity HTN planning?consider detail HTN planning problem (Section 1.2) argue althoughBDI execution cycle certain similarities HTN planning, differencessignificant, and, particular, mean problem HTN planningsimply different problem testing BDI programs.3. use combinatorial analysis, rather complexity analysis?combinatorial analysis precise: yields formulae exact number72fiOn Testability BDI Agent Systemspaths, exact probabilities failure. latter (see Section 4.5)informative order magnitude complexity. Additionally allowsus consider issues complexity analysis would address, effectnumber failures number paths.1.1 Software Testingtrying assess hard agent systems test. concretely, given BDIagent program, want know hard program test. reduceddirectly question test set adequacy. agent program P easy test preciselyextent exists test set adequate testing P ,infeasibly large. Conversely, agent program P hard test extentadequate test set would infeasibly large. words, hardnesstesting program directly assessed size required test set adequaterespect suitable adequacy criteria.many criteria used assess whether given set tests adequate (for recent overview, see Mathur, 2008). Given interested assessingdifficulty testing given program, clearly looking white box testing. Furthermore, working abstract goal-plan trees rather detailed programs(see Section 2). means need consider control-flow based metrics, ratherdata-flow, since abstract goal-plan tree contain data-flow information.Focussing white box testing criteria control-flow based, basiclong-standing criterion assessing test set adequacy paths programcovered (Miller & Maloney, 1963). example, consider program following form.3. ...1. Input x2. condition ...5. endif6. C4. ... else Btwo paths program: (1, 2, 3, 5, 6) (1, 2, 4, 5, 6),adequate test set must least two tests adequate: one exercise first path,another exercise second. case test set single testinadequate, result part program executed testing.obvious complication covering paths program loop resultinfinite number paths, since loop potentially executed numbertimes. standard technique dealing bound length paths,number executions loop (Zhu, Hall, & May, 1997, p. 375). Bounding executionloops done either calculating upper bound number iterations baseddata (Mathur, 2008, p. 53), considering paths loops executedzero times one time (Mathur, 2008, p. 408).One question might asked consider paths, rather weakercriterion. Agent applications typically involve environments non-episodic. is,environments history matters. means behaviour given plan goalis, general, sensitive agents history, hence need consider differentpossible histories. Achieving goal may different done first thing73fiWinikoff & Cranefieldagent does, failed plan already performed number actions.means makes sense consider path-based criterion testing.Furthermore, although paths adequacy criterion often considered impractical, reason appears primarily existence infinite number pathspresence loops. instance, Zhu et al. (1997, p. 375) say plan coverage criterionstrong practically useful programs, infinitenumber different paths program loops. setting,loops, existence infinite number paths issue, considering numberpaths possible.therefore use number paths proxy measure testing difficulty:paths program, adequate test set (accordingpaths criterion) need large. hand, number pathslarge, adequate test set need large.one issue need consider: since paths strong criterion,possible that, even absence (or bounding) loops, criterion always resultsinfeasibly large numbers paths. order address issue also analysisnumber paths procedural programs (of equivalent size), comparenumber paths BDI programs (see Section 6).Finally, bears noting paths criterion considers partsprogram traversed testing, ignores values variables. So, example,trivial program consisting single statement x := x x single one-step path,trivially covered, many traces (x = 0, 1, 2 . . .).1.2 HTN Planningsimilarities Hierarchical Task Network (HTN) planning (Erol, Hendler,& Nau, 1994) BDI execution (de Silva & Padgham, 2004): use hierarchicalrepresentation goals (non-primitive tasks HTN terminology), plans (decomposition methods) goal-plan trees (task networks). complexity HTN planningexplored. Given similarities, simply exploit known complexityresults?turns cannot so, simple reason complexity HTNplanning concerns plan finding problem, different BDI plan execution,Sardina Padgham explain:BDI agent systems HTN planners come different communitiesdiffer many important ways. former focus execution plans,whereas latter concerned actual generation plans.former generally designed respond goals information; latterdesigned bring goals. addition, BDI systems meantembedded real world therefore take decisions based particular(current) state. Planners, hand, perform hypothetical reasoningactions interactions multiple potential states. Thus, failuredifferent meaning two types systems. contextplanning, failure means plan potential plan suitable; withinBDI agent systems failure typically means active (sub)plan ought74fiOn Testability BDI Agent Systemsaborted. Whereas backtracking upon failure option planning systems,generally BDI systems, actions taken real world. (Sardina& Padgham, 2011, p. 45, bold emphasis added)words, HTN systems plan ahead execution, whereas BDI systems interleaveexecution planning2 .HTN plan existence problem answers question plan exist?complexity studied. settings correspond BDI execution (many goals,total ordering within plans, variables) known EXPSPACE-hardDEXPTIME (Erol, Hendler, & Nau, 1994, 1996). However, work addressquestion BDI execution. considering complexity plan existence HTNplanning asking computational complexity search processresult plan. hand, asking number pathsgoal-plan tree asking possibilities arise executing plan.illustrate point, consider following example. Suppose single goalG decomposed two alternative plans, P1 P2 . Plan P1 consistssequential execution actions a, b, c; plan P2 consists sequential executionactions e. plan existence problem boils considering options P1P2 , since case search space simple, offering two options.hand, question many paths exist BDI execution considers differentways goal-plan tree executed. Whereas HTN planning considers P1single atomic decomposition, BDI execution needs consider sequence actions a, b, cdistinct steps. possible three actions succeed (giving trace a, b, c),also possible action b fail, followed P2 (successfully) used (giving tracea, b8, d, e), action c fail, followed P2 (successfully) used (giving tracea, b, c8, d, e).Overall, means complexity analysis Erol et al. (1994, 1996)different problem, HTN complexity results relevant. Finally, notethat, fact, setting, plan existence problem actually trivially true: sinceBDI programs constraints always expansion programsequence actions.remainder paper structured follows. begin briefly presentingBDI execution model (Section 2) discussing BDI execution viewedprocess transforming goal-plan trees (Section 3). Section 4 core paperanalyse number paths BDI-style goal-plan tree. consideranalysis assumptions hold real system real platform (Section 5),analysis BDI programs compares analysis (number paths)conventional procedural programs (Section 6). Finally, conclude discussionimplications testing future work (Section 7).2. approaches blur difference adding look-ahead planning BDI online executionHTNs, example planner RETSINA multi-agent system (Paolucci, Shehory, Sycara, Kalp,& Pannu, 2000) ability interleave planning execution. However, theoretical analysisextension reported, analysis Erol, Hendler, Nau (1994, 1996) appliesclassical HTN planning.75fiWinikoff & Cranefield2. BDI Execution Modeldescribe Belief-Desire-Intention (BDI) model explain chosemodel agent execution. addition well known widely used, BDI modelwell defined generic. well defined allows us analyse behaviour spacesresult using it. generic implies analysis applies wide rangeplatforms.BDI model viewed philosophical (Bratman, 1987) logical (Rao& Georgeff, 1991) perspectives, interested implementation perspective, exhibited range architectures platforms, JACK (Busetta,Ronnquist, Hodgson, & Lucas, 1999), JAM (Huber, 1999), dMARS (dInverno, Kinny, Luck,& Wooldridge, 1998), PRS (Georgeff & Lansky, 1986; Ingrand, Georgeff, & Rao, 1992),UM-PRS (Lee, Huber, Kenny, & Durfee, 1994), Jason (Bordini, Hubner, & Wooldridge,2007), SPARK (Morley & Myers, 2004), Jadex (Pokahr, Braubach, & Lamersdorf, 2005)IRMA (Bratman, Israel, & Pollack, 1988). purposes analysis here,formal detailed presentation unnecessary. interested formal semanticsBDI languages referred work Rao (1996), Winikoff, Padgham, Harland,Thangarajah (2002) Bordini et al. (2007), example.implementation BDI agent key concepts beliefs (or, generally,data), events plans. reader may find surprising goals key conceptsBDI systems. reason goals modelled events: acquisition new goalviewed new goal event, agent responds selecting executing planhandle event3 . remainder section, keeping establishedpractice, describe BDI plans handling events (not goals).BDI plan consists three parts: event pattern specifying event(s) relevantfor, context condition (a Boolean condition) indicates situations planused, plan body executed. plans event pattern context conditionmay terms containing variables, matching unification process (dependingparticular BDI system) used BDI interpreters find plan instances respondgiven event. general plan body contain arbitrary code programminglanguage4 , however purposes assume5 plan body sequence steps,step either action6 (which succeed fail) event posted.example, consider simple plans shown Figure 1. first plan, Plan A,relevant handling event achieve goal go-home, applicable situationsagent believes train imminent. plan body consists sequencefour steps (in case assume actions, could also modelledevents handled plans).key feature BDI approach plan encapsulates conditionsapplicable defining event pattern context condition. allowsadditional plans given event added modular fashion, since invoking3. types event typically include addition removal beliefs agents belief set.4. example, JACK plan body written language superset Java.5. follows abstract notations AgentSpeak(L) (Rao, 1996) (Winikoff et al., 2002)aim capture essence range (more complex) BDI languages.6. includes traditional actions affect agents environment, internal actionsinvoke code, check whether certain condition follows agents beliefs.76fiOn Testability BDI Agent SystemsPlan A: handles event:achieve goal go-homecontext condition:train imminentplan body:(1) walk train station(2) check train running time(3) catch train(4) walk homePlan B: handles event:achieve goal go-homecontext condition:raining bicycleplan body:(1) cycle homePlan C: handles event:achieve goal go-homecontext condition:true (i.e. always applicable)plan body:(1) walk bus stop(2) check buses running(3) catch bus(4) walk homeFigure 1: Three Simple Planscontext (i.e. triggering event posted) contain code selects amongstavailable plans, key reason flexibility BDI programming.typical BDI execution cycle elaboration following event-driven process(summarised Figure 2)7 :1. event occurs (either received outside source, triggered withinagent).2. agent determines set instances plans plan library event patternsmatch triggering event. set relevant plan instances.3. agent evaluates context conditions relevant plan instances generateset applicable plan instances. relevant plan instance applicable contextcondition true. applicable plan instances event deemedfailed, posted plan, plan fails. Notesingle relevant plan may lead applicable plan instances (if context conditionfalse), one applicable plan instance (if context condition,may contain free variables, multiple solutions).4. One applicable plan instances selected executed. selection mechanism varies platforms. generality, analysis make as7. BDI engines are, fact, complicated interleave execution multipleactive plan instances (or intentions) triggered different events.77fiWinikoff & CranefieldBoolean function execute(an-event)let relevant-plans = set plan instances resultingmatching plans event patterns an-eventlet tried-plans =truelet applicable-plans = set plan instances resultingsolving context conditions relevant-plansapplicable-plans := applicable-plans \ tried-plansapplicable-plans empty return falseselect plan p applicable-planstried-plans := tried-plans {p}execute(p.body) = true return trueendwhileBoolean function execute(plan-body)plan-body empty return trueelseif execute(first(plan-body)) = false return falseelse return execute(rest(plan-body))endifBoolean function execute(action)attempt perform actionaction executed successfully return true else return false endifFigure 2: BDI Execution Cyclesumptions plan selection. plans body may create additional eventshandled using process.5. plan body fails, failure handling triggered.brevity, remainder paper use term plan loosely meaneither plan plan instance intention clear context.Regarding final step, approaches dealing failure. Perhapscommon approach, used many existing BDI platforms,select alternative applicable plan, consider event failedremaining applicable plans. determining alternative applicable plans one mayeither consider existing set applicable plans, re-calculate set applicableplans (ignoring already tried), done Figure 2. makessense situation may changed since applicable plans determined.Many (but all) BDI platforms use failure-handling mechanism retryingplans upon failure, analysis applies platforms.One alternative failure-handling approach, used Jason (Bordini et al., 2007),post failure event handled user-provided plan. Althoughflexible, since user specify upon failure, place burden78fiOn Testability BDI Agent Systemsspecifying failure handling user. Note Jason provides pattern allowstraditional BDI failure-handling mechanism specified succinctly (Bordini et al., 2007,pp. 171172). Another alternative failure-handling approach used 2APL (Dastani,2008) predecessor, 3APL: permit programmer write plan repair rulesconditionally rewrite (failed) plan another plan. approach, like Jasons,quite flexible, possible analyse general way plan rulesquite arbitrary. Another well known BDI architecture IRMA, describedhigh-level prescribe specific failure-handling mechanism:full development architecture would give accountways resource-bounded agent would monitor prior planslight changes belief. However developed, coursetimes agent give prior plan light new beliefplan longer executable. happens, new processdeliberation may triggered (Bratman et al., 1988).Given BDI execution cycle discussed above, three example plans given earlier(Figure 1) give rise range behaviours, including following:Suppose event achieve goal go-home posted agent believestrain imminent. walks train station, finds train runningtime, catches train, walks home.Suppose upon arrival train station agent finds trainsdelayed. Step (2) Plan fails, agent considers alternative plans.raining present time, Plan B applicable, Plan C adopted(to catch bus).Suppose agent decided catch bus (because train believedimminent, raining), attempting execute Plan C fails (e.g.bus strike). agent reconsider plans rain stopped (andbicycle) may use Plan B.Note correct (respectively incorrect) behaviour distinct successful(respectively failed) execution plan. Software testing essence process running system checking whether observed behaviour trace correct (i.e. conformsspecification, model). hand, BDI agents behaviour tracesclassified successful failed. However, correctness given executiontrace independent whether trace successful failed execution. successfulexecution may, fact, exhibit behaviour correct, instance, traffic controlleragent may successfully execute actions set traffic signals intersection greenachieve goal so. successful execution, incorrect behaviour.also possible failed execution correct. instance, traffic controller agentattempting route cars point point B, traffic accident blocked keybridge two points, rational (and correct) behaviour agentfail achieve goal.79fiWinikoff & Cranefield3. BDI Execution Goal-Plan Tree ExpansionBDI execution, summarised Figure 2, dynamic process progressively executesactions goals posted. order easily analyse process, presentalternative view declarative. Instead viewing BDI execution process,view data transformation (finite) goal-plan tree sequence actionexecutions.events plans visualised tree goal8 childrenplan instances applicable it, plan instance children sub-goalsposts. goal-plan tree and-or tree: goal realised one planinstances (or) plan instance needs sub-goals achieved (and).Viewing BDI execution terms goal-plan tree action sequences makesanalysis behaviour space size9 easier. consider BDI execution process takinggoal-plan tree transforming sequence recording (failed successful)executions actions, progressively making decisions plans usegoal executing plans.process non-deterministic: need choose plan goal tree.Furthermore, consider failure, need consider action whether failsnot, fail, failure recovery done.define transformation process detail. Prolog code implementingprocess found Figure 3. defines non-deterministic predicate exec firstargument (input) goal-plan tree, second argument (output) sequenceactions. goal-plan tree represented Prolog term conforming followingsimple grammar (where GPT abbreviates Goal-Plan Tree, AoGL abbreviates ActionGoal List, symbol):hGPT ::= goal([]) | goal([hPlanListi])hPlanListi ::= hPlani | hPlani,hPlanListihPlani ::= plan([]) | plan([hAoGLi])hAoGLi ::= act(A) | hGPT | act(A),hAoGLi | hGPT i,hAoGLiexample, simple goal-plan tree shown Figure 4 modelled Prolog termgoal([plan([act(a)]), plan([act(b)])]).analysis make simplifying assumption. Instead modelling instantiationplans plan instances, assume goal-plan tree contains applicable planinstances. Thus, order transform goal node sequence actions (nondeterministically) select one applicable plan instances. selected plantransformed turn, resulting action sequence (line 2 Figure 3). selectingplan, consider possibility applicable plans could chosen,first plan. done different points time different plan instances mayapplicable. saw example earlier, Plan chosen failed,8. order consistent existing practice shall use term goal rather eventremainder paper.9. remainder paper use term behaviour space size, rathercumbersome term number paths BDI program.80fiOn Testability BDI Agent Systems123456789101112131415exec ( goal ([]) ,[]).exec ( goal ( Plans ) , Trace ) : - remove ( Plans , Plan , Rest ) , exec ( Plan , Trace1 ) ,( failed ( Trace1 ) -> recover ( Rest , Trace1 , Trace ) ; Trace = Trace1 ).exec ( plan ([]) , []).exec ( plan ([ Step | Steps ]) , Trace ) : - exec ( Step , Trace1 ) ,( failed ( Trace1 ) -> Trace = Trace1 ; continue ( Steps , Trace1 , Trace )).exec ( act ( Action ) , [ Action ]).exec ( act ( Action ) , [ Action , fail ]).failed ( Trace ) : - append (X ,[ fail ] , Trace ).recover ( Plans , Trace1 , Traces ) : exec ( goal ( Plans ) , Trace2 ) , append ( Trace1 , Trace2 , Traces ).continue ( Steps , Trace1 , Trace ) : - exec ( plan ( Steps ) , Trace2 ) ,append ( Trace1 , Trace2 , Trace ).remove ([ X | Xs ] ,X , Xs ).remove ([ X | Xs ] ,Y ,[ X | Z ]) : - remove ( Xs ,Y , Z ).Figure 3: Prolog Code Expand Goal-Plan TreesgoalplanplanbFigure 4: Simple Goal-Plan TreePlan C selected (and also failed), finally Plan B (which applicablePlan failed) selected.selected plan executes successfully (i.e. action trace doesnt end failmarker; line 9), resulting trace trace goals execution (line 3). Otherwise, perform failure recovery (line 10), done taking remaining planstransforming goal plans options. resulting action sequenceappended action sequence failed plan obtain complete action sequencegoal.process easily seen match described Figure 2 (with exception,discussed above, begin applicable plans, relevant plans). Specifically,applicable plan selected executed, successful execution stops.successful, alternative plan selected execution continues (i.e. actionsequences appended).order transform plan node first transform first step plan,either sub-goal action (line 5). successful, continue transformrest plan, append two resulting traces together (lines 6 12).first step plan successful, trace simply trace first step(line 6); words stop transforming plan step fails. Again, processeasily seen correspond plan body execution Figure 2.81fiWinikoff & CranefieldFinally, order transform action action sequence simply takeaction singleton sequence (line 7). However, need also take accountpossibility action may fail, thus second possibility action followedfailure indicator (line 8). Again, process easily seen correspond actionexecution Figure 2. Note model dont concernaction fails: could lack resources, environmental issues.example applying process two example goal-plan trees foundAppendix A.4. Behaviour Space Size BDI Agentsconsider many paths goal-plan tree usedBDI agent realise goal10 using tree. use analysis previous sectionbasis; is, view BDI execution transforming goal-plan tree actiontraces. Thus, question large behaviour space BDI agents, answeredderiving formulae allow one compute number behaviours, successfulunsuccessful (i.e. failed), given goal-plan tree.make following uniformity assumptions allow us perform analysis.simplifying assumptions concern form goal-plan tree.1. assume subtrees goal plan node structure. is,leaves goal-plan tree distance (number edges) awayroot tree. therefore define depth goal-plan treenumber layers goal nodes contains. goal-plan tree depth 0 plansub-goals, goal-plan tree depth > 0 either plan nodechildren goal nodes depth goal node children plannodes depth 1. Note definition depth reverse usualdefinition (where depth trees root defined 0). use definitionsimplifies presentation derivations later section.2. assume plan instances depth > 0 k sub-goals.3. assume goals j applicable plan instances. casegoal j relevant plans, results exactly one applicable planinstance, also case ways. instance, goal may 2jrelevant plans, half applicable current situation, goal maysingle relevant plan j applicable instances. Note assumption rulespossibility infinite number applicable plan instances,would case plans context condition infinite number solutions.cannot occur context condition defined terms conjunctionspropositions refer finite belief base. However, occur agentscontext conditions also make use Prolog-like knowledge base (as caseagent-oriented programming languages, Jason Goal). Nevertheless,since deal applicable plans, dont model context conditions.10. focus single goal analysis: multiple goals treated concurrent interleavingindividual goals. Multiple agents also treated concurrent interleaving, careneeds taken details agent waiting another agent respond.82fiOn Testability BDI Agent SystemsFigure 5 shows uniform goal-plan tree depth 2.g2d=2@R pj1p11 . . . @...@R@g11 . . . gk1...@R@p10 . . . pj0d=1d=1d=0Figure 5: uniform goal-plan treeassumptions made clearly unrealistic. means considerpossibility real agent programs behave quite differently, since meetassumptions. address issue number ways. Firstly, Section 4.4consider relaxation assumptions defining semi-uniform trees,number available plan instances (j) vary across different levels tree. Secondly,Section 5.2 consider example (non-uniform) goal-plan tree industrialapplication. derive number paths real goal-plan tree compareanalysis similarly-sized uniform goal-plan trees see whether real (non-uniform)tree significantly lower number paths uniform tree. Finally, Section 4.7,consider issue infinite trees allowing trees recursive, definingnumber paths (up bound path length) recursive tree.analysis uses following terminology:uniformity assumptions mean structure subtree rooted goalplan node determined solely depth, therefore denote goalplan node depth gd pd (respectively).use n4(xd ) denote number successful execution paths goal-plan treedepth rooted x (where x either goal g plan p). specifyingimportant sometimes elide it, writing n4(x).Similarly, use n8(xd ) denote number unsuccessful execution pathsgoal-plan tree depth root x (either g p).extend notation plan body segments, i.e. sequences x1 ; . . . ; xn xigoal action ; denotes sequential composition. abbreviate sequencen occurrences x xn (for example, g13 = g1 ; g1 ; g1 ).4.1 Base Case: Successful Executionsbegin calculating number successful paths goal-plan treeabsence failure (and failure handling). analysis follows PadghamWinikoff (2004, pp. 1719).Roughly speaking, number ways goal achieved sum numberways children achieved (since children represent alternatives,83fiWinikoff & Cranefieldi.e. goal represented node). hand, number ways planachieved product number ways children achieved(since children must achieved, i.e. plan represented node).precisely, n4(x1 ; x2 ) = n4(x1 ) n4(x2 ); is, sequence successful x1x2 successful.Given tree root g (a goal), assume j children achievedn different ways11 ; then, select one children, number waysg achieved jn. Similarly, tree root p (a plan), assumek children achieved n different ways, then, execute children,number ways p executed n n, nk . plan children(i.e. depth 0) executed (successfully) exactly one way. yields followingdefinition:n4(gd ) = j n4(pd1 )n4(p0 ) = 1n4(pd ) = n4(gd k ) = n4(gd )kExpanding definition obtainn4(g1 ) = j n4(p0 ) = j 1 = jkn4(g2 ) = j n4(p1 ) = j (n4(g1 ) ) = j (j k ) = j k+1n4(g3 ) = j n4(p2 ) = j (j k+1 )k = j kn4(g4 ) = j n4(p3 ) = j (j k2 +k+12 +k+1)k = j k3 +k 2 +k+1generalised to:n4(gd ) = jPd1i=0kik > 1 simplified using equivalence k i1 + . . . + k 2 + k + 1 = (k 1)/(k 1)give following closed form definition: (and k = 1 n4(gd ) = n4(pd ) = j )n4(gd ) = j (k4n (pd ) = j1)/(k1)k (kd 1)/(k1)(1)(2)Note equation n4(pd ) assumes sub-goals achieved sequentially.executed parallel number options higher, since need considerpossible interleavings sub-goals execution. example, suppose plan pdtwo sub-goals, g1d g2d , sub-goals n4(gd ) successful executions,execution l steps (we assume ease analysis execution pathslength). number ways interleaving two parallel executions,length l, calculated follows (Naish, 2007, Section 3):(2 l)!2l=l(l!) (l!)11. tree assumed uniform, children achieved numberways, thus interchangeable analysis, allowing us write j n rather n1 + . . . + nj .84fiOn Testability BDI Agent Systemshence number ways executing pd parallel execution subgoals is:442n (pd ) = n (gd )2ll= n4(gd )2(2 l)!(l!) (l!)remainder paper assume sub-goals plan achievedsequentially, since common case, since yields lower figure which,shall see, still large enough allow conclusions drawn.4.2 Adding Failureextend analysis include failure, determine number unsuccessfulexecutions, i.e. executions result failure top-level goal. momentassume failure handing (we add failure handling Section 4.3).order determine number failed executions know failureoccur. BDI systems two places failure occurs: goalapplicable plan instances, action (within applicable plan instance) fails.However, uniformity assumption means address former caseitassumed goal always j instances applicable plans. Noteconservative assumption: relaxing results number unsuccessful executionseven larger.order model latter case need extend model plans encompassactions. example, suppose plan body form a1; ga; a2; gb; a3 aiactions, ga gb sub-goals, ; denotes sequential execution. planfollowing five cases unsuccessful (i.e. failed) executions:1. a1 fails2. a1 succeeds, ga fails3. a1 ga succeed, a2 fails4. a1, ga, a2 succeed, gb fails5. a1, ga, a2 gb succeed, a3 failsSuppose ga executed successfully n4(ga) different ways. thirdcase corresponds n4(ga) different failed executions: successful execution ga,extend adding failed execution a2 (actions executed one way,i.e. n4(a) = 1 n8(a) = 1). Similarly, gb n4(gb) successful executions fifthcase corresponds n4(ga) n4(gb) different failed executions. ga unsuccessfullyexecuted n8(ga) different ways second case corresponds n8(ga) different executions. Similarly, fourth case corresponds n4(ga) n8(gb) different executions. Puttingtogether, total number unsuccessful executions plan pbody a1; ga; a2; gb; a3 sum five cases:1 + n8(ga) + n4(ga) + n4(ga) n8(gb) + n4(ga) n4(gb)85fiWinikoff & Cranefieldformally, n8(x1 ; x2 ) = n8(x1 ) + n4(x1 ) n8(x2 ); is, sequence fail eitherx1 fails, x1 succeeds x2 fails. follows n4(xk ) = n4(x)k n8(xk ) =n8(x) (1 + + n4(x)k1 ), easily proven induction.generally, assume ` actions before, after, sub-goalsplan, i.e. example plan corresponds ` = 1, following plan bodycorresponds ` = 2: a1; a2; g3; a4; a5; g6; a7; a8. plan sub-goals (i.e. depth0) considered consist ` actions (which quite conservative: particular,use ` = 1 assume plans depth 0 consist single action).number unsuccessful execution traces goal-plan tree defined,based analysis above, follows. First calculate numbers successesfailures following repeated section plan body: gd ; a` :n4(gd ; a` ) = n4(gd ) n4(a` )= n4(gd ) n4(a)`= n4(gd ) 1`= n4(gd )n8(gd ; a` ) = n8(gd ) + n4(gd ) n8(a` )= n8(gd ) + n4(gd ) n8(a) (1 + + n4(a)`1 )= n8(gd ) + n4(gd ) `> 0:n8(pd ) = n8(a` ; (gd ; a` )k )= n8(a` ) + n4(a` ) n8((gd ; a` )k )= n8(a) (1 + + n4(a)`1 )) + n4(a)` n8((gd ; a` )k )= ` + 1 n8(gd ; a` ) (1 + + n4(gd ; a` )k1 )= ` + (n8(gd ) + n4(gd ) `) (1 + + n4(gd )k1 ))n4(gd )k 1= ` + (n8(gd ) + ` n4(gd )) 4(assuming n4(gd ) > 1)n (gd ) 1yields following definitions number unsuccessful executions goalplan tree, without failure handling. equation n8(gd ) derived usingreasoning previous section: single plan selected executed, jplans.n8(gd ) = j n8(pd1 )n8(p0 ) = `n4(gd )k 1n4(gd ) 14(for > 0 n (gd ) > 1)n8(pd ) = ` + (n8(gd ) + ` n4(gd ))Finally, note analysis number successful executions goal-plantree absence failure handling presented Section 4.1 unaffected additionactions plan bodies. one way sequence actionssucceed, Equations 1 2 remain correct.86fiOn Testability BDI Agent Systems4.3 Adding Failure Handlingconsider introduction failure-handling mechanism affects analysis.common means dealing failure BDI systems respond failureplan trying alternative applicable plan event triggered plan.example, suppose goal g (e.g. achieve goal go-home) three applicable plans pa,pb pc, pa selected, fails. failure-handling mechanismrespond selecting pb pc executing it. Assume pc selected. pc fails,last remaining plan (pb) used, fails, goal deemedfailed.result that, might hope, harder fail: way goalexecution fail applicable plans tried fails12 .number executions computed follows: goal gd j applicable plan instances, n8(pd1 ) unsuccessful executions, n8(pd1 )junsuccessful executions plans sequence. Since plans selectedorder multiply j! yielding n8(gd ) = j! n8(pd1 )j .number ways plan fail still defined equationfailure handling happens level goalsbut n8(g) refers newdefinition:jn8(gd ) = j! n8(pd1 )(3)8n (p0 ) = `(4)4)kn (gd 1n4(gd ) 14(for > 0 n (gd ) > 1)n8(pd ) = ` + (n8(gd ) + ` n4(gd ))(5)Turning number successful executions (i.e. n4(x)) observeeffect adding failure handling convert failures successes, i.e. executionwould otherwise unsuccessful extended longer execution may succeed.Consider simple case: depth 1 tree consisting goal g (e.g. achieve goal go-home)three children: pa, pbandpc. Previously successful executions correspondedpi (i.e. select pi execute it). However, failure handling,following additional successful executions (as well additional cases correspondingdifferent orderings plans, e.g. pb failing pa successfully executed):pa fails, pb executed successfullypa fails, pb executed fails, pc executed succeedsleads definition formn4(g) = n4(pa) + n8(pa) n4(pb) + n8(pa) n8(pb) n4(pc)12. fact, actually underestimate: also possible goal fail none untriedrelevant plans applicable resulting situation. noted earlier, assume analysisgoals cannot fail result applicable plan instances. conservative assumption:relaxing results number behaviours even larger.87fiWinikoff & CranefieldHowever, need account different orderings plans. instance, casefirst selected plan succeeds (corresponding first term, n4(pa)) fact appliesj plans, first term, including different orderings, j n4(p).Similarly, second term (n8(pa) n4(pb)), corresponding case initiallyselected plan fails next plan selected succeeds, fact applies j initial plans,j 1 next plans, yielding j (j 1) n8(p) n4(p).Continuing process (for j = 3) yields following formulae:2n4(g) = 3 n4(p) + 32 n8(p) n4(p) + 3! n8(p) n4(p)generalisesj1n4(g) = j n4(p) + j (j 1) n8(p) n4(p) + + j! n8(p)n4(p)resulting following equations (again, since failure handling done goal level,equation plans Section 4.1):4n (gd ) =jXi1n8(pd1 )n4(pd1 )i=1n4(p0 ) = 14j!(j i)!(6)(7)4n (pd ) = n (gd )k(for > 0 )(8)used standard BDI failure-handling mechanism trying alternativeapplicable plans. let us briefly consider alternative failure-handling mechanismsimply re-posts event, without tracking plans already attempted.fairly easy see this, fact, creates infinite number behaviours: supposegoal g achieved pa pb, pa could selected, executed resultingfailure, pa could selected again, fail again, etc. suggestsstandard BDI failure-handling mechanism is, fact, appropriate, avoidsinfinite behaviour space, possibility infinite loop. discussed earlier (inSection 2), failure recovery mechanism used 3APL 2APL (Dastani, 2008) cannotanalysed general way, since depends details specific agent program;IRMA (Bratman et al., 1988) provide sufficient details allow analysis.Tables 1 2 make various equations developed far concrete showing illustrative values n8 n4 range reasonable (and fairly low) values j, kusing ` = 1. Number columns show number goals, plans actions tree. number actions brackets many actions executedsingle (successful) execution failure handling. number goals calculatedfollows. depth 1 single goal (see Figure 5). depth n + 11 + (j k G(n)) goals, G(n) denotes number goals depth n tree.gives G(n) = 1 + (j k) + (j k)2 + + (j k)n1 . example, j = k = 2,G(3) = 1 + 4 + 16 = 21. Since goal exactly j plans, number plans treedepth n j G(n). consider number actions. non-leaf plan` (k + 1) actions (since k goals, k + 1 places ` actions).leaf plan ` actions. tree depth n j (j k)n1 leaf plans. Let P (n)number plans depth n tree, comprised Pn (n) non-leaf plans88fiOn Testability BDI Agent SystemsParametersj k2 233 332 343 43goals2191259157Numberplans42273518471actions62 (13)363 (25)776 (79)627 (41)n4(g)1281,594,323n8(g)6146,337,4251,099,511,627,7766,523,509,472,17410,460,353,20341,754,963,603Table 1: Illustrative values n4(g) n8(g) without failure handling. first number actions (e.g. 62) number actions tree, second(e.g. 13) number actions single execution failures occur.Parametersj k2 233 332 343 43goals2191259157Numberplans42273518471actions62 (13)363 (25)776 (79)627 (41)n4(g)6.33 10121.02 101071.82 101573.13 10184n8(g)1.82 10132.56 101077.23 101577.82 10184Table 2: Illustrative values n4(g) n8(g) failure handlingPl (n) leaf plans, i.e. P (n) = Pn (n) + Pl (n). number actions depth n tree(` (k + 1)) Pn (n) + ` Pl (n). example, j = k = 2 ` = 1,P (3) = 2 G(3) = 42, comprised 32 leaf plans 10 non-leaf plans.therefore (1 3 10) + (1 32) = 62 actions.4.4 Recurrence Relationsequations previous sections define functions n4 n8 mutual recurrencedepth goal-plan tree uniform branching structure. effectincreasing parameters k ` evident level recursion,clear effect increasing number applicable plan instances jgiven goal. aim section explore effects changing j.relaxing uniformity assumption. Specifically, allow number plans availablevary goal nodes different depths tree, still assuming nodesgiven depth structure. refer semi-uniform goal-plan trees.derive set recurrence relations n4 n8 presence failure handlingexplicitly show effect adding new plan goal root particularsub-tree.begin defining generalised notation n8(gj ) n4(gj ) j list13(jd , jd1 , . . . , j0 ) element ji represents number plans available goalsdepth goal-plan tree. denote empty list hi write j j representlist head j tail j.13. order corresponds definition depth, decreases tree.89fiWinikoff & Cranefieldgeneralise Equations 3 6 apply semi-uniform goal-plan trees,derivation equations depended sub-nodes goal plan nodestructure. assumption preserved generalised setting.therefore rewrite equations using new notation, also express righthand sides functions f 8 f 4 n8(pj ) (for f 4 ) n4(pj ). aim find recursivedefinition f 8 f 4 recurrence j.n8(gjj ) = f 8 (j, n8(pj ))n4(gjj ) = f 4 (j, n8(pj ), n4(pj ))f 8 (j, a) = j! ajf 4 (j, a, b) =jXb ai1i=1j!(j i)!(change bounds 0 . . . n, hence replace + 1)=j1Xb a(i+1)1i=0j!(j (i + 1))!(simplify using (j (i + 1))! = (j i)!/(j i) )=j1Xi=0b aij!(j i)(j i)!(multiple i!/i! reorder)=j1Xi=0j!i! ai (j i) bi!(j i)!j(use definition binomial:= j!/i!(j i)!)j1Xji! ai (j i) b=(9)i=0expression right last line corresponds following combinatorial analysis f 4 . goal gjj , successful execution involve sequenceplan executions thatfail (for i, 0 j 1) followed one plan executionsucceeds. ji ways choosing failed plans, ordered i! ways,plan = n8(pj ) ways fail. j ways choosing finalsuccessful plan, b = n4(pj ) ways succeed.goal find explicit characterisation incremental effect addingextra plan n8(gjj ) n4(gjj ) finding definitions f 8 f 4 recurrence relationsterms parameter j. Deriving recurrence relation f 8 straightforward:f 8 (j, a) = j! aj = (j (j 1) . . . 21) (a. . a}) = (j a) ((j 1) a) . . . (2 a) (1 a)| .{zj times90fiOn Testability BDI Agent Systemsn4(gjj ) = f 4 (j, n8(pj ), n4(pj ))n8(gjj ) = f 8 (j, n8(pj ))f 4 (0, a, b) = 0f 4 (j +1, a, b) = (j +1) (b + f 4 (j, a, b))(10)8f (0, a) = 18f (j +1, a) = (j +1) f 8 (j, a)n4(phi ) = 1n8(phi ) = `n4(pj ) = n4(gj )k , j 6= hin4(gj )k 1, j 6= hin8(pj ) = ` + n8(gj ) + ` n4(gj )n4(gj ) 1Figure 6: Recurrence relations numbers failures successes goal plan treepresence failure handlingshows f 8 (0, a) = 1 f 8 (j +1, a) = (j +1) f 8 (j, a)However, derivation recurrence relation f 4 simple. usetechnique first finding exponential generating function (e.g.f.) (Wilf, 1994)sequence {f 4 (j, a, b)}j=0 , using derive recurrence relation. detailsgiven Appendix B, yield equation 10 Figure 6.Equation 10 (copied Equation 25 Appendix B) gives us recurrence relation14sequence {f 4 (j, a, b)}j=0 seeking . Figure 6 brings togetherequations far failure-handling case (including previoussection defining n4(pd ) n8(pd ), generalised semi-uniform trees).formulation gives us different way looking recurrence, allows useasily see behaviour space grows number applicable plans, j,goal grows. Considering meaning parameters b numbers failuressuccesses (respectively) plan level current goal node, equationf 4 (j +1, a, b) seen following combinatorial interpretation. One planmust selected try initially (there j +1 choices) either succeed (in oneb different ways), meaning plans need tried, fail (in one differentways). fails, goal must succeed using remaining j plans,occur f 4 (j, a, b) ways.see growth number successful executions goal growsrate greater j!aj , presence b term. relaxed uniformity14. simple case = b = 1 listed sequence A007526 On-Line EncyclopediaInteger Sequences (Sloane, 2007): number permutations nonempty subsets {1, , n}.91fiWinikoff & Cranefieldconstraint used recurrence relations also gives us way investigate numberstraces goal-plan trees different semi-uniform shapes. However, remainderpaper focus uniform trees using original parameter j (with exceptionSection 4.7).4.5 Probability FailingSection 4.3 said introducing failure handling makes harder fail. However,Tables 1 2 appear first glance contradict this, many waysfailing failure handling without failure handling.key understanding apparent discrepancy consider probabilityfailing: Tables 1 2 merely count number possible execution paths, withoutconsidering likelihood particular path taken. Working probabilityfailing (as below) shows although many ways failing (and alsosucceeding), probability failing is, indeed, much lower.Let us denote probability execution goal-plan tree root x depthfailing p8(xd ), probability succeeding p4(xd ) = 1 p8(xd ).assume probability action failing 15 . probabilitygiven plans actions succeeding simply (1 )x x number actions.Hence probability plan failing failure (one of) actions simply1 (1 )x , i.e. plan depth 0 probability failure is:0 = 1 (1 )`plan depth greater 0 probability failure due actions is:= 1 (1 )` (k+1)(recall plan ` actions before, after, between, k sub-goals).Considering actions also sub-goals g1 , . . . , gk plan p,plan succeed, sub-goals must succeed, additionally, plansactions must succeed giving p4(pd ) = (1 ) p4(gd )k . easily deriveequation p8(pd ) (given below). Note reasoning applies plan regardlesswhether failure handling, failure handling done goal level.absence failure handling, goal g possible plans p1 , . . . , pj succeedmust select one plan execute it, probability success probabilityplan succeeding, i.e. p4(gd ) = p4(pd1 ). ignore moment possibilitygoal failing applicable plans. assumption relaxed later on.Formally, then, case without failure handling:p8(gd ) = p8(pd1 )p8(p0 ) = 0kp8(pd ) = 1 [(1 ) (1 p8(gd )) ]15. simplicity, assume failure action plan independent failureactions plan.92fiOn Testability BDI Agent Systems0.050.01234234failure handling30%72%98%07%22%55%failure handling0.64%0.81%0.86%0.006%0.006%0.006%Table 3: Goal failure probabilities without failure handlingconsider happens failure handling added. case, ordergoal fail, plans must fail, i.e. p8(gd ) = p8(pd1 )j . Since failure handlinggoal level, equation plans unchanged, giving:p8(gd ) = p8(pd1 )jp8(p0 ) = 0kp8(pd ) = 1 [(1 ) (1 p8(gd )) ]easy see equations patterns probabilities actually are,so, illustration purposes, Table 3 shows probability failure is,without failure handling, two scenarios. values computed using j = k = 3(i.e. relatively small branching factor) ` = 1. consider two cases:= 0.05 hence 0.185 (which rather high), = 0.01 hence0.04.seen, without failure handling, failure magnified : larger goalplan tree is, actions involved, hence greater chance actionsomewhere failing, leading failure top-level goal (since failurehandling). hand, failure handling, probability failure low,doesnt appear grow significantly goal-plan tree grows.relax assumption goal cannot fail applicableplans, i.e. goal fail plans tried. Unfortunately, relaxingassumption complicates analysis need consider possibility noneremaining plans applicable point failure handling attemped.Let us begin reconsidering case failure handling. use gdenote probability goal failing none remaining plans applicable.case failure handling non-zero g indicates situationsgoal applicable plans, may indicate error partprogrammer, certain situations goal may possible achieve.assume, analysis purposes, probability constant, particular,depend plans already tried number relevantplans remaining.probability goal failing p8(gd ) = g + (1 g ) p8(pd1 ), i.e. goal failseither plans applicable applicable plans selectedplan fails. before, equation plans unchanged, since failure handling done93fiWinikoff & Cranefieldgoal level. following equations case without failure handling:p8(gd ) = g + (1 g ) p8(pd1 )p8(p0 ) = 0kp8(pd ) = 1 [(1 ) (1 p8(gd )) ]Observe setting g = 0 yields equations derived earlier, assumedgoal cannot fail due inapplicable plans.consider probability failure failure handling. goal twoplans following cases:goal fail plans applicable (g )applicable plans ((1 g ) . . .) goal fail first selectedplan fails (p8(pd1 ) . . .) failure handling successful, occureither applicable plans (g ) applicable plans ((1 g ) . . .)selected plan fails (p8(pd1 )).Putting together, goal two plans have:p8(gd ) = g + (1 g ) p8(pd1 ) (g + (1 g ) p8(pd1 ))general case j available plans, goal fail if:A. applicable plans outset, probability g ,B. applicable plans (1 g ), selected plan fails (p8(pd1 ))either applicable plans (g ),C. applicable plans (1 g ), selected plan fails (p8(pd1 ))either applicable plans (g ),D. on: reasoning B repeated j times.gives definition following form:g + (1 g ) p8(pd1 ) (g + (1 g ) p8(pd1 ) (g + . . .g ))|{z} |{z}|{z} |{z}BCdefined terms auxiliary function p8(gd , i) defines probabilityfailure goal g depth remaining relevant plan instances may(or may not) yield applicable plan instances:p8(gd ) = p8(gd , j)p8(gd , 1) = g + (1 g ) p8(pd1 )p8(gd , + 1) = g + (1 g ) p8(pd1 ) p8(gd , i)p8(p0 ) = 0kp8(pd ) = 1 [(1 ) (1 p8(gd )) ]94fiOn Testability BDI Agent Systems0.050.01234234failure handlingg = 0g = 0.01 g = 0.0530%33%43%72%76%86%98%99%100%g = 0 g = 0.005 g = 0.017%9%10%22%27%32%55%63%70%failure handlingg = 0g = 0.01 g = 0.050.64%2.2%9.4%0.81%2.6%12.8%0.86%2.8%16.5%g = 0 g = 0.005 g = 0.010.006%0.5%1.1%0.006%0.6%1.1%0.006%0.6%1.1%Table 4: Goal failure probabilities without failure handling goalsapplicable plansObserve setting g = 0 reduces definition derived earlier, since g +(1g ) Xsimplifies X, hence p8(gd , i) = p8(pd1 )i .before, immediately clear formulae actual patternsprobability are. Considering illustrative examples, Table 4 shows (a) overall behaviour before, (b) g assumed relatively low comparedprobability action failure ( 0 ), doesnt significantly affect probabilities.4.6 Analysis Rate Failuressection briefly examine number traces goal-plan tree affectedplacing bound rate action failures occur within trace. simplicity,work uniform goal-plan trees, construction extends trivially semiuniform goal-plan trees.Figure 6 presented equations calculating total number behavioursgoal-plan tree (with failure handling). many behaviours involve possiblyunrealistic number action failures? make assumption upperlimit rate action failures16 , i.e. number failures divided lengthtrace, affect number possible behaviours? large numbersseen reduce significantly?instance, considering j = k = 2, ` = 1 = 2, 1,922 possible executionsresult failure. many involve high rate action failure manyinvolve small percentage failures? Figure 7 contains (cumulative) countsgenerated looking possible executions (small) case, plottednumber action failures. x axis shows given value N {0, . . . , 6} manytraces N fewer action failures. instance, N = 2, 426traces 2 fewer action failures. 426 traces, 328 successful 98unsuccessful. Figure 8 shows equivalent graph rate action failure: tracefailure rate computed (the number failures divided length trace),16. Bounding rate action failures allows us model assumption environment limitedunpredictability, perhaps programmer limited incompetence!95fiWinikoff & Cranefieldok"failed"both"3500"Number'of'traces'(cumula0ve)'3000"2500"2000"1500"1000"500"0"0"1"2"3"4"5"6"ok"8"80"328"704"960"1024"1024"failed"0"0"98"546"1282"1794"1922"both"8"80"426"1250"2242"2818"2946"Number'of'failures'Figure 7: Number traces (cumulative) vs. number failures j = k = 2, ` = 1, = 235003000Number traces (cumulative)250020001500100050000.10.20.30.40.50.60.70.80.91Failure RateFigure 8: Number traces (cumulative) vs. failure rate j = k = 2, ` = 1, = 296fiOn Testability BDI Agent Systemsnumber traces counted range failure rate. instance, firstdata point graph shows 40 traces failure rate 0.1.question generalise analysis larger execution spaces. Clearly,counting possible executions feasible. Instead, turn generating functions.given plan body segment17 (and particularly = gd ), interestedcomputing numbers successful failed traces failure rate boundedgiven ratio r number failed actions total number actions,i.e. proportion actions execution trace fail. denote n4r(s)n8r(s). compute values, first determine integers > 0 n 0numbers successful failed traces length contain exactly n actionfailures, denoted n4r(s, m, n) n8r(s, m, n), respectively. define length tracenumber actions (both successful unsuccessful) contains. Notefinite goal-plan tree, uniform semi-uniform one, maximumpossible trace length n4r(s, m, n) n8r(s, m, n) non-zero finitenumber integer pairs (m, n) positive quadrant plane positiveaxis (in case n = 0). values, calculate n4r(s)nr, similarly n8rs using n8r(s, m, n).sum n4r(s, m, n)begin considering ordinary 18 bivariate generating functions (Wilf, 1994)values n4r(s, m, n) n8r(s, m, n):4Fr (s, x, y) =Fr8(s, x, y) =XXn4r(s, m, n) xm nm=0 n=0XXn8r(s, m, n) xm nm=0 n=0action one successful execution, length 1 contains actionfailures, Fr4(a, x, y) = x (a power series coefficient x1 0 1coefficients 0). Similarly, Fr8(a, x, y) = x1 1 = xy, one failed execution,length 1 one action failure.consider Fr4(s1 ; s2 ):Fr4(s1 ; s2 , x, y) ==XXn4r(s1 ; s2 , m, n) xm nm=0 n=0XXm=0 n=0XXnr(s1 , p, t) nr(s2 , q, u) xm n44p+q=m t+u=ndouble sum parentheses considers, trace, possible ways allocatingnumber actions number action failures n (necessarily) successfulexecutions s1 s2 , sums non-negative integer values p, q, u.17. Recall that, defined towards start Section 4 (page 83), plan body segment sequencex1 ; . . . ; xn xi either goal action.18. Ordinary generating functions differ exponential generating functions including denominatorsfactorials powers variable(s).97fiWinikoff & Cranefieldhave:Fr4(s1 ; s2 , x, y) ==XXX Xn4r(s1 , p, t) xp n4r(s2 , q, u) xq um=0 p+q=m n=0 t+u=nXXXXn4r(s1 , p, t) xp n4r(s2 , q, u) xq up=0 q=0 t=0 u=0XXXX4p4q u=nr(s1 , p, t) xnr(s2 , q, u) xp=0 t=0q=0 u=044= Fr (s1 , x, y) Fr (s2 , x, y)PP PPsecond line derived using identityqpm=0p+q=m f (p, q) =f (p, q). expressions sum non-negative integers p q, first expressionfirst summing non-negative values horizontal axis,summing pairs (p, q) non-negative integers lying line slope 1intersects horizontal axis m.Considering Fr8(s1 ; s2 , x, y), have:Fr8(s1 ; s2 , x, y) ==XXn8r(s1 ; s2 , m, n) xm nm=0 n=0XXn8r(s1 , m, n)m=0 n=0+=XXnr(s1 , p, t) nr(s2 , q, u) xm n48p+q=m t+u=nXXn8r(s1 , m, n) xm nm=0 n=0XX+m=0 n=0XXnr(s1 , p, t) nr(s2 , q, u) xm n48p+q=m t+u=n8= Fr (s1 , x, y)XXXX+n4r(s1 , p, t) xpn8r(s2 , q, u) xq up=0 t=0q=0 u=0= Fr8(s1 , x, y) + Fr4(s1 , x, y) Fr8(s2 , x, y)second line based observation failed execution s1 ; s2length n action failures either failed execution s1 length naction failures occurring execution, successful execution s1 length pfailures followed failed execution s2 length q u failures,p + q = + u = n.Now, assuming know Fr4(gd , x, y) Fr8(gd , x, y) depth d,construct functions Fr4(pd , x, y) Fr8(pd , x, y) applying results expandright hand sides following equations (which simply replace pd plan body):98fiOn Testability BDI Agent SystemsFr4(pd , x, y) = Fr4(a` ; (gd ; a` )k , x, y)Fr8(pd , x, y) = Fr8(a` ; (gd ; a` )k , x, y)remains define Fr4(gd , x, y) Fr8(gd , x, y) terms Fr4(pd1 , x, y) Fr8(pd1 ,x, y). count successful executions gd length n action failures,must first choose one j applicable plans one ultimately succeeds.must choose 0 j1 remaining applicable plans triedfailed, consider possible orderings plans. actions tracen action failures must distributed across failed successful plans. leadsus following derivation procedure construct Fr4(gd , x, y):Fr4(gd , x, y)XX=n4r(gd , m, n) xm nm=0 n=0=XXjm=0 n=0= jp=0j1Xj 1pp=0= jj1Xj 1p!pXp!Xn4r(pd1 , `0 , f0 )pXXm=0 n=0n8r(pd1 , `i , fi ) xm ni=1`0 ++`p =m f0 ++fp =nXXn4r(pd1 , `0 , f0 )pn8r(pd1 , `i , fi ) xm ni=1`0 ++`p =m f0 ++fp =nj1Xp=0XXpXXj 14` f8` fp!nr(pd1 , `, f ) xnr(pd1 , `, f ) xp`=0 f =0`=0 f =0j1Xj 1= jp! Fr4(pd1 , x, y) Fr8(pd1 , x, y)ppp=0Constructing Fr8(gd , x, y) simpler. failed execution goal involves failed attemptsexecute j applicable plans. j! orderings plans must considered.gives us following construction Fr8(gd , x, y):Fr8(gd , x, y) ==XXn8r(gd , m, n) xm nm=0 n=0XXXj!m=0 n=0= j!XXX`1 ++`j =m f1 ++fj =n8` fnr(pd1 , `, f ) x`=0 f =0= j! Fr8(pd1 , x, y)j99jnr(pd1 , `1 , f1 ) nr(pd1 , `j , fj ) xm n88fiWinikoff & Cranefieldequations define recursive procedure computing Fr4(gd , x, y) Fr8(gd ,x, y) given values d, j, k `. discussed earlier section, given wayncalculating n4r(s, m, n), calculate n4r(s) sum n4r(s, m, n)r,8similarly nrs. used Python rmpoly GMPY2 libraries generatepolynomial representations functions Fr4(gd , x, y) Fr8(gd , x, y) specifiedvalues d, j, k l, calculate n4r(s) n8r(s) various ratios r19 . Figure 9shows results = j = k = 3 ` = 1.Examining Figure 9 conclude two things. one hand, number tracesreally explodes larger rates action failures. example, Figure 9 tracesfailure rate greater 0.4. hand, although making assumptionsfailure rate reduce number possible traces, number traces still quitelarge (note scale y-axis). instance, failure rate 0.1 around4.8 1044 failed executions 8.7 1047 successful executions. failure rate 0.2respective numbers 1.0 1077 6.7 1077 , failure rate 0.31.2 1096 2.7 1096 .shape Figure 9 explained follows. Firstly, occurrence actionfailure triggers activity (alternative plans), failures result longer traces.Secondly, longer traces shorter traces, simplylonger trace, possibilities variations (e.g. different orders tryingplans). explains increase Figure 9 starts slowly accelerates:get failures, longer traces, longer tracesthem. word, plot non-cumulative number pathsratio action failures would see initial increase: ratio grows,paths. doesnt explain beyond certain point get fewer traces,cumulative graph levels out. explanation quite simple: beyond certainratio (which appears around 0.4) successful traces, numberfailed traces also declines.4.7 Recursive TreesSection 4.4 developed recurrence relations allowed us relax assumptiongoal-plan trees uniform, considered semi-uniform trees. sectionrelax assumption goal-plan trees finite, also allow treesshape. considering arbitrary trees allowed contain labelsrefer parts tree, i.e. allow trees recursive. derive generatingfunctions, seen extension derived previous section,number paths (both successful unsuccessful) executing recursive goalplan trees. Obviously, infinite tree infinite number paths, definegenerating functions take parameter bound lengths paths counted.19. finite number actions attempted execution goal-plan tree,bounds length possible traces number action failures occur within them.Thus Fr4(gd , x, y) Fr8(gd , x, y) polynomials finite orderonly finite number coefficientsnon-zero infinite sums define them.100fiOn Testability BDI Agent SystemsFailed executions (cumulative)Successful executions (cumulative)(cumulative)4E+1073.5E+107Number traces (cumulative)3E+1072.5E+1072E+1071.5E+1071E+1075E+1060.0050.0250.0450.0650.0850.1050.1250.1450.1650.1850.2050.2250.2450.2650.2850.3050.3250.3450.3650.3850.4050.4250.4450.4650.4850.5050.5250.5450.5650.5850.6050.6250.6450.6650.6850.7050.7250.7450.7650.7850.8050.8250.8450.8650.8850.9050.9250.9450.9650.9850Failure rateFigure 9: Number traces (cumulative) vs. failure rate j = k = = 3 ` = 1given upper bound path length equations specify number pathsmany actions20 .begin defining notation representing recursive trees: goals, plan-bodymultisets, plans, variables bindings. goal represented term formgoal(plan-body-multiset) plan-body-multiset multiset representing differentapplicable plan instances used satisfy goal. multisetcombinatorial analysis, structure plans significant. Thereforeuse single abstract action represent actions21 , goal may achievableusing multiple plan instances structure, must treatdistinct. need represent bodies plan instances, elementmultiset (i.e. plan) sequence terms separated right-associative sequentialcomposition operator ;. term sequence either abstract action term a,goal term defined (representing sub-goal), label (see below). Formally,plan-body multiset P multiset plans, written {p1 :c1 , . . . , pj :cj }ci number times associated plan pi appears multiset.define following multiset operations: set(P ) set pi multiset P ,P (pi ) characteristic function denoting number times plan pi appearsmultiset (i.e. ci ); P M1 P M2 multiset subtraction, defined P M1 P M2 (x) =max(P M1 (x) P M2 (x), 0). Finally |P | size multiset, i.e. sum ci .20. also use equations derived section non-recursive trees, case allow= , define 1 = F power(x) = F .21. However, avoid confusion, use numeric subscripts (a1 , a2 , . . .) distinguish different occurrences actions.101fiWinikoff & Cranefieldorder allow recursive trees represented, possible step planlabel (denoted , 0 ) referring term provided binding, simplymapping labels terms (either goal plan terms). b binding writeb[] denote item mapped b, entry b.example, consider simple tree below, consisting goal two plans, togetherbinding maps variable root tree. first plan (on left)two steps: action (a1 ), recursive reference root tree ().second plan (on right) single action (a2 ).: goalplanplana1a2recursive tree represented follows. define binding b = { 7goal({(a1 ; ):1, a2 :1})} maps whole tree, tree .proceed defining generating functions, introduce auxiliary notation. P power series use standard notation [xp11 xpnn ]P denotecoefficient term xp11 xpnn series. define P cond denote powerxnseries containing terms P satisfy condition cond. define f g(f g) power(x)n , i.e. f g terms power x greater n removed.ndefine f mx (f )m power(x)n , i.e. (f )m terms power x greatern removed.22position derive generating functions specify numberpaths arbitrary, possible recursive, goal-plan tree, given boundpath length. define BDI program represented term (i.e. goal, plan,plan multiset, action, label), b binding mapping labels terms (as definedabove). define n4(s, m, n, b) number successful paths, respectbinding b, actions, n failed actions. Similarly definen8(s, m, n, b) number failed paths, respect b, actions,n failed actions. want derive recurrence relations generatingfunctions23 :4F(s, x, y, b, ) =8F(s, x, y, b, ) =XXm=0 n=0XXn4(s, m, n, b)xm nn8(s, m, n, b)xm nm=0 n=0upper bound number actions path.22. This, previous operation, directly supported rmpoly Python library multivariatepolynomials series, used compute generating functions.23. subscript used distinguish generating functions, allow recursive tree,generating functions defined elsewhere paper.102fiOn Testability BDI Agent Systemsorder simplify presentation, details complex derivationsgiven Appendix C. resulting equations shown Figure 10. first twoequations (Equations 11 12 Figure 10), applicable term t, captureassumption > 0 (and remaining equations apply > 0). nexttwo equations simply specify labels looked provided binding. Equation 15indicates single successful path action a, singleaction unsuccessful actions (i.e. generating function 1x1 0 ). Equation 16similarly indicates single unsuccessful path single action a, which,unsurprisingly, single unsuccessful action (so generating function 1x1 1 ).Equations 17 18 deal sequences: sequence s1 ; s2 succeed s1s2 must succeed, count paths concatenating sub-paths, correspondsmultiplying power series. sequence s1 ; s2 fail either s1 fails, s1 succeedss2 fails (alternatives correspond addition power series). equationsspecial case: s1 action, divide overall path-length limitprecisely: s1 must trace length 1 (since action) s2 must thereforemaximum length 1.dealt labels (), single actions, sequences, next turn goals (equations 19 20). cases derivation complex, covered Appendix C.4F(Equation 19 Appendix C.1), intuition successful pathgoals execution involves single successful plan p, number failed executionsplan selected remaining multiset plans (P {p:1}). case planappears multiset, select occurrences, hencemultiplication P (p). number failed paths goal (Equation 20Appendix C.2) introduce auxiliary generating function G8(P M, x, y, z, b, ),P multiset plans, z variable whose power z indicates exact number plans P used. words, given power series denotedG8(P M, x, y, z, b, ), term cmno xm n z /o! indicates cmno paths involve actions, n failed, exactly plans P . generating8function G8 technical device allows us derive definition Fneed.8Given power series, definition F simply selects terms |P |power z (since plans must fail goal fail) using removesz |P | terms dividing. G8 exponential generating function z,means includes division factorial, need multiply factorial |P |!remove it.8Equation 21 defines F(P M, x, y, b, ), used Equation 19, terms8auxiliary function G. derivation given Appendix C.3. intuitionpossible number plans could used (o) limit power series G8value o, remove z dividing. o! due G8 exponentialgenerating function z (see Appendix C).Finally, Equations 22 23 give definition G8(P M, x, y, z, b, ) (see Appendix C.4derivation). Intuitively, Equation 22 creates power series plan type,xcombines (using ). Equation 23 little complex: single wayfailing (when plans used, corresponding term x0 0 z 0 = 1). Otherwiseselect c plans, plans must fail (corresponding term103fiWinikoff & Cranefield4F(t, x, y, b, ) = 0 0(11)8F(t, x, y, b, ) = 0 0(12)44(13)88(14)F(, x, y, b, ) = F(b[], x, y, b, )F(, x, y, b, ) = F(b[], x, y, b, )4(15)8(16)F(a, x, y, b, ) = xF(a, x, y, b, ) = xy4F(s1 ; s2 , x, y, b, )(44F(s1 , x, y, b, 1) F(s2 , x, y, b, 1) s1 action=x44F(s1 , x, y, b, ) F(s2 , x, y, b, ) otherwise8F(s1 ; s2 , x, y, b, )(488(s1 , x, y, b, 1)+F(s1 , x, y, b, 1) F(s2 , x, y, b, 1) s1 actionF=x848F(s1 , x, y, b, )+F(s1 , x, y, b, ) F(s2 , x, y, b, ) otherwise4F(goal(P ), x, y, b, )Xx48=P (p)F(p, x, y, b, ) F(P {p:1}, x, y, b, )(17)(18)(19)pset(P )8F(goal(P ), x, y, b, ) = |P |!|P |8F(P M, x, y, b, ) =Xo=0o!G8(P M, x, y, z, b, ) power(z)=|P |z |P |G8(P M, x, y, z, b, ) power(z)=ozo(20)(21)G8({p1 :c1 , . . . , pj :cj }, x, y, z, b, )xxG8({p1 :c1 }, x, y, z, b, ) G8({pj :cj }, x, y, z, b, )=cXcG({p:c}, x, y, z, b, ) = 1 +F 8 (p, x, y, b, )ox z8(22)(23)o=1Figure 10: Equations Recursive Goal-Plan Trees8F(p, x, y, b, )), giving number failed traces across plans as:xx888F(p, x, y, b, )ox = F(p, x, y, b, ) F(p, x, y, b, )|{z}timesused Python rmpoly GMPY2 libraries generate polynomial repre48sentations functions F(t, x, y, b, ) F(t, x, y, b, ) (as defined Figure 10)specified values t, b, . defined simple tree (the one given earliersection example) computed number paths different values .104fiOn Testability BDI Agent Systemsvalues chosen correspond values Table 2 (whichvalues n4(g) n8(g) come from24 ). Table 2, values 62 363 correspondlongest path, argue comparing recursive tree uniform tree,consider path length limit. results shown Table 5.62363n4(g)6.33 10121.02 10107n8(g)1.82 10132.56 10107n4(s)3.8 10131.9 1076n8(s)4.3 1096.1 1054Table 5: Comparing n4 n4 (respectively n8 n8).Looking numbers Table 5, worth noting recursive treeused extremely simple: two plans, single action. low numberactions (and sparseness tree) account relatively low number unsuccessfulpaths. instance, modify tree adding extra actions (giving treebinding below) = 62 around 3.9 1013 successful paths, 1.5 1011unsuccessful paths. Unfortunately, Python unable calculate n4 n8 tree= 363, manage = 362, 1.26 1064 successful paths,3.281063 unsuccessful paths. shows, expected, number unsuccessfulpaths higher complex tree. fewer successful pathscomplex tree explained observing that, tree, traces longer (moreactions need done), traces excluded bound tracelength .: goalplanplana1a3a2a4Overall, analysis section, application = 62 363 confirmsnumber paths recursive tree depends trees structure (whichunsurprising), also indicates even simple recursive tree, numberpaths given upper bound path length quickly becomes extremely large.5. Reality Checkprevious section analysed abstract model BDI execution order determinesize behaviour space. analysis yielded information sizebehaviour space affected various factors, probability goalfailing.section consider two issues whether analysis faithful, whetherapplicable real systems. analysis made number simplifying assumptions,24. correspond first two rows table, respectively involve 62 363 actions.105fiWinikoff & Cranefieldmean results may faithful semantics real BDI platform,may apply real systems. thus conduct two reality checks assesswhether analysis faithful (Section 5.1) whether applicable (Section 5.2).firstly assess whether analysis faithful real BDI platforms, i.e.omit significant features, contain errors. comparing abstractBDI execution model results real BDI platform, namely JACK (Busetta et al.,1999). comparison allows us assess extent analysis abstract BDIexecution model matches execution takes place real (industrial strength) BDIplatform. comparison is, essence, basic reality check: simply checkinganalysis previous section indeed match execution semantics typicalBDI platform. modelling artificial goal-plan tree BDI platform.Next, order assess extent analysis results apply real systems,analyse goal-plan tree real industrial application. analysis allows usdetermine extent conclusions analysis uniform (and semi-uniform)goal-plan trees applies real applications, goal-plan trees likelyuniform. words, extent large numbers Tables 1 2 applyreal applications?5.1 Real Platformorder compare real BDI platforms execution results abstract BDIexecution model implemented two goal-plan trees Appendix JACK agentprogramming language25 . structure plans events26 precisely mirrorsstructure tree. goal-plan tree, event two relevant plans,always applicable, selectable either order. Actions implementedusing code printed action name, then, depending condition (describedbelow), either continued execution triggered failure (and printed failure indicator):System.out.print("a"); // Action "a"((N.i & 1)==0) {System.out.print("x");false; // trigger failure}conditions determined whether action failed succeeded, planselected first, controlled input (N.i, Java class variable). test harnesssystematically generated inputs, thus forcing decision options explored.results matched computed Prolog code Figure 3, giving preciselysix traces smaller tree, 162 traces larger tree.indicates abstract BDI execution model indeed accurate descriptiontakes place real BDI platform (specifically JACK).Note selected JACK two reasons. One modern, well known,industry-strength BDI platform. other, important, reason, JACK descendent line BDI platforms going back PRS, thus good representative25. code available upon request authors.26. JACK models goal BDIGoalEvent.106fiOn Testability BDI Agent SystemsParametersNumberj kgoalsactions2 232162 (13)3 3391 363 (25)Workflow 57 goals(*)(*) paper says 60 goals,Figure 11 57 goals.failure handling(secs 4.1 4.2)n4(g)n8(g)1286141,594,323 6,337,425294,912 3,250,604294,912 1,625,302294,912812,651failure handling(Section 4.3)n4(g)n8(g)6.33 10121.82 10131071.02 102.56 101072.98 10209.69 1020156.28 108.96 10159.66 10116.27 1011Table 6: Illustrative values n4(g) n8(g) (bottom part ` = 4 first row, ` = 2second, ` = 1 last row)larger family BDI platforms. words, showing BDI execution modelanalysed matches JACKs model, also able argue matches executionJACKs predecessors (including PRS dMARS), close relatives (e.g. UM-PRSJAM).5.2 Real Applicationconsider extent real systems deep branching goal-plan trees,extent large numbers shown Tables 1 2 apply real applications,rather uniform goal-plan trees. example real application considerindustrial application Daimler used BDI agents realise agile business processes(Burmeister, Arnold, Copaciu, & Rimassa, 2008). Note finding suitable applicationsomewhat challenging: need application real (not toy system). However,order able analyse it, application BDI-based, furthermore,details applications goal-plan tree need available. Unfortunately, manyreported BDI-based industrial applications provide sufficient detailsinternals allow analysis carried out.Figure 11 shows27 goal-plan tree work Burmeister et al. (2008)60 achieve goals 7 levels. 10 maintain goals, 85 plans 100 contextvariables (Burmeister et al., 2008, p. 41). Unlike typical goal-plan trees used BDIplatforms, tree Figure 11 consists layers and-refined goals,refinements leaves (where plans are). terms analysis presentedpaper treat link goal g set goals, say, g1 , g2 , g3equivalent goal g single plan p performs g1 , g2 , g3 (and actions,i.e. ` = 0 non-leaf plans).last row Table 6 gives various n values goal-plan tree, ` = 4 (toprow), ` = 2 (middle row) ` = 1 (bottom row). Note figures actuallylower bounds assumed plans depth 0 simple linear combinations` actions, whereas clear Burmeister et al. (2008) plans fact27. details meant legible: structure matters.107fiWinikoff & CranefieldmodelLS/ABdifferemodelkeep thfiguretree6:fromgoalACMprototypeFigure 11: Goal-planthehierarchywork Burmeisteret al. (2008,Figure 6) (reproducedpermission IFAAMAS)advantage modeling approach implicitly offerssupport parallel execution process partsdepend other. reduce overall time neededcomplicated, contain nested decision making (e.g., see Burmeister et al., 2008,processexecution. Moreover maintain goals good meansFigure 4).provideprocessagentmonitorsroughtheindicationsizeaadditionalgoal-plan tree isagility:numbergoals.57 goals,tree Figuresizetofirstthroughouttwo rows TableComparing(e.g.conditionsthat11fulfilledthe6. processnumber possible behaviours uniform goal-plan trees real (and nontimeconstraints)pro-activelyactivitiesavoiduniform)goal-plan tree,seebehaviour initiatesspace somewhatsmallerrealtree, thatbeforestilltheyquite appear.large, especially case failure handling. However,problemsnote following points:development prototype support rapid1. tree Figure 11 plans leaves, reduces complexity.prototypingexecution process models providedwords goal-plan tree typical plans alternatinggoals wouldlarger numberpossiblebehaviours.LS/ABPMprovenofveryhelpful.developed modelsrepresentliving process models, directly executed2. figures tree conservative estimate, since assume leaf plansvisualized.part ofIninterfacethatcalculatedcoupledsimplebehaviour.otherwebwords,usernumberpathstheisactualnumber directlypaths thefromreal application.under-estimateworkflowgeneratedprocess model.interface computed directly parameters6. Comparison Procedural Programscorresponding task: context variables, types possibleorder argue BDI programs harder test non-agent programs, needvalues.approachofprocessprograms,quicklycomparison.Specifically,need changesanalyze numberpaths non-agentcompareandtested.agentprograms.us addressconcernmodeledThuserrorsThistheallowmodelsbethediscoveredpaths criterion test suite adequacy always requires infeasibly large numbercorrectedbrieflyshorttests.sectiondoestime.this, analyzing number paths proceduralprogram.stated starting point building ACM-prototypemodel ACM-reference 108process model developedsoftware demonstrator. underlying agent enginedemonstrator (JadeX) partially different modelingexecution semantics compared LS/ABPM tool.modelprototycompldependchallenexecutdependprocesBasedconcontexmanipmodelplans,complpossibvariabvariaboneprocesvariabstartingoals.createhmodelfiOn Testability BDI Agent SystemsNumber actions / statements62363776627n(m)6,973,568,8025.39 10572.5 101235.23 1099n4(g)6.33 10121.02 101071.82 101573.13 10184n8(g)1.82 10132.56 101077.23 101577.82 10184Table 7: Comparison values n(m), n4(g) n8(g).define program composed primitive statements s, sequences statements P1 ; P2 , conditionals select two sub-programs. Since captureconditions statements, elide condition, write conditional P1 + P2 indicating one Pi selected. Note that, BDI analysis, exclude loops.define number paths program P n(P ). straightforward28 seedefinition n(P ) is:n(s) = 1n(P1 ; P2 ) = n(P1 ) n(P2 )n(P1 + P2 ) = n(P1 ) + n(P2 )order compare BDI programs, consider size program,compare programs size. key question is: procedural programnodes significantly fewer paths BDI program size? definesize program P number primitive statements contains, denote|P |. Note means count internal nodes syntax tree(i.e. + ;). Therefore, comparing BDI programs, consider sizeBDI program number actions29 .work number paths varies size program P .size program (and therefore natural number), define n(m) max{n(P ) :|P | = m}. is, n(m) largest number paths possible program size m.Appendix contains derivation n(m), resulting following definition (where6 multiple 3):n(1)n(3)n(5)n(m + 1)====136433m/3n(2)n(4)n(m)n(m + 2)====243m/32 3m/3Table 7 shows comparison values n(m) n4(g) n8(g), same-sizedprograms, based Table 2. worth emphasising n(m) highest possible value:defined maximum possible programs. However, maximal programhighly atypical. example, considering programs seven statements,28. path P1 ; P2 simply concatenates path P1 path P2 , hence product; pathP1 + P2 either path P1 path P2 , hence addition.29. Using total number nodes tree yields almost identical results.109fiWinikoff & Cranefieldtotal 8,448 possible programs. 8448 programs, 32 12 paths (themaximum). Figure 12 shows number paths (112) many programsmany paths. maximum 12 clearly typical: indeed, mean number pathsseven statement program 4.379, median 4. consider programs9 statements, 366,080 programs, 16 maximalnumber paths (which 27). average number paths across programs5.95.Overall, looking Table 7, conclude number paths BDI programsmuch larger even (atypical) maximal number paths procedural programsize. supports conclusion BDI programs harder testprocedural programs.2500"Number'of'programs'2000"1500"1000"500"0"1"2"3"4"5"6"7"8"9"Number'of'paths'in'a'procedural'program'10"11"12"Figure 12: Profile number paths 7-statement programs7. Conclusionsummarise, analysis found space possible behaviours BDI agents is,indeed, large, absolute sense, relative sense (compared proceduralprograms size).expected, number possible behaviours grows trees depth (d) breadth(j k) grow. However, somewhat surprisingly, introduction failure handling makessignificant difference number behaviours. instance, uniform goalplan tree depth 3 j = k = 2, adding failure handling took number successfulbehaviours 128 6,332,669,231,104.consider negative consequences analysis, worth highlightingone positive consequence: analysis provides quantitative support long-held belief110fiOn Testability BDI Agent SystemsBDI agents allow definition highly flexible robust agents. Flexibilitydefined number possible behaviours agent, shown large.Robustness defined ability agent recover failure. analysisSection 4.6 showed BDI failure recovery mechanism effective achieving lowrate actual failure (< 1%), even action reasonable chance failing (5%).analysis paper tell us testability BDI agent systems?answer question, need consider tested. Testingtypically carried levels individual components (unit testing), collectionscomponents (integration testing), system whole.Consider testing whole system. behaviour space sizes depicted Tables 1, 26 suggest quite strongly attempting obtain assurance systems correctnesstesting system whole feasible. reason (as discussedSection 1.1), adequate test suite (using paths criterion adequacy) requiresleast many tests paths program tested. program has, say,1013 paths, even test suite tens thousands tests inadequate,hugely inadequate, since covers tiny fraction percent numberpaths.fact, situation even worse consider number possibleexecutions also probability failing: space unsuccessful executions particularly hard test, since many unsuccessful executions (more successful ones),probability unsuccessful execution low, making part behaviourspace hard reach. Furthermore, shown Section 4.6, although making assumptionspossible numbers action failures occur given execution reducesnumber possible behaviours, still many many behaviours, even relativelysmall trees (e.g. j = k = = 3).system testing BDI agents seems impractical. unit testingintegration testing? Unfortunately, always clear apply usefullyagent systems interesting behaviour complex possibly emergent.example, given ant colony optimisation system (Dorigo & Stutzle, 2004), testing singleant doesnt provide much useful information correct functioning wholesystem. Similarly, BDI agents, testing sub-goal difficult ensuretesting covers situations goal may attempted. Consequently,difficult draw conclusions correctness goal results testingsub-goals.need acknowledge analysis somewhat pessimistic: real BDI systemsnecessarily deep heavily branching goal-plan trees. Indeed, treereal application described Section 5 smaller behaviour space abstractgoal-plan trees analysed Section 4. However, even though smaller, still quite large,cause problems validation:One big challenges test phase keep model consistentdefine right context conditions result correct executionscenarios. Therefore support dependency analysis, automated111fiWinikoff & Cranefieldsimulation testing process models needed (Burmeister et al., 2008,p. 42)30 .leave us respect testing agent systems? conclusionseems testing whole BDI system feasible. number possibleapproaches dealing issue testability could recommended:Keep BDI goal-plan trees shallow sparse. keeps number behaviours small. issue approach lose benefits BDIapproach: reasonably large number behaviours desirable providesflexibility robustness.Avoid failure handling. Since failure handling large contributor behaviour space, could modify agent languages disable failure handling. Again,useful approach disabling failure handling removes benefitsapproach, specifically ability recover failures.Make testing sophisticated. Could testing coverage perhaps improvedincorporating additional information domain knowledge, detailedmodel environment (which indicates possible failure modes probabilities)? answer known, potentially interesting areawork. However, large number paths encourage much optimismapproach.Another, related, direction see whether patterns exist behaviour space.Since failure recovery mechanism certain structure, mayresults behaviour space large, but, sense, structured.structure exists, may useful making agents testable. However,point time, research direction may may turn fruitful;viable testing strategy.Finally, related direction try intelligent selectiontest cases, order gain coverage given number test cases. Oneapproach this, recently described, evolutionary testing(Nguyen, Miles, Perini, Tonella, Harman, & Luck, 2009a), genetic evolutionused find good (i.e. challenging) test cases.Supplement testing alternative means assurance. Since testingable cover large behaviour space, consider forms assurance.promising candidate form formal method31 . Unfortunately, formalmethods techniques yet applicable industry-sized agent systems (we returnbelow, Section 7.1).30. Burmeister et al. made following observation: approach changes processquickly modeled tested. Thus errors models discovered corrected short time.discussing advantages executable models, arguing able executemodel allowed testing, useful detecting errors model. able executemodel undoubtedly useful, evidence given (nor specific claim made) testingsufficient assuring correctness agent system.31. See volume edited Dastani et al. (2010) recent overview current state-of-the-art,including chapter role formal methods assurance agents (Winikoff, 2010).112fiOn Testability BDI Agent SystemsProceed caution. Accept BDI agent systems general robust (duefailure-handling mechanisms), is, present, practical wayassuring behave appropriately possible situations. worthnoting humans similar respect. Whilst train, examinecertify human certain role (e.g. pilot surgeon), way assuringhe/she behave appropriately situations. Consequently, situationsincorrect behaviour may dire consequences, surrounding system needssafety precautions built (e.g. process double-checks information,backup system co-pilot).7.1 Future Workroom extending analysis Section 4. Firstly, analysis singlegoal within single agent. Multiple agents collaborating achieve single highlevel goal viewed shared goal-plan tree certain goals and/or plansallocated certain agents. course, distributed goal plan treeconcurrency. concurrency introduced, would useful consider whethercertain interleavings concurrent goals fact equivalent. Furthermore,considered achievement goals. would interesting consider types goals (vanRiemsdijk, Dastani, & Winikoff, 2008). Secondly, analysis focused BDI agents,one particular type agent. would interesting consider sortsagent systems, and, broadly, sorts adaptive systems.Another extension analysis consider criteria test suite adequacy.paper used paths criterion, arguing appropriate.recognize paths actually quite strong criterionit subsumes manycriteria (Zhu et al., 1997, Figure 7). alternative criterion could consideredges, also known branch coverage decision coverage. requireschoice program, statement, tests test suiteexercise options, i.e. edges program graph covered. edgescriterion weaker paths regarded generally accepted minimum(Jorgensen, 2002).Another area refinement analysis make less abstract. Two specific areascould made detailed resources environment. analysisconsider resources environment directly, instead, considers actions mayfail range reasons might include resource issues, environmental issues.analysis could extended explicitly consider resources interaction goals(Thangarajah, Winikoff, Padgham, & Fischer, 2002). could also extendedexplicit model environment.Whilst analysis consider real application, would desirable considerrange applications. could provide additional evidence analysis undulypessimistic, would also lead understanding variance goal-plan treescharacteristics across applications. key challenge finding suitable applicationsBDI-based, sufficiently complex (ideally real applications), detailed designinformation available (and preferably source code). Another challenge methodology:analysed shape goal-plan tree Daimler workflow application,113fiWinikoff & Cranefieldaccess run system. alternative methodology, requires accessimplemented system probably source code, run it, force generatetraces sub-goals32 (which would require modification either source codeunderlying agent platform). collected data shape real-worldindustrial applications, able analyse whether uniform semi-uniform goalplan trees good models types system, whether seek waysrelax uniformity assumption.importantly, highlighted difficulties assuring BDI agent systemstesting, need find ways assuring systems.approach promise automatic generation test cases agentsystems (Nguyen, Perini, & Tonella, 2007; Zhang, Thangarajah, & Padgham, 2009). However, size behaviour space suggests number test cases needed maylarge, testing failed plan execution difficult. One interesting,potentially promising, avenue use formal techniques help guide test generationprocess (e.g. symbolic execution specification-guided testing) (Dwyer, Hatcliff, Pasareanu, Robby, & Visser, 2007).Another approach33 attracted interest model checking agent systems(Wooldridge, Fisher, Huget, & Parsons, 2002; Bordini, Fisher, Pardavila, & Wooldridge,2003; Raimondi & Lomuscio, 2007). work promising model checking techniques use range abstractions cover large search space without dealindividual cases one-at-a-time (Burch, Clarke, McMillan, Dill, & Hwang, 1992; Fix,Grumberg, Heyman, Heyman, & Schuster, 2005). Furthermore, verifying subgoal considers possibilities, possible combine verification different sub-goals.However, work needed: Raimondi Lomuscio (2007) verify systems agentsdefined abstractly, i.e. terms plans goals. MABLE agent programminglanguage (Wooldridge et al., 2002) actually imperative language augmented certain agent features, BDI language; work Bordini et al. (2003)include failure handling. general, state art model checking agent systemimplementations still limited quite small systems (Dennis, Fisher, Webster, & Bordini,2012).Acknowledgementswould like thank members Department Information Science UniversityOtago discussions relating paper. would also like thank Lin Padghamcomments draft paper. Finally, would like thank anonymousreviewers insightful comments helped improve paper.work paper done Winikoff sabbatical RMIT, visitingUniversity Otago.32. Generating traces top level goal likely feasible.33. also work deductive verification, (based research verificationconcurrent systems) appears less likely result verification tools (relatively)easy use applicable real systems.114fiOn Testability BDI Agent SystemsAppendix A. Example Goal-Plan Trees ExpansionsSuppose following two trees: sample (left) sample2 (right). treescorrespond j = 2, k = ` = 1, = 1 sample = 2 sample2.goalplangoalplangoalb cgoalplanplanplanplanplanplanbefghtrees expanded respectively following sequences actions,letter indicates execution action, 8 indicates failure34 . predictedformulae, four successful executions two unsuccessful executionsfirst tree:ba8bb8aa8b8b8a8second tree, expansions following 162 possibilities (consisting 64successful 98 unsuccessful traces).eeeeeeeeeeeeeeeeeeeebb8cgdb8cgd8b8cg8hdb8cg8hd8b8cg8h8b8chdb8chd8b8ch8gdb8ch8gd8b8ch8g8b8c88fb8fb8cgd8fb8cgd88fb8cg8hd8fb8cg8hd88fb8cg8h88fb8chd8fb8chd8ffffffffffffffffffffb8chdb8chd8b8ch8gdb8ch8gd8b8ch8g8b8c88eb8eb8cgd8eb8cgd88eb8cg8hd8eb8cg8hd88eb8cg8h88eb8chd8eb8chd88eb8ch8gd8eb8ch8gd88eb8ch8g88eb8c88e8cgd8e8cgd8ccccccccccccccccccccggggggggggggggggggggd8d8d8d8d8d8d8d8d8d8d88h8h8h8h8h8h8h8h8haebaeb8ae8fbae8fb8ae8f8afbafb8af8ebaf8eb8af8e8a8d8aebd8aeb8d8ae8fbd8ae8fb8d8ae8f8d8afbd8afb8d8af8ebcccccccccccccccccccchhhhhhhhhhhhhhhhhhhhd8afb8d8af8ebd8af8eb8d8af8e8d8a88gd8gd8aeb8gd8aeb88gd8ae8fb8gd8ae8fb88gd8ae8f88gd8afb8gd8afb88gd8af8eb8gd8af8eb88gd8af8e88gd8a88g8aeb8g8aeb88g8ae8fb34. Note failure marker isnt counted considering length trace Section 4.6.115fiWinikoff & Cranefielde8fb8ch8gde8fb8ch8gd8e8fb8ch8g8e8fb8c8e8f8cgde8f8cgd8e8f8cg8hde8f8cg8hd8e8f8cg8h8e8f8chde8f8chd8e8f8ch8gde8f8ch8gd8e8f8ch8g8e8f8c8fbfb8cgdfb8cgd8fb8cg8hdfb8cg8hd8fb8cg8h8af8e8cg8hdaf8e8cg8hd8af8e8cg8h8af8e8chdaf8e8chd8af8e8ch8gdaf8e8ch8gd8af8e8ch8g8af8e8c8a8cgda8cgd8a8cg8hda8cg8hd8a8cg8h8a8chda8chd8a8ch8gda8ch8gd8a8ch8g8a8c8cgdcccccccccccccccccccccg8hd8af8eb8g8hd8af8e8g8hd8a8g8h8aebg8h8aeb8g8h8ae8fbg8h8ae8fb8g8h8ae8f8g8h8afbg8h8afb8g8h8af8ebg8h8af8eb8g8h8af8e8g8h8a8hdhd8aebhd8aeb8hd8ae8fbhd8ae8fb8hd8ae8f8hd8afbccccccccccccccccccch8g8ae8fb8h8g8ae8f8h8g8afbh8g8afb8h8g8af8ebh8g8af8eb8h8g8af8e8h8g8a88aeb8aeb88ae8fb8ae8fb88ae8f88afb8afb88af8eb8af8eb88af8e88a8Appendix B. Analysis Recurrence Relationsappendix contains details derivation Section 4.4.exponential generating function F (x) sequence {f 4 (j, a, b)}j=0 functiondefined following power series:F (x) =Xf 4 (j, a, b)j=0xjj!(24)(by definition f 4 )!!j1jXXXXxxjjj=i!ai (j i)b=i!ai (j i)bj!j!j=0i=0j=0i=0right hand sidechanged upper limit inner sum basedjgeneralised definition j(j 1)(j 2) . . . (j + 1)/i!, validcomplex numbers j non-zero integers (Wilf, 1994) gives ji = 0 > j.right hand side form product exponential generating functions (Wilf,1994, Rule 30 , Section 2.3):!jjXXXXxxjxj(j)(j) =(i)(j i)j!j!j!j=0j=0j=0i=0where, case, (j) = j! aj (j) = j b. Therefore, write:jjXX(ax)xF (x) =j!jbj!j!j=0j=0116fiOn Testability BDI Agent SystemsP1left hand sum G(ax) G(y) = n n = 1y(Wilf, 1994, Equation 2.5.1)35 .Pnxx0right hand sum equal bx dxn! (Wilf, 1994, Rule 2 , Section 2.3) = bx dx ex(Wilf, 1994, Equation 2.5.3) = bxe . Thus have:F (x) =1bxexbxex =1 ax1 axPxj4Therefore, f 4 (0, a, b) constant term power seriesj=0 f (j, a, b) j! ,F (0) = 0. find recurrence relation defining f 4 (j + 1, a, b) equate originaldefinition F (x) Equation 24 closed form function, differentiateside (to give us power series f 4 (j, a, b) values shifted one position left),multiply denominator closed form, giving us following derivation.X 4bxexxj(1 ax)= (1 ax)f (j, a, b)dxj!dx 1 axj=0Xxj1b(x+1)exabxex4= (1 ax)f (j, a, b)j= (1 ax)+j!1 ax(1 ax)2j=0=Xj=0bxexxj1 X 4xj= b(x+1)ex +f (j, a, b)jaf (j, a, b)jj!j!1 ax4j=04(recall f (0, a, b) = 0)XXxjxj4=f (j +1, a, b)(j +1)ajf 4 (j, a, b)(j +1)!j!j=0j=0= bxex + bex +Xf 4 (j, a, b)j=0(recallbxex=j=0Equating coefficientsxjj!xjj=0 jb j! ,jXex =j=0j=0PXxj=bj +bj!xjj!xjj=0 j! )PXxjx+af 4 (j, a, b)j!j!get:f 4 (j +1, a, b) ajf 4 (j, a, b) = bj + b + af 4 (j, a, b)= f 4 (j +1, a, b) = b(j +1) + af 4 (j, a, b) + ajf 4 (j, a, b)= (j +1)(b + af 4 (j, a, b))(25)35. Note many operations performed generating functions (and used paper)valid without concern convergence series. combinatorics, generating functions oftentreated analytic functions evaluated specific variable values, rather formal (possiblyinfinite) algebraic objects, well defined operations addition multiplication.set formal power series finite set variables structure ring abstract algebra,ring notion function convergence evaluation (Wilf, 1994, ch. 2).117fiWinikoff & CranefieldAppendix C. Analysis Recursive Goal-Plan Treesappendix contains detailed derivations relating Section 4.7.4C.1 Derivation F(goal(P ), x, y, b, )4define F(goal(P ), x, y, b, ) terms n4 usual way, noting upperbound realise length bound:4F(goal(P ), x, y, b, ) =XXn4(goal(P ), m, n, b)xm nm=0 n=04n(goal(P ), m, n, b) defined Section 4.7. also make use nonbounded version (which four arguments):4F(goal(P ), x, y, b) =XXn4(goal(P ), m, n, b)xm nm=0 n=04define n counting successful traces:n4(goal(P ), m, n, b)X=P (p)Xn4(p, m1 , n1 , b) n8(P {p:1}, m2 , n2 , b)m1 +m2 =mn1 +n2 =npset(P )n8(P M, m, n, b) number unsuccessful paths using zero plansplan multiset P (with respect binding b) actions, nfailed actions.inner sum considers ways partition numbers actions (m) actionfailures (n) caused single plan shape p PPcausedplans.Section4.6(page98)useidentitym=0p+q=m f (p, q) =P Pp=0q=0 f (p, q) rewrite:4F(goal(P ), x, y, b)XXXX=P (p)n4(p, m1 , n1 , b) n8(P {p:1}, m2 , n2 , b)xm nm1 +m2 =mn1 +n2 =nm=0 n=0 pset(P )X=P (p)pset(P )XXXn4(p, m1 , n1 , b) n8(P {p:1}, m2 , n2 , b)xm nm=0 n=0 m1 +m2 =mn1 +n2 =ngive us:4F(goal(P ), x, y, b)X=P (p)pset(P )XXXXn4(p, m1 , n1 , b)xm1 n1 n8(P {p:1}, m2 , n2 , b)xm2 n2m1 =0 m2 =0 n1 =0 n2 =0118fiOn Testability BDI Agent Systems=XP (p)pset(P )XX4n(p, m1 , n1 , b)x=n8(P {p:1}, m2 , n2 , b)xm2 n2m2 =0 n2 =0m1 =0 n1 =0XXXm1 n148P (p) F(p, x, y, b) F(P {p:1}, x, y, b)pset(P )8F(P M, x, y, b) generating function n8(P M, m, n, b). Section C.38provide definition F(P M, x, y, b) terms auxiliary function G8 (see Section C.4).introduce bound length paths giving:4F(goal(P ), x, y, b, )Xx48=P (p) (F(p, x, y, b) F(P {p : 1}, x, y, b))pset(P )=Xx48P (p) (F(p, x, y, b, ) F(P {p : 1}, x, y, b, ))pset(P )8C.2 Derivation F(goal(P ), x, y, b, )Similarly previous derivation, define:8F(goal(P ), x, y, b, ) =XXn8(goal(P ), m, n, b)xm nm=0 n=08(goal(P ), . . . ) terms plans P ,derive recursive definition F8first define new function n(P M, m, n, o, b), denotes number unsuccessfulpaths use plans multiset P . have:n8(goal(P ), m, n, b) = n8(P M, m, n, |P |, b)states goal fail, |P | plans multiset must tried.define generating function G8(P M, x, y, z, b, ) n8(P M, m, n, o, b) ordinary x exponential z, i.e. coefficients xm n z /o! valuesn8(P M, m, n, o, b).have:8F(goal(P ), x, y, b, ) =XXn8(P M, m, n, |P |, b)xm nm=0 n=0wish rewrite terms G8 . generalising right hand sidesum possible values number plans used (o), followed restrictionselect values = |P |:119fiWinikoff & Cranefield8F(goal(P ), x, y, b, )= |P |!XXn8 (P M, m, n, |P |, b) z |P ||P |! z |P |xm nm=0 n=0XP8Xo=0 n(P M, m, n, o, b)z /o! power(z)=|P ||P |!xm n|P |zm=0 n=0=P= |P |!P P 8nm=0n=0o=0 n(P M, m, n, o, b)x z /o!power(z)=|P |z |P |Since nested sum definition G8 (see Section C.4), simplify to:8F(goal(P ), x, y, b, ) = |P |!G8(P M, x, y, z, b, ) power(z)=|P |z |P |8Section C.4 derive definition G8(P M, . . .) terms F(p, . . .) pset(P ).8C.3 Definition F(P M, x, y, b, )Recall n8(P M, m, n, b) number unsuccessful paths using zeroplans plan multiset P (with respect binding b) actions, n8failed actions, F(P M, x, y, b, ) ordinary generating function.First consider case P empty. case, precisely one way8fail, generates trace length zero. Therefore, F({}, x, y, b, ) = 1x0 0 = 1.case P non-empty sum number plans usedexecution, yields following definition:|P |8n(P M, m, n, b) =Xn8(P M, m, n, o, b)o=0n8(P M, m, n, o, b) is, before, number unsuccessful paths plan8multiset, using plans. Therefore, using definition F(P M, x, y, b, ) =P PP8nn(PM,m,n,o,b)x,have:m=0n=08F(P M, x, y, b, )=M|X|PXXn8(P M, m, n, o, b)xm nm=0 n=0 o=0(replace n8 looking coefficient corresponding term G8 ,o! accounts division o! G8 ; also reorder summations)|P |=XX Xo![xm n z ]G8(P M, x, y, z, b, )xm no=0 m=0 n=0120fiOn Testability BDI Agent Systems(we shift o! outwards, multiply z /z )P|P | Pn8nXn=0 [x z ]G(P M, x, y, z, b, )x zm=0=o!zoo=0|P |=Xo=0o!G8(P M, x, y, z, b, ) power(z)=ozoC.4 Definition G8(P M, x, y, z, b, )define G8(P M, x, y, z, b, ) generating function n8(P M, m, n, o, b) ordinary x exponential z (hence division o! below), ( 0)maximum allowed trace length:G8(P M, x, y, z, b, ) =XXXn8(P M, m, n, o, b)m=0 n=0 o=0xm n zo!Recall n8(P M, m, n, o, b) denotes number unsuccessful paths useplans multiset P . empty multiset plans successful execution,single unsuccessful execution 0 actions, uses 0 plans, hence:(1 = n = = 08n({}, m, n, o, b) =0 otherwiseTherefore, G8({}, x, y, z, b, ) = 1. non-empty multisets must partition actionstrace, action failures, numbers plans used, across different plan bodiesmultiset, also consider ways plans various plan shapesinterleaved give overall order attempting plans:n8({p1 :c1 , . . . , pj :cj }, m, n, o, b)X=n8({p1 :c1 }, m1 , n1 , o1 , b) n8({pj :cj }, mj , nj , oj , b)m1 ++mj =mn1 ++nj =no1 ++oj =omultinomial coefficientThus:o1 ...oj=o!o1 !...oj !G8({p1 :c1 , . . . , pj :cj }, x, y, z, b, )(by definition G8, using restriction, rather bounded sum m,expanding n8 above)XX=n8({p1 :c1 }, m1 , n1 , o1 , b)...1j++m =mm,n,o=01jn1 ++nj =no1 ++oj =oxm n zn({pj :cj }, mj , nj , oj , b)o!8121!power(x)fiWinikoff & Cranefield=XXo!n8({p1 :c1 }, m1 , n1 , o1 , b)!...!1j=mm,n,o=0 m1 ++mjn1 ++nj =no1 ++oj =oxm n zn8({pj :cj }, mj , nj , oj , b)o!!power(x)(cancelling o! distributing oi ! xmi , ni z oi )=XXn8({p1 :c1 }, m1 , n1 , o1 , b)m,n,o=0 m1 ++mj =mn1 ++nj =no1 ++oj =oxm1 n1 z o1o1 !xmj nj z ojn({pj :cj }, mj , nj , oj , b)oj !!8(replacingXXm=0 m1 +m2 =m=XXXredistributing sums)m1 =0 m2 =0n8({p1 :c1 }, m1 , n1 , o1 , b)xm1 n1 z o1m1 ,n1 ,o1 =0Xpower(x)o1 !n8({pj :cj }, mj , nj , oj , b)xmj nj z ojmj ,nj ,oj =0oj !power(x)x(replacing restriction )nX111x z x=n8({p1 :c1 }, m1 , n1 , o1 , b)o1 !m1 ,n1 ,o1 =0nXjjjxx zn8({pj :cj }, mj , nj , oj , b)oj !mj ,nj ,oj =08(by definition G . . . )xx= G8({p1 :c1 }, x, y, z, b, ) G8({pj :cj }, x, y, z, b, )need define G8({pi :ci }, x, y, z, b, ).Consider n8({p:c}, m, n, o, b). simple cases = 0: useplans, single unsuccessfulpath, actions (m = n = 0).c!hand, > 0 oc = o!(co)!ways selecting c availablecopies plan p. selected plans executed o! different orders.execution sum possible distributions actions (successful unsuccessful)122fiOn Testability BDI Agent Systemsamongst plans. gives:Xco!n8(p, m1 , n1 , b) n8(p, mo , , b)m1 ++mo =mn1 ++no =nn8({p:c}, m, n, o, b) =1= n = = 00otherwise> 0therefore following definition G8({p:c}, x, y, z, b, ), initial 1 abbreviates 1x0 0 z 0 /0!, i.e. base case = n = = 1, restdefinition G8 , expanding n8 using definition.G8({p:c}, x, y, z, b, )= 1+Xco!XXn8(p, m1 , n1 , b) n8(p, mo , , b)1 ++mo =mn1 ++no =nm=0 n=0,o=1xm n zo!(cancel o!/o!, rearrange sums replace upper boundrestriction)XXXXc88n= 1+n(p, m1 , n1 , b) n(p, mo , , b)xpower(x) zo=1m=0 n=0m1 ++mo =mn1 ++no =nX(replacingXm=0 m1 +m2 =m= 1+XXredistributing sums)m1 =0 m2 =0Xco=1XX!n8(p, m1 , n1 , b)xm1 n1m1 =0 n1 =0XX!!n8(p, mo , , b) xmomo =0 =0power(x) z!oXXXc= 1+n8(p, m, n, b) xm npower(x) zm=0 n=0o=1XX8(Replacen8(p, m, n, b)xm n F(p, x, y, b, ) per definition)ncXc= 1+F 8 (p, x, y, b, )o power(x) zo=1cXcF 8 (p, x, y, b, )ox z= 1+o=1123fiWinikoff & CranefieldAppendix D. Analysis Procedural Code Structuresseek derive expression largest possible number paths programgiven size have, i.e. definition n(m) = max{n(P ) : |P | = m}. Recallprogram either (atomic) statement single path (i.e. n(s) = 1), sequencetwo programs P1 ; P2 n(P1 ; P1 ) = n(P1 ) n(P2 ), conditional P1 + P2n(P1 + P2 ) = n(P1 ) + n(P2 ).relatively easy see examining possible programs 3n(m) = m. instance, largest number paths = 3 obtained program+ + s. also easy show = 4 largest number paths possible 4.larger values m? observe > 4 program36largest number paths follows particular form. = 5 programlargest path written P5 = (s + + s); (s + s), n(P5 ) = 3 2.generally, define S2 + s, S3 + + s, followingresult, shows programs maximal number paths size,considered particular form.Theorem D.1 program size (for > 4) largest possible numberpaths written Pi = Pi1 ; Pi2 ; . . . ; Pik Pij (1 j k) either S2S3 .Proof: establish result induction. assume holds n4 < n m, show must also hold + 1. So, let us assumeprogram Pm+1 maximal number paths, formj12kPm+1; Pm+1; . . . ; Pm+1Pm+1either S2 S3 . two cases, dependingstructure Pm+1 . consider case turn show fact either (a)Pm+1 rewritten desired form, preserving number pathsprogram size; (b) Pm+1 cannot maximal, since construct program size+ 1 larger number paths Pm+1 .j12kCase 1: Pm+1 form Pm+1; Pm+1; . . . ; Pm+1least one Pm+1neitherS2 S3 . Let Pm+1 one sub-programs neither S2 S3 . conveniencedefine P shorthand Pm+1. Now, since P size less + 1, inductionhypothesis applies37 , written form Pi1 ; Pi2 ; . . . Pil Pijeither S2 S3 . easy see one rewrite Pm+1 desired formexploiting associativity ;, rewriting follows:i1i+1i1i+1. . . Pm+1; (Pi1 ; Pi2 ; . . . Pij ); Pm+1; . . . = . . . Pm+1; Pi1 ; Pi2 ; . . . Pij ; Pm+1;...Applying rewriting Pm+1S2 S3 yields program size+ 1, number paths original program, desired form:sequence sub-programs, either S2 S3 . shows result holds+ 1, i.e. maximal-path program written desired form.12kCase 2: Pm+1 form Pm+1; Pm+1; . . . ; Pm+1k, means1kPm+1 must consist single conditional, i.e. Pm+1 = Pm+1 + . . . + Pm+1k > 1.36. fact one maximal-path program, structure, moduloswapping order arguments + ;.37. Or, size 4, written S2 ; S2 maximal number paths programsize 4 meets desired form.124fiOn Testability BDI Agent Systems12Without loss generality view Pm+1 form Pm+1+ Pm+1(by viewing1k1kPm+1 + . . . + Pm+1 (Pm+1 + . . .) + Pm+1 k > 2). consider following12sub-cases, depending values n(Pm+1) n(Pm+1).12Case 2a: n(Pm+1) n(Pm+1) greater 2. show Pm+1012maximal. Consider program Pm+1= Pm+1; Pm+1(i.e. + replaced1212;). know n(Pm+1 ; Pm+1 ) = n(Pm+1 ) n(Pm+1). Without loss gen12erality, lets assume n(Pm+1 ) n(Pm+1 ). show original Pm+101fewer paths Pm+1. number paths Pm+1 n(Pm+1 ) = n(Pm+1)+21212n(Pm+1 ). Since n(Pm+1 ) n(Pm+1 ), n(Pm+1 ) = n(Pm+1 ) + n(Pm+1 )22221n(Pm+1) + n(Pm+1) = 2 n(Pm+1). Since n(Pm+1) n(Pm+1) greater21202, 2 n(Pm+1 ) < n(Pm+1 ) n(Pm+1 ) = n(Pm+1), i.e.0Pm+1 paths Pm+1 , hence Pm+1 maximal + 1.12Case 2b: least one n(Pm+1) n(Pm+1) greater 2. Without loss121generality, assume n(Pm+1 ) n(Pm+1). two cases: n(Pm+1)either 2 1.1) = 1.Sub-case 2b(i): Let us consider first case n(Pm+1program one path statement s, sequence statements s; s; . . . ; s.Clearly latter maximal since replacing + + . . . + wouldresult program size paths. So, therefore Pm+1122maximal, Pm+1must s, Pm+1 = + Pm+1. Therefore Pm+1size m. two sub-cases: either still greater 4, = 4.second sub-case simple: 4 show, inspecting possible2programs size 4, n(4) = 4, therefore n(s + Pm+1)1 + 4 = 5. However, also know (s + + s); (s + s) size 5 6 paths,hence sub-case Pm+1 cannot maximal number paths.first sub-case, still greater 4, induction hypothesis applies22written desired form. abbreviate Pm+1therefore Pm+1j12P2 , Pm+1 = + (P2 ; P2 ; . . . ; P2 ) P2 either S200S3 . Consider variant program Pm+1= ((s + P21 ); P22 ; . . . P2j ),00clearly size Pm+1 . show Pm+1pathsj00122Pm+1 : n(Pm+1 ) = ((1 + n(P2 )) n(P2 ; . . . ; P2 )) = n(P2 ; . . . ; P2j ) + (n(P21 )n(P22 ; . . . ; P2j )). Now, n(Pm+1 ) = 1 + (n(P21 ) n(P22 ; . . . ; P2j )). order show00n(Pm+1 ) < n(Pm+1) need show 1 < n(P22 ; . . . ; P2j )follows fact must least one P2i , that, since P2ieither S2 S3 , size least 2.121Sub-case 2b(ii): know n(Pm+1) = 2 2 n(Pm+1). Since n(Pm+1)222n(Pm+1 ) n(Pm+1 ) 2 hence n(Pm+1 ) = 2 + n(Pm+1 )2022 n(Pm+1) = n(Pm+1). Now, n(Pm+1) strictly greater 20n(Pm+1 ) strictly less n(Pm+1 ) shown Pm+1 actually2maximal number paths. hand, n(Pm+1)=22n(Pm+1 ) = 2 + n(Pm+1 ) = 2 + 2 = 4. However, valuestheorem applies, know n(m) > 4, thereforeshown sub-case Pm+1 maximal + 1.125fiWinikoff & Cranefieldshown assume Pm+1 maximal structurespecified, fact one derive another program, also size + 1, eithersatisfy desired structure, larger number paths Pm+1 , contradicts assumption Pm+1 maximal. establishes desired property Pm+1 .induction result applies > 4, desired.previous result shows considering programs given sizelargest possible number paths (denoted Pm ), limit consideringprograms form P1m ; P2m ; . . . ; Pkm Pim either + + + s.derive definition n(m). Firstly, observe that, inspecting cases:n(m) = m, 4n(5) = 6n(6) = 9two first cases discussed above. last case, two programsappropriate structure size 6: S2 ; S2 ; S2 (with 8 paths) S3 ; S3(with 9 paths).consider > 6. Adding statement program (i.e. going m+1)effect modifies Pm adding one Pim , increments n(Pim ) one.Since multiplication commutative associative, without loss generality, assume) n(P )increment n(Pkm ). therefore n(Pm ) = n(P1m ; . . . ; Pk1kn(Pm+1 ) = n(P1 ; . . . ; Pk1 ) (n(Pk ) + 1). two cases:)3 thereforeCase 1: Pim S3 , n(Pm ) = n(P1m ; . . . ; Pk1) 4 = n(P ) 4 . Note case Pn(Pm+1 ) = n(P1m ; . . . ; Pk1m+13written P1 ; . . . ; Pk1 ; S2 ; S2 .Case 2: Pim S2 S3 observe replacing 2 3 givesgreater increase number paths replacing 3 4, hence (after)2possibly reordering Pim Pkm = S2 ) n(Pm ) = n(P1m ; . . . ; Pk13n(Pm+1 ) = n(P1 ; . . . ; Pk1 ) 3 = n(Pm ) 2 .therefore recursive definition n(m) depending form Pm . nextobserve fact form Pm follows simple cycle. know = 6, case 1holds (as above, P6 = S3 ; S3 ). therefore P7 written S3 ; S2 ; S2 , henceP8 written S3 ; S3 ; S2 S3 ; S2 ; S3 , hence P9 written S3 ; S3 ; S3 .generally, prove induction Pm written P1m ; . . . ; Pkmfollowing holds: (a) multiple 3, Pim S3 ; (b)one multiple 3, exactly two Pim S2 rest S3 ;(c) two multiple 3, exactly one Pim S2 restS3 . gives us following recursive definition, 6 multiple 3:n(m + 1) = n(m)43n(m + 2) = n(m + 1)12632fiOn Testability BDI Agent Systemsn(m + 3) = n(m + 2)32simplified to:4334n(m + 2) = n(m)= 2 n(m)23334= 3 n(m)n(m + 3) = n(m)223n(m + 1) = n(m)easily derive non-recursive definition focusing last case observingn(6) = 9 = 32 n(m + 3) = 3 n(m) (for 6 multiple 3),n(m) = 3m/3 . substitute definition obtainfollowing complete definition n(m), 6 multiple 3:n(1) = 1n(2) = 2n(3) = 3n(4) = 4n(5) = 6n(m) = 3m/34n(m + 1) =3m/33n(m + 2) = 2 3m/3127fiWinikoff & CranefieldReferencesBenfield, S. S., Hendrickson, J., & Galanti, D. (2006). Making strong business casemultiagent technology. Stone, P., & Weiss, G. (Eds.), Proceedings Fifth International Joint Conference Autonomous Agents Multiagent Systems (AAMAS),pp. 1015. ACM Press.Bordini, R. H., Fisher, M., Pardavila, C., & Wooldridge, M. (2003). Model checking AgentSpeak. Proceedings Second International Joint Conference AutonomousAgents Multiagent Systems (AAMAS), pp. 409416. ACM Press.Bordini, R. H., Hubner, J. F., & Wooldridge, M. (2007). Programming multi-agent systemsAgentSpeak using Jason. Wiley.Bratman, M. E., Israel, D. J., & Pollack, M. E. (1988). Plans resource-bounded practicalreasoning. Computational Intelligence, 4, 349355.Bratman, M. E. (1987). Intentions, Plans, Practical Reason. Harvard University Press,Cambridge, MA.Burch, J., Clarke, E., McMillan, K., Dill, D., & Hwang, J. (1992). Symbolic model checking:1020 states beyond. Information Computation, 98 (2), 142170.Burmeister, B., Arnold, M., Copaciu, F., & Rimassa, G. (2008). BDI-agents agile goaloriented business processes. Proceedings Seventh International ConferenceAutonomous Agents Multiagent Systems (AAMAS) [Industry Track], pp. 3744.IFAAMAS.Busetta, P., Ronnquist, R., Hodgson, A., & Lucas, A. (1999). JACK Intelligent Agents Components Intelligent Agents Java. AgentLink News (2).Dastani, M. (2008). 2APL: practical agent programming language. Autonomous AgentsMulti-Agent Systems, 16 (3), 214248.Dastani, M., Hindriks, K. V., & Meyer, J.-J. C. (Eds.). (2010). Specification VerificationMulti-agent systems. Springer, Berlin/Heidelberg.de Silva, L., & Padgham, L. (2004). comparison BDI based real-time reasoningHTN based planning. Webb, G., & Yu, X. (Eds.), AI 2004: Advances ArtificialIntelligence, Vol. 3339 Lecture Notes Computer Science, pp. 11671173. Springer,Berlin/Heidelberg.Dennis, L. A., Fisher, M., Webster, M. P., & Bordini, R. H. (2012). Model checking agentprogramming languages. Automated Software Engineering, 19 (1), 363.dInverno, M., Kinny, D., Luck, M., & Wooldridge, M. (1998). formal specificationdMARS. Singh, M., Rao, A., & Wooldridge, M. (Eds.), Intelligent Agents IV:Proceedings Fourth International Workshop Agent Theories, Architectures,Languages, Vol. 1365 Lecture Notes Artificial Intelligence, pp. 155176,Berlin/Heidelberg. Springer.Dorigo, M., & Stutzle, T. (2004). Ant Colony Optimization. MIT Press.Dwyer, M. B., Hatcliff, J., Pasareanu, C., Robby, & Visser, W. (2007). Formal software analysis: Emerging trends software model checking. Future Software Engineering2007, pp. 120136, Los Alamitos, CA. IEEE Computer Society.128fiOn Testability BDI Agent SystemsEkinci, E. E., Tiryaki, A. M., Cetin, O., & Dikenelli, O. (2009). Goal-oriented agent testingrevisited. Luck, M., & Gomez-Sanz, J. J. (Eds.), Agent-Oriented Software Engineering IX, Vol. 5386 Lecture Notes Computer Science, pp. 173186, Berlin/Heidelberg. Springer.Erol, K., Hendler, J., & Nau, D. (1996). Complexity results HTN planning. AnnalsMathematics Artificial Intelligence, 18 (1), 6993.Erol, K., Hendler, J. A., & Nau, D. S. (1994). HTN planning: Complexity expressivity.Proceedings 12th National Conference Artificial Intelligence (AAAI), pp.11231128. AAAI Press.Fix, L., Grumberg, O., Heyman, A., Heyman, T., & Schuster, A. (2005). Verifyinglarge industrial circuits using 100 processes beyond. Peled, D., & Tsay, Y.K. (Eds.), Automated Technology Verification Analysis, Vol. 3707 LectureNotes Computer Science, pp. 1125, Berlin/Heidelberg. Springer.Georgeff, M. P., & Lansky, A. L. (1986). Procedural knowledge. Proceedings IEEE,Special Issue Knowledge Representation, 74 (10), 13831398.Gomez-Sanz, J. J., Bota, J., Serrano, E., & Pavon, J. (2009). Testing debuggingMAS interactions INGENIAS. Luck, M., & Gomez-Sanz, J. J. (Eds.), AgentOriented Software Engineering IX, Vol. 5386 Lecture Notes Computer Science,pp. 199212, Berlin/Heidelberg. Springer.Huber, M. J. (1999). JAM: BDI-theoretic mobile agent architecture. ProceedingsThird International Conference Autonomous Agents (Agents99), pp. 236243.ACM Press.Ingrand, F. F., Georgeff, M. P., & Rao, A. S. (1992). architecture real-time reasoningsystem control. IEEE Expert, 7 (6), 3344.Jorgensen, P. (2002). Software Testing: Craftsmans Approach (Second edition). CRCPress.Lee, J., Huber, M. J., Kenny, P. G., & Durfee, E. H. (1994). UM-PRS: implementation procedural reasoning system multirobot applications. Proceedings Conference Intelligent Robotics Field, Factory, Service, Space(CIRFFSS94), pp. 842849. American Institute Aeronautics Astronautics.Mathur, A. P. (2008). Foundations Software Testing. Pearson.Miller, J. C., & Maloney, C. J. (1963). Systematic mistake analysis digital computerprograms. Communications ACM, 6 (2), 5863.Morley, D., & Myers, K. (2004). SPARK agent framework. ProceedingsThird International Joint Conference Autonomous Agents Multiagent Systems(AAMAS), pp. 714721, New York. ACM.Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., & Luck, M. (2006).Crossing agent technology chasm: Experiences challenges commercial applications agents. Knowledge Engineering Review, 21 (4), 345392.129fiWinikoff & CranefieldNaish, L. (2007). Resource-oriented deadlock analysis. Dahl, V., & Niemela, I. (Eds.),Proceedings 23rd International Conference Logic Programming, Vol. 4670Lecture Notes Computer Science, pp. 302316. Springer, Berlin/Heidelberg.Nguyen, C., Miles, S., Perini, A., Tonella, P., Harman, M., & Luck, M. (2009a). Evolutionary testing autonomous software agents. Proceedings 8th InternationalConference Autonomous Agents Multiagent Systems (AAMAS), pp. 521528.IFAAMAS.Nguyen, C. D., Perini, A., & Tonella, P. (2009b). Experimental evaluation ontology-basedtest generation multi-agent systems. Luck, M., & Gomez-Sanz, J. J. (Eds.),Agent-Oriented Software Engineering IX, Vol. 5386 Lecture Notes ComputerScience, pp. 187198, Berlin/Heidelberg. Springer.Nguyen, C. D., Perini, A., & Tonella, P. (2007). Automated continuous testing multiagent systems. Proceedings Fifth European Workshop Multi-Agent Systems(EUMAS).Padgham, L., & Winikoff, M. (2004). Developing Intelligent Agent Systems: PracticalGuide. John Wiley Sons.Paolucci, M., Shehory, O., Sycara, K. P., Kalp, D., & Pannu, A. (2000). planning component RETSINA agents. Jennings, N. R., & Lesperance, Y. (Eds.), Proceedings6th International Workshop Agent Theories, Architectures, Languages(ATAL), Vol. 1757 Lecture Notes Computer Science, pp. 147161, Berlin/Heidelberg. Springer.Pokahr, A., Braubach, L., & Lamersdorf, W. (2005). Jadex: BDI reasoning engine.Bordini, R. H., Dastani, M., Dix, J., & El Fallah Seghrouchni, A. (Eds.), Multi-AgentProgramming: Languages, Platforms Applications, chap. 6, pp. 149174. Springer.Raimondi, F., & Lomuscio, A. (2007). Automatic verification multi-agent systemsmodel checking via ordered binary decision diagrams. J. Applied Logic, 5 (2), 235251.Rao, A. S. (1996). AgentSpeak(L): BDI agents speak logical computable language.de Velde, W. V., & Perrame, J. (Eds.), Agents Breaking Away: ProceedingsSeventh European Workshop Modelling Autonomous Agents Multi-AgentWorld (MAAMAW96), Vol. 1038 Lecture Notes Artificial Intelligence, pp. 4255,Berlin/Heidelberg. Springer.Rao, A. S., & Georgeff, M. P. (1991). Modeling rational agents within BDI-architecture.Allen, J., Fikes, R., & Sandewall, E. (Eds.), Proceedings Second InternationalConference Principles Knowledge Representation Reasoning, pp. 473484.Morgan Kaufmann.Sardina, S., & Padgham, L. (2011). BDI agent programming language failure handling, declarative goals, planning. Autonomous Agents Multi-Agent Systems,23 (1), 1870.Shaw, P., Farwer, B., & Bordini, R. (2008). Theoretical experimental resultsgoal-plan tree problem. Proceedings Seventh International ConferenceAutonomous Agents Multiagent Systems (AAMAS), pp. 13791382. IFAAMAS.130fiOn Testability BDI Agent SystemsSloane, N. J. A. (2007). on-line encyclopedia integer sequences. http://www.research.att.com/njas/sequences/.Thangarajah, J., Winikoff, M., Padgham, L., & Fischer, K. (2002). Avoiding resourceconflicts intelligent agents. van Harmelen, F. (Ed.), Proceedings 15thEuropean Conference Artificial Intelligence (ECAI), pp. 1822. IOS Press.van Riemsdijk, M. B., Dastani, M., & Winikoff, M. (2008). Goals agent systems:unifying framework. Proceedings Seventh Conference Autonomous AgentsMultiagent Systems (AAMAS), pp. 713720. IFAAMAS.Wilf, H. S. (1994). generatingfunctionology (Second edition). Academic Press Inc., Boston,MA. http://www.math.upenn.edu/wilf/gfology2.pdf.Winikoff, M. (2010). Assurance Agent Systems: Role Formal Verificationplay?. Dastani, M., Hindriks, K. V., & Meyer, J.-J. C. (Eds.), SpecificationVerification Multi-agent systems, chap. 12, pp. 353383. Springer, Berlin/Heidelberg.Winikoff, M., Padgham, L., Harland, J., & Thangarajah, J. (2002). Declarative & procedural goals intelligent agent systems. Proceedings Eighth InternationalConference Principles Knowledge Representation Reasoning (KR2002), pp.470481, Toulouse, France. Morgan Kaufmann.Wooldridge, M. (2002). Introduction MultiAgent Systems. John Wiley & Sons,Chichester, England.Wooldridge, M., Fisher, M., Huget, M.-P., & Parsons, S. (2002). Model checking multi-agentsystems MABLE. Proceedings First International Joint ConferenceAutonomous Agents Multi-Agent Systems (AAMAS), pp. 952959. ACM Press.Zhang, Z., Thangarajah, J., & Padgham, L. (2009). Model based testing agent systems.Filipe, J., Shishkov, B., Helfert, M., & Maciaszek, L. (Eds.), Software DataTechnologies, Vol. 22 Communications Computer Information Science, pp.399413, Berlin/Heidelberg. Springer.Zhu, H., Hall, P. A. V., & May, J. H. R. (1997). Software unit test coverage adequacy.ACM Computing Surveys, 29 (4), 366427.131fi