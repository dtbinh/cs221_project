



primer

yoav goldberg

yoav goldberg gmail com


bar ilan israel


years emerged powerful
yielding fields
started textual
signals promising tutorial surveys
perspective
bring researchers tutorial
covers feed convolutional
recurrent
gradient


decade nlp dominated
logistic trained
sparse
switching
sparse dense
drop replacements
old classifiers barrier entry tutorial
nlp practitioners
tools methodology understand behind
tutorial
presenting unified
repeats lot material elsewhere
external advanced topics
primer intended comprehensive
develop machinery serve entry
aimed readers
creative nlp
behind advanced
advanced topics reader
book bengio courville
recommended




figoldberg



deliberately
tutorial vast acoustic

signals videos caption
caching
vocabularies embeddings
extent understand
unsupervised
fall
treatment
comprehensive
terminology
linguistic suffix
tag tagger

fed
entry refers
lot overloaded
primarily entry

bold letters matrices bold letters
matrices
superscript indices
rare
brackets exponentiated
unless stated
concatenation
multiplied matrices xw
somewhat lot
multiplied matrices wx trust reader adapt
reading

inspired benefits
diagrams drawn layered
transparent puts
nested din dout dout din
libraries



primer nlp

architectures
powerful kinds
architectures matched feed recurrent
feed layers
perceptron convolutional pooling
layers classifiers strengths
feed learners
drop replacement wherever learner
multiclass linearity
integrate trained embeddings superior managed
replacing parser feed straight feed replacement
coupled trained benefits ccg
dialog tracking
iyyer boyd graber daume
feed sentiment factoid
convolutional pooling layers
clues membership
clues places
ngram
johnson
indicators care
convolutional pooling layers indicators
regardless convolutional pooling promising
categorization sentiment
paraphrase
predicting office rev

chen manning weiss collins petrov pei ge chang
durrett klein
lewis steedman
henderson thomson young
gispert byrne
bengio vincent zhao fossum chiang
johnson
wang xu xu liu wang hao
kalchbrenner grefenstette blunsom kim
zeng liu lai zhou zhao dos santos xiang zhou
chen xu liu zeng zhao nguyen grishman
yin schutze
collobert weston bottou kavukcuoglu
dong wei zhou xu



figoldberg

movies critic reviews
character tags

regularities
similarities
pass
learner convolutional pooling architectures
encode capturing salient
sacrificing recurrent
architectures
preserving lot recurrent elman

generalizations recurrent
recurrent stacks dyer ballesteros ling
matthews watanabe sumita
recurrent tagging
sentiment noisy normalization dialog tracking character tags

constituency parse discourse
political parse sentiment
sentiment























cohn
gao pantel deng
dos santos zadrozny
notable mikolov khudanpur mikolov
khudanpur mikolov
vu schultz auli galley quirk zweig auli gao

cardie xu auli clark ling dyer black amir
luis
ney tamura watanabe sumita sutskever
vinyals le cho schwenk bengio

dyer watanabe sumita
wang liu sun wang wang
chrupala
seaghdha thomson su wen young
galley auli brockett ji mitchell nie gao dolan
ling
socher bauer manning
le zuidema zhu qiu chen huang
hovy
liu wei ji zhou wang
iyyer boyd graber resnik
socher wu chuang manning potts hermann blunsom
dong wei tan tang zhou xu
iyyer boyd graber socher daume



primer nlp


discussing pay
think feed
nn din dout
assigning
membership dout

dealing encodes
tags linguistic perhaps
biggest conceptual jump sparse
stop
hot dense
embedded
embeddings trained
nn

embeddings entries treated
trained
obtaining embeddings
embeddings
nlp feed

linguistic fk predicting

retrieve
combine concatenation summation

feed feed
biggest sparse
dense mapped
elaborate briefly
dense hot
benefits ids
dense lets kinds


embedded




figoldberg

sparse dense dog tag det
sparse receive dimensionality
dense embeddings
entries dimensionality
mappings come embedding



primer nlp

hot
dimensionality hot


dog dis thinking cat
dense
dimensionality


shared
benefit dense
toolkits sparse
technical obstacle resolved

benefit dense
clues worthwhile
similarities dog
cat handful
occurrences dog
tell anything occurrences cat dense
dog cat
strength assumes
somehow obtaining


correlations hot going correlations
tags verb
vb vbz behave concerned
worthwhile correlations gain strength
sharing circumstances
plentiful wish
gains
hot
pioneered collobert weston
collobert chen manning advocate dense trainable
embedding sparse
johnson
dense integral

sparse dense
sparse hot amounts
dense embedding
touch


figoldberg

bag
feed accommodate
extracts
concatenated

advance
unbounded
socalled bag cbow mikolov chen corrado dean
cbow bag
discard summing averaging embedding

cbow fk










variation cbow cbow
receive

fk












indicating

tf idf

serve informative
trigger
asked predict
trigger trigger
signal nlp setup distances encoded
binning distances associating
bin hot composed
indicator seem allocate entry
entry
encoded
hot dense cbow
bag
sparse indicator

predefined
purchase triggered
triggering commonly verbs slots filled
purchased purchased



primer nlp

bin trained zeng
dos santos zhu nguyen grishman

deals nlp
designer manually
introducing stating
stating tag stating tag
tag
crucial
transforming closer linearly separable
designer
spend lot coming
promises
linearity
care indicative alleviating

shawe taylor cristianini kernels
matsumoto designer
leaving kernels admitting
scales
linearly slow purposes

scales linearly
regardless
dimensionality
allocate established practices dimensionality
grow members probably
embeddings embeddings
dimensionality embedding
hundreds extreme thousands dimensionality
thumb

sharing
vocabulary
assigning


figoldberg


concatenate
distinguish
indicators treat differently
dog dog mostly
behave differently
behaves behaves
vocabularies
behave
something gained shared vocabulary



strength
scalar

columns thought embeddings
similarities
similarities
historical
dense popularized bengio
nlp
pioneering collobert weston colleagues embeddings
popularized chen
manning

feed
introduces feed popular brain
inspired metaphor triggered switches
feed
linearities
brain inspired metaphor
inspired
neurons metaphor neuron scalar
bengio collobert weston colleagues popularized
dense
lee
schwenk



primer nlp

neuron multiplies sums
passes neurons
forming neuron feed neurons
capable devices
neurons activation
wide precise
















































feed layers
feed drawn circle
neuron incoming arrows neurons outgoing arrows neurons arrow carries reflecting neurons
arranged layers reflecting flow incoming
arrows outgoing arrows
layers sigmoid shape
neurons middle layers logistic
exa neurons passing
neuron neurons
affine
brain metaphor intriguing distracting cumbersome
manipulate mathematically switch concise
neurons thought
thought
summing



figoldberg

implements
multiplication xw connection ith neuron
jth neuron wij transformed passed
xw


abandon brain metaphor exclusively

simplest perceptron
xw



rdin rdin dout

layers
perceptron mlp feed

nnmlp xw



rdin rdin

wise linearity
activation
transform
breaking xw din

transform
activation crucial
linearity
transformations
transformations linearities mlp
layers
nnmlp xw



perhaps clearer deeper intermediary

ith jth neuron wij
hj hj wij

neuron incoming connections
transformations



primer nlp

nnmlp
xw





transform outer
transform transforms
layers activation
forced dropped
layers transformations
affine architectures benefit
convolutional pooling layers layers
layers deep
deep
describing layers
din transform
dout dimensionality dimensionality
xw dimensionality din
dimensionality dout din din dout
dout
dout dout
scalar
consulting sign
dout associating
looking
entries
normalization
softmax
matrices transformations

responsible


white
mlp approximator

rn
feed squashing activation borel measurable




figoldberg


mlp architectures
learnability



layers
approximated layers unless layers

amounts
gradient descent layers
modest thousands
ideal definitely
benefit trying architectures mlp
mlp feed book bengio


linearities
linearity
linearity linearity
sigmoid tanh tanh
relu nlp researchers experimented linearities
cube tanh cube

sigmoid
sigmoid activation ex logistic
shaped transforming sigmoid
canonical linearity
layers listed
empirically

tangent tanh


tangent tanh ee
activation shaped trans
forming


primer nlp

tanh
tanh activation tanh
derivatives










relu
activation bengio
activation
excellent relu clips
despite simplicity
dropout regularization


relu







thumb relu tanh tanh
sigmoid
transformations
transformed
softmax


softmax





technical advantages relu sigmoid tanh activation
involve expensive importantly saturate sigmoid
tanh activation capped gradients
driving gradient relu activation
layers susceptible vanishing gradients
trained
activation nlp community
linearities cube activation
suggested chen manning linearities
feed predict
parser tanh cube activation tanh pei
linearities feed
parser
cube tanh cube activation motivated desire activation
applicability



figoldberg



softmax
entropy

softmax
multinomial logistic
entropy
embedding layers
ignored treating
nlp composed embeddings


embedding
concatenate

nnmlp nnmlp
nnmlp




embedding assumes embedding dimensionality

nnmlp nnmlp
nnmlp




essential
likewise treat embeddings
embedding lookup vocabulary
embedded thought
embedding embedded
zeros
ith hot
multiplication






primer nlp


cbow fk

















hot
elegant mathematically
hash embedding
going hot
tutorial
dense passed
familiar terminology remember
propagate gradients embedding


describing layers concatenated
concatenation affine
xu yv zw matrices affine
notations
sparse dense
sparse
embedding
fk















ignoring activation


xw







selects rows sums
embedding cbow
acts embedding
embedding
undergo activation passed
forces receive
embedding flexibility
dog dog


figoldberg

subtle comes feed
dense sparse seem
sight


stating predicting
minimize
assigns numerical scalar

attained

matrices biases commonly embeddings minimize
losses minimized
scalar
purposes restrict
gradients gradients advisable rely

lecun chopra huang lecun
huang bengio
commonly nlp
hinge
scalar intended
sign
sign hinge
margin svm




sign
hinge attempts
margin
hinge multiclass
hinge multiclass crammer singer
yn hot


arg





think scalar
hot



primer nlp

arg maxi arg maxi
multiclass hinge
multiclass yt yk



multiclass hinge attempts
margin
multiclass hinge losses intended
hinge losses
membership

variation hinge soft
hinge margin lecun
exp yt yk



categorical entropy
categorical entropy

yn multinomial
yn transformed
softmax activation membership
categorical entropy dissimilarity
predicted
entropy
entropy










hot
entropy simplified
entropy yt



attempts mass
transformed softmax
mass
decreasing mass
entropy
predict predicts
entropy
transformed softmax


figoldberg

losses
supervision
incorrect incorrect
arise

margin incorrect
margin nn nn



nn
incorrect margin
variation
exp nn nn



hinge auxiliary deriving trained embeddings
corrupted
collobert weston
selectional trained verb incorrect
weston usunier trained
trail triplets corrupted
gao
variation margin
dos santos

embeddings
embeddings
come

initialization
supervised treat embeddings
initialize embedding
tune
care initialization
vec mikolov mikolov sutskever
chen corrado dean initialize uniformly sampled


option

initialize uniformly sampled
initialization






primer nlp

initialization initialize embedding commonly tags
letters supervised unsupervised initialize
rare trained
treated commonly
treated initialized tuned
supervised

auxiliary tagging

predictors trained
utilize amounts
treat trained tune
option jointly objectives

unsupervised
auxiliary amounts
annotated maybe bootstrap auxiliary
resort unsupervised trained huge
amounts unannotated
supervised
supervision care practically
unlimited supervised raw hoping
care
behind unsupervised embedding

distributional
harris stating
supervised
predict predict
benefit embeddings amounts unannotated
supervised ideally
generalize
unseen unsupervised captures
intended
creating auxiliary raw inspired ando
ando



figoldberg

unsupervised embedding vec mikolov
glove pennington socher manning collobert weston
embeddings inspired
gradient deeply
evolved nlp communities
factorization levy goldberg levy
arguably auxiliary predicted
affects
auxiliary
software packages
deriving vec implementing
vec windows
vec glove implementing
glove trained download
tutorial noting embeddings
unsupervised wide nlp
initializing embeddings
objectives
formulate auxiliary
initialized
auxiliary
embeddings relating
embedding
inspired mikolov
kavukcuoglu glove pennington auxiliary
predict posed
setup trying


pairings
come differ
optimized collobert
weston margin feed
incorrect mikolov
bilinear predict
come
treated vec software package
objectives hyperparameters
levy goldberg dagan
https google com vec
https com
https org
nlp stanford projects glove



primer nlp



surrounding paragraph
parsed parser
neighbourhood parse

prefixes suffixes
embeddings originated
trained predict preceding bengio
auxiliary aim predict

auxiliary embeddings needlessly

care
embeddings ignoring


sliding auxiliary
looking middle

predict
cbow mikolov concatenation collobert weston
pairing
popularized mikolov
skip gram skip gram
mikolov pennington
sliding similarities windows tend topical similarities
dog grouped walking windows tend similarities
poodle walking approaching
positional windows cbow skip gram
treated equally distinction
farther
likewise distinction

positional indicating
indicating
positional
windows tend similarities
tendency grouping
functionally positional ling


figoldberg

dyer black
initialize tagging

normalization filter
capitalization dos santos gatti
skipping creation
windows rare
weigh
differently focusing trying predict
away
hyperparameters levy
paragraphs
skip grams cbow
paragraph

topical
receive

levy
goldberg bansal parsed
parser
proximity parse
similarities grouping
fill colors schools verbs movement
grouping grouping levy
goldberg
multilingual
option multilingual hermann blunsom
faruqui dyer aligned
bilingual ibm
giza software alignments
foreign aligned
alignments tend synonym receiving
relying alignments gouws
bengio corrado
embeddings hill cho jean bengio appealing
mix monolingual multilingual
creating kinds auxiliary
reducing somewhat undesired

primer nlp

antonyms hot cold tend receive
faruqui dyer
character
attempts
characters compose
character
benefit producing
character alphabet handful
matrices stored embedding
encountered dos santos gatti dos santos zadrozny
kim embedding convolutional
characters ling embedding
concatenation rnn lstm encoders
reading characters
tagging ballesteros
lstms ling beneficial
morphologically rich
deriving characters motivated encounter
embedding working characters alleviates
extent vocabulary characters
vocabulary working character
challenging characters
loose restricting oneself stay character
unnecessarily researchers middle

comprise embeddings sharing

forced rely solely
blunsom embedding
morphological comprise
unsupervised
morphological segmentation gao
embedding
letter trigrams


trying minimize
gradient roughly speaking repeatedly
gradient
opposite gradient
differ opposite


figoldberg

gradient gradient
descent sgd briefly pointers
reading gradient calculation central gradients
reverse mode differentiation
algorithmic gradient

gradient
gradient descent
sgd bottou lecun bottou muller
sgd receives parameterized
attempts


gradient descent
parameterized
yn

stopping met





gradients



minimize
repeatedly gradient
treated
opposite
gradient scaled
decay

calculated
rough wide aiming minimize
inaccurate gradients reducing
gradients
rise minibatch sgd
gradient
minibatch gradient
toward minibatch vary
wide gradients
decay sgd



primer nlp

minibatch gradient descent
parameterized
yn

stopping met

minibatch xm ym








gradients





besides gradients
minibatch opportunities
modest architectures gpus
properly decreasing
sgd converge optimum
optimize
optimum

parameterized
matrices embedding matrices
gradient sgd

gradients fortunately
backpropagation rumelhart hinton williams
lecun bottou bengio backpropagation
derivatives caching intermediary backpropagation
reverse mode differentiation
pearlmutter siskind bengio
reverse mode differentiation

sgd
sgd advanced sgd momentum nesterov momentum
sutskever martens dahl hinton nesterov
sgd gradients accumulated argue convexity manifested proliferation saddle pascanu cho
bengio explain despite




figoldberg

duchi hazan singer
hinton adam ba
minibatch coordinate
alleviating
book bengio
software frameworks implementations
worthwhile

gradients
implement cumbersome prone purposes preferable tools gradient bengio

pass gradients
scalar losses backward pass


flow intermediary

dependencies
continuations















shared restrict



mlp softmax oval shaded rectangle
treated drawn surrounding
incoming arcs outgoing arcs
dimensionality indicated

specifying
mlp predicts
tags
scalar



primer nlp


neg









pick







softmax

softmax

softmax



















mul

mul

mul


tanh












tanh






















mul

mul

mul









concat














tanh













concat















lookup

lookup

lookup

lookup

lookup

lookup



black

dog



black

dog











mlp unbound



embeddings
black dog
pick implements indexing receiving
returning entry
backward gradients
constructing daunting
dedicated software libraries apis

pass
incoming
traversing topological
predecessors


figoldberg

associate
topological multiplication





pass






backward derivatives backprop
backward pass begins scalar
backward

gradients quantity


backpropagation
backward pass fills
backward pass backpropagation



fj




fj
derivative fj

fj
pass

quantity


calculating calculating



differentiation
backpropagation
flow bengio
lecun bengio popular technical
presentation


primer nlp

software
software packages implement
cnn packages essential wide architectures covering
tutorial creation transparent
overloading
commonly constructing

python creating

import pc
initialization
pc
pw
pb
pw
pb
lookup

pc cg

pc pw
pc pb
pc pw
pc pb
def holder
embeddings

pc lookup
pc lookup black

pc lookup dog
connect
pc concatenate
pc softmax pc tanh
pc pc pick

backward gradient
stored


initializations
shared
turns retrieves embeddings





software
org
https org
https github com clab cnn



figoldberg

fourth transparent
creation correspondence creating
describing mathematically backward
pass software frameworks
optimizing compiler
curse compiled
gpu ideal
costly
interface cumbersome packages
fly
suffer optimized
packages convenient working recurrent


recipe












gradients backward

gradients

builds
returning
optimizer recipe specifies
accommodates
varies recurrent

vary


compose
creating
abstractions
designated


primer nlp

arbitrarily deep
thanks gradient

recurrent


gradient care trained sgd
gradient optimized
black
care
tune tutorial intended comprehensive
guide prominent

book bengio ch
bengio recommendations
lecun bottou
initialization
convexity stuck
saddle

advised restarts initializations

formulations predicted
advance

bengio initialization
initializing rdin dout








din dout
din dout




uniformly sampled suggestion
tanh activation occasions
initialization
relu linearities
initialized
gaussian


deviation
din initialization
initialization deep
debugging reproducibility advised seed



figoldberg

vanishing exploding gradients
deep gradients vanish exceedingly
explode exceedingly propagate severe deeper
recurrent pascanu mikolov bengio dealing
vanishing gradients
shallower wise layers auxiliary
signal fix layers
signal batch normalization
minibatch normalizing layers
specialized architectures assist gradient flow
lstm gru architectures recurrent dealing
exploding gradients gradients
norm exceeds gradients
norm pascanu


saturation dead neurons
layers tanh sigmoid activations saturated
activation saturated
neurons gradients avoided layers relu activation
saturated die clipped
gradient
advisable monitor layers saturated dead neurons
saturated neurons caused entering controlled
initialization scaling
dead neurons caused signals entering
happen gradient reducing
saturated layers option normalize
saturated activation tanh tanh
tanh
normalization saturation expensive
gradient batch normalization
activations
mini batch batch normalization
became deep vision
writing popular
shuffling

sgd specifies
implementations advised shuffle
pass


primer nlp


rates prevent
converging rates
converge thumb rates
monitor
stops decreases
dividing
leon bottou recommends

tth hyperparameter
recommends



benefit minibatch

connecting averaging
minibatch
beneficial specialized architectures
gpus replacing
tutorial
regularization
overfitting overfitting
alleviated extent regularization regularization
regularization placing squared penalty
kk minimized
squared norm squares
hyperparameter controlling regularization
regularization dropout hinton srivastava
sutskever salakhutdinov dropout prevent
rely dropping
half neurons
establishes connection dropout
regularization
dropout contributing
sutskever hinton
relu activation dahl hinton
dropout nlp


figoldberg

cascading
gradient computations
cascading
sharing
cascading
powerful composing
feed predicting
neighbouring characters compose
pipeline predicting
feed chunking
think layers
captures predicting cascading
layers connect

characters
propagate
gradients characters
combat vanishing gradient deep
material
bootstrapped separately plugging
tuning predicting
trained accurately predict annotated
plugging
supervision
creating
summing losses
gradients
cascading convolutional recurrent
recurrent encode
sized supervision
signal recurrent comes primarily consumes
recurrent

feed

chunking named ner
synergistic predicting chunk
boundaries named boundaries rely
shared

feed concatenation


primer nlp

layers passed layers
shared
disambiguate
gradients
supervision signal summing
losses gradients
corpora supervision signal ner
chunking shuffle
gradient

collobert cascaded
feed weiss
recurrent le sutskever vinyals kaiser
sgaard goldberg


nlp involve

canonical tagging tagging
segmentation chunking ner
feed
specialized dealing


decompose

trained
tagging gimenez marquez
nivre adapted
replacing svm
logistic demonstrated chen manning
lewis steedman
suffer mistakes
carry achievable nonlinear classifiers helps extent
mitigating
attempting easier harder goldberg elhadad
mistakes hal daume
langford marcu goldberg nivre
demonstrated zhu
tagger ballesteros goldberg dyer oracle



figoldberg


predicting nlp book
adapted
lecun
setup terminology familiar
nlp community
formulated
predict arg



yy


tag parse
looking maximize







decomposed








scored separately








py


py









py

shorthand


trivially







py



nn

py

din
feed




primer nlp





nnmlp






py

py

rdin rdin
gold
perceptron








cgp
calculate scored
connect
gold predicted summing
cg connect cg minus gradients
argued lecun perceptron

margin margin hinge








modify hinge
lose nice
simplest
nonetheless

slower
gradients
vast tutorial
augmented
decoding adapted
crf
fields crf treat
discussions lafferty mccallum
pereira
keep mind objectives lack
transfer



figoldberg


exp py


exp py

exp py nn


exp py nn




wish
maximized

gradient
tricky denominator summing

summation
backward viterbi recurrences recurrences adapted


beam
beam

crfs potentials peng bo
xu
biological signals wang manning
tagging chunking ner hinge
pei
durrett klein crf constituency parser
beam zhou
parser
reranking
searching intractable inefficient integrate
reranking reranking charniak
johnson collins trained
gold scored

scored
reranking modeler
integrate decoder
reranking experimenting
integrate decoder convolutional recurrent
reranking


primer nlp

schwenk socher auli le
zuidema zhu
memm
formulations memm mccallum
freitag pereira trivially adapted replacing
logistic entropy mlp

weiss
feed mlp
trained
isolation held layers
concatenated
perceptron collins trained
beam
regime
simpler isolated researchers extensive
hyper tuning activation rates
complicated

convolutional layers


predicting sentiment neutral
informative sentiment
informative informative clue informative regardless
feed
learner clues
feeding cbow mlp
downside cbow ignores
assigning bad bad

indicators bad matter
bad
naive embedding grams
cbow embedded bigrams
huge embedding matrices ngrams suffer sparsity strength
grams embedding
learner saw
deduce anything
convolution pooling convolutional
elegant convolutional
indicative predictors combine


figoldberg

capturing
informative
convolution pooling architectures lecun bengio evolved
vision community great detectors recognizing predefined cat regardless

mainly concerned
convolutional nlp community pioneering collobert weston colleagues
kalchbrenner kim
sentiment
convolution pooling
behind convolution pooling
instantiation sliding
filter transforms
captures
channel pooling
combine windows

channels windows intention
regardless
fed gradients propagated
tune
filter highlight
trained sliding
filter learns informative grams
embedding convolution
sliding filter
filter
activation
concatenated ith
pad
narrow convolution windows wide
convolution kalchbrenner convolution
pm




activation wise

refers convolution operating opposed




primer nlp






quick brown jumped lazy dog
quick brown

mul tanh

quick brown

mul tanh

brown jumped

mul tanh

jumped

mul tanh

jumped

mul tanh

lazy

mul tanh

lazy dog

mul tanh

convolution

pooling

convolution pooling quick brown jumped
lazy dog narrow convolution padding
translated dim embedding
embedding concatenated dim
seven windows transfered
filter wise tanh seven
filtered pooling
pooled


ideally captures indicative pooling


im



jth pooling
salient ideally specialize
sort predictors pick
predictor
illustration

reflects salient fed
downstream layers perhaps
calculates
gradients propagated
pooling convolution layers embedding layers
besides
embeddings convolution pooling encode



figoldberg

pooling pooling
pooling
pooling
pooling
pooling
retain positional

pooling separately concatenate
division conjecture appearing
indicative appearing late equally
sized pooling johnson
classifying topics
pooling separating
sentiment pooling
suggesting
signals sentiment regardless
asked
argue
kinds
chen accordingly pooling
separately windows
variation convolutional layers succession convolution pooling layers convolution
pools neighboring convolution pooled
convolution sensitivity increasingly

kalchbrenner pooling
retained preserving
appeared


























pooling pooling




rows concatenated









primer nlp

pooling pool indicators
apart preserves insensitive
discern finely
activated kalchbrenner
variations
convolutional convolutional layers
convolutional layers
capturing gram lengths
convolutional pooled concatenated
fed kim
convolutional generalize convolution

pooling liu
convolutional le
zuidema pooling
derivations chart chart parser

recurrent stacks
dealing
letters saw feedforward accommodate
concatenation cbow cbow encode sized
cbow forces disregard convolutional
convolutional
cbow offer sensitivity sensitivity
mostly disregards apart

recurrent rnns elman arbitrarily sized
paying

rnn
rnn

sn
yn
rnn fashion
rnn observing



figoldberg

predicting
softmax jth
softmax rnn conditioning
resorting traditionally
rnn
perplexity gram
mathematically recursively

constructing
rnn constructing feed


rnn





rdin rf dout
rnn keeps
kept passed
invocations
graphically rnn traditionally












rnn
kth

gram
somewhat unify rnn
rnn elman rnn gru architectures
identity lstm selects
rnn architectures
popular architectures rnn lstm gru
flexibility



primer nlp

presentation
sized
recursion












































rnn unrolled

visualization
highlight shared
instantiations exhibit
trained
gradient adhere interface
instantiations rnn lstm
gru lets rnn

expanding recursion

















sn yn thought
usefulness
conveys
tying
unless
stronger sn



figoldberg

rnn
unrolled rnn deep
somewhat
shared
rnn unrolled
unrolled backward
backpropagation gradients
rnn backpropagation
supervision signal
acceptor
option supervision signal yn
rnn acceptor
rnn read characters
predict inspired ling
rnn reads decides
conveys sentiment inspired wang rnn
reads decides
yn sn gradients

familiar entropy hinge margin
encoder
acceptor encoder supervision yn
acceptor solely
treated
signals extractive
summarization rnn
unrolling rnn
rnn


rnn gradients tend vanish
omitting negligible arbitrarily
rnn lstm gru mitigate vanishing
gradients unrolling motivated
book breaking
propagates gradients

terminology borrowed rnn
rely lookup

supervision signal vanishing gradients
tell



primer nlp


predict
calc































acceptor rnn
yn summarizing yn
summarization
transducer
option treat rnn transducer producing
reads modeled signal
yp
unrolled


transducer
tagger
predicting tag
ccg super tagger
ccg super tagging xu




predict
calc

predict
calc








predict
calc








predict
calc








predict
calc















transducer rnn
transduction setup
predict rnn

mikolov ney mikolov
vinyals schuster wu
rnns transducers relax traditionally hmm taggers


figoldberg

arbitrarily histories demonstrated
generative character rnn character character character conditioning sutskever martens hinton
texts sensitivity captured gram
lengths nested parenthesis balancing demonstration
rnn character
karpathy johnson
encoder decoder
encoder encoder decoder
cho bengio sutskever rnn
encode yn
auxiliary rnn decoder
setup rnn encodes
yn fed decoder rnn
trained predict transducer
predicted yn
supervision happens decoder rnn gradients propagated
encoder rnn




predict
calc

predict
calc


sd

od


sd

oe



se

sd

od



oe



se

predict
calc



od



se

predict
calc


sd

od



oe



se

predict
calc

sd

od



oe



se



oe

se



encoder decoder rnn
surprisingly sutskever
lstm rnns sutskever
reverse


primer nlp

easier rnn establish

encoder decoder transduction
tags tn encoder rnn encode
sized fed
transducer rnn predict
alfonseca kaiser vinyals
deletion
stacked rnns
rnns stacked layers forming bengio rnns

rnn jth rnn

rnn jth rnn rnn



formation rnn
layered architectures deep rnns
rnn





































































































deep rnn
theoretically gained deeper
empirically deep rnns shallower
sutskever layers deep crucial encoder decoder
cardie birnn layers
layered rnn architectures rnns
bidirectional rnns birnn
elaboration rnn bidirectional rnn birnn commonly
birnn schuster
tagging rnn ith
rnn lstm



figoldberg


evident sliding
categorized surrounding
rnn relaxes looking arbitrarily
birnn relaxes arbitrarily

birnn maintaining

sfi
backward backward
rnns rnn rf fed
rnn rb fed reverse
composed backward
concatenation
sfi
fed
rnns gradients flow backward rnns
birnn




concat

concat

sb

rb


sf

rf



concat



sb

rb

sb

rf

rb


sf

rf







concat



sf





concat


sb

rb


sf

rf




sb

sb

rb


sf

sf

rf



birnn brown jumped
tagging nlp community
cardie
rnns stacks
nivre
confined
looking rnn
sized
intuition
feeding rnn



primer nlp

rnn
maintained push
pushed
rnn dealing
pop challenging persistent
goldberg zhao huang persistent
keep old intact persistent
pointer linked
push appends returning
pop keeping intact
someone held pointer
push
lifetime
intermediary

creating rnn

participated
rnn
dyer watanabe
sumita













push








push













push









pop

push












pop














































pop

push

push

push push
push pop push pop pop push push

reading
inferring reading
challenging


figoldberg

ya



ya



ya xe



ya







ya





xb



ya





xf









xd



xc

rnn

standardized researchers
things rnn hot
embedding rnn embedded
padded
rnn
fed layers softmax
presentation tutorial softmax rnn
rnn
concatenation layers encoder decoder
conditioning encoder
lstm
lstm
careful reading
behind ambiguous phrasing
reader aware reading interpret
writer aware

software package knowing
software package
dont rely solely describing
ambiguous


primer nlp

rnn architectures
instantiations rn

rnn lstm gated
recurrent gru
rnn
simplest rnn elman rnn rnn
elman mikolov
rnn
wx ws




rds rdx wx rdx ds ws rds ds rds

passed activation commonly tanh relu

spite simplicity rnn tagging
xu comprehensive
rnns phd mikolov
lstm
rnn vanishing gradients pascanu
signals gradients diminish
signals
rnn dependencies lstm
schmidhuber vanishing gradients
behind lstm
preserve gradients
controlled smooth
simulate gates gate

forgotten concretely gate
multiplied wise rn

sigmoid indices
pass blocked
treat complicated
mlp presentation
rnn computations rnns



figoldberg

mathematically lstm

hj


hj tanh

wxi hj whi

hj


xo

hj

ho





tanh hj
yj hj

dh rdx hj wx rdx dh wh dh
wise composed hj hj
gates controlling
gate
hj passed sigmoid activation
hj passed tanh activation forget gate controls
keep gate controls
keep hj yj
passed tanh linearity controlled
gate gradients
stay ranges
lstm phd alex

lstm character karpathy

motivation behind lstm
gru vanishing gradient recurrent
cho
lstms rnn responsible competitor
lstm rnn gru
lstm forget gates
proposal schmidhuber
connections gate tying overview
comprehensive lstm architectures srivastava
schmidhuber



primer nlp

considerations
lstm recommend
initialize forget gate dropout
rnn lstm crucial dropout
recurrent connection layers

gru
lstm complicated
computationally expensive
gated recurrent gru cho
lstm subsequently chung comparably
lstm textual
lstm gru substantially
gates



tanh wsg



yj
rds rdx rds wx rdx ds ws rds ds
gate serves yj
interpolation proposal proportions
interpolation controlled gate
gru
gru lstm rnn
architectures actively researched
gru lstm architectures

gated architectures lstm gru alleviating vanishing gradients rnn rnns dependencies span
ranges researchers simpler architectures lstm
gru benefits
mikolov multiplication ws coupled
nonlinearity rnn undergo
gru



figoldberg

remembering
periods slow
slow
interpolation
wx accumulate
rnn
changed wx wh wc
concatenation slow
mikolov
lstm
mikolov constraining
ws rnn multiply identity
mikolov le hinton simpler
activation rnn relu initialize biases
zeroes ws rnn copy

copying
ws freely le modification
rnn comparable lstm



rnn
discourse
sentiment socher
predict predict

care spans
merely backbone helps
guide
recnn pollack popularized nlp
richard socher colleagues socher manning socher lin manning socher socher rnn

rnn encodes prefix recnn encodes
predict

spans rooted
depart mikolov reuse lstm
diverges rnn fixing linearity sigmoid
central
proposal
parse transfer recursively
major technical challenge




primer nlp

intuition behind subtree
vec vec vec
returning
rnn encode recnn
encodes subtree rooted
illustration


combine





combine





illustration




parse reminder
unlabeled string triplets
triplet spanning
spanning triplets terminal
unlabeled
spans
spanning
terminal
production
boy saw duck


figoldberg








det verb


boy

saw

det


duck

unlabeled
unlabeled











det det det

verb verb verb
det det det

det
verb
det


span

boy
saw

duck
duck
saw duck
boy
boy saw duck


production uniquely converted
indicating span ignoring
production

recnn parse

production
recnn





encodes rooted rnn

shaped recnn recursively




recnn













qk



le zuidema recnn
outside subtree rooted
classic outside
thought birnn counterpart recnn
le zuidema



primer nlp


activation



sb




ignores dd


unreliable
embeddings
terminal
embedded



sb




qian tian huang liu zhu
zhu socher
terminals






sb




terminal

parse encode subtrees

variations
suffer vanishing gradients
rnn sought inspired lstm gated shaped lstms tai socher
manning zhu guo
vast
rnns
socher manning
tensor socher

acts
subtle compositions averaging implied
concatenation
usual
basing tensor





figoldberg


recipe
spell gradients
backpropagation sgd
regard rnn associate

losses summation
associates quantity

additionally treat recnn encoder rooted

passed

phd richard socher


powerful learners opportunities ranging
markovian hope exposition helps nlp researchers incorporate



vu schultz recurrent
switching
sofia bulgaria

ando semi supervised
chunking
acl ann arbor michigan

ando
unlabeled

auli galley quirk zweig recurrent
seattle washington
backpropagation
gradients recnn




primer nlp

auli gao decoder integration bleu recurrent


baltimore maryland
ballesteros dyer
characters lstms lisbon
portugal
ballesteros goldberg dyer
improves lstm parser arxiv
bansal tailoring

baltimore
maryland
pearlmutter siskind
differentiation arxiv
bengio recommendations gradient deep architectures arxiv
bengio vincent mach res
bengio courville deep book preparation

cohn deep convolutional

beijing china

blunsom compositional morphology
modelling
icml beijing china award
bottou gradient descent tricks tricks

charniak johnson coarse fine maxent discriminative reranking
acl ann arbor michigan

chen manning parser

emnlp doha qatar


figoldberg

chen xu liu zeng zhao
pooling convolutional


beijing china
cho
arxiv stat
cho bengio
eighth
doha qatar
cho schwenk
bengio rnn

emnlp doha qatar

chrupala normalizing tweets edit scripts recurrent embeddings
baltimore maryland

chung cho bengio gated
recurrent arxiv
collins discriminative
perceptron

collins discriminative reranking

collobert weston unified
deep multitask

collobert weston bottou kavukcuoglu
scratch

crammer singer algorithmic multiclass
unsupervised segmentation
morphology trans lang
mathematics
signals


primer nlp

dahl hinton deep
dropout
acoustics signal icassp
pascanu cho bengio
identifying attacking saddle
ghahramani welling cortes lawrence weinberger
curran associates inc
gispert byrne preordering smt
north american
chapter technologies denver colorado
fields

dong wei tan tang zhou xu
twitter sentiment

baltimore maryland

dong wei zhou xu
convolutional


beijing china
dos santos gatti deep convolutional sentiment
texts coling technical dublin ireland
dublin
dos santos xiang zhou classifying
convolutional
beijing
china
dos santos zadrozny character partof tagging
icml
duchi hazan singer


adaptation


figoldberg

sofia bulgaria

durrett klein crf


beijing china
dyer ballesteros ling matthews


beijing china
elman cognitive
faruqui dyer multilingual chapter
sweden

alfonseca kaiser vinyals
deletion lstms
lisbon portugal

associative memories
biological neuroscience

gao pantel deng
deep
emnlp doha qatar

gimenez marquez tagger generator
lrec lisbon portugal
bengio deep feedforward


bengio deep sparse

goldberg elhadad directional
technologies
north american chapter
los angeles california
goldberg levy vec deriving mikolov embedding arxiv stat


primer nlp

goldberg nivre parsers
oracles

goldberg zhao huang beam
incremental parsers
sofia bulgaria


backpropagation

gouws bengio corrado bilingual alignments

supervised labelling recurrent ph
technische universitat munchen
srivastava schmidhuber
lstm arxiv
hal daume langford marcu
mlj
harris distributional
customization


seattle washington
ren sun delving deep surpassing
imagenet arxiv
henderson thomson young deep
dialog tracking challenge sigdial
france
hermann blunsom
compositional sofia
bulgaria
hermann blunsom multilingual compositional
baltimore maryland

bengio recurrent
dependencies touretzky mozer



figoldberg

hill cho jean bengio embedding
arxiv
hinton srivastava sutskever salakhutdinov
preventing adaptation detectors
arxiv
schmidhuber

white multilayer feedforward
approximators
batch normalization accelerating deep
reducing arxiv
cardie opinion mining deep recurrent

emnlp doha qatar
iyyer boyd graber socher daume
factoid paragraphs
emnlp
doha qatar
iyyer boyd graber resnik political


baltimore maryland
iyyer boyd graber daume deep unordered


beijing china
johnson categorization
convolutional north
american chapter technologies denver colorado

vinyals schuster wu exploring
limits arxiv
sutskever recurrent architectures
icml
kalchbrenner grefenstette blunsom convolutional
modelling baltimore
maryland


primer nlp

karpathy johnson visualizing recurrent
arxiv
kim convolutional
emnlp doha qatar
kim sontag rush character aware
arxiv stat
ba
arxiv

adam

sutskever hinton imagenet deep
convolutional pereira burges bottou weinberger
curran associates inc
matsumoto
acl stroudsburg

lafferty mccallum pereira fields
segmenting icml
le zuidema outside

emnlp doha qatar

le zuidema forest convolutional compositional distributional chart binarization

lisbon portugal
le hinton initialize recurrent
arxiv
lecun bengio convolutional handbook brain

lecun bottou muller backprop
tricks
lecun bottou bengio gradient

lecun chopra huang tutorial predicting
lecun huang discriminative
aistats aistats


figoldberg

lee flowers dyer conceptual script story connectionist

levy goldberg embeddings

baltimore maryland

levy goldberg embedding factorization ghahramani welling cortes lawrence weinberger

curran associates inc
levy goldberg dagan distributional
lessons embeddings
lewis steedman ccg semi supervised
hovy deep discourse
emnlp doha qatar
ling dyer black adaptations
vec north
american chapter technologies denver colorado

ling dyer black amir
luis compositional character
vocabulary
lisbon portugal

liu wei ji zhou wang

beijing
china
le sutskever vinyals kaiser
arxiv stat
zhu tagging tagger
baltimore
maryland
huang zhou xiang convolutional
embedding


primer nlp

beijing
china
mccallum freitag pereira entropy
segmentation icml
mikolov chen corrado dean
arxiv
mikolov chopra
recurrent arxiv
mikolov khudanpur recurrent
interspeech
japan september
mikolov khudanpur recurrent acoustics signal
icassp
mikolov sutskever chen corrado dean compositionality burges
bottou welling ghahramani weinberger
curran associates inc
mikolov ph ph

kavukcuoglu embeddings burges bottou welling ghahramani
weinberger
curran associates inc
seaghdha thomson su wen
young dialog tracking recurrent

beijing china

differentiation matlab siam
nesterov
soviet mathematics doklady
nesterov introductory lectures kluwer academic publishers
nguyen grishman adaptation
convolutional


figoldberg

beijing
china
nivre incremental
purely york
backpropagation retrieved
github io posts backprop
lstm retrieved
github io posts lstms
pascanu mikolov bengio recurrent
arxiv
pei ge chang


beijing china
peng bo xu fields bengio schuurmans
lafferty williams culotta
curran associates inc
pennington socher manning glove
emnlp doha qatar
pollack

speeding
ussr mathematics physics
qian tian huang liu zhu zhu tag embeddings
tag


beijing china
vec arxiv
rumelhart hinton williams
propagating
schuster bidirectional recurrent
signal


primer nlp

schwenk
coling acl
poster sessions
shawe taylor cristianini

linguistic synthesis lectures technologies claypool
socher deep
vision ph stanford
socher bauer manning compositional
grammars
sofia bulgaria

socher manning compositionality


jeju island korea

socher lin manning scenes
getoor scheffer
icml
washington june july
socher manning
deep
unsupervised nips

socher wu chuang manning potts
deep compositionality sentiment treebank

seattle washington

sgaard goldberg deep supervised
layers


galley auli brockett ji mitchell nie gao
dolan
conversational responses north
american chapter technologies denver colorado

ney
bidirectional recurrent


figoldberg

emnlp doha
qatar
ney lstm
interspeech
sutskever martens dahl hinton initialization
momentum deep
icml
sutskever martens hinton recurrent

icml
sutskever vinyals le
ghahramani welling cortes lawrence
weinberger
curran associates inc
tai socher manning

beijing china
tamura watanabe sumita recurrent

baltimore
maryland
benefits arxiv stat
hinton divide gradient

selectional emnlp doha qatar

zhao fossum chiang decoding improves
seattle washington
wang liang dropout regularization
burges bottou welling ghahramani weinberger
curran
associates inc
wang manning deep
ijcnlp


primer nlp

wang xu xu liu wang hao convolutional categorization


beijing china
wang liu sun wang wang predicting tweets
composing embeddings


beijing china
watanabe sumita constituent

beijing china
weiss collins petrov

beijing
china
backpropagation

weston usunier connecting
embedding

seattle washington
xu auli clark ccg recurrent

beijing china

yin schutze convolutional paraphrase
north american chapter technologies
denver colorado
sutskever vinyals recurrent regularization
arxiv
arxiv

zeng liu lai zhou zhao convolutional deep coling


figoldberg

technical dublin
ireland dublin
weiss


zhou huang chen

beijing china
zhu qiu chen huang
parser convolutional

beijing china
zhu guo
arxiv




