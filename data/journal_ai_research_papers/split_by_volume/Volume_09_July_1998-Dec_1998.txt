Journal of Artificial Intelligence Research 9 (1998) 167-217

Submitted 6/98; published 10/98

Probabilistic Inference from Arbitrary Uncertainty
using Mixtures of Factorized Generalized Gaussians
Alberto Ruiz
Pedro E. Lpez-de-Teruel
M. Carmen Garrido
Universidad de Murcia, Facultad de Informtica,
Campus de Espinardo, 30100, Murcia, Spain

ARUIZ@DIF.UM.ES
PEDROE@DITEC.UM.ES
MGARRIDO@DIF.UM.ES

Abstract
This paper presents a general and efficient framework for probabilistic inference and learning from
arbitrary uncertain information. It exploits the calculation properties of finite mixture models, conjugate families and factorization. Both the joint probability density of the variables and the likelihood
function of the (objective or subjective) observation are approximated by a special mixture model, in
such a way that any desired conditional distribution can be directly obtained without numerical integration. We have developed an extended version of the expectation maximization (EM) algorithm to
estimate the parameters of mixture models from uncertain training examples (indirect observations). As
a consequence, any piece of exact or uncertain information about both input and output values is consistently handled in the inference and learning stages. This ability, extremely useful in certain situations, is not found in most alternative methods. The proposed framework is formally justified from
standard probabilistic principles and illustrative examples are provided in the fields of nonparametric
pattern classification, nonlinear regression and pattern completion. Finally, experiments on a real application and comparative results over standard databases provide empirical evidence of the utility of the
method in a wide range of applications.

1. Introduction
The estimation of unknown magnitudes from available information, in the form of sensor measurements or subjective judgments, is a central problem in many fields of science and engineering.
To solve this task, the domain must be accurately described by a model able to support the desired
range of inferences. When satisfactory models cannot be derived from first principles, approximations must be obtained from empirical data in a learning stage.
Consider a domain Z composed by a collection of objects z =(z1, z2, ..., zn), represented by
vectors of n attributes. Given some partial knowledge S (expressed in a general form explained
later) about a certain object z, we are interested in computing a good estimate z ( S ) , close to the
true z. We allow heterogeneous descriptions; any attribute zi may be continuous, discrete, or symbolic valued, including mixed types. If there is a specific subset of unknown or uncertain attributes
to be estimated, the attribute vector can be partitioned as z = (x, y), where y  z denotes the target
or output attributes. The target attributes can be different for different objects z. This scenario includes several usual inference paradigms. For instance, when there is a specific target symbolic
attribute, the task is called pattern recognition or classification; when the target attribute is continuous, the inference task is called regression or function approximation. In general, we are interested in a general framework for pattern completion from partially known objects.
Example 1: To illustrate this setting, assume that the preprocessor of a hypothetical computer
vision system obtains features of a segmented object. The instances of the domain are described

1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

by the following n=7 attributes: AREA: z1  , COLOR: z2  {white, black, red, ...}, DISTANCE:
z3  , SHAPE: z4  {circular, rectangular, triangular, ...}, TEXTURE: z5  {soft, rough, ...},
OBJECTTYPE: z6  {door, window, ...} and ANGLE: z7  . A typical instance may be z = (78,
blue, 3.4, triangular, soft, window, 45). If the object is partially occluded or 3-dimensional,
some attributes will be missing or uncertain. For instance, the available information S about z
could be expressed as (748, blue OR black, 3.4, triangular, ?, window 70% door 30%, ?),
where z1, z2, z6 are uncertain, z3, z4 are exact and z5, z7 are missing. In this case we could be interested in estimates for y = {z5, z6, z7} and even in improving our knowledge on z1 and z2.

The non-deterministic nature of many real world domains suggests a probabilistic approach,
where attributes are considered as random variables. Objects are assumed to be drawn independently and identically distributed from p(z) = p(z1, ..., zn) = p(x, y), the multivariate joint probability
density function of the attributes, which completely characterizes the n-dimensional random variable z. To simplify notation, we will use the same function symbol p() to denote different p.d.f.s if
they can be identified without risk of confusion.
According to Statistical Decision Theory (Berger 1985), optimum estimators for the desired attributes are obtained through minimization of a suitable expected loss function:

y OPT ( S ) = argmin y E{ L( y , y )| S}


where L(y, y ) is the loss incurred when the true y is estimated by y . Estimators are always features of the conditional or posterior distribution p(y|S) of the target variables given the available
information. For instance, the minimum squared error (MSE) estimator is the posterior mean, the
minimum linear loss estimator is the posterior median and the minimum error probability (EP, 0-1
loss) estimator is the posterior mode.
Example 2: A typical problem is the prediction of an unknown attribute y from the observed attributes x. In this case the available information can be written as S = (x, ?). If y is continuous, it
is reasonable to use the MSE estimator: y MSE ( S ) = E{ y | x} , the general regression function. If y
is symbolic and the same loss is associated to all errors, the EP estimator is adequate:
y EP ( S ) = argmaxy p(y|x) = argmaxy p(x|y)p(y). It corresponds to the Maximum A Posteriori rule
or Bayes Test, widely used in Statistical Pattern Recognition.

The joint density p(z) = p(x, y) plays an essential role in the inference process. It implicitly
includes complete information about attribute dependences. In principle, any desired conditional
distribution or estimator can be computed from the joint density by adequate integration. Probabilistic Inference is the process of computing the desired conditional probabilities from a (possibly
implicit) joint distribution. From p(z) (the prior, model of the domain, comprising implications) and
S (a known event, somewhat related to a certain z), we could obtain the posterior p(z|S) and the
desired target marginal p(y|S) (the probabilistic consequent).
Example 3: If we observe an exact value xo in attribute x, i.e. S = { x = xo}, we have:

p( y| S )  p(y| xo) =



Y

p ( xo , y )
p( xo , y )dy

If we know that instance z is in a certain region R in the attribute space, i.e. S = {z  R}, we
compute the marginal density of y from the joint p(z) = p(x, y) restricted to region R (Fig. 1):

168

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

p( y| S ) =



X

p( x, y|{( x , y )  R}) dx =

 p( x, y) dx
 p( x, y) dxdy
R

R

More general types of uncertain information S about z will be discussed later.

y
p(y|R)
R

p(x,y)
x

Figure 1. The conditional probability density of y, assuming z = (x,y)  R.

In summary, from the joint density p(z) of a multivariate random variable, any subset of variables y  z may be, in principle, estimated given the available information S about the whole z = (x,
y). In practical situations, two steps are required to solve the inference problem. First, a good model
of the true joint density p(z) must be obtained. Second, the available information S must be efficiently processed to improve our knowledge about future, partially specified instances z. These two
complementary aspects, learning and inference, are approached from many scientific fields, providing different methodologies to solve practical applications.
From the point of view of Computer Science, the essential goal of Inductive Inference is to
find an approximate intensional definition (properties) of an unknown concept (subset of the domain) from an incomplete extensional definition (finite sample). Machine Learning techniques
(Michalski, Carbonell & Mitchell 1977, 1983, Hutchinson 1994) provide practical solutions (e.g.
automatic construction of decision trees) to solve many situations where explicit programming
must be avoided. Computational Learning Theory (Valiant 1993, Wolpert 1994, Vapnik 1995)
studies the feasibility of induction in terms of generalization ability and resource requirements of
different learning paradigms.
Under the general setting of Statistical Decision Theory, modeling techniques and the operational aspects of inference (based in numerical integration, Monte Carlo simulation, analytic approximations, etc.) are extensively studied from the Bayesian perspective (Berger 1985, Bernardo
& Smith 1994). In the more specific field of Statistical Pattern Recognition (Duda & Hart 1973,
Fukunaga 1990), standard parametric or nonparametric density approximation techniques (Izenman
1991) are used to learn from training data the class-conditional p.d.f.s required by the optimum
decision rule. For instance, if the class-conditional densities p(x|y) are Gaussian, the required parameters are the mean vector and covariance matrix of the feature vector in each class and the decision regions for y in x have quadratic boundaries. Among the nonparametric classification techniques, the Parzen method and the K-N Nearest Neighbors rule must be mentioned. Analogously, if
the target attribute is continuous and the statistical dependence between input and output variables
p(x,y) can be properly modeled by joint normality, we get multivariate linear regression: y MSE(x) =
A x + B, where the required parameters are the mean values and the covariance matrix of the attrib-

169

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

utes. Nonlinear regression curves can be also derived from nonparametric approximation techniques. Nonparametric methods present slower convergence rates, requiring significantly larger
sample sizes to obtain satisfactory approximations; they are also strongly affected by the dimensionality of the data and the selection of the smoothing parameter is a crucial step. In contrast, they
only require some kind of smoothness assumption on the target density.
Neural Networks (Hertz et. al 1991) are computational models trainable from empirical data
that have been proposed to solve more complex situations. Their intrinsic parallel architecture is
especially efficient in the inference stage. One of the most widely used neural models is the Multilayer Perceptron, a universal function approximator (Hornik et al. 1989) that breaks the limitations
of linear decision functions. The Backpropagation learning algorithm (Rumelhart et al. 1986) can,
in principle, adjust the network weights to implement arbitrary mappings, and the network outputs
show desirable probabilistic properties (Wan 1990, Rojas 1996). There are also unsupervised networks for probability density function approximation (Kohonen 1989). However, neural models
usually contain a large number of adjustable parameters, which is not convenient for generalization
and, frequently, long times are required for training in relatively easy tasks. The input / output role
of attributes cannot be changed in runtime and missing and uncertain values are poorly supported.
Bayesian Networks, based in the concept of conditional independence, are among the most
relevant probabilistic inference technologies (Pearl 1988, Heckerman & Wellman 1995). The joint
density of the variables is modeled by a directed graph which explicitly represents dependence
statements. A wide range of inferences can be performed under this framework (Chang & Fung
1995, Lauritzen & Spiegelhalter 1988) and there are significant results on inductive learning of
network structures (Bouckaert 1994, Cooper & Herskovits 1992, Valiveti & Oomen 1992). This
approach is adequate when there is a large number of variables showing explicit dependences and
simple cause-effect relations. Nevertheless, solving arbitrary queries is NP-Complete, automatic
learning algorithms are time consuming and the allowed dependences between variables are relatively simple.
In an attempt to mitigate some of the above drawbacks, we have developed a general and efficient inference and learning framework based on the following considerations. It is well known
(Titterington et al. 1985, McLachlan & Basford 1988, Dalal & Hall 1983, Bernardo & Smith 1994,
Xu & Jordan 1996) that any reasonable probability density function p(z) can be approximated up to
the desired degree of accuracy by a finite mixture of simple components Ci, i = 1..l:

p( z )   P{Ci } p( z| Ci )

(1)

i

The superposition of simple densities is extensively used to approximate arbitrary data dependences (Fig. 2). Maximum Likelihood estimators of the mixture parameters can be efficiently obtained from samples by the Expectation Maximization (EM) algorithm (Dempster, Laird & Rubin
1977, Redner & Walker 1984) (see Section 4).

170

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

(a)
(b)
(c)
Figure 2. Illustrative example of density approximation using a mixture model. (a) Samples from a p.d.f. p(x,y) showing a nonlinear dependence. (b) Mixture model for p(x,y)
with 6 gaussian components obtained by the standard EM algorithm. (c) Location of
components.

The decomposition of probability distributions using mixtures has been frequently applied to
unsupervised learning tasks, especially Cluster Analysis (McLachlan & Basford 1988, Duda &
Hart 1973, Fukunaga 1990): the a posteriori probabilities of each postulated category are computed
for all the examples, which are labeled according to the most probable source density. However,
mixture models are specially useful in nonparametric supervised learning situations. For instance,
the class conditional densities required in Statistical Pattern Recognition were individually approximated in (Priebe & Marchette 1991, Traven 1991) by finite mixtures; hierarchical mixtures of
linear models were proposed in (Jordan & Jacobs 1994, Peng et. al 1995); mixtures of factor analyzers have been developed in (Ghahramani & Hinton 1996, Hinton, Dayan, & Revow 1997) and
mixture models have been also useful for feature selection (Pudil et al. 1995). Mixture modeling is
a growing semiparametric probabilistic learning methodology with applications in many research
areas (Weiss & Adelson 1995, Fan et al. 1996, Moghaddam & Pentland 1997).
This paper introduces a framework for probabilistic inference and learning from arbitrary uncertain data: any piece of exact or uncertain information about both input and output values is consistently handled in the inference and learning stages. We approximate both the joint density p(z)
(model of the domain) and the relative likelihood function p(S|z) (describing the available information) by a specific mixture model with factorized conjugate components, in such a way that numerical integration is avoided in the computation of any desired estimator, marginal or conditional
density.
The advantages of modeling arbitrary densities using mixtures of natural conjugate components were already shown in (Dalal & Hall 1983), and, recently, inference procedures based in a
similar idea have been proposed in (Ghahramani & Jordan 1994, Cohn et al. 1996, Peng et al. 1995,
Palm 1994). However, our method efficiently handles uncertain data using explicit likelihood
functions, which has not been extensively used before in Machine Learning, Pattern Recognition or
related areas. We will follow standard probabilistic principles, providing natural statistical validation procedures.
The organization of the paper is as follows. Section 2 reviews some elementary results and
concepts used in the proposed framework. Section 3 addresses the inference stage. Section 4 is
concerned with learning, extending the EM algorithm to manage uncertain information. Section 5
discusses the method in relation to alternative techniques and presents experimental evaluation.
The last section summarizes the conclusions and future directions of this work.

171

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

2. Preliminaries
2.1 A Calculus of Generalized Normals
In many applications, the instances of the domain are represented simultaneously by continuous
and symbolic or discrete variables (as in Wilson & Martinez 1997). To simplify notation, we will
denote both probability impulses and Gaussian densities by means of a common formalism. The
generalized normal
(x,,) denotes a probability density function with the following properties:

T

T(x,,) 

If  > 0,

  ( x  ) 2 
1
exp

2  22 

T(x,,) = T(x,,0)  T(x,)  (x)

If  = 0,

Tzero,(x,,)
is a Gaussian density with mean  and standard deviation   0. When the dispersion is
T reduces
to a Diracs delta function located at . In both cases T is a proper p.d.f.:
T(x,,) > 0
 T(x,,) dx = 1
X

The product of generalized normals can be elegantly expressed (Papoulis 1991 pp. 258, Berger
1985) by:
for 1+2 >0:

T(x,1,1)  T(x,2,2) = T(x,,)  T(1,2, 12 + 22 )

(2)

where the mean  and dispersion  of the new normal are given by:

122 + 22 1
=
12 + 22

1222
 = 2
1 + 22
2

This relation is useful for computing the integral of the product of two generalized normals:
for 1+2 >0:



X

T(x,1,1)  T(x,2,2) dx = T(1, 2, 12 + 22 )

(3)

And, for consistency, we define



for 1 = 2 = 0:

X

T(x,1) T(x,2) dx = T(1,2)  I{1=2}

where I{predicate} = 1 if predicate is true and zero otherwise. Virtually any reasonable univariate
probability distribution or likelihood function can be accurately modeled by an appropriate mixture
of generalized normals. In particular, p.d.f.s over symbolic variables are mixtures of impulses.
Without loss of generality, symbols may be arbitrarily mapped to specific numbers and represented
over numeric axes. Integrals over discrete domains become sums.
Example 4: Let us approximate the p.d.f. p(x) of a mixed continuous and symbolic valued random variable x by a mixture of generalized normals. Assume that x takes with probability 0.4
the exact value 10 (with a special meaning), and with probability 0.6 a random value continuously distributed following the triangular shape shown in Fig. 3. The density p(x) can be accurately approximated (see Section 4) using 4 generalized normals:

T(x,10) + .21T(x,.04,.23) + .28T(x,.45,.28) + .11T(x,.99,.21)

p(x)  .40

172

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

Figure 3. The p.d.f. of a mixed random variable approximated by a mixture of generalized normals.

2.2 Modeling Uncertainty: The Likelihood Principle
Assume that the value of a random variable z must be inferred from a certain observation or subjective information S. If z has been drawn from p(z) and the measurement or judgment process is
characterized by a conditional p(S|z), our knowledge on z is updated according to p(z|S)=p(z) p(S|z)
/ p(S), where p(S) = Z p(S|z) p(z) dz (see Fig. 4).
The likelihood function fS(z)  p(S|z) is the probability density ascribed to S by each possible
z. It is an arbitrary nonnegative function over z that can be interpreted in two alternative ways. It
can be the objective conditional distribution p(S|z) of a physical measurement process (e.g. a
model of sensor noise, specifying bias and variance of the observable S for every possible true
value z), also known as error model. It can also be a subjective judgment about the chance of the
different z values (e.g. intervals, more likely regions, etc.), based on vague or difficult to formalize
information. The dispersion of fS(z) is directly related to the uncertainty associated to the measurement process. Following the likelihood principle (Berger 1985), we explicitly assume that all the
experimental information required to perform probabilistic inference is contained in the likelihood
function fS(z).

p(z)
z1

z2

z3

z

p(s|z2)

p(s|z1)

p(s|z3)
s

so

prior
model of
measurement

p(s)
s
fSo(z)= p(so|z)

z

observable

likelihood of
observation so

p(z|so)
posterior
z
Figure 4. Illustration of the elementary Bayesian univariate inference process.

173

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

2.3 Inference Using Mixtures of Conjugate Densities
The computation of p(z|S) may be hard, unless p(z) and p(S|z) belong to special (conjugate) families (Berger 1985, Bernardo & Smith 1994). In this case the posterior density can be analytically
obtained from the parameters of the prior and the likelihood, avoiding numeric integration. The
prior, the likelihood and the posterior are in the same mathematical family. The belief structure is
closed under the inference process.

T

Example 5: In the univariate case, assume that z is known to be normally distributed around r
with dispersion r, i.e. p(z) = (z, r, r). Assume also that our measurement device has Gaussian noise, so the observed values are distributed according to p(s|z) =
(s, z, s). Therefore,
if we observe a certain value so, from the property of the product of generalized normals in eq.
(2), the posterior knowledge on z becomes another normal
(z, , ). The new expected location of z can be expressed as a weighted average of r and so:  =  so + (1-)r and the uncertainty is reduced to 2 =  S2 . The coefficient  =  2r / (  2r + S2 ) quantifies the relative im-

T

T

portance of the experiment with respect to the prior.

This computational advantage can be extended to the general case by using mixtures of conjugate families (Dalal & Hall 1983) to approximate the desired joint probability distribution and the
likelihood function.
Example 6: If the domain and the likelihood are modeled respectively by

p(z) 


i

Pi

T(z,  ,  )
i

p(so|z) 

i



r

r

T(z,  ,  )
r

r

(where  r , r and  r depend explicitly on the observed so), then the posterior can be also
written as the following mixture:



p(z|so) 

i, r

From properties (2) and (3), the parameters

i , r

i ,r

T(z, 

,  i ,r )

(4)

i ,r and  i ,r and the weights i ,r are given by:

i2 r +  2r  i
=
i2 +  r2

 i ,r 

i ,r

 i ,r =

i  r
 i2 +  r2

Pi  r T( i , r , i2 +  r2 )

P 
k

l

T( k , l , 2k +  2l )

k ,l

2.4 The Role of Factorization
Given a multivariate observation z partitioned into two subvectors, z = (x, y), assume that we are
interested in inferring the value of the unknown attributes y from the observed attributes x. Note
that if x and y are statistically independent, the joint density is factorizable: p(z) = p(x, y) = p(x)
p(y) and, therefore, the posterior p(y|x) equals the prior marginal p(y). The observed x carries no
predictive information about y and the optimum estimators do no depend on x. For instance,
y MSE ( x ) = E{y|x} = E{y} and y EP ( x ) = argmax y p( y ) . This is the simplest estimation task. No
runtime computations are required for the optimum solution, which may be precalculated.

174

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

In realistic situations the variables are statistically dependent. In general, the joint density cannot be factorized and the required marginal densities may be hard to compute. However, interesting
consequences arise if the joint density is expressed as a finite mixture of factorized (with independent variables) components C1, C2, ..., Cl :

p(z) = p( z 1 ,..., z n ) =  P{Ci } p( z| Ci ) =  P{Ci }  p( z j | Ci )
i

i

(5)

j

This structure is convenient for inference purposes. In particular, in terms of the desired partition of z = (x, y):

p(z) = p(x, y) =

 P{C } p( x| C ) p( y| C )
i

i

i

i

so the marginal densities are mixtures of the marginal components:

p( x ) =  p( x , y )dy =  P{Ci } p( x| Ci )
Y

i

p( y ) =  P{Ci } p( y| Ci )
i

and the desired conditional densities are also mixtures of the marginal components:

p( y| x ) =   i ( x ) p( y| Ci )

(6)

i

where the weights  i (x) are the probabilities that the observed x has been generated by each component Ci :

i ( x) =

P{Ci } p( x|Ci )

 P{C } p( x|C )
j

= P{Ci | x}

j

j

The p.d.f. approximation capabilities of mixture models with factorized components remain
unchanged, at the cost of a possibly higher number of components to obtain the desired degree of
accuracy, avoiding artifacts (see Fig. 5). Section 5.2 discusses the implications of factorization in
relation with alternative model structures.

(a)
(b)
Figure 5. (a) Density approximation for the data in Fig. 2, using a mixture with 8 factorized components. (b) Location of components. Note how an arbitrary dependence can be
represented as a mixture of components which itself have independent variables (observe
that a somewhat smoother solution could be obtained increasing the number of components).

175

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

3. The MFGN Framework
The previous concepts will be integrated in a general probabilistic inference framework that we call
MFGN (Mixtures of Factorized Generalized Normals). Fig. 6 shows the abstract dependence relations among attributes in a generic domain (upper section of the figure) and between the attributes
and the observed information (lower section). In the MFGN framework, both relations are modeled
by finite mixtures of products of generalized normals. The key idea is using factorization to cope
with multivariate domains and heterogeneous attribute vectors, and conjugate densities to efficiently perform inferences given arbitrary uncertain information. In this section, we will derive the
main inference expressions. The learning stage will be described in Section 4.

p(z)
z1

zn
z2

model of
domain
p(z)

zj
model of
measurement
p(S|z)

S

Figure 6. Generic dependences in the inference process.

3.1 Modeling Attribute Dependences in the Domain
In the MFGN framework the attribute dependencies in the domain are modeled by a joint density in
the form of a finite mixture of factored components, as in expression (5), where the component
marginals p( z j | Ci )  T( z j ,  ij ,  ij ) are generalized normals:

p( z ) =  Pi
i

 T( z

j

,  ij ,  ij )

i=1..l, j=1..n,

(7)

j

If desired, the terms associated to the pure symbolic attributes z j (with all the  ij = 0) can be
collected in such a way that the component marginals are expressed as mixtures of impulses:

p( z j | Ci )   ti j, T( z j ,  )

(8)



where ti j,  P{z j = | Ci } is the probability that z j takes its -th value in component Ci . This
manipulation reduces the number l of global components in the mixture. The adjustable parameters
of the model are the proportions Pi = P{Ci } and the mean value  ij and dispersion  ij of the j-th
j
attribute in the i-th component (or, for the symbolic attributes, the probabilities ti , ). While the

structure (8) will be explicitly used for symbolic attributes in applications and illustrative examples, most of the mathematical derivations will be made over the concise expression (7).

176

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

When all variables are continuous, the MFGN architecture reduces to a mixture of gaussians
with diagonal covariance matrices. The proposed factorized structure extends the properties of
diagonal covariance matrices to heterogeneous attribute vectors. We are interested in joint models,
which support inferences from partial information about any subset of variables. Note that there is
not an easy way to define a measure of statistical depencence between symbolic and continuous
attributes, to be used as a parameter of some probability density function1. The required "hetereogeneous" dependence model can be conveniently captured by superposition of simple factorized
(with independent variables) densities.
Example 7: Figure 7 shows an illustrative 3-attribute data set (x and y are continuous and z is
symbolic) and the components of the MFGN approximation obtained by the EM algorithm
(see Section 4) for their joint density. The parameters of the mixture are shown in Table 1.
Note that, because of the overlapped structure of the data, some components (5 and 6) are assigned to both values of the symbolic attribute z.

(a)
(b)
Figure 7. (a) Simple data set with two continuous and one symbolic attribute.
(b) Location of the mixture components.

i

Pi

 ix

 ix

 iy

 iy

tiz,white

1
.14
-.40
.24
-.27
.20
0
2
.09
-.76
.19
-.68
.18
0
3
.20
.55
.23
.66
.24
0
4
.17
-.71
.27
.76
.22
1
5
.13
.21
.17
-.14
.19
.74
6
.18
-.14
.18
.26
.17
.55
7
.09
.65
.16
-.64
.19
1
Table 1. Parameters of the Mixture Model for the Data Set in Fig. 7.

tiz,black
1
1
1
0
.26
.45
0

3.2 Modeling Arbitrary Information about Instances
The available information about a particular instance z is denoted by S. Following the likelihood
principle, we are not concerned with the true nature of S, whether it is some kind of physical meas1

For this reason, in pattern classification tasks separate models are typically built for each class-conditional density.

177

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

urement or a more subjective judgment about the location of z in the attribute space. All we need to
update our knowledge about z, in the form of the posterior p(z|S), is the relative likelihood function
p(S|z) of the observed S. In general, p(S|z) can be any nonnegative multivariable function fS(z) over
the domain. In the objective case, statistical studies of the measurement process can be used to
determine the likelihood function. In the subjective case, it may be obtained by standard distribution elicitation techniques (Berger 1985). In either case, under the MFGN framework, the likelihood function of the available information to be used in the inference process will be approximated, up to the desired degree of accuracy, by a sum of products of generalized normals:

p( S| z ) =  P{S | s r } p( s r | z ) =  P{S | s r } p( srj | z j )
r

r

=

   T( z
r

j

j

, srj ,  rj )

(9)

j

r

Without loss of generality, the available knowledge is structured as a weighted disjunction S =
1
2
{1s1 OR 2s2 ... OR  R s R } of conjunctions sr = { sr AND sr ... AND srn ) of elementary uncertain
observations in the form of generalized normal likelihoods T( z j , srj ,  rj ) centered at srj with
uncertainty  rj . The measurement process can be interpreted as the result of R (objective or subjective) sensors sr , providing conditionally independent information p( srj | z j ) about the attributes
(each srj only depends on z j ) with relative strength  r . Note that any complex uncertain information about an instance z, expressed as a nested combination of elementary uncertain beliefs srj
about z j using probabilistic connectives, can be ultimately expressed by structure (9) (OR translates to addition, AND translates to product and the product of two generalized normals over the
same attribute becomes a single, weighted normal).
Example 8: Consider the hypothetical computer vision domain in Example 1. Assume that the
information about an object z is the following: AREA is around a and DISTANCE is around b or,
more likely, SHAPE is surely triangular or else circular and AREA is around c and ANGLE is
around d or equal to e. This structured piece of information can be formalized as:

T(z , a,  ) T(z , b,  )]
+ .7 [ (.9T(z ,triang)+.1T(z ,circ)) T(z , c,  ) (T(z , d,  )+T(z ,e)) ]

p(S|z) = .3 [

1

3

a

4

b

4

1

7

c

7

d

which, expanded, becomes the mixture of 5 factorized components operationally represented by
the parameters shown in Table 2.
In a simpler situation, the available information about z could be a conjunction of uncertain attributes similar to {Color = red 0.8 green 0.2} and {Area = 3  .5} and {Shape = rectangular 0.6
circular 0.3 triangular 0.1}. The likelihood of Shape values can be obtained from the output of a
simple pattern classifier (e.g. K-N-nearest neighbors) over moment invariants, while attributes
as Color and Area are directly extracted from the image. In this case we could be interested in
the distribution of values for other attributes as Texture and ObjectType. Alternatively, we
could start from {ObjectType = door 0.6 window 0.4} and {Texture = rough} in order to determine the probabilities of Color and Angle values for selecting a promising search region.

178

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

r

sr1 , 1r

sr3 ,  r3

sr2 , 2r

sr4 , 4r

sr5 , 5r

sr6 , 6r

sr7 , 7r

.30
a, a
-, 
b, b
-, 
-, 
-, 
.63
triang, 0
c, c
-, 
-, 
-, 
-, 
.63
triang, 0
c, c
-, 
-, 
-, 
-, 
.07
circ, 0
c, c
-, 
-, 
-, 
-, 
.07
circ
,
0
c, c
-, 
-, 
-, 
-, 
Table 2. Parameters of the Uncertain Information Model in Example 8.

-, 
d,d
e, 0
d,d
e, 0

3.3 The Joint Model-Observation Density
The generic dependence structure in Fig. 6 is implemented by the MFGN framework as shown in
Fig. 8. The upper section of the figure is the model of nature, obtained in a previous learning stage
and used for inference without further changes. Dependences among attributes are conducted
through an intermediary hidden or latent component Ci. The lower section represents the available
uncertain information, measurement model or query structure associated to each particular inference operation.

Ci

p( z 1 | Ci )
1

Domain:

p( z)   P{Ci } p( z j | Ci )
2

z

z

s21

s22

...

zn

...

s2n

j

i

p( s11 | z1 )
s11

s12

...

s1n

s1

...

s1R

sR2

...

sRn

sR

s2
P{S|s1}

Measurement:
p( S| z )   P{S | s r } p( srj | z j )

S

r

j

Figure 8. Structure of the MFGN model. The attributes are conditionally independent.
The measurement process is modeled by a collection of independent virtual sensors
p( srj | z j ) .

The joint density of the relevant variables becomes:

p(Ci , z , sr , S ) = P{S | s r } p( s r | z ) p( z| Ci ) P{Ci }

 p( s

= P{Ci } P{S | s r }

j
r

| z j ) p( z j | Ci )

j

= Pi  r

 T( z

j

, srj ,  rj ) T( z j ,  ij ,  ij )

j

179

(10)

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Now we will derive an alternative expression for eq. (10) which is more convenient for computing the marginal densities of any desired variable. Using the following relation:

p( srj | z j ) p( z j | Ci ) = p( z j , srj | Ci ) = p( z j | srj , Ci ) p( srj | Ci )
and properties (2) and (3), we define the dual densities of the model:

 ij,r  p( srj | Ci ) =



Zj

p( srj | z j ) p( z j | Ci ) dz j =T( srj ,  ij ,  ij,r )

(11)

p( srj | z j )
p( z j | Ci ) = T( z j , ij,r ,  ji ,r )
p( srj | Ci )

(12)

 ij,r ( z j )  p( z j | srj , Ci ) =

where the parameters ij,r , ij,r and  ji ,r are given by:

 ij,r  ( ij ) 2 + ( rj ) 2
(  ij ) 2 srj + (  rj ) 2  ij
 
(ij,r ) 2
j
i ,r



j
i ,r

 ij  rj
 j
i ,r

ij,r is the likelihood of the r-th elementary observation srj of the j-th attribute z j in each
component Ci and  ij,r ( z j ) is the effect of the r-th elementary information srj about the j-th attribute z j over the marginal component p( z j | Ci ) in each component Ci . Using the above notation, the MFGN model structure can be conveniently written as:

p(Ci , z , sr , S ) = Pi  r  ij,r  ij,r ( z j )

(13)

j

3.4 The Posterior Density
In the inference process the available information is combined with the model of the domain to
update our knowledge about a particular object. Given a new piece of information S we must compute the posterior distribution p( y| S ) of the desired target attributes y  z. Then, estimators
y ( S )  y can be obtained from p( y| S ) to minimize any suitable average loss function. This is
efficiently supported under the MFGN framework regardless of the complexity of the domain p(z)
and the structure of the available information S = { r sr } .
The attributes are partitioned into two subvectors z = (x, y), where y = { z d } are the desired
target attributes and x = { z o } are the rest of attributes. Accordingly, each component sr of the
available information S is partitioned as s r = ( s rx , s ry ) . The information about the target attributes
y in the r-th observation, independent from the model p(z), is denoted by sry (often y is just missing
and there are no such pieces of information) and srx represents the information about the rest of
attributes x. Using this convention we can write:

180

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

p( z , S ) = p( x , y , S ) =  Pi  r i ,r i ,r ( x ) i ,r ( y )
i ,r

Where i ,r is the likelihood of the r-th conjunction sr of S in component Ci :

i ,r   ij,r

(14)

j

and the terms  ij,r ( z j ) are grouped according to the partition of z = (x, y):

i ,r ( x )    io,r ( z o )

i ,r ( y )    id,r ( z d )

o

d

The desired posterior p(y|S) = p(y,S) / p(S) can be computed from the joint p(z,S) by marginalization: along x to obtain p(y,S) and along all z to obtain p(S). Note that each univariate marginalization of p(z,S) along attribute z j eliminates all the terms  ij,r ( z j ) in the sum (13):

p( y , S ) =  p( x , y , S ) dx =  Pi  r i ,r i ,r ( y )
X

i ,r

p( S ) =  p( z , S ) dz =  Pi  r i ,r
Z

i ,r

Therefore, the posterior density can be compactly written as:

p( y| S ) =   i ,r i ,r ( y )

(15)

i ,r

where  i ,r is the probability that the object z has been generated by component Ci and the elementary information sr is true, given the total information S:

 i ,r  P{Ci , sr | S} =

Pi  r i ,r

P

k

 l  k ,l

(16)

k ,l

and i ,r ( y ) = p( y| sry , Ci ) is the marginal density p( y| Ci ) of the desired attributes in the i-th
component, modified by the contribution of all the associated sry . Since p( y| sry , Ci ) =
p( y| sr , Ci ) , the expression (16) also follows from the expansion:

p( y| S ) =  p( y| sr , Ci ) P{Ci , sr | S}
i ,r

In summary, when the joint density and the likelihood function are approximated by mixture
models with the proposed structure, the computation of conditional densities given events of arbitrary geometry is notably simplified. Factorized components reduce multidimensional integration
to simple combination of univariate integrals and conjugate families avoid numeric integration.
This property is illustrated in Fig. 9.

181

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

S

y

p(y|C2)

1y

E{y|S}

C2

p(y,x1,x2)

 2y

C1

p(y|C1)
p(x2|C2)

p(y|S)
p(x2|C1)

22,1 22,2
1

p(x |C2)

21,1

x2

p(x1|C1)

11,2

11,1

s1
s2

x

1

S

Figure 9. Graphical illustration of the essential property of the MFGN framework. Consider the MSE estimate for y, conditioned to the event that (y, x1, x2) is in the cylindrical
region S. The required multidimensional integrations are computed analytically in terms
of the marginal likelihoods ji,r associated to each attribute and each pair of components
Ci and sr of the models for p(y, x1, x2) and for S, respectively. In this case i,r(y)=p(y|Ci)
because no information about y is supplied in S.

Example 9: Fig. 10.a shows the joint density of two continuous variables x and y. It is modeled
as a mixture with 30 factorized generalized normals. Fig. 10.b shows the likelihood function
of the event S1 = {(x  y OR x  -y) AND y>0}. Fig. 10.c shows the posterior joint density
p(x,y|S1). Fig. 10.d shows the likelihood function of the event S2 = {(x,y)  (0,0) OR x3}.
Fig. 10.e shows the posterior joint density p(x,y|S2). Fig. 10.f and 10.g show respectively the
posterior marginal density p(x|S2) and p(y|S2). These complex inferences are analytically computed under the MFGN framework, without any numeric integration.

182

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

(a)

(b)

(c)

(d)
(e)
Figure 10. Illustrative examples of probabilistic inference from arbitrary uncertain information in the MFGN framework (see Example 9).

183

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

p(y|S)

0.4

0.8

0.3

0.6

0.2

0.4

0.1

p(x|S)

0.2

y

x

-4

-2

2

4

-4

-2

2

(f)
Figure 10. (cont.).

4

(g)

3.5 Expressions for the Estimators
Approximations to the optimum estimators can be easily obtained by taking advantage of the
mathematically convenient structure of the posterior density. Under the MFGN framework, the
conditional expected value of any function g(y) becomes a linear combination of constants:

E{g ( y )| S} =  g ( y ) p( y| S ) dy =
Y

=

 
i ,r

Y

i ,r

g ( y ) i ,r ( y ) dy =



i ,r

E i ,r {g ( y )}

(17)

i ,r

where E i ,r {g ( y )}  E{g ( y )| s ry , Ci } is the expected value of g(y) in the i-th component2 modified3 by the r-th observation sry :

E i ,r {g ( y )} 



Y

g ( y )  T( z d ,  id,r , di ,r ) dy
d

We can now analytically compute the desired optimum estimators. For instance, the MSE estimator for a single continuous attribute y = z d requires the mean values E i ,r {z d } =  id,r :

y MSE ( S ) = E{ y| S} =   i ,r  id,r
i ,r

From our explicit expression for p(y|S) we can also compute the conditional cost:

{

}

e 2MSE ( S ) = E ( y  y MSE ( S ) ) | S = E{y 2 | S}  y 2MSE ( S ) =
=


i ,r

i ,r

2

[( 

d
i ,r

) + ( )
2

d
i ,r

2

2

]



    i ,r  id,r 
 i ,r


2

Note that computing the conditional expected value of an arbitrary function g(y) of several variables may be difficult. In
general g(y) can be expanded as a power series to obtain E{g(y)|S} in terms of moments of p(y|S).
3
When S is just sx (there is no information about the target attributes) the constants Ei,r{g(y)} can be precomputed from
the model of nature p(z) after the learning stage.

184

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

Therefore, given S, from Tchevichev inequality we can answer y  y MSE ( S )  2e MSE ( S )
with a confidence level above 75%. When the shape of p(y|S) is complex it must be reported explicitly (the point estimator y  y MSE ( S ) only makes sense if p(y|S) is unimodal).
Example 10: Nonlinear regression. Fig. 11 shows the mixture components and regression
lines (with a confidence band of two standard deviations) obtained in a simple example of
nonlinear dependence between two variables. In this case the joint density can be adequately
approximated by 3 or 4 components: MSE (1 component) = 0.532, MSE (2 comp.) = 0.449,
MSE (3 comp.) = 0.382, MSE (4 comp.) = 0.381.

(a)
(b)
Figure 11. Nonlinear regression example: (a) 2 components, (b) 4 components.

When the target y is symbolic we must compute the posterior probability of each value. In this
case all the di ,r = 0 and the id,r =  id are the possible values  taken by y = z d . Collecting together all the id,r = , as in (8), eq. (15) can be written as:

p ( y| S ) =    i ,r , T ( y ,  )


i ,r

where  i ,r , are the coefficients of the impulses located at . The posterior probability of each
value is:

q   P{ y = | S} =   i ,r ,
i ,r

For instance, the minimum error probability estimator (EP) is:

y EP ( S ) = argmax



q

and any desired rejection threshold can be easily established. We can reject the decision if the entropy of the posterior, H =  q log q, or the estimated error probability, E = 1- max q, are too
high.
Example 11: Nonparametric Pattern Recognition. Fig. 12 shows a bivariate data set with elements from two different categories, represented as the value of an additional symbolic attribute. The joint density can be satisfactorily approximated by a 6-component mixture (Fig.
12.a). The decision regions when the rejection threshold was set to 0.9 are shown in Fig. 12.b.

185

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Note that Statistical Pattern Classification usually start from an (implicit or explicit) approximation to the class-conditional densities. In contrast, we start from the joint density, from
which the class-conditional densities can be easily derived (Fig. 12.c).

(a)
(b)
(c)
Figure 12. Simple nonparametric 2feature pattern recognition task and its 3attribute
joint mixture model: (a) Feature space and mixture components. (b) Decision boundary.
(c) One of the class-conditional densities.

The computation of the optimum estimators for other loss functions is straightforward. Observe that the estimators are based on the combination of different rules, weighted by their degree
of applicability. This is a typical structure used by many other decision methods. In our case, since
the components of the joint density have independent variables the rules reduce to constants, the
simplest type of rule.
3.6 Examples of Elementary Pieces of Information
Some important types of elementary observations srj about z j are shown, including the corresponding likelihoods ij,r and modified marginals  ij,r ( z j ) (j=d) required in expression (15).
Exact information: srj = z j . The observation is modeled by an impulse:

p( srj | z j ) =

T( srj , z j ) = ( srj  z j ) . Therefore:
 ij,r = T( srj ,  ij ,  ij )
 ij,r ( z j ) = T( z j , srj )
The contribution ij,r of exact information about the input attribute z j is the standard likelihood p( z j | Ci ) of the observed value z j in each component. On the other hand, if we acquire
exact information about a target attribute z j (when there is only one (R=1) elementary observation
and s j = z j ) then the inference process is trivially not required: p( z j | S ) = ( z j  s j ) .
Gaussian noise with bias rj and standard deviation  rj : The observation is modeled by a 1component mixture: p( srj | z j ) = T( srj , z j + rj ,  rj ) , which can also be expressed as a 95% confidence interval z j  srj + rj  2 rj . From property (2-2):

186

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

 ij,r = T( srj ,  ij  rj , ( ij ) 2 + (  rj ) 2 )
The effect of a noisy input z j  srj  2 rj is equivalent to the effect of an exact input z j = srj
in a mixture with components of larger variance:  ij  ( ij ) 2 + (  rj ) 2 . Uncertainty spreads the
effect of the observation, increasing the contribution of distant components.
Example 12: Fig. 13.a shows a simple two-attribute domain approximated by a 3-component
mixture. We are interested in the marginal density of attribute x given different degrees of uncertainty  in the input attribute y  .4  2, modeled by

p( s y | y ) = T( s y ,.4,  ) . If  = 0

we have the sharpest density (A) in Fig. 13.b, providing x.4.5. If  = .25 we obtain density
(B) and x.3.7. Finally, if  = .5 we obtain density (C) and x.2.8. Obviously, as the uncertainty in y increases, so does the uncertainty in x. The expected value of x moves towards
more distant components, which become more likely as the probability distribution of y expands. In this situation an interesting effect appears: the mode of the marginal density does not
change at the same rate than the mean. Uncertainty in y skews p(x). This effect suggests that
the optimum estimators for different loss functions are not equally robust against uncertainty.

A
B
C

(a)
(b)
Figure 13. Effect of the amount of uncertainty (see text). (a) Data set and 3-component
model. (b) p(x | uncertain ys around 0.4).

j
j
For the output role,  i ,r ( z ) becomes the original marginal, modified in location and disper2
2
2
sion towards srj according to the factor  = ( ij ) / [( ij ) + (  rj ) ] , which quantifies the relative
importance of the observation:

 ij,r ( z j ) = T(z j ,  ( srj   rj ) + (1   ) ij ,  1/ 2  rj )
Missing data. When there is no information about the j-th attribute, srj = {z j = ?} , the observation can be modeled by p( srj | z j ) = constant or, equivalently, p( srj | z j ) = T( srj , a , b) with a
arbitrary and b  . All the components contribute with the same weight:

 ij,r = p( z j = anything| Ci )  constant  1
If the target is missing the  ij,r ( z j ) reduce to the original marginal components:

187

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

 ij,r ( z j ) = T( z j ,  ij ,  ij ) = p( z j | Ci )
Arbitrary uncertainty. In general, any unidimensional relative likelihood function can be approximated by a mixture of generalized normals, as shown in Example 6, where ij,r and  ij,r ( z j )
are given respectively by eqs. (11) and (12).
Intervals. Some useful functions cannot be accurately approximated by a small number of normal
components. A typical example is the indicator function of an interval, used to model an uncertain
observation where all the values are equally likely: srj = {z j  (a, b)} . If z j is only considered as
input, we can use the shortcut  ij,r = Fi j (b)  Fi j (a ) , where Fi j ( z j ) is the cumulative distribution of the normal marginal component p( z j | Ci ) . Unfortunately, the expression for  ij,r ( z j ) ,
required for z j considered as output, may not be so useful for computing certain optimum estimaj
j
j
tors.  i ,r ( z ) is the restriction of p(z j|Ci) to the interval (a,b) and normalized by i ,r .
Disjunction and conjunction of events. Finally, standard probability rules are used to build
structured information from simple observations: if from subjective judgments or objective evidence we ascribe relative degrees of credibility  rj to several observations srj about z j , the overall
j
j j
likelihood becomes  i =  r  r  i ,r . In particular, if s j = {z j =  1 OR z j =  2 } and the two

possibilities are equiprobable then  ij = p( 1 | Ci ) + p( 2 | Ci ) . Analogously, conjunctions of
events translate to multiplication of likelihood functions.
3.7 Summary of the Inference Procedure
Once the domain p(z) has been adequately modeled in the learning process (as explained in Section
4), the system enters the inference stage over new, partially specified objects. From the parameters
of the domain p(z) ( Pi ,  ij and  ij ) and the parameters of the model of the observation p(S|z)
(  r , srj and  rj ), we must obtain the parameters  ij,r ,  id,r and di ,r of the desired marginal posterior densities and estimators. The inference procedure comprises the following steps:


Compute the elementary likelihoods ij,r , using eq. (11).



Obtain the product i ,r for each conjunction sr and component Ci , using eq. (14).



Normalize Pi  r  i ,r to obtain the coefficients  i ,r of the posterior, using eq. (16).



Choose the desired target attributes y = { z d } and compute the parameters id,r , and

di ,r of the modified component marginal densities  id,r ( z d ) using eq. (12).


Report the joint posterior density of y. Show graphs of the posterior marginal densities
of the desired attributes z d using eq. (15). Provide optimum (point, interval, etc.) estimators using eq. (17).

Example 13: Iris Data. The inference procedure is illustrated over the well known Iris benchmark: 150 objects represented by four numeric features (x, y, z and w) and one symbolic category U  {U1 (setosa), U2 (versicolor), U3 (virginica)}. The whole data set was divided into
two disjoints subsets for training and validation. The joint density can be satisfactorily approxi-

188

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

mated (see Section 4) by a 6component mixture (the error rate classifying U in the validation
set without rejection was 2.67%). Fig. 14 shows two projections of the 150 examples and the location of the mixture components learned from the training subset. The parameters of the mixture are shown in Table 3.

(a)
(b)
Figure 14. Two views of the Iris examples and the components of the joint density mixture model. U1: white, U2: black, U3: gray. (a) Attributes x, y (b) Attributes z, w.

i

Pi

1
2
3
4
5
6

0.15
0.13
0.21
0.18
0.15
0.17

 ix

 ix

 iy

 iy

 iz

 iz

7.13 0.48 3.12 0.34 6.17 0.45
5.48 0.41 2.50 0.28 3.87 0.32
6.29 0.39 2.93 0.27 4.59 0.20
4.75 0.23 3.25 0.23 1.42 0.21
5.36 0.26 3.76 0.29 1.51 0.16
6.16 0.42 2.77 0.28 5.22 0.30
Table 3. Parameters of the Iris Data Joint Density Model

 iw

 iw

2.18
1.20
1.45
0.19
0.32
1.94

0.20
0.21
0.14
0.05
0.10
0.23

P{U1|Ci} P{U2|Ci} P{U3|Ci}

0
0
0
1
1
0

0
0.93
1
0
0
0.00

Table 4 shows the results of the inference process in the following illustrative situations:
Case 1: Attribute z is known: S = {z = 5}.
Case 2: Attributes x and U are known: S = {(x = 5.5) AND (U=U2)}.
Case 3: Attribute x is uncertain: S = {x  71}.
Case 4: Attributes x and w are uncertain: S = {(x  71) AND (w  10.5)}. Note that uncertainty decreases when more information is supplied (compare with Case 3).
Case 5: Structured query expressed in terms of logical connectives over uncertain elementary
events: S = {[(z  13) OR (z  73)] AND [(U = U1) OR (U = U2)]}.

189

1
0.07
0.00
0
0
1

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

CASE
1
2
3
4
5

INPUT
OUTPUT
INPUT
OUTPUT
INPUT
OUTPUT
INPUT
OUTPUT
INPUT
OR

OUTPUT

X
?
6.20.9
5.5
5.5
71
6.70.9
71
6.50.7
?
?

y
?
2.80.6
?
2.60.6
?

z
5.0
5.0
?
4.00.8
?

w
?
1.80.6
?
1.30.4
?

3.00.7
?

5.31.8
?

2.90.6
?
?

1.80.8
10.5
1.30.3
?
?

5.31.2

3.30.9

4.50.8
13
73
23

(approx.
unimodal)

(unimodal)

(bimodal)

0.51

U
?

U2: 22% U3: 78%
U2: 100%
U2: 100%
?
U2: 36% U3: 63%
?
U2: 95% U3: 5%
U1: 50% U2: 50%
U1: 50% U2: 50%
U1: 75% U2: 25%

(bimodal)

Table 4. Some Inference Results Over the IRIS Domain

The consistency of the results can be visually checked in Fig. 14. Finally, Table 5 shows the
elementary likelihoods

i
1
2
3
4
5
6

ix,1

iy,1

i,j r of Case 5, illustrating the essence of the method.

iz,1

iw,1

Ui ,1

i ,1

ix,2

1
1
1
0
1
.001
0
1
1
1
1
.045
.47 .02
1
1
1
1
.016
.50 .01
1
1
1
1
.254
.50 .13
1
1
1
1
.250
.50 .13
1
1
1
0
1
.006
0
Table 5. Elementary likelihoods in Case 5 from Table 4.

iy,2
1
1
1
1
1
1

iz,2

iw,2

Ui ,2

i ,2

.221
.032
.074
3E-4
4E-4
.132

1
1
1
1
1
1

0
.47
.50
.50
.50
0

0
.02
.04
.00
.00
0

3.8 Independent Measurements
One of the key features of the MFGN framework is the ability to infer over arbitrary relational
knowledge about the attributes, in the form of a likelihood function adequately approximated by a
mixture model with the structure of eq. (9). For instance, we could answer questions as: what happens to z d when z i tends to be less than z j ? (i.e., when p(S|z) is high in the region z i  z j < 0 ).
However, there are situations where the observations over each single attribute z j are statistically
independent: we have information about attributes (e.g. z i is around a and z j is around b) but not
about attribute relations. We will pay attention to this particular case because it illustrates the role
of the main MFGN framework elements. Furthermore, many practical applications can be satisfactorily solved under the assumption of independent measurements or judgments. In this case, the
likelihood of the available information can be expressed as the conjunction of n marginal observations s j about z j :

p ( S | z ) =  p( s j | z j )
j

190

(18)

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

This means that the sum of products in equation (9) is complete, i.e., it includes all the elements
in the N-fold cartesian product of attributes:

p( S | z ) =    rj T( z j , srj ,  rj )
j

r

where  j  rj =  r . This factored likelihood function can be considered also as a 1-component
mixture (with R=1 in (9) and s j  s1j ) where the marginal observation models are allowed to be
mixtures of generalized normals: p( s j | z j ) =  r '  rj' T( z j , srj' ,  rj ' ) . In this case we can even think
1
1
of function valued attributes z  [ f ( z ),..., f n ( z n )] , where f j ( z j )  p( s j | z j ) models the

range and relative likelihood of the values of z j . Loosely speaking, attributes with concentrated
f j ( z j ) may be considered as inputs, and attributes with high dispersion play the role of outputs.
Since y is conditionally independent of s x given Ci , the posterior can be obtained from the expansion:

p( y| S ) =  p( y| S , Ci ) P{Ci | S} =  p( y| Ci , s y ) P{Ci | S}
i

(19)

i

The interpretation of (19) is straightforward. The effect of sx over y = {zd} must be computed
through x = {zo} and the components Ci . Then, a simple Bayesian update of p( y| s x ) as a new
prior is made using s y (see Fig. 15).

Ci
z1

i

p( z j | Ci )

zd
 id ( z d )

s1

...


...

sd

j
i

zj
p( s j | z j )

sj

...

Figure 15. Structure of the MFGN inference process from independent pieces of information. In this case, the likelihood function is also factorizable. The data flow in the inference process is shown by dotted arrows.

191

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

4. Learning from Uncertain Information
In the previous section, we have described the inference process from uncertain information under
the MFGN framework. Now we will develop a learning algorithm for the model of the domain,
where the training examples will be also uncertain. Specifically, we must find the parameters Pi ,

 ij ,  ij (or ti j, ) of a mixture with structure (7) to approximate the true joint density p(z) from a
training i.i.d. random sample {z(k)}, k=1..M, partially known through the associated likelihood
functions {S(k)} with structure (9).
4.1 Overview of the EM Algorithm
Maximum Likelihood estimates for the parameters of mixture models are usually computed by the
well-known Expectation-Maximization (EM) algorithm (Dempster, Laird and Rubin 1997, Redner
and Walker 1984, Tanner 1996), based in the following idea. In principle, the maximization of the
training sample likelihood J =  k p( z ( k ) ) is a mathematically complex task due to the product of
sums structure. However, note that J could be conveniently expressed for maximization if the components that generated each example were known (this is called complete data in EM terminology). The underlying credit assignment problem disappears and the estimation task reduces to several uncoupled simple maximizations. The key idea of EM is the following: instead of maximizing
the complete data likelihood (which is unknown), we can iteratively maximize its expected value
given the training sample and the current mixture parameters. It can be shown that this process
eventually achieves a local maximum of J.
Instead of a rigorous derivation of the EM algorithm, to be found in the references (see especially McLachlan and Krishnan, 1997), we will present here a more heuristic justification which
provides insight for generalizing the EM algorithm to accept uncertain examples. We will review
first the simplest case, where no missing or uncertain values are allowed in the training set. The
parameters of the mixture are conditional expectations:

E z |Ci {g ( z )| Ci )} =  g ( z ) p( z| Ci ) dz
Z

(20)

2
2
In particular,  ij = E{z j | Ci } , ( ij ) = E{( z j   ij ) | Ci } and ti j, = E{I {z j = }| Ci } . The

mixture proportions are Pi = E{ P{Ci | z} } .
We rewrite the conditional expectation (20) using Bayes Theorem in the form of an unconditional expectation:

E z |Ci {g ( z )| Ci )} =  g ( z ) P{Ci | z} p( z ) / P{Ci } dz =

(21)

= E z {g ( z ) P{Ci | z}} / Pi

(22)

Z

The EM algorithm can be interpreted as a method to iteratively update the mixture parameters
using expression (22) in the form of an empirical average over the training data4. Starting from a
tentative, randomly chosen set of parameters, the following E and M steps are repeated until the
4

Expression (21) can be also used for iterative approximation of explicit functions which are not indirectly known by
i.i.d. sampling (e.g., subjective likelihood functions sketched by the human user, as in Example 4). In this case p(z) is set
to the target function and P{Ci| z} is computed from the current mixture model.

192

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

total likelihood J no longer improves (the notation (expression)(k) means that (expression) is computed with the parameters of example z(k)):
( )
( )
(E) Expectation step. Compute the probabilities qi k  P{Ci | z k } that the k-th example has been
generated by the i-th component of the mixture:

qi( k )  p( z ( k ) | Ci ) P{Ci } / p( z ( k ) )
(M) Maximization step. Update the parameters of each component using all the examples, weighted
by their probabilities qi( k ) . First, the a priori probabilities of each component:

Pi 

1
 q (k )
M k i

Then, for continuous variables, the mean values and standard deviations in each component:

1
MPi

 ij 
( ij ) 2 

1
MPi

 [q

z j ]( k )

i

k

 [q

( z j ) 2 ]( k )  ( ij ) 2

i

(23)

k

and for symbolic variables, the probabilities of each value:

ti j, 

1
MPi

 [q

I {z j = }]( k )

i

k

4.2 Extension to Uncertain Values
In general, in the MFGN framework we do not know the true values z j of the attributes in the
training examples, required to compute g ( z ) P{Ci | z} in the (empirical) expectation (22). Instead,
we will start from uncertain observations S ( k ) about the true training examples z
likelihood functions expressed as mixtures of generalized normals:

(k )

, in the form of

p( S ( k ) z ( k ) ) =  P{S ( k ) sr( k ) } p( sr( k ) z ( k ) )
r

Therefore, we must express the expectation (22) over p(z) as an unconditional expectation
over p(S), the distribution which generates the available information about the training set. This
can be easily done by expanding p( z| Ci ) in terms of S:

E z |Ci {g ( z )| Ci )} =  g ( z ) p( z| Ci ) dz
Z

[  p ( z | S , C ) p ( S | C ) dS ] dz
=  [ g ( z ) p( z| S , C ) dz ] P{C | S} p( S ) dS / P{C }
=

S

Z



Z

g ( z)

i

S

i

i

i

If we define

193

i

(24)

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

i ( S )  E z |S ,Ci {g ( z )| S , Ci )} =  g ( z ) p( z| Ci ) p( S | z ) dz / p( S | Ci )
Z

then the parameters of p(z) can be finally written5 as an unconditional expectation over the observable p(S) in a form similar to eq. (22):

E z |Ci {g ( z )| Ci )} = E S {i ( S ) P{Ci | S}} / Pi

(25)

This expression justifies an extended form of the EM algorithm to iteratively update the parameters of p(z) by averaging i ( S ) P{Ci | S} over the available training information { S ( k ) }
drawn from p(S). This can be considered as a numerical/statistical method for solving p(z) in the
integral equation:



Z

p ( S | z ) p ( z ) dz = p ( S )

Note that we cannot approximate p(S) as a fixed mixture in terms of p( S | Ci ) and then computing back the corresponding p( z| Ci ) because, in general, p( S | z ) will be different for the different training examples. For the same reason, elementary deconvolution methods are not directly
applicable.
This kind of problem is addressed by Vapnik (1982, 1995) to perform inference from the result of
indirect measurements. This is an ill-posed problem, requiring regularization techniques. The
proposed extended EM algorithm can be considered as a method for empirical regularization, in
which the solution is restricted to be in the family of mixtures of (generalized) gaussians. EM is
also proposed by You and Kaveh (1996) for regularization in the context of image restoration.
The interpretation of (25) is straightforward. Since we do not know the exact z required to
approximate the parameters of p(z) by empirically averaging g ( z ) P{Ci | z} , we obtain the same
result by averaging the corresponding i ( S ) P{Ci | S} in the S domain, where i ( S ) plays the
role of g(z) in (22). As z is uncertain, g(z) is replaced by its expected value in each component
given the information about S. In particular, if there is exact knowledge about the training set at( )
tributes ( S ( k ) = z k , i.e., R = 1 and the marginal likelihoods are impulses) then (25) reduces to
(22). Fig. 16. illustrates the approximation process performed by the extended version of the EM
algorithm in a simple univariate situation.
It is convenient to develop a version of the proposed Extended EM algorithm for uncertain
training sets, structured as tables of (sub)cases  (uncertainly valued) variables (see Fig. 17).
First, let us write eq. (24) expanding S in terms of its components sr:

p( z| Ci , S ) P{Ci | S ) = p( z , Ci | S )
=

 p( z, C | s ) P{s | S} =  p( z| C , s ) P{C | s } P{s | S}
i

r

r

r

i

r

i

r

r

r

Therefore

i ( S ) P{Ci | S} =  i ,r ( s r ) P{Ci , s r | S}
r

This result can be also obtained from the relation Ez{w(z)} = ES{ Ez|S{w(z)| S} } for w(z)  g(z) P{Ci|z} and Bayes Theorem.

5

194

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

(a)

(b)
(d)

(c)

Figure 16. The extended EM algorithm iteratively reduces the (large) difference between
(a) the true density p(z), and (b) the mixture model p ( z ) , indirectly through the (small)
discrepancies between (c) the true observation density p(S) and (d) the modeled observation density p ( S ) . In real cases p(S) must be estimated from a finite i.i.d. sample
{S(k)}.

S(1)

s1(1)
s2(1)
s(2)
s1(3)
s2(3)
s3(3)
...

.4
.6
1
.2
.5
.3
...

S(2)
S(3)

 (r k )

S(r)

( sr1 ,  1r ) ( k )

( srj ,  rj ) ( k )

...

...

...
...
Figure 17. Structure of the uncertain training information for the Extended EM Algorithm. The coefficients

 (r k ) are normalized for easy detection of the rows included in

each uncertain example. When z



(k )

= 1 and all the

(k)

(k)

is not uncertain, S

reduces to a single row with

 = 0.
j

Using the notation introduced in (12),

i ,r ( s r )  E z |sr ,Ci {g ( z )| s r , Ci } =



Z

g ( z ) p( z| s r , Ci ) dz =



Z

P{Ci , s r | S} = P{Ci | s r } P{s r | S} =  i ,r
we can write (25) as:

195

g ( z )  ij,r ( z j ) dz
j

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO



E z|Ci {g (z ) | Ci } = E S   i ,r  g (z )  ij,r ( z j )dz / Pi
Z
j
 r

In the MFGN framework the contributions i ,r ( s r ) P{Ci , s r | S} to the empirical expected
values required by the Extended EM algorithm can be obtained again without numeric integration.
We only need to consider the case g(z) = z j to compute the means  ij and probabilities ti j, , and
g(z) = ( z j )2 for the deviations  ij . From (12) we already know an explicit expression for the parameters of  ij,r ( z j ) = T( z j ,  ij,r ,  ji ,r ) . Hence:



Z

z j i ,r ( z ) dz =

 (z
Z

j



Z

z j  ij,r ( z j )dz j =  ij,r

) 2 i ,r ( z ) dz = (  ij,r ) 2 + (  ji ,r ) 2

In conclusion, the steps of the Extended EM algorithm are as follows:
(E) Expectation step. Compute all the elementary likelihoods of the training set:

(

 ij,r( k ) = T srj ,  ij , ( ij ) 2 + (  rj ) 2

)

(k )

(26)

( )
( )
Obtain the likelihood of each conjunction sr k of example S k in component Ci:

 i(,kr) =   ij,r( k )
j

( )
Obtain the total likelihood of example S k :

 ( k )  p( S ( k ) ) =   Pi  r( k )  i(,kr )
i

r

( )
( )
( )
Compute the probabilities qi ,kr  P{Ci , s r k | S k } that the r-th component of the k-th exam-

ple has been generated by the i-th component of the mixture:

qi(,kr )   i(,kr) = Pi  r ( k )  i(,kr) /  ( k )
(M) Maximization step. Update the parameters of each component Ci using all the components

s r( k ) of all the examples weighted with their probabilities qi(,kr ) . First, the prior probabilities of each
component:

Pi 

1
M

 q
k

(k )
i ,r

r

Then the mean value and standard deviation in each component:

 ij 

1
MPi

  [q
k

r

196

i ,r

ij,r

]

(k )

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

( ij ) 2 

1
MPi

  [q
k

i ,r

]

[( ij,r ) 2 + ( ij ,r ) 2 ]

r

(k )

 ( ij ) 2

(27)

For symbolic variables under representation (8) we may use:

ij,r( k ) =  P{srj = }( k ) ti j,

(26)



ti j, 

1
MPi

  [q
k

i ,r

P{srj = } t i j, /  ij,r

r

]

(k )

(27)

Consider the particular case in which the attributes in the training examples are contaminated
with unbiased Gaussian noise. The likelihood of the uncertain observations is modeled by 1( )
( )
( )
( )
( )
( ) 2
component mixtures: p( s k | z k ) =  j T( z j k , s j k ,  j k ) , where ( j k ) is the variance of
the measurement process over z j ( k ) which obtains the observed value s j ( k ) . This can be also ex( )
( )
( )
pressed as a confidence interval z j k  s j k  2 j k . In this case, the basic EM algorithm (23)
( )
can be easily modified to take into account the effect of the uncertainties  j k . In the E step, com( )
pute qi k using the following deviations:

 ij  ( ij ) 2 + (  j ( k ) ) 2
and, in the M step, apply the substitution:

z j ( k )   s j ( k ) + (1   )  ij

[

( z j ( k ) ) 2   s j ( k ) + (1   ) ij

] +  [ ]
2

j( k ) 2

where

( ij ) 2
= j 2
( i ) + (  j ( k ) ) 2
measures the relative importance of the observed s j k for computing the new  ij and  ij .
The previous situation illustrates how missing values must be processed in the learning stage.
( )
j (k )
If z
is exact then  j k = 0 and  = 1, so the original algorithm (23) is not changed. In the other
( )
extreme, if z j ( k ) is missing, which can be modeled by  j k  , we get  = 0 and therefore the
( )
observation s j k does not contribute to the new parameters at all. The correct procedure to deal
with missing values in the MFGN framework is simply omitting them in the empirical averages.
Note that this fact arises from the factorized structure of the mixture components, providing conditionally independent attributes. Alternative learning methods require a careful management of
missing data to avoid biased estimators (Ghahramani & Jordan 1994).
( )

4.3 Evaluation of the Extended EM Algorithm
We have studied the improvement of the parameter estimations when the uncertainty of the observations, modeled by likelihood functions, is explicitly taken into account. The proposed Extended

197

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

EM is compared with the EM algorithm over the "raw" observations (Basic EM), which ignores the
likelihood function and typically uses just its average value (e.g., given x82, Basic EM uses
x=8). We considered a synthetic 3-attribute domain with the following joint density:
p(x,y,w) = 0.5 (x,0,2) (y,0,1) (w,white)
+ 0.5 (x,2,1) (y,2,2) (w,black)

TT

TT

TT

Different learning experiments were performed with varying degrees of uncertainty. In all
cases the training sample size was 250. All trained models had the same structure as the true density (2 components), since the goal of this experiment is to measure the quality of the estimation
with respect to the amount of uncertainty, without regard of other sources of variability such as
local minima, alternative solutions, etc., which are empirically studied in Section 5. Table 6 shows
the mixture parameters obtained by the learning algorithms. Fig. 18 graphically shows the difference between Extended and Basic EM in some illustrative cases.
Case 0: Exact Data (Fig. 18.a).
Cases m %: Results of the Extended EM learning algorithm when there is a m % rate of missing
values in the training data.
Case 1: Basic EM when attribute y is biased +3 units with probability 0.7. Case 2: Extended EM
algorithm over Case 1 (see Fig. 18.b). Here, the observed value is sy=y+3 in 70% of the samples
and sy=y in the rest. In all samples, Basic EM uses the observed value sy and Extended EM uses
the explicit likelihood function f(y) = 0.3 (ysy) + 0.7 (y(sy3)).
Case 3: Basic EM when attributes x and y have Gaussian noise with  = 0.5 and w is changed with
probability 0.1. Case 4: Extended EM algorithm over Case 3.
Case 5: Basic EM when x and y have Gaussian noise with  = 1 and w is changed with probability
0.2. Case 6: Extended EM algorithm over Case 5 (see Fig. 18.c).
Case 7: Basic EM when x and y have Gaussian noise with  = 2 and w is changed with probability
0.3. Case 8: Extended EM algorithm over Case 7 (see Fig. 18.d).

T

Case 9: Extended EM when values y>3 are missing (censoring). Case 10: Extended EM over Case
9 when the missing y values are assumed to be distributed as
(y, 4, 1), providing some additional information on the data generation mechanism.
Table 6 and Fig. 18 confirm that for small amounts of deterioration in relation to the sample
size, the estimates computed by the basic EM Algorithm over the raw observed data are similar
to those obtained by the Extended EM algorithm (e.g., Cases 3 and 4). However, when the data sets
are moderately deteriorated the true joint density can be correctly recovered by Extended EM using
the likelihood functions of the attributes instead of the raw observed data (e.g., Cases 5 and 6, Fig.
18.c). Finally, when there is a very large amount of uncertainty with respect to the training sample
size the true joint density cannot be adequately recovered (e.g., Cases 7 and 8, Fig. 18.d).

198

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

(a)

(b)

(c)
(d)
Figure 18. Illustration of the advantages of the Extended EM algorithm (see text). (a)
Case 0 (exact data). (b) Cases 1 and 2 (biased data). (c) Cases 5 and 6 (moderated noise).
(d) Cases 7 and 8 (large noise). All figures show the true mixture components (gray ellipses), the available raw observations (black and white squares), the components estimated by Basic EM from the raw observations (dotted ellipses) and the components estimated by Extended EM taking into account the likelihood functions of the uncertain
values (black ellipses).

Note that the ability to learn from uncertain information suggests a method to manage non
random missing attributes (e.g., censoring) (Ghahramani & Jordan 1994) and other complex
mechanisms of uncertain data generation. As illustrated in Case 9, if the missing data generation
mechanism depends on the value of the hidden attribute, it is not correct to assign equal likelihood
to all components. In principle, statistical studies or other kind of knowledge may help to ascertain
the likelihood of the true values as a function of the available observations. For instance, in Case 10
we replaced the missing attributes of Case 9 by normal likelihoods y  42 (i.e, y is high), improving the estimates of the mixture parameters.

199

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Case

P1

1x

1y

1x

1y

t1,wwhite

 2x

 2y

true
.5
0
0
2
1
1
2
2
0
.48 -.04 .03 2.11 1.00 1.00 2.09 1.83
20%
.48 .16 -.03 1.91 1.01 0.96 2.31 2.09
40%
.49 .14 -.16 1.78 .99 0.95 2.39 2.29
60%
.45 .02 -.29 2.50 .78 1.00 1.86 2.01
80%
.49 -.11 1.69 2.21 1.73 .50 1.91 0.31
1
.48 .06
.90 1.88 1.73 1.00 1.88 2.98
2
.48 -.04 .05 1.90 .92 1.00 1.98 1.97
3
.47 .02
.09 1.60 1.02 .87 2.00 1.78
4
.49 .27 -.08 1.97 .90
.70 2.10 2.06
5
.43 -.04 -.18 2.40 1.48 .82 1.85 1.52
6
.54 -.07 -.02 1.97 1.09 .56 1.93 2.11
7
.46 .15 -.16 2.73 2.53 .31 1.94 1.62
8
.79 .87
.08 2.09 1.52 .51 1.96 3.61
9
.48 .32 -.02 1.77 1.10 1.00 1.92 0.67
10
.45 .00
.03 2.20 1.01 1.00 2.13 1.55
Table 6. Parameter Estimates from Uncertain Information (see text)

 2x

 2y

t2,wblack

1
1.00
.88
.94
1.03
1.14
.96
1.01
1.14
1.01
1.52
.85
2.47
0.87
0.94
1.04

2
2.08
2.10
2.14
1.77
0.68
2.40
1.90
2.07
1.94
2.33
1.69
2.90
1.21
1.22
1.77

1
1.00
1.00
1.00
1.00
0.47
1.00
1.00
0.85
0.71
.80
.62
.29
.54
1.00
1.00

Example 14: Learning from examples with missing attributes has been performed over the IRIS
domain to illustrate the behavior of the MFGN framework. The whole data set was randomly
divided into two subsets of equal size for training and testing. 5-component mixture models
were obtained and evaluated, combining missing data proportions of 0% and 50%. The error
prediction on attribute U (plant class) was the following:

missing attributes

training set
0%
0%
50%
50%

test set
0%
50%
0%
50%

prediction error
2.7%
12.0%
4.0%
18.7%

In the relatively simple IRIS domain, the performance degradation due to 50% missing attributes is much greater in inference than in learning stage. The Extended EM algorithm is able
to correctly recover the overall structure of the domain from the available information.

4.4 Comments
Convergence of the EM Algorithm is very fast, requiring no adjustable parameters such as learning
rates. The algorithm is robust with respect to the random initial mixture parameters: bad local
maxima are not frequent and alternative solutions are usually equally acceptable. All the examples
contribute to all the components, which are never wasted by unfortunate initialization. For a fixed
number of components, the algorithm progressively increases the likelihood J of the training data
until a maximum is reached. When the number of components is incremented the maximum J also
increases, until a limit value is obtained that cannot be improved using extra components (Fukunaga 1990). Some simple heuristics can be incorporated to the standard Expectation-Maximization
scheme to control the value of certain parameters (e.g., lower bounds can be established for variances) or the quality of the model (e.g., mixture components can be eliminated if their proportions
are too small).
In our case, factorized components are specially convenient because matrix inversions are not
required and, what is more important, uncertain and missing values can be correctly handled in a

200

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

simple and unified way, for heterogeneous attribute sets. It is not necessary to provide models for
uncertain attribute correlations since no covariance parameters must be estimated. Finally, the
training sample size must be large enough in relation to both the degree of uncertainty of the examples and the complexity of the joint density model in order to obtain satisfactory approximations.
On the other hand, the number of mixture components required for a satisfactory approximation to the joint density must be specified. A pragmatic option is the minimization of the experimental estimation cost over the main inference task, if it exists. For instance, in regression we
could increase the number of components until an acceptable estimation error is obtained over an
independent data set (cross-validation). The same idea applies to pattern classification: use the
number of components that minimizes the error rate over an independent test set. However, one of
the main advantages of the proposed method is the independence between the learning stage and
the inference stage, where we can freely choose and dynamically modify the input and output role
of the attributes. Therefore, a global validity criterion is desirable. Some typical validation methods
for mixture models are reviewed in McLachlan & Basford (1988); the standard approach is based
on likelihood ratio tests on the number of components. Unfortunately, this method does not validate
the mixture itself, only selects the best number of components (DeSoete 1993).
Since the MFGN framework provides an explicit expression for the model p(z), we can apply
statistical tests of hypothesis over an independent sample T taken from the true density (e.g. a subset of the examples reserved for testing) to find out if the obtained approximation is compatible
with test data. If the hypothesis H = {T comes from p(z)} is rejected, then the learning process must
continue, possibly increasing the number of components. It is not difficult to build some statistical
tests, e.g. over moments of p(z), because their sample means and variances can be directly obtained.
However, as data sets usually include symbolic and numeric variables, we have also developed a
test on the expected likelihood of the test sample, which measures how well p(z) covers the examples. The mean and variance of p(z) can be easily obtained using the properties of generalized
normals. Some experiments over simple univariate continuous densities show that this test is not
very powerful for small sample sizes, i.e. incompatibility is not always detected, while other standard tests significantly evidence rejection. Nevertheless, clearly inaccurate approximations are
detected, results improve as the sample size increases and the test is valid for data sets with uncertain values.
The Minimum Description Length (Li & Vitnyi 1993) principle can be also invoked to select
the optimum number of components by trading-off the complexity of the model and the accuracy in
the description of the data.

201

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

5. Discussion and Experimental Results
5.1 Advantages of Joint Models
Most inductive inference methods compute a direct approximation of the conditional densities of
interest, or even obtain empirical decision rules without explicit models of the underlying conditional densities. In these cases, both the model and the learning stage depend on the selected input /
output role of the variables. In contrast, we have presented an inference and learning method based
on an approximation to the joint probability density function of the attributes by a convenient
parametric family (a special mixture model). The MFGN framework works as a pattern completion
machine operating over possibly uncertain information. For example, given a pattern classification
problem, the same learning stage suffices for predicting class labels from feature vectors and for
estimating the value of missing features from the observed information in incomplete patterns. The
joint density approach finds the regions occupied by the training examples in the whole attribute
space. The attribute dependences are captured at a higher abstraction level than the one provided by
strictly empirical rules for pre-established target variables. This property is extremely useful in
many situations, as shown in the following examples.
Example 15: Hints can be provided for inference over multivalued relations. Given the data
set and model from Example 10, assume that we are interested in the value of x for y = 0. We
obtain the bimodal marginal density shown in Fig. 19.a and the corresponding estimator x 
0.2  1.4 which is, in some sense, meaningless. However, if we specify the branch of interest
of the model, inferring from y = 0 AND x  -11 (i.e., x is small), we obtain the unimodal
marginal density in Fig. 19.b and the reasonable estimator x  0.80.5.

(a)
(b)
Figure 19. The desired branch in multivalued relations can be selected by providing
some information about the output values. (a) Bimodal posterior density inferred from
y=0. (b) Unimodal posterior density inferred from y = 0 and the hint x is small.

Example 16: Image Processing. The advantages of a joint model supporting inferences from
partial information on both inputs and outputs can be illustrated in the following application
with natural data (see Fig. 20). The image in Fig. 20.a is characterized by a 5-attribute density
(x, y, R, G, B) describing position and color of the pixels. A random sample of 5000 pixels was
used to build a 100-component mixture model. We are interested in the location of certain ob-

202

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

jects in the image. Figs. 20.b-f show the posterior density of the desired attributes given the following queries:


"Something light green". Fig. 20.b. Two groups can be easily identified in the posterior
density, corresponding to the upper faces of the green objects6. S = C1={x, y unknown;
R=11050, G=24510, B=16050}.



"Something light green OR dark red". Fig. 20.c. We find the same groups as above and an
additional, more scattered group, corresponding to the red object. This greater dispersion
arises from the larger size of the red object and also from the fact that the R component of
dark red is more disperse than the G component of light green. S =Two equiprobable components with C1 as above and C2={x, y unknown; R=11010, G=B=3050}.



"Something light green on the right". Fig. 20.d. Here we provide partial information on the
output: S = C3={x=24030; y unknown; R=11050, G=24510, B=16050}



"Something white". Fig. 20.e. S = C4={x,y unknown; R=24510, G=24510, B=24510}



"Something white, in the lower-left region, under the main diagonal (y<240-x)". Fig. 20.f.
Here we provide relational information on the attributes that can be modeled by S = 6 equiprobable components (note that in this case the posterior distribution contains 600 components, but it is still computationally manageable) =

{x=6030, y=18030, R=24510, G=24510, B=24510}+
{x=6030, y=12030, R=24510, G=24510, B=24510}+
{x=6030, y=6030, R=24510, G=24510, B=24510}+
{x=12030, y=12030, R=24510, G=24510, B=24510}+
{x=12030, y=6030, R=24510, G=24510, B=24510}+
{x=18030, y=6030, R=24510, G=24510, B=24510}
In all cases, the posterior density is consistent with the structure of the original image. The time
required to compute the posterior distribution is always lower than one second. Learning time
was of order of hours in a Pentium 100 system. Simpler models (25-component, obtained from
1000 random pixels) produced also acceptable results with much lower learning time. Furthermore, the EM algorithm can be efficiently parallelized.

On the other hand, when there is a large number of irrelevant attributes, the joint model strategy wastes resources to capture a proper probability density function along unnecessary dimensions. (This problem does not arise in the specification of a likelihood function, since only the relevant attributes explicitly appear in the model.) Joint modeling is appropriate for domains with a
moderated number of "meaningful" variables without fixed input / output roles.

6

Note that a sharp peak (a component with small dispersion) was obtained in the learning process, which also "transmits"
to the posterior density. This kind of artifacts are inocuous and can be easily removed by post processing.

203

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

(a)

(b)

(c)

(d)

(e)
(f)
Figure 20. Inference results for the image domain in Example 16. (a) source image.
(b) posterior density of attributes x-y given "Something light green". (c) the same for
"Something light green OR dark red". (d) for "Something light green on the right".
(e) for "Something white". (f) for "Something white, in the lower-left region, under the
main diagonal of the image (Y<240-X)"

5.2 Advantages of Factorization
The proposed methodology is supported by the general density approximation property of mixture
models. We use components with independent variables in order to make computations feasible in

204

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

the inference and learning stage. Factorized components can be imposed to a mixture model without loss of generality. Any statistical dependence between variables can be still captured, at the cost
of a possibly larger number of components in the mixture to achieve the required accuracy in the
approximation.
The simplicity of the building block structure is entirely compensated by an important saving in computation time. High-dimensional integrals are analytically computed from univariate
integrals and matrix inversions are avoided in the learning stage. Additionally, high-dimensional
domains can be easily modeled using a small number of parameters in each mixture component.
From the viewpoint of Computational Learning Theory (Vapnik 1995), models with a small number of adjustable parameters (actually, with low expressive power) have favorable consequences
for generalization.
Mixtures of factorized components are also used in Latent Class Analysis (DeSoete 1993), a
well-known unsupervised classification technique. It is assumed that the statistical dependences
between attributes can be fully explained by a hidden variable specifying the latent class of each
example. This method is similar to the Gaussian decomposition clustering algorithm mentioned in
Section 1, constrained to component-conditional attribute independence. However, our goal is not
unsupervised classification but obtaining an accurate and mathematically convenient expression for
the joint density of the variables, required to derive the desired estimators. The meaning of the
components is irrelevant, as long as the whole mixture is a good approximation to the joint density.
More expressive architectures, which combine mixture models with local dimensionality reduction, have been also considered: Mixtures of Linear Experts (Jordan & Jacobs 1994), Mixtures
of Principal Component Analyzers (Sung & Poggio 1998) or Mixtures of Factor Analyzers (Ghahramani & Hinton 1996, Hinton, Dayan, & Revow 1997). Unfortunately, the general kind of inference and learning from uncertain data considered in this work cannot be directly incorporated into
these architectures with the computational advantages demonstrated by the MFGN model.
The restriction to factorized components may produce undesirable artifacts in the approximations of certain domains learned from small training samples. Nevertheless, this problem always
occurs to any approximator when the structure of the building block does not match the shape of
the target function. In this case, many terms (or components, units, etc.) are required for a good
approximation and the associated parameters can be correctly adjusted only from a large training
sample. However, note that the complexity of the model should not be measured uniquely in terms
of the number of mixture components. The number of adjustable parameters is probably a better
measure of complexity. For instance, full covariance models show a quadratic growth of the number of free parameters with respect to the dimension of the attribute vector. For factorized components the growth is linear, so the amount of training data need not be unreasonably high even if the
number of mixture components is large.
In real applications, the nature of the target function is unknown, so little can be said a priori
about the best building block structure to be used by a universal approximator. We have chosen a
very simple component structure to make inference and learning feasible from uncertain information. Section 5.4 provides experimental evidence that in realistic problems the proposed model is
not inferior to other popular approaches.
5.3 Qualitative Comparison with Alternative Approaches
Instead of the proposed methodology, based on mixture models and the EM algorithm, other alternative nonparametric density approximation methods could also be used (either for the joint density
or for specific conditional densities). For instance, the nearest neighbor rule locally approximates
the target density using a certain number of training samples near to the point of interest. Symbolic

205

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

attributes are directly estimated by a voting scheme and continuous attributes can be also estimated
by averaging the observed values of training instances which are near, in the subspace of observed
attributes, to the point of interest. However, for small sample sizes, the above estimators are not
smooth and show strong sensitivity to random fluctuations in the training set, which penalizes the
estimation cost. For large sample size, the time required to find the nearest neighbors becomes very
long. As an example, consider the regression problem in Example 10, Section 3.5. Fig. 11.b shows
the MFGN solution with 4 components and MSE=0.381. Fig. 21.a shows the regression line obtained by 5-nearest-neighbors average, with a higher MSE=0.522.
Parzen windows and similar kernel approximation methods are used to smooth the results of
the simple nearest neighbors rule (Duda & Hart 1973, Izenman 1991). They are actually mixtures
of simple conventional densities located at the training samples. In principle, the properties of the
MFGN framework could be adapted to that kind of approximation (Ruiz et al. 1998). Learning
becomes trivial, but strong run time computation effort is required since a concise model of the
domain is not extracted from the training set. This kind of rote learning has also negative consequences on generalization according to the Occam Razor Principle (Li & Vitnyi 1993). An adequately cross-validated mixture model with a small number of components in relation to the training sample size reasonably guarantees that probably the true attribute dependencies are correctly
captured.

C2

C2
C1

C1

(a)
(b)
(c)
Figure 21. Alternative solutions in regression and classification (see text for details).

The nature of the solutions obtained by Backpropagation Multilayer Perceptrons (Rumelhart et
al. 1986) in pattern classification is also illustrative. In general, each decision region can be geometrically expressed as the union of intersections of several halfspaces defined by the units in the
first hidden layer. However, backprop networks often require very long learning times, many adjustable parameters and, what is worse, apparently simple distributions of patterns are hard to learn.
For instance, the solution to the circle-ring classification problem in Fig. 21.b, obtained by a network with 6 hidden units requires hundreds of standard backprop epochs. The decision regions are
not very satisfactory, even though the network has extra flexibility for this task (3 hidden units
suffice to separate the training examples). Better solutions exist using all the resources in the network architecture, but backprop learning does not find them. In contrast, the solution obtained by
the MFGN approach using 7 components (Fig. 21.c) requires a learning time orders of magnitude
shorter than backprop optimization. All the components in the mixture contribute to synthesize
reliable decision regions and acceptable solutions can be also obtained with a smaller number of
components.

206

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

The proposed approach is closely related to a well-known family of approximation techniques
which, essentially, distribute (using some kind of clustering or self-organizing algorithm) detectors over the relevant regions of the input space and then combine their responses for computing
the desired outputs. This is the case of Radial Basis Functions (RBF) (Hertz et al.), the classification and regression trees proposed in (Breiman et al. 1984) and the topological maps used in (Cherkassky & Najafi 1992) to locate the knots required for piecewise linear regression.
A relevant methodology is proposed in (Jordan & Jacobs 1994, Peng et al. 1995), where the
EM algorithm is used to learn hierarchical mixtures of experts in the form of linear rules in such a
way that the desired posterior densities can be explicitly obtained. The properties of the EM algorithm are also satisfactorily used in (Ghahramani & Jordan 1994) to obtain unbiased approximations from missing data in a mixture-based framework similar to ours. Our framework extends this
successful approach by exploiting the conjugate properties of the chosen universal approximation
model: uncertain information of arbitrary complexity can be efficiently processed in the inference
and learning stages.
The MFGN framework is appropriate for a moderated number of variables showing relatively
complex dependencies. In contrast, Bayesian Networks satisfactorily addresses the case of a large
number of variables with clear conditional independence relations. There are situations in which a
certain subset of the variables in a Bayesian Network shows no explicit causal structure. This subdomain could be empirically modeled by a mixture model in order to be considered later as a composite node embedded in the whole network. If the subdomain can be conditionally isolated from
the rest of variables through a set of communication nodes, the MFGN framework can be used to
perform the required inferences.
Finally, mixture models are typically used for unsupervised classification: the examples are
labeled with the index of the component with highest posterior probability. In fact, the MFGN
framework explicitly finds clusters in the training set. Furthermore, continuous and symbolic attributes are allowed in the joint density, so the examples are clustered using an implicit probabilistic metric which automatically weighs all the (heterogeneous) attributes, even with missing and
uncertain values. However, this method is effective only when the groups of interest have the same
structure as the component densities. In order to simplify inference the mixture components have
been selected with constraints (Gaussian, independent variables) which are not necessarily verified
by the natural groups found in real applications.
A tentative possibility (inspired in a common heuristic clustering technique) consists of joining overlapping components (e.g., according to the Battachariya distance, a well-known bound on
the Bayes error used in Statistical Pattern Recognition (Fukunaga 1990)). Unfortunately, our experiments indicate that the overlapping threshold is a free parameter that strongly determines the
quality of the results. A universal threshold, independent of the application, does not seem to exist.
In principle, clusters of arbitrary geometry may be discovered, but this cannot be easily automated.
Therefore, other nonparametric cluster analysis methods (e.g. density valley seeking) are suggested
for labeling complex groups.
5.4 Experimental Evaluation
The MFGN method has been evaluated on standard benchmarks from the Machine Learning database repository at the University of California, Irvine (Merz and Murphy 1996). It contains inductive learning problems which are representative of real world situations. We have experimented
with the following databases: Ionosphere, Pima Indians, Monk's Problems, and Horse Colic, which
illustrate different properties of the proposed methodology. In most cases MFGN has been compared to alternative learning methods with respect to the inference task considered of interest in

207

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

each problem (typically, prediction of a specific attribute given the rest of them). We usually give
the error rate over both the training and the test set to indicate the amount of overfitting obtained by
the learning algorithms.

(a) Ionosphere
(b) Pima Indians
Figure 22. Most discriminant 2D projections of two representative databases.

5.4.1 IONOSPHERE DATABASE
Two classes of radar returns from the ionosphere must be discriminated from vectors of 32 continuous attributes7. There are 351 examples, randomly partitioned into two disjoint subsets of approximately equal size for training and testing. The prevalence of the minoritary class (random
prediction rate) is 36%. Figure 22.a and Table 7 show that this is a typical statistical pattern recognition problem, easily solvable by standard methods. The results suggest that the Bayes (optimum)
error probability is around 5%.
error rate
PE
(training set)
METHOD
(test set)
Linear MSE (pseudoinverse)
1-1 Nearest Neighbor
2-3 Nearest Neighbor
Parzen Model
Backprop Multilayer Perceptron 2 hidden units
Support Vector Machine, RBF kernel, width 1, (105 s.v.)
Support Vector Machine, RBF kernel, width 3, (35 s.v.)
Support Vector Machine, polinomial kernel, order 2, (41 s.v.)
Support Vector Machine, polinomial kernel, order 3, (45 s.v.)
Support Vector Machine, polinomial kernel, order 4, (42 s.v.)
Full covariance gaussian mixture, 1 component/class
Full covariance gaussian mixture, 2 component/class
Full covariance gaussian mixture, 3 component/class
MFGN 4 components (average)
MFGN 8 components (average)
MFGN 15 components (average)
MFGN, best result by cross-validation (8 components)
Table 7. Ionosphere Database Results

7

.11

.05
.00

.03
.01
.005
.22.15
.11.06
.10.05
.07

Originally the database contains 34 attributes. Two of them, meaningless or ill behaved, were eliminated.

208

.14
.13
.18
.08
.08
.05
.09
.13
.17
.20
.11
.19
.26
.21.08
.13.06
.13.06
.06

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

In this problem, the plain MFGN method, without special heuristics in the learning stage, is
comparable in average to the alternative methods. The best solution on the training set (crossvalidation) is entirely satisfactory.
For the Ionosphere database we also present an exhaustive study of performance given varying
proportions of missing values in the training and testing examples. A value of x % means that in all
training or test examples the value of each attribute is deleted with probability x. The basic experiment consists of learning a MFGN model with the prescribed number of components (4, 8 and 15)
and computing the error rate on the training and test sets. Table 8 shows the mean value  2 standard deviations of the error rates obtained in 10 repetitions of the basic experiment in each configuration. Column M contains the error rate of each configuration over its own training set. The training/test partition is kept fixed to analyze the variability of the solutions due to random initialization
of the EM.
LEARNING
M

0%

INFERENCE
10%
25%

50%

4 COMP. - 0%
8 COMP. - 0%
15 COMP. - 0%

22  15
11  6
10  5

21  8
13  6
13  6

21  8
12  6
13  6

22  8
13  5
13  5

22  9
12  6
13  4

4 COMP. - 10%
8 COMP. - 10%
15 COMP. - 10%

21  14
11  3
10  3

23  11
13  5
12  6

23  11
12  5
12  7

23  11
13  5
12  6

23  11
13  4
12  3

4 COMP. - 25%
8 COMP. - 25%
15 COMP. - 25%

18  7
12  7
95

19  5
14  10
12  9

19  5
14  9
13  11

18  6
15  8
13  9

18  6
14  7
13  7

4 COMP. - 50%
27  18 26  15 27  15 27  14 26  13
8 COMP. - 50%
16  12 21  15 21  15 21  13 20  11
15 COMP. - 50% 13  6
26  17 25  15 25  14 23  13
Table 8. Evaluation of MFGN on Ionosphere Database given
different proportions of missing data in the training and testing subsets.

As expected, the MFGN model is robust with respect to large proportions of missing values in
the test patterns, and to moderated proportions of missing data in the training set. We have compared the above behavior with a standard algorithm for Decision Tree construction inspired in
(Quinlan 1993), which is also able to support missing values8. Table 9 shows the error rates of the
decision trees for the same experimental setting as in Table 8. This kind of Decision Tree obtains
error rates that are better than the averages obtained by MFGN. However, MFGN's best solutions
(selected by cross-validation) are better than the ones obtained by Decision Tree. Furthermore,
Decision Tree performance degrades faster than MFGN, especially with respect to the proportion
of missing values in the inference stage.

8

Essentially, missing values are handled as follows. In the learning stage, when an attribute is selected, examples with
missing values are sent to all the partitions with appropriate weights. In the inference stage, if a node asks for a missing
value, it follows all the branches with appropriate weights and finally the outputs are combined.

209

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

LEARNING
M

0%

INFERENCE
10 %
25 %

50 %

1%
0%
9%
10 %
11 %
12 %
5%
10 %
14 %
15 %
19 %
18 %
6%
25 %
15 %
17 %
17 %
18 %
8%
50 %
17 %
18 %
18 %
19 %
Table 9. Evaluation of basic Decision Tree on Ionosphere Database given different proportions of missing data in the training and testing subsets.

5.4.2 PIMA INDIANS DATABASE
In this problem we must discriminate between two possible results of a diabetes test given to Pima
Indians. There are 8 continuous attributes, and 768 examples, randomly partitioned into two disjoint subsets of equal size for training and testing. The prevalence of the minority class is 35%. The
attribute vector has been normalized. Table 10 presents comparative results.
error rate
PE
(training set)
METHOD
(test set)
Linear MSE (pseudoinverse)
Oblique Decision Tree 8 decision nodes
1-1 Nearest Neighbor
2-3 Nearest Neighbor
Full covariance gaussian mixture, 1 component/class
Full covariance gaussian mixture, 2 component/class
Full covariance gaussian mixture, 3 component/class
Full covariance gaussian mixture, 4 component/class
Backprop Multilayer Perceptron 2 hidden units
Backprop Multilayer Perceptron 4 hidden units
Backprop Multilayer Perceptron 8 hidden units
Support Vector Machine, RBF kernel, width 1 (297 s.v.)
Support Vector Machine, RBF kernel, width 3 (176 s.v.)
Support Vector Machine, polynomial kernel, order 4 (138 s.v.)
Support Vector Machine, polynomial kernel, order 5 (131 s.v.)
MFGN 4 components
MFGN 6 components
MFGN 8 components
Table 10. Pima Indians Database Results

.22
.18

.24
.19
.17
.17
.17
.14
.05

.28
.25
.29

.23
.24
.30
.25
.26
.29
.30
.31
.25
.24
.29
.30
.35
.36
.34
.35
.32
.35

Despite of low dimensionality and large number of examples, this classification problem is
hard (see Figure 22.b). Even sophisticated learners such as backpropagation networks, decision
trees or support vector machines, which are able to store a reasonable proportion of the training set,
do not achieve significant generalization. MFGN shows a similar behavior, although it is slightly
less prone to overfitting (the error rate on the training set is not misleading).
5.4.3 HORSE COLIC DATABASE
This database contains a classification task from a heterogeneous attribute vector including symbolic, discrete and continuous variables, with 30% missing values. It illustrates the problem of

210

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

feature selection in the context of joint modeling, mentioned in Section 5.1. Table 11 shows the
error rates obtained by MFGN using different attribute subsets9. To take advantage of its general
inference properties, the MFGN model must be applied to the attribute subset of interest. If the
inference task is fixed and the number of attributes is very large, alternative methods should be
used.
METHOD

PE

PE

(distribution, 10 initializations)

(best)

6 Selected attributes
MFGN 2 components
MFGN 3 components
MFGN 4 components
MFGN 5 components
MFGN 6 components
MFGN 7 components
MFGN 10 components
MFGN 12 components
MFGN 15 components

.32.00
.20.05
.19.02
.20.02
.19.03
.20.04
.22.04
.19.03
.19.04

.32
.18
.18
.18
.16
.18
.18
.16
.15

.22.01
.21.02
.21.03
.23.02
.21.02
.21.02

.21
.19
.18
.18
.18
.18

.28.02
.29.03
.34.08
.34.06

.25
.25
.25
.28

8 Selected attributes
MFGN 4 components
MFGN 6 components
MFGN 8 components
MFGN 10 components
MFGN 12 components
MFGN 15 components

23 Selected attributes
MFGN 6 components
MFGN 8 components
MFGN 10 components
MFGN 15 components
Table 11. Horse Colic Database Results (random rate = .5)

5.4.4 MONK'S PROBLEMS
The Monk's problems are three concept learning tasks from 6 symbolic attributes, widely used as
benchmarks for inductive learning algorithms (Thrun et al. 1991). As seen in Table 12, MFGN fails
on MONK1 (where acceptable generalization is not obtained) and MONK2 (where the training
examples cannot even be stored). In contrast, MFGN correctly solves MONK3. This behavior is
related to the fact that the MONK's problems are based on deterministic or abstract concepts which
may lack the kind of geometric regularities in the attribute space required by probabilistic models10.
9

Features were individually selected using a simple discrimination index related to the Kolmogorov-Smirnov statistic
(Ruiz 1995).
10
A typical example is the parity problem: acceptable off-training-set generalization cannot be achieved if the inductive
bias of the learning machine is biased towards "smooth" solutions.

211

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Fig. 23 shows the most discriminant 2D projections of the datasets and illustrates the fact that
MONK2 cannot be easily captured by statistic techniques. In this benchmark, MFGN performance
is similar to that of other popular probabilistic methods (Thrun et al. 1991).

(a) MONK1
(b) MONK2
Figure 23. Most Discriminant 2D Projections of the Monk's Datasets.

(c) MONK3

error rate
(training set)

METHOD

PE
(test set)

MONK1 (random rate = .5)
Linear MSE (pseudoinverse)
1-1 Nearest Neighbor
Support Vector Machine, RBF kernel, width 1 (78 s.v.)
Cascade Correlation
MFGN 4 components
MFGN 8 components

.29

.06
.00

.34
.17
.08
0
.40
.33

MONK2 (random rate  .4)
Linear MSE (pseudoinverse)
1-1 Nearest Neighbor
Support Vector Machine, RBF kernel, width 1 (117 s.v.)
Cascade Correlation
MFGN 4 components
MFGN 8 components
MFGN 15 components

.40

.31
.26
.14

.37
.19
.20
0
.38
.44
.50

MONK3 (random rate  .5)
Linear MSE (pseudoinverse)
1-1 Nearest Neighbor
Support Vector Machine, RBF kernel, width 1 (69 s.v.)
Cascade Correlation
MFGN 2 components
MFGN 4 components
MFGN 8 components
Table 12. Monk's Problems Results

212

.19

.07
.04
.03

.19
.18
.08
.03
.03
.03
.08

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

5.4.5 COMMENTS
The above experiments demonstrate that the MFGN model is able to obtain acceptable results on
many real world applications. In particular, the error rates obtained in standard classification tasks
are comparable to those obtained by other popular learners. Additionally, MFGN is able to perform
inferences over any other attribute given uncertain or partial information, which is not possible for
most of the alternative methods. This property makes MFGN a very attractive alternative for many
inference problems such as the one illustrated in Example 16. The experiments have also contributed to characterize the kind of problems for which the MFGN model is best suited. Essentially, the
relationship among attributes must be of a true probabilistic nature, and the attribute vector must be
of a moderated size containing "relevant" variables. A previous feature selection / accommodation
stage is recommended in certain applications.

6. Conclusions
We have developed an efficient methodology for probabilistic inference and learning from uncertain information. Under the proposed MFGN framework, the joint probability density function of
the attributes and the likelihood function of the available information are approximated by Mixtures of Factorized Generalized Normals. This mathematical structure allows efficient computation,
without numerical integration, of posterior densities and expectations of the desired variables given
events of arbitrary geometry. An extended version of the EM learning algorithm has been developed to estimate the parameters of the required mixture models from uncertain training examples.
Different paradigms as pattern recognition, regression or pattern completion are subsumed under a
common framework.
A comprehensive collection of examples illustrates the methodology, which has been critically
compared with alternative techniques. The Extended EM algorithm is able to learn satisfactory
domain models from a reasonable number of examples with uncertain values, taking into account
the explicit likelihood functions of the available information. Results are satisfactory whenever the
sample size is large in relation to the amount of (known) degradation of the training set. The experiments also characterized the kind of situations that the model manages better: Domains described by a moderate number of heterogeneous attributes with complex probabilistic dependences,
problems in which the output variables are not necessarily known in the learning stage (i.e. pattern
completion), and, finally, problems in which an explicit management of uncertainty is needed, either in the learning or in the inference stage (or even in both). The MFGN framework has obtained
a very favorable trade-off between useful features and model complexity in the solutions to different applications and benchmarks.
Future developments of our work include improving the learning stage with some heuristic
steps that are combined with the standard E and M steps to control the adequacy of the acquired
models. Additional studies are required on validation tests, generalization, scalability, robustness
and data preprocessing. The essential idea of working with explicit likelihood functions will be
incorporated into the Parzen approximation scheme and we are also interested in more expressive
model structures such as mixtures of factor analyzers, principal component analyzers or linear experts. Finally, the methodology can be developed in a pure Bayesian framework or subsumed under
the Dempster-Shafer Evidence Theory.

213

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Acknowledgments
The authors would like to thank the anonymous reviewers for their careful reading and helpful suggestions. This work has been supported by the Spanish CICYT grants TIC95-1019, TIC97-1343C02-02 and TIC97-0897-C04-03.

References
Berger, J., (1985). Statistical Decision Theory and Bayesian Analysis. Springer-Verlag.
Bernardo, J.M., Smith, A.F.M. (1994). Bayesian Theory. Wiley.
Bouckaert, R.R. (1994). Properties of Bayesian Belief Network Learning Algorithms. Proceedings of Uncertainty in AI, pp. 102-109.
Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J. (1984). Classification and Regression
Trees. Wadsworth International Group, Belmont, CA.
Chang, K. & Fung, R. (1995). Symbolic Probabilistic Inference with Both Discrete and Continuous Variables. IEEE Tran. on Systems, Man, and Cybernetics, Vol. 25, No. 6, june, pp. 910916.
Cherkassky, V. and Lari-Najafi, H. (1992). Nonparametric Regression Analyisis Using SelfOrganizing Topological Maps in H. Wechsler (ed.), Neural Networks for Perception. Vol.2,
Computation, Learning and Architectures, San Diego: Academic Press.
Cohn, D.A., Ghahramani, Z. & Jordan, M.I. (1996). Active Learning with Statistical Models.
Journal of Artificial Intelligence Research 4, pp. 129-145.
Dalal, S.R. & Hall, W.J. (1983). Approximating Priors by Mixtures of Natural Conjugate Priors.
J. R. Statist. Soc. B, Vol. 45, No. 2, pp. 278-286.
De Soete, G. (1993). Using Latent Class Analysis in Categorization Research in I. V. Mechelen,
J. Hampton, R.S. Michalski, P. Theuns (eds.), Categories and Concepts: Theoretical Views
and Inductive Data Analysis, San Diego: Academic Press.
Dempster, A.P., Laird, N.M., Rubin, D.B., (1977). Maximum Likelihood Estimation from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, Series B, Vol. 39:
pp. 1-38.
Duda, R.O. and Hart, P.E. (1973). Pattern Classification and Scene Analysis. John Wiley & Sons.
Fan, C.M., Namazi, N.M. and Penafiel, P.B. (1996). A New Image Motion Estimation Algorithm
Based on the EM Technique. IEEE Transactions on Pattern Analisys and Machine Intelligence, Vol.18, No.3, March, pp. 348-352.
Fukunaga, K. (1990). Introduction to Statistical Pattern Recognition. Academic Press.
Ghahramani, Z. and Jordan, M.I. (1994) Supervised learning from Incomplete data via an EM
approach In Cowan, J.D., Tesauro, G., and Alspector, J. (eds.). Advances in Neural Information Processing Systems 6. Morgan Kauffman
Ghahramani, Z. and Hinton, G.E. (1996) The EM algorithm for mixtures of factor analyzers.
Tech. Rep. Univ. Toronto. CRG-TR-96-1.

214

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

Heckerman, D. & Wellman, M.P. (1995). Bayesian Networks. Communications of the ACM, Vol.
28, No.3, pp. 27-30, March.
Hertz, J., Krogh, A., Palmer, R.G., (1991). Introduction to the Theory of Neural Computation.
Addison Wesley.
Hinton, G.E., Dayan, P. and Revow, M. (1997). Modeling the manifold of images of handwritten
digits. IEEE T. on Neural Networks 8, pp. 65-74.
Hornik, K., Stinchcombe, M., White, H., (1989). Multilayer FeedForward Networks are Universal
Approximators. Neural Networks, No.2.
Hutchinson, A. (1994). Algorithmic Learning. New York: Oxford Univ. Press.
Izenman, A.J. (1991). Recent Developments in Nonparametric Density Estimation. J. Amer. Statist. Assoc. Vol. 86, No. 413, pp. 205-224.
Jordan, M.I., Jacobs, R.A., (1994). Hierarchical Mixtures of Experts and the EM Algorithm.
Neural Computation, 6, pp. 181-214.
Kohonen, T., (1989). Self-Organization and Associative Memory. Springer-Verlag.
Lauritzen, S.L. & Spiegelhalter, D. J. (1988). Local Computations with Probabilities on Graphical
Structures and their Application to Expert Systems. J. R. Statist. Soc. B. 50, No. 2, pp. 157224.
Li, M. and Vitnyi, P. (1993). An Introduction to Kolmogorov Complexity and Its Applications.
New York: Springer-Verlag.
McLachlan, G.J., Basford, K.E., (1988). Mixture Models. New York: Marcel Dekker.
McLachlan, G.J. and Krishnan, T. (1997). The EM Algorithm and Extensions. John Wiley and
Sons.
Michalski, R.S., Carbonell, J. and Mitchell, T.M., eds. (1983). Machine Learning: An Artificial
Intelligence approach. Palo Alto, CA: Tioga Press. Also reprinted by Morgan Kaufmann
(Los Altos, CA).
Michalski, R.S., Carbonell, J. and Mitchell, T.M., eds. (1986). Machine Learning: An Artificial
Intelligence approach, Vol. II. Los Altos, CA: Morgan Kaufmann.
Mohgaddam, B. and Pentland, A. (1997). Probabilistic Visual Learning for Object Representation. IEEE T PAMI, Vol. 19, No.7, 710. pp. 696-.
Merz, C.J. and Murphy, P.M. (1996). UCI Repository of machine learning databases.
[http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, CA: University of California,
Department of Information and Computer Science.
Palm, H.C. (1994). New method for generating statistical classifiers assuming linear mixtures of
Gaussian densities, Proceedings 12th IAPR International Conference on Pattern Recognition (Jerusalem, October 9-13, 1994), vol.2, IEEE, Piscataway, NJ, USA,
Papoulis, A., (1991). Probability, Random Variables and Stochastic Processes. MCGraw-Hill.
Pearl, J., (1988). Probabilistic reasoning in intelligent systems: Networks of plausible inference.
Morgan Kaufmann.

215

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Peng, F., Jacobs, R.A., Tanner, M.A. (1995). Bayesian Inference in Mixtures-of-Experts and Hierchical Mixtures-of-Experts Models With an Application to Speech Recognition. Accepted
in the Journal of the American Statistical Association.
Priebe, C.E., Marchette, D.J., (1991). Adaptive mixtures: recursive nonparametric pattern recognition. Pattern Recognition, V24 N12, pp. 1197-1209.
Pudil, P., Novovicova, J., Choakjarernwanit, N., Kittler, J. (1995). Feature Selection Based on the
Approximation of Class Densities by Finite Mixtures of Special Type. Pattern Recognition
Vol.28 No.9 pp. 1389-1398.
Quinlan, J.R., (1993). C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann.
Redner, R.A., Walker, H.F., (1984). Mixture densities, maximum likelihood estimation and the
EM algorithm. SIAM Review, Vol. 26, pp. 195-239.
Rojas, R. (1996). A Short Proof of the Posterior Probability Property of Classifier Neural Networks. Neural Computation Vol.8 Issue 1, January.
Rumelhart, D.E., Hinton, G. E. and Williams, R. (1986). Learning Internal Representations by
Error Propagation in Rumelhart, McClelland & the PDP Group (1986), pp. 319-362.
Rumelhart, D.E., McClelland, J.L. & the PDP Research Group. (1986). Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. 1, Foundations. Cambrigde
MA, Bradford Books/MIT Press.
Ruiz, A. (1995). A nonparametric bound for the Bayes Error. Pattern Recognition, Vol. 28, No.
6, pp. 921-930.
Ruiz, A., Lpez-de-Teruel, P.E. and Garrido, M.C. (1998). Kernel Density Estimation from Indirect Observations. In preparation.
Sung, K.-K. and Poggio, T. (1998),. "Example Based Learning for View-Based Human Face Detection". IEEE Trans. Pattern Analyisis and Machine Intelligence. Vol.20, N.1. January, pp.
39-51.
Tanner, M.A. (1996). Tools for statistical inference. (3rd ed.). Springer.
Thrun, S. et al (1991). "The MONK's Problems. A performance Comparison of Different Learning
Algorithms". Technical Report CMU-CS-91-197.
Titterington, D.M., A.F.M. Smith and U.E. Makov (1985). Statistical Analysis of Finite Mixture
Distributions, Wiley, New York.
Traven, H.G.C., (1991). A Neural Network Approach to Statistical Pattern Classification by
"Semiparametric" Estimation of Prob. Den. Func.. IEEE T Neural Networks, V2 N3.
Valiant, L.G. (1993). A View of Computational Learning Theory in Meyrowitz and Chipman,
eds. (1993). Foundations of Knowledge Acquisition: Machine Learning. Kluwer Acad. Pub.
Valiveti, R.S., Oommen, B.J., (1992). On using the chi-squared metric for determining stochastic
dependence. Pattern Recognition, V25 N11 pp. 1389-1400.
Vapnik, V.N. (1982). Learning Dependencies based on Empirical Data. Springer, New York.
Vapnik, V.N. (1995). The Nature of Statistical Learning Theory. Springer, New York.

216

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

Wan, E.A., (1990). Neural Networks Classification: A Bayesian Interpretation. IEEE Trans. on
Neural Networks, V1 N4.
Weiss, Y. and Adelson E.H. (1995). Perceptually organized EM: A framework for motion segmentation that combines information about form and motion. TR MIT MLPCS TR 315.
Wilson, D.R. and Martinez, T.R. (1997). Improved Heterogeneous Distance Functions. JAIR, V6,
pp. 1-34.
Wolpert, D.H., ed. (1994a). The Mathematics of Generalization. Proc. SFI/CLNS workshop on
Formal Approaches to Supervised Learning.
Xu, L. & Jordan, M.I. (1996). On the Convergence Properties of the EM Algorithm for Gaussian
Mixtures. Neural Computation Vol.8 Issue 1, January.
You, Y.L. and Kaveh, M. (1996). A regularization approach to joint blur identification and image
restoration. IEEE T on Image Processing, vol 5.no.3, pp. 416-428.

217

fi
	ff
fi 
			 ! #"$ % 
'&)(+*, ((-/.10/23540/6!,

789:;  <)-/=
(-!>@?9	%&<A, 2/=
(-

BDCAEGFIH/JLKNMOQPRCTSVUXWYM/CAWYM
OZM/[;J]\_^`J]S@acbIHOedfCTgAMh;i

jYkmlnlno)pqo'rsntur#vwr

x1yz1{}|uz$~zAyz$x1]|{1y$zuy$}T~


  T
 8@   Q$  ]/8/    
 
'  

n}@

A]YY)

]I

)nZQ
/@m58 nTn#@Q5 ;nw58nffn/5ZQ/8/n
58@5/85nn#555/585 qn
@LTn58n}I/
A
QTn5;58NA88/T!
@n+/@T!//NI85n/c58 
8@@!qcLnn5qQnn8n 
n5qncnn
5NqN/n
588#n 5#+ffn!5}5Ln5n8@nm@15L@}n
;88 /n
nn+nn]@nffn/@5n
5@+8 5!@58 

@85c55A@;5/n55@/ffw5!/nAnn
nLnn]n88;@5;L@q5!/@n5 !f
//n5nL/nnn$L]@n@8Z]} nn@5n
Z@Q8]n
5#8ncn)
n8u@5mQnffn@}85n5
ff8@}8umQn@5+Q/8
555nn}
n/5n5/)n;58 5/!8Ln5!/@;588
ZI} ;+Q 
AI1	
fffifffififiA1I1ff

!"#fi  	$ %ff&
2

3
1


4



6
5


&




9






:






<
;


=












;
4




fi
@

6







?
	
>Q$ 8 "	@4fi
'()*,+.'()/+0'()*678
Afi	@591
 B:@"@B"CED,;fififfF"GHI1?	
fffi&I">G5?J4fiLK4
;ff?	
ffN;/
; 
fi" I 	
fiM9
N 1I1 ffO 	
fffi P@ EfiQ HF<G
 
5RFfifffi# 8 5S $1SF0A PP/ 
TUfi
  EfiMWVX5@ OED,@  	Y5fffiZfi&
fi 
?+5
P@Efi#[=F< 
?9 B:\ff]&, 5@ E59fi
 ^ P4F"_ EfiU1 c
fi 1U1 ffffff
E4fiM1` ED?;"&] a5G>, 
 bfi  	cED;4fifiCfiQ 54ff EN
  8 >fi$ A
d e&
& 
  8 4>ff >cfi2@5U;4"fifi9FffF0
 1Kgff 
15 
ffffQF<:4	A
fi "M3h5fifffi&
'((\fB7 
 =5F.fi@
 E
 W 644=  i ]&,?fi!,jkff@ 6#/fi @  ff!5?E @;4"fi
8 ff$ 1R4 >l: >\,M^Vm"15 
!A 6iffE ]&,  4 [fi 	O5R5_;4"fiR1 BfiA4 >
:,>\[ @F<]& 8 5ff45=iEfi5;: 
!fi# ff!fi4 ]M
h5
 n o  
; 4[;4 4ffgC  4 C 1I$ ff? 	
fffic$ pI" fii5"q& '()/+
rA 8 >,& '()(+Os EZ
t =uwvS4x& '((y+ %
 z& '((y+ rR4 8 >\,& '((\f+ r3u{T
d 
ff$
! &
; 
fifi4"Fffffffgc5Y 8 @ !"_4!fffiUG o ,}.ffEM
~ lfi 
!4EN
 bfi!i5
'((/
7 fifi_P/ 
|5O
;4 4"gO  4 ?fffiW5fi;<J."e
 <P!"fiMSVX5@ #4!"3fi2fib5;4fffifiW!fi!ffff91 
1U4 >[
  15_fi,fi5Y?@!fi!ff"[1 &. P@ Efi# E  	"9;4"fifi#fi415 
!ff
F<OF.fi
 o5lJ4fi
 GF<!fiG"fffiQI?fi;<J.&<fiffO;i"fifiQ?=fiffWfi!F.fiA
 
?@]MH;<J."enfiGFfi4fffi Gfi 
":, 	 }.ffEfilF< 8 @ P!fffiO5fiOF<@  6:fiff	

 5=P4? 8 > Y;5,KFfi 54ff
 Ek5 4"fi$ A
d g& '((\fB7 M5N 	@ 4x&b15  8 :2&
;4 4""fiYC 
?G 
t<@ 6Qfi 
!iEfi&z o5 ?"Qfffib!fiJa = 6:fiff	

 1,K
I1
 ff/fi  	 8 "5 aFfi4E
 1 f^;4 4ff"fiA
fi 44 	

fi aP!fffiM35N 5fffi
Q&



, ((-Q %%.!<5c
<3/
AZ
!fi:]
bQ9!	%&%g		n&/%u%/ <

fiU".


ffO_."ffb?E.YUR4YQEE#ffERff[_EYz4ff4""
 4a0B<
ffG<4ffCiff"gXPi" A,

 4B"
@ 
z 
 al2@O43aff

 "_ffU "ff
ff.<<4ffkff4"E4
S "i"_[i"4""P"U"
ffc UffB] 
 Y[GE.""
#
?,3ff""
ffE
""
RBffff?B_E<[Zix 
 ff4ff4""
"4E4""
o Z" 4ff 
 U"Effk04A "U"=4B"
 ffRAaE
EB4o
A,C \B 3kU
ff2= 

<
PAffE<4ff4"4"ff^lPffR"
ff,AR39<
3A"EffE
4ff3E
?4ff
 UffB] 
   #4#<
ffW4^ "W
QU"
 "9lcffff@I"k4ff"ORH
E
?ffE,"eHU


ffff
ff
 9,ff] , ,ffffffz , _ffz 
 _.<
_Z
6ff
"
@ff
G<
ff6R
3P?B.04E3k4ffffffi""^<gA="
44Ei"=B
<
",
ff<ff4ffff"ek<eRa"Uff"
@"cffB"ciffE
ffUbPff4"YU49
ffk4Eff  94Uff^"UG 
PbE?"Y".ff?2,ff94BRU"
ffRff"
z  Uff,
EffEQz<
",
ffff?_4"
R0eR=eRff"
kffRffR3
4ffff""k<<B"ff##P"ff[
"
Off"
="
ff?."kiffffk"YklQ_ff<"^<
"
ff]ffO4ff"
UE,_
4n
<?"z[ffP
O[ff4"
H."i"
 E
n,ffo3ffIffB.ff
CAiE"[SffE"
ffEk"BZ9ff
<
",
ff<ff.ff4"
@,E4PU4ff4""=
ff[<ffz  lffff"e
_"i"?<G
ff"
[]  [6k@4E"

?"C<
",
ffbff?@ffff
4EG=BH@H@cE
4Eff"H x@




ff[
ffaff"aO<k044nff
<
",
ffff?"6#ff=ff"?B"
k4B32"A<
"ff

"["[Y,"
ffA?Bok"G _"?e
 ,ff?
 
 kYEk
9
?
0EU^
?

ffQ
ff?4Eff"= xU4@Y
."AffB,ff[4ff4"gaffP4"
CffH
?

ffl
ff@?  ff
<<
",ffG
ffffff3ff=Y"E"
z
 l"
offUR<
"ff
I "Q"
ffl",C
o?E
H"W
<
",
0"44i6H_"z 
 ]E9c[<"
?[9ffU_
3EgY"^Y<
"
ff"i44BE
ffffOffib"x  <]ff4"
#
ff?ff@ "#"
_<Q044c=0
ff
[ff?ff?."?B
iffEffYP4lffUffk "b4"Rl94E"Q]ffb]<
"ff
"
ffYff"
 gff"ibffk z  x^|,[" 

 OY4ffc YUffffa|Qff" \B 
 #PEQa0Oa_cE
E9^O<
ffffff"eE
,.Eff[ "3E
.ffE
<gA "G
ff?4ff=l"ff"[E,ff
?<Ha",ffG
"i"
3,?U
"a=E
.ffE?ffl"]n[H "lQ"=^=O<l
E
,.EA[<_<"
O^k4@E
.ffE3c
"U_O<RzE,ff

YE3AO<ZE,ff
ZG<R<"
6 \ W4"i""IOB9^ff"
4[ff9E
.ffE^<gAkPffff^
"]B[EUU_",R"
nE,<
6#BO<GYE,"
?<=2
ff]  ffO
"4kB"

U4"i""lR
ffff
?a4EnE
?ff
6off
QGffff
."_E
ffa0l
"C?lEk"B"affaGi"4""
\

fiA.b6.Bn.GRff.,ff2
o_6aZ0ff,

Z	ff
fi		"E		!""_#%$&4"4ffY'	(#)*fi+R	,
#.-/ff 102/#3fi4ff54" 

 6
7 ff *	8fi&fi4ff9:;	E
 +($&	fiff fi4<E +($&4ff!=ff >9[ #?>A(
 4" 4ff9(	4@fi$&4" 4ff  :A#3fi4ff
4"
 

 (B3C	/,fiED.FGGIHJKCLfifiNMPOU 4@41DKFGGQRS69/
 T4ff 

 fi	VU"fio W	T+[ E EX
*&!ff A!"ff"
 o#%!=Eff  #5#3fi4ffY4" 

 ZB3-[ff DFG\]RS^V	A$4" 4 ff E  48	
E
 S!" G#_!=`4" D*&4ff 
Y .	 5	fiaY
 .	$<!b	5$4ff 4ff ff 6dcef	5
*fi4
fiV	NE
 +($&4!=,ff g9k #b	V " h$& U&4ff+3 #b		V4" 

 /fi4@5	NE +($"4ff!=, >9= #K	 
#/-/ 02N#3fi4ffY4ff 

 DKU< 
ZE +($&4ff# V	E &i4ff4b #	A$< 4ff9E+ fi4. *fi**96
j
k	($i"
 4ff ff ;fi	fih 	fi4b *4 
c '	#lfi&4ff	D?	E +$&4ff!=ff >9m!	fiNU9o 
4ff4nD4fffi 
[
 +fi9Y*fi!	fiU&4fiboN	%$4 $0 4ff " fi4*fi 
_  4ffO@ d*fi!	fiU"4ffD
#P
 8!=fi+($&4ff5U 	OU S4fib pqX>4ff *fi44fi	6 j ff 	Afi	Uff *fi*9Ti !a$&fi	@ fi4, **8		
 5 + 4@fi!*fi6
j
mE
  `ZU`9rfifi4ff9`: 
sfik$i" 4ff ff :t#lfi4f4ff 

 u	fi[@ (U&fi vE +($&fi4@ 
k	
-/ N!=E"
  #fiZ#lfi4	 	9'U9k4ff!= E 
*fi$" fE +($&fi4@  wB3-U@ 	fi?D%FGGGRS6=/
 
 4"
 A$& U&4ff+  #d	@ 4ff 

 Y@ &fi*	fih	 ff A$4 U&4ff+R #d-/ 025#3fi4ffL4" 

 D
fi		&+ 
x	fih	'$0
 49`+[ fi4[ *fi**9w` huE 44fi$&	6 7  Z#lfi&4ff	 4ff yUff 	zfi

 	fi4$&4" 4ff g9Y	4fiff  +(/9E	fi! fi44ff9T	4@ !4fi	8fi	/fiff 8	fi(	/E 	*$<  

$&4"
 4ff  :k D&U& u
*fi4d{ 	fi4K$4ff 4ff ff 5
 /	&!Y	NE +($"4ff!=, >9= #.	
 4"
 r$ U&4+6 7  $&fi	 fi4/$4" 4 ff T*fi!	fiU 4ff >9tfi|U<Zfi*ff  &4ff9xYff 	r!=`*+(
9E	fi!@
 V*4 !" &D"	TE +($"4ff!=, >9ZU< 
	f	fi+(ffiUff 	Z	T$&4" 4ff  :Z#3fi4ffo4ff 

 
U`9yC	/,fifiZU9hCLfifiofi{OU
 44@6
}b~(&>gz>a<>&KPd%a_Kayd%.



"mOE!N+VU&fi4YE!$	QWE+($&	fifffi4KE+$&4ff!=ff>96 7 	fi4

[
 E +$&4ff!=ff >9T4fi	bkE  		Z #_ ffN$4U&4ff+
	fiTfi	4IfiU&4[ k$<
 4ff9E+ fi4b@ +(AU`9kfi{*+[ @ ?K
 i 
Z+fiS 6u;T@ 	4@fi	
#fft$ U&4+(	fiAfi*W 4fffiU&4ff@ t$< 4ff9`+ fi4L +(uU`9|fis&*+  =
 4 

+fiS6C/
 h4fi	TE X>oE  	l #/$& U&4ff+N	hE +($&4ff+(	l #/[@ *fi	c@ x;6c

*fi4D?	4fi	NE
 XeE @ 	Q #%$& U&4ff+;	E +($&4ff+(	 #[ S'fi	[ m	4fi*V;6
 N$0 49`+[ fi4ff *fiS*9h%O /fioE"ffff *fi*S9= #aE +($"4ff!=, >9u4fi	b D" Vfi&W 
/
#P
 [fi44<8v](	fiU 5!"hU`9y4 
G *fi4ff9
 4@ 
(+fi*@ /fi5#P 44"q [6
cZ	 o!

YB3CLfi4fi
 :fiD`T  fi:DMfiU&fi	&
 DFGGQRS6

b 


 K




 
;% 



 K



E


 X





 ffK





 

b#_$&U&4ff+a	fiR8!"4ff,/	[4@fi	   !=!$%	fib*fi4Y4

l*fi4ffL#bfi;$U&4+   fi*	O&fil#K4
[+Afi*KYff	
	SkfiH*fi4ff6;*fi4Ki
h+fi*@Uff	'fiH*fi4(#Yfih$4U&4ff+fi*4ffff,G*&fi	9
4
u+fiSY!=E!$V	fiY	9k+fi19'$_*# *+		Y#P V+(+VU<*	ff $H kU 	sE 	fi
E6$4U&4ff+ AESly		nff/
 fi($ U"4ff+a]ff #d		 5fi@*fi49K
 4 
f+fi* 
Uff	{fi*fi4ffN#
 ;  	fi[ 4ffo[6R/
 Y$ U&4+  U
 4 
T*ff U&4Q@ u$< 4ff9`+ fi4_ +(9 #
	A
 *fi4ff3K
 4 
[+Afi*  4mU 	Vfio$< 4ff9`+ fi4`+VU_S #!=E" f$&6t$& U&4ff+
T  fg` #
 Vfi{E +($&4!=,ff g9k4fi	  ff #Lfi44.$4 U&4ff+  fi	$< 4ff9`+ fi4 +(Z  &  _
		!%Gff J	&fiA D#P
 fi4@4$ U"4ff+88"  		_@ 8fiV#l&!" <	fifiU<oE +($"
 '$<
 4ff9E+ fi4 +? k	ff :[ #Aff 	9 $&fii   BlRYtTff #%fi&H 4ff9off #8|t  6 j 	fi9
Y 



 ffK

 

 a	;4fi	



+fiS 	fibLfi

I1

fi[ffq"

		Tl&!	ff'.*		Wl	aKh[	&f*[_ff>ffaff;<ff
	V	ouo>*


	fifffffifffi
 !	#"%$&(')ff*
+

l&ffYfff(	('l*@-
, 	ff;L(&T	.0[
/ ff2143257698S: 
<;#=?>9@E LBAE	9CEDFHGHI0JLKNMPO/!			/aA	[Q;#=?>9@E RCN@EES&TVUXW Y2K[Z[Z[Z\K]W^_\`a[	bTc.l	
d	
C 	S9ed;9f30f37eg:N1h`.l	{S9e"i@gSNfn9eg:&jWkK]lnmpo39K[Z[Z[Z\K]qrL.l	tsN@gS 0f u%N>0f37evS]:Y	()SV@T
	h		@%&	<	ff	ff8ff98
1 'wM Tyx0s 	0fBz {=?> 1BN
S 	('	hl*V&
		*_	<	ff	ff&&ffal&ff%Sff|TVU}W Y2K[Z[Z[Z~K]W^_\'
` h<;	Al/l	*Y	
3!w
` ff
T &N<i*-  .
1 xA[	Al*TW Y2K[Z[Z[Z2KNhWs
^ <AS- {;
*I
 &@fffif
 yS-I "ffV
 	V	-(
 <[`1 		%**fi(
 	;?_ ff	ff
)o
a 5L*- &ff;hl&ffo
 &- ff@[1[
a 	;	!2` !5
5l*V&Y5?"
 u/	l*T	[Z_S- ugy
 *T	N ffLb	
ff(&'	Al	!f*&ff[
1 	!@	ffLu3ff	FGI0JKNM
O 	?" 
bE
 _@	N/W&(A<*j/
 A<*(\i
 \ y[ S!
` <	[b
1 
Qo
a *T	uffE <[b1 &&_ *bE _@	Tr<yt(m	*ffN;&-E 
<
 	VT	'
 @!(&	ff"ff	T[3ffV*ff[
 E <	Y/	<*f)(
3ff5	y
 	N	 >?evSE3f 9eL
S 	Nl	 
 l*-7K1 	h!*!	s/Sx?E 	i/m3ffY	s
 			T)* 	y
 !	- A3!	Mb1 s&	`!&ys
 &u!&	ff/l*ff		!&ff	ff
a @*A&2 ff	vf
[
 <y*-  `1 	  	_ 	ffa)[
a SA\ ;<A[
a @
/<Y!*!?!/
 Y!	*!	ffu&[
a u(	N3ff	5{<Y&ff?


n(
hBL(# + ? "FGI0JKNMOg
dCEn
f ?S  Uy&;9CNfBev~SS Iz 2;7> >9eUd;9CEfe\SSzI\;9> dCEfn?S  U>9e
K
K
K/
J G 
;9CEfe\SS Iz \;9>
9; CEfBev\S z2;9>
>9e

M
G !o CNf E!S #rZ
nTVUXWh_~Ww'<f*	'	2Ey	k&-`@a[h*_*fiTV1?
<s		zw|\
 i	k&	<	fi
W ffT	uu!&	[
a ff	o
a usE ~[
a 
/Vl&ff/	
F [fi
a ?E 	ff&[1dGhed.!o dCEf ES K >7edKN;7CNfBev\S z 2;7>9r[
:   G
hed.o!dCEf ES fiK >9e
K\;9CEfBev\S z 2;9>r:N`1 	8	&	a	/%
a V_*	ff&@ff	ffK_*-E 	[!v" !
<fi
a s	" *fi
a {3ff	N{
J [
 f?` *ff
 !		<{	*@!{&
	t"
 *53ff[1 u	?` *ffY
 <	N*@!YA&A
 	N	!

 E _@?" &ff	ff'L?` &	ffY@- i?` /
/
 L	A&	<	ff	ff
ff{Awg[
 /
 mff	*Ziy)*T
 &Affo!2` !'
hed.:4Go~mj{X G)r &/
 No\TVUW(Y?K[Z[Z[Z~K]W^_\` q8K?o\TVK]W Y~K[Z[Z[Z\K]W^dK`rQV
r a3ff
*@5ugy
 /
 SEf-q Lb(l&ffL	I0JKNM
O L	Y*K	Yff	L	
l*V&YM
Zul	5{
J % 3ff5	y
 @R" ffffh
J &LM 	t" 
l

 y h~Bd #\yyRy
d VFGI0JLKNMPOwx*>;#=?>9@E /BAE*9CNDd9C>9eDS[=R=9CS
E
@ >Q/ffV.:wx*ABAESS>lff?S SRSE@?AmBA>RM PV.:bhe.0V.:]:nG4.:|>9ed;f =
TR}W Y2K[Z[Z[Z~K]W^_\`mJ>9ed;Tcm4.:w>7ed;ohW Y2K[Z[Z[Z\KNW^r|jGHy;BAE[eL`pm4.:?S;=
=9SC E@ -> {pf/
S u?E 	u)Ff =>7edu
; 9e iDwf =QV.0:G
2

fi4d!~7dydVvdv\%nvL



	fffiff!!"$#&%'(*),+*-%),#/.01243563789fi!5;:<fi!ff8
51==819363>
!8!?938@fi1	fiBACDfi6?	ff3=$E8363!F
G$HJIKffLMINPOQ@RTSUVIWCXYIZLZ[\Z]4^ff_a`cb )edgfihkjml!)n'poqo'0+*"r-stluvwl0(4xys!)zg{}|,~mj
lxZ,o!l0-s)q)/'0+*69r,(and1${ l04
( xo'0+9lsCsaDvdB{(a,d! !!!Y8ffY
~d  M  !!!Y 0 d{  wJ
 !(
d#Cel0(V6)(#&'0(V'poz# oel(4x'0(sB# od{ /ff d  
 r!A(<!+pl6),#C(Zyxu&ol-st)/BA3E83563@3u52ffBE835633ffZ573
A24358n!n13!!
3Zff?
:a!A!A/ff5e243ff58F
 ZI WCXW/K4OQOSUVIWCXYIZLZ[\Z]ff^4_a` -6'Myz {|,~#Clxu,o!l0-st)9)/'0+l0(4xdP#Cyl(
u)(*#,'('8oqz;J!)e'8o=3!03=9!A,5w'8o1d#/)/;+*%Y))'$z#C
u  !!!8 
~ 
 ;d}l0(4xVM!!! e d{4r



G$HJIKffLMINPOQ5SUVIWCXYIZLZ[\Z]4^ff_a` -6'Mdi#@1l0(;u)(*#,'('8o1lwxu&ol-st))/'0+*nz{|,~
J!(md{(4! e!!!Y8Yy  $,dzV  
 $,d9z9{

 1830fi6383fiq0563A!A/ff55=6fi0$%l-),#&'0-1+lM!'0(#/(Z {B3ffj+8l0.0
+lM!'0(#/(Z {B*F
 ZI WCXW/K4OQ@b !)zk{|,~9jlxu,o!l0-st)1)/'0+l0(4xhilo'+"r-stlucJ(yz {B
# o$l0(4x'0(s# oDcdo'+rl0s/s6)!(#&'0(Bd'poBzvl0(<xz {B# orl0(4x;'(sm# oDcydo'0+w'"
u)(*#,'(md}'8oqz;
 AC6@5Y1@3=B8!?36=$83!AC!3=B8w!A/5Afi!@383ufifi1AC?!F
D!A/ff5A<A/?7BBY7$!!!Y8ffY0	<fi5V(<'0+"l0s7<!!"$#/(4'0+*"l0s7
3<+8!+8!-#@*#/)!o*+F  8?w:<638qrM@mAC?$F!58>AC!B!A/0
A/83;883;156	!58  !!!8  YFw@86?!?q38$8Z3fi
!!!8ffAff8t24fi!063:Zmaa,13863F
qa8?3?ff!A,5e!effBE83ff563!7:ff'0+8xu+x
!A/ff5!5F
p36?$fi!!7Jfi!563	:45?A/nmfi!DA!A,5q!5BB3Z8fi0:ff7<:4qA/B
:fi!31@fi$!A,5!10w!74518fi0:ff5,
8$i?37YF
p3908!8fi563ffeD3ff5Z!qBfi6?	ff5E5Afi!5639	:ff5?9:a91939156
!3fi63ff5563F
 ZI WCXW/K4OQSqXMHJILMW,XYK<[\Z]4^4`b )z {|,~jl!!"$#/(4'0+*"l0s1xu&ol0-st)n)/'+6
#,)/'0-)s5'MV'po
Z!(<+8l0s#/),ylM*-"9V)/l)Bl0s/so'+"r-stlZml0+89#/(%stl-Z!l0so!'0+*"J+8!stl6),#,'0(
l0(4	
x  l0+rxu,(<xlMo!'0s/st'
 ffCoqv)/!(;{Jo!'0+!'0"9c6J'0+$l0s/s4 wcY!!!Y

 fi
)/l)J{
 Mvs!) u

 ffCo~v1)/(V{;
 fi

 v
b ) 

o!'0+*"$$'po1vJl0(4xwJv+8%),#/.0s6
$ &
l %'ffCoqJ(l0(<x
)*!Vvs!)J+

!

,Z
-/.10

 *-%!

l0(4x#"gj9)/!!)/9'pos#,)+8l0s V'8o)/%!s5l0-l0s

fi243658719:5<;=5

>@?BACEDGF,HJIKMLN:OQP,RSI(TVUXW6Y[ZJ\F)H^]_P,Ra`
>Bb@AcfiWedf/U)PghP^i&jlkkkmjP<nDofqpXdofqr	Ysutwv=`yxzfp{Y|L=bo}G~ sU)PHgP<Hi|kkkJP<H nyJ}YpY
s

H tv=`z},,dS D{P H R P H 

I[P ii[ kkk  P nG nl

 `z}8YyDofWW6fEN)#Zp@LqN8dZE1ZpYoW6LqZfqN8dS})fW6OfiDfp]
>BL&A'CED]

PLNOP]

FzU Z}8YoN#]

g P H  UXW6Y[Z\JP HE R ]
LNOP HE R	h

LN:O	]

P H  `

`

F`

>@?BACED]_PLNOQP]FU Z}YoN#']F`
>Bb@ACED]_PLNOQPJ]

Ffp4]

PLN:OQP]FzUyZ}8YoN#']F`

z}YO,YDLWZlZ}Yfqp	dGq|<[ DLN:OfNWm DG']'DofpN:fQl`
 q|<<*G<q[vX8|8q=^v[q=qq|8['<q<8q|6[6q	<q|	q
8q<68q|[{q<#<<q[8q|	q8q<68q6[{q[8|q<<[qG[q{=8o[
6=
XVa,=8'<@yGz8#J=8:^yJ^X<^ z+ yX<
 6q|66[#8q6*6q=  [q[aq[=[w)lqq<q<4=<<8'@v[qq=q<
 
lG@v[q/,G86|6q|6[	qV8Eq	6=<[6  V4<  <q<8
[8o48[

 =<|  8qo)[<|6=^

|q<|66q	q<q)| 

 6q|66[|  V|qq|8|4<*
|[6=<='8<q<BG[<[8V  6=qG
88q6(Qq#6=<  qa

Qq#8[|6	q<  6=q  q<
#<6=<|6q|6q

lGzql[qSqq8

o[<6=<)*)6E8<Q[  =<|  6q

q<q

q<4=<<8 :=6q	#<:<66=ql<[
8<66=(qo)[<|6=<G8q[))m(G8

 =<  6=q^qo[<6=X=<4
	

qqfiff@  q4  ,q<	<  [<X	q[

)#[[<6*<)<8Q6=8[<|6q|+#<  
 6S8q<64B<  6=yq<q6
8

 =	6o86+q<q  [(<  6=<|q6[	

!#"

O,YDLWZ%$ P& FEd4  6qN('*),+
c

 D-'/. gLN:O0'

. gw\JPLN:O1'

.  ghF`

R ?|YLOaYDLW ZZ}8Yfp&LqN:O
!32,4651787:9;<=7>9@?ACBBED>9<GFH"IIJLKNM Y[ZPOEQ  	S
Ld&Zp&b ZU8
T Lp&ZLWfpOaYopfN
Q ` M YZ'V),+ ?YQL	d[YZf@DXDfp&rQW6LaY[`XWQY#YXNY Dfp LWW<~ tZ)U

z}YNk'

'\[
g

	

'GH^]i
g

'4H`_baFdc

beqP

c
c

c
EdL ml^T:pYD[Yop&pYOnLo

F

IAQ

 'GHf. g  '

Yqp=Z+YN8dfN'f@DjOEQ

.  g\JP  N:f[

geqP
F

Ed

L=b[ZYN'GHhji

 	SR DLN:O#fNWmV D-'gsr Ht8[-u N:E' H o`

lGfiv Q8:<6=Sa    q6	[xwV[qo<[|[6S'#[<Q<6=
6=,8zy|[  [qq8q#4=<<8{v 8<66=^G<fiq#<zy[  SG8
 =<  6=VqzQ<[	o[<=V  [ )<)<S8q6yVqq|8  z[,
=|   qq:q8 <yo)[< <4<|q|66[|<8<|qq)y|G/=6q[l<X8<66=
qfi<q|	q8q<68q|6[=<6q^q<8

 =<|[  *q!ffz  6=[[=q<fi8

X'GH<[q#q8S\'	

}~{

fi
Gz0`^`zfi^G6`:6;C8^fi

!^61=fiU
88UN;1EASAxXG#fizfiqbANEE
 E#fmAd>fiA@3j1m  6E{q{*q:#g0EAS jfi60jdEE
G:;1s\8HLE P fi%18,




 


 ^;





 
6 



Eg
 C6k
 8C



 
6 %



 j 



 




	

{E j( 


fiff
!"#$"%'&)(*,+-"+-*.!,/0/1%&2)3"%*1+-*4+56"7+8*9":&;)=<?>?%;@:A
<*+51>B0	%C-D:+5*EF;)*G+5;)"H/"JIK/%*L&/<,C-"H*M"7,<+8H6"N1+51+-"+-%/OQP+-RS/%*T"')3A
"%*+5*U1/%%*T"&,+VRS/1%*9"IK<XW=Y%C-D:+5*EQ"Z;)*G+5;)"/OHZ//%&[)3"%*,+-*=<\?<
/"4$<C8C])3"%*,+-*6^<N&/_<C-"6"/1WO
`a ,bdc$e A>#gf /"^h EAS K<=&/_<C-"!"/WZ<*&  <="@+5;)"i,<\"+5<CT1&/i* kj Ifi/
 
 0
l  {)l  :   x  0l  {)l  8     x  
 


% 0l {)l 8 
 0l {/l : 
x




m  on   <*&
# 
 0
l  {/l  :   m  8l  {/l  :  
qp 



 8
l  {/l  : 
 8
l   )l  :  r

s,1+-1+-"+5%  "<\"t"<\"[IU%*u%<*,+5*Ev<\"F>N<C-s,1+-%" j +5*1>N<\"+5*w%;/+V(;x"
1+5%"Q1C5&yXD/1+8&x+8*0@>N<\"+-*y;)*,;)/1*+5*Ez>?%*{+5*gE%*/1<CoO}|H"x)3:"%*+-*  
~m  x8@
 0
l  {)l  : n  <*& %  ~m  x8 8l  )l  : Vn  *C-W  +5
 A//%&St<*,&  A//%&SS6OK*F"	6<<&/=<*&[fiC5C5*,&/&)(*+5"+-*+-"U+5U"<+5*%&k<
  \8HCE  IU/   m  #_
n 6  m  x8@% 0l  {)l  8 n <*&   * 
f

{


j
Y<C5C O^*"+8^)3"%*,+-* "U;)*:G+5;)"^/"~I./%*7"4(,1"$"~I.Z&/_<C-"q+5i1%C-D%&7+8*<XD
i"=(,1"4*O

	&)(*,+-"+-*<\1	*"%T,+-D<C5%*9" j <fi&%>N*"1<\"%&xW"C5C-IU+5*EN)3<>?,C5E+-D%*kTW
6<<&/	<*,&MUC8C5*&/ %  O2K<<&/	<*&MfiC5C5,*&/H<C5FXI"<\"	>?N)3:"%*+-*,'<\
//%& S "*"1/0/1%&  O
n <*,&  m  
1n OYU)3:"%*:A
`a ,bdc$e A>8f /"  m   

+-*   ~m  @ n +5  A//%&  ,"H*"  A,/0/1%&  O7?)3"%*1+-*  ;/<*,*"
	,"<+5*%&xIU+-" x "? 6<<& /' <V*)&\ fi %C5C5 *&8 / %& )(S *+-"+-*]O=    % ~m %    n  "?K<<&/
<*&fiC5C5*,&/K&)(*+-"+-*21&,;)%."'/" ' 8j ' m  nj ' m      n <*&   '
<C5C Pj <*,&"7*+8T  A//%& S )3"%*1+-*M+5    /\8U' ON *s K/IU<:	&)(A
*+-"+5*I.;/<*xTW,<U"+-E/fi1+-1+5"JW#&/<,C-" WF+5*E7<7""<C$1&/1+5*ENIfi/
C5C-XIfi <*&
ON+8=IK<XWxIK?"<+5*M"N% /"   j   m nj   m @ /\n j
U\
 m @:  n <*, &   %   U U<C5C v O

 
 *, %T% *;)4%C5<\"+-*,!",<\"q;)%*&"';/<"+5i%<*+8*EH+5*%+-"/%Y&/<,C-"$C5E+5;4<\
&)(*%&<C5C-XIfi/O
%
d

fiU-
Ti$0g-k@\%/,/@[)-__xH\SzS\kT_qS1L/[H
  F\S
\F Z/\-  80z\S?8_%)1  9J)_\NH#fi\2    N\k\F 
:./\Z:-  0H0M\?8o//@S}J/@o\UF
KLt5%q_[@'@?\?!9_/\:Vq:@Q')_@o%q\\9)\2x?:/
/\'\0:v]0N8o//@wJ/@o\=_k@fi H\Q\ 60=HQ8o//@S
9J)_s	_[@)
6%/	~4 5=5 8 % 	
fiff% fiff75 5 % %, s5 fiff  fi/ 	fi!,
 fi/$ 	fifi!  %% & H( '&)% fi *+[ 5, 5fi % 	'.-/1032fi *+(fiff " N5
"  #
" fi/$ 	fifi!,4%, 5 '_k@6 '87:9;" <S  sB>
#
 =@?$ABDC$E -_ ? 06GF@ff	fi7 ? " fi " 	5
HI< J L
 K'NM'PO
	fiR ?Q  SPTVUWXfiff	 	fi ? 58 HI<J  L K'NY'PZ;Zfi ff[F4fiff "  ? # ?Q  \ "\]J]
S^TVU% "  ff% U_
 = ?$AB C$E -_ ?Q 0i5 " # fi/$ 	fifi!` %%J + ( '^a@\%_o\#9@:@06 b
?
  ?Q 'c%N@/# S.d
 U%be **+! \'a@\%_$ f\N1% SgTihjb@2fi *+Rfiff $ \k:bmlGnol1pMq 
 ?$rts 4 k: ?rts3uw
 vxl "  kzy3 k:bm" lGnol 5 " J {\Z5[ ?$rts 'g|@% }l&pk ? 'g" 7P9Lfiff5  
ff9~\fiff%8Y ?$Q rts  
 k "  Q,yX, k:bmlGnols5 "  {\87 ?$Q rts 'Zfi ff[FlpF ?Q  ^fi!* " 5 fi ffoF
fiff " ' u
 vGl'+7P!  7 5 8 %   s ?    lpz ?  u
 vGl',|@% lVpz# ?Q
" d ? } ?Q '8^fi" P fi ?Q  ? 5 *N "]J" !'Q Zfi %z%" } ~5 #}fiff "  u

 vGl{ *+ ]  %
$
?
t
r
s

?
t
r
s
u
 4 vxltJ *+ ] 9W~! "  ' g '
-10#O	Z
 W~  / 	fi! %%  _[@6 'L|@% QB>
 = ?AB C$X -_ ?Q 06Fff	fi
 ?Q  ST>U " fi(fiff " 	?8 HI<,   K'NY'O	N ?  ST>UqW~fiff 	N5 HI< J  K'NM'Z;
fi ffoFW9t8  
 fiff " ?
 = ?AB  ? 'a@X__v9@:@06 b ?  # ?Q 'c/[1%, S	
 U%b
e **+! " \'a@X_ f1\/3
 S}Thjb+2fi *+fiff " Z$ \}k:blGnoldpy i ?$Q rts 
 k^^ u
 vGl " 
 y,
 k^bmlxnol8 "  {\5v# ?$Q rts '.|@% lp # ?Q '.7:9fiff[5   ff9~\fiff%57 ?$rts 
 k
 k:blGnolg8 "  {\75t ?$rts '
xfi ffoFil.pv ?  #fi!* " 5 [fi ff[Ffiff "  ?$rts(u
 vGl'
"  MyX,
7P! "  2 ?$Q rts   "  s u
 vGl^ ?$Q rts u
 vxl'7P9fiffN5   ff9~\fiff%5H ?rts  ?$Q rts '
|% k ?rtsu
 vGl'Dff	fi/$ \fi[R ?Q B ? '^fi H ?  R ?Q fi%	!71J *N ]J"  ] 9\'2fi *+
fiff " $ \k^bmlGnolp
 Y ?$rts 
 k:. ?rtsu
 vGl   ;y3 k:bmlGnol8  {\F8L ?$rts '7:9fiff
5  ( ff9~\fiff%54R ?$Q rts  k "  Fy3 k:bmlGno" lz5 "  {\Z5# ?$Q rts 'q" gfi!* " 5 Qfi ff[F_fiff " 
 u
 vGl') fi *N 
 vGl^',7P! fi k^bNlGnoly5 fiffN#     J {\/ "  ]  "  s 
 vGl
 ?Q R ?$Q rts  "  " sfififf	HR  Q }R ?$Q rts " $ \ "\]J] TSfi'@ff	fi/$ \] fi" N" 
#
 C$E -_# ?$Q rts 06'Dff8 ]  " 
 FU fifffiff " 1fi *+J s
 vGl "  ;fiffN " 	 ?$Q rts u
 vGl\W 5 !;Ffi fifffiff
"   " JJ ;
5   ff9~\fiff%5! ',@ff	fi/$ \fi+fiff "  *+ 5' "\] fi \ "  M u
 vGl^'Dff"	fi/$ \filp  ?Q
 ?Q '

"  F ? g#
 	S x 0 vyo !; #!  @	 X
Zu fiff	 fi%fi 1J  
 3 fiffK \*wE / "  ] 	 oW~\fiff}fiffg7Pfi	FD ""  }fiff7 "\" 	 "  3|@ ]J]  	
1J \1  	!,/ "  ]  ] \ / " fi@* ] 	$ \fiffUfi !
 	{\ ~ fiffg~ 9 *N ff 	 ff9\'Dff5
*+ " 
 fiff " fiff	fi " fi+~ ] 9: *N "\] J *+, "  ]"  I W~] 	FP] 	%5 "  ]   "  "\] 5 #" 5 D%J 	!
/ "  ]  ] \ "  (fiff#@ \1  	!{\	11 g
 x "  D fi "  5 N5( W~\fiffL fiff%  ] \J /!'
  J )_ NBH  /\Z\N\q9_/\:4\o)H#Y_o%!:\)_o\q\T)@ \S
 
/\@:-9  0D:\1US-%J%
6%/: DffPff " ,%K5 W~! "  IFU fiff "  !*91 \1 9,fi ]J"   7Pfi	FD "  ] \J @5 /J%
FU fiff@% 	!
 / "  ]  ] \J L-^fi\~  ; #5 -7Pfi	FD " ^h!\K006GF@ffJ 6ffx8  6ff " 	{\%5 FU fiff
o[

fiPE^o\E5E,:JE%J[`IfifiqXJ%









	

ff

\ 
!
!


	J\fiD\x	$\Jgfi	\!
Jfi%$fi	
j
Jfi%$fi	\	fi!
\	!8jfi
Jfi%$fi	\\
 \
\\jfi
 fi	
 fi$fi	
 fi	
 fi$fi	\	fi!
 fi	
 fi$fi	jfi
 fi	
 fi$fi	\	fi!8jfi
 fi	
 fi$fi	\\
 fi	
 fi$fi	\\jfi
 fi	
 fi$fi	  fi fi\#\\tjfi

\>\G	\fi
	%mm  %mm jemm 
     e\ % m m  



	%mm  %mm   emm  
     e\ % m m  



  %mm    %mm  
 %mm  mm    mm    


     
 
 mm je%mm 
%mm  
 mm je%mm 
     %mm  
        



 mm 
     mm   
   

   \>\x	\fiDJL+~	g\x	\fi!g\	\gfi	\6!

fizfi!JfiJ;q\6\^	\fiI\fi  		\ ! (fi#"fij}fiz  !+!#\gfi
 fi
 E! J3J%
 $  & '"@J6!fi;J  JJ!fij3fi  fi! JRJ)(  & *   !!fijfi
 fi
 E!@fi3, +J!,\^.
 -  fi	$	fifi!0
 /1, +%!fi*1fi`fij3256 4 1 3 7!fifi87z\
`J3\fi\P\	:
 9fij;
 7\!	j!\, +!fi%
 1fi6fij<
 2=6 4 1	\)~z


		JfiJ 6J8 7,\#JL %J\~>
 ~	g\x    7!fi?
 9\@
 7!gfi
B


	Jfi\A
 7\!	jfi8 7	\fig\'
 1
C
 fi\	E DE	jfi8fijF
 1$E DEJJB
 GPfiff "?HjJ I@, DXfi
O
fi H\!I  J\X
 ~	g\  K
 "@JfiL\!
 LFMV\\	R\  fi  fifi\~fijfiN DE J 
P;QSRUTWVXRUY[Z\^]`_0a,bdcfe^gih?j`k l3mon
p
2 eb (  & { r } x aca
 srrNqffu
H\fi 
7!fifi! 

23q,rsut a q,vwUx coc^yJa rs eadb'j.z0b,c s e|{c~} vs fc e 
v xrst a s b -=v g t'q,rsdwJxEv a

fi\\Gfi  fifffiJPfi	\fi!>,+  Dfij@g\fi\\	6D	!L<~
O

Z\`TYA^R~Sd^oS#^0^.RiVX^#RiRU


 fiJfi!fi<"PJifi\!fi7jfi@  ,+%J,\~fi  J\fi	!+	\J7J	Jfifi\fiJ	\J
+
fi!6J!L	\!  JPfi8%!P:!Jj!(+fifi!!j6((fi~jfi:~	":	!(\fiEJ
\5J\fi JJJ`\	$\J# 7J
 "@fi  \6fi!
 
 #\ \)
 %!J\
   		%  \



	

	
~




*



fiJJ\
 
J3
 "P\\i 	zfiz	\fiJ
 "@J6;fi \Jfi!#jfiJ}\fi\^\	 
 !>


 ": J	:fi J, +%#\\%fiJS
 , +%!J
 7@fi:fi!fifij jfiJ  jfifiJ\
\	6 
#\ \
 !\5\\U 	fiz  , +`\g		J8 7fi, +J!\P, +%!fiJff 
	\fi^fi!\fiJ8 78%\
  fi\Ifi!\J8 7R$\g	\^fi	\!
 "@fi	$\Jfi^\fi@$\J!




J  
$, +	JJ
 7Lfi fi	 fi$fi		J\fi! \
 "fi; %!fi fi\  jfifi#fij}jfi	fi





\@JJ	\J
   fiJ		
 0#: fifi Eqfi
 fi  !
 ;!\	\Jff x\fi
#

fi8#0

ffffBi.88o0ff>U<,8ffF!W
W88~08,8ff>8ff8ffff'FiffB8fff8ff8FJ,
WFW;ffFffFff#ffff8ffF8ff
8fffffU,ffE?8;:8ff,0:ff8fffF>F88
u3C8
 8KJ,?W[<,i<F<ffff~8u08,:8K^
) 
C8fffW::J.ff~:^ffJ8,W%%!8U<
~ ?u8u8<?8ff~
 [,W'#ff8ff~?W8C^8?ffiW,^ffK8<8ff
) 
K,W.U ~ ?u8u8<?8ff@~
 ,ffffWF3B8?WffU,^ff<ff8
) S
FffB,W:f38CU< J U< ~ Ku8W8
?,W^F;8<SW@fff38:U< J 3u8W8
^?8>ffU,ffCff???off!i
 <Wi@<J^C8<::ffCff8fffffC8
>ff3ffCB8ff838fffff  i?^?ff?8ff
oJ8<:3ffU,JF!'8u:,F8?Wff
0	
fiff	
 ff*38*,,*838*80:3ffffJW8E

ff8fff;ffi  ffU  8ff:)8ff,,88i8
ff^ff JB8fffff'F,W8ffffC8:
ff',i8W,<3*Wo::,Wf?ff
 8 U  ?Woff
8ff<BU3Wi;8fff30*8 Jff*8fffCff8Co8ff^
U~)8fffWffUuff8ff8fffff
,:W.W!ff?
 ff  ?ff^?
ff0.<ff8.,<W 8  !#"$
  !#%'&
 <W8,
;J~,.,)
 (  ff
 X*8U;8:,8ff8J<B8ff^W>8ffff
,:W J*3<8<'ff>,,ff 8,Wi,*
  !#%'& W  
ff8ff+
 ?ff
  Fff^?,0;Wff!8fff>F8ffFJ,
ffJW8>,:W J.,ffKffo8f,
Wfi
 ,ff^ff,'>8?8ff:,'F8ffu8?'  
K?8.
 -0/3ffu?~,W>8?ff',:0i;8
i,W.'88ff1
 -@8:,W<W82
 /3- ffC
8F,W:<W;U:8?,W:o88offff4
 -:8:,<  8
8!,:0 J%<,<Wi)8!,:0 J)8ffff:C8fff
8ff?i!U38>U<  J ffui@>>o


57698:<;>=>=>?@BADC E	FHGJIKAL;>:<;>MBANMBOQPLR<GSR<GUTWVX;>EY;Z:<GJ[\O]R<G^G_VXAX@`RYa
@BAXF
PLR<GJR<G^TVL;>EY;>:<GS[ObR<G^G_MBRcFHGJR<GUFdVXAX@`RYafe^=@BEYEYG^Eg;>AXe^=>VKFHG^E	M7AX=Za
FLGSOh@BVL=Z:R<VL=GJEM`Oi:<jXGkOlM`R<?1E	m npoqBnpor@BAXF
m s'trnpoq<su6wvxG7y7jXMUz'GJ{GJRUyWe^MBAXEY;FLGSR'@`=EYM|FHGJOh@`VX=Z:9R<VL=GJEM`Oi:<jXGkOlM`R<?}m s~q<s
;>A+M`RcFLGJRr:<MfjK@U{G@fe^=>M7EYGSReJMBRYR<G^EYPiMBAKFLGJAXe^Gz;Z:<j:<jXG_@`VL:<@BAKF

8HG^=>?@`A+FHGJIKAL;Z:<;MBA

M`O'VXAX@`RYa#e^=@`EYEYG^E^6'jX;>E

ecjK@BAL7G_FHMWG^EALMB:	E<@BeJR<;ZIXe^GBG^AXGSRc@B=>;Z:aWy~@`E	:<jLG_FLGJOl@BVX=Z:m siqcsdz'M`R<Eg=>;>G1m stdnpo7qcsNzjXGJAXG^{GSRgoFLMWG^E	AXM`:MWe^eJVLR
G^=>EYGJzjLGJR<G_;Afi:<jXGrFHGJOh@`VX=Z:	:<jXG^M`RYaW6

LL

fi|DLilxw\wlp\H~)
DwQg9\p

W\ii4QWiQ\1wWi`h

~.hwhcfiwWiQU\K


Lc\\UW`i

7c\hUWBi

Q\UQh~p<W

Y|^

`Y|kDHJ

HkD^



QQK`

Y|^

`Y|kDHJ

HkD^



Q\UQh~p<Wdi`wW

Y|^

`Y|kDHJ

HkD^

i`wW`QQK

Y|^

`Y|kDHJ

HkD^



Q\UQh~p<WNwi`i

Y|^

`Y|kDHJ

HkD^





Y|^

`Y|kDHJ

HkD^







i`

i`

wi`iQQK

Y|^

`Y|kDHJ

HDJ.^~



QW`QhU<W

Y|^

`Y|kDHJ

HkD^



QW`QhU<Wdi`wW

Y|^

`Y|kDHJ

HkD^

W

QW`QhU<WNQQK

Y|^

`Y|kDHJ

HkD^

i

QW`QhU<Wdi`wWQK

Y|^

`Y|kDHJ

HkD^



QW`QhU<WNwi`i

Y|^

`Y|kDHJ

HkD^

 

QW`QhU<WNwi`iQK

Y|^

`Y|kDHJ

	4<

 

QW`QhU<Wd9~`hhi#wi`ikQQK

Y|^

`Y|kDHJ

	4<



Khu~.hwhSiw~Qw#\Kh~ NiQ 9

WW`Q

W\ii4QWiQ\1wWi`h




i`

Lc\hUWBi

7c\hUW`i

Q\UQh~p<W









QQK`









Q\UQh~p<Wdi`wW









i`wW`QQK









Q\UQh~p<WNwi`i

















wi`iQQK

 

 


QW`QhU<W

 

 





QW`QhU<Wdi`wW

 

 





W

QW`QhU<WNQQK

 

 

i



i

QW`QhU<Wdi`wWQK

 

 


w  



QW`QhU<WNwi`i

 

 


w  

 

QW`QhU<WNwi`iQK

 






 

i`

 


QW`QhU<Wd9~`hhi#wi`ikQQK



K\

w 


w 

w  

w i


 

 |WWQfiUwWifi~w~hwhci1 #iQ #'

whip\w+K



Khd#h\\h~hxwQB\\wdQKwfHii`KhN~.QKh~QiQiw

Wh\QfiUii\hxi`QWWiUdK
\w+HU\.w
~.hwhS
K\U`K`hi`h\ 

ik`iU~Q\wfih

h.KhUBKQBhi`hhLi|i\.~_i\pW\iUri'wWiQh4wWi`\r\xw

#iwUJp\ipJu\\iQ\W`K``w+~hwhc\fiwi

ifw+~hwhci_\UW>

wWiQhhi~NiQwhw\ii`KQ\
Q`hiBhhWwWiQhhi~\dKfi\1Q\`WQU\



ph~

 

fi
	fiff		

 "!# $&%'#(*))$)
+-,fi.0/21343
15647fi8:9;,.<1>=;.?A@;,15@9;,CBD9E56
9;,F9;,.G=;.@H9H=I6JK9;6
17L9H1NMC=I6
1>=I6
9;64.@'9;,CBD9E9H1>9;B>343
OP1>=IQfi.<=R9;,fi.
Qfi.</SB>T3
9;@<UWVX1>9;,Y9;,fi.[Z\B>B>Qfi.<=B>7CQ:]1343T7Qfi.<=\B>7QY9;,fi.[Z^=;.<5-_DB0Qfi.K`7C6
9;6
17a1>/*M=I.</b.<=;=I.Qa.KcW9H.7@;6417@
OW6
.34Q:B>7:.KdaJ<64.7 9Q.J<64@;6
17YM=I1eJK.QT=;.f/b1>=-9;,.[=;.@HMX.JK9;6
g>.M=h6
1>=I6
9;6
i<.QYQfi.</SB>T3
9-341>864JDj
kml"neoCpqnertsu4vxw"y<z|{S}~hx0Iy0C2zy0(yS<DWz|z2WyIDND0AYzSSz^zzDD(yK0DN}FY"fiy
W<Cy4;ySyK;y;Xy; zyKfiSDP *Hy<y<h;y;yIzyKhDfiy;>bz[-{S}~h-ID:Iy'ID'fizy;
2NfiDD>mD|zS2GyY D2z2fiyyKIy<Ih zyKz22S-Gbz2Wy ySC2zSDEDHyfKDY
WDDDDzSby[hCy<z\Ez2Wy-hfiqhSzSS>D
;>
 H<R'736
_>.R9;,.GQfi.K`76
9;6417N1>/M=;.</2.<=;=;.QC.KcW9H.7@;6
17@U9;,fi.0Qfi.K`76
9;6417N1>/^M=;.</2.<=;=;.QX.KcW9H.7W
;@ 6
17C@Q1(.@7fi1>9[QC6
=;.JK9;3
OOW6
.34QFBYMX13
OW7fi1?G64B>3*9;64?0.0Qfi.J<6@;6
17PM=;1WJK.QTfi=;.m/21>=9H1>9;B>3M=I641>=I6
9;6
.@B>7Q
9H=IB>JK9;BDVC34.J<3B>@;@;64J<B>3*=;.B>@H1767fi8fij+-,64@64@'V.J<B>TC@H.9;,fi.R9H.@;99;,CBD99;,.;T@H9;6
`J<BD9;6
17@1>/Qfi.</SB>T3
9;@'9H1
VX.|BDMMC346
.QmQfi1[7fi1>9VX.3
178'9H19;,fi.|.KcW9H.7@;6417V.67fi8JK17@H9H=ITJK9H.Q0J<B>7C7fi1>9VX.\M.<=I/b1>=I?G.QRVX.</21>=;.\9;,fi.
.KcW9H.7@;6
17G6@/2TC343
OE_W7fi157j]15.<g>.<=UW64/B[@;6?0MC3
.\9H.@H9/b1>=HT@;9;6`J<BD9;6
17@1>/Qfi.</SB>T3
9;@64@B>QQfi.QU(9;,fi.
B>3
8>1>=I649;,?9;,BD9[5^1>=;_W@[5649;,LM=;.</2.<=;=;.QC.KcW9H.7@;6417@5^1>=;_e@RB>34@H15'6
9;,FM=;.</2.<=;=;.QX.KcW9H.7@;6417@<j
 1?0MTfi9H.[@H.<9;@E  B>@|/b133
15@j

 



 
 
}N~; 
~; |
64@|B>JK9;64g>.R647:   

 ~-71a



 >

b
 =@H1?0.
 ;U E
 /[  
2
/
>
1

=
H
@

1
0
?
.

}
I
@

T
I
J

,
;
9

,
D
B

9
2
/
>
1
;~ [    
B>7Q7fi1[




 

64@'B>JK9;6
g>.0647NE  U9;,fi.7N9;,.RB>3
8>1>=I649;,?=I.<9;Tfi=I7@KD<y<j  /[   E4 * /21>
= @H1?G.E|WU9;,fi.7N=I.<9;Tfi=I7
S   ^B>7QQ.K`7fi.[ 



S      <    


    /21>='B>343;j
 
 *

 &CY
 C






 .<9V.^Bf[M=I.</b.<=;=I.Q X .KcW9H.7@;6
171>/{S}N~hxhj].7CJK.\
 2  S-hU>5,fi.<=I.^~H
BD=;.m9;,fi.R@;.<9;@67.K`76
9;6
17NfijejL.0@;,fi15VeO647QCTJK9;6
17:9;,BD9f x b  E  B>7QN9;,BD9'9;,.EgDB>34Tfi.
<Dy64@7fi1>9=;.<9;Tfi=h7fi.QjhDzSD(Ifi>z2Wy     jq<yhq<y    ?G?0.Q64BD9H.>jhDzSb>y
ID<y[- L.`C=h@H9'@;,15x9;,BD9-^[  j@;@;T?0.m9;,BD9/b1>=
}NUb  
U   

 
B>7QF7fi1
  jEZ^ON9;,fi.0647QCTJK9;6
17N
,(OeM1>9;,.@;64@[2   
 64@'B>JK9;64g>.G647F 2 jE].7CJK.  

B>
B>7Q71 
   649=;.?G
647 @[9H1N@;,fi159;,BD9m  2       j
 64@[B>JK9;6
g>.647 b  j+"1P@;,fi15 

   B>7QL  

Z^.J<B>T@H.-2
m
  U"-2     jZ^OF9;,fi.G647QTCJK9;6
17F, OeMX1>9;,fi.@;64@Eb   -2j
]'.7JK.a b       j+-,fi.<=;.</21>=;. 
   B>7Q    j
=;1e1>/^1>/\    \M=;1WJK.<.Q@E@;6?G6434BD=I34 O>j

@;@;T?0.E9;,BD9'/21>=
}UE2    
UX[2     
B>7Q7fi1

 6@B>JK9;6
g>.647NE2  j'ZO

 1>9;,fi.@;6@-2 
9;,fi.647QTCJK9;6
17,(O(MX
>
B

7
P
Q
fi
7
Y
1



4
6
'
@
>
B
K
J
;
9


6
>
g
0
.
4
6

7


2

Kj  9=;.?GB>67@9H1Y@;,fi15


  64@E9;,fi.[
@H 9Qfi.</SB>T3
9RB>7CQ& 
9;,BD9     j e
1P
B>@; @;T?0.   j Z^.J<B>T@H.
3
.B>
  U








    2 UB>7QF/STfi=;9;,fi.<=[    b /21>=EB>343Fx;jG+,fi.<=;.</21>=;.G  S 2 hjG+-,6@34.B>Q@f9H1
 .
BJK17(9H=IB>Q64JK9;64 17L56
9;,F9;,fi.GB>@I@;T?0M9;641 7L   
B>7
QL 9;,.G/2B>JK9R-2    
1>V9;B>647fi.QL5'6
9;,F9;,fi
647QCTJK9;6
17F, OeMX1>9;,fi.@;64@jG+-,fi.<=;.</21>=;.09;,fi.B>@I@;T?0M9;6417L64@/SB>34@H.>U"B>7Q      j+,fi.<=;.</21>=;. 
-

B>7Q    jF+-,BD9m9;,fi.YB>3
8>1>=I6
9;,C? Qfi1e.@m7fi1>9R =I.<9;Tfi=I7KD<yR8>1e.@0@;64?6434BD=I3
 O>j@;@;TC?0.a9;,CBD9R/21>=
@H1?0.
}U"[    
B>7CQF/b1>=E@;1?0.EPIU*[   
B>7CQ7fi1 

 64@B>JK9;6
g>.647FE  j

L.R,CBg>.E9H1a@;,fi159;,BD9E     j+-,C64@|64@-64?GMC346
.QaV(OY9;,fi.E/2B>JK9   
@;,fi157BDVX1g>.RVX.J<B>T@H.
j 



   G
@;@;T?G.9;,BD99;,fi.\B>3
8>1>=I6
9;,C? OW6
.34Q@*  2  SE  hj*L.|J<34B>64?9;, BD9 64@B fMC=;.</b.<=I=;.QX
.KcW9H.7@;6
17 1>/{S}N~hxhj Z^.J<B>T@H.:9;,.YB>348>1>=I6
9;,? Q64Q7fi1>9=;.<9;Tfi=h7<D<yB>7CQ   2  S   hU







	

fi
fiffff !"ff!#"$%'&)(*,+!-.,0/1ff324"5#6
7 8:9<;>= ?A@B8C=ED.F	GIH"D!FJLKNM,OAMQPSR
TVUWP XYZK\[B]*=^_8C=`[a@B8C^ZbVced!Ff
TVU Khg0P [B]*=^i8C=:[a@>8C^kjmlnpoVFf
TVUrqsKutvK g0w Ox[B]*=^i8C=:[a@>8C^kjmln"o,Ff
TVUrq Kut K g0w Ox[B]*=^i8C=:[a@>8C^kbVced!Ff
TVUrqsKutvKzy{q K|t K<[B]*=^_8C=:[a@>8C^ZbVced!Ff
8C=:[a@>8C^Zjmln"o,F
= ^a?
}hH~'d!ceFqSD!F	GIH"o,H'4c,GF	D4d!c,Fjzc.ceHceH"b,HIF	D.c,FIc,F	d.H"o,H"bVFujzc,FIF\!cQlnBd..lc,b,!FIceH"F	o
xXwYjzcln"nfiqht%kgOo,d4GeEb,.lbjzc{oV'{F{!:  XY M,<  XwYl.DW!)qht%H"o
lGb,HF:Hp<  bCH"ooVbVcQlH~'bVjmc,lceDbVo,.%kb,.lba< Y  jmcCln"nr#!a!FIc,F<  lc,F\b,!F`oVFIb,o
H"*F.H"b,H' s  .FIc,FIjzceF<H"ol\u.c,FIjmFIc,c,F	D33WFbVF	.oeH'jCLOAMQPS 

4H"ob,!FIc,F	H"4D.H"GIlbVF	ob,4lb H"b,l:ceF	oVbVceH"Gb,H'bV`bVb,ln3.cQHceHb,HF	obVF	oVb,H"!~`b,!Fa{F	`FIcV
o,.H"A jln"HbVFIceln1H"Aln"n\u4c,FIjzFIcec,F	DF#bVF	.o,H"'.ojD.FIjmld.n"bb,!FIceH"F	oaH")ln"nb,!FGInplo,oVF	oajb,!F
<ld!bV`l.DAF	n"lAl.DAb,H"n"npl.HFIcelceGQH"oH"A 
`3mm.>" [LmALO0MQPSaXY:3
pmQ#%L . e  #m# |{LO0MQPSefih# 
P p  	I  ]  EI  I { L  V AII  I eyz      L%C I     V    0OA    p  
m    e ' 3`z      hL  	
`3mm.>| [3Lz{LO0MQPSCXY\

zz3	L . ea
  #m! |*LO0MQP_e
h#  \P z  	I V ]  I  I \ {L V  1I  Ie yQ p  Q  L%   I   V    OA   
z  L V s  Q  3m      L  %
7 V	I ceFICvlA~'HF	olAD!F3.Hb,H'jC4c,FIjzFIcec,F	DF#bVF	.o,H'.o`jmc`!celnD!FIjLld.nb<b,.FIceHF	o`'.n 
.c,FIjm>FFIc,	c,F	DFl<# bVF	 .b,o,!H"'FC.4oc,FIjz!FIF	ce.c,F	FID{FIFcC#bVb,F	!.F<o,.H'cQ.HoceH"Hb,HF	b,o.H"loc,GIF<llo,FaoVbVGce'H"H"G.bGIbVHpD!Fb,lnHb,ce{D!b,FI.c F  llD!FIc l.D'n"npd..D!FIc


}!cad.!c,F	oVbVcQH"GbVF	D.ceHceH"b,HF	ob,.F<GIn"lo,oj4c,FIc,F	d.H"o,HbVFujmc,FIF!celnBd4.lc,D!FIjLld.nbb,!FIcQHF	oH"o
bVcelGb,l4n"F  !F\c,F	lH".H"!~{GIn"loeoVF	oCH"b,!F`.HFIcelceGQlc,F:HpbVcelGb,lnF 

	 4 	fiff >|4 U  L V K         	#pmI  Q: { >#  m#  u:LOAMQPS*h!  *P
z  II  L V e LmLO0MQPSCXY< K    LO0MQPSXY\ KC  e ' <m{      L  	
7
!Fln"~ceHb,. H")}hH~'d!c,FbVF	oVb,oLO0MQP_`XY\ Kfil.DLOAMQPS<XY:3 K !FGc,c,F	Gb,!F	oeo
jV	b,!IF l n~ceHb,4jmcXY  H"o:lojm'n"no  }!cAXY  b,!F4c,jCHpo:o,H"H"n"l c   :F l.lnIFb,!F0  
oVb,lbVF	{F	b,o<H"oVF	d!F	.GF : )F	lGeGIloVF{ F{l%)d4oVF`b,.F.FI~'lb,H'.ojb,.Floeo,d.{.b,H"'.oj b,!F
.c,FI#H'd.oGIlo,F	o 
  ao,oed.{Fb,.lbP XYK   KH"o\H"lnpnyu.c,FIjmFIc,c,F	DF#bVF	.o,H"'.o<j 3F	GIld4oVF{
D!F4Hb,H')P k aF	.GF{Hb*HpoGc,c,F	Gb\bVc,FIb,d!cQ 4 *o,o,d4{F`b,4lb K gEP F	GIld.oVFP
H"oaG'.o,H"oVbVF	bJLlo<P  XwYiKuRQln"nBF#bVF	.o,H'.o*lc,FG'.o,H"oVbVF	bI\l .DA!FbVF	4o,H')G'b,l H"4 oaK  aF	.GF
Hb<H"o\Gcec,F	Gb<bV c,FIb,d.ce  s	   *o,o,d.F{b,.lbqKutvK<g
w O F	GIld4oVFK<gr
w P l.Dln"nhF#bVF	.o,H"'.o
lc,FG'.o,H"oVbVF	bI.FbVF	4o,H'AG'b,lH".oaK  aF	.GF`H"bH"oCGc,ce F	GbabVc,FIb,d!ce I I    *o,o,d.F\b,.lb
q Kut KgAw O   F	GIld.o,F Kfig)w P !<F#bVF	.o,H"'G'b,lH".o KNl.D!F	.GF:qsKutvKBH"ol.4npHF	D{H"{ln"n!F#bVF	.o,H"'.oI


fi!"#%$&
')(%*,+.-0/-0(1')20243658793:(%/-;&(%/:<>=?3:(%@63A-7-0/@6;)BB3:@6779;CB3D7E!BF(HGJILK%MD<1Nfi<1O.//E%P37Q%'R7TSU+WVX+ZYS +WV +[<
O.//E%P3\7Q]'R7^7Q!3DB3?-0/_'a`%BF3Dbc3DBBF3:*#dfeg365fi793:(]/-;&(ihj/E%@kQi7Q%'R7 +mlnh<po;q +mlnhrtsXhrcuwvxbc;)B4/9;&P3
y <p=?3:(%@63\+xl{
z h|rZuwvm')(%*}SU+WVX+]-~/m(%;)7m')@67-)3\-0(h|rZuwv:<=3:(]@63|p+l{h|rZuwvD)qQ%-0@kQQ!;qx3D)3DB_@6;&(79BF')*%-~@67/
7Q!3bZ')@67.7Q%'R7 +|l
z h|rZuwv:<|=3:(]@63 +_-~(n(!;iYW`]B3Dbc3DBFB3:*#dfe,3658793:(%/-0;&(n;)b^f')(%*g de +<|=?3:(%@63
-7.-0/@6;)BBF3:@67?79;}B3D7E!BF(GJILK%MD<?fi<[(A7Q%3B3:Pi')-~(%-0(!@D')/3(!;)7SU+WVX+ZYS +WV +<?bS +WV +ZYS+WVX+#7Q%3:(nfiA')(
'RB&E%Pi3:(7./-0Pi-02~'RB|79;7Q%3`%B3D8-;&E%/@D')/93')202wYW`%B3DbZ3DBB3:* de 3658793:(%/-;&(%/|}fiA')//E]P`%7-;&(n7Q!3DB3
-0/a'R723:')/97a;&(!3@6;&(7')-0( +m')(%*Q!3:(]@63i-7-0/@6;)BBF3:@6779;AB3D7E!Bk(T6):M6<ib4(%3:-7Q!3DBS +WV +ZYS+WVX+(!;)B
SU+WVX+ZYS +WV +[#7Q!3:({fin/98PiP3D79BT7Q!3DBF3-0/.')(n3658793:(%/-0;&(g(%;)7?@6;&(7')-0(%-0(!}+f')(]*nQ!3:(%@63-7.-0/?@6;)BFB3:@67
79;iB3D7E%BF(6):M6<
 Q%3DB3Dbc;)BF37Q!3')2);)BF-7Q]PB3D7E%BF(%/?79BFE!3-0b_')(%*n;&(]2A-b_+_-0/.-0({')202
YW`]B3Dbc3DBFB3:* dfe 3658793:(%/-0;&(%/
;)b_<  Q!3')20);)BF-7Q%P;)fifi-0;&E%/2BFE%(%/\-0(`f;&28(!;&Pi-~')2#7-~P3)<

w ~#
w]?:#
[(79BF')@67'R#-020-7A;)b^')202B3:Pi')-0(]-0(!@D20')/F/93:/.3658@63D`%77Q!3(!;)BFP')2E%(]'RBA@D20')//-0/.*%-B3:@6720A-0P`]20-03:*T
7Q!3-0(79BF')@67'R]-020-07;)b%7Q!34/')P3@D20')//93:/m-0(|3:-793DB:U/m*!3DbJ')E%27m2;)&-0@R)')//FQ!;q(fi')E!79')(%*83:20Pi')(
[:)8 ')(%*  Q!3D;)BF3:Pi/\!< : ]!< : ')(%*!<  !<
fi7-~2020Pi')( [:)R ')(%')20D3:/x7Q!3?@6;&Pi`]2365!-7;)bw`%BF3DB3:E]-0/-7936WbZB3D3|*!3DbJ')E%27_7Q%3D;)BF-3:/D8')(%*@D2~')-0Pi/
7Q%'R7?%BF':)3B3:')/9;&(]-0(!ibZ;)B7Q!3`%B3DB3:fiE%-0/F-7936WbcBF3D3(!;)BFPi')2@D20')//?q-7Q  20-0793DBF')2
@D20')E]/93:/-0/?/9;&2R'R]23
-0(}`f;&28(!;&Pi-0')2]7-0P3)<^=?;q43D)3DB:]Q!3a*!;fi3:/\(!;)7|')(%')20D37Q!3@6;&P`#23658-07;)bm@D')E!7-;&E%/BF3:')/9;&(%-0(!!<p7
7E!BF(]/;&E!77Q%'R7a3D)3:(q?-7Qn7Q%3BF3:/979BF-0@67-;&(g79;`%B3DBF3:E%-~/-7936WbZB3D3(%;)BFPi')2m*!3DbZ')E]27/.q-7Qg`%B;)`f;&/-
7-;&(%')2X'RBF-~'R]23:/\-0(E%/97-@D'R7-;&(%/')(%*A@6;&(%@D2~E%/-;&(%/:fiBF3:')/9;&(%-0(!i-~/\-0(79Bk')@67'R]23)<

fi]Xfitfi MLLGJZJnk g+?DRIZGMDI[RA+AR##I9MDI9MFDK8ckJGM6kIMFMT8XLZGJZRM#RILiR
K8#RILnMJDRK8G\GZ8MRILWMLpZGZJ:MGJc)M8RI6GZiGZfi)GkR!L~6G[&JGM6I9R
)Ik>:)K:MLkckRW?\
fiRI9&
I9:[D^3?&-)3.'P')(W;&(!3BF3:*%E%@67-;&(bcB;&P`%B;)`f;&/-7-0;&(%')2%/'R7-0/9#'R]-020-0779;7Q!3.@6;&P`]23:P3:(7^;)b
7Q!3|`]B;)]23:PT<m3D74f3|'/93D7_;)bw@D20')E%/3:/_')(%*i7Q!3/93D7_;)b`%B;)`f;&/-07-;&(%')2fiX'RBk-0'R]23:/p7Q]'R7_;fi@D@DE%B^-0(
<43D7. f3')({-0(X93:@67-)3bJE%(%@67-;&(7Q%'R7.Pi'R`]/\3:')@kQ@D20')E%/93  l79;'i`%B;)`f;&/-07-;&(%')2X'RBk-0'R]23
z i<_3D7
 H    /E%@kQT7Q%'R7  ln
 
S  nlAff
fi  S   nlnff
fi  S   lg.  H    
')(%*
 
 	
  
 






      {ln fi     p Aln 

fi     Al  lg.      fiX
       Al  lg.  H    
fi   DRDM  lg.  H    
3@D2~')-0P7Q%'R7J{k.z6):M|-b')(%*;&(%20}-bp-0/\/F'R7-0/[#'R]203)<
O.//E%Pi37Q%'R7 -0/./'R7-0/[#'R#23 #7Q]'R7-~/D]7Q!3DBF3-0/.'}P;fi*%3:"2 ! /FE%@FQ{7Q%'R#7 !   <3/Q!;q
7Q%'R77Q!3DB3{-~/')( 3658793:(%/-0;&( ;)bJ{kj7Q%'R7*%;3:/(!;)7@6;&(7')-0(>6RDMD< 3D7}h %$m     l
\& !  '
 ("fi W {lA\& ! z'
 ))fi  <  ;/Q%;q7Q%'R74h-~/^')(365fi793:(%/F-;&(;)b4Jnkk8-7x/+E *@63:/
79;A/Q!;q 7Q%'R7h -0/a@6;&(%/-0/793:(7')(%*bZ;)B')202-S ,w.V ,,l1{m/ ,1l1h-b4')(%*;&(%2{-b , l
z h<
3D07 ! 
f3'}P;8*!3:2m/E%@FQg7Q%'R7bZ;)Ba')2012 >lC2 !3_ 4
 6- 57!  4
 
2 !8_ 9
 6- 57!  4
 
w')(%:
* !8p 4
  


6- 5;!
z<
 
#')(]=
* !  z  bc;)B')202  /FE%@FQ{7Q%'R7     bZ;)B?/;&P3  l<\7-~//979BF')-0&Q79bZ;)Bq'RBF*
79;{/FQ!;q 7Q%'R7>!8 hi')(%*>h-~/7Q!3DB3DbZ;)B3}@6;&(%/-0/9793:(7D<  '.?)3')(,S-,fV.,l n<TO.//E%P37Q%'R7
@BADC

fiEFGIHKJBL MN6OQP;FRST+NUFT+N6OVNUWDLXYLKR&Z+[1J&O\"F]NU^V_

`/abcdIefgih j1kflcnmUkogpq1k&mUkrfqQrisaub=
t cdvk&k&j1wxf#r&y1h.r	aub=
t cdIz|{}a~s+r&y1fq+fq1mr&mpq
   b7cxs(h q1:h k>  D `) i )x3cxs"`  b;cd>VmUwxmUh.{p a;~9   dz{a;~8k&j1gyr&y1h.r
~u{p 	k&pwf0#bs1r&y+fq=fgih j1kf0
 ~Bsr&y+fi&f0mk1mUk&j1q1gr/b7)`(>p {}0k&j1gyr&y1h.r

 ~|s/h q17y1fq1gf+fq1mr&mUpq;b4cs}h q7h kl  `b9csI`b4cdy+fi&fi{p &fcmUk>h q
fVrfq1k&mpqp {d
vk&k&j1wxfr&y1h.r t ~0 -ir&yh.rmUkisr&y+fi&f=mkxh qfVrfq1k&mpq4cp {xk&j1gy<r&y1h.r
i.-b4
t cxd(firl
fh=wpV+fk&jgy;r&yh.r0{p >h U(9b9s}
 ~8um6:9b4cxdfffky+pDnr&y1h.r

 ~s+h q1y+fq1gf0mUkk&h.r&mUkh.f d/efgih j1kf -#b=
t ch q1  .ib=c{p h Uk&j1gyr&y1h.r
<~{p 0kpwfb9s)4b<c{p 0q1p<k&j1gyr&y1h.r<~nu{p 0kpwfb9defgih jkf
 "Bb={p h Ukj1gy/s1`bc{p h k&j1gy/sQoymUgywfh q1kr&y1h.r{p fi fi&giUh j1kfmUqspq+f
p {"mUr&k1mUk&j1q1gr&kmUkmUqffcxdeff+fq1mUr&mpqr&ymUk1mUk&j1q1grmUkrj+f#mUqffndofq1gf#fi figiUh jkf#mUqmUk
rj+fmqd

v q7h rfiq1h.r&m fxhp {p 1r&h mUqmUq+r&y+fmqKrh gr&h.mUUmr=p {gih j+r&mpj1k0&fh kpq1mq+{p lh UgiUh kkfk

omr&y0ho	p qp Vfgr&m fh.&r)mUk)rp	h.1oy+fip &fw8d-omUqh j1r4VfUwxh q2s VDr&y1h.r)&fj1gfk
1hD f&fh kpqmUq+#rp0gih j+r&mUpj1k/&fh kpqmUq+#lh 1mUq+#h+fi{h j1r  >h q1r&y1fomUqQrh gr&h.mUUmUr0fk&j1r
{p o1hD f0&fh k&pq1mUq+xmUq=op qgiUh k&kfklr&mUUwxh q2s2 .Kdoy1mkmUkq+p roh.UmUgih.Ufrpr&y+flBmrfih 
gih kffgih j1k&f0mUqmr1h f&fh k&pq1mUq+xmUkrh gr&h.f d
l)(Q  Q: ~o..-.7i
l&&&Vi.x.
K.V60/..+l .&.V.+i.0..
	#)firnfhffkfirp {IgiUh j1kfkd#fxk&y+pDr&y1h.rr&y+fifmk	hff+fi{h j1rr&y+fip &'k&j1gyr&y1h.r

  t ~i.-mU{h q1pq1Ulm{2 mUk/k&h.r&mkh.f d}(firfr&y1fkfirp {1&p pk&mr&mpqh Q.h.mUh.fkr&y1h.r
pVgigij+omUqd(firo f#h q=mUq.fgr&m f#{j1q1gr&mUpqffr&y1h.rowxh.kfh gygiUh j1kf0brph&p pk&mr&mpq1h 
.h.mUh.f~ k&j1gyr&y1h.rb
t dff
fq+f#r&y+f>kfirp {}+fi{h jr&k h k{ppDokd


~

fi

 

b&}b=~





fi

 `>i.i



i






vk&k&j1wxf#r&y1h.rmkk&h.r&mUkh.f d(firo
c 

b~

`li.i

~

fhxwpV+f2k&j1gyr&y1h.ro

D b&}b=~

&




 ~d(fir
~



 mUpj1k&7c  mkgpq1k&mUkrfqQrxh q1<mUr1pQfkq+p rgpqKr&h mUq;.iidfh.'c  mUkh qfVrfq1k&mpqp {

  
 "ff;b #b&/b:B~& 7bcdey+fip &fw"!d-xmUq7$#ofmrfis .K
r&y+fi&flmUkh q=fVrfq1k&mpqc p {o=	k&jgyr&y1h.r	c  cxdefgih jkf>ubc   c {p h (;~3
omr&yb:s i.-b=
t cd
vk&k&j1wxf0r&y1h.rr&y+fi&fmUkoh qfVrfq1k&mpqc p {	k&jgyr&y1h.r.ib:
t cd	)fir
f0hwpV+f
k&j1gyr&y1h.r	{p h U2&p pk&mr&mpq1h .h.mUh.fk})s
 ~:m{h qpq1m{:bcdefgih j1k&f	i.i>b
t cs
 `%
 .i `&
 i.i(
b ')
xcxD{p q1p3~ &b8d7+
h * fh qKu=b8d ,opB
~ubc h q1y+fq1gf  
 ff"
 ;.
b '/
xcxDo{p kpwf0}bBd	fq1gf0
 7bc h q1

 ~ dIefgih j1kf0r&ymUky+pU1k{p oh U)	b:sVq1h 
 ~d

021 5
3 4687:9<;=:9?>A@B>B77DCED6FHGIC9I>KJ=D>A9?76;L9M;N5OQPRTSLUK1WVX;9?768Y<>AE:=DZ<>[7DC=D67$\+C]?F>^N_;ED`[a<F8Ccbed/fe=DZ?>e@BF8Ca<7DCFNg;ED`h;N
i Z?6@TZ/675j%k.lLlmbR$fonLnqpC9?Y=DZ?>5`r;2Y>BFtsu=DZ?C=vC776wL9<7exzyT{?|}=D;rb/C9+Y^~AL A|=D;rf1v; i 3:4k  lmb:cnK1
 ; i >Bq>AEmp2=DZ?>KED>5687=DZ<>7>A=3&k 
  lobtRfcnX=DZ?C=}>AJ2=D>B9?Y<7}354C9+Yc67C9>AJ2=D>B9?76;L9;NOQPRTSLUK1
<?

fizt< 

I?}XLL$$%& )r	+_T	B++X-2	

BI oqoo2LT	Lm+mqoo

 +LqgMM+DcrT+B
^m2B	:Q/Dt:o2qoz)ohXzoz?o+oQBX+QQzKmWm2ooQtM$%/  /5A
 	um		Qo2	5Q	Iotm	roozoQ:++Q+Q2		toQQ
  	%Q?m2qoz$qozo++r2	QmM/.mooQozX?+Q+z


$2^oo+ 


 
 	ff  



  
 $2
	ff  
q+	
 
  
$2
   


	
 /&  $2 

	+	
Wt^	o2t2oqzozQvg Qz2X2	mrot^o2Q+oz $$2	 )& ?!I"     
# $2	 )&?$."$		 M& %#o q+	?
 &
 $'(  ? WQrzoo) * z
+ 	Q o+[5o+oQB + zrz z z$//q 8 2	-, oo ^o+^Q5o+oQm +z/.
o+WQ2o	oQr  t2$0
o+10
   + MqmmqW
* m2oz2 ^$%^o
o+:	+	3
 
2   	 4

  r50
  67	ff   r50   


 

	ff 

 	ff   W50   
  

  	fi  /%   $2 	ff ?50   


 4  	fi   /&  $2  %<50    
4  
4
8 q t
2 :v9  "<5;  ;  ? A .	Q-o+2 Q* m2oQI$%IzI

B q2Im
25.
C o+		o >=@; ? ?<=   	 (
; D
 
2 -		 ; ?<=  E  	 ; F2 .oQ)Q
mmQm5^
G +IcH 2q$%/	+	2
, oo5o+e$%/  	+	+. o+vQ	2o	offQ}* m2z%2 $%Mo+	+82) 2 

 	J0
M 2oo+QoozoQ++Q+z2
fi0

 2z5SR M zQz   Ko2  
+ ot<G o+0   #[L 2	mq82&M
 
2  	+	 N	  ? 	+ 82%P
 O 8 Q
o+   $2 m   c
H 2q  T
2 IQ5o  6H 2q 		o  ot	Q
8 Q2 SR UcH 2qW		o	QmWVYX[ZZZ!
X ]\ 

	ff$?  (O 8 Q2 SR^^ W2V     o]\_	ff  ?   KO A

 2 m 
 2 m 
^

W2
V
  o]\_

A` o  m/	m 
 tq zoz0   	ff -B` otm2q%	m  [2 2q )tq zoz0   acH 2q		o
	ff


	QoQQ^mtMQ0 % Qro+oQBX+z
b
L 2dec Qz 8 2m	fehg/g/i :tjG o+:m2moQ /e	Q5t	z5ot	z2:o+
[
ot o 2W	Qoo2cQot
k ml #n 2QKn oQQz	+o+26<o Tzm	
m2qoz)+oW[q+d
p 5Tqz	mUcH 2q)ot)oz2^QW 	o2@tq ]eho _tq ]ehr tq ]eq +o
Qq+d
p  qmhs t2ozq+dcp :Tq z	maW 1eL 2tdffc Qz  8 2m	ff2oz	t<eG 		
Wzo2qWQQQ	+ozeotMq Q
* zKvotzzo]	l 2	oz^ot2mMoz2	











   	

 


	ff
	fi


	fi  & 


	fi   /& 






 



t<ujv

fiwyxffzI{}|<~/ff7NxffU]xff
]j~K~}5_|5[xfi]

false
n1

n2
n1

T

p1

T

nm
n2

T

p1

T T

pk

T

T T

nm

pk
T

5%ImS/5]5555fff]dm_S/S]5h/1/]
  !&W/NdQ76 
 ff  fi I   K} 
 
S%6
QWh&  Q  m}  ff  d /   a% S@"   m 
@mh /5%57//_S5Sh  mh_S_1/1157//_S5Sh  
mh

5&]& 5]]h&a_5/
]$Sh__55M5/55_//S5]fff]dm15yhh7
/56_5/]h& j<hj5 [5/_S/fiS5_/a/_mh /_ 56m/
_5/5]5_/S]ffh
5]Y  U/] mh5/y_55_-Yh/S/_m
 ff65_/_]%5/5
   mK[_5/55]_/!]   
	 fi ff S_SA5_   

 
 [  /_S/S]5h    /] 
 
  "
 !$#  & %'%'(    "
 !
6  
 

[& %' ,*
 '* 
  "!+# 
  "!
$)  
$-










 * 
[. 
 / 

 
12*  3   
# 
 * 
 *    
# 
h  /


/


  #  ) #  -
 4%',* 0,*
3
87

%' *  *:9



%' * 

,*fi
! # 
%' 
   
	  0ff 
 /     
	 fiff !
 
 4%52* 
 	 0ff !
 
	 fiff !+#  #
3     

%5 * 

    6	 fiff !
  
 !$#;	 6=< 	 $) # $-ff>ff


?a/S5h5hhm(_S/7m 5_3 A@ ]1/__NB'*8A
 @+C /_Am/5/@_S]/S5h
/_/_mhh/_mDU /_/5h5mS] m#5m
h]5_/55hm_Am(_555h

mh_5_	E 5F5h5fihmm/S]#T5S]h ffFC /5F53/_hh55S]]

h]/1 /hNy ]55]ffi//5_/5K]mh(]#S$ /5
HGI
G 55J
 /y55J
 K"LMG C /_6Q/_5N O%GI.%
G /U_5./P h655
 NQLR
 7G TJ
S m
/U_/_]5T UVIJ
 /_ * HVIJ
 /5/55_W   /    
	 0ff / X  C S C
YJZ[

fi\=]_^a`Jb'^dc^

e.fdgihkjal0mkl0nEo.mklphkjdl0qresmklptkjdou=fve.twg,e.tkjalfigiesmmkou=tfixzy;l{0|}e.~hkjdesh~t=tkeshk~}t'es,|_lp~_n5e.f,giofd|q
~_nVhkjal0mkl~te+/dmkl0nEl0mkmklfig'l&h>lfif,tk~_ofo.nWhkjdeshwgao4lfit=fao.h={of4hke.~f$0sfix
 tktkd+lhkjdesh~t3tkeshk~t'es'|_l.'hkj,esh/~}t0,hkjal0ml~t3eD$o&gdlfi|V td{jhkjdesh  $x=y;l$tkjaoJu
hkjdesh+hkjdl0mklr~t$e;/dml0nl0mkmlfig,l&h>lfif,tk~_ofo.ntkd{j
hkj,esh8.fi  +xKl0h5'> 
    Ni>,     6Ni'     6Ni',      6KTifi     
 0=v2     ;  0Fx3Nl0h3lpet>h>mF~{h=h>o.hke.|Vo.mgal0m3oftk,{jhkjdeshwAe.fdg
nEo.mze.||4   2 +'2.'r~_nK  ie.fdgvO'.'+ o.hkjal0mku=~tkl.e.f,gBO  .5  +UVJnEo.m
e.||  X&  fiFxWh:~t:t>h>me.~j1h>nEo.mku:esmgDh>o.l0mF~_nq$hkj,esh:~te/dml0nl0mkmlfig  l&h>lfifdt~_ofDo.nV
.lfifal0mesh>lfigi4qx:|_lfiesm|q0sfi  +x
 tktkd+lzhkjdesh~te/dmkl0nEl0mkmklfig  l&h>lfif,tk~_oftd{jhkjdeshK0sO0/  +xWNl0hlwe8$ogalfi|dtkd{Fj
hkjdeshznEo.mwe.|}|   ,  v~n5e.fdgrofd|_q"~_n,  +x:lfi{0e.dt>lwsO08  'fao+gdl0nEe.d|h2 0sO0FsO0
~t/esd,|}~_lfigi~fv+2uwjal0mklX  0wnEo.mt>o$l$  $x8zjal0mkl0nEo.mkl    nEo.m/e.|}|V  fiwtkd{Fj
hkjdesh  $xzjal0mkl0nEo.mkl52  e.f,g;  nEo.mpe.||Btk,{jBx=lfifd{leigd~t>df,{hT
~t8~f;o.m
  ~t/~fno.m8eg,~tEkdfd{h/nEo.ml0.l0mkq  $x8fhkjdlX,mt>h/{0e.t>l$&qgal'f,~_hk~_ofo.nK  x
f"hkjalt>lfi{ofdgr{0e.t>l:  +alfi{0e.dt>lo.hkjdl0mku=~t>l'',uod|gDl/~}fDe.tKO''.',$ ,,xWwlfifd{l
  'e.fdgi  xTlfi{0e.dtklhkjd~tjdo|gdt:nEo.m=e.||N  $'fde.||_q  (Xx

+=aWNN1T4N1a,A;,2=NK
 gdl'fd~_hk~_ofo.nNdm~o.m~_hk~_0lfiggal0nEe.,|_h|o.~{w~}t,t>lfig&qw~f4hke.falfifvfi..Fx:zjd~}tgalfd~_hk~_of~tT'e.t>lfig
o f6e.flfiesm|~l0mofalno.me.ah>o4l0'~t>h>lfi+~{|_o.~{w~f4hke.falfif2:fi.4Fxzjaldm~_o.m~hq$lfi{jde.f,~tk,t>lfit
|_la~{o..mes,j,~{X{o$,esm~tkofe.fdg;hkjal$,mkl0nl0mmklfig;lh>lfifdtk~_ofdt~fhkjd~tesd,mkoe.{jgaorfao.h~f.lfifdl0me.|
{o~fd{0~}galuw~_hkjhkjal:dmkl0nEl0mkmklfigl&h>lfif,tk~_ofdt~fhkjal:dm~o.m~_hk~_0lfigpgal0nEe.,|_h5|o.~{0tBgd~tk{0,tkt>lfig~f+&lfi{hk~_ofax
Nla~{o..mes'jd~{{o+,esm~t>ofjde.t3lfiesm|~_l0mwl0lfifdtklfigi~fihkjal{of4h>lh3o.nfaofd$ofao.h>of,~{8mklfie.tkofd~fa
&q+t>l0.l0me.|mklfit>lfiesm{Fjal0mt/K~_ntk{Fjd~_h>.fi..&'3lfdl0m(5lfiesm|fi. &,zqe.f2fi. Fx::o$,esm~fd8huo
lh>lfifdtk~_ofdtw~tz'e.t>lfigrofu=jal0hkjal0mwhkjalpgdl0nEe.d|hktzesmkl.lfifdl0meshk~faDgal0nEe.,|_hktzo.nBhkjalpl&h>lfif,tk~_ofdt0,hkjdesh
~t0.uwjal0hkjal0mBhkjalfi~_mB,mkl0mkl &d~tk~_h>lfitVlfi|_ofa/h>ohkjallh>lfifdtk~_of$e.f,gXhkjalzfal0eshk~_ofdtWo.n'hkjalVkdt>hk~{0eshk~_ofdt
gao$fao.hlfi|_ofah>o$hkjal/lh>lfifdtk~_of2xBy;lptkefiqDhkjdeshze.lfifal0meshk~fd+gal0nEe.,|_ho.ne.f"lh>lfifdtk~_of~t ' 
 	
~f"hkjall&h>lfifdt~_of2x
fffi!#"%$$&')(*,+.- 	400/21436587:9 <;<;<; 7*=a?>@es,,|~_lfig6~f
s
	 7 9 <;<;<;  7 = 6E 5'XWGFIHKJML}NO	1P)1
	RQTS")'}U36587 9 <;<;<; 7 = ?>

RBAC X
 XTH
y;lesddmkl0~esh>l8esd,|V  ze.fdgrfao.hwesd,|V  &qhkjalpfao.hkeshk~_ofies,,|V    8EFx

 D3

fffiXWY
ZN[fi]\^fi][_[fi]`afi]bb0+.c <1ed f@g QXh	1/&i1j1L
P0klSrs	p l1UklUm?1:kn
1U sPko1
	 <kp.
P iqH c 
1 rQ +s
s1 alUPs!P3RHhJML0it/dmkl0nEl0mkmklfig*uvlh>lfifdtk~_ofvPw 
1
L <>k % T1Ukm1x1yP1 sjPko1
	 <
k zsP  /*mTL{1&
L 
1   Rs|
	 <P0
k .#s)1 dsP a8}>P  s|	
V  ~
)' }V     XOx' 2 O1&L 1 <P0
k TP+
 N    k i
V s"
	 '
 }       ;
 /*mTLil1Uklm1j1yP1sPko	1<kOpX

 /o.mgal0m~}fa<P0k=H

dmz~f1.lfitkhk~_eshk~_ofof|_l~}{o..mes,jd~{3dmF~_o.m~_hk~_fieshk~ofD~}fgal0nEe.,|_h:mklfie.t>ofd~fd$ue.tz+o.hk~_esh>lfigi4q
lfiesm|~l0mBuo.m
Xofhkjalh>o.,~{5e.fKmklfidmfi,fi.&a:e.e.gal0mAwo||,fdgal0mfidfi..FxT=jalfit>lzgal'f,~_hk~_ofdt
o.nT,m~_o.m~_hk~lfit=nEo.m8gal0ne.d|_h/|_o.~{XesmklXdmko{lfigdame.|e.thkjdl0qesmklX~_.lfife.t/lh>lfifdtk~ofdt3o.n/faof,gal0h>l0m>
+~f,~t>hk~{=galfi{0~tk~ofdmko{lfigdamklfit/nEo.mgal0ne.d|_h|_o.~{sx+=jd~t3dmko{lfigdame.|Bf,eshkamkl$o.ndm~_o.m~hk~_fieshk~_of~t


?

fijipIIi?)q
I*
a|

*#
pR
I*
*j
ITj*<*
).I<#IhT<

T
<N)I
^jKT)|T
II<U*
6
*)I}
h
I?j<4*##^T)O0*h)%<*

IR]I*TI<U*
I<*v*<#I.
p*
<
]*#X
I
<Uj
<40<
) **#)#Ij*##
INI<U*

*060
x)%TIT#0*}T)0#)qUM

* ] j
<}_ ]: T:I<xe0# ]| ): ) IITI*
*0}]%
* 
MTT0*T)0#)*N
<

N
00
%?#0%
)w
I)N<
I?}I
<*


0
)  *
)pI<p
I<
v0
)

X<

<
0
)*

I*<*
I<

<*#
XI*i<
j?]K
)I.
IT)T
K|<<I<U*
#*  ##
pT]*#

I*#^{*i<
j?]%
)]*{
IT)|T
<*
T)*]I
TX<
<*

0
)q*
<<
*T<^j<q
IT]*#)*  I
*<U*0*#|<#^}ITIT#0*T)0#X)N:
0<<
*I<U*:
*<
*0M
)I
*
<<

T*
)pxII<<,*0,
)I  <T*
) a ,I]I<eX*


*0
I*#
T)*hT)
RI#0
)K
*0*<0j
*04I)Ia
IT]*#)*6

x*Ipa
II<6
I<
  I
I<4*
<**
<
%j
*0<<#%T*
)h
I<U*
I<
qhy*
<<

{T]
).O
*
0R#I)
{ p .
I<j#*
I
6R*#
0#*RI]XI
IOT*
)*}
*)*RIx<x#*)Ix
IO)
#^

*0}KT<

T*
)6*
<<

}
*0}46
*O**]I*
<<

hT*
) 
 
I)
***|<
Tx<^j<
ITXT#0*}*#
<I<U*j)x*
I<
j
).*#
x.I<U*)
*0
I*)I
x*#RI<4
I<
x)I
0*X:
<<#
*RT*
)*}a
I<
4T*
)}I<
N}40*  
<<#

*
<<#
T]
)*4*I}6}0*K
IN0<0)I6*##^I<U*
 
q:p_K:] )
 
I<6
INI<U*j
I<
	ff
fi}I<


.
*
 fi! #"$$%' &(#")&($%* +",
T
 *N
I
0
)
/1032

x4 K
 5&(
 8&3 %
K
 9
x4 K
 8

2

2

&(76
&
(
 6
:
 6<;>=
:

 < T#*IO - T*
)*(? 8 *@? B A 
Ix<
.CED:0
})?}<*
Ix**]I6y*
<<

 
*y
<<#
  T*
)F X GCIH DKJMLONQP C D 

CERS
CTLU
CEWS
C<D:


V&(
V&($X
CY
W [ZF\ A


 
I<j#*|

Ip*)I
x*#h*<*]!#"$XxI0*<0|*qI*Tp
I
T)*.I<U*Y^&3+")&3Ox0**&(Nx*
* Y_ **)*}*#h*<*}

 ` ?1
I0*X<0*.I*Tp
Ip
*I<U*]
 * +",O0*:*
 OXx
I a
 ! #"$
 b
j<
O0*X<0M
*OT)*T
I*<*<
 * &(#")&(j)**}*O|<0*% c

_ *q0*<0
) N
I{*)*K*# I< }I*<<)
#h j
 )*< 
*<U*I<X0#0
R*#|<#^%*)*)I)*Xh
)**qx
v*##
 d_ IR
 #0
UT
)

4
*XN*
<
*qTT0*##
0
) R I
I|O}TT0*
*##
<I<U*)NX}
T)*

xI<U*4
I<}x
II<U*
Ye!f("f

ggih

fijEk'lnmoVlp4l

q)rsut+v'w4xzy#v'{|Xy#v}q,~zy#ssy+Kq)v}~~%ny+w)|X}y#xE}~%|Xy+|%y#9}x%'~zyK|%y+y@rnw)|q)vOsny+q)v'~%x!Q$F
 nyv}q,~z~zy+|Gsny+q)v'~%xq)v}v'w|%y#q)xzw4r}rn8{@tw4r9~z|Xq,[w4x%'~%'w4rE'~%@~%ny]}v}t+q,~%'w4rx+4q)rs~%nyYw)|y+|
snwrnw)~+M~w)~%y+|%E}xzy>~%ny+{|%y+|%y#x%y#ri~]|%y#v}q,~zy#sVq,~z~zy+|Xrx]w)O|Xy#q)xzw4r}rnn  nyv'w)4}t+x]8{|%y+<,q
q)rs9{aq)q)sny+|3q)rs>Ew4vv}rsny+|Mw)|sny+|y5~zy#rx%}w4rxv'yn}tw))|Xq,V}t+q)v}v'{Kw)|~%nyv}q,~z~zy+|Q8}rsaw)sny+q)v'~
~%ny+w)|X}y#x+nn~<rnw)~Ew)|<~%yTw)|Xy+|#
9(QKV[}n,n+X+z+), ]9FX+
+,5'  ]K,K}z++%%+%in,>E

 T
 

 ,,^ Kn

Q MI y+~YQ[y<qasy+Kq)v}~3~%ny+w)|%{Eny+|%y< z !4$8  ^(4)(%8i8Yq)rs 
z 4  y+~    *Qi)Q$%84iX[y@qx%~z|X}t~Vq,|%~%}q)vFw)|sny+|w4ru  nysny+Kq)v'~~%ny+w)|X{q)x
~wy5~zy#rx%'w4rx#5a QVz %8Q8$Yny+|%y~%nyTsny+Kq)v'~%x  *Qi)Qq)rs  !4$q,|%yTq,Vv}'y#sM5q)rs
E QVz %8z8$Yny+|%y  !4$q)rs9iq,|%yq,v}'y#s  ny#xzyay5~zy#rx%}w4rx<q)rsq)v}vMxz~z|X}t~E~zw)~%q)v
 
w)|Xsny+|
x
w4rx%t~%q,~ 
q,|%ysy+}t~zy#s[y#v'wa  nyw4x%~Yx%'4r Vt+q)r9~<sny+Kq)v'~%xEq,|%y~%ny

v'w.y#xz~+ nyTxz{5[w4
v >xX'4r y#x~%q,~<~%nyasny+q)v'~<}xGq,v}'y#sq)r	
s ~%q,~E'~<}xGrw)~Eq,v}}y#sM


fiff



 



 

  


fiff

 

 

 

 
!
  


fiff"

 


 

#$
  


 

 yy8~zy#rxX'w4r  }xq  |%y+Ky+|%|%y#s  y5~zy#rx%}w4r[y#t+q)xzy~%nyv}y+~%w4x%~xz~z|X}t~~zw)~%q)vGw)|sny+|
}xq    w)|sny+|X}rnw)|T+8i}x~%nyw4rv'{sny+q)v'~&%x%tX ~%q,~q,v  %%E)%aEq)rs
q,v * *Qi)Q%a#%E<q)rs  *(4)($8!i8  yay8~zy#rx%'w4rY}xErnw)~Yq  |%y+Ky+|%|%y#s  y5~zy#r5
x%'w4r[y#t+q)xzyTrnw4rnyw)~%yY~%n|Xy+y]x%~z|X}t~~zw)~%q)v[w)|Xsny+|X'
x + n$ )(Y}xq    w)|Xsny+|}rn>w)|<Y3Kw)|Gq)v}v
*,+ . - 0 /80 194(~%ny+|Xy}x]~%nysny+q)v'~  *Qi)Qx%t~%q,~q,v * Qi)Q%a+%Eq)rs~%ny+|Xy}xTrnw
sny+q)v'
~ %>x%t~%q,
~ % 32  *Qi)Qq)rsq,v  %%E4%T
4

5

  M 687#.  X9+,5^K5%):9(5%  FK<;,V}+. X
u
),M,zi],  =7+[5$>,T4}%,3,TX4,V5Kn%Xi4*,.V%#zX+
%in)zY

?z#A@'B y>x%nw~%q,~E~%y+|%y}x<q)ry8~zy#rx%'w4r1w)  x%tX~%q,~  }x<~%ny    w)|sny+|X}rn@w)|E@
 y+~'% #ACACAC#0%ED[yG~%nyEw)|sny+|X}rn  w)GFy<Vrny!2  %#ACACAC0%E2 EKw)|q)vv *+ H ACACAC$"I(4Fy<Vrny
Kw)|Eq)v}v *+ .- ACACAC "IKJ - 4
LNS
M    PT
O  	}xEq)ry8~zy#rx%'w4rw)  4Eq)rs
L 2RQ  S L   + L 2  q,v  %E2RQ #%>X 'Qq,v  %E2RQ#%>w)|Ex%w4y + L 2 
2
w)~%ny+|XE}xzy)
T V,,i5)K5 nU w)|GV + H ACACAC  * 4 W- 3~%nyExzy+~ LYX }x.rnw4r5y#~{)  4/ FKw)|.q)v}v + LYX q)rs
[Z + L X q)rs\% +  X 9q,Vv  % %>O
] q,v  % %[Z 5q)rs  41 3w)|q)v}v + L X q)rs@[Z + LNM^ L X ~%ny+|%y
X

+
}x'%
 x%tX~%q,~q,v % %% Z Fq)rs@~%ny+|Xy}xrnw!% Z + xXtX~%q,~_% Z  a
% q)rsq,Vv  % Z % Z %
 yT|%w8w)Kxw)([w)~%~%nyTq)x%yt+q)xzyq)rs~%ny}rst~%a)` y]t+q)xzyq,|%yxz~z|Xq)'49~zw)|Xq,|Xs
 yt+v}q)1w)~%ny]v'y#q}x.w)~%q)}rny#sK|%w4 ~%ny]Kq)t~%x<y#xz~%q,v}}xXny#s@}r~%nyT}rst~%'w4r@|%w8w)q)x
Kw4v}v'wYx+3.{ W- F~%nyExzy+~ L D}xFrnw4r5y#~{)F{  /4Oq)rs  ny+w)|Xy#xfi/8cbq)rs\/8ed}r gf y#'~zy+|# -hji$H 

kkl

fim=npoq.r sjtpuvxwynpz!{fi|)uRnp|)uv3uR}s~:s.z)rv	nuR3
fiY[fi<3"Rgy#
[
fi[
Y 8[g
fifiAAA")\
"g[gG''jg
 'jjRyAAA "
Y[.	A
fi
"Y[ g
"gN0'_jjR	  jg  0=_jjR:
fi"0)Pj"
fi

a)Yfi0<,j<jaR<"0a
   3$_ <   RRaA"  AYj.&<3"aj  [$)A
R  0$,$    # ! <N  8
 j   N    ,)AR
!!$_$[fij)A0,fi) $ YYj$ Y! 
 )A0Aj[j  Y.pj AjR_""0<_"jjj0)AaR')[Aj"$Y   fiAj"
R)RjfijRpK j\jR3<"afi[)j&.R "j0)A0R[jfi   )A0Aj
R_!0AA0 <3"a&jG  AYjx\<3"R 0&$  Y     
j     pj	AAj0)AR,\y 	$,$  j	)AR)
G 0$[ Yj$pg[g!  <NR)!aAA<3"aj

 

	ff
fi !"$#%'&()!%+*-,./fi,01
 )Rj$j,$<j"fi"!)[<"32x< R$a  j)ARjRAjjaaj
ajRYR))j"	K_jR54p)Kj)Y"Ajja0Y76,098;:=<?>00$6   
@ YjG)<pa<3R &R"[Y3<)0[R a)#$Ra0<a\pj"K	)
"N<""0<BAj[)54paR\j<3"a_CAjKR  )Aj  8 
D ) E,FGB IHKJFLMONQPRJSH1TUNWVKX)JWNEYZ [YC\KXOJ]N^;_QLMONETa`;bbNWcLdN5ZfeYgHKZh	ji J]N3LgXOJUZfe "0 Y k
`;ZRVlHKZbnmEY koZg#pYe?`;ZqNWcLdN5ZfeYgH;ZqH]kpLMON=VNffk5`;XObrLsLMONWH;Jm g5t[cT3bnXV;YZ+\uLMON9LdN5eLe9HSk
v N vxw NyJaeUMYzPYZ LMON{PRJ]H3TNWV;XOJWN|JXOZfejYZxPOH;b}m;ZRH v Yff`;b~LgY v NH;ZFLMONeYC$NH]kg5t
 )Aa3<)=j)'a<)R<jj0$R0aj0aRAY)AgjaajR'Rj"#!0
a\"Y)Rj)$jxRR0RARa_"N'RN))A"A0NRR"R'a3)NRj
R   )A0$Ax3R,j{2)"Rj8Ajj"=A#R[)\ajRAjAR)Nj"A
j[j0#R$	j <3"a jY)Aja)Aj0jR""0<\"jjj0)A&&"j0)A0)j
j<"aGjR#j0#RajAjfi<32x)<\j:"A!jj0#R$   )j)$j
13  a8x|<p8-"<?8 : <p8 : <ff8-+:(>_=R_)54p j)YAjo2Rx)R
 g 3	yNW\)A# R\Ajfi)AjaA >R&"Ajj$jjoZpg!
j <3"a jg	
1$

fi/Cf$R
jIpG{Ej~WpUFWWxU
IfE[
O 5)]3.WCg?U'WsxG[qKy5yoOlWS35
; 3W/ffy]yaxBB]WyKBC  ]yj	K~5y  aCKp
  n   
 fK/5)]3aCg?U'WKfK5O]3WC%gpUFW
G[aS35
oO  E5|.;WgxW  W  ]  ?G[W]35
oO Ky5y
) gWEG[Ky5yoOlWS35
KWfyW/]  W]35

jIpG{E;..BC3g  G yyy1 .O1 WsU
 K/K  '   yyy$  
 gW~KFgxU  E
Gq{G/]  [of"{G{K]

jIpG{E5.;WgW7W|U
Cy{  WKafyaB |5     yy   
[ n lp
 fK;.BC3%  WK;.C3  W  Gq{G/gKB]K
 ;RBC3  WK.QfK/;.C3  WGq{G/]  
[
{I{]  

  a [ f3yBWC(.W)53  WK[
   W?UFWW/yWj{|]y{K~fyg  CWy. "B]y/K%Ka  B;KB]y{K
Ka  B;K)B[j]]aB5[]KWKKafy?+KB~Kxg?UUWj7KayaB  KoRgsU
K.
   W?UFWaG//fyWxgxa	E
	ff
fi[ "!$#%'&"(9I)=*(;*+ &,$-./0$&1132(#54;*(6-'87#9&1:-;=<

 (?(A@ BDC{BK    BKy5y]3  ? fy]yaB]WBEC  aB  KUBf  FK3=KWf
>
WO53  aQo  WfGC{5)3  WB{KoWfja)53  W?FKB)fxBK	C3  W G
H
I'J K/LMNO
:P RQ;S(-32*# -T5/&U(@V&:QW2*A(+!$#%$-X(A@.&,$&1+ZY[Q &:Q ;7\@$(6S-.#]7^!$#(;+6.&"(Z76##
2*_1@?$ `a_bc&,51(;Z(A@Z7	c8@$76#]&D&:Q(S40=S+	 "!$#%Z&"(?xdl*(;*+ &,$-./0$&11
2(#e;4 *(-.87#9&1-T?<
f=f=f

figh*iGjckml6n*o]p+q	h*r.suto%h*to]po%v=lxwzyWlcr_{|k_p^}~h/o%
uEuff$, 6_x=1^SffS
 ?_x  ;0%_% 60;_/_ 6[$x %x>69
 ?_x  ;0%_% 60;%$U6_66 3x^
  ff3u_?$ 
1_S	_	x>u\
 WA_^S_	_;3
 GD
 W8_	ff\	
 u\
ff3u6 $ 
G[_?$
u3
[  _G? %_%x_$?_\:66S_ ? 6_x_?6x% 
 ?A u\+?__x0G6 _?TV0  ;6S%W?_$O?T+_W_$?3%  _
>
E'~%  ;60'/?366S6 V:6\_%6    .GDWO[W_.;60  ?_?D
 60;$?_x*1W61^SD6	^%$6_6u6S Vx_V$?/
_Z_?0_x?_;60Z6 ]/?._/*1a'%%16$T6$?_%x:_
/_$x_%__x6_T66 6.GDS_V01^SS\"6 0  :6zO1
:_T_?$x	$x_/__x*SR6/_
 *1^.:_T_$x_/__xff5u6%[_?
6 ] _x;_/ $ ?ffu_~%  ;6/%6 $ _ ? 6_  ?_?T6W6_
x  ;%0_%_VT6%6 $ _WU? 6 W_ _'%6^$?/_x_/3$x+_6%\6
6^1SS\"6 S%  
9xW/__6_/_$?/_9_RU6/D%D$x_6+R_%;G__ux  T%%_0
 ?_%  6>_; T  ?_%  6Z%$D6_6G6 T~66 S%  _%D$?/D
? 6Z__ ?'6UX6_x%xT%6G%1^SS9_VO V6U_ /. ??

 ?__?%>Oxx;060	1SS9 * _xZ6[  )_6?UOxx;06*_06
  66O 6\mu Gx/:69_ _x.6/_? %_xV_6*?%'_>x%c
x;06 S+6>? 6_'u\%%  ? /%W>%x  ;0%_%WOxx;%6
_%6_Ex0=U0  ?;;%0;?%6
	ffR	+DGff
fi
 
Z 6_? .   U6O Ox/:6U_E _%xZ6[_? %x_6/%? % _
Oxx;%6+ Sc69?_[%[%;?/%u_>$*_xV6/_Oxx;06+%  +6

 R"*! $#%%%& A'[)(:$*W$6+&,-./#%fi("012!8  S_43)546$(,$fi7
16fi.A,
 M NO 
3c8%1-.(8(:_79:
;fi<
::=> ?
3$GuU=_/_? %x._6/%? %A@< ? ,6_ _ '6/_6%%  Dmu uOx'x
_ _%x%D_GOxx;%66% +6[%ff?_%R/6?xVU_ 6_?d+<D
B _?+?*ADC6C x
_.%?+_]/?@< ?E ,$x;/ _6/?TE:6GFI
H zT_ 6_? $x$ S.$0$  /% 60
;M%;6O6%_x/66+_]/?Zx?6:6'%66Z:6__$0J$^
K %6x%%=U 
 M NOMLNPORQ/m -SUT=-VXWYLZZ[]\R^!*$#%%%& ,_S1&`!Z(a
bc
2d$e_S1&`! (\A^fhg*
fi
@  ?jiR_1&`!Z%)(,%k8'[ lf g 
fi3camG43:jfi 12]1'fi)
n	njo

fip"qrsutZrwvr
xzy|{~}`|j|~y~|U}}w-~~`~$y
wR4%8`	11^%+,-. h)R/7-%48"1$	)R)	Maal)1"1a	U
RllD1 41h117a$)%2+47X-1ZjGcj1waM1e  )))	R 
< R  )>4a+4R+$M0*D2*1e	-Dj  2-8%a >%

      *   R  ]	+
 ;	";D**^] 4  >%wD%4 ])X>)D  4.D8D   1"" 
  w)A  $4.Z)    R)GR,  $4)w  : )>%wRD  Dw%4D 14 
w4]4 4w] 1
>zehaZ)`^D)~+112w2G4$$Da1U`1Ul%)1)+/D  )))DR   
|D  2  )))	R   %4<Z  : l  h1Z  e2%7;D  -%/  )a;2>7
    hDR %2< 1Zwc 
   hDR/7 

YRD`4  G  1`  414 ZD     *   R  2+`Dw%l     %R    j  D2
~ )))uR~
  w )1RDc	*eY	c   %^~R    1  D>2YZ )))	RUff 
* eDR/7 >R  
fi`	 )
]0*D"      1   e"D% 44*  GR    1  D
Rb~ )))	R| ,h)
 fi"u
    * eDR/7
wh  %Zw 4`  )8%RD  Dw%
  *   R  
l4  *  1A  *   R  
 `Dw%"28 GwD        1   P w
 4w
	 *R     1  DRU )))DRZ 
    hDR7A c%Zw4 G~  $4.ZD> Dc2  
 1
A  wG	,    "      R  U R  
$  1 
$  414 RD     *   R  2+




 8wwZ  - D**  RD  Y4GR4 14Z  4.D,>D,2   1>R
)a    )+D"!lR4#" "w4$&%  X 1wD)  4  DRw *;('DR))" ")a  
   ^R4#
+**+
>zeha ,.-"U-a2Gl21):+1`-1U)  *xzy~{~}2"| j|~y~|U}}w|~`~$y
Y-1jU1,a1Aa2*1w1U441w)% ,:2-U$)) X%7a2-)2%Gl%17,-.0+%11
:12ff /;Z4aD%7R 0 X%w+M1a)1->2-4+l%7a7MD>1$)1 / 
YRD 32`%wRR  %8w)2     )4 5 7 6/9 8  %ZwRR;%144 +)3
 : 
 6 R
  1  & ;  %G]D|   88ww< <= 5  ! >0  ?   A1  8wD%G]D
   :xzy~{~}`|A
 @|~y~|U};-}w-~~`~y >%ZDcw< *         *   R  2A;8414; 
1 
44  GY )C B~) DUlw4]4 4wj1;1>  1  cD~ )))DR| RU )))	RZ 
El%Z
6
 F          )))u           *   RD   |GD H "




*



R


~ D
   R  ~ D
:
 F
6NM F
D
HJILK
D
HPO
    S  VWYX3Z\[]W
FR
Q
H
   S
 TVU
U
~ )))D | UU F | )))	 | 
F
Q
  U H\I  
  H
 
^)^`_

fia&bdc3ef`ghdikj	lmbdnpoqirbdqikjirs)gGtvu1gn4wx=f4jyzb|{ffir}~

#!9r49G4=+==&]3=14r+z4!ff<4=4v79G
=4149r4|94!rr+9r=444rG=44rz +=4=
G=9ff+(rG<Gr|4r]
C<94&&4`4=+9N+=4rGN79G4=G4rff49!m4=4=
  94rff+d       ffN1r3)CGd    <v     4=ff=4<G
  +<+444Y+<++Y4ffrP|Y+=4<G79G=
&+=]7=<4r49drG=4r+	+=p=ff   +d4+=
4VG=r7=<=49r4G=rff4<GNr     d3+=     P(
4=r+z m+v3r!N<GrG=4+	+ff4=pGffr=4<Gff&+94(7=<4!
&+=+r     G      +    )    G




&vP+(r  P|+    1<3   \=    N44r#44rff+
dv   m+v
=+J&N4`4=p]+  4=]44rd+d       44Nrp\+=9<G
N79GN9=94=.=ff  mmk#  49=]4=m44rff+
d       #p+    ff    p+v]C!JC    ff    +A]CG
r<G49(9<4=r+=9<G(79G&+=44+=9<Gr
G=4+d=ff       d4+ffp     r4=!<+1ff<rNffr
=444+
 N+=4rG==ffr     ff
=G=	4ff4V4V=444+
+=4<G r4<r9d=rr<rm+
=rN99=r]p	\4(G==4<GV&9N47=<4   `  )  ` 




4]9=4+p#  `#  # `P +V
 441ff  mpV4]m
44r4rd

   

 





 kv   +<GG4=V=949+

+=4<G(79G



 4r(4=#+Jr
	m=|4+Gff9=+ r4(=<9<4<+7=<]<G]	

&4fiffff=Grrff=#+=4G=9=4rGrzff+G=rG+Pff9<9< 
7=<4ff+G]+=ffrrd<pG=<p=<)&ff9<9< v==r4=)+
=ff<+ =rp4=++34G<G& ff]r ff4rrff=r&=9r9<4<+p7=<
<Gr
 =rr+4r 
 J&r49&r&=<G&r4=G<Grd=<99	]4=
4<Gff4+G=rN=ff9<9<4r+7=<<G=+4r 
 Y
 = 
  G<+#r)&+

 !"$#%'&)(+*-,/.021'354/6798:8<;=1,/.?>@1BADC 7?EGF AB8H.B>HIJ8K8@,L8@4M7,/.N021O.GA
V F 8<;=1fiWR.X,2YO71OTZ,N3[8\1BAB8H><P)]^C  _ V >`AfiCa bMc\;4/.N0Ld


 F 4MPR0S35,M.BTU6=79421
,MP Q

e X. ,
,N3Of r=9ff<+vr C_a b 4G<Gr4r]	G4+==<d<hgji-kl`monm$pLq
r n-i-stulwv=l`xyhv=z-{-mon-|)iPffr9	P+vr
	G<Gr4r]	G4+==<d<]
Gff4ff<+N

z+4rp 
  zr#4	99#rPr C_a b =9	4=r 7z#ff+GG]d4r
4ff=r!=444++=4<G=N#ff+49r=r4=44
+=4<G]=rpG4==r4r=dr<+pr=ff<4<Gp4G47=<4ffr9]&=)
<4+=G<+(

}  "j"~#%H&)+*,M.8<;=1WR.X,
IB10M6=.1>`P*>]/6=.X1 F 8<;=1I?4M7<7fi1G/>`AB8<AM ?E . 1O8H6=.BPA8H.B6 1> 34MPu0
,MP7> 38<;=1O.X1[>`A[4MP1L8\1OPA?>',MP  ,N3 7?E AB6IO;8<;=4/8S4?WLWR7 ' 4 3O,M.[4M7<7  Q dU;1fiWR.N,JI?10/6.1
.B6PA>`PPR,MPR0)1O8\15.BTU>`P >wA58H>HIW=,M7!Pu,MTU>H4M7:8H><T1]L>`/15P4MPQ e ,M.X4LI
71D3O,M.-[d
e .X,
,N3Of C<94z4=9+==4G=+4+!CdG==4<G= 4=CG44<d<V44G=r=4rG=P
4+94Vff<4Cff4G44rff<+=4<G P  Er!+=9<G  ?E4p
JM

fiRL

-Qfi5XHQ_?
L 
fi  XOo+/:5L O  L /OH  Q

X/fi5=X
LH?hfio-fi</wX/
S^-
OOO
XJ+
XH:j
 h/[H?:</XL^

OOOX 
hfio-fi</wX

fio-fiX  
o
: L h=5
  G MfiX
Xo`/fi_5=X
5/:5X
 L
-Qfij
O2H?
X/fi5XJH?GLofi[oX  /

O  OOO
G   /GO?  
SoD
X5XJH?   !o    

 O
   JL
X
  hfio-fiX  
 
	=o-fiH/X
o
 L fiff)+
OGL=5
  </fiX//- G/G

X
X[wO
O/X-
o5==5
  X



O





O

fi



O

<






fi





+MM w

R@ +?/O  GX  K/ LKwS


O/XD
O  X[ 5
  O 
X
)oLOXO?  ?  /?/u[=5
  
O  ?UX  Q/Lw^L  X L 
 XM
R@++/5X
GL
 OGU
  OG/ M w
 o
5 "!$#&%(')%5H?_O  ?X  ^/ L Q O
/5=X
LQ/SHQ?+  G MM @'JS`//-[Q*
 
fi 5
    U L, +\OXOGwXL L/^/  
^/.
 -/G/O</
/LL/M0 /u  1 2
5_  
 XL 3/
  fiS5
 476 89
:

 Z 

;=<?>@BA9>CEDGFIHD K JL*MNOQPSR,T'U')V,"JSLW#XW%ZY H ?+\[,OQJSLW]^RTPN?[UPS_M^%('L`#Iab'U'XJ'XPST?JSLcMN(L`%fidJ
5[U')V,fe_LcJNg(Th(]iJ0O'j(%W'#)lkmYn p
o q#&%#)rY 6 s$t
 LcJbJ0OQvxw  3/^
OLG5
  o</fi_   
MS  OXOGXwL=L/
u
^/   X
/ /G/O</   
 M  
L  UXy4 6 8 z^
^L XXGMX
 M
  
 wwY 6 s *fiU=5
  GrMN\aQ#IMN  3/
w: L mff/ {4 6 8 /?/OU=5
  
"!$#&%(')%   3/
U: L o/U5/
5
GKMX
^fi
(|}fi5/
5
GK/R
=5
  G
 MN\aQ#IMN:/`Lw  b 
OHQ?+  S~
 /RXOH  :O/"   
 
 LL_XG5
X///GOLQ/</G  , o  ^^MHQ?+/[5=X
L Q NO/X
~
 / GMX
^
2fiw 5
  m
 MN"ab#MNSO  G X    G5/G
5  
O  XQX? 3w/ 





L


o

X



/

w



5

=

X










L




/

o

H



?

+


`
_o  ^ MUH?+fi/M_
/XL5X
 L ~fi





fix_UN_l_^}h_},h$~N"}" Bh,

(""b("b"^"fh"h=Qcr",(Qb*hB("$$mNSQQQc7$fSh}f"}
"`SS\hSBS)cQ."X"cQ\S"$&Z}b}cb5l.1uKQbQQQb\WrG"}\Q\h}
 bQBc  hu^cc\&(xc"K\}Qu$5}?b{{{m"}Q"hx(}("$}  \QQ"\b
(,cb"$5``
`_}N`,),(`` Z&{NSQQQcc$_SBBXIc"^
$2c5.1
~9b^\9b 7 U .ZmNSQQQ9ccU.b(fi"}NG"}b"h&c\}
`_}`9Q 
  {B Qm"=Sh}
 
   9.l
 
 *"Q"}\}b 5(}hc" `" 	 ff


QcbQ
 fi."\m"S
 c\Zb(fiZK.  W $xr"}(""b("}b\"",(b}"
 $&() "}Q\^&.(,cb"$
 "`"SSBII"
 Z&
.	  
 W $
 fi."\m="S
 hm}"B\z (cb\$z.b(5S_XIW "  "=`yhB("$qllG"}b"&mSBXIc"^
  c .	 =h?NSQQQcff 9$   GbQcSBBXIb"^&mBhbS_XIb"
 ~=h
    "}Q"S"}      "\y"BS{SBXI  ""
 `yZb( ($hq}^G  "Q)Q""bBK
r"}fi""mB"$"SZ}fS_XIW "^xuff"$}ff
 fi."\m"S
 hc
 fi~"}Q\h}
(,cb"$
 7"`"SSBXI"
 Zh2  	  
   $KSBXI"=.)h
 	  
}fS_XIW1"^`   .?Z	 Qr"}&("$llG"}b"&x&u) _hb
fh
 _&"}bx"}
hB("$"1
    hm" cQ ~bQ\S"h}Q)h"{"}5B l  "Q)Q""bBz(,cb"$z 
 ``2f(bQ}"h${G"}"` ($N"&l}bS"}Z&cc"ScbmblUG"}fB"(bB}"7
f}Q\b  !  X 
 # "$ % ' &)( " .9`.b(
 *  $ 
 # "$ % + &)( " 
.,
 $ *
.

/ 0324065	5	78249;:=<?>A@CB ED.N?F(SHG?),\EDWI(I   `KJ WLDW?XXEGED0lMD`ffN S OJ _F(EDQP^HGN
1
*)J ),f3DcSR#G#P TFfijWW)UI $  V * &IXW Y[Zc\EP.3GhQj\
]^cD bTF#_^` "b("fQ"b{Zha5hb
.
b ."}\Bb2mbfiQ`"r
\=hSfihG"Q)Q""br(,cb"$B2{}Q"}Q"
Z"$}u"}"bcc\&("$rc{c"KB\\"hbQl"mb*GQ\\rh IXW Y fxbhGZ"dc W Y  S\B}b"
"b{&rbK

:=<feg/106hjik5	lSm)noQ9pn	qsr=9)q6o78tuo4n	tv735	59pwxlyao2[n	tuo4lz{/178yalAy
fi~2h,,| b("$=}'} NxuBQ u"}u($mBh(,1*_"}x($Bcal b("bhS"$="uh(,h(`SBh
\h\"Q br}Q$h*}Qc,N"("&"bcc`h("$Q
~3f3LC3))
f}\b""2${"}fG$S\*GQjxQb{c\("S_hh1^B{hlc\("SBhhhj*)$  V ."{cc\&(2c"
\}Q` N S\ "{S\Q b&S_}0 QcbfiZ}Q" "}($m_(,hju"} h(,h(`SBh
\h\"Q b{Q)h$h~G Q\U"$f
 bhcQa u}Q)B$&Sl"}($mBh(,1hxcQuh{G$h}(

SBmhBhQScb.Z}Q"="}=").U"}^($mBh(,1"b"B".QG=$B.f=h{B(.
\h\"b*$"($m_(,hj."}bQh"$"Bhb{fi&ficc\$}bcmh"} B"Q"ah"c  )"Q
}\ZQh""bQ  bz"}}Q{\Bb{hz"}bc Q&"cbS"5c"hhq\}Q"b" fl}
"Q)Q""b(cbB"$xQ.b"hm)$l SB,h}"}}Q\bh{"}~$b\}Qk fi
m".($mBhhQScb{$h}$h"hmf}bQh\$^",(b}"(}hc"U.\ }Q)B"7Z"  &cQ\
H cb("("f}Q"b ' }u}$` b ="hQ&"Q
"}(}hccb( (,cb"$"SfiSBBhb
$bcQ}Q"Q{QcQ\{h}bhG$${h
"hm
"hcQ"QfiZh""}

4

fi438v
#uuk8#	u8d#uKa










#
u 
a



MvMxa#fu8aE
 uK
4#Ku Q#Ku
LS 	S 	S
LS 	S 	S
LS 	S 	S
LS 	S 	S
LS 	S 	S
LS 	S dp
LS 	S )k
LS 	S 	S
LS 	S 	S
LS 	S 	S
LS 	S 	S
dp dp dp
dp dp )k
dp dp )k

8L8MvH	#
88EK
8L8MvH	#uK#a
uK#KaO88E
8L8MvH	#XuKu
 uK
uKu88E
8#Ka8fM	#
8#Ka8fM	#uK#a
8#Ka8fM	#X88E
8#Ka8fM	#uK#aO8E
8#Ka8fM	#XuKu
8#Ka8fM	#XuKu8E
8#Ka8fM	#6vKuuKu88E

 E  vMxuMv8aAa8MaEv XOuu)8Kua
##Ka8M
 uK 4#u




 
 
 A E  A E
 A    A  
1
1
 A E  A E
u 
u 

u
u 
u 
u 
 A E  A E
A H A H
 a  a
 a  a

#uuk8#	u8d#uKa












#
u 
a



8L8MvH	#
88EK
8L8MvH	#uK#a
uK#KaO88E
8L8MvH	#XuKu
 uK
uKu88E
8#Ka8fM	#
8#Ka8fM	#uK#a
8#Ka8fM	#X88E
8#Ka8fM	#uK#aO8E
8#Ka8fM	#XuKu
8#Ka8fM	#XuKu8E
8#Ka8fM	#6vKuuKu88E

Q#Ku


 
 A E
 A  
A u
 
u 
u 
u 
 A E
A H
 a
 a

 E3   #	#a8Ma#uKadv1MvMxu Ouu8uKa
 #AxMv8KMvu8#	#ajMAa8Kv81vuau188KAMa88kv  Ha
Lua#Kv8HMa8 
[a

fi3S4u33,3HvS8 	

fifffiff "!# $%$&fi$('*)+,.-0/ 213)4/)5)fi$
6879;:9=<	>?>79A@CB9&D*EGFHB9#IFCEJ9&KL@M:*BN9&KPOPQ=>RS@M:PE?T+BN@MDUR: >JBNFCQ=>FHIVS9#WPB@CIV9&DXEY@CTIPBZF&[C9;B9&FCEJ@M:PR:P\(R:
] 9&R>J9AB&^_E8K9AT4FCOPVS>8VS@C\MRQ`>J@XWPBNRS@CBZRS>RSaA9&KbK9AT4FCOPVS>8VS@C\MRQ`c#RS>7b>J@C>FCVWPBZRS@CBNRS>RS9&E&d
egfhjiklhjmonqpsrtvuw&xzy|{Zw*}~AS}lN*3AHjS}w}HfiX}2~&}HN*3YP4xwLw4A}H	x#x5	wCZw~A
x5j}Cx
  24Zvgx5w=4 qNMZb	wAJwbyo}HX+}fiJZ	l5x4sHfi}C
 }H4}{ASwx5j}CxM%w0fiCx3&~N~&	+H`G}Cfi
 wN}M~=*wAX{Zw=0J3j}l0}Mx?w}l=x3Hwwxw=4Cfi
	w?{ASwA`xwx45  `zXAH3}CfibyY	HlH4}Hx45*w;X}HPHJHw`wC~&s{ASw
x x5w;fiJ{ASwAxw=x4+ L? ` .	wAJwg}gfiJZ	l5x4sHfi}C  }HZs}{ASw=#}HfiX}
x4s~&xxCx}HCwAC w4A}H	x55  
 J&A9A>42Z2Iq9#FK9AT4FCOPVS>?>7P9A@CBCPyFgT5@CBND0OVF	jFC:KLFgWPB@CWq@MERS>RS@M:FCV[lFHBZRFHIVS9
>7PFH>*KP@ 9&E:@C>*@jQAQAOBXR:42Z0@CBLd9A>*I9FC: EJ>JBNRQ=>WFHBN>RFCV@CBNK9AB@M:PHfi
EOPQZ7>7FH>gPH"RE(>79L`VS9&FCEJ>g9&VS9&D9&: >Ad9bQAVFCRD>7FH>b4Z* `RTFC:K@M:PVRT4
PHfiMZL   d687REREgKPRSB9&Q=>VIq9&QAFCOPE9*>79b7PRS\M79&E>WPBNRS@CBZRS>K9AT4FCOPVS>(RE0FHWPWfiVRS9&K"R:FCVV
WPB9AT59ABB9&K9=<	>J9&:PER@M:PE#RTR>;REWq@MEERSIV9>J@FHWWVSRS>AFC:PKIq9&VS@M:\ME#>J@L>7P@MEJ99=<j>J9&:PENRS@M:PE;RSTYFC:PK
@M:PVSRSTz2Iq9&VS@M:P\ME3>J@X>79&Dd

 EFgQ=@CBN@MVVFHBC	>J@C\C9A>79AB3cR>7LFg>79A@CB9&D>7FH>EN7@%cEG>79R: >JBNFCQ=>FHIRVRS>@CTIBNF&[C9B9&FCE@M:PR:\
@CTq>79QAVFCENE?R: ] 9&RS>J9AB&^_EGK9AT5FCOVS>YVS@C\MRQ(4(FCO>Ja;j9&VDXFC:fi&C	%Zjc98@CIP>FCR:X>798T5@MVVS@cR:\BN9&EOPVS>Ad
 ifiki55Pknqpsrjw=x4+ 242Z 
  AH=x4Zs~&xxCx}CHJwAN`G5xwA}C  }HfiH++	fi~&x4sHP
JwZw0fiH}Cws=}Cjxx5	wHwg4Z	wAJw( (}*Aw&xJ4xw=J}HN#  j}HJM
egfhjiklhjmonqpsr uw&x{ZwX}~&}HNXG?P5xwXw4A}H	xx5	wHwgZP~=x5	}Cx8x5	w*~NHfi~&j4HXw}M~A
ws=}H	xY+`}X4xw=J}H}HLw}~=bXwA*{Nw=j}l(}CxYSwN}lxHwgwMxwAZsHfiXwfiJ{ASwAxw=x4+j
 `  AH}Hfi4xw=J}H  +`	HlH4}H3x45*wL}CPHHqwJwNHP~A{=wxx5	wfiJ{=w=
xw=x4+j `  w=w;0}*x4s~&xxCx}HCwAgH w4A}H	x55
 J&A;9A>42Z8Iq90FK9AT5FCOVS>>79A@CBN2R:d;9gBN9&KPOPQ=9(>J9&E>R:\ `  >J@b   FCE
5T @MVVS@%c#EAdz9A>3  Iq9#>79`EJ9A>@CTKP9AT5FCOPV>EGc#RS>7  FCE>79`Q=@M:PQAVOPER@M:dz9A>3Iq9;FEJ>JBZRQ=>G>J@C>FCVq@CBZK9AB
@M:EOPQZ7>7PFH>L4blL.Xd?90QAVFCRD>7FH># `  RSTFC:PK@M:PVSRSTz `  d
 EEOPDX9g>7FH>0  dG9&QAFCOEJ9XIjFCEENOPDWP>RS@M:>79ABN9XREFH>(VS9&FCEJ>@M:P99=<j>J9&:PENRS@M:@CTI 
9&D*DX
F jd>7P9AB9gRE9=<FCQ=>VS2@M:P9WPB9AT59ABB9&K  9=<	>J9&:PER@M
: @CTb
d VS9&FHBNVS  
  *d6879AB9AT5@CB9
`  d
 EEOPDX9>7PFH>(  `  d6879&:>79ABN9XREFC:9=<	>J9&:PERS@M	
: @CTENOPQN7>7FH>  

 Xfi
d ff T3    
>79&:QAVS9&FHBNVS2    d  EEOD9(>7PFH>   
d @cFHWWV & T+@CBEJ@MDfi
9 X  d  ENEOPD90>7PFH>
RE`FC:9=<j>J9&:ERS@M:@CTEOQN7>7PFH>  
  *
d #@%cFHWPWV % X (5Zd  
E &z*T+@CBFCV
V %S
 LFC:PK
:@C>`FHWPWV      T+@CB`FCV
V   "  q>79AB9RE;:
@  fi
 EOPQZ7>7FH>`FHWWVs       Z
d 9&:PQ=
9   RE;:@C>
(WPB9AT59ABB9&KPdG6879AB9AT5@CB9  Iq9&VS@M:\ME>J@*FCVVWPB9AT59ABB9&K9=<	>J9&:PERS@M:PE8@CTFC:K2 `  d

687P9T+@MVV@%cR:P\gQ=@CB@MVVFHBbRE@CIP>FCR:9&Kbc#RS>7b>79R: >JBNFCQ=>FHIRVRS>LB9&EOPVS>E@CTIPBNF%[C9B9&FCEJ@M:R:\I 
FCO>Ja0FC:PK2j9&VDXFC:&C	%8FC:PK2	>RVVDXFC:&C8T+@CB>79QAVFCEEJ9&E#D9&: >RS@M:9&Kd

 

fi!#"%$'&)(*$,+-$
./*0)/213134,0)57698;:=<?>2@BADCFEHGJILKFMONQPSRT UWXZ
V
ADC E`aCbC Cdce @ AWhicG f ejEFCk@ _ cejA Y EHGc G,l ]g[ C3m'@
`^e%cAQA^@BA ]n[f @ [ copeqCrC3m'@ ]_ E@DAfist@^e ] u EHAvxwbY\ykmp[^c ]_g_ f{z _ ] ] _gf _
M
P
G ] _Q| ce\o}G2c _ l
~ ]_ G`^e%co}A^@BA
G ] _Q| ce\o}G2c _ l
{yeE3Ck@ _ c e`^e%co}A^@DA
@
@
p
o
*
G
c
l
]_nf@ @_ aopf E;ABE3Ck@^y _ @Q@ @ @ o}G2c l eeEFEFCkCk@@ _ cceeAA
_ _
[B_ ]_nf _ f _
_
*;*%2{*{ZxJ\a*
xD}r- annQDnant D=%^ r =t% t%^a'^F ,%'^ t%ar3 rx,t',,;J'
hW ,^3^a=VD}na=%-,^ 9D-='na},;' h  'W t,^;x,t',*^%-
 '^F ,%rnna,na', ,n xnatQra -'a9a^ ,x'^t'fi'^anfi^nt D
'a^t%-nfi,=%'^F ,%^'x'^ a- a; a'^t *H t3  Jnt D=Wt'=n^
 ^ t^ ,t 2-t%%-, % -;*% # t^ ,a^2  n^t ^ =na^= fi  %n^Q ^ ,a^b'
'^ a ,'   t%,t= '3-%p% W '-,-' n-=Wa n-,;''
 /=0 } 698;:@aC*st@c`aeqcAtA ]g[\[D] _Q| opeqcJ@AQo,`^mC3mpc CADc CFEHAHrc{sDE3ejEFCFlfiCk@BABCFE3GJI [^]_b Cdc @DA
 ] elG ]| EceCFE | @LEHGC3mp@OABE%)@ ]n[bz @aCbsQ@Lc`aeqcAQA ]g[Lf @ [ c o}eqCC3mp@ ]_ Ed@BALKFMLNQPRAQo,`^m
C3mpc C
MUJjT O [^]_ A ]| @G,EFCk@xA^@aC  cG f P  EHAG,EFCk@ z >mp@DG [D]_b 9
cG f ABC _ EF`CbC ] Cdce ]_gf @ _ Ah ] GC3m'@ f @ [ copeqC3AE3G  rC3m'@  _n] sDe@ |i]g[ Ck@BADCFEHG}I  T UX
V  E;AA ] ec{s^e%@
E3G  ] ejlG ] | EceCFE | @ ] G	C3mp@ABE%)@ ]g[ MPL   z
w _n]a]g[  a}'^W--' hW ,^3^a,VD}na,t%-H a Qa9^   
 ,fi9a^ ,n'b'^F ,%fia9^t   ' t  '^b%a n-'bDpna,-fiH a Q
a9^   }'^ a}LF#a%n^a9a {Q'^t^H x'^D' D%-, hW ,^3^a V
Dpna,%-fi3 a ta2^   'a^-fi,pDa,'rH 'b^  9'^F ,%'^ t%ana
fi'b,^ a-% a%-'}',t}Da,,bD-='na^ ,D-,n  P  ,
'D-,^;,%-,W r' a'^Q'O'^F ,%W r'Dpna,-
  G    KFMLNQPSR \ ,^,t,
C _ o,@ % ,-,%% T U 
^
  	 B
 afiN fffifffiff)N   p  9'O t'^t, h  W,^3 , M 
  P ,D-=nnaJ'
U  '-,%WD}na=%-  KFMLNQPSR #=t t^a
,pDa,'t^'t, C _ o=@ 	 g,\^ n  G  P  
#%,nnaaJ r'%ap W'^#Dpna,-,W  KFMONQPSR D-=nnaJ^  *'
M U       fiN fffifffiffN   k k  ,

  U )  fiN fffifffiffNt  H      fiN fffifffiff)N  
'D aD'at
U 
,} J,=,D%--
  = , ' hW =^H^ta,'a V  !  G  b '^ n"  #
$ G o'`CFE GmJl  C3mp@BABE;A H  & % 	  fiN fffifffiffN   J% P D-,na{^' (bWp; pD-,nnaJ
h *  M +M    ,t^H^ta,VD}na,t%-
'*n^#f   )] ( OP ] t,t, P  ( ,
  G   ## 
  KFM NQPSR 
,=,,D%-,} nt %-JnH t#,,,t}Da,,t=,;9-%p'- 
-'	%^  KFMLNQPSR ; Z }}-,^}, {72-}'- ,=n^   ^ t^ 
, 9-%-, 9% -W,t},=Dabnt D=%^ b '^F ,%b'^ ta^
,
-%'-'\,t% Q%%a9a n-='#x%,^taJ,;%n  3^' t {'^F ,%Q,%a ,x t^ =na
/ .  D-=^n1 0}; *a {Qr' x, t 9'^F ,%^pnt D=;%k^ 9 = 'a nfi3 
'^F ,%#,^a},t%na%\=%=%WannQD%-,b9-na-',^a},na^}D-=^ 
%-=^* ,O' 3 2gaD =t# 'fi'^F ,%,^ t%a^#xDpr=anaJ=tO^ x,W
'a%,^'==aJ' #'*,ar'nt D*%,^taJ,;%n  3^^; na^#''afi9a,,
46587

fi9:;<>=@?&ABDCFEG:HIKJLBM:JLBDC3BMN8?'OQP/?>HSRLTU=SCWV:YXZBM[3\
]K^`_!a
bKcedf^gbihUjlkfimMhLj>noWprqsp tupwvfx
y	b"z|{}v
~v	~lpfi{}vY8vYpfififi8pfi{}v3vjSLj!&whLjfirmMLt'oW
/{q
_^#K{Wz_c_
 | ~Q vZ`mMk'USmM  jlF"njlUm&jlFS&Ug ~v1x
zf/bg{ ~W v
b/c`
{  
 |  v
zfbK^gb"zdf^ wUj"byLb^gb"zdf^g& j
bKc
mM'LSj!{KhLjlkfimMSmM'US3kjlhULSj&kfiM&Sg&ZSjfiSjlUmMSm j1SjfijL&w&fhLjfi&ZSLjfi&wmjl
SLjSjlwUmMSLjw&ujg&SLj'UjgU jlhmMSLjUS&Z&gLjfi&SjlMl)&S'MMw"Ml!mMZhUmMkfi jl
SUgSUj!SjlSUM)kfi&ZL&gj&jlLjfiw&MmMfijlhn&SSUmMLu  /xK &>&SLj!kfiM&SSjlUm'UjfigmMSLj
!&L "&UhejlM&eUmjfiwwkr>&
	Z6Z y	l
#r"3fi6w" /`&rs>fi3DK3woeprq!3Sq `U!lfi
g }u>3DguoS3&r ~F"fifi  Y   3fi   " f ~ pfififi8p 
S
  36r
 6&r>M 
&
 
	|fi ffg >'S`fifi!vff/lg1&1&K >r!t
 3">fi3
 ff3
  >M !   
  vi/ 6FM!
 L &"QL
r 8 go  q W v
 
]  l  gLj3hLW&SUjhLjlkfimMwm'sUS3kjlhULwjmM!'mM&jl|mMmM'LSj&UhmMS`k&SSjlkSUjlSUS&
)
mM!mUkfiMUhLjlhsmsSLjUwF&&ggLjfi&Sjl MljfiSj!KjU jSUjSLUZSkjlhZLSje#"'msmM'LSj
$ GgLjUS3kjlhULwj # "'`wUZ"mM|'3L'mM&SmMuj'|SUjSmMfij&o  q &% jlkfi&U jSLj
U`jfi&m jfiwSmM'UgmMSLj`SjfijlU>Sm
&emM'   !
o '  ( o  &Zh&'mMkfi&
k'Z jlFUjlUkj
 jl S)
 mSZS&'SmSm'U&* &wkfiM&U jlkfi&jjfiS&rujlhm'3L'm&USmMuj&,j +3)
 KjUS@&j
SUSLj!US3kjlhULwj# "&kfi&Mjlh mSw'UujlFS"noWprqsp % xwjfiSLwU!ZgmM&Uh'U mSLjfiSj
mMg&,j + jlUwm'e &.   oWprq)SUkrSU %0/2143 n5p xr)gLj!k&wSjlkSLjlSUS&&Zh SLj
&&&wmMSU SjuZ& jlh ' SLjumMhUjlSZ hLjfiwm&jSLjuUSjfiwjlFUmSm jl&KhLjfi&USmM % 	hLjfi&US
mM 
o 6 % mS Ljfi'SmM&j`k'UkfiMUwm' Sj`L&gUjfijlhLjlhf
mwS7
 KjSL8 SU5 "&SjfiSLrU wLjm&Zhs'UsmSLjfiSjm!&G,j +3 jlUSm'|7
 '&9
 :'
!
o 'prquwUkwSU %;/
13 n7
 '5p :'xr
 LjfiSje!
o ' % 8 < { w=
 	 o  Y 	
G
q
p

'
D
{




& 	 

<
 >uq  8 @  < { @  @ 	 % mM mMUk'ZSmM  jlF&Zh % B
  ASLjlSLjfiwjGSjGL
% pQmM 'mk'?
j,+3 jlUSm'UKSU)UZ % 3&UhUjlUkj/mMKmMk&SSjlk) SjfiSLw"&}lK&SSUjq 8 @  < { @  @ 	
 C q s8 @  < { @  @ 	|o ' um/k'ZSmM  jlFFQhL,j EUmMSm'&)o ' 
% mMk'USmM  jlFfi"8 SLju jfiD
/>,j + jlUwm' 7
 '&F :'YSSmMG EZjl
 ' / aYHn CxrU&Zhjlkfi&USj&YhUjfi&UMSm!
o 'Sj!U&w&3L
Ljfi'Sm'Q&7
 I Z SJm EkfiSm'e&hLjfi&ZmMQK
o '
mmM#aHn Cxr/gLjfiSjfi&Sj,j +3&kSMSL' jhLjfi&US
Sj!UMmjlhmM,j + jlZSm'U)&L  ' &)
 ZmMkwSLjUwjfiSjlFZmMSm jmMhLjfiwmM6ZMj&F >mM  w&mM'> &# )wh
  SLM SUSLjUS3kjlhULSjk'uZL jlSLjuUZm'W &q &UhQSLju jfi&k'UkfiMUSmM'U&KSLj
UUmFLjuSZkwSjfi`&hLjfi&USfi&UhLjlZkj|anx!mM!SUjUUmMLj,j +3 jlUSm'&)
  ' mMU&M&SLj
US3kjlhULwjSjfiSLwUg&MSj`m&Uh'ZmM <	Q
  &/ 'uj < { @  @ 	 % gUmMmMjlFZm6&Mjl>g SLj
&kgSUO
 N 	  143 n5p  ' x& 'u
j N	 % U&UhUjlUkj % /P
 143 n5p  ' xr

QSRUT

fiVXWZY\[M]^Y`_aY
bdce7fhgdi-jkc9gml,n\oprq#pMsut-vxwyv#z4{
|r}mz
~ ` wB8 dS&-z: 
k gc9g  jhc9  prl
tK*  z-8d 5-t^y
~ w&v#fi^^y
~ zvuop  qraoa
  w
c9gbdg 
    
}9ecgdf  dS&t  i!e
|r}m7   k g2  28F
gdOi
jk  |r  7
|r}m~ XpralS&z k gc9g  jhc9  prl
ggc9g  jkc9qr5\l
gdOi
oZa\#ldp#```#,l  \#l  lUop#o  `#,l  \#l
9 l K lp    q   qq  l#l4op  l,nqrl  p#oZ  7fi:p#  q   q)zPsu7uv5:{.oZ   `
Zo Lq  l5lDop  l,nqrl  p5oZ  ?p5`  q   q4z4suKv5K{x 9 lG Z oZ#  oZ#lU,q#oZ  oZ#lU,q 
    pd#a 9 l5lU?:o  suXlUoZqrlUhU{lU  `prl4t  t ` top  5  LOp#p5`lq   q
q  l#l:op  l,nqrl  p#o  ?dp#`  q   q4z4suv5K{x4qOopprqr  o  qr    qrp    q   q
op  l,nqrl  p#o  74suv5!{xvxw0xOl,nq  l!#lUMl  l    q#p  oZq   l  q#oZl,    p#oZ  p
q   q  5lo  t\q  q-o  z  o ` `,q#o  #  oZq 2X l#lU ` q  lyo  `,q#oZ 
  q  lUp#op.     fF^su M sH8 dS&zkr d 54suv5K{xvu&op  qraokDw{r{p    p
q   qD    fF^sH8 dSz4rP d 5Pt-vuop  qrao&w{7op  l,nqrl  p5oZ  
4suKv5K{  t!vxw0  z=:su v84suv5K{  t!vxwr{x  9 l#lU
o  su9lUoqrlU
U{x*q  l#l7op  l,nqrl  p5oZ    L  p#`  q   qXzP4su  v5  {x
9 l#l5lq  l`#,l  \5lD5lq#\  pu7oZ `  ` oZdq  l#lKop  l,nqrl  p#oZ  7ut-vxw
p#`  q   qXzP4suKv5K{x

9 lqr  ,q  o  oZq  #lUp#  qop#l  qrl  qr:q  lqr  ,q  o  oZq  fi`  l#l  pr  o  Dq  l7p  l4  p#p
p Xp   X   lU  !o  su  `qrl    kU8{x
^^J7*h k	
fiff^
9 l`5loZapp#lU,q#oZ  #lUprqrxo,q#pqrq  lyprlUo    prl   l#l&`5o5oZq#oZlUp  #l  prqr5o,qqrq   
 X  ll89!o  prq  ,lo  #l`#lUp#l  q#o  yo   l5oq  ,l  lq   p)q  
 l5o    q  l  l    q#p
 l    q#p)pralq#olUp  ll  qr   l  !l   sXo  ,a      lM{L`xoZ5oZq  " Ol  ,l7q  lap#p5oZo  oq 
9qr  ,q    lo  l5l  ,l  oq   lUp#p#lUprqrxo,qrl  5oZ5oZq#olUpopXo  qrl#lUprq 9 l!,a  l,n\oZq  #lUp5  q#p
X  #lUp#qr5o,qrl  `5o5oZq#oZlUp  #lp5`  5ol  o      l    #o lo  l    oZl9p#lU,q#oZ  p`5ll#l  ,lUp)qr
q  l#lUKp  5laol  o      %
l $ 9 l#lUp#  q#pdo  q  l4`#loZa`pdprlU,q#oZ  p  p  l * p#lUp#  q#pd  q  l
,a  l,noZq    `q#oZa`p95l  pr  o  su  \qrD l    hU8{  oZ#lU,q Z o  q  lo  qr  ,q  ^o  oZq 
    p5prlUp)F#l  pr  o    oZq   #oqr    5oZ5oZq#olUplU  `p#lq  l75l)q    5l  prlUo 
  prl)q  l  qrqrlU'
 &  p#prlUp7  l    q7q  l5oZlUp   o  q  lKqr  ,q  ^o  oZq  \lUp#q#oZ  #lU  o  p
l - #lq  l7`5l#!l op#oZqr(l #ll  x    p#p#lUp ` q  l  x        p#p  oq   oZqrl  pd`
`#l5!l `op#oZqr(l #ll  x   `   q  l5olUp  oZq  )   oZqrl    `prlUp#l  p#  o   op!qr  ,q    l\q
q  l5lU  o  o    p#prlUp  5lp#+ *oZl  q  l,n`#lUp#p#olqrl  ,  l#ap#oZq#oZ ` p  q#o-p ,  o  oZq  q
.0/1.

fi24357698;:=<>@?AB3CEDGFH>I3FH>@?+>IJ:LK'MN:9COHPQ8?SRT3U>IV+W

X1YIZ=[[\=]"^Q_1]Z=`QYIabacH_1\=dfe#_![
z
x







z1
z z
=
z!x
z 
z 

X(\LgihY#_(jHe#alknmocH_!pqX1YrZ=`Q[_![eIptsuZvd_
w \=dfp
x;ylYIeIa_1dfZ=Y
z)ylYIe#a_1d{Z=Y
!7fi!-
f!7}9
}G}9!-
!7fi!-
f!7}9
}G}9!-
!7fi!-
f!7}9
}G}9!-
!7fi!-
f!7}9
}G}9!-
!7fi!-
f!7}9
}G}9!-
!7fi!-
f!7}9
}G}9!-
!7fi!-
f!7}9
9i4-L(
!7fi!-
f!7}9
}G}9!-
!7fi!-
f!7}9
}G}9!-
!7fi!-
f!7}9
}G}9!-
!7fi!-
f!7}9
}G}9!-
!7fi!-
f!7}9
}G}9!-
!7fi!-
f!7}9
"~
!7fi!-
f!7}9
"~

^QeI[}|`QpX(ae#\Lp+y~]d_1_
`QpQZvdfk
^QeI[}|`QpX(ae#\Lp+y~]d_1_\=df^H_1d_!^
\=df^H_1df_!^`QpQZvdk
^QeI[}|`QpX(ae#\Lp+y~]d_1_pH\=dfgZ=Y
w \=dfp
pH\=dfgZ=Yfi`QpQZvdk
hQd_1df_!`Qer[e#a_(y~]d_1_
hQd_1df_!`Qer[e#a_(y~]d_1_\=df^H_1d_!^
hQd_1df_!`Qer[e#a_(y~]d_1_`QpQZvdk
hQd_1df_!`Qer[e#a_(y~]d_1_\=df^H_1d_!^`pQZvdk
hQd_1df_!`Qer[e#a_(y~]d_1_pH\=dfgZ=Y
hQd_1df_!`Qer[e#a_(y~]d_1_pH\=dfgZ=Yfi`pQZvdk
hQd_1df_!`Qer[e#a_(y~]d_1_h\L[fe#ae#=_%pH\=dfgZ=Y`QpQZvdk

 ZvY#_ 4 \LgihY#_(jHe#alkn\=]acH_X(\LpQ[_!`H_!pQX(_d_!YIZvae#\Lp moe#acqZvdfe#adfZvdkhQdfe#\=d{e#ae#_![
d_1]_1df_!pQX(_
w \=dfp
x;ylYIe#a_1d{Z=Y












z 
z 
z 
z 
z 
z 
z 
z 
z 
z 
z 
z 
z 
z 
   z!x
   z!x

X1YIZ=[[\=]"^Q_1]Z=`QYIabacH_1\=dfe#_![
z
x







z1
z z
=
z!x
z 
z 

^QeI[}|`QpX(ae#\Lp+y~]d_1_
`QpQZvdfk
^QeI[}|`QpX(ae#\Lp+y~]d_1_\=df^H_1d_!^
\=df^H_1df_!^`QpQZvdk
^QeI[}|`QpX(ae#\Lp+y~]d_1_pH\=dfgZ=Y
w \=dfp
pH\=dfgZ=Yfi`QpQZvdk
hQd_1df_!`Qer[e#a_(y~]d_1_
hQd_1df_!`Qer[e#a_(y~]d_1_\=df^H_1d_!^
hQd_1df_!`Qer[e#a_(y~]d_1_`QpQZvdk
hQd_1df_!`Qer[e#a_(y~]d_1_\=df^H_1d_!^`pQZvdk
hQd_1df_!`Qer[e#a_(y~]d_1_pH\=dfgZ=Y
hQd_1df_!`Qer[e#a_(y~]d_1_pH\=dfgZ=Yfi`pQZvdk
hQd_1df_!`Qer[e#a_(y~]d_1_h\L[fe#ae#=_%pH\=dfgZ=Y`QpQZvdk

)z ylYIe#a_1dfZ=Y






o x 
z=z 
z=z 
z=z 
  z 
  z 
o x 
 z 

 ZvY#_  7 _1]_1d_!pQX(_![oa\acH_1\=df_!g[b\LpqacH_X(\LgihY#_(jHe#alkn\=]o me#acqZvde#ad{ZvdkhQd{e#\=dfe#ae#_![
a`Hdfp[\L`QaNacZvaacH_EadfZ=X(aZveIYIeIalk\=]4Y#_(jHeIX(\==dfZvhcQerXhQd{e#\=dfe#ae#1_!^'^H_1]}Z=`QY#aY#\=LeIXEX(\LeIpQX1er^H_![Nmoe#ac'acH_
adfZ=X(aZverYIe#alk\=]  _!e#a_1d![^Q_1]Z=`QYIa
Y#\=LerXt]\=dZ=YIY`HaE\LpH_X1YIZ=[[  cH_1df_tgZ!k[aeIYIY4_q^Qe@_1d_!pQX(_![
eIp'acH_iX(\LgihYI_(j+e#ak\=]GacH_ieIp9ad{Z=X(aZvY#_X1YIZ=[[_![1]\=d_(jHZ=gihYI_  _!e#a_1d![%^H_1]}Z=`QY#aY#\=LeIXEX(\L`QYr^S_EeIp
X(\vylNZ=p^Y#_(jHeIX(\==dfZvhcQerXhQdfe#\=dfeIae#1_!^q^H_1]}Z=`QY#aY#\=LeIXX(\L`YI^q_ {ylcQZvdf^  s_cZ!=_pQ\=aoZ=pQZ=Y#k1_!^
acH_erp9adfZ=X(aZvY#_%X1YIZ=[[_![oeIpqgi\=d_^Q_1aZ=eIY 
0

fio#H;QL


0rQ!b{HS!@vfS-9(=@Hv)(}S{q"+(q
B1!-






l1-vvf=H{)-q9}1v+q-t+1v) 
"+(
n }l(v !
 f')})}


v

(

}



v

T

v



9

(



=




v



i

I

1


{

%





}

l

(



v




fi


ff
(

)

}













{

v

B

f

%

9

v



%




v



v



=
v}%=
	
	

+)
 ;E-o v
  v  
!-1!
"!#!$&%
'(*)+%-,fi./%
"10*$(324'5765'(6}S{8  
=B#1-
#DC46 9E F!,*!
GIHJ'(n L
 KL65'(M'57)N7,O'(6P%-,Q657RI07,*;<

:9 *4;<#!$($(7;<'(,*76(6#!=>'(?!
"!#!$&%
'(*)@%-6A!6



#!=/'(W!6(6&0*)YX*'(%
#1,Z6#!=/'(WX*$(J[\%
#10*6];J!6576 9

98S ,T7!;UV;J!653CF)N7GQ0*65W'(*3,J"1M'(%-#1,*6

.#!$?'(P^Z$&6('A=B#10$86('(M'57)Y7,O'(6?'(PXZ$(##!=/%-6_%
`!P'(

X*$(#I#!=a#!=a*J#!$(7)cb 9 2!24!6d,#PX*$U%
#!$&%
'(%
76/M$(4%-,O[!#1-[!7e 9gfI9/h
o

=B#!$3!-A65'5$&%_;<'W'5#!'(!A#!$&eJ$&6

t5K#!$&e*J$&%-,"


wI[I%-#10*6(
G! 

o

ot9uE

6(0*;Uq'(ZM'sr

#mCni po

fI9 2 9

%-6P%_,'(*M'<z\'57,*6(%
#1,!,*e};<#1,*6(7R07,'(
G%-,V!-/KX*$&J=J$($&7e*q<z\'57,*6(%-#1,*6 9N

})Q

9WI9PS ,{'(3$(7)N!%_,*%-,"v;J!65Y,#!'Fi  Yi  9P

'5#!'(!>#!$UeJ$&%-,"F#1,6(0*;U'(*M'r
o

!,ZeVi  Bo

i j9dh

 !,*eT'(J$(J=B#!$(e#I76,#!'8;<#1,'(!%-,

7,*;<

7,*;<F'(J$(Y%-6?v6('5$&%-;<'

,uM$("10*)Y7,'86(%-)fi%--M$'5#N'(P#1,%-,y'(

<z\'57,*6(%
#1,QC8%
'(Q'(t5K#!$&eJ$U%-,"

X*$(J[\%
#10*6?;J!65Y6(#mC86?'(*M'?'(J$(Y%-6?tKXZ$(J=J$&$(7e 

i  

v#!w*'(!%-,x'(y0*,*%-RI0N<z\'57,*6(%
#1,xC8%
'(q'(

wG{'(N!
"!#!$&%-'(*)|"1%
[!7,}%-,V./%
"10*$(fi~y!,*eX*$(#m[!7,q;<#!$($&7;<'W%-,J#!$(7)

%
'%-6?;<#!$($(7;<'P'5#y$(J'(0$&,

;<#1,'(!%-,*6

6(6(0Z)Y]'(*M'i  Yi  j9lk

o

'(*M'

7,*;<%-'%_6;<#!$($(7;<''5#fi$(J'(0$&,i1v1 9

j9g

*J$(J=#!$&P'(!
"!#!$&%
'(Z)$(J'(0$&,*6A'5$&0*%
=!,*ey#1,Z
Gt%
=  %_6]%-,!-KX*$(J=BJ$($(7eZ<zI'57,Z6(%
#1,*6]#!=



9
E

-7M$&
G!'(W!-"!#!$&%
'(*)$&0Z,*6]%-,yX#1
G\,#1)N%-!'(%-)Y 9



%
'(#10'X*$&%
#!$U%
'(%
76JO;J!0*'(%
#10*6d!,Zefiw*$U7[!?$(7!65#1,*%-,*"=#!$,#!$&)fi!Z0*,*M$&G3'(*J#!$&%
76A!,*eQ2KL_%
'5J$&!

;J-!0*6(76?%_68'5$&!;<'(Mw
ypW!0'5HN\7-)N!,27!\2D 9E

Y6(#DC'(*M'X*$U%
#!$&%
'(%
768%-,*;<$(7!653'(3<zIXZ$(76K

6(%
[\%
':Gt6(0\v;J%
7,O'(-Gt'5#fi)fiM`!'(*%-6];J-!6&6%-,'5$&!;<'(MwZ
 9


0rff+ 	  91#1 -l))}'"+!+(tS}l(v  f(#v=~ vY 	 }!()  (!
l(Q{vH-y
  "+(  v)i=+vn9(=@+=Uqv r)})} 	 v)}v"v-(
=
9(v+@tv
  f!
   v
 1
!-1xTX*$&##!=%-6fiwG$(7eZ0*;<'(%
#1,=B$(#1)X*$(#!X#16(%
'(%-#1,*!]6(M'(%-6^aMwZ%--%
':Gx'5#x'(;<#1)YXZ
7)Y7,'fi#!=
'({X*$(#!wZ-7) 9 J'T
 Dm!J JJ( Ww{x65J'y#!=XZ$(#!X#16(%
'(%
#1,*!8;J-!0*6576y!,*es'({6(J'v#!=


X*$(#!X#16(%-'(%
#1,*!][MM$&%-MwZ-76Y#I;J;J0*$($&%-,"x%-,

9 J'ywa{!,%_,7;<'(%-[!u=p0*,*;<'(%
#1,'(ZM'v)NMX6fi7!;U

;J-!0*6(vu'5#VQX*$(#!X#16(%
'(%-#1,*!g[MM$&%-MwZ-fi
'(J#!$(G



}&L!,*eX*$&%-#!$&%
'(%
76W#1,

p7W6&0*;&'(*M'F
 

!6F%-,q'(*vXZ$(##!=8#!=8J#!$(7)b

9V

<^,v'(eJ=p!0*
'

9 2 fI9qE

T;J-!%-)'(*M'

'(W6(J'8#!=l;J-!0*6576c%_66(M'(%-6^aMwZ
%
=/!,*e#1,*-Gy%-="   (= !J'(*M'8%-6JZ'(J$(W%-64YKX*$(J=BJ$($(7eZ

<z\'57,*6(%
#1,#!
= '(*M'te#I76fi,*#!'fi;<#1,O'(!%_B
, 1v ! 9S ,'(uX*$&##!=?Cu$(J=J$t'5#q'(Q;<#1,*6(%-65'57,Z;<G#!=
<z\'57,*6(%
#1,*6#!=G}S{lC8Z%-;&t%-6gwIGN'(?;<#1,*6(%-65'57,Z;<GN#!=!,*efi'(?=B!;<''(*M'eJ=p!0*
'(6d%_,*D[!
(0*65'(%^;JM'(%-#1,*6 


h

#!$(#1-_M$(G

I9

wGy7%
'5J$327!~MO5 9

6(6&0*)Y'(*M't'(J$(T%-6Nq)Y#Ie*7]

KX*$(J=BJ$($(7eZu<z\'57,*6(%
#1,Q#!= 
 6(0*;&u'(*M'

6&0*;&'(ZM'fi

1v1WQ
 
 xb(
 

 s

9E

u6&#DC'(*M'N'(J$(Q%-6N

:5Qv 
 >5Z {
b(
  >]}/ qb(
  >V 
 >8V7 YN5
 	
 pJ&?V   F
N5+p7& 9NS '%-6P65'5$&!%-"1O'5=B#!$(CM$&eV'5#[!J$&%
=BG{'(*M'%-6P!,}<zI'57,*6&%
#1,V#!=}&
 L 9fi J' o w
o
!,Gu65'5$&%-;<'4'5#!'(!/#!$&eJ$?#1, 6(0*;&Q'(*M'?r
!,*eQ=#!$P!-gm!&
 DYr v
 ?!,*e{!-dm!& DYr Y
 M
 o   %-=gMX*XZ(3!,*e,#!'MX*XZ  (3 9WE F6&#DC'(*M' o
%_64}S&
 L{5 K#!$&eJ$&%-,*"=#!$P 9 J'
wF!,Gu<z\'57,*6(%-#1,Q#!=}&L?6&0*;&'(*M''(J$&F%_6v 
 6(0*;U'(*M'PMX*XZ( ( F 9WE N6(#mC
'(*M''(*J$(W%-6  6&0*;&'(ZM'8MX*XZ  (
 (    9
h 6(6(0*)N'(*M'?3   9k #mC
 i  =#!$?65#1)Y3-%
'5J$&!  !,Ze  QW>!,*e  Q 9] 7;J!0*653
%-6];<#1,*6&%-65'57,'J  T
   9d 7,*;<WMX*Xji   (
 (    9d Ge<^,*%-'(%
#1,{i  o i :9
7

9A J'4

fiAdOm!}F-\-D1{?O(*(QaZ-\

 	ff
fi fi  fi  "! #!fi $ %
'&"&(*)+,+-.  -. 0/ %2134 65 7%  ! 85 fi #! :9ff-. <=
"5 '&"&(*)+>+-.  -. / % / %  13
; % 
4?  "! #!fi @,+-.A-B+C D	.0
E>+-..A-.B
0fi fi F GH4 65 I G%<J9 '&"&(*)K L	 / %<M13
4 65 Nfi '&&(*)+>+-.  -. 0/ %21<O  "! #!fi P Q
5 Nfi '&"&(S)K L	 / %13UT "5
; %R9
-. V%
'&"&(*)KA-.L-. / %1374 65 P%W! 85 fi #! :9X-. V
'&"&(*)K+-BL-. / % / %<M13
; %
4?  "! #!fi G+-BL-.CF>+-B  -.  
8Y>[Z,\Z]_^F( '3(D@ `; %a9 '&&(*)K+-.L-. / % / %  1 b+-.c-..dI,[Z,\Z]?eX#fifi


0fi E0[Z,\Zb! #! R!( '6
fg[Z  \Z  ]T "5 '&&(*)0Z]+-hZ  -iZ / % / %  13]4?  "! #!Dfi @Z]+-hZ  -iZ  dRjZ  \Z  
eH	fifi
h
fi f=Z]+-hZcA-hZcc! 	! a!( '
f=Zc k:l'm+n:op3k l'm+n:o6]T 5 '&"&.(*)KjZcA\Zc / %< / %1 '&"&(*)Kq-iZcA-hZc / % / %@13H4ff
 ! #!Dfi r+-hZ  -hZ  d@Z] k lmsn6o33k:l'm+n:o6
((
U t : :
0fi `dv! =w /	xzy3/ Cf{*fi :	! "|}
0fi7% ~%! Cf{
t ! u
&" :
 :#  u #!Dfi 7fi
_w /	xzy 
)*1
8%! GCf{*&" :
 :#  u #!Dfi fi
 "5
k l'm+n:o ~
; %aU Nfi\
: ! fi 6(? "5
2 ` 
fi 6(  !D =%a9]
0fi
((. @ <@S /:::6/ M `5:( ! bE Nfi < \8 fZ}Y)J613
4 65 8k l'm+n:o 
; %a9 Nfi '&&(*)0Z   k:l'm+n:op3k lmsn6o / %13P4 65 $-Bk:l'm+n:o@! Nfi @! 5 fi "5:( #!Dfi fi

:
 "( ! G9LZc ; %a<4 65 Nfi '&"&.(*)K[ZcD\Zc / %2139h-iZc %R<T 5 '&&(*)0Z]+-hZc-hZc / %13
T "5 Z b%Rf4 65 %! 5 fi 	! :9 '&&(S)JsZ,\Z / %21_
fi fi  Y /   2 : _ b@fi
-. \K
FP>9 7 
I %aa <%  
; % 65 fi
 
0fiz((Dfi\! N|N
h )J%Vr)J$-.i  $X$*  UX7-.*  $1Jff-..M13H _!  fi Nfi\
%  ! u #!Dfi Pfi
?w   /	xzy ff4? t :fi Os! `)J 6! :96'1 : E! u 	!Dfi
%<.fi
?w /	xzy "5 _E)J%< / w PL /	xzy 1F~fa)J%@ / w /	xzy 13?fi\ '&"&(*)+>+-..D-.. / %@ / %1
: @! Nfi
"5
'&"&(*) / % / %  1 \d>+-.  -.  
0fi fi 	!M5 fi (>fi :
d "5 _CVda t ! I5 fi  "!5 <Cf{*&" :
 :#
#$fi
]%R9
5 <! _
5
_ 
; %a7T "5 RO  "! #!Dfi  ,K
I7  9 5:( '	(D  V%aT "5 
 ! #!Dfi P -.> t : :
fi E =H4 65
"! Nfiz( ?
0fi P5:( < r9" _
?((*Nv(ZMfi}nYy!*



N 1N

A J!*5

  


-

  




1

n

!*

!*7 <y

1



?((*N?(ZM4u

!*u

8 1Y

A J!*5y !


n

1Z -557

}  


!Zq

  


1

 J!Z5

u



-

A J!*5

u

*

/!Zx7 <t !3

1* -(57O



47 <

!Zq

1

?((*N(*M?

4


7

- -





!*

-

?((*NA(ZMd

87 <

?

-] -

3 a 
 1

-

?((*N8(*M

87 <?

g!*t

  
 1

 -N *!*5(v

( B



(7  I57Z 1
-3Q



>J

x

47 <



_{

-!*5

!*

_t

-

VN&

1* -557 



 J!*5

u

p





( &y(ZM

p

 -

47 <

  
 1

 
5

(

}

J



-A7! v N(

(*M

-!v I57*

1



(7**7(

4 1N5

< !(

Z!*T7 <


Z5

(7

A J!*5( -

g

( (

]!*u
( B

_* 1

4 (

-

V  - 1

 &

t

F?((*YY(ZM

p

<((*

1

87 <F

j

(7



1O U!

_}!

-



N

-4 tF|( UT(*M?

@( &T(*M

>J

- !

Q

NY

Vy(



(*M

P 1Y

(



I* d!



_

u

-!y \57* 1



-

& &x(*M

? v(y

A J!*5N !P

A J!*(

J!5t(*M

&

A J!*5

>(7

!*u( (

p

 \57* 1

( Uq(ZM



x

p

(7

-fi

/ !W




( B

_W

\

F!

A J!*5

y p! 


(

p

W?((*Yt(*M

(*M3( (

 (

-*

-7

8!



-!Z5

U

P(

87 <

?

i`.>z>g*Sz*N"cXSGrL>ffX
 ' :
t  65 : !P)6z1! 	fi 5 V&"	!Dfi	! #!D :
 "( r(Dfi|z!5 ! #! a!( ' fi
4? :_  a)61f(Dfi|z!5' t u! "5 fi
X&" :
 :# u #!Dfi ! "fi | ' ! G| : (S9
 2
0fi Nfi ( :
 "( ! ! : $ 6(! : 
fi "fi ( :
 "( :fi	! :9 75 fi &( uO{
! fi
  ' :
t  65 : !X(Dfi|z!55 fiz! "5:! <! P5 fi a&( uN! `fi
_4? :_  (Dfi|z!5'
 6(D|
85 N)6z1&"
 #( #!Dfi f
	fi &"	!fi	! #!D :
 "( :fi3! fi N&"	!A{
fi	! #! :
 "( :fi	! :9 #fi
u #!Dfi Xfi
  "( #! "| :fi	! ?fi : &"	!fi	! #! :
t 6!  #( #!fi b5  & :#
0fi b! `&Bfiz( Nfi R! ( #! P4 5:5 '
 N		! (*r)6z1@&"
P "fi( "| 2 :&"
#!Dfi ( N| '|
u E(fi|z!52&"#fi| ! U&"3!Dfi	! #! 69B5:( #!5 (
:| #!Dfi 9 "! 0 "5 #!fi c t :5:( !
I 6 E #fi "! N|
0fi 6!DI( N| '| <! ?! | : (
7! Nfi "! J 5 #!Dfi $5:( #!M5 ( :| #!Dfi $! _! I&Lfiz(D Nfi a! ( #! 
i {5 fi &(
4? :_ UX! :2)6z1&"
Nfi #!Dfi Ufi
h.oSk:o:3#o#Gl  npffo 8n:o:0nF
0fi u
G(Dfi|z!5
&"#fi| : t 6!D  "! #!Dfi "!D :#| 2
0#fi '3(! :2fffi#b!
: }! fi (?fi :	! N|
fi  "( : 6` afi fi R&" :
 :#
 : :9 &" :
 :#
 :
 ( !*

(

M

Z

&*

*



N

3

&N  p! 
(




 ( q!*

U*




](

?

&!**d!*


 J7t p! 
A(
P &!* -M

I


7 !

1

&

1

-J5W!*

(

Mfi!Z

U!N

1V(



( B



- J7n p! 


(7{ \57* 1Z


 !v(*M

*

-

N

-*7

(7(7O8 &!* _M 1

-7 I 3(ZM( \57* 1Z

J!





(757(M 1

 1M 1 *!* - (* <


I* <x

V

U*!





-

-O

-(57 <

&Y7

_

1

1

*

- -@(*M

&7( 
 - (

- / -Y

-5

& !

-!(




A

J B

&!N

(7! 1 -

J > 1M 1

](

-

-




(


7

F

!

!7

(Y(*!

1

1

7

1

M







7> 8Z

 N(


 
7

(757

-(


 
7

-! 1* !

\ 1

_

-!&

_

J

!7* &

-  -Y

4 \57*7




_q(*M3 !7

( B

(7!* A W5J !*

\

_ T

!7* &

lJ

(757(3 ! 1

*  
 1q

_


7 (


 J7 p! 
(

-! 1Z !3(ZM? \57**

-( 1 - 5* < 1t!Z

U 
7( (Nfi


(}(

-{ -

1Z &!O5J7

&N  p! 
3(

1

(

I 1

x(*M

_? !


(u !(

( B

&*

-

(7V!Z  5J(

fiDN\."z
NU"@DF6< N#E087z6b"#3:6?"#ff:@#:#8NNP?#:_'$"ffD:@N\
"'6#MNN76B:3"D}MD:	FY?"#:0:##6Y"?:R:#a`"#3v 	

fiff "'	YN6YN7"6'#7#D}	N:#6"6D#D''#Dz0R#O:NG"#:0:##6
"?:8#:#@f"'86'	D:< #8F"	D	#D6fbDz#	a<bbN:J"D<zN
Nff".O?@"3:D6hMN6#6O2"67 ff 	M:DD]HDz'6N"	D3D#:6N:J"D
Dz:ff"'I'#fi6zPN#6a z"! #D""."D#DzR 6"#Dz"$#JFN:&%('z)""N:+*
,+--.0/ ?#:_'1* ,+--324/65 '#:7%8>9+":&: O;* ,+--<= *6NFN>RN<D N 	'")_"		D#:6
N:J"DfDz2"#6UM@?O+ #z .  ff 	:DA:@L2'	'"	6r0zD\:CB:CDEL
GF$*"	::#	6 O6#DzYIHKJML!N8O2"QPSRUTWVYX=N:J"Da'P[\Z DRQN6]D a
FC^_*"#:0:##6P 6"#DzP`HKJba7cYPSRUTWVYXedL!N8OffIfFC^BP"'gFC^3hM#KJjiJ =lk FX?#:_'
"mX:$N'P"r	D	D#:6VN:J"DDz`6#Dz'6 Mn?+ #Dz . ODzM'6"
"	M:DDo'\ff::Y* ff 3:D&?INIDz'67OP:Dz#6DR#6M'67"	D3D#:6PN:0DIDz
"'#: 6'":'#zD$Nz4N:6'p#Kq#N6r* ,+---= s ":0"PSRUTWVYXMutwvKx+vyzvt7{|
DSD~}k PWTH&N:#:0#DEENEr6N:	]"I:"'<D N 	'N"."D#z"
"#:0:##6N6#?ffOz' ff 	:DD34'?::+*?AL6D:"'?#:"ffD:I?"
N::#"6D6#g."IN".D#DzPi"#:0:##6 6"#Dz"& ":3D#D
fio B>:7J k cYR_"V	LR4V36LR_W4VWdb"N k c3YdzB:L`}#	 P#
	N:8zJ !b"'R_"V	RU1V3RU4V3RW4VW"RN:J"DEN:#QHKJL!N8O$"8?
 6"#Dz"+*DC k |6#cY6L!Yd = "D& k |6#c3W6L!Yd = g&NE O6#DzMDAN:06'#6RV	E"
RW4VW6*"D  ":6'#uRU1V3.X?+:"D  N:06'#_NADN6I"3D	DRN:J"D6R"V	fD 
NO6_N*"zDD  _u*"#:0:##6 6"#Dz7HKJL!N8O3

 L6D:`"'$MV aD*fff	:_nXD:$?"~FD  ]Lr"z"Du
"#:0:##6$ O6"	DzL+:"<"@z"D7 z1.) >L:?:6R1V3"RW1V*7N@	D	D
fiRU1V3rC"D":6uKE:6aE"'E"IN6##Dz'A""Naa6"N?"	D	#D6M:#6
&N:	_N:0D#]"'X'	D6#HD#+ #D2N:	D'	D>"A"N:]H+0"""3D	DoB>  3'"
 z'3z<z6>zN?6""4*:#@"'N3zNFEN,"Dz*YN,ff	:_"uffD:i:6
aFRDzN#E"u"DN:I	D	DK67i"N#6#	N?#:_'a"ffD:+j> z :	M
N:#"N:N".D#Dz">"#:0:##62 6"#Dz*zDW "L?""#6#6O@N"#""		D#D6
"'E"	O p"6"N6G#6"D#6Kff#6#	 #DzrUN:J"D#D:	h"#:	+"M#D6f"
 z:#Dz"FIa"*""> "M7BENz"<#I#+0"D	"8"'RVrFA^RU^Vr^.&"6N::
 k  ^  ff 	D3D#D6>"6U7	"	'#NaN:	D# EN:ff	OR"3D	D#:6N:J"DDzfK".
"F	+"#66u#Kq#N6r* ,+---= 

S4`	;e`
 "P#666}rN	NY"D#8N7 zD NDbN	::		Dz"2_"		D#:6
N:J"D>z	*\zO"I#6"D#"'>M XN6?Dz:,ENffBzONzR	"D:	'!*"8"D06N
N0N6#DzL	 #	MD8:	"]3 #	D@0X# #):D2#6	 6:#6]LN:J"D
N:	6:
"fR#6"#ff. fNE"#Lz#D#z"''	#FhN#:E6N:	L0	aM6'#Dz"F,"	
	D#:6N:0Df#6z""4*NRDz):$`?#:_'# ,+--32= "?":@"'z"N:p# ,+--.=
"'_'#u6z7N<#6a z"! #DEN"."#DzP, 6"#Dz*N"$0	a6'#z'
6z7  3'"$ z'3zr*zNED\ff::6F,"LzDNzaM6":	'lsIN
	#,ffE0	a6'#Dz"o:Dz6D<	66u.Dff6!N:+*DhM,N]"#"		NI"'iLzDNza#
3+

fi6Sw66p`4641Yfiw4f1
9	`+4&4+p+I4+9`!I+	W"1W&4&	9
14Io9	C9G~+9+Uu4KA	I@I)+	r
	9+"4"	"6	99G9++"4+"19+4p+69
+9+U``I14I)4pK4`49	""4;4)4
+
 1o4A""4o4C4++Ig791w")+99"9++
Y4$+	S`p0A49&0+IM+1YS1I+	w!4+w6+W4 +"
+gM4u	+9"	)w9!"	)4u+ 9
>W4u4+6+I m+`	&974u4K&M
4+4A	`9"	`"4&& &!9+S	 9!69+
+94)ff
 
	1Up)ff
 
g4Ifi
 &)4YUAf04W 44
I+	7!+4Kg+94p	&+>"1)"	$"6	9r14
g1+79!"	A+44u	+ M>
4+&&44 9"r`+Ie&4 )9+ 
r  4+	9	M+$	)Y+r)GW4C+94C9!"	I
&4+Q4I	fiM
 $9! C]M4!+	9
9!


"!$#&%(')*,+.-/*102*(#4365

)r+9+	!uge$	+A4	47 +419 8fi: $4e+!4u$64+C	
4;8<fi=8fib&WK4+AC4>1>
ff?9A@0CBfi+4!4;04++!	
G!	9)G14Y&+44+44Cg4
 o)ff
 D>4 	
4E W  +ul1+"A F99&	&4+f+"I4p)g`f
GA44C4>40I>G
 1g>449ICg7&H 1"$
+99!"44	I@4+"+C9+	9+`@0+"ff
 I0 
J +>"
 0+ 3	C"u7;+4
KML=NOL=PAL/QCR1L/S

g4+	CTU4+7
fi+9?! J !+Q4Kp 09+S4+
		A9+	4f+6Cu9Iwe	WV1XZY1[]\W^Z_=XO`7abYHcXZd^9cfehg
i ej^kGXZ\=l\,m
 nAooY!W pr q(I9s0

gu
 	t +	+> v4u Fl$ Bwxt 	$ yz T|{$		/ 4t z v4S+9 ?! }Wc[]Y=~cY1[O^9_XZdW_:eh9lc!09 	4

)r

+1f S)A>
 TBfi+9+lA+9 I! BfiA+9)4S	 ab[]cl C~l^Z_
r\Wcfe__l:m,eG\W~rez
 g !o 6q6 ?	1

	1.
 {S+9 s! J +G44!+G @"19+4+o!`A4Kfi+3
44C y  	[OX6~rehehgZl\,mZkXO`c1ewnn9cr\/cfeG[r\W^AclXZ\4^Z_>V1XZl\/cXZ\`e[je\W~]eXZ\ab[cl C~Gl^Z_4r\/cfeG_
_l:m,e\4~re"rZ
 p6q6Z psBfi9/ Mu4 J 464!

	1
 {1+ p!  >+9+6fi9$4Ke	A y G )rA Dfi J +	!"
B/
 T J +11 Fl S+!= zXjm9l~El\ a[]cl C~Gl^Z_/r\/cfeG__lm(e\W~]e[X~rerejgZl\,m9kXO`c1e/c





fi:H"W=A

 1Or=jZZr6h1rA7& GHH(99<zjHj9;<jjWG<91O,<<:9=9,=$
	
,(9Z 9j4/CHA<9=u/(/h<H9GGr<ZH
;jGZ1	w:CU7<GCO99ArjGGjj9/CGGj91=H:9A/j99h9GO
 A=uEbw:1h(HGjG:1/Z/:jH4  E7/GrE]W     4
   	 ff
,
fiWj]
 
 ZZ
 fijZZ=
 
;	O]h	 Z
 
Z 1
 W r
 f]W
 ZW  Zff
 jW]
  fi

 !#"/=zH 6   &jH/j9% $9
 &9jA99H9= CH/<<hHGhG
=GGZHjh(';:;$GAH9;E:*)=<<H>.O99Ar,+<.-j==j%/9 9hHGhfi<9A<0 j9jG9=
1=jhj% /9Hj   A=$Wu :z1r,=4GhG47W:u H=Z/:jHu4  .7/Grz]W   
 1W

   2 ff
(3
 fi WGG
 
 Z Z
 4fih9=
 
	O6rh	 Z
 
Z  515/%6rfGr49W 
 9ff
 GGGWr4
 . fi  6
 !#""=/$    &j,H/ j9% $9
 &9jA9E9H9= 7HW<<jHGrG
 h=9=$C(8O9  r  A/<<Hh9A=<HM<j9=ZAHw=jGG
1
/<,w=G9=<;:9AZ&Orr2Z
Z1:9;rf]WZW  19  ZffjW]9
< =CG  rf . % 
,GWrGH=$$     " Z9>
 $A1?
 7Z/9u@
 &9jA9E9H9= 7HW<<jHGrG

+b:9h9==97=&:;

+b6<HHA';:1

9<<<G7H8EO9  rzz<=Zj<9:99r:j=97jj<HEj=jZj<O4Z/<<:B$
9>=h94Ah:j:A=9C89h 9h/<Z9	19(]W  b$	
9E2
9DEDw
9F9&Ar 9A(   

j=Gh<H9Au+A >O9  rG'H9h9<%H<H=A=AH9A=j9A=H$1Gb# =;G  rf JI
 % 
,4r
 K9&O6r    (
L C7jhZjM/9A<=3KWArW/Z9 9L(
(/9$w4:j9 E:w
H=9:EwO Z
 j<GjZW:29= =:j
  rON jH A/<1:B$ 9
A(/:P$RQ/6 =j9/<GS1T H
 Z1]W    DWU
AV;.  r   1 1 (
b#WuHGX8E:1 .ZhU7HO9  r  A/=:j:A=91,j9<<Y0>=hHA<Hf;wZ==jA9rH7wHG9=:
h9A=<HHC< =CG  rf . %
,GWrGZ;K.  Ar Z  (9 
b9j:9zb O9  r  A/:H:P$ jj/:j9
 
 D4X6Z/\Ar9  L(

=A=AH9A=:9A<G

19(]W  z2
9

89jP$9?7H['C>O9  rb(A=:jj=G9h:9=A=AH9A=<=HGr:j9=97OZ=/ff$9C+

Z[

&2:

8999G  ?7H:u])9/=Au7=47=GrZ^Z_h$2
A< =CG  rf . 
(W]

9
 $2 
A	O	
9DED 
a`Z@DZW9=bfijGZ=
Z[dc&W]#Z4fih9=
94A/(
/$$99      <Zj==A 7jjGNb(9hu
9HH9a8E:	 1<9u	bO916re8bZh
r
 f . % 
,GWrG//.O]Ar   (A (

=j9/:9j</:HG9=:jHG9h:G

j,[&ffA >O9  rbHGr9<%HZj:A=9fNb=
 
 D4X
 fG4GWrG&Uh  r.9  9(

b< =C 

j=EA%$1HA<9u=<GhZhh$95gz1j9ffr 

z<jr=:H91 zO9LAr  A/=j<HwG:hG=hh:=j:A$$ 
 7AAj=1z7urW	O]h	Z
Z H6
r
 f]WZW  1Z  ZffjW]Z< =CG  r f i %
,4r;=$     A $A=9:G
&9jA99H9= 7=/<<j=GhG
&ZjGW4

 A: zh/ H# HY$G/j (

 &ff>O99Ark`Z@DZW6Z= z2
9l
GZ=
9(=h<=9GGh<ZH=;Gh<u
p>qlr

 Zf	6 Inm

 H[ofij I

fisStuovw>xyz{|}t~EfUztUz{Xzlx6x~2U@w2{t[zX

l22U

ff]	Y@32#%@2%6i	O:U6UP6U6@2ff6UU< fff

P#.J%YY<B6C6ff
2(ff6Rff32YF#6@	UY	2%6a6U6@6U6E%6Ek< fYZPYii
#<#llU
%ffffPX
 [%?X6YVFff6:FRYffkffff226(2U1Sffn.6@@Uff@22%
3Uffff52Yff}UYffoGoJSff2.M.U6U.aPY[
 ffYYYbaX<iXR	2EEi6X@l61S6@?Y3U
(2ff	Y
ffYff@ff%6k.GUY@%2ff6UUf# SY[PYi#<#6V<B6?Xff
G2UffVUoff4o	%	%2YffUYff363}Y2F%VVff	#Fb%F
%VY2ff%	XX(f@YJU26k.R# SYXPYi%#[ffSff2.

.X@.X<U2

	>	<fZ ffUaU[3ff#2U	:Gffa22[YCffff@#@@l
(fU6@@	@YFY	U

G2UffUff6VU#	@	%%2%ffFUY@%f%6< fffUP#.#<#[
	6Z6
X
6Zoff6RY@2ff2ff2@M@Y.@2
a	k%G

26UYfiff

ffff@#ffG2	ff@@#ff

kf@YJa ff<		i

@		%PbaYCff%

.XPY6[

 ffYYY

 .Y <5b 2lB[#Y#n2Y@.V@?l:	U
22Y2YC26U3@4fU	UY	Y
23?Uff

R

URb

UY@%2U#63%U%P,

ff2.3E.X.b  
 ffff#2#<5

2ff26@UdUY@%R%6

< fff?P#.#<#[@[66

S6622	@Y2ff[26:U.5@4oU2UYY
V?E:%(*2ffUfff@ff6S6	@#2!4UY@%E%6M@d2U#626UYiff	%%
	ff6@UU4aff@3?Vo?Jf ff<		iMR.X4
	.
k< fY[P#.J%YYU@l6"%ff@X6@#ff

$&%('

Y



XU	
6Y

 lYYY4

fiJournal of Artificial Intelligence Research 9 (1998) 367-421

Submitted 7/98; published 12/98

The Automatic Inference of State Invariants in TIM
Maria Fox
Derek Long

maria.fox@dur.ac.uk
d.p.long@dur.ac.uk

Department of Computer Science
University of Durham, UK

Abstract
As planning is applied to larger and richer domains the effort involved in constructing
domain descriptions increases and becomes a significant burden on the human application
designer. If general planners are to be applied successfully to large and complex domains
it is necessary to provide the domain designer with some assistance in building correctly
encoded domains. One way of doing this is to provide domain-independent techniques for
extracting, from a domain description, knowledge that is implicit in that description and
that can assist domain designers in debugging domain descriptions. This knowledge can
also be exploited to improve the performance of planners: several researchers have explored
the potential of state invariants in speeding up the performance of domain-independent
planners. In this paper we describe a process by which state invariants can be extracted
from the automatically inferred type structure of a domain. These techniques are being
developed for exploitation by stan, a Graphplan based planner that employs state analysis
techniques to enhance its performance.

1. Introduction
Stan (Long & Fox, in press) is a domain-independent planner based on the constraint
satisfaction technology of Graphplan (Blum & Furst, 1995). Its name is derived from the
fact that it performs a variety of pre-processing analyses (STate ANalyses) on the domain
description to which it is applied, that assist it in planning eciently in that domain. Stan
took part in the aips-98 planning competition, the first international competition in which
domain-independent planners were compared in terms of their performance on well-known
benchmark domains. Of the four planners that competed in the strips track, three were
based on the Graphplan (Blum & Furst, 1995) architecture. The most important difference between stan and the other Graphplan-based planners was its use of state analysis
techniques. Although these techniques were not, at that stage, fully integrated with the
planning algorithm stan gave an impressive performance as can be determined by examination of the competition results. There is a description of the competition, its objectives
and the results, at the aips-98 planning competition FTP site (see Appendix A).
One of the most important of the analyses performed by stan is the automatic inference
of state invariants. As will be described in this paper, state invariants are inferred from the
type structure of the domain that is itself automatically inferred, or enriched, by stan. The
techniques used are completely independent of the planning architecture, so can be isolated
in a pre-processing module that we call tim (Type Inference Module). Tim can be used
by any planner, regardless of whether it is based on Graphplan or on any other underlying
c 1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiFox & Long

architecture. Tim has been implemented in c++ and executables and examples of output
are available at our web site (see Appendix A) and in Online Appendix 1.
Tim takes a domain description in which no type information need be supplied and infers
a rich type structure from the functional relationships between objects in the domain. If type
information is supplied tim can exploit it as the foundation of the type structure and will
often infer an enriched type structure on this basis. State invariants can be extracted from
the way in which the inferred types are partitioned. The consequence is that the domain
designer is relieved of a considerable overhead in the description of the domain. Whilst it is
easy to hand-code both types and state invariants for simple domains containing few objects
and relations, it becomes progressively more dicult to ensure cross-consistency of handcoded invariants as domains become increasingly complex. Similarly, the exploitable type
structure of a domain may be much richer than can easily be provided by hand. We have
observed that tim often infers unexpected type partitions that increase the discrimination of
the type structure and provide corresponding benefits to stan's performance. We therefore
see tim as a domain engineering tool, helping to shift the burden of domain design from
the human to the automatic system.
The usefulness of both types and state invariants is well-documented. Types have been
provided by hand since it was first observed that they reduce the number of operator instantiations that have to be considered in the traversal of a planner's search space. The
elimination of meaningless instantiations is particularly helpful in a system such as Graphplan, in which the structure to be traversed is explicitly constructed prior to search. We
believe that the benefits to be obtained from type inference in planning are similar to those
obtained in programing language design: type inference is more powerful than type checking
and can assist in the identification of semantic errors in the specification of the relational
structure of the domain. Indeed, we have found tim to be a useful domain debugging
tool, allowing us to identify aws in some published benchmark domains. We also used
tim to reveal the underlying structure of the Mystery domain, a disguised transportation
problem domain, used in the planning competition. The Mystery domain is described in
Appendix C.2.
The use of domain knowledge can significantly improve the performance of planners,
as shown by a number of researchers. Gerevini and Schubert (1996a, 1996b) have considered the automatic inference of some state constraints and demonstrated that a significant
empirical advantage can be obtained from their use. Kautz and Selman (1998) have handcoded invariants and provided them as part of the domain description used by Blackbox.
They demonstrate the performance advantages obtained and acknowledge the importance of
inferring such invariants automatically. McCluskey and Porteous (1997) have also demonstrated the important role that hand-coded state invariants can play in domain compilation
for ecient planning. Earlier work by Kelleher and Cohn (1992) and Morris and Feldman
(1989) explores the automatic generation of some restricted invariant forms. We discuss
these, and other, related approaches in section 5.
In this paper we will describe the type inference process employed by tim and explain
how four different forms of state invariant can be extracted from the inferred type structure.
We will argue that tim is correct since it never infers sentences that are not state invariants.
We will then provide experimental results demonstrating the performance advantages that
can be obtained by the use of types.
368

fiAutomatic Inference of State Invariants

drive

unfuelled 1
fuelled

1

drive

at 1

load

in 1
at

1

unload

Figure 1: A simple transportation domain seen as a collection of FSMs.

2. The Type Inference Module
One way of viewing strips (Fikes & Nilsson, 1971) domains is as a collection of finite-state
machines (FSMs) with domain constants traversing the states within them. For example, in
a simple transportation domain there are rockets and packages, with rockets being capable
of being at locations and of moving, by driving, from being at one location to being at
another, and of being fuelled or unfuelled, and of moving between these two states. at can
be seen as forming a one-node FSM, and fuelled and unfuelled as forming a two-node FSM.
This view is depicted in Figure 1.
369

fiFox & Long

Packages can be at locations or in rockets, and can move between these states in the
resulting two-node FSM. In this example, rockets can be in states that involve more than
one FSM, since they can be both at and fuelled, or at and unfuelled. STRIPS domains
have been seen in this way in earlier work (McCluskey & Porteous, 1997; Grant, 1996), as
discussed in Section 5.

2.1 Types in TIM

When two objects participate in identical FSMs they are functionally equivalent and can be
seen to be of the same type. The notion of type here is similar to that of sorts in the work of
McCluskey and Porteous (1997). A primary objective of the tim module is to automatically
identify the equivalence classes that form the primitive types in a domain description and
to infer the hierarchical type structure of a domain on the basis of the primitive types. The
way this is done is discussed in Section 2.3. The primitive types are functional equivalence
classes, and the objects of the domain are partitioned into these classes. Having identified
the types of the domain objects tim infers the types of the parameters of all of the operators.
State invariants are inferred as a final stage.
The early parts of this process rely on three key abstract data types, the property space,
the attribute space and the transition rule. Formal definitions of these components are
provided in Section 2.3, but we provide informal descriptions here to support the following
definitions. Transition rules represent the state transformations that comprise the FSMs
traversed by the objects in the domain. Property spaces are FSMs, together with the objects
that participate in them, the properties these objects can have and the transition rules by
which they can acquire these properties. Attribute spaces contain collections of objects that
have, or can acquire, the associated attributes. Attributes differ from properties because
they can be acquired, or lost, without the associated loss, or acquisition (respectively),
of another attribute. Attribute spaces also contain the transition rules that enable the
acquisition (or loss) of these attributes. Once the state and attribute spaces have been
constructed we assign types to the domain objects according to their membership of the
property and attribute spaces. Any two objects that belong in identical property and
attribute spaces will be assigned the same type. It is therefore very important to ensure
that the property and attribute spaces are adequately discriminating, otherwise important
type distinctions can be lost. Much of the subtlety of the algorithm described in Section 2.2
is concerned with maintaining adequate discrimination in the construction of these spaces.
We present the following definitions here to support our informal characterisation of the
roles of types in strips and in tim. The definitions are used again in Sections 2.4 and 2.6,
which discuss how types are assigned to objects and operator parameters.

Definition 1 A type vector is a bit vector in which each bit corresponds to membership, or
otherwise, of a unique state or attribute space. The number of bits in the vector is always
equal to the number of distinct state and attribute spaces.

Definition 2 A type is a set of domain objects each associated with the same type vector.
Definition 3 A type vector, V1, in which two distinct bits, si and sj , are set corresponds to

a sub-type of the type associated with a vector, V2, in which only si is set (all other settings
370

fiAutomatic Inference of State Invariants

being equal). Then the type associated with V2 can be seen to be a super-type of the type
associated with V1 .

Definition 4 A type structure is a hierarchy of types organised by sub-type relationships
between the component types.

Definition 5 A type structure is adequately discriminating if objects are only assigned to
state (and attribute) spaces that characterize their state transitions (and attributes).

Definition 6 A type structure is under-discriminating if it fails to distinguish types that
are functionally distinct.

Definition 7 A type structure is over-discriminating if functionally identical objects are
assigned to different types.

There are two distinct ways in which types play a role in the specification of a domain.
They can restrict the set of possible operator instances to eliminate all those that are
meaningless in the domain and hence improve eciency by reducing the size of the search
space, and they can eliminate unsound plans that could be constructed if they were not
provided. The following examples clarify the difference between these two roles. The
untyped schema:

drive(X,Y,Z)
Pre:
Add:
Del:

at(X,Y), fuelled(X), location(Z)
at(X,Z), unfuelled(X)
at(X,Y), fuelled(X)

permits more instances than the typed schema:

drive(X,Y,Z)
params:
Pre:
Add:
Del:

X:rocket,Y:package,Z:location
at(X,Y), fuelled(X), location(Z)
at(X,Z), unfuelled(X)
at(X,Y), fuelled(X)

but all meaningless instances will be eliminated during search because their preconditions
will not be satisfiable. On the other hand, the typed schema:

y(X,Y,Z)
params:
Pre:
Add:
Del:

X:aircraft,Y,Z:location
at(X,Y)
at(X,Z)
at(X,Y)

ensures that only aircraft can be own, whilst the untyped schema:
371

fiFox & Long

y(X,Y,Z)
Pre:
Add:
Del:

at(X,Y)
at(X,Z)
at(X,Y)

allows ying as a means of travel for any object that can be at a location, including packages,
and other objects, as well as aircraft. Tim is capable of automatically inferring all types
playing the restrictive role indicated in the typed drive operator. However, tim cannot infer
type information that is not implicit in the domain description. Thus, given the untyped fly
schema, there are no grounds for tim to infer any type restrictions. Tim will draw attention
to unintended under-discrimination by making packages and aircraft indistinguishable at
the type level, unless there is distinguishing information provided in other schemas. At
the very least tim will make explicit the fact that packages are amongst those objects that
can y. This assists the domain designer in tracking errors and omissions in a domain
description, but unstated intended distinctions cannot be enforced by tim.

2.2 An Overview of the TIM Algorithm

Figure 2 gives a broad outline of the tim algorithm. A more detailed description is given
in Appendix B. The role of each component of the algorithm is described, together with a
commentary on discussing related issues and justifications, in Sections 2.3, 2.4 and 2.7.
Broadly, tim begins with an analysis of the domain operators, extracting transition rules
that form the foundations of the property and attribute spaces described previously. These
rules are used to separate properties into equivalence classes from which the property and
attribute spaces are constructed. Tim then analyses the initial state in order to assign the
domain objects to their appropriate spaces. This analysis also identifies the initial properties
of individual objects and uses them to form states of the objects in the property spaces. The
initial states in a property space are then extended by the application of the transition rules
in that space to form complete sets of states accounting for all of the states that objects in
that property space can possibly inhabit. As described in Section 2.4, attribute spaces do
not behave like FSMs, as property spaces do, and the extension of these is carried out by a
different procedure: one that can add new objects to these spaces, rather than new states.
Tim then assigns types to objects using the pattern of membership of the spaces it has
constructed. Finally, tim uses the spaces to determine invariants that govern the behaviour
of the domain and the objects in it.

2.3 Constructing the Transition Rules

We begin by describing the process by which the transition rules are constructed. The
following definitions are required.

Definition 8 A property is a predicate subscripted by a number between 1 and the arity of
that predicate. Every predicate of arity n defines n properties.

Definition 9 A transition rule is an expression of the form:
property ) property  ! property 
372

fiAutomatic Inference of State Invariants

Construct base PRSs (Section 2.3)
Split PRSs (Section 2.3)
Construct transition rules (Section 2.3)
Seed property and attribute spaces (Section 2.3)
Assign transition rules (Section 2.4)
Analyse initial state (Section 2.4)
Extend property spaces (Section 2.4)
Extend attribute spaces (Section 2.4)
Identify types (Section 2.6)
Construct invariants (Section 2.7)
Figure 2: Outline of the tim algorithm.
in which the three components are bags of zero or more properties called enablers, start and
finish, respectively.

The double arrow, ), is read enables and the single arrow, !, is read the transition
from. So:

E)S!F

is read: E enables the transition from S to F. The properties in S are given up as a result of
the transition. The properties in F are acquired as a result of the transition. The properties
in E are not given up.
If enablers is empty we write:

start ! finish
If start is empty we write:

Transition rule 1

enablers ) null ! finish
If finish is empty we write:

Transition rule 2

enablers ) start ! null
The bag null is the empty bag of properties. Its role is to emphasise that, in transition
rule 1, nothing is given up as a result of the transition and, in transition rule 2, nothing is
acquired. Rules that have a null start and a null finish are discarded because they describe
null transitions.
When the property bags contain more than one element they are separated by commas.
The collection:

pk ; qm ; ::: rn
373

fiFox & Long

is interpreted to mean that each of the properties in the collection can be satisfied as
many times as they appear in the collection. The comma is therefore used to separate the
elements of a bag. We use  to denote bag union, 	 to denote bag difference, 
 to denote
bag intersection and v to denote bag inclusion.

Definition 10 A Property Relating Structure (PRS) is a triple of bags of properties.
The first stage of the algorithm constructs a set of transition rules from a set of operator
schemas. Each operator schema is analysed with respect to each parameter in turn and, for
each parameter, a PRS is built. The first bag of properties is formed from the preconditions
of the schema, and the number used to form the property is the argument position of
the parameter being considered. For example, if the precondition is on(X; Y ), and the
parameter being considered is X , the property formed is on1 . This bag, called precs,
contains the enablers that will be used in the formation of the transition rules. The second
bag, called deleted precs, of properties is formed from all of the preconditions that appear
on the delete list of the schema (with respect to this same parameter). The third bag, called
add elements, contains the properties that can be formed from the add list of the schema.
The PRS contains no deleted elements component { it is assumed that every element on
the delete list of a strips operator appears in the precondition list. This is a reasonable
restriction given that strips operators do not allow the use of conditional effects. It is
further assumed that every pair of atoms on the delete list of a schema will be distinct for
all legal instantiations of the schema. This does not constitute a significant restriction since
operator schemas can always be easily rephrased whenever this condition is violated.
We now consider the process by which PRSs are constructed. Given the schema:

drive(X,Y,Z)
Pre:
Add:
Del:

at(X,Y), fuelled(X), location(Z)
at(X,Z), unfuelled(X)
at(X,Y), fuelled(X)

and considering the parameter X , the following PRS will be built:

PRS 1

precs :
at1 ; fuelled1
deleted precs : at1 ; fuelled1
add elements : at1 ; unfuelled1
By considering the parameter Y we obtain:

PRS 2
precs :
at2
deleted precs : at2
add elements :
and by considering the parameter Z we obtain:
374

fiAutomatic Inference of State Invariants

PRS 3

precs :
location1
deleted precs :
add elements : at2

In constructing these structures we are identifying the state transformations through which
the objects, instantiating the operator parameters, progress. Note that objects that instantiate X go from being fuelled and at somewhere to being unfuelled and at somewhere; objects
that instantiate Y lose the property of having anything at them and gain nothing as a result of application of this operator, and objects that instantiate Z continue being locations
and gain the property of having something at them. We now convert these structures into
transition rules in order to correctly capture these state transformations.
Our standard formula for the construction of rules from PRSs is:

precs 	 deleted precs ) deleted precs ! add elements
Thus, using the PRS 1 above, we could build the rule:

at1 ; fuelled1 ! at1 ; unfuelled1
A potential problem with this rule is that it causes at1 and fuelled1 to be linked in state
transformations, so that at1 and fuelled1 become associated with the same property space
and, as a consequence, objects that can be at places, but that cannot be fuelled, may be

indistinguishable from objects that require fuelling before they can be moved. In fact, we
wish the transition rules to express the fact that being fuelled enables things to go from
being at one place to being at another place, whilst not excluding the possibility that there
may be other enablers of this transition.
We therefore begin a second phase of PRS construction by identifying, for special treatment, PRSs in which a property appears in both the deleted precs and the add elements.
This is a property that is exchanged on application of the operator. That is, the relation
continues to hold between the identified argument and some other object or objects (not
necessarily the same object or objects as before the application of the operator). For example, in PRS 1, the vehicle is at a new location after application of the operator, and no
longer at the old location. We observe that the vehicle must be fuelled to make this transition. To separate the transition from this condition we split the PRS. Splitting identifies
the exchanged properties in a PRS and creates one new PRS for each exchange and one
for the unexchanged properties. Therefore, splitting a PRS always results in at most k + 1
(and at least k) new PRSs, where k is the number of exchanges that the PRS represents.
By splitting PRS 1 we construct two new PRSs: one characterizing the exchange of the at
property, and one characterising the fuelled to unfuelled transition.
The first of the new PRSs is:

PRS 4

precs :
at1 ; fuelled1
deleted precs : at1
add elements : at1
375

fiFox & Long

from which the rule

fuelled1 ) at1 ! at1
is constructed. It should be noted that the property of being fuelled is no longer seen as
part of the state transformation but only as an enabler, which is why it does not appear in
the deleted precs bag in the resulting PRS.
The second new PRS captures the fact that at1 can be seen as an enabler for the
transition from fuelled1 to unfuelled1:

PRS 5

precs :
at1 ; fuelled1
deleted precs : fuelled1
add elements : unfuelled1

In this PRS there are no further splits required since no other properties are exchanged in
it. A more general example is as follows:

PRS 6

precs :
p1; p2    pn
deleted precs : p1    pi pi+k    pm
add elements : p1    pi q1    qk

from which i PRSs would be constructed to deal with each of the i exchanged pairs and a
final PRS, PRS 7, would be constructed to describe the remainder of the transition making
i + 1 PRSs in total.

PRS 7

precs :
p1 ; p2    pn
deleted precs : pi+k    pm
add elements : q1    qk

There is no need to consider additional pairings of add and delete-list elements, since these
would not correspond to exchanges of properties. The splitting process is justified in Section 3.1. The standard rule construction formula can be applied to PRS 5, yielding the
rule
at1 ) fuelled1 ! unfuelled1
It should be observed that, even if the add elements bag contains multiple properties, a
single rule will always be built when the standard construction formula is applied.
On considering the remaining PRSs, 2 and 3, it can be observed that they each contain
an empty field: in 2 the add elements field is empty and in 3 the deleted precs field is empty.
When a PRS has an empty field special treatment is required. From PRS 2 we build the
rule

at2 ! null

to represent the fact that the object that instantiates Y gives up the property of having
something at it, and gains nothing in return. From 3 we build the rule

location1 ) null ! at2
376

fiAutomatic Inference of State Invariants

to represent the fact that the object that instantiates Z gains the property of having something at it by virtue of being a location, and gives up nothing in return. These rules have
a somewhat different status from the ones that characterize the exchange of properties. In
these cases properties are being lost or gained, without exchange, so can be seen as resources
that can be accumulated or spent by domain objects rather than as states through which
the domain objects pass. For example, a location can acquire the property of having something at it, without relinquishing anything in return, whereas an object that requires fuel
can only become fuelled by relinquishing the property of being unfuelled, and vice versa.
Increasing and decreasing resources are identified as attributes and are distinguished from
states. This distinction will later prove to be very important, since the generation of true
state invariants depends upon it being made correctly. Properties that can increase and
decrease without exchange are not invariant, and false assertions would be proposed as
invariants if they were treated in the same way as state-valued properties.
A rule of the form constructed from PRS 3 must be constructed separately for every
property in the add elements bag because these properties must be individually characterized as increasing resources. Rules constructed using null are distinguished as attribute
transition rules. If the null is on the left side of the ! the rule is an increasing attribute
transition rule. If the null is on the right hand side then the rule is a decreasing attribute
transition rule.
A final case to consider during rule construction is the case in which a PRS has an empty
precs field. This happens if the parameter, with respect to which the PRS was constructed,
did not appear in any of the preconditions of the operator schema. In this case a set of
rules is constructed, one for each property, a, in the add elements bag, of the form

null ! a
reecting the fact that a is an increasing resource (the deleted precs field will necessarily
also be empty in this case).

Definition 11 A state is a bag of properties.
When it is necessary to distinguish a bag from a set, square brackets will be used to denote
the bag.
Definition 12 A property space is a tuple of four components: a set of properties, a set
of transition rules, a set of states and a set of domain constants.

Definition 13 An attribute space is a tuple of three components: a set of properties, a set

of transition rules and a set of domain constants.

It is helpful to observe here that the state and attribute spaces represent disjoint collections of properties, and that these disjoint collections are formed from the transition
rules by putting the start and finish properties of each rule into the same collection. For
example, given two rules:
E1 ) [p1; p2; p3] ! [q1 ; q2]
and
E2 ) [r1; r2] ! [s1 ]
377

fiFox & Long

the collections [p1; p2; p3; q1; q2] and [r1; r2; s1] would be formed. If a property appears in
the start or finish of both rules then a single collection will be formed from the two rules.
The last stage in the rule construction phase is to identify the basis for the construction
of property and attribute spaces. This is done by uniting the left and right hand sides of the
rules. Uniting forms collections of properties that each seed a unique property or attribute
space. It is not yet possible to decide which of the seeds will form attribute spaces, so
treatment of both kinds of space is identical at this stage. The enablers of the rules are
ignored during this process. We do not wish to make enablers automatically fall into the
same property spaces as the states in the transformations they enable. This could result
in incorrect assignment of properties to property and attribute spaces since enablers only
facilitate, and do not participate in, state transformations. The output of this phase is the
collection of rules, with some properties marked as attributes, and the property space seeds
formed from the uniting process. All properties that remain unassigned at this stage are
used to seed separate attribute spaces, one for each such property.
The role played by the second phase of PRS construction is to postpone commitment
to the uniting of collections of properties so that the possibility of objects, which can have
these properties, being associated with different property spaces is left open for as long as
possible. It may be that consideration of other schemas provides enough information for
this possibility to be eliminated, as in the following abstract example, but we support as
much type discrimination as possible in the earlier phases of analysis. We consider this
simple example to illustrate the problem.
2.3.1 Postponing Property Space Amalgamation

Given a domain description containing the following operator schema:

op1(X,Y,Z)
Pre:
Add:
Del:

p(X,Y), q(X,Y)
p(X,Z), q(X,Z)
p(X,Y), q(X,Y)

the PRS:

precs :
p1 ; q1
deleted precs : p1 ; q1
add elements : p1; q1
will be constructed, during the first phase, for X . The properties p1 and q1 are bound
together in this PRS, and the resulting rule would be:

p1; q1 ! p1 ; q1
which forces objects that can have property p1 to occupy the same property space as objects
that can have property q1 . Since this PRS models the exchange of p1 we will split it, and
replace it with two new PRSs:

precs :
p1 ; q1
deleted precs : p1
add elements : p1
378

fiAutomatic Inference of State Invariants

precs :
p1 ; q1
deleted precs : q1
add elements : q1
We do not consider other pairings of p1 and q1 , since these will be found in the PRSs
of other operator schemas if the domain allows them. The two PRSs generated lead to the
generation of the rules:

q1 ) p1 ! p1

and

p1 ) q1 ! q1

The two rules indicate that p1 and q1 should be used to form different property spaces since
they could, in principle, be independent of one another. Then objects assigned to these two
spaces can turn out to be of distinct types. However, if we add the following two schemas:

op2(X,Y)
Pre:
Add:
Del:

op3(X,Y,Z)

q(X,Y)
p(X,Y)
q(X,Y)

Pre:
Add:
Del:

p(X,Y)
q(X,Y)
p(X,Y)

we generate, for X , the PRSs:

precs :
q1
deleted precs : q1
add elements : p1
and

precs :
p1
deleted precs : p1
add elements : q1
and the rules:
and

q1 ! p1
p1 ! q1

indicating that p1 and q1 should be united in the same set and hence form a single property
space, and that objects that can have these properties are really of the same type. The
uniting overrides the potential for separate property spaces to be formed but, in the absence
of these two schemas, there would have been insucient information available to determine
the nature of the relationship between the two properties.
379

fiFox & Long

2.4 Constructing the Property Spaces and Synthesising the Types

The objective of this stage is to construct the type structure of the domain by identifying
domain objects with distinct property spaces. Objects can appear in more than one property
space, giving us a basis for deriving a hierarchical type structure.
The first part of the process involves completing the seeded property spaces. The first
task is to associate transition rules with the appropriate property space seeds. This can be
easily done by picking an arbitrary property of the start or finish component of each rule
and identifying the property space seed to which that property belongs. There can never
be ambiguity because every property belongs to only one seed and uniting ensures that all
of the properties referred to in a rule belong to the same seed. At this point the distinction
between states and attributes becomes important. Any property space seed that has an
attribute transition rule associated with it becomes an attribute space and is dealt with
differently from property spaces in certain respects explained below.
The next step is to identify the domain objects associated with each property space and
attribute space.
For each object referred to in the initial state we construct a type vector in which a bit
is set if the corresponding space is inhabited by the object. An object can inhabit more
than one space. Habitation is checked for by identifying all of the properties that hold, in
the initial state, of the object being considered and allocating them as states, rather than
as properties, to the appropriate state and attribute spaces. When every domain object has
been considered a unique type identifier is associated with each of the different bit patterns.
The next task is to populate the property spaces with states. The following definitions
are required to support the explanation of this process.

Definition 14 A world-state is a collection of propositions characterising the configuration
of the objects in a given planning domain description.

Definition 15 Given a world-state, W , a property space, P = (Ps; TRs; Ss; Os), or an
attribute space, P = (Ps; TRs; Os), and an object o 2 Os, the P -projection of St for o is
the bag of properties, possessed by o in W , each of which belongs to Ps.

The collection of properties of an object, o, in the initial state can be divided into a set of
bags of properties, each bag corresponding to the P -projection of the initial state for o, for
some property or attribute space P . Each bag is added to the state set of the corresponding
property space, or discarded if the corresponding space is an attribute space. We now need
to extend the spaces by, for each property space, adding states that can be inferred as
reachable by objects within that space along transitions within that space. This is done for
every state in the space, including states that are newly added during this process, until no
further new states are reachable. The ordering of the properties within states is irrelevant, so
two states are considered equal if they contain the same properties, regardless of ordering
(they are considered order-equivalent). Since, when we come to use this information in
parts of the process of invariant generation, we will not require knowledge of any inclusion
relations between pairs of states, it is convenient to mark these at this stage. The addition
of reachable states is important for the inference of state invariants, and their use will be
discussed in Section 2.7. The attribute spaces receive different treatment at this point. The
380

fiAutomatic Inference of State Invariants

important difference to observe is that, since property spaces characterize the exchange
of properties, objects in a property space must start off in the initial state as members
of that property space. However, since attributes can be acquired without exchange, it
is possible for objects that do not have particular attributes in the initial state to acquire
those attributes later. This is only possible if the attribute space has an increasing attribute
transition rule associated with it. We now, therefore, consider each attribute space to see
whether further objects can be added by application of any corresponding increasing rule.
An object can be added to an attribute space if it potentiates all of the enablers of an
increasing rule in that attribute space. An object potentiates an enabling property if it
is a member of the state or attribute space to which that property belongs. Membership
of all of these spaces indicates that the object could enter a state in which it satisfies all
of the enabling properties, which would justify an application of the increasing rule. Any
enabling property that is not associated with a state or attribute space is a static condition,
so the initial state can be checked to confirm that the property is true of the object being
considered.
A complication arises if any enabling property was itself used to seed an attribute space
(in which case it is itself an attribute), because it is then necessary to identify all of the
objects in its attribute space and consider them for addition to the current attribute space.
Of course this could, in principle, initiate a loop in the process but we avoid this by marking
attribute spaces as they are considered and ensuring, by iterating until convergence, that
all of the attribute spaces in the loop are completely assigned. The correctness of this part
of the procedure is discussed in Section 3.
When this is done the state and attribute spaces are complete and the types of the
domain objects can be extracted. The completeness of this construction phase is discussed
in Section 3.1.

2.5 A Worked Example

A fully worked example of all stages of the process will help to clarify what is involved.
Consider a simplified version of the Rocket domain in which there are two operator schemas:

drive(X,Y,Z)
Pre:
Add:
Del:

load(X,Y,Z)

at(X,Y), fuelled(X), location(Z)
at(X,Z), unfuelled(X)
at(X,Y), fuelled(X)

Pre:
at(X,Y), at(Z,Y)
Add:
in(X,Z)
Del:
at(X,Y)
and an initial state containing four constants: rocket, package, London and Paris, and
the relations: at(rocket,Paris), fuelled(rocket) and at(package,London). It can be observed
that this simplified Rocket domain has the rather odd feature that the load schema is not
restricted to loading packages into rockets. This oddity will be highlighted by the analysis
that is constructed, showing how the analysis performed by tim can help in understanding
(and debugging) the behaviour of the domain. From the drive operator schema the following
PRSs are constructed for variables X , Y and Z respectively:
381

fiFox & Long

precs:
at1 , fuelled1
deleted precs: at1 , fuelled1
add elements: at1 , unfuelled1
precs:
at2
deleted precs: at2
add elements:
precs:
location1
deleted precs:
add elements: at2
From the load operator schema the following PRSs are constructed for variables X , Y
and Z respectively:
precs:
at1
deleted precs: at1
add elements: in1
precs:
at2 , at2
deleted precs: at2
add elements:
precs:
at1
deleted precs:
add elements: in2
and the following rules are built. The first PRS generates the first two rules and subsequent
PRSs each generate one rule.
fuelled1 ) at1 ! at1
at1 ) fuelled1 ! unfuelled1

at2 ! null
location1 ) null ! at2
at1 ! in1
at2 ) at2 ! null
at1 ) null ! in2

We now construct the following united sets of properties:

fat1; in1g
ffuelled1; unfuelled1g
fat2g
fin2g
382

fiAutomatic Inference of State Invariants

These are used to seed property spaces. We first associate the rules with these property
space seeds, resulting in the following assignment:

fat1; in1g
at1 ! in1 ; fuelled1 ) at1 ! at1
ffuelled1; unfuelled1g at1 ) fuelled1 ! unfuelled1
fat2g
location1 ) null ! at2 ; at2 ) at2 ! null;
at2 ! null
fin2g
at1 ) null ! in2
The last two spaces have been converted into attribute spaces by their association with
attribute transition rules. The resulting spaces can now be supplemented with domain
constants and their legal states. We first identify the subset of the legal states of the
domain objects that are identifiable from the initial state. We do not use the goal state
to provide further information about the properties of objects. The goal state might be
unachievable because objects cannot obtain the required properties. This would invalidate
tim's analysis of the domain. In the initial state the rocket has properties at1 and fuelled1,
the package has property at1 , London has property at2 and Paris has property at2 . Using
this information we associate domain constants with the developing state and attribute
spaces to obtain:

fat1; in1g
at1 ! in1; fuelled1 ) at1 ! at1
frocket; packageg
ffuelled1; unfuelled1g at1 ) fuelled1 ! unfuelled1
frocketg
fat2g
location1 ) null ! at2 ; at2 ) at2 ! null; fLondon; Parisg
at2 ! null
fin2g
at1 ) null ! in2
The next step is to add the legal states of these objects, which are identifiable so far, to
the property spaces. This results in the following structures, the first two of which can
be extended by inference (as will be explained) into completed property spaces. The last
two will be extended into completed attribute spaces by the addition of objects that can
potentially acquire the associated attributes (also described below).

fat1; in1g

at1 ! in1; fuelled1 ) at1 ! at1

ffuelled1; unfuelled1g at1 ) fuelled1 ! unfuelled1
fat2g
fin2g

frocket; packageg
[at1 ]
frocketg
[fuelled1]

location1 ) null ! at2 ; at2 ) at2 ! null; fLondon; Parisg
at2 ! null
at1 ) null ! in2

The last stage in the construction of the two property spaces is to add any states that
can be inferred as reachable, via transition rules, by objects in the property spaces. For
example, packages can go from being at1 to being in1, by application of the rule at1 ! in1,
and since that rule is available in the property space to which package belongs, and at1
is one of the legal states in that property space, we add in1 as a further legal state. In
general, we construct the extension by, for each state in the space, identifying applicable
rules and, for each rule, creating a new state by removing the properties in the start of the
383

fiFox & Long

rule and adding the properties in the finish of the rule. This is done until all further states
are order-equivalent to those already generated. The enablers of the rules are ignored, with
the consequence that some of the new states generated might be unreachable. When this
process is completed in the current example the finished property spaces are as follows:

Property space 1
fat1; in1g at1 ! in1; fuelled1 ) at1 ! at1 frocket; packageg
[at1]; [in1]

Property space 2
ffuelled1; unfuelled1g at1 ) fuelled1 ! unfuelled1 frocketg

[fuelled1 ]; [unfuelled1 ]

We now consider each attribute space in turn and add domain objects (not already
members) that potentiate their increasing rules. No new domain objects can be added to
the first attribute space since only London and Paris can potentiate the increasing rule,
and they are already present. However, when the second attribute space is considered it can
be observed that rocket and package both potentiate the increasing rule and are therefore
both added as new members. The resulting attribute spaces are:

fat2g location1 ) null ! at2; at2 ) at2 ! null; fLondon; Parisg
at2 ! null
fin2g at1 ) null ! in2
frocket; packageg
The oddity of the load operator is revealed at this stage, since both package and rocket
have been assigned as members of the in2 attribute space (meaning that they both can have
the attribute of having things in them).

The number of distinct bit patterns that are constructed, indicating object membership
of the state and attribute spaces, determines the number of distinct types that exist in the
domain. Hence, in this simplified encoding of the Rocket domain, there are three distinct
types. The rocket has type [1101], the package has type [1001] and Paris and London both
have type [0010]. These types are given abstract identifiers, T0; T1 and T2, but might be
more meaningfully interpreted as the types of: movable object requiring fuel, movable object
and location respectively. As expected, London and Paris are of type location, whilst the
package is of type movable object and the rocket is of type movable object requiring fuel,
which is a sub-type of movable object.
The distinction we have made between state and attribute spaces is further exploited in
the process of inferring state invariants, discussed in Section 2.7.

2.6 The Assignment of Types to Operator Parameters

Types are assigned to the parameters of the operators in the following way. Given an
operator schema and a collection of property spaces and attribute spaces we allocate a type
vector to each of the variables in the schema. The membership in the state and attribute
spaces of each of the properties of a given variable is recorded by setting the appropriate bits
in the vector for that variable. Only the properties that appear in the preconditions of the
384

fiAutomatic Inference of State Invariants

schema are considered, because any object that can satisfy the preconditions of an operator
can have the properties represented by the postconditions and is therefore of the right type
for instantiation of the operator. When a type is associated with the vector the union of all
of its sub-types is taken. This union is then the type assigned to the variable. Any domain
object, the type of which is a sub-type of the type associated with the variable, can then
be used to instantiate that variable. To see how this process works, consider the variable X
in the drive schema above. The precondition properties of X are: at1 , fuelled1. These are
members of the two property spaces 1 and 2. Therefore, the type vector associated with X
is [1100]. It can be observed that the type vector associated with the rocket is [1101], so that
the type of rocket is a sub-type of the type of X . This is the only sub-type, so the union of
sub-types contains only T0, the type of rocket. This means that X can be instantiated by
rocket, but not by any other domain constant, since no other domain constant has a type in
the appropriate sub-type relation. To type the operator parameters we introduce new type
variables, Tk ::Tn for unused values between k and n, where k is the number of existing types
and n is k plus the number of variables in the schema being considered. The type vector
for variable Y will be [0010] and Z will have no type vector because location is a static
relation and Z does not appear as an argument to any other predicate in the preconditions.
Z therefore acquires the same type as London and Paris, the only two objects for which
location is true in the initial state. T4 is a super-type of T2. After taking the unions of the
sub-types we can now specify the drive schema in the following way:

drive(X,Y,Z)

Params:
X:T0 Y:T2 Z:T2
Pre:
at(X,Y), fuelled(X), location(Z)
Add:
at(X,Z), unfuelled(X)
Del:
at(X,Y), fuelled(X)
stan exploits the sub-typing relations that have been inferred when constructing instances of the drive operator. Any variable that appears in a schema but does not appear
in its preconditions can be instantiated by objects of any type. This is because the domain
description contains no basis for inferring type restrictions in this case. No variable can
appear on the delete list without appearing on the precondition list, since we assume that
all delete list elements appear as preconditions. So such a variable would have to occur on
the add list. This would mean that, regardless of the properties holding of the object used
to instantiate that variable, in the initial state, it can acquire that add list property freely.
Since this acquisition would occur irrespective of the type of the object, such variables are
essentially polymorphic.

2.7 The Inference of State Invariants

The final phase of the computation of tim is the inference of the state invariants from
the property spaces. The attribute spaces are not used for the inference of invariants:
incorrect invariants would be proposed by tim if attribute spaces were inadvertantly used.
This explains the importance of identifying the attribute spaces in the earlier stages of the
algorithm.
The current version of tim is capable of inferring four kinds of invariant, three of which
are inferred from the property spaces (identity invariants, state membership invariants and
385

fiFox & Long

invariants characterizing uniqueness of state membership) and one of which is inferred from
the operator schemas and initial state directly (fixed resource invariants). In the simplified
Rocket domain, considered above, an example of an identity invariant is:

8x : Tk :8y:8z:(at(x; y) ^ at(x; z) ! x = z)
A state membership invariant is:

8x : Tk:(9y : Tn:at(x; y) _ 9y : Tm:in(x; y))
A uniqueness invariant is:

8x : Tk ::(9y : Tn:at(x; y) ^ 9y : Tm:in(x; y))
To infer the identity invariants each property space is considered in turn, with respect
to their properties and states. If a property, for example Pk with P of arity n > 1, occurs
at most once in any state an invariant of the following form, in which y and z are vectors
containing n , 1 values, can be constructed:

8x:8y:8z:(P (y1::k,1; x; yk::n,1) ^ P (z1::k,1; x; zk::n,1) ! y = z)
The form of this invariant can be generalised to deal with the case where there are at most
m > 1 occurrences of Pk in any state in the space. In this case we build the following
expression, in which we have assumed that k = 1, for simplicity.

8x:8y1:::ym:(P (x; y1) ^ ::: ^ P (x; ym) ! (y1 = y2 _ y1 = y3 _ ::: _ ym,1 = ym ))
The state membership invariants are of the form:

8x:(Disjunct1 _ :: _ Disjunctn)
where each disjunct is constructed from a single state. Thus, if a property space contains k
states there will be at most k disjuncts in the invariant constructed for that property space.
Only one state membership invariant is constructed for each property space.
Given the collection of states in a property space we first identify those that are supersets
of other states in the collection. All supersets are discarded, since the invariants that would
be built from them would be logically equivalent to those built from their subset states.
Each remaining state is used to build a single disjunct. If the state being considered contains
a single property, Pk with P of arity n, then the expression

9y:P (y1::k,1; x; yk::n,1)
is constructed. Of course, if n = 1 then there is no existential quantifier and the disjunct
is just P (x). If the state contains more than one property, say m of them denoted P 1 ::P m ,
then we build (again, assuming that k = 1 for simplicity):

9y1 :::ym:(P 1(x; y1) ^ P 2(x; y2) ^ ::: ^ P m (x; ym))
The uniqueness invariants are constructed in a similar way. For each property space we
begin by analysing the superset states to identify non-exclusive pairs of subset states. For
386

fiAutomatic Inference of State Invariants

example, given the subset states fat1 g and fin1g and the superset state fat1 ; in1g, it can
be observed that the two subset states are not mutually exclusive since at1 and in1 can be
simultaneously held. Having done this analysis and identified all mutually exclusive pairs
of states we mark the subset states as unusable for generation of invariants. The remaining
states are considered in all possible pairings. For every pair of states, P; Q, we generate
an invariant of the following form assuming, for simplicity, that x is in the first position in
P 1 ::P n and Q1 ::Qm. The form of the invariant is easily generalised, as before.

8x::(9y1:::yn:(P 1(x; y1) ^ P 2(x; y2) ^ ::: ^ P n (x; yn))
^(9y1:::ym:(Q1(x; y1) ^ Q2(x; y2) ^ ::: ^ Qm (x; ym))))
The fourth kind of invariant can be inferred from the structure of the operator schemas
without reference to the property spaces or domain type structure. We call these invariants
fixed resource invariants since they capture the physical limitations of the domain. Fixed
resource invariants cannot be inferred from the state and attribute spaces because they describe properties of the domain rather than of objects within it. The following schema from
the Gripper domain provides an example of why fixed resource invariants are distinguished
from the other three kinds:

move(X,Y)

Pre:
Add:
Del:

at robot(X), room(Y)
at robot(Y)
at robot(X)

The PRSs that would be built from this operator are:

precs :
at robot1
deleted precs : at robot1
add elements :
precs :
room1
deleted precs :
add elements : at robot1
and the rules constructed from these are:

at robot1 ! null
and

room1 ) null ! at robot1

It can be observed that both of these rules are attribute transition rules and that at robot1
is attribute rather than state-valued. This means that no invariants of the first three kinds
discussed would be constructed.
The reason for the lack of invariants of the first three forms is that the encoding of the
robot is embedded in a predicate, so the robot cannot participate directly in state transitions.
An obvious invariant of the robot, which would naturally be true of this domain, is that the
387

fiFox & Long

robot is always in exactly one room but this cannot be inferred using the techniques so far
described. In fact, this is an axiom about the world, or domain, rather than specific objects
within it, and has to be obtained from information other than the state transformations of
the objects.
It can be seen from the operator schemas for the Gripper domain that at robot1 is balanced. That is, it is always deleted whenever it is added and added whenever it is deleted.
This means that the number of occurrences of at robot in the initial state determines the
number of occurrences that are possible in any subsequent state. This leads to the construction, for this domain, of the invariant

jfx : at robot(x)gj = 1
since there is only one at robot relation in the initial state. The form of fixed resource
invariants is always equational. Such an invariant states that the size of the set of combinations of objects satisfying a certain predicate is equal (or, in some cases, less than or equal)
to a certain positive integer. Because this integer can be very large it is more convenient to
write an equation than it would be to write a logical expression. The information encoded
in the fixed resource invariants is very useful for identifying unsolvable goal sets without attempting to plan for them. For example, in the ICPARC version of the three-blocks Blocks
world (Liatsos & Richards, 1997), in which there are only three table positions, there must
always be exactly three clear surfaces. Any goal specifying more than three clear relationships can be identified as unachievable from the fixed-resource invariants for that domain.
The fixed-resource and uniqueness invariants produced by tim can be seen as providing a
form of multi-mutex relations, in contrast to the binary mutex relations inferred during the
construction of the plan graph in Graphplan-based planners (Blum & Furst, 1995). Binary
mutex relations indicate that two actions or facts are mutually incompatible, whilst multimutex relations indicate that larger groups of actions or facts are collectively incompatible.
Binary mutex relations, preventing a fact that can be true of only one object from holding
of two different objects simultaneously, can be extracted from the identity invariants that
tim infers. Multi-mutex relations are more powerful than binary ones. Stan can detect
unsolvable goal-sets by using the fixed-resource and uniqueness invariants even when the
binary mutex relations at the corresponding level do not indicate that any problem exists.
To infer these invariants we examine the predicates in the language to see whether
they are exchanged on the add and delete lists of the operator schemas. If a predicate is
exchanged equally in all schemas (it always appears the same number of times on the add
list as on the delete list of a schema) then the predicate corresponds to a fixed resource.
If a single schema upsets this balance then the predicate is not treated as fixed. Given a
fixed resource predicate, it can be inferred that there can never be more combinations of
objects satisfying that predicate than there are in the initial state. Because of the slightly
odd encoding of the rocket world considered in this paper, only location is a fixed resource.
at is not fixed because it is not equally exchanged in the load schema. Examples of fixed
resource invariants inferred from various standard domains are provided in Appendix C.
There are certain circumstances under which it is necessary to infer the weaker invariant
that
jfx : P (x)gj  k
388

fiAutomatic Inference of State Invariants

for some positive integer k. If P holds of multiple objects in the initial state then it is
possible for subsequent state transformations, or attribute acquisitions, to result in states
in which two or more instances of P collapse into one. If P holds multiply often in the initial
state (or in any other reachable state) then it is necessary to build the invariant using 
instead of =. If P is state-valued, and multiple instances never occur in any state in its
property space, then it is safe to assert equality in the construction of the invariant.
Automatic inference of the first three kinds of invariants relies on the construction
of the property spaces as discussed in Section 2.4. As has been discussed, the distinction
between state and attribute spaces is critical for the inference of correct invariants. However,
using just the techniques described so far, tim would lose information from which it could
construct useful invariants. To give an example of how this could occur we now consider
the following simple encoding of the standard Blocks world:

move(X,Y,Z)
Pre:
Add:
Del:

on(X,Y), clear(X), clear(Z)
on(X,Z), clear(Y), clear(table)
on(X,Y), clear(Z)

In this operator, used by Bundy et al. (1980), the add list element clear(table) makes
reference to a constant. If the operator schema were to be submitted to our analysis in its
current form no PRS would be built for the constant, so the rules that would be constructed,
and hence the state and attribute spaces constructed, would fail to record the fact that every
application of move results in a state in which the table is clear. The resulting analysis
would result in incorrect invariants and types. Grant (1996) identifies this version of the
move operator as awed, because of the need to maintain state correctness by the addition
of the invariant clear(table) to the add list. However, we can analyse this schema correctly
if we first abstract it to remove the constant, yielding the following new schema:

move(X,Y,Z,T)
Pre:
Add:
Del:

on(X,Y), clear(X), clear(Z), table(T)
on(X,Z), clear(Y), clear(T)
on(X,Y), clear(Z)

Now, given an initial state in which blockC is on blockA and blockB is on the table,
we add the proposition table(table) (so that the new precondition can be satisfied) and the
property and attribute spaces that are constructed are as follows:

fon1g

clear1 ) on1 ! on1

fblockA; blockB; blockC g
[on1 ]
fon2; clear1g on2 ! clear1; clear1 ! on2; fblockA; blockB; blockC; tableg
table1 ) null ! clear1

The second of these is an attribute space, so our invariant extraction algorithm is not
applied to it. Consequently, the only invariants we can infer are those that characterize
the positions of blocks (every block is on exactly one surface). This is a pity, as there
is information available in the attribute space that could yield useful extra invariants. In
particular, we would like to infer the invariant that every block can be either clear or have
389

fiFox & Long

something on it, but it cannot be both clear and have something on it. The reason we
cannot infer this as an invariant is because it would be asserted to hold for every object in
the attribute space, including the table, even though it is not actually true of the table (the
table can have things on it and still be clear).
2.7.1 Sub-space Analysis on Property and Attribute Spaces

The solution to the problem of loss of invariants is to decompose any property or attribute
space that contains k > 1 object types into k sub-spaces. A property sub-space is structurally identical to a property space. Attribute sub-spaces are identified but not used, as
no invariants can be obtained from them. Property sub-spaces can be obtained by analysis on attribute spaces, as the following example will show. The reason for distinguishing
sub-spaces from property and attribute spaces is that the properties are not partitioned
in sub-spaces as they are in the property and attribute spaces. The original property or
attribute space is not discarded and the sub-spaces are not used for determining the types
of objects. The only role of the sub-space analysis is to enable the construction of additional
invariants.
We now consider the Blocks domain described in the previous section as an example of
the benefits of sub-space analysis. At the point of invariant construction the types of the
domain objects have been identified by their property and attribute space membership, so
table is already known to be of a different type to that of the blocks. This is because table is
not a member of the property space for on1 . Therefore, two sub-spaces can be constructed
from the attribute space, one for the type [11], of blocks, and one for the type [01], of
tables. No sub-spaces can be constructed from the property space because it contains only
one type of object. The rules associated with the sub-spaces will be all of the rules from the
original attribute space that are enabled by objects of the appropriate type. The second
of the two sub-spaces is an attribute sub-space because of the inclusion of the increasing
attribute transition rule. At this stage the two sub-spaces are as follows:

fon2; clear1g on2 ! clear1; clear1 ! on2
fblockA; blockB; blockC g
fon2; clear1g table1 ) null ! clear1; on2 ! clear1; ftableg
clear1 ! on2
The attribute sub-space will not be used for invariant construction because it contains an
attribute transition rule and would result in incorrect invariants (as is the case for attribute
spaces), so there is nothing to be gained from developing it further. However, the state
sub-space is now completed by the addition of the states associated with the objects in the
space, both in the initial state and by extension. The resulting sub-spaces are:

fon2; clear1g on2 ! clear1; clear1 ! on2

fblockA; blockB; blockC g
[on2 ]; [clear1]
fon2; clear1g table1 ) null ! clear1; on2 ! clear1; ftableg
clear1 ! on2
From the new state sub-space we can infer the following invariants, using the type name
Block to stand for the type vector [11]. We infer the identity invariant:
8x : Block  (8y  8z  (on(y; x) ^ on(z; x) ! y = z))
390

fiAutomatic Inference of State Invariants

the state membership invariant:

8x : Block  (9y : Block  on(y; x) _ clear(x))
and the unique state invariant:

8x : Block  :(9y : Block  (on(y; x) ^ clear(x)))
Although there is an additional invariant, that the table is always clear, we cannot infer
this at present.

2.8 The Problem of Mixed Spaces

It can happen that the encoding of a domain conceals the presence of attributes within
schemas until the point at which property space extension occurs. This can prevent the
property space extension process from terminating. For example, a simple lightswitch domain contains the following two schemas:

switchon(X)
Pre:
Add:
Del:

switchoff(X)
Pre:
Add:
Del:

off(X)
on(X), touched(X)
off(X)
on(X)
off(X), touched(X)
on(X)

and an initial state in which switchA is on. Two PRSs are constructed:
precs :
off1
deleted precs : off1
add elements : on1 ; touched1

precs :
on1
deleted precs : on1
add elements : off1 ; touched1
giving rise to two rules:
and

off1 ! on1 ; touched1

on1 ! off1; touched1

Uniting then seeds one property space containing all three properties. After addition of the
rules the property space is as follows:

fon1; off1; touched1g off1 ! on1; touched1; fswitchAg
on1 ! off1 ; touched1 [on1 ]
391

fiFox & Long

It is at the point of extension of the space that the problem arises. The following states
are added: [off1; touched1], [on1; touched1; touched1 ], [off1; touched1; touched1; touched1 ] and
so on. We cannot simply avoid adding properties that are already in the state being extended because the two, apparently identical, properties might in general refer to different
arguments.
The problem here is due to the fact that touched1 is actually an increasing attribute
but this does not become apparent in the PRSs. The consequence is that mixed spaces are
constructed. A mixed space is a property space containing hidden attributes. Tim detects
hidden attributes by checking, on extension, that no new state contains a state already
generated from the same initial state starting point. Thus, on extension of the mixed space
above, tim would detect the hidden attribute when the state [on1 ; touched1; touched1] is
constructed, because this state contains the state [on1 ] that initiated this extension.
Having detected the hidden attribute there are two possibilities: either tim can convert
the mixed space into an attribute space, in which case no invariants will be constructed, or
it can attempt to identify the attribute and split the mixed space into an attribute space
and a property space containing the state-valued components of the mixed space. We take
this option and split the state. This allows us to infer invariants concerning the state-valued
properties.
tim takes the difference between the including and included states and, for each distinct
property in the difference, processes the rules by cutting any rule containing that property
into two rules, at least one of which will be an attribute rule. The following method is
used to cut the rules. In the following, attr+ indicates one or more occurrences of the
attribute-valued property and the comma is overloaded to mean both bag conjunction and
bag union. If the rule is of the form:

enablers ) start ! adds; attr+
then the two new rules will be of the forms:

enablers; start ) null ! attr+
and
If the rule is of the form:

enablers ) start ! adds
enablers ) attr+ ; precs ! adds

then the two new rules are of the forms:

enablers; precs ) attr ! null
and

enablers; attr ) precs ! adds

The rule cutting separates the attribute-valued properties from the state-valued properties.
Now pure attribute and property spaces can be constructed. However we do not discard
the original mixed space because it has been used in determining the type structure of
392

fiAutomatic Inference of State Invariants

the domain. Any additional type information that could be extracted from the state and
attribute spaces built following this analysis is not currently exploited.
When this analysis is applied to the lightswitch domain, the following new property
space and attribute space are built:
fon1; off1g off1 ! on1; on1 ! off1
fswitchAg
[on1 ]; [off1]
ftouched1g off1 ) null ! touched1; on1 ) null ! touched1 fswitchAg
Using Lightswitch to stand for the type [11], the following state membership invariant
can be constructed from the property space:
8x : Lightswitch  (on(x) _ off(x))
tim also constructs the uniqueness invariant:
8x : Lightswitch  :(on(x) ^ off(x))

3. Properties of TIM

The correctness of tim relies on it constructing only necessarily true invariants. The demonstration that only true invariants are constructed guarantees the construction of an adequately discriminating type structure. We cannot guarantee against under-discrimination
but we argue that over-discrimination does not occur in the type structures generated by
tim. These properties were defined in Section 2.1.
Over-discrimination would be the result of distinguishing functionally identical objects
at the type level. This would occur if tim placed objects that participate in identical
state transitions in different property spaces but, because of the underlying partitioning of
properties between property spaces, this cannot happen. Further, membership of different
property spaces requires that there be distinguishing state transformations, which there
are not in functionally identical objects. Flawed assignment (assigning an object to a
property space without its corresponding state transformations), should simply be seen as
erroneous, rather than as over-discrimination. The possibility of this occurring can be
excluded because property and attribute space construction and extension are shown to be
correct in Section 3.1.
A failure to detect type differences (under-discrimination) in the domain will result
in weak invariants, and over-discrimination, if it could occur, would lead to over-targeted
invariants that would still be true, but only for a subset of the objects they ought to
cover. Flawed assignment would clearly lead to the construction of false invariants. Underdiscrimination, which can arise, therefore affects the completeness of the state-invariant
inference procedure. It can also lead to over-generalisation of the operators since the types
assigned to the operator parameters will be equally under-discriminating. This can enable meaningless instances to be formed, needlessly increasing the size of the search space
that must be explored by the planner. This clearly raises eciency issues but it does not
undermine the formal properties of the planner that exploits tim.
As observed, the consequence of under-discrimination is the construction of weak (but
valid) invariants. The following example illustrates how under-discrimination can occur.
Given a schema:
393

fiFox & Long

op(X,Y)
Pre:
Add:
Del:
and an initial state in which

p(X,Y)
q(X,Y)
p(X,Y)

p(a; c); p(b; c); q(b; d)

hold, the following two property spaces are constructed:

fp1; q1g p1 ! q1 fa; bg
[p1]; [q1]; [p1; q1 ]; [q1; q1 ]
fp2; q2g p2 ! q2 fc; dg
[q2 ]; [p2; p2 ]; [q2; p2 ]; [q2; q2 ]
Given these property spaces it is impossible to distinguish a from b or c from d, even
though analysis of the operator schema and initial state reveal that a is functionally distinct
from b and c from d. It can be seen that, although a must always exchange a p1 for a q1 ,
b can have both p1 and q1 simultaneously. A similar observation can be made for c and
d. However, the process by which invariants are constructed cannot gain access to this
information. An identity invariant constructed for the first property space is:

8x : T  8y  8z  8u  (q(x; y) ^ q(x; z) ^ q(x; u) ! y = z _ y = u _ z = u)
This invariant is weaker than is ideal, because a can participate in only one q relation (b can
participate in two simultaneously). A state membership invariant for this property space
is:
8x : T  ((9y : T1  p(x; y)) _ 9y : T1  q(x; y))
which understates the case for b, which can have p1 and q1 simultaneously. No unique
state invariant is constructed for this property space, because p1 and q1 are not mutually
exclusive.

3.1 Correctness and Completeness of the Transition Rule Construction Phase

The correctness of the algorithm used in tim depends on two elements. Firstly, the property
spaces identified by the algorithm must be correctly populated. That is, no objects should
be assigned to property spaces to which they do not belong and every achievable state must
be included in the appropriate property space. Secondly, these property spaces must only
support the generation of correct invariants. This second element is examined in Section 3.2.
An interesting relationship exists between the states in a property space and the invariants generated from the space. Incorrect invariants will be contructed if a property space is
missing achievable states. This is because the state membership invariants assert that each
object in the property space must be in one of the states in the property space. If states
are missing then this invariant will be false. We now prove that all achievable states will
be in the appropriate property space.

Theorem 1 Given an initial state, I , a collection of operator schemas, O, a property space,
P = (Ps; TRs; Ss; Os), generated by tim when applied to I and O, and any state, St, which
394

fiAutomatic Inference of State Invariants

is reachable from I by application of a valid linearised plan formed from ground instances
of operator schemas in O, then for any o 2 Os, the P -projection of St for o, StoP , is in Ss.

Proof:

The proof is by induction on the length of the plan that yields the state St. In the base
case the plan contains no operator instances so St = I . The P -projection of I for o is in
Ss, by definition of the first phase of the property space construction process described in
Section 2.4.
Suppose St is generated by a plan of length k + 1, with last step a and penultimate
state pre-St. Let the P -projection of pre-St for o be pre-StoP . By the inductive hypothesis,
this state is in Ss. If a does not affect the state of o, then the P -projection of St for o
will be pre-StoP , and therefore in Ss trivially. Otherwise, consider the operator schema,
Op 2 O, from which a is formed. As described in Section 2.7, no constants appear in Op
and all variables in the body of Op are parameters of Op. Let the initial collection of PRSs
constructed from Op, for those parameters instantiated with o in the creation of a, be the
set PRS1 :::PRSn where every PRSi has the form:
precs :
Pi
deleted precs : Di
add elements : Ai
and the initial collection is the collection formed prior to splitting.
For each value of i the ith PRS will lead to the construction of k + 1 transition rules,
where k is the size of the bag intersection, Xi , of Di and Ai . The k rules will be of the
following form:
8c 2 Xi  (Pi 	 fcg ) c ! c)
and the remaining rule will be of the form:

Pi 	 (Di 	 Xi ) ) (Di 	 Xi) ! (Ai 	 Xi )
We refer to the latter rule for PRSi as the ith complex rule. A subset of the n complex
rules will contain a property in Ps in either the start or the finish and will, therefore, be
relevant to the transition from pre-St to St. It can be observed that these m complex
rules (PRS1 :::PRSm without loss of generality) must be in P because of the uniting process
described in 2.3.
We define pres(a)oP to be the P -projection of the preconditions of a for o. Similarly,
adds(a)oP and dels(a)oP are defined to be the P -projections of the add and delete lists
respectively. By construction of the PRSs, defined in Section 3.1,
m
M
= Pi
Mm
adds(a)oP = Ai
Mm
dels(a)oP = Di

pres(a)oP

1
1

1

395

fiFox & Long

Because of the restriction that delete lists must be a subset of preconditions, and the
fact that a is applicable to pre-St, it follows that dels(a)oP v pres(a)oP v pre-StoP . Since
v represents bag inclusion it can be seen that all of the separate bags Di are included in
pre-StoP without overlap.
The extension process involves the iterated application of the rules as explained in
Section 2.4 and indicated in the pseudo-code algorithm presented in Appendix B.
For a rule to be applicable to a state its start must be included in the state. Therefore
the m complex rules are all applicable, regardless of the sequence of application, to pre-StoP .
It follows that the state
(pre-StoP 	

Mm (Di 	 Xi))  Mm (Ai 	 Xi)
1

1

is generated in the extension process. By definition of Xi , and the fact that Di v pre-StoP ,
this state can be written as:
(pre-StoP 	

Mm Di)  M Ai
1

which, as observed above, is just:
(pre-StoP 	 dels(a)oP )  adds(a)oP
which equals StoP by the standard semantics of operator application in strips.

2

The proof demonstrates that splitting, discussed in 2.3, does not result in the generation
of invalid invariants. However, splitting can compromise the completeness of the invariantgeneration process. It can result in the inclusion of unreachable states in property spaces,
with the consequence that the identity and state membership invariants that are generated
are weaker than would otherwise be the case. This is further discussed in Section 3.2.
We now explain the role of splitting in the PRS construction phases. Each domain
object in a strips domain has an associated finite automaton in which the states consist of
the properties (for example, at1 ) it can have, either initially or as a result of the application
of an arbitrary length sequence of operators. Objects that can be observed to be of the
same type will have identical automata at the property level. The PRSs capture the ways
in which operator applications modify the configurations of individual objects and hence
provide an encoding of these automata.
The PRSs are built in two phases. In the first phase, all of the parameters in all of the
schemas are considered, so all possible object state transitions are captured. However, some
of these transitions conceal the functional distinctions inherent in the domain description
and would lead to premature amalgamation of property spaces, as was observed in the
discussion of the Rocket domain in Section 2.5. In that example it was observed that use
of our standard formula for the construction of rules from these PRSs alone would result in
the failure to detect the type distinction between rockets and packages.
The second phase assists the type inference processes in avoiding under-discrimination
by distinguishing enablers of a state transformation from the properties that are exchanged
396

fiAutomatic Inference of State Invariants

during the transformation. Each PRS characterizing the exchange of k properties is split
to form at most k + 1 new PRSs. The PRSs 4 and 5, given in Section 2.3, show how two
PRSs are constructed from a single PRS containing a single exchanged property. This is a
simple example, as only one split is required to remove exchanges. In general it might be
necessary to split repeatedly until all exchanges are removed, as shown in the example given
by PRS 6 in Section 2.3. No non-exchange combinations of the properties in deleted precs
and add elements should be considered during splitting. The resulting PRSs lead to the
construction of transition rules which allow generic state transformations, such as movement
from one location to another, to be separated from the specific nature of the objects that
can make those transformations.
It can be observed that the rules that result from the splitting process are more general
than the rules that would have been obtained from the PRS prior to splitting. They
distinguish more precisely between the properties that take part in state transitions and
the properties that simply enable those transitions, allowing finer type distinctions to be
inferred on the basis of the functionalities of the objects in the domain. Finer distinctions
are made during the process of seeding property and attribute spaces by uniting. This is
because uniting merges, into single equivalence classes, all of the properties that appear in
both the start and finish of a rule.
We argue that all state transformations are accounted for by the end of this second
phase. The result of the second phase is that the automata formed during the first phase
are separated into collections of simpler automata where possible, so that no transitions
are lost but there is a finer grained encoding of the possible transitions that can be made
by objects with appropriate properties. The PRSs constructed in this phase support the
construction of rules that allow objects making these transitions to occupy different property
spaces. Some of the second phase PRSs may be under-constraining, in the sense that
analysis of subsequent schemas might eliminate the possibilities they are keeping open, as
in example 2.3.1, but the set of PRSs obtained at the end of the second phase cannot be
over-constraining because all of the first phase PRSs are considered for splitting.
A subtlety concerns the consequence, at the type level, of assigning two functionally
distinct objects to the same state or attribute space. For example, in example 2.5, rocket
and package are both assigned to the property space for fin1; at1 g and the attribute space for
fin2g. However, because rocket can be fuelled or unfuelled, and the package cannot, there is
a distinction between them that emerges in the property and attribute membership vectors
associated with the rocket and package objects. Membership of the additional property space
for ffuelled1; unfuelled1g means that rocket is assigned a type that is a sub-type of the type
of package and the functional distinctness of rocket and package is recognised. As discussed,
there is an oddity in this encoding that results in the package being assigned membership
of the fin2g attribute space. Furthermore, at1 and in1 were united, with the effect that
rockets can make the at1 ! in1 transition and can be used to instantiate variables of type
movable object, even when variables of this type are intended only to be instantiated with
the package. There is nothing in the domain description to prevent this interpretation. A
more conventional encoding of the load schema would prevent the rocket from being loaded
into any other object, and this would cause a refinement in the type structure that would
identify loadable objects, and would prohibit the use of the rocket in forming instances of
operators that should be restricted to operating on those objects.
397

fiFox & Long

The construction of transition rules follows a simple rule whereby any undeleted preconditions are used to enable a transformation from a state in which the deleted preconditions
of a PRS hold to one in which the added elements of the PRS hold. Given the assumption
that all deleted atoms in an operator schema must appear as preconditions in that schema,
these rules correctly characterize strips-style state transformations. All possible transformations are captured because of the second phase of PRS construction. A complete set of
correct transition rules is therefore constructed.
Given the correctness and completeness of the transition rule construction phase, correct
initial allocation of objects to spaces depends simply on correctly checking membership of
the initial properties of the object in the property sets, formed by uniting the rules, that
are used to seed the spaces. Extension of the property spaces is done by straightforward
application of the transition rules, so all configurations of properties that can be occupied
by the objects in the property space will have been added by the end of the extension phase.
Extension of the attribute spaces is unproblematic in the cases where no potential enabler
is itself an attribute. If one is, then the process by which the attribute space of that enabler
is completed could, it appears, initiate a loop in the attribute space extension process. In
fact, this does not happen as tim is able to detect when a loop has occurred and avoid
repeatedly iterating over it.
The following example illustrates the problem and the way it is solved in tim. Suppose
we have three attribute spaces:

Attribute space 1

fq1g p1 ) null ! q1 fa; bg

Attribute space 2
fr1g q1 ) null ! r1 fcg

Attribute space 3
fp1g r1 ) null ! p1 fdg
These spaces are extended by the addition of objects that potentiate their increasing rules,
as discussed in Section 2.4. No problem arises if the enablers of these rules are states, and
not attributes, but in the extension of attribute space 1 above the enabler, p1 , is an attribute.
The attribute space for p1 has not yet been extended, so it is necessary to complete that
space before using it to complete 1. Extension of 3 requires the extension of 2, for the same
reason, and that requires the extension of 1 which requires the extension of 3, and so on.
The way tim avoids re-entering this loop is by marking each space, as it is considered, as
having been seen on this iteration. When a marked space is encountered it is not extended
but is used as if it is already complete. Then a second iteration is required to extend
any spaces that still require completion. Subsequent iterations will be required until the
process converges. Our experiments suggest that it is unusual for there to be more than
two iterations required. A worst case upper bound is o  As, where o is the number of
domain constants and As is the number of attribute spaces (which is limited by the number
of properties), and hence quadratic in the size of the domain description.
398

fiAutomatic Inference of State Invariants

If the extension process starts with attribute space 1, in the above example, attribute
space 1 will be marked as having been seen on the first iteration. Tim then goes on to
extend space 3 because the extension of space 1 depends upon space 3 being complete.
Space 3 is marked as having been seen on this iteration and space 2 is considered. Space 2
is marked and space 1 is revisited. Because space 1 is marked tim infers that a loop has
been entered. Its objects are added to space 2 without extension and the objects of space 2
are then added to space 3. Finally, the objects of space 3 can be added to space 1 and the
first iteration is complete.

fq1g p1 ) null ! q1 fa; bg [ fc; dg
fr1g q1 ) null ! r1 fcg [ fa; bg
fp1g r1 ) null ! p1 fdg [ fc; a; bg
However, space 2 is not yet complete, so a second iteration is required. This iteration
starts in the same place as the first and the process is repeated, except that no further
iterations will be required in this example.

3.2 Correctness of the State Invariants

We now argue for the correctness of the invariant inference procedure by considering each
of the four kinds of invariant in turn. The following arguments rely upon correctly distinguishing property spaces from attribute spaces, since the invariant analysis cannot be
performed on attribute spaces. The only scope for confusing this distinction is in the extension of mixed spaces, but we extract attributes from mixed spaces by checking for inclusion
of existing states in the new states generated during extension. This process was discussed
in Section 2.8.

Definition 16 Given a property space P = (Ps; TRs; Ss; Os), Ss can be partitioned into
three disjoint sets: Sssubs and Sssups that contain all of the states in Ss that are included
(as bags) or that include (as bags), respectively, at least one other state in Ss and Ssind
that contains all of the independent states in Ss that are neither in Sssubs nor in Sssups .
Theorem 2 Given a property space P = (Ps; TRs; Ss; Os), in which the set of states Ss

is a union of the three disjoint sets of states Ssind , Sssubs and Sssups , for each object, o, in
Os the following families of invariants will hold:
1. identity invariants;
2. state membership invariants;
3. unique state invariants.
as defined in Section 3.2.

Proof:

We address each kind of invariant in turn. By Theorem 1 every object in Os must be in
a state in Ss. Furthermore, all states of each object in Os, with respect to each property in
Ps, will be in Ss. This follows because the properties are partitioned between the spaces
399

fiFox & Long

during the seeding process. Therefore, the maximum number of occurrences of a property
p in Ps, possessed by any object in Os in any state of the world, will be bounded by the
maximum number of instances of that property in any state in Ss (these maximum values
might not be equal since Ss can contain inaccessible states). The identity invariants simply
express this bound on the properties of the objects in Os.
Every object in Os must be in a state in Ssind [Sssubs . This follows by definition of
these sets in Definition 16 and by Theorem 1. The state membership invariants assert that
every object in Os must be in at least one of these states, with each disjunct in the invariant
corresponding to the assertion of membership of one of these states.
To argue for the correctness of the unique state invariants, we observe that the proposed
invariants would only be false if they paired states that were not mutually exclusive. In
this case, either the state extension process would have put properties that could be simultaneously held into the same bag, or such properties would be simultaneously held in the
initial state and hence would appear in the same bag on initial construction of the property
space. In either case, a state will exist in the property space that is a superset of both of
the non-exclusive states. However, uniqueness invariants are generated for pairs of states
drawn only from Ssind [ Sssups so these non-exclusive pairs of states will not lead to the
generation of incorrect invariants.

2
The fixed resource invariants are always associated with a particular predicate. If atoms
built with that predicate are balanced on the add and delete lists of all of the operator
schemas then the number of occurrences of these atoms in the initial state is fixed over all
subsequent states. This is what the invariant expresses. An invariant is constructed for
every predicate that forms balanced atoms.
Since no new techniques are required to infer invariants from sub-spaces, no further
argument is required to support correctness of the invariants formed following sub-space
analysis.
Although Theorem 2 demonstrates the correctness of the invariants inferred by tim it
is possible for weak invariants to be inferred from the presence of unreachable states in Ss.
Weak identity invariants are inferred if an unreachable state is generated, during extension,
containing more instances of a property than are contained in any reachable state. When
this happens an identity invariant will be generated that is weaker than would be ideal, but
is still valid. Further, if a property space contains unreachable states they will cause the
inclusion of additional false disjuncts in the state membership invariants, but since these
false disjuncts will not exclude satisfying assignments their presence will not invalidate
the invariants. Unreachable states cause additional tautologous uniqueness invariants to be
generated but do not affect the strength of the invariants that refer only to reachable states.
Clearly we cannot hope to identify all of the unreachable states, as such an analysis would
be as hard as planning itself.
Because no invariants are generated for attribute spaces tim cannot be claimed to be
complete. Sub-space analysis rectifies this to some extent by identifying property spaces
that exist within attribute spaces and allowing further invariants to be generated. This
analysis could be further refined.
400

fiAutomatic Inference of State Invariants

3.3 Effects of TIM on the Properties of the Planner

Tim is itself sound, so no planner that uses tim is in danger of losing soundness as a result.
Tim is certainly not complete for all domain axioms because there are invariant properties of

other kinds that cannot be extracted by the current version. For example, Kautz and Selman
(1998) identify optimality conditions and simplifying assumptions amongst the different
kinds of axioms that might be inferred from a domain. An optimality condition in the
Logistics domain might be: a package should not be returned to a location it has been
removed from. A simplifying assumption in the same domain might be: once a truck is
loaded it should immediately move (assuming all necessary loads can be done in parallel).
These constraints require a deeper analysis of the domain than is currently performed by
tim, but we intend to characterise them and infer them in our future work.
We cannot guarantee that the type structure inferred by tim is always fully discriminating, although we do guarantee that it is not over-discriminating. However, failure on
tim's part to infer all of the structure that is there to be inferred does not impact on the
completeness of a planner using tim because, in these cases, tim will return an unstructured
domain and the planner can therefore default to reasoning with the unstructured domain
when necessary.

4. Experimental Results

An examination of tim's performance can be carried out on several dimensions. We consider
three specific dimensions here: the viability of the analysis on typical benchmark domains;
the scalability of the analysis and the utility of performing the analysis prior to planning. Its
general performance on standard benchmark problems provides an indication of the scale of
the overhead involved in using tim as a preprocessing tool. All experiments were performed
under Linux on a 300MHz PC with 128 Mb of RAM. Figure 3 shows that, even on large
problem instances, the overhead is entirely acceptable. All of the Mystery problems listed
in this table are very large (involving initial states containing hundreds of facts) and could
not be solved by stan, ipp (Koehler, Nebel, & Dimopoulos, 1997) or Blackbox (Kautz &
Selman, 1998) in the aips-98 competition. The nature of the Mystery domain is described
in Appendix C. This emphasises the relative costs of the preprocessing and planning efforts.
The selection of problems used to construct table 3 is justified as follows. In the Blocks
world we have used a representative example from each of three encodings supplied in the
pddl release. These are: the simple encoding (prob12), the att encoding (prob18) and the
snlp encoding (prob23). The Hanoi set contains a collection of reasonably sized problems.
A representative group of relatively large Mystery instances was chosen from the pddl
release. The two Tyre world instances are the only two strips instances available in the
release. The three Logistics problems are the three largest for the simple strips encoding
included in the pddl release.
The second dimension is scalability of the analysis. An analytic examination of the
algorithm can determine an upper bound on performance that is polynomial in all of the
key domain and problem components, including number of operator schemas, number of
literals in operators, numbers of objects and facts in the initial state and the number and
arities of predicates in the language. Figure 4 shows that the performance of tim is roughly
quadratic in the size of the problem specification. In the graph, size is crudely equated with
401

fiFox & Long

Domain and problem
Blocks
prob12.pddl
prob18.pddl
prob23.pddl
Hanoi
3-disc
4-disc
5-disc
6-disc
7-disc
Mystery
prob060.pddl
prob061.pddl
prob062.pddl
prob063.pddl
prob064.pddl
Tyre-World prob01.pddl
prob02.pddl
Logistics
prob04.pddl
prob05.pddl
prob06.pddl

Parse time
2
3
2
2
2
3
3
4
17
48
26
11
21
5
6
4
4
4

Analysis time
0
1
1
1
1
1
1
2
15
82
37
7
21
2
2
2
2
2

Output time Total
2
5
2
7
1
5
4
7
4
7
4
8
4
9
4
11
9
43
29
160
10
74
8
27
10
52
28
36
28
37
5
12
6
12
6
13

Figure 3: Table showing tim's performance in milliseconds on standard domains and problems. All timings are elapsed times and minor discrepancies in totals arise from
rounding.

402

fiAutomatic Inference of State Invariants

Tim Analysis of Mystery Domain

12000
10000



8000
Millisecs 6000



4000
2000

 








0 
0

   



10000 20000 30000 40000 50000 60000 70000 80000
Size of file

Figure 4: Graph showing tim's performance on Mystery problems, plotting time against
size (in characters) of problem file. The solid line is a plot of a quadratic function.
the number of characters in the specification file. This graph was constructed by running
tim on all of the strips Mystery domain problems in the pddl release. The increasing
sizes of the problem specifications reect increases in any and all of the various categories
of objects in the domain and corresponding facts to describe their initial states.
Figure 5 shows the effect on tim's performance as the number of operator schemas
increases. This graph was constructed using an artificial domain in which each new operator
causes two new state transitions described by two new literals. Thus, both number of
operators and number of properties is increasing whilst the number of objects stays constant.
The domain is described in detail in Appendix E. The graph indicates the linear growth of
cost of analysis.
The final dimension for evaluating tim is the effect of exploitation of its output by a
planner. Gerevini and Schubert (1998) and Kautz and Selman (1998) provide convincing
evidence supporting the powerful role of state invariants in enhancing the performance of
SAT-based planning. In Figure 6 we demonstrate the power of inferred types by showing the
advantage that stan with tim obtains over stan without tim on untyped Rocket domain
problems. Figure 6 shows the effect on performance of increasing the number of packages
to be transported. The time taken by stan with tim grows linearly, whilst stan without
tim follows a cubic curve. If there are p packages in a problem instance then stan with tim
constructs 4(p +1) operator instances while stan without tim constructs (p +3)2(p +5)+2p
instances. This demonstrates that type information is the most significant factor in the
advantage depicted in the graph. Figure 7 demonstrates that a similar improvement is
obtained in the Logistics domain. In this graph a series of sub-problems were considered in
403

fiFox & Long

70
65
60
55
50
Millisecs 45
40
35
30
25
20

Tim Performance with Increasing Number of Operators

3
0

3
2

3

3
4

3

3

3

3

3

3

3

6
8
10
Number of operators

3

3

3

12

33

14

16

Figure 5: Graph showing the consequences of increasing the number of schemas and inferrable property spaces.
The Effect of Tim on the Performance of Stan

12000

3

STAN without TIM 3
STAN with TIM +

10000
8000

3

Millisecs6000
4000

3

3

3

3

3

3

33
3
333
33
+ + ++ + + + ++ + + + ++ +
+ 3
++
+ 3
0 3

2000

0

5

10

15
20
25
Number of packages

30

35

40

Figure 6: Graph showing comparison between stan with and stan without tim on Rocket
domain problems generated from the Rocket domain provided in Appendix D.
404

fiAutomatic Inference of State Invariants

The Effect of Tim on the Performance of Stan

12000
10000

STAN without TIM
STAN with TIM

8000
Millisecs6000
4000
2000
0

1

1.5

2

2.5
3
3.5
Number of sub-problems

4

4.5

5

Figure 7: Graph showing comparison between stan with and stan without tim on Logistics
domain problems.
which each sub-problem involves the independent transportation of a single package between
two cities.
In very simple domains, the overhead of carrying out this analysis can outweigh the
advantages offered. For example, in the Movie domain used in the competition stan gained
no benefits from using tim but paid the overhead to the detriment of its performance on
instances from that domain. However, in general we observe that the benefits of this analysis
increase with the increasing complexity of domains.

5. Related Work
Although the importance of state invariants for ecient planning has been observed there
has been relatively little work on automatic inference of invariants. The published work that
most closely resembles the research described in this paper is the state constraint inference
system discoplan, of Gerevini and Schubert (1998). Discoplan enables the inference
of sv-constraints that correspond to a subset of our identity invariants. The reason that
discoplan is restricted to a subset is that it generates sv-constraints only for pairs of literals
(one on the addlist of a schema and the other on the delete list) in which the arguments
vary in only one place. Tim can infer identity invariants in which vectors of arguments vary,
as shown in Section 2.7. Discoplan cannot currently infer all singly varying constraints
(although the techniques described by Gerevini and Schubert (1996a) are not yet fully
implemented in discoplan). For example, discoplan cannot infer that all blocks can only
405

fiFox & Long

be on one surface, in its analysis of the Blocks world domain cited in the paper. Tim can
infer these invariants from its sub-space analysis.
Gerevini and Schubert (1996a, 1996b) have also examined the potential for inferring
parameter domains that are similar to the operator parameter types inferred by tim. Their
domains are inferred by an iterative process of accretion which is similar to the attribute
space extension process of tim. However, the accretion process they describe is synthetic,
in that the parameter domains are synthesised directly from the operator descriptions and
initial state. Tim is an analytic system that constructs its types from an analysis of the functional properties of the domain objects. This analytic approach provides a rich information
source from which other structures, including the domain invariants, can be derived.
Some of the implicative constraints inferred by discoplan correspond to an implicit
type assignment and would arise in the type structure built by tim. A further implicative
constraint generated by discoplan refers to the separation of functional roles of objects.
In particular, the irreexivity of on, as in:
8x  8y  (on(x; y) ! :(x = y))
can be captured using this kind of constraint. Tim cannot currently infer these invariants.
Because tim uses an analysis based on the state view of objects in the domain it is able
to generate a broader collection of invariants, including state membership and unique state
invariants currently not produced by discoplan.
Although discoplan can deal with negative preconditions and tim cannot yet manage
them, the invariants they produce overall are currently less powerful than those inferred by
tim.
Apart from the work of Gerevini and Schubert, there is some older work on the inference
of invariants which also relies on the generation of candidate invariants which are then
confirmed by an inductive process against the domain operators. Two examples are the
work of Kelleher and Cohn (1992) and Morris and Feldman (1989). The former work
concentrates on identifying directed mutual persistence relations, which hold between pairs
of facts in a domain when, once both are established, the second continues to hold while
the first does. The use of these relations leads to the inference of a collection of constraints
which fall into the uniqueness invariants inferred by tim. In the work described in (Morris
& Feldman, 1989) the authors build invariants by using truth counts which are counts of the
number of propositions from particular identified sets which must be true in any state of
the domain. Sets for which this count is 1 can then be used to build invariants which are a
subset of the state membership and uniqueness invariants. The authors describe methods for
attempting to identify the sets of facts from which to work. This work, in common with that
of Kelleher and Cohn and of Gerevini and Schubert, builds invariants by first hypothesising
a possible seed for the invariants and then determining their validity by analysing the effects
of the operators on these seeds. In contrast to this generate-and-test strategy, tim produces
only correct invariants which it infers from a deep, structural analysis of the domain. The
inference of invariants does not exhaust the possibilities of this analysis. For example,
the type structure is inferred automatically during this analysis, which has been shown to
have dramatic potential for the eciency of planning. The relationship between enablers,
and the state transitions they enable, determines an ordering on the satisfaction of goals,
which also has significance for eciency. Further, the state-based view of the behaviour of
406

fiAutomatic Inference of State Invariants

domain objects would allow the techniques described by McCluskey and Porteous (1997)
to be automated.
McCluskey and Porteous (1997) have proposed and explored an object-centred approach
to planning. This approach is based on the provision, by a domain engineer, of a rich
collection of state invariants for object sorts participating in functional relationships in the
domain. These invariants are then exploited in a domain compilation phase to facilitate an
ecient planning application to that domain. Tim infers precisely the sorts and collections
of state invariants that McCluskey and Porteous provide by hand.
Grant (1996) generates state invariants from state descriptions, provided by hand, and
then uses these invariants to build operator schemas. His approach is clearly related even
though the objectives of his analysis are different. Grant is concerned with the automatic
synthesis of domain descriptions from a rich requirements specification provided by an
expert user. Our concern is with reverse-engineering a domain description to obtain the information that can help increase the eciency of planners applied to that domain. Although
the primary objectives in the use of tim are to enhance the performance of planning within
a domain, tim also provides a valuable tool in the construction of domain descriptions by
revealing the underlying behaviours that the domain engineer has implicitly imposed, and
helping with the debugging of domain descriptions.

6. Conclusion
Tim is a planner-independent set of techniques for identifying the underlying structure of a

domain, revealing its type structure and a collection of four different kinds of invariant conditions. One important application of these techniques is as a domain debugging aid during
the construction of large and complex domains. Using tim has revealed many anomalies
in domains encoded by us and by others, and has greatly assisted us in understanding
stan's performance on many domains and problems. Another important application is in
increasing the eciency of planners by making explicit to the planner information about
the domain that it would otherwise have to infer, from the domain representation, during
planning.
Tim generates a rich collection of invariants containing many that are not inferrable by
related systems, as discussed in the previous section. The results presented by Gerevini
and Schubert (1998) suggest that a marked improvement can be obtained from the use of
invariants in the performance of planners based on SAT-solving techniques. No analysis
has yet been done to determine what advantages might be obtainable by using invariants in
planners based on other architectures. Stan does not yet exploit all of the invariants produced by tim during planning. It uses the type structure and the fixed resource invariants
and we are currently developing an extension of stan that will fully exploit the other kinds
of invariant. We expect to be able to use the uniqueness and identity invariants to shortcut
the effort involved in deducing a significant subset of the necessary mutex relations during
graph construction.
The analysis performed by tim is ecient, growing more slowly than a quadratic function
of the size of the initial state being analysed. Our empirical analysis does not consider
the effect on tim's performance of increasing numbers of operator schemas. However, the
argument presented in Section 4 shows that tim's analysis grows linearly with the number
407

fiFox & Long

of operator schemas, linearly with the number of domain constants and linearly with the
size of the initial state. There are other factors to take into account, but this confirms a
polynomial performance as the size (and related structure) of the domain increases.
The type analysis performed by tim differs, in some important respects, from the various
forms of type analysis performed during the compilation of programs written in strongly
typed languages. In the latter context the type-correctness of a program is judged with
respect to an imposed context of basic types. Tim infers the basic types from the domain
description so it is impossible for a domain specification not to be well-typed. Consequently
we do not attempt to type-check domain descriptions using tim. This is a direction in which
we hope to move in the near future, because type-checking will enable some unsolvable
problems to be detected as unsolvable statically rather than at planning time. We currently
focus only on type inference and the exploitation of the inferred type structure in the
management of the search space of the planner.

7. Acknowledgements

We would like to thank Alfonso Gerevini, Gerry Kelleher and the anonymous referees for
useful discussions and helpful comments on earlier drafts of this paper.

Appendix A. FTP and Web Sites

The aips-98 Planning Competition FTP site is at:

http://ftp.cs.yale.edu/pub/mcdermott/aipscomp-results.html.

Our web site, on which stan and tim executables can be found, is at:



http://www.dur.ac.uk/ dcs0www/research/stanstuff/planpage.html

Appendix B. The TIM Algorithm

The following is a pseudo-code description of the tim algorithm.
fConstruct base PRSs (Section 2.3)g
Ps := fg;
for each operator schema, O,
for each variable in O, x,
construct a PRS for x from O and add to Ps;

fSplit PRSsg

for each PRS in Ps, P,
if a property, p, appears in P in both the adds and deleted precs fields
then split P over p, into P' and Q and replace P with P' and Q in Ps,
where to split P over p:
construct PRS Q with the same precs as P, deleted precs and adds both set to fpg;
construct PRS P' from P by removing p from deleted precs and adds of P;

fConstruct transition rules (Section 2.3)g
Ts := fg;

for each PRS in Ps, P,
construct a transition rule for P and add to Ts;

fSeed property and attribute spaces (Section 2.3)g

let each property be initially assigned to a separate equivalence class;
for each rule, r, in Ts
merge together (unite) the equivalence classes for all the properties in the start and finish of r;

408

fiAutomatic Inference of State Invariants

construct a separate space for each equivalence class of properties;

fAssign transition rules (Section 2.4)g

for each rule, r, in Ts
place r in the space associated with the equivalence class containing the properties
in the start (and finish) of r, s;
if r is an increasing or decreasing rule
then mark s as an attribute space;

fAnalyse initial state (Section 2.4)g

for each object, o, in the domain
identify the bag of initial properties of o, I(o);
for each space, s,
construct the bag of properties from I(o) which belong to the equivalence class
associated with s, b;
if b is non-empty
then add o to the space s;
if s is not an attribute space
then add b as a state in s;

fExtend property spaces (Section 2.4)g

for each property space, p,
while there is an unextended state in p, s,
mark s as extended;
newgen := fg;
for each rule in p, r,
if the start of r is included in s
then add the state snew = (s ominus start oplus end) to newgen;
if snewis a superset of any state in newgen
then mark p is an attribute space and exit the analysis of p;
add newgen to the states in p;

fExtend attribute spaces (Section 2.4)g

changes := TRUE;
while changes,
changes := FALSE;
for each unmarked attribute space, a,
extend a where to extend a:
mark a;
for each rule in a, r,
for each property in enablers of r, p,
if p's equivalence class is associated with an unmarked attribute space, a',
then extend a';
add all objects that appear in every space associated with an enabling property for r to a;
if objects are added
then changes := TRUE;

fIdentify types (Section 2.6)g

for each object in the domain, o,
identify the pattern of membership of spaces for o, tt;
associate the type pattern, tt, with o;
for each operator schema, O,
for each argument of O, x,
identify the pattern of membership of spaces for x implied by the properties of x in the
preconditions of O, tt;
associate type pattern, tt, with x in O;

fConstruct invariants (Section 2.7)g

for each property space, P,
for each property in P, p,
construct an identity invariant for p;
construct a state membership invariant for P;
construct a uniqueness invariant for P;

409

fiFox & Long

Appendix C. Example Output

The following output was produced by tim and can be found, along with other examples,
on the stan webpage. These examples show the details of the analysis performed on each
of three domains: a Flat-tyre domain, a Mystery domain and a Logistics domain. The
analysis is done with respect to an initial state and a set of operator schemas. The operator
schemas used in these three domains are those provided with the pddl strips releases for
these domains. The initial states were taken from the pddl release. The pddl release can
be found at http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/mcdermott.html.

C.1 The Tyre World
TIM: Type Inference Mechanism - support for STAN: State Analysis Planner
D. Long and M. Fox, University of Durham

Reading domain file: domain01.pddl
Reading problem file: prob01.pddl
TIM: Domain analysis complete for flat-tire-strips
TIM: TYPES:
Type
Type
Type
Type
Type
Type
Type
Type

T0
T1
T2
T3
T4
T5
T6
T7

=
=
=
=
=
=
=
=

{wrench}
{wheel2}
{wheel1}
{trunk}
{the-hub}
{pump}
{nuts}
{jack}

It will be noticed that the two wheels are separated into different types. This is because
one wheel is intact and the other is not intact, and there is no operator for repairing wheels
that are not intact. The tools have each been given different types. This is because they
each appear as constants in different operators and therefore are functionally distinct.
TIM: STATE INVARIANTS:
FORALL x:T4. (on-ground(x) OR lifted(x))
FORALL x:T4. NOT (on-ground(x) AND lifted(x))
FORALL x:T3. (closed(x) OR open(x))
FORALL x:T3. NOT (closed(x) AND open(x))

410

fiAutomatic Inference of State Invariants

FORALL x:T1 U T2. (deflated(x) OR inflated(x))
FORALL x:T1 U T2. NOT (deflated(x) AND inflated(x))

The invariants for hubs (below) suggest that almost anything could be on a hub. Since
this is not the case the type structure is under-discriminating. However, the additional
invariants drawn from the sub-space analysis provide enough information, in principle, to
discriminate more fully between the types. This information is not yet being fully exploited.
FORALL x:T4. FORALL y1. FORALL z1. on(y1,x) AND on(z1,x) => y1 = z1
FORALL x:T4. (Exists y1:T0 U T1 U T2 U T5 U T6 U T7. on(y1,x)
OR free(x))
FORALL x:T4. NOT (Exists y1:T0 U T1 U T2 U T5 U T6 U T7. on(y1,x)
AND free(x))
FORALL x:T4. FORALL y1. FORALL z1. tight(y1,x) AND tight(z1,x) => y1 = z1
FORALL x:T4. FORALL y1. FORALL z1. loose(y1,x) AND loose(z1,x) => y1 = z1
FORALL x:T4. ((Exists y1:T0 U T1 U T2 U T5 U T6 U T7. tight(y1,x)
AND fastened(x))
OR (Exists y1:T0 U T1 U T2 U T5 U T6 U T7. loose(y1,x)
AND fastened(x)) OR unfastened(x))
FORALL x:T4. NOT ((Exists y1:T0 U T1 U T2 U T5 U T6 U T7. tight(y1,x)
AND fastened(x))
AND (Exists y1:T0 U T1 U T2 U T5 U T6 U T7. loose(y1,x)
AND fastened(x)))
FORALL x:T4. NOT ((Exists y1:T0 U T1 U T2 U T5 U T6 U T7. tight(y1,x)
AND fastened(x)) AND unfastened(x))
FORALL x:T4. NOT ((Exists y1:T0 U T1 U T2 U T5 U T6 U T7. loose(y1,x)
AND fastened(x)) AND unfastened(x))

TIM: DOMAIN INVARIANTS:
|{x0:
|{x0:
|{x0:
|{x0:
|{x0:
|{x0:
|{x0:
|{x0:
|{x0:

container(x0)}| = 1
hub(x0)}| = 1
intact(x0)}| = 1
jack(x0)}| = 1
nut(x0)}| = 1
pump(x0)}| = 1
unlocked(x0)}| = 1
wheel(x0)}| = 2
wrench(x0)}| = 1

TIM: ATTRIBUTE SPACES:

411

fiFox & Long

The attribute space for the properties in the first of these groups is subjected to a much
more rigorous analysis in the sub-space invariants below.
Objects, x, in T0 U T1 U T2 U T5 U T6 U T7 can have property:
Exists y1:T3. in(x,y1);
Exists y1:T4. on(x,y1);
Exists y1:T4. tight(x,y1);
Exists y1:T4. loose(x,y1);
have(x);
Objects, x, in T3 can have property:
Exists y1:T0 U T1 U T2 U T5 U T6 U T7. in(y1,x);
Objects, x, in T3 all have property: container(x);
Objects, x, in T4 all have property: hub(x);
Objects, x, in T1 all have property: intact(x);
Objects, x, in T7 all have property: jack(x);
Objects, x, in T6 all have property: nut(x);
Objects, x, in T5 all have property: pump(x);
Objects, x, in T3 all have property: unlocked(x);
Objects, x, in T1 U T2 all have property: wheel(x);
Objects, x, in T0 all have property: wrench(x);

TIM: OPERATOR PARAMETER RESTRICTIONS:
inflate(x1:T1)
put-on-wheel(x1:T1 U T2,x2:T4)
remove-wheel(x1:T1 U T2,x2:T4)
put-on-nuts(x1:T6,x2:T4)
remove-nuts(x1:T6,x2:T4)
jack-down(x1:T4)
jack-up(x1:T4)
tighten(x1:T6,x2:T4)
loosen(x1:T6,x2:T4)
put-away(x1:T0 U T1 U T2 U T5 U T6 U T7,x2:T3)
fetch(x1:T0 U T1 U T2 U T5 U T6 U T7,x2:T3)
close-container(x1:T3)
open-container(x1:T3)
cuss()

TIM: ADDITIONAL STATE INVARIANTS, USING SUB-SPACE ANALYSIS:

We report here only the additional state invariants that add information to the invariants
already listed. TIM currently reports invariants that are subsumed by the earlier collection.
It should be observed that the first wheel is intact but the second is not, and this gives
rise to the following new invariant for wheels of the second type.
412

fiAutomatic Inference of State Invariants

FORALL x:T2. (deflated(x))

The first attribute space, which contains all objects except the trunk and the hub, is now
subjected to sub-space analysis yielding a rich new collection of invariants.
FORALL x:T0. FORALL y1. FORALL z1. in(x,y1) AND in(x,z1) => y1 = z1
FORALL x:T0. (Exists y1:T3. in(x,y1) OR have(x))
FORALL x:T0. NOT (Exists y1:T3. in(x,y1) AND have(x))
FORALL x:T1. FORALL y1. FORALL z1. in(x,y1) AND in(x,z1) => y1 = z1
FORALL x:T1. FORALL y1. FORALL z1. on(x,y1) AND on(x,z1) => y1 = z1
FORALL x:T1. (Exists y1:T3. in(x,y1) OR have(x)
OR Exists y1:T4. on(x,y1))
FORALL x:T1. NOT (Exists y1:T3. in(x,y1) AND have(x))
FORALL x:T1. NOT (Exists y1:T3. in(x,y1) AND Exists y1:T4. on(x,y1))
FORALL x:T1. NOT (have(x) AND Exists y1:T4. on(x,y1))
FORALL x:T2. FORALL y1. FORALL z1. in(x,y1) AND in(x,z1) => y1 = z1
FORALL x:T2. FORALL y1. FORALL z1. on(x,y1) AND on(x,z1) => y1 = z1
FORALL x:T2. (Exists y1:T4. on(x,y1) OR have(x)
OR Exists y1:T3. in(x,y1))
FORALL x:T2. NOT (Exists y1:T4. on(x,y1) AND have(x))
FORALL x:T2. NOT (Exists y1:T4. on(x,y1) AND Exists y1:T3. in(x,y1))
FORALL x:T2. NOT (have(x) AND Exists y1:T3. in(x,y1))
FORALL x:T5. FORALL y1. FORALL z1. in(x,y1) AND in(x,z1) => y1 = z1
FORALL x:T5. (Exists y1:T3. in(x,y1) OR have(x))
FORALL x:T5. NOT (Exists y1:T3. in(x,y1) AND have(x))
FORALL x:T6. FORALL y1. FORALL z1. in(x,y1) AND in(x,z1) => y1 = z1
FORALL x:T6. FORALL y1. FORALL z1. tight(x,y1)
AND tight(x,z1) => y1 = z1
FORALL x:T6. FORALL y1. FORALL z1. loose(x,y1)
AND loose(x,z1) => y1 = z1
FORALL x:T6. (Exists y1:T4. tight(x,y1)
OR Exists y1:T4. loose(x,y1)
OR have(x) OR Exists y1:T3. in(x,y1))
FORALL x:T6. NOT (Exists y1:T4. tight(x,y1)
AND Exists y1:T4. loose(x,y1))
FORALL x:T6. NOT (Exists y1:T4. tight(x,y1) AND have(x))
FORALL x:T6. NOT (Exists y1:T4. tight(x,y1)
AND Exists y1:T3. in(x,y1))
FORALL x:T6. NOT (Exists y1:T4. loose(x,y1) AND have(x))
FORALL x:T6. NOT (Exists y1:T4. loose(x,y1)
AND Exists y1:T3. in(x,y1))
FORALL x:T6. NOT (have(x) AND Exists y1:T3. in(x,y1))

413

fiFox & Long

C.2 The Mystery Domain

The Mystery domain was devised by Drew McDermott for the aips-98 planning competition.
His intention was to conceal the structure of the problem domain by employing an obscure
encoding of a transportation domain. The code replaces locations with the names of foods
and the routes between them with eats relations. The transports are pleasures while cargos
are pains. Cargos and transports can be at locations, with the at relation encoded as craves.
A cargo is either at a location or in a transport encoded by the fears relation. Transports
have restricted capacity encoded by planets and consume fuel in travelling between locations.
Fuel exists in limited quantities at locations measured by provinces. Using TIM we were
able to decode the domain and identify the roles played by each of the components of the
encoding.
TIM: Domain analysis complete for mystery-strips (prob048.pddl)
TIM: TYPES:

It should be noted that provinces (types T6, T7 and T8) are divided into three separate
types because they form a sequence, defined by the attacks relation, in which the first and
last have a slightly different functional role to the others. The same is true of the planets
(types T1, T2 and T3).
Type T0 = {beef,cantelope,chocolate,flounder,guava,mutton,onion,
pepper,rice,shrimp,sweetroll,tuna,yogurt}
Type T1 = {saturn}
Type T2 = {pluto}
Type T3 = {neptune}
Type T4 = {achievement,lubricity}
Type T5 = {abrasion,anger,angina,boils,depression,grief,hangover,
laceration}
Type T6 = {alsace,bosnia,guanabara,kentucky}
Type T7 = {goias}
Type T8 = {arizona}

TIM: STATE INVARIANTS:
FORALL x:T4. FORALL y1. FORALL z1. harmony(x,y1)
AND harmony(x,z1) => y1 = z1
FORALL x:T4. (Exists y1:T1 U T2 U T3. harmony(x,y1))
FORALL x:T0. FORALL y1. FORALL z1. locale(x,y1)
AND locale(x,z1) => y1 = z1
FORALL x:T0. (Exists y1:T6 U T7 U T8. locale(x,y1))
FORALL x:T4 U T5. FORALL y1. FORALL z1. fears(x,y1)

414

fiAutomatic Inference of State Invariants

AND fears(x,z1) => y1 = z1
FORALL x:T4 U T5. FORALL y1. FORALL z1. craves(x,y1)
AND craves(x,z1) => y1 = z1
FORALL x:T4 U T5. (Exists y1:T0. craves(x,y1)
OR Exists y1:T4. fears(x,y1))
FORALL x:T4 U T5. NOT (Exists y1:T0. craves(x,y1)
AND Exists y1:T4. fears(x,y1))

TIM: DOMAIN INVARIANTS:
|{(x0,x1): attacks(x0,x1)}| = 5
|{(x0,x1): eats(x0,x1)}| = 36
|{x0: food(x0)}| = 13
|{(x0,x1): harmony(x0,x1)}| = 2
|{(x0,x1): locale(x0,x1)}| = 13
|{(x0,x1): orbits(x0,x1)}| = 2
|{x0: pain(x0)}| = 8
|{x0: planet(x0)}| = 3
|{x0: pleasure(x0)}| = 2
|{x0: province(x0)}| = 6

TIM: ATTRIBUTE SPACES:
Objects, x, in T1 U T2 U T3 can have property:
Exists y1:T4. harmony(y1,x);
Objects, x, in T6 U T7 U T8 can have property:
Exists y1:T0. locale(y1,x);
Objects, x, in T4 can have property:
Exists y1:T4. fears(y1,x);
Objects, x, in T0 can have property:
Exists y1:T4 U T5. craves(y1,x);
Objects, x, in T6 U T7 all have property:
Exists y1:T6 U T8. attacks(x,y1);
Objects, x, in T6 U T8 all have property:
Exists y1:T6 U T7. attacks(y1,x);
Objects, x, in T0 all have property:
Exists y1:T0. eats(x,y1);
Objects, x, in T0 all have property:
Exists y1:T0. eats(y1,x);
Objects, x, in T0 all have property: food(x);
Objects, x, in T2 U T3 all have property:
Exists y1:T1 U T2. orbits(x,y1);

415

fiFox & Long

Objects, x, in
Exists y1:T2 U
Objects, x, in
Objects, x, in
Objects, x, in
Objects, x, in

T1 U T2 all have property:
T3. orbits(y1,x);
T5 all have property: pain(x);
T1 U T2 U T3 all have property: planet(x);
T4 all have property: pleasure(x);
T6 U T7 U T8 all have property: province(x);

TIM: OPERATOR PARAMETER RESTRICTIONS:
succumb(x1:T5,x2:T4)
feast(x1:T4,x2:T0,x3:T0)
overcome(x1:T5,x2:T4)

TIM: ADDITIONAL STATE INVARIANTS, USING SUB-STATE ANALYSIS:

These additional invariants show that the transports are always at a location and never
loaded into other transports.
FORALL x:T4. FORALL y1. FORALL z1. craves(x,y1)
AND craves(x,z1) => y1 = z1
FORALL x:T4. (Exists y1:T0. craves(x,y1))

C.3 The Logistics Domain
TIM: Domain analysis complete for logistics-strips (prob05.pddl)
TIM: TYPES:
Type T0 = {bos-truck,la-truck,pgh-truck}
Type T1 = {bos-po,la-po,pgh-po}
Type T2 = {bos-airport,la-airport,pgh-airport}
Type T3 = {bos,la,pgh}
Type T4 = {package1,package2,package3,package4,package5,package6,
package7,package8}
Type T5 = {airplane1,airplane2}

TIM: STATE INVARIANTS:
FORALL x:T0 U T4 U
AND at(x,z1) => y1
FORALL x:T0 U T4 U
AND in(x,z1) => y1
FORALL x:T0 U T4 U

T5. FORALL y1. FORALL z1. at(x,y1)
= z1
T5. FORALL y1. FORALL z1. in(x,y1)
= z1
T5. (Exists y1:T1 U T2. at(x,y1)

416

fiAutomatic Inference of State Invariants

OR Exists y1:T0 U T5. in(x,y1))
FORALL x:T0 U T4 U T5. NOT (Exists y1:T1 U T2. at(x,y1)
AND Exists y1:T0 U T5. in(x,y1))

TIM: DOMAIN INVARIANTS:
|{x0: airplane(x0)}| = 2
|{x0: airport(x0)}| = 3
|{x0: city(x0)}| = 3
|{(x0,x1): in-city(x0,x1)}| = 6
|{x0: location(x0)}| = 6
|{x0: obj(x0)}| = 8
|{x0: truck(x0)}| = 3

TIM: ATTRIBUTE SPACES:
Objects, x, in
Exists y1:T0 U
Objects, x, in
Exists y1:T0 U
Objects, x, in
Objects, x, in
Objects, x, in
Objects, x, in
Objects, x, in
Objects, x, in
Objects, x, in
Objects, x, in

T1
T4
T0
T4
T5
T2
T3
T1
T3
T1
T4
T0

U T2 can have property:
U T5. at(y1,x);
U T5 can have property:
U T5. in(y1,x);
all have property: airplane(x);
all have property: airport(x);
all have property: city(x);
U T2 all have property: Exists y1:T3. in-city(x,y1);
all have property: Exists y1:T1 U T2. in-city(y1,x);
U T2 all have property: location(x);
all have property: obj(x);
all have property: truck(x);

TIM: OPERATOR PARAMETER RESTRICTIONS:
drive(x1:T0,x2:T1 U T2,x3:T1
fly(x1:T5,x2:T2,x3:T2)
unload(x1:T0 U T4 U T5,x2:T0
load-plane(x1:T4,x2:T5,x3:T1
load-truck(x1:T4,x2:T0,x3:T1

U T2,x4:T3)
U T5,x3:T1 U T2)
U T2)
U T2)

TIM: ADDITIONAL STATE INVARIANTS, USING SUB-STATE ANALYSIS:

417

fiFox & Long

The following invariants add the constraints that trucks and airplanes must always be at a
location and never loaded into one another.
FORALL x:T0. FORALL y1. FORALL z1. at(x,y1) AND at(x,z1) => y1 = z1
FORALL x:T0. (Exists y1:T1 U T2. at(x,y1))
FORALL x:T5. FORALL y1. FORALL z1. at(x,y1) AND at(x,z1) => y1 = z1
FORALL x:T5. (Exists y1:T1 U T2. at(x,y1))

Appendix D. The Rocket Domain

The Rocket domain used in the construction of Figure 6 is as follows:
(define (domain rocket)
(:predicates
(at ?x ?y)
(in ?x ?y)
(fuelled ?x)
(unfuelled ?x)
(loc ?x)
(obj ?x)
(container ?x))
(:action fly
:parameters (?x ?y ?z)
:precondition (and (at ?x ?y) (loc ?z) (fuelled ?x))
:effect (and (not (at ?x ?y)) (at ?x ?z) (unfuelled ?x)
(not (fuelled ?x))))
(:action load
:parameters (?x ?y ?z)
:precondition (and (obj ?x) (container ?y) (at ?x ?z)
(at ?y ?z))
:effect (and (in ?x ?y) (not (at ?x ?z))))
(:action unload
:parameters (?x ?y ?z)
:precondition (and (at ?y ?z) (in ?x ?y))
:effect (and (at ?x ?z) (not (in ?x ?y)))))

Appendix E. Operator Test Domain

This domain is an artificial domain used to test the effects of increasing operators and literals
in the domain encoding on the performance of TIM. This example is the third instance - the
variation was achieved by adding more operator schemas in the pattern of those included
here.
418

fiAutomatic Inference of State Invariants

(define (domain od)
(:predicates
(p1 ?x ?y) (q1 ?x ?y)
(p2 ?x ?y) (q2 ?x ?y)
(p3 ?x ?y) (q3 ?x ?y)
(p4 ?x ?y) (q4 ?x ?y)
(p5 ?x ?y) (q5 ?x ?y)
(p6 ?x ?y) (q6 ?x ?y)
(p7 ?x ?y) (q7 ?x ?y)
(p8 ?x ?y) (q8 ?x ?y)
(p9 ?x ?y) (q9 ?x ?y)
(p10 ?x ?y) (q10 ?x ?y)
(p11 ?x ?y) (q11 ?x ?y)
(p12 ?x ?y) (q12 ?x ?y)
(p13 ?x ?y) (q13 ?x ?y)
(p14 ?x ?y) (q14 ?x ?y)
(p15 ?x ?y) (q15 ?x ?y)
(p16 ?x ?y) (q16 ?x ?y)
(p17 ?x ?y) (q17 ?x ?y)
(p18 ?x ?y) (q18 ?x ?y)
(p19 ?x ?y) (q19 ?x ?y)
(p20 ?x ?y) (q20 ?x ?y))
(:action o1
:parameters (?x ?y ?z)
:precondition (and (p1 ?x ?y) (q1 ?x ?z))
:effect (and (not (p1 ?x ?y)) (not (q1 ?x ?z))
(p1 ?x ?z) (q1 ?x ?y)))
(:action o2
:parameters (?x ?y ?z)
:precondition (and (p2 ?x ?y) (q2 ?x ?z))
:effect (and (not (p2 ?x ?y)) (not (q2 ?x ?z))
(p2 ?x ?z) (q2 ?x ?y)))
(:action o3
:parameters (?x ?y ?z)
:precondition (and (p3 ?x ?y) (q3 ?x ?z))
:effect (and (not (p3 ?x ?y)) (not (q3 ?x ?z))
(p3 ?x ?z) (q3 ?x ?y))))

The problem instance was fixed as follows:
(define (problem op)
(:domain od)
(:objects a b c)

419

fiFox & Long

(:init

(p1 a b)
(q1 a c)
(p2 a b)
(q2 a c)
(p3 a b)
(q3 a c)
(p4 a b)
(q4 a c)
(p5 a b)
(q5 a c)
(p6 a b)
(q6 a c)
(p7 a b)
(q7 a c)
(p8 a b)
(q8 a c)
(p9 a b)
(q9 a c)
(p10 a b)
(q10 a c)
(p11 a b)
(q11 a c)
(p12 a b)
(q12 a c)
(p13 a b)
(q13 a c)
(p14 a b)
(q14 a c)
(p15 a b)
(q15 a c)
(p16 a b)
(q16 a c)
(p17 a b)
(q17 a c)
(p18 a b)
(q18 a c)
(p19 a b)
(q19 a c)
(p20 a b)
(q20 a c))
(:goal (and (p1 a c) (q1 a b))))

References

Blum, A., & Furst, M. (1995). Fast Planning through Plan-graph Analysis. In IJCAI.
Bundy, A., Burstall, R., Weir, S., & Young, R. (1980). Artificial Intelligence: An Introductory Course. Edinburgh University Press.
Fikes, R., & Nilsson, N. (1971). STRIPS: A New Approach to the Application of TheoremProving to Problem-Solving. Artificial Intelligence, 2 (3).
Gerevini, A., & Schubert, L. (1996a). Accelerating Partial Order Planners: Some Techniques for Effective Search Control and Pruning. JAIR, 5, 95{137.
Gerevini, A., & Schubert, L. (1996b). Computing Parameter Domains as an Aid to Planning.
In AIPS-96.
Gerevini, A., & Schubert, L. (1998). Inferring State Constraints for Domain-Independent
Planning. In AAAI.
Grant, T. J. (1996). Inductive Learning of Knowledge-based Planning Operators. Ph.D.
thesis, Rijksuniversiteit Limburg de Maastricht.
Kautz, H., & Selman, B. (1998). The Role of Domain Specific Knowledge in the Planning
as Satisfiability Framework. In The Fourth International Conference on Artificial
Intelligence Planning Systems.
420

fiAutomatic Inference of State Invariants

Kelleher, G., & Cohn, A. (1992). Automatically Synthesising Domain Constraints from
Operator Descriptions. In Proceedings ECAI92.
Koehler, J., Nebel, B., & Dimopoulos, Y. (1997). Extending Planning Graphs to an ADL
Subset. In Proceedings of 4th European Conference on Planning.
Liatsos, V., & Richards, B. (1997). Least Commitment: An Optimal Planning Strategy. In
Proceedings of the 16th Workshop of the UK Planning and Scheduling Special Interest
Group.
Long, D., & Fox, M. (in press). The Ecient Implementation of the Plangraph in stan. In
JAIR.
McCluskey, T. L., & Porteous, J. (1997). Engineering and Compiling Planning Domain
Models to Promote Validity and Eciency. Artificial Intelligence, 95 (1).
Morris, P., & Feldman, R. (1989). Automatically Derived Heuristics for Planning Search.
In Proceedings of the 2nd Irish Conference on Artificial Intelligence and Cognitive
Science, School of Computer Applications, Dublin City University.

421

fiJournal of Artificial Intelligence Research 9 (1998) 1{36

Submitted 1/98; published 8/98

The Computational Complexity of Probabilistic Planning
Michael L. Littman

mlittman@cs.duke.edu

Department of Computer Science, Duke University
Durham, NC 27708-0129 USA

Judy Goldsmith

goldsmit@cs.engr.uky.edu

Martin Mundhenk

mundhenk@ti.uni-trier.de

Department of Computer Science, University of Kentucky
Lexington, KY 40506-0046 USA
FB4 - Theoretische Informatik, Universitat Trier
D-54286 Trier, GERMANY

Abstract
We examine the computational complexity of testing and finding small plans in probabilistic planning domains with both at and propositional representations. The complexity
of plan evaluation and existence varies with the plan type sought; we examine totally
ordered plans, acyclic plans, and looping plans, and partially ordered plans under three
natural definitions of plan value. We show that problems of interest are complete for a
variety of complexity classes: PL, P, NP, co-NP, PP, NPPP, co-NPPP, and PSPACE. In
the process of proving that certain planning problems are complete for NPPP, we introduce
a new basic NPPP-complete problem, E-Majsat, which generalizes the standard Boolean
satisfiability problem to computations involving probabilistic quantities; our results suggest
that the development of good heuristics for E-Majsat could be important for the creation
of ecient algorithms for a wide variety of problems.

1. Introduction
Recent work in artificial-intelligence planning has addressed the problem of finding effective plans in domains in which operators have probabilistic effects (Drummond & Bresina,
1990; Mansell, 1993; Draper, Hanks, & Weld, 1994; Koenig & Simmons, 1994; Goldman &
Boddy, 1994; Kushmerick, Hanks, & Weld, 1995; Boutilier, Dearden, & Goldszmidt, 1995;
Dearden & Boutilier, 1997; Kaelbling, Littman, & Cassandra, 1998; Boutilier, Dean, &
Hanks, 1998). Here, an \effective" or \successful" plan is one that reaches a goal state
with sucient probability. In probabilistic propositional planning , operators are specified
in a Bayes network or an extended STRIPS-like notation, and the planner seeks a recipe
for choosing operators to achieve a goal configuration with some user-specified probability.
This problem is closely related to that of solving a Markov decision process (Puterman,
1994) when it is expressed in a compact representation.
In previous work (Goldsmith, Lusena, & Mundhenk, 1996; Littman, 1997a), we examined the complexity of determining whether an effective plan exists for completely observable
domains; the problem is EXP-complete in its general form and PSPACE-complete when limited to polynomial-depth plans. (A polynomial-depth, or polynomial-horizon, plan is one
that takes at most a polynomial number of actions before terminating.) For these results,
c 1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiLittman, Goldsmith & Mundhenk

plans are permitted to be arbitrarily large objects|there is no restriction that a valid plan
need have any sort of compact (polynomial-size) representation.
Because they place no restrictions on the size of valid plans, these earlier results are not
directly applicable to the problem of finding valid plans. It is possible, for example, that for
a given planning domain, the only valid plans require exponential space (and exponential
time) to write down. Knowing whether or not such plans exist is simply not very important
because they are intractable to express.
In the present paper, we consider the complexity of a more practical and realistic
problem|that of determining whether or not a plan exists in a given restricted form and of a
given restricted size. The plans we consider take several possible forms that have been used
in previous planning work: totally ordered plans, partially ordered plans, (totally ordered)
conditional plans, and (totally order) looping plans. In all cases, we limit our attention to
plans that can be expressed in size bounded by a polynomial in the size of the specification
of the problem. This way, once we determine that a plan exists, we can use this information
to try to write it down in a reasonable amount of time and space.
In the deterministic planning literature, several authors have addressed the computational complexity of determining whether a valid plan exists, of determining whether a plan
exists of a given cost, and of finding the valid plans themselves under a variety of assumptions (Chapman, 1987; Bylander, 1994; Erol, Nau, & Subrahmanian, 1995; Backstrom,
1995; Backstrom & Nebel, 1995). These results provide lower bounds (hardness results) for
analogous probabilistic planning problems since deterministic planning is a special case. In
deterministic planning, optimal plans can be represented by a simple sequence of operators
(a totally ordered plan). In probabilistic planning, a good conditional plan will often perform better than any totally ordered (unconditional) plan; therefore, we need to consider
the complexity of the planning process for a richer set of plan structures.
For ease of discussion, we only explicitly describe the case of planning in completely
observable domains. This means that the state of the world is known at all times during
plan execution, in spite of the uncertainty of state transitions. We know that the state of the
system is sucient information for choosing actions optimally (Puterman, 1994), however,
representing such a universal plan is often impractical in propositional domains in which
the size of the state space is exponential in the size of the domain representation. For this
reason, we consider other types of plan structures based on simple finite-state machines.
Because the type of plans we consider do not necessarily use the full state of the system to
make every decision, our results carry over to partially observable domains, although we do
not explore this fact in detail in the present work.
The computational problems we look at are complete for a variety of complexity classes
ranging from PL (probabilistic logspace) to PSPACE. Two results are deserving of special
mention because they concern problems closely related to ones being actively addressed
by artificial-intelligence researchers; first, the problem of evaluating a totally ordered plan
in a compactly represented planning domain is PP-complete.1 A compactly represented
1. The class PP is closely related to the somewhat more familiar #P; Toda (1991) showed that P#P = PPP .
Roughly speaking, this means that #P and PP are equally powerful when used as oracles. The counting
class #P has already been recognized by the artificial-intelligence community as an important complexity
class in computations involving probabilistic quantities, such as belief-network inference (Roth, 1996).

2

fiComplexity of Probabilistic Planning

planning domain is one that is described by a two-stage temporal Bayes network (Boutilier
et al., 1998) or similar notation.
Second, the problem of determining whether a valid totally ordered plan exists for a
compactly represented planning domain is NPPP -complete. Whereas the class NP can be
thought of as the set of problems solvable by guessing the answer and checking it in polynomial time, the class NPPP can be thought of as the set of problems solvable by guessing the
answer and checking it using a probabilistic polynomial-time (PP) computation. It is likely
that NPPP characterizes many problems of interest in the area of uncertainty in artificial
intelligence; this paper and earlier work (Goldsmith et al., 1996; Mundhenk, Goldsmith, &
Allender, 1997a; Mundhenk, Goldsmith, Lusena, & Allender, 1997b) give initial evidence
of this.

1.1 Planning-Domain Representations
A probabilistic planning domain M = hS ; s0 ; A; t; Gi is characterized by a finite set of states
S , an initial state s0 2 S , a finite set of operators or actions A, and a set of goal states
G  S . The application of an action a in a state s results in a probabilistic transition

to a new state s0 according to the probability transition function t, where t(s; a; s0 ) is the
probability that state s0 is reached from state s when action a is taken. The objective is to
choose actions, one after another, to move from the initial state s0 to one of the goal states
with probability above some threshold .2 The state of the system is known at all times
(fully observable) and so can be used to choose the action to apply.
We are concerned with two main representations for planning domains: at representations, which enumerate states explicitly, and propositional representations (sometimes
called compact, structured, or factored representations), which view states as assignments
to a set of Boolean state variables or propositions. Propositional representations can represent many domains exponentially more compactly than can at representations.
In the at representation, the transition function t is represented by a collection of
jSj  jSj matrices,3 one for each action. In the propositional representation, this type of
jSj  jSj matrix would be huge, so the transition function must be expressed another way.
In the probabilistic planning literature, two popular representations for propositional planning domains are probabilistic state-space operators (PSOs) (Kushmerick et al., 1995) and
two-stage temporal Bayes networks (2TBNs) (Boutilier et al., 1995). Although these representations differ in the type of planning domains they can express naturally (Boutilier et al.,
1998), they are computationally equivalent; a planning domain expressed in one representation can be converted in polynomial time to an equivalent planning domain expressed in
the other with at most a polynomial increase in representation size (Littman, 1997a).
In this work, we focus on a propositional representation called the sequential-effectstree representation (ST) (Littman, 1997a), which is a syntactic variant of 2TBNs with
conditional probability tables represented as trees (Boutilier et al., 1995, 1998). This representation is equivalent to 2TBNs and PSOs and simplifies the presentation of our results.
2. It is also possible to formulate the objective as one of maximizing expected total discounted reward (Boutilier et al., 1995), but the two formulations are essentially polynomially equivalent (Condon, 1992). The only diculty is that compactly represented domains may require discount factors
exponentially close to one for this equivalence to hold. This is discussed further in Section 5.
3. We assume that the number of bits used to represent the individual probability values isn't too large.

3

fiLittman, Goldsmith & Mundhenk

In ST, the effect of each action on each proposition is represented as a separate decision
tree. For a given action a, the set of decision trees for the different propositions is ordered,
so the decision tree for one proposition can refer to both the new and old values of previous propositions; this allows ST to represent any probability distribution. The leaves of a
decision tree describe how the associated proposition changes as a function of the state and
action, perhaps probabilistically. Section 1.2 gives a simple example of this representation.
As in other propositional representations, the states in the set of goal states G are not
explicitly enumerated in ST. Instead, we define a goal set , which is a set of propositions
such that any state in which all the goal-set propositions are true is considered a goal state.
The set of actions A is explicitly enumerated in ST, just as it is in the at representation.
The ST representation of a planning domain M = hS ; s0 ; A; t; Gi can be defined more
formally as M = hP; I; A; T; G i (we use blackboard-bold font to stand for an ST representation on a domain). Here, P is a finite set of distinct propositions. The set of states S is
the power set of P; the propositions in s 2 S are said to be \true" in s. The set I  P is
the initial state. The set G is the goal set, so the set of goal states G is the set of states s
such that G  s.
The transition function t is represented by a function T, which maps each action in
A to an ordered sequence of jPj binary decision trees. Each of these decision trees has a
distinct label proposition, decision propositions at the nodes (optionally labeled with the
sux \:new"), and probabilities at the leaves. The ith decision tree T(a)i for action a
defines the transition probabilities t(s; a; s0 ) as follows. For the ith decision tree, let pi be
its label proposition. Define i to be the value of the leaf node found by traversing decision
tree T(a)i , taking the left branch if the decision proposition is in s (or s0 if the decision
proposition has the \:new" sux) and the right branch otherwise. Finally, we let
Y  i;
if pi 2 s0 ,
0
t(s; a; s ) =
(1)
1 , i ; otherwise.
i

This definition of t constitutes a well-defined probability distribution over s0 for each a and
s.
To insure the validity of the representation, we only allow \p:new" to appear as a
decision proposition in T(a)i if p is the label proposition for some decision tree T(a)j for
j < i. For this reason, the order of the decision trees in T(a) is significant. To put this
another way, a proposition only has a new value after this new value has been defined by
some decision tree.
The complexity results we derive for ST apply also to PSOs, 2TBNs, and all other computationally equivalent representations. They also hold for the \succinct representation,"
a propositional representation popular in the complexity-theory literature, which captures
the set of transition matrices as a function, most commonly represented by a Boolean circuit
that computes that function. ST can straightforwardly be represented as a Boolean circuit,
and, in the proof of Theorem 6, we show how to represent particular Boolean circuits in
ST. Thus, although we have not shown that the succinct representation is formally equivalent to ST, the two representations are closely related; the proofs we give for ST need to
be changed only slightly to work for the succinct representation (Goldsmith, Littman, &
Mundhenk, 1997a, 1997b; Mundhenk et al., 1997b). Our results require that we restrict
the succinct representation to generate transition probabilities with at most a polynomial
4

fiComplexity of Probabilistic Planning

number of bits; the results may be different for other circuit-based representations that can
represent probabilities with an exponential number of bits (Mundhenk et al., 1997a).

1.2 Example Domain

To help make these domain-representation ideas more concrete, we present the following
simple probabilistic planning domain based on the problem of building a sand castle at the
beach. There are a total of four states in the domain, described by combinations of two
Boolean propositions, moat and castle (propositions appear in boldface). The proposition
moat signifies that a moat has been dug in the sand, and the proposition castle signifies
that the castle has been built. In the initial state, both moat and castle are false, and the
goal set is fcastleg.
There are two actions: dig-moat and erect-castle (actions appear in sans serif). Figure 1
illustrates these actions in ST. Executing dig-moat when moat is false causes moat to
become true with probability 1=2; if moat is already true, dig-moat leaves it unchanged.
The castle proposition in not affected. The dig-moat action is depicted in the left half of
Figure 1.
The second action is erect-castle, which appears in the right half of Figure 1. The decision
trees are numbered to allow sequential dependencies between their effects to be expressed.
The first decision tree is for castle, which does not change value if it is already true when
erect-castle is executed. Otherwise, the probability that it becomes true is dependent on
whether moat is true; the castle is built with probability 1=2 if moat is true and only
probability 1=4 if it is not. The idea here is that building a moat first protects the castle
from being destroyed prematurely by the ocean waves.
The second decision tree is for the proposition moat. Because erect-castle cannot make
moat become true, there is no effect when moat is false. On the other hand, if the moat
exists, it may collapse as a result of trying to erect the castle. The label castle:new in the
diagram refers to the value of the castle proposition after the first decision tree is evaluated.
If the castle was already built when erect-castle was selected, the moat remains built with
probability 3=4. If the castle had not been built, but erect-castle successfully builds it, moat
remains true. Finally, if erect-castle fails to make castle true, moat becomes false with
probability 1=2 and everything is destroyed.
Note that given an ST representation of a domain, we can perform a number of useful
operations eciently. First, given a state s and action a, we can generate a next state s0 with
the proper probabilities. This is accomplished by calculating the value of the propositions
of s0 one at a time in the order given in the representation of a, ipping coins with the
probabilities given in the leaves of the decision trees. Second, given a state s, action a,
and state s0 , we can compute t(s; a; s0 ), the probability that state s0 is reached from state s
when action a is taken, via Equation 1.

1.3 Plan Types and Representations

We consider four classes of plans for probabilistic domains. Totally ordered plans are the
most basic type, being a finite sequence of actions that must be executed in order; this
type of plan ignores the state of the system. Acyclic plans generalize totally ordered plans
to include conditional execution of actions. Partially ordered plans are a different way of
5

fiLittman, Goldsmith & Mundhenk

dig-moat
1: moat

2: castle

moat
T
1T

erect-castle
1: castle

castle
F

1/2 T

T
1T

2: moat

castle
F

T

0T

1T

moat
F

T

moat

F

castle

T

F

1/2 T

1/4 T

3/4 T

0T

F

T

castle: new
T

F

1T

1/2 T

Figure 1: Sequential-effects-tree (ST) representation for the sand-castle domain
generalizing totally ordered plans in which the precise sequence is left exible (McAllester
& Rosenblitt, 1991). Looping plans generalize acyclic plans to the case in which plan steps
can be repeated (Smith & Williamson, 1995; Lin & Dean, 1995). This type of plan is also
referred to as a plan graph or policy graph (Kaelbling et al., 1998).
In the following sections, we prove computational complexity results concerning each
of these plan types. The remainder of this section provides formal definitions of the plan
types, illustrated in Figure 2 with examples for the sand-castle domain.
In its most general form, a plan (or policy, controller or transducer) is a program that
outputs actions and takes as input information about the outcome of these actions. In this
work, we consider only a particularly restricted finite-state-controller-based plan representation.
A plan P for a planning domain M = hS ; s0 ; A; t; Gi can be represented by a structure
(V; v0 ; E; ; ) consisting of a directed (multi) graph (V; E ) with initial node v0 2 V , a
labeling  : V ! A of plan nodes|called plan steps |to domain actions, and a labeling
of edges with state sets  : E ! P (S ) such that for every v 2 V with outgoing edges,
S
0
v 2V :(v;v )2E (v; v ) = S and (v; v1 ) \ (v; v2 ) = ; for all v1 ; v2 2 V , v1 6= v2 . Some plan
steps have no outgoing edges at all|these are the terminal steps . Actions for terminal
steps are not executed. Note that the function  can be represented in a direct manner for
at domains, but for propositional domains, a more compact representation is needed. We
assume that for propositional domains, edge labels are given as conjunctions of literals.
The behavior of plan P in domain M is as follows. The initial time step is t = 0. At time
step t  0, the domain is in state st and the plan is at step vt (s0 is defined by the planning
domain, v0 by the plan). Action (vt ) is executed, resulting in a transition to domain state
st+1 with probability t(st ; (vt ); st+1 ). Plan step vt+1 is chosen so that st+1 2 (vt ; vt+1 );
the function  tells the plan where to \go" next. At this point, the time-step index t is
incremented and the process repeats. This continues until a terminal step is reached in the
plan.
One can understand the behavior of domain M under plan P in several different ways.
The possible sequences of states of M can be viewed as a tree: each node of the tree at
depth t is a state reachable from the initial state at time step t. Alternatively, one can view
the state of M at time step t under plan P as a probability distribution over S . At time
0

0

6

fiComplexity of Probabilistic Planning

step 0, with probability 1 the process is in state s0 . The probability that M is in state s0
at time step t + 1, Pr(s0 ; t + 1), is the sum of the probabilities of all length t + 1 paths from
s0 to s0, i.e.,

X

t
Y

s0 ;s1 ;s2 ;:::;st;st+1=s j =1

t(sj ; aj ; sj+1 );

0

where aj is the action selected by plan P at time j given the observed sequence of state
transitions s0 ; : : : ; sj . This view is useful in some of the later proofs.
Next, we formalize the probability that domain M reaches a goal state under plan P .
We need to introduce several notions. A \legal" sequence of states and steps applied is
called a trajectory , i.e., for M and P this is a sequence ff = h(si ; vi )iki=0 of pairs with

 t(si; (vi ); si+1 ) > 0 for 0  i  k , 1,
 si+1 2 (vi ; vi+1 ) for 0  i  k , 1, and
 v0 ; : : : ; vk,1 are not terminal steps.
A goal trajectory is a trajectory that ends in a goal state of M , sk 2 G . Note that
each goal trajectory is finite.Q Thus, we can calculate the probability of a goal trajectory
,1 t(s ; (v ); s ), given that s 2 G . The probability that
ff = h(si ; vi )iki=0 as Pr(ff) = ki=0
i
i i+1
k
M reaches a goal state under plan P is the sum of the probabilities of goal trajectories for
M,
X
Pr(M reaches a goal state under P ) :=
Pr(ff);
ff goal trajectory

we call this the value of the plan.
We characterize a plan P = (V; v0 ; E; ; ) on the basis of the size and structure of its
underlying graph (V; E ). If the graph (V; E ) contains no cycles, we call it an acyclic plan ,
otherwise it is a looping plan . It follows that an acyclic plan has a terminal step, and that
a terminal step will be reached after no more than jV j actions are taken; such plans can
only be used for finite-horizon control. A totally ordered plan (sometimes called a \linear
plan" or a \straight line" plan) is an acyclic plan with no more than one outgoing edge for
each node in V . Such a plan is a simple path.
In this work, we also consider partially ordered plans (sometimes called \nonlinear"
plans) that express an entire family of totally ordered plans. In this representation, the steps
of the plan are given as a partial order (specified, for example, as a directed acyclic graph).
This partial order represents a set of totally ordered plans: all totally ordered sequences of
plan steps consistent with the partial order that consist of all steps of the partially ordered
plan. Each of these totally ordered plans has a value, and these values need not all be the
same. As such, we have a choice in defining the value for a partially ordered plan. In this
work, we consider the optimistic, pessimistic, and average interpretations. Let 
(P ) be the
set of totally ordered sequences consistent with partial order plan P . Under the optimistic
interpretation,
The value of P := max Pr(M reaches a goal state under p):
p2
(P )

7

fiLittman, Goldsmith & Mundhenk

Under the pessimistic interpretation,
The value of P := min Pr(M reaches a goal state under p):
p2
(P )

Under the average interpretation,
X
Pr(M reaches a goal state under p):
The value of P := j
(1P )j
p2
(P )
To illustrate these notions, Figure 2 gives plans of each type for the sand-castle domain
described earlier. Initial nodes are marked an incoming arrow, and terminal steps are
represented as filled circles. The 3-step totally ordered plan in Figure 2(a) successfully
builds a sand castle with probability 0:4375. An acyclic plan is given in Figure 2(b), which
succeeds with probability 0:46875 and executes dig-moat an average of 1:75 times. Note
that it succeeds more often and with fewer actions on average than the totally ordered plan
in Figure 2(a).
Figure 2(c) illustrates a partially ordered plan for the sand-castle domain. While this
plan bears a superficial resemblance to the acyclic plan in Figure 2(b), it has a different
interpretation. In particular, the plan in Figure 2(c) represents a set of totally ordered plans
with five (non-terminal) plan steps (3 dig-moat steps and 2 erect-castle steps). In contrast
to the solid arrows in Figure 2(b), which indicate ow of control, the dashed arrows in
Figure 2(c) represent ordering constraints: each erect-castle step must be preceded by at
least two dig-moat steps,for example.
Although there are 52 = 10 distinct ways of arranging the five plan steps in Figure 2(c) into a totally ordered plan, only two distinct totally ordered plans are consistent
with the ordering constraints:
dig-moat ! dig-moat ! dig-moat ! erect-castle ! erect-castle ! 

(success probability 0:65625) and

dig-moat ! dig-moat ! erect-castle ! dig-moat ! erect-castle ! 

(success probability 0:671875). Thus, the optimistic success probability of this partially
ordered plan is 0:671875, the pessimistic 0:65625. Note that the pessimistic interpretation is closely related to the standard interpretation in deterministic partial order planning (McAllester & Rosenblitt, 1991), in which a partially ordered plan is considered successful only if all its consistent totally ordered plans are successful. The average success
probability is 0:6614583, here, because there are 4 orderings that yield the poorer plan
described above, and 2 that yield the better one.
The looping plan in Figure 2(d) does not terminate until it succeeds in building a sand
castle, which it will do with probability 1:0 eventually. Of course, not all looping plans
succeed with probability 1; the totally ordered plan in Figure 2(a) and the acyclic plan in
Figure 2(b) are special cases of such looping plans, for instance.
We define jP j the size of a plan P to be the number of steps it contains. We define jM j
the size of a domain M to be the sum of the number of actions and states for a at domain
and the sum of the sizes of the ST decision trees for a propositional domain.
8

fiComplexity of Probabilistic Planning

dig-moat

dig-moat

erect-castle

dig-moat

erect-castle

dig-moat

erect-castle

dig-moat

(a) A totally ordered plan.

(c) A partially ordered plan.
not(moat)
dig-moat

not(moat)
dig-moat

moat

dig-moat

erect-castle

not(moat)

moat and not(castle)

moat

moat
dig-moat

castle
erect-castle

(b) An acyclic (conditional) plan.
not(moat) and not(castle)
(d) A looping plan.

Figure 2: Example plans for the sand-castle domain
We consider the following decision problems. The plan-evaluation problem asks, given
a domain M , a plan P of size jP j  jM j, and threshold , whether its value is greater than
, i.e., whether
Pr(M reaches a goal state under P ) > :
Note that the condition that jP j  jM j is just a technical one|we simply want to use jM j
to represent the size of the problem. Given an instance in which jP j is larger than jM j, we
simply imagine \padding out" jM j to make it larger. The important thing is that we are
considering plans that are roughly the size of the description of the domain, and not the
size of the number of states (which might be considerably larger).
The plan-existence problem asks, given domain M , threshold , and size bound z  jM j,
whether there exists a plan P of size z with value greater than . Note that because we
bound the size of the target plan, the complexity of plan generation is no more than that of
plan existence; the technique of self-reduction can be used to construct a valid plan using
polynomially many calls to an oracle for the decision problem.
Each of these decision problems has a different version for each type of domain (at
and propositional) and each type of plan category (looping, acyclic, totally ordered, and
partially ordered under each of the three interpretations). We address all of these problems
in the succeeding sections.

1.4 Complexity Classes

For definitions of complexity classes, reductions, and standard results from complexity
theory, we refer the reader to Papadimitriou (1994).
Briey, we are looking only at the complexity of decision problems (those with yes/no
answers). The class P consists of problems that can be decided in polynomial time; that is,
given an instance of the problem, there is a program for deciding whether the answer is yes
or no that runs in polynomial time. The class NP contains the problems with polynomialtime checkable polynomial-size certificates: for any given instance and certificate, it can be
checked in time polynomial in the size of the instance whether the certificate proves that
the instance is in the NP set. This means that, if the answer to the instance is \yes," this
9

fiLittman, Goldsmith & Mundhenk

can be shown in polynomial time given the right key. The class co-NP is the opposite|if
the answer is \no," this can be shown in polynomial time given the right key.
A problem X is C -hard for some complexity class C if every problem in C can be reduced
to it; to put it another way, a fast algorithm for X can be used as a subroutine to solve any
problem in C quickly. A problem is C -complete if it is both C -hard and in C ; these are the
hardest problems in the class.
In the interest of being complete, we next give more detailed descriptions of the less
familiar probabilistic and counting complexity classes we use in this work.
The class #L (A lvarez & Jenner, 1993) is the class of functions f such that, for some
nondeterministic logarithmically space-bounded machine N , the number of accepting paths
of N on x equals f (x). The class #P is defined analogously as the class of functions f
such that, for some nondeterministic polynomial-time -bounded machine N , the number of
accepting paths of N on x equals f (x). Typical complete problems are computing the
determinant for #L and computing the permanent for #P.
A function f is defined to be in GapL if it is the difference f = g , h of #L functions g
and h. While #L functions have nonnegative integer values by definition, GapL functions
may have negative integer values (for example, if g always returns zero).
Probabilistic logspace (Gill, 1977), PL, is the class of sets A for which there exists a
nondeterministic logarithmically space-bounded machine N such that x 2 A if and only if
the number of accepting paths of N on x is greater than its number of rejecting paths. In
the original definition of PL, there is no time bound on computations; Borodin, Cook, and
Pippenger (1983) later showed PL  P. Jung (1985) proved that any set computable in
probabilistic logspace is computable in probabilistic logspace where the PL machine has a
simultaneous polynomial-time bound. In apparent contrast to P-complete sets, sets in PL
are decidable using very fast parallel computations (Borodin et al., 1983).
Probabilistic polynomial time, PP, is defined analogously. A classic PP-complete problem is Majsat: given a Boolean formula in conjunctive normal form (CNF), does the
majority of assignments satisfy it? According to Balcazar, Daz, and Gabarro (1990), the
PP-completeness of Majsat was shown in a combination of results from Gill (1977) and
Simon (1975).
For polynomial-space-bounded computations, PSPACE equals probabilistic PSPACE,
and #PSPACE is the same as the class of polynomial-space-computable functions (Ladner,
1989).
Note that L, NL, #L, PL and GapL are to logarithmic space what P, NP, #P, PP, and
GapP are to polynomial time. Also, the notion of completeness we use in this paper relies
on many-one reductions. In the case of PL, the reduction functions are logarithmic space;
in the case of NP and above, they are polynomial time.
For any complexity classes C and C 0 the class C C consists of those sets that are C -Turing
reducible to sets in C 0 , i.e., sets that can be accepted with resource bounds specified by C ,
using some problem in C 0 as a subroutine (oracle) with instantaneous output. For any class
C  PSPACE, it is the case that NPC  PSPACE, and therefore NPPSPACE = PSPACE.
The primary oracle-defined class we consider is NPPP . It equals the \NP
m " closure of
PP (Toran, 1991), which can be seen as the closure of PP under polynomial-time disjunctive reducibility with an exponential number of queries (each of the queries computable in
polynomial time from its index in the list of queries). To simplify our completeness results
0

10

fiComplexity of Probabilistic Planning

for this class, we introduce a decision problem we call E-Majsat (\exists" Majsat), which
generalizes the standard NP-complete satisfiability problem and the PP-complete Majsat.
An E-Majsat instance is defined by a CNF Boolean formula  on n Boolean variables
x1 ; : : : ; xn and a number k between 1 and n. The task is to decide whether there is an
initial partial assignment to variables x1 ; : : : ; xk so that the majority of assignments that
extend that partial assignment satisfies . We prove that this problem is NPPP -complete
in the Appendix.
The complexity classes we consider satisfy the following containment properties and
relations to other well-known classes:
NP  PP  NPPP  PSPACE  EXP:
L  NL  PL  P  co-NP
co-NPPP
Because P is properly contained in EXP, EXP-complete problems are provably intractable;
the other classes may equal P, although that is not generally believed to be the case.
Several other observations are worth making here. It is also known that PH  NPPP ,
where PH represents the polynomial hierarchy. In a crude sense, PH is close to PSPACE,
and, thus, our NPPP {completeness results place important problems close to PSPACE.
However, some early empirical results (Littman, 1997b) show that random problem instances from PP have similar properties to random problem instances from NP, suggesting
that PP might be close enough to NP for NP-type heuristics to be effective.

1.5 Results Summary

Tables 1 and 2 summarize our results, which are explained in more detail in later sections.
The general avor of our main results and techniques can be conveyed as follows. To
show that a plan-evaluation problem is in a particular complexity class C , we take the
cross product of the steps of the plan and the states of the domain and then look at the
complexity of evaluating the absorption probability of the resulting Markov chain (i.e., the
directed graph with probability-labeled edges). The complexity of the corresponding planexistence problem is then bounded by NPC , because the problem can be solved by guessing
the correct plan non-deterministically and then evaluating it; in many cases, it is NPC complete. The appropriate complexity class C depends primarily on the representation of
the cross-product Markov chain.
Exceptions to this basic pattern are the results for partially ordered plans in Section 4.
These appear to require a distinct set of techniques.
It is also worth noting that, although propositional domains can be exponentially more
compact than at domains, the computational complexity of solving problems in propositional domains is not always exponentially greater; in one instance, evaluating partially
ordered plans under the average interpretation, the complexity is actually the same for at
and propositional domains!
We also prove results concerning plan evaluation and existence for compactly represented
plans (PP-complete and NPPP -complete, Corollary 5), plan existence of \large enough"
looping plans in at domains (P-complete, Theorem 7), plan evaluation and existence
for looping plans in deterministic propositional domains (PSPACE-complete, Theorems 8
and 9), and plan existence for polynomial-size looping plans in partially observable domains
(NP-complete, Section 5.1).
11

fiLittman, Goldsmith & Mundhenk

Plan Type
Plan Evaluation Plan Existence
unrestricted
|
P-complete
polynomial-depth
|
P-complete
looping
PL-complete
NP-complete
acyclic
PL-complete
NP-complete
totally ordered
PL-complete
NP-complete
partially ordered, optimistic
NP-complete
NP-complete
partially ordered, average
PP-complete
NP-complete
partially ordered, pessimistic co-NP-complete
NP-complete

Reference
P & T (1987)
P & T (1987)
Section 3
Section 2
Section 2
Section 4
Section 4
Section 4

Table 1: Complexity results for at representations (P & T (1987) is Papadimitriou and
Tsitsiklis (1987))

Plan Type
Plan Evaluation Plan Existence Reference
unrestricted
|
EXP-complete Littman (1997a)
polynomial-depth
|
PSPACE-complete Littman (1997a)
looping
PSPACE-complete PSPACE-complete Section 3
acyclic
PP-complete
NPPP -complete Section 2
totally ordered
PP-complete
NPPP -complete Section 2
PP
partially ordered, optimistic
NP -complete
NPPP -complete Section 4
partially ordered, average
PP-complete
NPPP -complete Section 4
PP
partially ordered, pessimistic co-NP -complete NPPP -complete Section 4
Table 2: Complexity results for propositional representations

12

fiComplexity of Probabilistic Planning

2. Acyclic Plans

In this section, we treat the complexity of generating and evaluating acyclic and totally
ordered plans.

Theorem 1 The plan-evaluation problem for acyclic and totally ordered plans in at domains is PL-complete.

Proof: First, we show PL-hardness for totally ordered plans. Jung (1985) proved that a
set A is in PL if and only if there exists a logarithmically space-bounded and polynomially
time-bounded nondeterministic Turing machine N with the following property: For every
input x, machine N must have at least half of its computations on input x be accepting
if and only if x is in A. The machine N can be transformed into a probabilistic Turing
machine R such that for each input x, the probability that R(x) accepts x equals the
fraction of computations of N (x) that accepted. Given R, a planning domain M can be
described as follows. The state set of M is the set of configurations of R on input x. Note
that a configuration consists of the contents of the logarithmically space-bounded tape, the
state, the location of the read/write heads, and one symbol each from the input and output
tapes. Thus, a configuration can be represented with logarithmically many bits, and there
are only polynomially many such configurations. The state-transition probabilities of M
under the unique action a are the configuration transition probabilities of R. All states
obtained from accepting configurations are goal states. The totally ordered plan consists
of a \step counter" for R on input x, and each of its plan steps takes the only action a.
The probability that the planning domain under this plan reaches a goal state is exactly
the probability that R(x) reaches an accepting configuration. Thus, evaluating this totally
ordered plan is PL-hard.
Since totally ordered plans are acyclic plans, this also proves PL-hardness of the planevaluation problem for acyclic plans.
Next, we show that the plan-evaluation problem is in PL for acyclic plans. Let M =
hS ; s0 ; A; t; Gi be a planning domain, let P = hV; v0 ; E; ; i be an acyclic plan, and let
threshold  be given. We show how our question, whether the probability that M under P
reaches a goal state with probability greater than , can be equivalently transformed into
the question of whether a GapL function is greater than 0. The transformation can be done
in logarithmic space. As shown by Allender and Ogihara (1996), it follows that our question
is in PL.
At first, we construct a Markov chain C from M and P , which simulates the execution
or \evaluation" of M under P . Note that a Markov chain can be seen as a probabilistic
domain with only one action in its set of actions. Since there is no choice of actions, we do
not mention them in this construction. The state space of C is S  V , the initial state is
(s0 ; v0 ), the set of goal states is G  V , and the transition probabilities tC for C are
8
t(s; (v); s0 ); if s0 2 (v; v0 );
<
tC ((s; v); (s0 ; v0 )) = : 1;
if v is a terminal step node, and (s; v) = (s0 ; v0 );
0;
otherwise.
Let m be the number of plan steps of P (i.e., jV j, the number of nodes in the graph
representing P ). Since states of C that contain a terminal step of P are sinks in C , it follows
13

fiLittman, Goldsmith & Mundhenk

that
Pr(M reaches a goal state under P ) = Pr(C reaches a goal state in exactly m steps):
Let
pC (s; m) := Pr(C reaches a goal state in exactly m steps from initial state s):
Then, pC ((s0 ; v0 ); m) is the probability we want to calculate. The standard inductive definition of pC used to evaluate plans by dynamic programming is

s is a goal state of C ,
pC (s; 0) = 10;; ifotherwise,
X
pC (s; k + 1) =
tC (s; s0)  pC (s0 ; k); 0  k  m , 1:
s 2SV
0

Let h be the maximum length of the representation of a state-transition probability tC .
Then, for

if s is a goal state of C ,
ph (s; 0) = 10;; otherwise,
X h
ph(s; k + 1) =
2  tC (s; s0 )  ph (s0 ; k); 0  k  m , 1;
s 2SV
0

it follows that pC ((s0 ; v0 ); m) = ph((s0 ; v0 ); m)  2,hm . Note that ph((s0 ; v0 ); m) is an integer
value. Therefore, pC ((s0 ; v0 ); m) >  if and only if ph ((s0 ; v0 ); m) , b2hm c > 0. In order
to show that pC ((s0 ; v0 ); m) >  is decidable in PL, it suces to show that ph((s0 ; v0 ); m)
is in GapL. Therefore, we \unwind" the inductive definition of ph. Let T be the integer
matrix obtained from tC with T(s;s ) = tC (s; s0 )  2h . We introduce the integer-valued T to
show that ph can be composed from GapL functions using compositions under which GapL
is closed; as tC is not integer valued, it cannot be used to show this. We can write
0

ph(s; m) =

X

s 2SV

(T m )(s;s )  ph(s0 ; 0):
0

0

We argue that ph is in GapL. Each entry T(s;s ) is logspace computable from the domain M and plan P . Therefore, the powers of the matrix are in GapL, as shown by
Vinay (1991). Because GapL is closed under multiplication and summation of polynomially
many summands, it follows that ph 2 GapL. Finally, we use closure properties of GapL
from Allender and Ogihara (1996); since GapL is closed under subtraction, it follows that
the plan-evaluation for acyclic plans is in PL.
Because totally ordered plans are acyclic plans, the plan-evaluation problem for totally
ordered plans is also in PL.
0

The technique of forming a Markov chain by taking the cross product of a domain and
a plan will be useful later. Plan-existence problems require a different set of techniques.

Theorem 2 The plan-existence problem for acyclic and totally ordered plans in at domains is NP-complete.

14

fiComplexity of Probabilistic Planning

Proof: First, we show containment in NP. Given a planning domain M , a threshold ,
and a size bound z  jM j, guess a plan of the correct form of size at most z and accept if

and only if M reaches a goal state with probability greater than  under this plan. Note that
checking whether a plan has the correct form can be done in polynomial time. Because the
plan-evaluation problem is in PL (Theorem 1), it follows that the plan-existence problem
is in NP (i.e., it is in NPPL = NP).
To show the NP-hardness of the plan-existence problem, we give a reduction from the
NP-complete satisfiability problem for Boolean formulae in conjunctive normal form. We
construct a planning domain that evaluates a Boolean formula with n variables, where a
(n + 2)-step plan describes an assignment of values to the variables. In the first step, a
clause is chosen randomly. At step i + 1, the planning domain \checks" whether the plan
satisfies the appearance of variable i in that clause. If so, the clause is marked as satisfied.
After n + 1 steps, if no literal was satisfied in that clause, then no goal state is reached
through this clause, otherwise, a transition is made to the goal state. Therefore, the goal
state will be reached with probability 1 (greater than 1 , 1=m) if and only if all clauses are
satisfied|the plan describes a satisfying assignment.
We formally define the reduction, which is similar to one presented by Papadimitriou
and Tsitsiklis (1987). Let  be a CNF formula with n variables x1 ; : : : ; xn and m clauses
C1 ; : : : ; Cm . Let the sign of an appearance of a variable in a clause be ,1 if the variable is
negated, and 1 otherwise. Define the planning domain M () = hS ; s0 ; A; t; Gi where

S
A
G

= fsat(i; j ); unsat(i; j ) j 1  i  n + 1; 1  j  mg [ fs0 ; sacc ; srejg;
= fassign(i; b) j 1  i  n; b 2 f,1; 1gg [ fstart; endg;
= fsacc g;
8 1
0
>
m ; if s = s0 ; a = start; s0 = unsat(1; j ); 1  j  m;
>
>
>
1; if s = s0 ; a 6= start; s = srej;
>
>
>
>
>
1; if s = unsat(i; j ); a = assign(i; b); s0 = sat(i + 1; j ); i  n;
>
>
>
>
xi appears in Cj with sign b;
>
>
>
>
1
;
if
s
= unsat(i; j ); a = assign(i; b); s0 = unsat(i + 1; j ); i  n;
>
>
>
>
xi does not appear in Cj with sign b;
>
>
>
>
1
;
if
s
= unsat(i; j ); a = assign(i0 ; b) or a = start or a = end;
>
>
<
0
srej; i0 6= i  n; b 2 f,1; 1g;
t(s; a; s0) = > 1; if ss =
= unsat(n + 1; j ); s0 = srej;
>
>
>
>
1; if s = sat(i; j ); a = assign(i; b); s0 = sat(i + 1; j ); i  n;
>
>
>
>
1; if s = sat(i; j ); a = assign(i0 ; b) or a = start or a = end;
>
>
>
>
s0 = srej; i0 6= i  n;
>
>
>
>
1; if s = sat(n + 1; j ); a = end; s0 = sacc ;
>
>
>
>
1; if s = sat(n + 1; j ); a 6= end; s0 = srej;
>
>
>
>
s = s0 = srej or s = s0 = sacc ;
>
>
: 10;; ifotherwise.
The meaning of the states in this domain is as follows. When the domain is in state
sat(i; j ) for 1  i  n, 1  j  m, it means the formula has been satisfied, and we are
currently checking variable i in clause j . State sat(n + 1; j ) for all 1  j  m means that
we've finished verifying clause j and it was satisfied. The meanings are similar for the
15

fiLittman, Goldsmith & Mundhenk

s0

start

1=2

unsat(1; 1)

assign(1; 1)

assign(1;

sat(2; 1)

assign(2; x)

1=2

,1)

start

assign(1;

,1)

sat(3; 1)

assign(2; 1)

sat(3; 2)

assign(3; x)

sat(4; 1)

unsat(4; 1)

end

end

sacc

unsat(2; 2)

assign(2; x)

unsat(3; 1)

assign(3; x)

assign(1; 1)

sat(2; 2)

unsat(2; 1)

assign(2;

unsat(1; 2)

,1)

unsat(3; 2)

assign(3; 1)
sat(4; 2)

end

assign(3;

,1)

unsat(4; 2)

end

srej

Figure 3: A domain generated from the Boolean formula (x1 _ :x2 ) ^ (:x1 _ x3 )
\unsat" states. Of course, s0 is the initial state and sacc and srej are the accepting and
rejecting states, respectively.
The actions in this domain are start and end, which mark the beginning and end of the
assignment, and assign(i; b) for 1  i  n, b 2 f,1; 1g, which assign the truth value b to
variable i. Figure 3 gives the domain generated by this reduction from a simple Boolean
formula. By the description of the reduction, M () can be computed from  in time
polynomial in jj.
By construction, M () under z = (n + 2)-step plan P can only reach goal state sacc if
P has the form
start ! assign(1; b1 ) ! assign(2; b2 ) !    ! assign(n; bn ) ! end ! :
P reaches sacc with probability 1 if and only if b1 ; : : : ; bn is a satisfying assignment for the
n variables in . This shows that Boolean satisfiability polynomial-time reduces to the
plan-existence problem for totally ordered and acyclic plans, showing that it is NP-hard.
Note that if we bound the plan depth (horizon) instead of the plan size, the planexistence problem for acyclic plans in at domains is P-complete (Goldsmith et al., 1997a;
Papadimitriou & Tsitsiklis, 1987). Limiting the plan size makes the problem more dicult
because it is possible to force the planner to take the same action from different states;
figuring out how to do this without sacrificing plan quality is very challenging.
In propositional domains, plan evaluation is harder because of the large number of states.

Theorem 3 The plan-evaluation problem for acyclic and totally ordered plans in propositional domains is PP-complete.

Proof: To show PP-hardness for totally ordered plans, we give a reduction from the

PP-complete problem Majsat: given a CNF Boolean formula , does the majority of
assignments satisfy it?
16

fiComplexity of Probabilistic Planning

evaluate
1: xi
1/2

2: xi

T

1/2

T

n+1: clause1
xa1:new
T

F

xb1:new

1T

T
1

T

T
0

T

T
1

T

1T

xbm:new

clause1:new

T

T

xcm:new
T

1T

0

F

F

1T

1T

n+m+2: done

done
T

F

T

F

n+m+1: satisfied

xam:new

...

xc1:new

xd1:new

1/2 T

n+m: clausem

F

T

n: xi

...

T

F
0

F

clause2:new
...

T

0T

F
0

T

clausem:new

F

T

1T

1

F
T

0T

Figure 4: Sequential-effects-tree representation for evaluate
Given , we construct a planning domain M () and a 1-step plan such that the plan
achieves the goal with probability greater than  = 1=2 if and only if the majority of
assignments satisfies . The planning domain M () consists of a single action evaluate,
which is also the 1-step plan to be evaluated. There are n + m + 2 propositions in M ();
x1 through xn, which correspond to the n variables of ; clause1 through clausem , which
correspond to the m clauses of ; satisfied, which is also the sole element of the goal set;
and done, which insures that evaluate is only executed once (this is important when this
domain is used later in Theorem 4 to show the complexity of plan existence). In the initial
state, all propositions are false.
The evaluate action generates a random assignment to the variables of , evaluates the
clauses (clausei is true if any of the literals in the ith clause is true), and evaluates the entire
formula (satisfied is true if all the clauses are true). Figure 4 gives an ST representation
of evaluate, in which xa ; xb ; : : : represent the variables in clause i.
By construction,  is in Majsat if and only if M () reaches a goal state with probability
greater than  = 1=2 under the plan consisting of the single action evaluate.
We next show membership in PP for acyclic plans. We do this by showing that a
planning domain M and an acyclic plan P induce a computation tree consisting of all
paths through M under P . Evaluating this computation tree can be accomplished by a PP
machine.
Let b be a bound on the number of bits used to specify probabilities in the leaves of the
decision trees representing M .4 Consider a computation tree defined as follows. It has root
labeled hs0 ; v0 i. If, in the planning domain M , the probability of reaching state s0 from s
i

i

4. We represent numbers in polynomial-precision binary representation. In principle, this could introduce
round-off errors if planning problems are specified in some other form.

17

fiLittman, Goldsmith & Mundhenk

given action (v) is equal to  , then hs; (v)i will have   2b children labeled hs0 ; (v; s0 )i.
Each of the identically labeled child nodes is independent but is defined identically to the
others. Thus, the number of paths with a given set of labels corresponds to the probability
of that trajectory through the domain and plan multiplied by (2b )h , where h is the depth
of the plan.
The number of accepting computations is, therefore, more than   (2b )h if and only if
the probability of achieving the goal is more than . Note that b is inherent in the planning
domain, rather than in h. A PP machine accepts if more than half of the final states are
accepting, so if  6= 1=2, it will be necessary to pad the computation tree by introducing
\dummy" branches that accept or reject in the right proportions.
The plan-existence problem is essentially equivalent to guessing and evaluating a valid
plan.

Theorem 4 The plan-existence problem for acyclic and totally ordered plans in propositional domains is NPPP -complete.

Proof: Containment in NPPP for both totally ordered and for acyclic plans follows from

the fact that a polynomial-size plan can be guessed in polynomial time and checked in PP
(Theorem 3).
Hardness for NPPP for both totally ordered and acyclic plans can be shown using a
reduction from E-Majsat, shown NPPP -hard in the Appendix. The reduction echoes the
one used in the PP-hardness argument in the proof of Theorem 3.
Given a CNF Boolean formula  with variables x1 ; : : : ; xn , and a number k, we construct
a planning domain M (; k) such that a plan exists that can reach the goal with probability
greater than  = 1=2 if and only if there is an assignment to the variables x1 ; : : : ; xk such
that the majority of assignments to the remaining variables satisfies . The planning domain
M (; k ) consists of the action evaluate from Theorem 3 and one action, set-xi , for each of
the first k variables. Just as in the proof of Theorem 3, there are n + m + 2 propositions in
M (; k ), all initially false: x1 through xn , which correspond to the n variables of ; clause1
through clausem , which correspond to the m clauses of ; satisfied; and done, which
insures that evaluate is only executed once. The goal set contains satisfied and done.
For 1  i  k, action set-xi makes proposition xi true. Analogously to Theorem 3, the
evaluate action generates a random assignment to the remaining variables of , evaluates
the clauses (clausei is true if any of the literals in the clause is true), and evaluates the
entire formula (satisfied is true if all the clauses are true), and sets done to true. If done
is true, no further action can make satisfied true.
If the pair ; k is in E-Majsat, then there exists an assignment b1 : : : bk to the first k
variables of  such that the majority of assignments to the rest of the variables satisfies .
Therefore, the plan applying steps set-xi for all i with bi = 1 followed by an evaluate action
reaches a goal state with probability greater than  = 1=2.
Conversely, assume M (; k) under totally ordered plan P reaches a goal state with
probability greater than 1=2. Since the evaluate action is the only action setting done to
true, and since no action reaches the goal once done is set to true, we can assume without
loss of generality that P consists of a sequence of steps set-xi that ends with evaluate. By
construction, the assignment to x1 ; : : : ; xk assigning 1 exactly to those variables set by P
18

fiComplexity of Probabilistic Planning

is an assignment under which the majority of the assignments to the rest of the variables
satisfies , and therefore ; k is in E-Majsat.
Since every totally ordered plan is acyclic, the same hardness holds for acyclic plans.
In the above results, we consider both at and compactly represented (propositional)
planning domains but only at plans. Compactly represented plans are also quite useful.
A compact acyclic plan is an acyclic plan in which the names of the plan steps
are encoded by a set of propositional variables and the step-transition function
 between plan steps is represented by a set of decision trees, just as in ST. We
require that the plan has depth polynomial in the size of the representation,
even though the total number of steps in the plan might be exponential due to
the logarithmic succinctness of the encodings.
Because the plan-domain cross-product technique used in the proof of Theorem 3 generalizes to compact acyclic plans, the same complexity results apply. This also holds true
for a probabilistic acyclic plan , which is an acyclic plan that can make random transitions
between plan steps (i.e., the step-transition function  is stochastic). These insights can be
combined to yield the following corollary of Theorems 3 and 4.

Corollary 5 The plan-evaluation problem for compact probabilistic acyclic plans in propositional domains is PP-complete and the plan-existence problem for compact probabilistic
acyclic plans in propositional domains is NPPP -complete.

We mention probabilistic plans here for two reasons. First, the behavior of some planning structures (such as partially ordered plan evaluation under the average interpretation,
discussed in Section 4) can be thought of as generating probabilistic plans. Second, there
are many instances in which simple probabilistic plans perform nearly as well as much larger
and more complicated deterministic plans; this notion is often exploited in the field of randomized algorithms. Work by Platzman (1981) (described by Lovejoy, 1991) shows how the
idea of randomized plans can come in handy for planning in partially observable domains.

3. Looping Plans

Looping plans can be applied to infinite-horizon control. The complexity of plan existence
and plan evaluation in at domains (Theorems 1 and 2) does not depend on the presence
or absence of loops in the plan.

Theorem 6 The plan-evaluation problem for looping plans in at domains is PL-complete.
Proof: Given a domain M and a looping plan P , we can construct a product Markov

chain C as in the proof of Theorem 1. As in the proof of Theorem 6 of Allender and
Ogihara (1996), this chain can be constructed such that it has exactly one accepting and
exactly one rejecting state; both of these states are absorbing. The probability that M
reaches a goal state under P equals the probability that C reaches its accepting state if
started in its initial state, which is the product of the initial states of M and P . In the
19

fiLittman, Goldsmith & Mundhenk

proof of Theorem 6 of Allender and Ogihara (1996), it is shown that the construction of
the Markov chain and the computation of whether it reaches its final state with probability
greater than  can be performed in PL.
PL-hardness is implied by Theorem 1, since acyclic plans are a special case of looping
plans.

Theorem 7 The plan-existence problem for looping plans in at domains is NP-complete

in general, but P-complete if the size of the desired plan is at least the size of the state or
action space (i.e., z  min(jSj; jAj)).

Proof sketch: NP-completeness follows from the proof of Theorem 2; containment and

hardness still hold if plans are permitted to be looping.
However, this is only true if we are forced to specify a plan whose size is small with
respect to the size of the domain. If our looping plan is allowed to have a number of states
that is at least as large as the number of states or actions in the domain, the problem can
be solved in polynomial time.
It is known that for Markov decision processes such as these the maximum probability
of reaching a goal state equals the maximum probability of reaching a goal state under
any infinite-horizon stationary policy , where a stationary policy is a mapping from states
to actions that is used repeatedly to choose actions at each time step. It is known that
such an optimal stationary policy can be computed in polynomial time via linear programming (Condon, 1992). Any stationary policy for a domain M = hS ; s0 ; A; G ; ti can be
written as a looping plan, although, of course, not all looping plans correspond to stationary
policies.
We show that for any fixed stationary policy p : S ! A, there are two simple ways a
looping plan P = (V; v0 ; E; ; ) can be represented. First, let V = A, v0 = p(s0 ), (v) = v,
and (v; v0 ) = fs 2 S j p(s) = v0 g. It follows that whenever M reaches state s, then the
action applied according to the looping plan is the same as according to P .
Second, let V = S , v0 = s0 , (v) = p(v), and (v; v0 ) = fv0 g. It follows that whenever
M reaches state s, the plan will be at the node corresponding to that state and, therefore,
the appropriate action for that state will be applied by the looping plan. Therefore, the
maximum probability of reaching a goal state can be obtained by either of these looping
plans.
Since the best stationary policy can be computed in polynomial time, the best looping
plan can be computed in polynomial time, too. P-hardness follows from a theorem of
Papadimitriou and Tsitsiklis (1987).
In propositional domains, the complexity of plan existence and plan evaluation of looping
plans is quite different from the acyclic case. Looping plan evaluation is very hard.

Theorem 8 The plan-evaluation problem for looping plans in both deterministic and stochastic propositional domains is PSPACE-complete.

Proof: Recall that the plan-evaluation problem for at domains is in PL (Theorem 1).

For a planning domain with cn states and a representation of size n, a looping plan can
20

fiComplexity of Probabilistic Planning

be evaluated in probabilistic space O(log(cn )) (Theorem 6), which is to say probabilistic
space polynomial in the size of the input. This follows because the ST representation of
the domain can be used to compute entries of the transition function t in polynomial space.
Since probabilistic PSPACE equals PSPACE, this shows that the plan-evaluation problem
for looping plans in stochastic propositional domains is in PSPACE.
It remains to show PSPACE-hardness for deterministic propositional domains. Let N
be a deterministic polynomial-space-bounded Turing machine. The moment-to-moment
computation state (configuration) of N can be expressed as a polynomial-length bit string
that encodes the contents of the Turing machine's tape, the location of the read/write head,
the state of N 's finite-state controller, and whether or not the machine is in an accepting
state.
For any input x, we describe how to construct in polynomial time a deterministic planning domain M (x) and a single-action looping plan that reaches a goal state of M (x) if and
only x is accepted by Turing machine N .
Given a description of N and x, one can, in time polynomial in the size of the descriptions
of N and x, produce a description of a Turing machine T that computes the transition
function for N . In other words, T on input c, a configuration of N , outputs the next
configuration of N . (In fact, T can even check whether c is a valid configuration in the
computation of N (x) by simulating that computation.) By an argument similar to that
used in Cook's theorem, T can be modeled by a polynomial-size circuit. This circuit takes
as input the bit string describing the current configuration of N and outputs the next
configuration.
Next, we argue that the computation of this circuit can be expressed by an action compute in ST representation. There is one proposition in M (x) for each bit in the configuration,
plus one for each gate of the circuit. The three standard gates, \and," \or," and \not" are
all easily represented as decision trees. By ordering the decision trees in compute according to a topological sort of the gates of the circuit, a single compute action can compute
precisely the same output as the circuit. Figure 5 illustrates this conversion for a simple
circuit, which gives the form of the \not" (i1 ), \and" (i2 ), and \or" (i3 ) gates.
We can now describe the complete reduction. The planning domain M (x) consists of
the single action compute and the set of propositions described in the previous paragraph.
The initial state is the initial configuration of the Turing machine N , and the goal set is the
proposition corresponding to whether or not the configuration is an accepting state for N .
Because all transitions are deterministic and only one action can be chosen, it follows
that the goal state is reached with probability 1 (greater than 1=2, for example) under
the plan that repeatedly chooses compute until an accepting state is reached if and only if
polynomial-space machine N on input x accepts.
A similar argument shows that looping plan existence is not actually any harder than
looping plan evaluation.

Theorem 9 The plan-existence problem for looping plans in both deterministic and stochastic propositional domains is PSPACE-complete.
21

fiLittman, Goldsmith & Mundhenk

compute
1: i1

2: i2

c2
c1

c2

not
i1

or
i3
and

c3

not

T

i2

c2

F

c3

1T

1T

0T

T

F

c2

1T

F

T

0T

F

1T

0T

or

4: c1
c1

T

T
and

c1

i1:new
F

0T

3: i3

5: c2
i2:new

c1

c3
T

F

i2:new
T
1T

6: c3

0T

T
0T

F
1T

i3:new
T

F

i2:new

1T

T

F
0T

1T

F
0T

Figure 5: A circuit and its representation as a sequential-effects tree

Proof: Hardness for PSPACE follows from the same construction as in the proof of

Theorem 8: either the one-step looping plan is successful, or it is not. No other plan yields
a better result.
Recall that we are only interested in determining whether there is a plan of size z , where
z is bounded by the size of the domain, that reaches the goal with a given probability. The
problem is in PSPACE because the plan can be guessed in polynomial time and checked in
PSPACE (Theorem 8). Because NPPSPACE = PSPACE, the result follows.
As we mentioned earlier, the unrestricted infinite-horizon plan-existence problem is
EXP-complete (Littman, 1997a); this shows the problem of determining unrestricted plan
existence is EXP-hard only because some domains require plans that are larger than polynomialsize looping plans.
Because Theorem 9 shows PSPACE-completeness for determining plan existence in deterministic domains, it is closely related to the PSPACE-completeness result of Bylander (1994). The main difference between the two results is that our theorem applies to
more compact plans (polynomial instead of exponential) with more complex operator descriptions (conditional effects instead of preconditions with add and delete lists) that can
include loops. Also, as the proofs above show, PSPACE-hardness is retained even in planning domains with only one action, so it is the looping that makes looping plans hard to
work with.

4. Partially Ordered Plans
Partially ordered plans are a popular representation because they allow planning algorithms
to defer a precise commitment to the ordering of plan steps until it becomes necessary in
22

fiComplexity of Probabilistic Planning

the planning process. A k-step partially ordered plan corresponds to a set of k-step totally
ordered plans|all those that are consistent with the given partial order. The evaluation of
a partially ordered plan can be defined to be the evaluation of the best, worst, or average
member of the set of consistent totally ordered plans; these are the optimistic, pessimistic,
and average interpretations, respectively.
The plan-evaluation problem for partially ordered plans is different from that of totally
ordered plans. This is because a single partial order can encode all totally ordered plans.
Hence, evaluating a partially ordered plan involves figuring out the best (in case of optimistic
interpretation) or the worst (for pessimistic interpretation) member, or the average (for
average interpretation) of this combinatorial set.

Theorem 10 The plan-evaluation problem for partially ordered plans in at domains is
NP-complete under the optimistic interpretation.
Proof sketch: Membership in NP follows from the fact that we can guess any totally

ordered plan consistent with the given partial order and accept if and only if the domain
reaches a goal state with probability more than . Remember that this evaluation can be
performed in PL (Theorem 1), and therefore deterministically in polynomial time.
The hardness proof is a variation of the construction used in Theorem 2. The partiallyordered plan to evaluate has the form given in Figure 6; the consistent total orders are of
the form
start ! assign(1; b1 ) ! assign(1; ,b1 ) ! assign(2; b2 ) ! assign(2; ,b2 ) !

   ! assign(n; bn) ! assign(n; ,bn) ! end ! ;
where bi is either 1 or ,1. Each of the possible plans can be interpreted as an assignment

to n Boolean variables by ignoring every second assignment action. The construction in
Theorem 2 shows how to turn a CNF formula  into a planning domain M (), and it
can easily be modified to ignore every second action. Thus, the best totally ordered plan
consistent with the given partially ordered plan reaches the goal with probability 1 if and
only if it reaches the goal with probability greater than 1 , 2,m if and only if it satisfies all
clauses of  if and only if  is satisfiable.

Theorem 11 The plan-evaluation problem for partially ordered plans in at domains is
co-NP-complete under the pessimistic interpretation.

Proof sketch: Both the proof of membership in co-NP and the proof of hardness are

very similar to the proof of Theorem 10. We show a reduction from the co-NP-complete set
of unsatisfiable formulae in CNF. The plan to evaluate has the form given in Figure 6
and is interpreted as above. As in the proof of Theorem 2, we construct a planning domain
M 0(), but we take G = fsrejg as goal states, where the state srej is reached with probability
greater than 0 if and only if the assignment does not satisfy one of the clauses of formula .
A formula is unsatisfiable if and only if under every assignment at least one of the clauses
is not satisfied. Therefore, the probability that M 0 () reaches a goal state under a given
totally ordered plan is greater than 0 if and only if the plan corresponds to an unsatisfying
Sat

23

fiLittman, Goldsmith & Mundhenk

start

assign(1,1)

assign(1,-1)

assign(2,1)

assign(2,-1)

assign(3,1)

assign(3,-1)

...
assign(n,1)

assign(n,-1)

end

Figure 6: A partially ordered plan that can be hard to evaluate
assignment. Finally, the minimum of that probability over all consistent partially ordered
plans is greater than 0 if and only if  is unsatisfiable.

Theorem 12 The plan-evaluation problem for partially ordered plans in at domains is

PP-complete under the average interpretation.
Proof: Under the average interpretation, we must decide whether the average evaluation
over all consistent totally ordered plans is greater than threshold . This can be decided in
PP by guessing uniformly a totally ordered plan and checking its consistency with the given
partially ordered plan in polynomial time. If the guessed totally ordered plan is consistent,
it can be evaluated in polynomial time (Theorem 1) and accepted or rejected as appropriate.
If the guessed plan is inconsistent, the computation accepts with probability  and rejects
with probability 1 , , leaving the average over the consistent orderings unchanged with
respect to the threshold .
The PP-hardness is shown by a reduction from the PP-complete Majsat. Let  be a
formula in CNF. We show how to construct a domain M () and a partially ordered plan
P () such that  2 Majsat if and only if the average performance of M () under a totally
ordered plan consistent with P () is greater than 1=2.
Let  consist of the m clauses C1 ; : : : ; Cm , which contain n variables x1 ; : : : ; xn . Domain
M () = hS ; s0 ; A; t; Gi has actions
A = fassign(i; b) j i 2 f1; : : : ; ng; b 2 f,1; 1gg [ fstart; check; endg:
Action assign(i; b) will be interpreted as \assign sign b to xi ." The partially ordered plan
P () has plan steps
V = f(i; b; h) j i 2 f1; : : : ; ng; b 2 f,1; 1g; h 2 f1; : : : ; mgg [ fstart; check; endg
and mapping  : V ! A with
() =  for  2 fstart; check; endg, and ((i; b; h)) = assign(i; b):
24

fiComplexity of Probabilistic Planning

The order E requires that a consistent plan has start as the first and end as the last step.
The steps in between are arbitrarily ordered. More formally,
E = f(start; q) j q 2 V , fstart; endgg [ f(q; end) j q 2 V , fstart; endgg:
Now, we define how the domain M () acts on a given totally ordered plan P consistent
with P (). Domain M () consists of the cross product of the following polynomial-size
deterministic domains Ms and M , to which a final probabilistic transition will be added.
Before we describe Ms and M precisely, here are their intuitive definitions. The domain
Ms is satisfied by plans that have the form of an assignment to the n Boolean variables with
the restriction that the assignment is repeated m times (for easy checking). The domain
M is satisfied by plans that correspond to satisfying assignments. The composite of these
two domains is only satisfied by plans that correspond to satisfying assignments. We will
now define these domains formally.
First, Ms checks whether the totally ordered plan matches the regular expression
start (assign(1; 0)m jassign(1; 1)m )
   (assign(n; 0)m jassign(n; 1)m )
check ((assign(1; 0)jassign(1; 1))    (assign(n; 0)jassign(n; 1)))m :
Note that the m here is a constant. Let \good" be the state reached by Ms if the plan
matches that expression. Otherwise, the state reached is \bad". To clarify, the actions before check are there simply to \use up" the extra steps not used in specifying the assignment
in the partially ordered plan.
Next, M checks whether the sequence of actions following the check action satisfies
the clauses of  in the following sense. Let a1    ak be this sequence. M interprets each
subsequence a1+(j ,1)n    an+(j ,1)n with al+(j ,1)m = assign(x; bl ) as assignment b1 ; : : : ; bn
to the variables x1 ; : : : ; xn , and checks whether this assignment satisfies clause Cj . If all
single clauses are satisfied in this way, then M reaches state \satisfied".
Note that Ms and M are defined so that they do not deal with the final end action.
M () consists of the product domain of Ms and M with the transitions for action end
as follows. If M is in state (bad; q) for any state q of M , then action end lets M go
probabilistically to state \accept" or to state \reject", with probability 1=2 each; if M is
in state (good; satisfied), the M under action end goes to state \accept" (with probability
1); otherwise, M under action end goes to state reject (with probability 1). The set of goal
states of M consists of the only state \accept".
We analyze the behavior of M () under any plan P consistent with P (). If Ms under
P reaches state \bad", then M () under P reaches a goal state with probability 1=2. Now,
consider a plan P under which Ms reaches the state \good"|called a good plan. Then
P matches the above regular expression. Therefore, for every i 2 f1; : : : ; mg there exists
bi 2 f,1; 1g such that all steps s(i; bi ; h) are between start and check. Thus, all steps
between check and end are
s(1; 1 , i1 ; 1)    s(n; 1 , in ; 1)s(1; 1 , i1 ; 2)    s(n; 1 , in; m)
Consequently, the sequence of actions defined by the labeling of these plan steps are
(assign(1; i1 )assign(2; i2 )    assign(n; in ))m :
25

fiLittman, Goldsmith & Mundhenk

This means, that M checks whether all clauses of  are satisfied by the assignment i1    in ,
i.e., M checks whether i1    in satisfies . Therefore, M () accepts under plan P with
probability 1, if the plan represents a satisfying assignment, and with probability 0 otherwise.
Note that each assignment corresponds to exactly one good plan. Therefore, the average
over all good plans that M () accepts equals the fraction of satisfying assignments of .
Since M () accepts under \bad" plans with probability 1=2, this yields that the average
over all plans consistent with P () of the acceptance probabilities of M () is greater than
1=2 if and only if  2 Majsat.
The complexity of the plan-existence problem for partially ordered plans is identical to
that for totally ordered plans.

Theorem 13 The plan-existence problem for partially ordered plans in at domains is NPcomplete under the pessimistic, optimistic and average interpretations. The plan-existence
problem for partially ordered plans in propositional domains is NPPP -complete under the
pessimistic, optimistic and average interpretations.

Proof: First, note that a totally ordered plan is a special type of partially ordered plan

and its evaluation is unchanged under the pessimistic, optimistic, or average interpretation.
In particular, because there is only one ordering consistent with a given totally ordered
plan, the best, worst, and average orderings are all the same. Therefore, if there exists a
totally ordered plan with value greater than , then there is a partially ordered plan with
value greater than  (the same plan), under all three interpretations.
Conversely, if there is a partially ordered plan with value greater than  under any of
the three interpretations, then there is a totally ordered plan with value greater than .
This is because the value of the best, worst, and average ordering of a partially ordered
plan is always a lower bound on the value of the best consistent totally ordered plan.
Given this strong equivalence, the complexity of plan existence for partially ordered
plans is a direct corollary of Theorems 2 and 4.
The pattern for partially ordered plan evaluation in at domains is that the average
interpretation is no easier to decide than either the optimistic or pessimistic interpretations.
In propositional domains, the pattern is the opposite: the average interpretation is no harder
to decide than either the optimistic or pessimistic interpretations.

Theorem 14 The plan-evaluation problem for partially ordered plans in propositional do-

mains is NPPP -complete under the optimistic interpretation, co-NPPP -complete under the
pessimistic interpretation, and PP-complete under the average interpretation.

Proof sketch: For the optimistic interpretation, membership in NPPP follows from the

fact that we can guess a single suciently good consistent total order and evaluate it in
PP (Theorem 3). Hardness for NPPP can be shown using a straightforward reduction from
E-Majsat (as in the proof of Theorem 4).
For the pessimistic interpretation, membership in co-NPPP follows from the fact that we
can guess the worst consistent total order and evaluate it in PP (Theorem 3). Hardness for
26

fiComplexity of Probabilistic Planning

co-NPPP can be shown by reducing to it the co-NPPP version of E-Majsat (E-Majsat);
the proof is a simple adaptation of the techniques used, for example, in Theorem 4 above.
For the average interpretation, the problem can be shown to be in PP by combining
the argument in the proof of Theorem 12 showing how to average over consistent totally
ordered plans with the argument in the proof of Theorem 3 showing how to evaluate a
plan in a propositional domain in PP. Alternatively, we could express the evaluation of a
partially ordered plan under the average interpretation as a compact probabilistic acyclic
plan; Corollary 5 states that such plans can be evaluated in PP. PP-hardness follows directly
from Theorem 3, because totally ordered plans are a special case of partially ordered plans
and evaluating totally ordered plans is PP-hard.

5. Applications
To help illustrate the utility of our results, this section cites several planners from the
literature and analyzes the computational complexity of the problems they attack. We do
not give detailed explanations of the planners themselves; for this, we refer the reader to
the original papers. We focus on three planning systems: witness (Brown University),
buridan (University of Washington), and treeplan (University of British Columbia). In
the process of making connections to these planners, we also describe how our work relates to
the discounted-reward criterion, partial observability, other domain representations, partial
order conditional planning, policy-based planning, and approximate planning.

5.1 Witness

The witness algorithm (Cassandra, Kaelbling, & Littman, 1994; Kaelbling et al., 1998)
solves at partially observable Markov decision processes using a dynamic-programming approach. The basic algorithm finds optimal unrestricted solutions to finite-horizon problems.
Papadimitriou and Tsitsiklis (1987) showed that the plan-existence problem for polynomialhorizon partially observable Markov decision processes is PSPACE-complete.
As an extension to their finite-horizon algorithm, Kaelbling et al. (1998) sketch a method
for finding optimal looping plans for some domains. Although this is not presented as a
formal algorithm, it is not unreasonable to say that the pure form of the problem that this
extended version of witness attacks is one of finding a valid polynomial-size looping plan
for a partially observable domain. The similarities between this problem and that described
in Section 3 are that the domains are at and that the plans are identical in form. The
apparent differences are that witness optimizes a reward function instead of probability
of goal satisfaction and that witness works in partially observable domains whereas our
results are defined in terms of completely observable domains. Both of these apparent
differences are insignificant, however, from a computational complexity point of view.
First, witness attempts to maximize the expected total discounted reward over an
infinite horizon (sometimes called optimizing a time-separable value function). As argued
by Condon (1992), any problem defined in terms of a sum of discounted rewards can be
recast as one of goal satisfaction. The argument proceeds roughly as follows. Let 0 <  < 1
be the discount factor and R(s; a) be the immediate reward received for taking action a in
state s.
27

fiLittman, Goldsmith & Mundhenk

Define

s ;a R(s0 ; a0 )
R0(s; a) = max R(s;Ra()s0,; amin
0 ) , min R(s0 ; a0 ) :
0

s ;a
0

0

s ;a

0

0

0

From this, we have that 0  R0 (s; a)  1 for all s and a and that the value of any plan with
respect to the revised reward function is a simple linear transformation of its true value.
Now, we introduce an auxiliary state g to be the goal state and create a new transition
function t0 such that t0 (s; a; g) = (1 ,  )R0 (s; a) and t0 (s; a; s0 ) = (1 , (1 ,  )R0 (s; a))t(s; a; s0 )
for s0 6= g; t0 is a well-defined transition function and the probability of goal satisfaction for
any plan under transition function t0 is precisely the same as the expected total discounted
reward under reward function R0 and transition function t. Thus, any problem stated as
one of optimizing the expected total of discounted immediate rewards can be turned into an
equivalent problem of optimizing goal satisfaction with only a slight change to the transition
function and one additional state. This means there is no fundamental computational
complexity difference between these two different types of planning objectives.
The second apparent difference between the problem solved by the extended witness
algorithm and that described in Section 3 is that of partial versus complete observability.
In fact, our results do address partial observability, albeit indirectly. In our formulation of
the plan-existence problem, plans are constrained to make no conditional branches (in the
totally ordered and partially ordered cases), or to branch only on distinctions made by the
step-transition function  (in the acyclic and looping cases); these two choices correspond
to unobservable and partially observable domains, respectively. In a partially observable
domain, the plan-existence problem becomes one of finding a valid polynomial-size finitestate controller subject to the given observability constraints. Nothing in our complexity
proofs depends on the presence or absence of additional observability constraints. Therefore,
it is a direct corollary of Theorem 2 that the plan-existence problem for polynomial-horizon
plans in unobservable domains is NP-complete (Papadimitriou & Tsitsiklis, 1987) and of
Theorem 7 that the plan-existence problem for polynomial-size looping plans in partially
observable domains is NP-complete (this is a new result).
It is interesting to note that the computational complexity of searching for size-bounded
plans in partially observable domains is generally substantially less than that of solving the
corresponding unconstrained partially observable Markov decision process. For example,
we found that the plan-existence problem for acyclic plans in propositional domains is
NPPP -complete (Theorem 4). The corresponding unconstrained problem is that of determining the existence of a history-dependent policy for a polynomial-horizon, compactly
represented partially observable Markov decision process, which is EXPSPACE-complete
(Theorem 4.15 of Goldsmith et al., 1996, or Theorem 6.8 of Mundhenk et al., 1997b). The
gap here is enormous: EXPSPACE is to EXP what PSPACE is to P, and EXP is already
provably intractable in the worst case. In contrast to EXPSPACE-complete problems, it is
conceivable that good heuristics for NPPP -complete problems can be created by extensions
of recent advances in heuristics for NP-complete problems. Therefore, there is some hope
of devising effective planning algorithms by building on the observations in this paper and
searching for optimal size-bounded plans instead of optimal unrestricted plans; in fact, recent planners for both propositional domains (Majercik & Littman, 1998a, 1998b) and at
domains (Hansen, 1998) are motivated by these results.
28

fiComplexity of Probabilistic Planning

Domain Type
at
propositional
at
propositional

Horizon Type
polynomial
polynomial
infinite
infinite

Size-Bounded Plan Unrestricted Plan
NP-complete
NPPP -complete
NP-complete
PSPACE-complete

PSPACE-complete
EXPSPACE-complete
undecidable
undecidable

Table 3: Complexity results for plan existence in partially observable domains
Table 3 summarizes complexity results for planning in partially observable domains.
The results for size-bounded plans are corollaries of Theorems 2, 4, 7, and 9 of this paper. The results for unrestricted plans are due to Papadimitriou and Tsitsiklis (1987)
(at, polynomial), Goldsmith et al. (1996) (propositional, polynomial), and Hanks (1996)
(infinite-horizon). This last result is derived by noting the isomorphism of the infinitehorizon problem to the emptiness problem for probabilistic finite-state automata, which is
undecidable (Rabin, 1963).

5.2 Buridan

The buridan planner (Kushmerick et al., 1995) finds partially ordered plans for propositional domains in the PSO representation. There are two identifiable differences between
the problem solved by buridan and the problem analyzed in Section 4: the representation
of planning problems and the fact that buridan is not restricted to find polynomial-size
plans. We address each of these differences below.
Although, on the surface, PSO is different from ST, either can be converted into the
other in polynomial time with at most a polynomial increase in domain size. In particular,
the effect of an action in PSO is represented by a single decision tree consisting of proposition
nodes (like ST) and random nodes (easily simulated in ST using auxiliary propositions).
At the leaves are a list of propositions that become true and another list of propositions
that become false should that leaf be reached. This type of correlated effect is also easily
represented in ST using the chain rule of probability theory to decompose the probability
distribution into separate probabilities for each proposition and careful use of the \:new"
sux. Thus, any PSO domain can be converted to a similar size ST domain quickly.
Similarly, a domain in ST can be converted to PSO with at most a polynomial expansion.
This conversion is too complex to sketch here, but follows from the proof of equivalence between ST and a simplified representation called IF (Littman, 1997a). Given the polynomial
equivalence between ST and PSO, any complexity results for ST carry over to PSO.5
The results described in this paper concern planning problems in which a bound is given
on the size of the plan sought. Although Kushmerick et al. (1995) do not explicitly describe
their planner as one that prefers small plans to large plans, the design of the planner as
one that searches through the space of plans makes the notion of plan size central to the
algorithm. Indeed, the public-domain buridan implementation uses plan size as part of a
best-first search procedure for identifying a suciently successful plan. This means that, all
other things being equal, shorter plans will be found before larger plans. Furthermore, to
assure termination, the planner only considers a fixed number of plans before halting, thus
5. To be more precise, this is true for complexity classes closed under log-space reductions.

29

fiLittman, Goldsmith & Mundhenk

putting a limit indirectly on the maximum allowable plan size. So, although buridan does
not attempt to solve precisely the same problem that we considered, it is fair to say that the
problem we consider is an idealization of the problem attacked by buridan. Regardless,
our lower bounds on complexity apply to buridan.
Kushmerick et al. (1995) looked at generating suciently successful plans under both
the optimistic interpretation and the pessimistic interpretation. They also explicitly examined the plan-evaluation problem for partially ordered plans under both interpretations.
Therefore, Theorems 13 and 14 apply to buridan.
The more sophisticated c-buridan planner (Draper et al., 1994) extends buridan to
plan in partially observable domains and to produce plans with conditional execution. The
results of our work also shed light on the computational complexity of the problem addressed
by c-buridan. Draper et al. (1994) devised a representation for partially ordered acyclic
(conditional) plans. In this representation, each plan step generates an observation label as
a function of the probabilistic outcome of the step. Each step also has an associated set of
context labels dictating the circumstances under which that step must be executed. A plan
step is executed only if its context labels are consistent with the observation labels produced
in earlier steps. In its totally ordered form, this type of plan can be expressed as a compact
acyclic plan; Corollary 5 can be used to show that the plan-evaluation and plan-existence
problems for a totally ordered version of c-buridan's conditional plan representation in
propositional domains are PP-complete and NPPP -complete, respectively.
In our results above, we consider evaluating and searching for plans that are partially
ordered and plans that have conditional execution, but not both at once. Nonetheless, the
same sorts of techniques presented in this paper can be applied to analyzing the problems
attacked by c-buridan. For example, consider the plan-existence problem for c-buridan's
partially ordered conditional plans under the optimistic interpretation. This problem asks
whether there is a partially ordered conditional plan that has some total order that reaches
the goal with sucient probability. This is equivalent to asking whether there is a totally
ordered conditional plan that reaches the goal with sucient probability. Therefore, the
problem is NPPP -complete, by the argument in the previous paragraph.
In spite of many superficial differences between the problems analyzed in this paper and
those studied by the creators of the buridan planners, our results are quite relevant to
understanding their work.

5.3 Treeplan
A family of planners have been designed that generate a decision-tree-based representation
of stationary policies (mappings from state to action) (Boutilier et al., 1995; Boutilier &
Poole, 1996; Boutilier & Dearden, 1996) in probabilistic propositional domains; we refer
to these planners collectively as the treeplan planners. Once again, these planners solve
problems that are not identical to the problems addressed in this paper but are closely
related.
The planner described by Boutilier et al. (1995) finds solutions that maximize expected
total discounted reward in compactly represented Markov decision processes (the domain
representation used is expressively equivalent to ST). As mentioned earlier, the difference
between maximizing goal satisfaction and maximizing expected total discounted reward is a
30

fiComplexity of Probabilistic Planning

superficial one, so the problem addressed by this planner is EXP-complete (Littman, 1997a).
Although the policies used by Boutilier et al. (1995) appears quite dissimilar from the finitestate controllers described in our work, policies can be converted to a type of similarly
sized compact looping plan (an extension of the type of plan described in Corollary 5).
The conversion from stationary policies to looping plans is as described in the proof of
Theorem 7, except that the resulting plans are represented compactly.
In later work, Boutilier and Dearden (1996) show how it is possible to limit the size
of the representation of the policy in treeplan and still obtain approximately optimal
performance. This is necessary because, in general, the size of decision trees needed to
represent the optimal policies can be exponentially large. By keeping the decision trees
from getting too large, the resulting planner becomes subject to an extension of Theorem 9
and, therefore, attacks a PSPACE-complete problem.
One emphasis of Boutilier and Dearden (1996) is on finding approximately optimal
solutions, with the hope that doing so is easier than finding optimal solutions. We do
not explore the worst-case complexity of approximation in this paper, although Lusena,
Goldsmith, and Mundhenk (1998) have produced some strong negative results in this area.
A related issue is one of using simulation (random sampling) to find approximately optimal
solutions to probabilistic planning problems. Some empirical successes have been obtained
with the related approach of reinforcement learning (Tesauro, 1994; Crites & Barto, 1996),
but, once again, the worst-case complexity of probabilistic planning is not known to be any
lower for approximation by simulation.

6. Conclusions
In this paper, we explored the computational complexity of plan evaluation and plan existence in probabilistic domains. We found that, in compactly represented propositional
domains, restricting the size and form of the policies under consideration reduced the
computational complexity of plan existence from EXP-complete for unrestricted plans to
PSPACE-complete for polynomial-size looping plans and NPPP -complete for polynomialsize acyclic plans. In contrast, in at domains, restricting the form of the policies under
consideration increased the computational complexity of plan existence from P-complete
for unrestricted plans to NP-complete for totally ordered plans; this is because a plan that
is smaller than the domain in which it operates is often unable to exploit important Markov
properties of the domain. We were able to characterize precisely the complexity of all
problems we examined with regard to the current state of knowledge in complexity theory.
Several problems we studied turned out to be NPPP -complete. The class NPPP promises
to be very useful to researchers in uncertainty in artificial intelligence because it captures
the type of problems resulting from choosing (\guessing") a solution and then evaluating its
probabilistic behavior. This is precisely the type of problem faced by planning algorithms in
probabilistic domains, and captures important problems in other domains as well, such as
constructing explanations in belief networks and designing robust communication networks.
We provide a new conceptually simple NPPP -complete problem, E-Majsat, that may be
useful in further explorations in this direction.
The basic structure of our results is that if plan evaluation is complete for some class C ,
then plan existence is typically NPC -complete. This same basic structure holds in determin31

fiLittman, Goldsmith & Mundhenk

istic domains: evaluating a totally ordered plan in a propositional domain is P-complete (for
suciently powerful domain representations) and determining the existence of a polynomialsize totally ordered plan is NPP = NP-complete.
From a pragmatic standpoint, the intuition that searching for small plans is more efficient than searching for arbitrary size plans suggests that exact dynamic-programming
algorithms, which are so successful in at domains, may not be as effective in propositional
domains; they do not focus their efforts on the set of small plans. Algorithm-development
energy, therefore, might fruitfully be spent devising heuristics for problems in the class NPPP
as this class captures the essence of searching for small plans in probabilistic domains|some
early results in this direction are appearing (Majercik & Littman, 1998a, 1998b). Complexity theorists have only recently begun to explore classes such as NPPP that lie between the
polynomial hierarchy and PSPACE and algorithm designers have come to these classes even
more recently. As this paper marks the beginning of our exploration of this class of problems, much work is still to be done in probing algorithmic implications, but it is our hope
that heuristics for NPPP could lead to powerful methods for solving a range of important
uncertainty-sensitive combinatorial problems.

Acknowledgements
This work was supported in part by grants NSF-IRI-97-02576-CAREER (Littman), and
NSF CCR-9315354 (Goldsmith). We gratefully acknowledge Andrew Klapper, Anne Condon, Matthew Levy, Steve Majercik, Chris Lusena, Mark Peot, and our reviewers for helpful
feedback and conversations on this topic.

Appendix A. Complexity of E-Majsat
The E-Majsat problem is: given a pair (; k) consisting of a Boolean formula  of n
variables x1 ; : : : ; xn and a number 1  k  n, is there an assignment to the first k variables
x1 ; : : : ; xk such that the majority of assignments to the remaining n,k variables xk+1 ; : : : ; xn
satisfies ?
For k = n, this is precisely Boolean satisfiability, a classic NP-complete problem. This
is because we are asking whether there exists an assignment to all the variables that makes
 true. For k = 0, E-Majsat is precisely Majsat, a well-known PP-complete problem.
This is because we are asking whether the majority of all total assignments makes  true.
Deciding an instance of E-Majsat for intermediate values of k has a different character.
It involves both an NP-type calculation to pick a good setting for the first k variables and a
PP-type calculation to see if the majority of assignments to the remaining variables makes
 true. This is akin to searching for a good answer (plan, schedule, coloring, belief network
explanation, etc.) in a combinatorial space when \good" is determined by a computation
over probabilistic quantities. This is just the type of computation described by the class
NPPP , and we show next that E-Majsat is NPPP -complete.

Theorem 15

E-Majsat

is NPPP -complete.
32

fiComplexity of Probabilistic Planning

Proof: Membership in NPPP follows directly from definitions. To show completeness of
E-Majsat, we first observe (Tor
an, 1991) that NPPP is the NP
m -closure of the PP-complete

set Majsat. Thus, any NPPP computation can be modeled by a nondeterministic machine
N that, on each possible computation, first guesses a sequence s of bits that controls its
nondeterministic moves, deterministically performs some computation on input x and s,
and then writes down a formula qx;s with variables in z1 ; : : : ; zl as a query to Majsat.
Finally, N (x) with oracle Majsat accepts if and only if for some s, qx;s 2 Majsat.
Given any input x, like in Cook's Theorem, we can construct a formula x with variables
y1; : : : ; yk and z1; : : : ; zl such that for every assignment a1; : : : ; ak ; b1 ; : : : ; bl it holds that
x (a1 ; : : : ; ak ; b1 ; : : : ; bl ) = qx;a1a (b1 ; : : : ; bl ). Thus, (x ; k) 2 E-Majsat if and only if for
some assignment s to y1 ; : : : ; yk , qx;s 2 Majsat if and only if N (x) accepts.
k

References

Allender, E., & Ogihara, M. (1996). Relationships among PL, #L, and the determinant.
Theoretical Informatics and Applications, 30 (1), 1{21.
A lvarez, C., & Jenner, B. (1993). A very hard log-space counting class. Theoretical Computer
Science, 107, 3{30.
Backstrom, C. (1995). Expressive equivalence of planning formalisms. Artificial Intelligence,
76 (1{2), 17{34.
Backstrom, C., & Nebel, B. (1995). Complexity results for SAS+ planning. Computational
Intelligence, 11 (4), 625{655.
Balcazar, J., Daz, J., & Gabarro, J. (1988/1990). Structural Complexity I/II. EATCS
Monographs on Theoretical Computer Science. Springer Verlag.
Borodin, A., Cook, S., & Pippenger, N. (1983). Parallel computation for well-endowed
rings and space-bounded probabilistic machines. Information and Control, 58 (1{3),
113{136.
Boutilier, C., Dean, T., & Hanks, S. (1998). Decision theoretic planning: Structural assumptions and computational leverage. In preparation.
Boutilier, C., & Dearden, R. (1996). Approximating value trees in structured dynamic programming. In Saitta, L. (Ed.), Proceedings of the Thirteenth International Conference
on Machine Learning.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure in policy construction. In Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence, pp. 1104{1113.
Boutilier, C., & Poole, D. (1996). Computing optimal policies for partially observable
decision processes using compact representations. In Proceedings of the Thirteenth
National Conference on Artificial Intelligence, pp. 1168{1175. AAAI Press/The MIT
Press.
33

fiLittman, Goldsmith & Mundhenk

Bylander, T. (1994). The computational complexity of propositional STRIPS planning.
Artificial Intelligence, 69, 161{204.
Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally in partially
observable stochastic domains. In Proceedings of the Twelfth National Conference on
Artificial Intelligence, pp. 1023{1028 Seattle, WA.
Chapman, D. (1987). Planning for conjunctive goals. Artificial Intelligence, 32, 333{379.
Condon, A. (1992). The complexity of stochastic games. Information and Computation,
96 (2), 203{224.
Crites, R. H., & Barto, A. G. (1996). Improving elevator performance using reinforcement
learning. In Touretzky, D. S., Mozer, M. C., & Hasselmo, M. E. (Eds.), Advances in
Neural Information Processing Systems 8 Cambridge, MA. The MIT Press.
Dearden, R., & Boutilier, C. (1997). Abstraction and approximate decision-theoretic planning. Artificial Intelligence, 89 (1{2), 219{283.
Draper, D., Hanks, S., & Weld, D. (1994). Probabilistic planning with information gathering
and contingent execution. In Proceedings of the AAAI Spring Symposium on Decision
Theoretic Planning, pp. 76{82.
Drummond, M., & Bresina, J. (1990). Anytime synthetic projection: Maximizing the
probability of goal satisfaction. In Proceedings of the Eighth National Conference on
Artificial Intelligence, pp. 138{144. Morgan Kaufmann.
Erol, K., Nau, D. S., & Subrahmanian, V. S. (1995). Complexity, decidability and undecidability results for domain-independent planning. Artificial Intelligence, 76, 75{88.
Gill, J. (1977). Computational complexity of probabilistic Turing machines. SIAM Journal
on Computing, 6 (4), 675{695.
Goldman, R. P., & Boddy, M. S. (1994). Epsilon-safe planning. In Proceedings of the 10th
Conference on Uncertainty in Artificial Intelligence (UAI94), pp. 253{261 Seattle,
WA.
Goldsmith, J., Littman, M., & Mundhenk, M. (1997a). The complexity of plan existence and
evaluation in probabilistic domains. Tech. rep. CS-1997-07, Department of Computer
Science, Duke University.
Goldsmith, J., Littman, M. L., & Mundhenk, M. (1997b). The complexity of plan existence
and evaluation in probabilistic domains. In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI{97), pp. 182{189 San Francisco,
CA. Morgan Kaufmann Publishers.
Goldsmith, J., Lusena, C., & Mundhenk, M. (1996). The complexity of deterministically
observable finite-horizon Markov decision processes. Tech. rep. 268-96, Department
of Computer Science, University of Kentucky.
34

fiComplexity of Probabilistic Planning

Hanks, S. (1996). Decision-theoretic planning in unobservable domains is undecidable.
Personal communication.
Hansen, E. A. (1998). Finite-Memory Control of Partially Observable Systems. Ph.D. thesis,
University of Massachusetts.
Jung, H. (1985). On probabilistic time and space. In Proceedings 12th ICALP, pp. 281{291.
Lecture Notes in Computer Science, Springer-Verlag.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in
partially observable stochastic domains. Artificial Intelligence, 101 (1{2), 99{134.
Koenig, S., & Simmons, R. G. (1994). Risk-sensitive planning with probabilistic decision
graphs. In Proceedings of the 4th International Conference on Principles of Knowledge
Representation and Reasoning, pp. 363{373.
Kushmerick, N., Hanks, S., & Weld, D. S. (1995). An algorithm for probabilistic planning.
Artificial Intelligence, 76 (1-2), 239{286.
Ladner, R. (1989). Polynomial space counting problems. SIAM Journal on Computing, 18,
1087{1097.
Lin, S.-H., & Dean, T. (1995). Generating optimal policies for high-level plans with conditional branches and loops. In Proceedings of the Third European Workshop on
Planning, pp. 205{218.
Littman, M. L. (1997a). Probabilistic propositional planning: Representations and complexity. In Proceedings of the Fourteenth National Conference on Artificial Intelligence,
pp. 748{754. AAAI Press/The MIT Press.
Littman, M. L. (1997b). Solving large POMDPs: Lessons from complexity theory. Talk
presented at the DARPA AI Workshop in Providence, RI. Slides available at URL
http://www.cs.duke.edu/mlittman/talks/darpa97-pomdp.ps.
Lovejoy, W. S. (1991). A survey of algorithmic methods for partially observable Markov
decision processes. Annals of Operations Research, 28 (1), 47{65.
Lusena, C., Goldsmith, J., & Mundhenk, M. (1998). Nonapproximability results for Markov
decision processes. Tech. rep. UK CS Dept TR 275-98, University of Kentucky.
Majercik, S. M., & Littman, M. L. (1998a). MAXPLAN: A new approach to probabilistic
planning. In Simmons, R., Veloso, M., & Smith, S. (Eds.), Proceedings of the Fourth
International Conference on Artificial Intelligence Planning, pp. 86{93. AAAI Press.
Majercik, S. M., & Littman, M. L. (1998b). Using caching to solve larger probabilistic
planning problems. In Proceedings of the Fifteenth National Conference on Artificial
Intelligence, pp. 954{959. The AAAI Press/The MIT Press.
Mansell, T. M. (1993). A method for planning given uncertain and incomplete information.
In Proceedings of the 9th Conference on Uncertainty in Artificial Intelligence, pp.
350{358. Morgan Kaufmann Publishers.
35

fiLittman, Goldsmith & Mundhenk

McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings of
the 9th National Conference on Artificial Intelligence, pp. 634{639.
Mundhenk, M., Goldsmith, J., & Allender, E. (1997a). The complexity of policy-evaluation
for finite-horizon partially-observable Markov decision processes. In Proceedings of
22nd Symposium on Mathematical Foundations of Computer Science (published in
Lecture Notes in Computer Science). Springer-Verlag.
Mundhenk, M., Goldsmith, J., Lusena, C., & Allender, E. (1997b). Encyclopaedia of complexity results for finite-horizon Markov decision process problems. Tech. rep. UK CS
Dept TR 273-97, University of Kentucky.
Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley, Reading, MA.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). The complexity of Markov decision processes. Mathematics of Operations Research, 12 (3), 441{450.
Platzman, L. K. (1981). A feasible computational approach to infinite-horizon partiallyobserved Markov decision problems. Tech. rep. J-81-2, Georgia Institute of Technology,
Atlanta, GA.
Puterman, M. L. (1994). Markov Decision Processes|Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY.
Rabin, M. O. (1963). Probabilistic automata. Information and Control, 6 (3), 230{245.
Roth, D. (1996). On the hardness of approximate reasoning. Artificial Intelligence, 82 (1{2),
273{302.
Simon, J. (1975). On some central problems in computational complexity. Ph.D. thesis,
Cornell University. Also Cornell Department of Computer Science Technical Report
TR75-224.
Smith, D. E., & Williamson, M. (1995). Representation and evaluation of plans with loops.
Working notes for the 1995 Stanford Spring Symposium on Extended Theories of
Action.
Tesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves masterlevel play. Neural Computation, 6 (2), 215{219.
Toda, S. (1991). PP is as hard as the polynomial-time hierarchy. SIAM Journal on Computing, 20, 865{877.
Toran, J. (1991). Complexity classes defined by counting quantifiers. Journal of the ACM,
38 (3), 753{774.
Vinay, V. (1991). Counting auxiliary pushdown automata and semi-unbounded arithmetic
circuits. In Proc. 6th Structure in Complexity Theory Conference, pp. 270{284. IEEE.

36

fiJournal of Artificial Intelligence Research 9 (1998) 317-365

Submitted 5/98; published 12/98

AntNet: Distributed Stigmergetic Control for
Communications Networks
Gianni Di Caro
Marco Dorigo

IRIDIA, Universite Libre de Bruxelles
50, av. F. Roosevelt, CP 194/6, 1050 - Brussels, Belgium

gdicaro@iridia.ulb.ac.be
mdorigo@ulb.ac.be

Abstract

This paper introduces AntNet, a novel approach to the adaptive learning of routing
tables in communications networks. AntNet is a distributed, mobile agents based Monte
Carlo system that was inspired by recent work on the ant colony metaphor for solving
optimization problems. AntNet's agents concurrently explore the network and exchange
collected information. The communication among the agents is indirect and asynchronous,
mediated by the network itself. This form of communication is typical of social insects
and is called stigmergy. We compare our algorithm with six state-of-the-art routing algorithms coming from the telecommunications and machine learning fields. The algorithms'
performance is evaluated over a set of realistic testbeds. We run many experiments over
real and artificial IP datagram networks with increasing number of nodes and under several paradigmatic spatial and temporal trac distributions. Results are very encouraging.
AntNet showed superior performance under all the experimental conditions with respect
to its competitors. We analyze the main characteristics of the algorithm and try to explain
the reasons for its superiority.

1. Introduction
Worldwide demand and supply of communications networks services are growing exponentially. Techniques for network control (i.e., online and off-line monitoring and management
of the network resources) play a fundamental role in best exploiting the new transmission
and switching technologies to meet user's requests.
Routing is at the core of the whole network control system. Routing, in conjunction
with the admission, ow, and congestion control components, determines the overall network
performance in terms of both quality and quantity of delivered service (Walrand & Varaiya,
1996). Routing refers to the distributed activity of building and using routing tables, one
for each node in the network, which tell incoming data packets which outgoing link to use
to continue their travel towards the destination node.
Routing protocols and policies have to accommodate conicting objectives and constraints imposed by technologies and user requirements rapidly evolving under commercial
and scientific pressures. Novel routing approaches are required to eciently manage distributed multimedia services, mobile users and networks, heterogeneous inter-networking,
service guarantees, point-to-multipoint communications, etc. (Sandick & Crawley, 1997;
The ATM Forum, 1996).
The adaptive and distributed routing algorithm we propose in this paper is a mobileagent-based, online Monte Carlo technique inspired by previous work on artificial ant
c 1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiDi Caro & Dorigo

colonies and, more generally, by the notion of stigmergy (Grasse, 1959), that is, the indirect communication taking place among individuals through modifications induced in
their environment.
Algorithms that take inspiration from real ants' behavior in finding shortest paths (Goss,
Aron, Deneubourg, & Pasteels, 1989; Beckers, Deneubourg, & Goss, 1992) using as information only the trail of a chemical substance (called pheromone) deposited by other ants,
have recently been successfully applied to several discrete optimization problems (Dorigo,
Maniezzo, & Colorni, 1991; Dorigo, 1992; Dorigo, Maniezzo, & Colorni, 1996; Dorigo &
Gambardella, 1997; Schoonderwoerd, Holland, Bruten, & Rothkrantz, 1996; Schoonderwoerd, Holland, & Bruten, 1997; Costa & Hertz, 1997). In all these algorithms a set of artificial
ants collectively solve the problem under consideration through a cooperative effort. This
effort is mediated by indirect communication of information on the problem structure the
ants concurrently collect while building solutions by using a stochastic policy. Similarly,
in AntNet, the algorithm we propose in this paper, a set of concurrent distributed agents
collectively solve the adaptive routing problem. Agents adaptively build routing tables and
local models of the network status by using indirect and non-coordinated communication
of information they collect while exploring the network.
To ensure a meaningful validation of our algorithm performance we devised a realistic
simulation environment in terms of network characteristics, communications protocol and
trac patterns. We focus on IP (Internet Protocol) datagram networks with irregular
topology and consider three real and artificial topologies with an increasing number of
nodes and several paradigmatic temporal and spatial trac distributions. We report on
the behavior of AntNet as compared to some effective static and adaptive state-of-the-art
routing algorithms (vector-distance and link-state shortest paths algorithms (Steenstrup,
1995), and recently introduced algorithms based on machine learning techniques).
AntNet shows the best performance and the most stable behavior for all the considered
situations. In many experiments its superiority is striking. We discuss the results and the
main properties of our algorithm, as compared with its competitors.
The paper is organized as follows. In Section 2 the definition, taxonomy and characteristics of the routing problem are reported. In Section 3 we describe the communication
network model we used. Section 4 describes in detail AntNet, our novel routing algorithm,
while in Section 5 we briey describe the algorithms with which we compared AntNet. In
Section 6, the experimental settings are reported in terms of trac, networks and algorithm
parameters. Section 7 reports several experimental results. In Section 8 we discuss these
results and try to explain AntNet's superior performance. Finally, in Section 9, we discuss
related work, and in Section 10, we draw some conclusions and outline directions for future
research.

2. Routing: Definition and Characteristics
Routing in distributed systems can be characterized as follows. Let G = (V; E ) be a directed
weighted graph, where each node in the set V represents a processing/queuing and/or forwarding unit and each edge is a transmission system. The main task of a routing algorithm
is to direct data ow from source to destination nodes maximizing network performance.
318

fiAntNet: Distributed Stigmergetic Control for Communications Networks

In the problems we are interested in, the data ow is not statically assigned and it follows
a stochastic profile that is very hard to model.
In the specific case of communications networks (Steenstrup, 1995; Bertsekas & Gallager,
1992), the routing algorithm has to manage a set of basic functionalities and it tightly
interacts with the congestion and admission control algorithms, with the links' queuing
policy, and with the user-generated trac. The core of the routing functions is (i) the
acquisition, organization and distribution of information about user-generated trac and
network states, (ii) the use of this information to generate feasible routes maximizing the
performance objectives, and (iii) the forwarding of user trac along the selected routes.
The way the above three functionalities are implemented strongly depends on the underlying network switching and transmission technology, and on the features of the other
interacting software layers. Concerning point (iii), two main forwarding paradigms are in
use: circuit and packet-switching (also indicated with the terms connection-oriented and
connection-less). In the circuit-switching approach, a setup phase looks for and reserves the
resources that will be assigned to each incoming session. In this case, all the data packets
belonging to the same session will follow the same path. Routers are required to keep state
information about active sessions. In the packet-switching approach, there is no reservation
phase, no state information is maintained at routers and data packets can follow different
paths. In each intermediate node an autonomous decision is taken concerning the node's
outgoing link that has to be used to forward the data packet toward its destination.
In the work described in this paper, we focus on the packet-switching paradigm, but
the technique developed here can be used also to manage circuit-switching and we expect
to have qualitatively similar results.

2.1 A Broad Taxonomy

A common feature of all the routing algorithms is the presence in every network node of
a data structure, called routing table, holding all the information used by the algorithm to
make the local forwarding decisions. The routing table is both a local database and a local
model of the global network status. The type of information it contains and the way this
information is used and updated strongly depends on the algorithm's characteristics. A
broad classification of routing algorithms is the following:
 centralized versus distributed;
 static versus adaptive.
In centralized algorithms, a main controller is responsible for updating all the node's
routing tables and/or to make every routing decision. Centralized algorithms can be used
only in particular cases and for small networks. In general, the delays necessary to gather
information about the network status and to broadcast the decisions/updates make them
infeasible in practice. Moreover, centralized systems are not fault-tolerant. In this work,
we will consider exclusively distributed routing.
In distributed routing systems, the computation of routes is shared among the network
nodes, which exchange the necessary information. The distributed paradigm is currently
used in the majority of network systems.
In static (or oblivious) routing systems, the path taken by a packet is determined only
on the basis of its source and destination, without regard to the current network state. This
319

fiDi Caro & Dorigo

path is usually chosen as the shortest one according to some cost criterion, and it can be
changed only to account for faulty links or nodes.
Adaptive routers are, in principle, more attractive, because they can adapt the routing policy to time and spatially varying trac conditions. As a drawback, they can cause
oscillations in selected paths. This fact can cause circular paths, as well as large uctuations in measured performance. In addition, adaptive routing can lead more easily to
inconsistent situations, associated with node or link failures or local topological changes.
These stability and inconsistency problems are more evident for connection-less than for
connection-oriented networks (Bertsekas & Gallager, 1992).
Another interesting way of looking at routing algorithms is from an optimization perspective. In this case the main paradigms are:
 minimal routing versus non-minimal routing;
 optimal routing versus shortest path routing.
Minimal routers allow packets to choose only minimal cost paths, while non-minimal
algorithms allow choices among all the available paths following some heuristic strategies
(Bolding, Fulgham, & Snyder, 1994).
Optimal routing has a network-wide perspective and its objective is to optimize a function of all individual link ows (usually this function is a sum of link costs assigned on the
basis of average packet delays) (Bertsekas & Gallager, 1992).
Shortest path routing has a source-destination pair perspective: there is no global cost
function to optimize. Its objective is to determine the shortest path (minimum cost) between
two nodes, where the link costs are computed (statically or adaptively) following some
statistical description of the link states. This strategy is based on individual rather than
group rationality (Wang & Crowcroft, 1992). Considering the different content stored in
each routing table, shortest path algorithms can be further subdivided into two classes
called distance-vector and link-state (Steenstrup, 1995).
Optimal routing is static (it can be seen as the solution of a multicommodity ow problem) and requires the knowledge of all the trac characteristics. Shortest paths algorithms
are more exible, they don't require a priori knowledge about the trac patterns and they
are the most widely used routing algorithms.
In appendix A, a more detailed description of the properties of optimal and shortest
path routing algorithms is reported.
In Section 4, we introduce a novel distributed adaptive method, AntNet, that shares the
same optimization perspective as (minimal or non-minimal) shortest path algorithms but
not their usual implementation paradigms (as depicted in appendix A).

2.2 Main Characteristics of the Routing Problem

The main characteristics of the routing problem in communications networks can be summarized in the following way:

 Intrinsically distributed with strong real-time constraints: in fact, the database and

the decision system are completely distributed over all the network nodes, and failures
and status information propagation delays are not negligible with respect to the user's
320

fiAntNet: Distributed Stigmergetic Control for Communications Networks

trac patterns. It is impossible to get complete and up-to-date knowledge of the distributed state, that remains hidden. At each decision node, the routing algorithm can
only make use of local, up-to-date information, and of non-local, delayed information
coming from the other nodes.
 Stochastic and time-varying: the session arrival and data generation process is, in
the general case, non-stationary and stochastic. Moreover, this stochastic process
interacts recursively with the routing decisions making it infeasible to build a working model of the whole system (to be used for example in a dynamic programming
framework).
 Multi-objective: several conicting performance measures are usually taken into account. The most common are throughput (bit/sec) and average packet delay (sec).
The former measures the quantity of service that the network has been able to offer
in a certain amount of time (amount of correctly delivered bits per time unit), while
the latter defines the quality of service produced at the same time. Citing Bertsekas
and Gallager (1992), page 367: \the effect of good routing is to increase throughput
for the same value of average delay per packet under high offered load conditions and
to decrease average delay per packet under low and moderate offered load conditions".
Other performance measures consider the impact of the routing algorithm on the network resources in terms of memory, bandwidth and computation, and the algorithm
simplicity, exibility, etc.
 Multi-constraint: constraints are imposed by the underlying network technology, the
network services provided and the user services requested. In general, users ask for
low-cost, high-quality, reliable, distributed multimedia services available across heterogeneous static and mobile networks. Evaluating technological and commercial factors,
network builders and service providers try to accommodate these requests while maximizing some profit criteria. Moreover, a high level of fault-tolerance and reliability is
requested in modern high-speed networks, where user sessions can formulate precise
requests for network resources. In this case, once the session has been accepted, the
system should be able to guarantee that the session gets the resources it needs, under
any recoverable fault event.
It is interesting to note that the above characteristics make the problem of routing belong
to the class of reinforcement learning problems with hidden state (Bertsekas & Tsitsiklis,
1996; Kaelbling, Littman, & Moore, 1996; McCallum, 1995). A distributed system of agents,
the components of the routing algorithm in each node, determine a continual and online
learning of the best routing table values with respect to network's performance criteria. An
exact measure of evaluation that scores forwarding decisions is not available, neither online
nor in the form of a training set. Moreover, because of the distributed nature of the problem
and of its constraints, the complete state of the network is hidden to each agent.

3. The Communication Network Model

In this paper, we focus on irregular topology connection-less networks with an IP-like network layer (in the ISO-OSI terminology) and a very simple transport layer. In particular,
we focus on wide-area networks (WAN). In these cases, hierarchical organization schemes
321

fiDi Caro & Dorigo

are adopted.1 Roughly speaking, sub-networks are seen as single host nodes connected to
interface nodes called gateways. Gateways perform fairly sophisticated network layer tasks,
including routing. Groups of gateways, connected by an arbitrary topology, define logical
areas. Inside each area, all the gateways are at the same hierarchical level and \at" routing
is performed among them. Areas communicate only by means of area border gateways. In
this way, the computational complexity of the routing problem, as seen by each gateway, is
much reduced (e.g., in the Internet, OSPF areas typically group 10 to 300 gateways), while
the complexity of the design and management of the routing protocol is much increased.
The instance of our communication network is mapped on a directed weighted graph
with N processing/forwarding nodes. All the links are viewed as bit pipes characterized
by a bandwidth (bit/sec) and a transmission delay (sec), and are accessed following a
statistical multiplexing scheme. For this purpose, every node, of type store-and-forward,
holds a buffer space where the incoming and the outgoing packets are stored. This buffer
is a shared resource among all the queues attached to every incoming and outgoing link of
the node. All the traveling packets are subdivided in two classes: data and routing packets.
All the packets in the same class have the same priority, so they are queued and served on
the basis of a first-in-first-out policy, but routing packets have a greater priority than data
packets. The workload is defined in terms of applications whose arrival rate is dictated by
a selected probabilistic model. By application (or session, or connection in the following),
we mean a process sending data packets from an origin node to a destination node. The
number of packets to send, their sizes and the intervals between them are assigned according
to some defined stochastic process. We didn't make any distinction among nodes, they act
at the same time as hosts (session end-points) and gateways/routers (forwarding elements).
The adopted workload model incorporates a simple ow control mechanism implemented
by using a fixed production window for the session's packets generation. The window
determines the maximum number of data packets waiting to be sent. Once sent, a packet is
considered to be acknowledged. This means that the transport layer neither manages error
control, nor packet sequencing, nor acknowledgements and retransmissions.2
For each incoming packet, the node's routing component uses the information stored in
the local routing table to assign the outgoing link to be used to forward the packet toward
its target node. When the link resources are available, they are reserved and the transfer
is set up. The time it takes to move a packet from one node to a neighboring one depends
on the packet size and on the link transmission characteristics. If, on a packet's arrival,
there is not enough buffer space to hold it, the packet is discarded. Otherwise, a service
time is stochastically generated for the newly arrived packet. This time represents the delay
between the packet arrival time and the time when it will be put in the buffer queue of the
outgoing link the local routing component has selected for it.
Situations causing a temporary or steady alteration of the network topology or of its
physical characteristics are not taken into account (link or node failure, adding or deleting
of network components, etc.).
1. A hierarchical structure is adopted on the Internet, organized in hierarchical Autonomous Systems and
multiple routing areas inside each Autonomous System (Moy, 1998).
2. This choice is the same as in the \Simple Trac" model in the MaRS network simulator (Alaettinoglu,
Shankar, Dussa-Zieger, & Matta, 1992). It can be seen as a very basic form of File Transfer Protocol
(FTP).

322

fiAntNet: Distributed Stigmergetic Control for Communications Networks

We developed a complete network simulator in C++. It is a discrete event simulator
using as its main data structure an event list, which holds the next future events. The
simulation time is a continuous variable and is set by the currently scheduled event. The aim
of the simulator is to closely mirror the essential features of the concurrent and distributed
behavior of a generic communication network without sacrificing eciency and exibility
in code development.
We end this section with some remarks concerning two features of the model.
First, we chose not to implement a \real" transport layer for a proper management
of error, ow, and congestion control. In fact, each additional control component has a
considerable impact on the network performance,3 making very dicult to evaluate and to
study the properties of each control algorithm without taking in consideration the complex
way it interacts with all the other control components. Therefore, we chose to test the
behavior of our algorithm and of its competitors in conditions such that the number of
interacting components is minimal and the routing component can be evaluated in isolation,
allowing a better understanding of its properties. To study routing in conjunction with error,
ow and congestion control, all these components should be designed at the same time, to
allow a good match among their characteristics to produce a synergetic effect.
Second, we chose to work with connection-less and not with connection-oriented networks because connection-oriented schemes are mainly used in networks able to deliver
Quality of Service (QoS) (Crawley, Nair, Rajagopalan, & Sandick, 1996).4 In this case,
suitable admission control algorithms have to be introduced, taking into account many
economic and technological factors (Sandick & Crawley, 1997). But, again, as a first step
we think that it is more reasonable to try to check the validity of a routing algorithm by
reducing the number of components heavily inuencing the network behavior.

4. AntNet: An Adaptive Agent-based Routing Algorithm
The characteristics of the routing problem (discussed in Section 2.2) make it well suited
to be solved by a mobile multi-agent approach (Stone & Veloso, 1996; Gray, Kotz, Nog,
Rus, & Cybenko, 1997). This processing paradigm is a good match for the distributed and
non-stationary (in topology and trac patterns) nature of the problem, presents a high
level of redundancy and fault-tolerance, and can handle multiple objectives and constraints
in a exible way.
AntNet, the routing algorithm we propose in this paper, is a mobile agents system showing some essential features of parallel replicated Monte Carlo systems (Streltsov & Vakili,
1996). AntNet takes inspiration from previous work on artificial ant colonies techniques to
solve combinatorial optimization problems (Dorigo et al., 1991; Dorigo, 1992; Dorigo et al.,
1996; Dorigo & Gambardella, 1997) and telephone network routing (Schoonderwoerd et al.,
3. As an example, some authors reported an improvement ranging from 2 to 30% in various performance
measures for real Internet trac (Danzig, Liu, & Yan, 1994) by changing from the Reno version to the
Vegas version of the TCP (Peterson & Davie, 1996) (the current Internet Transport Control Protocol),
and other authors even claimed improvements ranging from 40 to 70% (Brakmo, O'Malley, & Peterson,
1994).
4. This is not the case for the current Internet, where the IP bearer service is of \best-effort" type, meaning
that it does the best it can do but no guarantees of service quality in terms of delay or bandwidth or
jitter, etc., can be assured.

323

fiDi Caro & Dorigo

1996, 1997). The core ideas of these techniques (for a review see Dorigo, Di Caro, and
Gambardella, 1998) are (i) the use of repeated and concurrent simulations carried out by a
population of artificial agents called \ants" to generate new solutions to the problem, (ii)
the use by the agents of stochastic local search to build the solutions in an incremental way,
and (iii) the use of information collected during past simulations to direct future search for
better solutions.
In the artificial ant colony approach, following an iterative process, each ant builds a
solution by using two types of information locally accessible: problem-specific information
(for example, distance among cities in a traveling salesman problem), and information added
by ants during previous iterations of the algorithm. In fact, while building a solution, each
ant collects information on the problem characteristics and on its own performance, and
uses this information to modify the representation of the problem, as seen locally by the
other ants. The representation of the problem is modified in such a way that information
contained in past good solutions can be exploited to build new better solutions. This form
of indirect communication mediated by the environment is called stigmergy, and is typical
of social insects (Grasse, 1959).
In AntNet, we retain the core ideas of the artificial ant colony paradigm, and we apply
them to solve in an adaptive way the routing problem in datagram networks.
Informally, the AntNet algorithm and its main characteristics can be summarized as
follows.

 At regular intervals, and concurrently with the data trac, from each network node








mobile agents are asynchronously launched towards randomly selected destination
nodes.
Agents act concurrently and independently, and communicate in an indirect way,
through the information they read and write locally to the nodes.
Each agent searches for a minimum cost path joining its source and destination nodes.
Each agent moves step-by-step towards its destination node. At each intermediate
node a greedy stochastic policy is applied to choose the next node to move to. The
policy makes use of (i) local agent-generated and maintained information, (ii) local
problem-dependent heuristic information, and (iii) agent-private information.
While moving, the agents collect information about the time length, the congestion
status and the node identifiers of the followed path.
Once they have arrived at the destination, the agents go back to their source nodes
by moving along the same path as before but in the opposite direction.
During this backward travel, local models of the network status and the local routing
table of each visited node are modified by the agents as a function of the path they
followed and of its goodness.
Once they have returned to their source node, the agents die.

In the following subsections the above scheme is explained, all its components are explicated and discussed, and a more detailed description of the algorithm is given.
324

fiAntNet: Distributed Stigmergetic Control for Communications Networks

4.1 Algorithm Description and Characteristics

AntNet is conveniently described in terms of two sets of homogeneous mobile agents (Stone
& Veloso, 1996), called in the following forward and backward ants. Agents5 in each set
possess the same structure, but they are differently situated in the environment; that is,
they can sense different inputs and they can produce different, independent outputs. They
can be broadly classified as deliberative agents, because they behave reactively retrieving a
pre-compiled set of behaviors, and at the same time they maintain a complete internal state
description. Agents communicate in an indirect way, according to the stigmergy paradigm,
through the information they concurrently read and write in two data structures stored in
each network node k (see Figure 1):
Outgoing Links

Network Nodes

Routing Table
Network
Node

Local

P1 1

P1 2

........

P1 N

P2 1

P2 2

........

P2 N

PL 1

PL 2

........

PL N

Traffic
Statistics

Network Nodes
Stat (1)

Stat (2)

Stat(N)

Figure 1: Node structures used by mobile agents in AntNet for the case of a node with
L neighbors and a network with N nodes. The routing table is organized as in
vector-distance algorithms, but the entries are probabilistic values. The structure
containing statistics about the local trac plays the role of a local adaptive model
for the trac toward each possible destination.
i) A routing table Tk , organized as in vector-distance algorithms (see Appendix A),
but with probabilistic entries. Tk defines the probabilistic routing policy currently
adopted at node k: for each possible destination d and for each neighbor node n, Tk
stores a probability value Pnd expressing the goodness (desirability), under the current
network-wide routing policy, of choosing n as next node when the destination node
is d:
X
Pnd = 1; d 2 [1; N ]; Nk = fneighbors(k)g:
ii)

n2Nk
An array Mk (d ; d 2 ; Wd ), of data structures defining a simple parametric statistical

model for the trac distribution over the network as seen by the local node k. The
model is adaptive and described by sample means and variances computed over the
trip times experienced by the mobile agents, and by a moving observation window Wd
used to store the best value Wbestd of the agents' trip time.

5. In the following, we will use interchangeably the terms ant and agent.

325

fiDi Caro & Dorigo

For each destination d in the network, an estimated mean and variance, d and d 2 ,
give a representation of the expected time to go and of its stability. We used arithmetic, exponential and windowed strategies to compute the statistics. Changing strategy does not affect performance much, but we observed the best results using the
exponential model:6
d
d + (ok!d , d );
2
d
d 2 + ((ok!d , d )2 , d 2 );
(1)
where ok!d is the new observed agent's trip time from node k to destination d.7
The moving observation window Wd is used to compute the value Wbestd of the best
agents' trip time towards destination d as observed in the last w samples. After each
new sample, w is incremented modulus jWjmax , and jWjmax is the maximum allowed
size of the observation window. The value Wbestd represents a short-term memory
expressing a moving empirical lower bound of the estimate of the time to go to node
d from the current node.
T and M can be seen as memories local to nodes capturing different aspects of the
network dynamics. The model M maintains absolute distance/time estimates to all the
nodes, while the routing table gives relative probabilistic goodness measures for each linkdestination pair under the current routing policy implemented over all the network.
The AntNet algorithm is described as follows.
1. At regular intervals t from every network node s, a mobile agent (forward ant) Fs!d
is launched toward a destination node d to discover a feasible, low-cost path to that
node and to investigate the load status of the network. Forward ants share the same
queues as data packets, so that they experience the same trac loads. Destinations are
locally selected according to the data trac patterns generated by the local workload:
if fsd is a measure (in bits or in number of packets) of the data ow s ! d, then the
probability of creating at node s a forward ant with node d as destination is
f
pd = N sd :
X
fsd0

(2)

d0 =1

In this way, ants adapt their exploration activity to the varying data trac distribution.
2. While traveling toward their destination nodes, the agents keep memory of their paths
and of the trac conditions found. The identifier of every visited node k and the time
elapsed since the launching time to arrive at this k-th node are pushed onto a memory
stack Ss!d (k).
6. This is the same model as used by the Jacobson/Karels algorithm to estimate retransmission timeouts
in the Internet TCP(Peterson & Davie, 1996).
7. The factor  weights the number of most recent samples that will really affect the average. The weight
of the ti -th sample used to estimate the value of d after j samplings, with j > i, is: (1 , )j,i . In
this way, for example, if  = 0:1, approximately only the latest 50 observations will really inuence the
estimate, for  = 0:05, the latest 100, and so on. Therefore, the number of effective observations is
 5(1=).

326

fiAntNet: Distributed Stigmergetic Control for Communications Networks

3. At each node k, each traveling agent headed towards its destination d selects the node
n to move to choosing among the neighbors it did not already visit, or over all the
neighbors in case all of them had been previously visited. The neighbor n is selected
0 computed as the normalized sum of the probabilistic
with a probability (goodness) Pnd
entry Pnd of the routing table with a heuristic correction factor ln taking into account
the state (the length) of the n-th link queue of the current node k:
0 =
Pnd

Pnd + ffln
1 + ff(jNk j , 1) :

(3)

The heuristic correction ln is a [0,1] normalized value proportional to the length qn
(in bits waiting to be sent) of the queue of the link connecting the node k with its
neighbor n:
q
ln = 1 , jN jn :
k
X
qn0

(4)

n0 =1

The value of ff weights the importance of the heuristic correction with respect to the
probability values stored in the routing table. ln reects the instantaneous state of the
node's queues, and assuming that the queue's consuming process is almost stationary
or slowly varying, ln gives a quantitative measure associated with the queue waiting
time. The routing tables values, on the other hand, are the outcome of a continual
learning process and capture both the current and the past status of the whole network
as seen by the local node. Correcting these values with the values of l allows the
system to be more \reactive", at the same time avoiding following all the network
uctuations. Agent's decisions are taken on the basis of a combination of a long-term
learning process and an instantaneous heuristic prediction.
In all the experiments we ran, we observed that the introduced correction is a very
effective mechanism. Depending on the characteristics of the problem, the best value
to assign to the weight ff can vary, but if ff ranges between 0.2 and 0.5, performance
doesn't change appreciably. For lower values, the effect of l is vanishing, while for
higher values the resulting routing tables oscillate and, in both cases, performance
degrades.
4. If a cycle is detected, that is, if an ant is forced to return to an already visited node,
the cycle's nodes are popped from the ant's stack and all the memory about them is
destroyed. If the cycle lasted longer than the lifetime of the ant before entering the
cycle, (that is, if the cycle is greater than half the ant's age) the ant is destroyed. In
fact, in this case the agent wasted a lot of time probably because of a wrong sequence
of decisions and not because of congestion states. Therefore, the agent is carrying an
old and misleading memory of the network state and it is counterproductive to use it
to update the routing tables (see below).
5. When the destination node d is reached, the agent Fs!d generates another agent
(backward ant) Bd!s , transfers to it all of its memory, and dies.
327

fiDi Caro & Dorigo

6. The backward ant takes the same path as that of its corresponding forward ant, but
in the opposite direction.8 At each node k along the path it pops its stack Ss!d(k) to
know the next hop node. Backward ants do not share the same link queues as data
packets; they use higher priority queues, because their task is to quickly propagate to
the routing tables the information accumulated by the forward ants.
7. Arriving at a node k coming from a neighbor node f , the backward ant updates the
two main data structures of the node, the local model of the trac Mk and the routing table Tk , for all the entries corresponding to the (forward ant) destination node
d. With some precautions, updates are performed also on the entries corresponding
to every node k0 2 Sk!d; k0 6= d on the \sub-paths" followed by ant Fs!d after visiting the current node k. In fact, if the elapsed trip time of a sub-path is statistically
\good" (i.e., it is less than  + I (; ), where I is an estimate of a confidence interval
for ), then the time value is used to update the corresponding statistics and the
routing table. On the contrary, trip times of sub-paths not deemed good, in the same
statistical sense as defined above, are not used because they don't give a correct idea
of the time to go toward the sub-destination node. In fact, all the forward ant routing
decisions were made only as a function of the destination node. In this perspective,
sub-paths are side effects, and they are intrinsically sub-optimal because of the local
variations in the trac load (we can't reason with the same perspective as in dynamic
programming, because of the non-stationarity of the problem representation). Obviously, in case of a good sub-path we can use it: the ant discovered, at zero cost, an
additional good route. In the following two items the way M and T are updated is
described with respect to a generic \destination" node d0 2 Sk!d.
i) Mk is updated with the values stored in the stack memory Ss!d(k). The time
elapsed to arrive (for the forward ant) to the destination node d0 starting from
the current node is used to update the mean and variance estimates, d0 and d0 2 ,
and the best value over the observation window Wd0 . In this way, a parametric
model of the traveling time to destination d0 is maintained. The mean value of
this time and its dispersion can vary strongly, depending on the trac conditions:
a poor time (path) under low trac load can be a very good one under heavy
trac load. The statistical model has to be able to capture this variability
and to follow in a robust way the uctuations of the trac. This model plays a
critical role in the routing table updating process (see item (ii) below). Therefore,
we investigated several ways to build effective and computationally inexpensive
models, as described in the following Section 4.2.
ii) The routing table Tk is changed by incrementing the probability Pfd0 (i.e., the
probability of choosing neighbor f when destination is d0 ) and decrementing, by
normalization, the other probabilities Pnd0 . The amount of the variation in the
probabilities depends on a measure of goodness we associate with the trip time
Tk!d0 experienced by the forward ant, and is given below. This time represents
the only available explicit feedback signal to score paths. It gives a clear indication about the goodness r of the followed route because it is proportional to its
8. This assumption requires that all the links in the network are bi-directional. In modern networks this is
a reasonable assumption.

328

fiAntNet: Distributed Stigmergetic Control for Communications Networks

length from a physical point of view (number of hops, transmission capacity of the
used links, processing speed of the crossed nodes) and from a trac congestion
point of view (the forward ants share the same queues as data packets).
The time measure T , composed by all the sub-paths elapsed times, cannot be
associated with an exact error measure, given that we don't know the \optimal"
trip times, which depend on the whole network load status.9 Therefore, T can
only be used as a reinforcement signal. This gives rise to a credit assignment
problem typical of the reinforcement learning field (Bertsekas & Tsitsiklis, 1996;
Kaelbling et al., 1996). We define the reinforcement r  r(T; Mk ) to be a
function of the goodness of the observed trip time as estimated on the basis of
the local trac model. r is a dimensionless value, r 2 (0; 1], used by the current
node k as a positive reinforcement for the node f the backward ant Bd!s comes
from. r takes into account some average of the so far observed values and of
their dispersion to score the goodness of the trip time T , such that the smaller T
is, the higher r is (the exact definition of r is discussed in the next subsection).
The probability Pfd0 is increased by the reinforcement value as follows:
Pfd0

Pfd0 + r(1 , Pfd0 ):

(5)

In this way, the probability Pfd0 will be increased by a value proportional to the
reinforcement received and to the previous value of the node probability (that is,
given a same reinforcement, small probability values are increased proportionally
more than big probability values, favoring in this way a quick exploitation of new,
and good, discovered paths).
Probabilities Pnd0 for destination d0 of the other neighboring nodes n implicitly
receive a negative reinforcement by normalization. That is, their values are
reduced so that the sum of probabilities will still be 1:
Pnd0

Pnd0 , rPnd0 ; n 2 Nk ; n 6= f:

(6)

It is important to remark that every discovered path receives a positive reinforcement in its selection probability, and the reinforcement is (in general) a non-linear
function of the goodness of the path, as estimated using the associated trip time.
In this way, not only the (explicit) assigned value r plays a role, but also the
(implicit) ant's arrival rate. This strategy is based on trusting paths that receive
either high reinforcements, independent of their frequency, or low and frequent
reinforcements. In fact, for any trac load condition, a path receives one or more
high reinforcements only if it is much better than previously explored paths. On
the other hand, during a transient phase after a sudden increase in network load
all paths will likely have high traversing times with respect to those learned by
the model M in the preceding, low congestion, situation. Therefore, in this case
good paths can only be differentiated by the frequency of ants' arrivals.
9. When the network is in a congested state, all the trip times will score poorly with respect to the times
observed in low load situations. Nevertheless, a path with a high trip time should be scored as a good
path if its trip time is significantly lower than the other trip times observed in the same congested
situation.

329

fiDi Caro & Dorigo

Assigning always a positive, but low, reinforcement value in the case of paths
with high traversal time allows the implementation of the above mechanism based
on the frequency of the reinforcements, while, at the same time, avoids giving
excessive credit to paths with high traversal time due to their poor quality.
The use of probabilistic entries is very specific to AntNet and we observed it
to be effective, improving the performance, in some cases, even by 30%-40%.
Routing tables are used in a probabilistic way not only by the ants but also
by the data packets. This has been observed to improve AntNet performance,
which means that the way the routing tables are built in AntNet is well matched
with a probabilistic distribution of the data packets over all the good paths.
Data packets are prevented from choosing links with very low probability by remapping the T 's entries by means of a power function f (p) = pff ; ff > 1, which
emphasizes high probability values and reduces lower ones (in our experiments
we set ff to 1.2).
Figure 2 gives a high-level description of the algorithm in pseudo-code, while Figure
3 illustrates a simple example of the algorithm behavior. A detailed discussion of the
characteristics of the algorithm is postponed to Section 8, after the performance of the
algorithm has been analyzed with respect to a set of competitor algorithms. In this way,
the characteristics of AntNet can be meaningfully evaluated and compared to those of other
state-of-the-art algorithms.

4.2 How to Score the Goodness of the Ant's Trip Time

The reinforcement r is a critical quantity that has to be assigned by considering three main
aspects: (i) paths should receive an increment in their selection probability proportional
to their goodness, (ii) the goodness is a relative measure, which depends on the trac
conditions, that can be estimated by means of the model M, and (iii) it is important not to
follow all the trac uctuations. This last aspect is particularly important. Uncontrolled
oscillations in the routing tables are one of the main problems in shortest paths routing
(Wang & Crowcroft, 1992). It is very important to be able to set the best trade-off between
stability and adaptivity.
We investigated several ways to assign the r values trying to take into account the above
three requirements:

 The simplest way is to set r = constant: independently of the ant's \experiment

outcomes", the discovered paths are all rewarded in the same way. In this simple but
meaningful case, what is at work is the implicit reinforcement mechanism due to the
differentiation in the ant arrival rates. Ants traveling along faster paths will arrive
at a higher rate than other ants, hence their paths will receive a higher cumulative
reward.10 The obvious problem of this approach lies in the fact that, although ants
following longer paths arrive delayed, they will nevertheless have the same effect on
the routing tables as the ants who followed shorter paths.

10. In this case, the core of the algorithm is based on the capability of \real" ants to discover shortest paths
communicating by means of pheromone trails (Goss et al., 1989; Beckers et al., 1992).

330

fiAntNet: Distributed Stigmergetic Control for Communications Networks

t := Current time;
tend := Time length of the simulation;
t := Time interval between ants generation;
foreach (Node) =  Concurrent activity over the network  =
M = Local trac model;
T = Node routing table;
while ( t  tend )
in parallel =  Concurrent activity on each node  =
if ( t mod t = 0)
destination node := SelectDestinationNode(data trac distribution);
LaunchForwardAnt(destination node, source node);
end if
foreach (ActiveForwardAnt [source node, current
while (current node 6= destination node)

node, destination node])

next hop node := SelectLink(current node, destination node,T ; link queues);
PutAntOnLinkQueue(current node, next hop node);
WaitOnDataLinkQueue(current node, next hop node);
CrossTheLink(current node, next hop node);
PushOnTheStack(next hop node, elapsed time);
current node := next hop node;

end while

LaunchBackwardAnt(destination node, source node, stack data);
Die();

end foreach
foreach (ActiveBackwardAnt [source node, current
while (current node 6= destination node)

node, destination node])

next hop node := PopTheStack();
WaitOnHighPriorityLinkQueue(current node, next hop node);
CrossTheLink(current node, next hop node);
UpdateLocalTracModel(M, current node, source node, stack data);
reinforcement := GetReinforcement(current node, source node, stack data, M);
UpdateLocalRoutingTable(T , current node, source node, reinforcement);

end while
end foreach
end in parallel
end while
end foreach

Figure 2: AntNet's top-level description in pseudo-code. All the described actions take place
in a completely distributed and concurrent way over the network nodes (while, in
the text, AntNet has been described from an individual ant's perspective). All the
constructs at the same level of indentation inside the context of the statement
in parallel are executed concurrently. The processes of data generation and
forwarding are not described, but they can be thought as acting concurrently
with the ants.
331

fiDi Caro & Dorigo

Forward Ant (1

4)

1

2

3
(1

4
4) Backward Ant

Figure 3: Example of AntNet behavior. The forward ant, F1!4 , moves along the path
1 ! 2 ! 3 ! 4 and, arrived at node 4, launches the backward ant B4!1 that
will travel in the opposite direction. At each node k; k = 3; : : : ; 1, the backward
ant will use the stack contents S1!4 (k) to update the values for Mk (4 ; 4 2 ; W4 ),
and, in case of good sub-paths, to update also the values for Mk (i ; i 2 ; Wi ); i =
k + 1; : : : ; 3. At the same time the routing table will be updated by incrementing
the goodness Pj 4 , j = k + 1, of the last node k + 1 the ant B4!1 came from,
for the case of node i = k + 1; : : : ; 4 as destination node, and decrementing the
values of P for the other neighbors (here not shown). The increment will be a
function of the trip time experienced by the forward ant going from node k to
destination node i. As for M, the routing table is always updated for the case of
node 4 as destination, while the other nodes i0 = k + 1; : : : ; 3 on the sub-paths
are taken in consideration as destination nodes only if the trip time associated to
the corresponding sub-path of the forward ant is statistically good.
In the experiments we ran with this strategy, the algorithm showed moderately good
performance. These results suggest that the \implicit" component of the algorithm,
based on the ant arrival rate, plays a very important role. Of course, to compete with
state-of-the-art algorithms, the available information about path costs has to be used.
 More elaborate approaches define r as a function of the ant's trip time T , and of the
parameters of the local statistical model M. We tested several alternatives, by using
different linear, quadratic and hyperbolic combinations of the T and M values. In
the following we limit the discussion to the functional form that gave the best results,
and that we used in the reported experiments:
r = c1

W

best

T







+ c2 (I , IIsup ),+Iinf
(T , Iinf ) :
sup inf

(7)

In Equation 7, Wbest is the best trip time experienced by the ants traveling toward
the destination d, over the last observation window W . The maximum size of the window
(the maximum number of considered samples before resetting the Wbest value) is assigned
on the basis of the coecient  of Equation 1. As we said,  weights the number of
samples effectively giving a contribution to the value of the  estimate, defining a sort of
moving exponential window. Following the expression for the number of effective samples
as reported in footnote 7, we set jWjmax = 5(c=), with c < 1. In this way, the longterm exponential mean and the short-term windowing are referring to a comparable set of
observations, with the short-term mean evaluated over a fraction c of the samples used for
332

fiAntNet: Distributed Stigmergetic Control for Communications Networks

the long-term one. Isup and Iinf are convenient estimates of the limits of
p an approximate
jWj), with z =
confidence
interval
for

.
I
is
set
to
W
,
while
I
=

+
z
(
=
sup
inf
best
p
11
1= (1 ,  ) where  gives the selected confidence level. There is some level of arbitrariness
in our computation of the confidence interval, because we set it in an asymmetric way and
 and  are not arithmetic estimates. Anyway, what we need is a quick, raw estimate of the
mean value and of the dispersion of the values (for example, a local bootstrap procedure
could have been applied to extract a meaningful confidence interval, but such a choice is
not reasonable from a CPU time-consuming perspective).
The first term in Equation 7 simply evaluates the ratio between the current trip time and
the best trip time observed over the current observation window. This term is corrected
by the second one, that evaluates how far the value T is from Iinf in relation to the
extension of the confidence interval, that is, considering the stability in the latest trip
times. The coecients c1 and c2 weight the importance of each term. The first term is the
most important one, while the second term plays the role of a correction. In the current
implementation of the algorithm we set c1 = 0:7 and c2 = 0:3. We observed that c2 shouldn't
be too big (0.35 is an upper limit), otherwise performance starts to degrade appreciably.
The behavior of the algorithm is quite stable for c2 values in the range 0.15 to 0.35 but
setting c2 below 0.15 slightly degrades performance. The algorithm is very robust to changes
in  , which defines the confidence level: varying the confidence level in the range from 75%
to 95% changes performance little. The best results have been obtained for values around
75%80%. We observed that the algorithm is very robust to its internal parameter settings
and we didn't try to \adapt" the set of parameters to the problem instance. All the different
experiments were carried out with the same \reasonable" settings. We could surely improve
the performance by means of a finer tuning of the parameters, but we didn't because we
were interested in implementing a robust system, considering that the world of networks is
incredibly varied in terms of trac, topologies, switch and transmission characteristics, etc.
The value r obtained from Equation 7 is finally transformed by means of a squash
function s(x):

 a  !,1
;
s(x) = 1 + exp
xjN j
k

r

s(r)
:
s(1)

x 2 (0; 1]; a 2 R+ ;

(8)
(9)

Squashing the r values allows the system to be more sensitive in rewarding good (high)
values of r, while having the tendency to saturate the rewards for bad (near to zero) r
values: the scale is compressed for lower values and expanded in the upper part. In such a
way an emphasis is put on good results, while bad results play a minor role.
11. The expression is obtained by using the Tchebycheff inequality that allows the definition of a confidence
interval for a random variable following any distribution (Papoulis, 1991) Usually, for specific probability
densities the Tchebycheff bound is too high, but here we can conveniently use it because (i) we want
to avoid to make assumptions on the distribution of  and, (ii) we need only a raw estimate of the
confidence interval.

333

fiDi Caro & Dorigo

1

0.8
5 neighbors
4 neighbors
3 neighbors
2 neighbors

0.6
s(r)/s(1)

The coecient a=jNk j determines a
parametric dependence of the squashed
reinforcement value on the number
jNk j of neighbors of the reinforced node
k: the greater the number of neighbors,
the higher the reinforcement (see Figure 4). The reason to do this is that we
want to have a similar, strong, effect of
good results on the probabilistic routing tables, independent of the number
of neighbor nodes.

0.4

0.2

0
0

0.2

0.4

0.6

0.8

1

r

Figure 4: Examples of squash functions with a
variable number of node neighbors.

5. Routing Algorithms Used for Comparison

To evaluate the performance of AntNet, we compared it with state-of-the-art routing algorithms from the telecommunications and machine learning fields. The following algorithms,
belonging to the various possible combinations of static and adaptive, distance-vector and
link-state classes (see Appendix A), have been implemented and used to run comparisons.

OSPF (static, link state): is our implementation of the current Interior Gateway Pro-

tocol (IGP) of Internet (Moy, 1998). Being interested in studying routing under the
assumptions described in Section 3, the routing protocol we implemented does not
mirror the real OSPF protocol in all its details. It only retains the basic features of
OSPF. Link costs are statically assigned on the basis of their physical characteristics
and routing tables are set as the result of the shortest (minimum time) path computation for a sample data packet of size 512 bytes. It is worth remarking that this
choice penalizes our version of OSPF with respect to the real one. In fact, in the real
Internet link costs are set by network administrators who can use additional heuristic
and on-field knowledge they have about trac workloads.

SPF (adaptive, link-state): is the prototype of link-state algorithms with dynamic met-

ric for link costs evaluations. A similar algorithm was implemented in the second
version of ARPANET (McQuillan, Richer, & Rosen, 1980) and in its successive revisions (Khanna & Zinky, 1989). Our implementation uses the same ooding algorithm,
while link costs are assigned over a discrete scale of 20 values by using the ARPANET
hop-normalized-delay metric12 (Khanna & Zinky, 1989) and the the statistical window average method described in (Shankar, Alaettinoglu, Dussa-Zieger, & Matta,
1992a). Link costs are computed as weighted averages between short and long-term
real-valued statistics reecting the delay (e.g., utilization, queueing and/or transmis-

12. The transmitting node monitors the average packet delay d (queuing and transmission) and the average
packet transmission time t over fix observation windows. From these measures, assuming an M/M/1
queueing model (Bertsekas & Gallager, 1992), a link utilization cost measure is calculated as 1 , t=d.

334

fiAntNet: Distributed Stigmergetic Control for Communications Networks

sion delay, etc.) over fixed time intervals. Obtained values are rescaled and saturated
by a linear function. We tried several additional discrete and real-valued metrics but
the discretized hop-normalized-delay gave the best results in terms of performance
and stability. Using a discretized scale reduces the sensitivity of the algorithm but at
the same time reduces also undesirable oscillations.

BF (adaptive, distance-vector): is an implementation of the asynchronous distributed
Bellman-Ford algorithm with dynamic metrics (Bertsekas & Gallager, 1992; Shankar
et al., 1992a). The algorithm has been implemented following the guidelines of Appendix A, while link costs are assigned in the same way as described for SPF above.
Vector-distance Bellman-Ford-like algorithms are today in use mainly for intra-domain
routing, because they are used in the Routing Information Protocol (RIP) (Malkin
& Steenstrup, 1995) supplied with the BSD version of Unix. Several enhanced versions of the basic adaptive Bellman-Ford algorithm can be found in the literature (for
example the Merlin-Segall (Merlin & Segall, 1979) and the Extended Bellman-Ford
(Cheng, Riley, Kumar, & Garcia-Luna-Aceves, 1989) algorithms). They focus mainly
on reducing the information dissemination time in case of link failures. When link
failures are not a major issue, as in this paper, their behavior is in general equivalent
to that of the basic adaptive Bellman-Ford.

Q-R (adaptive, distance-vector): is the Q-Routing algorithm as proposed by Boyan

and Littman (1994). This is an online asynchronous version of the Bellman-Ford
algorithm. Q-R learns online the values Qk (d; n), which are estimates of the time
to reach node d from node k via the neighbor node n. Upon sending a packet P
from k to neighbor node n with destination d, a back packet Pback is immediately
generated from n to k. Pback carries the information about the current time estimate
tn!d = minn0 2Nn Qn (d; n0 ) held at node n about the time to go for destination d, and
the sum tPk!n of the queuing and transmission time experienced by P since its arrival
at node k. The sum Qnew (d; n) = tn!d + tPk!n is used to compute the variation
Qk (d; n) = (Qnew (d; n) , Qk (d; n)) of the Q-learning-like value Qk (d; n).

PQ-R (adaptive, distance-vector): is the Predictive Q-Routing algorithm (Choi & Ye-

ung, 1996), an extension of Q-Routing. In Q-routing the best link (i.e., the one with
the lowest Qk (d; n)) is deterministically chosen by packets. Therefore, a link that
happens to have a high expected Qk (d; n), for example because of a temporary load
condition, will never be used again until all the other links exiting from the same node
have a worse, that is higher, Qk (d; n). PQ-R learns a model of the rate of variation of
links' queues, called the recovery rate, and uses it to probe those links that, although
not having the lowest Qk (d; n), have a high recovery rate.

Daemon (adaptive, optimal routing): is an approximation of an ideal algorithm. It

defines an empirical bound on the achievable performance. It gives some information about how much improvement is still possible. In the absence of any a priori
assumption on trac statistics, the empirical bound can be defined by an algorithm
possessing a \daemon" able to read in every instant the state of all the queues in the
network and then calculating instantaneous \real" costs for all the links and assigning
335

fiDi Caro & Dorigo

paths on the basis of a network-wide shortest paths re-calculation for every packet
hop. Links costs used in shortest paths calculations are the following:
S
S
S
Cl = dl + p + (1 , ff) Q(l) + ff Q(l) ;
b
b
b
l

l

l

where dl is the transmission delay for link l, bl is its bandwidth, Sp is the size (in
bits) of the data packet doing the hop, SQ(l) is the size (in bits) of the queue of link
l, SQ(l) is the exponential mean of the size of links queue and it is a correction to the
actual size of the link queue on the basis of what observed until that moment. This
correction is weighted by the ff value set to 0.4. Of course, given the arbitrariness
we introduced in calculating Cl , it could be possible to define an even better Daemon
algorithm.

6. Experimental Settings
The functioning of a communication network is governed by many components, which may
interact in nonlinear and unpredictable ways. Therefore, the choice of a meaningful testbed
to compare competing algorithms is no easy task.
A limited set of classes of tunable components is defined and for each class our choices
are explained.

6.1 Topology and physical properties of the net

Topology can be defined on the basis of a real net instance or it can defined by hand, to
better analyze the inuence of important topological features (like diameter, connectivity,
etc.).
Nodes are mainly characterized by their buffering and processing capacity, whereas links
are characterized by their propagation delay, bandwidth and streams multiplexing scheme.
For both, fault probability distributions should be defined.
In our experiments, we used three significant net instances with increasing numbers
of nodes. For all of them we describe the main characteristics and we summarize the
topological properties by means of a triple of numbers (, , N ) indicating respectively the
mean shortest path distance, in terms of hops, between all pairs of nodes, the variance of
this average, and the total number of nodes. From these three numbers we can get an idea
about the degree of connectivity and balancing of the network. The diculty of the routing
problem roughly increases with the value of these numbers.

 SimpleNet (1.9, 0.7, 8) is a small network specifically designed to study some aspects

of the behavior of the algorithms we compare. Experiments with SimpleNet were
designed to closely study how the different algorithms manage to distribute the load
on the different possible paths. SimpleNet is composed of 8 nodes and 9 bi-directional
links with a bandwidth of 10 Mbit/s and propagation delay of 1 msec. The topology
is shown in Figure 5.
 NSFNET (2.2, 0.8, 14) is the old USA T1 backbone (1987). NSFNET is a WAN
composed of 14 nodes and 21 bi-directional links with a bandwidth of 1.5 Mbit/s. Its
336

fiAntNet: Distributed Stigmergetic Control for Communications Networks

2

1

4

3

8
5

6

7

Figure 5: SimpleNet. Numbers within circles are node identifiers. Shaded nodes have a
special interpretation in our experiments, described later. Each edge in the graph
represents a pair of directed links. Link bandwidth is 10 Mbit/sec, propagation
delay is 1 msec.
topology is shown in Figure 6. Propagation delays range from 4 to 20 msec. NSFNET
is a well balanced network.

Figure 6: NSFNET. Each edge in the graph represents a pair of directed links. Link bandwidth is 1.5 Mbit/sec, propagation delays range from 4 to 20 msec.

 NTTnet (6.5, 3.8, 57) is the major Japanese backbone. NTTnet is the NTT (Nippon

Telephone and Telegraph company) fiber-optic corporate backbone. NTTnet is a
57 nodes, 162 bi-directional links network. Link bandwidth is of 6 Mbit/sec, while
propagation delays range around 1 to 5 msec. The topology is shown in Figure 7.
NTTnet is not a well balanced network.

Figure 7: NTTnet. Each edge in the graph represents a pair of directed links. Link bandwidth is 6 Mbit/sec, propagation delays range from 1 to 5 msec.
337

fiDi Caro & Dorigo

All the networks are simulated with zero link-fault and node-fault probabilities, local
node buffers of 1 Gbit capacity, and data packets maximum time to live (TTL) set to 15
sec.

6.2 Trac patterns

Trac is defined in terms of open sessions between pairs of different nodes. Trac patterns
can show a huge variety of forms, depending on the characteristics of each session and on
their distribution from geographical and temporal points of view.
Each single session is characterized by the number of transmitted packets, and by their
size and inter-arrival time distributions. More generally, priority, costs and requested quality
of service should be used to completely characterize a session.
Sessions over the network can be characterized by their inter-arrival time distribution
and by their geographical distribution. The latter is controlled by the probability assigned
to each node to be selected as a session start or end-point.
We considered three basic patterns for the temporal distribution of the sessions, and
three for their spatial distribution.
Temporal distributions:
 Poisson (P): for each node a Poisson process is defined which regulates the arrival of
new sessions, i.e., sessions inter-arrival times are negative exponentially distributed.
 Fixed (F): at the beginning of the simulation, for each node, a fixed number of oneto-all sessions is set up and left constant for the remainder of the simulation.
 Temporary (TMPHS): a temporary, heavy load, trac condition is generated turning
on some nodes that act like hot spots (see below).
Spatial distributions:
 Uniform (U): the assigned temporal characteristics for session arrivals are set identically for all the network nodes.
 Random (R): in this case, the assigned temporal characteristics for session arrivals are
set in a random way over the network nodes.
 Hot Spots (HS): some nodes behave as hot spots, concentrating a high rate of input/output trac. A fixed number of sessions are opened from the hot spots to all
the other nodes.
General trac patterns have been obtained combining the above temporal and spatial
characteristics. Therefore, for example, UP trac means that, for each node, an identical
Poisson process is regulating the arrival of new sessions, while in the RP case the process is
different for each node, and UP-HS means that a Hot Spots trac model is superimposed
to a UP trac.
Concerning the shape of the bit stream generated by each session, we consider two basic
types:
 Constant Bit Rate (CBR): the per-session bit rate is maintained fixed. Examples of
applications of CBR streams are the voice signal in a telephone network, which is
converted into a stream of bits with a constant rate of 64 Kbit/sec, and the MPEG1
compression standard, which converts a video signal in a stream of 1.5 Mbit/sec.
338

fiAntNet: Distributed Stigmergetic Control for Communications Networks

 Generic Variable Bit Rate (GVBR): the per-session generated bit rate is time varying.

The term GVBR is a broad generalization of the VBR term normally used to designate
a bit stream with a variable bit rate but with known average characteristics and
expected/admitted uctuations.13 Here, a GVBR session generates packets whose
sizes and inter-arrival times are variable and follow a negative exponential distribution.
The information about these characteristics is never directly used by the routing
algorithms, like in IP-based networks.
The values we used in the experiments to shape trac patterns are \reasonable" values
for session generations and data packet production taking into consideration current network
usage and computing power. The mean of the packet size distribution has been set to 4096
bits in all the experiments. Basic temporal and spatial distributions have been chosen to
be representative of a wide class of possible situations that can be arbitrarily composed to
generate a meaningful subset of real trac patterns.

6.3 Metrics for performance evaluation

Depending on the type of services delivered on the network and on their associated costs,
many performance metrics could be defined. We focused on standard metrics for performance evaluation, considering only sessions with equal costs, benefits and priority and
without the possibility of requests for special services like real-time. In this framework, the
measures we are interested in are: throughput (correctly delivered bits/sec), delay distribution for data packets (sec), and network capacity usage (for data and routing packets),
expressed as the sum of the used link capacities divided by the total available link capacity.

6.4 Routing algorithms parameters

All the algorithms used have a collection of parameters to be set. Common parameters
are routing packet size and elaboration time. Settings for these parameters are shown
in table 1. These parameters have been assigned to values used in previous simulation
Packet size (byte)
Packet elaboration time (msec)

AntNet OSPF & SPF
BF
Q-R & PQ-R
24 + 8Nh 64 + 8jNn j 24 + 12N
12
3
6
2
3

Table 1: Routing packets characteristics for the implemented algorithms (except for the
Daemon algorithm, which does not generate routing packets). Nh is the incremental number of hops made by the forward ant, jNn j is the number of neighbors of
node n, and N is the number of network nodes.
works (Alaettinoglu et al., 1992) and/or on the basis of heuristic evaluations taking into
13. The knowledge about the characteristics of the incoming CBR or VBR bit streams is of fundamental
importance in networks able to deliver Quality of Service. It is only on the basis of this knowledge that
the network can accept/refuse the session requests, and, in case of acceptance, allocate/reserve necessary
resources.

339

fiDi Caro & Dorigo

consideration information encoding schemes and currently available computing power (e.g.,
the size for forward ants has been determined as the same size of a BF packet plus 8 bytes for
each hop to store the information about the node address and the elapsed time). Concerning
the other main parameters, specific for each algorithm, for the AntNet competitors we used
the best settings we could find in the literature and/or we tried to tune the parameters
as much as possible to obtain better results. For OSPF, SPF, and BF, the length of the
time interval between consecutive routing information broadcasts and the length of the time
window to average link costs are the same, and they are set to 0.8 or 3 seconds, depending on
the experiment for SPF and BF, and to 30 seconds for OSPF. Link costs inside each window
are assigned as the weighted sum between the arithmetic average over the window and the
exponential average with decay factor equal to 0.9. The obtained values are discretized
over a linear scale saturated between 1 and 20, with slope set to 20 and maximum admitted
variation equal to 1. For Q-R and PQ-R the transmission of routing information is totally
data-driven. The learning and adaptation rate we used were the same as used by the
algorithm's authors (Boyan & Littman, 1994; Choi & Yeung, 1996).
Concerning AntNet, we observed that the algorithm is very robust to internal parameters
tuning. We did not finely tune the parameter set, and we used the same set of values for all
the different experiments we ran. Most of the settings we used have been previously given
in the text at the moment the parameter was discussed and they are not reported in this
section. The ant generation interval at each node was set to 0.3 seconds. In Section 7.4
it will be shown the robustness of AntNet with respect to this parameter. Regarding the
parameters of the statistical model, the value of , weighting the number of the samples
considered in the model (Equation 1), has been set to 0.005, the c factor for the expression
of jWjmax (sect. 4.2) has been put equal to 0.3, and the confidence level factor z (sect. 4.2)
equal to 1.70, meaning a confidence level of approximately 0.95.

7. Results

Experiments reported in this section compare AntNet with the competing routing algorithms described in Section 5. We studied the performance of the algorithms for increasing
trac load, examining the evolution of the network status toward a saturation condition,
and for temporary saturation conditions.
 Under low load conditions, all algorithms tested have similar performance. In this
case, also considering the huge variability in the possible trac patterns, it is very
hard to assess whether an algorithm is significantly better than another or not.
 Under high, near saturation, loads, all the tested algorithms are able to deliver the
offered throughput in a quite similar way, that is, in most of the cases all the generated trac is routed without big losses. On the contrary, the study of packet delay
distributions shows remarkable differences among the different algorithms. To present
simulation results regarding packet delays we decided either to report the whole empirical distribution or to use the 90-th percentile statistic, which allows one to compare
the algorithms on the basis of the upper value of delay they were able to keep the 90%
of the correctly delivered packets. In fact, packet delays can be spread over a wide
range of values. This is an intrinsic characteristics of data networks: packet delays
can range from very low values for sessions open between adjacent nodes connected by
340

fiAntNet: Distributed Stigmergetic Control for Communications Networks

fast links, to much higher values in the case of sessions involving nodes very far apart
connected by many slow links. Because of this, very often the empirical distribution
of packet delays cannot be meaningfully parametrized in terms of mean and variance,
and the 90-th percentile statistic, or still better the whole empirical distribution, are
much more meaningful.
 Under saturation there are packet losses and/or packet delays that become too big,
cause all the network operations to slow down. Therefore, saturation has to be only
a temporary situation. If it is not, structural changes to the network characteristics,
like adding new and faster connection lines, rather than improvements of the routing
algorithm, should be in order. For these reasons, we studied the responsiveness of the
algorithms to trac loads causing only a temporary saturation.
All reported data are averaged over 10 trials lasting 1000 virtual seconds of simulation
time. One thousand seconds represents a time interval long enough to expire all transients
and to get enough statistical data to evaluate the behavior of the routing algorithm. Before
being fed with data trac, the algorithms are given 500 preliminary simulation seconds with
no data trac to build initial routing tables. In this way, each algorithm builds the routing
tables according to its own \vision" about minimum cost paths. Results for throughput
are reported as average values without an associated measure of variance. The inter-trial
variability is in fact always very low, a few percent of the average value.
Parameter values for trac characteristics are given in the Figure captions with the
following meaning (see also previous section): MSIA is the mean of the sessions inter-arrival
time distribution for the Poisson (P) case, MPIA stands for the mean of the packet interarrival time distribution. In the CBR case, MPIA indicates the fixed packet production
rate. HS is the number of hot-spots nodes and MPIA-HS is the equivalent of MPIA for the
hot-spot sessions. In the following, when not otherwise explicitly stated, the shape of the
session bit streams is assumed to be of GVBR type.
Results for throughput and packet delays for all the considered network topologies are
described in the three following subsections. Results concerning the network resources
utilization are reported in Section 7.4.

7.1 SimpleNet

Experiments with SimpleNet were designed to study how the different algorithms manage
to distribute the load on the different possible paths. In these experiments, all the trac,
of F-CBR type, is directed from node 1 to node 6 (see Figure 5), and the trac load has
been set to a value higher than the capacity of a single link, so that it cannot be routed
eciently on a single path.
Results regarding throughput (Figure 8a) in this case strongly discriminate among the
algorithms. The type of the trac workload and the small number of nodes determined
significant differences in throughput. AntNet is the only algorithm able to deliver almost
all the generated data trac: its throughput after a short transient phase approaches very
closely the level of that delivered by the Daemon algorithm. PQ-R attains a steady value
approximately 15% inferior to that obtained by AntNet. The other algorithms behave very
poorly, stabilizing on values of about 30% inferior to those provided by AntNet. In this
341

fiDi Caro & Dorigo

case, it is rather clear that AntNet is the only algorithm able to exploit at best all the three
available paths (1-8-7-6, 1-3-5-6, 1-2-4-5-6) to distribute the data trac without inducing
counterproductive oscillations. The utilization of the routing tables in a probabilistic way
also by data packets in this case plays a fundamental role in achieving higher quality results. Results for throughput are confirmed by those for packet delays, reported in the
graph of Figure 8b. The differences in the empirical distributions for packet delays reect
approximatively the same proportions as evidenced in the throughput case.
14.0

1.0

13.5

Throughput (106 bit/sec)

12.5
12.0

0.8
Empirical Distribution

OSPF
SPF
BF
Q-R
PQ-R
AntNet
Daemon

13.0

11.5
11.0
10.5

0.6

0.4

OSPF
SPF
BF
Q-R
PQ-R
AntNet
Daemon

0.2

10.0
9.5

0.0
0

100

200

300

400

500

600

700

800

900

1000

Simulation Time (sec)

0

0.05

0.1

0.15

0.2

Packet Delay (sec)

(a)

(b)

Figure 8: SimpleNet: Comparison of algorithms for F-CBR trac directed from node 1 to node 6
(MPIA = 0.0003 sec). (a) Throughput, and (b) packet delays empirical distribution.

7.2 NSFNET

We carried out a wide range of experiments on NSFNET using UP, RP, UP-HS and TMPHSUP trac patterns. In all the cases considered, differences in throughput are of minor
importance with respect to those shown by packet delays. For each one of the UP, RP
and UP-HS cases we ran five distinct groups of ten trial experiments, gradually increasing
the generated workload (in terms of reducing the session inter-arrival time). As explained
above, we studied the behavior of the algorithms when moving the trac load towards a
saturation region.
In the UP case, differences in throughput (Figure 9a) are small: the best performing
algorithms are BF and SPF, which can attain performance of only about 10% inferior to
those of Daemon and of the same amount better than those of AntNet, Q-R and PQ-R,14
while OSPF behaves slightly better than these last ones. Concerning delays (Figure 9b) the
14. It is worth remarking that in these and in some of the experiments presented in the following, PQ-R's
performance is slightly worse than that of Q-R. This seems to be in contrast with the results presented
by the PQ-R's authors in the article where they introduced PQ-R (Choi & Yeung, 1996). We think that
this behavior is due to the fact that (i) their link recovery rate matches a discrete-time system while
in our simulator time is a continuous variable, and (ii) the experimental and simulation conditions are
rather different (in their article it is not specified the way they produced trac patterns and they did
not implement a realistic network simulator).

342

fiAntNet: Distributed Stigmergetic Control for Communications Networks

AntNet

2.4

OSPF

2.3

SPF

2.2

BF

2.1

Q-R

2

PQ-R

Daemon

4.5
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
AntNet

OSPF

2.4

SPF

2.3

(b)

2.2

BF

2.1

Q-R

2

PQ-R

Daemon

situation is rather different, as can be seen by the fact that all the algorithms but AntNet
have been able to produce a slightly higher throughput at the expenses of much worse
results for packet delays. This trend in packet delays was confirmed by all the experiments
we ran. OSPF, Q-R and PQ-R show really poor results (delays of order 2 or more seconds
are very high values, even if we are considering the 90-th percentile of the distribution),
while BF and SPF behave in a similar way with performance of order 50% worse than those
obtained by AntNet and of order 65% worse than Daemon.
18
16
14
12

8

10

6
4
2
0

(a)

90-th percentile of packet delays (sec)

343

In the RP case (Figure 10a), throughputs generated by AntNet, SPF and BF are very
similar, although AntNet has a slightly better performance. OSPF and PQ-R behave only
slightly worse while Q-R is the worst algorithm. Daemon is able to obtain only slightly
better results than AntNet. Again, looking at packet delays results (Figure 10b) OSPF,
Q-R and PQ-R perform very badly, while SPF shows results a bit better than those of BF
but of order 40% worse than those of AntNet. Daemon is in this case far better, which
indicates that the testbed was very dicult.
For the case of UP-HS load, throughputs (Figure 11a) for AntNet, SPF, BF, Q-R and
Daemon are very similar, while OSPF and PQ-R clearly show much worse results. Again
(Figure 11b), packet delays results for OSPF, Q-R and PQ-R are much worse than those
of the other algorithms (they are so much worse that they do not fit in the scale chosen
to make clear differences among the other algorithms). AntNet is still the best performing
algorithm. In this case, differences with SPF are of order 20% and of 40% with respect to
BF. Daemon performs about 50% better than AntNet and scales much better than AntNet,
which, again, indicates the testbed was rather dicult.
The last graph for NSFNET shows how the algorithms behave in the case of a TMPHSUP situation (Figure 12). At time t = 400 four hot spots are turned on and superimposed
to the existing light UP trac. The transient is kept on for 120 seconds. In this case, only
one, typical, situation is reported in detail to show the answer curves. Reported values

Figure 9: NSFNET: Comparison of algorithms for increasing load for UP trac. The load is
increased reducing the MSIA value from 2.4 to 2 seconds (MPIA = 0.005 sec). (a)
Throughput, and (b) 90-th percentile of the packet delays empirical distribution.

Throughput (106 bit/sec)

fi12

10

8

6

4

2

0
AntNet

2.8

OSPF

2.7

SPF

(a)

2.6

BF

2.5

Q-R

2.4

Daemon

4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
AntNet

Di Caro & Dorigo

PQ-R

90-th percentile of packet delays (sec)

18
16
14
12
10
8
6
4
2
0
AntNet

OSPF

2.4

SPF

2.3

BF

2.2

Q-R

2.1

PQ-R

2

(a)

Daemon

344

0.5

0.4

0.3

0.2

0.1

0.0
AntNet

OSPF

2.8

SPF

2.7

OSPF

2.4

SPF

2.3

BF

2.6

(b)

(b)

2.2

BF

2.5

Q-R

2.1

Q-R

2.4

2

PQ-R

PQ-R

Daemon

Daemon

are the \instantaneous" values for throughput and packet delays computed as the average
over 5 seconds moving windows. All algorithms have a similar very good performance as
far as throughput is concerned, except for OSPF and PQ-R, which lose a few percent of
the packets during the transitory period. The graph of packet delays confirms previous
results: SPF and BF have a similar behavior, about 20% worse than AntNet and 45%
worse than Daemon. The other three algorithms show a big out-of-scale jump, being not
able to properly dump the sudden load increase.

Figure 11: NSFNET: Comparison of algorithms for increasing load for UP-HS trac. The load is
increased reducing the MSIA value from 2.4 to 2.0 seconds (MPIA = 0.3 sec, HS = 4,
MPIA-HS = 0.04 sec). (a) Throughput, and (b) 90-th percentile of the packet delays
empirical distribution.

90-th percentile of packet delays (sec)

Figure 10: NSFNET: Comparison of algorithms for increasing load for RP trac. The load is
increased reducing the MSIA value from 2.8 to 2.4 seconds (MPIA = 0.005 sec). (a)
Throughput, and (b) 90-th percentile of the packet delays empirical distribution.

Throughput (106 bit/sec)
Throughput (106 bit/sec)

fiThroughput (106 bit/sec)

AntNet: Distributed Stigmergetic Control for Communications Networks

16.0
14.0
12.0
10.0
8.0
6.0

Packet Delay (sec)

0.06
OSPF
SPF
BF
Q-R
PQ-R
AntNet
Daemon

0.05

0.04

0.03
200

300

400

500

600

700

800

900

1000

Simulation Time (sec)

Figure 12: NSFNET: Comparison of algorithms for transient saturation conditions with
TMPHS-UP trac (MSIA = 3.0 sec, MPIA = 0.3 sec, HS = 4, MPIA-HS =
0.04). (a) Throughput, and (b) packet delays averaged over 5 seconds moving
windows.

7.3 NTTnet
The same set of experiments run on the NSFNET have been repeated on NTTnet. In this
case the results are even sharper than those obtained with NSFNET: AntNet performance
is much better that of all its competitors.
For the UP, RP and UP-HS cases, differences in throughput are not significant (Figures
13a, 14a and 15a). All the algorithms, with the OSPF exception, practically behave in
the same way as the Daemon algorithm. Concerning delays (Figures 13b, 14b and 15b),
differences between AntNet and each of its competitors are of one order of magnitude.
AntNet keeps delays at low values, very close to those obtained by Daemon, while SPF,
BF, Q-R and PQ-R perform poorly and OSPF completely collapses.
In the UP and RP cases (Figures 13b and 14b) SPF and BF performs similarly, even if SPF
shows slightly better results, and about 50% better than Q-R and PQ-R.
In the UP-HS case, again, SPF and BF show similar results, while Q-R performs comparably but in a much more irregular way and PQ-R can keep delays about 30% lower. OSPF,
which is the worse algorithm in this case, shows an interesting behavior. The increase in the
generated data throughput determines a decrease or a very slow increase in the delivered
throughput while delays decrease (Figure 15a and 15b). In this case the load was too high
for the algorithm and the balance between the two, conicting, objectives, throughput and
345

fi45
40
35
30
25
20
15
10
5
0
AntNet

3.1

OSPF

3

SPF

BF

2.9

(a)

2.8

Q-R

2.7

Daemon

0.0

1.0

2.0

3.0

4.0

5.0

6.0

7.0

8.0

9.0

10.0

Di Caro & Dorigo

PQ-R

90-th percentile of packet delays (sec)

45
40
35
30
25
20
15
10
5
0
AntNet

OSPF

3.1

SPF

3

BF

2.9

(a)

2.8

Q-R

2.7

PQ-R

Daemon

346

10.0
9.0
8.0
7.0
6.0
5.0
4.0
3.0
2.0
1.0
0.0

AntNet

AntNet

OSPF

3.1

SPF

3

2.9

(b)

OSPF

3.1

SPF

3

2.9

(b)

BF

BF

2.8

2.8

Q-R

Q-R

2.7

2.7

PQ-R

PQ-R

Daemon

Daemon

packet delays, showed an inverse dynamics: having a lot of packet losses made it possible
for the surviving packets to obtain lower trip delays.
The TMPHS-UP experiment (Figure 16), concerning sudden load variation, confirms
the previous results. OSPF is not able to follow properly the variation both for throughput
and delays. All the other algorithms are able to follow the sudden increase in the offered
throughput, but only AntNet (and Daemon) show a very regular behavior. Differences in
packet delays are striking. AntNet performance is very close to those obtained by Daemon
(the curves are practically superimposed at the scale used in the Figure). Among the other
algorithms, SPF and BF are the best ones, although their response is rather irregular and,
in any case, much worse than AntNet's. OSPF and Q-R are out-of-scale and show a very
delayed recovering curve. PQ-R, after a huge jump, which takes the graph out-of-scale in

Figure 14: NTTnet: Comparison of algorithms for increasing load for RP trac. The load is
increased reducing the MSIA value from 3.1 to 2.7 seconds (MPIA = 0.005 sec). (a)
Throughput, and (b) 90-th percentile of the packet delays empirical distribution.

90-th percentile of packet delays (sec)

Figure 13: NTTnet: Comparison of algorithms for increasing load for UP trac. The load is
increased reducing the MSIA value from 3.1 to 2.7 seconds (MPIA = 0.005 sec). (a)
Throughput, and (b) 90-th percentile of the packet delays empirical distribution.

Throughput (106 bit/sec)
Throughput (106 bit/sec)

fiAntNet: Distributed Stigmergetic Control for Communications Networks

4

3.9

3.8

3.7

90-th percentile of packet delays (sec)

4.1
50

Throughput (106 bit/sec)

45
40
35
30
25
20
15
10
5
0
AntNet

OSPF

SPF

BF

Q-R

PQ-R

Daemon

4.1

4

3.9

3.8

3.7

7.0
6.0
5.0
4.0
3.0
2.0
1.0
0.0
AntNet

OSPF

(a)

SPF

BF

Q-R

PQ-R

Daemon

(b)

Figure 15: NTTnet: Comparison of algorithms for increasing load for UP-HS trac. The load is

increased reducing the MSIA value from 4.1 to 3.7 seconds (MPIA = 0.3 sec, HS = 4,
MPIA-HS = 0.05 sec). (a) Throughput, and (b) 90-th percentile of the packet delays
empirical distribution.

Throughput (106 bit/sec)

the first 40 seconds after hot spots are turned on, shows a trend approaching those of BF
and SPF.
55.0
45.0
35.0
25.0
15.0

Packet Delay (sec)

0.8
OSPF
SPF
BF
Q-R
PQ-R
AntNet
Daemon

0.6
0.4
0.2
0.0
200

300

400

500

600

700

800

900

1000

Simulation Time (sec)

Figure 16: NTTnet: Comparison of algorithms for transient saturation conditions with
TMPHS-UP trac (MSIA = 4.0 sec, MPIA = 0.3 sec, HS = 4, MPIA-HS =
0.05). (a) Throughput, and (b) packet delays averaged over 5 seconds moving
windows.
347

fiDi Caro & Dorigo

7.4 Routing Overhead

Table 2 reports results concerning the overhead generated by routing packets. For each
algorithm the network load generated by the routing packets is reported as the ratio between
the bandwidth occupied by the routing packets and the total available network bandwidth.
Each row in the table refers to a previously discussed experiment (Figs. 8 to 11 and 13
to 15). Routing overhead is computed for the experiment with the heaviest load in the
increasing load series.
SimpleNet - F-CBR
NSFNET - UP
NSFNET - RP
NSFNET - UP-HS
NTTnet - UP
NTTnet - RP
NTTnet - UP-HS

AntNet OSPF SPF BF Q-R PQ-R
0.33
0.01 0.10 0.07 1.49 2.01
2.39
0.15 0.86 1.17 6.96 9.93
2.60
0.15 1.07 1.17 5.26 7.74
1.63
0.15 1.14 1.17 7.66 8.46
2.85
0.14 3.68 1.39 3.72 6.77
4.41
0.14 3.02 1.18 3.36 6.37
3.81
0.14 4.56 1.39 3.09 4.81

Table 2: Routing Overhead: ratio between the bandwidth occupied by the routing packets
and the total available network bandwidth. All data are scaled by a factor of 10,3 .
All data are scaled by a factor of 10,3 . The data in the table show that the routing
overhead is negligible for all the algorithms with respect to the available bandwidth. Among
the adaptive algorithms, BF shows the lowest overhead, closely followed by SPF. AntNet
generates a slightly bigger consumption of network resources, but this is widely compensated
by the much higher performance it provides. Q-R and PQ-R produce an overhead a bit
higher than that of AntNet. The routing load caused by the different algorithms is a function
of many factors, specific of each algorithm. Q-R and PQ-R are data-driven algorithms: if
the number of data packets and/or the length of the followed paths (because of topology
or bad routing) grows, so will the number of generated routing packets. BF, SPF and
OSPF have a more predictable behavior: the generated overhead is mainly function of the
topological properties of the network and of the generation rate of the routing information
packets. AntNet produces a routing overhead depending on the ants generation rate and
on the length of the paths they travel.
The ant trac can be roughly characterized as a collection of additional trac sources,
one for each network node, producing very small packets (and related acknowledgement
packets) at constant bit rate with destinations matching the offered data trac. On average
ants will travel over rather \short" paths and their size will grow of only 8 bytes at each hop.
Therefore, each \ant routing trac source" represents a very light additional trac source
with respect to network resources when the ant launching rate is not excessively high. In
Figure 17, the sensitivity of AntNet with respect to the ant launching rate is reported.
For a sample case of a UP data trac model on NSFNET (previously studied in Figure
9) the interval g between two consecutive ant generations is progressively decreased (g
is the same for all nodes). g values are sampled at constant intervals over a logarithmic
scale ranging from about 0.006 to 25 seconds. The lower, dashed, curve interpolates the
348

fiAntNet: Distributed Stigmergetic Control for Communications Networks

AntNet Normalized Power Vs. Routing Overhead

1.0

0.8
Normalized Power
Routing Overhead
0.6

0.4

0.2

0.0
0.001

0.01

0.1

1

10

100

Interval g Between Two Consecutive Ants Generations (sec)

Figure 17: AntNet normalized power vs. routing overhead. Power is defined as the ratio
between delivered throughput and packet delay.
generated routing overhead expressed, as before, as the fraction of the available network
bandwidth used by routing packets. The upper, solid, curve plots the data for the obtained
power normalized to its highest value, where the power is defined as the ratio between the
delivered throughput and the packet delay. The value used for delivered throughput is the
throughput value at time 1000 averaged over ten trials, while for packet delay we used the
90-th percentile of the empirical distribution.
In the figure, we can see how an excessively small g causes an excessive growth of the
routing overhead, with consequent reduction of the algorithm power. Similarly, when g
is too big, the power slowly diminishes and tends toward a plateau because the number of
ants is not enough to generate and maintain up-to-date statistics of the network status. In
the middle of these two extreme regions a wide range of g intervals gives raise to similar,
very good power values, while, at the same time, the routing overhead quickly falls down
toward negligible values. This figure strongly confirms our previous assertion about the
robustness of AntNet's internal parameter settings.

8. Discussion
In AntNet, the continual on-line construction of the routing tables is the emergent result
of a collective learning process. In fact, each forward-backward agent pair is complex
enough to find a good route and to adapt the routing tables for a single source-destination
path, but it cannot solve the global routing optimization problem. It is the interaction
between the agents that determines the emergence of a global effective behavior from the
network performance point of view. Ants cooperate in their problem-solving activity by
communicating in an indirect and non-coordinated way. Each agent acts independently.
Good routes are discovered by applying a policy that is a function of the information
349

fiDi Caro & Dorigo

accessed through the network nodes visited, and the information collected about the route
is eventually released on the same nodes. Therefore, the inter-agent communication is
mediated in an explicit and implicit way by the \environment", that is, by the node's data
structures and by the trac patterns recursively generated by the data packets' utilization
of the routing tables. This communication paradigm, called stigmergy, matches well the
intrinsically distributed nature of the routing problem. Cooperation among agents goes
on at two levels: (a) by modifications of the routing tables, and (b) by modifications of
local models that determine the way the ants' performance is evaluated. Modifications of
the routing tables directly affect the routing decisions of following ants towards the same
destination, as well as the routing of data, which, in turn, inuences the rate of arrival
of other ants towards any destination. It is interesting to remark that the used stigmergy
paradigm makes the AntNet's mobile agents very exible from a software engineering point
of view. In this perspective, once the interface with the node's data structure is defined,
the internal policy of the agents can be transparently updated. Also, the agents could be
exploited to carry out multiple concurrent tasks (e.g., collecting information for distributed
network management using an SNMP-like protocol or for Web data-mining tasks).
As shown in the previous section, the results we obtained with the above stigmergetic
model of computation are excellent. In terms of throughput and average delay, AntNet
performs better than both classical and recently proposed routing algorithms on a wide
range of experimental conditions. Although this is very interesting per se, in the following
we try to justify AntNet superior performance by highlighting some of its characteristics
and by comparing them with those of the competing algorithms. We focus on the following
main aspects:

 AntNet can be seen as a particular instance of a parallel Monte Carlo simulation






system with biased exploration. All the other algorithms either do not explore the
net or their exploration is local and tightly connected to the ux of data packets.
The information AntNet maintains at each node is more complete and organized in a
less critical way than that managed by the other algorithms.
AntNet does not propagate local estimates to other nodes, while all its competitors
do. This mechanism makes the algorithm more robust to locally wrong estimates.
AntNet uses probabilistic routing tables, which have the triple positive effect of better redistributing data trac on alternative routes, of providing ants with a built-in
exploration mechanism and of allowing the exploitation of the ants' arrival rate to
assign cumulative reinforcements.
It was experimentally observed that AntNet is much more robust than its competitors
to the frequency with which routing tables are updated.
The structure of AntNet allows one to draw some parallels with some well-known
reinforcement learning (RL) algorithms. The characteristics of the routing problem,
that can be seen as a distributed time-varying RL problem (see sect. 2.2), determines
a departure of AntNet from the structure of classical RL algorithms.

These aspects of AntNet are discussed in more detail in the following.
350

fiAntNet: Distributed Stigmergetic Control for Communications Networks

8.1 AntNet as an on-line Monte Carlo system with biased exploration
The AntNet routing system can be seen as a collection of mobile agents collecting data
about the network status by concurrently performing on-line Monte Carlo simulations (Rubistein, 1981; Streltsov & Vakili, 1996). In Monte Carlo methods, repeated experiments
with stochastic transition components are run to collect data about the statistics of interest. Similarly, in AntNet ants explore the network by performing random experiments
(i.e., building paths from source to destination nodes using a stochastic policy dependent
on the past and current network states), and collect on-line information on the network
status. A built-in variance reduction effect is determined (i) by the way ants' destinations
are assigned, biased by the most frequently observed data's destinations, and (ii) by the way
the ants' policy makes use of current and past trac information (that is, inspection of the
local queues' status and probabilistic routing tables). In this way, the explored paths match
the most interesting paths from a data trac point of view, which results in a very ecient
variance reduction effect in the stochastic sampling of the paths. Differently from usual
off-line Monte Carlo systems, in AntNet the state space sampling is performed on-line, that
is, the sampling of the statistics and the controlling of the non-stationary trac process are
performed concurrently.
This way of exploring the network concurrently with data trac is very different from
what happens in the other algorithms where, either there is no exploration at all (OSPF,
SPF and BF), or exploration is both tightly coupled to data trac and of a local nature
(Q-R and PQ-R). Conveniently, as was shown in Section 7.4, the extra trac generated by
exploring ants is negligible for a wide range of values, allowing very good performance.

8.2 Information management at each network node
Key characteristics of routing algorithms are the type of information used to build/update
routing tables and the way this information is propagated. All the algorithms (except the
static OSPF) make use at each node of two main components: a local model M of some
cost measures and a routing table T . SPF and BF use M to estimate smoothed averages
of the local link costs, that is, of the distances to the neighbor nodes. In this case, M is
a local model maintaining estimates of only local components. In Q-R the local model is
fictitious because the raw transition time is directly used as a value to update T . PQ-R
uses a slightly more sophisticated model with respect to Q-R, storing also a measure of the
link utilization. All these algorithms propagate part of their local information to the other
nodes, which, in turn, make use of it to update their routing tables and to build a global
view of the network. In SPF and BF the content of each T is updated, at regular intervals,
by a \memoryless strategy": the new entries do not depend on the old values, that are
discarded. Therefore, the whole adaptive component of the routing system is represented
by the model M. Otherwise, in Q-R and PQ-R the adaptive content of M is almost
negligible and the adaptive component of the algorithm is represented by the smoothed
average carried out by the Q-learning-like rule. AntNet shows characteristics rather different
from its competitors: its model M contains a memory-based local perspective of the global
status of the network. The content of M allow the reinforcements to be weighted on the
basis of a rich statistical description of the network dynamics as seen by the local node.
These reinforcements are used to update the routing table, the other adaptive component
351

fiDi Caro & Dorigo

maintained at the node. The T updates are carried out in an asynchronous way and as a
function of their previous values. Moreover, while T is used in a straightforward probabilistic
way by the data packets, traveling ants select the next node by using both T , that is, an
adaptive representation of the past policy, and a model of the current local link queues,
that is, an instantaneous representation of the node status. It is evident that AntNet builds
and uses more information than its competitors: two different memory-based components
and an instantaneous predictor are used and combined at different levels. Moreover, in this
way AntNet robustly redistributes among these completely local components the criticality
of all the estimates and decisions.

8.3 AntNet's robustness to wrong estimates

As remarked above, AntNet, differently from its competitors, does not propagate local
estimates to other nodes. Each node routing table is updated independently, by using
local information and the ants' experienced trip time. Moreover, (i) each ant experiment
affects only one entry in the routing table of the visited nodes, the one relative to the ant's
destination, and, (ii) the local information is built from the \global" information collected
by traveling ants, implicitly reducing in this way the variance in the estimates. These
characteristics make AntNet particularly robust to wrong estimates. On the contrary, in
all the other algorithms a locally wrong estimate will be propagated to all other nodes and
will be used to compute estimates to many different destinations. How bad this is for the
algorithm performance depends on how long the wrong estimate effect lasts. In particular,
this will be a function of the time window over which estimates are computed for SPF and
BF, and of the learning parameters for Q-R and PQ-R.

8.4 AntNet's probabilistic use of routing tables to route data packets

All the tested algorithms but AntNet use deterministic routing tables.15 In these algorithms,
entries in the routing tables contain distance/time estimates to the destinations. These
estimates can provide misleading information if the algorithm is not fast enough to follow
the trac uctuations, as can be the case under heavy load conditions. Instead, AntNet
routing tables have probabilistic entries that, although reecting the goodness of a particular
path choice with respect to the others available, do not force the data packets to choose
the perceived best path. This has the positive effect of allowing a better balancing of
the trac load on different paths, with a resulting better utilization of the resources (as
was shown in particular in the experiments with the SimpleNet). As remarked at the
end of Section 4.1, the intrinsic probabilistic structure of the routing tables and the way
they are updated allow AntNet to exploit the ant's arrival rate as a way to assign implicit
(cumulative) reinforcements to discovered paths. It is not obvious how the same effect
could be obtained by using routing tables containing distance/time estimates and using
this estimates in a probabilistic way. In fact, in this case each new trip time sample would
15. Singh, Jaakkola, and Jordan (1994) showed that stochastic policies can yield higher performance than
deterministic policies in the case of an incomplete access to the state information of the environment. In
(Jaakkola, Singh, & Jordan, 1995), the same authors developed a Monte-Carlo-based stochastic policy
evaluation algorithm, confirming the usefulness of the Monte-Carlo approach, used in AntNet too, to
deal with incomplete information problems.

352

fiAntNet: Distributed Stigmergetic Control for Communications Networks

modify the statistical estimate that would simply oscillate around its expected value without
inducing an arrival-dependent cumulative effect.
Probabilistic routing tables provide some remarkable additional benefits: (a) they give to
the ants a built-in exploration method in discovering new, possibly better, paths, and (b)
since ants and data routing are independent in AntNet, the exploration of new routes
can continue while, at the same time, data packets can exploit previously learned, reliable
information. It is interesting to note that the use of probabilistic routing tables whose entries
are learned in an adaptive way by changing on positive feedback and ignoring negative
feedback, is reminiscent of older automata approaches to routing in telecommunications
networks. In these approaches, a learning automaton is usually placed on each network
node. An automaton is defined by a set of possible actions and a vector of associated
probabilities, a continuous set of inputs and a learning algorithm to learn input-output
associations. Automata are connected in a feedback configuration with the environment
(the whole network), and a set of penalty signals from the environment to the actions is
defined. Routing choices and modifications to the learning strategy are carried out in a
probabilistic way and according to the network conditions (see for example (Nedzelnitsky
& Narendra, 1987; Narendra & Thathachar, 1980)). The main difference lies in the fact
that in AntNet the ants are part of the environment itself, and they actively direct the
learning process towards the most interesting regions of the search space. That is, the
whole environment plays a key, active role in learning good state-action pairs.

8.5 AntNet robustness to routing table update frequency

In BF and SPF the broadcast frequency of routing information plays a critical role, particularly so for BF, which has only a local representation of the network status. This frequency
is unfortunately problem dependent, and there is no easy way to make it adaptive, while,
at the same time, avoiding large oscillations. In Q-R and PQ-R, routing tables updating
is data driven: only those Q-values belonging to pairs (i; j ) of neighbor nodes visited by
packets are updated. Although this is a reasonable strategy given that the exploration of
new routes could cause undesired delays to data packets, it causes delays in discovering new
good routes, and is a great handicap in a domain where good routes could change all the
time. In OSPF, in which routing tables are not updated, we set static link costs on the
basis of their physical characteristics. This lack of an adaptive metric is the main reason
of the poor performance of OSPF (as remarked in Section 5, we slightly penalized OSPF
with respect to its real implementations, where additional heuristic knowledge about trac
patterns is used by network administrators to set link costs). In AntNet, we experimentally
observed the robustness to changes in the ants' generation rate: for a wide range of generation rates, rather independent of the network size, the algorithm performance is very good
and the routing overhead is negligible (see Section 7.4).

8.6 AntNet and reinforcement learning

The characteristics of the routing problem allow one to interpret it as a distributed, stochastic time-varying RL problem. This fact, as well as the structure of AntNet, make it natural
to draw some parallels between AntNet and classical RL approaches. It is worth remarking
that those RL problems that have been most studied, and for which algorithms have been de353

fiDi Caro & Dorigo

veloped, are problems where, unlike routing, assumptions like Markovianity or stationarity
of the process considered are satisfied. The characteristics of the adaptive routing problem
make it very dicult and not well suited to be solved with usual RL algorithms. This fact,
as we explain below, determines a departure of AntNet from classical RL algorithms.
A first way to relate the structure of AntNet to that of a (general) RL algorithm is
connected to the way the outcomes of the experiments, the trip times Tk!d , are processed.
The transformation from the raw values Tk!d to the more refined reinforcements r are
reminiscent of what happens in Actor-Critic systems (Barto, Sutton, & Anderson, 1983):
the raw reinforcement signal is processed by a critic module, which is learning a model (the
node's component M) of the underlying process, and then is fed to the learning system (the
routing table T ) transformed into an evaluation of the policy followed by the ants. In our
case, the critic is both adaptive, to take into account the variability of the trac process,
and rather simple, to meet computational requirements.
Another way of seeing AntNet as a classical RL system is related to its interpretation as
a parallel replicated Monte Carlo (MC) system. As was shown by Singh and Sutton (1996),
a first-visit MC (only the first visit to a state is used to estimate its value during a trial)
simulation system is equivalent to a batch temporal difference (TD) method with replacing
traces and decay parameter =1. Although AntNet is a first-visit MC simulation system,
there are some important differences with the type of MC used by Singh and Sutton (and
in other RL works), mainly due to the differences in the considered class of problems. In
AntNet, outcomes of experiments are both used to update local models able to capture
the variability of the whole network status (only partially observable) and to generate a
sequence of stochastic policies. On the contrary, in the MC system considered by Singh and
Sutton, outcomes of the experiments are used to compute (reduced) maximum-likelihood
estimates of the expected mean and variance of the states' returns (i.e., the total reward
following a visit of a state) of a Markov chain. In spite of these differences, the weak parallel
with TD() methods is rather interesting, and allows to highlight an important difference
between AntNet and its competitors (and general TD methods): in AntNet, following the
generation of a stochastic transition chain by the forward ant, there is no back-chaining
of the information from one state (i.e., a triple fcurrent node, destination node, next hop
nodeg) to its predecessors. Each state is rewarded only on the basis of the ant's trip time
information strictly relevant to it. This approach is completely different from that followed
by (TD methods) Q-R, PQ-R, BF and, in a different perspective, by SPF. In fact, these
algorithms build the distance estimates at each node by using the predictions made at other
nodes. In particular, Q-R and PQ-R, which propagate the estimation information only one
step back, are precisely distributed versions of the TD(0) class of algorithms. They could be
transformed into generic TD(), 0 <   1, by transmitting backward to all the previously
visited nodes the information collected by the routing packet generated after each data hop.
Of course, this would greatly increase the routing trac generated, because it has to be
done after each hop of each data packet, making the approach at least very costly, if feasible
at all.
In general, using temporal differences methods in the context of routing presents an important problem: the key condition of the method, the self-consistency between the estimates
of successive states16 may not be strictly satisfied in the general case. This is due to the
16. For instance, the prediction made at node k about the time to-go to the destination node d should be

354

fiAntNet: Distributed Stigmergetic Control for Communications Networks

fact that (i) the dynamics at each node are related in a highly non-linear way to the dynamics of all its neighbors, (ii) the trac process evolves concurrently over all the nodes,
and (iii) there is a recursive interaction between the trac patterns and the control actions
(that is, the modifications of the routing tables). This aspect can explain in part the poor
performance of the pure TD(0) algorithms Q-R and PQ-R.

9. Related Work
Algorithms based on the ant colony metaphor were inspired by the ant colony foraging
behavior (Beckers et al., 1992). These were first proposed by Dorigo (1992), Colorni et
al. (1991) and Dorigo et al. (1991, 1996) and were applied to the traveling salesman
problem (TSP). Apart from the natural metaphor, the idea behind that first application
was similar to the one presented in this paper: a set of agents that repeatedly run Monte
Carlo experiments whose outcomes are used to change the estimates of some variables used
by subsequent ants to build solutions. In ant-cycle, one of the first ant-based algorithms,
a value called \pheromone trail" is associated to each edge of the graph representing the
TSP. Each ant builds a tour by exploiting the pheromone trail information as follows.
When in node i an ant chooses the next node j to move to among those not visited yet
with a probability Pij that is a function of the amount of pheromone trail on the edge
connecting i to j (as well as of a local heuristic function; the interested reader can find a
detailed description of ant-cycle elsewhere (Dorigo, 1992; Dorigo et al., 1996)). The value
of the pheromone trails is updated once all ants have built their tours. Each ant adds
to all visited edges a quantity of pheromone trail proportional to the quality of the tour
generated (the shorter the tour, the higher the quantity of pheromone trail added). This
has an effect very similar to AntNet's increase of routing tables probabilities, since a higher
pheromone trail on a particular edge will increase its probability of being chosen in the
future. There are obviously many differences between ant-cycle and AntNet, mostly due
to the very different types of problems to which they have been applied, a combinatorial
optimization problem versus a distributed, stochastic, time varying, real-time problem.
Though the majority of previous applications of ant colony inspired algorithms concern combinatorial optimization problems, there have been recent applications to routing.
Schoonderwoerd et al. (1996, 1997) were the first to consider routing as a possible application domain for ant colony algorithms. Their ant-based control (ABC) approach, which is
applied to routing in telephone networks, differs from AntNet in many respects. The main
differences are a direct consequence of the different network model they considered, which
has the following characteristics (see Figure 18): (i) connection links potentially carry an
infinite number of full-duplex, fixed bandwidth channels, and (ii) transmission nodes are
crossbar switches with limited connectivity (that is, there is no necessity for queue management in the nodes). In such a model, bottlenecks are put on the nodes, and the congestion
degree of a network can be expressed in terms of connections still available at each switch.
As a result, the network is cost-symmetric: the congestion status over available paths is
completely bi-directional. The path n0 ; n1 ; n2 ; : : : ; nk connecting n0 and nk will exhibit the
additively related to the prediction for the same destination from each one of k's neighbors, being each
neighbor one of the ways to go to d.

355

fiDi Caro & Dorigo

same level of congestion in both directions because the congestion depends only on the state
of the nodes in the path. Moreover, dealing with telephone networks, each call occupies
exactly one physical channel across the path.
Therefore, \calls" are not multiplexed over the
Link 1
links, but they can be accepted or refused, depending on the possibility of reserving a physical
N bidirectional channels
circuit connecting the caller and the receiver. All
Link 2
of these modeling assumptions make the probLink 4
lem of Schoonderwoerd et al. very different from
the cost-asymmetric routing problem for data
networks we presented in this paper. This difn << N possible connections
ference is reected in many algorithmic differences between ABC and AntNet, the most imFigure 18: Network node in the portant of which is that in ABC ants update
telecommunications network pheromone trails after each step, without waiting
model of Schoonderwoerd et for the completion of an experiment as done in
AntNet. This choice, which is reminiscent of the
al. (1996).
pheromone trail updating strategy implemented
in ant-density, another of the first ant colony based algorithms (Dorigo et al., 1991; Dorigo,
1992; Colorni et al., 1991), makes ABC behavior closer to real ants', and was made possible
by the cost-symmetry assumption made by the authors.
Other differences are that ABC does not use local models to score the ants trip times,
nor local heuristic information and ant-private memory to improve the ants decision policies.
Also, it does not recover from cycles and does not use the information contained in all the
ant sub-paths.
Because of the different network model used and of the many implementation details
tightly bound to the network model, it was impossible for us to re-implement and compare
the ABC algorithm with AntNet.
Subramanian, Druschel, and Chen (1997) have proposed an ant-based algorithm for
packet-switched nets. Their algorithm is a straightforward extension of Schoonderwoerd
et al. system by adding so-called uniform ants, an additional exploration mechanism that
should avoid a rapid sub-optimal convergence of the algorithm. A limitation of Subramanian
et al. work is that, although the algorithm they propose is based on the same cost-symmetry
hypothesis as ABC, they apply it to packet-switched networks where this requirement is
very often not met.
Link 3

10. Conclusions and Future Work
In this paper, we have introduced AntNet, a novel distributed approach to routing in packetswitched communications networks. We compared AntNet with 6 state-of-the-art routing
algorithms on a variety of realistic testbeds. AntNet showed superior performance and
robustness to internal parameter settings for almost all the experiments. AntNet's most
innovative aspect is the use of stigmergetic communication to coordinate the actions of a
set of agents that cooperate to build adaptive routing tables. Although this is not the
first application of stigmergy-related concepts to optimization problems (e.g., Dorigo et al.,
356

fiAntNet: Distributed Stigmergetic Control for Communications Networks

1991; Dorigo, 1992; Dorigo et al., 1996; Bonabeau, Dorigo, & Theraulaz, 1999), the application presented here is unique in many respects. First, in AntNet, stigmergy-based control
is coupled to a model-building activity: information collected by ants is used not only to
modify routing tables, but also to build local models of the network status to be used to
better direct the routing table modifications. Second, this is the first attempt to evaluate
stigmergy-based control on a realistic simulator of communications networks: the used simulator retains many of the basic components of a real routing system. An interesting step
forward, in the direction of testing the applicability of the idea presented to real networks,
would be to rerun the experiments presented here using a complete Internet simulator.
Third, this is also the first attempt to evaluate stigmergy-based control by comparing a
stigmergetic algorithm to state-of-the-art algorithms on a realistic set of benchmark problems. It is very promising that AntNet turned out to be the best performing in all the
tested conditions.
There are obviously a number of directions in which the current work could be extended,
which are listed below.
1) A first, natural, extension of the current work would consider the inclusion in the
simulator of ow and congestion control components (with re-transmissions and error management). This inclusion will require a paired tuning of the routing and ow-congestion
components, to select the best matching between their dynamics.
2) In AntNet, each forward ant makes a random experiment: it builds a path from a
source node s to a destination node d. The path is built exploiting the information contained
in the probabilistic routing tables and the status of the queues of the visited nodes. While
building the path, the ant collects information on the status of the network. This is done
by sharing link queues with data packets, and by measuring waiting times of queues and
traversal times that will be used as raw reinforcements by backward ants. Since forward
ants share queues with data packets, the time required to run an experiment depends on
the network load, and is approximately the same as the time Ts!d required for a packet to
go from the same source node s to the same destination node d. This delays the moment
the information collected by forward ants can be distributed by backward ants, and makes
it less up-to-date than it could be. A possible improvement in this schema would be to
add a model of link-queue depletion to nodes, and to let forward ants use high priority
queues to reach their destinations without storing crossing times (for a first step in this
direction see Di Caro & Dorigo, 1998). Backward ants would then make the same path, in
the opposite direction, as forward ants, but use the queue local models they find on their
way to estimate local \virtual" queueing and crossing times. Raw reinforcements, used to
update the routing tables, are then computed using these estimates. Clearly, here there is a
trade-off between delayed but real information and more recent but estimated information.
It will be interesting to see which scheme works better, although we are confident that the
local queue models should allow the backward ants to build estimates accurate enough to
make the improved system more effective than the current AntNet, at a cost of a little
increase in computational complexity at the nodes.
3) As we discussed in Section 8, AntNet is missing one of the main components of classical
RL/TD algorithms: there is no back-chaining of information from a state to previous ones,
each node policy is learned by using a complete local perspective. An obvious extension
of our work would therefore be to study versions of AntNet closer to TD() algorithms.
357

fiDi Caro & Dorigo

In this case each node should maintain Q-values expressing the estimate of the distance
to each destination via each neighbor. These estimates should be updated by using both
the ant trip time outcome and the estimates coming from successive nodes (closer to the
destination node) that could be also carried by the backward ant.
4) In this paper we applied AntNet to routing in datagram communications networks. It
is reasonable to think that AntNet could be easily adapted to be used for the generation of
real-time car route guidance in Dynamic Trac Assignment (DTA) systems (see for example
Yang, 1997). DTA systems exploit currently available and emerging computer, communication, and vehicle sensing technologies to monitor, manage and control the transportation
system (the attention is now focused mainly on highway systems) and to provide various
levels of information and advice to system users so that they can make timely and informed
travel decisions. Therefore, adaptive routing of vehicle trac presents very similar features
to the routing of data packets in communications networks. Moreover, vehicle trac control
systems have the interesting property of a very simplified \transport" layer. In fact, many
activities that interfere with routing and that are implemented in the transport layer of
communications networks do not exist, or exist only to a limited extent, in vehicles trac
control algorithms. For example, typical transport layer activities like data acknowledgement and retransmission cannot be implemented with real vehicles. Other activities, like
ow control, have strong constraints (e.g., people would not be happy to be forbidden to
leave their oces for, say, one hour on the grounds that there are already too many cars on
the streets!). This makes AntNet still more interesting since it can express its full potential
as a routing algorithm.
5) In AntNet, whenever an ant uses a link its desirability (probability) is incremented.
Although this strategy, which finds its roots in the ant colony biological metaphor that
inspired our work, allowed us to obtain excellent results, it would be interesting to investigate
the use of negative reinforcements, even if it can potentially lead to stability problems, as
observed by people working on older automata systems. As discussed before, AntNet differs
from automata systems because of the active role played by the ants. Therefore, the use
of negative reinforcements could show itself to be effective, for example, in reducing the
probability of choosing a given link if the ant that used it performed very badly.

Acknowledgements
This work was supported by a Madame Curie Fellowship awarded to Gianni Di Caro (CECTMR Contract N. ERBFMBICT 961153). Marco Dorigo is a Research Associate with the
FNRS. We gratefully acknowledge the help received from Tony Bagnall, Nick Bradshaw and
George Smith, who proofread and commented an earlier draft of this paper, as well as the
many useful comments provided by the three anonymous referees and by Craig Boutilier,
the associate editor who managed the review process.

Appendix A. Optimal and Shortest Path Routing
In this appendix, the characteristics of the two most used routing paradigms, optimal and
shortest path routing (introduced in Section 2.1) are summarized:
358

fiAntNet: Distributed Stigmergetic Control for Communications Networks

A.1 Optimal routing
Optimal routing (Bertsekas & Gallager, 1992) has a network-wide perspective and its objective is to optimize a function of all individual link ows.
Optimal routing models are also called ow models because they try to optimize the total
mean ow on the network. They can be characterized as multicommodity ow problems,
where the commodities are the trac ows between the sources and the destinations, and
the cost to be optimized is a function of the ows, subject to the constraints of ow conservation at each node and positive ow on every link. It is worth observing that the ow
conservation constraint can be explicitly stated only if the trac arrival rate is known.
The routing policy consists of splitting any source-target trac pair at strategic points,
then shifting trac gradually among alternative routes. This often results in the use of
multiple paths for a same trac ow between an origin-destination pair.
Implicit in optimal routing is the assumption that the main statistical characteristics of the
trac are known and not time-varying. Therefore, optimal routing can be used for static
and centralized/decentralized routing. It is evident that this kind of solution suffers all the
problems of static routers.

A.2 Shortest path routing
Shortest path routing (Wang & Crowcroft, 1992) has a source-destination pair perspective.
As opposed to optimal routing, there is no global cost function to be optimized. Instead,
the route between each node pair is considered by itself and no a priori knowledge about
the trac process is required (although of course such knowledge could be fruitfully used).
If costs are assigned in a dynamic way, based on statistical measures of the link congestion
state, a strong feedback effect is introduced between the routing policies and the trac
patterns. This can lead to undesirable oscillations, as has been theoretically predicted and
observed in practice (Bertsekas & Gallager, 1992; Wang & Crowcroft, 1992). Some very
popular cost metrics take into account queuing and transmission delays, link usage, link
capacity and various combination of these measures. The way costs are updated usually
involves attempting to reduce big variations considering both long-term and short-term
statistics of link congestion states (Khanna & Zinky, 1989; Shankar, Alaettinoglu, DussaZieger, & Matta, 1992b).
On the other hand, if the costs are static, they will reect both some measure of the
expected/wished trac load over the links and their transmission capacity. Of course,
serious loss of eciency could arise in case of non-stationary conditions or when the a priori
assumptions about the trac patterns are strongly violated in practice.
Considering the different content stored in each routing table, shortest path algorithms can
be further subdivided in two classes called distance-vector and link-state (Steenstrup, 1995;
Shankar et al., 1992b). The common behavior of most shortest path algorithms can be
depicted as follows.
1. Each node assigns a cost to each of its outgoing links. This cost can be static or
dynamic. In the latter case, it is updated in presence of a link failure or on the basis
of some observed link-trac statistics averaged over a defined time-window.
359

fiDi Caro & Dorigo

2. Periodically and without a required inter-node synchronization, each node sends to all
of its neighbors a packet of information describing its current estimates about some
quantities (link costs, distance from all the other nodes, etc.).
3. Each node, upon receiving the information packet, updates its local routing table and
executes some class-specific actions.
4. Routing decisions can be made in a deterministic way, choosing the best path indicated
by the information stored in the routing table, or adopting a more exible strategy
which uses all the information stored in the table to choose some randomized or
alternative path.
In the following, the main features specific to each class are described.

A.2.1 Distance-vector
Distance-vector algorithms make use of routing tables consisting of a set of triples of the
form (Destination, Estimated Distance, Next Hop), defined for all the destinations in the
network and for all the neighbor nodes of the considered switch.17 In this case, the required
topological information is represented by the list of the reachable nodes identifiers. The
average per node memory occupation is of order O(Nn), where N is the number of nodes in
the network and n is the average connectivity degree (i.e., the average number of neighbor
nodes considered over all the nodes).
The algorithm works in an iterative, asynchronous and distributed way. The information
that every node sends to its neighbors is the list of its last estimates of the distances from
itself to all the other nodes in the network. After receiving this information from a neighbor
node j , the receiving node i updates its table of distance estimates overwriting the entry
corresponding to node j with the received values.
Routing decisions at node i are made choosing as next hop node the one satisfying the
relationship:
arg min fdij + Dj g
j 2N
i

where dij is the assigned cost to the link connecting node i with its neighbor j and Dj is
the estimated shortest distance from node j to the destination.
It can be shown that this process converges in finite time to the shortest paths with
respect to the used metric if no link cost changes after a given time (Bertsekas & Gallager,
1992).
The above briey described algorithm is known in literature as distributed
Bellman-Ford (Bellman, 1958; Ford & Fulkerson, 1962; Bertsekas & Gallager, 1992) and it
is based on the principles of dynamic programming (Bellman, 1957; Bertsekas, 1995). It
is the prototype and the ancestor of a wider class of distance-vector algorithms (Malkin
& Steenstrup, 1995) developed with the aim of reducing the risk of circular loops and of
accelerating the convergence in case of rapid changes in link costs.
17. In some cases, only the best estimates are kept at nodes. Therefore, the above triples are defined for all
the destinations only.

360

fiAntNet: Distributed Stigmergetic Control for Communications Networks

A.2.2 Link-state

Link-state algorithms make use of routing tables containing much more information than
that used in vector-distance algorithms. In fact, at the core of link-state algorithms there is a
distributed and replicated database. This database is essentially a dynamic map of the whole
network, describing the details of all its components and their current interconnections.
Using this database as input, each node calculates its best paths using an appropriate
algorithm like Dijkstra's (1959) algorithm (a wide variety of alternative ecient algorithms
are available, as described for example in Cherkassky, Goldberg, & Radzik, 1994). The
memory requirements for each node in this case are O(N 2 ).
In the most common form of link-state algorithm, each node acts autonomously, broadcasting information about its link costs and states and computing shortest paths from itself
to all the destinations on the basis of its local link costs estimates and of the estimates
received from other nodes. Each routing information packet is broadcast to all the neighbor
nodes that in turn send the packet to their neighbors and so on. A distributed ooding
mechanism (Bertsekas & Gallager, 1992) supervises this information transmission trying to
minimize the number of re-transmissions.
As in the case of vector-distance, the described algorithm is a general template and a
variety of different versions have been implemented to make the algorithm behavior more
robust and ecient (Moy, 1998).

References

Alaettinoglu, C., Shankar, A. U., Dussa-Zieger, K., & Matta, I. (1992). Design and implementation of MaRS: A routing testbed. Tech. rep. UMIACS-TR-92-103, CS-TR-2964,
Institute for Advanced Computer Studies and Department of Computer Science, University of Maryland, College Park (MD).
Barto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements that
can solve dicult learning control problems. IEEE Transaction on Systems, Man and
Cybernetics, SMC-13, 834{846.
Beckers, R., Deneubourg, J. L., & Goss, S. (1992). Trails and U-turns in the selection of the
shortest path by the ant Lasius Niger. Journal of Theoretical Biology, 159, 397{415.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bellman, R. (1958). On a routing problem. Quarterly of Applied Mathematics, 16 (1), 87{90.
Bertsekas, D. (1995). Dynamic Programming and Optimal Control. Athena Scientific.
Bertsekas, D., & Gallager, R. (1992). Data Networks. Prentice-Hall.
Bertsekas, D., & Tsitsiklis, J. (1996). Neuro-Dynamic Programming. Athena Scientific.
Bolding, K., Fulgham, M. L., & Snyder, L. (1994). The case for chaotic adaptive routing.
Tech. rep. CSE-94-02-04, Department of Computer Science, University of Washington,
Seattle.
361

fiDi Caro & Dorigo

Bonabeau, E., Dorigo, M., & Theraulaz, G. (1999). From Natural to Artificial Swarm
Intelligence. Oxford University Press.
Boyan, J., & Littman, M. (1994). Packet routing in dinamically changing networks: A reinforcement learning approach. In Advances in Neural Information Processing Systems
6 (NIPS6), pp. 671{678. San Francisco, CA:Morgan Kaufmann.
Brakmo, L. S., O'Malley, S. W., & Peterson, L. L. (1994). TCP vegas: New techniques for
congestion detection and avoidance. ACM Computer Communication Review (SIGCOMM'94), 24 (4).
Cheng, C., Riley, R., Kumar, S. P. R., & Garcia-Luna-Aceves, J. J. (1989). A loop-free
extended bellman-ford routing protocol without bouncing effect. ACM Computer
Communication Review (SIGCOMM '89), 18 (4), 224{236.
Cherkassky, B. V., Goldberg, A. V., & Radzik, T. (1994). Shortest paths algorithms: Theory
and experimental evaluation. In Sleator, D. D. (Ed.), Proceedings of the 5th Annual
ACM-SIAM Symposium on Discrete Algorithms (SODA 94), pp. 516{525 Arlington,
VA. ACM Press.
Choi, S., & Yeung, D.-Y. (1996). Predictive Q-routing: A memory-based reinforcement
learning approach to adaptive trac control. In Advances in Neural Information
Processing Systems 8 (NIPS8), pp. 945{951. MIT Press.
Colorni, A., Dorigo, M., & Maniezzo, V. (1991). Distributed optimization by ant colonies.
In Proceedings of the European Conference on Artificial Life (ECAL 91), pp. 134{142.
Elsevier.
Costa, D., & Hertz, A. (1997). Ants can colour graphs. Journal of the Operational Research
Society, 48, 295{305.
Crawley, E., Nair, R., Rajagopalan, B., & Sandick, H. (1996). A framework for QoS-based
routing in the internet. Internet Draft (expired in September, 1997) draft-ietf-qosrframework-00, Internet Engineering Task Force (IEFT).
Danzig, P. B., Liu, Z., & Yan, L. (1994). An evaluation of TCP Vegas by live emulation.
Tech. rep. UCS-CS-94-588, Computer Science Department, University of Southern
California, Los Angeles.
Di Caro, G., & Dorigo, M. (1998). Two ant colony algorithms for best-effort routing
in datagram networks. In Proceedings of the Tenth IASTED International Conference on Parallel and Distributed Computing and Systems (PDCS'98), pp. 541{546.
IASTED/ACTA Press.
Dijkstra, E. W. (1959). A note on two problems in connection with graphs. Numer. Math.,
1, 269{271.
Dorigo, M. (1992). Optimization, Learning and Natural Algorithms (in Italian). Ph.D.
thesis, Dipartimento di Elettronica e Informazione, Politecnico di Milano, IT.
362

fiAntNet: Distributed Stigmergetic Control for Communications Networks

Dorigo, M., Di Caro, G., & Gambardella, L. M. (1998). Ant algorithms for distributed
discrete optimization. Tech. rep. 98-10, IRIDIA, Universite Libre de Bruxelles. Submitted to Artificial Life.
Dorigo, M., & Gambardella, L. M. (1997). Ant colony system: A cooperative learning
approach to the traveling salesman problem. IEEE Transactions on Evolutionary
Computation, 1 (1), 53{66.
Dorigo, M., Maniezzo, V., & Colorni, A. (1991). Positive feedback as a search strategy.
Tech. rep. 91-016, Dipartimento di Elettronica, Politecnico di Milano, IT.
Dorigo, M., Maniezzo, V., & Colorni, A. (1996). The ant system: Optimization by a colony
of cooperating agents. IEEE Transactions on Systems, Man, and Cybernetics{Part
B, 26 (1), 29{41.
Ford, L., & Fulkerson, D. (1962). Flows in Networks. Prentice-Hall.
Goss, S., Aron, S., Deneubourg, J. L., & Pasteels, J. M. (1989). Self-organized shortcuts in
the Argentine ant. Naturwissenschaften, 76, 579{581.
Grasse, P. P. (1959). La reconstruction du nid et les coordinations interindividuelles
chez bellicositermes natalensis et cubitermes sp. La theorie de la stigmergie: essai
d'interpretation du comportement des termites constructeurs. Insectes Sociaux, 6,
41{81.
Gray, R., Kotz, D., Nog, S., Rus, D., & Cybenko, G. (1997). Mobile agents: The next
generation in distributed computing. In Proceedings of the Second Aizu International
Symposium on Parallel Algorithms/Architectures Synthesis (pAs '97), pp. 8{24. IEEE
Computer Society Press.
Jaakkola, T., Singh, S. P., & Jordan, M. I. (1995). Reinforcement learning algorithm for
partially observable Markov decision problems. In Advances in Neural Information
Processing Systems 7, pp. 345{352. MIT Press.
Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: A survey.
Journal of Artificial Intelligence Research, 4, 237{285.
Khanna, A., & Zinky, J. (1989). The revised ARPANET routing metric. ACM SIGCOMM
Computer Communication Review, 19 (4), 45{56.
Malkin, G. S., & Steenstrup, M. E. (1995). Distance-vector routing. In Steenstrup, M. E.
(Ed.), Routing in Communications Networks, chap. 3, pp. 83{98. Prentice-Hall.
McCallum, A. K. (1995). Reinforcement learning with selective perception and hidden state.
Ph.D. thesis, Department of Computer Science, University of Rochester, Rochester
(NY).
McQuillan, J. M., Richer, I., & Rosen, E. C. (1980). The new routing algorithm for the
ARPANET. IEEE Transactions on Communications, 28, 711{719.
363

fiDi Caro & Dorigo

Merlin, P., & Segall, A. (1979). A failsafe distributed routing protocol. IEEE Transactions
on Communications, COM-27 (9), 1280{1287.
Moy, J. T. (1998). OSPF Anatomy of an Internet Routing Protocol. Addison-Wesley.
Narendra, K. S., & Thathachar, M. A. (1980). On the behavior of a learning automaton in a changing environment with application to telephone trac routing. IEEE
Transactions on Systems, Man, and Cybernetics, SMC-10 (5), 262{269.
Nedzelnitsky, O. V., & Narendra, K. S. (1987). Nonstationary models of learning automata
routing in data communication networks. IEEE Transactions on Systems, Man, and
Cybernetics, SMC-17, 1004{1015.
Papoulis, A. (1991). Probability, Random Variables and Stochastic Process (Third edition).
McGraw-Hill.
Peterson, L. L., & Davie, B. (1996). Computer Networks: A System Approach. Morgan
Kaufmann.
Rubistein, R. Y. (1981). Simulation and the Monte Carlo Method. John Wiley & Sons.
Sandick, H., & Crawley, E. (1997). QoS routing (qosr) working group report. Internet
Draft, Internet Engineering Task Force (IEFT).
Schoonderwoerd, R., Holland, O., & Bruten, J. (1997). Ant-like agents for load balancing
in telecommunications networks. In Proceedings of the First International Conference
on Autonomous Agents, pp. 209{216. ACM Press.
Schoonderwoerd, R., Holland, O., Bruten, J., & Rothkrantz, L. (1996). Ant-based load
balancing in telecommunications networks. Adaptive Behavior, 5 (2), 169{207.
Shankar, A. U., Alaettinoglu, C., Dussa-Zieger, K., & Matta, I. (1992a). Performance
comparison of routing protocols under dynamic and static file transfer connections.
ACM Computer Communication Review, 22 (5), 39{52.
Shankar, A. U., Alaettinoglu, C., Dussa-Zieger, K., & Matta, I. (1992b). Transient and
steady-state performance of routing protocols: Distance-vector versus link-state. Tech.
rep. UMIACS-TR-92-87, CS-TR-2940, Institute for Advanced Computer Studies and
Department of Computer Science, University of Maryland, College Park (MD).
Singh, S. P., & Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces.
Machine Learning, 22, 123{158.
Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Learning without state estimation
in partially observable Markovian decision processes. In Proceedings of the Eleventh
Machine Learning Conference, pp. 284{292. New Brunswick, NJ: Morgan Kaufmann.
Steenstrup, M. E. (Ed.). (1995). Routing in Communications Networks. Prentice-Hall.
Stone, P., & Veloso, M. M. (1996). Multiagent systems: A survey from a machine learning
persective. Tech. rep. CMU-CS-97-193, Carnegie Mellon University, Pittsburgh, PA.
364

fiAntNet: Distributed Stigmergetic Control for Communications Networks

Streltsov, S., & Vakili, P. (1996). Variance reduction algorithms for parallel replicated
simulation of uniformized Markov chains. Discrete Event Dynamic Systems: Theory
and Applications, 6, 159{180.
Subramanian, D., Druschel, P., & Chen, J. (1997). Ants and reinforcement learning: A
case study in routing in dynamic networks. In Proceedings of IJCAI-97, International
Joint Conference on Artificial Intelligence, pp. 832{838. Morgan Kaufmann.
The ATM Forum (1996). Private Network-Network Interface Specification: Version 1.0.
Walrand, J., & Varaiya, P. (1996). High-performance Communication Networks. Morgan
Kaufmann.
Wang, Z., & Crowcroft, J. (1992). Analysis of shortest-path routing algorithms in a dynamic
network environment. ACM Computer Communication Review, 22 (2).
Yang, Q. (1997). A Simulation Laboratory for Evaluation of Dynamic Trac Management Systems. Ph.D. thesis, Department of Civil and Environmental Engineering,
Massachusetts Institute of Technology (MIT).

365

fiJournal of Artificial Intelligence Research 9 (1998) 99{137

Submitted 10/97; published 9/98

Computational Aspects of Reordering Plans
Christer Backstrom

Department of Computer and Information Science
Linkopings universitet, S-581 83 Linkoping, Sweden

cba@ida.liu.se

Abstract

This article studies the problem of modifying the action ordering of a plan in order
to optimise the plan according to various criteria. One of these criteria is to make a plan
less constrained and the other is to minimize its parallel execution time. Three candidate
definitions are proposed for the first of these criteria, constituting a sequence of increasing
optimality guarantees. Two of these are based on deordering plans, which means that ordering relations may only be removed, not added, while the third one uses reordering, where
arbitrary modifications to the ordering are allowed. It is shown that only the weakest one
of the three criteria is tractable to achieve, the other two being NP-hard and even dicult
to approximate. Similarly, optimising the parallel execution time of a plan is studied both
for deordering and reordering of plans. In the general case, both of these computations are
NP-hard. However, it is shown that optimal deorderings can be computed in polynomial
time for a class of planning languages based on the notions of producers, consumers and
threats, which includes most of the commonly used planning languages. Computing optimal reorderings can potentially lead to even faster parallel executions, but this problem
remains NP-hard and dicult to approximate even under quite severe restrictions.

1. Introduction
In many applications where plans, made by man or by computer, are executed, it is important to find plans that are optimal with respect to some cost measure, typically execution
time. Examples of such applications are manufacturing and error-recovery for industrial
processes, production planning, logistics and robotics. Many different kinds of computations can be made to improve the cost of a plan|only a few of which have been extensively
studied in the literature. The most well-known and frequently used of these is scheduling. A plan tells which actions (or tasks) to do and in which order to do them, while a
schedule assigns exact release times to these actions. The schedule must obey the action
order prescribed by the plan and must often also satisfy further metric constraints such as
deadlines and earliest release times for certain actions. A schedule is feasible if it satisfies all
such metric constraints. It is usually interesting to find a schedule that is optimal in some
respect, eg the feasible schedule having the shortest total execution time, or the schedule
missing the deadlines for as few actions as possible.
In principle, planning and scheduling follow in sequence such that scheduling can be
viewed as a post-processing step to planning|where planning is concerned with causal
relations and qualitative temporal relations between actions, while scheduling is concerned
with metric constraints on actions. In some planning systems, eg O-Plan (Currie & Tate,
1991) and Sipe (Wilkins, 1988), both planning and scheduling are integrated into one
single system. Similarly, temporal planners, eg Deviser (Vere, 1983) and IxTeT (Ghallab
& Laruelle, 1994), can often reason also about metric constraints. This does not make it
c 1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBackstro m

irrelevant to study planning and scheduling as separate problems, though, as can be seen
from the vast literature on both topics. The two problems are of quite different character
and studying them separately gives important insight also into such integrated systems as
was just discussed. For instance, Drabble1 says that it is often very dicult to see when
O-Plan plans and when it schedules; it is easy to see that O-Plan works, but it is dicult
to see why.
A further complication in understanding the difference between planning and scheduling,
both for integrated systems and for systems with separated planning and scheduling, is
that certain types of computations fall into a grey zone between planning and scheduling.
Planners are good at reasoning about effects of actions and causal relationships between
actions, but are usually very poor at reasoning about time and temporal relationships
between actions. Schedulers, on the other hand, are primarily designed to reason about
time and resource conicts, but have no capabilities for reasoning about causal dependencies
between actions. The problems in the grey zone require reasoning of both kinds, so neither
planners nor schedulers can handle these problems properly. If these problems are not
solved, then the scheduler does not get sucient information from the planner to do the
best of the situation|the planner and the scheduler may fail in their cooperation to find a
plan with a feasible schedule, even when such a plan exists.
This article focusses on one of these grey-zone problems, namely the problem of optimising the action order of a plan to allow for better schedules. Whenever two actions conict
with each other and cannot be allowed to execute in parallel, a planner must order these
actions. However, it usually does not have enough information and reasoning capabilities
to decide which of the two possible orders is the best one, so it makes an arbitrary choice.
One of the choices typically allows for a better schedule than the other one, so if the planner
makes the wrong choice it may prevent the scheduler from finding a good, or even feasible,
schedule. This situation arises also when plans are made by a human expert, since it is difficult to see which choice of ordering is the best one in a large and complex plan. Planning
systems of today usually cannot do anything better than asking the planner for a new plan
if the scheduler fails to find a feasible schedule. This is an expensive and unsatisfactory
solution, especially if there is no feedback from the scheduler to help the planner making
a more intelligent choice next time. Another solution which appears in the literature is to
use a filter between the planner and scheduler which attempts to modify the plan order to
put the scheduler in a better position. Such filters could remove certain over-commitments
in the ordering, which will be referred to as deordering the plan, or even change the order
between certain actions, which will be referred to as reordering the plan.
This article is intended to provide a first formal foundation for studying this type of
problems. It defines a number of different optimality criteria for plan order modifications,
both with respect to the degree of over-committment in the ordering and with respect to
the parallel execution time, and it also provides computational results for computing such
modifications. The article also analyses some filtering algorithms suggested in the literature
for doing such order modifications.
The remainder of this article is structured as follows. Section 2 introduces the concepts
and computations studied in this article by means of an example. Then Section 3 starts the
1. Brian Drabble, personal communication, Aug. 1997.

100

fiComputational Aspects of Reordering Plans

theoretical content of the article, defining the two planning formalisms used in the following
sections. The problems of making a plan least-constrained are studied in Section 4 where
some candidate definitions for this concept are introduced and their computational properties investigated. Section 5 defines the concepts of parallel plans and parallel executions of
plans. This is followed by Section 6 where optimal deorderings and reorderings of parallel
plans are introduced and the complexity of achieving such optimality is analysed. Section 7
then studies how the complexity of these problems is affected by restricting the language.
This includes the positive result that an algorithm from the literature finds optimal deorderings for a class of plans for most common planning languages. Some other filtering
algorithms from the literature as well as some planners incorporating some ordering optimisation are discussed in Section 8. Finally, Section 9 discusses some aspects of this article
and some related work, while Section 10 concludes by a brief recapitulation of the results.

2. Example
In order to illustrate the concepts and operations studied in this article a simple example
of assembling a toy car will be used. The example is a variation of the example used by
Backstrom and Klein (1991), which is a much simplified version of an existing assembly line
for toy cars used for undergraduate laborations in digital control at Linkoping University
(for a description of this assembly line, see eg. Klein, Jonsson, & Backstrom, 1995, 1998;
Stromberg, 1991). The problem is to assemble a LEGO2 car from pre-assembled parts as
shown in Figure 1. There is a chassis, a top and a set of wheels, the two latter to be mounted
onto the chassis.
Top

Chassis

Car

Wheels

Figure 1: Schematic assembly process for a toy car
The workpiece ow of the factory is shown in Figure 2. There are three storages, one
for each type of preassembled part, two workstations, number 1 for mounting the top and
number 2 for mounting the wheels, and there is a car storage for assembled cars. Tops can
be moved from the top storage to workstation 1 and sets of wheels can be moved from the
2. LEGO is a trade mark of the LEGO company

101

fiBackstro m

wheels storage to workstation 2. Chassis can be moved from the chassis storage to either
workstation and also, possibly with other parts mounted, between the two workstations and
from either workstation to the car storage. Furthermore, before mounting the wheels on a
chassis, the tyres must be inated, so workstation 2 incorporates a compressed-air container
which must be pressurized before inating the tyres (this is not shown in the figure).

Top
Storage

Workstation 1

Chassis
Storage
Wheels
Storage

Car
Storage
Workstation 2

Figure 2: Schematic lay-out of the toy-car factory
This article is concerned with modifying the order between the actions in a given plan,
and does not consider modifying also the set of actions. Hence, the example will assume
that a plan for assembling a toy car is given|whether this plan was produced by hand or
by a planning algorithm is not important. It will also be assumed that this assembly plan
contains exactly those actions listed in Table 1, in some order. Since most results in this
article are independent of the particular planning language used, no assumptions about
the planning language will be made in this example either. To make things simple, the
obvious common-sense constraints on which plans are valid will be used. For instance, a
part must be moved to a workstation before it is mounted there, the wheels must be inated
before being mounted and the air container must be pressurized before inating the tyres.
Furthermore, since a chassis can only be at one single place at a time, the top cannot be
mounted in parallel with mounting the wheels, and neither of the mounting operations can
be done in parallel with moving either the chassis or the part to be mounted.
The purpose of modifying the action order in a given plan is usually to optimize the
plan in some aspect, for instance, to make the plan least constrained. Consider the totally
ordered plan in Figure 3a, for producing a chassis with wheels, which is a subplan of the
plan for assembling a car. Note that since the plan is totally ordered, all pairs of actions
are ordered, but the implicit transitive arcs are not shown in the figure. This plan is clearly
over-constrained. For instance, it is not necessary to move the set of wheels to workstation
2 before pressurizing the air container, and removing this ordering constraint results in
the plan in Figure 3b. Note that orderings have only been removed|the arc from MvW2
to IT existed already in the original plan, but was implicit by transitivity. A plan where
some orderings have been removed will be referred to as a deordering of the original plan.
102

fiComputational Aspects of Reordering Plans

Action
MvT1
MvW2
MvC1
MvC2
MvS
MtT
MtW
PAC
IT

Description
Move top to workstation 1
Move wheels to workstation 2
Move chassis to workstation 1
Move chassis to workstation 2
Move chassis to car storage
Mount top on chassis
Mount wheels on chassis
Pressurize air container
Inate tyres

Duration
1
1
2
2
3
7
4
5
4

Table 1: Actions of the assembly plan
This new plan is less constrained than the original plan, since it is now possible to move
the wheels and pressurize the air container in either order or, perhaps, even in parallel.
However, further orderings can be removed; it is not necessary to inate the wheels before
moving the chassis to the workstation. Removing also this ordering results in the plan in
Figure 3c, which is a least constrained deordering of the original plan in the sense that
it is not possible to remove any further ordering constraints and still have a valid plan.
That is, if removing any further ordering constraint, it will be possible to sequence the
actions in such a way that the plan will no longer have its intended result. In addition to
deorderings, one may also consider arbitrary modifications of the ordering relation, that is,
both removing and adding relations. Such modifications will be referred to as reorderings.
Three differents least-constrainment criteria for plans based on deorderings and reorderings
will be studied in Section 4, and the plan in Figure 3c happens to be optimal according to
all three of these criteria.

MvW2 - PAC

- IT

- MvC2 -MtW

a) A total order plan
MvW2PPPP
q

1 IT


PAC

- MvC2 -MtW

b) A less constrained version of a

MvW2PPPP
q1IT PPPPq

PAC
*MtW

MvC2
c) A least constrained
version of a

Figure 3: Three plans for mounting the wheels
103

fiBackstro m

Making a plan least constrained is clearly useful if certain actions can be executed in
parallel. However, even in the case where no parallel execution is possible, it may still be
worth making a plan least constrained. Although the partial order of this least constrained
plan must again be strengthened into a total order for execution purposes, this need not be
the same total order as in the original plan. Suppose the actions have temporal constaints
like deadlines and earliest release times and that a scheduler will post-process the plan to
try finding a feasible schedule. It may then be the case that the original plan has no feasible
schedule, but a less constrained version of it can be sequenced into a feasible schedule. The
idea of a least constrained plan is that the scheduler will have as many alternative execution
sequences as possible to choose from.
The most important reason for modifying the action ordering of a plan, however, is to
execute the plan faster by executing actions in parallel whenever possible. For this purpose
it is better to use the length of the optimal schedule for a plan as a measure, rather than
some measure on the ordering itself. Suppose the following car-assembly plan is given
hMvW 2; PAC; IT; MvC 2; MtW; MvT 1; MvC 1; MtT; MvS i:
If the actions are executed sequentially in the given order, the minimum execution time
is the sum of the durations of the actions, that is 29 time units. However, just as in the
previous example this plan is over-constrained, since several of the actions could be executed
in either order, or in parallel.
It is possible to remove orderings as far as shown in Figure 4a, but no further, and
still have a valid plan (the implicit transitive orderings are not shown in the figure). This
deordered version of the original assembly plan can be scheduled to execute in 25 time units
by exploiting parallelism whenever possible. An example of such a schedule is shown in
Figure 3b. However, no faster execution is possible, since the plan contains a subsequence
of actions which cannot be parallelized and which has a total execution time of 25 time
units.
It is obvious from the schedule in Figure 4b that not many actions can be executed in
parallel, and that the gain of deordering the plan is quite small. A much better performance
is possible if arbitrary modifications to the action ordering are allowed, that is, if also
reorderings are considered. For instance, in the assembly plan there is no particular reason
why the wheels should be mounted before the top is mounted, and it will be seen shortly
that much time can be saved by reversing the order of these two operations. A deordering
cannot do this, however, since removing the ordering between the wheel-mounting action
(MtW) and the top-mounting action (MtT) would make these unordered. This would be
interpreted as if the two actions could be executed in parallel, which is not possible. This
is also the reason why these actions must be ordered in the original plan. However, when
allowing arbitrary modifications, the order between these two actions can be reversed, and
Figure 5a shows such a reordering of the original plan. This plan can be scheduled to
execute in only 16 time units, which is a considerable improvement over both the original
plan and the optimal deordered version of it. An example of an optimal schedule is shown
in Figure 5b. In fact, this plan is an optimal reordering in the sense that no other ordering
of the actions results in a valid plan that can be scheduled to execute faster. The problems
of finding optimal deorderings and reorderings of plan with respect to parallel execution is
the main topic of this article, and are studied in Sections 5 to 7.
104

fiComputational Aspects of Reordering Plans

PAC
MvW2

MvT1

IT

MtW

MvC2

MtT

MvC1

MvS

a) A deordering of the assembly plan admitting a shortest
parallel execution time
PAC

IT

MvW2

MtW

MvC1

MtT

MvS

MvC2
MvT1
0

5

10

15

20

25

b) An optimal schedule for the plan above

Figure 4: An optimal deordering of the assembly plan
It is obvious that reordering is a more powerful operation than deordering, since the
reordered plan in Figure 5a allows for a shorter schedule than the optimal deordering in
Figure 4a. On the other hand, if the original plan had been

hMvT 1; MvC 1; MtT; MvS; MvW 2; PAC; IT; MvC 2; MtW i;
then deordering would have been sucient for arriving at the optimal plan in Figure 5a.

3. Planning Formalisms
This section defines actions, plans and related concepts, which basically appear in two
different guises in this article. Definitions and tractability results will mostly be cast in a
general, axiomatic framework in order to be as general and independent of formalism as
possible. Hardness results, on the other hand, will mostly be cast in a specific formalism,
Ground Tweak, and often subject to further restrictions, this in order to strengthen the
results. Both these formalisms are defined below. In addition to these, a third formalism
will be used, but its definition will be deferred until it is used, in Section 7.
105

fiBackstro m

PAC

IT

MvW2

MvT1

MtT

MvC1

MvC2

MvS

MtW

a) A reordering of the assembly plan admitting a shortest
parallel execution time
PAC
MvC1

IT

MvW2

MvT1 MtT
0

5

MtW

MvS

MvC2
10

15

b) An optimal schedule for the plan above

Figure 5: An optimal reordering of the assembly plan

3.1 The Axiomatic Planning Framework

The axiomatic framework makes only a minimum of assumptions about the underlying formalism. It may be instantiated to any planning formalism that defines some concept of
a planning problem a domain of entities called actions and a validity test. The planning
problem is assumed to consist of planning problem instances (ppis),3 with no further assumptions about the inner structure of these. The validity test is a truth-valued function
taking a ppi and a sequence of actions as arguments. If the validity test is true for a ppi 
and an action sequence ha1 ; : : : ; an i, then the action sequence ha1 ; : : : ; an i is said to solve
. While the inner structure of the ppis and the exact definition of the validity test are crucial for any specific planning formalism, many results in this article can be proven without
making any such further assumptions. Results on the computational complexity of certain
problems will make an assumption about the complexity of the validity test, though. Based
on these concepts, the notion of plans can be defined in the usual way.

Definition 3.1 A total-order plan (t.o. plan) is a sequence P = ha1 ; : : : ; ani of actions,
which can alternatively be denoted by the tuple hfa1 ; : : : ; an g; i where for 1  k; l  n,
ak  al iff k < l. Given a ppi , P is said to be -valid iff the validity test is true for 

and P .

3. This is the complexity-theoretic terminology for problems. Planning problem instances in the sense of
this article are sometimes referred to as planning problems in the planning literature.

106

fiComputational Aspects of Reordering Plans

A partial-order plan (p.o. plan) is a tuple P = hA; i where A is a set of actions and
 is a strict ( ie. irreexive) partial order on A. The validity test is extended to p.o. plans
s.t. given a ppi , P is -valid iff hA; 0 i is valid for every topological sorting 0 of .

The actions of a t.o. plan must be executed in the specified order, while unordered
actions in a p.o. plan may be executed in either order. That is, a p.o. plan can be viewed
as a compact representation for a set of t.o. plans. There is no implicit assumption that
unordered actions can be executed in parallel; parallel plans will be defined in Section 5.
p.o. plans will be viewed as directed acyclic graphs in figures with the transitive arcs often
tacitly omitted to enhance readability. Furthermore, all proofs and algorithms in this article
are based on this definition, ie assuming the order of a plan is transitively closed, while
many practical planners do not bother about transitive closures. This difference does not
affect any of the results presented here.

3.2 The Ground TWEAK Formalism

The Ground TWEAK (GT) formalism is the TWEAK language (Chapman, 1987) restricted
to ground actions. This formalism is a variation on propositional STRIPS and it is known
to be equivalent under polynomial transformation to most other common variants on propositional STRIPS (Backstrom, 1995). In brief, an action has a precondition and a postcondition, both being sets of ground literals.
In order to define the GT formalism, the following two definitions are required. Given
some set S , the notion Seqs (S ) denotes the set of all sequences formed by members of S ,
allowing repetition of elements and including the empty sequence. The symbol `;' will be
used to denote the sequence concatenation operator. Further, given a set P of propositional
atoms, the set LP of literals over P is defined as LP = P [ f:p j p 2 Pg. Since no other
formulae will be allowed than atoms and negated atoms, a double negation ::p will be
treated as identical to the unnegated atom p. Finally, given a set of literals L, the negation
Neg(L) of L is defined as Neg(L) = f:p j p 2 Lg[fp j :p 2 Lg and L is said to be consistent
iff there is no atom p s.t. both p 2 L and :p 2 L.

Definition 3.2 An instance of the GT planning problem is a quadruple  = hP ; O; I; Gi

where

 P is a finite set of atoms;
 O is a finite set of operators of the form hpre; posti where pre; post  LP are consistent
and denote the pre and post condition respectively;

 I; G  LP are consistent and denote the initial and goal state respectively.
For o = hpre; posti  O, we write pre(o) and post(o) to denote pre and post respectively. A
sequence ho1 ; : : : ; on i 2 Seqs (O) of operators is called a GT plan (or simply a plan) over .
Definition 3.3 The ternary relation valid  Seqs (O)  2L  2L is defined s.t. for arbitrary ho1 ; : : : ; on i 2 Seqs (O) and S; T  LP , valid(ho1 ; : : : ; on i; S; T ) holds iff either
1. n = 0 and T  S or
P

107

P

fiBackstro m

2. n > 0, pre(o1 )  S and
valid(ho2 ; : : : ; on i; (S , Neg(post(o1 )) [ post(o1 ); T ).
A t.o. plan ho1 ; : : : ; on i 2 Seqs (O) solves  iff valid(ho1 ; : : : ; on i; I; G).

An action is a unique instance of an operator, ie a set of actions may contain several
instances of the same operator, and it inherits its pre- and post-conditions from the operator
it instantiates. Since all problems in this article will consider some fixed set of actions, the
atom and operator sets will frequently be tacitly omitted from the GT ppis. In figures,
GT actions will be shown as boxes, with precondition literals to the left and postcondition
literals to the right.

4. Least Constrained Plans

It seems to have been generally assumed in the planning community that there is no difference between t.o. plans and p.o. plans in the sense that a t.o. plan can easily be converted
into a p.o. plan and vice versa. However, while a p.o. plan can be trivially converted into
a t.o. plan in low-order polynomial time by topological sorting, it is less obvious that also
the converse holds. At least three algorithms for converting t.o. plans into p.o. plans have
been presented in the literature (Pednault, 1986; Regnier & Fade, 1991a; Veloso, Perez, &
Carbonell, 1990) (all these algorithms will be analyzed later in this article). The claim that
a t.o. plan can easily be converted into a p.o. plan is vacuously true since any t.o. plan is
already a p.o. plan, by definition. Hence, no computation at all needs to be done. This
is hardly what the algorithms were intended to compute, however. In order to be useful,
such an algorithm must output a p.o. plan satisfying some interesting criterion, ideally some
optimality criterion. In fact, two of the algorithms mentioned above are claimed to produce
optimal plans according to certain criteria. For instance, Veloso et al. (1990, p. 207) claim
their algorithm to produce least constrained plans. They do not define what they mean
by this term, however, and theirs is hardly the only paper in the literature using this term
without further definition.
Unfortunately, it is by no means obvious what constitutes an intuitive or good criterion
for when a p.o. plan is least constrained and, to some extent, this also depends on the
purpose of achieving least-constrainment. The major motivation for producing p.o. plans
instead of t.o. plans (see for instance Tate, 1975) is that a p.o. plan can be post-processed
by a scheduler according to further criteria, such as release times and deadlines or resource
limits. Either the actions are ordered into an (ideally) optimal sequence or, given criteria for
parallel execution, into a parallel plan that can be executed faster than if the actions were
executed in sequence. In both cases, the less constrained the original plan is, the greater
is the chance of arriving at an optimal schedule or optimal parallel execution respectively.
Both of the algorithms mentioned above are motivated by the goal of exploiting possible
parallelism to decrease execution time.
It is not only interesting to make t.o. plans partially ordered, but also to make partially
ordered plans more partially ordered, that is, to generalise the ordering. An algorithm
for this task has been presented in the literature in the context of case-based planning
(Kambhampati & Kedar, 1994). Since t.o. plan are just a special case of p.o. plans, this
section will study the general problem of making partially ordered plans less constrained.
108

fiComputational Aspects of Reordering Plans

4.1 Least-constrainment Criteria

There is, naturally, an infinitude of possible definitions of least-constrainment. Some seem
more reasonable than others, however. Three intuitively reasonable candidates are defined
and analyzed below. Although other definitions are possible, it is questionable whether
considerably better or more natural definitions, with respect to the purposes mentioned
above, can be defined without using more information than is usually present in a t.o. or
p.o. plan.

Definition 4.1 Let P = hA; i and Q = hA; 0i be two p.o. plans and  a ppi. Then,
1. Q is a reordering of P wrt.  iff both P and Q are -valid.
2. Q is a deordering of P wrt.  iff Q is a reordering of P and 0 

3. Q is a proper deordering of P wrt.  iff Q is a reordering of P and 0 

Definition 4.2 Given a ppi  and two p.o. plans P = hA; i and Q = hA; 0i,
1. Q is a minimal-constrained deordering of P wrt.  iff
(a) Q is a deordering P wrt.  and
(b) there is no proper deordering of Q wrt. ;
2. Q is a minimum-constrained deordering of P wrt.  iff
(a) Q is a deordering P wrt.  and
(b) there is no deordering hA; 00 iof Q wrt.  s.t. j 00 j < j  j;
3. Q is a minimum-constrained reordering of P wrt.  iff
(a) Q is a reordering P wrt.  and
(b) there is no reordering hA; 00 iof Q wrt.  s.t. j 00 j < j  j;

Note that the previous publication (Backstrom, 1993) used the terms LC1-minimality for
minimal-constrained deordering and LC2-minimality for minimum-constrained reordering.
This change in terminology has been done with the hope that more will be gained in clarity
than is lost by confusion.
It is easy to see that minimum-constrainment is a stronger criterion than minimalconstrainment|any minimum-constrained deordering of a plan P is a minimal-constrained
deordering of P , but the opposite is not true. As an example, consider the plan in Figure 6a.
If removing all ordering constraints from action C, the result is the plan in Figure 6b, which
is still valid. This plan has an order of size 3 (there is one implicit transitive order) and it
is a minimal-constrained deordering since no further deordering can be made. It is not a
minimum-constrained deordering, however, since if instead breaking the ordering constraints
between the subsequences AB and CB, the result is the plan in Figure 6c, which is also valid.
This plan has an ordering of size 2 and it can easily be seen that it is a minimum-constrained
deordering, and that it happens to coincide with the minimum-constrained reordering in
this case. This coincidence is not always the case, however, since a reordering is allowed to
109

fiBackstro m

do more modifications than a deordering; a minimum deordering can obviously never have
a smaller ordering relation than a minimum reordering. Examples of this difference was
shown already in Section 2, where Figure 4a shows a minimum-constrained deordering and
Figure 4b shows a minimum-constrained reordering.
A

-p

p

B

-

q

C

q

-q

D

a) A total-order plan
C
A

p

-p

B

q

-q

q

C

D

A

b) A minimal deordering

q

-q

D

p

-p

B

q

b) A minimum deordering

Figure 6: The difference between minimal and minimum constrained deorderings.
Other alternative definitions of least-constrainment could be, for instance, to maximize
the unorderdness or to minimize the length of the longest chain in the modified plan. However, to find a de-/reordering which has as many pairs of unordered actions as possible is the
dual of computing a minimum de-/reordering and it is, thus, already covered. Minimizing
the length of the longest chain is a condition which may be relevant when actions can be
executed in parallel and the overall execution time is to be minimized. However, since the
number of ordering constraints is quadratic in the length of a chain (because of transitive
arcs), minimizing the size of the relation will often be a reasonable approximation of minimizing the chain length. Furthermore, minimizing the longest chain is still a rather weak
condition for this purpose, so it is better to study directly the problem of finding shortest
parallel executions of plans, which will be done later in this article.
Another issue is whether to minimize the size of the ordering relation as given, or to
reduce the transitive or reductive closure of it. Since plans may have superuous orderings
with no particular purpose, it is reasonable to standardize matters and either add all possible
transitive arcs, getting the transitive closure, or to remove all transitive arcs, getting the
reductive closure. The choice between these two is not important for the results to be
proven. However, minimizing the transitive closure will give a preference to plans with
many unordered short chains of actions over plans with a few long chains, and so seems to
coincide better with the term 'least constrained'.

4.2 Computing Least-constrained Plans
Minimal deordering is weaker than the two other least-constrainment criteria considered,
but it is the least costly to achieve|it is the only one of the three criteria which can be
satisfied by a polynomial-time modification to a plan.
110

fiComputational Aspects of Reordering Plans

Definition 4.3 The search problem

Minimal-Constrained Deordering (MlCD) is

defined as follows:
Given: A ppi  and a -valid plan P .
Output: A minimal-constrained deordering of P wrt. .

Theorem 4.4

MlCD can be solved in polynomial time if validity for p.o. plans can be

tested in polynomial time.

Proof: Consider algorithm MLD in Figure 7 and let Q = hA; 0 i be the plan output by
the algorithm on input P = hA; i. The plan Q is obviously a valid deordering of P wrt.

. It is further obvious from the termination condition in the while loop that there is no
other ordering 00 0 s.t. hA; 00 i is -valid. It follows that Q is a minimal-constrained
deordering. Since the algorithm obviously runs in polynomial time, the theorem follows.

2

Furthermore, if validity testing is expensive, this will be the dominating cost in the MLD
algorithm.

Corollary 4.5 If validity testing for p.o. plans can be solved in time O(f (n)) for some
function f (n), then MlCD can be solved in O(maxfn7=2 ; n2 f (n)g) time.
1 procedure MLD
2
Input: A valid p.o. plan P = hA; i and a ppi 
3
Output: A minimal deordering of P
4 while there is some e 2 s.t. hA; ( ,feg)+i is -valid do
5
remove e from 
6 return hA; + i;

Figure 7: The minimal-deordering algorithm MLD
In particular, note that plan validation is polynomial for the usual variant of propositional STRIPS without conditional actions (Nebel & Backstrom, 1994, Theorem 5.9).
More precisely, this proof pertains to the Common Propositional STRIPS formalism (CPS)
and, thus, holds also for the other common variants of propositional STRIPS, like Ground
TWEAK (Backstrom, 1995). Furthermore, note that in practice it may not be necessary
to compute the transitive closure either for the output plan or for validating a plan in the
algorithm.
While minimum de-/reordering are stronger criteria than minimal deordering, they are
also more costly to achieve.

Definition 4.6 The decision problem

Minimum-Constrained Deordering (MmCD)

is defined as follows:
Given: A ppi , a -valid plan P and an integer k  0.
Question: Is there a deordering hA; i of P s.t. j  j  k?
111

fiBackstro m

Definition 4.7 The decision problem

Minimum-Constrained Reordering (MmCR)

is defined as follows:
Given: A ppi , a -valid plan P and an integer k  0.
Question: Is there a reordering hA; i of P s.t. j  j  k?

Theorem 4.8

Minimum-Constrained Deordering is NP-hard.

Proof: Proof by reduction from Minimum Cover (Garey & Johnson, 1979, p. 222),
which is NP-complete. Let S = fp1 ; : : : ; pn g be a set of atoms, C = fC1 ; : : : ; Cm g a set of
subsets of S and k  jC j a positive integer. A cover of size k for S is a subset C 0  C s.t.
jC 0j  k and S  [T 2C T . Construct, in polynomial time, the GT ppi  = h;; frgi and the
-valid t.o. plan P = ha1 ; : : : ; am ; aS i where pre(ai ) = ; and post(ai ) = Ci for 1  i  m,
and further pre(aS ) = S and post(aS ) = frg. Obviously, S has a minimum cover of size k
iff there exists some -valid p.o. plan Q = hfa1 ; : : : ; am ; aS g; i s.t. j  j  k, since only
0

those actions contributing to the cover need remain ordered wrt. to aS

Corollary 4.9

2

Minimum-Constrained Reordering is NP-hard.

Corollary 4.10

Minimum-Constrained Deordering and Minimum-Constrained

Reordering both remain NP-hard even when restricted to GT plans where the actions

have only positive pre- and post-conditions.

Theorem 4.11 If validity for p.o. plans is in some complexity class C, then

MinimumConstrained Deordering and Minimum-Constrained Reordering are in NP C.

Proof: Guess a solution, verify that it is a de-/reordering and then validate it using an

2

oracle for C.

For most common planning formalisms without conditional actions and context-dependent
effects, minimal de-/reordering is NP-complete.

Theorem 4.12 If validity for p.o. plans can be tested in polynomial time, then Minimum-

Constrained Deordering and Minimum-Constrained Reordering are NP-complete.

Proof: Immediate from Theorems 4.8 and 4.11 and from Corollary 4.9.

2

It follows immediately that the corresponding search problems, that is, the problems of
generating a minimum-constrained de-/reordering are also NP-hard (and even NP-equivalent
if validity testing is tractable).
Furthermore, MmCD and MmCR are not only hard to solve optimally, but even to
approximate. Neither of these problems is in the approximation class APX (Crescenzi
& Panconesi, 1991), ie neither problem can be approximated within a constant factor.
(Both here and elsewhere in this article the term approximation is used in the constructive
sense, that is the results refer to the existence/non-existence of algorithms producing an
approximate solution in polynomial time).
112

fiComputational Aspects of Reordering Plans

Theorem 4.13

Minimum-Constrained Deordering and Minimum- Constrained
Reordering cannot be approximated within a constant unless NP 2 DTIME (npoly log n ).

Proof: Suppose there were a polynomial-time algorithm A approximating MmCD within
a constant. Since the reduction in the proof of Theorem 4.8 preserves the solutions exactly,
also approximations are preserved. Hence, Minimum Cover could be approximated within
a constant, but this is impossible unless NP 2 DTIME (npoly log n ) (Lund & Yannakakis,
1994), which contradicts the assumption. The case for MmCR is a trivial consequence. 2
If using the number of propositional atoms in the plan as a measure of its size, this
bound can be strengthened to (1 , ") ln jPj for arbitrary " unless NP 2 DTIME (nlog log n )
by substituting such a result for Minimum Cover (Feige, 1996) in the proof above.

5. Parallel Plans

In order to study the problem of finding a shortest parallel execution of a plan, the formalisms used so far are not quite sucient. Since they lack a capability of modelling when
actions can be executed in parallel or not, it is impossible to say with any reasonable precision how a certain action ordering will affect the parallel execution time. Partial-order
plans are sometimes referred to as parallel plans in literature. This is misleading, however.
That two actions are left unordered in such a plan means that they can be executed in
either order, without affecting the validity of the plan, but in the general case there is
no guarantee that the plan will remain valid also if the executions of the actions overlap
temporally. In some cases, unorderedness means that parallel or overlapping execution is
allowed, while in other cases it does not mean that, depending on the action modelling and
its underlying domain assumptions. In the first case, the plan must have a stronger ordering
committment, any two actions that must not have overlapping executions must be ordered,
thus making the plan over-committed.
In order to distinguish the two cases, a concept of parallel plans will be introduced below.
A parallel plan is a partial-order plan with an extra relation, a non-concurrency relation,
which tells which actions must not be executed in parallel. In this article two actions are
considered parallel if their executions have any temporal overlap at all. Plans where all
unordered actions can be executed in parallel constitute the special case of definite parallel
plans.

Definition 5.1 A parallel plan is a triple P = hA; ; #i, where hA; i is a p.o. plan and
# is an irreexive, symmetric relation on A. A definite parallel p.o plan is a parallel plan
P = hA; ; #i s.t. #  ( [ ,1 ).
Intuitively, a parallel plan is a p.o. plan extended with an extra relation, # (a nonconcurrency relation), expressing which of the actions must not be executed in parallel.
This relation is primarily intended to convey information about actions that are unordered
under the  relation, although it is allowed to relate also such actions. That is, the #
relation is intended to capture information about whether two actions can be executed in
parallel or not, in general. That two actions are ordered in a plan forbids executing them
in parallel in this particular plan, but does not necessarily mean that the actions could not
113

fiBackstro m

be executed in parallel under different circumstances. Planning algorithms frequently produce overcommitted orderings on plans, and the whole purpose of this article is to study the
problem of optimizing plans by finding and removing such overcommitted orderings. Hence,
there are no restrictions in general on the relation # in addition to those in Definition 5.1.
For instance, a  b does not imply that a#b. However, the non-concurrency relation will
frequently be constrained to satisfy the post-exclusion principle.
Definition 5.2 A parallel GT plan P = hA; ; #i satisfies the post-exclusion principle
iff for all actions a; b 2 A, a#b whenever there is some atom p s.t. p 2 post(a) and
:p 2 post(b).
The definition of plan validity is directly inherited from p.o. plans.
Definition 5.3 Given a ppi , a parallel plan hA; ; #i is -valid iff the p.o. plan hA; i
is -valid.
The non-concurrency relation is, thus, not relevant for deciding whether a plan is valid or
not. Instead, it is used for constraining how parallel plans may be executed and it is the
core concept behind the definition of parallel executions.
Consider, for instance, the GT plan hfA; B; C g; fhA; B ig; fhB; C igi which is shown in
Figure 8 (arrows denote ordering relations and dashed lines denote nonconcurrency relations). This plan is valid wrt. the ppi  = h;; fr; sgi, that is the final value of the atom q
does not matter. Since B #C holds the actions B and C are constrained not to be executed
in parallel, but may be executed in either order, that is, the plan is not definite. This could
be because the post-exclusion principle is employed, or for some other reason. Although
A#B does not hold the actions A and B clearly cannot be executed in parallel, since A  B
holds. There are four ways to execute this plan, in either of the three sequences A,B,C;
A,C,B and C,A,B, or by executing A and C in parallel, followed by B (unit length is assumed). Also note that this plan would no longer be valid if the goal contained either q or
:q, since the final truth value of q depends on the actual execution order. Furthermore,
any reordering of the plan would have to keep the ordering constraint A  B to satisfy the
validity criterion, why it is not necessary to have the constraint A#B . It would do no harm
here to include this restriction, but in more complex plans it may be an over-constrainment,
if there are several producers for the atom p to choose between, for instance. To sum up,
the non-concurrency relation should primarily be used to mark which actions must not be
in parallel in addition to those already forbidden to be in parallel because of validity.
This framework for parallel plans admits expressing possible parallelism only; necessary
parallelism is out of the scope of this article and requires a planner having access to and
being able to make use of further additional information, perhaps a temporal algebra.
Furthermore, a set of non-concurrent actions can easily be expressed by making all actions
in the set pairwise non-concurrent, but the formalism is not sucient to say that k of the
actions, but not more, in such a set may be executed in parallel. Similarly, it is not possible
to express that an action must executed before or after an interval, or that two sets of
actions must have non-overlapping executions.
Definition 5.4 Let P = hA; ; #i be a parallel plan and let the function d : A 7! N denote
the duration of each action. A parallel execution of P is a function r : A 7! N , denoting
release times for the actions in A, satisfying that for all a; b 2 A,
114

fiComputational Aspects of Reordering Plans

A

p

q
r

B
#

C :s q
Figure 8: A parallel plan
1. if a  b, then r(a) + d(a)  r(b) and
2. if a#b, then either
(a) r(a) + d(a)  r(b) or
(b) r(b) + d(b)  r(a).
The length of the parallel execution is defined as maxa2A fr(a) + d(a)g, ie, the latest finishing time of any action. A minimum parallel execution of plan is a parallel execution with
minimum length among all parallel executions of the plan. The length of a parallel plan P ,
denoted length(P ), is the length of the minimum parallel execution(s) for P .

P

Obviously, every parallel plan has a parallel execution of length a2A d(a) (which is the
trivial case of sequential execution). Furthermore, in certain cases, hardness results will be
strengthened by restricting the duration function.
Definition 5.5 The special case where d(a) = 1 for all a 2 A is referred to as the unit
time assumption.
Deciding whether a release-time function is a parallel execution is tractable.
Theorem 5.6 Given a parallel plan P = hA; ; #i, a duration function d : A 7! N and a
release-time function r : A 7! N , it can be decided in polynomial time whether r is a parallel
execution for P and, in the case it is, what the length of this execution is.
Proof: Trivial.
2
Consider the plan in Figure 8 and three release-time functions r1 , r2 and r3 , defined as
follows
r1(A) = 1 r1 (B ) = 2 r1 (C ) = 3
r2(A) = 1 r2 (B ) = 2 r2 (C ) = 1
r3(A) = 1 r3 (B ) = 2 r3 (C ) = 2:
Both r1 and r2 are parallel executions of the plan, while r3 is not. Furthermore, r2 is
a minimum parallel execution for the plan, having length 2. However, computing the
minimum parallel execution of a parallel plan is dicult in the general case.
115

fiBackstro m

Definition 5.7 The decision problem Parallel Plan Length (PPL) is defined as follows:
Given: A parallel plan P = hA; ; #i, a duration function d and an integer k.
Question: Does P have a parallel execution of length k or shorter?

Theorem 5.8

Parallel Plan Length is NP-hard.

Proof: Hardness is proven by transformation from Graph K-Colourability (Garey
& Johnson, 1979, p. 191), which is NP-complete. Let G = hV; E i be an arbitrary undirected graph, where V = fv1 ; : : : ; vn g. Construct, in polynomial time, a GT ppi as follows. Define the ppi  = h;; fp1 ; : : : ; pn gi. Also define the parallel plan P = hA; ;; #i,
where A contains one action ai for each vertex vi 2 V , s.t. pre(ai ) = ; and post(ai ) =
fpi; qi g [ f:qj j fvi; vj g 2 E g. Finally, let ai#aj iff fvi ; vj g 2 E , which satisfies the post-

exclusion principle. The plan P just constructed is obviously -valid. It is easy to see that
G is k-colourable iff P has a parallel execution of length k wrt.  since each colour of G
will correspond to a unique release time in the parallel execution of P .
2

Corollary 5.9

Parallel Plan Length remains NP-hard even when restricted to GT ac-

Theorem 5.10

Parallel Plan Length is in NP.

tions with empty preconditions and under the assumption of unit time and the post-exclusion
principle.

Proof: Guess a parallel execution. Then verify it, which can be done in polynomial time
2

according to Theorem 5.6.

Computing a minimum parallel execution of a plan is tractable for the special case of definite
plans, however.

Theorem 5.11
parallel plans.

Parallel Plan Length can be solved in polynomial time for definite

Proof: Use the algorithm DPPL (Figure 9), which is a straightforward stratification
2

algorithm for directed DAGs.

6. Reordering Parallel Plans
Having defined the concept of parallel plan, it is possible to define concepts similar to
the previous least-constrainment criteria which are more appropriate for minimizing the
execution time of parallel plans.

Definition 6.1 Let P = hA; ; #i and Q = hA; 0 ; #i be two parallel plans and  a ppi.
Then,

1. Q is a parallel reordering of P wrt.  iff both P and Q are -valid;
116

fiComputational Aspects of Reordering Plans

1
2
3
4
5
6
7
8
9
10
11
12

procedure DPPL

Input: A definite parallel plan P = hA; ; #i
Output: A minimum parallel execution r for P
Construct the directed graph G = hA; i
for all a 2 A do
r(a) 0
while A 6= ; do
Select some node a 2 A without predecessors in A
for all b 2 A s.t. a  b do
r(b) max(r(b); r(a) + d(a))

A

return r

A , fag

Figure 9: Algorithm for computing a minimum parallel execution for definite parallel plans.
2. Q is a parallel deordering of P wrt.  iff Q is a parallel reordering of P and 0 ;
3. Q is a minimum parallel reordering of P wrt.  iff
(a) Q is a parallel reordering of P wrt.  and
(b) no other parallel reordering of P wrt.  is of shorter length than Q;
4. Q is a minimum parallel deordering of P wrt.  iff
(a) Q is a parallel deordering of P wrt.  and
(b) no other parallel deordering of P wrt.  is of shorter length than Q.

Modifying plans to satisfy either of the latter two criteria is dicult in the general case,
however.

Definition 6.2 The decision problem Minimum Parallel Deordering (MmPD) is defined as follows.
Given: a ppi , a parallel plan P , a duration function d and an integer k.
Question: Does P have a deordering with a parallel execution of length k wrt. ?

Definition 6.3 The decision problem Minimum Parallel Reordering (MmPR) is defined as follows.
Given: a ppi , a parallel plan P , a duration function d and an integer k.
Question: Does P have a reordering with a parallel execution of length k wrt. ?

Theorem 6.4 Minimum Parallel Deordering is NP-hard.
Proof: Similar to the proof of Theorem 6.4. Given a graph G and an integer k, construct
a ppi  and a plan P = hA; ; #i in the same way as in the proof of Theorem 5.8, but
let  be an arbitrary total order on A. Obviously, P is -valid and Q = hA; ;; #i is a

deordering of P s.t. no other deordering of P is shorter than Q. Hence, Q, and thus P , has
a deordering with a parallel execution of length k iff G is k-colourable.
2
117

fiBackstro m

Corollary 6.5
Corollary 6.6

Minimum Parallel Reordering is NP-hard.

Minimum Parallel Deordering and Minimum Parallel Reordering remain NP-hard even when restricted to totally ordered GT plans and under the as-

sumptions of unit time and simple concurrency.

Note that the restriction to definite input plans is covered by this corollary. If output
plans are also required to be definite, then the reordering case remains NP-hard.

Theorem 6.7

Minimum Parallel Reordering remains NP-hard also when the output

plan is restricted to be definite.

Proof: Reuse the proof for Theorem 6.4 as follows. Let r be a shortest parallel execution
for the plan Q and assume this execution is of length n. Construct an order 0 on A s.t.
for all actions a; b 2 A, a 0 b iff r(a) < r(b). Obviously the plan hA; 0 ; #i is a definite

minimum parallel reordering of P . It follows that P has a definite parallel reordering of
length k iff G is k-colourable.
2

It is an open question whether minimum deordering remains NP-hard when also output
plans must be definite, but an important special case is polynomial, as will be proven in
the next section.

Theorem 6.8

Minimum Parallel Deordering and Minimum Parallel Reorder-

ing are in NP C if validation of p.o. plans is in some complexity class

C.

Proof:

Given a plan hA; ; #i, a duration function d and a parameter k, guess a
de/reordering 0 and a release-time function r. Then verify, using an oracle for C , that
hA; 0 ; #i is valid. Finally, verify that r is a parallel execution of length  k, which is
polynomial according to Theorem 5.6.
2

Theorem 6.9 Minimum parallel de-/reordering is NP-complete if p.o. plans can be vali-

dated in polynomial time.

Proof: Immediate from Theorems 6.4 and 6.8 and Corollary 6.5.

2

The problems MmPD and MmPR are not only hard to solve optimally, but also to
approximate.

Theorem 6.10

Minimum Parallel Deordering and Minimum Parallel Reordering cannot be approximated within jAj1=7," for any " > 0, unless P=NP.

Proof:
Suppose there were a polynomial-time algorithm A approximating MmCD within
jAj1=7," for some " > 0. Then it is immediate from the proof of Theorem 6.4 that also
Graph K-Colourability could be approximated within jAj1=7," , which is impossible
unless P=NP (Bellare, Goldreich, & Sudan, 1995).

2

With the same reasoning, this bound can be strengthened to jAj1," , under the assumption
that co-RP6=NP (Feige & Kilian, 1996).
118

fiComputational Aspects of Reordering Plans

7. Restricted Cases
Since the problems of computing minimum de-/reorderings are very dicult, and are even
dicult to approximate, an alternative way of tackling them could be to study restricted
cases. One special case already considered is the restriction to definite plans only. While the
problem MmPR is still NP-complete under this restriction, it is an open question whether
also MmPD is NP-complete. A positive result can be proven, though, to the effect that
MmPD is polynomial for definite plans for a large class of planning languages, including
most of the commonly used ones. This result will be proven by generalising an algorithm
from the literature for deordering total-order plans.
Based on the (not necessarily true) argument that it is easier to generate a t.o. plan than
a p.o. plan when using complex action representations, Regnier and Fade (1991a, 1991b)
have presented an algorithm for converting a t.o. plan into a p.o. plan. The resulting plan
has the property that all its unordered actions can be executed in parallel, that is, the plan
is definite. The authors of the algorithm further claim that the algorithm finds all pairs
of actions that can be executed in parallel and, hence, the plan can be post-processed to
find an optimal parallel execution. They do not define what they mean by this criterion,
however.
Incidentally, the algorithm proposed by Regnier and Fade is a special case of an algorithm earlier proposed for the same problem by Pednault (1986), who did not make any
claims about optimality. If removing from Regnier and Fade's algorithm all details relevant
only for their particular implementation and planning language, the two algorithms coincide
and they are thus presented here as one single algorithm, the PRF algorithm4 (Figure 10).
PRF is slightly modified from the original algorithms. First, it does not assume that the input plan is totally ordered, since it turns out to be sucient that it is a definite partial-order
plan. Second, PRF returns a parallel plan, rather than a p.o. plan|a harmless modification since the only additional piece of information is the non-concurrency relation, which
is already given as input, either explicitly or implicitly. Third, PRF returns the transitive
closure of its ordering relation. This is by no means necessary, and is motivated, as usual,
by conforming to the definitions of this article.
1 procedure PRF;
2
Input: A ppi , a -valid definite p.o. plan hA; i and a non-concurrency
relation #
3
Output: A -valid parallel plan
4 for all a; b 2 A s.t. a  b do
5
if a#b then
6
Order a 0 b;
7 return hA; 0+ ; #i;

Figure 10: The PRF algorithm
Obviously, PRF computes a deordering of its input, and it is unclear whether it is possible to compute a minimal definite deordering in polynomial time. However, the algorithm
4. Here and afterwards, the algorithms from the literature will be referred to by acronyms consisting of the
initials of its authors, in this case Pednault, Regnier and Fade.

119

fiBackstro m

has been abstracted here to a very general formalism, and an analysis for restricted formalisms reveals more about its performance. The language used by Regnier and Fade is
unnecessarily restricted so the algorithm will be shown to work for a considerably more
general formalism, based on generalising and abstracting the concepts of producers, consumers and threats used in most common planners and planning languages, eg STRIPS and
TWEAK. This formalism will be referred to as the Producer-Consumer-Threat formalism
(PCT).
Let prod(a; ) denote that a produces the condition , cons(a; ) that a consumes  and
threat(a; ) that a is a threat to . To simplify the definitions, the standard transformation
will be used of simulating the initial and goal states with actions. That is, every PCT plan
contains an action ordered before all other actions which consumes nothing and produces
the initial state. Similarly, there is an action ordered after all other actions which consumes
the goal state and produces nothing. This means that the ppi is contained within the plan
itself, so all references to ppis can be omitted in the following. Validity of plans can then
be defined as follows.

Definition 7.1 A t.o. PCT plan ha1 ; : : : ; an i is valid iff for all i, 1  i  n and all
conditions  s.t. cons(ai ; ), there is some j , 1  j < i s.t. prod(aj ; ) and there is no k,
j  k  i s.t. threat(ak ; ). A p.o. PCT plan is valid iff all topological sortings of it are

valid.

Chapman's Modal-truth Criterion (MTC) (Chapman, 1987) can be abstracted to the
PCT formalism and be analogously used for validating p.o. plans.

Definition 7.2 The modal truth criterion (MTC) for a PCT plan hA; i is:
8aC 8(cons(aC ; ) !
9aP (prod(aP ; ) ^ aP  aC ^
8aT (threat(aT ; ) !
aC  aT _
9aW (prod(aW ; ) ^ aT  aW ^ aW  aC ))))
Theorem 7.3 The MTC holds for a PCT plan P iff it is valid.
Proof: Trivial generalization of the proofs leading to Theorem 5.9 in Nebel and Backstrom

2

(1994).

Only a minimum of constraints for when two actions may not be executed in parallel
will be required. These constraints are obeyed by most planners in the AI literature.

Definition 7.4 Simple concurrency holds if for all actions a, b s.t. a 6= b, the nonconcurrency relation satisfies the following three conditions
1. prod(a; ) ^ cons(b; ) ! a#b
2. prod(a; ) ^ threat(b; ) ! a#b
3. cons(a; ) ^ threat(b; ) ! a#b
120

fiComputational Aspects of Reordering Plans

Note that it is not required that two producers, two consumers or two threats of the same
condition are non-concurrent, thus allowing, for instance, plans with multiple producers, eg
Nebel and Backstrom (1994, Fig. 4) and Kambhampati (1994). The axioms do not prevent
adding such restrictions, though. Furthermore, note that the definition only states a necessary condition for non-concurrency|it is perfectly legal to add further non-concurrency
constraints on the actions in a plan. It may also be worth noting that the MTC requires
producers and threats to be ordered only if there is a correpsonding consumer, while a
definite plan satisfying the simple concurrency criterion always require them to be ordered.
The following observation about PRF is immediate from the algorithm and will be used
in the proofs below.

Observation 7.5 If hA; ; #i is the input to PRF and hA; 0 ; #i is the corresponding
output, then it holds that a 0 b iff a  b and a#b.
Based on this lemma, it can be proven that PRF preserves validity.

Lemma 7.6 If the plan input to PRF is a valid PCT plan and # satisfies the simple
concurrency criterion, then the output plan is valid.

Proof: Let P = hA; ; #i be the input plan and Q = hA; 0 ; #i the output plan. Since

P is valid, it follows from Theorem 7.3 that the MTC holds for P . Adding the implied
simple-concurrency constraints to the MTC yields the following condition:

8aC 8(cons(aC ; ) !
9aP (prod(aP ; ) ^ aP  aC ^ aP #aC ^
8aT (threat(aT ; ) !
(aC  aT ^ aC #aT )_
9aW (prod(aW ; )^aT  aW ^ aT #aW ^
aW  aC ^ aW #aC )))).
By applying Observation 7.5 this can be simplified to:

8aC 8(cons(aC ; ) !
9aP (prod(aP ; ) ^ aP 0 aC ^
8aT (threat(aT ; ) !
aC 0 aT _
9aW (prod(aW ; ) ^ aT 0 aW ^ aW 0 aC )))),
which is the MTC for the plan Q. Once again using Theorem 7.3, it follows that Q is valid.

2

This allows for proving that PRF produces definite minimum deorderings of definite PCT
plans under simple concurrency.

Theorem 7.7 If using the PCT formalism and simple concurrency, then PRF produces a
minimum-deordered definite version of its input.
121

fiBackstro m

Proof: Let P = hA; ; #i be the input plan, which is assumed valid and definite, and
Q = hA; 0 ; #i the output plan. It is obvious that 0  and it follows from Lemma 7.6 that

Q is valid, so Q is a deordering of P . It remains to prove that Q is a minimum deordering
of P .
Suppose that P has a deordering R = hA; 00 ; #i s.t. j 00 j < j 0 j. Then, there must
be some a; b 2 A s.t. a 0 b, but not a 00 b. It can be assumed that a 0 b is not

a transitive arc in 0 , since the transitive closure is anyway computed at the end of the
algorithm. Since the order 0 is produced by PRF, it follows from Observation 7.5 that
a  b and a#b. Because of the latter constraint, it is necessary that either, a 00 b or
b 00 a holds, but only the former is possible since a  b and R is a deordering of P . This
contradicts the assumption, so Q must be a minimum deordering of P .
2
Since PRF is a polynomial algorithm, it follows that definite minimum deorderings of
definite PCT plans can be computed in polynomial time under simple concurrency. Furthermore, since PRF produces definite plans it is possible to actually compute the shortest
parallel execution eciently.

Theorem 7.8 If the plan input to PRF is a valid and definite PCT plan satisfying the
simple concurrency criterion, then PRF outputs a definite minimum deordering of this plan.

Proof: PRF runs in polynomial time and obviously produces definite parallel plans.
Hence, it follows from Theorem 5.11 that a minimum parallel execution for the output plan
can be found in polynomial time, which proves the theorem.
2

It seems likely that this is what Regnier and Fade meant with their optimality claim, although for a special instance of the PCT formalism. This result says nothing about the
diculty of finding a minimum reordering of a plan, since PRF only considers deorderings.
Since minimum deorderings do not approximate minimum reorderings well, it can be suspected that it is more dicult to compute the latter. The following theorem confirms this
suspicion, showing that the latter problem remains NP-hard under quite severe restrictions,
including the following two.

Definition 7.9 A GT action a is toggling iff for all literals l 2 post(a), it is also the case
that :l 2 pre(a). A GT action a is unary iff jpost(a)j = 1.
Theorem 7.10 Minimum Parallel Reordering remains NP-hard even when restricted

to total-order GT plans with only toggling unary actions and under the assumption of unit
time, simple concurrency and that no actions are redundant.

The proof of this theorem appears in Appendix A.
While minimum reorderings are more dicult to compute than minimum deorderings,
they can also produce arbitrarily better results.

Theorem 7.11

Minimum Parallel Deordering cannot approximate Minimum Parallel Reordering within jAjk for any constant k  0.

The proof of this theorem appears in Appendix A.
122

fiComputational Aspects of Reordering Plans

Corollary 7.12 Minimum Parallel Deordering cannot approximate Minimum Parallel Reordering within jAjk for any constant k  0 even when the problems are restricted to GT plans with only positive preconditions and under the assumption of simple
concurrency.

It may, thus, appear as though minimum reordering is a preferable, albeit more costly,
operation than minimum deordering. However, if the plan modification is to be followed
by scheduling, it is no longer obvious that a reordering is to prefer. Since scheduling may
take further information and constraints into account, eg upper and lower bounds on the
release time and limited resources, a feasible schedule for the original plan may no longer be
a feasible schedule for a reordering of the same plan. That is, some or all feasible solutions
may be lost when reordering a plan. In contrast to this, deordering a plan is harmless
since all previously feasible schedules are preserved in the deordering. Of course, the de/reordered plan may have new and better schedules than the old plan, which is why the
problems studied in this article are interesting at all. However, while minimum deordering
is a safe and, usually cheap, operation, minimum reordering is neither and must thus be
applied with more care. To find a reordering of a plan with an optimum schedule would
require combining minimum reordering and scheduling into one single computation, but it
is out of the scope of this article to study such combinations. Suce it to observe that such
a computation is never cheaper than either of its constituent computations.

8. Related work
This section analyses and discusses some algorithms suggested in the literature for generalising the ordering of a plan, in addition to the PRF algorithm already analysed in the
preceeding section. Also some planners that generate plans with some optimality avour
on the ordering are discussed.
Some of the algorithms to be analysed use the common trick of simulating the initial
state and the goal of a planning instance by two extra operators, in the following way. Let
P = hA; i be a plan and  = hI; Gi a ppi, both in the GT language. Introduce two extra
actions aI , with pre(aI ) = ; and post(aI ) = I , and aG , with pre(aG ) = G and post(aG ) = ;.
Define the plan Q = hA [ faI ; aG g; 0 i where 0 = [faI  a; a  aG j a 2 Ag[faI  aG g,
that is aI is ordered before all other actions and aG is ordered after all other actions. The
plan Q is a representation of both the plan P and the ppi . Such a combined representation
will be referred to as a self-contained plan. A self-contained plan is valid iff it is valid wrt.
to the ppi h;; ;i. It is trivial to convert a plan and a ppi into a corresponding self-contained
plan and vice versa. Hence, both ways of representing a plan will be used alternately
without further notice.

8.1 The VPC Algorithm

Veloso et al. (1990) have presented an algorithm (here referred to as VPC5 ) for converting
t.o. plans into `least-constrained' p.o. plans. They use the algorithm in the following context.
First a total-order planner (NoLimit) is used to produce a t.o. plan. VPC converts this plan
5. In the original publication the algorithm was named Build Partial Order.

123

fiBackstro m

1 procedure VPC;
2
Input: a valid self-contained t.o. plan ha1 ; : : : ; an i
where a1 = aI and an = aG
3
Output: A self-contained valid p.o. plan
4 for 1  i  n do
5
for p 2 pre(ai ) do
6
Find max k < i s.t. p 2 post(ak );
7
if such a k exists then
8
Order ak  ai
9
for :p 2 post(ai ) do
10
for 1  k < i s.t. p 2 pre(ak ) do
11
Order ak  ai
12
for each primary effect p 2 post(ai ) do
13
for 1  k  i s.t. :p 2 post(ak ) do
14
Order ai  ak
15
for 1 < i < n do
16
Order aI  ai and ai  aG
17
return hfa1; : : : ; ang; +i;

Figure 11: The VPC algorithm
into a p.o. plan which is then post-processed to determine which actions can be executed
in parallel. The action language used is a STRIPS-style language allowing quantifiers and
context-dependent effects. However, the plans produced by the planner, and thus input
to VPC, are ground and without context-dependent effects. That is, they are ordinary
propositional STRIPS plans. The VPC algorithm is presented in Figure 11, with a few minor
differences in presentation as compared to its original appearance: First, the algorithm is
presented in the GT formalism, in order to minimize the number of formalisms in this article,
but all preconditions are assumed to be positive, thus coinciding with the original algorithm.
Second, while the original algorithm returns the transitive reduction of the computed order
it instead returns the transitive closure here, an unimportant difference in order to coincide
with the definition of plans in this article. Furthermore, Veloso6 has pointed out that the
published version of the VPC algorithm is incorrect and that a corrected version exists.
The version presented in Figure 11 is this corrected version. A proposition is a primary
effect if it appears either in the goal or in the subgoaling chain of a goal proposition.
VPC is a greedy algorithm which constructs an entirely new partial order by analysing
the action conditions, using the original total order only to guide the greedy strategy. The
algorithm is claimed (Veloso et al., 1990, p. 207) to produce a `least-constrained' p.o. plan,
although no definition is given of what this means. Veloso7 has confirmed that the term `least
constrained plan' was used in a `loose sense' and no optimality claim was intended. However,
if this term is not defined, then it is impossible to know what problem the algorithm is
intended to solve or how to judge whether it makes any improvement over using no algorithm
at all. In the absence of such a definition from its authors, the algorithm will be analysed
with respect to the least-constrainment criteria defined in Section 4. This is admittedly a
6. Personal communication, oct. 1993.
7. Veloso, ibid.

124

fiComputational Aspects of Reordering Plans

p
q
1

b r


p 
a 
Pq 
PPP
PP
qc s
P1

a pq

q

-p b qr

-q c s

P2

Figure 12: The p.o. plans in the failure example for VPC.
somewhat unfair analysis, but it reveals some interesting facts about the algorithm, and
about what problems it does not solve. It is immediate from Theorem 4.8 and Corollary 4.9
that VPC cannot be expected to produce minimum-constrained de-/reorderings. Perhaps
more surprisingly, VPC does not even guarantee that its output is a minimal -constrained
deordering of its input, a problem already proven trivially polynomial (Theorem 4.4). This
is illustrated by the following example.
Suppose a total-order planner is given the ppi  = h;; fr; sgi as input. It may then
return either of the -valid t.o. plans ha; b; ci and ha; c; bi, with action conditions as shown
in Figure 12. When used as input to VPC, these two t.o. plans will give quite different results|the plan ha; c; bi will be converted to the p.o. plan P1 in Figure 12, while
the plan ha; b; ci will be converted to the p.o. plan P2 in Figure 12. That is, in the first
case VPC produces a plan which is not only a minimal-constrained deordering but even
a minimum-constrained deordering, while in the second case it does not even produce a
minimal-constrained deordering.8
The reason that VPC may fail to produce a minimal-constrained deordering is that it
uses a non-admissible greedy strategy. Whenever it needs to find an operator a achieving
an effect required by the precondition of another operator b, it chooses the last such action
ordered before b in the input t.o. plan. However, there may be other actions earlier in the
plan having the same effect and being a better choice.

8.2 The KK algorithm

Kambhampati and Kedar (1994) have presented an algorithm for generalising the ordering of a p.o. plan, using explanation-based generalisation. The algorithm is based on first
constructing a validation structure for the plan and then use this as a guide in the generalisation phase. In the original paper, these computations are divided into two separate
algorithms (EXP-MTC and EXP-ORD-GEN), but are here compacted into one single algorithm, KK (Figure 13). Furthermore, the version presented here is restricted to ground
GT plans, while the original algorithm can also handle partially instantiated plans. This is
no restriction for the results to be shown below.
The first part of the KK algorithm constructs a validation structure V for the plan, that
is, an explanation for each precondition of every action in the plan. The validity criterion
underlying this phase is a simplified version of Chapmans modal-truth criterion (Chapman,
8. Note that transitive arcs are omitted in the figures, so P2 really has an ordering relation of size three.
Although this example would not work if plans had been defined in the equally reasonable way that
ordering relations should be intransitive, it is possible to construe similar examples also for this case.

125

fiBackstro m

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

procedure KK

Input: A valid self-contained p.o. plan hA; i
Output: A deordering of the input plan
comment Build a validation structure V for the plan

V

;

Let ha1 ; : : : ; an i be a topologically sorted version of hA; i
for 1  i  n do
for p 2 pre(ai ) do
Find min k < i s.t.
1. p 2 post(ak ) and
2. there is no j s.t. k < j < i and :p 2 post(aj )
Add hak ; p; ai i to V
comment Construct a generalised ordering 0 for the plan
for each ha; bi 2 do
Add ha; bi to 0 if either of the following holds
1. a = aI or a = aG
2. ha; p; bi 2 V for some p
3. hc; p; ai 2 V and :p 2 post(b)
4. hb; p; ci 2 V and :p 2 post(a)

return hA; 0i

Figure 13: The KK algorithm
1987) without white knights. Since the algorithm is simplified to only handle ground plans
here, an explanation is a causal link haP ; p; aC i, meaning that the action aP produces the
condition p which is consumed by the action aC . The algorithm constructs exactly one
causal link for each precondition, and it chooses the earliest producer of p preceeding aC
with no intervening action producing :p between this producer and aC . The second phase
of the algorithm builds a generalised ordering 0 for the plan based on this validation
structure. To put things simply, only those orderings of the original plan are kept which
either correspond to a causal link in the validation structure or that is required to prevent
a threatening action to be unordered wrt. the actions in such a causal link.
It turns out that also the KK algorithm fails in generating plans that are guaranteed to be even minimal-constrained deorderings. Consider the t.o. plan hA; B; C; Di
with action conditions as indicated in Figure 14. This t.o. plan is valid for the ppi
h;; fr; s; t; ugi. Since the KK algorithm always chooses the earliest possible producer
of a precondition for the validation structure, it will build the validation structure
fhA; p; Di; hA; s; aG i; hB; q; Di; hB; t; aG i; hC; r; aG i; hD; u; aG ig. Hence, the final ordering
produced by KK will be as shown in Figure 14a. However, this plan is not a minimalconstrained deordering of the original plan, since it can be further deordered as shown in
Figure 14b and remain valid. In this example, the input plan was totally ordered. In the
case of partially ordered input plans, the behaviour of the algorithm depends on the particular topological order choosen. So the algorithm may or may not find a minimal-constrained
deordering, but it is impossible to guarantee that it will succeed for all plans. Similarly, the
authors mention that one may consider different ways of constructing the validation struc126

fiComputational Aspects of Reordering Plans

ture. This would clearly also modify the behaviour and it remains an open question whether
it is possible to generate, in polynomial time, a validation structure that guarantees that a
minimal-constrained deordering is constructed in the second phase of the algorithm. Finding a validation structure that guarantees a minimum-constrained deordering is obviously
an NP-hard problem since the second phase of the algorithm is polynomial.
A ps

A ps

ZZ

B qt

ZZ~-p
q

p
r

D u

C q

a) Plan produced by KK

q

D u

B qt

C pq

r

-p

b) Minimal deordered version of a

Figure 14: Failure example for the KK algorithm

8.3 Planners with Optimality Guarantees
The planning algorithm Graphplan (Blum & Furst, 1997) has a notion of time steps and
tries to pack as many non-interacting actions as possible into one single time step. Furthermore, Graphplan finds the shortest plan, using the number of time steps as the measure.
If assuming unit time and that all actions considered as non-interacting by Graphplan
can be executed in parallel, then there is no plan having a shorter parallel execution than
the plan produced by Graphplan. That is, Graphplan produces minimum reordered
parallel plans under these assumptions. The second assumption is no limitation in practice,
since each non-concurrency relation can be encoded by introducing a new atom and letting
one of the interacting actions add it while the other one deletes it. The unit time assumption is more serious, however, especially since this assumption is likely not to hold in most
applications. In the car-assembly scenario in Section 2, for instance, Graphplan would
produce a plan that corresponds to the plan in Figure 5. Hence, the plan produced under
the unit-time assumption happens to coincide with the optimal plan when taking actual
execution times into account. This is just a fortunate coincidence, however, depending on
the particular durations of actions in this example. Suppose instead that the durations of
the actions are slightly different such that PAC has duration 2 and MvT1 has duration 8.
Then the plan produced by Graphplan, which corresponds to the plan in Figure 5, does
not have a faster schedule than 19 time units. This is not optimal since the plan in Figure 4
can be scheduled to execute in 17 time units for these particular duration times. Furthermore, it must be remembered that Graphplan is anyway restricted to those cases where
a GT-equivalent planning language is sucient, although recent improvements extend it to
127

fiBackstro m

somewhat more expressive languages (Gazen & Knoblock, 1997; Kohler, Nebel, Hoffman,
& Dimopoulos, 1997).
Knoblock (1994) has modified the UCPOP planner with a resource concept which makes
it avoid unordered interacting actions. This means that the resulting planner produces
definite parallel plans. Knoblock further modified the evaluation heuristic of the search to
take parallel execution time into account. It thus seems as if this planner might be able to
produce minimum reordered parallel plans, but the paper does not provide sucient details
to determine whether this is the case. It is also unclear whether the heuristic can handle
actions with different duration times.
Yet another example is the polynomial-time planner for the SAS+-IAO planning language (Jonsson & Backstrom, 1998) which produces plans which are minimum-constrained
reordered. That is, for this restricted formalism it is clearly possible to optimise the ordering
in polynomial time.

9. Discussion
The previous section listed a few planning algorithms from the literature that produce or
attempt to produce plans which are least constrained or minimum parallel reordered. They
do so only under certain restrictions, though. Furthermore, plans are not always generated
`from scratch', but can also be generated by modifying some already existing plan, referred
to as case-based planning, or by repairing a plan that has failed during the execution phase.
In such cases, the old plan may contain many ordering relations that will be obsolete in
the modified/repaired plan. In fact, the KK algorithm (Kambhampati & Kedar, 1994) is
motivated in the context of case-based planning. It is also important to remember that
today, and probably for a long time into the future, very few plans are generated entirely
by computer programs. The vast majority of plans in various applications are designed by
humans, possibly with computer support. Already for quite small plans, it is very dicult
for a human to see whether the ordering constraints are optimal or not, so computer support
for such analyses is vital for designing optimal plans. For the same reason, also hierarchicaltask-network planners, eg O-Plan (Currie & Tate, 1991) and Sipe (Wilkins, 1988), produce
plans where reordering actions could lead to better schedules. Such a planner often commits
to one of the two possible orderings for a pair of actions based on expert-knowledge rules.
However, it is hardly possible for a human expert to design rules that in all situations will
guarantee that the optimal ordering choice is made.
On the coarseness level of complexity analysis it does not matter whether the tasks
of planning, plan optimization and scheduling are integrated or separated since the total
resulting complexity will be the same in both cases|the latter two computations are at most
NP-complete and will, thus, be dominated by the planning, which is PSPACE-complete or
worse. However, for good reasons this has not prevented the research community from
studying planning and scheduling as separate problems, since understanding each problem
in isolation also helps understanding the overall process. For the same reason, it is important
to also study separately the problems discussed and analysed in this article. Furthermore,
on a more fine-grained, practical level there might be considerable differences in eciency
between integrating the three computations and doing them separately. For instance, even
if all three computations take exponential time, each of the problems considered in isolation
128

fiComputational Aspects of Reordering Plans

may have fewer parameters, in which case it may be much more ecient to solve them in
isolation. On the other hand, solving the whole problem at once may make it easier to
do global optimisation. Which is the better will depend both on which methods are used
and on various properties of the actual application, and it seems unlikely that one of the
methods should always be the better.
As has been shown in this article, minimum reordering is a much better optimality
criterion than minimum deordering, if only considering the overall parallel execution time.
However, this is not necessarily true if also considering further metric constraints for subsequent scheduling. Deordering a plan can only add to the number of feasible schedules, while
reordering may also remove some or, in the worst case, all feasible schedules. On the other
hand, reordering may also lead to new and better schedules not reachable via deordering.
Deordering can thus be viewed as a safe and, sometimes, cheap way to allow for better
schedules, while reordering is an expensive method which has a potential for generating
considerably better plans, but which may also make things worse. If using reordering in
practice in cases where also metric scheduling constraints are involved, it seems necessary
to use feedback from the scheduler to control the reordering process, or to try other reorderings. One could imagine a reordering algorithm which uses either heuristic search or
randomized local-search methods a la GSAT (Selman, Levesque, & Mitchell, 1992) to find
reorderings and then use the scheduler as evaluation function for the proposed reorderings.
While the plan modifications studied in this article may add considerably to the optimizations that are possible with traditional scheduling only, there is still a further potential
of optimization left to study|modifying not only the action order, but also the set of actions. Such modification is already done in plan adaptation, but then only for generating a
new plan from old cases, and optimizations in the sense of this article are not considered.
Some preliminary studies of action-set modifications appear in the literature, though. Fink
and Yang (1992) study the problem of removing redundant actions from total-order plans,
defining a spectrum of redundancy criteria and analysing the complexity of achieving these.
It is less clear that it is interesting to study action addition; adding actions to a plan could
obviously not improve the execution time of it if it is to be executed sequentially. However,
in the case of parallel execution of plans it has been shown that adding actions to a plan can
sometimes allow for faster execution (Backstrom, 1994). Finally, if allowing both removal
and addition of actions, an even greater potential for optimising plans seems available, but
this problems seems not yet studied in the literature.

10. Conclusions
This article studies the problem of modifying the action ordering of a plan in order to
optimise the plan according to various criteria. One of these criteria is to make a plan
less constrained and the other is to minimize its parallel execution time. Three candidate
definitions are proposed for the first of these criteria, constituting a spectrum of increasing
optimality guarantees. Two of these are based on deordering plans, which means that ordering relations may only be removed, not added, while the last one builds on reordering,
where arbitrary modifications to the ordering are allowed. The first of the three candidates,
subset-minimal deordering, is tractable to achieve, while the other two, deordering or re129

fiBackstro m

ordering a plan to minimize the size of the ordering, are both NP-hard and even dicult
to approximate.
Similarly, optimising the parallel execution time of a plan is studied both for deordering
and reordering of plans. In the general case, both of these computations are NP-hard and
dicult to approximate. However, based on an algorithm from the literature it is shown
that optimal deorderings can be computed in polynomial time for definite plans for a class
of planning languages based on the notions of producers, consumers and threats, which
includes most of the commonly used planning languages. Computing optimal reorderings
can potentially lead to even faster parallel executions, but this problem remains NP-hard
and dicult to approximate even under quite severe restrictions. Furthermore, deordering
a plan is safe with respect to subsequent scheduling, while reordering a plan may remove
feasible schedules, making deordering a good, but often suboptimal, approach in practice.

Acknowledgements

Tom Bylander, Thomas Drakengren, Mark Drummond, Alexander Horz, Peter Jonsson,
Bernhard Nebel, Erik Sandewall, Sylvie Thibeaux and the anonymous referees provided
helpful comments on this article and previous versions of it. The research was supported
by the Swedish Research Council for Engineering Sciences (TFR) under grants Dnr. 92-143
and 95-731.

Appendix A

Theorem 7.10 Minimum Parallel Reordering remains NP-hard even when restricted

to total-order GT plans with only toggling unary actions and under the assumption of unit
time, simple concurrency and that no actions are redundant.

Proof: Proof by reduction from 3SAT (Garey & Johnson, 1979, p. 259). Let P =
fp1 ; : : : ; png be a set of atoms and C = fC1 ; : : : ; Cm g a set of clauses over P s.t. for
1  i  m, Ci = fli;1 ; li;2 ; li;3 g is a set of three literals over P .
First define the set of atoms

Q = fpFi ; pTi ; qi j 1  i  ng [ fci;j ; ri;j j 1  i  n; 1  j  3g:
Then define a GT ppi  = hI; Gi with initial and goal states defined as
I = Neg(Q)
G = fpFi ; pTi ; :qi j 1  i  ng [ fci;j ; :ri;j j 1  i  n; 1  j  3g
Also, for each atom pi 2 P , define four actions according to Table 2.
Further, for each clause Ci 2 C , define nine actions according to Table 3 where
( F

l = pk if li;j = :pk
i;j

pTk

if li;j = pk :

Let A be the set of all 4n + 9m actions thus defined. Clearly there is some total order 
s.t. the plan P = hA; i is -valid. It is also obvious that none of the actions is redundant.
130

fiComputational Aspects of Reordering Plans

It is a trivial observation that any parallel execution r of any -valid reordering of P
must satisfy that for each i, 1  i  n, either

r(AFi ) < r(A+i ) < r(ATi ) < r(A,i )
or
and for each i, 1  i  m,

r (C +

i;k1

r(A+i ) < r(ATi ) < r(A,i ) < r(AFi );

( , )
, ))
r
(Ci;k
r(Ci;k2 )
+
+
,
1
i;k1 ) < r(C + ) < r(Bi;k2 ) < r(C + ) < r(Bi;k3 ) < r(Ci;k3 );
i;k2
i;k3

) < r(B +

(

where k1 ; k2 ; k3 is a permutation of the numbers 1; 2; 3. (This is to be interpreted s.t. the
, and C + can be released in either order, or simultaneously, and analogously
actions Ci;k
i;k2
1
, and
+ ).
for the actions Ci;k
Ci;k
2
3
The remainder of this proof shall show that P can be reordered to have a parallel
execution of length 8 iff the set C of clauses is satisfiable.
if: Suppose C is satisfiable. Let I be a truth assignment for the atoms in P that satisfies
C . Wlg. assume I (pi ) = T for all i. Further, for each clause Cj , let lj be any literal in Cj
which is satisfied by I . Disregarding the action order for a moment, choose a release-time
function r for the actions as follows. For 1  i  n, let

r(A+i ) = 0; r(ATi ) = 1; r(A,i ) = 2; r(AFi ) = 3:
Further, for each j , 1  j  m, choose k1 s.t. lj;k1 2 Cj is satisfied by I (at least one such
choice must exist by the assumption). Let lj;k2 and lj;k3 be the remaining two literals in Cj .
Assign release times s.t. for 1  h  3,
+ ) = 2h , 1; r (B + ) = 2h ; r (C , ) = 2h + 1:
r(Cj;k
j;kh
j;kh
h

Now define the partial order 0 on A s.t. for all actions a; b 2 A, a 0 b iff r(a) < r(b).
Clearly, the plan hA; 0 i is a -valid reordering of P and r is a parallel execution of length
8 for hA; 0 i. (Note that no other choice of I could force a longer execution, while there is
an execution of length 7 in the case where C is satisfied by setting all atoms false.)
operator precond. postcond.

AFi
ATi
A+i
A,i

:pFi ; :qi pFi
:pTi ; qi pTi
:qi
qi
qi
:qi

Table 2: Generic actions for each atom pi in the proof of Theorem 7.10.
131

fiBackstro m

operator precond.

Bi;+1
Bi;+2
Bi;+3
Ci;+1
Ci;,1
Ci;+2
Ci;,2
Ci;+3
Ci;,3

li; 1; ri;1 ; :ri;2 ; :r1;3 ; :ci;1
li; 2; :ri;1; ri;2 ; :r1;3 ; :ci;2
li; 3; :ri;1; :ri;2; r1;3 ; :ci;3
:ri;1
ri;1
:ri;2
ri;2
:ri;3
ri;3

postcond.

ci;1
ci;2
ci;3
ri;1
:ri;1
ri;2
:ri;2
ri;3
:ri;3

Table 3: Generic atoms for each clause Ci in the proof of Theorem 7.10.
only if: Suppose C is not satisfiable. Further suppose that Q is a minimum reordering
of P and that r is a parallel execution of length 8 or shorter for Q. Wlg. assume that every
action is released as early as possible by r. Then, according to the observation above it
must hold for each i, 1  i  n, that either
r(AFi ) = 0; r(A+i ) = 1; r(ATi ) = 2; r(A,i ) = 3
or
r(A+i ) = 0; r(ATi ) = 1; r(A,i ) = 2; r(AFi ) = 3:
Hence, exactly one of the atoms pFi and pTi is true at time 2. Let pi denote this atom. Since
+)2
r is of length 8, it follows from the earlier observation that for all j , 1  j  m, r(Bj;k
for some k, 1  k  3. Hence, lj;k = pi for some i, since Q is -valid and r is a parallel
execution for Q. Define an interpretation I s.t. for all i, 1  i  n,
(
if pi = pFi
I (pi ) = F;
T; otherwise :
However, this interpretation is obviously a model for C , which contradicts the assumption.
It follows that r must be of length 9 or longer.
This concludes the proof and shows that C is satisfiable iff P has a reordering with a
parallel execution of length 8 or not.
2

Theorem 7.11

Minimum Parallel Deordering cannot approximate Minimum
Parallel Reordering within jAjk for any constant k  0.

Proof: The proof assumes GT plans and simple concurrency. First, define the generic

actions aki (m), bki and cki (m) according to Table 10. Further, define recursively the generic
plans
( 1
ha
(1); b0
; c1
(1); : : : ; a1im (1); b0im ; c1im (1)i;
for k = 1
k
Pi (m) = ha(ki,1)m+1 (m); P(i,k1),1m+1 ((mi,);1)cmk+1
k
,
1
k
k
1 (m); : : : ; aim (m); Pim (m); cim (m)i; for k > 1:
(i,1)m+1
(i,1)m+1
132

fiComputational Aspects of Reordering Plans

Furthermore, for arbitrary k; n > 0 define the ppi kn = hfpk1 ; : : : ; pkn g; fq1k ; : : : ; qnk gi.
Now, prove the claim that for arbitrary k; n > 0, the plan P1k (n)
1. is kn -valid,
P ,1 2ni and
2. has no deordering of length less than 3nk + ki=1
3. has a reordering of length 2k + 1.
Proof by induction over k.
Base case (k=1): Choose an arbitrary n > 0. The plan P11 (n) is obviously kn -valid and
has no deordering other than itself, which is of length 3n. Consider the reordering Q11 (n) of
P11 (n) with the same actions and with ordering relation  defined s.t. for all i, 1  i  n,
a1i (1)  b0i  c1i (1) and for all i, 1 < i  n, a1i (1)  b0i,1 . This reordering is k (n)-valid and
has a parallel execution r11 (n) of length 3, defined s.t. for all i, 1  i  n, r11 (n)(a1i (1)) = 1,
r11(n)(b0i ) = 2 and r11 (n)(c1i (1)) = 3. (This plan is shown in Figure 15.) The claim is thus
satisfied for the base case.
Induction: Suppose the claim is satisfied for all l < k, for some k  1 and prove that
the claim holds also for l = k. Choose an arbitrary n > 0. It follows from the induction
hypothesis that none of the subplans P1k,1 (n) : : : ; Pnk,1 (n) can be deordered, so they have
to remain totally ordered. Furthermore, for all i, 1  i  n, it is necessary that the action
aki (n) is ordered before the subplan Pik,1(n) and that the action cki (n) is ordered after it.
It is also clear that for no i, 1  i  n can the order cki (n)  aki+1 (n) be removed without
making the plan invalid. Hence, P1k (n) has no other deordering than itself, which is of
length
n
X
i=1

(2 + length(Pik,1 (n)) = n(2 + length(P1k,1 (n)))

= 2n + n(3nk,1 +

kX
,2
i=1

2ni ) = 3nk +

kX
,1
i=1

2ni ;

which proves the deordering case of the claim.
For the reordering case, define a reordering Qk1 (n) of P1k (n) with the same actions and
with ordering relation defined as follows. For each subplan Pik,1 (n) of P1k (n), reorder its
actions so it has length 2(k , 1)+1, which is possible according to the induction hypothesis.
Further, for each i, 1  i  n, and each j , (i , 1)n + 1  j  in order aki (n)  akj ,1 (n) and
ckj ,1 (n)  cki (n) (or aki+1 (n)  akj ,1 (1) and ckj ,1 (1)  cki (n) for the case k = 2). Hence, each
action pre-condition

post-condition

aki (m) fpki g
fpk(i,,11)m+1 ; : : : ; pkim,1; :q(ki,,11)m g
bki
fpki g
fqik g
k,1 g post(ck (m)) = fqk g:
cki (m) fq(ki,,11)m+1 ; : : : ; qim
i
i
Table 4: Generic actions for the proof of Theorem 7.11.
133

fiBackstro m

P11 (n)
a11 (1)

a21 (n)


,
@


,,
1
,
p1
, : a1(1)
1 
,
2
p
2

@
p1n@@

...
.
:. q0

p01

:q10
p02
:q20

(n,1)

@@
R a1n(1)



p0n



b01

-

b02

-

b0n

...
..

q10

- c11(1)

@

@@q11
q20 - 1
c2 (1) XXXq21@
X@XXz@R c21(n)
...
,,
,
.
,qn1
,
0
qn - 1 ,
1 cn(1)





:qn1 
..
:



.

2

.
a2 (n) XXXX..
z
X
 :


 .
..
...
...
.
1
:q(1n,1) 
:
.

X
a2n (n) 
XXX...X
z

P21 (n)

Pn1 (n)

X..XXXXz 2
..: c2(n)
...
..

X..XXXXz 2
.
.: cn(n)

Figure 15: The reordering Q21 (n) of the plan P12 (n) as an example of the induction case in
the proof of Theorem 7.11 (solid arrows denote orderings required by producerconsumer relationships and are labelled with the atom produced/consumed,
while dashed arrows denote ordering constraints to avoid threats and are labelled with the possibly conicting atom).
segment of the type aki (n); Pik,1 (n); cki (n) is reordered to have length 2k + 1. Finally, for
each i, 1  i  n, order aki (n)  ak(i,,11)n (n) (or aki (n)  ak(i,,11)n (1) for the case k = 2). The
plan Qk1 (n) is k (n)-valid since the subplans P1k,1 (n); : : : ; Pnk,1 (n) do not have any atoms
in common and, thus, the # relation does not hold between any two actions belonging to
different such subplans. This reordered plan can be executed under the parallel execution
rik (n) defined s.t. rik (n)(aki (n)) = 1, rik (n)(cki (n)) = 2k + 1 and for all i, 1  i  n and
all actions a0 2 Qki ,1 (n), rik (n)(a0 ) = rik,1(n)(a0 ) + 1. Since this is a parallel execution of
length 2k + 1 for the reordered plan, the claim holds also for k.
This concludes the induction, so the claim holds for all k > 0. Since
P ,1 2ni
3nk + ki=1
1
k

2k + 1
(2k + 1)3k,1 jAj
for all k > 0, the theorem holds.
2
134

fiComputational Aspects of Reordering Plans

References

Backstrom, C. (1993). Finding least constrained plans and optimal parallel executions is
harder than we thought. In Backstrom, C., & Sandewall, E. (Eds.), Current Trends in
AI Planning: EWSP'93|2nd European Workshop on Planning, pp. 46{59 Vadstena,
Sweden. IOS Press.
Backstrom, C. (1994). Executing parallel plans faster by adding actions. In Cohn,
A. G. (Ed.), Proceedings of the 11th European Conference on Artificial Intelligence
(ECAI'94), pp. 615{619 Amsterdam, Netherlands. Wiley.
Backstrom, C. (1995). Expressive equivalence of planning formalisms. Artificial Intelligence,
76 (1{2), 17{34.
Backstrom, C., & Klein, I. (1991). Parallel non-binary planning in polynomial time. In
Reiter, R., & Mylopoulos, J. (Eds.), Proceedings of the 12th International Joint Conference on Artificial Intelligence (IJCAI'91), pp. 268{273 Sydney, Australia. Morgan
Kaufmann.
Bellare, M., Goldreich, O., & Sudan, M. (1995). Free bits, PCPs and non-approximability|
towards tighter results. In Proceedings of the 36th Annual IEEE Symposium on the
Foundations of Computer Science (FOCS'95), pp. 422{431 Milwaukee, WI, USA.
IEEE Computer Society.
Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90 (1{2), 281{300.
Chapman, D. (1987). Planning for conjunctive goals. Artificial Intelligence, 32 (3), 333{377.
Crescenzi, P., & Panconesi, A. (1991). Completeness in approximation classes. Information
and Computation, 93 (2), 241{262.
Currie, K., & Tate, A. (1991). O-Plan: The open planning architecture. Artificial Intelligence, 52 (1), 49{86.
Feige, U., & Kilian, J. (1996). Zero knowledge and the chromatic number. In 11th Annual
IEEE Conference on Computational Compelxity (CCC'96) Philadelphia, PA, USA.
IEEE Computer Society.
Feige, U. (1996). A threshold of ln n for approximating set cover (preliminary version). In
Proceedings of 28th Annual ACM Symposium on Theory of Computing (STOC'96),
pp. 314{318 Philadelphia, PA, USA. ACM.
Fink, E., & Yang, Q. (1992). Formalizing plan justifications. In Proceedings of the 9th Conference of the Canadian Society for Computational Studies of Intelligence (CSCSI'92),
pp. 9{14 Vancouver, BC, Canada.
Garey, M., & Johnson, D. (1979). Computers and Intractability: A Guide to the Theory of
NP-Completeness. Freeman, New York.
135

fiBackstro m

Gazen, C., & Knoblock, C. (1997). Combining the expressivity of UCPOP with the eciency
of Graphplan. In Steel, & Alami (1997), pp. 221{233.
Ghallab, M., & Laruelle, H. (1994). Representation and control in IxTeT, a temporal
planner. In Hammond (1994), pp. 61{67.
Hammond, K. (Ed.). (1994). Proceedings of the 2nd International Conference on Artificial
Intelligence Planning Systems (AIPS'94), Chicago, IL, USA. AAAI Press.
Jonsson, P., & Backstrom, C. (1998). State-variable planning under structural restrictions:
Algorithms and complexity. Artificial Intelligence, 100 (1{2), 125{176.
Kambhampati, S. (1994). Multi-contributor causal structures for planning: A formalization
and evaluation. Artificial Intelligence, 69 (1{2), 235{278.
Kambhampati, S., & Kedar, S. (1994). A unified framework for explanation-based generalization of partially ordered and partially instantiated plans. Artificial Intelligence,
67 (1), 29{70.
Klein, I., Jonsson, P., & Backstrom, C. (1995). Tractable planning for an assembly line.
In Ghallab, M., & Milani, A. (Eds.), New Directions in AI Planning: EWSP'95|
3rd European Workshop on Planning, Frontiers in AI and Applications, pp. 313{324
Assisi, Italy. IOS Press.
Klein, I., Jonsson, P., & Backstrom, C. (1998). Ecient planning for a miniature assembly
line. Artificial Intelligence in Engineering, 13 (1), 69{81.
Knoblock, C. (1994). Generating parallel execution plans with a partial-order planner. In
Hammond (1994).
Kohler, J., Nebel, B., Hoffman, J., & Dimopoulos, Y. (1997). Extending planning graphs
to an ADL subset. In Steel, & Alami (1997), pp. 273{285.
Lund, C., & Yannakakis, M. (1994). On the hardness of approximating minimization problems. Journal of the ACM, 41 (5), 960{981.
Nebel, B., & Backstrom, C. (1994). On the computational complexity of temporal projection, planning and plan validation. Artificial Intelligence, 66 (1), 125{160.
Pednault, E. P. D. (1986). Formulating multiagent, dynamic-world problems in the classical
planning framework. In Georgeff, M., & Lansky, A. L. (Eds.), Reasoning about Actions and Plans, Proceedings of the 1986 Workshop, pp. 47{82 Timberline, OR, USA.
Morgan Kaufmann.
Regnier, P., & Fade, B. (1991a). Complete determination of parallel actions and temporal
optimization in linear plans of action. In Hertzberg, J. (Ed.), European Workshop
on Planning, Vol. 522 of Lecture Notes in Artificial Intelligence, pp. 100{111 Sankt
Augustin, Germany. Springer.
136

fiComputational Aspects of Reordering Plans

Regnier, P., & Fade, B. (1991b). Determination du parallelisme maximal et optimisation
temporelle dans les plans d'actions lineaires. Revue d'intelligence artificielle, 5 (2),
67{88.
Selman, B., Levesque, H., & Mitchell, D. (1992). A new method for solving hard satisfiability problems. In Proceedings of the 10th (US) National Conference on Artificial
Intelligence (AAAI'92), pp. 440{446 San Jose, CA, USA. American Association for
Artificial Intelligence.
Steel, S., & Alami, R. (Eds.). (1997). 4th European Conference on Planning, ECP'97, Vol.
1348 of Lecture Notes in Artificial Intelligence, Toulouse, France. Springer.
Stromberg, J.-E. (1991). Styrning av LEGO-bilfabrik. Andra omarbetade upplagan. Department of Electrical Engineering, Linkoping University.
Tate, A. (1975). Interacting goals and their use. In Proceedings of the 4th International
Joint Conference on Artificial Intelligence (IJCAI'75), pp. 215{218 Tbilisi, USSR.
IJCAI, William Kaufmann.
Veloso, M. M., Perez, M. A., & Carbonell, J. G. (1990). Nonlinear planning with parallel
resource allocation. In Sycara, K. P. (Ed.), Workshop on Innovative Approaches
to Planning, Scheduling and Control, pp. 207{212 San Diego, CA, USA. Morgan
Kaufmann.
Vere, S. A. (1983). Planning in time: Windows and durations for activities and goals. IEEE
Transactions on Pattern Analysis and Machine Intelligence, PAMI-5 (3), 246{267.
Wilkins, D. E. (1988). Practical Planning. Morgan Kaufmann, San Mateo, CA.

137

fiJournal of Artificial Intelligence Research 9 (1998) 219-245

Submitted 3/98; published 11/98

The Gn;m Phase Transition is Not Hard for the Hamiltonian
Cycle Problem
Basil Vandegriend
Joseph Culberson

Department of Computing Science, University of Alberta,
Edmonton, Alberta, Canada, T6G 2H1

basil@cs.ualberta.ca
joe@cs.ualberta.ca

Abstract

Using an improved backtrack algorithm with sophisticated pruning techniques, we revise previous observations correlating a high frequency of hard to solve Hamiltonian cycle
instances with the Gn;m phase transition between Hamiltonicity and non-Hamiltonicity.
Instead all tested graphs of 100 to 1500 vertices are easily solved.
When we artificially restrict the degree sequence with a bounded maximum degree,
although there is some increase in diculty, the frequency of hard graphs is still low. When
we consider more regular graphs based on a generalization of knight's tours, we observe
frequent instances of really hard graphs, but on these the average degree is bounded by a
constant. We design a set of graphs with a feature our algorithm is unable to detect and so
are very hard for our algorithm, but in these we can vary the average degree from O(1) to
O(n). We have so far found no class of graphs correlated with the Gn;m phase transition
which asymptotically produces a high frequency of hard instances.

1. Introduction

Given a graph G = (V; E ); jV j = n; jE j = m, the Hamiltonian cycle problem is to find a
cycle C = (v1 ; v2 ; : : : ; vn ) such that vi 6= vj for i 6= j , (vi ; vi+1 ) 2 E and (vn ; v1 ) 2 E . As
for any NP-C problem, we expect solving it to require exponential time in the worst case
on arbitrary graphs (assuming P 6= NP). However, in recent years researchers examining
various NP-C problems such as SAT and graph coloring have discovered that the majority
of graphs are easy for their algorithms to solve. Only graphs with specific characteristics or
graphs which lie within a narrow band (according to some parameter) seem to be hard to
solve for these problems.
It is known (Posa, 1976; Komlos & Szemeredi, 1983) that under a random graph model
(Gn;m ) as the edge density increases there is a sharp threshold (the phase transition) such
that below that edge density the probability of a Hamiltonian cycle is 0, while above it the
probability is 1. Previous research (Section 2.1) suggested that there is a high correlation
of dicult problems with instances generated with edge density near the phase transition.
Using an improved Hamiltonian cycle backtrack algorithm (Section 3) that employs various
pruning operators and an iterated restart technique, we observe no hard instances at the
transition for large n. Section 4 describes our results on Gn;m and related random graphs.
In an attempt to find a higher frequency of hard graphs, in Section 5 we examine a low
degree random graph class we call Degreebound graphs. However, these graphs are also
usually easy for our backtrack algorithm, although we do find a few hard graphs. Analysis
of these graphs indicates a test for non-Hamiltonian instances discussed in Section 5.3. In
c 1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiVandegriend & Culberson

Section 6 we examine a graph class based on a generalization of the knight's tour problem.
These graphs are significantly harder for our algorithm in general. In Section 7 we present a
constructed graph class which produces exponential behavior for our backtrack algorithm.
Our experimental results provide evidence that the average degree of a graph is not a
sucient indicator for hard graphs for the Hamiltonian cycle problem. With our backtrack
algorithm, the phase transition regions of the Gn;m and Degreebound graph models are
generally asymptotically easy.

2. A Discussion of Hardness and Previous Work
The concept of hardness of instances and hard regions within graph classes, considered from
an empirical basis, is not easy to define. In order to clarify what we mean, in this section
we present our notions of hardness, relating this to previous work.

2.1 What is Hardness?

A problem of size n is a set n of instances. For the Hamiltonian cycle problem, n is
the set of undirected graphs on n vertices. Any discussion of the hardness of a particular
instance of a problem is always with respect to an algorithm (or set of algorithms). In
general, different algorithms will perform differently on the instance. Furthermore, for each
particular instance of Hamiltonian cycle there is an associated algorithm that either correctly answers NO or outputs a cycle in O(n) time. To meaningfully talk about the hardness
of an instance, we must assume a fixed algorithm (or a finite class of algorithms) that is
appropriate for a large (infinite) class of instances, and then consider how the algorithm
performs on the instance. Hardness of an instance is always a measure of performance
relative to an algorithm.
We are left with the question of how much work an algorithm must do before we consider
the instance hard for it. Note that for a single instance the distinction between polynomial
and exponential time is moot. Ideally, we would like to require the algorithm to take an
exponential (i.e. an for some a > 1) number of steps as size n increases. Note that empirical
corroboration of such is practically impossible for sets of large instances. In practice, we
must be content with evidence such as failure to complete within a reasonable time for
larger instances.
We would also like an instance to exhibit some robustness before we consider it hard for
a given algorithm. Ideally, for graph problems we would at a minimum require the instance
to remain hard with high probability under a random relabeling of the vertices. Relabeling
the vertices produces an isomorphic copy of the graph, preserving structural properties such
as degree, connectivity, Hamiltonicity, cut sets, etc. The design of algorithms is typically
based on identifying and using such properties, and as far as possible eciency should be
independent of the arbitrary assignment of labels.
Let us refer to a (probabilistic) problem class as a pair (n ; Pn ), where Pn (x) is the
probability of the instance x given that we are selecting from n . Problem classes are
sometimes called ensembles in the Artificial Intelligence literature (Hogg, 1998). The usual
classes for graph problems are Gn;p, where to generate an n vertex graph, each pair of
vertices is included as an edge with probability p, and Gn;m where m distinct edges are
220

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

selected at random and placed in the graph. These two models are related (Palmer 1985).
For this paper we use the Gn;m model.
We do not consider mean or average run times in our definitions. The primary reason
is that for exponentially small sets of exponentially hard instances, it is impractical to
determine the average with any reasonable assurance. For example, if 1=2n of the instances
require (n2 2n ) time and the remainder are solved in O(n2 ) time then the average time is
quadratic, while if the frequency increases to 1=20:9n the average time is exponential. Even
for n = 100 it would be utterly impractical to distinguish between these two frequencies
with empirical studies.
Furthermore, and for similar reasons, if we want to promote a class as a benchmark class
for testing and comparing algorithms, low frequencies of hard instances are not generally
sucient. We will say that a problem class is maximally hard (with respect to an algorithm
or set of algorithms) if the instances generated according to the distribution are hard with
probability going to one as n goes to infinity.
As an example of maximally hard classes, empirical evidence suggests that a variety of
hidden coloring graph generators based on the Gn;p model are maximally hard for a large
variety of graph coloring algorithms (Culberson and Luo, 1993). These hard classes are
all closely related to a coloring phase transition in random graphs. In general, a phase
transition is defined by some parameterized probability distribution on the set of instances.
As the parameter is varied past a certain threshold value, the asymptotic probability of the
existence of a solution switches sharply from zero to one.
Phase transitions are commonly considered to be identified with hard subsets of a particular problem (Cheeseman, Kanefsky, & Taylor, 1991). Many NP-C problems can be
characterized by a `constraint' parameter which measures how constrained an instance is.
Evaluation of a problem using this constraint parameter typically divides instances into
two classes: those that are solvable, and those that are unsolvable, with a sharp transition
occurring between them. When the problem is highly constrained, it is easily determined
that no solution exists. As constraints are removed, a solution is easily found.
Different researchers (Cheeseman et al., 1991; Frank & Martel, 1995; Frank, Gent, &
Walsh, 1998) have examined phase transitions on random graphs for the Hamiltonian cycle
problem. The obvious constraint parameter is the average degree (or average connectivity)
of the graph. As the degree increases, the graph becomes less constrained: it becomes easier
both for a Hamiltonian cycle to exist and for an algorithm to find one. These researchers
have examined how Hamiltonicity changes with respect to the average degree. Frank et
al. (1998) and Frank and Martel (1995) experimentally verified that when using the Gn;m
model the phase transition for Hamiltonicity is very close to the phase transition for biconnectivity, which occurs when the average degree is approximately ln n (or m = n ln n=2) 1 .
Cheeseman et al. (1991) experimentally confirmed theoretical predictions by Komlos and
Szemeredi (1983) that the phase transition (for the Hamiltonian cycle problem) occurs when
the average degree is ln n + ln ln n. The papers also provided empirical evidence that the
time required by their backtrack algorithms increased in the region of the phase transition
and noted that the existence of very hard instances appeared to be associated with this
transition.
1. Note that the average degree equals 2m=n.

221

fiVandegriend & Culberson

As mentioned above, the k-colorable Gn;p class appears maximally hard for all known
algorithms with respect to a phase transition defined by n; p and k, where k  n= logb n
and b = 1=(1 , p). The Hamiltonian cycle Gn;m class on the other hand does not appear
maximally hard for any value of m. In fact, for large n our algorithm almost never takes
more than O(n) backtrack nodes and O(nm) running time.
We will use a much weaker requirement and say an instance is quadratically hard if it
requires at least n2 search nodes by the backtrack algorithm described in section 3. Note
that 
(n2 ) search nodes would take our algorithm 
(n3 ) time. For practical reasons, we will
also use a weaker definition for robustness, and say that an instance is robustly quadratically
hard if our algorithm uses at least n2 search nodes when the iterated restart feature is used
with a multiplying factor of 2. (See section 3 for program details). We say a class is
minimally hard if there is some constant  > 0 such that the probability of a hard instance
is at least  as n ! 1.
In Section 4 we examine Gn;m random graphs using our backtrack algorithm on graphs
of up to 1500 vertices. The empirical evidence we collect suggests that in contrast to the
graph coloring situation, the Hamiltonian cycle Gn;m class is not minimally quadratically
hard, even for m at or near the phase transition, and even if we drop our minimal robustness
requirement.
Note that we do not dispute the claim that hard instances are more likely at the phase
transition than at other values of m, but rather claim that even at the transition the
probability of generating a hard instance rapidly goes to zero with increasing n.

2.2 Random Graph Theory and the Phase Transition

These results are not unexpected when one reviews the theoretical work on this graph class.
Since asymptotically the graph becomes Hamiltonian when an edge is added to the last
degree 1 vertex (Bollobas, 1984), any algorithm that checks for a minimum degree  2
will detect almost all non-Hamiltonian graphs. When the graph is Hamiltonian, various
researchers (Angluin & Valiant, 1979; Bollobas, Fenner, & Frieze, 1987) have proven the
existence of randomized heuristic algorithms which can almost always find a Hamiltonian
cycle in low-order polynomial time. In particular, it is shown (Bollobas et al., 1987) that
there is a polynomial time algorithm HAM such that
8
>
<

0
if cn ! ,1
,e,2c if cn ! c
lim
Pr
(HAM
finds
a
Hamilton
cycle)
=
e
n!1
>
:
1
if cn ! 1
where m = n=2(ln n + ln ln n + cn ).
Furthermore, as the authors point out, this is the best possible result in the sense
that this is also the asymptotic probability that a Gn;m graph is Hamiltonian, and is the
probability that it has a minimum degree of 2. In other words, the probability of finding
a cycle is the same as the probability of one existing. Given that it is trivial to check the
minimum vertex degree of a graph, this does not leave much room for the existence of hard
instances (for HAM and similar algorithms).
Another relevant theoretical result is that there is a polynomial time algorithm which
with probability going to one, finds some Hamiltonian cycle when a graph has a hidden
222

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

Hamiltonian cycle together with extra randomly added edges(Broder, Frieze, & Shamir,
1994). For the algorithm to work, the average degree of a vertex needs only be a constant.
They claim the result can be easily extended to the case that the average degree is a growing
function of n. This is another indication that Hamiltonian graphs near the phase transition
will be easy to solve by some algorithm.
For a non-Hamiltonian graph to be hard for an algorithm it must contain a feature
preventing the formation of a Hamiltonian cycle which the algorithm cannot easily detect.
Suppose a backtrack algorithm does not check for vertices of degree one. The algorithm may
then require exponential backtrack before determining the non-Hamiltonicity of the graph,
since the only way it can detect this is by trying all possible paths and failing. However,
degree one vertices are easily detectable, and so are not good indicators of hard instances.
They also disappear at the phase transition.
Similarly, an algorithm might not check for articulation points, and as a result waste
exponential time on what should be easy instances. As n ! 1, the probability of an
articulation point existing (in Gn;m ) goes to zero as fast as the probability of the existence
of a vertex of degree less than two. Other features can lead to non-Hamiltonicity of course,
such as k-cuts that leave k +1 or more components (Bondy & Murty, 1976), and these could
require time proportional to nk to detect. Under the assumption that NP6=CO-NP there
must also exist a set of non-Hamiltonian instances which have no polynomial proof of their
status.
However, it seems that at the phase transition the larger the feature the less likely it
is to occur. In fact, the theoretical results summarized above indicate this must happen.
Although we know hard graphs exist, and we may expect these localized types of hard
graphs to be more frequent near the phase transition than elsewhere when using Gn;m to
generate instances, we also expect the probability of such instances to go to zero as n
increases.

3. An Overview of our Backtrack Algorithm
Our backtrack algorithm comes from Vandegriend (1998), and is based upon prior work on
backtrack Hamiltonian cycle algorithms (Kocay, 1992; Martello, 1983; Shufelt & Berliner,
1994). It has three significant features which we will discuss. First, it employs a variety of
pruning techniques during the search that delete edges that cannot be in any Hamiltonian
cycle. This pruning is usually based upon local degree information. Second, before the
start of the search the algorithm performs initial pruning and identifies easily detectable
non-Hamiltonian graphs. The third feature is the use of an iterated restart technique.
Additionally, the program provides the opportunity to order the selection of the next vertex
during path extension using either a low degree first ordering, a high degree first ordering,
or a random ordering. We normally use the low degree first ordering.
At each level of the search, after adding a new vertex to the current path, search pruning
is used. The pruning identifies edges that cannot be in any Hamiltonian cycle and removes
them from the graph. (Note that if the algorithm backtracks, it adds the edges deleted
at the current level of the search back to the graph.) The first graph configuration that
the pruning looks for is a vertex x with 2 neighbours a; b of degree 2. Since the edges
incident on a and b must be used in any Hamiltonian cycle, the other edges incident on
223

fiVandegriend & Culberson

x can be deleted. The second graph configuration that the pruning looks for is a path
P = (v1 ; : : : ; vk ) of forced edges (so v2 : : : vk,1 are of degree 2). If k < n then the edge v1 ; vk

cannot be in any Hamiltonian cycle and can be deleted. If as a result of pruning, the degree
of any vertex drops below 2, then no Hamiltonian cycle is possible and the algorithm must
backtrack. The use of these operators may yield new vertices of degree 2 and therefore the
pruning is iterated until no further changes occur.
A pruning iteration takes O(n) time to scan the vertices to check for vertices with two
degree 2 neighbors, and O(n) time to extend all forced degree two paths. Since the iterations
terminate unless a new vertex of degree two is created, at most n iterations can occur. At
most O(m) edges can be deleted. On backing up from a descendant, the edges are replaced
(O(m)) and the next branch is taken. Thus, an easy upper bound on the pruning time for
a node searching from a vertex of degree d is O(d(n2 + m)), but this is overly pessimistic.
Note that along any branch from the root of the search tree to a leaf, at most n vertices
can be converted to degree 2. Also note that along each branch each edge can be deleted at
most once. If the degree is high we seldom take more than a few branches before success.
The implementation is such that when several vertices have two neighbors of degree two at
the beginning of an iteration, all redundant edges are removed in a single pass taking time
proportional to n plus the number of edges removed and checked. In practice, on Gn;m
graphs it typically takes O(n + m) time per search node on very easy Hamiltonian instances
as evidenced by CPU measurements, with harder instances taking at most twice as long
per search node.
Before the start of the recursive search, our algorithm prunes the graph as described
above. Then the algorithm checks to see if the graph has minimum degree  2, is connected,
and has no cut-points. If any of these conditions are not true, then the graph is nonHamiltonian and the algorithm is finished.
Some non-Hamiltonian instances may be very easy or very hard to detect, depending
on which vertex the algorithm chooses as a starting point. In these cases local features
exist that could be detected if the algorithm starts near them, but otherwise the algorithm
may backtrack many times into the same feature without recognizing that only the feature
matters. The seemingly hard instance on Gn for n = 100 discussed in Section 4.2 is such a
case. This is one type of \thrashing," and is a common problem in backtracking algorithms.
For example, Hogg and Williams (1994) noticed a sparse set of very hard 3-coloring problems
that were not at the phase transition. Baker (1995) showed that these instances were most
often hard as a result of thrashing, and that they could be made easy by backjumping or
dependency-directed backtracking.
To improve our algorithm's average performance we use an iterated restart technique.
The idea is to have a maximum limit M on the number of nodes searched. When the
maximum is reached, the search is terminated and a new one started with the maximum
increased by a multiple k (so Mi+1 = kMi ). Initially, M = kn. In our experiments, we
used k = 2. By incrementing the search interval in this way, the algorithm will eventually
obtain a search size large enough to do an exhaustive search and thus guarantee eventual
completion. The total search will never be more than double the largest size allocated.
Although random restarts are sometimes effective on non-Hamiltonian graphs, they are
more frequently effective on Hamiltonian instances. During search, as edges are added
to the set of Hamiltonian edges, the net effect is to prune edges from the graph. For a
224

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

Hamiltonian graph to be hard, the algorithm must select some set of edges which causes the
reduced graph to become non-Hamiltonian, and this non-Hamiltonian subgraph must itself
be hard to solve. With iterated restart, for the instance to remain hard the algorithm must
make such mistakes with high probability. As a result, we expect fewer hard Hamiltonian
instances.
Random restarts are an integral part of randomized algorithms (Motwani & Raghavan,
1995) and are used frequently in local search and other techniques to escape from local
optima (Johnson, Aragon, McGeoch, & Schevon, 1991; Langley, 1992; Selman, Levesque,
& Mitchell, 1992; Gomes, Selman, & Kautz, 1998). Further discussion of the impact of
restarts can be found in the analysis of the experiments on Gn;m graphs in Section 4.
The algorithm also provides for the possibility of checking for components and cut
vertices during recursive search after the pruning is completed at each search node. The
overhead of this extra work is O(n + m) per search node and rarely seems to pay off. Except
where noted these checks were not used in this study.
The experimental results reported in the remaining sections were run on a variety of
machines, the fastest of which is a 300 MHZ Pentium II. All CPU times reported are
either from this machine, or adjusted to it using observed speed ratios on similar tests.
Our algorithm terminated execution after 30 minutes2. Experimental results are frequently
reported as the ratio of the number of search nodes over the number of vertices. This node
ratio is used because we feel it provides a better basis for comparing results across different
graph sizes, since many of our results are O(n). Note that the number of search nodes is
calculated as the number of recursive calls performed.
We used several different methods of verifying the correctness of our algorithm and our
experimental results. The algorithm was independently implemented twice, and performs
automatic verification of all Hamiltonian cycles found. We performed multiple sets of experiments on generalized knight's circuit graphs and compared the results (graph Hamiltonian
or not) to our theoretical predictions. Initial sets of experiments on Gn;m graphs and Degreebound graphs were executed using two different pseudo-random number generators, and
were repeated multiple times. Our source code is available as an appendix.

4. Gn;m Random Graphs

We consider random graphs of 16 to 1500 vertices with m = dn=2. From previous work
(Cheeseman et al., 1991; Komlos & Szemeredi, 1983) we expect the phase transition to occur
when d  ln n + ln ln n. Thus we specify the constraint parameter (or degree parameter)
k = d =(ln n + ln ln n).

4.1 Gn;m Using Restart

For the premiere experiment, we generate Gn;m graphs with number of vertices n = 16 : : : 96
in steps of 4, n = 100 : : : 500 in steps of 100, n = 1000 and n = 1500. For each size n, the
degree parameter k ranges from 0:5 : : : 2:0 (step size 0.01 from k = 1:00 : : : 1:20, step size
2. Since the time limit of 30 minutes is at least two orders of magnitude greater than the typical running
time, the limit is rarely used. On slower machines this limit was increased. The Knight's tour graphs
reported in Section 6 were run on a slower machine with a 30 minute time limit, although some instances
were run much longer.

225

fiVandegriend & Culberson

100
100
200
300
400
500
1000
1500

% Hamiltonian

80

60

40

20

0
0.6

0.8

1

1.2
1.4
Degree Parameter k

1.6

1.8

2

Figure 1: % of Hamiltonian graphs as a function of graph size and degree parameter for
Gn;m graphs.
0.10 for other ranges of k). We generate 5000 graphs for each data point and execute our
backtrack algorithm once on each graph. This is a grand total of 4.76 million graphs, of
which 1.19 million are of 100 or more vertices.
We use the pruning described in section 3, check for components and articulation points
after the initial pruning, and use iterated restart with a multiplicative factor of 2. We do
not check for components or articulation points during the recursive search.
We expect the phase transition for biconnectivity to be very similar to the phase transition for Hamiltonicity (Cheeseman et al., 1991) and we expect the phase transition for
minimum degree greater than 1 to be almost identical to the phase transition for Hamiltonicity (Bollobas, 1984; Komlos & Szemeredi, 1983). Our experimental results matched
these expectations very closely. For the larger graphs of 100 to 1500 vertices, the percentage
of Hamiltonian graphs is plotted against the degree parameter in Figure 1. We found that
the 50% point at which half the graphs are Hamiltonian occurs when the degree parameter
k  1:08 , 1:10. More interestingly, all the curves pass close to a fixed point near k = 1,
and it seems they are approaching a vertical line at this point. That is, they appear to be
converging on k  1 as a phase transition, precisely as theory predicts.
226

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

n
100 200 300 400 500 1000 1500
Nodes 7:5n 7:0n 3:3n 7:0n 3:4n 3:3n 7:0n
Table 1: Maximum Search Nodes on Gn;m for Large n
All graphs were solved, that is were either determined to be non-Hamiltonian, or a
Hamiltonian cycle was found. We are primarily interested in asymptotic behavior, since
theories concerning the relation of the phase transition to hard regions are necessarily
asymptotic in nature. For graphs of 100 vertices or more, the longest running time was
under 11 seconds, on a graph of 1500 vertices using 10,500 (or 7:0n) search nodes to find a
Hamiltonian cycle.
All of the 549,873 non-Hamiltonian graphs in this range were detected during the initial
pruning of the graph, and thus no search nodes were expanded. Of the 640,127 Hamiltonian
Gn;m graphs, the vast majority ( 629,806 or 98:3%) used only n search nodes, which means
that the algorithm did not need to backtrack at all3. No quadratically hard graphs were
found in this range. Table 4.1 lists the maximum number of search nodes expressed as a
factor of n to illustrate the linearity of the search tree.
These results appear to differ from those of Frank et al. (1998), who found graphs
which took orders of magnitude more search nodes to solve. (Their hardest graph took
over 1 million nodes.) We believe this is due to two factors. Firstly, the algorithm used to
generate the results in their paper did not do an initial check for biconnectivity nor did it
use all of the pruning techniques used in our algorithm. Secondly and more importantly, on
the small random graphs they used ( 30 vertices) the probability of obtaining certain hard
configurations (such as biconnected and non-Hamiltonian or non-biconnected and minimum
degree  2) is much higher than when n is larger, as we discussed in section 2.2.
The experiments on small Gn;m graphs (between 16 and 96 vertices) confirm this conjecture. In this case we do find a small number of quadratically hard graphs, and a few
very hard graphs. We consider for purposes of this paper, that a very hard graph on less
than 100 vertices is any that takes at least 100,000 search nodes to solve. The very hard
graphs from this set of runs are given in Table 4.1.
Note that the very hardest took less than two minutes to solve, making our designation
of \very hard" questionable. Also, note that the smallest graph in this set has 36 vertices,
somewhat larger than the 30 vertex examples found by Frank et al. (1998). This is likely
because we do articulation point checking initially and better pruning. Finally, all of these
very hard graphs are non-Hamiltonian, and all occur in classes that produce less than 50%
Hamiltonian graphs. The hardest Hamiltonian graph in contrast required only 19,318 search
nodes, on a graph of 68 vertices with degree parameter 0.9.
In Figure 2 we plot the number of graphs that are quadratically hard for these small
graphs. For n from 68 to 92, all non-Hamiltonian graphs were detected during initial
pruning. One non-Hamiltonian graph at n = 96 required search (254:1n nodes). Notice
that the number of quadratically hard Hamiltonian graphs is far less than the number of
quadratically hard non-Hamiltonian graphs, and peaks for larger n. This is in accordance
with the discussion of random restarts in Section 3.
3. With 5% error in this measurement, this means that the algorithm might have backtracked over a
maximum of 0:05n search nodes.

227

fiVandegriend & Culberson

Vertices Degree Parameter Seconds Search Nodes Ratio
36
1.11
94.7
1179579 32766.1
40
1.00
36.5
638946 15973.6
40
1.07
18.7
327603 8190.1
44
1.00
12.3
156694 3561.2
44
1.04
20.0
293664 6674.2
48
1.02
91.2
1280135 26669.5
48
1.09
107.0
1243647 25909.3
Table 2: The Hardest Small Graphs

60
non-Ham
Ham

Number of Hard Instances

50

40

30

20

10

0
20

30

40

50
60
70
Number of Vertices

80

90

Figure 2: The Number of Quadratically Hard Graphs for Small n.

228

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

We ran additional tests for n from 32 to 54 in steps of 2, with the degree parameter
ranging from 0.96 to 1.16 with step size 0.01, generating 5000 graphs at each point. In
this case, we invoked articulation point checking at each search node. Again all graphs
were solved without timing out, and some very hard graphs were found, all of them nonHamiltonian. One 50 vertex graph required 9,844,402 search nodes, and required close to
20 minutes to solve. It is unclear whether the extra checking helped; the smallest graph
requiring at least 100,000 nodes had 32 vertices, while the smallest requiring over a million
had 40 vertices. Overall, the results were very similar to the first set of experiments on
small graphs.

4.2 Gn Using Restart
Clearly, the more edges we add to a graph, the more likely it is to be Hamiltonian. It
also seems that once a graph is Hamiltonian, adding more edges makes it less likely to be
hard. In an attempt to find hard graphs for larger n, we modified the Gn;m generator so
that instead of adding a fixed number of edges, it instead added edges until every vertex
has degree at least two, and then stops. In a sense this produces graphs exactly on the
Gn;m phase transition, since a minimum degree of two is the condition that asymptotically
distinguishes Hamiltonian from non-Hamiltonian graphs with high probability. We refer to
this distribution as the Gn model.
Initially we ran 1000 graphs with this generator for n from 100 to 500, but no hard
instances were found. We increased the search to 10,000 graphs at each n, and included a
search at n = 1000. Out of all these graphs, we found one very hard graph on 100 vertices.
Even after a second attempt using more than 26 million search nodes, it was still unsolved.
Doing post-mortem analysis, we checked for cut sets of size 2 and 3 that would leave 3
or 4 (or more) components and found none. We also checked the pruned graph using the
odd degree test mentioned in Section 5.3, but this too failed to show it is non-Hamiltonian.
Finally, we set up our fast machine with unlimited time and no restarts. Three search nodes
and less than 0.1 seconds later it was proven non-Hamiltonian.
Detailed analysis (see the appendix) shows that the graph has a small feature that is
easily detected when one of a few starting points is selected. Because we use an exponentially
growing sequence of searches, we only use a few restarts. In a test of 100 random starts
with a 3 second time limit 7 trials succeeded, using from 2 to 5 search nodes each to prove
the graph non-Hamiltonian.
We also ran 10,000 Gn graphs at each even value of n from 16 to 98. The smallest
instances requiring at least 100,000 search nodes were at n = 50. Only 5 graphs requiring
more than a million nodes were found for n < 100, two at n = 62, one at n = 70 and two at
n = 98. Two of these (one at 62, one at 98) initially timed out, but were solved in second
attempts in about 1/2 hour. Neither was susceptible to an attack by 100 restarts as on the
100 vertex graph.
Table 4.2 shows the number of non-Hamiltonian graphs for each n  100. All of these
except the one mentioned above were detected during initial pruning. The remaining graphs
were all easily shown to be Hamiltonian, with a maximum search ratio of 7.0.
Clearly the probability of non-Hamiltonian graphs drawn from Gn is decreasing with
n. It seems likely that the probability of hard instances is also going to zero.
229

fiVandegriend & Culberson

n
100 200 300 400 500 1000
Non-Ham 154 56 29 20 15
3
Table 3: Number of Non-Hamiltonian Graphs from Gn

n k = 1:00 k = 1:50 k = 2:00

500
1000
1500

0.20
0.43
0.68

0.20
0.50
0.80

0.21
0.60
0.87

Table 4: CPU Seconds per 1000 Search Nodes for Gn;m Graphs

4.3 Gn;m Without Using Restart

We wanted to know how important the restart feature is asymptotically. We ran 1000 Gn;m
graphs for n from 100 to 1500, for each of the parameter settings in the premiere experiment,
but this time using the backtrack algorithm without the iterated restart feature. As before,
all non-Hamiltonian instances were detected during initial pruning. One quadratically hard
Hamiltonian graph was found at n = 300, with degree parameter 1.20, which required
163,888, or 1:82n2 search nodes and took 28.5 seconds. A few other graphs were nearly
quadratic, for example on n = 1500 there were 4 graphs that required 0:15n2 , 0:19n2 ,
0:36n2 and 0:47n2 search nodes. It seems that asymptotically, even in the absence of
iterated restarts, the Gn;m class does not provide hard instances with high probability.

4.4 Gn;m Summary

Based on a set of timing runs, we present in Table 4.4 an indication of how running time per
search node increases with the number of vertices n and degree parameter k. Because the
times are usually so short, we cannot get reliable numbers for n < 500. The times shown
are for the evaluation of 1000 search nodes, and are averaged (total CPU divided by total
nodes searched) over graphs that were solved in less than 1:1n search nodes. For instances
that require significantly more search nodes, the time per 1000 nodes seems to increase
somewhat, but there are so few examples for large n that we are unable to provide exact
estimates. For n = 15004 , the average time per 1000 nodes for instances requiring more
than 2n search nodes is 0.89 seconds at k = 1:00, 1.04 at k = 1:50 and 1.31 at k = 2:00.
Note that this includes at least one instance that took 7n search nodes. This table indicates
that the growth is approximately linear in n + m.
The experimental evidence clearly indicates that Gn;m random graphs are asymptotically
extremely easy everywhere, despite the existence of a phase transition. Our results temper
the findings of the various researchers (Cheeseman et al., 1991; Frank et al., 1998; Frank
& Martel, 1995) studying phase transitions and the Hamiltonian cycle problem. Cheeseman et al.'s explanation of their observed increase in diculty near the phase transition
was that \on the border [between the regions of low and high connectivity] there are many
4. n = 1500 is the only value of n for which we have at least one instance requiring  2n search nodes at
each of the three values of k. The times for 1000 and 1500 come from separate runs on 1000 graphs per
sample point.

230

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

almost Hamiltonian cycles that are quite different from each other . . . and these numerous
local minima make it hard to find a Hamiltonian cycle (if there is one). Any search procedure based on local information will have the same diculty." (Cheeseman et al., 1991).
Unfortunately, while their observations were accurate, their observed hardness was due to
their algorithms and the limited size of the graphs tested, not to intrinsic properties of the
Hamiltonian cycle problem with respect to the phase transition on Gn;m graphs. We have
shown that an ecient backtrack algorithm finds the phase transition region of Gn;m graphs
easy in general.

5. Degreebound Graphs
Intuitively, the reason that it is so hard to generate a hard instance from Gn;m is that by
the time we add enough edges to make the minimum degree two, the rest of the graph is
so dense that finding a Hamiltonian cycle is easy. Alternatively, we see that to create a
non-Hamiltonian property or feature, we must have regions of low degree, while at the same
time meeting the minimal requirements that make the instance hard to solve. This problem
can be characterized as one of high variance of vertex degrees. The only region where we
get even a few hard graphs from Gn;m is when n is small enough that the average degree is
also low.
To avoid the consequences of this degree variation, in this section we use a different
random graph model Gn(d2 = p2 ; d3 = p3 ; : : :) for which n is the number of vertices and
di = pi is the percentage of vertices of degree i. As an example G100 (d2 = 50%; d3 = 50%)
represents the set of graphs of 100 vertices in which 50 are of degree 2 and 50 are of degree
3. We refer to a graph generated under this model as a Degreebound graph. In this paper
we only consider graphs whose vertices are of degree 2 or 3.
It is quite dicult to generate all graphs with a given degree sequence with equal probability (Wormald, 1984). Instead, we adopt two variations which generate graphs by selecting
available edges. In each case each vertex is assigned a free valence equal to the desired final
degree. In version 1 pairs of vertices are selected in random order, and added as edges
if the two vertices have at least one free valence each. This continues until either all free
valences are filled (a successful generation) or all vertex pairs are exhausted (a failure). If
failure occurs, the process is repeated from scratch. Initial tests indicate about 1/3 of the
attempts fail in general. For eciency reasons, in the implementation an array of vertices
holds each vertex once. Pairs of vertices, v; w are selected at random from the array and
if v 6= w, and (v; w) is not already an edge, then (v; w) is added as an edge, and the free
valence of each of v and w is reduced by one. When the free valence of a vertex is zero, the
vertex is deleted from the array. This step is repeated until only a small number (twice the
maximum degree) of vertices remains, and then all possible pairs of the remaining vertices
are generated and tested in random order.
In version 2 an array initially holds each vertex v deg[v] times. Pairs of vertices are
randomly selected, and if not equal and the edge does not exist, then the edge is added, and
the copies of the two vertices are deleted from the array. This is repeated until the array is
empty, or 100 successive attempts have failed to add an edge. The latter case is taken as
failure, and the process is repeated from scratch. This method seldom fails.
231

fiVandegriend & Culberson

Neither of these two methods guarantees a uniform distribution over the graphs of the
given degree sequence. For example, given the degree sequence on five vertices f1; 1; 2; 2; 2g,
there are seven possible (labeled) graphs. One consists of two components, an edge and a
triangle. The other six are all four paths; thus all six are isomorphic to one another. Of the
10! permutations of the pairs of vertices, 564,480 generate the graph on two components,
while for each four path there are 322,560 distinct permutations. The remaining permutations (31.2 %) do not yield a legal graph. Thus, the first graph is 1.75 times as likely as
any of the other six. Of course, a four path (counting all isomorphic graphs) is 3.428 times
as likely as the two-component graph.
On the other hand, a version 2 test program (not our generator which prohibits degree
one vertices) consistently generated the first graph about 8%{10% more often than any of
the others, based on several million random trials.

5.1 Experimental Results on Degreebound Graphs

We test graphs of 100 : : : 500 vertices (step size 100) 1000 and 1500 vertices with the mean
degree varying from 2:6 : : : 3:0 (step size of 0.01 from 2.75 to 2.95, step size of 0.05 elsewhere).
We generate 1000 graphs for each data point, execute our algorithm once on each graph,
and collect the results. This test was repeated for each of the two versions.
Figure 3 shows the percentage of graphs which are Hamiltonian as the mean degree and
graph size varies5 . There is a clear transition from a mean degree of 2.6 (near 0% chance of
a Hamiltonian cycle) to a mean degree of 3 (for which Robinson and Wormald, 1994 predict
an almost 100% chance of a Hamiltonian cycle on uniformly distributed graphs). For a
phase transition, we would expect the slope to grow steeper as the graph size increases.
Figure 3 shows this increase in steepness.
Note that the double points on the curve for n = 100 are due to unavoidable discretization. Since the total degree of a graph must be even, when the generators detect that the
total degree specified is odd, one of the minimum degree vertices is selected and its degree
incremented. Thus, for example, whether the fraction of degree 3 vertices specified is 0.81 or
0.82, the number of degree three vertices is 82. Discretization effects also occur for n = 300,
500 and 1500, but with lessened impact.
In Table 5.1 we summarize the observed hard instances from these graphs. We note
that several instances exceeded our time bounds, and although these are certainly at least
quadratically hard, they are not included in the quadratically hard instances. The frequency
of hard instances appears to be decreasing with n on these graphs. In particular there are
no quadratically hard non-Hamiltonian instances over 1000 vertices, except those that are
too hard to solve with our program.
Interestingly, there turns out to be an O(n + m) time test which shows that most of
the unresolved instances are non-Hamiltonian. This test is described briey in Section 5.3.
We implemented the test as a separate program and tested each of the unresolved graphs,
with the results indicated in the last column of Table 5.1. The remaining five graphs
remain unresolved. If this test were included in the initial pruning of our program, then
the instances enumerated in the last column of Table 5.1 would all be solved (proven nonHamiltonian) without search.
5. For these graphs, the mean degree is 2.0 plus the fraction of degree 3 vertices.

232

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

100
Version 1
n = 100
200
300
400
500
1000
1500

% Hamiltonian

80

60

40

20

0
60

65

70

75
80
85
% Vertices of Degree 3

90

95

100

100
Version 2
n = 100
200
300
400
500
1000
1500

% Hamiltonian

80

60

40

20

0
60

65

70

75
80
85
% Vertices of Degree 3

90

95

Figure 3: % of Hamiltonian graphs for Degreebound Graphs.
233

100

fiVandegriend & Culberson

Version 1
Number of Quadratically Hard Timed Out
Vertices No HC
HC
Total No HC
100
5
0
0
0
200
18
0
3
3
300
8
0
11
10
400
1
0
14
14
500
0
0
14
14
1000
0
0
7
7
1500
0
1
6
6
Version 2
Number of Quadratically Hard Timed Out
Vertices No HC
HC
Total No HC
100
5
0
0
0
200
9
0
6
5
300
10
0
13
13
400
3
0
11
11
500
1
1
10
9
1000
0
1
6
4
1500
0
0
6
6
Table 5: Number of Hard Graphs for Degreebound Graphs
Thus, although these classes may provide a small rate of hard instances for our current
program, it is not clear they are even minimally hard. Furthermore, it appears there exist
simple improvements to our program that would eliminate most of these hard instances.
In Figure 4 we illustrate the distribution of the graphs that timed out. The other
quadratically hard graphs had similar distributions. About all that can be concluded is
that the hard instances seem to be distributed over a mean degree range from 2.78 to 2.94.
The backtrack program is a little faster on Degreebound graphs than on Gn;m graphs,
as we would expect given fewer total edges. For 1500 vertices, the times per 1000 search
nodes ranged from 0.27 seconds for the easiest (no backtrack) instances to 0.56 seconds for
the harder ones.

5.2 Analysis of Degreebound Graphs

An analysis of the Degreebound graph class led us to conjecture that the prime factor
determining the Hamiltonicity of a graph was whether or not the graph had a degree 3
vertex with 3 neighbours of degree 2. We label this a 3D2 configuration (or a 3D2 event).
A graph with a 3D2 configuration is non-Hamiltonian. The following informal analysis
provides evidence for our conjecture.
Let E (n; ) represent the expected number of 3D2 configurations in a graph with n
vertices. Let D2 = n be the number of degree 2 vertices and D3 = (1 , )n the number of
D3 = 2n+3n(1,) = 3 , . Assuming
degree 3 vertices. Note that the mean degree d = 2D2 +3
n
n
equal probability of all combinations,
234

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

Number Failed
Version 1

5
4
3
2

95

1
90
200

85 % Degree 3

500
1000
Number of Vertices

80
1500

Number Failed
Version 2

5
4
3
2

95

1
90
200

85 % Degree 3

500
1000
Number of Vertices

80
1500

Figure 4: Distribution of Timed Out Instances for Degreebound graphs
235

fiVandegriend & Culberson

# of Mean Degree for 50% HC Point
Vertices Experimental
Theoretical
100
2.78
2.78
200
2.81
2.82
300
2.83
2.85
400
2.84
2.86
500
2.85
2.87
1000
2.88
2.90
1500
2.90
2.91
Table 6: Experimental and approximate theoretical values for the location of the 50%
Hamiltonian point for Degreebound graphs of various sizes.

E (n; ) = D3

,D2 

,n,13
3

, 

n(1 , ) n
)(n)(n , 1)(n , 2)
= ,n,1 3 = n(1(,n ,
1)(n , 2)(n , 3)
3

We restrict ourselves to the asymptotic case (n ! 1) which gives us

E (n; )  n(1 ,n3)(n)  n(1 , )3
3

When E (n; ) ! 0, the probability of having configuration 3D2 approaches 0. We
want to find  for which n(1 , )3 ! 0 as n ! 1. This occurs when  = o(n,1=3 ).
Since a Hamiltonian cycle cannot exist if E (3D2) > 0, this tells us that the phase transition
asymptotically occurs when the mean degree equals 3. Asymptotically, Degreebound graphs
with d < 3 are expected to be non-Hamiltonian while Degreebound graphs with d > 3
are expected to be Hamiltonian (ignoring other conditions). This agrees with results of
Robinson and Wormald (1994) who proved that almost all 3-regular graphs are Hamiltonian.
If we let  = n,1=3 this gives us E (n; )  1. Substituting this equation in our expression
for mean degree gives us d = 3 , n,1=3 . Table 5.2 lists mean degrees for different values of n
using this formula along with our experimentally determined values for the point where 50%
of the graphs are Hamiltonian. They are remarkably similar. This suggests that the 3D2
configuration is the major determinator of whether a Degreebound graph will be Hamiltonian or not. Minor effects (which we have ignored) come from propagation of deleted edges
while pruning and other less probable cases such as those mentioned in Section 5.3. Since
the 3D2 configuration is detected by our algorithm before the search is started, this also
implies that the phase transition will be easy for our algorithm, since most non-Hamiltonian
graphs are instantly detected. This matches our experimental observations.

5.3 A Non-Hamiltonicity Test for Sparse Graphs

While preparing the final version of this paper, we observed that in the 3D2 configuration
we could replace the vertex of degree three with a component of several vertices. In general,
236

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

if there are three vertices of degree two that form a minimal cut then the graph is nonHamiltonian. In fact, we can replace the three vertices by a minimal cut of any odd number
c of degree 2 vertices, and the claim of non-Hamiltonicity remains true.
Checking all possible subsets of size c would be very expensive, but fortunately there is
an even more general condition that includes all of these as special cases and can be tested in
linear (i.e O(n + m)) time. Let F be a set of edges that are forced to be in any Hamiltonian
cycle if one exists. For example, edges incident on a vertex of degree two are forced. Let
G0 = G , F be the graph formed by deleting the forced edges from G. Let C1 : : : Ch be
components of G0, and define the forced degree of component Ci to be the number of end
points of forced edges (from F ) in Ci . If any component has an odd forced degree, then G
is non-Hamiltonian.
The proof of correctness of this test is simple. Observe that if there is a Hamiltonian
cycle in G then while traversing the cycle each time we enter a component, there must be a
corresponding exit. Since the forced edges act as a cut set (that separates the components),
they are the only edges available to act as entries and exits to a component. All forced edges
must be used. Therefore, if there is a Hamiltonian cycle there must be an even number of
forced edges connecting any component to other components, each contributing one to the
forced degree of the component. Each forced edge internal to (with both end points in) a
component contributes two to the forced degree, so if there is a Hamiltonian cycle the total
forced degree of each component must be even.
To obtain the results in the last column of Table 5.1, we first did the initial pruning, and
then applied the test to the pruned graphs, using only the forced edges incident on degree
two vertices.

6. Generalized Knight's Circuit Graphs
In this section we examine a graph class based upon the generalized knight's circuit problem
in which the size of the knight's move is allowed to vary along with the size of the (rectangular) board. An instance of the generalized knight's circuit problem is a graph defined by
the 4-tuple (A; B ) , n  m where A; B is the size of the knight's move and n; m is the size
of the board. The vertices of the graph correspond to the cells, and thus jV j = nm. Two
vertices are connected by an edge if and only if it is possible to move from one vertex to
the other by moving A steps along one axis and B along the other. (See Vandegriend, 1998
for more information about this problem.)
For this graph class there is no easy way to define phase transitions since there is
no clear parameter which separates the Hamiltonian graphs from the non-Hamiltonian
graphs (although Vandegriend, 1998 shows that there are ways of identifying groups of
non-Hamiltonian graphs). Thus to find hard graphs, we look for graphs which take a significant amount of time to solve relative to their size. We perform 1 trial per graph (problem
instance) and report the ratio of search nodes to number of vertices.
We examined a total of 300 generalized knight's circuit graphs over ranges of A; B; n; m
(Specific A; B; n triplets with m allowed to vary, for A + B  9, n  13, m  60.) They
ranged in size from 80 to 390 vertices. Of the 300 instances examined, 121 graphs (40 %)
were found to be Hamiltonian and 141 graphs (47 %) were found to be non-Hamiltonian.
237

fiVandegriend & Culberson

search nodes # of trials % of trials

2n
1
0.8
5n
43
35.5
10n
37
30.6
20n
11
9.1
50n
8
6.6
100n
8
6.6
200n
2
1.7
500n
5
4.1
1000n
2
1.7
2000n
1
0.8
5000n
1
0.8
10000n
1
0.8
20000n
0
0.0
50000n
1
0.8
Table 7: Histogram of the search node ratio of our backtrack algorithm on 121 Hamiltonian
generalized knight's circuit instances.
For the remaining 38 graphs (13 %) our backtrack algorithm failed (reached the 30 minute
time limit), which implies these graphs are very hard for our backtrack algorithm.
A majority (91%) of the non-Hamiltonian graphs were solved without any search. However, a significant number of the remaining graphs took many search nodes to solve. 9
graphs (6.4%) took more than 10n nodes and 7 graphs (5.0%) took more than 100n nodes.
The hardest graph took  11276n search nodes (n = 324). So while the majority of the
non-Hamiltonian graphs were easy, a significant percentage of these generalized knight's
circuit graphs were quite hard for our algorithm.
A larger variance in hardness was observed with the Hamiltonian graphs. Table 6 shows
the distribution with respect to the number of search nodes required. Unlike Gn;m and
Degreebound graphs, these graphs could not be solved in only n search nodes. Almost
all the graphs required at least 2n search nodes. 33% of the graphs required at least 10n
nodes, 11% required at least 100n nodes and the hardest graph required  34208n nodes
(n = 198).

7. A Hard Constructed Graph Class
It is worthwhile when designing an algorithm to determine under what conditions and how
frequently it might fail to perform and just how badly it might do. The measure can be
in terms of how bad an approximation is, or how long an exact algorithm may take in the
worst case. There is a long tradition of designing instance sets that foil specific combinatorial algorithms (Johnson, 1974; Mitchem, 1976; Olariu & Randall, 1989; Spinrad & Vijayan,
1985). Other special classes are intended to be more general, and are frequently based on
certain features or constructs together with some randomization to hide the features (Culberson & Luo, 1996; Brockington & Culberson, 1996; Kask & Dechter, 1995; Bayardo Jr. &
238

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

Schrag, 1996). The Gn;m class is frequently used to study graph algorithms over all possible
graphs.
In this section we consider a special construction for a Hamiltonian graph which is
extremely hard (exponential increase in diculty with size) for our backtrack algorithm.
It consists mostly of special constructs tied together with some randomly chosen edges. It
bears some resemblance to graphs such as the Meredith graph (Bondy & Murty, 1976) used
to disprove certain theoretical conjectures. This graph remains dicult when we vary the
neighbour selection heuristic or pruning techniques used by our backtrack algorithm. The
graph we construct we refer to as the Interconnected-Cutset (ICCS ) graph.
Our class is intended merely to show that exponentially hard classes clearly exist for
our algorithm, and many other backtrack algorithms using similar approaches. We do not
claim our graphs are intrinsically hard, as there is a polynomial time algorithm that will
solve this particular class.
The basic concept we use in constructing these graphs is the non-Hamiltonian edge,
which we define as an edge which cannot be in any possible Hamiltonian cycle. Note that
since the graphs are Hamiltonian, each vertex must be incident on at least two edges which
are not non-Hamiltonian. Our goal is to force the algorithm to choose a non-Hamiltonian
edge at some point. The key observation is that once such an edge is chosen, the algorithm
must backtrack to fix that choice. With multiples of these bad choices, after backtracking
to fix the most recent bad choice, the algorithm must eventually backtrack to an earlier
point to fix a less recent bad choice, which means the more recent choice must be redone,
with the algorithm making the bad choice again. The amount of work performed by the
algorithm is at least exponential in the number of bad choices. See Vandegriend (1998) for
more details.
The ICCS graph is composed of k identical subgraphs ICCSS arranged in a circle.
To force the desired cycle we have a degree 2 vertex between each subgraph. Since each
subgraph has a Hamiltonian path between the connecting vertices, the ICCS graph is
Hamiltonian. Due to the construction of the ICCS subgraph, extra non-Hamiltonian edges
can be added between different subgraphs. These edges help prevent components from
forming during the search, which greatly reduces the effectiveness of the component checking
search pruning. See Figure 5. Heavy lines are forced edges that must be in any Hamiltonian
cycle.
Figure 6 contains a sample ICCS subgraph. Non-Hamiltonian edges are denoted by
dashed lines, and forced edges are denoted by heavy lines.
To see that the dashed lines cannot be part of any Hamiltonian cycle observe that
any path through the ICCSS must enter and exit on an SC vertex, and between any two
SC vertices in sequence the path can visit at most one SI vertex. Thus, each such path
uses at least one more vertex from SC than from SI . Since initially jSC j = jSI j + 1, any
Hamiltonian cycle can enter and exit the ICCSS only once, and must alternate between
SC and SI vertices. Since the ST vertices only have one edge leading to an SI vertex,
these edges are forced. This also allows us to interconnect subgraphs without adding new
Hamiltonian cycles by connecting vertices of SC of two different subgraphs (since these
additional edges are all non-Hamiltonian edges). By interconnecting the subgraphs in this
fashion, we strongly reduce the effectiveness of checking for components or cut-points during
the search. In the current implementation, for each vertex in each SC we randomly choose a
239

fiVandegriend & Culberson

ICCSS

ICCSS

ICCSS

ICCSS

Figure 5: A sample ICCS graph.

SI

SD
SC
ST

ST

to

SC vertices

in other subgraphs

connecting edges to adjacent subgraphs

Figure 6: A sample ICCS subgraph ICCSS .
240

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

vertex in another SC and add the edge. Thus, the average number of such edges per vertex
is a little less than two, since some edges may be repeated.
One additional design element was added to handle various degree selection heuristics
that our algorithm could use. At each stage in the search, the neighbours of the current
endpoint of the partial path are arranged in a list to determine the order in which they will
be chosen by our backtrack algorithm. There are 3 main heuristics: sorting the list to visit
lower degree neighbours first, sorting to visit higher degree neighbours first, and visiting in
random order. (Our backtrack algorithm normally uses the lower degree first heuristic.)
The SD vertex in the ICCS subgraph is used to fool the low degree first heuristic. The
SD vertex is only incident to the two ST vertices and to two vertices in SI , which makes
it degree 4. When the algorithm enters a subgraph from the degree 2 connecting vertex, it
reaches one of the ST vertices. From the ST vertex, the choices are the SD vertex (degree
4) and the one SI vertex (degree jSC j , 2, because it is not connected to the SD vertex and
the other ST vertex). If jSC j > 6 then the SD vertex will have a lower degree and thus will
be chosen first.
The high degree first heuristic avoids following the edge from the ST vertex to the SD
vertex, and instead goes to the SI vertex. From there it chooses one of the SC vertices (not
including SD or the other ST vertex, which are not adjacent). From this point, its choice is
one of the SI vertices (maximum degree = jSC j , 2) or one of the SC vertices in a different
subgraph (degree  jSC j if that subgraph has not yet been visited). Since the SC vertex
normally will have a higher degree, the algorithm will follow the non-Hamiltonian edge to
that vertex.
If the next neighbour is chosen at random, then from a ST vertex, the algorithm has a
50% chance of making the wrong choice. Similarly, at each SC vertex the algorithm has a
small chance of following a non-Hamiltonian edge. As the number of subgraphs is increased,
the probability of the algorithm making all the right choices rapidly approaches 0.
Another reason why the ICCS subgraph is expected to be hard for a backtrack algorithm
is that there are many possible paths between the two ST vertices. If a non-Hamiltonian
edge has previously been chosen, then the backtrack algorithm will try all the different
combinations of paths (and fail to form a Hamiltonian cycle) before it backtracks to the
bad choice.
We performed experiments on various ICCS graphs. We varied the number of subgraphs
from 1 to 4, and varied the independent set size (jSI j) from 6 to 8. We used our backtrack
algorithm as specified in Section 3 with the addition of checking for components and cutpoints during the search. We executed our algorithm 5 times per graph. Our results are
listed in Table 7 for the low degree first heuristic. Our experiments using the other degree
selection heuristics exhibited similar results.
We have also performed similar experiments using a randomized heuristic algorithm
(Frieze, 1988; Posa, 1976). Due to the significant difference in operation between this
algorithm and backtrack algorithms, it easily solved these small ICCS graphs. However its
performance rapidly decreased as the graphs were increased in size.
The average degree of ICCS graphs with more than one subgraph lies within the following range:
jSI j , 2:5 + jS 9j:5+ 1  d  jSI j , 2 + jS j8+ 1
I

I

241

fiVandegriend & Culberson

n #S jSI j

14
28
42
56
16
32
48
18
36
54

1
2
3
4
1
2
3
1
2
3

6
6
6
6
7
7
7
8
8
8

Min
Median
Max
14
14
210
606
616
3,777
10,467
47,328
112,795
6,538,842 32,578,160 36,300,827
16
48
112
13,056
21,797
70,949
1,350,084 5,247,287 8,027,520
18
54
270
283,164
430,620
750,211
> 1:2  108

Table 8: Search nodes required by our backtrack algorithm on ICCS graphs.
From this formula we see that as the size of each independent set is increased, the mean
degree increases linearly. However, as the number of subgraphs is increased, the mean
degree remains constant. The ICCS graphs remain hard over a very wide range of mean
degrees (from O(1) to O(n)). Therefore the average degree in this case is not a relevant
parameter for determining hardness.

8. Conclusions and Future Work
Our backtrack Hamiltonian cycle algorithm found Gn;m graphs easy to solve, along with
a majority of Degreebound graphs. We have also performed similar experiments (Vandegriend, 1998) using a randomized heuristic algorithm (Frieze, 1988; Posa, 1976) which had
a high success rate on Gn;m graphs, less so on Degreebound graphs. More interestingly, the
existence of a phase transition for both problems did not clearly correspond to a high frequency of dicult instances. We suspect that other properties play a more important role
than does the average degree. This is supported by our results on generalized knight's circuit
graphs, which are all highly regular (with many symmetries), and for which the majority
have average degrees between 4 and 8, compared to a mean degree  3 on Degreebound
graphs.
These results should not be surprising, since it has been shown that asymptotically for
randomly generated graphs, when the edge is added that makes the last vertex degree 2,
then with high probability the graph is Hamiltonian (Bollobas, 1984). In addition, ecient
algorithms have been shown to solve these instances in polynomial time with high probability (Bollobas et al., 1987). Since vertices of degree less than 2 are a trivially detectable
counter-indicator, it is hardly surprising that asymptotically determining Hamiltonicity of
graphs in Gn;m is easy.
We also observe that the performance of our backtrack algorithm can widely vary for
a single graph due to the selection of the initial vertex. Multiple restarts of our backtrack
algorithm after a time limit was reached often resulted in superior performance. We suggest
a little randomization of the algorithm be used while empirically identifying intrinsically
hard random instances of any problem.
242

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

Acknowledgements
This research was supported by Natural Sciences and Engineering Research Council Grant
No. OGP8053.

References

Angluin, D., & Valiant, L. G. (1979). Fast probabilistic algorithms for Hamiltonian circuits
and matchings. J. Comput. System Sci., 18 (2), 155{193.
Baker, A. (1995). Intelligent Backtracking on Constraint Satisfaction Problems. Ph.D.
thesis, University of Oregon.
Bayardo Jr., R. J., & Schrag, R. (1996). Using csp look-back techniques to solve exceptionally hard sat instances. In Proc. of the Second Int'l Conf. on Principles and Practice of
Constraint Programming, Vol. 1118 of Lecture Notes in Computer Science, pp. 46{60.
Bollobas, B., Fenner, T. I., & Frieze, A. M. (1987). An algorithm for finding Hamilton
paths and cycles in random graphs. Combinatorica, 7 (4), 327{341.
Bollobas, B. (1984). The evolution of sparse graphs. In Bollobas, B. (Ed.), Graph Theory
and Combinatorics, pp. 35{57. Academic Press, Toronto.
Bondy, J. A., & Murty, U. S. R. (1976). Graph Theory with Applications. Elsevier, Amsterdam.
Brockington, M., & Culberson, J. C. (1996). Camouaging independent sets in quasirandom graphs.. In Johnson, & Trick (Johnson & Trick, 1996), pp. 75{88.
Broder, A. Z., Frieze, A. M., & Shamir, E. (1994). Finding hidden Hamiltonian cycles.
Random Structures and Algorithms, 5 (3), 395{410.
Cheeseman, P., Kanefsky, B., & Taylor, W. M. (1991). Where the really hard problems are.
In Mylopoulos, J., & Reiter, R. (Eds.), IJCAI-91: Proceedings of the Twelfth International Conference on Artificial Intelligence, pp. 331{337 San Mateo, CA. Morgan
Kaufmann.
Culberson, J. C., & Luo, F. (1996). Exploring the k{colorable landscape with iterated
greedy.. In Johnson, & Trick (Johnson & Trick, 1996), pp. 245{284.
Frank, J., Gent, I. P., & Walsh, T. (1998). Asymptotic and finite size parameters for phase
transitions: Hamiltonian circuit as a case study. Information Processing Letters, In
press.
Frank, J., & Martel, C. (1995). Phase transitions in the properties of random graphs. In
CP'95 Workshop: Studying and Solving Really Hard Problems, pp. 62{69.
Frieze, A. M. (1988). Finding Hamilton cycles in sparse random graphs. Journal of Combinational Theory, Series B, 44, 230{250.
243

fiVandegriend & Culberson

Gomes, C. P., Selman, B., & Kautz, H. (1998). Boosting combinatorial search through
randomization. In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98), pp. 431{437. AAAI Press/ The MIT Press.
Hogg, T. (1998). Which search problems are random?. In Proceedings of the Fifteenth
National Conference on Artificial Intelligence (AAAI-98), pp. 438{443. AAAI Press/
The MIT Press.
Hogg, T., & Williams, C. P. (1994). The hardest constraint problems: A double phase
transition. Artificial Intelligence, 69, 359{377.
Johnson, D. S. (1974). Approximation algorithms for combinatorial problems. Journal of
Computer and System Sciences, 9, 256{278.
Johnson, D. S., Aragon, C. R., McGeoch, L. A., & Schevon, C. (1991). Optimization by
simulated annealing: An experimental evaluation; part II, graph coloring and number
partitioning. Operations Research, 39 (3), 378{406.
Johnson, D. S., & Trick, M. A. (Eds.). (1996). Cliques, Coloring, and Satisfiability: Second
DIMACS Implementation Challenge (1993), Vol. 26. American Mathematical Society.
Kask, K., & Dechter, R. (1995). GSAT and local consistency. In Mellish, C. S. (Ed.),
IJCAI-95 : Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence, pp. 616{622 San Mateo, CA. Morgan Kaufmann.
Kocay, W. (1992). An extension of the multi-path algorithm for finding Hamilton cycles.
Discrete Mathematics, 101, 171{188.
Komlos, M., & Szemeredi, E. (1983). Limit distribution for the existence of a Hamilton
cycle in a random graph. Discrete Mathematics, 43, 55{63.
Langley, P. (1992). Systematic and nonsystematic search strategies. In Artificial Intelligent
Planning Systems: Proceedings of the First International Conference, pp. 145{152.
Martello, S. (1983). Algorithm 595: An enumerative algorithm for finding Hamiltonian
circuits in a directed graph. ACM Transactions on Mathematical Software, 9 (1),
131{138.
Mitchem, J. (1976). On various algorithms for estimating the chromatic number of a graph.
The Computer Journal, 19, 182{183.
Motwani, R., & Raghavan, P. (1995). Randomized Algorithms. Cambridge University Press,
New York.
Olariu, S., & Randall, J. (1989). Welsh-Powell opposition graphs. Information Processing
Letters, 31 (1), 43{46.
Palmer, E. M. (1985). Graphical Evolution: an introduction to the theory of random graphs.
John Wiley & Sons, Toronto.
Posa, L. (1976). Hamiltonian circuits in random graphs. Discrete Mathematics, 14, 359{364.
244

fiThe Gn;m Phase Transition is Not Hard for the Hamiltonian Cycle Problem

Robinson, R. W., & Wormald, N. C. (1994). Almost all regular graphs are Hamiltonian.
Random Structures and Algorithms, 5 (2), 363{374.
Selman, B., Levesque, H., & Mitchell, D. (1992). A new method for solving hard satisfiability
problems. In Proceedings of the Tenth National Conference on Artificial Intelligence
(AAAI-92), San Jose, CA, pp. 440{446.
Shufelt, J. A., & Berliner, H. J. (1994). Generating Hamiltonian circuits without backtracking from errors. Theoretical Computer Science, 132, 347{375.
Spinrad, J. P., & Vijayan, G. (1985). Worst case analysis of a graph coloring algorithm.
Discrete Applied Mathematics, 12 (1), 89{92.
Vandegriend, B. (1998). Finding Hamiltonian cycles: Algorithms, graphs and performance.
Master's thesis, Department of Computing Science, University of Alberta. Online at
\http://www.cs.ualberta.ca/~basil/".
Wormald, N. C. (1984). Generating random regular graphs. Journal of Algorithms, 5,
247{280.

245

fiJournal of Artificial Intelligence Research 9 (1998) 463-506

Submitted 4/98; published 12/98

A Temporal Description Logic
for Reasoning about Actions and Plans
Alessandro Artale

artale@irst.itc.it

ITC-IRST, Cognitive and Communication Technologies Division
I-38050 Povo TN, Italy

Enrico Franconi

Department of Computer Science, University of Manchester
Manchester M13 9PL, UK

franconi@cs.man.ac.uk

Abstract

A class of interval-based temporal languages for uniformly representing and reasoning
about actions and plans is presented. Actions are represented by describing what is true
while the action itself is occurring, and plans are constructed by temporally relating actions
and world states. The temporal languages are members of the family of Description Logics,
which are characterized by high expressivity combined with good computational properties.
The subsumption problem for a class of temporal Description Logics is investigated and
sound and complete decision procedures are given. The basic language TL-F is considered
first: it is the composition of a temporal logic TL { able to express interval temporal
networks { together with the non-temporal logic F { a Feature Description Logic. It is
proven that subsumption in this language is an NP-complete problem. Then it is shown
how to reason with the more expressive languages TLU -FU and TL-ALCF . The former
adds disjunction both at the temporal and non-temporal sides of the language, the latter
extends the non-temporal side with set-valued features (i.e., roles) and a propositionally
complete language.

1. Introduction
The representation of temporal knowledge has received considerable attention in the Artificial Intelligence community in an attempt to extend existing knowledge representation
systems to deal with actions and change. At the same time, many logic-based formalisms
were developed and analyzed by logicians and philosophers for the same purposes. In this
class of logical formalisms, properties such as expressive power and computability have been
studied as regards typical problems involving events and actions.
This paper analyzes from a theoretical point of view the logical and computational
properties of a knowledge representation system that allows us to deal with time, actions
and plans in a uniform way. The most common approaches to model actions are based
on the notion of state change { e.g., the formal models based on the original situation
calculus (McCarthy & Hayes, 1969; Sandewall & Shoham, 1994) or the Strips-like planning
systems (Fikes & Nilsson, 1971; Lifschitz, 1987) { in which actions are generally considered
instantaneous and defined as functions from one state to another by means of pre- and
post-conditions. Here, an explicit notion of time is introduced in the modeling language
and actions are defined as occurring over time intervals, following the Allen proposal (Allen,
c 1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiArtale & Franconi

1991). In this formalism an action is represented by describing the time course of the
world while the action occurs. Concurrent or overlapping actions are allowed: effects of
overlapping actions can be different from the sum of their individual effects; effects may
not directly follow the action but more complex temporal relations may hold. For instance,
consider the motion of a pointer on a screen driven by a mouse: the pointer moves because
of the movement of the device on the pad { there is a cause-effect relation { but the two
events are contemporary, in the common-sense notion of the word.
A class of interval temporal logics is studied based on Description Logics and inspired by
the works of Schmiedel (1990) and of Weida and Litman (1992). In this class of formalisms
a state describes a collection of properties of the world holding at a certain time. Actions are
represented through temporal constraints on world states, which pertain to the action itself.
Plans are built by temporally relating actions and states. To represent the temporal dimension classical Description Logics are extended with temporal constructors; thus a uniform
representation for states, actions and plans is provided. Furthermore, the distinction made
by Description Logics between the terminological and assertional aspects of the knowledge
allows us to describe actions and plans both at an abstract level (action/plan types) and
at an instance level (individual actions and plans). In this environment, the subsumption
calculus is the main inference tool for managing collections of action and plan types. Action
and plan types can be organized in a subsumption-based taxonomy, which plays the role
of an action/plan library to be used for the tasks known in the literature as plan retrieval
and individual plan recognition (Kautz, 1991). A refinement of the plan recognition notion is proposed, by splitting it into the different tasks of plan description classification {
involving a plan type { and specific plan recognition with respect to a plan description {
involving an individual plan. According to the latter reasoning task, the system is able to
recognize which type of action/plan has taken place at a certain time interval, given a set
of observations of the world.
Advantages of using Description Logics are their high expressivity combined with desirable computational properties { such as decidability, soundness and completeness of deduction procedures (Buchheit, Donini, & Schaerf, 1993; Schaerf, 1994; Donini, Lenzerini,
Nardi, & Schaerf, 1994; Donini, Lenzerini, Nardi, & Nutt, 1995). The main purpose of this
work is to investigate a class of decidable temporal Description Logics, and to provide complete algorithms for computing subsumption. To this aim, we start with TL-F , a language
being the composition of a temporal logic TL { able to express interval temporal networks {
together with the non-temporal Description Logic F { a Feature Description Logic (Smolka,
1992). It turns out that subsumption for TL-F is an NP-complete problem. Then, we show
how to reason with more expressive languages: TLU -FU , which adds disjunction both at
the temporal and non-temporal sides of the language, and TL-ALCF , which extends the
non-temporal side with set-valued features (i.e., roles) and a propositionally complete Description Logic (Hollunder & Nutt, 1990). In both cases we show that reasoning is decidable
and we supply sound and complete procedures for computing subsumption.
The paper is organized as follows. After introducing the main features of Description
Logics in Section 2, Section 3 organizes the intuitions underlying our proposal. The technical
bases are introduced by briey overviewing the temporal extensions of Description Logics
relevant for this approach { together with the inter-relationships with the interval temporal
modal logic { specifically intended for time and action representation and reasoning. The
464

fiA Temporal Description Logic for Reasoning about Actions and Plans

basic feature temporal language (TL-F ) is introduced in Section 4. The language syntax is
first described in Section 4.1, together with a worked out example illustrating the informal
meaning of temporal expressions. Section 4.2 reveals the model theoretic semantics of TL-F ,
together with a formal definition of the subsumption and instance recognition problems.
Section 5 shows that the temporal language is suitable for action and plan representation
and reasoning: the well known cooking domain and blocks world domain are taken into
consideration. The sound and complete calculus for the feature temporal language TL-F
is presented in details in Section 6. A proof that subsumption for TL-F is an NP-complete
problem is included. The calculus for TL-F forms the basic reasoning procedure that
can be adapted to deal with logics having an extended propositional part. An algorithm
for checking subsumption in presence of disjunction (TLU -FU ) is devised in Section 7.1;
while in Section 7.2 the non-temporal part of the language is extended with roles and
full propositional calculus (TL-ALCF ). In both cases, the subsumption problem is still
decidable. Operators for homogeneity and persistence are presented in Section 8 for an
adequate representation of world states. In particular, a possible solution to the frame
problem, i.e., the problem to compute what remains unchanged by an action, is suggested.
Section 9 surveys the whole spectrum of extensions of Description Logics for representing
and reasoning with time and action. This Section is concluded by a comparison with State
Change based approaches by briey illustrating the effort made in the situation calculus
area to temporally extend this class of formalisms. Section 10 concludes the paper.

2. Description Logics

Description Logics1 are formalisms designed for a logical reconstruction of representation tools such as frames, semantic networks, object-oriented and semantic data models
{ see (Calvanese, Lenzerini, & Nardi, 1994) for a survey. Nowadays, Description Logics
are also considered the most important unifying formalism for the many object-centered
representation languages used in areas other than Knowledge Representation. Important
characteristics of Description Logics are high expressivity together with decidability, which
guarantee the existence of reasoning algorithms that always terminate with the correct
answers.
This Section gives a brief introduction to a basic Description Logic, which will serve as
the basic representation language for our proposal. As for the formal apparatus, the formalism introduced by (Schmidt-Schau & Smolka, 1991) and further elaborated by (Donini,
Hollunder, Lenzerini, Spaccamela, Nardi, & Nutt, 1992; Donini et al., 1994, 1995; Buchheit
et al., 1993; De Giacomo & Lenzerini, 1995, 1996) is followed: in this way, Description
Logics are considered as a structured fragment of predicate logic. ALC (Schmidt-Schau &
Smolka, 1991) is the minimal Description Logic including full negation and disjunction {
i.e., propositional calculus, and it is a notational variant of the propositional modal logic
K(m) (Halpern & Moses, 1985; Schild, 1991).
The basic types of a Description Logic are concepts, roles, features, and individuals. A
concept is a description gathering the common properties among a collection of individuals;
from a logical point of view it is a unary predicate ranging over the domain of individu1. Description Logics have been also called Frame-Based Description Languages, Term Subsumption Languages, Terminological Logics, Taxonomic Logics, Concept Languages or KL-One-like languages.

465

fiArtale & Franconi

C; D ! A j

>j
?j
:C j
C uD j
C tD j
8P .C j
9P .C j
p:Cj
p#qj
p"qj
p"
p; q ! f j
pq

(atomic concept)
(top)
(bottom)
(complement)
(conjunction)
(disjunction)
(universal quantifier)
(existential quantifier)
(selection)
(agreement)
(disagreement)
(undefinedness)
(atomic feature)
(path)

Figure 1: Syntax rules for the ALCF Description Logic.
als. Properties are represented either by means of roles { which are interpreted as binary
relations associating to individuals of a given class values for that property { or by means
of features { which are interpreted as functions associating to individuals of a given class
a single value for that property. The language ALCF , extending ALC with features (i.e.,
functional roles) is considered. By the syntax rules of Figure 1, ALCF concepts (denoted by
the letters C and D) are built out of atomic concepts (denoted by the letter A), atomic roles
(denoted by the letter P ), and atomic features (denoted by the letter f ). The syntax rules
are expressed following the tradition of Description Logics (Baader, Burckert, Heinsohn,
Hollunder, Muller, Nebel, Nutt, & Profitlich, 1990).
The meaning of concept expressions is defined as sets of individuals, as for unary predicates, and the meaning of roles as sets of pairs of individuals, as for binary predicates.
Formally, an interpretation is a pair I = (I ; I ) consisting of a set I of individuals (the
domain of I ) and a function I (the interpretation function of I ) mapping every concept
to a subset of I , every role to a subset of I  I , every feature to a partial function
from I to I , and every individual into a different element of I { i.e., aI 6= bI if a 6= b
(Unique Name Assumption) { such that the equations of the left column in Figure 2 are
satisfied.
The ALCF semantics identifies concept expressions as fragments of first-order predicate
logic. Since the interpretation I assigns to every atomic concept, role or feature a unary or
binary (functional) relation over I , respectively, one can think of atomic concepts, roles
and features as unary and binary (functional) predicates. This can be seen as follows: an
atomic concept A, an atomic role P , and an atomic feature f , are mapped respectively to
the open formulas FA ( ), P (ff; fi ), and Ff (ff; fi ) with Ff satisfying the functionality axiom
8y; z.Ff (x; y) ^ Ff (x; z) ! y = z { i.e., Ff is a functional relation.
The rightmost column of Figure 2 gives the transformational semantics of ALCF expressions in terms of FOL well-formed formul, while the left column gives the standard
extensional semantics. As far as the transformational semantics is concerned, a concept C ,
a role P and a path p correspond to the FOL open formulae FC ( ), FP (ff; fi ), and Fp (ff; fi ),
466

fiA Temporal Description Logic for Reasoning about Actions and Plans

>I = I
?I = ;

(:C )I =
(C u D)I =
(C t D)I =
(9P .C )I =
(8P .C )I =
(p : C )I =
p # qI =
p " qI =

true
false

I n C I

C I \ DI
C I [ DI
fa 2 I j 9b.(a; b) 2 P I ^ b 2 C I g
fa 2 I j 8b.(a; b) 2 P I ) b 2 C I g
fa 2 dom pI j pI (a) 2 C I g
fa 2 dom pI \ dom qI j pI (a) = qI (a)g
fa 2 dom pI \ dom qI j pI (a) 6= qI (a)g

(p ")I = I n dom pI
(p  q)I = pI  qI

:FC ( )
FC ( ) ^ FD ( )
FC ( ) _ FD ( )
9x.FP (; x) ^ FC (x)
8x.FP (; x) ) FC (x)
9x.Fp (; x) ^ FC (x)
(9x.Fp (; x) ^ Fq (; x))
(9x; y .Fp (; x) ^ Fq (; y ))^
(8x; y .Fp (; x) ^ Fq (; y ) ! x 6= y )
:9x.Fp (; x)
9x.Fp (ff; x) ^ Fq (x; fi )

Figure 2: The extensional and transformational semantics in ALCF .
respectively. It is worth noting that the extensional semantics of the left column gives also
an interpretation for the formulas of the right column so that the following proposition
holds.

Proposition 2.1 (Concepts vs. fol formul) Let C be an ALCF concept expression.

Then the transformational semantics of Figure 2 maps C into a logically equivalent first
order formula.

A terminology or TBox is a finite set of terminological axioms. For an atomic concept A,
called defined
concept, and a (possibly complex) concept C , a terminological axiom is of the
:
form A = C . An atomic concept not appearing on the left-hand side of any terminological
axiom is called a primitive concept. Acyclic simple TBoxes only are considered: a defined
concept may appear at most once as the left-hand side of an axiom, and no terminological
cycles are allowed, i.e., no defined concept may occur { neither directly nor indirectly {
within its own definition (Nebel, 1991). An interpretation I satisfies A =: C if and only if
AI = C I .
As an example, consider the unary relation (i.e., a concept) denoting the class of happy
fathers, defined using the atomic predicates Man, Doctor, Rich, Famous (concepts) and
CHILD, FRIEND (roles):
:
HappyFather = Man u (9CHILD.>) u 8CHILD.(Doctor u 9FRIEND.(Rich t Famous))
i.e., the men whose children are doctors having some rich or famous friend.
An ABox is a finite set of assertional axioms, i.e. predications on individual objects. Let
O be the alphabet of symbols denoting individuals; an assertion is an axiom of the form
C (a), R(a; b) or p(a; b), where a and b denote individuals in O. C (a) is satisfied by an
interpretation I iff aI 2 C I , P (a; b) is satisfied by I iff (aI ; bI ) 2 P I , and p(a; b) is satisfied
by I iff pI (aI ) = bI .
467

fiArtale & Franconi

A knowledge base is a finite set  of terminological and assertional axioms. An interpretation I is a model of a knowledge base  iff every axiom of  is satisfied by I .  logically
implies A v C (written  j= A v C ) if AI  C I for every model of : we say that A is
subsumed by C in . The reasoning problem of checking whether A is subsumed by C in
 is called subsumption checking.  logically implies C (a) (written  j= C (a)) if aI 2 C I
for every model of : we say that a is an instance of C in . The reasoning problem of
checking whether a is an instance of C in  is called instance recognition.
An acyclic simple TBox can be transformed into an expanded TBox having the same
models, where no defined concept makes use in its definition of any other defined concept.
In this way, the interpretation of a defined concept in an expanded TBox does not depend
on any other defined concept. It is easy to see that A is subsumed by C in an acyclic simple
TBox  if and only if the expansion of A with respect to  is subsumed by the expansion of
C with respect to  in the empty TBox. The expansion procedure recursively substitutes
every defined concept occurring in a definition with its defining expression; such a procedure
may generate a TBox exponential in size, but it has been proved (Nebel, 1990) that it works
in polynomial time under reasonable restrictions. The following interchangeably refers either
to reasoning with respect to a TBox or to reasoning involving expanded concepts with an
empty TBox. In particular, while devising the subsumption calculus for the logics considered
here, it is always assumed that all defined concepts have been expanded.

3. Towards a Temporal Description Logics
Schmiedel (1990) proposed to extend Description Logics with an interval{based temporal
logic. The temporal variant of the Description Logic is equipped with a model-theoretic
semantics. The underlying Description Logic is FLENR, (Donini et al., 1995): it differs
from ALCF in that it does not contain the > and ? concepts, it does not have neither
negation nor disjunction, and it has cardinality restrictions and conjunction over roles.
The new temporal term-forming operators are the temporal qualifier at, the existential and
universal temporal quantifiers sometime and alltime. The qualifier operator specifies the
time at which a concept holds. The temporal quantifiers introduce the temporal variables
constrained by means of temporal relationships based on Allen's interval algebra extended
with metric constraints to deal with durations, absolute times, and granularities of intervals.
To give an example of this temporal Description Logic, the concept of Mortal can be defined
by:
:
Mortal = LivingBeing u (sometime(x) (after x NOW) (at x (:LivingBeing)))
with the meaning of a LivingBeing at the reference interval NOW, who will not be alive
at an interval x sometime after the reference interval NOW. Schmiedel does not propose
any algorithm for computing subsumption, but gives some preliminary hints. Actually,
Schmiedel's logic is argued to be undecidable (Bettini, 1997), sacrificing the main benefit
of Description Logics: the possibility of having decidable inference techniques.
Schmiedel's temporal Description Logic, when closed under complementation, contains
as a proper fragment the temporal logic HS proposed by Halpern and Shoham (1991).
The logic HS is a propositional modal logic which extends propositional logic with modal
formul of the kind hRi. and [R]. { where R is a basic Allen's temporal relation and hi
468

fiA Temporal Description Logic for Reasoning about Actions and Plans

and [] are the possibility and necessity modal operators. For example, the modal formula
LivingBeing ^ hafteri.:LivingBeing corresponds to the abovementioned Mortal concept.
Unfortunately, the HS logic is shown to be undecidable, at least for most interesting classes
of temporal structures: \One gets decidability only in very restricted cases, such as when
the set of temporal models considered is a finite collection of structures, each consisting of
a finite set of natural numbers." (Halpern & Shoham, 1991)
Weida and Litman (1992, 1994) propose T-Rex, a loose hybrid integration between
Description Logics and constraint networks. Plans are defined as collections of steps together
with temporal constraints between their duration. Each step is associated with an action
type, represented by a generic concept in K-Rep { a non-temporal Description Logic. Thus
a plan is seen as a plan network, a temporal constraint network whose nodes, corresponding
to time intervals, are labeled with action types and are associated with the steps of the plan
itself. As an example of plan in T-Rex they show the plan of preparing spaghetti marinara:
(

defplan Assemble-Spaghetti-Marinara
((step1 Boil-Spaghetti)
(step2 Make-Marinara)
(step3 Put-Together-SM))
((step1 (before meets) step3)
(step2 (before meets) step3)))

This is a plan composed by three actions, i.e., boiling spaghetti, preparing marinara sauce,
and assembling all things at the end. Temporal constraints between the steps establish
the temporal order in doing the corresponding actions. A structural plan subsumption
algorithm is defined, characterized in terms of graph matching, and based on two separate
notions of subsumption: pure terminological subsumption between action types labeling
the nodes, and pure temporal subsumption between interval relationships labeling the arcs.
The plan library is used to guide plan recognition (Weida, 1996) in a way similar to that
proposed by Kautz (1991). Even if this work has strong motivations, no formal semantics
is provided for the language and the reasoning problems.
Starting from the assumption that an action has a duration in time, our proposal considers an interval-based modal temporal logic { in the spirit of Halpern and Shoham (1991)
{ and reduces the expressivity of (Schmiedel, 1990) in the direction of (Weida & Litman,
1992). While Schmiedel's work lacks computational machinery, and Halpern and Shoham's
logic is undecidable, here an expressive decidable logic is obtained, providing sound and
complete reasoning algorithms. Differently from T-Rex which uses two different languages
to represent actions and plans { a non temporal Description Logic for describing actions
and a second language to compose plans by adding temporal information { here an extension of a Description Logic is chosen in which time operators are available directly as term
constructors. This view implies an integration of a temporal domain in the semantic structure where terms themselves are interpreted, giving the formal way both for a well-founded
notion of subsumption and for proving soundness and completeness of the corresponding
procedure. As an example of the formalism, the plan for preparing spaghetti marinara
introduced above is represented as follows:
469

fiArtale & Franconi

: 3(y z w) (y (before; meets) w)(z (before; meets) w).
(Boil-Spaghetti@y u
Make-Marinara@z u
Put-Together-SM@w)

Assemble-Spaghetti-Marinara =

Moreover, it is possible to build temporal structured actions { as opposed to the atomic
actions proposed in T-Rex { describing how the world state changes because of the occurrence of an action: in fact, our language allows for feature representation in order to relate
actions to states of the world (see Section 5.2). This kind of expressivity is not captured
by T-Rex, since it uses a non-temporal Description Logic to represent actions. The main
application of T-Rex is plan recognition; according to the ideas of Kautz (1991) a Closed
World Assumption (CWA) (Weida, 1996) is made, assuming that the plan library is complete and an observed plan will be fully accounted for by a single plan. CWA is not relied
on here, following the Open World Semantics characterizing Description Logics. Weaker,
but monotonic, deductions are allowed in the plan recognition process. However, their procedures for recognizing a necessary, optional or impossible individual plan with respect to
a plan description is still applicable, if the plan library is given a closed world semantics.

4. The Feature Temporal Language TL-F
The feature temporal language TL-F is the basic logic considered here. This language is
composed of the temporal Logic TL { able to express interval temporal networks { and the
non-temporal Feature Description Logic F . Note that, each logic of the family of Temporal
Description Logics introduced in this paper is identified by a composed string in which
the first part refers to the temporal part of the language while the other one refers to the
non-temporal part.

4.1 Syntax
Basic types of the language are concepts , individuals, temporal variables and intervals.
Concepts can describe entities of the world, states and events. Temporal variables denote
intervals bound by temporal constraints, by means of which abstract temporal patterns in
the form of constraint networks are expressed. Concepts (resp. individuals) can be specified
to hold at a certain temporal variable (resp. interval). In this way, action types (resp.
individual actions) can be represented in a uniform way by temporally related concepts
(resp. individuals).
For the basic temporal interval relations the Allen notation (Allen, 1991) (Figure 3) is
used: before (b), meets (m), during (d), overlaps (o), starts (s), finishes (f), equal (=), after
(a), met-by (mi), contains (di), overlapped-by (oi), started-by (si), finished-by (fi). Concept
expressions (denoted by C; D) are syntactically built out of atomic concepts (denoted by A),
atomic features (denoted by f ), atomic parametric features (denoted by ?g) and temporal
variables (denoted by X; Y ). Temporal concepts (C; D) are distinguished from non-temporal
concepts (E; F ), following the syntax rules of Figure 4. Names for atomic features and
atomic parametric features are from the same alphabet of symbols; the ? symbol is not
intended as operator, but only as differentiating the two syntactic types.
470

fiA Temporal Description Logic for Reasoning about Actions and Plans

Relation

Abbr.

Inverse

before(i; j )

b

a

meets(i; j )

m

mi

overlaps(i; j )

o

oi

starts(i; j )

s

si

during(i; j )

d

di

finishes(i; j )

f

fi

i

j

Figure 3: The Allen's interval relationships.
Temporal variables are introduced by the temporal existential quantifier \3" { excluding
the special temporal variable ], usually called NOW, and intended as the reference interval.
Variables appearing in temporal constraints (Tc) must be declared within the same temporal
quantifier, with the exception of the special variable ]. Temporal variables appearing in the
right hand side of an \@" operator are called bindable. Concepts must not include unbound
(a.k.a. free) bindable variables. Informally, a bindable variable is said to be bound in a
concept if it is declared at the nearest temporal quantifier in the body of which it occurs;
this avoid the usual formal inductive definition of a bound variable. Moreover, in chained
constructs of the form ((C [Y1 ]@X1 )[Y2 ]@X2 : : :) non bindable variables { i.e., the ones on
the left hand side of an \@" operator { cannot appear more than once. Note that, since
Description Logics are a fragment of FOL with one free variable, the above mentioned
restrictions force the temporal side of the language to have only one free temporal variable,
i.e., the reference time ].
As usual, terminological axioms for building simple acyclic TL-F TBoxes are allowed.
While using in a concept expression a name referring to a defined concept, it is possible to
use the substitutive qualifier construct, to impose a coreference with a variable appearing
in the definition associated to the defined concept. The statement C [Y ]@X constrains the
variable Y , which should appear in the definition of the defined concept C , to corefer with X
(see Section 5.2 for an example). A drawback in the use of this operator is the requirement
to know the internal syntactical form of the defined concept, namely, the names of its
temporal variables.
Let O and OT be two alphabets of symbols denoting individuals and temporal intervals,
respectively. An assertion { i.e., a predication on temporally qualified individual entities {
is a statement of one of the forms C (i; a); p(i; a; b); ?g(a; b); R(i; j ), where C is a concept, p
is a feature, ?g is a parametric feature, R is a temporal relation, a and b denote individuals
in O, i and j denote temporal intervals in OT .
471

fiArtale & Franconi

TL

F

C; D ! E j
CuD j
C @X j
C [Y ]@X j
3(X ) Tc.C
Tc ! (X (R) Y ) j
(X (R) ]) j
(] (R) Y )
Tc ! Tc j Tc Tc
R; S ! R , S j
s j mi j f j : : :
X; Y ! x j y j z j : : :
X ! XjX X
E; F ! A j

>j
EuF j
p#qj

p:E
p; q ! f j
?g j
pq

(non-temporal concept)
(conjunction)
(qualifier)
(substitutive qualifier)
(existential quantifier)
(temporal constraint)

(disjunction)
(Allen's relations)
(temporal variables)
(atomic concept)
(top)
(conjunction)
(agreement)
(selection)
(atomic feature)
(atomic parametric feature)
(path)

Figure 4: Syntax rules for the interval Description Logic TL-F
4.1.1 A clarifying Example

Let us now informally see the intended meaning of the terms of the language TL-F (for the
formal details see Section 4.2). Concept expressions are interpreted over pairs of temporal
intervals and individuals hi; ai, meaning that the individual a is in the extension of the concept at the interval i. If a concept is intended to describe an action, then its interpretation
can be seen as the set of individual actions of that type occurring at some interval.
Within a concept expression, the special \]" variable denotes the current interval of
evaluation; in the case of actions, it is thought that it refers to the temporal interval
at which the action itself occurs. The temporal existential quantifier introduces interval
variables, related to each other and possibly to the ] variable in a way defined by the set of
temporal constraints. To evaluate a concept at an interval X , different from the current one,
it is necessary to temporally qualify it at X { written C @X ; in this way, every occurrence of
472

fiA Temporal Description Logic for Reasoning about Actions and Plans

]

-

Basic-Stack(BLOCK)

x

-

OnTable(BLOCK)

y

OnBlock(BLOCK)

-

Figure 5: Temporal dependencies in the definition of the Basic-Stack action.

] embedded within the concept expression C is interpreted as the X variable2. The informal

meaning of a concept with a temporal existential quantification can be understood with the
following examples in the action domain.
:
Basic-Stack = 3(x y ) (x m ])(] m y ). ((?BLOCK : OnTable)@x u (?BLOCK : OnBlock)@y )

Figure 5 shows the temporal dependencies of the intervals in which the concept Basic-Stack
holds. Basic-Stack denotes, according to the definition (a terminological axiom), any
action occurring at some interval involving a ?BLOCK that was once OnTable and then
OnBlock. The ] interval could be understood as the occurring time of the action type being
defined: referring to it within the definition is an explicit way to temporally relate states
and actions occurring in the world with respect to the occurrence of the action itself. The
temporal constraints (x m ]) and (] m y) state that the interval denoted by x should meet
the interval denoted by ] { the occurrence interval of the action type Basic-Stack { and
that ] should meet y. The parametric feature ?BLOCK plays the role of formal parameter of
the action, mapping any individual action of type Basic-Stack to the block to be stacked,
independently from time. Please note that, whereas the existence and identity of the ?BLOCK
of the action is time invariant, it can be qualified differently in different intervals of time,
e.g., the ?BLOCK is necessarily OnTable only during the interval denoted by x.
Let us comment now on the introduction of explicit temporal variables. The absence of
explicit temporal variables would weaken the temporal structure of a concept since arbitrary
relationships between more than two intervals could not be represented anymore. For
example, having only implicit intervals it is not possible to describe the situation in which
two concept expressions, say C and D, hold at two meeting intervals (say x, y) with the first
interval starting and the second finishing the reference interval (i.e., the temporal pattern
(x meets y)(x starts ])(y finishes ]) cannot be represented). More precisely, it is not possible
to represent temporal relations between more than two intervals if they are not derivable by
the temporal propagation of the constraints imposed on pairs of variables. While explicit
variables go against the general thrust of Description Logics, the gained expressive power
together with the observation that the variables are limited only to the temporal part of
the language are the main motivations for using them. However, it is easy to drop them by
limiting the temporal expressiveness as proposed by Bettini (1997) (see also Section 9).
An assertion of the type Basic-Stack(i; a) states that the individual a is an action of
the type Basic-Stack occurred at the interval i. Moreover, the same assertion implies that
a is related to a ?BLOCK , say b, which is of type OnTable at some interval j , meeting i, and
of type OnBlock at another interval l, met by i.
2. Since any concept is implicitly temporally qualified at the special ] variable, it is not necessary to
explicitly qualify concepts at ].

473

fiArtale & Franconi

(s)E = fh[u; v]; [u1 ; v1 ]i 2 T<?  T<? j u = u1 ^ v < v1 g
(f )E = fh[u; v]; [u1 ; v1 ]i 2 T<?  T<? j v = v1 ^ u1 < ug
(mi)E = fh[u; v]; [u1 ; v1 ]i 2 T<?  T<? j u = v1 g
: : : (meaning of the other Allen temporal relations)
E
(R , S ) = R E [ S E
hX; TciE = fV : X 7! T<? j 8(X (R) Y ) 2 Tc. hV (X ); V (Y )i 2 RE g:
Figure 6: The temporal interpretation function.

i; a) =) 9b. ?BLOCK(a; b) ^ 9j; l. (OnTable(j; b) ^ OnBlock(l; b) ^
m(j; i) ^ m(i; l))

Basic-Stack(

An individual action is an object in the conceptual domain associated with the relevant
properties { or states { of the world affected by the individual action itself via a bunch of
features; moreover, temporal relations constrain time intervals imposing an ordering in the
change of the states of the world.

4.2 Semantics

In this Section, a Tarski-style extensional semantics for the TL-F language is given, and a
formal definition of the subsumption and recognition reasoning tasks is devised.
Assume a linear, unbounded, and dense temporal structure T = (P ; <), where P is
a set of time points and < is a strict partial order on P . In such a structure, given an
interval X and a temporal relation R, it is always possible to find an interval Y such that
(X (R) Y ). The assumption of linear time { which means that for any two points t1 and
t2 such that t1  t2 the set of points ft j t1  t  t2 g is totally ordered { fits the intuition
about the nature of time, so that the pair [t1 ; t2 ] can be thought as the closed interval of
points between t1 and: t2 . The interval set of a structure T is defined as the set T<? of all
closed intervals [u; v] = fx 2 P j u  x  v; u 6= vg in T .
A primitive interpretation I =: hT<? ; I ; I i consists of a set T<? (the interval set of
the selected temporal structure T ), a set I (the domain of I ), and a function I (the
primitive interpretation function of I ) which gives a meaning to atomic concepts, features
and parametric features:

AI  T<?  I ;

f I : (T<?  I ) partial
7,! I ;

?gI : I partial
7,! I

Atomic parametric features are interpreted as partial functions; they differ from atomic
features for being independent from time.
In order to give a meaning to temporal expressions present in generic concept expressions, Figure 6 defines the temporal interpretation function. The temporal interpretation
function E depends only on the temporal structure T . The labeled directed graph hX; Tci
{ where X is the set of variables representing the nodes, and Tc is the set of temporal constraints representing the arcs { is called temporal constraint network. The interpretation
474

fiA Temporal Description Logic for Reasoning about Actions and Plans

AIV ;t;H = fa 2 I j ht; ai 2 AI g = AIt

>IV ;t;H = I = >I
(C u D)IV ;t;H = CVI ;t;H \ DVI ;t;H
(p # q)IV ;t;H = fa 2 dom pIt \ dom qtI j pIt (a) = qtI (a)g = (p # q)It
(p : C )IV ;t;H = fa 2 dom pIt j pIt (a) 2 CVI ;t;Hg

(C @X )IV ;t;H = CVI ;V (X );H
(C [Y ]@X )IV ;t;H = CVI ;t;H[fY 7!V (X )g
(3(X ) Tc. C )IV ;t;H = fa 2 I j 9W . W 2 hX; TciEH[f]7!tg ^ a 2 CWI ;t;; g
ftI = f^t : I partial
7,! I j 8a. (a 2 dom f^t $ ht; ai 2 dom f I ) ^
f^t (a) = f I (t; a)
(p  q)It = pIt  qtI
?gtI = ?gI
Figure 7: The interpretation function.
of a temporal constraint network is a set of variable assignments that satisfy the temporal
constraints. A variable assignment is a function V : X 7! T<? associating an interval value to
a temporal variable. A temporal constraint network is consistent if it admits a non empty
interpretation. The notation, hX; TciEfx1 7!t1 ;x2 7!t2 ;:::g , used to interpret concept expressions,
denotes the subset of hX; TciE where the variable xi is mapped to the interval value ti .
It is now possible to interpret generic concept expressions. Consider the equations
introduced in Figure 7. An interpretation function IV ;t;H, based on a variable assignment
V , an interval t and a set of constraints H = fx1 7! t1; : : :g over the assignments of inner
variables, extends the primitive interpretation function in such a way that the equations
of Figure 7 are satisfied. Intuitively, the interpretation of a concept CVI ;t;H is the set of
entities of the domain that are of type C at the time interval t, with the assignment for the
free temporal variables in C given by V { see (C @X )IV ;t;H { and with the constraints for
the assignment of variables in the scope of the outermost temporal quantifiers given by H.
Note that, H interprets the variable renaming due to the temporal substitutive qualifier {
see (C [Y ]@X )IV ;t;H { and it takes effect during the choice of a variable assignment, as the
equation for (3(X ) Tc. C )IV ;t;H shows.
In absence of free variables in the concept expression { with the exception of ]{ for
notational simplification the natural interpretation function CtI ; being equivalent to the
interpretation function CVI ;t;H with any V such that V (]) = t and H = ; is introduced. The
set of interpretations fCVI ;t;Hg obtained by varying I ; V ; t with a fixed H is maximal wrt set
inclusion if H = ;, i.e., the set of natural interpretations includes any set of interpretations
with a fixed H. In fact, since H represents a constraint in the assignment of variables, the
unconstrained set is the larger one. Note that, for feature interpretation only the natural
one is used since it is not admitted to temporally qualify them.
475

fiArtale & Franconi

]
]

Boil-Spaghetti

x

Make-Spaghetti

-

Boil

-

Figure 8: Temporal dependencies in the definition of the Boil-Spaghetti plan.
An interpretation I satisfies the terminological axiom A =: C iff AIt = CtI , for every t.
A concept C is subsumed by a concept D (C v D) if CtI  DtI for every interpretation I
and every interval t. An interpretation I is a model for a concept C if CtI 6= ; for some t.
If a concept has a model, then it is satisfiable, otherwise it is unsatisfiable.
Each TL-F concept expression is always satisfiable, with the proviso that the temporal
constraints introduced by the existential quantifiers are consistent. This latter condition
can be easily checked during the reduction of the concept into a normal form when the
minimal temporal network (see Section 11, definition 6.5) is computed.
It is interesting to note that only the relations s, f, mi are really necessary, because it is
possible to express any temporal relationship between two distinct intervals using only these
three relations and their transpositions si, fi, m (Halpern & Shoham, 1991). The following
equivalences hold:

3x (x a ]). C @x  3xy (y mi ])(x mi y). C @x
3x (x d ]). C @x  3xy (y s ])(x f y). C @x
3x (x o ]). C @x  3xy (y s ])(x fi y). C @x
To assign a meaning to ABox axioms, the temporal interpretation function E is extended
to temporal intervals so that iE is an element of T<? for each i 2 OT . The semantics of
assertions is the following: C (i; a) is satisfied by an interpretation I iff aI 2 CiIE ; p(i; a; b)
is satisfied by I iff pIiE (aI ) = bI ; ?g(a; b) is satisfied by I iff ?gI (aI ) = bI ; and R(i; j ) is
satisfied by I iff hiE ; j E i 2 RE . Given a knowledge base , an individual a in O is said to
be an instance of a concept C at the interval i if  j= C (i; a).
Now we are able to give a semantic definition for the reasoning task we already called
specific plan recognition with respect to a plan description. This is an inference service that
computes if an individual action/plan is an instance of an action/plan type at a certain
interval, i.e., the task known as instance recognition in the Description Logic community.
Given a knowledge base , an interval i, an individual a and a concept C , the instance
recognition problem is to test whether  j= C (i; a).

5. Action and plan representation: two examples
An action description represents how the world state may evolve in relation with the possible
occurrence of the action itself. A plan is a complex action: it is described by means of
temporally related world states and simpler actions. The following introduces examples of
action and plan representations from two well known domains, the cooking domain (Kautz,
476

fiA Temporal Description Logic for Reasoning about Actions and Plans

z

Make-Marinara

x

y
- y

Make-Spaghetti

-

-

Boil-Spaghetti
Boil

w

-

Put-Together-SM

Figure 9: Temporal dependencies in the definition of Assemble-Spaghetti-Marinara.
1991; Weida & Litman, 1992) and the block world (Allen, 1991), with the aim of showing
the applicability of our framework.

5.1 The Cooking Domain
Let us introduce the plan Boil-Spaghetti:
:
Boil-Spaghetti = 3x (x b ]). (Make-Spaghetti@x u Boil)
Figure 8 shows the temporal dependencies of the intervals in which the concept Boil-Spaghetti holds. The definition employs the ] interval to denote the occurrence time of the
plan itself; in this way, it is possible to describe how different actions or states of the world
concurring to the definition of the plan are related to it. This is why the variable ] is
explicitly present in the definition of Boil-Spaghetti, instead of a generic variable: the
Boil action should take place at the same time of the plan itself, while Make-Spaghetti
occurs before it.
The definition of a plan can be reused within the definition of other plans by exploiting
the full compositionality of the language. The plan defined above Boil-Spaghetti is used
in the definition of Assemble-Spaghetti-Marinara:
:
Assemble-Spaghetti-Marinara = 3(y z w) (y b w)(z b w).
(Boil-Spaghetti@y u
Make-Marinara@z u
Put-Together-SM@w)
In this case, precise temporal relations between the nodes of two corresponding temporal
constraint networks are asserted: e.g., the action Put-Together-SM takes place strictly after
the Boil action (Figure 9). Observe that the occurrence interval of the plan Assemble-Spaghetti-Marinara does not appear in the Figure because it is not temporally related with
any other interval.
A plan subsuming Assemble-Spaghetti-Marinara is the more general plan defined below, Prepare-Spaghetti, supposing that the action Make-Sauce subsumes Make-Marinara.
This means that among all the individual actions of the type Prepare-Spaghetti there are
all the individual actions of type Assemble-Spaghetti-Marinara:
:
Prepare-Spaghetti = 3 (y z ) (). (Boil-Spaghetti@y u Make-Sauce@z )
477

fiArtale & Franconi

]
- w

Stack(OBJ1, OBJ2)

Clear-Block(OBJ1)

v

- z
- y

Holding-Block(OBJ1) Clear-Block(OBJ1)

Clear-Block(OBJ2)

ON(OBJ1, OBJ2)

x

Figure 10: Temporal dependencies in the definition of the Stack action.
However, note that Boil-Spaghetti does not subsume Prepare-Spaghetti, even if it is a
conjunct in the definition of the latter. This could be better explained observing how the
definition of Prepare-Spaghetti plan is expanded:
:
Prepare-Spaghetti = 3 (x y z ) (x b y ). (Make-Spaghetti@x u Boil@y u
Make-Sauce@z )
Then, the Boil action occurs at the interval y { which can be different from the occurring
time of Prepare-Spaghetti { as the effect of binding Boil-Spaghetti to the temporal variable y. On the contrary, in the definition of Boil-Spaghetti the Boil action takes place necessarily at the same time. Subsumption between Prepare-Spaghetti and Boil-Spaghetti
fails since different temporal relations between the actions describing the two plans and the
plans themselves are specified. In particular, observe that the Boil-Spaghetti plan denotes
a narrower class than the plan expression
3(x y) (x b y). (Make-Spaghetti@x u Boil@y)
which subsumes both Prepare-Spaghetti and Boil-Spaghetti itself.

5.2 The Blocks World Domain

As a further example of the expressive power of the temporal language, it is now shown
how to represent the Stack action in the blocks world, in a more detailed way than the
previous simple Basic-Stack action used as a clarifying example. Thus a stacking action
involves two blocks, which should be both clear at the beginning; the central part of the
action consists of grasping one block; at the end, the blocks are one on top of another, and
the bottom one is no longer clear (Figure 10).
Our representation borrows from the Rat Description Logic (Heinsohn, Kudenko, Nebel,
& Profitlich, 1992) the intuition of representing action parameters by means of partial
functions mapping from the action itself to the involved action parameter (see Section 9). In
the language, these functions are called parametric features. For example, the action Stack
has the parameters ?OBJECT1 and ?OBJECT2, representing in some sense the objects that are
involved in the action independently from time. So, in the assertion \?OBJECT1(a; block-a)",
block-a denotes the first object involved in the action a at any interval. On the other hand,
an assertion involving a (non-parametric) feature, e.g., \ON(i; block-a; block-b)", does not
imply anything about the truth value at intervals other than i.
The concept expression, which defines the Stack action, makes use of temporal qualified
concept expressions, including feature selections and agreements: the expression (?OBJECT2 :
Clear-Block)@x means that the second parameter of the action should be a Clear-Block
478

fiA Temporal Description Logic for Reasoning about Actions and Plans

at the interval denoted by x; while (?OBJECT1  ON # ?OBJECT2)@y indicates that at the
interval y the object on which ?OBJECT1 is placed is ?OBJECT2. The formal definition of the
action Stack is:
:
Stack = 3(x y z v w) (x fi ])(y mi ])(z mi ])(v o ])(w f ])(w mi v ).
((?OBJECT2 : Clear-Block)@x u (?OBJECT1  ON # ?OBJECT2)@y u
(?OBJECT1 : Clear-Block)@v u (?OBJECT1 : Holding-Block)@w u
(?OBJECT1 : Clear-Block)@z )
The above defined concept does not state which properties are the prerequisites for the
stacking action or which properties must be true whenever the action succeeds. What this
action intuitively states is that ?OBJECT1 will be on ?OBJECT2 in a situation where both
objects are clear at the start of the action. Note that the world states described at the
intervals denoted by v; w; z are the result of an action of grasping a previously clear block:
:
Grasp = 3(x w z ) (x o ])(w f ])(w mi x)(z mi ]).
((?OBJECT1 : Clear-Block)@x u (?OBJECT1 : Holding-Block)@w u
(?OBJECT1 : Clear-Block)@z )
The Stack action can be redefined by making use of the Grasp action:
:
Stack = 3(x y u v ) (x fi ])(y mi ])(u f ])(v o ]).
((?OBJECT2 : Clear-Block)@x u (?OBJECT1  ON # ?OBJECT2)@y u
(Grasp[x]@v)@u)
The temporal substitutive qualifier (Grasp[x]@v) renames within the defined Grasp action
the variable x to v and it is a way of making coreference between two temporal variables,
while the temporal constraints peculiar to the renamed variable x are inherited by the
substituting interval v. Furthermore, the effect of temporally qualifying the grasping action
at u is that the ] variable associated to the grasping action { referring to the occurrence
time of the action itself { is bound to the interval denoted by u. Because of this binding on
the occurrence time of the grasping action, the ] variable in the grasping action and the ]
variable in the stacking action denote different time intervals, so that the grasping action
occurs at an interval finishing the occurrence time of the stacking action.
Now it is shown how from a series of outside observations action recognition can be
performed { i.e., the task called specific plan recognition with respect to a plan description.
The following ABox describes a situation in which blocks can be clear, grasped and/or on
each other, and in which a generic individual action a is taking place at time interval ia
having the blocks block-a and block-b as its parameters:

?OBJECT1(a; block-a); ?OBJECT2(a; block-b);
o(i1 ; ia ); Clear-Block(i1 ; block-a); fi(i2 ; ia ); Clear-Block(i2 ; block-b);
mi(i3 ; i1 ); f (i3 ; ia ); Holding-Block(i3 ; block-a);
mi(i4 ; ia ); Clear-Block(i4 ; block-a); mi(i5 ; ia ); ON(i5 ; block-a; block-b)
The system deduces that, in the context of a knowledge base  composed by the above
ABox and the definition of the Stack concept in the TBox, the individual action a is of
type Stack at the time interval ia , i.e.,  j= Stack(ia ; a).
479

fiArtale & Franconi

C @X u D@X
(C @X1 )@X2
(C @X1 u D)@X2
C u 3(X ) Tc. D

!
!
!
!

(C u D)@X
C @X1
C @X1 u D@X2
3(X ) Tc. (C u D)
if C doesn't contain free variables

3(X )Tc1 .(C u


3(Y ) Tc2 . D [Y1 ]@X1 : : : [Yp ]@Xq @X ) ! 3(X ][Y1 =X1 ]:::[Yp=Xq ] Y )Tc1 [ Tc2+[]=X] .(C u D+ @X )
if D doesn't contain existential temporal quantifiers
p : (q : C ) ! (p  q) : C
p : (C u D ) ! p : C u p : D
p : (q1 # q2 ) ! p  q1 # p  q2

,

Prescriptions: X ][Y1 =X1 ]:::[Yp=Xq ] Y returns the union of the two sets of variables X and Y , where each
occurrence of Y1 ; : : : ; Yp is substituted by X1 ; : : : ; Xq , respectively, while all the other elements of Y occurring
in X are renamed with fresh new identifiers. Z+ is intended to be the expression Z where the same
substitution or renaming has taken place. The condition on the last rule forces application to start from the
last nested existential temporal qualified concept.

Figure 11: Rewrite rules to transform an arbitrary concept into an existential concept.

6. The Calculus for TL-F
This Section presents a calculus for deciding subsumption between temporal concepts in the
Description Logic TL-F . The calculus is based on the idea of separating the inference on
the temporal part from the inference on the Description Logic part. This is achieved by first
looking for a normal form of concepts. Concept subsumption in the temporal language is
then reduced to concept subsumption between non-temporal concepts and to subsumption
between temporal constraint networks.

6.1 Normal Form
Every TL-F concept expression can be reduced to an equivalent existential concept of the
form: 3(X ) Tc. (Q0 u Q1 @X1 u : : : u Qn@Xn ), where each Q is a non-temporal concept, i.e., it
is an element of the language F . A concept in existential form can be seen as a conceptual
temporal constraint network, i.e., a labeled directed graph hX; Tc; Q@X i where arcs are

labeled with a set of arbitrary temporal relationships { representing their disjunction {
nodes are labeled with non-temporal concepts and, for each node X , the temporal relation
(X = X ) is implicitly true. Moreover, since the normalized concepts do not contain free
variables or substitutive qualifiers, in the following the natural interpretation function (see
Section 4.2) is used.

Proposition 6.1 (Equivalence of EF) Every concept C can be reduced in linear time
into an equivalent existential concept (ef C ), by exhaustively applying the set of rewrite
rules of Figure 11.
480

fiA Temporal Description Logic for Reasoning about Actions and Plans

Procedure

hX; Tci; y):

Covering(

, ;;
, ;;
Z = fz 2 X j (z (=; : : :) y) 2 Tcg;
8s 2 }(Z ) do

if j s j 2 and the graph hX; Tc i obtained by deleting the \=" temporal relation between

mid
result

the node y and each of the nodes in s is inconsistent
then mid
, mid [ fsg;

8s 2 mid do
if :9t 2 mid. t  s
then result
, result [ fsg;
return

result.

Figure 12: Procedure which computes a covering.
Note that (ef C ) makes explicit all the possible chains of features by reducing each nontemporal concept Q to a conjunction of atomic concepts, feature selections restricted to
atomic concepts and feature agreements { i.e., each Q is a feature term expression (Smolka,
1992).
The normalization proceeds by discovering all the possibles interactions between nodes
with the intention of making explicit all the implicit information. A crucial temporal interaction occurs when a node is always coincident with a set of nodes in every possible
interpretation of the temporal network.
Definition 6.2 (Covering) Given a temporal constraint network hX; Tci, let y 2 X and
Z = fz1 ; z2 ; : : : ; zp g  X , with p  1, and y 62 Z . Z is a Covering for y if 8V 2 hX; TciE ,
V (y) 2 fV (z1 ); V (z2 ); : : : ; V (zp)g and for each W  Z , W is not a covering for y. If Z = ;,
then y is called uncovered, otherwise y is said covered by Z .
Proposition 6.3 (Covering procedure) Given a temporal constraint network hX; Tci in
minimal form (see, e.g., (van Beek & Manchak, 1996)) and a node y 2 X then the procedure
described in Figure 12 returns all the possible coverings for y with size  2.
The idea behind the covering is that whenever a set of nodes fz1 ; z2 ; : : : ; zp g is a covering
for y the disjunctive concept expression (Qz1 t : : : t Qzp ) should be conjunctively added to
the concept expression Qy . Actually, since in TL-F concept disjunction is not allowed it will
be sucient to add to the node y the Least Commom Subsumer (lcs) of (Qz1 t : : : t Qzp )
as defined below.
Definition 6.4 (lcs) Let Q1; : : : ; Qn; Q; C be F concept expressions. Then, the concept
Q = lcsfQ1 ; : : : ; Qn g is such that: Q1 v Q ^ : : : ^ Qn v Q and there is no C such that
Q1 v C ^ : : : ^ Qn v C ^ C < Q.
Given a concept in existential form, the temporal completion of the constraint network is
computed as described below.
Definition 6.5 (Completed existential form) The temporal completion of a concept in
existential form { the Completed Existential Form, CEF { is obtained by sequentially applying the following steps:
481

fiArtale & Franconi

 (closure) The transitive closure of the Allen temporal relations in the conceptual

temporal constraint network is computed, obtaining a minimal temporal network (see,
e.g., (van Beek & Manchak, 1996)).

 (= collapsing) For each equality temporal constraint, collapse the equal nodes by
applying the following rewrite(rule:
3(X n fxj g) Tc[xj =xi]. Q[xj =xi ] if xi 6= xj and xj 6= ].
3(X ) Tc (xi = xj ). Q ! 3
(X n fxi g) Tc[xi=]] . Q[xi =]] if xi 6= xj and xj = ].
Then apply exhaustively the first rule of Figure 11.

 (covering) For each y 2 X let compute the covering = fZ 1; : : : ; Z ng following the
procedure showed by proposition 6.3. Whenever the covering is not empty, translate
Qy applying the following rewrite rule: Qy ! Qy ui=1:::n lcsfQi1 ; : : : ; Qim g where Z i =
fzi1 ; : : : ; zim g, and Qij @zij 2 hX; Tc; Q@X i.

 (parameter introduction) New information is added to each node because of the pres-

ence of parameters, as the following rules show. The ; symbol is intended so that,
each time the concept expression in the left hand side appears in some node of the
temporal constraint network, possibly conjoined with other concepts, then the right
hand side represents the concept expression that must be conjunctively added to all the
other nodes; square brackets point out optional parts; the letters f (?f ) and g (?g),
possibly with subscripts, denote atomic (parametric) features while p and q stand for
generic features.
?g1  : : :  ?gn [ f [ p]] : C
?g1  : : :  ?gn [ f [ p]] # g [ q]
?g1  : : :  ?gn # ?f1  : : :  ?fm
?g1  : : :  ?gn  g [ p] # ?f1  : : :  ?fm [ f [ q]]

;
;
;
;

?g1  : : :  ?gn : >.
?g1  : : :  ?gn : >.
?g1  : : :  ?gn # ?f1  : : :  ?fm .
?g1  : : :  ?gn : > u
?f1  : : :  ?fm : >.

Proposition 6.6 (Equivalence of CEF) Every concept in existential form can be reduced into an equivalent completed existential concept.

Both the covering and the parameter introduction steps can be computed independently
after the =-collapsing step and then conjoining the resulting concept expressions. Observe
that, to obtain a completed existential concept, the steps of the normalization procedure
require linear time with the exception of the computation of the transitive closure of the
temporal relations, and the covering step. Both these steps involve NP-complete temporal
constraint problems (van Beek & Cohen, 1990). However, it is possible to devise reasonable
subsets of Allen's algebra for which the problem is polynomial (Renz & Nebel, 1997). The
most relevant properties of a concept in CEF is that all the admissible interval temporal
relations are explicit and the concept expression in each node is no more refinable without
changing the overall concept meaning; this is stated by the following proposition.
Proposition 6.7 (Node independence of CEF) Let hX; Tc; Q@X i be a conceptual temporal constraint network in its completed form (CEF); then, for all Q 2 Q and for all
482

fiA Temporal Description Logic for Reasoning about Actions and Plans

F concept expressions C such that C 6w Q, there exists an interpretation I such that
hX; Tc; (Q u C )@X iIt 6= hX; Tc; Q@X iIt , for some interval t.

Proof. The proposition states that the information in each node of the CEF is independent
from the information in the other nodes. In fact, hX; Tc; (Q u C )@X iIt = hX; Tc; Q@X iIt if
the concept expression in one node implies new information in some other node. Two cases
can be distinguished.
i) Covered Nodes. Both the (= collapsing) rule and the (covering) rule provide to restrict a
covered node with the most specific F concept expression. Indeed, the (= collapsing) rule
provides collapsing two contemporary nodes conjoining the concept expressions of each of
them. On the other hand, the (covering) rule adds to the covered node the most specific
F concept expression that subsumes the disjunctive concept expression that is implicitly
true at the covered node. Note that, thanks to the (Closure) rule, all the possible equal
temporal relations are made explicit. So these two normalization rules cover all the possible
cases of temporal interactions between nodes.
ii) No coincident nodes. Every time-invariant information should spread over all the nodes.
Both parametric features and the > concept have a time-invariant semantics: the only timeinvariant concept expressions are >, ?g1  : : :  ?gn : >, ?g1  : : :  ?gn # ?f1  : : :  ?fm , with
n; m  1, or an arbitrary conjunction of these terms. The (parameter introduction) rule
captures all the possible syntactical cases of completion concerning time-invariant concept
expressions. By induction on the syntax, it can be proven that adding to a node any other
concept expression changes the overall interpretation.
2
The last normalization procedure eliminates nodes with redundant information. This
final normalization step ends up with the concept in the essential graph form, that will be
the normal form used for checking concept subsumption.
Definition 6.8 (Essential graph) The subgraph of the CEF of a conceptual temporal constraint network T = hX; Tc; Q@X i obtained by deleting the nodes labeled only with timeinvariant concept expressions { with the exception of the ] node { is called essential graph
of T : (ess T ).
Proposition 6.9 (Equivalence of essential graph) Every concept in completed existential form can be reduced in linear time into an equivalent essential graph form.

Theorem 6.10 (Equivalence of normal form) Every concept expression can be reduced

into an equivalent essential graph form. If a polynomial fragment of Allen's algebra is
adopted, the reduction takes polynomial time.
As an example, the normal form is shown { i.e., the essential graph { of the previously
introduced Stack action (see Section 5.2):
:
Stack = 3(x y v w z )(x fi ])(y mi ])(z mi ])(w f ])(v o ])(y mi x)(z mi x)(w f x)
(v (o; d; s) x)(z (=; s; si) y)(w m y)(v b y)(w m z )(v b z )(w mi v).
((?OBJECT2 : Clear-Block u ?OBJECT1 : >)@x u
(?OBJECT1 ON # ?OBJECT2)@y u
(?OBJECT1 : Clear-Block u ?OBJECT2 : >)@v u
(?OBJECT1 : Hold-Block u ?OBJECT2 : >)@w u
(?OBJECT1 : Clear-Block u ?OBJECT2 : >)@z )
483

fiArtale & Franconi

In this example, the essential graph is also the CEF of Stack since there are no redundant
nodes.

6.2 Computing Subsumption

A concept subsumes another one just in case every possible instance of the second is also an
instance of the first, for every time interval. Thanks to the normal form, concept subsumption in the temporal language is reduced to concept subsumption between non-temporal
concepts and to subsumption between temporal constraint networks. A similar general procedure was first presented in (Weida & Litman, 1992), where the language for non-temporal
concepts is less expressive { it does not include features or parametric features.
To compute subsumption between non-temporal concepts { which may possibly include
lcs concepts { we refer to (Cohen, Borgida, & Hirsh, 1992). In the following, we will write
\wF " for subsumption between non-temporal F concepts taking into account lcs concepts.
Definition 6.11 (Variable mapping) A variable mapping M is a total function M :
X 1 7! X 2 such that M(]) = ]. We write M(X ) to intend fM(X ) j X 2 X g, and M(Tc)
to intend f(M(X ) (R) M(Y )) j (X (R) Y ) 2 Tcg.

Definition 6.12 (Temporal constraint subsumption) A temporal constraint (X1 (R1 )Y1)
is said to subsume a temporal constraint (X2 (R2 )Y2 ) under a generic variable mapping M,
written (X1 (R1 )Y1 ) wM (X2 (R2 ) Y2 ), if M(X1 ) = X2 , M(Y1 ) = Y2 and (R1 )E  (R2 )E
for every temporal interpretation E .
Proposition 6.13 (TC subsumption algorithm) (X1 (R1 )Y1) wM (X2 (R2 )Y2) if and
only if M(X1 ) = X2 , M(Y1 ) = Y2 and the disjuncts in R1 are a superset of the disjuncts
in R2 .

Proof. Follows from the observation that the 13 temporal relations are mutually disjoint
and their union covers the whole interval pairs space.
2

Definition 6.14 (Temporal constraint network subsumption) A temporal constraint
network hX 1 ; Tc1 i subsumes a temporal constraint network hX 2 ; Tc2 i under a variable mapping M : X 1 7! X 2 , written hX 1 ; Tc1 i wM hX 2 ; Tc2 i, if hM(X 1 ); M(Tc1 )iE  hX 2 ; Tc2 iE
for every temporal interpretation E .
Proposition 6.15 (TCN subsumption algorithm) hX 1; Tc1 i wM hX 2 ; Tc2i iff, after
computing the temporal transitive closure, there exists a variable mapping M : X 1 7! X 2
such that for all X1i ; Y1j 2 X 1 exist X2m ; Y2n 2 X 2 which satisfy (X1i (R1i;j ) Y1j ) wM
(X2m (R2m;n ) Y2n ).

Proof. \( " Since from definition 6.12 (X1i (R1i;j ) Y1j ) wM (X2m (R2m;n ) Y2n ) implies that
(R1i;j )E  (R2m;n )E for every E , then, from the definition of interpretation of a temporal
constraint network, it is easy to see that each assignment of variables V in the interpretation
of hX 2 ; Tc2 i is also an assignment in the interpretation of hM(X 1 ); M(Tc1 )i.
\) " Suppose that one is not able to find such a mapping; then, by hypothesis, for each
possible variable mapping there exists some i; j such that R1i;j is not a superset of R2m;n .
484

fiA Temporal Description Logic for Reasoning about Actions and Plans

Since, by assumption, the temporal constraint networks are minimal, the temporal relation
R2m;n cannot be further restricted. So, for each variable mapping and each temporal interpretation E , we can build an assignment V  such that hV  (X2m ); V  (X2n )i 2 (R2m;n )E while
hV  (X1i ); V  (X1j )i 62 (R1i;j )E . Now, we can extend the assignment V  in such a way that
V  2 (hX 2; Tc2 i)E while V  62 (hM(X 1 ); M(Tc1)i)E . This contradicts the assumption that
2
hX 1; Tc1 i wM hX 2; Tc2 i.

Definition 6.16 (S-mapping) An s-mapping from a conceptual temporal constraint network hX 1 ; Tc1 ; Q@X 1 i to a conceptual temporal constraint network hX 2 ; Tc2 ; Q@X 2 i is a
variable mapping S : X 1 7! X 2 such that the non-temporal concept labeling each node
in X 1 subsumes the non-temporal concept labeling the corresponding node in S (X 1 ), and
hX 1; Tc1 i wS hX 2; Tc2i.
The algorithm for checking subsumption between temporal concept expressions reduces the
subsumer and the subsumee in essential graph form, then it looks for an s-mapping between the essential graphs by exhaustive search. To prove the completeness of the overall
subsumption procedure it will be showed that the introduction of lcs's preserves the subsumption. A model-theoretic characterization of the lcs will be given for showing this
property. Let's start to build an Herbrand model for an F concept. Let C 0(x) denote the
first order formula corresponding to a concept C (see proposition 2.1), while the functionality of features can be expressed with a set of formul F . By syntax induction it easy to
show that C 0 (x) is an existentially quantified formula with one free variable. Moreover, the
matrices of such formula is a conjunction of positive predicates. F [ fC 0 (x)g is logically
equivalent
to F [ fC 00 (x)g where the functionality axioms allow to map every subformula
V
00
y 9y.Ff (x; y) into 9!y.Ff (x; y). Then C (x) is such that all the existential quantifiers in
C 0 (x) (which come from the first order conversion of features) are replaced by 9! quantifiers.
Now, F [ fC 000 (a)g { where a is a constant substituting the free variable x and C 000 (a) is
obtained by skolemizing the 9! quantified variables { is a set of definite Horn clauses.

Definition 6.17 (Herbrand model) Let C be an F concept expression. Then we define
its Minimal Herbrand Model HC as the Minimal Herbrand Model of the above mentioned
set of definite Horn clauses F [ fC 000 (a)g.
Lemma 6.18 (F concept subsumption) Let C; D be F concept expressions, and HC ; HD
their minimal Herbrand models obtained by skolemizing the first order set F [fC 000 (a); D000 (a)g.
Then, C v D iff HD  HC .
Proof. C v D iff F [ fC 0 (x)g j= D0 (x), iff F [ fC 00 (x)g j= D00 (x), where C 00 and D00
are obtained by applying the functionality axioms to the set fC 0 (x); D0 (x)g (i.e., uni-

fying the variables in the functional predicates) and then replacing all the existential
quantifiers by 9! quantifiers. Now, C 000 (x) and D000 (x) are obtained by skolemizing the
9! quantified variables in the following way: let C 00(x) = 9!y1; : : : ; yn(x; y1 ; : : : ; yn) and let
D00 (x) = 9!y1 ; : : : ; yk ; z1 ; : : : ; zm (x; y1 ; : : : ; yk ; z1 ; : : : ; zm ), with 0  k  n, then skolemize
the formula:  = 9!y1 ; : : : ; yn ; z1 ; : : : ; zm (x; y1 ; : : : ; yn ) ^ (x; y1 ; : : : ; yk ; z1 ; : : : ; zm ), and
let 0 (x) indicate its skolemized form. Then, C 000 (x) = 0 (x) and D000 (x) = 0 (x). Now,
since every existential quantification in C 00 (x); D00 (x) was of type 89! then the thesis is true
485

fiArtale & Franconi

iff F [ fC 000 (a)g j= D000 (a), where a is a constant substituting the free variable x (see (van
Dalen, 1994)). Now, as showed by lemma 6.17, both C 000 (a) and D000 (a) have minimal Herbrand models HC ; HD that verify the lemma hypothesis. Then, F [ fC 000 (a)g j= D000 (a) iff
HD  HC .
2
We are now able to give a model-theoretic characterization of the lcs that will be crucial
to prove the subsumption-preserving property.
Lemma 6.19 (lcs model property) Let Q1; : : : ; Qn be F concept expressions, and HQ1 ;
: : : ; HQn their minimal Herbrand models obtained by skolemizing the first order set F [
fQ0001 (a); : : : ; Q000n (a)g. Then, Q = lcsfQ1 ; : : : ; Qn g iff HQ = HQ1 \ : : : \ HQn .
Proof. First of all, let show that HQ is the minimal Herbrand model of a concept Q in
the language F . Every HQi can be seen as a rooted directed acyclic graph where nodes
are labelled with (possible empty) set of atomic concepts and arcs with atomic features
while equality constraints between nodes correspond to features agreement. Whithout loss
of generality let us consider the case where HQ = HQ1 \ HQ2 . It is sucient to show
that HQ is a rooted directed acyclic graph. Let a be the root of HQ1 ; HQ2 , then will be
proved by induction that if Fi (ai,1 ; ai ) 2 HQ (where Fi is the first order translation of a
feature, ai,1 ; ai are obtained as a result of the skolemization process, and a0 = a) then
fF1 (a; a1 ); : : : ; Fi (ai,1; ai )g  HQ. The case i = 1 is trivial. Let i > 1. Now, Fi (ai,1 ; ai) 2
HQ iff Fi(ai,1 ; ai ) 2 HQ1 \ HQ2 . But ai,1 is uniquely defined by the skolem function
fFi,1 (where, the function symbols fFi are newly generated for each feature Fi by the
skolemization process). Then, Fi (ai,1 ; ai ) 2 HQ1 \ HQ2 iff Fi (ai,1 ; fFi,1 (ai )) 2 HQ1 \ HQ2
iff Fi,1 (ai,2 ; fFi,1 (ai )) 2 HQ1 \ HQ2 . Then the thesis is true by induction.
Let us now prove the \(" direction. Suppose by absurd that there is an F concept C
such that: Q1 v C ^ Q2 v C ^ C < Q. Then, Q1 v C iff HC  HQ1 , and Q2 v C , iff
HC  HQ2 . But then HC  HQ1 \ HQ2 , i.e., HC  HQ. Then Q v C which contradicts the
hypothesis.
The \)" direction can be proved with analogous considerations.
2

Proposition 6.20 (lcs subsumption-preserving property) Let A; B; C; D be F concepts, then A u (B t C ) v D iff A u lcsfB; C g v D.
Proof. A u (B t C ) v D iff A u B v D and A u C v D. Now, A u B v D iff F [
fA000 (a); B 000 (a)g j= D000(a) iff HA [ HB j= D000 (a) iff HD  HA [ HB . For the same reasons,
A u C v D iff HD  HA [ HC . But then, HD  HA [ HB and HD  HA [ HC , i.e.,
HD  HA [ (HB \ HC ), i.e., HD  HA [ HlcsfB;C g . But, HD  HA [ HlcsfB;C g iff
A u lcsfB; C g v D.
2

The following theorem provides a sound and complete procedure to compute subsumption. The completeness proof takes into account that the temporal structure is dense and
unbounded. This allows us to introduce any new node to a conceptual temporal constraint
network without changing its meaning. Remember that, for each of these redundant nodes,
time-invariant information holds.
Theorem 6.21 (TL-F concept subsumption) A concept C1 subsumes a concept C2 iff
there exists an s-mapping from the essential graph of C1 to the essential graph of C2 .
486

fiA Temporal Description Logic for Reasoning about Actions and Plans

Proof. Let T1 = hX 1 ; Tc1 ; Q@X 1 i be the essential graph of C1 , and T2 = hX 2 ; Tc2 ; Q@X 2 i
be the essential graph of C2 .
\( " (Soundness). Follows from the fact that the essential graph form is logically equivalent to the starting concept, and from the soundness of the procedures for computing
both the TCN subsumption (proposition 6.15) and the subsumption between non-temporal
concepts (Cohen et al., 1992).
\) " (Completeness). Suppose that such an s-mapping does not exist. Two main cases
can be distinguished.
i) There is not a mapping M such that hX 1 ; Tc1 i wM hX 2 ; Tc2 i. By adding redundant
nodes to T2 , an equivalent conceptual temporal constraint network T2 = hX 2 ; Tc2 ; Q@X 2 i
may be obtained. Let us consider such an extended network
in a way that there exists a


variable mapping M such that hX 1 ; Tc1 i wM hX 2 ; Tc2 i. Now, for all possible M , there
is a node X1i 2 X 1 such that M (X1i ) = X2j with X2j 62 X 2 . Now, Q1i 6wF Q2j , since X2j
cannot coincide with other nodes in X 2 neither can have a covering otherwise the hypothesis
that the mapping M does not exist would be contradicted. Then from proposition 6.7 Q2j
is in a time-invariant node, whereas Q1i is not since T1 is an essential graph. Then, although
the construction of M allows for the existence of a unique V 3 for both networks (follows
from proposition 6.15), it is possible to build an instance of T2 that is not an instance of T1 .
ii) For each possible mapping M such that hX 1 ; Tc1 i wM hX 2 ; Tc2 i there will be always
two nodes X1i and X2j such that M(X1i ) = X2j and Q1i 6wF Q2j . Now, the concept expression Q2j cannot be refined (looking for a subsumption relationship with Q1i ) by adding
to it an F concept since from proposition 6.7 this would change the overall interpretation.
On the other hand, the lcs introduction { which would substitute the more specific concept disjunction implicitly presents because of a node covering { is a subsumption-invariant
concept substitution, as showed by lemma 6.20.
Both cases contradict the assumption that T1 subsumes T2 .
2
6.2.1 Complexity of Subsumption

Now it is shown that checking subsumption between TL-F concept expressions in the essential graph form is an NP-complete problem. Therefore, a polynomial reduction from the
NP-complete problem of deciding whether a graph contains an isomorphic subgraph is presented. It is then shown that the subsumption computation, as proposed in theorem 6.21,
can be done by a non-deterministic algorithm that takes polynomial time in the size of the
concepts involved. First of all let us consider the complexity of computing subsumption
between non-temporal concepts.

Lemma 6.22 (F subsumpion complexity) Let C; D be F concept expressions that can
contain lcs's. Then, checking whether C vF D takes polynomial time.
Proof. See (Cohen et al., 1992).
2
Here the problem of subgraph isomorphism is briey recalled. Given two graphs, G1 =
(V1 ; E1 ) and G2 = (V2 ; E2 ), G1 contains a subgraph isomorphic to G2 if there exists a
3. Since subsumption is computed with respect to a fixed evaluation time, V maps the different occurrences
of ] to the same interval; this justifies the choice that M(]) = ].
487

fiArtale & Franconi

subset of the vertices V 0  V1 and a subset of the edges E 0  E1 such that j V 0 j=j V2 j,
j E 0 j=j E2 j, and there exists a one-to-one function f : V2 7! V 0 satisfying fu; vg 2 E2 iff
ff (u); f (v)g 2 E 0.
Given
a graph G = (V; E ), with V = fv1 ; : : : ; vn g associate a temporal concept expression:
:
C = 3(v1 ; : : : ; vn ) : : : (vi (b; a) vj ) : : : . (A@v1 u : : : u A@vn), where A is an atomic concept
and fvi ; vj g 2 E . This transformation allows us to prove that the problem of subgraph
isomorphism can be reduced to the subsumption of temporal concepts.
Proposition 6.23 Given two graphs G1 and G2, G1 contains a subgraph isomorphic to G2
iff C2 w C1 , where C1 and C2 are the corresponding temporal concepts expressions.
Proof. A temporal network with edges labeled only with the (before _ after) relation is always
consistent, minimal and non-directed4 (Gerevini & Schubert, 1994). Then, each temporal
concept is in the essential graph form. Now the proof easily follows since, every time G2 is
an isomorphic subgraph of G1 the one-to-one function f is also an s-mapping from C2 to
C1 , and it is true that C2 w C1 . On the other hand, the s-mapping that gives rise to the
subsumption is also the one-to-one isomorphism from G2 to G1 .
2

Theorem 6.24 (NP-hardness) Concept subsumption between TL-F concept expressions
in normal form is an NP-hard problem.

Proof. Follows from proposition 6.23 and the reduction being clearly polynomial.
Now the NP-completeness is proven.

2

Theorem 6.25 (NP-completeness) Concept subsumption between TL-F concept expressions in normal form is an NP-complete problem.

Proof. To prove NP-completeness it is necessary to show that the proposed calculus can
be solved by a nondeterministic algorithm that takes polynomial time. Now, given two
temporal concepts, T1 and T2 , in their essential graph form, let j X 1 j= N1 and j X 2 j= N2 .
Then, to check whether T1 w T2 , the algorithm guesses one of the N2N1 variable mapping
from T1 to T2 and verifies whether it is an s-mapping, too. This last step can be done in
deterministic polynomial time since, given a mapping M, it is possible to determine whether
hX 1; Tc1 i wM hX 2; Tc2i by checking at most N1(N1 , 1)=2 edges looking for subsumption
between the corresponding temporal relations (solved by a set inclusion procedure); while
the N1 non-temporal concept subsumptions can be computed in polynomial time.
2

7. Extending the Propositional Part of the Language

The propositional part of the temporal language can be extended to have a more powerful,
but still decidable, Description Logic. It is possible either to add full disjunction, both at
the temporal and non-temporal levels (TLU -FU ), or to have a propositionally complete
language at the non-temporal level only (TL-ALCF ).
Please note that in these languages it is not possible to express full negation, and
in particular the negation of the existential temporal quantifier. This is crucial, and it
4. If (vi (b; a) vj ) then (vj (b; a) vi ), too.

488

fiA Temporal Description Logic for Reasoning about Actions and Plans

(C t D)@X
p : (C t D )
(C1 t C2 ) u D
3(X ) Tc. (C t D)

!
!
!
!

C @X t D@X
p:Ctp:D
(C1 u D) t (C2 u D)
3(X ) Tc. C t 3(X ) Tc. D

Figure 13: Rewrite rules for computing the disjunctive form.
makes the difference with other logic-based approaches (Schmiedel, 1990; Bettini, 1997;
Halpern & Shoham, 1991). The dual of 3 (i.e., the universal temporal quantifier 2) makes
the satisfiability problem { and the subsumption { for propositionally complete languages
undecidable in the most interesting temporal structures (Halpern & Shoham, 1991; Venema,
1990; Bettini, 1993). For the representation of actions and plans in the context of plan
recognition, the universal temporal quantifier is not strictly necessary. This limitation makes
these languages decidable, with nice computational properties, and capable of supporting
other kinds of useful extensions. The examples shown throughout the paper may serve as a
partial validation of the claim. Section 8.1 proposes the introduction of a limited universal
temporal quantification that maintains decidability of subsumption.

7.1 Disjunctive Concepts: TLU -FU
The language TLU -FU adds to the basic language TL-F the disjunction operator { with
the usual semantics { both at the temporal and non-temporal levels:

C; D ! TL j C t D
E; F ! F j E t F

(TLU )
(FU )
Before showing how to modify the calculus to check subsumption, let us begin with a
clarifying example. The gain in expressivity allows us to describe the alternative realizations
that a given plan may have. Let us consider a scenario with a robot moving in an empty
room that can move only either horizontally or vertically. Let's call Rect-Move that which
involves a simple sequence of the two basic moving actions. Then, to describe a Rect-Move
plan we can make use of the disjunction operator:
:
Rect-Move = 3(x y ) (] m x)(x m y ). (Hor-Move@x u Ver-Move@y ) t
3(x y) (] m x)(x m y). (Ver-Move@x u Hor-Move@y)
7.1.1 The Calculus for TLU -FU
Normal Form

In computing subsumption, a normal form for concepts is needed. The normalization procedure is similar to that reported in Section 6.1. Let us start by reducing each concept
expression into an equivalent disjunctive concept of the form:

3(X 1 ) Tc1 . G1) t    t (3(X n) Tcn. Gn) t Q1 t    t Qm

(

489

fiArtale & Franconi

where Gi are conjunctions of concepts of the form Qik @Xik , and each Q does not contain
neither temporal information, nor disjunctions, i.e., it is an element of the language F .
Proposition 7.1 (Equivalence of disjunctive form) Every concept C can be reduced
into an equivalent disjunctive form (df C ), by exhaustively applying the set of rewrite rules
of Figure 13 in addition to the rules introduced in Figure 11.
It is now possible to compute the completed disjunctive normal form (cdnf C ). Each disjunct of such normal form has some interesting properties, which are crucial for the proof
of the theorem 7.4 on concept subsumption: temporal constraints are always explicit, i.e.,
any two intervals are related by a basic temporal relation; there is no disjunction, either
implicit or explicit, neither in the conceptual part nor in the temporal part, i.e., it is a
TL-F concept; the information in each node is independent of the information in the other
nodes and it does not contain time-invariant (i.e., redundant) nodes.
Definition 7.2 (Completed disjunctive normal form) Given a concept in disjunctive
form, the completed disjunctive normal form is obtained by applying the following rewrite
rules to each disjunct:
 (Temporal completion) The rules of definition 6.5 are applied to each disjunct with
the exclusion of the covering step, which is replaced by the t-introduction step. If a
disjunct is unsatisfiable { i.e., the temporal constraint network associated with it is
inconsistent { then eliminate it.
 (Essential form) The rules of definition 6.8 are applied to each disjunct.
 (t introduction) Reduce to concepts containing only basic temporal relationships:
3(X ) (X1 (R,S ) X2 ) Tc.C ! 3(X )(X1 R X2 )Tc.C t 3(X )(X1 S X2 )Tc.C
Proposition 7.3 (Equivalence of CDNF) Every concept expression can be reduced into
an equivalent completed disjunctive normal concept.
Subsumption

The theorem 7.4 reduces subsumption between CDNF concepts into subsumption of disjunction-free concepts, such that the results of theorem 6.21 can be applied. The following
theorem gives a terminating, sound, and complete subsumption calculus for TLU -FU .
Theorem 7.4 (TLU -FU concept subsumption) Let C = C1 t    t Cm and D = D1 t
   t Dn be TLU -FU concepts in CDNF. Then, C v D if and only if 8i9j . Ci v Dj .
Proof. Since it is easy to show that C1 t : : : t Cn v D iff 8i.Ci v D we need only to prove the
restricted thesis: Ci v D1 t  t Dn iff Ci v D1 _ : : : _ Ci v Dn . Every concept expression in
CDNF corresponds to an existential quantified formula with two free variables. Moreover,
the matrices of such formul are conjunctions of positive predicates. Let us denote the
formula corresponding to a concept C as C 0 (t; x). Now, the restricted thesis holds iff it is
true that F [ fCi000 (a; b)g j= D1000 (a; b) _ D2000 (a; b). Now, let HB the minimal Herbrand model
of F [fCi000 (a; b)g. Then, F [fCi000 (a; b)g j= D1000 (a; b) _ D2000 (a; b) iff HB j= D1000 (a; b) _ D2000 (a; b).
Since we are talking of a single model, D1000 (a; b) _ D2000 (a; b) is valid in HB if and only if either
D1000 (a; b) or D2000 (a; b) is valid in HB . This proves the theorem.5
2
5. The proof of this theorem comes from an idea of Werner Nutt.

490

fiA Temporal Description Logic for Reasoning about Actions and Plans

As a consequence of the theorems 6.25, 7.4 the following complexity result holds.

Theorem 7.5 (TLU -FU subsumption complexity) Concept subsumption between TLU -FU
concept expressions in normal form is an NP-complete problem.

7.2 A Propositionally Complete Language: TL-ALCF
TL-ALCF uses the propositionally complete Description Logic ALCF (Hollunder & Nutt,
1990) for non-temporal concepts by changing the syntax rules for TL-F in the following
way:

E; F ! FU j ? j :E j p " q j p "j 8P .E j 9P .E (ALCF )

The interpretation functions are extended to take into account roles:
P I  T<?  I  I
PtI = P^t  I  I j 8a; b. ha; bi 2 P^t $ ht; a; bi 2 P I
As seen in Section 2, ALCF adds to F full negation { thus introducing disagreement (p " q)
and undefinedness (p ") for features, and role quantification (8P .E; 9P .E ).
As an example of the expressive power gained, let us refine the description of the world
states involved in the Stack action (see Section 5.2). Suppose that a block is described by
saying that it has LATERAL-SIDEs (role) and BOTTOM- and TOP-SIDEs (features). Then, the
property of being clear could be represented as follows:
:
Clear-Block = Block u 8LATERAL-SIDE.Clear u TOP-SIDE : HAS-ABOVE "
which says that, in order to be clear, each LATERAL-SIDE has to be clear and nothing has
to be over the TOP-SIDE. Now, the situation in which a block involved in a Stack action is
on top of another one is reformulated with the following concept expression:
(?OBJECT1 TOP-SIDE  HAS-ABOVE # ?OBJECT2)
Furthermore, given the above definition of Clear-Blocks, it can be derived that:
(?OBJECT1 TOP-SIDE  HAS-ABOVE # ?OBJECT2) v (?OBJECT1 : :Clear-Block)
i.e., an object, having another object on top of it, is no more a clear object.
In TL-ALCF it is possible to describe states with some form of incomplete knowledge
by exploiting the disjunction among non-temporal concepts. For example, let us say that
the agent of an action can be either a human being or a machine: ?AGENT:(Person t Robot).
7.2.1 The Calculus for TL-ALCF

This Section presents a calculus for deciding subsumption between temporal concepts in
the Description Logic TL-ALCF . Again, the calculus is based on the idea of separating the
inference on the temporal part from the inference on the Description Logic part (\vALCF "),
and adopting standard procedures developed in the two areas.
Normal Form

Once more, the subsumption calculus is based on a normalization procedure. The first
step reduces a concept expression into an equivalent existential form { 3(X ) Tc. (Q0 u
Q1 @X1 u : : : u Qn@Xn) { by applying the rewrite rules of Figure 11 augmented with the
491

fiArtale & Franconi

:>
:?
:(C u D)
:(C t D)
: :C
:8P .C
:9P .C
:f : C
:p : C
:p # q
:p " q
(f  p) "

!
!
!
!
!
!
!
!
!
!
!
!

?
>
:C t :D
:C u :D
C
9P .:C
8P .:C
f " t f : :C
f " t f : (:q : C ) if p = f  q
p"tq"tp"q
p"tq"tp#q
f " t f : (p ")

Note: By f we denote both an atomic feature and an atomic parametric feature.

Figure 14: Rewrite rules to transform an arbitrary concept into a simple concept.
rule: p : (q1 " q2 ) ! p  q1 " p  q2 . Each Q is a non-temporal concept, i.e., it is an element
of the language ALCF .
In the following normalization step there will be a need to verify concept satisfiability
for non-temporal concept expressions. An ALCF concept E is unsatisfiable iff E vALCF ?.
Algorithms for checking satisfiability and subsumption of concepts terms in ALCF are well
known (Hollunder & Nutt, 1990).
Definition 7.6 (Completed existential form) The temporal completion of a concept
in existential form { the Completed Existential Form, CEF { is obtained by sequentially
applying the following steps:
 (closure, collapsing, covering) As reported in definition 6.5. As for the covering,
translate the concept expression Qy applying the rewrite rule: Qy ! Qy ui=1:::n (Qi1 t
: : : t Qim ).
 (parameter introduction) This requires two phases.
1. Each Q is translated in disjunctive normal form. First the simple form6 is obtained by transforming each Q following the rewrite rules reported in Figure 14.
The disjunctive normal form is then obtained by rewriting each Q { which is now
in simple form { using the following rules, which correspond to the first order
rules for computing the disjunctive normal form of logical formul:
(C1 t C2 ) u D ! (C1 u D) t (C2 u D)
p : (C t D) ! p : C t p : D
6. A simple concept contains only complements of the form :A, where A is a primitive concept, and no
sub-concepts of the form p ", where p is not an atomic (parametric) feature { this corresponds to a first
order logical formula in negation normal form.

492

fiA Temporal Description Logic for Reasoning about Actions and Plans

?g1  : : :  ?gn [ f [ p]] : C
?g1  : : :  ?gn [ f [ p]] # g [ q]
?g1  : : :  ?gn # ?f1  : : :  ?fm
?g1  : : :  ?gn  g [ p] # ?f1  : : :  ?fm [ f [ q]]

!
!
!
!

?g1  : : :  ?gn " ?f1  : : :  ?fm
?g "
?g1  : : :  ?gn : (?gn+1 ")
?g1  : : :  ?gn [ f [ p]] " g [ q]
?g1  : : :  ?gn  g [ p] " ?f1  : : :  ?fm [ f [ q]]

!
!
!
!
!

?g1  : : :  ?gn : >.
?g1  : : :  ?gn : >.
?g1  : : :  ?gn # ?f1  : : :  ?fm .
?g1  : : :  ?gn : > u
?f1  : : :  ?fm : >.
?g1  : : :  ?gn " ?f1  : : :  ?fm .
?g " .
?g1  : : :  ?gn : (?gn+1 ").
?g1  : : :  ?gn : >.
?g1  : : :  ?gn : > u
?f1  : : :  ?fm : >.

Figure 15: Rewrite rules that compute the parameter introduction step.
2. For each Qj = Ej1 t : : : t Ejn , on compute its time-invariant part (let us indicate
this particular concept expression as Q~ j ). This gives Q~ j by computing for each
disjunct Eji in Qj its time-invariant information E~ji . If Eji vALCF ?, then
E~ji = ?. Otherwise, rewrite every conjunct in Eji as showed in Figure 15, while
the conjuncts not considered there are rewrote to >. Now, unless there is an
E~ji = >, Q~ j = E~j1 t : : : t E~jn must be conjunctively added to all the other nodes.

Proposition 7.7 (Equivalence of CEF) Every concept in existential form can be reduced into an equivalent completed existential concept.

As for the TL-F case, both covering and parameter introduction can be computed independently. As a consequence of the above normalization phase, the proposition 6.7 (node
independence) is now true for TL-ALCF concepts in CEF. Observe that, to obtain a CEF
concept, the steps of the normalization procedure require the computation of the transitive
closure of the temporal relations { which is an NP-complete problem (van Beek & Cohen, 1990) { and the computation of ALCF subsumption { which is a PSPACE-complete
problem (Hollunder & Nutt, 1990).
Before the presentation of the last normalization phase, which will eliminate redundant
nodes, it is now possible to check whether a concept expression is satisfiable.
Proposition 7.8 (Concept satisfiability) A TL-ALCF concept in CEF, hX; Tc; Q@X i,
is satisfiable (with the proviso that the temporal constraints are satisfiable) if and only if the
non-temporal concepts labeling each node in X are satisfiable. Checking satisfiability of a
TL-ALCF concept in CEF is a PSPACE-complete problem.
Proof. Is a direct consequence of the node independence established by proposition 6.7,
which is true also for TL-ALCF concepts in CEF.
2
The normalization procedure now goes on by rewriting unsatisfiable concepts to ? and
then computing the essential graph form for satisfiable concepts. This last phase is more
493

fiArtale & Franconi

complex than for the other temporal languages considered in this paper essentially because
ALCF can express the > concept by means of a concept expression (e.g., > = A t :A).
From this consideration it follows that in TL-ALCF a redundant node can be derived from
a complex concept expression (e.g., both A t:A, and ?g : A t ?g : :A are redundant nodes).
The key idea is that all the time-invariant information is present in the ] node thanks to
the CEF. Thus it is needed only to extract this information from the ] node by computing
the disjunctive normal form of Q] , applying the ~ translation, and then testing whether
Q~ ] vALCF Qi, for a given node xi .

Definition 7.9 (Essential graph) The subgraph of the CEF of a TL-ALCF conceptual
temporal constraint network T = hX; Tc; Q@X i obtained by deleting the nodes xi such that
Q~ ] vALCF Qi { with the exception of the ] node { is called essential graph of T : (ess T ).

Proposition 7.10 (Equivalence of essential graph) Every CEF concept can be reduced
into an equivalent essential graph form (and, obviously, every concept can be reduced into
an equivalent essential graph form).
Subsumption

The overall normalization procedure reduces the subsumption problem in TL-ALCF to the
subsumption between ALCF concepts.

Theorem 7.11 (TL-ALCF concept subsumption) A concept C1 subsumes a concept
C2 if and only if there exists an s-mapping from the essential graph of C1 to the essential
graph of C2 .

The above theorem gives a sound and complete algorithm for computing subsumption between TL-ALCF concepts (the proof is the same as the one for theorem 6.21). The subsumption problem is now PSPACE-hard, since satisfiability and subsumption for ALCF
concepts were proven to be PSPACE-complete (Hollunder & Nutt, 1990).

8. Extending the Expressivity for States
The following suggests how to extend the basic language to cope with important issues in
the representation of states. (i) Homogeneity allows us to consider properties of the world {
peculiar to states { which remain true in each subinterval of the interval in which they hold.
(ii) Persistence guarantees that a state holding as an effect of an action continues to hold
unless there is no evidence of its falsity at some time. An approach to the frame problem is
then presented, showing a possible solution to one of the most (in)famous problems in AI
literature. The following subsections shall be interested more in semantically characterizing
actions and states than on computational properties. The extensions proposed now to the
temporal languages are for having a full edged Description Logic for time and action.

8.1 Homogeneity

In the temporal literature homogeneity characterizes the temporal behavior of world states:
when a state holds over an interval of time t, it also holds over subintervals of t. Thus, if
494

fiA Temporal Description Logic for Reasoning about Actions and Plans

]

-

Simple-Stack(BLOCK)

r
OnTable(BLOCK)
-

r
OnBlock(BLOCK)

x

y

-

Figure 16: Temporal dependencies in the definition of the Simple-Stack action.
a block is on the table for a whole day, one can conclude that it is also on the table in the
morning. On the other hand, actions are not necessarily homogeneous. In the linguistic
literature a difference is made between activity and performance verbs. The distinction
comes out in the fact that activity verbs do have sub-events that are denoted by the same
verb, whereas performance verbs do not. Generally, activity verbs represent ongoing events,
for example to eat and to run, and can be described as homogeneous predicates; whereas
performance verbs represent events with a well defined granularity in time, such as to prepare
spaghetti. Performance verbs are an example of anti-homogeneous events: if they occur over
an interval of time t, then they do not occur over a subinterval of t, as they would not yet
be completed.
The language is extended by introducing the Homogeneity operator:

C; D ! rC

(homogeneous concept)

The semantics of homogeneous concepts is easily given in terms of the semantics of the
temporal universal quantifier: rC  2x (x (=; s; d; f ) ]). C @x. This means that rC
is an homogeneous concept if and only if when it holds at an interval it remains true at
each subinterval. In particular, 2x universally qualifies the temporal variable x, while the
temporal constraint (x (=; s; d; f ) ]) imposes that x is a generic interval contained in ].
Moreover, it is always true that rC v C , i.e., rC is a more specific concept than C .
Let us consider as an example a more accurate definition of the Basic-Stack action
(see Section 4.1.1):
:
Simple-Stack = 3(x y )(x m ])(] m y ). ((?BLOCK : rOnTable)@x u
(?BLOCK : rOnBlock)@y)
Figure 16 shows the temporal dependencies of the intervals in which the Simple-Stack
holds. The difference with the Basic-Stack action is the use of the homogeneity operator.
In fact, since the predicates OnTable and OnBlock denote states, their homogeneity should
be explicitly declared. The assertion Simple-Stack(i; a) says that a is an individual action
of type Simple-Stack occurred at interval i. Moreover, the same assertion implies that a
is related to a ?BLOCK, say b, which is of type OnTable at some interval j { meeting i { and
at all intervals included in j , while it is of type OnBlock at another interval l { met by i {
and at all intervals included in l:
Simple-Stack(i; a) =) 9b. ?BLOCK(a; b) ^9 j; l. m(j; i) ^ m(i; l) ^
8 ^; ^l. (=; s; d; f )(^; j ) ^ (=; s; d; f )(^l; l) !
OnTable(^; b) ^ OnBlock(^
l; b):
495

fiArtale & Franconi

]

-r y

Instant-Stack(BLOCK)

r
OnTable(BLOCK)
z

OnBlock(BLOCK)

-

Figure 17: Temporal dependencies in the definition of the Instant-Stack action.
Note that the Simple-Stack action subsumes the Instant-Stack action, whose temporal
dependencies are depicted in Figure 17:
:
Instant-Stack = 3(z y )(] f z )(] m y ). ((?BLOCK : rOnTable)@z u
(?BLOCK : rOnBlock)@y)
Subsumption holds because the class of intervals { obtained by homogeneity of the state
OnTable as defined in the Simple-Stack action { including x and all its subintervals is a
subset of the class of intervals over which the block is known to be on the table, according
to the definition of Instant-Stack { this latter class includes all the subintervals of z .
If the Instant-Stack action had been defined without the r operator, then it would not
specialize any more the Simple-Stack action. In fact, according to such a weaker definition
of Instant-Stack, specifying that the object is on the table at z does not imply that the
object is on the table at subintervals of z ; in particular, it is not possible to deduce any
more that the object is on the table at x and its subintervals, as specified in the definition of
Simple-Stack action. Moreover, the weak Instant-Stack action type would not specialize
the weak Simple-Stack action type { i.e., Basic-Stack { too. Thus, homogeneity helps
us to define states and actions in a more accurate way, such that important inferences are
captured.
As seen above, the definition of homogeneity makes use of universal temporal quantification. Remember that subsumption in a propositionally complete Description Logic
with both existential and universal temporal quantification is undecidable and it is still an
open problem if it becomes decidable in absence of negation (Bettini, 1993). The homogeneity operator is a restricted form of universal quantification. An even more restricted
form interests us here, where the concept C in rC does not contain any other temporal
operator (called simple homogeneous concept). The expressiveness of the resulting logic is
enough, for example, to correctly represent the homogeneous nature of states. In (Artale,
Bettini, & Franconi, 1994) an algorithm to compute subsumption in TL-F augmented with
the homogeneity operator is proposed. Even if a formal proof is still not available, good
arguments are discussed to conjecture its completeness. This would also prove decidability
of this logic and of the corresponding modal logics.

8.2 Persistence
This Section shows how our framework can be successfully extended in a general way to
cope with inertial properties. In the basic temporal language, a property holding, say, as
a post-condition of an action at a certain interval, is not guaranteed to hold anymore at
other included or subsequent intervals. This is the reason why we propose an extended
496

fiA Temporal Description Logic for Reasoning about Actions and Plans

]

- x

Load(GUN)

Loaded(GUN)

]



-:
-

Fire(GUN,TARGET)

:= Loaded(GUN)
x
or

Loaded(GUN)

z

Dead(TARGET)

y

Figure 18: Definitions of the actions Load and Fire.
formalism, in which states can be represented as homogeneous and persistent concepts.
As a motivation for introducing the possibility of representing persistent properties in the
language, this Section considers how to solve the frame problem, and in particular the
famous example of the Yale Turkey Shooting Scenario (Sandewall, 1994; Allen & Ferguson,
1994), formerly known as the Yale Shooting Problem.
An inertia operator \= " is introduced here. Intuitively, = C is currently true if it was
true at a preceding interval { say i { and there is no evidence of the falsity of C at any
interval between the current one and i. Thus, the property of an individual of being of type
C persists over time, unless a contradiction arises.
The formalization of the inertia operator makes use of the epistemic operator K (Donini,
Lenzerini, Nardi, Schaerf, & Nutt, 1992), in which KC denotes the set of individuals known
to be instances of the concept C 7 .

Definition 8.1 (Inertia) = C (j; a) iff
9i. start(i)  start(j ) ^ C (i; a) ^
8h. start(h)  end(i) ^ end(h)  end(j ) ! :K:C (h; a).
where start and end are two functions giving respectively the starting and the ending point
of an interval { conditions on endpoints are simpler and more readable than their equivalents
on interval relations; :K:C (h; a) means that it is not known that a is not of type C at
interval h. Furthermore, the following relation holds: 8a; j . C (j; a) ! = C (j; a); i.e., = C
subsumes C . The above definition can be captured by a temporal language equipped with
the epistemic operator { K { and the homogeneity operator { r:
= C  C t 3(x y) (x (b; m; o; fi; di) ])(x (s; si) y)(y fi ]).(C @x u r(:K:C )@y)
Two action types are defined, Load { with the parameter ?GUN { and Fire { with the
parameters ?GUN and ?TARGET (Figure 18):
=: 3x (] m x). ?GUN : Loaded@x
:
Fire = 3(x y z ) (] f x)(] m y )(] m z ).
(?GUN : := Loaded@x t ?TARGET : Dead@y) u ?GUN : :Loaded@z
The action Load describes loading a gun. The action Fire describes firing the gun against
a target: effects of firing are that the gun becomes unloaded and either the target is dead
Load

7. An epistemic interpretation T
is a pair (I ,W ) in which I is an interpretation and W is a set of interpretations such that (KC )I;W = J 2W (C J ;W ).

497

fiArtale & Franconi

i

- i1

gun)

Load(

fred :
 gun
- j1
j
gun -
j0
j2

gun)

Loaded(

Fire(

= Loaded(

,

)

)

gun)

Loaded(

fred)

Dead(

Figure 19: Actions instances in the Yale Shooting Problem.
or the gun was not loaded { possibly by inertia { before firing. The Yale Shooting Problem
considers the situation described by the following set of assertions (ABox):
Load(i; load-action ); ?GUN(load-action ; gun ); a(j; i); Fire(j; fire-action );
?GUN(fire-action ; gun ); ?TARGET(fire-action ; fred ):
i.e., at the beginning the gun is loaded; then, the action of firing the gun against the target
fred is performed. According to the semantics of the language, logical consequences of the
knowledge base  are:
 j= 9i1 . m(i; i1 ) ^ Loaded(i1 ; gun )
 j= 9j1 . m(j; j1 ) ^ :Loaded(j1 ; gun )
 j= 9j0 . f (j; j0 ) ^ = Loaded(j0 ; gun )
 j= 9j2 . m(j; j2 ) ^ Dead(j1 ; fred ):
i.e., (see also Figure 19) (i) the Load action makes the gun loaded; (ii) the Fire action
makes the gun unloaded at the end; (iii) since there is no evidence to the contrary, the gun
is still loaded at j0 by inertia; (iv) since the gun is not unloaded at j0 , the target fred must
be dead.
Since the inertia operator is useful to describe the behavior of properties, which are
characterized as homogeneous concepts, a simple way of representing persistence in the
context of homogeneous concepts is proposed.
Proposition 8.2 Let P be a property { i.e., P =: rP 0 is an homogeneous concept { and
 a knowledge base such that  6j= P (j; a). = P (j; a) is true in  { i.e.,  j= = P (j; a) {
if and only if two intervals i; k exist such that:  j= (start(i)  start(j ) ^ P (i; a)) and
 [ fs(i; k); f (j; k); P (k; a)g is satisfiable.
Proof. The entailment test verifies the first part of the definition of inertia, while the
satisfiability test verifies that, between the interval at which the system knows that the
individual a belongs to P { i { and the interval at which P (a) is deduced by inertia { j
{ does not exist an interval h at which the system knows that P (a) is false. Indeed, such
interval h would be related to the interval k by the relation in and since it is supposed
that P is homogeneous, the knowledge base with :P (h; a) ^ P (k; a) ^ in(h; k) would be
inconsistent.
2
The deduction P (j; a) ! = P (j; a) can be obtained as a particular case of the above stated
proposition.
498

fiA Temporal Description Logic for Reasoning about Actions and Plans

9. Related Works
The original formalism devised by Allen (1991) forms, in its very basis, the foundation for
our work. It is a predicate logic in which interval temporal networks can be introduced,
properties can be asserted to hold over intervals, and events can be said to occur at intervals. His approach is very general, but it suffers from problems related to the semantic
formalization of the predicates hold and occur (Blackburn, 1992). Moreover, computational properties of the formalism are not analyzed. The study of this latter aspect was, on
the contrary, our main concern.
In the Description Logic literature, other approaches for representing and reasoning with
time and action were proposed. In the beginning the approaches based on an explicit notion
of time are surveyed, and then the Strips-like approaches are considered. This Section ends
by illustrating some of the approaches devoted to temporally extend the situation calculus.
Bettini (1997) suggests a variable-free extension with both existential and universal
temporal quantification. He gives undecidability results for a class of temporal languages
{ resorting to the undecidability results of Halpern and Shoham's temporal logic { and investigates approximated reasoning algorithms. Basically, he extends the ALCN description
logics with the existential and universal temporal quantifiers, but, unlike our formalism,
explicit interval variables are not allowed. The temporal quantification makes use of a set
of temporal constraints on two implicit intervals: the reference interval and the current one.
In this framework, the concept of Mortal can be defined as:
:
Mortal = LivingBeing u 3(after). (not LivingBeing)
Schild (1993) proposes the embedding of point-based tense operators in a propositionally
closed Description Logic. He proved that satisfiability in ALCT , the point-based temporal
extension of ALC , interpreted on a linear, unbounded and discrete temporal structure, is
PSPACE-complete. His ideas were applied by (Fischer, 1992; Neuwirth, 1993) in the Back
system. Note that a point-based temporal ontology is unable to express all the variety of
relations between intervals.
Baader and Laux (1995) integrate modal operators for time and belief in a terminological
system looking for an adequate semantics for the resulting combined language. The major
point in this paper is the possibility of using modal operators not only inside concept
expressions but also in front of concept definitions and assertions. The following example
shows the notion of Happy-father, where different modalities interact:
[BEL-JOHN](Happy-father =: 9MARRIED-TO.(Woman u [BEL-JOHN]Pretty) u
hfuturei8CHILD.Graduate)
In this case, it is John's belief that a Happy-father is someone married to a woman believed
to be pretty by John, and whose children will be graduates sometime in the future. The
semantics has a Kripke-style: each modal operator is interpreted as an accessibility relation
on a set of possible worlds, while the domain of objects is split into (possible) different
domain objects, each one depending on a given world. This latter:choice captures the case of:
different definitions for the same concept { such as [BEL-JOHN](A = B ) and [BEL-PETER](A =
C ) { since the two formul are evaluated in different worlds. The main restriction is that
all the modal operators do not satisfy any specific axioms for belief or time. On the other
hand, the language is provided with a complete and terminating algorithm that should
499

fiArtale & Franconi

serve, as the authors propose, \...as a basis for satisfiability algorithms for more complex
languages".
There are Description Logics intended to represent and reasoning about actions following
the Strips tradition. Heinsohn, Kudenko, Nebel and Profitlich (1992) describe the Rat
system, used in the Wip project at the German Research Center for AI (DFKI). They use a
Description Logic to represent both the world states and atomic actions. A second formalism
is added to compose actions in plans and to reason about simple temporal relationships. No
explicit temporal constraints can be expressed in the language. Rat actions are defined by
the change of the world state they cause, and they are instantaneous as in the Strips-like
systems, while plans are linear sequences of actions. The most important service offered
by Rat is the simulated execution of part of a plan, checking if a given plan is feasible
and, if so, computing the global pre- and post-conditions. The feasibility test is similar
to the usual consistency check for a concept description: they temporally project the preand post-conditions of individual actions composing the plan, respectively backward and
forward. If this does not lead to an inconsistent initial, final or intermediate state, the plan
is feasible and the global pre- and post-conditions are determined as a side effect.
Devanbu and Litman (1991, 1996) describe the Clasp system, a plan-based knowledge
representation system extending the notion of subsumption and classification to plans, to
build an ecient information retrieval system. In particular, Clasp was used to represent plan-like knowledge in the domain of telephone switching software by extending the
use of the software information system lassie (Devanbu, Brachman, Selfridge, & Ballard,
1991). Clasp is designed for representing and reasoning about large collections of plan
descriptions, using a language able to express temporal, conditional and looping operators.
Following the Strips tradition, plan descriptions are built starting from states and actions,
both represented by using the Classic (Brachman, McGuiness, Patel-Schneider, Resnick,
& Borgida, 1991) terminological language. Since plans constructing operators correspond
to regular expressions, algorithms for subsumption integrate work in automata theory with
work in concept subsumption. The temporal expressive power of this system can capture
to sequences, disjunction and iterations of actions and each action is instantaneous. Furthermore, state descriptions are restricted to a simple conjunction of primitive Classic
concepts. Like Rat, Clasp checks if an instantiated plan is well formed, i.e., the specified
sequence of individual actions are able to transform the given initial state into the goal state
by using the Strips rules.
We end up by reporting on the efforts made by researchers in the situation calculus
field to overcome the strict sequential perspective inherent to this framework. Recent works
enrich the original framework to represent properties and actions having different truth
values depending not only on the situation but also on time. The work of Reiter (1996),
moving from the results showed by Pinto (1994) and by Ternovskaia (1994), provides a
new axiomatization of the situation calculus able to capture concurrent actions, properties
with continuous changes, and natural exogenous actions { those under nature's control. The
notion of uent { which models properties of the world { and situation are maintained. Each
action is instantaneous and responsible for changing the actual situation to the subsequent
one. Concurrent actions are simply sets of instantaneous actions that must be coherent,
i.e., the action's collection must be non empty and all the actions occur at the same time.
Pinto (1994) and Reiter (1996) introduce the time dimension essentially to capture both
500

fiA Temporal Description Logic for Reasoning about Actions and Plans

the occurrence of the natural actions, due to known laws of physics { i.e., the ball bouncing
at times prescribed by motion's equations { and the dynamic behavior of physical objects
{ i.e., the position of a falling ball. This is realized by introducing a time argument for
each action function, while properties of the world are divided into two different classes:
classical uents that hold or do not hold throughout situations, and continuous parameters
that may change their value during the time spanned by the given situation.
More devoted to have a situation calculus with a time interval ontology is the work of
Ternovskaia (1994). In order to describe processes { i.e., actions extended in time { she
introduces durationless actions that initiate and terminate those processes. As a matter of
fact, processes become uents, with instantaneous events { Start(Fluent) and Finish(Fluent)
{ which respectively make true or false the corresponding uent, and with persistence
assumptions that make the uent true during the interval. For example, in a blocks world
the picking-up process is treated as a uent with Start(picking-up(x)) and Finish(pickingup(x)) instantaneous actions that enable or falsify the picking-up uent.

10. Conclusions
The main objective of this paper was the design of a class of logical formalisms for uniformly representing time, actions and plans. According to this framework, an action has a
duration in time, it can have parameters, which are the ties with the temporal evolution
of the world, and it is possibly associated over time with other actions. A model-theoretic
semantics including both a temporal and an object domain was developed, for giving both
a meaning to the language formul and a well founded definition of the various reasoning
services, allowing us to prove soundness and completeness of the corresponding algorithms.
The peculiar computational properties of this logic make it an effective representation and
reasoning tool for plan recognition purposes. An action taxonomy based on subsumption
can be set up, and it can play the role of a plan library for plan retrieval tasks.
This paper contributes to exploration of the decidable realm of interval-based temporal
extensions of Description Logics. It presented complete procedures for subsumption reasoning with TL-F , TLU -FU and TL-ALCF . In addition, the subsumption problem for
TL-F was proven an NP-complete problem. The subsumption procedures are based on
an interpretation preserving transformation that operates a separation between the temporal and the non-temporal parts of the formalism. Thus, the calculus can adopt distinct
standard procedures developed in the Description Logics community and in the temporal
constraints community. To obtain decidable languages the key idea was to restrict the temporal expressivity by eliminating the universal quantification on temporal variables. While
a propositionally complete Description Logic with both existential and universal temporal
quantification is undecidable, it is still an open problem if it becomes decidable in absence
of negation. With the introduction of the homogeneity operator investigation of the impact
of a restricted form of temporal universal quantification in the language TL-F was begun.
Several extensions were proposed to the basic temporal language. With the possibility
to specify homogeneous predicates the temporal behavior of world states can be described
in a more natural way, while the introduction of the non-monotonic inertial operator gives
rise to some forms of temporal prediction. Another extension { not considered in this paper
{ deals with the possibility of relating an action to more elementary actions, decomposing
501

fiArtale & Franconi

it in partially ordered steps (Artale & Franconi, 1995). This kind of reasoning is found in
hierarchical planners like Nonlin (Tate, 1977), Sipe (Wilkins, 1988) and Forbin (Dean,
Firby, & Miller, 1990).

Acknowledgements
This paper is a substantial extension and revision of (Artale & Franconi, 1994). The work
was partially supported by the Italian National Research Council (CNR) project \Ontologic
and Linguistic Tools for Conceptual Modeling", and by the \Foundations of Data Warehouse
Quality" (DWQ ) European ESPRIT IV Long Term Research (LTR) Project 22469. The
first author wishes to acknowledge also LADSEB-CNR of Padova and the University of
Firenze for having supported part of his work. Some of the work carried on for this paper
was done while the second author was working at ITC-irst, Trento. This work owes a lot to
our colleagues Claudio Bettini and Alfonso Gerevini, for having introduced us many years
ago to the temporal maze. Special thanks to Achille C. Varzi, for taking time to review the
technical details of the paper and for his insightful comments on the philosophy of events,
and to Fausto Giunchiglia, for useful discussions and feedback. Thanks to Paolo Bresciani,
Nicola Guarino, Eugenia Ternovskaia and Andrea Schaerf for enlightening comments on
earlier drafts of the paper. Werner Nutt and Luciano Serafini helped us to have a deeper
insight into logic. We would also like to thank Carsten Lutz for the helpful discussions we
had with him about temporal representations. Many anonymous referees checked out many
errors of previous versions of the paper. All the errors of the paper are, of course, our own.

References

Allen, J. F. (1991). Temporal reasoning and planning. In Allen, J. F., Kautz, H. A., Pelavin,
R. N., & Tenenberg, J. D. (Eds.), Reasoning about Plans, chap. 1, pp. 2{68. Morgan
Kaufmann.
Allen, J. F., & Ferguson, G. (1994). Actions and events in interval temporal logic. Journal
of Logic and Computation, 4 (5). Special Issue on Actions and Processes.
Artale, A., Bettini, C., & Franconi, E. (1994). Homogeneous concepts in a temporal description logic. In F.Baader, M.Lenzerini, W.Nutt, & P.F.Patel-Schneider (Eds.),
Workshop Notes of the Int. Workshop on Description Logics, DL-94, pp. 36{41 Bonn,
Germany. DFKI, Saarbrucken. Tech. Rep. DFKI-D-94-10.
Artale, A., & Franconi, E. (1994). A computational account for a description logic of
time and action. In J.Doyle, E.Sandewall, & P.Torasso (Eds.), Proc. of the 4 th
International Conference on Principles of Knowledge Representation and Reasoning,
pp. 3{14 Bonn, Germany. Morgan Kaufmann.
Artale, A., & Franconi, E. (1995). Hierarchical plans in a description logic of time and
action. In A.Borgida, M.Lenzerini, D.Nardi, & B.Nebel (Eds.), Workshop Notes of the
Int. Workshop on Description Logics. DL-95, pp. 1{5 Roma, Italy. Tech. Rep. 07.95.
Also in the Workshop Notes of the IJCAI-95 Workshop on \The Next Generation of
502

fiA Temporal Description Logic for Reasoning about Actions and Plans

Plan Recognition Systems: Challanges for and Insight from Related Areas of AI",
Montreal, 1995.
Baader, F., Burckert, H.-J., Heinsohn, J., Hollunder, B., Muller, J., Nebel, B., Nutt, W.,
& Profitlich, H.-J. (1990). Terminological knowledge representation: a proposal for a
terminological logic. Technical memo TM-90-04, DFKI, Saarbrucken, Germany.
Baader, F., & Laux, A. (1995). Terminological logics with modal operator. In Proc. of the
13 th IJCAI, pp. 808{814 Montreal, Canada.
Bettini, C. (1993). Temporal Extensions of Terminological Languages. Ph.D. thesis, Computer Science Department, University of Milan, Italy.
Bettini, C. (1997). Time dependent concepts: Representation and reasoning using temporal
description logics. Data & Knowledge Engineering, 22 (1), 1{38.
Blackburn, P. (1992). Fine grained theories of time. In Working Papers of the 4th Intl.
Workshop on Semantics of Time, Space, Movement, and Spatio-Temporal Reasoning,
pp. 299{320.
Brachman, R. J., McGuiness, D. L., Patel-Schneider, P. F., Resnick, L. A., & Borgida, A.
(1991). Living with classic: When and how to use a kl-one-like language. In Sowa,
J. (Ed.), Principles of Semantic Networks. Morgan Kaufmann.
Buchheit, M., Donini, F. M., & Schaerf, A. (1993). Decidable reasoning in terminological
knowledge representation systems. Information Systems, 1, 109{138.
Calvanese, D., Lenzerini, M., & Nardi, D. (1994). A unified framework for class-based
representation formalisms. In Proc. of the 4 th International Conference on Principles
of Knowledge Representation and Reasoning Bonn, Germany.
Cohen, W., Borgida, A., & Hirsh, H. (1992). Computing least common subsumers in
description logics.. pp. 754{760 San Jose, CA.
De Giacomo, G., & Lenzerini, M. (1996). Tbox and abox reasoning in expressive description
logics. In Proc. of the 5 th International Conference on Principles of Knowledge
Representation and Reasoning, pp. 316{327 Boston, MA. Morgan Kaufmann.
De Giacomo, G., & Lenzerini, M. (1995). What's in an aggregate: Foundations for description logics with tuples and sets. In Proc. of the 13 th IJCAI Montreal, Canada.
Dean, T., Firby, J., & Miller, D. (1990). Hierarchical planning involving deadlines, travel
time and resources. Computational Intelligence, 6 (1).
Devanbu, P. T., & Litman, D. J. (1991). Plan-based terminological reasoning. In Proc.
of the 2 nd International Conference on Principles of Knowledge Representation and
Reasoning, pp. 128{138 Cambridge, MA.
Devanbu, P. T., & Litman, D. J. (1996). Taxonomic plan reasoning. Artificial Intelligence,
84, 1{35.
503

fiArtale & Franconi

Devanbu, P., Brachman, R., Selfridge, P., & Ballard, B. (1991). LASSIE { a knowledgebased software information system. Communication of the ACM, 34 (5).
Donini, F. M., Hollunder, B., Lenzerini, M., Spaccamela, A. M., Nardi, D., & Nutt, W.
(1992). The complexity of existential quantification in concept languages. Artificial
Intelligence, 53, 309{327.
Donini, F. M., Lenzerini, M., Nardi, D., & Nutt, W. (1995). The complexity of concept
languages. Tech. rep. RR-95-07, DFKI, Germany. A preliminary version appears in
Proc. of the 2nd International Conference on Principles of Knowledge Representation
and Reasoning (KR-91).
Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1994). Deduction in concept
languages: from subsumption to instance checking. Journal of Logic and Computation,
4 (4), 423{452.
Donini, F. M., Lenzerini, M., Nardi, D., Schaerf, A., & Nutt, W. (1992). Adding epistemic
operators to concept languages. In Proc. of the 3 rd International Conference on
Principles of Knowledge Representation and Reasoning, pp. 342{353 Cambridge, MA.
Fikes, R. E., & Nilsson, N. (1971). STRIPS: a new approach to the application of theorem
proving as problem solving. Artificial Intelligence, 2, 198{208.
Fischer, M. (1992). The integration of temporal operators into a terminological representation system. Kit-report 99, Technische Universtitat Berlin, Germany.
Gerevini, A., & Schubert, L. (1994). On point-based temporal disjointness. Artificial
Intelligence, 70, 347{361.
Halpern, J. Y., & Moses, Y. (1985). A guide to the modal logic of knowledge and belief:
Preliminary draft. In Proc. of the 9 th IJCAI, pp. 480{490 Los Angeles, CA.
Halpern, J. Y., & Shoham, Y. (1991). A propositional modal logic of time intervals. Journal
of ACM, 38 (4), 935{962.
Heinsohn, J., Kudenko, D., Nebel, B., & Profitlich, H. (1992). RAT: representation of
actions using terminological logics. Tech. rep., DFKI, Saarbrucken, Germany.
Hollunder, B., & Nutt, W. (1990). Subsumption algorithms for concept languages. Tech.
rep. RR-90-04, DFKI, Germany.
Kautz, H. A. (1991). A formal theory of plan recognition and its implementation. In Allen,
J. F., Kautz, H. A., Pelavin, R. N., & Tenenberg, J. D. (Eds.), Reasoning about Plans,
chap. 2, pp. 69{126. Morgan Kaufmann.
Lifschitz, V. (1987). On the semantics of strips. In The 1986 Workshop on Reasoning
about Actions and Plans, pp. 1{10. Morgan Kaufman.
McCarthy, J., & Hayes, P. J. (1969). Some philosophical problems from the standpoint
of Artificial Intelligence. In Meltzer, B., & Michie, D. (Eds.), Machine Intelligence,
Vol. 4, pp. 463{502 Edinburgh, UK. Edinburgh University Press.
504

fiA Temporal Description Logic for Reasoning about Actions and Plans

Nebel, B. (1990). Terminological reasoning is inherently intractable. Artificial Intelligence,
43, 235{249.
Nebel, B. (1991). Terminological cycles: Semantics and computational properties. In Sowa,
J. F. (Ed.), Principles of Semantic Networks, chap. 11, pp. 331{362. Morgan Kaufmann.
Neuwirth, A. (1993). Inferences for temporal object descriptions in a terminological representation system: Design and implementation. Kit-report 107, Technische Universtitat
Berlin, Germany.
Pinto, J. A. (1994). Temporal Reasoning in the Situation Calculus. Ph.D. thesis, Department
of Computer Science, University of Toronto.
Reiter, R. (1996). Natural actions, concurrency and continuous time in the situation calculs.
In Proc. of the 5 th International Conference on Principles of Knowledge Representation and Reasoning Boston, MA.
Renz, J., & Nebel, B. (1997). On the complexity of qualitative spatial reasoning: a maximal
tractable fragment of the region connection calculus. In Proc. of the 14 th IJCAI, pp.
522{527 Nagoya, Japan.
Sandewall, E. (1994). Features and Fluents. The Representation of Knowledge about Dynamical Systems, Vol. I. Oxford University Press.
Sandewall, E., & Shoham, Y. (1994). Non-monotonic temporal reasoning. In Gabbay, D.
(Ed.), Handbook of Artificial Intelligence and Logic programming. Oxford University
Press.
Schaerf, A. (1994). Reasoning with individuals in concept languages. Data & Knowledge
Engineering, 13 (2), 141{176.
Schild, K. D. (1991). A correspondence theory for terminological logics: Preliminary report.
In Proc. of the 12 th IJCAI, pp. 466{471 Sidney, Australia.
Schild, K. D. (1993). Combining terminological logics with tense logic. In Proceedings of
the 6th Portuguese Conference on Artificial Intelligence, EPIA'93.
Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions with complements. Artificial Intelligence, 48 (1), 1{26.
Schmiedel, A. (1990). A temporal terminological logic. In Proc. of AAAI-90, pp. 640{645
Boston, MA.
Smolka, G. (1992). Feature constraint logics for unification grammar. Journal of Logic
Programming, 12, 51{87.
Tate, A. (1977). Generating project networks. In Proc. of the 5 th IJCAI, pp. 888{893
Cambridge, MA.
505

fiArtale & Franconi

Ternovskaia, E. (1994). Interval situation calculus. In Workshop Notes of the ECAI-94
Workshop \Logic and Change", pp. 153{164 Amsterdam.
van Beek, P., & Cohen, R. (1990). Exact and approximate reasoning about temporal
relations. Computational Intelligence, 6, 132{144.
van Beek, P., & Manchak, D. W. (1996). The design and experimental analysis of algorithms
for temporal reasoning. Journal of Artificial Intelligence Research, 4, 1{18.
van Dalen, D. (1994). Logic and Structure. Springer-Verlag.
Venema, Y. (1990). Expressiveness and completeness of an interval tense logic. Notre Dame
Journal of Formal Logic, 31 (4), 529{547.
Weida, R. (1996). Closed Terminologies and Temporal reasoning in Descriptions for Plan
Recognition. Ph.D. thesis, Department of Computer Science, Columbia University,
New York, NY.
Weida, R., & Litman, D. (1992). Terminological reasoning with constraint networks and
an application to plan recognition. In Proc. of the 3 rd International Conference on
Principles of Knowledge Representation and Reasoning, pp. 282{293 Cambridge, MA.
Weida, R., & Litman, D. (1994). Subsumption and recognition of heterogeneous constraint
networks. In Proceedings of CAIA-94.
Wilkins, D. (1988). Practical planning. Morgan Kaufmann, San Mateo CA.

506

fi	
fffi 	


 ! #"$ % 	'&)(+*', ((-/.10(/24365!, 7

89:;<  =)(/>(/?9@BA:
%&=C,,4>(-

DFEHGJILKNM/OQPSRHGCTUWVFXG<Y

IWX/Z\[]KNM^NE`_

acbdbde1fNg#hjilkme$nhpo

qlrlr<s1t1u$vwjxzy{s|Bv}~C6r+s#xzff}

4B'B4\j 
 l+4Bcl/\) <4
c BBp$4Bj
 gefkme1fNg#hjiNg#e$n'f

/r<sw~]r+s1wxu$1ws1$xr<

]j')l//`j'cNBj'BBjB
\llc
!/1B!/S9      


hpoohhjBhp
 oh

v}}v$xzv!v}vj~C6r<s$#xzff}

4B'B414#l$
 ]l//z


4j/14Bj

)Nj
4949)9 `44$9B!S44 )/B 4c
CC9494 {B4{ C!4B   4/! 94 4 4
<4 c
4!  99  B B  ]c9 ff4C!4
9Q+94
  !)c S]  <l  )94 /]  !)#H
'4c9B!
9vs1}
 vw }v ff9cc 949 !Cc99 v1w 1}v
# !49494 
 \9/!44 Q)+ 9vs1}
ff ff 649  Q49H ] !B4
4 !]9 S94 S49Bc9]9B  )SB 4J {4 6  
94!  99  ` ]C! 4]9+!  9Q4  99) 49
4 49 CC ff  
94C!4Bj ! #4#CB 6    
Q !/ <4 +
  '4/!44



ff
fi

	
  

	

  





%
ff
fi


	
fi
 	
*)+'
	 	 ,
	



 

"!






	


  
('



+
 

&

ff
fi


$#

$'

 

-"./	0 21435j2671 0
8:9<;>=*?	@A9B=DCE?FDF<GHF<I=DJ,KLDCH;MNGHO&KPQMR?SAKJTGHF@A;JU;OV@WPXKJYGHF<Z<[DOV@VJ,GE?C\JUKL]K@AGE^O_Y`aL*?OAGH^>b2;J,OAGEKFBKP
@A9<GHOQ=<J,KLDCH;Mc^KF<OAGHOV@AOKPedDF<Z<GHFDI&?fOA;g[<;F<^;:KP\MhK@AGEKF<OiPXKJj?&JUKL]K@PEJ,KMk?TOV@,?	J,@^KFldDI[<JU?	@AGHKF
@AKY?mIGHb2;FnIK2?Cl^KF<dDI[lJU?	@AGHKFnom9DGHCH;i?ffb2KGHZ<GHF<IW^KCHCHGEOAGHKF<O\oGp@A9n?FqfKLDOA@,?^CH;O4GHF>@A9<;;FbGpJ,KF<Mh;F@_
`kOAGHM=*CH;fb2;JUOAGHKFrKP@A9<;n=<J,KLDCH;Mts\@A9D?	@TKPi=*CE?F<F<GHFDIh@A9<;MhK@AGEKFrKPj?u=\KGHF@&J,KL\K@Y?MRKF<I
vwxZ<GHMR;F<OAGHKFD?CW=\KCpq9<;ZlJU?CWKLDOV@,?^CE;OsY9D?OyL\;;Fz=<J,Kb2;Z{@AK|L\;r}"~wx^KM=DCE;@A;|x?F<Fqs2U_
 ;F<;J?CHCpqYOV=\;ff?	GEF<Is@A9D;^KM=*CH;GH@xqTKP@A9<;i=<J,KLDCH;MGHO+;=\KF<;F@AGE?CGHFf@A9<;jF[<M>L\;JKPlZ<;IJU;;OfiKP
PJ,;;Z<KM74KP]@A9<;:J,KL\K@sl?F<Zh=\KCpqF<KMhG?ClGHF@A9<;F[<M>L\;JKP]KLDOV@,?^CE;OjGEF@A9<;m;FbGpJ,KF<Mh;F@_
KF<O,;g[<;F@ACpqs2d*F<Z<GHF<IW?&=*?	@A9PXKJ?WJ,KL\K@ omGp@A9MR?FqRMhKJU;@A9D?Fd<b2;fiGHFh?F;FbGpJ,KF<Mh;F@
omGp@A9OA;b2;J?CKLDOV@,?^CH;OhGEOsQGHF<Z<;;Z"si?b2;J,qZ<GHR^[<Cp@>=<J,KLDCH;Mt_F<PXKJA@A[<F*?	@A;CpqsiMR?FqJ,;ff?CEGHOV@AGH^
GHF<ZD[<OV@VJ,GE?C+=<J,KLDCE;MhO&Z<;ff?C omGp@A9J,KL\K@AOYKP?	@fCH;ff?OV@YOAGpB?F<Zr9[DF<ZlJ,;Z<O&KPjKLDOV@,?^CH;Off_ib2;F
oKJ,OA;slKPE@A;F@A9D;;FbGHJ,KF<Mh;F@GHOjZlqFD?MhGH^GHF@A9<;O,;F<OA;@A9D?	@jO,KMh;&KP\@A9<;WKLDOV@,?^CH;OMR?ffqRMhKb2;s
@A9<;J,;LqyP[<JA@A9<;JJ,;g[<GpJ,GEF<IT@A9D?	@F<;o=*?	@A9<O:L];YPXK[<F<ZyGEFb2;JAqyOA9<KJA@:^KM=*[l@AGHF<In@AGHMh;Off_


e



j



x

E 

, ((- j %% !=		= /	 N	!fi;+	 :!
%&% 

&/%1% / =

fiy

\DW$us<|t"]

s|Bv}



v99 v}v

 Ft@A9<GEO:=*?	=\;Jffs*oj;f=<J,;OA;F@W?RF<;o?	=<=DJ,K2?^,9B@AKh=e?	@A9=DC?F<F<GHF<Is<^ff?CECH;Zt@A9<;A`mJUGE?Z<F<;O^CH;o
?CHIKJ,GH@A9<M ,V _8:9<;?	=<=<J,K2?^U9GHO^KM=DCE;@A;CpqI;F<;JU?C:?FDZ?	=D=DCHGH;On@AKr?L<J,K2?ZJU?F<I;KPm=*?	@A9
=DCE?FDF<GHF<IY=DJ,KLDCH;MhOff_ WKffo;b2;JffsDGH@jGHOj=*?	J,@AGH^[<CE?	J,CHqZD;OAGHIF<;Zu@AKndDF<Z=*?	@A9<OPXKJJ,KL\K@AOoGp@A9MR?Fq
uGHFyZlqFD?MhGH^&;FbGHJ,KF<Mh;F@AO_
8:9D;T[<Cp@AGHM?	@A;fIK2?CfiKPQ?=DCE?FDF<;JmGHOm@AKRdDFDZ?=*?	@A9BPJ,KM@A9<;>GHF<Gp@AG?C\=\KOAGp@AGHKF@AK@A9<;T@,?	JUI;@_
WKffo;b2;Jffs"om9<GECH;TOA;ff?	J,^U9<GHF<IuPXKJW@A9DGHOm=e?	@A9\s]@A9<;n?CEIKJ,Gp@A9<MMR?qB^KF<OAGHZD;JW^KCHCE;^@AGHF<IuGHF<PXKJ,MR?	@AGHKF
?	L\K[l@T@A9<;RPEJ,;;ROV=*?^;?FDZ?	L\K[l@T@A9<;hO,;@fKPj=]KO,OAGpLDCH;n=*?	@A9DOT@A9D?	@fCEGH;GHFr@A9D?	@>PEJ,;;ROV=*?^;_8:9<;
`J,GE?Z<F<;Ofi^CH;o?CHIKJ,GH@A9<M@VJUGH;O+@AK&Z<KL\K@A9?	@@A9<;O,?MR;i@AGHMR;fi?OA[lLlw?CHIKJUGp@A9<M^ff?CHCH;Zl+~<+\
^KCHCH;^@AOGHF<PXKJ,MR?	@AGHKFh?	L\K[l@4@A9D;:PEJU;;:OV=*?^;oGp@A9GHF<^J,;ff?O,GHF<ICpq>dDF<;J,;OAKCE[l@AGHKF\som9DGHCH;s2GHF=e?	JU?CHCH;Cs
?F?CHIKJUGp@A9<M^ff?CHCE;Zffl+]eK=<=\KJA@A[<F<GEOV@AGH^ff?CHCpqh^U9<;^AO:o9<;@A9<;J:@A9<;T@,?	J,I;@^ff?FyL\;&J,;ff?^U9<;Z\_
8:9D;l+~D\?CHIKJ,Gp@A9DMoKJAOfLqr=DCE?^GHFDItCE?F<Z<M?	JAO>GHF@A9D;OA;ff?	JU^,9<;ZOV=*?^;GHFOA[<^,9?
o?qB@A9D?	@f?u=*?	@A9rPJ,KM@A9<;GHF<Gp@AG?C+=\KOAGp@AGEKF@AKt?FqBC?F<Z<MR?	JAGHOWF<KomF\_  FKJ,Z<;JY@AKyCE;ff?	J,F?O
Mn[<^,9h?Ofi=\KOAOAGpL*CH;i?	L\K[l@fi@A9<;PEJ,;;OV=e?^;s@A9<;l+~<+\n?CHIKJ,Gp@A9DM@VJ,GE;O@AK&OV=DJ,;ff?Zn@A9<;CE?F<Z<MR?	J,O
[<F<GEPKJ,MRCpqn?CHCeKffb2;J@A9D;mOV=*?^;_8KnZ<KT@A9DGHOsGH@i=DCE?^;O@A9<;WCE?F<Z<M?	JAOj?OPX?	J?Oj=]KO,OAGpLDCH;PJ,KMkKF<;
?F<K@A9<;J_DKJ>;ff?^,9F<;oCE?F<ZDMR?	JA=DJ,KZD[<^;ZBLqB@A9<;hl+~D\B?CEIKJ,Gp@A9<Mys\@A9D;Rffl+]e?CHIK	w
J,Gp@A9DMc^,9D;^AOYomGp@A9?>CHK^ff?C*MR;@A9<KZe om9<;@A9D;J@A9<;@,?	J,I;@^ff?FL];mJ,;ff?^,9D;ZuPJ,KM@A9D?	@CE?F<Z<MR?	J,*_
 K@A9t@A9<;>l+~<+\?F<Zl+]eB?CEIKJ,Gp@A9<MhO?	J,;&=\KOA;Z?OmK=D@AGHMhGHff?	@AGHKF=DJ,KLDCH;MhOff_
8:9D;Y`mJ,G?Z<F<;O^CH;o?CEIKJ,Gp@A9<MGEOf7*"?F<ZeV$
_m8m9<;T?CHIKJ,Gp@A9DMGHO;R^GH;F@mGHF@ojKOA;F<OA;Off
7?2Q=\;J,GEMh;F@AOfiOA9<Ko@A9D?	@@A9D;?CEIKJ,Gp@A9<MGHO ?	LDCH;@AKOAKCHb2;=e?	@A9>=*CE?F<F<GHFDI=<J,KLDCH;MROP?OV@
;FDK[<I9@AKBMRKffb2;y?O,G%?	J,MGHF?tJU;ff?CHGHOV@AGH^u?F<ZZlqF*?MhGH^R;FbGpJ,KF<Mh;F@fo9<;J,;
?FDK@A9<;JOAGpJ,KL\K@:GHO[<OA;Z?O?MhKbGHFDIhKLDOV@,?^CH;_
Le  @GEOQo;CHC<OA[<GH@A;ZRPXKJi=*?	JU?CECH;C<GHM=*CH;Mh;F@,?	@AGHKF?F<ZOA9<KomOiOAGEIF<GpdD^ff?F@iOV=\;;Zwx[l=Rom9<;F
@A9D;TF[DMfL\;J:KP=<J,K^;OAOAKJ,O:GEF<^J,;ff?OA;O_
_m8m9<;T?CHIKJ,Gp@A9DMGHOI;F<;JU?C+GHF@xoKROA;FDOA;O
7?2  @:MR?ffqL\;W[<OA;ZyPXKJm?nomGHZ<;JU?F<I;YKPfi?	=<=DCHGH^ff?	@AGEKF<OGHFuJ,KL\K@AGH^OomGp@A9CHGp@V@ACE;W?Z<ZDGp@AGHKFD?C
;]KJA@:@AKu?ZD?	=<@:Gp@_
Leu GHF*?CHCpqse@A9<;?CHIKJ,Gp@A9<MGHOWI;F<;JU?C4GEF@A9D?	@YGp@&MR?ffqL\;n?Z*?	=<@A;ZrPKJT?uCE?	J,I;nJU?FDI;nKP
O,;ff?	J,^,9B=<J,KL*CH;MhOmGEF^KF@AGHF[DK[<OmOV=e?^;O@A9*?	@&?	J,GEOA;fGHFdD;CHZ<O:@A9D?	@&?	JU;>F<K@J,;CE?	@A;Z@AK
JUKL]K@AGE^O_
8:9D;=*?	=\;JfGHOTKJ,I2?FDGH;Z?O>PKCHCEKffomO_;^@AGHKF=<J,;OA;F@AOT@A9<;h=*?	@A9r=DCE?FDF<GHF<I=<J,KLDCE;M?F<Z
Z<GHO,^[<OAOA;OfJ,;CE?	@A;ZojKJ,*_;^@AGHKFvt=DJ,;OA;F@AO>@A9D;R=<JUGHF<^Gp=DCE;nKP:@A9<;`J,GE?ZDF<;Of^CH;o?CHIKJUGp@A9<My_
;^@AGHKFyZ<;OA^J,GpL\;OW@A9<;h?	=<=*CHGH^ff?	@AGHKFKP@A9<;h?CHIKJ,Gp@A9DM@AKt?OAG%fi+B?	J,MGEF?Z<qFD?MRGH^;FbG%w
J,KF<MR;F@_ GHFD?CHCHqs"l;^@AGHKFu^KF<^CH[<ZD;O&@A9<;=*?	=\;JYomGp@A9r?Z<GHO,^[<OAOAGHKFBKP@A9<;^KF@VJUGpLD[l@AGHKFDOWKP
K[lJ?	=D=<J,K2?^,9"s<@A9<;YMR?GHFZ<GpR^[<CH@AGH;OGEFb2KCpb2;Z"s*?F<Zy=\KOAOAGHLDCH;mGEM=<J,Kb2;Mh;F@AOKP K[lJ:Mh;@A9<KZ\_
*fi7ff$f7fi(]$	*7	W	U7,4W(	\ (	n,Qe7U\	(VR]	+7
W(	,xU	j	X7\$(	$7	4,(7ff	U :U4	+ UV,(e]	4:A$&:(:pV
:]	*]e7 	:$e\7	U	7fi,(7l$	fi($UmU(:7fi	U(ffU($
(|i7	VU p"x$ \7	,"	Q$W\$	:(&U7\7 	:$\A7WUx
	ff


fifi
.



 

$vu]}s11t$vWwvLswe1}%l1r

0i0 6 0 2 1 

?Fqb2;J,OAGHKF<OKPm@A9<;u=*?	@A9=DCE?F<F<GEF<Iy=<J,KL*CH;M;GHOA@_`F;l9D?[<OV@AGpb2;^CE?O,OAGpdD^ff?	@AGHKFKP@A9D;OA;
=<J,KL*CH;MhO:?F<ZtKPfi@A9<;fMh;@A9<KZ<O:Z<;b2;CHK=\;Zy@AKOAKCpb2;Y@A9D;M ^ff?FtL\;YPXK[<F<ZyGHF?OA[lJAb2;qLquo?F<I
?F<Z`W9[	S,?B2U"_ !;^,9<KKOA;@AKGHCHCH[DOV@VJU?	@A;fK[<JTZ<GHOA^[<O,OAGHKFtoGp@A9B?u=e?	JA@AGH^[<CE?	J&^ff?OA;_n`J,KL\K@
?	J,M GHO&=*CE?^;Zr?MhKF<It?OA;@TKPjKLDOV@,?^CH;Off_  GHb2;F?FrGHF<Gp@AGE?C ?F<Z?dDFD?C =\KOAGH@AGHKFKP@A9<;nJ,KL\K@
?	J,Myse@A9<;>=<J,KLDCE;M GHO@AKRd*F<ZB?OA;@&KPQMhK@AGHKFDOm@A9D?	@WomGHCEC"CH;ff?Z@A9<;fJUKL]K@W@AKuMhKffb2;nL\;@xo;;F@A9<;
@xoKh=\KOAGH@AGHKF<OjomGp@A9<K[<@^KCECHGHZ<GHFDIfomGH@A9@A9<;TKLDOV@,?^CE;O_
8fiK>Z<J,Gpb2;@A9<;mJ,KL\K@j?MhGEZ<OV@Q@A9<;KLDOV@,?^CH;Offs;ff?	JUCpqhMh;@A9<KZ<Om  J,KKOs]v2iZDGpJ,;^@ACpq[<OA;Z@A9<;
v*+MhKZD;CHOKP]@A9D;mJ,KL\K@?F<ZKP"@A9<;mKLDOA@,?^CH;Oj@AK>d*F<Z?fOAKCH[l@AGHKF"sG_;_ps@A9<;qh^KF<O,GHZ<;J,;Z@A9<;
AK=\;JU?	@AGHKFD?Cjv OV=*?^;  _  F@A9<GEOfOV=*?^;s @A9<;R=*?	@A9=DCE?F<F<GEF<Iu=<J,KLDCH;M^KF<OAGHOV@AO>KPdDF<Z<GEF<I@A9<;
MhKb2;Mh;F@AOWKP4?^KM=DCH;vOV@VJ,[<^@A[lJU;@A9<;YJ,KL\K@UjGHF?^CE[l@V@A;J,;ZvOV=*?^;_
`|MR?SAKJfi?Z<b?F<^;o?O+@AK;=<JU;OAO"@A9<;i=<J,KLDCE;MGHFf?F<K@A9D;JfiOV=e?^;QF<KffomFn?O+@A9D;i^KFldDI[<JU?	@AGHKF
OV=*?^;sQZD;F<K@A;ZL$
q #& %Kff?F<K	(w '";) J,;sW+ *U_  F@A9<GEOfOV=*?^;si@A9<;h=\KOAGp@AGEKFKJ^KFldDI[lJ?	@AGHKF*
KP&?J,KL\K@hGHO^KM=DCE;@A;CpqZD;@A;J,MhGHF<;ZLq?OAGHF<ICH;=\KGHF@h9D?bGHF<I ,GHF<ZD;=];FDZ<;F@n=*?	JU?MR;@A;J,O
?Oy^KKJ,Z<GHFD?	@A;Off_8:9<;=]KO,Gp@AGHKF<O@A9D?	@t?	J,;F<K@=D9qOAGE^ff?CHCpqCH;I2?CL\;^ff?[<OA;KP>?^KCHCEGHOAGHKF*R?	J,;
J,;=<JU;OA;F@A;Z Lq|=*?	JA@AGH^[DCE?	JRJ,;IGEKF<OKP.#fis?FDZ ?	J,;B^ff?CHCH;Z/#1032547672%76_  F@A9<;B^KFldDI[<JU?	@AGHKF
OV=*?^;s@A9<;t=*?	@A9|=DCE?FDF<GHF<I=<J,KLDCE;M ^KF<OAGHOA@AOhKPWdDF<Z<GHFDIB?r^KF@AGHF[<K[<Oh^[<JAb2;J,;=<JU;OA;F@AGEF<I?
=*?	@A9uPKJ:?>OAGHF<ICE;:I;KMh;@VJ,GH^ff?Ce=\KGHF@U @A9D?	@&Gi^KF<F<;^@AOj@A9<;=\KGHF@AOiJ,;=<J,;O,;F@AGHFDIT@A9<;WGHF<Gp@AG?CD?F<Z
@A9<;dDF*?C^KF<dDI[lJU?	@AGHKFKP*@A9<;jJ,KL\K@s?F<ZyGHGX"Z<K;O4F<K@QGHF@A;J,OA;^@i?F8q #109254:672ffp: 6_48:9<GHOfiMh;@A9<KZ
@VJU?Z<;OW?hOAGEM=DCHGpd*^ff?	@AGHKFKP@A9D;T=*?	@A9=DCE?F<F<GEF<If=DJ,KLDCH;MGH@mOA;ff?	J,^U9<;OW?h=*?	@A9PXKJW?ROAGHF<ICE;&=]KGEF@U
?I2?GHF<OA@?9<GHI9<;JAwxZ<GHMh;F<O,GHKFD?C4OA;ff?	J,^U9OV=*?^;B@A9<;Z<GHMh;F<O,GHKFKP # GHO>@A9<;uF[DMfL\;Jnfi+KP@A9<;
J,KL\K@UW?F<Z?I2?GHF<OA@TMhKJ,;^KM=DCH;OA9D?	=\;O&KPKL*OV@,?^CH;Ob2;JAqOAGHM=*CH;f=*9qOAGH^ff?C KLDOV@,?^CE;OYMR?q
J,;OA[DCp@GHFb2;JAqy^KM=DCH;
 #1032547672%7 6AU_
<KJQ;<?M=DCE;s2CH;@fi[DO^KF<O,GHZ<;J @A9<;=DCE?FD?	J4?	J,M KP< GHI[lJ,;_  @AOfi=\KOAGp@AGEKF>?MhKF<IW@A9<;jKLDOV@,?^CE;O
GHO4@AK@,?CECpq>F<KomFRKF<^;:@A9<;b?CH[<;OQKPe@A9<;m?FDICH;O4L\;@xo;;FRGp@AOiCHGHFlO ;=<?>@; , 4?	J,;:F<KomF\_ 8:9[<OsPXKJ
;ff?^,9=*?GpJ ;=<?>@; , Us+GH@&GEO&=]KO,OAGpLDCH;f@AKyZ<;^GHZ<;nom9D;@A9<;JY@A9<;J,KL\K@Y^KCHCHGHZD;OoGp@A9B@A9<;OA[lJAJUK[<F<Z<GHFDI
KLDOV@,?^CE;O_8:9DGHOmGHOmom9D?	@Woj;>Z<GHZGHF GHI[lJU;nh@AKRJ,;=DJ,;OA;F@m@A9<;nMR?	=<=DGEF<IL\;@xo;;FB@A9<;T=D9qOAGH^ff?C
KLDOV@,?^CE;OWGEF@A9<;nK=\;JU?	@AGHKFD?CfiOV=*?^;?F<Z@A9<A; #1032547672%7 6U._ BWKffoTs\LqMhKbGHFDI?=\KGHF@&?CHKF<I@A9<;
^[lJAb2;"SAKGHF<GHF<D
I 	C; E?FDF
Z 	C; GmKF<;omGHCHC?CEOAKmZ<;d*F<;?^KCHCHGHO,GHKFwxPJ,;;QMhK@AGHKFnPXKJ@A9<;=DCE?F*?	Jfi?	J,ML\;@xo;;F
@A9<;^KJAJ,;OV=\KF<Z<GEF<Iu=]KO,Gp@AGHKF<8O HR? =C; ET?FDF
Z Hh 	C; G2TGHFr@A9<;K=\;JU?	@AGHKFD?CQOA=*?^;_t8:9<GHOY^[<JAb2;RGHO>KF<;
OAKCH[<@AGHKF@AK@A9<GHO=e?	JA@AGH^[<CE?	J=e?	@A9y=DCE?F<FDGHF<I>=<J,KLDCH;Mt_
`aJU;^;F@&@VJ,;F<ZBGHF@A9<;fd*;CHZGHO@AKu^KF<OAGHZ<;JW@A9<;V@VJU?SA;^@AKJAqOV=e?^;  D;JAL*?^,9"s4J I2:o9<;J,;
?o9<KCH;&=*?	@A9GHOJ,;=<JU;OA;F@A;ZLq?hOAGHF<ICH;&=\KGHF@_8m9<;T^KKJ,Z<GHFD?	@A;OKPfi@A9<GHO=\KGHF@?	J,;Y@A9D;Tb?CH[<;O
KPfi@A9<;T=e?	JU?Mh;@A;J,OZ<;dDF<GHFDI@A9<;TOA[<^^;O,OAGpb2;TMhKb2;Mh;F@AOWKP @A9D;YJ,KL\K@_jDKJWGHFDOV@,?F<^;sD@A9D;TCHGHOV@:KP
OA[<^^;O,OAGpb2;:^KMhM?F<Z<OiO,;F@Q@AKf@A9<;J,KL\K@i^KF@VJ,KCHCE;JiGHF<Z<;;ZR;F<^KZD;m?Fh;F@AGpJ,;=*?	@A9KPe@A9<;J,KL\K@_
 Ft@A9<GHOOV=*?^;se@A9<;T=e?	@A9=DC?F<F<GHF<In=<JUKLDCH;M GEO:J,;Z<[D^;Zt@AKh@A9D;fOA;ff?	J,^U9PKJY?hOAGEF<ICH;Y=\KGHF@L_ K&F<^;
?I2?GHF\soj;@VJU?ZD;:?&OAGHM=DCEGpdD^ff?	@AGHKF>KPD@A9<;=e?	@A9=DCE?F<F<GEF<I:=<JUKLDCH;M OA;ff?	J,^U9<GHF<IYPXKJ?W=\KGHF@U?I2?GEF<OV@
?T9DGHI9<;JiZ<GEMh;F<OAGHKFKP]@A9<;mOA;ff?	J,^,9uOV=*?^;>@A9<;mZDGHMh;F<OAGEKFKP*@A9D;:@VJU?SA;^@AKJAqROV=e?^;:GHOQ@A9D;:F[DMfL\;J
KP=*?	JU?Mh;@A;J,OfF<;;Z<;Z@AKtOV=\;^GHPq^KM=*CH;@A;Cpqr?uom9<KCH;n=*?	@A9*U_<KJn;l?Mh=DCH;s"GHFr GHI[lJU;s@A9<;
=*?	@A9yL\;@oj;;M
F =C; N:?F<O
Z ;=C PY^ff?FL\;&J,;=<J,;OA;F@A;ZyLq?=]KGEF@:GHFt?hOA;b2;FwxZ<GHMR;F<OAGHKFD?C]OV=e?^;TOAGHM=*Cpq
Lqu^KF<O,GHZ<;J,GHFDI>@A9D;YCH;F<I@A9tKP Gp@AOOA;b2;FOA;IMh;F@AO_
	?Q

fiy

\DW$us<|t"]

s|Bv}

XY[\&Z ab_



v99 v}v

RJS

RJW
\S
XY[\^Z ]`_
R+V

XY[\Z c9_

\T
RUT

 GHI[lJ,;i`@xoKfi+u?	J,M =DCE?^;Zt?MhKF<IRKLDOV@,?^CH;OGHF@A9<;TK=\;JU?	@AGHKF*?C\OV=*?^;
dfe^gih ki jmlei)Dnon#opj1e1#h

 HC KLe?CD?	=<=<J,K2?^U9<;O?	J,;:^CE?OAO,GH^ff?CHCpqZ<GpbGHZ<;ZGHF@AKY@xoKfM?GHFh^CE?OAO,;OGfiJ,;@VJU?^@AGEKFRMh;@A9<KZ<Os?F<Z
GHGZ<;^KMh=]KO,Gp@AGHKFMh;@A9<KZ<O_  F@A9<;J,;@VJU?^@AGEKFRMh;@A9<KZ<OsKF<;@VJ,GH;O4@AKTJ,;Z<[<^;@A9<;ZDGHMh;F<OAGEKFnKP
@A9<;>GHF<Gp@AGE?C]=DJ,KLDCH;M LqyJ,;^[<J,OAGpb2;Cpq^KF<OAGEZ<;J,GHF<IO,[lLlwxMR?F<GEPKCHZDOKP @A9<;f^KFld*I[lJU?	@AGHKFBOV=*?^;_  F
@A9<;&Z<;^KM=\KOAGH@AGHKFMR;@A9<KZDOslKF<;W@VJ,GH;Oj@AK^,9D?	JU?^@A;JUGH;Y@A9<;J,;IGHKF<OKP"@A9<;&^KFldDI[lJU?	@AGEKFuOA=*?^;
@A9D?	@?	JU;PEJ,;;KPeKLDOV@,?^CE;O_  K@A9Mh;@A9<KZ<O4;F<Zh[<=omGp@A9?Y^CE?OAO,GH^ff?ClIJU?	=D9OA;ff?	JU^,9RKb2;Jj?&Z<GEOA^J,;@A;
OV=*?^;_  F=<J,GEF<^Gp=DCH;s@A9<;O,;>Mh;@A9DKZ<OW?	J,;@2qsr*pxWL\;^ff?[<O,;T@A9<;qyomGECHC\dDF<ZB?=*?	@A9BGHP4KF<;>;GEOV@AO
?F<ZomGHCHC\OV@AGECHC\@A;J,MhGEFD?	@A;fGHF?hdDFDGp@A;&@AGHMh;>GHP4?=*?	@A9Z<K;OWF<K@W;GHOA@_:F<PXKJA@A[<F*?	@A;Cpqs*^KM=*[l@AGHF<I
@A9<;RJ,;@VJU?^@AGHKFKJf@A9<;Z<;^KM=\KOAGp@AGEKFIJU?	=D9GHOf?F}"~wx^KM=DCE;@A;=<J,KLDCE;Myf@A9<;^KM=DCH;lGp@qKP
@A9<GHOi@,?OVhIJ,KomO;=\KF<;F@AGE?CHCpq?Oi@A9D;mF[<MfL\;JjKP"GHF<^J,;ff?OA;Ox?F<Fqs*2U_KF<OA;g[<;F@ACpqs
@A9<;OA;T=DCE?F<F<;JUO?	JU;Y[<OA;ZyKF<CpqPXKJ:J,KL\K@AO9D?bGHF<Ih?hCHGHMhGp@A;ZuF[<MfL\;JT@A9lJU;;TKJmPXK[lJjKP D_  F
?Z<Z<GH@AGHKF\sl@A9<;q?	J,;>OACHKo?F<Z^ff?FKF<CHqL\;T[<O,;ZK*wxCHGHF<;@A9<;T=DC?F<F<;J:GHOGHFb2K2;ZomGp@A9?hMhKZ<;C
KP\@A9<;;FbGpJ,KFDMh;F@sGp@=<J,KZ<[<^;O?f=DCE?FR@A9D?	@GHOQ=e?OAOA;Z@AKf@A9<;J,KL\K@^KF@VJ,KCHCE;Jjo9<GH^,9"sGHFR@A[lJ,F\s
;l;^[l@A;O&Gp@_  FtI;F<;J?CsD@A9<;f@AGHMh;TFD;^;OAO,?	JAqy@AKu?^,9<GH;b2;f@A9<GHOGHO:F<K@WOA9<KJA@m;FDK[<I9t@AKu?CHCHKffo@A9<;
J,KL\K@@AKhMhKb2;fGHFt?hZlqF*?MhGH^&;FbGpJ,KFDMh;F@_
dfebd

 e



tFuvwjNe1i

ie1f f#'f



ie1f#f#hpo

K&F<;Wo?q@AK^KM>L*?	@:@A9<;Y^KM=*CH;GH@xqKP+@A9<;&=DJ,KLDCH;M GHO@AK@VJU?ZD;Y^KM=DCH;@A;F<;O,Om?I2?GHF<OV@=\;J,PXKJVw
M ?F<^;_Q8fiKZ<KT@A9DGHOs@A9<;CEK^ff?C*=DCE?F<FD;J,O?	J,;WI[<GHZD;ZhLqh@A9<;WIJU?Z<GH;F@KP+?n^KOV@PX[<F<^@AGHKFB[<OA[D?CECpq
R
@A9<;i[<^CHGEZ<;ff?FhZ<GHOA@,?F<^;:@AK>@A9<;mIK2?CXi?F<ZR@,?	2;&GHF@AKn?^^K[<F@@A9<;^KF<OV@VJU?GEF@AOjGHF@VJ,KZD[<^;ZhLq@A9<;
KLDOV@,?^CE;O@AK?ffb2KGHZ@A9<;M D?ffb2;JSAKFyx8K[<J,FD?OAOAK[DZ\s*+*U_GHF<^;@A9<;=*?	@A9=DCE?FDF<GHF<IY=DJ,KLDCH;M
GHOm}+~wx^KM=DCH;@A;sDF<KomGHF<Ih@A9<;T^KOV@WP[<FD^@AGHKF\s<Gp@:GHO?Cpo?ffqOm=\KOAOAGpLDCE;@AKRZ<;OAGHIF?RZ<;^;=<@AGpb2;f;FbG%w
J,KF<MR;F@om9<;J,;Y@A9D;>Mh;@A9DKZomGHCHC\I;@&@VJU?	=<=\;ZtGEF?hCEK^ff?CfiMhGHF<GEM>[<Mt_WKffo;b2;Jffs\@A9<;OA;nMh;@A9<KZ<O
?	J,;n[<OA;PX[<C+GEFMR?FqGHF<ZD[<OV@VJ,GE?C"?	=D=DCHGH^ff?	@AGHKFDOL\;^ff?[<OA;>@A9<;q^ff?FZ<;ff?ComGp@A9^KMh=DCH;tJ,KL\K@AOY?F<Z
	ffz

fifi

$vu]}s11t$vWwvLswe1}%l1r



{ ^| }b~~   
^ 

& 
 |

(



{:|^}b~~` 
& 

(
{ffU

[

{ffU

{ff5

 GHI[lJ,;f8:9<;h^KFld*I[lJU?	@AGHKFOV=*?^;h^KJAJU;OV=\KF<Z<GHF<I@AK GHI[lJ,;_BWK@A;.#GHOf?t@AKJ,[<Osx
Gp@hGHOhZ<GHbGHZD;ZGEF@AK@ojKJ,;IGHKF<
O #G79`p ?F<
Z #G79`p @A9D?	@R^ff?FDF<K@RL\;y^KF<F<;^@A;ZLq
?y^KF@AGEF[<K[DOY=*?	@A9\s ?F<Z7v2&@A9D;J,;hGHOY F<K@n
? #*wxKL*OV^@, ?^CH;RPXKJ , L\;^ff?[<OA;GH@TZ<K;O>F<K@
GHF@A;JUP;J,;&oGp@A9@A9<;f?	J,Mt_
;FbGpJUKF<Mh;F@ MRKZ<;CEOfi9*?bGHF<IW@A9<K[<O,?FDZ<O KPDP?^;Os@A9D?	@Q?	J,;KP@A;F@AKK&@AGHMh;wx^KF<O,[<MhGHF<IWPXKJQICHKL*?C
Mh;@A9<KZ<O_
U)hff f#g#he1fvle1fbde1op
8:9<;4OA@AK^U9D?OV@AGH^QKJ+JU?F<Z<KM?	=D=<J,K2?^,9To?O+d<J,OV@\GHF@VJ,KZ<[<^;Z&Lq  ?	JAJU?g[D?F<Z>?F<Z8%?	@AKM>L\;J2Us
?F<ZBCE?	@A;JY[<OA;ZLq$KWb2;J,MR?	J,On2Us?F<ZMhKJU;fJ,;^;F@ACpqtLqf ?ffbJU?	GJ2I U_T8:9<;nMR?GHFGHZ<;ff?
dfe

 e



tFu

ie1f f#'f

ojb

ef

nh

L\;9<GHF<ZB@A9<;O,;R?CHIKJ,Gp@A9DMhOTGHOY@AKLD[<GHCEZ?tIJU?	=D9GHF@A9<;h^KFldDI[<JU?	@AGHKFOV=e?^;_u8:9D;IJU?	=D9GEOTKLlw
@,?GHF<;ZGHF<^JU;Mh;F@,?CECpq?O>PXKCHCHKomOh?tCHK^ff?C=DCE?F<F<;JnGHOf[DOA;Z@AK@VJAq@AKJ,;ff?^,9@A9<;IK2?C_l9<K[<CHZ
@A9<;hMRK@AGHKFOV@AK=?	@n?yCHK^ff?CQMhGEF<GHMn[<Mys+?yF<;oF<KZD;yKJnCE?FDZ<MR?	JA<GHOT^J,;ff?	@A;ZLqrI;F<;J?	@AGHF<I
?RJU?F<Z<KMNMhK@AGHKFOV@,?	JA@AGHFDIRPJ,KM @A9*?	@WCHK^ff?C+MRGHF<GHMn[<My_:8:9D;TMh;@A9<KZGH@A;JU?	@A;OW@A9<;OA;>@xoKuOA@A;=DO
[<F@AGHC@A9<;IK2?CQ^KFldDI[lJ?	@AGHKF9D?O&L\;;FJU;ff?^,9<;ZPJ,KMKF<;KPQ@A9D;OA;GHF@A;J,Mh;Z<GE?	J,qy=]KO,Gp@AGHKF<OLq
?IJU?ZDGH;F@WZ<;OA^;F@mMhK@AGHKF"_j8:9D;OA;f?CHIKJ,GH@A9<MhOoKJAomGp@A9?hZ<GHOA^J,;@AGE;ZyJ,;=<J,;O,;F@,?	@AGHKFKP @A9<;
^KFldDI[<JU?	@AGHKFnOV=*?^;_48:9<;q>?	JU;iF<KffoF>@AKL\; r*3 254,5 4$b 6U @ 2qsr*pxfiL\;^ff?[<O,;i@A9<;=<J,KLe?	LDGHCHGp@q
KP @A;J,MhGEFD?	@AGHF<IoGp@A9?OAKCH[l@AGEKF7?h=e?	@A99D?OL\;;FPXK[<F<ZKJ&FDKh=*?	@A9;GHOA@AOU:^KFb2;J,I;O&@AKuKF<;
?O:@A9D;>?CHCHKoj;Z@AGHMh;TGEF<^J,;ff?OA;Y@AKo?	JUZ<OmGHF<dDF<Gp@q_`WOmGHFy@A9D;Y=<J,;bGHK[<O:O,;^@AGHKF\sDGp@GHOm?CHOAKh=]KO,OAGpLDCH;
@AKRZD;OAGHIFtOAGEM=DCH;&Z<;^;=<@AGHb2;T;FbGpJ,KFDMh;F@AOm@A9D?	@:omGECHC\MR?	2;T@A9DGHOGHF<ZyKP4?CHIKJ,Gp@A9DMOACHKoj;J@A9D?F
?B=D[lJU;JU?F<Z<KM ?	=<=DJ,K2?^,9\_WKffo;b2;Jffs@A9<;q9D?ffb2;yL\;;F|@A;OV@A;ZPXKJhJ,KL\K@AOomGH@A9?r9<GHI9F[<Mw
L\;JfKP:r?F<Z@A9<;qr9D?b2;L\;;FOA9<KomF@AKojKJ,g[<GE^ACpqBGEFrJ,;CE?	@AGpb2;CHq^KMh=DCH;?F<ZF*?	@A[lJU?C
;FbGpJUKF<Mh;F@AO_
KW@A9D;JMR;@A9<KZDOj[DOAGHF<ITC?F<Z<MR?	JAOj9D?b2;WL\;;FZ<;bGEOA;Z\_4<KJ:;<?M=DCH;sD}"+]sGHF@VJ,KZ<[<^;Z
Lqr9D;F?F<Zo?FDIl2UsQMR?	2;O>[DOA;KPCE?F<Z<MR?	J,OW@AK?	=D=<J,KlGHMR?	@A;@A9D;hPEJU;;hOV=*?^;_u8:9<GHO
?	=<=<JUK2?^,9GEOOAGHMhGHC?	J@AK@A9D;yA9<GH;JU?	JU^,9<GH^ff?C]=*CE?F<F<GHFDI  ?	=<=DJ,K2?^,9[<OA;ZyGHFyxiOA9<K[DCHZu@A9<;YMh;@A9<KZ
	ff

fiy

\DW$us<|t"]

s|Bv}



v99 v}v

P?GHCj@AKrJ,;ff?^,9 ?IK2?CsjFD;oOA[lLDIK2?CHOh?	JU;yI;F<;JU?	@A;Z[<F@AGHCj@A9<;=<JUKLDCH;MGHO;ff?OVq;F<K[<I9@AKrL\;
OAKCpb2;Z"_  F@A9<;GHJ?	=<=<J,K2?^U9\sdDJ,OV@i?YCHK^ff?Cl=DC?F<F<;JQGHOQ[DOA;Z@AKTJ,;ff?^U9h@A9<;:dDFD?Cl=\KOAGp@AGEKF\+OA9<K[DCHZ@A9<;
CHK^ff?Ce=DCE?F<F<;JP?GHCs@A9<;&^KFldDI[lJ?	@AGHKFOV=*?^;YGHOZ<GpbGHZ<;ZuGHF@AKn@xoKOA[lLDOV=e?^;OslKF<;&^KF@,?GHFDGHF<In@A9<;
IK2?C*?FDZ@A9<;:K@A9<;J?YF<;oO,[lLlwxIK2?C_48:9<;:=<J,KLDCH;MaGEO @A9<;J,;PXKJ,;:Z<GpbGHZ<;ZGEF@AK&@ojK>OA[lLlw7=<JUKLDCH;MhO
GfIKGHF<IBPEJUKM@A9<;uGHF<GH@AGE?C4=\KOAGp@AGHKF@AK@A9D;OA[<LDIK2?Csi?F<Z GHGTIKGHF<IBPEJ,KM @A9<;uOA[lLDIK2?C@AKB@A9<;
dDFD?Cfi=\KOAGp@AGEKF\_>+}++]>9D?O&L\;;FrOA9<KomFB@AKL\;>=e?	JA@AGH^[<CE?	JUCpqoj;CHCQ?Z*?	=<@A;Z@AKdDF<Z=*?	@A9<OYPXKJ
MR?F<GH=D[<CE?	@AKJ,Off_  @j9*?OiL\;;FGHM=DCE;Mh;F@A;Zu?F<Z@A;OV@A;ZyPKJ=DC?F<F<GHF<I&=e?	@A9<OPKJ'i[<M?>?F<Zu`WZ<;=<@
J,KL\K@AO_
dfe

 e



t

ie1f f#'f

-oeBhjffjoffne$h

'fh

8:9<;=DJ,;bGEK[<O4Mh;@A9<KZ<O4o;J,;:;OAO,;F@AGE?CECpq>Le?OA;ZhKFh@A9D;:^KFldDI[lJ?	@AGHKFhOV=*?^; @A9<;:J,;@VJU?^@AGHKF\s@A9<;
Z<;^KM=\KOAGH@AGHKF\slKJ:@A9<;YK=<@AGEMhGHff?	@AGHKFyGHOM?Z<;YGHF@A9<GHOOA=*?^;_i`WF?Cp@A;J,FD?	@AGHb2;YGHO@AKh^KF<OAGEZ<;J@A9<;
V@VJU?SA;^@AKJAqOV=e?^;  _:<KJW;<?M=DCE;sDGHFt9<GEO:Mh;@A9<K
Z ~]sD<;J,L*?^,9J I2OV@,?	JA@AOLqy^KF<OAGEZ<;J,GHF<I
@A9<;OV@VJ?GHI9@TCEGHF<;nOA;IMh;F@jSVKGHFDGHF<I@A9<;GHF<Gp@AG?Cfi?F<Z@A9D;dDFD?C ^KFldDI[lJ?	@AGHKFrGHy
F #fi_8:9DGHOW=*?	@A9rGHO
=<J,KIJU;OAOAGpb2;CpqtMRKZ<GHdD;ZtGHFOA[<^,9r?MR?F<F<;J@A9D?	@W@A9<;>PXKJALDGEZ<Z<;FyJ,;IGEKF<OGH@^J,KOAO,;O&?	JU;fJ,;ZD[<^;Z\_
`@Y;ff?^,9Gp@A;JU?	@AGEKF\s]?OA[lL<wxMR?F<GHPXKCHZtK1P #^KF@,?GHF<GEF<Ih@A9<;n^[lJAJU;F@W=*?	@A9BGHOmJ?F<Z<KMhCpqyI;F<;J?	@A;Z\_
 @hGHO@A9<;F|Z<GHO,^J,;@AGH;Z?F<Z|;=DCHKJ,;Z|[DOAGHF<I?ZlqFD?MhGH^=<J,KIJU?MRMhGHF<IMh;@A9<KZ@A9D?	@h[<O,;O@A9<;
CH;F<I@A9?^J,KOAOf@A9<;RPXKJALDGHZDZ<;FBJ,;IGHKF?O>@A9<;h^KOA@>PX[<F<^@AGHKFGEFKJ,Z<;J>@AKMhGHFDGHMhGH;_R8m9<;OA;ff?	J,^U9
J,;OA[DCp@AOQGHFh?fF<;o@VJU?SA;^@AKJAqom9<KO,;:GHF@A;J,OA;^@AGHKFRomGp@A9@A9<;mPKJAL*GHZ<Z<;FnJ,;IGHKFDOiGHOQO,MR?CHCH;J4@A9D?FR@A9<;
KJ,GHIGEFD?C+@VJ?SV;^@AKJAq_n8:9<;>=<J,K^;OAO&GHOWJ,;=\;ff?	@A;Z[<F@AGECfi?F?Z<MRGHOAOAGpL*CH;&@VJU?SA;^@AKJAqBGEOWPXK[<F<Z"_&`WO&GHF
@A9<;n=<J,;bGHK[<OWOA;^@AGHKFDOs\Gp@WGEO&?CEOAKR=\KOAO,GpLDCH;Y@AKyZD;OAGHIFBOAGHM=DCE;TZ<;^;=<@AGpb2;;FbGpJ,KF<Mh;F@AO@A9D?	@YomGHCHC
MR?	2;Y@A9DGHOGHF<ZKP4?CHIKJ,Gp@A9<M O,CHKffo;Jm@A9D?F?n=D[lJ,;&JU?FDZ<KM?	=<=<J,K2?^U9\_
8:9D;ioKJA>KP %+GHF\s &GE?Ks?F<Z GH^,9D?CE;omGH^W GHO OAGHMhGECE?	J+@AK&K[lJQ?	=<=<JUK2?^,9\_ `O4GEFn?F;ff?	J,Cpq
b2;J,OAGEKFnKP*K[lJi?CHIKJ,GH@A9<M `9[D?^@AGHF\s ?;Jffs  ;O,OA^G ; J,;Us xz8 ?CpLDGs2UsI;F<;@AGH^:?CHIKJ,GH@A9<MhO4?	J,;
[<OA;Z@AKf^ff?	JAJAq>K[<@QK=<@AGHMhGHff?	@AGEKFGHFn@A9<;@VJU?SA;^@AKJAqhOV=*?^;_48JU?SV;^@AKJUGH;Oi?	J,;=e?	JU?Mh;@A;J,GH;Z[DOAGHF<I
@A9<;n^KKJUZ<GHFD?	@A;OWKPQGHF@A;J,Mh;Z<G?	JAqbGE?w7=\KGHF@AO_m`WFB;b2KCH[l@AGHKFD?	J,q?CHIKJ,Gp@A9<MGHOW[<OA;Z@AKuK=D@AGHMhGH;
?^KOA@uPX[<F<^@AGHKFL*?OA;Z KF@A9<;CH;F<I@A9 KP&@A9<;t@VJU?SA;^@AKJAq ?FDZ@A9<;PKJAL*GHZ<Z<;FJ,;IGEKF ^J,KO,OA;Z\_
8:9<;ROV@,?F<ZD?	J,ZK=];J?	@AKJ,O>KPj@A9<;I;F<;@AGH^u?CHIKJ,Gp@A9<MROT9D?b2;L\;;FMhKZ<Gpd*;Z?F<ZCE?	@A;Jn;@A;FDZ<;Z
@AKt=DJ,KZD[<^;R?tC?	J,I;b?	J,GH;@xqKP=e?	@A9<OR^ &GE?Ks GE^,9D?CH;oGH^1s x]9D?F<IsjJ I2U_8:9D;RF[<MfL\;JfKP
GHF@A;J,Mh;Z<G?	JAqhbGE?w7=\KGHF@AOGEOdl;Zt?FDZy^,9<KOA;F[<OAGHFDIn?Ft9D;[lJ,GHOV@AGE^_  Gpb2;F@A9<GHOF[<M>L];JsFDK@A9<GHF<I
=<J,;b2;F@AOfi@AKZD;OAGHIF?mZ<;^;=D@AGpb2;i=DJ,KLDCH;Mom9<GH^U9>OAKCE[l@AGHKFToGHCHCJ,;g[<GpJU;QMhKJ,;jGHF@A;J,Mh;Z<GE?	J,qW=\KGHF@AOs
CH;ff?Z<GEF<In@A9<;f?CHIKJ,GH@A9<M @AKhP?GHC]om9<GHCE;WKFD;YOAKCH[l@AGHKFy;lGHOV@AOff_
6 0 l6 ( 1  w 6 \3 0 $ (  ( 16 w
`WO:o;T9D?b2;nOA;;FGHFy@A9<;Y=DJ,;bGEK[<OOA;^@AGHKF\s*@A9<;T^KM=*[l@,?	@AGHKFKPfi@A9<;f^KFldDI[lJU?	@AGEKFOV=*?^;.#rGEO?
b2;JAqy@AGHMR;wx^KF<OA[<MhGEF<I@,?OVe_8m9<;TMR?GHFGHZ<;ff?L\;9<GHFDZ@A9D;f`J,GE?Z<F<;Om^CH;o?CHIKJUGp@A9<MGHO:@AK?b2KGEZ
@A9<GHO^KM=*[l@,?	@AGHKF\_  FKJ,Z<;J@AKhZDK>@A9DGHOs@A9<;T?CHIKJ,GH@A9<M OA;ff?	J,^,9D;O:Z<GpJ,;^@ACHqPXKJm?P;ff?OAGHLDCH;=*?	@A9yGHF
@A9<;Y@VJ?SV;^@AKJAqyOV=e?^;_8:9<;&^KFld*I[lJU?	@AGHKFtOA=*?^; #GHOF<;b2;JW;=DCEGH^Gp@ACpqR^KMh=D[l@A;Z\_
`WOQomGECHCL];O,9<KffomF"sGHF@A9<;@VJ?SV;^@AKJAqhOA=*?^;s=*?	@A9R=DCE?F<FDGHF<IWMR?qL\;OA;;F?O?FRK=<@AGHMRGHff?	@AGHKF
=<J,KL*CH;M ?F<Z OAKCpb2;Zz?OOA[<^U9 Lq?F{?CHIKJ,Gp@A9DM ^ff?CHCH;Z{ff<+ee _  @uGHOR=\KOAO,GpLDCH;@AKLD[<GHCEZ?F
?	=<=<JUKlGHMR?	@AGHKFKP&PJ,;;tOV=e?^;yLq?F<K@A9D;Ju?CHIKJ,GH@A9<M ^ff?CHCH;Z l+~<+\@A9D?	@GHOR?CHOAK=]KO,;Z?O
?FK=D@AGHMhGHff?	@AGHKF=<J,KLDCH;Mt_ 8:9<;`mJ,G?Z<F<;Oh^CH;o?CHIKJ,Gp@A9DM GHO@A9D;yJ,;OA[<CH@RKPW@A9<;tGHF@A;J,CH;ff?ffb2;Z
;l;^[l@AGHKFKP4ff<+eer?F<Zt<+~<\]_
 
. 



ffff

fifi

$vu]}s11t$vWwvLswe1}%l1r

ROBOT

\ Z 

W

W VV


T

\ Z 


T

\ Z  Y  c c9_


 GHI[lJ,;Tv`=e?	JU?Mh;@A;J,GH;Z@VJU?SA;^@AKJAq , >@ , >@ 0 >@ 0 >=J>@5:?F<Z?OV@,?	JA@AGHFDI=\KGHF@D=C; <YGEM=DCHGH^w
Gp@ACpqZD;dDF<;T?=*?	@A9GHF@A9<;TK=\;JU?	@AGHKF*?C\OV=*?^;PXKJW?9<KCEKF<KMhGH^YMhKL*GHCH;WJ,KL\K@_
t
n'bdnekjf  opjl ihjb/>ff<+ee
 Gpb2;F|?J,KL\K@>oGp@A9*si?t@VJU?SA;^@AKJAqKP:CH;F<I@A9MR?ffqL\;h=e?	JU?Mh;@A;J,GH;Z|?O?BOA;g[<;F<^;
KP,O8OA[<^^;OAOAGHb2;MhKb2;Mh;F@AO_` OV@,?	J,@AGHF<It=\KGHF@$	C; h
 ?CEKF<ItomGH@A9OA[<^U9?t=*?	JU?Mh;@A;JUGH;Z
@VJU?SA;^@AKJAqGHM=*CHGH^Gp@ACpqyZ<;d*F<;h?=*?	@A9?FDZ?dDFD?C4^KFld*I[lJU?	@AGHKFC;  GHF@A9<;^KFld*I[lJU?	@AGHKFOA=*?^;_
<KJ;<?M=DCE;s&PXKJ?9<KCHKFDKMhGH^MhKLDGHCE;J,KL\K@@A9<;@VJU?SA;^@AKJAqa , >@ , >@ 0 >@ 0 >=J&>@5X u^ff?F{L\;
GHF@A;JA=<J,;@A;Zr?OTM?	GHFDIu? , Z<;IJ,;;n@A[lJUF\s"MhKffbGHF<IOV@VJ?GHI9@8 , s"MR?	GEF<Iu? 0 Z<;IJU;;>@A[<J,F?F<Z
OAKnKF\_  Gpb2;F@A9<;WOV@,?	JA@AGHFDI>^KFld*I[lJU?	@AGHKF; C  s@A9DGHOi@VJ?SV;^@AKJAquCH;ff?Z<O@AKf@A9D;mdDF*?CD^KFldDI[<JU?	@AGHKF/C; 
e^g

 e



ie1f f#'f de$`ef

OA;;T GHI[<J,;Tv2U_
 GHb2;F?>Z<GHOA@,?F<^;mPX[<F<^@AGHKFKFu@A9<;m^KFld*I[lJU?	@AGHKFuOV=*?^;slGHP]o;md*F<Z?f@VJU?SA;^@AKJAqOA[D^,9@A9D?	@
Gp@jZ<K;OjF<K@^KCHCHGHZD;omGp@A9?FqKLDOV@,?^CH;O?F<ZuOA[<^U9R@A9D?	@j@A9<;WZ<GHOV@,?FD^;mL\;@xo;;
F C;  ?F<ZR@A9D;IK2?C 	C; 
GHO4;J,Ks@A9<;Fo;9D?b2;?&OAKCH[l@AGHKF@AKYK[<JQ=*?	@A9=DCE?FDF<GHF<I=<J,KLDCE;My_8:9D;J,;PKJU;s2@A9<;=*?	@A9=*CE?F<F<GHFDI
=<J,KL*CH;M MR?qL\;&OA;;F?OW?MhGHF<GEMhGHff?	@AGHKFu=<J,KL*CH;M om9<;J,;
_m8m9<;QOA;ff?	J,^U9>OA=*?^;iGHO?:OV=*?^;KPOA[<Gp@,?	LDCHqm=*?	JU?MR;@A;J,GH;Z>@VJU?SV;^@AKJUGH;Os@A9<;4@VJU?SA;^@AKJAqfOA=*?^;_
_m8m9<;P[<FD^@AGHKFn@AKYMhGHF<GHMRGH;iGHO D? C;  >C;  GHPD@A9<;j=*?	@A9GHO4^KCHCHGEOAGHKFwxPJ,;;s?F<
Z < 7C; 3>C;  fiK@A9<;J,omGHOA;
 C;  L\;GHFDI>@A9D;&d<J,OV@:^KCHCEGHOAGHKFu=\KGHF@UU_ 0
ff*U7lX((+7	U$\e$]7 ,(\7	 {A$]7 7	\ff$XxU""4,($X(:ff	7 fiVe
7$4$Q(VQ$%U:,(7,7	:7	%		(hV,7	 	7Xj	UiUTU$(	T	,7hU$m
fffXxUfi7A]U7Y7	i,*U	>$Y	,4$U7V U7Q7Afi7	jXfi%		(f 	U((
Um(:,X7$7&U\7fi(	,7&Ul7fi77 XV7U&U"74UU	\,<	,mV2
ff+

fiy

\DW$us<|t"]

s|Bv}



v99 v}v

:8 9D;?CEIKJ,Gp@A9<Mff<+ee sL*?O,;ZKF@A9<GHOfib2;JAq>O,GHM=DCH;i@A;^,9<FDGHg[D;?FDZh?mJ?F<Z<KMhGH;ZK=<@AGEMhGHff?w
@AGHKFMh;@A9<KZ\sGHOj?CpJ,;ff?Zlq?	LDCH;@AK>OAKCpb2;:g[<Gp@A;m^KM=DCH;=<J,KL*CH;MhO4KP]J,KL\K@iMhK@AGEKF=DCE?F<FDGHF<I_DKJ
;<?M=DCH;s  GHI[lJ,;tJ,;=<JU;OA;F@AOf@A9<;h@ojK=*?	@A9<OnPXK[<F<ZPXKJ>@A9D;R9<KCHKFDKMhGH^MhKLDGECH;J,KL\K@_y?^U9
=*?	@A9o?OY^KM=D[l@A;ZKF?uOV@,?F<Z*?	J,ZBojKJAOV@,?	@AGHKFxff~+]/
 WGHFBCH;OAOW@A9D?
F _$uOA;^KF<ZoGp@A9<K[l@
[<OAGEF<I>?Fq=DJ,;wx^KM=D[l@,?	@AGEKFuKP+@A9<;&^KFldDI[lJ?	@AGHKFyOV=*?^;_Q8:9[<OslGp@GHOj=\KOAOAGpL*CH;s?CpL\;Gp@jOACHKomCpqs@AK
I;@W?=DCE?F<F<;J@A9D?	@^ff?FtL\;Y[<OA;ZtGEFt?ZlqFD?MhGH^&;FbGpJUKF<Mh;F@Tom9<;J,;&@A9<;fKLDOV@,?^CH;OMR?qyMhKb2;
LqBAZ<J,K=<=DGHFDI  ?YF<;ooKJ,CHZhGEF@AKY@A9D;mOVqOV@A;Mc;b2;J,q_$fOA;^KF<Z"_Ql+]eGHO4b2;JAqh;R^GE;F@QL*[l@QGp@
GHOF<K@^KM=DCH;@A;sDO,GHF<^;YGp@:MR?ffqyPX?GHC]@AKhdDF<Zt?n=*?	@A9;b2;FGHPKF<;T;lGHOV@AO:PXKJ:@xoKRZDGpe;JU;F@J,;ff?OAKFDO
L_ &[<;W@AKn@A9<;WK=<@AGHMhGEff?	@AGHKFw7L*?OA;ZuPXKJ,M>[DCE?	@AGHKF\s<ffleet^ff?FyI;@@VJU?	=D=];ZLqCHK^ff?C*MRGHF<GHMR?
KPi@A9<;nKLSV;^@AGHb2;P[<FD^@AGHKF\seom9<GH^U9GHF@A[lJ,FBMR?ffq=DCE?^;n@A9<;>J,KL\K@&PX?	Jf?ffo?ffqBPJ,KM@A9<;nIK2?C
O,;;f GHI[lJ,;TU_
_m8m9<;:CH;F<I@A
9 ]KPe@A9<;:@VJU?SA;^@AKJ,GH;Oj^KF<OAGEZ<;J,;ZhMR?ffqL\;@AKK>OA9<KJA@i@AKTJ,;ff?^U9u?CHC<@A9<;m?^^;OAOAGHLDCH;
JU;IGHKF<O:KPfi@A9<;Y^KFld*I[lJU?	@AGHKFtOA=*?^;_

fiGEI[lJ,;Yw;ff?^@AGpb2;TJ,;=*CE?F<F<GHFDI>GHFt?h^,9D?F<IGEF<I;FbGpJ,KFDMh;F@
a n ki jo'ft
ebd 

e$

n'bdne$jf

e1f



ffjml#ihpbfl~<\
o

 FnKJUZ<;J @AK&LD[<GHCHZ>?^KMh=DCH;@A;=DCE?FDF<;Jffs	oj;j=<J,K=\KOA;?WOA;^KF<Zh?CHIKJUGp@A9<M ^ff?CHCH;Zl+~D\]_!z9<GHCH;
@A9<;j=D[lJ,=]KO,;QKPel+]*Ro?O@AK&CHKKfZ<GHJ,;^@ACpqfPXKJQ?:=*?	@A9PJ,KM; C  @AKyffC; s@A9D;=*[lJA=\KOA;QKPel+~<+\
GHOj@AK^KM=D[<@A;&?Fy?	=<=DJ,KlGHMR?	@AGHKFKP+@A9<;WJ,;IGHKFyKP"@A9D;W^KF<dDI[lJU?	@AGHKFOV=e?^;T?^^;OAOAGpLDCE;PJ,KM; C  _
8:9D;l+~D\?CHIKJ,GH@A9<M L*[<GHCHZ<O?Fz?	=D=<J,KlGHMR?	@AGEKFKPY@A9<;?^^;OAOAGHLDCH;BOV=*?^;Lq=*CE?^GHF<I
CE?F<ZDMR?	JAOGHFy@A9<;T^KF<dDI[lJU?	@AGHKFOV=*?^; #GEFyOA[<^,9B?o?qy@A9D?	@W?=*?	@A9PEJ,KM@A9<;fGHFDGp@AGE?Ce=\KOAGp@AGHKF
	C; @AKT?FqnCE?FDZ<MR?	JAfGEO F<KomF\_  FKJUZ<;J4@AKTCH;ff?	J,F?OQMn[<^U9R?O4=\KOAOAGpL*CH;?	L\K[l@4@A9<;PJ,;;OV=*?^;s@A9<;
l+~D\y?CHIKJ,Gp@A9<M @VJUGH;O@AKhOV=DJ,;ff?Zy@A9<;TC?F<Z<MR?	JAO[<F<GEPKJ,MRCpqRKffb2;J@A9<;TOV=e?^;hOA;;f GHI[<J,8; I2U_
8fiKnZDKf@A9<GEOsGH@i@VJ,GH;Oj@AKn=D[l@@A9<;&CE?F<Z<M?	JAOj?OPX?	J:?Oj=\KOAOAGpL*CH;:PJ,KM KF<;W?FDK@A9<;JLqMR?GEMhGHGHF<I
@A9<;TZDGHOV@,?F<^;OL\;@xo;;Ft@A9D;My_
8:9D;J,;PKJU;s*l+~<+\uMR?ffqL];YO,;;F?O?MR?lGHMhGEff?	@AGHKFy=<J,KL*CH;Mkom9<;J,;
ffp

fifi

$vu]}s11t$vWwvLswe1}%l1r
C; 

	C; 

 GHI[lJ,;f` =<J,KLDCH;M CH;ff?Z<GHF<Ih@AK?hCEK^ff?CfiMhGHF<GEM>[<Mt_  FOA[<^U9?^ff?OA;s\?hOAKCH[<@AGHKFt=*?	@A99D?Od<J,OV@
@AKRMhKb2;n?ffo?ffqPEJUKM@A9<;>IK2?C_8:9<;TIK2?CO,?	@V@VJU?^@AGHKF  L*?O,;ZKFt@A9D;fMhGHFDGHMhGHff?	@AGHKF
KP@A9D;Yi[<^CHGEZ<;ff?FyZ<GHOV@,?FD^;&=<J,;b2;F@AOffleePJ,KM dDF<ZDGHF<InOA[<^,9?=*?	@A9\_

 GHI[lJ,;8I8:9<;&d<J,OA@=DGH^@A[lJ,;&JU;=<J,;OA;F@AO@A9<;TGEF<Gp@AGE?Ce=\KOAGp@AGHKFy?F<Zt@A9<;Yd<J,OA@:CE?F<Z<MR?	J,*_Q8:9D;YOA[lLlw
OA;g[D;F@CE?F<Z<M?	JAO?	J,;@A9D;F [<FDGHPKJUMhCpqOA=<J,;ff?ZKb2;Jy@A9<;O,;ff?	J,^,9zOV=*?^;om9<GECH;@A9<;
Mh;@A9<KZ2;;=DO@VJU?^, KPT?CHCm=*?	@A9<OTSVKGHFDGHF<I@A9<;CE?F<Z<MR?	J,OR@AK@A9<;GHF<Gp@AG?C=\KOAGp@AGHKF\_
8:9<;?CHIKJ,Gp@A9DMGEOWF*?Mh;Z?PE@A;J>`mJ,G?Z<F<;>L];^ff?[DOA;nLq=DCE?^GHF<IuCE?F<ZDMR?	JAOs"l+~<+\
[<FoGHF<Z<O?O:GHPfiGp@o;J,;T[<OAGEF<I?@A9lJ,;ff?Z?O8:9<;OA;[DOZ<GHZ\_
_m8m9<;OA;ff?	J,^,9OV=*?^;GHOn@A9<;yOA;@KP&?CHCj=*?	@A9<OOV@,?	JA@AGHFDIBPJ,KM KF<;KPm@A9<;=<J,;bGHK[<OACHqB=*CE?^;Z
C?F<Z<MR?	JAO_
_m8m9<;&P[<FD^@AGHKFu@AKMR?GEMhGH;&GHO < C;  >Us<o9<;J,; GHO@A9D;WO,;@:KPCE?FDZ<MR?	JAOj?CHJ,;ff?ZlqR=*CE?^;Z\_
ff l+]e
 FnKJUZ<;J4@AK&9D?b2;m?=DC?F<F<;Jfi@A9D?	@QGHOL]K@A9^KMh=DCH;@A;?F<Z;R^GH;F@s2o;^KMfLDGEF<;Zn@A9<;j@xoK&=<J,;bGHK[<O
?CHIKJ,GH@A9<MhOmff<+eer?F<Zl+~<+\@AKhKL<@,?GEFy@A9<;T`J,GE?Z<FD;O^CH;o?CHIKJ,GH@A9<My_
8:9D;&=<J,GHF<^GH=DCH;mKPfi@A9<;Y`J,GE?ZDF<;O^CH;o?CHIKJUGp@A9<MGHOb2;J,qO,GHM=DCH;
_mWOA;&@A9<;fl+]eB?CEIKJ,Gp@A9<Mk@AKdDF<Zuom9D;@A9<;Jm?BAOAGHM=*CH;  =*?	@A9y;GEOV@AOL\;@xo;;
F C;  ?F<
Z ;	C _
e hoefhihUu

tjo#b/fl+~<+\F

i

ffff

fiy

\DW$us<|t"]

s|Bv}



v99 v}v

a

b

c

d

 GHI[lJ,;*  K[DF<^GHF<It?I2?GHFDOV@#*wxKL*OV@,?^CH;O_ufiGEI[lJ,;y7?2W=<JU;OA;F@AOf@A9<;KJ,GHIGEFD?C =*?	@A9GHF@A9D;^KFw
dDI[lJU?	@AGHKFOV=*?^;_r GHI[lJU;LefOA9<KomO>@A9<;O,?Mh;=*?	@A9|?PE@A;J@xoKL\K[<F<^;O?CHKF<I@A9<;
OA;^KF<ZOA;IMh;F@KFKLDOA@,?^CH;Tf?F<ZuKFKLDOV@,?^CH;>_Q GHI[lJ,;>^GHOi@A9D;mJ,;O,[<Cp@KLD@,?GHF<;Z
?PE@A;JR?L\K[<F<^;uKPmOA;IMR;F@Rv?I2?GHF<OA@hKLDOV@,?^CH;t_ GHFD?CECpqsQ GHI[lJ,;BZ*f=<J,;OA;F@AOh?
b?CHGEZ=*?	@A9tKL<@,?GHFD;Z?PE@A;JW?dDFD?C]L\K[<F<^;YKPfiOA;IMh;F@?I2?GHF<OV@:KLDOA@,?^CH;>_
_  PfiF<KAOAGHM=*CH;  =*?	@A9tGHOPXK[<F<ZLqyOV@A;=s<@A9D;Ft^KF@AGEF[<;f[<F@AGEC]?=*?	@A9tGHO:PXK[<F<Z"_
7 ?2WOA;f<+~<\@AKRI;F<;J?	@A;f?F<;oC?F<Z<MR?	JAe_
LeuWOA;>l+]eB@AKhCHKKuPXKJ?AOAGEM=DCH;  =*?	@A9tPJ,KM @A9*?	@mCE?F<Z<M?	JAR@AKffC; _
8:9D;m`J,GE?Z<F<;O^CH;o{?CHIKJ,Gp@A9<McomGHCHCd*F<Z?T=*?	@A9GEPeKFD;m;lGHOV@AO_  F?FuKffb2;JAo9<;CHMhGHFDITF[DMfL\;J
K P4^ff?OA;OsSA[<OV@&?RPX;oC?F<Z<MR?	JAOm?	J,;nF<;^;OAOU?	JAqPXKJ@A9<;>`mJUGE?Z<F<;O^CH;o ?CEIKJ,Gp@A9<MN@AKRJU;ff?^,9B@A9<;
@,?	J,I;@&?F<ZtOV@AK="_
e



UffjoBbn ffo jhjbdhjf5jg#f#'ftjf#m=l#e1ih

kme

` @ q=DGE^ff?CZ<GHR^[<Cp@qPXKJ?t=*?	@A9=DCE?FDF<GHF<It?CHIKJUGp@A9<M GHOT@AKBdDF<Z?^KCECHGHOAGHKFlwxPEJ,;;=e?	@A9@A9<J,K[<I9
?tO,MR?CHCQ^KJAJUGHZ<KJ>GEFr@A9<;R^KF<dDI[lJU?	@AGHKFOV=*?^;_8m9<GHOTGHOn?CHOAKy@A9D;R^ff?OA;uPKJ>@A9<;hLe?OAGH^hb2;JUOAGHKFKP
@A9<;`J,GE?Z<F<;O&^CH;oc?CEIKJ,Gp@A9<Mys]=<JU;OA;F@A;Z?	L]Kb2;_n8m9<;f=DJ,KLDCH;MGHOW@A9D?	@Yb2;JAqBP;oa@VJ?SV;^@AKJ,GE;O
;F<^KZ<;jOA[<^,9n=*?	@A9DOfi?F<Zn@A9<;JU;PKJ,;@A9<;q?	J,;ib2;J,qfZ<GpR^[DCp@"@AKWdDF<Z\_ KOA@fi@VJ?SV;^@AKJ,GE;O4^KCHCHGHZ<;QomGp@A9
@A9<;:KL*OV@,?^CH;O_ !;m=<J,K=\KOA;:?&b2;JAqOAGEM=DCH;GHZ<;ff?W@AK>Z<;ff?ClomGH@A9@A9<GHO =<JUKLDCH;MyfiIKGHF<IYL*?^,o?	J,Z<Oj?	@
;ff?^,9^KCHCHGHO,GHKFn=]KGEF@_  Ps2PXKJ?YIGpb2;F@VJ?SV;^@AKJAqsl?&^KCHCHGEOAGHKFGHO4Z<;@A;^@A;Zu?CHKF<I&@A9<;^KJAJU;OV=\KF<Z<GHF<I
=*?	@A9\sD@A9<;Fto;fOAGEM=DCpqR^KFDOAGHZ<;J:@VJ?F<OAPXKJ,MhGHF<I@A9*?	@m@VJU?SA;^@AKJAqtOAK@A9*?	@Gp@m;FD^KZ<;OW?F<;o=*?	@A9\s
KF<;@A9D?	@TGEOYPK[<FDZLqL]K[DF<^GHF<IuK@A9D;KLDOV@,?^CH;R?	@Y@A9D;^KCHCHGHOAGEKF=\KGHF@OA;;h GHI[lJ,
; *U
_ BWK@A;
@A9D?	@@A9<GHOR^KFDOV@VJ,[<^@AGHKF GHO?	=D=DCHGH;ZJ,;^[<J,OAGpb2;Cpq|[<F@AGHC@A9<;;F@AGHJ,;t@VJU?SA;^@AKJAq^KJ,J,;OV=\KF<Z<O@AK?
^KCHCHGEOAGHKFwxPJ,;;=*?	@A9\_
ff	

fifi

$vu]}s11t$vWwvLswe1}%l1r

W OAGHF<Iu@A9<GEO@A;^U9<F<GHg[<;s?CEC@VJ?SV;^@AKJ,GE;O>?	J,;OAK@VJU?FDOAPKJUMh;Z@A9D?	@Y@A9<;q;F<^KZ<;nb?CHGEZ=e?	@A9<O_
8:9<GEO\GHM=<J,Kb2;ZTb2;J,O,GHKFfKP@A9<;Q`J,GE?ZDF<;O+^CH;o?CHIKJ,Gp@A9<MF<K:CHKFDI;J+^ff?	J,;O ?	L]K[<@+KLDOV@,?^CH;Off_filJUKM
@A9<;=\KGHF@YKPjbGH;oKP?yO,;ff?	J,^,9GHF@A9<;@VJU?SA;^@AKJAqOV=e?^;sGp@TGHO>?OfGHPi@A9<;hKLDOV@,?^CE;Of9D?ffb2;hOAGHM=*Cpq
b?F<GHOA9<;Z"_8:9<GEO4Mh;@A9<KZGHOQ;OV=\;^GE?CHCHqf;R^GH;F@iPXKJiFD?	JAJ,Ko^KJAJ,GHZDKJ,O4GHF@A9<;:^KF<dDI[lJU?	@AGHKFOA=*?^;_
!zGp@A9<K[l@&L\K[<F<^GHFDIsD@A9<;MR?	=<=*GHF<IuKPi?^KJAJ,GHZDKJWGHFB@A9<;^KFld*I[lJU?	@AGHKFOV=*?^;n@AK@A9<;n@VJU?SA;^@AKJAq
OV=*?^;GEOT?O,;@TKPib2;JAqPX;oa=]KGEF@AOA_ ! GH@A9L]K[DF<^GHF<Is\;b2;JAqOAGHF<ICE;f@VJU?SA;^@AKJAqrIKGHF<I@A9<J,K[<I9?
=*?	JA@KP@A9D;u^KJ,J,GHZ<KJGHOAPKCEZ<;Z  GHF@AK@A9<;^KJAJUGHZ<KJyOA;; GHI[lJ,;*U_8:9D;RJ,;O,[<Cp@,?F@M?	=<=DGHF<I
KP&@A9<;t^KJ,J,GHZ<KJuGHF|@A9<;y@VJ?SV;^@AKJAq OV=*?^;GEOR^KF<OA;g[<;F@ACpq?Mn[<^,9 CE?	J,I;JOA;@KPW=\KGHF@AOs:?F<Z
@A9<;J,;PXKJ,;hGH@TGHOYM>[D^,9;ff?O,GH;JT@AKydDFDZ?yMh;M>L];JTKPj@A9<GEOYOA;@_8:9DGHO&;M=DGpJ,GE^ff?CfiGEM=<J,Kb2;Mh;F@>9D?O
?MR?SAKJm=DJU?^@AGH^ff?C+GEM=*?^@:L\;^ff?[<OA;fGp@:MR?	2;O@A9<;Y=<J,K=\KOA;Z?CHIKJ,Gp@A9<M PX?OV@A;JndDP@A;;Ft@AGHMh;OGHF
@A9<;Y=DJ,KLDCH;M ^KF<OAGEZ<;J,;ZyL\;CHKoT_
eb hiktjo#b

!;R^ff?FF<KoIGpb2;R?tdDFD?C b2;J,O,GHKFKPj@A9<;h`J,GE?ZDF<;OT^CH;o ?CHIKJ,Gp@A9<Mt_  @f9D?OT@A9<J,;;hGHFl=*[l@AO$C; 
@A9<;uGHF<Gp@AG?C4=\KOAGp@AGHKF*UsA	C; 
 @A9<;IK2?Cj=\KOAGp@AGEKF*Us ?FDZ|@A9<;uMR?lGHMn[<M ?CHCEKffo;ZZDGHOV@,?F<^;PKJh?
=*?	@A9u@AKn@A9<;L#o09254:6 72ffp:,6 U_  @JU;@A[lJ,F<O?nCH;I2?Ce=*?	@A9KJj@A;JUMhGHFD?	@A;OGHP\F<Kn=*?	@A9;lGHOV@AO?	@@A9<;&IGpb2;F

J,;OAKCE[l@AGHKF\_


	fiff 
	 ?C;  > 	C; ?>

L];IGEF
i   C , C;  
  F<GH@AGE?CHGH;@A9<;TOA;@KPCE?FDZ<MR?	JAOomGp@A9@A9<;YGEF<Gp@AGE?Ce=\KOAGp@AGHKF
 ,    C ,  "! , $#&%'
ZDKom9<GHCH;n( ! *) + 
 JU[<FffleeiCHKKuPXKJ:@A9<;fIK2?C"omGp@A9t?CEK^ff?C"MR;@A9<KZ
GHPMhGHF -,/. 021 < ffC; ff>C;  V 2
J,;@A[lJU3F   `=*?	@A9t9D?OL\;;FtPXK[<F<
Z 4
;CHO,;
 JU[<Ftl+~<+\Q=*CE?^;T?F<;oC?F<Z<MR?	JA
 56#  
 C   >C; O,[l= 7,8. 091 <&  ;: , >C;  V+ 
    <: ,>=   C   
!   MD&  <: , >  C  + 
;F<ZDGHP
;FDZ<Z<K
  
!?
 !	@

;F<Z

JU;@A[lJ,F"(!+  BWK=*?	@A9A4
 GHI[<J,;Ti8:9<;Y`J,GE?ZDF<;OmCH;o`WCHIKJ,Gp@A9DM
ff/B

fiy

\DW$us<|t"]

s|Bv}



v99 v}v

8:9D;T?CHIKJ,Gp@A9<M GEOL*?OA;ZtKFt@A9<;TPXKCHCHKffoGHF<IK=<@AGHMhGEff?	@AGHKF=<J,KLDCE;MhO
C H  3D ,O> [lE = 	 < &F  <: , > C; V

GHF<C;  > 	C; ff
G H  D MR
> E 	 F
C;  nZ<;F<K@A;OR@A9<;y;@VJU;MhGp@xqKP&?CH;I2?C=*?	@A9|=e?	JU?Mh;@A;J,GH;ZomGp@AJ
9 IuJU;ff?C=*?	JU?Mh;@A;J,Ou?F<Z
VO @,?	JA@AGEF<I;Gp@A9<;J:PJ,KM;ff?^U9KP@A9<;&=<J,;bGHK[<OACHqh=DCE?^;ZyCE?FDZ<MR?	JAO&7l+~<+\DKJmPJ,KM @A9<;YC?	@A;OV@
=DCE?^;ZCE?F<Z<MR?	J,xff<+ee+U_
8:9D;T?CHIKJ,Gp@A9<M GEOJ,;OAKCH[<@AGHKFwx^KM=DCE;@A;Y[<F<Z<;J@A9<;TPXKCHCHKomGHF<I?OAO,[<M=<@AGHKFDO
K U=*?^;dDCHCHGEF<Ir^KM=DCH;@A;FD;OAO  8:9<;BICHKL*?C&MR?lGHM>[DM ZDGHOV@,?F<^;B^ff?FzL\;PXK[<F<Z Lq @A9<;
K=D@AGHMhGHff?	@AGHKF?CHIKJ,Gp@A9<M [DOA;ZtGHFtl~<\L @A9<;T^KFldDI[<JU?	@AGHKFtOV=e?^;YGHOm?^KM=e?^@mOA;@_
K f^KM=DCH;@A;F<;O,O  fi8m9<;K=D@AGHMhGHff?	@AGHKF>=<J,K^;Z<[lJU;i[<OA;ZGHFff<+eeu?Cpo?qO dDF<Z?m^KM=*CH;@A;

=e?	@A9rKJJ,;@A[lJ,F<O2om9<;Fu@A9<;&OV@,?	J,@AGHF<I?F<Zu@A9<;&IK2?C]=]KO,Gp@AGHKF<O?	J,;&CHK^ff?	@A;ZomGp@A9<GHFu?nL*?CHC
KPfiJU?ZDGH[<O KP@A9<;TPJ,;;YOV=e?^;_

 F=<JU?^@AGE^;s@A9<;dDJ,OV@4^KF<Z<Gp@AGEKF^ff?F<F<K@QL\;Mh;@QoGp@A9h?&JU?F<ZDKMhGH;ZhK=<@AGEMhGHff?	@AGHKFh?CHIKJUGp@A9<M
GHF ?L\K[<FDZ<;Z|@AGHMh;sW?F<Z KFDCpqCHK^ff?CMR?lGHMR??	J,;BPK[DF<Z\_WKffo;b2;JffsW@A9<;C?F<Z<MR?	JAOh=*CE?^;Z
?^^KJ,Z<GEF<IR@AK@A9D;>F<;o?CHIKJUGp@A9<M ?	J,;nL\;@V@A;JTZ<GHOV@VJ,GHLD[l@A;ZtKb2;JT@A9<;PEJU;;>OV=e?^;>@A9*?FrCE?F<Z<MR?	J,O
=DCE?^;ZtJU?F<Z<KMhCHqs<CH;ff?Z<GHF<I@AKhL];@V@A;J=\;J,PKJUMR?F<^;O_i8m9<;YIK2?CKP@A9<;TFD;@mO,;^@AGHKFGHO:@AKWSV[DOV@AGHPq
@A9<GHO^C?GHMys<;=];JUGHMh;F@,?CHCpq_
i0 0 6 0w 1 ON4(6 PRQTSTU   6 0 WVYX 0   46 YZ 0\[ 6 21 0wOD0 
 FKJ,ZD;J>@AKZ<;MhKFDOV@VJU?	@A;@A9<;PX;ff?OAGpL*GHCHGp@q?F<Zg[D?CHGp@AGH;O>KP@A9<;u`mJUGE?Z<F<;O>^CH;o?CHIKJ,Gp@A9<Mtso;
9D?ffb2;RZ<;b2;CHK=\;Z?tJ,;ff?CHGHOA@AGH^?	=<=DCHGH^ff?	@AGEKFKPj@A9<;R?CHIKJ,GH@A9<My
_ !;OA;CH;^@A;Z?t=<J,KLDCE;Mom9<;J,;o;
o?F@h@AK9D?b2;?r=*?	@A9|=DCE?FDF<;JPKJ?OAG%fi+J,KL\K@R?	J,M GEF?rZlqFD?MhGH^;FbGpJUKF<Mh;F@no9<;J,;
?F<K@A9<;JY?	J,MGHOm[DOA;Z?O&?hMhKL*GHCH;YKLDOV@,?^CE;_m8:9<;YJUKL]K@nJ,KL\K@`YGHO[<F<Z<;Jm@A9<;f^KF@VJ,KC KP @A9<;
`J,GE?Z<F<;On^CH;o ?CHIKJ,Gp@A9DMy_  @nOA9*?	J,;O>GH@AO>oKJAOV=*?^;omGp@A9?OA;^KFDZJUKL]K@uJ,KL\K@  f@A9D?	@GHO
MhKbGHFDIB[DF<Z<;Jn@A9<;^KF@VJ,KCmKP?JU?F<Z<KMMhK@AGEKF|I;F<;JU?	@AKJff_8:9<;u`mJUGE?Z<F<;O^CH;o ?CHIKJUGp@A9<M
Mn[<OV@mL\;f?	L*CH;Y@AKu^KM=D[l@A;f=*?	@A9<OmPXKJ`aGEFVJ,;ff?C@AGHMh;  9<;J,;sDJU;ff?C"@AGHMh;>Mh;ff?F<OWPX?OA@;F<K[<I9@AK
;F<OA[<J,;&@A9D?	@:J,KL\K@:`omGHCHC]F<;b2;JW^KCHCHGHZD;momGH@A9yJ,KL\K@  U_
 FKJ,Z<;J@AKJ,;ff?^U9 O,[<^,9 ?rCH;b2;CmKPW=\;J,PXKJ,MR?F<^;sjo;^,9DKOA;t@AKGHM=DCH;MR;F@@A9<;`mJ,G?Z<F<;O
^CH;o?CHIKJ,GH@A9<M KF|?MR?OAOAGHb2;Cpqr=*?	JU?CHCE;CMR?^U9<GHF<; ;I2?FDKZ<;oGp@A98mJ JB8JU?F<OV=*[l@A;J,OUU_
<[lJ,@A9<;J,MhKJ,;s\o;OA;CH;^@A;Z?I;F<;@AGH^h?CHIKJ,GH@A9<M ?OTK[lJTK=<@AGHMRGHff?	@AGHKFB@A;^,9<FDGHg[D;_>8:9<;nJ,;ff?O,KF<O
PXKJ:@A9<GHO^U9<KGH^;>?	JU;
_  ;F<;@AGH^?CEIKJ,Gp@A9<MhO>?	J,;o;CHCiOA[DGp@A;ZrPKJ>=<J,KLDCE;MhOYom9<;JU;@A9<;ROA;ff?	JU^,9OA=*?^;RGHOf9[<I;RLD[l@
o9<;J,;@A9<;JU;h?	J,;RMR?Fqr?^^;=<@,?	LDCH;OAKCH[l@AGHKFDO_h8:9<GHOYGEOT;l?^@ACHqr@A9<;h^ff?OA;h9D;J,;_8:9D;@VJU?w
SA;^@AKJAqOV=e?^;RGHO>9[<I;hLD[l@T@A9<;JU;R?	J,;s L*?	JAJ,GEF<I;^;=<@AGEKFD?C^ff?O,;Os4F[<MR;J,K[<Of?^^;=<@,?	L*CH;
=e?	@A9<O:IKGHF<IRPEJ,KM ; C  @AK 	C; omGp@A9<K[<@:^KCHCHGHOAGEKF\_
MQ.



  

ffff


fifi

$vu]}s11t$vWwvLswe1}%l1r

_  ;F<;@AGH^h?CHIKJ,GH@A9<MhOs\[<F<CEGp2;>?F[<M>L];J&KPQ@A9D;K@A9<;JTK=<@AGHMhGEff?	@AGHKFB@A;^,9<F<GEg[<;O  ;OAOAG&; JU;s
8 ?CpLDG7s`W9[D?^@AGEF\s1x ?;JsjJI2Usi?	JU;hb2;JAq;ff?OVqr@AKGHM=DCE;Mh;F@TKF=*?	JU?CHCE;Ci?	J,^,9DGp@A;^w
@A[<J,;O_!;f9D?ffb2;T=<J,;bGHK[<O,CpqZ<;b2;CEK=];Z?=*?	JU?CECH;C\I;F<;@AGH^f?CEIKJ,Gp@A9<M7~^]"m?F<Zyo;f9D?ffb2;
?CHJ,;ff?Zlq9D?ZtOAGHIFDGpdD^ff?F@;=\;J,GH;F<^;Y[DOAGHF<IGp@T8 ?CpLDGs]v2U_
v_^~ ]"4sW[<F<CHGH2;MhKOV@=*?	J?CHCH;C=<J,KIJU?MhOffsOA9<KomOyCHGHFD;ff?	JuOV=\;;Zwx[<=om9D;F q2K[{ZDK[lLDCH;@A9<;
F[<M>L\;JiKP]=<J,K^;OAOAKJ,OQq2K[RJ,;ZD[<^;@A9<;:^KMh=D[l@,?	@AGHKF@AGHMh;Lqh9D?CHP, ?F<Z;b2;FO,[l=\;JVwxCHGHF<;ff?	J
OA=];;Zlwx[l=t[<F<Z<;J:^;J,@,?GHFt^GpJ,^[DMhOV@,?F<^;OT8 ?CpLDGx  ;O,OAG^; J,;s+JI2U_
e^g

 e1oe1i'ihpi

h

hpfh

tjo

i

b

 ;F<;@AGE^?CHIKJ,Gp@A9DMhOR?	J,;tOA@AK^U9D?OV@AGH^K=<@AGHMhGHff?	@AGEKF|@A;^,9<FDGHg[D;OhGHF@VJ,KZD[<^;ZLq|WKCHCE?F<Z{+*
@xo;F@xqq2;ff?	J,Oj?IK_Q8:9<;qn?	JU;:[<OA;ZGHF?YC?	J,I;b?	JUGH;@xqKP*ZDKMR?GHF<O4GHFD^CH[<Z<GHFDIJ,KL\K@AGH^O:`W9[*?^@AGHF
;@fi?C7_ps2 %?omJ,;F<^;s D?Cp2;F*?[<;1J x  K[leK[DG%es *?Cp2;FD?[<;J x&;CH^U9D?MfLDJ,;s2 
;qIJ,;@ x%+;bGHF<;s\2L\;^ff?[<OA;Y@A9<;qy?	J,;T;ff?OAqu@AKRGHM=DCH;MR;F@:?F<ZtZDKF<K@:J,;g[<GpJ,;Y?CHI;LDJU?GH^
;=<J,;OAOAGEKFyPKJ:@A9D;TP[DF<^@AGHKF@AKL\;YK=<@AGHMRGH;Z\_
_3`ba2`ba$c f X}+ ~<+A
 d<}+9 emXJ
 fng ]f ;e]i h
8:9<;fIK2?CKP @A9<;>?CHIKJ,Gp@A9<MNGHO@AKdDF<Z?h=\KGHF@JU;ff?^,9<GHFDI?AIKKZ  b?CH[<;TKP4?RIGpb2;FPX[<F<^@AGEKFkj
Kffb2;J?YOA;ff?	J,^U9ROV=e?^; G _4fiGHJ,OV@s?&g[D?F@AGHff?	@AGHKFhOV@A;=GHO4Z<;dDF<;ZPXKJj?F<Zh@A9<;OA;ff?	J,^U9RGHO4^KF<ZD[<^@A;Z
Kffb2;JY?Z<GEOA^J,;@A;TOA[<LDOA;@s G PTKP G _ G PT^KF@,?GHF<O&ml;CH;MR;F@AO_  Ft=<JU?^@AGH^;s*@A9<;T^ff?	J,Z<GEFD?CHGp@qKP G P
^ff?FL];uo n2A7 qRff nCE?	J,I;_m<KJ&;<?M=DCH;seGHFK[lJWGHM=DCE;Mh;F@,?	@AGEKFKPQl+~D\]s  ff I_8:9[<Os
?^KF@AGHF[<K[<O:Z<KM?GHFyGHOZ<GHO,^J,;@AGH;ZyomGH@A9t?IGpb2;FyJ,;O,KCH[l@AGHKF\_
&[lJ,GEF<IR?FGEF<Gp@AGE?CHGEff?	@AGHKF=D9D?OA;n?hOAM?CHC"OA[lL*OA;@KP G P>GHOmZ<JU?omFB?	@mJU?FDZ<KMy_:8:9<GEO:OA[lLDO,;@mGHO
^ff?CHCH;ZA? r2:r"pl%^ 2<_?^,9;CH;Mh;F@mKP@A9<GHO=\K=D[<C?	@AGHKFGHO^KZD;ZyLqy?OV@VJUGHF<IKP  LDGH@AO_
8:9D;YI;F<;@AGH^f?CHIKJUGp@A9<M Gp@A;JU?	@A;Om@A9<;TPXKCHCHKomGHF<IPXK[lJmOV@A;=DOm[<F@AGEC]?hOAKCH[l@AGHKFyGEO:PK[DF<Z\_
C_ ae1i'g#ejfi ?Fl@A9<;=\K=D[DCE?	@AGHKFh?^^KJ,Z<GEF<IY@AKT@A9D;:b?CH[<;K3P jPXKJ;ff?^U9;CE;Mh;F@jKP G P_
&;^GHZD;TGHP"@A9D;YL\;OV@:;CH;Mh;F@m^ff?FtOA;J,b2;f?O?F?^^;=D@,?	LDCH;fO,KCH[l@AGHK3F lGHP+q2;Os*;lGp@_
._ NhpihjjfQfiOA;i@A9<;QPX[<F<^@AGHKF j@AKmZD;dDF<;i?=DJ,KL*?	LDGHCEGp@xqWZ<GHOV@VJ,GHLD[l@AGHKF&Kffb2;J @A9<;Q=]K=*[<CE?	@AGHKF\_
l;CH;^@?=*?GHJ:KP;CH;MR;F@AOmJU?F<Z<KMhCHqu?^^KJ,Z<GEF<I@AK@A9<GHO:=<J,KL*?	LDGECHGp@xqhZDGHOV@VJ,GpL*[l@AGHKF\_
v_ h n#opj g#jfQ 'QJ,KZ<[<^;T?FD;o;CH;Mh;F@mPEJUKM;ff?^,9=*?GpJ:[<O,GHF<IBAI;F<;@AGH^  K=];J?	@AKJ,O_
_ h n#ie$hpbdhjf2" m;=*CE?^;@A9<;;CH;Mh;F@AO>KP@A9<;ROV@,?	JA@AGHF<I=\K=D[<CE?	@AGHKFLqL\;@V@A;JnF<;ok;CH;w
MR;F@AOm=<J,KZ<[<^;ZyGHFyOV@A;=Bv_
?FqI;F<;@AGH^uK=\;JU?	@AKJ,O& Y?bGHZ<KJffs:2?	JU;u?ffb?GHC?	LDCH;_Koj;b2;Jffs@A9<;uMhKJ,;u^KMhMhKF<Cpq
[<OA;Z ?	J,;y@A9<; qqpl7& 2?F<Z|@A9<;9 2?6@6:032mrhK=\;JU?	@AKJ,Off_ 8:9<;yMn[l@,?	@AGHKF K=\;JU?	@AKJ^KF<O,GHOV@AORKP
JU?F<ZDKMhCpt
q sDGH=<=DGHF<IOAKMh;LDGH@AOuKP?F;CH;MR;F@tKP>@A9<;=\K=D[<CE?	@AGEKF\_c8:9<;^J,KO,OwxKffb2;JK=\;JU?	@AKJ
^KF<OAGEOV@AOTKPd<J,OV@&JU?F<ZDKMhCpq^U9<KKOAGHF<It?u=DC?^;om9<;J,;@AKt^[<@Y@A9<;@xoKtOV@VJUGHF<IOYKPLDGp@AOs?FDZ@A9<;F
LD[<GECHZ<GHF<Ih@xoKtF<;oc;CH;MR;F@AOTPJ,KM@A9<GEO&=*?GpJYLqOAGHMh=DCpqICH[DGHF<I@A9<;J,GHI9@T?F<Zr@A9<;CH;P@&=*?	JA@AOTKP
@A9<;TGEF<Gp@AGE?C*=*?GpJ:KP OV@VJ,GEF<IOYOA;;T GHI[lJU;T2U_
!;[<OA;TL]K@A9BK=\;JU?	@AKJ,O@AKR=DJ,KZD[<^;TF<;o ;CH;Mh;F@AO_m GpJ,OA@s*o;f[<O,;T@A9<;n^J,KOAOwxKb2;JTK=\;JU?	@AKJ
@AKBI;@?FGHF@A;J,Mh;ZDGE?	@A;ROV@VJUGHF<I_8m9<;F\s @A9<;uM>[l@,?	@AGEKFK=\;JU?	@AKJGHOn[<OA;ZKF@A9<GHOfGEF@A;J,MR;Z<GE?	@A;
OV@VJ,GEF<In@AKhI;@m@A9<;TdDFD?C\OV@VJ,GEF<I_
ff?Q

fiy

\DW$us<|t"]

s|Bv}

PARENTS



v99 v}v

NEW ELEMENTS

Crossover

k

fiGEI[lJ,;Y8:9D;W^JUKOAOwxKffb2;JYK=\;JU?	@AGHKF"_
_3`ba2`vuc fX}+~<+we]" c ]+<d<}+9emXJfng]f;e]ihyx~"]+{z

8:9<;JU;Q?	J,;4MR?FqW=e?	JU?CHCH;C	b2;J,OAGEKF<O"KPI;F<;@AGH^i?CHIKJUGp@A9<MhO\@A9D;4OV@,?F<ZD?	J,ZT=*?	JU?CHCH;C	b2;J,O,GHKFh&mKL\;JA@w
OAKF\s+ *Us@A9<;4Z<;^KM=\KOAGH@AGHKF&b2;J,OAGHKFR8fi?F<;O,;s+ *+?F<ZY@A9<;4M?OAOAGpb2;CpqW=*?	JU?CECH;C	b2;J,OAGHKFh8 ?CpLDG7s
v2U_ !;T^,9<KO,;Y@A9<GHOCE?OA@:Mh;@A9<KZ\_Q8:9<;YMR?GEFyGHZ<;ff?GHO@AKR?CECHK^ff?	@A;fKF<;Y;CH;Mh;F@mKP@A9<;&=\K=D[<CE?w
@AGHKFPKJ&;ff?^,9B=<J,K^;OAOAKJWOAKh@A9*?	@WOV@A;=*OTs\vs]?F<ZR^ff?FL\;f;l;^[l@A;ZGEFt=*?	JU?CECH;C_<[lJA@A9<;JUMhKJ,;s
@A9<;tO,;CH;^@AGHKFOV@A;=OV@A;=zGEOh^ff?	JAJ,GH;Z|K[<@RCHK^ff?CHCpqsGEF@A9D?	@R;ff?^,9 GHF<ZDGpbGEZ<[D?CQMR?ffqMR?	@A;tKF<CHq
omGp@A9@A9D;YGHF<Z<GpbGHZ<[*?CHO4=DCE?^;ZKFy=<J,K^;OAOAKJ,O:=D9qOAGH^ff?CECpqR^KF<F<;^@A;Z@AKGp@_i8:9DGHO;F<OA[lJ,;O@A9*?	@:@A9<;
^KMhMn[<F<GH^ff?	@AGEKFrKffb2;J,9<;ff?ZZ<K;OYFDK@&GEF<^J,;ff?OA;?OY@A9<;F[<M>L];JYKPi=<J,K^;OAOAKJ,OYGHFD^J,;ff?OA;O_n8:9<GHOWGHO
@A9<;YJU;ff?OAKFyom9q^~ ]"O,9<KffomOmCHGHF<;ff?	J:OV=\;;Zwx[<="_
8:9D;m=*?	J?CHCH;CDI;FD;@AGH^W?CEIKJ,Gp@A9<MkGp@A;JU?	@A;O@A9<;WPXKCHCHKffoGHF<ITPXK[lJOV@A;=DO[<F@AGHC*?nOAKCH[<@AGHKFRGHOjPXK[<F<Z\_
C_ ae1i'g#ejfiiQb?CH[D?	@A;X
 rVX%ffD?CHC]@A9<;TGEF<Z<GpbGHZ<[D?CEO_
._ NhpihjjfQQ;CH;^@X8 rVX%ff(sl?MhKF<IW@A9<;jF<;GHI9L\KJ,Os@A9<;MR?	@A;jomGp@A9>@A9<;jL\;OV@ ;b?CH[*?	@AGHKF\_
v_ h n#opj g#jfQ m;=<J,KZ<[<^;X
 rVp<omGp@A9@A9<;T^U9<KOA;FMR?	@A;_
_ h n#ie$hpbdhjf2 ;=DCE?^;X
 rVX%ffl@A9<;&=*?	J,;F@AO:Lqu@A9<;YKeOA=<J,GHF<I_
K&FB@A9<; ;I2?F<KZ<;s\oj;GHMh=DCH;Mh;F@A;ZB@A9<;n^~ ]"|KFr?@AKJ,[<O&KPQ=DJ,K^;O,OAKJ,OWom9<;J,;n;ff?^,9GHF<Z<G%w
bGHZ<[D?C]9D?O:PXK[lJmFD;GHI9L\KJ,OYO,;;T GHI[lJ,;ff 2
ebd



j /j^|#h Aj~}#g#f#jf

 e1oe1i'ihpia peig#e f

:8 9<;j;b?CH[D?	@AGHKFPX[<F<^@AGHKF<Ofi[<OA;ZnGHFffleeu?F<Zl+~<+\>?	J,;jb2;JAq>OAGEMhGHCE?	Jff]@A9<;q>L\K@A9n^KMh=D[l@A;
@A9<;4d*FD?C=\KOAGp@AGHKFYKP@A9D;i?	J,MIGHb2;F>? ?F<9D?	@V@,?F>=*?	@A9>KP?dl;Z>KJ,Z<;Jff_  FfK[lJGHMh=DCH;Mh;F@,?	@AGHKF\s
L*?OA;ZKF{;=\;J,GH;FD^;so;^,9DKOA;r@AK|[<OA; ?F<9D?	@V@,?F=*?	@A9<OtKPnKJ,Z<;J
_ KWJUZ<;J|?	=<=\;ff?	J,;Z
@AKL\;?IKKZ^KMh=<J,KMhGHO,;yL\;@xo;;F@A9<;F[DMfL\;JRKP&CE?F<ZDMR?	JAOhF<;;Z<;ZGHF<^J,;ff?OA;O?ORKJUZ<;J
Z<;^J,;ff?O,;OUT?F<Z@A9<;h^KM=*[l@AGHF<I@AGHMR;hF<;^;OAO,?	J,qPXKJf@A9D;hK=<@AGHMhGEff?	@AGHKFPX[<F<^@AGHKF<ORGHF<^J,;ff?OA;On?O
KJ,Z<;JfiGHF<^J,;ff?O,;OUU_4lGHF<^;QK[lJfiJ,KL\K@+9D?OfiOAG%ffi+*s	@A9<;?	J,I[DMh;F@fiKPl@A9<;^KOV@fiPX[<F<^@AGEKFTGHFnl+]eGHO
?Wb2;^@AKJGHF  ,40   ,, > 0, >=> 7, >===ff> 0, >===	> 70  ?F<Z@A9<;:?	J,I[<MR;F@QKP*@A9<;^KOV@iPX[<F<^@AGEKF[<OA;Z
PXKJl+~<+\GEOm?b2;^@AKJ&GHF 	 	  ,40 b > ,, > 0, >=> 7, >===	> 0, >===ff> 70 om9<;J,
; j^KZ<;Om@A9<;
CE?F<ZDMR?	JA[<OA;ZR?O?TOV@,?	J,@AGHF<IY=\KGHF@iPXKJi@A9<;=e?	@A9\_  FhL]K@A9^ff?OA;O@A9<;mPX[<F<^@AGHKFDOi?	J,;mZD;dDF<;ZhKF<CHq
KF?L\K[<FDZ<;ZyOA[lLDO,;@:KP 	  ,40 ?F<Z 	 	  ,40 sDom9<KOA;YCHGEMhGp@AO:?	J,;Yd<;ZLq@A9<;TMh;^U9D?F<GH^ff?C"OA@AK=DO
KPj@A9<;J,KL\K@>?F<Z@A9D;hMR?lGHM>[DM F[<M>L\;JTKPCE?F<Z<MR?	J,O_` Z<GHO,^J,;@AGHff?	@AGHKFrOV@A;=GHOY^,9DKOA;FPXKJ
@A9<;OA;m@xoK>O,[lLDOA;@AO4LqhZ<;dDFDGHF<IW@A9<;:JU;OAKCH[l@AGHKF?	@iom9<GE^,9h;ff?^U9u;CE;Mh;F@,?	J,qhMhK@AGHKFRGEOiZ<GHO,^J,;@AGH;Z\_
ffffz

fi$vu]}s11t$vWwvLswe1}%l1r
fi

"HOST"

"ROOT"



INDIVIDUALS

 GHI[lJ,;ff` A@ KJ,[DOomGp@A9OAG%@A;;F=<J,K^;OAOAKJUO_/K&F<;GHF<Z<GHbGHZD[D?CQGHO=DC?^;Z|KF|;ff?^,9=DJ,K^;O,OAKJff_
?^,9GHF<Z<GHbGHZD[D?CD9D?OmPK[lJF<;GHI9L]KJUO_
 FrK[lJY^ff?OA;s;ff?^U9& GHO&Z<GEOA^J,;@AGH;ZBomGp@A9ruLDGH@AO&?FDZ@A9<;F[<M>L\;JYKPiCE?F<ZDMR?	JAOWGEOWCEGHMhGp@A;Z@AK
 I_f8:9[<Os\IGpb2;Fr?LDGHF*?	JAqtOV@VJ,GHFDIRKP:ffIa#  LDGp@AOffs*o;^ff?Fr^KFb2;JA@TGH@WGHF@AK?Rb2;^@AKJ
7?O?F?	J,I[<Mh;F@UPXKJm@A9<;Y^KOV@P[<FD^@AGHKFtKP4ffl+]e slKJl+~D\]sJ,;OV=\;^@AGpb2;Cpq_
?F<9D?	@V@,?Fr=e?	@A9<OT?	J,;;b?CE[D?	@A;ZGHFr?uOAGHM=*CHGpdD;ZtMRKZ<;C KP@A9<;;FbGpJ,KFDMh;F@_n8:9<GHOWMhKZ<;C
GHOKL<@,?GEF<;ZyLqu;F<^CEKOAGHF<I;ff?^U9;CH;Mh;F@mKPfi@A9<;YOA^;FD;TGHF@AKR?nL\K[<F<Z<GHFDIfJ,;^@,?FDI[<CE?	J:L\K]_
8:9D;Y;b?CH[*?	@AGHKFtKP ?b2;^@AKJWGEO=\;J,PKJUMh;Zy?OmPXKCHCHKffoO
<KJW;ff?^,
9 & GHFr  ,, > 0, >=> 7, >===ff> 0, >===p> 70 
KM=D[<@A;&@A9<;YCHGHMhGH@AOjKF@A9<;YMhK@AGHKFPKJQSVKGHF@ A_
KM=D[<@A&; &o LquL\K[<F<^GHFDI>KF@A9<;OA;TCHGEMhGp@AO&OA;;>;^@AGEKFv_U_
m=\ZD?	@A;&@A9D;Y=\KOAGp@AGHKFKPfi@A9<;&J,KL\K@_
8:9D;TCHGHMhGH@AOKFt@A9<;>MhK@AGHKFKP*SAKGHF@ ?	J,;TKL<@,?GHFD;ZtLqMR;J,IGHF<I@A9D;fCH;I2?C+JU?F<I;OWKPfiMRK@AGHKF
KP+?CHC*@A9<;WCHGHFlOQ@A9*?	@jMhKb2;&om9<;FYSVKGHF@  MRKffb2;OsD?F<Zy?CHCD@A9<;WKLDOV@,?^CH;O_8KnKL<@,?GHFy?>CH;I2?CeJU?F<I;
KP]MhK@AGHKFL\;@oj;;Fu?YCHGHFl?F<Zh?FRKL*OV@,?^CH;so;:^KF<O,GHZ<;JQ@A9<;@ojK>;F<^CHKOAGEF<I&=*?	JU?CHCE;CH;=DGp=\;Z<O ?F<Z
;=<J,;OAOW@A9<;GpJY^KKJ,Z<GHF*?	@A;OYGHFB@A9<;mSVKGHF@YPEJ?Mh;_f8:9<;F"seo;[<OA;?u^CE?OAOAGE^ff?C4Mh;@A9<KZ@AKy^KMh=D[l@A;
@A9<;YJ?F<I;& %+Kff?F<K	(w '";) J,;s + *U_
 FtK[lJ:=e?	JU?CHCH;C\GHMh=DCH;Mh;F@,?	@AGHKF\so;TZ<GHOV@VJUGpLD[l@A;Zu@A9<;YI;KMh;@VJUGH^f^KM=*[l@,?	@AGHKF<Om?MRKF<IhOA;bw
;JU?C]=<JUK^;OAO,KJ,O_Q?^U9y=<J,K^;OAOAKJGHOZ<;Z<GH^ff?	@A;Zy@AK@A9D;Y^KM=D[l@,?	@AGHKFKP?OAGHFDICH;W@xq=];&KPfiGHF@A;JU?^w
@AGHKF\_
e

 n

 e1oe1i'ihpi Bb

j

ihjbdhpfe f

j^|#hoefhDih5u

tjo

i

b

 HG FD?CHCHqsl@A9<;f`J,GE?Z<FD;Om^CH;o ?CHIKJ,Gp@A9<M GHOmGHMh=DCH;Mh;F@A;ZtGHFy=e?	JU?CHCH;C\oGp@A9y@A9lJ,;;>CH;b2;CHOKP=e?	JU?C%w
CH;CHGEOAMy_
s_ K&LbGHK[<OACHqs?d<J,OA@RCH;b2;C&KP&=*?	JU?CHCE;CHGHff?	@AGHKF^ff?F L\;tKL<@,?GEF<;ZLq|J,[<FDF<GHF<Il+]*?F<Z
<+~<\?	@@A9<;O,?MR;@AGHMR;KF @xoKOA;@AOyKPY=<JUK^;OAO,KJ,O_! 9DGHCH;Bl+]eGHOu^,9<;^,GEF<I
ffff

fiy

\DW$us<|t"]

s|Bv}



v99 v}v

o9<;@A9<;Jh?=*?	@A9;GHOA@AO>L\;@oj;;F@A9<;CE?OA@>=DC?^;ZC?F<Z<MR?	JA?FDZ@A9D;IK2?Csjl+~<+\GHO
I;FD;JU?	@AGHF<I@A9D;TF<;@:CE?F<Z<MR?	J,*_
_m8m9<;>O,;^KF<ZCE;b2;CfiKPQ=*?	JU?CHCH;CEGHOAM^KJAJU;OV=\KF<Z<O:@AK?=*?	JU?CHCE;C"GHM=DCE;Mh;F@,?	@AGEKFKP4L\K@A9BI;w
FD;@AGH^y?CHIKJ,Gp@A9<MROn;Mh=DCHKffq2;ZLq|ffl+]e ?FDZl+~<+\@AK@VJ,;ff?	@h@A9<;GHJnJU;OV=\;^@AGpb2;K=<@AG%w
MRGHff?	@AGHKFy=<JUKLDCH;MhO_
v_m8m9<;&@A9<GpJ,ZCH;b2;C+^KJAJ,;OV=\KF<Z<O@AKh?=*?	JU?CHCH;CEGHff?	@AGHKFKP+@A9D;Y^KCHCHGHOAGEKFu^U9<;^AGHF<IRP[<FD^@AGHKFt?F<Z
J?F<I;T^KM=D[<@,?	@AGHKF\_
!;Y^KM=*CH;@A;Zy?>PX[<CHC*GHM=DCE;Mh;F@,?	@AGEKFKP\@A9D;OA;W@A9lJ,;;&CH;b2;CHOKFy? ;I2?F<KZ<;WomGp@A9
 e]J J
@VJU?F<OA=D[l@A;J,O_>fiGEI[lJ,;uJ,;=<J,;O,;F@AOTK[<JY=*?	JU?CHCH;CfiGHM=*CH;Mh;F@,?	@AGHKFKPQ@A9<;`J,GE?Z<FD;OW^CE;oc?CHIK	w
J,Gp@A9DM?F<Zt GHI[lJ,;nhOA9<KffoO:9<Kffooj;T9*?b2;T;M>L\;Z<Z<;Zy@A9<GEO:?	J,^,9<GH@A;^@A[lJ,;TGHF@AKK[lJm;=\;J,GHMh;F@,?C
OA;@A[l=+_` *+|OVqOV@A;
M x"6 e\z&GEOY[<OA;Z@AKtMhKZ<;C @A9<;O,^;F<;omGp@A9@A9<;@ojKtJ,KL\K@AO_8:9<;nJ,KL\K@AO
?	J,;Y[<FDZ<;J@A9<;Y^KF@VJUKC"K>P +5 i&?qo?	J,Z"s Y?F<;OA9<Mh;FDZ\s xaW?ffq?	@AGs"2U_ GpJ,OV@sD?OAGEM=DCHGpd*;Z
I;KMh;@VJ,GE^MhKZD;CKPi@A9<;>O,^;F<;GHO&Z<KffomFDCHK2?Z<;ZGHF@AKR@A9<;Mh;MRKJAqKPQ@A9D;>@VJ?F<OV=D[l@A;JUO_Y8:9<;F\s"?
GHCEGH^KF  JU?	=D9<GE^O ojKJAOV@,?	@AGHKFRojKJ,Oi?Oj?YICHKL*?Cl^KF@VJ,KCHCE;J?F<ZhCHKK=DOQKb2;Ji@A9<;mPKCHCEKffomGHFDIWOA@A;=DO

_  ;F<;JU?	@A;>?FDZt;;^[<@A;>?CH;I2?C\J?F<Z<KMMhK@AGEKFtPKJmJ,KL\K@  _
_Wl;F<Z@A9<;F<;o^KFld*I[lJU?	@AGHKFrKPJ,KL\K@  @AKy@A9<;
^KF<dDI[lJU?	@AGHKFPKJ:JUKL]K@:`>_
v_  ;@m@A9<;&=DC?F<F<;Z=*?	@A9tPXKJ:J,KL\K@:` PJ,KM @A9<;

;I2?FDKZ<;?OYoj;CECQ?OY@A9<;Z<;OAGHJ,;ZBdDFD?C
;I2?F<KZ<;f?F<Zt;l;^[l@A;>Gp@_

_ !?GH@mPKJ&?nJU?F<Z<KM@AGHMh;T?F<ZtOA@AK=tJ,KL\K@:`f_
_  K@AKy_
8:9DGHOYOA;g[<;F<^;R?CHCEKffomOT[DOY@AKt@A;OV@nK[lJ>?CHIKJUGp@A9<M;@A;F<OAGpb2;CHqBGHFJ,;ff?CQOAGp@A[*?	@AGHKF<OYLqB9*?bGHF<I
@AKBZ<;ff?CioGp@A9MR?FqZ<GHe;J,;F@f;FbGHJ,KF<Mh;F@AO_$KYPj^K[<J,OA;s4@A9<;uMhKOV@nGHF@A;JU;OV@AGHF<IdDI[lJ,;Roj;u^ff?F
KL<@,?GHFPEJUKM @A9<GEOm;=];JUGHMh;F@mGHO:@A9D;fMh;ff?F@AGHMh;>F<;^;OAO,?	JAqy@AKu^KM=D[<@A;fKF<;f=*?	@A9IGHb2;FB?RF<;o
;FbGpJUKF<Mh;F@_Q<KJ:@A9<GHO;=\;J,GHMh;F@,?C]OA;@A[l=@A9<GHOMh;ff?Fy@AGEMh;GEO&_fOA;^KF<Z<Off_QOAGEF<If@A9D;YO,?Mh;
?	J,^U9<Gp@A;^@A[lJ,;WomGH@A9MhKJU;W[<=lw7@AK	wxZD?	@A;W=<J,K^;OAOAKJ,O& e]J JJ2joK[<CHZJ,;Z<[<^;@A9<GHO@AGHMh;Lqu?>PX?^@AKJmKP
@A;F\_i8m9<;&O,?Mh;T^KM=*[l@,?	@AGHKFtKF?O,GHF<ICH;W=<J,K^;OAOAKJ>xff~+]
 joK[<CHZ@,?	2;Y@A9<J,;;Y@AGHMh;OCEKF<I;J
@A9D?Fy@A9D;T^[lJAJ,;F@:GHM=*CH;Mh;F@,?	@AGHKF\_
 6+p q"qh:   m r2 7 ro D2mp8 qhX5 2ff2o 478rD3 2rXy< jik 6Y6 , Ar2?6@6& 4ff%
 4< 
< 
 W e9  6Y%   5 2< 5q
h 2r*%r@ 2X(k 6U& 2 0VU r< +6*72YhV,\9 25422 4< qh ffi+X
 * qV,$b 6R9 rX9 2 qRD_

. 1 0   5l671 0 1 0 6j5<2671 0 g V6Fl5  26  g + 0 3  ?  j26 [



`WOMh;F@AGEKF<;Z|GHF@A9<;  F@VJ,KZ<[<^@AGEKF\s4@A9<;y`J,GE?Z<F<;O^CH;o ?CEIKJ,Gp@A9<M9D?O@ojKrM?GHFg[D?CEGp@AGH;O
ff*= sD?F<Z]V(^_ %;@m[<OslGHFt^KFD^CH[<OAGHKF"s;=DCE?GEF?FDZtZ<GHOA^[DOAO@A9<;OA;Y@ojKRg[D?CEGp@AGH;O_
+ 

fifi

$vu]}s11t$vWwvLswe1}%l1r

+  7;v- o 6 7-  ;
L<7
+(  

 vv
<

<

 7v-v ;v7  v

 7v7-;v7  -



 ;g
 8

 ;gg
 8

 

  (

 

 

 

  (
  v

  v

  

  

 GHI[lJU;i`=*?	JU?CHCH;C]GHMh=DCH;Mh;F@,?	@AGHKFyKPfi@A9<;T`J,GE?Z<FD;O^CH;o?CHIKJ,GH@A9<M
+ff

fiy

\DW$us<|t"]

v99 v}v

Robot II

Robot I

68030

68030
KALI



s|Bv}

Bus VME

KALI

Bus VME
(VxWorks)

(VxWorks)

Sun 4
(Unix)

(SEARCH)

Ethernet

server
VxWorks

GENETIC ALGORITHM

Sun 3
(Unix)

GENETIC ALGORITHM
(EXPLORE)

server
MegaNode
MegaNode
128 Transputers

ACT
CAD SYSTEM

Silicon Graphics
(Unix)

 GHI[<J,;i8:9<;Y;=\;J,GHMh;F@,?C\OA;@A[l=
fe^g

/|(jobde1f#h

 hjo

 KM=e?	J,GHF<I@A9D;Q=\;J,PXKJ,MR?F<^;KPl@A9<GHO+GHFDZTKPD?CHIKJ,GH@A9<MGHO ?b2;J,qfZ<;CHGH^ff?	@A;OA[lLSA;^@_1'fi;JUPKJ,M?F<^;
MR?ffqBL\;h?yM?	@V@A;JnKP^KM=D[<@AGHF<I@AGHMh;sfi;eKJ,@AOfF<;;Z<;Z@AKt=<J,KIJ?MysKJn;ff?OA;hKP?	=<=DCHGE^ff?	@AGHKF@AK
Z<Gp];J,;F@=<J,KLDCH;MROOA;;B;^@AGHKF_$U_|Qb?CH[D?	@AGHFDI@A9<;u=\;J,PXKJ,MR?F<^;GHF@A;J,MROKP^KM=*[l@AGHF<I
@AGHMh;YGEOb2;JAqyZ<GpR^[<CH@PXKJmKF<;YPX[<F<Z*?Mh;F@,?C?F<Z@A9lJ,;;Y=DJU?^@AGH^ff?C\J,;ff?O,KF<O
+:

fifi

$vu]}s11t$vWwvLswe1}%l1r

_m8m9<;tPX[<F<ZD?Mh;F@,?C:JU;ff?OAKF GEOsKF<^;?I2?GHF"s@A9<;}"~wx^KM=DCH;@A;F<;O,ORKP&@A9<;=*?	@A9=*CE?F<F<GHFDI
=DJ,KLDCH;My_`WOfZ<;^;=<@AGHb2;R^ff?OA;OnMR?q?CHo?ffqO>L\;hZ<;OAGHIFD;Z\s+@A9<;KF<CpqB=\;J,PKJUMR?F<^;J,;O,[<Cp@AO
KFD;TMR?qJ,;ff?O,KFD?	LDCpqu=<J,;OA;F@m?	J,;f?CHo?ffqOOV=\;^GpdD^_
_m8m9<;&@A9lJ,;;Y=DJU?^@AGH^ff?C\J,;ff?O,KF<Om?	J,;
7?2
 K&LbGHK[<OACHqs@A9<;id<J,OV@+J,;g[DGpJ,;Mh;F@+PXKJfiOA[D^,9?:^KM=*?	JUGHOAKFnGHO"@A9D?	@ Z<Gp];J,;F@?CHIKJ,GH@A9<MhO
JU[<FYKFT@A9<;iO,?Mh;4MR?^U9<GHF<;O+omGp@A9&@A9<;QOU?Mh;i?b?GHCE?	L*CH; Mh;MhKJAq_ 8:9<GHO\M?qYOA;;MO,GHM=DCH;
L*[l@YGp@WGEOT?M?GHFZ<GpR^[<CH@xqGHFK[lJY^ff?OA;hL];^ff?[DOA;K[lJf?CHIKJ,Gp@A9DM9*?O&L];;FZ<;O,GHIF<;Z
@AKJU[<FKFBJU?	@A9D;JTOV=\;^GpdD^TGHF<ZDOKPMR?^,9<GEF<;Os\FD?Mh;Cpqs"M?OAOAGpb2;Cpqy=e?	JU?CHCH;CfiKF<;O_  @
^K[DCHZr?CHOAKL];nGHM=*CH;Mh;F@A;ZKFF<KFw7=e?	JU?CHCH;CMR?^,9<GEF<;Os]LD[l@W@A9<;FGp@&MR?qBCHKOA;>=*?	JA@
KPjGp@AO&GHF@A;J,;OV@_n`cP?GpJY^KM=*?	J,GHO,KFojK[<CEZL\;>@AKy^KM=*?	J,;n@A9<;h?CEIKJ,Gp@A9<MhO&KFL\K@A9
@q=\;OKPMR?^U9<GHF<;Off_i8:9<GHOjoK[<CHZyGHMh=DCpqh=<JUKIJU?MhMhGHFDIK@A9<;J?CHIKJ,GH@A9<MhOGHFy=e?	JU?CHCH;Cs
o9<GH^,9yGEOb2;JAqyZ<GpR^[DCp@jGEF=<JU?^@AGH^;_
Le ?FqF<KffomF =*?	@A9 =DC?F<F<GHF<I?CHIKJ,Gp@A9<MROdDJ,OV@^KMh=D[l@A;B@A9<;^KFldDI[lJ?	@AGHKF OA=*?^;
KJY?F?	=D=<J,KlGHMR?	@AGEKFtKP Gp@UK*wxCHGHF<;s*?FDZt@A9<;F;R^GH;F@ACpqyOAKCpb2;Y@A9<;f=*?	@A9t=*CE?F<F<GHFDI
=DJ,KLDCH;MKFwxCHGHFD;_&`WOWoj;O,?ffoTs"GHFBKJ,Z<;J&@AKZ<;ff?CfiomGp@A9?uZlqF*?MhGH^f;FbGpJ,KF<Mh;F@s]@A9<;
`J,GE?ZDF<;O:^CH;o?CEIKJ,Gp@A9<M?Z<K=D@AOm?^KM=DCE;@A;CpquZDGpe;JU;F@m?	=<=DJ,K2?^,9\_
^DKJT=<JU?^@AGH^ff?C JU;ff?OAKF<Os"MR?Fq@A;OA@Y=<J,KLDCH;MROW?	J,;n@AKqB=DJ,KLDCH;MhOnx4s"PX;oKLDOV@,?^CE;Os
PX;o P?^;OsOAGHMn[<CE?	@A;ZJ,KL\K@AOU?F<Z@A9D;=];JUPKJ,M?F<^;uJ,;OA[<Cp@AO[DOAGHF<IB@A9<;OA;GHF<ZDOnKP
=DJ,KLDCH;MhO?	J,;mb2;J,qhZ<GpR^[<CH@4@AK>I;F<;J?CHGH;:@AKfJ,;ff?CHGHOA@AGH^:GHF<ZD[<OV@VJ,GE?C=<JUKLDCH;MhO7v4s@A;F<O
KP KLDOA@,?^CH;OsD9[<F<ZlJU;Z<OKPfiP?^;Os<JU;ff?C\J,KL\K@AOUU_
KFDOAGHZ<;J,GEF<Ij?CEC@A9<;OA;4JU;ff?OAKF<Os	oj;Q@A;OV@A;Z>K[<J?CHIKJ,Gp@A9DM{Lq&GHM=*CH;Mh;F@AGHF<I?J,;ff?CHGEOV@AGH^fiJ,KL\K@AGH^
?	=<=DCEGH^ff?	@AGHKF@AK@A9<;Rb2;JAq;F<Z\_8KB?^,9<GH;b2;u@A9<GEOfIK2?Cs4o;u?O,OA;MfL*CH;Z?^KM=DCH;;=\;J,GHMh;F@,?C
OA;@A[l=zGHF<^CE[<Z<GHF<IBOAG%Z<GHe;J,;F@RMR?^U9<GHF<;OBY
 h" ]++}++\s
 IJ Jv s& "}s?F<Zr= X+ X]}
]]+~< eUsm@ojK|Mh;^U9D?F<GH^ff?CY?	J,MROsW?F<ZzJU[<F<F<GHFDIOA;b2;F{Z<Gp];J,;F@u^KK=\;JU?	@AGpb2;=<JUKIJU?MhOx
++ xs\f"^ es*
 3 R+3  se>~+]4sD?FDZf`mJUGE?Z<F<;O:^CE;o?CHIKJ,Gp@A9DMU_
K&[<J^U9D?CHCH;FDI;o?O@AKL\;W?	L*CH;m@AKO,KCpb2;W@A9<;&=*?	@A9u=DCE?F<FDGHF<IY=<JUKLDCH;M PX?OA@;FDK[<I9@AKZlJ,Gpb2;Y?
J,;ff?CeOAG%Rh?	J,MkGHFu?fZlqFD?MhGE^:;FbGpJ,KFDMh;F@_48:9D;m`J,GE?Z<F<;O^CH;o{?CHIKJ,Gp@A9<MGHF<Z<;;Zu?^,9<GE;b2;Z
@A9<GHOIK2?C"GEFK[lJ:;=\;J,GHMh;F@AOjom9<;J,;W@A9<;&;FbGHJ,KF<Mh;F@GHO^KM=\KOA;ZyKP"d<b2;&d<;ZKLDOV@,?^CE;Om?F<Z
?OAGp?	J,MMhKbGEF<IGHF<Z<;=\;F<ZD;F@ACpq_
!;&?	J,;:F<K@?o?	J,;WKP]?FqhK@A9<;JMh;@A9<KZ<OQ^ff?	=*?	L*CH;:KP\OA[<^U9h=\;J,PXKJ,MR?F<^;_48fiKY@A9<;mL];OA@iKP]K[lJ
F<KffomCE;Z<I;s^[lJAJU;F@ACpqnGHM=DCE;Mh;F@A;Zn=DC?F<F<;J,OojK[DCHZ@,?	2;m?&F[<MfL\;J4KPeOA;^KF<Z<O@A;F* @AK&=DCE?^;:?
OA;@KP+C?F<Z<MR?	JAOjKFt?;l?M=*CH;&PKJ:?nJ,KL\K@jomGp@A9ud<b2;Yt& f?ffbJ?	G\;@:?C_ps]J I2U_ W;OA=DGp@A;
@A9<;YP?^@:@A9D?	@dDFDZ<GHF<I?I;F<;JU?C\=*[lJA=\KOA;=DCE?F<F<GEF<If@A;^U9<F<GHg[<;&PXKJ:J,;ff?C\GHF<ZD[<OV@VJ,GE?C]?	=<=DCEGH^ff?	@AGHKFGHO
?nb2;JAqyZ<Gp^[<Cp@i=DJ,KLDCH;Myso;YL\;CHGH;b2;W@A9D?	@@A9<;Y`J,GE?ZDF<;O^CH;o?CHIKJUGp@A9<M =<J,KbGEZ<;O?Ft;e;^@AGHb2;
?	=<=<JUK2?^,9t@AKROA[<^,9t=<J,KLDCH;MRO_
8:9D;F[<M>L];JKPmJU?F<I;^KM=D[l@,?	@AGEKF<OPKJh? ?F<9D?	@V@,?F|MhK@AGEKF|KPmKJ,Z<;JyGHO H@L s,
om9<;JU.; ,GHOm@A9<;TF[<MfL\;JWKPfiP?^;Os @A9<;>F[DMfL\;JmKPQfi+*s*?F<Z H ?R^KF<OV@,?F@P?^@AKJffs]Z<;=\0;F<Z<GHF<I
KF@A9<;F[<M>L\;JKP\=*?	J,@AO[<O,;Zh@AKMhKZ<;C<@A9<;mJ,KL\K@_ KWLbGHK[DOACpqsO,[<^,9?fF[<M>L\;JKP\P?^;OMR?qhL\;
?OA;b2;JU;TZ<GpR^[DCp@xquPKJ:@A9D;TGHM=DCE;Mh;F@,?	@AGEKFyKP+@A9<;f`mJ,G?Z<F<;O^CH;o ?CHIKJ,Gp@A9<M ZD;OA^J,GpL\;ZOAKhP?	Jff_
8fiKOV=\;;Zu[<=y@A9<;Y^KM=D[l@,?	@AGEKFoj;Y[<O,;&?F[<M>L\;JKPfiI;KMh;@VJ,GE^&dDCp@A;J,Oj@A9D?	@J,;ZD[<^;&@A9<;&F[DMfL\;J
KPfi=*?GpJ,OKPfi;F@AGp@AGH;O@AKL\;T?FD?CHq;Z\_
WKffo;b2;Jffs+Gp@Wo?O=\KOAO,GpLDCH;Y@AKPXKCHCHKo@ojKuJ,;OA;ff?	JU^,9@VJU?^,OYGHFB^KM>LDGHFD?	@AGHKF"_W GpJ,OA@s*o;^K[<CHZ
[<OA;R^KCHCHGHOAGEKFr^,9<;^,GHFDIMh;@A9<KZ<Of@A9D?	@n?CHCEKffo?^^;OAOn@AK@A9<;h=*?GHJ,OTGHF^KCECHGHOAGHKFrGEF?yCHKI2?	J,GH@A9<MhGH^
+ 

fiy

\DW$us<|t"]

s|Bv}



v99 v}v

@AGHMh; *?b2;JSAKF/x8K[<J,FD?OAOAK[DZ\s+*U_;^KF<Z\s o;u^K[DCHZ=DJ,;OA;JAb2;=*?	JA@nKP@A9<;uCE?F<Z<MR?	J,
IJU?	=D9tom9<;F@A9<;T;FbGpJ,KF<Mh;F@:GHO^U9D?F<IGHF<I ^=%;ff?Fyx ?KF\s+JI2U_
febd h

hjf#hjoe1i'



:8 9<;W`J,GE?Z<F<;O^CH;o?CHIKJUGp@A9<M GHOI;F<;JU?C\GHF@A9<;&OA;F<O,;@A9*?	@GH@M?qRL\;W[<OA;ZPXKJ:F[<MR;J,K[<O?F<Z
b2;JAqtZ<GHe;J,;F@W?	=D=DCHGH^ff?	@AGHKFDO:GHFyJ,KL\K@AGH^Off_  ?O,GH^ff?CHCpqs*@A9<;fMR?GEFt@A9<GHFDI@A9D?	@FD;;Z<O:@AKL\;T^,9D?FDI;Z
GHFy@A9D;f?CHIKJ,Gp@A9DM GHO@A9D;TZ<GHOV@,?FD^; u[<OA;ZtGHFt@A9<;T;b?CH[D?	@AGHKFP[<FD^@AGHKF<O:KPfi@A9<;Y@ojKuK=<@AGHMRGHff?	@AGHKF
=<J,KL*CH;MhO_
;b2;J?C<=DCE?FDF<;J,O 9D?b2;mL\;;FhGHM=DCE;Mh;F@A;ZGEF@A9<GHO o?qei?&dDF<;MRK@AGHKF=DCE?FDF<;J:& &;mCE? KO,?s
%?[DIGH;Jffs x B&?SV;J?sJ I2UsQ@xoKMhK@AGHKF=DCE?F<F<;JUOTPKJ9<KCHKFDKMhGH^R?F<ZF<KFwx9<KCEKF<KMhGH^hMRKLDGHCH;
J,KL\K@AOmxl^,9<;[<;JxlJU?GH^U9D?	J,Z\s*+*UsD?YJ,;KJ,GE;F@,?	@AGHKF=DCE?FDF<;JiPXKJj?F?	JA@AGH^[<CE?	@A;Z9D?F<Z"  [l=<@,?s
2Usj?=DCE?FDF<;JfPXKJIJ?OV=DGHF<I?F<ZJU;IJU?OV=DGHFDI`W9[D?^@AGEF\s  [l=<@,?ws x ?;Js:2Usj?F<Z?
=DCE?FDF<;JhPKJ?rJ,KL\K@AGH^?	J,M =DCE?^;Z GHF|@A9<;OV@A;ff?M I;FD;JU?	@AKJyKP&?F[D^CH;ff?	Jh=DCE?F@ =^ %+;ff?
F x
?KF\s:J I2U_`WZD?	=<@AGHFDIt@A9<;y?CHIKJ,GH@A9<M@AK?BF<;o ?	=<=DCHGH^ff?	@AGEKFGHOffsfi@A9D;J,;PKJU;si^CH;ff?	J,Cpq?b2;JAq
;ff?OVq@,?OV*_<KJmGHF<OA@,?F<^;s@A9D;&?	=D=DCHGH^ff?	@AGHKF@AK=*?	@A9u=DCE?FDF<GHF<ITPXKJ@A9<;&F<KFwx9DKCHKF<KMhGH^m@VJU?GHCH;Jo?O
Z<;b2;CHK=\;ZtGEF@A9lJ,;;YZD?ffqO_
8:9D;i`J,GE?Z<F<;O^CH;o|?CHIKJ,Gp@A9<MGHOfi?CHOAKmI;FD;JU?CGHFT@A9<;OA;F<OA;@A9D?	@ Gp@M?qTL\;Q[<O,;Z>PXKJ ?FqTGHF<Z
KP =*?	@A9=DCE?F<F<GEF<In=<J,KLDCH;M GHF?h^KF@AGHF[<K[<OOV=*?^;s]GHFydD;CHZDO:K@A9<;JW@A9D?FtJ,KL\K@AGH^Off_`WCp@A9<K[DI9tGp@
MR?ffqfL\;iOA[<R^GH;F@@AK&^,9*?F<I;j@A9<;Z<GHOV@,?F<^;P[DF<^@AGHK"F ls2KF<;MR?ffqn?CHOAKW^KF<OAGEZ<;Jfi^U9D?F<IGHFDIm@A9<;jPXKJ,M
KPQ@A9D;P[<FD^@AGHKF ls"KJf;b2;F@A9<;F*?	@A[lJ,;KPQ@A9<;O,;ff?	J,^,9<;ZrOA=*?^;O_<KJfGHF<OA@,?F<^;s\@A9<;^KF<^;=<@TKP
KLDOV@,?^CE;O4MR?qTL\;J,;^KF<OAGEZ<;J,;Z\_  F<OA@A;ff?ZnKPQA9D?	J,Z  KLDOV@,?^CH;Offs2KF<;^K[DCHZfJU;=DCE?^;@A9<;MLqfKFD;O
KPQ^KF<OA@VJU?GHF@AO_  F@A9D?	@Y^ff?OA;s\@A9<;>=*?	@A9B=DCE?F<F<GEF<I=<J,KLDCE;M Z<K;O&F<K@W^KFDOAGHOV@WKP4dDF<Z<GEF<IR?R=*?	@A9
omGp@A9DK[l@^KCHCEGHOAGHKF<OmLD[l@WJU?	@A9<;J&dDF<ZDGHF<IR?=*?	@A9BL\;OV@&O,?	@AGHOAPqGEF<Ih@A9<;Z<GHe;J,;F@&^KF<OV@VJU?GHF@AO_>[D^,9
?>=DCE?F<FD;Jj9D?OL\;;FZ<;b2;CHK=\;ZyPXKJ:?FD?b?C\?	=<=DCHGH^ff?	@AGEKFo9<;J,;@A9<;W=<J,KLDCH;Mko?O@AKdDF<Z?>=*?	@A9
PXKJ?L\K2?	@omGp@A9b?	J,GHK[<O^KFDOV@VJU?GHF@AO:KFt@A9D;&@VJU?SV;^@AKJ,q_8:9<GEOK=];FDOF[<MR;J,K[<O=\;J,OV=\;^@AGpb2;OKP
?	=<=DCEGH^ff?	@AGHKF<O PXKJ?	=<=DCpqGHF<I@A9<;`J,GE?Z<FD;O4^CH;o ?CHIKJ,GH@A9<MaGHF?&L<J,K2?ZD;J4dD;CHZ@A9D?F=D[<J,;jJ,KL\K@AGH^O_
? juihUotbdhpf

 Nf

8:9<;n?[l@A9<KJ,O&?	J,;>IJ,;ff?	@ACpqGHFDZ<;L<@A;Zy@AKWJff_f?M?C  [l=D@,?RPJ,KMlGHMhKF<JU?OA;JYFDGpb2;J,OAGp@qom9<K
^ff?	J,;PX[<CHCpqfJ,;ff?Z@A9<;=*?	=\;Ji?F<ZO,[<II;OV@A;Zb?CH[D?	LDCE;j^KJ,J,;^@AGHKF<O4@A9*?	@QIJ,;ff?	@ACpqGHM=<J,Kb2;@A9<;g[D?CHGp@q
KPfi@A9<;&dDFD?C]=e?	=];J_
8:9DGHOojKJ,9*?OL\;;FtMR?Z<;Y=\KOAO,GpLDCH;Lqew %;>;F@VJ,8; B&?	@AGHKFD?C+ZD;YCE"? m;^U9<;J,^,9D;n^GH;F@AGpdDg[<;
lJU?FD^;Us]KF<O,;SV
K B&?^GHKFD?C+Z<;fGH;F<^G?nqu8fi;^F<KCHKIGE?t ;lGH^K:?FDZtDff~<f <ers 'j7<D]U_
     D 0   
`W9[D?^@AGEF\s<_ps  [l=D@,?ss_psx

? ;JffsYm_&2U_ ?FDGp=D[<CE?	@AGEKF'CE?F<F<GHFDIPKJ;Z<[<F<Z*?F@
KL\K@AO ` 'QJ?^@AGH^ff?Cf`=<=<J,K2?^U9\_\l  *x*&2* 2mp*225422=6w::6ffAVs
9*(*Us*v *	5*_

`W9[D?^@AGEF\s{<_ps ?;Jffs4:_ps  ;O,OAG^; J,;s'fi_ps1x 8 ?CpLDG7s:_Q2U_tWOAGHFDI  ;F<;@AGH^`CHIKJUGp@A9<MhOYPXKJ
KL\K@ K@AGHKF'CE?F<F<GEF<I_  F92ff,oJ6s2@:<
8p92:rAJ2 ffA*s2W ff
 *xX(p*Us<=<="_ I+* I+*_
+@

fifi

$vu]}s11t$vWwvLswe1}%l1r

 ?	JAJU?g[D?F<Z\sLl_psx %fi?	@AKMfL\;sil_fiJ2U_Y` KF@A;h?	J,CHK`CHIKJUGp@A9<M PXKJ' ?	@A9$'CE?F<F<GHFDIomGp@A9
?F
q W;IJU;;OjKP]lJU;;Z<KMy_  F~92ffUUoXJ6.2@W<fifi  T  Dx*&2*2{2 V*U
2k
  25427 6^pl2qh&2<s*=<="_\p* Dp*p*_
 ;OAOA&G ; J,;8s 'fi_psT8 ?CpLDGs&:_psY`W9[*?^@AGHF\s l_p8s x ?;JffsTm_fJI2U_NWF`WCHIKJ,Gp@A9<MR;  ;) F ;) @AGHg[D;
'4?	JU?CE^C ; CHCH;W=\K[lJjC7 KW=<@AGHMhGEO,?	@AGHKF\
_ eA <( pDn* *ff*U   =2: q2b  p<s 72Us+ff2 DvJ_
 J,KKOs T_v2U_|lKCpbGEF<It@A9<; GHF<Zl(w ' ?	@A9 'iJ,KLDCH;M Lq  KK
Z ;=<J,;OA;F@,?	@AGHKF|KP:@A9D;u<J,;;
=*?^;_  T lV 62ff& 2 62
 ?6x7 q  ^ o54,Ueff= 6Us UsfiJ D+ *_
?F<Fq^s <_"2U_ \
 2Jqre% n^ D2@
25422  2& 2
 %<<X2_ t M
8 'iJ,;OAO_
Y?bGHZ<KJffs _fi2U_>`FD?CEKIK[<OTjJ,KOAOAKb2;Jff_  Y
F 9 2ff,o J 62@<  \X   DxUe^ 2* 2 0
ffAeU"
 2
 :]& & U 2X< 5q6s<=<="_D Dff v_
&;mCE8
? mKO,?s_ps %?[DIGH;Jffs&_ps xMB&?SA;JU?s <_DJ I2U_ KLD[<OV@ '4?	@A
9 'iCE?F<FDGHF<I&GHF@A9<L; 'iCE?F<;_  T
<V 6ff& 2 6"2
 25422= 6n^ pl 2qh& 2<{s mfi7v2Us\v5 * v2_
D?Cp2;F*?[<;JffsDm_ps x  K[leK[DG%es*]_"U_j`  ;F<;@AGH^f`CHIKJUGp@A9<M PXKJ 2KL9DK="_  
F 3 2U, X 6
2:< 
 fi  T  DxUe^ 2*\ 2 V*U 2 254@2= 6^ pl 2qh& 2<s2=D="_2	/ 2_
D?Cp2;F*?[<;Jffs\:_ps x &;CH^U9D?M>L<J,;s\`f_ 2U_Y`  ;F<;@AGE^>`WCHIKJ,Gp@A9DM PXKJ  GHF '4?^AGHF<I?F<y
Z %GHF<;
 ?CE?FD^GHF<I_  W
F 3 2ff,  6y2@t< 
 fim  T  Dx*& 2* 2 ffA* 2 2U427 6
^ pl 2qh& 2<sD=D="_]J ID2_
D?ffb2;JSVKF"s  _ps x8K[lJUFD?OAOAK[<Z"s '_+ *U_` %+K^ff?C  ?OA;Z|`m=D=<J,K2?^,9|PXKJ '4?	@A
9 'iCE?F<FDGHF<IKP
?F<Gp=*[<CE?	@AKJ,OomGp@A9u?fWGHID
9 B[DMfL\;JKP &;IJ,;;OKP"lJ,;;ZDKMy_  
F 3 2U, X 6A2@&<  fi	9
 T  DxU*2^ 2e 2 V*U"
 2k
  25427 6^ pl 2qh& 2<s*=<="_\ D_
<;JALe?^,9\fs 'fi_+J I2U_KF@VJ,GpLD[l@AGEKF
F ?h CE? 'CE?F<Gpd*^ff?	@AGHKFyZ<;T8JU?SA;^@AKGpJ,;O_ ?	=<=\KJA@Z<.; m;^U9<;J,^,9D;
 &+(w & s '4w  J I  2 Is &GpJU;^@AGHKFtZ<;O i) @A[<Z<;O:;@ m;^,9D;J,^,9<;Ow &j_
 [l=D@,?s h_]2U_ K@AGHK
F 'CE?F<F<GHFDIfPXKJ ;`w KWJ,GH;F@,?	@AGHKFyWOAGHF<In GHF<I;J8JU?^AGHF<I %fi?F<Z<MR?	JAO
GEF G  7v2 ff
 _  k
F 9 2ffUUo X 6A2&< l fifi    Dx*& 2*2 2 ffA* 2
 2U427 6
^ pl 2qh& 2<sD=D="_l+ I_
&?qo?	J,Z"s fi>_ps Y?F<;OA9<MR;F<Z\s %Q_ps x&?q?	@AGs]_m2U_`
F KWb2;JAbGH;o KP Ts` %  i`qOV@A;M
@A
K 'iJ,KIJU?M ?F<ZKF@VJ,KCTKK=];J?	@AGpb2; ?F<GH=D[<CE?	@AKJ,Off_  F 9 2ffUUo X 62@< 
 2mp< 
 *x*& 2* 2 ffA*
 2 mr	eUo  25427 6sD=<=+_*Jv I	+ _
WKCHCE?F<Z\is <_ + *U~
_ 2: re7^ 2|
 fb pVi6 W > ?6x= qA6_nWF<Gpb2;JUOAGp@xqBKP GH^,9DGHI2?F
'iJ,;OAO_
o?F<Is _ps x `W9[	SA?s B>_m2U_  J,KO,O K@AGHK/
F 'CE?F<F<GHFDI`[<JAb2;q*O
_   2qsr"pl
"p r= ?6Uis j7v2U_
f?ffbJU?	Gw
s %i_psjb2;OV@V?s 'fi_ps %?	@AKM>L];s l_ps x K&b2;J,MR?	J,Os _J I2U_ 'QJ,KL*?	L*GHCHGHOV@AGE
^ mK2?Z<M?	=DO

PXKJ '4?	@A
9 'iC?F<F<GHF<IfGHFGHI9l(w WGEMh;F<OAGHKF*?C]KF<dDI[lJU?	@AGHKFt=e?^;O_ T lV 62ff& 2 6A2
 25427 6^ pl 2qh& 2<{s Us" IJIJ _
%?ffo:J,;F<^;s >_iZ\_$U_+U
_ >^ 54@2	2D2:eff & 5 2U< 5q6U_ fii?F BWKOV@VJU?F<
Z ;GHF<9<KCEZ\_
+B

fiy

\DW$us<|t"]

s|Bv}



v99 v}v

%+GEF\s>_ps&GE?Ks2l_psx GH^U9D?CH;omGH^sfi_*U_ Qb2KCE[l@AGHKFD?	JAqB&?bGHI2?	@AKJ:PXKJ? KLDGECH; mKL\K@_  F
32 U,X62@Y<fi  T  Dx*&2*22 ffA*2254@276T^qpl2qh^2<s
=D="_*J _
%+Kff?FDK	w(';) J,;s]8Y_"+*U_j`aGHMh=DCH; K@AGHKFlw('iCE?F<FDGHF<I`CEIKJ,Gp@A9<M PXKJ  ;F<;JU?CmKL\K@
[DCE?	@AKJ,O_  T lV6 2ff&2 62  254@2=6n^
p2Jq2^2Dsfi7v2Us]	/v_

?F<Gp=lw

^=%+;ff?F\sj`f_psx ?KF"s8Y_JI2U_  F<^J,;MR;F@,?CK2?Z<MR?	=DO?F<Z  CHKLe?C'4?	@A9'CE?F<F<GHFDItGHF
ib2KCpbGHF<I  F<Z<[<OA@VJ,GE?CiFbGHJ,KF<Mh;F@AO_  F'32ff,6$2<fi    DxUe^2*
2 V*U2 25422=6n^pl2qh&2<sD=<="_\ffDffJI_
;qIJ,;@s`f_ps x %;bGHFD;s _&2U_4@VJU?^@AGHKF Z<
; 'iJ,GHMhGH@AGpb2;O  ;) KM;) @VJ,GHg[<;Om@AGHCEGHO,?	@AGHKF
Z"[<Fh`WCHIKJ,Gp@A9<MR;  ;) F ;) @AGHg[D;_ ?	=<=\KJA@Q`WF<F[<;Cs;F@A;JPXKJ  F@A;CECHGHI;F@ ?^U9<GHF<;Os ^  GHCHC
WF<Gpb2;JUOAGp@xqs KF@VJ ;ff) ?C_
KWb2;J,M?	J,Os _42U_T
` ?FDZ<KM`m=<=DJ,K2?^,9@AK K@AGHK$
F 'CE?F<F<GHFDI_:8;^U9<F<GH^ff?1C m;=\KJAs@ mw
mw2wv2s &;=*?	JA@AMR;F@WKP4KM=D[l@A;J^GE;F<^;s<m@VJU;^,9@WWF<Gpb2;J,O,Gp@xqe_
mKL\;JA@AO,KF\s  _+ *U_ '4?	JU?CECH;C  M=DCH;MR;F@,?	@AGHKFnKP  ;F<;@AGE^`WCHIKJ,Gp@A9DMhO+GHFn?WCE?OAOAGpd*;JfiqOV@A;My_
 F Y?ffbGHOffs %Q_+Z\_$Us :e& & 5 2< 5q6n^ Dk qqpl2xo W<]A$_ KJ,I2?y
F f?[<PXMR?F<F
'[lLDCHGEOA9<;J,O_
^U9<;[<;Jffsj`f_pws xlJ?GH^,9D?	JUZ\s8Y_+ *U_ KF@AGHF[<K[<Ow[lJAb?	@A[lJ,
; ' ?	@A/
9 'iCE?F<FDGHF<IPXKJ?	JV(w %Gp2;
fiQ;9DGH^CH;O_  F 9 2ffUUo X 6 2@<   T!9  DxUe^ 2* 2 V*U
 2  DxX(pD
 2542 6^ ?6x7 q6Us<=D="_D+ * Dff Jv_
8 ?CpLDGs :_v2U_ &X 2,& 2'
 ~
 9 2ffU7 6@6pU66+p%7 6qWV xAb pV7 6:X " p: 6# " %= $ q2'
 &fb 6:0

 4p ,$ w_ 'i9\_ n_l@A9<;O,GHOs  F<OV@AGp@A[l@ B&?	@AGHKFD?C 'fiKCHq@A;^U9<F<GHg[<;YZ<; JU;F<KLDCH;WwilJ?F<^;_
8 ?CpLDGsQm_ps x  ;OAO,^G ; J,;s 'fi_:J I2U_` '4?	JU?CHCH;C  ;FD;@AGH^y`CEIKJ,Gp@A9<M`=<=DCHGH;Z@AK@A9<; ?	=D=DGHF<I
'iJ,KLDCH;Mt_  F|`WOV@APX?CH*s  _:iZ"_$U
s r+r*$U& 2 6-2t
 mr	eUo YWV Xx,ffb pV 2qsr"plx@ 6_
 ` _
8 ?F<;OA;+s T_+ *U_ '4?	JU?CHCH;C  ;F<;@AGE^`WCHIKJ,Gp@A9DMPXKJ4?q=\;J,^[lL\;_  F 9 2ffUUo X 62< l e,@ 26 
 *x*& 2* 2 ffA*
 2
 :]
 Y U 2U< 5q6Us*=<="_\p *J* Dv_
&GE?K6s <_ps GE^,9D?CH;oGH^ms fi_ps x ]9D?F<Is %i_+J I2U_:Qb2KCE[l@AGHKFD?	JAD
q 'iC?F<F<;J  B&?bGHI2?	@AKJff KW=\;JU?	@AKJ
' ;J,PXKJ,MR?FD^;&?FDZyl;CHPHwx8fi[<F<GEF<I_  k
F 3 2ff,  6A2@&fi  T  *x*& 2* 2 V*U
2k
 r?2 pl^ 2*: 2qsr"pl7& 2<se=<="_DvJv I+v *_

+ 


fi
	ff
fi 
			 ! #"$ % 
'&)(+*, ((-/.0, 1(32', 44

56789  :+;<
(-!=?>7	%&:@, A/<
(-

BDCFEHGJILKMONQPRETSLEHU6U/NVUXWYI0NZSLE)I0K/M@N\[]NVNVG^NV_FK_a`cbdNZETSLegf

hjikml#nporq)sJtLtmu
O'r{?Zx'x/x
@?Y@rY
/ff

d Jff?0+ff?
+a?L+ff?)Yp

vxwwmy{zOvx|}m~g~}m

q+skmik$l#nx

vmm}zO|6/|6{~}m

O'r{?Zx'x/x

a$+ffFa/

+a??0@
!
/3HY!

)LY
?ff?$3) 3Y/?Y6/+6
!#6!3@?)6J3)6?


3F?3@ /!VV3YO @?L36@6+?a3/336/@
33?/x@36/d??Z?39d?Y!3J?6
ff@6H?r?
?{?^ 3!?3
6+3 !?


?6
Y3?)m3H !?

d3/!6
 #?#3@6J?d3
!3Z?r? 6!3#3Y !?

33?/
!T?{Y3?/?9 !?

6^3?YT!
Z 
3
36339!?66@3V m}ym 3a9
/6LZY/
{??ff
x33
6Z?
?63 63
mr63?0?F3/36?0?
6@#3?@!!Z3J363j?Y!@F
?V?
3 !?

O 
3^936O@
Z6?3!
6036gZ?
3x6?

 
6Or@d  aH66?


 r@
3aY $}y 6Z6336)63/3O !?

a/a@
 6?

36/3T
?O6/g3+33?Z63
 
6 $}y #636 !dF
3L/!!m 3!/
?r  3?6mF  

@3336 9! 
+63?!F3 3^63Y?T?3?33
60 3Y/?g63
 /!0?3T?3O $}y Y3
O?3Z36 !33?66{ ! /  3/!##VH!3
36/)H63
dr6H?36
















 	fiff V Y 
!"$#%'&()$*)+,-!*)-!.0/12&"3(4-!5768)fi*)9fi#-!&":3;&"$)+,#-<(!):357"&5=>";:3?(@ACB#:357&(!9D
:";:?3():3E.#-F&57-F4A7ACGH"IJ9fi#:3>.KL5C-F&M9&()#8*4N#%O";:3?(!57-)BP&():#)B(J&()LA=;:BQ+4.
.:;&"*HR8GS&()"fiACB#:35C&(49TM/'*)U;-!.V57-S+4;:3A=ACAW-!*X*!57"&":357R4)&"*S.#9fi+4)&57-!BL#;Y,:N+,#&"-[Z
&57A7A7GNA7;:B\57-!.:]5=-M+,:%^#:39-4.&"#V!?(_.#9fi+4)&".Z`57-F&"-!57U&I8TW1a-E:"+,#-!"Db-F492R,:
#%W+4;:3A7A7Ac;+4+!:#3(!d(!eU2R,-$*)UAC#+,*$&"#>5=9fi+!:#fUNU;:?5C#!\";:3?(gACB#:?5C&(!9\57-4A7!*!57-!B
*)+!&()Zih4:3"&N";:?3(kjml2!9;:Mnpod;#)Drqss;tvu?DR!:3-!?([Z`-!*[ZR,#!-!*X";:?3(wjm/dB:3exADyv-!;I[5C:39$D
n{zg(!:#&":3[D
qs||u?D4/2}jm~U&"&D4-!*!AC:D8zP(4-v&5iD!n{'
D
qss84zP(4;+4;&":3fin{'!&"&Dqssu?D
1aO/N}XjmzP(!-F&5nb-!57A7Dqss8]#exdACGD]):B!#-
D]nlV#:3%Dqss8]#exdA7GHnlN#:%Dqss[qeu?D
-!*SB9fifi&":fi";:?3(jm)A=*!9-!-
D
zPG83A75Cxd5C&"D
nzP#-!5C-
DqssFu?DbxA7AWb&"#$579fi+!:3#eU_&()
:3!-P&579fiE#%+c5Ch2;+!+A757;&5C#-!'!?(Xb&()Eh4%&"-S+4!ACM+4:#R4AC9jmlN!9;:Nnod;#)Dqss;tvu
-!*:#R,#&0;:39+4;&(+4A7-!-457-)Bkjr(4A7AC#
D'V57-!5iDdnlN!9>;:D2qssu?T1-w*!*!57&5C#-&"#zP1zS




r





`

= f

, ((-Y %% !:3H
: /
 L
!fi8Z
 Y7!	%&% 		&/%m% / :

fi

P#$}

wwyXm$

!* 57&":35CR4)&"*)Z`9fi9fi#:G>A7B#:35C&(!9D)+4;:3A7ACA
;:33(SACB#:35C&(49\(!UMR,-*)UAC#+,*$%^#:zP1zS
(!;:3*[Z`9fi9fi#:G<"G["&"9gjmlNA7gn[AC&"#:Dbqss;t[l2!9;:>nod;#)DVqss;tvu-!*81azP;:3?(!5Z
&".&):Sj#8#InG#-!Dbqss8b~U&"&>&LAiTCD'qss8'lN;:G8+457nl2!9;:D'qss8'zS(!-F&5dn
b-!5CA7Dqss8!]#exdA7Gfi&AiTCDcqssu?Tw(!57AC\.)57"&57-!B2;+!+!:#?()&"#2+4;:3A=ACA!";:3?(L(4Ub9-vG
.#-F&":35CR4)&57#-!N&"#X#;Y,:D.#9fi+4;:35=-)Bg&()0;+4+!:#3(!E-4**)&":?957-!57-!B&()QRc&M!"Q#%O?(
.#-F&":35CR4)&57#-fi57*!5KQ!AC&WR,!"\#%,&()d*!57U:3"O";:3?(LA7B#:35C&(!9D579fi+4AC9-v&;&5C#-+4A7;&"%^#:39D
-!*;+4+4A757;&5C#-4:3+c#:3&"*57-0&()2A=5C&":3;&):T
1a-:"+,#-!"S&"#&(!57L+!:#R4A79$DOxX(4U*)UAC#+,*&()J\
!{+4;:?A7ACA'";:33(w-!B57-)
&(!;&.#9MR457-)fi9>-vG<#%d&();+!+!:#?()&"#H+4;:?A7ACA()):357&57L;:33(
TO
!]jr#F#In
;:3-)A=AiD\qssu_57Eg+4;:?A7ACA1aO/2}g";:3?(;:33(!57&".&):Q&(!;&fi9fi:Bfi9E!AC&5C+4A7>;+!+!:#?()E&"#
&"I<*!57"&":357R4)&5C#-
D]AC#*R4A7-!57-!B)D-!*J&":3Q#:?*):357-)B)D-!*-<R,L:?!-J#-XzP1zS3(!;:*
9fi9fi#:3GS#:E*!57"&":?5CR4)&"*X9fi9fi#:3GP+4;:?A7ACAW+!:#[."#:eD]*457"&":35CR)&"*X-)&`x#:IX#%rx#:I["&;&5C#-4D
#:0J57-!BAC93(457-)gx5C&(@9E!AC&5C&():3*!57-)B)Tw357-)B#):Q.#A7AC.&5C#-#%N";:3?(A7B#:35C&(!9Dx
+,:%^#:39&(!#:&57Ab-!*k9fi+45C:35=Ad.#9fi+4;:?57"#-!Q-!*k#R4":UX&(!;&0+,:%^#:39-!.P&":-!*!Q*)#
.)57"&fifi";:3?(@"+4.$%^;&):;:3QU;;:35C*TJ#;+5C&A75C0#-&()"$&":3-!*!DO
!]!"fi
9?(!57-)2A7;:3-!57-)BG8"&"9p&"#>+!:3*!57.&\&()M#+!&579A&d#%W+4;:3A7A7A
";:33(S"&":3;&"B57d%^#:>B5CU+!:#RAC9$D[xd(!57?($;:N&()-g!"*$&"#>.#9+4AC&"N&()N;:33(g&"IT
 XV { LFm 
W 	fi 0
``4bW$ 0v
/-8!92R,:#%
:";:3?():3r(!U'.[+4AC#:*L9fi&()#[*!%^#:5=9fi+!:#fU857-!BV&()d.KL57-!.G#%";:3?(L457-)B
+4;:3A=ACA(!;:?*)x;:3TgH0xd57A7A%#[!E57-&(!5=2+4;+,:M#-+;:3A7ACA";:33(<&"?(!-!5768)M&(!;&fi-R,
;+!+4A=5C*&"#1aO/2}J";:33(T1aO/N}+,:%^#:390":357L#%E57-!.:9-v&A7A7GvZ`*)+,-!5=-)BJ*)+!&()Zih4:3"&
";:3?()M&():#)B(H&(!L";:3?(+4.TL1a-?(57&":3;&5C#-&(!:#)B(H&()L"+4.DW&()>*)+4&(H#%\&()
";:3?(57M.#-F&":#A7A7*JR8G-/2}$.#"&_&():()#A=*
T$1%OPB#Ar-)#[*)L57M-)#&E%#!-4*J*!):?57-)BPB5CU5C&":3;&57#-
D];:33(<R,B57-!M;&2&()Q:#8#&M-)#[*)xd57&(Jg.#&M&():3()#A7*"&2&"#S&()L9>57-!579E!9j^u
U;A7)r57-2&(!";:?3(fi"+4.r&(!;&].).*)*_&()r+!:U[5C#!&():(!#A7*
T1aO/2}d57-_*!9>575CRAC";:3?(
ACB#:357&(!9xd(!57?(0:68!5C:\-P9fi#!-F&\#%W99fi#:G0A757-);:O57-0&(!2*)+!&(0#%W&()N"#A=)&5C#-
T
1a-fi&(!57.&5C#-xOxd57A=Av:U[5Cx@.)57"&5=-)BV9fi&()#[*!%^#:+4;:3A7A7A75C57-)B'1aO/N}'";:33(T1-fi+;:&57[Z
A7;:D)xVxd57A=A.#-!357*):OAC&":3-4;&5CUV&"3(!-4576F!\%#:d&"IQ*457"&":35CR)&5C#-
DF%#:d*!G8-!9>57A7ACGfiR4A=-!57-)B
x#:ILR,&x-g+!:#[."#:3D-!*0%#:3(!-)B5=-)B&()NAC%^&aZ&"#;Z:35CB(F&d#:?*):\#%W&()N";:3?(g&":T
 qi)k!?u hji^ i^W itl
/";:3?(PA7B#:35C&(!9579fi+4AC9-v&"*$#-S+;:3A7ACAG8"&"9:68!5C:dR4A7-4.*g*!5CU[5757#-0#%x#:I
R,&`x-w.#-v&":357R4)&57-)BJ+!:#[."#:3L&"#:*!!.S57*!AC&5=9fiP-!*957-45795C$:*4!-!*!-F&>#:Qx&"*
.Y,#:&Tb-)X9fi&()#[*#%2*45CU85=*!57-)BH)+&()Sx#:Ik57-1aO/2}";:33(5=>xd57&(+4;:3A7ACAdxd57-!*!#ex
";:3?(jm!u?D)57-F&":#8*4!.*>R8G>]#exACGL-!*0lV#:3%jaqss[qeu?T'57-!BMcD[3(0+4:#8.3"#:\57B5CUX.#+FG<#%d&(!Q-F&5C:$";:?3(&":$-!*H!-4576F!L.#"&fi&():3()#A7*
THO(!Q+4:#8.3"#:3_";:3?(&()
9fiH&":3&"#*!5Y,:-F&0&():()#A=*!$579E!AC&-)#4ACGT1%E+!:#[."#:g.#9fi+AC&"-5C&":?;&5C#xd5C&(!#)&dhc-!*!57-)BQ$"#A7)&57#-
D
5C&V57bB5CU-H$-)x!-4576F!2&():3()#A7*jm*)+,:V&(!-H-FGP&():3()#A7*
G&L;:33()*cufi-!*RcB5=-!fiJ-)x";:33(k+4>xd5C&(&(!g-)xp&(!:()#A7*T@(!--@#+4&579A
"#A7!&5C#-57N*!5C:*
D+!:#[."#:32&(!;&Nhc-!*B#A-)#[*)>9E!"&N:9>57-J57*4AC!-F&57AA7A+!:3#8.#:3
xd5C&(_AC#fx:.#&&():()#A=*!(4Ud.#9fi+AC&"*_&()5C:)::3-v&5C&":3;&57#-
T]/&`G8+457A)*!57U85735C#-N#%x#:I
!5=-)Bfi057\57A7A=!"&":3;&"*$57-$W5CB!:fiq;T
3f

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8

INITIAL

GOAL
= expanded by processors 1, 2, and 3
= expanded by processors 2 and 3
= expanded by processor 3

W5CB!:q;b5CU[575C#-0#%]x#:3I05=-0+4;:3A7ACAcxd5=-!*)#ex;:33(
b-!*)U;-v&;Br#%)+4;:3A=ACAxd57-4*)#exJ;:33(E57&(4;&&():*!!-!*4-v&;:33(E57-!():3-v&57-21d/N}\57
-)#&+c:3%#:39*":357A=ACGT]b):?57-)BV3(0-)#-)Z`57-!5C&57A[5C&":?;&5C#-#%
1aO/2}D[A=A!#%,&()-!#8*).8+-!*)*
57-b&()+!:U[5C#!5C&":?;&5C#-N;:.[+4-!*!*N;B57-
TW357-)BO9E!AC&5C+4A7+!:3#8.#:3D&(!57,:*!4-!*!-F&
x#:I
57'+,:%^#:39fi*X.#-!)::3-v&ACGTM/".#-!*H*!U-F&;B#%+;:3A7ACAxd57-!*!#ex";:3?(H57'&()579fi+!:3#eU*
&579fiE57-0h-4*!57-)Bh4:3"&d#A7)&5C#-
Tr1%>";:?3(S"+4._()#A7*!O9>-vG$B#AW-)#[*)D!1aO/2}9G0h-4*g
*)+>#A7)&5C#-fi9E!3(>9fi#:d68!573I8ACGM&(!--fi#+4&579A)"#A7!&5C#-
TW];:?A7ACA[xd57-4*)#ex@";:?3(>-&;I
*)U;-v&;B>#%&(!5=&GF+,fi#%r";:3?("+4.T_:#[."#:?b&(!;&N;:";:3?(!57-)B$R,G#-!*S&()fi#+4&579A
&():3()#A7*9Ghc-!*k"#A=)&5C#-*!#exd-&()Sh4:3&>R!:3-43(&()G@.[+4AC#:D-!*w-k:3&):3-k&(!;&
"#A7!&5C#-PA7#-)B0Rc%^#:E#&():V+!:#[."#:?'h-!57(g&()5C:V5C&":3;&57#-
TNO(!57'9eGP:3!AC&b57-P3)+,:3A757-);:
"+,*!)+>Rc4"d&()'":357AACB#:35C&(49.#-!":3U;&5CUA7G>57-!.:9-v&&().#"&&():3()#A7*>-4*>*)#8
-)#&dA7#F#IQR,G#-!*0&()2):3:-v&\&(!:()#A7*T
b-&(!#&():>(!-4*
D+4;:?A7ACAxd57-!*!#ex";:3?(-%m.g*!A757-)$57-<.KL5C-4.G<x()-&()
-8!92R,:\#%]+!:#[."#:3d57\5CB-!5Ch-v&A7G>B:;&":&(!-$&()2-8!9MR,:\#%]5C&":?;&5C#-!\:68!5C:3*L&"#h-!*
-#+4&579Aj^#:'fih4:?"&?ur"#A7!&5C#-
D!!5=-)B>A7A
:3957-!57-!Bfi+!:#[."#:3d&"#L5C&O5=*!ACTd(!57O57&!;&5C#xd57A=A,#8):'xd()-P9-FG$+!:#[."#:3';:MeU5=A7;R4ACDcG&%^x5C&":3;&5C#-!;:2:36F!57:*R,!"M&()
()):?57"&57"&579;&"E57r%m5C:3ACGQ):3;&"T
/'-AC&":3-4;&5CU+4;:3A=ACAW";:33(;+!+!:#?(:A=5Cb#-*!57&":35CR4)&5=-)B&()&":3>9fi#-)B&(!fi+!:#;Z
."#:?'jmlN49;:nod;#)D,qss;t[!od;#)D[lN!9;:D[no9fi(
Dcqs|u?Tw5C&(&(!5=;+!+!:#?(
D8&()d:#8#&
-)#[*)#%r&()Q";:33(<"+4.Q57VB5CU-J&"#P&(!fih4:3"&2+!:#[."#:M-4*H#&():E+!:#[."#:3M;:L35CB-)*
)R4&":E#%O&(!;&E:#F#&fi-)#[*)Q_&()G:68)"&Ex#:3ITH/'E-AC&":?-!;&5CUD&()0*!57&":35CR4)&"*&":
";:3?(A7B#:35C&(!9jm''!ub9fi+AC#eG[NR!:3*)&([Zih4:3&V.8+4-45C#-!-F&57A&(!:L;:Q;&MAC&ME9-vG
.[+4-!*)*SAC;%-)#[*)'eU57A=;R4AC2+4:#8.3"#:3T':#[."#:3:.5CU_!-!5=6F)M-)#8*!d%^:#9&()2.8Z
+4-!*457-)BH+!:#[.Q-!*;:3g:"+,#-!35CR4AC0%^#:L&()P-v&57:P!R!&":g:3#F#&"*;&L&(!g:.5CU*-)#[*)T
#99E!-!5=;&5C#-[Z%^:dU:335C#-!#%&(!57*!57"&":?5CR4)&5C#-fi33()9fiV(!eUVA7"#2R,-Q:+,#:&"*XjmzP(4;+4Z
&":3_nb)&"&D,qss84od57-)%^A7*>n[3(!-!IDqssFu?T1a->A=A!#%&()&":'*!57"&":?5CR4)&5C#-fi;+4+!:#3(!D
&()O+4:#8.3"#:3+,:%^#:391aO/2}#-&(!5C:!-!5=6F)\!R!&":3579M4AC&-)#!A7GT/'A7A)+!:#[."#:?";:3?(
&"#N&()O39fi\&():(!#A7*
TW/d%^&":A7A[+!:3#8.#:3(!UOh-!57(!*EV357-)BAC\5C&":3;&57#-
D&()G_R,B57-_V-)x
";:3?(fi+4&():#)B(E&()O9O"&#%3)R!&":!357-)BVVA7;:B:&():3()#A7*
T]/k39fi+4AC\*!57&":35CR4)&57##%W&()N";:3?(g"+.257\()#fxd-57-$W5CB!:28T
3v

fi

P#$}

wwyXm$

INITIAL

Processor 1

Processor 2

Processor 3

= expanded on all three iterations
GOAL
= expanded on last two iterations
= expanded on last iteration

W5CB):328b5CU85=5C#-Q#%Wx#:I057-$*!5="&":35CR4!&"*>&":E";:3?(
b -!0*!U-F&;Bg#%&(!57fi*!57&":35CR4)&57#-J?()9fi57_&(!;&-!#S+4:#8.3"#:57_+,:%^#:3957-)BPx&"*
x#:IgR,G#-!*P&()_B#A*)+!&(
TV!"fi&(!EACB#:?5C&(!9";:3?()b&()_"+4.fi.#9fi+4AC&"A7G&"#$#-)
&():3()#A7*kRc%^#:H"&;:&5=-)B&(!X;:33(&"#@-)x&():3()#A7*
D-)#-)X#%2&()X+!:3#8.#:3057QU:
";:3?(!57-)BS;&EPACUARcG#-4*&()QACUA#%\&(!L#+!&5=9A"#A=)&5C#-
T01&M572+c#35CR4ACD]()#exU:D%^#:
b&"#<+,:%^#:39xr"&"*x#:I;&Q&()gB#A'*)+!&(Tw!#:Q.)9fi+4A7Dr5=-W5CB):SH+!:#[."#:$
";:3?()-)#[*)_;&fi&()0B#AOACUAr&(4;&fix#!A=*<-!#&ER,0";:3?()*@57-X":357Ar";:?3(@ACB#:?5C&(!9
9fi#fU857-!BAC%&aZ&"#;Z:?5CB(v&d&():#)B($&(!N&":T
/*457*)U;-v&;BQ#%\&(!57E;+!+!:3#3(<572&(!>%m.&E&(4;&M+!:3#8.#:3M;:3L#%^&"-57*!ACT#P-4):
#+!&579>A75C&`GD\+!:#[."#:&(!;&68!57I[ACGdh-!57(!#-!5C&":3;&5C#-29M!&
x5C&
%^#:A7A#&():+!:#[."#:?
&"#
h-!5=(_R,%#:"&;:&57-)BM&()-!.8&57&":3;&5C#-
TO(!5=57*!AC\&5=9fi-L9;I&(!"G["&"9U:G57-).KL57-v&
-!*P:*!!.M&()2+,:%^#:39-4.2#%&()E";:3?(S;+4+4A757;&5C#-TOO()2.KQ5C-!.G$#%&(!5=d;+!+!:3#3(XR,N579fi+!:3#eU*R8GQ+c:3%#:39>57-)BfiAC#*$R4A=-!57-)BfiR,&`x-S-)5CB(FRc#:?57-)BE+!:#[."#:3dx#:I[57-)Bfi#-g&()
9fiM5C&":3;&5C#-
T
O(!"$*).:35CR,*<;+!+!:#?()_#;Y,:!-!5768)>R,-).h&TJW;:3A7ACAxd57-4*)#ex";:3?(57E.Yc.&57U
xd()-P9-FG057&":3;&5C#-!d#%1O/2}fi;:2:68!5C:3*
D)xd()-g&()2&":E57d#L579MR4A7-!.*&(!;&bxd57A7A
:68!5C:_.).5CULAC#*HR4A7-457-)B)D#:Nx()-H*)+D]-)#-[Z#+!&5=9A"#A7)&57#-H57V.+!&;R4ACT$b&()$#&():L(!-!*
D*45CU85=*!57-)B&(!";:33(k"+4.9fi#-!BH+!:#[."#:3-@Rc$9fi#:3$.Yc.&5CUgxd()&()QR!:3-!?(!57-)BP%^.&"#:5=MU:3GJA7;:B$-!*&()$-F492R,:E#%O1d/N}g5C&":?;&5C#-!E57E:A7;&5CUACG39A7AiT
/.#9fi+4:#957"MR,&`x-P&(!"E;+4+!:#3(!'*45CU85=*)\&()_"&'#%+!:#[."#:3'57-F&"#S8`3Mjr#F#IcD
qssu?T~?(fiA7!"&":57B5CU-fi'!-!5768).#"&W&():()#A=*
D-4*M&(!r;:33(_"+4.\57W*!57U857*!*VR,&`x+!:#[."#:?rxd5C&(!57-L3(gA7!"&":D!\3()#exd-$5=-QW5CB!:N8T8&"&57-)B_&()N-8!9MR,:#%WA7!&":3&"##-)
579E!A7;&"$*457"&":35CR)&"*@&":;:33(
DV-!*&"&57-)B&()-8!92R,:0#%MA=!"&":30&"#&()-8!9MR,:Q#%
U;57A7;RACV+!:#[."#:3d579E!A7;&"r+4;:3A=ACA,xd57-!*!#ex";:3?(
T
3

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8

Cluster 2

Cluster 1
INITIAL

INITIAL

Processor 1
Processor 2
Processor 3

Processor 4
Processor 5
Processor 6

W5CB!:288+4.N;:33()*gRFG0&x#>A=!"&":3D)?(gx5C&(_+!:#[."#:?
 q  tmk!a kmkml i'lg
w()-PL+!:#RAC957dR!:#I-S57-v&"#0*!57ma#57-F&O)R4&"I8&()Mx#:I[AC#*Pxd57A7AA75CIACGQU;:3G9fi#-)BQ+!:#;Z
."#:?T!"d#-!d+!:#[."#:9Gfi:?!-fi#)&#%cx#:IfiR,%^#:O#&():3DFAC#*>R4A7-!57-!BN57!*fi&"#
.&5CU;;&"2&()M57*!ACb+!:#[."#:eTO()bh:3"&\+4(!N#%AC#*$R4A7-457-)B57-FU#ACUdAC.&57-)Bfi+4:#8.3"#:
%^:#9xd(4573(J&"#P:68)"&Mx#:3ITPb-!>.)9fi+4AC>5=N&()Q-);:"&_-)5CB(FR,#:2;+!+!:#?(jmzP(!;+;&":3
nb)&"&Ddqssu?OAC&":3-4;&5CU$;+!+!:#?()fi57-!A74*)>"AC.&5=-)BP:?-!*)#9+!:#[."#:3E#:fiA7AC#fxd57-)BX
9"&":+!:#[."#:O&"#LI+g&":33I$#%W+!:#[."#:dAC#*4O-!*"-4*$&()N1a#%()U[57ACG0AC#*)*+!:#;Z
."#:&"#V#-)\&(!;&57W57*!A7Tb):357-!B&()O.#-!*_+4(!"r#%,AC#*ER4A7-!5=-)B)D&()O*)#-!;&5=-)B+4:#8.3"#:
*)57*!x(!573(Sx#:IcD5C%-FGD&"#0B5CUTM/";:33(JACB#:35C&(!9&GF+45=A7ACGg"&"#:3V-)#8*!x(!573(X(!eU
-)#&VR,-X%m!A7ACG.[+4-!*!*X#--'+,-XA757"&TEw()-HB5CU[57-)B0x#:IP&"#P-)#&():N+!:3#8.#:D-)#[*)
-$R,'B5CU-$%^:#9&()N()*0#%]&()VA757"&Vjm*)+57-Q&()V&":fu?D4&()b&57Ajm-);:\&()V:#8#&?u?D)#:O%^:#9
9fi+A757-)BE#%A7A
ACUA72jml2!9;:no;#)Dqss;tvu?T
/-8!92R,:O#%;+4+!:#3(!d(!eUMR,-57-F&":#[*!!.*$%^#:d:3*!!57-)Bfi+4:#8.3"#:d57*!A7V&579fi2!357-)B
AC#*>R4A7-!57-!BN#+,:3;&5C#-
T'57-)BN&(!68!A75C&GE68!A75757-)BN"&":3;&"BGSjmzP(!;+;&":3Enb)&"&D
qssu?D
+!:#[."#:?fi-v&5757+4;&"57*!ACQ&579fi0R8G"-4*!57-)BS#)&Xx#:I:36F)&Ex()-&()5C:fiAC#*57fiA=9fi#"&
9fi+!&GD"#>&(!;&d&()G$-P.#-v&57-8)N+4:#8.357-)Bfi:9>57-!57-)Bfi-)#[*)\x(!57ACbxr5C&57-)B%^#:d>:"+,#-!"T
/'AC&":3-!;&5CU;+4+!:#3(!g;:J-!#&$:.5CU:gR"*
DbR4)&$A7AC#fx-#fU:3ACGFZ`AC#*)*+!:3#8.#:&"#
57-!57&57;&"dEAC#*>RA7-!.d#+,:3;&57#-Sjm!!:3!57?(!5iDF];I[5iD[n1a3(FG#(!5iD,qss;t[4odf"+4A,nl2!9;:Dcqssu
#:\A7AC#fxwA=A4+!:3#8.#:3&"#_+,:35C#[*!57A=ACGM(45C%&x#:3I&"#_I+$&()'eU:3;B2AC#*Lxd5C&(457->.+!&;RAC
R,#!-!*!Vjm/'-!*):3#-gnr()-D,qs|F[AC&"#:3Dqss;tvu?T
 q7)nnJfnffi'lg
:3#R4AC9"#A7)&5C#-4-0.)57"&\-FGFxd(!:'5=-L&(!b;:33($"+.T357-)B21aO/2}2";:3?(
D)&()V3(!5=A7*):;:S.8+-!*)*5=-<*)+!&([Zih4:?"&>9-!-!:>%^:#9A7%&L&"#:357B(v&D\R,#!-!*!*@57-*!+!&(@R8G&()X.#"&
&():3()#A7*
TM1%&()fi"#A7)&5C#-HA=5C'#-X&()fi:35CB(F&V57*)_#%&(!fi&":D0%m;:VB:;&":M-8!9MR,:'#%r-)#[*)
3f

fi

P#$}

wwyXm$

0 1 23

0 1

2 3

0 1

2 3

.
.
.

0

Original Ordering: 0123

1 2

0

3

1 2 3

0

New Ordering: 1320

1 2 3

 Most promising node
*
*
]57B):b)r'+,:3;&"#:d#:3*):357-!BE.)9fi+4A7
9E!"&bR,M.[+4-!*)*X&(4-S57%&()"#A7!&5C#-PA=5C'#-X&()fiAC%&V5=*)M#%&()_&":TE1%5=-)%#:?9;&5C#-XR,>%^#!-!*J&"#P:.Z#:?*):M&(!>#+,:3;&"#:3E57-&(!L&":Q%^:#9#-!L";:3?(5C&":3;&5C#-&"#P&(!L-).[&D&()
+,:%^#:39-!.V#%]1d/N}_-R,bB:;&ACG$579fi+!:3#eU*
T
]#exACGV-!*2lV#:3%8!BB"&&`x#b9fi&()#[*!
#%[#:3*):35=-)Br&()";:3?(2"+.'jaqss[qeu?TW5C:3"&D3(!5=A7*):#%W3(P-)#8*!N-R,V#:3*):*g-!*0.8+-!*)*$R8GQ5=-!.:57-!B()):357&57*457"&-!.V&"#>B#A-)#[*)T
/'AC&":3-!;&5CUA7GD&()>";:?3(ACB#:?5C&(!9-H.[+4-!*X&(!&":$%^xACUA72-!*H"#:&V&() ?4 .
j^&()Q"&E#%O-)#8*!2;&2&(4;&EACUA57-&(!>&":fuNRFGH57-4.:57-)	
B JU;A7)Tg8;:?3(<R,B57-!V?(
5C&":3;&57#-J%^:#9 &()>%^:#-F&5C:E&M-!*&(!572%:#-F&5C:_"&M5=M)+
*!;&"*J3(5C&":3;&57#-
T1a-R,#&(J#%
&()"L"DAC&()#)B(J&()>-)#[*)N9>G(!eU>&()>39fiLU;A7)D]-)#8*!Vxd5C&(39A7AC: U;A7)
B-):3A=ACGL:3ff 
4.&dfi9fi#:M):3;&"2&579;&"**!5="&-!.N-!*;:3N+!:%^::*
T
1a-!"&"*H#%r#:3*!:357-)B$57-!*45CU85=*!!A-!#8*)D]#8#IP&2ATPjaqssuV#:?*):N&()"&2#%#+,:3;&"#:?V&"#
B!57*!>&()0-).[&M1aO/2}$57&":3;&5C#-<&"#P&(!
 fi.  fi= F.TO()Q9fi#"&E+!:#95=57-)Bg-!#8*)Q57
&()Q-)#8*!>%^:#9&()0)&aZ#;Yk&QjmS3(!57A=*J-)#[*)>-)#&M.8+-!*)*<57-H&()Q+!:U[5C#!N57&":3;&5C#-uVxd5C&(
&()9>A7AC"&U;A7)T/'-.[9+4ACDW5CB!:0()#exH";:3?(&":.[+4-!*)*!357-)BX#-)
5C&":3;&57#-H#%1aO/2}>xd5C&(X#+,:3;&"#:M#:3*):35=-)BQt[Dq;D]8DW8TO()fi+4;&(H&"#g&()9fi#"&V+!:3#95757-!BQAC;%
-)#[*)Pjm57-4*!57;&"*x5C&(JS"&;:uN57>q0gSt[Tgd()L-)x#+,:3;&"#:_#:3*):357-!B57M.#9+4)&"*J457-)B
&()Q#:3*):_#%O#+,:3;&"#:3__&()G;+!+,;:E57-&(!57M+4;&(;%^&":E:39fi#eU[57-)BX*!)+4A=57;&"T'+,:3;&"#:3
-)#&\;+!+,;:357-!BE57-Q&()b+4;&($;:3b*4*)*Q&"#fi&()V-!*Q#%&()b#+,:3;&"#:dA=57"&D8:&57-!57-!BM&()57:#:?5CB57-!A
:A7;&57UV#:3*):357-!B)TWd(F!&()V#:?*):357-)BE#%]#+c:?;&"#:3\%^#:\&()V.[9+4ACV57-0W5CB):b3(4-)B\%^:#9
t[DOq;D8Dj^&":3G#+,:3;&"#:Etgh4:3&D]#+,:3;&"#:0qQ-).8&D#+,:3;&"#:fiP-).8&D-4*#+,:3;&"#:gA="&2%^#:
U:G$-)#[*)N57-0&()V&":3fur&"#gq;D8Dc8D4t[T
3e

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8
O(457r".&5C#-*)3.:35CR,rfi-F!9MR,:r#%]AC&":?-!;&5CUN;+!+!:3#3()\&"#>+4;:3A7ACAc";:3?(
TrV):r&()#;Z
:&57A9fi+457:357A-!A7G8"'57-g&()2%^#A7AC#fxd57-)B>.&5C#-!b*)9fi#-!&":3;&"2&(!;&b9-FG#%&()E:+,#:&"*
;+!+!:3#3()M#;Yc:_"#9fiLR,-).h4&257-.:&57-.#-!*!5C&5C#-4DR4!&2-)#P357-)BAC;+!+4:#3(<572A7xeG8M&()
9fi#"&d.Yc.&5CUE;&dA75=-)Bfi/d1\ACB#:357&(!9T
W	 k+0v    dH$  "!RX+{Lvm 

,4V]$0F
b-)09fi&()#[*#%*!&":3957-!5=-)B&().#9fi+;:3;&5CU0R,-).h4&E#%d+4;:?A7ACA";:33(;+!+!:#?()fi57ERFG
*)&":39>57-!57-)B<&()&(!#:&57ANR,#!-4*!Q#-+,#5CR4A7S"+,*!!+#R4&57-)*!357-)B3( ;+!+!:#?(
T
/".#-!*k9fi&()#[*57fi&"#J+c:3%#:39 9fi+45C:35=A\.#9fi+4;:357#-!fiR,&`x-@&()S;+!+!:#?()T1-@&(!57
".&5C#-0xbxd57A7A4*):3x#-Q&()#:3&57A
-!ACG["r-!*Q9fi+45C:?57A4.#9fi+;:357"#-!&"#fi*)&":39>57-)dx():
+,:%^#:39-!.L&":-!*!2.[57&M-!*J&"#S57A=A7!"&":3;&"L.#-!*!5C&5C#-4N!-!*):Mxd(!57?(JAC&":?-!;&5CUL;+4+!:#3(!
-+,:%^#:39R,"&T


 qi$#gnYtmnFi^km&%lgkm'i^
L
1a-X&()>A75C&":?;&):_x_hc-!*X&()#:&57A-!ACG["V%^#:N&()AC&":3-4;&5CU>;+!+!:3#3()V&"#g#-)"+,.&
#%O+4;:?A7ACA";:33(D-!9fiACGJ&"IJ*!57&":35CR4)&57#-
TglN!9>;:E-!*o;#jaqss;tvu_+!:#eU[57*)Q--!ACG[57
#%&()2+c*4)+#%*!57&":35CR4)&"*Q&":_";:3?(
D-4*g]#exdA7G-!*glV#:3%\jaqss[qeud+!:3#eU[57*)2-P-!ACG[57
#%b+4;:3A7A7Ax57-!*)#fx";:33(T1a-&(!57".&57#-xP!99;:?5C$&()"g-4ACG8fixd5C&(4-!5C%^G857-!B
:+!:3"-v&;&57#-
D-!*N9fi+5C:357A7A7G.#9fi+4;:3&()+c:3%#:39>-!.#%)&()&"3(!-!5=6F)]!5=-)B\&()*!:35CU*
68!;&5C#-!T
O(!"O-!ACG["3!9fi\&(!;&&(!OU:3;B'R!:3-!?(!57-)Bb%m.&"#: (:95=-!.#-!"&-F&&():#!B()#)&
&()'";:33(Q"+4.'-!*L&(!;&&()'AC"&aZ`.#"&B#Ac57AC#[;&"*Q;&2*!+!&*
( )4TH'A7#MAC+& (\:+!:"-F&
&()b()):35="&57\R!:?-!3(!5=-)B2%m.&"#:D[#:r&()b:3;&5C#E#%-)#[*)B-!:3;&"*$*!):35=-)BN#-)b5C&":3;&5C#-L#%1aO/2}
&"#@&()-8!9MR,:Q#%fi-)#[*)0B-):?;&"**!):?57-)BJ&(!+!:U[5C#!05C&":3;&57#-#%M1aO/2}T)#:357-!B@&()
()):?57"&57ER!:3-43(!57-!B$%^.&"#:_&"#gR,68!A&"#g&()>eU:3;B0R!:3-!?(!57-)BA=AC#exd2&()L-4ACG8357b&"#PR,
&()239fiNO%^#:d57-!.:39fi-v&ACZ`*)+,-!57-)Bfi*)+!&()Zih4:3"&\";:3?(
T
)#:&(!r*457"&":35CR)&"*V&":\";:3?(fi-!ACG[57D;x\!9fi&(!;&'R!:*)&()Zih4:3"&.8+4-45C#-E57W!"*
&"#_B-):3;&"V-)#!B(Q-!#8*)DFD[&"#fi*!57"&":?5CR4)&"O#-!b-!#8*)&"#_3($#-% , +4:#8.3"#:3T[57-4./
 .0(21
-!* 3 ,fiDxg-3!9fi$&(!;&&(!&":g57.8+4-4*)*&"#*)+4&5
( 4kxd(!:6
 4 387:9<;>=,fiT
H$xd57A=A49fiQS&579L#%b> ( 1@?  ,&"#X+,:%#:?9 &(!Q-)#[*)0*!57"&":?5CR4)&5C#-J-!*<&"#.#A7AC.&_&()
"#A7!&5C#-:4AC&%^:#9 ?(+!:#[."#:TO()P"+,*!)+#%N*!57&":35CR4)&"*&":3P";:3?(
Dd9fi3):*
V&()>:3!-S&579fifi#%&()":35=AACB#:35C&(!9*45CU85=*)*SRFGP&():3!-S&579fi#%&()fi+;:3A7ACAACB#:?5C&(!9$D
-PRcM.#9fi+4)&"*S():Ed&()E-F492R,:O#%":?57A-)#[*)dB-):3;&"*jm!9>57-)B>.#-4"&-v&V-)#[*)
.[+4-!5C#-L&579fifu*!5CU[57*)*>RFG>&()'-8!9MR,:#%:357Ac-)#8*!d.[+4-!5C#-4+,:%^#:39fi*LRFG>&()+;:3A7ACA
ACB#:357&(!9$TO(!5=5=O*):35CU*$5=-0&()2A75C&":?;&):jmlN!9;:'nod;#)Dqss;t[c;:3-)A=AiD
qssuO
,
A .B,DC (2( EE FGFG(( EffEffH H , FGFG(( EffEffHJHJIKI FMFBLNLNLNLNLOLOFGFG(( IKFG,S( R F > q(
jaqeu
1QP
1UT
/'S)H5=-!.:"DW&()QAC%^&9fi#"&2%^:3.&5C#-4A+4;:3&2#%\&(!5726F4;&5C#-;+4+!:#3(!QqQ-!*-R,
5CB-)#:3*
TOO()QQq V;> (W1L&":39.#-v&":357R4)&"'L95=-!579A
9fi#4-v&'&"#L&(!Nh-!AUA=)2-!*S-PA="#LR,
5CB-)#:3*
T1-0&(457O"D4+c*4)+;+!+!:#?()X,D)xd(!57?($:+!:-v&OA=57-);:O"+,*!)+T
W5CB):3bE()#fxdr&()b+,:%^#:39-!.'#%&()b*!57&":35CR4)&"*>&":N";:3?($ACB#:35C&(49R"*Q#-L&(!"
68!;&5C#-!r#-$E+,:%^.&ACGR4A7-4.*0&":V-!*0#-$_()U[57ACG>5=92R4A=-!.*Q&":V%#Y: ,Z.qt[D ([.{8D
3O\

fiP#$}



wwyXm$

8e+07
Perfect Balance
Imbalanced
Single processor

Nodes Generated

7e+07
6e+07
5e+07
4e+07
3e+07
2e+07
1e+07
0
0

0.2

0.4
0.6
Goal Position

0.8

1

W5CB):28b57&":35CR4)&"*0:E8;:?3(P#-F&":3"&5=-)B>:NrA7-!.
 -!*])^.qt[TN1a-g&()fi579MR4A7-4.*P"D
&(!_35CM#%&()_+!:#[."#:3N_,";:3?(H"+4.bU;;:35C'V.[+c#-!-v&57AW%m!-!.&57#-Xxd():fi&()_h:3"&V+!:#[."#:N5=N5CB-!*H9f"#:35C&`GS#%&()x#:IS-4*H&()
AC#**!.:"2N&()fi+!:#[."#:2-8!9MR,:N57-!.:"Tfi1-X&(!5=bB:3;+4(
D&(!B#A+,#5C&5C#-X:?-)B
%^:#9&()N%m;:A7%&O5=*)V#%W&()N&":Lj^+,#57&5C#
- .tvu\&"#&()N%m;:d:357B(v&O5=*)V#%W&()N&":Lj^+,#57&5C#^
- .
qeu?TO]:%^#:39-!.M#%&()M";:?3(SACB#:35C&(!9ACxrG[d+,;I[Oxd()-g&()NB#A]57O#-P&()N%m;:bAC%&57*)
#%r$+!:#[."#: _ b+,#:&57#-X#%&();:33("+4.T)#:M&()"#%r-579MR4A7-!.*X&":3D9E!3(H#%
&()_";:3?(H"+4._5735CB-)*P&"#Q05=-)BACN+!:3#8.#:Dx(!573(S57-!.:3"'&()M:3!AC&57-!B9fi#!-F&'#%
":35=A,.Yc#:&d:6F45C:*
T
H_-).[&.#-!5=*):d&()M&()#:&57A+,:%^#:39-!.M#%&()2+4;:?A7ACA
xd5=-!*)#ex ";:33(XACB#:?5C&(!9$T
oOA=AF&(4;&+4;:3A7ACAFxd57-4*)#ex";:3?(fi#+,:3;&"WRFGE*!57"&":?5CR4)&57-!B\&()rxd57-!*)#fx5CD#:.#"&W&():3([Z
#A7*!D&"#g?(eU57A=;R4ACfi+!:#[."#:M"#3(+!:#[."#:2+c:3%#:39>b#-)>5C&":3;&57#-H#%r1O/2}T[57-!.
&():3()#A7*!O;:3E-)#&.8+AC#:*g6F)-F&57A7A7GD4&()2h4:3"&d#A7)&5C#-g%#!-4*g9eG-)#&:3+!:"-F&-P#+)Z
&579A+4;&(
T#-4):S-k#+4&579A'"#A7)&5C#-DdA7A+!:#[."#:3Lxd5C&(A7#ex:$&():()#A=*9E!"&
.#9fi+4A7&"&()57:r!::-F&5C&":3;&57#-L#%1O/2}T1a-Q&()x#:3"&r"D)&(!5=-09;Ib&()b+,:%^#:39-!.
#%W+4;:3A7A7Ax57-!*)#fx";:33(g6F4A
&"#&(!;&O#%W":?57A,U:35C#-#%]1aO/2}T
1a-E&(!57W-!ACG[57&()\!9+!&5C#-E57W9*)&(!;&V[KL5C-F&W-F!9MR,:#%+!:#[."#:3].)57"&W!?(
&(!;&'&()EB#A]57&":3;&5C#-x57A7A"&;:3&xd57&()#)&*!A7GTV8+,*!)+#%+;:3A7ACA
x57-!*)#fx ";:3?(X-PR,
A7!A=;&"*L&()O:3;&57#2#%
&()'-F!9MR,:#%-)#-[ZB#A+4A74B#A57&":3;&5C#->-!#8*)&"#M&(!d-F492R,:#%
-)#[*)rB-):3;&"*PR8GL&()2+!:#[."#:\+,:%^#:3957-)B_&()VB#A5C&":3;&5C#-T#fxdACG$-!*lN#:%B-):?;&"
&(!57r:?;&5C#fi!57-)BE&()N-)#&57#-0#%&()NA7%&aZ&"#;Z:357B(v&\B#A+c#35C&5C#
- `4D!*).hc-)*$\&()V%^:3.&5C#-0#%]&()
&"#&A
-8!92R,:#%-)#[*)r57-L&()B#A
57&":3;&5C#-Q&(!;&\9M4"&R,'.8+-!*)*LRc%^#:':3(457-)BE&()'h4:3"&
B#A-!#8*)>jaqss[qeu?TO8+,*!)+0#%]+4;:3A7A7A,xd57-!*)#fx";:3?(g-$&(8!\R,V.[+!:"*g
( ENH ,a = H = ,Qb
A .
`d( E

I G
F `c( E a = H = ,Qb
q
.
e
q
F
=
`cj"(fkqeu T
a = ,Qb
H

jiu

b 57U-&(!5=%^#:39M4A7[DrxS-k9+45C:357A=ACGJ.#9fi+4;:3P&()P+,:%#:?9-!.#%N*!5="&":35CR4!&"*&":
";:3?(-!*@+4;:3A7ACAxd5=-!*)#ex;:33(%#:,g.qt[DY(h.ji8Dr-!*k)G.qt[Tk):3#9 &()$B:3;+(@57W5CB):YibxO-"d&(!;&+4;:3A7A7AFx57-!*)#fx;:33(>xd57A7AF#)&"+,:%^#:39*!57"&":?5CR4)&"*M&":O";:3?(#-!A7G
5C%]&()2B#A57OAC#[;&"*P#-&()N%m;:A7%&O#%&(!2";:3?(P"+.THEA7"##R":U2&(4;&d+,:%^#:39-!.
3<l

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8

% Performance Improvement

30

m

Parallel Window Search
Distributed Tree Search

25
20
15
10
5
0
0

0.2

0.4
0.6
Goal Position

0.8

1

W5CB):3i8b57"&":35CR)&"*0:_8;:33(gU8T];:?A7ACA,w57-!*)#fx{8;:3?(
#%*457"&":35CR)&"*&":$";:?3(+,;I[Mx()-)U:fi&()0B#A\-)#[*)057_AC#8;&"*#-&(!Q%m;:fiAC%^&E#%&()
)R"+4.N57B-)*0&"#L_+4;:3&57!A7;:\+!:3#8.#:T
[5=957A7;:N-!A7G8"E(!eUQR,-J+!:#eU[57*)*J&"#S.#9+4;:0-)#8*!#:3*):35=-)B&"3(4-!5768)M-4*J&"#X*).Z
&":395=-)M&(!E#+4&579A]-F!9MR,:b#%A7!&":3Ejr#F#IP&NAiTCDqss8]]#exACGSnlV#:%Dqss[q;;:?-)A7AiD
qssu?Td()"b-!ACG["\*)#_57-!*!57;&"'&":-!*457-Q&()'+,:%^#:39-!.'#%AC&":?-!;&5CUV"&":3;&"B5CrR"*
#-0%^;&):\#%&()b+!:#R4AC9"+.D!-!*$-$A7"#fiR,b!"*0&"#fi*)&":395=-)b&()b&()#:&57A
+,:%^#:"Z
9-!.\#%cb+4;:&574A7;:&"?(!-!5768)r%^#:bB5CU-_+!:#R4A79$T#exU:D8&()r&":39!"*_&"#V+4:*!57.&]&()
+,:%^#:39-!._57-X9-FGg#%&(!"fi-!ACG["V;:fi-)#&VACxrG[V9fi):3;RACE-4*H9-vGS!9+!&5C#-!
9*)N;:3N&"#F#L.#-!"&":357-457-)B_%#:d:AZx#:3A7*$+!:3#R4AC9T
 q onqpsr iffikm+%l#km'i^
L
/".#-4*S9&()#8*P#%*)&":?957-!57-!Bfi&()E.#9+4;:3;&5CU_R,-).h4&#%+4;:3A7A7A";:3?(H;+!+!:#?()b57
U[57V9fi+457:357A[-!ACG["TWH(4Ub579fi+4AC9-v&"*fiN-8!9MR,:#%,&()d;+!+!:3#3()&"#N+4;:3A=ACA)";:3?(
*).:?5CR,*2;:3A=5C:57-2&(457]+4;+,:W5=-M&(!OO
!]0"G8&"9$TWHO(!UOA7#b.#-4"&":3!.&"*-_;:&5Ch57A
";:3?(L+4.dB-):?;&"#:&"#M+!:#fU857*!ON&""&"R,*%^#:&()"d.[+,:3579fi-F&T8;:3?(L"+.d+4;:39&":3
-R,b"&;R4A=57()*0R8GL&(!2!":D!5=-!A7!*!5=-)B)
t &(!2.#"&O#%W&()V#+!&5=9A"#A7!&5C#-
D

t &(!2AC%^&aZ&"#;Z:35CB(F&O+,#5C&5C#-Q#%W&()NB#A-!#8*)N5=-0&()2"+4.D
t &(!NR!:3-!?(!57-)B_%m.&"#:D
t &(!N&":2579MR4A7-4.D

t &(!0#A7)&5C#-*)-!5C&Gkj^%^:3.&5C#-<#%-)#[*)E;&_#:ER,G#-!*<&()Q#+!&579>A"#A=)&5C#-<.#"&_&(!;&

:3+!:"-F&OB#A-!#8*)u?D!-!*
t &(!()):357&57::3#:gj^&()*!5Y,:-!.QR,&`x-&()$"&579>;&"*-!*&":3)$*!57&-!._&"#H&()
-!;:"&OB#A-)#8*!fu?T
3u

fiP#$}



wwyXm$

5
4
#Clusters

v

3
2
1
0
2

2.5

3
3.5
4
Branching Factor

4.5

5

W5CB):2F:3-43(!57-!B_4.&"#:b-4*P'+!&579>Ac'!9MR,:\#%\A7!"&":3
10
8
#Clusters

v

6
4
2
0
0

0.2

0.4
0.6
Imbalance

0.8

1

W5CB):2|8:N1a9MR4A7-!.N-!*S'+!&579A
'!9MR,:\#%rA=!"&":3
/'A7A4#%&()'.8+,:3579-v&*).:357Rc*Q():'x:b:3!-L#-Q-0-\r~119;B.Z+4357-)Bfi9M4AC&5C+!:#;Z
."#:'!57-!Bfifi+!:#[."#:3T
1a->#!:h:3"&.[+,:3579fi-F&xb.#-!57*):r()#fx&(!#+!&5=9A-8!9MR,:#%A=!"&":39G>Rc'Yc.&"*
R8Gg%^;&):V#%&()fi+!:#R4A79+4.fi57-!A7!*457-)BR!:?-!3(!5=-)BL%m.&"#:D&":579MR4A7-!.D-!*X"#A=)&5C#+,#5C&5C#-T]57B):FD,|8D4-!*Ps*)9fi#-!"&":?;&"N&(!;&d&(!N#+!&579A-F!9MR,:\#%A7!"&":3d57-!.:"
&()fiR!:?-!3(!5=-)B0%^.&"#:2*!.:"Lj^x5C&(H0R4A7-!.*&":D-H#+!&5=9A.#"&N#%bq i8D-!*X&(!fiB#A
#-P&()2%m;:d:357B(v&d357*)N#%&()M&":fu?D,d&()E579MR4A7-!.25=-!.:"_j^xd5C&(>R!:3-!?(!57-)B>%^.&"#:'#%8D
-H#+!&5=9AW.#"&N#%q8D]-!*X&()fiB#A5=-X&()fi957*!*!A7M#%&()fi&":fu?D#:MV&()_B#A-)#[*)fi9fi#fU
&"#>&(!N:35CB(F&d57*!V#%&()N&":0j^xd5C&(>R4A7-!.*$&":3DR4:3-!3(457-)Bfi%m.&"#:#%8D-4*g-#+4&579A
.#"&O#%\qu?T1-$-)#L"2*!57*0#-!N57-)BACV-8!9MRc:\#%A74"&":3OACxrG[\+,:%^#:39R,"&T
1a-Q&()V-).8&.[+c:?579fi-F&xb%#[!#-Q&()b.Yc.&\#%#+,:3;&"#:r#:3*):357-!B)TWW5CB!:Eqtfi*!9fi#-[Z
"&":3;&"O&(!;&\9fi+4AC#fG85=-)BM#+,:3;&"#:d#:3*):357-!BE!O-$57-!.:b5=-L&(!V#+!&579A,-8!92R,:r#%WA7!aZ
&":3D'-!*]57B):qt;R()#exL&(4;&Q#+,:3;&"#:0#:3*!:357-)Bwjm!57-)B+,:%.&Q#:3*):?57-)B5=-)%#:?9;&5C#-u
:4AC&_57-H9fi#:57B-!5h-F&E5=9fi+!:#fU9fi-v&fi#fU:L-)#H#:?*):357-)BXfi&()#A7)&5C#--)#[*)$57_+,#5Z
&5C#-)*%^;:&(!:&"#2&(!O:35CB(F&57-fi&()d&":T1-&(!57.[+,:3579fi-F&&()d";:?3(L&":;:3OR4A7-!.*xd5C&(
3<w

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8

10
8
#Clusters

v

6
4
2
0
0

0.2

0.4
0.6
0.8
Solution Position

1

1.8

20

Speedup

Change in Optimal #Clusters

]57B):2s8'#A]#5C&5C#--4*P'+!&579>Ac'!9MR,:O#%\A7!"&":3

10

1.6

1.4

1.2

1.0
0
0.0

0.8

0.4

0.0

Solution Position

0.4

0.8

Solution Position

a) Ordering effect on optimal #clusters

b) Solution position effect on speedup

]57B):qt[rb:3*):357-!BM.Y,.&
MR!:3-!?(!57-)BN%m.&"#:\#%_-!*Q->#+!&5=9A.#&#%q8TO()*45C+L57->&()'+4AC#&#[):3xd()-L&()B#A
57r+,#5C&57#-)*0#-$&()N%m;:dAC%^&\#%Wfi+4;:&574A7;:\+!:#[."#: _ d)R4"+.T
x	y ez|{~}]z

- 9J
O()E9fi+457:357A-4*g&()#:3&57A.#9fi+4;:35="#-!+!:"-F&"*X57-g&(!M+!:3U85C#4".&57#-S5=-!*!57;&"M&(!;&
AC&":3-4;&5CUN+4;:3A=ACA
";:3?(g"&":3;&"B57O+,:%^#:39xA7A
!-!*):O*45Yc:3-v&O.#-!*45C&5C#-!D!-4*0&(!;&d+,:"Z
%^#:39-!.r&":-!*4]*!#.)57"&]xd(!57?(_-MR,r!"*E&"#V)&"#9;&57A=ACGM"A7.&W&":3;&"B5C-!*M+4;:39&":
"&"&57-!B\%#:-)x+!:#R4A79Td#fxU:eD&()V:3!AC&r#%]&()"N"&4*!5C\;:N-)#&[KL5C-F&r%#:)&"#;Z
9;&57A=ACGQ*!&":3957-!5=-)BM&(!2;+!+!:#+4:357;&"N"&O#%"&":?;&"B5CTrV-)NA757957&;&5C#-$57r&(!;&'57-)%^#:39;&5C#!"*&"#B-):3;&"M&()N%^#:39M4A7O-!*$&"#Q.#-v&":3#A&()V.[+,:3579fi-F&D!!?(gdB#A+,#5C&5C#-D[5=O-)#&
I[-)#exd-g57-$*)U;-!.T/'-)#&():dA=5795C&;&5C#-$5=r&(!;&d"#9fi2#%]&(!2!9+!&5C#-!D)43(.#-!"&-F&
R!:3-43(!57-!B$%^.&"#:D;:Q-)#&2:3A757"&57_%^#:_9-vGH;+!+A757;&5C#-!TQ/'2g:!AC&DxQ-)*<9fi&()#[*


3<

fi

P#$}

wwyXm$

&"#)&"#9;&5=A7ACGLAC.&r#+!&579A,"&":3;&"B57\%#:\:3AZx#:?A7*Q+!:#R4AC9>B5CU-$5=-)%#:?9;&5C#-L&(!;&O57
U;57A7;RACT
1a-J:3"+,#-!">&"#X&(!5=2-)*
Dx0*!*P9?(!57-)QAC;:3-!5=-)Bg.#9fi+,#-)-F&_&"#P&()\
!"G[aZ
&"9$TO
!]9fi:B9>-vG#%'&();+!+!:#?()&"#J+4;:3A7ACAr";:?3(*457!*&(!$+!:U[5C#!
".&5C#-TW;:39fi&":3O-RcV"&\&(4;&O.#-v&":#A,&(!V&"IQ*457"&":35CR)&5C#->&":3;&"BGD4&()VAC#*0RA7-!Z
57-)BV"&":?;&"B5CDv-!*_&(!\#:3*):357-!B&"?(!-!5768)T]1-_+;:&57!A7;:eD&(!O"&":3;&"B5C&(4;&-fiR,"A7.&"*
57-!A=!*)
G  lN!9;:O-4*od;#)D4:*!&([Zih4:3"&
t b57&":35CR4)&57#-Q&":3;&"B
t '!9MR,:O#%]A=!"&":3q TNTNT +!:#[."#:?
B  b-
D'Y 
t #*R4A=-!57-)h
-  5CB(FRc#:eD!od-!*)#|9 
t :#[."#:AC.&5C#
t ]:3.-F&;BE#%W&I*!57&":35CR4)&"*Q)+,#-0:68)"& t  TNTNT qtt S
G  *'%m5="&D!5=A'%m57"&
t '#-!;&5=-)B>"&":3;&"B
:  t[TCT "&3I 57ff 
t /'-F&575C+4;&"#:G0AC#*$RA7-!57-)B_&":357BB$
B  W5[*
D!#8AiD!''1dY/ 
t b:3*):357-!6
O(!b4":\57rA7AC#ex*0&"#*)&":39>57-)'&()V&`G8+,r#%"&":3;&"B57\&"#fiR,'!*0%#:OEB5CU-0+!:3#R4AC9$T
#exU:D!R,!"V!?($_*)575C#-Q57r*!5KQ!AC&&"#9;IVx5C&()#)&5CB-!5hc-v&r57-)%^#:39;&5C#-Q;R,#)&
&()r";:3?(E+4.DF\
4QA7"#b(!&();+;R457A75C&Gb#%49;I[57-)BA=AF-).3;:G2"&":?;&"BGM"AC.&57#-!T
r"*#-&()3(!;:?.&":357"&572#%\$";:33("+4.D\
4)&"#9>;&57A7ACGX.#-[h4B):3N5C&"AC%&"#
#+!&579>5C+,:%^#:39-!.P#-+4;:&57!A=;:>;+!+4A75=;&5C#-
T[9fi+4A7$+!:#R4AC9>>;:P%*kL&":?57-!57-)B
.)9fi+4AC&"#MN93(457-)OAC;:3-457-)BV"G["&"9$Dvx(!573(fi5=-_&):3-fiAC;:3-4&()\#+!&579>A!"&":3;&"B5C&"#2;+!+4A7G
&"#+4;:&5=!A7;:$A7"0#%E";:3?("+4.T/+!+4ACG[57-)B<93(!5=-)HAC;:3-!57-!B&"##+!&5=95CX"#%&x;:3
;+!+4A=57;&5C#-!Q(!QR,-k+):3)*57-k#&():;:$#%M:";:3?(
T)#:g.[9fi+ACDzS57-F&"#-jaqs>s iu
(!0;+!+4A=5C*k5=957A7;:L&"3(!-!5=6F)X&"#)&"#9;&5=A7ACGkG8-F&()5CS+!:#RAC9_Z`"+,5hgU:35C#-4L#%
.#-!"&":?57-v&aZ`3;&57"%m.&5C#-ACB#:357&(!9Tod";:33(g57-0#&():d;:O#%].#9fi+)&":d5C-4.N(!\G[5CA7*)*
579>57A7;:b57*)b#%r!"&"#95C;RAC_-vU[5C:#-!9-v&N;+4+4A75C*P&"#g.#9fi+4!&":N-)&`x#:I[Ljmr(!;&"&3(4;:maD
rACU:3&Dr
n ,B):3[DbqssFV8&"-!I857&"DW57(!:D
n 
(!-)B)Dqssu-!*&"#J5=-v&":3.&57Ug(F49-[Z
.#9fi+4!&":Q57-F&":%m.Sjm!:3-)IcD'[)I;U[5C:35 [D\n)#A7GDMqss8V5CR,:39>-
DVqss|u?T{O(457fix#:I@57
!-!5=6F)V57-$A=AC#exd5=-)BfiR,#&($+!:#R4A79_Z`"+,5h'-!*;:3?(!5C&".&):3.Z`"+,5hV%^;&):d&"#L57d- 
c)-!.b&()
3(!#57.#%"&":3;&"B5CW-!*M57-2;+!+ACG85=-)Bd*!;+!&57U"#%^&`xr;:r&"3(!-4576F!&"#'+4;:3A7A7A";:3?(
T\
!]
A7"#S#;Y,:3fiS%:39x#:I&(!;&fi-<+c#&"-F&57A7A7G)&"#9;&"R,#&(<"&;&570-!**)G[-!957L"#%&x;:3
!"&"#9>5C;&5C#-
T
#+,:%^#:39+4;:3A7A7Ac;:33(
D
\
4X.[)&"&()V%#A=AC#exd5=-)Bfi"&"+4
q;Tdd57957-)B%:#9 9fi+4A7+!:#R4A7957-!"&-4.;:;+!&):*DfU;;:G[57-)BO?(E&":3;&"BGN+4;:39&":
5=-!*)+,-!*)-F&ACGT1a-L#:3*):&"#E68!5C:-L.Y,.&5CUb9fi+4A7"&D8+!:#R4A79;:b"AC.&"*L%^:#9
U;:357&`G0#%]*!#957-!\-!*+4;:3A7ACA
;:?3(!5C&".&!:T
8Td!#:'?(S+!:#R4AC9p57-!"&-!.D
\
4;+!&):%;&!:d#%&(!2+!:#RAC9p"+.2&(!;&b;:
I[-)#fxd-$&"#>Y,.&d&()2#+!&579A
?()#57.N#%"&":3;&"BGTO()"2%;&):3O57-!A74*)
\

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8
akmlO#i'lgegk!tm~(QQbO()NeU:3;BMR4:3-!3(457-)B_%^.&"#:'#%]&()2";:3?(&":T
n8# ffi i^ n tm]J9OcQVO()L*!5Y,:-!.D]#-eU:3;BDR,&`x-<&()>&579;&"*<*!57aZ
&-4.g&"#B#A-)#[*)-!*@&()g&":3!*!57"&-!.g&"#J&()gA7#""&B#A'-)#[*)TkO(457fi57
&579;&"*X%^:#9&()fi(!A7AC#fx";:33(XR8GP.#9+4)&57-)BL&()fi*!5Y,:-!.ER,&`x-H&()EaZ
&5=9;&"*k*457"&-!.&"#<&()PB#A;&Q&()gR,B57-4-!57-)BX#%b&()P;:33(w-!*&()P9>A7AC"&
&579;&"*$*!57&-!.b&"#fi&()bB#A,%#:OA7A,AC;%-)#8*!r#%&()V(!A=AC#exw";:3?(
D4957-8!&()
.&4A*!57&-!.V%:3#9&()N:#8#&O-)#[*)V&"#&()MAC;%]-!#8*)T
 p  kmkml ns^(QQVO()2*)B:3M&"#Lxd(!57?(-)#[*)d;:E!-)U-!A7G0*457"&":35CR)&"*$9fi#-)BQ)R)Z
&":3d57-0&()M";:33(P"+4.T
 tmkm  t
 k) itl7:9<QVO()QAC%^&aZ&"#;Z:35CB(F&E+,#57&5C#-J#%d&()>h:3"&E#+!&579A"#A7)&57#-J-)#[*)T
d(!5757"&579;&"*>%:#9&()d(4A7AC#exk";:33(LR8G_*!&":3957-!5=-)Bbxd(!57?()R!&":3d.#-v&5=-!
-!#8*)Oxd5C&(0&()NAC#fx&d"&579>;&"**!57"&-4.\&"#&()NB#AT
n8# ffi i^a
 kml ##i'l#e#k! t	WJ(. QbO()r:3;&5C#b#%,-)#8*!.[+4-!*!*_Rc&x-&()\):"Z
:3-v&d-4*$+!:U[5C#!r1aO/2}M5C&":3;&5C#-4T
!;&):M%:#9&(!-)#-[ZB#A5C&":?;&5C#-!V:+4:"-F&N;&"&":35CR4)&"b%^#:N&()fi&"&2"D-4*H&()
&":3;&"BG2&(!;&W:3!AC&5=-2&()R,"&+,:%^#:39-4.'jm()#:3&""&W:3!-N&5=9fifu
:3+!:"-F&]&().#:3:.&
A=5h;&57#-$#%&(!N+!:#R4A795=-!"&-!.N%^#:d_B5CU-P"&":3;&"BG?()#57.T
8Td:#R4AC9;&"&":35CR4)&";:.#9MR457-)*2xd5C&(V&(!.#::"+,#-!*457-)B\A7"-!*2;:3%*M&":?57-!57-)B
.)9fi+4A7M&"#X9?(!57-)0AC;:3-457-)BPG8"&"9TH$!"g)T j b!57-4A7-
DrqssuE&"#H57-!*!!.0
*!575C#-P&":_%:3#9p&()E+!:.Z`A=5h4*P"TN/:?!AC2R"E5=dB-):?;&"*X%#:b3(H.#-4.+!&
&"#QR,MAC;:?-)*
D.#:3:"+,#-!*!57-!Bfi&"#>3(X#%&(!M"&":3;&"BGS*)5757#-!OA757&"*g;R,#eUE&(!;&b-)*
&"#>RcN9>*)T
)Td#0"#ACUMQ-)x +!:#R4AC9Dc\
4J+c:3%#:39>d>3(!A7AC#fx ";:3?(S&():#)B(g&()M"+.E!-F&57A
:3#)B(!ACG;tt[D ttt-)#[*);:.[+4-!*!*
TW1%MB#A,57-)#&%^#!-!*>*4):357-)BV&(!'(4A7AC#ex";:3?(
D
&(!$%;&):3_#%&()0&":;:3A7!A7;&"*;&fi&(!5=M+,#57-F&fi-!*!"*&"#57-4*).;+!+!:3#+!:357;&"
:?!ACr%^:#9&()Er)T fi*!;&;R4"T
8Tdd()NAC;:3-)*:3!ACr:.#9>9fi-!*"&":?;&"BG3()#57.dB5CU-$&()2%;&):3\#%W&()2-)x+4:#R4AC9
+4.T\
!X&(!-57-!5C&5=;&"_+;:3A7ACA,";:3?($%^:#9&()b:#8#&\#%&(!N"+4.D!9fi+AC#eG[57-)B
&(!M"AC.&"*P"&":3;&"B5CTO)#:'9>-vG$;+!+4A=57;&5C#-!D)&(!257-!5C&5=A,.8+4-45C#-$&;I#-!ACG$fi%^x
.#-!*!d-4*$*)#Fd-)#&OB:;&ACG$Y,.&d&()V:?!-v&5=9fib#%]&(!2";:3?(gACB#:357&(!9$T
O(!E*)3.:35CR,*P&'#%%;&!:V-!*S&()_9fi#!-F&'#%&579fiE&"#"+,-!*P#-P&(!_5=-!5C&57AW\
!]
5C&":3;&57#-P;:E3()#-PR"*g#-P#):'.8+,:35=9fi-v&A*!;&L&"#LG[5CA7*$&(!E9fi#&()AC+4%^!A57-)%^#:39;&5C#57-0&()N3()#:&""&\&579T8;:3?(!57-)B_-)#)B(g5C&":3;&5C#-!r#%]&()N+4:#R4AC9"+4.M!-v&5=A,;tt[D tttQ-)#[*)
;:PB-):3;&"*&;I0AC&(4- qt<".#-!*!>#-&(!g+!:#RAC9 *)#95=-!xg&""&"*T{8+,-!*!57-!B
AC&579fi$&(!-&(457fi9GG[5CA7*<::#-!#!57-)%^#:39;&57#-Rc4"$%;&!:#%'&()&":P*)#-)#&
"&;R45=A75C$!-F&57Ar"U:3AOACUA=*)#ex-57-&()$&":Tk8;:?3(!57-!B*!*!5C&5C#-4A57&":3;&5C#-!fi57-B-!:3A
*)#8V-)#&N5CB-45h-F&ACGg579fi+4:#eUfi&()68!A757&`G#%r57-)%^#:39;&5C#-X-!*X&(!fi&579fifi:68!5C:9-v&'B:#fx
.[+c#-!-v&57A=ACGT@1a9fi+!:#fU*;+!+!:3#3()Q9G@5=-!A7!*)Q+,:%^#:3957-)BX&()P57-!5C&57A\;:33(k!-v&5=A\&()
"&;R45=A75C&`G$#%&()fi"+4.fi:?()NQ+!:3.:35CR,*gACUAD
#:V+c:?5C#8*457A7ACG0)+
*!;&57-)BL&()>)T Q3()#5=.
-!**;"!&57-)B_&()V+4;:3A=ACA
";:3?(g;+!+!:3#3()*)G8-4957A7ACGL*!):357-!BM.[)&5C#-
T
\

fi

P#$}

wwyXm$

'U[57;&5C#-g5C&(457-0:#R4A79 'U[57;&5C#-&x-X:3#R4AC9
#9>57- 8&;&
% ::#: 19MR R!%
% ::#: 19MR R!%
q+4)AC 8&*gU
t[T ttt q;Tfv| t[T tt)q 8T |s t[T tt 8T  t[T tt| )T ||s
/UFB>A7) q;T ;ts 8T ts t[T >| i i8T s| q;T ;ts 8T ts t[T |>i i8T s|
odzS
8&*gU
t[T t;t s8T [q t[T tt)q t[T tt t[T ;t v8T tt t[T tt t[T7q|
/UFB>A7) 8T iv qq;T > i t[T7q.v| q;T7qt 8Tiv qq;T >i t[T7q.v| q;T7qt
];R4ACq;:#R4AC9);&!:2A7!2'U857;&57#-!
] :%^#:3957-!B2->57-45C&57A[(!A7A7#exk;:33(Q(!A="#NR,->(!#exd-fi&"#MR,O.Yc.&57U'57-#&():+;:3A7ACA
";:3?(0:";:3?(
T)#:\.)9fi+4A7D!))&"&-):Vjaqssu+,:%#:?9+;:3A7ACAc";:3?(0#%&()'h4:3"&%x1aO/2}
5C&":3;&57#-!&`xd57.V*!!:357-)BN+;:3A7ACA";:33(g57->#:?*):r&"#_*!&":3957-)&()V-F492R,:#%57-!*45CU85=*!!A8&"I[
&(!;&_()#!A=*R,L*!5="&":35CR4!&"*X&"#P?(+!:#[."#:TH#F#IJ&EAiTjaqssuEA7"#P+c:3%#:39 -57-!5C&57A
(!A=AC#ex";:3?(57-<#:3*):_&"#H#R!&57-9fi#:30!:3;&"#+,:3;&"#:fi#:3*):?57-)BX57-)%^#:39;&5C#-&"#!"057:#:3*!:357-)BV&()'";:3?(L"+.T1-3(0"D[&()9#!-v&#%
&579fiO:36F!57:*fi&"#2+,:%^#:39&(!d.[&":3
57-!57&57A,";:33(P57\957-4579AcG&OB:;&ACG$579+!:#eUO&()N+,:%^#:39-!.b#%W&()V#eU:?A7A";:?3(&IT
O(!g"AC.&"*%;&):3>?(k*)9#-!"&":3;&"g5CB-!5Ch-v&5=d- 
)-!.0#-&()#+!&579>AO";:3?(
"&":3;&"BGTE/A7&()#)B(S%;&!:_UA7!'-H?(!-)Bfi*):39>;&57A7ACG%^:#9#-)E+!:#R4A79p&"#$&()fi-).[&D
3(E%^;&)::957-4%m5C:3ACGV"&;R4A7R,&`x-_ACUA7#%)&()39fi&":T/'WO:4AC&D.#9+4)&57-)B&()
U;A7)#%
&()"'%;&):3;&rMA7#exwACUA5=-&()&":+4:#eU[57*)NB#8#8*Q57-!*457;&5C#-#%
&(!"&":34.&):
#%W&()N-F&5C:N&":3TO(!h
 "U[57;&5C#-Pw5C&(!57-$:#R4AC9W_-v&":357d57-$];R4AC>q2()#ex &()2"&-4*!;:3*
*)U[57;&5C#-<#%O?(%;&!:0UA7!>#eU:LA7Ar";:33(&":30A7UA7THO()Q:!A7&M;:$eU:3;B*@#fU:
s|L+!:#R4A79p57-!&-!.b57-g&(!2h4%^&"-S+4)ACE*)#957- , -!*X;tQ+!:#R4AC957-!&-!.5=-g&()E:#R,#&
;:399fi#&5C#->+4A7-!-!5=-)BN*)#957-T]#&(Q#%
&()"&""&*!#957-!;:*)3.:35CR,*57-fi&(!-).[&.&5C#-
T
O()MAC#ex UA7!O57-$&()2.#A7!9-4O57-!*!5=;&"V&(!;&d&()2%;&):3d+!:3#eU[57*)NB#8#[*g57-!*457;&"#:\#%&()
"&":34.&):d#%
&(!d+!:#RAC9"+4.b;&rA7A4ACUA=57->&()d&":T1-L.#-v&":3&D8&(!
 "'U85=;&5C#-0&x:3#R4AC92&;R4AC'-v&":?5C3()#exw&()b"&-!*!;:?*L*)U[57;&5C#-L#%&()bU:?;BV%;&):3U;A7)_jmU:3;B*
#eU:LA7ArACUA7_57-3(&":fuM#fU:>A7A+!:#R4A79 "+.TJO()0A7;:B:fiU;A7)_57-&()"$.#A749-!
57-!*457;&"b&(!;&O&()N%^;&):-$.Y,.&5CUACG*!57&57-)B!573(R,&`x-g*!5CYc:-F&\+!:#RAC9"+4.T
O
!]S5=xd:35C&"&"-057-$-!*057r)::-F&ACG>579+4AC9fi-F&"*>#-$-0-\r~8D)#-05=-F[Lx#:IFZ
"&;&5C#-4L!357-)BX&()PW;:3A=ACAOb5C:&!AdzP?(!57-)jm'zHu>.#99M4-!57;&5C#-k"#%&x;:3DO#-'~r
/'AC+4(!P!57-)BP-<5=9fi+4AC9fi-F&;&5C#-#%d]#5J&(!:*!D#-<w57-!*)#fxdM':3!-!-457-)BgyeUS&():3*!
-!*$r57ACI_&():3*!D[-!*LE*!57"&":357R4)&"*>yvU;M"G["&"9!5=-)B2odzP1TFHV;:bA7"#E)::-F&ACG57-FUaZ
&5CB;&57-!Bb9-!]#%*)G[-!957A=ACGV"xd5C&?(!57-)BRc&x-fi&":3;&"B5C*!!:357-)BO.[)&5C#-fiW&(!+4:#R4AC9
"&":34.&):V#:O-FU857:#-!9fi-F&O3(!-!BT
OW2-<q<OQOkOWO+<2OQ[QOU&OKWON2UN<
2OWOO+W<"<2O

\e

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8
W	 z$\$vk  
Y}Xm;

1a-&(!572".&5C#-x>x57A7AW+!:-v&N.[+,:3579fi-F&A:3!AC&V&(4;&M.#9fi+4;:3>&()+,:%^#:39-4.#%r#):
*!;+!&57UE+;:3A7ACAW";:3?("&":3;&"BGSxd5C&(S?(Xh![*H"&":3;&"BGX!"*X.)A7!5CUA7G%#:VA7AW+4:#R4AC9
57-!&-!.T1a-$+4;:&574A7;:D[x2xd57A7AU:35C%^GL&(!N%#A=AC#exd5=-)Bfi(vG8+,#&()"O57-0&(457\".&5C#-

t O
!] _ b*!;+!&5CU_";:3?(X&"3(!-4576F!-XR,M4"*g&"#$?(!5CUfi"+,*!)+S#eU:NQU;;:35C&`G
#%;+!+A757;&5C#-!D[-4*$-g*)9fi#-4"&":3;&"2579+!:#eU*0:3!AC&\#fU:!357-)Bfi_h![*$"&":3;&"BG$%^#:
A=A
+!:#R4A795=-!"&-!.T
t d()E*4;+!&5CUfi";:3?(H&"3(!-4576F!E-X9fi+AC#eG&":?57-!57-)BL.[9+4AC'%:3#99M!A7&5C+4AC2;+4+4A757Z
&57#-!&"#J5=9fi+!:#fU#eU:3A=AO+,:%^#:39-!.TkHgxd57A=A*!9fi#-!"&":3;&"g&(!57!357-)BH&""&57-!BJ-!*
&":?57-!57-)B_.)9fi+4ACd.#92R45=-)*0%:3#9&`x#L;+!+A757;&5C#-$*)#9>57-!T
t d()AC;:3-!5=-)BH.#9fi+,#-)-v&fi#%2O
!]57;R4AC$&"#5CB-!5hc-v&ACGJ#)&"+,:%#:?9 -vG<#%b&()
&"&"*h![*<&":3;&"B5Cfi57-<&":39E#%+!:*!57.&5=-)Bg&()0R,"&_"&":3;&"BG<%#:SB5CU-+4:#R4AC9
5=-!"&-!.T
t 1a-X*!*!5C&57#-g&"#0.Y,.&5CUACGP9>;I857-!BL#-)fi"&":?;&"BGS?()#57._%#:V0-)x+!:#RAC9$D\
!]57
9#"&fi.Yc.&57Ug#%bA7A&""&"*;+!+4:#3()>;&9;I85=-)BHA7A\"&":3;&"BG*)5735C#-!E%#:LXB5CU+4:#R4AC957-4"&-!.T
t /U;:?5C&`GP#%AC;:3-457-)B>&"?(!-!5768)b-XR,E4"*g&"#357"&b57-S"&":3;&"BGX"AC.&5C#-H-!*S#;Y,:
+c*4)+>#fU:d":357Ac";:3?(
D[&()#)B(Q+,:%^#:39-!.xd57A7A4U;:G>%:#9#-)VAC;:3-457-)BN&"?(!-!5768)
&"#L-)#&():T
 iq )n8ht p


b-)#%!#):W&""&*)#9>57-!]57]&()xA7AZI[-)#fxd-Nh%&"-_+4!AC+!:3#R4AC9$T]O(!57+!:#R4A79.#-!5="&]#%
PdB:?57*N.#-F&57-!57-)B&57AC-8!9MR,:*N#-)&"#Oh4%^&"-
D-!*Md57-!BAC9fi+!&GV&57AC+,#57&5C#-NA7AC*2&()
 '&57ACT/ &57ACb-$R,'9fi#fU*g57-F&"#_&()bR4A7-)I>+c#35C&5C#-L%:#9-$*;".-F&O+,#5C&5C#-DF:3!AC&57-)B
57-X&()fi%^#):V#+,:3;&"#:32)+D*)#fxd-
DAC%^&D-!*X:357B(v&T>b5CU-&()57-!5C&5=A-!*XB#A.#-[h4B):3;&57#-!D
&()M+!:#R4AC957O&"#>h-!*P>"68)-!.E#%9fi#eU'&"#Q:3(P&()EB#AiTd/9+4ACNB#AW.#-[h4B!:3;&5C#57O3()#exd-g57-W5CB):qq;TrO()NzS-!(!;&"&-Sb57"&-!.M!!-!.&5C#-g+!:#fU857*!\-g*!95=5CR4A7'()!:357"&57
%^#:&(!5=+!:#RAC9$Dv-!*>xb!"d&()EqttM+!:#RAC957-!&-!.+4:#eU[57*)*5=->lV#:N% _ &""&r*!;&Ljaqss[qeu?T
I
b!:r.#-!*;+!+4A=57;&5C#-0*)#957-$5=&(!V:#R,#&r;:?99fi#&5C#-0+4A=-!-!57-)BM+!:#R4A79$T:3*45C&5C#-!A
9fi#&5C#-N+4A7-!-!5=-)B9&()#8*4;:U:GV.#"&A7G'x()-V;+!+4A757*b&"#d\:#R,#&;:39$DR,!"?(\"#57-F&(!
-g5=-[h-!5C&"b-8!92R,:O#%-)BA7O&"#xd(4573(57&O-g9fi#fUM%^:#9fiB57U-g.#-[h4B!:3;&5C#-
D4-4*$R,!"
.#A7A75=5C#-?()I[57-)B@9E!"&0R,S+c:3%#:39*%^#:03(;:39 B9fi-v&T:35CB<+!:#fU85=*)0<*!&57AC*
*).:?5C+!&5C#-#%d&()$A74A7;&5C#-!_-).3;:GJ&"#H*)&":39>57-)Q&()0+c#35C&5C#-#%d&()0-!*<.Y,.&"#:>57&()>;x#:I["+4.fiB5CU-H&(!)::-F&a#5=-v&N-)BA7>jaqs|su?T>)#:M#):V.8+,:35=9fi-v&Dx4"fi&()
+4;:39&":3$*).h-)*%^#:$&()H!9>> i;t:3#Rc#&$;:39x5C&(k35k*)B:#%2%^:*)#9 ()#fxd-w57W5CB):q8TO(!g5Cg-4*@A7G#!&>#%b&()g:3#F#957fi&()g39fi%^#:>3(#%b#):&"&>+!:#RAC9D
R4)&\xNU;;:GQ&()257-!57&57A,-!*$B#A;:?9p.#-[h4B!:3;&5C#-!\&"#>B-):3;&"E;t+!:#R4AC95=-!"&-!.TO()
kmi'l

2O"WO"W<YWO"WcOXU2OUW2<[O<<<
OWO<"<OQ<>-2N<SOQ&QWO"<2<"<2OO<O"O$2" KW>
2OX<U2OQQ2<"ff2ff OU<ff-<2OGOWO

\

fi

P#$}

wwyXm$

1

2

3

4

5

6

7

8

9 10

11

12 13 14

15

W5CB):3qq;W5C%&"-g+4)ACV+4:#R4AC957-!&-!.

W5CB!:q8oO#R,#&d;:?99fi#&5C#-$+A7-!-!57-!BM+!:3#R4AC957-!"&-4.
:#R,#&;:39+4;&(fi+4A7-4-!57-)B'+!:#R4A7957W+4;:&574A7;:3ACG2*45KL!AC&WR,!"O.#-457*):35=-)BbU:G_+c#35CR4AC
;:399fi#eU9-v&V:3!AC&d5=-PQ";:3?(S+4.Mx5C&(g-X57-[hc-!5C&"VR!:3-43(!57-!B>%m.&"#:TH_-!.#[*)E#-)
+,#5CRAC9fi#fUN5Cb%^#:\3(E"#57-F&D8:3!AC&57-!BE57-Q_R!:3-43(!57-!B2%m.&"#:O#U% i8Td()b:"#A7)&57#-L#%&()
9fi#fU'-gR,2*)&":395=-)*$R8GQ&(!M!":eD4-!*%^#:d&(!"2.[+,:3579fi-F&\xM3(!#F#"E:#A7)&5C#-$#%
qN*)B:3T
b!:fi&(!5C:3*<&""&>*!#957-!"&();:&5h5=A;:33(k"+4.g57-<xd(4573(+4;:?9fi&":357-4A7!*!57-!B
R!:3-43(!57-!B\%^.&"#:D&":3579MR4A7-!.D;"#A7!&5C#-N.#"&D()):?57"&57W::3#:D-4*2AC%^&aZ&"#;Z:35CB(F&B#AF+,#5C&5C#-$R,b"+,5h4*QRFGL&()N!":eT]H2B-):3;&"M;t_+!:3#R4AC957-!"&-!.r%^#:O!"V57-Q&()V.[+,:3579fi-F&T
)#:#):W%#):3&(M&"&*)#957-
Dx\57-F&"B:3;&"\#):#ex->ZR4"*EU:357#-_#%4&()d['H-)#-4A757-);:
+4A7-4-):Njm;:3:&"&'n{HA7*Dqssu\57-F&"#L\
!T#.#-)%^#:39x5C&(0&()M\
!H;:?3(!5C&".&!:D
&()$57-F&"B:3;&"*+4A7-4-):E!&57A75CM1d/N}g";:?3(57-!&"*#%&()$"&aZ`W5C:3"&fi";:3?(9fi&()#[*9_Z
+4AC#fG*XRFGH[']T
~3(+4A7-S:+45C:V"&"+jhcA7A757-)BL-X#+c-H.#-4*!5C&5C#-S#:N(!-!*4A757-)B>0&(!:;&?u'57
&":;&"*Wb"+4;:?;&"O-)#8*!57-M&()\";:3?(fi"+4.r&"#bR,.[+4AC#:*TH\.#9fi+4!&"&(!r.#&#%4'+4A7"#A7!&5C#-Xb&()fi-F!9MR,:b#%#+,:3;&5C#-!b-!*H.#-4"&":357-F&V57-S&()_+4A7-
D
-!*S&()fi*!5="&-!._&"#$&()
B#A]57\"&579>;&"*g!5=-)B_&()2-8!92R,:O#%W:9>57-!57-)$B 
exdTHM"A7.&;t+!:3#R4AC957-4"&-!.\%^#:
#):].8+,:35=9fi-v&%:3#9&()R4AC#[I[aZx#:?A7*
D;#fx:3]#%!'-)#5iD-!*M9fi#-)IGFZ`-!*[ZR4-!-4+A7-!-!57-!B
*)#95=-!T
#M.:;&"'&""&"D8x:3!-fi?(L+4:#R4AC957-4"&-!.d9E!AC&5C+ACr&579fiDF#-!.d%^#:?(L+;:3A7ACA
";:3?(g"&":?;&"BG57-$57"#A7;&57#-
TO()N";:3?(g"&":?;&"BG$&(!;&O+!:#[*!!.r&(!NR,"&O"+,*!)+$57O.#-457*[Z
:*&"#PR,L&()e
 ".#:3:.& XA75hc;&5C#-#%O&(!Q.#::3"+,#-!*!57-)B;:33(<&":Q%#:)T g&"#HAC;:3-
T
"&"r;::3!-fi#
- iE+!:#[."#:3#%->-r'r~N-!*>#-Q|2*!57"&":357R4)&"*Ex#:3I8"&;&57#-!457-)B
\

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8
/d+!+4:#3(
q;!AC W57AZ3q; odzS W57AZ`ozP
lN!9;:O-!*god;# 8T t
i8T |s i8T7qt
FT i
:3;&([Zih4:3"&
i8T | s8T
8T s;t
 Lq4
)T 
8T [q
Lqi J{qi  $qJ
#92R57-)*[Z)T 
i[q;T ;t
];R4AC28b57&":35CR4)&"*$:E[;:33(P[+c*4)+$oO!A7&
bzT1a-E&()h4:3"&W"&W#%4.[+,:3579fi-F&xh!MA7A8"&":3;&"BG_3(!#57.WR4)&]#-)\-!*EU;:3GN&()\"&":3;&"BG
3(!#57.2!-!*!:O.#-!57*):?;&5C#-
TO()N*!%^!A7&+;:39fi&":?()#57.;:
G .b57"&":357R4)&"*0:3E8;:3?(
t b57&":35CR4)&57#-Q&":3;&"B
t '!9MR,:O#%]A=!"&":3.q
B .bt #*R4A=-!57-)*
- .5CB(FR,#:
t :#[."#:AC.&5C#
t ]:3.-F&;BE#%W&I*!57&":35CR4)&"*Q)+,#-0:68)"&.;t 
G .57A^'%^57"&
t '#-!;&5=-)B>"&":3;&"B
t /'-F&575C+4;&"#:G0AC#*$RA7-!57-)B_&":357BB[: .t
B .]5C8*
t b:3*):357-!
HH.#9fi+4;:3P&()P:!A7&>#%M)T fZ`AC.&"*"&":3;&"B5C0&"#<3(&":3;&"BGk!*@.)A7!5CUA7G
%^#:A7A'+!:#R4AC957-!&-!.T8+,*!!+k:3!AC&Q%#:$U;;:35C#4Q&":3;&"BG*!575C#-!QU:?;B*#fU:
A7A\+4:#R4AC957-4"&-!.>;:3P()#fxd-@57-@&()%^#A7AC#ex57-)B".&5C#-4Td()$R,"&>U:?;BX+c*4)+@57
(!5CB(4A75CB(F&"*@57-@?(w"TO()H)T J:!AC&>;:S;+4&):*@R8G+,:%^#:3957-)BJ&"-[Z%^#A7*k.:3#
U;A757*!;&5C#-#-@&()$h4%^&"-+4)A7*!;&-!*@&():.Z%^#A7*@.:#UA75=*!;&5C#-#-&()g:#R,#&;:39
9fi#&5C#-J+4A7-!-457-)B)D
;:&5h5=AiD-!*-!#-!A757-);:V+A7-!-!57-!BQ*!;&P"&T$8+,5hA7ACGD*)5757#-X&":
57V.:;&"*%:#9&(!fi&":357-!57-!BQ"M-!*H!"*X&"#AC.&V&()"&":3;&"BGX%^#:V&()fi&""&2T>)T 
"+,*!)+057rU:?;B*g#fU:dA7Ac&""&O"\%^#:\#-)N5C&":?;&5C#-Q#%&(!5=+4:#8.3D)-!*Q&()bh-!AcU;A7)
;:2eU:3;B*P#eU:b.:#aZU;A757*!;&57#-$5C&":3;&5C#-!T
^ ^]itl ^nFee
"&:!A7&];:3#R4&57-)*_%^#:&()r&`x#NAC&":3-!;&5CUO*457"&":35CR)&5C#-N;+!+4:#3()TO()h4%&"-_+)ACD
&():3#Rc#&M;:399fi#&57#-+4A7-4-!57-)BQ+!:#R4A79$D-!*&()>;:&5hc57A";:3?(+4.>":UQ2+4:#R4AC9
*)#95=-!T~[+,:3579fi-F&A):4AC&;:3'(!#exd-Q57->];R4ACb8T)#:r3($*)#9>57-
Dv&(!'.#-F&":#AcU;:?57;R4AC
57\57-4*!57;&"*$!-!*!:\&()^
 "/+!+!:#?( .#A7!9>-
T
O(!MR!:3*)&([Zih4:3&'*!5="&":35CR4!&5C#-$+,:%^#:39bA75CB(F&ACG$R,&"&":V&(!-)T Q%^#:V&()Eh4%^&"-X+4)AC
*!;&N&F()#fxU:D8&()'r)T ':.#99fi-!*4;&5C#-!#)&"+,:%^#:39&()\R4:*)&([Zih4:?"&;+!+!:#?(%^#:&()
:#R,#&29fi#&57#-J+!:3#R4AC9*)#9>57-
T0O():#fxA=;RcA7
* fi  :c >ffJ:@5=V&()>:3!AC&N#%O9:B57-)B
 q

hji ffi

\Q\

fi

P#$}

wwyXm$

[&;&
)9A7A zg*457!9 ;:B
/U8BQ#8%;: q;T 
|8T |
s8T s
/U8BQ8+,*!)+ FT 
8T t )T ts
];R4AC28/U:3;Bfi8+,*!)+P8&-!*4;:3*$'U857;&57#&()'h4%^&"-Q+4)AC:!AC&xd57&(>&()':#R,#&r9fi#&5C#-Q+4A7-!-457-)BN:3!AC&-!*Q:3!-!-!5=-)BV&()V.#92R57-)*
*!;&&V&():#)B(r)T 8TO()fi+,:%^#:39-!.fi57bR,&"&":N&(!-%#:N&(!_h4%&"-+4)AC_R4)&N3A75CB(F&ACG
x#:3"N&(!-%#:O&(!N:#R,#&O9fi#&5C#-$+A7-!-!57-!BE*)#9>57-
T
#&"$&(!;&fi!57-!Bg&()QhAC&":3*kqP+4)A7Q*!;&[Dr\
!]w3(!57UP+c*4)+#%V;s8T7qeSAZ
&()#)B(&()g-8!9MRc:>#%V+!:#[."#:34"*57#-!ACG
G i)Twd()"+4;:?A7ACAO";:?3(ACB#:35C&(!9>+!:#[*!!.N3)+,:3A757-);:r"+,*!)+Jjm+c*4)+$B:;&":'&(!-$&()2-8!9MR,:O#%W+!:#[."#:3?uR,!N&()
+4;:3A=ACA,ACB#:35C&(!9>O*)#-)#&O.#9fi+AC&"ACG05795C&;&"N&(!N":357A
ACB#:?5C&(!9$T)#:.[9+4ACD!!5=-)Bfi*!57aZ
&":35CR)&"*0&":M;:33(P57-!*!5CU[57*!4A!R!&":O;:M5CB-)*&"#L"+4;:?;&"2+!:#[."#:?T1%]B#A-)#[*)
57MAC#8;&"*#-&()>%m;:EA7%&E57*)#%\&(!>:35CB(F&9fi#"&E)R!&":3>57-&(!L";:3?("+4.DW&()>+4:#8.3"#:
";:3?(!57-)BP&(!57M)R!&":Lxd57A7A6F!5=I[ACGSh-!*H&()QB#Ar-)#[*)D&(8!M&":3957-4;&57-)Bg";:?3(;%&":_#-!A7G
L%x-)#[*)2.[+4-!57#-!TO1a-P.#-F&":3"&D
&(!E":?57AACB#:?5C&(!9p5=-PQAC%^&aZ&"#;Z:35CB(F&'";:?3(Sxd57A7A.#9_Z
+4AC&"A7GL";:3?(A7Ac#&():d3)R!&":&"#&()V.#"&d&():()#A=*LR,%^#:V";:3?(!57-)Bfi-!*Qh-4*!57-)BM&()bB#A
-)#[*)57-X&()>:35CB(F&9fi#"&2!R!&":TO(8!V&()L":357AACB#:?5C&(!9xd5=A7A]+c:3%#:39*457"+!:#+,#:&57#-!;&"ACG
9fi#:fi";:?3(H&(!-XA7A]+!:3#8.#:3V.#92R57-)*X!57-!B>&()fi+4;:?A7ACA]ACB#:35C&(!9TN~3(&`G8+c_#%+4;:"Z
A7ACA;:33(;+!+!:#?(J*).:?5CR,*H57-X&(!57V+;+c:N-JG857A7*H)+,:3A75=-);:'+c*4)+!-!*):N.:3&57.#-!*!57&5C#-!TL8#9>ACB#:35C&(49N9fi#:LAC#"ACGX5795C&;&"L":357A";:?3(
DWR4)&N;&M$+,#&"-v&57AAC#V#%
#eU:?A7A+,:%^#:39-!.jmlNA7Mn[AC&"#:3Dqss;tvu?T
O
!] _ b"AC.&5C#-S#%"&":?;&"B5CV57-P&()2h%&"-X+4!ACM*)#9>57-P*!#Fb-)#&b+,:%^#:39p.#-!357aZ
&"-F&ACGVR,&"&":]&(4-M!357-)BO"#9fi#%)&()"&":3;&"B5C57-257"#A7;&5C#-TWb-):"#-M%#:]&(!57*!57;+4+c#5=-v&57-!B
+,:%^#:39-!.b57&()V-!;&):'#%&(!'&":?57-!57-)BE*!;&[T/'AC&()#)B(QxV!"'&()V"&":3;&"BGQ&(!;&O3(!57U
&()LRc&N:3!-H&5=9fi>M&()L.#::3.
& "A735h;&5C#- g%#:EB5CU-+!:#RAC957-!&-!.D&(!:>*)#8
-)#&OACxrG[O.)57"&\A7;:Oxd57-!-!:%^#:\3(P+!:#RAC957-!"&-!.TV-$"#9fiN+!:3#R4AC957-!"&-4.\#-)
"&":3;&"BG@*!:39;&57A7A7GJ#)&"+,:%^#:39_&()g#&():3TV-#&():>+!:3#R4AC957-!&-!.fi&`x##:Q9fi#:
"&":3;&"BGAC.&5C#-!\+,:%^#:39A79#"&\6F4A7ACG>xA7AiT
O(457+4:#R4AC9{57].[.:3R4;&"*R8GN&()r%m.&&(!;&&():\57W"#9fi\-)#5="5=-!():-F&]5=-M&(!r.#A=AC.&"*
:3!-&579fiT#E*)9#-!"&":3;&"&()9fi#4-v&#%
::#:&(4;&-LR,O+!:"-F&5=-fi&()d&5=957-)BxAC.&
&`xACUX57-!"&-4.>#%V&()gh4%^&"-+4!AC+!:#RAC9 j^%^#):Q39A7AiD%#!:L9fi*!5=!9$Dr-!*%^#):QA7;:B
57-!&-!.?u?D-!*&5=9fiQh4U0:3!-!E#%d?(@57-!&-!.0xd5C&(57*)-F&57Ar"&":3;&"BG+4;:39&":3_#--r'r~8TfiH>.#9fi+)&"fi&()"&-!*4;:3*H*)U[57;&5C#-X#%&()"+,*!)+4'%^#:Vh4Ufi:34-!'#%&()L9fi
+!:#RAC957-!&-!.D8-4*&()-Q*!5CU[57*)\&():!A7&RFGfi&(!9fi+ACd9fi-Q&"#2-4):O&(!:3!AC&57-)#&
Y,.&"*LR8Gfi&()'9;B-!5C&!*!O#%
&()+c*4)+U;A7)TO(!5=.#F.KQ5C-v&#%
U;;:357;&5C#-LU:3;B*0#fU:
A7A+4:#R4AC957-4"&-!.N57-&()>;&"B#:GJ5=VA757"&"*5=-H;R4A7L$AC#-)Bgxd5C&(X&()>eU:3;B$"+,*!)+
%^#:2&()Q57-!"&-!.M57-H&()L+!:#R4A79;&"B#:GTg/'M3()#exd-D&()L9fi#!-F&2#%\::#:M+!:"-F&M5=-H&()
&5795=-)B-fiR,O6F45C&"OA7;:BD8-!*fixd()-_&x#E"&":3;&"B5C+,:%#:?9A79fi#"&68!A7A7G2xA7AiD&()Ox57-!-):
%^#:d-vG0B5CU-:3!-$-R,NA79#"&O;:R45C&":?;:GT
#.#!-F&d%^#:O!3($9>57AC*!5=-)BE*4;&[D!xN#:&OA7Ac+!:#R4A795=-!"&-!.O5=-L&(!bh4%&"-+4)AC
*)#95=-0RFGQ&()N9fi#!-F&O#%]U;:?57-!.V#%W&()N"&":3;&"BG$&5=957-)BE:!A7&TO()#"V+4:#R4AC957-!&-!.
\l

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8
/d+4+!:#3(
q;!AC W57AZ3q;
odzS
W57ACZ`odzS
lN!9;:dnod;# Ti;tSjT tvu T |;tsPjT ttvu T ;tv;sPjT Fu T ;tttSjT tu
:*)&([Zih4:?"& T s>iPjT7q.Fu T !qs;tSjT t)qeu T vs[qQjT7qqeu T |;tttSjT t)qeu
)T 
T v
T7qi[qs
Tivs
T ti>i
];R4ACV)b57&":35CR4)&"*$:M8;:?3(S\A75hc;&5C#-$oO4AC&
&(!;&G85CA=*$>AC;:'"&":3;&"BG$xd57-4-):O;:2+A7.*;&d&(!N&"#+P#%W&()MA757"&TH2&()-hAC&":O&()M*!;&
"&M&"#gI+#-!ACGS&()>&"#+<&(!5C:?*H#%r&()L"#:3&"*J+!:3#R4AC957-4"&-!.T$O()L57-!"&-!.M57-H&()L&"#+
&(!5C:?*P#%&(!57'hA7&":*H*!;&$"&N;:*4)+4A757;&"*X57-S&()fi&":?57-!57-)B0"&TfiO()_:3!AC&'#%\O
!] _
+,:%^#:39-!.#-E&(!57hAC&":*M&":357-!5=-)Bd"&57]()#fxd-E%#:W3(_.[+,:3579fi-F&W57-N&()\W57AZ3q;H.#A=!9-
T
H2+,:%^#:39fi5=957A7;:hAC&":?57-)Bfi"&"+&"#&(!N:#R,#&O;:399fi#&57#-$+4A7-!-457-)B_*)#957-$*!;&[T
O(457V;+!+!:#?(-R,!*H57-X3(<*)#957-H5=-Xxd(!57?(X+!:#R4AC95=-!"&-!.N*!#-)#&NACxrG[
G[5CA7*<HAC;:"&":3;&"BG<xd57-!-!:T)#:.)9fi+4ACD+!:#R4AC957-!&-!.fi*):3exd-<%:#9&()$+A7-!-!57-!B
*)#95=-<-4*;:&5h57Ar*)#9>57-A7Ar*)9fi#-!&":3;&"(!5CB(<U;;:357-!.$#%b"&":3;&"BG<&57957-!BD&(8!fiA7A
+!:#RAC957-!"&-4.V;:>)&57A=5C*
T_)#:M-FGPB57U-*)#957-D&()-8!92R,:N-!*X&GF+,#%&""&2
&"#!"br&":357-457-)BM*4;&fi-0Rcb"AC.&"*0R"*Q#-0&()V9fi#!-F&#%U;:?57-!.b#%"&":?;&"BGL:3!AC&T
O()$*!5=*)U;-v&;B$#%&()0hAC&":*)Z`*!;&X9fi&()#[*57E&(!;&">5=-x(!573(<&x#"&":?;&"B5CG[5CA7*
579>57A7;:r&57957-!BO9G$R,N*!5=;:3*)*
D)U-Pxd()-&()2&x#Q"&":3;&"B5C+,:%#:?99M43(R,&"&":&(!#&():+c#35CR4ACb"&":3;&"B57T
O(!S:3!AC&$57-w];R4ACU:357%Gk&(!;&P\
4-!&"#9;&57A7ACG"AC.&$+4;:?A7ACAb";:3?(
"&":3;&"B57&(!;&WG857A7*2B:3;&":"+,*!)+M&(!-fi!57-!B-FGM57-!BAC"&":?;&"BGM%^#:A7AF+!:#R4AC9{57-!&-!.
+4!A=AC*@%^:#9&()ShAC&":3**4;&"&T#exU:Db&(!57Q&;R4A7X*!#F$-)#&$57-4*!57;&"X()#exxA7A'&()
9?(!57-)NAC;:?-!57-)B.#9fi+,#-)-F&d57r+,:%^#:395=-)Bfi;&d&()NA735h;&5C#-&"ITOb-)N*4-)B:57-$A=57"&57-)B
#-!ACG$"+,*!!+:!A7&d57\&(4;&&()E-F492R,:3O9>GR,NR45="*R8G$&()M9>;B-!5C&!*)2#%"U:3A]A7;:B
+!:#RAC957-!"&-4.O57-0xd(!5=3(g\
!H.#:3:.&ACG0+4573I8\&(!VRc&O"&":3;&"BGT
1a-];R4AC\Nxd9fi!:d()#fxxA7A[&()d"G["&"9A75hW3(Q-)x@&""&+!:#R4AC9TWb-!.;B57x2+,:%^#:39&"-[Z%^#A7*g.:3#OU;A757*!;&57#--!*g3()#ex{9fi-gA735h;&5C#-g::#:%#:3(S;+!+!:#?(
T
O()Mh![*X"&":3;&"B5CjmlN49;:V-!*Xod;#)D:*)&([Zih:3"&?uACxrG[b+457Ig&()E.#:3:.&NA75Ch;&5C##%M+!:#R4A7957-!"&-4.'&()5C:#exd-D8-!^
* !"5C&*)575C#->&":'&"#M+45=Ifi&()bA75Ch;&5C##%N&()S+!:#RAC957-4"&-!.T[5CB-!5hc-!.gU;A7)Q;:SB;&():*!5=-)BJ+45C:*"&!*)-F&>&aZ&"&
-!*;:3L()#fxd-57-X+;:-v&(!"N%^#A7AC#fxd57-)B$&()L9fi-::3#:T>1a-H3(#%r&()fihA7&":**!;&&D
)T L5CB-!5Ch-v&A7GQ#!&"+c:3%#:39>O5C&():h![*P;+4+!:#3(j^+ t[T tuOx()-+!:3*!57.&57-)B>&()E.#:3:.&
A735h;&5C#-0#%!-!-0+!:#R4AC95=-!"&-!.T
 7q 

mnYi'l#^nFf#e
1a-0&(!57r.[+c:?579fi-F&\O
!]H"AC.&\&()V#+4&579A
-8!92R,:r#%WA7!"&":3r&"#L!"b%#:d3(+4:#R4AC9
57-!&-!.TG$.#92R45=-!57-)B_&()N%^;&):d#%*!57"&":357R4)&"*Q&":2";:?3(g-4*$+4;:3A7A7A,xd57-!*)#fx";:3?(
D
5C&O57r+,#35CR4AC'&"#>3(45CU2R,&"&":O+,:%^#:39-!.V&(4-$xd()-$?(g;+!+4:#3(P57\!"*5=-$57"#A7;&5C#-T
O(!PA7!&":357-)BACB#:?5C&(!95=&""&"*w!5=-)Bq;Dd8Dd-!*A74"&":3L#M
- i+!:#[."#:3L#%N-r'r~ Q%^#:N&()_h4%^&"-H+4!ACD:#R,#&V9fi#&5C#-H+A7-!-!57-!B)D,-4*J[*)#9>57-!D-!*H457-)BHq
-!*0EA7!"&":3%^#:&(!dh4%^&"-Q+4)ACb*)#95=->#-0E*!57"&":357R4)&"*-)&x#:3I>#%]|M\T"&r:3!AC&
%^#:O&()2A74"&":357-)BfiACB#:?5C&(!9;:N+4:"-F&"*57-$];R4AC28T
s

\<u

fi

P#$}

wwyXm$

/+!+!:#?(
q ;)A7 W57AZ3q; odzS W57AZ`ozP
qM\A7!"&":
 8T t
i8T [q
i;t[T t)q
i8T;
>\A7!"&":3
 FT t;
i)T s
|;t[T ts
;s8T s|
Q\A7!"&":3
> i8T |
vs8T  c  q q8T7qe
)T fZ`-crr~
Lq:d q>i8T  cgq
 { qJ
#9MR457-!*[Z)T 
;8T s;t
];R4AC28\A7!"&":35=-)Bfi[+c*4)+oO4AC&
/+!+!:3#3(
qErA7!"&":
LrA7!"&":?
0rA7!"&":?
r)T 

A7-!-457-)B  bzZ3q;
qt|8T i
F Tis
q.v8T t
F T t
qs8T7q

   qi 
q: 


q;)AC W57AZ3q;
odzS
W57AZ`odzS
T >iPjT t;Fu T vsPjT ttvu T;tPjT7qqeu T;|gjT t)qeu
Tis> iPjT u Ti;|PjT ttvu T s;t;v|gjT t;Fu T;|gjT tu
Tfv|sPjT7q|u T |>i>igjT ttvu T !qgjT7qqeu T SjT t)qeu
Ti;> i
T 
T vs[q
T7q.v|[q
];R4AC i8\A7!"&":35=-)Bfi\A75hc;&5C#-$oO4AC&

];R4ACL*)9fi#-!"&":?;&"N&(!;&fi\
!] _ N!&"#9;&57"&":?;&"BGJ"AC.&5C#-J!57-)BPr)T 0#)&"+,:"Z
%^#:39r-vGLh![*0"&":3;&"BG$57-QA79fi#"&\A7A,*)#95=-!D)-!*0ACxrG[\+,:%#:?9R,"&rxd(!-L&(!hcAC&":*
*!;&_"&\;:V!*
TO()'&;R4ACbA7"#fi57-!*!5=;&"&(!;&r&()'#+!&579>Ac-8!9MR,:#%A=!"&":3#-0U:3;B
U;;:35C%:#9#-)b*)#957->&"#E-!#&():D[&(F4:5=-)%#:?57-)BN&()'-)*L%#:\)&"#9;&5="A7.&5C#->#%&(!57
+4;:39&":T1a-L&(!''z.8+,:3579-v&D[R,!"'#-!ACG>5CB(v&r+!:3#8.#:3r;:VU;57A7;R4AC'x'.[+,:"Z
579fi-F&"*<xd5C&(kqL#:fiSA7!"&":?2%^#:_3(+!:#RAC957-4"&-!.TXO()0.#92R45=-)*J:3!AC&E;:0;B57.#A7AC.&"*Q%^:#9&(!&""&r"%^#:&()dh4%^&"->+4!AC-!*L:#R,#&;:?99fi#&5C#-L+4A7-4-!57-)BN*)#9>57-!T
O(!NA75hc;&5C#-$:4AC&r%#:3()#57.2#%W-8!9MR,:\#%WA=!"&":3O;:3N()#ex-57-];R4AC i8TV-$&()
hAC&":3*$*!;&"&D
)T _#)&"+,:%^#:39\A7Ach48*&":3;&"B5Cd;&5CB-!5Ch-!.VACUA
#%W+ dt[T t8T
 q #nYi'l#^nFf#e

1a-0&(!57.8+,:35=9fi-v&xN*)9fi#-!"&":?;&"M\
!]_ r;R457A=5C&`G&"#fi+57IQfi9fi&()#[*L#%]#:3*):?57-)BE&()b&":
%^#:fi.8+4-45C#-
T];R4ACgP()#fxd_&()0:!A7&M#%&(!57E.[+c:?579fi-F&TJ)#:&()Qh4%^&"-+4)A7D&()
&`x#$&""&"*Sh![*S#:3*):35=-)B;:3$jm+D%&Do5CB(v&D
'#ex-u'-!*jm'#exd-
D%&Do5CB(F&Dd+cu?TN!#:
&()0:#R,#&;:399fi#&57#-+4A7-!-!5=-)Bg*)#95=-
D&`x#Hh![*<#:3*):35=-)B_;:$&""&"*@.#::3"+,#-!*!57-)BP&"#
#:3*):?57-)B\"#57-F&9#eUr%^:#9&(!'R"#%&()';:?9&"#_&()'-!*L.Yc.&"#:eD4-!*L#:3*):357-!B\"#57-v&r9fi#fU
%^:#9&()M-!*.Y,.&"#:'h4:3"&*)#exd-g&"#L&(!NR4"M#%&()M;:39A7"&Tdb-!A7GL#-)M#:3*):35=-)B>57O4"*%^#:
&()2;:3&5h57A,*)#95=-
T
1a-<&(!57E.8+,:35=9fi-v&Dr)T PG85CA=*!2&(!QR,"&fi"+,*!!+:3!AC&E%#:fiA7Ar*!;&;R"DhcAC&":*<#:
!-[hcAC&":*
T$1a-J&(!Q;:&5Ch57A*!#957-
D]R,!"Q+c:3%.&M#:3*):357-!Bg57-)%^#:39;&5C#-57EU;57A7;R4ACL&()
'1aO/&":3;&"BGA7#PG[5CA7*!N&()LR,"&E+,#5CRAC"+,*!)+:!AC&TgO()Q.#92R45=-)*J:3!AC&E;:
B-):3;&"*P!57-!BE&(!Vh4%^&"-$+4)ACN-4*$:#R,#&O;:399fi#&57#-$+4A7-!-457-)BE+!:#R4AC95=-!"&-!.T
];R4ACN|fi(!#exdr&()V:3!AC&r#%]A75C%^G[57-)BE#:3*):357-!BE+4:#R4AC9#-0&()bhcAC&":*$-!*$!-)hAC&":*
*!;&X"&THw(!57A70)T SACxrG[_G857A7*!N&(!LR,"&_U:3;BP"+,*!)+DW&()0AC;:3-!5=-)BP"G["&"9*)#8
\w

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8
/+!+!:3#3(
q ;!AC W57AZ3q; odzS W57AZ3q;
b:3*):357-!BQq
v s8T |
vs8T | i)T;s |8T v
b:3*):357-!B_
 8T t
i8T !q i8T !q |;t[T t
''1d/
; t[T
i>i8T7q ;8T ;t ;s8T ;t
#8A
; t[T;
i|8T s ;8T  |8T 
r)T 
 L q   Lq: Lq 4 q  
r#92R45=-)*[Z)T 
8T |
;RACMF':3*!:357-)B>8+,*!)+$od!AC&
/+!+!:#?(
W5[*
'1aO/
#[A
)T 

q;)A7 ]5=AZ3q;
ozP
Tis;gjT Fu Ti>ijT ttvu q;T tttSjT t)qeu
Ti[qsSjT tvu T8qijT ttvu T ;t)qiPjT [qeu
Ti|gjT Fu T [qijT ttvu Tis|XjT tu
Tt is
T7q.vs
T 
;RAC2|8':3*!:357-)BrA735h;&5C#-$od!AC&

/:&5hc57A
s8T 

m q
i;t[T i
m q


W57AZ`ozP
q;T tttSjT ttvu
T ;tttSjT tu
TtttSjT ttvu
T tttt

)- #&G85CA=*fi&()dR,"&A75hc;&5C#->):?.G#-L4-[hAC&":*>*!;&[DF&(!#)B(L57&*)#83(45CU&(!dR,"&
:4AC&#-L&()OhcAC&":*>*4;&2"&TV-&()dhAC&":*L*!;&M&D)r)T N#)&"+,:%^#:39h![*>"&":3;&"B57
;&d35CB-!5h-4.'U;A7)V#%W+ t[T t_#:OR,&"&":T
 q
  tmk!akmkmli'lg

8ee
#*R4A7-457-)Bg57B-!5h-F&ACGHYc.&_&(!>+,:%^#:39-!.L#%OP9f"#:35C&GH#%\+4;:3A7A7AACB#:357&(!9T
w()-Xx#:3IS57b*!57U857*!*gU-!A7GP9fi#-)B$+4:#8.3"#:3D-)#$AC#*XRA7-!57-)B057V-).3;:GT_d):?57"&57
";:3?(@%^:68)-v&A7G<.:;&"0(!57B(!ACGJ5C:3:B!A7;:";:?3("+4.Drxd(!5=3(:4AC&57-AC#*k579MR4A7-!.
R,&`x-$+!:#[."#:?T\
4X+,:3957&AC#*0RA7-!57-)B_#+,:3;&5C#-4*4):357-)B_5C&":3;&57#-!r#%1aO/2}T
/+!:#[."#:Nx5C&(H-)#[*)VU;57A7;R4A7fi#-5C&V#+,-A75="&V9GH*)#-!;&"L"#9fi#:2A=AW#%&(!>-)#[*)b&"#
fi:68)"&5=-)B_+!:#[."#:T'5735C#-!\&(!;&Yc.&'"G8&"9+,:%#:?9-!.N57-!A=!*)b*)57*!57-!BEx()-0&"#
R4A7-4.Q&(!AC#*
D5=*)-v&57%G[57-)BSP+4:#8.3"#:%^:#9xd(!57?(<&"#H:6F!"&_x#:IcD-4**)57*!57-!BS()#fx
9E!3($x#:I0&"#>*)#-4;&"T
1a-_&()h4:3"&WAC#*_R4A=-!57-)B'.8+,:35=9fi-v&]x\&""&\
4 _ W;R45=A75C&`G2&"#N"AC.&&()\;+!+!:3#+!:357;&"
+!:#[."#:E+,#A7A757-!B"&":3;&"BGTHH$(!U$579+4AC9fi-F&"*&()0"G[-!3(!:#-)#!M:#!-!*:#R45=-J-!*&()
:3-!*!#9 +,#A7A=57-)Bg;+!+4:#3()Tb-<&()$-r'r~8DS+!:#[."#Q: _ $-)5CB(FR,#:3M;:3Q+,#A7A7*J%^#:
x#:Ijm!57-)BH&(!g-r'rY~ _ (FGF+,:3!Rc0&"#+,#AC#BGD .#::"+,#-!*4_&"s
# 79<; ,Mu_xd():5=-&()
bz -FU857:#-!9fi-F&Dg+!:3#8.#: _ E:35CB(F&M-!*<AC%^&M-)5CB(FR,#:32;:3L+,#A7AC*j I .pgR,!"Q&()
x#:I8&;&5C#-!\;:N.#-!-!.&"*$xd5C&(0E:357-)BE&"#+,#AC#BG!u?TO()b:4AC&#%]&(!57.[+,:3579fi-F&;:3bA=57"&"*
57-$];R4AC2s8T
];R4ACs0()#fxdb&(!;&V#-!.;B5=-)T QG[5CA7*!&()fiR,"&V"+,*!)+X57-H9fi#"&V"M-!*XACxrG[
G[5CA7*!d&()MR,"&'"+,*!)+P#-ghcAC&":*S*!;&L&TV/9fi#-!BL&()Mh![*g:3!AC&Dc-)#057-)BACE;+!+!:#?(
#)&"+,:%^#:39r&()2#&():3\#-A=A*!;&"&T
n

\

fi

P#$}

wwyXm$

/+!+!:3#3(
q;)AC W57AZ3q; ozP W57AZ`ozP
5CB(FR,#:
8T t
i8T [q |8T |[q
FTi
o-!*)#9
t[T; |8T7qe
>i8T t
 q7 
r)T 
;t[T 
  q $q7 Lq
r#92R45=-)*[Z)T 
>i8T8q
];R4AC2s8#*rA7-!57-!BL8+,*!)+$oO4AC&
/+!+!:#?( q;)A7 ]5=AZ3q;
ozP
W57AZ`ozP
5CB(vR,#: T ;tigjT t|u T igjT ttvu T sgjT [qeu T ;tttSjT t;Fu
od-4*)#9
T isSjT tu T v|gjT ttvu TtiPjT tu T |;tttSjT ttvu
)T 
T |;ti
T7q.vs
T t;v|
T tttt
;R4A7qt[#*PA=-!57-)BLrA=5h;&57#-Qod!AC&
] ;R4ACbqtV!99>;:35C&()\A75hc;&5C#-2:3!AC&#%4&()h![*_"&":3;&"B5C5=-M.#9fi+;:357"#-E&"#b&()
)T fiA=5h;&57#-!T)#:'3(P#%&(!VhAC&":*g*!;&"&D,)T _#)&"+,:%^#:39O-FGQh![*"&":3;&"BG
xd5C&($>5CB-!5hc-!.b#%]+dt[T t;#:OR,&"&":T
O(!X.#-!*wAC#*wR4A7-!5=-)BJ.[+,:3579fi-F&Q*!9fi#-!"&":3;&"S\
!] _ 0;R457A=5C&`G&"#@*)&":?957-)
&()#+4&579AO9fi#4-v&#%bx#:I&"#*)#-4;&"g)+,#-:68)"&Tk1%b&"#8#JA75C&"&AC$x#:I57*!#-!;&"*
Dr&()
:68)"&57-!BH+!:#[."#:>x57A7Ar"#8#-@:&):?-%#:Q9fi#:x#:IcTk1%b&"#F#9M!?(@x#:I@57*!#-!;&"*
Dr&()
B:3-F&57-)BS+!:#[."#:_xd57A=A"#8#-<Rc057-<*!-)B:_#%dR,.#95=-)BP57*4ACTS];R4ACXqq0A757"&M&()0:4AC&2#%
&(!57.8+,:3579-v&D8*)9fi#-!"&":?;&57-)BN#-!.';B57-&(4;&&()'AC;:3-!57-!BbG8"&"957;+;R4ACd#%,.Y,.&5CUACG
"AC.&5=-)BPAC#*R4A7-457-)Bg"&":?;&"B5CD.).+!&Mx()-&()0!-[hcAC&":*H&""&_"E%^:#9&(!h4%^&"+4)A7g;:P4"* j^#-k&()P-crr~-!*@#-k&()S*!57"&":357R4)&"*-)&x#:I#%Vx#:I["&;&5C#-!u?TO()
.#9MR457-)*g:!AC&';:MB-):3;&"*H!5=-)Bfi&":357-!5=-)B>"'%^:#9p&(!Nh4%^&"-S+4)AC2-4*g:#R,#&';:39
9fi#&5C#-+4A7-!-!5=-)BE-crr~.[9fi+ACT
];R4ACHqXA757&2&()A=5h;&57#-<!:3.G:3!AC&Tr)T X*)#Ffi-!#&E+,:%^#:395CB-45h-F&ACG
R,&"&":&(!-&()rh![*"&":3;&"B5C%^#:&()O!-[hcAC&":*fi*!;&[DFR4)&*)#8+,:%^#:395CB-!5Ch-v&A7G2R,&"&":
j^+ dt[T t;Fu\&(!-$&()Nh!8*$&":3;&"B5C%#:O&(!bhAC&":**4;&[T
/d+4+!:#3(
q;)AC W57ACZ3q; odzS W57ACZ`odzS bzZ3q;
;t MZ`-r'r~
8T t
i8T [q i8T7qt
8T vs
qJ
;t MZ`-r'r~
i[q;T s i[q;T >i
i;t[T7q
FT vs
 Lq
)T fZ`-r'r~
[q;T |
d{qC  4{q   qm8
FT ;t
#9MR457-)*[Zr)T 
8T 

;RACqq;b57"&":?5CR4)&5C#-0/'9fi#!-F&8+,*!)+$oO3!AC&
2le

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8
/+!+!:#?( q;)A7 ]5=AZ3q;
ozP
W57AZ`ozP
;t
T >isgjT >iu T gjT ttvu T7qs|XjT ttvu T ;tttSjT t;Fu
;t
T v>i[qLjT7qeu T i>ijT ttvu T |;t)qiPjT t)qeu TtttSjT t)qeu
)T 
T ;t>i
T7q.vs
T7qs|
T ti>i
;RACq8b57"&":?5CR4)&5C#-Q/'9fi#!-F&'rA735h;&5C#-$od!AC&
/+!+!:#?(
8+,*!)+
n gnYuYk
f)T 
od-4*)#9:3#8.#:
t[T;
#[A]b:3*):357-!B
i|8T s
:3-!"%^#:39;&5C#-S':3*):35=-)B
i>i8T7q
lN!9>;:d-!*o;#
i8T |s
b57"&":357R4)&"*$:
i8T |
W5[*~U;A7!;&5C#-Jq
i8T !q
qMrA=!"&":
i8T [q
5CB(vR,#:d
i8T [q
;t b57&":35CR4)&57#i8T [q
>rA=!"&":3
i)T s
;t b57&":35CR4)&57#i[q;T s
W5[*~U;A7!;&5C#-P
vs8T |
QrA=!"&":3
vs8T 
%{{qdt@i{n8
 , k) nYin8 i8T v
];R4ACq8rr#92R45=-!;&5C#-0#%)T od.#99fi-!*!;&57#-!
F pep nxlk)itl
+&"#&(457Q+c#5=-v&DdA=A.[+,:3579fi-F&Q(!U(!#exd-&()X:4AC&Q#%_\
!"A7.&57-)B<357-)BAC
"&":3;&"BGD\A7A#&():L"&":3;&"BG:!A7&_Rc5=-)Bgh48*
T1-&(457_.8+,:35=9fi-v&ExA7AC#ex\
!]&"#
"AC.&'A7A
"&":3;&"BGP3()#57.;&d#-!.M%#:B5CU-g+!:#R4AC9-4*$.8!&"M&()2+4;:3A7ACA
;:33(gxd5C&(
&()b:.#99-!*)*$"&":3;&"B57THN&()-$.#9+4;:b&()V:4AC&&"#fi3(0h![*0"&":3;&"BGj^&()bh![*
"&":3;&"BG?()#57.57$eU:3;B*#fU:PA7Ab+!:3#R4AC957-!"&-4.$-!*wA7Ab+,#5CR4A7P3(!#57.$#%E#&():
"&":3;&"BGQ*)5757#-!?u?TW/:3-!*)#9"&#%;tE+!:#R4AC9>%^:#9&(!Oh4%^&"-Q+4)AC'*)#957-L57"A7.&"*
-!*:?!-#
- iJ+!:#[."#:3>#%'&(!g-r'r~8Tw;R4A7q!9>9;:35Cfi&(!g"+,*!)+%^#:>?(
;+!+!:3#3(
T
O(!":4AC&N57-!*457;&"fi&(!;&_O
!]@-.Y,.&5CUACG9>;ILA7A&":3;&"BG3()#5=.M;&M#-!.T
O()A7;:3-)*V:34AC
3(!57UR,&"&":+,:%^#:39-!.&(4-V&(!;&#R4&57-)*VR8G'-FGV#-)#%8&()""&":3;&"BG
3(!#57.TO()O:3!ACA7"#N#)&"+,:%^#:39-FG57-!BACh!8*L"&":3;&"BG>?()#57.'U:3;B*Q#eU:\A7A!#&():
+4;:39&":d#+!&5C#-4T
 q 

sJt

s {q 
p i'li'lg $

^n t

2l

fi

P#$}

wwyXm$

zP&()#8*
~::#:
1ab
t[T7qti
rb
t[T7qq
)T 
t[T7qi
rG5=- t[T F8q
zSfa#:357&`G t[T F8q.
rI8+!:#+ t[Ti
];R4ACq.)zP?(!57-)N;:3-!57-!Br#9fi+4;:357# :q D !k ##i'l#n 

nYk$li'lg

^
1a-&()E)::-F&OU:357#-#%&()EO
!]J"G["&"9$DxE!"Er)T &"#Q57-!*!!.NL*)5757#-$&":2R"*
#-&(!2&":357-457-)Bfi*!;&[T)T >(!O+!:3#eU-P&"#>R,V.Yc.&57UE57-+!:*!57.&5=-)Bfi&()2"&":?;&"BGg3()#5=.d%^#:
&()"L&""&M*!#957-!TL1-J*!*!5C&5C#-D&()>#!&"+4)&N#%r&()>G8"&"957NeU5=A7;R4AC>Mg"G[9MRc#A=57E:3!AC
R4"Dxd(!57?(H9GXA7AC#fx&()"G["&"9 *)UAC#+,:V&"#g*)&":39>57-)fi&()fi%m.&"#:3N&(4;&2Yc.&E"&":3;&"BG
"AC.&57#-$%#:fiB5CU-;+!+A757;&5C#-$*)#9>57-
T
'&(!:_9>3(!57-!LAC;:3-457-)BP;+4+!:#3(!E-A7#PR,0!"*&"#S+,:%^#:39 "&":3;&"BGAC.&5C#-57&()\
4"G["&"9$TX#S&""&_&()Q:!AC&M#%OU;;:35C#!M.)57"&57-)BP;+!+!:#?()DxQ3)+!+4A757*H&()
*!;&Q%^:#9A7A]#%&()0q0)A7EA735h;&5C#-S.[+,:3579fi-F&*!.:35CR,*S57-g&(!E+4:U857#!.&5C#r57-)+4!&&"#EU:?5C#-!#%)T 8D[&()'1VM*)575C#->&":V57-!*!4.&5C#-ACB#:357&(!9j b457-!A7-
Dqs>| iu?D!&()
rb>6F)-F&57AW.#eU:?57-)B$ACB#:35C&(49jrA=;:IPn57R4AC&"&DWqs|su?DQR4I8+!:#+4;B;&57#-H-)):3A]-)&
jmod49fiA7(!;:&nwzSerACA7A=-!*
D)qs>| iu?D8brG357->A735h4:Oj"&-45CID!qss;tvu?D[-!*fiV9f"#:35C&GvZx57-!
A735h4:T/'Oxd5C&(0&(!V#&():d.[+,:3579fi-F&D[:!A7&O;:VR4"*#-&"-[Z%^#A7*.:#3aZUA=57*!;&5C#-
T
];R4AC_q.E3()#exd&(!;&&()b*)5757#->&":bACB#:357&(!9+c:3%#:39*R,"&#->&(!5=+4;:3&57!A7;:*!;&
"&TN'AC&579;&"A7GDc&(!MR,"&b93(457-)_AC;:3-!57-!BLACB#:357&(!95=-g&(!5=.#-F&".8&N5=&(!EACB#:?5C&(!9&(!;&
.#-!5="&"-v&A7G@G857A7*!L&()PR,"&0"+,*!)+T{1%NxS.#-457*):$-)#:39>A75C*k+!:#R4AC9"+,*!)+D\&()
ACB#:357&(!9&(!;&_+!:#[*!!.E&()0R,"&EA=5h;&57#-#-eU:3;Bgxd5=A7AA7#S+4:#8*4!.>&()0B:;&"&
"+,*!)+T'H>xd57A7A.#-v&57-8)_&"#$.[+4AC#:EU;:?5C#!b93(457-)fiAC;:3-!5=-)BL9fi&(!#8*!'&"#*)&":?957-)E&()
;+!+!:3#3(&(4;&Oxd57A7Ax#:I0Rc&\%#:O&(457r&`G8+,N#%];+4+4A757;&5C#-T
W	   ) `+; 



sJt

psr

k$i ?tl

 ZVs{
O(!5=W+4;+,::+,#:&#->x#:I_+,:%^#:39fi*_&"#E.#92R45=-)\&()OR,-).h4&#%c+;:3A7ACA)";:?3(>;+4+!:#3(!
57-<&()g\
!G8"&"9T<~8+,:3579-v&;&5C#-:UA7&(!;&"&":3;&"B5C>*)UAC#+,*#eU:L&()$A7"&
%^xG;:?r#;Yc:O*!5="&57-!.&Rc-!.h4&&"#5=9fi+!:#fU857-!BN&()b+,:%#:?9-!.d#%]/d1;+!+4A757;&57#-!Td#fxU:eD
xd(!5=ACQ-FG+4;:&574A7;:ACB#:35C&(49-@+!:#fU85=*)$5CB-!5hc-v&"+,*!!+<%^#:#-)g&`G8+cg#%+!:3#R4AC9$D
#-S#&():b+!:#R4A79O&()EACB#:?5C&(!9'-X.&!A7ACG$+!:3#8*!4.2x#:3"E:!AC&d&(!-X!57-!B>L:357A
U:357#-#%\&()Q";:33(<ACB#:357&(!9$T$/M:3!AC&D]&()"L&":3;&"B5CE-)*J&"#PR,>;:%m!A7ACGX3()#R4"*#-$&()2?(!;:3.&":357&57O#%Wfi+4;:&5=!A7;:\+!:#RAC9$T
1a-&(!570+;+c:$x*)9fi#-4"&":3;&"&();R57A75C&G#%\
!]&"#!&"#9;&57A7ACGw"AC.&$+;:3AZ
ACA";:?3(J&":3;&"B5CE-!*"&2;+4+!:#+!:35=;&"E+;:39fi&":3T$\
49fi+AC#eG[V&()Qr)T $93(457-)
AC;:3-457-)BLG8"&"9&"#$9;I-X57-F&"A7A75CB-F&?()#57._#%&":3;&"B5Cb%^#:'?(H-)x+!:#R4A7957-4"&-!.T
~[+,:3579fi-F&%:#9{&()r*)#95=-!#%&()h4%^&"-M+)AC+!:#R4AC9Df:#R,#&W;:399fi#&5C#-E+4A=-!-!57-)B)D2lf

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8
;:&5hc57A
";:3?(P"+.D-!*g+4A7-!-!5=-)BM+4:#R4AC9\5=-!*!57;&"V&(!;&V\
4HG[5CA7*4rRc#&(P579fi+!:3#eU*
"+,*!)+M:!A7&-!*E5CB-45h-F&ACGV579fi+!:#fU*MA=5h;&57#-M):?5CW#eU:-FG2"&":3;&"BGE!"*E5757"#A=;&5C#-gx()-g(!57B([ZU;:?57-!.2&":?57-!57-)BL"V;:_!"*
T'O()"E.8+,:35=9fi-v&'A7"#Q*)9fi#-!&":3;&"
\
! _ ;R457A757&`GJ&"#"AC.&LA=Ar+4;:39fi&":3L;&#-!.S-!*&"##)&"+,:%^#:39 -FG<h![*@"&":3;&"BG
#eU:VL"&d#%+!:3#R4AC957-4"&-!.Tr1a-g*!*!57&5C#-
D)xM*)9fi#-4"&":3;&"2&(4;&VO
!]-gR,-).h&ORFG
)&57A=5C57-)BP&":357-!57-!BP"L*):3x-%^:#9XU;;:35C&G#%'&""&*)#95=-!D&(8!_x$x#!A=*.[+,.&fi&()
+,:%^#:39-!.#%&()b"G8&"9&"#fi579+!:#eU'U-09fi#:bxV57-!.#:3+c#:?;&"*!;&E%^:#9-!x+4:#R4AC9
*)#95=-!\-!*;:3?(!5C&".&):3T
\x#Q#%&()E?(!A7AC-!B57-F&":#[*!!.*gRFG$#):':";:?3(X;:E&()E;R57A75C&GL&"#$*)&":39>57-)2*!5=.:3579_Z
57-!;&5=-)BP%^;&):fi-4*&()$;R457A75C&GH&"#H+!:#fU857*!QAC;:A=5h;&57#-!E%#:fi&":35=-!57-)BP.[9fi+ACTJ1a#):O.[+,:3579fi-F&rxNU:35h4*0&(4;&O&()V%^;&):OxN3(!#"2.#!A7*RcV9fi3):*$;:3A7GQ*!!:357-)BE&()
";:3?(+!:3#8.V-4*"&57A7AR,fi:+!:-v&;&5CU>#%&()fi+4:#R4AC9"+4.LA7;&":N#-*!!:357-)BL.8)&57#-
T
/'Ox2;+!+4ACGQ#):d;+4+!:#3(g&"#LfiB:3;&":U;;:35C&G$#%]+4:#R4AC9rxNxd5=A7A,-)*&"#Q*)UAC#+P>9fi#:
%^#:39A9fi&(!#8*)#A7#BGH%#:E"AC.&57-)B$:3+!:"-F&;&5CU0-!**!573.:357957-4;&57-)BQ%;&):3TL1a-*!*45C&5C#-
D
x\#R4":U**!:39;&57r+,:%^#:39-!.\579+!:#eU9-v&x()-_&""&"x:O*!:3xd-_%^:#9+4:#R4AC9
57-!&-!.'xd5C&(XAC;:VA75Ch;&5C#-!TVHfix#!A7*SA75CIE&"#$+4):?)M9&()#8*4#%AC;:3-!5=-)B>%^:#957-[Z
"&-!.W&(4;&].)(!5CR5C&AC#fxU;:35=;&5C#-_57-2+,:%^#:39-4.#%4A7&":3-!;&5CU\"&":3;&"B57-!*_(!5CB(MU;:?57;&5C#57-0+!:3#R4AC95CT
O(!_!::-F&'579+4AC9fi-F&;&5C#-S#%rO
!]%^#[!"b#-X-X1d/N}>;+!+4:#3(H&"#$";:?3(
Tfib-)
:"#-L%^#:&(457?()#57.'#%";:33(09fi&(!#8*Q57&(!'A75=-);:9fi9fi#:3Gfi:6F45C:9fi-F&#%
&(!'ACB#:?5C&(!9$T
/p.#-!*@*)U;-v&;Bg#%d&(!57_";:?3(@9fi&(!#8*57_&(!;&fi-5C&":?;&5CU*)+,-!57-)BX";:3?(9fi&()#[*
+!:#fU85=*)%^*)R43I57-@?(5C&":3;&5C#-k&(!;&0-R,S!"*k&"#<*!"&>+4;:39fi&":3Q%^#:L&(!S-).[&
";:3?(5C&":3;&57#-
T/'\fi:!A7&D!O
!]H-$+,#&"-v&5=A7ACG>*!"&\&(!N"&":3;&"BG$3()#5=.\%:3#9#-)
5C&":3;&57#-fi#%,&()d";:3?(>ACB#:?5C&(!9&"#2&(!O-).8&%^;&):#%c&()d+4.OU;;:GTd#fxU:eD857->"#9fi
+!:#RAC9*)#95=-!O-)#-[Z`57&":3;&5CU2";:?3(SACB#:35C&(!9>d9eG$RcV+4:%:3:*
Tr/ %m)&):323(!A=AC-)BN%^#:
#):b:";:?3(X57&"#Q:3.h-)2&(!_*4;+!&5CUE+4;:3A7ACA";:3?(HACB#:35C&(!9%^#:'!E57-SLB:;&":NU;;:35C&`G
#%5C&":3;&5CUN-4*-)#-[Z`5C&":?;&5CU2";:3?(gACB#:?5C&(!9T
O
!]P579+4AC9fi-F&;&5C#-!;:3'):3:-v&A7GU;57A7;R4ACd#-Q2U;:?5C&`G#%;:33(!57&".&):3A+4A7;&"%^#:39
57-!A=!*!57-)BSzg1azS*!57"&":357R4)&"*9fi9fi#:G-!*@3(!;:*@9fi9#:G9M!A7&5C+!:#[."#:3D*!57&":35CR4)&"*
-)&x#:I<#%b93(!5=-)_:3!-!-457-)BP'zD]#59M4AC&5C&():*457-)BP9>3(!57-!D-!*9?(!57-)fi457-)B
yeU&():*!\-!*g\57ACI&():3*!T:3#R4AC9*)#957-4\)::-F&ACG>!-4*):O57-FU"&5CB;&5C#-$57-4A7!*)b*[Z
*!5C&57#-!AF.#9MR457-4;&"#:357A8#+!&57957;&5C#-E+!:#R4AC9>]3!3(fiW&()O-)Z b)-!]+!:#RAC9 -4*fi57-v&"B:?;&5C##%9>3(!57-!bA7;:3-!57-)B)D8&()#:9+!:#eU[57-)B)D[-!*$-!;&!:3A,A7-)B!;BNACB#:?5C&(!9r57-F&"#_&(!57r";:3?(;:"Z
3(45C&".&):TH'()#+,\&"#2*)9fi#-!&":3;&"d&(!;&+4;:3A7ACA)(!):357"&5=;:33(LACB#:35C&(!9>-G[5CA7*_R,#&(
#+!&579>A MA7;R4ACV;+!+4:#3()d&"#+4A7-4-!57-)B)D[93(457-)NAC;:3-457-)B)D)-!;&):3AA7-)B!;BD!&(!#:9
+!:#fU85=-)B)D!-!*9-FGQ#&():.#9fi+)&;&5C#-[Z`57-F&"-!57UN;:O#%W/d1T


  JP`4J 




O (!5=Wx#:IExr)+4+c#:3&"*_RFG_;&57#-!A[5C-4.d)#!-!*4;&5C#-fiB:3-F&1aoO1Zs;t|;t|8D)1od1`Zs;t>i;t[D
-!*bzP1`Zs!qs8Td()$)&()#:3_x#!A7*A75CI0&"#H&(!-!Ib-@r):3-!_-!*zS;&"&(!5719(!#%d;&
&()HzP1~;:&(woO"#):?.$;R%^#:$+!:#eU[57*!5=-)BJ.g&"#<&(!5C:$-r'~J&"#.#9fi+AC&"H&()
.[+c:?579fi-F&r:+,#:&"*57-0&(!5=r+4;+,:T
2le

fi

P#$}

wwyXm$

}Xa$  [ m

/B:3xrAiDETCDyv-!;I857:39$DWMTCDnzP():#&":?[DoNT\jaqs||u?TJ/:3-4*)#95C*+4;:3A7A7AR!:3-43(-!*
R,#!-4*>ACB#:357&(!9$T
1a
- \ 3W c; V:[ 2`.  ";+ >"N ?S X; ;7. \ e??2 
:D!+!+T is [;8T,O()N]-!-!"G[ACU;-!578&;&"2'-!5CU:?5C&`GT
/'-!*):3"#-DccTCDnr()-
D4zT,bT]jaqs|u?TdW;:3A7A7A
R!:3-!?([Z`-!*[ZR,#!-4*$ACB#:35C&(!9>O#-&(!M(FGF+,:"Z
!RcT1a
- \ 3W c;$ M:[
 .ff | W!."  	 H8  e??ff ;??Dc+!+T4;ts 8[qeFT
r;::&"&D)/MTCD8nHA=*
DFET
jaqssu?TW;:&57A!#:?*):+4A=-!-!57-)B)UA=!;&57-)BN+,#35CR4ACr.KL5C-4.GEB5=-!T
3(T4:+Td[~@Oo{sfZ`tfZ3q;Dc'-!5CU:335C&`GQ#%W(!57-)B&"#-T
r(!;&"&3(4;:maD,TCD8rACU:&D[lfiTCD[
n cB):3[D8~OT{Tcjaqssu?T/-;:?3(!5C&".&!:d%^#:.&5CU'-)&`x#:IFZ
5=-)B)T1-$]-F&xOGD,/2T!MTjm~*
Tu?
D q.: ff; fi	 ffM fiK;W OcD!+!+T4> i 8;s8T
r(4;+49nA7AT
"&-45CIDTjaqss;tvu?TL~&579;&57-)B$+4:#R4;R457A=5C&5C'.:3!57AW&IS57-H9>3(!57-!AC;:3-!57-!B)TE1as
- \ 
3W c;$ M:[	 ffq:4: r[ 2) >.ff | '.  N ";
 24`.^ 8ff .D!+!+T,qef qe;s8T
r(!A=AC#
DETCDb57-!5DzJTCDnplN!9;:DMTrjaqssu?TW;:3A7ACA";:33(<ACB#:357&(!9V%^#:2:#R,#&29#&5C#+A7-!-!57-!B)T1a-bA7AC:Ddy!T'jm~*Tu?D r ?? :;@
 S:[
 fid d fi 2O ;
& iW  ")|
 @;32 ;[ X; 7. = fi_D4+!+T)t vFFT
rA7;:3IDd]TCDOn57R4AC&"&Do2Tbjaqs|su?TO()rbJ5=-!*!!.&5C#-ACB#:357&(!9$
T @N ,
 W ; c!D D
> i["q 8|)T
#8#IDOETry)TCDn;:3-)A7ADro2T'Tbjaqssu?TzP)5795757-)BH&()PRc-!.h4&#%b+4;:3A=ACAO";:3?(k457-)B
9>3(!57-!OAC;:3-!57-!B)T81a*
- r 3 > d:)# ff " .ff q $'.   % 24`.^ 8ff .D
+4+T4s 8> i)T
//'/d1r:3T
#8#ID)ET4y)T
jaqssu?T/(FG8R!:357*Q;+!+!:#?(0&"#fi579fi+!:#fU85=-)B2&()b+,:%^#:39-!.'#%+4;:?A7ACAc";:33(T]1aX; ;7. r ?.3 qff;& '   ;' 24`.^ FN ?(
 D4+!+Tq;t q.v8T
~A7U85C:eT
#8#ID!ET4y!TCD!A7AD[TCD4nO()#9D) Tjaqssu?T];:?A7ACA,";:3?(!57-!BE&":?-!"%^#:39;&5C#-[Z#:?*):357-)B
57&":3;&5CU.Z`*)+,-!57-!B>/N}
T )[	 2`.  ";+ *;[2 U &24`.^ FN 4, `N fiM.D -Wji|u?D
| 
|;8T
#8#IDETy)TCDnG#-!Dr_Tjaqssu?TzS5CUACG+4;:3A7A7A1aO/2}P";:3?(
/
T *d8 ;X 0'   ;
2`.^ FN ?1
 );?3D 2Wjiu?DWq i q|;t[T
:357B)D4y)T4y)T]jaqs|su?#T 24m ;!e  >    m?T/*!*457"#-[ZHA7GT
~U&"&DzJTCD!-!*!AC:eD)y!TCD!zP(4-v&5iD!/MTCD4n'
D!ETjaqssu?To/N}9>5CUACG>+4;:3A7ACA,()!:357"&57
;:33(
T *d8 ;- Y ;^7.- >$4=.m2   )` ]fi) :5D 2;Dq q.v8T
)A7*49-!-
D4o2TCDczPG[A75Cxd57&"D4]TCD,nzg#-!57-
D'TjaqssFu?Tb8&!*!G857-!Bfi#eU:3()*4'5=-957UACG0+4;:"Z
A=ACA9>57
- 698Z&":_UA=!;&5C#-
T'1@
- \ 3W c;*
 fi:[7 J98v: 1J! ;: ;<fid2 ^d fi
>Y; ;^C= ' ; m: fiE 	'"ff m`em[".?D8+!+Ts qt8T8/'"#[57;&5C#-%#:#9+4)&57-)BVzSZ
?(!57-):3GT
2l

fi x4)}>Lmxm}E3}x)}L}}v}!4<Y}mxv8
):3-!ID'zJTCDN[)I;U[5C:35 [DO]TCDbn)#A7GDby!TET2jaqssu?T1a-)%^:-!.SR,;:J*)57B-!57-)B<57-v&":?.&5CU
5=-v&":%m.&():#)B(>R,%#:-!*L;%^&":r-!;+4(!#&T
1
- r ?? :q b:[ |;>fid2 ^d fi
>!4_ c24`. e ?@ .`ff fiE?D+!+Tvq iA qe;8Tv/'"#[57;&5C#-M%#:#9fi+4!&57-)BOzS3(!57-!:GT
!):?!573(45iDzTCD\;I[5iD\lfiTCDdn1?(vG#(45iD\2Tbjaqss;tvu?T/9M!A7&5Z`ACUAOAC#*R4A7-!5=-)B3(!9fig%^#:
#:Z+4;:3A7ACAc.)(!!"&57Ub;:33($+4:#B:39r#-$&(!b9E!AC&5CZ+45iT1a^
- r 3 >$ V:[B ,3W 
;>=CKD3ffE
Ofid;2 ^c fi >r  cC.
 \ v i? &Y ;^7. r  fifiD)+!+T
qtt qt i8T
/'"#[57;&5C#-$%^#:'#9+4)&57-)BfizS3(!5=-):GT
lNACDT,MTCDn[AC&"#:3DMT
/MTjaqss;tvu?T_W;:3A=ACAW"&;&".Z`"+4.Q";:33(X%^#:NLh4:3"&V"#A=)&5C#-Pxd5C&(
.#-457"&"-F&LA75=-);:L"+,*!!+4T/2`.  ";& *;[2  X; ;7.Y r  fifi:!D FHGj^Fu?D
["q 8s8T
lN;:G8+457D,ETCDnlN!9;:DcETWjaqssu?T2'-!"&":?!.&):*g&":fi";:33(S#-81azP+4;:3A7A7A.#9fi+4!&":3T
1a
- \ 3W c; &4< )."W fi) :GI2D4+!+T8v v i8T,1a~~~#9fi+)&":8#[5C&`GT
lN!9>;:DETCDno;#)DET2Tjaqss;tvu?T)A7;R4ACfi+4;:?A7ACA]%#:?9M!A=;&5C#-!V#%r*)+!&([Zih:3"&N";:3?(
T>1al2!9;:D8lN-!AiD)n '#+A7;IF:?57(!-Sjm~*!Tu?D Y ;^7. ' ; ^: fiMU N;# ff c 24`.^ 8ff 
>KJ 2 "!D)+4+T"q v!q;T,8+4:357-)BL: v:?A7;B)T
57Rc:?9-
D!MTjaqss|u?T\1-F&"B:3;&57-!BL!":'57-F&":%^.M;B-v&'xd5C&(g.#-vU-F&5C#-!A];+!+4A75=;&5C#-!T1a\ e?3W :;
 fi:[M |; >"N ?
 N24`.^ 8ff 4! O 2`.: ffD
+!+T
s v i8T/3"#;Z
5=;&5C#-$%^#:'#9fi+4!&57-)BfizP?(!57-):3GT
zS(!-v&5D,/MTCDn'-45CA7D'Tjaqssu?TE81azP+4;:3A7A7A()):?57"&57N;:33(
(T b  N ";; 24`.^ 8ff .D
PRQ jiu?D
v 8|[q;T
zS(!;+4;&":3[DvMToNTCDn'!&"&D8cT)jaqssu?T4dx@-F&575C+;&"#:G2AC#*ER4A7-457-)B&":3;&"B5CW%^#:]+;:3A7ACA
/2}>ACB#:357&(!9TM1@
- r 3 >
 fi:[! 4&S|3T,. i.| >N4.`M @: [ff fi i.
 
)) ; iW ; fi)`.B
 cN ff c?.D4+!+T
qsA 88T
/9fi:?57-zS;&()9;&57AW8#857&`GT
zS57-v&"#-D,T8jaqs>s iu?T!/')&"#9;&57A7A7GN.#-[h4B):35=-)BO.#-!"&":35=-v&W;&57"%m.&5C#-M+!:#B:39>'"&!*)GT
>)m :4^?UD Fjaqeu?DA vv8T
]#exdA7GD'TCD):B!#-
D'TCDnlV#:%DWo2T]~dTjaqssu?T'+!&([Zih:3"&M(!):357"&5=";:3?(#-H81azP
9>3(!57-!VT '.   
 24`.^ FN ?D PHQ jiu?D]qss 8v8T
]#exdA7GDc'TCD4nlV#:%D!oNT!~dTjaqss[qeu?Tr[57-!BAC.Z`;B-v&\+;:3A7ACAcxd57-4*)#ex";:3?(
,T SD,W)! )ff v ")
>hXi`. XY 2 S
 7ff :c	 24` Fff c?.5D FH]jiu?Dc i>ivFFT
b!5=-!A7-
D)y)T!o2Tjaqssu?|
T HZKc  >fiM& N; fiff c_CW ;2 JTzP#:B-Pl2)%^9>-!-
T
b!5=-!A7-
D)y)Tjaqs>| iu?T1-4*!!.&5C#-0#%W*)5757#-0&":TVff :c	  ; :F;3D F]jaqeu?Dc|["q qt i8T
odf"+4AiD!cT[]TCD)n{lN49;:D!cT,jaqssu?TW;:3A=ACA(!):357"&5=d";:3?(0ACB#:35C&(!9>%^#:\9fi;Bb+4357-)B
9E!AC&57+!:#[."#:O"G["&"9T>fiXc[` c iN ? >72ff2 fi i.3D 2RWj^Fu?DA q|8T
od;#)D\ETMTCDOlN!9>;:DrMTCDdnod9fi(DOlfiTjaqs|u?T /+4;:?A7ACAO579+4AC9fi-F&;&5C#-#%N5C&":?;&5CU.Z
*!+c-457-)B;Z`/2}Tg1a5
- \ 3W c;h
 Q:)
 ffS >;q .ff ^
 [b  N ";\ 24`.^ 8ff .D
+4+Tqe;| q|8T
zg#:B-PlN)%m9-!-
T
2l<\

fi

P#$}

wwyXm$

oO5=-)%A=*
DF/MTCD!n)3(!-)3ID)MTjaqssFu?T/1d/N}bZ"G[-!3():3#-)#!+4;:3A7A7A!1aO/N}T]1a-6\e?3W:;
:[]
 )N 4: G>" .ff 
 Nb  N ";3 24` Fff c?.D+!+T
s 8;t8Tr-4*!571a-)%^#:39;&5C#-g:#[.57-!B[#85C&GT
od49fiA7(!;:&DcET~OTCD
nzPerA7A7A7-!*
Dcy)TT]jaqs>| iu?T Y ;^7. =.m2   )`W $ ?.3 ^Z_ 8ff   "
:H: [S
 fim ;m)em["| MWm a`!J-;d fi>( F2TzP1:D\92R!:?57*)BD)zP/MT
[AC&"#:3D\MT/2Tbjaqss;tvu?T/*!57"&":?5CR4)&"*-!*k*!;+!&5CUS*)G8-4957$AC#*kR4A7-!5=-)B3(!9fig%^#:
+;:3A7ACAd+!:#[.57-)B#%29fi*!5=!9_ZB:357-@&"I8T 1aB
- \ e?3W :;@
 P:)
 bK .:c4=.m2   )`W 
ff fi;d 	fi) :@Q"N ?D4+!+T!["q 88T
8&"-)I[57"&"D]TCDW57(!:D$/2TCDgn ,(4-)B)D$2Tjaqssu?T ';:3xd57-
:3"#):3.9-!;B9-v&%^#:
;+4+4A757;&5C#-)Z`xr;:N-)&`x#:I[T?(
T!:+T)rzSdZOFZseZ3qs8Dr;:3-)B5CVzPA7AC#-0'-!5CU:357&`GT
[)&"&-!:D,'TWjaqssu?TV[&;&57N+4;:3&5C&5C#-!57-!Bfixd5C&(g3A7I[-)T\1a	
- X; ;7. \ ?2 :qN; '   ;
2`.^ FN ?M
 D4+!+T
qt iq;t[T
~A7"U[5C:T
;:3-)A=AiDoNTbTdjaqssu?e
T MaN ^`3m[S ff;6
 :fi Q:<: [| ).: N;2 fic?~
 Sd ;^7.O ;"ff FT
(
T ET!&()5=D!-45CU:35C&G>#%.[;&d/d:?A757-)B&"#-
T

2lQl

fiJournal of Artificial Intelligence Research 9 (1998) 37-97

Submitted 2/98; published 9/98

The Divide-and-Conquer Subgoal-Ordering Algorithm
for Speeding up Logic Inference
Oleg Ledeniov
Shaul Markovitch

olleg@cs.technion.ac.il
shaulm@cs.technion.ac.il

Computer Science Department
Technion { Israel Institute of Technology
Haifa 32000, Israel

Abstract

It is common to view programs as a combination of logic and control: the logic part
defines what the program must do, the control part { how to do it. The Logic Programming paradigm was developed with the intention of separating the logic from the control.
Recently, extensive research has been conducted on automatic generation of control for
logic programs. Only a few of these works considered the issue of automatic generation of
control for improving the eciency of logic programs. In this paper we present a novel algorithm for automatic finding of lowest-cost subgoal orderings. The algorithm works using
the divide-and-conquer strategy. The given set of subgoals is partitioned into smaller sets,
based on co-occurrence of free variables. The subsets are ordered recursively and merged,
yielding a provably optimal order. We experimentally demonstrate the utility of the algorithm by testing it in several domains, and discuss the possibilities of its cooperation with
other existing methods.

1. Introduction
It is common to view programs as a combination of logic and control (Kowalski, 1979). The
logic part defines what the program must do, the control part { how to do it. Traditional
programming languages require that the programmers supply both components. The Logic
Programming paradigm was developed with the intention of separating the logic from the
control (Lloyd, 1987). The goal of the paradigm is that the programmer specifies the logic
without bothering about the control, which should be supplied by the interpreter.
Initially, most practical logic programming languages, such as Prolog (Clocksin & Mellish, 1987; Sterling & Shapiro, 1994), did not include the means for automatic generation of
control. As a result, a Prolog programmer had to implicitly define the control by the order of
clauses and of subgoals within the clauses. Recently, extensive research has been conducted
on automatic generation of control for logic programs. A major part of this research is concerned with control that affects correctness and termination of logic programs (De Schreye
& Decorte, 1994; Somogyi, Henderson, & Conway, 1996b; Cortesi, Le Charlier, & Rossi,
1997). Only a few of these works consider the issue of automatic generation of control for
improving the eciency of logic programs. Finding a good ordering that leads to ecient
execution requires a deep understanding of the logic inference mechanism. Hence, in many
cases, only expert programmers are able to generate ecient programs. The problem intensifies with the recent development of the field of inductive logic programming (Muggleton
c 1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiLedeniov & Markovitch

& De Raedt, 1994). There, logic programs are automatically induced by learning. Such
learning algorithms are commonly built with the aim of speeding up the induction process
without considering the eciency of resulting programs.
The goal of the research described in this paper is to design algorithms that automatically find ecient orderings of subgoal sequences. Several researchers have explored the
problem of automatic reordering of subgoals in logic programs (Warren, 1981; Naish, 1985b;
Smith & Genesereth, 1985; Natarajan, 1987; Markovitch & Scott, 1989). The general subgoal ordering problem is known to be NP-hard (Ullman, 1982; Ullman & Vardi, 1988).
Smith and Genesereth (1985) and Markovitch and Scott (1989) present search algorithms
for finding optimal orderings. These algorithms are general and carry exponential costs for
non-trivial sets of subgoals. Natarajan (1987) describes an ecient algorithm for the special
case where subgoals in the set do not share free variables.
In this paper we present a novel algorithm for subgoal ordering. We call two subgoals
that share a free variable dependent. Unlike Natarajan's approach, which can only handle
subgoal sets that are completely independent, our algorithm can deal with any subgoal
set, while making maximal use of the existing dependencies for acceleration of the ordering
process. In the worst case the algorithm { like that of Smith and Genesereth { is exponential.
Still, in most practical cases, our algorithm exploits subgoal dependencies and finds optimal
orderings in polynomial time.
We start with an analysis of the ordering problem and demonstrate its importance
through examples. We then show how to compute the cost of a given ordering based on
the cost and the number of solutions of the individual subgoals. We describe the algorithm
of Natarajan and the algorithm of Smith and Genesereth and show how the two can be
combined into an algorithm that is more ecient and general than each of the two. We
show drawbacks of the combined algorithm and introduce the new algorithm, which avoids
these drawbacks. We call it the Divide-and-Conquer algorithm (dac algorithm). We prove
the correctness of the algorithm, discuss its complexity and compare it to the combined
algorithm. The dac algorithm assumes knowledge of the cost and the number of solutions
of the subgoals. This knowledge can be obtained by machine learning techniques such as
those employed by Markovitch and Scott (1989). Finally, we test the utility of our algorithm
by running a set of experiments on artificial and real domains.
The dac algorithm for subgoal ordering can be combined with many existing methods
in logic programming, such as program transformation, compilation, termination control,
correctness verification, and others. We discuss the possibilities of such combinations in the
concluding section.
Section 2 states the ordering problem. Section 3 describes existing ordering algorithms
and their combination. Section 4 presents the new algorithm. Section 5 discusses the
acquisition of the control knowledge. Section 6 contains experimental results. Section 7
contains a discussion of practical issues, comparison with other works and conclusions.

2. Background: Automatic Ordering of Subgoals
We start by describing the conventions and assumptions accepted in this paper. Then we
demonstrate the importance of subgoal ordering and discuss its validity. Finally, we present
a classification of ordering methods and discuss related work.
38

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

2.1 Conventions and Assumptions

All constant, function and predicate symbols in programs begin with lower case letters,
while capital letters are reserved for variables. Braces are used to denote unordered sets
(e.g., fa; b; cg), and angle brackets are used for ordered sequences (e.g., ha; b; ci). Parallel
lines (k) denote concatenations of ordered sequences of subgoals. When speaking about
abstract subgoals (and not named predicates of concrete programs), we denote separate
subgoals by capital letters (A; B : : :), ordered sequences of subgoals by capitalized vectors
~ O~ S : : :), and sets of subgoals by calligraphic capitals (B; S : : :).  (S ) denotes the set of
(B;
all permutations of S .
We assume that the programs we work with are written in pure Prolog, i.e., without cut
operators, meta-logical or extra-logical predicates. Alternatively, we can assume that only
pure Prolog sub-sequences of subgoals are subject to ordering. For example, given a rule of
the form
A B1 ; B2; B3; !; B4; B5; B6:
only its final part fB4 ; B5; B6g can be ordered (without affecting the solution set).
In this work we focus upon the task of finding all the solutions to a set of subgoals.

2.2 Ordering of Subgoals in Logic Programs
A logic program is a set of clauses:

A

B1 ; B2 ; : : :; Bn :

(n  0)

where A; B1 ; : : :; Bn are literals (predicates with arguments). To use such a clause for
proving a goal that matches A, we must prove that all B -s hold simultaneously, under
consistent bindings of the free variables. A solution is such a set of variable bindings. The
solution set of a goal is the bag of all its solutions created by its program.
A computation rule defines which subgoal will be proved next. In Prolog, the computation rule always selects the leftmost subgoal in a goal. If a subgoal fails, backtracking is
performed { the proof of the previous subgoal is re-entered to generate another solution.
For a detailed definition of the logic inference process, see Lloyd (1987).

Theorem 1 The solution set of a set of subgoals does not depend on the order of their

execution.

Proof: When we are looking for all solutions, the solution set does not depend on the

computation rule chosen (Theorems 9.2 and 10.3 in Lloyd, 1987). Since a transposition of
subgoals in an ordered sequence can be regarded as a change of the computation rule (the
subgoals are selected in different order), such transposition does not change the solution
set.
2
This theorem implies that we may reorder subgoals during the proof derivation. Yet the
eciency of the derivation strongly depends on the chosen order of subgoals. The following
example illustrates how two different orders can lead to a large difference in execution
eciency.
39

fiLedeniov & Markovitch

parent(abraham,isaac).
parent(sarah,isaac).
parent(abraham,ishmael).
parent(isaac,esav).
parent(isaac,jakov).

... More parent clauses ...

male(abraham).
male(isaac).
male(ishmael).
male(jakov).
male(esav).

... More male clauses ...

brother(X,Y)
male(X), parent(W,X), parent(W,Y), X=/=Y.
father(X,Y)
male(X), parent(X,Y).
uncle(X,Y)
parent(Z,Y), brother(X,Z).

... More rules of relations ...

Figure 1: A small fragment of a Biblical database describing family relationships.

Example 1
Consider a Biblical family database such as the one listed in Figure 1 (a similar database
appears in the book by Sterling & Shapiro, 1994). The body of the rule defining the
uncle-nephew (or uncle-niece) relation can be ordered in two ways:
1. uncle(X,Y) brother(X,Z), parent(Z,Y).
2. uncle(X,Y) parent(Z,Y), brother(X,Z).
To prove the goal uncle(ishmael,Y) using the first version of the rule, the interpreter will
first look for Ishmael's siblings (and find Isaac) and then for the siblings' children (Esav
and Jacov). The left part of Figure 2 shows the associated proof tree with a total of 10
nodes. If we use the second version of the rule, the interpreter will create all the parentchild pairs available in the database, and will test for each parent whether he (or she) is
Ishmael's sibling. The right part of Figure 2 shows the associated proof tree with a total
of 4(N , 2) + 6  2 + 2 = 4N + 6 nodes, where N is the number of parent-child pairs in the
database. The tree contains two success branches and N , 2 failure branches; in the figure
we show one example of each. While the two versions of the rule yield identical solution
sets, the first version leads to a much smaller tree and to a faster execution.
Note that this result is true only for the given mode (bound,free) of the head literal;
for the mode (free,bound), as in uncle(X,jacov), the outcome is the contrary: the second
version of the rule yields a smaller tree.

2.3 Categories of Subgoal Ordering Methods

Assume that the current conjunctive goal (the current resolvent) is fA1; A2g. Assume that
we use the rule \A1 A11; A12:" to reduce A1 . According to Theorem 1, the produced
resolvent, fA11 ; A12; A2g, can be executed in any order. We call ordering methods that
allow any permutation of the resolvent interleaving ordering methods, since they permit
40

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

uncle(X,Y)

brother(X,Z), parent(Z,Y). uncle(X,Y)

uncle(ishmael,Y)

uncle(ishmael,Y)

parent(Z,Y), brother(ishmael,Z)

brother(ishmael,Z), parent(Z,Y)

Z=adam,
Y=cain

male(ishmael), parent(W,ishmael), parent(W,Z),
ishmael =/= Z, parent(Z,Y)

brother(ishmael,adam)
parent(W,ishmael), parent(W,Z),
ishmael =/= Z, parent(Z,Y)
W=abraham
parent(abraham,Z), ishmael=/=Z, parent(Z,Y)
Z=ishmael
Z=isaac
ishmael =/= ishmael,
parent(ishmael,Y)

isaac =/= ishmael,
parent(isaac,Y)

parent(Z,Y), brother(X,Z).

Z=isaac,
Y=jacov

other
parent-child
pairs

male(ishmael), parent(W,ishmael),
parent(W,adam), ishmael =/= adam
parent(W,ishmael), parent(W,adam),
ishmael =/= adam
W=abraham
parent(abraham,adam), ishmael =/=adam

parent(isaac,Y)
Y=esav
Y=jacov

brother(ishmael,isaac)

male(ishmael), parent(W,ishmael),
parent(W,isaac), ishmael=/=isaac
parent(W,ishmael), parent(W,isaac),
ishmael =/= isaac
W=abraham
parent(abraham,isaac), ishmael=/=isaac
ishmael =/= isaac

Figure 2: Two proof trees obtained with different orderings of a single rule in Example 1.
interleaving of subgoals from different rule bodies. When ordering is performed only on
rule bodies before using them for reduction, the method is non-interleaving. In the above
example, interleaving methods will consider all 6 permutations of the resolvent, while noninterleaving methods will consider only two orderings: hA11 ; A12; A2i and hA12; A11; A2i.
Interleaving ordering methods deal with significantly more possible orderings than noninterleaving methods. That means that they can find more ecient orderings. On the
other hand, the space of possible orderings may become prohibitively large, requiring too
many computational resources.
Subgoal ordering can take place at various stages of the proof process. We divide all
subgoal ordering methods into static, semi-dynamic and dynamic.

 Static ordering: The rule bodies are ordered before the execution starts. No ordering takes place during the execution.

 Semi-dynamic ordering: Whenever a rule is selected for reduction, its body is
ordered. The order of its subgoals does not change after the reduction takes place.

 Dynamic ordering: The ordering decision is made at each inference step.
Static methods add no overhead to the execution time. However, the optimal ordering
of a rule often depends on a particular binding of a variable, which can be known only at
run-time. For instance, in Example 1 we saw that the first ordering of the rule is better
for proving the goal uncle(ishmael,Y). And yet, for the goal uncle(X,jacov), it is the
second ordering that yields more ecient execution. To handle such cases statically, we
must compute the optimal ordering for each possible binding.
41

fiLedeniov & Markovitch

Obviously, static ordering can only be non-interleaving. The dynamic method is more
exible, since it can use more updated knowledge about variable bindings, but it also carries
the largest runtime overhead, since it is invoked several times for each use of a rule body.
The semi-dynamic method is a compromise between the two: it is more powerful than the
static method, because it can dynamically propose different orderings for different instances
of the same rule; it also carries less overhead than the dynamic method, because it is invoked
only once for each use of a rule body.
The total time of proving a goal is the sum of the ordering time and the inference time.
Interleaving and dynamic methods have the best potential for reducing the inference time,
but may significantly augment the ordering time. Static methods do not devote time to
ordering (it is done off-line), but have a limited potential for reducing the inference time.
The algorithms described in this paper can be used for all categories of ordering methods,
although in the experiments described in Section 6 we have only implemented semi-dynamic,
non-interleaving ordering methods: on each reduction, the rule body is ordered and added
to the left end of the resolvent, and then the leftmost literal of the resolvent is selected for
the next reduction step.

2.4 Related Work

The problem of computational ineciency of logic inference was the subject of extensive
research. The most obvious aspect of this ineciency is the possible non-termination of
a proof. Several researchers developed compile-time and run-time techniques to detect
and avoid infinite computations (De Schreye & Decorte, 1994). A certain success was
achieved in providing more advanced control through employment of co-routining for interpredicate synchronization purposes (Clark & McCabe, 1979; Porto, 1984; Naish, 1984).
Also, infinite computations can be avoided by pruning infinite branches that do not contain
solutions (Vasak & Potter, 1985; Smith, Genesereth, & Ginsberg, 1986; Bol, Apt, & Klop,
1991). In the NAIL! system (Morris, 1988) subgoals are automatically reordered to avoid
nontermination.
Still, even when the proof is finite, it is desirable to make it more ecient. Several
researchers studied the problem of clause ordering (Smith, 1989; Cohen, 1990; Etzioni,
1991; Laird, 1992; Mooney & Zelle, 1993; Greiner & Orponen, 1996). If we are looking for
all the solutions of a goal, then the eciency does not depend on the clause order (assuming
no cuts). Indeed, if some predicate has m clauses, and for some argument bindings these
clauses produce all their solutions in times t1 ; t2 : : :tm , then all solutions of the predicate
under these bindings are obtained in time t1 + t2 + : : : + tm , regardless of the order in
which the clauses are applied. Different clause orderings correspond to different orders in
which branches are selected in a proof tree; if we traverse the entire tree, then the number
of traversal steps does not depend on the order of branch selection, though the order of
solutions found does depend on it.
Subgoal ordering, as was demonstrated in Example 1, can significantly affect the eciency of proving a goal. There are two major approaches to subgoal ordering. The first
approach uses various heuristics to order subgoals, for example:

 Choose a subgoal whose predicate has the smallest number of matching clauses (Minker,
1978).

42

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

 Prefer a subgoal with more constants (Minker, 1978).
 Choose a subgoal with the largest size, where the size is defined as the number of
occurrences of predicate symbols, function symbols, and variables (Nie & Plaisted,
1990).

 Choose a subgoal with the largest mass, where the mass of a subgoal depends on
the frequency of its arguments and sub-arguments in the entire goal (Nie & Plaisted,
1990).

 Choose a subgoal with the least number of solutions (Warren, 1981; Nie & Plaisted,
1990).

 Apply \tests" before \generators" (Naish, 1985a).
 Prefer calls that fail quickly (Naish, 1985b).
The heuristic methods usually execute quickly, but may yield suboptimal orderings.
The second approach, which is adopted in this paper, aims at finding optimal orderings (Smith & Genesereth, 1985; Natarajan, 1987; Markovitch & Scott, 1989). Natarajan
proposed an ecient way to order a special sort of subgoal set (where all subgoals are independent), while Smith and Genesereth proposed a general, but inecient algorithm. In
the following section we build a unifying framework for dealing with subgoal ordering and
describe variations on Natarajan's and Smith and Genesereth's algorithms. We also show
how the two can be combined for increased eciency.

3. Algorithms for Subgoal Ordering in Logic Programs
The goal of the work presented here is to order subgoals for speeding up logic programs. This
section starts with an analysis of the cost of executing a sequence of subgoals. The resulting
formula is the basis for the subsequent ordering algorithms. Then we discuss dependence
of subgoals and present existing ordering algorithms for independent and dependent sets of
subgoals. Finally, we combine these algorithms into a more general and ecient one.

3.1 The Cost of Executing a Sequence of Subgoals

In this subsection we analyze the cost of executing a sequence of subgoals. The analysis
builds mainly on the work of Smith and Genesereth (1985).
Let S = fA1; A2; : : :Ak g be a set of subgoals and b be a binding. We denote Sols(S ) to
be the solution set of S , and define Sols(;) = f;g. We denote Ai jb to be Ai whose variables
are bound according to b (Ai j; = Ai ). Finally, we denote Cost(Ai jb ) to be the amount of
resources needed for proving Ai jb . Cost(Ai jb ) should reect the time complexity of proving
Ai under binding b. For example, the number of unification steps is a natural measure of
complexity for logic programs (Itai & Makowsky, 1987).
To obtain the cost of finding all the solutions of an ordered sequence of subgoals

S~ = hA1; A2; A3; : : :Ani;
43

(1)

fiLedeniov & Markovitch

we note that the proof-tree of A1 is traversed only once, the tree of A2 is traversed once
for each solution generated by A1 , the tree of A3 { once for each solution of fA1; A2g, etc.
Consequently, the total cost of proving Equation 1 is

Cost(hA1; : : :An i) = Cost(A1) +
=

X

Cost(A2jb) + : : : +

b2Sols(fA1 g)

n
X

X

Cost(An jb ) =

b2Sols(fA1 ;:::An,1 g)

X

Cost(Aijb):
i=1 b2Sols(fA1 ;:::Ai,1 g)

(2)

To compute Equation 2 one must know the cost and the solution set for each subgoal
under each binding. To reduce the amount of information needed, we derive an equivalent
formula, which uses average cost and average number of solutions.

Definition: Let B be a set of subgoals, A a subgoal. Define cost(A)jB to be the average
cost of A over all solutions of B and nsols(A)jB to be its average number of solutions over
all solutions of B:
8
>
P (A); Cost(Ajb) B = ;
< Cost
B
cost(A)jB = > b2Sols
; B=
6 ;; Sols(B) =6 ;
j
Sols(B)j
: undefined;
B 6= ;; Sols(B) = ;
( )

8
>
j;
B=;
< jPSols(fAgj)Sols
(
f
A
j
g
)
j
b
B
nsols(A)jB = > b2SolsjSols
; B=
6 ;; Sols(B) 6= ;
: undefined; (B)j
B 6= ;; Sols(B) = ;
( )

From the first definition, it follows that:

X

Cost(Aijb ) = jSols(fA1; : : :Ai,1 g)j  cost(Ai)jfA ;:::Ai, g :
1

b2Sols(fA1 ;:::Ai,1 g)

1

(3)

If we apply the second definition recursively, we obtain

jSols(fA1; : : :Aig)j =
=
=

X

jSols(fAijbg)j
b2Sols(fA1 ;:::Ai,1 g)
jSols(fA1; : : :Ai,1 g)j  nsols(Ai)jfA1;:::Ai,1g
Yi
: : : = nsols(Aj )jfA1:::Aj,1 g:
j =1

(4)

Note that we defined Sols(;) = f;g; thus, these equations hold also for i = 1. Incorporation
of Equations 3 and 4 into Equation 2 yields

Cost(hA1; A2; : : :An i) =

20i,1
n
X
4@ Y
i=1

j =1

1
3
nsols(Aj )jfA :::Aj, gA  cost(Ai )jfA :::Ai, g5 :
1

44

1

1

1

(5)

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

For each subgoal Ai , its average cost is multiplied by the total number of solutions of
all the preceding subgoals. We can define average cost and number of solutions for every
continuous sub-sequence of Equation 1: 8k1 ; k2; 1  k1  k2  n,
cost(hA1; : : :Ak i)j; , cost(hA1; : : :Ak ,1 i)j;
(6)
cost(hAk ; : : :Ak i)jfA ;:::Ak , g =
20 nsols(hA1; : : :Ak ,1 i)1j;
3
1

2

2

1

1

1

1

=

k
i,1
X
4@ Y
2

i=k1

j =k1

1

nsols(Aj )jfA ;:::Aj, g A  cost(Ai )jfA ;:::Ai, g 5
1

1

1

1

k
Y
nsols(hA1; : : :Ak i)j;
nsols(hAk ; : : :Ak i)jfA ;:::Ak , g =
=
nsols(Ai)jfA ;:::Ai, g
nsols(hA1; : : :Ak ,1 i)j; i=k
1

2

2

1

1

1

1

2

1

1

(7)

1

The values of cost(Ai ) and nsols(Ai ) depend on the position of Ai in the ordered sequence. For example, assume that we want to find Abraham's sons, using the domain of
Example 1. The unordered conjunctive goal is fmale(Y),parent(abraham,Y)g. Let there
be N males in the database (two of them, Isaac and Ishmael, are Abraham's sons):
nsols(male(Y))j; = N
nsols(parent(abraham,Y))j; = 2
nsols(male(Y))jfparent(abraham,Y)g = 1 nsols(parent(abraham,Y))jfmale(Y)g = 2=N
Note that nsols(hmale(Y),parent(abraham,Y)i) = 2 = nsols(hparent(abraham,Y),male(Y)i),
exactly as Theorem 1 predicts.
Having defined the cost of a sequence of subgoals, we can now define the objective of
our ordering algorithms:

Definition: Let S be a set of subgoals. Define (S ) to be set of all permutations of
S . O~ S 2 (S ) is a minimal ordering of S (denoted Min(O~ S ; S )), if its cost according to
Equation 5 is minimal over all possible permutations of S :
Min(O~ S ; S ) () 8OS0 2  (S ) : Cost(O~ S )  Cost(OS0 ):
The total execution time is the sum of the time which is spent on ordering, and the
inference time spent by the interpreter on the ordered sequence. In this paper we focus
upon developing algorithms for minimizing the inference time. Elsewhere (Ledeniov &
Markovitch, 1998a, 1998b) we present algorithms that attempt to reduce the total execution
time.
The values of cost and number of solutions can be obtained in various ways: by exact
computation, by estimation and bounds, and by learning. Let us assume at the moment
that there exists a mechanism that returns the average cost and number of solutions of a
subgoal in time  . In Section 5 we show how this control knowledge can be obtained by
inductive learning.

3.2 Ordering of Independent Sets of Subgoals

The general subgoal ordering problem is NP-hard (Ullman & Vardi, 1988). However, there
is a special case where ordering can be performed eciently: if all the subgoals in the
45

fiLedeniov & Markovitch

given set are independent, i.e. do not share free variables. This section begins with the
definition of subgoal dependence and related concepts. We then show an ordering algorithm
for independent sets and prove its correctness.
3.2.1 Dependence of Subgoals

Definition: Let S and B be sets of subgoals (B is called the binding set of S ). A pair of
subgoals in S is directly dependent under B, if they share a free variable not bound by a
subgoal of B.
A pair of subgoals is indirectly dependent with respect to S and B if there exists a third
subgoal in S which is directly dependent on one of them under B, and dependent (directly
or indirectly) on the other one under B. A pair of subgoals of S is independent under B if
it is not dependent under B (either directly or indirectly). A subgoal is independent of S
under B if it is independent of all members of S under B.
Two subsets S1  S and S2  S are mutually independent under the binding set B if
every pair of subgoals (A1; A2), such that A1 2 S1 and A2 2 S2, is independent under B.
The entire set S is called independent under the binding set B if all its subgoal pairs
are independent under B, and is called dependent otherwise. A dependent set of subgoals
is called indivisible if all its subgoal pairs are dependent under B, and divisible otherwise.
A divisibility partition of S under B, DPart(S ; B), is a partition of S into subsets that
are mutually independent and indivisible under B, except at most one subset which contains
all the subgoals independent of S under B. It is easy to show that DPart(S ; B) is unique.
For example, let S0 = fa; b(X ); c(Y ); d(X; Y ); e(Z ); f (Z; V ); h(W )g. With respect to
S0 and an empty binding set, the pair fb(X ); d(X; Y )g is directly dependent, fb(X ); c(Y )g
is indirectly dependent and fb(X ); e(Z )g is independent. If we represent a set of subgoals

as a graph, where subgoals are vertices and directly dependent subgoals are connected by
edges, then dependence is equivalent to connectivity and indivisible subsets are equivalent
to connected components of size greater than 1. The divisibility partition is the partition
of a graph into connected components, with all the \lonely" vertices collected together, in
a special component. Figure 3 shows an example of such a graph for the set S0 and for
an empty binding set. The whole set is divisible into four mutually independent subsets.
The subsets fe(Z ); f (Z; V )g and fb(X ); c(Y ); d(X; Y )g are indivisible. Elements of the
divisibility partition DPart(S0 ; ;) are shown by dotted lines.
If a subgoal is independent of the set, then its average cost and number of solutions do
not depend on its position within the ordered sequence:
P
Cost(Ajb) jSols(B)j  Cost(A)
=
= Cost(A);
cost(A)jB = b2Sols(B)
jSols(B)j
jSols(B)j

P

b2Sols(B) jSols(fAjbg)j

= jSols(B)j  jSols(fAg)j = jSols(fAg)j:
jSols(B)j
In this case we can omit the binding information and write cost(Ai ) instead of cost(Ai)jfA :::Ai, g ,
and nsols(Ai ) instead of nsols(Ai )jfA :::Ai, g.
In practice, program rule bodies rarely feature independent sets of literals. An example
is the following clause, which states that children like candy:
nsols(A)jB =

jSols(B)j

1

1

1

46

1

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

fa,

b(X), c(Y), d(X,Y), e(Z), f(Z,V), h(W)

a
h(W)

b(X)

e(Z)
f(Z,V)

g)

c(Y)

d(X,Y)

Figure 3: An example of a graph representing a set of subgoals. Directly dependent subgoals

are connected by edges. Independent subgoals and indivisible subsets are equivalent to
connected components (surrounded by dashed lines). The divisibility partition (under
the empty binding set) is shown by dotted lines.
likes(X,Y)

child(X), candy(Y).

More often, independent rule bodies appear not because they are written as such in the
program text, but because some variables are bound in (initially dependent) rule bodies, as
a result of clause head unification. For example, if the rule
father(X,Y)

male(X), parent(X,Y).

is used to reduce father(abraham,W), then X is bound to abraham, and the rule body
becomes independent. Rule bodies often become independent after substitutions are performed in the course of the inference process.
3.2.2 Algorithm for Ordering Independent Sets by Sorting

Let S~ be an ordered sub-sequence of subgoals, B a set of subgoals. We denote
~
cn(S~ )jB = nsols(S)~jB , 1 :
cost(S )jB
The name \cn" reects the participation of cost and nsols in the definition. When the subsequence S~ is independent of other subgoals, the binding information (jB ) can be omitted.
Together, the average cost, average number of solutions, and cn value of a subgoal will be
called the control values of this subgoal.
For independent sets, there exists an ecient ordering algorithm, listed in Figure 4. The
complexity of this algorithm is O(n( + log n)): O(n   ) to obtain the control values of n
subgoals, and O(n log n) to perform the sorting (Knuth, 1973). To enable the division, we
must define the cost so that cost(Ai ) is always positive. If we define the cost as the number
of unifications performed, then always cost(Ai )  1, under a reasonable assumption that
predicates of all rule body subgoals are defined in the program. (In this case, at least one
unification is performed for each subgoal). Similar algorithms were proposed by Simon and
Kadane (1975) and Natarajan (1987).
Example 2 Let the set of independent subgoals be fp; q; rg, with the following control values:
47

fiLedeniov & Markovitch

Algorithm 1
Let S = fA1; A2; : : :An g be a set of subgoals.
(Ai ),1
Sort S using cn(Ai ) = nsols
cost(Ai ) as the key for Ai , and return the result.
Figure 4: The algorithm for ordering subgoals by sorting.
p q
r
cost 10 20
5
nsols 1 5
0:1
cn
0 0:2 ,0:18
We compute the costs of all possible orderings, using Equation 5:

Cost(hp; q; ri) = 10 + 1  20 + 1  5  5 = 55
Cost(hp; r; q i) = 10 + 1  5 + 1  0:1  20 = 17
Cost(hq; p; ri) = 20 + 5  10 + 5  1  5 = 95
Cost(hq; r; pi) = 20 + 5  5 + 5  0:1  10 = 50
Cost(hr; p; q i) = 5 + 0:1  10 + 0:1  1  20 = 8
Cost(hr; q; pi) = 5 + 0:1  20 + 0:1  5  10 = 12
The minimal ordering is hr; p; q i, and this is exactly the ordering which is found much
more quickly by Algorithm 1 for the set fp; q; rg: r has the smallest cn value, ,0:18, then
goes p with cn(p) = 0, and finally q with cn(q ) = 0:2.
Note that the sorting algorithm reects a well-known principle: The best implementations of generate-and-test programs are obtained with the tests placed as early as possible
in the rule body and the generations as late as possible (Naish, 1985a). Of course, the
cheap tests should come first, while the expensive ones should come last. If one looks at
the cn measure, one quickly realizes that tests should be put in front (because nsols < 1,
so cn < 0), while generator subgoals should move towards the end (nsols > 1, so cn > 0).
The weakness of the \test-first" principle is in the fact that not every subgoal can be easily
tagged as a test or a generator. If one subgoal has nsols < 1 and another one has nsols > 1,
then their order is obvious even without looking at the costs (because their cn values have
different signs). But if both subgoals have nsols < 1, or both have nsols > 1, then the
decision is not so simple. Sorting by cn can correctly handle all the possible cases.
3.2.3 Correctness Proof of the Sorting Algorithm for Independent Sets

We saw that Algorithm 1 found a minimal ordering in Example 2. We are now going to
prove that Algorithm 1 always finds a minimal ordering for independent sets. First we
show an important lemma which will also be used in further discussion. This lemma states
48

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

that substitution of a sub-sequence by its cheaper permutation makes the entire sequence
cheaper.

Lemma 1
Let S~ = A~ kB~ kC~ , S~ 0 = A~ kB~ 0 kC~ , where B~ and B~ 0 are permutations of one another, and A~
either is empty or has nsols(A~ ) > 0. Then

Cost(S~ ) < Cost(S~ 0 ) () cost(B~ )jA~ < cost(B~ 0 )jA~ ;
Cost(S~ ) = Cost(S~ 0 ) () cost(B~ )jA~ = cost(B~ 0 )jA~ :

Proof: If A~ and C~ are not empty,
Cost(S~ ) , Cost(S~ 0 ) = Cost(A~ kB~ kC~ ) , Cost(A~ kB~ 0 kC~ ) =


(5)
= cost(A~ )j; + nsols(A~ )j;  cost(B~ )jA~ + nsols(A~ kB~ )j;  cost(C~ )jA~ kB~ ,
 ~

cost(A)j; + nsols(A~ )j;  cost(B~ 0 )jA~ + nsols(A~ kB~ 0 )j;  cost(C~ )jA~ kB~ 0 :

By Theorem 1, B~ and B~ 0 produce the same solution sets. Hence, the third terms in the
parentheses above are equal, and





Cost(S~ ) , Cost(S~ 0) = nsols(A~ )j;  cost(B~ )jA~ , cost(B~ 0 )jA~ :
Since nsols(A~ ) > 0, the sign of Cost(S~ ) , Cost(S~ 0 ) coincides with the sign of cost(B~ )jA~ ,
cost(B~ 0 )jA~ .
If A~ or C~ is empty, the proof is similar.
2
Definition: Let S~ = A~ kB~ 1kC~ kB~ 2kD~ be an ordered sequence of subgoals (A~, C~ and D~ may
be empty sequences). With respect to S~ , the pair hB~ 1 ; B~ 2i is

 cn-ordered, if cn(B~ 1)jA~  cn(B~ 2)jA~[B~ [C~
1

 cn-inverted, if cn(B~ 1)jA~ > cn(B~ 2)jA~[B~ [C~
1

We now show that two adjacent mutually independent sequences of subgoals in a minimal
ordering must be cn-ordered.

Lemma 2
Let S~ = A~ kB~ 1 kB~ 2 kC~ , S~ 0 = A~ kB~ 2 kB~ 1 kC~ , where B~ 1 , B~ 2 are mutually independent under A~ .
Let A~ either be empty or have nsols(A~ ) > 0. Then

Cost(S~ ) < Cost(S~ 0) () cn(B~ 1)jA~ < cn(B~ 2)jA~ ;
Cost(S~ ) = Cost(S~ 0) () cn(B~ 1)jA~ = cn(B~ 2)jA~ :
49

fiLedeniov & Markovitch

Proof:
Cost(S~ ) < Cost(S~ 0) Lemma
() 1 cost(B~ 1kB~ 2)jA~ < cost(B~ 2kB~ 1)jA~
() cost(B~ 1)jA~ + nsols(B~ 1)jA~ cost(B~ 2)jA~[B~ <
cost(B~ 2 )jA~ + nsols(B~ 2 )jA~ cost(B~ 1 )jA~[B~
indep.fB~ 1 ; B~ 2g
()
cost(B~ 1 )jA~ + nsols(B~ 1 )jA~ cost(B~ 2 )jA~ <
cost(B~ 2 )jA~ + nsols(B~ 2 )jA~ cost(B~ 1 )jA~
() nsols(B~ 1)jA~  cost(B~ 2)jA~ , cost(B~ 2)jA~ <
nsols(B~ 2 )jA~  cost(B~ 1 )jA~ , cost(B~ 1 )jA~
cost(B~ i )jA~ >0 nsols(B~ 1 )jA~ , 1 nsols(B~ 2 )jA~ , 1
()
<
cost(B~ 1 )jA~
cost(B~ 2 )jA~
() cn(B~ 1)jA~ < cn(B~ 2)jA~
1
2

Cost(S~ ) = Cost(S~ 0)

()

cn(B~ 1 )jA~ = cn(B~ 2)jA~ | similar.

2

In an independent set, all subgoal pairs are independent, in particular all adjacent pairs.
So, in a minimal ordering of an independent set, all adjacent subgoal pairs must be cnordered; otherwise, the cost of the sequence can be reduced by a transposition of such pair.
This conclusion is expressed in the following theorem.

Theorem 2
Let S be an independent set. Let S~ be an ordering of S . S~ is minimal iff all the subgoals in
S~ are sorted in non-decreasing order by their cn values.

Proof:

1. Let S~ be a minimal ordering of S . If S~ contains a cn-inverted adjacent pair of subgoals,
then transposition of this pair reduces the cost of S~ (Lemma 2), contradicting the
minimality of S~ .

2. Let S~ be some ordering of S , whose subgoals are sorted in non-decreasing order by
cn. Let S~ 0 be a minimal ordering of S . According to item 1, S~ 0 is also sorted by
cn. The only possible difference between the two sequences is the internal ordering
of sub-sequences with equal cn values. The ordering of each such sub-sequence in
S~ can be transformed to the ordering of its counterpart sub-sequence in S~ 0 by a
finite number of transpositions of adjacent subgoals. By Lemma 2, transpositions of
adjacent independent subgoals with equal cn values cannot change the cost of the
sequence. Therefore, Cost(S~ ) = Cost(S~ 0), and S~ is a minimal ordering of S (since S~ 0
is minimal).
2

Corollary 1 Algorithm 1 finds a minimal ordering of an independent set of subgoals.
50

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

3.3 Ordering of Dependent Sets of Subgoals

Algorithm 1 does not guarantee finding a minimal ordering when the given set of subgoals
is dependent, as the following proposition shows.

Proposition 1 When the given set of subgoals is dependent, then:
1. The result of Algorithm 1 on it is not always defined.
2. Even when the result is defined, it is not always a minimal ordering of the set.

Proof: Both claims are proved by counter-examples.
1. We show a set of subgoals that cannot be ordered by sorting.
The program:
Control values:
a(X )j; a(X )jfb(X )g b(X )j; b(X )jfa(X )g
a(c1).
b(c1).
cost
2
2
2
2
a(c2).
b(c2).
nsols
2
1
2
1
1
1
cn
0
0
2
2

The set fa(X), b(X)g has two possible orderings, ha(X ); b(X )i and hb(X ); a(X )i.
Both orderings have minimal cost, though neither one is sorted by cn: each ordering
has cn = 12 for its first subgoal, and cn = 0 for the second one. Sorting by cn is
impossible here: when we transpose subgoals, their cn values are changed, and the
pair becomes cn-inverted again.
2. We show a set of subgoals that can be ordered by sorting, but its sorted ordering is
not minimal.
The program:
Control values:
a(X )j; a(X )jfb(X )g b(X )j; b(X )jfa(X )g
a(c1).
cost
2
2
8
2
a(c1).
2
2
1
1
nsols
b(c1).
1
1
cn
0
0
b(c2)
a(c1), a(c2).
2
2
Let the unordered set of subgoals be fa(X), b(X)g. Its ordering hb(X ); a(X )i is sorted
by cn, while ha(X ); b(X )i is not. But ha(X ); b(X )i is cheaper than hb(X ); a(X )i:
cost(ha(X ); b(X )i) = 2 + 2  2 = 6

cost(hb(X ); a(X )i) = 8 + 1  2 = 10

2
Since sorting cannot guarantee minimal ordering for dependent subgoals, we now consider alternative ordering algorithms. The simplest algorithm checks every possible permutation of the set and returns the one with the minimal cost. The listing for this algorithm
is shown in Figure 5.
This algorithm runs in O(  n!) time, where  is the time it takes to compute the control
values for one subgoal, and n is the number of subgoals.
The following observation can help to reduce the ordering time at the expense of additional space. Ordered sequences can be constructed incrementally, by adding subgoals to
51

fiLedeniov & Markovitch

Algorithm 2
For each permutation of subgoals, find its cost according to Equation 5.
Store the currently cheapest permutation and update it when a cheaper
one is found.
Finally, return the cheapest permutation.

Figure 5: The algorithm for subgoal ordering by an exhaustive check of all permutations.

Algorithm 3
Order(S )
let P0 f;g; n jSj
loop for kn= 1 tofi n
o
Pk0 nP~ kB fifi P~fi 2 Pk,1; B h2 S n P~
io
~ P~ 0 ) ) Cost(P~ )  Cost(P~ 0 )
Pk P~ 2 Pk0 fifi 8P~ 0 2 Pk0 ; permutation(P;
Return the single member of Pn .
Figure 6: The ordering algorithm which checks permutations of ordered prefixes.
the right ends of ordered prefixes. By Lemma 1, if a cheaper permutation of a prefix exists,
then this prefix cannot belong to a minimal ordering. The ordering algorithm can build
prefixes with increasing lengths, at each step adding to the right end of each prefix one of
the subgoals that do not appear in it already, and for each subset keeping only its cheapest
permutation (if several permutations have equal cost, any one of them can be chosen). The
listing for this algorithm is shown in Figure 6. At each step k, Pk0 stores the set of prefixes
from step k , 1 extended by every subgoal not appearing there already. Pk  Pk0 , and
in Pk each subset of subgoals is represented only by its cheapest permutation. Obviously,
jPk j = (nk) (one prefix is kept for every subset of S of size k). For each prefix of length k , 1,
there are n , (k , 1) possible continuations of length k. The size of Pk0 is as follows:
!
k
n!
n
jPk0 j = (k,n1)(n,(k,1)) = (n , (k ,n1))!(
k , 1)! (n,(k,1)) = k  (n , k)!(k , 1)! = k (k ):
For each prefix, we compute its cost in  time. The permutation test can be completed
in O(n) time, by using, for example, a trie structure (Aho et al., 1987), where subgoals in
prefixes are sorted lexicographically. Each step k takes O((n +  )  k  (nk )) time, and the
52

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

whole algorithm runs in
n
X

k=1

O((n +  )  k  (nk )) = O(n  (n +  ) 

n
X
n
k=1

(k )) = O(n  (n +  )  2n ):

If  = O(n), this makes O(n2  2n ).
Smith and Genesereth (1985) and Natarajan (1987) point out that in a minimal ordered
sequence every adjacent pair of subgoals must satisfy an adjacency restriction. The most
general form of such a restriction in our notation says that two adjacent subgoals Ak and
Ak+1 in a minimal ordering hA1; A2 : : :Ani must satisfy
cost(hAk ; Ak+1i)jfA :::Ak, g  cost(hAk+1 ; Ak i)jfA :::Ak, g:
1

1

1

1

(8)

The restriction follows immediately from Lemma 1. However, it can only help to find a
locally minimal ordering, i.e., an ordering that cannot be improved by transpositions of
adjacent subgoals. It is possible that all adjacent subgoal pairs satisfy Equation 8, but the
ordering is still not minimal. The following example illustrates this statement.

Example 3 Let the unordered set be fp(X ); q(X ); r(X )g, where the predicates are defined
by the following program:

p(c1):
q (c1):
r(c1):
p(c2) f: q (c2):
r(c1):
q (c3) f:
f fails after 50 unifications.
The ordering hp(X ); q (X ); r(X )i satisfies the adjacency restriction (Equation 8):
cost(q (X ); r(X ))jp(X ) = 5
cost(p(X ); q (X ))j; = 55
cost(q (X ); p(X ))j; = 107
cost(r(X ); q (X ))jp(X ) = 8
But it is not minimal:

cost(hp(X ); q(X ); r(X )i) = 57
cost(hr(X ); p(X ); q (X )i) = 12

To find a globally minimal ordering, it seems beneficial to combine the prefix algorithm
with the adjacency restriction: if a prefix does not satisfy the adjacency restriction, then
there is a cheaper permutation of this prefix. The adjacency test can be performed faster
than the permutation test, since it must only consider the two last subgoals of each prefix. Nevertheless, the number of prefixes remaining after each step of Algorithm 3 is not
reduced: if a prefix is rejected due to a violation of the adjacency restriction, it would have
also been rejected by the permutation test. Furthermore, if the adjacency restriction test
does not fail, we should still perform the permutation test to avoid local minima (as in
Example 3). The adjacency test succeeds in at least half of the cases: if we examine a
prefix hA1 ; : : :Ak ; B1; B2 i, we shall also examine hA1; : : :Ak ; B2 ; B1i, and the adjacency test
cannot fail in both. Consequently, addition of the adjacency test can only halve the total
running time of the ordering algorithm, leaving it O(n2  2n ) in the worst case.
53

fiLedeniov & Markovitch

Smith and Genesereth propose performing a best-first search in the space of ordered
prefixes, preferring prefixes with lower cost. The best-first search can be combined with
the permutation test and the adjacency restriction. In addition, when the subgoals not
in a prefix are independent under its binding, they can be sorted, and the sorted result
concatenated to the prefix. By Lemma 1 and Corollary 1, this produces the cheapest
completion of this prefix. When we perform completion, there is no need to perform the
adjacency or permutation test: if a complete sequence is not minimal, it will never be chosen
as the cheapest prefix; even if it is added to the list of prefixes, it will never be extracted
therefrom. The resulting algorithm is shown in Figure 7.

Algorithm 4
Order(S )

let prefix-list ;, prefix ;, rest S
loop until empty(rest)
if Independent(restjprefix)
then
let completion prefixkSort-by-cn(restjprefix)
Insert-By-Cost(completion, prefix-list)
else
loop for subgoal 2 rest
let extension prefixksubgoal
if Adjacency-Restriction-Test(extension)
and Permutation-Test(extension)
then
Insert-By-Cost(extension, prefix-list)
prefix Cheapest(prefix-list)
Remove-from-list(prefix, prefix-list)
rest Snprefix
Return prefix

Figure 7: An algorithm for subgoal ordering, incorporating the ideas of earlier researchers.
The advantage of using best-first search is that it avoids expanding prefixes whose cost
is higher than the cost of the minimal ordering. The policy used by the algorithm may,
however, be suboptimal or even harmful. It often happens that the best completion of a
cheaper prefix is much more expensive than the best completion of a more expensive prefix.
When the number of solutions is large, it is better to place subgoals with high costs closer
to the beginning of the ordering to reduce the number of times that their cost is multiplied.
For example, let the set be fa(X ); b(X )g, with cost(a(X )) = 10, cost(b(X )) = nsols(a(X ))
= nsols(b(X )) = 2. Then a minimal ordering starts with the most expensive prefix:
Cost(ha(X ); b(X )i) = 10 + 2  2 = 14
54

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Cost(hb(X ); a(X )i) = 2 + 2  10 = 22
If there are many prefixes whose cost is higher than the cost of the minimal ordering, then
best-first search saves time. But if the number of such prefixes is small, using best-first
search can increase the total time, due to the need to perform insertion of a prefix into a
priority queue, according to its cost.
A sample run of Algorithm 4 will be shown later (in Section 4.7).

4. The Divide-and-Conquer Subgoal Ordering Algorithm

Algorithm 1 presented in Section 3.2 is very ecient, but is applicable only when the entire
set of subgoals is independent. Algorithm 3 can handle a dependent set of subgoals but is
very inecient. Algorithm 4, a combination of the two, can exploit independence of subgoals for better eciency. However, the obtained benefit is quite limited. In this section,
we present the Divide-and-Conquer (dac) algorithm, which is able to exploit subgoal independence in a more elaborate way. The algorithm divides the set of subgoals into smaller
subsets, orders these subsets recursively and combines the results.

4.1 Divisibility Trees of Subgoal Sets

In this subsection we define a structure that represents all the ways of breaking a subgoal
set into independent parts. Our algorithm will work by traversing this structure.
Definition: Let S and B be sets of subgoals. The divisibility tree of S under B, DTree(S ; B),
is an AND-OR tree defined as follows:
8 leaf(S ; B)
, S is independent under B
>
>

>
<
DTree(S ; B) = > OR(S ; B; fDTree(S n fBi g; B [ fBi g) j Bi 2 Sg) , S is indivisible under B
>
: AND(S ; B; fDTree(Si; B) j Si 2 DPart(S ; B)g) , S is divisible under B

Each node N in the tree DTree(S0; B0) has an associated set of subgoals S (N )  S0 and
an associated binding set B(N )  B0. For the root node, S (N ) = S0, B(N ) = B0 . If the
binding set of the root is not specified explicitly, we assume it to be empty. For AND-nodes
and OR-nodes we also define the sets of children.

 If S (N ) is independent under B(N ), then N is a leaf.
 If S (N ) is indivisible under B(N ), then N is an OR-node. Each subgoal Bi in S (N )
defines a child node whose set of subgoals is S (N ) n fBi g and the binding set is
B(N ) [ fBi g. We call Bi the binder of the generated child. Note that the binding
set of every node in a divisibility tree is the union of the binders of all its indivisible
ancestors and of the root's binding set.

 If S (N ) is divisible under B(N ), then N is an AND-node. Each subset Si in the
divisibility partition DPart(S (N ); B(N )) defines a child node with associated set of
subgoals Si and binding set B(N ). Divisibility partition was defined in Section 3.2.1.
55

fiLedeniov & Markovitch

= {a, b, c(X), d(X), e(X)}
n1 S(n1)
B(n1) = O

S(n2) = {a, b}
B(n2) = O

n2

S(n3) = {c(X), d(X), e(X)}

n3 B(n3) = O

S(n4) = {d(X), e(X)}
S(n6) = {c(X), d(X)}
n5
n6 B(n6) = {e(X)}
B(n4) = {c(X)} n4
S(n5) = {c(X), e(X)}
B(n5) = {d(X)}

Figure 8: The divisibility tree of fa; b; c(X ); d(X ); e(X )g under empty initial binding set. The set
associated with node n1 is divisible, and is represented by an AND-node. Its children
correspond to its divisibility subsets { one independent, S (n2) = fa; bg, and one indivisible, S (n3) = fc(X ); d(X ); e(X )g. n3 is an OR-node, whose children correspond to its
three subgoals (each subgoal serves as a binder in one of the children). The sets S (n2),
S (n4), S (n5) and S (n6) are independent under their respective binding sets, and their
nodes are leaves. Here we assumed that the subgoals c(X ), d(X ) and e(X ) bind X as a
result of their proof.

It is easy to show that the divisibility tree of a set of subgoals is unique up to the order of
children of each node. Figure 8 shows the divisibility tree of the set fa; b; c(X ); d(X ); e(X )g
under empty initial binding set. The associated sets and binding sets are written next to
the nodes.
The following lemma expresses an important property of divisibility trees: subgoals of
each node are independent of the rest of subgoals under the binding set of the node.

Lemma 3 Let S0 be a set of subgoals. Then for every node N in DTree(S0; ;), for every
subgoal A 2 S (N ), and for every subgoal Y 2 S0 n (S (N ) [B(N )), A and Y are independent
under B(N ).
Proof: by induction on the depth of N in the divisibility tree.
Inductive base: N is the root node, S0 n S (N ) is empty, and no such Y exists.
Inductive hypothesis: The lemma holds for M , the parent node of N .
Inductive step: Let A 2 S (N ), Y 2 S0 n (S (N ) [ B(N )). A 2 S (M ), and for M the
lemma holds, thus either A and Y are independent under B(M ), or Y 2 S (M ).
If A and Y are independent under B(M ), then they are also independent under B(N ),
since B(M )  B(N ). Otherwise, A and Y are dependent under B(M ), and Y 2 S (M ).
56

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

 If M is an AND-node, and A and Y are dependent under B(M ), then A and
Y belong to the same element of DPart(S (M ); B(M )), and Y 2 S (N ) { a

contradiction.
 If M is an OR-node and Y 2 S (M ) n S (N ), then Y must be the binder of N .
But then B(N ) = B(M ) [ fY g and Y 2 B(N ) { a contradiction again.
2
The lemma relates to subgoal independence inside divisibility trees. We shall sometimes
need to argue about independence inside ordered sequences of subgoals. The following
corollary provides the necessary connecting link.

Corollary 2 Let S0 be a set of subgoals, N be a node in the divisibility tree of S0, S~ an
ordering of S0, S~ = S~1 kS~2, where B(N )  S~1 and S (N )  S~2 . Then S (N ) is mutually
independent of S~2 n S (N ) under S~1.
Proof: Let A 2 S (N ), Y 2 S~2 n S (N ). A and Y are independent under B(N ), by the
preceding lemma. Since B(N )  S~1 , A and Y are independent under S~1. Every subgoal of
S (N ) is independent of every subgoal of S~2 nS (N ) under S~1; therefore, S (N ) and S~2 nS (N )
are mutually independent under S~1 .

2

4.2 Valid Orderings in Divisibility Trees

The aim of our ordering algorithm is to find a minimal ordering of a given set of subgoals.
We construct orderings following a divide-and-conquer policy: larger sets are split into
smaller ones, and orderings of the smaller sets are combined to produce an ordering of the
larger set. To implement this policy, we perform a post-order traversal of the divisibility
tree corresponding to the given set of subgoals under an empty initial binding set. When
orderings of child nodes are combined to produce an ordering of the parent node, the inner
order of their subgoals is not changed: smaller orderings are consistent with larger orderings.

Definition: Let S and G  S be sets of subgoals. An ordering O~ G of G and an ordering
O~ S of S are consistent (denoted Cons(O~ G; O~ S )), if the order of subgoals of G in O~ G and in
O~ S is the same.

The divide-and-conquer process described above seems analogous to Merge Sort (Knuth,
1973). There, the set of numbers is split into two (or more) subsets, each subset is independently ordered to a sequence consistent with the global order, and these sequences are
merged. Is it possible to use a similar method for subgoal ordering? Assume that a set
of subgoals is partitioned into two mutually independent subsets, A and B. Can we build
an algorithm that, given A, produces its ordering consistent with a minimal ordering of
A [ B, independently of B? Unfortunately, the answer is negative. An ordering of A may
be consistent with a minimal ordering of A [ B1 but at the same time not be consistent
with a minimal ordering of A [ B2 for some B1 6= B2.
For example, let A = fa1(X ); a2(X )g, B1 = fbg, B2 = fdg and the control values be as
specified in Figure 9. The single minimal ordering of A [ B1 is ha2(X ); b; a1(X )i, while the
single minimal ordering of A[B2 is hd; a1(X ); a2(X )i. There is no ordering of A consistent
with both these minimal global orderings.
57

fiLedeniov & Markovitch

The program:
a1(c1).
a1(c1).
a2(c1).
a2(c1).
a2(c2)

b
b
d.

a1(X).
d.

a1(c2).

The control values:
a1(X )j; a1(X )jfa2(X )g a2(X )j; a2(X )jfa1(X )g b d
cost
2
2
5
3
5 1
nsols
2
2
2
2
3 1

Cost(b; a1(X ); a2(X )) = 5 + 3  2 + 3  2  3 = 29
Cost(b; a2(X ); a1(X )) = 5 + 3  5 + 3  2  2 = 32
Cost(a1(X ); b; a2(X )) = 2 + 2  5 + 2  3  3 = 30
Cost(a1(X ); a2(X ); b) = 2 + 2  3 + 2  2  5 = 28
Cost(a2(X ); b; a1(X )) = 5 + 2  5 + 2  3  2 = 27
Cost(a2(X ); a1(X ); b) = 5 + 2  2 + 2  2  5 = 29

Cost(d; a1(X ); a2(X )) = 1 + 1  2 + 1  2  3 = 9
Cost(d; a2(X ); a1(X )) = 1 + 1  5 + 1  2  2 = 10
Cost(a1(X ); d; a2(X )) = 2 + 2  1 + 2  1  3 = 10
Cost(a1(X ); a2(X ); d) = 2 + 2  3 + 2  2  1 = 12
Cost(a2(X ); d; a1(X )) = 5 + 2  1 + 2  1  2 = 11
Cost(a2(X ); a1(X ); d) = 5 + 2  2 + 2  2  1 = 13

Figure 9: We show a small program and the control values it defines. Then we compute costs of all
permutations of the sets fb; a1(X ); a2(X )g and fd; a1(X ); a2(X )g. Different orderings of
fa1(X ); a2(X )g are consistent with minimal orderings of these sets.
Since, unlike the case of Merge Sort, we cannot always identify a single ordering of the
subset consistent with a minimal ordering of the whole set, our algorithm will deal with
sets of candidate orderings. Our requirement from such a set is that it contain at least
one local ordering consistent with a global minimal ordering, if such a local ordering exists
(\local" ordering is an ordering of the set of the node, \global" ordering is an ordering of
the set of the root). Such a set will be called valid. The following definition defines valid
sets formally, together with several other concepts.
Definition: Let S0 be a set of subgoals and N be a node in the divisibility tree of S0.
Recall that  (S ) denotes the set of all permutations of S .
1. O~ S 2  (S0) is binder-consistent with O~ N 2  (S (N )) (denoted BCN (O~ N ; O~ S )), if they
are consistent, and all subgoals of B(N ) appear in O~ S before all subgoals of O~ N :
BCN (O~ N ; O~ S ) () 9O~ B 2 (B(N )) : Cons(O~ B kO~ N ; O~ S ):
O~ S 2 (S0) is binder-consistent with the node N (denoted BCN (O~ S )), if it is binderconsistent with some ordering of S (N ):
BCN (O~ S ) () 9O~ N 2 (S (N )) : BCN (O~ N ; O~ S ):
2. O~ N 2  (S (N )) is min-consistent with O~ S 2  (S0) (denoted MCN;S (O~ N ; O~ S )), if they
are binder-consistent, and O~ S is minimal:
MCN;S (O~ N ; O~ S ) () BCN (O~ N ; O~ S ) ^ Min(O~ S ; S0):
O~ N 2 (S (N )) is min-consistent (denoted MCN;S (O~ N )), if it is min-consistent with
some ordering of S0:
MCN;S (O~ N ) () 9O~ S 2 (S0) : MCN;S (O~ N ; O~ S ):
0

0

0

0

0

58

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

3. An ordering O~ N 2  (S (N )) is MC-contradicting, if it is not min-consistent:
MCCN;S (O~ N ) () :MCN;S (O~ N ):
0

0

4. Two orderings O~ 1; O~ 2 2  (S (N )) are MC-equivalent, if one of them is min-consistent
iff the other one is:
MCEN;S (O~ 1; O~ 2) () [MCN;S (O~ 1) () MCN;S (O~ 2)]:
0

0

0

5. A set of orderings CN   (S (N )) is valid, if CN contains a min-consistent ordering
(when at least one min-consistent ordering of S (N ) exists):

V alidN;S (CN ) () [9O~ N0 2  (S (N )) : MCN;S (O~ N0 )] ! [9O~ N 2 CN : MCN;S (O~ N )]:
0

0

0

An important property of valid sets is that a valid set of orderings of the root of

DTree(S0; ;) must contain a minimal ordering of S0. Indeed, in the root S (N ) = S0,
and consistency becomes identity. Also, B(N ) = ;, so that binder-consistency becomes

consistency, and min-consistency becomes minimality. Since there always exists a minimal
ordering of S0 , a valid set of orderings of the root must contain a minimal ordering of S0.

4.3 The Outline of the Divide-and-Conquer Algorithm

We propose an algorithm that is based on producing valid sets of orderings. Each node in
a divisibility tree produces a valid set for its associated set of subgoals, and passes it to
its parent node. After the valid set of the root node is found, we compare costs of all its
members, and return the cheapest one.
The set of orderings produced by the algorithm for a node N is called a candidate set
of N . Its members are called candidate orderings of N , or simply candidates. To find
a candidate set of N , we first consider the set of all possible orderings of S (N ) that are
consistent with candidates of N 's children. This set is called the consistency set of N .
Given the candidate sets of N 's children, the consistency set of N is defined uniquely. A
candidate set of N is usually not unique.
Definition: Let N be a node in a divisibility tree of S0. The consistency set of N , denoted
ConsSet(N ), and the candidate set of N , denoted CandSet(N ), are defined recursively:

 If N is a leaf, its consistency set contains all permutations of S (N ):
ConsSet(N ) = (S (N )):

 If N is an AND-node, and its child nodes are N1; N2; : : :Nk , we define the consistency
set of N as the set of all possible orderings of S (N ) consistent with candidates of
N1; N2; : : :Nk :

fi
n
o
ConsSet(N ) = O~ N 2 (S (N )) fifi 8i (1  i  k); 9O~ i 2 CandSet(Ni) : Cons(O~ i ; O~ N ) :
59

fiLedeniov & Markovitch

 If N is an OR-node, and its child node corresponding to every binder A 2 S (N ) is

NA , then the consistency set of N is obtained by adding binders as the first elements
to the candidates of the children:
fi
n
o
ConsSet(N ) = AkO~ A fifi A 2 S (N ); O~ A 2 CandSet(NA) :

 A candidate set of N is any set of orderings produced by removing MC-contradicting

and MC-equivalent orderings out of the consistency set of N , while keeping at least
one representative for each group of MC-equivalent orderings:
CandSet(N )  ConsSet(N );
~ON 2 (ConsSet(N ) n CandSet(N )) ) MCCN;S (O~ N ) _
h 0
i
9O~ N 2 CandSet(N ) : MCEN;S (O~ N ; O~ N0 ) :
0

0

(In other words, if some ordering is rejected, it is either MC-contradicting, or MCequivalent to some other ordering, which is not rejected.)
There are two kinds of orderings which can be removed from ConsSet(N ) while retaining its validity: MC-contradicting and MC-equivalent orderings. Removal of an MCcontradicting ordering cannot change the number of min-consistent orderings in the set; if
we remove an MC-equivalent ordering, then even if it is min-consistent, some other minconsistent ordering is retained in the set. If there exists a min-consistent ordering of the set
of the node, then its candidate set must contain a min-consistent ordering, and therefore
the candidate set is valid.
Note that when our algorithm treats an OR-node, the binder of each child is always
placed as the first subgoal of the produced ordering of this node. On higher levels the inner
order of subgoals in the ordering does not change (consistency is preserved). Therefore,
our algorithm can only produce binder-consistent orderings. This explains the choice of
the names \binder" and \binding set": the subgoals of B(N ) bind some common variables
of S (N ), since they stand to the left of them in any global ordering that our algorithm
produces. In particular, if S (N ) is independent under B(N ), then the subgoals of B(N )
bind all the shared free variables of S (N ).
To implement the DPart function, we can use the Union-Find data structure (Cormen,
Leiserson, & Rivest, 1991, Chapter 22), where subgoals are elements, and indivisible sets
are groups. In the beginning, every subgoal constitutes a group by itself. Whenever we
discover that two subgoals share a free variable not bound by subgoals of the binding set,
we unite their groups into one. To complete the procedure, we need a way to determine
which variables are bound by the given binding set. Section 7.1 contains a discussion of
this problem and proposes some practical solutions. Finally, we collect all the indivisible
subgoals into a separate group. These operations can be implemented in O(nff(n; n)) amortized time, where ff(n; n) is the inverse Ackermann function, which can be considered O(1)
for all values of n that can appear in realistic logic programs. Thus, the whole process of
finding the divisibility partition of n subgoals can be performed in O(n) average time.
The formal listing of the ordering algorithm discussed above is shown in Figure 10.
The algorithm does not specify explicitly how candidate sets are created from consistency sets. To complete this algorithm, we must provide the three filtering procedures
60

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Algorithm 5
Order(S0)

RootCandSet

CandidateSet(S0 ; ;)

Return the cheapest member of RootCandSet

CandidateSet(S ; B)
case (S under B)

independent:
let ConsSetN  (S )
let CandSetN ValidLeafFilter(ConsSetN )
divisible:
let fS1; S2; : : : Sk g DPart(S ; B)
loop for i = 1 to k
let Ci CandidateSet
(Si ; B)
n~
o
let ConsSetN
ON 2 (S (N )) j 8i = 1 : : :k; 9O~ i 2 Ci : Cons(O~ i; O~ N )
let CandSetN ValidANDFilter(ConsSetN ; fS1; : : : Sk g; fC1; : : : Ck g)
indivisible:
loop for A 2 S
let C (A) CandidateSet
n ~ ~ (S n foAg; B [ fAg)
0
let C (A)
ASkOA j OA 2 C (A)
0
let ConsSetN
A2S C (A)
let CandSetN ValidORFilter(ConsSetN )
Return CandSetN

Figure 10: The skeleton of the dac ordering algorithm. For each type of node in a divisibility tree,

a consistency set is created and refined through validity filters. The produced candidate
set of the root is valid; hence, its cheapest member is a minimal ordering of the given
set.

{ ValidLeafFilter , ValidANDFilter and ValidORFilter . Trivially, we can define them
all as null filters that return the sets they receive unchanged. In this case the candidate
set of every node will contain all the permutations of its subgoals, and will surely be valid.
This will, however, greatly increase the ordering time. Our intention is to reduce the sizes
of candidate sets as far as possible, while keeping them valid.
In the following two subsections we discuss the filtering procedures. Section 4.4 discusses detection of MC-contradicting orderings, and Section 4.5 discusses detection of MCequivalent orderings. Finally, in Section 4.6 we present the complete ordering algorithm,
incorporating the filters into the skeleton of Algorithm 5.
61

fiLedeniov & Markovitch

4.4 Detection of MC-Contradicting Orderings

In this subsection we show sucient conditions for an ordering to be MC-contradicting.
Such orderings can be safely discarded, leaving the set of orderings valid, but reducing its
size. The subsection is divided into three parts, one for each type of node in a divisibility
tree.
4.4.1 Detection of MC-Contradicting Orderings in Leaves

The following lemma shows that subgoals in a min-consistent ordering of a leaf node must
be sorted by cn.

Lemma 4
Let S0 be a set of subgoals, N be a leaf in the divisibility tree of S0. Let O~ N be an ordering of
S (N ). If the subgoals of O~ N are not sorted by cn under B(N ), then O~ N is MC-contradicting.
Proof: Let O~ S be any ordering of S0, binder-consistent with O~ N . We show that O~ S cannot
be a minimal ordering of S0, thus O~ N is not min-consistent.
O~ N is not sorted by cn, i.e., it contains an adjacent cn-inverted pair of subgoals hA1; A2i.

(Recall that a pair is cn-inverted if the first element has a larger cn value than the second
one { Section 3.2.3). Since O~ S is consistent with O~ N , we can write O~ S = X~ kA1 kY~ kA2kZ~ ,
where X~ , Y~ and Z~ are (possibly empty) sequences of subgoals. Since O~ S is binder-consistent
with O~ N , B(N )  X~ .
If Y~ is empty, then A1 and A2 are adjacent in O~ S . Since B(N )  X~ , A1 and A2 are
independent under X~ . Therefore, the cost of the whole ordered sequence can be reduced
by transposing A1 and A2 , according to Lemma 2 (they are adjacent, independent and
cn-inverted).
If Y~ is not empty, then no subgoal of Y~ belongs to S (N ), since otherwise it would appear
in O~ N between A1 and A2 . By Corollary 2, Y~ is mutually independent of both A1 and A2
under X~ .

 If cn(Y~ )jX~ < cn(A1)jX~ then, by Lemma 2, a transposition of Y~ with A1 produces an
ordering with lower cost.
 Otherwise, cn(Y~ )jX~  cn(A1)jX~ . Since the pair hA1; A2i is cn-inverted, cn(A1)jX~ >
cn(A2)jX~ . Hence, cn(Y~ )jX~ > cn(A2)jX~ , and transposition of Y~ with A2 reduces the
cost, by Lemma 2.
In either case, there is a way to reduce the cost of O~ S . Therefore, O~ S cannot be minimal,
and O~ N is MC-contradicting.
2
4.4.2 Detection of MC-Contradicting Orderings in AND-nodes

Every member of the consistency set of an AND-node is consistent with some combination
of candidates of its child nodes. If there are k child nodes, and for each child Ni the sizes
of subgoal and candidate sets are jS (Ni)j = ni and jCandSet(Ni)j = ci , then the total
number of possible consistent orderings is c1  c2  : : :ck  (nn+!nn +!::::::+nnk !k )! . Fortunately, most
of these orderings are MC-contradicting and can be discarded from the candidate set. The
1

2

1

62

2

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

following lemma states that it is forbidden to insert other subgoals between two cn-inverted
sub-sequences. If such insertion takes place, the ordering is MC-contradicting and can be
safely discarded.

Lemma 5
Let S0 be a set of subgoals, N a node in the divisibility tree of S0 , and O~ S an ordering of
S0, binder-consistent with an ordering O~ N of S (N ).
If O~ N contains an adjacent cn-inverted pair of sub-sequences hA~ 1 ; A~ 2i, A~ 1 and A~ 2 appear

in O~ S not mixed with other subgoals, and A~ 1 and A~ 2 are not adjacent in O~ S , then O~ S is
not minimal.

Proof: Let O~ S be such an ordering of S0, binder-consistent with O~ N :
O~ S = X~ kA~ 1kY~ kA~ 2kZ~ ;
where Y~ is not empty. No subgoal of Y~ belongs to S (N ), since otherwise it would stand
in O~ N between A~ 1 and A~ 2 . O~ S is binder-consistent with O~ N ; therefore, B(N )  X~ . By
Corollary 2, Y~ must be mutually independent of both A~ 1 and A~ 2 under X~ , and by Lemma 2
a transposition of Y~ with either A~ 1 or A~ 2 reduces the cost { exactly as in the proof of
Lemma 4.
2

If a pair of adjacent subgoals hAi ; Ai+1i is cn-inverted, then by the previous lemma any
attempt to insert subgoals inside it results in a non-minimal global ordering. Thereupon
we may join Ai and Ai+1 into a block Ai;i+1 , which can further participate in a larger block.
The formal recursive definition of a block follows. For convenience, we consider separate
subgoals to be blocks of length 1.

Definition:

1. A sub-sequence A~ of an ordered sequence of subgoals is a block if it is either a single
subgoal, or A~ = A~ 1 kA~ 2, where hA~ 1; A~ 2i is a cn-inverted pair of blocks.
2. A block is maximal (max-block) if it is not a sub-sequence of a larger block.
3. Let N be a node in a divisibility tree, M be some descendant of N , O~ N 2  (S (N ))
and O~ M 2  (S (M )) be two consistent orderings of these nodes. A block A~ of O~ M is
violated in O~ N if there are two adjacent subgoals in A~ that are not adjacent in O~ N (in
other words, alien subgoals are inserted between the subgoals of the block).
4. Let N be a node, M be its descendant, O~ N 2  (S (N )) and O~ M 2  (S (M )) be two
consistent orderings of these nodes. O~ M is called the projection of O~ N on M . We
shall usually speak about projection of an ordering on a child node.
The concept of max-block is similar to the maximal indivisible block introduced by Simon
and Kadane (1975) in the context of satisficing search. The following corollary presents the
result of Lemma 5 in a more convenient way.

Corollary 3 Let N be a node in a divisibility tree, M be one of its children, O~ N be an

ordering of N , and O~ M be the projection of O~ N on M . If O~ M contains a block that is
violated in O~ N , then O~ N is MC-contradicting.
63

fiLedeniov & Markovitch

Proof: Let A~ be the smallest block of O~ M violated in O~ N . According to the definition
of a block, A~ = A~ 1 kA~ 2 , where A~ 1 and A~ 2 are not violated in O~ N , and the pair hA~ 1; A~ 2i

is cn-inverted. Let O~ S be any ordering of the root node binder-consistent with O~ N . O~ S
violates A~ , since O~ N violates A~ . To show that O~ N is MC-contradicting, we must prove that
O~ S is not minimal.
 If A~1 and A~ 2 are not violated in O~ S , then they are not adjacent in O~ S , and O~ S is not
minimal, by Lemma 5.
 Otherwise, A~ 1 or A~2 is violated in O~ S . Without loss of generality, let it be A~1. Let A~0
be the smallest sub-block of A~ 1 violated in O~ S . According to the definition of a block,
A~ 0 = A~01 kA~ 02 , where the pair hA~ 01; A~ 02i is cn-inverted, A~ 1 and A~ 2 are not violated and
not adjacent in O~ S . By Lemma 5, O~ S is not minimal.
2
For example, if control values of subgoals are as shown in Figure 9, then ha1(X ); a2(X )i
is a block, since cn(a1(X ))j; = 2,2 1 = 12 , cn(a2(X ))jfa1(X )g = 2,3 1 = 13 . As one can see from
the figure, insertion of b or d inside this block results in a non-minimal ordering.
As was already noted above, the consistency set of an AND-node can be large. In
many of its orderings, however, blocks of projections are violated, and we can discard
these orderings as MC-contradicting. In the remaining orderings, no block of a projection
is violated, and each such ordering can be represented as a sequence of max-blocks of the
projections. In each projection, its max-blocks stand in cn-ascending order (otherwise, there
is an adjacent cn-inverted pair of blocks, and a larger block can be formed, which contradicts
their maximality). As the following lemma states, in the parent AND-node these blocks
must also be ordered by their cn values; otherwise, the ordering is MC-contradicting.
Lemma 6 If an ordering of an AND-node contains an adjacent cn-inverted pair of maxblocks of its projections on the children, then this ordering is MC-contradicting.
Proof: If these blocks are violated in the binder-consistent global ordering, the global
ordering is not minimal by Corollary 3. If the blocks are not violated, the proof is similar
to the proof of Lemma 4.
2
The two sucient conditions for detection of MC-contradicting orderings expressed in
Corollary 3 and Lemma 6 allow us to reduce the size of the candidate set significantly.
Assume, for example, that the set of our current node N is split into two mutually independent subsets whose candidates are ha1; a2i and hb1; b2i (one candidate for each child). There
are six possible orderings of S (N ), all shown in Figure 11. Assume that both ha1; a2i and
hb1; b2i are blocks, and cn(ha1; a2i)jB(N ) < cn(hb1; b2i)jB(N ). Out of six consistent orderings,
four (2{5) can be rejected due to block violation, and one of the remaining two (number 6)
puts the blocks in the wrong order. So, only one ordering (number 1) can be left in the candidate set of N . Even if neither ha1; a2i nor hb1; b2i are blocks, Lemma 6 dictates a unique
interleaving of their elements (max-blocks), assuming that cn(a1 )jB(N ) 6= cn(a2 )jB(N )[fa g
6= cn(b1)jB(N ) 6= cn(b2)jB(N )[fb g.
1

1

4.4.3 Detection of MC-Contradicting Orderings in OR-nodes

The following lemma states that if a block has a cheaper permutation, then the ordering is
MC-contradicting (and can be discarded from the candidate set).
64

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

1.

a1 a2

b1 b2

2.

a1 a2

b1 b2

3.

a1 a2

4.

b1 b2

a1 a2

b1 b2

5.

a1 a2

b1 b2

6.

a1 a2

b1 b2

Figure 11: The possible ways to combine ha1, a2 i and hb1, b2i

Lemma 7 Let N be a node in the divisibility tree of S~0, O~ N 2 (S~(N )). Let A~ be a leading
block of O~ N : O~ N = A~ kR~ . If there is a permutation of A~ , A~ 0 , such that cost(A~ 0 )jB(N ) <
cost(A~ )jB(N ), then O~ N is MC-contradicting.
Proof: Let O~ S 2 (S~0) be binder-consistent with O~ N . If A~ is violated in O~ S , O~ S cannot

be minimal (Corollary 3). Otherwise, A~ occupies a continuous segment in O~ S , and its
replacement by a cheaper permutation reduces the cost of the global ordering (Lemma 1).
Thus, O~ S cannot be minimal.
2
This check should be done only for leading blocks of OR-nodes:

 Every ordering of a leaf node that has not been rejected due to Lemma 4 must be

sorted by cn. Consequently, it contains no cn-inverted adjacent pair of subgoals, and
no block of size  2 can be formed.

 Every ordering of an AND-node that has not been rejected due to Corollary 3 or
Lemma 6 must have its blocks unbroken and in cn-ascending order. Consequently,
new blocks cannot be formed here either.

 In OR-nodes, new blocks can be formed when we add a binder as the first element of
an ordering, if the cn value of the binder is greater than that of the subsequent block.
All new blocks start from the binder, and we must perform the permutation test only
on the leading max-block of an ordering.

4.5 Detection of MC-Equivalent Orderings

In the previous subsection we presented sucient conditions for detecting MC-contradicting
orderings. In this subsection we specify sucient conditions for identifying MC-equivalent
orderings. Recall that two orderings of a node are MC-equivalent if minimal consistency
of one implies minimal consistency of the other. Finding such sucient conditions will
allow us to eliminate orderings without loss of validity of the candidate set. We start
with defining a specialization of the MC-equivalence relation: blockwise equivalence. We
then show that orderings whose max-blocks are sorted by cn are blockwise-equivalent, and
therefore MC-equivalent.
65

fiLedeniov & Markovitch

Definition: Let S0 be a set of subgoals and N be a node in the divisibility tree of S0. Let
O~ 1 and O~ 2 be two orderings of S (N ) with an equal number of max-blocks. Let O~ S be an
ordering of S0 , binder-consistent with O~ 1, where blocks of O~ 1 are not violated.
O~ S jOO~~ is the ordering obtained by replacing in O~ S every max-block of O~ 1 with a max2

block of O~ 2, while preserving the order of max-blocks (the i-th max-block of O~ 1 is replaced
by the i-th max-block of O~ 2).
O~ 1 and O~ 2 are blockwise-equivalent if the following condition holds: O~ 1 is min-consistent
with O~ S iff O~ 2 is min-consistent with O~ S jOO~~ .
As can be easily seen, if two orderings are blockwise-equivalent, then they are MCequivalent. Now we show that a transposition of adjacent, mutually independent cn-equal
max-blocks in an ordering of a node produces a blockwise-equivalent ordering. The proof
of the following lemma is found in Appendix A.
1

2
1

Lemma 8
Let S0 be a set of subgoals, N be a node in the divisibility tree of S0, O~ N = Q~ kA~ 1 kA~ 2kR~ be
an ordering of S (N ), where A~ 1 and A~ 2 are max-blocks, mutually independent and cn-equal
under the bindings of B(N ) [ Q~ . Then O~ N is blockwise-equivalent with O~ N0 = Q~ kA~ 2 kA~ 1kR~ .
Corollary 4 All sorted by cn orderings of a leaf node are blockwise-equivalent.
For example, if S (N ) = fA; B; C; Dg, cn(A)jB(N ) = 0:1, cn(B )jB(N ) = cn(C )jB(N ) = 0:3,
cn(D)jB(N ) = 0:5, then the orderings hA; B; C; Di and hA; C; B; Di are blockwise-equivalent,
and we can remove from the candidate set any one of them (but not both).

Corollary 5 All orderings of an AND-node, where blocks of projections are not violated

and adjacent max-blocks from different children projections are cn-ordered, are blockwiseequivalent.

~ B;
~ C~ ; D~ are
For example, if the candidates of the children are A~ kB~ and C~ kD~ , where A;
max-blocks, cn(A~ )jB(N ) = 0:1, cn(B~ )jB(N )[A~ = cn(C~ )jB(N ) = 0:3 and cn(D~ )jB(N )[C~ = 0:5,
then the orderings A~ kB~ kC~ kD~ and A~ kC~ kB~ kD~ are blockwise-equivalent, and we can remove
from the candidate set any one of them (but not both).
To prove both Corollaries 4 and 5, we note that in each case one of the mentioned
orderings can be obtained from the other by a finite number of transpositions of adjacent,
mutually independent and cn-equal max-blocks. According to Lemma 8, each such transposition yields a blockwise-equivalent ordering. It is easy to show that blockwise equivalence
is transitive.
The following corollary states that subgoals within a block can be permuted, provided
that the cost of the block is not changed.

Corollary 6 All orderings of a node, identical up to cost-preserving permutations of subgoals inside blocks, are blockwise-equivalent.

The proof of the corollary follows immediately from Lemma 1. For example, if the set
is fa(X ); b(X )g, and the control values are as in the first counter-example of Proposition 1,
66

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Node Set
MC-contradicting
Leaf Independent Subgoals not sorted by cn
| Lemma 4
Contains violated blocks
AND Divisible
| Corollary 3
Max-blocks not sorted by cn
| Lemma 6
The leading max-block has
OR Indivisible a cheaper permutation
| Lemma 7

blockwise-equivalent
Subgoals sorted by cn
| Corollary 4
Max-blocks not violated,
sorted by cn
| Corollary 5
Cost-preserving permutations
of blocks
| Corollary 6

Table 1: Summary of sucient conditions for detection of MC-contradicting and blockwiseequivalent orderings.

i.e. cn(a(X )j;) = cn(b(X )j;) = 12 , and cn(a(X )jfb(X )g) = cn(b(X )jfa(X )g) = 0, then in both
possible orderings, ha(X ); b(X )i and hb(X ); a(X )i, the two subgoals are united into a block,
and these blocks have equal cost. In any global ordering containing the block ha(X ); b(X )i,
we can replace this block with hb(X ); a(X )i without changing the total cost. Therefore
ha(X ); b(X )i is blockwise-equivalent to hb(X ); a(X )i.
The sucient condition expressed in Corollary 6 should be checked only in OR-nodes,
since in leaves and AND-nodes no new blocks are created, as was argued in Section 4.4.3.

4.6 The Revised Ordering Algorithm

In the two preceding subsections we saw several sucient conditions of MC-contradiction
and MC-equivalence, summarized in Table 1. These results permit us to close the gaps
in Algorithm 5 by providing the necessary validity filters. Each filter tests the sucient
conditions of MC-contradiction and MC-equivalence on every ordering in the consistency
set. If some of these sucient conditions hold, the ordering is rejected. The formal listing
of these procedures is shown in Figure 12.
While the generate-and-test approach described above served us well for methodological
purposes, it is obviously not practical because of its computational limitations. For example,
for an independent set of size n, the algorithm creates n! orderings, then rejects n! , 1
and keeps only one. This process takes O(n!  n) time and produces an ordering which
is sorted by cn. The same result could be obtained in just O(n log n) time, by a single
sorting. So, instead of uncontrolled creation of orderings and selective rejection, we want to
perform a selective creation of orderings. In other words, we want to revise our algorithm to
deal directly with candidate sets, instead of generating large consistency sets. The revised
algorithm produces the candidate set of a node N as follows:
 If N is a leaf, the subgoals of S (N ) are sorted by cn under the bindings of B(N ), and
the produced ordering is the sole candidate of N .
 If N is an AND-node, then for each combination of its children's candidates a candidate of N is created, where the max-blocks of the children's candidates are ordered
67

fiLedeniov & Markovitch

ValidLeafFilter(ConsSetN )
let CandSetN ;
loop for O~ N 2 ConsSetN

if O~ N is sorted by cn
and there is no O~ N0 2 CandSetN which is sorted by cn
then CandSetN CandSetN [ fO~ N g
Return CandSetN

ValidANDFilter(ConsSetN ; fS1; : : : Sk g; fC1; : : : Ck g)
let CandSetN ;
loop for O~ N 2 ConsSetN

loop for i = 1 to k
let O~ i be the projection of O~ N on Si
if 8i O~ i 2 Ci
and max-blocks of O~ i -s are not violated in O~ N ,
and max-blocks of O~ i -s are ordered by cn in O~ N ,
and there is no O~ N0 2 CandSetN consistent with all O~ i-s,
then CandSetN CandSetN [ fO~ N g
Return CandSetN

ValidORFilter(ConsSetN )
let CandSetN ;
loop for O~ N 2 ConsSetN

if O~ N does not start with a block having a cheaper permutation,
and there is no O~ N0 2 CandSetN , identical to O~ N up to
cost-preserving permutations in blocks,
then CandSetN CandSetN [ fO~ N g
Return CandSetN

Figure 12: The three filter procedures that convert a consistency set into a candidate set. Together
with Algorithm 5, they form a complete ordering algorithm. The eciency of the
algorithm can be improved, as we shall see in Algorithm 6.

by cn. The candidate is produced by merging: moving in parallel on the candidates
of the children and extracting max-blocks that are minimal by cn.

 If N is an OR-node, then for each candidate of its child an ordering of N is created
by adding the binder to the left end of the child candidate. If this results in creation
of a block that has a cheaper permutation, the ordering is rejected; otherwise, it is
added to the candidate set. It suces to check only the leading max-block.
68

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Note that the revised algorithm does not include a test for cost-preserving permutations
of blocks in different orderings (expressed in Corollary 6), because of the high expense of
such a test.
The revised algorithm described above contains manipulations of blocks. For this purpose, we need an easy and ecient way to detect blocks in orderings. Since we do not
permit block violation (by Corollary 3), we can unite all the subgoals of a max-block into
one entity, and treat it as an ordinary subgoal. The procedure of joining subgoals into
blocks is called folding, and the resulting sequence of max-blocks { a folded sequence. After
subgoals are folded into a block, there is no need to unfold this block back to separate
subgoals: on upper levels of the tree, these subgoals will again be joined into a block, unless
the block is violated. The unfolding operation is carried out only once before returning the
cheapest ordering of the set (of the root node). The candidate sets of the nodes are now
defined as sets of folded orderings.
As was already stated, new blocks can only be created in the candidates of OR-nodes,
when the binder is added as the first element of the ordering, if the cn value of the binder
is greater than the cn value of the first max-block of the child projection. Therefore, in the
revised algorithm we only build new blocks that start from the binder: the max-blocks in
the rest of the ordering remain from the child's candidate. First we try to make a block
out of the binder and the first max-block of the child's candidate. If they are cn-ordered,
we stop the folding. If they are cn-inverted, we unite them into a larger block, and try
to unite it with the second max-block of the child's candidate, and so on. The produced
folded ordering contains only maximal blocks: the first block is maximal, since we could not
expand it further to the right, and the other blocks are maximal, since they were maximal
in the child's candidate.
Lemma 7 states that an ordering whose leading max-block has a cheaper permutation
is MC-contradicting. One way to detect such a block is to exhaustively test all its permutations, computing and comparing their costs. This procedure is very expensive. Instead,
in our revised algorithm we employ the adjacency restriction test (Equation 8). The test is
applied to every pair of adjacent subgoals of a block, and if some adjacent pair has a cheaper
transposition, then the whole block has a cheaper permutation, by Lemma 1. Since blocks
are created by concatenation of smaller blocks, it suces to test the adjacency restriction
only at the points where blocks are joined (for other adjacent pairs of subgoals, the tests
were performed on the lower levels, when smaller blocks were formed). The adjacency restriction test does not guarantee detection of all not-cheapest permutations (as was shown
in Example 3), but it detects such blocks in many cases, and works in linear time.
The final version of the dac subgoal ordering algorithm is presented in Figure 13. The
complete correctness proof of Algorithm 6 is found in Appendix B.

4.7 Sample Run and Comparison of Ordering Algorithms
We illustrate the work of the dac algorithm, using the subgoal set shown in Figure 8,
S0 = fa; b; c(X ); d(X ); e(X )g. After proving c(X ), d(X ) or e(X ), we can assume that X is
bound. Let the control values for the subgoals be as shown in Table 2. The column c(free)
contains control values for the subgoal c(X ) when X is not yet bound by the preceding
subgoals (i.e., the binding set does not contain d(X ) or e(X )). The column c(bound)
69

fiLedeniov & Markovitch

Algorithm 6 : The Divide-and-Conquer Algorithm
Order(S0)
let RootCandSet CandidateSet(S0 ; ;)
Return Unfold(the cheapest element of RootCandSet)
CandidateSet(S ; B)
let fS1; S2; : : : Sk g DPart(S ; B)
case

 k = 1, shared-vars(S1) = ; (S is independent under B):
Return fSort-by-cn(S ; B)g
 k = 1, shared-vars(S1) =6 ; (S is indivisible under B):
loop for A 2 S
let C (A) CandidateSet
(S fin fAg; B [ fAog)
n
0
~
let C (A)
Fold
(AkOA ; B) fifi O~ A 2 C (A)
S
Return A2S C 0(A)
 k > 1 (S is divisible under B):
loop for i = 1 to k
let Cin CandidateSet(Si ; B)
Return Merge(fO~ 1; O~ 2; : : : O~ k g; B)

fifi
o
fi O~ 1 2 C1; O~ 2 2 C2; : : : O~ k 2 Ck

Merge(fO~ 1; O~ 2; : : : O~ k g; B)

let min-cn-candidate O~ i that minimizes cn(first-max-block(O~ i ))jB , 1  i  k
let min-cn-block first-max-block(min-cn-candidate)
remove-first-max-block(min-cn-candidate)
Return min-cn-blockkMerge(fO~ 1; O~ 2; : : : O~ k g; B [ min-cn-block)

Fold(hA1; A2 : : :Ak i; B)
if k  1 or cn(A1)jB  cn(A2)jBkA
then Return hA1; A2 : : :Ak i

1

else
if the last subgoal of A1 and the first subgoal of A2 satisfy the adjacency restriction
then
let A0 block(A1 ; A2)
Return Fold(hA0 ; A3 : : :Ak i; B)
else Return ;

Figure 13: The revised version of the dac algorithm. The candidate sets are built selectively,

without explicit creation of consistency sets. Candidate sets contain folded orderings,
and unfolding is performed only on the returned global ordering. The code of the
Unfold and Sort-by-cn procedures is not listed, due to its straightforwardness. The
merging procedure recursively extracts from the given folded orderings max-blocks that
are minimal by cn. The folding procedure joins two leading blocks into a larger one, as
long as they are cn-inverted.
70

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

a
b c(free) c(bound) d(free) d(bound) e(free) e(bound)
cost
10 5
5
5
10
5
20
10
nsols 0.8 2
2
0.5
4
1
0.4
0.1
cn
-0.02 0.2 0.2
-0.1
0.3
0
-0.03
-0.09
Table 2: Control values for the sample runs of the ordering algorithms.
contains cost values of c(X ) when d(X ) or e(X ) have already bound X . For example,
cost(c(X ))jfa;d(X )g = cost(c(bound)) = 5. The dac algorithm traverses the divisibility tree
of S0 as follows. (The names of the nodes are as in Figure 8.)
1. The root of the divisibility tree, n1, has empty binding set B(n1) = ;, and the
associated subgoal set S (n1) = fa; b; c(X ); d(X ); e(X )g. The set S (n1) is partitioned into two subsets under B(n1): one independent { fa; bg, and one indivisible {
fc(X ); d(X ); e(X )g. These two subsets correspond to two child nodes of the ANDnode n1: n2 and n3, both with empty binding sets.
2. S (n2) is independent under B(n2). Therefore, n2 is a leaf, and its sole candidate
ordering is obtained by sorting its subgoals by cn under B(n2). cn(a)j; = ,0:02,
cn(b)j; = 0:2, thus CandSet(n2) = fha; big.
3. S (n3) is indivisible under B(n3). Therefore, n3 is an OR-node, and its three children
are created { one for each subgoal of S (n3) serving as the binder.

 Binder c(X ) yields the child node n4 with the associated set S (n4) = fd(X ); e(X )g
and the binding set B(n4) = fc(X )g. S (n4) is independent under B(n4). There-

fore, n4 is a leaf, and its sole candidate is obtained by sorting its subgoals by
cn:
cn(d(X ))jfc(X)g = 0; cn(e(X ))jfc(X )g = ,0:09;
thus, the candidate of n4 is he(X ); d(X )i.
 Binder d(X ) yields the child node n5 with the associated set S (n5) = fc(X ); e(X )g
and the binding set B(n5) = fd(X )g. S (n5) is independent under B(n5), and its
sorting by cn produces the candidate hc(X ); e(X )i.
 Binder e(X ) yields the child node n6 with the associated set S (n6) = fc(X ); d(X )g
and the binding set B(n6) = fe(X )g. S (n6) is independent under B(n6), and its
sorting by cn produces the candidate hc(X ); d(X )i.
4. We now add each binder to its corresponding child's candidate and obtain three orderings of the OR-node n3: hc(X ); e(X ); d(X )i, hd(X ); c(X ); e(X )i, he(X ); c(X ); d(X )i.
5. We now perform folding of these orderings and check violations of the adjacency
restriction, in order to determine whether a block has a cheaper permutation.
71

fiLedeniov & Markovitch

 First, we perform the folding of hc(X ); e(X ); d(X )i. The pair hc(X ); e(X )i is
cn-inverted: cn(c(X ))j; = 0:2, cn(e(X ))jfc(X )g = ,0:09. We thus unite it into a
block. This block does not pass the adjacency restriction test (Equation 8):
cost(hc(X ); e(X )i)j; = 5 + 2  10 = 25;
cost(he(X ); c(X )i)j; = 20 + 0:4  5 = 22:

Therefore, this ordering is MC-contradicting and can be discarded.
 We perform the folding of hd(X ); c(X ); e(X )i. cn(d(X ))j; = 0:3, cn(c(X ))jfd(X )g =
,0:1, the pair is cn-inverted, and we unite it into a block. This block does not
pass the adjacency restriction test:
cost(hd(X ); c(X )i)j; = 10 + 4  5 = 30;
cost(hc(X ); d(X )i)j; = 5 + 2  5 = 15:

This ordering is rejected too, even before its folding is finished. If we continue
the folding process, we shall see that the subgoal e(X ) must also be added to this
block, since cn(hd(X ); c(X )i)j; = 4030:5,1 = 0:0333, and cn(e(X ))jhd(X );c(X )i =
,0:09.
 We perform the folding of he(X ); c(X ); d(X )i. cn(e(X ))j; = ,0:03, cn(c(X ))jfe(X)g
= ,0:1, the pair is cn-inverted, and we form a block ec(X ) = he(X ); c(X )i, which
passes the adjacency restriction test:
cost(he(X ); c(X )i)j; = 20 + 0:4  5 = 22;
cost(hc(X ); e(X )i)j; = 5 + 2  10 = 25:

We compute the control values of the new block:
cost(ec(X ))j; = 20 + 0:4  5 = 22
nsols(ec(X ))j; = 0:4  0:5 = 0:2
cn(ec(X ))j; = 0:222, 1 = ,0:0363636
cn(d(X ))jfec(X )g = 0, thus the pair hec(X ); d(X )i is cn-ordered, no more folding
is needed, and we add the folded candidate hec(X ); d(X )i to the candidate set
of n3.

6. We now perform merging of the candidate set of n2, fha; big, with the candidate set
of n3, fhec(X ); d(X )ig. In the resulting sequence max-blocks must be sorted by cn.

cn(a) = ,0:02; cn(b) = 0:2; cn(ec(X ))j; = ,0:0363636; cn(d(X ))jfec(X )g = 0:
The merged ordering, hec(X ); a; d(X ); bi, is added to the candidate set of n1.
7. We compare the costs of all candidates of n1, and output the cheapest one. In our case,
there is only one candidate, hec(X ); a; d(X ); bi. The algorithm returns this candidate
unfolded, he(X ); c(X ); a; d(X ); bi.
72

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Extension/Completion
Cost
h ai
10
hbi
5
hc(X )i
5
hd(X )i
10
he(X )i
20
hbi
hb; ai
the adjacency restriction test fails
hb; c(X )i
5 + 2  5 = 15
hb; d(X )i
5 + 2  10 = 25
hb; e(X )i
the adjacency restriction test fails
hc(X )i
hc(X ); e(X ); a; d(X ); bi
5 + 2(10 + 0:1(10 + 0:8(5 + 1  5))) = 28:6
hai
ha; bi
10 + 0:8  5 = 14
ha; c(X )i
10 + 0:8  5 = 14
ha; d(X )i
10 + 0:8  10 = 18
ha; e(X )i
the adjacency restriction test fails
hd(X )i
hd(X ); c(X ); e(X ); a; bi 10 + 4(5 + 0:5(10 + 0:1(10 + 0:8  5))) = 52:8
ha; bi
ha; b; c(X )i
14 + 0:8  2  5 = 22
ha; b; d(X )i
14 + 0:8  2  10 = 30
ha; b; e(X )i
the adjacency restriction test fails
ha; c(X )i
ha; c(X ); e(X ); d(X ); bi
14 + 0:8  2(10 + 0:1(5 + 1  5)) = 31:6
hb; c(X )i
hb; c(X ); e(X ); a; d(X )i
15 + 2  2(10 + 0:1(10 + 0:8  5)) = 60:6
ha; d(X )i
ha; d(X ); c(X ); e(X ); bi
18 + 0:8  4(5 + 0:5(10 + 0:1  5)) = 50:8
he(X )i
he(X ); c(X ); a; d(X ); bi 20 + 0:4(5 + 0:5(10 + 0:8(5 + 1  5))) = 25:6
ha; b; c(X )i
ha; b; c(X ); e(X ); d(X )i
22 + 0:8  2  2(10 + 0:1  5) = 55:6
hb; d(X )i
hb; d(X ); c(X ); e(X ); ai
25 + 2  4(5 + 0:5(10 + 0:1  10)) = 109
he(X ); c(X ); a; d(X ); bi complete ordering

;

Cheapest prefix

Table 3: A trace of a sample run of Algorithm 4 on the set of Figure 8. The left column shows the
cheapest prefix extracted from the list on each step, the middle column { its extensions
or completions that are added to the list, and the right column { their associated costs.

For comparison, we now show how the same task is performed by Algorithm 4. The
algorithm maintains a list of prefixes, sorted by their cost values, and which initially contains
an empty sequence. On each step the algorithm extracts from the list its cheapest element,
and adds to the list the extensions or completions of this prefix. Extensions are created when
the set of remaining subgoals is dependent, by appending each of the remaining subgoals
to the end of the prefix. Completions are created when the set of remaining subgoals is
independent, by sorting them and appending the entire resulting sequence to the prefix. An
extension is added to the list only when the adjacency restriction test succeeds on its two
last subgoals. To make the list operations faster, we can implement it as a heap structure
(Cormen et al., 1991).
The trace of Algorithm 4 on the set S0 is shown in Table 3. The left column shows the
cheapest prefix extracted from the list on each step, the middle column { its extensions or
completions that are added to the list, and the right column { their associated costs.
It looks as if the dac algorithm orders the given set S0 more eciently than Algorithm 4.
We can compare several discrete measurements to show this. For example, Algorithm 6
73

fiLedeniov & Markovitch

p2(X2,X5,X7,X9)
p1(X1,X2,X3,X4)
p5(X4,X8,X9,X10)
p3(X1,X5,X6,X8)
p4(X6,X3,X7,X10)

Figure 14: An example of the worst case for ordering. When all variables are initially free, every

subset of subgoals is indivisible under the binding of the rest of subgoals, and the overall
complexity of ordering by Algorithm 6 is O(n!).

performs 4 sorting sessions, each one with 2 elements, while Algorithm 4 performs 5 sortings
with 2 elements, and 3 sortings with 3 elements. The adjacency restriction is tested only 3
times by Algorithm 6, and 11 times by Algorithm 4. Algorithm 6 creates totally 8 different
ordered sub-sequences, with total length 22, while Algorithm 4 creates 24 ordered prefixes,
with total length 55.

4.8 Complexity Analysis

Both Algorithm 4 and Algorithm 6 find a minimal ordering, and both sort independent
subsets of subgoals whenever possible. Algorithm 6, however, offers several advantages due
to its divide-and-conquer strategy.
Let n be the number of subgoals in the initial set. For convenience, we assume that
the time of computing the control values for one subgoal is O(1); otherwise, if this time
is  , all the complexities below must be multiplied by  . The worst case complexity of
Algorithm 6 is O(n!). Figure 14 shows an example of such a case for n = 5. In this set
every two subgoals share a variable that does not appear in other subgoals. Thus, other
subgoals cannot bind it. The set of the root is indivisible, and no matter which binder is
chosen, the sets of the children are indivisible. So, in each child of the root, we must select
every remaining subgoal as the binder, and so on. The overall complexity of this execution
is O(n!). This is indeed the worst-case complexity: presence of AND-nodes in the tree can
only reduce it.
Note that even when n is small, such a complex rule body with (n2 ) free variables is
very improbable in practical programs. Also, the worst-case complexity can be reduced
to O(n2  2n ), if we move from divisibility trees to divisibility graphs (DAGs), where all
identical nodes of a divisibility tree (same subgoal set, same binding set) are represented
by a single vertex. The equivalence test of the tree nodes can be performed eciently with
the help of trie structures (Aho et al., 1987), where subgoals are sorted lexicographically.
Let there be n subgoals, with v shared variables appearing in m subgoals. As was
already noted in Section 4.3, the partition of subgoals into subsets can be performed in
74

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

O(n) average time, using a Union-Find data structure (Cormen et al., 1991, Chapter 22).

In the worst possible case, there are no AND-nodes in the divisibility tree, apart from the
root node (whose set is divisible into a dependent set of size m and an independent set of
size n , m). The overall complexity of the dac algorithm in such a case is
T (n; m; v ) = O(n)
| divisibility partition
+ O((nQ, m) log(n , m))
| ordering of independent subgoals
k
+ O(( i=0
ordering of dependent subgoals
Q (,m1(,mi)), i))log(m , k)) |
+ O(m Q ki=0
| folding
,1 (m , i))
+ O(n  ki=0
| merging
where k is the maximal possible number of bindings performed before the remaining subset
is independent. If we assume that every subgoal binds all its free variables (which happens
very frequently in practical logic programs), then k = minfv; m , 1g; otherwise k = m , 1.
k is equal to the maximal number of OR-nodes on a path from the root to a leaf of the
divisibility tree. Therefore, the height of the divisibility tree is limited by k + 1. Actually,
the tree can be shallower, since some binders can bind more than one shared variable each.
This means that the number of shared variables can decrease by more than 1 in each ORnode. Below we simplify the above formula for several common cases, when k is small and
when the abovementioned assumption holds (every subgoal binds all its free variables after
its proof terminates).
 If v < m  n: T (n; m; v) = O(n  mv + n  log n)
 If m  v  n: T (n; m; v) = O(n  mm,1 + n  log n)
 If v  m ' n: T (n; m; v) = O(nv+1  log n)
 If m  v ' n: T (n; m; v) = O(n  m! + n  log n)
Generally, for a small number v of shared variables, the complexity of the algorithm is
roughly bounded by O(nv+1  log n). In particular, if all subgoals are independent (v = 0),
the complexity is O(n log n). In most practical cases, the number of shared free variables
in a rule body is relatively small, and every subgoal binds all its free variables; therefore,
the algorithm has polynomial complexity. Note that even if a rule body in the program
text contains many free variables, most of them usually become bound after the rule head
unification is performed (i.e., before we start the ordering of the instantiated body).

5. Learning Control Knowledge for Ordering

The ordering algorithms described in the previous sections assume the availability of correct
values of average cost and number of solutions for various predicates under various argument
bindings. In this section we discuss how this control knowledge can be obtained by learning.
Instead of static exploration of the program text (Debray & Lin, 1993; Etzioni, 1993),
we adopt the approach of Markovitch and Scott (1989) and learn the control knowledge
by collecting statistics on the literals that were proved in the past. This learning can be
performed on-line or off-line. In the latter case, the ordering system first works with a
training set of queries, while collecting statistics. This training set can be built on the
75

fiLedeniov & Markovitch

distribution of user queries seen in the past. We assume that the distribution of queries
received by the system does not change significantly with time; hence, the past distribution
directs the system to learn relevant knowledge for the future queries.
While proving queries, the learning component accumulates information about the control values (average cost and number of solutions) of various literals. Storing a separate
value for each literal is not practical, for two reasons. The first is the large space required
by this approach. The second is the lack of generalization: the ordering algorithm is quite
likely to encounter literals which have not been seen before, and whose control values are
unknown. Recall that when we transformed Equation 2 into Equation 5, we moved from
control values of single literals to average control values over sets of literals. To obtain the
precise averages for these sets, we still needed the control values of individual literals. Here,
we take a different approach, that of learning and using control values for more general
classes of literals. The estimated cost (nsols) value of a class can be defined as the average
real cost (nsols) value of all examples of this class that were proved in the past.
The more refined the classes, the smaller the variance of real control values inside each
class, the more precise the cost and nsols estimations that the classes assign to their members, and the better orderings we obtain. One easy way to define classes is by modes
or binding patterns (Debray & Warren, 1988; Ullman & Vardi, 1988): for each argument we denote whether it is free or bound. For example, for the predicate father the
possible classes are father(free,free), father(bound,free), father(free,bound) and
father(bound,bound). Now, if we receive a literal (for example, father(abraham,X)),
we can easily determine its binding pattern (in this case, father(bound,free)) and retrieve the control information stored for this class. Of course, to find the binding pattern
of a subgoal with a given binding set, we need a method to determine which variables are
bound by the subgoals of the binding set. The same problem arose in DPart computation
(Section 4.3). We shall discuss some practical ways to solve this problem in Section 7.1.
For the purpose of class definition we can also use regression trees { a type of decision tree
that classifies to continuous numeric values and not to discrete classes (Breiman et al., 1984;
Quinlan, 1986). Two separate regression trees can be stored for every program predicate,
one for its cost values, and one for the nsols. The tests in the tree nodes can be defined
in various ways. If we only use the test \is argument i bound?", then the classes of literals
defined by regression trees coincide with the classes defined by binding patterns. But we
can also apply more sophisticated tests, both syntactic (e.g., \is the third argument a term
with functor f?") and semantic (e.g., \is the third argument female?"), which leads to
more refined classes and better estimations. A possible regression tree for estimating the of
number of solutions for predicate father is shown in Figure 15.
Semantic tests about the arguments require logic inference (in the example of Figure 15
{ invoking the predicate female on the first argument of the literal). Therefore, they must
be as ecient as possible. Otherwise the retrieval of control values will take too much time.
The problem of ecient learning of control values is further considered elsewhere (Ledeniov
& Markovitch, 1998a).
Several researchers applied machine learning techniques for accelerating logic inference
(Cohen, 1990; Dejong & Mooney, 1986; Langley, 1985; Markovitch & Scott, 1993; Minton,
1988; Mitchell, Keller, & Kedar-Cabelli, 1986; Mooney & Zelle, 1993; Prieditis & Mostow,
1987). Some of these works used explanation-based learning or generalized caching tech76

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Average: 3.1416
Test: bound(arg1)?

yes

no

Average: 0.3
Test: female(arg1)?

yes
Average: 0.0

Average: 5
Test: bound(arg2)?

no

yes

Average: 0.5
Test: bound(arg2)?

Average: 0.98

yes

no

Average: 0.0001

Average: 1.0

no
Average: 50

Figure 15: A regression tree that estimates the number of solutions for father(arg1,arg2).
niques to avoid repeated computation. Others utilized the acquired knowledge for the problem of clause selection. None of these works, however, dealt with the problem of subgoal
reordering.

6. Experimentation
To test the effectiveness of our ordering algorithm, we experimented with it on various
domains, and compared its performance to other ordering algorithms. Most experiments
were performed on randomly created artificial domains. We also tested the performance of
the system on several real domains.

6.1 Experimental Methodology
All experiments described below consist of a training session, followed by a testing session.
Training and testing sets of queries are randomly drawn from a fixed distribution. In the
training session we collect the control knowledge for literal classes. In the testing session we
prove the queries of the testing set using different ordering algorithms, and compare their
performance using various measurements.
The goal of ordering is to reduce the time spent by the Prolog interpreter when it
proves queries of the testing set. This time is the sum of the time spent by the ordering
procedure (ordering time) and the time spent by the interpreter (inference time). Since the
CPU time is known to be very sensitive to irrelevant factors such as hardware, software
and programming quality, we also show two alternative discrete measurements: the total
number of clause unifications, and the total number of clause reductions performed. The
number of reductions reects the size of the proof tree.
For experimentation we used a new version of the lassy system (Markovitch & Scott,
1989), using regression trees for learning, and the ordering algorithms discussed in this
paper.
77

fiLedeniov & Markovitch

6.2 Experiments with Artificial Domains

In order to ensure the statistical significance of the results of comparing different ordering
algorithms, we experimented with many different domains. For this purpose, we created a
set of 100 artificial domains, each with a small fixed set of predicates, but with a random
number of clauses in each predicate, and with random rule lengths. Predicates in the
rule bodies, and arguments in both rule heads and bodies are randomly drawn from fixed
distributions. Each domain has its own training and testing sets (these two sets do not
intersect).
The more training examples are fed into the system on the learning phase, the better
estimations of control values it produces. On the other hand, the learning time must be limited, because after seeing a certain number of training examples, new examples do not bring
much new information, and additional learning becomes wasteful. We have experimentally
built a learning curve which shows the dependence of the quality of the control knowledge
on the amount of training. The curve suggests that after control values were learned for
approximately 400 literals, there is no significant improvement in the quality of ordering
with new training examples. Therefore, in the subsequent experiments we stopped training
after 600 cost values were learned. The training time was always small: one learned cost
value corresponds to a complete proof of a literal. Thus, if every predicate in a program has
four clauses that define it, then 600 cost values are learned after 2400 unifications, which is
a very small time.
The control values were learned by means of regression trees (Section 5), with simple
syntactic tests that only checked whether some argument is bound or whether some argument is a term with a certain functor (the list of functors was created automatically when
the domain was loaded). However, as we shall see, even these simple tests succeeded in
making good estimations of control values.
We tested the following ordering methods:
 Random: The subgoals are permuted randomly and the control knowledge is not
used.
 Algorithm 3: Building ordered prefixes. Out of all prefixes that are permutation of
one another, only the cheapest one is retained.
 Algorithm 3a: As Algorithm 3, but with best-first search method used to define the
next processed prefix. A similar algorithm was used in the lassy system of Markovitch
and Scott (1989).
 Algorithm 3b: As Algorithm 3a, but with adjacency restriction test added. A
similar algorithm was described by Smith and Genesereth (1985).
 Algorithm 4: As Algorithm 3b, but whenever all the subgoals that are not in the
prefix are independent (under the binding of the prefix), they are sorted and the result
is appended to the prefix as one unit.
 Algorithm 6: The dac algorithm.
In our experiments we always used the Bubble-Sort algorithm to sort literals in independent sets. This algorithm is easy to implement, and it is known to be ecient for small
78

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Ordering
Method
Random
Algorithm 3
Algorithm 3.a
Algorithm 3.b
Algorithm 4
Algorithm 6

Unifications Reductions Ordering Inference Total Ord.Time
Time
Time Time Reductions
86052.06
27741.52
8.1910
27.385 35.576 0.00029
2600.32
911.04 504.648
1.208 505.856 0.55
2829.00
978.59 347.313
1.178 348.491 0.35
2525.34
881.12 203.497
1.137 204.634 0.23
2822.27
976.02
40.284
1.191 41.475 0.04
2623.82
914.67
2.3620
1.102 3.464 0.0025

Table 4: The effect of ordering on the tree sizes and the CPU time (mean results over 100 artificial
domains).

sets, when the elements are already ordered, or nearly ordered. In practice, programmers
order most program rules optimally, and the sorting stops early.
Since the non-deterministic nature of the random method introduces additional noise,
we performed on each artificial domain 20 experiments with this method, and the table
presents the average values of these measurements.
Table 4 shows the obtained results over 100 domains: the rows correspond to the ordering
methods used, and the columns to the measurements taken. The rightmost column shows
the ratio of the ordering time and the number of reductions performed, which reects the
average ordering time of one rule body. The inference time was not measured separately,
but was set as the difference of the total time and the ordering time.
Several observations can be made:
1. Using the dac ordering algorithm helps to reduce the total time of proving the testing
set of queries by a factor of 10, compared to the random ordering. The inference time
is reduced by a factor of 25.
2. All deterministic ordering methods have similar number of unifications and reductions,
and similar inference time, which is predictable, since they all find minimal orderings.
Small uctuations of these values can be explained by the fact that some rules have
several minimal orderings under the existing control knowledge, and different ordering
algorithms select different minimal orderings. Since the control knowledge is not
absolutely precise, the real execution costs of these orderings may be different, which
leads to the differences. The random ordering method builds much larger trees, with
larger inference time.
3. When we compare the performance of the deterministic algorithms (3 { 6), we see
that the dac algorithm performs much better than the algorithms that build ordered
prefixes. In the latter ones, the ordering is expensive, and smaller inference time
cannot compensate for the increase in ordering time. Only Algorithm 4, a combination
of several ideas of previous researchers, has total time comparable with the time of
the random method (though still greater).
79

fiLedeniov & Markovitch

4. It may seem strange that the simple random ordering method has larger ordering time
than the sophisticated Algorithm 6. To explain this, note that the random method
creates much larger proof trees (on average), therefore the number of ordered rules
increases, and even the cheap operations, like random ordering of a rule, sum up to
a considerable time. The average time spent on ordering of one rule is shown in the
last column of Table 4; this value is very small in the random method.

6.3 Experiments with Real Domains

We tested our ordering algorithm also on real domains obtained from various sources. These
domains allow us to compare orderings performed by our algorithm with orderings performed by human programmers.
The following domains were used:
 Moral-reasoner: Taken from the Machine Learning Repository at the University
of California, Irvine1 . The domain qualitatively simulates moral reasoning: whether
a person can be considered guilty, given various aspects of his character and of the
crime performed.
 Depth-first planner: Program 14.11 from the book \The Art of Prolog" (Sterling
& Shapiro, 1994). The program implements a simple planner for the blocks world.
 Biblical Family Database: A database similar to that described in Example 1.
 Appletalk: A domain describing the physical layout of a local computer network
(Markovitch, 1989).
 Benchmark: A Prolog benchmark taken from the CMU Artificial Intelligence Repository2. The predicate names are not informative: it is an example of a program where
manual ordering is dicult.
 Slow reverse: Another benchmark program from the same source.
 Geography: Also a benchmark program from the CMU Repository. The domain
contains many geographical facts about countries.
Table 5 shows the results obtained. For ordering we used the dac algorithm, with literal
classes defined by binding patterns. It can be seen that the dac algorithm was able to speed
up the logic inference in real domains as well. Note that in the Slow Reverse domain the
programmer's ordering was already optimal; thus, applying the ordering algorithm did not
reduce the tree sizes. Still, the overhead of the ordering is not significant.

7. Discussion

In this concluding section we discuss several issues concerning the practical implementation
of the dac algorithm and several ways to increase its eciency. Then we survey some
related areas of logic programming and propose the use of the dac algorithm there.
1. URL: http://www.ics.uci.edu/~mlearn/MLRepository.html
2. URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/html/air.html

80

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Domain

Without ordering
With ordering
Gain ratio
unifications seconds unifications seconds (time/time)
Moral-reasoner
352180 98.39
87020 23.53
4.2
Depth-first planner
10225 19.01
9927 18.16
1.05
Biblical Family
347827 112.68
120701 46.08
2.5
Appletalk
5036167 1246.30
640092 221.73
5.6
Benchmark
62012 554.31
46012 395.04
1.4
Slow reverse
6291 10.33
6291 11.92
0.9
Geography
428480 141.47
226628 82.76
1.7
Table 5: Experiments on real domains.

7.1 Practical Issues

In this subsection we would like to address several issues related to implementation and
applications of the dac algorithm.
The computation of the DPart function (Section 3.2.1) requires a procedure for computing the set of variables bound by a given binding set of subgoals. The same procedure
is needed for computing control values (Section 5). There are several possible ways to
implement such a procedure. For example:
1. The easiest way is to assume that every subgoal binds all the variables appearing in its
arguments. This simplistic assumption is sucient for many domains, especially the
database-oriented ones. However, it is not appropriate when logic programs are used
to manipulate complex data structures containing free variables (such as difference
lists). This assumption was used for the experiments described in Section 6.
2. Some dialects of Prolog and other logic languages support mode declarations provided
by the user (Somogyi et al., 1996b). When such declarations are available, it is easy
to infer the binding status of each variable upon exiting a subgoal.
3. Even when the user did not supply enough mode declarations, they can often be
inferred from the structure of the program by means of static analysis (Debray &
Warren, 1988). Note, however, that as was pointed out by Somogyi et al. (1996b),
no-one has yet demonstrated a mode inference algorithm that is guaranteed to find
accurate mode information for every predicate in the program.
4. We can learn the sets of variables bound by classes of subgoals using methods similar
to those described in Section 5 for learning control values.
Several researchers advocate user declarations of available (permitted) modes. Such
declarations can be elegantly incorporated into our algorithm to prune branches that violate
available modes. When we fix a binder in an OR-node, we compute the set of variables
that become bound by it. If this results in a violation of an available mode for one of the
subgoals of the corresponding child, then the whole subtree of this child is pruned. Note
that we can detect violations even when the mode of the subgoal is partially unknown
81

fiLedeniov & Markovitch

CandidateSet(S ; B)
let fS1; S2; : : : Sk g DPart(S ; B)
case

:::
 k = 1, shared-vars(S1) 6= ; (S is indivisible under B):
loop for A 2 S
if B [ fAg does not violate available modes
in any subgoal of S n fAg
then
let C (A) CandidateSet
(S fin fAg; B [ fAog)
n
0
let C (A)
Fold(AkO~ A; B) fifi O~ A 2 C (A)
else let
C 0(A) ; (don't enter the branch)
S
Return A2S C 0 (A)

Figure 16: Changes to Algorithm 6 that make use of available mode declarations.
The rest of the algorithm remains unchanged.

at the moment. For example, if all the available modes require that the first argument
be unbound, then binding of the argument by the OR-node binder will trigger pruning,
even if the binding status of the other arguments is not yet known. Figure 16 shows how
Algorithm 6 can be changed in order to incorporate declarations of available modes. Any
other correctness requirement can be treated in a similar manner: a candidate ordering will
be rejected whenever we see that it violates the requirement.
The experiments described in Section 6 were performed with a Prolog interpreter. Is
it possible to combine the dac algorithm with a Prolog compiler? There are several ways
to achieve this goal. One way is to allow the compiler to insert code for on-line learning.
The compiled code will contain procedures for accumulating control values and for the dac
algorithm. Alternatively, off-line learning can be implemented, with training as a part of
the compilation process.
Another method for combining our algorithm with existing Prolog compilers is to use
it for program transformation, and to process the transformed program by a standard
compiler. Elsewhere (Ledeniov & Markovitch, 1998a) we describe the method for classifying
the orderings produced by the dac algorithm. For each rule we build a classification tree,
where classes are the different orderings of the rule body, and the tests are applied to the
rule head arguments. These are the same type of tests described in Section 5 for learning
control values. Figure 17 shows two examples of such trees.
Given such a classification tree, we can write a set of Prolog rules, where each rule has
the same head as the original rule, and has a body built of all the tests on the path from
the tree root to a leaf node followed by the ordering at the leaf. For example, the second
tree in Figure 17 yields the following set of rules:
82

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

A classification tree for the rule
uncle(X,Y) of Example 1.

nonvar(Y) ?
yes

no

[parent(Z,Y),brother(Z,X)]
[brother(Z,X),parent(Z,Y)]
nonvar(X) ?
yes

A possible classification tree for the rule
head(X,Y)

male(X) ?

p1(X), p2(Y), p3(X,Y).

yes
[p1(X),p3(X,Y),p2(Y)]

no

nonvar(Y) ?

no

yes

no

[p2(Y),p3(X,Y),p1(X)]

[p1(X),p2(Y),p3(X,Y)]

[p3(X,Y),p1(X),p2(Y)]

Figure 17: Examples of classification trees that learn rule body orderings.

head(X,Y)
head(X,Y)
head(X,Y)
head(X,Y)

nonvar(X), male(X), p1(X), p3(X,Y), p2(Y).
nonvar(X), not(male(X)), p1(X), p2(Y), p3(X,Y).
var(X), nonvar(Y), p2(Y), p3(X,Y), p1(X).
var(X), var(Y), p3(X,Y), p1(X), p2(Y).

From Table 4 we can see that while the dac algorithm helped to reduce the inference
time by a factor of 25, the total time was reduced only by a factor of 10. This difference
is caused by the additional computation of the ordering procedure. There is a danger that
the benefit obtained by ordering will be outweighed by the cost of the ordering process.
This is a manifestation of the so-called utility problem (Minton, 1988; Markovitch & Scott,
1993). In systems that are strongly-moded (such as Mercury { Somogyi et al., 1996b) we can
employ the dac algorithm statically at compilation time for each one of the available modes,
thus reducing the run-time ordering time to zero. The mode-based approach performs only
syntactic tests of the subgoal arguments. The classification tree method, described above,
is a generalization of the mode-based approach, allowing semantic tests as well.
Due to insucient learning experience or lack of meaningful semantic tests, it is quite
possible that the classification trees contain leaves with large degrees of error. In such cases
we still need to perform the ordering dynamically. To reduce the harmfulness of the utility
problem in the case of dynamic ordering, we can use a cost-sensitive variation of the dac
algorithm (Ledeniov & Markovitch, 1998a, 1998b). This modified algorithm deals with the
problem by explicit reasoning about the economy of the control process. The algorithm is
anytime, that is, it can be stopped at any moment and return its currently best ordering
(Boddy & Dean, 1989). We learn a resource-investment function to compute the expected
return in speedup time for additional control time. This function is used to determine a
stopping condition for the anytime procedure. We have implemented this framework and
found that indeed we have succeeded in reducing ordering time, without significant increase
of inference time.
83

fiLedeniov & Markovitch

7.2 Relationship to Other Works

The work described in this paper is a continuation of the line of research initiated by Smith
and Genesereth (1985) and continued by Natarajan (1987) and Markovitch and Scott (1989).
This line of research aims at finding the most ecient ordering of a set of subgoals. The
search for minimal-cost ordering is based on cost analysis that utilizes available information
about the cost and number-of-solutions of individual subgoals.
Smith and Genesereth (1985) performed an exhaustive search over the space of all
permutations of the given set of subgoals, using the adjacency restriction to reduce the
size of the search space (Equation 8). This restriction was applied on pairs of adjacent
subgoals in the global ordering of the entire set. When applied to an independent set of
subgoals, the adjacency restriction is easily transformed into the sorting restriction: the
subgoals in a minimal ordering must be sorted by their cn values. Natarajan (1987) arrived
at this conclusion and presented an ecient ordering algorithm for independent sets.
The dac algorithm uses subgoal dependence to break the set into smaller subsets. Independent subsets are sorted. Dependent subsets are recursively ordered, and the resulting
orderings are merged using a generalization of the adjacency restriction that manipulates
blocks of subgoals. Therefore the dac algorithm is a generalization of both algorithms.
During the last decade, a significant research effort went into static analysis (SA) of
logic programs. There are three types of SA that can be exploited by the dac algorithm to
reduce the ordering time.
A major part of the SA research deals with program termination (De Schreye & Decorte,
1994). The dac algorithm solves the termination problem, as a special case of the eciency
problem (it always finds a terminating ordering, if such orderings exist). During learning,
we set limits on the computation resources available for subgoal execution. If a subgoal is
non-terminating (in a certain mode), the learning module will associate a very high cost
with this particular mode. Consequently, the dac algorithm will not allow orderings with
this mode of the subgoal. Nevertheless, while the use of static termination analysis is
not mandatory for a proper operation of the dac algorithm, we can exploit such analysis
to increase the eciency of both the learning process and the ordering process. During
learning, the limit that we set on the computation resources devoted to the execution of
a subgoal must be high, to increase the reliability of the cost estimation. However, such
a high limit can lead to a significant increase in learning time when many subgoals are
non-terminating. If termination information obtained by SA is available, we can use it to
avoid entering infinite branches of proof trees. During ordering, termination information can
serve to reduce the size of space of orderings searched by the algorithm. If the termination
information comes in the form of allowed modes (Somogyi et al., 1996b), orderings that
violate these modes are filtered out, as in the modified algorithm shown in Figure 16. If the
termination information comes in the form of a partial order between subgoals, orderings
that violate this partial order can be filtered out in a similar manner.
The second type of SA research that can be combined with the dac algorithm is correctness analysis, where the program is tested against specifications given by the user.
The folon environment (Henrard & Le Charlier, 1992) was designed to support the
methodology for logic program construction that aims at reconciling the declarative semantics with an ecient implementation (Deville, 1990). The construction process starts with
84

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

a specification, converts it into a logic description and finally, into a Prolog program. If
the rules of the program are not correct with respect to the initial specification, the system performs transformations such as reordering literals in a clause, adding type checking
literals and so on. De Boeck and Le Charlier (1990) mention this reordering, but do not
specify an ordering algorithm different from the simple generate-and-test method. Cortesi,
Le Charlier, and Rossi (1997) present an analyzer for verifying the correctness of a Prolog
program relative to a specification which provides a list of input/output annotations for the
arguments and parameters that can be used to establish program termination. Again, no
ordering algorithm is given explicitly. The purpose of the dac algorithm is complementary
to the purpose of folon, and it could serve as an auxiliary aid to make the resulting Prolog
program more ecient.
Recently, the Mercury language was developed at the University of Melbourne (Somogyi
et al., 1996a, 1996b). Mercury is a strongly typed and strongly moded language. Type and
mode declarations should be supplied by the programmer (though recent releases of the
Mercury system already support partial inference of types and modes { Somogyi et al.,
1996a). The compiler checks that mode declarations for all predicates are satisfied; if
necessary, it reorders subgoals in the rule body to ensure mode correctness (and rejects the
program if neither ordering satisfies the mode declaration constraints). When the compiler
performs this reordering, it does not consider the eciency issue. It often happens that
several orderings of a rule body satisfy the mode declaration constraints: in such cases
the Mercury compiler could call the static version of the dac algorithm to select the most
ecient ordering. Another alternative is to augment the dac algorithm by mode declaration
checks, as was shown in Figure 16.
Note that Mercury is a purely declarative logic programming language, and is therefore
more suitable for subgoal reordering than Prolog. It has no non-logical constructs that
could destroy the declarative semantics which give logic programs their power; in Mercury
even I/O is declarative.
The third type of relevant SA is the cost analysis of logic programs (Debray & Lin,
1993; Braem et al., 1994; Debray et al., 1997). Cortesi et al. (1997) describe a cost formula
similar to Equation 5 to select a lowest-cost ordering. However, they used a generate-andtest approach which can sometimes be prohibitively expensive. Static analysis of cost and
number of solutions can be used to obtain the control values, instead of learning them.
The eciency of logic programs can also be increased by methods of program transformation (Pettorossi & Proietti, 1994, 1996). One of the most popular approaches is the
\rules+strategies" approach, which consists in starting from an initial program and then
applying one or more elementary transformation rules. Transformation strategies are metarules which prescribe suitable sequences of applications of transformation rules.
One of the possible transformation rules is the goal rearrangement rule which transforms
a program by transposing two adjacent subgoals in a rule body. Obviously, any ordering
of a rule body can be transformed into any other ordering by a finite number of such
transpositions. Thus, static subgoal ordering can be considered a special case of program
transformation where only the goal rearrangement rule is used. On the other hand, dynamic
and semi-dynamic ordering methods cannot be represented by simple transformation rules,
since they make use of run-time information (expressed in bindings that rule body subgoals
85

fiLedeniov & Markovitch

obtain through unifications of rule heads), and may order the same rule body differently
under different circumstances.
A program transformation technique called compiling control (Bruynooghe, De Schreye,
& Krekels, 1989; Pettorossi & Proietti, 1994) follows an approach different from that of
trying to improve the control strategy of logic programs. Instead of enhancing the naive
Prolog evaluator using a better (and often more complex) computation rule, the program is
transformed so that the derived program behaves under the naive evaluator exactly as the
initial program would behave under an enhanced evaluator. Most forms of compiling control
first translate the initial program into some standard representation (for example, into an
unfolding tree), while the complex computation rule is used, and then the new program is
constructed from this representation, with the naive computation rule in mind.
Reordering of rule body subgoals can be regarded as moving to a complex computation
rule which selects subgoals in the order dictated by the ordering algorithm. In the case of
the dac algorithm, this computation rule may be too complex for simple use of compiling
control methods. Nevertheless, it can be easily incorporated into a special compiling control
method. In Section 7.1 we described a method of program rewriting which first builds
classification trees based on the orderings that were performed in the past, and then uses
these classification trees for constructing clauses of a derived program. The derived program
can be eciently executed under the naive computation rule of Prolog. This technique is
in fact a kind of compiling control. Its important property is the use of knowledge collected
from experience (the orderings that were made in the past).
One transformation method that can significantly benefit from the dac algorithm is
unfolding (Tamaki & Sato, 1984). During the unfolding process subgoals are replaced by
their associated rule bodies. Even if the initial rules were ordered optimally by a human
programmer or a static ordering procedure, the resulting combined sequence may be far from
optimal. Therefore it could be very advantageous to use the dac algorithm for reordering
of the unfolded rule. As the rules become longer, the potential benefit of ordering grows.
The danger of high complexity of the ordering procedure can be overcome by using the
cost-sensitive version of the dac algorithm (Section 7.1).

7.3 Conclusions
In this work we study the problem of subgoal ordering in logic programs. We present both a
theoretical base and a practical implementation of the ideas, and show empirical results that
confirm our theoretical predictions. We combine the ideas of Smith and Genesereth (1985),
Simon and Kadane (1975) and Natarajan (1987) into a novel algorithm for ordering of
conjunctive goals. The algorithm is aimed at minimizing the time which the logic interpreter
spends on the proof of the given conjunctive goal.
The main algorithm described in this paper is the dac algorithm (Algorithm 6, Section 4.6). It works by dividing the sets of subgoals into smaller sets, producing candidate
sets of orderings for the smaller sets, and combining these candidate sets to obtain orderings
of the larger sets. We prove that the algorithm finds a minimal ordering of the given set
of subgoals, and we show its eciency under practical assumptions. The algorithm can
be employed statically (to reorder rule bodies in the program text before the execution
86

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

starts), semi-dynamically (to reorder the rule body before the reduction is performed) or
dynamically (to reorder the resolvent after every reduction of a subgoal by a rule body).
Several researchers (Minker, 1978; Warren, 1981; Naish, 1985a, 1985b; Nie & Plaisted,
1990) proposed various heuristics for subgoal ordering. Though fast, these methods do
not guarantee finding minimal-cost orderings. Our algorithm provably finds a minimal-cost
ordering, though the ordering itself may take more time than with the heuristic methods. In
the future it seems promising to incorporate heuristics into the dac algorithm. For example,
heuristics can be used to grade binders in OR-nodes: rather than exhaustively trying all
subgoals as binders, we could try just one, or several binders, thus reducing the ordering
time. Also, the current version of our ordering algorithm is suitable only for finding all
solutions to a conjunctive goal. We would like to extend it to the problem of finding one
solution, or a fixed number of solutions.
Another interesting issue for further research is the adaptation of the dac algorithm to
interleaving ordering methods (Section 2.3). There, if subgoals of a rule body are added
to an ordered resolvent, it seems wasteful to start a complete ordering process; we should
use the information stored in the existing ordering of the resolvent. Perhaps the whole
divisibility tree of the resolvent should be stored, and its nodes updated when subgoals of
a rule body are added to the resolvent.
The ordering algorithm needs control knowledge for its work. This control knowledge is
the average cost and number of solutions of literals, and it can be learned by training and
collecting statistics. We make an assumption that the distribution of queries received by
the system does not change with time; thus, if the training set is based on the distribution
seen in the past, the system learns relevant knowledge for future queries. We consider the
issue of learning control values more thoroughly in another paper (Ledeniov & Markovitch,
1998a), together with other issues concerning the dac algorithm (such as minimizing the
total time, instead of minimizing the inference time only).
Ullman and Vardi (1988) showed that the problem of ordering subgoals to obtain termination is inherently exponential in time. The problem we work with is substantially
harder: we must not only find an order whose execution terminates in finite time, but one
that terminates in minimal finite time. It is impossible to find an ecient algorithm for
all cases. The dac algorithm, however, is ecient in most practical cases, when the graph
representing the subgoal dependence (Figure 3) is sparsely connected.
We have implemented the dac algorithm and tested it on artificial and real domains.
The experiments show a speedup factor of up to 10 compared with random ordering, and
up to 13 compared with some alternative ordering algorithms.
The dac algorithm can be useful for many practical applications. Formal hardware
verification has become extremely important in the semiconductor industry. While model
checking is currently the most widely used technique, it is generally agreed that coping with
the increasing complexity of VLSI design requires methods based on theorem proving. The
main obstacle preventing the use of automatic theorem proving is its high computational
demands. The dac algorithm may be used for speeding up logic inference, making the use
of automatic theorem provers more practical.
Logic has gained increasing popularity for representation of common-sense knowledge.
It has several advantages, including exibility and well-understood semantics. Indeed, the
CYC project (Lenat, 1995) has recently moved from frame-based representation to logic87

fiLedeniov & Markovitch

based representation. However, the large scale of such knowledge bases is likely to present
significant eciency problems to the inference engines. Using automatic subgoal ordering
techniques, such as those described here, may help to solve these problems.
The issue of subgoal ordering obtains a new significance with the development of Inductive Logic Programming (Lavrac & Dzeroski, 1994; Muggleton & De Raedt, 1994). Systems
using this approach, such as FOIL (Quinlan & Cameron-Jones, 1995), try to build correct
programs as fast as possible, without considering the eciency of the produced programs.
Combining the dac algorithm with Inductive Logic Programming and other techniques for
the synthesis of logic programs (such as the deductive and the constructive approaches)
looks like a promising direction.

Appendix A. Proof of Lemma 8

In this appendix we present the proof of a lemma which was omitted from the main text of
the paper for reasons of compactness. Before we prove it we show two auxiliary lemmas.

Lemma 9

Let A~ 1 and A~ 2 be two ordered sequences of subgoals, and B a set of subgoals. The value of
cn(A~ 1kA~2)jB lies between the values cn(A~ 1)jB and cn(A~ 2)jB[A~ .
1

Proof:

Denote c1 = cost(A~1 )jB
n1 = nsols(A~1)jB
cn1 = cn(A~1)jB
c2 = cost(A~2 )jB[A~ n2 = nsols(A~2)jB[A~ cn2 = cn(A~2)jB;[A~
c1;2 = cost(A~1 kA~ 2)jB n1;2 = nsols(A~ 1kA~ 2 )jB cn1;2 = cn(A~ 1 kA~ 2)jB
cn = n1;2 , 1 = n1 n2 , 1 = (n1 , 1) + n1 (n2 , 1) =
1

1;2

1

1

c1;2

c1;2
c1;2
n
n
1 ,1
2 ,1
c
+n c
= 1 c1 c 1 2 c2 = c1 cn1 +c n1 c2cn2 = cc1  cn1 + nc1 c2  cn2
1;2
1;2
1;2
1;2
n
c
1 c2
1
So, cn1;2 always lies between cn1 and cn2 (because c1;2 and c1;2 are positive and
to 1). More exactly, the point cn1;2 divides the segment [cn1; cn2] with ratio

sum

(cn1;2 , cn1 ) : (cn2 , cn1;2 ) = n1 c2 : c1:

In other words, cn1;2 is a weighted average of cn1 and cn2 . Note that c1 is the amount
of resources spent in the proof-tree of B~ 1 , n1 c2 { the resources spent in the tree of B~ 2 , and
c1;2 is their sum. So, the more time (relatively) we dedicate to the proof of B~ 1 , the closer
cn1;2 is to cn1. This conclusion can be generalized to a larger number of components in a
concatenation (the proof is by induction):
cost(A~ 1 )jB
cn(A~ 1 kA~ 2 k : : : A~ k )jB =
 cn(A~ 1 )jB +
cost(A~ 1 kA~ 2 k : : : A~ k )jB
nsols(A~ 1 )jB  cost(A~ 2 )jB[A~ 1
+
 cn(A~ 2 )jB[A~ 1 + : : : +
cost(A~ 1 kA~ 2k : : : A~ k )jB
nsols(A~ 1 kA~ 2 k : : : A~ k,1)jB  cost(A~ k )jB[A~ 1 [:::A~ k,1
+
 cn(A~ k )jB[A~ 1 [:::A~ k,1
cost(A~ 1kA~ 2 k : : : A~ k )jB
88

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

2

Lemma 10
Let S0 be a set of subgoals and N be a node in the divisibility tree of S0. Let O~ N =
Q~ kA~ 1kA~2 kR~ be an ordering of S (N ), where A~ 1 and A~ 2 are cn-equal max-blocks: cn(A~ 1 )jB(N )[Q~ =
cn(A~ 2)jB(N )[Q~ [A~ .
Let M be an ancestor of N and O~ M be an ordering of S (M ) consistent with O~ N , where
1

A~ 1 and A~ 2 are not violated. Then either A~ 1 and A~ 2 are both max-blocks in O~ M and all
max-blocks that stand between them are cn-equal to them, or A~ 1 and A~ 2 belong to the same
max-block in O~ M , or O~ M is MC-contradicting.
Proof: By induction on the distance between N and M . If M = N , then A~1 and A~2
are max-blocks, and the lemma holds. Let M 6= N , and let M 0 be the child of M whose
0 be the
descendant is N . By inductive hypothesis, the lemma holds for N and M 0 . Let O~ M
0
0
~
~
~
~
projection of OM on M . A1 and A2 are not violated in OM , since they are not violated in
O~ M .
 If A~ 1 and A~2 are both max-blocks in O~ M0 , then by the inductive hypothesis all maxblocks that stand between them are cn-equal to them. If M is an OR-node, no new
subgoals can enter between A~ 1 and A~ 2 . If M is an AND-node, the insertion of new

subgoals is possible, but if it violates blocks, or places max-blocks not ordered by cn,
then O~ M is MC-contradicting, by Corollary 3 or Lemma 6. So, if O~ M is not MCcontradicting, then all new max-blocks inserted between A~ 1 and A~ 2 must be cn-equal
to them both.
Assume that A~ 1 and A~ 2 are not both max-blocks in O~ M . Without loss of generality,
let A~ 1 be member of a larger max-block in O~ M . We show that A~ 2 must also participate
in the same max-block.
Since A~ 1 joined a larger block, there must exist another block, B~ , adjacent to A~ 1,
such that their pair is cn-inverted. Let B~ stand to the left of A~ 1 (in the opposite case,
~ A~ 1i is cn-inverted, i.e.,
the proof is similar): O~ M = X~ kB~ kA~ 1kY~ kA~ 2 kZ~ . The pair hB;
~
~
~
~
cn(B )jB(M )[X~ > cn(A1)jB(M )[X~ [B~ . From Lemma 9, cn(B kA1)jB(M )[X~ > cn(A~ 1)jB(M )[X~ [B~ ,
and we must add to the block B~ kA~ 1 all blocks from Y~ , because they are all cn-equal
to A~ 1 . Also, cn(A~ 1)jB(M )[X~ [B~ = cn(A~ 2 )jB(M )[X~ [B~ [A~ , and A~ 2 must also be added
to the block. Thus, A~ 1 and A~ 2 belong to the same max-block in O~ M .
 If A~1 and A~ 2 belong to the same max-block in O~ M0 , then this block is either violated
in O~ M , or not. In the former case, O~ M is MC-contradicting, by Corollary 3. In the
latter case, A~ 1 and A~ 2 belong to the same max-block in O~ M .
 If O~ M0 is MC-contradicting, then O~ M is MC-contradicting too (the proof is easy). 2
1

Now we can prove Lemma 8:

Lemma 8
Let S0 be a set of subgoals, N be a node in the divisibility tree of S0 and O~ N = Q~ kA~ 1 kA~ 2kR~
89

fiLedeniov & Markovitch

be an ordering of S (N ), where A~ 1 and A~ 2 are max-blocks, mutually independent and
cn-equal under the bindings of B(N ) [ Q~ . Then O~ N is blockwise-equivalent with O~ N0 =
Q~ kA~ 2kA~1 kR~ .

Proof:

Let S~ be a minimal ordering of S0 binder-consistent with O~ N . By Corollary 3, S~ does not
~0
violate the blocks of O~ N , in particular A~ 1 and A~ 2 : S~ = X~ kA~ 1kY~ kA~ 2 kZ~ . Let S~ 0 = S~ jOO~ NN =
X~ kA~ 2 kY~ kA~ 1 kZ~ . We must show that S~ 0 is minimal, which implies blockwise equivalence of
O~ N and O~ N0 .
If Y~ is empty, then Cost(S~ ) = Cost(S~ 0 ) by Lemma 2 (A~ 1 and A~ 2 are adjacent, mutually
independent and cn-equal; thus, their transposition does not change the cost).
If Y~ is not empty, then by Corollary 2 Y~ is mutually independent of both A~ 1 and A~ 2
(S~ is binder-consistent with O~ N , therefore B(N )  X~ , and consequently Y~ \ B(N ) = ;).
Y~ can be divided into several blocks, each one of them cn-equal to A~ 1 and A~ 2: since S~
is minimal, O~ N cannot be MC-contradicting, and the claim follows from Lemma 10. By
Lemma 9, cn(Y~ )jX~ = cn(A~ 1 )jX~ = cn(A~ 2)jX~ . By Lemma 2:

Cost(S~ ) = Cost(X~ kA~ 1kY~ kA~ 2kZ~ )
= Cost(X~ kA~ 1kA~ 2 kY~ kZ~ )
= Cost(X~ kA~ 2kA~ 1 kY~ kZ~ )
= Cost(X~ kA~ 2kY~ kA~ 1kZ~ )

=
=
=
=

== swap(Y; A2)
== swap(A1; A2 )
== swap(A1; Y )
Cost(S~ 0 )

Minimality of S~ 0 implies blockwise equivalence of O~ N and O~ N0 .

2

Appendix B. Correctness of the dac Algorithm

In this section we show that the dac algorithm is correct, i.e., given a set of subgoals S0,
it returns its minimal ordering. It suces to show that the candidate set of the root node
of DTree(S0; ;) is valid. In such a case, as follows from the definition of valid sets, it must
contain a minimal ordering. The algorithm returns one of the cheapest candidates of the
root. Therefore, if the candidate set of the root is valid, the dac algorithm must return a
minimal ordering of S0.
We start by defining strong validity of sets of orderings. We then prove that strong
validity implies validity. Finally, we use induction to prove a theorem, showing that the
candidate set produced for each node in the divisibility tree is strongly valid.
Definition: Let S0 be a set of subgoals, N be a node in the divisibility tree of S0. The set
CN  (S (N )) is strongly valid, if every ordering in (S (N )) nCN is either MC-contradicting
or blockwise-equivalent to some member of CN , unless no ordering of S (N ) is min-consistent.

StronglyV alidN;S (CN ) ()
[9O~ N0 2  (S (N )) : MCN;S (O~ N0 )] ! [O~ N 2  (S (N )) n CN ! MCCN;S (O~ N )_
(9O~ N00 2 CN ^ MCEN;S (O~ N ; O~ N00 ))]
0

0

0

0

Lemma 11 A strongly valid set of orderings is valid.
90

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Proof: Let S0 be a set of subgoals, N be a node in the divisibility tree of S0, C (N ) be a
strongly valid set of orderings of N .
If there is no min-consistent ordering of N , then C (N ) is valid, by the definition of a
valid set (Section 4.2).
Otherwise, there exists at least one minimal ordering of S0, binder-consistent with N .
Every ordering in  (S (N )) n C (N ) is either MC-contradicting or blockwise-equivalent to
some member of C (N ). To prove that C (N ) is valid, we must show that it contains an
ordering O~ N , which is binder-consistent with some minimal ordering S~ of S0 .
Let S~ 0 be a minimal ordering of S0, binder-consistent with N . Let O~ N0 be the projection
of S~ 0 on N . If O~ N0 2 C (N ), we are done (O~ N = O~ N0 , S~ = S~ 0). Otherwise, O~ N0 2  (S (N )) n
C (N ). O~ N0 cannot be MC-contradicting (it is min-consistent to S~ 0), therefore it must be
blockwise-equivalent to some O~ N00 2 C (N ). Blocks of O~ N0 are not violated in S~ 0, since S~ 0 is
~ 00
minimal (Corollary 3). Therefore the substitution S~ 00 = S~ 0jOO~ N0 is well defined. S~ 00 is minimal,
N
since S~ 0 is minimal and O~ N0 and O~ N00 are blockwise-equivalent. S~ 00 is binder-consistent with
O~ N00 , since S~ 0 was binder-consistent with O~ N0 . Thereupon S~ 00 and O~ N00 satisfy the requirements
of validity (O~ N = O~ N00 , S~ = S~ 00).
2
Theorem 3
Let S0 be a set of subgoals. For each node N of the divisibility tree of S0, Algorithm 6
creates a strongly valid candidate set of orderings.

Proof: By induction on the height of N 's subtree.
Inductive base: N is a leaf node, which means that S (N ) is independent under B(N ).

The candidate set of N contains one element, whose subgoals are sorted by cn. All
orderings that belong to  (S (N )) n CandSet(N ) are either not sorted by cn, and
hence are MC-contradicting (Lemma 4), or are sorted by cn, and hence are blockwiseequivalent to the candidate (Corollary 4). Consequently, CandSet(N ) is strongly
valid.
Inductive hypothesis: For all children of N , Algorithm 6 produces strongly valid candidate sets.
Inductive step: An internal node in a divisibility tree is either an AND-node or an ORnode.
1. N is an AND-node. Let N1; N2; : : :Nk be the children of N . First we show that
ConsSet(N ) is strongly valid.
Let O~ N 2  (S (N )) n ConsSet(N ). For all 1  i  k, let O~ i be the projection of
O~ N on Ni. The set of projections fO~ 1; O~ 2; : : : O~ k g can belong to one of the three
following types, with regard to O~ N .
(a) The sets of the first type contain at least one MC-contradicting projection. In
such a case O~ N is MC-contradicting too. Assume the contrary: there exists
a minimal ordering S~ of S0, binder-consistent with O~ N . Let O~ i be an MCcontradicting projection. Since O~ i is consistent with O~ N , it is also consistent
91

fiLedeniov & Markovitch

with S~ . Since B(Ni ) = B(N ), all subgoals of B(Ni) appear in S~ before
subgoals of S (Ni ). Therefore, O~ i is binder-consistent with S~ , and since S~ is
minimal, O~ i is min-consistent and not MC-contradicting { a contradiction.
(b) The sets of the second type do not contain MC-contradicting projections, but
in O~ N some block of some projection is violated, or max-blocks from different
projections are not ordered by cn. In such a case, O~ N is MC-contradicting,
by Corollary 3 and Lemma 6.
(c) The sets of the third type do not contain MC-contradicting projections, and
max-blocks of the projections are not violated in O~ N and are sorted by cn.
Every projection O~ i either belongs to CandSet(Ni), or not. If O~ i 62 CandSet(Ni),
then there exists O~ i0 2 CandSet(Ni) such that O~ i is blockwise-equivalent to
O~ i0 (because CandSet(Ni) is strongly valid by the inductive hypothesis, and
O~ i is not MC-contradicting). If O~ i 2 CandSet(Ni), we can set O~ i0 = O~ i.
~0
~0 ~0
Let O~ N0 = O~ N jOO~ jOO~ : : : jOO~ kk . This substitution is well defined, since each O~ i
has the same number of max-blocks as O~ i0 , and max-blocks of the projections
are not violated in O~ N . Let S~ be a minimal ordering of S0, binder-consistent
with O~ N . Since S~ is minimal, blocks of O~ 1 are not violated in S~ . Since O~ 1
~0
is blockwise-equivalent to O~ 10 , the ordering S~1 = S~ jOO~ is well-defined and
minimal. In S~1 the positions of the subgoals from B(N ) did not change;
thus, O~ 2 is min-consistent with S~1, and blockwise equivalence of O~ 2 and O~ 20
~0
~0 ~0
entails minimality of the ordering S~2 = S~1 jOO~ = S~ jOO~ jOO~ . We continue with
~0
~0 ~0
other O~ i -s, and finally obtain that S~ 0 = S~ jOO~ jOO~ : : : jOO~ kk is minimal. From the
~0
definition of O~ N0 , S~ 0 = S~ jOO~ NN (note that we introduced blockwise equivalence
and strong validity only to be able to perform this transition). S~ 0 is minimal,
therefore O~ N is blockwise-equivalent to O~ N0 . O~ N0 2 ConsSet(N ), since all its
projections are candidates of the child nodes. Thereupon, O~ N is blockwiseequivalent to a member of ConsSet(N ).
So, ConsSet(N ) is strongly valid. To prove that CandSet(N ) is strongly valid,
it suces to show that all the members of ConsSet(N ) that are not included in
CandSet(N ) by Algorithm 6, are either MC-contradicting or blockwise-equivalent
to members of CandSet(N ). Such orderings can be of three types:
(a) Orderings that violate blocks of the children projections. They are MCcontradicting by Corollary 3.
(b) Orderings that do not violate blocks, but where max-blocks of children projections are not ordered by cn. They are MC-contradicting by Lemma 6.
(c) Orderings that do not violate blocks and have them sorted by cn. For each
combination of projections, one consistent ordering of N is retained in the
candidate set, and all the other are rejected. By Corollary 5, the rejected
orderings are blockwise-equivalent to the retained candidate.
Consequently, CandSet(N ) is strongly valid.
1

2

1

2

1
1

92

2

1

2

2

1

2

1

2

1

2

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

2. N is an OR-node. Again, we start with showing that ConsSet(N ) is strongly
valid.
Let O~ N 2  (S (N )) n ConsSet(N ). O~ N is constructed from a binder H and a
\tail" sequence T~ : O~ N = H kT~ . Let NH be the child of N that corresponds
to the binder H . By the inductive hypothesis, CandSet(NH ) is strongly valid.
T~ 62 CandSet(NH ), since otherwise O~ N 2 ConsSet(N ). Therefore, T~ is either MC-contradicting, or blockwise-equivalent to some T~ 0 2 CandSet(NH ).
If T~ is MC-contradicting, O~ N is MC-contradicting too (proof by contradiction, as for AND-nodes). If T~ is blockwise-equivalent to T~ 0 , then O~ N = H kT~
is blockwise-equivalent to H kT~ 0 2 ConsSet(N ) (the proof is easy). Hence,
ConsSet(N ) is strongly valid. The only orderings of ConsSet(N ) that are not included in CandSet(N ) by the dac algorithm have cheaper permutations of their
leading max-blocks, and therefore are MC-contradicting, by Lemma 7. Hence,
CandSet(N ) is strongly valid.
2

Corollary 7 The candidate set found by Algorithm 6 for the root node is valid.
Corollary 8 Algorithm 6 finds a minimal ordering of the given set of subgoals.

References

Aho, A. V., Hopcroft, J. E., & Ullman, J. D. (1987). Data Structures and Algorithms.
Addison-Wesley.
Boddy, M., & Dean, T. (1989). Solving time-dependent planning problems. In Sridharan, N. S. (Ed.), Proceedings of the 11th International Joint Conference on Artificial
Intelligence, pp. 979{984, Detroit, MI, USA. Morgan Kaufmann.
Bol, R. N., Apt, K. R., & Klop, J. W. (1991). An analysis of loop checking mechanisms for
logic programs. Theoretical Computer Science, 86 (1), 35{79.
Braem, C., Le Charlier, B., Modar, S., & Van Hentenryck, P. (1994). Cardinality Analysis
of Prolog. In Bruynooghe, M. (Ed.), Logic Programming - Proceedings of the 1994
International Symposium, pp. 457{471, Massachusetts Institute of Technology. The
MIT Press.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification and
Regression Trees. Wadsworth International Group, Belmont, CA.
Bruynooghe, M., De Schreye, D., & Krekels, B. (1989). Compiling control. The Journal of
Logic Programming, 6, 135{162.
Clark, K. L., & McCabe, F. (1979). The control facilities of IC-Prolog. In Michie, D. (Ed.),
Expert Systems in The Microelectronic Age., pp. 122{149. University of Edinburgh,
Scotland.
Clocksin, W. F., & Mellish, C. S. (1987). Programming in Prolog (Third edition). SpringerVerlag, New York.
93

fiLedeniov & Markovitch

Cohen, W. W. (1990). Learning approximate control rules of high utility. In Proceedings of
the Seventh International Machine Learning Workshop, pp. 268{276, Austin, Texas.
Morgan Kaufmann.
Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1991). Introduction To Algorithms. MIT
Press, Cambridge, Mass.
Cortesi, A., Le Charlier, B., & Rossi, S. (1997). Specification-based automatic verification
of Prolog programs. In Gallagher, J. (Ed.), Proceedings of the 6th International Workshop on Logic Program Synthesis and Transformation, Vol. 1207 of LNCS, pp. 38{57,
Stockholm, Sweden. Springer-Verlag.
De Boeck, P., & Le Charlier, B. (1990). Static type analysis of Prolog procedures for ensuring
correctness. In Deransart, P., & Maluszynski, J. (Eds.), Programming Languages
Implementation and Logic Programming, Vol. 456 of LNCS, pp. 222{237, Linkoping,
Sweden. Springer-Verlag.
De Schreye, D., & Decorte, S. (1994). Termination of logic programs: The never-ending
story. The Journal of Logic Programming, 19 & 20, 199{260.
Debray, S., Lopez-Garca, P., Hermenegildo, M., & Lin, N.-W. (1997). Lower bound cost
estimation for logic programs. In Maluszynski, J. (Ed.), Proceedings of the International Symposium on Logic Programming (ILPS-97), pp. 291{306, Cambridge. MIT
Press.
Debray, S. K., & Lin, N.-W. (1993). Cost analysis of logic programs. ACM Transactions
on Programming Languages and Systems, 15 (5), 826{875.
Debray, S. K., & Warren, D. S. (1988). Automatic mode inference for logic programs. The
Journal of Logic Programming, 5, 207{229.
Dejong, G., & Mooney, R. (1986). Explanation-based learning: An alternative view. Machine Learning, 1, 145{176.
Deville, Y. (1990). Logic Programming: Systematic Program Development. International
Series in Logic Programming, Addison-Wesley.
Etzioni, O. (1991). STATIC: A problem-space compiler for PRODIGY. In Dean, Thomas
L.; McKeown, K. (Ed.), Proceedings of the 9th National Conference on Artificial
Intelligence, pp. 533{540, Anaheim, California. MIT Press.
Etzioni, O. (1993). Acquiring search-control knowledge via static analysis. Artificial Intelligence, 62, 255{301.
Greiner, R., & Orponen, P. (1996). Probably approximately optimal satisficing strategies.
Artificial Intelligence, 82 (1-2), 21{44.
Henrard, J., & Le Charlier, B. (1992). FOLON: An environment for declarative construction
of logic programs. In Bruynooghe, M., & Wirsing, M. (Eds.), Proceedings of the Fourth
International Symposium on Programming Language Implementation and Logic Programming, Vol. 631 of LNCS, pp. 217{231, Leuven, Belgium. Springer-Verlag.
94

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Itai, A., & Makowsky, J. A. (1987). Unification as a complexity measure for logic programming. The Journal of Logic Programming, 4, 105{117.
Knuth, D. E. (1973). The Art Of Computer Programming, Vol. 3. Addison-Wesley, Reading,
Mass.
Kowalski, R. A. (1979). Algorithm = Logic + Control. Communications of the ACM, 22(7),
424{436.
Laird, P. D. (1992). Ecient dynamic optimization of logic programs. In Proceedings of
the ML92 Workshop on Knowledge Compilation and Speedup Learning Aberdeen,
Scotland.
Langley, P. (1985). Learning to search: From weak methods to domain-specific heuristics.
Cognitive Science, 9, 217{260.
Lavrac, N., & Dzeroski, S. (1994). Inductive Logic Programming: Techniques and Applications. Artificial Intelligence. Ellis Harwood, New York.
Ledeniov, O., & Markovitch, S. (1998a). Controlled utilization of control knowledge for
speeding up logic inference. Tech. rep. CIS9812, Technion, Haifa, Israel.
Ledeniov, O., & Markovitch, S. (1998b). Learning investment functions for controlling the
utility of control knowledge. In Proceedings of the Fifteenth National Conference on
Artificial Intelligence, pp. 463{468, Madison, Wisconsin. Morgan Kaufmann.
Lenat, D. B. (1995). CYC: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38 (11), 33{38.
Lloyd, J. W. (1987). Foundations of Logic Programming (Second edition). Springer-Verlag,
Berlin.
Markovitch, S., & Scott, P. D. (1989). Automatic ordering of subgoals | a machine learning
approach. In Lusk, E. L., & Overbeek, R. A. (Eds.), Proceedings of the North American
Conference on Logic Programming, pp. 224{242, Cleveland, Ohio. MIT Press.
Markovitch, S. (1989). Information Filtering: Selection Mechanisms in Learning Systems.
Ph.D. thesis, EECS Department, University of Michigan.
Markovitch, S., & Scott, P. D. (1993). Information filtering: Selection mechanisms in
learning systems. Machine Learning, 10, 113{151.
Minker, J. (1978). Search strategy and selection function for an inferential relational system.
In ACM Transactions on Database Systems, Vol. 3, pp. 1{31.
Minton, S. (1988). Learning Search Control Knowledge: An Explanation-Based Approach.
Kluwer, Boston, MA.
Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. (1986). Explanation-based generalization: A unifying view. Machine Learning, 1, 47{80.
95

fiLedeniov & Markovitch

Mooney, R. J., & Zelle, J. M. (1993). Combining FOIL and EBG to speed-up logic programs.
In Bajcsy, R. (Ed.), Proceedings of The Thirteenth International Joint Conference for
Artificial Intelligence, pp. 1106{1111, Chambery, France. Morgan Kaufmann.
Morris, K. A. (1988). An algorithm for ordering subgoals in NAIL!. In Proceedings of the
Seventh ACM SIGACT-SIGMOD Symposium on Principles of Database Systems, pp.
82{88, Austin, TX. ACM Press, New York.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory and methods.
The Journal of Logic Programming, 19 & 20, 629{680.
Naish, L. (1984). MU-Prolog 3.1db Reference Manual. Dept. of Computer Science, Univ.
of Melbourne.
Naish, L. (1985a). Automatic control for logic programs. The Journal of Logic Programming,
3, 167{183.
Naish, L. (1985b). Prolog control rules. In Joshi, A. (Ed.), Proceedings of the 9th International Joint Conference on Artificial Intelligence, pp. 720{723, Los Angeles, CA.
Morgan Kaufmann.
Natarajan, K. S. (1987). Optimizing backtrack search for all solutions to conjunctive problems. In McDermott, J. (Ed.), Proceedings of the 10th International Joint Conference
on Artificial Intelligence, pp. 955{958, Milan, Italy. Morgan Kaufmann.
Nie, X., & Plaisted, D. A. (1990). Experimental results on subgoal ordering. In IEEE
Transactions On Computers, Vol. 39, pp. 845{848.
Pettorossi, A., & Proietti, M. (1994). Transformation of logic programs: Foundations and
techniques. The Journal of Logic Programming, 19 & 20, 261{320.
Pettorossi, A., & Proietti, M. (1996). Rules and strategies for transforming functional and
logic programs. ACM Computing Surveys, 28 (2), 360{414.
Porto, A. (1984). Epilog: A language for extended programming. In Campbell, J. (Ed.),
Implementations of Prolog. Ellis Harwood.
Prieditis, A. E., & Mostow, J. (1987). PROLEARN: Towards a prolog interpreter that
learns. In Forbus, Kenneth; Shrobe, H. (Ed.), Proceedings of the 6th National Conference on Artificial Intelligence, pp. 494{498, Seattle, WA. Morgan Kaufmann.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1, 81{106.
Quinlan, J. R., & Cameron-Jones, R. M. (1995). Induction of logic programs: FOIL and
related systems. New Generation Computing, Special Issue on Inductive Logic Programming, 13 (3-4), 287{312.
Simon, H. A., & Kadane, J. B. (1975). Optimal problem-solving search: All-or-none solutions. Artificial Intelligence, 6, 235{247.
Smith, D. E. (1989). Controlling backward inference. Artificial Intelligence, 39 (1), 145{208.
96

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Smith, D. E., & Genesereth, M. R. (1985). Ordering conjunctive queries. Artificial Intelligence, 26, 171{215.
Smith, D. E., Genesereth, M. R., & Ginsberg, M. L. (1986). Controlling recursive inference.
Artificial Intelligence, 30 (3), 343{389.
Somogyi, Z., Henderson, F., Conway, T., Bromage, A., Dowd, T., Jeffery, D., & al. (1996a).
Status of the Mercury system. In Proc. of the JICSLP '96 Workshop on Parallelism
and Implementation Technology for (Constraint) Logic Programming Languages, pp.
207{218, Bonn, Germany.
Somogyi, Z., Henderson, F., & Conway, T. (1996b). The execution algorithm of Mercury,
an ecient purely declarative logic programming language. Journal of Logic Programming, 29 (1{3), 17{64.
Sterling, L., & Shapiro, E. (1994). The Art of Prolog (Second edition). MIT Press, Cambridge, MA.
Tamaki, H., & Sato, T. (1984). Unfold/fold transformation of logic programs. In Tarnlund,
S.-
A. (Ed.), Proceedings of the Second International Conference on Logic Programming, pp. 127{138, Uppsala, Sweden.
Ullman, J. D., & Vardi, M. Y. (1988). The complexity of ordering subgoals. In Proceedings of
the Seventh ACM SIGACT-SIGMOD Symposium on Principles of Database Systems,
pp. 74{81, Austin, TX. ACM Press, New York.
Ullman, J. D. (1982). Principles of Database Systems. Computer Science Press, Rockville,
MD.
Vasak, T., & Potter, J. (1985). Metalogical control for logic programs. Journal of Logic
Programming, 2 (3), 203{220.
Warren, D. H. D. (1981). Ecient processing of interactive relational database queries
expressed in logic. In Zaniola, & Delobel (Eds.), Proceedings of the 7th International
Conference on Very Large Data Bases, pp. 272{281, Cannes, France. IEEE Computer
Society Press.

97

fi