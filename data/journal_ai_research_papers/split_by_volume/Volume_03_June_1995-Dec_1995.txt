Journal of Artificial Intelligence Research 3 (1995) 405-430

Submitted 6/95; published 12/95

Decision-Theoretic Foundations for Causal Reasoning

David Heckerman

heckerma@microsoft.com

Microsoft Research, One Microsoft Way
Redmond, WA 98052-6399 USA

Ross Shachter

shachter@camis.stanford.edu

Stanford University
Stanford, CA 94305-4025 USA

Abstract

We present a definition of cause and effect in terms of decision-theoretic primitives and
thereby provide a principled foundation for causal reasoning. Our definition departs from
the traditional view of causation in that causal assertions may vary with the set of decisions
available. We argue that this approach provides added clarity to the notion of cause. Also
in this paper, we examine the encoding of causal relationships in directed acyclic graphs.
We describe a special class of inuence diagrams, those in canonical form, and show its
relationship to Pearl's representation of cause and effect. Finally, we show how canonical
form facilitates counterfactual reasoning.

1. Introduction

Knowledge of cause and effect is crucial for modeling the affects of actions. For example, if
we observe a statistical correlation between smoking and lung cancer, we can not conclude
from this observation alone that our chances of getting lung cancer will change if we stop
smoking. If, however, we also believe that smoking is a cause for lung cancer, then we can
conclude that our choice whether to continue or quit smoking will affect whether we get
lung cancer.
Work by artificial intelligence researchers, statisticians, and philosophers have emphasized the importance of identifying causal relationships for purposes of modeling the effects
of actions. For example, Simon (1977), Robins (1986), Spirtes et al. (1993), and Pearl (1993,
1995) have developed graphical models of cause and effect, and have demonstrated how these
models are important for reasoning about the effects of actions. In addition, Robins (1986),
Rubin (1978), Pearl and Verma (1991), and Spirtes et al. (1993) have developed approaches
that embrace causality for learning the effects of actions from data.
One useful framework for causal reasoning is that of Pearl (1993, 1995)|herein Pearl.
Using his framework, we construct a causal graph G. The nodes in G correspond to a set
of variables U that we wish to model. Each variable has a set of mutually exclusive and
collectively exhaustive values or instances. The arcs in G represent (informal) assertions of
cause|in particular, the parents of x 2 U are direct causes of x. Pearl gives these informal
assertions of cause an operational meaning by introducing a special class of actions on the
variables U and then describing the affects of these actions using the structure of the causal
graph. Specifically, he posits that, for every variable x 2 U , there exists another variable x^,
which we call an atomic intervention on x. The variable x^ has an instance set(x) for every
instance x of x, and an instance idle. The instance set(x) corresponds to an action that
c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiHeckerman & Shachter

forces x to take on instance x and indirectly affects other variables through the change in
x. The instance idle corresponds to the action of doing nothing.1 Pearl then asserts that
the effects of atomic interventions on the variables in U are determined by the structural
equations
x = fx (PaG (x); x^; x)
for all x 2 U , where (1) PaG (x) are the parents of x in G|that is, the direct causes of x, (2)
the variables x are exogenous and mutually independent random disturbances, and (3) the
function fx has the property that x = x when x^ =set(x) regardless of the values of PaG (x)
and x . Following Pearl, we call this framework for defining cause a structural-equation
model.
Another useful framework for causal reasoning, closely related to Pearl's, is that of
Spirtes et al. (1993)|herein SGS.
Despite these and other important advances in reasoning about cause and effect, foundations for such approaches are lacking. In any framework for causal reasoning, it is important
to consider what concepts are primitive|that is, assumed to be self evident and used to
define other concepts. As much as is possible, these primitives should have simple and universal meanings so that claims of causation can be empirically tested and causal inferences
can be trusted. Unfortunately, the primitives used by Pearl, SGS, and other researchers are
not ideal in this respect.
For example, SGS take cause itself to be a primitive. Given the controversies in statistics
and other disciplines concerning the meaning of cause, we believe that a better primitive
can be found. Pearl takes random disturbance, exogenous variable, and atomic intervention
as primitives. One problem with this approach is that we need an understanding of cause
and effect to identify an intervention as atomic. To illustrate the problem, suppose we
wish to model the causal relationship between the binary variables w and h representing
whether or not a person considers himself to be wealthy and happy, respectively. Further,
suppose we can give this person a large sum of stolen money along with the knowledge that
this money is stolen. Now we ask the question: Is this action an instance of an atomic
intervention for w? If this person does not care about how he becomes wealthy, then the
answer is \yes." If this person is more typical, however, then the answer is \no," because
this action would affect both w and h directly. Thus, we must first determine whether or
not the action is a direct cause of h to determine whether or not this action is an instance
of an atomic intervention.
In this paper, we provide a principled foundation for causal reasoning. In particular,
we explicate a set of primitives from decision theory, and use these primitives to define
the concepts of cause and atomic intervention as well as those of random disturbance and
exogenous variable. These primitives are simple to understand and used uniformly across
many disciplines.
The basic idea behind our definition of cause is as follows. Following the paradigm of
decision theory, we focus on a person|the decision maker|who has one or more decisions to
make. For each variable that we wish to model in considering these decisions, we distinguish
1. We adopt a variant of Pearl's notation for the instances of an atomic intervention. In addition, whereas
Pearl calls the action set(x) an atomic intervention, we find it convenient to use this phrase to refer to
the entire variable x^.

406

fiDecision-Theoretic Foundations for Causal Reasoning

the variable as being either a decision variable or chance variable. A decision variable is a
variable whose instances correspond to possible actions among which the person can choose.
A chance variable is any other variable. This framework is similar to Pearl's, where chance
variables correspond to the variables U and decision variables correspond to interventions.
The differences are that (1) we do not require there to be a decision variable for every
chance variable, and (2) decision variables need not be atomic interventions.
Now, for simplicity, suppose that we have a model consisting of only one decision variable
d and a set of chance variables U . Imagine that we choose one of the instances of d and
subsequently observe x 2 U . If we believe that x can be different for different choices, then,
by our definition, we say that d is a cause for x. For example, suppose decision variable
s represents the decision of whether or not to continue smoking and chance variable l
represents whether or not we get lung cancer. If we believe that we will get lung cancer if
we continue smoking and that we may not get lung cancer if we quit, then we can say that
s is a cause of l.
Our definition is related to the notion of a counterfactual: a hypothetical statement or
question that can not be verified or answered through observation (Lewis, 1973; Holland,
1986). In our smoking example, we ask the question \Will deciding differently possibly
change our health outcome?" This question can not be answered by any observation, because we must either quit or continue to smoke; we can not do both. Using counterfactuals,
Rubin (1978) defines a notion of causal effect that is closely related to our definition of
cause.
The problem with most definitions of cause based on intervention is that they do not
allow chance variables to be the causes of other chance variables. Consider the variables g
and c representing a person's gender at birth and whether or not that person gets breast
cancer, respectively. Although g is a chance variable (we cannot choose our gender), we
often hear people say in natural discourse that g causes c. In general, we would like to
accommodate such assertions.
The definition of cause that we present does indeed permit chance variables to be causes.
There is, however, one catch. Namely, when we assert that a set of chance variables X is a
cause of chance variable y , we must also specify the decision or decisions that bring about
the possible changes in X and y . In our breast-cancer problem, we can assert that g causes
c, but we must explicate a decision that possibly leads to a change in gender and breast
cancer. For example, we can say that g causes c with respect to decision variable d, where
d represents the decision of whether or not to perform genetic surgery at conception.
By including a decision context in assertions of cause, our definition departs from the
traditional view of causation. Nonetheless, this departure makes causal assertions more
precise. For example, consider another decision that will likely lead to a change in gender:
a decision o of whether or not to have a sex-change operation at birth. In this case, it may be
reasonable to assert that g is not a cause of c with respect to o. Thus, causal relationships
among chance variables may depend on the decisions available for intervention; and our
definition accommodates this dependence.
Our paper is organized into four parts. In part 1 (Sections 2 and 3), we develop our
definition of cause, using the decision-theoretic primitives of Savage (1954). In Section 2,
we introduce a simpler relation than cause, which we call limited unresponsiveness. In
Section 3, we define cause in terms of limited unresponsiveness.
407

fiHeckerman & Shachter

In part 2 (Sections 4 through 7), we address the graphical representation of cause. In
Section 4, we review a directed-acyclic-graph (DAG) representation, known as an inuence
diagram, which has been used for two decades by decision analysts to model the effects
of decisions (Howard and Matheson, 1981). We demonstrate the inadequacies of the inuence diagram as a representation of cause. In the following three sections, we develop
a special condition on the inuence diagram, known as canonical form, that improves the
representation of cause.
In part 3 (Section 8), we use our definitions of cause, atomic intervention, and mapping
variable, along with canonical form to build a correspondence with (and thus a foundation
for) Pearl's causal framework.
In part 4 (Section 9), we demonstrate an important use of canonical form. Namely, we
show how to use inuence diagrams in canonical form to do general counterfactual reasoning.
We present our framework in the traditional decision-analytic paradigm of a \one shot"
decision. In particular, we do not consider experimental studies, where variables are measured repeatedly. Nonetheless, one can easily extend our framework to such situations by
introducing the assumption of exchangeability (de Finetti, 1937). Bayesian methods for
learning models of cause that are based on this approach are discussed in Angrist et al.
(1995) and Heckerman (1995).

2. Unresponsiveness

In this section, we introduce the notion of limited unresponsiveness, a fundamental relation
that we use to define cause. We define limited unresponsiveness using primitives from
decision theory as explicated (for example) by Savage (1954).
We begin with a description of the primitives act, consequence, and possible state of the
world. Savage describes and illustrates these concepts as follows:
To say that a decision is to be made is to say that one or more acts is to
be chosen, or decided on. In deciding on an act, account must be taken of the
possible states of the world, and also of the consequences implicit in each act for
each possible state of the world. A consequence is anything that may happen to
the person.
Consider an example. Your wife has just broken five good eggs into a bowl
when you come in and volunteer to finish making the omelet. A sixth egg, which
for some reason must either be used for the omelet or wasted altogether, lies
unbroken beside the bowl. You must decide what to do with this unbroken
egg. Perhaps it is not too great an oversimplification to say that you must
decide among three acts only, namely, to break it into the bowl containing the
other five, to break it into a saucer for inspection, or to throw it away without
inspection. Depending on the state of the egg, each of these three acts will have
some consequence of concern to you, say that indicated by Table 1.
For purposes of our discussion, there are two points to emphasize from Savage's exposition. First, it is important to distinguish between that which we can choose|namely,
acts|and that which we can not choose|namely, consequences. Second, once we choose
an act, the consequence that occurs is logically determined by the state of the world. That
408

fiDecision-Theoretic Foundations for Causal Reasoning

state of the
world
break into bowl
good egg six-egg omelet
bad egg

act
break into saucer
throw away
six-egg omelet and a five-egg omelet and one
saucer to wash
good egg destroyed
no omelet and five good five-egg omelet and a five-egg omelet
eggs destroyed
saucer to wash

Table 1: An example illustrating acts, possible states of the world, and consequences.
(Taken from Savage [1954].)
is, the consequence is a deterministic function of the act and the state of the world. Of
course, the consequence can be (and usually is) uncertain, and this uncertainty is captured
by uncertainty in the state of the world. The concepts of act, consequence, and state of
the world, together with the deterministic mapping from act and state of the world to
consequence are our only primitives.2
In the omelet story, the possible states of the world readily come to mind given the
description of the problem. Furthermore, we can observe the state of the world (i.e., the
condition of the egg). In many if not most situations, however, the state of the world is
unobservable. That is, the assertion \the state of the world is x" is a counterfactual. In
these situations, we can bring the possible states to mind by thinking about the acts and
consequences. For example, suppose we have a decision to continue smoking or quit, and
we model the consequences of getting cancer or not. These acts and consequences bring to
mind four possible states of the world, as shown in Table 2. These possible states have no
familiar names; and we simply label them with numbers. The actual state of the world is
not observable, because, if we decide to quit, then we won't know for sure what would have
happened had we continued, and vice versa.
The acts and consequences in this problem may actually bring to mind more than four|
even an infinite number|of states of the world. For example, the state of the world may
correspond to degree of susceptibility of lung tissue to tar as measured by a biochemical
assay. Nonetheless, given the discrete acts and consequences that we have chosen to model
in the problem, the four states in Table 2 are suciently detailed. Savage recognizes this
issue of detail in his definition of state of the world: \a description of the world, leaving no
relevant aspect undescribed." In general, if we have a decision problem with c consequences
and a acts, then at most ca possible states of the world need be distinguished.3
The idea that the state of the world may not be observable can be traced to Neyman
(1923), who derived statistical methods for estimating the differences in yields of different
crops planted on the same plot of land, in circumstances where only one crop was actually
planted on a plot. Rubin (1978) and Howard (1990) have formalized this idea.
2. Savage (1954) defines an act to be \a function attaching a consequence to each state of the world." In
contrast, we take act to be a primitive, as do many decision analysts (e.g., Howard, 1990).
3. When acts and consequences are continuous, the specification of S is more complicated. In this paper,
we address only situations where acts and consequences are discrete.

409

fiHeckerman & Shachter

state of the
world
1
2
3
4

act
continue
quit
cancer
no cancer
no cancer no cancer
cancer
cancer
no cancer cancer

Table 2: The four possible states of the world for a decision to continue or quit smoking.
In practice, it is often cumbersome if not impossible to reason about a monolithic set
of acts, possible states of the world, or consequences. Therefore, we typically describe each
of these items in terms of a set of variables that take on two or more values or instances.
We call a variable describing a set of consequences a chance variable. For example, in
the omelet story, we can describe the consequences in terms of three chance variables: (1)
number of eggs in the omelet?4 (o) having instances zero, five, and six, (2) number of
good eggs destroyed? (g ) having instances zero, one, and five, and (3) saucer to wash? (s)
having instances no and yes. That is, every consequence corresponds to an assignment of
an instance to each chance variable.
We call a variable describing a set of acts a decision variable (or decision, for short).
For example, suppose we have a set of possible acts about how we are going to dress for
work. In this case, we can describe the acts in terms of the decision variables shirt (plain
or striped), pants (jeans or corduroy), and shoes (tennis shoes or loafers). In this
example and in general, every act corresponds to a choice of an instance for each decision
variable.
The description of possible states of the world in terms of component variables is a bit
more complicated, and is not needed for our explication of unresponsiveness and limited
unresponsiveness. We defer discussion of this issue to Section 6.
As a matter of notation, we use D to denote the set of decisions that describe the acts for
a decision problem, and lower-case letters (e.g., d; e; f ) to denote individual decisions in the
set D. Also, we use U to denote the set of chance variables that describe the consequences,
and lower-case letters (e.g., x; y; z ) to denote individual chance variables in U . In addition,
we use the variable S to denote the state of the world (the instances of S correspond to
the possible states of the world).5 Thus, any given decision problem|or domain, as we
sometimes call it|is described by the variables U , D, and S .6
With this introduction, we can discuss the concept of limited unresponsiveness. To
illustrate this concept, consider the following decision problem adapted from Angrist et al.
(1995). Suppose we are a physician who has to decide whether to recommend for or against
a particular treatment. Given our recommendation, our patient may or may not actually
4. To emphasize the distinction between chance and decision variables, we put a question mark at the end
of the names of chance variables.
5. We use an uppercase \S " to denote this single variable, because later we decompose S into a set of
variables.
6. Sometimes, for simplicity, we leave S implicit in the specification of a decision problem.

410

fiDecision-Theoretic Foundations for Causal Reasoning

r (recommendation)
take
don't take
t (taken?) c (cured?) t (taken?) c (cured?)
1: complier, helped
yes
yes
no
no
2: complier, hurt
yes
no
no
yes
3: complier, always cured
yes
yes
no
yes
4: complier, never cured
yes
no
no
no
5: defier, helped
no
no
yes
yes
6: defier, hurt
no
yes
yes
no
7: defier, always cured
no
yes
yes
yes
8: defier, never cured
no
no
yes
no
9: always taker, cured
yes
yes
yes
yes
10: always taker, not cured
yes
no
yes
no
11: never taker, not cured
no
no
no
no
12: never taker, cured
no
yes
no
yes
13: (impossible)
yes
yes
yes
no
14: (impossible)
yes
no
yes
yes
15: (impossible)
no
no
no
yes
16: (impossible)
no
yes
no
no
S (state of the world)

Table 3: A decision problem about recommending a medical treatment.
accept the treatment, and may or may not be cured as a result. Here, we use a single
decision variable recommendation (r) to represent our acts (i.e., D = frg), and two chance
variables taken? (t) and cured? (c) to represent whether or not the patient actually accepts
the treatment and whether or not the patient is cured, respectively (i.e., U = ft; cg).
The possible states of the world for this problem are shown in Table 3. For example,
consider the first row in the table. Here, the patient will accept the treatment if and only if
we recommend it, and will be cured if and only if he takes the treatment. We describe this
state by saying that the patient is a complier and is helped by the treatment. We discuss
the description of these states in more detail in Section 6.
As is indicated in the table, suppose that we believe the last four states of the world are
impossible (i.e., have a probability of zero). These last four states share the property that
t takes on the same instance for both acts, whereas c does not. Thus, this decision problem
satisfies the following property: in all of the states of the world that are possible, if t is the
same for the two acts, then c is also the same. We say that c is unresponsive to r in states
limited by t.
In general, suppose we have a decision problem described by variables U , D, and S .
Let X be a subset of U , and Y be a subset of U [ D. We say that X is unresponsive
to D in states limited by Y if we believe that, for all possible states of the world, if Y
assumes the same instance for any two acts then X must also assume the same instance
for those acts. We describe the notion of limited unresponsiveness in earlier work in terms
411

fiHeckerman & Shachter

of a conditional fixed set (Heckerman and Shachter, 1994). Angrist et al. (1995) discuss an
instance of limited unresponsiveness, which they call the exclusion restriction.
To be more formal, let X [S; D] be the instance that X assumes (with certainty) given
the state of the world S and the act D. For example, in the omelet story, if S is the state of
the world where the egg is good, and D is the act throw away, then o[S; D] (the number
of eggs in the omelet) assumes the instance five. Then, we have the following definition.

Definition 1 (Limited (Un)responsiveness) Given a decision problem described by
chance variables U , decision variables D, and state of the world S , and variable sets X  U
and Y  D [ U , X is said to be unresponsive to D in states limited by Y , denoted X 6 -Y D,
if we believe that

8 S 2 S; D1 2 D; D2 2 D : Y [S; D1] = Y [S; D2] =) X [S; D1] = X [S; D2]
X is said to be responsive to D in states limited by Y , denoted X -Y D, if it is not the
case that X is unresponsive to D in states limited by Y |that is, if we believe that
9 S 2 S; D1 2 D; D2 2 D s:t: Y [S; D1] = Y [S; D2] and X [S; D1] =6 X [S; D2]
When X is (un)responsive to D in states limited by Y = ;, we simply say that X is
(un)responsive to D. The notion of unresponsiveness is significantly simpler than that of
limited unresponsiveness. That is, when Y = ;, the equalities on the left-hand-side of the
implications in Definition 1 are trivially satisfied. Thus, X is unresponsive to D if we believe
that, in each possible state of the world, X assumes the same instance for all acts; and X is
responsive to D if there is some possible state of the world where X differs for two different
acts.
As examples of responsive variables, consider the omelet story. Let S denote the state
where the egg is good, and D1 and D2 denote the acts break into bowl and throw away,
respectively. Then, for the variable o (number of eggs in omelet?), we have o[S; D1] =six and
o[S; D2] =five. Consequently, o is responsive to D.7 In a similar manner, we can conclude
that g (number of good eggs destroyed?), and s (saucer to wash?) are each responsive to D.
Note that if a chance variable x is responsive to D, then|to some degree|it is under
the control of the decision maker. Consequently, the decision maker can not observe x prior
to choosing an act for D. For example, in the omelet story, we can not observe any of the
responsive variables o, g , or s before choosing an act.8
As an example of an unresponsive variable, suppose we include S (the state of the world)
as a variable in U . (E.g., in the omelet story, we can take U to be fS; o; g; sg.) By Savage's
definition of S , it must be unresponsive to D. Note that including S in U creates no new
states of the world.
As we have discussed, the notions of unresponsiveness and limited unresponsiveness are
closely related to concepts in counterfactual reasoning. When we determine whether or not
7. Technically, we should say that fog is responsive to D. For simplicity, however, we usually drop set
notation for singletons.
8. To be more precise, the variable o represents the number of eggs in the omelet after we choose an act
for D. This variable should not be confused with another variable{say o |corresponding to the number
of eggs in the omelet before we choose D. Whereas o is responsive to D and cannot be observed before
choosing an act, o is unresponsive to D and can be observed before choosing D.
0

0

412

fiDecision-Theoretic Foundations for Causal Reasoning

a chance variable x is unresponsive to decisions D, we essentially answer the query \Will the
outcome of x be the same no matter how we choose D?" Furthermore, when we determine
whether or not x is unresponsive to D in states limited by Y , we answer the query \If Y
will not change as a result of our choice for D, will the outcome of x be the same?" One
of the fundamental assumptions of our work presented here is that these counterfactual
queries are easily answered. In our experience, we have found that decision makers are
indeed comfortable answering such restricted counterfactual queries.
The concepts of responsiveness and probabilistic independence are related, as illustrated
by the following theorem.

Theorem 1 If a set of chance variables X is unresponsive to a set of decision variables D,
then X is probabilistically independent of D.

Proof: By definition of unresponsiveness, X assumes the same instance for all acts in any
possible state of the world. Consequently, we can learn about X by observing S , but not
by observing D. 2
Nonetheless, the two concepts are not identical. In particular, the converse of Theorem 1
does not hold. For example, let us consider the simple decision of whether to bet heads or
tails on the outcome of a coin ip. Assume that the coin is fair (i.e., the probabilities of
heads and tails are both 1/2) and that the person who ips the coin does not know our
bet. Here, the possible outcomes of the coin toss correspond to the possible states of the
world. Further, let decision variable b denote our bet, and chance variable w describe the
possible consequences that we win or not. In this situation, w is responsive to b, because for
both possible states of the world, w will be different for the different bets. Nonetheless, the
probability of w is 1/2, whether we bet heads or tails. That is, w and b are probabilistically
independent.
Limited unresponsiveness and conditional independence are less closely related than are
their unqualified counterparts. Namely, limited unresponsiveness does not imply conditional
independence. For example, in the medical-treatment story, c (cured?) is unresponsive to r
(recommendation) in states limited by t (taken?), but it is reasonable for us to believe that
c and r are not independent given t, perhaps because there is some factor that|partially
or completely|determines how a person reacts to both recommendations and treatment.
We can derive several interesting properties of limited unresponsiveness from its definition.
1. X 6 -Y D () 8x 2 X; x 6 -Y D
2. X 6 -W D () X [ W 6 -W D
3. X 6 -D D
4. X 6 -Y D =) X 6 -Y [Z D
5. X 6 -Y [Z D and Y 6 -Z D =) X 6 -Z D
6. X -Z D and W 6 -Z D =) X -W [Z D
413

fiHeckerman & Shachter

where D is the set of decision variables in the domain, X and W are arbitrary sets of chance
variables in U , and Y and Z are arbitrary sets of variables in U [ D.
The proofs of these properties are straightforward. For example, consider property 5.
Given X 6 -Y [Z D, we have

8 S 2 S; D1 2 D; D2 2 D : Y [S; D1] = Y [S; D2] and Z [S; D1] = Z [S; D2]
Given Y 6 -Z D, we have

=) X [S; D1] = X [S; D2]

8 S 2 S; D1 2 D; D2 2 D : Z [S; D1] = Z [S; D2] =) Y [S; D1] = Y [S; D2]
Consequently, we obtain

8 S 2 S; D1 2 D; D2 2 D : Z [S; D1] = Z [S; D2] =) X [S; D1] = X [S; D2]
That is, X 6 -Z D.
Other properties follow from these. For example, it is true trivially that ; 6 -Y D.
Consequently, by Property 2, we know that Y 6 -Y D. As another example, a special case
of Property 4 is that whenever X is unresponsive to D, then X will be unresponsive to D
in states limited by any Z . Also, Properties 4 and 5 imply that limited unresponsiveness is
transitive: X 6 -Y D and Y 6 -Z D imply X 6 -Z D.

In closing this section, we note that the definition of limited unresponsiveness can be
generalized in several ways. In one generalization, we can define what it means for X  U to
be unresponsive to D in states of the world limited by Y , a set of instances of Y . Namely, we
say that X is unresponsive to D in states limited by Y if, for all possible states of the world
S, and for any two acts D1 and D2 , Y [S; D1] = Y [S; D2] 2 Y implies X [S; D1] = X [S; D2].
In a second generalization, we can define what it means for a set of chance variables to
be unresponsive to a subset of all of the decisions. In particular, given a domain described
by U and D, we say that X  U is unresponsive to D0  D in states limited by Y if
X 6 -Y [(DnD ) D.
0

3. Definition of Cause

Given the notion of limited unresponsiveness, we can formalize our definition of cause.

Definition 2 (Causes with Respect to Decisions) Given a decision problem described
by U and D, and a variable x 2 U , the variables C  D [ U n fxg are said to be causes for
x with respect to D if C is a minimal set of variables such that x 6 -C D.
In our framework, decision variables can not be caused, because they are under the
control of the decision maker. Consequently, we define causes for chance variables only.
Also, as we have discussed, our definition is an extension of existing intervention-based
definitions of cause (e.g., Rubin [1978]) in that we allow causes to include chance variables.
In addition, our definition of cause departs from traditional usage of the term in that causeeffect assertions may vary with the set of decisions available. We discuss the advantages of
this departure shortly.
414

fiDecision-Theoretic Foundations for Causal Reasoning

As an example of our definition, consider the decision to continue or quit smoking,
described by the decision variable s (smoke) and the chance variable l (lung cancer?). If we
believe that s and l are probabilistically dependent, then, by Theorem 1, it must be that
l - s. Furthermore, by Property 3, we know that l 6 -s s. Consequently, by Definition 2,
we have that s is a cause of l with respect to s.
As another example, consider the medical-treatment story. We have that c (cured?)
is responsive to r (recommendation), because (among other reasons) in the first row in
Table 3, the patient is cured if and only if we recommend the treatment. Furthermore, as
we discussed in the previous section, c is unresponsive to r in states limited by t (taken?).
Consequently, we have that t is a cause of c with respect to r.
The advantage of defining cause relative to decisions is made clear by our breast-cancer
example given in the introduction. Let g and c denote the chance variables gender? and
breast cancer?, respectively. Now, imagine two decisions available to alter gender: o, a
decision to have a sex-change operation at birth, and d, a decision to change chromosomes
at conception by microsurgery. It is possible for someone to believe that c 6 - o and yet
c - d and c 6 -g d. That is, it is possible for someone to believe that gender is a cause of
breast cancer with respect to the chromosome change but not with respect to the sex-change
operation. In this situation, it does not make sense to make the unqualified statement
\gender is a cause of breast cancer." In general, our decision-based definition provides
added clarity.
Several consequences of Definition 2 are worth mentioning. First, although cause is
irreexive by definition, it is not always asymmetric. For example, in our story about the
coin toss, consider another variable m that represents whether or not the outcome of the
coin toss matches our bet b. In the story as we have told it, m is a deterministic function of
w (win?), and vice versa. Consequently, we have w 6 -m b and m 6 -w b; and so m is a cause
of w and w is cause of m with respect to b. Note that any hint of uncertainty destroys this
symmetry. For example, if there is a possibility that the person tossing the coin will cheat
(so that we may lose even if we match), then we can conclude that m is a cause of w, but
not vice versa. This symmetry would also be destroyed if we had a decision controlling w
to which m is unresponsive.
Second, cause is transitive for single variables. In particular, if x is a cause for y and y
is a cause for z with respect to D, then z -D and (by the transitivity of unresponsiveness)
z 6 -x D. Consequently, x is a cause for z with respect to D. Note that transitivity does not
necessarily hold for causes containing sets of variables, because the minimality condition in
Definition 2 may not be satisfied.
Third, C = ; is a set of causes for x with respect to D if and only if x is unresponsive
to D.
Fourth, we have the following theorem, which follows from Definition 2 and several of
the properties of limited unresponsiveness given in Section 2.
Theorem 2 Given any x 2 U , if C is a set of causes for x with respect to D, and w 2 C \U ,
then w must be responsive to D.
Proof: For any chance variable w 2 C , let C 0 = C n fwg. By the minimality condition in
our definition, we have
x -C D
(1)
0

415

fiHeckerman & Shachter

Suppose that w 6 - D. Then, by Property 4, we have

w 6 -C D
0

(2)

Applying Equations 1 and 2 to Property 6, we have that x -C D, which contradicts that
C is a set of causes for x with respect to D. 2
To illustrate the use of this theorem, let us extend the medical-treatment example by
imagining that there is some gene that affects how a person reacts to both our recommendation and to therapy. In this situation, it is reasonable for us to assert that the variable g
(genotype?) is unresponsive to r. Thus, by Theorem 2, g can not be among the causes for
any variable.
This consequence of our definition may seem unappealing. Intuitively, we would like to
be able to say that (in some sense) g is a cause of c. Indeed, our definition does not preclude
the ability to make such assertions. Namely, there is no reason to require that the decisions
D be implementable in practice or at all. If we want to think about whether or not the
patient's genotype is a cause for his cure, then we can imagine an action that can alter
one's genetic makeup|for example, retroviral therapy (v ). In this case, it is reasonable to
conclude that fr; g g is a cause for t with respect to the decisions fr; v g. Nonetheless, as
we have discussed, we must be clear about the action(s) that alter genotype to make this
statement of cause precise.
Finally, we can generalize our definition of what it means for a set of variables to cause
x to a definition of what it means for a set of instances to cause x. Namely, we say that
C , a set of instances of C , is a cause for x 2= C with respect to D if C is a minimal set of
variables such that x is unresponsive to D in states limited by C . That is, C is a cause for
x with respect to D if we replace our definition of cause with the weaker requirement that
x be unresponsive to D in states limited by C .

4. Inuence Diagrams
In this and the following three sections, we examine the graphical representation of cause
within our framework. This study is useful in its own right, and also will help to relate our
framework with Pearl's structural equation model. We begin, in this section, with a review
of the inuence-diagram representation.
An inuence diagram is (1) a acyclic directed graph G containing decision and chance
nodes corresponding to decision and chance variables, and information and relevance arcs,
representing what is known at the time of a decision and probabilistic dependence, respectively, (2) a set of probability distributions associated with each chance node, and optionally
(3) a utility node and a corresponding set of utilities (Howard and Matheson, 1981).
An information arc is one that points to a decision node. An information arc from chance
or decision node a to decision node d indicates that variable a will be known when decision
d is made. (We shall use the same notation to refer to a variable and its corresponding
node in the diagram.) A relevance arc is one that points to a chance node. The absence of
a possible relevance arc represents conditional independence. To identify relevance arcs, we
start with an ordering of the variables in U = (x1; : : :; xn ). Then, for each variable xi in
order, we ask the decision maker to identify a set PaG (xi )  fx1; : : :; xi,1 ; Dg that renders
416

fiDecision-Theoretic Foundations for Causal Reasoning

xi and fx1; : : :; xi,1; Dg conditionally independent. That is,
p(xijx1; : : :; xi,1; D; ) = p(xijPaG (xi ); )

(3)

where p(X jY;  ) denotes the probability distribution of X given Y for a decision maker with
background information  . For every variable z in PaG (xi ), we place a relevance arc from
z to xi in graph G of the inuence diagram. That is, the nodes PaG (xi) are the parents of
xi in G.
Associated with each chance node xi in an inuence diagram are the probability distributions p(xijPaG (xi );  ). From the chain rule of probability, we know that
n
Y
p(x1; : : :; xn jD; ) = p(xijx1; : : :; xi,1; D; )
i=1

(4)

Combining Equations 3 and 4, we see that any inuence diagram for U [ D uniquely
determines a joint probability distribution for U given D. That is,

p(x1; : : :; xnjD; ) =

Yn p(xijPaG(xi); )

i=1

(5)

Inuence diagrams may also contain special chance nodes. A deterministic node corresponds to variable that is a deterministic function of its parents. A utility node encodes
preferences of the decision maker. Finally, an inuence diagram is unambiguous when its
decision nodes are totally ordered|that is, when there is a directed path in the inuence
diagram that traverses all decisions. This total order corresponds to the order in which
decisions are made.
In this paper, we concern ourselves neither with the ordering of decision nodes nor the
observation of chance variables before making decisions. Therefore, we are not concerned
with information arcs. Likewise, although our new concepts apply to models that include
utility nodes, we can illustrate these concepts with models containing only chance, deterministic, and decision variables.
Figure 1a contains an inuence diagram for the omelet story. As is illustrated in the
figure, we use ovals, double ovals, and squares to represent chance, deterministic, and
decision nodes, respectively. Among the possible relevance arcs in the inuence diagram,
several are missing. For example, there is no arc from D to S , representing the independence
of D and S (which follows from the assertion that S is unresponsive to D). Figures 1b and
1c contain inuence diagrams for the medical-treatment example. The chance variable g
(genotype?) is explicitly modeled in Figure 1c.
The ordinary inuence diagram was designed to be a representation of conditional independence. Furthermore, as we have discussed, the concepts of conditional independence
and limited unresponsiveness are only loosely related. Consequently, the inuence diagram
is an inadequate representation of causal dependence, at least by our definition of cause.
In particular, an inuence diagram may contain an arc from node x to node y , even
though x is not among a set of causes for y . For example, the inuence diagram of Figure 1b
has an arcs from r and t to c due to the dependencies in the domain. Nonetheless, we have
established that the singleton ftg is a cause for c with respect to r.
417

fiHeckerman & Shachter

decision

D

state of the
world (egg)

number of eggs
in omelet?

S

recommendation

r

r

treatment?

t

t

o

number of good
eggs destroyed?

g

saucer to wash?

s

genotype?
g

cured?

(a)

c

(b)

c

(c)

Figure 1: Inuence diagrams for (a) the omelet story, and (b,c) the medical-treatment example.
Furthermore, an inuence diagram may contain no arc from x to y , even though x is a
cause of y . For example, consider the coin example, illustrated by the inuence diagram in
Figure 2a. If we believe that the coin is fair, and if we do not bother to model the variable
c explicitly (as shown in Figure 2b), then we need not place an arc from d to w, because
the probability of winning will be 1=2, regardless of our choice d. Nonetheless, b is a cause
for w with respect to b, by our definition.
Despite these limitations, the inuence diagram is adequate for purposes of making decisions under uncertainty. In the introduction, we argued that causal information is needed
for predicting the effects of actions. Thus, the question arises: \Why do we need anything
more than the inuence diagram as a representation of the effects of actions?" We give
an answer to this question in Section 9, where we discuss counterfactual reasoning. There,
we show that the ordinary inuence diagram is inadequate for purposes of counterfactual
reasoning unless it is in canonical form|a form that accurately reects cause.

5. Direct and Atomic Interventions
In order to define canonical form, we need the concept of a mapping variable. Likewise, in
order to define a mapping variable, we need the concept of atomic intervention. We also
need the concept of atomic intervention to explicate Pearl's structural-equation model. In
this section, we define atomic intervention along with a more general concept called direct
intervention.
Roughly speaking, we say that a set of decisions I is a direct intervention on a set of
chance variables X if the effects of I on all chance variables are mediated only through
the effects of I on X . SGS, who take cause to be a primitive, provide a formal definition
of direct intervention (which they call a direct manipulation) that is consistent with our
notion. We find it simpler to define direct intervention in terms of limited unresponsiveness.
418

fiDecision-Theoretic Foundations for Causal Reasoning

bet

b

bet
coin
c

win

b

win

w

w

(a)

(b)

Figure 2: Inuence diagrams for betting on a coin ip.

Definition 3 (Direct Intervention) Given a domain described by U and D, a set of
decisions I  D is said to be a direct intervention on X  U with respect to D if (1) for
all x 2 X , x - I , and (2) for all y 2 U , y 6 -X I .
For example, in the medical-treatment story, r is a direct intervention on t, because
t - r and c 6 -t r. As another example, suppose the physician has an additional decision
p of whether or not to pay the patient to take the treatment. It is reasonable to expect
that t - p. Furthermore, if the amount of payment is small, it is reasonable that c 6 -t p.
Consequently, p qualifies as a direct intervention on t. Nonetheless, if the amount of payment
is suciently large, the patient may use that money to improve his health care. Thus,
c -t p; and p does not satisfy the condition 2 for a direct intervention on t.
Given the notion of direct intervention, we can define atomic intervention.

Definition 4 (Atomic Intervention) Given a domain described by U and D, a decision
x^ 2 D is said to be an atomic intervention on x 2 U with respect to D if (1) fx^g is a direct
intervention on fxg with respect to D, and (2) x^ has precisely the instances (a) idle, which
corresponds to the instance of doing nothing to x, and (b) set(x) for every instance x of x,
where x = x whenever x^ =set(x).
As we discussed in the introduction, Pearl takes the concept of atomic intervention
to be primitive. Whether or not a decision is a direct (or atomic) intervention, however,
depends on the underlying causal relationships in the domain. In the medical-treatment
story, suppose the physician has a decision k of whether or not to administer the treatment
(a drug) without the patient's knowledge. If we believe that the treatment is truly effective
and has no placebo effect, then we can assert that k is a direct intervention on t. If,
however, we believe that the treatment has only a placebo effect, then k will not be a direct
intervention on t, because k will also directly affect c. Thus, the notions of direct and
atomic intervention require definitions, lest the meaning of cause would be hidden in these
primitives.
We note that, when there are bi-directional causal relationships among variables in U ,
it is not always possible for every chance variable to have its own atomic intervention. For
example, consider an adiabatic system consisting of a cylindrical chamber with a moveable
419

fiHeckerman & Shachter

instance of t(r)
1: complier
2: defier
3: always taker
4: never taker

r =take r =don't take
t =yes
t =no
t =no
t =yes
t =yes
t =yes
t =no
t =no

Table 4: The mapping variable t(r).
top, in which we model the variables pressure? (p) and volume? (v ).9 If we allow the
top of the chamber to move freely, then placing various weights on the top of the chamber
constitutes an atomic intervention on p; and we have that p is a cause of v with respect to
p^. In contrast, fixing the top of the chamber at particular locations constitutes an atomic
intervention on v ; and we have that v is a cause of p with respect to v^. By the laws of
physics, however, both decisions p^ and v^ can not be available simultaneously.

6. Mapping Variables
To understand the concept of a mapping variable, let us reexamine Savage's basic formulation of a decision problem. Recall that the chance variables U are a deterministic function
of the decision variables D and the state of the world S . In effect, each possible state of the
world defines a mapping from the decisions D to the chance variables U . Thus, S represents
all possible mappings from D to U . We can characterize S as a mapping variable for U as
a function of D, and use the suggestive notation U (D) to denote this mapping variable.
In general, given a domain described by U , D, and S , a set of decision variables Y  D,
and a set of chance variables X  U , the mapping variable X (Y ) is a variable that represents
the possible mappings from Y to X .
As an example, consider the medical-treatment story. The mapping variable t(r) represents the possible mappings from the decision variable r (recommendation) to the chance
variable t (taken?). In this example, the instances of t(r), shown in Table 4, have a natural
interpretation. In particular, the instance where the patient accepts treatment if and only if
we recommend it represents a patient who complies with our recommendation; the instance
where the patient accepts treatment if and only if we recommend against it represents a
patient who defies our recommendation; and so on.
The notion of a mapping variable is discussed in Heckerman and Shachter (1994), and
in Balke and Pearl (1994) under the name \response function." A related counterfactual
variable is described by Neyman (1923), Rubin (1978), and Howard (1990). They discuss
what we would denote X (Y = Y): the variable X if we choose instance Y for Y .
An important property concerning mapping variables is that, given variables X; Y; and
X (Y ), we can always write X as a deterministic function of Y and X (Y ). For example, t is
a deterministic function of r and t(r); and U is a deterministic function of D and U (D)  S .
9. This example is not appropriate technically, as it uses continuous variables. Nonetheless, this example
illustrates our point.

420

fiDecision-Theoretic Foundations for Causal Reasoning

In the discussions that follow, it is useful to extend the definition of a mapping variable to
include chance variables as arguments. For example, in the medical-treatment story, it seems
reasonable to define the mapping variable c(t) with instances helped, hurt, always cured,
and never cured. Together, the mapping variables t(r) and c(t) describe the possible states
of the world U (D)  S . (E.g., t(r) =complier and c(t) =helped corresponds to state 1 in
Table 3.) As we shall see, this decomposition of U (D) facilitates the graphical representation
of causal relationships.
Unfortunately, defining mapping variables with chance-variable arguments is not always
possible. In the medical-treatment domain, when the patient is an always taker (states 10
and 11 in Table 3), t=yes regardless of r. Consequently, we can not tell whether c(t) is
helped or always cured|that is, c(t) is not uniquely identified. Because Savage's decisiontheoretic framework requires that the state of the world and the act uniquely determine the
instance of c(t) (a consequence), the instance of c(t) is not well defined. Nonetheless, c(t) is
well defined whenever D includes an atomic intervention on t (t^), guaranteeing that t will
take on all instances (as t^ varies) in every state of the world.
In general, we have the following definition of mapping variable.

Definition 5 (Mapping Variable) Given a domain described by U and D, chance variables X , and variables Y such that, for every y 2 Y \ U , there exists an atomic intervention
y^ 2 D,10 the mapping variable X (Y ) is the chance variable that represents all possible
mappings from Y to X .

There are several important points to be made about mapping variables as we have now
defined them. First, as in the more specific case, X is always a deterministic function of Y
and X (Y ).
Second, additional probability assessments typically are required when introducing a
mapping variable into a probabilistic model. For example, two independent assessments
are needed to quantify the relationship between r and t in the medical-treatment story;
whereas three independent assessments are required for the node t(r). In general, many
additional assessments are required. If X has c instances and Y has a instances, then
X (Y ) has as many as ca instances. In real-world domains, however, reasonable assertions
of independence decrease the number of required assessments. In some cases, no additional
assessments are necessary (see, e.g., Heckerman et al., 1994).
Third, we have the following theorem, which follows immediately from the definitions
of limited unresponsiveness and mapping variable. In this and subsequent theorems that
mention mapping variables, we assume that atomic interventions required for the proper
definition of the mapping variables are included in D.

Theorem 3 (Mapping Variable) Given a decision problem described by U and D, variables X  U , and Y  U [ D, X 6 -Y D if and only if X (Y ) 6 - D.
For example, in the medical treatment domain that includes the atomic intervention t^,
we have c 6 -t fr; ^tg and c(t) 6 - fr; ^tg. Roughly speaking, Theorem 3 says that X is
unresponsive to D in states limited by Y if and only if the way X depends on Y does not
depend on D. This equivalence provides us with an alternative set of conditions for cause.
10. Recall from Section 5 that it is not always possible to have atomic interventions for every y 2 Y .

421

fiHeckerman & Shachter

Corollary 4 (Causes with Respect to Decisions) Given a decision problem described
by U and D, and a chance variable x 2 U , the variables C  D [ U n fxg are causes for x
with respect to D if only if C is a minimal set of variables such that x(C ) 6 - D.
When C are causes for x with respect to D, we call x(C ) a causal mapping variable with
respect to D. Thus, we have the following consequence of Theorem 3.

Corollary 5 (Causal Mapping Variable) If x(C ) is a causal mapping variable for x
with respect to D, then x(C ) is unresponsive to D.

7. Canonical Form Inuence Diagrams
We can now define what it means for an inuence diagram to be in canonical form.

Definition 6 (Canonical Form) An inuence diagram for a decision problem described
by U and D is said to be in canonical form if (1) all chance nodes that are responsive to D
are descendants of one or more decision nodes and (2) all chance nodes that are descendants
of one or more decision nodes are deterministic nodes.
An immediate consequence of this definition is that any chance node that is not a descendant
of decision node must be unresponsive to D.
We can construct an inuence diagram in canonical form for a given problem by including
in the inuence diagram a causal mapping variable for every variable that is responsive to
the decisions. In doing so, we can make every responsive variable a deterministic function of
its mapping variable and the corresponding set of causes. For example, consider the medicaltreatment story as depicted in the inuence diagram of Figure 3a. The variables t and c
are responsive to r, but their corresponding nodes are not deterministic. Consequently, this
inuence diagram is not in canonical form. To construct a canonical form inuence diagram,
we introduce the mapping variables t(r) and c(r), as shown in Figure 3b. The responsive
variables are now deterministic; and the mapping variables are unresponsive to the decision.
This example illustrates an important point: Mapping variables may be probabilistically
dependent. We return to this issue in Section 8.
In general, we can construct an inuence diagram in canonical form for any decision
problem characterized by U and D as follows.

Algorithm 1 (Canonical Form)
1. Add a node to the diagram corresponding to each variable in U [ D
2. Order the variables x1; : : :; xn in U so that the variables unresponsive to D come first
422

fiDecision-Theoretic Foundations for Causal Reasoning

r

r

t

t

c

c

t(r)

c(r)

(a)

(b)

Figure 3: (a) An inuence diagram for the medical-treatment story. (b) A corresponding
inuence diagram in canonical form.
3. For each variable xi 2 U that is responsive to D,
(a) Add a causal-mapping-variable chance node xi (Ci) to the diagram,
where Ci  D [ fx1; : : :; xi,1g
(b) Make xi a deterministic node with parents Ci and xi (Ci )
4. Assess independencies among the variables that are unresponsive to D11

This algorithm is well defined, because it is always possible to find a set Ci satisfying the
condition in step 3a. In particular, xi 6 -D D by Property 3. Consequently, even when D
contains no atomic intervention, we can always create a causal mapping variable for every
responsive variable in U .
Also, the structure of any inuence diagram constructed using Algorithm 1 will be valid.
Namely, by Corollary 5, all causal mapping variables added in step 3 are unresponsive
to D. Thus, suppose we identify the relevance arcs and deterministic nodes according
to Equation 3 by using a variable ordering where the nodes in D are followed by the
unresponsive nodes (including the causal mapping variables), which are in turn followed by
the responsive nodes in the order specified at step 2. Then, (1) we would add no arcs from
D to the unresponsive nodes by Theorem 1 (and the algorithm adds none); (2) we would
add arcs among the unresponsive nodes as described in step 4; and (3) for every responsive
variable xi , we would make xi a deterministic node (as described in step 3b) by definition
of a mapping variable.
In addition, the structure that results from Algorithm 1 will be in canonical form. In
particular, because there are no arcs from D to the unresponsive nodes, only responsive
variables can be descendants of D. Also, by Theorem 2, we know that every responsive
node is a descendant of D, and (by construction) a deterministic node.
11. Because mapping variables are random variables, the assessment of dependencies among the unresponsive
variables is, in principle, no different than that for assessing dependencies among ordinary random
variables. Nonetheless, the counterfactual nature of the variables can be confusing. Howard (1990)
describes a method of probability assessment that addresses this concern.

423

fiHeckerman & Shachter

r

r
t

t

t(r, t )
t

t

g

g
c(t)
c

c

(a)

(b)

Figure 4: (a) Another inuence diagram for the medical-treatment story. (b) A corresponding inuence diagram in canonical form.
Furthermore, by construction, every responsive variable xi 2 U has one set of causes
explicitly encoded in the diagram (Ci).
To illustrate the algorithm, consider the medical-treatment story as depicted by the
inuence diagram in Figure 4a, where the variable g (genotype?) is represented explicitly,
and where c 6 -t fr; ^tg and g 6 - fr; ^tg. To construct an inuence diagram in canonical
form for this problem, we first add the variables fr; ^t; g; t; cg to the diagram and choose
the ordering (g; t; c). Both t and c are responsive to D = fr; ^tg, and have causes fr; ^tg
and t, respectively. Consequently, we add causal mapping variables t(r; ^t) and c(t) to the
new diagram, and make t a deterministic function of r, t^, and t(r; ^t) and c a deterministic
function of t and c(t). Finally, we assess the dependencies among the unresponsive variables
fg; t(r; ^t); c(t)g, adding arcs from g to t(r; ^t) and c(t) under the assumption that the causal
mapping variables are conditionally independent given g . The resulting canonical form
inuence diagram is shown in Figure 4b.
Canonical form is a generalization of Howard Canonical Form, which was developed
by Howard (1990) to facilitate the computation of value of information.12 Before making
important decisions, decision analysts investigate how useful it is to gather additional information. This investigation is typically done by computing the extra value the decision
maker would obtain by observing earlier one or more chance variables in the domain. If the
decision maker does not expect to observe chance variable x prior to making decision d, the
value of information about x is the extra value he would obtain if he were able to observe
chance variable x just before making decision d. The value of information is never negative,
and it serves as a bound on the value of any experiment: it would never be worthwhile
to spend more than the value of information about x to obtain any (possibly imperfect)
observation about x just before making decision d.
Given an ordinary inuence diagram, we can not compute the value of information about
variables responsive to D, because such variables can not be observed before decisions D
are made. In contrast, we can always compute the value of information about mapping
12. Inuence diagrams in HCF do not allow mapping variables whose arguments contain chance variables.

424

fiDecision-Theoretic Foundations for Causal Reasoning

variables corresponding to responsive variables in a canonical form inuence diagram, because such variables are unresponsive to D by definition. For example, consider the decision
to continue or quit smoking described by decision variable s (smoke) and chance variables
l (lung cancer?) and l(s). Although we cannot compute the value of information about l
because it is responsive to D, we can compute the value of information about l(s).
At first glance, it may seem pointless to determine the value of information about a
variable that cannot be observed (such as l(s)). Nonetheless, we can often learn something
about a mapping variable. For example, imagine a test that measures the susceptibility of
someone's lung tissue to lung cancer in the presence of tobacco smoke. Learning the result
of such a test may well update our probability distribution over l(s). By computing the
value of information of l(s), we obtain an upper bound on the most we would be willing to
pay to undergo such a test.

8. Pearl's Causal Framework

We can now demonstrate the relationship between Pearl's causal framework and ours. As
mentioned, Pearl's framework is similar to that of SGS (see the background notes in SGS
for a discussion). Thus, many of the remarks in the section apply to SGS's model for cause
as well. A notable exception is that SGS formally define direct intervention.
The following theorem outlines the relationship.

Theorem 6 Given chance variables U , suppose the set of decision variables D contains a
unique atomic intervention x^ for every x 2 U and no other decisions. Given graph G, a

directed acyclic graph with nodes corresponding to the variables in U , suppose that, for all
x 2 U , PaG (x) [ fx^g are causes for x with respect to D.13 Then, the relationships among
the variables in U [ D can be expressed by the set of simultaneous equations

x = fx (PaG (x); x^; x(PaG (x); x^))
for all x 2 U , where fx is a deterministic function such that x = x if x^ =set(x).

Proof: The theorem follows by applying Algorithm 1 using an ordering over U consistent
with the graph G. 2
Thus, we see that Pearl's structural-equation model is a specialization of canonical form
when we identify (1) Pearl's domain variables with our chance variables U , (2) Pearl's atomic
interventions with our atomic interventions D, (3) Pearl's causal graph with our graph G,
and (4) Pearl's random disturbance x with our causal mapping variable x(PaG (x); x^).
This correspondence permits several clarifications of Pearl's framework. First, we have a
precise definition of atomic intervention. Unlike Pearl's model, where the concept of atomic
intervention is primitive, our framework provides a way to verify that interventions are
indeed atomic.
Second, we see what it means for the random disturbances to be exogenous. Namely,
these random variables are unresponsive to the decisions D.
13. It is not dicult to show that this condition is consistent with the condition that, for all x 2 U , x^ is an
atomic intervention on x.

425

fiHeckerman & Shachter

Third, we have a precise definition of random disturbance in terms of causal mapping
variable. Consequently, we have a means for assessing the joint probability distribution of
these variables, and|in particular|a means for assessing independencies among these variables. In fact, whereas Pearl requires that random disturbances be marginally independent,
our definition imposes no such requirement.
Theorem 6 shows that any structural-equation model can be encoded as an inuence
diagram in canonical form. The converse is also true|that is, any inuence diagram in
canonical form can be encoded as a structural-equation model. This result may seem surprising, because in Pearl's model every domain variable must have an atomic intervention,
all decision variables must be atomic interventions, and random disturbances must be independent. Given an inuence diagram in canonical form, however, we can encode its chance
and decision variables in a structural equation model. Specifically, a chance variable x can
be encoded as the variable pair fx; x^g where x^ is instantiated to idle, and a decision variable
d can be encoded as the variable pair fd; d^g where the act idle is forbidden. In addition, as
noted by Pearl, we can remove dependencies among mapping variables (at least in practice)
by introducing hidden common causes.14
Nonetheless, because hidden common causes sometimes need to be introduced, Pearl's
structural-equation model can be a less ecient representation than canonical form. For
example, to represent the relationships in Figure 4b, we would use a structual-equation
model with disturbance variables corresponding to g (^g), t(r; g; ^t), and c(t; g; ^c). Assuming
r; g; t and c are binary variables, the disturbance variables have 2, 16, and 16 instances,
respectively.15 Assuming the disturbance variables are independent, the joint probability
distribution of these variables contain 31 probabilities. In contrast, both mapping variables
in Figure 4b have only four instances. Consequently, the joint probability distribution over
the unresponsive variables in the canonical-form representation contain only 13 probabilities.
We note that Balke and Pearl (1994) relax the assumption that mapping variables are
independent. Nonetheless, their generalization of the structural-equation model, which
they call a functional model, is still less ecient than canonical form. The ineciency
comes from the fact that canonical form encodes a joint probability distribution among all
unresponsive variables (possibly including both domain and mapping variables), whereas
a functional model encodes a joint probability distribution among mapping variables only.
For example, the canonical-form inuence diagram in Figure 4b encodes the assertion that
t(r; ^t) and c(t) are independent given g. This assertion can not be encoded in the BalkePearl representation. When we represent the relationships in Figure 4b using a functional
model, we can include the variable g , in which case we obtain the 31-probability model
described in the previous paragraph. Alternatively, we can exclude variable g from the
model, and encode the dependency between the mapping variables t(r; ^t) and c(t; c^) with
14. The assumption that the mapping variables are independent has the convenient consequence that the
graph G can be interpreted as a Bayesian network in the traditional sense. That is, if variables X and
Y are d-separated by Z in G, then X and Y are conditionally independent given Z according to the
structural-equation model corresponding to G. (See Pearl, 1988, for a definition of d-separation.) SGS
(p. 54) refer to this association as the causal Markov condition.
15. Note that the mapping variable x(Y; x^) has the same number of instances as does the mapping variable
x(Y ).

426

fiDecision-Theoretic Foundations for Causal Reasoning

an arc between these two variables. The resulting Balke-Pearl model has 15 probabilities
in contrast to the 13 required by canonical form.

9. Counterfactual Reasoning
As we have noted, the ordinary inuence diagram is adequate for making decisions under
uncertainty, but is inadequate for counterfactual reasoning. In this section, we examine this
form of reasoning and suggest how it can be facilitated by inuence diagrams in canonical
form.
Given a domain described by U and D with X; Y; Z  U , counterfactual reasoning
addresses questions of the form: If we choose D = D1 and observe X = X, what is the
probability that Y = Y if we choose D = D2 and observe Z = Z? For example, in the
medical-treatment domain, we may wish to know: If we recommend the treatment and the
patient takes the drug and is cured, what is the probability that the patient will be cured
if we recommend against the treatment? Such reasoning is often important in the realworld|for example, in legal argument (Ginsberg, 1986; Balke and Pearl, 1994; Goldszmidt
and Darwiche, 1994; Heckerman et al., 1994).
We can answer such queries using inuence diagrams in canonical form. To illustrate this
approach, consider the medical-treatment question in the previous paragraph. To answer
this query, we begin with the inuence diagram in canonical form shown in Figure 4b.
Then, we duplicate all decision variables and all chance variables that are responsive to the
decisions, as shown in Figure 5. The original variables represent the act r =take, t^=idle and
its consequences. The duplicate variables (denoted with primes) represent the act r0 =don't
take, t^=idle and its consequences. There is no need to duplicate the unresponsive variables
(including the causal mapping variables) because, by definition, they can not be affected
by the decisions.16 Next, we copy the deterministic function associated with each original
variable to its primed counterpart. Then, we instantiate the decision and chance variables
as described in the query (r =take, t^=idle, t =taken, c =cured, r0 =don't take, and
t^0 =idle). Finally, we use a standard Bayesian-network inference method to compute the
probability of the variable(s) of interest (c0 in our example).
The canonical form inuence diagram is a natural representation for counterfactual
reasoning for two reasons. One, the deterministic relationships between a responsive chance
variable and its parents remains the same for any choice of D. Two, the instances assumed
by unresponsive variables are unaltered by the decisions. The ordinary inuence diagram
offers neither of these guarantees.
Our approach, described in Heckerman and Shachter (1994), is similar to that of Balke
and Pearl (1994). The main difference between the two approaches is that Balke and Pearl
use their functional model as the base representation, making their approach less ecient
than ours. Goldszmidt and Darwiche (1994) describe a graphical language for modeling
the evolution of real-world systems over time. Although their approach does not explicitly
address counterfactual reasoning, it can be adapted to so do, yielding an alternative to our
approach.
16. In general, we need duplicate only (1) those decision variables that change in the query and (2) those
chance variables that are responsive to the decisions that change.

427

fiHeckerman & Shachter

g

r
t

t(r)

r
t

c(t)

t

t

c

c

Figure 5: The use of canonical form to compute a counterfactual query. Shaded variables
are instantiated.

10. Conclusions
We have presented a definition of cause and effect in terms of the decision-theoretic primitives of act, state of the world, and consequence determined by act and state of the world,
and have shown how this definition provides a foundation for causal reasoning. Our definition departs from the traditional view of causation in that our causal assertions are made
relative to a set of decisions. Consequently, as we have argued, our definition allows for a
more precise specification of causal relationships.
In addition, we have shown how our definition provides a basis for the graphical representation of cause. We have described a special class of inuence diagrams, those in
canonical form, and have shown that it is equally expressive and more ecient than Pearl's
structural-equation model. Finally, we have shown how inuence diagrams in canonical
form, unlike ordinary inuence diagrams, can be used for counterfactual reasoning.

Acknowledgments
We thank Jack Breese, Tom Chavez, Max Chickering, Eric Horvitz, Ron Howard, Christopher Meek, Judea Pearl, Mark Peot, Glenn Shafer, Peter Spirtes, Patrick Suppes, and
anonymous reviewers for useful comments.

References
Angrist, J., Imbens, G., & Rubin, D. (1995). Identification of causal effects using instrumental variables. Journal of the American Statistical Association, in press.
Balke, A., & Pearl, J. (1994). Probabilistic evaluation of counterfactual queries. In Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence, Seattle, WA,
pp. 46{54. Morgan Kaufmann.
428

fiDecision-Theoretic Foundations for Causal Reasoning

de Finetti, B. (1937). La prevision: See lois logiques, ses sources subjectives. Annales de
l'Institut Henri Poincare, 7, 1{68. Translated in Kyburg and Smokler, 1964.
Ginsberg, M. (1986). Counterfactuals. Artificial Intelligence, 30, 35{79.
Goldszmidt, M., & Darwiche, A. (1994). Action networks: A framework for reasoning
about actions and change under uncertainty. In Proceedings of Tenth Conference on
Uncertainty in Artificial Intelligence, Seattle, WA, pp. 136{144. Morgan Kaufmann.
Heckerman, D. (1995). A Bayesian approach for learning causal networks. In Proceedings
of Eleventh Conference on Uncertainty in Artificial Intelligence, Montreal, QU, pp.
285{295. Morgan Kaufmann.
Heckerman, D., Breese, J., & Rommelse, K. (1994). Sequential troubleshooting under uncertainty. In Proceedings of Fifth International Workshop on Principles of Diagnosis,
New Paltz, NY, pp. 121{130.
Heckerman, D., & Shachter, R. (1994). A decision-based view of causality. In Proceedings of
Tenth Conference on Uncertainty in Artificial Intelligence, Seattle, WA, pp. 302{310.
Morgan Kaufmann.
Holland, P. (1986). Statistics and causal inference. Journal of the American Statistical
Association, 81, 945{968.
Howard, R. (1990). From inuence to relevance to knowledge. In Oliver, R., & Smith, J.
(Eds.), Inuence Diagrams, Belief Nets, and Decision Analysis, chap. 1. Wiley and
Sons, New York.
Howard, R., & Matheson, J. (1981). Inuence diagrams. In Howard, R., & Matheson, J.
(Eds.), Readings on the Principles and Applications of Decision Analysis, Vol. II, pp.
721{762. Strategic Decisions Group, Menlo Park, CA.
Lewis, D. (1973). Counterfactuals. Harvard University Press, Cambridge, MA.
Neyman, J. (1923). On the application of probability theory to agricultural experiments.
Translated in Statistical Science, 5:465-480 (1990).
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann, San Mateo, CA.
Pearl, J. (1993). Comment: Graphical models, causality, and intervention. Statistical
Science, 8, 266{269.
Pearl, J. (1995). Causal diagrams for empirical research. Biometrika, in press.
Pearl, J., & Verma, T. (1991). A theory of inferred causation. In Allen, J., Fikes, R., &
Sandewall, E. (Eds.), Knowledge Representation and Reasoning: Proceedings of the
Second International Conference, pp. 441{452. Morgan Kaufmann, New York.
Robins, J. (1986). A new approach to causal interence in mortality studies with sustained
exposure results. Mathematical Modelling, 7, 1393{1512.
429

fiHeckerman & Shachter

Rubin, D. (1978). Bayesian inference for causal effects: The role of randomization. Annals
of Statistics, 6, 34{58.
Simon, H. (1977). Modles of Discovery and Other Topics in the Methods of Science. D.
Reidel, Dordrecht, Holland.
Spirtes, P., Glymour, C., & Scheines, R. (1993). Causation, Prediction, and Search.
Springer-Verlag, New York.

430

fiJournal of Artificial Intelligence Research 3 (1995) 147-185

Submitted 8/94; published 8/95

An Integrated Framework for Learning and Reasoning
Christophe G. Giraud-Carrier

Department of Computer Science, University of Bristol
Bristol, BS8 1TR U.K.

cgc@compsci.bristol.ac.uk

Tony R. Martinez

Department of Computer Science, Brigham Young University
Provo, UT 84602 U.S.A.

martinez@cs.byu.edu

Abstract

Learning and reasoning are both aspects of what is considered to be intelligence. Their
studies within AI have been separated historically, learning being the topic of machine
learning and neural networks, and reasoning falling under classical (or symbolic) AI. However, learning and reasoning are in many ways interdependent. This paper discusses the
nature of some of these interdependencies and proposes a general framework called FLARE,
that combines inductive learning using prior knowledge together with reasoning in a propositional setting. Several examples that test the framework are presented, including classical
induction, many important reasoning protocols and two simple expert systems.

1. Introduction

Induction and deduction are both underlying processes in intelligent agents. Induction \involves intellectual leaps from the particular to the general" (D'Ignazio & Wold, 1984). It
plays an important part in knowledge acquisition or learning. D'Ignazio and Wold (1984)
claim that indeed, \All the laws of nature were discovered by inductive reasoning." Deduction is a form of reasoning with and about acquired knowledge. It typically does not result
in the generation of new facts, rather it establishes cause-effect relationships between existing facts. Deduction may be applied forward by seeking the consequences of certain existing
hypotheses or backward to discover the necessary conditions for the achievement of certain
goals. Despite their differences, induction and deduction are strongly interrelated. The
ability to reason about a domain of knowledge is often based on rules about that domain,
that must be acquired somehow; and the ability to reason can often guide the acquisition
of new knowledge or learning.
Inductive learning has been the subject of much research leading to the design of a
variety of algorithms (e.g., Clark & Niblett, 1989; Michalski, 1983; Quinlan, 1986; Salzberg,
1991). In general, inductive learning systems generate classification rules from examples.
Typically, the system is first presented a set of examples (objects, situations, etc.), also
known as a training set. Examples are usually expressed in the attribute-value language
and represent recorded instances of attribute-value pairs together with their corresponding
classification. The system's goal is then to discover sets of sucient critical features or rules
that properly classify the examples of the training set (convergence) and adequately extend
to previously unseen examples (generalization).
Though machines are still a far cry from matching human qualitative inductive leaps,
inductive learning systems have proven useful over a wide range of applications in medicine

c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiGiraud-Carrier & Martinez

(breast cancer, hepatitis detection), banking (credit screening), defense (mine-rock discrimination), botany (iris variety identification, venomous mushroom detection) and others
(Murphy & Aha, 1992).
The study of deductive reasoning goes at least as far back as the early Greek philosophers, such as Socrates and Aristotle. Its formalization has given rise to a variety of logics,
from propositional to first-order predicate logic to default logic to several non-monotonic
extensions. Many of these logics have been successfully implemented in artificial systems
(e.g., PROLOG, expert systems). They typically consist of a pre-encoded knowledge or rule
base, a given set of facts (identified as either causes or consequences) and some inference
engine. The inference engine carries out the deductive process using the rules in the rule
base and the facts it is provided. Several of these systems have been successfully used in
various domains, such as medical diagnosis (Clancey & Shortliffe, 1984) and geology (Duda
& Reboh, 1984).
One of the greatest challenges of current deductive systems is knowledge acquisition,
that is, the construction of the rule base. Typically, the rule base is generated as domain
knowledge is extracted from human experts and carefully engineered into rules. Knowledge
acquisition is a tedious task that presents many diculties both practically and theoretically.
If a suciently rich training set can be obtained, then inductive learning may be used
effectively to complement the traditional approach to knowledge acquisition. Indeed, a
system's knowledge base can be constructed from both rules encoded a priori and rules
generated inductively from examples. In other words, rules and examples need not be
mutually exclusive. The strong knowledge principle (Waterman, 1986) and early work on
bias (Mitchell, 1980) suggest the need for prior knowledge. Rules supplied a priori are one
simple form of prior knowledge that has been used successfully in several inductive systems
(e.g., Giraud-Carrier & Martinez, 1993; Ourston & Mooney, 1990). Similarly, proposals
have been made to enhance deductive systems with learning capabilities (e.g., Haas &
Hendrix, 1983; Rychener, 1983).
It is these authors' contention that the study of the interdependencies between learning and reasoning and the subsequent integration of induction and deduction into unified
frameworks may lead to the development of more powerful models. This paper describes a
system, called FLARE (Framework for Learning And REasoning), that attempts to combine
inductive learning using prior knowledge together with reasoning. Induction and deduction
in FLARE are carried out within the confines of non-recursive, propositional logic. Learning is effected incrementally as the system continually adapts to new information. Prior
knowledge is given by a teacher in the form of rules. Within the context of a particular
inductive task, these rules may serve to produce useful learning biases. Simple defaults
combined with learning capabilities enable FLARE to exhibit reasoning that is normally
considered non-monotonic.
The paper is organized as follows. Section 2 presents FLARE and argues the validity of
the unified framework. FLARE's representation language is described and the algorithms
employed in learning and reasoning are detailed. Section 3 reports experimental results
on classical datasets, a number of \well-designed" reasoning protocols and several other
applications, including two simple expert systems. Some of the limitations of the system
are also described. Section 4 discusses related work in induction and deduction. Finally,
section 5 concludes the paper by summarizing the results and discussing further research.
148

fiAn Integrated Framework for Learning and Reasoning

2. FLARE - A Framework for Learning and Reasoning

In this section, FLARE's learning and reasoning mechanisms are detailed. A description
and discussion of FLARE's representation language are given first in Section 2.1, along
with some useful definitions and a simple, practical example that will serve as a running
example throughout the paper. Sections 2.2 to 2.5 then follow a top-down approach to the
description of FLARE.

2.1 FLARE's Representation Language

FLARE's representation language is an instance of the attribute-value language (AVL). In
FLARE, attributes may range over nominal domains and bounded linear domains, including
closed intervals of continuous numeric values. The basic elements of knowledge in AVL are
vectors defined over the cross-product of the domains of the attributes. The components of
a vector specify a value for each attribute. The following simple extension is made to AVL.
If A is an attribute and D is the domain of A, then A takes on values from D [ f?; ?g. The
special symbols ? and ? stand for don't-care and don't-know, respectively.
The semantics associated with ? and ? are different. An attribute whose value is ? is
one that is known (or assumed) to be irrelevant in the current context, while an attribute
whose value is ? may be relevant but its actual value is currently unknown. The ? symbol
allows the encoding of rules, while the ? symbol accounts for missing attribute values in
real-world observations.
2.1.1 First-Order to Attribute-Value Translation

Since learning and reasoning tasks are often expressed in English with simple, direct counterparts in the classical first-order logic language (FOL), it is necessary for FLARE to
translate FOL clauses into their AVL equivalent. AVL is clearly not as expressive as FOL,
so that FLARE has some inherent limitations. For the purposes of this discussion, let predicates of the form p(x) and p(x; C ) where C is a constant be called avl-predicates. Then,
the FOL clauses that can be translated into AVL are of two kinds:

ground facts: p(C ) or :p(C ) where C is a constant (e.g., block(A)).
2. simple implications: (8x)P (x) ) q (x) where P (x) is a conjunction of avl-predicates

1.

and q (x) is, without loss of generality, a single, possibly negated avl-predicate (e.g.,
block(x) ^ weight(x; heavy ) ) :on table(x)).

All clauses involve at most one universally quantified variable and are thus essentially nonrecursive, propositional clauses. Despite its restricted language, FLARE effectively handles
a significant range of applications. Moreover, AVL accounts for simple, ecient matching
mechanisms and lends itself naturally to many inductive learning problems as witnessed
by its use in many successful learning systems (Clark & Niblett, 1989; Michalski, 1983;
Quinlan, 1986).
FOL statements of the aforementioned forms are translated in a straightforward way
into an equivalent symbolic-valued AVL representation, as shown in Figure 1. A similar
transformation has been proposed in the context of ILP (Dzeroski, Muggleton, & Russell,
1993). Like FLARE, some ILP systems, such as LINUS (Lavrac, Dzeroski, & Grobelnik,
149

fiGiraud-Carrier & Martinez

1. Attribute definition: For each avl-predicate, create a matching Boolean (for p(x)) or multi-valued
(for p(x; C )) attribute. If there are ground facts, create a multi-valued attribute, called label, whose
values are those of the constants.
2. Vector definition: For each implication, create a matching vector where attributes corresponding
to premise and conclusion have their appropriate value and all other attributes are set to ?. For
each ground fact, create a matching vector where the value of label is that of the constant and the
attribute corresponding to the predicate has its appropriate value. Tag the attribute corresponding
to the conclusion.

Figure 1: FOL to AVL Transformation
FOL

AVL
Rep (b) Qua (b) Pac (b)
Republican(x)) :Pacifist(x)
1
?
0T
Quaker(x))Pacifist(x)
?
1
1T

Table 1: Nixon Diamond (Reiter & Griscuolo, 1981)
1991), first map ILP problems to propositional learning problems and then rely on attributebased learning.
The creation of attribute label in step 2 stems from the fact that ground facts of the form
p(C ) can be rewritten as simple implications of the form label(x; C ) ) p(x). Notice how
the attributes whose values are ? in a vector correspond exactly to those predicates that do
not appear in the premise of the corresponding FOL clause. The attribute corresponding
to q (x) has different usages. It functions as a conclusion during forward chaining and as a
target classification during inductive learning. In some cases, it can also be used as a goal.
To avoid unnecessary confusion, the attribute corresponding to q (x) is simply referred to
as the target-attribute. The values of the target-attribute are subsequently tagged with the
subscript T . The translation from FOL to AVL is currently performed manually.
It is clear that as the number of predicates increases, so does the size of the vectors. Since
all vectors are of the same size and many of them may only have values set for a relatively
small number of their attributes, this may result in large memory requirements, as well as
in an increase of execution time of operations on vectors. When there are predicates that
qualify different values of the same concept (e.g., red(x), yellow(x), for color), it is possible
to limit the size of the vectors by translating such predicates into a single multi-valued
attribute (e.g., color(x; V ), where V is a constant: red, yellow, etc.). This is particularly
useful for the conclusion part q (x) when it corresponds to a classification for x.
Tables 1 through 4 contain four simple examples that demonstrate the transformation.
Each derived attribute in the AVL column is followed by its type (b for Boolean, m for
multi-valued). Table 1 shows the Nixon Diamond, a classical example of conicting defaults. Informally, the Nixon Diamond states that Republicans are typically not pacifist
but Quakers are typically pacifist. The conict then arises as one asserts that Nixon is both
a Republican and a Quaker. Table 2 contains assertions about animals and their ability to
y. It states that animals normally do not y, birds are typically ying animals and penguins are birds that do not y. Table 3 shows statements regarding eyes and their fitness
for lenses. Finally, Table 4 contains some facts about a simple blocks world.
150

fiAn Integrated Framework for Learning and Reasoning

FOL

AVL
Ani (b) Bir (b) Pen (b) Fly (b)
Animal(x)) :Fly(x)
1
?
?
0T
Bird(x))Animal(x)
1T
1
?
?
Bird(x))Fly(x)
?
1
?
1T
Penguin(x))Bird(x)
?
1T
1
?
Penguin(x)) :Fly(x)
?
?
1
0T

Table 2: Flying or Not Flying (Lifschitz, 1988)
FOL

AVL
Tpr (m) Eye (m) Fit (b)
Tear-prod-rate(x,low))Eyes(x,dry)
low
dryT
?
Eyes(x,dry)) :Fit(x)
?
dry
0T

Table 3: Fitting Lenses
2.1.2 Examples vs. Precepts vs. Rules

Informally, the problem of supervised learning may be described as follows. Given (1) a set
of categories, (2) for each category, a set of instances of \objects" in that category and (3)
optional prior knowledge, produce a set of rules sucient to place objects in their correct
category. In AVL, instances consist of sets of attribute-value pairs or vectors, describing
characteristics of the objects they represent, together with the object's category. In this
context, the category is a target-attribute.
An example is a vector in which all attributes are set to either ? or one of their possible
values. A rule is a vector in which some of the attributes have become ? as a result of
generalization during inductive learning. A precept is similar to a rule but, unlike a rule,
it is not induced from examples. Precepts are either given by a teacher or deduced from
general knowledge relevant to the domain under study. In the context of a given rule or
precept, the ? attributes have no effect on the value of the category. Precepts and rules
thus represent several examples. For instance, let p = (?; 1; 0; 0T ) be a precept, where all
attributes range over the set f0,1,2g. Then p represents the three examples: (0; 1; 0; 0T ),
(1; 1; 0; 0T ) and (2; 1; 0; 0T ).
The distinction between rules and precepts is limited to learning. In reasoning, all
vectors (including examples that do not generalize) are rules. In FLARE, rules are formed
by dropping conditions (Michalski, 1983), that is, under certain circumstances (see Section
2.4.2), one attribute is set to ?. Precepts, on the other hand, are rules encoded a priori.
They reect some high-level knowledge (or common sense) about the real-world. A precept
\suggests something advisory and not obligatory communicated typically through teaching"
(Webster's Dictionary).
2.1.3 Running Example

To illustrate the above definitions and algorithms of the following sections, a final example
of the transformation is constructed, based on the mediadv knowledge base (Harmon &
King, 1985). This purposely simple example will serve as a running example throughout
151

fiGiraud-Carrier & Martinez

FOL

AVL
Lab (m) Blk (b) Hvy (b) OnT (b)
block(A)
A
1T
?
?
block(B)
B
1T
?
?
heavy(A)
A
?
1T
?
heavy(B)
B
?
1T
?
block(x) ^ heavy(x) ) on table(x)
?
1
1
1T
:on table(A)
A
?
?
0T

Table 4: Simple Blocks World, adapted from (Lifschitz, 1988)
the paper. A discussion of the complete mediadv knowledge base is in Section 3.5. Here,
two conditions (i.e., instructional feedback and presentation modification) are left out and
only a few of the original rules are used. Table 5 contains the informal English version
of the knowledge used (with reference to the rules of mediadv it was generated from when
applicable) and its corresponding translation into AVL vectors.
Let KB be the resulting set of vectors. The attributes are given in the order: situation,
stimulus-situation, response, appropriate-response, stimulus-duration, training-budget and
media. Note that all the attributes are nominal. The symbolic values used in the English statements are transformed into equivalent nominal values in the vectors. Hence, for
example, the first statement gives rise to a vector in which the attribute situation is set
to 0 (the corresponding nominal value of schematics for this attribute), and the targetattribute stimulus-situation is set to 0 (the corresponding nominal value of symbolic for
this attribute).
The top goal is for the system to suggest the most effective media for training, based on
four conditions: situation, response, stimulus-duration, and training-budget. Note that the
attributes stimulus-situation and appropriate-response can be used as subgoals in reaching
the final conclusion. Vectors v13 to v17 are examples since all of the conditions have set
values. They are not part of the original mediadv knowledge base but are added to exercise
important features of the algorithms. As KB is given, all vectors of KB with condition
attributes set to ? are precepts rather than rules. For instance, v7 and v12 are precepts.
Then, the term rule applies to new generalizations, induced by FLARE from KB . For
instance, v80 (see Section 2.4.3) is a rule.

2.2 Algorithmic Overview
FLARE is a self-adaptive, incremental system. It uses domain knowledge and empirical
evidence to construct and maintain its knowledge base. FLARE's knowledge base is interpreted as a \best so far" set of rules for coping with the current application. In that
sense, FLARE follows the scientific approach to theory formation/revision: available prior
knowledge and experience produce a \theory" that is updated or refined continually by new
evidence.
FLARE involves three main functions whose definitions and high-level algorithmic interactions are given in Figure 2. The details of each function's implementation are given in
the following sections. An intuitive overview is presented here.
152

fiAn Integrated Framework for Learning and Reasoning

If
Then
If
Then
If
Then
If
Then
If
Then
If
Then
If

Then
If
Then
If
Then
If

Then
If
Then
If
Then
If
Then

English Statements
situation = schematics
(Rule3)
stimulus-situation = symbolic
situation = conversation
(Rule4)
stimulus-situation = verbal
situation = photograph
(Rule2)
stimulus-situation = pictorial
response = observing or
(Rule5)
response = thinking
appropriate-response = covert
response = emoting
(Rule10)
appropriate-response = affective
stimulus-situation = verbal and (Rule13)
appropriate-response = covert and
stimulus-duration = brief
media = lecture
stimulus-situation = verbal or (Rule14)
stimulus-situation = symbolic or
stimulus-situation = pictorial and
appropriate-response = covert and
stimulus duration = brief and
training-budget = medium
media = lecture-with-slides
stimulus-situation = verbal and (Rule16)
stimulus-duration = brief
media = role-play-w/verbal-feedback
stimulus-situation = verbal and (Rule17)
appropriate-response = affective
media = role-play-w/video-feedback
situation = conversation and
response = observing or
response = thinking and
stimulus-duration = brief and
training budget = medium
media = lecture
situation = photograph and
response = emoting and
stimulus-duration = persistent and
training-budget = small
media = role-play-w/verbal-feedback
situation = photograph and
response = emoting and
stimulus-duration = persistent and
training-budget = small
media = lecture
situation = photograph and
response = emoting and
stimulus-duration = persistent and
training-budget = small
media = role-play-w/verbal-feedback

v1

Equivalent AVL Vectors
= 0 0T ? ? ? ? ?

v2

= 1 1T ? ?

? ? ?

v3

= 2 2T ? ?

? ? ?

v4
v5

= ? ?
= ? ?

0 0T ? ? ?
1 0T ? ? ?

v6

= ? ?

2 1T ? ? ?

v7

= ? 1

? 0

0 ? 2T

v8 = ? 1
v9 = ? 0
v10 = ? 2

? 0
? 0
? 0

0 1 3T
0 1 3T
0 1 3T

v11 = ? 1
pty = 1

? ?

0 ? 0T

v12 = ? 1
pty = 3

? 1

? ? 1T

v13 = 1 ?
v14 = 1 ?

0 ?
1 ?

0 1 2T
0 1 2T

v15 = 2 ?

2 ?

1 0 0T

v16 = 2 ?

2 ?

1 0 2T

v17 = 2 ?

2 ?

1 0 0T

Table 5: Simple KB Running Example
153

fiGiraud-Carrier & Martinez

DEFINITION






Function: Generate-Precepts
{ Input: a set of general rules, a set of facts and one designated target-attribute.
{ Output: one or more precepts.
Function: Reasoning
{ Input: the current knowledge base, a set of facts encoded in a vector v, one designated target-

attribute and optionally, the target value of the target-attribute.
{ Output: a vector v+ equal to v together with further facts deduced from v, including a derived
value for the target-attribute.
Function: Adapting
{ Input: the current knowledge base, the vector v+ output by function Reasoning and the target
value of the target-attribute.
{ Output: updated knowledge base.

IMPLEMENTATION
1. Preprocessing: Perform Generate-Precepts
2. Main loop: For each vector presented to the system
(a) Perform Reasoning
(b) If there is a target value for the target-attribute, perform Adapting

Figure 2: FLARE - Algorithmic Overview
Conceptually, FLARE's execution consists of two phases. In the preprocessing phase,
FLARE uses prior knowledge in the form of general rules that may be viewed as encoding
\commonsense" knowledge. Using deduction from given facts, domain-specific precepts are
generated as an instantiation of the general knowledge to the domain at hand. Section 2.5
details the Generate-Precepts function. The need for generating and explicitly encoding
precepts as individual vectors in such a preprocessing phase arises because FLARE's inductive mechanisms take place at the vector level. Thus, even though it is always possible to
deduce them from the general knowledge, precepts are most useful in induction when they
are made explicit.
In normal processing, FLARE executes an, at least conceptually, infinite loop. Steps (a)
and (b) are executed every time new information (in the form of AVL vectors) is presented
to the system. In step (a), FLARE reasons from the \facts" provided by the input vector
and the rules found in the current knowledge base. Rule-based reasoning and similaritybased reasoning are combined as discussed in Section 2.3 to derive a value for the targetattribute, as well as other attributes along the forward chain to the conclusion. In step
(b), FLARE adapts its current knowledge base. Because FLARE is a supervised learner,
it can only adapt when a target value for the target-attribute is explicitly given as part of
the information presented. The combination of steps (a) and (b) is referred to as learning.
Section 2.4 details the Adapting function.
154

fiAn Integrated Framework for Learning and Reasoning

Note that reasoning based upon available knowledge prior to adapting is plausible.
Even when available information is insucient and/or incomplete, humans often attempt
to make a tentative decision and get corrected if necessary. At any one time, the decision
made represents a kind of \best guess" given currently available information. The more
(correct) information becomes available, the more accurate decisions become.

2.3 FLARE's Reasoning

FLARE implements a simple form of rule-based reasoning combined with similarity-based
reasoning, similar to CONSYDERR (Sun, 1992). Sun has argued that such a combination
effectively decreases the system's susceptibility to brittleness (Sun, 1992). In particular, in
the absence of applicable rules or when information is incomplete, FLARE relies on similarity with previously encountered situations to make useful predictions. Others have also
argued that analogy is a necessary condition for commonsense reasoning and the subsequent
overcoming of brittleness (Minsky & Riecken, 1994; Wollowski, 1994). Section 2.3.1 shows
how the notion of Clark's completion (1978) can be applied to inductively learned rules
and exploited by similarity-based reasoning to generate new rules. Sections 2.3.2 to 2.3.7
describe and illustrate FLARE's reasoning mechanisms.
2.3.1 Completion

Inductively learned rules of the form (8x)P (x) ) q (x), where P is a conjunction of avlpredicates, are essentially classification rules or definitions that establish relationships between features, captured by P (x), and concepts, expressed by q (x). In keeping with the
classical assumption that what is not known by a learning system is false by default, inductively generated rules lend themselves naturally to the completion principle proposed
by Clark (1978). That is, classification rules become \if and only if" statements, i.e.,
P (x) , q(x). Hence, under completion, if q (x) is known to be true, then it is possible to
conclude that P (x) is true.
Clearly, completion does not apply to all rules. Inductively learned rules are inherently
definitional as they essentially encode a concept's description in terms of a set of features.
Other rules, such as those relating concepts at the same relative cognitive level, are not
definitional. For example, given that birds are animals and that some x is an animal,
it does not follow that x is a bird. Note that, in addition to inductively learned rules,
definitions may be given to FLARE as prior knowledge.
The completion principle is particularly useful when it interacts with similarity-based
reasoning to generate new rules, as shown in the following derivation.


Hypotheses:

1.
2.
3.
4.


(8x)P(x) ) q(x), which may be completed.
(8x)P (x) ) q (x).
P \ P 6= ; (i.e., P and P have some attributes in common).
q(x) is true.
0

0

0

0

Derivation:

1. q(x) from hypothesis 4.
155

fiGiraud-Carrier & Martinez

2. P(x) from completion applied to hypothesis 1.
3. q (x) from similarity-based reasoning using hypotheses 2 and 3.
0

A new implication between concepts, namely q (x) ) q 0(x), is thus generated. Though
FLARE is capable of deriving q 0(x) from q (x), it does not actually store the new implication
in its knowledge base.
The following example adapted from (Collins & Michalski, 1989) illustrates the use of
the above derivation. Assume that the system has learned a description of the Chaco area in
terms of a set G of geographical conditions (i.e., G(x) )area(x; theChaco)). Furthermore,
assume that the system knows a rule that encodes a set of conditions C sucient for the
raising of cattle (i.e., C (x) )raise(x; cattle)) and C is such that C and G share a number of
conditions. If the system is now told that the area of interest is the Chaco, it first deduces by
completion that the conditions in G are met and then, by taking advantage of the similarity
between G and C , the system concludes that cattle may be raised in the Chaco. Note that
the level of confidence in the conclusion depends upon the amount of similarity.
In FLARE, the representation is extended and a definition indicator is tagged to those
statements that may be completed (i.e., prior definitions, inductively learned classifications).
Note that, though somewhat cumbersome, this extension is needed since FLARE does
not physically separate concepts and the features used to describe them. CONSYDERR
on the other hand provides natural support for the dichotomy. FLARE's representation
makes learning more readily applicable and preserves consistency with previously developed
models. At this point, the issue of achieving both the dichotomy and easy learning remains
open.
2.3.2 FLARE's Reasoning Function

Deduction in FLARE is applied forward. Hence, facts must be provided so as to initiate
reasoning. These facts are coded into a vector in which attributes whose values are known
are accordingly set, while all other attributes are ? (i.e., don't-know). One attribute is
designated as the target-attribute and, if known, its value is also provided. FLARE then
uses the rules of its knowledge base and the facts to derive a value for the target-attribute.
The Reasoning function is shown in Figure 3. Note that in this discussion, the current
knowledge base is assumed to be non-empty. If the knowledge base is empty, the system
cannot deduce anything other than ?.
Step (1) applies completion first. FLARE finds all asserted (i.e., neither ? nor ?) attributes of v that are target-attributes of definitions in the current knowledge base. If any
such attribute is found, and for all of them, completion is applied by \copying" into v all
asserted attributes of the corresponding definitions that are ? in v . The following two issues
must be addressed by FLARE in implementing completion.
1. Since some attributes may be involved in the definitions of more than one targetattribute or concept, it follows that there may be more than one values to be copied
into a given attribute when completing these definitions.
2. Since FLARE's concepts and rules consist of sets of vectors, where each vector is a
conjunction and all the vectors sharing the same target-attribute form a disjunction,
it follows that some definitions may be disjunctive as well.
156

fiAn Integrated Framework for Learning and Reasoning

DEFINITION



Input: the current knowledge base, a set of facts encoded by a vector v, one designated target-attribute
and optionally, the target value of the target-attribute.
Output: a vector v+ equal to v together with further facts deduced from v, including a value for the
target-attribute.

IMPLEMENTATION
1. Completion: For each asserted attribute a of v other than the target-attribute, if a is the targetattribute of a definition d and their values are equal, then copy all asserted attributes of d that are ?
in v, into v.
2. Forward chaining: If v's target-attribute has not been asserted
(a) Repeat until no new attribute of v has been asserted
i. Let w = v.
(* create a temporary copy of v *)
ii. For each non-asserted attribute a of v other than the target-attribute, if a rule can be
applied to v to assert a, then apply it by asserting a in w.
(* based on v, assert all possible attributes (other than the target-attribute) in w *)
iii. Let v = w.
(* copy result back into v for next level of inference *)
(b) If a rule can be applied to assert v's target-attribute, then apply it. Otherwise, perform
similarity-based assertion.

Figure 3: Function Reasoning
The current implementation resolves these two issues as follows. In the first case, potential
conicts are resolved simply by giving precedence to the first copy made (which depends
upon the order in which asserted attributes are processed). In the second case, FLARE
simply chooses one of the defining conjunctions at random and applies completion to it.
Other mechanisms (e.g., apply to all, select a winner based on some criteria, etc.) are the
topic of further research.
Completion causes further information (in the form of asserted attributes) to be gained,
thus improving the chance of reaching a goal. Indeed, the purpose of step (1) is two-fold.
First, completion allows the system to reach goals that are not otherwise achievable by
existing rules. Second, even if the top goal is not achieved directly by completion, further
reasoning to achieve it is enhanced as described in Section 2.3.1.
When the target-attribute has not been asserted by completion, step (2) pursues the
reasoning process using forward chaining. As mentioned above, v has a single targetattribute, corresponding to the final goal to achieve. However, at any given time, any
one of the (yet) non-asserted1 attributes of v may be designated as a subgoal that may
1. These are either ? or ?. They are ? when precepts and rules with differing premises and conclusions are
used. In such cases, it is not clear until reasoning whether they are true don't-cares or only don't-knows.

157

fiGiraud-Carrier & Martinez

be useful (or necessary) in reaching the final conclusion. Step (2)(a) is the heart of the
reasoning process. Each execution of step (2)(a)(ii) corresponds to the achievement of all
possible subgoals at a given depth in the inference process. Each iteration uses knowledge
acquired in the previous iteration to attempt to derive more new conclusions using existing
rules. Step (2)(b) concludes the reasoning phase by asserting the target-attribute.
Notice that the target-attribute is always asserted, either by rule application or similaritybased assertion. Hence, FLARE always reaches a conclusion. In the worst case, when there
is no information about the target-attribute in the current knowledge base, the value derived for the conclusion must clearly be ?. In all other cases, the validity and accuracy of
the derived conclusion depend upon available information. The accuracy or confidence level
may be computed in a variety of ways from information about static priorities, dynamic
priorities, covers and counters (see Section 2.4).
The two complementary mechanisms used in asserting the target-attribute (i.e., rule
application and similarity-based assertion) are described in the next two sections. They
apply sequentially. If a rule exists that can be applied, then it is applied. Otherwise,
similarity-based reasoning takes effect.
Finally, note that information regarding the way the goal is achieved could be displayed
by FLARE for the purpose of human examination and inspection. Currently, FLARE is
non-interactive, that is, it cannot query a user for the values of missing attributes that may
help improve the accuracy of its result.
2.3.3 Rule Application

Let val(a; x) denote the value of attribute a in vector x. In the state of knowledge represented by a vector v , a rule may be applied if it covers v . A vector x is said to cover a
vector y if and only if:
1. x and y have the same target-attribute and
2. for all remaining attributes a of x, either val(a; x) = ? or val(a; x) = val(a; y ).
For example, in KB , v11 covers v7 and v8 but v11 does not cover v9 or v12. Ignoring attributes
whose value is ?, the second condition states that the set of remaining attribute-value pairs
of x is a proper subset of the set of remaining attribute-value pairs of y . Intuitively, x covers
y if y satisfies all of the premises of x.
To accommodate real-valued attributes, the notion of equality is slightly extended.
Given that the probability of two real values being equal is extremely small, the cover
relation, because of condition 2, would essentially never hold. The following extension,
borrowed from ILA (Giraud-Carrier & Martinez, 1995), is suggested. Two linear values x1
and x2 are equal if and only if jx1 , x2 j   , for some  > 0. Hence, the vector (?, 1.2, 3.52,
?, 0T ) covers the vector (2, 1.3, 3.48, ?, 0T ) if  = 0:5. In the current implementation,  is
some fraction of the range of possible values of each attribute.
2.3.4 Similarity-Based Assertion

The notion of similarity in FLARE is captured by a non-symmetric distance function defined
over (n-dimensional) vectors. If vector x is stored in the knowledge base and vector y is
158

fiAn Integrated Framework for Learning and Reasoning

presented to the system to reason about, then the distance from x to y is given by:

Xn d(xi; yi)

D(x; y ) = numi=1asserted(x)
where, if x+i ; yi+ denote values of attribute i other than ? and ?,

d(?; yi)
d(?; yi)
d(x+i ; ?)
d(x+i ; ?)
d(x+i ; yi+)
d(x+i ; yi+)

=
=
=
=
=
=

0
0:5
0:5
0:5
(x+i 6= yi+ ) if attribute i is nominal
jx+i , yi+j if attribute i is linear
range(i)
such that range(i) is the range of values of attribute i and num asserted(x) is the number
of attributes that are not ? in x. The above equations for d are consistent with the semantics
of ? and ? defined in Section 2.1.
D(x; y ) is meaningful only if x and y have the same target-attribute and the targetattribute is left out of the computation. For example, D(v11; v8) = 0, D(v13; v14) = 1=4,
D(v7; v16) = 2=3, D(v16; v7) = 5=8 and D(v1; v4) is undefined. A detailed discussion of
and justification for the definition of D are found elsewhere (Giraud-Carrier & Martinez,
1994a). Since every ordered set is in one-to-one correspondence with a subset of the natural
numbers, D is well defined. To eliminate the effects of statistical outliers on range(i),
the dataset must be ridden of vectors whose attributes have such irregular values. D is
an extension of the similarity function defined for IBL (Aha, Kibler, & Albert, 1991),
to inductive learning algorithms that use and/or create general rules. D applies to both
nominal and linear domains, and relies on the corresponding notion of distance between
values. In particular, D handles continuous values directly, without need for discretization.
Currently, D treats each attribute equally. Existing methods assigning weights to each
attribute-wise distance (Salzberg, 1991; Stanfill & Waltz, 1986; Wettschereck & Dietterich,
1994) may be incorporated in D.
Similarity-based assertion consists of asserting the target-attribute of a vector v to the
value of that attribute in v 's closest match given by D. Note that (since D is not symmetric)
x covers y if and only if D(x; y) = 0. Hence, since 0 is the minimum of the distance function,
D can be used to apply both reasoning mechanisms with the correct order (i.e., rules first,
similarity next), by computing the distance from all the rules in the current knowledge base
to v and simply selecting the rule that minimizes D. As it is possible that more than one
rule minimizes D, a priority scheme is devised to choose a winner. This conict resolution
procedure which relies partially on FLARE's ability to learn is outlined in Section 2.3.5.
2.3.5 Conflict Resolution

Along with each vector, FLARE also stores the following information:
 a static priority value (static priority),
159

fiGiraud-Carrier & Martinez

 a dynamic priority value (dynamic priority) and
 the number of vectors covered (num covers).
The value of static priority is set to 0 by default but may be changed by a teacher to any
other value a priori (e.g., v11 and v12 in KB ). Static priorities provide a means whereby rules
may be prioritized according to some externally provided information or meta-knowledge.
The value of dynamic priority is initialized to 0. Its value is not changed by a teacher,
however, but evolves over time and is intended to resolve conicting defaults extensionally.
Conicting defaults, such as the Nixon Diamond of Table 1 may be encoded a priori as
precepts or induced from examples. In either case, they are identified in the reasoning
phase as FLARE discovers two rules that apply equally well to the input vector. Formally,
two rules R and S are in conict over a vector v if all of the following conditions hold.
1.
2.
3.
4.
5.

D(R; v ) = D(S; v) = 0
R and S have the same specificity
R and S have the same static priority
R and S have different target-attribute's value
R and S overlap (i.e., the sets of all possible vectors each of them covers intersect)

Two vectors are said to be concordant if they have the same target-attribute and the targetattribute's value is the same in both vectors. When reasoning about vector v and coming
upon conicting defaults, FLARE simply increments by 1 the value of dynamic priority of
the default that is concordant with v . If none of the defaults are concordant with v , then
no change is made. The value of dynamic priority reects the number of times a particular
default has been supported by evidence drawn from the environment. It is thus evidence,
rather than meta-knowledge, that is responsible for the emerging ordering of defaults under
dynamic priority. Note that a target value must be given for the target-attribute for the
above update to take place so that dynamic priority is a result of the combination of learning
and reasoning. Notice also that dynamic priority evolves over time so that the system's
response changes based on accumulated evidence.
The value of num covers is also a result of the combination of learning and reasoning.
It records the number of other vectors seen by the system so far that are concordant with
and covered by the vector. It is a kind of confidence level for that vector, as it essentially
counts the number of times the rule represented by the vector is confirmed by empirical
evidence.
When more than one rule minimizes D (i.e., can be selected for application), a winner
is chosen according to the following priority scheme, where each subsequent condition is
invoked if a tie exists at the previous level.
1. Most specific
2. Highest static priority
3. Highest dynamic priority
160

fiAn Integrated Framework for Learning and Reasoning

4. Greatest cover
Specificity is defined as the number of attributes, other than the target-attribute, whose
value is not ?. A vector x is more specific than a vector y if specificity(x)> specificity(y ).
For example, in KB , v7 has specificity 3, v8 has specificity 4 and v13 has specificity 4, so
that v8 and v13 are more specific than v7 .
Giving priority first to the most specific vector allows FLARE to handle exceptions and
cancellation of inheritance. Using static priorities next makes it possible to handle conicting defaults as defined by a teacher, while dynamic priorities account for epistemological
inconsistencies that may be resolved over time as more information becomes available in
support of one belief or the other. Finally, selecting the vector with greatest cover allows
evidence gathered from experience to guide a final selection. Note that the current scheme
gives precedence to teacher-provided information. Other ordering schemes can easily be
defined. For example, static priorities could be given as a simple form of initial bias and
evidence gathered through learning (e.g., dynamic priority and cover) could be used to
confirm or modify these priorities.
2.3.6 Illustration

Consider KB and assume the vector v = (1; ?; 1; ?; 0; 0; ?T ) is input to the reasoning function. Execution proceeds as follows. Step (1) is essentially skipped as none of the attributes
of v meet the looping condition. Then, there are only two loops through the forward chain
before the target-attribute is set.
Execution Trace
(1) (a) Let w = v
(b) (i) Designate the second attribute as a first subgoal
(ii) Apply rule v2 : result is w = (1; 1; 1; ?; 0; 0; ?T )
(i) Designate the fourth attribute as second subgoal
(ii) Apply rule v5 : result is w = (1; 1; 1; 0; 0; 0; ?T )
(c) Let v = w
(2) Two conicting rules exist: D(v7; v ) = D(v11; v ) = 0
Apply v7 (more specific): result is v = (1; 1; 1; 0; 0; 0; 2T )
2.3.7 Approximate Reasoning

Notice that, in forward chaining, the assertion of attributes that are subgoals does not
involve similarity-based assertion but results from rule application only. As a result, the
accuracy of the final goal is increased but the ability to perform approximate reasoning is
reduced. It is possible to relax this restriction thus potentially achieving more subgoals but
reducing the confidence in the final result. For example, the condition in step (2)(a)(ii)
could be modified to allow not only rules (which are perfect matches, i.e., D = 0) but also
matches deemed to be \close enough." The measure of closeness can be implemented via a
threshold value TD , placed on D. That is, the current condition is replaced with:
 Let D0 = distance to closest match
 If D0 = 0 perform rule application
Else if D0  TD perform similarity-based assertion
161

fiGiraud-Carrier & Martinez

The value of TD then offers a simple mechanism to increase the level of approximate reasoning. This is particularly useful for cases such as the Chaco example (Section 2.3.1), where,
after completion, most of the reasoning is based on the amount of similarity between concepts. Notice that the above condition is functionally equivalent to the current one when
TD = 0.

2.4 FLARE's Learning

This section addresses the construction of FLARE's knowledge base through incremental,
supervised learning. FLARE learns by continually adapting to the information it receives.
Indeed, training vectors are assumed to become available one at a time, over time and, as is
inherent in nature, some vectors may be noisy while others may be encountered more than
once. Moreover, FLARE extends inductive learning from examples with prior knowledge
in the form of precepts. Sections 2.4.1 to 2.4.3 describe and illustrate FLARE's learning
mechanisms and Section 2.4.4 highlights some of the advantages of combining extension and
intension in learning.
2.4.1 FLARE's Adapting Function

Over time, FLARE is presented with a sequence of examples and precepts that are used to
update its current knowledge base. The set of all examples, rules and precepts that share
the same target-attribute can be viewed as a partial function mapping instances into the
goal-space. In this context, an example maps a single instance to a value in the goal-space,
while precepts and rules are hyperplanes that map all of their points or corresponding
instances to the same value in the goal-space.
Learning then follows a form of nearest-hyperplane learning. As mentioned in Section
2.2, it consists of first applying the reasoning scheme, and then making further adjustments
to the current knowledge base to reect the newly acquired information. The reason the
algorithm is said to be nearest-hyperplane is that the reasoning phase essentially identifies a
closest match for the input vector. This closest match is either a rule (i.e., true hyperplane)
or a stored example (i.e., point or degenerated hyperplane).
The prior application of reasoning allows the system to predict the value of the targetattribute based on information in the current knowledge base. Also, if there are missing
attributes in the input vector and the knowledge base contains rules that can be applied
to assert these attributes, the rules are applied so that as many of the missing attributes
as possible are asserted before the final goal is predicted. Hence, the accuracy of the
prediction is increased and generalization is potentially enhanced, thus enabling FLARE to
more effectively adapt its knowledge base.
The system starts with an empty knowledge base. It then adapts to each new vector v ,
where v is either a precept or an example. If v is the first vector, then there is no closest
match and v is automatically stored in the current knowledge base. In that sense, the
first learned vector represents yet another bias for the learning system. If v is not the first
vector, then reasoning takes place producing v + . A closest match, say m, is also found and
the knowledge base adapts itself based on the relationship between v + and m, as shown in
Figure 4. Note that m is closest given the current available information and can, indeed,
be \far" from v + . Hence, the order in which training takes place impacts the outcome.
162

fiAn Integrated Framework for Learning and Reasoning

DEFINITION



Input: the current knowledge base, the vector v+ output by function Reasoning and the target value
of the target-attribute.
Output: updated knowledge base.

IMPLEMENTATION
1. Let m be the vector of the current knowledge base such that D(m;v+ ) is smallest (i.e., m is v+ 's
closest match in the current knowledge base).
2. If all the attributes have equal values in v+ and m, then add 1 to m.counters[v+ .target-attribute's
value]
(* if m is identical to or the prototype of v+ , then: do not store v+ , update m's counters *)
3. Else if m covers v+ and m and v+ are concordant, then add 1 to m.num covers
(* if m subsumes or is a generalization of v+ , then: do not store v+ , increase m's confidence *)
4. Else if v+ covers m and m and v+ are concordant, then add 1 to v+ .num covers, delete m from the
knowledge base and add v to the knowledge base
(* if v+ subsumes or is a generalization of m, then: replace m by v+ , increase v+ 's confidence *)
5. Else if v+ and m can produce a generalization, then
(* if there is a possibility of generalization *)
 If v+ is more specific than m and m has more than one non ? attribute, then drop the condition
in m and set m.static priority to maxfm.static priority, v+ .static priorityg
(* if m is more general than v+ and dropping condition is possible, then: do not store v+ , drop
condition in m, update static priority *)




Else if v+ has more than one non ? attribute, then drop the condition in v+ , set v.static priority
to maxfm.static priority, v+ .static priorityg, set v+ .num covers to m.num covers, delete m
from the knowledge base and add v+ to the knowledge base
(* if v+ is more general than m and dropping condition is possible, then: drop condition in v+ ,
replace m by v+ , update parameters *)
Else add v+ to the knowledge base
(* if dropping condition is impossible, then: store v+ in knowledge base *)

6. Else add v+ to the knowledge base
(* default case: store v+ in the knowledge base *)

Figure 4: Function Adapting
The array counters contains an entry for each possible value of the target-attribute
and is also stored with each vector. All the counters are initialized to 0, except the one
corresponding to the vector's target-attribute value which is initialized to 1. The counters
evolve over time and are used to handle noise. For any vector p in the current knowledge
base, exactly one counter value is incremented (by 1) each time a new vector is presented,
163

fiGiraud-Carrier & Martinez

whose attributes' values are all equal to those of p. The value incremented corresponds to
the new vector's target-attribute value. The counter value that is highest represents the
statistically \most probable" target-attribute value. In effect, the target-attribute value of
a vector is always the one with highest count. Note that this value may change over time,
as new information becomes available.
Because a best match is first identified, changes to the knowledge base are localized and
are guided by the kinds of possible relationships between v + and m. These relationships
are summarized below.
 v+ is equal to m (i.e., noise or duplicates) or
 v+ is subsumed by m (i.e., v+ is a special case of m) or
 v+ subsumes m (i.e., v+ is a general case of m) or
 v+ and m can produce a generalization or
 all other cases (e.g., v+ is an exception to m, v+ and m are too far apart, etc.)
In the first case, v + (or its prototype m) is already in the knowledge base and only the
counters need to be updated. Note that the extension of the notion of equality discussed in
Section 2.3.3 enables this part of the algorithm, in conjunction with the counters, to produce
some generalization for linear attributes. In effect, the vector retained in the knowledge base
acts as a \prototype," and its target-attribute's value is the one most probable among its
 -close neighbors. In the second case, there is no need to store v + as the current knowledge
base has sucient information to correctly predict v + 's target value. In the third case, v +
is stored and m removed as v + is more general than m and thus accounts for it. The fourth
case captures the possibility of generalization by dropping conditions (see Section 2.4.2 for
details). If generalization takes place, only one of v+ or m is generalized and stored. The
values of static priority and num covers are also reset so that the generalization inherits the
maximum static priority value and the current value of num covers. Finally, in the fifth
case, v + must be added as the current knowledge base either does not produce the correct
target value for v + (e.g., exceptions) or is not deemed reliable enough to properly account
for v + .
Notice that the adaptation phase takes place regardless of the target value predicted
by the reasoning phase. A possible alternative would be to adapt only if the predicted
target value differs from the actual target value. It has been found empirically, however,
that too much useful information is lost with this approach due to the incrementality of
the system and its sensitivity to ordering. A possibly viable alternative would make use of
memory. Vectors currently accounted for could be saved in memory and presented later to
the system. This may be done a few times over some period of learning time until either
the vectors must be stored in the knowledge base (due to changes in the knowledge base)
or they are discarded as the system has gained enough confidence in its ability to account
for them.
2.4.2 Generalization

Two vectors that have the same target-attribute's value can produce a generalization when
the following five conditions hold.
164

fiAn Integrated Framework for Learning and Reasoning

1. They differ in the value of exactly one of their attributes.
2. The attribute on which they differ is nominal.
3. They are concordant.
4. The number of their attributes not equal to ? differ by at most 1.
5. At least one of them has more than one non ? attribute.
Generalization then consists of setting to ? the attribute on which the two vectors differ, in
the vector that is most general, as long as that vector has more than one non ? attribute.
For example, vectors v8 and v9 of KB satisfy the above conditions and would generalize to
produce a vector, say v8+9 = (?; ?; ?; 0; 0; 1; 3T ). The value of 1 in the fourth condition is
based upon empirical evidence.
Choosing the most general vector maximizes generalization and the condition on the
number of non ? attributes guarantees that no rule is generated that would cover every other
vector. This version of the dropping-the-condition rule (Michalski, 1983) is only applied to
nominal attributes as it makes little sense for linear (especially real-valued) domains. For
linear attributes, generalization is achieved through the artifact due to the extended notion
of equality discussed above.
Let v and w be two vectors representing n and m  n examples, respectively. Furthermore, let v 0 be the generalization obtained from v and w by dropping a p-valued attribute
in v . Then v 0 represents pn examples. Since v and w represent at most 2n examples, generalization causes at least (p , 2)n new examples to be represented. As p increases, this
value also increases and, for large values of p, could lead to over generalization as only two
values of a given attribute suce to predict the outcome of all values in the current context.
However, such potential over generalizations are partially offset by the system's ability to
identify, retain and give precedence to exceptions.
There are still drawbacks to FLARE's generalization scheme. Given a set of vectors
of the form vi = SxiTk where S is fixed, T is the target-attribute and xi 6= xj for all
i 6= j , any pair of concordant (i.e., same k) vectors satisfy the generalization condition, yet
only the first such pair will generalize. All other vectors then become either subsumed by
this generalization or exceptions to it. If most are exceptions, this leads to the storage of
more vectors than needed, especially for large domains where various subsets of values give
rise to different target-attribute's values. Moreover, the outcome depends upon ordering
of the vectors. Also, if there exists conicts involving one (or more) value of x, then the
system will end up giving unfounded precedence to the exceptions (being more specific) and,
again, these depend on the ordering. Support for internal disjunction or a more complex
generalization scheme may help alleviate some of these problems. They are the topic of
future research.
2.4.3 Illustration

This section shows the evolution of FLARE's knowledge base as the vectors of KB (see
Section 2.1.3) are presented to it as inputs. It highlights several interesting features of
both reasoning and adaptation. Let KB 0 denote the current knowledge base of FLARE. As
165

fiGiraud-Carrier & Martinez

discussed above, FLARE starts with KB 0 = ;. Each vector is presented to FLARE in the
order in which it appears in KB .
1. Presentation of v1 . KB 0 = ;. v1 is simply added to KB 0 .
2. Presentation of v2. KB 0 = fv1g. v1 is the closest match. v1 and v2 are not concordant,
so v2 is added to KB 0 .
3. Presentation of v3. KB 0 = fv1; v2g. Same as above with either v1 or v2 . So v3 is
added to KB 0 .
4. Presentation of v4. KB 0 = fv1; v2; v3g. No winner can be found since none of the
vectors in KB 0 have the same target-attribute as v4 . So v4 is added to KB 0 . While the
earlier KB 0 had information about a single concept (i.e., stimulus-situation), the new
KB0 now provides FLARE with knowledge about a new concept, namely appropriateresponse. By \partitioning" vectors along their target-attribute, FLARE naturally
supports multiple concept learning.
5. Presentation of v5 . KB 0 = fv1 ; v2; v3; v4g. v4 is the only (and hence closest) match
since none of the other vectors in KB 0 have the same target-attribute as v5 . v4 and
v5 satisfy conditions 1-4 for generalization but violate condition 5, so v5 is added to
KB0 .
6. Presentation of v6 . KB 0 = fv1 ; v2; v3; v4; v5g. Similar to step 3 with v4 and v5 . So v6
is added to KB 0 .
7. Presentation of v7 . KB 0 = fv1; v2; v3; v4; v5; v6g. Note that v7 is a precept. No winner
can be found since none of the vectors in KB 0 have the same target-attribute as v7.
So v7 is added to KB 0 . A third concept, namely media, is now available.
8. Presentation of v8. KB 0 = fv1; v2; v3; v4; v5; v6; v7g. v7 is the only (and hence closest)
match since none of the other vectors in KB 0 have the same target-attribute as v8.
v8 is an exception to v7 since v7 covers v8 but they are not concordant. Hence, v8 is
added to KB 0 . Though v7 suggests to use lecture as a media, the added condition on
training-budget found in v8 causes that suggestion to change to lecture with slides.
9. Presentation of v9 . KB 0 = fv1; v2; v3; v4; v5; v6; v7; v8g. Only v7 and v8 may compete.
D(v7; v9) = 1=3 and D(v8; v9) = 1=4. Hence, v8 wins. v8 and v9 satisfy all five
conditions for generalization. The second attribute is dropped (i.e., replaced by ?)
in either one, say v8 , to produce v80 = (?; ?; ?; 0; 0; 0; 3T ). v80 is added to KB 0 . All of
the attributes in v8 and v9 have the same value, except for stimulus-situation. This is
sucient for FLARE to hypothesize that the value of stimulus-situation is not critical
and the attribute may thus be ignored. In other words, FLARE decides that the value
of stimulus-situation is not needed when predicting lecture with slides.
10. Presentation of v10. KB 0 = fv1; v2; v3; v4; v5; v6; v7; v80 g. Only v7 and v80 may compete.
D(v7; v10) = 1=3 and D(v80 ; v10) = 0. Hence, v80 wins. v80 covers v10 and they are
concordant, so FLARE adds 1 to num covers(v80 ). v10 need not be added to KB 0 . v10
is one of the many special cases now handled by the new generalization v80 .
166

fiAn Integrated Framework for Learning and Reasoning

11. Presentation of v11 . KB 0 = fv1 ; v2; v3; v4; v5; v6; v7; v80 g. Notice that v11 is also a
precept, so that precepts may be given at any time during learning. Only v7 and
v80 may compete. D(v7; v11) = 1=6 and D(v80 ; v11) = 1=3. Hence, v7 wins. Neither
one covers the other; they are not equal; they cannot produce generalization (violate
condition 3). Thus, v11 is added to KB 0 . Note that v11 has a static priority of 1.

12. Presentation of v12. KB 0 = fv1 ; v2; v3; v4; v5; v6; v7; v80 ; v11g. v7, v80 and v11 compete.
D(v7; v12) = 1=2, D(v80 ; v12) = 2=3 and D(v11; v12) = 1=4. Hence, v11 wins. Neither
one covers the other; they are not equal; they cannot produce generalization (violate
condition 3). Thus, v12 is added to KB 0 . Note that v12 has a static priority of 3.
Since v11 and v12 overlap, precedence would be given to v12 in case of a conict.

13. Presentation of v13. KB 0 = fv1 ; v2; v3; v4; v5; v6; v7; v80 ; v11; v12g. In this case, some
non-asserted attributes of v13 may be asserted through reasoning, before the targetattribute. Rules v2 and v4 are applied to assert the second and fourth attributes
0 = (1; 1; 0; 0; 0; 1; 2T ). v7, v80 , v11 and v12 compete to
respectively. The result is v13
0 ) = 0, D(v80 ; v130 ) = 0, D(v11; v130 ) = 0, and
assert the target-attribute. D(v7; v13
0 ) = 1=2. Both v7 and v80 win over v11 since they are more specific. However,
D(v12; v13
v7 and v80 have the same specificity. In fact, they satisfy all five conditions that
identify them as conicting defaults in the current context. Hence, FLARE adds 1
0 are concordant. Reasoning then proceeds,
to dynamic priority(v7 ) since v7 and v13
0 and they are concordant, v130 need not be
giving precedence to v7 . Since v7 covers v13
added to KB 0 .
14. Presentation of v14. KB 0 = fv1 ; v2; v3; v4; v5; v6; v7; v80 ; v11; v12g. Some non-asserted
attributes of v14 may be asserted through reasoning, before the target-attribute. Rules
v2 and v5 are applied to assert the second and fourth attributes respectively. The result
0 = (1; 1; 1; 0; 0; 1; 2T ). The rest is identical to step 13. Now, dynamic priority(v7)
is v14
0 is not added to KB0.
= 2 and v14

15. Presentation of v15. KB 0 = fv1 ; v2; v3; v4; v5; v6; v7; v80 ; v11; v12g. Some non-asserted
attributes of v15 may be asserted through reasoning, before the target-attribute. Rules
v3 and v6 are applied to assert the second and fourth attributes respectively. The result
0 = (2; 2; 2; 1; 1; 0; 0T ). v7, v80 , v11 and v12 compete to assert the target-attribute.
is v15
0 ) = 1, D(v80 ; v150 ) = 1, D(v11; v150 ) = 1, and D(v12; v150 ) = 1=2. Hence,
D(v7; v15
v12 wins. Neither one covers the other; they are not equal; they cannot produce
0 is added to KB0.
generalization (violate condition 3). Thus, v15

0 g. Both
16. Presentation of v16 and v17 . KB 0 = fv1 ; v2; v3; v4; v5; v6; v7; v80 ; v11; v12; v15
0
0
are equal to v15. Neither v16 nor v17 need be added to KB but the appropriate
0 . The result is counters[0] = 2, counters[1] =
counter values are incremented in v15
0 is
0, counters[2] = 1 and counters[3] = 0. Thus, the target-attribute's value of v15
currently 0.

The resulting KB 0 , after processing KB is shown in Figure 5. The variables p, c and dp
stand for static priority, cover number and dynamic priority, respectively. At an intuitive
level, FLARE has used both learning and reasoning mechanisms to deal with KB . Induction
167

fiGiraud-Carrier & Martinez

v1
v2
v3
v4
v5
v6
v7
v8
v11
v12
v15
0

0

=
=
=
=
=
=
=
=
=
=
=

0
1
2
?
?
?
?
?
?
?
2

0T
1T
2T
?
?
?
1
?
1
1
2

?
?
?
0
1
2
?
?
?
?
2

?
?
?
0T
0T
1T
0
0
?
1
1

?
?
?
?
?
?
0
0
0
?
1

?
?
?
?
?
?
?
1
?
?
0

?
?
?
?
?
?
2T
3T
0T
1T
0T

Figure 5: KB 0

(p = c = dp = 0)
(p = c = dp = 0)
(p = c = dp = 0)
(p = c = dp = 0)
(p = c = dp = 0)
(p = c = dp = 0)
(p = c = 0; dp = 2)
(p = 0; c = 1; dp = 0)
(p = 1; c = dp = 0)
(p = 3; c = dp = 0)
(p = c = dp = 0)

(on vectors v8; v9; v10) has allowed the system to decide that the stimulus-situation was
irrelevant in predicting the use of lecture-with-slides. Deduction from the empirical evidence
provided by vectors v13 and v14 has caused FLARE to break the \tie" between rules v7 and v80
in favor of v7. Prior knowledge relative to vectors v11 and v12 was encoded as static priorities,
thus giving precedence to v12 in case of conicts. Hence, if the vector v = (1; ?; 0:?; 0; 1; ?T )
is presented to FLARE after KB 0 is acquired, the second and fourth attributes are first
asserted as previously discussed to produce v 0 = (1; 1; 0; 0; 0; 1; ?T ). Then, v7 , v80 and v11
compete. v7 and v80 win due to specificity. v7 and v80 also have same static priorities but
v7 wins due to dynamic priority and the result is (1; 1; 0; 0; 0; 1; 2T ). Now, if the vector
(1; ?; 2; ?; 0; 0; ?T ) is presented, a similar situation arises between v11 and v12 . The conict
is resolved with static priorities.
2.4.4 Extensionality and Intensionality

As it is able to use prior knowledge in the form of precepts together with raw examples,
FLARE effectively combines the intensional approach (based on features, expressed here
by precepts) and the extensional approach (based on instances, expressed by examples) to
learning and reasoning. With this combination, FLARE can resolve conicting defaults,
such as the Nixon Diamond (Reiter & Griscuolo, 1981), by either being told explicitly
which default prevails (e.g., religious conviction is more important than political aliation)
or by computing relative dynamic priorities (see Section 2.3.5) from examples of RepublicanQuakers.
Most inductive learning systems are purely extensional, while most reasoning systems are
purely intensional. It is therefore these authors' contention that, if induction and deduction
are to be integrated, then a combination of the two approaches is desirable. It is also clear
that the combination increases exibility. On the one hand, extensionality accounts for the
system's ability to adapt to its current environment, i.e., to be more autonomous. On the
other hand, intensionality provides a mechanism by which the system can be taught and
thus does not have to unnecessarily suffer from poor or atypical learning environments.
In the context of reasoning, precepts provide a useful medium to encode certain firstorder language statements (e.g., the rule base of an expert system) that can, in turn, be
learned by FLARE (in the usual way) and later be used for reasoning purposes.
168

fiAn Integrated Framework for Learning and Reasoning

DEFINITION



Input: a set of general rules, a set of facts and one designated target-attribute
Output: one or more precepts

IMPLEMENTATION
1. Learning general knowledge: Perform Learning on the set of general rules.
2. Reason from facts: Perform Reasoning with a vector encoding the given facts and the designated
target-attribute.

Figure 6: Function Generate-Precepts

2.5 FLARE's Automatic Generation of Precepts

Section 2.1.2 introduced the notion of precepts as generalized AVL vectors in which some of
the attributes have the special value ? (i.e., don't-care). Precepts may be encoded directly
by a teacher or deduced automatically from general knowledge. FLARE provides a simple
(off-line) mechanism for the automatic generation of precepts in the preprocessing phase
described in Section 2.2.
FLARE uses prior knowledge in the form of general rules that may be viewed as encoding
\commonsense" knowledge involving some of the attributes of the application domain. With
the appropriate setting for deduction, FLARE can then generate domain-specific precepts
that can be used as biases for inductive learning or for further reasoning about the specific
domain to which they apply.
Consider the example in Table 3 from Section 2.1.1. Assume that the system is to
inductively learn rules regarding the suitability of lenses for patients from a set of examples
whose attributes include the patient's tear-production rate (tpr). The statements in Table
3 capture general knowledge about eyes. Informally, they state that:
1. Low tear-production rate causes dryness of the eyes.
2. Dry eyes are not fit for lenses.
When provided with the fact that the target-attribute of the system has to do with fitting
lenses, the general knowledge may be used to produce a domain-dependent precept that
states that, if a patient has low tear-production rate, then he/she should not be fitted lenses.
The precept, in turn, provides a useful bias to the system during further induction from
examples.
The process of generating precepts described above is essentially one of acquiring the
general knowledge (or rules) and reasoning from it as described in Figure 6. When general
rules are available, the function Generate-Precepts is always invoked prior to any other work
by FLARE.
The function Generate-Precepts actually makes use of the other functions of FLARE. In
step (1), it constructs a knowledge base from the general rules using learning as described
in Section 2.4. In step (2), it reasons, as described in Section 2.3, using the acquired
knowledge and facts enabling the general knowledge to be applied to the domain. The
169

fiGiraud-Carrier & Martinez

facts are encoded as a vector in which attributes found in the general knowledge are set to
appropriate values and all others set to ?. Since precepts are mostly used as learning biases,
the designated target-attribute is typically the target concept of an inductive application.
In the lenses example of Table 3, the appropriate setting is obtained by creating a vector
such that attribute Tpr is set to low and attribute Fit is designated as the target-attribute.
Having incorporated the two rules in its knowledge base in step (1), FLARE would then
easily deduce a precept of the form: if Tpr is low then Fit is false, independent of any other
don't-care conditions.
Though the function Generate-Precepts is automated, the setting of the relevant attributes and interpretation of the result rely on a teacher. More automatic mechanisms
may be considered, where the system could try any combination of a learning problem's
attributes values to instantiate general knowledge. Then, any such instantiation that causes
the target-attribute to become asserted is a potential precept. However, that process would
be exponential and most of it would probably not lead to any useful conclusion.

3. Experimental Results and Demonstrations

A set of classical commonsense benchmark problems has been proposed by Lifschitz (1988)
and the UCI repository (Murphy & Aha, 1992) contains many useful training sets for
inductive learning. This section reports results obtained with FLARE on several of these
datasets. Results on a number of other uses of the framework, including two expert systems,
are also presented. Finally, some of the limitations of the system are described.
One artifact of the implementation is that, since variables cannot be added dynamically,
all attributes must be defined a priori. All attributes that do not appear in rules, examples
or precepts are set to don't-care. This is consistent with the semantics of don't-care and does
not interfere with the algorithm since the distance D essentially treats learned don't-cares
as neutral values.

3.1 Inductive Learning and Prior Knowledge

In order to test the predictive accuracy of FLARE, the standard training set/test set approach is used. The value of v 's target-attribute is provided but it is not used during
reasoning. Rather, the system reasons based on its current knowledge base and all of the
asserted attributes of v . When reasoning is completed, the \computed" target value is
compared with the \actual" target value.
Several datasets from the UCI repository (Murphy & Aha, 1992) were chosen. They
represent a wide variety of situations, as shown in Table 6. The column labelled \Size"
indicates the total number of examples in the dataset. The column labelled \Attributes"
records the number and type (L for linear, N for nominal) of all the attributes, other than
the target (or output) attribute. The column labelled \Output" shows the number of output
classes.
FLARE's results were gathered for each of the above applications, using 10-way crossvalidation. Each dataset is randomly broken into 10 sets of approximately equal size. Then,
in each turn, one of the sets is used for testing, while the remaining 9 are used for learning.
This process is repeated 10 times, one for each test set, so that every item of data is in the
test set once and only once. Because FLARE's outcome is dependent upon the ordering of
170

fiAn Integrated Framework for Learning and Reasoning

Application
lenses
voting-84
tic-tac-toe
hepatitis
zoo
iris
soybean-small
segmentation
glass
breast-cancer
sonar

Size Attributes Output
24
4N
3
435
16N
2
958
9N
2
155 6L&13N
2
90
16N
7
150
4L
3
47 4L&31N
4
420
19L
7
214
9L
7
699
9L
2
208
60L
2

Table 6: Selected Applications
Application PA IR ID3 CN2
lenses
79.0 .43 65.0 83.3
voting-84
92.9 .63 95.4 93.8
tic-tac-toe
81.5 1.0 85.6 98.0
hepatitis
80.0 .94 77.9 76.1
zoo
97.4 .36 97.8 93.3
iris
94.0 .13 94.0 93.3
soybean-small 100 .98 98.0 100
segmentation 94.0 .99 96.9 94.1
glass
71.8 .22 67.7 62.7
breast-cancer 96.6 .47 95.1 95.1
sonar
83.8 .77 77.4 44.3
Averages
88.3 .63 86.4 84.9
Table 7: FLARE: Induction

BP
76.7
96.0
96.6
97.8
97.3
100
38.0
99.7
87.8

data during learning, each turn was repeated 10 times with a new random ordering of the
training set. The predictive accuracy for a given turn is the average of the 10 corresponding
trials and the predictive accuracy for the dataset is the average of the 10 turns.
Results are shown in Table 7. The first number (PA) represents predictive accuracy
(in %) on the test set after training and the second number (IR) is the inductive ratio,
defined as the ratio of the size (in number of rules) of the final knowledge base to the
number of instances used in learning. IR is another measure of the generalization power of
FLARE, as well as an indication of FLARE's memory requirements. Results of PA with ID3
(Quinlan, 1986), ordered CN2 (Clark & Niblett, 1989) and Backpropagation (Rumelhart &
McClelland, 1986) are also included for comparison. They were also obtained using 10-way
cross-validation and are as reported by Zarndt (1995).
For the set of selected applications, FLARE's performance in generalization compares
favorably with that of ID3, CN2 and Backpropagation, as well as with that of other inductive
171

fiGiraud-Carrier & Martinez

Application
lenses
voting-84
tic-tac-toe
hepatitis
zoo
Averages

no prec.
79.0 - .43
92.9 - .63
81.5 - 1.0
80.0 - .94
97.4 - .36
86.2 - .67

w/prec.
80.5 - .33
94.5 - .25
88.5 - .72
81.2 - .68
97.4 - .32
88.4 - .46

Table 8: FLARE: Induction with Prior Knowledge
learning algorithms (e.g., see Aha et al., 1991; Wettschereck & Dietterich, 1994; Zarndt,
1995). In addition, the knowledge base maintained by FLARE is generally significantly
smaller than the set of all training vectors.
The first five applications were further selected to illustrate the effect of prior knowledge
on predictive accuracy and inductive ratio. For each application, the above experimental
procedure is repeated but the set of training examples is now augmented by precepts given
a priori (i.e., before the training set is presented). Results are reported in Table 8. Each
column shows both PA and IR. Here, the precepts are obtained from domain knowledge
provided with the application (voting-84) or generated from the authors common sense (zoo,
lenses, hepatitis, tic-tac-toe). They serve as learning biases. The results with precepts show
an average increase of 2.6% in predictive accuracy and a decrease of 31.3% of the inductive
ratio. The decrease in IR demonstrates that prior knowledge allows pruning of parts of the
input space during learning. Indeed, starting with the same number of training vectors,
FLARE ends up with a knowledge base containing about one-third less vectors than when
precepts are not used. Hence, precepts not only increase generalization performance, they
also reduce memory requirements.
The lenses application was also used to demonstrate how precepts may be generated
automatically by deducing domain-dependent information from general knowledge, as discussed in Section 2.5. The example of Table 3 from Section 2.1.1 was implemented (as
described in Section 2.5) and a precept stating that, if the Tpr attribute is set to low, then
lenses should not be prescribed was generated. That precept was, in turn, used prior to
performing inductive learning as described above.
The process of inductive learning with automatically generated prior knowledge is twophase, where both phases perform the same operations on different pieces of information. In
the first phase, general knowledge expressed as rules (and translated into AVL) is learned by
FLARE. Then FLARE reasons based on some instantiation that links the general knowledge
to the current domain. The result of this reasoning phase is one (or more) precept containing
domain-dependent information. In the second phase, FLARE learns from the generated
precepts and any other available examples. The result is a set of inductively generated
rules.

3.2 Classical Reasoning Protocols

Several problems from the set of Benchmark Problems for Formal Nonmonotonic Reasoning
(Lifschitz, 1988), were presented to FLARE. The problems were first translated into their
172

fiAn Integrated Framework for Learning and Reasoning

corresponding AVL representation. FLARE is able to properly incorporate the premises
and correctly derive the expected conclusions for the following classes of problems from
(Lifschitz, 1988):
 A1 - basic default reasoning.
 A2 - default reasoning with irrelevant information
 A3 - default reasoning with several defaults
 A5 - default reasoning in an open domain
 A9 - priority between defaults
 B1 - linear inheritance (top-down)
 B2 - tree-structured inheritance
 B3 - one-step multiple inheritance
 B4 - multiple inheritance
Problem A4 involves a disabled default and problems A6 through A8 deal with unknown
exceptions. Such problems cannot be represented in FLARE. Problems A10 and A11 deal
with instances of defaults and reasoning about priority. Though not directly representable in
FLARE, they are effectively solved via the use of static (A10) or dynamic (A11) priorities.
The other classes of problems defined by Lifschitz (1988) (i.e., reasoning about actions,
uniqueness of names and autoepistemic reasoning) are beyond the current scope of FLARE.
Note that, in order to work properly, some of the above problems require added processing. In particular, problems A1, A2, A3 and A5 involve both classes of objects and
particular instances of these classes. Problem A1, for example, is given as follows: blocks
A and B are heavy, heavy blocks are normally located on the table, A is not on the table.
Translating to AVL gives: A 1T ?, B 1T ?, ? 1 1T and A ? 0T , where the first attribute is
a multi-valued attribute representing the objects in the universe and the second and third
attributes are Boolean, encoding the predicate heavy and ont able respectively. Now, if A
? ?T is shown, A 1 ?T will be derived from the first vector and will then match both ?
1 1T and A ? 0T exactly. It seems reasonable that priority should be given to the later
since it involves A (an instance) explicitly. To solve this problem, vectors involving explicit
references to instances of objects have their static priority set to 1 while all other vectors
have their static priority set to 0. This is, of course, an artifact of encoding. An alternative
is to write all facts relative to a given instance as a definition whose target-attribute is the
instance value. Then completion would guarantee the correct outcome.
The above problems are characteristics of important forms of human patterns of reasoning. However, they are artificial as they have been manufactured explicitly with the intent
of isolating one salient feature of nonmonotonic reasoning, independent of all others. To
further investigate the properties of FLARE and the combination of learning and reasoning,
other more \real-world" applications must be designed and experimented with. Section 3.5
presents such preliminary applications. The next two sections present simple applications
that further exercise FLARE's ability to learn incrementally and to combine learning with
reasoning in useful ways.
173

fiGiraud-Carrier & Martinez

3.3 The Nixon Diamond

The Nixon Diamond (Reiter & Griscuolo, 1981), reproduced as Table 1 in Section 2.1.1, is
important as a prototype of a class of interesting problems involving conicting defaults. It
is used here to demonstrate FLARE's mechanisms to handle such conicts both intensionally
and extensionally.
FLARE's static priorities offer a simple way of resolving the Nixon Diamond intensionally, based on some externally provided information (e.g., religious convictions supersede
political aliations). In that case, both defaults are given along with an appropriate static
priority.
Another alternative consists of providing the defaults without any priority. This corresponds to a possibly more natural situation where the system really is in a don't-know
state when it comes to deciding on Nixon's dispositions. Yet, such don't-know states are
uncomfortable and it is the authors' contention that any kind of information that may allow
a decision to be made should be used. Hence, a simple epistemological approach is adopted,
where the conict arises due to beliefs rather than facts. In this case, it is possible to
attempt to resolve the conict by observing instances of Republican-Quaker. The relative
number of pacifists and non-pacifists can then serve as evidence to lean towards one decision
or the other. In other words, it is the system's observation of what seems most common in
its environment that creates its belief. This is not unlike the way humans deal with many
similar situations.
A final approach, which combines inductive learning and reasoning, consists of not
providing the system with any default. Rather, examples of Republicans, Quakers and
Republican-Quakers are shown and the system automatically comes up with both the defaults (through induction) and their relative priorities.
All three of these experiments were run with FLARE and the results are as expected.
In the third case, the actual knowledge base depends upon the ordering. It consists of one
vector for Republicans that are not Quakers or one vector for Quakers that are not Republicans, one default vector for Quakers or Republicans and one vector for Republican-Quakers.
The target value of the vector for Republican-Quakers is not had via dynamic priority but
via the counters. Functionally, however, the result is identical.

3.4 Do Birds Typically Fly?

Incremental learning is one of FLARE's important features. With incrementality, the system
is self-adaptive in the sense that its current knowledge base is representative of its experience
with its environment so far. And the knowledge base can be continually updated as new
information becomes available. To exercise incrementality a simple example of bottom-up
inheritance involving birds was designed.
The application has four attributes, two of which correspond to Ostrich and Bird. The
other two are other (undetermined) attributes of birds (e.g., Feather). The target-attribute
is Boolean and characterizes the ability to y. At first, the system is exposed mostly to
ostrich-birds (maybe the experiment is started in Australia). When asked whether birds
typically y (i.e., only the Bird attribute is asserted and all other inputs are don't-know),
FLARE concludes that birds do not y, which is consistent with its current experience with
the \world." However, as more new instances of ying birds (i.e., other than ostriches and
174

fiAn Integrated Framework for Learning and Reasoning

penguins) are encountered, FLARE adapts its knowledge and when asked again, concludes
that birds y. Correct knowledge about ostrich-birds is also preserved. That is, if the
system is shown an ostrich, it will still conclude that the ostrich does not y.
Of course, a precept may also be given to the system at any given time, stating that
birds typically y. The idea is that FLARE offers both options naturally. The system may
be taught so as not to suffer from poor or atypical learning environments (e.g., Australia for
birds' ying ability prediction), or it may be left to adapt to its environment. As research
on autonomous agents continues, this later ability becomes important.
Note that the above example also illustrates one of FLARE's limitations. The system
either concludes that birds do y or that they do not. There is no mechanism for representing a middle ground in such a way that FLARE could reason about it at the meta-level.
Decisions made in the presence of conicts are also \crisp" as demonstrated by the simple,
rigid conict resolution mechanism discussed in Section 2.3.5. Even though, the system may
be able to produce more fuzzy-like results by associating each decision with a confidence
level, it would still not be able to reason about these at the meta-level.

3.5 Learning Expert Systems
In order to better assess FLARE's reasoning mechanisms, two expert system knowledge
bases are used. One is called mediadv (Harmon & King, 1985) and is intended to help
designers or committees choose the most appropriate media to deliver a training program.
It consists of 20 rules with chains of inference of length 2 at most. The other is called health
(Sawyer & Foster, 1986) and is intended to predict the longevity of patients based on a
variety of factors (e.g., weight, personality, etc.). It is much larger as it contains 77 rules
and more complex as it involves longer chains of inference. Five rules were left out as one
is redundant (i.e., rule 17 is identical to rule 14) and four are only needed in the interactive
setting in which the original system is described. Hence, only 72 rules are considered.
Both sets of rules were translated into AVL. The 20 rules of mediadv produce 99 vectors
and the 72 rules of health produce 72 vectors. The number of vectors for mediadv is much
larger than the original number of rules because many of the rules contain internal disjunctions. In AVL, a new vector must be constructed for each possible combination arising from
the disjunctions. For example, the rule: If ((A=1 or A=2 or A=5) and (B=2.9 or B=7.8))
then C, gives rise to 6 vectors corresponding to the equivalent set of rules: If ((A=1) and
(B=2.9)) then C, If ((A=1) and (B=7.8)) then C, If ((A=2) and (B=2.9)) then C, etc.
The sets of vectors corresponding to the original knowledge bases are not encoded into
FLARE. Rather, they are presented to the system to be learned. Hence, some generalization
may take place. In fact, the final number of vectors (after learning) in mediadv is only 71,
while in health it is 65.
The mediadv example is clearly very simple and presents little interest in terms of
deduction. However, its purpose here is to show how the system's current knowledge base
may be updated through learning. Of particular interest is the case of conicts that arise
because two or more rules may apply to a given situation, while implying different goal
values. In mediadv, such a conict exists between rules 13 and 14 and between rules 16 and
17. Rules 13 and 14 are used as illustration. Let X be some fixed conjunction of conditions
not shown. Then:
175

fiGiraud-Carrier & Martinez

 rule 13: if (X) and (training budget = small or training budget = medium) then

media to consider = lecture
 rule 14: if (X) and (training budget = medium) then media to consider = lecturewith-slides
It is clear that, in some cases, these rules conict. The important issue is that it is dicult to avoid such occurrences in large knowledge bases elicited from experts. As FLARE
supports learning, it is possible, however, to look at various (historical) situations where
training budget was medium and check which media was used then. This information can,
in turn, be used to give precedence to one rule over the other. Moreover, this precedence
need not be fixed after so many examples have been considered. Indeed, it may evolve over
time and even change radically depending on circumstances.
An example, using several additional instances of [(X) and (training budget = medium)]
together with a target value for media to consider, was implemented. The instances used
caused the value of dynamic priority of rule 13 to be greater than that of rule 14, thus
effectively giving (evidential) precedence to rule 13.
Our experiments with the health knowledge base demonstrate FLARE's ability to perform deduction. The experiments conducted involve chains of inference of reasonable lengths
and are fairly intuitive. Results are summarized in Table 9. The first column contains the
list of attributes used in the knowledge base. Then, each pair (setting, result) of columns
represents an experiment in reasoning with the knowledge base. The setting column contains the data FLARE starts with. Unknown conditions (or attributes) are initialized as
don't-knows (i.e., ?). The result column shows the state of knowledge after reasoning. Each
derived piece of information is italicized and subscripted by the depth of inference at which
it was derived.
Starting with the facts in the setting column, FLARE successively infers new conclusions
until it reaches a value for the top goal, longevity. Details of the inference process are given
for the first setting only. They are easily extended to the other settings. The first setting
corresponds to an average adult female of Asian race, with little vices or excesses and a
reasonable diet. FLARE first infers that:
 Her relative weight is normal (absolute weight < 110 lbs and small frame).
 Her personality type is A, as she is aggressive.
 Her blood pressure is average (normal fat and salt intake).
 Her base longevity is average, namely 67 (range is 48-84).
 Her chances of living longer (i.e., add years to base-longevity) are good.
And then, based on this added information, infers that her risk is actually high and though
the chances of living longer are good, the actual value added to base-longevity is 0 (i.e.,
factor = none). Finally, as one would have expected, the woman's longevity is predicted to
be average (i.e., 67).
The other settings further illustrate FLARE's ability to perform forward chaining. The
second setting corresponds to a very unhealthy older male whose longevity is accordingly
176

fiAn Integrated Framework for Learning and Reasoning

setting 1 result 1
setting 2
result 2
rel. weight
?
normal1
?
obese1
val
?
yes2
?
?
heart-dis-risk
?
?
?
> average1
hddanger
?
?
?
?
start
yes
yes
yes
yes
age
25-55
25-55
>55
>55
gender
F
F
M
M
base-longevity
?
671
?
601
weight
<110
<110
>170
>170
frame
small
small
small
small
cholesterol
?
?
high
high
fat intake
normal
normal
high
high
salt intake
normal
normal
high
high
blood-pressure
?
average1
?
> average1
calcium
?
?
?
?
osteoporo-risk
?
?
?
?
smoker
no
no
yes
yes
outlook
?
?
?
bleak2
race
asian
asian
caucasian caucasian
origin
?
?
medit.
medit.
risk
?
high2
?
high2
personality
aggressive aggressive aggressive aggressive
person. type
?
type a1
?
type a1
alcohol cons.
moderate moderate excessive excessive
add
?
good1
?
poor1
factor
?
none3
?
minus 123
longevity
?
674
?
484

setting 3
?
?
?
?
yes
<25
F
?
110-170
large
low
normal
normal
?
normal
?
no
?
caucasian
n-amer.
unknown
docile
?
none
?
?
?

result 3
normal1
yes2
average1
low2
yes
<25
F
721
110-170
large
low
normal
normal
average1
normal
average1
no
fair3
caucasian
n-amer.
unknown
docile
type b1
none
fair1
plus 124
845

Table 9: Health Knowledge Base
predicted to be low and the third setting describes a young healthy female whose life is
expectedly predicted to be quite long. Note that though the results may seem impressive,
the experiments are only \anecdotal."
Note that as in classical expert systems, the identification of a closest match during reasoning could be used to extend FLARE so that it may query the user for missing information
as well as justify both the queries and the decisions made.

3.6 Limitations
The above applications serve to demonstrate that FLARE holds promise. However, FLARE
has many important limitations, several of which were mentioned throughout the paper.
Some of them are summarized here.
FLARE's use of AVL as a representation language limits its applicability to relatively
simple problems. Induction and deduction are carried out within the confines of nonrecursive, propositional logic. Such a restriction makes the combination of learning and
reasoning more accessible since much research has taken place within this context. However,
177

fiGiraud-Carrier & Martinez

first-order predicate logic seems a minimum requirement for any system claiming reasoning
abilities.
Although FLARE produces good results, the applications it was tested on are relatively
simple. For example, many of the databases in the UCI repository have low complexity
and relatively unsophisticated learning methods perform well on them. This explains why
FLARE's extremely coarse generalization scheme seems sucient to attain reasonable predictive accuracy. Similarly, the reasoning problems presented are somewhat straightforward.
It follows that simple mechanisms such as static priorities and other counting devices used
by FLARE are sucient.
FLARE does not have any meta-level abilities. The system is unable to reason about
its own knowledge and is subsequently unable to produce meaningful middle ground solutions. Yet, work on Cyc (Guha & Lenat, 1994) strongly suggests that meta-knowledge is
indispensable in carrying out uncertain reasoning.
It is clear that FLARE only \scratches the surface" of the problem of effectively and
eciently combining induction and deduction. Work on ILP (Muggleton, 1992) may shed
some light on the issue of bringing systems like FLARE to a first-order logic level.

4. Related Work
FLARE follows in the tradition of PDL2 (Giraud-Carrier & Martinez, 1994b) and ILA
(Giraud-Carrier & Martinez, 1995), as it attempts to combine inductive learning using
prior knowledge together with reasoning. Unlike PDL2 and ILA whose prior knowledge
must be pre-encoded and whose reasoning power is limited to classification (i.e. 1-step forward inferences only), FLARE supports the automatic generation of precepts and forward
chaining to any arbitrary depth. Whereas PDL2's actual operation tends to decouple learning and reasoning (i.e., the system essentially uses distinct mechanisms to perform either
one), ILA implements an inherently more incremental approach by combining them into a
2-phase algorithm that always reasons first and then adapts accordingly. FLARE further
extends ILA by providing a natural transformation from constrained first-order clauses to
attribute-value vectors and a more accurate characterization of conicting defaults.
In attempting to construct an unified framework for learning and reasoning, FLARE
follows a synergistic approach, similar (at least in concept) to that taken in SOAR (Laird,
Newell, & Rosenbloom, 1987) and NARS (Wang, 1993) for example. There are also a
variety of inductive learning models and reasoning systems that bear similarity with the
corresponding components of FLARE. Some of them are discussed here.
Induction in FLARE is carried out much the same way as in NGE (Salzberg, 1991).
However, because generalization is effected only by setting some attribute(s) to don't-care,
the produced generalizations or generalized exemplars (Salzberg, 1991), are hyperplanes,
rather than hyperrectangles, in the input space. Hence, FLARE implements a nearesthyperplane learning algorithm. FLARE also uses static and dynamic priorities to break
ties between equidistant generalizations. Moreover, where it was shown that overlapping
hyperrectangles may hinder performance (Wettschereck & Dietterich, 1994), FLARE allows
overlapping hyperplanes for purposes of dealing with conicting defaults.
In the case that no generalizations are constructed from the training examples, FLARE
degenerates into a restricted form of MBR (Stanfill & Waltz, 1986). The distance metric
178

fiAn Integrated Framework for Learning and Reasoning

used is similar to IBL's metric (Aha et al., 1991) but it also handles don't-care attributes
(which are non-existent in instance-based learners) and treats missing attributes somewhat
differently. Where IBL considers missing attributes to be complete mismatches, FLARE
chooses a more middle-ground approach that may better capture the inherent notion of
missing or \don't-know" attributes.
Learning in FLARE contrasts with algorithms such as CN2 (Clark & Niblett, 1989),
where all training examples must be available a priori. Rather, FLARE follows an incremental approach similar to that argued by Elman (1991), except that it is the knowledge
itself that is evolved, rather than the system's structure. Moreover, learning in FLARE
can be effected continually. Any time an example or a precept is presented and its target
output is known, FLARE can adapt.
Prior knowledge may take a variety of forms, some of which are discussed by Mitchell
(1980) and Buntine (1990). The form most relevant to FLARE consists of domain-specific
inference rules, either pre-encoded or deduced from more general rules. Systems that explicitly combine inductive learning with this kind of prior knowledge include PDLA (GiraudCarrier & Martinez, 1993), ScNets (Hall & Romaniuk, 1990), ASOCS (Martinez, 1986) and
ILP (Muggleton, 1992; Muggleton & De Raedt, 1994). ScNets are hybrid symbolic, connectionist models that aim at providing an alternative to knowledge acquisition from experts.
Known rules may be pre-encoded and new rules can be learned inductively from examples.
The representation lends itself to rule generation but the constructed networks are complex
and generalization does not appear trivial. ASOCS and PDLA are dynamic, self-organizing
networks that learn, incrementally, from both examples and rules. In ASOCS, order matters and conicts are simply solved by giving priority to the most recent rules. PDLA is
less order-dependent and provides evidence-driven mechanisms for the handling of conicts.
As in ScNets, prior knowledge in ASOCS and PDLA takes the form of explicitly encoded,
domain-specific rules. FLARE's approach is more exible. Because the system can reason,
domain-specific rules (or precepts) can be deduced automatically from more general rules.
ILP models offer the same exibility. At the intersection of logic programming and inductive learning, ILP takes advantage of the full expressiveness of first-order predicate logic to
learn first-order theories from background theories and examples. FLARE's representation
language, though capable of handling both nominal and linear (including continuous and
numerical) data, is only as expressive as non-recursive, propositional clauses. However, in
this simpler setting, FLARE supports evidential reasoning and the prioritization of rules.
FLARE's use of rules and similarity in reasoning is similar to CONSYDERR's (Sun,
1992). However, CONSYDERR is strictly concerned with a connectionist approach to concept representation and commonsense reasoning. The resulting model is elegant. It consists
of a two-level architecture that naturally captures the dichotomy between concepts and the
features used to describe them. However, it does not address the problem of learning (how
such a skill could be incorporated is also unclear) and is currently limited to reasoning from
concepts. FLARE's representation is not as elegant but the model can effectively reason
from concepts or from features. CONSYDERR deals only with Boolean features and a
concept's representation is limited to a single conjunction of features. FLARE's concepts
generally consist of several conjunctions of features, each representing partial and complementary definitions of the concept. Also, since the domain of features is not restricted,
FLARE uses a more general distance metric than CONSYDERR's similarity measure based
179

fiGiraud-Carrier & Martinez

on feature overlap. However, FLARE currently has no mechanisms for individual weighting
of features, which may cause performance degradation and increased memory requirements
in the presence of a large number of irrelevant features.
FLARE's ability to evolve its knowledge base over time is similar to that found in
theory-refinement systems such as RTLS (Ginsberg, 1990), EITHER (Ourston & Mooney,
1990, 1994) and KBANN (Towell, Shavlik, & Noordewier, 1990; Towell & Shavlik, 1994).
RTLS implements a 3-phase algorithm for refinement. It first reduces the current theory
to a form suitable for inductive learning, then performs learning and, finally, retranslates
the result into a new theory. This process is potentially costly. In FLARE, the language
of the theory is the same as the language of induction, that is, the theory is always in
reduced form. Though the language is not as rich, it allows revision to take place eciently
for each new example, incrementally. EITHER is similar to FLARE as it assumes an
approximate theory and allows correction of both overly-general and overly-specific rules.
The mechanisms for revision are different. EITHER may add/remove antecedents and rules,
while FLARE may remove antecedents and add rules and exceptions. EITHER currently
only handles Boolean attributes, while FLARE has no such restriction. However, EITHER
uses both explanation-based learning and inductive learning in revision, while FLARE is
strictly inductive. KBANN, like EITHER, only deals with propositional, non-recursive
Horn clauses. Prior knowledge is expressed identically to FLARE's pre-encoded precepts
(i.e., domain-specific inference rules in the form of Prolog-like clauses). KBANN translates
the given knowledge base into an equivalent artificial neural network (ANN) and may then
perturb it and learn using the backpropagation algorithm. In FLARE, there is no ANN;
the knowledge base is simply stored as individual rules. Overall, FLARE provides a slightly
more general and synergistic approach. New evidence is constantly used to revise the current
state of knowledge. There are currently no mechanisms in FLARE to deal explicitly with
fuzzy rules. However, several mechanisms exist to handle inconsistencies and conicts.
FLARE always makes a decision based on available evidence. A confidence level can also
be produced to characterize the \goodness" of the decision.
FLARE's limited handling of non-monotonicity differs from the approach taken in logic.
Non-monotonic logics typically extend first-order predicate logic through added \machinery," such as circumscription (McCarthy, 1980), semi-normal defaults (Reiter & Griscuolo,
1981) or hierarchical theories (Konolige, 1988), while essentially preserving consistency.
FLARE's approach consists of tolerating inconsistencies in the knowledge base but providing reasoning mechanisms that ensure that no inconsistent conclusions are ever reached.
It essentially consists of using normal defaults for inheritance and an external criterion for
cancellation (Vilain, Koton, & Chase, 1990). The current criterion relies mostly on a simple
counting argument (for dynamic priorities and covers). Though this approach has proven
sucient for the simple propositional examples described here, it is likely to break down on
more sophisticated examples and domains.

5. Conclusion
This paper highlights some of the interdependencies between learning and reasoning and
details a system, called FLARE, that combines inductive learning using prior knowledge
180

fiAn Integrated Framework for Learning and Reasoning

together with reasoning within the confines of non-recursive, propositional logic. Several
important positive conclusions may be drawn from the results of this research. In particular,

 Performance in induction is improved in terms of both memory requirement and
generalization when prior knowledge is used.

 Induction from examples can be used to effectively resolve conicting defaults extensionally.

 Combining rule-based and similarity-based reasoning provides a useful means of performing approximate reasoning and tends to reduce brittleness.

 Induction offers a valuable complement to classical knowledge acquisition techniques
from experts.

Experiments with FLARE on a variety of applications demonstrate promise. However,
much work still remains to be done to achieve a more complete and meaningful integration
of learning and reasoning. Areas of future work include the following:






Designing mechanisms to use reasoning to guide learning.






Possibly incorporating backward chaining.

Attempting to overcome (or appropriately use) the order-dependency.
Providing support for internal disjunction.
Improving the use of inductively learned rules in reasoning (the support is available
but the induction may not produce useful rules).
Translating the system's knowledge base back from AVL to FOL.
Further experimenting with larger applications.
Extending the language to first-order.

Acknowledgements
This work was supported in part by grants from Novell Inc. and WordPerfect Corp. Many
thanks also to our reviewers for helpful and constructive comments.

References

Aha, D., Kibler, D., & Albert, M. (1991). Instance-based learning algorithms. Machine
Learning, 6, 37{66.
Buntine, W. (1990). A Theory of Learning Classification Rules. Ph.D. thesis, University of
Technology, School of Computing Science, Sidney, Australia.
181

fiGiraud-Carrier & Martinez

Clancey, B., & Shortliffe, E. (Eds.). (1984). Readings in Medical Artificial Intelligence: The
First Decade. Reading, MA: Addison-Wesley Publishing Company.
Clark, K. (1978). Negation as failure. In Gallaire, H., & Minker, J. (Eds.), Logic and
Databases, pp. 293{322. Plenum Press.
Clark, P., & Niblett, T. (1989). The CN2 induction algorithm. Machine Learning, 3,
261{283.
Collins, A., & Michalski, R. (1989). The logic of plausible reasoning: A core theory. Cognitive Science, 13, 1{49.
D'Ignazio, F., & Wold, A. (1984). The Science of Artificial Intelligence, p. 13. Franklin
Watts Library Edition.
Duda, R., & Reboh, R. (1984). AI and decision making: The PROSPECTOR experience.
In Reitman, W. (Ed.), Artificial Intelligence Applications for Business. Norwood, NJ:
Ablex Publishing Corp.
Dzeroski, S., Muggleton, S., & Russell, S. (1993). Learnability of constrained logic programs.
In Proceedings of the European Conference on Machine Learning (ECML'93), LNAI
667, pp. 342{347.
Elman, J. (1991). Incremental learning, or the importance of starting small. Tech. rep.
CRL 9101, University of California, San Diego, Center for Research in Language, La
Jolla, CA.
Ginsberg, A. (1990). Theory reduction, theory revision, and retranslation. In Proccedings
of the Eighth National Conference on Artificial Intelligence (AAAI'90), pp. 777{782.
Giraud-Carrier, C., & Martinez, T. (1995). ILA: Combining inductive learning with prior
knowledge and reasoning. Tech. rep. CSTR-95-03, University of Bristol, Department
of Computer Science, Bristol, UK.
Giraud-Carrier, C., & Martinez, T. (1993). Using precepts to augment training set learning.
In Proceedings of the First New Zealand International Two-Stream Conference on
Artificial Neural Networks and Expert Systems (ANNES'93), pp. 46{51.
Giraud-Carrier, C., & Martinez, T. (1994a). An ecient metric for heterogeneous inductive
learning applications in the attribute-value language. In Proceedings of the Third
Golden West International Conference on Intelligent Systems (GWIC'94), Vol. 1, pp.
341{350. Kluwer Academic Publishers.
Giraud-Carrier, C., & Martinez, T. (1994b). An incremental learning model for commonsense reasoning. In Proceedings of the Seventh International Symposium on Artificial
Intelligence (ISAI'94), pp. 134{141.
Guha, R., & Lenat, D. (1994). Enabling agents to work together. Communications of the
ACM, 37 (7), 126{142.
182

fiAn Integrated Framework for Learning and Reasoning

Haas, N., & Hendrix, G. (1983). Learning by being told: Acquiring knowledge for information management. In Michalski, R., Carbonell, J., & Mitchell, T. (Eds.), Machine
Learning: An Artificial Intelligence Approach, Vol. I, chap. 13. Morgan Kaufmann
Publishers, Inc.
Hall, L., & Romaniuk, S. (1990). A hybrid connectionist, symbolic learning system. In
Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI'90),
pp. 783{788.
Harmon, P., & King, D. (1985). Expert Systems. John Wiley & Sons, Inc.
Konolige, K. (1988). Hierarchic autoepistemic theories for nonmonotonic reasoning. In
Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI'88),
pp. 439{443.
Laird, J., Newell, A., & Rosenbloom, P. (1987). SOAR: An architecture for general intelligence. Artificial Intelligence, 33, 1{64.
Lavrac, N., Dzeroski, S., & Grobelnik, M. (1991). Learning nonrecursive definitions of relations in LINUS. In Proceedings of the Fifth European Working Session on Learning,
pp. 265{281.
Lifschitz, V. (1988). Benchmark problems for formal nonmonotonic reasoning. In Proceedings of the Second International Workshop on Non-Monotonic Reasoning, LNCS 346,
pp. 202{219.
Martinez, T. (1986). Adaptive Self-Organizing Networks. Ph.D. thesis, University of California, Los Angeles. Tech. rep. CSD 860093.
McCarthy, J. (1980). Circumscription: A form of nonmonotonic reasoning. Artificial Intelligence, 13, 27{39.
Michalski, R. (1983). A theory and methodology of inductive learning. Artificial Intelligence,
20, 111{161.
Minsky, M., & Riecken, D. (1994). A conversation with Marvin Minsky about agents.
Communications of the ACM, 37 (7), 22{29.
Mitchell, T. (1980). The need for biases in learning generalizations. Tech. rep. CBM-TR
5-110, Rutgers University, New Brunswick, NJ.
Muggleton, S. (Ed.). (1992). Inductive Logic Programming. Academic Press.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory and methods.
Journal of Logic Programming, 19,20, 629{676.
Murphy, P., & Aha, D. (1992). UCI repository of machine learning databases. Tech. rep.,
University of California, Irvine, Department of Information and Computer Science.
183

fiGiraud-Carrier & Martinez

Ourston, D., & Mooney, R. (1990). Changing the rules: A comprehensive approach to
theory refinement. In Proceedings of the Eighth National Conference on Artificial
Intelligence (AAAI'90), pp. 815{820.
Ourston, D., & Mooney, R. (1994). Theory refinement combining analytical and empirical
methods. Artificial Intelligence, 66 (2), 273{309.
Quinlan, J. (1986). Inductive learning of decision trees. Machine Learning, 1, 81{106.
Reiter, R., & Griscuolo, G. (1981). On interacting defaults. In Proceedings of the Seventh
International Joint Conference on Artificial Intelligence (IJCAI'81), pp. 270{276.
Rumelhart, D., & McClelland, J. (1986). Parallel and Distributed Processing: Explorations
in the Microstructure of Cognition, Vol. 1. MIT Press.
Rychener, M. (1983). The instructible production system: A retrospective analysis. In
Michalski, R., Carbonell, J., & Mitchell, T. (Eds.), Machine Learning: An Artificial
Intelligence Approach, Vol. I, chap. 14. Morgan Kaufmann Publishers, Inc.
Salzberg, S. (1991). A nearest hyperrectangle learning method. Machine Learning, 6,
251{276.
Sawyer, B., & Foster, D. (1986). Programming Expert Systems in Pascal. John Wiley &
Sons, Inc.
Stanfill, C., & Waltz, D. (1986). Toward memory-based reasoning. Communications of the
ACM, 29 (12), 1213{1228.
Sun, R. (1992). A connectionist model for commonsense reasoning incorporating rules and
similarities. Knowledge Acquisition, 4, 293{321.
Towell, G., & Shavlik, J. (1994). Knowledge-based artificial neural networks. Artificial
Intelligence, 70 (1-2), 119{165.
Towell, G., Shavlik, J., & Noordewier, M. (1990). Refinement of approximate domain
theories by knowledge-based neural networks. In Proceedings of the Eighth National
Conference on Artificial Intelligence (AAAI'90), pp. 861{866.
Vilain, M., Koton, P., & Chase, M. (1990). On analytical and similarity-based classification.
In Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI'90),
pp. 867{874.
Wang, P. (1993). Non-axiomatic reasoning system (version 2.2). Tech. rep. 75, Indiana
University, Center for Research on Concepts and Cognition, Bloomington, IN.
Waterman, D. (1986). A Guide to Expert Systems. Addison Wesley.
Wettschereck, D., & Dietterich, T. (1994). An experimental comparison of the nearestneighbor and nearest-hyperrectangle algorithms. Machine Learning, 19, 5{28.
184

fiAn Integrated Framework for Learning and Reasoning

Wollowski, M. (1994). Case-based reasoning as a means to overcome the frame problem. In
Proceedings of the Seventh Florida AI Research Symposium (FLAIRS'94), pp. 241{
244.
Zarndt, F. (1995). A comprehensive case study: An examination of connectionist and
machine learning algorithms. Master's thesis, Brigham Young University, Department
of Computer Science.

185

fi	
	fffi	
	

	!"$#%$&('*),+--/.01'2-43'657

89:;<%$=>.9?-/.6@A:
#&	=<+7?-6.

BDCFEHGJILK/ENMPOGJQSRUT*MWVXTNTYG[Z]\HCPG_^`TNabCLTcIdTYGeMFENMPO/\fGJI
EHGJghMPiJTYO/CkjliJEYCFEnm<MFTNCPO/IoMPO/mqp

\rgbTYK/I

sbtLuwvYxDydz"{}|LtFu

~WWWnW"} WW~	W~W< o

N	*"	9Fn",9
9%6	4rl		DW

D(d!
P	$$,NreJ$!	!6W$9	%	(	$
	6"<!P9J$<r$P1nPn$9	1c$N!	<f/	$r!
$			P	LN	$6dF$4%	$n$$H<P$c4	Pd$
1bf%	$"1P1H1$$	6(9$	$nrN$,
 1$($N1$N<9nP(,$P1	w$,$
6d<<!($9(14	r1	reF$n$	/
(1J$1r$$P$,$	Dr$b  	r1!	
r,o/!	1/F!b$n$U$r  	N$J$!	1/$U
 }6$1,$1n  	9}Le6,e<19(n9r9dP*<
!9$rL%	<H		9J<$$"
J$b,9b$	e$$}!$	D$	$*FP<}1<
<,$P9/H	1(1,	1r*>f%	P<,$e!	r,$r
	F<"dFP<	9W	P$,rNF/<
b*H}!$	b$1	9F$	!H!/,P$e$b$e  	9}
H6


F <*(! 
NWN/1141"l"W94	!Wff
N,	fi*,wP9WfW1!cfi

!"#$% c9&')(H !"*"fi+ ,
nWUcW,}-61.9/.1.e021
5b	

76o

6$26!,



ff18:962rX,"4/11;
n

34!%



<=WW,>?.9

,!1,U"@6o!16,"	AB=WCBH	.'ED62F !""#% BH"/!G'H9N1F !""ffI+ 8

.4	F.$,,"!%1W!Wff
N,	finW96,J6<"&		"4K!%
?6.<,L	 !6LWM	NO&PQPNRTSQKUVWRTUXNMYZ[S\]V8JN!^/,.(W!,
ff6oWW1!_("

.6T!n1},.	!,

6?8`"K!%a.$,;bW!6

1}}/",
N,WHb9"/6^bW!6c"W
GdW;W99	





ff!wJ/<!,	e6&

bW!6rKi!1J6!,6Wo&$,,"*34!%cF,
,W!%j
n!*(Y

!	fi.6
nW9WW1Wn!%

9WWff
N,	fiU1rW8gf

!,h9"/6

!^"ff"
N2,bWJ!,



"W6,<W!/41k9"/69ab !68

Wl9"/6abW!6m(Y!%P"1@6on4Wo
Yr	6op.6T!q8*NW*(,r
	*2b!	fi.641

21,s.$,J9!a!"=6fi.6,0t6"u"b	Jt"/!W%

2b,
n!,.$,!%Y	vwbrx(e/ABffWCx82 !""#fi+ 8py_WWd
r!6=!	1!0.;9"/69^b !6"u6o

21,

!16,"	,>	z
nWr

!%{6"	}/1,M*nWJ!Wff
n0BH"6!n'|9NF !""#fi+ 8k(Y.6!W	dW41

nWWW0

K}n9,4/

9/.$%h(Y

44

!%,9,L	"66<bW!6

 +--6.FfioWfi"$##	=	=aN9@dff;(@1:
#&	#x	fi

%&##T=ff
~

fip,

ff.g.?{fiu?--?fi2tL?,a;4fi343fi-L=.ff?2G3fi2&pJ
 Tu@5/@.ffKK???	fiKWL

 	3@j.ff.,=/4p?jff_2fiLK22fiF.Jfi2Lfffi2JJ..	3.fi?22

?3.K.fi2.fiG50fffiKJfit03?p022ff3
?3.KpffpJ?<.2?affFK_=fi2lfiafi/s.an?3./tK.fi
 3&3{:ff?x@F	ff3{x2@	fio

.=0.K3fifi?ff?2

ff.225/Ffffi?xFfiLkj3fio2pfi/:?JffKn32fiJLff.ff5/T
7?tff3.ffLK???fiKn2&-K32.=MKfi2.t2H;;fi3;fi
L=.ff?20fi2&  Tu.K	?fi&5=?2,L
&3??2ff2u;ff;K.5.KL0fffft23ff{Kff5/.4ffLff.?H	
?ffp<2G4K.fffi2t.ff.  u.ffLfiFfffi.JF,fiL3ppK3?..fi.K
.?JpK2.0=.>_.K?2.  .fi.3/c{ff.K?.fi.K;.?J
3.ffLK55?fiKff.fft?Kff?2F;Jtfi/ff2&0??2ffL=.ff?2
.?J.K5/.fi..fi.32ff.uM?3tKff?2.	.p?.2K3  ?x
 ff,F.=ff2FLffM=Fff.fffi:-2.J23?2F@u?2
:22Ffipff.^K2offfi^	3Mfi2ffKff5/.lffKff.?  fffi)-2.
fiLFfi2	fia.ff.pfifi@K.J3pfi2Mb20	ffLff.5fiM2Ffi?uffF
?fi2La5&<	fft.	2.ff?2K20fi?22.J?fi2Lb20ffLff.5p
.2G3Mff.fffi?2fi2o2.K  ;^fifi?T;&?3?K2jff^F;pffJ
fiJ.ffpMffJ?	5fi/KJ	K/fi2J2-?5/fffit?fi2La2h=Lff.?
3=.0J2G2fi2ufiffqc  0	-.ff.0?	|?fi2Lpff3	?fi,pfi3
?=/{5?2Gpfi3GpaG.02Gu=.t2G{=Lff.?{fiM2F
.TH7pF,7k
,^.-/fifi.K<fi2Mu2??2A;ffAK_ff<fi/t&?3J?.u
ff3?2{fi3LffKffL3L=2hgo2t2`o=hfi3p^2^0K.t
Kffkfiff.0p&ffJ2fiu-ff2Gfitu=2ffJ20K.?Lo/fiL

J



 -_  ^  a_L



u-0K2;ff.<2-p3M20.?0tfi3fip3KK2hK.?3L2.
   _Lff. 
    ^_Lffp-?fi.ff
ffa2fi2t&^2@K.fffi2 


?Lff.b2fi=?2FpJfiba3M2.?T<fifi?22M^fiff,@K.?L2fi

25?2FT	5.7fiffKfi?L3/<fiff5/ fififf,Ax4K.fffi20J
Jfit?Ku.ff  
L&ff.<KT  -Kk	b  ^K&JuJfi<qF

 uJfiuL@ff&g
?

2:3o2..ff.<-Kn32&-Tpp30K32.  o
 l2M2ffK

..3&ff=.G3K4K.32.fi3.ffp2u-p2fi2A&

u72.ff?2G=&3;^;3;3t/<fiff5/Kff?27fi;fiK32.;
 p

  `3;fiKff 
?;gfiKpff



eoff<	<fiK@ffl=GK2
	ff		oo/.=;K3.mL^0nfi3fi
 7
p 
 Jff2fi2;.?JfiKzff  kff.3,F&F-
fffiM=fi&Jff?Tfi@
 G-K_ff<fi/  .		fiK
fiff      oL_W_fffififfW_ffW_ffff
fiff?2fiK.7ff2ff

 p3{2fiLJfi	uoo/u/t&K3?..2ff2L,ff.J	Jfi





.	3? 00^ff3G -fiT.ff3K/_2Lk3{fi4=fiKuM





fi"!$#%&'()(+*,-(/.0
132..5476'&*8(96*,4;:<=0-'&>?(
@-ACBDAFE9GIH,J?E9K&A&L=A)M5NPO&QAFQRTSUJVHWSXQfiR9YACQZ[I@PAFEO]\$BVJ?ETSUJ]@7H^A_`LaJ?Q-bcR+YAFE9A)\dODE9AR+Y7SU_ffNE9O5e)Ab7ZE+ACfgShHiHjQO&R
@-AkA)l<e+SXAFQRm
n YApoDZA_qRqSXO&QrJ?ETSU_TA_cR9YAFE9A)\O&E9A&LgfYAFR9YAFEfiR+YAFE9AsSU_fiJt_T[uJHhH/_TZ@-_qAFRfiO?\v[wOb7A)H,_fifY7S,e+Yx_qRqShHhH
K&Z-J?E)J?QR+AFA_e)O&E9E9Ae)RyE9A_qZ7HXRz_yfYAFQ{Z-_qAbfifgSXR9YsR9YAv[|O5b7A)H}e9YAe+~DS^QKcNE9Oe)Ab7ZE9A&mC&Z-e+YJ|_qZ@-_qAFRySU_
eFJHhHXAbcR9YAC_qAFR}O?\gF&q&)XF8C&O?\J?Q-bSXRz_3A)MS,_qR+AFQ-e)ACY-J&_@PAFAFQfiNE9O?B&AbcCJ?ZR+AFRgJHmXL
&D Y-J?Ezb7O&QgO&R9YWL &? mQsO&ZEA)M-J?[wN7HXAvR9Y7S,__TAFRgS,_F
F-7 q r?&)?D?FF?DF&F&F&?&]
_qOsSXRISXQ-e+HXZ-b7A_fifiO&ZRIO?\R+YA & [wOb7A)H,_IO?\m
sOb7A)He9YAe+~&SXQKfffS^R+YtR9Y7S,__TAFRIS,_vKDZ-J?EzJ?Q7R9AFAb
R9OwNE+O5b7Z-e)Awe)O&E9E+Ae)RE9A_TZ7H^R)_g\O&EJ?Q7GfY7S,e+YS,_JuyO&E9QfiA)MNE9A_9_S^ODQWLWJ?Q-bfiZ-_SXQKJw_HiSXK&Y7RqHXG[|O&E9A
e)O&[wN7HhS,eFJ?R9AbtJVH^KDO&EqSXR9Y[ODQA<eFJ?QtJ?Q-_qfgAFEIe)O&E9E9Ae)RqHXGs\O&EvAFB&AFE+GpCJ?ZR9cAFRJHmXL D&7 m|QO&ZE
A)M-J?[wN7HXA&L7SXRgS,_gAJ&_qG|R9Owe9YAe+~uR+Y-J?R FI  S,_gAFBVJHXZ-J?R+AbfiR9O)R9E+ZR9Y-CO&QJVHiHWR9YAvJD_9_SXK&Q[wAFQ7Rz_SXQ
F-7   J]Q-bfiR9Y-J?R   S,_3\JVHU_Si-Abs@7G = m
n YAvZRqShHhSXRGsO?\R9YA_qAvE+AFNE9A_qAFQ7RzJ?RqSXO&Q-_FLPO&E+QA)MNE9A_9_SXO&Q-_CJ?Q-bse9Y-J]EzJ&e)R9AFEqS,_qRTSUe/[wOb7A)HU_`LS,_yQO&R
e)O&[wN-J]EzJ?@7HXA&m}}JDe9YwO]\R+YA_qAE+AFNE9A_qAFQ7RzJ?RqSXO&Q-_3Y-J&_S^R)_JDb7BVJ?Q7RzJ?K&A_O?B&AFE}R9YAyO&R9YAFEmSXEz_qRLDR9YAC_SXFA
O?\gR9YA_qAwE9AFNE+A_qAFQR)J?RqSXO&Q-_S,_S^Q-e)OD[wN-J?EzJ?@7HXA&m n YAFE9AuJ?E+A_TYO&E9RvyO&E9QA)MNE9A_9_SXO&Q-_\dODECfY7S,e9YR9YA
_qAFRgO?\$e+Y-J?EzJ&e)R9AFETSU_TRqS,e[wO5b7A)H,_S,_gO?\WA)MN-O&QAFQ7RqS,JH_SXFA&L-J]Q-bwBDS,e)ACB&AFEz_9JLDR9YAFE9ACJ?E9ACJH,_qOvA)MN-ODQAFQRqS,JH
_SXFA
yO&E9QA)MNE9A_9_SXO&Q-_<\dO&EcfY7S,e+YrR9YA_qAFRfiO?\we9Y-J?E)J&e)R9AFEqS,_qRqS,es[wOb7A)H,_cS,__q[cJHhHwCJ?ZR9sAFRJHmXL
&D7 m n YAuE9AFNE+A_qAFQR)J?RqSXO&Q-_vJVHU_TOpbDShWAFEkSXQ
R9YAc_qAFE9BDSUe)A_fY7S,e9YR9YAFG
_qZNNPO&E9RmsQO&QAcY-J?Q-bL
yO&E9QA)MNE9A_9_SXO&Q-_|J?E9Aw[wODE9Aue)O&[wNE+AFYAFQ-_SX@7H^A&mQR9YAwODR9YAFEvY-J?Q-be+Y-J?EzJ&e)R9AFETSU_TRqS,ew[wOb7A)H,_IJ?E9A
J&b7B?J?QR)J?K&AFO&Z-_SXQwR9Y-J?RjR9YAFGwJHhHXOVf\O&EjA)l<e+SXAFQRgJHXK&O&EqSXR9Y[c_\dO&EJ?@b7Z-e)RqSXO&QuJ?Q-bwb7A)\J?Z7HXRE9AJ&_TO&Q7SXQK-m
QfiR+Y7SU_ffN-J?N-AFEfgAvJ?E+AJ&_T~&SXQKwYO?fY-J?EzbSXRgS,_gR9OIR9EzJ?Q-_HUJ]R9A@PAFRqfAFAFQfiR+YA_qACE9AFNE9A_qAFQ7RzJ?RTS^ODQ-_FL_qOwJ&_
R9OwAFQ?qO?GuR+YAC@-AFQA)-Rz_yO?\@-O&R9YWm
- 8I5$U-?-j
QR9Y7S,_cN-J?NPAFEfifgA_qR9Z-b7GR9YAe)O&[wN7HXA)M7S^RqGO?\vR9YA{R9EzJ?Q-_HUJ]RqSXO&QNE9OD@7H^AF[c_{J?Q-b&qImO&E
R9YA_qAcNE9O&@7HXAF[u_`L}R9YAcO&ZR9NZRI[uJG{@-AfiA)MN-ODQAFQRqS,JHhHXGH,J?E9K&AFEIR9Y-J]Q
R9YAcSXQNZRm n YAFE9A)\dODE9A&LjSXRkSU_
J?NNE9ODNEqS,J?R9AR9OJ&_T~fyYAFR9YAFEgR9YAFE9ACJ?E9AJHXK&ODEqSXR9Y[u_fyY7SUe+YeFJ?QcN-AFEq\O&E9[R9YACJ?@-O?B&AR)J&_q~_jSXQfiRqSX[wA
fY7S,e+YS,_kN-O?HXG7QO&[IS,JHgSXQ
@PO&R9YR9YAISXQNZRu_SXFAfiJ?Q-b{R9YAcO&ZR9NZRI_SXFA&m n YA_qAfiJ?E9AceFJHhH^AbODZR9NZR
N-O]H^G7QO&[IS,JH}JHXK&O&ETS^R+Y[u_Fm
 A)\O&E9Au_TRzJ?E9RqSXQKfiO&ZE/SXQB&A_TRqSXKJ?RqSXO&QfgAwQODR9AwR9Y-J?RSXRIY-J&_@PAFAFQ_qYO?fQCJ?ZR9wAFRIJHmXL &&7
R9Y-J?RZ-_SXQKfiR9YAI_qAFRO]\e9Y-J?E)J&e)R9AFEqS,_qRqS,ek[wOb7A)H,_O&QAweFJ?QJ?Q-_TfAFECJ?@b7Z-e)RqSXO&Q
oDZAFEqSXA_E9A)H,J?R9AbsR9Ofi
SXQuNPO?HXGQOD[IS,JHaRqSX[wA&L7fY7ShHXAK?SXB&AFQwR9YAg\O&E9[IZ7H,JvSXRS,_gJ]EzbkR9OvNPAFEq\dODE9[J?@ab7Z-e)RTS^ODQ9&A)HX[uJ]Q
AFB&A_9oDZA&L &&7 m n Y7S,_YO?fAFBDAFEb7O7A_QO&RyS^[|N7H^G"R9Y-J?RCe)O&[wNZRqSXQKfiR9YAw_qAFRO?\ffe+Y-J?EzJDe)R9AFEqS,_qRqS,e
[wOb7A)H,_ySU_CJ?Ezbs_SXQ-e)AwR9YAIe)O&Q-_qR9E9Z-e)RTS^ODQSXQ{R9YAkNE9OO]\}G&SXA)H,b_vJcODE9Qfi\O&E9[IZ7H,JwfyYO_qA_TAFRO?\
e+Y-J?EzJ&e)R9AFETSU_TRqS,eC[wOb7A)H,_gS,_gO?\A)MN-O&QAFQ7RqS,JH}_SXFA&m
ZE|[uJSXQE9A_TZ7H^Rfi_9JG_vR+Y-J?RfiJ?Q-b&qJ?E9AfiAoDZ7SXBVJHXAFQ7RfiR9OAJ&e+YtO&R9YAFELgJ?Q-btJ?E+AJH,_qO
AoDZ7S^B?JHXAFQR|R9OfiR9YAue)O&E+E9A_qN-ODQ-bDS^QKb7Ae+S,_SXO&QtNE9OD@7H^AF[sm n YAuNE9OD@7H^AF[O?\C}Y-J?EzJDe)R9AFEqS,_qRqS,ewsOb7A)HU_
b7AFQ7RqShaeFJ?RqSXO&Q+s  L=S,_vR+YAwNE9O&@7HXAF[O?\ffb7Ae+S,bDSXQK-L3K?SXB&AFQ
JsO&E+QA)M5NE+A_9_SXO&Q
J?Q-b"J_qAFRCO?\
[wOb7A)H,_wLDfYAFR9YAFE  FP   mA_TYOVfR9Y-J?RyL&qIL5J]Q-bfisJ?E9AAo&Z7SXB?JHXAFQRZQ-b7AFE
N-O]H^G7QO&[IS,JHgE9Ab7Z-e)RqSXO&Q-_FmwJ?[wA)HXG&LR9YAwR9EzJ]Q-_H,J?RqSXO&Q{NE9O&@7HXAF[u_kJ?E9Aw_qO?HXBVJ]@7H^A|SXQN-O]H^G7QO&[IS,JHgRqSX[wA
Sh\J?Q-b{O&Q7HXG{Sh\R9YAub7Ae+S,_SXO&QtNE9OD@7H^AF[S,__TO?HXBVJ?@7HXA|S^QN-O]H^G7QO&[IS,JHgRqSX[wA&m n YA_qAuJ]E9AIQAFfE+A_qZ7HXRz_
fY7S,e+YY-JB&ASX[w[wAbDS,J?R9AIe)O&E9O?HhH,J?EqSXA_ffSXQR9YAIbJ?RzJ?@-J&_TAb7O&[cJSXQWm
"AR9YAFQw_qYO?f
Je+HXO_qAgE9A)H,J?RqSXO&Q-_TY7S^NI@PAFRfgAFAFQkR9YA_qA}NE+O&@7HXAF[u_$J]Q-bCR9YAgG7N-AFE9KDEzJ?NY n EzJ?Q-_TB&AFE9
_9JHWE9OD@7H^AF[ n   m=SXB&AFQfiJCY7GNPAFE9K&EzJ?NYfiJCR9EzJ]Q-_qB&AFEz_9JVH-O?\S^R)_Ab7K&A_S,_gJv_qAFR3O?\WQO5b7A_3fY7S,e+Y
&

fi
	ff
fifft		 !t	#"%$'&(	)*+,	-.
/0123#4	3
/

fi 94+-	 C	:
;F'<=+/+9>*
;2
>*.
?<@^4
'ffff
A
$B	C"%$'&D	2)*1+

EF*&G<9ff82H>*7I
 +J<Kff
q4 ?; 
EL'.M	ff0ENH;

)		ff
?<O4+-	
 '
4QP@R+-*ff
S9T%@*1-4+7	; 	U0)V); :2.	
;ff
XWY&QZ
X ff5=[ff\2]^_5`a	+7?; 	#	b1+7:+-*J.7
 
cGTd$'e fEWY&Q4X hgji%-kX*ff5d[>\]^_56E"X
		>R+9?; 
%W;klffH22m2
'Fn
 H*om5[ff\\pq_rLXms0:*a
;4*t4*.? ff; 9) bm
T; .%+-q4*.

u  	90ff
%*G	?
 )2
;w 	vWYeE	J*.Ggw&'>	5'[>\]x_5=y?*ff34+-*4R^; 5Q
2.
q ;)	
 ff 
;q
T +9
cWYP@X :g{znD9; *)5@[ff\\	[_4'TM4+-		
|9
q }<} 	ff
;/	2)*1+
Xm

	ff
;7 ff)CPrX
 -~zn&+; *)3W4[ff\\	_4s"%$'&(.
->*.
; ff8HH*17c {	t	)*+<l>*Z
ff?; v<'+-2	&	ty?a*ff4R		ff

|
5?L'.m
k
 	7<+wvL'.vLQ9	ff
bfi
 X/
 	
$Q.
?	)*+#52ff8ff
3
 4
|*.?; -)`F; Lc 	lQnE-il%E	ff
;4 ?; 
=0<+7	&	
<	); 

$B	h4+-*4RXY E<}
 	U"%$'&	)*+2
')`I
q 2ffE)`4<K-WYff+9Ggjk%15
[ff\\0	P@X
 lgjznD9; *)56[ff\\`qkl>H2.
XFl
 >*!5[ff\\2p_d.
%
q; J*J*St2s82	ff
q;  u }	
G*.

c<	2)*1+
%L'.I0#"n$X&N4+-*F
 fft2
%)I4	ffE
q 2ffWYPrX 
g(znD9; *)56[ff\\_O$Q.
'*.

?*1ff
%+a7	)*+9
?<+jH0;
'	*J.?; Eff
?L'.
/ff82H>*7
 "%$'&(W!	fi*	+/.>*ff); 
4_ u { 	/&	l{ 	h	)*+m

	)0)*1	&-%=Z;d+7*1F
 I&Qff47; *5'	ff+3k%a0W[ff\2\	_'>	ff
;7 ffv

;	)	Z4R		7; .>*d`=H3; +-:>*2;X
 	+<g 	:"%$'&	)*+#
 Q
q'
 
;	L ?} 	%	)*+'e{.
Q?? *ff
qQ 
?-
Q"%$'&l	y?I ?d LQ%+-ffc ?
J< 	7.
-
 	/
 0*1	+/.>*n>*2;X 	+<K'Qe  	
 	-.
-0I 	:
 0*1	+/.>*
>*b^
 	+j<K2n"n$X&l$Q.
X
Q)E
q ? fft
't`#	2)*1+)kl>H2.
XFff d>*!QW4[ff\\p_45
L'	}	ffG#
Y1+U*.7	ff

fiff
;*Xfi
 </fYi/=y?&G0	ff

fiff
;*X 
fiI)`H*^ ?; 4*
;ff)a#4+/)	9	
Xff
;*X
 
Bs	? 0)
;C 	#WYPrX ng(znD9; *)5[ff\2\ayS	Gg
!)02o5[ff\\p_d7X
 
X4*.?; s /	'	2)*1+
lWYk%0tFn
 >*!5[ff\\_4
 
 	t4
|.; LQ:4*mHR?b 12
d0<$ 	ff
; 
Y*m]; 9	)*+9
d$Q	X
q? .
Q4
|.;	
ff		79
 "X4Rq	ff

|
/L'.347 >2. 	"Xv	b1+7-+-*J.? ff
U<K2U}v4R	Z
	ff

Y12$B	h2
 	n 0<'fYi.
 	4<hH*^ ffs -)| 	h
;F%
 <>*J*d	;+-fi+-*J.? ff
5@

|+/J*.;*{
 	fi		:
 <''Qe*1ff
/>*J*} 	fi	;+-/+-*J.? ff
1
T ff}<B7+/+9>*
;	)
bFff 
?
 .
'
;	L'c ?? fi .
'
;`ff.>*r
;5`f;i/5Xe5at"%$'&%ff8>*%
 	'0*1	+/.>*
ff); 
A$B	4<K25-
 	UH*12;X 	+	ff
;7 ff})a#ff+9G}k%a0IW4[ff\\	_Q#)`

;ff| h
b*:Xe5tf;ic; +-l S2H ?$Q.
dff
;*XB
 09)`>*X ?; 4*9;ff7<+
 	ff
;*X
 
fi <K	)b 12>*ff
7ve#Tlk<K+W;P@X -gwz%&9; *)5Q[ff\2\	[_  #
;	L
	LQg
 ?' 	%	+-7Q 	>*Jff
 I	c*m0Q<!+/J*t<rZ!82
|r"Xt4R		ff

Y12

$B	:
;ff44*.>R?; m
ff
 	:	2)*1+{<S42+-	; 	H**j 	l	;+-l+-*J.7 
B<K%:012
"X4Rq	ff

|I$Q.
fi.
-}4*.>R?; <:Xe
|4t
|	
 	t	;+-+-*Jm0 
U	#
4+-	
 
 	4) ;.
q; . +-	4*m
Aw!7 ff
qb 1	0*15k 	v>*;X 	+<K2s"%$'&WY	ff+g
k%a5d[>\\	_0s)`9	
 ff u.
c	)*+#5ff
;*X; 	sE>*;X 	+wLQX "; +--4+-Z
*4RaX; #`=
'
 .
fi
;	L'5	LQff5P ?l
 )`&E4*.>R?; 
l	&l	4*EI
;*2	fi 	/	>*
bff
'<
'eEf;iN	v
 74Rq`	b mH*ra0
=s 	h
|fi<$ 	/4ff
;`2	9	ff
;7 ?; 

 
q; *56LQh4
Ym:74*.?
 ff}	2)*1+}5	&ffEP u n56LXm}.
l-+/	l+7q2JV?; E<
'e7fYi/$Q.
@	)*+(.
=
;	L'I
 l)n42Z!nSZ;d+-*F d$Bm
?
;ff
j n*J7B 
b+-
<g 	-2t4*^; }E2	{
 	/4R):
 42+-*4RaX; E<	l	)*+9
fiTMH0;.fi
 <} .
lff
;*Xff 5


'H*1ff#	ff0ff7
 	/	? )
b'*JX ? 	-Wz%&9; *)tg 1)	25=[ff\\q_
		 ff567.
'ff82ff

ffff

fi'		

O		a4.	c		I?			q			.

co-NP-Complete

EOC

unknown
CMI

HTR-Complete

SID

CCM

SID(all PIs)

HTR

CCM(all PIs)

sub-exponential
exact complexity unknown

	92d	79Qd-41;#Qffb14

 	ff;E;	-90;ff12	IQX	s

av0:	9Umt.4ff6

Q	fi	  3.%4!%=;d71'Q	:	21}?.n-bff.>d;:0S  nV07m
ff21>altY(E'QI`Q	:	21w%'.%-;`ffmH;:0}Q.XffHH1
}Yj

'	l	/ff;;.4;E0n	-X}4q	ff|E.%	ff;ff}t	-;

d>JO	;-c17.ff
B	-ff;l'	7cmca01ffXJH%tff4;vt4	ff/04;.;bmU-q4.
ff4;`ff';-%	4Q	`;ffhK9>J94	ffQ	%	9S#	ffb;Qff4;t
2.4;ff?	Q4.b12-;Q'JYU	'4ff;`2	hff.|	#Qff4b12-
2.4;ffB	n4m0;9:	n%'~	#SG'4;dff;4Jmbt	n0	ff?ff;ff	7	
4|.fi	/YQ-4mHb12%-ab12	ffGfiff4;t;	n'  .l4!nS!n06
}ff4;E-4ff%QE-;	-90


	fiff	
	



Q.dbff4;-ffQ	X|.'4;0fiaq4ff?;>	?ff;S'.
;ff1}	l`ff

 >!#"$>aQ	Q4-h	Bb%&('!)*)!)+,& 
/>JffG;.ff.-l|	-X/>
U	2ff}a0&132(346587!9;:=<=>+?@&BAB	2ff
	l	/%l?}	:|	7aC
& 
- 4>.'4	lfiHbm01&=D%Y? Hff}/aY1b12
J>EAO2@14@	a; &=D1?Y%	;?J1>E
A 
- m0;'.=l2.G;F 	4;J>.	/fi%
/.%.':42FY	4;t0.bffd	2d4-H?@&'BI &J*AKH?@&LMI &'NIO&P@
A .Q/%}/.
QE;Q.;ffA.-Mmlt42YF 	4; =J1>.O0stl%/.Umfi92.G;F 	4;
O9=	24-Q?@&'MK &J*AIR?@&LSK &'MKT&=P!=
A .'/l%E/.fiQt;Qh9MN
- %
/.%.Q'-JOU.b%17Qd?-a;?	%a|;XJ>!3
- /.%.Q7		




4|. dfffi	4; E

J>Jr	lJ>.%l	`ff'}0laY1b12nQ	-|:0'nv0sl%	ffba0;

U!V!U

fiWYXZ[\]^
_a`cbd+e!`gf(e!h,ig_kje,lkmb1i+ne#oqprHs(ecd.tuShvlawp`xe!`.woyzi+neOoprHsecd.tu%i+ecdvrT`_ko0i+neHd+ecfd+e!`gecoi3wig_kto1{}|~e
yecoti+e.smR0#@Ni+neH`fi_kceOtui+neO`grRw*lle!`xi.8~dvecfd+e!`gecoi3wig_ktoRuEtdCM{
 o~w`+`fi_korQecoqi.//!!`vwig_a`fie!`._uYN@Y{Hph+n~wo~w`v`fi_korecoiO_a`#w*la`gthcwllke!y
wrQtye,ltuM{%mc_krfl_e!`byecotive!y 1b1eRre!wo~ivnwiOecjecdvmrQtye,ltuC/_a`Hw*la`gt
wRrtye,l%tu1{Ond+tpntpi.i+neHfwfecd*bYneco~ot6h,touEp`fi_ktohcwo~wdx_`xebe#_ayecoig_u;mwRtqtle!wo
uEpoh,ig_ktoTR_ki+nivne`xeci%tu(_i,`rtye,la`cbowrQe,lmN*33{Ys`gecdvjei+nwii+neCh,tooe!h,ig_kjev_krfl_ke!`+
 .p`ge!ys(ecigececo~ttlke!wouEpoh,ig_kto`}_a`Ye!p_kjw*lkecoiOi+tQi+neh,tooe!h,ix_je`gps`geci8tdCe!pw*la38
p`ge!yu;tdY`gps`xeci3`Ytu%*  {%nwi_`b~ _uwoyRtolkmR_uz/B{
 i+ecd+r_a`#woa}G3(MtuwQu;poh,ix_toMb_u M{  i+ecd+r}_`#wH3GHa}G3(1tuYw
uEpoh,ig_kto6Mb_uS_`}woR_krfl_hcwoqi8tu0woyRi+neOh,to@poh,ig_ktotuSwomfd+tfecdC`gps`xeciYtui+ne8l_ki+ecd3wl`
_ko6_a`otiCwoR_rQfl_ahcwoi.tu%M{
 hvlwp`ge#_a`YwoG}a,EYtuwu;poh,ig_kto0Mb_u%~ ({  hvlawp`geO#_a`Yw3GHHa}G3Etuw
uEpoh,ig_ktoMb_u#_a`8woR_krfl_hcwi+eHtuS0woyRivney_a`Ggpoh,ig_kto6tu%womTfdvtfecdC`xps`geciCtui+ne8l_ki+ecd3wl`
_ko_a`YotiYwoR_krfl_ahcwi+eHtu%M{
fii_`e,ll1ot}oTi+nwi!bqw#rH_ko_krTw*l.8d+ecfd+e!`gecoi3wig_ktoRtu_a`YwHy_`;@poh,ig_ktoztu`gtreCtu1_ki3`
fdg_kre8_krfl_ahcwoi3`c{  r_o_krTwl%80d+ecfd+e!`gecoi3wix_toTtuR_a`wh,to@poh,ig_ktotuN`xtre8tu_i,`%fdx_rQe
_krfl_ahcwi+e!`c{
uQ_a`rtoti+toebivneco_kinw`w.po_ape.rH_ko_rRw*l.80d+ecfd+e!`gecoi3wix_to6
p`@_ow*lli+neCfdx_rQe
_krfl_ahcwoqi,`,3bwoywHpo_apeHrH_ko_krTw*l8~dvecfd+e!`gecoi3wig_kto
p`fi_koTw*ll1_ki3`}fdg_kre._krfl_ahcwi+e!`3{
 E*cGc!G011a
neH_aye!wztuYp`fi_kohvnwd3wh,i+ecdg_a`gig_ahQrtye,la`Hw`.wqotlke!yeRd+ecfd+e!`gecoi3wix_to~Yw`}_koqi+dvtyph,e!ysqm
.wpi+Reci!{6wl{3!3{%nwd3wh,i+ecdg_a`gix_hQrtye,la`Oecd+e`gi+py_ke!y_ko  H@8e!hvni+ecdMe!wdgl
b*
.w*jqjwy_aw`Reciw*l
{kbO!Y8nwd3ytoti+n1b.!=woypoyecd6wy_1ecd+ecoirTwo_uEe!`gi3wig_kto_ko
ywi3wsw`ge.i+nectdvm@ececdg_eciCwl{b1!(q0woo_lawTYw_nw=b!(Ctiviglkts_s_ko1b!=_ki+ecd
8ti+iglkts1b!b!3{1n_a`M`ge!h,ig_ktoHye,oe!`h+nwd,wh,i+ecdg_a`gig_ahMrtye,la`woy8i+ne,_kdMsw`fi_ahfd+tfecdvig_ke!`c{
=td%1,0!  beCye,oeCivneGE*+c3*Ea.tuBwoy#i+tOs(eCi+ne8w`+`fi_korecoiYH0* 
`gphvn6i+nwiM8_uSwoytolkmR_uS=MOwoy*MHE_
{e{bqivneOs_kig_a`ge.lkt_ahcw*l 
woytu%6woyz{,{
=td.w`geci.tuw`+`fi_korecoi3
` b 
 (	 ff
fiff,+ _`i+neTw`v`fi_korecoi.eHeci.sm_koi+ecd3`ge!h,ix_o
w*lli+neOw`+`fi_korecoi3`_k
o {M|~eO`+w*mTivnw
i ~_a`.g q (_u%i+necdve.e q_a`gi3`}6
 wo
y 
  `gphvn
i+nwiY 
    woy6!
 (" 
fiffff,v   3{Yi+necdv_a`g#
e _`#% $@g &' (E{
neO!( qgt)
u =!  byecoti+e!*
y (+-,.fi*' 
/ %3b*_a`ye,oe!yTw`%ivneC`grTw*llke!`giY`gecih,toqi,w*_ko_o
ivnwi_`8hvlktq`ge!ypoyecdY_koi+ecd3`ge!h,ig_kto1{
Nt_llkp`gi+d,wi+e0i+ne!`ge0ye,o_ix_to`6h,to`fi_ayecdzi+ne`gec1
i 0 $q*!!+c{nec2
o 0 _`
ot%o Ed+e!ypoywoqi!3b (	 ff
fiff,+" 0 !bwo4
y (+-,fi! 
" 0  q!!*+c+c=c!q{
Bec6
i 5s(e8
w 7}td+oze fd+e!`v`fi_kto1{.ne`geciCtu%i+n*
e 9,~; :qg!E*3cEaH ! Yt<
u 5/byecotive!y
necd+1
e (='>?
" 5Y_a`Hye,oe!y w`i+ne`xecituCrtye,l`#t@
u 5 i+nwiwd+eotiivne_koi+ecd3`ge!h,ig_ktotuti+necd
rtye,la`YtA
u 5/{Ytive.i+nwB
i (='>?
" 5_a`}ot%o Ed+e!ypoywoi!{%=td+rTwllkmb
 Mz45

(=>C
 "5

=4

(+-,fi*'
"5ED}CF

3

tdYe=wrflkeb'(='>?
+q!=!!3!3!1q*!!+!*q{
fii_`e,llotYoRi+nwii+neO`geciYturtye,l`tG
u 7}td+oe fd+e!`+`fi_kto`_a`8hvlktq`ge!yRpoyecd_koi+ecd3`ge!h,ig_kto1{
n_a`8d+e!`gplki8_a`8ypeivt0hc}_ko`gecm~,!q,bYntRfd+tje!yQ_ki8u;td.wRh,ecd+i3w*_kohvlaw`v`8tud3`giYtd3yecd`xec%o
i+ecoh,e!`c{  lu;d+e!H
y 7Ytdvo3!=h,to`fi_ayecd+e!yRw8rtdveecoecd3w*lhvlaw`+`Mtu1`gecoi+ecoh,e!`c{@BecrrTJ
w IYsK
m 7}td+o
LffM(N

fiOBP%Q%R

ST%U&Q%VCWW/XYP%R'W[Z%R%\^]`_%Z%Q%Zba?c%V&Q%XdWc%XYafegPh\'V&iW

jkfflm%knpo?qffrsYtvuxwyzyz%qx{%|}&{'}?t~wy	w}&'rsp(r&t	q&)qff/zCy/q(|Gr'oHqffr|	s'jkl&l&?n3{%|qfft	q(?yAr%}&yz%q(|{%|}?}'-}&|
yz%q{%|}&{}Ct~wy	w}&'rs<(rt	q&n}&|q(}&q(|pt~w'q8/z'r|ryq(|	wYt	y	wY#}%o?qsYt(r{%y/%|q#rssAyz%qJw?-}&|*ry	w}&
r}&%yByz%q/s}Ct	%|/q&'yz%q(1rsYt	}8(r{%y%|qKr.ssyz%qw?}&|/8ry	w}&4r}&%yByz%qB}|1qb{%|/qfftt~w}&

3?p'2f3p(*?'~-A?'33*b'(%?'C3@'/@&1(/p	(d&
dCg!(-ff j (C j 

nn

pdp(3p?p3#p''(%Y;ffYbd
z%q#}&%}y}&%qy/z%q(}&|4uBr&twCy/|}bo?'qffo?xt	z%}&%y	


jkffl&l&bn'r'ouBr&tsryq(|'tqffo}|6rHyz%q(}&|

-}&|#}bo?qs-'r&tqffo|qffr&t}&?w%j"z'r|o?}x}&yzkffl&lhn

z?wYt8tqffy	w}&qb{?s}&|/qfft#yz%q*|qsYry	w}&'t


'q(y	uxq(q(4y/z%q@#}&%}&y}%qyz%q(}|gr'o4/z'r|r&yq(|wty	wY#}bo?qsYt(

't'rsA{'r|y	wYrs}&|o?q(|

*<ff(#%1*&'&-8;1dC



kCH C&`  &
 .% k   4 '*&'# 
	fiff  
 ff'.% k  #< &'1'# !"#$&%-@ff	'd?
 &&Y-d&(
 ffC )  1J(&#dC*,+ &'*&H ,-&.01
/  

&@& &

2

Cy/?wywqs&hw

!

3ff yz8?wy wYtxyz%q@%}&|*rsp}&|o?q(|54w  k&
6 7h}&|Bq'r#{?sq  k  k 898:8:8B k & %r'o

yz%q(1y/z%q}&|o?q(|B|qsYry	w}&1}&1y/z%q

yz%qJ}&|o?q(|B|qsYryw}1wYtB|/q(&q(|t	qffor'o1uxq@z'r&qy/z'ry@k
 k  k
z%q

; q@%}u o?q='%q?>

/ 8:8:8:`
8  k&k  <
&&-&'H(.-%(Y4 @ .%

k 

 >

CED,- @?GF



&&-&'H(.-%(Y4 Huwy/z1|qfft	{'qffyy}  >

z%q

 Bj Hn
A

z%qt	q(y




uxwyz1|qfft	{qffyy/}

 Bj @?n
A



d?

&Kd'C1(HdC#&	&

(d



dC*--YJ.% kff?

}

w

 Bj Hn

A

8:8:8:8
A

S


H


LK

ff 1

d'M&&ff'd 3Huxwyz|qfft	{'qffy

7%}|Bqhr#{?sq

R q(y&H

CIDJ&- @

2

UT V

 j 

OT(V

nj 

@ D @  H



j  k  kn

8:8:8:8



y}

 >

,N"?N 	 O H  @ +
/ P G F

'(

2. k  k && k  k %&&  

jk&k & n



r'o

QF

2 k&k &% k &h k &%&& 

H
H n
8:8:898 jB

nxyz%q(wyz%qt	q(y8%}yry	w}&

. k  k & k  k &h&&% k&k % k & C

@  HGF

z%qt	q(y

w



 k&k &% k  kC)r'o

A

8:898:8 Bj Hn


2 k&k &% k  kC&r'o*yz%qKt	q(y

XW9W:W 8 jBHn 2. k  kC
Y sqffr|	s&}|Kq(q(|^r&tt"w%#q(Cy &% k  H[Z A  j\Hn}&|q(}&q(|3w ] / Hvyz%q( 0 /
H nxj"t~w'q  wYt yz%qt	8rssqfft	y@r&tt~w&%Hq(CyBuwy/z1|qfft	{'qffyy}#y/z%q@}&|o?q(|  n  z%q(|q}|q?>
 jB

w

A

H


A
^
`_?a cW b 89d:e

 Bj Hn f^ A
:_?g h

 Bj HnF

ji%qfft	y	w}&HwYt`wGuxq@(r=''o1rt	*rsstq(y<}%q(Cry	w&q@q'r#{?sqfft(%r'o*'tqwyxy}|q({%|/qfft	q(CyHr&t

z%q


r}.q&

k5l5l

fimnopqsrt

1000

1011

1001

1010

1100

0000

1101

1110

0001

0100

1111

0011

0101

0110

0010

0111

uwvyx?z{:|!}~?z\vyxO!vyX9BwLX&Bw
Oy5[I'X5MLc5syX?`v?{
E , `C 
\PcG5M? jM
yj&:c?'?wMGQX?JjO

 Bw9Lc?5L

`vyx:QvQ|XQvy\vy?s
|#x?|cLy:|c{:XLv|#{:|c{:|5\|cQL\vy?O?{
zX\vy?X
& 
`L

 BwPS
`L

  BQ
B}Q
  v s\y5
vGL,:XL:|\|c#L(ECL5
|vyx?GBLGJvOX?`v{3LQ?{:
uEzX\vy?#u?{|XLQy|?X`vQ|c{#:|{:|{:|5:`vy?( BcJ:BcU:BcJ
v:zX:\|5vy9|vQ:{:QzX\vy?

|5cP:XL
:|.9L\v`?vyx?9`vyx?|cQ
L {:|?~
& QBP[C???J??J?:L5:c?J:c?c5?Lc?Q?c55Qc??Q5L5L5L?L?
(|PX?|P:?Uz:|:||cs!vy

 B Q{!|c{:| [G???5??c?c?c5Q?:?LLG
?:|#:XLv #9L\v`X|5:|cv  BwP CQ X&  Bw U:Xvc
	J  \9ff	sPL
fi
|c{:|?{:|?Q!vy\G??LG?LXU!vy
c\PQ?5LQ|#.9z:|:|3|c
L
!vyQvO?:`vyx?|cQ3vG(Q{
vyx,:|?{:{9|5\X?Xvyx,L:\v|5'LX?\vyx,:|{:|L\vy?X
:|c{:|?Euwvyx?z{:|}\L':|OL:\v|&
vy:${9|5\X|59],fJ???
 fi
|9L\v`?vyxI?9`vyx?|cQ
L L{:|!L{:|5Ovy(X ?|?
 fi
|!!vyQvO?:`vyx?U|cG#{:|jzXQ|c{\vy|5sLX\|jL
:|
?{Q|c{P{9|\vy?XcQQv:O\L$:XLP:|{9|5\PLfi9|?:`vyx?U|cGP{:|?P!vyQvyL{:|Q{
 fiw
?z9|#  \
|#X?|:?&G&?:`vyx?U|cGQv9&vLXL?|#9|#!vyQvO:`vyx?|cQc
fi
QvOvO{:?|5G:|IQ?9:|5Evy|5,Qv: C 9XL?LX??IL{9|vy  B 
`vyx$:|&Xx?z{9|
|,?X\|c{:?|,:XLv
  B &G???J5?:?J G
 fi
|,?:|c{&\|cOL{:|
!vy  B [G???5?5Gc5G?LXO!vy  B PG???55?c5Q5QQ
`vGL&:X:|j`v c|'L:|X?`v?{'zX\vy?vX?zXQ|5G9|j`v c|#Lvyu
{:|c{:|5|cG\vy?fiX9XL
?{|c|c{:#:|`v c|!L!vys:\v?zXQ|5Q&:|`v c|!LPvy #u
{:|c{:|5|cG\vy?
uJ?{LQzX\vy?LX,\|cLP?:`vyx?|cQ y|c5~

!  !vy  Bw#
 " `C C!!vy  Bw:G
$%&

fi')(+*+,.-/+01*+243356(+,7398+,+:<;>=+8+*+8@?BA+21*+5C3A+56?EDF(:721GH3

IKJ+LNMOHPQPOSRUTWV+XZYJ+L[O1\L[]^XHT_1L`NaHVbacPYL[\V7aHYTW_dL RUacefYOhgBLi7V+Lkj[lm4ndoffp.qr
sutvBw
xcv7y{z}|~t
7xHww7Ht 4+ZK^Z 1<
[6d
.4c j[l7mBndoffp.q)

  
 O1V4YTWVBBTV+X<YJ+La7OH_1LL7aH]fBPLRKTYJYJ+LM+V7YTO1V
o[j
qoffj[
cqoff[j
mq
RKLbO1V7P7gBLbYJ7aYj[lm4ndoffqfS1++Sc[1+H[+[1[1+[1[+1[dSdH4r `uYJ+LZMOHPQPOSRUTWV+X
YJ+L[O1\L[]`J+OHR`>YJ+Lk`L[Y)OHMJ7aH\a1YL[\T6`YT6 ]fO+gBLP6`[aV7L7`LgYOhaV7`RKL[\UgBLgB7YTWOdVd+L[\TL`[r
sutvBw
xcv7y|~E
[vB7.47
~t
7xHw
fwHt4+< pfcfpf
[
[C1C4c pf  pf  1h1Qf [116 j[l7mBndoffpfqpfo  q 

 1

YuT6`f7`LMBP YOJ7ac_1LYJ+L \L[+\L`L[VBYaHYTO1VOHMkaM+V7YTO1VrM9T`XHT_1L[VTWVTY`Z 
\L[+\L`L[V4YaYTO1VYJ+L[VTYuT6`uLa1`eYOO1]f++YLYJ+LF`L[YhoffqMO1\faHVBe<Hr<>a1JYL[\]TWVYJ+L
 N\L[+\L`L[VBYaHYTWOdV<[aHVO1V4Y\T++YL d k OdV+LFa1``TX1V+]fL[VBY]uTV7oqR)J+L[\LZYJ+LZ_SaH\TaBPWL`
YJ7aHYKaH+7La\TVhYJ+LYL[\]aH\LUi+@LgZaHV7guYJ+LNO1YJ+L[\`aH\LN`L[YYOYJ+LT\K]uTVBTW]ZacP_SacP+L1rIKJBT6`T6`>Y\+L
`TV7LM\Od]L[_1L[\eO1YJ+L[\k`aHYT`ffMedTV+Xad``TX1V+]fL[VBYOHM)YJ+LYL[\]RKLh[aVR)acPgBOSRVYJ+LPaYYT6L
YOHR)aH\g+`KYJBT` ad``TX1V+]fL[VBYO1Vaf7aHYJOd]f7O4`LgbOM`aHYT6`Me1TV+Xba1``TX1V+]fL[VBY`[r +O1\LaH]BPWL1YJ+L
]uTVBT]hacPad``TX1V+]fL[VBYMO1\uYJ+LYL[\]f   + RKTYJ\L`LYfYOYJ+LZ7a1`T6`uLPL[]fL[VBYhd+11
T6`u]uTV7oqu41+4rIUJ+LFa1``TX1V+]fL[VBYFd1 R)JBT6JaSP`O`aHYT6`i7L`uNT6`uV+O1Yu]uTVBT]hacP `TV7L
1  d+  11r +\YJ+L[\>O1V7LRKLJ7ac_1LO1V+Lba1``TX1V+]fL[VBYuM\O1]La1JYL[\]bTY
T6`La1`e]haH1L`+\LYJ7aYhYJ+L`L[YfT6`EV+O1
V \LgB+V7g+aHVBYbBeJ+LdTV+XR)JBT6JOHMkYJ+La1``TX1V+]fL[VBY`
X1L[V+L[\aYLgT6`>TVYJ+LNTV4YL[\`LYTO1VbOMYJ+L O1YJ+L[\`[
r L RKO1BP6gZ7`L YJBT6`)acPX1O1\TWYJ+] P6aHYL[\>TV`O1]LOHM
O1+\)\LgB7YTO1V7`[r
.Lk`aceYJ7aHYNauM+V7YTO1V T6`  ]fO1V+O1YOdV+L TQMTYNT`N]O1V+O1YO1V+Lad[O1\gdTV+XYOZYJ+LO1\gBL[\\LP6aHYTWOdV
g ff
)  YJ+L[V<fio q 1rZ)O1YTLZYJ7aHYNTQMRKLf\L[V7aH]fL
 [rZaH]LPWe1TQMR)J+L[V+L[_dL[\ko  q  haHV7	
YJ+Lf_HaH\T6aHBPL 
  4e TY`V+L[X4aHYTWOdVMO1\ La1J )`7JYJ7aHYk   EoTr L1rfRJ+L[\LhYJ+LfO1\gBL[\ \LP6aHYTWOdV
T6`Z\L[_1L[\`Lgq)YJ+L[VLO1]fL`Z]fOdV+O1YO1V+L1rIKJ+L[\LMO1\L1) ]O1V+O1YO1V+LEM+V7YTWOdV7`ZL[
V ffOHe`ffTW]T P6aH\
+\O1L[\YTL`[r+Od\La]fBPL1HYJ+L[eJ7ac_1L+VBT6d+LK]uTVBTW]ZacP NaHV7g  Z\L[+\L`L[VBYaHYTO1V7`[rV+OdYJ+L[\
+\O1L[\YeuT6`KYJ7aHYKYJ+L ]TWVBT]haSPa1``TX1V+]L[V4Y)RJBTJbO1\\L`7O1V7g+`UYOuL[_1L[\eZYL[\] T6`>TV7gBL[Lg7aH\YKOHM
YJ+L`L[Y]uTV
offqr

 y

  u17d17>4C1  C4u  1 
 |~t7xHw fwHt)B71 17 
#
[@11C4
Ck6
Q 1C  1hCBucu6 oq "!1uc $#
% '&[1 (cu  6C4u66u1*
 )+!,
[[1C1Nd  C4H[.-d7uuC oq C
u6 offq 
%0/ &  u6 offqd2 14365offq 

.L ROdBPg acP6`OZ7`LYJ+LV+O1YTO1V OHMaPLa1`Y)++L[\ 7O1+V7g OHMa87>OBOHPLaHV M +V7YTO1V.o:91LP]haHV	;
< aH+Y:1= ?>$>+Hq1R)JBT6J[aHV`O1]fL[YT]fL`U7LkJ7aH\adYL[\T@=[LgBehYJ+L ]fO1V+O1YOdV+L Y J+L[O1\e1r
A vCBD?w|FEv4G4HJIKIv+xL:Mw,N FO PQHb	Q)1Q14C1+SR)T(c
 JNU
 u[1fC41W
V  O  OX  H[Y[ d@uC4   117Qf  6Z V1C4u
[  O \ bC41 
Z [D] 
V 
^?_?`

fiacbdefhgi

jkhlnmpolqsrutFvwkpxoyWmDz|{~}8mk,\rDK:$8c$F$\? $6$8J.F
$ hc?$$n? $*YT '@c4Dn?K ? $F8
 \XfiFX$?:$n+Y

 2 ? X
0 

' W

*$,:8'C:,"$:6'?:0@$?C8c":'?::@?$$h
 fi:8C$:$:C:$J$fi:?$F:$
jkhlnmpolqtFlCklo{,lnxoYnpKvxhplnxX\pDvwkpxoyWmDz{}mkY\rW
D? ?s 'c$F$nTCpW\?J 2D?    W:C n   2D? 
Y
 $,*nJ$0Tn:@$              
	  	ff	   fi
*
@$  : ?+K'$:0@$8nFcC $c$c$$C"$?
??  *?$?$   $$$': C:  2?   
p

j kplmWq pxmDzhxo?m Klq!

finT ?@$6@'@n?8nn@@$ $''$n@4CT:: ?6@	:nT?"$#4
%&  :?:fiCff "$:P$:6@4\  #  @F 'F$:  T8nTfi
:F0T@$J:$n@4,$0Tn:?6'$
 ('*) % $@ % FC:TT$+ nT

,8"$: %-& #ff
 :? :,n  #  

.0/ 1)32 ::::4,n\ 5p@$  % $8  c$'76:?:0@$ 
,8 c$'0@$8nF8Y
 :?X8cC: %& 9#''8   \  #  

: j}7) "n:$;fiWFCF:T < .=?  C>+ $C:$$6:?:fiC 
,$$:$ %& ff'?:0@$;?
 :?X8$$'$ < & P:?:0@$1@ 
 : :?BAC@ 
fin?'T0@$6:$n@4,CT::?h

(' / ) % FC:TT4+ nTD,finnEp@$
,8"$: %-& #ffJ8GF 'T0$@J$:0@$8nFH#ff
 :?HIY?EJn  # > F
& $:$Kfi+$C@@$T L$n@nc:M#ON    F  $ ?:nTJT:(F  n  #  
PRQ S) 6 \\, % @\ :
,8"$: %-& #ffTF  $'0@$8nF
 :?HIY?$n@U#ON    F  
V	TC':,:,@ \c % +W,
 ' / S) % $:TT4+ nD,finnEp@$6@: % $\'c*8n@
(
,8"$: %-& #ffTF  :T0 C@ C:0@$8nFK#
 :?
,4?n  # X F :ff$:
IY?  :'T$$: & n@ $n:
8n@(YWZ7?\  # [ F8
\]^

fi_`babcedgfbhiabjlkknmo`bckRpbcbqsrutbpbabpwvyxbjiabmzk{xbmov}|~`qjik

EG K 09K
7g94 0bK000
4UlibT-b{Myb$ii{bUnnybiMogffnS{-{$nb
ib{bb>ffb{Go{TnSb4ffn(${7Myyi0Gn{i7>{youo{bG
iyS3{}T{iibTGG{bT--3{blDynoE71iyy
w5iMoibi{b7
b{b4b{bb>b{iyo-byob;yb $n{T7i{bo
ib{bb4lbMoffi{bXEuMi4*i{n71Dyn1oGybiMou
{b$
gffbbbXib{bbUffyffo-{bMi4-{7G41
b{bWib{bb(b{yobibyobyb T-1ioy7{bb{{
nMo{yib{bbTKUbbb4 KM{{T;iinboTb{bbybiMo
iinbiT5uXTi{{1$DynoGlbig1{b(M3ffbbb
S{b4$ggeTbinT{UiDi{bGib{bbuo-bi-byoib7yb
{b{
obbybMMySg{bnliiDi{bi
eUyblbMownyi0ffn${gnT4inDi0-K-0bbG{ynyEEo
{4{(ibi0 yyn b{iy-oHw5iMoizE$ilEU{$Rb{iyUX53{b{
o7ib{bb7lbMoi{b{7i7D1biBi{}{b{bb
ybiMo4bb{ibb}iub yX ybiMoTib  i9nbS{bbyb4{b
bb{bbi


	fiff	
ff	fi!"ff#$
%'&(")*+ff
{yoUiGb9{K{b-b{iySffyiMWiWn{-yyby
ybiMo3{yi ,Tibin{ob{iySG{4yRWybiMoK
EiyE>nb(i{nibynoi1b{iy -7HoTy47lbMo>i
M y/.10#2 ulE43565798;:<>=8?579<@8i57A<B5si$DCyFEiEgyHwiEiMziHG
iyzi*3JI
 M
$ "KDL -bb{yboKyEobMS43S3noyU-b9{UWnN
WOMiPMWnT7n$-7MQMW{GbnRMBybinSnoGybiMoE
{yny{U TW
 V SX V Sb* V S
 Y" V S
 V S
 ZM{ii

[ C]\^0_2 DlE157A<B5C`3-biEiMzioEiyE$ 2 DlE1579<aI
b i{-b{ybM{bb{y0ioy-bC45o-{yuoU{yiK{bT{yE5
b{iy {X{~Bibi03yw,Tib ynoi1b{byb{4iTnoyEE1

{in{T7i{b*{-b{oibS{ibyu5Ub4Dclo-ibbl
dDegfihkjJl@mfimkl!npo>hkl@qsrJtDhFuvtDrJw_x_jHuvyjxfilzqBt{rBt|hfi}BwkljJl@mkl~omkl6kl@r}J-l@mohFuvt|rxfiuvhkj6t>nvrBt|upongqBlnpoorJql?r}B-l?mF
o>hFuvtDrxfiuvhkjuvrJy@mkl@{l@rhonzt>nvrBt|uponqJl!npo@;uvhkl@m6it|hkhFnvtD
idDa!e4#jJl@wkl6mkl@}uvmklhkjoh-hkjJlOonv|tDmFuvhkjJ
xfiunngy@tD-J}JhklhkjJllnvl@-l@rhkwt>uvhkwt|}BhkJ}BhztDrJl6oh~o-hFuv-lDUorJqsmkl@wkhkmFuvy@hhkjJlhFuv-lqJl!npo@4l@hFxgl@l@rsy@t|rBwkl@y?}BhFuv|l
t|}BhkJ}Bhkw@egFrJy@mkl@-l?rhon*tnvrJtDupo@n"qBl!npo@onnvt>x_w#hkjJlqJl!npo@hktqJl@l@rJqstDrOhkjBlzBmkt|Hnvl@'wFuv@lo>rBqOtDr6hkjBlr}J
l?m#t>*l!nvl?-l@rhkwiy?tD-J}Bhkl?qwkt;o>m@e"tnvrJtDupongqBl!npo@6uvw#wkhkmFuvy?hkl@mfiuvrhkjohfiuvh_mkl@}Huvmkl?wiqJl@l@rJqBl?rBy@ltDrnvt|rhkjJl
JmktDnvl@'wFuv@lDe_"tDhkjt>*hkjJl@wklrJtDhFuvtDrJwo>mkliwkhkmFuvy@hkl?m#hkjorst|}BhkJ}Jh#tnvrJtDupo@n_o@nvDtDmFuvhkjJ-wzwFuvrBy@lhkjJlinpohkhkl@m_6o@
x_o@uvhonvtDrJhFuv-ll!`t|mkl-y@t|-B}JhFuvrJuvhkw#mkwkhtD}JhkB}Jh@e_rHtDmkhk}Jro>hkl!nvD*-t|wkhit>fit|}Bmzmkl@qB}Jy@hFuvt|rBwDuvl!nvqt|}BhkJ}Bh
t>nvrBt|upon#onvDt|mFuvhkjB{w@UorJqsxfily>orJrJtDhi|}o>morhkl@lhkjoh_hkjBlwkhkmkt|rB|l@mirJtDhFuvt|rBwzjBt>nvqe
He"_johuvw@oitnvrJtDupo@nuvr6hkjJliquv-l@rJwFuvtDr4tXhkjBlzBmkt|Hnvl@`hkjJlir}Jl@m_tU|omFupo>Hnvl@w!
hkjJl#uvrJJ}BhzwFuv@lDorJqhkjJl
t|}BhkJ}BhzwFuv@lDe
JJ

fi6***fi*
-"B
-k
>;6J"s"_-U*W"fi`_;i>@;!g
("@Js>*-J;!`
*>XDJ*
>"">"*J46*D
@;>""s>@;!`*4
>!`
*Uik
i`Ji"z-6;@*"@`>*>
">
`D;`*k
>;s-`>W'JH*4k
>;ss;>@;!g
"
`#"
O@;!k
`*>!`
*i`6;>*iN*#W->*>*6J;!`
*>XDJ*6

  ~>*4"@--;-J->*-s*>"O"
6@;!k
`*$
!`
*-`6;>]  N*
ki>*O"@--4>*!`"D4;->@;!g
`"@"B
O4@;!k
`*
>!`
*U6;
  
]D`>*
-k
*"]@*"@@`>*@`

s  6;W
"DUJ6>*D;@>"D{
>?`*R
!`
*Ji6{s"B
s>>JD*|4-`>9>;-*>*DJ*>4
A

D`;pk4`>

**U@6-"B
>*-DX*J>!`
$?#{B>? D
6;s;z@;!g`
_""4>@;!k
`*s
>!`
*U_-@*"@`>*>{N>-
z  @B>? D|
">*A>*J;!`
W*>*DJ*>
6*"@--;6J_">*>Dk
>s-D
U*-`>
  W*DXs@*"@*>>
s >  W4**J;!`
(*>XDJ*UB`__"
>*"@-;s"*>D>{D
"`"$"s-4"@4@*"@*>$`"@J
]{*
-;-`D*   W>*AD
@`U*>"">"66"@-
>?*J]"B`>

>*--
""*4>@;!k
`*
>!`
*UON*

(4-
;9`
>"@>*>>@;A*>
4-
Jfi;`
W
s$s*D"
-4**
6@k
>>;O*"@@`>*@`
9*>XDJ*
J
!`U
>>*D`J>H"
->*s*U@*O
@
>`*$!`;4"
*

 Bfi `|@
"@
->"--"J{s@`
ik
	9D
-*>Dk
>g>*O"@-_;Ji--"B

4
**>
`Afi-W!``W"@-sJs*>
` @;@J`W*@6*>>*"@-
"i>;-
@

 	9i;"*UR>B`*"**ff
 	9fi "{*;D
*>D

 
>$p`
-4

 |
***s
 	9 N"]
@`>*
 >$@`

 	9D*W>*
 


 D`***J"" >`J6J->`$J6>*-;@s-O*'>"->*OD`|
A
 A "  ( {Ws*J>""@"XD~">>*>`s
->*s
***6
 	9fi O

"!;i"|@>*-J
?|
@s"""`*
  " !#"%$'&%(?* )B>*O>!`
*;,
 +.-0/#13244
7
k
6>*
D
 65 N !#"%$'&38(?D
9 JBz>*s@*"@@`>*@`
@;>k>
"

*"
"@
O"
-  s!`;O@*"@; :
@`
]-
>*  
<
  -!``]"@k
> >*@*"@@`>*W>
D**>J>?
 W= _"k
s
-O
U?
 >,996;>
 >HiN>4
->*4 = iW4;B`P>"->*>;6-`>*J>k

|-`>]  p"
`p*>;O-`>*J>k
 @
=  
= -;-k`O-k>
>*D6>"
 W !#"%$'&38(?"]  "
`A
A
 W !#"%$B&%( = |_@4"J`4A
 W !#"%$'&38(?D
@">"-*iC )pE
 D3 F(B%$'(J Ffi GDU"
 HIAGW>*
 HJiX">*D

 A !#"%$'&38(J = |

 W !#"%$'&38( = {>*]A
  !#"%$B&%(?|--*>Dk
>
{*>s;-`>*JO9-`>]   
>*9{s]>JD6>;
D-$*J@U*
 -`>K
 L= M = 
`***
6;`-p>BR*|
*-
J*
O-
k
s**6!A>;s
9DU`]>*`U*>>' N
`>J
`J
@s
*D`O
 G P
 H   ***s-4"B
*k
>@`
]
]>*4>*sD`UD
-*>Dk
>-A*"@k
> >*>JD*|!`
 `>*
6*>A@*"@*@`
'4  1
>' N`>Jfi
(`|D*
s>;s*
` "!`*A*4k`-`*`>|*>*DJ*>
]J
 @|
4-
>>>*>]
*
>@;`>]
  
4B	
 D>-
>*DX@`J>D@`J 
4
@*"@*@`*R  
k>*4"{"
@
Q
 DU-49*>*DJ>*s*DXO|

B R@"@

-D>k`
*@`*$>J|-k
6>*6*JS
 D|>*"@--;6JOBS
 D*>*9{s*
TVUXWBY'Z#[#\']^Z#_`Z#]*a[#\']cbedf`g#]^h<ij`Z#_k`*lB]Mbfimfil
h']^Zonfp]	YBgonfl'qff[#\']c_	`l'`[#`l'][#\']^`Z#rQs0tu\%mfiZ#hB`*l,vxwy`[#\Ca;zfi{{fi|}JhB`]^g
l'`[~g#]^]^_k[#`%].Y'g#]ijYdjU
''

fi;;L~;C;0;;;xc;;;;C;ofi;0C%

fi%
#C,'Cfi0@;C,fi'jCA8Bj;e%'ufi;;jfi;'fi
jfiQKCc^
fi;
Cjfij,jfi;'fi0fffi;<C;,CfijCj;fiB~fi;Q%%^0%j'<fifi;fifi;Q;'fi
A8;B%'fi^jC''fi;Q%38Q^;^^jfi;^jCVXCcfi;,C;fi%jQ%c	fi
 fiCfi;<C^fi^jC,jc0	j'%fi	~#8B%%^,*B;;fi;fi'jfi	
0<fiQxk3LJI,8fi<fi0Q^fiC;C<Cj^jCx;;VC;;^;C'u%
jL^xfiQk3'%B'fiM*;kj'%^jk 
;C,^;C
Cfi;0^ #%'3%%*?y@IJfi;kjOe3B%''fiM~ *3
 %jC.Q;
Kfi;BQCc;fi0
0ACfi*C0^jCJjjkfi;0C^,fi;kfi;
BjCC;#Cff	*jCCj'fi%;fi<8%'ffC03Q^fiCj^^
?Bj*Bjj;jfi;'fi
L;@fi;,;fi%j;fi;;^jC@ ,fiBfi
;;'^'
jAfi;
fifi;^jCJ'Bfi%3 8C;'*;'C'C;%'''%'C'%'CC%C%C;%
^;;3^Q%^I ffC; 8C';'BC;C' C'%;C'8CCB~%0yB;;fffi;
; ;B;CJLfi;	~C@*%y;j<'c;C.jjByfi;%*Cfi^0; 
0<%^*  ce*fi
 fi;;fi%I
Q? 	
 ^%*fiQ^;^^jfi;
C;ff#C'C8^0%j;^^jfi;fi'fiC
cC0fi;^;^jfi;^jCE 0,0;ff
  
   fi ;*%fi
 6

 
'BC;B'';''C%Cy;fi^fi?% Afi%
fi;8%^0%jA,CQ^;^jfi;fi'yAfi;

 %j ;<C;fffi%fffi;fi
0QC;fi<%jefi;CjC; CB;
0j
	C  
  
 
 ;;ff;Cjx#%'% <
 *X;fifi<jQ%^*  '%^j;,C;
BjCC;jfiCu#CfiCM8 Jfi;#Cfi%^jC,#fiC fi;Cfi;^;^^jfi;fi%.fi
;C0fi'jB%C;Q;<;<^fiJ
fi;<;;<^fiQ^;^jfi;fi?fi'C%M3*%*3cMC0
 *^;^;^;^jC#C3
0
Cj0;
  
  ^, %
  
 8'BC;*B';*'C%
 ;;j;@	fi0<Bj,,C
fi;<%^
 .'V  j  
 E#%'% <
 McC0;%fi;<^;^;^jC#C.0cCj0;
 
   ~%
  
  'BCCB'CB%
 ;;j;	JCcBjc	C.fi;%	  ^j
CBCff0ffC;fi%j%*.Qfi;fi#Cfi<';*^<Cfi08Bj;C
fi;A;*%^jCfi ^;^^jfi;^jC%O#C%Cj0 fi;%^
 .'<%
fi;fi#Cfi<Q^;^jfi;fiQ,fiICfiCfi;
 8C;fi%j
E;'C
 j;kfi0%jAj0'^fik^Afi%;Pj;8;fi;;j;^jA%,fi;
fi'^jC jjfi,3^j;kfi;%fifi^j;;fi'I#C0fi''fi;%^
 .'
;'^A%fifi;;'QBCfi@Cfi;8Bj;xfi;y EIBxEfi% fi;
8^0%j'C;ffC;C%Ifi';*<C?;
 *	%Cj0;; %	  .0fi'jfi'Cj
AB0fffi	*

!#"%$'&)(*,+.-)/102(43ff56879;:<*=->02?A@->39CBD0C02?E+.(GF>H)I49J/K02(MLN-LO&)(*P+Q-/102(R3S7TVU
WVX4YPY[ZG\

 %fi
%jC%C;fi;;ff%j;C,0BM^jBjC^jfi;  #C
CQ],y%I;C,0B

;ACAj*fffi;;j;A^jfi%V~;C,0Bcjxfi;,;,Q	8^0%j'uyfi;;;;
;^CJ%fi;QC;;;; ^%*
 _jC* Cj;;;fffi	*;Q; CA;^~j<^fiCff%
C;fi;;<8
 `yC<;^.j<';j*ffI3C;@e'^'<fifi;; ^%	L*ff;V
 ^
C^ff ;'a ;fi; E ` e0<C;ffj%j;C,0BJ^j,Q ]ff%j;E
 bc_<B j'
R d>ef;fiM%A%^<CC*j;%jCffeAfi;,^'CAC^
	g
 ;% fi%;Qfi'B
 hCfi;;fi'fi
0Cfi'^C;fi 00%fiC%K%fi;fi#Cfi<Q%^  

iRj>k

film=n=o=pPq=r
sut=vxw=yaz)z{[zf{[|t=v<}=vG~|;v1EyGR'z>}gw=yv1DzD[yvR.;|;}GzDEw==||.;z>}[A);vRy};}=g|t=v1z>y>
}|tC{Nyv1xzDy{J=}[G|.;z>}A=4R A=RfECEtC=v1}{Nyz>#;vRy}=v1y|t[|Et[>|z
yv1w=yz=[Gv;|)8D1GvR;}=Gv1ya|4;}GzDy>a;vR=cv1Mv1y.t;wD=v1y842;z	|t=v;vRy}=v1y|z>.
{Nz>y|t=v4;=vVz{|t=v{N=}[G|.;z>}8z>}8Gv1y|4;}8wz;})|4
Dx';R'M4E4G)RRG>.>4>[4NC>gAR   =RDM>4[>NG
QD!>.>R2O)>)R,.11R[NGEOOARR#Q4N)G=	Q
 }vRD;A4;v1}[Gv>=v1ya42;z	|t=v;vRya}=v1y|z![}[z>=|t=v1|t=v1y8t)wz>|t=vRCt=vt[>
vRD4;v1})||zz>y}=z>|R}1>.vg;|}=z>|vR>;AAv1}|R|t=vg;vRy}=v1yCV=w=w;vR<;|tGz>=}|v1y
vG~[Ew;v>
Dx';R'GRC4>2RRRG>.>R21>81[RNO>=R   A=4>V>4[>NG
Q;>8DQDR2VO)D)4,.11RNOO)a>O)K8A=RfRVA=Rf>'Q4N)G=[K
 	#gO)41MO.4N)G=D[>[N41>,2V18O)DQP  P.P
!v[.vyvR.;||at[|t[><v1v1}8z>=|G4;}=vR;}8|tCu{JyGEv1<z>yP
 ,4[#'N	
fiff)V1	')4.;<>>>>GCO O>')4!DC4R
>GK<NQx>Q,)4.!ug)O>>4#">GK,.K1CD='=CNC >2[DMO>
OOE[E4.VR>GO)121VD[OOV1% $.	xD[>) P)OE&
 ">G1a,.1GO>(
 ' )O1;
GRC4>2R[NE
))a=>O)11POO)> >>GOO)*
 ) >G1G11	NQ;VDDRV
 ">G1PQK1O>,
s<t=v J{ z2;z<;}=;v1E=>}[|t=vxQC|.;z>}g;}g;|'w=yaz)z{yv;Ew2Ca;|;}w=yv1DzD[xzDy ,+vRt|v1y
-/. vRfy.,fi00123=|54v1|4;'6008723D;}=v1} -/9 }=}2C='600;:=
=ff(>?'),.)124A@B5C<D>C>O2E.D)4JKgNO),.)124ED	FGBfiDx
<

V4PHJIfi!vEyv;>v1}=K>;}=w==|V|zML>,+P}[w[zf}=z>C4|vADz>y.;|t=ON{Nz>yQP 9 5P	
R =y4;>z>y.;|t=
u2Vy=}|at=v4;>z>y|at=S {Jyaz> s<t=v1z>yav1UT}[}[.<v1yE|t=v}[
D=v1y.;vR|t[fV|  w=yavR.v1})|G1
W ;>v1}ARR{Nz>y |t=v4;>z>y.;|t=|vR.|t=v1|t=v1yEY
 X[Z]\_^fi`acbd Ks<tC1}!v
z>}=vV|avR.|.;}=Vt=v1|at=v1yxgCvRD[4[|z|at=v<;})|v1yG.vRG|.;z>}Ez{42[vG;v1Ev1}|f e;g
} K [tE|at[f| e#h
W ;>v1}Mj
 iz>y}gvG~w=yvRa;z>}Eg{JzDy |t=v<|t=v1z>yv1>[yf})|v1vR|t[||t=v<t)w[z>|at=vRCk iz>y}
vG~=w=yvR;z>}PD<vxt[4>v<|z|avR.|t=v1|t=v1yl
 X[Z]\c^6`a_bf, KGP!v<[y.||avR.|t=v1|t=v1fy X[Z]\c^6`acb, Kn m
tCat!CgvR>;4;v1})||G
z X[Z]\c^6`a_bf, K!
 o  '8s<t=v1zDyv1p
 7|z>Dv1|t=v1yV<;|ts<t=v1z>yv1p
 q;Ew;|t[|
2{|t=v}[.<v1yQ
 rz'|t=v1}{Nz>yV.zDEvs
 K'PQ  =(
 L>[atf}CGz>=})|av1yVvG~[Ew;vE{Nz>y
|t=vvRD;A4;v1}[Gv!>=v1ya>}[|t=v|vR.|1}vw[v1y.{Nz>yvR;Ew;)v1A4;[|}=zD}42	|t=v
>QD}=Ev1})|G;t
} KE
 u{ X[Z]\c^6`acbf, Ku o  '}[EvG;842|t=vM>a;>}=Ev1}|<;*
} K a|.C{N[<vVw=yavR.v1})|'J Kc>u}=w==|
|zg|t=v	4;>z>y|at=v
 N{Nz>y|t=vw=yz>;v1w
 P 9 5 P)s<t=v<;}=w==||a#
z P 9 5 PC;v1)A x N 4E}[.<v1xy yvR1
EvR};}==
 X1 zaDQPj
 m/K'tCat;Ew2vRs
 m{X[Z]\c^6`a_bf, KG}|tCV1>.vxv}[.<v1jy yvRg|z|t=v
vRD4;v1}[GvD=v1y> R |t=v1yu|
v N4=} rz8}[8.=w=w2;vRVGz>=}|v1yvG~[Ew;vE=
 X1 za>.P ~}KE
L;}[G
v K{m<v>v1|<8 ~X[Z]\_^fi`acbd K f}[|t=v1yvG{Nz>yv<v	1}Ew[>az>}DVGzD=})|v1yvG~fEw;v
|zE|at=vvR>;AAv1}[Gv>=v1y>


66

fi}Jx5c

=jJJ8[V5j[O}
	s=d[fiE	Gk_8#dJfi!]k,[6E	
fiJ=u;5fc[c85c8fi_8d5{]	u6ccfi8
x5d#5c[Qcfi(5j[}cccdc6V5kj[c
55j[c[=GJ/5MuM5==d	jd58VcM
jjd	JJ[6kj&#5=566Jj5(5[#c|Q=5|d[d
J#;5E;  ccu[[J8cd6J8[du(c/5;!d[c



cJ5[%dj#J}c	Ec5[}cG/c[5J85jcd[ 

wd[fiE}	_8#dJfi!]k,[6EG6u

	s=

fiJ
}[sU]#ucfid5?8gG
5J5jJ[#[c
d=J

fi

	ff


[c(65[6c685

c55#
#65[Jcd#5uM5=#MY5j[	k[lc8J5[ddd
#J}5&u5j# }J}#fi5}5;J5[dd%x#JuM= cd[ 	6[




"



 *8J6juM5 % cfixk#[5[=[!c J5[#[fif;fi
fi  5[ V85d
}}5[[s55[Q5c J
 5c|%_ 
5 856([5

$

 



!

#

%'&(*)+-,.+	/103241576"2598;:<=+3>?@>0A<)B(C>0A6DFEG+	>HDI0A/JD
5d6Jd=#6cd5#5Jc85M5#[5;5Jcd[fin[G(=Jd
dc 5c}k5[gj;5!c6djcG8
j=[J8[j,#J%_c8}c

KCL=M
N  OYc&}uLdJ%_c8|J8[56c[56d[cd'K5J565,8[
}(6fi	5d##%[;56[
L !d6J5JficdtJ8[VJ8#dg&
#[cJ	d6#; cJ5[%d#J%x
P 5#c56d}J%_c8	[!dQ _#dS]R J8[dfi
fig[	Vdc}c[[65c5Q5Jficd}#8}J(G5j8[[fin[

56dkcJ[dfi	cG[c	5J56d8(5[6d[Jcd[

GTVUWYX[Z&]\_^a`cbd7e9b[gfWhZ
L Q5[iKCL=MYkJ6*}J#(OjCk5[56d[c8k#kJ8

c[*lCkY]5GjQ6dM5G[55cQ5kj8
$ _fi[#5GJ#=MlCk
5[56[;dM&5J8=c[**jOjCk G=#[c=#=Jc
5;m con [cd&5=[&G58[
]5
5G[[Y=t[cY5[5J]5Gd#
5ck5k]jKLpM kc[=J5[OjCkj(kSk
q 5[jJ	d[5ccd%
5cQ5[c*5#ccdc6#6Q!c5#58[Mr
L [JVg5[;##[[5
_;dc[cdMc#J85[Jj[[5sS
t 5 sSt 8u[f5$ __[}58[Ac
5;cdg[du[[M]Jdk}c5Q#585*56d6JVg5J[V5Jc8gu=vx
w
=g[_ Vd*]Jd(]zdy ]86{
L QdJ}J;dz]R #85]JdQfi
lCk 5[56d[c8Qs}lfiVccdc6(c5[c56%#d6fi[d(ch
K 5
J565[|V
K fi8(6dd[5ccd[5Jjdu&5[5[g]cc/fi#d}#&68%;5J
]5A5Jd[c[(5J56d8[J[d[[G|g_J[#[J}cd#& zR
[c6Jd6Y	fi}
 L ; c*56dk[[sc56*Qc=[=5[
~ fic8[6	fi}F 
j




fi=SSSAS
AffF	}7p@#SI3HFV*CAffp'
 FA	A3!FzHzSVjSSxffz=l-A[		cIHzSV
SSxffzC=j7[*FrS	!|SzSSS|c!ffzVSHS{lCG!	.
=!fflxSSCzp!VSShjC-Scz@
S=	J#ff	!	JzS_#IplpS!9S!SSxSjrHz	ff
!		!jxSh!xCh{S	|!lxSYx	!HSff*FS.ff*Cz
SSxjSx	!xCSff#Sffff{Sz  S!	{HSShxShj
xVIzS{FzzSS	[SS	ffHz !	
SSxjSh@  
 	 ff
 
fi      cz[Szffjc=ff-!
zffzFx	.S*ffhx#cz
 *  	
 1S SSxhxS !ffzVSHS
jC- ]SxJz!
 #"$%&(' *
,
$
.
+

/
2
0

1
S

=

ff






z

=



S



S

S
 )
SxS!xSFzxScz!x'Fzj=ffx.	!!ffx	!=S
SS!H{! xShj;VS=! x	!Hj]ff#Cx
3 _HlxSjzhffz ff*	5 4ff	JzS_#Iff	!=S=	Jffcff*!x
	!S7 6!'Sj	ff#CSJz	hVI8 !9 
:<;	=|, >7? 
H@
 
 z], >7? 
2AB
 z Cx, >7? 

!	
 
 E DF
HH
 G<I9
7h Sff{xS !3J4=ff	z@3SS	j	!C	K4=ffz*SL4I!!ffz=MJ!!	
	!SCzSSSpB  N"  O" Shx!JVzSxJzSff=!QPRP(P2MPRPS(5PSSPSPJP2SPS!
!	-SS THffx9jCB]SxJz   N" !SrFzHzShSSx{S!
x	!#Sff#|	ffHC
 SPS(PSSP2KP(P25P(PffJ]H	cjlVU		.	!QWYXZG x C
 PSS P
S P ( MPSK P[ASx=ffx H3	S'*S PS !!	YSx=ffH3	S'\
  j!	z	ffj=SjzffjC- ]SxJz@
] JzShxS=SxSSSc!. 4|JzffzSff!AV'SV!zIS|S=ffx
	=Ixff{	SJ 4*Y
 ^jK 4_4!Hp'F1S `(`Raff
 AffF	cbBdMegf_hih2fj9klfmj[nNfoqpsrut_bbvxw{7F*@ #']H!3hFVff

Fyz|{
 SH	!j3.Sxj![3ffff	-.ffzffS.ffCzS!c	S
;
,} x!~Cz@|S`(`J	jK`(`S(Q_zS~1	!c1S`R`(aff!		JzSS!3J4.HS
STHff7 4!Fz	z{SHff*zrSc!	jxS, ^C	!ff{CF1z@S `(`Rff
 JkV-F9okf2nm
CS4HzS SFzSS.	!ffSx*ffzzS.S4z	{SS1z@j-	!SITSz#
=SSC=c	{(  !x'HC=jSffffSz{zCS 	ffz z!
S=SxffzH#x	AS!	Hz	H!ffzlHff#YzffzhSSV ]3S#I7IHzSV
SSffzffz=Sj	xhff	H	H@3hVS	A!lSI-S	!Az'S!S7zC	xF
_[ SI|S! 4H	!Sj!zSz{SFzSffF*S  ]3S#IGz
zSjHSF 
ff#ffz=	=Ixff3SFzSShS!c	jffFz., } x
 ~xz@AS `(`S!
=Sz	CcVS	z	Flff		ff	xz!*	z 3 ^H |H#x	AY
 =ShcS
ff!S! 4!	pSF. 6xjS!S	cc	jz	cff!S!S 'xS1!z
!8 ffq T	HJ p{ SSJz	Cff U	S{3!h
 p@SSf2nSkx

S

fi2292(2_|222u222[2(252g(J

Z#J5l[J5K#5225R[7<(g5|2	J5(SE(525S|Z.(s(
s2.(	.5S(J8Z.2!Y<KOE(5	225S5Z.(q|[l(\52\25([.(JF(2[M
5F(F2272YK952\(|x27*N.F[J5SLJ9|2L2.(g[_S[2RE
K(5\(F225E528J9|JM(5lF2[uJ.(|2uJ9K2.FE.F[J5SEMJ52
5J*K9225S5Z.(
Q([Q52.J.2YJF[.\5F.5M5\52\2(.(7	5[N22qS.(8(2
Z[Y52s2.( # K5, K| L [MB25ZR.2(5Z.(2*_M*J J5
 J(((2((2R52(252JS2RSJ2(([_uJ52|JM(|F[YJ|![R,
(R2(M(S2(((([_2J(	5Q  E [5  M9J5JN52SFJ|F52
([.(2R..(K(|Y2.FN.F[J5SLJ8
(FEx5[FS.(( 2N2|2Q.\S(Z.\5	(F225<52Y5J(5*F2[l
M5.2N.5F52 R[.K.[2|S5Z.( #,V K5, K5  [5 2 5   M 2((52
R2S.(2|2L7ELS(Z.L5R2522|2Q2J.\J52L5J
 N\\.[.K2S
2SFJ|Q5KJ.(EJ85225([.NZ.(J	K.((.52 s(N(BNK.JSYF(|Q.F\5
(F22|8.M R2522S(J* lOJ.(\F(5O.[(|J.(FJF(|7*8s(i.M(F22M.(
 	
 E(5 25S|Z.(52Z.252	F(2(|(2Y52(5|25S[MJ.(ff 
fiRJ
.(	ff 5
 [2J#5J

 ff i 
ff [
SK5J!#"$&%

2S'(.([ff$ *),+.-H_J[2(|[0/2143u65,785,+9i52
(5,7R2F_.5:9 1 5<;5J<K(52[.M5<(2JF[ />=3 52Q(5Z.(2*_N '  Z
(2JF[.\ /2=3 R(((2JY /4?@3 H(((
 fiA
52uJ |JS5J85uKZS*_F />143 (J.C
 B 1 [2(5|28J2RJ(JK
1
CDL	225S5Z.(u.5|JSs5(E
 A 1 9ZR2J*[7(.5\s(28JJJ[.SJMJMS|JS
.FA ? <S*SRQJMJM\(2_J.(L.5MKNJY<(V_Z..(.5MKqJ  N 8
 %:
G A

?
22N,  fi
 %:A ?   Kfi
 %:A ? 
2(5I
 H\.F[.SL5J#
  KJMLON ff i RK5\52N.S(22L(2Jfi
 g.F
 B 1P
 DLJF.(
52E._5S.(K9|lSu.:
 A 1 [5J|.F[.S_6
  8L2!
 Q8Rff  795\N|2J
2.F7*[J|SLJ
 .5Y|SSL5 />143 82(|K.T
 S
Q8RffU7

#V%:A
1

WF J	XZY\[]G ZY^

_LZ.2F5[E2(MJ7RYNRS


l

KJ4L`N ff i

a
 
j
d

f
c
e
b
/Mgih 143

kH2

J.2F(m||2QJF[.2NK(ZS
Q8R,M[
Q8R,SJ
Q8R,[
Q8R,[
Q8R,@H2

 O [|  [5   
 O [|  [
,V K5,K5   [ | N 
 O [| N 
552(

DE(5\552*J5..(	J52F2.F\.F[J5SJn	2(QRK,7[S\ZJ5[JSi52
J[.sF(2(|(227*<7*[J|Sq2s( 9 ?  9joLp0R 9 ' L.\F2.FY.F[J5J
qrs

fitnuwvwxwy\zw{

|d}w}~|d84p~Z~|8ffn~i!fij|d4Z&ffP@~U>j8@j|P~|d~UZ}^~@w~<wjUM
4^m6j024KwZ@Zw~8wjU4ZjPfij|d4Zkw@~U>8jnwd@~UZV}j^~<@w~m|dj~nd
@w~PwjU4Zw|djmw~nZwZmZw~P@w~Z@<@~U>jw@!~UZ}^~nmw~@j||ZU@~~U
 w~~<Zj~@d|d4Zj4~|Z@@w~8d>4dP4w6@w~@~:
\T0j^^VT^i20n.!^K^]KdT460d:K^.<
Kn\2KUZUfi42dZZK>U0Z>j


|0>

&\K

*


 w~Zm~.j~|d|d^MVZwZ@Zw~wjU4Z

0\jU 4Zj~@~6@j|!@w~@~jU4Z&4

nmj|Z8|ww~6#Z@.@~}w@~~|d4Z<|dV~UM.@w~6|0>4~!|dj.@w~<|d@~m~}w@~~|d
4Zj8|d@~<@w~@|~84Fm8|Z~Z  4}>4~@j|d#@w~!}w@M~m~6|04j|nj|dU|Z n 4
@8}j~m|0fi|Z~Z
 ZP@w~!@w~4@~U4ZP~8jn}j|dm44Z@w~4w}ww#4^@V@w~~Un8jjU|dj@w~.j~
|}w@wU~w@~Z n 4~@@|dj|dm~~|@~@|  @~}w@~~^|4Z  w~FjffMw
 |04P~@|j|d@~@w~  ~Uw}w@~@4Z@6mw~6~dfi446|0n|Z@4ZwV~^i  w
 ~U@jm|0
}jM<@j|d<C~j|0Z~  m~}w@~~|d4ZjZ@w~wjU4ZjK4ff U|d@w~@j|d
 #4}>4~!mj|d~|@]@~@

4@w~~ 

@~}wm~~^U|d4Zj#8@~}w@~~@~||d]~U4~~84

ff

|j]@w~@~UZ@~@w~m~jU4Z]}d4^wZ| 
	~6|0:~!Z~dP@w~~UM~V~^*4

ff

Z@~@j|ZjU~Zwww|dP^.4~iwn@2>}d4^w|0k>

<Zw~Uj|d}4~ZdP~Z~@w~24dP4w 
ff

8jfip




8jfip0d





8jfip#

-

4>|d4.C~~P

~Uw}w@~@4Zj|dj@w~U4@U|dj|d4Z84@*|@4Zw~







8jfipU


8jfipmw

@@w~

ff







'

wZZ









!wZwZ"





$%

ZZ" & 



()*
+,

ZZ

ZZ"



\T0jj

^8\ff>/.10328C^0K^ZOTn0^K&4 0
| >n
2dZZK>U0Z>j  *


0\jU

Z	TKKn\2KUZUfi4

 w~}w@d<4>|d!@mw~}wm^dPdCmw~}w@~Mj@w~Zm~:

 w~j|w~@8d>4d

 w~Z@~6

5w
 Zn@w~<Z@w~4@~U4Z|Z@w~!P~!~*|Z4w}ww!|6~7|dj.|dF|04Z4@w98

@Z

Z #

	~PjUfi}j|dm44Z:4^@~'

|ZUZ4w@!M46|24P4@@~}~Ufi@024ff#Z@~P@j|d@w~


~P|dm~nwZn<;ffM>
w~6P~8j~  |04n@@U|djZ@,~|Zm=
4@|!  ~Uw}w@~@4ZZ



@w~wjU4Z.
Zn~|Zm:jmF
~U}w@~m4Z]C~mw:@w~<}w@U~wm?
~ 8 @6U}ww@~
 4> U
4    @~}w@~~^|4ZA@	Zj|4Z]ff@w~84@~~UMjP4@m~}j~U8@@d@w~~   
~Uw}w@~@4Zjn@w~!#Z@~Uw}w@~@ffM:P~w~~\
]@w~!~Uj|d}4~ZP~4}4(|d@nMmFmw~~7
|dj.j~<@w~@|d~<~Zj|dMj|Z|jdZ~


Zd4w4.@w~!Zmw~4@~UM
@ZE@w~!|jdZ~Pmw~Z@~6PP~Z~P@w~24dP4wUZ@d>|d@Z
B

ff

\CDCFEjG
K^ 0

E ^T^i20<n
ZU

|0>
U0Z>j  *

Z4H.10328!^K  ZUJI@TiFTj4d
TKpK#T>UZiZZ>djZKZK2&U0Z>jKZ14 ZwZ>djZKZ>

K"L"L



fiMONQPQRTSVUQWPQXZYY\[DNQRY,]QRQ^`_'aQ]QPQ]cbdQXPQ[<Y3dQ[Dbfe=Ng^XhiY

jlkQmnm"o!pqsrituvmwxmzyi{}|O|~twqvw*3kqD1m"x\qDtu,xt!mktmmwym3r!m"m{y3mfqvw
3kQm}Qti&ttm}y:tqvwH%mqvZ!q~tiwQwquFtgVQqsriti3m}xy}pQwqDxtiqvyww{t!x)3kqD%usm"p3y3kQm
3m"puv&fyi{3kqDnm"xqvyw

)mwZqvywQm"tiyirmtqvquFt\m"puv{y\muFtqvywtu?Qti&ttm"qF

3m1y33m"H*qv3m:tw7y!3uvy&"Q%kQm3m3kQmn\m"3qDxqvywqFxtuusm"3kQm~{y3{y
{pQwxqvywtum1mwmwx\qsmi
AF
"icJ"<i"F

:jkQm1yiuvZwQyqFtulm"o!pqvrtuvmwxm3y3kQm}Q\yuvmjO)1qv}uqvm"3kQm

mqF3mwxmyi{7pQQmQywQmwqDtu71>>\)tuvy!qv3kQ{y\kQm"m}Q3yuvm:OkqDx\k:tnktrm:y}m
Q&txqFxtuOqv}uqFxtqvywOyimrm"t\kQm{yiuuvyiqvwQzmti}uvmkQyi7ywQmnxtwQwQy:tiQuvTqs3y
yiuvrm3kQm'mwQm&tuxtmyi{\kQmAQ3yuvmOqvmw3mqvwtiwqv3&"!QQ3m"mwAy}m{pQwxqvyw
qv3kqvw3m3m"qswQQ3y1m3qvm":jlkQm"m{pQwxqvywxtiwm:tiwqvQpuDti3m"T3ynx3m"ti3mmti}uvm"qv3k
3kQm,{yiuuvyiqvwQnQ3y!m3qvm"ilkt7tkQy\%y3wmcQ3m"\qvyw

c!!qD:tuu
3kQm

wpQ?1m7y{Oy!3w`Qqv}m,qsuqDxti\m"3qDmc1ywQmwqDtu

wti3qFxpuDti

 '

1'n""J

kt\kQm"mAQ3y!m3qvm"jkQmm

g)



i



n""Jng



yi{QQqv}mqsuqDxti\m"qswx\uvpm7tuuQ3kQmA!qD<pQwxqvyw'"

OkQm3m Q
 "Z
TmkQyxt!m:tiwtuvQqD)3kt)3kQm}m)yi{lx\kti&t!x3mqDqDx}yQmuD,qF):tuu
Om3r!m}3kti
qvwnymO3y:3tqD{V1tiuvm"tOy!wQmyi{V\kQm?QrtqDtiuvm"OpO1m?t\qvwQm"nQQtiw\ktiq{Q
3kQmw"p7tuDym?t!3qvwQm"ng
|'ywqDm&\kQm:mqsw






 

=%yqDxm3kti,q{3{y!?ym

V





 



tiw`tuu3kQm

y3kQmritiqDtiuvm":t3m=m?\y`\kQmw*HqD}3tiqFm"`jkqD}xyw3qvQpQ3m"mtxuv
3yqvw

 

&




gy

tiwrtiqFtusmy&mqvwQn


	

 

 

ff	

t\qvwQ}mw&

     3kqD,qvmuDQ)3kQm}t\qvwQ}mw&

QQ1"Q"gQ"Q

fi

|'ywqDmwQmcqvw
q{Q

&ti}muv\kQm:tqFmuvm}mwqsw%kqFx\k*Q,

Qj

yz3tiqF{z

f3kQmw"pmQVtiwHt,m{y3m}mxtiw`mtuuy3kQmrtiqFtusm"3y`{%g

3kQmwz\kQm3m}p)mtiwQy3kQmrtiqDtiuvm

fi 

jkQm3m{y3m)qsw

3



qvw

fi 

tuvxywqFmqvw

 




OkqDx3kqDm3yng
wz3kqDxtm}





ptuDymQ

&

&'7timus3kQmtqDmuvm}mwOqvwOkqDx3kVQOm3r!m)3kti7



qD7twZqv}y!wQy3ywQm%qvw37timus1iqvrmwntiw3tiqD{qvwQt3qvwQmwZOlqs\kn"

!QZ

QqvQqvwQn"

3y}mm)tiwQy3kQm)3tiqF{!qvwQnt3qvwQ}mw"OkqDx\kqD):tuuvm)3ktiw3kQmyqsqswtutxxy!&!qvwQn3y

 fi   

AjlkQm3m{y!3mQm):t:t3pQ}m)3kti7Q{Q3kQmwnmxtiwnmtuuy\kQmOrtiqFtusm"

3yn!{gV





)3kQmwn\kQm3mpO1m?tiwQy!3kQmOrtqDtiuvm



 

 7gAjkqDOt3qs!wQ}mwZlqFqv&7tOt{3y!

"

qvw3kqDxtm3yZy1


wy!pQAmti}uvm)qv3k

QQtiwzQ"Q



%uv3ym\kQm7m!m
}mw&'{3y



$#

ff021 340

:1c

<

Q"F

% &('

)

+*

,

gqDwQy7p

=x\qvmwZ%{y!7yiuvr!qswQ

kquvmmxywxmwZ\&ti3m%qvw:3kqDtmywOy\w:mQQ3m"3qvywJZmwQy3m3kti

65

3kQm3ti}mti\pQ}mw&tiwQ3yyi{
kQyiuDqvw3kQm}y!3m)mwQm&tuxtm)yi{

7

jkQm"mzti\mnmQQ3m"3qvywqvw

|

{y3


opt!q%y3wnmQQ3m"3qsy!w

&= 6? @'A5

mqvkZJ 

*

85
"9!:<;
C*EDGH F ;

OkQm3mqvwmrm3x\uDtipm3kQm3mti3mt:}y

uqv3m&tuDyH3ktiOy\w*mQQ3m"3qvywti\m`
o!ptq%y3wmcQ\m"3qvyw&
Qi 

t\qvwQ}mw&

Qm%m3kQm7t3qs!wQ}mwZ7Q"Q!

\kQm)&y}3ypQtiw

iwQmt!3qvwQ

AjkqD%}m"tiw\kti7ti3qv3ti3mwZpQ}mtiqvyw

\kQmuFt!7tiwf3kQm3m{y3m:1Z!

yi{3kQm)Qqv}m,qsuqDxti\m"Q{y7tiqvrmwm%yi{V}yQmuD

-/.

!

twqs%qDqvwqv:tu
zm)m

6Qtiw:\t"

t3qvwQ}mw&l{\y

7OkqDx\kqD7m73y}Q1tiw3kQm\m{y\mtuDy

B5

17qDttqD'{y

jkQmzm


o!ptqV%y3w}mQQ3m"3qsy!wtiw

IKJKL

yZqsqsr!m

>=





xtiwnm3rmt

fiM!N
O
P
QER
S
TU
VWXVYT[Z]\<^U2_]`a_4^aTVY`XbcWXTXbc^7d$Z
eVafcWg\hZ4`!ikjXl7U2_]`meZ4n"oqp[Z4TUGrEsKtut]v
wmx<y[U
V7z4VYn
VY`m_fffb{YVKeA|4VY`mWbZ4n2WgZ]\
}!}B~ _]n2e%4X@r
!U
VYnA`VKWXT`Xbc^aTVKeCTZ$U
Z]fce_ffff
`bdVbd$fbc^Y_]TVKW_]`VWTXbf fVK4b|>_>fVYnTTZ7y!px
(G
K
KE]h8ffcE2K
 Wd$VYnTXbZ4n
VKe_]2Z]|4V4rz]b|4VYn_A+`VY
`VKWXVYnTa_]TXbZ4n\Z4`i[V^Y_nVK_4W&b f^aZud$

TV$TU
VCWXVYTZ]\
^U2_]`m_4^aTVY`bWTXbc^d$Z
eVafcWYx!!n
Vd@bz4UT+TU
VY`Va\Z4`VT`CTZ$WXZ]f|4V }}B~ 2`mWXT!T`m_]n2Wfc_]TXbn
zCTU
VZ4`n
Va

`VKWWbZ4nAbnTZ8_C+Va

`VKWW&bZun_n2e"TU
VYn%^aZ4d$

TXbn
z"TU
V@^U2_]`a_4^aTVY`XbcWXTXbc^d$ZeVafcW\h`Z4dTUbcW
WXVYTKx  n
Z4TU
VY`!ZWWbfV`Vaf_>_]TbZun$bcW[TZ2`mWXT!^aZ4d$

TV_ffffTU
V+
`Xbd$V7bd$fbc^Y_]nTmWZ]\TU
V\
n2^aTbZun
_]n2eTU
VYnCTZVa
T`m_4^aT[_7%`VY
`VKWVYnTm_TXbZ4n$\h`Z4dbTKxkV^aZun2WbceVY`BTUbW
`Z4fVYd(U
VY`V4x67_]d$Vaf4r
[V^aZun2WbceVY`!TU
V
`Z4fVYdZ\VYn
dVY`m_]TXbn
z"44TU
V
`Xbd$V7bd$fbc^Y_]nTmWZ]\_@!Zu`nVa

`VKWW&bZunGr2_]n2e
bTmW!_]
fbc^Y_]TXbZ4nA\hZ4`!TU
VWZ]f
TXbZ4nZ]\ }!}B~ x
UbfVk[V%U2_K|uVn
ZuTC\Z4
n2e_z4VYn
VY`m_fff`VKe2^aTXbZ4n\h`Z4dTUbW
`Z4fVYdTZ7y!pr7_Wbd$fV
_4e
_]
TbZunZ]\7TU
V8_>fzuZ4`XbTU
d\hZ4`$7y!pj&
`VKedC_]nol7U2_4^Ub_]nGr!sKtut]v
w!ubVafce
WC_]nbn2^a`VYd$VYnTa_fff
<X] _>fzuZ4`XbTU
d\Z4`TUbcW!
`ZufVYdAx+Z>[VY|4VY`ffr2_4W[[VeubcW^a2WW72VafZ]r2VYn
d$VY`m_TXbZ4n%Z]\
`bdV
bd$fbc^Y_]nTaW7Z]\B_$Z4`nVa

`VKWW&bZun"bcW7n
Z4T+WX"^bVYnT\Z4`WXZ]f|ubn
z }!}[~ xy[U
V
`ZufVYdbnkWX2^U%_]n
_]
fbc^Y_]TXbZ4nAbcW+_n"VaZ4n
VYnTb_>fz_]CbnTU
VWb{YVKWZ]\TU
VKWXV`VY
`VKWXVYnTm_]TbZun2WYx
Z4`^aZ4d$fVYTVYn
VKWWBV$WXuVYTm^UATU
V@d_ffbn%bceVK_4W7Z]\[TU
V@VYn
dVY`m_]TXbZ4nk_fffz4Z4`bTU
dU
VY`V4xEVYT@
2V@_$Z4`n"Va

`VKWWbZ4nGrE_]n2efVYT2VTU
V7Va
`VKWWbZ4nk^aZ4d$2ZWXVKeCZ]\<TU
V
`Xbd$Vbd$fbc^Y_]nTmW
VYn
d$VY`m_]TVKeWXZ\_]`ffx8yU
V_fffz4Z4`XbTU
d/2n2e
W@_]n_uWWbz4n
d$VYnT$k!Ubc^UW_]TXbcW2VKW@_]n2eeZVKWn
Z4T
W_]TbW&\kAx7Wbn
zbTbcWVK_uWX%TZC2n2e_Cn
VY
`Xbd$Vbdf bc^Y_]nT$Z]\!xy[U
V_>fzuZ4`XbTU
dTZC2n2e
2WVKW@TU
V\Z]ffZ][bn
z^aZudbn2_]TZ4`Xbc_fff[\_u^aTjX
`VKed_nol7U2_4^Ub_]nGrsKtut]v
wm@VabTU
VY`$TU
VY`VCbcW$_
|>_`Xbc_]fV
!TU2_T_
2VK_]`mWbn [bTUUbz4U\h`VKu
VYn2^a4r6Zu`TU
VCVa
`VKWWbZ4n U2_4W"Y_
fZ4TmZ]\6W_]TXbcW\hubn
zC_4WWbz4n
d$VYnTmWYxn"TU
V72`mWXT+^Y_uWXV4rZ4n
V^Y_]n"`VK^a
`mW&b|uVafWZ]f|4V+TX[Z$WX

h
`Z4fVYdW
_]``b|uVKe_TWX
2WTXbT
TXbn
z ) r4_n2e G sgbn@TU
V[Va

`VKWWbZ4n2Wg_]n2e@xn$TU
VWVK^aZ4n2e@^Y_4WXV
bT7bcW7VK_4WX"TZ2n2e%_]n_4WWbz4n
d$VYnTjV4xz2x[W_]df bn
z
waxyU
VWZ]f
TXbZ4n%Z]\6TU
V`VK^a
`mWbZ4n%4bVafce
W
TU
V@WXTm_]TVKeCTbdV2Z4
n2eEx!Z4`+^aZ4d$fVYTV@eVYTm_ffbfcW7[V`Va\VY`7TU
V`VK_ueVY`+TZTU
V_]`TXbc^fV@"
`VKed_]n
_]n2eAl7U2_4^Ub_nkjmsKtut]v
wmxUbfV@TU
V@_]n2_fff
WbcW!TU
VY`VbcW+W2VK^bc_fffb{YVKeA\Zu`!d$Z4n
Z4TZ4n
V7\
n2^aTbZun2WbTbW
VK_4WXCTZ$Va
TVYn2eAjTU
V72`mWXT!2_]`T!Z]\KwbT[\hZ4`+Z4`n"Va

`VKWWbZ4n2WX4x
 KE]EK]c]KG< 7VYn
Z4TV[$7
]jXiGwGTU
V!n
d@VY`<Z]\
`Xbd$Vbdf bc^Y_]nTmWgZ]\GixUbfV
TU
V$`VY
`VKWXVYnTm_]TXbZ4n2Wjms]w+<`bdVd$fb^Y_nTmWj&&Wawar<j&w+7`VY
`VKWXVYnTa_]TXbZ4nGr_n2ej&w } U2_`m_4^a
TVY`XbcWXTb^d$Z
eVafcWYr<W_]TXbcW\h%TU
V@bn
VK42_>f bTXbVKW7
]j&iw
 j&iw   	ff
4jXiGwY fi  rEVK_4^UkZ]\!TU
V
bn
VK42_>f bTXbVKWd_ff_ffffZ>\hZ4`!_]nVa
2Z4n
VYnTXbc_fffz_]Gx6y[U
V7\h
n2^aTXbZ4n
i   j 	      ff     w
 ff"j                !  w
j&l7U2_]`aeZ4no p[ZuTUGrGsKt4tv
wWXU
Z]+W_@z_]"VYT&[VYVYn%j&w_n2e%j&waxyU
V7\
n2^aTXbZ4n
i    Y	  & "#   $% 	  $%&   " $ "
j  b{YVYn2WXTVabnobTTKrgsKt4t 'w!WXU
Z]+W_z_]%2VYTX[VYVYnjms]w+_n2ej&wmxj&yZ"Z42WXVY`|uVTU2_]TKrGn
ZuTXbc^aV$TU
V
Wbd@bfc_]`XbTXVYTXBVYVYni _n2ekTU
V"e2_fff[Z]\+TU
V\
n2^aTbZunk\h`Z4d/TU
V
`VY|4bZ42W$W

WXVK^aTXbZ4nGx)
w (gZ4TU
\h
n2^aTXbZ4n2W_]`V!Zu`n jh\Zu`i dfTXbfubn
zZ4
T"[VWXVYVkTU2_TVY|4VY` ^fc_]2WVk\hZ4`i bWA!Zu`nGr

* +-,./%021234/517686:9156;</=91234/%6:>?6@150BADCE/%FEG8/H.IJ!/HK@176LFE.MONKLFEJ!/FEJPNCQFE0515.6:G?12RS6:/HKUTVFEGU1HCEK:/517WXY/[ZIFE3412CE/[.6%6:>]\Y+
^ 9FEG_0217.a`/W>D./UI GLFE.Mcbd0[>D.G:/H. G:IG:e?>DN/[K@176LFE>D.GHf;g9FE0@9a0217.aM4/H./HK@176:/=1HCQCff6:9/NKLFEJ!/FEJ!NCQFE0515.6:G!hji_FEkH/H.G:6:/BFE.
lnm FE6:6Hfffo5p4pDq4r
stu

fiv?wxy{z8|}x~D7wy	y%xff~xL5w	~
 j5j4Y?5D57Bja5]		5	 HP2O7	4D7HHHYffD
 HD	ffjc27	]ff4Hjn=HjSffff4Y 	SD7Sff?:
5ff	7j5c5	4D5HH2affDaHa:	DHj	
8&@	)g=B	
 5ff?HDHj!cH5	?7D55ffj?ff5aYj?HPP	 H 
]D@]H=ffj5= 5cffD	ffjHj=!Oa
 D Sa=Uj25
  P5!]VV	H]7Bjff4
?5U!P	ffj= : 	[4
 j	74ffg5	DHPP  	V!Oc7	O5HYO5ffj5V5
j	7j	ffffj5HBjn5Bjff4]  ffY	ffj5:S!j5ff	DHj{:O]
?ffjO5]DffSDffjHUP  !HHg	g[S5Hffj%S5]4	Haff j	U
4S]5Hj	jjDffD
<   	ff
fi !#"$%'&)(+*,-.0/21435/*5%6"$7809
: ;<=   5ffj 7Hj jnjD@] B5#5Bjff?
 >#	#5 A
 @ B+ >DC
 E=: 	H54
F
 Hj4!	Dff5?!O5ff	DD@5H GH  5!Oa	5. GH  5 5
IKJ 	Hgg 
 Lff?7c5ffj =5HB-ffjH!Pa{7ffPS?ff5j&57	2
@	M
 G j54 	DS7& c5j54 5	BjHj@! c7Hn7	H5
 ?5j54 75 @!]H	7 7	HHj:54n O NQ
 P RTS 
j	H4	D?5O GH  ?5U P ff5cD0 V[	D2S?57	H	T
 S
]D0 V[	D2S&ffHj:5c7	HX W{c54	B5aj]75Oj	HD	Dc=!Oa	 Y
 S
	[
 Z]
 \_^a`0bc5dMe2f<^7 P 4)_j4H!&7 j 5	7&7ff	DHj 	Sff @ 55	
Bj	DO P O5!]! P O  aV:5ffjnff7n =5cHffjc5c5<
	n57D7	?Hj55VcV2S:5Pa :5ff  ffajffj7	 P 
5g!!a	!j=j  ]75Hff4Hjg=55!DHjO Z%Hj5  ]
5524Hj P 		SD]B- g		ffffh fjiUHj?7O	7	2!jk
 P 
W{c 7 j 5	+
 NO52[-ffjc?	ffj]m
 l : 	h Z4?O5HY	42]5	H N
O5HB-ffS	j. >nCoS[RpP   ffajffjO5	H >nCn 	q
 >rCl P Yj	D P c5g
	7ffDY]?7	D2S	57jH{	ffYSff542DHj   5 ]!Y5	
>nC
l :	 P 4	Bj	Ds
 ZTt P >nCl :h Z4  7D7u
 l:h ZD
?57 j5DHjn7Hm
 l):	h Z4<	j. >	n7Bjff H	75	
>vC# 	q
 >[C#
l :	h Z4+
 W{O5	+ >rCvSc	Bj	Dff  5 	  5x
 w
P :h ZU!O	 .
 >qCl P !	 >yCzS[ROP 	z
 N]5HB-ffj
 Dff:5c	O5ff	DHjD	Bffa5a	D2S
N) {4| }~| 5 }~| 
 | 5 {4| |p5h {4|O}|  
 ff:	DHj!-P74	B5jff5c  {| }|  5 }|  | 4  %	D2S P  {  | { }  
 7HB:jn5Bj4 P 5	 4 5 -	n
 Z # { P O
  4 
 D	Bff57Bjffs >V  ?ff7 5HB	 N8Uj2S~ >V5HB	Vg	{
757	%S!!j57jH7O Z


fi			<	

~UO	2a5
6?		a-X0j0#   XjX0 X.;-# -#	# -8~	)0?
-O0 -q0		.5	hXO-06	.	#-q	??		 -; #6-.#0
 		5	s0j+#	 #0 -#	6-k	0 +	#O	#.	#U;.0-	
a-r00;)	00#M0		  -n	#	#XO#0v0a0?o0
8# X##U	 #	 #0 -5s#0h0	)#	0?X	_aO0#
X+;. -M	.?		0;?--hM	+	-#O#
 	6O;-z60X	. X0a0M	-#Ov0 0-#+
.	#X2	#_0-	; -00	#s050;-#	 	
a-	-#X)?-)0a0 -	v-,-	#k- -YU-;-#q-	
	#	#X40-#O #~?	~	4 #~00#   X?	8540-#O6		a-
n;4; . 	0O+- 60 -U# X##?	~	-#O0	6#0	?0 #
;5	-#4+_H.M0a0 -q	#q+0TMU0q0	$#0 --
?		#.-#p0kn0 0.M)<T	6k04-T#	#;6v0h
040- ~0j54+00-O. ;_# +Xo 0j),0-0;-#
ns	--T0?-r 	00.a-?-p -	O68+#	?#0 -	M	 -?j
$#0)#0		0#4p-#H 	M8+v	0Ma-$0 -#	?#0 -	?	 -?j
$#0O;A	MTp-#M8qh4  -4XT 	T	-#  )-	
?	$#0 -z	y24~ 5j#X	.?-- M-#0U0~	+	-#O
 y#y-k-	U
?O)	U -	#n-r#?#0rM	;-	nx	#--
	0 5	#;a-	+	X-#4# X##.4O	40 -~0#  ?	
4#U-TU?0-0#h4z#+;2-60X##		4	020M	
a- -O0+	+ -6sM?.#a--
	+j- M05	0U0X04D-8)X#M	-#O~,-?4
j 		8	# ; -	z		+	-#O5-)  $$~s2)2)0<
 ~5z25D
 0ffOX	U-#;6-q0	$0+0kM8	.??#X

-nz ?j0	? --v.#U4X oz0	q	U0	?+8##+6	 
?j#	--?	0X	5	 #0 -06-- 54#0_~	--	08
-	O	+0#X	+ 0U8X#X 		y5##X8X2$-#5
	 #OX	#4+  6)+020 )2		
 	5	
6##-.-

-<  	4-00$$- z05h	s	O;<88#
 0  	


	fiffff 

4	0-	- -#o!D-~;	<-	s0 	 -~6?0 #"#	ff
$%ff&fi'
 0
()#*+(
## 24-540)<5v-~0-+- 0U05+. (	  r	z	U0)4O	
0 -8X8	 -;###-#/012305476&	fi8:9;k<fi0fi(*	
,

=>?

fi@BACDFE3GHCIfiJJLKAD%JNMDOQPSRMCMUTVICK&JWVKTYXZA[O%I\ J

]_^a`UbLcdfe[gad;hjilk%m nm o.^pdfqg_rstt.uvg"w-`.xzy{a|}fi^a~Q`+B^|#fi~^'b#m ~5^`.m %%m{^am ~^a``+y%`fi^~5^.|
]-`` {a|m "%b~^a`%#gzq|bLcgnW|#ygad;nmx:%B^a|#nl^a~d`~W~L|#nm xg:q`m+yy%|m nN^Fip`nW
xm ~^a`m %wS`xzy~m ~5^`.g
]c`~dfigfigrst.tuvg%}[m.b~{|m+nW^a.^m~Wc|xz``~W`.|B~Wc|#`nWgipYf&fi+f&fiWS-
' $

fi&"z- 





%dyygu!Uuss;m{a`zB{a~W`dw-:g

|bLcfi~W|#ndUNgad%h|m+n{pd%egrsttvg_~WnW%b~WnL|^|#fi~5^'b#m+~^a`^nW|{m ~^a`%m{;m ~mg-N f
%d d%uU#U![g
` {^a%d%ggad%hm!{{^a|#ndeggrst vgB'^|m+nW~^axz|zm{a`n^a~Wcx`nB~L|~^aWm ~^l'm+k^{^a~` 
ynW`y%`l^a~^a`%m{`nW`nWx:{m |g. :

dd%.#U+%g

^a~W|#nd'qgdUh`~L~{a`kdg$rstts+vgi|#~^.^a~Wc|7x:^a^axm{;~Wnm+%|#nWm!{` fm:cfiy|#nWnm yc"m %
nW|{m ~W|ynW`k{a|#x#gq|bLcgnW|#ygwfpqpts fisdw-cn^~^m `.yy{|#nBm+k%`nm+~W`nW`nf}Uy|#nW~
U~L|#x#dqBB^a|#%md%5~Wn^mg
^a~W|#nd'qgdUh`~L~{a`kdg$rstt [vgi|#~^.^a~Wc|7x:^a^axm{;~Wnm+%|#nWm!{` fm:cfiy|#nWnm yc"m %
nW|{m ~W|ynW`k{a|#x
gz%z  7&



& 

- 

g

nL|xm dgadShc%m.bWc^afim d;grst.t vgB~Lc|b`xzy{a|}^a~` %m!{^am ~5^`.` x``~W`|
.^%b~^a|`nWxm!{%`nWx#gq|bLcgnW|#yg%wpqBp[dU|#y%m+nW~Wxz|#~-` Sw-`xzy~W|#nfibL^a|#%b|d
~W|#nB^a|#n^~g
m+nW|#dQgdUhe+`c%`d%:grst!tvg- 
S- 

L7%.fi
& +

B:&fi 



%
#ggg%nW|#|#xm dfim [nm %bL^Wb`%g

`.~W~{a`kd:gadh'^ko.^ad'_grstt.vgip|~^afim ~5^`.%`nWx~WnW`nW|{m ~^a`%#d|#y%|#%|#%b^a|#nL
|#%b|d%m %|}%bL{a%|Y%b~^a`%m!{|#y%|#%|#%bL^a|#g

 L%drp[vd%u!fifig

`nWdSgSrst.s vgB|#fi~L|#%b|zBc^bLcm nL|~WnW|`.^anW|b~z^a`%` 7m{a|#knm
gB
' 

W&.drs vds# U[sg

m ~Wd7gd|m nW%
dgadBh|{axm d]grsttUvg`nWm yynW`}fi^axm+~^a`%` 7|#xzy^an^b#m{zm ~mg
 f.%%&.dWds.t's#fi[g
mfi m.^m#d:gad;m y%m.^ax:^a~Wn^a`dwgadh ^|#n^pd'grsttuvg$BZ`nW|#fi.|{`.y%|m %cfiy%|#nW.nm yc
~Wnm %.|#nWm{#gil%&%.

!  & 

"&& 

.%S 

B

fi.



:Sd

yyg%uttfig[yn^a|#nW;|#n{m %gwfN g



c%m n`.d-gad-"m ^{md-gad-h`~Lcd-:g;rsttvg|m`^a^~Lc|}[m xy{| ;nL`y%`fi^~5^`.%m{
`nLx{m |m %m ~m k%m.||#y|#%|#%bL^a|#gBq|bLcgnW|#ygqBWs+ptdB^ao|#"wS`xzy~m ~5^`.Zm+kgad
m nL!m n^a|#nl^a~g
c%m n`.d3gadh

`.~Wcd:grstt+vg|m`.^^a~Wcxz`|{#gip 7z&fi.%

:qBps
pt

-##%. f.&.%dyyg;ss#'ss.ug7{{|#nl^a`

%d^o.|#

w-`xzy~m+~^a`Zm kgad%m nW m n^.|#nl^a~d'em+fi%m nLst.t %g
c%m n`.dfgdh

`~Wcd:g-rsttvg|pm {a~WnW|m5`^aQ^a~Wcx`U|{#gip&fi z&

%.%B-.##N f$%d yyg%ust!Uug

	

fi
fiff

	!#"%$&')(*$,+-.0/21	1&34057668:92;'=<>$&?:='@A8:,BCD9&@@AE,BC?F9	,$2(HG'6,,G',BIJ7@A8:9	<
8:C(*$&?F9	,JLK,MONCP	PQ0N	RDSUTVKXWYQ0MZR[Q2\AS*]CN	^H_`M	aPcbY\AQ2RDdc]2S*Q2T,]CQ75e68IC(f<D,$&8Igh	80Ji9	%9&@j?:k
6,$&6,8l$m66,$&8:Gnpo:qjrts#/1	1	u
"%$&')(*$fi+vwyxfi$2k,$fiDwfi.0/21	z	{'40|r7Ji}	~'g|C;$&<6'(=	57$&66'()B$m?F9	9&@578:<J?:8:9	}
8:C(*$&?F9	,JjM	b'R0TN	^MF>_wM	aPcb'\AQR-N	T,dc2\AQad]SUQT,]CQ	X.[u'40X/u	{2c/3,/	
"%B2q$&8:?IkYg	&Z.0/1		z!40	H8:9Z}	80$&<Jj?:kBC9	<<9	J,JF	'o>RFM]CQCQ0	SUTYmwMFfi\UWYQjdaPM&SUb'a#M	T\UW'Q
QC]WYN	TSN	\ASUM	T>MFKWYMZbmWY\RFM]CQCQH9&(,/	Z66	!z&3,&l$m?F9	,$2(cHk'gJi*B$2($m~,9	80$m?:9	8:g	
xj68FY?IGDO"O,JF'gfJ`.G)4`dcQaDN	T\AS*]0TM	RCaDN	\ASUM	T>RFM]CQCS*T'	"o[sH8:JIJC.0/1	{Zz'4023Y	2
3Y	1'5(J9>=hxt`80$	BIk<$&p$&,G+v	JI	E	xj$ZGZ=}'J=h79&j(G'}	xj68:JY?0$m?F9	
/1	z	
"%B2q$&8:?IkYg	H`+7$2g	Jw`.0/1	{Z1'40|	9	<>6k')(9YJF9	6k'*B$2(v68:9Z~'(=<>J-@8I9	<?:knJ?0$&,G'6,9m='?9&@
$&8:?F)cBI*$2(fi'?:C()(f}	,BC	o"C(?:8j7Hy"BIk'	firD`.[wGJ=40`N	]WYS*TQO0T\AQ^U^)SfZQT,]CQD
wGZY~E8I}	k'	80Ji?FgpH8:JIJ
"%B,JFg	fijql`ql.0/1&3Y!40hsjkpG'BI*Ji9	68:9	~'(<@9	8JF9Z<nBI(*$	JIJFJ9m@JF'?:,BCJ=?Ik9	E?
	E,$&'?F),8jM	b'R0TN	^MF-dc&a:MZ^fSU]tMC	SU].F'40{/C,&{
xjC?:8xt.C/1	z'	4C5?:k9	8:g9m@GZ*$&}	9'Ji*Jj@A8:9	<,80JF?68F,BI=6'(JfiR0\AS ]2S*N	^0T,\AQ2^*^)S)	QT]0Q2	X.0/&40
xjC?:8fixwyr7p(8fi.0/1	z!	40h9ZE,G$&?F9	,J9&@$ZJ:JFE<6?F9	A~,$ZJFG|?:8:E?:k<>$2Y?:,$m,BC
JFg!J?:<Jo#jR[M2]0QCQC	S*T'&OMF\UW'QhtN	\ASUM	T,N	^-_wM	TQ2RFQT,]CQpMZTtRC\AS j]SUN	^j0T\AQ^U^fS)	Q2T,]CQ`66
/z	2c/2z	z
	C(<$&H7t$&E?:	+-H.C/1	1/&4C79&j(G'}	OBC9	<6')(*$&?F9	|E,Ji}h+fi9	8I$m668:92;Y<$&?=9Z,Joi
RFM]CQ0QC	SUTYmM\UWYQ-tN	\AS*MZT,N	^H_`M	TQRFQT]0Q-M	TOtRC\AS j]SUN	^0T,\AQ2^*^)S)	QT]0Q2&66,1	&3m!1		1
	C(<$&'l	J:ZE		+vZ.0/1Z1	'40&57~cG'E,BC?F	fi$&,GDG'C@$&E'(?H8:$	JF9Z'=}5BC9	<6E?C$&?F9	,$2(!BC9Z8:	
o%RFM2]0QCQ0ZS*T'&lMFD\UWYQ-NZ\AS*M	TN	^H_wMZTQRFQT]0QtM	TOtRC\AS ]SUN	^CT,\AQ^U^)SfZQT]0Q2&66,&3Y2&3Yz



fiJournal of Artificial Intelligence Research 3 (1995) 431-465

Submitted 9/95; published 12/95

OPUS: An Efficient Admissible Algorithm for
Unordered Search
Geoffrey I. Webb

webb@deakin.edu.au

Deakin University, School of Computing and Mathematics
Geelong, Vic, 3217, Australia.

Abstract
OPUS is a branch and bound search algorithm that enables efficient admissible search
through spaces for which the order of search operator application is not significant. The
algorithms search efficiency is demonstrated with respect to very large machine learning
search spaces. The use of admissible search is of potential value to the machine learning
community as it means that the exact learning biases to be employed for complex learning
tasks can be precisely specified and manipulated. OPUS also has potential for application
in other areas of artificial intelligence, notably, truth maintenance.

1. Introduction
Many artificial intelligence problems involve search. Consequently, the development of
appropriate search algorithms is central to the advancement of the field. Due to the complexity of the search spaces involved, heuristic search is often employed. However, heuristic
algorithms cannot guarantee that they will find the targets they seek. In contrast, an admissible search algorithm is one that is guaranteed to uncover the nominated target, if it
exists (Nilsson, 1971). This greater utility is usually obtained at a significant computational
cost.
This paper describes the OPUS (Optimized Pruning for Unordered Search) family of
search algorithms. These algorithms provide efficient admissible search of search spaces in
which the order of application of search operators is not significant. This search efficiency
is achieved by the use of branch and bound techniques that employ domain specific pruning
rules to provide a tightly focused traversal of the search space.
While these algorithms have wide applicability, both within and beyond the scope of
artificial intelligence, this paper focuses on their application in classification learning. Of
particular significance, it is demonstrated that the algorithms can efficiently process many
common classification learning problems. This contrasts with the seemingly widespread
assumption that the sizes of the search spaces involved in machine learning require the use
of heuristic search.
The use of admissible search is of potential value in machine learning as it enables better
experimental evaluation of alternative learning biases. Search is used in machine learning
in an attempt to uncover classifiers that satisfy a learning bias. When heuristic search is
used it is difficult to determine whether the search technique introduces additional implicit
biases that cannot be properly identified. Such implicit biases may confound experimental
results. In contrast, if admissible search is employed the experimenter can be assured that
c
1995
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiWebb

the search technique is not introducing confounding unidentified implicit biases into the
experimental situation.
The use of OPUS for admissible search has already led to developments in machine
learning that may not otherwise have been possible. In particular, Webb (1993) compared
classifiers developed through true optimization of Laplace accuracy estimate with those obtained through heuristic search that sought but failed to optimize this measure. In general,
the latter proved to have higher predictive accuracy than the former. This surprising result,
that could not have been obtained without the use of admissible search, led Quinlan and
Cameron-Jones (1995) to develop a theory of oversearching.
This paper offers two distinct contributions to the fields of computing, artificial intelligence and machine learning. First, it offers a new efficient admissible search algorithm for
unordered search. Second, it demonstrates that admissible search is possible for a range of
machine learning tasks that were previously thought susceptible only to efficient exploration
through non-admissible heuristic search.

2. Unordered Search Spaces
For most search problems, the order in which operators are applied is significant. For
example, when attempting to stack blocks it matters whether the red block is placed on the
blue block before or after the blue block is placed on the green. When attempting to navigate
from point A to point B, it is not possible to move from point C to point B before moving to
point C. However, for some search problems, the order in which operators are applied is not
significant. For example, when searching through a space of logical expressions, the effect
of conjoining expression A with expression B and then conjoining the result with expression
C is identical to the result obtained by conjoining A with C followed by B. Both sequences
of operations result in expressions with equivalent meaning. In general, a search space is
unordered if for any sequence O of operator applications and any state S, all states that can
be reached from S by a permutation of O are identical. It is this type of search problem,
search through unordered search spaces, that is the subject of this investigation.
Special cases of search through unordered search spaces are provided by the subset
selection (Narendra & Fukunaga, 1977) and minimum test-set (Moret & Shapiro, 1985)
search problems. Subset selection involves the selection of a subset of objects that maximizes
an evaluation criterion. The minimum test-set problem involves the selection of a set of
tests that maximizes an evaluation criterion. Such search problems are encountered in many
domains including machine learning, truth maintenance and pattern recognition. Rymon
(1992) has demonstrated that Reiters (1987) and de Kleer, Mackworth, and Reiters (1990)
approaches to diagnosis can be recast as subset selection problems.
The OPUS algorithms traverse the search space using a search tree. The root of the
search tree is an initial state. Branches denote the application of search operators and the
nodes that they lead to denote the states that result from the application of those operators.
Different variants of OPUS are suited to each of optimization search and satisficing search.
For optimization search, a goal state is an optimal solution. For satisficing search, a goal
state is an acceptable solution. It is possible that a search space may include multiple goal
states.
432

fiAn Efficient Admissible Algorithm for Unordered Search

The OPUS algorithms take advantage of the properties of unordered search spaces to
optimize the effect of any pruning of the search tree that may occur. In particular, when
expanding a node n in a search tree, the OPUS algorithms seek to identify search operators
that can be excluded from consideration in the search tree descending from n without
excluding a sole goal node from that search tree. The OPUS algorithms differ from most
previous admissible search algorithms employed in machine learning (Clearwater & Provost,
1990; Murphy & Pazzani, 1994; Rymon, 1992; Segal & Etzioni, 1994; Webb, 1990) in that
when such operators are identified, they are removed from consideration in all branches of
the search tree that descend from the current node. In contrast, the other algorithms only
remove a single branch at a time without altering the operators considered below sibling
branches, thereby pruning fewer nodes from the search space.
If it is not possible to apply an operator more than once on a path through the search
space, search with unordered operators can be considered to be a subset selection problem
select a subset of operators whose application (in any order) leads to a goal state. If a single
operator may be applied multiple times on a single path through the search space, search
with unordered operators can be considered as a sub-multiset selection problemselect the
multiset of operators whose application leads to the desired result.
A search tree that traverses an unordered search space in which multiple applications of
a single operator are not allowed may be envisioned as in Figure 1. This example includes
four search operators, named a, b, c and d. Each node in the search tree is labeled by the
set of operators by which it is reached. Thus, the initial state is labeled with the empty
set. At depth one are all sets containing a single operator, at depth two all sets containing
two operators and so on, up to depth four. Any two nodes with identical labels represent
equivalent states.
There is considerable duplication of nodes in this search tree (the label {a, b, c, d} occurs
24 times). In Figure 1 (and the following figures), the number of unique nodes is listed
below each depth of the search tree. Where this number can be derived from the number
of combinations to be considered, this derivation is also indicated.
It is common during search to prune regions of the search tree on the basis of investigations that determine that a goal state cannot lie within those regions. Figure 2 shows a
search tree with the sub-tree below {c} pruned. Note that, due to the duplication inherent
in such a search tree, the number of unique nodes remaining in the tree is identical to that
in the unpruned tree. However, if it has been deemed that no node descending from {c}
may be a goal, then all nodes elsewhere in the search tree that have identical labels (are
reached via identical sets of operator applications) to any nodes that occur in the pruned
region of the tree could also be pruned. Figure 3 shows the search tree remaining when all
nodes below {c} and all their duplicates have been deleted. It can be seen that the number
of unique nodes in the remaining search tree (the tree at depths 2, 3 and 4) has been pruned
by more than half. Similar results are obtained in the case where multiple applications of
a single operator are allowed and the nodes are consequently labeled with multisets.
The OPUS algorithms do not provide pruning rulesmechanisms for identifying sections
of the search tree that may be pruned. Rather, they take pruning rules as input and seek
to optimize the effect of each pruning action that results from application of those rules.
The OPUS algorithms were designed for use with admissible pruning rules. When
used solely with admissible pruning rules the algorithms are admissible. That is, they are
433

fiWebb

{ a, b }

{a}

{ a, c }

{ a, d }

{ a, b }

{b}

{ b, c }

{ b,d }

{ a, b, c }

{ a, b, c, d }

{ a, b, d }

{ a, b, c, d }

{ a, b, c }

{ a, b, c, d }

{ a, c, d }

{ a, b, c, d }

{ a, b, d }

{ a, b, c, d }

{ a, c, d }

{ a, b, c, d }

{ a, b, c }

{ a, b, c, d }

{ a, b, d }

{ a, b, c, d }

{ a, b, c }

{ a, b, c, d }

{ b, c, d }

{ a, b, c, d }

{ a, b, d }

{ a, b, c, d }

{ c, b, d }

{ a, b, c, d }

{ a, b, c }

{ a, b, c, d }

{ a, c, d }

{ a, b, c, d }

{ a, b, c }

{ a, b, c, d }

{ b, c, d }

{ a, b, c, d }

{ a, c, d }

{ a, b, c, d }

{}
{ a, c }

{c}

{ b, c }

{ c, d }

{ a, d }

{d}

{ b, d }

{ c, d }
4C =1
0

4C =4
1

4C =6
2

{ b, c, d }

{ a, b, c, d }

{ a, b, d }

{ a, b, c, d }

{ a, c, d }

{ a, b, c, d }

{ a, b, d }

{ a, b, c, d }

{ b, c, d }

{ a, b, c, d }

{ a, c, d }

{ a, b, c, d }

{ b, c, d }

{ a, b, c, d }

4C =4
3

4C =1
4

Figure 1: Simple unordered operator search tree
guaranteed to find a goal state if one exists in the search space. However, the algorithms
may also be used with non-admissible pruning heuristics to obtain efficient non-admissible
search.
The OPUS algorithms are not only admissible (when used with admissible pruning
rules), they are systematic (Pearl, 1984). That is, in addition to guaranteeing that a goal
will be found if one exists, the algorithms guarantee that no state will be visited more than
once during a search (so long as it is not possible to reach a single node by application of
different sets of operators).

3. Fixed-order Search
A number of recent machine learning algorithms have performed restricted admissible search
(Clearwater & Provost, 1990; Rymon, 1993; Schlimmer, 1993; Segal & Etzioni, 1994; Webb,
1990). All of these algorithms are based on an organization of the search tree, that, when
considering the search problem illustrated in Figures 1 to 3, traverse the search space in the
manner depicted in Figure 4. Such an organization is achieved by arranging the operators
in a predefined order, and then applying at a node all and only operators that have a higher
order than any operator that appears in the path leading to the node. This strategy will be
434

fiAn Efficient Admissible Algorithm for Unordered Search

{ a, b }

{ a}

{ a, c }

{ a, d }

{ a, b }

{ b}

{ b, c }

{ b, d }

{ a, b , c }

{ a, b , c, d }

{ a, b , d }

{ a, b , c, d }

{ a, b , c }

{ a, b , c, d }

{ a, c, d }

{ a, b , c, d }

{ a, b , d }

{ a, b , c, d }

{ a, c, d }

{ a, b , c, d }

{ a, b , c }

{ a, b , c, d }

{ a, b , d }

{ a, b , c, d }

{ a, b , c }

{ a, b , c, d }

{ b, c, d }

{ a, b , c, d }

{ a, b , d }

{ a, b , c, d }

{ c, b, d }

{ a, b , c, d }

{ a, b , d }

{ a, b , c, d }

{ a, c, d }

{ a, b , c, d }

{ a, b , d }

{ a, b , c, d }

{ b, c, d }

{ a, b , c, d }

{ a, c, d }

{ a, b , c, d }

{ b, c, d }

{ a, b , c, d }

{}

{c}

{ a, d }

{ d}

{ b, d }

{ c, d }
4C =1
0

4C =4
1

4C =6
2

4C =4
3

4C =1
4

Figure 2: Simple unordered operator search tree with pruning beyond application of operator c

called fixed-order search. (Fixed-order search has also been used for non-admissible search,
for example, Buchanan, Feigenbaum, & Lederberg, 1971).
Figure 5 illustrates the effect of pruning the sub-tree descending below operator c, under
fixed-order search. As can be seen, this is substantially less effective than the optimized
pruning illustrated in Figure 3. Schlimmer (1993) ensures that the pruning effect illustrated
in Figure 3 is obtained within the efficient search tree organization illustrated in Figure 4, by
maintaining an explicit representation of all nodes that are pruned. The resulting search tree
is depicted in Figure 6. This approach requires the considerable computational overhead of
identifying and marking all pruned states following every pruning action, and the restrictive
storage overhead of maintaining the representation. (One of the search problems tackled
below contains 2162 states. To represent whether a state is pruned requires a single bit.
Thus, 2162 bits would be required to represent the required information for this problem,
a requirement well beyond the capacity of computational machinery into the foreseeable
future.) Further, it is open to debate whether this approach does truly prune all identified
nodes from the search space. Nodes that that have been pruned will still need to be
generated when encountered in previously unexplored regions of the search tree in order to
435

fiWebb

{ a, b }

{ a, b , d }

{ a}
{ a, b , d }
{ a, d }

{ a, b }

{ a, b , d }

{ b}
{ a, b , d }
{ b, d }
{}

{c}

{ a, b , d }
{ a, d }
{ a, b , d }
{ d}

4C =1
0

4C =4
1

{ b, d }

3C =3
2

3C =1
3

3C =0
4

Figure 3: Simple unordered operator search tree with maximal pruning beyond application
of operator c

{ a, b }

{ a, b, c }

{ a, b , c, d }

{ a, b, d }
{ a}

{ a, c }

{ a, c, d }

{ a, d }
{ b, c }
{}

{ b, c, d }

{ b}
{ b ,d }
{c}

{ c, d }

{ d}
4C =1
0

4 C =4
1

4C =6
2

4C =4
3

4C =1
4

Figure 4: Static search tree organization used by fixed-order search

436

fiAn Efficient Admissible Algorithm for Unordered Search

{ a, b, c }
{ a, b }
{ a}

{ a, c }

{ a, b , c, d }

{ a, b, d }
{ a, c, d }

{ a, d }
{}

{ b}

{ b, c }

{ b, c, d }

{ b ,d }
{c}
{ d}

4C =1
0

4 C =4
1

4C =4
3

5

4C =1
4

Figure 5: Effect of pruning under fixed-order search
{ a, b }

{ a, b, d }

{a}
{ a, d }
{}

{b}

{ b,d }

{c}
{d}
4C =1
0

4 C =4
1

3 C =3
2

3 C =1
3

3C =0
4

Figure 6: Optimal pruning under fixed-order search
be checked against the list of pruned nodes. Consider, for example, the node labeled {a} in
Figure 5. When expanding this node it will be necessary to generate the node labeled {a,
c}, even if this node has been marked as pruned. Only once it is generated is it possible
to identify it as a node that has been pruned. This node could in principle be pruned
anyway by application of some variant of the technique that identified it as prunable in
the first place. Viewed in this light, it can be argued that Schlimmers (1993) approach
does not reduce the number of nodes that must be generated under fixed-order search.
All that it saves is the computational cost of determining for some nodes whether they
require pruning or not. (This assumes that the optimistic pruning mechanism will be able
to determine for any node n from the search space below a pruned node m that n should
also be pruned, irrespective of where n is encountered in the search tree. If the optimistic
pruning mechanism is deficient in that it cannot do this, then Schlimmers (1993) approach
will increase the amount of true pruning performed to the extent that it overcomes this
deficiency.)

4. The Feature Subset Selection Algorithm
Fixed-order search traverses the search space in a naive mannerthe topology of the search
tree is determined in advance and takes no account of the efficiency of the resulting search.
In contrast, the Feature Subset Selection (FSS) algorithm (Narendra & Fukunaga, 1977)
performs branch and bound search in unordered search spaces, traversing the search space
437

fiWebb

{c}

{ a ,b }

{ a,b, d }

{ a}

{}

{ a ,d }
{ b}

{ b, d }

{ d}
4C =1
0

4 C =4
1

4C =6
2

4C =4
3

4C =1
4

Figure 7: Pruning under FSS-like search
so as to visit each state at most once and dynamically organizing the search tree so as to
maximize the proportion of the search space placed under unpromising operators. It can
be viewed as a form of fixed-order search in which the order is altered at each node of
the search tree so as to manipulate the topology of the search tree for the sake of search
efficiency. Unlike Schlimmer (1993), the pruning mechanism ensures that nodes that are
identified as prunable are not generated.
The power of this measure is illustrated by Figure 7. In this figure, fixed-order search
is performed on the simple example problem illustrated in Figures 1 to 6, with the order
changed so that the operator to be pruned, c, is placed first. As can be seen, this achieves
the amount of pruning achieved by optimal pruning. This effect can be achieved with
negligible computational or storage overhead.
However, FSS is severely limited in its applicability as
 it is restricted to optimization search;
 it is restricted to tasks for which each operator may only be applied once (subset
selection);
 it is restricted to search for a single solution;
 it requires that the values of states in the search space be monotonically decreasing.
That is, the value of a state cannot increase as a result of an operator application;
and
 the only form of pruning that it supports is optimistic pruning.

5. The OPUS Algorithms
The OPUS algorithms generalize the idea of search space reorganization from FSS. Two
variations of OPUS are defined. OPUSs is a variant for satisficing search (search in which
any qualified object is sought). OPUSo is a variant for optimization search (search in which
an object that optimizes an evaluation function is sought). Whereas FSS uses node values
for pruning, OPUSo uses optimistic evaluation of the search space below a node. This
removes the requirement that the values of states in the search space be monotonically
decreasing and opens the possibility of performing other types of pruning in addition to
optimistic pruning.
438

fiAn Efficient Admissible Algorithm for Unordered Search

In the analysis to follow, where comments apply equally to both variants the name
OPUS will be employed. When a comment applies to only one variant of the algorithm, it
will be distinguished by its respective superscript.
OPUS uses a branch and bound (Lawler & Wood, 1966) search strategy that traverses
the search space in a manner similar to that illustrated in Figure 4 so as to guarantee
that no two equivalent nodes in the search space are both visited. However, it organizes
the search tree so as to optimize the effect of pruning, achieving the effect illustrated in
Figure 6 without any significant computational or storage overhead.
Rather than maintaining an operator order, OPUS maintains at each node, n, the set
of operators n.active that can be applied in the search space below n. When the node is
expanded, the operators in n.active are examined to determine if any can be pruned. Any
operators that can be pruned are removed from n.active. New nodes are then created for
each of the operators remaining in n.active and their sets of active operators are initialized
so as to ensure that every combination of operators will be considered at only one node in
the search tree.
It should be kept in mind that it is possible that many states in a search space may
be goal states. For satisficing search all states that satisfy a given criteria are goal states.
For optimization search, all states that optimize the evaluation criteria are goal states. For
efficiency sake, the OPUS algorithms allow sections of the search space to be pruned even if
they contain a goal state, so long as there remain other goal states in the remaining search
space.
5.1 OPUSs
The OPUSs algorithm is presented in Figure 8. This description of OPUSs follows the
conventions employed in the search algorithm descriptions provided by Pearl (1984).
This definition of OPUSs assumes that a single operator cannot be applied more than
once along a single path through the search space. If an operator may be applied multiple
times, the order of Steps 8a and 8b should be reversed. Unless otherwise specified, the
following discussion of OPUS assumes that each operator may be applied at most once
along a single path.
If it is desired to obtain all solutions that satisfy the search criterion,
 Step 2 should be altered to exit successfully, returning the set of all solutions;
 Step 6b should be altered to not exit, but rather to add the current node to the set
of solutions; and
 The domain specific pruning mechanisms employed at Step 7 should also be modified
so that no goal state may be pruned from the search space.
This form of search could be used in an assumption-based truth maintenance system to find
the set of all maximally general consistent assumptions. This would provide efficient search
without the need to maintain and search an explicit database of inconsistent assumptions
such as the ATMS no-good set (de Kleer et al., 1990). Unless otherwise specified, the
discussion of OPUS below assumes that a single solution is sought.
The algorithm does not specify the order in which nodes should be selected for expansion
at Step 3. Nodes may be selected at random, by a domain specific selection function, or by
439

fiWebb

Data structure:
Each node, n, in the search tree has associated with it three items of information:
n.state the state from the search space that is associated with the node;
n.active the set of operators to be explored in the sub-tree descending from the node; and
n.mostRecentOperator the operator that was applied to the parent nodes state to create the current nodes state.
Algorithm:
1. Initialize a list called OP EN of unexpanded nodes as follows,
(a) Set OP EN to contain one node, the start node s.
(b) Set s.active to the set of all operators, {o1 , o2 , ...on }
(c) Set s.state to the start state.
2. If OP EN is empty, exit with failure; no goal state exists.
3. Remove from OP EN a node n, the next node to be expanded.
4. Initialize to n.active a set containing those operators that have yet to be examined, called
RemainingOperators.
5. Initialize to {} a set of nodes, called N ewN odes, that will contain the descendants of n that
are not pruned.
6. Generate the children of n by performing the following steps for every operator o in n.active,
(a) Generate n0 , a node for which n0 .state is set to the state formed by application of o to
n.state.
(b) If n0 .state is a goal state, exit successfully with the goal represented by n0 .
(c) Set n0 .mostRecentOperator to o.
(d) Add n0 to N ewN odes.
7. While there is a node n0 in N ewN odes such that pruning rules determine that no sole remaining goal state is accessible from n0 using only operators in RemainingOperators, prune
all nodes in the search tree below n0 from the search tree below n as follows,
(a) Remove n0 from N ewN odes.
(b) Remove n0 .mostRecentOperator from RemainingOperators.
8. Allocate the remaining operators to the remaining nodes by processing each node n0 in
N ewN odes in turn as follows,
(a) Remove n0 .mostRecentOperator from RemainingOperators.
(b) Set n0 .active to RemainingOperators.
9. Add the nodes in N ewN odes to OP EN .
10. Go to Step 2.

Figure 8: The OPUSs Algorithm
440

fiAn Efficient Admissible Algorithm for Unordered Search

the order in which nodes are placed in OP EN . First-in-first-out node selection results in
breadth-first search while last-in-first-out node selection results in depth-first search.
The order of processing is also unspecified at Steps 7, 8 and 9. Depending upon the
domain, practical advantage may be obtained by specific orderings at these steps.
OPUSs has been used in a machine learning context to search the space of all generalizations that may be formed through deletion of conjuncts from a highly specific classification
rule. The goal of this search is to uncover the set of all most general rules that cover
identical objects in the training data to those covered by the original rule (Webb, 1994a).
5.2 OPUSo
A number of changes are warranted if OPUS is to be applied to optimization search. The
following definition of OPUSo, a variant of OPUS for optimization search, assumes that
two domain specific functions are available. The first of these functions, value(n), returns
the value of the state for node n, such that the higher the value returned, the higher the
preference for the state1 . The second function, optimisticV alue(n, o) returns a value such
that if there exists a node, b, that can be created by application of any combination of
operators in the set of operators o to the state for node n, and b represents a best solution
(maximizes value for the search space), optimisticV alue(n, o) will be no less than value(b).
This is used for pruning sections of the search tree. In general, the lower the values returned
by optimisticV alue, the greater the efficiency of pruning. At any time, it is possible to prune
any node with an optimistic value that is less than or equal to the best value of a node
explored to date.
OPUSo is able to take advantage of the presence of optimistic values to further optimize
the effect of pruning beyond that obtained solely by maximizing the proportion of the
search space placed under nodes that are immediately pruned. Generalizing a heuristic
used in FSS, nodes with lower optimistic values are given more active operators and thus
have greater proportions of the search space placed beneath them than nodes with higher
optimistic values. This is achieved by the order of processing at Step 9. The rationale
for this strategy is that the lower the optimistic value the higher the probability that the
node and its associated search tree will be pruned before it is expanded. Maximizing the
proportion of the search space located below nodes with low optimistic value maximizes the
proportion of the search space to be pruned and thus not explicitly explored.
Figure 9 illustrates this effect with respect to a simple machine learning tasksearch
for a propositional expression that describes the most target examples and no non-target
examples. The seven search operators each represent conjunction with a specific proposition
male, f emale, single, married, young, mid and old, respectively. Search starts from the
expression anything. A total of 128 expressions may be formed by conjunction of any
combination of these expressions. Twelve objects are defined:
male, single, young, TARGET
male, single, mid, TARGET
male, single, old, TARGET
male, married, young, NON-TARGET
1. For ease of exposition, it will be assumed that optimization means maximization of a value. It would be
trivial to transform the algorithm and discussion to accommodate other forms of optimization.

441

fiWebb

male, married, mid, NON-TARGET
male, married, old, NON-TARGET
female, single, young, NON-TARGET
female, single, mid, NON-TARGET
female, single, old, NON-TARGET
female, married, young, NON-TARGET
female, married, mid, NON-TARGET
female, married, old, NON-TARGET.
Of these objects, the first three are distinguished as targets. The value of an expression
is determined by two functions, negCover and posCover. The negCover of an expression
is the number of non-target objects that it matches. The posCover of an expression is the
number of target objects that it matches. The expression anything matches all objects. The
value of an expression is  if negCover is not equal to zero. Otherwise the value equals
posCover. This preference function avoids expressions that cover any negative objects and
favors, of those expressions that cover no negative objects, those expressions that cover the
most positive objects. The optimistic value of a node equals the posCover of the nodes
expression.
Figure 9 depicts the nine nodes considered by OPUSo for this search task. For each
node the following are listed:
 the expression;
 the number of target and the number of non target objects matched (cover);
 the value;
 the potential value; and
 the operators placed in the nodes set of active operators and hence included in the
search tree below the node.
The search space is traversed as follows. The first node, anything, is expanded, producing its seven children for which values and optimistic values are determined. No node
can be pruned as all have potential values greater than the best value so far encountered.
The active operators are then distributed, maximizing the proportion of the search space
placed below nodes with low optimistic values. Of the two nodes with the highest optimistic
values, male and single, one receives no active operator and the other receives the first as
its sole active operator. One or the other is then expanded. If it is the one with no active
operators, single, no further nodes are generated. Then the other, male, is expanded, generating a single node, male  single, with a value of 3. Immediately this node is generated,
all remaining open nodes can be pruned as none has an optimistic value greater than this
new maximum value, 3.
Note that no nodes can be pruned until the node for malesingle is considered as, up to
that point, no node has been encountered with a lower optimistic value than the best actual
value. Consequently, if the search tree was not distributed in accord with potential value, the
set of active operators for the node male would be {f emale, single, married, young, mid, old}.
Instead of considering a single node when male was expanded, it would be necessary to
442

fiAn Efficient Admissible Algorithm for Unordered Search

male
Cover=3/3
Value=infinity
PotVal=3
AO={single}

male  single
Cover=3/0
Value=3
PotVal=3
AO={}

female
Cover=0/6
Value=infinity
PotVal=0
AO={male, married,
single, young, mid old }
single
Cover=3/3
Value=infinity
PotVal=3
AO={}
anything
Cover=3/9
Value=infinity
PotVal=3
AO={male, female,
married, single,
young, mid, old}

married
Cover=0/6
Value=infinity
PotVal=0
AO={male, single, young,
mid, old }
young
Cover=2/2
Value=infinity
PotVal=2
AO={male, single,
mid, old}
mid
Cover=2/2
Value=infinity
PotVal=2
AO={male, single, old}
old
Cover=2/2
Value=infinity
PotVal=2
AO={male, single}

Figure 9: Effect of pruning when search tree ordered on optimistic value

443

fiWebb

consider six nodes. If the search space was more complex and continued to depth three
or beyond, there would be a commensurate increase in the proportion of the search space
explored unnecessarily.
Note also that the search in this example does not terminate when the goal node is first
encountered, as the system cannot determine that it is a goal node until all other nodes
that might have higher values have been explored or pruned.
5.2.1 The OPUSo Algorithm
OPUSo, the algorithm for achieving the above effect, can be defined as in Figure 10. Note
that optimistic pruning need not be performed at Step 8 as it is performed at Step 10a,
irrespective.
This definition of OPUSo assumes that a single operator cannot be applied more than
once along a single path through the search space. To allow multiple applications of a single
operator, the order of Steps 9a and 9b should be reversed.
The algorithm could also be modified to identify and return all maximal solutions
through a modification similar to that outlined above to allow OPUSs to return all solutions.
It is possible to further improve the performance of OPUSo if there is a lower limit on
an acceptable solution. Then, the objective of the search is to find a highest valued node
so long as that value is greater than a pre-specified minimum. In this case, all nodes whose
potential value is less than or equal to the minimum may also be pruned at Step 10a.
Like OPUSs, OPUSo does not specify the order in which nodes in OPEN should be
expanded (Step 4). Selection of a node with the highest optimistic value will minimize the
size of the search tree. If there is a single node n that optimizes the optimistic value, the
search cannot terminate until n has been expanded. This is because no node with a lower
optimistic value may yield a solution with a value higher than the optimistic value of n.
However, an expansion of n may yield a solution that has a value higher than other candidates optimistic values, allowing those other candidates to be discarded without expansion.
Thus, selecting a single node with the highest optimistic value is optimal with respect to the
number of nodes expanded because it maximizes the number of nodes that may be pruned
without expansion. Where multiple nodes all maximize the optimistic value, at least one
of these must be expanded before the search can terminate (and then the search will only
terminate if expansion of that node leads to a node with a value equal to that optimistic
value.)
In many cases it is more important to consider the number of nodes explored by an
algorithm, rather than the number of nodes expanded. A node is explored if it is evaluated.
Every time a node is expanded, all of its children will be explored. Many of these children
may be pruned, however, and never be expanded. In addition to minimizing the number
of nodes expanded, this form of best-first search will also minimize the number of nodes
explored (within the constraint that where nodes have equal optimistic values it is not
possible to anticipate which one to select in order to minimize the number of nodes explored).
This is due to the strategy that the algorithm employs to distribute operators beneath nodes.
The nodes that OPUSo expands under best-first search will be those with highest optimistic
value. OPUSo always allocates fewer active operators to a node with higher optimistic value
than to a node with lower optimistic value. The number of nodes examined when a node
444

fiAn Efficient Admissible Algorithm for Unordered Search

Algorithm:
1. Initialize a list called OP EN of unexpanded nodes as follows,
(a) Set OP EN to contain one node, the start node s.
(b) Set s.active to the set of all operators, {o1 , o2 , ...on }
(c) Set s.state to the start state.
2. Initialize BEST , the best node examined so far, to s.
3. If OP EN is empty, exit successfully with the solution represented by BEST .
4. Remove from OP EN a node n, the next node to be expanded.
5. Initialize to n.active a set containing those operators that have yet to be examined, called
RemainingOperators.
6. Initialize to {} a set of nodes, called N ewN odes, that will contain the descendants of n that
are not pruned.
7. Generate the children of n by performing the following steps for every operator o in n.active,
(a) Generate n0 , a node for which n0 .state is set to the state formed by application of o to
n.state.
(b) If value(n0 ) is greater than value(BEST )
i. Set BEST to n0 .
ii. Prune the search tree by removing from OP EN all nodes x such that
optimisticV alue(x, x.active) is less than value(BEST ).
(c) Add n0 to N ewN odes.
(d) Set n0 .mostRecentOperator to o.
8. While there is a node n0 in N ewN odes such that pruning rules determine that no sole remaining goal state is accessible from n0 using only operators in RemainingOperators, prune
all nodes in the search tree below n0 from the search tree below n as follows,
(a) Remove n0 from N ewN odes.
(b) Remove n0 .mostRecentOperator from RemainingOperators.
9. Allocate the remaining operators to the remaining nodes by processing each node n0 in
N ewN odes in turn as follows, each time selecting the previously unselected node that minimizes optimisticV alue(n0 , RemainingOperators),
(a) Remove n0 .mostRecentOperator from RemainingOperators.
(b) Set n0 .active to RemainingOperators.
10. Perform optimistic pruning while adding the remaining nodes to OP EN by processing each
node n0 in N ewN odes in turn as follows,
(a) If optimisticV alue(n0 , n0 .active) is greater than value(BEST ),
i. Add n0 to OP EN .
11. Go to Step 3.

Figure 10: The OPUSo algorithm

445

fiWebb

n is expanded equals the number of active operators at n. Hence, the number of nodes
examined for those nodes expanded will be minimized (within the constraints of the use
only of information that can be derived from the current state and the operators that are
active at that state).
However, while this best-first approach minimizes the number of nodes expanded, it may
not be storage optimal due to the large potential storage overheads. If the storage overhead
is of concern, depth-first rather than best-first traversal may be employed, at the cost of a
potential increase in the number of nodes that must be expanded. If depth-first search is
employed, nodes should be added to OP EN by order of optimistic value at Step 10. This
will ensure that nodes open at a single depth will be expanded in a best-first manner, with
the benefits outlined above.
5.2.2 Relation to Previous Search Algorithms
OPUSo can be viewed as an amalgamation of FSS (Narendra & Fukunaga, 1977) with A*
(Hart, Nilsson, & Raphael, 1968). FSS performs branch and bound search in unordered
search spaces, traversing the search space so as to visit each state at most once and dynamically organizing the search tree so as to maximize the proportion of the search space placed
under unpromising operators. However, FSS requires that the values of states in the search
space be monotonically decreasing. That is, the value of a state cannot increase as a result
of an operator application. OPUSo generalizes from FSS by employing both the actual
values of states and optimistic evaluation of nodes in the search tree, in a manner similar
to A*. In consequence, there are only two minor constraints upon the values of states and
the optimistic values of nodes in the search spaces that OPUSo can search. These are the
requirements that
 for at least one goal state g and for any node n, if g lies below a node n in the search
tree, the optimistic value of n be no lower than the value of g; and
 that any and only states of maximal value be goal states.
It follows that OPUSo has wider applicability than FSS.
OPUSo also differs from FSS by integrating pruning mechanisms other than optimistic
pruning into the search process. This facility is crucial when searching large search spaces
such as those encountered in machine learning.
A further innovation of the OPUS algorithms is the use of the restricted set of operators
available at a node in the search tree to enable more focused pruning than would otherwise
be the case. There may be circumstances in which it would be possible to reach a goal
from the state at a node n, but only through application of operators that are not active
at n. The pruning rules are able to take account of the active operators to provide pruning
in this contextpruning that would not otherwise be possible. Similarly, the set of active
operators can be used to calculate a more concise estimate of the optimistic value than
would otherwise be possible.
OPUSo differs from A* in the manner in which it dynamically organizes the search tree
so as to maximize the proportion of the search space placed under unpromising operators.
It also differs from A* in that A* relies upon the value of a node being equivalent to the
446

fiAn Efficient Admissible Algorithm for Unordered Search

sum of costs of the operations that lead to that node, whereas OPUS allows any method
for determining a nodes value.
Rymon (1993) discusses dynamic organization of the search tree during admissible search
through unordered search spaces for the purpose of altering the topology of the data structure (SE-tree) produced. This contrasts with the use of dynamic organization of the search
tree in OPUSo to increase search efficiency.
5.3 OPUS and Non-admissible Search
As was pointed out above, although the OPUS algorithms were designed for admissible
search, if they are applied with non-admissible pruning rules they may also be used for
non-admissible search. This may be useful if efficient heuristic search is required. Most
non-admissible heuristic search strategies embed the heuristics in the search technique. For
example, beam search relies upon the use of a fixed maximum number of alternative options
that are to be considered at any stage during the search. The heuristic is to prune all but
the n best solutions at each stage during search. The precise implications of this heuristic
for a particular search task may be difficult to evaluate. In contrast, the use of OPUS with
non-admissible pruning rules places the non-admissible heuristic in a clearly defined rule
which may be manipulated to suit the circumstances of a particular search problem.
Another feature of OPUSo is that at all stages it has available the best solution encountered to date during the search. This means that the search can be terminated at any
time. When terminated prematurely, the current best solution would be returned on the
understanding that this solution may not be optimal. If the algorithm is to be employed
in this context, it may be desirable to employ best-first search, opening nodes with highest
actual (as opposed to optimistic) value first, on the assumption that this should lead to
early investigation of high valued nodes.
5.4 Complexity and Efficiency Considerations
OPUS ensures that no state is examined more than once (unless identical states can be
formed by different combinations of operator applications), using a similar search tree organization strategy to that of fixed-order search. It differs, however, in that instead of placing
the largest subsection of the search space under the highest ordered operator, the second
largest subsection under the second highest ordered operator, and so on, whenever pruning
occurs, the largest possible proportion of the search space is placed under the pruned node,
and hence is immediately pruned.
If there are n operators active at the node e being expanded, the search tree below and
including that node will contain every combination of any number of those operators (the
application of none of the operators results in e). Thus, the search tree below and including
e will contain 2n nodes. Exactly half of these, 2n1 , will have a label including any single
operator o. OPUS ensures that if any operator o is pruned when a node e is expanded, that
all nodes containing o are removed from the search tree below e and are never examined
(except, of course, the node reached by a single application of o that must be examined in
order to determine that o should be pruned). Thus, the search tree below a node is almost
exactly halved if a single operator can be pruned. Each subsequent operator pruned at
that node reduces the remaining search tree by the same proportion. Thus, the size of the
447

fiWebb

remaining search tree is divided by almost exactly 2p , where p is the number of operators
pruned.
In contrast, the number of nodes pruned under fixed-order search depends upon the
ranking of the operator within the fixed operator ranking scheme. Only for the highest
ranked operator will the same proportion of the search tree be pruned as under OPUS. In
general, when an operator is pruned, only those nodes whose labels include that operator in
combination exclusively with lower ranked operators will be pruned. This effect is illustrated
in Figure 5 in which pruning below {c} removes only {c, d} from the search tree. Thus,
2nr  1, nodes are immediately pruned from the search tree, where n is the number of
operators active at the node being expanded and r is the ranking within those operators of
the operator being pruned, with the highest rank being 1. This contrasts with the 2n1  1
nodes pruned by OPUS.
However, the difference in the number of nodes explored under the two strategies is not
quite as great as this analysis might suggest, as (assuming the availability of a reasonable
optimistic pruning mechanism) fixed-order search can also prune the operator every time
that it is examined deeper in the search tree in combination with higher ranked operators.
Thus, in Figure 5, when they were eventually examined, pruning would occur at nodes
{a, b, c}, {a, c} and {b, c}. Thus, {a, b, c, d}, {a, c, d} and {b, c, d} would also be pruned from
the search tree. In other words, under fixed-order search, if an operator is pruned it will not
be considered in combination with any lower ranked operator, but will be considered with
every combination of any number of higher ranked operators. There are 2r1 combinations
of higher ranked operator. It follows that fixed-order search considers this many more nodes
than OPUS when a single operator is pruned. Thus, for each operator that can be pruned
at a node n, OPUS explores 2r1 less nodes below n than fixed-order search.
As the rank order of the operators pruned will tend to grow as the number of operators
grows, it follows that, in the average case, the advantage accrued from the use of OPUS will
grow exponentially as the number of operators grows. OPUS will tend to have the greatest
relative advantage for the largest search spaces.
Note that OPUS is not always able to guarantee that the maximal possible pruning
occurs as the result of a single pruning action. For example, if OPUS is being used to
search the space of subsets of a set of items, and it can be determined that no superset
of the set s at a node may be a solution, but some items are not active at the current
node, supersets of s that contain the items that are not active may be explored elsewhere
in the search tree. An algorithm that could prune all such supersets could perform more
pruning than OPUS. While it might be claimed that Schlimmers (1993) search method
performs such pruning, it should be recalled that it does not prevent the pruned nodes from
being generated elsewhere in the tree, but rather, ensures that such nodes are pruned once
generated. OPUS, if armed with suitable pruning rules, should also be able to prune such
nodes when encountered. OPUS maximizes the pruning performed within the constraints
of the localized information to which it has access.
However, while the constraint provided by the active operators prevents OPUS from
performing some pruning, it also enables it to perform other pruning that would not otherwise be possible. This is because it is only necessary when considering whether to prune
a node to determine whether nodes that can be reached by active operators may contain a
solution. Thus, to continue the example of subset search, even when supersets of the set at
448

fiAn Efficient Admissible Algorithm for Unordered Search

the current node n are potential solutions, it will still be possible to prune the search tree
below n if all of the supersets that are potential solutions contain items that are not active
at n. Schlimmers (1993) approach does not allow pruning in such a context.
To illustrate this effect, let us revisit the search space examined in Figures 1 to 7. Even
though the search space below {c} has been pruned, optimal pruning cannot take this
into account in its optimistic evaluations of other nodes as there is no mechanism by which
this information can be communicated to the optimistic evaluation function (other than by
actually exploring the space below the node to be evaluated, which defeats the purpose of
optimistic evaluation). For example, when evaluating the optimistic value of the node {a},
the optimistic evaluation function cannot return a different value than would be the case
if {c} had not been pruned. By contrast, the optimistic evaluation function employed by
OPUSo can take account of this by taking the active operators for the current node into
consideration. Such an optimistic evaluation function is described in Section 6.1 below. It
will often be possible to use the information that particular operators are not available in
the search tree below a node to substantially improve the quality of the optimistic evaluation
of that node.
It should also be noted that no algorithm that does not employ backtracking can guarantee that it will minimize the number of nodes expanded under depth-first search. If a
poor node is chosen for expansion under depth-first search, the system is stuck with having
to explore the search space below that node before it can return to explore alternatives. No
algorithm can guarantee against a poor selection unless the optimistic evaluation function
has high enough accuracy to prevent the need for backtracking. It follows that no algorithm
that requires backtracking can guarantee that it will minimize the number of nodes that are
expanded. Thus, OPUS is heuristic with respect to minimizing computational complexity
under depth-first search.
The storage requirements of OPUS will depend upon whether depth, breadth or bestfirst search is employed. If depth-first search is employed, the maximum storage requirement
will be less than the maximum depth of the search tree multiplied by the maximum branching factor. However, if breadth or best-first search is employed, in the worst case, the storage
requirement is exponential. At any stage during the search, the storage requirement is that
of storing the frontier nodes of the search. The number of frontier nodes cannot exceed the
number of leaf nodes in the complete search tree. For search in which no operator may be
applied more than once (subset selection), if there is no pruning, the number of leaf nodes
is 2n1 , where n is the number of operators. This assertion can be justified as follows. If
the order in which operators are considered is invariant, all nodes reached via the last operator considered will be a leaf node. As the search is admissible, the last operator must be
considered with every combination of other operators. There are 2n1 , other combinations
of operators. The order in which operators are considered will not alter the number of
leaf nodes in the absence of pruning. For search in which there is no limit on the number
of applications of a single operator (sub-multiset selection) there is no upper limit on the
potential storage requirements.
Irrespective of the storage requirements, in the worst case OPUS will have to explore
every node in the search space. This will only occur if no pruning is possible during a search.
If operators can only be applied once per solution, the number of nodes in the search space
will equal 2n , where n is the number of operators. Thus, the worst case computational
449

fiWebb

complexity of OPUS is exponential, irrespective of whether depth, breadth or best-first
search is employed.
OPUS is clearly inappropriate, both in terms of computational and, when using breadth
or best-first search, storage requirements, for search problems in which substantial proportions of the search space cannot be pruned. For domains in which substantial pruning is
possible, however, the average case complexity (computational and/or storage) may turn
out to be polynomial. Experimental evidence that this is indeed the case for some machine
learning tasks is presented below in Section 6.3.
5.5 How the Search Efficiency of OPUS Might be Improved
As is noted in Section 5.4, the OPUS algorithms are not always able to guarantee that the
maximum possible amount of pruning is performed. As noted, one restriction upon the
amount of pruning performed is the localization inherent in the use of active operators.
While this localization allows some pruning that would not otherwise be possible, it also
has the potential to restrict the number of supersets of the set of operators at a pruned
node that are also pruned. There may be value in developing mechanisms that enable
such pruning to be propagated beyond the node at which a pruning action occurs and the
sub-tree below that node.
Another aspect of the algorithms that has both positive and negative aspects is the type
of information returned by the pruning mechanisms. These mechanisms allow the pruning
of any branch of the search tree so long as at least one goal is not below that branch. This
contrasts with an alternative strategy of only pruning branches that do not lead to any
goal. The strategy used can be beneficial, as it maximizes the amount of pruning that can
be performed. However, it is always possible that a branch containing a goal that could
be found with little exploration will be pruned in favor of a branch containing a goal that
requires extensive exploration to uncover. There is potential for gain through augmenting
the current pruning mechanisms with means of estimating the search cost of uncovering a
goal beneath each branch in a tree.

6. Evaluating the Effectiveness of the OPUS Algorithms
Theoretical analysis has demonstrated that OPUS will explore fewer nodes than fixed-order
search and that the magnitude of this advantage will increase as the size of the search
space increases. However, the precise magnitude of this gain will depend upon the extent
and distribution within the search tree of pruning actions. Of further interest, there are a
number of distinct elements to each of the OPUS algorithms, includingoptimistic pruning;
other pruning (pruning in addition to optimistic pruning); dynamic reorganization of the
search tree; and maximization of the proportion of the search space placed under nodes with
low optimistic value. The following experiments evaluate the magnitude of the advantage
to OPUS obtained for real world search tasks and explore the relative contribution of each
of the distinct elements of the OPUS algorithms.
To this end, OPUSo was applied to a class of real search tasksfinding pure conjunctive
expressions that maximize the Laplace accuracy estimate with respect to a training set of
preclassified example objects. This is, for example, the search task that CN2 purports to
heuristically approximate (Clark & Niblett, 1989) when forming the disjuncts of a disjunc450

fiAn Efficient Admissible Algorithm for Unordered Search

tive classifier. Machine learning systems have employed OPUSo in this manner to develop
rules for inclusion both in sets of decision rules (Webb, 1993) and in decision lists (Webb,
1994b). (The current experiments were performed using the Cover learning system, which,
by default, performs repeated search for pure conjunctive classifiers within a CN2-like covering algorithm that develops disjunctive rules. This more extended search for disjunctive
rules was not used in the experiments, as it makes it difficult to compare alternative search
algorithms. This is because, if two alternative algorithms find different pure conjunctive
rules for the first disjunct, their subsequent search will explore different search spaces.)
Numerous efficient admissible search algorithms exist for developing classifiers that are
consistent with a training set of examples. The two classic algorithms for this purpose are
the least generalization algorithm (Plotkin, 1970) and the version space algorithm (Mitchell,
1977). The least generalization algorithm finds the most specialized class description that
covers all objects in a training set containing only positive examples. The version space algorithm finds all class descriptions that are complete and consistent with respect to a training
set of both positive and negative examples. Hirsh (1994) has generalized the version space
algorithm to find all class descriptions that are complete and consistent to within defined
bounds of the training examples. The least generalization and version space algorithms will
usually require a strong inductive bias in the class description language (restriction on the
types of class descriptions that will be considered) if they are to find useful class descriptions
(Mitchell, 1980). SE-tree-based learning (Rymon, 1993) demonstrates admissible search for
a set of consistent class descriptions within more complex class description languages than
may usefully be employed with the least generalization or version space algorithms. Oblow
(1992) describes an algorithm that employs admissible search for pure conjunctive terms
within a heuristic outer search for k-DNF class descriptions that are consistent with the
training set.
However, for many learning tasks it is desirable to consider class descriptions that are
inconsistent with the training set. One reason for this is that the training set may contain
noise (examples that are inaccurate). Another reason is that it may not be possible to accurately describe the target class in the available language for expressing class descriptions.
In this case it is necessary to consider approximations to the target class. A further reason
is that the training set may contain insufficient information to reliably determine the exact
class description. In this case, the best solution may be an approximation that is known to
be incorrect but for which there is strong evidence that the level of error is low.
Both Clearwater and Provost (1990) and Segal and Etzioni (1994) use admissible fixedorder search to explore classifiers that are inconsistent with the training set. However, the
admissible search of Clearwater and Provost (1990) is not computationally feasible for large
search spaces. Segal and Etzioni (1994) bound the depth of the search space considered in
order to maintain computational tractability. Smyth and Goodman (1992) use optimistic
pruning to search for optimal rules, but do not structure their search to ensure that states
are not searched multiple times. No other previous admissible search algorithm has been
employed in machine learning to find classifiers that are inconsistent with the training
set and maximize an arbitrary preference function. The following experiments seek to
demonstrate that such search is feasible using OPUS.
Where it is allowed that a class description may be inconsistent with the training set,
it is helpful to employ an explicit preference function. Such a function is applied to a
451

fiWebb

class description and returns a measure of its desirability. This evaluation will usually take
account of how well the description fits that training set and may also include a bias toward
particular types of class descriptions, for example, a preference for syntactic simplicity. Such
a preference function expresses an inductive bias (Mitchell, 1980).
OPUSo may be employed for admissible search in such contexts, provided a search space
can be defined that may be traversed by a finite number of unordered search operators.
For example, OPUSo may be employed to search for a class description in a language of
pure-conjunctive descriptions by examining a search space starting with the most general
possible class description true and employing search operators, each of which has the effect
of conjoining a specific clause to the current description. Such search may be performed
with an arbitrary preference function, provided appropriate optimistic evaluation functions
can be defined.
The next section describes experiments in which OPUSo was applied in this manner.
6.1 The Search Task
The pure conjunctive expressions consisted of conjunctions of clauses of the form attribute 6=
value. For attributes with more than two values, such a language is more expressive than
a language allowing only conjunctions of clauses of the form attribute = value. Indeed,
it has equivalent expressiveness to a language that supports internal disjunction. For example, with respect to an attribute a with the values x, y and z, a language restricted to
conjunctions of equality expressions cannot express a 6= x, whereas a language restricted to
conjunctions of inequality expressions can express a = x using the expression a 6= y  a 6= z.
In internal disjunctive (Michalski, 1984) terms, a 6= x is equivalent to a = y or z.
It should be noted that
 For attributes with more than two values the search space for conjunctions of inequality expressions is far larger than the search space for conjunctions of equality
expressions. For each attribute, the size of the search space is multiplied by 2n for the
former and by n + 1 for the latter, where n is the number of values for the attribute.
 The software employed in this experimentation can also be used to search the smaller
search spaces of equality expressions with the same effects as are demonstrated in the
following experiments.
Search starts from the most general expression, true. Each operator performs conjunction of the current expression with a term A 6= v, where A is an attribute and v is any
single value for that attribute.
The Laplace (Clark & Boswell, 1991) preference function was used to determine the goal
of the search. This function provides a conservative estimate of the predictive accuracy of
a class description, e. It is defined as
value(e) =

posCover(e) + 1
posCover(e) + negCover(e) + noOf Classes

where posCover(e) is the number of positive objects covered by e; negCover(e) is the
number of negative objects covered by e; and noOf Classes is the number of classes for the
learning task.
452

fiAn Efficient Admissible Algorithm for Unordered Search

The Laplace preference function trades-off accuracy against generality. It favors class
descriptions that cover more positive objects over class descriptions that cover fewer, and
favors class descriptions for which a lower proportion of the cover is negative over those for
which it is higher. In the following study, the Laplace preference function was employed
with a pruning mechanism at Step 10a of the OPUSo algorithm that pruned sections of
the search space with optimistic values less than or equal to the value of a class description
that covered no objects. If there was no solution with a value higher than that obtained by
a class description that covered no objects, no rule was developed for the class.
The optimistic value function is derived from the observation that the cover of specializations of an expression must be subsets of the cover of that expression. Thus, specializations
of an expression may not cover more positive objects, but may cover fewer negative objects
than are covered by the original expression. As the Laplace preference function is maximized when positive cover is maximized and negative cover is minimized, no specialization
of the expression at a node may have higher value than that obtained with the positive
cover of that expression and the smallest negative cover within the sub-tree below the node.
The smallest negative cover within a sub-tree below a node n is obtained by the expression
formed by applying all operators active at n to the expression at n.
Other pruning can be performed through the application of cannotImprove(n1 , n2 ), a
boolean function that is true of any two nodes n1 and n2 in the search tree such that n2 is
either the child or sibling of n1 and no specialization of n2 may have a higher value than
the highest value in the search tree below n1 inclusive but excluding the search tree below
n2 . This function may be defined as
cannotImprove(x, y)  neg(x)  neg(y)  pos(x)  pos(y)
where neg(n) denotes the set of negative objects covered by the description for node n
and pos(n) denotes the set of positive objects covered by the description for node n. If
cannotImprove(n1 , n2 ) then search below n2 cannot lead to a higher valued result than can
be obtained by search through specializations of n1 excluding nodes in the search space
below n2 . This can be shown where n1 is the parent and n2 is the child node as follows. If
n1 is the parent of n2 then the expression for n2 must be a specialization of the expression
for n1 and all operators available for n2 must be available for n1 . For any expression g
and its specialization, s, if neg(g)  neg(s) then neg(g) = neg(s) (as specialization can
only decrease cover). It follows that for any further specialization of n2 , n3 , obtained by
applications of operators O, there must be a specialization of n1 obtained by application
of operators O, n4 , which is a generalization of n3 and which has identical negative cover
to n3 . As n4 is a generalization of n3 , it must cover all positive objects covered by n3 .
Therefore, n4 must have equal or greater positive cover and equal negative cover to n3 and
consequently must have an equal or greater value. It follows that it must be possible to
reach from n1 a node of at least as great a value as the greatest valued node below n2
without applying the operator that led from n1 to n2 .
Next we consider the case where n1 and n2 are siblings. It follows from the definition
of cannotImprove that neg(n1 )  neg(n2 ) and pos(n1 )  pos(n2 ). Let the operators o1
and o2 be those that led from the parent node p to n1 and n2 , respectively. It follows that
o2 cannot exclude any negative objects from expressions below p not also excluded by o1
and that o1 cannot exclude any positive objects from expressions below p not also excluded
453

fiWebb

Table 1: Summary of experimental data sets

Domain
Audiology
House Votes 84
Lenses
Lymphography
Monk 1
Monk 2
Monk 3
Multiplexor (F11)
Mushroom
Primary Tumor
Slovenian Breast Cancer
Soybean Large
Tic Tac Toe
Wisconsin Breast Cancer

Description
Medical diagnosis
Predict US Senators political
affiliation from voting record.
Spectacle lens prescription.
Medical diagnosis.
Artificial data.
Artificial data.
Artificial data.
Artificial data, requiring disjunctive concept description.
Identify poison mushrooms.
Medical diagnosis.
Medical prognosis.
Botanical diagnosis.
Identify won or lost positions.
Medical diagnosis.

#
Values
162
48

#
Objects
226
435

#
Classes
24
2

9
60
17
17
17
22

24
148
556
601
554
500

3
4
2
2
2
2

126
42
57
135
27
91

8124
339
286
307
958
699

2
22
2
19
2
2

by o2 . Therefore, application of o2 below n1 will have no effect on the negative cover of
the expression but may reduce positive cover. For any expression e reached below n2 by a
sequence of operator applications O, application of O to n1 cannot result in an expression
with lower positive or higher negative cover than that of e.
The cannotImprove function was employed to prune nodes at Step 8 of the OPUSo
algorithm.
6.2 Experimental Method
This search was performed on fourteen data sets from the UCI repository of machine learning
databases (Murphy & Aha, 1993). These were all the data sets from the repository that the
researcher could at the time of the experiments identify as capable of being readily expressed
as a categorical attribute-value learning tasks. These fourteen data sets are described in
Table 1. The number of attribute values (presented in column 3) treats missing values as
distinct values. The space of class descriptions that OPUS considers for each domain (and
hence the size of the search space examined for each pure conjunctive rule developed) is
2n , where n is the number of attribute values. Thus, for the Audiology domain, for each
class description developed, the search space was of size 2162 . Columns 4 and 5 present the
number of objects and number of classes represented in the data set, respectively.
The search was repeated once for each class in each data set. For each such search, the
objects belonging to the class in question were treated as the positive objects and all other
objects in the data set were treated as negative objects. This search was performed using
454

fiAn Efficient Admissible Algorithm for Unordered Search

each of the following search methodsOPUSo; OPUSo without optimistic pruning; OPUSo
without other pruning; OPUSo without optimistic reordering; and fixed-order search, such
as performed by Clearwater and Provost (1990), Rymon (1993), Schlimmer (1993), Segal
and Etzioni (1994) and Webb (1990).
Optimistic pruning was disabled by removing the condition from Step 10a of the OPUSo
algorithm. In other words, Step 10(a)i was always performed.
Other pruning was disabled by removing Step 8 from the OPUSo algorithm.
Optimistic reordering was disabled by changing Step 9 to process each node in a predetermined fixed-order, rather than in order by optimistic value. Under this treatment, the
topology of the search tree is organized in a fixed-order, but operators that are pruned at
a node are removed from consideration in the entire subtree below that node.
Fixed-order search was emulated by disabling Step 8b and disabling optimistic reordering, as described above.
All of the algorithms are to some extent under-specified. OPUSo, no optimistic pruning
and no other pruning are all leave unspecified the order in which operators leading to nodes
with equal optimistic values should be considered at Step 9. Such ambiguities were resolved
in the following experiments by ordering operators leading to nodes with higher actual
values first. Where two operators tied on both optimistic and actual values, the operator
mentioned first in the names file that describes the data was selected first.
No optimistic reordering and fixed-order search both leave unspecified the fixed-order
that should be employed for traversing the search space. As fixed-order search is representative of previous approaches to unordered search employed in machine learning, and thus
it is important to obtain a realistic evaluation of its performance, ten alternative random
orders were generated and all employed for each fixed-order search task. While, due to
the high variability in performance under different orderings, it would have been desirable
to explore more than ten alternative orderings, this was infeasible due to the tremendous
computational demands of this algorithm. The comparison with no optimistic reordering
was considered less crucial, as it is used solely to evaluate the effectiveness of one aspect
of the OPUSo algorithm, and thus, due to the tremendous computational expense of this
algorithm, a single fixed ordering was used, employing the order in which attribute values
are mentioned in the names file.
All of the algorithms leave unspecified the order in which nodes with equal optimistic
values should be selected from OP EN under best-first search, or directly expanded under
depth-first search. Under best first search nodes with equal optimistic values were removed
from OP EN in a last-in-first-off order. Under depth-first search, nodes with equal optimistic
value were expanded in the same order as was employed for allocating operators at Step 9.
Note that the fixed-order search and OPUSo with disabled optimistic reordering conditions both used optimistic and other pruning. Note also that while fixed-order search
ordered the topology of the search tree in the manner depicted in Figure 4, it explored that
tree in either a best or depth-first manner.
6.3 Experimental Results
Tables 2 and 3 present the number of nodes examined by each search in this experiment.
For each data set the total number of nodes explored under each condition is indicated.
455

fiWebb

Table 2: Number of nodes explored under best-first search

Data set
Audiology
House Votes 84
Lenses
Lymphography
Monk 1
Monk 2
Monk 3
Multiplexor (F11)
Mushroom
Primary Tumor
Slovenian B. C.
Soybean Large
Tic Tac Toe
Wisconsin B. C.

OPUSo
7,044
533
41
1,142
357
4,326
281
2,769
391
10,892
17,418
8,304
2,894
447,786

No
optimistic
pruning

661
176
1,143
9,156
6,578
25,775
96,371
392
10,893
4,810,129
8338
4,222,641


No
other
pruning
24,199
554
41
1,684
371
4,335
281
2,769
788
13,137
32,965
21,418
2,902
1,159,011

No
optimistic
reordering

355,040
38
658,335
925
10,012
682
4,932
233,579
4,242,978

21,551,436
16,559


Fixed-order
(mean)

1,319,911
64
2,251,652
788
5,895
656
4,948

29,914,840
42,669,822

16,471


 Execution terminated after exceeding virtual memory limit of 250 megabytes.
 Only one of ten runs completed.

For fixed-order search, the mean of all ten runs is presented. Tables 4 and 5 present for
fixed-order search the number of runs that completed successfully, the minimum number of
nodes examined by a successful run, the mean number of nodes examined by successful runs
(repeated from Tables 2 and 3) and the standard deviations for those runs. Every node
generated at Step 7a is counted in the tally of the number of nodes explored. A hyphen
() indicates that the search could not be completed as the number of open nodes made
the system exceed a predefined virtual memory limit of 250 megabytes. An asterisk (*)
indicates that the search was terminated due to exceeding a pre-specified compute time
limit of twenty-four CPU hours. (For comparison, the longest CPU time taken for any data
set by OPUSo was sixty-seven CPU seconds on the Wisconsin Breast Cancer data under
depth-first search.)
It should be noted that one pure conjunctive rule was developed for each class. As a
separate search was performed for each rule, the number of searches performed equals the
number of classes. Thus, for the Audiology data using best-first search OPUSo explored
just 7,044 nodes to perform 24 admissible searches of the 2162 node search space.
For only two search tasks does OPUSo with best-first search explore more nodes than an
alternative. For the Lenses data, OPUSo explores 41 nodes while no optimistic reordering
explores 38. For the Monk 2 data, OPUSo explores 4,326 nodes while the best of ten fixedorder runs with different random fixed orders explores 4,283 nodes. It is possible that these
outcomes have arisen from situations where two sibling nodes share the same optimistic
value. In such a case, if two approaches each select different nodes to expand first, one
456

fiAn Efficient Admissible Algorithm for Unordered Search

Table 3: Number of nodes explored under depth-first search

Data set
Audiology
House Votes 84
Lenses
Lymphography
Monk 1
Monk 2
Monk 3
Multiplexor (F11)
Mushroom
Primary Tumor
Slovenian B. C.
Soybean Large
Tic Tac Toe
Wisconsin B. C.

OPUSo
7,011
568
38
1,200
364
16,345
289
2,914
386
18,209
30,647
9,562
3,876
465,058

No
optimistic
pruning
*
17,067,302
513
39,063,303
54,218
85,425
63,057
188,120
*
34,325,234
172,073,241
*
11,496,736
*

No
other
pruning
17,191
596
38
1,825
378
16,427
289
2,914
761
23,668
61,391
23,860
4,010
1,211,211

No
optimistic
reordering
3,502,475
10,046
38
728,276
980
12,879
588
6,961
1,562,006
3,814,422
271,328,080
17,138,467
93,521
*

Fixed-order
(mean)
*
3,674,418
66
22,225,745
1,348
12,791
1,236
6,130
132,107,513
31,107,648
308,209,464
*
110,664
*

* Execution terminated after exceeding the 24 CPU hour limit.
 Only three of ten runs completed.

Table 4: Number of nodes explored under best-first fixed-order search
Data set
Audiology
House Votes 84
Lenses
Lymphography
Monk 1
Monk 2
Monk 3
Multiplexor (F11)
Mushroom
Primary Tumor
Slovenian B. C.
Soybean Large
Tic Tac Toe
Wisconsin B. C.

Runs
0
10
10
10
10
10
10
10
0
10
1
0
10
0

Minimum

451,038
51
597,842
463
4,283
527
4,210

10,552,129
42,669,822

8,046


Mean

sd


1,319,911
64
2,251,652
788
5,895
656
4,948

29,914,840
42,669,822

16,471



624,957
9
1,454,583
225
931
110
364

12,390,146
0

5,300


 Execution terminated for all ten runs after exceeding the
virtual memory limit of 250 megabytes.

457

fiWebb

Table 5: Number of nodes explored under depth-first fixed-order search
Data set
Audiology
House Votes 84
Lenses
Lymphography
Monk 1
Monk 2
Monk 3
Multiplexor (F11)
Mushroom
Primary Tumor
Slovenian B. C.
Soybean Large
Tic Tac Toe
Wisconsin B. C.

Runs
0
10
10
10
10
10
10
10
3
10
10
0
10
0

Minimum
*
1,592,391
50
484,694
553
9,274
627
4,467
105,859,320
10,458,421
110,101,761
*
49,328
*

Mean

sd

*
3,674,418
66
22,225,745
1,348
12,791
1,236
6,130
132,107,513
31,107,648
308,209,464
*
110,664
*

*
2,086,159
12
27,250,834
922
2,686
891
1,164
22,749,211
14,907,744
303,800,659
*
65,809
*

* Execution terminated for all ten runs after exceeding the
24 CPU hour limit.

may turn out to be a better choice than the other, leading to the exploration of fewer
nodes. To test the plausibility of this explanation, OPUSo was run again on the Lenses
data set with Step 8 altered to ensure that where two siblings have equal optimistic value
they are ordered in the same order as was employed with no optimistic reordering. This
resulted in the exploration of just 36 nodes, fewer than any alternative. When OPUSo and
fixed-order were run with fixed-order using the order of attribute declaration in the data
file to determine operator order and OPUSo using the same order to order siblings with
equal optimistic values, the numbers of nodes explored for the Monk 2 data were 4,302 for
OPUSo and 8,812 for fixed-order search.
It is notable that this effect is only apparent for very small search spaces. This is
significant because it suggests that there is only an effect of small magnitude resulting from
a poor choice of node to expand when two nodes have equal optimistic value. This is to
be expected. Consider the case where there are two nodes n1 and n2 with equal highest
optimistic value, v, but n1 leads to a goal whereas n2 does not. If n2 is expanded first, so
long as no child of n2 has an optimistic value greater than or equal to v, the next node to
be expanded will be n1 , as n1 will now be the node with the highest optimistic value. (If a
child of n2 has an optimistic value greater than v, the optimistic evaluation function cannot
be very good, as the fact that n2 had an optimistic value of v means that no node below n2
can have a value greater than v.) Thus, the number of unnecessary node expansions due to
this effect can never exceed the number of times that nodes with equal highest optimistic
values are encountered during the search.
In contrast to the case with best-first search, as discussed in Section 5.4, OPUS is only
heuristic with respect to minimizing the number of nodes expanded under depth-first search.
458

fiAn Efficient Admissible Algorithm for Unordered Search

Nonetheless, for only one search task, the Monk 2 data set, does OPUSo explore more
nodes under depth-first search (16,345) than an alternative (both no optimistic reordering
and fixed-order search that explore 12,879 and 12,791 nodes respectively). These results
demonstrate that this heuristic is not optimal for this data. It should be noted, however,
that the single exception for depth-first search again occurs only for a relatively small search
space. This suggests that efficient exploration of the search space below a poor choice of
node can do much to minimize the damage done by that poor choice, even when there is
no backtracking as is the case for depth-first search.
For five data sets (House Votes 84, Lymphography, Mushroom, Primary Tumor and
Soybean Large), disabling optimistic pruning has little effect under best-first search. Disabling optimistic pruning always has large effect under depth-first search. Under best-first
search the smallest increase caused by disabling optimistic pruning is an increase of just
one node for both the Lymphography and Mushroom data sets. Of those data sets for
which it was possible to complete the search without optimistic pruning, the biggest effect
was an almost 1,500 fold increase in the number of nodes explored for the Tic Tac Toe
data. Under depth-first search, of those data sets for which processing could be completed
without optimistic pruning, the smallest increase was a five-fold increase for the Monk 2
data and the largest increase was a 30,000 fold increase for the Lymphography data.
For seven data sets (House Votes 84, Lenses, Monk 1, Monk 2, Monk 3, F11 Multiplexor,
Tic Tac Toe) disabling other pruning had little or no effect under best-first or depth-first
search. The largest effects are 2.5 fold increases for the Soybean Large and Wisconsin
Breast Cancer data sets under best-first search and for the Audiology, Soybean Large and
Wisconsin Breast Cancer data sets under depth-first search.
From these results it is apparent that while there are some data sets for which each
pruning technique has little effect (so long as the other is also employed), there are also
data sets for which other pruning more than halves the amount of the search space explored
and data sets for which optimistic pruning reduces the amount of the search space explored
to thousandths of that which would otherwise be explored.
The effect of optimistic reordering was also highly variable. For two search tasks (bestfirst search for the Lenses data set and depth-first search for the Monk 2 data set) its use
actually resulted in a slight increase in the number of nodes explored. This is discussed
above. In many cases, however, the effect of disabling optimistic reordering was far greater
than that of disabling optimistic pruning. Processing could not be completed without
optimistic reordering for three of the best-first search tasks and one of the depth-first search
tasks. Of those tasks for which search could be completed, the largest effect for best-first
search was a 2,500 fold increase in the number of nodes explored for the Soybean Large data.
Under depth-first search, of those tasks for which search could be completed, the largest
effect was an 8,000 fold increase for the Slovenian Beast Cancer data. While it would be
desirable to evaluate the effect of alternative fixed-orderings of operators on these results,
it seems that optimistic reordering is critical to the general success of the algorithm.
For all but one data set (the Monk 2 data under depth-first search), fixed-order search
on average explores substantially more nodes than OPUSo. It was asserted in Section 5.4
that the average case advantage from the use of OPUSo as opposed to fixed-order search
will tend to grow exponentially as the number of search operators increases. The number
of search operators for the search tasks above equal the number of attribute values in the
459

fiLog Advantage

Webb

20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
-1













 best-first search
 depth-first search


 
  

10 20 30 40 50 60 70 80 90 100 110 120 130 140
Search Space Size

Figure 11: Plot of difference in nodes explored by fixed-order and OPUSo search against
search space size.

corresponding data sets. Analysis of Tables 2 and 3 reveals that the relative advantage
to OPUSo for the data sets with fewest attribute values (Lenses, Monk 1, 2 and 3, and
F11 Multiplexor) is approximately a two-fold reduction in the number of nodes examined.
As the number of attribute values increases, so does the relative advantage. For the four
data sets with the greatest number of attribute values (Audiology, Mushroom, Soybean
Large and Wisconsin Breast Cancer) in only one case (depth-first search of the Mushroom
data) does the fixed-order search terminate. In this one case, OPUSo enjoys a 350,000-fold
advantage. These results lend credibility to the claim that OPUSos average case advantage
over fixed-order search is exponential with respect to the size of the search space. This is
illustrated in Figure 11. In this figure, for searches for which fixed-order search terminated
within the resource constraints, the size of the search space is plotted against log2 (f /o)
where f is the number of nodes explored by fixed-order search and o is the number of nodes
explored by OPUSo.
It seems clear from these results that admissible fixed-order search is not practical for
many of these search tasks within the scope of current technology.
It is interesting to observe that under best-first search, for all of the four artificial data
sets (Monk 1, Monk 2, Monk 3 and F11 Multiplexor) fixed-order search often explores
slightly fewer nodes than OPUSo with optimistic reordering disabled. The difference between these two types of search is that the latter deletes pruned operators from the sets
of active operators under higher ordered operators whereas the former does not. Thus
the latter prunes more nodes from the search tree with each pruning operation. It seems
460

fiAn Efficient Admissible Algorithm for Unordered Search

counter-intuitive that this increased pruning should sometimes lead to the exploration of
more nodes. To understand this effect it is necessary to recall that other pruning can prune
solutions from the search tree so long as there are alternative solutions available. For the
artificial data sets in question, retaining alternative solutions in the search tree in some cases
leads to a slight increase in search efficiency as the alternative can be encountered earlier
than the first solution. Despite this minor advantage for a number of artificial data sets to
fixed-order search over OPUSo with optimistic reordering disabled, the latter enjoys a large
advantage for all other data sets for which processing could be completed. For the House
Votes 84 data, fixed-order search explores over 3.5 times as many nodes under best-first
search and over 350 times as many under depth-first search.
It can be seen that there is some reason to believe that the the average case number
of nodes explored by OPUSo is only polynomial with respect to the search space size for
these machine learning search tasks. The numbers of nodes explored for the three largest
search spaces are certainly not suggestive of an exponential explosion in the numbers of
nodes examined (Audiology2162 nodes in the search space: 7,044 and 7,011 nodes examined. Soybean Large2135 nodes in the search space: 8,304 and 9,562 nodes examined.
Mushroom2126 nodes in the search space: 391 and 386 nodes examined.)
It is interesting that there is little difference in the number of nodes explored by OPUSo
using either best or depth-first search for most data sets. Surprisingly, slightly fewer nodes
are explored by depth-first search for three of the data sets (Audiology, Lenses and Mushroom). This will be for similar reasons to those presented above in the context of the occasional slight advantage enjoyed by fixed-order search over OPUSo with optimistic reordering
disabled. In some cases depth-first search fortuitously encounters alternative solutions to
those found by best-first search. To evaluate the plausibility of this explanation, OPUSo
was run on the three data sets in question using the fixed-order ordering to order operators
with equal optimistic values. The resulting numbers of nodes explored were Audiology:
6678, Lenses: 36 and Mushroom: 385. As can be seen, these numbers are in all cases lower
than the numbers of nodes explored under depth-first search. As is the case when OPUSo
was outperformed by other best-first strategies, this effect appears to be of small magnitude
and thus is only significant where small numbers of nodes need to be explored. For four
of the data sets depth-first search explores substantially more nodes than best-first search
(Slovenian Breast Cancer, 75%; Monk 2, 275%; Primary Tumor, 67%; and Tic Tac Toe,
33%).
6.4 Summary of Experimental Results
The experiments demonstrate that admissible search for pure conjunctive classifiers is feasible using OPUSo for the types of learning task contained in the UCI repository.
They also support the theoretical findings that OPUSo will in general explore fewer
nodes than fixed-order search and that the magnitude of this advantage will tend to grow
exponentially with respect to the size of the search space.
Optimistic pruning and other pruning are both demonstrated to individually provide
large decreases in the number of nodes explored for some search spaces but to have little
effect for others. Optimistic reordering is demonstrated to have a large impact upon the
number of nodes explored.
461

fiWebb

The results with respect to the search of the largest search spaces suggest that the
average case complexity of the algorithm is less than exponential with respect to search
space size.

7. Summary and Future Research
The OPUS algorithms have potential application in many areas of endeavor. They can
be used to replace admissible search algorithms for unordered search spaces that maintain
explicit lists of pruned nodes, such as currently used in ATMS (de Kleer, 1986). They
may also support admissible search in a number of application domains, such as learning
classifiers that are inconsistent with a training set, that have previously been tackled by
heuristic search.
In addition to their applications for admissible search, the OPUS algorithms may also be
used for efficient non-admissible search through the application of non-admissible pruning
rules. The OPUSo algorithm is also able to return a solution if prematurely terminated at
any time, although this solution may be non-optimal.
The availability of admissible search is an important step forward for machine learning
research. While the studies in this paper have employed OPUSo to optimize the Laplace
preference function, the algorithm could be used to optimize any learning bias. This means
that for the first time it is possible to isolate the effect of an explicit learning bias from
any implicit learning bias that might be introduced by a heuristic search algorithm and its
interaction with that explicit bias.
The application of OPUSo to provide admissible search in machine learning has already
proved to be productive. Webb (1993) used OPUSo to demonstrate that heuristic search
that fails to optimize the Laplace accuracy estimate within a covering algorithm frequently
results in the inference of better classifiers than found by admissible search that does optimize this preference function. It was to explain this result that Quinlan and Cameron-Jones
(1995) developed their theory of oversearching.
The research reported herein has demonstrated that OPUS can provide efficient admissible search for pure conjunctive classifiers on all categorical attribute-value data sets in the
UCI repository. It would be interesting to see if the techniques can be extended to more
powerful machine learning paradigms such as continuous attribute-value and first-order logic
domains.
The research has also demonstrated the power of pruning. This issue has been given
scant attention in the context of search for machine learning. Although it is presented here
in the context of admissible search, the pruning rules presented are equally applicable to
heuristic search. The development of these and other pruning rules may prove important
as machine learning tackles ever more complex search spaces.
OPUS provides efficient admissible search in unordered search spaces. When creating
a machine learning system it is necessary to consider not only what to search for (the
explicit learning biases) but also how to search for it (appropriate search algorithms). It
has been assumed previously that such algorithms must necessarily be heuristic techniques
for approximating the desired explicit biases. Admissible search decouples these two issues
by removing confounding factors that may be introduced by the search algorithm. By
guaranteeing that the search uncovers the defined target, admissible search makes it possible
462

fiAn Efficient Admissible Algorithm for Unordered Search

to systematically study explicit learning biases. By supporting efficient admissible search,
OPUS for the first time brings to machine learning the ability to clearly and explicitly
manipulate the precise inductive bias employed in a complex machine learning task.

Acknowledgements
This research has been supported by the Australian Research Council. I am grateful to
Riichiro Mizoguchi for pointing out the potential for application of OPUS in truth maintenance. I am also grateful to Mike Cameron-Jones, Jon Patrick, Ron Rymon, Richard
Segal, Jason Wells, Leslie Wells and Simon Yip for numerous helpful comments on previous
drafts of this paper. I am especially indebted to my anonymous reviewers whose insightful,
extensive and detailed comments greatly improved the quality of this paper.
The Breast Cancer, Lymphography and Primary Tumor data sets were provided by the
Ljubljana Oncology Institute, Slovenia. Thanks to the UCI Repository, its maintainers,
Patrick Murphy and David Aha, and its donors, for providing access to the data sets used
herein.

References
Buchanan, B. G., Feigenbaum, E. A., & Lederberg, J. (1971). A heuristic programming
study of theory formation in science. In IJCAI-71, pp. 4050.
Clark, P., & Boswell, R. (1991). Rule induction with CN2: Some recent improvements. In
Proceedings of the Fifth European Working Session on Learning, pp. 151163.
Clark, P., & Niblett, T. (1989). The CN2 induction algorithm. Machine Learning, 3,
261284.
Clearwater, S. H., & Provost, F. J. (1990). RL4: A tool for knowledge-based induction. In
Proceedings of Second Intl. IEEE Conf. on Tools for AI, pp. 2430 Los Alamitos, CA.
IEEE Computer Society Pres.
de Kleer, J. (1986). An assumption-based TMS. Artificial Intelligence, 28, 127162.
de Kleer, J., Mackworth, A. K., & Reiter, R. (1990). Characterizing diagnoses. In Proceedings AAAI-90, pp. 324330 Boston, MA.
Hart, P., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination
of minimum cost paths. IEEE Transactions on System Sciences and Cybernetics,
SSC-4 (2), 100107.
Hirsh, H. (1994). Generalizing version spaces. Artificial Intelligence, 17, 546.
Lawler, E. L., & Wood, D. E. (1966). Branch and bound methods: A survey. Operations
Research, 149, 699719.
Michalski, R. S. (1984). A theory and methodology of inductive learning. In Michalski,
R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning: An Artificial
Intelligence Approach, pp. 83129. Springer-Verlag, Berlin.
463

fiWebb

Mitchell, T. M. (1977). Version spaces: A candidate elimination approach to rule learning.
In Proceedings of the Fifth International Joint Conference on Artificial Intelligence,
pp. 305310.
Mitchell, T. M. (1980). The need for biases in learning generalizations. Technical report
CBM-TR-117, Rutgers University, Department of Computer Science, New Brunswick,
NJ.
Moret, B. M. E., & Shapiro, H. D. (1985). On minimizing a set of tests. SIAM Journal on
Scientific and Statistical Computing, 6 (4), 9831003.
Murphy, P., & Aha, D. (1993). UCI repository of machine learning databases. [Machinereadable data repository]. University of California, Department of Information and
Computer Science, Irvine, CA.
Murphy, P., & Pazzani, M. (1994). Exploring the decision forest: An empirical investigation
of Occams Razor in decision tree induction. Journal of Artificial Intelligence Research,
1, 257275.
Narendra, P., & Fukunaga, K. (1977). A branch and bound algorithm for feature subset
selection. IEEE Transactions on Computers, C-26, 917922.
Nilsson, N. J. (1971). Problem-solving Methods in Artificial Intelligence. McGraw-Hill, New
York.
Oblow, E. M. (1992). Implementing Valiants learnability theory using random sets. Machine Learning, 8, 4573.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving.
Addison-Wesley, Reading, Mass.
Plotkin, G. D. (1970). A note on inductive generalisation. In Meltzer, B., & Mitchie, D.
(Eds.), Machine Intelligence 5, pp. 153163. Edinburgh University Press, Edinburgh.
Quinlan, J. R., & Cameron-Jones, R. M. (1995). Oversearching and layered search in
empirical learning. In IJCAI95, pp. 10191024 Montreal. Morgan Kaufmann.
Reiter, R. (1987). A theory of diagnosis from first principles. Artificial Intelligence, 32,
5795.
Rymon, R. (1992). Search through systematic set enumeration. In Proceedings KR-92, pp.
268275 Cambridge, MA.
Rymon, R. (1993). An SE-tree based characterization of the induction problem. In Proceedings of the 1993 International Conference on Machine Learning San Mateo, Ca.
Morgan Kaufmann.
Schlimmer, J. C. (1993). Efficiently inducing determinations: A complete and systematic
search algorithm that uses optimal pruning. In Proceedings of the 1993 International
Conference on Machine Learning, pp. 284290 San Mateo, Ca. Morgan Kaufmann.
464

fiAn Efficient Admissible Algorithm for Unordered Search

Segal, R., & Etzioni, O. (1994). Learning decision lists using homogeneous rules. In AAAI94.
Smyth, P., & Goodman, R. M. (1992). An information theoretic approach to rule induction
from databases. IEEE Transactions on Knowledge and Data Engineering, 4 (2), 301
316.
Webb, G. I. (1990). Techniques for efficient empirical induction. In Barter, C. J., &
Brooks, M. J. (Eds.), AI88  Proceedings of the Third Australian Joint Conference
on Artificial Intelligence, pp. 225239 Adelaide. Springer-Verlag.
Webb, G. I. (1993). Systematic search for categorical attribute-value data-driven machine
learning. In Rowles, C., Liu, H., & Foo, N. (Eds.), AI93  Proceedings of the Sixth
Australian Joint Conference on Artificial Intelligence, pp. 342347 Melbourne. World
Scientific.
Webb, G. I. (1994a). Generality is more significant than complexity: Toward alternatives to
Occams Razor. In Zhang, C., Debenham, J., & Lukose, D. (Eds.), AI94  Proceedings of the Seventh Australian Joint Conference on Artificial Intelligence, pp. 6067
Armidale. World Scientific.
Webb, G. I. (1994b). Recent progress in learning decision lists by prepending inferred
rules. In SPICIS94: Proceedings of the Second Singapore International Conference
on Intelligent Systems, pp. B280B285 Singapore.

465

fiJournal of Artificial Intelligence Research 3 (1995) 325-348

Submitted 5/95; published 12/95

Vision-Based Road Detection in Automotive Systems:
A Real-Time Expectation-Driven Approach
Alberto Broggi
Simona Berte

Dipartimento di Ingegneria dell'Informazione
Universita di Parma
Viale delle Scienze
I-43100 Parma, Italy

broggi@ce.unipr.it
simona@ce.unipr.it

Abstract

The main aim of this work is the development of a vision-based road detection system
fast enough to cope with the dicult real-time constraints imposed by moving vehicle
applications. The hardware platform, a special-purpose massively parallel system, has
been chosen to minimize system production and operational costs.
This paper presents a novel approach to expectation-driven low-level image segmentation, which can be mapped naturally onto mesh-connected massively parallel Simd architectures capable of handling hierarchical data structures. The input image is assumed to
contain a distorted version of a given template; a multiresolution stretching process is used
to reshape the original template in accordance with the acquired image content, minimizing
a potential function. The distorted template is the process output.

1. Introduction
The work discussed in this paper forms part of the Eureka Prometheus activities, aimed at
improved road trac safety. Since the processing of images is of fundamental importance
in automotive applications, our current work has been aimed at the development of an
embedded low-cost computer vision system. Due to the special field of application, the
vision system must be able to process data and produce results in real-time. It is therefore
necessary to consider data structures, processing techniques, and computer architectures
capable of reducing the response time of the system as a whole.
The system considered is currently integrated on the Mob-Lab land vehicle (Adorni,
Broggi, Conte, & D'Andrea, 1995). The MOBile LABoratory, the result of Italian work
within the Prometheus project (see Figure 1.a), comprises a camera for the acquisition
and digitization of images, which pipelines data to an on-board massively parallel computer
for processing. As illustrated in Figure 2, the current output configuration comprises a set
of warnings to the driver, displayed by means of a set of Leds on a control-panel (shown in
Figure 1.b). But, due to the high performance levels achieved, it will be possible to replace
this output device with a heads-up display showing the enhanced features superimposed
onto the original image.
This paper presents a move toward the use of top-down control (the following feature
extraction mechanism is based on a model-driven approach), instead of the traditional datadriven approach, which is generally used for data-parallel algorithms.

c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiAlberto Broggi, Simona Berte

Figure 1: (a) The Mob-Lab land vehicle; (b) the control panel used as output to display
the processing results

Heads-up
Display

(a)

LED-based
Control Panel

(b)

Road Detection
System
Camera

Input

Processing

Output

Figure 2: Block diagram of the complete system: (a) planned heads-up display output and
(b) current Led-based output
Starting from the experience gained in the development of a different approach (Broggi,
1995c) based on the parallel detection of image edges pointing to the Focus of Expansion,
this work presents a model-driven low-level processing technique aimed at road detection and
enhancement of the road (or lane) image acquired from a moving vehicle. The model which
contains a-priori knowledge of the feature to be extracted (road or lane) is encoded in the
traditional data structure handled in low-level processing: a two-dimensional array. In this
case, a binary image representing two different regions (road and off-road) has been chosen.
Hereinafter this image will be referred to as Synthetic Image. It is obvious that different
synthetic images must be used according to different acquisition conditions (camera position,
orientation, optics, etc., which are fixed) and environment (number of lanes, one way or two
way trac, etc., which may change at run-time). In the final system implementation some
326

fiVision-Based Road Detection

a-priori world knowledge enables the correct synthetic model selection. As an example,
Figure 3 presents several different synthetic images for different conditions.

(a)

(b)

Figure 3: Synthetic images used as road models for: (a) different camera positions and/or
orientations (b) different number of lanes (assuming driving on the right)
The following Section presents a survey of vision-based lane detection systems; Section
3 explains the choice of the multiresolution approach; Section 4 presents the details of the
complete algorithm; Section 5 discusses the performances of current implementation on
the Paprica system; Section 6 presents some results and a critical analysis of the approach
which leads to the present development; finally Section 7 presents some concluding remarks.

2. Comparison with Related Systems
Many different vision-based road detection systems have been developed worldwide, each
relying on various characteristics such as different road models (two or three dimensional),
acquisition devices (color or monochromatic camera, using mono or stereo vision), hardware systems (special- or general-purpose, serial or parallel), and computational techniques
(template matching, neural networks, etc.).

 The Scarf system (tested on the Navlab vehicle at Carnegie Mellon University)

uses two color cameras for color-based image segmentation; the different regions are
classified and grouped together to form larger areas; finally a Hough-like transform is
used to vote for different binary model candidates. Due to the extremely high amount
of data to be processed, the two incoming color images (480  512) are reduced to
60  64 pixel. Nevertheless, a high performance computer architecture, a 10 cell
Warp (Crisman & Webb, 1991; Hamey, Web, & Wu, 1988; Annaratone, Arnould,
T.Gross, H.Kung, & J.Webb, 1987), has been chosen to speed-up the processing. The
system, capable of detecting even unstructured roads, reaches a processing rate of
1 Hz (Crisman & Thorpe, 1993, 1991, 1990; Thorpe, 1989). In addition to its heavy
3
computational load, the main problems with this approach are found in the implicit
models assumed: if the road curves sharply or if it changes width, the assumed shape
model becomes invalid and detection fails (Kluge & Thorpe, 1990).
 The Vits system (tested on the Alv vehicle and developed at Martin Marietta) also
relies on two color cameras. It uses a combination of the red and blue color bands to
segment the image, in an effort to reduce the artifacts caused by shadows. Information
on vehicle motion is also used to aid the segmentation process. Tested successfully
327

fiAlberto Broggi, Simona Berte

on straight, single lane roads, it runs faster than Scarf, sacrificing general capability
for speed (Turk, Morgenthaler, Gremban, & Marra, 1988).
 Alvinn (tested on Navlab, Cmu) is a neural network based 30  32 video retina
designed, like Scarf, to detect unstructured roads, but it does not have any road
model: it learns associations between visual patterns and steering wheel angles, without considering the road location. It has also been implemented on the Warp system,
reaching a processing rate of about 10 Hz (Jochem, Pomerleau, & Thorpe, 1993;
Pomerleau, 1993, 1990).
 A different neural approach has been developed at Cmu and tested on Navlab:
a 256  256 color image is segmented on a 16k processor MasPar MP-2 (MasPar
Computer Corporation, 1990). A trapezoidal road model is used, but the road width
is assumed to be constant throughout the sequence: this means that although the
trapezoid may be skewed to the left or right, the top and bottom edges maintain a
constant length. The high performance offered by such a powerful hardware platform
is limited by its low I/O bandwidth; therefore a simpler reduced version (processing
128  128 images) has been implemented, working at a rate of 2.5 Hz (Jochem &
Baluja, 1993).
Due to the high amount of data (2 color images) and to the complex operations involved (segmentation, clustering, Hough transform, etc.) the system discussed, even if implemented on extremely powerful hardware machines, achieve a low processing rate. Many
different methods have been considered to speed-up the processing, including the processing
of monochromatic images and the use of windowing techniques (Turk et al., 1988) to process
only the regions of interest, thus implementing a Focus of Attention mechanism (Wolfe &
Cave, 1990; Neumann & Stiehl, 1990).

 As an example, in VaMoRs (developed at Universitat der Bundeswehr, Munchen)

monochromatic images are processed by custom hardware, focusing only on the regions
of interest (Graefe & Kuhnert, 1991). The windowing techniques are supported by
strong road and vehicles models to predict features in incoming images (Dickmans &
Mysliwetz, 1992). In this case, the vehicle was driven at high speeds (up to 100 kph) on
German autobahns, which have constant lane width, and where the road has specific
shapes: straight, constant curvature, or clothoidal. The use of a single monochromatic
camera together with these simple road models allows a fast processing based on
simple edge detection; a match with a structured road model is then used to discard
anomalous edges. This approach is disturbed in shadow conditions, when the overall
illumination changes, or when road imperfections are found (Kluge & Thorpe, 1990).
 The Lanelok system (developed at General Motors) also relies on strong road models:
it estimates the location of lane boundaries with a curve fitting method (Kenue &
Bajpayee, 1993; Kenue, 1991, 1990), using a quadratic equation model. In addition to
being disturbed by the presence of vehicles close to the road markings, lane detection
generally fails in shadow conditions. An extension for the correct interpretation of
shadows has therefore been introduced (Kenue, 1994); unfortunately this technique
relies on fixed brightness thresholds which is far from being a robust and general
approach.
328

fiVision-Based Road Detection

The main aim of the approach discussed in this paper, on the other hand, is to build
a low-cost system capable of achieving real-time performance in the detection of structured roads (with painted lane markings), and robust enough to tolerate severe illumination
changes such as shadows. Limitation to the analysis of structured environments allows the
use of simple road models which, together with the processing of monocular monochromatic
images on special-purpose hardware allows the achievement of low-cost high performances.
The use of a high performance general-purpose architecture, such as a 10 cell Warp or a
16k MasPar MP-2 (as in the case of Carnegie Mellon's Navlab), involves high costs which
are not compatible with widespread large-scale use. It is for this reason that the execution
of low-level computations (eciently performed by massively parallel systems) has usually
been implemented on general purpose processors, as in the case of VaMoRs. The design
and implementation of special-purpose application-oriented architectures (like Paprica,
Broggi, Conte, Gregoretti, Sansoe, & Reyneri, 1995, 1994), on the other hand, keep the
production costs down, while delivering very high performance levels. More generally, the
features that enable the integration of this architecture on a generic vehicle are:
(a) its low production cost,
(b) its low operational cost, and
(c) its small physical size.

2.1 The Computing Architecture

Additional considerations on power consumption show that mobile computing is moving
in the direction of massively parallel architectures comprising a large number of relatively
slow-clocked processing elements. The power consumption of dynamic systems can be
considered proportional to CfV 2 , where C represents the capacitance of the circuit, f is
the clock frequency, and V is the voltage swing. Power can be saved in three different ways
(Forman & Zahorjan, 1994), by minimizing C , f , and V respectively:
 using a greater level of Vlsi integration, thus reducing the capacitance C ;
 trading computer speed (with a lower clock frequency f ) for lower power consumption
(already implemented on many portable PCs);
 reducing the supply voltage VDD .
Recently, new technological solutions have been exploited to reduce the IC supply voltage
from 5 V to 3.3 V. Unfortunately, there is a speed penalty to pay for this reduction: for
a Cmos gate (Shoji, 1988), the device delay Td (following a first order approximation) is
VDD 2 , which shows that the reduction of VDD determines a quasiproportional to
(VDD , VT )
linear increment (until the device threshold value VT ) of the circuit delay Td . On the other
hand, the reduction of VDD determines a quadratic reduction of the power consumption.
Thus, for power saving reasons, it is desirable to operate at the lowest possible speed, but, in
order to maintain the overall system performance, compensation for these increased delays
is required.
The use of a lower power supply voltage has been investigated and different architectural solutions have been considered so as to overcome the undesired side effects caused
by the reduction of VDD (Courtois, 1993; Chandrakasan, Sheng, & Brodersen, 1992). The
329

fiAlberto Broggi, Simona Berte

reduction of power consumption, while maintaining computational power, can be achieved
by using low cost Simd computer architectures, comprising a large number of extremely
simple and relatively slow-clocked processing elements. These systems, using slower device
speeds, provide an effective mechanism for trading power consumption for silicon area, while
maintaining the computational power unchanged. The 4 major drawbacks of this approach
are:

 a solution based on hardware replication increases the silicon area, and thus it is not

suitable for designs with extreme area constraints;
 parallelism must be accompanied by extra-routing, requiring extra-power; this issue
must be carefully considered and optimized;
 the use of parallel computer architectures involves the redesigning of the algorithms
with a different computational model;
 since the number of processing units must be high, if the system has size constraints
the processing elements must be extremely simple, performing only simple basic operations.

This paper investigates a novel approach to real-time road following based on the use of
low-cost massively parallel systems and data-parallel algorithms.

3. The Multiresolution Approach
The image encoding the model (Synthetic Image) and the image from the camera (Natural
Image) cannot be directly compared with local computations, because the latter contains
much more detail than the former. From all the known methods used to decrease the
presence of details, it is necessary to choose one that does not decrease the strength of the
feature to be extracted. For this purpose, a low-pass filter, such as a 3  3 neighborhoodbased anisotropic average filter, would not only reduce the presence of details, but also the
sharpness of the road boundaries, rendering their detection more dicult. Since the road
boundaries exploit a long-distance correlation, a subsampling of both the natural and the
synthetic image would lead to a comparison which was less dependent on detail content.
More generally, it is much easier to detect large objects at a low resolution, where only
their main characteristics are present, than at a high resolution, where the details of the
specific represented object can make its detection more dicult. The complete recognition
and description process, on the other hand, can only take place at high resolutions, where
it is possible to detect even small details because of the preliminary results obtained at a
coarse resolution.
These considerations lead to the use of a pyramidal data structure (Rosenfeld, 1984;
Ballard & Brown, 1982; Tanimoto & Kilger, 1980), comprising the same image at different
resolutions. Many different architectures have been developed recently to support this computational paradigm (Cantoni & Ferretti, 1993; Cantoni, Ferretti, & Savini, 1990; Fountain,
1987; Cantoni & Levialdi, 1986): where the computing architecture contains a number of
processing elements which is smaller than the number of image pixels and an external processor virtualization mechanism (Broggi, 1994) is used. A useful side effect due to resolution
reduction is a decrease in the number of computations to be performed. Thus, the choice
330

fiOutput image

Input image

Synthetic image

Vision-Based Road Detection

1

256
X
256

1

1

128
X
128

undersampling

8

256
X
256

8

8

8

average

...

1

32
X
32

8

32
X
32

128
X
128

8

8

8

average

...

undersampling

8
1

256
X
256

DBS

1

1

oversampling

1

undersampling

undersampling

8
1

1

1

8
1

128
X
128

1

1

oversampling

DBS

...

1

DBS

Figure 4: Block diagram of the whole algorithm; for each step the depth of every image is
shown (in bit/pixel)
of a pyramidal computational paradigm, in addition to being supported by theory, offers
an advantage in terms of computational eciency.

4. Algorithm Structure

As shown in Figure 4, before each subsampling the natural image is filtered. In this way it is
possible to decrease both the inuence of noise and redundant details, and the distortion due
to aliasing (Pratt, 1978) introduced by the subsampling process. The image is partitioned
into non-overlapping square subsets of 2  2 pixels each; the filter comprises a simple average
of the pixels values for a given subset, which reduces the signal bandwidth. The set of
resulting values forms the subsampled image.
The stretching of the synthetic image is performed through an iterative algorithm
(Driven Binary Stretching, Dbs), a much simpler and morphological (Haralick, Sternberg,
& Zhuang, 1987; Serra, 1982) version of the \snake" technique (Blake & Yuille, 1993; Cohen & Cohen, 1993; Cohen, 1991; Kass, Witkin, & Terzopolous, 1987). The result is then
oversampled, and further improved using the same Dbs algorithm, until the original resolution is reached. The boundary of the stretched template represents the final result of the
process.

4.1 The DBS Filter

The purpose of the Dbs filter, illustrated in Figure 5, is to stretch the binary input model in
accordance with the data encoded in the grey-tone natural image and to produce a reshaped
version of the synthetic model as an output.
Usually the boundary of a generic object is represented by brightness discontinuities
in the image reproducing it, therefore, in the first version, the first step of the Dbs filter
comprises an extremely fast and simple gradient-based filter computed on the 3  3 neighborhood of each pixel. Then, as shown in Figure 5, a threshold is applied to the gradient
image, in order to keep only the most significant edges. The threshold value is now fixed,
but its automatic tuning based on median filtering is currently being tested (Folli, 1994).
331

fiAlberto Broggi, Simona Berte

More precisely, two different threshold values are computed for the left and right halves
of the image, in order to detect both road boundaries even under different illumination
conditions.
8
8
1

1

gradient

DBS

8

threshold

threshold
value

1

DT
8
1

1

iterative algorithm

Figure 5: Block diagram of the Dbs filter
Since a 2D mesh-connected massively parallel architecture is used, an iterative algorithm
must be performed in order to stretch the synthetic model toward the positions encoded in
the thresholded image. A further advantage of the pyramidal approach is that the number
of iterations required for successful stretching is low at a coarse resolution since the image
size is small; and again, only a few iterations are required for high-resolution refinement,
due to the initial coarse low-resolution stretching. Each border pixel of the synthetic image
is attracted towards the position of the nearest foreground pixel of the thresholded image,
as shown in Figure 6.

Model
section

Threshold

(a)

(b)

Figure 6: The attraction of the boundary pixels of the model (in grey) toward the thresholded image (the rectangular contour): (a) two-dimensional case; (b) monodimensional section
For this purpose, a scalar field V is defined on E 2. Field V : E 2 ! E , links the position
of each pixel p 2 E 2 to a scalar value V (p) 2 E , which represents the potential associated
to the pixel itself. The set of these values is encoded in a potential image. The difference
V (p) , V (q) represents the cost of moving pixel p toward position q. The scalar field is
defined in such a way that a negative cost corresponds to the movement toward the nearest
332

fiVision-Based Road Detection

position of the foreground pixels ti of the thresholded image. As a consequence, the iterative
process is designed explicitly to enable all the pixel movements associated to a negative cost.
Thus, the value V (p) depends on the minimum distance between pixel p and pixels ti :
V (p) = , min
(1)
i d(p; ti) ;

where d : E 2  E 2 ! E represents the distance between two pixels, measured with respect
to a given metric. In this case, due to the special simplicity of the implementation, a city
block (Manhattan distance) metric has been chosen (see Figure 7).
4

3

2

3

4

3

2

1

2

3

2

1

0

1

2

3

2

1

2

3

4

3

2

3

4

Figure 7: City block or Manhattan distance from the central pixel
A very ecient method for the computation of the potential image using 2D mesh-connected
architectures is based on the iterative application of morphological dilations (Haralick et al.,
1987; Serra, 1982):
1. a scalar counter is initialized to 0; for parallel architectures which cannot run any
fragment of scalar code, this counter is associated to every pixel in the image, thus
constituting a \parallel" counter;
2. the counter is decremented;
3. the binary input image is dilated using a 4-connected structuring element N , formed
by the following elements:
n
o

N = (0; 1); (0; ,1); (0; 0); (1; 0); (,1; 0) =  "!  ;


(2)

4. the value of the counter is assigned to the potential image in the positions where the
pixel, due to the previous dilation, changes its state from background to foreground.
5. the process is then repeated from step 2, until the output of the morphological dilation
is equal to its input.
This shows that the potential image can be generated by the application of a Distance
Transform, DT (Borgefors, 1986) to the binary thresholded image. Due to a more ecient
implementation-dependent data handling, the final version of the potential image DT is
obtained by adding a constant  to every coecient, so as to work with only positive
values:  represents the maximum value allowed for grey-tone images1. Thus the new
definition of the scalar field V is:
V (p) =  , min
(3)
i d(p; ti) :
1. In this specific case, since 8-bit images are considered,  = 255.

333

fiAlberto Broggi, Simona Berte

Linear profile of
the input image
[8 bit/pixel]

threshold value

Gradient
[8 bit/pixel]

Threshold
[1 bit/pixel]

Distance
Transform (DT)
[8 bit/pixel]

Synthetic
input image
[1 bit/pixel]
Synthetic
output image
[1 bit-pixel]

Figure 8: Example of 4 iterations of the Dbs algorithm (monodimensional case)
Furthermore, since the `distance' information is used only by the pixels belonging to the
border of the synthetic image and their neighbors, the iterative process is stopped when the
DT has been computed for these pixels, producing a noticeable performance improvement.
As already mentioned, the crux of the algorithm is an iterative process whose purpose
is to move the edge pixels of the synthetic image in the directions in which the DT gradient
has a maximum. Figure 8 shows an example of a monodimensional stretching. As shown
in Appendix A, the strength of this approach lies in the fact that the stretching algorithm
can be expressed simply by a sequence of morphological operations, and can therefore be
mapped eciently on mesh-connected massively parallel architectures, achieving higher
performance levels than other approaches (Cohen & Cohen, 1993; Cohen, 1991; Kass et al.,
1987).
In order to describe the Dbs algorithm, let us introduce some definitions using Mathematical Morphology operators (Haralick et al., 1987; Serra, 1982) such as dilation (),
erosion (	), or complement (). A two-dimensional binary image S is represented as a
subset of E 2, whose elements correspond to the foreground pixels of the image:

n
o
S = s 2 E 2  s = (x; y); x; y 2 E ;
(4)
where vector (x; y ) represents the coordinates of the generic element s.
334

fiVision-Based Road Detection

The external edge of S is defined as the set of elements representing the difference
between S and the dilation of S by the 4-connected structuring element N shown in expression (2):
(5)
Be(S ) = (S  N ) \ S :
In a similar way, the set of elements representing the difference between S and its erosion
using the same structuring element N is defined to be the internal edge of S :

Bi(S ) = (S 	 N ) \ S :

(6)

4.1.1 The Iterative Rules

The set of elements B(S ) = Be (S ) [ Bi (S ) are the only elements which can be inserted or
removed from set S by the application of a single iteration of the Dbs algorithm. More
precisely, two different rules are applied to the two edges: the first, applied to the external
edge Be (S ), determines the elements to be included in set S ; while the second, applied to
the internal edge Bi (S ), determines the elements to be removed from set S .

 Rule for the external edge:
{ each pixel of the external edge of S computes the minimum value of the DT
associated to its 4-connected neighbors belonging to set S ;
{ all the pixels, whose associated DT is greater than the value previously computed, are inserted into set S .

The application of this rule has the effect of expanding the synthetic image towards
the foreground pixels ti which are not included in the synthetic model (see the right
hand side of Figure 6.b).

 Rule for the internal edge:
{ each pixel of the internal edge of S computes the minimum value of the DT
associated to its 4-connected neighbors not belonging to set S ;
{ all the pixels, whose associated DT is greater than the value previously computed, are removed from set S .
The application of this rule has the effect of shrinking the synthetic image (see the
left hand side of Figure 6.b).
Note that rule 2 is the inverse of rule 1: the latter tends to stretch the foreground onto
the background, while the former acts in the opposite way, using the complement of the
synthetic image.
4.1.2 Flat Handling

Figure 8 refers to a monodimensional stretching. Unfortunately, when dealing with 2D data
structures, the DT image does not present a strictly increasing or decreasing behavior, even
locally. Thus, an extension to the previous rules must be considered for correct at-handling
in the 2D space.
335

fiAlberto Broggi, Simona Berte

2

1

2

3

4

5

4

3

2

1

0

1

2

2

1

2

3

4

5

4

3

2

1

0

1

2

2

1

2

3

4

5

4

3

2

1

0

1

2

1

0

1

2

3

4

3

2

1

0

0

0

1

1

0

1

2

3

4

3

2

1

0

0

0

1

1

0

1

2

3

4

3

2

1

0

0

0

1

0

1

2

3

4

4

3

2

1

0

1

1

2

0

1

2

3

4

4

3

2

1

0

1

1

2

0

1

2

3

4

4

3

2

1

0

1

1

2

1

0

1

2

3

4

3

2

1

0

1

2

3

1

0

1

2

3

4

3

2

1

0

1

2

3

1

0

1

2

3

4

3

2

1

0

1

2

3

1

0

1

2

3

4

3

2

1

0

1

2

3

1

0

1

2

3

4

3

2

1

0

1

2

3

1

0

1

2

3

4

3

2

1

0

1

2

3

1

0

1

2

3

4

3

2

1

0

1

2

3

1

0

1

2

3

4

3

2

1

0

1

2

3

1

0

1

2

3

4

3

2

1

0

1

2

3

1

0

1

2

3

4

4

3

2

1

0

1

2

1

0

1

2

3

4

4

3

2

1

0

1

2

1

0

1

2

3

4

4

3

2

1

0

1

2

2

1

0

1

2

3

4

4

3

2

1

0

1

2

1

0

1

2

3

4

4

3

2

1

0

1

2

1

0

1

2

3

4

4

3

2

1

0

1

2

1

0

1

2

3

4

4

3

2

1

0

1

2

1

0

1

2

3

4

4

3

2

1

0

1

2

1

0

1

2

3

4

4

3

2

1

0

1

3

2

1

2

3

4

5

5

4

3

2

1

2

3

2

1

2

3

4

5

5

4

3

2

1

2

3

2

1

2

3

4

5

5

4

3

2

1

2

a) Input image

b) Stretching without flat-handling

c) Stretching with flat-handling

Figure 9: Two-dimensional stretching: different square markings represent different states
of input binary image; dark grey areas represent the stretching area
Figure 9.a shows the modulus of the DT coecients (in the case of  = 0), together
with the input binary image (shown in grey). The output of the iterative Dbs algorithm is
presented in Figure 9.b.
Since the movement of a generic pixel towards positions holding an equal DT coecient is expressly disabled, the resulting binary image does not completely follow the shape
encoded in the DT image. A minor revision to the definition of rule 1 is thus required.
Figure 9.c is obtained with the following rule applied to the external edge.

 Rule for the external edge, including at handling:
{ each pixel of the external edge of S computes the minimum value of the DT
associated to its 4-connected neighbors belonging to set S ;
{ all the pixels, whose associated DT is greater than the value previously computed, and all the pixels not belonging to the thresholded image, whose associated DT is equal to the value previously computed are inserted into set S .

The specific requirement for the pixels moving toward a at region, of not belonging to the
thresholded image, ensures that the binary image does not follow the DT chain of maxima.
With such a requirement, in the specific case of Figure 9, the maxima in the upper-right
hand area are not included in the resulting binary image.

5. Performance Analysis of the Current Implementation

Since the final aim of this work is to integrate the road detection system on a mobile
vehicle, the main requirement is to achieve real-time performance. The choice of a specialpurpose massively parallel architecture has already been justified in Section 1; moreover,
the algorithm discussed in this paper maps naturally onto a single-bit mesh-connected Simd
machine, because the whole computation can be eciently expressed as a sequence of binary
morphological operators, as shown in Appendix A (which presents the morphology-based
description of the whole Dbs algorithm).
The complete processing, first tested on a Connection Machine CM-2 (Hillis, 1985),
is now implemented on the special-purpose massively parallel Simd architecture Paprica.
336

fiVision-Based Road Detection

The Paprica system (PArallel PRocessor for Image Checking and Analysis), based on a hierarchical morphology computational model, has been designed as a specialized coprocessor
to be attached to a general purpose host workstation: the current implementation, consisting of a 16  16 square array of processing units, is connected to a Sparc-based workstation
via a Vme bus, and installed on Mob-Lab. The current hardware board (a single 6U Vme
board integrating the Processor Array, the Image and Program Memories, and a frame
grabber device for the direct acquisition of images for the Paprica Image Memory) is the
result of the full reengineering of the first Paprica prototype which has been extensively
analyzed and tested (Gregoretti, Reyneri, Sansoe, Broggi, & Conte, 1993) for several years.
The Paprica architecture has been developed explicitly to meet the specific requirements of real-time image processing applications (Broggi, 1995c, 1995b; Adorni, Broggi,
Conte, & D'Andrea, 1993); the specific processor virtualization mechanism utilized by Paprica architecture allows the handling of pyramidal data structures without any additional
overhead equipment.
As shown in Figure 2, the output device can be:
(a) a heads-up display in which the road (or lane) boundaries are highlighted;
(b) a set of Leds indicating the relative position of the road (or lane) and the vehicle.
Starting from 256  256 grey-tone images and after the resolution reduction process, in the
first case (a) the initial resolution must be recovered in order to superimpose the result onto
the original image. In the second case (b), on the other hand, due to the high quantization
of the output device, the processing can be stopped at a low resolution (e.g., 64  64), where
only a stripe of the resulting image is analyzed to drive the Leds. Table 1 presents the
computational time required by each step of the algorithm (considering 5 DT iterations
and 5 Dbs iterations for each pyramid level).
Operation
Image size
Time [ms]
Resolution Reduction
2562!322
149.70
2
2
DT, Dbs & Oversampling
32 !64
18.55
DT, Dbs & Oversampling
642!1282
69.13
DT, Dbs & Oversampling
1282!2562
296.87
Total (Led output)
2562!322 !642
168.25
2
2
2
Total (heads-up display output) 256 !32 !256
534.25
Table 1: Performance of Paprica system; the numbers refer to 5 iterations of the DT
process and 5 iterations of the Dbs filter for each pyramid level.
In the current Mob-Lab configuration, the output device (shown in Figure 1.b) consists
of a set of 5 Leds: Table 1 shows that the 2562 ! 322 ! 642 filtering of a single frame takes
150180 ms (depending on the number of iterations required), allowing the acquisition and
processing of about 6 frames per second.
Moreover, due to the high correlation between two consecutive sequence frames, the
final stretched template can be used as the input model to be stretched by the processing
of the following frame. In these conditions a lower number of Dbs and DT iterations
337

fiAlberto Broggi, Simona Berte

a

b

c

d

e

a

b

c

d

e

a

b

c

d

e

Figure 10: The detection of road markings through the removal of the perspective effect in
three different conditions: straight road with shadows, curved road with shadows, junction. (a) input image; (b) reorganized image, obtained by non-uniform
resampling of (a); (c) result of the line-wise detection of black-white-black transitions in the horizontal direction; (d) reintroduction of the perspective effect,
where the grey areas represent the portion of the image shown in (c); (e) superimposition of (d) onto a brighter version of the original image (a).
are needed for the complete template reshaping, thus producing a noticeable performance
improvement. The reduction in the time required to process a single frame also increases
the correlation between the current and the following frame in the sequence, thus allowing
a further reduction in computation time2 .
Images acquired in many different conditions and environments have been used for
extensive experimentation, which was performed first off-line on a functional simulator
implemented on an 8k processor Connection Machine CM-2 (Hillis, 1985) and then in realtime on the Paprica hardware itself on Mob-Lab. The complete system was demonstrated
at the final Prometheus project meeting in Paris, in October 1994: the Mob-Lab land
vehicle was driven around the two-lane track at Mortefontaine, under different conditions
(straight and curved roads, with shadows and changing illumination conditions and with
other vehicles on the path).
2. In the processing of highly correlated sequences (namely when the road conditions change slowly or in an
off-line slow-motion tape playback in the laboratory) the computational time required by the processing
of a single frame can reach a minimum value of 150 ms.

338

fiVision-Based Road Detection

The performances obtained during this demonstration allowed the main limitations of
the system to be detected and enabled a critical analysis of the approach, thus leading to
proposals for its development.

6. Critical Analysis and Evolution

The approach discussed achieves good performances in terms of output quality when the
model matches (or is suciently similar to) the road conditions, namely when road markings
are painted on the road surface (on structured roads, see Figures 11.a and 12.a) inducing
a suciently high luminance gradient. This approach is successful when the road or lane
boundaries can be extracted from the input image through a gradient thresholding operation
(see Figures 11.b and 12.b). Unfortunately, this is not always possible, for example when the
road region is a patch of shadow or sunlight, as in Figure 13.a. In this case the computation
of the DT starting from the thresholded image (see Figure 13.b) is no longer significant: a
different method must be devised for the determination of the binary image to be used as
input for the Distance Transform.

a

b

c

d

e

Figure 11: Lane detection on a straight road: (a) input image; (b) image obtained by
thresholding the gradient image; (c) image obtained by the perspective-based
filter; (d) stretched template; (e) superimposition of the edges of the stretched
template onto the original image. In this case both the thresholded gradient (b)
and the perspective-based filtered (c) images can be used as input to the DT.

a

b

c

d

e

Figure 12: Lane detection on a curved road: also in this case both the thresholded gradient
(b) and the perspective-based filtered (c) images can be used as input to the
DT.
In a recent work (Broggi, 1995a) an approach based on the removal of the perspective
effect is presented and its performances discussed. A transform, a non-uniform resampling
similar to what happens in the human visual system (Zavidovique & Fiorini, 1994; Wolfe &
339

fiAlberto Broggi, Simona Berte

Cave, 1990), is applied to the input image (Figures 10.a); assuming a at road, every pixel
of the resampled image (Figures 10.b) now represent the same portion of the road3. Due to
their constant width within the overall image, the road markings can now be easily enhanced
and extracted by extremely simple morphological filters (Broggi, 1995a) (Figures 10.c).
Finally the perspective effect is reintroduced (Figures 10.d).

a

b

c

d

e

Figure 13: Lane detection on a straight road with shadows: this is a case where the thresholded gradient (b) cannot be used to determine the DT image due to the noise
caused by shadows. On the other hand, the perspective-based processing is able
to filter out the shadows and extract the road markings: the DT is in fact
determined using image (c) instead of (b).

a

b

c

d

e

Figure 14: Lane detection on a straight road with a vehicle in the path: this is the only
case in which the Dbs algorithm cannot stretch the binary model successfully,
due to the presence of the vehicle occluding the road markings. The use of a
pair of stereo cameras to remove this problem is currently being investigated.
Due to the high effectiveness of this filter, in the test version of the system currently
operational in the laboratory, the Dbs filter has been improved by replacing the gradient
thresholding with the perspective-based filter. The extended version of the Dbs filter is
shown in Figure 15.
Since the removal (and reintroduction) of the perspective effect can be reduced to a
mere image resampling and the filter is based on simple morphological operators (Broggi,
1995a), the implementation on the Paprica system is straightforward. The preliminary
results obtained by the current test version of the system are encouraging both for the
output quality (the problems caused by shadows are now resolved) and for computation
3. Note that, due to the perspective effect, the pixels in the lower part of the original image (Figures 10.a)
represent a few cm2 of the road region, while the central pixels represent a few tens of cm2 . The nonuniform resampling of the original image has been explicitly designed for a homogeneous distribution of
the information among all the pixels in the image.

340

fiVision-Based Road Detection

8
8
1

extended
DBS

persp. eff.
removal

1

8

morph.
filter
1

persp. eff.
reintrod.
1

DT
8
1

1

iterative algorithm

Figure 15: Diagram of the extended Dbs filter including the perspective-based filtering
time: a single frame is processed in less than 100 ms, thus allowing the processing of about
10 frames per second. The improvement of the Dbs process by means of the perspectivebased filter, in addition to allowing correct road (or lane) detection in presence of shadows,
can be implemented extremely eciently on the Paprica system, taking advantage of a
specific hardware extension designed explicitly for this purpose (non uniform-resampling)
(Broggi, 1995a).
Figures 11, 12, 13, and 14 show the results of the processing in different conditions:
straight and curved road, with shadows and other vehicles in the path, respectively4 . In
the last case the lane cannot be detected successfully due to the presence of an obstacle
The use of a pair of stereo images is currently being investigated to overcome this
problem: the removal of the perspective effect from both the stereo images would lead to
the same image iff the road is at, namely if no obstacles are found on the vehicle path. A
difference in the two reorganized images (namely when an obstacle is detected) would cause
the algorithm to stop the lane detection and warn the driver.
The general nature of the presented approach enables the detection of other suciently
large-sized features: for example, using different synthetic models it is possible to detect
road or lane boundaries, as shown in Figure 16.

7. Conclusions
In this paper a novel approach for the detection of road (or lane) boundaries for visionbased systems has been presented. The multiresolution approach, together with top-down
control, allows achievement of remarkable performances in terms of both computation time
(when mapped on massively parallel architectures) and output quality.
4. Some sequences in MPEG format (200 and 1000 frames respectively) are available via World Wide
Web at http://WWW.CE.UniPR.IT/computer vision, showing lane detection in particularly challenging
conditions. The images shown in Figures 11, 12, 13, 14, and 16 are part of the sequences available in the
previously mentioned WWW page.

341

fiAlberto Broggi, Simona Berte

a

b

c

d

e

Figure 16: (a) input image; (b) synthetic model used for lane detection; (c) superimposition
of the edges of the stretched model onto the original input image; (d) synthetic
model used for road detection; (e) superimposition of the edged of the stretched
model onto the original input image.
A perspective-based filter has been introduced to improve system performance in shadow
conditions. However, even if the perspective-based filter alone is able to extract the road
markings with a high degree of confidence, the hierarchical application of the Dbs filter
(and its extension to the handling of image sequences) is of basic importance since it allows
the exploitation of the temporal correlation between successive sequence frames (performing
solution tracking).
The presence of an obstacle in the vehicle path is still an open problem, which is currently
being approached using stereo vision (Broggi, 1995a).
The algorithm has been implemented on the Paprica system, a massively parallel lowcost Simd architecture; because of its specific hardware features, Paprica is capable of
processing about 10 frames per second.

Appendix A. The Morphological Implementation of the DBS Filter

In this appendix, the rule for the external edge will be considered, assuming step n in
the iterative process. Recalling the Mathematical Morphology notations used to identify a
grey-tone two-dimensional image, the DT image is a subset of E 3:

DT = fd 2 E 3 j d = (u; v); v = V (u); 8 u 2 E 2g ;

(7)

where u represents the position of element d in E 2, and v represents its value.
The pixelwise masking operation between a binary and a grey-tone image is here defined
as a function
fi : E2  E3 ! E3
(8)
such that A fi B represents a subset of B containing only the elements b = (u; v ) whose
position vector u 2 E 2 also belongs to A:

A fi B = fx 2 E 3 j x = (u; v) 2 B; u 2 Ag

(9)

In order to compute the minimum value of the DT in the specified neighborhood, let us
consider image Ke(n)

 

Ke(n) = Se(n) fi DT [ Se(n) fi L ;
(10)
342

fiVision-Based Road Detection

where Se(n) represents the binary image at step n, the subscript e indicates that the rule for
the external edge is being considered, and finally

L = fl 2 E 3 j l = (u; ); 8 u 2 E 2g :

(11)

As shown in (Haralick et al., 1987), in order to compute the minimum value of a grey-tone
image Ke(n) in a 4-connected neighborhood, the following grey-scale morphological erosion
should be used:
Me(n) = Ke(n) 	 Q ;
(12)
where
Q = f(1; 0; 0); (,1; 0; 0); (0; 1; 0); (0; ,1; 0)g ;
(13)
as shown in Figure 17.
0
0

0
0

z
y

Q = { (1, 0, 0) ; (0, 1, 0) ; (-1, 0, 0) ; (0, -1, 0) }

x

Figure 17: Structuring element Q
In order to determine the set of elements in which Me(n) has a value smaller than DT ,
a new function M is required:
M : E3  E3 ! E2 :
(14)
Such a function is defined as

M(A; B) = fx 2 E 2 j V (A; x) < V (B; x)g ;

(15)

where V : E 3  E 2 ! E is defined as

V (A; x) =

(

a if 9 a 2 E j (x; a) 2 T (A) :
,1 otherwise

(16)

In equation (16), T (A) represents the top of A (Haralick et al., 1987), here defined as

T (A) = ft 2 A; t = (u; v) j 6 9 t0 = (x; v0) 2 A for which v0 > vg :

(17)

The set of elements which will be included in set Se(n+1) is given by the logical intersection
between M(Me(n); DT ) and the set of elements belonging to the external edge of Se(n) :








Ee(n) = M Me(n); DT \ Be Se(n) :
343

(18)

fiAlberto Broggi, Simona Berte

Distance
Transform (DT)
[8 bit/pixel]

Distance
Transform (DT)
[8 bit/pixel]
Input
image S
[1 bit/pixel]

Input
image S
[1 bit/pixel]

Synthetic image
complemented S
[1 bit/pixel]

Masked
image K
[8 bit/pixel]

Masked
image K
[8 bit/pixel]

DT

DT
Eroded
image M
[8 bit/pixel]

Dilated
image M
[8 bit/pixel]

M (M,DT)
[1 bit/pixel]

M (M,DT)
[1 bit/pixel]

External edge
[1 bit/pixel]

Internal edge
[1 bit/pixel]

Output image
[1 bit/pixel]

Output image
[1 bit/pixel]

Figure 18: Monodimensional stretching in the case of external edge (left) and internal edge
(right)
Thus, the final result of iteration n is given by
Se(n+1) = Se(n) [ Ee(n) :
(19)
Figure 18.a shows the execution of an individual iteration of rule 1 on a monodimensional
image profile.
Following similar steps, it is possible to formalize rule 2. In order to compute the
maximum value of the DT in the specified neighborhood, let us consider image Ki(n)

Ki(n) = Si(n) fi DT ;

(20)
where the subscript i indicates that the rule for the internal edge is being considered. As
shown above, in order to compute the maximum value of a grey-tone image Ki(n) in a
4-connected neighborhood, the following morphological dilation should be used:
Mi(n) = Ki(n)  Q ;
(21)
where Q is shown in Figure 17.
The set of elements which will be removed from set Si(n+1) is given by the logical intersection between M(Mi(n); DT ) and the set of elements belonging to the internal edge of Si(n) :




Ei(n) = M Mi(n); DT \ Bi Si(n) :
(22)
344

fiVision-Based Road Detection

Thus, the final result of iteration n is given by




Si(n+1) = Si(n) [ Ei(n) = Si(n) \ Ei(n) :

(23)

Figure 18b shows the execution of an individual iteration of rule 2 on a monodimensional
profile of an image.

Acknowledgements
This work was partially supported by the Italian Cnr within the framework of the Eureka
Prometheus Project { Progetto Finalizzato Trasporti under contracts n. 93.01813.PF74
and 94.01371.PF74.
The authors are indebted to Gianni Conte for the valuable and constructive discussions
and for his continuous support throughout the project.

References

Adorni, G., Broggi, A., Conte, G., & D'Andrea, V. (1993). A self-tuning system for realtime Optical Flow detection. In Proceedings IEEE System, Man, and Cybernetics
Conf, Vol. 3, pp. 7{12.
Adorni, G., Broggi, A., Conte, G., & D'Andrea, V. (1995). Real-Time Image Processing for
Automotive Applications. In Laplante, P. A., & Stoyenko, A. D. (Eds.), Real-Time Image Processing: Theory, Techniques and Applications. IEEE and SPIE Press. In press.
Annaratone, M., Arnould, E., T.Gross, H.Kung, & J.Webb (1987). The Warp Computer: Architecture, Implementation and Performance. IEEE Trans on Computers,
C-36 (12), 1523{1538.
Ballard, D. H., & Brown, C. M. (1982). Computer Vision. Prentice Hall.
Blake, A., & Yuille, A. (1993). Active Vision. MIT Press.
Borgefors, G. (1986). Distance Transformations in Digital Images. In Computer Vision,
Graphics and Image Processing, Vol. 34, pp. 344{371.
Broggi, A. (1994). Performance Optimization on Low-Cost Cellular Array Processors. In
Proceedings IEEE Intl Conf on Massively Parallel Computing Systems, pp. 334{338.
Broggi, A. (1995a). A Massively Parallel Approach to Real-Time Vision-Based Road Markings Detection. In Masaky, I. (Ed.), Proceedings IEEE Intelligent Vehicles'95, pp.
84{89.
Broggi, A. (1995b). A Novel Approach to Lossy Real-Time Image Compression: Hierarchical
Data Reorganization on a Low-Cost Massively Parallel System. Real-Time Imaging
Journal, 1 (2).
Broggi, A. (1995c). Parallel and Local Feature Extraction: a Real-Time Approach to Road
Boundary Detection. IEEE Trans on Image Processing, 4 (2), 217{223.
Broggi, A., Conte, G., Gregoretti, F., Sansoe, C., & Reyneri, L. M. (1995). The Evolution
of the PAPRICA System. Integrated Computer-Aided Engineering Journal - Special
Issue on Massively Parallel Computing. In press.
345

fiAlberto Broggi, Simona Berte

Broggi, A., Conte, G., Gregoretti, F., Sansoe, C., & Reyneri, L. M. (1994). The PAPRICA
Massively Parallel Processor. In Proceedings IEEE Intl Conf on Massively Parallel
Computing Systems, pp. 16{30.
Cantoni, V., & Ferretti, M. (1993). Pyramidal Architectures for Computer Vision. Plenum
Press, London.
Cantoni, V., Ferretti, M., & Savini, M. (1990). Compact Pyramidal Architectures. In Tou,
J., & Balchen, J. (Eds.), Highly Redundant Sensing in Robotic Systems, Vol. 58, pp.
157{174. NATO ASI Series F.
Cantoni, V., & Levialdi, S. (1986). Pyramidal Systems for Computer Vision. Springer
Verlag, Berlin.
Chandrakasan, A., Sheng, S., & Brodersen, R. (1992). Low-Power CMOS Digital Design.
IEEE Journal of Solid-State Circuits, 27 (4), 473{484.
Cohen, L. D. (1991). Note on Active Contour Models and Balloons. CGVIP: Image Understanding, 53 (2), 211{218.
Cohen, L. D., & Cohen, I. (1993). Finite-Element Methods for Active Contour Models and
Balloons for 2-D and 3-D Images. IEEE Trans on PAMI, 15 (11), 1131{1147.
Courtois, B. (1993). CAD and Testing of ICs and systems: Where are we going?. Tech.
rep., TIMA & CMP.
Crisman, J., & Thorpe, C. (1990). Color Vision for Road Following. In Thorpe, C. E. (Ed.),
Vision and Navigation. The Carnegie Mellon Navlab, pp. 9{24. Kluwer Academic
Publishers.
Crisman, J., & Thorpe, C. (1991). UNSCARF, A Color Vision System for the Detection
of Unstructured Roads. In Proceedings IEEE Intl Conf on Robotics and Automation,
pp. 2496{2501.
Crisman, J., & Thorpe, C. (1993). SCARF: A Color Vision System that Tracks Roads and
Intersections. IEEE Trans on Robotics and Automation, 9 (1), 49{58.
Crisman, J. D., & Webb, J. A. (1991). The Warp Machine on Navlab. IEEE Trans on
PAMI, 13 (5), 451{465.
Dickmans, E. D., & Mysliwetz, B. D. (1992). Recursive 3-D Road and Relative Ego-State
Recognition. IEEE Trans on PAMI, 14, 199{213.
Folli, A. (1994). Elaborazione parallela di immagini per applicazioni in tempo reale su
autoveicolo. Master's thesis, Universita degli Studi di Parma, Facolta di Ingegneria.
Forman, G. H., & Zahorjan, J. (1994). The Challenge of Mobile Computing. Computer,
27 (4), 38{47.
Fountain, T. (1987). Processor Arrays: Architectures and applications. Academic-Press,
London.
Graefe, V., & Kuhnert, K.-D. (1991). Vision-based Autonomous Road Vehicles. In Masaki,
I. (Ed.), Vision-based Vehicle Guidance, pp. 1{29. Springer Verlag.
Gregoretti, F., Reyneri, L. M., Sansoe, C., Broggi, A., & Conte, G. (1993). The PAPRICA
SIMD array: critical reviews and perspectives. In Dadda, L., & Wah, B. (Eds.),
Proceedings ASAP'93 - IEEE Intl Conf on Application Specific Array Processors, pp.
309{320 Venezia, Italy.
346

fiVision-Based Road Detection

Hamey, L. G. C., Web, J. A., & Wu, I. (1988). Low-level vision on Warp and the Apply
Programming Model. In Kowalik, J. S. (Ed.), Parallel Computations and Computers
for Artificial Intelligence, pp. 185{200. Kluwer Academic Publishers.
Haralick, R. M., Sternberg, S. R., & Zhuang, X. (1987). Image Analysis Using Mathematical
Morphology. IEEE Trans on PAMI, 9 (4), 532{550.
Hillis, W. D. (1985). The Connection Machine. MIT Press, Cambridge, Ma.
Jochem, T. M., & Baluja, S. (1993). A Massively Parallel Road Follower. In Bayoumi,
M. A., Davis, L. S., & Valavanis, K. P. (Eds.), Proceedings Computer Architectures
for Machine Perception '93, pp. 2{12.
Jochem, T. M., Pomerleau, D. A., & Thorpe, C. E. (1993). MANIAC: A Next Generation Neurally Based Autonomous Road Follower. In Proceedings of the Intl Conf on
Intelligent Autonomous Systems: IAS -3 Pittsburgh, Pennsylvania, USA.
Kass, M., Witkin, A., & Terzopolous, D. (1987). Snakes: Active Contour Models. Intl
Journal of Computer Vision, 1, 321{331.
Kenue, S. K. (1990). LANELOK: detection of lane boundaries and vehicle tracking using
image-processing techniques. In Proceedings of SPIE - Mobile Robots IV, Vol. 1195,
pp. 221{233.
Kenue, S. K. (1991). LANELOK: An Algorithm for Extending the Lane Sensing Operating
Range to 100 Feet. In Proceedings of SPIE - Mobile Robots V, Vol. 1388, pp. 222{233.
Kenue, S. K. (1994). Correction of Shadow Artifacts for Vision-based Vehicle Guidance. In
Proceedings of SPIE - Mobile Robots VIII, Vol. 2058, pp. 12{26.
Kenue, S. K., & Bajpayee, S. (1993). LANELOK: Robust Line and Curvature Fitting of
Lane Boundaries. In Proceedings of SPIE - Mobile Robots VII, Vol. 1831, pp. 491{503.
Kluge, K., & Thorpe, C. E. (1990). Explicit Models for Robot Road Following. In Thorpe,
C. E. (Ed.), Vision and Navigation. The Carnegie Mellon Navlab, pp. 25{38. Kluwer
Academic Publishers.
Kluge, K. (1994). Extracting Road Curvature and Orientation from Image Edge Points
without Perceptual Grouping into Features. In Proceedings IEEE Intelligent Vehicles'94.
MasPar Computer Corporation, Sunnyvale, California (1990). MP-1 Family Data-Parallel
Computers.
Neumann, H., & Stiehl, H. (1990). Toward a computational architecture for monocular
preattentive segmentation. In G.Hartmann, R., & G.Hauske (Eds.), Parallel Processing in Neural Systems and Computers. Elsevier (North Holland).
Newman, W. M., & Sproull, R. F. (1981). Principles of Interactive Computer Graphics.
McGraw-Hill, Tokyo.
Pomerleau, D. A. (1990). Neural Network Based Autonomous Navigation. In Thorpe,
C. E. (Ed.), Vision and Navigation. The Carnegie Mellon Navlab, pp. 83{93. Kluwer
Academic Publishers.
Pomerleau, D. A. (1993). Neural Network Perception for Mobile Robot Guidance. Kluwer
Academic Publishers, Boston.
Pratt, W. K. (1978). Digital Image Processing. John Wiley & Sons.
347

fiAlberto Broggi, Simona Berte

Rosenfeld, A. (1984). Multiresolution Image Processing and Analysis. Springer Verlag,
Berlin.
Serra, J. (1982). Image Analysis and Mathematical Morphology. Academic Press, London.
Shoji, M. (1988). CMOS Digital Circuit Technology. Prentice Hall.
Tanimoto, S. L., & Kilger, K. (1980). Structured Computer Vision: Machine Perception
trough Hierarchical Compuation Structures. Academic Press, NY.
Thorpe, C. (1989). Outdoor Visual Navigation for Autonomous Robots. In Kanada, T.,
Groen, F. C. A., & Hertzberger, L. O. (Eds.), Intelligent Autonomous Systems, Vol. 2,
pp. 530{544.
Tsai, R. (1986). An Ecient and Accurate Camera Calibration Technique for 3D Machine
Vision. In Proceedings IEEE Intl Conf on Computer Vision and Pattern Recognition,
pp. 364{374.
Turk, M. A., Morgenthaler, D. G., Gremban, K. D., & Marra, M. (1988). VITS - A Vision
System for Autonomous Land Vehicle Navigation. IEEE Trans on PAMI, 10 (3).
Wolfe, J. M., & Cave, K. R. (1990). Deploying visual attention: the guided model. In AI
and the eye, pp. 79{103. A.Blake and T.Troscianko.
Zavidovique, B., & Fiorini, P. (1994). A Control View to Vision Architectures. In Cantoni,
V. (Ed.), Human and Machine Vision: Analogies and Divergencies, pp. 13{56. Plenum
Press.

348

fiJournal of Artificial Intelligence Research 3 (1995) 223-248

Submitted 6/94; published 10/95

Improving Connectionist Energy Minimization
Gadi Pinkas

pinkas@cs.wustl.edu

Rina Dechter

dechter@ics.uci.edu

Center for Optimization and Semantic Control, Washington University
AMDOCS Inc, 1611 Des Peres Rd., St Louis, MO 63131 USA
Department of Information and Computer Science
University of California, Irvine, CA 92717, USA

Abstract

Symmetric networks designed for energy minimization such as Boltzman machines and
Hopfield nets are frequently investigated for use in optimization, constraint satisfaction
and approximation of NP-hard problems. Nevertheless, finding a global solution (i.e., a
global minimum for the energy function) is not guaranteed and even a local solution may
take an exponential number of steps. We propose an improvement to the standard local
activation function used for such networks. The improved algorithm guarantees that a
global minimum is found in linear time for tree-like subnetworks. The algorithm, called
activate, is uniform and does not assume that the network is tree-like. It can identify
tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid local minima
along these trees. For acyclic networks, the algorithm is guaranteed to converge to a
global minimum from any initial state of the system (self-stabilization) and remains correct
under various types of schedulers. On the negative side, we show that in the presence of
cycles, no uniform algorithm exists that guarantees optimality even under a sequential
asynchronous scheduler. An asynchronous scheduler can activate only one unit at a time
while a synchronous scheduler can activate any number of units in a single time step. In
addition, no uniform algorithm exists to optimize even acyclic networks when the scheduler
is synchronous. Finally, we show how the algorithm can be improved using the cycle-cutset
scheme. The general algorithm, called activate-with-cutset improves over activate and has
some performance guarantees that are related to the size of the network's cycle-cutset.

1. Introduction

Symmetric networks such as Hopfield networks, Boltzmann machines, mean-field and Harmony networks are frequently investigated for use in optimization, constraint satisfaction
and approximation of NP-hard problems (Hopfield, 1982, 1984; Hinton & Sejnowski, 1986;
Peterson & Hartman, 1989; Smolensky, 1986; Brandt, Wang, Laub, & Mitra, 1988). These
models are characterized by a symmetric matrix of weights and a quadratic energy function that should be minimized. Usually, each unit computes the gradient of the energy
function and updates its own activation value so that the free energy decreases gradually.
Convergence to a local minimum is guaranteed although in the worst case it is exponential
in the number of units (Kasif, Banerjee, Delcher, & Sullivan, 1989; Papadimitriou, Shaffer,
& Yannakakis, 1990).
In many cases the problem at hand is formulated as a minimization problem and the
best solutions (sometimes the only solutions) are the global minima (Hopfield & Tank, 1985;
Ballard, Gardner, & Srinivas, 1986; Pinkas, 1991). The desired algorithm is therefore one
c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiPinkas and Dechter

that manages to reduce the impact of shallow local minima, thus improving the chances
of finding a global minimum. Some models such as Boltzmann machines and Harmony
nets use simulated annealing to escape from local minima. These models asymptotically
converge to a global minimum, meaning that if the annealing schedule is slow enough, a
global minimum is found. Nevertheless, such a schedule is hard to find and therefore, in
practice, these networks are not guaranteed to find a global minimum even in exponential
time.
In this paper we look at the topology of symmetric neural networks. We present an
algorithm that finds a global minimum for acyclic networks and otherwise optimizes treelike subnetworks in linear time. We then extend it to general topologies by dividing the
network into fictitious tree-like subnetworks using the cycle-cutset scheme.
The algorithm is based on the method of nonserial dynamic programming methods
(Bertele & Brioschi, 1972), which was also used for constraint optimization (Dechter,
Dechter, & Pearl, 1990). There the task was divided between a precompilation into a
tree structure via a tree-clustering algorithm and a run-time optimization over the tree.
Our adaptation is connectionist in style; i.e., the algorithm can be stated as a simple,
uniform activation function (Rumelhart, Hinton, & McClelland, 1986; Feldman & Ballard,
1982) and it can be executed in parallel architectures using synchronous or asynchronous
scheduling policies. It does not assume the desired topology (acyclic) and performs no worse
than the standard local algorithms for all topologies. In fact, it may be integrated with many
of the standard algorithms in such a way that the new algorithm out-performs the standard
algorithms by avoiding a certain class of local minima (along tree-like subnetworks).
Our algorithm is also applicable to an emerging class of greedy algorithms called local
repair algorithms. In local repair techniques, the problem at hand is usually formulated as a
minimization of a function that measures the distance between the current state and the goal
state (the solution). The algorithm picks a setting for the variables and then repeatedly
changes those variables that cause the maximal decrease in the distance function. For
example, a commonly used distance function for constraint satisfaction problems is the
number of violated constraints. A local repair algorithm may be viewed as an energy
minimization network where the distance function plays the role of the energy. Local repair
algorithms are sequential though, and they use a greedy scheduling policy; the next node to
be activated is the one leading to the largest change in the distance (i.e., energy). Recently,
such local repair algorithms were successfully used on various large-scale hard problems such
as 3-SAT, n-queen, scheduling and constraint satisfaction (Minton, Johnson, & Phillips,
1990; Selman, Levesque, & Mitchell, 1992). Since local repair algorithms may be viewed as
sequential variations on the energy minimization paradigm, it is reasonable to assume that
improvements in energy minimization will also be applicable to local-repair algorithms.
On the negative side, we show that in the presence of cycles, no uniform algorithm
exists that guarantees optimality even under a sequential asynchronous scheduler. An
asynchronous scheduler can activate only one unit at a time while a synchronous scheduler
can activate any number of units in a single time step. In addition, no uniform algorithm
exists to optimize even acyclic networks when the scheduler is synchronous. Those negative
results involve conditions on the parallel model of execution and therefore are applicable
only to the parallel versions of local repair.
224

fiImproving Connectionist Energy Minimization

The paper is organized as follows: Section 2 discusses connectionist energy minimization.
Section 3 presents the new algorithm activate and gives an example where it out-performs
the standard local algorithms. Section 4 discusses negative results, convergence under various schedulers and self-stabilization. Section 5 extends the approach to general topologies
through algorithm activate-with-cutset and suggests future research. Section 6 summarizes
and discusses applications.

2. Connectionist Energy Minimization

Given a quadratic energy function of the form:
n
n
X
X
E (X1; :::; Xn) = , wi;j Xi Xj , +i Xi:
i<j

i

Each of the variables Xi may have a value of zero or one called the activation value, and
the task is to find a zero/one assignment to the variables X1; :::Xn that minimizes the
energy function. To avoid confusion with signs, we will consider the equivalent problem of
maximizing the goodness function:
X
X
G(X1; :::; Xn) = ,E (X1; :::; Xn) = wi;j XiXj + i Xi
(1)
i<j

i

In connectionist approaches, we look at the network that is generated by assigning a node
(i) for every variable (Xi) in the function, and by creating a weighted arc (with weight wi;j )
between node i and node j , for every term wi;j Xi Xj . Similarly, a bias i is given to unit i, if
the term i Xi is in the function. For example, Figure 1 shows the network that corresponds
to the goodness function E (X1; :::; X5) = 3X2X3 , X1 X3 +2X3X4 , 2X4X5 , 3X3 , X2 +2X1.
Each of the nodes is assigned a processing unit and the network collectively searches for
an assignment that maximizes the goodness. The algorithm that is repeatedly executed in
each unit/node is called the activation function. An algorithm is uniform if it is executed
by all the units.
4
2
-3

-2

3

5

1

-1
-1

3
2

2

1

Figure 1: An example network
We give examples for two of the most popular activation functions for connectionist
energy minimization: the discrete Hopfield network (Hopfield, 1982) and the Boltzmann
225

fiPinkas and Dechter

machine (Hinton & Sejnowski, 1986). In the discrete Hopfield model, each unit computes
its activation value using the formula:
(
P w X  ,
1
if
i
j i;j j
Xi = 0 otherwise
In Boltzmann machines the determination of the activation value is stochastic and the
probability to set the activation value of a unit to one is:
P
P (Xi = 1) = 1=(1 + e,( w X + )=T );
j

i;j

j

i

where T is the annealing temperature. Both approaches may be integrated with our
topology-based algorithm; i.e., nodes that cannot be identified as parts of a tree-like topology use one of the standard local algorithms.

3. The Algorithm

We assume that the model of communication between neighboring nodes is a shared memory,
multi-reader, single-writer model. We also assume (for now) that scheduling is done with
a central scheduler (asynchronous) and that execution is fair. In a shared memory, multireader, single-writer each unit has a shared register called the activation register. A unit
may read the content of the registers of all its neighbors but write only its own. Central
scheduler means that the units are activated one at a time in an arbitrary order.1 An
execution is said to be fair if every unit is activated infinitely often. We do not require selfstabilization initially. Namely, algorithms may have an initialization step and can rely on
initial values. Later we will relax some of the assumptions above and examine the conditions
under which the algorithm is also self-stabilized.
The algorithm identifies parts of the network that have no cycles (tree-like subnetworks),
and optimizes the free energy on these subnetworks. Once a tree is identified, it is optimized
using a dynamic programming method that propagates values from leaves to a root and
back.
Let us assume first that the network is acyclic; any such network may be directed into a
rooted tree. The algorithm is based on the observation that given an activation value (0/1)
for a node in a tree, the optimal assignments for all its adjacent nodes are independent of
each other. In particular, the optimal assignment to the node's descendants are independent
of the assignments for its ancestors. Therefore, each node i in the tree may compute two
values: G1i is the maximal goodness contribution of the subtree rooted at i, including the
connection to i's parent whose activation is one. Similarly, G0i is the maximal goodness
of the subtree, including the connection to i's parent whose activation value is zero. The
acyclicity property will allow us to compute each node's G1i and G0i as a simple function of
its children's values, implemented as a propagation algorithm initiated by the leaves.
Knowing the activation value of its parent and the values G0j ; G1j of all its children, a
node can compute the maximal goodness of its subtree. When the information reaches the
1. Standard algorithms need to assume the same condition in order to guarantee convergence to a local
minimum (Hopfield, 1982). This condition can be relaxed by restricting that only adjacent nodes are
not activated at the same time (mutual exclusion).

226

fiImproving Connectionist Energy Minimization

root, it can assign a value (0/1) that maximizes the goodness of the whole network. The
assignment information propagates now toward the leaves. Knowing the activation value of
its parent, a node can compute the preferred activation value for itself. At termination (at
stable state), the tree is optimized. The algorithm has 3 basic steps:
1.

Directing a tree: knowledge is propagated from leaves toward the center so that

after a linear number of steps, every unit in the tree knows its parent and children.
2. Propagation of goodness values: the values (G1i and G0i ), are propagated from
leaves to the root. At termination, every node knows the maximal goodness of its
subtree and the appropriate activation value it should assign given that of its parent.
In particular, the root can now decide its own activation value so as to maximize the
whole tree.
3. Propagation of activation values: starting with the root, each node in turn determines its activation value. After O(depth of tree) steps, the units are in a stable
state which globally maximizes the goodness.
Each unit's activation register consists of the following fields: Xi: the activation value;
0
Gi and G1i : the maximal goodness values; and (Pi1 ; ::; Pij ): a bit for each of the j neighbors
of i that indicates i's parent.

3.1 Directing a tree

The goal of this algorithm is to inform every node of its role in the network and its childparent relationships. Nodes with a single neighbor identify themselves as leaves first and
then identify their neighbor as a parent (point to it). A node identifies itself as a root when
all neighbors point toward it. When a node's neighbors but one point toward it, the node
selects the one as a parent. Finally, a node that has at least two neighbors not pointing
toward it, identifies itself as being outside the tree.
The problem of directing a tree is related to the problem of selecting a leader in a
distributed network, and of selecting a center in a tree (Korach, Rotem, & Santoro, 1984).
Our problem differs (from general leader selection problems) in that the network is a tree. In
addition, we require our algorithms to be self-stabilized. A related self-stabilizing algorithm
appeared earlier (Collin, Dechter, & Katz, 1991). That algorithm is based on finding a center
of the tree as the root node and therefore creates more balanced trees. The advantage of
the algorithm presented here is that it is space ecient requiring only O(logd) space, when
d is the maximum number of neighbors each node has. In contrast, the algorithm in Collin
et al. requires O(logn), n being the network size.
In the algorithm we present, each unit uses one bit per neighbor to keep the pointing
information: Pij = 1 indicates that node i sees its j th neighbor as its parent. By looking at
Pji , node i knows whether j is pointing to it.
Identifying tree-like subnetworks in a general network may be done by the algorithm in
Figure 2.
In Figure 3a, we see an acyclic network after the tree directing phase. The numbers
on the edges represent the values of the Pij bits. In Figure 3b, a tree-like subnetwork is
227

fiPinkas and Dechter

Tree Directing (for unit i):

1. Initialization: If first time, then for all neighbors j : Pij = 0; /* Start with clear
pointers (step is not needed in acyclic nets or with almost uniform versions)
2. If there is only a single neighbor (j ) and Pji = 0, then Pij = 1; /* A leaf selects its
neighbor as parent if that neighbor doesn't point to it */
3. else, if one and only one neighbor (k) does not point to i (Pki = 0), then
Pik = 1, and for the rest of the neighbors: Pij = 0. /* k is the parent */

4. else, for all neighbors j : Pij = 0. /* Node is either a root or outside the tree */
Figure 2: Tree directing algorithm

identified inside a cyclic network. Note that node 5 is not a root since not all its neighbors
are pointing toward it.
7

0

7

0
0

0

5 1

5

6

0

5

6

1 4

4

0

3
0
1

1

0

4

1

3

0

3
0
1

1

1

1

2

2

1

4

0

3

5 0

0
1

1

2

2

(a)

(b)

Figure 3: Directing a tree: a) A tree b) A cyclic network with a tree-like subnetwork.

3.2 Propagation of goodness values

In this phase every node i computes its goodness values G1i and G0i , by propagating these
two values from the leaves to the root (see Figure 4).
Given a node Xi , its parent Xk and its children, children(i) in the tree, it can be shown,
based on the energy function (1), that the goodness values obey the following recurrence:
X
GXi = maxX 2f0;1gf
GXj + wi;k Xi Xk + i Xi g
i

k

i

j 2children(i)

Consequently a nonleaf node i computes its goodness values using the goodness values of
its children as
follows: If Xk = 0, then i must decide between P
setting Xi = 0, obtaining a
P
0
goodness of j Gj , or setting Xi = 1, obtaining a goodness of j G1j + i . This yields:
X
X
G0i = maxf
G0j ;
G1j + i g
j 2children(i)

228

j 2children(i)

fiImproving Connectionist Energy Minimization

G0 = 2
4

G1 = 3
4

X4 = 0

4

3
0
G =2
3

1
G =2
3

1

3

1

3

2

0
G =1
5

1

1
G 21 = 2

X3 = 0

5

2
G0 = 0

0

4

2

2

G1 = 2

3

5

1
G =0
5

X5 = 1

2
1

G1 = 1

2

1

X2 = 0

X1 = 1
(b)

(a)

Figure 4: a) Propagating goodness values. b) Propagating activation values.
Similarly, when Xk = 1, the choice between Xi = 0 and Xi = 1, yields:
X
X
G1i = maxf
G0j ;
G1j + wi;k + i g
j 2children(i)

j 2children(i)

The initial goodness values for leaf nodes can be obtained from the above (no children).
Thus, G0i = maxf0; ig, G1i = maxf0; wik + i g.
For example, if unit 3 in Figure 4 is zero then the maximal goodness contributed
by node 1 is G01 = maxX1 2f0;1gf2X1g = 2 and is obtained at X1 = 1. Unit 2 (when
X3 = 0) contributes G02 = maxX2 2f0;1gf,X2 g = 0 obtained at X2 = 0, while G12 =
maxX2 2f0;1gf3X2 , X2g = 2 is obtained at X2 =P1. As for nonleaf nodes, if X4 = 0, then
when X3 = 0, the goodnessPcontribution will be k G0k = 2 + 0 = 2, while if X3 = 1, the
contribution will be ,3 + k G1k = ,3 + 1 + 2 = 0. The maximal contribution G03 = 2 is
achieved at X3 = 0.
Goodness values may be computed once for every node when its children's goodness
values are ready; however, for self-stabilization (to be discussed later) and for simplicity,
nodes may compute their goodness values repeatedly and without synchronization with
their children.

3.3 Propagation of activation values

Once a node is assigned an activation value, all its children can activate themselves so as to
maximize the goodness of the subtrees they control. When such value is chosen for a node,
its children can evaluate their activation values, and the process continues until the whole
tree is assigned.
There are two kinds of nodes that may start the process: a root which will choose an
activation value to optimize the entire tree, and a non-tree node which uses a standard
activation function.
P
When a root Xi is identified,Pif the maximal goodness is j G0j , it chooses the value
\0." If the maximal goodness is j G1j + i , it chooses \1." In summary, the root chooses
its value according to:
(
P G1 +   P G0
1
if
i
j j
j j
Xi = 0 otherwise
229

fiPinkas and Dechter

In Figure 4 for example, G15 + G13 + 0 = 2 < G05 + G03 = 3 and therefore X4 = 0. P
An internal node whose parent is k chooses an activation
value that maximizes Pj Gxj +
P
wi;k XiXk + i Xi. The choice therefore, is between j G0j (when Xi = 0) and j G1j +
wi;k Xk + i (when Xi = 1), yielding:
(
P G1 + w X +   P G0
i;k k
i
j j
j j
Xi = 10 ifotherwise
i

As a special case, a leaf i chooses Xi = 1 if wi;k Xk  ,i , which is exactly the discrete
Hopfield activation function for a node with a single neighbor. For example, in Figure 4,
X5 = 1 since w4;5X4 = 0 > ,5 = ,1, and X3 = 0 since G11 + G12 +2X4 + 3 = 1+2+0 , 3 =
0 < G02 + G01 = 2. Figure 4b shows the activation values obtained by propagating them
from the root to the leaves.

3.4 A complete activation function

Interleaving the three algorithms described earlier achieves the goal of identifying tree-like
subnetworks and maximizes their goodness. In this subsection we present the complete
algorithm, combining the three phases while simplifying the computation. The algorithm
is integrated with the discrete Hopfield activation function demonstrating how similar the
formulas are.
The steps of the algorithm can be interleaved freely; i.e., a scheduler might execute
each step for all the nodes or all steps for any given node (or combinations). These steps
are computed repeatedly with no synchronization with the node's neighbors.2 Algorithm
activate executed by unit i (when j denotes a non-parent neighbor of i and k denotes the
parent of i) is given in Figure 5. Algorithm activate improves on an arbitrary local search
connectionist algorithm in the following sense:

3.1 If a1 is a local minimum generated by \activate" and a2 is a local minimum
generated by a local-search method (e.g., Hopfield), and if a1 and a2 have the same activation
values on non-tree nodes, then G(a1)  G(a2).
Theorem

Proof: Follows immediately from the fact that activate generates a global minimum on
tree-subnetworks. 2
Additional properties of the algorithm will be discussed in Section 4.

3.5 An example

The example illustrated in Figure 6 demonstrates a case where a local minimum of the
standard algorithms is avoided. Standard algorithms may enter such local minimum and
stay in a stable state that is clearly wrong.
The example is a variation on a Harmony network example (Smolensky, 1986) (page
259), and (McClelland, Rumelhart, & Hinton, 1986) (page 22). The task of the network
is to identify words from low-level line segments. Certain patterns of line segments excite
2. As we will see later, the amount of parallelism will have to be limited somewhat, as from time to time
two neighboring nodes should not execute the tree-directing step at the same time.

230

fiImproving Connectionist Energy Minimization

Algorithm activate: Optimizing on Tree-like Subnetworks (unit i):

1. Initialization: If first time, then (8j ) Pij = 0; /*Clear pointers (cyclic nets)*/
2. Tree directing: If there exists a single neighbor k, such that Pki = 0,
then Pik = 1 and for all other neighbors j , Pij = 0;
else, for all neighbors Pij = 0;
3. Computing P
goodness values: P
0
Gi = maxf j2neighbors(i) G0j Pji ; j 2neighbors(i) G1j Pji + i g;
G1i = maxfPj2neighbors(i) G0j Pji ; Pj 2neighbors(i)(G1j Pji + wi;j Pij ) + i g;
4. Assigning activation values:
If at least( two neighbors
pointing to i, then /*Not in tree: use Hopfield*/
P w Xarenot
1
if
,

i
j i;j j
Xi = 0 otherwise
else, /*
root and leaves) */
( Node inPa tree1 (including
0)P i + wi;j Xj P j )  ,i
1
if
((
G
,
G
j j
j j
i
Xi = 0 otherwise
Figure 5: Algorithm activate for unit i

units that represent characters, and certain patterns of characters excite units that represent
words. The line strokes used to draw the characters are the input units: L1,..., L5. The
units \N," \S," \A" and \T" represent characters. The units \able," \nose," \time" and
\cart" represent words, and Hn, Hs, Ha, Ht, H1,... H4 are hidden units required by the
Harmony model. For example, given the line segments of the character S, unit L4 is activated
(input), and this causes units Hs and \S" to be activated. Since \NOSE" is the only word
that contains the character \S," both H2 and the unit \nose" are also activated and the
word \NOSE" is identified.
The network has feedback cycles (symmetric weights) so that ambiguity among characters or line-segments may be resolved as a result of identifying a word. For example, assume
that the line segments required to recognize the word \NOSE" appear, but the character
\N" in the input is blurred and therefore the setting of unit L2 is ambiguous. Given the
rest of the line segments (e.g., those of the character \S"), the network identifies the word
\NOSE" and activates units \nose" and H2. This will cause unit \N" and all of its line
segments to be activated. Thus, the ambiguity of L2 is resolved.
The network is designed to have a global minimum when L2, Hn, \N," H2 and \nose"
are all activated. However, standard connectionist algorithms may fall into a local minimum
when all these units are zero, generating goodness of 5 , 4 = 1. The correct setting (global
minimum) is found by our tree-optimization algorithm (with goodness: 3-1+3-1+3-1+5-14+3-1+5=13). The thick arcs in the upper network of Figure 6 mark the arcs of a tree-like
subnetwork. This tree-like subnetwork is drawn with pointers and weights in the lower
part of the figure. Node \S" is not part of the tree and its activation value is set to one
231

fiPinkas and Dechter

Hn

Hs

Ha

Ht

H1

N
L1

L2

L4
L3
Linesegments

A

S

able
Characters

4
Hn

1

H2
3

H3

H4

T

L5

1
3

H2

Hs

nose
time
Words

cart

cyclic subnetwork

3
5
1

1
L2

N

S

1

1

nose

Figure 6: A Harmony network for recognizing words: local minima along the subtrees are
avoided.
because the line-segments of \S" are activated. Once \S" is set, the units along the tree are
optimized (by setting them all to one) and the local minimum is avoided.

4. Feasibility, Convergence, and Self-Stabilization

So far we have shown how to enhance the performance of connectionist energy minimization
networks without losing much of the simplicity of the standard approaches. The simple
algorithm presented is limited in three ways, however. First, it assumes unrealistically that
a central scheduler is used; i.e., a scheduler that activates the units one after the other
asynchronously. The same results are obtained if the steps of the algorithm executes as
one atomic operation or if neighbors are mutually excluded. We would like the network
to work correctly under a distributed (synchronous) scheduler, where any subset of units
may be activated for execution at the same time synchronously. Second, the algorithm
guarantees convergence to global optima only for tree-like subnetworks. We would like to
find an algorithm that converges to correct solutions even if cycles are introduced. Finally,
we would like the algorithm to be self-stabilizing. It should converge to a legal, stable state
given enough time, even after noisy uctuations that cause the units to execute arbitrary
program states and the registers to have arbitrary content. Formally, an algorithm is selfstabilizing if in any fair execution, starting from any input configuration and any program
state (of the units), the system reaches a valid stable configuration.
In this section, we illustrate two negative results regarding the first two problems; i.e.,
that it is not feasible to build uniform algorithms for trees under a distributed scheduler,
and that such an algorithm is not feasible for cyclic networks even under a central scheduler.
232

fiImproving Connectionist Energy Minimization

We then show how to weaken the conditions so that convergence is guaranteed (for tree-like
subnetworks) in realistic environments and self-stabilization is obtained.
A scheduler can generate any specific schedule consistent with its definition. Thus, the
central scheduler can be viewed as a specific case of the distributed scheduler. We say
that a problem is impossible for a scheduler if for every possible algorithm there exists a
fair execution generated by such a scheduler that does not find a solution to the problem.
Since all the specific schedules generated by a central scheduler can also be generated by
a distributed scheduler, what is impossible for a central scheduler is also impossible for a
distributed scheduler.

4.1 Negative results for uniform algorithms

Following Dijkstra (1974), negative results were presented regarding the feasibility of distributed constraint satisfaction (Collin et al., 1991). Since constraint satisfaction problems
can be formulated as energy minimization problems, these feasibility results apply also for
computing the global minimum of energy functions. For completeness we now adapt those
results for a connectionist computation of energy minimization.
Theorem 4.1 No deterministic3 uniform algorithm exists that guarantees a global minimum under a distributed scheduler, even for simple chain-like trees, assuming that the
algorithm needs to be insensitive to initial conditions.
Proof (By counter example): Consider the network of Figure 7. There are two global minima
possible : (11:::1101:::1) and (11:::1011:::1) (when the four centered digits are assigned to
units, i , 1; i; i + 1; i + 2). If the network is initialized such that all units have the same
register values, and all units start with the same program state, then there exists a fair
execution under a distributed scheduler such that in every step all units are activated. The
units left of the center (1; 2; 3; :::i) \see" the same input as those units right of the center
(2i; 2i , 1; 2i , 2; :::; i + 1) respectively. Because of the uniformity and the determinism,
the units in each pair (i; i + 1); (i , 1; i + 2); :::; (1; 2i) must transfer to the same program
state and produce the same output on the activation register. Thus, after every step of
that execution, units i and i + 1 will always have the same activation value and a global
minimum (where the two units have different values) will never be obtained. 2
This negative result should not discourage us in practice since it relies on an obscure
infinite sequence of executions which is unlikely to occur under a random scheduler. Despite
this negative result, one can show that algorithm activate will optimize the energy of treelike subnetworks under a distributed scheduler if at least one of the following cases holds
(see the next section for details):
1. If step 2 of algorithm activate in Section 3.4 is atomic; i.e., no other neighbor may
execute step 2 at the same time.
2. if for every node i and every neighbor j , node i is executed without j infinitely often
(fair exclusion);
3. if one node is unique and acts as a root, that is, does not execute step 2 (an almost
uniform algorithm);
3. The proof of this theorem assumes determinism and does not apply to stochastic activation functions.

233

fiPinkas and Dechter

1

1
1
1

1
1

2

1
1

i1

1

1
5

i

1

1
i+1

1

i+2

1
2i

Global Minima: 11...1101...1
11...1011...1

Figure 7: No uniform algorithm exists to optimize chains under distributed schedulers.
4. if the network is cyclic (one node will be acting as a root).4
Another negative result similar to (Collin et al., 1991) is given in the following theorem.
Theorem 4.2 If the network is cyclic, no deterministic uniform algorithm exists that guarantees a global minimum, even under a central scheduler, assuming that the algorithm needs
to be insensitive to initial conditions.
Proof (by counter example): This may be proved even for cyclic networks as simple as rings.
In Figure 8 we see a ring-like network whose global minima are (010101) and (101010).
Consider a fair execution under a central scheduler that activates the units 1,4,2,5,3,6 in
order and repeats this order indefinitely. Starting with the same program state and same
inputs, the two units in every pair of (1,4), (2,5), (3,6) \see" the same input, therefore they
have the same output and transfer to the same program state. As a result, these units never
output different values and a global minimum is not obtained. 2
Note that any tree-like subnetwork of a cyclic network will be optimized even under a
distributed scheduler (since nodes that are part of a cycle are identified as roots and the
algorithm acts as an almost uniform algorithm).

4.2 Convergence and self-stabilization

In the previous subsection we proved that under a pure distributed scheduler there is no
hope for a uniform network algorithm. In addition, we can easily show that the algorithm
is not self-stabilizing when cycles are introduced. For example, consider the configuration
of the pointers in the ring of Figure 9. It is in a stable state although clearly not a valid
tree.5
In this subsection we weaken the requirements allowing our algorithm to converge to correct solutions and to be self-stabilizing under realistically weaker distributed schedulers. We
will not use the notion of a pure distributed scheduler; instead, we will ask our distributed
scheduler to have the fair exclusion property.
4. Global solutions are not guaranteed to be found but all tree-like subnetworks will be optimized.
5. Such configuration will never occur if all units start at the starting point; i.e., clearing the bits of P . It
may only happen due to some noise or hardware uctuations.
i

234

fiImproving Connectionist Energy Minimization

1

1
4

3
5

3
1

3

3

1

6
3

Global Minima: 010101
101010

3
2

1

3
1

Schedule: 1, 4, 2, 5, 3, 6, 1, 4, 2, 5,......

1

Figure 8: No uniform algorithm exists that guarantees to optimize rings even under a central
scheduler.

1

7

1

0

6

0

1 0

5

Figure 9: The uniform algorithm is not self-stabilizing in cyclic networks.
Definition 4.1 A scheduler has the fair exclusion property if for every two neighbors, one

is executed without the other infinitely often.
Intuitively, a distributed scheduler with fair exclusion will no longer generate infinite sequences of the pathological execution schedules used in the previous subsection to prove the
negative results. Instead, it is guaranteed that from time to time, every two neighboring
units will not execute together.
As an alternative, we might weaken the requirement on the uniformity of the algorithm
(that all nodes execute the same procedure). An almost uniform algorithm is when all
the nodes perform the same procedure except one node that is marked unique. In the
almost uniform version of algorithm activate, the root of the tree is marked and executes
the procedure of Section 3.4 as if all its neighbors are pointing to it; i.e., it constantly sets
Pij to zero.

4.3 Algorithm activate of Section 3.4 has the following properties: 1. It converges to a global minimum and is self-stabilizing6 in networks with tree-like topologies under
a distributed scheduler with fair exclusion. 2. The algorithm also converges in tree-like subnetworks (but is not self-stabilizing) when the network has cycles. 3. It is self-stabilizing
for any topology if an almost uniform algorithm is applied, even under a pure distributed
scheduler.
Theorem

6. The initialization step of the algorithm is omitted in the self-stabilizing version.

235

fiPinkas and Dechter

For proof see appendix.

5. Extensions to Arbitrary Networks

The algorithm we presented in Section 3 is limited in that it is restricted to nodes of tree-like
subnetworks only. Nodes that are part of a cycle execute the traditional activation function
which may lead to the known drawbacks of local energy minima and slow convergence. In
this section we discuss generalizations of our algorithm to nodes that are part of cycles,
that will work well for near-tree networks. A full account of this extension is deferred for
future work.
A well known scheme for extending tree algorithms to non-tree networks, is cycle-cutset
decomposition (Dechter, 1990), used in Bayes networks and constraint networks. Cyclecutset decomposition is based on the fact that an instantiated variable cuts the ow of
information on any path on which it lies and therefore it changes the effective connectivity
of the network. Consequently, when the group of instantiated variables cuts all cycles in
the graph, (e.g., a cycle-cutset), the remaining network can be viewed as cycle-free and can
be solved by a tree algorithm. The complexity of the cycle-cutset method can be bounded
exponentially in the size of the cutset in each connected component of the graph (Dechter,
1992). We next show how to improve our energy minimization algorithm, activate using
the cycle-cutset idea.
Recall that the energy minimization task is to find a zero/one assignment to the variables X = fX1; :::; Xng that maximizes the goodness function. Define Gmax(X1; :::; Xn) =
maxX1;:::;X G(X1; :::; Xn). The task is to find an activation level X1; :::; Xn satisfying
X
X
Gmax(X1; :::Xn) = maxX1 ;:::;X ( wi;j XiXj + i Xi):
(2)
n

n

i<j

i

Let Y = fY1 ; :::; Ykg be a subset of the variables X = fX1; :::; Xng. The maximum can
be computed in two steps. First compute the maximum goodness conditioned on a fixed
assignment Y = y , then maximize the resulting function over all possible assignments to Y .
Let Gmax(X jY = y ), be the maximum goodness value of G conditioned on Y = y . Clearly,
Gmax(X ) = maxY =y Gmax(X jY = y ) = maxY =y maxfX =xjx =yg fG(X )g;
Y

where, xY is the zero/one value assignments in the instantiation x that are restricted to
the variable subset Y . If the variables in Y form a cycle-cutset, then the conditional
maxima Gmax(X jY = y ) can be computed eciently using a tree algorithm. The overall
maxima may be achieved subsequently by enumerating over all possible assignments to Y .
Obviously, this scheme is effective only when the cycle-cutset is small. We next discuss
some steps towards implementing this idea in a distributed environment.
Given a network with a set of nodes X = fX1; :::; Xng, and a subset of cutset variables
Y = fY1; :::; Ykg, presumably a cycle-cutset, and assuming a fixed, unchangeable assignment
Y = y, the cutset variables behave like leaf nodes, namely, they select each of their neighbors
as a parent if that neighbor does not point to them. Thus, a cutset variable may have several
parents and zero or more child nodes.
Considering again the example network in Figure 3b and assuming node (7) is a cutset
variable, a tree-directing may now change so that node (7) points both to (5) and to (6),
236

fiImproving Connectionist Energy Minimization

(6) points to (5) and (5) remains the root. Note that with this modification all arcs are
directed and the resulting graph is an acyclic directed graph. Once the graph is directed,
each regular non-cutset node has exactly the same view as before. It has one parent (or
no parent) and perhaps a set of child nodes, some of which may be cutset nodes. It then
computes goodness values and activation values almost as before.
An algorithm along these lines will compute the maximum energy conditioned on Y = y ,
if Y is a cycle-cutset. Note however that such an assignment is not guaranteed to converge
to a local maxima of the original (Hopfield) activation function. Some of the cutset nodes
may be unstable relative to this function.
Enumerating all the conditional maxima to get a global maxima cannot be done distributedly, unless the cutset size is small. When the cutset is small, the computation can
be done in parallel, yielding a practical distributed solution for networks, as follows. Once
the tree-directing part is accomplished, a node computes a collection of goodness values,
each indexed by a conditioning assignment Y = y . The goodness values of a node that are
associated with the cutset assignment Y = y will be computed using the goodness values
of child nodes that are also associated with the same assignment Y = y . The maximum
number of goodness values each node may need to carry is exponential in the cutset size.
Upon convergence, the roots of the trees will select an assignment Y = y that will maximize the overall goodness value and propagate this information down the tree so that nodes
will switch values accordingly. The above algorithm is certainly not in the connectionist
spirit and is practically limited to small cutsets. Its advantage is that it finds a true global
optimum.
In the following subsection, we will modify the cutset approach more towards the connectionist spirit by integrating the cutset scheme with a standard energy minimizing activation
function. This yields a connectionist-style algorithm with a simple activation function and
limited memory requirements once the identity of the cycle-cutset nodes is known. We can
determine the cutset variables initially using a centralized algorithm for computing a small
cutset (Becker & Geiger, 1994). Although not guaranteed to find a global solution, the new
activation function is more powerful than standard approaches on cyclic topologies.

5.1 Local search with cycle cutset

Algorithm activate-with-cutset in Figure 10 assumes that the cutset nodes are known a
priori; this time however, their values are changing using standard local techniques (e.g.,
Hopfield). The algorithm is well-defined also when the cutset nodes do not cut all cycles or
when the cutset is not minimal. However, it is likely to work best when the cutset is small
and when it cuts all cycles.
Note that the goodness value computation of cutset nodes (step 4) is not performing
the maximization operation over the two possible activation values of the cutset variables
since the activation value of cutset nodes is fixed as far as the tree algorithm is concerned.
Intuitively, if performed sequentially the algorithm would iterate between the following
two steps: 1. finding a local maximum using Hopfield activation function for the cutset
variables; 2. finding a global maximum conditioned on the cutset values determined in the
previous step via the tree algorithm. In the connectionist framework these two steps are
not synchronized. Nevertheless, the algorithm will converge to a local maxima relative to
237

fiPinkas and Dechter

Algorithm activate-with-cutset (unit i)
Assumption: The cutset nodes are given a priori.
1. Initialization: If first time, then (8j ) Pij = 0;
2. Tree directing:
If i is a cutset node, for every neighbor (j ), if Pji = 0, then Pij = 1;
(neighbors become parents unless they already point to it)
else (not a cutset node), if there exists a single neighbor k, such that Pki = 0,
then (part of a tree but not a root) Pik = 1 and for all other neighbors j , Pij = 0;
else (root or non-tree node), for all neighbors Pij = 0;
3. Assigning activation values:
If all neighbors
ofPi point to it except maybe one (i.e., it is part of a tree) then,
(
j
1
0 i
j ((Gj , Gj )Pj + wi;j Xj Pi )  ,i
Xi = 10 ifotherwise
else (a cutset
( nodePor a node that is not yet part of any tree), Compute Hopfield:
j wi;j Xj  ,i
Xi = 10 ifotherwise
4. Computing goodness values: (only nodes in trees need goodness values)
If i is a cutset node, then For each neighbor j ,
G0i = Xi i ,
Gji 1 = Xi (i + wij ) (G0i ; Gji 1 are goodness values for neighbor j ).
else (a regular P
tree node),
0
Gi = maxf j2neighbors(i) G0j Pji ; Pj 2neighbors(i) G1j Pji + ig;
G1i = maxfPj2neighbors(i) G0j Pji ; Pj 2neighbors(i)(G1j Pji + wi;j Pij ) + i g;
Figure 10: Algorithm activate-with-cutset

238

fiImproving Connectionist Energy Minimization

Hopfield algorithm as well as a conditional global maxima relative to the cutset variables.
Convergence follows from the fact that the tree directing algorithm is guaranteed to converge
given fixed cutset variables. Once it does, a node ips its value either as a result of a Hopfield
step or in order to optimize a tree. In both steps the energy does not increase.7
Example

5.1 The following example demonstrates how the algorithm finds a better min-

imum than what is found by the standard Hopfield algorithm when there are cycles. Consider the energy function: energy = 50AB , 200BC , 100AC , 3AD , 3DE , 3AE +
0:1A + 0:1B + 0:1C + 4E + 4D. The associated network consists of two cycles: A; B; C
and A; D; E . If we select node A as a cutset node, the network would then be cut into
two acyclic (tree-like) subnetworks. Assume that the network starts with a setting of zeros
(A; B; C; D; E = 0). This is a local minimum (energy = 0) of the Hopfield algorithm. Our
activate-with-cutset algorithm breaks out of this local minimum by optimizing the acyclic
subnetwork A; B; C conditioned on A = 0. The result of the optimization is the assignment
A = 0; B = 1; C = 1; D = 0; E = 0 with energy = ,199:7. It is not a stable state because
A obtains an excitatory sum of inputs (50) and therefore ips its value to A = 1 using its
Hopfield activation algorithm. The new state A; B; C = 1; D; E = 0 is also a local minimum of the Hopfield paradigm (energy = ,249:7). However, since nodes A; D; E form a
tree, the activate-with-cutset algorithm also manages to break out of this local minimum.
It finds a global solution conditioned on A = 1 which happens to be the global minimum
A; B; C; D; E = 1 with energy = ,250:97. The new algorithm was capable of finding the
only global minimum of the energy function and managed to escape two of the local minima
that trapped the Hopfield algorithm.
It is easy to see that algorithm activate-with-cutset improves on activate in the following
sense:
Theorem 5.1 If a1 is a local minimum generated by activate and a2 is a local minimum
generated by activate-with-cutset, then if a1 and a2 have the same activation value on all
non-tree nodes then, G(a2)  G(a1).

5.2 Local search with changing cutset variables

We can imagine a further extension of the cutset scheme idea that will improve the resulting energy level further by conditioning and optimizing relative to many cutsets. In
a sequential implementation the algorithm will move from one cutset to the next, until
there is no improvement. This process is guaranteed to monotonically reduce the energy.
It is unclear however how this tour among cutsets can be implemented in a connectionist
environment. It is not clear even how to identify one cutset distributedly. Since finding a
minimal cycle-cutset is NP-complete a distributed algorithm for the problem is unlikely to
exist. Nevertheless, there could be many brute-force distributed algorithms that may find
a good cutset in practice. Alternatively, cutset nodes may be selected by a process that
randomly designates a node to be a cutset node.
7. Fluctuations that temporarily increase the energy may occur from time to time before the tree propagation has completely stabilized. For example, a node may use the goodness values of its children before
their goodness values are ready (but after they point to the node).

239

fiPinkas and Dechter

In the following paragraphs we outline some ideas for a uniform connectionist algorithm
that allows exploration of the cutset space. We propose the use of a random function to
control the identity of the cutset nodes. The random process by which a node becomes a
cutset node or switch from a cutset node to a regular node may be governed by a random
heuristic function f (). A non-tree node may turn into a cutset node with probability
P = f (). A cutset node may turn into a non-cutset node if it becomes part of a tree or
by the random process with probability P = g (). The function f () should be designed in a
way that it will assign high probabilities to nodes with potential to become \good" cutset
nodes. The probability of de-selecting a cutset node may be defined as g () = 1 , f ().
Algorithm activate-with-cutset can be augmented with a cutset selection function that
will be running in parallel with the three procedures (tree-directing, assigning activation
values and goodness computing). Thus, we may add a forth procedure that selects (or
de-selects) the node as a cutset node with probability P = f (). Note that the randomly
selected cutset is not perfect and that there might be too many or too few cutset nodes.
As long as there are cycles, cutset nodes should be selected. At the same time, nodes
functioning too long as cutset nodes should be de-selected thus reducing the chances for
redundant cutset nodes while continuously exploring the space of possible cutsets.
One way to implement a heuristic function f is to base it on the following ideas: 1.
Increase probability to non-tree nodes that have not been cutset nodes for a long time. 2.
Increase probability to nodes that have not ipped their value for a long time. 3. Increase
the probability to nodes with high connectivity.
Note that a de-selected cutset node may cause a chain reaction of undirecting nodes.
Nodes that lost their tree-pointers become not-part-of-tree and thus have a potential to
become cutset nodes. The network may continue the tour in the cutset space indefinitely
and may never become static. The selection-de-selection process may never converge. Nevertheless, if a function f is designed to allow enough time for convergence in between cutset
changes than during the whole process, the energy tends to decrease. Temporary uctuations may sometimes cause an energy increase when a node relies on its not yet stable
neighbors.8 We conjecture that such a heuristic function f can be constructed so as to allow
trees to be stabilized before they are distroyed by de-selection. Formalizing the algorithm's
properties and further investigation and experimentation are left for future research.

6. Conclusions

The main contributions of the paper are:
1. We provide a connectionist activation function (algorithm activate, Figure 5), that
is self-stabilizing and is guaranteed to converge to a global minima in linear time for
tree-like networks. On general networks the algorithm will generate a global minima
on all tree subnetworks and on the rest of the network it will coincide with regular local
gradient activation functions (e.g., Hopfield). The algorithm dominates an arbitrary
local search connectionist algorithm in the following sense: If a1 is a local minimum
generated by activate and a2 is a local minimum generated by a corresponding localsearch method, then if a1 and a2 have the same activation values on all non-tree nodes
8. For example, temporarily relying on old goodness values of a de-selected node.

240

fiImproving Connectionist Energy Minimization

(if it is a tree then the set is empty), then the energy of a1 is smaller or equal to the
energy of a2.
2. We showed that activate can be further extended using the cycle-cutset idea. The
extended algorithm called activate-with-cutset (Figure 10) is guaranteed to converge
and generate solutions that are at least as good and normally better than algorithm
activate. The algorithm converges to conditional global minima relative to the values
of the cutset variables. If a1 is a local minima generated by activate and a2 is the local
minima generated by activate-with-cutset then if a1 and a2 have the same activation
values on all the cutset variables (if it is a tree, the cutset is empty) than the energy
of a2 is smaller or equal to the energy of a1 . Therefore activate-with-cutset is better
than activate which in turn is better than a regular energy-minimization connectionist
algorithm in the above sense. A third variation of the algorithm is sketched for
future investigation. The idea is that the cutset nodes are randomly and continuously
selected, thus allowing exploration of the cutset space.
3. We stated two negative results: 1) Under a pure distributed scheduler no uniform
algorithm exists to globally optimize even simple chain-like networks. 2) No uniform
algorithm exists to globally optimize simple cyclic networks (rings) even under a
central scheduler. We conjecture that these negative results are not of significant
practical importance since in realistic schedulers the probability of having infinite
pathological scheduling scenarios approaches zero. We showed that our algorithm
converges correctly (on tree-like subnetworks) when the demand for pure distributed
schedulers is somewhat relaxed; i.e., adding either fair exclusion, almost uniformity
or cycles. Similarly, self-stabilization is obtained in acyclic networks or when the
requirement for a uniform algorithm is relaxed (almost uniformity).
The negative results apply to connectionist algorithms as well as to parallel versions of
local repair search techniques. The positive results suggest improvements both to connectionist activation functions and to local repair techniques.
We conclude with a discussion of two domains that are likely to produce sparse, neartree networks and thus benefit from the algorithms we presented: inheritance networks and
diagnosis.
Inheritance is a straightforward example of an application where translations of symbolic rules into energy terms form networks that are mostly cycle free. Each arc of an
inheritance network, such as A ISA B or A HAS B is modeled by the energy term A , AB .
The connectionist network that represents the complete inheritance graph is obtained by
summing the energy terms that correspond to all the ISA and HAS relationships in the
graph. Nonmonotonicity can be expressed if we add penalties to arcs and use the semantics
discussed by Pinkas (1991b, 1995). Nonmonotonic relationships may cause cycles both in
the inheritance graph and the connectionist network (e.g. Penguin ISA Bird; Bird ISA
FlyingAnimal; Penguin ISA not(FlyingAnimal)). Multiple inheritance may cause cycles as
well, even when the rules are monotonic (e.g., Dolphin ISA Fish; Dolphin ISA Mammal;
Fish ISA Animal; Mammal ISA Animal). Arbitrary constraints on the nodes of the graph
may be introduced in this model. Constraints may be represented as proposition logic formulas and then translated into energy terms (Pinkas, 1991) potentially causing cycles. In a
241

fiPinkas and Dechter

\pure" inheritance network that has no multiple inherited nodes and no nonmonotonic relationships, the network is cycle-free and can be processed eciently by various algorithms. If
we allow multiple inheritance, nonmonotonicity, or arbitrary propositional constraints, we
may introduce cycles into the network that are generated. Nevertheless, it is reasonable to
assume that in large practical inheritance domains cycles (multiple inheritance, nonmonotonicity and arbitrary constraints) are only scarcely introduced and the few that exist may
be handled by our extension using the cycle-cutset idea.
Another potential application that will generate mostly cycle-free subnetworks is diagnosis. Here is a possible formulation of a diagnosis framework. Let X1; X2; :::Xn be
True(1)/false(0) propositions that represent symptoms and hypotheses. In a diagnosis
application we may have diagnosis rules of the form: (ff1X 1; ff2X2 ; :::; ffmXm ! fiX ).
These rules announce that the symptoms X1; :::; Xm with importance factors ff1 ; :::; ffm,
suggest the hypothesis X with sensitivity fi . A subset of the symptoms may be enough
to suggest the hypothesis if the sum of the importance factors of the active symptoms
is larger than the sensitivity fi . Intuitively, the larger the sum of the factors, the larger
thePsupport for thePhypothesis. The corresponding energy function for a diagnosis rule
is mi ,ffi Xi X + mi ffi Xi + fiX . In addition, arbitrary propositional constraints may
also be added, like (X ! Xi) i.e., if the hypothesis X holds, so does the symptom Xi.
(X1 ! (:X2 ^ :X3 ) ^ X2 ! (:X1 ^ :X3) ^ X3 ! (:X1 ^ :X2)) i.e., only one of the
propositions X1 ; X2; X3 can be true (mutual exclusion). Any propositional logic formula
is allowed and nonmonotonicity may be expressed using conicting constraints (augmented
with importance factors). Quadratic energy functions may be generated from arbitrary
propositional constraints by introducing hidden variables (Pinkas, 1991).
Sparseness of such networks emerges as a result of assuming conditional independency of
symptoms relative to their hypothesis. Independency assumptions of this kind (that makes
computation tractable) are quite common in actual implementations of Bayes networks,
inuence diagrams (Pearl, 1988), and certainty propagation of rule-based expert systems
(Shortliffe, 1976). When our knowledge base consists only of diagnosis rules (and maybe
the corresponding X ! Xi rules) and the symptoms are all independent of each other,
there are no cycles in the network, and the tree algorithm converges to a global maximum
in linear time. When we add dependent symptoms which affect a hypothesis through more
than one path; e.g., X1 ! X , and X1 ! X2 ! ::: ! X , or when we start adding arbitrary
constraints, cycles are added. When dependent symptoms and arbitrary constraints are
only scarcely added, the network generated will most likely lend itself eciently to the
activate-with-cutset algorithm.
Abandonment of ecient algorithms exists for both inheritance and diagnosis in their
tractable forms. Our algorithm offers both to solve eciently tractable versions of the problem and to approximate intractable versions of it in massively parallel, simple to implement
methods. The eciency of the suggested process depends on the \closeness" of the problem
to an ideal, tractable form.

A. Appendix

Proof sketch of theorem 4.3: The second and third phases of the algorithm are adaptations
of an existing dynamic programming algorithm (Bertele & Brioschi, 1972), and their cor242

fiImproving Connectionist Energy Minimization

rectness is therefore not proved here. The self-stabilization of these steps is obvious because
no variables are initialized. The proof is therefore dependent on the convergence of the tree
directing phase.
Let us first assume that the scheduler is distributed with fair exclusion and that the
network is a tree. The first part of the theorem is proved by points 1-4. We want to show
that the tree-directing algorithm converges, that it is self-stabilizing and that the final stable
result is that the pointers Pij represent a tree. Points 5 and 6 prove parts 2 and 3 of the
theorem. A node is called legal if it is either a root (i.e., all its neighbors are legal, point
to it and it doesn't point to any of them), or an intermediate node (i.e., it points to one of
the neighbors and the rest of its neighbors are all legal and point back). A node is called a
candidate if it is an illegal node and has all its neighbors but one pointing to it. We would
like to show that:
1. The property of being legal is stable; i.e., once a node becomes legal it will stay legal.
2. A state where the number of illegal nodes is k > 0, leads to a state where the number
of illegal nodes is less than k; i.e., the number of illegal nodes decreases and eventually
all nodes turn legal.
3. If all the nodes are legal then the graph is marked as a tree.
4. The algorithm is self-stabilizing for trees.
5. The algorithm converges even if the graph has cycles (part 2 of the theorem).
6. The algorithm is self-stabilizing in arbitrary networks if an almost uniform version is
used, even under a distributed scheduler (part 3 of the theorem).
We will now prove each of the above points.
1. Show that a legal state is stable. Assume a legal node i becomes illegal. It is either a
root node and one of its children became illegal, or an intermediate node whose one
of its children became illegal (it cannot be that its parent suddenly points to i or that
one of the children stopped pointing and still is legal). Therefore, there must be a
chain of i1 ; i2; :::; ik of nodes that became illegal. Since there are no cycles, there must
be a leaf that was legal and turned illegal. This cannot occur since a leaf does not
have children; leading to a contradiction.
2. Show that if there are illegal nodes, their number is reduced. To prove this claim we
need three steps:
(a) Show that eventually, if there are illegals, then there are also candidates.
Because of the fair exclusion, eventually a state is reached where each node has
been executed at least once. Assume that at least one node is illegal, and all
the illegal nodes are not candidates. If a node is illegal and not a candidate,
then either it is a root-type (all point to it) but at least one of its children
is illegal, or there are at least two of its neighbors that are illegal. Suppose
there are no root-type illegal nodes. Then all illegal nodes have at least two
243

fiPinkas and Dechter

illegal neighbors. Therefore there must be a cycle that connects illegal nodes
(contradiction). Therefore, one of the illegal nodes must be root-type. Suppose
i is a root-type illegal node. It must have a neighbor j which is illegal. Consider
the subtree of j that does not include i: it must contain illegal nodes. If there
are no root-type illegal nodes we get a contradiction again. However, if there is
a root-type node, we eliminate it and look at the subtree of some illegal j 0 that
does not include j . Eventually, since the network is finite, we obtain a subtree
with no root-like illegal nodes but which includes other illegal nodes. This leads
to a contradiction. The conclusion is that there must be candidates if there are
illegal nodes.
(b) Show that a candidate is stable unless it becomes legal.
If a node i is a candidate, all its legal children remain legal. There are three
types of candidate nodes (node j is an illegal neighbor of i):
i. node j points to i;
ii. the pointer goes in both directions;
iii. there is no pointer from i to j or vice-versa.
All possible changes in the pointers Pij or Pji will cause i to remain a candidate
or to turn legal (the rest of the pointers will not be changed).
(c) Show that every candidate node will eventually turn legal: Assume j is the illegal
neighbor of the candidate i. In the next execution of i without j (fair exclusion),
if Pji = 0 then i becomes legal by pointing to j ; otherwise, i becomes a root-type
candidate (all its neighbors point to it) but j is illegal. We will prove now that
if an illegal node j points to i then eventually a state is reached where either
j is legal or Pji = 0, and that this proposition is stable once it holds. If this
statement is true then when i is executed eventually, if j is legal then all of i0 s
neighbors are legal and therefore i turns legal. If j is illegal then Pji = 0, and i
will point to it (Pij = 1) making itself legal.
We next prove that if j is an illegal node pointing to i then there will be a state
where either j is legal or Pji = 0, and this state is stable. We prove it by induction
on the size of the subtree of j that does not include i.
Base step: If j is a leaf and j points to i then if at the time j is executed (without
i) Pij = 0, then node j points to i and becomes legal; otherwise, j updates Pji = 0.
This status is stable because the legal state is stable and since a leaf will point
to a node only if it turns legal.
Induction step: Assume hypothesis is True for trees of size less than n. Suppose j
is the illegal neighbor if i. Node j points to i and it has j1 ; :::; jk other neighbors.
Because we assume that all nodes were executed at least one time, since j points
to i we assume that at the last execution of j all the other neighbors j1 ; :::; jk
pointed to j . The subtrees rooted by jl (not including j ) are of size less than n
and therefore by the hypothesis there will be a state where all the nodes j1 ; :::; jk
are either legal or Pjj = 0. This state is stable, so when eventually j is executed,
it will either point to i turning legal (if all j1; :::; jk are pointing to it), or it will
make Pji = 0 (if some of its neighbors do not point to it). Since the status of
l

244

fiImproving Connectionist Energy Minimization

j1; :::; jk is stable at that point, whenever j is executed it will either become legal

3.

4.

5.

6.

or its pointers become zero.
Show that if all the nodes are legal then the graph is marked as a tree: If a node is
legal, then all its children are legal and point to it. Therefore each node represents
a subtree (if not a leaf) and has one parent at the most. To show that there is
only one root we make the following argument. If several roots exist, then because
of connectivity, there is one node that is shared between at least two subtrees and
therefore has two parents (contradiction).
The algorithm is self-stabilizing for cycle-free networks since no initialization is needed
(in the proof we haven't use the first initialization step; i.e., Pij = 0). In the case where
no cycles exist we do not need this step. The pointers can get any initial values and
the algorithm still converges.
The algorithm (with Pij = 0 initialization) converges even if the graph has cycles.
Since all the nodes start with zero pointers, a (pseudo) root of a tree-like subnetwork
will never point toward any of its neighbors (since it is part of a cycle and all of its
neighbors but one must be legal).
Show that the algorithm is self-stabilizing in arbitrary networks if an almost uniform
version is used, even under a distributed scheduler. We need to show that a candidate
will eventually turn legal even if its neighbors are executed in the same time.
Suppose node i is a candidate and node j is its illegal neighbor:
(a) If j is a root, then it will never point to i, and therefore i will eventually turn
legal by pointing to j .
(b) If i is the root, then Pij = 0, and if j becomes legal it will point to i making i
legal. Node j will turn eventually legal using the following induction (on the size
of the subtree of j ):
Hypothesis: In a subtree without a node that acts as a root, all illegal nodes will
eventually turn legal.
Base step: If j is a leaf, it will point eventually to its neighbor i which in its turn
will make j legal by Pij = 0.
Induction step: If j1 ; :::; jk are other neighbors of j , then they will eventually
turn legal (induction hypothesis) while pointing to j . Eventually j is executed
and also turns legal.
(c) Suppose neither i nor j are roots, but one of them is not part of a cycle (and
therefore is part of a subtree that does not include a node marked as a root).
Using the above induction, all the nodes in the subtree will eventually turn legal.
As a result either i or j eventually turns legal, and therefore i will eventually
turn legal as well.

2

245

fiPinkas and Dechter

7. Acknowledgement

This work was supported in part by NSF grant IRI-9157636, by Air Force Oce of Scientific
Research, AFOSR 900136, by Toshiba of America and by a Xerox grant. We would also
like to thank Kalev Kask for commenting on the latest version of this manuscript, Kaoru
Mulvihill for drawing the figures, for Lynn Haris for editing and the anonymous reviewers
who helped improve the final version of this paper. A shorter version of this paper appears
earlier (Pinkas & Dechter, 1992).

References

Ballard, D. H., Gardner, P. C., & Srinivas, M. A. (1986). Graph problems and connectionist
architectures. Tech. rep. 167, University of Rochester.
Becker, A., & Geiger, D. (1994). Approximation algorithms for loop cutset problems. In
Proceedings of the 10th conference on Uncertainty in Artificial Intelligence (UAI-94),
pp. 60{68 Seattle, Washington.
Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press, New
York.
Brandt, R. D., Wang, Y., Laub, A. J., & Mitra, S. K. (1988). Alternative networks for
solving the traveling salesman problem and the list-matching problem. IEEE International Conference on Neural Networks, 2, 333{340.
Collin, Z., Dechter, R., & Katz, S. (1991). On the feasibility of distributed constraint
satisfaction. In Proceedings of IJCAI Sydney.
Dechter, R. (1990). Enhancement schemes for constraint processing: Backjumping, learning
and cutset decomposition. Artificial Intelligence, 41(3), 273{312.
Dechter, R. (1992). Constraint networks. In Encyclopedia of Artificial Intelligence, 2nd ed.,
pp. 276{285. John Wiley & Sons, Inc.
Dechter, R., Dechter, A., & Pearl, J. (1990). Optimization in constraint networks. In
in R.M. Oliver, J. S. (Ed.), Inuence diagrams, belief nets and decision analysis. John
Wiley and Sons.
Feldman, J. A., & Ballard, D. H. (1982). Connectionist models and their properties. Cognitive Science 6.
Hinton, G., & Sejnowski, T. (1986). Learning and re-learning in boltzmann machines. In
Parallel Distributed Processing: Explorations in The Microstructure of Cognition I,
in J. L. McClelland and D. E. Rumelhart, pp. 282{317. MIT Press, Cambridge, MA.
Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective
computational abilities. In Proceedings of the National Academy of Sciences 79, pp.
2554{2558.
246

fiImproving Connectionist Energy Minimization

Hopfield, J. J. (1984). Neurons with graded response have collective computational properties like those of two-state neurons. In Proceedings of the National Academy of
Sciences 81, pp. 3088{3092.
Hopfield, J. J., & Tank, D. W. (1985). Neural computation of decisions in optimization
problems. Biological Cybernetics, 52, 144{152.
Kasif, S., Banerjee, S., Delcher, A., & Sullivan, G. (1989). Some results on the computational complexity of symmetric connectionist networks. Tech. rep. JHU/CS-89/10,
Department of Computer Science, The John Hopkins University.
Korach, K., Rotem, D., & Santoro, N. (1984). Distributed algorithms for finding centers and
medians in networks. ACM Transactions on Programming Languages and Systems,
6(3), 380{401.
McClelland, J. L., Rumelhart, D. E., & Hinton, G. (1986). The appeal of pdp. In J. L.
McClelland and D. E. Rumelhart, Parallel Distributed Processing: Explorations in
The Microstructure of Cognition I. MIT Press, Cambridge, MA.
Minton, S., Johnson, M. D., & Phillips, A. B. (1990). Solving large scale constraint satisfaction and scheduling problems using a heuristic repair method. In Proceedings of
the Eighth Conference on Artificial Intelligence, pp. 17{24.
Papadimitriou, C., Shaffer, A., & Yannakakis, M. (1990). On the complexity of local search.
ACM Symposium on the Theory of Computation, 438{445.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann Publishers, San Mateo, California.
Peterson, C., & Hartman, E. (1989). Explorations of mean field theory learning algorithm.
Neural Networks 2, 6.
Pinkas, G. (1991). Energy minimization and the satisfiability of propositional calculus.
Neural Computation 3, 2.
Pinkas, G., & Dechter, R. (1992). A new improved activation function for energy minimization. In Proceedings of the Tenth National Conference on Artificial Intelligence
(AAAI), pp. 434{439 San Jose.
Rumelhart, D. E., Hinton, G. E., & McClelland, J. L. (1986). A general framework for
parallel distributed processing. In in J. L. McClelland and D. E. Rumelhart, Parallel
Distributed Processing: Explorations in The Microstructure of Cognition I. MIT Press,
Cambridge, MA.
Selman, B., Levesque, H., & Mitchell, D. (1992). A new method for solving hard satisfiability
problems. In Proceedings of the Tenth National Conference on Artificial Intelligence,
pp. 440{446.
Shortliffe, E. H. (1976). Computer-Based Medical Consultation, Mycin. Elsevier, New York.
247

fiPinkas and Dechter

Smolensky, P. (1986). Information processing in dynamical systems: Foundations of harmony theory. In J. L. McClelland and D. E. Rumelhart, Parallel Distributed Processing: Explorations in The Microstructure of Cognition I. MIT Press, Cambridge,
MA.

248

fiJournal of Artificial Intelligence Research 3 (1995) 383-403

Submitted 6/95; published 12/95

Rule-based Machine Learning Methods for Functional
Prediction

Sholom M. Weiss

weiss@cs.rutgers.edu

Nitin Indurkhya

nitin@cs.usyd.edu.au

Department of Computer Science, Rutgers University
New Brunswick, New Jersey 08903, USA
Department of Computer Science, University of Sydney
Sydney, NSW 2006, AUSTRALIA

Abstract

We describe a machine learning method for predicting the value of a real-valued function, given the values of multiple input variables. The method induces solutions from
samples in the form of ordered disjunctive normal form (DNF) decision rules. A central
objective of the method and representation is the induction of compact, easily interpretable
solutions. This rule-based decision model can be extended to search eciently for similar cases prior to approximating function values. Experimental results on real-world data
demonstrate that the new techniques are competitive with existing machine learning and
statistical methods and can sometimes yield superior regression performance.

1. Introduction
The problem of approximating the values of a continuous variable is described in the statistical literature as regression. Given samples of output (response) variable y and input
(predictor) variables x = fx1 :::xn g, the regression task is to find a mapping y = f(x). Relative to the space of possibilities, finite samples are far from complete, and a predefined
model is needed to concisely map x to y. Accuracy of prediction, i.e. generalization to new
cases, is of primary concern. Regression differs from classification in that the output variable y in regression problems is continuous, whereas in classification y is strictly categorical.
From this perspective, classification can be thought of as a subcategory of regression. Some
machine learning researchers have emphasized this connection by describing regression as
\learning how to classify among continuous classes" (Quinlan, 1993).
The traditional approach to the problem is classical linear least-squares regression
(Scheffe, 1959). Developed and refined over many years, linear regression has proven quite
effective for many real-world applications. Clearly the elegant and computationally simple linear model has its limits, and more complex models may fit the data better. With
the increasing computational power of computers and with larger volumes of data, interest has grown in pursuing alternative nonlinear regression methods. Nonlinear regression
models have been explored by the statistics research community and many new effective
methods have emerged (Efron, 1988), including projection pursuit (Friedman & Stuetzle,
1981) and MARS (Friedman, 1991). Methods for nonlinear regression have also been developed outside the mainstream statistics research community. A neural network trained
by back-propagation (McClelland & Rumelhart, 1988) is one such model. Other models
c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiWeiss & Indurkhya
can be found in numerical analysis (Girosi & Poggio, 1990). An overview of many different
regression models, with application to classification, is available in the literature (Ripley,
1993). Most of these methods produce solutions in terms of weighted models.
In the real-world, classification problems are more commonly encountered than regression problems. This accounts for the greater attention paid to classification than to regression. But many important problems in the real world are of the regression type. For
instance, problems involving time-series usually involve prediction of real values. Besides
the fact that regression problems are important on their own, another reason for the need to
focus on regression is that regression methods can be used to solve classification problems.
For example, neural networks are often applied to classification problems.
The issue of interpretable solutions has been an important consideration leading to
development of \symbolic learning methods." A popular format for interpretable solutions
is the disjunctive normal form (DNF) model (Weiss & Indurkhya, 1993a). Decision trees and
rules are examples of DNF models. Decision rules are similar in characteristics to decision
trees, but they also have some potential advantages: (a) a stronger model (b) often better
explanatory capabilities. Unlike trees, DNF rules need not be mutually exclusive. Thus,
their solution space includes all tree solutions. These rules are potentially more compact
and predictive than trees. Decision rules may also offer greater explanatory capabilities
than trees because as a tree grows in size, its interpretability diminishes.
Among symbolic learning methods, decision tree induction, using recursive partitioning, is highly developed. Many of these methods developed within the machine learning
community, such as ID3 decision tree induction (Quinlan, 1986), have been applied exclusively to classification tasks. Less widely known is that decision trees are also effective
in regression. The CART program, developed in the statistical research community, induces both classification and regression trees (Breiman, Friedman, Olshen, & Stone, 1984).
These regression trees are strictly binary trees, a representation which naturally follows
from intensive modeling using continuous variables.1
In terms of performance, regression trees often are competitive in performance to other
regression methods (Breiman et al., 1984). Regression trees are noted to be particularly
strong when there are many higher order dependencies among the input variables (Friedman,
1991). The advantages of the regression tree model are similar to the advantages enjoyed by
classification trees over other models. Two principal advantages can be cited: (a) dynamic
feature selection and (b) explanatory capabilities. Tree induction methods are extremely
effective in finding the key attributes in high dimensional applications. In most applications,
these key features are only a small subset of the original feature set. Another characteristic
of decision trees that is often cited is its capability for explanation in terms acceptable
to people. On the negative side, decision trees cannot represent compactly many simple
functions, for example linear functions. A second weakness is that the regression tree
model is discrete, yet predicts a continuous variable. For function approximation, the
expectation is a smooth continuous function, but a decision tree provides discrete regions
that are discontinuous at the boundaries. All in all though, regression trees often produce
strong results, and for many applications their advantages strongly outweigh their potential
disadvantages.
1. A comparative study (Fayyad & Irani, 1992) suggests that binary classification trees are somewhat more
predictive even for categorical variables.

384

fiRule-based Functional Prediction
In this paper we describe a new method for inducing regression rules. The method
takes advantage of the close relationship between classification and regression and provides a
uniform and general model for dealing with both problems. Additional gains can be obtained
by extending this method in a manner that preserves the strengths of the partitioning
schemes while compensating for their weaknesses. Rules can be used to search for the most
relevant cases, and a subset of these cases can help determine the function value. Thus,
some of the model's interpretability can be traded off for better performance. Empirical
results suggest that these methods are effective and can induce solutions that are often
superior to decision trees.

2. Measuring Performance

The objective of regression is to minimize the distance between the sample output values,
yi and the predicted values yi . Two measures of distance are commonly used. The classical
regression measure is equation 1, the average squared distance between yi and yi , i.e. the
variance. It leads to an elegant formulation for the linear least squares model. The mean
absolute distance (deviation) of equation 2 is used in least absolute deviation regression,
and is perhaps the more intuitive measure.
The mean absolute distance (deviation) of equation 2 is used in our studies. This is a
measure of the average error of prediction for each yi over n cases.
0

0

Xn (yi , yi)2
i=1
n
X
MAD = 1 jyi , yi j

V ariance = n1

0

(1)

(2)
n i=1
The regression problem is sometimes described as a signal and noise problem. The model
is extended to include a stochastic component  in equation 3. Thus, the true function may
not produce a zero error distance. In contrast to classification where the labels are assumed
correct, for regression the predicted y values could be explained by a number of factors
including a random noise component, , in the signal, y.
0

y = f (x1 : : : xn) + 
(3)
Because prediction is the primary concern, estimates based on training cases alone
are inadequate. The principles of predicting performance on new cases are analogous to
classification, but here the mean absolute distance is used as the error rate. The best
estimate of true performance of a model is the error rate on a large set of independent test
cases. When large samples of data are unavailable, the process of train and test is simulated
by random resampling. In most of our experiments, we used (10-fold) cross-validation to
estimate predictive performance.

3. Regression by Tree Induction

In this section, we contrast regression tree induction with classification tree induction. Like
classification trees, regression trees are induced by recursive partitioning. The solution takes
385

fiWeiss & Indurkhya
the form of equation 4, where Ri are disjoint regions, ki are constant values, and yji refers
to the y-values of the training cases that fall within the region Ri .

if x  Ri then f (x) = ki = medianfyji g
(4)
Regression trees have the same representation as classification trees except for the terminal nodes. The decision at a terminal node is to assign a case a constant y value. The
single best constant value is the median of the training cases falling into that terminal node
because for a partition, the median is the minimizer of mean absolute distance. Figure 1 is
an example of a binary regression tree. All cases reaching shaded terminal node 1 (x13)
are assigned a constant value of y=10.
1
x1<=3

x1>3

1

2

y=10

x2<=1

x2>1

2

3

y=2

y=5

Figure 1: Example of Regression Tree
Tree induction methods usually proceed by (a) finding a covering set for the training
cases and (b) pruning the tree to the best size. Although classification trees have been
more widely studied, a similar approach can be applied to regression trees. We assume
the reader is familiar with classification trees, and we cite only the differences in binary
tree induction (Breiman et al., 1984; Quinlan, 1986; Weiss & Kulikowski, 1991). In many
respects, regression tree induction is more straightforward. For classification trees, the error
rate is a poor choice for node splitting, and alternative functions such as entropy or gini
are employed. For regression tree induction, the minimized function, i.e. absolute distance,
is most satisfactory. At each node, the single best split that minimizes the mean absolute
distance is selected. Splitting continues until fewer than a minimum number of cases are
covered by a node, or until all cases within the node have the identical value of y.
The goal is to find the tree that generalizes best to new cases, and this is often not a
full covering tree, particularly in presence of noise or weak features. The pruning strategies
employed for classification trees are equally valid for regression trees. Like the covering
procedures, the only substantial difference is that the error rate is measured in terms of
mean absolute distance. One popular method is the weakest-link pruning strategy (Breiman
et al., 1984). For weakest-link pruning, a tree is recursively pruned so that the ratio delta/n
is minimized, where n is the number of pruned nodes and delta is the increase in error.
386

fiRule-based Functional Prediction
x13 ! y=10
x21 ! y=2
Otherwise y=5
Figure 2: Example of Regression Rules
Weakest link pruning has several desirable characteristics: (a) it prunes by training cases
only, so that the remaining test cases are relatively independent (b) it is compatible with
resampling.

4. Regression by Rule Induction
Both tree and rule induction models find solutions in disjunctive normal form, and the model
of equation 4 is applicable to both. Each rule in a rule-set represents a single partition or
region Ri . However, unlike the tree regions, the regions for rules need not be disjoint. With
non-disjoint regions, several rules may be satisfied for a single sample. Some mechanism
is needed to resolve the conicts in ki , the constant values assigned, when multiple rules,
Ri regions, are invoked. One standard model (Weiss & Indurkhya, 1993a) is to order the
rules. Such ordered rule-sets have also been referred to as decision lists. The first rule that
is satisfied is selected, as in equation 5.

if i < j and x  both Ri and Rj then f (x) = ki
(5)
Figure 2 is an example of an ordered rule-set corresponding to the tree of Figure 1. All
cases satisfying rule 3, and not rules 1 and 2, are assigned a value of y=5.
Given this model of regression rule sets, the problem is to find procedures that effectively
induce solutions. For rule-based regression, a covering strategy analogous to the classification tree strategy could be specified. A rule could be induced by adding a single component
at a time, where each added component is the single best minimizer of distance. As usual,
the constant value ki is the median of the region formed by the current rule. As the rule is
extended, fewer cases are covered. When fewer than a minimal number of cases are covered,
rule extension terminates. The covered cases are removed and rule induction can continue
on the remaining cases. This is also the regression analogue of rule induction procedures
for classification (Michalski, Mozetic, Hong, & Lavrac, 1986; Clark & Niblett, 1989).
However, instead of this approach, we propose a novel strategy of mapping the regression
covering problem into a classification problem.

4.1 A Reformulation of the Regression Problem

The motivation for mapping regression into classification is based on a number of factors
related to the extra information given in the regression problem: the natural ordering of yi
by magnitude: if i > j then yi > yj .
Let fCi g be a set consisting of an arbitrary number of classes, each class containing
approximately equal values of fyi g. To solve a classification problem, we expect that the
classes are different from each other, and that patterns can be found to distinguish these
387

fiWeiss & Indurkhya
1. Generate a set of Pseudo-classes using the P-class algorithm (Figure 4).
2. Generate a covering rule-set for the transformed classification
problem using a rule induction method such as Swap-1
(Weiss & Indurkhya, 1993a).
3. Initialize the current rule set to be the covering rule set and save it.
4. If the current rule set can be pruned, iteratively do the following:
a) Prune the current rule set.
b) Optimize the pruned rule set (Figure 5) and save it.
c) Make this pruned rule set the new current rule set.
5. Use test cases or cross-validation to pick the best of the saved rule sets.
Figure 3: Overview of Method for Learning Regression Rules
classes. Should we expect classes formed by an ordering of fyi g to be a reasonable classification problem? There are a numbers of reasons why the answer is yes, particularly for a
rule induction procedure.
The most obvious situation is the classical linear relationship. In this instance, by
definition, some ordering of fx1i . . . xni g corresponds to the ordering of yi . Although classical
methods are very strong in compactly determining linear functions, most interest in modern
methods centers around their potential for finding nonlinear relationships. For nonlinear
functions, we know there is usually no such ordering of fx1i . . . xni g corresponding to the
fyig. Still, we expect that the true function is smooth, and in a local region the ordering
relationship will hold. In terms of classification, we know that a class Cj with similar values
of y is quite different than class Ck with much lower values of y. For a nonlinear function
within a class of similar values of y, some of these y have very similar values of fx1i . . . xni g.
These correspond to some local region of the function. However, it is also true that some
identical values of y can have very different fx1i . . . xni g so that multiple clusters can be
found within the class. Because rule induction methods do not cover a class with a single
rule, the expectation is that multiple patterns will be found to cover these clusters.
Once the cases have been assigned such (pseudo-)classes, the classification problem can
be solved in the following stages: (a) find a covering set and (b) prune the rule set to an
appropriate size, with improved results achieved when an additional technique is considered:
(c) refine or optimize a rule set. The overall method is outlined in Figure 3.

4.2 Generating Pseudo-classes
In the previous section, we described the motivation for pseudo-classes. The specification
of these classes does not use any information beyond the ordering of y. No assumptions
about the true nature of the underlying function are made. Within this environment, the
goal is to make the y values within one class most similar and y values across classes most
dissimilar. We wish to assign the y values to classes such that the overall distance between
each yi and its class mean is minimum.
388

fiRule-based Functional Prediction
Input: fyi g a set of output values
Initialize n := number of cases, k := number of classes
For each Classi
Classi := next n/k cases from list of sorted y values
end-for
Compute Errnew
Repeat
Errold = Errnew
For each Casej
When it is in Classi
1. If Dist[Casej , Mean(Classi,1 )] < Dist[Casej , Mean(Classi )]
Move Casej to Classi,1
2. If Dist[Casej , Mean(Classi+1 )] < Dist[Casej , Mean(Classi )]
Move Casej to Classi+1
Next Casej
Compute Errnew
Until Errnew is not less than Errold
Figure 4: Composing Pseudo-Classes (P-Class)
Figure 4 describes an algorithm (P-Class) for assigning the values fyi g to k classes. Essentially the algorithm does the following: (a) sorts the y values; (b) assigns approximately
equal numbers of contiguous sorted yi to each class; (c) moves a yi to a contiguous class
when that reduces the global distance Err from each yi to the mean of its assigned class.
Classes with identical means should be merged. P-Class is a variation of k-means clustering, a statistical method that minimizes a distance measure (Hartigan & Wong, 1979).
Alternative methods that do not depend on distance measures (Lebowitz, 1985) may also
be used.
Given a fixed number of k classes, this procedure will relatively quickly assign the yi to
classes such that the overall distances are minimized. Because the underlying function is
unknown, it is not critical to have a global minimum assignment of the yi. This procedure
matches well to our stated goals for ordering the yi values. The obvious remaining question is
how do we determine k, the number of classes? Unfortunately, there is no direct answer, and
some experimentation is necessary. However, as we shall see in Section 7, there is empirical
evidence suggesting that results are quite similar within a local neighborhood of values of
k. Moreover, relatively large values of k, which entail increased computational complexity
for rule induction, are typically necessary only for noise-free functions that can be modeled
exactly. Analogous to comparisons of neural nets with increasing numbers of hidden units,
the trends for increasing numbers of partitions become evident during experimentation.
One additional variation on the classification theme arises for rule induction schemes
that cover one class at a time. The classes must be ordered, and the last class typically
389

fiWeiss & Indurkhya
becomes a default class to cover situations when no rule for other classes is satisfied. For
regression, having one default partition for a class is unlikely to be the best covering solution,
and instead the remaining cases for the last class are repeatedly partitioned (by P-Class)
into 2 classes until fewer than m cases remain.
An interesting characteristic of this transformation of the regression problem is that
we now have a uniform and general model that once again relates both classification and
regression. If the yi values are discrete and categorical, P-Class merely restates the standard
classification problem. For example, if all values of yi are either 0 or 1, then the result of
P-Class will be be 2 non-empty classes.

4.3 A Covering Rule Set
With this transformation, rule induction algorithms for classification can be applied. We
will consider those induction methods that fully cover a class before moving on to induce
rules for the next class. At each step of the covering algorithm, the problem is considered
a binary classification problem for the current class Ci versus all Cj where j > i, i.e. the
current class versus the remaining classes. When a rule is induced, its corresponding cases
are removed and the remaining cases are considered. When a class has been covered, the
next class is considered. An example of such a covering algorithm is that used in Swap-1
(Weiss & Indurkhya, 1993a), and this is the procedure used in this paper. The covering
method is identical for classification and regression. However, one distinction is that the
regression classes are transient labels that are replaced with the median of the y values for
the cases covered by each induced rule. Because the rules are ordered and multiple rules
may be satisfied, the medians are derived only from those instances where the rule is the
first to be satisfied.
Although this procedure may yield good, compact covering sets, additional procedures
are necessary for a complete solution.

4.4 Pruning the Rule Set
Typical real-world applications have noisy features that are not fully predictive. A covering
set, particularly one composed of many continuous variables, can be far too over-specialized
to produce the best results. For classification, relatively few classes are specified in advance.
For regression, we expect many smaller groups because values of yi are likely to be quite
different.
We noted earlier that for regression trees the usual classification pruning techniques can
be applied with the substitution of mean absolute distance for the classification error rate.
As in weakest-link tree pruning, the same ratio of delta/n can be recursively minimized for
weakest-link rule pruning. The intuitive rationale is to remove those parts of a rule set that
have the least impact on increasing the error. Pruning rule sets is usually accomplished by
either deleting complete rules or single rule components (Quinlan, 1987; Weiss & Indurkhya,
1993a). In general, rule pruning (for both classification and regression) is less natural and
far more computationally expensive than tree pruning. Tree pruning has a natural ow
from set to subset. Thus a tree can be pruned from bottom up, typically considering the
effect of removing a subtree. Non-disjoint rules have no such natural pruning order, for
390

fiRule-based Functional Prediction
example every component in a rule is a candidate for pruning and may affect all other rules
that follow it in the specified rule order.
There is a major difference in pruning regression rules vs. classification rules. For
classification, deleting a rule or a rule component has no effect on the class labels. For
regression, pruning will change the median-values of y for the regions. Even the deletion of
a rule will affect other region medians because the rules are ordered and multiple rules may
be satisfied. This characteristic of rule pruning for regression adds substantial complexity
to the task. However, by assuming that the median-values of y remain unchanged during
the evaluation of candidate rules to prune, a pruning procedure can achieve reasonable
computational eciency at the expense of some loss in the accuracy of evaluation. Once
the best rule or component for deletion is selected, the medians of all regions can then be
re-evaluated.
Even for classification rules, rule pruning has some inherent weaknesses. For example,
rule deletion will often create a gap in coverage. For classification rules though, it is quite
feasible to develop an additional procedure to refine and optimize a rule set. To a large
extent, this overcomes the cited weakness in pruned rules sets. A similar refinement and
optimization procedure can be developed for regression and is described next.

4.5 Rule Refinement and Optimization

Given a rule set RSi , can it be improved? This question applies to any rule set, although
we are mostly motivated by trying to improve the pruned rules sets fRSo . . . RSi . . . RSng.
This is a combinatorial optimization problem. Using error measure Err(RS), can we improve
RSi without changing its size, i.e. the number of rules and components? Figure 5 describes
an algorithm that minimizes Err(RS), the MAD of the model prediction on sample cases,
by local swapping, i.e. replacing a single rule component with the best alternative. It is a
variation of the techniques used in Swap-1 (Weiss & Indurkhya, 1993a).
The central theme is to hold a model configuration constant and make a single local
improvement to that configuration. Local modifications are made until no further improvements are possible. Making local changes to a configuration is a widely-used optimization
technique to approximate a global optimum and has been applied quite successfully, for
example to find near-optimum solutions to traveling salesman problems (Lin & Kernighan,
1973). An analogous local optimization technique, called backfitting, has been used in the
context of nonlinear statistical regression (Hastie & Tibshirani, 1990).
Variations on the selection of the next improvement move could include:
1. First local improvement encountered (such as in backfitting)
2. Best local improvement (such as in Swap-1)
In our experiments with rule induction methods, the results are consistently better for
(2); (1) is more ecient, but the (pruned) rule induction environment is mostly stable with
relatively few local improvements prior to convergence. In a less stable environment, with
very large numbers of possible configuration changes, (2) may not be feasible or even better.
In the pruned rule set environment, if the covering procedure is effective, then each pruned
solution should be relatively close to a local minimum solution. Weakest-link pruning
391

fiWeiss & Indurkhya
Input: RS a rule set consisting of rules Ri , and
S a set of training cases
D := TRUE
while (D is TRUE) do
RSnew := RS with the single best replacement for a
component of RS that most reduces Err(RS) on
cases in S using current Median(Ri )
If no replacement is found then
D := FALSE
else
RS := RSnew ; recompute Medians(Ri )
endwhile
return the rule set RS
Figure 5: Optimization by Rule Component Swapping
results in a series of pruned rule sets RSi that number far fewer than sets which would
result from a single prune of a rule or rule component. Each of the RSi are optimized prior
to continuing the pruning process. However, rule set optimization can usually be suspended
until substantial segments of the covering set have already been pruned.
If (1) is used, then either sequentially ordered evaluations (as in backfitting) or stochastic evaluations can be considered. Empirical evidence in the optimization literature supports the superiority of stochastic evaluation (Jacoby, Kowalik, & Pizzo, 1972). Further
improvements may be obtained by occasionally making random changes in configuration
(Kirpatrick, Gelatt, & Vecchi, 1983). These are general combinatorial optimization techniques that must be substantially reworked to fit a specific problem type. Most are expected
to be applied throughout problem solving.
The result of pruning a covering rule set, RSo , is a series of progressively smaller rule
sets fRSo . . . RSi . . . RSn g. The objective is to pick the best one, usually by some form
of error estimation. Model complexity and future performance are highly related. Both
too complex or too simple a model can yield poor results, the objective being to find just
the right size model. Independent test cases or resampling by cross-validation are effective
for estimating future performance. In the absence of these estimates, approximations, such
as GCV (Craven & Wahba, 1979; Friedman, 1991), as described in equation 6, have been
used in the statistics literature to estimate performance2. Both measures of training error
and model complexity are used in the estimates. C(M), is a measure of model complexity
expressed in terms of parameters estimated (such as the number of weights in a neural net)
or tests performed, where C(M) is assumed to be less than n, the number of cases.
2. GCV is an acronym for generalized cross-validation, but only the apparent error on training cases is used
and not true cross-validation by resampling.

392

fiRule-based Functional Prediction

Xn
GCV (M ) =

jy ,y j
0

i

i

n
C(M)
i=1 1 , n

(6)

In our experiments we used cross-validated estimates to guide the final model selection
process, but other measures such as GCV may also be used.

4.6 Potential Problems with Rule-based Regression

Regression rules, like trees, are induced by recursive partitioning methods that approximate a function with constant-value regions. They are relatively strong in dynamic feature
selection in high-dimensional applications, sometimes using only a few highly predictive
features. An essential weakness of these methods is the approximation of a partition or
region by a constant value. For a continuous function and even a moderately sized sample,
this approximation can lead to increased error.
To deal with this limitation, instead of constant-value functions, linear functions can
be substituted in a partition (Quinlan, 1993). However, a linear function has the obvious
weakness that the true function may be far from linear even in the restricted context of
a single region. In general, use of such linearity compromises the highly non-parametric
nature of the DNF model. A better strategy might be to examine alternative non-linear
methods.

5. An Alternative to Rules: k-Nearest Neighbors
The k-nearest neighbor method is one of the simplest regression methods, relying on table
lookup. To classify an unknown case x, the k cases that are closest to the new case are
found in a sample data base of stored cases. The predicted y(x) of equation 7 is the mean
of the y values for the k-nearest neighbors. The nearest neighbors are found by a distance
metric such as euclidean distance (usually with some feature normalization). The method
is non-parametric and highly non-linear in nature

yknn(x) = K1

XK yk for K nearest neighbours of x

k=1

(7)

A major problem with this approach is how to limit the effect of irrelevant features.
While limited forms of feature selection are sometimes employed in a preprocessing stage,
the method itself cannot determine which features should be weighted more than others. As
a result, the procedure is very sensitive to the distance measure used. In a high-dimensional
feature space, k-nearest neighbor methods may perform very poorly. These limitations are
precisely those that the partitioning methods address. Thus, in theory, the two methods
potentially complement one another.

6. Model Combination

In practice, one learning model is not always superior to others, and a learning strategy
that examines the results of different models may do better. Moreover, by combining
393

fiWeiss & Indurkhya
different models, enhanced results may be achieved. A general approach to combining
learning models is a scheme referred to as stacking (Wolpert, 1992). Additional studies have
been performed in applying the scheme to regression problems (Breiman, 1993; LeBlanc &
Tibshirani, 1993). Using small training samples of simulated data, and linear combinations
of regression methods, improved results were reported. Let Mi be the i-th model trained
on the same sample, and wi , the weight to be given to Mi .3 If the new case vector is
x, the predictions of different models can be combined as in Equation 8 to produce an
estimate of y. The models may use the same representation, such as k-nearest neighbors with
variable-size k, or perhaps variable-size decision trees. The models could also be completely
different, such as combining decision trees with linear regression models. Different models
are applied independently to find solutions, and later a weighted vote is taken to reach a
combined solution. This method of model combination is in contrast to the usual approach
to evaluation of different models, where the single best performing model is selected.

y=

XK wkMk(x)

k=1

(8)

While stacking has been shown to give improved results on simulated data, a major
drawback is that properties of the combined models are not retained. Thus when interpretable models are combined, the result may not be interpretable at all. It is also not
possible to compensate for weaknesses in one model by introducing another model in a
controlled fashion.
As suggested earlier, partitioning regression methods and k-nearest neighbor regression
methods are complementary. Hence one might expect that by suitably combining the two
methods, one might obtain better performance. In one recent study (Quinlan, 1993), model
trees (i.e., regression trees with linear combinations at the leaf nodes) and nearest neighbor
methods were also combined. The combination method is described in equation 9, where
the N (x)k is one of the K nearest neighbors of x, V(x) is the y-value of the stored instance
x, and T(x) is the result of applying a model tree to x.

y = K1

XK V (N (x)k) , (T (N (x)k) , T (x))

k=1

(9)

The k-nearest neighbors are found independently of the induced regression tree (results
were reported with K=3). In that sense, the approach is similar to the combination method
of equation 8. The k-nearest neighbors are passed down the tree, and the results are used
to refine the nearest neighbor answer. Thus, we have a combination model formed by
independently computing a global solution, and later combining results.
However, there are strong reasons for not determining the global nearest neighbor solution independently. While, at the limit, with large samples, the non-parametric k-nearest
neighbor methods will correctly fit the function, in practice though, their weaknesses can
be substantial. Finding an effective global distance measure may not be easy, particularly
in the presence of many noisy features. Hence a different technique for combining the two
methods is needed.
3. These weights are obtained so as to minimize the least squared error under some constraints (Breiman,
1993).

394

fiRule-based Functional Prediction

6.1 Integrating Rules with Table-lookup

Consider the following strategy: To determine y-value of a case x that falls in region Ri ,
instead of assigning a single constant value ki for region Ri , where ki is determined by the
i (x), the mean of the k-nearest
median y value of training cases in the region, assign yknn
(training set) instances of x in region Ri . Thus for regression trees, we now have equation
10. For regression rules, we also have equation 11.
i (x)
if x  Ri then f (x) = yknn

(10)

i (x)
if i < j and x  both Ri and Rj then f (x) = yknn

(11)

An interesting aspect of this strategy is that k-nearest neighbor results need only be
considered for the cases covered by a particular partition. While this increases the interaction between the models and eliminates the independent computation of the two models,
the model rationale and, as we shall show, the empirical results, are supportive of this
approach.
We now have a representation which potentially alleviates the weakness of partitions
being assigned single constant values. Moreover, some of the global distance measure difficulties of the k-nn methods may also be relieved because the table lookup is reduced to
partitioned and related groupings.
This is the rationale for a hybrid partition and k-nn scheme. Note that unlike stacking,
our hybrid models are not independently determined, but interact very strongly with one
another. However, it must be demonstrated that these methods are in fact complementary,
preserving the strengths of the partitioning schemes while compensating for the weaknesses
that would be introduced if constant values were used for each region. With respect to model
combination, two principal questions need to be addressed by empirical experimentation:

 Are results improved relative to using each model alone?
 Are these methods competitive with alternative regression methods?

7. Results
Experiments were conducted to assess the competitiveness of rule-based regression compared
to other procedures (including less interpretable ones), as well as to evaluate the performance of the integrated partition and k-nn regression method. Experiments were performed
using seven datasets, six of which are described in previous studies (Quinlan, 1993). In addition to these six datasets, new experiments were done on a very large telecommunications
application, which is labeled pole. In each of the seven datasets, there was one continuous
real-valued response variable. Experimental results are reported in terms of the MAD, as
measured using 10-fold cross-validation. For pole, 5,000 cases were used for training and
10,000 for independent testing. The features from the different datasets were a mixture of
continuous and categorical features. For pole, all 48 features were continuous. Descriptions
395

fiWeiss & Indurkhya

Dataset Cases Vars
price
servo
cpu
mpg
peptide
housing
pole

159
167
209
392
431
506
15000

16
19
6
13
128
13
48

Table 1: Dataset Characteristics
of the other datasets can be found in the literature (Quinlan, 1993).4 Table 1 summarizes
the key characteristics of the datasets used in this study.
Table 2 summarizes the original results reported (Quinlan, 1993). These include modeltrees (MT), which are regression trees with linear fits at the terminal nodes; neural nets
(NNET); 3-nearest neighbors (3-nn); and the combined results of model-trees and 3-nearest
neighbors (MT/3-nn).5
Table 3 summarizes the additional results that we obtained. These include the CART
regression tree (RT); 5-nearest neighbors with euclidean distance (5-nn); rule regression
using Swap-1; rule regression with 5-nn applied to the rule region (Rule/5-nn); and MARS.
5-nn was used because the expectation is that the nearest neighbor method incrementally
improves a constant-value region when the region has a moderately large sample of neighbors
to average.
For the rule-based method, the parameter m, the number of pseudo-classes, must be
determined. This can be found using cross-validation or independent test cases (in our
experiments, cross-validation was used). Figure 6 represents a typical plot of the relative
error vs. the number of pseudo-classes (Weiss & Indurkhya, 1993b). As the number of
partitions increases, results improve until they reach a relative plateau and deteriorate
somewhat. Similar complexity plots can be found for other models, for example neural nets
(Weiss & Kapouleas, 1989).
The MARS procedure has several adjustable parameters.6 For the parameter mi, values
tried were 1 (additive modeling), 2, 3, 4 and number of inputs. For df, the default value of
3.0 was tried as well the optimal value estimated by cross-validation. The parameter nk was
varied from 20 to 100 in steps of 10. Lastly, both piece-wise linear as well as piece-wise cubic
solutions were tried. For each of the above setting of the parameters, the cross-validated
accuracy was monitored, and the value for the best MARS model is reported.
For each method, besides the MAD, the relative error is also reported. The relative
error is simply the estimated true mean absolute distance (measured by cross-validation)
normalized by the initial mean absolute distance from the median. Analogous to classifi4. The peptide dataset is a slightly modified version of the one Quinlan refers to as lhrh-att in his paper.
In the version used in our experiments, cases with missing values were removed.
5. Because peptide was a slightly modified version of the lhrh-att dataset, the result listed is one that was
provided by Quinlan in a personal communication.
6. The particular program used was MARS 3.5.

396

fiRule-based Functional Prediction

Relative Error
0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3
2

3

4

5
6
7
Number of Pseudo-Classes

8

9

10

Figure 6: Prototypical Performance for Varying Pseudo-Classes

Dataset MT NNET 3-nn MT/3-nn
price 1562
servo
.45
cpu
28.9
mpg
2.11
peptide .95
housing 2.45

1833
.30
28.7
2.02
2.29

1689
.52
34.0
2.72
2.90

1386
.30
28.1
2.18
2.32

Table 2: Previous Results
cation, where predictions must have fewer errors than simply predicting the largest class,
in regression too we must do better than the average distance from the median to have
meaningful results.
In comparing the performance of two methods for a dataset, the standard error for
each method was independently estimated, and the larger one was used in comparisons.
If the difference in performance was greater than 2 standard errors, the difference was
considered statistically significant. As with any significance test, one must also consider the
overall pattern of performance and the relative advantages of competing solutions (Weiss
& Indurkhya, 1994).
For each dataset, Figure 7 plots the relative best error found by the ratio of the best
reported result to each model's result. A relative best error of 1 indicates that the result is
the best reported result for any regression model. The model results that are compared to
the best results are for regression rules, 5-nn, and the mixed model. The graph indicates
397

fiWeiss & Indurkhya

Dataset

RT

5-nn

Rule

Rule/5-nn

MARS

MAD Error MAD Error MAD Error MAD Error MAD Error
price
1660 .40 1643 .40 1335 .32 1306 .31 1559 .38
servo
.195 .21 .582 .63 .235 .25 .227 .24 .212 .23
cpu
30.5 .39 29.4 .38 27.62 .35 26.32 .34 27.29 .35
mpg
2.28 .35 2.14 .33 2.17 .33 2.04 .31 1.94 .30
peptide .97
.46
.95
.45
.86
.40
.86
.40
.98
.46
housing 2.74 .42 2.77 .42 2.51 .38 2.35 .36 2.24 .34
pole
4.10 .14 5.91 .20 3.76 .13 3.70 .12 7.41 .25
Table 3: Performance of Additional Methods
Relative Best Erate
1.2
5-nn
rule
1

rule/5-nn

0.8

0.6

0.4

0.2

0
servo

house

mpg

cpu

price

peptide

pole

Figure 7: Relative Best Erates of 5-nn, Rules, and Rule/5-nn
trends across datasets and helps assess the overall pattern of performance. In this respect,
both Rule and Rule/5nn exhibit excellent performance across many applications.
These empirical results allow us to consider several relevant questions regarding rulebased regression:
1. How does rule-based regression perform compared to tree-based regression? Comparing
the results for Rule with RT, one can see that except for servo, Rule does consistently
better than RT on all the remaining six datasets. The difference in performance also
398

fiRule-based Functional Prediction
tests as significant. The results of the significance tests, and the general trend (which
can be seen visually in Figure 7) leads us to conclude that rule-based regression is
definitely competitive to trees and often yields superior performance.
2. Does integrating 5nn with rules lead to improved performance relative to using each
model alone? A comparison of Rule/5nn with 5nn shows that for all datasets, Rule/5nn
is significantly better. In comparing Rule/5nn with Rule, the results indicate that for
three datasets (mpg, pole and housing), Rule/5nn was significantly better than Rule,
and for the remaining three datasets both were about the same. The overall pattern
of performance also appears to favor Rule/5nn over Rule. Thus the empirical results
indicate that our method improved results relative to using each model alone. The
general trend can be seen in Figure 7.
3. Are the new methods competitive with alternative regression methods? Among the previous reported results, MT/3nn is the best performer. Other alternatives to consider
are: Regression Trees (RT) and MARS. None of these three methods were significantly
better than Rule/5nn on any of the datasets under consideration except for RT doing
significantly better on servo. Furthermore, Rule/5nn was significantly better than
MT/3nn on three of five datasets (servo, cpu and mpg) on which comparison is possible. The overall trend also is in favor of Rule/5nn. Comparing RT to Rule/5nn, we
find that except for servo, Rule/5nn is significantly better than RT on all the remaining datasets. Comparing MARS to Rule/5nn, we find that for three of the datasets
(price, peptide and pole), Rule/5nn is significantly better. Hence the empirical results overwhelmingly suggest that our new method is competitive with alternative
regression methods, with hints of superiority over some methods.

8. Discussion

We have considered a new model for rule-based regression and provided comparisons with
tree-based regression. For many applications, strong explanatory capabilities and high dimensional feature selection can make a DNF model quite advantageous. This is particularly
true for knowledge-based applications, for example equipment repair or medical diagnosis,
in contrast to pure pattern recognition applications such as speech recognition.
While rules are similar to trees, the rule representation is potentially more compact
because the rules are not mutually exclusive. This potential of finding a more compact
solution can be particularly important for problems where model interpretation is crucial.
Note that the space of all rules includes the space of all trees. Thus, if a tree solution is the
best, theoretically the rule induction procedure has the potential to find it.
In our experiments, the regression rules generally outperformed the regression trees.
Fewer constant regions were required and the estimated error rates were generally lower.
Finding the DNF regions was substantially more computationally expensive for the regression rules than the regression trees. For the regression rules, fairly complex optimization
techniques were necessary. In addition, experiments must be performed to find the appropriate number of pseudo-classes. This is more a matter of scale: scale of the application
versus the scale of available computing. Excluding the telecommunications application,
none of the cited applications takes more than 15 minutes of cpu time on a SS-20 for a sin399

fiWeiss & Indurkhya
gle pseudo-classification problem and a full cross-validation.7 As computing power increases
the timing distinction is less important. Even a small percentage gain can be quite valuable for the appropriate application (Apte, Damerau, & Weiss, 1994) and computational
requirements are a secondary factor.
We have provided results on several real-world datasets. Mostly, these involve nonlinear relationships. One may wonder how the rule-based method would perform on data
with obvious linear relationships. In our earlier experiments with data exhibiting linear
relationships (for example, the drug study data (Efron, 1988)), the rule-based solutions did
slightly better than trees. However, the true test is real-world data which, often involve
complex non-linear relationships. Comparisons with alternative models can help assess the
effectiveness of the new techniques.
Looking at Figure 7 and Tables 2 and 3, we see that the pure rule-based solutions are
competitive with other models. Additional gains are made when rules are used not for
obtaining the function values directly, but instead used to find the relevant cases which are
then used to compute the function value. The results of these experiments support the
view that this strategy of combining different methods can improve predictive performance.
Strategies similar to ours have been applied before for classification problems (Ting, 1994;
Widmer, 1993) and similar conclusions were drawn from those results. Our results indicate
that the strategy is useful in the regression context too. Our empirical results also support
the contention that for regression, partitioning methods and nearest neighbor methods are
complementary. A solution can be found by partitioning alone, and then the incremental
improvement can be observed when substituting the average y of the k-nearest neighbors for
the median y of a partition. From the perspective of nearest neighbor regression methods,
the sample cases are compartmentalized, simplifying the table lookup for a new case.
While not conclusive, there are hints that our combination strategy is most effective for
small to moderate samples: it is likely that when the sample size grows large, increased
numbers of partitions, in terms of rules or terminal nodes, can compensate for having single
constant-valued regions. This conjecture is supported by the large-sample pole application,
where the incremental gain for the addition of k-nn is small.8
In our experiments we used k-nn with k=5. Depending on the application, a different
value of k might produce better results. The optimal value might be estimated by crossvalidation in a strategy that systematically varies k and picks the value that gives the best
results overall. However, it is unclear whether the increased computational effort will result
in any significant performance gain.
Another practical issue with large samples is the storage requirement: all the cases must
be stored. This can be a serious drawback in real-world applications with limited memory.
However, we tried experiments in which the cases associated with a partition are replaced
by a fewer number of \typical cases". This results in considerable savings in terms of storage
requirements. Results are slightly weaker (though not significantly different).
It would appear that further gains might be obtained by restricting the k-nn to consider
only those features that appear in the path to the leaf node under examination. This might
seem like a good idea because it attempts to ensure that only features that are relevant to
7. A 10-fold cross-validation requires solving a problem essentially 11 times: once on all training cases and
10 times for each group of test cases.
8. Although small, this difference tests as significant because the sample is large.

400

fiRule-based Functional Prediction
the cases in the node, are used in the distance calculations. However, we found results for
this to be weaker.
A number of regression techniques have been presented by others to demonstrate the
advantages of combined models. Most of these combine methods that are independently
invoked. Instead of a typical election where there is one winner, the alternative models
are combined and weighted. These combination techniques have the advantage that the
outputs of different models can be treated as independent variables. They can be combined
in a form of post-processing, after all model outputs are available.
In no way do we contradict the value of these alternative combination techniques. Both
approaches show improved results for various applications. We do conclude, however, that
there are advantages for more complex regression procedures that dynamically mix the
alternative models. These procedures may be particularly strong when there is a fundamental rationale for choice of methods such as partitioning methods, or when properties of
the combined models must be preserved.
We have presented the regression problem with one output variable. This is the classical form for linear models and regression trees. The issue of multiple outputs has not been
directly addressed although such extensions are feasible. This issue and further experimentation await future work. Our model of regression can provide a basis for these efforts,
while leveraging current strong methods in classification rule induction.

References

Apte, C., Damerau, F., & Weiss, S. (1994). Automated Learning of Decison Rules for Text
Categorization. ACM Transactions on Oce Information Systems, 12 (3), 233{251.
Breiman, L. (1993). Stacked regression. Tech. rep., U. of CA. Berkeley.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification and Regression
Tress. Wadsworth, Monterrey, Ca.
Clark, P., & Niblett, T. (1989). The CN2 induction algorithm. Machine Learning, 3,
261{283.
Craven, P., & Wahba, G. (1979). Smoothing noisy data with spline functions. estimating
the correct degree of smoothing by the method of generalized cross-validation. Numer.
Math., 31, 317{403.
Efron, B. (1988). Computer-intensive methods in statistical regression. SIAM Review,
30 (3), 421{449.
Fayyad, U., & Irani, K. (1992). The attribute selection problem in decision tree generation.
In Proceedings of AAAI-92, pp. 104{110 San Jose.
Friedman, J. (1991). Multivariate adaptive regression splines. Annals of Statistics, 19 (1),
1{141.
Friedman, J., & Stuetzle, W. (1981). Projection pursuit regression. J. Amer. Stat. Assoc.,
76, 817{823.
401

fiWeiss & Indurkhya
Girosi, F., & Poggio, T. (1990). Networks and the best approximation property. Biological
Cybernetics, 63, 169{176.
Hartigan, J., & Wong, M. (1979). A k-means clustering algorithm, ALGORITHM AS 136.
Applied Statistics, 28 (1).
Hastie, T., & Tibshirani, R. (1990). Generalized Additive Models. Chapman and Hall.
Jacoby, S., Kowalik, J., & Pizzo, J. (1972). Iterative Methods for Non-linear Optimization
Problems. Prentice-Hall, New Jersey.
Kirpatrick, S., Gelatt, C., & Vecchi, M. (1983). Optimization by simulated annealing.
Science, 220, 671.
LeBlanc, M., & Tibshirani, R. (1993). Combining estimates in regression and classification.
Tech. rep., Department of Statistics, U. of Toronto.
Lebowitz, M. (1985). Categorizing numeric information for generalization. Cognitive Science, 9, 285{308.
Lin, S., & Kernighan, B. (1973). An ecient heuristic for the traveling salesman problem.
Operations Research, 21 (2), 498{516.
McClelland, J., & Rumelhart, D. (1988). Explorations in Parallel Distributed Processing.
MIT Press, Cambridge, Ma.
Michalski, R., Mozetic, I., Hong, J., & Lavrac, N. (1986). The multi-purpose incremental learning system AQ15 and its testing application to three medical domains. In
Proceedings of AAAI-86, pp. 1041{1045 Philadelphia, Pa.
Quinlan, J. (1986). Induction of decision trees. Machine Learning, 1, 81{106.
Quinlan, J. (1987). Simplifying decision trees. International Journal of Man-Machine
Studies, 27, 221{234.
Quinlan, J. (1993). Combining instance-based and model-based learning. In International
Conference on Machine Learning, pp. 236{243.
Ripley, B. (1993). Statistical aspects of neural networks. In Proceedings of Seminair Europeen de Statistique London. Chapman and Hall.
Scheffe, H. (1959). The Analysis of Variance. Wiley, New York.
Ting, K. (1994). The problem of small disjuncts: Its remedy in decision trees. In Proceedings
of the 10th Canadian Conference on Artificial Intelligence, pp. 91{97.
Weiss, S., & Indurkhya, N. (1993a). Optimized Rule Induction. IEEE Expert, 8 (6), 61{69.
Weiss, S., & Indurkhya, N. (1993b). Rule-based regression. In Proceedings of the 13th
International Joint Conference on Artificial Intelligence, pp. 1072{1078.
402

fiRule-based Functional Prediction
Weiss, S., & Indurkhya, N. (1994). Decision tree pruning: Biased or optimal?. In Proceedings
of AAAI-94, pp. 626{632.
Weiss, S., & Kapouleas, I. (1989). An empirical comparison of pattern recognition, neural
nets, and machine learning classification methods. In International Joint Conference
on Artificial Intelligence, pp. 781{787 Detroit, Michigan.
Weiss, S., & Kulikowski, C. (1991). Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems.
Morgan Kaufmann.
Widmer, G. (1993). Combining knowledge-based and instance-based learning to exploit
qualitative knowledge. Informatica, 17, 371{385.
Wolpert, D. (1992). Stacked generalization. Neural Networks, 5, 241{259.

403

fiJournal of Artificial Intelligence Research 3 (1995) 1-24

Submitted 1/95; published 6/95

Induction of First-Order Decision Lists:
Results on Learning the Past Tense of English Verbs

Raymond J. Mooney
Mary Elaine Califf

Department of Computer Sciences, University of Texas
Austin, TX 78712-1188

mooney@cs.utexas.edu
mecaliff@cs.utexas.edu

Abstract

This paper presents a method for inducing logic programs from examples that learns
a new class of concepts called first-order decision lists, defined as ordered lists of clauses
each ending in a cut. The method, called Foidl, is based on Foil (Quinlan, 1990) but
employs intensional background knowledge and avoids the need for explicit negative examples. It is particularly useful for problems that involve rules with specific exceptions,
such as learning the past-tense of English verbs, a task widely studied in the context of the
symbolic/connectionist debate. Foidl is able to learn concise, accurate programs for this
problem from significantly fewer examples than previous methods (both connectionist and
symbolic).

1. Introduction

Inductive logic programming (ILP) is a growing subtopic of machine learning that studies
the induction of Prolog programs from examples in the presence of background knowledge
(Muggleton, 1992; Lavrac & Dzeroski, 1994). Due to the expressiveness of first-order logic,
ILP methods can learn relational and recursive concepts that cannot be represented in the
attribute/value representations assumed by most machine-learning algorithms. ILP methods have successfully induced small programs for sorting and list manipulation (Shapiro,
1983; Sammut & Banerji, 1986; Muggleton & Buntine, 1988; Quinlan & Cameron-Jones,
1993) as well as produced encouraging results on important applications such as predicting protein secondary structure (Muggleton, King, & Sternberg, 1992) and automating the
construction of natural-language parsers (Zelle & Mooney, 1994b).
However, current ILP techniques make important assumptions that restrict their application. Below are three common assumptions:
1. Background knowledge is provided in extensional form as a set of ground literals.
2. Explicit negative examples of the target predicate are available.
3. The target program is expressed in \pure" Prolog where clause-order is irrelevant and
procedural operators such as cut (!) are disallowed.
The currently most well-known and successful ILP systems, Golem (Muggleton & Feng,
1990) and Foil (Quinlan, 1990), both make all three of these assumptions. However, each
of these assumptions brings significant limitations since:
1. An adequate extensional representation of background knowledge is frequently infinite
or intractably large.

c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiMooney & Califf

2. Explicit negative examples are frequently unavailable and an adequate set of negative
examples computed using a closed-world assumption is infinite or intractably large.
3. Concise representation of many concepts requires the use of clause-ordering and/or
cuts (Bergadano, Gunetti, & Trinchero, 1993).
This paper presents a new ILP method called Foidl (First-Order Induction of Decision Lists) which helps overcome each of these limitations by incorporating the following
properties:
1. Background knowledge is represented intensionally as a logic program.
2. No explicit negative examples need be supplied or constructed. An assumption of
output completeness can be used instead to implicitly determine if a hypothesized
clause is overly-general and, if so, to quantify the degree of over-generality by simply
estimating the number of negative examples covered.
3. A learned program can be represented as a first-order decision list, an ordered set of
clauses each ending with a cut. This representation is very useful for problems that
are best represented as general rules with specific exceptions.
As its name implies, Foidl is closely related to Foil and follows a similar top-down,
greedy specialization guided by an information-gain heuristic. However, the algorithm is
substantially modified to address the three advantages listed above. The use of intensional
background knowledge is fairly straightforward and has been incorporated in previous Foil
derivatives (Lavrac & Dzeroski, 1994; Pazzani & Kibler, 1992; Zelle & Mooney, 1994b),
The development of Foidl was motivated by a failure we observed when applying existing ILP methods to a particular problem, that of learning the past tense of English
verbs. This problem has been studied fairly extensively using both connectionist and symbolic methods (Rumelhart & McClelland, 1986; MacWhinney & Leinbach, 1991; Ling,
1994); however, previous efforts used specially-designed feature-based encodings that impose a fixed limit on the length of words and fail to capture the position-independence of
the underlying transformation. We believed that representing the problem as constructing a logic program for the predicate past(X,Y) where X and Y are words represented
as lists of letters (e.g past([a,c,t], [a,c,t,e,d]), past([a,c,h,e], [a,c,h,e,d]),
past([a,r,i,s,e], [a,r,o,s,e])) would produce much better results. However, due to
the limitations mentioned above, we were unable to get reasonable results from either Foil
or Golem. However, by overcoming these limitations, Foidl is able to learn highly accurate programs for the past-tense problem from many fewer examples than required by
previous methods.
The remainder of the paper is organized as follows. Section 2 provides important background material on Foil and on the past-tense learning problem. Section 3 presents the
Foidl algorithm and details how it incorporates the three advantages discussed above. Section 4 presents our results on learning the past-tense of English verbs demonstrating that
Foidl out-performs all previous methods on this problem. Section 5 reviews related work,
Section 6 discusses limitations and future directions, and Section 7 summarizes and presents
our conclusions.
2

fiInduction of First-Order Decision Lists: Learning English Past Tense

2. Background

Since Foidl is based on Foil, this section presents a brief review of this important ILP system; Quinlan (1990), Quinlan and Cameron-Jones (1993), and Cameron-Jones and Quinlan
(1994) provide a more complete description. The section also presents a brief review of
previous work on the English past tense problem.

2.1 FOIL

Foil learns a function-free, first-order, Horn-clause definition of a target predicate in terms
of itself and other background predicates. The input consists of extensional definitions of
these predicates as tuples of constants of specified types. For example, input appropriate
for learning a definition of list membership is:
member(Elt,Lst): { <a,[a]>, <a,[a,b]>, <b,[a,b]>, <a,[a,b,c]>, ...}
components(Lst,Elt,Lst): { <[a],a,[]>, <[a,b],a,[b]>, <[a,b,c],a,[b,c]> ...}

where Elt is a type denoting possible elements which includes a,b,c, and d; Lst is
a type defined as consisting of at lists containing up to three of these elements; and
components(A,B,C) is a background predicate which is true iff A is a list whose first element is B and whose rest is the list C (this must be provided in place of a function for
list construction). Foil also requires negative examples of the target concept, which can
be supplied directly or computed using a closed-world assumption. For the example, the
closed-world assumption would produce all pairs of the form <Elt,Lst> that are not explicitly provided as positive examples (e.g., <b,[a]>).
Given this input, Foil learns a program one clause at a time using a greedy-covering
algorithm that can be summarized as follows:
Let positives-to-cover = positive examples.
While positives-to-cover is not empty
Find a clause, C , that covers a preferably large subset of positives-to-cover
but covers no negative examples.
Add C to the developing definition.
Remove examples covered by C from positives-to-cover.
For example, a clause that might be learned for member during one iteration of this loop is:
member(A,B) :- components(B,A,C).

since it covers all positive examples where the element is the first one in the list but does
not cover any negatives. A clause that could be learned to cover the remaining examples is:
member(A,B) :- components(B,C,D), member(A,D).

Together these two clauses constitute a correct program for member.
The \find a clause" step is implemented by a general-to-specific hill-climbing search that
adds antecedents to the developing clause one at a time. At each step, it evaluates possible
literals that might be added and selects one that maximizes an information-gain heuristic.
The algorithm maintains a set of tuples that satisfy the current clause and includes bindings
for any new variables introduced in the body. The following pseudocode summarizes the
procedure:
3

fiMooney & Califf

Initialize C to R(V1; V2; :::; Vk) :-. where R is the target predicate with arity k.
Initialize T to contain the positive tuples in positives-to-cover and all the negative tuples.
While T contains negative tuples
Find the best literal L to add to the clause.
Form a new training set T containing for each tuple t in T that satisfies L,
all tuples of the form t  b (t and b concatenated) where b is a set of bindings
for the new variables introduced by L such that the literal is satisfied
(i.e., matches a tuple in the extensional definition of its predicate).
Replace T by T .
Foil considers adding literals for all possible variablizations of each predicate as long
as type restrictions are satisfied and at least one of the arguments is an existing variable
bound by the head or a previous literal in the body. Literals are evaluated based on the
number of positive and negative tuples covered, preferring literals that cover many positives
and few negatives. Let T+ denote the number of positive tuples in the set T and define:
I (T ) = , log2 (T+ =jT j):
(1)
The chosen literal is then the one that maximizes:
gain(L) = s  (I (T ) , I (T ));
(2)
where s is the number of tuples in T that have extensions in T (i.e., the number of current
positive tuples covered by L).
Foil also includes many additional features such as: heuristics for pruning the space
of literals searched, methods for including equality, negation as failure, and useful literals
that do not immediately provide gain (determinate literals), pre-pruning and post-pruning
of clauses to prevent over-fitting, and methods for ensuring that induced programs will
terminate. The papers referenced above should be consulted for details on these and other
features.
0

0

0

0

2.2 Learning the Past Tense of English Verbs

Rumelhart and McClelland (1986) were the first to build a computational model of pasttense learning using the classic perceptron algorithm and a special phonemic encoding of
words employing so-called Wickelphones and Wickelfeatures. Their general goal was to show
that connectionist models could account for interesting language-learning behavior that was
previously thought to require explicit rules. This model was heavily criticized by opponents
of the connectionist approach to language acquisition for the relatively poor results achieved
and the heavily-engineered representations and training techniques employed (Pinker &
Prince, 1988; Lachter & Bever, 1988). MacWhinney and Leinbach (1991) attempted to
address some of these criticisms by using a standard multi-layer backpropagation learning
algorithm and a simpler UNIBET encoding of phonemes (in which each of 36 phonemes is
encoded as a single ASCII character).
Ling and Marinov (1993) and Ling (1994) criticize all of the current connectionist models of past-tense acquisition for heavily-engineered representations and poor experimental
methodology. They present more systematic results on a system called SPA (Symbolic Pattern Associator) which uses a slightly modified version of C4.5 (Quinlan, 1993) to build a
4

fiInduction of First-Order Decision Lists: Learning English Past Tense

forest of decision trees that maps a fixed-length input pattern to a fixed-length output pattern. Ling's (1994) head-to-head results show that SPA generalizes significantly better than
backpropagation on a number of variations of the problem employing different phonemic
encodings (e.g., 76% vs. 56% given 500 training examples).
However, all of this previous work encodes the problem as fixed-length pattern association and fails to capture the generativity and position-independence of the true transformation. For example, they use 15-letter patterns like:
a,c,t,_,_,_,_,_,_,_,_,_,_,_,_ => a,c,t,e,d,_,_,_,_,_,_,_,_,_,_

or in UNIBET phonemic encoding:
&,k,t,_,_,_,_,_,_,_,_,_,_,_,_ => &,k,t,I,d,_,_,_,_,_,_,_,_,_,_

where a separate decision tree or output unit is used to predict each character in the output
pattern from all of the input characters. Therefore, learning general rules, such as \add
`ed'," must be repeated at each position where a word can end, and words longer than 15
characters cannot be handled. Also, the best results with SPA exploit a highly-engineered
feature template and a modified version of C4.5's default leaf-labeling strategy that tailor
it to string transformation problems.
Although ILP methods seem more appropriate for this problem, our initial attempts
to apply Foil and Golem to past-tense learning gave very disappointing results (Califf,
1994). Below, we discuss how the three problems listed in the introduction contribute to
the diculty of applying current ILP methods to this problem.
In principle, a background predicate for append is sucient for constructing accurate
past-tense programs when incorporated with an ability to include constants as arguments
or, equivalently, an ability to add literals that bind variables to specific constants (called
theory constants in Foil). However, a background predicate that does not allow appending
with the empty list is more appropriate. We use a predicate called split(A, B, C) which
splits a list A into two non-empty sublists B and C. An intensional definition for split is:
split([X, Y | Z], [X] , [Y | Z]).
split([X | Y], [X | W], Z) :- split(Y,W,Z).

Using split, an \add `ed"' rule can be represented as:
past(A,B) :- split(B,A,[e,d]).

which, in Foil, is learned in the form:
past(A,B) :- split(B,A,C), C = [e,d].

Providing an extensional definition of split that includes all possible strings of 15 or fewer
characters (at least 1021 strings) is clearly intractable. However, providing a partial definition that includes all possible splits of strings that actually appear in the training corpus
is possible and generally sucient. Therefore, providing adequate extensional background
knowledge is cumbersome and requires careful engineering; however, it is not the major
problem.
Supplying an appropriate set of negative examples is more problematic. Using a closedworld assumption to produce all pairs of words in the training set where the second is not
the past-tense of the first is feasible but not very useful. In this case, the clause:
5

fiMooney & Califf

past(A,B) :- split(B,A,C).

is very likely to be learned since it covers most of the positives but very few (if any)
negatives since it is unlikely that a word is a prefix of another word which is not its past
tense. However, this clause is useless for producing the past tense of novel verbs, and, in this
domain, accuracy must be measured by the ability to actually generate correct output for
novel inputs, rather than the ability to classify pre-supplied tuples of arguments as positive
or negative. The obvious solution of supplying all other strings of 15 characters or less as
negative examples of the past tense of each word is clearly intractable. Providing specially
constructed \near-miss" negative examples such as past([a,c,h,e],[a,c,h,e,e,d]), is
very helpful, but requires careful engineering that exploits detailed prior knowledge of the
problem.
In order to address the problem of negative examples, when Quinlan (1994) applied
Foil to this problem, he employed a different target predicate for representing the pasttense transformation.1 He used a three-place predicate past(X,Y,Z) which is true iff the
input word X is transformed into past-tense form by removing its current ending Y and
substituting the ending Z; for example: past([a,c,t], [], [e,d]), past([a,r,i,s,e],
[i,s,e], [o,s,e]). A simple preprocessor can map data for the two-place predicate into
this form. Since a sample of 500 verb pairs contains about 30-40 different end fragments,
this results in a more manageable number of closed-world negatives, approximately 1000
for every positive example in the training set. Using this approach on UNIBET phonemic
encodings, Quinlan obtained slightly better results than Ling's best SPA results that exploited a highly-engineered feature template (83.3% vs. 82.8% with 500 training examples)
and significantly better than SPA's normal results (76.3%). Although the three-place target predicate incorporates some knowledge about the desired transformation, it arguably
requires less representation engineering than most previous methods.
However, Quinlan (1994) notes that his results are still hampered by Foil's inability to
exploit clause order. For example, when using normal alphabetic encoding, Foil quickly
learns a clause sucient for regular verbs:
past(A,B,C) :- B=[], C=[e,d].

However, since this clause still covers a fair number of negative examples due to many
irregular verbs, it continues to add literals. As a result, Foil creates a number of specialized
versions of this clause that together still fail to capture the generality of the underlying
default rule. This problem is compounded by Foil's inability to add constraints such as
\does not end in `e'." Since Foil separates the addition of literals containing variables and
the binding of variables to constants using literals of the form V = c, it cannot learn clauses
like:
past(A,B,C) :- B=[], C=[e,d], not(split(A,D,[e])).

Since a word can be split in several ways, this is clearly not equivalent to the learnable
clause:
past(A,B,C) :- B=[], C=[e,d], not(split(A,D,E)), E /= [e].

1. Quinlan's work on this problem was motivated by our own early attempts to use Foil.

6

fiInduction of First-Order Decision Lists: Learning English Past Tense

Consequently, it must approximate the true rule by learning many clauses of the form:
past(A,B,C) :- B=[], C=[e,d], split(A,D,E), E = [b].
past(A,B,C) :- B=[], C=[e,d], split(A,D,E), E = [d].
...

As a result, Foil generated overly-complex programs containing more than 40 clauses for
both the phonemic and alphabetic versions of the problem.
However, an experienced Prolog programmer would exploit clause order and cuts to
write a concise program that first handles the most-specific exceptions and falls through to
more-general default rules if the exceptions fail to apply. For example, the program:
past(A,B)
past(A,B)
past(A,B)
past(A,B)

::::-

split(A,C,[e,e,p]), split(B,C,[e,p,t]), !.
split(A,C,[y]), split(B,C,[i,e,d]), !.
split(A,C,[e]), split(B,A,[d]), !.
split(B,A,[e,d]).

can be summarized as:
If the word ends in \eep," then replace \eep" with \ept" (e.g., sleep, slept),
else, if the word ends in \y," then replace \y" with \ied"
else, if the word ends in \e," add \d"
else, add \ed."
Foidl can directly learn programs of this form, i.e., ordered sets of clauses each ending in a
cut. We call such programs first-order decision lists due to the similarity to the propositional
decision lists introduced by Rivest (1987). Foidl uses the normal binary target predicate
and requires no explicit negative examples. Therefore, we believe it requires significantly
less representation engineering than all previous work in the area.

3. FOIDL Induction Algorithm

As stated in the introduction, Foidl adds three major features to Foil: 1) Intensional
specification of background knowledge, 2) Output completeness as a substitute for explicit
negative examples, and 3) Support for learning first-order decision lists. The following
subsections describe the modifications made to incorporate these features.

3.1 Intensional Background

As described above, Foil assumes background predicates are provided with extensional
definitions; however, this is burdensome and frequently intractable. Providing an intensional definition in the form of general Prolog clauses is generally preferable. For example,
instead of providing numerous tuples for the components predicates, it is easier to give the
intensional definition:
components([A | B], A, B).

Intentional background definitions are not restricted to function-free pure Prolog and can
exploit all features of the language.
7

fiMooney & Califf

Modifying Foil to use intensional background is straightforward. Instead of matching
a literal against a set of tuples to determine whether or not it covers an example, the
Prolog interpreter is used in an attempt to prove that the literal can be satisfied using the
intensional definitions. Unlike Foil, expanded tuples are not maintained and positive and
negative examples of the target concept are reproved for each alternative specialization of
the developing clause. Therefore, the pseudocode for learning a clause is simply:
Initialize C to R(V1; V2; :::; Vk) :-. where R is the target predicate with arity k.
Initialize T to contain the examples in positives-to-cover and all the negative examples.
While T contains negative tuples
Find the best literal L to add to the clause.
Let T be the subset of examples in T that can still be proved as instances of the
target concept using the specialized clause.
Replace T by T
Since expanded tuples are not produced, the information-gain heuristic for picking the best
literal is simply:
gain(L) = jT j  (I (T ) , I (T )):
(3)
0

0

0

0

3.2 Output Completeness and Implicit Negatives

In order to overcome the need for explicit negative examples, a mode declaration for the
target concept must be provided (i.e., a specification whether each argument is an input (+)
or an output (-)). An assumption of output completeness can then be made, indicating that
for every unique input pattern in the training set, the training set includes all of the correct
output patterns. Therefore, any other output which a program produces for a given input
can be assumed to represent a negative example. This does not require that all positive
examples be part of the training set, only that for each unique input pattern in the training
set, all other positive examples with that input pattern (if any) must also be in the training
set. This assumption is trivially met if the predicate represents a function with a single
unique output for each input.
For example, an assumption of output completeness for the mode declaration past(+,-)
indicates that all of the correct past-tense forms are included for each input word in the
training set. For predicates representing functions, such as past, this implies that the
output for each example is unique and that all other outputs implicitly represent negative examples. However, output completeness can also be applied to non-functional cases
such as append(-,-,+), indicating that all possible pairs of lists that can be appended
together to produce a list are included in the training set (e.g., append([],[a,b],[a,b]),
append([a],[b],[a,b]), append([a,b],[],[a,b])).
Given an output completeness assumption, determining if a clause is overly-general
is straightforward. For each positive example, an output query is made to determine all
outputs for the given input (e.g., past([a,c,t], X)). If any outputs are generated that
are not positive examples, the clause still covers negative examples and requires further
specialization. Note that intensional interpretation of learned clauses is required in order
to answer output queries.
In addition, in order to compute the gain of alternative literals during specialization, the
negative coverage of a clause needs to be quantified. Each incorrect answer to an output
8

fiInduction of First-Order Decision Lists: Learning English Past Tense

query which is ground (i.e., contains no variables) clearly counts as a single negative example (e.g., past([a,c,h,e], [a,c,h,e,e,d])). However, output queries will frequently
produce answers with universally quantified variables. For example, given the overly-general
clause past(A,B) :- split(A,C,D)., the query past([a,c,t], X) generates the answer
past([a,c,t], Y). This implicitly represents coverage of an infinite number of negative
examples. In order to quantify negative coverage, Foidl uses a parameter u to represent a
bound on the number of possible terms. Since the set of all possible terms (the Herbrand
universe of the background knowledge together with the examples) is generally infinite, u
is meant to represent a heuristic estimate of the finite number of these terms that will ever
actually occur in practice (e.g., the number of distinct words in English). The negative coverage represented by a non-ground answer to an output query is then estimated as uv , p,
where v is the number of variable arguments in the answer and p is the number of positive
examples with which the answer unifies. The uv term stands for the number of unique
ground outputs represented by the answer (e.g., the answer append(X,Y,[a,b]) stands for
2
u different ground outputs) and the p term stands for the number of these that represent
positive examples. This allows Foidl to quantify coverage of large numbers of implicit
negative examples without ever explicitly constructing them. It is generally sucient to
estimate u as a fairly large constant (e.g., 1000), and empirically the method is not very
sensitive to its exact value as long as it is significantly greater than the number of ground
outputs ever generated by a clause.
Unfortunately, this estimate is not sensitive enough. For example, both clauses
past(A,B) :- split(A,C,D).
past(A,B) :- split(B,A,C).

cover u implicit negative examples for the output query past([a,c,t], X) since the first
produces the answer past([a,c,t], Y) and the second produces the answer past([a,c,t],
[a,c,t | Y]). However, the second clause is clearly better since it at least requires the output to be the input with some sux added. Since there are presumably more words than
there are words that start with \a-c-t" (assuming the total number of words is finite), the
first clause should be considered to cover more negative examples. Therefore, arguments
that are partially instantiated, such as [a,c,t | Y], are counted as only a fraction of a
variable when calculating v . Specifically, a partially instantiated output argument is scored
as the fraction of its subterms that are variables, e.g., [a,c,t | Y] counts as only 1=4 of
a variable argument. Therefore, the first clause above is scored as covering u implicit negatives and the second as covering only u1=4. Given reasonable values for u and the number
of positives covered by each clause, the literal split(B,A,C) will be preferred.
The revised specialization algorithm that incorporates implicit negatives is:
Initialize C to R(V1; V2; :::; Vk) :-. where R is the target predicate with arity k.
Initialize T to contain the examples in positives-to-cover and output queries for all
positive examples.
While T contains output queries
Find the best literal L to add to the clause.
Let T be the subset of positive examples in T that can still be proved as instances
of the target concept using the specialized clause, plus the output queries in T
0

9

fiMooney & Califf

that still produce incorrect answers.
Replace T by T .
0

Literals are scored as described in the previous section except that jT j is computed as the
number of positive examples in T plus the sum of the number of implicit negatives covered
by each output query in T .

3.3 First-Order Decision Lists

As described above, first-order decision lists are ordered sets of clauses each ending in a
cut. When answering an output query, the cuts simply eliminate all but the first answer
produced when trying the clauses in order. Therefore, this representation is similar to
propositional decision lists (Rivest, 1987), which are ordered lists of pairs (rules) of the
form (ti ; ci) where the test ti is a conjunction of features and ci is a category label and an
example is assigned to the category of the first pair whose test it satisfies.
In the original algorithm of Rivest (1987) and in CN2 (Clark & Niblett, 1989), rules are
learned in the order they appear in the final decision list (i.e., new rules are appended to
the end of the list as they are learned). However, Webb and Brkic (1993) argue for learning
decision lists in the reverse order since most preference functions tend to learn more general
rules first, and these are best positioned as default cases towards the end. They introduce an
algorithm, prepend, that learns decision lists in reverse order and present results indicating
that in most cases it learns simpler decision lists with superior predictive accuracy. Foidl
can be seen as generalizing prepend to the first-order case for target predicates representing
functions. It learns an ordered sequence of clauses in reverse order, resulting in a program
which produces only the first output generated by the first satisfied clause.
The basic operation of the algorithm is best illustrated by a concrete example. For
alphabetic past-tense, the current algorithm easily learns the partial clause:
past(A,B) :- split(B,A,C), C = [e,d].

However, as discussed in section 2.2, this clause still covers negative examples due to irregular verbs. However, it produces correct ground output for a subset of the examples (i.e., the
regular verbs).2 This is an indication that it is best to terminate this clause to handle these
examples, and add earlier clauses in the decision list to handle the remaining examples.
The fact that it produces incorrect answers for other output queries can be safely ignored
in the decision-list framework since these can be handled by earlier clauses. Therefore, the
examples correctly covered by this clause are removed from positives-to-cover and a new
clause is begun. The literals that now provide the best gain are:
past(A,B) :- split(B,A,C), C = [d].

since many of the irregulars are those that just add \d" (since they end in \e"). This clause
also now produces correct ground output for a subset of the examples; however, it is not
complete since it produces incorrect output for examples correctly covered by a previously
learned clause (e.g., past([a,c,t], [a,c,t,d])). Therefore, specialization continues until
all of these cases are also eliminated. This results in the clause:
2. Note that this is untrue until both of the literals are added to this initially empty clause.

10

fiInduction of First-Order Decision Lists: Learning English Past Tense

past(A,B) :- split(B,A,C), C = [d], split(A,D,E), E = [e].

which is added to the front of the decision list and the examples it covers are removed from
positives-to-cover. This approach ensures that every new clause produces correct outputs for
some new subset of the examples but doesn't result in incorrect output for examples already
correctly covered by previously learned clauses. This process continues adding clauses to
the front of the decision list until all of the exceptions are handled and positives-to-cover is
empty.
The resulting clause-specialization algorithm can now be summarized as follows:
Initialize C to R(V1; V2; :::; Vk) :-. where R is the target predicate with arity k.
Initialize T to contain the examples in positives-to-cover and output queries for all
positive examples.
While T contains output queries
Find the best literal L to add to the clause.
Let T be the subset of positive examples in T whose output query still produces
a first answer that unifies with the correct answer, plus the output queries in T
that either
1) Produce a non-ground first answer that unifies with the correct answer, or
2) Produce an incorrect answer but produce a correct answer using a
previously learned clause.
Replace T by T .
0

0

In many cases, this algorithm is able to learn accurate, compact, first-order decision lists
for past tense, like the \expert" program shown in section 2.2. However, due to highly irregular verbs, the algorithm can encounter local-minima in which it is unable to find any literals
that provide positive gain while still covering the required minimum number of examples.3
This was originally handled by terminating search and memorizing any remaining uncovered examples as specific exceptions at the top of the decision list (e.g., past([a,r,i,s,e],
[a,r,o,s,e]) :- !.). However, this can result in premature termination that prevents
the algorithm from finding low-frequency regularities. For example, in the alphabetic version, the system can get stuck trying to learn the complex rule for when to double a final
consonant (e.g., grab ! grabbed) and fail to learn the rule for changing \y" to \ied" since
this is actually less frequent.
The current version, like Foil, tests if the learned clause meets a minimum-accuracy
threshold; however, unlike Foil, only counting as errors incorrect outputs for queries correctly answered by previously learned clauses. If it does not meet the threshold, the clause
is thrown out and the positive examples it covers are memorized at the top of the decision
list. The algorithm then continues to learn clauses for any remaining positive examples.
This allows Foidl to just memorize dicult irregularities, such as consonant doubling, and
still continue on to learn other rules such as changing \y" to \ied."
If the minimum-accuracy threshold is met, the decision-list property is exploited in a
final attempt to still learn a completely accurate program. If the negatives covered by the
clause are all examples that were correctly covered by previously learned clauses, Foidl
3. Like Foil, Foidl includes a parameter for the minimum number of examples that a clause must cover
(normally set to 2).

11

fiMooney & Califf

treats them as \exceptions to the exception to the rule" and returns them to positives-tocover to be covered correctly again by subsequently learned clauses. For example, Foidl
frequently learns the clause:
past(A, B) :- split(A, C, [y]), split(B, C, [i, e, d]).

for changing \y" to \ied." However, this clause incorrectly covers a few examples that are
correctly covered by the previously learned \add `ed"' rule (e.g., bay ! bayed; delay !
delayed). Since these exceptions to the \y" to \ied" rule are a small percentage of the words
that end in \y," the system keeps the rule and returns the examples that just add \ed" to
positives-to-cover. Subsequently, rules such as:
past(A, B) :- split(B, A, [e, d]), split(A, D, [a, y]).

are learned to recover these examples, resulting in a program that is completely consistent
with the training data. By setting the minimum clause-accuracy threshold to 50%, Foidl
only applies this uncovering technique when it results in covering more examples than it
uncovers, thereby guaranteeing progress towards fitting all of the training examples.

3.4 Algorithmic and Implementation Details

This section briey discusses a few additional details of the Foidl algorithm and its implementation. This includes a discussion of the use of modes, types, weak literals, and theory
constants. The current version of Foil includes all of these features in basically the same
form.
Foidl makes use of types and modes to limit the space of literals searched. The argument of each predicate is typed and only literals whose previously-bound arguments are
of the correct type are tested when specializing a clause. For example, split is given the
types split(word,prefix,suffix), preventing the system from further splitting prefixes
and suxes and exploring arbitrary substrings of a word for regularities. Each predicate is
also given a mode declaration, and only literals whose input arguments are all previouslybound variables are tested. For example, split is given the mode split(+,-,-), preventing
a clause from creating new strings by appending together previously generated prefixes and
suxes.
In case no literal provides positive information gain, Foidl gives a small bonus to literals
that introduce new variables. However, the number of such weak literals that can be added
in a row is limited by a user parameter (normally set to 1). For example, this allows the
system to split a word into possible prefixes and suxes, even though this may not provide
gain until these substrings are constrained by subsequent literals.
Theory constants are provided for each type, and literals are tested for binding each
existing variable to each constant of the appropriate type. For example, the literal X=[e,d]
is generated if X is of type suffix. For our runs on past-tense, theory constants are included
for every prefix and sux that occurs in at least two words in the training data. This helps
control training time by limiting the number of literals searched, but does not affect which
literals are actually chosen since the minimum-clause-coverage test prevents Foidl from
choosing literals that don't cover at least two examples anyway.
12

fiInduction of First-Order Decision Lists: Learning English Past Tense

Foidl is currently implemented in both Common Lisp and Quintus Prolog. Unlike
the current Prolog version, the Common Lisp version supports learning recursive clauses4
and output-completeness for non-functional target predicates. However, the Common Lisp
version is significantly slower since it relies on an un-optimized Prolog interpreter and
compiler written in Lisp (from Norvig, 1992). Consequently, all of the presented results
are from the Prolog version running on a Sun SPARCstation 2.5

4. Experimental Results
To test Foidl's performance on the English past tense task, we ran experiments using the
data which Ling (1994) made available in an appendix.

4.1 Experimental Design
The data used consist of 6939 English verb forms in both normal alphabetic form and
UNIBET phoneme representation along with a label indicating the verb form (base, past
tense, past participle, etc), a label indicating whether the form is regular or irregular, and
the Francis-Kucera frequency of the verb. The data include 1390 distinct pairs of base and
past tense verb forms. We ran three different experiments. In one we used the phonetic
forms of all verbs. In the second we used the phonetic forms of the regular verbs only,
because this is the easiest form of the task and because this is the only problem for which
Ling provides learning curves. Finally, we ran trials using the alphabetic forms of all verbs.
The training and testing followed the standard paradigm of splitting the data into testing
and training sets and training on progressively larger samples of the training set. All results
were averaged over 10 trials, and the testing set for each trial contained 500 verbs.
In order to better separate the contribution of using implicit negatives from the contribution of the decision list representation, we also ran experiments with IFoil, a variant
of the system which uses intensional background and the output completeness assumption,
but does not build decision lists.
We ran our own experiments with Foil, Foidl, and IFoil and compared those with the
results from Ling. The Foil experiments were run using Quinlan's representation described
in section 2.2. As in Quinlan (1994), negative examples were provided by using a randomlyselected 25% of those which could be generated using the closed world assumption.6 All
experiments with Foidl and IFoil used the standard default values for the various numeric
parameters (term universe size, 1000; minimum clause coverage, 2; weak literal limit, 1).
The differences among Foil, IFoil, and Foidl were tested for significance using a twotailed paired t-test.
4. Handling intensional interpretation of recursive clauses for the target predicate requires some additional
complexities that have not been discussed in this paper since they are not relevant to decision-lists, which
are generally not recursive.
5. Both versions are available by anonymous FTP from net.cs.utexas.edu in the directory
pub/mooney/foidl.
6. We replicated Quinlan's approach since memory limitations prevented us from using 100% of the generated negatives with larger training sets.

13

fiMooney & Califf

100

80

Accuracy

60

40
FOIDL
IFOIL
FOIL
SPA
Neural Network

20

0
0

100

200

300
Training Examples

400

500

Figure 1: Accuracy on phonetic past tense task using all verbs

4.2 Results
The results for the phonetic task using both regular and irregular verbs are presented
in Figure 1. The graph shows our results with Foil, IFoil, and Foidl along with the
best results from Ling, who did not provide a learning curve for this task. As expected,
Foidl out-performed the other systems on this task, surpassing Ling's best results with 500
examples with only 100 examples. IFoil performed quite poorly, barely beating the neural
network results despite effectively having 100% of the negatives as opposed to Foil's 25%.
This poor performance is due at least in part to overfitting the training data, because IFoil
lacks the noise-handling techniques of Foil6. Foil also has the advantage of the three-place
predicate, which gives it a bias toward learning suxes. IFoil's poor performance on this
task shows that the implicit negatives by themselves are not sucient, and that some other
bias such as decision lists or the three-place predicate and noise-handling is needed. The
differences between Foil and Foidl are significant at the 0.01 level. Those between Foidl
and IFoil are significant at the 0.001 level. The differences between Foil and IFoil are
not significant with 100 training examples or less, but are significant at the 0.001 level with
250 and 500 examples.
Figure 2 presents accuracy results on the phonetic task using regulars only. The curves
for SPA and the neural net are the results reported by Ling. Here again, Foidl outperformed the other systems. This particular task demonstrated one of the problems with
using closed-world negatives. In the regular past tense task, the second argument of Quinlan's 3-place predicate is always the same: an empty list. Therefore, if the constants are
generated from the positive examples, Foil will never produce rules which ground the second argument, since it cannot create negative examples with other constants in the second
argument. This prevents the system from learning a rule to generate the past tense. In order
14

fiInduction of First-Order Decision Lists: Learning English Past Tense

100

80

Accuracy

60

40
FOIDL
IFOIL
FOIL
SPA
Neural Network

20

0
0

50

100

150

200
250
300
Training Examples

350

400

450

500

Figure 2: Accuracy on phonetic past tense task using regulars only
to obtain the results reported here, we introduced extra constants for the second argument
(specifically the constants for the third argument), enabling the closed world assumption
to generate appropriate negatives. On this task, IFoil does seem to gain some advantage
over Foil from being able to effectively use all of the negatives. The regularity of the data
allows both IFoil and Foil to achieve over 90% accuracy at 500 examples. The differences
between Foil and Foidl are significant at the 0.001 level, as are those between IFoil and
Foidl. The differences between IFoil and Foil are not significant with 25 examples, and
are significant at the 0.02 level with 500 examples, but are significant at the 0.001 level with
50-250 training examples.
Results for the alphabetic version appear in Figure 3. This is a task which has not
typically been considered in the literature, but it is of interest to those concerned with
incorporating morphology into natural language understanding systems which deal with
text. It is also the most dicult task, primarily because of consonant doubling. Here we
have results only for Foidl, IFoil, and Foil. Because the alphabetic task is even more
irregular that the full phonetic task, IFoil again overfits the data and performs quite poorly.
The differences between Foil and Foidl are significant at the 0.001 level with 25, 50, 250,
and 500 examples, but only at the 0.1 level with 100 examples. The differences between
IFoil and Foidl are all significant at the 0.001 level. Those between Foil and IFoil are
not significant with 25 training examples and are significant only at the 0.01 level with 50
training examples, but are significant at the 0.001 level with 100 or more examples.
For all three of these tasks, Foidl clearly outperforms the other systems, demonstrating
that the first order decision list bias is a good one for this learning task. A sucient set of
negatives is necessary, and all five of these systems provide them in some way: the neural
network and SPA both learn multiple-class classification tasks (which phoneme belongs in
each position); Foil uses the three-place predicate with closed world negatives; and IFoil
15

fiMooney & Califf

90

80

70

Accuracy

60

50

40

30

FOIDL
IFOIL
FOIL

20

10

0
0

50

100

150

200
250
300
Training Examples

350

400

450

500

Figure 3: Accuracy on alphabetic past tense task
and Foidl, of course, use the output completeness assumption. The primary importance
of the implicit negatives is not that they provide an advantage over propositional and
neural network systems, but that they enable first order systems to perform this task at
all. Without them, some knowledge of the task is required. Foidl's decision lists give it a
significant added advantage, though this advantage is less apparent in the regular phonetic
task, where there are no exceptions.
Clearly, Foidl produces more accurate rules than the other systems, but another consideration is the complexity of the rule sets. For the ILP systems, two good measures of
complexity are the number of rules and number of literals generated. Figure 4 shows the
number of rules generated by Foil, IFoil, and Foidl for the phonetic task using all verbs.
The number of literals generated appears in Figure 5. Since we are interested in generalization and since Foil does not attempt to fit all of the training data, these results do not
include the rules Foidl and IFoil add in order to memorize individual exceptions.7 Although the numbers are comparable with only a few examples, with increasing numbers of
examples, the programs Foil and IFoil generate grow much faster than Foidl's programs.
The large number of rules/literals learned by IFoil show its tendency to overfit the data.
Foidl also generates very comprehensible programs. The following is an example program generated for the alphabetic version of the task using 250 examples (again excluding
the memorized examples).
past(A,B) :- split(A,C,[e,p]), split(B,C,[p,t]),!.
past(A,B) :- split(A,C,[y]), split(B,C,[i,e,d]), split(A,D,[r,y]),!.
past(A,B) :- split(A,C,[y]), split(B,C,[i,e,d]), split(A,D,[l,y]),!.

7. Because of the large number of irregular pasts in English, Foidl memorizes an average of 38 verbs per
trial with 500 examples.

16

fiInduction of First-Order Decision Lists: Learning English Past Tense

80

70

FOIDL
IFOIL
FOIL

Number of Rules

60

50

40

30

20

10

0
0

50

100

150

200
250
300
Training Examples

350

400

450

500

Figure 4: Number of rules created for phonetic past tense task

350

300

FOIDL
IFOIL
FOIL

Number of Literals

250

200

150

100

50

0
0

50

100

150

200
250
300
Training Examples

350

400

450

500

Figure 5: Number of literals created for phonetic past tense task
17

fiMooney & Califf

past(A,B)
past(A,B)
past(A,B)
past(A,B)

::::-

split(B,A,[m,e,d]), split(A,C,[m]), split(A,[s],D),!.
split(B,A,[r,e,d]), split(A,C,[u,r]),!.
split(B,A,[d]), split(A,C,[e]),!.
split(B,A,[e,d]),!.

The training times for the various systems considered in this research are dicult to
compare. Ling does not provide timing results, though we can probably assume based on
research comparing symbolic and neural learning algorithms (Shavlik, Mooney, & Towell,
1991) that SPA runs fairly quickly since it is based on C4.5 and that backpropagation took
considerably longer. Our tests with Foil and Foidl are not directly comparable because
they were run on different architectures. The Foil runs were done on a Sparc 5. For
500 examples, Foil averaged 48 minutes on the phonetic task with all verbs. The Foidl
experiments ran on a Sparc 2 and averaged 1071 minutes on the same task. Even allowing
for the differences in speed of the two machines (about a factor of two), Foidl is quite a
bit slower, probably due largely to the cost of using intentional background and in part to
its implementation in Prolog as opposed to C.

5. Related Work

5.1 Related Work on ILP

Although each of the three features mentioned in the introduction distinguishes Foidl from
most work in Inductive Logic Programming, a number of related pieces of research should be
mentioned. The use of intensional background knowledge is the least distinguishing feature
since a number of other ILP systems also incorporate this aspect. Focl (Pazzani & Kibler,
1992), mFoil (Lavrac & Dzeroski, 1994), Grendel (Cohen, 1992), Forte (Richards &
Mooney, 1995), and Chillin (Zelle & Mooney, 1994a) all use intensional background to
some degree in the context of a Foil-like algorithm. Some other ILP systems which employ
intensional background include early ones by Shapiro (1983) and Sammut and Banerji (1986)
and more recent ones by Bergadano et al. (1993) and Stahl, Tausend, and Wirth (1993).
The use of implicit negatives is significantly more novel. As described in section 3.2, this
approach is considerably different from explicit construction using a closed-world assumption, and therefore can be employed when explicit construction of sucient negative examples is intractable. Bergadano et al. (1993) allows the user to supply an intensional definition
of negative examples that covers a large set of ground instances (e.g (past([a,c,t],X),
not(equal(X,[a,c,t,e,d])))); however, to be equivalent to output completeness, the user
would have to explicitly provide a separate intensional negative definition for each positive
example. The non-monotonic semantics used to eliminate the need for negative examples in
Claudien (De Raedt & Bruynooghe, 1993) has the same effect as an output completeness
assumption in the case where all arguments of the target relation are outputs. However,
output completeness permits more exibility by allowing some arguments to be specified as
inputs and only counting as negative examples those extra outputs generated for specific
inputs in the training set. Flip (Bergadano, 1993) provides a method for learning functional programs without negative examples by making an assumption equivalent to output
completeness for the functional case. Output completeness is more general in that it permits learning non-functional programs as well. Also, unlike Foidl, none of these previous
18

fiInduction of First-Order Decision Lists: Learning English Past Tense

methods provide a way of quantifying implicit negative coverage in the context of a heuristic
top-down specialization algorithm.
The notion of a first-order decision list is unique to Foidl. The only other ILP system
that attempts to learn programs that exploit clause-order and cuts is that of Bergadano et al.
(1993). Their paper discusses many problems with learning arbitrary programs with cuts,
and the brute-force search used in their approach is intractable for most realistic problems.
Instead of addressing the general problem of learning arbitrary programs with cuts, Foidl
is tailored to the specific problem of learning first-order decision lists, which use cuts in a
very stylized manner that is particularly useful for functional problems that involve rules
with exceptions. Bain and Muggleton (1992) and Bain (1992) discuss a technique which uses
negation as failure to handle exceptions. However, using negation as failure is significantly
different from decision lists since it simply prevents a clause from covering exceptions rather
than learning an additional clause that both over-rides an existing clause and specifies the
correct output for a set of exceptions.

5.2 Related Work on Past-Tense Learning
The shortcomings of most previous work on past-tense learning were reviewed in section 2.2,
and the results in section 4 clearly demonstrate the generalization advantage Foidl exhibits
on this problem. However, a couple of issues deserve some additional discussion.
Most of the previous work on this problem has concerned the modelling of various
psychological phenomenon, such as the U-shaped learning curve that children exhibit for
irregular verbs when acquiring language. This paper has not addressed the issue of psychological validity, rather it has focused on performance accuracy after exposure to a fixed
number of training examples. Therefore, we make no specific psychological claims based on
our current results.
However, humans can obviously produce the correct past tense of arbitrarily-long novel
words, which Foidl can easily model while fixed-length feature-based representations clearly
cannot. Ling also developed a version of SPA that eliminates position dependence and fixed
word-length (Ling, 1995) by using a sliding window like that used in NETtalk (Sejnowski
& Rosenberg, 1987). A large window is used which includes 15 letters on either side of
the current position (padded with blanks if necessary) in order to always include the entire
word for all the examples in the corpus. The results on this approach are significantly better
than normal SPA but still inferior to Foidl's results. Also, this approach still requires a
fixed-sized input window which prevents it from handling arbitrary-length irregular verbs.
Recurrent neural networks could also be used to avoid word-length restrictions (Cotrell &
Plunkett, 1991), although it appears that no one has yet applied them to the standard
present-tense to past-tense mapping problem. However, we believe the diculty of training
recurrent networks and their relatively poor ability to maintain state information arbitrarily
long would limit their performance on this task.
Another issue is that of the comprehensibility and transparency of the learned result.
Foidl's programs for past-tense are short, concise, and very readable; unlike the complicated networks, decision forests, and pure logic programs generated by previous approaches.
Ling and Marinov (1993) discusses the possibility of transforming SPA's decision forest into
19

fiMooney & Califf

more comprehensible first-order rules; however, the approach of directly learning first-order
rules from the data seems clearly preferable.

6. Future Work
One obvious topic for future research is Foidl's cognitive modelling abilities in the context
of the past-tense task. Incorporating over-fitting avoidance methods may allow the system
to model the U-shaped learning curve in a manner analogous to that demonstrated by Ling
and Marinov (1993). Its ability to model human results on generating the past tense of
novel psuedo-verbs (e.g., spling ! splang) could also be examined and compared to SPA
(Ling & Marinov, 1993) and connectionist methods.
Although first-order decision lists represent a fairly general class of programs, currently
our only convincing experimental results are on the past-tense problem. Many realistic
problems consist of rules with exceptions, and experimental results on additional applications are needed to support the general utility of this representation.
Despite its advantages, the use of intensional background knowledge in ILP incurs a
significant performance cost, since examples must be continually reproved when testing
alternative literals during specialization. This computation accounts for most of the training
time in Foidl. One approach to improving computational eciency would be to maintain
partial proofs of all examples and incrementally update these proofs as additional literals
are added to the clause. This approach would be more like Foil's approach of maintaining
tuples, but would require using a meta-interpreter in Prolog, which incurs its own significant
overhead. Ecient use of intensional knowledge in ILP could greatly benefit from work on
rapid incremental compilation of logic programs, i.e., incrementally updating compiled code
to account for small changes in the definition of a predicate.
Foidl could potentially benefit from methods for handling noisy data and preventing
over-fitting. Pruning methods employed in Foil and related systems (Quinlan, 1990; Lavrac
& Dzeroski, 1994) could easily be incorporated. In the decision list framework, an alternative
to simply ignoring incorrectly covered examples as noise is to treat them as exceptions to
be handled by subsequently learned clauses (as in the uncovering technique discussed in
section 3.3).
Theoretical results on the learnability of restricted classes of first-order decision lists is
another interesting area for research. Given the results on the PAC-learnability of propositional decision lists (Rivest, 1987) and restricted classes of ILP problems (Dzeroski, Muggleton, & Russell, 1992; Cohen, 1994), an appropriately restricted class of first-order decision
lists should be PAC-learnable.

7. Conclusions
This paper has addressed two main issues: the appropriateness of a first-order learner for
the popular past-tense problem, and the problems of previous ILP systems in handling
functional tasks whose best representation is rules with exceptions. Our results clearly
demonstrate that an ILP system outperforms both the decision-tree and the neural-network
systems previously applied to the past-tense task. This is important since there have been
very few results showing that a first-order learner performs significantly better than apply20

fiInduction of First-Order Decision Lists: Learning English Past Tense

ing propositional learners to the best feature-based encoding of a problem. This research
also demonstrates that there is an ecient and effective algorithm for learning concise,
comprehensible symbolic programs for a small but interesting subproblem in language acquisition. Finally, our work also shows that it is possible to eciently learn logic programs
which involve cuts and exploit clause order for a particular class of problems, and it demonstrates the usefulness of intensional background and implicit negatives. Solutions to many
practical problems seem to require general default rules with characterizable exceptions,
and therefore may be best learned using first-order decision lists.

Acknowledgements
Most of the basic research for this paper was conducted while the first author was on leave at
the University of Sydney supported by a grant to Prof. J.R. Quinlan from the Australian
Research Council. Thanks to Ross Quinlan for providing this enjoyable and productive
opportunity and to both Ross and Mike Cameron-Jones for very important discussions and
pointers that greatly aided the development of Foidl. Thanks also to Ross for aiding us
in running the Foil experiments. Discussions with John Zelle and Cindi Thompson at
the University of Texas also inuenced this work. Partial support was also provided by
grant IRI-9310819 from the National Science Foundation and an MCD fellowship from the
University of Texas awarded to the second author.

References

Bain, M. (1992). Experiments in non-monotonic first-order induction. In Muggleton, S.
(Ed.), Inductive Logic Programming, pp. 423{435. Academic Press, New York, NY.
Bain, M., & Muggleton, S. (1992). Non-monotonic learning. In Muggleton, S. (Ed.), Inductive Logic Programming, pp. 145{162. Academic Press, New York, NY.
Bergadano, F. (1993). An interactive system to learn functional logic programs. In Proceedings of the Thirteenth International Joint Conference on Artificial intelligence, pp.
1044{1049 Chambery, France.
Bergadano, F., Gunetti, D., & Trinchero, U. (1993). The diculties of learning logic programs with cut. Journal of Artificial Intelligence Research, 1, 91{107.
Califf, M. E. (1994). Learning the past tense of English verbs: An inductive logic programming approach. Unpublished project report.
Cameron-Jones, R. M., & Quinlan, J. R. (1994). Ecient top-down induction of logic
programs. SIGART Bulletin, 5 (1), 33{42.
Clark, P., & Niblett, T. (1989). The CN2 induction algorithm. Machine Learning, 3,
261{284.
Cohen, W. W. (1994). Pac-learning nondeterminate clauses. In Proceedings of the Twelfth
National Conference on Artificial Intelligence, pp. 676{681 Seattle, WA.
21

fiMooney & Califf

Cohen, W. (1992). Compiling prior knowledge into an explicit bias. In Proceedings of
the Ninth International Conference on Machine Learning, pp. 102{110 Aberdeen,
Scotland.
Cotrell, G., & Plunkett, K. (1991). Learning the past tense in a recurrent network: Acquiring the mapping from meaning to sounds. In Proceedings of the Thirteenth Annual
Conference of the Cognitive Science Society, pp. 328{333 Chicago, IL.
De Raedt, L., & Bruynooghe, M. (1993). A theory of clausal discovery. In Proceedings of
the Thirteenth International Joint Conference on Artificial intelligence, pp. 1058{1063
Chambery, France.
Dzeroski, S., Muggleton, S., & Russell, S. (1992). Pac-learnability of determinate logic
programs.. In Proceedings of the 1992 Workshop on Computational Learning Theory
Pittsburgh, PA.
Lachter, J., & Bever, T. (1988). The relation between linguistic structure and associative
theories of language learning: A constructive critique of some connectionist learning
models. In Pinker, S., & Mehler, J. (Eds.), Connections and Symbols, pp. 195{247.
MIT Press, Cambridge, MA.
Lavrac, N., & Dzeroski, S. (Eds.). (1994). Inductive Logic Programming: Techniques and
Applications. Ellis Horwood.
Ling, C. X. (1994). Learning the past tense of English verbs: The symbolic pattern associator vs. connectionist models. Journal of Artificial Intelligence Research, 1, 209{229.
Ling, C. X. (1995). Personal communication.
Ling, C. X., & Marinov, M. (1993). Answering the connectionist challenge: A symbolic
model of learning the past tense of English verbs. Cognition, 49 (3), 235{290.
MacWhinney, B., & Leinbach, J. (1991). Implementations are not conceptualizations: Revising the verb model. Cognition, 40, 291{296.
Muggleton, S., & Buntine, W. (1988). Machine invention of first-order predicates by inverting resolution. In Proceedings of the Fifth International Conference on Machine
Learning, pp. 339{352 Ann Arbor, MI.
Muggleton, S., & Feng, C. (1990). Ecient induction of logic programs. In Proceedings of
the First Conference on Algorithmic Learning Theory Tokyo, Japan. Ohmsha.
Muggleton, S., King, R., & Sternberg, M. (1992). Protein secondary structure prediction
using logic-based machine learning. Protein Engineering, 5 (7), 647{657.
Muggleton, S. H. (Ed.). (1992). Inductive Logic Programming. Academic Press, New York,
NY.
Norvig, P. (1992). Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp. Morgan Kaufmann, San Mateo, CA.
22

fiInduction of First-Order Decision Lists: Learning English Past Tense

Pazzani, M., & Kibler, D. (1992). The utility of background knowledge in inductive learning.
Machine Learning, 9, 57{94.
Pinker, S., & Prince, A. (1988). On language and connectionism: Analysis of a parallel
distributed model of language acquisition. In Pinker, S., & Mehler, J. (Eds.), Connections and Symbols, pp. 73{193. MIT Press, Cambridge, MA.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann, San
Mateo,CA.
Quinlan, J. R. (1994). Past tenses of verbs and first-order learning. In Zhang, C., Debenham,
J., & Lukose, D. (Eds.), Proceedings of the Seventh Australian Joint Conference on
Artificial Intelligence, pp. 13{20 Singapore. World Scientific.
Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: A midterm report. In Proceedings
of the European Conference on Machine Learning, pp. 3{20 Vienna.
Quinlan, J. (1990). Learning logical definitions from relations. Machine Learning, 5 (3),
239{266.
Richards, B. L., & Mooney, R. J. (1995). Automated refinement of first-order Horn-clause
domain theories. Machine Learning, in press.
Rivest, R. L. . (1987). Learning decision lists. Machine Learning, 2 (3), 229{246.
Rumelhart, D. E., & McClelland, J. (1986). On learning the past tense of English verbs. In
Rumelhart, D. E., & McClelland, J. L. (Eds.), Parallel Distributed Processing, Vol.
II, pp. 216{271. MIT Press, Cambridge, MA.
Sammut, C., & Banerji, R. B. (1986). Learning concepts by asking questions. In Michalski,
R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning: An AI Approach,
Vol. II, pp. 167{191. Morgan Kaufman.
Sejnowski, T. J., & Rosenberg, C. (1987). Parallel networks that learn to pronounce English
text. Complex Systems, 1, 145{168.
Shapiro, E. (1983). Algorithmic Program Debugging. MIT Press, Cambridge, MA.
Shavlik, J. W., Mooney, R. J., & Towell, G. G. (1991). Symbolic and neural learning
algorithms: An experimental comparison. Machine Learning, 6, 111{143.
Stahl, I., Tausend, B., & Wirth, R. (1993). Two methods for improving inductive logic
programming systems. In Machine Learning: ECML-93, pp. 41{55 Vienna.
Webb, G. I., & Brkic, N. (1993). Learning decision lists by prepending inferred rules. In
Proceedings of the Australian Workshop on Machine Learning and Hybrid Systems,
pp. 6{10 Melbourne, Australia.
Zelle, J. M., & Mooney, R. J. (1994a). Combining top-down and bottom-up methods in
inductive logic programming. In Proceedings of the Eleventh International Conference
on Machine Learning New Brunswick, NJ.
23

fiMooney & Califf

Zelle, J. M., & Mooney, R. J. (1994b). Inducing deterministic Prolog parsers from treebanks:
A machine learning approach. In Proceedings of the Twelfth National Conference on
Artificial Intelligence, pp. 748{753 Seattle, WA.

24

fi	
fffi 	


 	 !"$#&%(')*),+-/.*02143(.*5*)

6,798$:;<+=),+2>@?97
 !:"A;B'*C*=),+

DFEHGIEKJMLONP,QRLKS<PTUGVTXWZY[N,L]\_^`Ea^b\cG_dcEaJfehgjikN,Pl$LKS<PTUG

mXn/o	nqpsrut<nqv2o	wxzy{}|xz~M<vuo

X,Buu

/uUK]<9RA@OuXqu4a@@4,@,u9
4@A,_X@, q@A

RA//`q
c9,s&qq9_,/`	**@9,9Ac_9A4	 qk/q9
 q/A9RqqAX,}A}	A(*9M(*9A $,*}*q9X*c

**X(/@h***@9@Au$,AX_A/a,9/qKR	qq
,9/q9*:	A/429A9X//c*(9A,/A
/,	U9(*9A $R_9,A_Mq*Ka*u$qH<,/(A
	,	**@9 	**@9,9A$qR/	/B/	$*$/(*	
$2**<A/429Aq*M/	K /	/(9M:	 29R,a(9*,
$q(9	q,/A_M/&q9(<	A@9q9	A(A	4,
A&<(*9A***(9*$q,(9AqU&*:	Xqa
	/	*$:	9 /(* u*9,<99 ,$(IB,(9***(9*,Ms
/uq:	**kq/OR(9*]*_U,HA**@9$	**@9,9A*
<*9A as*@__,	/,qA**@9,	/a*H(*	_9
(9X	**@9,9A<q*<,/A9 $qa*9q9A9 		
(9AMA*u*KuzA<**A@9A	A(*9q(/a*&X<	*
q9A $$(,Oq9I*9A**@,	q*</(*}9O(9
,,XA**@,	q*R	`aq9A 

q

M_ 




	fiffff
	!	#"$	#%&'(ff")*%+$,	-(.0/123!4(	"5ff6%72%	#89&":ff
	
	(;=<>"$?@+
ff6BA5	DCEF8G8GB"
IHJ<KCLFMNHPO+
%	#ff"E.Q-RRQ.SQ-RRT5MVU3<KCLFW("X7		(	#"
Yff$	!&"5ff	#	(#ff"3ZB"
?@+$#ff&A5	8[B"
	!%	("B"
I"$?\-8G2+ffff6"
%0%U]<>"^&"$?_+$#ff&A5	
8N2&"$	`%	-6"2&"$Gff$	*%aSffG?2	#A5	#%bff	(2"c5+$	(dPZB"
?@+$#&"$[5e_ff$	(	(d8f	#g28G2%	(
HP7
	#Aff6"
VMVUihjeY+$6B"
ff$	Z[,		(	#"kff,ff"lP8N%B8ma-8G2+ffff6"
%E%nHP#%+$	(VM
op5e_ff
	-	(q"$?Y	g8G2%	-(.<rCEFD-"*(A5	,-8N	jff6$	%&&8s&ffff6"
pt%(%28N2&"$	j%	("2&"$
	#2,	-,	"5ffff6"u(.E+$bi?	-#"vff	(	(H>w1+B"%"E.Q-Rxy5MVU
hqel+
&"$b!#%+
%,		(	"5ff,ff"X/z	*$-A5	Gff
	s7B%&Bff>eff!%	-6"I%B%ff>e_	-z5e@ff6$	(	-
?2	(#&7$7%	!&"I{2,ff|>,?2	#[%k.a&";$6ff+%Nff$	NB8Gff,"kffG#%,S	(+&A5	!5e@ff6$	(	((U
} "$ff6$	#!?@Ak"kff,	~Z+$6B"
;~#%+
%Z	#2,	-	#"5ffff6"3ff6$ffv%+$%Sff6$	(	-v		(eIff
8N"2&2+2%ff	iPj8N2&"$	i%	("2&"$n%6Bff628N(Up?_+$	Sff=ff
ff
"$	(jff`#%+
%tff
	-6e
75eD?2?@&"$b?2	#%	#ff&"$#%+$	(`=%B&ff	#%d
-A5	G%	(`"
?~&8s%	n		(ff,`"Dff
	s	#"$	#%&Bff>e
ff
	0ff6$	(e@Uj$	S,	-?	i	#o	#	(?[ff,ff/z&"kff6_?@+
ff6"
0ff,<KCLFp.5"$	Z2	(	#"5ff	(?[75e[O+$%	#ff"
"$?G=	S	(?@ff0HQ-RR5M."
?s"
	i7keYCL-A_Z
 "
?s*'( 	,@aH6Q-RR5MVUjCL-A_2=
 "$?Y=	iS	-?_ff0HQ-RR5M
2,	-,	"5ff=	(-	#"5ff=+26A5	ev<KCLF,	-,	-,U
	S+$	iff6$	Zo%&%(/iB"
*?2	#{2"Bff6"&"$?@+
ff6"EU } ff
	-6eDH7
2,+2"
?v_"$(/i%	-?	Mj`.2	#ff
t&ff&A5	S	g8G2%	-q'S
 -((-   "$?Y=	fft"
	-ff6BA5	S	#g8s%	(q*'0
 (-(#  `
  Zff	#ff
("$(	ffS	Z&A5	"EUj$	#"!`5e_ff0
 	-SPff
	0ff,	#ffS("$(	ffiq"~PuVKo
V#->=&
"$?"%BeNB
 ')*),+Mfi

2





a

K

J 

fi !! 9	;*:	; K* `ff8R9 /7
 !"A! fi

"!! *;

fi
kVtKS2V
 b((  
 [)]fi
0
 (-  
[ [  ]fi
 

Z
  (-   
5[ [ ]*fi
`
 I
Yz26$[6B6B5Ns(
2I$*[(i($,-5$#$(!S
[
-6
$kk22zZ-u-
-5$#$(St
i
-6,6$#i&*6$i5_$((2$Y$Z$(&5
#2G2*$D
Y[ (j($, -5$#$(s q 
6$ ( $fi6$*5@6$((NZ&$N$,

#2, -,56    S (-  0`$D Z ((  d`#Sp#
((
Y2j
zS($(#k6,`G$Z22#&YB
@$#&5Z(B
*$@&$*$,

`#$#&(4i#z&&5s#2G2((  
z(a$@&$D#
Y

$
 ]fi
 b(-  


 [!
6#2&4B5#(,-\BI-6*
,Z
,B($(SB
-v5#I#$#&-6
z!#dz#
-==![#$#&-64q
n(`#$#z#$#&(Iq2#=
# 
-a
,PSS(#$#t#$#&-6GBY,[#$q#2,-#5zB

,B-6
(
-z#$##$#&-6b($#5Si&[6$=$(&5`#2G2-i

5#i
,
#@1($6,5=#$#&(

*[S
i
~65o,VI
=o
,B(IZBG2&-6E6B
-&2
@
6=2#2
-~&D#N`q-q($(5$#$(s6@&D$-#&(4 -5  -5_   
(25$No6$G-G24(#$#1
,B-6
Yi#
-2$Y!,
(B->62$62G2S#fi
0(b((2(fi*
B5,,-L
BSZ#D$(
&$,-LBG2&($&$(0&i(zY-G2kZ-z5#(2
,`*@&##$(`>z-#
>$s]
^&sB(i\N6BN(`($(*$#$#&-6
[2,B
-\5
6 @& (25$,*`-5#>#$#&-6t=i&!,-(#i*&sB-6E

2#S$=o&(i&$G$(i&!i2
 `
(S$$((-,S2
6 

  	ff
fi_ 5 
    	ff
fi_  55
    	ff
fi_ _ 6   
fi_ kp 5 6p 
    

fifi__ kk"" !! 6  		

fifi__$$ #!   


0#$    j`(#$#a
,B(!2
2#Z>2
2G26$ %'&(&d   E  $!
 
 $!
 $G
d#
  zz`(z
,#$#&(
2




2

#

i

&

s



B






6





fi


"
)
%
*
&
&





 
SZ#(S
   Z66BN 
,6$  
D2
2#`>$s;
v$
&sB-6E i1*#-i6$ 0z[,d2V*BvY2#22&D$62,5*
(!,-#2,B5n$(($
,B(l$Z>$sz$i5#v2(k
ku
&B&$6,-\-5k^
[2&&B>Ib-6^,-#2,&5$(**#
Kq6B
-[,-#2,X

$6 =2,f 6$#YZ2,N(
I(6 zG2(6BN$N[BG,5,-62&G-
-#B
~#$#&(X$ 
>$s $~(#kY6$(#-&B^
@;0
,B-64$2#YBG2&-6E 
(6 zz2(#5b~(25$D;(@
-DBG2&-6\I>$s3$(] 
&k6_@
6E&B,,,
 +B
B&@_i(B,(
(s2,$p(s#_B>G$[2626[,
@# $,-!&!(#
 -
Z2Ni
d,-i,So GB&qi&N$Z$6Z$6
S $[
$S&.%L=
-0 /5 $ 1 N0z$(#(@&$v $
3
-0/0G
 24%L-k -5 5 &=B&#(
65 G B
Y" %(2

798:

fi;<>=<>?A@CBDFEG@0HADJIK=.IKLNMOB@PQ<>QRP=Sff<T?UVOW>BDJX>@0HADJIK=
[Y Z]\_^C`O^Ca>bKcde bfffTdg`hgKi3jkcbKlOmn^Cm
oprqtsuvnwxuyv{zn|(opff}~ffqt>u_z0nF,wz0nwxvnN]pr"o,qtsu|Jwvn.uznw|znw0upffuwvnFyvnqFznphzn|
Jvn~ffxuG}u>uFznKuG}{>,FznqApG>n RG>ARG>AGv>,sv0Kuyup{v0yGznw}uy}~ffxsp>qxuwxuyqyop
qs JxuGqFznpu} uGxwKuqsF.|Fwxvnuznw	s JxsJNv0xuy}znpvwxuFvnqFznppffzGpv0.n
~ ~ ] qFznpvnpff}qsu(.z0q]Kznwtqxvnp>qRwxuy~qxyznppffuGqxuy}qtsqy
 pffGu0upuwxvnJyvnqFznp~pff}uwn~ ~ ] qFznpF)pffznqt~NFupTq|znw0upffuwvnFyvnqFznpzn|wxuy~ wx>u
Jvn~ffxuGv0Nszyppqtsup>qtwxzA}~qFznp*uRqt~}Aqsffuqsuyznwzn|*0upuwxvnFGvnqtJznp~ p} uw
] FGvnqtJznp)upffznqxuqsffvnqNFGvnqznp.KuquyupFvn~xuyF~ p}uyJ} vnFu0vnpff}]uRKqsffuwu|"znwxu
pTqtwxzA}~ffGukvwxuyqtwFquG}|znwtzn|)NJyvnqFznpCGvnFuG}$NFGvnqtJznp
K. A$>AnC''A nx   '9C
 >O'J9C Jvn~ffxu4AGGA]Ov*Jvn~ffxu} upffznqxuy}rT|[vnp}znp3|KqsffuwuuFqv
~ qtqt~qFznpRt~xsqsffvnq*r[zFvn~uGvnp}vnwxuG09GAKTGAG G9 
}upznquG}h|,vnp}znp.|'vnpff}r]
n~ ~ ] qFznp4JNwxuu>uvnpff}qwxvnpffq>u0zJvn~ffxuGvGKuuG>~0vnFup>q~ p} uw_n
~ ~ ] qFznpqsffzn~qKupN0vnwFvnp>qxy*zkFvn~uGvnp}vnwu999A$}upznqxuy}h
|'qsffuvnwxu*uy>~vn~ qxz0vnwtJvn JuwxupvnNpff 
 ff znpffJ} uwRqtsu|znJzypff]Fvn~xuyy
44[$C A$  	A
 fffi> A fi A xC 
4n["C A$  A
 tC A$ xC' Cvnp}
 9[$C A$ t' 
usvy>u tpffGu fi n !"#  $
  pffGu(% &'n( #k$
 ]ffvnpff}qts>~$
 ku
vnFxzsvG>)u r	 *upffGuT  +
  vnpff}q[ +
 , ]
sffuGznwutqxvnqxuyOqsvnqn~ ~ ] qFznp]Kuquyup]Fvn~xuyJO}uyJ} vnFu0,sF'v0/ .wqszyp
>% 0Rzn pffxznpG2 1 3Affffv005u 40>
687 A :
 9  < ;A' $$J = ff > nx   '9C OT ?A 
 ;A   2 @BA[] 9r 
 D
C J9TE DFGff
 GffI H0$ C 9t.> C > JrK
 up>qtJznpffuG}pqsffup>qwxzA}~ffqtJznp'Ku*vnwuffvnwqF~Fvnwp>qxuwxuyqxuy}pJuyv0qR0upuwxvn0upuw
vnJyvnqFznpysu.vnpwxuyv0xznpFqsvnq(vJuyv0q0upuwxvnO0upuwxvnJyvnqFznpp~}uyRqsffup |znwvnqFznp
zn|vnyznptJqup>q0upuwxvnJyvnqFznpy
 >O'J9C Fvn~xuJkM
v L>K9 O N9A>AG G9zn|v_xuqNzn|Jvn~ffxuGQ P

 R2ESESx T#|(vnp} znp||"znwu>uwtV
 UXWYU[Z  \  0upuwxvnJyvnqFznp~pff}uwn
~ ~ ] qFznp{zn]| PrFv_Jy] L>K9^ L>K9 O N9KTGAGGA9" _)_>Rzn`| P|
vnp}znp |A|"znwu>uw0upffuwvnFyvnqFznp~ p} uwn~ ~ ] qFznpb a	znc| PAb ak
 ff znpffJ} uwRqtsu|znJzypff]Fvn~xuyy
4 [
 dc  d  A e 
 4[f e A e gA$C 
4 [
 `
  A /A e  vnpff}
h4 [
 
 A A /A e gAf fi> A  
iznqtsFvn~uGj
  vnpk} hvnwxu* _b_0zn| ) x% #>
lnm&o

fipqrtsuwvyxcz|{~}xc(su
f2Ifj)k5ff fft`2Ej V%Q(S 2S &8
 ffQIkfQ!S O2ffStc!<O25Ej!E 'E	ff22Q 25I
<]Kb"5/ !O5j	ffSS
8'ffwjw ff]k  tgI:cO!`w`ntwIE
ffEI"I2O
"n
"  ]j
 %b"58~ffSjSt%c!<O%ff2]S!(SE 'E j2
Oj!ffS`j2!! O"Q~ffYE2I5Qt&t~EQ!O
<(ffO t
( '&ff
 'y'SM	
t'
fE!Vb!ff%2bI"ffI !`+282ffOE!BVOff'ffO 
E!OffffE`ff~SffS(<ff%S(jEQ2/j  ~(2SO8 O &
  cny  b I ff
 kyffSK
 'O/kO%	 '(
"</

 &
 
	 fffi`%Sbff
 I  E< 2nOE''  &|n 
ffIEK
 kOffk	
 ff
 
f`OO
<Ejk!ffjOSff<O bIffOO t`8SS(S O<
 ~
 QOS]!ff(E O2 ~"fQ!
jV/ ~!ff"OSjffS 
 ff  g" !$#&%'#)( +

 *,g- %.#(/#0! #yff
1ff  g" !2#0%'#(t3 *4g5 (/#0!$#&%' 6
%`ffE )
 7OffS8
 ]5S   S SjOYIOffffk 5SO  
9 SO E%jOYIO9 :)5E 54
 < ; kffffk k=
 > ; 
f2(SOS5OSfffkSt2t`ff
E
EScE2 25~jIffk2Ejtk!ffQ5(E t5ff8(S 
KffS~5O~kQOE)ff?X'E58ffSES!Of!ff!
fO
'ffSj$!ffQOSQ!O`S !(f`E+ffS!ff
fff

@ wffcffwyBA gI  n CQntwIE+D F
 EG 
  
c(2OIH%2b(ES E |EI  O  E2JH2/ff22%'&?K~!

OE!K(fSMffIE~!OffS5ffE2j ff2(E S E

 L'Q'M L<ffE22225J N 
8'ffwPO &QY'<SR/

T8
V/
 <nyRctU<'ffS'2gI  n jn2wE
 W 

EI5nO
b"y& n'I]!?
  V  "  
X~O!YE22 5ff2OS !ffOE!M(f`EE85S2V
S~2jI~(SYS EkI(" YK E~!<9 Z /2ff'fE2 2
O+kE22I~2IS$ffQQOS
b"OS~8
! /Kb"

[]\_^

fi`badc.adegfih_jlkmfJngjpoqcroqsutvh_f.w.xadxyw.c.z'a{e
|5}v~dh_jpdfJngjpoqc
rdvp]ifi7l .&Il
dm]p5 gd55  r& ?p '&m7
_Jmm&/  .I /M" d1  J') lm 0p S'Su
 lm 0p F 9lp_9dm]/dq)]_V]rgdV_V]"80  . S
MS d0J.& p l /./  lm 0p  v3.. 
2'I9 'p/y0. p'l .&
 V"/34-S 
  V{5v,5 
 V"i0="g   .
 VVd+,5d2 
'l .)l     0  .Gl 8    0d 3.r-l .)
yl80l0? &J.& +. .u8-l .) q S./  p l  ././
 5 ./  l $ .8 
 /S
   . .
 
 S/	
 
  

.'')fi ffgl& I8 yd
 /S&& yl .&p03 G    & /p 
m3d.r  l l 
 q53m l .&p/./mp/ SlJlp ' GJ')  
89l.    S) Sl 
	 -I9 J ]i
 .&
  lm 0p 
 q53mp '&m?puS./mlS Sl{3.&d) g.I) .J
 
   p l   p3-  p l $Sl& lr/mlS Sl
 q53mp '&m  Vup?m lm
  lm 0p G0'mulS S'mG      
 S/)&  & S')0r   5G S  )m)d& 0p 
3.&0.. 0p ' .0& .u& 3l .)m" !$#d p .00Sl $ ' )0 &8 3&& 
p '&m 
rdvp]i
2%
 qp '&J   dmm  i .&b 2J 0p /pS .&
 B&b &? 







,

+


m



m







+
'0.rS0 q8)( '3" yy v    p *
 )J 
 -. .& . -M mm
-/0
 /  d
rdvp]i2 13q4 '/l 2 p '&{   Jmm  i .& J l Sly /0'
r/&b 2l .&. .r'8)3 2lS.l 
g5
 q p+gmS0. 7
 6   d .&
 1l
 8;:  pmfim; <)pJV]" y       9 . /?   = +   mm  ; +  i / 1 ')  mm  
9
 & l. .) {)2 .?
 >   Jmm  . A
 @
rdvp]i
2 _ mm&. q8& l .)m0.&0.  Jmm& .d8.J 0p /p
 ? $qB
 qr& +l .& 1?4 '/l  .
  r& )0r  3.C
 l8 0
)2  _]m0. dD1
      9 . /?
dE 1
pbF
 !$#d p4 '/l "  _]m0.       S '
iGpH SS& ..'S $&? . S)0rygmS0.u _ m0. 1 
5I =lK Jd. &'&u &? .4 /&?gmS0. _]m0. 1'?
 p
_



J

m



m



&



/






." '0m 2 
d&1
   
rdvp]iI2= '  ql .& .L
  r)0)  _ dM1       d .





    S. & Q
 P 'v '
 SR '3"  y0 )  1  3p '& .
NO V)
  )T Sg UJ pmg3gd NO 55       -'S. )mW
 V PX  .v ' Sv  PX 
 .   PX  ') 3l&&y    &d
Y[Z]\

fi^`_ba]cTd,e(fhgjilkfhmn4opcTd

qsrtvuwtyx;zt9tyxbuD{4u|4}~t~r}rsE~94~pKzt~r}~p5~}b{4uufi}b{4u}]t3rltyx;uDvxbr~=uDrstyxbuD]$rpufi
y;yt~t4ty~pr}?w}Ltx;uFrprK2~}b~GHuFvz=tyx;ztzfipz;vu9E~94~puKzDfipz;u7~tyxbr4t
u~pfi~tyDytzty~};*bHu*u=z}*tx;zt0E~F~pu=0 = t=z59~}~wztvuyvufitsr2.$2qsrtvu
tyxbzt~hE~94~u=0 = t=z79~}~wztvuyufitsrhtx;u}ME~F~puKl0 = t=z}$Dtvuy
vutlr3
 bEr};~{ufityxbu rr=~}b*zbvu=z}b{C3b4;ty~ty4t~r}"ufitsrhtvuywlWz};{Mvut

rzbvu=GE0IT
WR`R(R("fi
WR`]yGR"fi
&.b$
?.(TRTT  v$z};{
E0Il,RRyERfi
,R  4R`Ryfi
,Rb]4R`  4R$
Exbufi}M%~pz*F~}4~wztvuyufitrI.&7$M0 = t=.5$z}b{DH0ylI~pstyx;u5~};ytz};=u*vutr
0 = t=bILu0x;zK$u tx;ztIE0I  9z};{Dtx]bC32qsrtvu tx;zt03z}b{3tx;zt
	  3
~$uM~94~=zty~pr}H~94~=zty~pr}A~p*vu;u~$u]44tD}4~$u~94~=zty~pr}H~94~=zty~pr}A~p
};rt9tyz};y~ty~$u{4u=ytvz9lw]~t=sKz$TAEx;u&ufipzty~pr};x4~AufitHuKu}?~94~=zty~pr}?z};{?E
~F~pKzt~r}b{uKvy~u={~}Errpzy5ur=0brprKlpvrIuyvz}b{tyxbuKrufiCE(rsz54vrr
rsuy4z};{tyx;u=rvutx;uvu=z{4uI~pEufiufivu={3tr7z5rr*$DGx;z};9z};{DRuKu7yK$(;zu .fi
}Dr4lvrrhrhErvrpzyD5HuIbvutx;u}brty~pr}rtyxbu=rFu*u}$tErz7pz;vu]
 $h"[(Wut&K==KvI s=K==I47uMzfipz;vu]2zufitr5fipz;uK=sz};{
*
.    K==fivb[4]5z9]$rpufiy4byty~tyty~pr}Lr  . t.lEx;u}Dtyxbu vutsrhvr};{4}4~t
zbvuK7$`  =K=.=`  j=  jK==.=  jRF~pItx;uCT[IK r	$C
0 = t=;
 fi.fi ;s
  
 ,fi 4 "!#$
 &l
% ')(*!#+!,.-/!0(1!#'2-/!435'768- %9');:)=<>- %?A@BC$
	ff
 ,fi 4&
$D*-/!+A!E'03FB-/GH(!#'2-/!43I'J6K!#+,LH&%KM)C)(;:N-4!+G')+$! &% ')(*!#+!O-PQ,0R
S  fi.Tfi8UWV$XF[(Y 
 "h]+Z7AP[$[(; 	]\ [$(?_^$G`');: 
6+ 8/')(7!#$!8RbaY@BBc
'd- H
% WQwe,% fCK,! [KGCe!#$&%I.&QA@B9Whg5');:
6id- H
% W`A@BjA@B8C$k+D)-/!+A!E'EG8C l!#+
 &I
% .!8;( $@jA@B'm
 QnR
o_fi,p$qQz$5`

   tyxbufi}BE0IC 9G2x;uvu&~ztvuy vut9r .$   . t.
.5$Dsufi}bKu]H0ylFr   ts "x;uvu ~tx;uwKr94pufiwu}]t0r$LvuGMEx;u=rvuxw
.55
 r  "
 s z}b{&tyx$;sWD
(h tx;u}Q.50
 r  y
 s ;x;uvu ~tx;u5=rFuwufi}$trh$DC0 = t=R.5$Ex;u}
$ExbuKrufiz
 wtx;uvuDu~pyt5z3tufivut5r0.&Dybvxtx;zt7E0I?
 r  {
 s z}b{tyx$;
H0yls ?FExbufi}&${4u|4}4~ty~pr}MWQw3}
 |
~)

fiJJ8*QFJ9B7nFJJ*

O4f89]fIf$ffBfffIA/mffmfm8fjffffGT/fG.*1fT$8
/fGff>/mffJ.BBh8fBTm$GxmIAQT/8fGfffG/
Gf*O$ffn58f$89Bf/QT/mff0I]Pf/m0BTfQT>/B$fm_Gf
/ fff$O/QT/m?NfBOtBnT85f9mm8$f/IAQT/Q
0YB PffIfB9GHPf/m0Bfm9NTfBhNfh$9fjIH
yfi4G9ti+
yfi4G0tiH
yy#4+T5}A+4 +f
+4J
IB/mNOh$8$fJJfff0tmf/J8G/ftOf/Gfmf
IBK$5f$GF5_QjfT$Gt5fN>TOBf0B8B$KG>0f>54]9
TNiBf/JhfhBfN54]9.`fG7Bh 
 N>85JjHI
5;$4++;7AT$]GjIA]&0  NfhGJ>y  N
 8.5IB8.Bf?/>ffTBTj8 HfB0GWfHIA/mYvIJ
5
IA/mffkG/58fJk$ff58.fB8]fffvf
B8yTmfT//nf IA/8fGfj58nfB8
_	ff
fi !#""5$% &(')"+*-,0h/.10	,F3246578E9.
_ :;<bi5=
 2{GQHGGTGfGfEO?>WvA@H ff
NfEhhJCB ] m m0bJI.IQmJv]DB3E 54]9 hfnmJODBF>}GB
fJEOD BH2IBbIBJKJ $f*GfML]DBzIB&ODB  GB+IfBON7BJ/
54]9  I
 BBfBhBJh( PT/fyhR
 Q
[ /]V	
^ `_aA 6bc7; Sed A 		]JV	ff
f_ ghffi
j]P7kl8V#mnBoIpq8!D""
SUT V#TXWZY\j
>U%8&8'$"n*r,Kh/.Isl578f657 &tuvw"6"xy&*z!/&n%*f# v{g|v ,5h?.

 :;<K?QBOT8PTGff IA/QT/mffh5KB8J>54]9  IBn0B$O"0k 9f
_
{J}Bk   bJEkB8JQf_IA]&KKOh$]fIf$fTBfmOfBGB]hff
fB*kIJ/HPf/m9Kf GNTmTf//if*f8fFmf$ NJmffnT$f*/ff0*f
Bf>IA/8fGf08TfT7
 Q
 
Y6	JV	ff
k
l^ Sed A  ]J/	ff

~:4 `
m*>*fF*f/mffT]IA/mff@e_K?9(PmnQ/f5Qf9@eH
ff?@eH+


[g
	ff
@Y]efff7fm8mmGxbh$Hf5fmmYfj`Hf
] m mffJ?IB 9G/8 &vwz-v{*j!Vg8&Gs%v	'yv{z-v{*H
 f
 ] m m/fBnfT//&TPf
JGKeC yxG   *B$f8 fGfnT]IAQT/8fGff
 ] m mW0z"
g 8&g 8&vwz-v{*9!Vg8&?sl\v	'yvO-v*h
 @A_
 99fn ] m m 5ffT/v/&4fkJG
*f/m ffTBT>IAQT/8fGfjHBf
 ] m m1HH   N10B$K0;k/T/jfT

9f 

z

fi{7#ff-q:i

l7a	U7`(q%|	wO7I8\OO
{`1o6i
 {#8(
 {ff%RV 
 {{ggl
7q(\+fA|a A jg/7c:%jfA7cfe|Ca A jga
AH%8wcf\|: 8w%%te`iwO|3R
 
1( 1I`\ gj%7(j8/\UCe|q8g(%i`\(q( O}	D8U
i g9\f(%iwD8O17c8w}7I%8\98/\zG w+KOI`8\je|
8g(%ji`t\8`6\iDD 187\ ze7OI`8\|Aq``cOjg
gj%8D88\w %9i7i8|6Giw %l
 :jCAK\8U|(\O
 A z z9\8 (
 e7(  U


&

?

A


O


O

'

C









G



i

w







#

	



1

(

g

8



G

D

8



8

\



w




O







j

`



i

8

q6wGiwO #(
ff "!#!%$
*


(
C
+

A O O)|

c
	fffi(

   qI8\e%89|8 %, `(7\U7t\8qa:Dwwgtw\8\q  77 - 7U\8
ODwgw(w  7q	wO7ei:DwD+:O8`\Og cgf% .0/213 V7DD
.2.04g 
5768fifi)98
	:fi<;
ST






 T
 T

1



X,Y+

VUWQG

[iC}]\'^



con

 

=>
f


 ,

=>



 OO \

,%?
BffGH



-Z+



@ _B7`aG)OXE8>% T
h



-





n

?

 O



,%?

-



GffGLKMBffD2ff
>N2ObBffaQ
BffD2E8Ec



BE8>p T%q ff
B kQObB @ 
EcrB:GIHJs

OEJBr`G)O7E8 T



BJGffGLKMB:DFff
>NffOPB:IQ
BffD2E80R

R



B:yN0GQkBffzO
{8BuiOP:G|EjD'GH0kY$}K~OzOPBuiOP:G T






-A@ #BC0
B:DFE8#B:GIH

 

RyklR
iR

\

f




 OO 



edgf OO O dih

Bj0klmE8>

VU)tWuEE0O @ w vOG0`G)O
bxE8>

R$}UWQGVUWE8>s

(q6

f



n



?

Q
BffDFE8>E0R

5768fiIfi)>l8
 :|7ecVwc:`(`Ka(wZ
 o&%t\8\qi\O/O\
 i7(%HVU:x/O(`%7 w7c87\ tw7 s \O:O(%g8wg 7(Co ? n 

79\(D%`OOi\O/O\K`8VU:UV 8i%ww987\Ow s ( +
Y
 ` s |c\8e1D88\w
 Ot`i8twI`O}1A7(lgZ\:Dw /
G7`(`iw9l``I`o
 YC&
 7c7qcVf`(`t87\q s r
 
(7 x&
  ? n iiw\U7tfVfi8iI8\ew s ( 
 :9 7\u #7(gw s (
ti`t\8qa87\ z
 
@
=> 
edMf O O  dih

[iC}]\)\
BffGH





$}K~OwObBeiObffGY T



 O 8\ he BE8>C T_q BffkQObB @ 
>Ec
RkQR
coE0D)>UVUWBu 
O
E%B%N'QG0k>BffwO{8BeiObffGjDLGHQk


6A n  E0D)>UYVUWBu  OEB
K


T
R$}UWQGYVUWQk>2OEVEBffGY"!#!


B0BffDFE8>c



B%0klE8> T
RkQRiR



BE8>C T



N0GQkBffzO
{8BuiOP:GD'GIHF0k_$}K~OwObBeiObffG T

@ v#



Q
BffD2E8Ecn



 

f

  



RyklR
iR 

   * f  OO \ *' g98  f  OO ~' \fDi(6`ii7w`O
e
	18g(%e| 1A   
 G(g8 .% /1Og + G +M*F 7(l
7(e8/je|e3 1A   7ID%`86i`\U`%%i|8f O OO   7\c7O	
 F 
8g(% .# `A   *
5768fiIfi)>l8

1A n

R

ff0

fi)Le02L
II_e))Q))WFPe
2L
I

}Q*eW8:g:0uu0~7:i7&:WgWlZry%g%e*'l=0uue~*'QF)u_*2Q)Qg
:
u:g:|W|Q







Vj0:
:Y:X e eI*=r)0e*'l=0uu0~*'uZ o&:XQl

C %uy<%F=]

Wl}&:Ql



:))%e*:=uu08%8u 

J &

e*'>=0uue~*'Q

~:I2gg:

%u

*F

W0%820uul~8X:~*2:
:
QV|~g)g

	 ff

fi
 "!$#&%(' *)+-,/.0#&1324+252617,89:87#; <#&%('=!*8>#&?&@%BAC?&#&D/EF#G)/.0+25EH%,+A&I IJAK%
#&%('L.M+ N)+O#OP/D/IQ24+RATS U)6V XWYHD/YMZY P! Y\[HS ]_^ #&%('`! _^ a:876%
cbd!\ ^ eY
f5g4hBh7ijlk m ^
R!\ ^
/n
o!
qp
o!
;!
P!
rbd!
7bd!@s
yx
zbR!
ut
v cw
ut
{bd!@s l|
x
v uw Bx
t
x
C!@s
Cx
mv }w
~x
:!
lx
 ff

bd!
/n
cbd!\ ^ Xfi
  h7g4 }~H4"GJh7iqGh{H4Cq  3y z)+@#	%7EMP~24qATS%BAK%ZP#&1PAK.0A+&EF,+#&.
,/.0#&1242+q#&%(' #P/D/I24ATS ~Y 876%9:8>/D-&E02+:2#9,+A&I{.0+Pl	=cAZS mWYHD/YMZY @Y
f5g4hBh7ij

9

n

q=
 b l + >b
n
y= n
6
 l +
-n

@
+
C

_

q

+

	

+



q

n
+ l 
@n

lb lb
6

/n


C

m  ^  m
`

; 
 6

(

fi
  h7g4 \~H4"G`h>i5q   )+ #%7EMP524ATS ,/.0#&1324+2+~#&%(' #CP/D/I24
ATS ~Y 876%9:8>/D-&E02+:2#&%	=cAZS UWYHD/YMZY @Y
f5g4hBh7ij !
G
e!
c !
 ^ ! 9k


k
5

/



 6



fi

gg:)uarCW0e*8l=u0ue~*:u
rygp 

o

Vg]rFWuryg 

:)~ )QF
2Q)Q~:0:
:|W|Q#

o

*

7

e*'>=0uue~*'Q:)

V_|0:
::aYX e eL*

o

eC







7e% 









P

:)

ry%g"e:0uu0~
:W

r





|X:)_r Ma
=l



Wg*I_L|Q:

'l)y:ry%g":
ry

2

pQ)u*

Jy:lQg

Xa*L|Q):a

*Z:)r gp %&a}Q

 Cffu0uu J7ar)Q_

_IXCL|Q y:ry

 jC'l=:#r



gWl

gWlF| |
:ry%gp 

gl

V

ger

Wl}:#lQg

X *

J

_
W:#C'l=:#r&y:7>:



:)

r

r

r)Q%:
uXgW:lgWl



 glW:)_g)

g *&:)gWl| ||g:Z

X
"*'l)y:%

X

r:)~ FWl'

j




}Q





eM2u0uQi*

eW20u0u~*IX~Qr:

|

2:
:
uu):)

e:

gWpI2~g|* |>eo~Q::L2Q)Q~:
u:g:)|W|QarV_|
u:g:X:% e eu*


'gW7~Q

)

ry2 JP

:Ql

V

V2 X #

ff * uuF
"

ff Xl0u0w7:
>
 a}Qj



2Q)Q~:
u:g:)|Q%
gW~

V

gW:u7:Ql

|>*~Qr:}Q
:W~uuaF)u



2 agWl> L
~:



:)

 



7

:

V

  XXg)~A):

])Q)QlryF X

)0WF:g:|0Au:_:}]

g)ury2 XP

2 Xl0uuz


||~F=l



Wl~Q):7)X2:
:
u:~

:gWl~:)A

V

"*=l_

||~F)#~Q

|~F

V_|0:
::r% u uW*





e0u0w

m

)QAg)~le:~





V






:0u0

' X_:)

:0u0w


|~F

u0w
~Au:)> )Qg

Q
:W~2

:Ql
Ql

 gW| ||g:



 



~:I2
:

:p:

Lr



7r



_gW0r0

2Q)Q~:
u:g:)|QrV_|
u:g:_:=% u uF*&
|W|Q#

V_|0:
::% e eL*

:)> )Qg

ia:)FWr
:Iu2:QQg

Wl}=l_




r0:_|l>p





]}Q

I>:|~:2:A:)A

gWJy:QlQ
:W~_

2lWl>:u:g:|W|QX
% u u}#}Q




*2Q)Q~:0:
:

*

)

] :
Q

g|Wgggg::

)Q



_&a:)

PQlQ:)~
J~:~:2:agWllQgYl:)>

&IC)_~QX:Q
:W~uX:|>:W0~:

V_|
u:g::r



Vju:g:]:X e e#}:)ZlQg>:|~:2:Z
p:Y

lu:g):#Ql2Q)Q~:
u:g:Z|)l%
|W|QX

% u uW*

:W~ )QggWl>X Lg~:

Wl}a:

 r7

#:% u u|#

"



C



#:

FY~Qu'):~:~:2:
uu

P

Vju:g::% u uW:~C2Q)Q~:0:
:

ar)u:~Q

#:r

&;

 gWl>C L
g>p:]

#:



% u uW*

fiF>+4{ZKB+
  {3BKz{3r3u/KKr3
>K@3>KffKK7y7MeM0ffKKR>KyKy7Ky3/7/KM0ffKK7y>/yK>e
K y9Kff 7K3/7/KM0ffKK7@7=MJ6KP0K (0Jff>6 MJffKK
@K>eP0K lJffKCB3	 GffCff 
 Mfi GPM7 / 0:ffKMP0K"  0	ffP0K q ~M
ff/MBJ9ff G7 oKMff G/P>`6KK73ffK_K:MP>Kc MK0;   ~M
KMG7ffP77KP0K_ 	 7K>K 	/0K>6 ~edKP0ffff~Kyffd ff>P0K
 MJffKKRCK7JK
!#"%$'&()(+*,fi(%-.(ff/102(3(+*  46587 - 587:9<; /$!=*?>@*ACB 9D;FE $!,ff>/$!=*
 6/KH
 GI GOP7J qff P7KL K M NOoKJ P 0K K QRNS=7K>6Kfi G/
Ty>6ff >KR3/7/KM0ffKK7979eM6KP0KP7KeKR>KJ3/7/KM6KP0K>d>/RK
7JKU
 F oKJ  0K ffMJfi 
WV=KKY
 X[ZJ\^]fi_^`C7Ke> aB>6B/b ffK
7JKK>dMJ6KP0K`K 
RffK7ffP7yK  3KG/0K>ffc >3~	 7ffBJ @
 jklJm!nejeo'prqts(u K7K 
d P7/r  B3M G5MM/Kw vyxzK K7
"(fid.*:$'/e$!=f*hg /0K>i K0
 7ff3K GMK: {|xzK 77K} vUK>~ { 7c G>K9~ffffK [ 
 BK
 > 9<;wE ( >5/0K>
K XXffXt``.Xb	`	+Xt``

K 3KGl>6X  X+`[`K>~fX+`>JG5P7@Kd~ff6K[
 BK:TyJq	G/cfKU0	7K
ff/PuGP>6=7/P7/.X+`7KXb	` 7 Ki~ M` KKyX  Xt``+

 8) = ; = 5 $'/$!=f*y3 psFKl1pj~ro'je^8pjeqNRjzqeq	jeklrm!n^jeopJqsro'je^8pJPpJqK|MNm 2jeq
eq@om KQNP
KB3MK|
 ZO73RBffzc G6mfi 
V=KK|
 X[ZJ\^]fi_5>33W
 Z^ZJfi`+
 % HKMJ%K 
KB3MKu>KeK:ViV=K7K:VV=yyK7ffcM7/>M7OK ff3 K797K
K  3KG/K7ff >Kc u 3K/;
73K/K
 X[ZJ\^\fiG^`~73Mff GffP03K67@/0KP0K>MB/b ff6KMP0K K7MJffKK
B/ qffU/K7ffffy
 Tyff/MB6dP7>eP0KUP7ffK/ X"ffff iZJ\^fi_^`JMPd9 M
ffKMP0K K>Ku GffyffKKMK[ 
KBKy>/0KP0K>MBb q6/K>eP0K K7MJM
ffKKJB/b ff9/0K>6; t oK7K 
fi70. HKP KKR B>>eP0K97ffK/h ff~
K/~K73	B6/d>ffJfi 
Z6KJ g M G X[ZJ\^\^3ff ZJ\^\^3fi `1>3Bff7c ~dB ~K>
0/G+ 
 	7/7K7O2
  Ku X[ZJ\^\fi^`5>J GdK G/6KG/r Ke ~0L7c >K >
7JK7ffK/
 oK~MffKMK6q7Kyr G97KR97@BffK"ff3 ~7/=>
ffK7/ffR/ /0K>6q6KGK>K 
dK>/K7F /J WyK GlP7yffKff/ HKP KK 
>7JK7ffK/h +~0R>36K7 (PRffKMP0K
 XK>K wZJ\^fi^`
 q@m proK MC/ MK0  K>Ku 
 e0=
"(fid.*:$'/e$!=f*Lg P7PMP0Kce0yh
M7KK g  7/C% HK r<
 k1sfiprqprjeowq@m pJ2X9Kf `oK uu lK7Ku 
u HKCff3
u >}  .O>	 ~C7PPP0K 7P7K u 
 KRdK w7/P K0~i jfi1s FKC
"(fid.*:$'/e$!=f* "/ K B=CK7ff @K K>%
e

fifi@fifJ^'.@@rfi@@ffPb.fi'fi^'
fi.:'e!fL	@6H'}S18eeJ: %r'6J 
	fiff
	t~6
| [ @
 ~' ^  } 
P !^ 6 }
P
@ fi @ ^ ' 'c J 
 ' 6 6 ~
  
 %
f
 ' @r@6
ir'6J 
 6J ert J ff8 F
fi.:'e!fT
 S. 4U 0 ~6.  r'6c3 I/J) 7VhXWL18YRffHZe%1U	+63	G[]\
?^U_'`F
	t. aL 	P^3 %
('b[]cd?^Ue'gfhU4+ +	
0 'g[\i?^U_'bfj[\(k
ld?^U_'7D#;n
 m&C<$ffpo][\(k
ld?^U_' 
	~'*
 6c6 2fi. (}   +	#ff,= qsrTt!I
 w8 x y{z}|`~78x! v 
 w! x1{ U1P N8 POrQ NR^811 Ne }N  > NRff'` ZO`Ne
u5v 

OrQ NRff8. F7Wr
 UpmfZ Ne ~e@Y :Z }X WJ dZ1
 N,OJQ NRff8 ffoF[]\+?Ue'L R
OWX W
Nb ff}e
8 1 qsTt

J)
 	3   ()Jc. g3  J)J  6L@3 2fi 0 J. 6c6. (63 	  0 . fi + b +9
 
	s	+  K?>Ld(d'`  
	P  0 # P 
	#~
 9d9. 6 ?Ld(d'I_J)@. 6'6. 3  c^ e

	3 . 6. c ['. 7!6 !. >t
 J  6 C    [ d t  0 5fi'd 9 b 
9 
	5SJK ?Ldd('
 
	z @ 0 3   ?>L(t('`Ib 6 ( J L 	% _ fi 6 0 .  6c6 '  (2fi 2fiL 	 

c 6-+ 6 '  r@6   
	s  6
 i ({ 	@c
 >@5
 @ '  gq>#6c6u! [ ?L fi. 3Ii
r'6 >@ q>6c6u! [ %L fi. b^ 0 c  0 6 @3 	]Q 6@ Q 9 @6.  r@6c 0 qCA#
6c6
 ' 3 I
fi.:'e!fF
 S KU 0 i6.  wr'6c3 I)J)@. 
@zX W18eY R^H Ze~Q Nffr U4	t. @ L 	fi \ ?^Ue'
P 	t. aL 	P^3 %
 'P l ?^Ue'gfhU4 
	
(
0 'g \ ?Ue'gf;
 meU'. 6c6 2fi. ( o#*?Ue'. 
	ffoV(?^U_'P _ j*fqCA#d{
 
	5/=i qsrdI



 w XwLy|>Lx (!fs
 .ff` 23+F
 
J 5787 ,w8{ ci X e!f 7 1 NOrQ NRff8 Ne 
ffNe
 > NR^e' dZO`Ne7 OrQ NR^8L 7Wr5
 ffZ CN e@Y *Z X W@J Z'1C N-OJ NRff8g o# \ ?H;&='
L R
OWX W+N
 ffne8 1 qs

 u h&uL _ ^6 6J)J  6L! 
	s 0 6. 2^ '  3i. 2fi. fiqd  
r'6
 o \ ?^Ue' D'6:
 ofi[ \ ?^U_'`I-J)+'}J u fi6r'}@ @  6t'c [  0  J. 
r'6J3 ff+ 2ff'. (]6D?c 6 0  @ [   }6r QX6c6 '  +	h"b[ 0 [! 6-![ I~
 9d9. 6 
?Ld(d'g@^  ({
 	@cL 	,>@e @ ' ~ L . 6 +	-6 6 r'6c  c	Q c [ @ +	]9^. 
. 6Q c
 [ @ r'6c4 P@. 6
 %	@J  J6 2fi3 	B >B 6z6r 2fic3 I,r'3 }
 !6c6 (
	t. a+ '
  b>@c6 +	6r 63 	?c @c !6*
 6z{ 	 aL 	sCI 3I 3IEc J. 5	t. a+ '  1[ 0 
[! 6-![ ,J  6FI
fi.:'e!f%W	@6e ff#. q:X WM J ir@6e   
	 t  ff . 2^ ' ( }	@
s\i?;&5='?@q&'`I663 ] y sqX Wz1 ff/IF	@6C ff  BZ Z!` O1 qX W


ff! 
	#"
$# &%
(')
*    +	,ff-
* .  ff/
0 '1 
	#ff# 32 4 52  0
 66 7
8')
 .
# 
	#
 
#ff-
	
'1"
,6:9 );&*< 5=( 
	
8'
)>
-??@:AB;&C='EDF?@ff:AG;&-=''H"!I
J)
K 
	#ff 
L	:M+N
PO QNR
 *I

&

fi+`8iHe^`


).5.Q+$
]E>
.$*.{^)5.Q+$
$
 +^)	
 >

 fiff1.CQ$5L]>
QP#8 
! .g#"e4.Q+$e
$}-
L.% 
 ff).P&)
.$}Q`+.'()@*+,
).
V
#].
-  ,
 Q$$3]>
^P`+./ ,
 807  
1%23547689;: +Q!e>
QPff 5< .^
$3=
?>A@CBD@FED@HG+IJI%KLBD@G+IINM
O>A@)BD@PEQR@G+ISITKUBD@G+IINM
A>V@CBD@FEXWY@HGISIZK[BD@HGIIM
\ >A@CBD@FEQR@H]^ISI_K`BD@H]^IMXBD@PabIINM!+
c >A@CBD@G+ITKUBD@PabIDId
& +.Q+$ ^)*L3
]{_f!e +,4!,$egh)& +.Q+$ ^)^$*,
L.
)
$3L+-{e \ 
e 
3 ^b5L3
$ep
 
$[i)j k+" k:
l L \ m :d.PQ1 
  
 
\
\

{$3.*$3L+/{
e7
35n- poibq(.r7sOtue
.Q+$ c Qs+{$3v>
$$$.wek
" +
4V
3$4
L.e$$e^P 
  
 
L.e$
& $Ls+5,.Q+$5xl 
 ^L
>
.*
^Xj $dQu< ^L.^
$4,$3F>
*>
^


^4
L.*{51+]+^
$R
e ^fi(y zdq ^( { fi1ff N(q .e $#$3F>
5,.^
$QC

{$3.|
 >,$e$):
l  
 3z
} 3s
 QZ:
l $e@S:
l z(q u
mFl 
 ^3^~" F1ff L.j^
L537" ]!L$.z" 3ds3)l;" 
^je$.Xj $3$^

i)j^"
^l 
 ^~^S <d>>kl sL)l 
 !$ !, id<zq(L*+5#$)"!Q]{1;}!
j
X<d
< .
.$3 ^
5!
.;i)j ^
" ^l 
 ^{ +LREe $" )" ^$!} 
~d< .
.$3 ^
*!
!
zl 
 ^L>Q$)
ff CQ*
3L#*L$!y +*$5Y(q .$3$>Q

(x.+ 9) g 9fg9fi+8) ( fiN  C0(D 
 
.;1ff SBGYq(.X<fi$3$^T
d 
 zl]>3
>Q!LB>
 
 $)"^bl e3
>+>5<

 
 $.^
$;< z(
q .F>
5$3$z(q (*+F>
+ 
 $.(5.Q+$$@H#X<u<^.$7.g(
Re
tfiuu^Ze k(q .' Xd< .etfiuu^_j
 e|tfiuu^T#
 5<u<Q.$e'tfiuu^T{e^l])lgl5!etfiu^tu
m3$)ljFe xl$y !Q>e+tfiuY5e ^(q $etfiuYuIN) .Qff)ff |Pff !L$.z" +fi
ff JXj (>${i.$^3
"4+$L#-3
>+CPff1 
 $.(4.Q+$3K^$)l!
*Lxq(.( ! .}+f"*^
Le

,>
QP
ff 5< .^
$
 *L$z(q .
 f=
#>#@J@@ /$70)I@g$70)IJIi^M
ffP+]
 ^*#d.$ege,
 ^*,@d- e
 /Dehh
 +7,
i Q*7l$< fi
0vM gY
 4$3N 
 $.(4.^
$3
 +4pl z:
l d< +F1& 
. )ff *>
^#Q.'De

$i
" Nl 
 ^
" >>^
0ePff ^$,3L 
 +e$5:d$^l 
 ^,+)+*+$(^^fi
qd>Q)!" Q3)
d" L.,!
+_L$>QsV
fe 
->(
%1ff K+fi(q /> D
 _>
^
^$5Q.P?~0>70YeXP
ff ^$sL>$3 
 
!/d$kl 
 >Q#>
3d$#$.$f ^

.$3 
" #^$)
l 
 fh)& +	)ff 
(q 
A>0*
 
 O>  YM
ff +~
P
 L!Q" fi($.$4
e 
1ff $3>
fi +# $)" +LQ)l(" Sj
>${+>QG+v
0

fiYXY+0fiu((_0XXbY'XX5RF_Y0Yu(
%57;Z)5S^N'JX/)z%x5gb)X
?ACDFRH^S_`DPDH^S5DH^
 )X
 A)DPRkJ DPDH^S%`DH^
A

)

D

P







k

J



`

D

H

^







Z5b)5fi  )X
 `)	
 X)*NYN)X%)
 fiff
Z5 
 ( Yk^ YXr
 ^X
 X)bYN)X*fifi)
 (X*fi  *fiX	kz)S) *
S XS ^S) ffZk/)zYbSp)v  k u)NYpb)X" !SXu#z JX$^k;(Y%!
)'kS5N*p)	
 k (b|x5zN|*)*" ff'&+)'( $k k'SXN)X*
)?ACDFD+ *+J%LD, *+
% k* (N|zXzb*fi)X*)|). -!+)Xz/ FS XS / ()S  )X
 0ff
1_
 (uSzS)3
 24SX'JX)SX|b)Fpb)5fi k*)x5fi
 65)^Zk^ YX|)x6 YbSJx5
)X)zkJ)~|u)fi)z 5 " Yk u)bY*SX)S)zX)N)X% ff7F8
 1_ (uSzS)9
 2:z//SX%
S5)|); zYSN)~)h)Xzzb)%;ubXb)*fi^k< R5p)'z6 YNJSzX)5*fi)zkJ)= ff
>#?A@XB
 @DCAEFEG@IHKJMLONQPSRTVUP-WYX(ZW\[^]AP-W\_a`cbOWZedRfP(gWZehfii=jkP_Ylmcnpoqlmrlmbsn trl b/n^nMh
>#?A@u@DvSwxRzX
 zy{|r3lmbsng)X}
 zy~9rl bsn6! Z;5 Y
 z~9rlmbsng)58
 z~9r3l b/n6ff
Z5N
 65Y1h* (uSzS)
 !rlmb/n/)X
 qrYl b/n6ffBZYX	
 lmn  lmrlmb/n rl b/n^n6ff
Z5b)5
 U; ) YNY)p qrlmbsn)X
 qr}l b/n6ffZ5N
 657)^XXg)'*fi*)x^S) !
 YXbRJ 5M!Tlmn#oqlmr0lmb/n rl bsn^n6ff
lmrlmbsn r0l bn^n  lmn6ffZZ)X"
>#?A@XB
 @DCAEFEG@IHKJMONQPSR  W\_a`UPcXZW\[%]APS]	W\_a`WfigSPS]A\Ze\P_RB  W\_a`0hi=jkP(_R,jkP(gSPPS^d]R,]
WZedRfPgSW\Z6bq][DXSjYR,jkW"R  r0lmb/n0W\_a`	trl bsnMh
>#?A@u@DvSw	5JXp^ ^kzS)~)_;) YNY/Z/X
 Y  JH#  lmn  r    lmn J  %5N*
  $~ub))  !s#$~ub))#
  ! A)X
 
 0! 	
 p  - %5N*8
  r    lmn S !)X
)
 $) H)3
 lm n6ffb
 b  p!)5S5N{
 rlmb/n  H	
 0 lmn  r  SzX
9rl bsn [S#
 p  -ff bX #  y #  r    lmn S  )X
  lmn  r  !^)5 fi)5 YXbYS 5  trlmbsn)X
fiy JH#
 trl bsn6ff
(
 hgC"@+,F\EG@IH=C u9?E,HkFA?m@u
Q\F\EG@IH
Z5*fi^k<R5)x6YbSJx5)X'*fi)zkJ))(|ubXb)z;p*fi^k<R5)x6YbSSzX
"R5N5fi))zkJ)5uZH)xfi" ff'ZXb
u HI M?"FE,H={ FE

lmrlmb"n

rl bmn^n

%5N*/ _b)X%)5bh|zx*N*)!R*N_)+Fb)5fi k JX%b)X/ )z'
65)5)zkJ)=ffsxYx)S5M!YJXFN*
lmrlmbmnrlmb n rlmb"nrl b n rl b"n^n )X


lmrlmbmn rl b"nrlmb n rl bmnrl b n^n


%5N*  b)5%)5
 b)X
 b  )%zzb)" !)%*N*h)+Jk*fi|b)5fi k )z'
65F *fi)zkJ)5"ff7P	SXv'5M!H)|)Yb zx*N*)bm!b
)Xb'^!5YpJ$$(N*NY
b%)D)^|N)X `%k
 H)xfi'/ 65S^)zkJ)5"  !)X	;) ff.'z(SX/b%)*
); zYS*+fi S5b)5c
 fiff
\m

fik%AIBBu<

6BT\GI|Qu3<DSD98"%kkpSSmD(8(	<D"Y<
 O
 0TD Y.;k
 
\SfGSm\D S
/

 	fiffm
 ck
 	 D

 	
 ffff ff!"#$ ff !%&pDY 	 !('#*)+)*)+#,!.-/#,!012/D(	
 6<#(=(<D"
4 3,6;SMI"
 VK
 56!('#+)*)+)*#,!0-7QD8
 :9
 G

;=<?>/@ACBGDk;fD	G*2//E(<D"*
	GFH1IKJMLN4%OPFHMLN?Q#

 ' 	fi7FH1I/JRLS$Q#KFHTIVURLS$WO:FHMLN?X#

	fi7FH1I/JRLS$.OPFHTIVURLS$Q#KFHMLN?X#
U
YZ'=	FHTI/J[RLS$Q#KFHTIVU7RLS$Q#KFHTIHMLN(OPFHMLN?X#kk
Y
	FHTI/J[RLS$Q#KFHTIVU7RLS$.OPFHTIHRLS$Q#VFHRLSHQ)
U
kSfi(k"\ff]'#$
 <c4,3 6;SMD"S ^ FH1I U MLN411


 DfD(-/(<D"
U
ffYZ'#$Y .<43,%fMI""fi<_]Q
' \ FH1I?RLS$T (
D DS6D(6;`\
 fD(=<DSWff #,YZ'#$Y 
U
U
U
</43,6;Ma"0<VK5 FHTI U MLN4Q#KFHTIHRLS$T 
afDXbdcu*e6&D(kf]'/<c"`e66pHYZ'sDY DY~<'-SSe6(6g]'
U
D\
DD"%k6f[9B(;`e^/<:ff
#,YZ'G#,Y  Sf='D	(B(<D"
U
U
U
4 3,6;SMI"8< <DSc/<&E<06`e^<6p;D (<Dk<fD+2s0fD-+<+2/KE
;kSX 
 ~ f
hi KjAS@lkKmon,;qpHrTtsK>/BGHu%vAgjmxwxj+s/>M\GIKyBKj*z,IjAV{Wr?u^GI.|Z}  
 qASBpt\&AQ
 fiffm[
\SfGSm\D%(S7+tA ~+ AQ(D(u-ef7 6!%'#*)+)*)#$!.-]? (
  


jAuK
 yD
 cSM/<#
 Y;D
 Y;<
DD(;<
 .]a;k<8u D"8fD=3;k
;S(6
;k#;kSXfDc
kb".
 /$3,6fMI"S { p(D"cffm6
 *Af5f/, .
 uDS S k"  Q `vfKq .?ZfiffmS2/D(fi#/$3,6fMI"S { !('#*)+)*)+#,!(Q
 =(.0
9 ?\
 
D(YQ ' 	 gffffff!%Q ' #,ff !(Q ' &
 uDSf8fKZx"X =
<
4,3 %fMI""S6!%'#*)+)*)*#$!(#,!%Q '  .v.
 Su
c ^;;<+N7
2 k+6
e ffff!(Q ' #,
ff !(Q ' &ffD
 D"D"6D(%f3
 ?Q ' 
 ?%
 %
 fD#DD(;<xu
c ;kf"mff D
 D
;6k#?Q ' fim
ff 6
 P
a3(;<
  \2fi;D*
 2"0fD	p<q
 cu^;<c
 e6(;-k-";<8
 $ 3,6;SMI(;<
DSSfi(*22;k+2fDY<vcu^S;Se6(;8"6D(D"0	"fk
4 3,6;SMIf
"%kkT(<"
BS@

@5>kfi} S   \a Y~ t\AQ ffm"'#*)+)*)#f-K AfiYt\&AQQu ff]'#+)*)*)+#,v-/ 
AS#+`\
 A 
 SGm\"(S7  %D    m Y \am\ \ \\=  =.v$
? (  (S^
 ` R\A 	
 t\
 A  ffYZ'&#+)*)*)X#,Y"-K \Gm\Dq(S7 Y =D    mm\^( 
0  g Y"$

jAuK
 yQ%
 vauf;S4 (<D/
 ff]'7#*)+)*)#$v-6 D(d
 2D*
 e6fv	 %+S
(pB(<(afk\ff]'&#*)+)*)#$v-<43,6;SMI"8<0%D" 0v$k;kSb;S
;k;;;;<5
 .;D;D	
 *C_(.
 6k
 :Y\D;kS-X
 b<;/;D;fff
8;k
;k
 Y	%kZ
 2ck+
 e6
 M%R _MY%Xk"DS6D(6;` 8
 *MY% 
=(q
 Y"	Yd(3D
 2k+
 e6-=
 gY";
 
7

fi"Kx+&xt0KKqKK/10t&xt

 G Q	ff
fiG
 
fi ff! #"%$'&)(  *( + -, *./ 012
fi #34+656 	+879
 7:+<;  9
-(*(=
8 + 
(>@?A3 *( + , *B
fi CD? EF
( *(= GC4HI>J$
K G9L-MNPO=QSRUT-VWV<XY[Z]\_^a`V<bBRUc[Q%d-Q0bfe<dgO9Qbfe<dgY`he<c!Yji=kWl9`dmY[V<inV<iAoqpGrsd:ZmO=V<lUc[k\!Q#i=V<d6QhkdmO9e<d 
e<i9kt3vuUYjidmO=QwZmd6e<d-Q%bQid#V<XxdmO=QydmO=QV<T-Q%bzu{YidgO9QSRFT6V{V<X|e<T6QSYi9kFQ%}FQhk\^aop
~ hG .Q $	g9%|^AdmO9QBkUQUiFYjdgYV<iIV<XPo!dmOnT6QhZ6V<clUdmY[V<ic[e^_QTw]<#"S yu)e<i9kzdgO_l9Z  ? yp
 QPO9eh_Q:dmO9e<d3    < Y[ZV<Tm*Yi_dmT-VWk{l`QhkfX[T6V<b   \^dmO9Q0QbBRUds^.Z6Qhl=Q%i=`Q0V<XcYd6QT6e<c[ZP    sp
 Q%i=`QuUXV<T   ?A3  dgO9QT6QwQ}WY[Zmd-Z:e2`%c[e<l9Z6Q   ? Zml=`6OAdmO9e<d   H   p
 !7<9
 	s<(U-= *K( g9#6V<TQ_QTm^DtudmO=Q%T-QQ}WY[Zmd-ZeZ6QdV<Xw`c[e<l9Z6QhZA3!wV<Tm
YidgT6V{kWl=`QhkXT-V<b  \_^Z6V<bQfZ-Q_l9Qi9`hQtV<X#cYd6QT6e<c[Z|w  <hh   0Zml9`6OdmO=e<dfXV<TfQhe`6O
>@?A3!dgO9QT6Q.Q}WY[Zmd-Z:e.`%c[e<l9Z-QyC@?Zml9`6OAdgO9e<dCDH>2p
 !7<9
 	s<Q $	Dg9#{t ]^dmO=QzkFQ%FiUYdmY[V<i V<Xyo!dmOT6QhZ6V<cjlFdmY[V<ic[e^_QThu /  YZBeT6QhZ6V<c_Q%i_d
V<XZ6V<bQJ`ce<l=Z6QhZ  ? #"/e<i=k ff ? 0"2Zml9`6OdmO=e<dfu e<i9k
BIp.NPO9Qiz\_^nT6V<R!VZmYdmY[V<iFudmO=Q%T-QJQ}WY[Zmd-ZSeBcjYd6QT6e<c|Zml=`6OzdgO9e<d  H / : 8 
e<i9k ff H / ]    p
]^ndmO=QfYi9kWl=`%dgYV<iO_^WR!V<dgO9QhZmY[Zhu]dmO9QT6QaQ%}{YZgd6Z.etZ6QdwV<X`ce<l=Z6QZ23  V<Tm*Yi_dmT-VWk{l9`hQkXT6V<b  \_^
Z6V<bQ.Z6Qhl=Q%i=`QBV<XPcYjd-Q%T-e<cZ/   8hhh     |Zml=`6OdmO=e<dwXV<T/Qe`6O>?3  dgO9QT6QfQ}{YZmd-Zwe
`ce<l=Z6QAC? Zml9`6OdmO=e<dCH>2pNPO=Q%i\_^)QbJbeA{uPdmO9QT6QtQ}WY[Zmd-Z/eZ6QdJV<X`ce<l=Z6QhZJ3G
V<Tm*Yi_dmT-VWk{l`QhkXT-V<b / 8 \_^BZ6V<bQ:Z6Qhl=Q%i=`QV<XcjYd6QT6e<c[Zv|  # # Zml=`6OfdgO9e<d
XV<TQhe`6OA>y?A3x dgO9QT6QwQ}W Y[Zmd-Z:e2`%c[e<l9Z6QC@?Zml9`6OAdgO9e<dC@HI>y p  hh   
]^2dgO9QSYi9k{l9`dmY[V<itO^{R!V<dmO=QZgYZhudmO9QT6Qffe<cZ-V.Q%}{YZgd6ZPe.Z-Q%dV<X`%c[e<l9Z-QZ3  V<Tm*Yi_dmT6V{kWl=`QhktXT6V<b ff \_^
Z6V<bQ]Z-Q_l9Qi9`hQ:V<XcYjd-Q%T-e<cZv  D  <hh     =Zml9`6O2dgO9e<d|XV<TQhe`6OJ>@?A3  dmO=Q%T-Q0Q%}{Y[Zmd6Ze`%c[e<l9Z-Q
CD? Zml=`6OdmO=e<d0C@HI>2pNPO9Qif\_^Q%bBbeS{uUdmO=Q%T-QyQ}WY[Zmd-Z:e.Z6Qd:V<Xx`c[e<l9Z6QhZ03   V<Tg*Yji_dgT6V{kWl9`hQhk
XT-V<b /   \_^Z6V<bQ#Z-Q_l9Qi9`hQSV<XGcjYd6QT6e<c[Z0|  w hh ff )Zml=`6OdmO=e<d0XV<T0Qhe`6OA>y?A3G 
dmO=Q%T-QwQ}WY[Zmd6qZ e/`c[e< l9Z6QffCD? Zml9`6OAdgO9e<dC@HI>yp
NPO=Q%i2YdqXV<cjc[V#ZqX[T6V<bdgO9QkUQUiFYjdgYV<iaV<XV<Tg*Yji_dgT6V{kWl`dmY[VidmO=e<d03   3   3  Y[ZeyZ6QdV<X)`c[e<l9Z6QhZ
V<Tm*Yi_dmT-VWk{l`QhkXT-V<b /  \^   D     hhh      hh    	pqPV<i=Z6Q_l9Qi_dmc^_uUdmO=Q%T-QSQ}{YZmd-Z
efZ-Q%dV<Xv`%c[e<l9Z-QZ3   V<Tg*Yji_dmT-VWk{l9`hQkX[T6V<b /  Zml=`6OzdgO9e<dXV<TffQe`6On>?3   dgO9QT6Q2Q}WY[Zmd6Ze
`ce<l=Z6QyCD? Zml=`6OdmO=e<dSC4HI>2p
!:x ?G 9Lff   ?*[
rsidmO=QfZ6Qh`dmY[V<iFpYdyveZwkUQhZ6`TmY\!QkO=VeT-Qk{l9`dmY[V<iV<XQ%i=Q%T-e<cjY[e<dgYV<i`he<i\!QBe`6OUY[Q%_Qhk\_^
T6QRUc[e`Yji=ze`c[e<l9Z6QB\^eAZ6QdwV<X`%c[e<l9Z6QhZhp  Q%T-QvQBZmO9VhO9VhdmOUY[ZwZ6QdwV<X#`%c[e<l9Z6QhZwQhlFYje<cQi_dmc^
`he<i'\!QkUQhZ6`%TgYj\!Qhkt\^teBZmYi9<c[Qw`ce<l=Z6QuOUY[`6O'vQw`he<cjcGe<inQ%}{R9e<i=ZmYV<iV<XdmO=Q.V<TmY[<Yji=e<c]`c[e<l9Z6Qp#]^
kUQUiFYjdgYV<i)uvYXe`ce<l=Z6QC<sZglU\9ZglUbQZBQ%_QTm^I`%c[e<l9Z6QYjienZ6Q%d2V<X`ce<l=Z6QhZJ3vuqdgO9QiCYjcc:e<c[Z6V
<sZmlF\9ZmlFbfQe<iGySffV<X3vpGNvOUY[ZPcQhekUZ|l9ZPd-V.V<lUT0kFQ%FiUYdmY[V<itV<XxQ}WR=e<i9ZmY[V<itV<Xx`c[e<l9Z6QhZhpqNPO=Q0YkFQe
V<XQ}WR=e<i9ZmY[V<inV<X`ce<l=Z6QZvPeZ:FT6Zmd0RFT6QhZ6Q%i_d-QkA\_^Ar1kUQhZmd6e<bBs0cjblFYZgd0g`8"%p
 _0 gt7)Qd  \!Q:eff`%c[e<l9Z6Q0e<i=kfeffZ6Q_l9Qi9`hQ0V<XcjYd6QT6e<c[ZhpGNvO9Qifeff`%c[e<l9Z6Q0>Y[Ze<
i ,=<U
V<X  \_^AYXe<i9kAV<iFcj^YjXG>4Y[Z:e<iAxffSwV<Xqe/Z-Q%dV<X`%c[e<l9Z-QZ0V<Tg*Yji_dmT-VWk{l`hQktXT-V<b  \^'p

?xKGSlKo4*[*RHG[*R7^?RH/+QRGC?&70H? Q
G

/  / Q Q
GQ
G
x
Q

 /  Q
G

 

fi=	0<!U[
:)=

	ff
fi
	!"$#%&'
(*),+.-/+102+4365ff5879-/+4365:5<;
)>+.-/+10	?@+BA5ff5C7D-/+BA525<;
=
=FE

)>+.-/+10	?@+BA5ff5G7H-/+I0KJL+4AM55<;-/+BA525;

=

)>+.-/+10	?@+BA5ff5;-/+10KJL+4AM55<;-/+I0/+BA57D-/+BA525;
J

=

)>+.-/+10	?@+BA5ff5;-/+10KJL+4AM55N7D-/+I0/+BA5ff5;K-/+BA525; #	fi

?
O

)>+-/+I0/+4365ff5;K-/+I0	?P+BA5ff5C79-/+4AM5;-/+4365:5<Q

R	ST/"#%	&VUW=FE ; =

; =
J

Oe
$V#gf:hihkj/UW=FE ; =
J

; =


/ffY4
Z&[fi@%	\fiF$&]*C$#%&G=*^P_!` -/+I0KJa+BA5ff5;$-/+102+4AM55cbcd #fi

?.X

T\mP%"Pff_ d Oe
$V#gn[o#	ff
$!p=>^P_q` -/+I0KJZ+4AM55<;T-/+10/+BA5ff5Ib

?.XPl

rs&gff	#
]toM
$\#
u#av^pw&fi@%	\fix&yjY1ff%^ff%]!off
$z
wff{n#]toM$F#^p|P
#|P

U

6 9= A

[.  /:

M/Z

=

l

x

W[\[\ @L6

TL[\IL=yg2	\gO*=y

d

k.pTN_ufffiM"MM
ff
$sn[o#	ff


/W6Nv/TV=<g"LZ.T8LKOL

g@	\Dff	#gff"&"n[
ffTv#u&F$#%T\g>ffY


Z&[fi@%	\fi&]=^P_%&q#O
#qf:hihkjvS
N_{ff	vfiM"M

G#yf:hhj d O
#a&





RP%iUWO
l

k#|Pv=H
}



lq}

(*~
=#	fi ( 
= d ^%Min[o#	ff
uOV=9s#\|P (
O

l
Gn@o	#ff
$#v^pV&a#&fi\fig#a:#&#ff]{#ff
$g&&M
mP% d ff
S"n[o#
K#$#%T


$a
$#_mP%M
|a#"P&ff	v"#%	&
&T


l



X

)

^P_qG&opaff
ff
$yg=

~
O




~

=

N_{R	\T] d #|PUW=
l

l

R"^Z_STopaff

u d O

l
	&mZ%	Pff_ d O*=

Xk

~




l

R"q^P_qff	fiM"M

wV#fhhj d 	#\|Pv=




l

Bk#a&

d #fiT\mP%"Pff_yO


s#a&

e



O

d #fi

lD

$\*oMT\|Pv#s"|P_a	T#
$#ff
$%M	fiM"k
]toM
#ff
$G#t$#%&iff"&"n[
ffT
#wn[o#	ff
Ng$#%TF%&#iffta""&#
#ff
$x%M	fiM"v
]!o
$\#
u
$T\fi[%\fiTq#
a""&#
#ff
$y%MfijY1ff%^ff%]!off
$

U

l

* 



[. M&t@S2a6*g

(

<\Z.LK{=

x

4$ [KWj&.2:.

KL[ZLTL1<"LZ.!\<x4	

TL[\ILgO=\T{4W

(




#a&

2.6Gt/T

=y/\4\Tq<L<4L

Ot

k.pT!N_w&$#ff_W d "&tn[
&#$#%&g=!
y

_)G

2K2.42e

(~

5 %&#=! 
=vT]
X
N_{R&"]a d "&v"n@
$ffT#&"V"#%	&s:KY4
P&[fi@%fiT]=!Kff%	&qff	#

l





::#|P

(

R"
kB$\T]f2"]!]{#t[ff	#vff	T!"n@
$ffT#Ti
l
"#%	&\ffY4
Z&[fi@%	\fixT]=ff%	&#si#a&
R	y$sO^p
9 	#\|P (

l
#xfhhj8 d #	fiffP%	#x"n@o	#ff
$z= d #fiu#|P (
OD^P_tfiM"M

#

f:hihkj



V + U (

,



l

 B:x

pHv

PW

2.6

hk""&#
$\#
	%fig
]!o
$\#
#y$#%T!#x^pg&fi@%fiTya	T#
$\#
	F%fitjY
ff%^ff%]!off
$#x"n@o	#ff
$"$#%&

!#T!o	#ffff
$"%M$#ff_z
P&"&ffT\fiz
x"n@o	#ff
$
l{}
"$#%&ff%	&xff	#"|P_a""&#
#ff
$z%M	fiM"
]toM
$\#
u
&fi@%	\fix&q#a""&#
$\#

%M	fiM"jY1ff%M^	ff%M]toM
u:#o#ff
$%#i"n@o	#ff
$

LW

l

l

fi

 	PWff
Pa[p

8PP[6W$\a[pptGW "  	Z

fi	ff!"$#&%(')#+*,-/.0#+102435#+14(6870#+1,:9);+1<;+=	4>@?BA(13C9ff,D#EGFHIKJffLNMOLPLGQGI0R+SUTWVXFS
;+=YZ9[=@#+12\;+1]'[^_9[=:.]=;+`a(b(`:^\%(')#+*,-$cd.Ucfeg3ihDA(1(b`ackjl4>
m o%W#+'p'OA#+P#+1q(6870#+1,:9);+1r;+=B#%('ff#+*0,-P9),s#_%')#+*,-/.t#+12:A*,$uU1U9p-/>wv4*0xyx+'ff(-;+1z#+12|{Y#yxy
n
}:~Wyy ./70#yxy ~Wy A#y,	,:A0;Wh1OA#+%o;+7]'ff(-B67#+10,:9ff;+10,o./hA]9ff%-AOA(^%W#+'p'Uu]1U9p-,N'p=ff,-#+O*U`N#+:9);+1,o.
2U;10;+a69ff,O=;+`a#+'p'%')#+*,-o,o>
 U5-\K-(UtYtt/t0L(GL\LNQyVpTGM$S"F+SUMOR/MOF+JffFGVXEGRJ
E(JffR/TLNT@FsUVXEG&M0LWNLPLGQVpTGMSKF_EGF+HIKJffLNMOL$LNQIRSTWVXFSTW
?A041;+1U(69ff,:N10%Wz;+=%o;+7]'ff(-(6870#+1,:9);+1,_9ff,_28*-;rOA#+_=;+`4,-;+_\%('ff#+*0,-o,:A0`N|#+`N
9p1UuU1]9[N'p^<#+1^429ff,:O9[10%wxy10`N#+'[9)o#+:9);+1,P*U102U(`P9p7]'[9)%W#+O9ff;+1t>$BW%o#+*,N;+=OAU9),shBP:*U`O1|N;:A0
7U`N;+U');+=$`-o28*%(9p1xb(`:^xy(1(`-#+'p9ffo#+:9);+1*U102U(`?9[7U'p9)%W#+O9ff;+1-;r#zxy10`N#+'[9)W#+O9ff;+1*]12]`
 ,:*],:*]7]:9);+1;+=@#,:9[10x+'ff$67#+10,:9);+1>
fi	ff|a"P#d%')#+*,-/.35#+1<(6870#+1,:9);+1|;+=	#+124#&N`O,-(a;+=B>D?A01<3C9),
#GEGFHIKJffLNMOLLNQIRSTWVXFS;+=Bh$> `> >Ki9p=#+12q;+1U'p^49[=:."=;+`(b(`:^<%')#+*,N&c.ce3hA010b(`
cjz>
t0Y;+10,:9ff2]`nOA=;+'p'ff;oh9[10x%(')#+*,-o,o
cn
cn
!
3  
3  

}+}X}:Z}KG
}+}X  }:}8G
}}0/}UOZ}]t
}+}X  }:G}X08}UO	}]G} #+102
}+}X}OGK}  }8OG}X08}UO@}]G}G}G

?A0%('ff#+*0,-W,c  #+12c  #+`-_7U`-;+7"(`d9[10289p`-o%`N;8;+N,d;+=n4.	,:*%-ArOA#+c  jz#+12c  jz
>q?A0\%('ff#+*0,-\3  9),#+167#+10,:9);+1;+=/^ }  }]: .#+123  9ff,#+1(6870#+1,:9);+1;+=s^
 }X  }UOG}X0/}UOG}X}]: >?A0s(6870#+1,:9);+1<3  9ff,#d?%o;+7U')N(67#+1,O9ff;+1U*]a3  9ff,10;+o>
1:A(6]#+7]'ffP#+";obP:A0&?B%W;+7U')NP(6870#+1,:9);+1|3  ;+=9ff,a#+'),-;#_%W;+7U')(-P(6870#+1,:9);+1
+; =a>za;WhBb(`o.9p1%W;+1O`-#y,:_-;<%W;+7U')N(6870#+1,:9);+1,.B?%o;+7U')N\67#+10,:9);+1,69ff,O=;+`#+'['
1;+1]-#+*]-;+');yx9)%o#+'%(')#+*,-o,o>
 U5-]ts  N/Kz@/t0tt/LNMUGLRDSKFSMOR+/MOFJffFNVEGR+JUEWJpR/TL
RSKaRMOL((HTLNMDF\8t0L(S_M0LWNLPLGQVpTGMTwRGEGFHIKJffLNMOLLNQGI0RSTWVXFSd3F	(pMU$
 "Nq^r?A0W;+`N ~W .OA(`-69),:-,#q%W;+7U')Ns?;+=$\4h$> `> >>(1%o/.@=;+`
s
(b`O^%')#+*,-c.09p=	c5jz:A01qcZe>s^\:A02U(uU1]9[O9ff;+1z;+=#_%W;+7U')(-Ps?.hBA#ob
jz_.U#+12OA(1\^\B;+`-;+'p'ff#+`O^  .j>^&OAo;+`-(  .U:A0`N(689),:N,#+1\(67#+1,O9ff;+13;+=
,O*%-A<:A#+!e3d>?A*0,o.K=;+`(b`O^%')#+*,-wcd.]9p=	cfjr:A(1qce3d."#+12%W;+10,-o/*01:'p^
3i9),#d?%o;+7]'ff(-(6870#+1,:9);+1;+=YZh$> `> >]>
=hB%o#+1%W;+7U*]-?B%W;+7U')N(6870#+1,:9);+1,;+=	#d,-(n;+=	%')#+*,NW,OA(1hB%o#+1\*0,-P{	'ff;+O89p1,
+# ')xy;+`:9p:AUl=;+`%o;+7]*UO9[10x4#+1|  -;%W;+7U*]-#+1q?>K`N;+:A0w7]`-;;+=;+=B?Ao;+`-(l 
9pn=;+'p'ff;ohn,n:A#+:A0P%o#+1289)2U#+N,Na;+=Y'p9[N`N#+'ff,a-;d"*0,-W2\N;%o;+7]*UNP#?B%W;+7U')N(6870#+1,:9);+1
9),Du]1U9p-/>&9[10%W(6870#+1,:9);+1|9),aW*U9pby#+')10%W7]`-o,-`Ob89p1x\hB%o;+*U')2,O9[7U'p^\-o,:#+'p'	289p"`N1hB#W^],
-;(6870#+12#d%')#+*,-$/^,-W*(1%oo,n;+=Y'[9p-(`-#+'),n=ff`-;+:A]9ff,a%o#+1289)2U#+Nd,-.0#+1029p1\OAU9),DhB#W^;+U-#+9p1
#r?%o;+7U')N(67#+1,O9ff;+1t>?A]9ff,9ff,;+=P%o;+*U`N,-|#+16:`N_'p^%o;+7]'ff(67U`N;8%oW,N,o.U*]4#+')o#y,:
:A0W;+`NO9ff%o#+'p'[^8.K?%o;+7]'ff(-(6870#+1,:9);+1,#+124s?D,#+`NP%W;+7U*]-#+]'ff/>


fiX0/G	ff
fi)G

 "!#%$ff&(')
*,+.-0/21(+43656708:9;+28<5=->+@?ACBDFEG+IHJBKMLN+IO>+IAC/EP9GQR/569;BOSBKMTIEG/703C+R3RUWVOX3C+2TI569;BOZY\[^]_+8F+23CTIA69PDff+R8<56-0+
KGA`/HW+I]_BA6aKbBA_LN+cO0+cA`/Ed9;QR/569;BOeBKTcE;/7>3C+R3f8+I1(+cE;B?ff+28D(g@hiE;B56a\9dOkj=l2m(noF[pl2m(n\lcD^[ql2m(n\l2/(rc[F]M-F9GTC-9;3
D>/N3`+284BOWst367FD>367FH?F569;BOUuVH?FEd9;T2/5=9GBO9;3i5=->+vHWBN3=5O>/567FAC/EwD0/N369;3xKbBAf9PO>8:70Tc5=9d1(+yLN+cO0+cA`/Ed9;QR/569;BOU
VOe3C+RTc5=9GBO"Y\[]_+z56-0+cA`+cK{BAC+|/E;3CB3656708:9;+28}5=->+~5=->+RBA6geBKiLN+cO0+cA`/Ed9;Q2/5=9GBO"7FO>8+IAv9dH@?EP9;T2/5=9GBO^U
u-0+~TRBO(5C+IO5`3zBKi3C+2TI569;BO"Y|TR/O}Dff+~367FHH4/A69;QR+28@/N3uKbBEPE;B2]3R
lNUV5_9G3M8F+2TI9;8/DE;+~]M-0+c5=->+IA/|TcE;/7>3C+yst367D0367H4+R3/O>B56-0+cATIEG/703C+U
Y\Uu->+IAC+_+c\9G3=5C3w/ME;+2/N3=5LN+cO0+cA`/E>LN+IO>+IAC/EP9GQR/569;BO@7O>8F+cAst3=7D>3=7H@?569;BOkj{~zs(rBK>+I1(+cA=g~OF9d5`+
3C+c5BKTIEG/703C+23U
 UV5_9G3u7FO>8F+2TI9G8F/DE;+]M->+I56-0+cA/|TIEG/703C+z9PH@?EP9G+R3u/O0B56->+IATcE;/7>3`+NU
 UV5f9G3_/O}B?ff+IOW?FACBDE;+IH]M-0+c5=->+IAM56-0+cA`+~+I:9;365`3u/E;+2/N365_LN+IO>+IAC/EwLN+IO>+IAC/EP9;Q2/5=9GBOe7FO>8+IA9PH?FEd9Pt
T2/5=9GBO,jwyzV=ruBK+I1(+cA=gWOF9d5`+3C+I5MBKTIE;/7>3C+R3RU
 Uut9dH@?EP9;T2/5=9GBOS9;3/,3656A=9GTI56EPg3=56ACBO0LN+cAAC+IEG/5=9GBODff+c5]_+2+IO%TIEG/703C+R356-0/O9PH?FEd9;TR/569;BO[M/O>8
3656A=9GTI56EPg]_+2/a(+IA}5=->/Ost367D0367H@?5=9GBO^[y/O08ut9PH?FEd9;T2/5=9GBOT2/ODff+2TRBHW+"/O/A=D9P56A`/A69PEdg
LNB:B\8}/??FACBR:9PHW/5=9GBOBKw9PH@?EP9GTR/569;BOWD(ge+I:5`+cO08:9PO>L56-0+T2BO0369;8+IAC+R85`+cA=H3C+c5U
 UV5_9G3M8F+2TI9;8/DE;+~]M-0+c5=->+IA/|TcE;/7>3C+yut9PH?FEd9;+23_/O>B5=->+IAzTIE;/7>3C+U
n\Uu->+IAC+M+I\9G365`3/E;+R/N365LN+cO0+cA`/EffLN+cO0+cA`/Ed9;QR/569;BO7FO>8+IA_ut9dH@?EP9;T2/5=9GBOjw~r^BKff+I1(+cA=gOF9d5`+
3C+c5BKTIEG/703C+23U
V O~3`+2TI569;BO  []u+365=7>8\9G+R8~56-0+x8\9dff+IAC+IO>TR+xDff+I5]u+R+IOst367FD>367FH?F569;BO/O>8z9PH@?EP9GTR/569;BO~BO.TcE;/7>3`+23RU
*,+.?FAC+R3C+IO5`+28ZB7FA/?F?ACBN/NTC-Z5`B}O08/EdELN+IO>+IAC/EP9;Q2/5=9GBO03.7FO>8F+cA9PH@?EP9GTR/569;BO[^D(gkA`+28\7>TI9dO0Le9dH@t
?EP9;T2/5=9GBO,5CB}st367FD>367FH?F569;BOUu-F9G3TR/ODff+|/NTC-F9G+I1(+28D(gk9PO(1(+cA=569PO>L"3C+IEdK;tAC+R3CBEP7569;BO[/O>8"]u+|8F+ct
3CTIA69PDff+28Z/}5`+2TC-FO9;70+}KbBA|9PO(1(+cA=569PO>LA`+23`BEd7F569;BOD0/N3C+R8ZBOZBA6t9PO(56ACB\8:7qTc5=9GBOBKMEd9P5C+IAC/E;3RUk*,+W/E;3CB
8+R3CTIA69PDff+28X+c\?>/O0369GBOBKMTIEG/703C+23[]M-F9GTC-X367H@HW/A=9GQR+R3B7FA9;8+R/eBKuAC+R8:70Tc5=9GBOSBK_9dH@?EP9;T2/5=9GBO5CB
st367FD>367FH?F569;BOU
u-0+~TRBO(5C+IO5`3zBKi3C+2TI569;BO  TR/O}Dff+~367FHH4/A69;QR+28@/N3uKbBEPE;B2]3R
lNUOk+c\?>/O0369GBO,BK/@TcE;/7>3C+9;3/O"wyzsBKf/3C+c5vBKfTIEG/703C+R3BDF5C/9PO>+R8kD(g"BA=t9dO(56A`B:8\7TI569;BO
KGACBH56-0+~TIEG/703C+U
Y\UBAx+I1(+IA6gLN+IO>+IAC/EP9GQR/569;BO}7O08+IAu9PH?FEd9;TR/569;BOBK^/TcE;/7>3`+M56->+IAC++I:9;365C3/O4+c\?>/O0369;BO}BKff56-0+
TcE;/7>3C+[FE;BNL9;T2/EPEdg+2(79P1N/E;+cO(5v5CB56-0+TIEG/703C+N[ff367>TC-"56-0/5v56->+.LN+cO0+cA`/Ed9;Q2/5=9GBO7O08+IAz9PH@?EP9GTR/t
569;BO}9G3_AC+R8:7>TR+R85`B/|LN+cO0+cA`/Ed9;Q2/5=9GBO7O08+IAst367D0367H@?5=9GBO^U
 Uu->+IAC+e+I:9;365O>BOFt5C/7F5CBE;BNL9GTR/E|TIEG/703C+23KbBA@]M-F9GTC-S56-0+cA`++I\9G365|O0BTRBH?FEG+I5C+}+I\?>/O>3=9GBO03R[
]M-9;TC-H4+R/O>3|56-0/5W56-0+cA`+"/AC+eO>B+I\?>/O>3=9GBO03}BKz5=->+"TIEG/703C+23@3670TC-5=->/5}+I1(+IA6gLN+cO0+cA`/Edt
9GQR/569;BO7FO>8F+cAy9dH@?EP9GTR/569;BO9G3AC+R8:70T2+R8<5`B}/WLN+IO>+IAC/EP9GQR/569;BO7FO>8+IAst3=7D>3=7H@?569;BOZBKf56-0+
+c\?>/O0369GBO03RU
 UBA+R/NTC-O>BOFt5C/75`BEGBNL9;TR/EvTIE;/7>3C+@56->+IAC+4+c\9;365C3/e_tT2BH@?E;+c5`++I\?>/O>3=9GBO^[i]M-9;TC-H4+2/O03
56->/5_+I1(+cA=gLN+IO>+IAC/EP9;Q2/5=9GBO}7FO>8F+cAut9dH@?EP9;T2/5=9GBO4BKff56->+vTIEG/703C+M9;3iA`+28\7>TR+28@5CB/yLN+cO0+cA`/Ed9;Q2/t
569;BO}7O08+IAzst3=7D>3=7H@?569;BOBKw56-0+~+I:?0/O>369;BOU
2

fi(>(\;2N\GffWff@>>I(>>0"(G(N\Gff

v>`2"dCRI6;F\puRFGIC.c\>06G0~0~zMCRFCFGqF60C
 2@FC6;GI:=CIWIP}RN66P(u;M;06=6;6P>}=d02FvGC4Iu=ebNc0c`d
@
;2=GF>Fc|PFd;R6;<;z>N`2X>I\CI>G=:Pz;C4c_6,b|NI>ICP;2=G
0Iv606@=G^0MFGCePCRN:6ffICM;CRFGI:PFCFGIWiNc0c`ww~
~CIfc;>CRWR|CR%I:ff0c(6;Pd@d|=>(|ffIuIG0CRiP6>vCI{e>N;IC}
 I>F>2NN(cii(Iwy`2\>R20Iu6F>6FF6;:MFGC420^60uPPd`c`G=>
C`2\>F(F>Fc606@=Ge`CIWR(2p(42C2c\ff>I(6;dP@d@60(F|ffc_
IG0C2{v;c`N2NN(P>R<I:0>6;f4c;>C|;v<~fWCIu=d(=C\:>RR
IG0C260uRF`6;>RN6w0|I:0>6;CRc\ff>I(6;dPP~=>x(F|ffcFdPCIC;
>`2@d46>y6P(6C\:0c`GMW60zR@`6;"w_2@;c`vc\>06G"yG`N(|ffI
Pd`c`G42ffWR>6;ICRffM>ZR>`2(>I(6P=>CS"2@FC=GS_;Xff4c\6`c4Id
RN66P(
 2_I(cFP60>i;6\PxG`WI_6@bNI>ICPGR6;e0Iv606@=GG_2@
FC=G0dPc\ffc06P(N0P>NMffRIeMGFcPW0CReN=>RCI6;2;C4c_6p__>IeCW(
Pu`Nc=GRq(|ffI:PffICIv`26=6;c=G0z460~IG0C^;>0NN~>NMffRck206GFc`2p
b.cF@;|GNICI6@d0Nc{e0NGICZ  I>Ff2NN(,}6PPG_2"_|>ffCWF>
CR6=6;c=G0M>Fc;CWFM>IC`2`c(CR}G`WI_6@bMNI>ICP;2=GeF>IMPFd;R6;
R}ffzFCNI6;2Pd>CIGF
R6C@d4(;662NN^2NNN(0NWRCI6Pff2<`2CF;0k`c@IGI(6PS2@FC"
CR6=6;c`2Sb=vNc0c`d;R6;>WF>I@d@PGR6;S2Rc(=d:xe>N;c`0NFCRCc(`2
>=>IFCNNC,>NCR""Nc0c`6P>e|(|ffIc;>CRRq`Rd;Re6FC6FC\CRwM;C
C20:;CRbffId0P>\d`2IC\CR:060cCR6=d0M0c=>I6>I|`uCv>e>N;c
C^p2N(NciuFGfFCNNCe@G(ffz_2CW`c@IGI(6P2@FCC4MNI>ICPGR6;>
0Id@PGR6;:4f`NNC>Rx`;R6|CRIC=d(MFcFP6;>z;CRIC=d(u;N;x`NC4
("Nc0c`d;R6;<0IzP@PGR6;e>R(ff2I"CRCI(CRS{ffd(`kMP2N(\^>F
^ffP(CupP>FikMP2NNFuR6C@d4(;6R_2N(Nu>RCWF`NNC>RWC4>NCR
%=660c=`>P6;}y6>P(ccFFGRRMk26>RCI6;RdPffRCI6Pff2P
G`WI_6p
660:Z(Su0c=2N(Fz2N(q|6>4GR60PdP`2I`6P(}GN;W`NC4|>N
`c\;>6P}ffRc}FCRCI`2eP}6FG
 =>f}=;660:}Pu_N6>RM}=>v|CRcFC6P(~;NG
`NC R>6;66P>Zy>eR>=C(6Fc>` ICI6@d0C"c;NCR
 (6SCRIC=d(c;>`e>
>|R>=C(6FcFCZICI6@P>C|0CRIC=d(@c;>`~Gvx;2=>FG|P(c"NF:P6;>
6>N`cRNC|CNI; FqMFGCFc`c=P>RPffN6d=d(.cFFGGvR2(ICRk(}=>0CRIC=d(
>N`I;>C|f60`CNI~`NCJG0N|v_N~;CW=>2<60NI>ICPGIP>}6FGyc;NC
;2=P><FC;I dZ06FCvuR;2NF.`<"RFC=G0dP:P@cFd|FC;I"Zu;
CR6FdM`cP;u>u=>`|c@IGI(6PW;2=}W`v2@;c`2I`6d((:ff=>RC2vC4vc\6`|dFb6
46;60C@N=;u4:Fc;_{vGIC
 `ffc2N(NCN` CRcFC6;4CC>IWRf  @bIP
 P;CC2NN(Gu02RRff
	
u0xR(6=dFC;>_>6FGw0ffc`i=CRIb;ff  d`6Ru>R(u6F6`c46;2PP`c\;c_2.>
:;CI>C`2"60R>RICyCIGIN(~`WNI>ICPGR6;P<@C=6CFc`c=6P>F\2R>p_~>R(
P=C\:02RWuP@PGR6;N=6C0Ncfb= P@PGR6;MFGC|;wRIGF;uffc_2I@c;>`2R
uFd`ff^u02(G=60cFc(IGffRFCI:;>yu=f60604{R6`PW(;6R2NNN
4c\CI>\d0;6\d^ fGC4Iu={MNI>ICPGR6;e0I6F>6FF6;}CNI>ICP;2=G
0Ivd@PGR6;


fiff

fi !"$#&%'
(*),+.-0/21436570895:;5-=<>
?A@BDC!EGFH@2I!JLKNM'OH@PFQIRFH@C!S&TU?VI!JHT,BXWNY2JQC!SZ\[B SU]^I!J_M`S,abC!WcE2C!dGWeBf@2BXW`gUhiI!S2hiBXJHS&McS2jkFH@BDKlI!JHTPI!SU?Am
McnDgGW`M'hC!FoMeI!S=pD?A@2BLC!EGFH@2I!JLC!W'OQIqKrMeOH@*FsIRFH@2C!SGTUt,@2C!S&m6u KlB\Mlv MeBXS&@,EGw&OHmyxz@2BXS2jG{}| I!S2C!W'~~GB_I!Wc]
C!S2~FH@BC!SI!SwnI!E2OqJsB\aMeBXKlB\JsOD]I!JCS,E&n_d$BXJRI!]FH@2I!Ej!@,FH]eEW_hI!nfnB\S,FQOLC!S~OHEjbjbBOHFoMeI!SOI!]
McnDgGJQIa,BXnB\S,FQOp
?A@GMeOKlI!JHT@2CbO.d$BiB\S_OoE&g&g$I!JoFQBi~d,wFo@2BrtKlB~MeOH@L| BiOQBiC!JQhQ@fxAI!EGS2hXM`W]I!JS2j!McS2BiB\JoM`Sjth\M'B\ShBiO
 ?rY.|}C!S2~RFo@2BE&JsI!g$BC!SkxlI!nDn_EGS&McFwltz| ?A| b,!fS~Eh\FoM`a,B_Ibj!Meh0JsIbj!JQC!nfnDMcS2jGp
*52Q5,5-)&5>
 @2CG{._pp`{=C!g$I!McS,FQB{Atpc{VMcS2jG{xpp`{*C!F6KNMcS9{tp Hbb p*S,a,BXJHFHMcS2jkM`nfg&WcM'hC!FoMeI!SPKNM`Fo@
OHnqC!WcW=FoJQC!McS&McS2jOQBXFQOp6Szfi_y29fiX&zs!UAfis_fi*k,s&$
 !X2^!p&t,g&JHMcS2jbBXJHmzBXJHW'Cbjp
AC!McS9{p`{E2jbj!W'BXFQI!S9{ztp Hb b\pfvI!SGmnqI!SI!FQI!S&M'hDW'BiC!JHS&McS2jGpq6SE2jbj!W'BXFQI!S9{tp  z~$p{.\
!2s^fi  fiAsfifi__!p,hiCb~&BXnfMehJQBiOQOi{2=I!S2~GI!S9p
x}@C!S2jG{xpc{2=BiB{G|p H,! \pfiD!  sbfisfi2fi2fiXAsfi^!p.hiCb~&BXnfMeh
0JsBOQO{I!S2~GI!S9p
xAI!@B\S={p Hb, C,p.Cbh\mW'BC!JoS&McS2j_JQBihXE&JQOoM`a,BWeIbj!M'h g&JsIbj!JQC!nOi}0fhXMeBXS,FAC!W'jbI!JHMcFH@GnqOpNfiXfiy
 s Afi&XoX^e,X$rrsfis{${   &!b p
xAI!@B\S={0p Hb, dp.Cbh\mW'BC!JoS&McS2j*JQBih\EGJQOHMca,BqW'Ibj!Mehfg&JsIbj!JQC!nOiDvBjbC!FoM`a,BfJQBiOHE&WcFQOipfiXfiAy
 s Afi&XoX^e,X$rrsfis{${ !G&b! p
 C!WcWcMeBXJi{AGpVup Hb ,\p  sb fiAfioXD9X6X9fi,fi  bofiD=fis
zfifipuC!Jog$B\J	| IKE&dGW`M'OH@B\JsOip
 I!FoFHW'I!d9{  p Hb, \ptE&dOHE&nfg&FoMeI!SPC!S2~*McnDgGW`M'hC!FoMeI!S=pXfiXDi6!As^  soQX{  b\{
  bb p
uC!nD]^B\WcFi{2p`{2v McWeOsOQI!S9{$&pYlp obb p6S2~E2hXFHMca,BnqBXFQC!W'Ibj!M'hg&JsIbj!JQC!nfnDMcS2jGpA6SPAsfi&y
2 9fisL\oXfifiA4fiskfif\fi2sb  sbrAsfifi__!p  BiOQBXW`W'OQhQ@2C!]'FA]E& J
C!Fo@2BXnqC!FoM`TfE&S~C!FsB\S,a,BXJQC!Jod$B\McFHE2SjRn_d2u{&ACb~RuI!SGS2BX]!AI!SGS9{  B\JonqC!S,wp
uB\Wc]eF{rvp ob, pS~Eh\FoM`a,BPjbB\SB\JsC!W`M'ZC!FoMeI!S=W'Ibj!M'hC!W]eJsC!nqBXKlI!JHTpSAsfi&R
9fiAs2fi4fisfi&9s6!fi  fiX2fipGt,Mej!nC_JQBiOQOi{&McW`nOHW'IK{S2j!W'C!S2~p
u E&nBb{pc{tC!nDn_EGFi{xp HbG \pOHMcS2jM`S,a,BXJQOQBLJQBiOQI!WcE&FHM'I!SUFQIRW'BiC!JHSJsB\W'C!FHM'I!S2O]'JQI!nBXg$BXJHm
M`nB\S,FsOip6SzfiLyAc,R\&oXXfifi4fiQsfik&  !X2^!p
I!JsjbC!S*C!EG]enC!S&S=p
y~&BiOHFQC!nfm6 W`nEGMeOoFi{fi0p ob bp==BiC!JHSGM`SjnDM'OQOHMcS2jhXWeC!EOQBiOVd,w_McS,a,B\JsOQB JQBiOQI!W`EGFHM'I!S9p=6SRAfi
0VXoXXfifiAfisX$}fi0 sDXsfiqAfibo$ioXA&,!pbr@GnqOo@2C
0EGd&WcMeOH@B\JsOi{?VI!Tw,IGp
fi

fir,2,$	ff
fiX2,,$

!#"%$'&()*+&-,/.102"!3547698#:<;;=$>?6A@CBEDffBEFHGJILKNMOGPQKSRJDTRUWVIGJX/YOBHYE6Z4\[]6_^`6a"#[b<20!3c^dfeb$'g#"#&-Eh/"ji'k
l i'&me1."Hfgd$'honp#"Hf&-CnrqE0fhbq<!!3]n/"Hirq%s[i'*N&uth0NvfgH#0N"Sp$'hbWwi!p$'*7h#"20+"2."%i'kxy!q%[(
hi'*iz'p6
!#"%$'&()*+&-,/.102"!3{476|8#:<;;='}~>f6
EhEg%$'*N0!$'"#0i'h.h1fg0N&me1*+0q<$'"20i'h}/pgH<qE.gH#0Nv$'h/"20+(
.h0Nq!$'"#0i'h65h5FHRO?B?B?JKDYRQUPbBmBEDPfDP2B<FEDGPQKSRJDGJIVR'D!UBEF?BED?BdRJD-oGHKDBB?GJFED
KD'6i'g%z$'hd$'.k&	$'h1h6
!#"%$'&()*+&-,/.102"!347682:<;;=q>f61dfhbfgH$'*+0!$'"#0i'h.h1fg0N&me1*+0q!$'"#0i'h}p.b#0NhzCi'g#(0Nh"#g%i.~qE"#0i'h6bh
F?R?B?B?JKDOYCRU9PbBKP5XrF?RHbB?GJDV5RJD<UBEF?BED?BCRJD-oGHKDffB9B?GJFEDKDJ6n/eg20+hbzfg2(fg2*$z16
!#"%$'&()*+&-,/.102"!3r4\68#:<;;>f6\\qE0Eh"0Nh.qE"#0i'hi'k]gH<qE.gH#0+vdEh10+"20i'hb}/p-#"2g#.qE"#.1g%$'*$'hb$'*+(
p#0i'k%$'"#.1g%$'"20i'hb!6h|5FHR?B?B?JKDY`RQUPbB\K UHP	EDP2BEFEDGPQKSRJDGJIjR'FHOY?R?RJDfDJXHPQKB
RHJKd5F?RHJF?GJKD'6~^Ee$'g2"#&-fh"i'k l i'&e.1"%EgnrqE0fhbq</3d$'"#[bi'*+0Esmth0NvEg%#0N"%E0+"]E.(
vfh]31f*z'0+.1&6
C0E"%/3a16 (t678#:<;;=>?6	)q!i'&e$'g%$'"20+v#"2.rp|i'kc2"#g#.bqf"2.gH$'*c&	i2"2eff<qE0+bqzfhbfgH$'*+0<$'"20i'hb.%!
0+h&-$q%[0Nh*<$'g2h0Nhz16h|j.bzz'*E"%i'h]3an678ff6>?35FHR?B?B?JKDY`RQUPbBjKFHEDP2BEFEDGPQKSRJDGJI
jR'FHOY?R?|RJD	EDJX1?PQKJBR?JKS5FHRHFHGJKDJ616bn"%Ek$'hoShb#"#0N"#.1"%/3.}*Q$'hb$13]n/*i<vEh0$16
C0E"%/316 (t6N3bog%i'}ffE*Q3]n~678#:<;;>f6 l i'h"#gHi'*+*N0+hbz"#[bq<i'&e*f0N"Spji'k*!$'g#h10+hbz	0+h*iz'0q"#[1g%i'.z'[
#prh"H$qf"20qC$'h"%$2sr(i'g20Eh"%!&	iE*6bh	.zz'*E"%i'h3~n~6~86>f3fDJXHPQKB9RHKS55FHRHFHGJ
KD'6)Cq!$E&m0q4\gH<H!3bn$'h^0!zi13 l $'*N0Nki'g#h10$16
di<$'*#s0Q3w6782:<;'>?6	x5[q!$%ki'g.#0Nhz!,/.b$'*+0N"p$'0i'&	C0+h|$'."Hi'&	$'"20qE&-i'h#"2g%$'"#0i'h6	h
'RY<KXR'D|CXP2RJmGPQKS	BEmRJDY?PQF?G!PQKR'DB??PQXrF?B-DRP2BHYjKD{G!PbB<mG!PQKHY?bR'ILC6
n/eg20+hbzfg2(fg2*$z16
]$'effi'0Nh"%/3n~6N3O$'"c0+h]3rn~68#:<;;>f6n.}(.1h0Nq!$'"%0i'h)"%ii'*rki'gfqE0Eh"]0Nh.qE"#0i'hmi'kbg%!qf.1g%#0Nv
eg%iz'gH$'&	61h5FHRO?B?B?'KDY9RUPBKDPfDP2BEFEDGPQKSRJDGJIbVR'D!UBEF?BED?BcRJDoGHKDffBB?G'FEDKD'6
i'gHz$'h$'.1k&-$'hh]6
]$!vrgH$q/ 3\6+3^dw$!r"!3y682:<;;>?6mhr.bqf"20+v*iz'0qegHiz'g%$'&&m0Nhz1)#.1g#vEpi'k.g%i'eff!$'h
g%!%<$'gHq%[6c`V5RJXDKS?GPQKSRJDYE382:>?3b=O~:<;16
]$!vrgH$q/ 36N3^! fgHi#s03n682:<;;>?6fDJXHPQKBRHKS5FHR?JFHGJKDJffB??DKSEXBHYGJD~ILKS?G
PQKSRJDYE6a*+*N05Ci'g#iriff6
]!/3 l 
6 8#:<;>f69V5RJINB?P2BEDB?Y?Y]bB?RJFHBEGJD-GV5RJXP2B<F5F?RHJF?GJTURJF\KDJKD]bB?RJFHB<Y
BEFEKJG<INBaUEFHRJ@9KJB<DCJKSRJYE674\[]6_^`6r"#[!#0!3bth10+vEg%#0N"pi'k l $'*N0Nki'g#h10$13ff5Eg#sE*Ep6
*i<p1ff3ff166]82:<;>?6RJXrDff/GPQKSRJDY`RURHJK95FHR?JFHGJKD'6yn/e1g#0NhzEg#(fg2*$zb6nr!q<i'hb<0+"20i'h]6
$'g%qE0+h1si<2sr0Q3c6N3aA47$q%[i'*#s03658#:<;;>f6th1<qE01$'}0N*+0N"pi'k"#[b[i'g#hqE*$'.b%	0N&me1*+0q!$'"#0i'h
eg%i'}1*E&W6ah5FHRO?B?B?'KDYRUPbBm]KFHPQ!PKF?DDXGJI~H\'RY<KXRJD-RJXrDGPQKSRJDY
RQUVR'X/P2BEF9<KSBED?BE3e1e6~='Or=`40+"2"%#}1.gHz1347fh1h#p*+v/$'h0$16
.zz'*f"Hi'h35n658#:<;;>f6WShbr.bqf"20+v*iz'0qegHiz'g%$'&&m0Nhz16ShF?R?B?B?JKDOYRU`PbBjRJF%OY?bRHRJD
IRJFEKNPKScB?G'FEDKD]bB?RJFEO69[1&	#[b$4\.1}*N02[Eg%!3xai'srpi16
J

fiSb/?O~Q'ff?

'fH'	ff
fi
SrE	%

G IHJ-K
JLNMBOffP

!"!#+b$&%ff')(*%,+-%ff. /021435+768359;:-<fi021=+?>A@B35<C.+D/EF

'fH'QR	ff
fi
JKfiSTSJ,#U+bVW!#Xff'U'YZ\[8.

^3 ]S%S%S_1I+C>`A3ba"0dc%"e%S]S35+-_Af,+C0U%,.,+-/021435+-/5E
g 3. h^`Sc3S:i3+jf+-_5<] 021=kfi%8l3 >51m]n[8.S3 >5.S/59o9o1=+C>ffUpQq*rTryNts?XN'ff,!-t'?!urcwv fiffPJKC

'fH'ox	ff
fi
fiOJyE	z?% !"!#N{}|dUW~'	'N#ff!ff % ffi'	sb#	v
< !#+bfi NS
 [8. 3]S%S%S_51=+?>`3bai0dc%we-10dc\8<B.S3S:%S/5+6835+ffa%ff. %ff+-]S%3+z/J] c?1=+-%wl%S/.,+1I+C>
~?U+b Uvd 2  
'fH'DD	ff
fi
JLfi~,tHEtNW!-fR'%'2$&%,'(*%,+-%ff.

~ 

/021435+68359;:-<~021=+C>j@B35<C.+D/EF

'fH'8-W	N~n	ff
fiPfiPJfits+b"NJf	'Q? #	v ,%NBXff'HffoJVN?v
 2	 N
 Hff %'NU 'Y
 
 [n.S3]S%S%S_51=+C>^`o3ba0dc%&1 a 0dcf+C0U%ff.,+-/021435+-/5E6835+ffa%ff. %,+D]S%o3+z/J] c?1=+D%
l%S/.,+1I+C>r %fi '
 x'?
 X!'1
 
'fH'?|QNr?	ff
fi
fiHJE	Q%

@B35<C.+D/E3ba;l3 >fi14];[8. 3S>5. /59o9o1=+?> ?4? JK
^MBJ


!"!#+b{r8sb<U"'"!-fUsC?N

'fH'Y-W-~fb-p*	ff
fi
fiJR",fQNE	'{y&?% !Z[8. 3]S%S%S_51=+?>`&3ba
0dc% g 3. h^`Sc3S:3+#*E>J35.,10dc?9o14]Rl%S/5.,+1=+C>Yc%S3.,^qs?!A	so? 1WX	sb N-ryC
'fH'Qfi|j8Uff
fi
fiHJSrEW*t'2? '
 	'Z*,+HwfX'HffN7\[8. 3]S%S%S_51=+?>`A3ba
0dc%35<B. 0dcf+C0U%ff.,+-/021435+-/5E g 35. h`Sc3S:z35+#f+-_5<] 021=kfi%Rl3 >fi14]R[8.S3 >5.S/59o9o1=+C>JN %E+Xtts8S? 
 'U s, !'U W

 w|'HfJ
 , tUff N#b
 !o?8fiC'1, d5'1
 Yd U!'J
 B

W1E#r	ff
fiPfiPJS#2B-fb 'WXff'U'wNXQ?Ht!ANw[8.S3]S%S%S_51=+C>^`*3baR0dc%jYc?1=.S_
n<C. 3S:%S/+ g 35. h51I+C>e%S`S`ff1435+3+il%S/5.,+1=+C>5?N	!A'
E?s?	vbpnsE- vmC1;|'W	&Uff
fi
JLfiSrsx#?#?!#U'Us!%,!N#+rE	
?%  !"!#N {"fiE to' 7I' +Xfi, NN \[8. 3]S%S%S_51=+?>`#3ba"0dc%j1 a 0dcf+?0U%,.,+-/021435+-/5E g 35. h
`Sc3S:Z35+f,+-_5<]S021=k5%l3S>514];[8.S3 >5.S/59o9o1=+C>Bd
| ,# U	
! ECn5
p !# HxC E! fi' Us ' , 
 W ,tU+ HN ]
9
 E? ED5EN ?!
'UBNo|jUff
JJSw'%-'VNE	fb 'WXN'	'{E+ <,N* wts~|o
In?NSDz/J] c?1=+-%;f,+C0U%,EIEF1>J%,+D]S%~'mLC??LO^MfffiO-C+J1?%sZ? 	N4Hfft
'UBN?j5|j?	ff
JCffJ*<fi0U359"/0214]Qz% 0dc3^_N`3banf+D_<] 021Ik5%8f+ffa%ff. %ff+-]S%sY|jOUsN	XNBC+J1?%s
 W ,tU+ mB
9
'UBNJo|oJ	ff
JC-S?2	s,'%'NrfUWE,t'Xff'U'YfNt!5WwXts?fi
|oDIn?z/J] c?1=+D%Rf+C0U%ffE=E1>J%ff+-]S%B'2C?YYffSMKH-nBN J ?H sV
 W ,t	N mw% NtN
 ? NX'~&	ff
fiPfiJSrfU'j1ff,X	'U%!ffNz/J] c?1=+-%8l%S/5.,+1=+C>@C3<C.,+-/5EF  US?PSMfffi
?NH'?J	ff
fiJLfiSY\!fits+bvUE%N	X8fi%ff'	sQHffH'+1	'?U+~Q@C35<B.,+-/5E
32a;0dc%R6w ~ 	SDKO^MBHfi
5

fiRJJCXfffiCDD#,J~mJJfiCD

?J,t2*	fffiJfiiB 	QJJ 	}QtNt?	X?WXNVtUN	TN",	X
4ifi,tYQ8d	ff
fifffiff  fi!#"*Nfi?,"X%$ ff NC'&;Xfffi
8W=U?X
?J,t2*)(*$fi,N	+ ,.-	ff/fiJS0"UW#?XQt?U#=WJJ,		o fft?UYm12334
fi5!36798:
<;=:>?!@
<;BA2!ffC=EDBffFfi5GIH?ff66Jfi.KLM	fi5!5G $	A
Bt"XBN,bBttN7	ff//JSo4#?XffUzXtN*??NX?O~QPR;=ff
fiSUT:C#)
NM
H?MfiJ#W VGX /ZY\[? ]
~XfiXfi0+&W*^W(_^YNfi-&*W`xUff]fiJSj",t,Nt*	Nt,A;It,A~U
tNtW	XW fftJ aJ,NM ,<b?CW-mc
 ffdfi!36e98B
<;=1>ffi5ff6
I!
NJ\
fi.Shg!fi5!

TJ83MM#iLj:ff
fi kJfiSGM!
NMSSlfimZM?tfio
 nx?A?Y# ^Yfip
 "Rtfi8=	

q\WU	&Uff/fiJSj",	#fiX? fitArOJWJJ,tto fft?UYms23fi5!d6 98h
<;=
>?tff
<;uA!ffC	EDBffFfi5!7H?ff66JfiKLM	fi5!5 !$ 	A

vw3x

fiJournal of Artificial Intelligence Research 3 (1995) 271-324

Submitted 2/95; published 11/95

Flexibly Instructable Agents
Scott B. Huffman

Price Waterhouse Technology Centre, 68 Willow Road
Menlo Park, CA 94025 USA

John E. Laird

Artificial Intelligence Laboratory
The University of Michigan, 1101 Beal Ave.
Ann Arbor, MI 48109{2110 USA

huffman@tc.pw.com
laird@eecs.umich.edu

Abstract

This paper presents an approach to learning from situated, interactive tutorial instruction within an ongoing agent. Tutorial instruction is a exible (and thus powerful)
paradigm for teaching tasks because it allows an instructor to communicate whatever types
of knowledge an agent might need in whatever situations might arise. To support this exibility, however, the agent must be able to learn multiple kinds of knowledge from a broad
range of instructional interactions. Our approach, called situated explanation, achieves
such learning through a combination of analytic and inductive techniques. It combines a
form of explanation-based learning that is situated for each instruction with a full suite of
contextually guided responses to incomplete explanations. The approach is implemented
in an agent called Instructo-Soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions. Instructo-Soar meets
three key requirements of exible instructability that distinguish it from previous systems:
(1) it can take known or unknown commands at any instruction point; (2) it can handle
instructions that apply to either its current situation or to a hypothetical situation specified in language (as in, for instance, conditional instructions); and (3) it can learn, from
instructions, each class of knowledge it uses to perform tasks.

1. Introduction
The intelligent, autonomous agents of the future will be called upon to perform a wide
and varying range of tasks, under a wide range of circumstances, over the course of their
lifetimes. Performing these tasks requires knowledge. If the number of possible tasks and
circumstances is large and variable over time (as it will be for a general agent), it becomes
nearly impossible to preprogram all of the knowledge required. Thus, knowledge must
be added during the agent's lifetime. Unfortunately, such knowledge cannot be added to
current intelligent systems while they perform; they must be shut down and programmed
for each new task.
This work examines an alternative: intelligent agents that can be taught to perform tasks
through tutorial instruction, as a part of their ongoing performance. Tutorial instruction
is a highly interactive dialogue that focuses on the specific task(s) being performed. While
working on tasks, a student may receive instruction as needed to complete tasks or to
understand aspects of the domain or of previous instructions. This situated, interactive
form of instruction produces very strong human learning (Bloom, 1984). Although it has

c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiHuffman & Laird

received little attention in AI, it has the potential to be a powerful knowledge source for
artificial agents as well.
Much of tutorial instruction's power comes from its communicative exibility: The instructor can communicate whatever type of knowledge a student may need in whatever
situation it is needed. The challenge in designing a tutorable agent is to support the
breadth of interaction and learning abilities required by this exible communication.
In this paper, we present a theory of learning from tutorial instruction within an ongoing
agent. In developing the theory, we have given special attention to supporting communicative exibility for the instructor (the human user). We began by identifying the properties
of tutorial instruction from the instructor's perspective. From these properties, we have
derived a set of requirements that an instructable agent must meet to support exible instructability. These requirements drove the development of the theory and its evaluation.
Finally, we have implemented the theory in an instructable agent called Instructo-Soar
(Huffman, 1994; Huffman & Laird, 1993, 1994), and evaluated its performance.1
Identifying requirements for exible instructability provides a target { a set of evaluation
criteria { for instructable agents. The requirements encompass the ways an agent interacts
with its instructor, comprehends instructions, and learns from them. The most general
requirements are common to all interactive learning systems; e.g., the agent is expected
to learn general knowledge from instructions, to learn quickly (with a minimal number of
examples), to integrate what is learned with its previous knowledge, etc. Other requirements
are specific to tutorial instruction.
Our theory of learning from tutorial instruction specifies how analytic and inductive
learning techniques can be combined within an agent to meet the requirements, producing
general learning from a wide range of instructional interactions. We present a learning
framework called situated explanation that utilizes the situation an instruction applies to
and the larger instructional context (the instruction's type and place in the current dialogue)
to guide the learning process. Situated explanation combines a form of explanation-based
learning (DeJong & Mooney, 1986; Mitchell, Keller, & Kedar-Cabelli, 1986) that is situated for each individual instruction, with a full suite of contextually guided responses to
incomplete explanations. These responses include delaying explanation until more information is available, inducing knowledge to complete explanations, completing explanations
through further instruction, or abandoning explanation in favor of weaker learning methods.
Previous explanation-based learning systems have employed one or in some cases a static
sequence of these options, but have not chosen dynamically among all the options based on
the context of each example. Such dynamic selection is required for exible instructability. The learning framework is cast within a computational model for general intelligent
behavior called the problem space computational model.
Instructo-Soar is an implemented agent that embodies the theory. From interactive natural language instructions, Instructo-Soar learns to perform new tasks, extends
known tasks to apply in new situations, and acquires a variety of other types of domain
knowledge. It allows more exible instruction than previous instructable systems (e.g.,
learning apprentice systems, Mitchell, Mahadevan, & Steinberg, 1990) by meeting three
1. Because our work is inspired by human students, we have also taken cues from psychological effects
where appropriate. The theory's potential as a cognitive model is discussed elsewhere (Huffman, 1994;
Huffman, Miller, & Laird, 1993).

272

fiFlexibly Instructable Agents

Push the green button.
Thats a new one. How do I do that?
Move to the grey table.
Ok. What next?
Move above the green button.
How do I do that?
Move the arm up.
Oh, I see! What next?
Move down.
Ok. What next?
The operator is finished.

To turn on the light, push the button.
Does this mean pushing the button
causes the light to come on?
Yes.
...
Never grasp red blocks.
Why not?
Red blocks are explosive.
If the light is off then turn it on.
...

Figure 1: An example of tutorial instruction.
key requirements of tutorial instruction: (1) it can take known or unknown commands at
any instruction point; (2) it can handle instructions that apply to either its current situation or to a hypothetical situation specified in language (as in, for instance, conditional
instructions); and (3) it can learn, from instructions, each class of knowledge it uses to
perform tasks.
In what follows, we first discuss the properties and requirements of tutorial instruction.
Then, we present our approach and its implementation in Instructo-Soar, including a
series of examples illustrating the instructional capabilities that are supported. We conclude
with a discussion of limitations and areas for further research.

2. Properties of Tutorial Instruction
Tutorial instruction is situated, interactive instruction given to an agent as it attempts to
perform tasks. It is situated in that it applies to particular task situations that arise in the
domain. It is interactive in that the agent may request instruction as needed. This type of
instruction is common in task-oriented dialogues between experts and apprentices (Grosz,
1977). An example of tutorial instruction given to Instructo-Soar in a robotic domain
is shown in Figure 1.
Tutorial instruction has a number of properties that make it exible and easy for the
instructor to produce:
P1. Situation specificity. Instructions are given for particular tasks in particular situations. To teach a task, the instructor need only provide suggestions for the specific
situation at hand, rather than producing a global procedure that includes general conditions for applicability of each step, that handles all possible contingencies, etc. The
situation can also help to disambiguate an otherwise ambiguous instruction. A number of authors have discussed the advantages of situation-specific knowledge elicitation
(e.g., Davis, 1979; Gruber, 1989).
P2. Situation specification as needed. Although instructions typically apply to the
situation at hand, the instructor is free to specify other situations as needed; for
instance, specifying contingencies using conditional instructions.
P3. Incremental as-needed elicitation. Knowledge is elicited incrementally as a part
of the agent's ongoing performance. Instructions are given when the agent is unable
273

fiHuffman & Laird

to perform a task; thus, they directly address points where the agent's knowledge is
lacking.
P4. Task structuring by instructor. The instructor can structure larger tasks into
smaller subtasks in any way desired. For instance, a task requiring ten primitive steps
may be taught as a simple sequence of the ten steps, or as two subtasks of five steps
each, etc. If the agent does not know what an instructed action or subtask is, or how
to perform it in the situation at hand, it will ask for further instruction.
P5. Knowledge-level interaction. The instructor provides knowledge to the agent at
the knowledge level (Newell, 1981). That is, the instructions refer to objects and
actions in the world, not to symbol-level structures (e.g., data structures) within the
agent. The interaction occurs in natural language, the language that the instructor
uses to talk about the task, rather than requiring artificial terminology and syntax to
specify the agent's internal data and processes.
Tutorial instructions provide knowledge applicable to the agent's current situation or
a closely related one. Thus, this type of instruction is most appropriate for tasks with
a local control structure, in which control decisions are made based on presently available
information. Local control structure is characteristic of constructive synthesis tasks, in
which primitive steps are composed one after another to form a complete solution. Our
work focuses on this type of task.2

3. Requirements on an Instructable Agent
Although easing the instructor's burden in providing knowledge, the properties of tutorial
instruction described above place severe requirements on an instructable agent. In general,
such an agent must solve three conceptually distinct problems: it must (1) comprehend individual instructions to produce behavior, (2) support a exible dialogue with its instructor,
and (3) produce general learning from the interaction. The properties of tutorial instruction described in the previous section place requirements on the solutions to each of these
problems. In what follows, we identify key requirements for each problem in turn.

3.1 Comprehending Instructions: The Mapping Problem

The mapping problem involves comprehending instructions that are given in natural language and transforming the information they contain into the agent's internal representation
2. In contrast, problem solving methods like constraint satisfaction and heuristic classification involve global
control strategies. These strategies either follow a fixed global regime or require an aggregation of
information from multiple problem solving states to make control decisions. It is possible to produce a
global control strategy using a combination of local decisions (Yost & Newell, 1989). However, teaching
a global method by casting it purely as a sequence of local decisions may be dicult. Other types of
instruction, beyond the scope of this work, are required to teach global methods in a natural way. To
acquire knowledge for tasks that involve a known global control strategy, it may be most ecient to use a
method-based knowledge acquisition tool (e.g., Birmingham & Klinker, 1993; Birmingham & Siewiorek,
1989; Eshelman, Ehret, McDermott, & Tan, 1987; Marcus & McDermott, 1989; Musen, 1989) with that
control strategy built in.

274

fiFlexibly Instructable Agents

language. This is required for the agent to apply information communicated by instructions
at the knowledge level (property P5, above) to its internal processing.
Solving the mapping problem in general involves all of the complexities of natural language comprehension. As Just and Carpenter (1976) point out, instructions can be linguistically complex and dicult to interpret independent of the diculty of the task being
instructed. Even in linguistically simple instructions, actions and objects are often incompletely specified, requiring the use of context and domain knowledge to produce a complete
interpretation (Chapman, 1990; DiEugenio & Webber, 1992; Frederking, 1988; Martin &
Firby, 1991).
The general requirement for the mapping problem on a tutorable agent is straightforward:

M1 . A tutorable agent must be able to comprehend and map all aspects of each instruction
that fall within the scope of information it can possibly represent.

The agent cannot be expected to interpret aspects that fall outside its representation abilities (these abilities may be augmented through instruction, but this occurs by building up
from existing abilities). A more detailed analysis could break this general requirement into
a set of more specific ones.
This work has not focused on the mapping problem. Rather, the agent we have implemented uses fairly standard natural language processing techniques to handle instructions
that express a sucient range of actions and situations to demonstrate its other capabilities.
We have concentrated our efforts on the interaction and transfer problems.

3.2 Supporting Interactive Dialogue: The Interaction Problem
The interaction problem is the problem of supporting exible dialogue with an instructor.
The properties of tutorial instruction indicate that this dialogue occurs during the agent's
ongoing performance to address its lacks of knowledge (property P3); within the dialogue,
the agent must handle instructions that apply to different kinds of situations (properties
P1 and P2) and that structure tasks in different ways (property P4).
An instructable agent moves toward solving the interaction problem to the degree that
it supports these properties. In this work, we concentrate on the instructor's utterances
within the dialogue, since exibility for the instructor is the goal. We have not considered
the potential complexity of the agent's utterances (e.g., to give the instructor various kinds
of feedback) in much detail.
The properties of exible interaction can be specified in terms of individual instruction
events, where an instruction event is the utterance of a single instruction at a particular
point in the discourse. To support truly exible dialogue, an instructable agent must be
able to handle any instruction event that is coherent at the current discourse point. Each
instruction event is initiated by either the student or the teacher, and carries knowledge
of some type to be applied to a particular task situation. Thus, a exible tutorable agent
should support instruction events with:

I1. Flexible initiation. Instruction events can be initiated by agent or instructor.
275

fiHuffman & Laird

I2. Flexibility of knowledge content. The knowledge carried by an instruction event
can be any piece of any of the types of knowledge the agent uses that is applicable in
some way within the ongoing task and discourse context.

I3. Situation exibility. An instruction event can apply either to the current task
situation or to some specified hypothetical situation.

The following sections discuss each of these requirements in more detail.
3.2.1 Flexible Initiation

In human tutorial dialogues, initiation of instruction is mixed between student and teacher.
One study indicates that teacher initiation is more prevalent early in instruction; student
initiation increases as the student learns more, and then drops off again as the student
masters the task (Emihovich & Miller, 1988).
Instructor-initiated instruction is dicult to support because instruction events can
interrupt the agent's ongoing processing. Upon interrupting the agent, an instruction event
may alter the agent's knowledge in a way that could change or invalidate the reasoning in
which the agent was previously engaged. Because of these diculties, instructable systems
to date have not fully supported instructor-initiated instruction.3 Likewise, InstructoSoar does not handle instructor-initiated instruction.
Agent-initiated instruction can be directed in (at least) two possible ways: by verification
or by impasses. Some learning apprentice systems, such as LEAP (Mitchell et al., 1990) and
DISCIPLE (Kodratoff & Tecuci, 1987b) ask the instructor to verify or alter each reasoning
step. The advantage of this approach is that each step is examined by the instructor; the
disadvantage, of course, is that each step must be examined. An alternative approach is
to drive instruction requests by impasses in the agent's task performance (Golding, Rosenbloom, & Laird, 1987; Laird, Hucka, Yager, & Tuck, 1990). This is the approach used by
Instructo-Soar. An impasse indicates that the agent's knowledge is lacking and it needs
instruction. The advantage of this approach is that as the agent learns, it becomes more
autonomous; its need for instruction decreases over time. The disadvantage is that not all
lacks of knowledge can be recognized by reaching impasses; e.g., no impasse will occur when
performance is correct but inecient.
3.2.2 Flexibility of Knowledge Content

A exible tutorable agent must handle instruction events involving any knowledge that is
applicable in some way within the ongoing task and discourse context. This requirement is
dicult to meet in general, because of the wide range of knowledge that may be relevant to
any particular situation. It requires a robust ability to relate each utterance to the ongoing
discourse and task situation. No instructable systems have met this requirement fully.
However, we can define a more constrained form of this requirement, limited to instructions that command actions (i.e., imperatives). Imperative commands are especially
prevalent in tutorial instruction of procedures. Supporting exible knowledge content for
3. Some systems have learned purely by observing an expert (e.g., Dent, Boticario, McDermott, Mitchell &
Zabowski, 1992; Redmond, 1992). Observation is a type of instructor-initiatedness, but the instruction
is not an interactive dialogue.

276

fiFlexibly Instructable Agents

commands means allowing the instructor to give any relevant command at any point in the
dialogue for teaching a task. We call this ability command exibility.
For any command that is given, there are three possibilities: (1) the commanded action
is known, and the agent performs it; (2) the commanded action is known, but the agent
does not know how to perform it in the current situation (extra, unknown steps are needed);
or (3) the commanded action is unknown. Thus, command exibility allows the instructor
teaching a procedure to skip steps (2) or to command a subtask that is unknown (3) at
any point. In such cases, the agent asks for further instruction. The interaction pattern
that results, in which procedures are commanded and then taught as needed, has been
observed in human instruction. Wertsch (1979) notes that \...adults spontaneously follow a
communication strategy in which they use directives that children do not understand and
then guide the children through the behaviors necessary to carry out these directives."
Command exibility gives the instructor great exibility in teaching a set of tasks because the instructions can hierarchically structure the tasks in whatever way the instructor
wishes. A mathematical analysis (Huffman, 1994) revealed that the number of possible
sequences of instructions that can be used to teach a given procedure grows exponentially
with the number of actions in the procedure. For a procedure with 6 primitive actions,
there are over 100 possible instruction sequences; for 7, there are over 400.
3.2.3 Situation Flexibility

A exible tutorable agent must handle instructions that apply to either the current task
situation or some hypothetical situation that the instructor specifies. Instructors make
frequent use of both of these options. For instance, analysis of a protocol of a student being
taught to use a ight simulator revealed that 119 out of 508 instructions (23%) involved
hypothetical situations, with the remainder applying to the current situation at the time
they were given.
Instructions that apply to the current situation, such as imperative commands (e.g.,
\Move to the yellow table"), are called implicitly situated (Huffman & Laird, 1992). Since
the instruction itself says nothing about the situation to which it should be applied, the
current situation (the task being performed and the current state) is implied.
In contrast, instructions that specify elements of the situation to which they are meant
to apply are explicitly situated (Huffman & Laird, 1992). The agent is not meant to carry
out these instructions immediately (as an implicitly situated instruction), but rather when a
situation arises that is like the one specified. Examples include conditionals and instructions
with purpose clauses (DiEugenio, 1993), such as the following:4


When using chocolate chips, add them to coconut mixture just before pressing into
pie pan.



To restart this, you can hit R or shift-R.



When you get to the interval that you want, you just center up the joystick again.

4. These examples are taken from a protocol of tutorial instruction and a written source of instruction (a
cookbook).

277

fiHuffman & Laird

As a number of researchers have pointed out (Ford & Thompson, 1986; Haiman, 1978;
Johnson-Laird, 1986), conditional clauses introduce a shared reference between speaker and
hearer that forms an explicit background for interpreting or evaluating the consequent.5
Here, the clauses in italics indicate a hypothetical situation to which the command in the
remainder of the instruction is meant to apply. In most cases, the situation is only partially
specified, with the remainder drawn from the current situation, as in \When using chocolate
chips [and cooking this recipe, and at the current point in the process]..."
In general, a hypothetical situation may be created and referred to across multiple
utterances. The agent presented here handles both implicitly and single explicitly situated
instructions, but does not deal with hypothetical situations that exist through multiple
instructions.

3.3 Producing General Learning: The Transfer Problem

The transfer problem is the problem of learning generally applicable knowledge from instructions, that will transfer to appropriate situations in the future. This general learning
is based on instructions that apply to specific situations (property P1, above). Many types
of knowledge may be learned, since instructions can provide any type of knowledge that the
agent is lacking (property P3).
Solving this problem involves more than simply memorizing instructions for future use;
rather, conditions for applying each instruction must be determined from the situation.
Consider, for example, the following exchange between instructor and agent:
Block open our oce door.
How do I do that?
Pick up a red block.
Now, drop it here, next to the door.
What are the proper conditions for performing the \pick up" action? Simple memorization yields poor learning; e.g., whenever blocking open an office door, pick up a
red block. However, the block's color, and even the fact that it is a block, are irrelevant in this case. Rather, the fact that the block weighs (say) more than five pounds,
giving it enough friction with the oor to hold open the door, is crucial. Thus, the proper
learning might be:
If trying to block open a door, and
there is an object obj that is can be picked up, and
obj weighs more than 5 pounds
then propose picking up obj.
Here, the original instruction is both generalized (color red and isa block drop out) and
specialized (weight > 5 is added).
The transfer problem places a number of demands on a tutorable agent:
T1. General learning from specific cases. The agent is instructed in a particular
situation, but is expected to learn general knowledge that will apply in suciently
similar situations.
5. Some types of conditionals do not follow this pattern (Akatsuka, 1986), but they are not relevant to
tutorial instruction.

278

fiFlexibly Instructable Agents

T2. Fast learning. An instructable agent is expected to learn new procedures quickly.
T3.

T4.
T5.

T6.

Typically, a task should only have to be taught once.
Maximal use of prior knowledge. An agent must apply its prior knowledge in
learning from instruction. This is a maxim for machine learning systems in general (if
you have knowledge, use it), and is particularly relevant for learning from instruction
because learning is expected to happen quickly.
Incremental learning. The agent must be able to continually increase in knowledge
through instruction. New knowledge must be smoothly integrated with the agent's
existing knowledge as a part of its ongoing performance.
Knowledge-type exibility. Since any type of knowledge (e.g., control knowledge,
causal knowledge, etc.) might be communicated by instructions, a exible tutorable
agent must be able to learn each type of knowledge it uses. We make this a testable
criterion below by laying out the types of knowledge in an agent based on a particular
computational model.
Dealing with incorrect knowledge. The agent's knowledge is clearly incomplete
(otherwise, it would not need instruction); it may also be incorrect. A general tutorable agent must be able to perform and learn effectively despite incorrect knowledge.

T7. Learning from instruction coexisting with learning from other sources.

In addition to instruction, a complete agent should be able to learn from other
sources of knowledge that are available. These might include learning from observation/demonstrations, experimentation in the environment, analogy, etc.
The theory of learning from tutorial instruction presented here focuses on extending
incomplete knowledge through instruction { requirements T1 through T5 of this list. Handling incorrect knowledge (T6) and learning from other sources (T7) are planned extensions
in progress.
Table 1 summarizes the requirements that must be met by an instructable agent to support exible tutorial instruction, and indicates the requirements targeted by InstructoSoar. We have made two simplifications in using the requirements to evaluate InstructoSoar. First, we treat each requirement as binary; that is, as if either completely met or
unmet. In reality, some requirements could be broken into finer-grained pieces to be evaluated separately. Second, we treat each requirement independently. The table indicates
Instructo-Soar's performance on each requirement alone, but does not account for potential interactions between them. These interactions can be complex; for instance, in
pursuing fast learning (T2), an agent may sacrifice good general learning (T1) because it
bases its generalizations on too few examples. We have not addressed such tradeoffs in our
evaluation of Instructo-Soar.

4. Related Work

Although there has not been extensive research on agents that learn from tutorial instruction per se, learning from instruction-like input has been a long-time goal in AI (Carbonell,
279

fiHuffman & Laird

Problem Requirement
Mapping M1 Mapping of all representable
information
Interaction I1 Flexible initiation of instruction
I2 Flexibility of instructional knowledge
content
I3 Situation exibility
 implicitly situated
 explicitly situated single utterance
 explicitly situated multiple utterance
Transfer T1 General learning from specific cases

T2
T3
T4
T5
T6
T7

Instructo-Soar?

no

(as needed to show
other capabilities)
no
(only agent-initiated)
partial (command exibility)

partial
yes
yes
no
yes
(via situated explanation)
Fast learning
yes
(new procedures only
taught once)
Maximal use of prior knowledge
yes
Incremental learning
yes
Knowledge-type exibility
yes
(learns all PSCM
knowledge types)
Ability to deal with incorrect knowledge no
(only extending incomplete knowledge)
Learning from instruction coexisting no
(not demonstrated)
with learning from other sources

Table 1: The requirements on a exible tutorable agent, and Instructo-Soar's performance on them.
Michalski, & Mitchell, 1983; McCarthy, 1968; Rychener, 1983). Early non-interactive systems learned declarative, ontological knowledge from language (Haas & Hendrix, 1983; Lindsay, 1963), simple tasks from unsituated descriptions (Lewis, Newell, & Polk, 1989; Simon,
1977; Simon & Hayes, 1976), and task heuristics from non-operational advice (Hayes-Roth,
Klahr, & Mostow, 1981; Mostow, 1983).
Other work has concentrated on behaving based on interactive natural language instructions. SHRDLU (Winograd, 1972) performed natural language commands and did a small
amount of rote learning { e.g., learning new goal specifications by directly transforming
sentences into state descriptions. More recent systems that act in response to language
(concentrating on the mapping problem) but do only minimal learning include SONJA
(Chapman, 1990), AnimNL (DiEugenio & Webber, 1992), and Homer (Vere & Bickmore,
1990).
Some recent work has focused more on learning from situated natural language instructions. Martin and Firby (1991) discuss an approach to interpreting and learning from
elliptical instructions (e.g., \Use the shovel") by matching the instruction to expectations
generated from a task execution failure. Alterman et al.'s FLOBN (Alterman, Zito-Wolf, &
Carpenter, 1991; Carpenter & Alterman, 1994) searches for instructions in its environment
280

fiFlexibly Instructable Agents

in order to operate devices. FLOBN learns by relating a device's instructions to known
procedures for operating similar devices. These systems do not target learning from exible
interactive instructions or types of instructions other than imperatives, however.
The bulk of work on learning from instruction-like input has been under the rubric
of learning apprentice systems (LASs), and closely related programming-by-demonstration
(PbD) systems (Cypher, 1993) { as employed, for instance, in recent work on learning
within software agents (Dent et al., 1992; Maes, 1994; Maes & Kozierok, 1993; Mitchell,
Caruana, Freitag, McDermott, & Zabowski, 1994). These systems learn by interacting with
an expert; either observing the expert solving problems (Cypher, 1993; Donoho & Wilkins,
1994; Mitchell et al., 1990; Redmond, 1992; Segre, 1987; VanLehn, 1987; Wilkins, 1990),
or attempting to solve problems and allowing the expert to guide and critique decisions
that are made (Golding et al., 1987; Gruber, 1989; Kodratoff & Tecuci, 1987b; Laird
et al., 1990; Porter, Bareiss, & Holte, 1990; Porter & Kibler, 1986). Each LAS has learned
particular types of knowledge: e.g., operator implementations (Mitchell et al., 1990), goal
decomposition rules (Kodratoff & Tecuci, 1987b), operational versions of functional goals
(Segre, 1987), control knowledge and control features (Gruber, 1989), procedure schemas (a
combination of goal decomposition and control knowledge) (VanLehn, 1987), useful macrooperations (Cypher, 1993), heuristic classification knowledge (Porter et al., 1990; Wilkins,
1990), etc.
Tutorial instruction is a more exible type of instruction than that supported by past
LASs, for three reasons. First, the instructor may command unknown tasks or tasks with
unachieved preconditions to the agent at any instruction point (command exibility). Past
LASs limit input to particular commands/observations at particular times (e.g., only commanding or observing directly executable actions) and typically do not allow unknown
commands at all. Second, tutorial instruction allows the use of explicitly situated instructions (situation exibility), to teach about contingencies that may not be present in the
current situation; past LASs do not. Third, tutorial instruction requires that all types of
task knowledge can be learned (knowledge-type exibility). Past LASs learn only a subset
of the types of knowledge they use to perform tasks.

5. A Theory of Learning from Tutorial Instruction

Our theory of learning from tutorial instruction consists of a learning framework, situated
explanation, placed within a computational model for general agenthood, the problem space
computational model. We first describe the computational model and then the learning
framework.

5.1 The Problem Space Computational Model

A computational model (CM) is a \a set of operations on entities that can be interpreted
in computational terms" (Newell et al., 1990, p. 6). A computational model for a general
instructable agent must meet two requirements:
1. Support of general computation/agenthood.
2. Close correspondence to the knowledge level. Because tutorial instructions
provide knowledge at the knowledge level (Newell, 1981), the further the CM com281

fiHuffman & Laird

ponents are from the knowledge level, the more dicult mapping and learning from
instructions will be. In addition, a close correspondence to the knowledge level will
allow us to use the CM to identify the types of knowledge the agent uses.
Many potential CMs are ruled out by these requirements. Standard programming languages (e.g., Lisp) and theoretical CMs like Turing machines and push-down automata
support general computation, but their operations and constructs are at the symbol level,
without direct correspondence to the knowledge level. Similarly, connectionist and neural
network models of computation (e.g., Rumelhart & McClelland, 1986) employ (by design)
computational operations and entities at a level far below the knowledge level. Thus, these
models are not appropriate as the top-level CM for an instructable agent. However, because higher levels of description of a computational system are implemented by lower levels
(Newell, 1990), these CMs might be used as the implementation substrate for the higher
level CM of an instructable agent.
Another alternative is logic, which has entities that are well matched to the knowledge
level (e.g., propositions, well-formed formulas). Logics specify the set of legal operations
(e.g., modus ponens), thus defining the space of what can possibly be computed. However,
logic provides no notion of what should be computed; that is, logics alone do not specify the
control of the logical operations' application. It is desirable that the CM of an instructable
agent include control knowledge, because control knowledge is a crucial type of knowledge
for general agenthood, that can be communicated by instructions.
Since one of our goals is to identify an agent's knowledge types, it might appear that
selecting a theory of knowledge representation would be more appropriate than selecting
a computational model. Such theories define the functions and structures used to represent knowledge (e.g., KL-ONE, Brachman, 1980); some also define the possible content of
those structures (e.g., conceptual dependency theory, Schank, 1975; CYC, Guha & Lenat,
1990). However, computational structure must be added to these theories to produce working agents. Thus, rather than an alternative to specifying a computational model, a theory
of knowledge representation is an addition. A content theory of knowledge representation would provide a more fine-grained breakdown of the knowledge to be learned by an
instructable agent within each category of knowledge specified by its CM. We have not
employed a particular content theory in this work thus far, however.
The computational model adopted here is called the problem space computational model
(PSCM) (Newell et al., 1990; Yost, 1993). The PSCM is a general formulation of computation in a knowledge-level agent, and many applications have been built within it (Rosenbloom, Laird, & Newell, 1993a). It specifies an agent in terms of computation within
problem spaces, without reference to the symbol level structures used for implementation.
Because its components approximate the knowledge level (Newell et al., 1990), the PSCM
is an apt choice for identifying an agent's knowledge types. Soar (Laird, Newell, & Rosenbloom, 1987) is a symbol level implementation of the PSCM.
A schematic of a PSCM agent is shown in Figure 2. Perception and motor modules
connect the agent to the external environment. A PSCM agent reaches a goal by moving
through a sequence of states in a problem space. It progresses toward its goals by sequentially
applying operators to the current state. Operators transform the state, and may produce
motor commands. In PSCM, operators can be more powerful than simple STRIPS operators
(Fikes, Hart, & Nilsson, 1972), because they can perform arbitrary computation (e.g., they
282

fiFlexibly Instructable Agents

external environment

perceptual modules

motor modules

Figure 2: The processing of a PSCM-based agent. Triangles represent problem spaces;
squares, states; arrows, operators; and ovals, impasses.
can include conditional effects, multiple substeps, reactivity to different situations, etc.).
The PSCM agent reaches an impasse when its immediately available knowledge is not
sucient either to select or fully apply an operator. When this occurs, another problem
space context { a subgoal { is created, with the goal of resolving the impasse. This second
context may impasse as well, causing a third context to arise, and so on.
The only computational entities in the PSCM mediated by the agent's knowledge are
states and operators. There are a small set of basic PSCM-level operations on these entities
that the agent performs:
1. State inference. Simple monotonic inferences that are always to be applied can be
made without using a PSCM operator. Such inferences augment the agent's representation of the state it is in by inferring state properties based on other state properties
(including those delivered by perception). For instance, an agent might know that a
block is held if its gripper is closed and positioned directly above the block.
2. Operator selection. The agent must select an operator to apply, given the current
state. This process involves two types of knowledge:
2.1. Proposal knowledge: Indicates operators deemed appropriate for the current situation. This knowledge is similar to \precondition" knowledge in simple STRIPS
operators.
2.2. Control knowledge: Orders proposed operators; e.g., \A is better than B"; \C is
best"; \D is rejected."
283

fiHuffman & Laird

3. Operator application. Once selected, an operator may be applied directly, or
indirectly via a subgoal:
3.1. Operator effects. The operator is applied directly in the current problem space.
The agent has knowledge of the effects of the operator on the state and motor
commands produced (if any).
3.2. Sub-operator selection. The operator is applied by reaching an impasse and selecting operators in a subgoal. Here, knowledge to apply the operator is selection
knowledge (2, above) for the sub-operators.
4. Operator termination. An operator must be terminated when its application has
been completed. The termination conditions, or goal concept (Mitchell et al., 1986),
of an operator indicate the state conditions that the operator is meant to achieve. For
example, the termination conditions of pick-up(blk) might be that blk is held and
the arm is raised.6
Each of these functions is performed by the agent using knowledge; thus, they define the set
of knowledge types present within a PSCM agent. The knowledge types (five types total)
are summarized in Table 2. Because Soar is an implementation of the PSCM, all knowledge
within Soar agents is of these types.
In Soar's implementation of the PSCM, learning occurs whenever results are returned
from a subgoal to resolve impasses. The learning process, called chunking, creates rules
(called chunks) that summarize the processing in the subgoal leading to the creation of the
result. Depending on the type of result, chunks may correspond to any of the five types of
PSCM knowledge. When similar situations arise in the future, chunks allow the impasse
that caused the original subgoal to be avoided by producing their results directly. Chunking
is a form of explanation-based learning (Rosenbloom & Laird, 1986). Although it is a
summarization mechanism, through taking both inductive and deductive steps in subgoals,
chunking can produce both inductive and deductive learning (Miller, 1993; Rosenbloom
& Aasman, 1990). Chunking occurs continuously, making learning a part of the ongoing
activity of a Soar/PSCM agent.
The PSCM clarifies the task of an instructable agent: it must be able to learn each of
the five types of PSCM knowledge from instruction. The next section discusses the learning
process itself.

5.2 Learning from Instructions through Situated Explanation

Learning from instruction involves both analytic learning (learning based on prior knowledge) and inductive learning (going beyond prior knowledge). Analytic learning is needed
because the agent must learn from instructions that combine known elements { e.g., learning to pick up objects by combining known steps to pick up a particular object. The agent's
prior knowledge of these elements can be used to produce better and faster learning. Inductive learning is needed because the agent must learn new task goals and domain knowledge
6. PSCM operators have explicit termination knowledge because they can have a string of conditional
effects that take place over time, they can respond to (or wait for) the external environment, etc.
STRIPS operators, in contrast, do not need explicit termination knowledge, because they are defined by
a single list of effects, and are \terminated" by definition after applying those effects.

284

fiFlexibly Instructable Agents

Entity Knowledge type Example
state
inference
If gripper is closed & directly above obj ! holding obj.
operator proposal
If goal is to pick up obj on table-x, and not docked at tablex, then propose moving to table-x.
operator control
If goal is to pick up small metal obj on table-x, prefer moving to table-x over fetching magnet.
operator effects
An effect of the operator move to table-x is that the robot
becomes docked at table-x.
operator termination
Termination conditions of pick up obj are that the gripper
is raised & holding obj.
Table 2: The five types of knowledge of PSCM agents.
that are beyond the scope of its prior knowledge. The goal of this research is not to produce
more powerful analytic or inductive techniques, but rather to specify how these techniques
come together to produce a variety of learning in the variety of instructional situations faced
by an instructable agent. The resulting approach is called situated explanation.
Instruction requirements T1 through T3 specify that general learning (T1) must occur
from single, specific examples (T2), by making maximal use of prior knowledge (T3). These
requirements bode strongly for a learning approach based on explanation. The use of
explanation to produce general learning has been a common theme in machine learning (e.g.,
DeJong & Mooney, 1986; Fikes et al., 1972; Minton, Carbonell, Knoblock, Kuokka, Etzioni,
& Gil, 1989; Rosenbloom, Laird, & Newell, 1988; Schank & Leake, 1989; many others) and
cognitive science (Anderson, 1983; Chi, Bassok, Lewis, Reimann, & Glaser, 1989; Lewis,
1988; Rosenbloom & Newell, 1986). Forming explanations enables general learning from
specific cases (requirement T1) because the explanation indicates which features of a case
are important and which can be generalized. Learning by explaining typically requires only
a single example (requirement T2) because the prior knowledge employed to construct the
explanation (requirement T3) provides a strong bias that allows this fast learning.
Thus, we use an explanation-based method as the core of our learning from instruction approach, and fall back on inductive methods when explanation fails. In standard
explanation-based learning, explaining a reasoning step involves forming a \proof" (using
prior knowledge) that the step leads from the current state of reasoning toward the current
goal. The proof is a path of reasoning from the current state to the goal, through the step
being explained, as diagrammed in Figure 3. General learning is produced by forming a
rule that includes only the causally required features of the state, goal, and step appearing
in the proof; features that do not appear are generalized away.
Figure 3 indicates the three key elements of an explanation: the step being explained,
the endpoints of the explanation (a state S and goal G to be reached), and the other steps
required to complete the explanation. What form do these elements of an explanation take
for situated explanation of an instruction?


Step to be explained. In situated explanation, the step to be explained is an individual
instruction given to the agent.
285

fiHuffman & Laird

reasoning step to be
explained
(indicated by instruction I)

other steps, from agents knowledge

...

S

G

(Mk )

Figure 3: Caricature of an explanation of how a reasoning step applies to a situation starting
in a state S , with a goal G to be achieved.





Alternatively, an entire instruction episode { e.g., the full sequence of instructions for
a new procedure { could be explained at once. Applying explanation to single steps
results in knowledge applicable at each step (as in Golding et al., 1987; Laird et al.,
1990); explaining full sequences of reasoning steps results in learning schemas that
encode the whole reasoning episode (as in Mooney, 1990; Schank & Leake, 1989; VanLehn, 1987). Learning factored pieces of knowledge rather than monolithic schemas
allows more reactive behavior, since knowledge is accessed locally based on the current situation (Drummond, 1989; Laird & Rosenbloom, 1990). This meshes with the
PSCM's local control structure. Explaining individual instructions is also supported
by psychological results on the self-explanation effect, which have shown that subjects
who self-explain instructional examples do so by re-deriving individual lines of the example. \Students virtually never reect on the overall solution and try to recognize
a plan that spans all the lines" (VanLehn and Jones, 1991, p. 111).
Endpoints of explanation. The endpoints of the explanation { a state S and a goal G
to be achieved { correspond to the situation that the instruction applies to. Situation
exibility (requirement I3 ) stipulates that this situation may be either the current
state of the world and goal being pursued or some hypothetical situation that is specified explicitly in the instruction. An instruction that does not specify any situational
features is implicitly situated, and applies to the agent's current situation. Alternatively, an instruction can specify features of S or G, making for two kinds of explicitly
situated instructions. For example, \If the light is on, push the button" indicates a
hypothetical state with a light on; \To turn on the machine, ip the switch" indicates
a hypothetical goal of turning on the machine. A situation [S; G] is produced for
each instruction, based on the current task situation and any situation features the
instruction specifies.
Other required steps. To complete an explanation of an instruction, an agent must
bring its prior knowledge to bear to complete the path through the instruction to
achievement of the situation goal. A PSCM agent's knowledge applies to its current
situation to select and apply operators and to make inferences. When explaining an
instruction I , this knowledge is applied internally to the situation [S; G] associated
with I . That is, explanation takes the form of forward internal projection within
that situation. As depicted in Figure 3, the agent \imagines" itself in state S , and
then runs forward, applying the instructed step and any knowledge that it has about
subsequent states/operators. This knowledge includes both what is normally used
286

fiFlexibly Instructable Agents

in the external world and knowledge of operators' expected effects that is used to
produce those effects in the projected world. If G is reached within the projection,
then the projected path from S , through the step instructed by I , to G comprises
an explanation of I . By indicating the features of I , S , and G causally required for
success, the explanation allows the agent to learn general knowledge from I (as in
standard EBL, realized in our agent by Soar's chunking mechanism, Rosenbloom &
Laird, 1986). However, the agent's prior knowledge may be insucient, causing an
incomplete explanation, as described further below.
Combining these elements produces an approach to learning from tutorial instruction
that is conceptually quite simple. For each instruction I that is received, the agent first
determines what situation I is meant to apply to, and then attempts to explain why the
step indicated by I leads to goal achievement in that situation (or prohibits it, for negative
instructions). If an explanation can be made, it produces general learning of some knowledge
IK by indicating the key features of the situation and instruction that cause success.
If an explanation cannot be completed, it indicates that the agent is missing one or
more pieces of prior knowledge MK (of any PSCM type) needed to explain the instruction.
Missing knowledge (in Figure 3, missing arrows) causes an incomplete explanation by precluding achievement of G in the projection. For instance, the agent may not know a key
effect of an operator, or a crucial state inference, needed to reach G. More radically, the
action commanded by I may be completely unknown and thus inexplicable.
As shown in Figure 4, there are four general options a learning agent might follow when
it cannot complete an explanation. (O1) It could delay the explanation until later, in the
hope that the missing knowledge (MK ) will be learned in the meantime. Alternatively,
(O2-O3) it could try to complete the explanation now by somehow learning the missing
knowledge. The missing knowledge could be learned (O2) inductively (e.g., by inducing
over the \gap" in the explanation, as described by VanLehn, Jones & Chi, 1992, and many
others), or, (O3) in an instructable agent's case, through further instruction. Finally, (O4) it
could abandon the explanation altogether and try to learn the desired knowledge in another
way instead.
Given only an incomplete explanation, it would be dicult to choose which option to
follow. Identifying the missing knowledge MK in the general case is a dicult credit assignment problem (with no algorithmic solution), and there is nothing in the incomplete
explanation itself that predicts whether MK will be learned later if the explanation is delayed. Thus, past machine learning systems have responded to incomplete explanations
either in only a single way, or in multiple ways, but that are tried in a fixed sequence.
Many authors (Bergadano & Giordana, 1988; Hall, 1988; VanLehn, 1987; VanLehn, Jones,
& Chi, 1992; Widmer, 1989), for instance, describe systems that make inductions to complete incomplete explanations (option O2). Because of the diculty of determining missing
knowledge, these systems either base their induction on multiple examples, and/or bias the
induction with an underlying theory or a teacher's help. SIERRA (VanLehn, 1987), for
example, induces over multiple partially explained examples, and constrains the induction
by requiring that each of the examples is unexplainable because of the same piece of missing knowledge (the same disjunct, in SIERRA's terminology). SWALE (Schank & Leake,
1989) uses an underlying theory of \anomalies" in explanations to complete incomplete explanations of events. OCCAM (Pazzani, 1991b) uses options O2 and O4 in a static order:
287

fiHuffman & Laird

delay explanation until Mk is learned

O1

after delay:
M

instruction
context
S

I

M

S

G

incomplete explanation
(missing knowledge Mk)

I

M

k

G

learn Mk inductively to complete explanation

O2
k

k

?

induce

I

S

G
M

k

learn Mk from instruction to complete explanation

O3

S

M

I

S

k

G

further I

G

abandon explanation (learn another way)

O4

S

I

M

k

G

Figure 4: Options when faced with an incomplete explanation because of missing knowledge
MK .
It first attempts to fill in the gaps in an incomplete explanation inductively, biased by a
naive theory; if that fails, it abandons explanation and falls back on correlational learning
methods. PET (Porter & Kibler, 1986) is an example of a system that delays explanation
of a reasoning step until it learns further knowledge (option O1).
However, as indicated in Figure 4, an instructable agent has additional information
available to it besides the incomplete explanation itself. Namely, the instructional context
(that is, the type of instruction and its place within the dialogue) often indicates which
option is most appropriate for a given incomplete explanation. Thus, situated explanation includes all four of the options and dynamically selects between them based on the
instructional context. For a situated explanation of an instruction I in a situation [S; G],
where missing knowledge MK precludes completing the explanation to learn knowledge IK ,
options O1-O4 take the following form:

O1. Delay the explanation until later. The instructional context can indicate a likelihood that the missing knowledge MK will be learned later. For instance, an instruction
I given in teaching a new procedure cannot be immediately explained because the re-

maining steps of the procedure are unknown, but they will be known later (assuming
the instructor completes teaching the procedure). In such cases, the agent discards its
current, incomplete explanation and simply memorizes I 's use in [S; G] (rote learning). Later, after MK is learned, I is recalled and explained in [S; G], causing IK to
be learned.
288

fiFlexibly Instructable Agents

Given instruction I from which knowledge IK can be learned:
 Determine the situation [S; G] (current or hypothetical) to which I applies
I
 Explain I in [S; G] by forward projecting from S !;    ; G
! Success (G met): learn IK from the complete explanation ( EBL).
! Failure: missing knowledge MK . Options:
O1. Delay explanation until later.
O2. Induce MK , completing the explanation.
O3. Take instruction to learn MK , completing the explanation.
O4. Abandon explanation; instead, learn IK inductively.
Table 3: Situated explanation.

O2. Induce MK , completing the explanation. In some cases, the instructional context
localizes the missing knowledge MK to be part of a particular operator. For instance,

a purpose clause instruction (\To do X, do Y") suggests that the single operator Y
should cause X to occur. Because this localization tightly constrains the \gap" in
the incomplete explanation, the agent can use heuristics to induce a strong guess at
the MK needed to span the gap. Inducing MK allows the explanation of I to be
completed and IK to be learned.
O3. Take instruction to learn MK , completing the explanation. The default response of the agent (when the other options are not deemed appropriate) is to ask
the instructor to explain I further. The further instruction can teach the agent MK .
Again, learning MK allows the explanation of I to be completed and IK to be learned.
O4. Abandon the explanation and learn IK in another way. The instructional
context can indicate that the missing knowledge MK would be very dicult to learn.
This occurs when either the instructor refuses to give further information when asked
to, or when the agent has projected multiple operators that may be missing pieces
of knowledge (multiple potential MK s). Since it is unknown whether MK will ever
be acquired, the agent abandons its explanation of I altogether. Instead, it attempts
to learn IK directly (using inductive heuristics), without an explanation to base the
learning on.
These options will be made clearer through examples presented in the following sections.
Situated explanation is summarized in Table 3. Unlike some knowledge acquisition approaches, it does not include an explicit check for consistency when newly learned knowledge
is added to the agent's knowledge base. As Kodratoff and Tecuci (1987a) point out, techniques like situated explanation are biased toward consistency because they only acquire
new knowledge when current knowledge is insucient, and they use current knowledge when
deriving new knowledge. However, in some domains, explicit consistency checks (such as
those used by Wilkins' (1990) ODYSSEUS) may be required.
Situated explanation meets the requirement that learning be incremental (T4) because
it occurs during the ongoing processing of the agent and adds new pieces of knowledge to
289

fiHuffman & Laird

T1
T2
T3
T4
T5

I2
I3

General learning from specific cases
Fast learning (each task instructed only once)
Maximal use of prior knowledge
Incremental learning
Knowledge-type exibility
a. state inference
b. operator proposal
c. operator control
d. operator effects
e. operator termination
Command exibility
a. known command
b. skipped steps
c. unknown command
Situation exibility
a. implicitly situated
b. explicitly situated: hypothetical state
hypothetical goal

Table 4: Expanded requirements of tutorial instruction met by Instructo-Soar.
the agent's memory in a modular way. The local control structure of the PSCM allows new
knowledge to be added independent of current knowledge. If there is a conict between
pieces of knowledge (for example, proposing two different operators in the same situation),
an impasse will arise that can be reasoned about or resolved with further instruction.

6.

Instructo-Soar

Instructo-Soar is an instructable agent built within Soar { and thus, the PSCM { that
uses situated explanation to learn from tutorial instruction.7 Instructo-Soar engages

in an interactive dialogue with its instructor, receiving natural language instructions and
learning to perform tasks and extend its knowledge of the domain. This section and the
next describe how Instructo-Soar meets the targeted requirements of tutorial instruction,
which are shown in expanded form in Table 4. This section describes the system's basic
performance when learning new procedures, and extending procedures to new situations,
from imperative commands (implicitly situated instructions); the next describes learning
other types of knowledge and handling explicitly situated instructions.

7. For an overview of Soar, and other systems built within it, see (Rosenbloom, Laird, & Newell, 1993b).

290

fiFlexibly Instructable Agents

Figure 5: The robotic domain to which Instructo-Soar has been applied.

6.1 The Domain and the Agent's Initial Knowledge
The primary domain to which Instructo-Soar has been applied is the simulated robotic
world shown in Figure 5.8 The agent is a simulated Hero robot, in a room with tables,
buttons, blocks of different sizes and materials, an electromagnet, and a light. The magnet
is toggled by closing the gripper around it. A red button toggles the light on or off; a green
button toggles it dim or bright, when it is on.
Instructo-Soar consists of a set of problem spaces within Soar that contain three
main categories of knowledge: natural language processing knowledge, originally developed
for NL-Soar (Lewis, 1993); knowledge about obtaining and using instruction; and knowledge of the task domain itself. This task knowledge is extended through learning from
instruction. Instructo-Soar does not expand its natural language capabilities per se as
it takes instruction, although it does learn how sentences map onto new operators that it
learns. It has complete, noiseless perception of its world, and can recognize a set of basic
object properties (e.g., type, color, size) and relationships (e.g., robot docked-at table,
8. The techniques have also been applied in a limited way to a ight domain (Pearson, Huffman, Willis,
Laird, & Jones, 1993), in which Soar controls a ight simulator and instructions are given for taking off.

291

fiHuffman & Laird

Pick up the red block.
Move to the yellow table.
Move the arm above the red block.
Move up.
Move down.
Close the hand.
Move up.
The operator is finished.

Figure 6: Instructions given to Instructo-Soar to teach it to pick up a block.
gripper holding object, objects above, directly-above, left-of, right-of one another).
The set of properties and relations can be extended through instruction, as described below.
The agent begins with knowledge of a set of primitive operators to which it can map
natural language sentences, and can execute. These include moving to tables, opening and
closing the hand, and moving the arm up, down, and above, left of, or right of things.
The agent can also internally project these operators. However, some of their effects under
various conditions are unknown. For instance, the agent does not know which operators
affect the light or magnet, or that the magnet will attract metal objects. Also, the agent
begins with no knowledge of complex operators (that involve combinations of primitive
operators), such as picking up or arranging objects, pushing buttons, etc.

6.2 Learning New Procedures through Delayed Explanation
Instructo-Soar learns new procedures (PSCM operators) from instructions like those

shown in Figure 6, for picking up a block. Since \pick up" is not a known procedure
initially, when told to \Pick up the red block," the agent realizes that it must learn a new
operator.
To perform a PSCM operator, the operator must be selected, implemented, and terminated. To select the operator in the future based on a command requires knowledge of
the operator's argument structure (a template), and how natural language maps to this
structure. Thus, to learn a new operator, the agent must learn four things:
1. Template: Knowledge of the operator's arguments and how they can be instantiated.
For picking up blocks, the agent acquires a new operator with a single argument, the
object to be picked up.
2. Mapping from natural language: A mapping from natural language semantic
structures to an instantiation of the new operator, so that the operator can be selected
when commanded in the future. For picking up blocks, the agent learns to map the
semantic object of \Pick up ..." to the single argument of its new operator template.
3. Implementation: How to perform the operator. New operators are performed by
executing a sequence of smaller operators. The implementation takes the form of
selection knowledge for these sub-operators (e.g., move to the proper table, move the
arm, etc.)
292

fiFlexibly Instructable Agents

4. Termination conditions: Knowledge to recognize when the new operator is achieved
{ the goal concept of the new operator. For \pick up," the termination conditions
include holding the desired block, with the arm raised.
Requirement T2 (\fast learning") stipulates that after the first execution of a new procedure, the agent must be able to perform at least the same task without being re-instructed.
Thus, the agent must learn, in some form, each of the four parts of a new operator during
its first execution.
A general implementation of a new operator can be learned through situated explanation
of each of its steps. During the first execution of a new operator, though, the instructions
for performing it cannot be explained, because the agent does not yet know the goal of the
operator (e.g., the agent does not know the termination conditions of \pick up") or the
steps following the current one to reach that goal. However, in this instructional context
{ explaining instructed steps of a procedure being learned { it is clear that the missing
knowledge of the remaining steps and the procedure's goal will be acquired later, because
the instructor is expected to teach the procedure to completion. Thus, the agent delays
explanation (option O1) and for now memorizes each implementation instruction in a rote,
episodic form. At the end of the first execution of a new procedure, the agent induces the
procedure's goal { its termination conditions { using a set of simple inductive heuristics.
On later executions of the procedure, the original instructions are recalled and explained to
learn a general implementation.
We describe the details of this process using the \pick up" example.
6.2.1 First Execution

The example, shown in Figure 6, begins with the instruction \Pick up the red block." The
agent comprehends this instruction, producing a semantic structure and resolving \the red
block" to a block in its environment. However, the semantic structure does not correspond
to any known operator, indicating that the agent must learn a new operator (which it
calls, say, new-op14). To learn a template for the new operator, the agent simply assumes
that the argument structure of the command used to request the operator is the required
argument structure of the operator itself. In this case, a template for the new operator is
generated with an argument structure that directly corresponds to the semantic arguments
of the \pick up" command (here, one argument, object). The agent learns a mapping from
the semantic structure to the new operator's template, to be used when presented with
similar requests in the future. This simple approach to learning templates and mappings is
sucient for imperative sentences with direct arguments, but will fail for commands with
complex arguments, such as path constraints (\Move the dynamite into the other room,
keeping it as far from the heater as possible").
Next, the new operator is selected for execution. Since its implementation is unknown,
the agent immediately reaches an impasse and asks for further instructions. Each instruction
in Figure 6 is given, comprehended and executed in turn. These instructions provide the
implementation for the new operator. They are implicitly situated { each applies to the
current situation in which the agent finds itself.
At any point, the agent may be given another command that cannot be directly completed { one that requests either another unknown procedure or a known procedure that
293

fiHuffman & Laird

instruction being
explained

other steps

...

S

?
?
?

G?

Figure 7: Instructions teaching a new operator cannot be explained before the termination
conditions of the new operator are learned.
the agent does not know how to perform in the current situation due to skipped steps. This
is command exibility (requirement I2). For example, within the instructions for \pick up,"
the command \Move above the red block" cannot be completed because of a skipped step
(the arm must be raised to move above something). An impasse arises where the instructor
indicates the needed step (\move up"), and then continues instructing \pick up."
Ultimately, the implementation of a new operator can be learned at the proper level of
generality by explaining each instructed step. However, as illustrated in Figure 7, during
its initial execution forming this explanation is impossible, because the goal of the new
operator and the other steps (further instructions) needed to reach it are not yet known.
Since these missing pieces of the explanation are expected to be available later, the agent
delays explanation and resorts to rote learning of each instructed step.
In Instructo-Soar, rote learning occurs as a side effect of language comprehension.
While reading each sentence, the agent learns a set of rules that encode the sentence's
semantic features. The rules allow NL-Soar to resolve referents in later sentences, implementing a simple version of Grosz's focus space mechanism (Grosz, 1977). The rules record
each instruction, indexed by the goal to which it applies and its place in the instruction
sequence. The result is essentially an episodic case that records the specific, lock-step sequence of the instructions given to perform the new operator. For instance, it is recorded
that \to pick-up (that is, new-op14) the red block, rb1, I was first told to move to the yellow table, yt1." Of course, the information contained within the case could be generalized,
but at this point any generalization would be purely heuristic, because the agent cannot
explain the steps of the episode. Thus, Instructo-Soar takes the conservative approach
of leaving the case in rote form.
Finally, the agent is told \The operator is finished," indicating that the goal of the
new operator has been achieved. This instruction triggers the agent to learn termination
conditions for the new operator. Learning termination conditions is an inductive concept
formation problem: The agent must induce which features of those that hold in the current
state imply a positive instance of the new operator's goal being achieved. Standard concept
learning approaches may be used here, as long as they produce a strong hypothesis within a
small number of examples (due to the \fast learning" requirement, T2). Instructo-Soar
uses a simple heuristic to strongly bias its induction: It hypothesizes that everything that
has changed between the initial state when the new operator was requested and the current
state is part of the new operator's termination conditions. In this case, the changes are that
the robot is docked at a table, holding a block, and the block and gripper are both up in
the air.
294

fiFlexibly Instructable Agents

This heuristic gives a reasonable guess, but is clearly too simple. Conditions that
changed may not matter; e.g., perhaps it doesn't matter to picking up blocks that the
robot ends up at a table. Unchanged conditions may matter; e.g., if learning to build
a \stoplight," block colors are important although they do not change. Thus, the agent
presents the induced set of termination conditions to the instructor for possible alteration
and verification. The instructor can add or remove conditions. For example, in the \pick
up" case the instructor might say \The robot need not be docked at the yellow table" to
remove a condition deemed unnecessary, before verifying the termination conditions.
Instructo-Soar performs induction by EBL (chunking) over an overgeneral theory
that can make inductive leaps (similar to, e.g., Miller, 1993; Rosenbloom & Aasman, 1990;
VanLehn, Ball, & Kowalski, 1990). This type of inductive learning has the advantage that
the agent can alter the bias to reect other available knowledge. In this case, the agent
uses further instruction (the instructor's indications of features to add or remove) to alter
the induction. Other knowledge sources that could be employed (but are not in the current
implementation) include analogy to other known operators (e.g., pick up actions in other
domains), domain-specific heuristics, etc.
Through the first execution of a new operator, then, the agent:
 Carries out a sequence of instructions achieving a new operator.
 Learns an operator template for the new operator.
 Learns the mapping from natural language to the new operator.
 Learns a rote execution sequence for the new operator.
 Learns the termination conditions of the new operator.
Since the agent has learned all of the necessary parts of an operator, it will be able to
perform the same task again without instruction. However, since the implementation of the
operator is rote, it can only perform the exact same task. It has not learned generally how
to pick up things yet.
6.2.2 Generalizing the New Operator's Implementation

The agent now knows the goal concept and full (though rote) implementation sequence for
the new operator. Thus, it has the information that it needs to explain how each instruction
in the implementation sequence leads to goal achievement, provided its underlying domain
knowledge is sucient.
Each instruction is explained by recalling it from episodic memory and internally projecting its effects and the rest of the path to achievement of the termination conditions
of the new operator. The projection is a \proof" that the instructed operator will lead
to goal achievement in the situation. Soar's chunking mechanism essentially computes the
weakest preconditions of the situation and the instruction required for success (similar to
standard EBL) to form a general rule proposing the instructed operator. The rule learned
from the instruction \Move to the yellow table" is shown in Figure 8. The rule generalizes
the original instruction by dropping the table's color, and specializes it by adding the facts
that the table has the object sitting on it and that the object is small (only small objects
295

fiHuffman & Laird

If

the goal is new-op-14(?obj), and
?obj is on table ?t, and small(?obj), and
the robot is not docked at ?t, and
the gripper has status(open),
then propose operator move-to-table(?t).
Figure 8: The general operator proposal rule learned from the instruction \Move to the
yellow table" (new-op-14 is the newly learned \pick up" operator).
can be grasped by the gripper). The rule also tests that the gripper is open, because this
condition was important for grasping the block in the instructed case.9
After learning general proposal rules for each step in the instruction sequence, the agent
can perform the task without reference to the rote case. For instance, if asked to \Pick
up the green block," the agent selects new-op14, instantiated with the green block. Then,
general sub-operator proposal rules like the one in Figure 8 fire one by one, as they match the
current situation, to implement the operator. After performing all of the implementation
steps, the agent recognizes that the termination conditions are met (the gripper is raised
and holding the green block), and new-op14 is terminated.
Since the general proposal rules for implementing the task are directly conditional on
the state, the agent can perform the task starting from any state along the implementation
path and can react to unexpected conditions (e.g., another robot stealing the block). In
contrast, the rote implementation that was initially learned applied only when starting from
the original starting state, and was not reactive because its steps were not conditional on
the current state.

6.3 Recall Strategies

We have described how our agent recalls and explains each step of a new operator's implementation sequence, after the operator's termination conditions are induced. There are
still two open issues: (A) At what point after learning the termination conditions should
the agent perform the recall/projection?, and (B) How many steps should be recalled and
projected in sequence at a time?
To investigate these issues, we have implemented two different recall/project strategies:
1. Immediate/complete recall. The agent recalls and attempts to explain the full
sequence of instructions for the new operator immediately after learning the new
operator's termination conditions.
2. Lazy/single-step recall. The agent recalls and attempts to explain single instructions in the sequence when asked to perform the operator again starting from the
same initial state. That is, at each point in the execution of the operator, the agent
9. More technical details of how Soar's chunking mechanism forms such rules can be found in (Huffman,
1994; Laird, Congdon, Altmann, & Doorenbos, 1993).

296

fiFlexibly Instructable Agents

recalls the next instruction, and attempts to explain it by forward projecting it. However, if this projection does not result in a path to goal achievement without any
further instructions being recalled, then rather than recalling the next instruction in
the sequence to continue the forward projection, the agent gives up on explaining this
instruction and simply executes it in the external world.
These strategies represent the extremes of a continuum of strategies.10 The strategy to
use is a parameter of the agent; it does not dynamically select between strategies while it
is running. A possible extension would be to reason about the time pressure in different
situations to select the appropriate strategy. Next, we briey describe the implications of
each recall strategy.
6.3.1 Immediate/Complete Recall Strategy

Immediate/complete recall and explanation involves internally projecting multiple operators
(the full instruction sequence) immediately after the first execution of the new operator. The
projection begins at the state the agent was in when the new operator was first suggested.
If the projection successfully achieves the termination conditions of the new operator, the
agent learns general implementation rules for every step. The advantage of this strategy is
that the agent learns a general implementation for the new operator immediately after its
first execution (e.g., the agent can pick up other objects right away).
The strategy has three important disadvantages. First, it requires that the agent reconstruct the initial state in which it was commanded to perform the new operator. This
reconstruction may be dicult if the amount of information in the state is large (although
it is not in the small robotic domain being used here).
Second, recall and projection of the entire sequence of instructed steps is time-consuming,
requiring time proportional to the length of the instruction sequence. During the process,
the agent's performance of tasks at hand is suspended. This suspension could be awkward
if the agent is under pressure to act quickly.
Third, as illustrated in Figure 9, multiple step projections are susceptible to compounding of errors in underlying domain knowledge. The projection of each successive operator
begins from a state that reects the agent's knowledge of the effects of prior operators in
the sequence. If this knowledge is incomplete or incorrect, the state will move further and
further from reecting the actual effects of prior operators. Minor domain knowledge problems in the knowledge of individual operators, that alone would not produce an error in
a single step explanation, may combine within the projection to cause an error. This can
lead to incomplete explanations or (more rarely) to spuriously successful explanations (e.g.,
reaching success too early in the instruction sequence).
6.3.2 Lazy/Single-Step Recall Strategy

In the lazy/single-step recall strategy, the agent waits to recall and explain instructions until
asked to perform the new operator a second time from the same initial state. In addition,
the agent only recalls a single instruction to internally project at a time. After the recalled
10. We also implemented a lazy/complete recall strategy, which will not be described here (see Huffman,
1994, for details).

297

fiHuffman & Laird

opX
op1

S1

op1

op2

S2
S2

op3

S3

...

G tc

opX

Internally projected path
(reflecting agents incorrect
knowledge of operator effects)

Sx

states reflecting
actual operator effects

Sx

states reflecting
projected operator effects

G tc

state that meets the
termination conditions
for the current goal

op2

S3
op3

... Sn (= G )
tc

Correct path (reflecting
actual operator effects)

Figure 9: Multiple step projections can result in incomplete explanations due to compounding of errors in domain knowledge.
operator is projected, the agent applies whatever general knowledge it has about the rest
of the implementation of the new operator. This general knowledge, however, does not
include rote memories of other past instructions. That is, if the agent does not know the
rest of the path to complete the new operator using general knowledge, it does not recall any
further instructions in the sequence from its rote memories. Rather, the internal projection
is terminated and the single recalled operator is applied in the external world.
This strategy addresses the three disadvantages of the immediate/complete strategy.
First, it does not require reconstruction of the original instruction state; rather, it waits for
a similar state to occur again.
Second, recalling and projecting a single instruction at a time does not require a timeconsuming introspection that suspends the agent's ongoing activity. For \pick up," for
instance, Table 5 shows the longest time that the agent's external action (movements or
instruction requests) is suspended using each strategy (as measured in Soar decision cycles,
which last about 35 milliseconds each for Instructo-Soar on an SGI R4400 Indigo).
The immediate/complete strategy does no external actions for 304 decision cycles (about
11 seconds on our Indigo) immediately following the first execution, in order to recall
and explain the complete instruction sequence. Using the lazy/single-step strategy, only
one instruction is ever recalled/explained at a time before action is taken in the world;
thus, the longest time without action is only 75 decision cycles (about 2 seconds). The
total recall/explanation time is proportional to the length of the instruction sequence in
both cases (304 vs. 294 decision cycles), but in the lazy/single-step strategy, that time
is interleaved with the execution of the instructions rather than fully taken after the first
execution.
Third, the lazy/single-step strategy overcomes the problem of compounding of domain
theory errors by beginning the projection of each instruction from the current state of the
world after external execution of the previous instructions. Thus, the beginning state of
each projection correctly reects the effects of the previous operators in the implementation
sequence.
The major disadvantage of this strategy is that it requires a number of executions of
the new operator equal to the length of the instruction sequence in order to learn the whole
298

fiFlexibly Instructable Agents

Immediate/complete Lazy/single-step
Largest time without external action 304
75
Largest total recall/explanation time 304 (end of 1st exec'n) 294 (during 2nd exec'n.)
during an execution

Decision cycles (log scale)

Table 5: Timing comparison, in Soar decision cycles, for learning \pick up" using the immediate/complete and lazy/single-step recall strategies.
1000

(r = 0.98)
(r = 0.98)

1000

100

100

10
1

10

Execution number (log scale), "pick up"
(a)

1

10

Execution number (log scale), "move left of"
(b)

Figure 10: Decision cycles versus execution number to learn to (a) pick up and (b) move
objects left of one another, using the lazy/single-step strategy.
general implementation. This is because limiting recall to a single step allows only a single
sub-operator per execution to be generalized. This disadvantage, however, leads to two
interesting learning characteristics:
 Back-to-front generalization. Generalized learning starts at the end of the implementation sequence and moves towards the beginning. On the second execution of the
new operator, a path to the goal is known only for the last instruction in the sequence
(it leads directly to goal completion), so a general proposal for that instruction is
learned. On the third execution, after the second to last instruction is projected, the
proposal learned previously for the last operator applies, leading to goal achievement
and allowing a general proposal for the second to last instruction to be learned. This
pattern continues back through the entire sequence until the full implementation is
learned generally. As Figure 10 shows, the resulting learning curves closely approximate the power law of practice (Rosenbloom & Newell, 1986) (r = 0:98 for both (a)
and (b)).
 Effectiveness of hierarchical instruction. Due to the back-to-front effect, the
agent learns a new procedure more quickly when its steps are taught using a hierarchical organization than when they are taught as a at sequence. Figure 11 depicts a
at, nine-step instruction sequence for teaching Instructo-Soar to move one block
299

fiHuffman & Laird

1
move left of(block, block2)

6

4

2

move arm down

move to table (table)
3

graphical

10

moveleftof(arm,block2)

7

5

move above (block)

Figure 11: A

8

move arm up

close gripper

view

move arm down

move to table (table)

of

a

move-left-of(block,block2).

open gripper

9

at

instruction

sequence

for

1
move left of(block, block2)

2

11

9
moveleftof(arm,block2)

pick up (block)

put down (block)

10
move to table (table)
3

4
move to table (table)

8

6

5
move above (block)

Figure 12: A

12
move arm up

grasp (block)

graphical

move arm down

13
open gripper

7
move arm down

view

close gripper

of

a hierarchical instruction sequence
move-left-of(block,block2). New operators are shown in bold.

for

left of another; Figure 12 depicts a hierarchical instruction sequence for the same procedure, that contains 13 instructed steps, but a maximum of 3 in any subsequence. By
breaking the instruction sequence into shorter subsequences, a hierarchical organization allows multiple subtrees of the hierarchy to be generalized during each execution.
General learning for an N step operator takes N executions using
a at instruction
HpN subtasks in each
sequence. Taught hierarchically
as
an
H
-level
hierarchy
with
p
subsequence, only H  H N executions are required for full generalization. The hierarchy in Figure 12 has an irregular structure, but results in apspeedup because the
length of every subsequence is small (in this case, smaller than N ). Empirically, the
at sequence of Figure 11 takes nine (N ) executions to generalize, whereas the hierarchical sequence takes only six. Hierarchical organization has the additional advantage
that more operators are learned that can be used in future instructions.
300

fiFlexibly Instructable Agents

6.4 Supporting Command Flexibility

Command exibility (requirement I2 ) stipulates that the instructor may request either an
unknown procedure, or a known procedure that the agent does not know how to perform in
the current state (skipping steps), at any point. This can lead to multiple levels of embedded
instruction. As we have seen, Instructo-Soar learns completely new procedures from
instructions for unknown commands. In addition, when the agent is asked to perform a
known procedure in an unfamiliar situation { one from which the agent does not know what
step to take { it learns to extend its knowledge of the procedure to that situation.
An example is contained in the instructions for \Pick up the red block," when the agent
is asked to \Move above the red block." The agent knows how to perform the operator
when its arm is raised. However, in this case the arm is lowered, and so the agent reaches an
impasse and asks for further instruction.11 When told to \Move up," the agent internally
projects raising its arm, which allows it to achieve moving above the red block. From this
projection it learns the general rule: move the arm up when trying to move above an object
that is on the table the agent is docked at. This rule extends the \move above" procedure
to cover this situation.
Any operator { even one previously learned from instruction { may require extension to
apply to a new situation. This is because when the agent learns the general implementation
for a new operator, it does not reason about all possible situations in which the operator
might be performed, but limits its explanations to the series of situations that arises during
the actual execution of the new operator while it is being learned.
Newly learned operators may be included in the instructions for later operators, leading
to learning of operator hierarchies. One hierarchy of operators learned by Instructo-Soar
is shown in Figure 13. Learning procedural hierarchies has been identified as a fundamental
component of children's skill acquisition from tutorial instruction (Wood, Bruner, & Ross,
1976). In learning the hierarchy of Figure 13, Instructo-Soar learned four new operators, an extension of a known operator (move above), and an extension of a new operator
(extending \pick up" to work if the robot already is holding a block). Because of command
exibility, this same hierarchy can be taught in exponentially many different ways (Huffman, 1994). For instance, new operators that appear as sub-operators (e.g., grasp) can be
taught either before or during teaching of higher operators (e.g., pick up).

6.5 Abandoning Explanation when Domain Knowledge is Incomplete

All of the general operator implementation learning described thus far depends on explaining
instructions using prior domain knowledge (as opposed to the learning of operator termination conditions, which is inductive). What if the domain knowledge is incomplete, making
explanation impossible? For sequences of multiple operators, pinpointing what knowledge
is missing is an extremely dicult credit assignment problem (sequences known to contain
only one operator, however, are a more constrained case, as described in the next section).
11. Another option would be to search; i.e., to apply a weak method such as means-ends analysis. In this
example, the search would be easy; in other cases, it could be costly. In any event, since the goal of
Instructo-Soar is to investigate the use of instruction, our agent always asks for instructions when
it reaches an impasse in task performance. Nothing in Instructo-Soar precludes the use of search or
knowledge from other sources, however.

301

fiHuffman & Laird

lineup(block1,block2,block3)

move left of(block2,block1)

pick up (block)

move arm down

moveleftof(arm,block1)

move to table (table)

grasp (block)

put down (blockX)

move left of(block3,block2)

put down (block)

move arm up

(lg. metal)

open gripper
grasp (magnet)

move above (block)

move arm down

(small)

move to table (table)

move above (blk/mag)

move arm down

close gripper

move arm up

Figure 13: A hierarchy of operators learned by Instructo-Soar. Primitive operators are
in light print; learned operators are in bold.
In general, an explanation failure that is detected at the end of the projection of an instruction sequence could be caused by missing knowledge about any operator in the sequence.
Thus, when faced with an incomplete explanation of a sequence of multiple instructions,
Instructo-Soar abandons the explanation and instead tries to induce knowledge directly
from the instructions (option O4).
As an example, consider a case in which all of Instructo-Soar's knowledge of secondary operator effects (frame axiom type knowledge) is removed before teaching it a procedure. For example, although the agent knows that closing the hand causes it to have
status closed, it no longer knows that closing the hand around a block causes the block
to be held. Now, the agent is taught a new procedure, such as to pick up the red block.
After the first execution, the agent attempts to recall and explain the instructions as usual,
but fails because of the missing knowledge. That is, the block is not picked up during the
projection of the instructions, since the agent's knowledge does not indicate that it is held.
The agent records the fact that this procedure's instructions cannot be explained.
Later, the agent is again asked to perform the procedure, and again recalls the instructions. However, it also recalls that explaining the instructions failed in the past. Thus, it
abandons explanation and instead attempts to induce a general proposal rule directly from
each instruction.12
12. Since an incomplete explanation for a procedure may indicate that some effect(s) of an operator in the
instruction sequence is unknown, another alternative (not yet implemented in Instructo-Soar) would
be for the agent to observe the effects of each operator in the sequence as it is performed, comparing
the observations to the effects predicted by domain knowledge. Any differences would allow the agent

302

fiFlexibly Instructable Agents

G: newop14 ("pick up")
object: <redblock>

<redblock> ON <yellowtable>

OP: movetotable
destination: <yellowtable>

Figure 14: The use of the OP-to-G-path heuristic, with OP \move to the yellow table,"
and G \pick up the red block."
In the \pick up" example, the agent first recalls the command to move to the yellow
table. To learn a proposal rule for this operator (call it OP ), the agent must induce a set of
conditions of the state under which performing OP will contribute to achieving the \pick
up" goal (call it G). Instructo-Soar uses two simple heuristics to induce these state
conditions:
 OP-to-G-path. For each object Obj 1 filling a slot of OP , and each object Obj 2
attached to G, include the shortest existing path (heuristically of length less than
three) of relationships between Obj 1 and Obj 2 in the set of induced conditions.
This heuristic captures the intuition that if an operator involves some object, its
relationship to the objects relevant to the goal is probably important. Figure 14
shows its operation for \move to the yellow table." As the figure indicates, there is a
path between G's object, the red block, and the destination of OP , the yellow table,
through the relationship that the block is on the table.
 OP-features-unachieved. Each termination condition (essentially, each primary
effect) of OP that is not achieved in the state before OP is performed is considered
an important condition.
This heuristic captures the intuition that all of the primary effects of OP are probably
important; therefore, it matters that they are not achieved when OP is selected. In
our example, OP 's primary effect is that the robot ends up docked at the table; thus,
the fact that the robot is not initially docked at the table is added to the inferred set
of conditions for proposing OP .
These heuristics are implemented as Soar operators that compute the appropriate conditions. Once a set of conditions is induced, it is presented to the instructor, who can add or
remove conditions before verifying them. Upon verification, a rule is learned proposing OP
(e.g., move-to-table(?t)) when the induced conditions hold (e.g., goal is pick-up(?b),
?b isa block, on(?b,?t)). This rule is similar to the rule learned from explanation (Figure 8), but only applies to picking up a block (overspecific), and does not stipulate that the
to learn new operator effects that could complete the explanation of the procedure. Learning effects
of operators from observation has been explored by a number of researchers (Carbonell & Gil, 1987;
Pazzani, 1991b; Shen, 1993; Sutton & Pinette, 1985; Thrun & Mitchell, 1993).

303

fiHuffman & Laird

object must be small (overgeneral). A similar induction occurs for each step of \pick up,"
so that the agent learns a general implementation for the full \pick up" operator. However,
unless corrections are made by the instructor, this induced implementation is not as correct
as one learned from explanation; for instance, it applies (wrongly) to any block instead of
to any small object. In a more complex domain, inferring implementation rules would be
even less successful. Not surprisingly, psychological research shows that human subjects'
learning from procedural instructions also degrades if they lack domain knowledge (Kieras
& Bovair, 1984).
Returning to the targeted instruction requirements in Table 4, Instructo-Soar's learning of procedures illustrates (T1) general learning from specific instructions, (T2) fast learning (because each procedure need only be instructed once) by (T3) using prior domain
knowledge to construct explanations, and (T4) incremental learning during the agent's ongoing performance. Two types of PSCM knowledge are learned: (T5(b)) operator proposals
for sub-operators of the procedure, and (T5(e)) the procedure's termination conditions.
The learning involves either delayed explanation, or when domain knowledge is inadequate,
abandoning explanation in favor of simple induction. The instructions are each (I3 (a)) implicitly situated imperative commands, for either (I2(a)) known procedures, (I2(b)) known
procedures where steps have been skipped, or (I2(c)) unknown procedures.

7. Beyond Imperative Commands

Next, we turn to learning the remaining types of PSCM knowledge (T5(a,c,d)) from various
kinds of explicitly situated instructions (I3(b)). From an explicitly situated instruction,
Instructo-Soar constructs a hypothetical situation (goal and state) that includes the
objects, properties, and relationships mentioned explicitly in the instruction as well as
any features of the current situation that are needed to carry out the instruction.13 This
hypothetical situation is used as the context for a situated explanation of the instruction.

7.1 Hypothetical Goals and Learning Effects of Operators

A goal is explicitly specified in an instruction by a purpose clause (DiEugenio, 1993): \To
do X, do Y." The basic knowledge to be learned from such an instruction is an operator
proposal rule for doing Y when the goal is to achieve X.
Consider this example from Instructo-Soar's domain:
> To turn on the light, push the red button.

The agent has been taught how to push buttons, but does not know the red button's
effect on the light. From a purpose clause instruction like this example, the agent creates a
hypothetical situation with the goal stated in the purpose clause (here, \turn on the light"),
and a state like the current state, but with that goal not achieved (here, with the light off).
Within this situation, the agent attempts to explain the instruction by forward projecting
the action of pushing the red button.
If the agent knew that pushing the red button toggles the light, then in the projection,
the light would come on. Thus, the explanation would succeed, and a general operator
13. See (Huffman, 1994) for details of how these features are determined.

304

fiFlexibly Instructable Agents

proposal rule would be learned, that proposed pushing the red button when the light is off
and the goal is to turn it on.
However, since in actuality the agent is missing the knowledge (MK ) that pushing the
button affects the light, the light does not come on within the projection. The explanation
is incomplete.
When Instructo-Soar's explanation of a sequence of operators fails, the agent does
not try to induce the missing knowledge needed to complete the explanation, because it
could be associated with any of the multiple operators. Rather, the explanation is simply
abandoned, as described in Section 6.5. However, in this case, the unexplainable sequence
contains only one operator. In addition, the form of the instruction gives the agent a
strong expectation about that operator's intended effect. Based on the purpose clause, the
agent expects that the specified action (pushing the button) will cause achievement of the
specified goal (turning on the light). DiEugenio (1993) found empirically that this type of
expectation holds for 95% of naturally occurring purpose clauses.
The expectation constrains the \gap" in the incomplete explanation: the state after
pushing the button should be a state with the light on, and only one action was performed to
produce this effect. Based on this constrained gap, the agent attempts to induce the missing
knowledge MK in order to complete the explanation (option O2). The most straightforward
inference of MK is simply that an unknown effect of the single action is to produce the
expected goal conditions { e.g., pushing the button should cause the light to come on. The
instructor is asked to verify this inference.14
Once it is verified, Instructo-Soar heuristically guesses at the state conditions under
which the effect will occur. It uses the OP-to-G-path heuristic as a very naive causality
theory (Pazzani, 1991a) to guess at the causes of the inferred operator effect. Here, OP-toG-path notices that the light and the red button are both on the same table. In addition,
the agent includes the fact that the inferred effect did not hold (the light was off) before
the operator caused it. The result is presented to the instructor:
I think that doing push the button causes:
the light to be on
under the following conditions:
the light is not currently on, the light is on the table, the button is on the table
Are those the right conditions?
Here, the heuristics have not recognized that it matters which button is pushed (the red
one). The instructor can add this condition by saying ``The button must be red.''
Once the instructor verifies the conditions, the agent adds the new piece of operator effect
knowledge to its memory:
if

projecting push-button(?b), and
?l isa light with status off, on table ?t, and
?b isa button with color red, on table ?t,
then light ?l now has status on.

14. If the inference is rejected, the agent abandons the explanation and directly induces a proposal rule for
pushing the button from the instruction, as described in Section 6.5.

305

fiHuffman & Laird

Immediately after being learned, this rule applies to the light in the forward projection for
the current instruction. The light comes on, completing the instruction's explanation by
achieving its goal. From this explanation, the agent learns the proposal rule that proposes
pushing the red button when the goal is to turn on the light. Thus, the agent has acquired
new knowledge at multiple levels; inferring an unknown effect of an operator supported
learning a proposal for that operator.
This example illustrates (I3 (b)) the use of hypothetical goal instructions and the use
of option O2 for dealing with incomplete explanations { inferring missing knowledge { to
learn new operator effects (T5(d)), thus extending domain knowledge.

7.2 Hypothetical States to Learn About Contingencies

Instructors use instructions with hypothetical states (e.g., conditionals: \If [state conditions], do ...") either to teach general policies (\If the lights are on when you leave the
room, turn them off.") or to teach contingencies when performing a task. InstructoSoar handles both of these; here, we will describe the latter.
A contingency instruction indicates a course of action to be followed when the current
task is performed in a future situation different from the current situation. Instructors
often use contingency instructions to teach about situations that differ from the current
one in some crucial way that should alter the agent's behavior. Contingency instructions
are very common in human instruction; Ford and Thompson (1986) found that 79% of the
conditional statements in an instruction manual communicated contingency options to the
student.
Consider this interaction:
> Grasp the blue block.

That's a new one for me. How do I do that?

> If the blue block is metal, then pick up the magnet.

The blue block is not made of metal, but the instructor is communicating that if it were, a
different course of action would be required.
From the conditional instruction \If the blue block is metal, then pick up the magnet,"
the agent needs to learn an operator proposal rule for picking up the magnet under appropriate conditions. The agent begins by constructing the hypothetical situation to which
\pick up the magnet" applies. \If the blue block is metal" indicates a hypothetical state
that is a variant of the current state with the blue block having material metal. The
current goal (\Grasp the blue block") is also the goal in the hypothetical situation.
Within this situation, the agent projects picking up the magnet to explain how it will
allow the block to be grasped. However, the agent is missing much of the knowledge needed
to complete this explanation. It does not know the goal concept of \Grasp" yet, or the rest
of the instructions to reach that goal.
Since the instruction being explained is for a contingency, the rest of the instructions
that the agent is given to \Grasp the blue block" may not (and in this case, do not) apply
to the contingent situation, where the block is metal. In the normal grasp sequence, for
instance, the agent learns to close its hand around the grasped object, but when grasping
a metal object, the hand is closed around the magnet. Since knowledge of how to complete
306

fiFlexibly Instructable Agents

grasping a metal object is needed to explain the contingency instruction, and the agent
does not know when it might learn this missing knowledge, it abandons the explanation
(option O4). Instead, it uses the heuristics described in Section 6.5 to directly induce an
operator proposal rule for \Grasp the magnet." In addition to the conditions generated by
the heuristics, the conditions indicated in the antecedent of the instruction are included.
The result is presented to the instructor for alteration and verification:
So I'm guessing the conditions for doing \pick up the magnet"
when your goal is \grasp the block" are:
the block is metal
Is that right?
> Right.

From this interaction the agent learns a rule that proposes picking up the magnet when the
goal is to grasp a metal block. After this learning is completed, since the agent has not yet
finished grasping the blue block, it continues to receive instruction for that task. Further
contingencies can be indicated at any point. Learning contingencies illustrates (I3(b)) the
handling of hypothetical state instructions.

7.3 Learning to Reject Operators

Our final examples illustrate learning to reject an operator { a type of operator control
knowledge in the PSCM. The examples also detail the remaining option for dealing with
incomplete explanations: (O3) completing an explanation through further instruction.
Consider these instructions:
> Never grasp green blocks.

Why?
(a) > Trust me.
(b) > Green blocks are explosive.
A negative imperative prohibits a step from applying to a hypothetical situation in which
it might apply. Thus, Instructo-Soar creates a hypothetical situation in which the
prohibited action might be executed; in this case, a state with a graspable green block.
Since no goal is specified by the instruction, and there is no other current goal, a default
goal of \maintaining happiness" (which is always considered one of the agent's current goals)
is used. From this hypothetical situation, the agent internally projects the \grasp" action,
expecting an \unhappy" result. However, the resulting state, in which the agent is grasping
a green block, is acceptable according to the agent's knowledge. Thus, the projection does
not explain why the action is prohibited.
The agent deals with the incomplete explanation by asking for further instruction, in an
attempt to learn MK and complete the explanation. However, the instructor can decline
to give further information by saying (a) Trust me. Although the instructor will not provide MK , because the prohibition of a single operator (grasping the green block) is being
explained, the agent can induce a plausible MK that will complete the explanation (option
O2). Since the agent knows that the final state after the prohibited operator is meant to be
\unhappy", it simply induces that this state is to be avoided. This is the converse of learning to recognize when a desired goal has been reached (learning an operator's termination
307

fiHuffman & Laird

conditions). The agent conservatively guesses that all of the features of the hypothetical
state (here, that there is a green block that is held), taken together, make it a state to
be avoided. Because this inference is so conservative, in the current implementation the
instructor is not even asked to verify it. The state inference rule that results is as follows:
if

goal is ``happiness'', and
?b isa block with color green, and
holding(gripper,?b),
then this state fails to achieve ``happiness''.

This rule applies to the final state in the projection of \Never grasp..." The state's failure
to achieve happiness completes the agent's explanation of why it should \Never grasp...,"
and it learns a rule that rejects any proposed operator for grasping a green block.
Alternatively, the instructor could provide further instruction, as in (b) Green blocks
are explosive. Such instruction can provide the missing knowledge MK needed to complete an incomplete explanation (option O3). From (b), the agent learns a state inference
rule: blocks with color green have explosiveness high. Instructo-Soar learns state
inferences from simple statements like (b), and from conditionals (e.g., \If the magnet is
powered and directly above a metal block, then the magnet is stuck to the block") by essentially translating the utterance directly into a rule.15 Such state inference instructions
can be used to introduce new features that extend the agent's representation vocabulary
(e.g, stuck-to).
The rule learned from \Green blocks are explosive" adds explosiveness high to the
block that the agent had simulated grasping in the hypothetical situation. The agent knows
that touching an explosive object may cause an explosion { a negative result. This negative
result completes the explanation of \Never grasp...," and from it the agent learns to avoid
grasping objects with explosiveness high.
Completing an explanation through further instruction (as in (b)) can produce more
general learning than heuristically inferring missing knowledge (as in (a)). In (b), if the
agent is later told Blue blocks are explosive, it will avoid grasping them as well. In
general, multiple levels of instruction can lead to higher quality learning than a single level
because learning is based on an explanation composed from strong lower-level knowledge
(MK ) rather than inductive heuristics alone. MK (here, the state inference rule) is also
available for future use.
Because the agent has learned not only to reject the \grasp" operator but to recognize
the bad state that performing it would lead to, the agent can recognize the bad state if it
is reached from another path. For instance, the agent can be led through the individual
steps of grasping an explosive block without the instructor ever mentioning \grasp." When
the agent is finally asked to \Close the gripper" around the explosive object, it does so,
but then immediately recognizes the undesirable state it has arrived in and reverses the
close-gripper action. In the process, it learns to reject close-gripper if the hand is
around an explosive object, so that in the future it will not reach the undesirable state
through this path.
15. This translation occurs by chunking, but in an uninteresting way. Instructo-Soar does not use explanation to learn state inferences. An extension would be to try to explain why an inference holds using
a deeper causal theory.

308

fiFlexibly Instructable Agents

Notice here the effect of the situated nature of Instructo-Soar's learning. The agent
learns to avoid operators that lead to a bad state only when they arise in the agent's
performance. Its initial learning about the bad state is recognitional rather than predictive.
Alternatively, when the agent first learns about a bad state, it could do extensive reasoning
to determine every possible operator that could lead to that state, from every possible
previous state, to learn to reject those operators at the appropriate times. This unsituated
reasoning would be very expensive; the agent would have to reason through a huge number
of possible situations. In addition, whenever new operators were learned, the agent would
have to reason about all the possible situations in which they could arise, to learn if they
could ever led to a bad state. Rather than this costly reasoning, Instructo-Soar simply
learns what it can from its situations as they arise.
Another alternative for completely avoiding bad states would be to think through the
effects of every action before taking it, to see if a bad state will result. This highly cautious
execution strategy would be appropriate in dangerous situations, but is not appropriate
in safer situations where the agent is under time pressure. (Moving between more or less
cautious execution strategies is not currently implemented in Instructo-Soar.)
The \Never grasp..." examples have illustrated the agent's learning of one type of
operator control knowledge, namely operator rejection (T5(c)), learning of state inferences
(T5(a)), and the use of further instruction to complete incomplete explanations (option O3).
The final category of learning we will discuss is a second type of operator control knowledge.

7.4 Learning Operator Comparison Knowledge
Another type of control knowledge besides operator rejection rules is operator comparison
rules, which compare two operators and express a preference for one over the other in a given
situation. Instructo-Soar learns operator comparison rules by asking for the instructor's
feedback when multiple operators are proposed at the same point to achieve a particular
goal. Multiple operators can be proposed, for instance, when the agent has been taught two
different methods for achieving the same goal (e.g., to pick up a metal block either using
the magnet or directly with the gripper). The instructor is asked to either select one of
the proposed operators or to indicate that some other action is appropriate. Selecting one
of the proposed choices causes the agent to learn a rule that prefers the selected operator
over the other proposed operators in situations like the current situation. Alternatively,
if the instructor indicates some other operator outside of the set of proposed operators,
Instructo-Soar attempts to explain that operator in the usual way, to learn a general
rule proposing it. In addition, the agent learns rules preferring the instructed operator to
each of the other currently proposed operators.
There are two weaknesses to Instructo-Soar's learning of operator comparison rules.
First, the instructor can be required to indicate a preference for each step needed to complete a procedure, rather than simply choosing between overall methods. That is, the
instructor cannot say \Use the method where you grab the block with your gripper, instead
of using the magnet," but must indicate a preference for each individual step of the method
employing the gripper. This is because in the PSCM, knowledge about steps in a procedure
is accessed independently, as separate proposal rules, rather than as an aggregate method.
Independent access improves exibility and reactivity { the agent can combine steps from
309

fiHuffman & Laird

different methods as needed based on the current situation { but a higher level grouping of
steps would simplify instruction for selecting between complete methods.
The second weakness is that although the agent uses situated explanation to explain the
selection the instructor makes, it does not explain why that selection is better than the other
possibilities. Preferences between viable operators are often based on global considerations;
e.g., \Prefer actions that lead to overall faster/cheaper goal achievement." Learning based
on this type of global preference (which in turn may be learned through instruction) is a
point for further research.

8. Discussion of Results
We have shown how Instructo-Soar learns from various kinds of instructions. Although
the domain used to demonstrate this behavior is simple, it has enough complexity to exhibit
a variety of the different types of instructional interactions that occur in tutorial instruction.
Of the 11 requirements that tutorial instruction places on an instructable agent (listed in
Table 1), Instructo-Soar meets 7 (listed in expanded form in Table 4) either fully or partially. Three of these in particular distinguish Instructo-Soar from previous instructable
systems:






Command exibility: The instructor can give a command for any task at each
instruction point, whether or not the agent knows the task or how to perform it in
the current situation.

Situation exibility: The agent can learn from both implicitly situated instructions
and explicitly situated instructions specifying either hypothetical goals or states.

Knowledge-type exibility: The agent is able to learn each of the types of knowledge it uses in task performance (the five PSCM types) from instruction.

Earlier, we claimed that handling tutorial instruction's exibility requires a breadth of
learning and interaction capabilities. Combining command, situation, and knowledge-type
exibility, Instructo-Soar displays 18 distinct instructional capabilities, as listed in Table 6. This variety of instructional behavior does not require 18 different learning techniques,
but arises as one general technique, situated explanation in a PSCM-based agent, is applied
in a range of instructional situations.
Our series of examples has illustrated how situated explanation uses an instruction's
situation and context during the learning process. First, the situation to which an instruction applies provides the endpoints for attempting to explain the instruction. Second, the
instructional context can indicate which option to follow when an explanation cannot be
completed. The context of learning a new procedure indicates that delaying explanation
(option O1) is best, since the full procedure will eventually be taught. If a step cannot be
explained in a previously taught procedure, missing knowledge could be anywhere in the
procedure, so it is best to abandon explanation (option O4) and learn another way. Instructions that provide an explicit context, such as through a purpose clause, localize missing
knowledge by giving strong expectations about a single operator that should achieve a single
goal. This localization makes it plausible to induce missing knowledge and complete the
310

fiFlexibly Instructable Agents

1.
2.
3.
4.
5.
6.
7.

Instructional capability
Learning completely new procedures
Extending a procedure to apply in a new situation
Hierarchical instruction: handling instructions for a
procedure embedded in instruction for others
Altering induced knowledge based on further
instruction
Learning procedures inductively when domain
knowledge is incomplete
Learning to avoid prohibited actions
More general learning due to further instruction

8. Learning to avoid indirect achievement of a bad
state
9. Inferences from simple specific statements
10. Inferences from simple generic statements
11. Inferences from conditionals
12. Learning an operator to perform for a hypothetical
goal
13. Learning an operator to perform in a hypothetical
state: general policy (active at all times)
14. Learning an operator to perform in a hypothetical
state: contingency within a particular procedure
15. Learning operator effects
16. Learning non-perceivable operator effects and associated inferences to recognize them
17. Learning control knowledge: learning which of a set
of operators to prefer
18. Learning control knowledge: learning operators are
indifferent

Example
pick up

move up to move above
teaching pick up within line

removing docked-at from pick
up's termination conditions
learning with secondary operator
effects knowledge removed
\Never grasp red blocks."
Avoid grasping because \Red
blocks are explosive."
closing hand around explosive
block
\The grey block is metal."
\White magnets are powered."
\if condition [and condition]*
then concluded state feature"
\To turn on the light, push the
red button."
\If the light is bright, then dim
the light."
\If the block is metal, then grasp
the magnet" to pick up
pushing the red button turns on
the light
the magnet becomes stuck-to a
metal block when moved above it
two ways to grasp a small metal
block
two ways to grasp a small metal
block

Table 6: Instructional capabilities demonstrated by Instructo-Soar.

311

up

fiHuffman & Laird

explanation (option O2). In other cases, the default is to ask for instruction about missing
knowledge to complete the explanation (option O3).

8.1 Empirical Evaluation

Most empirical evaluations of machine learning systems take one of four forms, each appropriate for addressing different evaluation questions:
A. Comparison to other systems. This technique is useful for evaluating how overall
performance compares to the state of the art. It can be used when there are other
systems available that do the same learning task.
B. Comparison to an altered version of the same system. This technique evaluates the
impact of some component of the system on its overall performance. Typically, the
system is compared to a version of itself without the key component (sometimes called
a \lesion study").
C. Measuring performance on a systematically generated series of problems. This technique evaluates how the method is affected by different dimensions of the input (e.g.,
noise in training data).
D. Measuring performance on known hard problems. Known hard problems provide an
evaluation of overall performance under extreme conditions. For instance, concept
learners' performance is often measured on standard, dicult datasets.
These evaluation techniques have been applied in limited ways to Instructo-Soar.
They are dicult to apply in great depth for two reasons. First, whereas most machine
learning efforts concentrate on depth of a single type of learning from a single type of input,
tutorial instruction requires a breadth of learning from a range of instructional interactions. Whereas depth can be measured by quantitative performance, breadth is measured
by (possibly qualitative) coverage { here, our coverage of 7 out of 11 instructability requirements. Second, tutorial instruction has not been extensively studied in machine learning, so
there is not a battery of standard systems and problems available. Nonetheless, evaluation
techniques (B), (C), and (D) have been applied to Instructo-Soar to address specific
evaluation questions:
B. Comparison to altered version: We removed frame-axiom knowledge to illustrate the
effect of prior knowledge on the agent's performance, as described in Section 6.5.
Without prior knowledge, the agent is unable to explain instructions and must resort
to inductive methods. Thus, removing frame-axiom knowledge increased the amount
of instruction required and reduced learning quality. We also compared versions of
the agent that use different instruction recall strategies (Section 6.3).
C. Performance on systematically varied input: We examined the effects of varying three
dimensions of the instructions given to the agent. First, we compared learning curves
for instruction sequences of different lengths (Section 6.2). As the graphs in Figure 10
show, Instructo-Soar's execution time for an instructed procedure varies with the
number of instructions in sequence used to teach it. Total execution time drops each
312

fiFlexibly Instructable Agents

time the procedure is executed, according to a power law function, until the procedure has been learned in general form. Second, we compared teaching a procedure
through hierarchical subtasks versus using a at instruction sequence. Based on the
power law result, we predicted that hierarchical instruction would allow faster general
learning than at instruction. This prediction was confirmed empirically. Third, we
examined the number of instruction orderings that can be used to teach a given procedure to Instructo-Soar in order to measure the value of supporting command
exibility. Rather than an experimental measurement, we performed a mathematical
analysis. The analysis showed that due to command exibility, the number of instruction sequences that can be used to teach a given procedure is very large, growing
exponentially with the number of primitive steps in the procedure (Huffman, 1994).
D. Performance on a known hard problem: Since learning from tutorial instruction has
not been extensively studied in machine learning, there are no standard, dicult
problems. We created a comprehensive instruction scenario by crossing the command
exibility, situation exibility, and knowledge-type exibility requirements. The scenario, described in detail in (Huffman, 1994), contains 100 instructions and demonstrates 17 of Instructo-Soar's 18 instructional capabilities from Table 6 (it does
not include learning indifference in selecting between two operators). The agent learns
about 4,700 chunks during the scenario, including examples of each type of PSCM
knowledge, that extend the agent's domain knowledge significantly.

9. Limitations and Further Research
This work's limitations fall into three major categories: limitations to tutorial instruction as
a teaching technique, limitations of the agent's general capabilities, and limitations because
of incomplete solutions to the mapping, interaction, and transfer problems. We discuss each
of these in turn.

9.1 Limitations of Tutorial Instruction

Tutorial instruction is both highly interactive and situated. However, much of human
instruction is either non-interactive or unsituated (or both), and these have not been considered in this work. In non-interactive instruction, the content and ow of information to
the student is controlled primarily by the information source. Examples include classroom
lectures, instruction manuals, and textbooks. One issue in using this type of instruction is
locating and extracting the information that is needed for particular problems (Carpenter
& Alterman, 1994). Non-interactive instruction can contain both situated information (e.g.,
worked-out example problems, Chi et al., 1989; VanLehn, 1987) and unsituated information
(e.g., general expository text).
Unsituated instruction conveys general or abstract knowledge that can be applied in
a large number of different situations. Such general-purpose knowledge is often described
as \declarative" (Singley & Anderson, 1989). For example, in physics class, students are
taught that F = m  a; this general equation applies in specific ways to a great variety of
situations. The advantage of unsituated instruction is precisely this ability to compactly
communicate abstract knowledge that is broadly applicable (Sandberg & Wielinga, 1991).
313

fiHuffman & Laird

However, to use such abstract knowledge, students must learn how it applies to specific
situations (Singley & Anderson, 1989).

9.2 Limitations of the Agent

An agent's inherent limitations constrain what it can be taught. We have developed our
theory of learning from tutorial instruction within a particular computational model of
agents (the PSCM), and within this computational model, we implemented an agent with
a particular set of capabilities to demonstrate the theory. Thus, both the weaknesses of the
computational model and the specific implemented agent must be examined.
9.2.1 Computational Model

The problem space computational model is well suited for situated instruction because
of its elements' close correspondence to the knowledge level (facilitating mapping from
instructions to those elements), and its inherently local control structure. However, the
PSCM's local application of knowledge makes it dicult to learn global control regimes
through instruction, because they must be translated into a series of local decisions that
will each result in local learning.
A second weakness of the PSCM is that it provides a theory of the functional types of
knowledge used by an intelligent agent, but gives no indication of the possible content of
that knowledge. A content theory of knowledge would allow a finer grained analysis of an
agent's instructability, within the larger-grained knowledge types analysis provided by the
PSCM.
9.2.2 Implemented Agent's Capabilities

Producing a definitive agent has not been the goal of this work. Rather, the InstructoSoar agent's capabilities have been developed only as needed to demonstrate its instructional learning capabilities. Thus, it is limited in a number of ways.16 For instance, it
performs simple actions serially in a static world. This would not be sucient for a dynamic
domain such as ying an airplane, where multiple goals at multiple levels of granularity,
involving both achievement and/or maintenance of conditions in the environment, may be
active at once (Pearson et al., 1993). Instructo-Soar's procedures are implemented by a
series of locally decided steps, precluding instruction containing procedure-wide (i.e., nonlocal) path constraints (e.g., \Go to the other room, but don't walk on the carpeting!").
There is only a single agent in the world, precluding instructions that involve cooperation
with other agents (e.g., two robots carrying a couch) and instructions that require reasoning
about other agents' potential actions (e.g., \Don't go down the alley, because your enemy
may block you in.")
The agent has complete perception (clearly unrealistic in real physical domains), so it
never has to be told where to look, or asked to notice a feature that it overlooked. In contrast, our instruction protocols show that human students are often told where to attend or
what features to notice. Instructo-Soar's world is noise-free, so the agent does not need
16. These limitations are of the particular agent implemented here, not of Soar, which has been used to
build more powerful agents (e.g., Jones et al., 1993; Pearson et al., 1993).

314

fiFlexibly Instructable Agents

to reason or receive instruction about failed actions. Because it has complete perception
and a noise-free environment, the agent does not explicitly reason about uncertainty in its
perceptions or actions, and we have not demonstrated handling instructions that explicitly
describe uncertain or probabalistic outcomes.17 The agent also does not reason about time
(as, e.g., Vere and Bickmore's (1990) Homer does), so it cannot be taught to perform tasks
in a time-dependent way. It does not keep track of states it has seen or actions it performs
(other than its episodic instruction memory), so it cannot be asked to \do what you did
before." Similarly, it cannot learn procedures that are defined by a particular sequence of
actions, rather than a set of state conditions to achieve. For example, it cannot be taught
how to dance, because dancing does not result in a net change to the external world. Finally, whenever the agent does not know what to do next, it asks for more instruction. It
never tries to determine a solution through search and weak methods such as means-ends
analysis. Adding this capability would decrease its need for instruction.
In addition to the agent's capabilities, Instructo-Soar is limited because its solutions
to the mapping, interaction, and transfer problems are incomplete in various ways. These
limitations are discussed next.

9.3 Mapping Problem
Instructo-Soar employs a straightforward approach to mapping instructions into the

agent's internal language, and leaves all of the problems of mapping dicult natural language constructions unaddressed. Some of the relevant problems include reference resolution, incompleteness, and the use of domain knowledge in comprehension. Mapping can
even require further instruction, as in this interaction to resolve a referent:
> Grab the explosive block.

Which one is that?
> The red one.

This type of interaction is not supported by Instructo-Soar.
In addition to these general linguistic problems, Instructo-Soar makes only limited
use of semantic information when learning new operators. For example, when it first reads
\Move the red block left of the yellow block," it creates a new operator, but does not make
use of the semantic information communicated by \Move...to the left of." A more complete
agent would try to glean any information it could from the semantics of an unfamiliar
command.

9.4 Interaction Problem
The agent's shortcomings on the interaction problem center around its three requirements:
(I1) exible initiation of instruction, (I2) full exibility of knowledge content, and (I3)
situation exibility. (I1 ): In Instructo-Soar, instruction is initiated only by the agent.
17. In the instruction protocols we analyzed, most instructions were incomplete (missing conditions like those
Instructo-Soar learns), but rarely described uncertainty explicitly.

315

fiHuffman & Laird

This limits the instructor's ability to drive the interaction or to interrupt the agent's actions
with instruction: \No! Don't push that button!"18
(I2): Instructo-Soar provides exibility for commands, but not for instructions that
communicate other kinds of information. Similar to the notion of discourse coherence (Mann
& Thompson, 1988), a fully exible tutorable agent needs to support any instruction event
with knowledge coherence; that is, any instruction event delivering knowledge that makes
sense in the current context. The great variety of knowledge that could be relevant at any
point makes this requirement dicult.
(I3): Instructo-Soar provides situation exibility by handling both implicitly and
explicitly situated instructions, but hypothetical situations can only be referred to within a
single instruction. Human tutors often refer to one hypothetical situation over the course
of multiple instructions.

9.5 Transfer Problem

This work has focused primarily on the transfer problem { producing general learning from
tutorial instruction { and most of its requirements have been met. However, the inductive
heuristics that Instructo-Soar uses are not very powerful.
In addition, two transfer problem requirements have not been achieved. First, (T7)
Instructo-Soar has not yet demonstrated instructional learning in coexistence with learning from other knowledge sources. Nothing in Instructo-Soar's theory precludes this coexistence, however. Learning from other knowledge sources could be invoked and possibly
enhanced through instruction. For instance, an instructor might invoke learning from observation by pointing to a set of objects and saying \This is a tower"; similarly, an instruction
containing a metaphor could invoke analogical learning. One application where instruction
could potentially enhance other learning mechanisms is within \personal assistant" software
agents that learn by observing their users (e.g., Maes, 1994; Mitchell et al., 1994). Adding
the ability to learn from verbal instructions in addition to observations would allow users
to explicitly train these agents in situations where learning from observation alone may be
dicult or slow.
Second, (T6) Instructo-Soar cannot recover from incorrect knowledge that leads to
either invalid explanations or incorrect external performance. Such incorrect knowledge
may be a part of the agent's initial domain theory, or may be learned through faulty
instruction. Inability to recover from incorrect knowledge precludes instruction by general
case and exceptions; for instance, \Never grasp red blocks," and then later, \It's ok to
grasp the ones with safety signs on them." In order to avoid learning anything incorrect,
whenever Instructo-Soar attempts to induce new knowledge, it asks for the instructor's
verification before adding the knowledge to its long-term memory. Human students do not
ask for so much verification; they appear to jump to conclusions, and alter them later if
they prove to be incorrect based on further information.
Rather than always verifying knowledge being learned, our next generation of instructable
agents will learn from reasonable inferences without verification (although they may ask
for verifications in extreme cases). We have recently produced such an agent (Pearson &
18. We have recently added a simple interruptability capability to a new version of Instructo-Soar that
incorporates recovery from incorrect knowledge (Pearson & Huffman, 1995).

316

fiFlexibly Instructable Agents

Huffman, 1995) that incorporates current research on incremental recovery from incorrect
knowledge (Pearson & Laird, 1995). This agent learns to correct overgeneral knowledge that
it infers when completing explanations of instructions. The correction process is triggered
when using the overgeneral knowledge results in incorrect performance (e.g., an action that
the agent expects to succeed does not). In the long run, we believe this work could push
research on incremental theory revision and error recovery, because instructable agents can
be taught many types of knowledge that may need revision.

10. Conclusion

Although much work in machine learning aims for depth at a particular kind of learning,

Instructo-Soar demonstrates breadth { of interaction with an instructor to learn a variety

of types of knowledge { but all arising from one underlying technique. This kind of breadth is
crucial in building an instructable agent because of the great variety of instructions and the
variety of knowledge that they can communicate. Because instructable agents begin with
some basic knowledge of their domain, Instructo-Soar uses an analytic, explanationbased approach to learn from instructions, which makes use of that knowledge. Because
instructions may be either implicitly or explicitly situated, Instructo-Soar situates its
explanations of each instruction within the situation indicated by the instruction. Finally,
because the agent's knowledge is often deficient for explaining instructions, InstructoSoar employs four different options for dealing with incomplete explanations, and selects
between these options dynamically depending on the instructional context.
Because of its availability and effectiveness, tutorial instruction is potentially a powerful knowledge source for intelligent agents. Instructo-Soar illustrates this in a simple
domain. Realizing instruction's potential in fielded applications will require more linguistically able agents that incorporate robust techniques for not only acquiring knowledge from
instruction, but also refining that knowledge as needed based on performance and further
instruction.

Acknowledgements
This work was performed while the first author was a graduate student at the University of
Michigan. It was sponsored by NASA/ONR under contract NCC 2-517, and by a University
of Michigan Predoctoral Fellowship. Thanks to Paul Rosenbloom, Randy Jones, and our
anonymous reviewers for helpful comments on earlier drafts.

References

Akatsuka, N. (1986). Conditionals are discourse-bound. In Traugott, E. C. (Ed.), On
Conditionals, pp. 333{51. Cambridge Univ. Press, Cambridge.
Alterman, R., Zito-Wolf, R., & Carpenter, T. (1991). Interaction, comprehension, and
instruction usage. Journal of the Learning Sciences, 1 (3&4), 273{318.
Anderson, J. R. (1983). The architecture of cognition. Harvard University Press, Cambridge,
MA.
317

fiHuffman & Laird

Bergadano, F., & Giordana, A. (1988). A knowledge intensive approach to concept induction. In Proceedings of the International Conference on Machine Learning, pp.
305{317.
Birmingham, W., & Klinker, G. (1993). Knowledge acquisition tools with explicit problemsolving methods. The Knowledge Engineering Review, 8 (1).
Birmingham, W., & Siewiorek, D. (1989). Automated knowledge acquisition for a computer
hardware synthesis system. Knowledge Acquisition, 1, 321{340.
Bloom, B. S. (1984). The 2 sigma problem: The search for methods of group instruction as
effective as one-to-one tutoring. Educational Researcher, 13 (6), 4{16.
Brachman, R. J. (1980). An introduction to KL-ONE. In Brachman, R. J. (Ed.), Research
in Natural Language Understanding, pp. 13{46. Bolt, Beranek and Newman Inc.,
Cambridge, MA.
Carbonell, J. G., & Gil, Y. (1987). Learning by experimentation. In Proceedings of the
International Workshop on Machine Learning, pp. 256{265.
Carbonell, J. G., Michalski, R. S., & Mitchell, T. M. (1983). An overview of machine
learning. In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine
Learning: An artificial intelligence approach. Morgan Kaufmann.
Carpenter, T., & Alterman, R. (1994). A reading agent. In Proceedings of the Twelfth
National Conference on Artificial Intelligence Seattle, WA.
Chapman, D. (1990). Vision, Instruction, and Action. Ph.D. thesis, Massachusetts Institute
of Technology, Artificial Intelligence Laboratory.
Chi, M. T. H., Bassok, M., Lewis, M. W., Reimann, P., & Glaser, R. (1989). Selfexplanations: How students study and use examples in learning to solve problems.
Cognitive Science, 13, 145{182.
Cypher, A. (Ed.). (1993). Watch what I do: Programming by demonstration. MIT Press,
Cambridge, Mass.
Davis, R. (1979). Interactive transfer of expertise: Acquisition of new inference rules.
Artificial Intelligence, 12 (2), 409{427.
DeJong, G. F., & Mooney, R. J. (1986). Explanation-based learning: An alternative view.
Machine Learning, 1 (2), 145{176.
Dent, L., Boticario, J., McDermott, J., Mitchell, T., & Zabowski, D. (1992). A personal
learning apprentice. In Proceedings of the International Joint Conference on Artificial
Intelligence.
DiEugenio, B. (1993). Understanding natural language instructions: A computational approach to purpose clauses. Ph.D. thesis, University of Pennsylvania. IRCS Report
93-52.
318

fiFlexibly Instructable Agents

DiEugenio, B., & Webber, B. (1992). Plan recognition in understanding instructions. In
Hendler, J. (Ed.), Proceedings of the First International Conference on Artificial Intelligence Planning Systems, pp. 52{61 College Park, MD.
Donoho, S. K., & Wilkins, D. C. (1994). Exploiting the ordering of observed problem-solving
steps for knowledge ase refinement: An apprenticeship approach. In Proceedings of
the 12th National Conference on Artifical Intelligence Seattle, WA.
Drummond, M. (1989). Situated control rules. In Proceedings of the First International Conference on Principles of Knowledge Representation Toronto, Canada. Morgan Kaufmann.
Emihovich, C., & Miller, G. E. (1988). Talking to the turtle: A discourse analysis of Logo
instruction. Discourse Processes, 11, 183{201.
Eshelman, L., Ehret, D., McDermott, J., & Tan, M. (1987). MOLE: A tenacious knowledgeacquisition tool. International Journal of Man-Machine Studies, 26 (1), 41{54.
Fikes, R. E., Hart, P. E., & Nilsson, N. J. (1972). Learning and executing generalized robot
plans. Artificial Intelligence, 3, 251{288.
Ford, C. A., & Thompson, S. A. (1986). Conditionals in discourse: A text-based study
from English. In Traugott, E. C. (Ed.), On Conditionals, pp. 353{72. Cambridge
Univ. Press, Cambridge.
Frederking, R. E. (1988). Integrated natural language dialogue: A computational model.
Kluwer Academic Press, Boston.
Golding, A., Rosenbloom, P. S., & Laird, J. E. (1987). Learning search control from outside
guidance. In Proceedings of the Tenth International Joint Conference on Artificial
Intelligence, pp. 334{337.
Grosz, B. J. (1977). The Representation and use of focus in dialogue understanding. Ph.D.
thesis, University of California, Berkeley.
Gruber, T. (1989). Automated knowledge acquisition for strategic knowledge. Machine
Learning, 4 (3-4), 293{336.
Guha, R. V., & Lenat, D. B. (1990). Cyc: A mid-term report. AI Magazine, 11 (3), 32{59.
Haas, N., & Hendrix, G. G. (1983). Learning by being told: Acquiring knowledge for
information management. In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M.
(Eds.), Machine Learning: An artificial intelligence approach. Morgan Kaufmann.
Haiman, J. (1978). Conditionals are topics. Language, 54, 564{89.
Hall, R. J. (1988). Learning by failing to explain. Machine Learning, 3 (1), 45{77.
Hayes-Roth, F., Klahr, P., & Mostow, D. J. (1981). Advice taking and knowledge refinement:
An iterative view of skill acquisition. In Anderson, J. R. (Ed.), Cognitive skills and
their acquisition, pp. 231{253. Lawrence Erlbaum Associates, Hillsdale, NJ.
319

fiHuffman & Laird

Huffman, S. B. (1994). Instructable autonomous agents. Ph.D. thesis, University of Michigan, Dept. of Electrical Engineering and Computer Science.
Huffman, S. B., & Laird, J. E. (1992). Dimensions of complexity in learning from interactive
instruction. In Erickson, J. (Ed.), Proceedings of Cooperative Intelligent Robotics in
Space III, SPIE Volume 1829.
Huffman, S. B., & Laird, J. E. (1993). Learning procedures from interactive natural language instructions. In Utgoff, P. (Ed.), Machine Learning: Proceedings of the Tenth
International Conference.
Huffman, S. B., & Laird, J. E. (1994). Learning from highly exible tutorial instruction.
In Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94)
Seattle, WA.
Huffman, S. B., Miller, C. S., & Laird, J. E. (1993). Learning from instruction: A knowledgelevel capability within a unified theory of cognition. In Proceedings of the Fifteenth
Annual Conference of the Cognitive Science Society, pp. 114{119.
Johnson-Laird, P. N. (1986). Conditionals and mental models. In Traugott, E. C. (Ed.),
On Conditionals. Cambridge Univ. Press, Cambridge.
Jones, R. M., Tambe, M., Laird, J. E., & Rosenbloom, P. S. (1993). Intelligent automated agents for ight training simulators. In Proceedings of the Third Conference
on Computer Generated Forces, pp. 33{42 Orlando, FL.
Just, M. A., & Carpenter, P. A. (1976). Verbal comprehension in instructional situations. In
Klahr, D. (Ed.), Cognition and Instruction. Lawrence Erlbaum Associates, Hillsdale,
NJ.
Kieras, D. E., & Bovair, S. (1984). The role of a mental model in learning to operate a
device. Cognitive Science, 8, 255{273.
Kodratoff, Y., & Tecuci, G. (1987a). DISCIPLE-1: Interactive apprentice system in weak
theory fields. In Proceedings of the Tenth International Joint Conference on Artificial
Intelligence, pp. 271{273.
Kodratoff, Y., & Tecuci, G. (1987b). Techniques of design and DISCIPLE learning apprentice. International Journal of Expert Systems, 1 (1), 39{66.
Laird, J. E., Congdon, C. B., Altmann, E., & Doorenbos, R. (1993). Soar user's manual,
version 6..
Laird, J. E., Hucka, M., Yager, E. S., & Tuck, C. M. (1990). Correcting and extending
domain knowledge using outside guidance. In Proceedings of the Seventh International
Conference on Machine Learning.
Laird, J. E., Newell, A., & Rosenbloom, P. S. (1987). Soar: An architecture for general
intelligence. Artificial Intelligence, 33 (1), 1{64.
320

fiFlexibly Instructable Agents

Laird, J. E., & Rosenbloom, P. S. (1990). Integrating execution, planning, and learning in
Soar for external environments. In Proceedings of the Eighth National Conference on
Artificial Intelligence, pp. 1022{1029. AAAI Press.
Lewis, C. (1988). Why and how to learn why: Analysis-based generalization of procedures.
Cognitive Science, 12, 211{256.
Lewis, R. L. (1993). An Architecturally-Based Theory of Human Sentence Comprehension.
Ph.D. thesis, Carnegie Mellon University, School of Computer Science.
Lewis, R. L., Newell, A., & Polk, T. A. (1989). Toward a Soar theory of taking instructions for immediate reasoning tasks. In Proceedings of the Annual Conference of the
Cognitive Science Society.
Lindsay, R. K. (1963). Inferential memory as the basis of machines which understand natural
language. In Feigenbaum, E. A., & Feldman, J. (Eds.), Computers and Thought, pp.
217{233. R. Oldenbourg KG.
Maes, P. (1994). Agents that reduce work and information overload. Communications of
the ACM, 37 (7).
Maes, P., & Kozierok, R. (1993). Learning interface agents. In Proceedings of the National
Conference on Artificial Intelligence, pp. 459{465.
Mann, W. C., & Thompson, S. A. (1988). Rhetorical structure theory: Toward a functional
theory of text organization. Text, 8 (3), 243{281.
Marcus, S., & McDermott, J. (1989). SALT: A knowledge acquisition language for proposeand-revise systems. Artificial Intelligence, 39 (1), 1{37.
Martin, C. E., & Firby, R. J. (1991). Generating natural language expectations from a
reactive execution system. In Proceedings of the Thirteenth Annual Conference of the
Cognitive Science Society, pp. 811{815.
McCarthy, J. (1968). The advice taker. In Minsky, M. (Ed.), Semantic Information Processing, pp. 403{410. MIT Press, Cambridge, Mass.
Miller, C. M. (1993). A model of concept acquisition in the context of a unified theory of
cognition. Ph.D. thesis, The University of Michigan, Dept. of Computer Science and
Electrical Engineering.
Minton, S., Carbonell, J. G., Knoblock, C. A., Kuokka, D. R., Etzioni, O., & Gil, Y. (1989).
Explanation-based learning: A problem-solving perspective. Artificial Intelligence, 40,
63{118.
Mitchell, T., Caruana, R., Freitag, D., McDermott, J., & Zabowski, D. (1994). Experience
with a learning personal assistant. Communications of the ACM, 37 (7).
Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. (1986). Explanation-based generalization: A unifying view. Machine Learning, 1.
321

fiHuffman & Laird

Mitchell, T. M., Mahadevan, S., & Steinberg, L. I. (1990). LEAP: A learning apprentice
system for VLSI design. In Kodratoff, Y., & Michalski, R. S. (Eds.), Machine Learning:
An artificial intelligence approach, Vol. III. Morgan Kaufmann.
Mooney, R. J. (1990). Learning plan schemata from observation: Explanation-based learning for plan recognition. Cognitive Science, 14, 483{509.
Mostow, D. J. (1983). Learning by being told: Machine transformation of advice into a
heuristic search procedure. In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M.
(Eds.), Machine Learning: An artificial intelligence approach. Morgan Kaufmann.
Musen, M. A. (1989). Automated support for building and extending expert models. Machine Learning, 4 (3-4), 347{376.
Newell, A. (1981). The knowledge level. AI Magazine, 2 (2), 1{20.
Newell, A. (1990). Unified Theories of Cognition. Harvard University Press, Cambridge,
Massachusetts.
Newell, A., Yost, G., Laird, J. E., Rosenbloom, P. S., & Altmann, E. (1990). Formulating
the problem space computational model. In Proceedings of the 25th Anniversary
Symposium, School of Computer Science, Carnegie Mellon University.
Pazzani, M. (1991a). A computational theory of learning causal relationships. Cognitive
Science, 15, 401{424.
Pazzani, M. (1991b). Learning to predict and explain: An integration of similarity-based,
theory driven, and explanation-based learning. Journal of the Learning Sciences, 1 (2),
153{199.
Pearson, D. J., & Huffman, S. B. (1995). Combining learning from instruction with recovery
from incorrect knowledge. In Gordon, D., & Shavlik, J. (Eds.), Proceedings of the 1995
Machine Learning Workshop on Agents that Learn from Other Agents.
Pearson, D. J., Huffman, S. B., Willis, M. B., Laird, J. E., & Jones, R. M. (1993). A
symbolic solution to intelligent real-time control. IEEE Robotics and Autonomous
Systems, 11, 279{291.
Pearson, D. J., & Laird, J. E. (1995). Toward incremental knowledge correction for agents in
complex environments. In Muggleton, S., Michie, D., & Furukawa, K. (Eds.), Machine
Intelligence, Vol. 15. Oxford University Press.
Porter, B. W., Bareiss, R., & Holte, R. C. (1990). Concept learning and heuristic classification in weak-theory domains. Artificial Intelligence, 45 (3), 229{263.
Porter, B. W., & Kibler, D. F. (1986). Experimental goal regression: A method for learning
problem-solving heuristics. Machine Learning, 1, 249{286.
Redmond, M. A. (1992). Learning by observing and understanding expert problem solving.
Ph.D. thesis, Georgia Institute of Technology.
322

fiFlexibly Instructable Agents

Rosenbloom, P. S., & Aasman, J. (1990). Knowledge level and inductive uses of chunking
(EBL). In Proceedings of the National Conference on Artificial Intelligence.
Rosenbloom, P. S., & Laird, J. E. (1986). Mapping explanation-based generalization onto
Soar. In Proceedings of the National Conference on Artificial Intelligence, pp. 561{567.
Rosenbloom, P. S., Laird, J. E., & Newell, A. (1988). The chunking of skill and knowledge.
In Bouma, H., & Elsendoorn, A. G. (Eds.), Working Models of Human Perception,
pp. 391{410. Academic Press, London, England.
Rosenbloom, P. S., Laird, J. E., & Newell, A. (Eds.). (1993a). The Soar Papers: Research
on integrated intelligence. MIT Press, Cambridge, Mass.
Rosenbloom, P. S., Laird, J. E., & Newell, A. (Eds.). (1993b). The Soar Papers: Research
on integrated intelligence. MIT Press, Cambridge, Mass.
Rosenbloom, P. S., & Newell, A. (1986). The chunking of goal hierarchies: A generalized
model of practice. In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.),
Machine Learning: An artificial intelligence approach, Volume II. Morgan Kaufmann.
Rumelhart, D. E., & McClelland, J. L. (Eds.). (1986). Parallel distributed processing: Explorations in the microstructure of cognition. MIT Press, Cambridge, MA.
Rychener, M. D. (1983). The instructible production system: A retrospective analysis. In
Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning: An
artificial intelligence approach, pp. 429{460. Morgan Kaufmann.
Sandberg, J., & Wielinga, B. (1991). How situated is cognition?. In Proceedings of the
International Joint Conference on Artificial Intelligence, pp. 341{346.
Schank, R. C. (1975). Conceptual Information Processing. American Elsevier, New York.
Schank, R. C., & Leake, D. B. (1989). Creativity and learning in a case-based explainer.
Artificial Intelligence, 40, 353{385.
Segre, A. M. (1987). A learning apprentice system for mechanical assembly. In Third IEEE
Conference on Artificial Intelligence for Applications, pp. 112{117.
Shen, W. (1993). Discovery as autonomous learning from the environment. Machine Learning, 12, 143{165.
Simon, H. A. (1977). Artificial intelligence systems that understand. In Proceedings of the
Fifth International Joint Conference on Artificial Intelligence, pp. 1059{1073.
Simon, H. A., & Hayes, J. R. (1976). Understanding complex task instructions. In Klahr,
D. (Ed.), Cognition and Instruction. Lawrence Erlbaum Associates, Hillsdale, NJ.
Singley, M. K., & Anderson, J. R. (1989). The transfer of cognitive skill. Harvard University
Press.
323

fiHuffman & Laird

Sutton, R. S., & Pinette, B. (1985). The learning of world models by connectionist networks.
In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, pp.
54{64.
Thrun, S. B., & Mitchell, T. M. (1993). Integrating inductive neural network learning and
explanation-based learning. In Proceedings of the International Joint Conference on
Artificial Intelligence, pp. 930{936.
VanLehn, K. (1987). Learning one subprocedure per lesson. Artificial Intelligence, 31 (1),
1{40.
VanLehn, K., Ball, W., & Kowalski, B. (1990). Explanation-based learning of correctness:
Towards a model of the self-explanation effect. In Proceedings of the 12th Annual
Conference of the Cognitive Science Society, pp. 717{724.
VanLehn, K., & Jones, R. (1991). Learning physics via explanation-based learning of correctness and analogical search control. In Proceedings of the International Machine
Learning Workshop.
VanLehn, K., Jones, R. M., & Chi, M. T. H. (1992). A model of the self-explanation effect.
Journal of the Learning Sciences, 2 (1), 1{59.
Vere, S., & Bickmore, T. (1990). A basic agent. Computational Intelligence, 6, 41{60.
Wertsch, J. V. (1979). From social interaction to higher psychological processes: A clarification and application of Vygotsky's theory. Human Development, 22, 1{22.
Widmer, G. (1989). A tight integration of deductive and inductive learning. In Proceedings
of the International Workshop on Machine Learning, pp. 11{13.
Wilkins, D. C. (1990). Knowledge base refinement as improving an incomplete and incorrect
domain theory. In Kodratoff, Y., & Michalski, R. S. (Eds.), Machine Learning: An
Artificial Intelligence Approach, Volume III, pp. 493{514. Morgan Kaufmann.
Winograd, T. (1972). Understanding Natural Language. Academic Press, New York.
Wood, D., Bruner, J. S., & Ross, G. (1976). The role of tutoring in problem solving. Journal
of Child Psychology and Psychiatry, 17, 89{100.
Yost, G. R. (1993). Acquiring knowledge in Soar. IEEE Expert, 8 (3), 26{34.
Yost, G. R., & Newell, A. (1989). A problem space approach to expert system specification.
In Proceedings of the International Joint Conference on Artificial Intelligence, pp.
621{7.

324

fiJournal of Artificial Intelligence Research 3 (1995) 53-118

Submitted 3/95; published 7/95

Building and Refining Abstract Planning Cases
by Change of Representation Language
Ralph Bergmann
Wolfgang Wilke

bergmann@informatik.uni-kl.de
wilke@informatik.uni-kl.de

Centre for Learning Systems and Applications (LSA)
University of Kaiserslautern, P.O.-Box 3049, D-67653 Kaiserslautern, Germany

Abstract

Abstraction is one of the most promising approaches to improve the performance of problem
solvers. In several domains abstraction by dropping sentences of a domain description { as
used in most hierarchical planners { has proven useful. In this paper we present examples
which illustrate significant drawbacks of abstraction by dropping sentences. To overcome
these drawbacks, we propose a more general view of abstraction involving the change of
representation language. We have developed a new abstraction methodology and a related
sound and complete learning algorithm that allows the complete change of representation
language of planning cases from concrete to abstract. However, to achieve a powerful
change of the representation language, the abstract language itself as well as rules which
describe admissible ways of abstracting states must be provided in the domain model.
This new abstraction approach is the core of Paris (Plan Abstraction and Refinement
in an Integrated System), a system in which abstract planning cases are automatically
learned from given concrete cases. An empirical study in the domain of process planning
in mechanical engineering shows significant advantages of the proposed reasoning from
abstract cases over classical hierarchical planning.

1. Introduction
Abstraction is one of the most challenging and also promising approaches to improve complex
problem solving and it is inspired by the way humans seem to solve problems. At first, less
relevant details of a given problem are ignored so that the abstracted problem can be
solved more easily. Then, step by step, more details are added to the solution by taking an
increasingly more detailed look at the problem. Thereby, the abstract solution constructed
first is refined towards a concrete solution. One typical characteristic of most work on
hierarchical problem solving is that abstraction is mostly performed by dropping sentences
of a domain description (Sacerdoti, 1974, 1977; Tenenberg, 1988; Unruh & Rosenbloom,
1989; Yang & Tenenberg, 1990; Knoblock, 1989, 1994; Bacchus & Yang, 1994). A second
common characteristic is that a hierarchical problem solver usually derives an abstract
solution from scratch, without using experience from previous problem solving episodes.
Giunchiglia and Walsh (1992) have presented a comprehensive formal framework for
abstraction and a comparison of the different abstraction approaches from theorem proving
(Plaisted, 1981, 1986; Tenenberg, 1987), planning (Newell & Simon, 1972; Sacerdoti, 1974,
1977; Tenenberg, 1988; Unruh & Rosenbloom, 1989; Yang & Tenenberg, 1990; Knoblock,
1989, 1994), and model based diagnosis (Mozetic, 1990). For hierarchical planning, Korf's
model of abstraction in problem solving (Korf, 1987) allows the analysis of reductions in
c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBergmann & Wilke

search caused by single and multiple levels of abstraction. He has shown that in the optimal
case, abstraction can reduce the expected search time from exponential to linear. Knoblock
has developed an approach to construct a hierarchy of abstraction spaces automatically
from a given concrete-level problem solving domain (Knoblock, 1990, 1993, 1994). These
so called ordered monotonic abstraction hierarchies (Knoblock, Tenenberg, & Yang, 1991b)
have proven useful in many domains. Recently, Bacchus and Yang (1994) presented an
improved method for automatically generating abstraction hierarchies based on a more
detailed model of search costs.
All these abstraction methods, however, rely on abstraction by dropping sentences of
the domain description which is a kind of homomorphic abstraction (Holte et al., 1994,
1995). It has been shown that these kinds of abstractions are highly representation dependent (Holte et al., 1994, 1995). For two classical planning domains, different \natural\
representations have been analyzed and it turns out that there are several representations
for which the classical abstraction techniques do not lead to significantly improved problem
solvers (Knoblock, 1994; Holte et al., 1995). However, it is well known that normally many
different representations of the same domain exist as already pointed out by Korf (1980),
but up to now no theory of representation has been developed. In particular, there is no
theory of representation for hierarchical problem solving with dropping sentences.
From a knowledge-engineering perspective, many different aspects such as simplicity,
understandability, and maintainability must be considered when developing a domain representation. Therefore, we assume that representations of domains are given by knowledge
engineers and rely on representations which we consider most \natural" for certain kinds of
problems. We will demonstrate two simple example problems and related representations,
in which the usual use of abstraction in problem solving does not lead to any improvement.
In the first example, no improvement can be achieved because abstraction is restricted to
dropping sentences of a domain. In the second example, the abstract solution computed
from scratch does not decompose the original problem and consequently does not cut down
the search space at the next detailed level. We do not want to argue that the examples
can never be represented in a way that standard hierarchical problem solving works well.
However, we think it would require a large effort from a knowledge engineer to develop an
appropriate representation and we believe that it is often impossible to develop a representation which is appropriate from a knowledge-engineering perspective and which also allows
ecient hierarchical problem solving based on dropping sentences.
We take these observations as the motivation to develop a more general model of abstraction in problem solving. As already pointed out by Michalski (1994), abstraction, in
general, can be seen as switching to a completely new representation language in which the
level of detail is reduced. In problem solving, such a new abstract representation language
must consist of completely new sentences and operators and not only of a subset of the
sentences and operators of the concrete language. To our knowledge, Sipe (Wilkins, 1988)
is the only planning system which currently allows the change of representation language
across different levels of abstraction. However, a general abstraction methodology which
allows ecient algorithms for abstraction and refinement has not yet been developed. We
want to propose a method of abstraction which allows the complete change of representation language of a problem and a solution from concrete to abstract and vice versa, if the
concrete and the abstract language are given. Additionally, we propose to use experience
54

fiBuilding and Refining Abstract Planning Cases

from previously solved problems, usually available as a set of cases, to come to abstract
solutions. The use of experience has already proven useful in various approaches to speedup learning such as explanation-based learning (Mitchell, Keller, & Kedar-Cabelli, 1986;
DeJong & Mooney, 1986; Rosenbloom & Laird, 1986; Minton, 1988; Minton, Carbonell,
Knoblock, Kuokka, Etzioni, & Gil, 1989; Shavlik & O'Rorke, 1993; Etzioni, 1993; Minton
& Zweben, 1993; Langley & Allen, 1993; Kambhampati & Kedar, 1994), and analogical or
case-based reasoning (Carbonell, 1986; Kambhampati & Hendler, 1992; Veloso & Carbonell,
1993; Veloso, 1994).
As the main contribution of this paper, we present an abstraction methodology and a
related learning method in which beneficial abstract planning cases are automatically derived
from given concrete cases. Based on a given concrete and abstract language, this learning
approach allows the complete change of the representation of a case from the concrete to
the abstract level. However, to achieve such an unconstrained kind of abstraction, the
set of admissible abstractions must be implicitly predefined by a generic abstraction theory.
Compared to approaches in which abstraction hierarchies are generated automatically, more
effort is required to specify the abstract language, but we feel that this is a price we have
to pay to make planning more tractable in certain situations.
This approach is fully implemented in Paris (Plan Abstraction and Refinement in an
Integrated System), a system in which abstract cases are learned and organized in a case
base. During novel problem solving, this case-base is searched for a suitable abstract case
which is further refined to a concrete solution to the current problem.
The presentation of this approach is organized as follows. The next section presents an
analysis of hierarchical problem solving in which the shortcomings of current approaches
are illustrated by simple examples. Section three argues that a powerful case abstraction
and refinement method can overcome the identified problems. Furthermore, we present the
Paris approach informally, using a simple example. The next three sections of the paper
formalize the general abstraction approach. After introducing the basic terminology, Section 5 defines a new formal model of case abstraction. Section 6 contains a very detailed
description of a correct and complete learning algorithm for case abstraction. Section 7
explains the refinement of cases for solving new problems. Section 8 gives a detailed description of the domain of process planning in mechanical engineering for the production
of rotary-symmetric workpieces on a lathe and demonstrates the proposed approach on examples from this domain. Section 9 reports on a detailed experimental evaluation of Paris
in the described domain. Finally, we discuss the presented approach in relation to similar
work in the field. The appendix of the article contains the formal proofs of the properties
of the abstraction approach and the related learning algorithm. Additionally, the detailed
representation of the mechanical engineering domain used for the experimental evaluation
is given in Online Appendix 1.

2. Analysis of Hierarchical Problem Solving
The basic intuition behind abstraction is as follows. By first ignoring less relevant features
of the problem description, abstraction allows problems to be solved in a coarse fashion with
less effort. Then, the derived abstract (skeletal) solution serves as a problem decomposition
for the original, more detailed problem. Korf (1987) has shown that hierarchical problem
55

fiBergmann & Wilke

solving can reduce the required search space significantly. Assume that a problem requires
a solution of length n and furthermore assume that the average branching factor is b,
i.e., the average number of states that can be reached from a given state by applying a
single operator. The worst-case time complexity for finding the required solution by search
is O(bn ). Now, suppose that the problem is decomposed by an abstract solution into
k subproblems, each of which require a solution of length n1; : : : ; nk , respectively, with
n1 + n2 +    + nk = n. In this situation, the worst-case time complexity for finding the
complete solution is O(bn1 + bn2 +    + bnk ) which is O(bmax(n1;n2 ;:::;nk ) ). Please note that
this a significant reduction in search time complexity. In particular, we can easily see that
the reduction is maximal if all subproblems are of similar size, i.e., n1  n2      nk .
However, to achieve a significant search reduction, the computed abstract solution must
not only be a solution to the abstracted problem, it must additionally fulfill a certain
requirement presupposed in the above analysis. The subproblems introduced by the abstract
solution must be independent, i.e., each of them must be solvable without interaction with
the other subproblems. This avoids backtracking between the solution of each subproblem
and consequently cuts down the necessary overall search space. Even if this restriction is not
completely fulfilled, i.e., backtracking is still required in a few cases, several empirical studies
(especially Knoblock, 1991, 1993, 1994) have shown that abstraction can nevertheless lead
to performance improvements.
Unfortunately, there are also domains and representations of domains (Holte et al., 1994,
1995) in which the way abstraction is used in hierarchical problem solving cannot improve
problem solving because the derived abstract solutions don't fulfill the above mentioned
requirement at all. In the following, we will show two examples of such domains which
demonstrate two general drawbacks of hierarchical problem solving. Please note that in
these examples, a particular representation is assumed. We feel that these representations
are somehow \natural" and very likely to be used by a knowledge engineer developing a domain. However, there might be other representations of these domains for which traditional
hierarchical planning works. We assume that such representations are very dicult to find,
especially if the domain representation should also fulfill additional knowledge-engineering
requirements.

2.1 Abstraction by Dropping Sentences

In hierarchical problem solving, abstraction is mostly1 achieved by dropping sentences of
the problem description from preconditions and/or effects of operators (Sacerdoti, 1974,
1977; Tenenberg, 1988; Unruh & Rosenbloom, 1989; Yang & Tenenberg, 1990; Knoblock,
1989, 1994). The assumption which justifies this kind of abstraction is that less relevant
details of the problem description are expressed as isolated sentences in the representation
which can be addressed after the more relevant sentences have been established. Ignoring
such sentences is assumed to lead to an abstract solution useful to reduce the search at the
more concrete planning levels.
However, this assumption does not hold in all domains. For example, in many real world
domains, certain events need to be counted, e.g., when transporting a certain number of
1. Only Tenenberg's (1988) abstraction by analogical mappings and the planning system Sipe (Wilkins,
1988) contains first approaches that allow a change of representation language.

56

fiBuilding and Refining Abstract Planning Cases

containers from one location to another. Imagine a domain in which, in addition to several
other operators, there is an increment operator described as follows:
Operator: inc
Precondition: value(X )
Delete: value(X )
Add: value(X + 1)

In this representation, the integer value which is increased is represented by a single sentence. Each state consists only of a single sentence, and also the operator contains only
one single sentence.2 We think that this representation is very \natural" and very likely to
be chosen by a knowledge engineer. In this domain, incrementing value(0) to value(8)
requires a sequential plan composed of 8 inc-operators, leading to the state sequence:
value(0),value(1),: : : ,value(8). In this example, however, abstraction by dropping sentences does not work because, if this single sentence would be dropped, nothing would remain in the operator description and the whole counting problem would have been dropped
completely. So there is only the empty problem at the abstract level, and the empty plan is
going to solve it. Unfortunately, the empty plan cannot cause any complexity reduction for
solving the problem at the concrete level. Consequently, abstraction by dropping sentences
completely fails to improve problem solving in this situation.
However, we can adequately cope with this counting problem by abstracting the
quantitative value expressed in the sentence towards a qualitative representation (e.g.,
low=f0; 1; 2; 3g, medium = f4; 5; 6; 7g, high = f8; 9; 10; 11g). Such a qualitative representation would result in an abstract plan composed of two operators (subproblems) that
increase value from low to medium and further to high. This abstract plan defines two
independently refinable subproblems. To solve the first subproblem at the concrete level,
the problem solver has to search for a sequence of inc-operators which increment the value
from 0 to any medium value (any value from the set f4; 5; 6; 7g). This subproblem can be
solved by a sequence of 4 inc-operators leading to the concrete state with a value of 4. Similarly, the second subproblem at the concrete level is to find a sequence of operators which
change the value from 4 to the final value 8. Also this second subproblem can be solved by
a sequence of 4 inc-operators. So we can see that the complete problem which requires a
sequence of 8 concrete operators is divided into 2 subproblems where each subproblem can
be solved by a 4-step plan. Because of the exponential nature of the search space, the two
4-step problems together can be solved with much less search than the 8-step problem as a
whole. Following Korf's analysis sketched before, the time complexity is reduced from O(b8)
to O(b4)3 . Please note that the particular abstraction which leads to two subproblems is not
central for achieving the complexity reduction. The important point is that the problem is
decomposed into more than one subproblem. This kind of abstraction can be achieved by
introducing a new abstract representation language which consists of the qualitative values
and a corresponding abstract increment operator.
2. However, we might assume that the term X + 1 is modeled as a separate predicate in the precondition.
Unfortunately, this does not change the described situation at all.
3. Because we assume many other operators besides the inc-operator, b  1 holds.

57

fiBergmann & Wilke

We can even generalize from the specific example presented above. The problem with
the dropping condition approach is that it is not possible to abstract information (e.g.,
the value in our example) that is coded in a single sentence in the representation. This is
particularly a problem when the required solution contains a long sequence of states which
only differ in a single sentence. Dropping this particular sentence leads to dropping the
whole problem, and not dropping the sentence does not lead to any abstraction. What is
really required is to abstract the information encoded in this single sentence which obviously
requires more than just dropping the complete information.
To summarize, we have seen that abstraction by dropping sentences does not work for
the particular kind of problems we have shown. In general, abstraction requires changing the complete representation language from concrete to abstract which usually involves
the introduction of completely new abstract terms (sentences or operators). Within this
general view, dropping sentences is just a special case of abstraction. The reason why dropping sentences has been widely used in hierarchical planning is that due to its simplicity,
refinement is very easy because abstract states can directly be used as goals at the more
detailed levels. Another very important property of abstraction by dropping sentences is
that useful hierarchies of abstraction spaces can be constructed automatically from domain
descriptions (Knoblock, 1990, 1993, 1994; Bacchus & Yang, 1994).

2.2 Generating of Abstract Solutions from Scratch

Another limiting factor of classical hierarchical problem solving can be the way abstract solutions are computed. As pointed out by Korf, a good abstract solution must lead to mostly
independent subproblems of equal size. In classical problem solving, an abstract solution
is found by breadth-first or depth-first search using linear (e.g., Alpine, Knoblock, 1993)
or non-linear (e.g., Abtweak, Yang & Tenenberg, 1990) problem solvers. For these problem solvers, the upward-solution property (Tenenberg, 1988) usually holds, which means
that an abstract solution exists if a concrete-level solution exists. Usually, these problem
solvers find an arbitrary abstract solution (e.g., the shortest possible solution). Unfortunately, there is no way to guarantee that the computed solutions are refinable and lead to
mostly independent subproblems of suciently equal size, even if such a solution exists.
In general, there are not even heuristics which try to guide problem solving towards the
aspired kind of useful abstractions. This problem is illustrated by the following example,
which additionally shows the limitation of abstraction by dropping sentences.
Imagine a large (or even infinite) state space which includes at least the 8 distinct states
shown on the left of Figure 1. Each of these 8 states is described by the presence or absence of
three sentences E 1, E 2, and E 3 in the state description. In the 3-bit-vector shown in Figure
1, "0" indicates the absence of the sentence and "1" represents the presence of the sentence.
The 8 different states described by these three sentences are arranged in a 3-dimensional
cube, using one dimension for each sentence. The arrows in this diagram show possible state
transitions by the available operators of the domain.4 Each operator manipulates (adds or
deletes) exactly one sentence of the state description, if certain conditions on the other
sentences are fulfilled. The representation of two of these operators is shown on the right
4. The dashed lines do not represent operators and are only introduced to make the shape of the cube more
easy to see.

58

fiBuilding and Refining Abstract Planning Cases

Z

011

111

Two example operators:
010

110

X Z
001

E2
Y
X

000

101

E3
E1

100

O100->101 :
Precondition:
E1 and
(not E2) and
(not E3)

O101->100 :
Precondition:
E1 and
(not E2) and
E3

Add: E3

Delete: E3

Y

Figure 1: State space of an example domain and representation of two operators
side of this figure. The subscript of the operator name relates to the respective transition
in the state diagram. In general, we can see that

 E 1 can be manipulated, if and only if E 2  E 3 holds,
 E 2 can be manipulated, if and only if E 1  E 3 holds, and
 E 3 can be manipulated, if and only if E 1 _ E 2 holds.
Furthermore, assume that there are many more operators which connect some other states
of the domain, not shown in the diagram, to the 8 depicted states. Consequently, we
must assume a branching factor of b  1 at each state, which makes the search space for
problem solving quite large. Besides the description of the domain, Figure 1 also shows
three example problems: X ! X 0, Y ! Y 0 and Z ! Z 0 . For example, the solution to the
problem X ! X 0 is the 5-step path 000 ! 010 ! 110 ! 111 ! 101 ! 001.
Now, let's consider the abstract solutions which correspond to the concrete solutions for
each of the three problems. For each problem, we want to examine the three possible ways of
abstraction by dropping one of the sentences. For this purpose, the geometric arrangement
of the states turns out to be very useful because abstraction can be simply viewed as
projecting the 3-dimensional state space onto the plane defined by the sentences which are
not dropped by abstraction. The left part of Figure 2 shows the three possible abstract
state spaces which result from dropping one of the sentences. Here it is very important
to see that in each abstract state space, every sentence can be modified unconditionally
and independent of the other sentences. However, only one sentence can be modified by
each operator. Thereby, all the constraints that exist at the concrete level are relaxed.
The abstraction of the concrete solution to each of the three problems (X ! X 0, Y ! Y 0
and Z ! Z 0) with respect to the three possible ways of dropping conditions is shown on
59

fiBergmann & Wilke

State spaces by
dropping conditions

X->X

Y->Y

E2

4

E2
1

2

1

2
3
E1

2

2

E1
E3

Z->Z

3

1

1

E3
4

3

1

E1
4

E2 2

1
3

2

E2

3

1
3

2
1 E3

2
3

E1

3

1
E3

2

3

Figure 2: Abstract state spaces by dropping conditions
the right side of Figure 2. Each of the nine possible abstract solutions consists of three
or four abstract operators. The sequence in which they have to be applied is indicated
by the numbers which mark these operators. We can also see that whatever sentence we
drop for any of the problems, an appropriate abstract solution exists which decomposes
the original problem into independent refinable subproblems of suciently equal size. The
main point about this example is that none of these abstract solutions will be found by a
hierarchical problem solver! The reason for this is that for each of the abstracted problems
there also exists a 0-step or a 1-step solution in addition to the nine 3-step or 4-step solutions
indicated by the depicted paths. However, such a short solution is completely useless for
reducing the search at the next more concrete level because the original problem is not
decomposed at all. The central problem with this is that most problem solvers will find
the shorter but useless solutions first, and try to refine them. Consequently, the search
space on the concrete level is not reduced so that no performance improvement is achieved
at all. However, there might be other representations of this example domain in which a
hierarchical problem solver comes to a useful abstract solution. We think, however, that
the representation shown is quite natural because it represents the 8 different states with
the minimal number of binary sentences.
To summarize, we presented an example in which a useful abstract solution is not found
by hierarchical planning although it exists. The reason for this is that planners usually try
to find shortest solutions, which is a good strategy for the ground level, but which may
not be appropriate at the abstract level. Neither it is desirable to search for the longest
solutions because this might cause unnecessarily long concrete plans.

3. Case Abstraction and Refinement

As a way out of this problem, we propose to use experience given in the form of concrete
planning cases and to abstract this experience for its reuse in new situations. Therefore,
we need a powerful abstraction methodology which allows the introduction of a completely
new abstract terminology at the abstract level. This makes it possible that useful abstract
solutions can be expressed for domains in which abstraction by dropping conditions is not
sucient. In particular, this methodology must not only serve as a means to analyze
different abstraction approaches, but it must allow ecient algorithms for abstracting and
refining problems and solutions.
60

fiBuilding and Refining Abstract Planning Cases

3.1 The Basic Idea
We now introduce an approach which achieves case abstraction and refinement by changing
the representation language. As a prerequisite, this approach requires that the abstract
language itself (state description and operators) is given by a domain expert in addition to
the concrete level description. We also require that a set of admissible ways of abstracting
states is implicitly predefined by a generic abstraction theory. This is of course an additional
knowledge engineering requirement, but we feel that this is a price we have to pay to
enhance the power of hierarchical problem solving. Recent research on knowledge acquisition
already describes approaches and tools for the acquisition of concrete level and abstract level
operators in real-world domains (Schmidt & Zickwolff, 1992; Schmidt, 1994). An abstract
language which is given by the user has the additional advantage that abstracted cases are
expressed in a language with which the user is familiar. Consequently, understandability and
explainability, which are always important issues when applying a system, can be achieved
more easily.
As a source for learning, we assume a set of concrete planning cases, each of which
consists of a problem statement together with a related solution. As is the case in Prodigy
(Minton et al., 1989), we only consider sequential plans, i.e., plans with totally ordered
operators. The planning cases we assume do not include a problem solving trace as for example the problem solving cases in Prodigy/Analogy (Veloso, 1992; Veloso & Carbonell,
1993; Veloso, 1994). In real-world applications, a domain expert's solutions to previous
problems are usually recorded in a company's filing cabinet or database. These cases can
be seen as a collection of the company's experience, from which we want to draw power.
During a learning phase, a set of abstract planning cases is generated from each available
concrete case. An abstract planning case consists of an abstracted problem description
together with an abstracted solution. The case abstraction procedure guarantees that the
abstract solution contained in an abstract case can always be refined to become a solution
of the concrete problem contained in the concrete case that became abstracted. Different
abstract cases may be situated at different levels of abstraction or may be abstractions
according to different abstraction aspects. Different abstract cases can be of different utility
and can reduce the search space at the concrete level in different ways. It can also happen
that several concrete cases share the same abstraction. The set of all abstract planning cases
that are learned is organized in a case-base for ecient retrieval during problem solving.
During the problem solving phase, this case base is searched until an abstract case is
found which can be applied to the current problem in hand. An abstract case is applicable
to the current problem if the abstracted problem contained in the abstract planning case
is an abstraction of the current problem. However, we cannot guarantee that an abstract
solution contained in a selected abstract case can really be refined to become a solution to
the current problem. It is at least known that each abstract solution from the case base
was already useful for solving one or more previous problems, i.e., the problems contained
in those concrete cases from which the abstract case was learned. Since the new problem is
similar to these previous problems because both can be abstracted in the same way, there is
at least a high chance that the abstract solution is also useful for solving the new problem.
When the new problem is solved by refinement a new concrete case arises which can be
used for further learning.
61

fiBergmann & Wilke

Learning

The PARIS-System

Evaluation/
Indexing

Case Base

Generalization

Abstraction

Problem Solving
Retrieval

Domain Description
Concrete Domain,
Abstract Domain,
Generic Abstraction Theory

New Problem

Specialization

Refinement

Solved Problem

Training Cases

Figure 3: The components of the Paris System

3.2 The PARIS Architecture
Paris (Plan Abstraction and Refinement in an Integrated System) follows the basic ap-

proach just described. Figure 3 shows an overview of the whole system and its components.
Besides case abstraction and refinement, Paris also includes an explanation-based approach
for generalizing cases during learning and for specializing them during problem solving. Furthermore, the system also includes additional mechanisms for evaluating different abstract
cases and generalizations derived by the explanation-based component. This evaluation
component measures the reduction in search time caused by each abstract plan when solving those concrete problems from the case base for which the abstract plan is applicable.
Based on this evaluation, several different indexing and retrieval mechanisms have been developed. In these retrieval procedures those abstract cases are preferred which have caused
the most reduction in search during previous problem solving episodes. In particular, abstract cases which turn out to be useless for many concrete problems may even become
completely removed from the case-base. The spectrum of developed retrieval approaches
ranges from simple sequential search, via hierarchical clustering up to a sophisticated approach for balancing a hierarchy of abstract cases according to the statistical distribution of
the cases within the problem space and their evaluated utility. More details on the generalization procedure can be found in (Bergmann, 1992a), while the evaluation and retrieval
mechanisms are reported in (Bergmann & Wilke, 1994; Wilke, 1994). The whole multistrategy system including the various interactions of the described components will be the
topic of a forthcoming article, while first ideas can already be found in (Bergmann, 1992b,
1993). However, as the target of this paper we will concentrate on the core of Paris, namely
the approach to abstraction and refinement.
62

fiBuilding and Refining Abstract Planning Cases

State Abstractions
Abstract Plan

A3

011

111

A2
010

110

001

A1

100

A1

A2

A3

A4

000
011
010

110

111

001
101
100

Changing
Representration

101

A4

000

Figure 4: An example of case abstraction

3.3 Informal Description of the Abstraction Approach

We first give an informal description of the abstraction approach in Paris, based on our
small example shown in Figure 1 to enhance the understanding of the subsequent formal
sections. Suppose that the solution to the problem X ! X 0 is available as a concrete problem solving experience. The task is now to learn an abstract case which can be beneficially
used to solve future problems such as Y ! Y 0 and Z ! Z 0 . This learning task must be
achieved within an abstraction approach which is stronger than dropping sentences. If we
look at Figure 4, it becomes obvious that by changing the representation a single abstract
case can be learned which is useful for all three concrete problems. The abstract plan shown
indicates which concrete states have to be abstracted towards a single abstract state, such
that a single abstract plan exists which is useful for all three problems.
3.3.1 Abstract Language and Generic Abstraction Theory

To achieve this kind of abstraction, our approach requires that the abstract language (states
and operators), as well as a generic abstraction theory is provided by the user. For the example in Figure 4, the abstract language must contain the new abstract sentences A1 ; : : : ; A4
and the three abstract operators which allow the respective state transitions. These abstract
operators, called Oai (i 2 f1; : : : ; 3g), can be defined as follows:
Operator: Oai
Precondition: Ai
Delete: Ai
Add: Ai+1

For each new abstract sentence, the user must provide a set of generic abstraction rules
which describe how this sentence is defined in terms of the available sentences of the con63

fiBergmann & Wilke

crete language. The generic abstraction theory defined by these rules specifies a set of
admissible state abstractions. For our example, the generic abstraction theory must contain the following two rules to define the new abstract sentence A1 : :E 1 ^ E 2 ! A1 and
:E 1 ^ :E 2 ^ :E 3 ! A1. In general, the definition of the generic abstraction theory does
not require that all state abstractions are noted explicitly. Abstract states can be derived
implicitly by the application of a combination of several rules from the generic abstraction
theory.
Besides the kind of abstraction described above, the user may also want to specify
a different type of abstraction which she/he also considers useful. For example, we can
assume that abstraction by dropping the sentence E 1 should also be realized. In this case,
the abstract language must contain a copy of the two sentences which are not dropped, i.e.,
the sentences E 2 and E 3. Therefore, the user5 may define two abstract sentences A5 and
A6 by the following rules of the generic abstraction theory: E 2 ! A5 and E 3 ! A6. Of
course, the respective abstract operators must also be specified.
Since the domain expert or knowledge engineer must provide the abstract language and
the generic abstraction theory, she/he must already have one or more particular kinds of
abstraction in mind. She/he must know what kind of details can be omitted when solving
a problem in an abstract fashion. With our approach, the knowledge-engineer is given the
power to express the kind of abstraction she/he considers useful.
3.3.2 Model of Case Abstraction

Based on the given abstract language and the generic abstraction theory, the abstraction of
a planning case can be formally described by two abstraction mappings: a state abstraction
mapping and a sequence abstraction mapping. These two mappings describe two dimensions
for reducing the level of detail in a case. The state abstraction mapping reduces the level
of detail of a state description while changing the representation language. For the case
abstraction indicated in Figure 4, the state abstraction mapping must map the concrete
states 000, 011 and 010 onto an abstract state described by the new sentence A1, and
simultaneously it must map all other concrete states occurring in the plan onto the respective
abstract states described by the new sentences A2 , A3 , and A4 . The sequence abstraction
mapping reduces the level of detail in the number of states which are considered at the
abstract level by relating some of the concrete states from the concrete case to abstract
states of the abstract case. While some of the concrete states can be skipped, each abstract
state must result from a particular concrete state. For example, in Figure 4, the abstraction
of the plan 000 ! 010 ! 110 ! 111 ! 101 ! 001 requires a sequence abstraction mapping
which relates the first abstract state described by A1 to the first concrete state 000, the
second abstract state described by A2 to the third concrete state 110, and so forth. In this
example, the second and the fifth concrete states are skipped.
3.3.3 Learning Abstract Planning Cases

The procedure for learning such abstract planning cases from a given concrete planning case
is decomposed into four separate phases. For our simple example, these phases are shown in
5. Please note that for abstraction by dropping sentences, we can also consider an ALPINE-like algorithm
which generates the required abstract language and the generic abstraction theory automatically.

64

fiBuilding and Refining Abstract Planning Cases

A5A6

A5

Ca 2
Phase-IV
Ca 1 A1

Oa 1

Phase-III

A2

Oa 2

Oa 3

A3

A4

Oa 3

Oa 2

Oa 1

A6

Oa 1

Oa 3

Phase-II

A1

A 1 A5

A 2 A5

A 3 A5 A6

A 4 A6

A 4 A6

Phase-I

000

010

110

111

101

001

Figure 5: The four phases of case abstraction for the solution to the problem X ! X 0
Figure 5. In phase-I, the states which result from the execution of the plan contained in the
concrete case are determined. Therefore, each operator contained in the plan (starting from
the first operator) is applied and the successor state is computed. This process starts at the
initial state contained in the case and leads to a final state, which should be the goal state
contained in the case. In phase-II, we derive all admissible abstractions for each concrete
state computed in the first phase. For this purpose, the generic abstraction theory is used
to determine all abstract sentences that can be derived from a respective concrete state by
applying the rules of the generic abstraction theory. Figure 5 shows the abstract sentences
that can be derived by the generic abstraction theory sketched above. For example, we can
see that for the second concrete state an abstract description can be derived which contains
two abstract sentences: the abstract sentence A1 required to achieve the type of abstraction
shown in Figure 4 and additionally the abstract sentences A5 required for abstraction by
dropping sentences. Please note that by this process, the representation language of states
is changed from concrete to abstract. The next two phases deal with the abstract operators.
As already stated, abstract operators are given in the abstract language provided by the
user. However, we do not assume operator abstraction rules which associate an abstract
operator to a single concrete operator or a sequence of concrete operators. The reason for
this is that such operator abstraction rules are extremely hard to acquire and even harder
to keep complete. In the next two phases of case abstraction, we search for transitions of
abstract states based on the available abstract operators. In phase-III, an acyclic directed
graph is constructed. An edge leads from an abstract state i to a successor abstract state
j (not necessarily to the next abstract state), if the abstract operator is applicable in state
i and its application leads to the state j . The definition of the abstract operators are
used in this process. The available abstract operators determine which transitions can be
included in the graph. Figure 5 shows the resulting graph, provided that the abstract
operators sketched in Section 3.3.1 are contained in the abstract language. In this graph
65

fiBergmann & Wilke

the transitions shown in plain line style result from the operators Oai , while the transitions
shown in dashed line style result from the operators required for abstraction by dropping
conditions.
In phase-IV the graph is searched for consistent paths from the initial abstract state to
the final abstract state. The paths must be consistent in the sense that in the resulting
path (i.e., an abstract plan) every abstract operator is correctly applicable in the state that
results from the previous operator. Moreover, the state abstraction which is required for
this abstract plan must not change within the plan. In Figure 5 two paths of this kind are
shown. The lower path represents the abstract planning case Ca1 (abstract initial and final
state together with the operator sequence) that results from the kind of abstraction shown
in Figure 4. The upper path represents the abstract planning case Ca2 that results from
abstraction by dropping the sentence E 1. This is the same abstract plan as shown in Figure
2 for the problem X ! X 0. Together with the two plans, the abstract state descriptions that
result from the operator application are shown. Please note that these state descriptions
are always a subset of the description which are derived by the generic abstraction theory.
For example, the description of the fourth abstract state derived in phase-II, contains the
sentences A3 ; A5; A6. This abstract state occurs in both abstract cases which are computed
in phase-IV. In the case Ca2 , the respective state is described by the sentences A5 and
A6 because these are the only sentences which result from the application of the operators
starting at the abstract initial state. In the case Ca1, the abstract state is described by
the sentence A3 because this sentence results from the application of the operator Oa2.
From this example we can see that the abstract operators have two functions. The first
function is to select some of the concrete states that become abstracted. For example, in
the abstract case Ca1, the second concrete state is skipped, even if the first and the second
concrete states can be abstracted to different abstract descriptions in phase-II. The reason
for this is that there is no abstract operator that a) leads from the first abstracted state
to the second abstracted state and which b) is also consistent with the other operators in
the rest of the path. The second function of the abstract operators is to select some of the
abstract sentences that are considered in the abstract planning case. For example, in the
abstract case Ca1 , the sentences A1 ; : : : ; A4 are considered while the sentences A5 and A6
are left out. The reason for this is that the abstract operators Oa1 ; Oa2; Oa3 which occur in
the plan don't use A5 and A6 in their precondition and don't manipulate these sentences.
After phase-IV is finished, a set of abstract planning cases is available. These planning
cases can then be stored in the case-base and used for further problem solving.
3.3.4 Selecting and Refining Abstract Cases

During problem solving, an abstract case must be selected from a case-base, and the abstract
plan contained in this case must be refined to become a solution to the current problem.
During case retrieval we must search for an abstract case which is applicable, i.e., it contains
a problem description that is an abstraction of the current problem. For example, assume
that the problem Y ! Y 0 should be solved after the case X ! X 0 was presented for learning.
In this situation the case-base contains the two abstract cases Ca1 and Ca2 shown in phaseIV of Figure 5. The abstract case Ca1 can be used for solving the new problem, because
the initial state 000 of the new problem can be abstracted to A1 by applying the generic
66

fiBuilding and Refining Abstract Planning Cases

Selected abstract case:

A1

Defined search spaces:

000

Refined solution:

000

Oa 1

010

A2

Oa 2

A3

?

?

110

111

Oa 3

A4

100

101

100

Figure 6: Refinement of an abstract case for the solution of the problem Y ! Y 0
abstraction theory. Similarly, the final state 100 can be abstracted to A4 . However, the
abstract case Ca2 is not applicable because the final abstract state cannot be abstracted to
A6. Consequently, the lower abstract case must be used. During plan refinement we can
refine the abstract operators sequentially from left to right as shown in Figure 6. Thereby
each abstract operator defines an abstract goal state, i.e., the state that results after the
execution of the operator. For example, the abstract operator Oa1 defines the abstract goal
A2. To refine an abstract operator, we search for a concrete operator sequence, starting
from the current concrete state (i.e., the initial state for the first operator), until a concrete
state is reached that can be abstracted to the desired goal state. If such a state is found
it can be used as a starting state for the refinement of the next abstract operator. For
the solution of the problem Y ! Y 0 , the refinement of the abstract operator Oa1 can be
achieved by a sequence of two concrete operators leading to the concrete state 110. This
concrete state is then used as a starting state to refine the next abstract operator Oa2.
This refinement procedure finishes if the last abstract operator is refined in a way that the
final concrete state is achieved. Please note that in this type of refinement the operators
themselves are not used directly, instead the sequence of states which results from their
execution are used. Alternatively, we could have also stored an abstract case as a sequence
of abstracted states. From our experience, storing a sequence of operators requires less
space than storing a sequence of states. This will become obvious when looking at the
domain that will be introduced in Section 8. Besides this the abstract operators play an
important role in the learning phase.

3.4 Relations to Skeletal Plans
A similar experience-based or case-based variant for finding an abstract solution can be
found in an early paper by Friedland and Iwasaki (1985) in which the concept of skeletal
plans is introduced. A skeletal plan is "[...] a sequence of generalized steps, which, when
instantiated by specific operations in a specific problem context will solve a given problem
[p.161]. [...] Skeletal plans exist at many levels of generality. At the most general level, they
are only a few basic plans, but these are used as `fall-backs', when more specific, easier to
refine plans cannot be found. [p. 164]." Skeletal plans are solutions to planning problems at
different levels of detail and are consequently abstract plans. During problem solving they
67

fiBergmann & Wilke

are recalled from a library and refined towards a concrete solution. So this approach can be
seen as an early idea for integrating abstraction and case-based reasoning. However, there
are several differences between the skeletal plan approach and the Paris approach. In the
skeletal plan approach no model of the operators (neither concrete nor abstract) is used to
describe the preconditions and effects of operators as is done in Paris. There is no explicit
notion of states and abstraction or refinement of states. Instead, the plan refinement is
achieved by stepping down a hierarchy of operators, guided by heuristic rules for operator
selection. In particular, no approach which supports the automatic acquisition of skeletal
plans was provided. Unfortunately, the skeletal plan approach has not yet been investigated
in as much detail as current work in the field of speedup-learning. There is neither a formal
model of skeletal planning nor empirical evaluations.
In the rest of this paper we will introduce and investigate the Paris approach more
formally.

4. Basic Terminology
In this section we want to introduce the basic formal terminology used throughout the rest
of this paper. Therefore we will define a formal representation for problem solving domains.
We want to assume that problem solving in general can be viewed as transforming an initial
state into a final state by using a sequence of operators (Newell & Simon, 1972). Following
a Strips-oriented representation (Fikes & Nilsson, 1971), the domain of problem solving
D = hL; E ; O; Ri is described by a first-order language6 L, a set of essential atomic sentences
E of L (Lifschitz, 1987), a set of operators O with related descriptions, and additionally, a
set of rules (Horn clauses) R out of L. The essential sentences (which must be atomic) are
the only sentences that are used to describe a state. A state s 2 S describes the dynamic
part of a situation in a domain and consists of a finite subset of ground instances of essential
sentences of E . With the symbol S , we denote the set of all possible states descriptions in a
domain, which is defined as S = 2E  , with E  = fe je 2 E and  is a substitution such that
e is groundg. In addition, the Horn clauses R allow the representation of static properties
which are true in all situations. These Horn clauses must not contain an essential sentence
in the head of a clause.
An operator o(x1; : : : ; xn) 2 O is described by a triple hPreo ; Addo; Deloi, where the
precondition Preo is a conjunction of atoms of L, and the add-list Addo and the deletelist Delo are finite sets of (possibly instantiated) essential sentences of E . Furthermore,
the variables occuring in the operator descriptions must follow the following restrictions:
fx1; : : : ; xng  V ar(Preo)  V ar(Delo) and fx1; : : : ; xng  V ar(Addo).7
An instantiated operator is an expression of the form o(t1; : : : ; tn ), with ti being ground
terms of L. A term ti describes the instantiation of the variable xi in the operator description. For notational convenience we define the instantiated precondition as well as the instantiated add-list and delete-list for an instantiated operator as follows: Preo(t1 ;:::;tn ) := Preo  ,
Addo(t1;:::;tn) := fa ja 2 Addog, Delo(t1;:::;tn) := fd  jd 2 Delo g, with hPreo ; Addo; Deloi is
6. The basic language is first order, but with the deductive rules given in Horn logic only a subset of the
full first-order language is used.
7. These restrictions can however be relaxed such that fx1 ; : : : ; xn g  V ar(Preo ) is not required. But the
introduced restriction simplifies the subsequent presentation.

68

fiBuilding and Refining Abstract Planning Cases

the description of the (uninstantiated) operator o(x1; : : : ; xn), and  = fx1 =t1 ; : : : ; xn =tn g
is the corresponding instantiation.
An instantiated operator o is applicable in a state s, if and only if s [ R ` Pre
holds.8
o o
An instantiated operator o transforms a state s1 into a state s2 (we write: s1 ,! s2 ) if and
only if o is applicable in s1 and s2 = (s1 n Delo ) [ Addo . A problem description p = hsI ; sG i
consists of an initial state sI together with a final state sG . The problem solving task
is to find a sequence of instantiated operators (a plan) o = (o1; : : : ; ol ) which transforms
ol
o1
the initial state into the final state (sI ,!
   ,!
sG ). A case C = hp; oi is a problem
description p together with a plan o that solves p.
The introduced Strips-oriented formalism for defining a problem solving domain is
similar in form and expressiveness to the representations typically used in general problem
solving or planning. A state can be described by a finite set of ground atoms in which
functions can also be used. Full Horn logic is available to describe static rules. The restriction to Horn clauses has the advantage of being powerful while allowing ecient proof
construction by using the well known SLD-refutation procedures (Lloyd, 1984). Compared
to the Prodigy Description Language (PDL) (Minton, 1988; Blythe et al., 1992) our language does not provide explicit quantification by a specific syntactic construct, but a similar
expressiveness can be reached by the implicit quantification in Horn clauses. Moreover, our
language does not provide any kind of type specification for constants or variables as in
PDL but we think that this is not a major disadvantage. Besides these points our language
is quite similar to PDL.

5. A Formal Model of Case Abstraction
In this section we present a new formal model of case abstraction which provides a theory
for changing the representation language of a case from concrete to abstract. As already
stated we assume that in addition to the concrete language the abstract language is supplied
by a domain expert. Following the introduced formalism, we assume that the concrete level
of problem solving is defined by a concrete problem solving domain Dc = hLc ; Ec; Oc ; Rci
and the abstract level of (case-based) problem solving is represented by an abstract problem solving domain Da = hLa ; Ea; Oa; Rai. For reasons of simplicity, we assume that both
domains do not share the same symbols9 . This condition can always be achieved by renaming symbols. In the remainder of this paper states and operators from the concrete
domain are denoted by sc and oc respectively, while states and operators from the abstract
domain are denoted by sa and oa respectively. The problem of case abstraction can now be
described as transforming a case from the concrete domain Dc into a case in the abstract
domain Da (see Figure 7). This transformation will now be formally decomposed into two
independent mappings: a state abstraction mapping ff, and a sequence abstraction mapping
fi (Bergmann, 1992c). The state abstraction mapping transforms a selection of concrete
state descriptions that occur in the solution to a problem into abstract state descriptions,
8. In the following, we will simply omit the parameters of operators and instantiated operators in case they
are unambiguous or not relevant.
9. Otherwise, a symbol (or a sentence) could become ambiguous which would be a problem when applying
the generic abstraction theory. It would be unclear whether a generic abstraction rule refers to a concrete
or an abstract sentence

69

fiBergmann & Wilke

abstract
domain:

Da

a
O1

a

s0

s 1a


concrete
domain:

Dc

s c0

a
O2

a
Oj

s ja


Oc1

 (0) = 0

s c1

Oc2

c

s2

Oc3

c

s3
 (1) = 3

a
O j+1

a
Om

Oci+1

Ocn


Oc4

Oci

s ci
 (j) = i

s am

s cn
 (m) = n

Figure 7: General idea of abstraction
while the sequence abstraction mapping specifies which of the concrete states are mapped
and which are skipped.

5.1 State Abstraction

A state abstraction mapping translates states of the concrete world into the abstract world.
Definition 1 (State Abstraction Mapping) A state abstraction mapping ff : Sc ! Sa is a
mapping from Sc , the set of all states in the concrete domain, to Sa, the set of all states in
the abstract domain. In particular, ff must be an effective total function.
This general definition of a state abstraction mapping does not impose any restrictions
on the kind of abstraction besides the fact that the mapping must be a total many-toone function. However, to restrict the set of all possible state abstractions to a set of
abstractions which a user considers useful, we assume that additional domain knowledge
about how an abstract state relates to a concrete state can be provided. This knowledge
must be expressed in terms of a domain specific generic abstraction theory A (Giordana,
Roverso, & Saitta, 1991).

Definition 2 (Generic Abstraction Theory) A generic abstraction theory is a set of Horn
clauses of the form ea a1 ; : : : ; ak . In these rules ea is an abstract essential sentence,
i.e., ea = Ea for Ea 2 Ea and a substitution  . The body of a generic abstraction rule
consists of a set of sentences from the concrete or abstract language, i.e., ai are atoms out
of Lc [ La .
Based on a generic abstraction theory, we can restrict the set of all possible state abstraction
mappings to those which are deductively justified by the generic abstraction theory.

Definition 3 (Deductively Justified State Abstraction Mapping) A state abstraction mapping ff is deductively justified by a generic abstraction theory A, if the following conditions
hold for all sc 2 Sc :
 if  2 ff(sc ) then sc [ Rc [ A `  and
 if  2 ff(sc ) then for all s~c such that s~c [ Rc [ A `  holds,  2 ff(~sc ) is also fulfilled.
70

fiBuilding and Refining Abstract Planning Cases

In this definition the first condition assures that every abstract sentence reached by
the mapping is justified by the abstraction theory. Additionally, the second requirement
guarantees that if an abstract sentence is used to describe an abstraction of one state, it
must also be used to describe the abstraction of all other states, if the abstract sentence can
be derived by the generic abstraction theory. Please note that a deductively justified state
abstraction mapping can be completely induced by a set ff  Ea with respect to a generic
abstraction theory as follows: ff(sc ) := f 2 ff jsc [ Rc [ A ` g. Unless otherwise stated
we always assume deductively justified state abstraction mappings. To summarize, the
state abstraction mapping transforms a concrete state description into an abstract state
description and thereby changes the representation of a state from concrete to abstract.
Please note that deductively justified state abstraction mappings need not to be defined
by the user. They will be determined automatically by the learning algorithm that will be
presented in Section 6.

5.2 Sequence Abstraction

The solution to a problem consists of a sequence of operators and a corresponding sequence
of states. To relate an abstract solution to a concrete solution, the relationship between
the abstract states (or operators) and the concrete states (or operators) must be captured.
Each abstract state must have a corresponding concrete state but not every concrete state
must have an associated abstract state. This is due to the fact that abstraction is always a
reduction in the level of detail (Michalski & Kodratoff, 1990), in this situation, a reduction
in the number of states. For the selection of those concrete states that have a corresponding
abstraction, the sequence abstraction mapping is defined as follows:

Definition 4 (Sequence Abstraction Mapping) A sequence abstraction mapping fi : N ! N

relates an abstract state sequence (sa0 ; : : : ; sam ) to a concrete state sequence (sc0; : : : ; scn ) by
mapping the indices j 2 f1; : : : ; mg of the abstract states saj into the indices i 2 f1; : : : ; ng
of the concrete states sci , such that the following properties hold:

 fi(0) = 0 and fi(m) = n: The initial state and the goal state of the abstract sequence

must correspond to the initial and goal state of the respective concrete state sequence.

 fi(u) < fi(v) if and only if u < v: The order of the states defined through the concrete
state sequence must be maintained for the abstract state sequence.

Note that the defined sequence abstraction mapping formally maps indices from the abstract
domain into the concrete domain. As an abstraction mapping it should better map indices
from the concrete domain to indices in the abstract domain, such as the inverse mapping
fi ,1 does. However, such a mapping is more inconvenient to handle formally since the
range of definition of fi ,1 must always be considered. Therefore we stick to the presented
definition.

5.3 Case Abstraction

Based on the two abstraction functions introduced, our intuition of case abstraction is
captured in the following definition.
71

fiBergmann & Wilke

Hierarchies of abstraction spaces
Dl
Different kinds of abstractions
D2

Da
Da1
Dc

Da

D1

Da2
D0

D0

Dc

Figure 8: Different kinds of abstractions (a) and abstraction hierarchies (b)

Definition 5 (Case Abstraction) A case Ca = hhsa0 ; sami; (oa1; : : : ; oam)i is an abstraction
of a case Cc = hhsc0; scn i; (oc1; : : : ; ocn)i with respect to the domain descriptions (Dc ; Da) if
oaj a
oci c
sci,1 ,!
si for all i 2 f1; : : : ; ng and saj,1 ,!
sj for all j 2 f1; : : : ; mg and if there
exists a state abstraction mapping ff and a sequence abstraction mapping fi , such that:
saj = ff(scfi(j)) holds for all j 2 f0; : : : ; mg.

This definition of case abstraction is demonstrated in Figure 7. The concrete space shows
the sequence of n operations together with the resulting state sequence. Selected states are
mapped by ff into states of the abstract space. The mapping fi maps the indices of the
abstract states back to the corresponding concrete states.

5.4 Generality of the Case Abstraction Methodology

In the following, we briey discuss the generality of the presented case abstraction methodology. We will see that hierarchies of abstraction spaces as well as different kinds of abstractions can be represented simultaneously using the presented methodology.
5.4.1 Different kinds of Abstractions

In general, there will be more than one possible abstraction of an object in the world.
Abstraction can be performed in many different ways. An example of two different abstractions of the same case has already been shown in the example in Figure 5. In this example,
two different abstractions (see the abstract cases Ca1 and Ca2) have been derived from the
same concrete case. Our abstraction methodology is able cope with different abstractions
in case they are specified by the user. Assume we are given one concrete domain Dc and
two different abstract domains Da1 and Da2 , each of which represents two different kinds
of abstraction. Furthermore, assume that both abstract domains do not share the same
symbols10 . We can always define a single abstract domain Da by joining the individual
abstract domains which then includes both kinds of abstractions (see Figure 8 (a)). This
property is formally captured in the following simple lemma.
10. If the abstract domains are not disjoint, symbols can be simply renamed to achieve this property.

72

fiBuilding and Refining Abstract Planning Cases

Lemma 6 (Joining different abstractions) If a concrete domain Dc and two disjoint abstract domains Da1 and Da2 are given, then a joint abstract domain Da = Da1 [ Da2 can be
defined as follows: Let Da1 = (La1; Ea1; Oa1; Ra1) and let Da2 = (La2; Ea2; Oa2; Ra2). Then
Da = Da1 [ Da2 = (La1 [ La2 ; Ea1 [ Ea2 ; Oa1 [ Oa2 ; Ra1 [ Ra2). The joint abstract domain
Da fulfills the following property: if Ca is an abstraction of Cc with respect to (Dc, Da1) or
with respect to (Dc , Da2), then Ca is also an abstraction of Cc with respect to (Dc ; Da).
5.4.2 Hierarchy of Abstraction Spaces

Most work on hierarchical problem solving assume a multi-level hierarchy of abstraction
spaces for problem solving (e.g., Sacerdoti, 1974; Knoblock, 1989). Even if the presented
approach contains only two domain descriptions, a hierarchy of abstract domains can simply
be mapped onto the presented two-level model as shown in Figure 8 (b). Assume that a
hierarchy of disjoint domain descriptions (D0; : : : ; Dl) is given. In particular, the domain
D+1 is assumed to be more abstract than the domain D . In such a multi-level hierarchy
of abstraction spaces, a case C at the abstraction level D is an abstraction of a case C0, if
there exists a sequence of cases (C1 ; : : : ; C ,1 ) such that Ci is out of the domain Di and Ci+1
is an abstraction of Ci with respect to (Di ; Di+1) for all i 2 f0; : : : ;  , 1g. Such a multilevel hierarchy of domain descriptions can always be reduced to a two-level description. The
abstract domain of this two-level description contains the union of all the levels from the
multi-level hierarchy. This property is formally captured in the following lemma.

Lemma 7 (Multi-Level Hierarchy) Let (D0; : : : ; Dl) be an arbitrary multi-level
hierarchy
S
l
of domain descriptions. For the two-level description (Dc , Da ) with Da =  =1 D and
Dc = D0 holds that: if Ca is an abstraction of Cc with respect to (D0; : : : ; Dl) then Ca is
also an abstraction of Cc with respect to (Dc , Da ).
Since we have shown that different kinds of abstractions as well as hierarchies of abstraction spaces can be directly represented within our two-level case abstraction methodology,
we can further restrict ourselves to exactly these two levels.

6. Computing Case Abstractions

We now present the Pabs algorithm (Bergmann, 1992c; Wilke, 1993) for automatically
learning a set of abstract cases from a given concrete case. Thereby, we assume that a
concrete domain Dc and an abstract domain Da are given together with a generic abstraction
theory A. We use the functional notation Ca 2 PABS(hDc ; Da; Ai; Cc) to denote that Ca is
an element of the set of abstract cases returned by the Pabs algorithm.
The algorithm consists of the four separate phases introduced in Section 3. In the
following we will present these phases in more detail.
In the first three phases, we require a procedure for determining whether a conjunctive
formula is a consequence of a set of Horn clauses. For this purpose, we use a SLD-refutation
procedure (Lloyd, 1984) which is given a set of Horn clauses (a logic program) C together
with conjunctive formula G (a goal clause). The refutation procedure determines a set of
answer substitutions 
 such that C ` G holds for all  2 
. We write 
 = SLD(C; G).
This SLD-refutation procedure performs some kind of backward-chaining and works as
73

fiBergmann & Wilke

follows. It selects a literal from the goal clause G (i.e., the left most literal) and searches
for a Horn clause in the logic program C that contains a literal in its head that unifies with
the selected goal literal. The selected literal is removed from G and the body (if not empty)
of the applied clause is added at the beginning of the goal clause. Then the most general
unifier of the goal literal and the head of the clause is applied to the whole new goal clause.
The resulting goal clause is called resolvent. This process continues until the goal clause
becomes empty or until no more resolvents can be built. In the former case, the goal has
been proven and an answer substitution is computed by composing the substitutions used
during resolution. Backtracking is used to look for possible other selections of applicable
Horn rules to determine alternative answer substitutions. The set of all answer substitutions
is returned as set 
. If the whole space of possible applications of the available Horn rules
has been searched unsuccessfully, the goal clause is not a consequence of the logic program
C and the SLD-refutation procedure terminates without an answer substitution (
 = ;).
This must not be confused with the situation in which an empty substitution is returned
(
 = f;g), if no variables occur in G. In phase-III of the Pabs algorithm, we will also require
the derivation trees in addition to the answer substitutions. Then we write  = SLD(C; G)
and assume that  is a set of pairs (;  ), where  is an answer substitution and  is a
derivation of C ` G .
In order to assure the termination of the SLD-refutation procedure we have to require
that the abstract domain and the generic abstraction theory is designed according to the
following principles11 :
 For each concrete state sc 2 Sc and each concrete operator oc 2 Oc where oc is
described by hPreoc ; Addoc ; Deloc i, SLD(sc [ Rc ; Preoc ) must lead to a finite set of
ground substitutions of all variables which occur in Preoc .
 For each state abstract sa 2 Sa and each abstract operator oa 2 Oa where oa is
described by hPreoa ; Addoa ; Deloa i, SLD(sa [ Ra; Preoa ) must lead to a finite set of
ground substitutions of all variables which occur in Preoa .
 For each state sc 2 Sc and each abstract essential sentence E 2 Ea, SLD(sc [Rc [A; E )
must lead to a finite set of ground substitutions of all variables which occur in E .
In the following the four phases of the Pabs algorithm are explained in detail.

6.1 Phase-I: Computing the Concrete State Sequence

As input to the case abstraction algorithm, we assume a concrete case Cc =
hhscI ; scGi; (oc1; : : : ; ocn)i. Note that (oc1; : : : ; ocn) is a totally ordered sequence of instantiated operators similar to the plans in Prodigy (Minton, 1988; Minton et al., 1989; Veloso
& Carbonell, 1993). In the first phase, the state sequence which results from the simulation
of problem solution is computed as follows:

11. At first glance, this restrictions seem a bit hard to achieve but if we take a closer look at it we can see
that this is the standard requirement for a (terminating) logic program (i.e., a Prolog program).

74

fiBuilding and Refining Abstract Planning Cases

Algorithm 1 (Phase-I: Computing the concrete state sequence)
sc0 := scI
for i := 1 to n do
if SLD(sci,1 [ Rc; Preoci ) = ; then STOP \Failure: Operator not applicable"
sci := (sci,1 n Deloci ) [ Addoci

end
if scG 6 scn then STOP \Failure: Goal state not reached"

oc

i
By this algorithm, the states sci are computed, such that sci,1 ,!
sci holds for all
i 2 f1; : : : ; ng. If a failure occurs the given plan is not valid, i.e., it does not solve the given
problem.

6.2 Phase-II: Deriving Abstract Essential Sentences

Using the derived concrete state sequence as input, the following algorithm computes a
sequence of abstract state descriptions (sai ) by applying the generic abstraction theory
separately to each concrete state.

Algorithm 2 (Phase-II: State abstraction)
for i := 0 to n do
sai := ;
for each E 2 Ea do

 := SLD(sci [ Rc [ A; E )
for each  2 
 do
sai := sai [ fE g
end
end
end
Please note that we have claimed that the domain theories are designed in a way that

 is finite and only contains ground substitution of all variables in E . Therefore, every
description sai consists only of ground atoms and is consequently a valid abstract state
description. Within the introduced model of case abstraction we have now computed a
superset for the outcome of possible state abstraction mappings. Each deductively justified
state abstraction mapping ff is restricted by ff(sci )  sai = fe 2 Sa jsci [ Rc [ A ` eg for all
i 2 f1; : : : ; ng. Consequently, we have determined all abstract sentences that an abstract
case might require.

6.3 Phase-III: Computing Possible Abstract State Transitions

In the next phase of the algorithm, we search for instantiated abstract operators which
can transform an abstract state s~ai  sai into a subsequent abstract state s~aj  saj (i < j ).
Therefore, the preconditions of the instantiated operator must at least be fulfilled in the
state s~ai and consequently in also sai . Furthermore, all added effects of the operator must
be true in s~aj and consequently also in saj .
75

fiBergmann & Wilke

Algorithm 3 (Phase-III: Abstract state transitions)
G := ;
for i := 0 to n , 1 do
for j := i + 1 to n do
for each o(x1; : : : ; xu) 2 Oa do
let hPreo ; Delo; Addoi be the description of o(x1; : : : ; xu)
 := SLD(sai [ Ra ; Preo)
for each h;  i 2  do
letAdd0o = faja 2 Addog
(* Compute all possible instantiations *)
(* of added sentences which hold in saj *)
M := f;g
(* M is the set of possible substitutions *)
(* initially the empty substitution. *)

for each a 2 Add0o do
M 0 := ;
for each  2 M do
for each e 2 saj do
if there is a substitution  such that a = e then M 0 := M 0 [ fg
end
end
M := M 0

end

(* Now, M contains the set of all possible substitutions *)
(* such that all added sentences are contained in saj *)

for each  2 M do
G := G [ fhi; j; o(x1; : : : ; xu );  ig
end
end
end
end
end

The set of all possible operator transitions are collected as directed edges of a graph which
vertices represent the abstract states. In the algorithm, the set G of edges of the acyclic
directed graph is constructed. For each pair of states (sai,saj ) with i < j it is checked
whether there exists an operator o(x1; : : : ; xu ) which is applicable in sai . For this purpose,
the SLD-refutation procedure computes the set of all possible answer substitutions  such
that the precondition of the operator is fulfilled in sai . The derivation  which belongs
to each answer substitution is stored together with the operator in the graph since it is
required for the next phase of case abstraction. This derivation is an \and-tree" where
each inner-node reects the resolution of a goal literal with the head of a clause and each
leaf-node represents the resolution with a fact. Note that for proving the precondition of
an abstract operator the inner nodes of the tree always refer to clauses of the Horn rule set
Ra, while the leave-nodes represent facts stated in Ra or essential sentences contained in
76

fiBuilding and Refining Abstract Planning Cases

sai . Then each answer substitution  is applied to the add-list of the operator leading to a
partially instantiated add-list Add0o. Note that there can still be variables in Add0o because

the operator may contain variables which are not contained in its precondition but may
occur in the add-list. Therefore, the set M of all possible substitutions  is incrementally
constructed such that a 2 saj holds for all a 2 Add0o. The completely instantiated operator
derived thereby is finally included as a directed edge (from i to j ) in the graph G.
By this algorithm it is guaranteed that each (instantiated) operator which leads from sai
to saj is applicable in sai and that all essential sentences added by this operator are contained
in saj . Furthermore, if the applied SLD-refutation procedure is complete (it always finds all
answer substitutions), then every instantiated operator which is applicable in sai such that
all essential sentences added by this operator are contained in saj is also contained in the
oai
graph. From this follows immediately that if ff(scfi (i,1)) ,!
ff(scfi(i) ) holds for an arbitrary
deductively justified state abstraction mapping ff and a sequence abstraction mapping fi ,
then hfi (i , 1); fi (i); oai;  i 2 G also holds.

6.4 Phase-IV: Determining Sound Paths

Based on the state abstractions sai derived in phase-II and on the graph G computed in the
previous phase, phase-IV selects a set of sound paths from the initial abstract state to the
final abstract state. A set of significant abstract sentences ff and a sequence abstraction
mapping fi are also determined during the construction of each path.
Algorithm 4 (Phase-IV: Searching sound paths)12
Paths := fh(); ;; (fi(0) = 0)ig
while there exists h(oa1; : : : ; oak); ff; fii 2 Paths with fi(k) < n do
Paths := Paths n h(oa1 ; : : : ; oak ); ff; fii
for each hi; j; oa;  i 2 G with i = fi(k) do
let E be the set of essential sentences contained in the derivation 
let ff0 = E [ Addoa [ ff
if for all  2 f1; :a: : ; kg holds:
o a
(safi ( ,1) \ ff0 ) ,!
(sfi ( ) \ ff0 ) and
a
o (sa \ ff0 ) then
(safi (k) \ ff0 ) ,!
j
Paths := Paths [ fh(oa1 ; : : : ; oak; oa); ff0; fi [ ffi(k + 1) = j gi g

end
end

CasesAbs := ;
for each h(oa1 ; : : : ; oak); ff; fii 2 Paths with fi(k) = n do
CasesAbs := CasesAbs [ fhhsa0 \ ff ; san \ ff i; (oa1 ; : : : ; oak)ig

end
return CasesAbs

While the construction of the sequence abstraction mapping is obvious, the set ff represents the image of a state abstraction mapping ff and thereby determines the set of sentences

12. Please note that h(oa1 ; : : : ; oak ); ff; fi i matches fh(); ;; (fi (0) = 0)ig with k = 0. The operator n denotes
set difference.

77

fiBergmann & Wilke

that have to be reached in order to assure the applicability of the constructed operator sequence. Note that from ff the state abstraction mapping ff can be directly determined as
follows: ff(sci ) = fe 2 ff jsci [ Rc [ A ` eg. The idea of the algorithm is to start with an
empty path. A path is extended by an operator from G in each iteration of the algorithm
until the path leads to the final state with the index n. New essential sentences ff0 may
occur in the proof of the precondition or as added effects of this new operator. The path
constructed so far must still be consistent according to the extension of the state description
and, in addition, the new operator must transform the sentences of ff correctly.
As a result, phase-IV returns all cases that are abstractions of the given concrete input
case with respect to concrete and abstract domain definitions and the generic abstraction
theory. Depending on the domain theory, more than a single abstract case can be learned
from a single concrete case as already shown in Figure 5.

6.5 Correctness and Completeness of the PABS Algorithm

Finally, we want to state again the strong connection between the formal model of case
abstraction and the presented algorithm. The algorithm terminates if the domain descriptions and the generic abstraction theory are formulated as required in the beginning of this
section, so that the SLD-resolution procedure always terminates. The algorithm is correct,
that is every abstract case computed by the Pabs algorithm is a case abstraction according
to the introduced model. If the SLD-refutation procedure applied in Pabs is complete every
case which is an abstraction according to Definition 5 is returned by Pabs. This property
is captured in the following theorem.

Theorem 8 (Correctness and completeness of the PABS algorithm) If a complete SLDrefutation procedure is used in the Pabs algorithm, then Case Ca is an abstraction of case Cc
with respect to (Dc ; Da) and the generic theory A, if and only if Ca 2 PABS(hDc ; Da; Ai; Cc).

6.6 Complexity of the Algorithm

The complexity of the algorithm is mainly determined by the phases III and IV. The worst
case complexity of phase-III is O(n2  C1  C2) where n is the length of the concrete plan
and C1 and C2 are dependent on the domain theories as follows: C1 = jOa j  j
j and C2 =
jAddOa j (jEajj
j)jAddOaj. Thereby, jOaj represents the number of abstract operators, j
j is
the maximum number of substitutions found by the SLD-refutation procedure, jAddOa j is
the maximum number of added sentences in an abstract operator, and jEaj is the number of
abstract essential sentences. The complexity of phase-IV can be determined as O(n  2(n,1) 
C1). If we assume constant domain theories the overall complexity of the Pabs algorithm can
be summarized as O(n  2(n,1) ). The exponential factor comes from possibly exponential
number of paths in a directed acyclic graph with n nodes if every state is connected to
every successor state. Whether a graph of this kind appears is very much dependent on
the abstract domain theory, because it determines which transitions of abstract states are
possible. This exponential nature does not lead to a time complexity problem in the domains
we have used. Additionally, we want to make clear that this computational effort must be
spent during learning and not during problem solving. If the time required for learning is
very long, the learning phase can be executed off-line.
78

fiBuilding and Refining Abstract Planning Cases

The space complexity of the algorithm is mainly determined by phase-III because all
derivations of the proofs of the abstract operators' preconditions must be stored. This can
sum up to n2  C1  C2 derivations in the worst case. This did not turn out to be a problem
in the domains we used because each derivation was very short (in most cases not more
3 inferences with static Horn rules). The reason for this is that the derivations relate to
abstract operators which very likely contain less preconditions than the concrete operators.

7. Refinement of Abstract Cases

In the previous section we have described how abstract cases can be automatically learned
from concrete cases. Now we assume a case-base which contains a set of abstract cases. We
want to show how these abstract cases can be used to solve problems at the concrete level.
Furthermore, we discuss the impact of the specific form of the abstract problem solving
domain on the improvement in problem solving that can be achieved.

7.1 Applicability and Refinability of Abstract Cases

For a given abstract case and a concrete problem description, the question arises in which
situations the abstract case can be refined to solve the concrete problem. For this kind of
refinability an a-posterior definition can be easily given as follows.

Definition 9 (Refinability of an abstract case) An abstract case Ca can be refined to solve
a concrete problem p if there exists a solution oc to p, such that Ca is an abstraction of
hp; oci.

Obviously, the refinability property is undecidable in general since otherwise planning itself
would be decidable. However, we can define the applicability of an abstract case as a
decidable necessary property for refinability as follows.

Definition 10 (Applicability of an abstract case) An abstract case Ca = hhsa0 ; sami,
(oa1 ; : : : ; oam)i can be applied to solve a concrete problem p = hscI ; scG i if there exists a state
abstraction mapping ff such that sai 2 Im(ff) for all i 2 f0; : : : ; mg and ff(scI ) = sa0 and
ff(scG ) = sam . Thereby, Im(ff) denotes the image of the state abstraction mapping ff, i.e., all

abstract states that can be reached.

For an applicable abstract case, it is at least guaranteed that the concrete initial and goal
states map to the abstract ones and that concrete intermediate states exists that can be
abstracted as required by the abstract case.
Even if applicability is a necessary precondition for refinability it does not formally
guarantee refinability, since the downward solution property (Tenenberg, 1988), which states
that every abstract solution can be refined, is a too strong requirement to hold in general
for our abstraction methodology. However, it is indeed guaranteed that each abstract case
contained in the case-base is already an abstraction of one or more previous concrete cases
due to the correctness of the Pabs algorithm used for learning. If one of the problems
contained in these concrete cases has to be solved again it is guaranteed that the learned
abstract case can be refined to solve the problem. Consequently, each abstract case in
the case-base can at least be refined to solve one problem that has occurred in the past.
79

fiBergmann & Wilke

Abstract solutions which are useless because they can never be refined to solve any concrete
problem will never be in the case-base and are consequently never tried in solving a problem.
Therefore, we expect that each abstract case from the case-base has a high chance of being
also refinable for new similar problems for which it can be applied.

7.2 Selecting an Applicable Abstract Case

To decide whether an abstract case can be applied to solve a concrete problem P , we
have to determine a suitable state abstraction mapping. Because we assume deductively
justified state abstraction mappings,
the required state abstraction mapping ff can always
S
m

a
be induced by the set ff = i=0 si as shown in Section 5.1. Consequently, Ca is applicable
to the problem p = hscI ; scG i if and only if sa0 = f 2 ff j scI [ Rc [ A ` g and sam = f 2
ff j scG [Rc [A ` g. Since every abstract case we use for solving a new problem has been
learned from another concrete case, it is known that for each abstract state sai there must
be at least one concrete state (from that previous concrete state) that can be abstracted via
ff to sai . Consequently, sai 2 Im(ff) holds. Together with the introduced restrictions on the
definition of A and Rc with respect to a complete SLD-refutation procedure (see Section
6), the applicability of an abstract case is decidable. Algorithm 5 describes the selection of
an applicable abstract case for a problem p = hscI ; scG i in more detail.
Algorithm 5 (Selection of an applicable abstract case)
saI := saG := ;

for each E 2 Ea do

 := SLD(S
scI [ Rc [ A; E )
a
a
sI := sI [ 2
 E
for each E 2 Ea do

 := SLD(sScG [ Rc [ A; E )
saG := saG [ 2
 E
repeat
repeat
Select a new case Ca = hhsa0 ; sam i; (oa1; : : : ; oam)i from the case base
with sa0  saI and sam  saG
if no more cases available then
refineDFID (scI ; (); ;; scG)
return the result of refineDFID
for i :=Sm1 to m , 1 do sai := (sai,1 n Deloai ) [ Addoai
ff := i=0 sai
until (saI \ ff) = sa0 and (saG \ ff) = sam
refineDFID (scI ; (sa1; : : : ; sam,1 ); ff; scG)
until refineDFID returns success(p)
return success(p)

At first, the initial and final concrete states of the problem are abstracted using the
generic abstraction theory. Thereby, an abstract problem description hsaI ; saG i is determined.
Then, in a pre-selection step, an abstract case is chosen form the case base. All of the
abstract sentences contained in the initial and final abstract state of this case must be
80

fiBuilding and Refining Abstract Planning Cases

contained in the abstracted problem description hsaI ; saG i. This condition, however, does not
guarantee that the selected case is applicable with respect to Definition 10. The set ff
of abstract sentences inducing the respective state abstraction mapping is computed and
the applicability condition is checked to test whether the selected case is applicable. If
the selected case is not applicable, a new case must be retrieved. If an applicable abstract
case has been determined the refinement algorithm refineDFID (see following section) is
executed. This algorithm uses the sequence of intermediate abstract states (sa1 ; : : : ; sam,1 ),
previously determined from the abstract plan of the case, to guide the search at the concrete
level. The operators contained in the abstract plan are not used anymore. The refinement
procedure returns success(p), if the refinement succeeds with the solution plan p. If the
refinement fails (the procedure returns failure), another case is selected. If no more cases
are available the problem is solved by pure search without any guidance by an abstract
plan.

7.3 Refining an Abstract Plan
The refinement of a selected abstract case starts with the concrete initial state from the
problem statement. The search proceeds until a sequence of concrete operations is found
which leads to a concrete state sc , such that sa1 = f 2 ff j sc [ Rc [ A ` g holds. The
applicability condition of the abstract case guarantees that such a state exists (sai 2 Im(ff))
but it is not guaranteed that the required concrete operator sequence exists too. Therefore,
this search task may fail which causes the whole refinement process to fail also. If the first
abstract operator can be refined successfully a new concrete state is found. This state can
then be taken as a starting state to refine the next abstract operator in the same manner.
If this refinement fails we can backtrack to the refinement of the previous operator and try
to find an alternative refinement. If the whole refinement process reaches the final abstract
operator it must directly search for an operator sequence which leads to the concrete goal
state scG . If this concrete goal state has been reached the concatenation of concrete partial
solutions leads to a complete solution to original problem.
This refinement demands for a search procedure which allows an abstract goal specification. All kinds of forward-directed search such as depth-first iterative-deepening (Korf,
1985b) or best-first search (Korf, 1993) procedures can be used for this purpose because
states are explicitly constructed during search. These states can then be tested to see if they
can be abstracted towards the desired goal. In Paris we use depth-first iterative-deepening
search described by Algorithm 6. This algorithm consists of two recursive procedures. The
top-level procedure refineDFID receives the concrete initial state scI , the concrete final state
scG , the sequence of intermediate abstract states S a = (sa1 ; : : : ; sak) derived from the abstract
case, as well as the set ff which induces the state abstraction mapping. This procedure
increments the maximum depth for the depth-first search procedure searchbounded up to the
maximum DeepMax. The procedure searchbounded performs the actual search. The goal for
this search is either an abstract state, i.e., the first abstract state in S a , or the concrete
goal state scG if all abstract state have already been visited. The procedure performs a
depth-first search by applying the available concrete operators and recursively calling the
search procedure with the concrete state scnew which results from the operator application.
81

fiBergmann & Wilke

If an abstract goal state has been reached it is removed from the list S a and the refinement
continues with the next abstract state which is then again the first one in the list.
Algorithm 6 (Refinement by depth-first iterative-deepening (DFID) search)
procedure refineDFID (scI ; S a; ff; scG)
Deep := 0

repeat

searchbounded (scI ; S a; ff; scG; Deep)
if searchbounded returns success(p) then return success(p)
Deep := Deep + 1 (* Search unsuccessful: Increment search deepness *)
until Deep = DeepMax

return failure

procedure searchbounded (scI ; S a; ff; scG; Deep)
if S a = () (* No more abstract goals: Test the concrete final goal *)
and scI = scG then return success(())
if S a = (sa1; : : : ; sak) (* At least one abstract goal *)
and for all e 2 sa1 holds: SLD(scI [ Rc [ A; e) =6 ;
and for all e 2 ff n sa1 holds: SLD(scI [ Rc [ A; e) = ;
then (* Abstract state reached: Refine next abstract operator *)
refineDFID (scI ; (sa2 ; : : : ; sak ); ff; scG)
if refineDFID returns success(p) then return success(p)
if Deep = 0 then return failure (* Maximum depth reached *)
(* Apply operators: Create successor states *)

for all oc 2 Oc do

 = SLD(scI [ Rc; Preoc ) (* 
 is the set of all possible operator instantiations *)
for each  2 
 do
scnew := (scI n (Deloc )) [ (Addoc  ) (* Create successor state *)
searchbounded (scnew ; S a; ff; scG ; Deep , 1) (* Continue search with new state *)
if searchbounded returns success(p) then return success((oc)  p))
return failure
Please note that this kind of refinement is different from the standard notion of refinement in hierarchical problem solving (Knoblock et al., 1991b). This is because there is
no strong correspondence between an abstract operator and a possible concrete operator.
Moreover, the justification structure of a refined abstract plan is completely different from
the justification structure of the abstract plan itself because of the completely independent
definition of abstract and concrete operators. Even if this is a disadvantage compared to the
usual refinement procedure used in hierarchical problem solving, the main computational
advantage of abstraction caused by the decomposition of the original problem into smaller
subproblems is maintained.

7.4 Alternative Search Procedures for Refinement

Besides the forward-directed search procedure currently used in Paris backward-directed
search as used in means-end analysis (Fikes & Nilsson, 1971) or in nonlinear partial-ordered
82

fiBuilding and Refining Abstract Planning Cases

planning (McAllester & Rosenblitt, 1991) can also be applied for refinement under certain
circumstances. Therefore, we would either require a state concretion function or we have
to turn the rules of the generic abstraction theory A into virtual concrete operators.
A state concretion function must be able to determine a single state or a finite set of
concrete states from a given abstract state together with the concrete problem description.
Thereby, the concrete problem description may help to reduce the number of possible concrete states. The derived state concretions can then be used as concrete goal states from
which a backward directed search may start.
Alternatively, we can turn the process of state concretion directly into the search procedure by representing each rule in the generic abstraction theory as a virtual abstract
operator. The precondition of a rule in the generic abstraction theory becomes the precondition of the virtual operator and the conclusion of the rule becomes a positive effect
of this operator. When using the virtual concrete operators together with the operators of
the concrete domain, a backward-directed planner can use the abstract state directly as a
goal for search. The part of the plan in the resulting solution which only consists of concrete operators (and not of virtual operators) can be taken as a refinement of the abstract
operator.

7.5 Criteria for Developing an Abstract Problem Solving Domain
The abstract problem solving domain and the generic abstraction theory used have an important impact on the improvement in problem solving that can be achieved. Therefore,
it is desirable to have a set of criteria which state how a \good\ abstract domain definition should look. Strong criteria allowing quantitative predictions of the resulting speedups
can hardly be developed. For other hierarchical planners such criteria don't exist either.
However, we can give a set of factors which determine the success of our approach. The
overall problem solving time is inuenced mainly by the following four factors: independent refinability of abstract operators, goal distance of abstract operators, concrete scope of
applicability of abstract operators, and the complexity of the generic abstraction theory.
7.5.1 Independent Refinability of Abstract Operators

Following Korf's analysis of hierarchical problem solving (Korf, 1987) introduced in
2, our plan refinement approach reduces the overall search space from bn to
PSection
m b(fi (i),fi (i,1)). Thereby, b is the average branching factor, n is the length of the coni=1
crete solution, and fi is the sequence abstraction mapping used in the abstraction of the
concrete case to the abstract case. As already mentioned, we cannot guarantee that an
abstract plan which is applicable to a problem can really be refined. Furthermore, Korf's
analysis assumes that no backtracking between the refinement of the individual abstract
operators is required which cannot be guaranteed. Some of the computational advantage
of abstraction is lost in either of these two cases.
However, if the abstract operators occurring in the abstract problem solving domain
fulfill the strong requirement of independent refinability, then it is guaranteed that every
applicable abstract case can be refined without any backtracking. An abstract operator oa
is independently refinable if for each sc , s~c 2 Sc and every state abstraction mapping ff if
83

fiBergmann & Wilke

o
ff(sc ) ,!
ff(~sc ) holds,
then there exists a sequence of concrete operators (oc1; : : : ; ock ) such
c
c
ok c
o1
that sc ,!
: : : ,!
s~ holds.
a

The problem with this requirement is that it seems much to hard to develop an abstract
problem solving domain in which all operators fulfill this requirement. Although we cannot
expect that all operators in the abstract problem solving domain are independently refinable,
a knowledge engineer developing an abstract domain should still try to define abstract
operators which can be independently refined in most situations, i.e., for most sc , s~c 2 Sc
and most state abstraction mapping ff an applicable abstract operator can be refined to a
concrete operator sequence. Although this notion of mostly independent refinability is not
formal we feel that it is practically useful when developing an abstract domain definition.
The more abstract operators that can be refined independently in many situations, the
higher is the chance that an abstract plan composed of these operators is also refinable.
7.5.2 Goal Distance of Abstract Operators

The goal distance (cf. subgoal distance, Korf, 1987) is the maximum length of the sequence
of concrete operators required to refine a particular abstract operator. The longer the goal
distance the larger is the search space required to refine the abstract operator. In particular,
the complexity of the search required to refine a complete abstract plan is determined
by the largest goal distance of the abstract operators that occur in the abstract plan.
Hence there is a good reason to keep the goal distance short. However, the goal distance
negatively interacts with the next factor, namely the concrete scope of applicability of
abstract operators.
7.5.3 Concrete Scope of Applicability of Abstract Operators

The concrete scope of applicability of an abstract operator specifies how many concrete
states can be abstracted to an abstract state in which the abstract operator is applicable,
and how many concrete states can be abstracted to an abstract state that can be reached
by an abstract operator. This scope is determined by the definition of the abstract operator
and by the generic abstraction theory which is responsible for specifying admissible state
abstractions. The concrete scope of applicability of the abstract operators determines the
applicability of the abstract plans that can be learned. An abstract plan which is only applicable to a few concrete problems is only of limited use in domains in which the problems
to be solved vary very much. Hence, the concrete scope of applicability of abstract operators should be as large as possible. Unfortunately, according to our experience, abstract
operators which have a large scope usually also have a larger goal distance and operators
with a short goal distance don't have a large scope of applicability. Therefore, a compromise
between these two contradicting issues must be found.
7.5.4 Complexity of the Generic Abstraction Theory

The fourth factor which inuences the problem solving time is the complexity of the generic
abstraction theory. This theory must be applied each time a new concrete state is created
during concrete level search. The more complex the generic abstraction theory, the more
time is required to compute state abstractions. Hence, the generic abstraction theory should
84

fiBuilding and Refining Abstract Planning Cases

not require complicated inferences and should avoid backtracking within the SLD-refutation
procedure.
Although these four factors don't allow a precise prediction of the expected problem
solving behavior of the resulting system, they provide a focus on what to consider when
designing an abstract problem solving domain and related generic abstraction theory.

8. An Example Domain: Process Planning in Mechanical Engineering
The Paris approach has been successfully tested with toy-domains such as the familiar
towers of Hanoi (Simon, 1975). For these domains, hierarchical problem solvers which use
a dropping sentence approach have also proven very useful (Knoblock, 1994).
This section presents a new example domain we have selected from the field of process planning in mechanical engineering and which really requires a stronger abstraction
approach.13 We have selected the goal of generating a process plan for the production of
a rotary-symmetric workpiece on a lathe. The problem description, which may be derived
from a CAD-drawing, contains the complete specification (especially the geometry) of the
desired workpiece (goal state) together with a specification of the piece of raw material
(called mold) it has to be produced from (initial state).
The left side of Figure 9 shows an example of a rotary-symmetric workpiece which has to
be manufactured out of a cylindrical mold.14 Rotary parts are manufactured by putting the
mold into the fixture (chuck) of a lathe. The chucking fixture, together with the attached
mold, is then rotated with the longitudinal axis of the mold as rotation center. As the
mold is rotated a cutting tool moves along some contour and thereby removes certain parts
of the mold until the desired goal workpiece is produced. Within this process it is very
hard to determine the sequence in which the specific parts of the workpiece have to be
removed and the cutting tools to be used. When a workpiece is chucked a certain area of
the workpiece is covered by the chucking tool and cannot be processed by a cutting tool.
Moreover, a workpiece can only be chucked if the area which is used for chucking is plain.
Otherwise the fixation would not be suciently stable. Hence, many workpieces are usually
processed by first chucking the workpiece on one side and processing the accessible area.
Then the workpiece is chucked at the opposite side and the area that was previously covered
can be processed. Processing the example workpiece shown in Figure 9 requires that the
workpiece is first chucked at the left side while the right side is processed. Then the processed
right side can be used to chuck the workpiece because the area is plain and allows stable
fixing. Hence, the left side of the workpiece including the small groove can be processed.
Now we explain the representation of this domain in more detail. The complete definition
of the domain can be found in Online Appendix 1. Several simplifications of the real
domain were required in order to obtain a domain definition that could be eciently handled
in a large set of experiments. One restriction is that we can only represent workpieces
with right-angled contour elements. For example, a conical contour cannot be represented.
Many different cutting and chucking tools are available in real-life process planning. We
13. This domain was adapted from the CaPlan-System (Paulokat & Wess, 1994), developed at the University of Kaiserslautern.
14. Note that this figure shows a 2-dimensional drawing of the 3-dimensional workpiece. The measure 1 in.
equals 25.4 mm.

85

fiBergmann & Wilke

An Example Workpiece

Grid Representation for the example workpiece

2 mm
18 mm

y
40 mm

165 mm

Raw
Material
Workpiece

10 mm

5
4
3
2

8 mm

1

2mm

(4,2)
(1,1)
1 2

3

4

x

6 mm
8 mm

Figure 9: An example workpieces with grid representation
have restricted ourselves to a single chucking tool and three different cutting tools. The
specification of the tools themselves have also been simplified. For example, the rotation
speed of workpiece and the feed of the cutting tool are also parameters that can play a
role when processing a workpiece. The impact of these parameters has also been neglected.
Despite these simplifications the remaining part of this real-world domain is not trivial and
represents a substantial subset of the most critical problems in this domain.

8.1 Concrete Domain
We now explain the concrete problem solving domain by giving a detailed description of
the states and the operators.
8.1.1 State Description

For the representation of this domain at the concrete level, the exact geometry of the
workpiece must be represented as a state, including the specific measures of each detail
of the contour. However, the complete workpiece can always be divided into atomic areas
which are always processed as a whole. Therefore the state representation is organized by
using a grid which divides the entire workpiece into several disjoint rectangular areas of
different sizes (see the right side of Figure 9). Together with a grid coordinate the specific
position and size of the corresponding rectangular area are represented. This grid is used
as a static part of the state description which does not change during planning. However
different problems require different grids. The specific shape of a workpiece during planning
is represented by specifying the status for each grid rectangle. In Table 1 the predicates
used to represent the workpiece are described in more detail.
Besides the description of the workpiece, the state representation also contains information about how the workpiece is chucked and which kind of cutting tool is currently used.
Table 2 describes the predicates which are used for this purpose.
86

fiBuilding and Refining Abstract Planning Cases

Predicate Description
xpos max The predicates xpos max(xgrid ) and ypos max(ygrid ) specify the size of the
ypos max grid in the direction of the x-coordinate and the y-coordinate respectively.
A state consists of exactly one instance of each of these predicates, e.g.,
xpos max(4) and ypos max(5) in the example shown in Figure 9.
grid xpos
grid ypos

The predicates grid xpos(xgrid ; xstart; xsize ) and grid ypos(ygrid ; ystart; ysize )
specify the geometrical position and size of grid areas in the direction of
the x-coordinate and y-coordinate respectively. The first argument of these
predicates specifies the coordinate of the grid areas, the second argument
declares the geometrical starting position, and the third argument specifies
the size of the grid areas. A state consists of exactly one instance of each of
these predicates for each different x-coordinate and y-coordinate. For the
example above, grid xpos(1,0,18), grid xpos(2,18,2), grid xpos(3,20,165),
grid xpos(4,185,40) specify the grid in x-direction and grid ypos(1,0,8), : : : ,
grid ypos(5,26,8) specify the grid in y-direction.

mat

The predicate mat(xgrid ; ygrid; status) describes the status of a particular
grid area specified by the coordinates (xgrid ; ygrid). The argument status
can be instantiated with one of the three constants raw, workpiece, or none.
The constant raw indicates that the specified area still consists of raw material which must be removed by further cutting operators. The constant
workpiece specifies that the area consists of material that belongs to the
goal workpiece. The constant none specifies that the area does not contain
any material, i.e., there was no material present in the mold or the material
has already been removed by previous cutting operations. One instance of
a mat predicate is required for each grid area to specify its current state.
While the previously mentioned predicates does not change during the execution of a plan, the mat predicate is changed by each cutting operator. In
particular, the initial state and the goal state of a problem differs in the status assigned to those grid areas that must become removed. For example,
in the initial state of the example shown above, the sentence mat(4,2,raw)
will be present while the final state contains the sentence mat(4,2,none).
Table 1: Essential sentences for the representation of the workpiece

8.1.2 Operators

A process plan to manufacture a certain workpiece consists of a sequence of operators. The
total order of the operators is not a problem for this domain because the manufacturing
steps are also executed sequentially on a lathe.15 We have chosen four different operators
15. However, there are also a few new brands of lathe machine which also allow parallel processing.

87

fiBergmann & Wilke

Predicate
chuck pos

Description
The predicate chuck pos(side) describes whether the workpiece is currently
chucked on either side. The parameter side can be instantiated with one
of the three constants none, right, or left. The constant none specifies that
the workpiece is not chucked at all and the constants right and left specify
that the workpiece is chucked at the respective side. Each state contains
exactly one instance of this predicate.

covered

The predicate covered(xmin ; xmax) specifies the areas of the workpiece which
are currently covered by the chucking tool. This predicate declares those
areas with an x-coordinate lying within the interval [xmin ; xmax] as being
covered. Covered areas cannot be processed by a cutting tool. A state
consist of exactly one instance of this predicate if the workpiece is chucked.

cut tool
The predicates cut tool(id) and cut direction(dir) specify a unique identicut direction fication (id) of the cutting tool which is currently used when an area is
processed and the direction (dir) in which the cutting tool moves. The parameter id can be any symbol that specifies a legal cutting tool described
by predicates included in the static rules Rc of the concrete domain description. The parameter dir can be instantiated by one of the three constants
left, right and center. The value left specifies that the cutting tool moves
from left to right, right specifies that the cutting tool moves from right to
left, and center specifies that the cutting tool move from outside towards
the center of the workpiece.

Table 2: Essential sentences for the representation of the chucking and cutting tools
to represent the chucking of a workpiece, the selection of a cutting tool, and the cutting
process itself. These operators are described in Table 3.
Manufacturing the workpiece shown in Figure 9 requires a 15-step plan as shown in
Figure 10. At first, the workpiece is chucked on the left side. Then a cutting tool is selected
which allows cutting from right to left. With this tool the indicated grid areas are removed.
Please note that the left side of the workpiece cannot be processed since it is covered by
the chucking tool. Then (see the right side of Figure 10), the workpiece is unchucked and
chucked on its right side. With a tool that allows processing from left to right, the upper
part of the mold is removed. Finally, a specific tool is used to manufacture the small groove.

8.2 Abstract Domain

In this example we can see that the small groove can be considered a detail which can be
processed after the basic contour of the workpiece has been established. The most important
characteristic of this example is that the right part of the workpiece is processed before the
left side of the workpiece. This sequence is crucial to the success of the plan. If the groove
88

fiBuilding and Refining Abstract Planning Cases

Operator
chuck

Description
The operator chuck(side) specifies that the workpiece is chucked at the
specified side. The side parameter can be instantiated with the constants
left and right. Chucking is only allowed if the workpiece is not chucked already and if the surface used for chucking is plain. As effect of the chucking
operation, respective instances of the predicate chuck pos and covered are
included in the state description.

unchuck

The operator unchuck specifies that the chucking of the workpiece is removed. This operation can only be executed if the workpiece is chucked already. As effect of this operation, the parameter of the predicate chuck pos
is changed to none and the predicate covered is deleted.

use tool

The operator use tool(dir; id) specifies which tool is selected for the subsequent cutting operators and in which direction the cutting tool moves.
The workpiece must be chucked before a tool can be chosen. The effect
of the operator is that respective instantiations of the predicates cut tool
and cut direction are added to the state. The parameters of the use tool
operator have the same definition as in the respective predicates.

cut

The operator cut(xgrid ; ygrid) specifies that the raw material in the grid
area indicated by the coordinates (xgrid; ygrid ) is removed. The effect of
this operator is that the predicate mat which specifies the status of this
particular area is changed from status raw to the status none. However, to
apply this operator several preconditions must be fulfilled. The workpiece
must be chucked and the chucking tool must not cover the specified area
and the area must be accessible by the cutting tool. Moreover, a cutting
tool which allows the processing of the selected area must already have been
selected. Each cutting tool imposes certain constraints on the geometrical
size of the area that can be processed with it. For details, see the full
description of the domain in Online Appendix 1.
Table 3: Concrete operators

would have been processed first the workpiece could never be chucked on the left side and
the processing of the right side would consequently be impossible. Domain experts told us
that this situation is not specific for the example shown. It is of general importance for
many cases. This fact allows us to select parts of the problem description and the solution
which can be considered as details from which we can abstract. Parts which are \essential"
must be maintained in an abstract case. We found out that we can abstract from the
detailed shape of the workpiece as long as we distinguish between the processing of the left
and right side of the workpiece. Furthermore, it is important to distinguish between the
rough contour of the workpiece and the small details such as grooves. We have developed
89

fiBergmann & Wilke

1. chuck(left)

7.-8. unchuck, chuck(right)

2.-6. use_tool(right, t2), cut(4,5),...,cut(4,2)

9.-12. use_tool(left,t1),cut(1,5),..,cut(3,5)
13.-15. use_tool(center,t3), cut(2,4), unchuck

Figure 10: A plan for manufacturing the workpiece
an abstract domain definition containing a new language for describing states and operators
based on this abstraction idea.
8.2.1 State Description

We introduce a new abstract grid which divides the workpiece into a left, a middle, and a
right area to abstract from the specific location of a concrete grid area. These areas are
called complex processing areas. Each area is assigned a particular status. Furthermore,
an abstract state contains the information whether a complex processing area contains
small contour elements (such as grooves), but not how these grooves exactly look like. To
abstract from the very detailed conditions for chucking a workpiece, an abstract state only
contains an approximation of these conditions, stating that a workpiece cannot be chucked
at a particular side, if this side contains small contour elements that have been already
processed. The predicates used to represent an abstract state are described in more detail
in Table 4.
8.2.2 Operators

We consider an abstract operator which completely processes one complex area of the
workpiece, an operator which only processes a complex area roughly, and an operator which
processes all the small grooves of a complex area. We also consider an abstract chucking
operator because the chucking has a strong impact on the overall plan. Table 5 shows the
available abstract operators.

8.3 Generic Abstraction Theory
The generic abstraction theory defines the sentences used to describe an abstract state (see
Table 4) in terms of the sentences of the concrete state (see Tables 1 and 2) by a set of
Horn rules. The definition of abstract sentence is explained in more detail in Table 6.
90

fiBuilding and Refining Abstract Planning Cases

Predicate
abs area state

Description
The predicate abs area state(area; status) describes the status of
each of the three complex processing areas. The argument area
specifies one of the complex processing areas left, middle, and right.
The argument status describes the status of the respective area.
The status can be either todo, rough, and ready. The status todo
specifies that the area needs some processing of large contour elements, while in a rough area only some small contour elements such
as grooves need to be processed. The status ready specifies that
the area is completed. An abstract initial state usually contains
one or more complex processing areas of the status todo, while in
the abstract goal state all complex processing areas have the status
ready.

abs small parts

The predicate abs small parts(area) specifies that the complex processing area (area) contains small contour elements that need to be
manufactured.

abs chuck pos

The predicate abs chuck pos(side) describes whether the workpiece
is currently chucked on either side. The parameter side can be
instantiated with one of the three constants none, right, or left. This
predicate has exactly the same meaning as the chuck pos predicate
at the concrete level. This predicate is not abstracted at all but
only renamed.

abs chuckable wp

The predicate abs chuckable wp(side) describes whether the workpiece can be be chucked at the left or right side if this side has been
completely processed.

Table 4: Essential sentences for describing an abstract state
We have strongly considered the factors that inuence the quality of a domain (see Section 7.5) during the development of the abstract problem solving domain and the generic
abstraction theory. Although none of the defined abstract operators is independently refinable, all of them are mostly independently refinable. The preconditions of each abstract
operator still contains approximations of the conditions that must be fulfilled in order to assure that a concrete operator sequence exist that refines the abstract operator. For example,
the predicate abs chuckable wp(side) is an approximation of the detailed condition (a plain
surface) required for chucking. The goal distance of each operator is quite different and
strongly depends on the problem to be solved. While the goal distance of the set fixation
operators is no more than two (possibly one unchuck operator followed by a chuck operator)
the goal distances of the other abstract operators are different. For example, the goal distance of the process ready operator depends on the number of concrete grid areas belonging
91

fiBergmann & Wilke

Operator
set fixation

Description
The operator set fixation(side) specifies that the workpiece is chucked
at the specified side. The side parameter can be instantiated with the
constants left, right and none. The constant none specifies that the
chucking is removed. Compared to the concrete operator chuck the
preconditions for chucking at a side have been simplified. The effect of
this operator is that the predicate abs chuck pos is modified.

process rough

The operator process rough(area) specifies that the complex processing
area (area) is being processed completely up to the small contour elements. The parameter area can be either left, middle, or right. The
precondition of this operator only requires that the workpiece is chucked
at a different side than area. The effect of this operator is that the predicate abs area state is modified.

process fine

The operator process fine(area) specifies that all small contour elements
of the complex processing area (area) are being processed. The parameter area can be either left, middle, or right. The precondition of this
operator only requires that the large contour elements of this side of
the workpiece are already processed and that the workpiece is chucked
at a different side. The effect of this operator is that the predicate
abs area state is modified.

process ready

The operator process ready(area) specifies that the indicated complex
area of the workpiece is being completely processed, including large and
small contour elements. The effect of this operator is that the predicate
abs area state is modified.
Table 5: Abstract operators

to the respective abstract area and containing material that needs to be removed. The
goal distance is the number of these gird areas, say c, plus the number of required use tool
operations (less than or equal to c). Hence, the goal distance is between c and 2c. Because
this goal distance can become very long for the more complex problems, the two operators
process rough and process fine are introduced. They only cover the processing of the small
and the large grid areas respectively and consequently have a smaller goal distance than
the process ready operator. While the goal distance of these two operators is smaller they
have a smaller concrete scope of applicability than the process ready operator. For example
the process ready operator can be applied in any state in which some arbitrary areas need
to be processed, but process fine can only be applied in states in which all large grid areas
are already processed.
Although we have only developed a simplified version of the whole domain of production planning in mechanical engineering for rotary symmetrical workpieces we feel that
92

fiBuilding and Refining Abstract Planning Cases

Abstract Predicate Description in terms of the predicates of the concrete domain
abs area state
The predicate abs area state(area; status) describes the status of
each of the three complex processing areas. The left processing area
consists of the areas of the concrete grid which are covered, if the
workpiece is chucked at the left side. Similarly, the right processing
area consists of those concrete grid areas which are covered if the
workpiece is chucked at the right side. The middle processing area
consists of those areas which are never covered by any chucking
tool. The status of a complex processing area is todo, if there exists
a concrete large grid area which belongs to the complex processing
area and which needs to be processed. A grid area is considered
as large if its size in direction of the x-coordinate is larger than 3
mm. The status of a complex processing area is rough, if all large
grid areas of the complex processing area are already processed
and if there exists a concrete small grid area which belongs to the
complex processing area and which needs to be processed. A gird
area is considered as small if its size in direction of the x-coordinate
is smaller or equal than 3 mm. The status of a complex processing
area is ready if all concrete grid areas which belong to the complex
processing area have been processed.
abs small parts

The sentence abs small parts(area) holds if there exists a small concrete grid area (size smaller or equal than 3 mm) which belongs to
the complex processing area and which needs to be processed.

abs chuck pos

The sentence abs chuck pos(side) holds if and only if the concrete
sentence chuck pos(side) holds.

abs chuckable wp

The predicate abs chuckable wp(side) describes whether the workpiece can still be chucked at the left or right side if this side is
completely processed. This sentence holds if the part of the desired
workpiece which belongs to respective side is completely plain. That
is, all concrete grid areas with the status workpiece range up to the
same y-coordinate.
Table 6: Generic abstraction theory

a domain expert together with a knowledge engineer will be able to define an abstract
domain representation and a generic abstraction theory for a complete domain. In particular, model-based interactive knowledge acquisition tools like MIKADO (Schmidt, 1994;
Schmidt & Zickwolff, 1992) can make such a complete modeling task much more feasible.
93

fiBergmann & Wilke

Case C1
Initial state

Goal State

Solution
1. chuck(left)
8.
chuck(right)
13. use_tool(center, t3)
2. use_tool(right,t2) 9.
use_tool(left,t1) 14. cut(2,4)
3.-6. cut(4,5),..., cut(4,2) 10.-12. cut(1,5),..., cut(3,5) 15. unchuck
7. unchuck

Problem
Abstraction

1.

2.- 6.

7.- 8.

9.- 12.

I.

II.

III.

IV.

13.- 14.

15.
Problem
Abstraction

Abstraction
V.

VI.

Abstract Case Ca
Abstract initial state
abs_area_state(left, todo)
abs_area_state(right,todo)
abs_small_parts(left)
abs_chuckable_wp(right)

Problem
Abstraction

Abstract Solution
I. set_fixation(left)
II. process_ready(right)
III. set_fixation(right)

Abstract goal state

IV. process_rough(left)
V. process_fine(left)
VI. set_fixation(none)

I.

II.

III.

1.

2.- 3.

4.- 5.

IV.

V.

VI.

11.- 14.

15.

abs_area_state(left, ready)
abs_area_state(right,ready)
abs_chuckable_wp(right)

Refinement
6.- 11.

New Case C2

Initial state

Solution Solution
1. chuck(left)
2. use_tool(right,t2)
3. cut(7,3)
4. unchuck

Problem
Abstraction

Goal State

5.
chuck(right),
12. use_tool(center, t3)
6.
use_tool(left,t1), 13. cut(2,2)
7.-11. cut(1,3),...,cut(5,3), 14. cut(4,2)
15. unchuck

Figure 11: Abstracting and Refining an Example Case

8.4 Abstracting and Refining a Process Planning Case

We now explain how the example case shown in Figure 9 can be abstracted and how
this abstract case can be reused to solve a different planning problem. This process is
demonstrated in Figure 11. The top of this figure shows the concrete planning case C1
already presented in Figure 9. This case is abstracted by the Pabs algorithm presented in
Section 6. The algorithm returns 6 different abstract cases16. One of these abstract cases
is shown in the center of the figure. The abstract solution plan consists of a sequence of 6
abstract operators. The sequence of the operators in the plan is indicated by the Roman
numerals. The particular abstraction is indicated between the concrete and the abstract case
and denotes which sequence of concrete operators is turned into which abstract operator.
16. The other 5 abstract cases differ from the shown abstract case in two aspects: In the shown abstract
solution the additional abstract step set fixation(none) can be inserted between the steps II and III. The
abstract step V can also be replaced by the abstract step process ready, or the abstract steps IV and V
together can be replaced by the abstract step process ready.

94

fiBuilding and Refining Abstract Planning Cases

The learned abstract case can now be used to solve the new problem C2 whose initial and
final concrete states are shown in the bottom of the figure. Even if the concrete workpiece
looks quite different from the workpiece in case C1 the abstract case can be used to solve
the problem. The reason for this is that the new workpiece also requires that the left and
right side must be processed. In particular the right side must also be processed before
the left side is processed because the left side contains two small grooves which prevent the
workpiece from being be chucked at that side after it is processed. However, we can see that
most abstract operators (in particular the operators II, VI, and V) are refined to completely
different sequences of concrete operators than those from which they were abstracted.
As already mentioned, the abstract operators used are not independently refinable but
only mostly independently refinable. Consequently, it can happen that an applicable abstract case cannot be refined. Figure 12 shows an example of a concrete planning problem
for which the abstract case shown in Figure 11 is applicable but not refinable. The reason
for this is the location of the small abstract part at the left side of the workpiece. This small
part consists of the concrete grid area (1,3) in which raw material must be removed. However, in this specific situation, this small part must be removed before the large parts, the
left side of the workpiece contains (the grid areas (2,3), (3,3), and (2,2)), can be removed.
The reason for this is that without removing this small part, the larger parts located right
of the small part cannot be accessed by any cutting tool that is able to cut the areas (2,3)
and (3,3). Consequently this problem can only be solved with the plan shown on the right
side of Figure 12. Unfortunately, this plan is not a refinement of the abstract plan shown in
Figure 11, because this abstract plans requires that the large parts must be removed before
the small parts are removed. Hence, the refinement of the operator process rough(left) fails.
In this situation the problem solver must select a different abstract plan.

9. Empirical Evaluation and Results
This section presents the results of an empirical study of Paris in the mechanical engineering domain already introduced. This evaluation was performed with the fully implemented
Paris system using only the abstraction abilities of the system. The generalization component was switched-off for this purpose. We have designed experiments which allow us
to judge the performance improvements caused by various abstract cases derived by Pabs.
Furthermore, we have analyzed the average speed-up behavior of the system with respect
to a large set of randomly selected training and test cases.

9.1 Planning Cases
For this empirical evaluation 100 concrete cases have been randomly generated. Each case
requires about 100-300 sentences to describe the initial or final state, most of which are
instances of the mat predicate. The length of the solution plans ranges from 6 to 18
operators. Even if the generated cases only represent simple problems compared to the
problems a real domain expert needs to solve, the search space required to solve our sample
problems is already quite large. This is due to the fact that the branching factor b is between
1:7 and 6:6, depending on the complexity of the problem. Hence, for a 18-step solution the
complete search space consists of 3:7  1015 states.
95

fiBergmann & Wilke

Solution

New Problem

y
2 mm
3
2

2mm

1

Workpiece

8 mm

1. chuck(left)
2. use_tool(right,t2)
3. cut(4,3)
4. cut(4,2)
5. unchuck
6. chuck(right)
7. use_tool(left,t1)
8. cut(1,3)
9. cut(2,3)
10. cut(3,3)
11. use_tool(center, t3)
12. cut(2,2)
13. unchuck

18 mm
abs_small_parts(left)
1 2

3

4

x

Figure 12: An Example Case in which the refinement of the abstract plan shown in Figure
11 fails.

The case generation procedure leads to solutions which are optimal or nearly optimal.
All solutions which require less than 10 steps are optimal solutions in the sense that they are
known to be the shortest solution to the problem they solve. All solutions which are longer
than 10 steps have been manually checked to see whether they contain steps which are
obviously redundant. Such redundant steps have been removed. Although these solutions
are not necessarily shortest solutions, they are nevertheless acceptably short.

9.2 Evaluating Abstraction by Dropping Sentences
At first we used the recent version of Alpine (Knoblock, 1993) together with Prodigy4 (Blythe et al., 1992) to check whether abstraction by dropping sentences can improve
problem solving in our domain represented as described in Section 8. Therefore, we used
only the concrete problem solving domain as domain theory for Prodigy. Unfortunately,
for this representation, Alpine was not able to generate an ordered monotonic abstraction
hierarchy. The reason for this is that Alpine can only distinguish a few different groups of
literals because only a few different literal names (and argument types) can be used in the
problem space. For example, Alpine cannot distinguish between the different sentences
which are described by the mat or the grid xpos predicate. But this is very important for
abstraction. We would like to drop those parts of the grid which represent small rectangles
such as grooves. However, this would require the examination of the measures associated
with a grid area (as argument) and also the relation to other surrounding grid areas. Therefore, which sentence to drop (or which criticalities to assign) cannot be decided statically by
the name of the predicate or the type of the arguments. All hierarchical planners including
96

fiBuilding and Refining Abstract Planning Cases

Prodigy and Alpine are highly dependent on the representation used, in particular if their

strategy is restricted to dropping sentences (Holte et al., 1994, 1995). However, there might
be another representation of our domain for which those hierarchical planners can improve
performance but we think that our representation is quite "natural" for our domain.
From this first trial we can conclude that the application domain and representation we
have chosen for the following experiments with Paris really require more than dropping
sentences to achieve an improvement by abstraction.

9.3 Evaluating the PARIS Approach

The first experiment with Paris was designed to evaluate the hypotheses that in our domain
there is a need (I) for changing the representation language during abstraction, and (II) for
reusing abstract cases instead of generating abstract solutions from scratch. To test these
hypotheses we rely on the time for solving the randomly generated problems using different
modes of the Paris system.
9.3.1 Experimental Setting
In this experiment we used the Paris system to solve the 100 problems from the randomly

generated cases. Thereby the goal of abstraction is to improve the concrete-level problem
solver, which performs a brute-force search with a depth-first iterative-deepening search
strategy (Korf, 1985a) as introduced in Section 7.3. The improvement is determined in
terms of problem solving time required to solve a single problem. Paris is used to solve
the 100 problems in three different modes:

 Pure search: The problem solver is used to solve each problem by pure search without
use of any abstraction.

 Hierarchical planning: In this mode Paris uses the introduced abstract domain. How-

ever, abstract cases are not recalled from a case library but they are computed automatically by search as in standard hierarchical planning, but using the new abstraction language. So, the problem solver first tries to search for a solution to the original
problem at the abstract domain and then tries to refine this solution. During this
hierarchical problem solving, backtracking between the two levels of abstraction and
between each subproblem can occur. Thereby, we used hierarchical planning with the
new abstraction methodology instead of dropping sentences.

 Reasoning from abstract cases: In this mode we first used Paris to learn all abstract
cases which come out of the 100 concrete cases. For each problem, all abstract cases
that exists according to our abstraction methodology are available when one of the
problems is to be solved. During problem solving we measured the time required for
solving each problem using every applicable abstract cases. Then, for each problem,
three abstract cases are determined: a) the best abstract case, i.e., the case which leads
to the shortest solution time, b) the worst abstract case (longest solution time) which
is an abstraction of the aspired solution case, and c) the worst applicable abstract
case is determined. The difference between b) and c) relates to the difference between
applicable and refinable abstract cases introduced in Section 7.1. An abstract case
97

fiBergmann & Wilke

selected in c) is applicable to the current problem, but might not be an abstraction
of the case from which the problem is taken. In b) only abstract cases are selected
which are indeed abstractions of the current problem, i.e., abstract cases which have
been previously learned from the case from which the problem is taken. These three
different cases are selected to figure out the impact of case selection (which is not
addressed in this paper) on the proposed method.
Although every problem can theoretically be solved by our brute-force search procedure,
the exponential nature of the search space avoids the solution of complex problems within
reasonable time. Therefore, a time-bound of 200 CPU seconds on a Sun Sparc-ELC
computer was introduced in each of the three modes described above. If this limit-bound
is exceeded the problem remains unsolved. Increasing this time-bound would increase the
number of solvable problems in each of the three modes.
9.3.2 Results

We have determined the solution time for each of the 100 problems in each of the described
modes. The average solution time as well as the number of problems that could be solved
within the time limit is shown in Table 7. We have determined these values for reasoning
from abstract cases separately for each of the three types of abstract cases. The significance
of the speedup results has be investigated by using a maximally conservative sign test
(Etzioni & Etzioni, 1994). Unfortunately it turned out that the speedup of hierarchical
planning over pure search was not significant. We also couldn't find a significant speedup
of reasoning from abstract cases when using always the worst applicable abstract case (c)
over pure search. This was due to the large number of doubly censored data (both problem
solvers cannot solve the problem within the time limit), which were counted against the
speedup hypothesis. However, the improvements of pure search by reasoning from refinable
abstract cases were significant (p < 0:000001) when using the best refinable case (a) and
when using the worst refinable case (b). Furthermore, it turned out that the speedup of
reasoning from refinable cases over hierarchical planning was also significant for an upper
bound of the p-value of 0:001. The mentioned p-value is a standard value used in statistical
hypothesis tests. It is the probability, assuming that the hypothesis does not hold, of
encountering data that favors the hypothesis as much or more than the observed data in
the experiment (Etzioni & Etzioni, 1994). Therefore a result is more significant if the
p-value is smaller. From this analysis, we can clearly see, that our two basic hypotheses
are supported by our experimental data. Even if not significant we can see a moderate
improvement in the problem solving time and in the number of solved problems when using
hierarchical planning with changing the representation language. Please remember that
hierarchical planning by dropping conditions did not lead to any improvement at all (see
Section 9.2). Obviously, changing the representation language during abstraction is required
to improve problem solving in our domain as stated in the first hypothesis (I).
Very strong support for the second hypothesis (II) can also be found in the presented
data. We can see significant speedups by reasoning from abstract cases over pure search and
even over hierarchical planning. Only if the worst abstract case is used for each problem
to be solved, the speedup is not significant and the problem solving behavior is slightly
worse than in hierarchical planning. Please note that this situations is extremely unlikely
98

fiBuilding and Refining Abstract Planning Cases

Problem solving mode
Average solution time (sec.) Solved problems
Pure search
156
29
Hierarchical planning
107
50
Reasoning from abstract cases
(a) Best refinable case
35
94
(b) Worst refinable case
63
79
(c) Worst applicable case
117
45
Table 7: Comparison of the average solution time per problem and the number of solved
problems within a time-bound of 200 seconds. The table compares pure search
(depth-first iterative deepening), hierarchical planning using the abstract problem solving domain, and reasoning from abstract cases with differently selected
abstract cases.
to happen at all. With a sophisticated indexing and retrieval of abstract cases this situation
can be avoided for the most part.

9.4 Evaluating the Impact of Different Training Sets
In one respect the previous experiment is based on a very optimistic assumption. We always
assume that all abstract cases required for solving a problem have been learned in advance.
This situation is not a realistic scenario for an application. Usually, one set of cases is
available for training the system while a different set of problems needs to be solved. So
we cannot assume that good applicable abstract cases are always available to solve a new
problem. Furthermore, the presented example also shows that the problem solving time can
vary a lot if different abstract cases are selected during problem solving. Therefore, we have
designed a new experiment to evaluate the improvements caused by the Paris approach in
a more realistic scenario.
9.4.1 Experimental Setting

We have randomly chosen 10 training sets of 5 cases and 10 training sets of 10 cases from
the 100 available cases. These training sets are selected independently from each other.
Then, each of the 20 training sets is used for a separate experiment. In each of the 20
experiments, those of the 100 cases which are not used in the particular training set are
used to evaluate the performance of the resulting system. Training set and test set are
completely independent by this procedure. During this problem solving task, we did not
determine the problem solving behavior for all applicable abstract cases, but we used a
simple automatic mechanism to retrieve one (hopefully a good) applicable abstract case
for a problem. Therefore, the cases are organized linearly in the cases base, sorted by the
length of the abstract plan contained in the case. The case base is sequentially searched
from longer to shorter plans until an applicable case is found. This heuristic is based on the
assumption that a longer abstract plan is more specific than a shorter abstract plan and
99

fiBergmann & Wilke

Size of training sets
(cases)
5
10

Number of abstract cases
minimum
maximum
average
7
15
9.1
8
25
14.2

Table 8: Comparison of the number of learned abstract cases for a) the 10 training sets
each of which consists of 5 concrete cases and b) the 10 training sets each of
which consists of 10 concrete cases. The table shows the minimum, the maximum,
and the average number of abstract cases learned from the 10 training sets of the
respective size.
Size of training sets
(cases)
5
10

Average problem solving time (sec.)
best set
worst set
average
43
89
59
35
76
56

Table 9: Comparison of the problem solving time required for reasoning from abstract cases
after separate training with a) the 10 training sets each of which consists of 5
concrete cases and b) the 10 training sets each of which consists of 10 concrete
cases. The table shows the average problem solving time per problem for the best,
the worst and the average training set out of the 10 training sets of each size.
divides the actual problem into more, but smaller subproblems. Consequently the longest
applicable plan should lead to the best improvement.
9.4.2 Results

We have statistically evaluated the second experiment. Table 8 shows the number of abstract
cases which could be learned from the different training sets. The minimum, the maximum
and the average number of abstract cases that could be learned from the 10 training sets of
the same size is indicated. Note that altogether 42 abstract cases can be learned if all 100
cases would have been used for training as in the previous experiment. From the 10 training
sets which contained 5 cases each, between 7 and 15 abstract cases could be learned. As
expected, if the size of the training set is increased more abstract cases can be learned.
Table 9 shows the average problem solving time after learning from the different sets. This
table also shows the minimum, the maximum and the average problem solving time for the
10 different training sets of the two sizes. We can see that the best training sets leads to
a problem solving time which is similar or only slightly worse than the optimum shown in
Table 7. Even in the average case, considerable improvements over the pure search and
hierarchical problem solving (compare Table 7 and Table 9) can be discovered. The same
100

fiBuilding and Refining Abstract Planning Cases

Size of training sets
(cases)
5
10

Percentage of Solved Problems
best set
worst set
average
91
68
83
94
74
86

Table 10: Comparison of the percentage of solved problems after separate training with
a) the 10 training sets each of which consists of 5 concrete cases and b) the 10
training sets each of which consists of 10 concrete cases. The table shows the
percentage of solved problems for the best, the worst and the average training
set out of the 10 training sets of each size.
Size of training sets
(cases)
5
10

Number of training sets with significant speedups over
pure search
hierarchical planning
p < 0:0005
p < 0:0005
p < 0:05
9
4
8
10
5
7

Table 11: Comparison of the significance (p-value) of the speedup results over pure search
and hierarchical planning after separate training with a) the 10 training sets each
of which consists of 5 concrete cases and b) the 10 training sets each of which
consists of 10 concrete cases. The table shows the number of training sets which
cause significant speedups for different p-values.
positive results can also be identified when looking at the percentage of solved problems,
shown in Table 10. Here we can also see that for the best training sets the number of solved
problems is close to the maximum that can be achieved by this approach. Even in the worst
training set considerably more problems could be solved than by pure search or hierarchical
planning.
Additionally all of the above mentioned speedup results were analyzed with the maximally conservative sign test as described in (Etzioni & Etzioni, 1994). Table 11 summarizes
the significance results for speeding up pure search and a hierarchical problem solver. It
turned out that 19 of the 20 training sets lead to highly significant speedups (p < 0:0005)
over pure search. For this hard upper bound on p-values only about half of the training
sets lead to significant differences between reasoning from abstract cases and hierarchical
planning. At a slightly higher upper bound of p < 0:05, about 3=4 of the training sets
caused a significantly better performance than hierarchical planning.
Altogether, the reported experiment showed that even a small number of training cases
(i.e., 5% and 10%) can already lead to strong improvements on problem solving. We can
see that not all abstract cases must be present, as in the first experiment, to be successful.
Furthermore, this experiment has shown that even a simple retrieval mechanism (sequential
101

fiBergmann & Wilke

Size of training sets
(cases)
5
10

Average percentage of solutions with
shorter/equal/longer solution length
shorter
equal
longer
20
54
26
22
50
28

Table 12: Comparison of the length of the solutions created through reasoning from learned
abstract cases and the solutions available in the concrete cases. The table shows
the average percentage of solutions with shorter/equal/longer solution length
after separate training with a) the 10 training sets each of which consists of 5
concrete cases and b) the 10 training sets each of which consists of 10 concrete
cases.
search) can select beneficial abstract cases from the library. Neither of the training situations
in the second experiment lead to results which are as worse as the worst case shown in Table
7.

9.5 Quality of the Produced Solutions

Although the main purpose of this approach is to improve the performance of a problem
solver, the quality of the produced solutions is also very important for a practical system.
The solution length can be used as a very simple criterion to determine the quality of a
solution. However, in general the quality of a solution should reect the execution costs
of a plan, the plans robustness, or certain user preferences (Perez & Carbonell, 1993).
Because such quality measures are very dicult to assess, in particular in our manufacturing
domain, we rely on this simple criterion also used for evaluating the quality of solutions in
Prodigy/Analogy (Veloso, 1992).
9.5.1 Experimental Setting

We have analyzed the solutions computed in the previous set of experiments to assess the
quality of the solutions produced by Paris. Therefore, the length of solutions derived
during problem solving, after learning from each of the 20 training sets, are compared to
the length of the nearly optimal solutions contained in the concrete cases.
9.5.2 Results

For each training set the length of each solution derived in the corresponding testing phase
is compared to the length of the solution noted in the concrete case. The percentage of
solutions with shorter, equal, or longer solution length is determined for each training set
separately, and the average over the 10 training sets with equal size is determined. Table
12 shows the result of this evaluation.
It turned out that there was no big difference in the quality results between the 20
training sets. In particular, the size of the training sets did not have a strong inuence on
102

fiBuilding and Refining Abstract Planning Cases

the results. In Table 12 we can see that between 72% (22% + 50%) and 74% (20% + 54%)
of the solutions produced are of equal or better quality than the solutions contained in the
concrete cases. Please note that the concrete cases used for testing are always different
from the cases used for training. Additionally, the solutions to which we compare the
results produced by Paris are already nearly optimal solutions due to the case generation
procedure.17 Taking this into account, these results are already fairly good.

9.6 Impact of the Abstract Problem Solving Domain
The experiments reported before were conducted with the concrete and abstract domain
representation presented in Section 8 and in Online Appendix 1. In this final experiment
the impact of the specific choice of an abstract problem solving domain is investigated.
9.6.1 Experimental Setting

We created a new abstract problem solving domain which is less constrained than the one
used before. For this purpose one operator was completely removed and certain conditions
of the remaining operators were removed also. In particular, the set fixation operator was
removed and the conditions abs chuck pos, abs chuckable wp, and chuck comp were removed
from the preconditions of the three remaining operators. Hence, the fact that the chucking
of a workpiece has an impact on the production plan is now neglected at the abstract level.
However, the concrete problem solving domain and the generic abstraction theory was not
modified at all. Consequently, chucking still plays an important role at the concrete level.
The set of experiments described in Section 9.4 was repeated with the less constrained
abstract problem solving domain but using the same training and testing sets as before.
9.6.2 Results

Table 13 and 14 summarize the results of these experiments. Table 13 shows the average
problem solving time which occurs after learning from the different training sets. It turns
out that for all training sets, learning improves the concrete level problem solver, but that
the speedup is much smaller than when using the original abstract problem solving domain
(cf. Table 7 and 9). In particular, none of the resulting speedups over concrete level problem
solving were significant. A similar result can be observed when comparing the percentage
of solved problems (see Figure 14). There is still a slight improvement in the number of
problems that could be solved after learning but the improvement is much smaller than
when using the original abstract problem solving domain (cf. Table 7 and 10).

17. In all cases up to one, the shorter solutions produced by Paris are only one step shorter than the solution
contained in the concrete case.

103

fiBergmann & Wilke

Size of training sets
(cases)
5
10

Average problem solving time (sec.)
best set
worst set
average
114
118
117
107
112
110

Table 13: Using a less constrained abstract problem solving domain: Comparison of the
problem solving time required for reasoning from abstract cases after separate
training with a) the 10 training sets each of which consists of 5 concrete cases
and b) the 10 training sets each of which consists of 10 concrete cases. The table
shows the average problem solving time per problem for the best, the worst and
the average training set out of the 10 training sets of each size.

Size of training sets
(cases)
5
10

Percentage of Solved Problems
best set
worst set
average
55
52
53
58
54
56

Table 14: Using a less constrained abstract problem solving domain: Comparison of the
percentage of solved problems after separate training with a) the 10 training sets
each of which consists of 5 concrete cases and b) the 10 training sets each of which
consists of 10 concrete cases. The table shows the percentage of solved problems
for the best, the worst and the average training set out of the 10 training sets of
each size.
This experiment supported the general intuition that the abstract problem solving domain has a significant impact on the improvement in problem solving that can be achieved
through reasoning from abstract cases. The reason why the less constrained domain leads
to worse results than the original abstract domain can be explained with respect to the
criteria explained in Section 7.5. Since important preconditions of the abstract operators
were removed there are many situations in which the new operators cannot be refined. This
holds particularly for those situations in which a workpiece cannot be chucked to perform
the required cutting operations. The new abstract operators are not mostly independently
refinable. Moreover, since the abstract operator set fixation is removed the concrete chuck
and unchuck operator must be introduced during the refinement of the remaining abstract
operators. Consequently, the goal distance of these abstract operators is increased. These
two factors are the reason for worse results when using the less constrained abstract domain
theory.
104

fiBuilding and Refining Abstract Planning Cases

10. Discussion
In this paper we have shown in detail that in hierarchical problem solving (Sacerdoti, 1974;
Tenenberg, 1988; Unruh & Rosenbloom, 1989; Yang & Tenenberg, 1990; Knoblock, 1990)
the limited view of abstraction by dropping sentences as well as the strategy by which
abstract solutions are computed lead to poor behavior in various relevant situations. This
observation is supported by comprehensive artificial examples (see Section 2.1 and 2.2) and
a real-world example from the domain of mechanical engineering (see Section 8), further
supported by an experiment (see Section 9.2). The recent results reported in (Holte et al.,
1995) support these observations very well.
In general, abstraction is the task of transforming a problem or a solution from a concrete representation into a different abstract representation, while reducing the level of
detail (Michalski & Kodratoff, 1990; Giunchiglia & Walsh, 1992; Michalski, 1994). However, in most hierarchical problem solvers, the much more limited view of abstraction by
dropping sentences is shown to be the reason why ecient ways of abstracting a problem
and a solution are impossible (e.g., see Section 2.1 and Figure 4). The second weakness
of most hierarchical problem solvers is that they usually compute arbitrary abstract solutions and not solutions which have a high chance of being refinable at the next concrete
level. Although the upward solution property (Tenenberg, 1988) guarantees that a refinable abstract solution exists, it is not guaranteed that the problem solver finds this abstract
solution (e.g., see Section 2.2). Problem solvers are not even heuristically guided towards
refinable abstract solutions.
With the Paris approach we present a new formal abstraction methodology for problem
solving (see Section 5) which allows abstraction by changing the whole representation language from concrete to abstract. Together with this formal model, a correct and complete
learning algorithm for abstracting concrete problem solving cases (see Section 6) is given.
The abstract solutions determined by this procedure are useful for solving new concrete
problems, because they have a high chance of being refinable.
The detailed experimental evaluation with the fully implemented Paris system in the
domain of mechanical engineering strongly demonstrates that Paris can significantly improve problem solving in situations in which a hierarchical problem solver using dropping
sentences fails to show an advantage (see Table 7 to 11).

10.1 Related Work
We now discuss the Paris approach in relation to other relevant work in the field.
10.1.1 Theory of Abstraction

Within Giunchiglia and Walsh's (1992) theory of abstraction, the Paris approach can be
classified as follows: The formal system of the ground space 1 is given by the concrete
problem solving domain Dc using the situation calculus (Green, 1969) for representation.
The language of the abstract formal system 2 is given by the language of the abstract
problem solving domain Da . However, the operators of Da are not turned into axioms of
2 . Instead, the abstract cases build the axioms of 2 . Moreover, the generic abstraction
theory A defines the abstraction mapping f : 1 ) 2 . Within this framework, we can view
105

fiBergmann & Wilke

Paris as a system which learns useful axioms of the abstract system, by composing several

smaller elementary axioms (the operators). However, to prove a formula (the existence of
a solution) in the abstract system, exactly one axiom (case) is selected. So the deductive
machinery of the abstract system is restricted with respect to the ground space. Depending
on the learned abstract cases the abstractions of Paris are either theory decreasing (TD)
or theory increasing (TI). If the case-base of abstract cases is completely empty then no
domain axiom is available and the resulting abstractions are consequently TD. If the casebase contains the maximally abstract case hhtrue; truei(nop)i18 (and the generic abstraction
theory contains the clause ! true), then this case can be applied to every concrete problem
and the resulting abstraction is consequently TI. Even if this maximally abstract case does
not improve the ground level problem solving, it should be always included into the case-base
to ensure the TI property, that is not loosing completeness. The case retrieval mechanism
must however guarantee, that this maximally abstract case is only chosen for refinement if
no other applicable case is available. Note, that this is fulfilled for the retrieval mechanism
(sequential search from longer to shorter plans) we used in our experiments.
10.1.2 Skeletal Plans

As already mentioned in Section 3.4 the Paris approach is inspired by the idea of skeletal
plans (Friedland & Iwasaki, 1985). A abstract cases can be seen as a skeletal plan, and
our learning algorithm is a means to learn skeletal plans automatically out of concrete
plans. Even if the idea of skeletal plans is intuitively very appealing, to our knowledge, this
paper contains the first comprehensive experimental support of usefulness of planning with
skeletal plans. Since we have shown that skeletal plans can be acquired automatically, this
planning method can be applied more easily.
For the same purpose, Anderson and Farley (1988) and Kramer and Unger (1992) proposed approaches for plan abstraction which go in the same direction as the Paris algorithm.
However, this approach automatically forms abstract operators by generalization, mostly
based on dropping sentences. Moreover, in the abstracted plan, every concrete operator is
abstracted, so that the number of operators is not reduced during abstraction. Thereby
this abstraction approach is less powerful than Paris style abstractions.
10.1.3 Alpine's Ordered Monotonic Abstraction Hierarchies
Alpine (Knoblock, 1989, 1990, 1993, 1994) automatically learns hierarchies of abstraction

spaces from a given domain description or from a domain description together with a planning problem. As mentioned several times before, Alpine relies on abstraction by dropping
sentences. However, this enables Alpine to generate abstraction hierarchies automatically.
For a stronger abstraction framework such as the one we follow in Paris, the automatic
generation of abstraction hierarchies (or abstract domain descriptions) does not seem to
be realistic due to the large (infinite) space of possible abstract spaces. To use our powerful abstraction methodology, we feel that we have to pay the price of losing the ability to
automatically construct an abstraction hierarchy.
Another point is that the specific property of ordered monotonic abstraction hierarchies
generated by Alpine, allows an ecient plan refinement. During this refinement, an ab18. nop is the 'no operation' operator which is always applicable and does not change the abstract state.

106

fiBuilding and Refining Abstract Planning Cases

stract plan can be expanded at successively lower levels by inserting operators. Furthermore,
already established conditions of the plan are guaranteed not to be violated anymore during refinement. Unfortunately, this kind of refinement cannot be performed for Paris-style
abstractions. Especially, there is no direct correspondence between the abstract operators
and concrete operators. Consequently, an abstract plan cannot be extended to become a
concrete plan. However, the main function of the abstract plan is maintained, namely that
the original problem is decomposed into several smaller subproblems which causes the main
reduction in search.
10.1.4 Explanation-based Learning, Case-based Reasoning and Analogy

The presented Paris approach uses experience to improve problem solving, similar to several
approaches from machine learning, mostly from explanation-based learning (Mitchell et al.,
1986; DeJong & Mooney, 1986), case-based reasoning (Kolodner, 1980; Schank, 1982; Althoff & Wess, 1992; Kolodner, 1993) or analogical problem solving (Carbonell, 1986; Veloso
& Carbonell, 1988). The basic ideas behind explanation-based learning and case-based or
analogical reasoning are very much related. The common goal of these approaches is to
avoid problem solving from scratch in situations which have already occurred in the past.
Explanations (i.e., proofs or justifications) are constructed for successful solutions already
known by the system. In explanation-based approaches, these explanations mostly cover the
whole problem solving process (Fikes, Hart, & Nilsson, 1972; Mooney, 1988; Kambhampati
& Kedar, 1994), but can also relate to to problem solving chunks (Rosenbloom & Laird,
1986; Laird, Rosenbloom, & Newell, 1986) of some smaller size or even to single decisions
within the problem solving process (Minton, 1988; Minton et al., 1989). Explanation-based
approaches generalize the constructed explanations during learning by extensive use of the
available domain knowledge and store the result in a control rule (Minton, 1988) or schema
(Mooney & DeJong, 1985). In case-based reasoning systems like Priar (Kambhampati
& Hendler, 1992) or Prodigy/Analogy (Veloso & Carbonell, 1993; Veloso, 1994) cases
are usually not explicitly generalized in advance. They are kept fully instantiated in a
case library, annotated with the created explanations. Unlike cases in Paris which are
problem-solution-pairs, such cases are complete problem solving episodes containing detailed information of each decision that was taken during problem solving. During problem
solving, those cases are retrieved which contain explanations applicable to the current problem (Kambhampati & Hendler, 1992; Veloso & Carbonell, 1993; Veloso, 1994). The detailed
decisions recorded in these cases are then replayed or modified to become a solution to the
current problem. All these approaches use some kind of generalization of experience, but
none of these approaches use the idea of abstraction to speedup problem solving based on
experience. As already noted in (Michalski & Kodratoff, 1990; Michalski, 1994), abstraction and generalization must not be confused. While generalization transforms a description
along a set-superset dimension, abstraction transforms a description along a level-of-detail
dimension.
The only exception is given in (Knoblock, Minton, & Etzioni, 1991a) where Alpine's
abstractions are combined with EBL component of Prodigy. Thereby, control rules are
learned which do not refer to the ground space of problem solving but also to the abstract
spaces. These control rules speedup problem solving at the abstract level. However, the
107

fiBergmann & Wilke

control rules guide the problem solver at the abstract level so that it finds solutions faster
and not in a manner that it finds refinable abstract solutions. Although we did not have any
experience with this kind of integration of abstraction and explanation-based learning, we
assume that the control rules generated by the EBL component will also guide the problem
solver towards short abstract solutions which do not cause much reduction in search in
several circumstances.

10.2 Requirements and Limitations of PARIS

In the following, we will summarize again the requirements and limitations of the Paris
approach. The main requirements are the availability of a good abstract domain description
and in the availability of concrete cases.
10.2.1 Abstract Domain

The most important prerequisite of this method is the availability of the required background knowledge, namely the concrete world description, the abstract world description,
and the generic abstraction theory. For the construction of a planning system, the concrete
world descriptions must be acquired anyway, since they specify the \language\ of the problem description (essential sentences) and the problem solution (operators). The abstract
world and the generic abstraction theory must also be acquired. We feel that this is indeed
the price we have to pay to make planning more tractable in certain practical situations.
Nevertheless, the formulation of an adequate abstract domain theory is crucial to the
success of the approach. If those abstract operators are missing which are required to express
a useful abstract plan, no speedup can be achieved. What we need are mostly independently
refinable abstract operators. If such operators exist, they can be simply represented in the
abstract domain using the whole representational power. For hierarchical planning with
dropping conditions, such an abstract domain must also be implicitly contained in a concrete
domain in a way that the abstract domain remains, if certain literals of the concrete domain
are removed (see Section 2.1). We feel that this kind of modeling is very much harder to
achieve than modeling the abstract view of a domain explicitly in a distinct planning space
as in Paris. Additionally, the requirement that the abstract domain is given by the user
has also the advantage that the learned abstract cases are expressed in terms the user is
familiar with. Thereby, the user can understand an abstract case very easily. This can open
up the additional opportunity to involve the user in the planning process, for example in
the selection of an abstract cases she/he favors.
Research on knowledge acquisition has shown that human experts employ a lot of
abstract knowledge to cope with the complexity of real-world planning problems. Specific knowledge acquisition tools have been developed to comfortably acquire such abstract
knowledge from different sources. Especially, the acquisition of planning operators is addressed in much detail in (Schmidt & Zickwolff, 1992; Schmidt, 1994).
10.2.2 Availability of Cases
As a second prerequisite, the Paris approach needs concrete planning cases (problem-

solution pairs). In a real-world scenario such cases are usually available in a company's
filing cabinet or database. According to this requirement we share the general view from
108

fiBuilding and Refining Abstract Planning Cases

machine learning that the use of this kind of experience is the most promising way to cope
with highly intractable problems. For the Paris approach the available cases must be
somehow representative for future problem solving tasks. The known cases must be similar
enough to the new problems that abstract cases can really be reused. Our experiments
give strong indications that even a small set of concrete cases for training leads to high
improvements in problem solving (see Table 9 to 11).

10.3 Generality of the Achieved Results

The reported experiments were performed with a specific base-level problem solver which
performs a depth-first iterative-deepening search strategy (Korf, 1985a). However, we
strongly believe that the Paris abstractions are also beneficial for other problem solvers
using backward-chaining, means-end analysis or nonlinear partial-order planning. As shown
in (Veloso & Blythe, 1994), there is not one optimal planning strategy. Different planning
strategies usually rely on different commitments during search. Each strategy can be useful
in one domain but may be worse in others. However, for most search strategies, the length
of the shortest possible solution usually determines the amount of search which is required.
In Paris, the whole search problem is decomposed into several subproblems which allow
short solutions. Consequently, this kind of problem decomposition should be of use for most
search strategies.
Moreover, we think that the idea of reasoning from abstract cases, formulated in a
completely new terminology than the ground space will also be useful for other kinds of
problem solving such as design or model-based diagnosis. For model-based diagnosis, we
have developed an approach (Pews & Wess, 1993; Bergmann, Pews, & Wilke, 1994) similar
to Paris. The domain descriptions consist of a model of a technical system for which a
diagnosis has to be found. It describes the behavior of each elementary and composed
component of the system at different levels of abstraction. During model-based diagnosis,
the behavior of the technical system is simulated and a possible faulty component is searched
which can cause the observed symptoms. Using abstract cases, this search can be reduced
and focused onto components which have been already defective (in other similar machines)
and which are consequently more likely to be defective in new situations.

10.4 Future Work

Future research will investigate goal-directed procedures for refinement such as backwarddirected search or non-linear partial order planners (see Section 7.4). Additionally, more
experience must be gained with additional domains and different representations of them.
Furthermore, we will address the development of highly ecient retrieval algorithms for
abstract cases. As Table 7 shows, the retrieval mechanism has a strong inuence on the
achieved speedup. Even if the linear retrieval we have presented turned out to be pretty
good, we expect a utility problem (Minton, 1990) to occur when the size of the casebase grows. Furthermore, a good selection procedure for abstract cases should also use
some feedback from the problem solver to evaluate the learned abstract cases based on the
speedup they cause. This would eliminate unbeneficial cases or abstract operators from the
case-base or the abstract problem solving domain. Experiments with different indexing and
retrieval mechanisms have recently indicated that this is possible.
109

fiBergmann & Wilke

Furthermore, the speedup caused by a combination of different approaches such as
abstraction and explanation-based learning should be addressed. Within the Paris system
an explanation-based component for case generalization is still present (see Figure 3), but
was not used for the experiments because the plain abstraction itself had to be evaluated.
In further experiments, abstraction, explanation-based learning and the integration of both
has to be addressed comprehensively. This will hopefully lead to a better understanding of
the different strengths these methods have.
As a more long-term research goal, Paris-like approaches should be developed and
evaluated for other kinds of problem solving tasks such as configuration and design or, as
already started, for model-based diagnosis.

Appendix A. Proofs

This section contains the proofs of the various lemma and theorems.

Lemma 6 (Joining different abstractions) If a concrete domain Dc and two disjoint abstract domains Da1 and Da2 are given, then a joint abstract domain Da = Da1 [ Da2 can be
defined as follows: Let Da1 = (La1; Ea1; Oa1; Ra1) and let Da2 = (La2; Ea2; Oa2; Ra2). Then
Da = Da1 [ Da2 = (La1 [ La2 ; Ea1 [ Ea2 ; Oa1 [ Oa2 ; Ra1 [ Ra2). The joint abstract domain
Da fulfills the following property: if Ca is an abstraction of Cc with respect to (Dc, Da1) or
with respect to (Dc , Da2), then Ca is also an abstraction of Cc with respect to (Dc ; Da).
Proof: The proof of this lemma is quite simple. If Ca is an abstraction of Cc with respect
to (Dc , Dai), then there exists a sequence abstraction mapping ff and a sequence abstraction
mapping fi as required in Definition 5. As it is easy to see, the same abstraction mappings
will also lead to the respective case abstraction in (Dc ; Da). 2

Lemma 7 (Multi-Level Hierarchy) Let (D0; : : : ; Dl) be an arbitrary multi-level
S hierarchy
of domain descriptions. For the two-level description (Dc , Da ) with Da = l =1 D and
Dc = D0 holds that: if Ca is an abstraction of Cc with respect to (D0; : : : ; Dl) then Ca is
also an abstraction of Cc with respect to (Dc , Da ).
Proof: Let C = hhs0 ; smi; o i be a case in domain D (intermediate state are denoted by
sj ), let C0 = hhs00 ; s0n i; o0i be a case in domain D0 (intermediate state are denoted by s0i ),
and let C be an abstraction of case C0 with respect to (D0; : : : ; D ). Then a sequence of
cases (C1 ; : : : ; C ,1 ) exists such that Ci is from the domain Di and Ci+1 is an abstraction of
the case Ci with respect to (Di ; Di+1) for all i 2 f0; : : : ;  , 1g. Now we proof by induction
over  that C is also an abstraction of C0 with respect to (Dc , Da ) (see figure 13). The basis
( = 1) is obvious: C1 is abstraction of C0 with respect to (D0 ; D1) and is consequently also
an abstraction with respect to (Dc , Da ). Now, assume that the lemma holds for any cases up
to the domain D ,1 . It follows immediately that C ,1 is an abstraction of C0 with respect
to (Dc , Da ). Let C ,1 = hhs00; s0k i; o0i and let the intermediate states be denoted by s0r . From

Definition 5 follows, that a state abstraction mapping ff and a sequence abstraction mapping
fi exists, such that ff(scfi(r)) = s0r for all r 2 f0; : : : ; kg. Because C is an abstraction of C ,1
110

fiBuilding and Refining Abstract Planning Cases

D










D 1










D0

Figure 13: Abstraction mappings for hierarchies of abstraction spaces
with respect to (D ,1 ; D ), it also exists a state abstraction mapping ff0 and a sequence
abstraction mapping fi 0 such that ff0 (s0fi 0 (j ) ) = sj for all j 2 f0; : : : ; mg. Now, we can
define a state abstraction mapping ff00(s) = ff0 (ff(s)) and a sequence abstraction mapping
fi 00(j ) = fi(fi0(j )). It is easy to see, that ff00 is a well defined state abstraction mapping
(s  s0 ) ff(s)  ff(s0) ) ff0 (ff(s))  ff0 (ff(s0 ))) and that fi 00 is a well defined sequence
abstraction mapping (fi (fi 0 (0)) = 0 ; fi (fi 0(m)) = fi (k) = n ; u < v , fi 0(u) < fi 0 (v ) ,
fi (fi 0 (u)) < fi(fi0(v))). Furthermore it follows ff00(scfi 00 (j ) ) = ff0 (ff(scfi (fi 0(j )))) = ff0 (s0fi0(j ) ) = saj ,
leading to the conclusion that C is an abstraction of C0 with respect to (Dc , Da ). 2

Theorem 8 (Correctness and completeness of the Pabs algorithm) If a complete SLDrefutation procedure is used in the Pabs algorithm, then Case Ca is an abstraction of case Cc
with respect to (Dc ; Da) and the generic theory A, if and only if Ca 2 PABS(hDc ; Da; Ai; Cc).
Proof:

Correctness (\"): If Ca is returned by Pabs, then h(oa1 ; : : : ; oak); ff; fi i 2 Paths holds 19
in phase-IV. We can define a state abstraction mapping ff(s) := fe 2 ff jRc [ A [ s ` eg,
which, together with the sequence abstraction mapping fi will lead to the desired conclusion.
For every operator oai, we know by construction of phase-IV, that hfi (i , 1); fi (i); oai;  i 2 G
holds. By construction of phase-III, we can conclude that safi (i,1) [ Ra ` Preoai holds and
that consequently E [Ra ` Preoai also holds for the respective execution of the body of the
while-loop in phase-IV. Since E  ff0  ff holds and ` is a monotonic derivation operator,
it is obvious that ff(scfi (i)) [ Ra ` Preoai . Furthermore, the `if for all'-test, which is executed
oai
before the extension of the path, ensures that (safi (i,1) \ ff ) ,!
(safi (i) \ ff ) holds. Together
oai
with the fulfillment of the precondition of the operator we have ff(scfi (i,1)) ,!
ff(scfi(i)).
Thus, we have shown, Ca is correct abstraction with respect to Definition 5.
Completeness (\"): Assume, case Ca = hhsa0 ; sam i; (oa1; : : : ; oam)i is an abstraction of Cc
based on a deductively justified state abstraction mapping. Then there exists a state ab19. Note that ff refers to the set finally constructed after termination of the while-loop. We use ff to
denote the respective set during the construction in this loop.

111

fiBergmann & Wilke

oa

i
ff(scfi(i))
straction mapping ff and a sequence abstraction mapping fi such that ff(scfi (i,1)) ,!
holds for all i 2 f1; : : : ; mg. Since ff is deductively justified by A, it follows by construction
of phase-II, that ff(sci,1 )  sai,1 . Since ` is a monotonic derivation operator, the preconditon of oai is also fulfilled in safi (i,1) . Furthermore, the addlist of the operator is fulfilled in
ff(scfi (i)) and is consequently also fulfilled in sai . By the construction of phase-III, it is now
guaranteed, that hfi (i , 1); fi (i); oai;  i 2 G. Now, we would like to show, that in phase-IV:

 there exists a sequence of assignments to the variable Paths, such that h(); fi0; ff0i 2
Paths, h(oa1 ); fi1; ff1 i 2 Paths, : : : , h(oa1 ; : : : ; oam); fim; ffm i 2 Paths ,
 fik ( ) = fi( ) for  2 f0; : : : ; kg
 (ffk \ sal)  ff(scl) for l 2 f1; : : : ; ng and
 ffk  Skl=1 Addoal .

The proof is by induction on i. The induction basis is obvious due to the initialization
of the Paths variable. Now, assume that h(oa1 ; : : : ; oak); fik ; ffk i 2 Paths (with k < m)
at some state of the execution of phase-IV. Since, hfi (k); fi (k + 1); oak+1;  i 2 G holds as
argued before, and fi (k) = fik (k) by induction hypothesis, the selected operator sequence
is tried to be extended by oa = oak+1 in the body of the while-loop. Additionally, we
know, that E contains exactly those sentences which are required to proof the precondition
of oak+1 . Note, that since the SLD-resolution procedure is assumed to be complete and
oak+1 is applicable in ff(sck ), E is required to proof the preconditition of oa if and only if
E  ff(scfi(k) ). Since ff is deductively justified, 8e 2 E ; 8l 2 f1; : : : ; mg holds: e 2 ff(scfi (l))
if scfi (l) [ Rc [ A ` e. By construction of the sal , 8e 2 E ; 8l 2 f1; : : : ; mg holds: e 2 ff(scfi (l))
if e 2 sal . Consequently, E \ sal  ff(scl ) for all l 2 f1; : : : ; mg. On the other hand, we
also know that oak+1 leads to ff(scfi (k+1) ). Consequently, Addoak+1  ff(scfi (k+1)). Following
the same argumentation as above, we can conclude that (Addoak+1 \ sal )  ff(scl ) for all
l 2 f1; : : : ; mg. Consequently, for ff0 = ffk [ E [ Addoak+1 holds that ff0 \ sal  ff(scl ). Now,
oa
we can conclude that Paths is extended by oak+1 as follows. Since ff(scfi ( ,1) ) ,!
ff(scfi ( ))
holds and that Addoa 2 ff0 and (ff0 \ safi ( ) )  ff(scfi ( ) ), we can immediately follow that
oa
(ff0 \ safi ( ,1) ) ,!
(ff0 \ safi ( ) ). Consequently, h(oa1 ; : : : ; oak; oak+1 ); ffk+1; fik+1 i 2 Paths with
ffk+1 = ff0 and fik+1 ( ) = fik ( ) = fi ( ) for  2 f1; : : : ; kg and fik+1(k + 1) = fi(k). So,
the induction hypothesis is fulfilled for k + 1. Thereby, it is shown that Ca is returned by
Pabs. 2

Acknowledgements
The authors want to thank Agnar Aamodt, Jaime Carbonell, Padraig Cunningham, Subbarao Kambhampati, Michael M. Richter, Manuela Veloso, as well as all members of our
research group for many helpful discussions and for remarks on earlier versions of this paper. Particularly, we want to thank Padraig Cunningham for carefully proof-reading the
112

fiBuilding and Refining Abstract Planning Cases

recent version of the paper. We are also greatly indebted to the anonymous JAIR reviewers who helped to significantly improve the paper. This research was partially supported
by the German \Sonderforschungsbereich" SFB-314 and the Commission of the European
Communities (ESPRIT contract P6322, the Inreca project). The partners of Inreca are
AcknoSoft (prime contractor, France), tecInno (Germany), Irish Medical Systems (Ireland)
and the University of Kaiserslautern (Germany).

References
Althoff, K. D., & Wess, S. (1992). Case-based reasoning and expert system development.
In Schmalhofer, F., Strube, G., & Wetter, T. (Eds.), Contemporary Knowledge Engineering amd Cognition. Springer, Heidelberg.
Anderson, J. S., & Farly, A. M. (1988). Plan abstraction based on operator generalization. In
Proceedings of the 7th International Conference on Artifical Intelligence, pp. 100{104
San Mateo. Morgan Kaufmann.
Bacchus, F., & Yang, Q. (1994). Downward refinement and eciency of hierarchical problem
solving. Artificial Intelligence, 71, 43{100.
Bergmann, R. (1992a). Knowledge acquisition by generating skeletal plans. In Schmalhofer, F., Strube, G., & Wetter, T. (Eds.), Contemporary Knowledge Engineering and
Cognition, pp. 125{133 Heidelberg. Springer.
Bergmann, R. (1992b). Learning abstract plans to speed up hierarchical planning. In
Tadepalli, P. (Ed.), Proceedings of the ML92 Workshop on Knowledge Compilation
and Speedup Learning. University of Aberdeen, Scotland.
Bergmann, R. (1992c). Learning plan abstractions. In Ohlbach, H. (Ed.), GWAI-92 16th
German Workshop on Artificial Intelligence, Vol. 671 of Springer Lecture Notes on
AI, pp. 187{198.
Bergmann, R. (1993). Integrating abstraction, explanation-based learning from multiple
examples and hierarchical clustering with a performance component for planning.
In Plaza, E. (Ed.), Proceedings of the ECML-93 Workshop on Integrated Learning
Architectures (ILA-93) Vienna, Austria.
Bergmann, R., Pews, G., & Wilke, W. (1994). Explanation-based similarity: A unifying
approach for integrating domain knowledge into case-based reasoning. In Richter, M.,
Wess, S., Althoff, K., & Maurer, F. (Eds.), Topics in Case-Based Reasoning, Vol. 837
of Lecture Notes on Artificial Intelligence, pp. 182{196. Springer.
Bergmann, R., & Wilke, W. (1994). Inkrementelles Lernen von Abstraktionshierarchien
aus maschinell abstrahierten Planen. In Fensel, D., & Nakhaeizadeh, G. (Eds.),
Proceedings of the Workshop Maschinelles Lernen: Theoretische Ansatze und Anwendungsaspekte, No. 291. Institut fur angewandte Informatik und formale Beschreibungsverfahren, University of Karlsruhe, Germany.
113

fiBergmann & Wilke

Blythe, J., Etzioni, O., & et al. (1992). Prodigy4.0: The manual and tutorial. Tech. rep.
CMU-CS-92-150, Carnegie Mellon University, Pittsburgh, PA.
Carbonell, J. G. (1986). Derivational analogy: A theory of reconstructive problem solving
and expertise aquisition. In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M.
(Eds.), Machine learning: An artificial intelligence approach, Vol. 2, chap. 14, pp.
371{392. Morgan Kaufmann, Los Altos, CA.
DeJong, G., & Mooney, R. (1986). Explanation-based learning: An alternative view. Machine Learning, 1 (2), 145{176.
Etzioni, O. (1993). A structural theory of explanation-based learning. Artificial Intelligence,
60, 93{139.
Etzioni, O., & Etzioni, R. (1994). Statistical methods for analyzing speedup learning.
Machine Learning, 14, 333{347.
Fikes, R. E., Hart, P. E., & Nilsson, N. J. (1972). Learning and executing generalized robot
plans. Artificial Intelligence, 3, 251{288.
Fikes, R. E., & Nilsson, N. J. (1971). Strips: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2, 189{208.
Friedland, P. E., & Iwasaki, Y. (1985). The concept and implementation of skeletal plans.
Journal of Automated Reasoning, 1 (2), 161{208.
Giordana, A., Roverso, D., & Saitta, L. (1991). Abstracting background knowledge for
concept learning. In Kodratoff, Y. (Ed.), Proceedings of the European Working Session
on Learning (EWSL-91), Lecture Notes in Artificial Intelligence, pp. 1{13 Berlin.
Springer.
Giunchiglia, F., & Walsh, T. (1992). A theory of abstraction. Artificial Intelligence, 57,
323{389.
Green, C. (1969). Application of theorem proving to problem solving. In Proceedings of
IJCAI-69, pp. 219{239 Washington, DC.
Holte, R., Drummond, C., Perez, M., Zimmer, R., & MacDonald, A. (1994). Searching
with abstractions: A unifying framework and new high-performance algorithm. In
Proceedings of the 10th Canadian Conference on Artificial Intelligence, pp. 263{270.
Morgan Kaufmann Publishers.
Holte, R., Mkadmi, T., Zimmer, R., & MacDonald, A. (1995). Speeding up problem solving
by abstraction: A graph-oriented approach. Tech. rep. TR-95-07, Computer Science
Dept., University of Ottawa, Ontario, Canada.
Kambhampati, S., & Hendler, J. A. (1992). A validation-structure-based theory of plan
modification and reuse. Artificial Intelligence, 55, 193{258.
114

fiBuilding and Refining Abstract Planning Cases

Kambhampati, S., & Kedar, S. (1994). A unified framework for explanation-based generalization of partially ordered partially instantiated plans. Artificial Intelligence, 67,
29{70.
Knoblock, C. A. (1989). A theory of abstraction for hierachical planning. In Proceedings of
the Workshop on Change of Representation and Inductive Bias, pp. 81{104 Boston,
MA. Kluwer.
Knoblock, C. A. (1990). Learning abstraction hierarchies for problem solving. In Proceedings
Eighth National Conference on Artificial Intelligence, Vol. 2, pp. 923{928 London.
MIT Press.
Knoblock, C. A. (1991). Search reduction in hierarchical problem solving. In Proceedings of
the 9th National Conference on Artificial Intelligence, Vol. 2, pp. 686{691 Anaheim,
CA.
Knoblock, C. A. (1993). Generating abstraction hierarchies: An automated approach to
reducing search in planning. Kluwer Academic Publishers.
Knoblock, C. A. (1994). Automatically generating abstractions for planning. Artificial
Intelligence, 68, 243{302.
Knoblock, C. A., Minton, S., & Etzioni, O. (1991a). Integrating abstraction and
explanation-based learning in PRODIGY. In Proceedings of the 9th National Conference on Artificial Intelligence, Vol. 2, pp. 541{546 Anaheim, CA.
Knoblock, C. A., Tenenberg, J. D., & Yang, Q. (1991b). Characterizing abstraction hierarchies for planning. In Proceedings of the 9th National Conference on Artificial
Intelligence, Vol. 2, pp. 692{697 Anaheim, CA.
Kolodner, J. L. (1980). Retrieval and Organizational Strategies in Conceptual Memory.
Ph.D. thesis, Yale University.
Kolodner, J. L. (1993). Case-based reasoning. Morgan Kaufmann.
Korf, R. E. (1980). Toward a model of representation changes. Artifical Intelligence, 14,
41{78.
Korf, R. E. (1985a). Depth-first iterative-deepening: An optimal admissible tree search.
Artifical Intelligence, 27, 97{109.
Korf, R. E. (1985b). Macro-operators: A weak method for learning. Artifical Intelligence,
26, 35{77.
Korf, R. E. (1987). Planning as search: A quantitative approach. Artifical Intelligence, 33,
65{88.
Korf, R. E. (1993). Linear-space best-first search. Artifical Intelligence, 62, 41{78.
115

fiBergmann & Wilke

Kramer, M., & Unger, C. (1992). Abstracting operators for hierarchical planning. In
Hendler, J. (Ed.), Proceedings of the International Conference on AI Planning, pp.
287{288. Morgan Kaufmann.
Laird, J., Rosenbloom, P., & Newell, A. (1986). Universal Subgoaling and Chunking: The
Automatic Generation and Learning of Goal Hierarchies. Kluwer Academic Publishers, Norwell, MA.
Langley, P., & Allen, J. (1993). A unified framework for planning and learning. In Minton,
S. (Ed.), Machine Learning Methods for Planning, chap. 10, pp. 317{350. Morgan
Kaufmann.
Lifschitz, V. (1987). On the semantics of STRIPS. In Reasoning about Actions and Plans:
Proceedings of the 1986 Workshop, pp. 1{9 Timberline, Oregon.
Lloyd, J. (1984). Foundations of Logic Programming. Springer.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings of
the 9th National Conference on Artificial Intelligence, pp. 634{639.
Michalski, R. S. (1994). Inferential theory of learning as a conceptual basis for multistrategy
learning. In Michalski, R., & Tecuci, G. (Eds.), Machine Learning: A Multistrategy
Approach, No. 11, chap. 1, pp. 3{62. Morgan Kaufmann.
Michalski, R. S., & Kodratoff, Y. (1990). Research in machine learning: Recent progress,
classification of methods, and future directions. In Kodratoff, Y., & Michalski, R. S.
(Eds.), Machine learning: An Artificial Intelligence Approach, Vol. 3, chap. 1, pp.
3{30. Morgan Kaufmann, San Mateo, CA.
Minton, S. (1988). Learning Search Control Knowledge: An Explanation-Based Approach.
Kluwer, Boston, MA.
Minton, S. (1990). Quantitativ results concerning the utility of explanation-based learning.
Artifical Intelligence, 42, 363{391.
Minton, S., Carbonell, J. G., Knoblock, C., Kuokka, D. R., Etzioni, O., & Gil, Y. (1989).
Explanation-based learning: A problem solving perspective. Artificial Intelligence,
40, 63{118.
Minton, S., & Zweben, M. (1993). Learning, planning and scheduling: An overview. In
Minton, S. (Ed.), Machine Learning Methods for Planning, chap. 1, pp. 1{30. Morgan
Kaufmann.
Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. (1986). Explanation-based generalization: A unifying view. Machine Learning, 1 (1), 47{80.
Mooney, R. J. (1988). Generalizing the order of operators in macro-operators. In Laird,
J. (Ed.), Proceedings of the 5th International Conference on Machine Learning, pp.
270{283 San Mateo, CA. Morgan Kaufmann.
116

fiBuilding and Refining Abstract Planning Cases

Mooney, R. J., & DeJong, G. F. (1985). Learning schemata for natural language processing.
In Proceedings of IJCAI, pp. 681{687 Los Angeles, CA.
Mozetic, I. (1990). Abstraction in model-based diagnosis. In AAAI Workshop on Automatic
Generation of Approximations and Abstractions, pp. 64{75 Boston, MA.
Newell, A., & Simon, H. (1972). Human Problem Solving. Prentice-Hall Englewood Cliffs,
NJ.
Paulokat, J., & Wess, S. (1994). Planning for machining workpieces with a partial-order,
nonlinear planner. In AAAI-Fall Symposium on Planning and Learning: On to Real
Applications.
Perez, M., & Carbonell, J. (1993). Automated acquisition of control knowledge to improve
the quality of plans. Tech. rep. CMU-CS-93-142, Carnegie Mellon University.
Pews, G., & Wess, S. (1993). Combining model-based approaches and case-based reasoning
for similarity assessment and case adaptation in diagnositc applications. In Richter,
M. M., Wess, S., Althoff, K., & Maurer, F. (Eds.), Preprints of the First European
Workshop on Case-Based Reasoning (EWCBR-93), Vol. II, pp. 325{328. University
of Kaiserslautern, Germany.
Plaisted, D. (1981). Theorem proving with abstraction. Artifical Intelligence, 16, 47{108.
Plaisted, D. (1986). Abstraction using generalization functions. In Proceedings of the 8th
Conference on Automated Deduction, Vol. 16, pp. 365{376.
Rosenbloom, P., & Laird, J. (1986). Mapping explanation-based learning onto SOAR. In
Proceedings National Conference on Artificial Intelligence, Vol. 2 Philadelphia, PA.
Sacerdoti, E. (1974). Planning in a hierarchy of abstraction spaces. Artificial Intelligence,
5, 115{135.
Sacerdoti, E. (1977). A Structure for Plans and Behavior, Vol. 5. American-Elsevier, New
York.
Schank, R. C. (1982). Dynamic Memory: A Theory of Learning in Computers and People.
Cambridge University Press, New York.
Schmidt, G. (1994). Modellbasierte, interaktive Wissensakquisition und Dokumentation von
Domaenenwissen. Ph.D. thesis, University of Kaiserslautern, Germany.
Schmidt, G., & Zickwolff, M. (1992). Cases, models and integrated knowledge acquisition to
formalize operators in manufacturing. In Proceedings of the 7th Knowledge Acquisition
for Knowledge-based Systems Workshop (Banff).
Shavlik, J., & O'Rorke, P. (1993). Empirically evluation EBL. In Investigating ExplanationBased Learning, Vol. 5, chap. 7, pp. 222{294. Kluwer Academic Publishers.
Simon, H. (1975). The functional equivalence of problem solving skills. Cognitive Psychology,
7, 268{288.
117

fiBergmann & Wilke

Tenenberg, J. (1987). Preserving consistency across abstraction mappings. In McDermott,
J. (Ed.), Proceedings of the 10th International Conference on Artifical Intelligence,
pp. 1011{1014 Los Altos, CA. Morgan Kaufmann.
Tenenberg, J. (1988). Abstraction in Planning. Ph.D. thesis, Computer Science Department,
University of Rochester, New York.
Unruh, A., & Rosenbloom, P. (1989). Abstraction in problem solving and learning. In
Proceedings of the International Joint Conference on Artifical Intelligence-89, pp.
590{595 Detroit, MI. Morgan Kaufmann.
Veloso, M. M. (1992). Learning by analogical reasoning in general problem solving. Ph.D.
thesis, Carnegie Mellon University, Pittsburgh, PA.
Veloso, M. M. (1994). PRODIGY/ANALOGY: Analogical reasoning in general problem
solving. In Richer, M., Wess, S., Althoff, K., & Maurer, F. (Eds.), Topics in CaseBased Reasoning, pp. 33{52. Lecture Notes in AI, Vol. 837, Springer.
Veloso, M. M., & Blythe, J. (1994). Linkability: Examining causal link commitments
in partial-order planning. In Proceedings of the 2nd International Conference on
Planning for AI Systems AIPS-94.
Veloso, M. M., & Carbonell, J. G. (1988). Integrating derivational analogy into a general
problem solving architecture. In Minton, S. (Ed.), Proceedings of the First Workshop
on Case-Based Reasoning. Morgan Kaufmann.
Veloso, M. M., & Carbonell, J. G. (1993). Towards scaling up machine learning: A case
study with derivational analogy in PRODIGY. In Minton, S. (Ed.), Machine Learning
Methods for Planning, chap. 8, pp. 233{272. Morgan Kaufmann.
Wilke, W. (1993). Entwurf und Implementierung eines Algorithmus zum wissensintensiven
Lernen von Planabstraktionen nach der PABS-Methode. Projektarbeit, Universitat
Kaiserslautern.
Wilke, W. (1994). Entwurf, Implementierung und experimentelle Bewertung von
Auswahlverfahren fur abstrakte Plane im fallbasierten Planungssystem PARIS. Master's thesis, University of Kaiserslautern, Germany.
Wilkins, D. (1988). Practical Planning: Extending the classical AI planning paradigm.
Morgan Kaufmann.
Yang, Q., & Tenenberg, J. (1990). Abtweak: Abstracting a nonlinear, least commitment
planner. In Proceedings of the 8th National Conference on Aritificial Intelligence, pp.
204{209 Boston, MA.

118

fiJournal of Artificial Intelligence Research 3 (1995) 187-222

Submitted 5/95; published 10/95

Learning Membership Functions in a
Function-Based Object Recognition System
Kevin Woods

woods@bigpine.csee.usf.edu

Computer Science & Engineering
University of South Florida
Tampa, FL 33620-5399

Diane Cook

cook@centauri.uta.edu

Computer Science & Engineering
University of Texas at Arlington
Arlington, TX 76019

Lawrence Hall
Kevin Bowyer

hall@waterfall.csee.usf.edu
kwb@bigpine.csee.usf.edu

Computer Science & Engineering
University of South Florida
Tampa, FL 33620-5399

Louise Stark

stark@napa.eng.uop.edu

Electrical and Computer Engineering
University of the Pacific
Stockton, CA 95211

Abstract

Functionality-based recognition systems recognize objects at the category level by reasoning about how well the objects support the expected function. Such systems naturally
associate a \measure of goodness" or \membership value" with a recognized object. This
measure of goodness is the result of combining individual measures, or membership values,
from potentially many primitive evaluations of different properties of the object's shape. A
membership function is used to compute the membership value when evaluating a primitive
of a particular physical property of an object. In previous versions of a recognition system known as Gruff, the membership function for each of the primitive evaluations was
hand-crafted by the system designer. In this paper, we provide a learning component for
the Gruff system, called Omlet, that automatically learns membership functions given a
set of example objects labeled with their desired category measure. The learning algorithm
is generally applicable to any problem in which low-level membership values are combined
through an and-or tree structure to give a final overall membership value.

1. Introduction
In any computer vision (CV) application involving the recognition or the detection of \objects", descriptions of the types of objects to be recognized are required. Object descriptions
can be explicitly supplied by a human \expert". Alternatively, machine learning techniques
can be used to derive descriptions from example objects.
There are some advantages to learning object descriptions from examples rather than
from direct specification by an expert. Specifically, it may be dicult for a person to

c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiWoods, Cook, Hall, Bowyer, & Stark

provide a CV system with an accurate description of an object that is general enough to
cover the possible variations in the visual appearance of different instances of the object. For
example, no two tumors in medical images will look exactly the same. Similarly, it would
be cumbersome for a human to provide a CV system with the ranges of possible values
for all the different physical aspects of chairs (i.e., What are the possible surface areas
of the seating surface of a chair? How is the seating surface supported?). Considerable
\tweaking" of the object description parameters may be required by a human expert in
order to achieve satisfactory system performance. Machine learning techniques can be
used to generate concepts that are consistent with observed examples. Some examples of
such learning systems include C4.5 (Quinlan, 1992), and AQ (Michalski, 1983). System
performance is affected by the ratio of the number of training examples to the number of
features used to describe the examples, and the accuracy with which the examples represent
the \real-world" objects the CV system may encounter.
A function-based object recognition system is an example of a CV system for which
machine learning techniques can be useful in the development of object descriptions. A
function-based object recognition system recognizes an object by classifying it into one
or more generic object categories which describe the function that the object might serve
(Bogoni & Bajcsy, 1993; Brand, 1993; Di Manzo, Trucco, Giunchiglia, & Ricci, 1989; Kise,
Hattori, Kitahashi, & Fukunaga, 1993; Rivlin, Rosenfeld, & Perlis, 1993; Stark & Bowyer,
1991, 1994; Sutton, Stark, & Bowyer, 1993; Vaina & Jaulent, 1991). Each object category
is defined in terms of the functionality required of an object that belongs to the category.
For example, an object category might be defined as:
straight back chair ::= provides sittable surface & provides stability &
provides back support
indicating that an object can be classified as a straight back chair to the degree that it
satisfies the conjunction of the three functional properties.
The functional properties are themselves defined in terms of primitive evaluations of
different aspects of an object's shape. For example, candidate surfaces may be checked
for provides sittable surface by evaluating whether they have appropriate width, depth and
height above the support plane. In many cases, there is not a unique ideal value for some
given aspect of an object's shape, but instead there is a range of values that can be considered
equivalent in terms of \goodness". For example, anything between 0.45 to 0.55 meters
might be an equally acceptable height for a seating surface. However, as a particular shape
measurement becomes too small or too large, the evaluation measure should be reduced.
Fuzzy set theory provides a mathematical framework for handling this \goodness of fit"
concept. In our case, a fuzzy membership function transforms a physical measurement (i.e.,
height of an object's surface above the ground) into a membership value in the interval [0,1].
This membership value, or evaluation measure, denotes the degree to which the object (or
portion of the object) fits the primitive physical concept (i.e., how well the height of the
surface matches the seating surface height of typical chairs). Thus, a separate measure
of goodness is produced for each primitive evaluation. These measures are combined to
produce a final aggregate measure of goodness for the object.
The Gruff system (Stark & Bowyer, 1991) is a function-based object recognition system
which utilizes fuzzy logic, in the manner just described, to evaluate 3-D shapes. In previous
188

fiLearning Membership Functions in Object Recognition

versions of Gruff, the fuzzy membership functions embedded in the system have been
collectively hand-crafted and refined to produce the best results over a large set of example
shapes. These membership functions are ideal candidates to be learned from examples using
a machine learning approach.
In this paper, we present a method of automatically learning the collection of fuzzy
membership functions from a set of labeled example shapes. Due to the system constraints
imposed by Gruff, general-purpose machine learning algorithms, such as neural networks,
genetic algorithms, or decision trees, are not readily applicable. Thus, a new special-purpose
learning component, called Omlet, has been developed. Omlet is tested with synthetic
data for two different object categories (chairs and cups), and with data collected from
human evaluations of physical chairs. Results are presented to show that (a) learning the
membership functions in this way provides a level of recognition performance equivalent to
that obtained from the \hand-tweaked" Gruff, and (b) the learning method is compatible
with human interpretation of the shapes. The approach should be generally applicable
to any system in which a set of primitive evaluation measures is combined to produce an
overall measure of goodness for the final result.
This paper is organized as follows. Section 2 discusses some related work, and justifies our need to develop a special-purpose learning component. Section 3 introduces the
Gruff object recognition system. Section 4 presents the new learning component, called
Omlet. At this point, we should state that the material in Section 3 has previously been
published, and is presented here to facilitate an understanding of the new learning component. Although Omlet has been specifically \tailored" as an add-on learning component
for the Gruff system, it applies to a data structure that can be used in other systems.
In general, Omlet can be described as a system for learning in the context of a fuzzy
And/Or categorization tree. We point the reader with any questions concerning Gruff's
object recognition paradigm to the references provided. Section 5 describes our experimental design and the data sets that are utilized. Section 6 documents the experimental results
and gives our analysis of them. Finally, in Section 7 a summary of the paper is given and
conclusions are drawn.

2. Related Work
There are two ways that learning might be used to ease the construction of systems such as
Gruff. The first is that the rules (or proof tree) that make up Gruff could be built by an
inductive learning system. C4.5, a decision tree learner (Quinlan, 1992), is a good example
of this class of learning systems. However, these types of inductive classification systems
cannot adequately replace the functionality of the Gruff/Omlet system. Omlet allows
examples which have less than perfect membership in a class to be used for training. There
is no direct way to accomplish this in a system such as C4.5. A decision-tree based system
would probably require different trees to be trained for parent and child categories. The
functional concepts (provides sittable surface, for example) would get lost in the training
process if the individual features for a chair were directly used. We could train a series
of trees to learn functional concepts individually, then train a decision tree to combine the
results. In such an approach the parameters of the membership functions that are learned in
this paper would be learned implicitly in the construction of a decision tree for a functional
189

fiWoods, Cook, Hall, Bowyer, & Stark

concept and any resulting rules. Replacing Gruff/Omlet with a decision tree or other
general-purpose rule learner is possible, but would require extensive work to preserve the
idea of functional object recognition.
Omlet is aimed at the second area in which a Gruff-like system could benefit from
learning, which is in tuning the membership functions. A knowledge primitive might be a
sittable surface. Given measurements for a specific surface of an object in a specific orientation, it is necessary to develop a representation of acceptable bounds on the measurements
to determine whether the surface has the area to be sittable.
Techniques from other areas of machine learning have been used to represent and learn
probabilistic and fuzzy membership functions. For example, belief networks provide a mechanism for representing probabilistic relationships between features of a domain. Individual
feature probabilities can be combined to generate the probability of a complex concept by
propagating belief values and constraints through the network. Adaptive probabilistic networks are a kind of belief nets that can learn the individual probability values and distributions using gradient descent (Pearl, 1988; Cooper & Herskovits, 1992; Spiegelhalter, Dawid,
Lauritzen, & Cowell, 1993). The structure of belief nets and their update algorithms are
similar to the approaches found in Omlet. However, Omlet incorporates symbolic theorem proving, a feature that is fundamental to performing function-based object recognition,
as well as value propagation.
Similar research has been performed to learn fuzzy membership functions using adaptive techniques such as genetic algorithms and classifier systems (Parido & Bonelli, 1993;
Valenzuela-Rendon, 1991). Much of this work can only be used to learn individual membership functions and cannot handle combinations of input. Once again, little work has
been directed at learning fuzzy memberships in the context of a rule-based system. Additional refinement techniques such as reinforcement learning (Mahadevan & Connell, 1991;
Watkins, 1989), neural networks, and statistical learning techniques can also be used to
refine confidence values.
This project represents a new direction in computer vision and machine learning research; namely, the integration of machine learning and computer vision methods to learn
fuzzy membership functions for a function-based object recognition system. Although learning such functions in a rule-based context is a novel effort, similar research has been performed in the area of refining certainty factors for intelligent rule bases. For example,
Mahoney and Mooney (1993) and Lacher et al. (1992) use backpropagation algorithms to
adjust certainty factors of existing rules in order to improve classification of a given set of
training examples. In contrast to Omlet's approach, all of these systems refine values that
represent a measure of belief in a given result and are adjusted according to the combination
functions of certainty factors. Omlet's measures represent degrees of fuzzy membership in
an object class, and the refinement method propagates error through an And/Or tree.
The work by Wilkins and Ma (1994) focuses on revising probabilistic rules in a classification expert system. Probabilistic weights are applied to each rule, indicating the strength
of the evidence supplied by the rule. However, refinements to the rule occur in the form
of modifying the applicability of the rule by generalizing, specializing, deleting or adding
rules, instead of automatically refining the weight of the rule. The authors avoid automatic
refinement of weights because the resulting rule base may not be interpretable by experts.
190

fiLearning Membership Functions in Object Recognition

Towell and Shavlik (1993) convert a set of rules into a representation suitable for a
neural net, then train the network and re-extract the refined rules. The initial network
can be set up for a chain of rules. The extracted rules will not necessarily have the clear
functional meaning that our approach aims at preserving.
There are several new approaches to learning and tuning fuzzy rules (Ishibuchi, Nozaki,
& Yamamoto, 1993; Berenji & Khedkar, 1992; Jang, 1993; Jang & Sun, 1995) that use
genetic algorithms or specialized kinds of neural networks, some making use of reinforcement
learning. These approaches might provide an alternative way to learn the membership values
provided the initial functional rules are given as fuzzy rules. However, some modifications
to the learning approaches would be needed as they normally work in domains without rule
chaining or hierarchies of rules as there are in Gruff/Omlet.

3. The Gruff Object Recognition System
The Gruff acronym stands for Generic Representation Using Form and Function (Stark
& Bowyer, 1991). The Gruff recognition system takes a 3-D shape description as input,
reasons about whether the shape could belong to any of the object categories known to
Gruff, and outputs an interpretation for each category to which the object could belong.
An \interpretation" is a specified orientation and a labeling of the parts of the shape which
are identified as satisfying the functional properties. See Figure 1 for an example of an
interpretation.
GRUFF Input

GRUFF Output

Provides
Sittable
Surface

Provides
Stable
Support

Figure 1: Gruff interpretation of a 3-D shape for the category conventional chair. Elements of the shape are labeled with the functional property they provide.

191

fiWoods, Cook, Hall, Bowyer, & Stark

3.1 The Knowledge Primitives

All of Gruff's reasoning about shape is performed using \low level" procedural knowledge
which is implemented as a set of knowledge primitives . Each knowledge primitive represents
some primitive physical property concerning shape, physics, or causation. Each knowledge
primitive takes some (specified portions of a) 3-D shape description as its input, along with
values of the parameters for the primitive, and returns an evaluation measure between 0 and
1. The evaluation measure represents how well the shape element satisfies the particular
invocation of the primitive.
The knowledge primitives used by Gruff to recognize chairs are (Stark & Bowyer, 1991,
1994; Sutton et al., 1993):
1. relative orientation (normal one, normal two, range parameters)
This primitive determines if the angle between the normals for two surfaces (normal one and normal two) falls within a desired range.
2. dimensions ( shape element, dimension type, range parameters )
This primitive can be used to determine if the dimension (e.g. width or depth) of a
surface lies within a specified range.
3. proximity ( proximity type, shape element one, shape element two )
This primitive can be used to check qualitative relations between shape elements, such
as above , below and close to .
4. clearance ( object description, clearance volume )
This primitive can be used to check for a specified volume of unobstructed free space
in a location relative to a particular part of the shape.
5. stability ( shape, orientation, applied force )
This primitive can be used to check that a given shape is stable when placed on a at
supporting plane in a given orientation and with a (possibly zero) force applied.
Each of the first two knowledge primitives include four range parameters: z 1 (stands
for 1st zero point), n1 (1st normal point), n2 (2nd normal point), and z 2 (2nd zero point).
These parameters are used to define a trapezoidal fuzzy membership function, as in Figure 2,
for calculating an evaluation measure for the invocation of the primitive. The last three of
the knowledge primitives do not have range parameters. They return an evaluation measure
of 1 or 0 depending on whether or not the primitive physical property has been satisfied.
Trapezoidal membership functions reect a desire to name (categorize) objects in a
manner compatible with human naming. There is typically a non-trivial range for the
\ideal" value of many physical properties related to functionality. For example, while there
is a unique value for the mean sittable surface area of a population of chairs, that value
is not the only one that would rate a perfect \1.0" for sittability. Reasonable deviations
result in no decrease in the sittability. When the sittable surface area falls outside the ideal
range (i.e., between z 1 and n1, or between n2 and z 2 in Figure 2), the evaluation measure
is reduced, indicating the surface provides a less than perfect (but still functional) sittable
192

fiLearning Membership Functions in Object Recognition

1.0
Evaluation
Measure

0.0

z1 = least

z2 = greatest
n1 = low ideal n2 = high ideal

Physical Measurement for a Particular Property i

Figure 2: Fuzzy membership function returns an evaluation measure of a primitive physical
property.

area. Finally, when the area falls outside the range of values (less than z 1, or greater than
z 2 in Figure 2), the surface can no longer function as the sittable portion of a chair, and a
evaluation measure of 0 is returned.

3.2 The Category Definition Tree

Gruff's knowledge about different object categories is implemented as a category definition

tree , the leaves of which represent invocations of the knowledge primitives. The category
definition tree for the chair category is illustrated in Figure 3.
A node in a category definition tree may have two subtrees. One subtree gives the
definition of the category in terms of a list of functional properties. In our chair example, an
object must satisfy the functional properties of stability and provides sittable surface in order
to be considered a member of the category conventional chair. Each functional property
may be defined in terms of multiple primitives. The evaluation measures of individual
primitives are combined (in a manner to be discussed shortly) to determine how well the
functional properties have been satisfied. These functional property measures are further
combined to arrive at an overall evaluation measure for a category node.
The other subtree defines a subcategory. A subcategory is a specialization of its parent
(or superordinate) category, and thus provides a more detailed elaboration of the definition
of its parent. A subcategory node has a subtree of functional properties that are required
in addition to those of the parent category. For example, in Figure 3, the subcategory
straightback chair is a specialization of a conventional chair with the additional functional
requirement provides back support. The overall evaluation measure for a subcategory node
is a combination of its parent category evaluation measure and the evaluation measure
associated with the additional functional properties. In Figure 3, the overall measure for
the subcategory straightback chair is a combination of the measures from the conventional
193

fiWoods, Cook, Hall, Bowyer, & Stark

Name: CHAIR
Node
(SUB)CATEGORY
Type:
Funtional Subcategory
Definition
Trees

Name: CONVENTIONAL
CHAIR
Node
Type:(SUB)CATEGORY

Name:

Node
Type:(SUB)CATEGORY

Funtional Subcategory
Definition
Trees

Name: PROVIDES
SITTABLE SURFACE
Node
FUNCTIONAL
Type:
PROPERTY

PROVIDES
Name:
STABLE SUPPORT
Node
FUNCTIONAL
Type:
PROPERTY

Name:
Node
Type:

Funtional Subcategory
Definition
Trees
Name: PROVIDES
STABLE SUPPORT
Node
FUNCTIONAL
Type:
PROPERTY

Name: STRAIGHTBACK
CHAIR
Node
Type:(SUB)CATEGORY
Funtional Subcategory
Definition
Trees

PROVIDES
BACK SUPPORT
FUNCTIONAL
PROPERTY

LOUNGE
CHAIR

PROVIDES
Name:
LOUNGING SITTABLE SURFACE
Node
Type:

RECLINER

Node
Type: (SUB)CATEGORY
Funtional
Definition

Subcategory
Trees

FUNCTIONAL
PROPERTY
Name: PROVIDES
LOUNGING BACK SUPPORT
Node
FUNCTIONAL
Type:
PROPERTY

Name: ARMCHAIR

Name:

Name: PROVIDES
LOUNGING ARM SUPPORT
Node
Type:

FUNCTIONAL
PROPERTY

Node
Type:(SUB)CATEGORY
Funtional Subcategory
Definition
Trees

Name: PROVIDES
ARM SUPPORT
Node
Type:

FUNCTIONAL
PROPERTY

Figure 3: Category definition tree for the basic level category chair.
chair node and the provides back support subtree. Note that subcategory measurements do
not contribute to the cumulative measure for a parent category. There may be multiple levels
of subcategories, as with conventional chair, straightback chair, and armchair in Figure 3.
Category nodes which have no associated functional properties (such as the root node
chair in Figure 3) do not have associated evaluation measures. These nodes are used to
set up the control structure of the function-based definition. However, they do provide the
category definition since an object that is a member of a subcategory is automatically a
member of all its predecessor categories. For example, in Figure 3, an object that belongs
to the subcategory straightback chair also belongs to the categories conventional chair and
chair. A superordinate category furniture could be added above the chair category (Stark
& Bowyer, 1994).
194

fiLearning Membership Functions in Object Recognition

3.3 Combination of Evidence

The evaluation measures returned by the primitive invocations at a functional property
node are combined using the T-norm:

T (a; b) = a  b
where a and b are the measures being combined. This T-norm is commonly referred to as
the probabilistic and (Pand) function (Bonissone & Decker, 1986). The immediate parent
category node directly receives an associated measure by combining the measures of the
functional property nodes using the same T-norm.
For example, the functional property provides sittable surface is defined by six primitives. For simplicity, we'll denote the evaluation measures returned by these six primitives
as p1 through p6. The functional property stability is defined by a single primitive, which
also returns an evaluation measure (p7). To determine the overall evaluation measure of a
shape for the category conventional chair we compute
conventional chair ::= provides sittable surface Pand stability

where
provides sittable surface ::= p1 Pand p2 Pand p3 Pand p4 Pand p5 Pand p6

and
stability := p7

Since the definition of a (sub)category is a conjunction of required functional properties, the cumulative measure should be dominated by the \weakest link" in the individual
primitive evaluation measures, a property of the Pand function. So, an evaluation measure
of 0 for any one primitive physical property will result in a cumulative evaluation measure
of 0. An evaluation measure of 1 indicates that the primitive physical property has been
ideally satisfied, and the shape may belong to the object category. The final result depends
on the evaluation of other primitive physical properties.
It would seem that each category could simply be defined by the knowledge primitives without using the notion of functional properties. The functional property level was
introduced into the representation hierarchy for two reasons. First, the subgroupings of
functional properties intuitively follow the levels of named categorization typical of human
concepts of function. Secondly, most functional property evaluations result in the labeling
of the functional elements of the object (i.e., the portions of the structure) that fulfill the
functional requirement.
Since the subcategory definition represents an increasingly specialized definition, evidence for belonging to the subcategory should result in an increased measure for the object
belonging to the subcategory as opposed to just the parent category. The combination
of the functional property measurement of a subcategory node, a, with its parent node's
evaluation measure, b, is computed using the T-conorm:

S (a; b) = a + b , a  b
195

fiWoods, Cook, Hall, Bowyer, & Stark

This T-conorm is commonly referred to as the probabilistic or (Por) function (Bonissone
& Decker, 1986). While the T-conorm is used to combine measures at a subcategory node,
the final subcategory evaluation measure is actually computed as:

(

a > T;
Esubcategory = 0S;(a; b); ifotherwise
:
where T is a user defined threshold. Thus, the functional property measurement of a
subcategory node, a, must be greater than some minimum in order for a shape to receive a
non-zero evaluation measure for the subcategory. For the purposes of this work, a value of
T = 0 is assumed, indicating that a shape can be assigned to a subcategory as long as there
is some non-zero evidence that it meets the additional functional requirements associated
with the subcategory. In practice, a final classification decision might require much stronger
evidence, say T = 0:7, before a shape is assigned to a subcategory.
For example, to determine the overall evaluation measure of a shape for the category
straightback chair, we first compute the overall evaluation measure for the category conventional chair, as previously described. The functional property provides back support is defined by 8 primitives. Denoting the measurements returned by the 8 primitives as p8 through
p15, the overall evaluation measure (assuming the measure for provides back support > T )
for the category straightback chair is computed as:
straightback chair ::= conventional chair Por provides back support
where
provides back support ::= p8 Pand p9 Pand p10 Pand p11 Pand p12
Pand p13 Pand p14 Pand p15

An object that can function as a straightback chair can also by definition function as a
conventional chair. The T-conorm will give the object a higher evaluation measure for the
subcategory straightback chair since there is some evidence in addition to the \minimal"
amount of evidence required for the shape to belong to the parent category conventional
chair. Thus, Gruff performs recognition of a shape by selecting the (sub)category with the
highest overall evaluation measure. This should correspond to the most specific applicable
subcategory. One exception occurs when the parent category has an evaluation measure
of 1 and there is non-zero evidence supporting the subcategory functional requirements.
In this case, the T-conorm assigns an evaluation measure of 1 to both the category and
subcategory.
The particular T-norm/T-conorm pair utilized in this paper was chosen from among
representative T-norm/T-conorm possibilities (including non-probabilistic formulations) described by Bonissone and Decker (1986) after analyzing their performance in conjunction
with Gruff across a set of example shapes (Stark, Hall, & Bowyer, 1993a).

4. The OMLET Learning System

In this section, we describe the Omlet learning (sub)system. Omlet learns fuzzy membership functions, which are located at the leaves of an And/Or categorization tree, from sets
196

fiLearning Membership Functions in Object Recognition

of training examples. Omlet works together with Gruff to automatically learn object
category definitions and use those definitions to recognize new objects.
In the training mode, Omlet uses examples to learn the fuzzy ranges for primitive
measurements. Each training example consists of an object description coupled with a
desired overall evaluation measure. In the testing mode, Omlet uses the previously learned
ranges to act as a function-based object recognition system. Knowledge primitives form the
building blocks of the Omlet system, and rules make up the representation language. The
rules, which are fixed, are derived from Gruff's category definition tree. They indicate
1) how the knowledge primitives are combined to define functional properties, and 2) how
the functional properties are combined to give the function-based definition of an object
category.
Given a training example, Omlet uses the rules to construct a general proof tree for
the example's given object category. The proof tree is simply a data structure that mimics
the way Gruff combines primitive evaluation measures. The proof tree also maintains
the primitive ranges that are modified by the learning algorithm. An example proof tree
generated from the rules that define an object in the conventional chair category is shown in
Figure 4. The proof trees contain only those knowledge primitives which are defined using
range parameters. This is because the other knowledge primitives return only 0/1 measures,
and so there is no primitive membership function to learn. The training example must satisfy
these \binary", or necessary, functional properties and return evaluation measures of 1 in
order for the example to be a member of the given category. For example, in Figure 4, the
left branch of the top Pand node represents the functional property provides stable support.
This functional property is defined by a single knowledge primitive which has no range
parameters. Therefore, this input to the Pand node is fixed to always return a 1.
For Omlet to obtain an overall evaluation measure for an example object, the physical
measurements of the shape elements of the object are input to the primitive fuzzy membership functions in the leaves of the proof tree. The output at a leaf node represents
the evaluation measure for the individual functional property. The evaluation measures
are combined at the internal nodes of the tree using the probabilistic T-norm/T-conorm
combiners described in Section 2.3. The overall evaluation measure of the input example is
then output at the root node (see Figure 4).
Input to Omlet consists of a set of goals for specific examples from object (sub)categories.
The goal includes the example's (sub)category, the elements of the 3-D shape that fulfill
the functional properties, and an overall desired evaluation measure which is greater than 0
(otherwise the object is not an example of the object category). Figure 5 shows an example
of a goal for a conventional chair object.
Using the training examples, Omlet attempts to learn the ranges used in the trapezoidal
membership functions associated with the knowledge primitive definitions (see Figure 2).
When a training example is presented, Omlet attempts to prove via the rule base that the
object is a member of the specified category. Here, the check is to make sure the physical
elements of the object listed in the goal satisfy the binary, or necessary, functional properties.
So, for a conventional chair training example, Omlet checks that the given orientation is
stable, and the given seating surface is accessible (clearance in front and above) and meets a
minimum width to depth ratio. If the necessary functional properties have all been satisfied,
a proof tree is constructed. The actual overall evaluation measure is then calculated in the
197

fiWoods, Cook, Hall, Bowyer, & Stark

Rules for Conventional Chair

Conventional Chair
Evaluation Measure
= 0.572
PAND

(conventional_chair ?a ?b ?c) ::=
(provides_sittable_surface ?a ?b ?c) PAND
(provides_stable_support ?a)
(provides_sittable_surface ?a ?b ?c) ::=
(dimensions AREA range_parameters ?b) PAND
(WIDTH/DEPTH 1.0 ?b) PAND
(dimensions CONTIGOUS SURFACE range_parameters ?b) PAND
(dimensions HEIGHT range_parameters ?b) PAND
(clearance ABOVE ?a ?b) PAND
(clearance IN_FRONT ?a ?c)
(provides_stable_support ?a) ::=
(stability SELF ?a)

1.0
0.572

Binary functional property
"provides stable support"
is fixed to always return a 1

?b
?c

PAND

?a

0.763

1.0

Height = 0.67

0.750
Contiguous
Surface = 1.0

Area = 0.116

Knowledge primitives with ranges that are used
to compute the evaluation measures of the functional
property "provides sittable surface".

Figure 4: The simplified proof tree constructed for a learning example from the category
conventional chair. The ?a, ?b, and ?c symbols in the rules represent the physical
aspects of a shape that are used by the rules. An orientation of the shape, the face
of the sittable surface, and the front edge of the sittable surface are substituted
for ?a, ?b, and ?c, respectively. This way Omlet knows which elements of a
shape are to be \measured" and evaluated by the knowledge primitives.
198

fiLearning Membership Functions in Object Recognition

( conventional_chair mchair.00.orientation2 mchair.00.face2 mchair.00.edge1-8 )

Object Category

Object Orientation

Sittable Surface

Front Edge of
Sittable Surface

0.9808

Desired
Evaluation
Measure

Functional Properties

Figure 5: Training goal input to Omlet for a conventional chair object.
manner described above. If the actual evaluation measure is suciently different from
the desired evaluation measure, then the primitive fuzzy membership functions that were
included in the definition need to be adjusted.
Primitive membership functions are adjusted by propagating the overall error for each
training sample down through the nodes of the proof tree in a way that attempts to give
each leaf node (i.e., range) some portion of the error. The range parameters (z 1, n1, n2,
and z 2) that define the fuzzy membership trapezoids are then adjusted in an attempt to
reduce the total error of the examples in the training set. The next few subsections provide
details of the Omlet learning algorithm. First, we discuss the method for calculating an
error value and propagating it down through the proof tree. Next, we present a method for
making initial estimates of the parameters for each membership function. We describe error
propagation first because it is utilized in the initialization phase. We then describe how
Omlet makes adjustments to the membership functions in an attempt to reduce the error
over the entire training set. The last subsection describes the general learning paradigm
and provides some theoretical justification for our implementation.

4.1 Error Propagation

The error for a training example is defined as the difference between the desired evaluation
measure and the actual evaluation measure computed by the current state of the Omlet
system. A fraction of the error (defined by a \learning rate") is propagated down the proof
tree through the Pand and Por nodes. Error propagation through Pand and Por nodes
is handled differently. If the error at a three element Pand node is E , then each of the three
elements will receive a portion of the error equal to the cube root of E (i.e., the inverse of
the Pand function). For a Por node, the full amount of error, rather than an equal share,
is propagated down each link. The rationale for this treatment of error should become clear
in Section 4.4.
It should be noted that while the desired evaluation measure is fed to the root of the tree
and propagated down to the leaves, the error is directly computable since the actual and the
projected desired values are always known at each node. The actual values at each node are
those computed when the physical measurements of the object shape are fed into the leaf
199

fiWoods, Cook, Hall, Bowyer, & Stark

Desired = 0.6
PAND
Actual = 0.35

Desired = .795

Desired = .754

PAND

PAND

Actual = .612

Actual = .571

Actual = .85

Actual = .72

Desired = .959

Desired = .829

Actual = .81

Actual = .85

Actual = .83

Desired = .891 Desired = .911 Desired = .931

Figure 6: Example of error propagation through a Pand tree. Actual values are found
when an overall evaluation measure is computed for an object. Desired values are
propagated down the tree, and error is computed as Desired , Actual.
nodes and combined to produce an overall evaluation measure at the root. The projected
desired values in the proof tree are obtained by propagating the desired evaluation measure
from the root node down to the leaves. For example, given a two input Pand node with
actual inputs a1 and a2, the actual output A will be a1  a2 (from the T-norm in section
2.3). If the desired output of the node is D, then we can compute the desired inputs to the
node as d1 and d2 by solving the following set of equations:

a1  a2 , d1  d2 = A , D
and

a1 , d1 = a2 , d2

(1)

(2)
The first equation computes the error for the Pand node1, while the second equation assures
equal portions of the error are assigned to each input. Figure 6 shows an example of the
desired values computed via Equations 1 and 2 for every node in a proof tree. In this figure,
we have a known desired overall measure of D = 0:6 at the top Pand node, and an actual
measure of A = 0:35 which was computed as the Pand of the actual node inputs, a1 = 0:612
and a2 = 0:571. Using Equations 1 and 2, we can easily compute the two unknown desired
inputs d1 and d2 to the top Pand node (which are also the desired outputs of the bottom
1. The equivalent and simpler equation d1  d2 = D could be substituted here.
200

fiLearning Membership Functions in Object Recognition

two Pand nodes) as 0:795 and 0:754, respectively. If there are three inputs to a Pand node,
then we solve a set of three linear equations to derive the desired inputs. When there are
more than three inputs to a Pand node, we divide the set of inputs recursively into groups
of two or three and solve a set of two or three linear equations, respectively.
Since the Por nodes are used to combine a single parent category measure with a single
aggregate measure for a subcategory's functional properties, there will never be more than
2 inputs to this type of node. Therefore, the full amount of error can be propagated through
a Por node by simply solving the independent equations:
a2 + d1 , a2  d1 = D
(3)
and
a1 + d2 , a1  d2 = D
(4)
Eventually, some portion of the overall error is propagated to the ranges defined by
the trapezoid membership functions. When the error reaches the individual ranges for a
training example, the input to the primitive membership function (i.e., the x axis value)
and the desired primitive evaluation measure (the y axis value) define a point that should
lie somewhere on the trapezoid. We also note which leg of the trapezoid the point belongs
to, based on which side of the normal portion of the range [n1,n2] that the x value lies.
The set of desired points for each leg can be used to make adjustments to the trapezoid
in an attempt to reduce the error. Omlet collects these desired points for each leg of
each membership function by propagating the error for all training examples down the
proof trees. The trapezoid/range parameters (z 1,n1,n2,z 2) are adjusted at the end of each
training epoch. Training continues for a fixed number of epochs or until some satisfactory
level of performance, defined by minimal classification error rate averaged over the training
set, is achieved.

4.2 Initial Estimate of Measurement Functions

Omlet's learning algorithm begins by making reasonable initial estimates of all fuzzy trape-

zoid membership functions for the physical measurements. This is accomplished by assigning actual values of 0 for the membership functions for each training example and propagating the errors (which in this case would be equal to the desired evaluation measures)
down to the ranges in the leaf nodes of the proof trees. From the collections of desired
points, we make an initial estimate of each trapezoidal membership function. It is only
important at this stage to place the edges of the constructed normal range (the n1 and n2
range parameters) somewhere within the actual normal range. The learning algorithm will
make adjustments to the n1 and n2 points on subsequent training epochs. Additionally,
Omlet may set minimum or maximum limits on the values of some of the range parameters
(more on this shortly).
A training example with a desired evaluation measure of 1 is considered a \perfect"
example of an object from a given category. Perfect training examples are desirable in
the training set because all primitive measurements for perfect examples are known to fall
in the range [n1,n2]. For example, if a conventional chair training example has a desired
evaluation measure of 1, then we know that all of the membership functions in its proof
tree (see Figure 4) must return values of 1. This is because the result of the Pand function
can be no greater than the minimum input.
201

fiWoods, Cook, Hall, Bowyer, & Stark

Omlet now examines the set of desired points that have been propagated to each range
in the definition tree and determines \limit" points. These are defined as follows. If any
two desired points have y values (memberships) of 1, then at least a segment of the normal
range [n1,n2] is known. The n1 range parameter is set to the minimum x value of all desired
points with y values of 1. Similarly, the n2 parameter is set to the maximum x value of all
desired points with y values of 1. Note that if only one such desired point is found then
n1 and n2 are set to the same value, and the membership function is initially triangular.
Since some portion of the normal range is known to be correct, an upper limit is set on
the n1 value and a lower limit is set on the n2 value to assure that the known segment
of the normal range is not reduced during subsequent training. Since training examples
have desired membership values greater than 0, we know that all x input values must lie
between z 1 and z 2. Omlet uses the minimum and maximum x values from the set of
desired points to set limits on the z 1 and z 2 range parameters. The z 1 range parameter is
never permitted to increase above the minimum x value during training. Similarly, the z 2
value may never decrease below the maximum x value in the set of desired points. Figure 7
shows the range parameters (limit points) Omlet sets during the initialization phase given
a set of 10 examples.
p4

p5

p6

1.0
p3
p7

p8

p2
p9
0.0

p1
Maximum
z1 value
Allowed

Maximum
n1 value
Allowed

Minimum
n2 value
Allowed

p10
Minimum
z2 value
Allowed

Figure 7: Range parameter limits that may be set when initializing range parameters.
The limits on the range parameters serve several purposes. First, the limits assure that
perfect training examples will not be assigned evaluation measures less than 1, and that
all training examples will have evaluation measures greater than 0. More importantly, by
limiting the changes that can be made to some range parameters, better approximations
to the desired membership functions can be learned. In subsequent learning, the error is
propagated down the proof tree with the assumption that equal amounts of the error come
from each input to a node. This assumption is not always valid, and there is no way to
directly determine the portion of the error that belongs to each input. If an error propagated
to a membership function would cause a change in one or more of the range parameters
(z 1,n1,n2,z 2) that moves the parameter past its set limit, the portion of the overall error
assumed to be caused by the membership function has not been correctly estimated. When
202

fiLearning Membership Functions in Object Recognition

this occurs the parameter is set equal to its limit, effectively reducing the degree to which
changes in the membership function would compensate for the overall error. This should
allow the learning algorithm to find a good solution in the case where different membership
functions contribute different amounts of error.
If a segment of the normal range is known for some membership function, then initialization of the range parameters is straight-forward. The n1 and n2 values will have already
been set. The z 1 value is set simply by making the left leg of the trapezoid pass through
the point (n1,1.0) and the point from the set of desired points with the minimum x value.
Similarly, the z 2 value is set by making the right leg of the trapezoid pass through the point
(n2,1.0) and the desired point with the maximum x value. If there are no points to the
left (right) of the n1 (n2) point, then the membership function is assumed to be one-legged
(as for CONTIGUOUS SURFACE in Figure 4) and the parameters n1 and z 1 (n2 and z 2)
are extended to a very large negative (positive) value and not permitted to change during
training.
If no portion of the normal range of a membership function can be determined, then we
attempt to fit a trapezoid to the set of desired points. First, the two desired points with the
maximum y values are found. We assume that the normal range lies somewhere between
them. A best-fit trapezoid is determined by varying the n1 and n2 range parameters over
the assumed normal range, and selecting the normal range [n1,n2] that produces the lowest
error for the set of desired points. The error is the sum of the absolute values of the
difference between the desired y value and the actual y value found for each point. The z 1
(z 2) range parameter is set in the same manner as before, where the left (right) trapezoid
leg is forced to pass through the desired point with the minimum (maximum) x value. The
n1 value is varied from the leftmost point of the assumed normal range to the rightmost
point in small increments. For each different value of n1, the n2 value is varied from n1 to
the rightmost point of the assumed normal range in small increments. So, we are simply
testing a range of possible trapezoids (with the degree of accuracy, and number of trapezoids
tested, defined by the increments in which n1 and n2 are varied) that have a normal range
[n1,n2] somewhere within the assumed normal range. From these we select the set of range
parameters that minimize the total error over the set of training examples. The use of a
best-fit trapezoid approach is helpful, because we have no initial way to accurately associate
error with any given trapezoid.

4.3 Adjusting Membership Functions

To make adjustments to a membership trapezoid, each leg of the trapezoid is fit to a set
of desired points using a least squares line fit. Recall that after every training epoch we
have a set of desired points for each leg of each trapezoid. The new z 1 (z 2) value of the
trapezoid is set to the point at which the left (right) leg intersects 0. The new n1 (n2)
value is set to midway between the old n1 (n2) value and the value where the left (right)
leg of the fitted line intersects y = 1. The new n1 and n2 values are not directly set to
where the fitted trapezoid legs intersect 1 because overestimating the normal range [n1,n2]
can eliminate some desired points that should be used in the least squares line fit for a
trapezoid leg. Desired points in the normal [n1,n2] range by definition do not fall on a
leg of the trapezoid, and are not used when adjusting the trapezoid legs. Therefore, if the
normal range is overestimated, points that truly belong on a trapezoid leg will not be used
203

fiWoods, Cook, Hall, Bowyer, & Stark

to adjust the leg. By gradually moving the normal points n1 and n2, Omlet is better able
to converge on an appropriate solution. After the new range parameter values (z 1,n1,n2,z 2)
have been determined, Omlet checks to make sure that none of them lie outside any limits
that may have been set in the initialization phase. Restrictions on new range parameters
assure that the membership functions remain trapezoidal (or triangular if n1 = n2). First,
z 1 must be less than or equal to n1. Similarly z 2 must be greater than or equal to n2. If
z 1 (z 2) is greater (less) than n1 (n2) then z 1 (z 2) is set equal to n1 (n2). Also, n1 must be
less than or equal to n2. In the case that there is only a single point in the set of desired
points for a trapezoid leg, the leg is defined by the normal point for that leg (n1 for the left
leg and n2 for the right leg) and the single desired point.
The training data may provide target points for only a portion of a trapezoid for some
of the ranges. Omlet is capable of detecting this situation by observing the slope of the
fitted line, and adjusting the membership function appropriately. The slope of the left
trapezoid leg should be positive and the slope of the right leg should be negative. If the
slope of the fitted trapezoid leg is nearly horizontal (close to 0.0), or the sign of the slope
is opposite what is expected, then the normal point on that leg is moved (again, n1 for
the left leg and n2 for the right leg) outward. This adjustment allows Omlet to learn
one-legged membership functions, and to handle (as well as possible) situations when not
enough training data is available.
A method of escaping local minima was empirically found useful. Normally Omlet
does not allow a trapezoid leg to change if the change causes an increase in total error for
the training set. So, it is possible for zero, one or both trapezoid legs for each range to
get adjusted on an epoch. If learning slows down suciently, then Omlet will temporarily
allow trapezoid leg changes that cause an increase in overall error in hopes of escaping a
possible local minima. More precisely, if the total training set error for one epoch decreases
by less than a specified threshold, then range changes that cause an increase in overall error
are permitted for the next training epoch.

4.4 The Training Approach

In order to learn all the various subcategories defined in a category definition tree, we
utilize a machine learning approach which is based on an assumption about human learning
known as one disjunct per lesson (Lehn, 1990). Perhaps it is easiest to understand the
mechanics of our learning approach if we explain the one-disjunct-per-lesson assumption
in the terminology of cognitive science. Since many of the terms in machine learning are
derived from the cognitive sciences, it will not be dicult to show the similarities between
our algorithm and this characterization of human learning. We will also examine some of
the computational characteristics of our learning algorithm that support our choice of this
approach.
4.4.1 One Disjunct Per Lesson

Van Lehn (1990) tells us that an effective way of teaching more complicated concepts is to
build them up from simple subconcepts, as opposed to an \all-at-once" approach. For our
purposes, a disjunct can be considered one of these simple subconcepts. A lesson consists
of an uninterrupted sequence of demonstrations, examples, and exercises. The length of
a lesson varies. Thus, we might expect a human to better understand the concept of an
204

fiLearning Membership Functions in Object Recognition

armchair by presenting a series of lessons, each of which introduces a single new subconcept
that builds upon the previous subconcepts. For example, a first lesson teaches the concept of
a conventional chair which requires only a stable sittable surface in the correct orientation.
To learn what constitutes a straightback chair, we build upon the concept of conventional
chair by introducing the subconcept of back support in a second lesson. So, the second
lesson broadens our notion of chairs, in general. Finally, a third lesson builds upon our
understanding of a straightback chair by introducing the subconcept of arm support. By
contrast, the all-at-once approach may try to explain that an armchair provides a stable
sittable surface in the correct orientation with some back and arm support. Here, we
are trying to teach three subconcepts at one time, and show how the three subconcepts
together form the more complex concept of an armchair. Indeed, Van Lehn (1990) cites
some laboratory studies which indicate that the learning task is more dicult when more
than one disjunct (subconcept) is taught per lesson.
We have chosen to utilize a machine learning algorithm which has underpinnings similar
to Van Lehn's one-disjunct-per-lesson assumption. In our case, concepts and subconcepts
are represented by categories and subcategories. A lesson for our algorithm consists of
numerous epochs of the training examples from one (sub)category. Thus, our lesson can
be viewed as an uninterrupted sequence of positive examples that \teach" the functional
requirements for a single (sub)category. The length, or number of training epochs, of our
lessons may vary depending on the subcategory being learned. To learn all the ranges in
a category definition tree, we begin by learning the simplest concepts first. Then we learn
additional more complex subconcepts by building upon the notion of the more simple concept. For example in the simplified proof tree in Figure 8, the parent category conventional
chair will be learned before attempting to learn the subcategory (specialization) straightback
chair. Since the subcategory straightback chair is itself a parent category, it will be learned
before attempting to learn the even more complex subcategory armchair. The remainder
of this subsection discusses our implementation in finer detail.
From an implementation standpoint, the simplest concepts are the functional properties
associated with the categories that are directly linked to the root node in our category
definition tree such as provides sittable surface and provides stable support for the category
conventional chair. In our first lesson, we use positive examples from these \first level" (or
parent) categories to learn only those membership functions associated with these categories.
Once the first level categories have been learned, their membership functions are \frozen"
and not permitted to change during subsequent lessons.
In our second lesson, only the membership functions of the \second level" categories
(i.e., the subcategories of the first level categories in the definition tree) are learned. In
Figure 8, these membership functions belong to the node provides back support for the subcategory straightback chair. If we have learned the \simple" functional concept associated
with the parent category, the values computed for a parent category node are assumed to be
reasonably accurate. For example, when the actual values in a proof tree are computed for a
straightback chair training example, the actual values emanating from the parent category
node conventional chair should be accurate since the concepts associated with this node
have already been learned. That is, the evaluation measures for the functional properties
provides sittable surface and provides stable support of a straightback chair example are assumed to be correct. This implies that the membership functions making up the functional
205

fiWoods, Cook, Hall, Bowyer, & Stark

arm_chair
POR
Example of a subcategory
to be learned after the
parent category

provides_arm_support
PAND

straight_back_chair
POR

...
Example of a
"parent category"

conventional_chair
PAND

provides_back_support
PAND

...

...

Figure 8: Simplified proof tree for an armchair object.

requirement subtree (i.e., provides back support) are responsible for the entire error for a
subcategory training example. (This explains why Equations 3 and 4 are used to propagate
error through Por nodes.) Hence, the error is propagated to the modifiable leaves under a
functional requirement node through a Pand subtree and learning continues as before.
The lessons continue with each parent category being learned before any of its subcategories are learned, until all subcategories have been learned. By freezing the parent category
membership functions after they have been learned, we are applying to the one-subconceptper-lesson strategy. So in Figure 8 after learning straightback chair, the membership functions for that branch are frozen and the armchair subcategory is learned by modifying the
membership functions under the provides arm support branch of the proof tree.
Omlet begins learning by evaluating the rule base in order to determine subcategory
dependencies and assigns each (sub)category in the definition tree a level in the learning
hierarchy. For example, Omlet determines that the category conventional chair has no parent category and its membership functions can be learned immediately (level 1). However,
the evaluation measure of the subcategory straightback chair is dependent on the parent
category conventional chair. The straightback chair subcategory is assigned to learning
level 2. Subcategory armchair is dependent on parent category straightback chair, and is
therefore assigned to learning level 3.
206

fiLearning Membership Functions in Object Recognition

4.4.2 Practical Justification

In order to understand why we have taken a one-disjunct-per-lesson approach rather than
an all-at-once approach, let's make some observations concerning how accurately blame
assignment for an error can be determined for a typical training example.
Recall that error propagation through a proof tree involves projecting desired node input
values from a known node output value. Consider a Pand node with a known desired output
of 0.9, and two unknown inputs. We know that both of the inputs must be at least 0.9.
This means both inputs to the Pand node fall within the relatively small range [0.9,1.0].
However, when the desired output of a two input Por node is 0.9, we can only be sure
that both inputs fall in the range [0,0.9]. If the known output to the Pand or Por node
is very low, say 0.1, then there is an opposite effect. That is, the unknown inputs for a
Por node would lie in the relatively small range [0.0,0.1], and the unknown inputs for the
Pand node would fall somewhere in the much larger range [0.1,1.0]. These observations
suggest that the blame assignment for error can be propagated through a Pand node with
reasonable accuracy on examples that are relatively good, say 0.7 or above. However, for
high evaluation measures, an error value cannot be reliably propagated through a Por
node.
Since a subcategory evaluation measure is computed as the Por of a parent category
evaluation measure and the combination of additional functional requirements, all Por
nodes in a proof tree have two inputs. All Por nodes (in our proof trees) have at least 1
connecting node which consists of a parent (or more general) category whose membership
calculation involves only Pand connectives. The structure of the proof trees permits the
membership functions which contribute to the evaluation measure of a parent category to be
accurately learned prior to learning those defined in the additional functional requirements
of the subcategories. That is, we can determine one of the inputs to any Por node before we
attempt to propagate an error through that node. With one input and the desired output
of a Por node known, calculation of the unknown input is trivial. Thus, our learning
approach eliminates the reliability problems associated with propagating blame assignment
for error through Por nodes. This will be verified in Section 6 with experimental results
for the subcategories straightback chair and armchair.
The mechanics of our learning algorithm suggests that Omlet's performance depends on
how accurately blame assignment can be propagated through the Pand nodes of a proof tree.
Earlier, we observed that blame assignment is less reliably propagated through Pand nodes
for \bad" training examples. Not surprisingly, this suggests that the quality of the training
data will have an effect on system performance. This does not mean that \bad" examples
of an object (sub)category cannot, or should not, be included in the training set. Since we
use a least squares line fit to adjust the fuzzy membership functions, the use of some \bad"
training examples (for which the blame may have been inaccurately distributed among the
fuzzy membership functions) should not dramatically affect the overall reliability of the
learned system parameters. Rather, it is just desirable to train the system with examples
that, for the most part, are good examples of their labeled object category. However, this
is not unreasonable as we might expect a machine (or a human for that matter) to better
learn what constitutes a chair by observing good examples of chairs.
207

fiWoods, Cook, Hall, Bowyer, & Stark

5. Experimental Setup
Upon reading in the rule base, the knowledge primitive measurements of the training examples, and all training example goals, Omlet begins by learning the membership functions
of all level 1 categories. The first learning epoch is used to make initial estimates of the
membership functions, and then Omlet iterates for 1000 additional training epochs. A
learning rate of 0.15 is used during the 1000 training epochs, so that 15 percent of the actual error for each training example is propagated to the adjustable ranges on each epoch.
After the 1000 training epochs, the best range parameters (those that resulted in the lowest
overall error) for level 1 categories are restored and frozen. The 1000 training epochs are
then repeated for the level 2 categories, followed by the level 3 categories, and so on until
all ranges in the category definition tree have been learned2.
The performance task of the Omlet system is evaluated by how well the trained system
recognizes objects that were not used in the training phase. One measurement of system
performance is the error observed on the test examples. The error for a test example is
computed as the absolute value of the difference between the desired and actual evaluation
measures. Training/Test sets are configured two ways: random partitioning of all labeled
data into training and test sets, and leave-one-out testing. In the first case, for a given
size training set, 10 train/test set pairs are created by randomly partitioning all the labeled
data. The error for a single test set is the average error of all test examples. The results for
a given size training set are reported as the average error of the 10 partitions. In leave-oneout testing, one example in the data set is used to test while all remaining samples form
the training set. This is repeated using each example in the data as the test set, and results
are reported as the average error of all test examples. The average error per example versus
the training set size is plotted for training sets of 10, 20, 30, ... , N-1 samples. The point
with N-1 training examples represents the leave-one-out test results.

5.1 Test on the Gruff Chair Database

From the evaluations of Gruff (Stark & Bowyer, 1991), a large database of 3-D shapes
specified as polyhedral boundary representations has been built up. Figure 9 shows 52 chair
shapes. A number of the 52 shapes can belong to more than one category or can function
in more than one stable orientation. This results in a total of 110 training examples. There
are 78 labeled instances for the category conventional chair. Some 28 of these instances
additionally satisfy the function of straightback chair, and 4 instances satisfy the function
of armchair. For each shape, we have the evaluation measure for the shape's membership
in different object categories, as computed by Gruff with the hand-crafted functions for
the primitive evaluation measures. This set of shapes and their evaluation measures make
up the first set of training examples.
The first set of experiments will help determine how well Omlet learns a set of membership functions that minimize the overall error, and also how closely the learned membership
functions approximate the original functions hand-crafted by an expert for Gruff. A
question of great practical importance to vision researchers is whether a machine learning
2. In some preliminary experiments, Omlet converged on a low overall error for each level of categories
anywhere between 200 and 900 training epochs. Hence the decision to train for 1000 epochs per category
level. The learning rate was also determined empirically.

208

fiLearning Membership Functions in Object Recognition

Figure 9: The 52 object chair database.
technique can derive a set of system parameters equivalent to the hand-crafted results of
the system designer. If so, the manual effort in system construction could be greatly eased.
When the learning task is formulated as duplicating the Gruff measures, the training
data for these experiments is effectively \noiseless". (Noiseless in the sense that the desired
evaluation measures that are used as input to Omlet are all derived in the same manner
from the same set of hand-crafted fuzzy membership functions.)

5.2 Test on a Synthetic Cup Database

The definition and recognition of cups is a task that has been visited frequently in machine
learning research (Mitchell, Keller, & Kedar-Cabelli, 1986; Winston, Binford, Katz, &
Lowry, 1983). As Winston (1983) observes, it is hard to tell vision systems what cups
should look like. It is much easier to talk about the purpose and function of a cup. We
convey the description of a cup by providing its functional definition. In particular, a cup
is described as an object that can hold liquid, that is stable, liftable, and can be used
to drink liquids. The physical identification can be made using this functional definition.
In particular, for the synthetic set of objects created here, these functional properties are
broken down into 19 knowledge primitives, 17 of which have range parameters.
We generated a database of 200 synthetic cup examples, for which the measurements
of the knowledge primitives are randomly distributed. Hand-crafted range parameters
(z 1,n1,n2,z 2) are supplied for all 17 ranges in the cup functional definition. To generate a
209

fiWoods, Cook, Hall, Bowyer, & Stark

cup example, a primitive measurement is randomly selected for each range. Approximately
80% of the time the primitive measurement is randomly chosen between n1 and n2. The
other 20% of the time the measurement is randomly chosen outside n1 and n2, but inside
z 1 and z 2. This cup generator program provides us with the capability to create a large
number of cup examples without the time-consuming process of creating actual 3-D CAD
models for each example.

5.3 Learning from Human Evaluation Measures

In object recognition it is important to test a system on real objects, if possible, for a number
of reasons. First, we can see whether the system can approximate human judgment. Second,
it is important to observe system performance in the presence of noise, which real-world data
will inevitably contain. Finally, using real-world data will alleviate the need to completely
hand-craft the system with synthetic data. This is actually a useful guide for the scenario
where the \vision system engineer" gives the system a set of human-labeled examples, and
lets the system learn the parameters. To test Omlet, we have used a set of 37 actual
objects and human ratings of how well they might serve as a chair. Figure 10 shows some
of the objects used in these experiments.

Figure 10: Some examples of the chair objects used for human evaluation tests.
In order to determine how well Omlet can learn to recognize the set of real chair-like
objects, all the objects were collected together in a single room and each object was placed
in the orientation in which it would most likely be recognized as a chair. For actual chairs,
this is simply the orientation in which the chair would typically be used. For a metal trash
210

fiLearning Membership Functions in Object Recognition

can it would be an \upside down" orientation, etc. Then a group of 32 undergraduate
students in an Artificial Intelligence class was given the following instructions:
You are asked to rate each of the thirty-seven objects according to the degree
of \chair-ness" that is reected in its 3-D shape. For our purposes, \chair-ness"
measures if the object could be used as a chair. You are to consider only the
3-D shape in making your rating. You should assume that each object is made
of appropriate materials, so that this is not a factor in your ratings. You are
to consider the suitability of the object shape only in the orientation that you
see it, rather than some other orientation. Examples of factors that you should
consider in rating the \chair-ness" of a shape are height, width, depth, area,
relative orientation and apparent stability.
You are asked to rate each shape against the requirements of three different
aspects of \chair-ness". The first aspect is solely its ability to provide a stable
seating surface. The second aspect is solely its ability to provide back support
compatible with the seating surface. The third aspect is solely its ability to
provide arm support compatible with the seat and back. Each aspect should
be judged independently on a scale of 1 to 5, where 1 means it has no ability
to provide the required function and 5 means that it seems ideal to provide the
desired function. You may mark halfway between two numbers if you wish.
The ratings of each aspect of \chair-ness" were then averaged, normalized and rounded
to the nearest multiple of 0.02 to result in values in the range [0,1]. The overall evaluation
measures for the objects for the conventional chair category are taken as the normalized
evaluation measures for the first aspect of \chair-ness", that is the object's ability to provide
a stable seating surface. Overall evaluation measures for the categories straightback chair
and armchair are computed using the probabilistic or T-conorm to combine the three aspects
of \chair-ness" in the manner described in Subsection 3.3. Hence, a comfortable, sturdy
chair would have a value close to 1 for \chair-ness", while the upside-down trash can has a
considerably lower value (approx. 0.5).
After the objects had been rated, measurements were taken for each of the primitives
describing the chair in the Gruff system. The measurements were those required for the
Omlet rules, such as the clearance from the ground, the area of the sittable surface, the
height of the sittable surface, etc. Complete Omlet examples describing the objects were
then created, including the aggregate evaluation measure of the objects for the categories
conventional chair, straightback chair, and armchair. This resulted in 37 objects for the
conventional chair category, 22 objects in the straightback chair category (15 objects had
no back support at all), and 12 objects in the armchair category (10 objects that had
back support did not have any arm support). There are at least two sources of noise in
this experimental data: 1) the human evaluations, and 2) the actual measurements of the
physical properties of the objects. For example, the standard deviations of the normalized
human evaluations of the 37 objects for the conventional chair category are about 0.12,
or 12%, on average. The results of leave-one-out testing on the 37 real-world objects are
presented in the next section.
211

fiWoods, Cook, Hall, Bowyer, & Stark

6. Experimental Results

There are at least four factors that may affect the performance of the Omlet system: 1)
the number of training epochs, 2) the number of training samples for each category, 3) the
number of ranges to be learned for each category, and 4) the quality of the training data for
each category. Histograms of the desired evaluation measures of the training data are used
to convey the concept of training set \quality". They are shown in Figure 11 for the Gruff
chair data. The height of each histogram bin is the number of training samples with desired
evaluation measures that fall within a particular range. So, the histogram of a \good" set
of training data would be skewed towards the higher evaluation measures. Similarly, the
histogram representing \bad" training data would be skewed towards the lower evaluation
measures.

Figure 11: Histograms of desired evaluation measures of the Gruff chair training sets.
The histogram of a parent category, such as conventional chair or cup, represents the
distribution of the overall desired evaluation measures (which are the goal measures of the
examples in the data set provided as input to Omlet). However, the histograms for subcategories, such as straightback chair and armchair, represent the distributions of the desired
evaluation measures associated with the additional functional requirements defined for the
212

fiLearning Membership Functions in Object Recognition

subcategory. For example, the histogram for the straightback chair category represents the
quality of the provides back support portion of the straightback chair examples in a data
set, not the overall desired evaluation measures. Recall that the ranges associated with
the parent category conventional chair will be frozen (and presumably accurate) before
learning begins for the category straightback chair. So, Omlet only uses straightback chair
examples to learn the ranges associated with the provides back support functional property.
Thus, when learning the ranges for the category straightback chair, we want to observe the
quality of the back supports of the training examples. Similarly, we want to observe the
quality of the arm supports of the armchair examples, not the overall desired evaluation
measures.
A) Effect of Training Time for GRUFF Objects

B) Effect of Training Time for Synthetic Cups

Training with 77 GRUFF
Labeled Conventional Chairs

Training with 200
Synthetic Cups

Training with 27 GRUFF
Labeled Straightback Chairs

C) Effect of Training Time for Real Objects

Training with 36 Human
Labeled Conventional Chairs

Training with 21 Human
Labeled Straightback Chairs

Figure 12: Average training sample error versus number of training epochs for A) Gruff
chair objects, B) synthetic cups, and C) real chair objects. These plots are for
a single leave-one-out test run.
Figure 12 shows examples of the average training sample error plotted as a function of
the number of training epochs for each of the three data sets (Gruff objects, synthetic
cups, and real objects). From these plots, we can see that 1000 training epochs is more
than sucient for all of the categories in the three data sets. Training could most likely
213

fiWoods, Cook, Hall, Bowyer, & Stark

be stopped after 400 epochs for any of the categories without a degradation in system
performance. Since the number of training epochs is the same for all categories, and has
been shown to be sucient, we can eliminate this factor as a possible cause for the different
levels of performance among categories. Some experiments in addition to those described
in Section 5 were run to examine the effect of the other performance factors.

6.1 The Gruff Chair Database

Figure 13: Omlet results for test samples from the Gruff chair database.
Figure 13 shows the plot of the average error per sample versus training set size for examples from the conventional chair category, and a separate plot for examples from the
straightback chair category. Since there are only 28 straightback chair examples, only 3 different training set sizes (6,12,18) were evaluated in addition to the leave-one-out testing. All
78 conventional chair examples were used to train the ranges associated with the conventional chair category before the ranges for the straightback chair category were trained. No
testing was done for the subcategory armchair since there were only four training samples
available. The plot shows that increasing the number of training samples generally leads
to a reduction in the average error. When more than 20 training examples are used, the
actual evaluation measures of the test examples are within approximately 1% of the desired
evaluation measures for both the conventional chair and straightback chair categories.
We should note here that the errors in overall evaluation measures found for categories
at different learning levels are not directly comparable. So, the plot of the error rate for
the straightback chair category is not directly comparable to the plot for the conventional
chair category (Figure 13). As an example, consider an object with a desired overall evaluation measure of 0.85 for the category conventional chair. If Omlet computes an actual
214

fiLearning Membership Functions in Object Recognition

evaluation measure of 0.86, then the error for this example is 0.01. Let's assume the provides back support portion of this object has a desired evaluation measure of 0.75. The
overall desired evaluation measure for this example in the category straightback chair would
be 0.9625 (Por of 0.85 and 0.75). Now, suppose Omlet finds the actual evaluation measure for the back support of the object to be 0.76, or an error of 0.01. In this case, the
actual overall evaluation measure of this example for the category straightback chair would
be 0.9664 (Por of 0.86 and 0.76). As a result, the error of 0.01 attributed to the provides back support portion of the object is manifested as a much smaller error of 0.0039 in
the overall evaluation measure of the object.
The original range parameters (z 1,n1,n2,z 2) hand-crafted by an expert for the three
ranges in the conventional chair definition (see Figure 4) are:
AREA (0.057599 0.135 0.22 0.546699)
CONTIGUOUS SURFACE (0.0 1.0 1.0 1.0)
HEIGHT (0.275 0.4 0.6 1.1)
These are the range values used by Gruff to determine the desired evaluation measures
in the goals provided to Omlet. A typical example of the range parameters as learned by
Omlet is:
AREA (0.057599 0.135002 0.219992 0.546706)
CONTIGUOUS SURFACE (7.45591e-06 0.999995 10000 10000)
HEIGHT (0.275 0.400002 0.6 1.10009)
Omlet was able to determine that the CONTIGUOUS SURFACE range was a one-legged

membership function, and the n2 and z 2 values (i.e., the leg that does not exist) were set
to arbitrarily large values. These results show that the Omlet system is capable of using
labeled examples to automatically determine range parameters which are similar to those
that would be hand-crafted by an expert. This will facilitate the construction of other
object category definitions.
In Figure 13, we can see that the number of training samples does indeed affect the
error rate of test samples. With more than 20 or so training samples, the error rates
for both the conventional chair and straightback chair categories begin to level off. So,
the number of training samples becomes less of a factor affecting system performance if a
sucient number are used. What constitutes a sucient number of training samples for a
category may depend on the number of ranges to be learned and the quality of the training
data. There are 3 ranges that must be learned for the category conventional chair, and 5
ranges that must be learned for the category straightback chair. The histograms of desired
evaluation measures for the Gruff conventional chairs and the back supports of the Gruff
straightback chairs in Figure 11 A and B, respectively, reect the quality of the training
data used for the leave-one-out tests.
We can isolate the effect of the quality of the training data with some additional experiments utilizing two separate data sets of Gruff conventional chair examples. The number
215

fiWoods, Cook, Hall, Bowyer, & Stark

of training epochs, the number of training samples, and the number of ranges to be learned
will be identical for each data set. One data set of 38 \bad" examples contains all conventional chair examples with desired evaluation measures less than 0.6. A second data set of
\good" examples was created by selecting 38 of the remaining conventional chair examples.
The histograms of desired evaluation measures for the examples used in the \good" and
\bad" data sets are shown in Figure 11 C and D, respectively. Leave-one-out testing (37
training examples) resulted in an average error of 0.0001 for the examples in the \good"
data set, and 0.1869 for the examples in the \bad" data set. Thus, it would seem that the
quality of the training data has a considerable effect on the performance of the learning
algorithm.
Using the set of 38 \good" conventional chair examples to train Omlet, the average
error found using the 38 \bad" examples to test drops to 0.013 (compared to an average
error of 0.1869 when 37 \bad" examples are used to train). A closer examination of the
results reveals that one \bad" example contributes a relatively high error of 0.5 to the
average. If this single example is excluded from the test results, the average error of the
remaining 37 \bad" examples is only 0.00067. If the 38 \bad" examples are used to train
Omlet, the average error found using the 38 \good" examples to test is 0.242. These
results indicate that Omlet is not inherently biased to produce more accurate test results
for \good" examples since we are able to achieve a low error rate for the \bad" examples
when \good" training data is used. Rather, these results emphasize the importance of
controlling the quality of the data used to train Omlet.

6.2 The Synthetic Cups Database

Figure 14: Omlet results for test samples from the Gruff cup database.
216

fiLearning Membership Functions in Object Recognition

Figure 14 shows the plot of the average error per sample versus training set size for examples
from the randomly generated cup category. As before, Omlet's performance generally
improves as the number of training samples is increased. A comparison of the error plots
for the conventional chair data and the cup data reveals that the average error for the
cups is higher for the same number of training samples, and the error rate decreases more
erratically. The comparison of error rates between these two categories is valid since they
are both at the same level in the learning hierarchy. As before, there are two performance
factors that could be the cause of the different error rates. There are considerably more
ranges that need to be learned for the cup category than for the Gruff conventional chair
category (17 versus 3). Also, from Figure 15 A, we can see that data set created by the
cup generator program is of poor quality. Thus, due to the random nature of the synthetic
cup generator program, the system was trained with shapes that, on average, are not very
good examples of cups. Regardless of the poor training data, when more than 150 training
samples are used, the actual evaluation measures for the cup test examples are within
approximately 4% of the desired evaluation measures. In light of the \bad" set of shapes
used as training examples and the large number of ranges that must be learned, the higher
average error for cups seems reasonable.

Figure 15: Histograms of desired evaluation measures of the synthetic cup training sets.
As an additional test, we generated a set of 78 synthetic cups in the same manner as
before (see Section 5.2). However, we required the distribution of the desired evaluation
measures of the synthetic cups to have a similar distribution as the Gruff conventional
chair examples (shown in Figure 11 A). Figure 15 B shows the histogram of desired evaluation measures of the examples in this second synthetic cup data set. Since the number of
training epochs, the number of training examples, and the quality of the training data are
the same as for the first test using the Gruff conventional chair examples, this experiment
isolates the effect of the number of ranges that must be learned. Performing a leave-one-out
test (77 training examples), the average error per sample was found to be approximately
0.08. In Figure 13, the leave-one-out results on the 78 Gruff conventional chair examples
217

fiWoods, Cook, Hall, Bowyer, & Stark

(Sub)Category
Conventional
Chair
Straightback
Chair
Armchair

Number of
Average Desired Average Error
Training Samples Evaluation Measure per Sample
36
0.8447
0.0715373
21

0.9927

0.0066456

11

0.9973

0.0022430

Table 1: Leave-one-out test results for real-object database with evaluation measures derived from human ratings of the objects.

show an average error of less than 0.01 per sample. Thus, it would seem that the number
of ranges to be learned affects system performance considerably.
Finally, we created a set of 200 synthetic cups with a similar distribution as the Gruff
conventional chair examples. The histogram of desired evaluation measures of the examples
in this third synthetic cup data set would look similar to the histograms in Figure 11 A, and
Figure 15 B. Performing a leave-one-out test (199 training examples), the average error per
sample was found to be approximately 0.023. Compared to the error rate of the original 200
synthetic cups (approximately 0.04), we again note that \better" training data improved
system performance considerably. Compared to the error rate of the 78 synthetic cup data
set (approximately 0.08), which is similar in quality, we see the increased number of training
samples significantly improved system performance. The error rate for this third synthetic
cup data set with 200 examples is still higher than the error rate for the Gruff data set
of 78 conventional chair objects (less than 0.01), which has a similar quality distribution.
Consider that for the Gruff data set we used 77 training examples to learn the 3 ranges
of the conventional chair category, and for the synthetic cup data set, we used 199 training
examples to learn the 17 ranges of the cup category.

6.3 The Chair Database for Human Evaluation

Leave-one-out test results for the real-object database with evaluation measures derived
from human ratings of the objects are listed in Table 1. Recall that the error rates are not
directly comparable among the three categories. The actual evaluation measures for the
conventional chairs objects are within approximately 7% of the human evaluation measures.
The average error here is about 6% greater average error than for the Gruff data with a
similar number of training samples. The histogram in Figure 16 A shows that the data set of
real conventional chair objects contains mostly \good" examples. Thus, the higher average
error can probably be attributed to the \noise" associated with the real-object evaluation
measures. Considering an average standard deviation of 12% for the human evaluations of
the conventional chair objects, a 7% average error per sample for the Omlet results does not
seem unreasonable. The actual evaluation measures for the real-object straightback chairs
and armchairs differ on average by less than 1% from the desired measures. As before, all
conventional chair examples were used to train the ranges associated with the conventional
218

fiLearning Membership Functions in Object Recognition

chair category before the ranges for the straightback chair category were trained. The
histograms of desired evaluation measures for the back support of the real straightback
chair objects and the arm support of the real armchair objects are shown in Figure 16 B
and C, respectively.

Figure 16: Histograms of desired evaluation measures of the real-object training sets.

7. Summary and Discussion

We have presented a system (Omlet) which uses labeled training examples to learn fuzzy
membership functions embedded in a function-based object recognition system. The fuzzy
membership functions are used to provide evaluation measures which determine how well a
shape fits the functional description of an object category. The Omlet system is an example
of using machine learning techniques to aid in the development of a computer vision system.
We have shown that it is possible to accurately and automatically learn system parameters
which would otherwise have to be provided by a human expert. Omlet may be used to aid
in the construction of other object categories for the Gruff object recognition system. The
expert does not need to concentrate on \hand-tweaking" the range parameters to improve
system performance, but rather on providing a good set of example objects to \show" to
Omlet. This is intuitively appealing in that we are deriving descriptions of objects we would
219

fiWoods, Cook, Hall, Bowyer, & Stark

like Gruff to recognize by providing examples from the object category. Additionally, we
have been able to demonstrate that the performance of the learning algorithm is affected
by the number and quality of the training examples.
It should be possible for the learning approach described in this paper to be applied to
other systems in which measurements (or other values) are combined in a tree structure.
All cases are covered by our approach, except the case of 2 leaves leading directly to a Por
node. However, a generalization of our method for treating Por nodes may be developed
to handle this situation. The tree structure in our CV system is composed entirely of
probabilistic and and probabilistic or nodes, which are used to combine measurements. It
is possible that a similar approach is applicable to tree structures in which other types of
nodes (T-norms or T-conorms) are used.
The Omlet system should make it easier to adapt the Gruff system to new object
domains. Early versions of Gruff performed object recognition starting from complete
3-D shape descriptions (Stark & Bowyer, 1991, 1994; Sutton et al., 1993) rather than from
real sensory data. The task of reliably extracting accurate object shape descriptions from
normal intensity images is beyond the current state of the art in computer vision. Although
work in, for example, binocular stereo, is steadily progressing, accurate models of object
shape are more readily extracted from range imagery. Whereas in normal imagery a pixel
value represents the intensity of reected light, in range imagery a pixel value represents the
distance to a point in the scene. A version of Gruff has been developed which attempts to
recognize object functionality from the shape model that is extracted from a single range
image (Stark, Hoover, Goldgof, & Bowyer, 1993b). A major diculty here is, of course,
that a single range image does not yield a complete model of the 3-D shape of an object.
The \back half" of the object shape is unseen (Hoover, Goldgof, & Bowyer, 1995). The
accumulation of a complete 3-D shape model through a sequence of range images is a topic of
current research. If this problem was solved, then it is conceivable that an Omlet training
example might consist of a sequence of range images along with some operator annotations
to identify which portions of the images correspond to the functionally important parts of
the object (seating surface, back support surface, etc.).

Acknowledgements
This research was supported by Air Force Oce of Scientific Research grant F49620-92-J0223 and National Science Foundation grant IRI-91-20895.

References
Berenji, H., & Khedkar, P. (1992). \Learning and Tuning Fuzzy Logic Controllers Through
Reinforcements". IEEE Transactions on Neural Networks, 3, 724{740.
Bogoni, L., & Bajcsy (1993). \An Active Approach to Characterization and Recognition of
Functionality and Functional Properties". In AAAI-93 Workshop on Reasoning about
Function, pp. 201{202 Washington, D.C.
220

fiLearning Membership Functions in Object Recognition

Bonissone, P. P., & Decker, K. S. (1986). \Selecting Uncertainty Calculi and Granularity:
An Experiment in Trading-off Precision and Complexity". In Kanal, L., & Lemmer, J.
(Eds.), Uncertainty in Artificial Intelligence, pp. 217{247. North-Holland Publishing
Company.
Brand, M. (1993). \Vision Systems that See in Terms of Function". In AAAI-93 Workshop
on Reasoning about Function, pp. 17{22 Washington, D.C.
Cooper, G., & Herskovits, E. (1992). \A Bayesian Method for the Induction of Probabalistic
Networks from Data". Machine Learning, 9, 309{347.
Di Manzo, M., Trucco, E., Giunchiglia, F., & Ricci, F. (1989). \FUR: Understanding
FUnctional Reasoning". International Journal of Intelligent Systems, 4, 431{457.
Hoover, A., Goldgof, D., & Bowyer, K. (1995). Extracting a valid boundary representation
from a segmented range image. IEEE Transactions on Pattern Analysis and Machine
Intelligence. Accepted to appear.
Ishibuchi, H., Nozaki, K., & Yamamoto, N. (1993). \Selecting Fuzzy Rules by Genetic
Algorithm for Classification Problems". In 2nd IEEE International Conference on
Fuzzy Systems, pp. 1119{1124.
Jang, J. S. R. (1993). \ANFIS: Adaptive-Network-based Fuzzy Inference Systems". IEEE
Transactions on Systems, Man and Cybernetics, 23 (3), 665{685.
Jang, J. S. R., & Sun, C. T. (1995). \Neuro-Fuzzy Modeling and Control". Proceedings of
the IEEE, 378{406.
Kise, K., Hattori, H., Kitahashi, T., & Fukunaga, K. (1993). \Representing and Recognizing
Simple Hand-tools Based on Their Functions". In Asian Conference on Computer
Vision, pp. 656{659 Osaka, Japan.
Lehn, K. V. (1990). Mind Bugs: The Origins of Procedural Misconceptions. The MIT Press,
Cambridge, Massachusetts.
Mahadevan, S., & Connell, J. (1991). \Automatic Programming of Behavoir-Based Robots
Using Reinforcement Learning". In AAAI, pp. 768{773.
Michalski, R. S. (1983). \A theory and methodology of inductive learning". In Michalski,
R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning: An Artificial
Intelligence Approach. Tioga Publishing Company, Palo Alto, CA.
Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. (1986). \Explanation-Based Generalization: A Unifying View". Machine Learning, 1, 47{80.
Parido, A., & Bonelli, P. (1993). \A New Approach to Fuzzy Classifier Systems". In
Proceedings of the Fifth International Conference on Genetic Algorithms, pp. 223{
230.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann.
221

fiWoods, Cook, Hall, Bowyer, & Stark

Quinlan, J. R. (1992). C4.5: Programs for Machine Learning. Morgan Kaufmann.
Rivlin, E., Rosenfeld, A., & Perlis, D. (1993). \Recognition of Object Functionality in
Goal-Directed Robotics". In AAAI-93 Workshop on Reasoning about Function, pp.
126{130 Washington, D.C.
Spiegelhalter, D., Dawid, P., Lauritzen, S., & Cowell, R. (1993). \Bayesian Analysis in
Expert Systems". Statistical Science, 8, 219{282.
Stark, L., & Bowyer, K. W. (1991). \Achieving generalized object recognition through
reasoning about association of function to structure". IEEE Transactions on Pattern
Analysis and Machine Intelligence, 3 (10), 1097{1104.
Stark, L., & Bowyer, K. W. (1994). \Function-based recognition for multiple object categories". Image Understanding, 59 (10), 1{21.
Stark, L., Hall, L. O., & Bowyer, K. W. (1993a). \An investigation of methods of combining
functional evidence for 3-D object recognition". Int. J. of Pattern Recognition and
Artificial Intelligence, 7 (3), 573{594.
Stark, L., Hoover, A. W., Goldgof, D. B., & Bowyer, K. W. (1993b). \Function-based
recognition from incomplete knowledge of shape". In IEEE Workshop on Qualitative
Vision, pp. 11{22 New York, New York.
Sutton, M., Stark, L., & Bowyer, K. W. (1993). \Function-based generic recognition for
multiple object categories". In Jain, A. K., & Flynn, P. J. (Eds.), Three-dimensional
Object Recognition Systems, pp. 447{470. Elsevier Science Publishers.
Vaina, L., & Jaulent, M. (1991). \Object structure and action requirements: a compatibility
model for functional recognition". Int. J. of Intelligent Systems, 6, 313{336.
Valenzuela-Rendon, M. (1991). \The Fuzzy Classifier System: A Classifier System for Continuously Varying Variables". In Proceedings of the Fourth International Conference
on Genetic Algorithms, pp. 346{353.
Watkins, C. J. (1989). Models of Delayed Reinforcement Learning. Ph.D. thesis, Cambridge
University.
Winston, P. H., Binford, T. O., Katz, B., & Lowry, M. (1983). \Learning physical descriptions from functional definitions, examples, and precedents". National Conference on
Artificial Intelligence, 433{439.

222

fiJournal of Artificial Intelligence Research 3 (1995) 373-382

Submitted 7/95; published 12/95

Statistical Feature Combination for the
Evaluation of Game Positions
Michael Buro

NEC Research Institute
4 Independence Way
Princeton NJ 08540 U.S.A.

mic@research.nj.nec.com

Abstract

This article describes an application of three well{known statistical methods in the field of
game{tree search: using a large number of classified Othello positions, feature weights for
evaluation functions with a game{phase{independent meaning are estimated by means of
logistic regression, Fisher's linear discriminant, and the quadratic discriminant function for
normally distributed features. Thereafter, the playing strengths are compared by means
of tournaments between the resulting versions of a world{class Othello program. In this
application, logistic regression | which is used here for the first time in the context of game
playing | leads to better results than the other approaches.

1. Introduction

Programs playing games like chess, draughts, or Othello use evaluation functions to estimate
the players' winning chances in positions at the leaves of game{trees. These values are
propagated to the root according to the NegaMax principle in order to choose a move in
the root position which leads to the highest score. Normally, evaluation functions combine
features that measure properties of the position correlated with the winning chance, such as
material in chess or mobility in Othello. Most popular are quickly computable linear feature
combinations. In the early days of game programming, the feature weights were chosen
intuitively and improved in a manual hill{climbing process until the programmer's patience
gave out. This technique is laborious. Samuel (1959,1967) was the first to describe a method
for automatic improvement of evaluation function parameters. Since then many approaches
have been investigated. Two main strategies can be distinguished:

Move adaptation: Evaluation function parameters are tuned to maximize the frequency
with which searches yield moves that occur in the lists of moves belonging to training
positions. The idea is to get the program to mimic experts' moves.

Value adaptation: Given a set of labelled example positions, parameters are determined

such that the evaluation function fits a specific model. For instance, evaluation functions
can be constructed in this way to predict the final game result.

In move adaptation, proposed for instance by Marsland (1985), v.d. Meulen (1989), and
Mysliwietz (1994), a linear feature combination has two degrees of freedom: it can be multiplied by a positive constant and any constant can be added to it without changing the move
decision. If the evaluation function depends on the game phase, and positions from different
phases are compared (for example within the framework of selective extensions or opening
book play), these constants must be chosen suitably. Because evaluation functions optimized
c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBuro

by move adaptation for the moment have no global interpretation, a solution of this problem
is not obvious. Schaeffer et al. (1992) presented an ad hoc and game{specific approach.
In this respect, value adaptation is more promising. Here, evaluations from different
phases are comparable if the example position labels have a phase{independent meaning.
Mitchell (1984) labelled Othello positions occurring in a game with the final game result
in the form of the disc differential and tried to approximate these values using a linear
combination of features. Since a regression was used to determine the weights, it was also
possible to investigate the features' statistical relevance. Another statistical approach for
value adaptation was used by Lee & Mahajan (1988): example positions were classified as a
win or loss for the side to move and | assuming the features to be multivariate normal | a
quadratic discriminant function was used to predict the winning probability. This technique
ensures the desired comparability and applies also to games without win degrees, i.e. that
only know wins, draws, and losses.
Besides these classical approaches which heavily rely on given feature sets, in recent
years artificial neural networks (ANNs) have been trained for evaluating game positions. For
instance, Moriarty & Miikkulainen (1993) used genetic algorithms to evolve both the topology
and weights of ANNs in order to learn Othello concepts by means of tournaments against
fixed programs. After discovering the concept of mobility, their best 1{ply ANN{player was
able to win 70% of the games against a 3{ply brute{force program that used an evaluation
function without mobility features. The most important contribution in this field is by
Tesauro (1992,1994,1995). Using temporal difference learning (Sutton, 1988) for updating
the weights, his ANNs learned to evaluate backgammon positions at master level by means
of self{play. Tesauro conjectured the stochastic nature of backgammon to be responsible for
the success of this approach. Though several researchers obtained encouraging preliminary
results applying Tesauro's learning procedure to deterministic games, this work has not yet
led to strong tournament programs for tactical games such as Awari, draughts, Othello, or
chess, that allow deep searches and for which powerful and quickly computable evaluation
functions are known. It might be that due to tactics for these games more knowledgeable
but slower evaluation functions are not necessarily more accurate than relatively simple and
faster evaluation functions in conjunction with deeper searches.
In what follows, three well{known statistical models | namely the quadratic discrimination function for normally distributed features, Fisher's linear discriminant, and logistic
regression | are described for the evaluation of game positions in the context of value
adaptation. Thereafter, it is shown how example positions for parameter estimation were
generated. Finally, the playing strengths of three versions of a world{class Othello program1
| LOGISTELLO | equipped with the resulting evaluation functions are compared in order
to determine the strongest tournament player. It turns out that quadratic feature combinations do not necessarily lead to stronger programs than linear combinations, and that logistic
regression gives the best results in this application.

2. Statistical Feature Combination

The formal basis of statistical feature combination for position evaluation can be stated as
follows:
1. Since its appearance in October 1993 it won twelve of the 14 international tournaments it played.

374

fiStatistical Feature Combination

 
 is the set of positions to evaluate.
 Y : 
 ! fL; Wg classifies positions as a loss or win for the player to move,
assuming optimal play by both sides. Draws can be handled in the manner
outlined in Section 4.
 X1; : : :; X : 
 ! IR are the features.
 The evaluation of a position ! 2 
 with x = (X1; : : :; X )(!) is the conditional winning probability
n

n

V ( ! ) = P (Y

= W j (X1; : : :; X ) = x) =: P (W j x):
n

 There are N classified example positions !1; : : :; ! 2 
 available with
N

x = (X1; : : :; X )(! ) and y = Y (! ).
i

n

i

i

i

In the following subsections models which express P (W j x) as a function of linear or quadratic feature combinations are briey introduced in a way that is sucient for practical
purposes. Good introductions and further theoretical details are given for instance by Duda
& Hart (1973), Hand (1981), Agresti (1990), and McCullagh & Nelder (1989). Both Fisher's
classical method and logistic regression are used here to model P (W j x) for the first time; the
quadratic discriminant function has been used by Lee & Mahajan (1988), however, without
considering Fisher's discriminant first.

2.1 Discriminant Functions for Normally Distributed Features
Bayes' rule gives

j W )P (W )
= p(x j pW(x))P (W ) = p(x j W p)P(x(W
) + p(x j L)P (L)
,1

= 1 + pp((xxj jWL))PP ((LW)) ;
where p(x j C ) is the features' conditional density function and P (C ) is the a priori probability
of class C 2 fL; Wg. In the case that the a priori probablities are equal, and the features are
multivariate normally distributed within each class, i.e.
P (W j x )

p(x j C ) = (2 ), 2j j,1 2 exp
n=

C

=

n

, 21 (x ,  ),1(x ,  )0
C

C

C

with mean vector  and covariance matrix  for C 2 fW ; Lg, it follows
1
P (W j x) =
;
1 + exp(,f (x))
where f is the following quadratic discriminant function:
C

f (x)

o

C

n 



= , 21 x ,W1 , ,L 1 x0 + L ,L 1 , W ,W1 x0 +

o
1  ,1 0 ,  ,1 0 + log jW j , log jL j :
W
W
L
L
W
L
2

375

fiBuro

1.0

...........................................................
....................
............
..........
........
.
.
.
.
..
.......
.....
....
.....
....
....
.
.
...
....
.....
....
...
....
.
.
...
....
........................
...............................
......
.....
......
.
....
.... ....
....
....
.... ...... .....
....
...
.
.
.
.
.
......
...
..
....
.
.
.
.
.
.
....
.
.
.
........
....
.
.
.....
....
.
.
.
.
.
.
.
.
.
.....
.. ......
... ............
.
.
.
.
.
.
.
.....
.
......
.................
....
.....
.
.
.
.
.
.
.
.
.
.
.
.......
.......
................
....
.
.
.
.
......
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
........
....
...............
...........................................................................................................................................
........................................................................................................................................................................

0.5
0.0

L

x

W

Figure 1: Conditional densities and winning probability
If the covariance matrices are equal (=), the expression can be simplified to a linear function:
f (x) = (W , L ),1 fx , (L + W )=2g0:
Interestingly, this function is also a solution to the problem of finding a linear transformation
which maximizes the ratio of the squared sample mean distance to the sum of the within{class
sample variances after transformation. Therefore, it has good separator properties even if the
features are not normally distributed. This is called Fisher's linear discriminant. Figure 1
illustrates the relation between the conditional densities and the winning probability.
The maximum likelihood (ML) parameter estimates are
1 Xx
^ =
C

jI j
C

^ = jI1 j

2C

i

X

C

C

i

I

2C

i

(x , ^ )0(x , ^ )
i

C

i

C

I

with I = fi j y = C g. If the covariance matrices are equal,
X X
^ = jI j 1+ jI j
(x , ^ )0(x , ^ ):
W
L 2fL Wg 2 C
C

i

i

C

;

i

C

i

C

I

2.2 Logistic Regression

In logistic regression the conditional winning probability P (W j x) depends on a linear
combination of the x . Here, X1  1 is assumed in order to be able to model constant offsets.
The simple approach P (W j x) = xfi using a parameter column vector fi is unusable because
xfi 2 [0;1] cannot be guaranteed generally. This requirement can be fulfilled by means of
a link{function g : (0;1) ! IR according to g (P (W j x)) = xfi . Figure 2 shows a typical
nonlinear relation between the winning probability and one feature. Since the probability is
usually a monotone increasing function of the features, g should satisfy lim !0+ g (x) = ,1
and lim !1, g (x) = +1. The link{function g (t) = logit(t) := log(t=(1 , t)) has these
properties. Using g = logit, since g ,1(x) = f1 + exp(,x)g,1 , it follows that
1
P (W j x) =
1 + exp(,xfi ) :
i

x

x

376

fiStatistical Feature Combination

Hence, the winning probability has the same shape as for discriminant analysis. But logistic
regression does not require the features to be multivariate normal; even the use of very
discrete features is possible.
Again the parameter vector fi can be estimated using the ML approach. Unfortunately,
in this case it is necessary to solve a system of nonlinear equations. In what follows, a known
solving approach will be briey described (cf. Agresti, 1990; McCullagh & Nelder, 1989).
In order to ensure convergence of the iterative algorithm given below, it is necessary
to slightly
variable
P i generalize our model: from now on y is the observed value of random
,1 and are
Y =
Y
,
where
the
Y
:


!
f
0
;
1
g
have
mean

=
f
1
+
exp(
,
x
fi
)
g
=1
stochastically independent. This definition includes the old model (n = 1 and y 2 f0; 1g).
The likelihood function L(fi), which is a probability density, measures how likely it is to
see the realization y of the stochastically independent random variables Y , if fi is the true
parameter vector. In order to maximize L, it suces to consider log(L):
i

n

i

j

i;j

i;j

i

i

i

i

i

log(L(fi)) = log

Y
N

=1
XX
i

=

n

j

=1



i i (1 , i )ni ,yi
y

N

=1

yx
i

ij


fi

,

j

i

X
N

=1

X
N

=
h

=1

y

i

log  + (n , y ) log(1 ,  )

i

log 1 + exp

n

i

i

i

X
n

j

=1

i

x fi
ij

j

i

i

i
:

This function is twice differentiable, is strictly concave up to rare border cases, and has
a unique maximum location if 0 < y < n for all i (cf. Wedderburn, 1976) that can be
iteratively found using the Newton{Raphson method as follows:
( +1)
fi^
= (X 0( )X ),1 X 0( ) z ( )
i

i

t

t

t

t

with



the (N  n){matrix X built from the x ,



( ) = diag[n ^ ( )(1 , ^( ))]; ^( ) = 1 + exp ,



i

t

t

i

i

t

n

t

i



n

i

j

 ^ ( ) 
y , n ^ ( )
(
)
z = log
+
:
1 , ^ ( ) n ^ ( )(1 , ^ ( ))

=1

x fi^( )
t

ij

o,1

j

t

t

t

X

i

i

i

t

i

1.0
P (W j x) 0.5

0.0

i

i

t

i

t

i

i

.....................................
.......................................
.............
..........
.........
.......
.
.
.
.
.
.
....
.....
.....
.....
....
....
.
.
.
.
..
....
...
....
....
...
.
.
.
...
....
....
....
....
....
.
.
.
.
....
.....
......
.....
.......
.........
.
.
.
.
.
.
.
.
....
...................
..................................................................

x

Figure 2: Typical shape of the winning probability
377

;

and

fiBuro

Starting with ^ (0) = (y +1=2)=(n +1), the ML estimate fi^ may usually be computed with high
accuracy within a few steps since the method is quadratically convergent and relatively robust
with respect to the choice of the starting vector. Unfortunately, if there is an i with y = 0
or y = n the estimates might not converge. But our original model can be approximated,
for instance, by setting n = 100 and y = 1 or 99, depending on whether the position in
question is lost or won.
i

i

i

i

i

i

i

i

3. Generation and Classification of Example Positions
Value adaptation requires labelled example positions. Here, some problems arise. First of
all, for most nontrivial games only endgame positions can be classified correctly as won,
drawn, or lost; for opening and midgame positions optimal play is out of reach due to the
lack of game knowledge and time constraints. Furthermore, the example positions should
contain significant feature variance since otherwise no discrimination is possible. Hence, it
is problematic to use only high level games | which might be the first idea | since good
players and programs know the relevant features and try to maximize them during a game.
Therefore, these features tend to be constant most of the time and statistical methods would
assign only small weights to them. As a final diculty, estimating parameters accurately for
different game phases requires many positions.
A pragmatic \solution" to these problems is indicated in Figure 3: over a period of
two years, about 60,000 Othello games2 were played by early versions of LOGISTELLO and
- urd-anovic's program REV.3 Feature variance was ensured by examining all openings
Igor D
of length seven which led mostly to unbalanced starting positions. Since early program
versions were used which had only 5{10 minutes thinking time, the games, though well
played most of the time, are not error free. In some cases even big mistakes occurred in
which, for example, one side fell into a corner losing trap4 caused by a lack of look{ahead.
But without these errors, no reasonable weight estimation of principal features (such as corner
possession in Othello) is possible as explained above. Following Lee & Mahajan (1988), all
positions were then classified by the final game results. This approach is problematic because
the classification reliability decreases from the endgame to the opening phase due to player
mistakes. To reduce this effect, early outcome searches were performed for solving Othello
positions 20 moves before game end. Furthermore, from time to time the game database
was searched for \obvious" errors using new program versions and longer searches to correct
these games. Since in this process many lines of play were repeated, the misclassification
rate was further reduced by propagating the game results from the leaves to the root of the
game{tree, which had been built from all games according to the NegaMax principle. In
this way the classification of a position depends on that of all examined successors and is
therefore more reliable.
The proposed classification method is relatively fast and allows us to label many positions
in a reasonable time (on average about 42 new positions in 10{20 minutes). In addition to
2. The game file can be obtained via anonymous ftp.
(ftp.uni-paderborn.de/unix/othello/misc/database.zip)
3. A brief description of both programs is given in the help pages of the Internet Othello Server.
(telnet faust.uni-paderborn.de 5000)
4. In its implications this can be compared with losing material for nothing in chess.

378

fiStatistical Feature Combination

....................................................
.....................
............
............
.......
......
........
...
......
..
...
...
..
...
......
....
....
.......
.
.
.
.
.
..........
..........
.
................
.
.
.
.
.
.
.
.
.
.
.
.
.....................................................

.............
..........

.......
......
......
......
.......
.......
......
.......
..........
.........
...................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.........................

Database consisting
of ca. 60,000 games

L
W
L
W

W x
W
x L xD x
W D
x D xW x

..........
..... ...........
....
.....
......
....
......
....
..... ....
...................
..... ........
.
..
.
..
.
..
... ...
..... ......
............
......
.... ......
...
...
.
.
.
....
..
...
...
....
....
.
...
.........
.... ...
.
... ...
............

.......
.... ......
.
..
................

........................................................
....................
............
..........
........
........
.....
.....
...
....
..
..
..
..
....
....
.......
.....
......
.
.
.
.
.
.
............
...
..................................................................................

.............
..........

.........
.... ...
.
... ...
..... ...
..
.... .... ......
...
...
.
.
...
...
...
...
...
....
....

Game tree

.......
.
......
......
......
......
.......
.....
.........
........
...............
..........
............................................

About 2.5 Million
classified positions

Figure 3: The classification process
ensuring an accurate parameter estimation even for different game phases (which is indicated
by small parameter confidence intervals), this method enabled us to develop new pattern
features for Othello based on estimating the winning probability conditioned upon occurrence
of sub{configurations, like edge or diagonal instances, of the board.5

4. Parameter Estimation and Playing Strength Comparison

Although only about 5% of the example positions were labelled as drawn, it was decided
to use them for parameter estimation since these positions give exact information about
feature balancing. A natural way to handle drawn positions within the statistical evaluation
framework considered here is to define the winning probability to be 1=2 in this case. For
this extension the logistic regression parameters can easily determined by setting y = n =2
in case of a draw. Alternatively, doubling won or lost positions and incorporating drawn
positions once as won and once as lost leads to the same estimate because both log likelihood
functions are equal up to a constant factor. The latter technique was used for fitting the
other models.
Previous experiments showed that the parameters depend on the game phase, for which
disc count is an adequate measure in Othello. So the example positions were grouped according to the number of discs on the board, and adjacent groups were used for parameter
estimation in order to smooth the data and to ensure almost equal numbers of won and lost
positions.
The success of the Othello program BILL described by Lee & Mahajan (1990) shows
that in Othello table{based features can be quite effective. For instance, the important edge
structure can be quickly evaluated by adding four pre{computed edge evaluations which are
stored in a table. All 13 features used by LOGISTELLO are table{based. They fall into
two groups: in the first group pattern instances including the horizontal, vertical, and most
diagonal lines of the board are evaluated while in the second group two mobility measures
are computed.5
After parameter estimation for the three described models, tournaments between the players QUAD (which uses the quadratic discriminant function for normally distributed features),
i

i

5. Details are given by Buro (1994). The postscript file of this thesis can be obtained via anonymous ftp.

379

fiBuro

Pairing
LOG , QUAD
FISHER , QUAD
LOG , FISHER
LOG , FISHER
LOG , QUAD
FISHER , QUAD
LOG , QUAD

Time per game
Result
Winning
(Minutes)
(Win,Draw,Loss) Percentage
30 , 30
30 , 30
30 , 30
30 , 36
30 , 38
30 , 38
30 , 45

116 , 15 , 69
112 , 15 , 73
93 , 35 , 72
86 , 24 , 90
93 , 33 , 74
84 , 30 , 86
88 , 26 , 86

61.8%
59.8%
55.3%
49.0%
54.8%
49.5%
50.5%

Table 1: Tournament results
FISHER, and LOG were played in order to determine the best tournament player. Starting
with 100 nearly even opening positions with 14 discs (i.e. before move 11) from LOGISTELLO's opening book, each game and its return game with colours reversed was played.6 In

the opening and midgame phase all program versions performed their usual iterative deepening NegaScout searches (Reinefeld, 1983) with a selective corner quiescence search extension.
Endgame positions with about 22 empty squares were solved by win{draw{loss searches.
There was no pattern learning during the tournaments, and the facility to think on opponent's time was turned off in order to speed up the tournaments which were run in parallel on
seven SUN SPARC{10 workstations.
Applying a conservative statistical test5 it can be seen that all results listed in Table 1
stating a winning percentage greater than 59% are statistically significant at the 5% level. The
first two results show a clear advantage for the linear combinations under normal tournament
conditions (30 minutes per player per game). Furthermore, since LOG outperforms FISHER
the features would not seem to be even approximately normally distributed. Here lies the
advantage of logistic regression: even very discrete features like castling status in chess or
parity in Othello can be used.
Further tournaments were played with more time for the weaker players FISHER and
QUAD in order to determine the time factors which lead to an equal playing strength. As
shown in Table 1 FISHER reaches LOG's strength if it is given about 20% more time, and
QUAD needs about 50% more time to compete with LOG. With LOGISTELLO's optimized
implementation, the search speed when using the quadratic combination is still about 20%
slower than that with the linear combination. Thus, giving QUAD 25% more time (1=(1 ,
0:2) = 1:25) balances the total number of nodes searched during a game. But even with
this timing, LOG is stronger than QUAD, and FISHER can still compete with it. All in all,
the quadratic combination is not only slower than the linear combination, but it also has no
better discrimination properties. Indeed, a look at the estimated covariance matrices of each
6. LOG's 11{ply evaluation of these positions lies in the range [,0:4; +0:4] which corresponds to winning
probabilities in the range [0:4; 0:6]. Only nearly even starting positions should be used to compare
programs of similar playing strength since in clear positions the colour determines the winner and the
winning percentage would be 50% even if one player is stronger. Of the 100 starting positions only six
always led to game pairs with a balanced score.

380

fiStatistical Feature Combination

class revealed that they are almost equal, and therefore a better evaluation quality than that
of Fisher's linear discriminant could not be expected.

5. Discussion

In this paper three statistical approaches for modelling evaluation functions with a game{
phase{independent meaning have been presented and compared empirically using a world{
class Othello program. Quadratic feature combinations do not necessarily lead to stronger
programs than linear combinations since the evaluation speed can drop significantly. Of
course, this effect depends on the number of features used and their evaluation speed: if only
a few features are used or if it takes a long time to evaluate them, then the playing strength
differences cannot be explained by different speeds because in this case the evaluation times
are almost equal. In any case, before using quadratic combinations the covariance matrices
should be compared; if they are (almost) equal, the quadratic terms can be omitted and
Fisher's linear discriminant can be used. Therefore, the motivations of Lee & Mahajan (1988)
need refinement, since an existing feature correlation does not necessarily justify the use of
nonlinear combinations. Generally, possibly more accurate nonlinear feature combinations
(such as ANNs) should be compared to simpler but faster approaches in practice, since their
use does not always guarantee a greater playing strength.
Besides linear regression and discriminant analysis, logistic regression has proven to be
a suitable tool for the construction of evaluation functions with a global interpretation. The
drawback, that for parameter estimation a system of nonlinear equations has to be solved,
is more than compensated for by the higher quality of the evaluation function in comparison
to the other approaches, since in this application the parameters have to be determined only
once. The current tournament version of LOGISTELLO uses feature weights estimated by
means of logistic regression and profits from the comparability of evaluations from different
game phases which is ensured by the use of value adaptation. As a result it is possible
to perform selective searches in which values from different game phases are compared;
moreover, values from the opening can be compared even with late midgame values in order
to find promising move alternatives in the program's opening book (Buro 1994,1995). In this
sense, value comparability is a cornerstone of LOGISTELLO's strength.

Acknowledgements
I wish to thank my wife Karen for competently answering many of my statistical questions.
- urd-anovic for many fruitful discussions which have led to
I also thank my colleague Igor D
considerable improvements of our Othello programs. Furthermore, I am grateful to Colin
Springer, Richard E. Korf, and the anonymous referees for their useful suggestions on earlier
versions of this paper, which helped improve both the presentation and the contents.

References

Agresti, A. (1990). Categorical Data Analysis. Wiley.
Buro, M. (1994). Techniken fur die Bewertung von Spielsituationen anhand von Beispielen.
Ph.D. thesis, University of Paderborn, Germany.
(ftp.uni-paderborn.de/unix/othello/ps-files/mics_dis.ps.gz)

381

fiBuro

Buro, M. (1995). L'apprentissage des ouvertures chez Logistello. Magazine de la Federation
Francaise d'Othello FFORUM 37, 18{20.
Duda, R., Hart, P. (1973). Pattern Classification and Scene Analysis. Wiley.
Hand, D.J. (1981). Discrimination and Classification. Wiley.
Lee, K.F., Mahajan, S. (1988). A Pattern Classification Approach to Evaluation Function
Learning. Artificial Intelligence 36, 1{25.
Lee, K.F., Mahajan, S. (1990). The Development of a World Class Othello Program. Artificial
Intelligence 43, 21{36.
Marsland, T.A. (1985). Evaluation Function Factors. ICCA Journal 8(2).
McCullagh, P., Nelder, J.A. (1989). Generalized Linear Models. Chapman & Hall.
van den Meulen, M. (1989). Weight Assesment in Evaluation Functions. In: D.F. Beal
(Editor), Advances in Computer Chess 5, Elsevier Science Publishers.
Mitchell, D.H. (1984). Using Features to Evaluate Positions in Experts' and Novices' Othello
Games. Master Thesis, Northwestern University, Evanston Illinois U.S.A.
Moriarty, D., Miikkulainen, R. (1993). Evolving Complex Othello Strategies Using Marker{
Based Genetic Encoding of Neural Networks. Tech. rep. AI93{206, Department of
Computer Sciences, University of Texas at Austin.
Mysliwietz, P. (1994). Konstruktion und Optimierung von Bewertungsfunktionen beim
Schach. Ph.D. thesis, University of Paderborn, Germany.
Reinefeld, A. (1983). An Improvement of the Scout Tree Search Algorithm. ICCA Journal
6(4), 4{14.
Samuel, A.L. (1959). Some Studies in Machine Learning Using the Game of Checkers. IBM
Journal of Research and Development 3, 210{229.
Samuel, A.L. (1967). Some Studies in Machine Learning Using the Game of Checkers II.
IBM Journal of Research and Development 11, 601{617.
Schaeffer, J., Culberson, J., Treloar, N., Knight, B., Lu, P., Szafron, D. (1992). A World
Championship Caliber Checkers Program. Artificial Intelligence 53, 273{289.
Sutton, R.S. (1988). Learning to Predict by the Methods of Temporal Differences. Machine
Learning 3, 9{44.
Tesauro, G. (1992). Practical Issues in Temporal Difference Learning. Machine Learning 8,
257{277.
Tesauro, G. (1994). TD{Gammon, A Self{Teaching Backgammon Program, Achieves
Master{Level Play. Neural Computation 6, 215{219.
Tesauro, G. (1995). Temporal Difference Learning and TD{Gammon. Communications of
the ACM 38(3), 58{68.
Wedderburn, R.W.M. (1976). On the Existence and Uniqueness of Maximum Likelihood
Estimates for Certain Generalized Linear Models. Biometrika 63, 27{32.
382

fi	
fffi 	


	"!$#&%'(()*+,(.-+0/1

<>=?A@CBD=EGFHEJILKMEGFONQP&RSNUTJFCV
=Fa`

2345$68739()3:;4
!6&'19(3)

KMWQPXVY=N[Z\FCIEJW^]_TXN^=EGF

TbWQcJE&de=TJFf`

jSkmlonQprqtsguvrwmxk

EgVePihB

yz{|}~Qb}~8 X~{z

b&Xr3..b
 Yr 
Y$$QXr"
Jqkmk8qlokQvQx

~~\b~8 }{| {}} }

b Xr3Ori0Jbr3..
 ^33
SYGS3833O

Ar
$0Q"X"$be




g

S$e  SX0$C  0S^

JXbb3Q$QgO
Dg X >0goC800Y0
OS0b U03$e0bC
	
QO Sfffi
   0^8[
8 Smg080e$XS
 
O"gC0SYJS0
i0Sff$^8JJ3O^S
$g33$$$ S


 !#"%$&'!
(*),+.-0/21ff3545+768/21ff9):0;<:0=>+.:@?1ff3AB+0)97/*CB+0397;<:04Dff9:E-B14;2=.:0;GFHDff9:I?/2JLK0;M:0CB1ff),1NCL-J>?OK01PAB)	1N4Q1ff:0DN1
+765/+0:0=7R'?1ff)3SCB1ffAT1ff:0CB1ff:0DN;1N4U;M:V?WK01X?W)9Y;M:0;<:0=ZCH97?W9B[]\^41N_B`01ff:0DN1>+76)9:0CB+.3Sa79)	;<9-0/21N4Lbc1[d=H[fe
o
9g41N_h`01ff:0DN15+76*+0-041ff)	ai9Y?;2+.:045jffk
k
kn
kpo#q.erCB1ff:0+?1NCsk
;244W9Y;CX?+g1NuhK0;<-0;2?v/2+.:0=7R'?Q1ff)3
'Nl +7l
mffm
m
lffm
mffm
'Ht
CB1ffAB1ff:0CB1ff:0DN;21N4;G6w?WK01xai9)	;<9-0/21N4%k n 97?y9z=I;aI1ff:v?Q;M3{1}|~9)	1*4Q;=0:0;FHDff9I:?/2JCB1ffAT1ff:0CB1ff:?%+.:?WK01xai9),;M9I-0/1N4
k n<

97?35`0DNK>1ff9),/;21ff)5?Q;M3{1N4| 
|W[X':U?WK01N41Dff9741N4ffe#94QJ.4?Q1ff3?O)97;<:01NC+0:X?WK0;24CH97?W9Ebc1[d=H[fe?+
1 
35+0CB1N/x;?Q4CB;24?W)	;<-B`0?;2+.:wex+.)539I71DN/<974Q4;GFHDff97?;2+.:04+.)5AB)	1NCB;2DN?;2+.:04
KB974g?+>-B1s9-0/21?+X4?+0)	16+.)
t
9)-0;2?W)Q9)	;2/2J/2+.:0=vCH`B)9Y?;2+.:04}-0;2?4%+76;M:I6+.)3g97?;2+.:5;<:g;?Q4*4?W9Y?1xai9)	;<9-0/21e0Dff97/2/21NCs n K01ff),1[y:=I1ff:01ff)97/e

?WK01CB;Dff`0/?	JZ;4:0+?+0:0/J?Q+)	1ffAB)	1N41ff:I??WK01N41s/2+.:0=YR'?1ff)3^CB1ffAT1ff:0CB1ff:0DN;1
4ffe-B`0?P9Y/4Q+?+ZOO>9
)	1ffAB)	1N4Q1ff:?W9Y?;2+.:X+768AB974?OrNi8K0;2DKU?W971N4?OK01ff3;<:?Q+97DNDN+.`B:I?ff[}Off0W	ffrH
.	#rff#Qff
b`B3{1N/MKB9I)	?ffex;<:I?+.:wex;2/2/2;<9354ffeffIB%;2/2/2;<9354sw;<A041ff)NezffI

ep6+0)v1NuB93A0/21eKB9
a19:
t
;<:?Q1ff):B97/~4?W97?19:0CX9)	;2DNK1NuBAB)	1N44;2a15AB+7x1ff)?OKB97?AB)	+7a.;2CB15?WK01ff3;2?WK?WK01:01NDN1N4Q4W9)	J/2+.:0=7R'?Q1ff)3
351ff35+0)	JDff9IAB9-0;2/;2?;21N4ff[
\z/2=+.)	;2?WKB3{4*?WKB97?#DN+0`0/C1ODN;21ff:I?/2J/21ff9):5?+v),1ffAB)	1N41ff:I?*/2+.:0=7R'?1ff)Q3]DN+.:I?1Nu0?#}+0`0/C-B1z`041O6`0/r;<:
39:IJ9)	1ff974x+76\)	?;GFHDN;<97/:?1N/2/2;2=1ff:0DN1[xH+0)#1NuB93A0/21eB?WK01NJ5DN+.`0/2Cs-B1v9IABA0/;21NC?+39I:JAB)	+.-0/21ff354
;<:L:B97?O`B)97/x/<9:0=.`B97=1sAB),+.DN1N44Q;M:0=re-B+?WK9Y??OK014Jh35-T+/2;D/21Na1N/{b1[d=H[<e~/21ff9):0;<:0=E=.)933g9)	49:0C
/<9:0=.`B97=I135+0CB1N/24
4JB:?OK01N4;24

[

t

ew9:0Cs4W`B-04QJh35-B+I/;2D/21Na1N/bc1[d=r[<eH35+0CB1N/2;M:0=sAB),+4+0CBJ6+.)4WAB1N1NDNKX)	1NDN+=.:0;2?;2+.:+.)

t
:+.)	CB1ff)x?Q+?O)97;<:?WK01/21ff9):0;<:0=4J.4Q?1ff3eBK0+7}1NaI1ff)NeB9:1O1NDN?;2a1v351NDNKB9:0;24W3+Y6Dff)	1NCB;2?9Y44;2=.:IR

351ff:I?8?OKB)	+.`0=.K?;<3515;4v:01N1NCB1NC[+DKB9I:0=1v?WK01AB9)Q9351N?1ff)	4+76*?WK0154J04?1ff3;M:+.),CB1ff)?+DNKB9:0=1
?WK01x;<:?Q1ff):B97/H4Q?W97?1%+76?WK01x4J04?1ff3]9Y?#?;<351|WeI4+v974#?+;<3AB)	+7a1N?OK01};<:I?1ff):B97/r4?W97?Q1*+76w?WK01x4J.4Q?1ff3
 '((3)QfifiB6	Q	6%X0	#r	3ff5$	#4
!'fi

!3c67
g

fi#HrUBHOBBB

<7ff<W0Nh0ff0Nw00vff	NffB	2N2B,.B7.7Qff	NB28.ff	0x<I.Q72.B7I	B
<Z<5Lr.gNhg0xO0xB'>N2N72.	2WB.XXxB}~NW,000N
8
>N2ff%ffTwN.<00wy0<0ffN#0.0H0xffz0>O0{BYYB	.B7072.WB	.00X<5
72.,OB.	NffB	ffIv0ffB7x0N	}0B5N<B	Nv7<*ffI	N2X.XW0N>I<07	O

ffB	2.wzB5ff	00v.YBffI	'BNNffIBYN72.,OB5vB
BNffEB	.BINX.z20M0W0
ff	NB287Q2.B5ffIB,.02ff5xM,NffB	ffI0N	x.IdH<0B5N<B	xN7<wffT22<5

<0ffN#
N#N
Is	NQff	N0ff	B
 0B0B7NQffYBff022N5<sO7<0<0s	NffB	ffI



0N	x.I*5Bff.

W7WI*<802NW0zffT.7rN.I<0ff0N2N8B	NffIxMO0<BB0
	7.0WB0

NB0ff0NNOBE2.0<Iff	772@%ff02Hx0<	xQ7N.0zfffiffH%XNffNzffT}x0xffN
ffff.N%ff02N*78	ffff.B
72 0B0W0N.	NQffYH	ff7.0~.~W02xBff02	{I0B	7N

X0N.72	NW025.B5NO	2BBB52ff7Q.Qff5v2WX0.I'2<0ffgW7Q>0N.,'W7
	NffB,ff0NfiW

  22vBM0ff,ff7<02Bff0>WQ7<W0NL70ff

2W

.YBffIvBNQNff7W0HBQ72.X7W0BffBff0Bff0N2NTffI0WB	NEM0ff,ff7Nffs N

T

"!#$
%&('*) + 21 g7WI
%,&('*-/.) 0
ff7QffY}B720257W0B	.02ff^O0iWB7ffpBffBff0B<00XW00.43563 7W07.7N00MI
7O	28*.0Y8	}N00BQ00	2N<UW0NE.Qff5ff 0ff93:32;
IO0BhB{N57
W0vYW	27*B	<7wBff	2iY2N7#W0W7Q0N0	'W7QzB0N2.w

W0v0N	x.Y27

	N2MI0.,v0Q7#MI.g72.g.02WQ	HBY2.0ffBNff2W

B0B0BNs<BB0v02H07xNffN0.YBffIO	NWBNNzff,.x782ffQM{ffiII
2WNBB00ff<722'77x.0vB,.B7.7QN}O0ffB7N	<<52<W0W0ffxB0Bz0ff

3:3"=

I.7B2ffI*ff?>H7LB7N	0B0*W0.Qff]2x2.ff722B0W020ffB0x	N2<02

0	027}<I.7Q0.2.0<55%ff02sNv7	ffff.O0ixN>07

O05.B,W0

2ff0<0sY*007'ffBffBff0Bff0N2NIsB0<0NBB.0ffI<7225.	xN0z.O0<@>0ff0N
78O0.		'ffQBffTff0Bff0N2N

c<EN.BI	2.UX007'ff^BffTff0Bff0N2NW7ffW0.7B2ffI

7}gNx'B0N2.2WX	NWTNN8QWQ7<B02{BI5Nff,ff6A0{IB70N072,NW02B022N

s0.I'2<0ff5BI5Nff	BNNEBhB{ffY*0ff5O0>Yv5v	NffBQ	ff50N	}0
B050
2<0ffvB,.B022Q2s5.BN2W0U7v02BBff>>Yi50BN2PXONCA0N5.BN25	

WTNNMY*ff757x.BzB	N.2.05	NW02v<Xz0NXW0DU0.QE3:63 
 IBNff0gW027O	2
GFHI(JK"LfiFMHNOJvYW	2pd<g7W	2QP

7xW0QQ0>B	.B02222NRPS!#UTVXWY3:

80ff	O0vWY87	<02[#ffO7]\020B5Bff7y77<0Nff

 UZ'N

A0PYMEN0W,MB0Q0E7O0BTff52W0ff	O.	sI>N0ff02.E7zW0s0N.72s,NW02

.B0LE*ff0IUN{YL,fffiff. sO0ff7Q7C^QL@_a`fiIb(NOLcedfI(ghiF#z0NE<0N<0BsWI0H	
XxB

j

N7<%ffTwN0<0.ENYf*ffI7vxN2}Yi	<7Q00YXXW0U7

BB0k	<0WB0XX j <XXWs%ff02YN.0fflIBN*0L#,<72m<0Qff	i02

>77onNN22.V*	0NNN>p<zqnvW0.0B<HffTffTVrB	2Ww
NZ>\0
WB75<Eff0ffY%B0ff005ff0.E78Bs02.

7N.QN.0ff	NB27Q0B5ff
#H0XW0

ff	0B2N27#W0vWQ022.sB,.B022	X7W	2NNffrBBff,TWO0	ffB	NQffWY2.s0W0
2ff0<07y2.0Y'ffN.IN0<W0v02BBffWYi,MI0

%Os0.5Iff0N.0v0>0.I0.{ff0N.0z>770MIs50BN2	N.02Bff	NutvI@dIxw/h,y
czhkI{@F80ff	5ff0WBY8W05WQ022.XB	.B02222N57*W05>775.BN*I	vN.0QWI87ff
<5O
| ~}vI@c[yK"IdfIaw/hczhkI{@
F 5ff0WB7zW0NO022.>B,.B022QN	722ixN>QsBBGc
ff	ffz.ff7NsQM{QffwwdH<w7gB0NQ0sYN0ffB7~<BB0vWB7
sBgB
s ff	ffIv7
ff7N5<5Qffw j gW080.5Iff0N.0*ffYcdr<7W0HI	XXWN7W0N50BN2xff52ffgW0
B2W	<B02.~
T O ~7.0OB0#Nh0ff0NNY 

 I{Y0NMY<0v5.0WB0#B2O	G


( (a 
kzfiaY
2
:$Ofikx/(( afixOV(x(Y
OVUx:(fikx
 ' kYO6(OfiOkfi ''-".  x(   '*-/.[  'O

((

fififizaO[qqR[[z"[*[ezfi

["aB/fO
: Y x$(fififi"  fik"p[Ba(aSa
fiafi"B$ a ]
"V"/@"/f@("/"
(fia
a"aBaB/G"/"
["[Ba
["aB/"C@/"[BaB/[fi/[["a["("BfiV"a
f"[fiaB/"k""C6([["v"G/"
["Vx"("  
"C(afi SQ 2("B
 afix/"6  /"
""CB(x"?qf"[6
     fi$
"u/"[BaB/[fi[Ba
["aB/ fi
  "B(G["aqaR("a
/"
["a"("f  $"(q[["x"("
   B$@@( 
  fi 
a["("R/ aaB/q/Sfiaa	 z(fix"VB
o([a (C"/ff 
@  qk"(afi
 fi 
 "/"[ 
 p( [( S[B
2(   "a a/@
["aCVB
 "[[( a
a/"
V@aV"V/" ~aR["BBQk"MG?/[["$fiBaauaBv[ /[(x"("
   SO[Ba( a 
fiaB/"q"[(Vx ! "?# "~[Ba/[@aok[ fi"k[V[
[fi[([("[]/G
"fik
aB/ 
 
fi(
"~/"a( 8""
["Ca["("  " 
"~aafia [Ba
["aB/ $
"~a
fia
 ' af
[fik"x"("~fiaB/"(
  fiC"ok"x"("~fi6[["a

a% $x/&  (
  o)
 fi *
   Vq v"+  fafB@] k"[(z[fik" v
f
/"a"Bf"
"
"/@"/?("B& fip
"  , 
fifiR[fi $  k"CBBfiV"zz]
[fiB"xk"C
?Cff [f[ . 
a~[("af
"]a"("u
[fi/kB$ka"aBaB/q[/[@"BBaB(    [["a" Vq C"
  fi 
fiaB/"//
 fi *
 "p"/fiaB
"Ba[a(@aCk"[( [ a ["BB("BVaB(a""CfiB/ k[f
 2 ["(   /k"(/afi[B(@[fiaG/"a

" fi
"
 0  fiB/B
[   ("ax(
 1YfiY 3
f "(fiaB/ fiB/B
[f$"BMo # q/G/xfiz[fiB  @"QBa(xfiaB 8f"[4 "" /@a/"""fi"
[a@fa(O
"ofi
x"aBaB/q[/["BBBaB(/[@afa( $fiVu["x"q/["a"G
"a
[/[@"BBaB  "[(a""af "fB(a""~(Ba(B/ 

5687:9<;=?><@A9<;BDC9FEHG8I>JEKBD@ABKL?9FIBK>JM
NOa/

 (  ff
fifi"@f/[[P"of[Ba(xCaVakfiaRQ/SUTUT+TVW"z[/[@"BB
[
aB~
a"aBaB/Q["aB/ OakfiafaQ"X"akfia  "8 [ /["BBxaB/"
["["aB/Ox
fiafa
  "C[Ba( afa
fia] @"  (

fi]fi"ZQ/SUT+TUTV[W?$(fix?Caa( /\ 
/"
[" Y
VBBp$ x^
 ]/_	`"V
"fB(f(@  SDa  fi6fik ]]
 ]Yb  ]]cT+TUT&] /Vk"^
 V<dfeR[fi(fi
 ]]
"e ] b  _	`V"
"B(f(@R  SKa  fi?
 ] b  " 2 ""((   /@
"z"aB/x Vq (
"8 "("k[Y(   /[aBk(("C"[aBaBffi
  
 "(
 g*hiffj!klUmfhnf"[([("[("fix
["aB/u"Bk[fip
"a
aV@"B  
[u "

"[fiaffi$k"Ra["(" o   :  S
<
 pUS+TUT+TffS
  qF    :  qF 
 N"
"(?"[(("[("ofi


["x"/"(
"a
aV V"B[[(["Vf/"k["pYB6""a(Y"B
[fipk"V[Ba
["aB/
fipVaf(
 r2[/"V[(("q/
"@
"($[aV@"B$$"(G  B(  "
fiaka"a
aB/"Vfi2afR
 rp+ f[([(" "
"s
   O
"S([a(@6[["/ SQ /2
"([a(@VfiaB/
  fi   "( CBBaG@at
/Y
 fi *
 Vq tVq
x"aBaB/fik u]  [P "o 

] _	` f 



fi$

 ?
a , qFp

S,  w v


$"(*v B o@a/ffi!$ "ak"B [@afa(  9  
"Q"/?("/"~(fia
"q
x"aBaB/
fik BV/"a
@(@    ]   ]  "v[@afa( xvG
"("
[fiB[a~[(@a	zqVB


"B(f(@afik"
a"aBaB/Gfi
 #y] 
 "k["(fBaaB/"[([(" /Z  "e
"[a(@akfiapfif
	zo
"""
["
fiBau(fiBq(fBax"  [Ba
 ["x"Q    S,  ffz  "VB
 [xfa(H
 z   /2X u" p
"
 ff 
fifiV[fiB"/f@("/""]
"/"
["Yfifi"p[B/"a$
 "Bafi[[[( [[B& 
  k  
{[
"(~k"[a@fa(H
 z ([]BBaqG8
 {
 uVqfik u|u}
 |s~ _  O A

U

fi.J<ZJP

RwHffXUX( tUyRUU,!ffXt!XX&8ffwsffw&ff8f  ? JR
UXyU!ffU84   *UX8U!ffUy&U&X,UffXAw^X%8/U
 &fftff!,w!ff(R#sffXXXffX)!XXyff^#,w&ff*8Uxtff!-ffw!ffXw
!ff!?,XffX&X&AXw,ffXffU&U[(uJXcXX!ff
*#,%Rffw!ffUYUw,w!ff*RU*&XU&ffXA%XXff!(P&X	
ffw!ffy  .Zff))U+U&w[u%XX&*ffwtff!Hffw&ff
8w  ffP})Y8U%cfft,ffUFcwUcwUtUUff%ffX!XX&cff*w
ffw&ff*8f  X  ,  ff
P!ffRwffffffUX,wff!X)&PUP&PJ8H<ff(X(&!ffffX!ffX
ffyw8ffP!ffXs!?w8)ff%yX!FK)&ffUX[w8&&83X  A/Y#
Xffw,!Xff8XU&8	?wU&RR8X&ff,
} 	
fiff ff[
YtUff)XX,#8!ffUZff)8!Xtwtw!X 84   t.wy&
Y!X)UwR 8f[  P?   tXU&
  X8&ffXs!w(ffXffU&H!.w
X YyU%XX8wXYw!
 . %#&w Uw4ff !Rww
wffffff#&Xt&tff8*UFJwtU!ff!
 "Y** #H
XY!F% $'&'(* )FXfftXY!F% $+',* -/UX% $'+$.H/ .0"Y**( #U2
 1  ff!ffX4[% $'$'3
s#,Uff!ffXU<Y!XP(ws!X/wUxU&ffU&)fwX!sw&#
U&w!ff!ffcX&HXU)wywffXUXcwyXff&XX8X4
 #H
X%4*% $+'5Xw!!ffXc*,8,!U&%ffX)ffU!,#!&wt.&X
wYX8w!ff^!wYff%#UH!}PUffU&ffU&Ywy&XwXX?ffwX},P
ffU&+K%#<!}PXffU%ffXUPYUff!#PUK4ff&JXffPw,ffff
&#&8!w#XX8 #U2
 1  ff!,X% $'$'3J% $$'3XwXt!

 :4 Uw  A-P  w.A84  P  Aff,  = <?>A@ > 7 f   6 > 8  
6798  ;
&U
B 798 ;
 :f  A,ff  . < > @ 7 > f. C  ff8f   C  ;
 D&,9 C  E B > 8  C  
U&RP(!Uff!Fw/#!X)sw!X&Ps}!ffffffU F
84   ,   .;
 G 6 798  
7

 I8wtUffcU&ffU&B 798   JLK wUSB 78 R JTK KcffUff!
H ffyw!8	
JNM'OQP R
JNM OQP U
8!P&XXUHRUy&ff
V
V
 X
 W  @ZY [N[%[ W  @ZY  
]
\  ;
 @Y WH [%[N[ @  W  \ 
V
U& _
 ^ 6  8 U+ 6a` 8 9 b Y A \  _
 ^cB  8 U+d B ` 8 9 b Y e
 WH.(!!8!P&t!U^#,ff*&
ffX8f  u ff,D) ywZg f9hcX#U^Uw:Y8!P&i
 @/8Xw!tw)w,ffff
&#ffXR!Hff0
 jw @/D 7lk ;
 :s8P  Offff.&wFX.URU!ffU JXw!
wRX!Xw%ff
@0m o n 8  p q
 @Yo nL@/o nrC  [%[%[ @/  @Y
 ,
JXff-!}8%w&XX s.U(YffXff/XffU&YP tXX-!wH#,w&ff!JwHffw%ff?o n
o n  p
!,^u
 jD.swffw&ff!P.ffw!ff?%ffu
 jwvjK' F@ 7lm k 8 ;
 :sXx nxOffff o n PJP
yTz{|~}/co29|oL/9w%Nd/%d9}/909NN|'|99|%|u%d|%cc9Qo%
< ao/9o/z
%%

fior9o2xA'

w%*'wQgar2XQLwL''Q'Lw%'Q*wLgg%%?do
%%LxQ*d%r%A%%LgLLwQgqr%%%roeL*QrQ*
go  %ALL*~eQQldL*Qro*L~*eLLQrLQ'%  ALL*~L2*e'	*
o%%%0 o ?d%E%rL%4/eLLrQ*e='0Qr~Qr%rLg
'?
o%%rLN/gQworLLrQ*Erwo'*Lr*~r'%rQ%r'Q
*roLLrw*LrQ'QLL%
9  	
fiff
 fi
	ff!#"%$'&
)(*"%+,$'-,/.0213(4#"%+5$fi6 
C2D > $ E G
 FIHJC2DLKNM=OQPSR

$fi.).7#$98:"%<;= L*rQ'?>A@ +5$9"%"%B

T U.V$'"%$9&B QLL'0L?ro~Qo'Q'
W27
X r%rQ*=0wQQQrQr C2Y % KNM=OQP  ZFIHC2YZFIH 

 fi
	ff  [N"%B-7\]?.7"%$'-*(4#"%+5$'-,/.0*1^#_SQ]#"%$'&
`.7a
b!#+_*(4#"%+5$	6 cedgfihf $	.j-,k9kl58 
rrLrrQ Q#m
+ .7$9(ZnNklo r*Lrg $'p"'\]$	.qniVni
+0r$ Es H=f tu GFIHwvyx^KNMvzx|{
{B{}~{
 fi
	ff'1Jk'k @ ];7kl(4#"%+5$9-57.Q01S#SQ]"%$9&B(*"%+,$	6A$fi..V$98A"%;  L'ol
Q g $ EjB&
B+,og+_ @) -5#klb!(*/J.Vb!($	.n?.7$9"%$'&
 R 1J QQw'Q (4#"%+5$fi6r$	.j;=#"'\+_ @ #S8-,#klb!(4
#k'kl @ ];7kl R

*LrQ'l X%e0rrLorLeroLLLrrQrQ*r'4L*rro
G'Lo4w $9-
$'8#
-,m(4#"%+,$	6  L*rL*or'Q'%*L*rQ'0lX 
Q4 D  x l X0*L'~%oLogA'rQrQ'w%QL  '~ x uwoLQ%LEX
  Q0LLLrQ*Q2l XLrLA*o0r'4
 fi~LrALLLrL%E%L
*~Q'%Q2*A%rQL0  ~Lr%oLSo%r~04 "%5n#kl=!o 
 dQLL
l X  L*rL*er 
8#/. 
 ;9wQQuQr2r
 rrL ~Ll fi o2'
'= o'2*LoL


 fi
	ffL,+5+58#b!-
$%;7klq#"%+,$'-57.Q0w1S#!#"%$9&B }} (4#"%+,$	6  fi$ .q.7#$'8*"%4;= orLLoQ
$ EEV#+`
&B
+5omni#$'+ M=OQP _Ej$98#$'-,/. > v  M=OQP  n?.V$'"%$'&
4$9"%!
+Z. R " R  Z  FIH2DSR
l X  Q2orLLoQS*Q r*LorL!*r'SQ .V"%+_#]klo-,SS5-
"%58 9c'o
%7X*Qrr~eALL%''o0wrrL M=OQP L  +_,8#b!-
$%;7kl lXQ*0Q~'
   FlFGD 9c'o%~Qe?4Q%'Ad* * M r2QrLLL
orLLoQ'2:
 %c% m
  M Q%gQL n
+5$9B8 uo7 X M l   M Q0*%rLwL*~gQr*~9c' 'rj
 
   FlF~2D 9c'o%E'g2w Q%'p
*QL m
 =# S #Lr'oxowQ
  v  M L /
'?orLLoQE XguogLLN'~'~%Eg  Q g%QQL niB+,$'
8 
l XZ A%Q*el X~gc' Q%'gLLQLo0rrLorL
rrQrQ**r'
 
 e7 X'Q%EgQl Xe4%EgA
 ~QQQorrrL'e*r'
  u 
 uQ*E
 grro~d* *~u~rrL'LroerQ'E04Q%'
 #**~
rQrQ'orL'%m
 


 fi
	ff: T +5$9(4$'"%$9&Bg(4#"%+5$fi601#_SQ]#"%$'&
4(4#"%+5$fi6 
 `2DSR
76?$fi.7"'.J`ni?.7$9"%$'&
4$'S"%Q]B+  . R " R




$	.j.7#$98"%; o~QrQ' $ Eg"'\]B+_

fi#S9eS5

G!
_5%]_#fi]=#Z5A7]_7,!!!A_fi4l=l#g?5_lN]!g
#AUGB!JN!?,
?l
!#,]
?=
q,=]*p7_7
#qr!!7
#!I,!
_j7!l==j?,?l
!#5
p7_7
!!7q#!S,!
*#_4?l=?,!m?l
!#5`#jL7=#	fiG,!4#?l]
7l?j|qGlm7]!=l
,g?5_l77%#!7]_7,!!!),!]#!,`!lVL#_p_fi4l=l##
Z#=j5?fi=_7S!7fi!lpg?,lJ7l,!
J
_l]lg]m_fi4l=l##fiSU
_l!2
7#!,?
g_fi4l=l#`==!VU==l*gU,_lgl!777,#_ll?ll?G#!l#
Nfi^NAmBN=~
~S44!NBS
ql]7l#
#77=]`?`g?,#!g5!7 7]=_75]!fi!47l#
#??fi!7rB#q,!q%#llZ!
_]
_=l7g'=7
 |7l# 	 ff
fiS%]G4]]7l#
U!74#!7l#
#77=]_5
7=
=4fi#]
!
_l5!J
=l_pg?,S#!
% #fi


 
  "! 	!#

Z#=m,?|!`==!VU==ljg?,)( $ 
( ,!m_#7=q7l#
?fi!g?G!]=#!!l7
#rJ,!?
?q%#ll?G
&
 %7=fi!r'
 :#_]gU
!qJ]!,U



 
( (( (
( (
 +
 **  "! ** ,   -! ( ( ,  "! , ff
	! *
	!
**
*
*
*
.Z
!7p?l5!r7l#
?fi!7rB* #!]= l* 7=g]/i?q=)#0%7g!*5N!5!r#77]J?m]!7
1 32fiff4
547676684B:9<;fiGm!
_=5;G
!#=7,!,#!,=p?S?>!fi!7@ 1  1 25N!l=l]?
==!7?==ljg?5_l77
JlJ#7l#
#??fi!g#! 1 lm7]_7,!!!_l]G7l#
#77=]7
Z!%#ll?Gfi!j,!7]
GlS!=5!fi4V=?7=
_B7A fi!*!!4##
!7]!]S!7=|?S==!7?8C
=l*?,_l77*%UfiD."E0
E ,7
I~
H 
=_]5C#J_]
!fi!gG!7]
g$KML NffNPOQSR$UTVQXWffYZ[=Z\YOffY]^Y_R#`aWffbT<c	RdNMe:T<fgThbT<c	R
fgWffbe:TjiffklnmoR	YpbhmoR	e8RqR irTVQSbhQW5YsR	Tt`aR7Y_c7WffuBLaRwv$QLox mpbhmoWffb#y

4!NBSGF

z kv{TjQe8R|W5u}WffY~NO)QSThbT<c	R
 kv{x|W5YsRqW)Q|QO7x	T<WffbR:~$T<bhm$QSbe:Thx7buBNO)QSThbT<c	RguRISbWffY~$e:Tt`5mobR7T`oR7Y_c7R:x7bOffeQ:
( (
 v5
 kv  SOffedWffYR7T`oR7Y_c7WffuBLaR0
 kdbhmoRgR7T`oR7Y_c7R:x7bOffeQWQ|QO7x	T<W5bR|~$T<bhmv{Wffe8RLaYTh7LaRgbO{x|O5YPQbWffYbfgLaubTNMuBRQ7k
( (
( (
 kj ,\, W5Y~\TjQgWffYR	Tt`aR7Yc	WffuBLoRO8 g bhmoR	Y  , vPkg0Offe8R|O5c7R7e   v0Thf-NMuBT<R Q
 gk
 kv{TjQW$QT<f-NMuBRge8O7OffbO8bhmoRx moWffe8Wffx7bR	e|TVQbT<xgR:7LaWffbT<O5Yr7
=4fi#Uv]k
7

fijhP0j0XPPaPVP@ff
taP8aj{|agPaoX5{a7a|P}75PffP"ao87ffjff 
 j8noaPa7a)|a&n78o5#MoP7ajan|ao87ooP&|8B}Bn|a78:o8
|affBBr-a_
7<V	5qVgM:<gh<	7 o<$gff:jff0no7<hBffho S7o		ffBoVX
ff_sha78{jff_Bpff_X:ff:^ #Pff5<5s:t5o7o7_7:7ff j    :	 
}a|ha7:q5^fiff5<
ffho		ta7	ffBo 5^gB:hoff  <Dg7ffaBff7
	   + 5   Maa0&aPB|	  |awpff ffffM)n|agBff7 rja
P ff&g  ajaBno P5ffn"oa	    a#8o|a&n78a5#M8o7aja&"ao87|P
Bw|ajff8ff"Bff7 jaff  B"|aPaBagaj  Bff7 ffo 5  B}ff|a7Bff7 rja
 ff
 :apa ogBo$"BB  B7aX|a$oaPa7a|aBo8ffBjff8  o  !s	  
|aw5a7a|#
 "7ffP $ -|aoa  )qaff-|P  BoPPa 	|h75< "B|$P78B 
% o:a7  P % 57 ja    ajagwaBSDff8|a % oaB p8aff-ff

&('*)+,.-(/103240657

 d|aBa"ffPB98|a7&) P8jgB:ff "|5aBBog|8: nB':a57a7a7
"B|D)#7B  |ff_ffh:ff 8:#a  78ja{|aq)|8:{j  ;"  Ma  PB<$ =j5|8  a
7ha7ffh7hg|	?> 7h7_h j$o  7 g7|Pw|a  A@X7aB8Xj75ajaBoa#7  77  7a	
G QJSRNoJ UTVK NRVWX E6GYELE[ZNJ
B  DCFE?GI'H J4KL dKJNMPO F
  :Bo58 a8"ff  ffPaBB7Bow:an7o5#M8aP7ajag|ao87 ff  |a{aj  o8ff
jff80Bw5\57{j{|affBB"jaX|aa87{

]^ JaMJG`_bacX jp M:<gh<	0S	o)Sh0q5|Vffho	 )*d#e fgff hie kj Pl a78mj 
VX|5<B:phoDo_	n	o$ffhffff:p5j|?oahff pc$ho;q0ffsrff{aff<  o{8ff{pcX:5M87ff =j
o:ffg7:hr
taP8aj{|agPaoX5{a7a|P}75PffP"ao87ut wPv  
-a5|aBBo0P7aj  :aP8o) jaB08Ba$|aI)"|P"a7p)|8:  Bw:ff7
0{78|)P7  x nBBga: B7ffp|ff)qBgBff7r jap|a:ffgXr7  
Pff 77ff8BB7&)|a57 jaffBgoPP&poa$j=g  aja70"a78:o8ffn|a
Bff7r ja{  waBSff8$X|P5 {ff8p)a)  a7ffy8 78Bff7r ja$  x 
z e{f &-aoaBXBff7r jawaBS  affo5 78ffy8 78Xff8|affwwa5g  aaBff
"a78dBoaBpoad|aSDBff7r jajD|ad7)P8jgB:ff aPBq|8:|" oj  
|ad57ff o  $   {:a7)P78B  )|8Bw) 78B }%   B7a  Brd|a7ff8
oaB{
 Bff7r jawa5g  aaBdff  waBpffff oa|a %~	 8aff"Pa8ff
= q87Bn|P":a ffP)X)|8:  Bw|a  jg7aBo0&:aBja7ff|Pa|Pg|P5Pa 

5p|aBff7ff a8  ff  o8|Pa  $|aXaPgP7)Bja7ff8Bj  7P7  758"#h" o
ffjPa$ aa|aX:8:  aa|ja  5p|aP8  a 9j  8ff o8P5Pff
oa|):a{ffB"ja0#8o "ao87{v     BXP \5 ff"|a7Bj h h off 785
:8:
 waff{Bff7r )ap58)p a 7a odoa{Bff7r )a }h" "B|o8|Pa  a
Bff7ff o  $  ffjP|a5P$ :aP8  awaff 7ff"{ffPa0g7ffa":P"B8""ff8


fi(S	F4444
4 pkkps:k4k!sk4s\;*p:4pkkps:k4::p|:9::s}p44::
 [9? ?4[pA *6  XF  3:?:: <4<9y4:sp[49s:k! 9ysss[
I3 49k4::[9449494:s49\9#*? [ F9ss(.  
y  [9ps19::(:\[9sss[(:k4<4<9:9(4pk9(<9Ps94k
4p[:s:yss94s:P*p:49;s9:;kp9s*p44::[4*?  V [ 3
499(49mkp k. 93 4k[p(6914(k9<?kpp
[!s[1S?	fi?[* <F9}9s4::!\syk9s49999p
:;9p?kp4\;:;I4w9\s|94:k93s{4949\gs49s[
9s[9}!9k999i `:m4p[:s:;9s:s:k*p:N; s499![p99
p94::;9#:s[;999s\994< \:(49:s4\9s<9kp9Ns
999k999k4:i93 !91(9:.k*k9
4.9s:k;(ks9[k*s:k[;9kp 94  ! 44s:k9?
	ff
fi!#"%$ff&'()+*
U#s94m4pk9<9k9ks:**p:<99:s(?kss:km4k9[9\(ss
[},4[9?4ps:9[p:[*9<9k9k9k9(sfi:\S4.-9<p*0/	
p21903(([ps[pk994(4ps:4:s94pskp65#9798
: 05;2<.7=;w1 >  [ 5?71 A
5fi@7
B s4(s91k(?p?((9<<9 : 05 ; 9<+7 ; DC : E5 ; <+7 ;   94SF(G4fi
?[949:k[9*9 \[9k4ssk(Xs9skH5 ; 9I7 ; 4p[99KJs:ML ?kp4[9
s*944p4\4\9
N OHff)ffQPSR [,99TVUks\99#:!W.X<?.Y iS#Zp[1k?\ (]:9#Z]2]:(^(`_a]:
?[ibY0c*	!?ypdkfepMg?\?ih
:
W.X<?( > kml a4n  k 0: 5 0; 95 ; <+<+77 ;;   A
j jipo q
k4s99U<9#:W%r?.Y4?dk?i?ibY[EcS*((]]:(^h
W%r?<(  *>t su wv u  v u xA
k
R yW.XX9zW%r<:\{ep.e4y1k		030/S?[S9 49i4pkps:4
-ps\ff|C}WF?<DC 9k94:F4~WF?(|w9m99\A4<:4s:p?9
9p?kp#! 99#:spk4::p;4sA?;9*1k03**\N4?[N
<4ssP1.9P*p:*kps*s#;4p9skp#WF? r 6fCWF? r ,WF6!
  9F(G*9p?kp4:s9s|[9#99s\9N.9s*9#:s:s
49::!s*44w?;9 64pk9  p:kps#s#m
	ff
	9"ff&9ffz#"TT#
 b r> 2 49 +^(fepk9ss94ss:*p: r <!%<%?  .Npk 94k4p\
HW.X9W%rAWF6  6(<  |*9:[ ~ WF?  r,> 2?|4S?[N:[ ~   r,> 214P#!
9\4s:pD}(p94\:pm#4p99*\:<9m4.-9![ss
4pkp94:9#:W}9;!W X k=W r kp[9*I|#8


fiDTT,#I###)#(T
 Hff)fft%y+D`2%20z`fM%(`%M~0a`T02(0.2.!b. 26(M!D(y[
(#  (I( (9(2{T=}ffE , 0 
=        MM   H  (#   ,      +   
       , # ,
     ! 
 	# .   ,    fi ff    (    M  M , ,  `   , `    
 % ff%=  , 2  ~( (MyM%(`%M! (0 "y( (#    {(0+. $`%
 
 % ,  )& +TT`%( (I(  (# 2y+((02(  (y` fi '
 )D)  2, + .
 -6H[ $
( *
 =@?8A? B + {T=}H0T M# ! . 2 0I0bM. +0( 0(
 0
942 :3 6655 387 >
/ 1
0
;<243 : 387
C4D          ff#  ff  D     F EG#H!E  	JI  
 K  K  K ML N
O  ,
 ,` #        	   D   J #  ,` T  Q P `#,`  ffS R    DT U R   6,   D
T  T    x   V ,   , E ,W=   #  6       % XY ~. 2    #  (D  y,    
    , # R DD(9 (#   [Z \  6        H   D       T  ] T  T 	
       = ^+  {, ff  @0M(``%(2 

_U`ba&cffff dce=!
 dT
 gf h))ffg
 c\ ikj6! T
 i% dmlnocf  h
p   `      ,U R 6       
 ff =M6 ,   ff M ff   #  x         .(!
+(m(+.(f @02 D      	 EoG#H!E  	    #  ` T ff   q ^r ? 	!s ?  u twv
 |o||  |o|| F
yz r
 

z
z

    ,U R  #  ` T

^
r
}
o
|

|
|

o
|

|

|

z


z

 r o r ~


 
ff   q
z



 
z
 
 x zzz  ||| s^~<   |o|| 
k
Q  
  ##  ( T


 
z

 s^~< o s 



ff   q



o
|

|
|

|
o
|
|

]
s

{





[}

|o||

||o| 

t

!M  rV?H @s ? ff   q(M  ,M ff  	 r]? ff  ` #M     ,UR y {  s ? ff  ` #
(M   # #B       ,M `#    
    H 9 	  9 	  t    ff VM ff  	
ff!=    fi
 !         .T ( &r   sff   q 	#Q  	 M  M,   0M(#
 (    w          =       ,`     ,   } O  fi ME @ y   `   
 #M   , vB   fi *   ,          r ? 	t  s ? ]Z[y F 6 , 	   =   (    J
  `     (    J * (, R  ` UI       =          x f   	ff     
  E 	    % XY Q  ^M#  \  x Z[ u ` , 	Bff     
Q 
]M
\ B

[

 ]
  r ? M     ,UR  	 H     !  =  E ,I , !ff       	    ( ,    
      9 	 T    , ,`  ff#J   y, T    ~  [   ,   T y  

_j6!
 T
 i% dmlnocf  h
p   M #8 I
    ] I D, 9
   6  ,    TJ         y  , 	#Q   	# # D     fi D  




y




9




T





B











,   =, 	   ,    9 `  ,  `#w ff    [I    ? ff   


fi,#%
!!!!







Primitive sets

u

&B#!&o

Periodic sets
&J&

Transient set
&

,UJ!F#*#fiUUJgfi#!J6!Jfi!fi6fiuqfiJfiU^!fi!#fi
#F#u!q
FUF!Fqu6!fiJFfi!fiw&6ufik!!6FUVB]F]#fik
@!UJQ
 &]fi#Fq qF
 ]*J@6fi!!!FUJq&#FqfiFUU F!Fq
6#fififiFfi J]6, J 6,]
#fiV<,qfiUUJfi]#Jfi
6  !fiJFJ!u#!fi fiU!6fi#Fq&FU

2

2
0

1

0

3

1
3

5



4


	

,fi!Ffiff!
FUfi!U  J!FU#nQFUJ 	 ^fiu!!fiUJJ]U6u64


fi "!$#%#%&('$)$*!,+$-'$#.&/!1023

4$56879,:%9;
<>=,5?7@"A4156B79C:$7D(E,F9,DHGIJLKMN<ODPQRE15?S>=,9TUQWVYX,5Q,5Z<>P$[]\^_$\`<>=$Pa<<>PGb7Q,c(de4$6B9,:f,DZ< S
9;4$56B79,:$7DRghP<>6B7DZ5ZSiT7<>=%<j=,5kS>Pgl5l7Q,DZ7:$5Q,DZ5RgkP<>687m%PQ,:n4156B79C:.deo,75ZF:$SkPeE,F9,DHGapL:$7Pc9CQ$PF
gkP<j6B7m(TU=,9bS 5fidqE,F9CDZGS?Pb6B5?4$6B7gl7< 7r5KUst=Cf,SiPk4$6B9,:f,DZ<UIhuv-w>x v"yz6B5Z<>Pa7{Q,St7{Qb;N9C6 ghP< 79CQRPbE$9Cf,<O<>=,5
7Q,7< 7PF|E,F9,DHG7{Q}TU=,7DZ=W~ v-w TPaSKUt9T|5Zrb56Z[$;N9C6t5Zr56BoeS>f,DZ=%E,F9CDZG}9;zS 7Z5e\[7Qb;9C6gkP< 79CQT7FF
E$5cC6 P:f$PaFFoeF9S <OPE$9Cf,<t<>=,5i5ZmPaDZ<7:$5Q< 7<Boe9;
<>=,5iS <>Pa< 5fi
""C"q<>=$P<E,F9CDZGK
st=,7Sl7S(E15ZS <k:$5gl9CQ,S<>6 P< 5Z:<>=$6B9Cf,cC=P}S 7gk4,F5R5Zm$Pgk4,F5KW9CQ,S7:$56<>=,5(7Q,DZ7:$5Q,DZ5WgkP<>687m
6B54$6B5ZS5Q< 5Z:.Ebo<j=,5kcC6 P4$=9;
7c,f$6B5/1K/MY<fi=$PSl4$56B79,:RPQ,:<>=,5(9,Q,FoQ,9CQbpL:$5Z< 56 gl7Q,7S < 7D
<>6 PbQ,S 7< 79CQi7S;L6B9CgS<>P< 5\[TU=,7DH=DPQOoC75ZF:l7Q< 9O5Z7<>=,569CQ,5
9;$<T9F9C9,4,SK]=,5QighPQoOS < 9,DZ=$PS < 7D
gkP<j6B7DZ5ZSiT7<>=}<>=,7SicC6 P4$=%P6B5lglf,F<7{4,F75Z:< 9c5Z<>=,56[7Qb;N9C6 gkP<79,QPbE$9Cf,<k"CqjR7Q}TU=,7DZ=%<>=,5
7Q,7< 7PFUS<>P< 5kTPSl7SkcC6P:f$PFFonF9S <RV"7NK5K-[
7;<>=,5q7Q,7< 7PFUS<>P< 5qTPSfi}9C6l$[
<j=,7Sh7{Qb;N9C6 ghP< 79CQ7S
cC6 Pa:f$PFFo%F9S <j`ZK?=$P<i7Sl6B5Z<>P7Q,5Z:W7S?<j=,5?CZ?7Qb;N9C6 gkP<79,Q[7NK5K[7Q%TU=,7DZ=nE,F9,DHG.V,C[C\C[
9C6$[`9;|PkDZo,DZF7DkDH=$Pa7{Q}TPS<j=,5?7Q,7< 7PFzS <jP< 5Ks=,7SiS>f,ccb5ZS < S<>=$Pa<U7<OT7FFzE15l5PS oe< 9kF5P6Q
PE19Cf,<l<>=,5k<Bo415(9a;U9Cf,<>4$f,< SlPS S 9,DZ7P< 5Z:< 9R5PaDH=E,F9CDZGW9a;tP}DZo,DZF7DeDZ=$P7Q[zE$f,<l7<lT7FFOE$5e=$P6B:
< 9}F5Pb6 QnPbQo,<>=,7Q,cR5ZFS 5KRX$f$4$419S 5eQ,9T<>=$P<<>=,5kS 5Z$f,5Q,DZ5ZSk< 9RE15/g9C:$5ZF5Z:P6B5hS F7c,=< Fong9C6B5
DZ9Cgk4,F7DP< 5Z:[6B5Zf,76B7Q,cePQh5ZmC<>6PiF9C9,4%Bz$j?l7Q,S < 5Pa:(9;
$[$PSz7Qq
7cCf$6B5?$K
MQk<>=$Pa<|DPaS 5
7S4$687{g7<7rb5|PFF7Qb;N9C6 gkPa< 79CQePE$9,f,<<>=,5?7Q,7< 7PF]S <>P<5UT7FF
E$5?c,6 P:f$PFFoeF9S <K

k%LzOz
UZU

(a1


 P S 5Z:q9CQh<>=,5PbQ$PFoCS 7S9a;<>=,5i4$6B5ZrC79Cf,SS 5ZDZ<79,Q[$TU=,7DH=ePb4$4,FoqE$9<>=q<j=,5=,9Cg9c5Q,5Z9Cf,SPQ,:RQ,9CQbp
=,9Cgl9bc5Q,5Z9Cf,SlDPS 5ZS[]T5lQ,:.7Q<>=,7SfiS5ZDZ< 79CQ<>=$P<7{Q9C6B:$56< 9%PE,S 9Ff,< 5ZFo.Pr97:nPFF:$7f,S79,Q
9;
DZ9CQb< 5ZmC<OPQ,:eD6B5Z:$7<U7Qb;N9C6 gkPa< 79CQWVNE$9b<>=Wj>bPbQ,:nBBBHN"ODZ9CQ<5ZmC<>`[C<j=,5?<>6 PbQ,S 7< 79CQ,S
S>=,9Cf,F:%E$5i:$5Z< 56 g7{Q,7S <7DRVN9C6O\?4$689CE$PE,7F7<Bo$`ZK
9C6%%S[,<>=,7SUf$Qb;N9C6B<>f$Q$P<5ZFoeDZ9,6 6B5ZS>419CQ,:$S< 9
POS o,S < 5g<>=$P<]DPQl9CQ,Fokgl9,:$5ZFDZo,DZF5ZS?VNPbQ,:fi7S<>=,56B5j;N9C6B5UQ,9<
rb56Bofif,S 5j;Lf,F$;N9C6
gl9bS <zP4$4,F7DP< 79CQ,S>`ZK
 9<>=OF5P6 Q,7Q,c?PQ,:l6B54$6B5ZS 5Qb< 7Q,cODZ9CQ<5ZmC<
P6B5z=,f$6B<Eboi<>=,5zSjPgl5
56Bc9,:$7DZ7<o?4$=,5Q,9,gl5Q,9CQlE$5ZDPf,S5
<>=,5OS <>P<5U< 9lQ,5ZmC<tS <>P< 5t<>6 PQ,SB;N9C6 ghP< 79CQl7SF7Q,5P6Z[$7NK5K-[,;9,6BTP6B:(PQ,:eE$PDZGTP68:/4$6B9,4$PcCP< 79CQhP6B5
S o$gkgl5Z<>687DPaFK
5:$7S Df,SSR<>=,5n4$6PDZ< 7DPFi7gk4$PDZ<}9;fi<>=,7Se56Bcb9C:$7DZ7<Bo4$6B9CE,F5g;N9C6h7Q,D6B5gl5Qb<>PF?F5P6Q,7{Q,c
PFc9C687<j=$glSfiV"S>f,DZ=ePSUPQ,:ecC6 P:$75Qb<UPS DZ5Qb<7QeF7G5ZF7=,9C9,:`ZK
NCbN bq$]$ne",Z-
s9RE$5Z< < 56if$Q,:$568S <>PQ,:%<>=,5(4$689CE,F5ge[7<?7Si7Q< 5685ZS < 7Q,cR< 9qF9C9,GPa<Pe4$P68< 7Df,F{Pb6O7{Q,S<>PQ,DZ5k9;<>=,5

 PFc9,6B7<>=$g;N9C6q%%S[gl9C6B5eSj4$5ZDZ7DPFFoC[?P<qP};9,6 g9a;<>=,5%f$4$:P<56f,F5;9,6k<>6PQ,S 7< 79CQ
4$6B9CE$PbE,7F7< 75ZS[
 J/
 
V `
 JO

  J/
 
TU=,56B57S?<j=,5kF7{Ga5ZF7=,9,9C: 9;|<j=,5k<>6 P7Q,7Q,cRS5Zf,5Q,DZ5ZSKqW5qgl7cC=<T9CQ,:$56O7;8[]S <>Pb6B< 7Q,cq;Y689Cg P
4$9bS 7< 7r5S < 9,DH=$PS< 7D|gkP<j6B7m[<>=,5F5P6 Q,7Q,c?PFc9C6B7<>=$g DZ9Cf,F:lF5P6 Q<>=,5z<9C4$9F9cbo[7K 5K[6854,F{PaDZ5|S9Cgl5
<>6 PbQ,S 7< 79CQl4$6B9CE$PE,7F7< 75ZSEbo?Z5689C5ZSK
X,<>Pb6B< 7Q,cO;Y689Cg J OT5DZ9Cf,F:k9CE,<jP7QfiPiQ,5ZT J t9CQ,Fo
7; Z  $[$7NK5K[$9,QRPlF9,DPF ghPmC7glf$g 9; <j=,5?F7G5ZF7{=,9,9,:tK|s=,f,SO<>=,5?
<j6 P7Q,7Q,c/PFc9,6B7<>=$g
Z 
T7FFzQ,9<?
 	fi ffN 9CE,<>P7QeZ5689/4$6B9,E$PE,7F7< 75ZSKis]6 PQ,S 7< 79CQ%4$6B9,E$PE,7F7< 75ZSlgl7cC=<i=,9T5Zr56ijfi ffZ
$Klf$6B<>=,56 gl9,6B5[9CQ,DZ5( J =$PS<>PG5Q%PeQ,5P6pLZ5689RrPaF{f,5[]7<?Tt7FF< 5Q,:W< 9R6B5ghP7Q%S>gkPFFNK(st=,7S
S>f,ccb5ZS < S?<j=$P<fi4$6B79C6lG,Q,9TF5Z:$c5V9C6i7Q,7< 7PFrPFf,5ZSfi9a;U<>=,5e4$P6 Pgl5Z<56BS>`Z[]6 P<>=,56<>=$PQ%F5P6 Q,7Q,c[


fifi

"!$#%'&$$$

(*),+.-,/1032$45-,(6470"8:9<;>=$+?(@(@9A2,/14fi8B@+C0$47B@4D@EF9HG,4IB*),4J9AEJ=$+,DKB*LfiG?BM47/14EN4GfiB6(O+P;QB*),4JB6+.=$+fi/1+fiR?Sfi8:LfiG,0T;+.D
47(@B*L?2,/91(*),9AG,RCB*),4M/1+.G,RPUVB@4D@EWDK47/ALXB69+,G,(Y2Z47B\[]474G^47/4EF4GfiB@(_+X;:B'),4`+.2,(@4Dbafi470C(@47c$-,4G,d747(e
f B`91(OLP/(6+g9AGfiB64DK47(@B@9AG,RgB6+hLX(*i^9AGj[>),91d7)jd7+,G,0$9B69+,G,(O[]45L?DK4NR.-$LfiD@LfiG?B@47470jB')$LXBMB*),4DK4J[k9/1/]G,+fiB
2$4gL?GfiST0$9<l:-,(@91+.Gnmo+X;>9AG?pq-,4G,d74r9HGsB'),4J;+.DK[tLfiDK0j=$)$LX(64fi8uLfiG,0sdDK470$91BJ9AGsB*),4v2$LPdwi?[tLfiDK0x=$)$LX(645+X;
B*D@LP9HG,9AG,R.y
e f B]DK47cz-,9ADK47(]B*)$LPBuLX/1/+X;"B*),4t4791Rfi4G?a{LP/H-,47(_)$La?4>L`G,+.D6E|B*)$LXBu91(_}fie~t),91(QdL?GJ2$4MLXd7),947a?470
[t91B*)J=Z4DK91+.0$91d`EJLPB*DK91d747(um+X;=Z4DK91+.0C.y
8X[>),91dw))$Lafi4tM4791Rfi4G?a{LX/A-,47(tB*)$LPBuLfiDK4]B*),4>NDK+,+fiB@(+X;u}]+.G
B*),4d7+.EJ=,/147h-$G,91B_d79HDbd7/4fieM~+5Lafi+fi910hL?GfiS/1+fi(@(_+X;u9AG?;+.D6EJLXB@91+.GrLX/1(@+vDb47cz-,9ADK47(_B*)$LXBtN>|2$4NB*),4
910$4GfiB69BKSfi8?(@9AG,d74>LfiG?SM0$9HLPRfi+.G$LX/2,/1+,dwi+X;q



[t91B*)(@9174>EN+,DK4uB*)$L?G5}[t91/1/q2$DK9AG,R`L_/1+fi(@(+X;9HG?;+.D@EILXB@91+.G

m2Z47dLfi-,(@4M+X;"4DKRfi+,0$91d791B\S+X;=$Db9HEF9B69a?4`EJLXB*DK91d747(*y
e~k),9(kdLfiG52Z4>R?4G,4D@LX/1917470gB@+JDK470-,d79A2,/14OEJLPB*DK91d747(
[>),+fi(64MdLfiG,+.G,91dLX/;+.D6E

91(_d7+.EJ=Z+fi(@470C+X;u=$4DK91+,0$91dF2,/1+,dwi?(_t:[k9B')C



ne

~k),4Cd7+.G,0$91B@91+.G[Q4TLfiDK4C0$47(@dDb9H2,9AG,RLXd7B'-$LX/1/Sd7+.D@DK47('=$+.G,0$(JB6+xLxEILXB*DK91s[t91B*)3+.G,/1Sn}fi(5LfiG,0
k
 (e+.DB*),91(>BKS$=$4`+P;EJLPB*DK91q8ZB*),4M9AG,d790$4G,d74CEJLXB'DK91
  +X; t 9(_47c$-$LX/"B@+JB'),4OEJLXB'DK91 k 91B@(@47/<;be


~t),4DK4';+.DK4fi8Z[>),4G
91(_,470"8B*),4jLfiD@iX+XaJd7)$LX9AGC91(`LX/1(@+5),+,EN+fiRfi4G,47+.-,(e f B>Lfi=$=Z4LfiDK(_B*)$LXB>EILfiGfiS
9AGfiB64DK47(@B@9AG,RMd7+.EJ=$-,B'LXB@91+.G,(dLfiG$G,+fiB2$4tLXd7),9147afi470N[t91B*)(*-,d7)`d7+,G,(@B*D@LX9AG?B@(tmo9e4fieA8+.G,/1SFLP//1+X[t9AG,RM+.G,4+.D
EN+.Db4Qd7S,d7/147(u+X;B*),4t(*L?EN4t=$4DK91+,0vLfiG,0JL=$-$DK47/1SO0$47B64D@EN9AG,91(@B@91dYL?G,05),+.EN+fiR?4G,47+.-,(jLfiD@iX+XaMd7)$LX9AG$y7e
q-$DbB*),4D@EN+.Db4fi8fi9<;:B*),4>=$L?D@LfiEN47B@4Db(u+X;"B*),4t(6S.(@B64EL?DK4tB*),4tB'D@LfiG,(@91B@91+.G=$DK+.2$Lfi2,91/191B@9147(>B'),4EN(@47/1afi47(`mLX(
9AGM+.Db0$9HG$L?DKS`tTT(*y78
(*-,dw)_(6+fi/A-,B@91+.G,(:d7+,D@DK47(*=Z+.G,0`B@+_L](*-$2,(@47B"+P;zB*),4d7+.D@G,4DK(+X;zB'),4

 U*}u)?Sz=Z4DKd-$2$4

9AG5=$L?D@LfiEN47B@4D(*=$LXd74fie_[tLS`;VDK+.EB*),+fi(@4k(@+fi/A-,B@91+.G,(8,/14LfiD@G,9AG,RJ91(tEN+fi(@B6/SI9HG?pq-,4G,d7470T2fiS'.fi'u*
0$4=$4G,0$4G,d79147(8$2Z47dLfi-,(@4u+P;z0$9<l:-,(@91+.GM+P;zdDK470$91Be-$DKB*),4D@EN+,DK4fi8LX("(6474GM9AGM47c$-$LXB@91+.GJm?y78LX/1Rfi+.Db9B')$EN(
/19AiX45[t91//B64G,0hB6+5(@B'LSgG,4L?D>Ld7+.D6G,4Dt+.G,d74N91BM91(`Lfi=$=$DK+.LPdw),470"eM~t),91(`('-,RfiRfi47(@B@(_B*)$LXBNfiwK
'qoNA7fiofiTLX/1Rfi+.DK91B*)$EN(8D6LXB*),4DMd7+,GfiB@9AG,-,+.-,(`/1+,dLX/tLX/1Rfi+.DK91B*)$EF(8:EJLSj2$4CEN+.Db4FL?=$=$DK+.=$DK9ALXB@4B@+
47$=,/+,DK4`B*),45m/47R,LX/Aytd7+.D6G,4DK(t+X;:B'),9(M)?Sz=Z4DKd-$2$4fie
zL?EJ=,/147(O+P;B6+gB*),91(JLfi=$=$DK+.LPdw)jL?DK4N;+.-$G,0x9AGB*),4CLfiDK4LC+P;R,D@LfiEJEJL?D_9HG?;4DK4G,d74;+.D`G$LPB*-$D@LX/
/ALfiG,R.-$LXR?4OEN+,0$47/19HG,Rjmo4fieReA8,a{LfiDK9ALfi2,/14JEN4EN+,DKS5/14G,RfiB')CjLfiD@iX+XaJEN+,0$47/1(8q+.GC47B>LP/e8"}fiX8.+,D]d7+.G?U
(@B*D6-,d7B@91afi4gLX/1Rfi+.DK91B*)$EF(N;+,DN/14LfiD@G,9AG,Rxd7+,GfiB@47,BKU;VDK474CR.D@L?EJEJLfiDK(8u"LfiDK9_+.-$G,R8_}fi  8,B@+fi/1dwiP4h
 EN+.),-$G,0DK+8}fi?fiy7et~t),4J=$DK+.2,/14E+X;0$9<l:-,(@91+.GT(@B*-,0$91470j),4DK4JLfi=$=,/19147(`+.G,/1SrB@+5LX/1Rfi+.Db9B')$EN(MB*)$LXB
-,(@4JR,D@LX0$914GfiB9AG?;+.D6EJLXB@91+.Gm(*-,dw)TLX(MB'),45]Lfi-$EUVj47/1d7)xLfiG,0R.D@LX0$914G?BKU\2$LX(@470xLP/R?+.DK91B*)$EN(*yML?G,0xL
R.D@LP0-$LX/tEN+.0$9<dLXB69+,Gs+X;>B*D@L?G,(@91B@91+.G3=$DK+.2$Lfi2,91/191B@9147(e

f BJ[]+.-,/102$4C9AGfiB64DK47(@B@9AG,RjB@+T47aXLX/A-$LXB@4g),+X[

(*-,d7)jd7+,G,(@B*D@-,d7B@91afi4CLfiG,0j0$91(@dDb47B@4C(@4LfiDKd7)jLX/1Rfi+,DK91B*)$EN(J=$4D;+.D@E[>),4Gs=$DK+.=$4Db/Ss(@+fi/1a,9AG,RhB'),45B*LP(*i
DK47c$-,9ADK47(MB@+J/14LfiD@GrB@+CDK4=$DK47(64GfiB_/1+.G,RXUVB@4D@Ed7+.G?B@47,Be  GrB*),4O2$LX(69(_+X;uB*),4NDK47(*-,/1B@(M+X;B*),91(`=$Lfi=Z4D78
),+X[]47afi4D78[Q4J2Z47/9147afi4IB*)$LXBM9AGr+.DK0$4D_B@+(*-,d7d747(@(K;V-,/1/1Sj/14LfiD@Gr/1+.G,RXUVB@4D@E0$4=Z4G,0$4G,d79147(8(*-,d7)TLX/1Rfi+XU
DK91B*)$EN((*),+.-,/10J/1+,+.ik;+.D"a?4DKSM(*=$LfiDK(64uB@+.=Z+fi/1+fiRfi9147(tmo+.Dafi4DKSM0$47B@4D@EF9HG,91(@B69d_EN+.0$47/1(*y
ek+fiB@4B*)$LXB"(6+.EN4
+X;tB*),4CLX/ADK4LX0$Sx=$Db+.=$+fi(6470Lfi=$=$Db+.LXd7),47(^m]+.GT47BJLX/eA8]}fiP.yMLfiDK4/19HEF9B647039HGsB'),4JBKSz=Z45+X;_d7+.GfiB647.B
B*)$LXBdL?G52Z4>DK4=$DK47(@4G?B@470jmo4fieRe8fiG,+`/1+.+,=,(Q9AGIB*),4tR.D6Lfi=$)JLfiG,0JB'),4td7+.G,(@B*D6LX9AGfiBB*)$LXBuLX/1/9AG?B@4D@EN470$9ALXB64
+.2,(@4Dba{LXB@91+.G,(`2Z47BK[Q474GrB@9AEN47(MOLfiG,0T>EN-,(@BM2$4JDK4=$Db47(@4GfiB64702?SgB'),4`(@B*LPB@4`aXLfiDK9ALfi2,/14J9AGr+.DK0$4D_B@+
EN+,0$47/"B*),4M9AG?pq-,4G,d74J+X; A

:o

+.G  y7e

CHtAOzA

j4])$La?4QLX/ADK4LX0$S_;+.-$G,0JLfi2Z+{a?4uB*)$LXB47.d74=,B9AGMB'),4Q('=$47d79ALX/$dLX(@4+X; 

+.D}B*D@LfiG,(@91B@91+.G=$DK+.2$Lfi2,91/191B@9147(8

B*),4t(6B*LXB@4]a{L?DK9ALfi2,/14`2$47d7+.EN47(]EN+.DK4_LfiG,05EN+.Db4t9AG,0$4=$4G,0$4G?Bt+X;:DK4EN+fiB64>=$LX(@B(@B'LXB@47(>mLfiG,0IB*),4DK4';+.DK4
+X;YDb4EN+fiB@45=$LP(@BN9HG$=$-,B6(5LfiG,03+.-,B*=$-,B6(*y7eT,9HG,d74rB*),91(5=$DK47afi4G?B@(JDK+.2$-,(6B@/1SxDK4=$DK47(@4G?B@9AG,Rj/1+.G,RXUVB64D@E
d7+.G?B@47,B8$/14LfiD@G,9AG,RC(*-,d7)TLN/1+.G,RXUVB@4D@Ed7+.G?B@47,Bt91(`LX/1(@+vEILX0$4OEN+,DK4`LfiG,0hEF+.DK4`0$9<5d-,/1B_;+.Dk/+,G,Rfi4D
B@4D@E0$4=$4G,0$4G,d79147(e



fi]AXX@o$TAT5$$,$$3vZfiX

kX]7fi7"1J1NAfi6K7@@A,j@r7.,@1$J,X*,Kfi,$71st*,5'fi,@1@1.$K	
ff,fi1

X'Kj$AK77fi776`*,*,.KfiK	
X??KT
X6,, ff!"o6#$K$
P$X@J7,fi@,NA%


.  X6,&.b?K'?,(
X)?fiK*!+,@7rHfi1,fi@,A,(fi1fi.K1*  t-,7.X0/213fi,o  	fi1174fi5
!
.P
 $?]$7@7?,687V
fiK6,	fi9fi

fi17,k7.,@1$]*,;:M$$,*,AIKfi,$177.=<7?t*,7@
X'K'$K,*,7@,6?>Hb@
fi170@BADCFEGAIHJA
KLK,KEMNH+M$*,


M
OLP  @*AI!+Q

OLP IEGAI! OLP RH+AS!TK,KLK OLP  EMD! OLP RH+MN!8C

U
V,W

A

OLP RH V ! OLP IE V !

IXff!

Yjg
fi(fiAK,P
3@73*
P

O P  EAI!(Z\[($	fi17@*,r*fi,@1@1.$b$
ff,fi167]fiK(fifi0^..[ff6
L
_]    Zt*
Pt*,`  6@1.]$K$
ff,fi11  X*K.H
1_$`Pfi.
fiI6+ab
	fi5,A,C*,`$=cq,1@1.r
V
o









X

@

1

.



$
d

!
@

e

N

$

9

X

fi

.






fi

X
*

K



.

f
g
,

]

M

$


,


P
H


O P

OLP If!2C

[




g
f
hRi jFk,l

hhNm

f

hnj

loFl

f

hnjJm

f

jpj

[
C

lq






f
hRi jrkl

hh

losl

f

jpj

lq

t1*&t+
uC v	w

xt,K=.Kfi
O P RH A !2C

[





hyi j k-z

R{ A

l}|

A CFt~4 A !

>,17g1_*,fi4XfiMu*,KQfi9fiK?7@> 

R{ A

oz

l|

A Cv	~ A !
q

t1*&t+
uC v	~

6@1.]$K$
ff,fi11@17X_*,1>6 

`@@,N6xk,K=,Kfi

>,*,u*4fi,@1@1.$b$
ff,fi1678?KQ,fififi$^t,2[fiH*,u,P@$$6K7@]$,-
,@ O P RH A !GQF[fi
fi,s*,Kfi,$71 s7.=
 <571?J
 t*,  X*b5 $K.*
 ,7e
 @BAtAs
 

X@1.

IXff!M7,fifibfi7`@^X



m A,K,P@7,68kfi@_*
X]*,1$K,*,7tfi1fi7]*,M.P$?] hRi Mhk'5K7=$77t@e ji AVK 
 J% 
70gfiS6 [,%Xff^
*7,A,@.r70fiI6A[,ff%ff!+X



X@1.[L!fi,g1,@7rAg',/213fi1fi,K1* 
]fifi8X>Ar.P$?
X67gfi?.K1* 
[,ff%

u,?.
 >q4
 X@7.,D
 [,%ffff
!6

>. 

"I]K1
fi1fi8[Lffff^
*,fi1N:M1T.KB>2fi9 e

  A

Lff

Zfi

	fi1fiAC',M,X@M8fi1,fi@,A,K1@K1.(k
 

>,K

9$D_ 

CF@BA

 
  M

w,wLw#
6,A,7@ A 1.,@73@$K$
P$X@K7$1'
P)?fiK$
L	I 
Lff  
 ,_*
Xfi1.,gV@ 
K
 7$1M_.P*
fifi5fi1fi@0X_1>1$K	
X$X@7
 ,fi

1*,Cfi77@.;

1@_7.fi?Kfi,7`6v?
([


Xfifib$,"*,k.X$1?u*,fi1,fi@,A,NK1@K1.It1*JK7=$77u6fifi*,0
P@@-X678$77  7
fi$I6 ffA6  
7.?fiK?7t@e  	1fi @9	fi N9[ff~L[ff~,wLww~,[ 6

L 
x ,C7.fi6H	,
k
 $,
   1@@1.,C,X
 6g1  .KC$n5
< ,	1fi ($
 7,ff,
 6h',g$,@1
R{ A
AeCt4~2AI!
z
l|
,fi
 ]1fi ,,fifiZJ.K,X
 6_*
fi
 T.,ff6t
x ,"ffZ
 Xfi5K7-	1fi ,fi
 T@@fifiJZ
 5	,
 -X
 A,7jn>
 ]JK7@*K17$$
 

*,M- 

X@6fi@1.@M',Q,P@7uAN>,17N*,0$b.*,7*,fi9fiK?7@  1@@1.$K$
ff,fi11@17X,X7N@  
 fiJAe$
 4X
 7@17ff68V7 *,],X6t>,K]11Q,fi2$$$,$7",]
 fi@fi 9X

@@,1JZ$$,$7"fi_,7N1u',
7.LK77-$KM*
Pt*,M- 

YK7=	fi_,fi.Z`$,-PH,7(%7.,@1$KA,C@,fi17g 

6@1.($K	
ff,fi1

X'K177,t1*"@,gfiA,pX7@,8A*
X1[t>,',>  1@6,.$K$
ff,fi111fi17@]*
fi[fi
fi,
A CFt~4 A 
R{ A
! fi*,Kk6ff6'
 7@@A, A CE A  A H A KLK,KE M  M H M 2fi1*,$,,T*,
  h
z
l}|
Xfifi
fi .P
 $?]t1*CK7-Z
 77t@efi"
fi *,
P
 @]@-X
 @7],fi
 I.KX fi&fi`?Kfi"IX+ m A,K,X@7=!7


*
XM1.[,

*,Jfi
^A]@65fifi7.fi?Kfi7_@[fiNfi,j*,Nfi77@,+
fifi9fiKfi,!



	fi1@9	fi1N9[ff~,[%~,www~L[


6
,ff

A C




fi1@C7.?fiKfi7M6'IZfi@@9	fi

fi2ff*ByD
*=




weak
influence
strong
influence
e

remote
past
event

near
past
event



et
current
event
t

time

2$

;	,+
$	,-=	$p,4
,
,	
,	.	
`	-	e$	gp,
,
,	
,%
,

(
g+-9(G]]0-'		,	LG+ff,
-49	`	.G58$$
ff	]$$
,5,g55np$	
945		;$	$	
,b$	ff,G,
,,	
,	,+$	45
,
925	
+%,ff0R$,
49	'G9b%	((R	,
9	'gJ4`.	),%5,
-
ffDff	-

ffe=	,effe9%,9	ff.%,ffb	,
9	`
+,SS}ff9Nff,%
0	-	$	"9%	,	-	04-ff9ff	9029$
,84
4,8	
N$	-
	9
8	29,0	+$	
,+-	bff,g5B9%	,	4-++9+

$=	099	$	
-	$	-
	b9-(,ff	4"-	.
	,	$,		
	$,
,2ff	
,ff	4
$g
5,%+ffb

"4ffff-	,=	09%	,	b9%,ff,	9	ff,%S-
9.-	ffG$,
`	
-	$"
=I$b8G5	)&-		ff,2-
%e-	b9%	,	

-	,$ff$=	5
		5,
ff	
fiff	G+$+.9	,,g,

!!"$#&%('*)+"-,fi#&./0'1%2"4365879#:.<;=#2>?34;-@BA'-5DC$'

+,ff]e
ff4545%"e-ER$4-
		
`	.4"p	ffp$
	4$ff

G
`545%ffG$	;9
$
,G,ff$	p,


,,	
,	,2++$	]`g4'
Lffeeff%GF,ff;9-	ee-HE.

-%	$"
$
ff	545"S0	).*	G-	
$	,
	$N*+$%4(ff&-	0IE

,
9,%e
,9

9KJ	$KL]ff	59	-4

92$
ML%*N;%I$-

45ff?-	eff,	,

-

ffPO=	RQ:ST	UPVIW8ffff

	
9;$
	$$ff
N8"
4,Dff
	,$	
G#],S$IE
ffe	e-)ff$	$INffYXfiZ	[	Z]\1^Gff	9	,XfiZ	[	Z%8	
ff9$	*ff
-4	-
(	
`	s 
ff		``_

+
IaXfiZbZ	Lff8
5	e$	
ff'	,L	5	


-	"	
	$$ff
Ndc0`	4-

949	

925	
BeB9-)ff$	$$4,-	

,ff	9	89	
5	*
+-4b5	=
5$E$L
$2IE$,	b-	9
	$	,e	
,e	
,
4,ff-g$$	p4,$%IE	J=	,=S		%,ff	(%"-	G]ff	4545	.
$%
ff	0
+,$	
	8D-g8,
,%9	
	$	,;9eb,=9$%IE$+ff,ff	

p$

IE	0	;`%
-
]*=
G+ff,B`-
;,&-	]-	
5ff.	
S

	ffbgfV	V	fiSffg.	
I0,ff4	`	$	b9$
%9	.p	,
,	e8%,ff;-	]

hfiifih

fijPkl&l&m$nokp-q?kq?rBs-t-u$pWv-k<s-qKrwp]x$y	z&n

|

fixed a priori
sentences

noun
verb
cat
{

{

words

dog

d

g

o

phonemes

subphonemic
Learning the
meaning of states
only at this level
}~H-$R4fi	oW~WG&fi-Iofib&o~Hd&I	IR~M-III*II	W~Ho~H???~H1W~Hfi&HH
H~~IGoa-0-	--fiW~W~G9-WWfi
	~HWfiHI	I!fi-Ifi	0o~HW	IW&
ofi-~HWW&HI-	bWMW	-	ob
Iofi	9&ofiIoo~&IGo~WH
I&WIfi-ofiWIa&o~Hoo~HIfi

-WWfi
I9W-fiW	
4WIHfi&Hfi	W~W?Bfi-Iofib&o~Hg&PI	IR~HWfi
		~H-IMbGWW	o~W~HRWa-&o~HR&-~HRWW:I-b	


W	Wfi-~IIo~H&IofibWWH]B~fiWaG~WIWo-&oo
B-~WaWW:I-b

~HHfi	oW~W?I-	WHIfi-fi	WH~KG*-&o~WH~$IBWG-WHfi0Hfi	W~W
 WIRHW0ofio-fi]fiW-fiWI~HIfi*-fi-ofibIfi	IM~W~Ha-~IIo~H~H-&oIW
	H


~-fi&Wo~W
WHo~ofi0afi-Iofib&o~H4Wo&o&	~	WHa~HP-II]	oIG~	I	fio&

 -b&oI:	~bWI

 W		oIo~	g-W$WI~9I1-0oW

 WHgo&a:b~	WHfiI!fi&I

]fio&o~WG&2-~!fifi	Ro~

ofi&H	RW-~W~0o-Wo~Hd~H-&RWW&ofioIboI~H
-afi-IofiboIM	

 oH&ao&oR:	~	WHIfi]W~HG
Wo-~WofiW~~bao
W-II~Hoo~
~W&

I	fibofiW~H0H&W--&0o~H9&!I	I

 bWfiI-~	YHfi	oW~W&	fiYHWa$-o0o~HW

-WW?W	W~HWfiIbIo&&	~	WHIfiMDG~-boBWIow
WHo~WHB~Bofi0IWWfi	
~	$WI9IWo&~	oWobWo~Ho~HM-W-	W~H~Ho~HIfi4Wd-&PW
-	0$--	W~HH~
&$I-	W	~Wa&	bo~
Poofi4



 oH:&	~	WHIa&H

WbWfiWW$&-Wo~H9~H-&YW

obWo~Ho~H--bW~H~Ho~HIfi	2-P&IoI~
ofio
06WIW-~Ho~H-&T-bo&--	W~HH~Ho~HI&
fi&IGo~
9ofi&H	T	~H	figWWHo&o	aIII-9-&Wb1]	WIo~Ha-fiW~WW~
WHo~
ofi&HgooWI-G~G	--W-~&o
	

 -&-&H*I1WfiWI?Hfi	oW~W&b

W-	W]fiWoI

fifi

 WIK&W	o

fi	$T!-$---

9T	-D	

      	 ff
fi fffi  	 fi
ff 
   
fi

! "
# %$
& ' 

 ()
 ff +
 *
,   -


&fi  ff
 $ 
! .fi 
ff 
  & 
fi/  
0
 21 	
fifi
ff fifi 3
WH
oIIoHP
-bbM



WI

WI

Ha Y

Ib 

 ]

&W


o

	



]  
fib

	fiWI?

B





H

I

Ho

 o

WIM 
ooW

GRWIGW

YPB

&oH



& HII

	 o

]

?

Wfi8P

- -fi

 H

 -

-fiWIHI9



	

o $I

H

46587:9<;>=?A@B;	CDFEHGI?KJMLN;POQ'RTSA;JU?ARVQW;VC3D3@
  >
 
 ff YX8
  2[Z\ ^]  `_  3 '  
 &
Ba
ff
# %$ 3#  
&fi !
H&
 ff 
b
fi3 c d
 )e 	Hfi	
  


 <fBa ff

#a  $  0g,
_  
 -.
h ^] 
ff

ffi

$
# /jkZl mnfiofi
3p/qZl d
Hfi r sa at $ u  fivX w 	
 )
 ff yxz
  ff 	
 $fi>
#Zp{Z|l fi r %a at $ }. fi ra ff
 ff_ 	 u
# u
fiofi ~X 8 V xz } 
.  	
$ 
fi	ZpeZ zm 
 d  
  <e' fffi
) $ 
  #16
# %$

%    %$  % r 
 }

 
   
$

 fi $*  

  3 
 A   &
 ff 
ff
 
$
# 
B 
Hfi	
%$  %$  % r 
fi 

dfi 
 $ fi $a  ra
 
 - $ ! H   r
fifio
# %$
nfifi
,&
 ff
H b
fi % fio)e 	   
% b
fi/ `
  ff

 .
. fi %
&
 r z X:krqff{eZ 

-i
  Hfi) 
) ^  ff 	. /

WH

 HIIPI
	

-  fi	

	

	



 &oaW

-

W






W -

	fiWI

o



	fiWI9o&

]

H

o

- 

-

	



HH

	



-HH

W



PI

WWP &oP

Iofib

Y

W

&PIW	ofi



&



-H

	

&boWWa



H&P





I

&

0 HII

	WoHoH

& 



-

I

W

-adW

-

	oW&KWP

H

oW

G

H

0


6W

-

0 HII

	WW

-Ho 

o

o

MI-WIIo

-W 



H

b

- ]fi

	fiWI9

&



0ooH

&oHW - 02H

-W9WI -fiWI

M &o9W

?W9I

-

WIo 4ooW

ao

&o9

] H

-Y

	

-

HoW&

-IH MIW BIHfib $RWIGW




	

- ! fibIW

 b

-HI gI BIHfib

]IIo

-P



W o & b W

- H

	

b

&

WRH

ao 
o W Wo&

& HII

- ! fib

H  o

 b

0

-P

WW




&

KH

I  W9

	 

-fiWIHI 

&K

& I

H  

WP WI WoHH

-W

-fi

W

H

	

0  
H

gI-WIIo

	

	

W&

HRo

-   	

I


 0





W H

WoH

465|NBQ';8D3;D6mKLN;JMLD'W@
* 
fi	

 2]
	&
 	 ff
 $'
  r fifi    	
 d t i 3
ff
 $ b
 }$ 
 -.#
 	&fi  $  mgIg
fi $a
  
3   < $ H
* .
 <$ 
 	 fi	
 b
  <  <16 -
 fi	
 H. fi8*

 Hfi $a 

r fifi (.#
fi2 ff

- .
. fi  ff
 }q

 I3&&X8  q' 8/ qZ&X  ' 8/ Z
 ) 
fi2anfi ^  dfi $AXs^Z\fi #$XtZ
8 	\ r
 y^#Z fifi $I $ 
fiwIX%keZ'
 ff
 
 d$ 
  0
  ff
 
fi $a i+  
X	k -k#Z
  
     > B
s $
ff
 /q< 2]  ffi
 $ ff 	
fi  <
  ff
 
fi>  k# r
 $ b *
 $ 
  
&
 H
I&
#a
	H H.    (X8'q# Z 
B  <fi >_ fi  o
	&  o$ ^1 
 fi w 	
X8X%ZK,XkZZ'XtZBkB   X%Z
&fi $
   fi >_ fi   ffi
 $
`
  &r ff 	
fi
   ff sfiX  s$ 'Z
` fi 
mfi 	_ fi -
fi	

 $  .  
 dfi >_ fi   d$ 
 $<gIg
r
 d'
 ff

w mgg fifi
X     &1 
fi|
. i. $ ff
 Zd
  
 ms
$ H.   ff
  ff 	
fi M#oHH'Bff#'B U
,$  fi	 X%k :k 'Z
 
 $ H.   ff
 |
 i
  Hfi $) 
r

&
 ff o fi>
# %$ *
#16 &
r /}
fi $  $U  r


 ff fi
ff
#%&
BI fffi
 $ 
fi 


&fi H
 3
&
B . 


 ff
.fi fi	
#fio  s$  .fi  B&* &
  Hfi
gIg 
,{
B'/^n^ff^r^8|i'nid8tr%%i%n%"^n%`\[&'ffn8
ffKd^0ff^Tnr`\8d/nni%diuB^Bff8B
D

&oW&

WHdW
o

- -fi

-

WoH

0W



-fiWIHI



	WIWI

 Wofi	R W

o

&W

WfiWIIRP 

- WHIo g

	fiW

H

& 

bfiW



&o?o 


9oI YI	 

&o


o

H
W

]  
fi	P 




do

WfiWIIG WI W

Ibo WHI	 

H



GW9o

2W



o



H

	

o

T

- 



T

- -fi

R &I HI
	

W



H:

-fiWIHI


H

fi

	H*I

-  fi	o

M

&W

]   -WIW

&

&Y -WI

b

G

` 

]Ioo P

 W

 bM



o

Y

	

W

	fiWI

WIIIo
WH

bW

] 

WHGI
I

HB

?W

WfiWII

H 0 WW

H

H

	fiW

]

W

W HoHI9 o &I

H  GH

0o

H
W

H

IW

 W
 I W

H





H

	fiW





 W

	

W


	g

	

	

-

Wo

Wo


WK	 WW

	fiW

	



	fiWI



H 




W

-

	W

W



Wo



W

HI

	

-

HaP 




	

` W




-

o





&W

 & HWW

go &


bo

&

	

WoH

I IP2W

HW

&WoMWI

W

Wa

=

 

 	G WW $ b	

H 0 WW

YWfi9H

&o

RW

&W

- KIHW

- W	




	

&
o

o

I

bIW

- ! fiba



H 0 WW

H

6 &oI

Ha?P

WoH?oGo




&G





&

	

0W

o 9W

IW-WIIo

]  ao &I P 

W  &o

I

& 

-HIWo -   WIIIo

H



Ro

RW 

oI

W

	

bWoHoH

	g



	fiWI

-



	



H G



&

	WoHoH

WfiWI

-

&  W

Go &oI

?I-WIIo

Wo9W

g

W

bo HI

MW	

?a

]

	

fi|	'I	I'VU'
#fiff	ff$

5
0
-5

 fi!"

-10
-15



(

%'&

-20

	ff

fifi

-25

fiffff	
 
	

-30
-35
-40

0

5

10

15
T

20

25

30

)+*-,".$/021$35476"89:0;/,:0;8<=0>6? @A6"B$/C.DE*F8GHDI<=6"0JK<=*L0;8MONPDQ0=0R@A0S8*-MC*-6"8UT:VW*X8UY$/6Z.<=M76? DCMC6<=E$[DCMC*-<
\ [M]/^*L<=0=D5[DCDC6<=*X[MC0=Z_MQ6`86"8a!E6 \ 6,:0;80=6".D5bc[/Cd69e<=E$[*X8DA<=6"8DCM]/C[f*F80=ZgB:hUZ$*jik0;/0;8M
M]/C[:8DQ*LMQ*L68l,"/C[Y$ED;monIE0Up[fMCMC0;8*X8,l6?AM]E0qB$6:MCMQ6 \ <;.$/9:0r*-DUZ.0_MC6sM]E0_t-* \ *-MCDU6?
8". \ 0;/*-<;[t Y$/0=<=*-DC*-6"8r*X8UM]E0A<=6 \ Y$.MC0;/70=u$Y$0;/* \ 0;8MCD;m

t=1

t=2

t=3

t=4

)+*-,".$/02T$35v+9:6tF.MQ*L68A6? \ [M]/^*Lu2Y$/^6"Z.<=MCD+wyxzC{ |P}?~6"/k[ \ 6"Z$0=t$E$[9"*X8,A[7?.t-t-h2<=6"8$80=<=MC0=ZM]/Q[:8DC*-MC*-6"8
,"/C[:Y$Embq[M]/^*LuK0=t-0 \ 0;8MCDNM]/C[:8DC*-MC*-6"8UY$/^6"B$[:B*-t-*LMQ*L0=DVA[:/0A9*LD.$[t-*L=0=ZgI*-M]E,"/C[het-0=9:0=t-D;m
;;

fi+:k$$$$

0

1

2

3

1

2

3

0

0
4

2

5

4

5

6

7

+-"$2$5A;;CfCX`Igl$R$;RXrC]fCR=X=-=A$;:Q2C]fCRX$-==;"$$;^I"I
C]Q=X=-=$;:Q]$+C$O:-;+I->gIC=C5:;;CC7]7]QXF
]y~"7]A=$$;X;C5]$ee-==_XU+-"$2
100

Correct topology given
Randomly connected,
24 states

% Converged

80
Fully connected,
40 states

60

40

Fully connected,
16 states

20

0
0.1

1

Fully connected,
24 states

10
Span

100

1000

+-"$2$5;=;]:efR=":;:;=KCr:CFQLoP:;2:]X-]y~"ff:-"C;^L=
=$$;^F;:CeAe]$:gf$;$;$;=-e-OX;;fC=:rF;^;CXg]eC=jP
]C:QLQL2$"$L--C-=fCC= 5:O= IW]]>="C-CQ+F$=LXC=;==
:;;CQ=_:K]2>$;-=C=gXU+-"$2
;;

fi7XCP$gXg`$$$$l:

CX=e"^eC]Q=2]$:]K:;;CQFq>g:Q:$"=$==C-"e7;K;;C=sI-]
:: "$:---!:>+-"$O]II] ;C:A"$;I +=" :;:=r]X-
 " ]=C2$ k;;
  =I  >gC  L:-=;	
 UfL ;C=I]2$$;7
  ]===Cff ]X-RC  -$-U  >C
 fi=;
$="eC"
 XI   :  
 -L=K]X-  $ ff-;7 CC=
 A:
 5 ]7:;;QCX
> :K]$--=" C= ~':  ]=CeCFQLO$ 'A]  ]`$=F$Xq
  eC $;=
X:Q==$:eQ  =C;K]_$-C]^F$QL 
  ]rC$O:-;e]r;  A]rC ;=:
~U-UXC;=CCX CC_]$f_Igg`>L  :: "gC]Q=`]$ ===C] $U  C
="$==C -!  ; ~"C= =l$=CC; !
 W  -;-- Rr  :^C;"C  s="C=  "$eCqgX:^:;
=# "`=-;: ;:$-=L%
 $~5 &";  - '=sFc+L$
 ()R-	g--IA-" *C;C $  ;$;=-=
C`2  =C;C=g:U-;:C=q"5;C--:
+ :];>X:Q;=CCXK"C;ff C-"r-R]$fIX ::U;fC= ]2]QXFK;$ff ::=R]$^""
"A"7,
 :;
 -f  XQ;:;	
 .$=  XC;I="-A &  XX=q:K]A$  C-"  ^"-;0
 /
]_=XQ1 _"Cf$L;eI-] =  ==`CgC  :C:=C;-2
 :;^] -3
 $P]e]_-:-]$
 $ ;:5C`$C4 5)=6
 I=C  XQ;:A;:_=="
 :;^`C;^L  "-;5;_=F>C-  
 $ 	=5"$;^L;f  ==-C-"r"7]=X>-;:]U$=== $$f==  ]:-:
78:92;<>=@?BA>CED;<GF<	HGIAKJ
A>LMONP;LQ

  -"A7"R  "_^=;$C; =7"R 3
 $TSW;:-U=A~ UVV
WX)77O$Y
 $$ 5~">]=C
 "-X;:5$$$:-;  :Q:=C;fi==rC"QC;    f"CXe;=$-5:;W]A-"`Q;C I7X*
="  fCXL5I-] CC"^FAX~"CefC-"~"]>LOQ;CUS7QL;fL-$>Le;"e"*-X;:-
$PX:;e7=-"CZ )CCC"r-"
 *C;C ="C &`"$CQL >"C$-;CU$4 
*    f"C=l]$^""


CX
 ff:-]rC -$-:[
 UL   ; $72$\ :2-C]
 "$_=C :O^=]-CR==;CX`]  *
=C;]QL`: L;CXe  -" *Q;C ="C & $$]=K $ -`Cq:R  Fe$=L7]=K
>g ^AIggA",
 _>^5a
 `b_ ;">]=C`"$=-  
 ~"$q]$f :]g]e  ^=C;:C-"
:qe-;:CXg -" *Q;C="C &AX "QeC-"qC-=C:=];=
  g:;;CT  ]=q:
$]e$:]7;:$-=Le
 ]I]CC-C-"  "$L--ee] &0$P"]$$ef]-== -Z )= >6
 *
:; 5;2C:C-C-"  ^"$:--LQL=I:^=-:CIQ3
 UI:  X ~"CefC-";:$7CC"^=b
 ~" ]
-"eC;QP
 cdfeA;=$-I;U$  ^  "Q=` :;W]A-"eQ;CU
 g A"$>
 '$X:
 ~"7=;$C;
=7"R  ]-]:=CC+]$f+]  "-;  -;:CX-" *C;Q $  ;$;=-=5-"X "7-h f
 eikj4l\mon\pTn2  CX fi;C-"  "-;Uq
O
 ~e $ ;:$ "`;-
 ~"A-;7L;CX-:"-]$ ]=s
r  ">"C$-;A$=C=;2CK-;:Cg  QF  C:C-C-"  "$L--C-=`;:
 U"  ~':k C`-;:Q
]UC  :-:: 75L-r] FsX:Q=="$e-" *C;Q$  ;$;=-;s
 I-K]"- ;="$Q:
=C;	"U-C;C$fC :t$P$-C;^=C )RfL"-]$
 7$-C= :;X_IgC  :-::u
 $P=  ==FfL-q
 ~"
  =Q;:CXrL *C;C =":Q &"Z )v]	A]:C    :C=s:q
 .C:-4 f2
 wx^A"$%
 $ffUVV)
:u
 y7"r=2~2
 $zUVVWX) {
 ^A$A=]-CA::=C5]$5]=fL"-]$A]LC] :CK$-C= :;
  :^CIC  :-::-= "WX:CW$=C;CX-CC-2$=-;	
 IR"$;C  ^=C;:Q=;5:I=CQ; *
CX--:r $ --;C-"r =C]L-]=qef];eC-;=]-CA"Kq:R fE K=$XRC ]  L;
7L;CX_-"UC;C $  ;$;=-=eXq":;=" " *!":;="2>g;
 I=C
:$;:Q+;^R-C2 $ "Q= &  ;X;:QI"e:^C '=Ff]| :C]$X]  ;";
+$  QLU +;=$- U]A="C=  "$XK$} "K;LrF]CXX`>g>CL;CU-" *Q;C
$  ;$;=-=;
^AIgg
 $TSW;:-~
 w QC="T UVVWfUVV(:@ )>:u
 _	^5q
 `b_+3
 $."$ fUV@UV:@ fI$^Lz *
e: 6UVV  )A:K" *!":;="
 ff:X:C R>g ~': ] ]C:C-C-"  "$L--C-=U:
$=C-"r
 ]X  t
 $k~"[
 ^AIggZ )5">]Of=C-"%
 $k{
 _>^5a
 `b_ Z )I5;=rCX
 ],
 Ie# *




fif@f#@@@

Z5R[
Z5{@@Z5R#@[Z505Xo5X5X5[uR
E5:X@X5a|3@R
Z5R!0X,RZ@155Z[2ff@ffRR5~51R550155BRR5R5>fXZ5~55
|Xzh5:ZR5RRXq@ffX@5RbR2|35]X,RR[Z5bR5RRRtX@X
ffRR5~@55X@05a@oXP5R#B5X@5#RX5Bq@RRZ5b5Z@Z5
@5@ff5h5@@@RZR5RXR,@[5@2Z5551|[@RR5RRX]51
0@R5RR~B,qzX52Zff5,BffX0@R51R0XR]Z5~R
X@X5BRR@@5@5T>5RX5T[@qX[qqbZ555RZR
ffR#ffR[Z5b5@R]R535{54ff,5[ff:ffRR5f
,@@5R535@ff5|ffRZ5RZ@5R5,Xff	5@@ZR
Z@5RZzf[ZZ@zRbX@515fffff5,#5b@5RXq@ffX506,5Z5ffR
RZ5:R~@:RRXR1{5#5,R@55[R@@t@z15b55E1@2\ffRXR
fkRX5T@,Xz5uh5Tf55f@:@,@5R~b5X,X5:RX@15X
X55bZ|fEo@ffXff55[@b[Z:5h5:BffX@5
635b#5@XR6@ffRR5ff6ff[R,5Xzh55Z5@@ffX,R,5h5
X5BRx@|5@51Z@[55RRRhq@5@5a5Rz45	ff@zR#RX:Z5bZR
 5u|~45%aZff5f55uRR
Z@BRZ2ff5{5R2ffRRO@ff5f5
XRzZ@X5@>R,Z5[XRGRZ
R>4Z5[Z@BRZ
R6Eff5[b5@R{	,@ff
R:RZ553
1[5uff@a@ffX@
XR,X5qR]@XR
	Z@5TX,XZ5R
Eff5
q	|~B
 
 Z5@b>5[Rb[Z]Z5{
f5RRXR	@@ffZ    >5,T[h@
 u551qR:Z@@0 X	RRXtX53~5R#B5XR,5Z5R155
50Z5  @>6,@65aBB~@@:B@5@5t
5[TX6Z@@|Xff
q5Zff	@
65T@25X5@qff,	zv{u5@a5Z@
TXR15~R
RXY5@@quR0X@@RRff56Z@uffX5RX,B
 @@
 	 ff 
  5
6@05[#ff  5551\Xk uR  fff\fffi5h1
RXR@55X@zh5
h Z50RZ
RRR{@
15R:@zX@51R~B@5RX5:, Ru@5ff
ff k" !#\  $ fi|
>h]f,fffffi



%&('$)ff*+$'ff,'-.&/01ff243	4ff
52@RXaR\6o1T  

 Xf

>5f  	5,6a5ffTE,kh@k765@,fzff
v,X@5X5RRX{5@
5o6Xo5@@] R35@@@ff98:8;8=<>&?."@ff$'ff"A'ff,BC$*+&(ff2B1C	DE'ff&GF$H


ff 
	
@
>5f   R
RX5Tff\  6[ff@bRX@{Z@zX5X0hffI6{RR@
R,R0@4

@ffX@X
RXX6[  J5  Z@zfE] {Z|RX  fT@ K)ffL	ff%C@MNOB1C	*P&(ff2
/ @Q@'ff&%RS.N'ffTM&?'	/CU/@ffVW3	C	RWSX@qXffXY6,B@

Z$[$\

fi]M^`_J_Jab9^c"de^`defYg"h"ic+j"^g"dkflcnmoffpJb

q:r$s+tffu4vwffx-y`wffz|{}G~J9@vPs+uwPy(	ffffff~ffUyffu+9u4vPs-vJ>$}(r@"u4;u`s-~ff}9JvJ+u`~ffs-v+"r@4$y>sr@/~ff"}(vwffy`w
vP"}r@9$ffffwny>y`wzK4/"r@@9v+}@wy"$y@w"Kff	ff%@NT$+(ff>/Uff%-ffffT($%@%ff
 	WO@yn=;}r@9$y
q:r$s+tffu4vw>x-y`wz{>}9~J9@vPs+uwyE($ffff."@yAsTus""+%JvP+/"+e~ff}(@+u49r@@/"}(rffyser@/~."}(vw>y`w
vP"}r@9$ffffwny>y`wzK4/"r@@9v+}@wy"$y@w"Kff	ff%@NT$+(ff>/Uff%-ffffT($%@%ff
 	WO@yn=;}r@9$y
q:r$s+tffu4vwx-y`w+u`~ff}(wy`wz{>}9~J9@v+s+uwyS$ffJPUy>r$~ff}9s+u`s+t4vPs+tJ9r$}G"r$"r$s+"r$s+@u4r@Au4/
tP}9~J"u4r$s.A"r@9@r$s.Au4"uY$+4$y9;:>?."@ff$ff"KffT1	P(ff>$E.9	wffUw	ff	$."y
qM}(u4"4rffwffyP$ffffffUy$K`""~ffs+r@9$~A}(r@$"}9}(r$s.:s+r$"}9~J$s+r@(MvP}9K~ff}(@+u49r@@/"}(rEAu4/W~A+u4""r$s-~ff}9JvJ
-vP"r@us.9r$}9"}r@/~J9u4vPs>y . %/U;ff--+/ffffw($Uw.	ff"y
 "~ff++u`s>wx-y`wzqM~J4"uwy($.ffff@yAu4""r$sT~ff}9JvJY-v+"r@4v/+rWtJ?"}(vffGr@us.@v+"+r@}(r@@r$+GvP}
~ffOu4ffyM"ff+/ff(;ff  +ffffffAN.G  w> ff /ff%y
 "}(u4/~ffs>wEy$ffffffUy=Mr@u`s.vP}@r$-r$sff4r$~ff}Gs+us+t|Au4/"r$}(@r$+%"~J~J4u`~J9u`s+tkA+r"r$}(@r$+%"~J
"uG9u`s+@9u4vPs+Y~ff""}(v+~J>yskM?	/%%ffN.ff-(l+JffffffffK;.@@$($/./ 	N.
/	N4P$/	wff">y$ff $ff"y
r$+9Gr$}@w$-yJyw ~u}wffy@ky`wJz"+us>w+y$qKyP$ffffffUy$~J+u`S"Su`Jr@4u`+v+vP}vPu`s+@vP+4r@9r
~J/~WPu`~/+rE~J4tffv+}(u4/"yM"ff+/ff(M  ff  ff/.  $	N	  Aw+Jw%."y
{}G~J9@vPs+uw"y`wPvP}(uwffky`w.~tfftffu`s+uwPky`wffz+v+~"w"y($.JP@y>s+ur@u`s.9r@tP}9~JGuv+s-vJr@"+4u@u4}G+r@
~ffs+ 4r$~ff}Gs+us+tk. r@"~ff+4ru`s}(r@$"}G}(r$sffs+r@(MvP}9.$y9:; >(ff"@ff	Nffnff
 ffM%@+
ff
 1.,ff+N/	/ff+w>ff@w"J+	JPny
~ff}uw-y`w+z=xEv+"s+twP>y($.ffff@y+rr@9Gu~J9u4vPs-vJ9GvP@"~J99u4K@vPs.9r@P}(r@rtP}9~ff~ff}(;+9u`s+t-/+r
us+Gu"r%v++99u4"r~J4tffvP}(u4/"y;ff  +$ . %/UYff
 .ffP+@+$w 	"wff ff"y

>r@rffw
-y {Ay$ffffffUy+ff-ff . %/U%%9PNfffiffMP-ff$	$4  -	;P  K9  @$-y
K.Mr$}MK$~J"r$-u4;"+y
>r@+u`s+9vPs>w>y>Ay`w>A~ff+u`s+r$}@wEyy`wz+vPs++uwky"kyE($ffff.@y;su`s./}(v++@9u4vPse9v/+rS~ff"+4u4$~	
9u4vPs,vJE/+r/+r@vP}(,vJ"}vP"~ff+u44uG9u4-"s+@Guv+s+vJE~-~ff}9JvJY"}(v+@r@9AGvO~.+9vP~JGu%"r@r@@
}(r@@vfftPs+u49u4vPs>yMA	N > 	 >/UP%ff""ff+/ff4w 	+@w$ff.	$ffy
evff@r$}@w	ky  y"($.ffff@yffA+rMu`s++@9u4vPs-vS+49u49$~r9r$"v+}9~J+9/}9+@/"}rffysWv+v+"ffwJy`wJ~ffs+GvPs>w
>y`wPz>u`"~.s"s>wyE"	y@wPKff	ff%@KN 1	P(ff/Uff%-ffffM?	/U/@ff  	W	"w"">y
ffff	.ff"~ffs,~JGr@vw  Synv+}(tP~ff
s ~ff.~ffs"s>y
A~ff+u`s+r$}@w:EyyM($ff.ff@y- /+GvP}(u`~JEvPs+u4""r$s~ff}9v OvP"r@4Y~ffs+ 9r@4r@@9r@ ~."+u4$~JGuv+s+u`s
/"r@r@@}(r@@v.tPs+u49u4vPs>yM(	/%/.N.ff(-PW9;:wEff>ff@w"ff.	ffffny

MvP.Mr$}@wyE($.JP@y;+r9u`-rW"uOr$s+9u4vPsTvJs+r$"}G~Js+r@(MvP}9-vP"r@4$yK-  Pff%:+	N.Pw
.@w"ff	Pffy
MvPs>w+y`w++u`s+tffr$}@w+xSywzMu4/+ffffw"Wy(	ffJP@y>A+rnvJr$}:vJ~ffs+r@9u`~"ys  vJA~ffs>wJy+y`w+r@%~ff"}(vw
y`wffzK4/"r@@GvP}@w+yE"$yUwff.$ff/U,1	P(ffn/@@ff%S.Nff,M($%@%ff > 	Jw+">y
$ff		ff"~ffs,~JGr@vw  Synv+}(tP~ff
s ~ff.~ffs"s>y



fi
"!$#&%('*)+
,.-#!
/0132547689;:<=?>7<A@B7CD:FEGCfi<H>7<JILKMBN4O4OB7813P<Q/R>QSUTVWXDY5>[Zfi28D9FC\B7C\]^B7C:F29_C8`49U2a9;25PF2C:.8`:FBOEGC\P?bc
29F9UEG9Ra9UE\a8`]G8`:FBOEGCfi>edfCg/01h254i68D9U:<=?>7<*ILjlkmQ4O254O4i8DC\n*<o
>QSqpnPr>sY5<tQuvUuwxwNyrwzR{7|5}qv.{~G}qy.
tvU.y5|.|{x\<`E4q>JTD<Gk568afi>
W<aafi>fiTWX>Dj^d9U25PFP<m813b9UBOn]2>
 2C\25:8<"p>JSUTrVWTY5>D

yfGuD}q{$ryRuD}qv{xy|uD
uDvF`Gu{x|5>  a9UB7C\]295<fi25E\9F
>
 EGC\nB7"<Jp>S;TVY>6\2E\a\:FB71?8`4k5EGCD:9UE4AE`a89;:FB78`4O4NclEGb\P_29U8b\4O2j^89F`E`ea9UEGk525P_PF25PE`29:6\2
 C\BN:_26\EG9UBO5EGCfi>AyvUu}q{x|Ry5|5y.uvU5G<fi`<ATT.fiTWW>
 EGC\nB7"<Jp>S;TVWY>6\2E\a\:FB71?8`4k5EGCD:9UE4AE`a89;:FB78`4O4NclEGb\P_29U8b\4O2j^89F`E`ea9UEGk525P_PF25PE`29:6\2
BiC  C\BO:F2?6\EG9;BN5E\CfinBOPFk5EG0CD:F25nk8`P_2>RAyrvuD}q{$|Ry|y.uvU5G<fiG<
Wr`
>
 :FE4Ok5`2<3>7<IR13EG6\0C\n
9UE
<  >ASUTVVDY5>J@BOnn2Cej^89F`E`?13E\n254*B7C\n
0\k5:FBOEGClbceQ8rc25PFB78Ce13E\n254
1329U]B7C\]
>RdCl@8C\PFEGCfi<  >fio">7<mE8Cfi<"o
>fi=?>7<fiIHRBO4O25P<*mR>fiZ>SqpnP>sY5<fiu"y|{$yrGvUuw
55v.uD}q{$tQvry||5{xr|}qyr|`<
aafi>*TTTW  8Cj^8`:F25E
<mQ3>jlEG9U]\8C[R8D0D1?8DCCfi>
KMBO4N4OB7813Pr</R><IfiB7a\PF295<R=?>S;TVWVY>(4O289FC\B7C\]8`4O]EG9UBO:61qEG9k5EGC:_BiC\08`4O4Oc9F0CC\B7C\]0\4O4Oc
9U25k09F9U2CD:C\209F8`4AC\25:QEG9_P>yrGvUuw\}qu}q{$D
<`<
W>



fiJournal of Artificial Intelligence Research 3 (1995) 119-145

Submitted 3/95; published 8/95

Using Qualitative Hypotheses to Identify Inaccurate Data
qi-zhao@is.aist-nara.ac.jp
nishida@is.aist-nara.ac.jp

Qi Zhao
Toyoaki Nishida
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama-cho, Ikoma-shi, Nara 630-01, Japan

Abstract

Identifying inaccurate data has long been regarded as a significant and dicult problem in AI. In this paper, we present a new method for identifying inaccurate data on the
basis of qualitative correlations among related data. First, we introduce the definitions of
related data and qualitative correlations among related data. Then we put forward a new
concept called support coecient function (SCF ). SCF can be used to extract, represent,
and calculate qualitative correlations among related data within a dataset. We propose an
approach to determining dynamic shift intervals of inaccurate data, and an approach to
calculating possibility of identifying inaccurate data, respectively. Both of the approaches
are based on SCF . Finally we present an algorithm for identifying inaccurate data by
using qualitative correlations among related data as confirmatory or disconfirmatory evidence. We have developed a practical system for interpreting infrared spectra by applying
the method, and have fully tested the system against several hundred real spectra. The
experimental results show that the method is significantly better than the conventional
methods used in many similar systems.
1. Introduction

In many problems of artificial intelligence, inferences are drawn on the basis of interpretation
or analysis of measured data. However, when measured data are inaccurate, interpreting
or analyzing them is very dicult. In diagnosis or signal analysis, for example, the general
reasoning method is to compare measured data with reference values (Reiter, 1987; Shortliffe
& Buchanan, 1975). When measured data are not accurate due to noise or other unforeseen
reasons, the comparison between measured data and reference values can not lead to any
useful conclusion. A rule like \if there is a strong peak in 3000 cm01 - 3100 cm01 on the

infrared spectrum of an unknown compound, then the unknown compound may contain at
least one benzene-ring" may work in ideal cases. However, the rule can not work in general
cases. For example, when the spectral data are inaccurate, e.g., the measured peak in 3000
cm01 - 3100 cm01 is not a strong peak but a medium one, or a measured strong peak is
not exactly located in 3000 cm01 - 3100 cm01 but is slightly shifted, the rule may not be
applied.
In practical problems, especially in data rich problems such as diagnosis and interpretation, measured data are often inaccurate. One reason is that the measuring methods are
error-prone. For example, a patient's temperature or blood-pressure may be inaccurately
measured or entered, and a witness may inaccurately describe the features of a criminal.
The other reason is that the real data are not noise-free. For example, among the received
c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiZhao & Nishida
signals, there may be some noise mixed up, and what is worse, infrared spectral data (peaks)
themselves may be noisy, i.e., some peaks may be affected by noise or other factors.
Identifying inaccurate data has long been regarded as a significant and dicult problem
in AI. Many methods have been proposed to deal with the problem. Fuzzy logic provides
a mathematical framework for representation and calculation of inaccurate data (Zadeh,
1978). By fuzzy logic, reference value x0 is associated with a fuzzy interval 4x. If a
measured data item falls into [x0 0 4x; x0 + 4x], then it can be identified as the reference
value with a corresponding membership degree. Probability theory and possibility theory
are also widely used for handling inaccuracy and uncertainty (Dempster, 1968; Duda, Hart,
& Nilsson, 1976; Pearl, 1987; Shafer, 1976; Shortliffe & Buchanan, 1975). The above
methods are commonly used in AI systems. The way of applying them, however, depends
on the nature of domain problems, and there is not yet a standard and generally accepted
method thus far.
We present a method for identifying inaccurate data on the basis of qualitative correlations among related data. The method is based on the essential consideration that some
data items within a dataset are qualitatively dependent: a set of data may describe the same
phenomenon, or refer to the same behavior. For example, a patient's temperature, blood
pressure and other symptomatic data reect the patient's disease, and a couple of peaks on
an infrared spectrum indicate the presence of a partial component. We call the dependency
among data within a dataset qualitative correlations among related data1 . By considering
qualitative correlations among related data, we can obtain confirmatory or disconfirmatory
evidence to identify inaccurate data. In general, related data should be simultaneously
present or absent, so if most of the related data have been completely identified, these data
will enhance the identification of the rest. For example, a benzene-ring can create many
other peaks besides the strong peak in 3000 cm01 - 3100 cm01 . All the peaks created by the
benzene-ring are related data which have qualitative correlations. If all the peaks except
that in 3000 cm01- 3100 cm01 have been completely identified, the benzene-ring is quite
likely to be contained by the unknown compound. Therefore, the inaccurate peak around
3000 cm01 - 3100 cm01 may still be identified. In fact, spectroscopists frequently use the
following knowledge in addition to the rules given at the beginning of this section:

If there is a strong peak around 3000 cm01 - 3100 cm01 , then the spectrum may
be partially created by benzene-rings |{ check peaks around 1650 cm01 , 1550
cm01 and 700 cm01 - 900 cm01 to make sure because a benzene-ring may have
other peaks there at the same time.
The central idea of our method is to find evidence for identifying inaccurate data by
considering qualitative correlations among related data. The idea is very common in human
thinking. When all the data except blood pressure of a patient show that the patient
has a certain disease, we would naturally suspect that the blood pressure of the patient
was inaccurately entered. Similarly, when all the peaks except one indicate that a partial
component is present, we would naturally suspect that the unmatched peak was inaccurately
measured or the peak was affected by noise or something else. If acceptable solutions can
be made by assuming an inaccurate data item to be a reference value based on qualitative
1. Detailed definitions will be given later.

120

fiUsing Qualitative Hypotheses to Identify Inaccurate Data
correlations between the data item and its related data, the inaccurate data item may be
compensated and hence identified.
Our contributions include: (1) a method which assumes an inaccurate data item to be
a certain reference value based on the qualitative correlations between the inaccurate data
item and all of its related data, (2) an algorithm which crystallizes the method, and (3) a
practical system which uses the algorithm to interpret infrared spectra.
The key point is a new concept called support coecient function (SCF ) for extracting,
representing, and calculating qualitative correlations among related data. When measured
data are inaccurate, the qualitative correlations among related data can provide evidence
for confirming or disconfirming the hypothesis that the measured data are the same as the
reference values. An approach to determining dynamic shift intervals of inaccurate data,
an approach to calculating possibility of identifying inaccurate data, and an algorithm for
identifying inaccurate data are proposed on the basis of SCF , respectively.
The method requires few assumptions in advance, so it can avoid inconsistency in knowledge and data bases. The method identifies inaccurate data by considering qualitative correlations among related data, so it is quite effective and ecient, especially in the case
of problems where dependencies among data apparently exist. In general, qualitative correlations among data can always, more or less, be extracted. In the worst case where
qualitative correlations are not known a priori, the method degenerates to a conventional
fuzzy method2 .
We have developed a practical system for interpreting infrared spectra by using the
method (Zhao & Nishida, 1994). The primary task of the system is to identify unknown
compounds by interpreting their infrared spectra. We have fully tested the system against
several hundred real spectra. The experimental results show that the method is significantly
better than the traditional methods used in many similar systems. The rate of correctness
(RC ) and the rate of identification (RI ) which are two important standards for evaluating
the solutions of infrared spectrum interpretation are near 74% and 90% respectively, and
the former is the highest among known systems.
In the following sections, we first describe the problem of identifying inaccurate data in
Section 2. In Section 3 we give some definitions including the concept of support coecient
function (SCF ) and other concepts based on SCF . In Section 4 we introduce our method
for identifying inaccurate data by considering qualitative correlations among related data.
Section 5 demonstrates the application of the method to a knowledge-based system for
infrared spectrum identification, and shows the experimental results of the system. Related
work is discussed in Section 6. Conclusions are addressed in Section 7.

2. Problem Description

In practical problems, measured data can be represented as a finite set:
2. We refer to the fuzzy methods which use an empirical fuzzy interval for each inaccurate data item as
conventional fuzzy methods.

121

fiZhao & Nishida

M D = fd1 ; d2 ; :::; dn g;

and reference values can also be represented as a finite set:
RV = fr1 ; r2 ; :::; rN g:

Suppose interpreting or analyzing measured data is carried out on the basis of so-called
\if-then" rules in which the premises are comparisons between M D and RV like \if di = rj
then ...", or \if (ri 2 MD) ^ (rj 2 M D) then ...". When M D is accurate, the main
operation implied by these premises is usually to find a corresponding reference value from
RV for each data item in M D. However, when M D is inaccurate, the operation becomes
complicated. In this case, it is dicult to determine which reference value an inaccurate
data item corresponds to, e.g., for some measured data no reference value may be simply
identified, while for others more than one may be available.
For example, if received signals are known to be accurate, and an expected signal (reference value) can not be found from the signal series (measured data), then we can conclude
that the expected signal does not appear. However, if received signals are inaccurate, and
an expected signal can not be identified from the signal series, it is hard to decide whether
the expected signal does not appear or appears but looks different due to the inaccuracy.
Most currently known approaches for dealing with inaccurate data such as fuzzy logic
and probabilistic reasoning are mainly based on quantitative similarity or closeness between
measured data and reference values. In some cases, however, the identity of qualitative
features is more effective and reliable than quantitative similarity or closeness.
Consider signal analysis again. If an inaccurate signal has the same qualitative features
as the expected one such as the interval of frequency, the signal may still be identified even
though its quantitative features are slightly different from those of the expected one such
as strength etc.; conversely, an inaccurate signal may not be identified if it is quantitatively
similar to an expected signal but does not have the same qualitative features as the expected
one.
We discussed the following points in Section 1, (1) some data items within a dataset are
qualitatively dependent (i.e., they are related data), (2) there are qualitative correlations
among related data, and (3) qualitative correlations among related data enable us to confirm
or disconfirm the identity of qualitative features.
Therefore, RV and M D can be, explicitly or implicitly, divided into finite groups on
the basis of qualitative dependencies among data, and the data in each group are related
to each other. For example, RV can be divided into R1 , R2 , ... and Rk :
RV = R1 [ R2 [ ::: [ Rk ;

where
Rj = frjl j rjl

2 RV; 1  l  mg:

The qualitative correlations among related data in Rj include: (1) data in Rj should be
simultaneously present or absent which means that all reference values in Rj should have
122

fiUsing Qualitative Hypotheses to Identify Inaccurate Data
corresponding data in MD, (2) the presence of rjp may enhance the presence of rjq , and
the absence of rjp may depress the presence of rjq . Considering the qualitative correlations
among related data will lead to evidence for the identification of inaccurate data.
The problem of interpreting/analyzing inaccurate data is to make qualitative hypotheses
for M D, or in other words, to find a subset of RV for MD, which is corresponding to M D:
IN (M D);

(IN (MD)  RV ):

The problem can be briey represented as the following predicate calculus:

8di8Rj ((di @Rj ) ^ (Rj @M D) ! Rj  IN (M D))3 ;
where \di@Rj " and \Rj @MD" are two essential qualitative predicates in our method which
represent that di possibly (qualitatively) belongs to Rj (i.e., ? di 2 Rj ), and Rj possibly
(qualitatively) belongs to MD (i.e., ? Rj  M D), respectively. Determining \A@B " is
based on qualitative correlations among related data. The work presented in this paper
is mainly concentrated on determining \di @Rj " and \Rj @M D", and realizing the above
predicate calculus.
3. Preliminaries

Before introducing our method, we first put forward and explain several new concepts in
this section.

3.1 Qualitative Correlations among Related Data
Definition 3.1 Related data: If data d1 , d2 , ..., and dm describe a common phenomenon,
or they refer to the same behavior simultaneously, then they can be treated as related data.
For example, a patient's temperature, blood pressure and other symptomatic data are
related data, and all the features for describing a criminal are also related data. The phenomenon that some data within a dataset are related data is more apparent in engineering.
For instance, there are two types of related data in infrared spectrum interpretation as
shown in Figure 1. First, as far as a single peak is concerned, the frequency (position) fi ,
strength (height) si , and width (shape) wi of the peak are related data. Second, a partial
component may create numerous peaks at the same time. If we consider all the peaks that
a partial component may create, all of these peaks are related data.

Definition 3.2 Qualitative correlations among related data: If di and dj are two related data
items, then the presence of di enhances the presence of dj , and the absence of di depresses
the presence of dj . This kind of effect is called qualitative correlations among related data.
3. Conicts (overlaps) in IN (MD ) should be eliminated. We will not discuss conict-resolving in this
paper, but will concentrate on the method for identifying inaccurate data, i.e., ? di @Rj and ? Rj @MD.
Interested readers may refer to the paper by Zhao (1994) for specific discussion concerning the problem
of conict resolution.

123

fiZhao & Nishida

fi

s

i

wi

Figure 1: Example of related data in spectrum interpretation
Consider the above example of spectrum interpretation again. If spectral data are inaccurate (i.e., some measured peaks look like but are not exactly the same as reference
peaks), considering qualitative correlations among related data may lead to qualitative evidence for the identification of inaccurate data. For example, suppose the frequency of a
peak is slightly different from the reference value, and both the strength and width of the
peak are the same as the reference values. Then the frequency of the peak may still be
identified since both of its related data support it. Similarly, if peaks at low frequency sections are inaccurate, considering related peaks at high frequency sections may help identify
these peaks, and vice versa.

3.2 Support Coecient Function
Definition 3.3 Support coecient function (SCF): If there are m 0 1 data related to di,
then the support coecient function of di calculates the total effects from the related data
by considering the qualitative correlations between di and each of its related data.
Suppose (di ; dj ) represents the qualitative correlation between di and dj , then the
support coecient function of di can be defined as:
SCFi = fi (

m
X

j =1;j 6=i

(di; dj ); m):

SCFi should directly depend on how many and how much related data support di .
When SCFi is greater than a certain value given by domain experts, the related data tend
to support di ; otherwise, the related data tend to depress di .

3.3 Evidence Based on SCF
In Section 2, we used \di @Rj " to express that di can be qualitatively identified from Rj .
Realizing \di @Rj " requires a definition of a shift interval 4 for Rj such as:
124

fiUsing Qualitative Hypotheses to Identify Inaccurate Data

Rj 6 4 = f(rjl 6 4) j l = 1; 2; :::; mg;

and a definition of the possibility of \di 2 Rj 6 4".
The above formula is similar to that in fuzzy logic, but contains completely different
meanings. The primary difference is that the shift intervals are dynamically determined by
SCFi , while in fuzzy logic, the fuzzy intervals are usually provided by domain experts in
advance or calculated with quantitative criteria.

Definition 3.4 Shift interval: Shift interval is a dynamic region for inaccurate data. Given
a standard fuzzy interval for inaccurate data, the shift interval of di varies around the
standard fuzzy interval on the basis of SCFi . When SCFi shows that the related data
support di , the shift interval of di becomes wider than the standard fuzzy interval. On the
other hand, when SCFi shows that the related data do not support di , the shift interval of
di becomes narrower than the standard fuzzy interval.
Definition 3.5 Evidence based on SCFi : SCFi determines the shift interval of di , that is,
SCFi determines how widely di is allowed to shift. The wider the shift interval, the more
easily di is identified. Therefore, SCFi provides confirmatory or disconfirmatory evidence
for identifying di .
4. Making Qualitative Hypotheses for Inaccurate Data

In this section, we introduce and analyze our method for identifying inaccurate data. We
first discuss the processes of realizing two essential predicates in our method, \di @Rj " and
\Rj @MD" respectively. Then, we present an algorithm for making qualitative hypotheses
for inaccurate data (i.e., for realizing the predicate calculus described in Section 2).

4.1 Predicate \di @Rj "
When di is accurate, \di@Rj " is equal to \di 2 Rj ". If there is a reference value in Rj which
corresponds to di (i.e., rjp 2 Rj and rjp = di), then di @Rj = T . If there is no reference
value corresponding to di , then di@Rj = F . When di is inaccurate, however, it is not sure
whether rjp corresponds to di . In this case, \di@Rj " means that di possibly (qualitatively)
belongs to Rj , or in other words, rjp possibly (qualitatively) corresponds to di . The value
of \di @Rj " is not T or F , but the possibility of \rjp = di " or \di 2 Rj ".
We discussed in Section 2 that in some cases the identity of qualitative features is more
robust and reliable than quantitative similarity or closeness. We have also discussed that
qualitative correlations among related data can lead to evidence for the identity of qualitative features in diagnosis or interpretation. So if rjp (rjp 2 Rj ) is assumed to correspond to
di, and there are m 0 1 reference values (rj1 , rj2 , ..., rjp01 , rjp+1 , ..., rjm ) related to rjp , then
each of the m 0 1 reference values should correspond to a certain data item in MD, and the
m 0 1 data items in M D are also related to each other. Therefore, qualitative correlations
between di and its m 0 1 related data items in MD should be considered.
Our method first determines the possibility of \rjp = di " by calculating the similarity
or closeness between rjp and di like conventional fuzzy methods, then considers qualitative
125

fiZhao & Nishida
correlations among related data to obtain evidence for updating the possibility. When the
qualitative correlations show that the related data support \rjp = di ", the possibility of
\rjp = di " will increase. When the qualitative correlations show that the related data do
not support \rjp = di ", the possibility will decrease.
4.1.1 Defining Support Coefficient Function

Suppose rjq (rjq 2 Rj ) corresponds to dt . Because rjq is related to rjp , dt is related to di .
As we have discussed, the qualitative correlation between di and dt means that if dt exists,
then di is enhanced; otherwise, di is depressed.
We first define the qualitative correlation between two related data items, di and dt , as:
ci (dt ) =

(

1 if dt can be found from MD which satisfies: rjq 0 do  dt  rjq + do
0 if dt can not be found from M D which satisfies: rjq 0 do  dt  rjq + do

where do is a standard fuzzy interval of inaccurate data, and ci(dt) expresses the qualitative
correlation between di and dt . ci (dt )=1 means that di is enhanced since its related data
item dt can be found from the measured dataset, and ci (dt )=0 means that di is depressed
since its related data item dt can not be found from the measured dataset. The definition
of ci (dt ) is simply based on the consideration that if a data item is identified, then the data
item will support its related data items (i.e., the coexisting data items).
As there are m reference values in Rj , we can define the support coecient function
SCFi for di based on ci(dt) (t = 1; 2; :::; m; t 6= i):
SCFi =

1+

Pm

t=1;t6=i ci (dt )

m

where 0 < SCFi  1, and SCFi expresses the total qualitative correlations between di
and all of its related data. In other words, SCFi reects the support coecient of rjp
corresponding to di .
If m = 1, then SCFi = 1. When m > 1, SCFi is in the direct ratio to the number of
the related data which may be identified from M D.
4.1.2 Determining Dynamic Shift Interval

Suppose do is a standard fuzzy interval of inaccurate data, we define the dynamic shift
interval of di based on SCFi as:

0 1)do 2 SCF
4di = (2m m
i
where 0 < 4di < 2do , and 4di is in the direct ratio to SCFi .
If m = 1, then SCFi = 1, and 4di = do . In other words, when qualitative correlations
among data are not known a priori, SCFi = 1 and 4di = do . In this case, the method
degenerates to a conventional fuzzy method.
126

fiUsing Qualitative Hypotheses to Identify Inaccurate Data
When m is fixed, the more the related data are identified, the greater SCFi is, therefore
the greater 4di is. When SCFi is fixed, 4di depends on the number of related data.
Table 1 shows the relation among 4di , m and SCFi .

4di

1
do
/
/
/
/

1
0.8
SCFi 0.5
0.3
0.1

10
1.9000do
1.5200do
0.9500do
0.5700do
0.1900do

50
1.9800do
1.5840do
0.9900do
0.5940do
0.1980do

m

100
1.9900do
1.5920do
0.9950do
0.5970do
0.1990do

500
1.9980do
1.5984do
0.9990do
0.5994do
0.1998do

1000
1.9990do
1.5992do
0.9995do
0.5997do
0.1999do

Table 1: Relation among 4di , m and SCFi
We can draw the following properties from the above formulas.
Property 1: With the same m, the more the related data are identified, the greater SCFi
is; otherwise, the smaller SCFi is.
Property 2: With the same m, the greater the SCFi , the greater is 4di . In other words,
the more the related data support di , the more widely di is allowed to shift.
Property 3: With the same SCFi, the greater the m, the less 4di varies along with m. In

other words, the greater the number of related data, the less a single related data item can
affect di .
Property 2 and Property 3 are illustrated in Figure 2.
2do
SCFi = 1

di

SCFi = 0.5
do
SCFi = 0.1
SCFi = 0.3

0
m

Figure 2:

4di versus m with different SCFi

Property 4: 4di is in linear relation to SCFi . The slope is equal to, or greater than 1.5,
which means that 4di heavily depends on SCFi .
127

fiZhao & Nishida
Property 5: Along with the increase of m, the slope increases very slightly. In other
words, 4di depends on the number of the related data which support di , rather than the
total number of related data.
Property 4 and Property 5 are illustrated in Figure 3.
2d o

m=100

di
m=2
do

m=10

0
0

Figure 3:

1

SCFi

4di versus SCFi with different m

4.1.3 Calculating Value of Predicate \di @Rj "

The value of \di @Rj " is equal to the possibility of \rjp = di " which can be calculated by
using the following formula:
i = 1 0

j di 0 rj j
4di
p

where i  1.
At a glance, the representation of i looks like the membership degree of \rjp 0 4di 
di  rjp + 4di" in fuzzy logic. However, the meaning is completely different, for 4di is
neither provided by domain experts nor determined by quantitative similarity or closeness.
Here 4di is determined on the basis of qualitative correlations among related data. When
qualitative correlations among related data are not considered, 4di is do , and the possibility
jd 0r j
is 1 0 i do jp . With the consideration of qualitative correlations, the possibility is updated.
Two new properties can be drawn from the above formula for calculating i .
Property 6: With the same di, the greater the 4di , the greater is i . In other words, the
wider the dynamic shift interval, the greater is the value of \di @Rj ". Formally, if 4d00i 
4d0i 4di , then 00i 0i i .
Property 7: SCFi provides qualitative evidence for accepting or rejecting di as rjp since
i is in the direct ratio to 4di, and 4di is in the direct ratio to SCFi.
128

fiUsing Qualitative Hypotheses to Identify Inaccurate Data
Property 6 and Property 7 are illustrated in Figure 4.
1
ui
ui

ui

0
rjp

di
di
di

di

Figure 4: Value of \di@Rj " versus various 4di
The above process of realizing \di @Rj " and calculating the value of \di @Rj " can be
expressed by the following procedure.
Procedure di@Rj
select rjp f rom Rj ;
SCFi = 0;
if di = rjp f
SCFi = 1;
i = 1;

g

elsef

for each rjl 2 Rj (l = 1; :::; m; l 6= p)f
calculate ci (dt )4 ;
SCFi = SCFi + ci (dt );

g

g

SCFi = (1 + SCFi )=m;
4di = do 2 SCFi 2 (2m 0 1)=m;
i = 10 j di 0 rjp j =4di;

4. dt stands for the data item in MD which corresponds to rjl .

129

fiZhao & Nishida
if i > 0
return i;
else
return N IL

end procedure

When di can be identified with a certain possibility (i.e., i > 0), the procedure returns
T (i.e., the value of i); otherwise, the procedure returns F .

4.2 Predicate \Rj @M D"

When M D is accurate, \Rj @M D" is equal to \Rj  M D". If all the m reference values in
Rj can be identified from MD, then Rj @MD = T ; otherwise Rj @M D = F . When M D is
inaccurate, however, \Rj @M D" means that Rj is possibly (qualitatively) a subset of M D.
The value of \Rj @MD" is not T or F , but the possibility that all the reference values in
Rj can be identified from M D.
If l > 0 (l = 1; 2; :::; m), then Rj can be regarded as a subset of M D with a certain
possibility. Let s1 , s2 , ..., and sm be the priorities of the reference values in Rj , then the
value of \Rj @MD" can be calculated based on 1 , 2 , ..., and m by using the following
formula:

Pm s 2 
l
=1 l
Rj @MD = lP
m s ;
l=1 l

s l > 0;

l > 0:

Suppose i has been calculated by using procedure di @Rj , then the process of realizing
\Rj @MD" and calculating the value of \Rj @M D" can be expressed by a simple procedure.
Procedure Rj @M D
P = si 2 i ;
S = si ;
f or l = 1 to m (l 6= p)f
l = dt @Rj ;
if l > 0f
P = P + sl 2 l ;
S = S + sl ;

g

elsef

g

g

P = 0;
exit;

if P > 0
return P=S ;
else
return N IL

end procedure

130

fiUsing Qualitative Hypotheses to Identify Inaccurate Data
When Rj can be identified as a subset of M D with a certain possibility (i.e., P=S ), the
procedure returns T (i.e., the value of P=S ); otherwise, the procedure returns F .

4.3 Algorithm for Making Qualitative Hypotheses for Inaccurate Data
We give the following algorithm for interpreting/analyzing measured data based on procedure di @Rj and procedure Rj @MD. When measured data are not accurate, the
algorithm can identify inaccurate data items by considering qualitative correlations among
related data.

Algorithm M aking -Qualitative-Hypotheses
IN (MD) = ;;

f or i = 1 to n f
for j = 1 to k f
P (Rj ) = 0;
if di @Rj (i:e:; Procedure di @Rj )
if Rj @M D (i:e:; Procedure Rj @M D) f
Rj ! IN (MD);
P (Rj ) = Rj @MD;

g

g
g

end if
end if

end for

end f or

end algorithm
In the algorithm, P (Rj ) represents the value of \Rj @MD". The algorithm is actually
the realization of the predicate calculus: 8di8Rj ((di @Rj ) ^ (Rj @M D) ! Rj  IN (MD)).
For each measured data item in fd1 , d2 , ..., dn g, the algorithm searches fR1 , R2 , ...,
Rk g once. For each Rj (Rj = frj1 ; rj2 ; :::; rjm g), the algorithm checks other n 0 1 measured
data items for m times, and other m 0 1 reference values for n times. Therefore, with blind
search, the number of operations is about (at worst): n 2 k 2 [m 2 (n 0 1) + n 2 (m 0 1)] =
2 2 k 2 m 2 n2 0 k 2 n2 0 k 2 m 2 n. Since k and m are two constants, the complexity of
the algorithm is O(n2 ).
5. Application to Infrared Spectrum Interpretation

We have developed a knowledge-based system for interpreting infrared spectra by applying
the proposed method, and have fully tested the system against several hundred real spectra.
The experimental results show that the proposed method is significantly better than the
conventional methods used in many similar systems.
131

fiZhao & Nishida
5.1 Infrared Spectrum Interpretation
The primary task of infrared spectrum interpretation is to identify unknown objects by
interpreting their infrared spectra. In this paper, we will limit the problem to interpretation
of infrared spectra of compounds to determine composition of unknown compounds without
loss of generality.
Selecting infrared spectrum interpretation as the domain of application is out of the
following reasons:
1. Interpreting infrared spectra is a very significant problem in both academic research
and industrial application. For example, in chemical science and engineering, interpreting infrared spectra of compounds is the most effective way to identify unknown
compounds, and to analyze the composition and purity of compounds (Colthup, Daly,
& Wiberley, 1990).
2. Interpreting infrared spectra is a very dicult problem. First, spectral data are huge
in quantity, and complex in representation. Second, both symbolic reasoning and
numerical analysis are needed to interpret infrared spectral data (Puskar, Levine, &
Lowry, 1986; Sadtler, 1988).
3. Interpreting infrared spectra is a typical problem dealing with inaccurate data since
spectral data are often inaccurate. They often shift from their theoretical values due to
various reasons. For example, the following is an assertion for spectrum interpretation:

The high frequency peak of partial component P Cff is located at Fi .
In practice, however, the peak of P Cff may irregularly shift around Fi due to noise or
other unforeseen reasons. When the above assertion is used to identify real spectra,
uncertainty arises.

5.2 Applying the Proposed Method to Infrared Spectrum Interpretation
Interpreting infrared spectra is a special problem of diagnosis. Suppose the infrared spectrum of an unknown compound can be thresholded and represented as a finite set of peaks
(i.e., the measured dataset MD):
Sp = fp1 ; p2 ; :::; pn g;

where every peak consists of the frequency (position) f , strength (height) s, and width
(shape) w, respectively:
pi = (fi; si ; wi )

i = 1; 2; :::; n:

Because fi , si and wi refer to the same peak pi , they are related data. This is the first
kind of related data in infrared spectrum interpretation.
132

fiUsing Qualitative Hypotheses to Identify Inaccurate Data
Suppose there are finite partial components (i.e., reference values RV ):
P C = fP C1 ; P C2 ; :::; P Ck g
= ffpj1 ; pj2 ; :::; pjm g j j = 1; 2; :::; kg
= ff(fjp ; sjp ; wjp ) j p = 1; 2; :::; mg j j = 1; 2; :::; kg.
Because fjp , sjp and wjp also refer to the same reference peak pjp , they are the first kind
of related data as well.
The spectroscopic knowledge for interpreting infrared spectra is usually expressed as \if
pi is equal to pjp , then pi may be created by partial component P Cj ". Here \pi is equal to
pjp " represents that fi , si, and wi are equal to fjp , sjp , and wjp respectively.
The first kind of related data has the following qualitative correlations:
1. fi , si and wi should be identified simultaneously, that is,
 if fi is fjp , then si is sjp and wi is wjp , and
 if si is sjp , then fi is fjp and wi is wjp , and
 if wi is wjp , then fi is fjp and si is sjp .
2. related data support each other. For example, if both fi and si have been identified,
then they will enhance the identification of wi . Conversely, if fi and si have not been
identified, then they will weaken the identification of wi .
Our method for identifying fi , si and wi based on the qualitative correlations among
them can be formalized as the following predicate calculi, respectively:

8fi 8pj ((fi @pj ) ^ (pj @pi ) ! pi is created by P Cj ), and
8si 8pj ((si@pj ) ^ (pj @pi ) ! pi is created by P Cj ), and
8wi 8pj ((wi @pj ) ^ (pj @pi ) ! pi is created by P Cj ),
p

p

p

p

p

p

p

p

p

where \pi is created by P Cj " means that fi , si and wi can be qualitatively identified to be
fjp , sjp and wjp .
In general, each partial component may create finite peaks at the same time. So if pi is
created by P Cj , then Sp is partially created by P Cj ; if Sp is partially created by P Cj , then
all the peaks that P Cj may create should be contained by Sp simultaneously. Therefore,
all the peaks created by a partial component are also related data. This is the second kind
of related data in infrared spectrum interpretation.
The second kind of related data has the following qualitative correlations:
1. all the peaks of a partial component should be identified simultaneously, that is,
if pi is pjp , then pjl

2 Sp

(l = 1; 2; :::; m; l 6= p).

2. the peaks created by the same partial component support each other. For example,
if most of the peaks of a partial component have been identified, these peaks will
enhance the identification of the rest peaks. Conversely, if most of the peaks of a
partial component can not be identified, then the identification of the rest peaks will
be depressed.
133

fiZhao & Nishida
Our method for identifying related peaks based on the qualitative correlations can be
formalized as the following predicate calculus:

8pi 8P Cj ((pi @P Cj ) ^ (P Cj @Sp) ! P Cj  IN (Sp)).
5.3 System for Interpreting Infrared Spectra
Our system is implemented with C and MS-WINDOWS. Figure 5 shows the data ow
diagram of the system.
Knowledge
Base
spectroscopic
knowledge
input

H
C H
H

solution

INFERENCE ENGINE
reference
values
Sp: Unknown Infrared Spectrum

Data
Base

PCa

PCb

COC
PCc

IN(Sp): Interpretation of Sp

Figure 5: Data ow diagram of the system
The input data of the system are infrared spectra of unknown compounds, and the
solutions are partial components that the input spectra may contain. Because inferences
are based on qualitative features of spectral data and qualitative correlations among related
data, the system can gain high correct interpretation performance with noisy spectral data.
As we mentioned before, there are two types of related data in infrared spectrum interpretation: all the features of a single peak (i.e., fi , si and wi of pi), and all the peaks
of a single partial component (i.e., p1 , p2 , ... and pm ). The inference engine of the system
employs the proposed method to both types of the related data when inaccuracy arises.

5.4 An Example
We discuss the performance of the system through the following example. Figure 6 shows
an infrared spectrum of an unknown compound. The spectrum is very hard to interpret
since the peak with an arrow (named p1 ) shifts substantially. Our system correctly identifies
that p1 is created by partial component benzene-ring.
In contrast, many similar systems can not correctly identify the peak (Clerc, Pretsch,
& Zurcher, 1986; Hasenoehrl, Perkins, & Griths, 1992; Wythoff, Buck, & Tomellini,
1989) since the peak of a benzene-ring at this frequency position (named pb1 ) should be
a strong peak (i.e., sb1 > 1:000) according to spectroscopic knowledge, not a medium one
(s1 = 0:510) as the case in this example. Systems based on conventional fuzzy methods
usually assume a fuzzy interval for each inaccurate peak, then determine the membership
degree that the inaccurate peak is in the fuzzy interval. Suppose the reference value for
a strong peak is 1:000, and the fuzzy interval for a strong peak is 0:300 (Colthup, Daly,
134

fiUsing Qualitative Hypotheses to Identify Inaccurate Data
& Wiberley, 1990), then only peaks with strength of 1 6 0:300 can be regarded as strong
peaks. Obviously, by conventional fuzzy methods, the possibility of p1 being a strong peak
is zero, i.e., benzene0ring (s1 ) = 0.
Inferring on the basis of qualitative correlations among related data, our system makes
a correct interpretation of the spectrum. Through the following two cases, we introduce the
inference process of the system, and at the same time demonstrate the use of our method
for identifying inaccurate data.

Strength (Absorbance)

0.000

1.200
4000

Frequency(cm1)

600

Figure 6: An example of infrared spectrum
5.4.1 Case I: Considering the First Kind of Related Data

Because the frequency (position) and width (shape) of p1 are both the same as those of
benzene-ring, the possibility of f1 being identified as fb1 is 100% (i.e., benzene0ring (f1 ) = 1),
and the possibility of w1 being identified as wb1 is also 100% (i.e., benzene0ring (w1 ) = 15 .
As we have discussed before, f1 , s1 and w1 are related data, so we can obtain confirm
evidence for identifying s1 by considering qualitative correlations among s1 , f1 and w1 :
benzene0ring (f1 ) = 1,
so, cs1 (f1 ) = 1 (cs1 (f1 ) represents the qualitative correlation between s1 and f1 ),
benzene0ring (w1 ) = 1,

so, cs1 (w1 ) = 1 (cs1 (w1 ) represents the qualitative correlation between s1 and w1 )
so, SCFs1 =

1+2
3

= 1, and

4s1 = (601)23 0:300 2 1 = 0:500, and
:510 = 0:02.
s1 @pb1 = 1 0 100:0500
5. 3 (d) means the possibility of d being identified by conventional fuzzy methods, i.e., SCF is not
considered.

135

fiZhao & Nishida
By considering SCFs1 , the possibility of p1 being regarded as a strong peak of benzenering increases from 0 to 0:02. As possibility, 0:02 may not be different from 0:04 or 0:06, but
0:02 is significantly different from 0. Many near-misses may be handled by the negligible
possibility. For example, in most systems based on fuzzy and other methods (Clerc, Pretsch,
& Zurcher, 1986), it is impossible to identify p1 to be \strong" (i.e., benzene0ring (s1 ) = 0),
but considering qualitative correlations among related data makes it possible although the
possibility is only 0:02.
As mentioned before, f1 and w1 are both the same as the reference values, so f1 @pb1 = 1,
and w1 @pb1 = 1.
Suppose the priorities of f1 , s1 and w1 are 2, 1 and 1 respectively, then the possibility
of p1 being identified as pb1 is:
1 = pb1 @p1 =

2 2 1 + 0:02 + 1
= 0:755:
4

5.4.2 Case II: Considering the Second Kind of Related Data

The process of considering the second kind of related data is quite similar.
We have got that the possibility of p1 being created by a benzene-ring is 1 (1 = 0:755).
Suppose the benzene-ring can create m peaks: fpb1 , pb2 , ..., pbm g, then the m peaks are
related to each other. If p1 is created by the benzene-ring, then Sp is partially created
by the benzene-ring, i.e., the benzene-ring is contained by the unknown spectrum; if Sp
is partially created by the benzene-ring, then the other m 0 1 peaks of the benzene-ring
should also be identified.
By using the same procedure as obtaining 1 , we can get 2 , 3 , ... and m as well.
According to our method, the qualitative correlation between two related peaks, pi and pj ,
is defined as:
ci (pj ) =

(

if j  0:5
if j < 0:5:

1
0

So
SCFi =

1+

Pm

j =1;j 6=i ci (pj )

m

;

0 < SCFi  1:

Let do = 1, then

4di = 2mm0 1 2 SCFi ;

0 < 4di < 2;

and
pi @benzene 0 ring = 1 0

1 0 i
4di ;
136

pi @benzene 0 ring  1:

fiUsing Qualitative Hypotheses to Identify Inaccurate Data
Roughly, when SCFi > 0:5, related peaks tend to support pi . When related peaks
support pi , 4di > 1. When 4di > 1, pi @benzene 0 ring > i .
Table 2 shows the relation among pi @benzene 0 ring, i and 4di .
pi @benzene 0 ring

4di

1.3
1.1
1
0.9
0.7

1
1
1
1
1
1

0.8
0.846
0.818
0.8
0.778
0.714

i
0.5
0.615
0.545
0.5
0.444
0.286

0.3
0.462
0.364
0.3
0.222
0

0
0.231
0.091
0
-0.111
-0.429

Table 2: Relation among pi @benzene 0 ring, i and 4di
In the above example, SCF1 = 0:850, and 4d1 = 1:658, so
p1 @benzene 0 ring = 1 0

1 0 0:755
= 0:852:
1:658

Therefore, the possibility of p1 being identified as pb1 increases from 0:755 to 0:852 due
to qualitative correlations among related peaks. The process is similar to the probability
propagation in probabilistic reasoning. Here identifying p1 is a hypothesis, and qualitative
correlations among related data of p1 are pieces of evidence.
After all the peaks of the benzene-ring are identified, the possibility that the benzenering is contained by Sp can be finally calculated by employing the same method as described
in Section 5.4.1.

5.5 Analysis of Experimental Results
We compare two methods in the experiments. The first method (called \AF ") is a conventional fuzzy method which is used by most similar systems (Clerc, Pretsch, & Zurcher, 1986;
Wythoff, Buck, & Tomellini, 1989). To use AF , each reference value must be associated
with a fuzzy interval for dealing with inaccuracy. Both reference values and fuzzy intervals
are empirically determined (Colthup, Daly, & Wiberley, 1990).
Table 3 lists some reference values and their fuzzy intervals used by AF .
137

fiZhao & Nishida

2960 6 15cm01
2870 6 15cm01
1450 6 10cm01
...
benzene 0 ring 3055 6 25cm01
1645 6 10cm01
1550 6 30cm01
1450 6 3cm01
...
0CH2 0 OH 3635 6 5cm01
3550 6 25cm01
...

CH3

strong 6 0:3
sharp 6 1
strong 6 0:3
sharp 6 1
medium 6 0:3 sharp 6 0:5
strong 6 0:3
medium 6 0:3
medium 6 0:3
medium 6 0:3

sharp 6 1:5
sharp 6 0:5
sharp 6 1
sharp 6 0

strong 6 0:3
strong 6 0:3

broad 6 1
sharp 6 1

Table 3: Some reference values and their fuzzy intervals
The membership function of AF is:

r (d) = maxf0; 1 0

j d 0 r j g;
4d

where d is a measured data item, r is a reference value, 4d is the fuzzy interval of r, and
0  r (d)  1.
The second method (called \AF 3 ") is the proposed method. AF 3 uses the same reference values and fuzzy intervals as AF , but the fuzzy intervals in AF 3 are only used as
standard fuzzy intervals based on which dynamic shift intervals are determined by considering qualitative correlations among related data.
AF and AF 3 use the same reference values and empirical fuzzy intervals. The formula
for calculating membership degrees in AF (i.e., r (d) = maxf0; 1 0 jd40drj g) is also similar to
jd 0r j
the formula for calculating possibility in AF 3 (i.e., i = 1 0 i4dijp ). However, in AF , 4d
is simply an empirical fuzzy interval, while in AF 3 , 4di is a dynamic shift interval based
on qualitative correlations among related data.
We have tested the system against several hundred real infrared spectra of organic
compounds. The experimental results show that AF 3 is significantly better than AF .
Table 4 lists part of the experimental results in which the first column indicates the
solutions obtained by AF ; the second column indicates the solutions obtained by AF 3 ; and
the third column shows the correct solutions.
138

fiUsing Qualitative Hypotheses to Identify Inaccurate Data

AF (Without SCF)

2/3

CH2

CH3

CH2

C

AF* (With SCF)

[CH2]n

CH2

CH3

[CH2]n

CH2

CH3 [CH2]n

CH2

CH3

C

CH2

CH3

CH3
CH2

CH3

CH3
CH2

CH

CH3

CH3
CH2

CH3

CH3

CH2

C

CH3

CH2

CH

CH3

CH2

1/3

>C=CH

4/5

C=CH

CH2

CH3

>C=CH

1/2

3/4

CH2

C

Cl
Cl

CH3

NH2

CH3

CH3

CH3
>C=CH

2/2

CH2

CH3

CH3

2/3

CH2

CH3

CH2

CH3

2/3

CH3

>C=CH

CH3

>C=CH

[CH2]n

C=CH

CH2

CH3

CH3

CH3

C=C

C

NH2

>C=CH

CH2

CH3

>C=CH

>C=CH

CH[CH3]2

CH2

CH3

C

CH

CH3

CH[CH3]2

CH[CH3]2
2/3

CH2

CH2

CH2

CH

CH3

[CH2]n C=CH
>C=CH

CH3

CH3

CH3

CH3

[CH2]n

C

CH

CH3

CH3
2/3

CH2

CH3

CH3
3/4

CH

C
CH3

CH3

CH3
2/3

Correct Solutions

Cl

CH2

CH3

C=C

C

Cl
C

CH3

Cl
Cl

C

NH2

: identified PC set is the same as the PC set in the correct solution(in this case, RI=1)
n

: identified PC set is not the same as the PC set in the correct solution(the number indicates the RI)

Table 4: Experimental results with AF and AF 3
139

fiZhao & Nishida
There are two important standard metrics for evaluating solutions of infrared spectrum
interpretation:

Definition 5.1 Rate of correctness (RC): the rate that the identified partial component set
is exactly the same as the partial component set in the correct solutions.
Definition 5.2 Rate of identification (RI): the rate that the partial components in the
correct solutions are identified.
Table 5 shows the comparison between AF and AF 3 with the two standard metrics.
RC (error-rate) RI (error-rate)
AF
0.455
(0.545)
0.812
(0.188)
AF 3
0.736
(0.264)
0.894
(0.106)
Table 5: Evaluation of AF & AF 3 with RC and RI

Table 5 demonstrates that both the RC and RI increase by integrating SCF , but the
RC increases more significantly. The reason is that although AF can identify most partial
components of unknown compounds, the rate that it can identify all partial components
of unknown compounds is low because there are always some partial components whose
measured peaks seriously shift from the reference values.

5.6 Comparison with Related Systems
Related systems mainly fall into the following four categories: (1) Systems based on Y/N
classification, (2) Systems based on fuzzy logic, (3) Systems based on pattern recognition,
and (4) Systems based on neural networks.
5.6.1 Systems Based on Yes/No Classification

The method commonly used by spectroscopists in practice is numerical analysis (Colthup,
Daly, & Wiberley, 1990). Numerical analysis is primarily based on comparison between spectral data and reference values. Reference values are usually some regions like f requency :
3615 6 5cm01 or strength : 1:000 6 0:300. If spectral data are in certain regions, the answer
of classification is yes; otherwise, the answer is no.
Most systems for interpreting infrared spectra use this method (Hasenoehrl, Perkins, &
Griths, 1992; Puskar, Levine, & Lowry, 1986; Wythoff, Buck, & Tomellini, 1989). For
example, in Wythoff's system, rules for comparing spectral data are in the following forms.
ANY PEAK(S)

FREQUENCY:1700-1707
WIDTH:SHARP TO BROAD

STRENGTH:0.7-1.0

ANSWER -YESACTION - ***

The advantage of these systems is that they are very easy to develop because they
can directly use spectroscopic knowledge, and do not need further computation. However,
the problem is that each of these systems is only applicable to a class of compounds, or
pure compounds because in the case of seriously inaccurate spectral data, the reference
values (regions) can not reect the inaccuracy. For example, Hasenoehrl's system is only
140

fiUsing Qualitative Hypotheses to Identify Inaccurate Data
for distinguishing compounds containing at least one carbonyl functionality from other
compounds, although the RI of the system is about 98% (naturally, the RC is not available),
and Puskar's system is only for identifying hazardous substances.
In fact, spectroscopists also use qualitative analysis in some specific cases in addition to
the formal spectroscopic knowledge, such as \if the peaks in 600 cm01 - 900 cm01 look like
the peaks of benzene-rings, then the peaks in 3000 cm01 - 3100 cm01 are quite likely to be
created by a benzene-ring". Unfortunately, the qualitative analysis was hardly applied to
these systems since it can not be used in usual ways. In contrast, our system can successfully
use the qualitative analysis like spectroscopists. The way of using it is the method proposed
in this paper. As a result, our system is applicable to all compounds which exhibit high
performance with respect to correctness.
5.6.2 Systems Based on Fuzzy Logic

Since spectral data are always inaccurate, and the representation of spectroscopic knowledge
is quite like that in fuzzy logic, some systems naturally use fuzzy logic or some techniques
similar to fuzzy logic (Clerc, Pretsch, & Zurcher, 1986). In these systems, fuzzy intervals
which are similar to the regions described in Section 5.6.1 are given for reference values,
and memberships of inaccurate data are calculated on the basis of the degrees that the
inaccurate data are in the fuzzy intervals. These systems are better than those described in
Section 5.6.1 in some cases, but the degrees that inaccurate data are in fuzzy intervals do
not necessarily reect the possibility of the inaccurate data being the reference values. For
example, in Figure 7, it is dicult to determine which peak is closer to the reference value
only by considering the degrees that peak a and peak b are in the fuzzy interval.
peak a

peak b

reference value
(fuzzy interval)

Figure 7: Two peaks in a fuzzy interval
However, by applying the method proposed in this paper, the above problem can be
easily solved. As we discussed in Section 5.6.1, in practice spectroscopists also frequently
use knowledge about correlations among peaks in addition to the formalizable spectroscopic
knowledge. This kind of knowledge is essential to our method which enables us to use
qualitative correlations among related data as evidence for the identification of inaccurate
data.
We have compared the fuzzy method used by these systems with our method in Section
5.5. So far as we know, the RC of our system is the highest among the similar systems,
and the RI of our system is higher than that of most of the systems.
5.6.3 Systems Based on Pattern Recognition

Some systems use pattern recognition techniques to interpret infrared spectra (Jalsovszky &
Holly, 1988; Sadtler, 1988), of which Sadtler is the most popular commercial system. The
141

fiZhao & Nishida
system compares known patterns with unknown ones, and determines the possibility of an
unknown pattern being a known one by calculating the quantitative similarity or closeness
between the two patterns.
Unlike fuzzy techniques, pattern recognition considers a group of data (i.e., a pattern)
at the same time. However, pattern recognition is primarily based on quantitative analysis.
We have discussed that in many cases especially when the inaccuracy of spectral data is not
slight, qualitative features of spectral data are much more important than quantitative ones.
For example, Figure 8 shows two simple cases. The difference between the two patterns in
(a) is smaller than that in (b). From the viewpoint of Sadtler, the two patterns in (a)
are closer than those in (b). However, the two patterns in (b) may be the same in some
cases, while the two patterns in (a) may not be the same in any case. The reason is that the
qualitative features (frequency positions of peaks) of the two patterns in (a) are different.
pattern 2
difference

pattern 1

pattern 2
pattern 1

(a)

difference
(b)

Figure 8: Quantitative differences between patterns
Because quantitative similarity and closeness are not always sound, most systems based
on pattern recognition including Sadtler can not give concrete solutions. In general, the
solutions of these systems are only a series of candidates from which users have to finally
decide the possible one by themselves. It is dicult to compare these systems with ours
because the solutions of these systems are quite loose, and neither the RC nor the RI is
available. Sadtler, for example, usually gives the list of all known patterns associated
with the values of quantitative differences between the unknown patterns and these known
ones.
5.6.4 Systems Based on Neural Networks

Recently, neural networks have been applied to infrared spectrum interpreting systems
(Anand, Mehrotra, Mohan, & Ranka, 1991; Robb & Munk, 1990). In Anand's system, a
neural network approach is used to analyze the presence of amino acids in protein molecules.
To this specific classification, the RI of Anand's system is about 87%, and the RC is not
available. In Robb's system, a linear neural network model is developed for interpreting
infrared spectra. The system is for general purpose like our system. Without prior input of
spectrum-structure correlations, the RC of Robb's system is equal to 53.3%.
Although the RC and RI of our system are both higher than those of the two systems,
we still think that using neural networks is very promising, especially when model training
or system learning is a must. The research concerning applying neural networks to our
system is left for the future.
142

fiUsing Qualitative Hypotheses to Identify Inaccurate Data
6. Related Work and Discussion

Identifying inaccurate data has long been regarded as a significant and dicult problem in
AI. Many methods and techniques have been proposed.
Fuzzy logic provides the mathematical fundamentals of representation and calculation
of inaccurate data (Bowen, Lai, & Bahler, 1992; Negoita & Ralescu, 1987; Zadeh, 1978).
Our method is primarily based on fuzzy theory. But compared with conventional fuzzy
techniques, the advantages of our method include: (1) fuzzy intervals of inaccurate data
are dynamically determined so that dynamic information can be used; (2) fuzzy intervals
are based on qualitative features of data and qualitative correlations among related data so
that the solutions are more robust. The limitation of our method is that when qualitative
correlations among related data are not known in advance, the method degenerates to a
conventional fuzzy method. For instance, if SCF is unavailable, the two methods described
in Section 5.5 become the same.
Pattern recognition provides the techniques for interpreting measured data in group
(Jalsovszky & Holly, 1988). By pattern recognition methods, related data and connections
among data can be considered. However, there are two preconditions which must be satisfied
for complex data analysis by pattern recognition to be successful. The first precondition
is that we have to obtain adequate data bases from which we can derive the patterns we
need to recognize, and the second precondition is that we have to demonstrate that there
are suitable metrics of similarity between patterns. When patterns explicitly exist, and
measured patterns are not seriously noisy (e.g., fingerprint recognition), pattern recognition
methods are effective. However, if patterns are not explicit, or patterns change irregularly
which implies that there is not a stable metrics for determining the similarity between
patterns (e.g., spectrum interpretation), our method is more practical and robust.
In identifying inaccurate data, the roles of \di @Rj " and \Rj @M D" are quite similar
to the role of subjective statements or prior probabilities in other systems (Duda, Hart,
& Nilsson, 1976; Shortliffe & Buchanan, 1975). However, the essential difference is that
our method dynamically calculates the values of \di @Rj " and \Rj @MD" from qualitative
correlations among related data so that it does not need many assumptions beforehand,
and can avoid inconsistency in knowledge and data bases. Our method can also handle
possibility propagation among inference networks. Readers may have noticed it from the
process of considering the second kind of related data in spectrum interpretation (see Section
5.4.2).
When statistical samples are sucient, or subjective statements can be consistently obtained, probabilistic reasoning methods can be applied to inaccurate data identification.
When statistical samples of inaccurate data are not enough and consistent subjective statements are not available, our method is very effective.
Our ongoing research related to probabilistic reasoning is to consider the interaction
among identified partial components. As we discussed before, spectroscopists frequently
use the knowledge such as \if C6 H6 coexists with CH3 , then the peaks of CH3 around
2900 cm01 may shift", or \if -C-O-C- has been identified, then the strength of the peaks of
CH3 may change". Therefore, it is possible to update the possibilities of identified partial
components by considering the interaction among them. Using probabilistic reasoning to
analyze the effects among identified partial components would not only help us identify
143

fiZhao & Nishida
inaccurate data, but also provide us with the reason why the data are inaccurate. The
research and experiments will be the subject of our sequel paper.
7. Conclusions

In this paper, we have presented a new method for identifying inaccurate data on the
basis of qualitative correlations among related data. We first introduced a new concept
called support coecient function (SCF ). Then, we proposed an approach to determining
dynamic shift intervals of inaccurate data based on SCF , and an approach to calculating
possibility of identifying inaccurate data, respectively. We also presented an algorithm
for using qualitative correlations among related data as confirmatory or disconfirmatory
evidence for the identification of inaccurate data. We have developed a practical system
for interpreting infrared spectra by applying the proposed method, and have fully tested
the system against several hundred real spectra. The experimental results show that the
proposed method is significantly better than the conventional methods used in many similar
systems. In this paper we have also described the system and the experimental results.
Briey, our novel work includes:
1. A method which assumes an inaccurate data item to be a certain reference value on
the basis of qualitative correlations between the inaccurate data item and all of its
related data.
2. An algorithm which crystallizes the method.
3. A practical system which uses the algorithm to interpret infrared spectra.
Acknowledgments

Thanks to the editors and anonymous reviewers of JAIR for their helpful comments and
suggestions, and to Chunling Sui and Mitchell Bradt for proofreading the manuscript. This
research was partially supported by Horiba Ltd., Kyoto, Japan, and the first author wishes
to thank ASTEM Research Institute, Kyoto, Japan, where he worked as a researcher in
1991 - 1994.
References

Anand, R., Mehrotra, K., Mohan, C. K., & Ranka, S. (1991). Analyzing Images Containing
Multiple Sparse Patterns with Neural Networks. In Proceedings of IJCAI-91, pp. 838-

843.

Bowen, J., Lai, R., & Bahler, D. (1992). Lexical Imprecision in Fuzzy Constraint Networks.
In Proceedings of AAAI-92, pp. 616-621.
Clerc, J. T., Pretsch, E., & Zurcher, M. (1986). Performance Analysis of Infrared Library
Search Systems. Mikrochim. Acta [Wien], II, pp. 217-242.
Colthup, L., Daly, H., & Wiberley, S. E. (1990). Introduction to Infrared and Raman Spectroscopy. Academic Press.
144

fiUsing Qualitative Hypotheses to Identify Inaccurate Data
Dempster, A. P. (1968). A Generalization of Bayesian Inference. Journal of the Royal Sta-

tistical Society, B-30, pp. 205-247.

Duda, R. O., Hart, P. E., & Nilsson, N. J. (1976). Subjective Bayesian Methods for RuleBased Inference Systems. In Proceedings of National Computer Conference, pp. 1075-

1082.

Hasenoehrl, E. J., Perkins, J. H., & Griths, P. R. (1992). Expert System Based on Principal Components Analysis for the Identification of Molecular Structures from VaporPhase Infrared Spectra. Journal of Anal. Chem., 64, pp. 656-663.
Jalsovszky, G. & Holly, G. (1988). Pattern Recognition Applied to Vapour-Phase Infrared
Spectra: Characteristics of vOH Bands. Journal of Molecular Structure, 175, pp.

263-270.

Negoita, C. V. & Ralescu, D. (1987). Simulation, Knowledge-Based Computing, and Fuzzy
Statistics. Van Nostrand Reinhold Company.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers.
Puskar, M. A., Levine, S. P., & Lowry, S. R. (1986). Computerized Infrared Spectral Identification of Compounds Frequently Found at Hazardous Waste Sites. Journal of Anal.

Chem., 58, pp. 1156-1162.

Reiter, R. (1987). A Theory of Diagnosis From First Principles. Artificial Intelligence, (87)

32, pp. 57-95.

Robb, E. W. & Munk, M. E. (1990). A Neural Network Approach to Infrared Spectrum
Interpretation. Mikrochim. Acta [Wien], I, pp. 131-155.
Sadtler Research Laboratories. (1988). Sadtler PC Spectral Search Libraries, Product Introduction & User's Manual.
Shafer, G. (1976). A Mathematical Theory of Evidence. Princeton Uni. Press.
Shortliffe, E. H. & Buchanan, B. G. (1975). A Model of Inexact Reasoning in Medicine.

Mathematical Biosciences, 23, pp. 351-379.

Wythoff, B. J., Buck, C. F., & Tomellini, S. A. (1989). Descriptive Interactive ComputerAssisted Interpretation of Infrared Spectra. Analytica Chimica Acta, 217, pp. 203-216.
Zadeh, L. A. (1978). Fuzzy Set as a Basis for a Theory of Possibility. Fuzzy Sets Syst., 1,

pp. 3-28.

Zhao, Q. (1994). An Ecient Method of Solving Constraint Satisfaction Problems in IR
Spectrum Interpretation. In Proceedings of the 2nd International Conference on Expert

Systems for Development, pp. 165-170.

Zhao, Q. & Nishida, T. (1994). A Knowledge Model for Infrared Spectrum Processing. In

Proceedings of the International Symposium on Information Theory and Its Applications, pp. 781-786.
145

fiJournal of Artificial Intelligence Research 3 (1995) 25-52

Submitted 8/94; published 6/95

FLECS: Planning with a Flexible Commitment Strategy
Manuela Veloso
Peter Stone

Department of Computer Science, Carnegie Mellon University
Pittsburgh, PA 15213-3891 USA

veloso@cs.cmu.edu
pstone@cs.cmu.edu

Abstract
There has been evidence that least-commitment planners can eciently handle planning
problems that involve dicult goal interactions. This evidence has led to the common belief
that delayed-commitment is the \best" possible planning strategy. However, we recently
found evidence that eager-commitment planners can handle a variety of planning problems
more eciently, in particular those with dicult operator choices. Resigned to the futility
of trying to find a universally successful planning strategy, we devised a planner that can
be used to study which domains and problems are best for which planning strategies.
In this article we introduce this new planning algorithm, flecs, which uses a FLExible
Commitment Strategy with respect to plan-step orderings. It is able to use any strategy
from delayed-commitment to eager-commitment. The combination of delayed and eager
operator-ordering commitments allows flecs to take advantage of the benefits of explicitly
using a simulated execution state and reasoning about planning constraints. flecs can vary
its commitment strategy across different problems and domains, and also during the course
of a single planning problem. flecs represents a novel contribution to planning in that it
explicitly provides the choice of which commitment strategy to use while planning. flecs
provides a framework to investigate the mapping from planning domains and problems to
ecient planning strategies.

1. Introduction
General-purpose planning has a long history of research in Artificial Intelligence. Several
different planning algorithms have been developed ranging from the pioneering GPS (Ernst
& Newell, 1969) to a variety of recent algorithms in the SNLP (McAllester & Rosenblitt,
1991) family. At the most basic level, the purpose of planning is to find a sequence of
actions that change an initial state into a state that satisfies a goal statement. Planners use
the actions provided in their domain representations to try to achieve the goal. However
different planners use different means to this end.
Faced with a variety of different planning algorithms, some planning researchers, including these authors, have been increasingly curious to compare different planning methodologies. Although general-purpose planning is known to be undecidable (Chapman, 1987), it
has been a common belief that least-commitment planning is the \best," i.e., the most efficient planning strategy for most planning problems. This belief is based on evidence that
least-commitment planners can eciently handle planning problems that involve dicult
plan step interactions (Barrett & Weld, 1994; Kambhampati, 1994; Minton, Bresina, &
Drummond, 1991). Delayed commitments, in particular to step orderings, allow the plan

c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiVeloso & Stone

steps to remain unordered until the interactions are visible.1 In similar situations, eagercommitment planners may encounter severe eciency problems with early commitments to
incorrect orderings.
Recently we engaged in an investigation of other sorts of planning problems which would
be handled eciently by other planning strategies. Since all planning is driven by heuristics,
we identified different sets of heuristics that correspond to different planning methods. We
designed sets of planning domains and problems to test different planning strategies. While
studying the impact of these different strategies in different kinds of planning problems,
we came across evidence that eager-commitment planners can eciently handle a variety
of planning problems, in particular those with dicult operator choices (Stone, Veloso,
& Blythe, 1994). The up-to-date state allows them to make informed planning choices,
particularly in terms of the operator alternatives available. In similar situations, delayedcommitment planners may need to backtrack over incorrect operator choices (Veloso &
Blythe, 1994). We came to believe that no planner was consistently better than all others
across different domains and problems.
Resigned to the futility of trying to find a universally successful planning strategy, we
felt the need to study which domains and problems were best suited to which planning
methods.2 In order to do so, we devised and implemented a planner that can use any
operator-ordering commitment strategy along the continuum between, on the one extreme
delayed commitment, and on the other, eager commitment. This planner is completely
exible along one dimension of planning heuristics: operator-ordering commitments. Our
main contribution in this paper is to completely describe this planning algorithm and to
put it forth as a tool for studying the mapping between heuristics and domains or problems.
Rather than risking the possibility that the planner itself might get overlooked if it were
relegated to an \architecture" section of a future paper, we present flecs and its underlying
philosophy as a contribution in its own right.
The continuum of heuristics that can be explored by our planning algorithm lies between
the operator-ordering commitment strategies of delayed-commitment and eager-commitment
backward-chaining planners, which we now situate within a broad range of planning and
problem solving methods. One possible planning strategy is to search all the possible states
that can be reached from the initial state to find one that satisfies the goal. This method,
called progression or forward-chaining, can be very impractical. There are often too many
accessible states in the world to eciently search the complete state space. As an alternative, several planners constrain their search by using regression, or backward-chaining.
Rather than considering all possible actions that could be executed in the initial state and
searching recursively forward through the state space, they search backwards from the goal.
Their search is driven by the set of actions that can directly achieve the goal.
There are two main ways of performing backward-chaining. Several planners do regression by searching the space of possible plans. Planners, such as noah, tweak, snlp,
1. Least-commitment planners really delay commitments to plan step orderings and to variable bindings.
Throughout this article we use the term delayed commitment to contrast with eager commitment in the
context of step orderings.
2. Similar concerns regarding different constraint satisfaction algorithms have led recently to the design
of the Multi-Tac architecture (Minton, 1993). This system investigates a given problem to find a
combination of heuristics from a collection of available ones to solve the problem in an ecient way.

26

fiflecs: Planning with a Flexible Commitment Strategy

and their descendants (Chapman, 1987; McAllester & Rosenblitt, 1991; McDermott, 1978;
Sacerdoti, 1977; Tate, 1977; Wilkins, 1984) are plan-space planners that use a delayedcommitment strategy. In particular, they delay the decision of ordering operators as long
as possible. Consequently, the planner reasons from the initial state and from a set of
constraints that are regressed from the goal. On the other hand, planners such as gps,
strips, and the prodigy family (Carbonell, Knoblock, & Minton, 1990; Fikes & Nilsson,
1971; Rosenbloom, Newell, & Laird, 1990) use an eager-commitment strategy.3 They use
backward-chaining to select plan steps relevant to the goals. These eager-commitment planners make explicit use of an internal representation of the state of the world (their internal
state) and order operators when possible so that they can reason from an updated version
of this state. They trade the risk of eager commitment for the benefits of using an explicit
updated planning state.
In this article we introduce a planning algorithm, flecs, that uses a FLExible Commitment Strategy with respect to operator orderings. flecs is designed to provide us and
other planning researchers with a framework to investigate the mapping from domains and
problems to ecient planning strategies. This algorithm represents a novel contribution
to planning in that it introduces explicitly the choice of the commitment strategy. This
ability to change its commitment strategy makes it useful for studying the tradeoffs between
delayed and eager commitments. flecs is a descendant of prodigy4.0 and its current
implementation is directly on top of prodigy4.0. It extends prodigy4.0 by reasoning
explicitly about ordering alternatives and by having the ability to change its commitment
strategy across different problems and domains, and also during the course of a single
planning problem.4
This article gradually introduces flecs. Section 2 gives a top-level view of the algorithm
and describes the different ways in which flecs makes use of a uniquely specified state of
the world. Section 3 introduces the concepts used by the flecs algorithm. We provide
an annotated example to illustrate the details of the planning concepts defined. Section 4
presents flecs's planning algorithm in full detail and explains the algorithm step by step.
We discuss different heuristics to guide flecs's choices, in particular the exible choice of
commitment strategy. We analyze the advantages and disadvantages of delayed and eager
plan step ordering commitments. Section 5 shows specific examples of planning domains and
problems that we devised, which support the need for the use of flecs's exible commitment
strategy. We performed an empirical analysis on planning performance in these domains.
The corresponding empirical results demonstrate the tradeoffs discussed and show evidence
that exible commitment is necessary. Finally Section 6 draws conclusions from this work.
3. Planners in the prodigy family include prodigy2.0 (Minton, Knoblock, Kuokka, Gil, Joseph, & Carbonell, 1989), NoLimit (Veloso, 1989), and prodigy4.0 (Carbonell, Blythe, Etzioni, Gil, Joseph, Kahn,
Knoblock, Minton, Perez, Reilly, Veloso, & Wang, 1992). NoLimit and prodigy4.0, as opposed to
prodigy2.0, do not require the linearity assumption of goal independence and their search spaces are
complete (Fink & Veloso, 1994). They also have some control over their commitment choices as opposed
to the other earlier total-order planners.
4. We found that we needed a new name for our algorithm as flecs represents a significant change in
philosophy and implementation from prodigy4.0.

27

fiVeloso & Stone

2. A Top-Level View of flecs
prodigy4.0 and flecs differ most significantly from other state-of-the-art planning systems

in that they search for a solution to a planning problem by combining backward-chaining (or
regression) and simulation of plan execution (Fink & Veloso, 1994). While back-chaining,
they can commit to a total-ordering of plan steps so as to make use of a uniquely specified
world state. These planners maintain an internal representation of the state and update it by
simulating the execution of operators found relevant to the goal by backward-chaining. Note
that simulating execution while planning differs from interleaving planning and execution,
since the option of \un-simulating," or rolling back, must remain open. Interleaved planning
and execution is generally done by separate modules for planning, monitoring, executing,
and replanning (Ambros-Ingerson & Steel, 1988). flecs can either delay or eagerly carry
out the plan simulation. In this way, our planning algorithm has the exibility of both being
able to delay operator-ordering commitments and being able to use the effects of previously
selected operators to help determine which goals to plan for next and which operators to
use to achieve these goals. In short, it can emulate both delayed-commitment planners and
eager-commitment planners.
Table 1 shows the top-level view of the flecs algorithm.
1. Initialize.
2. Terminate if the goal statement has been satisfied.
3. Compute the pending goals and applicable operators.
 Pending goals are the yet-to-be-achieved preconditions of operators that have been
selected to be in the plan.
 Applicable operators are those that have all their preconditions satisfied in the
current state.
5. Choose to subgoal or apply: (backtrack point)
 To subgoal, go to step 6.
 To apply, go to step 7.
6. Select a pending goal (no backtrack point) and an operator that can achieve it (backtrack point); go to step 3.
7. Change the state as specified by an applicable operator (backtrack point); go to step 2.
Table 1: A top-level view of flecs. The step numbers here are made to correspond with
the step numbers in the detailed version of the algorithm presented in Table 2
(Section 4), which refines these steps and adds an additional necessary step 4.
All the terms used in this table are fully described along with the detailed version of the
algorithm in Section 4. In this section we focus on two main characteristics of this algorithm,
namely its use of an internal state and its exibility with respect to commitment strategies.
28

fiflecs: Planning with a Flexible Commitment Strategy

2.1 The Use of a Simulated Planning State
flecs uses its internal state for at least four purposes. First, it terminates when every goal

from the given problem is satisfied in the current version of the state (the current state):
at this point, a complete plan (the sequence of operators that transformed the initial state
into the current state) has been created and the planning process can stop. Second, in every
cycle, the algorithm uses the internal state to determine which goals need to be planned for
and which have already been achieved by following a means-ends analysis strategy. Unlike
some other planners which analyze all of the possible effects of the operators that may have
changed the initial state, flecs simply checks if a particular goal is true in the current
state.5 Third, the planner uses the state to determine which operators may now be applied:
i.e., those whose preconditions are all true in the state. Fourth, flecs can use its state to
choose an operator and bindings that are most likely to achieve a particular goal with a
minimum of planning effort (Blythe & Veloso, 1992). In summary, and with reference to
the algorithm in Table 1, flecs uses the state to determine:






if the goal statement has been satisfied (step 2);
which goals still need to be achieved (step 3);
which operators are applicable (step 3);
which operators to try first while planning (step 6).

In planners that do not keep an internal state, all four of these steps require considerable
planning effort when they are even attempted at all. In contrast, flecs can perform
these steps in sub-quadratic time. Furthermore, other planners do not have any particular
methods for choosing among possible operators to achieve a goal. This particular use of
state has been shown to provide significant eciency gains in prodigy4.0 (Veloso & Blythe,
1994).
Since flecs does use the state, it makes a big difference whether or not it chooses to
change its state (apply an operator) at a given time. The advantage of applying an operator
is that more informed planning results during each of the above four steps. However, the
choice to apply an operator involves a commitment to order this operator before all other operators that have not yet been applied. This commitment is only temporary since if no plan
can be found with the operator in this position, the operator can be \un-applied" by simply
changing the internal state back to its previous status. One may argue that the requirement that operators be applied in an explicit order opens up the possibility of exponential
backtracking. However this argument is vacuous, as planning is undecidable (Chapman,
1987). Due to the use of state, flecs can reduce the likelihood of requiring backtracking at
the operator choice point. In so doing, it may increase the likelihood of backtracking at the
operator-ordering choice point. However, it has the exibility of being able to come down
on either side of this tradeoff.
5. Note that since the goal and the state are fully instantiated, this matching can be accomplished in
constant time for each goal by using a hash table of literals.

29

fiVeloso & Stone

2.2 The Choice of Commitment Strategies

In order to control the tradeoff between eager and delayed state changes, flecs has a
toggle which determines whether the algorithm prefers subgoaling or applying an operator
in step 5. Which option flecs considers first may affect its path through the search space
and consequently its planning eciency. This ability to accommodate different types of
search is the most novel part of our algorithm. Its significance lies in the difference between
subgoaling and applying.
The difference between subgoaling and applying is illustrated in Figure 1. Subgoaling
can be best understood as regressing one goal, or backward chaining, using means-ends
analysis. It includes the choices of a goal to plan for and an operator to achieve this goal.
As seen in Section 2.1, both of these choices are affected by flecs's internal state. Thus,
subgoaling without ever updating the internal state (applying an operator) can lead to
uninformed planning decisions. On the other hand, by subgoaling extensively, flecs can
select a large set of operators that will appear in the plan before deciding in which order
to apply them. Then flecs takes into account the conicts, or \threats," among operators
and orders them appropriately when applying them.
x
I

s

C

z

G

s

x

y

x
I

s

z

C
t

I

G

y

C

y

z

G

Subgoaling

Applying

Operator t achieves a precondition of
operator y that is not true in state C.

All preconditions of operator x are true in state C.
Applying x changes the state to C.

Figure 1: This diagram from (Fink & Veloso, 1994) illustrates the difference between subgoaling and applying. A search node consisting of a \head-plan" and a \tailplan." The head-plan contains operators that have already been applied and
have changed the initial state \I" to the current state \C." The tail-plan consists
of operators that have been selected to achieve goals in the goal statement \G"
and operators that have been selected to achieve preconditions of these operators,
etc. The figure shows how the planner could either subgoal or apply at a given
search node.
Applying an operator is flecs's way of changing the current internal state so that
future subgoaling decisions can be more informed. However, applying an operator is a commitment (temporary since backtracking is possible) that this operator should be executed
30

fiflecs: Planning with a Flexible Commitment Strategy

before any other. This is the essential tradeoff between eagerly subgoaling and eagerly
applying: eagerly subgoaling delays ordering commitments (delayed commitment), while
eagerly applying facilitates more informed subgoaling (eager commitment).
flecs has a switch (toggle) that can change its behavior from eager subgoaling to
eager applying and vice versa at any time. This feature is the most significant improvement in flecs over prodigy4.0 and its predecessors. Since we saw evidence that neither delayed-commitment nor eager-commitment search strategies were consistently effective (Stone et al., 1994), we felt the need to provide flecs with the toggle. Thus, flecs
can combine the advantages of delayed commitments and eager commitments.6

3. An Illustrative Example

In this section we present an example that illustrates in detail most of the planning situations
that can arise in a general planning problem. Although planning may be well understood in
general, past descriptions of planning algorithms have not directly addressed most of these
situations in full detail. The flecs algorithm is designed to handle all of these situations.
In order to describe flecs completely, we need to define several variables that are
maintained as the algorithm proceeds. Since it is much easier to understand the algorithm
once one is familiar with the concepts that these variables denote, we present an annotated
example in Figures 2 through 9 before formally presenting flecs. We further recommend
following how each of the variables and functions C , G , P , O, A, a, and c change throughout
the annotated example, according to their definitions:
 C represents the current internal state of the planner. Its uses are summarized in
Section 2.1.
 G is the set of goals and subgoals that the planner is aiming to achieve. These are the
goals that are on the fringe of the subgoal tree. Goals in G may be goals that have not
yet been planned for, or goals that have been achieved (perhaps trivially) but not yet
used by the operator that needs them as one of its preconditions (i.e., this operator
has not been applied yet).
 P is the set of pending goals: goals in G that may need to be planned for in the current
state.
 O stands for the set of instantiated operators that have been selected to achieve goals
and subgoals.
 A is the set of applicable operators: operators in O whose preconditions are all satisfied
in the current state and which are needed in the current state to achieve some goal.
 For a goal G, a(G) is the set of its ancestor goal sets { the sequences of goals that
caused G to become a member of G . Trivially, a goal is an ancestor of each of the
preconditions of the operator selected to achieve this goal. a(G) is a set of sets because
G can have different sets of ancestors. This concept will become clearer through the
example.
6. In Section 5 we discuss different heuristics to guide this choice and we discuss our view of toggle as a
perfect focus for learning.

31

fiVeloso & Stone

 For an operator O, c(O) is the set of goals which O was selected to achieve { its causes.
Applying O establishes each member of c(O). As illustrated below, the functions a
and c are needed to determine which goals are pending and which operators are
applicable. They are analogous to causal links used to determine threats in other
planners (Chapman, 1987; McAllester & Rosenblitt, 1991).

The sequence of planning decisions in this example (Figure 2 through Figure 9) is designed to illustrate the uses of all of flecs's variables and functions. We recommend
becoming familiar with them by spending some time carefully tracing their values and returning to the above definitions throughout this example. Note that the figures show only
the tail-plan while we mention applied operators and state changes in the text. Goals are
in circles: solid circles if they are not true and dashed circles if they are true in the current
state. Operators are in boxes with arrows pointing to the goals which they \produce,"
i.e., the goals which the operators have been selected to achieve (their causes). In turn,
the preconditions of these operators are goals with arrows pointing to the operators which
\consume" them. Operators that are applicable in the current state appear in bold boxes.
Changes to the functions c and a are underlined in the captions.
We present now the example. Figure 2 shows the initial planning situation, in which we
consider a planning problem with three literals in the goal statement, G1 , G2, and G3 , i.e.,
G = fG1; G2; G3g. There is one literal in the initial state, G7, i.e., C = fG7g. As none of
the goals is true in the initial state, P = G . There are no operators selected, i.e., O = ;,
and therefore also no operators applicable, i.e., A = ;. At this point, since they are all
top-level goals, none of the goals has any ancestors: a(G1) = a(G2) = a(G3) = ;. As there
are no applicable operators, the next step must be to subgoal on one of the pending goals.

C = fG7g
G = fG1; G2; G3g
O=;
P = fG1; G2; G3g
A=;

G

G

G

1

2

3

Figure 2: An example: The initial specification of a planning situation.
Figure 3 shows the planning situation after flecs subgoals on G1 and G2 . Suppose that
operator O1 , with preconditions G6 and G7 , is selected to achieve G1, while O2 is chosen
to achieve G2 as indicated below. Note that the operators' preconditions replace their
causes in the set of fringe goals G ; since G7 is true in the current state, it is NOT included
in the set of pending goals P . Here G1 is the cause of O1, so c(O1) = fG1g; similarly,
32

fiflecs: Planning with a Flexible Commitment Strategy

c(O2) = fG2g. The new goals all have nonempty ancestor sets: a(G6) = a(G7) = ffG1gg,
and a(G4) = ffG2gg. There are still no applicable operators: O1 cannot be applied because
G6 62 C and O2 cannot be applied because G4 62 C . Therefore, flecs subgoals again.

C = fG7g
G = fG3; G6; G7; G4g
O = fO1; O2g
P = fG3; G6; G4g
A=;
G

G

G

7

6

4

O

O

1

2

G

G

G

1

2

3

Figure 3: Resulting planning situation after subgoaling on G1 and G2 .
Figure 4 shows the planning situation after flecs subgoals on G3 . Suppose that the
operator selected to achieve G3 has preconditions G4 and G5 . We now have c(O3) = fG3 g,
and a(G5) = ffG3gg. The causes of operators O1 and O2 do not change, so c(O1) = fG1g
and c(O2) = fG2g as in the previous step. Similarly, a(G6 ) and a(G7) remain unchanged.
However, G4 now has two sets of ancestor goals: a(G4) = ffG2g; fG3gg. To understand
the need to keep both ancestor sets, consider the possibility that G2 could be achieved
unexpectedly as a side-effect of some unrelated operator instead of being achieved by O2
as planned for. In this case, G4 would remain a pending goal since it would be needed to
achieve G3. Again, since there are no applicable operators, flecs must subgoal on one of
the pending goals, i.e., G6, G4, or G5.

C = fG7g
G = fG6; G7; G4; G5g
O = fO1; O2; O3g
P = fG6; G4; G5g
A=;
G

7

G

G

G

6

4

5

O

O

O

1

2

3

Figure 4: Resulting planning situation after subgoaling on G3 .
33

G

G

G

1

2

3

fiVeloso & Stone

Figure 5 shows the planning situation after flecs subgoals on G4 . Suppose that O4
| an operator with precondition G7 | is selected to achieve G4 . Since G7 is true in the
current state, O4 is our first applicable operator. Note that it is necessarily ordered before
O2 and O3 since its cause is a precondition of these operators. As usual, the cause of the
new operator is stored: c(O4) = fG4 g. In addition, the ancestors of G7 must be augmented
to include two new ancestor sets: a(G7) = ffG1g; fG4; G2g; fG4; G3gg. Although there is
now an applicable operator, let us assume that flecs chooses to delay its commitment to
order O4 as the first step in the plan and subgoals again on a pending goal.

C = fG7g
G = fG6; G7; G5g
O = fO1; O2; O3; O4g
P = fG6; G5g
A = fO4g
G

7

G

O

4

G

G

6

4

5

O

O

O

G

1

G

2

G

3

1

2

3

Figure 5: Resulting planning situation after subgoaling on G4 .
Figure 6 shows the planning situation after flecs subgoals on G5 . Suppose that operator O4 can also achieve G5 and that it is selected to do so. We now need to update both
the causes of this operator and the ancestors of its precondition: c(O4) = fG4; G5g and
a(G7) = ffG1g; fG4; G2g, fG4; G3g; fG5; G3gg. Now rather than subgoaling on the last remaining pending goal (G6), let us apply O4 . Note that this decision corresponds to an early
commitment in terms of ordering the operators O1 , O4, and any operators later selected to
achieve G6 which are unordered by the current planning constraints. flecs changes here
from its delayed-commitment strategy to an eager-commitment strategy.
C = fG7g
G = fG6; G7g
G
6
G
O
O = fO1; O2; O3; O4g
1
1
P = fG6g
A = fO4g
G

7

O

4

G

G

4

5

O

O

2

3

G

G

Figure 6: Resulting planning situation after subgoaling on G5 .
34

2

3

fiflecs: Planning with a Flexible Commitment Strategy

Figure 7 shows the planning situation after flecs applied O4 . Since operator O4 was
applied in order to achieve goals G4 and G5, they are both true in the current state and
back on the fringe of the goal tree, i.e., they are in C and G . Notice that they stay in G
until eventually they have been \consumed" by O2 and O3. However, since they are true
in the current state, they are not pending goals. Since G7 is once again the precondition
of only one selected operator, a(G7) = ffG1gg as before. O2 and O3 are now applicable
as their preconditions are all true in the current state thanks to O4. Let us assume that
flecs maintains the eager-commitment strategy and continues applying applicable operators. flecs orders O2 before O3, since O3 deletes a precondition of O2 (effects are not
shown).
C = fG7; G4; G5g
G = fG6; G7; G4; G5g
G
O = fO1; O2; O3g
O
6
G
1
1
P = fG6g
A = fO2; O3g
G

7

G

G

4

5

O

O

2

3

G

G

2

3

Figure 7: Resulting planning situation after applying O4 from Figure 6.
Figure 8 shows the planning situation after flecs applied O2. Suppose that, although
it was not selected to do so, operator O2 achieves G1 as a side-effect. Perhaps O2 has a
conditional effect that was not visible to the planner, or perhaps O1 simply looked more
promising than O2 as an operator to achieve G1 at the time when it was selected. In any

C = fG7; G4; G5; G1; G2g
G = fG6; G7; G4; G5; G2g
O = fO1; O3g
P=;
A = fO3g
G

7

G

G

G

6

O

1

G

4

5

G

O

3

G

1

2

3

Figure 8: Resulting planning situation after applying O2 from Figure 7.
35

fiVeloso & Stone

case, G1 is now in C and the planning done for it is no longer needed: G6 is no longer a
pending goal, since its sole ancestor is already in C . This fortuitous achievement of a goal
is the reason that we need to use the functions c and a to adjust the sets of pending goals
P and applicable operators A: it would be wasted effort for flecs to plan to achieve G6.
Note that were G6 a precondition of O3 as well as O1, it would be a pending goal since it
would still be relevant to achieving G3 . At this point, only the ancestors of G4 must be
reset: a(G4) = ffG3gg. Since there are no more pending goals, flecs must now apply the
last remaining applicable operator, O3.
Figure 9 shows the final planning situation after flecs applied O3 . At this point all of
the top level goals are true in the current state. Despite the fact that some of the planning
tree remains, flecs recognizes that there is no more work to be done and terminates. The
final plan is O4, O2, O3 , which is the sequence of operators applied in the head-plan (not
shown) corresponding to the steps in Figures 7, 8, and 9. An a posteriori algorithm (Veloso,
Perez, & Carbonell, 1990) can convert the sequence into a partially ordered plan capturing
the dependencies: O4 ; fO2; O3g.

C = fG7; G4; G5; G1; G2; G3g
G = fG6; G7; G2; G3g
O = fO1g
P=;
A=;
G

G

6

O

1

G

G

7

G

1

2

3

Figure 9: Final planning situation after applying O3 from Figure 8.

4. FLECS: The Detailed Description
Aside from the variables and functions introduced in the preceding section, we need to
define only four more things before presenting the complete algorithm. First, Initial State
and Goal Statement are the corresponding ground literals from the problem definition. Second, for a given operator O, pre(O), add(O), and del(O) are its instantiated preconditions,
add-list, and delete-list respectively. flecs takes these values straight from the domain representation, which may include disjunctions, negations, existentially and universally quantified preconditions and effects, and conditional effects (Carbonell et al., 1992). When O
has conditional effects, add(O) and del(O) are determined dynamically, using the state at
the time O is applied. Third, the \relevant instantiated operators that could achieve G"
(step 6) are all the instantiated operators O (operators with fully-specified bindings) which
36

fiflecs: Planning with a Flexible Commitment Strategy

have G 2 add(O) if G is a positive goal or G 2 del(O) if G is a negative goal. Fourth, toggle
is a variable that determines the avor of search, as described later.

4.1 The Planning Algorithm
We present the flecs planning algorithm in full detail in Table 2.7 While examining the algorithm, notice that the fringe goals G , the selected operators O, the ancestor function a(G),
the cause function c(O), and the current state C are maintained incrementally. On the other
hand, the pending goals P , the applicable operators A, and toggle are recomputed on every
pass through the algorithm.
Step 1 initializes most of these variables. At the beginning of the planning process, the
only goals in G are those in the goal statement, the current state C is the same as the initial
state, and since no operators have yet been selected, O is empty. Both the ancestor function
a and the cause function c are initialized to the constant function that maps everything to ;.
In practice, the domain of a is the set of goals and the domain of c is the set of operators
that appear in the problem. However, since most of these goals and all of these operators
have not been determined when the algorithm is first called, we must initialize the functions
with unrestricted domains.
Step 2 is the termination condition. It is called after each time a new operator is
applied. The algorithm terminates successfully if every goal G in the goal statement is true,
or satisfied, in the current state C , i.e., G 2 C .
In step 3, the sets of pending goals and applicable operators are computed based on the
current state. Pending goals are the goals that the planner may need to plan for. Initially,
the pending goals are the fringe goals that are not currently true or that were true in the
initial state.8 The applicable operators are the selected operators whose preconditions are
true in the state.
Then, step 4 computes the pending goals P and applicable operators A that are active
in the current state. A pending goal is active as long as it is on the fringe of the subgoal tree
and it still needs to be planned for. A goal is no longer active if every one of its ancestor sets
has at least one goal that has already been achieved: then all purposes for which the goal
was selected no longer exist (as was the case for G6 in Figure 8). An applicable operator is
active in the current state as long as it would achieve a goal that is still useful to the plan.
An applicable operator is no longer active if each of its causes is either true in the current
state or no longer active.
Step 5 is the most novel part of our algorithm. It allows for a exible search strategy
within a single planning algorithm. Since at this step, flecs has not yet terminated, there
must be either some active pending goals or active applicable operators, i.e., P or A must
be non-empty. However, if there is only one or the other, then there is no choice to be
made. If, on the other hand, both P and A are non-empty, then we can either proceed to
step 6 or to step 7. For the sake of completeness, we must keep both options open; but
which option flecs considers first may affect the amount of search required. By changing
7. The detail of this algorithm allows the reader to carefully study and re-implement flecs.
8. Since the planner cannot backtrack beyond the initial state, we must keep goals from the initial state as
pending goals for the sake of completeness.

37

fiVeloso & Stone

1. Initialize:
C : current state
a. G = Goal Statement.
G : fringe goals
b. C = Initial State.
P : pending goals
c. O = ;.
O
: instantiated operators
d. 8G:a(G) = ;.
A
: applicable operators
e. 8O:c(O) = ;.
a: ancestor goal sets
c: causes
2. Terminate if Goal Statement  C .
3. Compute applicable operators A and pending goals P :
a. P = fG 2 G j G 62 C _ G 2 Initial Stateg.
b. A = fA 2 O j pre(A)  Cg.
4. Adjust P and A to contain only active members:
a. P = P , fP 2 P j 8S 2 a(P ):9G 2 S s.t. G 2 Cg.
b. A = A , fA 2 A j 8G 2 c(A):[(G 2 C ) _ (8S 2 a(G):9G 2 S s.t. G 2 C )]g.
5. Subgoal or Apply:
a. Set or reset toggle to sub or app, i.e. Set default to delayed or eager commitment.
b. If A = ;, go to step 6.
c. If P = ;, go to step 7.
d. Choose to apply or to subgoal (backtrack point):
 If toggle = sub ^ P 6 C , subgoal first: go to step 6.
 If toggle = app, apply first: go to step 7.
6. Choose a goal P from P (not a backtrack point).
 Choose a goal not true in the Current State using means-ends analysis.
a. Get the set R of relevant instantiated operators that could achieve P .
b. If R = ; then
i. P = P , fP g.
ii. If P = ; then fail (i.e., backtrack).
iii. Go to step 6.
c. Choose an operator O from R (backtrack point).
 Choose the operator with minimum conspiracy number, i.e. the operator which
appears to be achievable with the least amount of planning.
d. O = O [ fOg.
e. G = (G , fP g) [ pre(O).
f. c(O) = c(O) [ fP g.
g. 8G 2 pre(O):a(G) = a(G) [ ffP g [ S j S 2 a(P )g.
h. Go to step 3.
7. Choose an operator A from A (backtrack point for interactions).
 Use a heuristic to find operators with fewer interactions { similar to the one used by
the SABA heuristic.
a. Apply A: C = (C [ add(A)) , del(A)
b. O = O , fAg.
c. 8G 2 pre(A):a(G) = a(G) , fS 2 a(G) j S \ c(A) 6= ;g.
d. G = (G [ c(A)) , fG 2 pre(A) j a(G) = ;g.
e. c(A) = ;.
f. Go to step 2.
0

Table 2: The full description of flecs.
38

0

fiflecs: Planning with a Flexible Commitment Strategy

the value of toggle, which can be done on any pass through the loop, flecs can change the
type of search as it works on a problem.
Each pass through the body of the algorithm visits either step 6 or step 7. When
subgoaling (step 6), an active pending goal P is chosen from P . Note that unlike the
corresponding choice in step 7, this choice of subgoals is not a backtrack point. However, if
there are no operators that could achieve this goal, then another goal is chosen (step 6b).
Means-ends analysis is used as a heuristic to prefer subgoaling on goals that are not currently
true. Next, an operator O is chosen that could achieve the chosen goal (step 6c). It can
either be a new operator or an existing one as in Figure 6 (O4, which had already been
selected to achieve G4, is also selected to achieve G5). The choice of operator is a backtrack
point. Unless some other heuristic is provided, the minimum conspiracy number heuristic
is used to determine which operator should be tried first (Blythe & Veloso, 1992). In short,
this heuristic selects the instantiated operator that appears to be achievable with the least
amount of planning.
Before returning to the top of the loop, all of the affected variables are updated. First,
O is added to O using set union so that the same operator never appears twice (step 6d).
Second, O's preconditions are added to G , while P is removed (step 6e): once P has an
operator selected to achieve it, it is no longer on the fringe of the subgoal tree. Third, the
cause of O is augmented to include P (step 6f). Fourth, the ancestor sets of O's preconditions
are augmented to include all sets of goals comprised of P and its ancestors (step 6g). As
explained in Figure 4, all ancestor sets must be included. Finally, since the state is not
changed at all, the termination condition cannot be met. The algorithm returns to step 3.
When applying an operator (step 7), an applicable operator A is chosen from A.
A heuristic that analyzes the applicable operators can be used to choose the best possible operator. One such heuristic analyzes interactions between operators by identifying
negative threats, similarly to the saba heuristic in (Stone et al., 1994). In short, this
heuristic prefers operators that do not delete any preconditions of, and whose effects are
not deleted by, other operators. This choice of an applicable operator is a backtrack point
where all orderings of interacting applicable operators are considered. Different orderings
of completely independent operators need not be considered. Completely independent operators are those with interactions neither between themselves nor among their ancestor
sets. Since the application of one such operator can make no difference to the application
of another, we only need to consider one ordering of these operators.
Once A is chosen, it is promptly applied (step 7a). This application involves changing the
current state as prescribed by A. Note that if A has conditional effects, they are expanded at
this point. Next, the relevant variables are updated. First, updating involves removing A
from the set of selected operators (step 7b). Second, the ancestors of A's preconditions
are only those ancestor sets which did not include A (step 7c): A does not need further
planning. Figure 7 shows an example in which a precondition (G7) does still have an
ancestor remaining. Third, since A has been applied, its preconditions that are not goals
for any other reason are no longer on the fringe, but its causes are (step 7d): if they are
unachieved they must be re-achieved. Fourth, in case A is ever selected again as an operator
to achieve some goal, c(A) is reset to ; (step 7e). Finally, since the current state has been
altered, the algorithm returns to step 2 where the termination condition is checked.
39

fiVeloso & Stone

4.2 Discussion: Backtracking, Heuristics, and Properties
One should pay close attention to the placement of backtrack points in the algorithm. In
particular, there are only three: the subgoal/apply choice in step 5, the choice of operator
to achieve a goal in step 6, and the choice of applicable operator in step 7. However, the
choice of goal on which to subgoal in step 6, which is a backtrack point in the prodigy
algorithm, is not a backtrack point here. flecs does not need this backtrack point because
the choice to apply or not to apply an operator at a given time is left open in step 5 and
all significantly different orders of applying applicable operators are considered in step 7.
As explained in the previous subsection, different orderings of completely independent operators are not considered. Nevertheless, all orderings that could lead to a solution are
considered. Therefore, backtracking on the choice of subgoal would only cause redundant
search. This elimination of a backtrack point is a significant improvement in flecs over
previous implementations, namely NoLimit and prodigy4.0. Note that no new backtrack
points are added to offset the eliminated backtrack point.
flecs's only explicit failure point is in step 6 and occurs when the algorithm has chosen
to subgoal, but none of the pending goals have any relevant operators. All other failures
are implicit. That is, at a backtrack point, if all choices have been unsuccessfully tried
then the algorithm backtracks. As presented, the algorithm only terminates unsuccessfully
if the entire search space has been exhausted. Other causes for failure, such as goal loops,
state loops, depth bounds, and time limits, are incorporated in the same manner as in
prodigy4.0 (Carbonell et al., 1992).
At each choice point, there is some heuristic to determine which branch to try (first). In
step 6, the goal is chosen using means-ends analysis, and the operator with the minimum
conspiracy number is chosen to achieve that goal. In step 7, the choice mechanism from
the saba heuristic is used to determine which applicable operator to try first. In step 5,
toggle, which can be changed at any time, determines whether the default commitment
strategy should be eager subgoaling or eager applying. Note that if all of the pending goals
are true in the Current State (or if there are no pending goals), the planner may apply an
applicable operator regardless of the value of toggle. Similarly, if there are no applicable
operators, the planner must subgoal even if toggle indicates to prefer applying. toggle is a
new variable to guide heuristic search at an existing choice point with a branching factor of
two: it does not represent the addition of a new backtrack point. As discussed throughout,
it provides flecs with the ability to change its commitment strategy. As suggested by its
name, toggle can be one of two values: sub and app indicating eager subgoaling and eager
applying respectively.
Here we describe a domain-independent heuristic that could be used to guide changes to
the value of toggle. Such a heuristic should allow eager commitments when there is reason to
believe that there will not be a need to backtrack over the resulting operator linearization.
In this case, setting toggle to app will increase the planning eciency by converting a
partially-ordered set of operators into a sequence that leads to a single possible state, which
can then be used to guide subsequent planning. This process is equivalent to starting a new
and smaller planning problem as all the previous choices will be embedded in the state.
The situation described above is similar to that which arises in the alpine system which
constructs ecient abstraction hierarchies (Knoblock, 1994). alpine can guarantee that
40

fiflecs: Planning with a Flexible Commitment Strategy

planning hierarchically using its generated abstraction hierarchies will not lead to backtracking across refinement spaces. Figure 10 illustrates how flecs can use this abstraction
planning information to control the value of toggle. If toggle changes to app when a particular abstract planning step is completely refined and the abstraction hierarchies preserve
alpine's ordered monotonicity property, then there should be no need to backtrack over the
resulting operator ordering. Then toggle can change back to sub, and flecs can continue
planning with updated state information.
Abstraction level
1. Begin with
toggle=sub.
S0

3. Continue planning:
toggle=sub.

Build a partial
order plan for the
first step in the
abstract plan

S0

5. Continue
until done...

S1
2. Set toggle=app.
Commit to an ordering and
compute the new state.

S1

S2
4. After another
step in the abstract
plan, commit again:
toggle=app.

Figure 10: Using abstraction information to guide changes to toggle.
The abstraction-driven heuristic is one method for exploiting this choice point. Similarly,
the minimum conspiracy number heuristic and the saba heuristic are not the only ways
to guide the choices of instantiated operator and applicable operator respectively. The
heuristics used can always be changed, and we do not claim that the ones we provide as
defaults are the best possible: no heuristic will work all the time.
The planning algorithm we present is both sound and complete if it searches the entire
search space, using a technique such as iterative deepening (Korf, 1985). flecs is sound
because it only terminates when it has reached the goal statement as a result of applying
operators. That is, the application of the operator sequence returned as the final plan has
been entirely simulated by the time the planner terminates. Thus the preconditions of each
operator will all be true at the time the operator is executed, and after all operators have
been executed, the goal statement will be satisfied. Consequently, flecs is sound.
Since no step in the algorithm prunes any of the search space, flecs with an iteratively
increasing depth bound is also complete: if there is a solution to a planning problem, flecs
will find one. To insure this property, we need only show that flecs can consider all possible
operators that may achieve a goal as well as all orderings of interacting applicable operators.
flecs does so by maintaining backtracking points at the choice of operator (step 6c) and
at both points at which the operator ordering could be affected: the choice of applicable
operator itself (step 7) and the choice of whether to subgoal or apply (step 5d). Selecting
41

fiVeloso & Stone

\apply" commits to ordering all operators that are not currently applicable after at least
one of the currently applicable operators. Note that completeness is achieved even without
maintaining the choice of goals to subgoal on as a backtrack point (step 6), since regardless
of the order in which the operators are chosen, they are applied according to their possible
interactions (i.e., similarly to resolving negative threats). Thus flecs's search space is
significantly reduced from that of prodigy4.0, while still preserving completeness. (See
Appendix A for formal proofs of flecs's soundness and completeness.)

5. Empirical Analysis of Heuristics to Control the Commitment Strategy

As we have seen, flecs introduces the notion of a exible choice point between delayed and
eager operator-ordering commitments. To appreciate the need for this exibility, consider
the two extreme heuristics: always eagerly subgoaling (delaying commitment) and always
eagerly applying (eager commitment). The former heuristic chooses to subgoal as long as
there is at least one active pending goal (Subgoal Always Before Applying or saba); the
latter chooses to apply as long as there are any active applicable operators (Subgoal After
eVery Try to Apply or savta). In this section we show empirical results that demonstrate
that both of these extremes can lead to highly sub-optimal search in particular domains.
Indeed, we believe that no single domain-independent search heuristic can perform well
in all domains (Stone et al., 1994). It is for this reason that we have equipped flecs
with the ability to use either extreme domain-independent heuristic or any more moderate
heuristic \in between" the two: during every iteration through our algorithm, there is an
opportunity to change from eagerly subgoaling to eagerly applying or vice versa. One could
define different heuristics to guide this choice, or one could leave the choice up to the user
interactively.
This exibility in search method provides our algorithm with the ability to search sensibly in a wide variety of domains. Any algorithm that is not so exible is susceptible to
coming across domains which it cannot handle eciently (Barrett & Weld, 1994; Veloso &
Blythe, 1994; Kambhampati, 1994). flecs's exibility makes it possible to study which
heuristics work best in which situations. In addition, this exible choice is a perfect learning
opportunity. Since no single search method will solve all planning problems, we will use
learning techniques to help us determine from experience which search strategies to try.
To illustrate the need for different search strategies, we provide one real world situation
in which eagerly subgoaling leads directly to the optimal solution, one in which eagerly
applying does so, and one in which an intermediate policy is best. These examples are not
intended to be an exhaustive demonstration of flecs's capabilities. Rather, our examples
are intended to illustrate the need to consider problems other than traditional goal ordering
problems and to motivate the potential impact of flecs.

5.1 Eagerly Subgoaling Can Be Better

First, consider the class of tasks in which the following is true: all operators are initially
executable, but they must be performed in a specific order because each operator deletes
the preconditions of the operators that were supposed to be executed earlier. For instance,
suppose that there is a single paint brush and several objects which need to be painted
different colors. The paint brush can be washed fairly well, but it never comes completely
42

fiflecs: Planning with a Flexible Commitment Strategy

clean. For this reason, if we ever use a lighter paint after a darker paint, some of the darker
paint will show up on the painted object and our whole project will be ruined. Perhaps the
shade of red is darker than the shade of green. Then to paint a chair with a red seat and
green legs, we had better paint the legs first.
Consider a range of colors ordered from light to dark: white, yellow, green, : : : , and
black. Initially, we could paint an object any color. However, if we start by painting
something black, then no other paint can be used. In order to represent this situation to a
planner, we created a domain with the operators shown in Table 3.
Operator:
preconds:
adds:
deletes:

paint-white <obj>

(usable white)
(white <obj>)

paint-yellow <obj>

(usable yellow)
(yellow <obj>)
(usable white)






..
.

paint-black <obj>

(usable black)
(black <obj>)
(usable white)
(usable yellow)
..
.
(usable brown)

Table 3: Example domain for which delayed step-ordering commitment results in ecient
planning.
Assume that all the colors are usable in the initial state. Since painting an object a certain
color deletes the precondition of painting an object a lighter color, and since this precondition cannot be re-achieved (no operator adds the predicate \usable"), the colors must be
used in a specific order.
This painting domain is a real-world interpretation of the artificial domain Dm S 1 introduced in (Barrett & Weld, 1994). The operators in Dm S 1 look like:
Operator: A
preconds: fI g
adds: fG g
deletes: fI jj < ig
Since each operator deletes the preconditions of all operators numerically before it, these
operators can only be applied in increasing numerical order. Thus, A1 corresponds to the
operator paint-white, A2 corresponds to paint-yellow, etc. We used this domain for our
experiments, all of which were run on a SPARC station. We generated random problems
having one to fifteen goals: ten problems with each number of goals. We used these same
150 problems to test both of the extreme heuristics. To get our data points, we averaged
the results for the ten problems with the same number of goals. All of the raw data is
contained in the online appendix. We graph the average time that flecs took to solve the
problems versus the number of goals.
As shown in (Stone et al., 1994),9 eagerly applying leads to exponential behavior (as
a function of the number of goals) in this domain, while eagerly subgoaling, when using
i

i

i

j

9. We began the study of our new planning algorithm | now named flecs| on prodigy4.0. We consider
the version of prodigy used in (Stone et al., 1994) to be a preliminary implementation of flecs.

43

fiVeloso & Stone

an operator choice heuristic from the same study, leads to approximately linear behavior
and no backtracking. The problem with eagerly applying is that, for example, if goal G7
is solved before G4, then flecs will immediately apply A7 and have to backtrack when
it unsuccessfully tries to apply A4 . Eagerly subgoaling allows flecs to build up the set
of operators that it will need to apply and then order them appropriately by selecting an
application order that avoids conicts or threats. Figure 11 shows a graphic comparison of
the two different behaviors.
Eager Subgoaling
Eager Applying

Time: msec

2500
2000
1500
1000
500
0
0

2

4

6
8
10 12
Number of Goals

14

16

Figure 11: flecs's performance with different heuristics in domains Dm S 1. Eager subgoaling and applying correspond to delayed commitments and eager commitments
respectively.

5.2 Eagerly Applying Can Be Better

Next, consider the class of tasks in which the following is true: several operators could
be used to achieve any goal, but each operator can only be used once. To use a similar
example, suppose we are trying to paint different parts of a single object different colors.
However, now suppose that we are using multiple brushes that never come clean: once we
use a brush for one color, we can never safely use it again. For instance, if we painted the
green parts using brush1, we would need to use brush2 (or any brush besides brush1) to
paint the red parts. Table 4 represents the operators in this new domain.
Operator:

paint-with-brush1

<parts> <color>
preconds: (unused brush1)
adds: (painted <parts> <color>)
deletes: (unused brush1)

: : : paint-with-brush8

<parts> <color>
   (unused brush8)
   (painted <parts> <color>)
   (unused brush8)

Table 4: Example domain for which eager step-ordering commitment and use of the state
results in ecient planning.
Note that each operator can be used for any color, but since it deletes its own precondition,
it can only be used once. We capture the essential features of this domain in an artificial
domain called D1 -use-once. The operators in D1-use-once look like:
44

fiflecs: Planning with a Flexible Commitment Strategy

A
fI g
f< g >g
fI g

Operator:
preconds:
adds:
deletes:

i

i

i

Any operator can achieve any goal, but since each operator deletes its own precondition, it
can only be used once. Each operator corresponds to painting with a different brush.
In this domain, it is better to eagerly apply than it is to eagerly subgoal. Eagerly
subgoaling causes flecs to select the same operator to achieve all of its goals. With a
deterministic method for selecting operators (such as minimum conspiracy number with
order of appearance in the domain specification as a tie-breaker), it selects operator A1 to
achieve two different goals. However, since it could only apply A1 once, it would need to
backtrack to select a different operator for one of the goals. As shown in Figure 12, eagerly
applying outperforms eagerly subgoaling in this case. We generated these results in the
same way as the results in the previous subsection.
Eager Subgoaling
Eager Applying

Time: msec

5000
4000
3000
2000
1000
0
0

2

4

6
8
10 12
Number of Goals

14

16

Figure 12: flecs's performance with different heuristics in domains D1-use-once.

5.3 An Intermediate Heuristic

Were it always possible to find good solutions either by always eagerly subgoaling, as in the
first example, or by always eagerly applying, as in the second, there would be no need to
include the variable toggle in flecs: we could simply have an eager-subgoal mode and an
eager-apply mode. However, there are cases when neither of the above alternatives suces.
Instead, we need to eagerly subgoal during some portions of the search and eagerly apply
during others. One heuristic for changing the commitment strategy is the abstraction-driven
method described in Section 4.2. Here we present a domain which can use a form of this
heuristic.
This time consider the class of tasks in which the following is true: top-level goals take at
least three operators to achieve, one of which is irreversible, can only be executed a limited
number of times, and restricts the bindings of the other operators. One representative of
this class is the one-way rocket domain introduced in (Veloso & Carbonell, 1993). For the
sake of consistency, however, we will present a representative of this class of domains in
the painting context. Suppose that we are painting walls with rollers. To paint a wall we
45

fiVeloso & Stone

need to first \ready" the wall, which for the purpose of this example means to decide that
the wall needs to be painted and to designate a color and roller to paint the wall. Next we
must fill the selected roller with the appropriately colored paint. Then we can paint the
wall. Unfortunately, our limited supply of rollers can never become clean after they have
been filled with paint, but they must be clean when they are selected to paint a wall. For
this reason, we must ready all the walls that we want to paint with the same roller before
we can fill the roller with paint. For the reader familiar with the one-way rocket domain,
the \fill-roller" operator here is analogous to the \move-rocket" operator in that domain:
it can only be executed once due to a limited supply of fuel, and it must be executed after
it has been fully loaded. Table 5 shows a possible set of operators in this painting domain.
Operator:

designate-roller

fill-roller

paint-wall

<wall> <roller> <color> <roller> <color> <wall> <roller> <color>
preconds: (clean <roller>)
(clean <roller>)
(ready
(needs-painting <wall>)
(chosen
<wall> <roller> <color>)
<roller> <color>) (filled-with-paint
<roller> <color>)
adds: (ready
(filled-with-paint
(painted <wall> <color>)
<wall> <roller> <color>) <roller> <color>)
(chosen <roller> <color>)
deletes:
(clean <roller>)
(ready
<wall> <roller> <color>)
(needs-painting <wall>)

Table 5: Example domain for which the exibility of commitments results in ecient planning.
When given this domain representation, flecs has a dicult time with some apparently simple problems if it uses the same search strategy throughout its entire search. For
example, consider the problem with five walls and two rollers (equivalent to a problem in
the one-way rocket domain with five objects and two destinations):
Initial State
(needs-painting wallA)
(needs-painting wallB)
(needs-painting wallC)
(needs-painting wallD)
(needs-painting wallE)
(clean roller1)
(clean roller2)

Goal Statement
(painted wallA red)
(painted wallB red)
(painted wallC red)
(painted wallD green)
(painted wallE green)

46

An Optimal Solution
<Designate-Roller wallA roller1 red>
<Designate-Roller wallB roller1 red>
<Designate-Roller wallC roller1 red>
<Fill-Roller roller1 red>
<Paint-Wall wallA roller1 red>
<Paint-Wall wallB roller1 red>
<Paint-Wall wallC roller1 red>
<Designate-Roller wallD roller2 green>
<Designate-Roller wallE roller2 green>
<Fill-Roller roller2 green>
<Paint-Wall wallD roller2 green>
<Paint-Wall wallE roller2 green>

fiflecs: Planning with a Flexible Commitment Strategy

flecs does not directly find this solution when always eagerly subgoaling or when always
eagerly applying. To search eciently, it must subgoal until it has considered all the walls
that need to be painted the same color; then it must apply all applicable operators before
continuing. There is no explicit information in the domain telling it to use one roller for red
and one roller for green.10 For this reason, when flecs eagerly subgoals, it initially selects
the same roller to paint all the walls. It extensively backtracks before finding the correct
bindings. flecs also does not realize that it should \ready" all the walls that are going
to be painted the same color before filling the roller. Thus, when flecs eagerly applies
operators, it tries filling a roller as soon as it has one wall \readied." Note that planning
with variables would not solve this problem since the planner would still need to make
binding selections before subgoaling beyond \paint-wall," hence facing the same problems.
When flecs tries to solve the above problem using either strategy described, it does
not succeed in a reasonable amount of time. Since flecs is complete, it would certainly
succeed eventually, but eventually can be a long time away when dealing with an NP-hard
problem: neither of these commitment strategies leads to a solution to the above problem
in under 500 seconds of search time. But all is not lost. By changing the value of toggle
at the appropriate times, flecs can easily find a solution to the above problem. In fact, it
can do so in just 4 seconds when toggle is manually changed at the appropriate times.
time(sec) solution
eager applying
500
no
eager subgoaling
500
no
variable strategy
4
yes
If flecs eagerly subgoals until it has decided to paint wallA, wallB, and wallC with
roller1, then it can begin eagerly applying. Once the three walls are painted red, flecs
can begin subgoaling again without any danger of preparing the other walls with the wrong
roller: only roller2 is still clean. This is an example in which the change in state allows the
minimum conspiracy number heuristic to select the correct instantiated operator.
The general heuristic here is that toggle should be set to sub until all walls that need
to be painted the same color have been considered. Then toggle should be set to app until
all the applicable operators have been applied. Then toggle should be set back to sub as
the process continues. In this way, flecs will need to do very little backtracking and it
can quickly reach a solution. This heuristic corresponds to using an abstraction hierarchy
to deal separately with the interactions between the different colors and the different walls.

6. Conclusion

We have presented a planner that is intended for studying the correspondence between planning problems and the search heuristics that are most suited to those problems. flecs has
the ability to eagerly subgoal, thus delaying operator-ordering commitments; eagerly apply,
thus maximizing the advantages of maintaining an internal state; or to exibly interleave
these two strategies. Thus it can operate at any point in the continuum of operator-ordering
heuristics { one important dimension of planning.
10. This problem is very common in planning as there is often no syntactically correct way to restrict bindings
in a domain representation while maintaining the intended exibility and generality in the domain.

47

fiVeloso & Stone

In this paper, we explained the advantages and disadvantages of delayed and eager
commitments. We presented the flecs algorithm in full detail, carefully motivating the
concepts and illustrating them with clear examples. We discussed different heuristics to
guide flecs in its choice points and discussed its properties. We showed examples of
specific planning tasks and corresponding empirical results which support our position that
a general-purpose planner must be able to use a exible commitment strategy. Although
all planning problems are solvable by complete planners, flecs may solve some of the
problems more eciently than other planners that do not have the ability to change their
commitment strategy and may fall into a worst case of their unique commitment strategy.
flecs provides a framework to study the characteristics of different planning strategies
and their mapping to planning domains and problems. flecs represents our view that
there is no domain-independent planning strategy that is uniformly ecient across different domains and problems. flecs addresses the particular operator-ordering choice as a
exible planning decision. It allows the combination of delayed and eager operator-ordering
commitments to take advantage of the benefits of explicitly using a simulated execution
state and reasoning about planning constraints.
We are currently continuing our work on understanding the tradeoffs among different
planning strategies along different dimensions. We plan to study the effects of eager versus
delayed commitments at the point of operator instantiations. We are also investigating the
effects of combining real execution into flecs. Finally, we plan to use machine learning
techniques on flecs's choice points to gain a possibly automated understanding of the
mapping between ecient planning methods and planning domains and problems.

Appendix A. Proofs
We prove that flecs is sound and that with iterative deepening it is complete. Consider
the flecs algorithm as presented in Table 2. A planning problem is determined by the
initial state, the goal statement, and the set of operators available in the domain. A plan
is a (totally-ordered) sequence of instantiated operators. The returned plan generated by
flecs for a planning problem is the sequence of applied operators upon termination. A
solution to a planning problem is a plan whose operators can be applied to the problem's
initial state so as to reach a state that satisfies the Goal Statement. A justified solution is
a solution such that no subsequence of operators in the solution is also a solution. flecs
terminates successfully when the termination condition is met (step 2).

Theorem 1.

flecs is sound.
We show that the flecs algorithm is sound; that is, if the algorithm terminates suc-

cessfully, then the returned plan is indeed a solution to the given planning problem.
Assume that flecs terminates successfully and that S = O1 ; O2; :::On is the returned
plan. flecs applies an operator only when the preconditions of the operator are satisfied
in the Current State C (step 7). Hence, by construction, after operators O1 ; O2; : : :Ok for
any k < n have been applied, the preconditions of operator Ok+1 are satisfied in C . At the
point of termination, the Current State C satisfies the Goal Statement (step 2). But C was
reached from the initial state by applying the operators of S . Therefore S is a solution.
QED.
48

fiflecs: Planning with a Flexible Commitment Strategy

Theorem 2.

flecs with iterative deepening is complete.

Recall that completeness, informally, means that if there is a solution to a particular
problem, then the algorithm will find it. We show that flecs's search space is complete and
that flecs's search algorithm is complete as long as it explores all branches of the search
space, for example using iterative deepening (Korf, 1985).11 Iterative deepening involves
searching with a bound on the number of search steps that may be performed before a
particular search path is suspended from further expansion; if no solution is found for a
particular depth bound, the search is repeated with a larger depth bound.
For a planning problem, assume that S = O1; O2; :::On is a justified solution. We will
show that if flecs searches with iterative deepening, it will find a solution.
The flecs algorithm has four choice points. Three of these choice points are backtrack
points: the choice between subgoaling and applying (step 5d), the choice of which operator
to use to achieve a goal (step 6c), and the choice of which applicable operator to apply
(step 7). One choice point is not a backtrack point: the choice of goal on which to subgoal
(step 6).
To prove completeness, we must show that at each backtrack point, there is some possible
choice that will lead flecs towards finding the plan S , no matter what choices flecs makes
at the non-backtrack choice point. Then if flecs explores all branches of the search space
by searching with iterative deepening, it must eventually find S unless it finds some other
solution (of length  n) first.
The proof involves constructing oracles that tell flecs which choices to make at the
backtrack points so as to find S . Then no matter what choices it makes at the other choice
point, it finds solution plan S .
Consider the point in the search at which operators O1; O2; O3; : : :; Ok for some k (and
no others) have already been applied. Then let there be oracles at the backtrack points
which operate as follows.
At the choice of subgoaling or applying (step 5d), the first oracle makes flecs choose to
apply if and only if Ok+1 is applicable (i.e., is in A); otherwise it makes flecs subgoal. If
flecs chooses to apply (Ok+1 2 A), then it reaches another choice point, namely the choice
of operator to apply (step 7). Another oracle makes flecs select precisely the step Ok+1 .
If flecs chooses to subgoal (Ok+1 62 A), then let flecs choose any goal P from the
set of pending goals P (step 6). Since step 6 is not a backtrack point, we cannot have an
oracle determine the choice at this point. Instead we have to show that, independently from
the choice made at this point, flecs will still find the solution S . It can find this solution
as a consequence of the construction of the next oracle that controls the final choice point
(below). That oracle guarantees that any P selected must either be a member of the goal
statement or a precondition of some operator of S .
The final choice point is the selection of an operator to achieve P (step 6c). The third
oracle makes flecs choose an operator of S to achieve P . Since S is a solution to the
planning problem and since P is either a member of the Goal Statement or a precondition
of some operator of S , there must be some operator of S that achieves P . If there is more
than one such operator, any one can be chosen. Since only operators from S are selected,
11. As opposed to breadth first search, iterative deepening does not harm eciency. It combines the eciency
of searching depth first with the completeness of searching breadth first.

49

fiVeloso & Stone

the condition that all pending goals are from the Goal Statement or are preconditions of
operators of S is maintained.
These three oracles will lead flecs to the justified solution S . Since S is justified, every
operator of S is necessary to achieve either some goal in the goal statement or some precondition of another operator. Consequently, since the third oracle only chooses operators
of S , every such operator will eventually be chosen and then applied as prescribed by the
first two oracles. Once every operator of S has been applied, the termination condition will
be met (since S is a solution) and flecs will terminate successfully. QED.

Acknowledgements
We would like to recognize in particular the contributions of Jim Blythe and Eugene Fink to
our research. Jim Blythe is highly responsible for the current implementation of prodigy4.0
upon which flecs is based. Eugene Fink helped with the formalization of our algorithms
and proofs. We thank Eugene Fink, Karen Haigh, Gary Pelton, Alicia Perez, Xuemei Wang,
and the anonymous reviewers for their comments on this article.
This research is sponsored by the Wright Laboratory, Aeronautical Systems Center, Air
Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA)
under grant number F33615-93-1-1330. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the
ocial policies or endorsements, either expressed or implied, of Wright Laboratory or the
U. S. Government.

References

Ambros-Ingerson, J., & Steel, S. (1988). Integrating planning, execution, and monitoring.
In Proceedings of the Seventh National Conference on Artificial Intelligence, pp. 83{88
St. Paul, MN.
Barrett, A., & Weld, D. S. (1994). Partial-order planning: Evaluating possible eciency
gains. Artificial Intelligence, 67, 71{112.
Blythe, J., & Veloso, M. M. (1992). An analysis of search techniques for a totally-ordered
nonlinear planner. In Proceedings of the First International Conference on AI Planning Systems, pp. 13{19 College Park, MD.
Carbonell, J. G., Blythe, J., Etzioni, O., Gil, Y., Joseph, R., Kahn, D., Knoblock, C.,
Minton, S., Perez, A., Reilly, S., Veloso, M., & Wang, X. (1992). PRODIGY4.0: The
manual and tutorial. Tech. rep. CMU-CS-92-150, Department of Computer Science,
Carnegie Mellon University.
Carbonell, J. G., Knoblock, C. A., & Minton, S. (1990). Prodigy: An integrated architecture for planning and learning. In VanLehn, K. (Ed.), Architectures for Intelligence.
Erlbaum, Hillsdale, NJ. Also Technical Report CMU-CS-89-189.
Chapman, D. (1987). Planning for conjunctive goals. Artificial Intelligence, 32, 333{378.
50

fiflecs: Planning with a Flexible Commitment Strategy

Ernst, G. W., & Newell, A. (1969). GPS: A Case Study in Generality and Problem Solving.
ACM Monograph Series. Academic Press, New York, NY.
Fikes, R. E., & Nilsson, N. J. (1971). Strips: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2, 189{208.
Fink, E., & Veloso, M. (1994). PRODIGY planning algorithm. Technical report CMU-CS94-123, School of Computer Science, Carnegie Mellon University.
Kambhampati, S. (1994). Desing tradeoffs in partial order (plan space) planning. In Proceedings of the Second International Conference on AI Planning Systems, AIPS-94,
pp. 92{97 Chicago, IL.
Knoblock, C. A. (1994). Automatically generating abstractions for planning. Artificial
Intelligence, 68.
Korf, R. E. (1985). Depth-first iterative-deepening: An optimal admissible tree search.
Artificial Intelligence, 27 (1), 97{109.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings of
the Ninth National Conference on Artificial Intelligence, pp. 634{639.
McDermott, D. V. (1978). Planning and acting. Cognitive Science, 2-2, 71{109.
Minton, S. (1993). Integrating heuristics for constraint satisfaction problems: A case study.
In Proceedings of the Eleventh National Conference on Artificial Intelligence, pp. 120{
126.
Minton, S., Bresina, J., & Drummond, M. (1991). Commitment strategies in planning: A
comparative analysis. In Proceedings of the Twelfth International Joint Conference
on Artificial Intelligence, pp. 259{265.
Minton, S., Knoblock, C. A., Kuokka, D. R., Gil, Y., Joseph, R. L., & Carbonell, J. G.
(1989). prodigy 2.0: The manual and tutorial. Technical report CMU-CS-89-146,
School of Computer Science, Carnegie Mellon University.
Rosenbloom, P. S., Newell, A., & Laird, J. E. (1990). Towards the knowledge level in
SOAR: The role of the architecture in the use of knowledge. In VanLehn, K. (Ed.),
Architectures for Intelligence. Erlbaum, Hillsdale, NJ.
Sacerdoti, E. D. (1977). A Structure for Plans and Behavior. American Elsevier, New York.
Stone, P., Veloso, M., & Blythe, J. (1994). The need for different domain-independent
heuristics. In Proceedings of the Second International Conference on AI Planning
Systems, pp. 164{169.
Tate, A. (1977). Generating project networks. In Proceedings of the Fifth International
Joint Conference on Artificial Intelligence, pp. 888{900.
51

fiVeloso & Stone

Veloso, M., & Blythe, J. (1994). Linkability: Examining causal link commitments in partialorder planning. In Proceedings of the Second International Conference on AI Planning
Systems, pp. 170{175.
Veloso, M. M. (1989). Nonlinear problem solving using intelligent casual-commitment.
Technical report CMU-CS-89-210, School of Computer Science, Carnegie Mellon University.
Veloso, M. M., & Carbonell, J. G. (1993). Derivational analogy in prodigy: Automating
case acquisition, storage, and utilization. Machine Learning, 10, 249{278.
Veloso, M. M., Perez, M. A., & Carbonell, J. G. (1990). Nonlinear planning with parallel
resource allocation. In Proceedings of the DARPA Workshop on Innovative Approaches
to Planning, Scheduling, and Control, pp. 207{212 San Diego, CA. Morgan Kaufmann.
Wilkins, D. E. (1984). Domain-independent planning: Representation and plan generation.
Artificial Intelligence, 22, 269{301.

52

fi