	
fffi 	


	"!$#%'&)(***,+-#.0/1((2

3,456782,9//:;4
!7<#*9*0*

=?>A@B@DCFEHGJILK$MONBPRQSTK$UVIWGYX[Z\XGYPHPHQP^]`_aYUcbedHP^f6ghUiaePHQj<K$Mlkm]YKJPonVI
QPlpqarPsC-@mK<nVK$UutvQPHQInuQf?@maetwGYQPhI

xzy8{W|~}VW|{V|{

F--o-<8 

}AW{VyW|}us|-L

'<Wo-<8 

VF$o,'V)'0)[L),)
 ),, -," Y 

['W
60su0[euh6^)
 0
 0eH)FF0)"0se)F)
0s<u[<~<)s  s)
h8<"  'he0s$"Vss
sz-  00[~~^"
W)<sehuoo)^H^
s6e^"e0i^
 "R'u 
- 
 $~--  "F^)<su$i"e
  VF0 
hH0 RA)Hh)F
  
"0s-e 6^,	
 6J0oe   
L)'0''s8V<$s
   \fi
h	
 F 

 ffsT  "F[[ 
  0-  F
   	
 6 - 
 )e0m	
  

V)ss,
 "'$Y0H0 H^
)
 "VuHHs e)
FV)V^"'ue08)u)iYsu^
60sHHJRe0^eoLT
- 
 s$^< 6YV6o$sV0
ssoReV6Hs0s0'
   
 '6
eFYH^fi ffs006Y

 ' 
 " !$#%#'&)(*!+-,."!//.&"/.01&2/4356378#9;:.7<!=.9%5?>@!$9%&6(A047*/.7*B%!$9%&65?/C5$DE!$(89%&65?/.#9%5F9;B%!37*Bfi#%7G!H#%9;!$9'7A#;,!+(87<9%5
!$(8:.&67837F#;,78(8&JI(C05?!$#'9;!$9%78#*KMLE!BN&)5.=.#<!$605?Bfi&69;:>F#<:!37FO787*/P783478)5.,78PQ9%51!$PPBfi78#'#G9;:.7F#%9;!$9%7SR
!$(89%&65?/TBfi7*,Bfi78#%7*/49;!$9'&)5./U!/.PT9;:.7<#%7*!4Bfi(V:WDX5.B	!$(89%&65?/.#*K	YZB%!$P&69%&65?/!+)6[T9;:.78#%7!$605?Bfi&69;:>\#A:!37]O^787*/
(8"!$#%#%&JI78P!$(8(85?BfiP&"/.0F9%5<9S:.78&2B_#%7*!Bfi(8:#S,!$(87`BN7*,Bfi78#%7*/49;!$9%&65?/C!$#E78&69;:.7*B_#%9;!$9%7SR#S,!$(87`,."!//.7*BN#]ab7Kc0Ked
fghZijbkl d	L7865#%5m7891!$nK"dGo*pp4qrF5?BC,."!/4R#;,!$(87Q,."!//.7*Bfi#mab7Kc0K"d st fhZf dAuZ7*/.O7*Bfi9;:4[wvyxz78)Pd
o*pp4{r8K
|}/.78~Bfi78#%7*!BN(V:@9;Bfi7*/.P1:!$#AO^787*/C9%5FP7837865?,1/.78~7*/.(85.P&2/.04#A5$D,.2!4//.&2/.01,Bfi5?O.67*>F#&2/C5.BfiP7*B
9%5!$P5?,.97S(8&67*/9A!+)045?Bfi&69;:>F# DBfi5?>59S:.7*B	Bfi78#%7*!BN(V:C!Bfi7*!$#d?67*!$P&"/.0C9%5#%&60?/.&JI(*!/49	P7837865?,>\7*/9%#
&"/,.2!4//.&2/.0w!+)045?Bfi&69;:>F#*d	!+##;=BN378[78PO4[mxQ786Pafio*pp4pr8KwY	:.&6#1(8"!$#%#C5$D<,."!//.&"/.0!$605?BN&)9S:>F#
&"/.(8"=.P78# k-g
-ff$- an"=>v=BN#%9*dEoppr9;:!$9F=.#%78#]!W5~_R0?B%!,:7*/.(85.P&"/.09%5U(85?/.#'9;B%!$&"/
9;:.7H#%7*!Bfi(8:!/.P f+- aG!=.9%]v.78">!/doppr 9S:!$9A7*/.(85.P78#G9;:.7F,.2!4//.&2/.0,Bfi5?O.67*>y!$#G!
#;!$9'&)#NI!O.&6)&69fi[,BN5?O.67*>!/.P=.#'78# D!+#%9A>F5.P78#;!$9'&)#ND!$(89'&)5./1!$605?Bfi&69;:>F#9%5FI/.P!H#%542=.9'&)5./K
e (**0*8fi-fi7	V	7$0	W	,ff56	4
!fi

!,Fb7$

fi

0J   

 78(87*/9%6[d!/.59S:.7*B/.78~w,.2!4//.7*BM f an &">!$9%9'&?789E!$nK"doppr~	!$#&"/9;BN5?P=.(878P9S:!$97*/.(85?P78# !
,."!//.&"/.0P5?>!+&2/@!$#!H/.5?/4RP789%7*B%>F&"/.&6#%9%&6(MI/.&69%7G!=.9%5?>@!$9%5?/1an|`rBN7*,Bfi78#%7*/49%78PWO4[!/GBfiP7*BN78P
&"/!Bfi[QG78(8&6#%&65?/G&"!$0?B'!>a h  ii dE Bfi[?!/49*d o*p4r8K1/z(85?/49;B%!$#%9H9%5U9S:.7W,Bfi783.&65?=.#!$605?Bfi&69;:>\#*d
M f 7S78(89%&63786[U78.9%7*/.P#H9%5W/.5./4RP789%7*B%>F&"/.&6#%9%&6(CP5?>!$&"/.#],BN5?P=.(8&"/.0=/.&637*BN#;!$E,."!/.#]!+#`Bfi5.O=.#%9
#%5"=.9%&65?/.#K<=.7 9%5A9S:.7	#%(*!$"!O.&66&69[F5$D9;:.7	=/.P7*Bfi6[?&"/.0>\5?P78(8:.78(V4&"/.0]Bfi7*,BN78#%7*/9S!$9%&65?/F!/.PF#%7*!Bfi(8:
9%78(8:/.&6=.78#*d^&)9(*!/O^7G#;:.5$~A/@9%5O7<!H37*Bfi[7SC(8&)7*/49A/.5?/4RP789'7*B%>F&"/.&6#%9%&6(],."!//.7*B<a &">!$9%9%&
789	!$nK"d
o*pp4!doppOr8K
G/.75$D5?=B>!$&"/Bfi78#'7*!Bfi(8:F5?O.fi78(89%&6378#&6# 9%5GP7837865?,,."!//.&"/.0#%[.#%9%7*>\##;=.&69;!O.67	Dn5?B,."!//.&"/.0
&"/=/.(87*Bfi9;!+&2/d.#%&"/.067d.5?B>F=.69%&JR!$07*/49E7*/43?&"Bfi5./>F7*/9'#AanA!+&)0.:vL7865#%5d
o*pp^$L78)54#%5<789E!$nK"dopp
.9%5?/.7vL7865#'5dEo*p4pr8K<Y:.7\=/.&637*BN#;!$,."!//.&"/.0Q!,,Bfi5.!$(V:d!+#G5?Bfi&60&"/!$66[QP7837865?,78Pan.(8:.5?,4R
,7*BN#*do*p4r8d&6#<!,,^7*!$6&"/.0UDn5?B9;:.&6#<9fi[,7F5$DE7*/43?&"Bfi5?/>\7*/9%#K<|=/.&)347*Bfi#;!$E,."!/T&6#]!@#%789G5$D #%9;!$9%7SR
!$(89%&65?/B%=.678#H9;:!$9G!$&">y!$9G(85$37*Bfi&"/.019;:.7,5#'#%&"O.)71>F=.69%&",.)7@#%&69;=!$9%&65?/.#H&2/9;:.7/.5?/4RP789%7*B%>F&"/.&6#%9%&6(
7*/43?&"Bfi5?/>\7*/9*K|=/.&637*Bfi#;!+-,."!/1&6#M78?78(*=.9%78PQO4[&"/49%7*Bfi67*!3?&"/.019;:.7H#%78678(89%&65?/15+D!/T!$(89%&65?/&2/9;:.7
,."!/w!4/.Pw5?O.#%7*BN3?&"/.09;:.7Bfi78#S=.)9'&2/.07S78(89%#&"/9;:.71~ 5?Bfi6PKA/.&637*Bfi#S!$A,."!//.&"/.0Bfi78#'7*>]O.678#C9;:.7
5?=.9%(85.>F7<5$DBN78&2/4Dn5?Bfi(87*>\7*/9G67*!B%/.&"/.0an=.9'9%5?/Uv !Bfi9'5do*pp4r8d&"/T9;:!$9M9;:.7<#'9;!$9%7SR!$(89'&)5./U>F5.P78
(*!,.9;=BN78#-9;:.7 =/.(87*Bfi9;!$&"/49fi[A5$D9S:.7E~ 5?Bfi6PK-M/.&637*Bfi#;!+,."!//.&"/.0G&6#!M,Bfi78(*=Bfi#'5?B!,,Bfi5.!$(V:d # ~A:.7*Bfi7 !$6
,."!//.&"/.0&6#	P5?/.7G,Bfi&65?B9%5<78.78(*=.9%&65?/dO=.&66P&"/.0W=,^5?/@9;:.7`!+#%#;=>,.9%&65?/@9;:!$9 !F/.5?/4RP789%7*B%>F&"/.&6#%9%&6(
>F5.P785$D-9S:.7G78?78(*=.9%&65?/7*/3.&"Bfi5?/>F7*/49	(*!/1O^7]!$(8=.&"Bfi78Pd!/.P167*!$P&"/.0C9;:.7*Bfi7SDn5?Bfi7G9%5!F#'5?=/.PU!/.P
(85?>,.6789%7F,."!//.&"/.0U!,,Bfi5?!$(8:K
5$~ 7837*B8d=/.&637*Bfi#;!$,."!//.&"/.0:!$#MO787*/T(*Bfi&69%&6(8&6878Pab7Kc0KedG&"/.#;O^7*Bfi0dZo*pprd?P=.7F9%5C!,59'7*/4R
9%&"!$78,5./.7*/9%&"!$Z0?Bfi5$~	9S:5$DZ9;:.7]=/.&637*Bfi#S!$-,."!/1#'&)87H~	&69;:19S:.7</?=>FO7*B5$D,BN5?,5#'&)9'&)5./.#AP7SI/.&"/.0
!CP5?>!+&2/#%9;!$9%7K|`/T&">,5.Bfi9;!/49G(85?/49;Bfi&"O=.9%&65?/5$D	A f &6#G9;:.=.#<9;:.7=.#%7F5$D h  ii #<9'5UBfi7*,Bfi78#%7*/49
=/.&637*Bfi#;!+,."!/.#*KE/F9;:.7	~ 5?BN#%9(*!$#%7d49;:.&6#Bfi7*,Bfi78#'7*/9;!+9%&65?/>![]!+)#'5<0?Bfi5$~m78,5?/.7*/49%&"!$66[~&)9S:F9;:.7
/.=>]O^7*BF5$D	P5?>!+&2/m,BN5?,5#'&)9'&)5./.#*dO=.9O^78(*!=.#%7 h  ii #C!Bfi7C37*Bfi[Q(85?>@,!$(89Bfi7*,Bfi78#%7*/49;!$9%&65?/.#F5$D
O5.567*!/D=/.(89%&65?/.#*d
9;:.&6#A&6#M5$DX9'7*/1/.59A9S:.7G(*!$#%7GDn5?B P5.>!$&"/.#A~&)9S:U!Bfi780.=.2!4B	#%9SB%=.(89;=Bfi7an_&2>@!$9%9%&
789<!$nK"dZo*pp4!r8KAY:.7*Bfi7SDX5.Bfi7d h  ii RO!$#'78P,."!//.&"/.0U#%787*>F#G9'5WO^7\!1,BN5?>F&6#%&"/.0U!,,Bfi5?!+(V:T9%51=/.&JR
37*Bfi#S!$,.2!4//.&2/.0
K
  f #;,^78(8&I78#C!U,."!//.&"/.0QP5?>!+&2/&2/z9;:.7!+(89%&65?/P78#%(*Bfi&",.9%&65?/"!/.0.=!$07HaG&"=/.(V:.&606&"!
789!$nK"dMo*pp4rF!/.Pm9;B%!/.#'2!+9%78#&699%5!(85?B%Bfi78#;,^5?/.P&"/.0|Fd:.7*/.(87T6&2>\&)9'78P9'5,."!//.&"/.0w,Bfi5?O4R
67*>F#F~	&69;:I/.&)9'71#%9;!$9%7@#;,!$(878#*KUY:.79;B'!/.#%&69%&65?/Bfi78"!$9%&65?/z5$DA9;:.7!4=.9%5?>!$9'5?/&6#F7*/.(85?P78Pw!$#F!/
h  ii 9;:!$9E!+)65$~	#DX5?BZ9S:.7A=.#%7	5$D7S(8&67*/9	OBfi7*!+P9;:4RnIBfi#%9#%7*!Bfi(8:H9%78(V:/.&6=.78# P7837865?,78PCDn5?B>F5.P78
(8:.78(V4&"/.0ab(8&66"!/doppr8K   f &2/.(8"=.P78# 9fi~ 5A!$605?Bfi&69;:>\#Dn5?BZ=/.&)347*Bfi#;!$,."!//.&"/.0KEY	:.7M8n4
6 
bG!$605?Bfi&69;:>9SBfi&678#	9%507*/.7*B'!$9%7G!,."!/19;:!+9&6#	0.=!B%!/49%7878P19%5!$(8:.&67837<9;:.7G045?!$Dn5?B	!+)5$D
9;:.7,^5#%#%&"O.6715?=.9%(85.>F78#<5$D	9;:.7C/.5?/4RP789%7*B%>F&"/.&6#%9%&6(!$(89%&65?/.#*K1bDA/.5T#;=.(V:z#%9;BN5?/.01#%5"=.9%&65?/z78?&6#%9'#*d
9;:.7!$605?BN&)9S:>D!$&66#*K<Y:.78nfiT*$  b 6 
bF!$605?Bfi&69;:>yBfi789S=B%/.#`!C#'9;Bfi5?/.0C#%5"=.9%&65?/d&D 5?/.7
78.&)#'9%#*dZ5?BG549;:.7*Bfi~	&6#%79SBfi&678#]9'5U07*/.7*B%!+9%7!U,."!/z9;:!$9F>![U(85?/49;!$&"/z65?5?,.#FO=.9F&)#F0?=!4B%!/49%7878PQ9%5
!$(8:.&678379S:.705.!$nd-04&)347*/Q9;:!+9]!$6E(8[.(86&6(178?78(*=.9%&65?/.#@7837*/9S=!$6)[9%7*B%>\&2/!+9%7KUbDA/.5#;=.(8:Q#'9;Bfi5?/.0
(8[.(8)&6(F#%5"=.9%&65?/78?&6#%9'#*d9;:.7G#%9SBfi5?/.0(8[.(86&)(,."!//.&"/.0U!$605?BN&)9S:>D!+&)6#*K
/m9;:.&6#1!Bfi9%&6(867U~ 7U,Bfi78#'7*/9C5?=B h  ii RO!$#%78P,."!//.&"/.0m#%[?#'9%7*>1d s hZf aA/.&637*Bfi#S!$	Q=.69%&JR
!$07*/49U< ii RO!$#'78Pu"!//.7*B%r8d 9;:!+9\=.#'78#W!Q/.78~ h  ii RO!$#%78P7*/.(85.P&"/.0d	07*/.7*B'!$9%78#=/.&637*Bfi#S!$
,."!/.#G&"/>]=.69%&JR!$047*/9G/.5?/4RP789%7*B%>\&2/.&6#%9'&)(@P5?>!$&"/.#*d!/.PU&"/.(8"=.P78#!/.78~y%5?,.9'&2>\&)#'9%&6(8W,."!//.&"/.0
!$605?BN&)9S:>1K
S nm?b'*nfi8'n'cSncF-fi'\fiZ%"
nS8;cF'*bSnn]b%b%$;Sn$bSZ*'\n
fi;)S8fi	b'
"'cff	*fifi
fi

fi  0



)-  
 -  -)

G=B5$37*B%!$6E!,,Bfi5.!$(V:DX5?BMP78#'&)0./.&2/.0!/ h  ii RO!+#%78Pw,."!//.7*BH&)#H#%&">F&6"!BF9%519;:.7C!,,Bfi5?!$(8:
P7837865?,^78PFDX5.BA f KG=B>!$&"/G(85?/49;Bfi&"O=.9%&65?/H&6#E!/7S(8&67*/9Z7*/.(85?P&"/.0H5$D!	/.78~DBfi5?/497*/.PFP5?>!$&"/
P78#%(*Bfi&",.9%&65?/C"!/.0.=!$07d"!#an5?/4RP789%7*B'>F&"/.&)#'9%&6(G|A07*/49EG5.>!$&"/%$!/.0?=!$07*rK&"!'#m:!$#>\5?Bfi7
Bfi78#%7*>FO."!/.(87T~	&69;:w,Bfi783.&65?=.#1,.2!4//.&2/.0"!/.0?=!$0478#9;:!4/9;:.7U!$(89%&65?/P78#%(*BN&2,.9'&)5./w"!/.0.=!$071<
(*=B%Bfi7*/49%6[m=.#%78PwO[A f K9:!$#,5$~ 7*BD=.!$(89'&)5./P78#%(*Bfi&",.9%&65?/.#C9;:!$9F(*!/,7*BDn5?B%> !Bfi&69;:>\789%&6(
5?,^7*B%!$9%&65?/.#M5?/1/.=>F7*Bfi&6(*!$ZP5?>!$&"/13$!Bfi&"!O.678#*K G5?>@!$&"/.#A(85?>@,Bfi&6#%78PU5$D#%[/.(V:BN5?/.&6878P!+07*/9'#	(*!/
O7G>F5.P786)78PUO4[F&"/9;BN5?P=.(8&"/.0F(85?/.(*=B%Bfi7*/49!+(89%&65?/.#O!$#'78P5?/@!<>F=.69%&JR!$07*/49EP78(85?>,^5#%&69%&65?/@5$D9;:.7
P5?>!+&2/K
/Q!$PP&69%&65?/
d "!#Q&"/9;BN5?P=.(878#!1#'7*,!B%!$9%7!/.Pz78,.6&6(8&697*/3.&"Bfi5?/>F7*/49>F5.P78P7SI/.78Pw!$#<!
#%789H5$)
D (.Snfi b6+ *8- , !$047*/9%#d&nKc7K"dE!+07*/9'#G~A:.5#%71!$(89%&65?/.#F(*!//.59FO7U!T,!Bfi9<5+D_9S:.707*/.7*B'!$9%78P
,."!/.
K /"!#:!+#AO787*/T(*!Bfi7SD=.66[1P78#%&60?/.78P9%5!$665$~DX5.B 7S(8&67*/9 h  ii R7*/.(85.P&"/.0KAY:?=.#*d
s hZf
(85?/49;Bfi&"O=.9%78#G!F,!Bfi9%&69%&65?/.78P9;B%!4/.#%&69%&65?/1Bfi78"!$9%&65?/TBfi7*,Bfi78#%7*/49;!$9%&65?/C5+D9;:.7<|}9;:!$9	&6#G?/.5$~A/@DBfi5?>
>F5.P78E(8:.78(V4&"/.09%5U#%(*!+)71=,z~786Gan =Bfi(V:789]!$nK"d o*pp^o  !4/N!/z789]!+XKedoppqr8KG=BH7*>,.&"Bfi&6(*!$
78,7*Bfi&">F7*/49%#M#;=.0078#%99;:!$9 9;:.&6#A&6#G!$6#%59;:.7G(*!+#%7MDX5.B	s h-f K
0  h-f &2/.(8"=.P78#M9;:.7A,Bfi783.&65?=.#%6[CP7837865?,78PU!$605.Bfi&69;:>F#Dn5?B h  ii RO!$#%78PU=/.&637*Bfi#;!+,."!//.&"/.0K
/m!+PP&)9'&)5./dE~ 7C&2/49;Bfi5.P=.(87U!/.78~ %5?,.9%&">F&6#%9%&6(8,."!//.&"/.0m!$605?Bfi&69;:> 9;:!+9\BN782!+?78#F5?,.9%&">!+)&69fi[
0?=!B'!/9'7878#A!/.P07*/.7*B%!$9'78#`,."!=.#%&"O.671=/.&)347*Bfi#;!$,."!/.#G&"/UP5.>!$&"/.#G~A:.7*Bfi7F/.51#%9;Bfi5?/.0C/.5?B	#'9;Bfi5?/.0
(8[.(8)&6(F#%5"=.9%&65?/78?&6#%9'#*K
Y:.7<!Bfi9%&6(867G&6#	5?Bfi0?!4/.&)878PU!+# DX54)65$~	#*K.78(89%&65?/T{HP&)#'(*=.#%#%78#G,Bfi783?&65?=.#G!,,Bfi5.!$(V:.78# 9%5,."!//.&"/.0
&"/Q/.5?/4RP789%7*B'>F&"/.&)#'9%&6(P5?>@!$&"/.#*K].78(89'&)5./Q04&)3478#<!1OBfi&67SD_5$37*BN3?&678~ 5$D h  ii #]!/.PQ|7*/.(85.P.R
&"/.0#*K n9A>@!*[1O7H#;4&2,,^78PQO[UBfi7*!+P7*Bfi#A!$"Bfi7*!$P[CD!>F&66&"!B~	&69;:19;:.7G#S=O.78(89K.78(89%&65?2
/ 1&"/9SBfi5?P=.(878#
"!'#d#;:.5$~	#:.5$~9'57*/.(85.P7U!,."!//.&"/.0,Bfi5?O.67*>1d !/.PzDX5?B'>!$66[QP78#%(*Bfi&"O78#C9;:.7C#%[/9;!+!/.P
#%7*>!4/9%&6(8#5$D9;:.&6# P78#%(*Bfi&",.9%&65?/@"!/.0?=!$047	&"/9%7*B'>F#5$D-!/|]K$xz7A!$6#%5<P&6#%(*=.#'#9S:.7A,Bfi5?,^7*Bfi9%&678#E5$D
9;:.7G"!/.0?=!+07<O!$#%78P15?/C!/C78!>,.67<!/.PU!BN0?=.7MDX5.BE5.=BEP78#'&)0./1(8:.5&6(878#*KE.78(89%&65?/Tq],BN78#%7*/9'#	9;:.7
D "!#QP5?>!$&"/TP78#%(*Bfi&",.9%&65?/.#K].78(89%&65?/zP78#%(*BN&2O^78#<9;:.7FP&J7*Bfi7*/49<!$605$R
h  ii Bfi7*,Bfi78#%7*/49;!$9%&65?/5$
Bfi&69;:>F#9S:!$9:!37 O787*/=.#'78P<Dn5?B h  ii RO!$#'78P,."!//.&"/.0\!4/.P<&"/9SBfi5?P=.(878#5?=B5?,.9%&">F&6#%9%&6(A,."!//.&"/.0
!$605?BN&)9S:>1K<.78(89%&65?/QC,Bfi78#%7*/49%#G7*>,.&"Bfi&6(*!$ Bfi78#;=.69%#G&"/#%78347*B%!$,.2!4//.&2/.0TP5?>!$&"/.#*dZB%!/.0&"/.0CDBfi5?>
#%&"/.067SR!$07*/49<!/.P1P789%7*B'>F&"/.&)#'9%&6(5?/.78#M9%5>F=.69%&JR!$07*/49`!/.P/.5?/4RP789%7*B%>\&2/.&6#%9'&)(@5?/.78#*K xQ7H78,^7*Bfi&JR
>F7*/49E~	&69;:C,Bfi783.&65?=.#%6[W=.#'78PP5?>@!$&"/.#_!4/.P&"/49;Bfi5.P=.(87G9~ 5</.78~5?/.78#*d./!>F786[1!<,5$~ 7*BE,."!/49	!/.P
!1#'5?(8(87*BP5?>!$&"/d!$#</.5?/4RP789%7*B'>F&"/.&)#'9%&6(d>F=.69%&JR!$07*/49],."!//.&"/.0,Bfi5.O.)7*>\#*K&"/!$66[dE.78(89%&65?/
PB%!~	# (85?/.(8"=.#%&65?/.#<!/.PTP&6#%(*=.#%#%78#MP&"Bfi78(89%&65?/.#MDn5?BD=.9;=BN7G~5.B%K
 9
3-54768  6 :

V<;

 87 (*=B%BN7*/9!4,,Bfi5?!$(8:.78#],^7*BDX5.B%>F&"/.0,."!//.&"/.0Q&"/9'7*Bfi67*!*3478Pm5?BH&2/,!B%!+)678	~&)9S:Q78.78(*=.9%&65?/m:!37
O787*/@~	&6P786[W=.#'78P&"/C/.5?/4RP789%7*B%>F&"/.&6#%9%&6(FBfi5?O549%&6(	P5?>!$&"/.#<ab7Kc0
K"dG785?BN07Sv=$!/.#;4[do*pH!$9*d
o*pp4{*x&6"&"/.# 789E!$nK"do*p4>p 1$A!$&60?:@vwL7865#'5do*pp4r8K|0?Bfi5?=,5$D,."!//.7*Bfi##;=.&69;!O.67 Dn5?BZBfi78(*=B%Bfi7*/49
,."!//.&"/.0G&6#E!$(89%&65?/G#'78)78(89'5?Bfi#O!$#%78P<5./G:.7*=Bfi&6#%9%&6(	#%7*!BN(V:anM5.7*/.&60<vw.&">>F5./.#*d?oppq$5?/.789-789Z!$nK"d
o*pp4r8K-Y:.7`>F&"/4R>!+ g4?&? !$605?Bfi&69;:>yanM5.7*/.&60v.&">>F5./.#*do*ppq>\&2B'/.53@789!$nK"do*pp4rZ(*!/
07*/.7*B%!+9%7#S=O5?,.9%&">!+A,."!/.#&"/m/.5?/4RP789'7*B%>F&"/.&6#%9%&6(P5?>!$&"/.#9;:BN5?=.0?:!#%7*!Bfi(8:!/.Pm78?78(*=.9'&)5./
&69%7*B%!$9'&)5./KAY	:.7H#%7*!Bfi(8:1&6#GO!$#%78PU5./U!C:.7*=Bfi&6#%9%&6(F05?!$P&6#%9S!/.(87HD=/.(89%&65?/T9;:!+9A>F=.#%9AO7,Bfi5$3.&6P78P
Dn5?B!#;,^78(8&JI(,BN5?O.67*>1KwY	:.7   f !$605.Bfi&69;:> an5?/.7897891!$nK"dMo*pp4r<=.#%78#1!#%&">F&6"!BC!,,Bfi5?!$(8:
!/.PD=Bfi9;:.7*BFP7SI/.78#1!Q:.7*=Bfi&6#%9'&)(D=/.(89'&)5./Dn5?A
B @CB)DFEHGI@R)&"$7mab&"$78#vy&6)#'#%5?/dGo*p^o*rF!$(89%&65?/
Bfi7*,Bfi78#'7*/9;!+9%&65?/.#*K</(85?/49;B%!$#%9M9%51>F&"/4R>!$ g4?J
 ? d   f P5.78#</.59<!$#%#S=>F7!1/.5?/4RP789%7*B%>F&"/.&6#%9%&6(
fiKfi

fi

0J   

7*/43?&"Bfi5?/>\7*/9*dO=.9H&)#FBfi5?O=.#%9H9%5/.5./4RP789%7*B%>F&"/.&6#;> (*!=.#%78PmO[Q!$(89'&)5./Q,7*Bfi9S=B%O!$9%&65?/.#Uab&nKc7K"d9;:!$9
!/.59S:.7*B	!$(89%&65?/C9;:!4/C9;:.7],."!//.78P!$(89%&65?/1&6#	(8:.5#%7*/~	&69;:C#%5?>\7<,Bfi5?O!O.&66&69[r8K
/T07*/.7*B%!$nd-BN78(*=B%Bfi7*/49<!,,Bfi5?!$(8:.78#<!Bfi7H&"/.(85?>,.6789%71O78(*!=.#%7C!$(89%&"/.015?/!/T&"/.(85?>,.6789%71,.2!4/
(*!/C>!$7 9;:.7M05?!+=/!$(V:.&6783$!O.67K	uEBfi78(*=Bfi#'5?BE!,,BN5?!$(8:.78#,7*BDX5?B'>!$6P78(8&6#%&65?/T>!4&"/.0],Bfi&65?B9%5
78.78(*=.9%&65?/U!/.P19S:?=.#	>![O^7<!O.67<9%5F07*/.7*B%!+9%7A(85.>,.6789%7<,."!/.#`O4[9;!4&"/.0!$6-,^5#%#%&"O.67<7S78(89%# 5$D
!$(89%&65?/.#&2/49%5!+(8(85?=/9KE	5$~ 7837*B8d.9;:.78[1Bfi786[15?/1!F(85.>,.6789%7]>F5.P785$D9;:.7G~ 5?Bfi6M
P Lc#M=/.(87*Bfi9;!$&"/49[K
Y:.7A,Bfi78(*=Bfi#%5.B!4,,Bfi5?!$(8:.78#E&"/.(8"=.P7<(85?/.P&69%&65?/!+Oa N9'8&)5./.&789!+XKedopp{uZ7859 v>\&)9S:dopp{
6[?9;:.7v L7865#%5dFo*pp4r8d	,Bfi5?O!4O.&)6&6#%9%&6(ab<B'=>>F5?/.Pv  Bfi78#%&"/!dFo*pKp P^GG7*!/789U!$nK"d<oppq
6[?9;:.7do*pp4rA!/.P=/.&)347*Bfi#;!$E,."!//.&"/.0wan.(8:.5?,,7*Bfi#dEo*p4 &">!$9%9'&-789<!$nK"dopp!dZo*pp4OG!R
O!/.*!F789M!$nK"do*pp4r8K
5?B 78!>,.67d9;:.7Ft f ,!Bfi9'&2!+5?BfiP7*B8d.(85?/.P&69%&65?/!+,."!//.7*BM:!/.P678#</.5?/4R
P789%7*B%>\&2/.&6#;> O[Q(85./.#%9;B%=.(89%&"/.0Q!T(85?/.P&69%&65?/!$	,."!/9;:!$9F!$(8(85?=/49%#HDX5.BG7*!$(8:,^5#%#%&"O.67U#'&)9S=!$9%&65?/
5?B(85?/49%&"/.07*/.(8[U9;:!+9A(85?=.6PQ!Bfi&6#%7UanuZ78549`v>F&69;:do*pp{4r8K	|	9M78.78(*=.9%&65?/T9%&">F7F&)9G&6#GP789%7*B'>F&"/.78P
~A:.&6(8:,!BN9G5$D	9;:.7,."!/z9%5178?78(*=.9'7WO4[Q,7*BDn5?B%>F&"/.0T#%7*/.#%&"/.0Q!$(89%&65?/.#<9S:!$9]!BN7]&"/.(8"=.P78Pw&"/z9;:.7
,."!/19'59%78#%9Dn5?B 9;:.7<!,,BN5?,Bfi&"!$9%7G(85?/.P&69%&65?/.#K
uBfi5?O!O.&66&6#%9%&6(],."!//.7*BN#A9;BN[9%5>!+?&">F&687<9S:.7<,Bfi5?O!O.&66&69[15+D05.!$#;!$9%&6#fiD!$(89%&65?/d.0&637*/(85?/.P&JR
9%&65?/!$Z!$(89%&65?/.#M~	&69;:U,Bfi5.O!O.&6)&6#%9'&)(@7S78(89'#*KE<B'=>>F5?/.PU!/.P Bfi78#%&"/!aNo*pKp Pr Bfi7*,BN78#%7*/9G,."!/.#G!$#
!@#%789	5$D.&69;=!$9'78P_5?/9SBfi5  =.678#an  #Sr]ab<B'=>>F5?/.Pdoppr	>@!,,.&"/.01#%&69;=!$9%&65?/.#M9%5!$(89'&)5./.#*K
Y	:.7,."!//.&"/.0!+)045?Bfi&69;:>yO7804&2/.#O4[Q!$PP&"/.0  #<(85?B'Bfi78#;,5./.P&2/.09%519S:.7>F5#%9<,BN5?O!O.6778.7SR
(*=.9%&65?/,!$9;:9;:!$9A!+(V:.&678378#G9;:.7H05?!$nK	9A9;:.7*/(85?/49%&"/?=.78#<!$PP&"/.0U  #	Dn5?B)78#'#`,Bfi5.O!O.67\,!+9;:.#
!/.PU>![C7*/.PU~	&69;:1!\(85?>,.6789%7],."!/9;!4&"/.0!$6-,54#%#%&"O.67\,!+9;:.#	&"/9'5!$(8(85?=/49*K
M/.&637*Bfi#;!+,.2!4/.#P&J7*BZDBfi5?> (85?/.P&69%&65?/!$!/.P,Bfi5?O!4O.&)6&6#%9%&6(F,.2!4/.#O4[]#S,78(8&JDX[.&"/.0!,,Bfi5.,Bfi&"!$9%7
!$(89%&65?/.#\DX5?BH7837*BN[,^5#%#%&"O.67U#%9S!$9%7C&2/ 9;:.71P5?>!+&2/Q
K $&"$71(85./.P&)9'&)5./!$`!4/.Pw,Bfi5?O!O.&66&6#%9%&6(Q,.2!4/.#*d
=/.&637*Bfi#;!+-,."!/.#`BN78=.&"Bfi7H9;:.7<~ 5?Bfi6P19%5O^7]!$(8(878#%#'&2O.67F&"/15?BNP7*B 9%578.78(*=.9%7<9S:.7<,.2!4/K
M/.&637*Bfi#;!+,."!//.&"/.0C~	!$# &2/49;Bfi5.P=.(878PO4[.(8:.5?,,7*BN#\afiopr~A:.5=.#%78P1P78(8&6#%&65?/19;BN7878#	9%5Bfi7*,4R
Bfi78#%7*/49<,."!/.#*K  78(87*/9F!,,Bfi5?!$(8:.78#M&2/.(8"=.P71G!O!/.*!C789<!$nKEafioppr	!4/.P_&2>@!$9%9%&Z789`!+XKEaNo*pp!^d
o*pp4Or8KAG!4O!/.*!789<!+XKafio*p4pr	Bfi7*,BN78#%7*/9'#`=/.&637*Bfi#S!$E,."!/.#<!$6#%5U!$#G!#'789A5$D.&69;=!+9%78P 5?/49;Bfi54
 =.678#*K Y	:.78&"BA!+)045?Bfi&69;:>&"/.(*Bfi7*>F7*/49;!$66[!+PP#A  #M9%5!HI/!$-,."!/&"/U!H~	!*[@#%&">F&62!4BM9%5F<B%=>HR
>F5?/.PQ!/.P Bfi78#%&"/!afiopKp Pr8K	Y:.7<05?!+-&6#<!\DX5?B'>]=."!@&"/9%7*>,^5?B%!$6504&)(@9;:!$9A>F=.#%9G:.56PQ5?/T!/[
3!+)&6P#%78=.7*/.(87<5$DE!$(89%&65?/.#K
9 $7*!B%/.&"/.0a  $r`a=.9%9%5?/Cv !BN9%5doppr(*!/!+)#'5\O^7<Bfi780?!BNP78PW!$#	=/.&637*Bfi#S!$
 78&2/4Dn5?Bfi(87*>\7*/
,."!//.&"/.0K /  $9S:.7A05.!$&6#	Bfi7*,Bfi78#%7*/49%78PUO4[!<BN78~_!4BfiPD=/.(89%&65?/C&"/1!GQ!B'$5$3]G78(8&6#%&65?/1uBfi5.(878#%#
ab<uErM>F5.P785$D9S:.7]P5.>!$&"/K]/9;:.7,Bfi78(*=Bfi#'5?BM37*Bfi#%&65?/5$D  $d9;:.7F<u&)#<!+#%#;=>F78PQ9'5WO^7
./.5~M/Q!/.Pm!C(85?/9SBfi5E,54)&6(8[m>!$.&">F&68&2/.0z9;:.778,78(89'78PBfi78~	!BNPQ&6#<Dn5?=/.P,BN&)5.B<9'5178?78(*=.9%&65?/K
Y	:.7,54)&6(8[(*!/78&)9S:.7*B<O7Bfi7*,BN78#%7*/9'78P78,.6&6(8&)9')[z&2/!@9;!O.67F5?B&2>@,.)&6(8&69%6[mO[U!\D=/.(89'&)5./wab7Kc0Ked
!/.7*=B%!$	/.789fi~ 5?B%.r8K78(*!=.#'7  $&6#!,Bfi5?O!O.&66&6#%9%&6(!4,,Bfi5?!$(8:d&69%#FP5?>!$&"/mBfi7*,BN78#%7*/9S!$9%&65?/z&)#
>F5?BN7G(85?>,.67819;:!/9;:.7GP5?>!$&"/TBfi7*,Bfi78#%7*/49;!$9%&65?/T=.#%78PO4[1!/.5?/4RP789'7*B%>F&"/.&6#%9%&6(,.2!4//.7*B8K	Y	:.=.#*d
~ 7`>@!*[F78,78(89A/.5./4RP789%7*B%>F&"/.&6#%9%&6(,."!//.7*Bfi#	9%5FO7<!O.67G9%5:!/.P67GP5?>@!$&"/.#~&)9S:1!<"!BN07*B#%9;!$9%7
#;,!$(87F9;:!4/  $K- =.9  $>![U,Bfi5?P=.(87C,56&6(8&678#~	&69;:!1:.&60?:.7*BG=!$6&69[T9;:!/!1=/.&637*Bfi#;!+E,.2!4/
07*/.7*B%!+9%78PO4[!z/.5?/4R,Bfi5?O!O.&66&6#%9%&6(d</.5?/4RP789'7*B%>F&"/.&6#%9%&6(,."!//.7*B8K5?Bfi785$37*Bd&"/m9;:.7Bfi78(*=B%Bfi7*/49
37*Bfi#'&)5./d  $z)7*!4B%/.#A9S:.7G~5.Bfi6PU>F5?P78P=Bfi&"/.078.78(*=.9%&65?/U!4/.P19;:?=.#P5?78#G/.59	Bfi78=.&2BN7]!F(85?>@,.)789'7
~ 5?Bfi6P>\5?P78,Bfi&65?B9%578.78(*=.9%&65?/KAY	:.5.=.0?:d&"/T9;:.785?BN[&69G/.7878P#G&2/4I/.&69%7@78?78(*=.9%&65?/T78!>,.678#G9%5
(85?/437*Bfi07G9'5]9S:.7<5?,.9%&">!+=/.&637*BN#;!$,.2!4/K
|M6,Bfi783.&65?=.#A!,,BN5?!$(8:.78#E9%5F=/.&)347*Bfi#;!$,.2!4//.&2/.0
d78.(87*,.9	 &">!$9%9%&789 !$nKafio*pp4!d?oppOr8d.=.#%7
!/78,.6&6(8&69]Bfi7*,Bfi78#'7*/9;!+9%&65?/C5$DE9;:.7F=/.&)347*Bfi#;!$,."!/ab7K 0K"d  #;rK	Y	:?=.#d&"/19S:.7<07*/.7*B%!+(*!$#%7d!/
fiR

fi  0



)-  
 -  -)

78,5?/.7*/49%&"!$#%&687H5$D-9;:.7<,."!/C&"/19S:.7`/.=>FO7*B 5$D,Bfi5?,54#%&69%&65?/.#AP7SI/.&"/.01!HP5?>!$&"/C#%9;!+9%7`>F=.#%9	O^7
78,78(89%78Pd!$#A!BN0?=.78PUO[1G&"/.#;O^7*Bfi0Qafio*pp4r8K
Y:.7<(85?>,!+(89<!/.P&2>@,.)&6(8&69Bfi7*,Bfi78#%7*/49;!$9'&)5./15$D	=/.&)347*Bfi#;!$,."!/.#<5?O.9S!$&"/.78P~&)9S: h  ii #<P5.78#
/.59F/.78(878#%#;!Bfi&66[Q0?BN5~ 78,5./.7*/9%&"!$66[zDX5?BGBN780?=."!Bfi6[Q#%9;B'=.(89;=Bfi78PQP5?>@!$&"/.#\!+#G#;:.5$~A/QO4[_&2>@!$9%9%&
789<!$nKafio*pp4!r8K =BN9;:.7*B8d9;:.7 h  ii RO!+#%78PBfi7*,Bfi78#'7*/9;!+9%&65?/T5$DE9;:.7	| 5+D!1/.5?/4RP789%7*B%>F&"/.&6#%9%&6(
P5?>!+&2/7*/!O.678#G9;:.7!,,.6&6(*!$9%&65?/T5$DE7S(8&67*/49A#%7*!4Bfi(V:T!$605?Bfi&69;:>\#	DBfi5?>>\5?P78Z(V:.78(84&2/.01(*!4,!O.67
5$DE:!/.P6&"/.0137*Bfi[C2!4Bfi07G#%9;!+9%7A#S,!$(878#*K
S '

  UTVWXW'

|G/wGBfiP7*Bfi78P&"/!Bfi[mG78(8&6#%&65?/G&"!$0.B%!> an Bfi[.!/9d	o*p4rF&6#W!z(*!/.5?/.&6(*!$GBfi7*,Bfi78#%7*/49;!$9'&)5./m5$D<!
O5.567*!/WD=/.(89%&65?/~	&69;Z
: YT6&"/.7*!BM5?BfiP7*BN78PU!Bfi0?=>F7*/49%#\[ #] [ (^]_`_a_`] [/b?K
|G/ h  ii &)#E!MBfi5.59%78PdP&"Bfi78(89%78P!$(8[.(8)&6(	0.B%!,:G~&)9S:<5./.7E5?B9fi~ 5	9%7*B'>F&"/!$/.5.P78#5$D5?=.9NRP780?Bfi787
87*Bfi5H2!4O78678PQo	5?"
B Pd?!4/.P!G#%7895$D3!BN&2!4O.)7</.5.P78d
# c@5$D5?=.9fiRP780.Bfi787A9fi~ 5KY	:.7M9fi~ 5G5?=.9%05&"/.0H78P078#
!Bfi7C0&637*/O[Q9S:.7CD=/.(89%&65?/.f
# e?JK eOa crF!/.P  K g<Oa c
r8h
K N!$(8:3!Bfi&"!O.67/.5?P71&6#!$#'#%5?(8&"!$9'78Pm~	&69;:m!
,Bfi5?,^5#%&69%&65?/!$3$!Bfi&"!O.67&"/9;:.7O^5?567*!/D=/.(89%&65?/9;:.7 h  ii BN7*,Bfi78#%7*/49%#*K<Y	:.7F0?B'!,:1&6#<5.BfiP7*Bfi78P
&"/19;:.7G#'7*/.#%7<9;:!+9_!+),!$9;:.#&2/9;:.7G0?B%!4,:1Bfi78#;,78(899;:.7<5?BNP7*Bfi&"/.05$D9;:.7G3!BN&2!4O.)78#K
|G/ h  ii Bfi7*,BN78#%7*/9'&2/.0C9;:.7D=/.(89%&65?f
/ iOa [ #] [ ( X
r jU[ #lk [ ( &6#G#;:.5$~A/C&"/&)0.=Bfi7oK G&637*/T!/
!$#%#'&)0./>F7*/95$D9S:.7<!Bfi0?=>F7*/49%
# [ # !/.Z
P [ ( d^9;:.7G3!$"=.7F5$X
D iT&)#MP789%7*B%>\&2/.78PQO4[W!@,!$9;:C#%9;!4Bfi9%&"/.0!$9
9;:.7GBfi5.59	/.5?P7<!/.P1&69%7*B'!$9%&63786[Dn5665$~	&"/.09;:.7G:.&)0.:178P07d.&JD9S:.7`!$#'#%5?(8&"!$9'78P13!Bfi&"!O.67H&)#9;B%=.7d!/.P
9;:.7G65$~78P07d&JDE9;:.7<!+#%#%5.(8&2!+9%78P13!BN&2!4O.)7F&6#D!$6#%7KY	:.7G3!$"=.7H5$J
D iT&6
# m ( , 5.o
B n   , &JD9;:.7G"!O78
5$D9;:.7<Bfi7*!$(8:.78PU9'7*B%>F&"/!$-/.5.P7G&6#]oG5.\
B PdBfi78#;,^78(89%&63786[K
x1
x2
0

1

&60?=Bfi7oKp|`/ h  ii Bfi7*,Bfi78#'7*/9%&"/.0C9;:.7GD=/.(89%&65?/7iaO[ # ] [ ( rjq[
PB%!~A/C!$#	#%56&6P!4/.PUP59%9'78P1)&"/.78#*dBfi78#;,^78(89%&63786[K

# k

[ (

K	&)0.:U!/.PT)5$~}78P078#G!Bfi7

| / h  i 0?B%!,:\&)#	Bfi78P=.(878PC#%5H9;:!$9/.5H9fi~5GP&6#%9'&2/.(89M/.5?P78#\cC!/.PsrF:!37	9S:.7	#;!>F7M3!4Bfi&"!O.67
G
/!>F7C!/.P65$~ !4/.P:.&60?:z#;=.(8(878#%#%5.Bfi#Uab&60?=Bfi7{!4r8d-!/.Pm/.5U3$!Bfi&"!O.67U/.5.P7sc:!$#F&6P7*/9%&6(*!$ 65$~
!/.PU:.&60?:#;=.(8(878#%#%5?BN#\ab&60?=BN7<{Or8K
Y:.7 h  ii Bfi7*,Bfi78#%7*/49;!$9'&)5./Q:!$#G9fi~ 5U>!$fi5?BM!$P3!/49;!$0478# pG&"Bfi#%9*d&69<&6#!/7S(8&67*/49]Bfi7*,Bfi78#'7*/4R
9;!$9'&)5./5$D`O^5?567*!/D=/.(89%&65?/.#O78(*!4=.#%719;:.71/?=>FO7*BH5$DA/.5.P78#F5$DX9'7*/Q&6#>F=.(V:z#;>!$667*BH9;:!/z9;:.7
/.=>]O^7*BM5$D9;B%=.9;:T!$#%#%&60?/>F7*/49%#M5$DE9;:.7H3!Bfi&"!O.678#*KHY	:.7]/.=>FO7*BM5$DE/.5?P78#G(*!4/10?Bfi5$~78,5./.7*/9%&"!$
~	&69;:@9;:.7</.=>]O^7*B5$D-3!4Bfi&"!O.678#*dO=.9	>F5#%9(85?>>F5./.)[C7*/.(85?=/49%7*Bfi78P@D=/.(89'&)5./.#A:!37`!FBfi7*!+#%5?/!O.67
Bfi7*,Bfi78#'7*/9;!+9%&65?/K.78(85?/.Pd!/[F5?,^7*B%!$9%&65?/\5?/F9fi~ 5 h  ii #*d.(85?B%BN78#;,5?/.P&"/.0F9%5F!<O5.567*!/@5?,7*B'!$9%&65?/
5?/F9S:.7 D=/.(89'&)5./.#9S:.78[\BN7*,Bfi78#%7*/49*d?:!$#E!65~(85?>,.678.&)9fi[O^5?=/.P78P1O[F9;:.7A,BN5?P=.(895$D9;:.78&"BE/.5.P7
(85?=/49%#]an Bfi[.!/9dopr8K
fit

fi

0J   

u

v

u

x

x

x

(b)

(a)

&60?=Bfi7<{p  87 P=.(89%&65?/.#H5$D h  ii #*K1an!r/.5?P78#F!$#%#%5.(8&"!$9%78P9%5C9;:.7F#;!>F7H3!Bfi&"!O.67~&)9S:78=!+-65$~
!/.PU:.&60?:1#S=.(8(878#%#%5?Bfi#~	&6)O7H(85?/437*Bfi9%78P19%5C!<#'&2/.04)7/.5.P7K`aOr/.5.P78#M(*!=.#%&"/.0WBN78P=/4R
P!/499'78#%9%#	5?/1!\3!Bfi&"!O.67F!Bfi7G786&">F&"/!$9%78PK
| P &)#S!$P3!/49;!$047A5$D h  ii #	&6# 9;:!$9 9;:.7M#%&687G5$D-!/ h  ii Bfi7*,Bfi78#'7*/9%&"/.0#'5?>F7D=/.(89%&65?/C&6# 37*Bfi[
P7*,7*/.P7*/49F5?/9;:.7@5?BfiP7*Bfi&"/.0U5+D9;:.73$!Bfi&"!O.678#*K1Y51I/.P!/5?,.9%&">!$3$!Bfi&"!O.675.BfiP7*Bfi&"/.0U&6#]!(85$R
MuZR(85?>,.6789%7,Bfi5.O.)7*> &"/&)9'#%78JDNdO=.9G!$#M&)6"=.#%9;B'!$9%78P&"/U&60?=BN7]C!05.5.P:.7*=Bfi&6#%9%&6(HDn5?B	(8:.5.5#%&"/.0
!/C5?BNP7*Bfi&"/.0&6#	9%5@)5.(*!$9%7FBfi78"!$9%78P13$!Bfi&"!O.678#</.7*!B 7*!$(8:C59;:.7*BHan "!B%$7G789M!$nK"do*ppprK
x1

x1

y1

x2

x2
x3

x2

x3

x3

y1

y2
y2

x3

y3

y1

y1

y1

y2

y3
1

0

x3

1

0

(a)

(b)

&60?=Bfi7<pMY	:.&6#<&60?=Bfi7C#;:.5$~	#G9;:.7C7S78(89<5$D	3$!Bfi&"!O.675.BfiP7*Bfi&"/.0TDX5?BM9S:.778,Bfi78#%#%&65?/aO[ #lkvu# r&w
Oa [ ( k7u ( &
r wmOa [ % k7u % r8KY:.7 h  ii &"/an!4rM5?/.6[0.Bfi5$~	#G6&2/.7*!4Bfi6[~	&69;:9;:.7/.=>]O^7*BG5$D
3!BN&2!4O.)78#C&"/9;:.7178,Bfi78#%#%&65?/d~A:.&6679S:.7 h  ii &"/anOrF:!$#C!/Q78,5./.7*/9%&"!$M0?Bfi5$~	9S:K
Y	:.7F78!4>,.67&66"=.#%9;B%!$9'78#<9;:!$9<,."!$(8&"/.0QBfi78"!$9%78Pz3!Bfi&"!O.678#/.7*!BM9%517*!+(V:T59;:.7*B&"/9;:.7
5?BfiP7*Bfi&"/.05+DX9%7*/C&6#G!F05.5?PU:.7*=BN&)#'9%&6(K
h  ii #E:!37EO787*/F#S=.(8(878#%#fiD=.)6[!,,.6&678P9%5G>F5.P78?(8:.78(V4&"/.0K/F>F5.P78?(8:.78(8&"/.0G9;:.7 O7*:!3?&65?B
5$D-!H#%[.#%9%7*> &6#	>F5.P786)78PO[C!MI/.&69%7G#%9;!+9%7A!=.9%5.>!$9%5?/H~	&69;:@9;:.7M9;B%!/.#'&)9'&)5./Bfi78"!$9'&)5./Bfi7*,BN78#%7*/9'78P
!$#	!/ h  ii KEG78#%&"B%!4O.)7<,BN5?,7*Bfi9'&)78#G!Bfi7M(8:.78(V$78PUO4[1=.#%&"/.0 h  ii >!/.&",=."!$9%&65?/.#M9%5!/!$6[?87H9;:.7
#%9;!+9%7A#S,!$(87<5$D9;:.7G#%[.#%9%7*> an "!B%$7M789A!$nK"do*p.(8&6)"!/dZo*pp4r8K
fiyx

fi  0



)-  
 -  -)

/9'7*Bfi78#%9%&"/.06[dG!#%&">F&6"!BU!,,BN5?!$(8:m(*!/O7=.#%78PDX5?B@#%563.&"/.0/.5?/4RP789%7*B'>F&"/.&)#'9%&6(,."!//.&"/.0
, Bfi5?O.67*>F#K|A# !/@78!>@,.)7d.(85?/.#'&)P7*B_9;:.7A|Bfi7*,Bfi78#%7*/49;!$9'&)5./F5$D-!</.5?/4RP789'7*B%>F&"/.&6#%9%&6(],."!//.&"/.0
P5?>!+&2/C#S:.5~M/&"/1&60?=BN71?!K/C9;:.&6#	P5.>!$&"/C9;:.7*Bfi7<!BN7	Dn5?=B #%9S!$9%78#E04&)347*/UO4[9;:.7MDn5?=B ,5#%#'&2O.67
3!+2=.7!$#'#%&60?/>F7*/49%#	5$D9;:.7G9fi~ 5\O^5?54)7*!4/1#%9;!+9%7G3!BN&2!4O.)78
# [ # !4/.Pz[ ( K /,=.9%#M9%5F9;:.7]|}P7*/.59%7
!$(89%&65?/.#G&"/9S:.7P5?>!$&"/!/.PQ!Bfi7FP7SI/.78PmO[T9;:.7O5.567*!/3$!Bfi&"!O.6s
7 {
KFY	:.7 h  ii Bfi7*,BN78#%7*/9'&2/.0
9;:.7G9;B'!/.#%&69%&65?/1Bfi78"!$9%&65?2
/ |]Oa { ] [ #] [ (] [} # ] [}( r 5$DZ9;:.7<	|}&6#	#;:.5$~A/C&"/1&60?=BN
7 1?OKEY	:.7GP7SI/.&69%&65?/
5$&
D |&6#M#%9;B%!+&)0.:9fiDn5?Bfi~	!BNM
P pDn5?B #%5?>\7<!$#%#%&60?/>F7*/49M5$D&69%#G!Bfi0.=>F7*/9'#*d |&6#A9;B'=.7<&J!$(89%&65?Z
/ {C(*!4=.#%78#
!F9;B'!/.#%&69%&65?/CDBfi5?> 9;:.7<(*=B%BN7*/9M#%9;!$9'7G0&637*/UO4[9S:.7<3!+2=.7F5$X
D [ # !/.z
P [ ( 9%59;:.7F/.78?9M#%9;!+9%7A04&)347*/
O4[19;:.7<3$!$"=.7]5+J
D [ } # !/.7
P [ }( K ( 	59'7<9;:!$99;:.7 h  ii BN7*,Bfi78#%7*/49%&"/.%
0 | DX5.B 9;:.7<78!>,.67F9;=B%/.#5?=.9
/.59	9'5]P7*,^7*/.P5?
/ [ }( K
a

0

00
1

10

0

01

1

11

1

x1

x1

x1

x1

x2

x2

1

0

(a)

(b)

a

P1
00

1

G
0

x1

01

x1
x2

1

10

0

1

(d)

(c)

&60?=Bfi71p|,.2!4//.&2/.0zP5?>!$&"/Bfi7*,Bfi78#%7*/49%78Pm!$#!/Q| &6##S:.5~M/&"/an!4r8K1.9;!$9%78#F!Bfi7CP7SI/.78P
O[O5.567*!/#%9S!$9%7<3$!Bfi&"!O.678)
# [ # !/.v
P [ ( d!/.P9;:.7!$(89%&65?/T&"/,=.9F9%5C9;:.7	|&)#G04&)347*/
O[@9;:.7`O^5?54)7*!4/13!4Bfi&"!O.6)
7 {
KY:.7G#%[>FO^56&)(FBfi7*,Bfi78#%7*/49;!$9'&)5./5$DZ9;:.7G9;B%!/.#'&)9'&)5./Bfi78"!$9'&)5./
5$D9;:.7	| &6#G#;:.5$~A/&2/manOrKA/mab(*rd ~ # &6#G9;:.7H#%789A5+DE#%9;!$9%7F!$(89%&65?/T,!$&"Bfi#MDX5.B ~A:.&6(V:d
78?78(*=.9'&)5./U5+D9;:.7F!$(89%&65?/T(*!/T67*!$P9%59S:.7]045?!$nKGY	:.7F#%[>FO^56&)(CBfi7*,Bfi78#%7*/49;!$9%&65?/T5$Dd~ #
&6##S:.5~M/F&"/QabPr8Kn9&)#5?O.9;!$&"/.78P@DBfi5.> 9;:.7M9;B%!/.#'&)9'&)5./Bfi78"!$9'&)5./O4[Bfi78#%9SBfi&6(89%&"/.0]9S:.7`/.78.9
#%9;!$9'7A9%5
5 Po4K
|M#%#;=>\79S:!$9-9S:.7#'9;!$9%
7 PoE&6#E!M05.!$?#'9;!$9%"
7 K|$78[<5.,7*B%!$9'&)5./d$~A:.7*/H07*/.7*B%!$9%&"/.0<!G=/.&637*Bfi#S!$
., "!/DX5.B	!$(8:.&)783.&"/.0d&6#M9%5FI/.PQ!$69;:.7H#%9;!$9'7<!$(89%&65?/U,!+&2BN#\aO ] {.r #;=.(8:19;:!+9\(*!/TO7Bfi7*!$(8:.78P
DBfi5?><O4[78.78(*=.9%&"/.0{KEY:.&)##%789	&6#	"!O78678Pv~ # &2/&60?=Bfi71(KY5FI/.Pz~ # DBfi5?>|~ 7G(85?/.#'9;B%!$&"/
[ } 9%
5 n   , !/.
P [ }( 9%
5 m ( , &"7
/ |HKY	:.&6#Bfi78P=.(878

# |9%519;:.7 h  ii #S:.5~M/U&"/z&60?=Bfi751PKCY	:.7
#
Bfi78#;=.69%&"/.0 h  ii Bfi7*,Bfi78#%7*/49%5
# ~ # ~&)9S:9;:.71#%9;!$9'78#]P78#'(*Bfi&"O78P&"/m9;:.7C(*=B%Bfi7*/49#%9;!+9%73$!Bfi&"!O.678
# [ #
$;b$Sn;b'<cff8\K'K--fi	*HHS&fi
	$nF"%	*n*-'8nnSnX%'nHfifiX<HS
n$;n<b&$	 ;nSn;b' fi	 nZbfiS 'MSn;b'<c< y* n$8 c
fi

fi

0J   

!/.PQ[ ( KZ$504&)(*!+)6[~ 7,7*BDn5?B%>\78P9;:.7@5?,7*B%!+9%&65?/h[
Bfi7*,Bfi78#'7*/9%&"/.0~ # K

} ] [ } _ [ } k
#
(
#

[ } k
(

|9%5T5?O.9;!$&"/9S:.7

h  ii

  '6
5AX6'Uv6 -6 q
   J
  '
/9;:.&6#<#%78(89%&65?/d~ 7<IBN#%9AP&6#%(*=.#'#]9S:.7\,BN5?,7*Bfi9'&)78#H5$
D "!#O!$#'78P5./!4/&"/4DX5?B'>!$ZP7SI/.&)9'&)5./Q5$D
9;:.7C"!/.0?=!$07U!4/.P!TP5?>!+&2/7*/.(85.P&"/.0Q78!>,.67KQxQ719S:.7*/QP78#%(*BN&2O^7U9;:.7@Dn5?B%>!$#%[/9;!+!/.P
#%7*>!4/9%&6(8#	5+d
D "!#K
|G2
/ /"!#QP5?>!+&2/@P78#%(*Bfi&",.9%&65?/(85?/.#%&6#%9%# 5$HD p-!HP7SI/.&69%&65?/C5$DV   ,* S + *8- , 8d?!P78#%(*Bfi&",.9%&65?/C5$D
8*8 , !/.P ,    fi ),    , 
bVd!/.PU!F#S,78(8&JI(*!$9%&65?/T5$DE!/bbn  !/.P1.  ; bn 
K
Y:.7G#%789	5$D-#'9;!$9%7M3!BN&2!4O.)7!$#'#%&60?/>F7*/49%# P7SI/.78#A9;:.7H#%9;!$9'7A#;,!+(87G5$D9S:.7GP5?>!$&"/K|G/1!$07*/49 Lc#
P78#%(*Bfi&",.9%&65?/H&6#-! #'7895$D  *nb8K-Y:.7E!$07*/49%#(8:!/.079;:.7#%9S!$9%75$D9;:.7~ 5?Bfi6P<O4[`,^7*BDX5.B%>F&"/.0`!+(89%&65?/.#
9;:!$9 !Bfi7G!$#%#;=>\78P19%5]O^7G78?78(*=.9%78P1#'[/.(8:Bfi5?/.5?=.#')[1!/.PC9%5:!37`!I.78PW!4/.P178=!$P=B%!+9%&65?/KZ|	9
7*!$(8:Q#'9%7*,d!$65+DA9;:.71!$07*/49%#],^7*BDX5.B%> 78!$(89')[z5?/.7W!+(89%&65?/d!/.Pm9;:.71Bfi78#;=.69%&"/.0!$(89%&65?/z9;=,.67C&)#
! V4 
  n 
KY	:.71#'[?#%9'7*> !+07*/9'#\>\5?P78	9S:.7UO7*:!3?&65?B\5$DA9;:.7U!$047*/9%#F(85?/49;Bfi54)"!O.67O[9;:.7

,."!//.7*B8d ~A:.&6679;:.717*/43?&"Bfi5?/>\7*/9!+07*/9'#>F5.P78	9;:.7U=/.(85?/49;Bfi56"!O.67~ 5?Bfi6PKm| 3!$6&6PP5?>!$&"/
P78#%(*Bfi&",.9%&65?/ Bfi78=.&"Bfi78#19S:!$99S:.7U#%[.#%9%7*> !/.P7*/3.&"Bfi5?/>F7*/491!$07*/49%#(85./.#%9;B%!$&"/!QP&6#nfi5&"/91#'7895$D
3!4Bfi&"!O.678#*K
|G/!$(89%&65?/:!$#19;:BN787,!Bfi9%^# p!z#%78915$D8   ,* S + *8- , 8dA!   , ; bn DX5?B'>]=."!dM!/.P!/
,Oo, Dn5?B%>F=."!K]/9S=.&)9'&)3478)[z9;:.7!$(89%&65?/T9;!$78#GBfi78#;,^5?/.#%&"O.&66&"9fi[Q5$D(85./.#%9;B%!$&"/.&"/.0U9S:.7]3$!$"=.78#<5$D 9;:.7
#%9;!+9%7F3!Bfi&"!O.678#F&"/9;:.7/.78.9<#'9;!$9%7KFn9D=Bfi9S:.7*BG:!$#<78.(8"=.#%&637W!+(8(878#%#<9%519S:.78#%73!4Bfi&"!O.678#]P=BN&2/.0
78.78(*=.9%&65?/K/5.BfiP7*B@DX5.B9;:.7!$(89%&65?/9%5wO7!,,.6&)(*!4O.)7dH9;:.7Q,Bfi78(85?/.P&69%&65?/Dn5?B%>F=."!m>]=.#'9WO^7
#;!$9'&)#NI78PU&"/T9;:.7<(*=B%BN7*/9M#%9;!$9'7KY	:.7F7S78(89	5$DE9S:.7]!$(89%&65?/&6#AP7SI/.78PO4[C9;:.7<7S78(89DX5?B'>]=."!@9;:!$9
>F=.#%9<O7@#;!$9%&6#fiI78P&"/9;:.7/.78.9G#%9;!+9%7K<Y5U!$6)5$~(85?/.P&69%&65?/!$7S78(89%#*d9;:.7@7S78(89G78,Bfi78#%#'&)5./(*!/
Bfi7SDn7*B	9'5O59S:1(*=B%Bfi7*/49G!/.P/.78.9A#'9;!$9%7H3!Bfi&"!O.678#*d~M:.7*Bfi7<9;:.7F/.78?9M#%9;!+9%7G3!BN&2!4O.)78#</.7878P9%51O7F!
,!Bfi9Z5$D9;:.7 #%789Z5$D(85?/.#'9;B%!$&"/.78PF3!BN&2!4O.)78#5$D9S:.7!$(89%&65?/KZ|M6/.78?9#%9S!$9%73!4Bfi&"!O.678#/.59Z(85?/.#%9;B'!$&"/.78P
O4[W!/4[1!$(89%&65?/C&"/U!Mfi5&"/49A!$(89%&65?/T>!$&"/9S!$&"/C9;:.78&"B	3$!$"=.7K
Y:.7	&"/.&)9'&2!+!4/.P05?!$.(85?/.P&69%&65?/1!BN7EDn5?B%>F=."!$#9;:!$9E>F=.#%9EO7M#;!$9'&)#NI78P&"/9;:.7&2/.&69%&"!$#%9S!$9%7	!/.P
9;:.7MI/!$#%9;!$9%7dBfi78#S,78(89%&63786[K
Y:.7*Bfi7!Bfi7F9fi~ 51(*!=.#%78#DX5?BG/.5./4RP789%7*B%>F&"/.&6#;> &"Q
/ "!#QP5?>!$&"/.# p1afio*rG!$(89'&)5./.#]/.59<Bfi78#'9;Bfi&6(89fiR
&"/.0!$6 9;:.78&"BF(85?/.#%9SB%!$&"/.78Pm3!BN&2!4O.)78#@9%5Q!U#S,78(8&JI(U3$!$"=.7U&"/9;:.71/.78?9F#%9;!+9%7d!/.Pan{r9;:.7U/.5?/4R
P789%7*B%>\&2/.&6#%9'&)(F#%78678(89%&65?/T5$DZ7*/3.&"Bfi5?/>F7*/49`!+(89%&65?/.#*K
|#%&">,.67C78!4>,.675$DA!
/ /"!#P5.>!$&"/P78#%(*Bfi&",.9%&65?/&6#F#;:.5$~A/T&"/Q&60?=BN7WqKCY	:.7CP5?>!$&"/
P78#%(*Bfi&"O^78#\!T,."!//.&"/.0,BN5?O.67*> Dn5?BG.(V:.5.,,7*Bfi# L afio*p4rABN5?O59NRO!O[TP5?>!$&"/KCY	:.7P5?>!+&2/z:!$#
9fi~ 5G#%9;!$9%7 3$!Bfi&"!O.678# pE!H/?=>F7*BN&)(*!+5?/.7d,54#%&69%&65?/  $	~	&69;:CB%!/.0F
7 P ] o ] { ]   !/.P!<,BN5?,5#'&)9'&)5./!$
# # 8   * A!/.P
5?/.7dfi *  g *.KEY	:.7GBfi5?O549&6#	9S:.7A5?/.6[C#%[.#%9%7*>!$07*/49!/.P1&69	:!$# 9fi~ 5]!$(89%&65?/.o
#K
 g ,    * KUY:.7
7 # V   y Uan!/.Q
P #K g , H   y 8r<!$(89%&65?/:!$#!T(85?/.P&69%&65?/!$ 7S78(89FP78#%(*Bfi&"O78P
O4[!/T&JD R9S:.7*/4R78)#'75?,^7*B%!$9%5.BOa myr p	&JD<fi *  g *&6#G9;B'=.7]9S:.7*
/ # 8   *y G&"/.(*Bfi7*!$#'78#<9;:.7O.65?(8
,54#%&69%&65?/mO[z5?/.7786#%7C9;:.71O.65?(8,^5#%&69%&65?/&6#=/.(V:!4/.078PKQY	:.71O!O[z&)#F9;:.7C5?/.6[7*/3.&"Bfi5?/>F7*/49
!$07*/49!4/.P&69_:!+#E5?/.7G!$(89%&65?A
/ Gb  * nK78(*!=.#%7M7*!$(8:!+07*/9 >F=.#%9E,7*BDn5?B%>78!$(89%6[F5?/.7G!$(89%&65?/
!$9Z7*!$(8:<#'9%7*,d$9;:.7*Bfi7	!Bfi79fi~5fi5&"/9!$(89%&65?/.#Aa # V   y d `b   * 4"r!/.PUa #K g ,    * -d Gb  * "rK
/.&69%&"!$66
[ >y^ >> &6#!+#%#;=>F78P9'5<O7	9;B'=.7d9;:.7	Bfi5?O^59&6#!$#'#;=>F78PF9%5<:.56P1!GO.)5.(8]!+9EuZ5#%&69%&65?/
Pd!4/.P1&)9'#	9;!$#;@&6#	9%56&JDn9A&69A=,9%5uZ5#%&69%&65?/1^K
Y:.73$!Bfi&"!O.6
7 >y^ o>_(*!/O7M>!$P7 D!$6#%7AO4[<9;:.7MO!O[KY	:.7MO!OC[ Lc#E!$(89'&)5.A
/ `b   * .&)#
/.5?/4RP789%7*B'>F&"/.&)#'9%&6(d!+#&69-5?/.6[H(85?/.#%9;B%!+&2/.# fi *  g *	O4[A9;:.77S78(89-78,Bfi78#%#%&65?/  fi *  g4H I
 
fi

fi  0



)-  
 -  -)

>KyKCK
< xK 

M+  i
H>
K+^ 
 ""8
 
  
y
 X t
Cy+ 
 &?+
^X   V

'

fi^ H[

&

fi^ H[

8u

  
y
Cy+ X 
 &?+
^X  V
>y+C>
K+^ 6
 
<
 
 
y  V

Cy+ 
 &
^X
  V
Ka
J &  V
 
+K

J

  V 

t

&60?=Bfi7<qp|G/f"!'#P5?>!$&"/CP78#%(*BN&2,.9'&)5./K
 fi *  g* } KCY	:.=.#*d~A:.7*/  *  g*&6#H9;B%=.7&"/z9;:.7F(*=B%Bfi7*/49<#%9;!+9%7d9;:.7F7S78(89<78,Bfi78#%#'&)5./
5$D\G  * P5?78#</.59G!,,.6[d!/.P *  g*F(*!/T78&)9S:.7*BGO7F9;B%=.7H5?B D!$6#%7H&2/T9;:.7/.78.9A#'9;!$9%7K
G/9;:.7F59S:.7*BG:!/.Pd&JD<fi * 4 g*&6#GD!$6#%7&"/9S:.7(*=B%Bfi7*/49G#%9;!$9'7d`b  * 4+787*,.#<&69MD!+)#'7&"/
9;:.7/.78.9<#%9;!+9%7K<Y	:.A
7 G   * E!$(89'&)5./>\5?P786#]!4/!+#;,78(89H5$DE9;:.77*/43?&"Bfi5./>F7*/9</.549G(85?/9SBfi56678P
O4[m9;:.7QBfi5?O549!$07*/49*d &"/9;:.&6#1(*!$#%7!O!O4[dAO4[m&69%#17S78(89%#5./fi *  g *.K/9;:.7T78!4>,.67
!O^5347dEfi *  g *C#%9;![.#MD!$6#%7@~A:.7*/Q&69F:!$#]O^78(85?>F7@D!$6#%7dBfi7S78(89'&2/.09;:!+9G9;:.7WBN5?O59G(*!4//.59
#;,^5?/9S!/.785?=.#%6[UO7I?78PUO4[U!F:.&69A5$D9;:.7<O!O4[d5?B!/[C59;:.7*B !$(89'&)5./1&"/19;:.7G7*/43.&2BN5?/>F7*/49*K
|G/m| Bfi7*,Bfi78#%7*/49%&"/.09;:.71P5?>!$&"/&6##;:.5$~A/&"/&60?=Bfi7KY:.71(*!$6(*=.2!+9%&65?/5$DA9;:.7U/.78.9
#%9;!+9%7<3$!$"=.75$D  $&"/9;:.A
7 # 8   * !+(89%&65?/T#;:.5$~	#G9;:!$9G/?=>\7*Bfi&6(*!$E3$!Bfi&"!O.678#<(*!/O^7=,P!$9'78P
O4[!/Q!4Bfi&69;:>F789%&6(78,Bfi78#%#%&65?/5?/9;:.7F(*=B%Bfi7*/49<#'9;!$9%7H3!Bfi&"!O.678#*KCY	:.7=,^P!$9%778,Bfi78#%#%&65?/5$D  $
!/.PC9;:.7<=.#%7M5$D-9;:.7G&JDbR9;:.7*/4R786#%7<5.,7*B%!$9'5?BZD=Bfi9;:.7*BP7*>F5?/.#'9;B%!$9%7M9;:.7G!$P3!4/9;!+07A5+D=.#%&"/.078,.)&6(8&69
Bfi7SDn7*Bfi7*/.(878#F9%5U(*=B'Bfi7*/9F#%9S!$9%7!/.P/.78?9F#%9;!+9%73!4Bfi&"!O.678#&"/Q7S78(89F78,BN78#%#%&65?/.#*
K "!'#QP5.78#/.59
Bfi78#%9SBfi&6(89	9;:.7<Bfi7*,Bfi78#'7*/9;!+9%&65?/1O[C7*/4Dn5?Bfi(8&"/.0W!\#%9;B%=.(89;=BN7G#%7*,!B%!$9'&2/.0@(*=B%Bfi7*/49	#%9;!$9'7`!/.P/.78?9#%9;!$9%7
78,Bfi78#%#%&65?/.#*KY	:.7&JD R9;:.7*/4R786#%75?,7*B%!+9%5?B:!$#O787*/F!$PP78PF9%5#;=,,5?BN9-(85?>@,.)78H(85?/.P&69%&65?/!$.7S78(89%#
9;:!$9 5$Dn9%7*/C!Bfi7M7S(8&67*/9')[U!4/.PU/!$9;=B%!$66[1Bfi7*,Bfi78#%7*/49%78PU!$#A!H#%789 5$D/.78#%9%78PU&JDbR9;:.7*/4R786#%7<5.,7*B%!$9'5?Bfi#*K
Y:.7A78,.6&)(8&69<Bfi7*,BN78#%7*/9S!$9%&65?/@5$D-(85?/.#%9SB%!$&"/.78P1#%9;!$9'7	3!BN&2!4O.)78#M7*/!O.678#G!/[C/.5?/4RP789%7*B%>F&"/.&6#%9%&6(
5?BMP789%7*B'>F&"/.&)#'9%&6(7S78(89<5+D_!/!$(89%&65?/9%5UO7BN7*,Bfi78#%7*/49%78Pd-!$#G9S:.7(85?/.#%9;B'!$&"/.78PQ3!BN&2!4O.)78#F(*!/O^7
!$#%#'&)0./.78P19%5]!/4[F3!$"=.7H&2/@9;:.7G/.78?9 #%9S!$9%7M9;:!$9E#S!$9%&6#fiI78# 9;:.7M7S78(89Dn5?B%>F=."!Kn9D=Bfi9;:.7*B9;=B%/.#5?=.9
9%5:!37<!\(8)7*!4B &"/9;=.&69%&637>F7*!4/.&2/.0
d!$#9;:.7<!$(89%&65?/9;!$78# 9;:.71;BN78#;,5?/.#'&2O.&66&69fi[?T5$D#S,78(8&JDX[.&"/.019;:.7
3!+2=.78#M5$D9;:.7G(85?/.#%9SB%!$&"/.78PU3$!Bfi&"!O.678#G&"/19;:.7</.78.9	#%9;!+9%7K
fi

fi

0J   

robot_works
false

true

G

I

0

G

1

2

3

pos

&60?=Bfi7<pMY	:.7|5$D	9;:.7WBN5?O59NRO!O[TP5?>!$&"/ab#%787C&60?=Bfi7qrKWY:.7*Bfi7&6#F5?/.7W,BN5?,5#'&)9'&)5./!$
!/.P5?/.7Q/?=>\7*Bfi&6(*!$<#%9S!$9%7U3$!Bfi&"!O.6K7 p} *  g *!/.P  +?K Y:.7a # 8   * -d G  
 * "rZ!/.PWa #<
 g ,    *y $-d G   * "r.fi5&"/9!$(89%&65?/.#E!4Bfi7EPB%!~A/~	&69;:G#'56&)PC!/.P<P!$#S:.78P
!B%Bfi5$~	#d?Bfi78#;,^78(89%&63786[K.9S!$9%78#A>!4B%$78PC~	&69;:;nC!/.Pm;G!BN7G&2/.&69%&"!$!/.P105.!$#%9S!$9%78#*K

_5?>,!Bfi78P9%5C9;:.7!+(89%&65?/TP78#%(*Bfi&",.9%&65?/z2!4/.0?=!$07 an78Dn5?/.PQv$&JDn9%#%(8:.&69%d o*pprM!/.P<
9;:!$9F!Bfi7C9;:.7C5?/.6[,Bfi&65?BH"!/.0?=!+078#=.#%78PDX5.B h  ii RO!+#%78P,."!//.&"/.0abG&Q!/.85789\!+XKedopp
 &">!$9'9%&	789U!$nK"dGo*p4p!d	oppOd	oppr8.
d "!'#&"/9;BN5?P=.(878#U!/ 78,.6&6(8&697*/3.&"Bfi5?/>F7*/49U>F5.P78XdM!
>F=.69%&JR!$07*/49P78(85?>,^5#%&69%&65?/d	!/.Pw/.=>F7*Bfi&6(*!$	#'9;!$9%7C3!BN&2!4O.)78#K9(*!/zD=Bfi9;:.7*BO^71#;:.5$~A/9;:!$9
"!'#(*!/1O7F=.#%78PU9'5\>\5?P78-!4/[CP5?>!$&"/9;:!$9 (*!/UO^7<>F5.P78)678PO4[C<
ab#%787G|G,,7*/.P&6U|Gr8K
Y:.7	(85?/.(*=B%Bfi7*/49	!$(89%&65?/.#&22
/ "!'#m!Bfi7A!+#%#;=>F78PC9%5]O^7A#'[/.(8:Bfi5?/.5?=.#')[@78?78(*=.9%78P1!/.PC9%5<:!37
I.78P!/.Pz78=!$P=B'!$9%&65?/K|07*/.7*B'!$Bfi7*,BN78#%7*/9S!$9%&65?/Q!$665$~	&"/.0,!BN9%&"!$6)[z5347*Bfi"!,,.&"/.0!+(89%&65?/.#
!/.P!+(89%&65?/.#~	&69;:FP&J7*Bfi7*/49P=B%!$9%&65?/.# :!$#EO787*/!354&)P78Pd.!$#&69Bfi78=.&"Bfi78#>\5?Bfi7 (85?>,.678F9%7*>,^5?B%!$
,."!//.&"/.0wab#%787F7Kc0K"d 
h Nf+- 5?B f-g t f$Z dw	=B%BN&)7vYZ!$9%7do*pp^o $7837*B<v  &6(8:!BfiP#*do*p>p 1?rK
G=Bfi5&"/49!$(89%&65?/@Bfi7*,Bfi78#%7*/49;!$9%&65?/F:!$# >F5?Bfi7	Bfi78#'7*>]O."!/.(87M~	&69;:FF
 an !B'!$v G78JDX5./.Pdo*pp4r!/.P
an&2=/.(8:.&606&"!q
v $&JDX#'(V:.&69%doppr8d4~A:.7*Bfi7M#%789%#5$D-!$(89%&65?/.#	!Bfi7G,7*BDn5?B%>\78PW!$97*!$(8:9%&">F7M#%9%7*,K/
(85?/49;B%!$#'99'5]9S:.78#%7]!,,BN5?!$(8:.78#*d.9;:.5?=.0?:d~ 7<>F5.P78->F=.69%&JR!$07*/49AP5.>!$&"/.#*K
|G/@&2>@,5?Bfi9S!/9 &6#%#;=.7G9'5\!+PPBfi78#%# ~A:.7*/C&"/9SBfi5?P=.(8&"/.0(85./.(*=B%Bfi7*/49_!+(89%&65?/.#	&6# #%[/.7*BN0789%&6(G7S78(89%#
O789fi~ 787*/@#%&">]=.69;!4/.785?=.#%6[78.78(*=.9%&"/.0!$(89%&65?/.#GOa $&"/.0?!BfiP1v  &6(V:!BNP#*do*pprK|(85?>>F5./F78!4>,.67
5$DP78#%9;B'=.(89%&637	#%[/.7*Bfi0789%&6(M7S78(89%#&6#~M:.7*/F9fi~5G5.B>F5?Bfi7A!+(89%&65?/.#Bfi78=.&"Bfi7M78?(8"=.#%&637G=.#%7	5$D-!#%&"/.067
Bfi78#%5.=Bfi(87G5?B ~A:.7*/9fi~5!$(89'&)5./.#A:!37G&"/.(85?/.#%&6#%9%7*/49A7S78(89%#)&"$7  $ } j@!/.P  $ } j{^K
7
/ "!#!$(89%&65?/.#M(*!//.59GO7,^7*BDX5.B%>F78PU(85./.(*=B%Bfi7*/49%6[U&"/T9;:.7GDn5665~&2/.019fi~ 5(85?/.P&69%&65?/.^# p<o*r
9;:.78[F:!*347E&"/.(85?/.#%&6#%9%7*/49E7S78(89%#d$5?B{r9;:.78[<(85?/.#'9;B%!$&"/F!/H5$37*Bfi"!,,.&"/.0G#%7895$D#%9;!$9%73!BN&2!4O.)78#KEY	:.7
IBfi#%9(85?/.P&69%&65?/C&6#EP=.7M9%5<9S:.7 D!+(89E9;:!$9#%9;!$9'7_./.5$~	678P07M&)# 78,Bfi78#%#%78PC&"/!H>F5?/.59'5?/.&6(	650&6(G9;:!$9
(*!//.59FBfi7*,Bfi78#%7*/49<&"/.(85?/.#'&)#'9%7*/9C?/.5$~	678P07K1Y	:.7#'78(85?/.P(85./.P&)9'&)5./m!$PPBfi78#%#%78#H9;:.7,Bfi5.O.)7*> 5$D
#;:!BN&2/.0Bfi78#'5?=Bfi(878#*K 5?/.#'&)P7*BDn5?B78!>,.67M9fi~5F!$07*/49%#9;Bfi[.&"/.0F9%5<PBfi&"/F9S:.7A#;!4>F7	0"!$#%#5$D~	!$9%7*B8K
bDF5?/.6[9;:.7TIBfi#%91(85?/.P&69%&65?/P7SI/.78P}&"/9'7*BDX7*BN&2/.0w!+(89%&65?/.#*dGO59S:!+07*/9'#(85?=.6P}#%&">F=.)9S!/.785?=.#%6[
7*>,.9fi[9;:.7M0"!$#%#d?!$# 9;:.7M7S78(89E 6 S , nF5$D-9S:.7A9fi~ 5<!$(89%&65?/.# ~ 5?=.6PUO7M(85?/.#%&6#%9%7*/49*Kx&69;:@9;:.7
#%78(85?/.P@(85?/.P&69%&65?/!$PP78Pd49;:.78#%7A!$(89'&)5./.#E!Bfi7 &"/9%7*BDX7*Bfi&"/.0F!/.PF(*!//.59EO7M,7*BDn5?B%>F78PF(85?/.(*=B'Bfi7*/9')[K
Y:.7U(*=B%Bfi7*/49137*Bfi#'&)5./w5+5
D /"!#Q5?/.6[!35&6P#P78#'9;B%=.(89%&637#%[/.7*Bfi0789%&6(7S78(89%#*K n9CP5.78#U/.59
&"/.(8"=.P7Q~	!*[.#5$D<Bfi7*,Bfi78#'7*/9%&"/.0(85?/.#%9;B'=.(89%&637U#%[/.7*Bfi0789'&)(T7S78(89%#O789fi~ 787*/m#'&2>F=.69;!/.785.=.#U!$(89%&"/.0
!$07*/49%#*KH|(85?/.#%9;B'=.(89%&637#%[/.7*Bfi0789%&6(7S78(89G&6#F&6)"=.#%9SB%!$9%78P&"/Q !B%!$E!/.PQ78Dn5?/.Pafio*pp4r8d
~A:.7*Bfi7
!/!$07*/49<#;,.&666##'5?=,DBfi5?> !UO^5~~M:.7*/Q9;BN[?&"/.09%5U6&JDn9&69\=,z~	&69;:z5?/.71:!/.PdEO=.9/.59<~M:.7*/
6&JDX9%&"/.0&69 =,~	&69;:CO549;::!/.P#*K
/ !/.P
 F9;:.&6#	&"/.PC5$D#%[/.7*BN0789%&6(A7S78(89%#(*!/O^7`Bfi7*,BN78#%7*/9'78P
fi

fi

  0

)-  
 -  -)

O4[78,.6&)(8&69%6[Q#'9;!$9%&"/.09;:.7H7S78(89	5$DE!F(85?>,^5?=/.PQ!$(89%&65?/K|#%&">F&6"!BG!,,BN5?!$(8:C(85?=.6PO^7]=.#%78PT&"/
"!'#mO=.9	&6#A(*=B'Bfi7*/9')[1/.59	#S=,,5?Bfi9'78PK
 C6{

5.B%>!$66[d!/f"!#P78#%(*Bfi&",.9%&65?/T&6#A!R9;=,.675!ja

]J]A]

* ]]] ]
 r8d~A:.7*Bfi7Kp



     &6#!zI/.&)9'7Q#%78915$D<#%9S!$9%73!BN&2!4O.)78#T(85?>,Bfi&6#%78P 5$D\!I/.&69%7#'7895$D
,Bfi5?,54#%&69%&65?/!$3$!Bfi&"!O.678#*d)  ?d!/.P!HI/.&69%7<#%7895$D/.=>F7*Bfi&6(*!$Z3!Bfi&"!O.678#*d  K



&)#G!HI/.&69%7d/.5?/.7*>,.9fi[C#%789	5$D#%[.#%9%7*>!$07*/49%#*K




&6#`!]I/.&)9'7<#%789	5+D-7*/3.&"Bfi5?/>F7*/49G!$07*/49%#*K

dj

)

AM&)#!T#%789H5$DA!$(89%&65?/zP78#%(*Bfi&",.9%&65?/.#a ]M] r	~M:.7*Bfi7\&)#F9;:.7C#%789F5$D	#%9;!+9%73!4Bfi&"!O.678#(85?/4R
#%9;B%!$&"/.78PUO4[9;:.7<!$(89'&)5./d  &6#A!F,Bfi78(85?/.P&69%&65?/T#%9S!$9%7 Dn5?B%>F=."!H&2/9;:.7M#%789on  !/.P  &6#A!/
7S78(89DX5.B%>F=.2!H&"/C9;:.7M#%789\n  KY	:.=.#<a ]	] rff
ZM*fi{n  n  KY	:.7A#'789%#
n  !/.7
P n  !4Bfi7GP7SI/.78PQO7865$~<K


p+.'
  { &)#G!HD=/.(89%&65?/U>@!,,.&"/.0W!$047*/9%#FaO?'j    r 9%5F9;:.78&"BM!$(89%&65?/.#*KE78(*!=.#'7
!/1!$(89%&65?/1O^78)5./.0#A9'578!+(89%6[5?/.7F!$07*/49*d  >F=.#%9 #;!+9%&6#fiDX[C9;:.7MDn5665~&2/.0C(85?/.P&69%&65?/.#p



 a Zr&jUM*


 
 

4


7n








'
7n 


#]  ( 
Z? _  #"j#
!
( 

r $
 a # %

r j#&
 a ( &

&6#	9;:.7H&"/.&)9'&2!+-(85?/.P&69%&65?/K
&6#A9S:.7G05?!$(85?/.P&69%&65?/K

5.BE!G3$!$6&6PUP5?>!+&2/@P78#%(*Bfi&",.9%&65?/d^~7MBfi78=.&"Bfi7M9;:!$9 !$(89%&65?/.# 5$D#%[.#%9%7*>!$07*/49%#!BN7	&"/.P7*,7*/.P7*/49A5$D
!$(89%&65?/.#5$D7*/43.&2BN5?/>F7*/49`!$047*/9%#^p
a{?r)$



{(








a r

aO{.r&j#&




*

{+




]


 aO*r

~A:.7*Bfi7*a{?r &6#	9S:.7<#%789	5+D-(85?/.#%9;B'!$&"/.78PU3!4Bfi&"!O.678#G5$D!$(89%&65?/f{K
Y:.7G#%789	5$DZDX5.B%>F=.2!+#n  &6#M(85?/.#%9;B'=.(89%78PCDBN5?>9;:.7GDn5665~&2/.01!$",:!O^789A5$DZ#%[>FO^56#p


|I/.&69%7H#%789	5$D(*=B%Bfi7*/49	#%9;!+9%7IrC!/.PU/.78.9A#'9;!$9%7rK}3$!Bfi&"!O.678#*d
~A:.7*Bfi7
r


Y	:.7</!$9;=B%!+/.=>FO7*Bfi#.-QK


Y	:.7<!Bfi&69;:>F789'&)(H5?,7*B'!$9%5?Bfi#0/Fd21Gd3d54]!/.P


Y	:.7<Bfi78"!$9%&65?/5?,7*B%!+9%5?Bfi#.6Gd27Gd28Gd:9Gdj !/.P F
j! K


Y	:.7<O5.567*!/15.,7*B%!$9'5?Bfi#

 d w

d k dd<;



!/.PZK
fi

^K

] r<},
7dFK

fi



0J   

Y	:.7G#;,78(8&"!$Z#%[>FO56#Fn( , ^d     , d
,!Bfi7*/49;:.78#%78#A!/.P1(85.>>!K

Y	:.7G#%7895$D!4Bfi&69;:>F789%&6(<78,Bfi78#%#'&)5./.#A&6#M(85?/.#%9;B%=.(89'78PCDBfi5.>9S:.7GDn56)5$~	&"/.01B%=.678#p
o4K\N37*Bfi[C/?=>F7*BN&)(*!+-#%9S!$9%7M3!Bfi&"!O.675r(


 <&6#A!4/1!Bfi&69;:>F789%&6(H78,Bfi78#'#%&65?/K
^{ K |/!$9S=B%!$/?=>FO^7*B	&6#A!/1!4Bfi&69;:>F789%&6(<78,Bfi78#%#'&)5./K
s

^K	bD # !4/.P ( !Bfi7F!Bfi&69;:>F789'&)(H78,BN78#%#%&65?/.#G!/.P>= &6#G!/1!Bfi&69;:>F789'&)(H5?,7*B'!$9%5?B8d^9;:.7*/
&)#M!/1!Bfi&69;:>F789'&)(H78,BN78#%#%&65?/K
&"/!$66[d9;:.7G#%789	5$DZDn5?B%>F=."!$#on4
o4KGn(

=

(

&)#07*/.7*B%!$9%78PO[C9;:.7<B%=.678#p

   , !Bfi7MDn5?B%>F=."!$#*K
{^K	uEBfi5?,^5#%&69%&65?/!$#%9;!$9%7M3!4Bfi&"!O.678#
r(
7
,

!/.P



#



^K	bD # !/.P (
!HDX5.B%>F=.2!^K

 <!4Bfi7MDX5.B%>F=.2!+#*K
!Bfi7]!4Bfi&69;:>F789%&6(F78,BN78#%#%&65?/.#<!/.P &)#<!CBfi78"!$9%&65?/T5?,^7*B%!$9%5.B8d9S:.7*/

1
K	bDdi # di ( !/.Pvi %
!/.PaOi # i (>] i %



!Bfi7DX5?B'>]=."!$#d#%51!Bfi7a 
r8K

i #

rdaOi

# wZi (

r8daOi

# k

i (

r8daOi

# 

i (

#

r8d-ai


# ;

&)#

(

i (

r

u!Bfi7*/49;:.78#%78#M:!*347<9;:.78&"BM=.#;=!$>F7*!/.&"/.0U!/.PU5.,7*B%!$9'5?Bfi#A:!37G9;:.78&"BG=.#;=!+-,Bfi&65?Bfi&69fi[!/.P!$#%#%5.(8&"!R
9%&63.&)9fi[C~	&69;:19S:.7G&DbR9;:.7*/4R786#%75.,7*B%!$9'5?BH0&637*/)5$~ 78#%9	,Bfi&65?Bfi&69fi[K
n 
fi
n 
&6#W!#S=O.#%7895+DA9;:.71Dn5?B%>F=."!$#F5?/.6[wBfi7SDn7*B%Bfi&"/.09%5(*=B%Bfi7*/49#%9S!$9%7C3!Bfi&"!O.678#*K
Y	:.78#%7DX5.B%>F=.2!+#_!4Bfi7G(*!$6678P8   , 8  ( 6 VK
 @?

W|,A[W{CBED

|M)5+D9;:.7 #%[>FO56#E&"/F9S:.7_!+2,:!4O789E5+DDn5?B%>F=."!$#:!37 9;:.78&"B=.#;=!$>F7*!/.&"/.0<~	&69;:\9;:.7 &DbR9;:.7*/4R786#%7
5?,^7*B%!$9%5?
B i # i (>] i % O78&"/.01!/1!OOBfi783.&"!$9%&65?/WDX5?BGaOi #Mk i ( rwQa  i #'k i % r8KXN!$(V:1/.=>F7*Bfi&6(*!$#%9;!$9%7
3!4Bfi&"!O.6
7 r+
7  <:!+#A!HI/.&69%7]B'!/.07S-aOr.r&jqP ] o ]CFGFHF] 	I>.d?~M:.7*Bfi7
I*6PK
Y:.7Dn5?B%>!+E#%7*>!4/9%&6(8#F5$DA!TP5?>!$&"/zP78#%(*Bfi&",.9%&65?/ ! j a J]J]5] * ]]] <r&6#0&637*/z&"/
9%7*B%>\#	5$D!/T	L
| Kp
M|ON

{2B`CBV{=QP -R ST7

ge ,  , X
y?
( 
*nb b

"   ,  FV   , 

`/ ,  ,
W
Y

   
"8n nEbb , (.n F
  ne VU  O( 
 -,GW K jaEX ]Y][Z r W
"   ,   b  (. *4 ( ,  W	  Z p\X] Y  _{ ^ e    ,a` 
 V   ,

/9;:.7TDX5665$~	&"/.0(85?/.#%9;B%=.(89'&)5./w5+DcKdA~ 778,Bfi78#%#U9S:.7/.78.9U#'9;!$9%7D=/.(89%&65?/}!$#U!9;B'!/.#%&69%&65?/
Bfi78"!$9%&65?/&
K $78:
9 dP7*/.59%79;:.7E#%7895$D^O^5?54)7*!4/<3$!$"=.78#m( , ] n   , .K=BN9;:.7*B8d)789Z9;:.7Mye     , S"8nb
y(?
*nb>
 ep"fgd!$#%#%5.(8&"!$9%78P9%51!C#%78*
9 e]hifO7P7SI/.78PmO>
[ eFOa [

r j Oa [j
keAr8K % G&637*/!/
l
| K ~ 7	P7SI/.7	&69%#An  ^Vbnb ,) n F
 |ihmXn Y .X!+#!M#%7895$D9;BN&2,.678# ~	&69;:F(8:!B%!+(89%7*Bfi&6#%9%&6(
D=/.(89%&65?
/ |]a  ]o]  } &
r jOa  } 
 Z Oa  ]o r'r8K
Y:.7E#%789Z5$D#%9;!+9%782
# X5$D K 78=!$6#9;:.7 #%789-5+D^!$6,^5#%#%&"O.67	3!4Bfi&"!O.67_!+#%#%&60?/>F7*/49%:
# X ja )  
 
d%
r Wa   
 p-Tr8K-Y:.7A&"/,=.9 Y 5$q
D K &)#9;:.7	#%7895$D?fi5&"/49!$(89%&65?/.#5$D#%[.#%9%7*> !$07*/49%#EBfi7*,BN78#%7*/9'78P
r,
 s-Sn[ tb';finnnb"
 -	*fib'<$;b$n; Z$; ESb$nfi
R

fi)-  
 -  -)



  0

!$#G#%789'#*KY	:!$9H&)#d&{ # ] { ( ]CFHFCFH] {,u vwuff>
 Y &JDA!/.PQ5?/.6[&JD]aO{ # ] { ( ]CFHFCF] {u v\ucrx
Qy
z z P7*/.549%78#	9;:.7F/?=>FO7*B_5$D7867*>F7*/49%#M&"/

 K
xz7GP7SI/.7<9;:.7G9;B'!/.#%&69%&65?/UBN782!+9%&65?
/ |Up_X) Y VX {dw5+DffK O[Cp
|]a ]o]  } r&jU_|}
~



v

 a -rd~A:.7*Bfi7

VaO ] | ]  } r ]

_[o hj| k

~A:.7*Bfi7 opX]~m>Xd&6#G9;:.7<9SB%!/.#%&69%&65?/Bfi78"!$9%&65?/Dn5?B5&"/49`!+(89%&65?/.#*~m5$DO^59;:T#%[.#%9%7*> !/.P
7*/43?&"Bfi5?/>\7*/9 !$07*/49%#*K-Y:.7	78?&6#%9'7*/9%&"!$=!/49%&JI(*!$9%&65?/>@!$78#9;:.7A!+(89%&65?/.#5$D7*/43.&2BN5?/>F7*/49!$07*/49%#
=/.(85?/49;Bfi56"!O.67d#'&2/.(87.|]aO ][oH]  } r&)#9SB%=.7d$&JD9;:.7*Bfi778?&6#%9'##%5?>F75&"/49!+(89%&65?/H5$D7*/3.&"Bfi5?/>F7*/49E!$07*/49%#
#;=.(V:9;:!$9 9;:.7H(85?>FO.&2/.78P@54&2/49A!$(89'&)5.
/ |)j o  o	 >!$78
# 8Oa  ] | ] y})r_9;B%=.7K
o	
Y:.719;B%!/.#%&69%&65?/Bfi78"!$9%&65?
/ F&6#W!(85?/N=/.(89'&)5./m5$DG9;:Bfi787UBfi78"!$9'&)5./.
# eH
d  !/.P  &
d 8Oa  ] | ]  } A
r j
eFOa  | y}r
Oa  | y}r
a |r8K&)347*/U!/1!$(89'&)5.Z
/
{

j
a


8
r
?
d
F
!
*
(

=
'
B
fi
B
*
7

/

9
%
#
;
9
$
!
%
9
7


F


!
.
/
U
P
.
/
8
7
.

M
9
%
#
;
9
!$9%7
] ]
k
] ]
kf
]M]
 } d.678\
9 ~q.Oa *r !/.P  Oa  ]  } rEP7*/.549%7A9S:.7A3$!$"=.7<5$D-9S:.7`,Bfi78(85./.P&)9'&)5./@DX5?B'>]=."!  !/.PC7S78(89Dn5?B%>F=."!
+
5
D
J


{
dBfi78#;,^78(89%&63786[K

e=pX)>~kVX
{d&6#A9;:.7*/CP7SI/.78PQO4
[ p
FaO

r j
] | ]  }&

e

~:?aOr k aO ]  } r _




H

P7SI/.78#9S:.7(85?/.#'9;B%!$&"/49%#<5?/z9;:.7C(*=B%Bfi7*/49<#%9S!$9%7!/.Pm/.78.9]#'9;!$9%75+DE54&2/49]!+(89%&65?/.#*KeD=BN9;:.7*B
7*/.#;=Bfi78#Z9;:!$9Z!$(89%&65?/.#Z~	&69;:M&"/.(85?/.#%&6#%9%7*/497S78(89%#(*!//.59ZO7	,7*BDn5?B%>\78PG(85?/.(*=B%Bfi7*/49%6[d!$#eBfi78P=.(878#
9%5FD!$6#%7H&D	!/4[W,!$&"BM5$DE!$(89%&65?/.#M&"/!M54&2/49G!$(89%&65?/U:!+#A&"/.(85?/.#%&6#%9'7*/9G7S78(89%#*K Y	:.=.#*5d e !$6#%5#'9;!$9%78#
9;:.7MIBfi#'9	(85?/.P&69%&65?/mab#%787<.78(89%&65?/ 1.rDX5.B	!35&6P&"/.0&"/9%7*BDX7*Bfi7*/.(87O^789~ 787*/C(85?/.(*=B'Bfi7*/9M!$(89%&65?/.#*K

pX)~kX d&)#<!]DB%!4>F7<Bfi78"!$9%&65?/T7*/.#;=Bfi&"/.019;:!+9A=/.(85?/.#%9;B'!$&"/.78PU3!4Bfi&"!O.678#<>!$&"/9S!$&"/
9;:.78&"B3!$"=.7
K $78"
9 Oa {.rP7*/.59%7<9S:.7G#%789	5$D(85?/.#%9;B'!$&"/.78PU3!4Bfi&"!O.678#A5$DE!$(89%&65?
/ {
KxQ7H9;:.7*/1:!3K7 p
e

aO

r j
] | ]  }&



~A:.7*Bfi7.


j# 

H

aOr
j=r } r


I

]

 2

aO{.r8K


p~d7*/.#;=Bfi78#G9;:!+9	(85?/.(*=B%Bfi7*/49<!$(89%&65?/.#M(85?/.#%9SB%!$&"/U!C/.5?/T5$37*Bfi"!,,.&"/.01#%789M5$DE3!BN&2!4O.)78#

!/.P19;:.=.#	#%9S!$9%78#	9;:.7G#'78(85?/.PU(85?/.P&69%&65?/WDX5.B	!35&6P&"/.0&"/9%7*BDX7*Bfi7*/.(87O^789~ 787*/C(85?/.(*=B'Bfi7*/9M!$(89%&65?/.#p
 a |rj

~A:.7*Bfi70|
-


(

TVWXW

P7*/.59'78#	9;:.7G#%789aO{

#y] { (

476 < 6  6 +

 






& 

r z a {



+ H



aO{ # r%$VaO{ ( rXj&_


r 
(|(|
#] { ( 
k

'Uv6 -6

]

{ #.jU
{ ( ?K
!




Y5O=.&6)PQ!/ h  i
i |
 Bfi7*,Bfi78#%7*/49%&"/.0C9;:.7G9;B%!/.#'&)9'&)5./UBfi78"!$9%&65?/2|]aO ]oH]  } r 5$D9;:.7]|}5$D!\P5?>!$&"/
P78#%(*Bfi&",.9%&65?/!ja J]&]5] M8 ]]] <r8d$~ 7<>F=.#%9	P7SI/.7!F#'789	5$DEO5.567*!/T3!Bfi&"!O.678#G9%5CBfi7*,Bfi7SR
#%7*/49G9;:.7F(*=B%Bfi7*/49G#%9;!$9')
7 d9;:.7M54&2/49<!$(89%&65?/T&"/,=.9 o d!/.PQ9;:.7/.78.9G#%9;!$9'7) } KH|A#G&"/Q.78(89'&)5./71K{
~ 7HIBfi#%9GO=.&)6Pm!C9;B%!/.#'&)9'&)5./BN782!+9%&65?/T~	&69;:9;:.7Mfi5&"/9G!$(89%&65?/.#H5$DO59S:U#'[?#%9'7*>y!/.P7*/3.&"Bfi5?/>F7*/49
!$07*/49%# !$# &2/,=.9M!/.P9S:.7*/1Bfi78P=.(87G&699'5\!9;B%!/.#'&)9'&)5./1Bfi78"!$9%&65?/@~	&69;:C5?/.6[<fi5&"/49_!+(89%&65?/.# 5$D#%[?#'9%7*>
!$07*/49%#	!$#	&"/,=.9*K
[ pA!+#%#;=>F7!$(89%&65?7
/ {&)#H&6P7*/9%&JI78P
 5&"/9<!$(89'&)5./U&"/,=.9%#F!Bfi7Bfi7*,Bfi78#'7*/9%78Pz&2/T9;:.7\DX5665$~	&"/.01~	!*C
O4[!/?=>FO7*B  !/.P(*!/O7Q,7*BDX5?B'>F78PO4[w!$07*/4>
9 E
K { &6#19;:.7*/P7SI/.78P9%5mO7z9;:.7Q!$(89%&65?/
RKfi

fi0J   



5$D`!+07*/9Ed&JDA9;:.7U/.=>FO7*BH78,BN78#%#%78Pm&"/mO.&"/!Bfi[mO[!T#%789F5$DGO5.567*!/3!BN&2!4O.)78#Ve  dE=.#%78Pm9%5
Bfi7*,Bfi78#'7*/9	9S:.7]!$(89%&65?/.#5$ff
D d&6#M78=!$9%5  KEuBfi5?,^5#%&69%&65?/!$#'9;!$9%7G3$!Bfi&"!O.678#`!4Bfi7<Bfi7*,Bfi78#%7*/49%78PO[1!
#%&"/.067O^5?567*!/3!Bfi&"!O.67dZ~A:.&667/.=>F7*Bfi&6(*!$E#'9;!$9%7H3!Bfi&"!O.678#F!Bfi7Bfi7*,Bfi78#%7*/49%78P&"/QO.&"/!Bfi[QO4[!@#%789
5$DEO5.567*!/13$!Bfi&"!O.678#*K
$78
9 e ]^___] e   !/.j
P e" ]_^__] e  C P7*/.59%7F#%789%#M5$DO^5?567*!/T3$!Bfi&"!O.678#<=.#%78P9%51Bfi7*,Bfi78#%7*/49
9;:.754&2/49A!$(89'&)5./5$DZ7*/3.&"Bfi5?/>F7*/49A!/.PC#%[.#%9%7*>!$07*/49%#*KZ=BN9;:.7*B8d.678
9 [,I  !/.
P [ } I  P7*/.59'7A9;:.7 $ e
O5.567*!/3$!Bfi&"!O.67U=.#%78P9%5Bfi7*,BN78#%7*/9H#%9;!$9'73!BN&2!4O.)Z
7 r  
U&2/9S:.7(*=B%Bfi7*/49!/.P/.78.9<#'9;!$9%7K
Y	:.7GO5.567*!/@3!Bfi&"!O.678#A!4Bfi7	5?BfiP7*BN78P~	&69;:\9;:.7A&"/,=.9 3!4Bfi&"!O.678# IBfi#%9*d+DX54)65$~ 78PWO4[!/@&2/49%7*Bfi67*!3.&2/.0
5$D9;:.7<O5.567*!/3!Bfi&"!O.678#G5$D(*=B%Bfi7*/49	#%9S!$9%7G!/.PU/.78?9M#%9;!+9%7A3$!Bfi&"!O.678# p
e

e

 
 FCFCF

[ I#


[ I# 


   

[ } I#

 

0#FHFCF

e

[wI  


[ }

 FCFCF

[ } I# 


e

FCFCFO

[ I 

 C
I





FHFCF





[ } I 

]

~A:.7*Bfi7V &6#]9S:.7/?=>FO^7*B<5+DO^5?54)7*!4/Q3$!Bfi&"!O.678#=.#%78P9%5UBfi7*,Bfi78#'7*/9F#%9;!+9%7F3!Bfi&"!O.67ZrH	!/.PQY&)#
78=!$9%5 z J z KY	:.7	(85./.#%9;B%=.(89%&65?/\5$D |m
 &6# =.&69%7#%&">F&62!4BE9'5G9;:.7	(85?/.#%9SB%=.(89%&65?/H5$DM|m&"/.78(89%&65?/1K{K
|G/ h  ii Bfi7*,BN78#%7*/9'&2/.0m!650&6(*!$M78,Bfi78#%#%&65?/ &)#1O=.&6691&2/9;:.7T#%9;!/.P!4BfiPm~	!*[an Bfi[?!/49*dMo*prK
|GBfi&69;:>F789%&6(<78,Bfi78#%#'&)5./.#`!BN7`Bfi7*,BN78#%7*/9'78P!$#	6&6#%9%#5$D h  ii #AP7SI/.&"/.0C9;:.7G(85?B%BN78#;,5?/.P&"/.01O.&"/!Bfi[
/.=>]O^7*B8KEY	:.78[1(854)"!,.#'7<9%5#'&2/.04)7 h  ii #M~A:.7*/UBN782!+9%78PO4[W!4Bfi&69;:>F789%&6(]BN782!+9%&65?/.#*K
Y5UO=.&66P!/ h  ii e
 P7SI/.&"/.0Q9;:.7@(85?/.#%9;B%!+&2/49%#H5$D9;:.7Hfi5&"/9F!$(89%&65?/.#*d~7C/.7878P9%5Bfi7SDn7*BG9%5
9;:.7H3!$"=.78#G5$DE9;:.7FO5.567*!/T3!BN&2!4O.)78#<BN7*,Bfi78#%7*/49%&"/.019;:.7]!$(89'&)5./.#*
K $789 o a -r	O^7F9;:.7GD=/.(89%&65?/T9;:!$9
>!,.#G!/!$07*/4.
9 m9%5C9;:.7<3$!$"=.75$DE9;:.7O^5?567*!/3!Bfi&"!O.678#<Bfi7*,BN78#%7*/9'&2/.0T&69%#`!+(89%&65?/!/.P)78

9 a {?r
O7G9S:.7G&)P7*/49%&JI7*B3!$"=.7<5+D!$(89'&)5.%
/ {
K=BN9;:.7*B 6789 ~ Oa {.r !/.P   Oa {.rP7*/.59%7 h  ii Bfi7*,Bfi78#'7*/9;!+9%&65?/.#
5$D9;:.7<,Bfi78(85?/.P&69%&65?/!/.P17S78(89 Dn5?B%>F=."!F5$DE!/1!$(89'&)5.
/ {K e  &6#A9S:.7*/10&637*/1O4[ p


e:
 j


?
 a -r

o

a-r&j=aO{?r&

~
 aO{?r k

  Oa {.r _

j
Z

{+


59%7G9;:!$9 6 50&6(*!$Z5?,^7*B%!$9%5?BN#_/.5$~P7*/.549%7G9;:.7G(85?B%BN78#;,5?/.P&"/.0 h  ii 5?,^7*B%!$9%5?BN#*K
|G/ h  ii Bfi7*,Bfi78#%7*/49%&"/.0C9;:.7MDB'!>F7<Bfi78"!$9%&65?/ }
 (V:!4/.078#A&"/T!F#%&">F&6"!B	~	![Cp
U
 j




I


v

a


?
 a -r

a o aZr'j=aO{?r&

r>
>
3 aO{.r%r%r }I jUaIC
]

k
Z

{+


~A:.7*Bfi7>a{?r&)#@9;:.71#%789F5$DA(85?/.#'9;B%!$&"/.78Pm3!4Bfi&"!O.678#5$DG!$(89%&65?/Q{Q!/.PaIZj y}I 78,Bfi78#%#%78#@9;:!$9!$6
(*=B%Bfi7*/49A!/.PU/.78.9	#%9S!$9%7<O5.567*!/3!Bfi&"!O.678#GBfi7*,Bfi78#%7*/49%&"/.
0 r!Bfi7<,!$&"Bfi~&)#'7<78=!$nKY	:.7G78,Bfi78#%#'&)5./
r>

3 Oa {?r783$!$"=!$9%78#	9%v
5 m ( , 5?.
B n   , !/.P1&6#ABN7*,Bfi78#%7*/49%78PO4[9;:.7 h  ii DX5?B m ( , 5?.
B n   , KEY	:.7
!$(89%&65?/C&"/49%7*BDn7*Bfi7*/.(87F(85?/.#%9;B'!$&"/9   &)#M0&637*/1O4[p


j




a{

a

# ]  ( r
 

{
# ] ( r
>a # ]  (

o

a # &
r jUaO{ # &
r 

(

r
RR

o

a ( r

jU
aO{ ( r  k
!

fi

  0




a{

a

r 

# ]  ( 

o

)-  
 -  -)

a # r&jUaO{ # r&

a ( r
o

(



r 
>a # ]  (
#] { ( 

jU
aO{ ( r ]
!

r

~A:.7*Bfi7*a #]  ( r&jqaO{ #y] { ( r z aO{ #] { ( r
  a # r  a ( r k aO{ # r2$VaO{ ( r j#
&?K
!
&"/!$66[U9S:.7 h  ii Bfi7*,Bfi78#%7*/49%&"/.0C9;:.7G9;B%!/.#'&)9'&)5./UBfi78"!$9%&65?/ |
6
&
A
#
S
9
.
:
<
7
8
(
?
5

/
%

=/.(89%&65?/15+D <
e  d 
  !/.P

	
~
6
&
;
9
1
:
$
!
8
(
%
9
6
&
?
5

/

3

!
fi
B
"
&

!
.
O
6

8
7
M
#
$
5

D
;
9
.
:
G
7
*
7

/
.
3
"
&
fi
B
?
5

/
F
>
*
7
4
/
G
9
$
!

0
*
7
4
/
%
9
#
8
7
?

6
&
%
#
%
9
*
7
4
/
%
9
"
&
$
!
6

6

U
[



=

!
4
/
%
9
J
&

I
8
7
M
P
p

|  jU\e ]HFCFCF] e  >_ e  k
 'Y,^CBaCB)i{B{ Q_W|jW{VCBaCBV{

 

k

 _

xH|-CB)i{

Y	:.7<!$605.Bfi&69;:>F#	~ 7<=.#%7MDn5?B07*/.7*B%!$9'&2/.01=/.&637*Bfi#;!+-,."!/.#A!$6(85?/.#'&)#'9A5$D!O!$(8~	!BfiPC#%7*!BN(V:@DBfi5?>
9;:.7M#%9;!$9'78#E#;!$9'&)#NDX[.&"/.0]9S:.7A05.!$(85?/.P&69%&65?/C9'5<9;:.7M#%9;!$9'78#E#;!$9'&)#NDX[.&"/.09;:.7M&2/.&69%&"!$(85?/.P&69%&65?/KXNE>@,.&2BR
&6(*!$E#'9;=.P&678#<&"/>\5?P78(V:.78(8&"/.0:!*347#;:.5$~A/T9;:!$9G9;:.7C>F5#%9G(85?>@,.)78z5?,7*B%!+9%&65?/TDn5?BM9;:.&6#]4&"/.P
5$D	!$605?Bfi&69;:>F#F/.5?B%>!+)6[&6#<9%5CI/.Pz9;:.7   ,  F  , 5$D	!C#%789<5$D 3?&6#%&69%78P#%9;!+9%78# J a  !4/N!/789<!$nK"d
o*pp4qr8K
M|ON

\| T	 ,   n+]K
 , EG8   , > z .
VX k

{2B`CBV{?Px|\BA

  ,  F 

,



J

e<e

,

jyaEX
 o 


]Y][Z r  
 } 
 Z aO [
Y]
] o

r

  ,  @V   , 
_  } 


J

b

J

hX
W

e
,

|w,BN78&2>@!$07E&6#Z#;!$&6PF9%5	78.&6#%9*d$&JD&69Z&)#/.5?/.7*>,.9fi[K59%7E9S:!$9#%9;!+9%78#-!$"Bfi7*!$P[<O^7865?/.0&"/.0<9%5 J (*!/
!$6#%5FO7A!G,!BN95$D9;:.7	,Bfi78&">!$07M5$D J KZ|A#%#S=>F7	9;:!$99S:.7	#%789E5+D3.&)#'&)9'78P#%9S!$9%78#!Bfi7ABfi7*,BN78#%7*/9'78PO[
!/ h  ii 78,Bfi78#%#%&65?/ J  5./1/.78?9M#%9;!$9'7A3$!Bfi&"!O.678#<!/.P19;:!$9dDn5?B &69%7*B'!$9%&65?/1,=B%,^5#%78#*d
~ 7A~	!/49	9%5
07*/.7*B%!+9%7A9S:.7<,Bfi78&">!$07 ~  !$6#%578,Bfi78#%#%78P1&"/1/.78.9 #%9;!$9%7M3!4Bfi&"!O.678#*KE
5?B !F>F5?/.56&69;:.&6(<9;B'!/.#%&69%&65?/
Bfi78"!$9%&65?/ |
7 p
 ~ 7G9;:.7*/C(*!$6(*=."!$9%K


j

aO[ 

~

j

 o } _  

} _ | k

r

G[\
 3:[  <} 
J

~A:.7*Bfi7 o d"[
 !4/.P[/
 }-P7*/.549%7F&2/,=.9d(*=B%Bfi7*/49M#%9;!$9%7F!/.PQ/.78?9G#'9;!$9%7H3!Bfi&"!O.678#*dZ!/.Pm[,
 3:[
 }  P7*/.549%78#
9;:.7F#;=O.#%9'&)9S=.9%&65?/T5$DE(*=B%Bfi7*/49A#'9;!$9%7H3!Bfi&"!O.678#H~	&69;:/.78?9M#%9;!$9'7<3!4Bfi&"!O.678#*K<Y	:.7F#%789M78,BN78#%#%78PQO[
7 AO^7865?/.0#	9%5H9;:.7G,Bfi78&">!$07	5+D J !/.P
  (85?/.#%&6#%9'#E5$D#%9;!+9%7	&"/,=.9A,!$&"Bfi#<Oa  ]o rdDX5?B~A:.&6(V:@9;:.7M#%9;!$9'
9;:.7	&"/,=.9 o >![H(*!=.#%7A!M9;B'!/.#%&69%&65?/HDBfi5?> 9%5<!M#%9;!+9%7 &2/ J KY	:.7&2/,=.9 5$D!/|Bfi7*,BN78#%7*/9'&2/.0
!Q,."!//.&"/.0wP5.>!$&"/m&6#1!#%7895$D<!$(89%&65?/.#KY	:?=.#dDn5?B!Q,."!//.&"/.0P5?>!$&"/m9S:.7178)7*>\7*/9%#1&"/  
!Bfi7H#%9;!$9'7SR!$(89%&65?/U,!+&2BN#*KGY	:.7F07*/.7*B%!$9%78P=/.&637*Bfi#;!$E,."!/.#G5+DE9;:.7]=/.&637*Bfi#S!$E,."!//.&"/.0!$605?BN&)9S:>F#
,Bfi78#%7*/49%78PF&"/F9;:.7	/.78.9#%78(89%&65?/F!Bfi7 #%789%#Z5$D9;:.78#'7#'9;!$9%7SR!$(89'&)5./],!+&2BN#*KZxQ7	Bfi7SDX7*B9%5G9;:.7#%9;!$9'7SR!$(89%&65?/
,!$&"Bfi#<!$#H8   ,   *nbQ ( - , VdO78(*!=.#'79;:.78[U!$#%#'5?(8&"!$9%7F#%9S!$9%78#	9%51!$(89%&65?/.#M9;:!$9M(*!/O7,^7*BDX5.B%>F78P
&"/19;:.78#'7G#%9;!$9%78#K
Y:.7 h  ii Bfi7*,Bfi78#'7*/9%&"/.0T9;:.79;B%!4/.#%&69%&65?/Bfi78"!$9%&65?/ |
 !/.Pz9;:.7F#%789G5$D3.&6#%&69%78PQ#%9S!$9%78# J  9%7*/.P#
9%5FO7M2!4Bfi07d!/.P!F>F5?BN7	7S(8&67*/9 (85?>@,=.9;!$9%&65?/@(*!/O^7A5?O.9S!$&"/.78PUO[,7*BDX5?B'>F&"/.0]9S:.7A78.&6#%9%7*/49%&"!$
=!/9'&I(*!+9%&65?/Q5+D/.78.9<#'9;!$9%7F3!4Bfi&"!O.678#7*!Bfi6[&"/Q9;:.7C(*!$6(*=."!$9%&65?/an =Bfi(8:789F!$nK"dEo*p4po  !/%!/
789-!$nK"d.o*pp4qr8KY5AP5	9S:.&)#d$9;:.79;B%!/.#'&)9'&)5./<Bfi78"!$9%&65?/H:!$#Z9%5	O7 #;,.6&69-&"/49%5`!_(85?/N=/.(89'&)5./<5+D^,!BN9%&69%&65?/.#
|  j |  #'kFCFHFk | bC!$6)5$~	&"/.0C9;:.7<>F5.P&JI78P(*!+)(*=."!$9'&)5.C
/ p


j

aO[ 

}b _ C
|  b kFHFCF Oa [  } _ |  (k
(

~

j

 o } _  

Rt

aO[ 

} _ |  #Mk
#

J

r%r

FHFCF

rG[\ 3q[ 

}

fi

0J   

Y	:!$9 &6#*d |  # (*!/1BN7SDX7*B9%5!+)3!Bfi&"!O.678#*d |  ( (*!4/1Bfi7SDX7*B_9%5!$63!BN&2!4O.)78#M78.(87*,.9>[  } # d |  % (*!/1Bfi7SDn7*B 9%5
!$63$!Bfi&"!O.678#G78?(87*,.9[  } # !/.P#[  }( !/.PU#%5@5?/K
|M#Z#;:.5$~A/<O4[  !4/N!/H789!$nKafio*pp4qr9;:.7(85?>,=.9;!$9'&)5./<9'&2>\7_=.#'78P]9'5A(*!$6(*=."!$9%79;:.7,Bfi78&">!+07 &)#
!G(85./378\D=/.(89%&65?/C5+D9S:.7`/.=>FO7*B5$D-,!Bfi9%&69%&65?/.#KEY	:.7GBfi7*!$#%5?/]DX5?B9;:.&6# &6# 9;:!$9*d+DX5?B#%5?>F7G/.=>]O^7*B
5$DA,!Bfi9'&)9'&)5./.#*d!@D=Bfi9S:.7*BG#;=O^P&)3.&6#%&65?/m5+D_9S:.7,!Bfi9%&69%&65?/.#F~	&66	/.59<Bfi78P=.(87C9;:.7C9%59;!$Z(85?>,.678.&69[d
O78(*!4=.#%7G9;:.7M(85?>,.678?&69fi[C&2/49;Bfi5.P=.(878PUO4[9;:.7M2!4Bfi07*BE/.=>FO7*B 5+D h  ii 5?,7*B'!$9%&65?/.# &6#_:.&60?:.7*B_9;:!/
9;:.7<Bfi78P=.(89'&)5./15$D9;:.7G(85?>,.678.&69[T5$D-7*!$(8: h  ii 5?,^7*B%!$9%&65?/K
Y:.7`Bfi7*,BN78#%7*/9S!$9%&65?/@5$D-9;:.7M650&6(*!$78,Bfi78#%#%&65?/@Dn5?B7*!$(V:1BN782!+9%&65?V
/ e<\d }!/.P  :!$#AO^787*/C(*!Bfi7SR
D=.)6[F(8:.5#%7*/H#;=.(8:<9S:!$9-&69Z(85?/.#%&6#%9%#5+D!M(85?/N=/.(89'&)5./<5+D#;=O78,Bfi78#%#%&65?/.# 9;:!+95?/.6[Bfi7SDX7*B9%5G!	#S>!$6
#;=O.#%7895$D/.78.9	#%9S!$9%7M3!Bfi&"!O.678#*K Y	:.&6#ABfi7*,BN78#%7*/9S!$9%&65?/1!$665$~	#A=.#M9%5F#%5?Bfi9 5?=.9 9;:.7G#;=O^78,BN78#%#%&65?/.#
&"/1(85?/%=/.(89%&637F,!Bfi9%&69%&65?/.#M~	&69;:1/.7*!B 5.,.9%&">!$#%&6878#M9;:!$9	#S!$9%&6#fiDn[9;:.7]!4O5$37GBfi78=.&"Bfi7*>\7*/9%#K
TVWXW'W 6 
 




 

6  8058 

  &


 4" 
8O V


xQ7 IBN#%9P78#%(*Bfi&"O7G9fi~ 5G,Bfi&65?BE!$605.Bfi&69;:>F#Dn5?B h  ii RO!$#%78PU=/.&637*Bfi#;!+,."!//.&"/.0!/.PP&6#%(*=.#%# ~A:.&6(8:
4&2/.Pz5$DP5?>@!$&"/.#G9;:.78[!BN7]#S=.&)9S!O.67FDX5.B8KGxQ7F9;:.7*/,Bfi78#%7*/49<!1/.78~!$605?BN&)9S:> (*!$6)78P  n  "8nb
6 
bG9;:!$9 &6#M#;=.&69;!O.67GDn5?B #%5.>F7GP5?>!$&"/.#A/.549	(85$37*Bfi78PUO4[9;:.7F,Bfi&65?B	!$605.Bfi&69;:>F#*K
Y:.79S:Bfi787	=/.&)347*Bfi#;!$,.2!4//.&2/.0F!$605?Bfi&69;:>\#P&)#'(*=.#%#%78P1!Bfi7	!$6O!$#%78P5./]!4/<&69%7*B'!$9%&65?/H5$D,BN78&2>]R
!$07(*!$6(*=."!$9%&65?/.#*KEY:.7E&69%7*B%!$9'&)5./G(85?B%BN78#;,5?/.P#9'5A!A,!4B%!$6678O!$(V4~	!BfiP<OBfi7*!+P9;:4RnIBfi#%9Z#%7*!Bfi(8:G#'9;!Bfi9fiR
&"/.0!$9H9;:.705.!$-#'9;!$9%78#<!/.Pz7*/.P&2/.0z~A:.7*/Q!$6E&"/.&69%&"!$ #%9;!+9%78#<!Bfi7F&"/.(8"=.P78Pw&"/Q9S:.7#%789G5$D	3.&)#'&)9'78P
#%9;!+9%78#<ab#%787G&60?=Bfi7Fr8KY	:.7<>!$&"/P&7*Bfi7*/.(87FO789fi~ 787*/19;:.7<!+)045?Bfi&69;:>F#	&6#M9;:.7G~	![9;:.7],BN78&2>@!$07
&6#AP7SI/.78PK

Pre3
Pre2
Pre1

Init

Goal

&60?=Bfi7<p|`/@&662=.#'9;B%!$9%&65?/5$D9;:.7<,!B%!+)678O!$(8~	!BNPWOBfi7*!+P9;:4RnIBfi#%9 #%7*!BN(V:C=.#%78P1O[ h  ii RO!$#%78P
=/.&637*Bfi#;!$Z,.2!4//.&2/.0U!+)045?Bfi&69;:>F#*d.(85?>@,=.9%&"/.0W,Bfi78&">!+078#`uBfi7oduEBN7*{!/.PUuEBN7*K

 CCV{5W{2m

|,x|\BA

\|

 "& >!$9'9%&789]!$nKEafiopp!r&"/9SBfi5?P=.(878#F9fi~ 51P&J7*BN7*/9F&"/.P#F5$D_,BN78&2>@!$078#<(*!+)678P8nfi!4/.Pg ;, 
  ,  F  , VK|#%9SBfi5?/.0,Bfi78&">!$07H&6#	P7SI/.78PQO[Cp
M|ON

\| Tm	 ,

  fon+K jaEX
 , EG8   ,  z .
VX k

][Y]Z

r     , GV   , 
aO ][o 
r h J \b
Y_Z

J

h

Vnfi4   ,  F  ,  J e<e ,
 o 

Y	:.=.#*d$Dn5?BZ!G#%9S!$9%7dMO7865?/.04&2/.0F9%5G9S:.7#'9;Bfi5?/.0<,Bfi78&">!+07	5$D!M#%7895$D#'9;!$9%78# J d9;:.7*Bfi7 78.&6#%9%#!+9)7*!+#%9
5?/.7!+(89%&65?/ o ~A:.7*BN7!$6E9;:.7@9;B%!/.#%&69%&65?/.#DBN5?> !+#%#%5.(8&2!+9%78PQ~	&69;: o )7*!+P&"/49%5 J K 5?/.#'&)P7*B9;:.7
X

W

e

{2B`CBV{PCCV{5Qx|\BA



,

Ryx

fi  0



)-  
 -  -)

78!>,.67	#;:.5$~A/F&"/F&60?=BN7AKY	:.7	P59'#E!/.P!B%BN5~#-&"/F9;:.&6#I0?=Bfi7	P7*/.59'7	#%9;!$9'78#E!/.P9;B%!4/.#%&69%&65?/.#
Dn5?BA!4/|~&)9S:Q!#'&2/.04)71/.5?/4RP789%7*B'>F&"/.&)#'9%&6(W!$(89'&)5./K<
5?B	9S:.7#%789M5$D#%9S!$9%78#G<T#;:.5$~A/T&"/9;:.7
I0?=Bfi7d49;:.7	9;:BN787#'9;!$9%78#E:!3.&2/.0F!M9;B%!/.#'&)9'&)5./F&"/9%5F<F!Bfi7 9;:.7#%9;Bfi5?/.0<,BN78&2>@!$07	5$D<UaX&2/.P&6(*!$9'78P
O4[W!F#'56&)PT786)&",.#%7C!/.P1"!O786678P,Bfi7o*rd!$#A!$69SB%!/.#%&69%&65?/.# DBfi5?>9;:.78#'7<#%9;!+9%78# )7*!+PU&"/9'5<K
|  |,:|BA \| &6#G78=!$Z9%51!/15.BfiP&"/!Bfi[,BN78&2>@!$07!$#MP7SI/.78P&"/G7SI/.&69%&65?/{KMY	:.=.#*d

&"/1&60?=Bfi7F!+)9;:.7<#%9SBfi5?/.0,Bfi78&">!+078#`!4Bfi7<!$6#%5C~7*!4,Bfi78&">!+078#*dO=.9M9;:.7],BN78&2>@!$078#M#;:.5$~A/1O[
P!$#;:.78PC7866&2,.#'78#`!BN7A5?/.6[~ 7*!F,Bfi78&">!+078#*d!$# 9;:.7	P!$#S:.78P19;B%!/.#%&69%&65?/.# P5F/.59 #;!$9%&6#fiDn[F9;:.7A#'9;Bfi5?/.0
,Bfi78&">!$047<P7SI/.&69%&65?/K

Pre4
Pre3
Pre2
Pre1
GS

&60?=Bfi7<pM.9;Bfi5?/.0!/.PT~ 7*!,BN78&2>@!$07<(*!+)(*=."!$9'&)5./.#*K	.56&6P7866&",.#%78#<P7*/.549%7],Bfi78&">!+078#	9;:!$9M!Bfi7
O59S:<#'9;Bfi5?/.0<!/.PF~ 7*!
d$~A:.&667AP!$#S:.78P7866&",.#%78# P7*/.59%7A,Bfi78&">!+078#E9;:!+9E!Bfi7 5?/.6[F~7*!4K
G/.6[5./.7\!+(89%&65?/&6#]!$#'#;=>F78P9%5178.&)#'9<&"/9;:.7FP5?>!+&2/KYZB%!/.#'&)9'&)5./.#<(*!=.#%&"/.0!C#%9;!$9%7
9%51O7865?/.019%51!~ 7*!T,Bfi78&">!$07B%!+9;:.7*B	9S:!/U!@#%9;Bfi5./.0W,Bfi78&">!+07!Bfi7FPB%!~A/1P!+#;:.78PK
Y	:.7G#%789	5+D05.!$#%9;!$9%78# &6#G>!B%$78P;H.K

 @?

CCV{5W{2:CCV{5
:DBED>)W{u{2B"{5

|w#'9;Bfi5?/.0	5.B#%9SBfi5?/.0M(8[?(86&6(A,."!/F&6#9;:.7 =/.&)5./F5$D9;:.7 #%9;!+9%7SR!$(89%&65?/HB%=.678#  Dn5?=/.PF~A:.7*/H(*!$6(*=."!$9%&"/.0
9;:.7<,Bfi78&">!+078#`/.78(878#'#;!Bfi[@DX5.B (85$37*Bfi&"/.09;:.7G#%7895$D&"/.&69%&"!$Z#%9;!$9'78#]a  &6#AP7SI/.78P&"/U.78(89'&)5./Uqr8K
.9SBfi5?/.0,."!//.&"/.0m5?/.6[m(85?/.#%&6P7*Bfi#1#'9;Bfi5?/.0,BN78&2>@!$078#*KbD]!z#%78=.7*/.(875+D`#%9SBfi5?/.0,Bfi78&">!+078#
#%9;!4Bfi9%&"/.0Q!$99;:.71#%789@5$DA05?!+#'9;!$9%78#F(*!/mO^7U(*!$6(*=."!$9%78Pd #;=.(8:9;:!$9@9;:.71#%7895$DM&2/.&69%&"!$M#%9;!+9%78#F&)#
(85$37*Bfi78PdZ#%9;BN5?/.0Q,.2!4//.&2/.0z#;=.(8(87878P#W!4/.PBfi789S=B%/.#F9;:.7W=/.&637*BN#;!$	,."!/(85?/.#%&6#%9%&"/.0Q5+DA9;:.71=/.&)5./
5$DE!$6-9S:.7<#%9;!+9%7SR!$(89%&65?/1B%=.678#G5$D9;:.7H(*!$6(*=.2!+9%78P#%9;Bfi5?/.0C,Bfi78&">!$078#*K	M9S:.7*Bfi~	&6#%7F&69	D!$&66#\an_&2>@!$9%9%&
789A!$nK"do*ppOrK
_5?/.#%&6P7*BF9;:.7C78!4>,.67U&"/&60?=Bfi7UKz|A#P7*,.&6(89%78P&"/m9;:.7@I0?=Bfi7d !T#%9;Bfi5?/.0Q,Bfi78&">!+071(*!/
O7\DX5?=/.P&"/9;:.7HIBfi#'9<,Bfi78&">!$07F(*!$6(*=."!$9%&65?/dO=.9G5?/.6[Q!C~ 7*!1,Bfi78&">!$07@(*!/O7FDn5?=/.P&"/9;:.7
#%78(85?/.PC(*!$6(*=."!$9%&65?/K Y	:.=.#*d#%9SBfi5?/.0],."!//.&"/.05./.)[@#;=.(8(87878P#	&"/9S:.&)# 78!>,.67d&JD-9;:.7#%789 5$D&2/.&69%&"!$
#%9;!+9%78# &)#M(85$37*Bfi78PUO4[9S:.7MIBfi#%9A,Bfi78&">!+07]!/.P19S:.7G#%789	5$D05?!$#%9S!$9%78
# 	K
.9SBfi5?/.0,."!//.&"/.0&6#	(85?>@,.)789'7<~	&69;:1Bfi78#S,78(89	9%5F#%9SBfi5?/.0F#%5"=.9%&65?/.#*KbDE!<#'9;Bfi5?/.0,."!/178.&6#%9%# Dn5?B
#%5?>\7\,."!//.&"/.0Q,Bfi5?O.67*> 9;:.7F#%9;Bfi5?/.01,."!//.&"/.0Q!$605?Bfi&69;:> ~	&66BN789;=B%/T&69*d59S:.7*Bfi~	&6#%7d&69<Bfi789S=B%/.#
R

fi

0J   

9;:!$9/.5Q#'5"=.9%&65?/78?&6#%9'#*Km.9;Bfi5?/.0Q,."!//.&"/.0&6#1!$6#%55?,.9%&">!$P=.719%59;:.7UOBfi7*!$P9;:4RnIBN#%9#%7*!BN(V:K
Y	:.=.#*d!F#'9;Bfi5?/.0,."!/1~&)9S:C9;:.7GDn78~ 78#%9	/?=>FO7*B5$D#%9%7*,.#	&"/9;:.7G~ 5?Bfi#%9 (*!$#%7G&6#GBfi789;=B%/.78PK
.9SBfi5?/.0F(8[?(86&6(,."!//.&"/.01&)#G!Bfi78"!$.78P137*Bfi#'&)5./15$D#%9;Bfi5./.0\,."!//.&"/.0dO78(*!4=.#%7<&69G!$6#%5(85?/.#%&6P7*Bfi#
~ 7*!U,BN78&2>@!$078#*KF.9;Bfi5?/.01(8[.(86&6(W,."!//.&"/.0TI/.P#]!C#%9SBfi5?/.01,.2!4/d&JD	&)9H78?&6#%9'#*K]M9S:.7*Bfi~	&6#%7d~M:.7*/
=/!O.67M9%5	I/.PC!M#%9;Bfi5?/.0<,BN78&2>@!$07	9;:.7	!$605.Bfi&69;:> !$PP#!M~ 7*!H,Bfi78&">!$07K99;:.7*/F9;Bfi&678#9%5`,B'=/.7
9;:.&6#G,Bfi78&">!$07<O4[WBN7*>F5$3?&"/.01!$6#%9;!$9'78#9S:!$9	:!*347G9;B%!/.#%&69%&65?/.#M67*!$P&"/.05.=.9	5$D9;:.7<,Bfi78&">!$07F!/.P
9;:.7F#%789M5$D3.&6#%&69%78PQ#%9S!$9%78# J K<XD&69G#;=.(8(87878P#*d9;:.7Bfi7*>!$&"/.&"/.01#%9;!$9'78#A&"/9;:.7,Bfi78&">!+07!Bfi7]!+PP78P
9%5 J !4/.P&69<!$0.!$&"/U9SBfi&678#G9%5W!+PPQ#%9;Bfi5./.0W,Bfi78&">!+078#*K<XD&69MD!+&)6#*d&69<!$PP#<!1/.78~<d~ 7*!U,BN78&2>@!$07
!/.PUBfi7*,^7*!$9%#M9;:.7<,B%=/.&"/.01,Bfi5?(878#'#\an_&2>@!$9%9%&789A!$nK"do*p4p!r8K
_5?/.#%&6P7*BA!+0?!$&"/19;:.7H78!4>,.67]&"/T&60?=Bfi7FK	Y	:.7F#;:.5$~A/C#%78=.7*/.(87F5$D,Bfi78&">!$07H(*!$6(*=."!$9%&65?/.#
(85?=.6PQ:!37<O787*/T(85?>,=.9'78PO4[9;:.7H#%9;Bfi5./.0(8[?(86&6(,."!//.&"/.0U!$605?Bfi&69;:>1K Y	:.7F!$605?Bfi&69;:> ,Bfi7SDn7*Bfi#
#%9;BN5?/.0Q,Bfi78&">!$078#*d&JD`9S:.78[Q78?&6#%9dE#%5z9;:.7IBN#%9!$PP78Pw,Bfi78&">!+07anuEBN7o*rH&)#@#%9;Bfi5?/.0
KQ	5Q#'9;Bfi5?/.0
#%78(85?/.Pw,BN78&2>@!$07C78?&6#%9%#!/.Pm9S:.7~ 7*!,Bfi78&">!$047anuEBfi7*{4rG(*!//.59O^7U,B%=/.78Pm9%5U5./.)[(85?/49;!$&"/
#%9;!+9%78#E/.59E:!3?&"/.0H9;B%!/.#%&69%&65?/.#67*!$P&"/.0F5?=.95$D9;:.7A,Bfi78&">!+07A!/.P9S:.7#'7895$D3?&6#%&69%78P#'9;!$9%78#*KZY	:.=.#*d
9;:.7G#%9SBfi5?/.0(8[.(86&)(!$605.Bfi&69;:>65?5.#DX5.B	!/.59;:.7*B~ 7*!C,Bfi78&">!$07K Y	:.&6#G,Bfi78&">!$071anuEBfi7*4r:!+#A/.5
5?=.9%045&"/.09;B%!/.#'&)9'&)5./.#*d~A:.&6(8:U>F7*!/.#9;:!$9	9;:.7G#'78=.7*/.(87H5$D~ 7*!C,Bfi78&">!$0478#	(*!/UO^7<9%7*B'>F&"/!$9%78P
!/.PC9;:.7G!$605?Bfi&69;:> (*!/BN789;=B%/@9%5<65.5?HDn5?B#%9SBfi5?/.0<,Bfi78&">!$0478#auEBfiy7 1?rKXD-9;:.7	#'7895+D&"/.&69%&"!$#'9;!$9%78#
!Dn9%7*BE!+PP&2/.0,Bfi78&">!+07`uBfiy7 1<(85$37*BN#9;:.7A#'789E5$D&"/.&69%&"!$#%9;!+9%78#9;:.7G!$605?Bfi&69;:> #;=.(8(87878P#*d59S:.7*Bfi~	&6#%7
&69<(85?/49%&"/.=.78#\=/49%&6E78&69;:.7*BF/.5T#%9;Bfi5?/.015.B<,B%=/.78P~ 7*!U,Bfi78&">!+07(*!4/QO7@DX5.=/.Pab&"/Q~M:.&)(8:z(*!$#%7
9;:.7<!$605.Bfi&69;:> D!$&66#;r5?B9;:.7G#%789 5$D-3.&6#%&69%78PU#'9;!$9%78# (85$37*Bfi# 9;:.7G#%789 5$D&"/.&69%&"!$#%9;!+9%78#<ab&"/C~A:.&6(V:(*!$#%7
9;:.7<!$605.Bfi&69;:>#;=.(8(87878P#;rK
| #'9;Bfi5?/.0C(8[?(86&6(,."!/T5?/.6[U0.=!B%!/49%7878#G,Bfi50?Bfi78#'#A9%5$~	!BfiP#9;:.7<05.!$-&"/T9;:.7F#%9;BN5?/.0,!Bfi9'#*KA/
9;:.7G~ 7*!C,!Bfi9%#d(8[?(8678#M(*!/15.(8(*=B8K Y5@7*/.#;=Bfi7<9S:!$9 9;:.7],."!/)7*/.049;:1&6#I/.&)9'7d~ 7<>F=.#%9A!$#'#;=>F7
9;:!$9G9SB%!/.#%&69%&65?/.#H)7*!+P&2/.05?=.9G5+D_9S:.7~ 7*!U,!BN9%#G7837*/49;=!$66[Q~	&66O7C9;!+7*/KY	:.7!$605?BN&)9S:> &)#
(85?>,.6789%7G~&)9S:1Bfi78#;,78(899%5F#%9;Bfi5?/.0F#%542=.9'&)5./.#*d!$#	!F#%9SBfi5?/.0<#'5"=.9%&65?/1~	&66-O^7<Bfi789;=B%/.78Pd&JD&69	78?&6#%9'#*K
 @

CC|{5_V^W{2lBAjBaCB)i{ z,CV{5~W{2=CV{5n
qDEBDsW{8{2B{ 

|G/1&">,5.Bfi9;!/49	Bfi7*!$#%5?/@Dn5?B #'9;=.P[?&"/.01=/.&637*Bfi#;!$Z,.2!4//.&2/.01&6#M9;:!$9A=/.&637*BN#;!$-,."!//.&"/.0U!$605?BN&)9S:>F#
(*!/,BN53.&6P7	#%9;!+9%7SR!$(89%&65?/FB%=.678# 9%5G(85?>,.6789%786[:!4/.P)7G!G/.5?/4RP789%7*B'>F&"/.&)#'9%&6(G7*/3.&"Bfi5?/>F7*/49*KY	:.=.#*d
&JD!w,."!/78.&6#%9%#1Dn5?B1,!$&"/49%&"/.09;:.7T5?5?Bd`!4/!$07*/49178?78(*=.9'&2/.0!m=/.&637*Bfi#S!$],."!/~&)6!$6~	![?#
!35&6Pw,!$&"/49%&"/.0&69%#%78JD<&"/49%59S:.71(85?B%/.7*BF5?B@Bfi7*!$(8:m!/[59;:.7*B@=/Bfi78(85$37*B%!O.67UP7*!+P.R7*/.PK.9;Bfi5?/.0
,."!//.&"/.0!4/.Pm#%9;Bfi5?/.0(8[.(86&)(Q,."!//.&"/.0m!$605?Bfi&69;:>F#@(85?/9SBfi&"O=.9%7UO[m,Bfi5$3.&)P&"/.0Q(85.>,.6789%7 h  ii R
O!$#%78P!$605?Bfi&69;:>\# DX5.B=/.&637*BN#;!$,.2!4//.&2/.0
K
M/4Dn5?Bfi9;=/!$9'78)[dBfi7*!+R~ 5?BN)PP5?>!+&2/.#F(*!/z:!*347P7*!$P.R7*/.P#9;:!+9]!Bfi7C/.59]!$6~	![?#F!35&6P!O.67K
 5?/.#'&)P7*BdDn5?B78!>@,.)7d.(8:.5?,,^7*Bfi# LBfi5?O549fiRO!O4[1P5?>!$&"/1P78#'(*Bfi&"O78PQ&"/.78(89%&65?f
/ 1K |M#AP7*,.&6(89%78P
&"/T&)0.=Bfi7d/.5U=/.&637*BN#;!$,.2!4/BN7*,Bfi78#%7*/49%78PQO[!#%789M5$DE#%9S!$9%7SR!$(89%&65?/TB%=.678#<(*!/0?=!B%!4/9%787H9;:.7
05?!+9%5FO7<Bfi7*!$(8:.78PC&2/1!I/.&69%7G5?B&"/4I/.&69%7]/.=>FO7*B5$D-#'9%7*,.#*d!$#!+)Bfi786783$!/9M!$(89%&65?/.#	>![67*!$P19%5
!/1=/Bfi78(85$37*B'!O.67<P7*!$P.R7*/.PK
|>F5?Bfi7F&"/49%7*Bfi78#%9%&"/.0T78!>@,.)7@&)#<:.5$~ 9%51047*/.7*B%!$9%7!1=/.&637*Bfi#;!+E,.2!4/TDX5.BG!#%[.#%9%7*> 9S:!$9G(*!/
O71&"/!O!+Pm#%9;!$9%7d05?5.Pm#%9;!+9%75?B!4/=/Bfi78(85$37*B'!O.671D!$&6)78P#%9;!$9'7abP7*!+P.R7*/.Pr8K|A#%#S=>F719;:!$9
!$(89%&65?/.#(*!/1O7G78.78(*=.9%78PU9S:!$9 (*!/1OBfi&"/.09;:.7G#%[.#%9%7*> DBfi5.> !4/[O!+P1#%9;!$9%7M9%5!H05.5?P1#%9S!$9%7dO=.9
7*/43?&"Bfi5?/>\7*/9!$(89%&65?/.#F=/4DX5.Bfi9;=/!$9%786[Q(*!4/!$6#%5>!$7@9;:.7#'[?#%9'7*>y#%9;![T&2/!UO!+P#%9S!$9%75?B7837*/
(8:!/.07M9%5!/1=/Bfi78(85$37*B%!O.67MD!$&6678P1#%9;!$9%7ab#%787M&60?=Bfi7<prK5F#%9;Bfi5./.0]/.5?B #'9;Bfi5?/.0H(8[?(86&6(<#%542=.9'&)5./
R

fi

  0

)-  
 -  -)

(*!/O7CDn5?=/.PdEO^78(*!=.#%7U!/Q=/BN78(85347*B%!O.671#%9;!$9'7(*!/O7UBfi7*!$(8:.78PzDBfi5.> !4/[&"/.&69%&"!$	#%9;!+9%7KU|G/
78!>,.67<5$D#;=.(8:1!FP5?>!+&2/an!F,5$~ 7*B	,."!/49;r &6#A#'9;=.P&678PU&"/U.78(89%&65?/TK"oKJ{K
Bad States

Good States

Unrecoverable
Failed
States
(Dead-Ends)

&60?=Bfi7]p/p-|GO.#%9;B'!$(89	P78#%(*Bfi&",.9%&65?/5$D9;:.7<	|}5$DE!F#%[.#%9%7*>~&)9S:1=/Bfi78(85$37*B%!O.67<#'9;!$9%78#*K
|G/.59;:.7*BH6&">F&69;!$9%&65?/5$DM#%9;Bfi5?/.0Q!/.Pm#'9;Bfi5?/.0(8[.(8)&6(,.2!4//.&2/.0m&6#9S:.71&2/:.7*BN7*/91,78#%#'&2>\&)#S> 5$D
9;:.78#%71!$605?BN&)9S:>F#*KU 5./.#%&6P7*BGDn5?B78!>@,.)7C9;:.7CP5?>!$&"/abG5?>!+&2/ o*rG&66"=.#%9;B%!$9'78Pm&2/z&60?=Bfi7oPK
Y	:.7GP5?>@!$&"/1(85?/.#%&6#%9'#	5$DXY(/oG#%9;!+9%78#	!/.PU9fi~ 5<P&J7*Bfi7*/49`!$(89'&)5./.#]abP!$#;:.78P!/.P1#%56&6Pr8K
...

IS
0

GS
n

1

&60?=Bfi7oP/p G5?>!$&"/Qo4KY	:.7|5+D!FP5?>!$&"/~	&69;:19fi~ 5!$(89%&65?/.#abPB%!~A/1!+#	#%56&6PQ!/.PUP!$#S:.78P
!B%Bfi5$~	#;r&662=.#'9;B%!$9%&"/.0H9;:.7	,5#%#'&2O.67M65#%#E5+D#;:.5?Bfi9E,."!/F67*/.09S:.#E~A:.7*/F,Bfi7SDn7*B%Bfi&"/.0<#'9;Bfi5?/.0
#%5"=.9%&65?/.#*K T!/.PU<1!BN7G9;:.7G&"/.&)9'&2!+E!/.P105?!$#%9S!$9%7dBfi78#;,^78(89%&63786[K
Y:.7#%9;Bfi5?/.0 (8[.(86&)(M!$605?Bfi&69;:>BN789;=B%/.#-! #'9;Bfi5?/.0	,."!/ aOP ] ^_ o r ] ao ] ^ o r ]GFGFHF] aOY:1Go ] ^_ o r?K
Y	:.&6#,.2!4/G~ 5?=.6P<:!37E!	O78#'9-!/.P<~ 5?BN#%9fiR(*!$#%7Z67*/.09;:M5$DYZK=.9Z! #%9;Bfi5./.0(8[.(86&6(_,."!/aOP ] {C  r ]
Oa YV1o ] ^_ o r<!$6#%5178?&6#%9'#<!/.PU(85?=.6PO^7,Bfi7SDX7*B'!O.67O78(*!=.#'7]9S:.7\O^78#%9fiR(*!$#'7]67*/.09S:U5+DAoF5$DE9;:.7
(8[.(8)&6(Q#'5"=.9%&65?/>![m:!*347!m>F=.(V::.&60?:.7*B1,Bfi5?O!O.&66&69fi[9S:!/m9;:.7&"/4I/.&69%7Q~ 5?Bfi#'9fiR(*!$#%7T)7*/.049;:K
.9;Bfi5./.0](8[.(86&6(],."!//.&"/.0~&)6-!+)~	![.#_,BN7SDX7*B 9'5\BN789;=B%/C!<#'9;Bfi5?/.0,."!/d&JD&)978?&6#%9%#d?78347*/C9;:.5?=.0?:C!
#%9;BN5?/.0F(8[?(86&6(,.2!4/U>![C78?&6#%9	~&)9S:U!F#;:.5.Bfi9%7*B8dO78#'9fiR(*!$#%7F,.2!4/167*/.09;:K
[Q!$PP&"/.0Q!/Q=/Bfi78(85$37*B'!O.67P7*!$P.R7*/.PDn5?BM9;:.7P!$#S:.78P!$(89'&)5./Q!/.P>!4&"/.0#%56&6P!+(89%&65?/.#
/.5?/4RP789%7*B'>F&"/.&)#'9%&6(Wab#%787MG5?>!+&2/C{d.&60?=Bfi7Foo*r8d4#%9;Bfi5./.0<(8[.(8)&6(F,.2!4//.&2/.0C/.5~Bfi789S=B%/.# 9;:.7A#'9;Bfi5?/.0
(8[.(8)&6(,."!7
/ Oa P ] ^_ o r ] ao ] ^ o r ]GFHFGF] Oa Y(1o ] ^_ o r ?K- =.9	~ 7F>F&60?:9#%9%&66O^7<&"/9'7*Bfi78#%9%78PT&2/9;:.7
,."!7
/ Oa P ] {C ^ r ] Oa Yx1wo ] _ o r 	7837*/9;:.5?=.0?:C9S:.7G05?!$&6#`/.549	0?=!B%!4/9%7878PC9%5O^7]!$(8:.&678378PK
  
 

MCBAjBCBDsW{u{B{ 

Y :.7	!/!$6[.#%&6#&"/<9S:.7,Bfi783.&65?=.##%78(89%&65?/H#;:.5$~	#Z9;:!$9Z9;:.7*Bfi7 !Bfi7EP5.>!$&"/.#E!/.P,."!//.&"/.0],Bfi5?O.67*>F#ZDn5?B
	
~A:.&6(8:~ 7G>![]~	!/499%5F=.#%7`!D=.6)[1Bfi78"!$.78PW!+)045?Bfi&69;:>9;:!$9E!$6~	![?#&"/.(82=.P78#M9;:.7GO78#%9NR(*!$#%7G,.2!4/
!/.PBfi789;=B'/.#!#%5"=.9%&65?/H7837*/H&JD&69&"/.(8"=.P78#EP7*!$P.R7*/.P#9;:!$9Z(*!//.59O7 0?=!B%!4/9%7878PH9%5GO7	!35&6P78PK
xQ7G&"/49;Bfi5.P=.(87]!/1!$605.Bfi&69;:>#%&">F&6"!B9%5F9;:.7G#%9;Bfi5./.0\,."!//.&"/.0U!$605.Bfi&69;:>9;:!$9	!$PP#M!/C5?BfiP&"/!Bfi[
,Bfi78&">!$047G&2/C7*!+(V:@&69%7*B%!$9%&65?/1:!+#9S:.78#%7<,Bfi5?,^7*Bfi9%&678#*KE78(*!=.#%7G#'9;!$9%7SR!$(89'&)5./B%=.678#	67*!$P&"/.0C9%5]=/BN7SR
(85$37*B%!O.67HP7*!$P.R7*/.P#]>![1O7!$PP78P9%5C9;:.7]=/.&637*BN#;!$E,."!/d~ 7G(*!$69;:.&6#<!$605?Bfi&69;:>   n  "8nb
R

fi

0J   

0

...

IS
0

GS
n

1

&60?=Bfi7oo<p G5?>!$&"/U{^KY	:.7|5+D!FP5?>!$&"/~	&69;:19fi~ 5!$(89%&65?/.#abPB%!~A/1!+#	#%56&6PQ!/.PUP!$#S:.78P
!B%Bfi5$~	#;r_&)6"=.#%9;B'!$9%&"/.09;:.7,5#%#'&2O.67C65#%#G#;:.5?BN9`,."!/67*/.09S:.#<~A:.7*/,Bfi7SDn7*B%Bfi&"/.0U#'9;Bfi5?/.0
(8[?(86&6(F#%5"=.9%&65?/.#*K T!/.PU<1!BN7G9;:.7G&"/.&)9'&2!+E!/.P105?!$#%9S!$9%7dBfi78#;,^78(89%&63786[K
6 
b?KY:.7!+)045?Bfi&69;:> &)#1#S:.5~M/m&"/&60?=Bfi7o*{KY:.7UD=/.(89%&65?/uEBN78&2>@!$07?a>Z"8b , >   , 8r
Bfi789;=B'/.#<9;:.7#'789<5$D #%9;!$9%7SR!+(89%&65?/QB%=.678#  !$#%#'5?(8&"!$9%78P~	&69;:9S:.7,Bfi78&">!$07@5$D	9;:.73.&)#'&)9'78P#%9S!$9%78#*K
uEB'=/.7?a    , M*nbHd Z"8b , >   , 8rGBfi7*>F5$378#M9;:.7#%9S!$9%7SR!$(89%&65?/B%=.678#*d~A:.7*Bfi79S:.7]#'9;!$9%7!$"Bfi7*!$P[
&6# &2/.(8"=.P78P&"/C9;:.7M#%789 5$D3?&6#%&69%78P1#%9S!$9%78#*d.!/.PU.9;!$9%78#SD'a   (. , >   , M*nb^;rBN789;=B%/.# 9;:.7M#%789E5$D
#%9;!+9%78#	5$DE9;:.7,B%=/.78P#%9S!$9%7SR!$(89%&65?/TB%=.678#*K 0  hZf &2/.(8"=.P78#H9;:.7F5?,.9%&">F&6#%9%&6(,."!//.&"/.0!$605?BN&)9S:>1K
Y	:.7	5.,.9%&">F&6#%9%&6(`,."!//.&"/.0!$605?BN&)9S:> &6#E&"/.(85?>,.6789%7M~	&69;:BN78#;,78(899%5G#%9;Bfi5./.0G#%5"=.9%&65?/.#*d.O78(*!=.#%7M&69
CyyCy <s[W) ) E , 
 )F0ff
 
W),  (ffj
 
C

  ) 
 )F 
F  )ff W^C 0 
 F  ,"ff W F  )  )F 
  [C>
@   F  ,)*j
L,  (
 ffAL,  0  F  ,)
 F0
 ff  ) F,
   0F  ) 
 )

O
y>[Cy

y>Cyk	 e6)

W),  

&60?=Bfi7Co*{pY	:.7<5.,.9%&">F&6#%9%&6(],."!//.&"/.0U!$605?BN&)9S:>1K
P5.78#`/.59M/.78(878#%#;!Bfi&66[BN789;=B%/U!\#%9;Bfi5?/.0@#%5"=.9%&65?/d
&D 5?/.7G78.&)#'9%#*K	/49;=.&69%&63786[d5?,.9%&">F&6#%9'&)(,."!//.&"/.0
5?/.6[0?=!B%!/49%7878#H9;:!$9<9S:.7*Bfi778.&6#%9%#F#%5?>\77S78(89F5$DA!,.2!4/m!$(89%&65?/z)7*!+P&2/.0z9%5U9;:.7C05?!+Xd~A:.7*Bfi7
#%9;BN5?/.0,.2!4//.&2/.010.=!B%!/49%7878#	9;:!+9_!+)7S78(89'#	5$DE,.2!4/U!$(89%&65?/.#)7*!+PU9%59S:.7G05?!$nK
Y:.7A,=B%,54#%7A5+D5.,.9%&">F&6#%9%&6(<,."!//.&"/.0&6#/.599%5<#;=O.#'9%&69;=.9%7M#%9;Bfi5?/.0G5.B#%9;BN5?/.0<(8[.(86&)(<,."!//.&"/.0K
Y	:.78#%7U!+)045?Bfi&69;:>F#F#;:.5?=.6PO7U=.#%78Pm&"/mP5.>!$&"/.#~A:.7*Bfi71#'9;Bfi5?/.0T5?BH#%9;Bfi5?/.0(8[.(86&)(,."!/.#(*!4/O^7
Dn5?=/.Pw!/.Pw045?!$	!$(8:.&)78347*>F7*/9C:!$#9;:.7U:.&60?:.78#%91,Bfi&65?Bfi&69fi[KwG,.9%&">F&6#%9'&)(Q,."!//.&"/.0w>F&60?:49O7T9;:.7
O789'9%7*B(8:.5&6(87G&"/P5?>@!$&"/.#E~A:.7*Bfi7M05?!+!$(V:.&67837*>\7*/9 (*!//.59 O7M0?=!B%!4/9%7878P@5?BZ9;:.7M#;:.5?Bfi9%78#'9E,.2!4/
#;:.5?=.6PO7<&"/.(8"=.P78PQ&"/19S:.7<=/.&)347*Bfi#;!$-,."!/K
_5?/.#%&6P7*B	!$0?!+&2/d
!$#A!/C78!>,.67d9;:.7<BN5?O59NRO!O[CP5?>!+&2/P78#%(*Bfi&"O78P&"/U.78(89'&)5.
/ 1KZ5.B 9;:.&6#
,Bfi5?O.67*> 9;:.7H5?,.9%&">F&6#%9%&6(#'5"=.9%&65?/>!$78#	9S:.7\BN5?O599;Bfi[19%5C6&Dn9G9;:.7FO.)5.(81~A:.7*/T9;:.7F,5#%&69%&65?/T5$D
9;:.71O.65?(8&6#678#%#F9;:!/U!/.P9;:.71Bfi5?O^59<&6#F~ 5?B%4&2/.0
KY	:.&6#F#%787*>F#F9%5O7C9;:.7C5?/.6[Bfi7*!+#%5?/!O.67
#%9;B'!$9%780[dZ7837*/9;:.5.=.0?:m/.5Q0?=!B'!/9'787Dn5?BH05?!$ !$(8:.&)78347*>F7*/9@(*!/mO7T0&637*/Kmn9F&6#~ 5?Bfi9S:~A:.&667
R

fi

  0

Z; 'c

8b'
 'n







n


8b'"8c
'n




n





)-  
 -  -)







 $bnb

+fin
'n









YZ!O.67oKp	Y	:.7	O^78#%9!4/.P]~ 5?BN#%9fiR(*!$#%7E,."!/H67*/.09;:H5$D,^5#%#%&"O.67	#%9SBfi5?/.0d#%9;Bfi5./.0	(8[?(86&6(A!/.PF5?,.9'&2>\&)#'9%&6(
,."!/.#&2/\G5?>!$&"/.#	o !/.P{Fab#%787	&60?=Bfi78#	o^PA!/.P1oor8K aeRrZ>F7*!/.#9;:!+9/.5G#%542=.9'&)5./F78?&6#%9'#*K
a k	r&"/.P&)(*!+9%78#G9;:!$9	9S:.7],."!/167*/.09S:U&6#M&"/4I/.&)9'7d!/.P!/T=/Bfi78(85$37*B%!O.67HP7*!$P.R7*/.P&)#
Bfi7*!+(V:.78PK
(85?/.#%9SB%=.(89%&"/.0W!4/C5?,.9%&">F&6#%9%&6(,."!/CDn5?B 9;:.&6#G&"/.PT5$DP5.>!$&"/.#A#'&2/.(87F9;:.7<!+)9'7*B%/!$9%&637H&)#G/.5,."!/!$9
!$6nK
|#%&">F&6"!BH5?,.9%&">F&6#%9'&)(1,."!/z&)#H07*/.7*B%!$9'78PDX5.BM9;:.7P5.>!$&"/#;:.5$~A/&"/&60?=BN7WpK@5?BG!+) O!$P
#%9;!+9%78#*d9;:.75.,.9%&">F&6#%9%&6(W,."!/!$#%#'5?(8&"!$9%78#<!4/Q!$(89%&65?/9S:!$9<OBfi&"/.0#F9;:.7F#%[.#%9%7*>y9'5U!C05?5.PQ#%9S!$9%7F&"/
5?/.7#%9%7*,K Y	:.&6#T(85?/9'&2/.=.78#U!$#C65?/.0w!$#C9;:.77*/3.&"Bfi5?/>F7*/49U$787*,.#19;:.7#%[.#%9%7*> &"/!O!+P#'9;!$9%7K
78(*!=.#%7/.5C#%9;B'!$9%780[1(*!/O^7\=.#'78PQ9%51,Bfi7837*/49G9;:.7F7*/3.&"Bfi5?/>F7*/49MDBN5?> OBN&2/.04&2/.0T9;:.7F#%[.#%9%7*> 9%5
!/1=/Bfi78(85$37*B'!O.67<P7*!$P.R7*/.Pd
9;:.7G5?,.9%&">F&6#%9'&)(F#%542=.9'&)5./1&6#	=.&)9'7<#%7*/.#%&"O.67K

5?BG5?>!$&"/.# o	!/.P{	#;:.5$~A/&"/F&60?=Bfi78#	o PM!/.P1ood5?,.9%&">F&6#%9%&6(A,."!//.&"/.0Bfi789;=B'/.#!G=/.&637*Bfi#S!$
,."!2
/ Oa P ] n , r ] Oa Y.1To ] 8   r ?KZ5?BO^59;:@P5?>!$&"/.#9;:.&6#&6#	!<=/.&637*BN#;!$,."!/@~	&69;:@9;:.7	#;:.5.Bfi9%78#%9
O78#'9fiR(*!$#%7M67*/.09;:KE 5.>,!Bfi78P9'5G9;:.7	#%9;BN5?/.0<(8[.(86&)(G#'5"=.9%&65?/9S:.7	(85#%9&2/@9;:.7 IBN#%9EP5?>@!$&"/&6# 9;:!$9
9;:.7A,."!/@>![]:!37	!/F&"/4I/.&69%7M)7*/.049;:d.~A:.&6)7M9;:.7	(854#%9&"/F9;:.7	#%78(85./.PP5?>!$&"/F&6#9;:!$9E!GP7*!+P.R7*/.P
>![O7<Bfi7*!+(V:.78PKEY:.7`Bfi78#S=.)9'#	5$D#%9;Bfi5./.0d#%9SBfi5?/.0<(8[.(86&6(d!/.PC5?,.9%&">F&6#%9'&)(F,."!//.&"/.0&"/CG5?>!$&"/.#Go
!/.PU{!4Bfi7G#;=>>!4Bfi&6878PU&"/UYZ!O.67oK


 (

- 8476 $ 8 

Y :.7 &"/,=.9E9%5<s hZf &)# !/"!'#QP78#%(*BN&2,.9'&)5./`!/.P!M#;,^78(8&JI(*!$9%&65?/@5$D~A:.&6(V:@,.2!4//.&2/.0!$605.Bfi&69;:>
	
9%5U=.#%7KCY	:.&6#<P78#%(*BN&2,.9'&)5./Q&6#F9;:.7*/(85?/437*Bfi9'78P9'5U!1#%789G5$D h  ii #Bfi7*,Bfi78#'7*/9%&"/.0T9;:.7,!Bfi9'&)9'&)5./.78P
9;B%!4/.#%&69%&65?/mBfi78"!$9%&65?/!$#FP78#%(*Bfi&"O78P&"/w.78(89%&65?/qKmY	:.7 h  ii Bfi7*,Bfi78#%7*/49;!$9'&)5./&)#1=.#%78PO[m!T#%789
5$D],."!//.&"/.0w!$605?BN&)9S:>F#9'507*/.7*B'!$9%7U!,.2!4/KY	:.75?=.9;,=.9C5$D<s h-f &)#U!z=/.&)347*Bfi#;!$<,."!/ 5?B
#%78=.7*/9'&2!+,."!/@P7*,7*/.P&"/.05?/@9;:.7A,."!//.&"/.0!$605?Bfi&69;:>K|=/.&)347*Bfi#;!$,.2!4/&6#BN7*,Bfi78#%7*/49%78PWO4[!/
h  ii KHn9GP7SI/.78#MDn5?B	7*!+(V:TP5?>!+&2/T#%9S!$9%7]!@#%789G5$Dfi5&"/49`!$(89'&)5./.#G9;:!$9M9;:.7F#%[?#'9%7*> !+07*/9'#`>F=.#%9
78.78(*=.9%7<#%[/.(8:Bfi5?/.5?=.#%6[T&2/5?BfiP7*B 9'5!$(8:.&)78347]9S:.7G05?!$nK Y	:.7<&">,.67*>F7*/49%78P,."!//.&"/.0U!$605?BN&)9S:>F#
!Bfi7Kp
o4K	.9;Bfi5?/.0,."!//.&"/.0K
{^K	.9;Bfi5?/.0F(8[.(8)&6(,."!//.&"/.0K
^K	G,.9%&">F&6#%9%&6(,.2!4//.&2/.0
K
1
K	

"!$#%#%&6(*!$P789'7*B%>F&"/.&6#%9%&6(,.2!4//.&2/.0
K

ffa
nnff$b%G;fifinCb	*n*
Xfi*nSb%Gc$ff	*fi'c$;'fi%.H;n'	$'* b
%nb$ b  Sn' '    M"
 *fi *nSb'\ bZfi 'c*cM +Sn%Znb%'* "%	 ' G%>M
'- H 	 	*n*2
 'O 


R

fi

0J   

G789%7*B%>\&2/.&6#%9'&)(1,."!//.&"/.0Q(*!/QO^73.&678~ 78P!$#<!#;,78(8&"!$ (*!$#'75$D_/.5./4RP789%7*B%>F&"/.&6#%9%&6(U,."!//.&"/.0KU/
s h-f d~ 7=.#%78PQ9S:.7]5.,.9%&">F&6#%9%&6(,."!//.&"/.0Q!$605?Bfi&69;:>Dn5?B9;:.7O!$(V4~	!BfiPT#%7*!Bfi(8:T5$D(8"!$#%#'&)(*!+P7SR
9%7*B%>\&2/.&6#%9'&)(,."!//.&"/.0KFanY	:.7<#'9;Bfi5?/.05.B #%9;Bfi5./.0(8[?(86&6(!$605.Bfi&69;:>(85?=.6PQ!$6#%51:!*347<O787*/=.#%78Pd!$#
!$69;:.7HP78#%(*Bfi&"O78Pm/.5?/4RP789%7*B%>\&2/.&6#%9'&)(1!$605?BN&)9S:>F#`O^7*:!37F#%&">F&62!4Bfi6[&"/P789'7*B%>F&"/.&6#%9%&6(P5?>!+&2/.#Kr
Y	:.7F5?/.6[/.78~ Dn7*!$9;=Bfi7H5$DE9;:.7FP789%7*B%>F&"/.&6#%9%&6(!+)045?Bfi&69;:> &6#A9S:!$9`!@#%78=.7*/9'&2!+E,.2!4/U&6#G07*/.7*B'!$9%78P
DBfi5?>9;:.7=/.&)347*Bfi#;!$E,."!/O4[U(8:.5?5#'&2/.0U!4/U&"/.&69%&"!$#%9;!$9%7F!/.P&69%7*B%!$9%&63786[!$PP&"/.0!/!$(89%&65?/DBfi5?>
9;:.7=/.&637*Bfi#;!+,."!/Q=/49%&6!C05?!$#'9;!$9%7H&)#<BN7*!$(V:.78PKGY	:.7FP789%7*B'>F&"/.&)#'9%&6(,."!//.&"/.0!+)045?Bfi&69;:> :!$#
O787*/C&">,.67*>F7*/49%78PT9%5<37*BN&Dn[9;:.7G,7*BDX5?B'>!/.(87M5$D-s hZf (85?>,!Bfi78PC9%5H59;:.7*B(8"!$#%#%&6(*!$,."!//.7*Bfi#*K
n9G:!$#</.59GO787*/T5?=B&"/9'7*/9%&65?/T&"/9;:.&6#M~ 5?B%
d9;:.5.=.0?:d9'5P7837865?,!\D!+#%9 h  ii RO!$#%78P(8"!$#%#%&6(*!$
,."!//.&"/.0!+)045?Bfi&69;:> 6&"$7UG& Q!/.85789!$nKAafio*p4pr8KG=B>!$&"/&"/49%7*Bfi78#%9&6#/.5./4RP789%7*B%>F&"/.&6#%9%&6(d
>F=.69%&JR!$07*/49A=/.&637*Bfi#;!$,."!//.&"/.0K
Y:.7Us hZf ,.2!4//.&2/.0m#'[?#%9'7*>&6#&">,.67*>F7*/49%78P&"/w
 40
 /./ !4/.Pw=.#%78#9;:.7U
s ii
l ,!+(V!$07
Oa $&"/.P.R&678)#'7*/d<o*pp4prDX5.B h  ii >@!/.&",=.2!+9%&65?/.#*KQ<=BN&2/.0Q,."!//.&"/.09;:.7P[/!>F&6(13$!Bfi&"!O.67UBfi7SR
5?BfiP7*BN&2/.0D!$(8&6)&69fi[<5+D9;:.7E
s ii
l ,!$(8*!$047&)#=.#%78PF9%5 I/.PF!	O789'9%7*B5.BfiP7*Bfi&"/.0A5+D9;:.7 h  ii 3!Bfi&"!O.678#*K
/9;:.7HDn56)5$~	&"/.0TDn5?=BM#;=O.#%78(89%&65?/.#H~ 7,Bfi78#%7*/49<Bfi78#;=.69%#H5?O.9;!$&"/.78PQ~	&69;:9;:.7s h-f ,."!//.&"/.0
#%[.#%9%7*>&"/U/.&"/.7<P&J7*Bfi7*/49	P5?>!+&2/.#MB%!/.0&"/.0HDBN5?>P789%7*B%>F&"/.&6#%9%&6(!/.P1#%&"/.067SR!$07*/49	~	&69;:1/.5@7*/3.&JR
Bfi5?/>\7*/9<!$(89'&)5./.#<9%5U/.5?/4RP789'7*B%>F&"/.&6#%9%&6(U!/.P>F=.69%&JR!$07*/49<~	&69;:(85?>,.6787*/3.&"Bfi5?/>F7*/49]!$(89'&)5./.#*K
|M)Z78,7*Bfi&">F7*/49%#G~ 7*Bfi7<(*!4B%Bfi&678P5?=.9	5?/T5
! 1?<q PFQ	FuZ7*/9%&"=> uE ~	&69;:QoF<[.9%7  |AB'=//.&2/.0
9 $&"/?=.
 1
K{KZ|>\5?Bfi7MP789;!$&6678PUP78#%(*Bfi&",.9%&65?/5$D-9;:.7G78,7*BN&2>\7*/9%#&2/.(8"=.P&"/.0U9S:.7A(85?>@,.)789'7
 78PUM!$\
P78#%(*Bfi&",.9%&65?/T5$DZ9;:.5
7 "!#QP5?>!$&"/.#M(*!/1O7DX5?=/.PT&"/  7*/.#%7*/afio*pp4pr8K




-



V{ M||\AjB{ByCBEDM2A[,B"{V

xQ7 IBN#%99%78#%9Es hZf Lc#E,7*BDn5?B%>@!/.(87DX5?BZ#'5?>F7 5$D9S:.7A/.5?/4RP789%7*B%>\&2/.&6#%9'&)(GP5.>!$&"/.##%56378PO4[A f K
78?9*d~ 7,Bfi78#'7*/9-!M,5$~ 7*B,."!/49-P5.>!$&"/<!/.PI/!$66[d~ 7#;:.5$~Bfi78#;=.69%#DBfi5?>!M>F=.)9'&R!+07*/9Z#%5.(8(87*B
P5?>!+&2/K

	ff
fiff
 h  Z j  B   i  l   f
G/.75$D<9;:.7P5?>!+&2/.#1#'56378PO[wM f &6#U!/.5?/4RP789%7*B%>\&2/.&6#%9'&)(9;B%!4/.#;,5?BN9;!$9%&65?/P5?>@!$&"/KY	:.7
P 5?>!+&2/(85?/.#%&6#%9%#H5$D_!#%789G5$D65.(*!$9%&65?/.#!/.PQ!1#'789G5$D_!$(89'&)5./.#<6&"$7PBN&)347SR9;B%=.(8dPBfi&637SR9;B%!$&"/!/.P
[9%5F>F5$37AO^789~ 787*/C9;:.7M65?(*!+9%&65?/.#*KE5?/4RP789%7*B%>\&2/.&6#;> &)# (*!=.#'78PUO[/.5./4RP789%7*B%>F&"/.&6#%9%&6(]!+(89%&65?/.#
ab7Kc0
K"d!Dn9%7*BE!PBfi&637`!+(89%&65?/!9;B%=.(8F>![]5.BE>![/.59:!37_D=.7867SDX9SrE!/.PC7*/3.&"Bfi5?/>F7*/49;!$
(V:!4/.078#
ab7Kc0
K"d8Dn50<!$9!$&"B%,^5?Bfi9%#*dH &">!$9%9%&789E!+XKedo*pp!4r8KxQ7 P7SI/.78P9S:.79fi~ 5MP5?>!$&"/F78!>,.678#E9'78#%9%78PO[
M f Dn5?B #%9;BN5?/.0!/.P1#%9;Bfi5./.0<(8[.(8)&6(,."!//.&"/.01&"f
/ /"!#!4/.PUB%!/1s h-f =.#%&"/.0C#%9;Bfi5?/.0!/.P1#'9;Bfi5?/.0
(8[.(8)&6(,."!//.&"/.0KA59S:178!>,.678#A~ 7*Bfi7H#%56378PT&2/678#%#A9S:!Z
/ PK PqH#%78(85?/.P#*K	.&">F&6"!BGBfi78#;=.69%#M~ 7*Bfi7
5?O.9;!+&2/.78PT~	&69;:1M f K|}07*/.7*B%!$37*Bfi#%&65?/C5$D9;:.7<:.=/9'7*B	!/.PU,Bfi78[C5?BH;uE=BN#;=.&69%P5?>!+&2/an7*/.P!
789]!+XKedo*prG!4/.P!1O7*!> ~	!$"UP5.>!$&"/Q:!37!$6#%5O^787*/9%78#%9'78PO[QM f KCY	:.707*/.7*B%!+)&6*!$9'&)5./
5$DE9;:.7F:?=/49%7*BG!/.P,Bfi78[UP5.>!$&"/1&6#</.59MP78#%(*Bfi&"O78P&"/TP789;!$&6-&"/man &">!+9%9%&789<!$nK"dZo*pp4!r8K Y	:.=.#*d
~ 7:!*347]/.59GO787*/Q!4O.)7@9%5W>@!$7]!4v
/ "!#&">,.67*>F7*/49;!$9%&65?/5$DE9;:.&6#<P5?>@!$&"/1Dn5?BM!1>F7*!/.&"/.0$D=.
(85?>,!4Bfi&6#%5?/K
Y:.7<,Bfi5?O.67*>&"/9;:.7]O^7*!>~	!$"P5.>!$&"/1&6#DX5?B !/T!$07*/49 9%5~	!$"FDBfi5?>5?/.7H7*/.PU5$DE!O^7*!>
9%5C9;:.7<549;:.7*B	~&)9S:.5?=.9	D!$66&"/.01P5~M/KAbDE9S:.7]!$07*/49D!$66#*d&69G:!$#A9'5~	!$"1O!$(8C9%59;:.7H7*/.P5$DE9;:.7
O7*!4>!/.PQ9SBfi[!$0.!$&"/KY	:.7FI/.&69%7#%9S!$9%7>!$(8:.&"/.75$D 9;:.7FP5?>!$&"/&6#F#;:.5$~A/T&"/&60?=Bfi7UoKY	:.7
78P078#P7*/.59%7	9;:.75?=.9%(85?>F7 5$D-!M~	!$"F!$(89%&65?/KZx:.7*/F9;:.7A!+07*/9&6#5?/F9;:.7AO^7*!>1d49;:.7	~	!$"F!$(89%&65?/
RKfi

fi  0



)-  
 -  -)

(*!/z78&69;:.7*B>F5$37C&69]5./.71#%9%7*,D=Bfi9S:.7*B<5./Q9;:.7UO^7*!>
O7*!4>1K

5?BF>!$7C&69HD!$6 9%5Q!T65?(*!$9'&)5./m=/.P7*BF9;:.7

up
true

...

false I
0

...
1

2

G

n-2

n-1

pos

&60?=Bfi7o*/p	Y	:.7<O7*!>~	!$"FP5?>@!$&"/KEY	:.7 "!'#7*/.(85?P&"/.05+D-9;:.7GO7*!>~	!$"FP5.>!$&"/1:!$# 5?/.7
,Bfi5?,54#%&69%&65?/!$#%9S!$9%7	3$!Bfi&"!O.67%(  9;:!$9 &6#	9;B%=.7M&JD9S:.7`!$047*/9&)#5?/9S:.7<O7*!>!/.PCD!$6#%7
59;:.7*Bfi~	&6#%7d!/.P!1/?=>F7*BN&)(*!+E#%9;!+9%73!4Bfi&"!O.67  $9;:!$9GP7*/.59'78#<9;:.7,^5#%&69%&65?/z5$D	9;:.7
!$07*/49<78&69;:.7*BG5./9;:.7O^7*!> 5?BM5?/9;:.7@0?Bfi5?=/.PK;U!/.PSGU!Bfi7@9;:.7&"/.&69%&"!$ #%9;!$9%7
!/.PU05.!$#%9;!$9%7GBfi78#;,^78(89%&63786[K
xz7	&">,.67*>F7*/9'78PW!07*/.7*B%!$9%5.B,Bfi50?B%!4> DX5?B""!#P78#%(*BN&2,.9'&)5./.#5+DO7*!4>~	!$"P5?>!$&"/.#	!/.P
, Bfi5.P=.(878PP5.>!$&"/.#<~	&69;: 119%51+PpC,54#%&69%&65?/.#*K178(*!=.#%79;:.7@P5?>!$&"/5?/.6[(85?/49;!$&"/.#<9fi~ 5C#%9;!$9%7
3!4Bfi&"!O.678#*ds h-f (*!//.59 78,.65&69A!<,!BN9%&69%&65?/.78P19;B%!/.#'&)9'&)5./Bfi78"!$9'&)5./@DX5?B9;:.&6#	P5?>!+&2/dO=.9	:!$# 9%5
=.#%7F!]>\5?/.56&69;:.&6(\BN7*,Bfi78#%7*/49;!$9%&65?/K
Y:.7],7*BDX5?B'>!/.(87F5$Ds h-f !/.PQM f &6#G#;:.5$~A/1&"/&60?=BN7Wo 1K	G&6#%(85?=/49%&"/.0U9S:!$9`M f ~	!$#
B%=/H5?/F!#%65~ 7*B>!+(V:.&"/.7d 9;:.7	,7*BDX5?B'>!/.(87 5$Ds hZf !/.PA f &6#=.&69%7	#%&">F&6"!B&"/F9S:.&)#P5.>!$&"/K
5.BMP5?>!$&"/.#<~M:.7*Bfi7s hZf (*!/78,.65&69]!1,!4Bfi9%&69%&65?/.78PmBfi7*,Bfi78#%7*/49;!$9%&65?/d~ 7~ 5?=.6PQ78,78(89H&69<9%5
O7!O.67F9%5C#%5637H2!4Bfi07*BM,Bfi5?O.67*>F#G9;:!/TA f d#%&"/.(87A f (*=B'Bfi7*/9')[T(*!/15./.)[=.#%7]!C>F5?/.54)&69;:.&6(
Bfi7*,Bfi78#'7*/9;!+9%&65?/K=BN9;:.7*B (85?>,!4Bfi&6#%5?/.#AO^789~ 787*/Ts hZf !/.PUM f !Bfi7G5?/C5?=BBfi78#%7*!Bfi(8:1!$07*/.P!^K
 vG h  g G $-  h  -jb
Y	:.7	,=B%,^5#%7 5$D9;:.7 Bfi7*>!$&"/.&"/.0G78,^7*Bfi&">F7*/49%#E&"/F/.5?/4RP789%7*B%>\&2/.&6#%9'&)(MP5?>!+&2/.#Z&6#9%5M#;:.5$~=/.&637*Bfi#S!$
,."!//.&"/.0UBfi78#;=.69%#DX5.BEP5.>!$&"/.#A~M:.7*Bfi7<9;:.7<>F=.69%&JR!$07*/49G!/.P17*/3.&"Bfi5?/>F7*/49G>F5?P7866&"/.0CDX7*!+9;=Bfi78#	5$D
"!'#m:!37`O^787*/=.#%78PK
Y:.7,5$~ 7*BZ,.2!4/9ZP5?>!$&"/HP7*>F5?/.#'9;B%!$9%78#Z!A>F=.69%&JR!$07*/49-P5.>!$&"/H~	&69;:<!/H7*/43?&"Bfi5./>F7*/9>F5.P78
!/.PMD=Bfi9;:.7*B78?7*>@,.)&JI78#5?,.9'&2>\&)#'9%&6(,."!//.&"/.0KE9-(85?/.#'&)#'9%#5$DBfi7*!$(89%5.Bfi#*d*:.7*!$978?(8:!/.07*Bfi#d*9;=B'O.&2/.78#
!/.P13$!$6378#*K|P5?>@!$&"/178!>,.67<&6#	#;:.5$~A/C&"/&)0.=Bfi7o*q^K
/C9;:.7],^5$~7*B,.2!4/9	P5.>!$&"/17*!$(8:C(85?/49;Bfi56"!O.67]=/.&69M&6#`!$#'#%5?(8&"!$9'78PU~	&69;:1!/T!$07*/49 #;=.(V:9;:!$9
!$6(85?/49;Bfi5Z!$(89%&65?/.#G(*!/O7,^7*BDX5.B%>F78P#%&">F=.)9S!/.785?=.#%6[KFY	:.7F7*/3.&"Bfi5?/>F7*/49G(85?/.#%&6#%9%#G5+D!C#%&"/.067
!$07*/49	9;:!+9`!$9M!/[19'&2>\7](*!4/CD!$&6!C/?=>FO^7*B	5$D:.7*!+9A78.(V:!4/.078#`!4/.P9S=B%O.&"/.78#<!/.P!$6#%5C7*/.#;=Bfi78#
9;:!$9!$"Bfi7*!$P[D!+&)678P=/.&69%#EBN7*>!$&"/D!$&6678PK|D!$&6678P:.7*!$978.(V:!/.047*B67*!4#~	!$9%7*BDBfi5.>9;:.7 &"/9%7*B'/!$
9%59S:.7G78?9%7*B'/!$~	!$9%7*B 65.5?,U!4/.PU>]=.#'9AO7H(8)54#%78PO4[U!O.65?(81!$(89%&65?f
/ $KEY	:.7G7*/.7*Bfi04[U,Bfi5?P=.(89'&)5./
DBfi5?> 9;:.71Bfi7*!$(89%5.BM(*!/QO7C(85?/49;Bfi56678PmO[  9'51I9<9S:.7P7*>!/.Q
P iZdO=.9H9;:.71Bfi7*!$(89%5?BM~&)6	!$6~	![?#
,Bfi5.P=.(87<5?/.7G7*/.7*BN0[W=/.&69*KY59SB%!/.#;,^5?Bfi9 9;:.7G7*/.7*Bfi0[@DBfi5?>9;:.7<Bfi7*!$(89'5?B !*~	![HDBN5?>9;:.7G,."!/9G!$9
67*!$#%9H5?/.7:.7*!$9H78?(8:!/.07*BG!/.Pz5?/.79;=B%O.&"/.71>F=.#%9<O7C~ 5?B%4&2/.0
KM9;:.7*Bfi~	&6#%7C9;:.7,."!/49<&6#<&"/!/
=/Bfi78(85$37*B%!4O.)7D!+&)678PT#%9;!$9%7d.~A:.7*BN7<9;:.7<Bfi7*!+(89%5?B~	&66-5$37*B%:.7*!+9*K

	ff
fi

B

v*bff	!ff,b`"58nfi$#M#;C	$nGnSSnbfi	nM"'&%('*)4
RKfifi

fi

0J   

10000

1000

Time / Sec

100

10

1

0.1

0.01

UMOP
MBP

0.001
0

500

1000

1500
2000
2500
3000
Number of Beam Locations

3500

4000

4500

&60?=Bfi7oy1p	u"!//.&"/.019%&">F7<5+Ds hZf !/.PM f &2/T9;:.7<O^7*!>~_!+2@P5?>!+&2/K Y	:.7FA f P!$9;!@:!$#
O787*/T78?9SB%!$(89%78PC~	&69;:U,^5#%#%&"O.67F65#%#	5$DE!$(8(*=B%!+(8[DBfi5?>a &">!$9%9%&789A!+XKedopp!rK
Y:.7#%9;!$9'7]#S,!$(875$D 9;:.7,5$~ 7*B<,."!/49<(*!4/QO7P&63.&6P78Pm&2/49%5T9;:Bfi787P&6#nfi5&"/9F#%789'# pG05.5?PdZO!$P
! /.PzD!+&)678P#%9;!$9%78#K/9;:.7045?5.P#'9;!$9%78#*dZ9;:.7*BN7SDX5?BN79;:.7C05?!+E#%9;!+9%78#*dZ9;:.7U,5$~ 7*B<,."!/49#;!+9%&6#fiI78#
&69%#F#;!Dn789fi[!/.P!+(89%&63?&69fi[Bfi78=.&"Bfi7*>F7*/49%#*KU/Q5?=B78!4>,.679;:.7@#;!Dn789[QBfi78=.&"Bfi7*>F7*/49%#F7*/.#;=Bfi7C9;:!$9
7*/.7*Bfi0[C(*!/TO7<9SB%!/.#;,^5?Bfi9%78P1!~_![FDBfi5?>9;:.7F,."!/9d!/.P19;:!$9_D!$&6678P=/.&69%#G!Bfi7G#S:?=.9	P5$~AC/ p

+-,/.0,/132*46587/.:90,<;3107/.>=@?0A/13;0,CB:7/D07/4:E31FA8G-;3H0,<?0I37/.*;
JA8K0HMLON3PQARK0H0S:N3P:A8K0H0T:N3P:A8KFH3U	VWP3N
JA8K0;MLON3PQARK0;0S:N3P:A8K0;0T:N3P:A8KF;3U	VWP3N
+:H0,37/;-,/X>5@H07/.320,C1>=W90I3A05KY,/B[ZEQE07>Z@I3,/B
JR\RA8KFH	L^]0_:9	L3VWP3N
JR\RA8KFH0S`]0_:90SfiVWP3N
JR\RA8KFH0T`]0_:90TfiVWP3N
JR\RA8KFH3U<]0_:93U	VWP3N
+:;3a3139fiZ.0,b=<=R;FA/?3?0,/BcZE:E07YZRI3,/B
JR\RA8KF;	L^]0_[=YL3VWP3N
JR\RA8KF;0S`]0_[=CSfiVWP3N
JR\RA8KF;0T`]0_[=CTfiVWP3N
JR\RA8KF;3U<]0_[=RU	V
Y :.7A!$(89%&63.&)9fi[Bfi78=.&"Bfi7*>F7*/49%# #%9;!$9%7 9;:!+9E9;:.7	7*/.7*BN0[,Bfi5?P=.(89'&)5./C78=!$6#9;:.7AP7*>@!/.P!/.PC9;:!$9

!$63$!$6378#A9'5~ 5?B%4&2/.0F9;=B'O.&2/.78#<!4Bfi7A5.,7*/Cp

RKfiR

fi  0



)-  
 -  -)

+:?0A/D0,/1:?31FA/B3a>5R;dZRA/.:,/e*a073Ib=WBF,8GY7/.3B
?:]QE-P3N
+:;3a3139fiZ.0,<f073I/f0,-Z/=:A/?F,/.cZE:;3a3139fiZ.0,QZ/=gA8K
JA8K0;ML^]0_hfML3VWP3N
JA8K0;FS`]0_hfFSfiVWP3N
JA8K0;FT`]0_hfFTfiVWP3N
JA8K0;*U<]0_hf*U	V
 /!HO!$PC#%9;!$9%7d$9S:.7`,."!/49 P5?78#	/.59#;!$9%&6#fiDn[F9;:.7A#S!DX789fi[F!/.PW!+(89%&63?&69fi[Bfi78=.&"Bfi7*>F7*/49%#	O=.9E&6#	/.59
=/Bfi78(85$37*B%!4O.)[CD!$&6678PK/U!HD!$&6678PU#%9S!$9%7G!$6-:.7*!$9	78.(V:!4/.07*Bfi#M5?B9;=B%O.&"/.78#`!4Bfi7MD!+&)678PK
Y:.7\=/.&637*BN#;!$,."!//.&"/.09;!$#S1&6#<9%51047*/.7*B%!$9%7!1=/.&637*BN#;!$E,."!/z9%510789MDBfi5?>y!/4[O!$Pz#%9;!$9%7
9%5T#%5?>F7045?5.P#'9;!$9%7~&)9S:.5?=.9F7*/.P&2/.0z&2/!D!+&)678Pm#%9S!$9%7KC|A#'#;=>F&"/.0Q9;:!$9F/.5=/.&69%#FD!$&6EP=BN&2/.0
78.78(*=.9%&65?/d&69E&6#5?O43.&)5.=.#9;:!$95?/.6[<5./.7-fi5&"/49E!$(89%&65?/H&6#/.7878P78PKEM/4DX5.Bfi9;=/!$9%786[d49;:.7 7*/3.&"Bfi5?/>F7*/49
(*!/HD!$&6!4/[/.=>]O^7*B5$D-=/.&69%#P=Bfi&"/.0]78.78(*=.9%&65?/d.9;:.=.#*d?!+#EP78#%(*Bfi&"O^78P1&2/C.78(89%&65?/CK{dDn5?B!/4[O!$P
#%9;!+9%7A9S:.7<Bfi78#;=.69%&"/.0<fi5&"/49A!$(89%&65?/1>![65.5?,1O!$(89'5\!FO!$P1#'9;!$9%7M5?B (*!=.#'7G9;:.7<,."!/99%5F7*/.PU&"/1!
D!$&6)78PT#%9;!$9'7ab#%787G&60?=Bfi7<prK5?B9;:.&6#GBfi7*!$#%5./1/.5#%9;Bfi5./.0]5.B #%9;Bfi5./.0](8[.(86&6(]#'5"=.9%&65?/178.&6#%9*K
okh1 b1

okh2

b2
v1 okt1

T1

H2

H1

s1

v2 okt2

s2

T2

R

p
okp

v3 okt3

T3
v4 okt4

H3
okh3

s3

f

T4

H4
b3 okh4

s4

b4

ijlkYmfinporqCsMtvuxwfiy3zvoCnWw>{}|0~bWfiyYr|3j}~oRd|brw>{lo0>o:npoC|3RyYnOjlmfinnpyYmfi~>fioRb[ffyYmfinO>oC|3
oRYRfi|0~>k0oCnpOq0vfi<|0~>	[>o>oC|3^oRYRfi|0~>k0oCnpOwfinpyY	m>Ro<>jlkYcwfinpoRmfinpo
oC|0yffyYmfinoR{oRRnpjlRjp`k0oC~>oCn|3j}~>kOmfin>j}~>oRq0fi|b~>v	vu|3jl{loR:>oC|3
oRYRfi|0~>k0oCnjm>fiorR{ly0oR[b|>{ly>@<|3Rjy>~:RiyYn|^|Fj{loRQmfin>j}~>oWvj>o
yYw:|FRjlyY~<Cm>fioOC|0nnpjloR:yYm>/r>oroC~>oCnpk0:wfinpy>	m>RjlyY~hy3>onpoC|3Ry>nj
|0~>C|0~`fioRyY~0npy0{l{loRy	>ofioCr|0~>:|3@mfin>j}~>orvj$C|0~`fioR{yboR:0W|
 |3{  oY>o3  |0npj}|0>{loRC|0w>mfinpo>ozvyYnbj}~>kr|3m>yF>omfi~>jl/
u~^yYw>j}jljly0{}m>jlyY~Oj}rw>{lWjlkY~>yYnpoRfi|3$py0j}~0|3RjlyY~>vC|0~O{lyYy>wWfi|3@yr|fi|FW|3oyYn
{loC|3Qy||3jl{loRQ|3oO|0~><~>fir|Wyb{m>jy>~:y`>owfinpy>>{oC|/ffoCnyY~>owfinpoRj}r|3kboC|3{lCm>{}|3jlyY~
~bm>jlj  oR{l0>oyYw>jjjlw>{}|0~|FmfioRfi|3~>ymfi~>jlzj{l{(|Fj{(	mfinpj}~>koR>oRCm>jlyY~|b~>|3{lz|/Y
R>yYy0oRpy0j}~0|3RjlyY~>fi|3{loC|3-fij}npoRR{gnpy>|Wfi|3Q|3oy|Wk0y>yYh|3o0O>oryYw>jjjl
w>{}|0~<jl|0~yYw>j}r|3{RyY~bnpyb{$n|FoRk00fioRC|0m>o^j|F{z|/>R>yYyboR>o>yYnpoRw>{}|0~<yW|^k0yYy>
0

fi00	0Q03	
|Fo|0~>Q~>yWy0>oCnn|3oRk0WoR>jlfi|3C|b~:|  y0jl<{y>yYw>j}~>kfi|F@`yW|Wfi|3:|3oy>noC~>:j}~:|
|3jl{oR`|3o0
>ojlRo:y3>o|Fowfi|FRoy3>o:|0My  owMy*zvoCnOw>{}|0~0Wfiy>r|3j}~[jl0p3u~yYw>jjjl
y0{}m>jlyY~`z|3kboC~>oCn|3oR<0$Wj}~<fi0oRRyY~>fi|0~>Ry>~0|Fj~>oR:0bfiqCr	$[~>y>fioRCu|0~
oRfi|0rw>{lo0	|py0j}~0|3Rjy>~Oz|FoR>n|FRoROnyY>ow>{}|0~^y>n|fi|3W|3oz>oCnor|b~>zvoCnpo
|3jl{oRc|b~>:oC~>oCnpk0:fioC^|0~>Qz|3WoC~>oCnpkb-mfi~>jlCz>jl{loW>ooC~>oCnk0:wfinpyY	m>Rjy>~<:z|3yY~>{l
qmfi~>jlC<>oOoRYn|3RjlyY~<joz|3fifiqCOoRRyY~>fir|0~>$|3oRdwMoRRoR$>oOoRyFpy0j}~b|FRjlyY~>
j}~>R{}m>fioR-|j}~>k0{lopy0j}~b|3RjlyY~@fi|0~>kbj~>kORr|0~>yrnm>o|0~>WoRj}~>kWyrfi
	 fffi(-  fi[ 
>owfimfinwMy0o`y3>oWy>RRoCnfiyY^|3j}~QjOy<fioCyY~>n|3oW|<m>{j|Fk0oC~0^fiyYr|3j}~[zj[|yYnpo
oR{}|0fiy>n|3oWoC~  j}npy>~fioC~0OyYfioR{fi|0~h>owfiy3zvoCnrw>{}|0~brfiyYr|3j}~[ffRyY~>jry3pzy`oC|0y3
w>{}|/0oCnpfi|3C|0~:y  oj}~:|OkYnpjl<zvyYnp{l:|0~>:wfi|3|Wfi|3{l{yWoC|F@`y0>oCn8uoC|3R`j}ooCw<|
w>{}|/0oCnoRjl>oCny  oRj}~`yY~>oy3>oyYmfinr|FyYnfijnoRRjlyY~>yYnwfi|3oR>ofi|3{l{yW|0~>y0>oCnoC|0
w>{}|/0oCnR>o|3jlykboC~>oCn|3o|mfi~>j  oCnp|3{$w>{}|0~OffyYnyY~>oy3$>ooC|0fi|3vC|0~Ofio|bwfiw>{jloR<y
RyYno|k0yY|3{z>oC~>o  oCnv>ooC|0wfiy0oRoR>ofi|3{l{ff
uj}rw>{lo "!'# fioRCnpj}w>jlyY~hy3>ory>RRoCnfiyY^|3j}~-y>fioR{l>oOoC|0wMy0oRj}~>k>ofi|3{l{
|3O>oC|3kboC~0Ofi|3^C|0~cy  o|b~>wfi|Fr>o:fi|3{l{j~>fioCwMoC~>fioC~0`y3oC|3R[y0>oCnR>m>C|
w>{}|/0oCnwfiy0oRj}~>k<>orfi|3{l{C|0~<|3{lz|/Ywfi|Fy|0~by0>oCnoC|0w>{}|/0oCnR>oryYwfiwMyY~>oC~0oC|0
jlyYfioR{l{loR-|3|^oRyFoC~  j}npyY~fioC~b|Fk0oC~0fi|3C|b~y  oj}~W>offyYmfin^|3pyYnfijnoRRjlyY~>fim>
fi|  o~>y<|3RjlyY~>y>nfi|0~>fi{lj~>kQ>ofi|F{{ff<>ork0y>|3{y3>oWmfi~>j  oCnp|3{vw>{|b~:jlyfi|  oO|w>{}|/0oCn
zjlO>ofi|3{l{$j}~OnpyY~bvy3>oyYwfiwfiyY~>oC~bk0y>|3{zjl>yYm>fi|  j~>kO|0~0OyYwfiwMyY~>oC~0j}~W>ok0y>|3{$|0npoC|fi
rjlj}rwMy0j}>{lo:yhk0oC~>oCn|3o|gnpyY~>kQw>{|b~[fi|3^Ry  oCnpr|3{l{wMy0j}>{lo<j~>jlj}|3{|3oRC6i	yYn
j}~>|0~>Ror|0~j}~>jj|F{|3ozjl|b~WyYwfiwMyY~>oC~0{ly>C|3oR`j~`>ok0y>|3{|0npoC|^fi|3~>yOnyY~>ky0{}m>jlyY~
vm>|npyY~>kw>{}|0~^Ry  oCnpj}~>k|3r|b~0j~>jlj}|3{|3oR|3wMy0j}>{lojljl{l{m>om>{fffifioRC|0m>ojlfio~>oR
|3{l{>o[RyYnpj}~>k0<|FoRy3>oWkY|0oW|0~>Qmfinp>oCnrwfinpy  jlfioR|<w>{|b~QffyYnRyYnpj}~>k:>oWkbyY|3{~>y
r|3oCn>o|3RjlyY~>CM>oyYwfiwMyY~>oC~bw>{}|CboCnpR>yYy0o0
hoj}rw>{loCoC~boRc|0~ "!'# k0oC~>oCn|3y>nffyYny>RRoCnfiyY^|3j}~>zj:fij$oCnpoC~b	oR{l[jlRoR|0~>
~>mfiMoCnpy3|3k0oC~bC>om>{lj|3k0oC~bkYn|0wfi^j}~WijlkYmfinpoOqC>y3z$ w>{}|0~fi~>j}~>kWj}om>j~>k
>onyY~>kw>{}|0~fi~>j}~>kg|F{kbyYnpjlfij}~<y>RRoCnfiyY^|3j}~>zjl:F{ly>C|3jlyY~>|b~>:yY~>oyWjl:w>{}|CboCnp
yY~OoC|3ROoC|0>ow>{|b~fi~>j~>kOj}ooRoCykYnpy3z!oRdwMyY~>oC~bj}|3{l{`zjlO>o~>mfifioCnvy3w>{|/0oCnC
>jlWjl~>y0rmfinwfinpjlj}~>k|3W~>y0ryY~>{l>o<|3oWwfi|3Ro:fim>|3{lyQ>o<~YmfifioCnOyFy0j}~b|FRjlyY~>
kYnpy3z[oRfiwfiyY~>oC~bj}|3{l{lzj>o~>mfifioCnyF(|Fk0oC~0C(yj}~  oRjlkY|3o>oRyY^w>{oR>jlprj}~0nyY	m>RoRW0
py0j}~0|3RjlyY~>zvoRy>~>nm>RoR<|  oCnpjlyY~WyF>oyYRRoCnfiyYr|Fj~`zjl`yY~>{l|^j}~>k0{lor>oC|0~>
oC~  j}npyY~fioC~0r|3k0oC~b|b~>n|b~-$-|3k>|3j}~>oWj}~>k0{lo|3k0oC~bkYn|0wfihj}~:ijlkYmfinoqC`>y3z>o
	n|0^|3jlfioRCnpoC|3oj}~<RyYrwfim>|3jlyY~`j}o0j~>ybyY  jlyYm>>yYm>k>fi|3m>j}~>kyYnpor|3k0oC~b
j}~>CnpoC|3oRO>oWRyY^wfim>|3jlyY~fi|3{v{lyY|3$v|3>jl~>yYnr|3{l{lc|3{ly:npoR	m>RoR>o~YmfifioCny3wfinoRj^|3k0o
C|3{lCm>{}|3jlyY~>CfioRC|0m>o|{}|0npk0oCn~YmfifioCny3|3oRjlrnpoC|3R>oR-j}~hoC|3@hjloCn|3jy>~~>fioRoR$j}~Q|
 oCnpjy>~yF>owfiy3zvoCnw>{}|0~bfiyYr|3j}~zjlfioRoCnj}~>jjl|3Rjy>~>C3zvoffyYmfi~>>ow>{}|0~fi~>j}~>kj}oy
fioRCnpoC|3o  oRo>owMy3zoCnw>{}|0~bkYn|0wfiWj}~WijlkYmfinoqC 8fiz>oC~<yYnpo|Fk0oC~0zvoCnpo|3fifioR  oC~>oC~
qC0b R<ukY|3j}~[zvoWoC|3mfinpoR[>oWj}oOy>noR>n|FRj}~>k-|3RjlyY~>npyY >oWkboC~>oCn|3oRcmfi~>j  oCnp|3{
w>{}|0~>CWiyYn>om>{lj|3k0oC~b  oCnjlyY~:yF>o  oWw>{|/0oCny>RRoCnfiy>r|3j}~:>orpzy^ybj~b|FRjlyY~>
|3R>jlo  j}~>k>ok0yY|F{$>y3z~`j}~ijlkYmfinoq/zvoCnpooRYn|3RoRWnpyY >ormfi~>j  oCnp|3{w>{}|0~<j}~`{oRfi|0~
fibfiqoRRyY~>fiC
0

fi
	fifffffi vfi	  
  

fi}

10000
Multi-Agent
Single-Agent
Power Plant
1000

Time / Sec

100

10

1

0.1

0.01
0

2

4

6
Number of Players

8

10

12

ijlkYmfinporqCMt {}|0~fi~>j}~>kWj}oyF$yYnk0oC~>oCn|3j}~>knpyY~>krmfi~>j  oCnp|F{w>{}|0~>j}~Wy>RRoCnfiy>r|3j}~>
zjlyY~>oyjlw>{}|/0oCnpyY~oC|3RoC|bW$iyYn$>om>{lj|3k0oC~boRdwMoCnpj}oC~0oC|F@w>{}|/0oCn
z|F|3yYRj}|3oROzjlr|b~|3kboC~0C3z>j{loyY~>{l|j}~>k0{lo>oC |b~>oC~  j}npy>~fioC~0|3kboC~0
z|Fm>oRj}~>oj}~>k0{lo|3k0oC~bvoRdwMoCnpj}oC~0/>owMy*zvoCnw>{}|0~bvkYn|0wfi>y3zw>{}|0~fi~>j}~>k
j}oy>n|fioRoCnj}~>jljl  oCnpjy>~ry3>owMy3zoCnvw>{}|0~bvfiyYr|3j}~Wm>j}~>kqyWqCYoC
|3k0oC~bC

2

2

1

2

5

3
2

4

2

1

5

3

5

4

3
4
5

4

5
1

1
2

4

4

5
1

1

3

3

3

(a)

(b)

(c)

ijlkYmfinporqCMt {}|0~OoR>oRCm>jlyY~Wo"!fim>oC~>Ro0v>ofinpoRo|3oRv>y3z|r0fiwfiyb>oRjlC|3{$|3|F@rfi|FoRWyY~
|mfi~>j  oCnp|3{	w>{}|0~>o|3o  |(jl|ORyYnj~>kb|3o03fioRC|bm>o>o|F|3R3oCnp  >{|F@
C|0~`oRYn|3R|W~>y>~>oCrw>poRy3py0j}~b|3RjlyY~>npyY>ormfi~>j  oCnp|3{w>{}|0~$
 #>y>y0j}~>k
yYoybj~b|3RjlyY~>npyY>ow>{|b~fi>o|3|3R3oCnpC|0~^oC~0oCn>ok0yY|F{|0npoC|  fi|3fioR 
zjlW>ofi|F{{zjl>j}~Wpzvyj}ooCw>  |3o   |0~>  % v~>yr|3oCnzfi|3|3Rjy>~>C
>oyYwfiwMyY~>oC~bw>{}|CboCnpR>yYy0o0
&(')+*-,.",0/214365738".%39:*-;<1:=3>578

u~YmfifioCny3oRdwMoCnpj}oC~0fi|  ofioRoC~C|0nnpjloRWyYm>vj}~OfioRoCnj}~>jljlfiyYr|Fj~>vj}~Wy>npfioCny  oCnpj
$ wMoCn y>nr|0~>Ro<|0~>!j{l{}m>n|3o<>o<k0oC~>oCn|3{ljlp[y3mfi~>j  oCnp|F{w>{|b~>  oCnpm>OR{}|3jlC|3{ffo
0@?

fi00	0Q03	
!fim>oC~0j}|3{w>{}|0~>C:oRyYrwfi|0nponmfi~jonpoRm>{ly>>|3j}~>oRzjl<$Wj}~`yYoy3>ouBAC
RyYrwMoRjljlyY~QfiyYr|Fj~>y`>oWnpoRm>{ly3>oORyYrwfioRjjy>~[w>{|b~fi~>oCnp D "
E oCny0/qC0bR O:o

>oC~>y3zfi|3|Omfi~>j  oCnp|F{w>{}|0~j~<|fioRoCnj}~>jljlrfiyYr|Fj~jlyYnpok0oC~>oCn|3{$fi|0~`|R{}|3jlC|3{
"o !fim>oC~0j|F{w>{}|0~fioRC|0m>o|O{}|0npkbo~YmfiMoCnyFR{}|3jlC|3{o"!fim>oC~0j}|3{vw>{}|0~>r|0nporRyY~b|3j}~>oR:j}~h>o
mfi~>j  oCnp|F{w>{}|0~
	 GFffIHKJLNMGOP:Q  R  R    <S
i	y>mfinw>{|b~fi~>j~>k>oCUT3$(V	XW 6Y |0m>[Z\A>oR{}r|0~qC002R*fi 6Y y>oC>{oCnoR|3{ff}	q/00R]S R 
6^ y>~>_
k Zi	y3	qCb C|b~>  S/  |3{mfi+Za`o~>oCnR>000cbRyYrwMoRoRj}~O>ofinpoRofiyYr|Fj~>vzvo
fi|  orm>fijoR$` T3$d Vd W!jlfi|FoR:yYe
~ SR R  T3$z>jl{lo fiQ|0~>f
 S R c|0npo^kYn|0wfifiw>{}|0~bfi|3oR
w>{}|0~fi~>oCnpC  SCQm>oRr|<>oCmfinpjljloC|0npR-|0wfiwfinyY|3R:fi|3oR[yY~Q|<wfinpoCwfinpy>RoRj}~>k:y3>ofiy>r|3j}~
>ou gA Cw>{}|0~fi~>oCnpzvoCnponmfi~OyY~O00 D   yYnYb D % hfioC~bj}mfii
 B#zj<qC C D >oR
vu D "o !dm>j}wfiwfioRhzjl ^ j}~Ym>
 >oWkYnpj}wfiwfioCnOfiyY^|3j}~[RyY~>jlry3zvy:npy>yYru|b~>c|-npy>fiy0

zjl<|^{off|0~>:npjlkYbk>npj}wfiwfioCn|0~>:|O~YmfifioCnyFfi|3{l{fi|3C|0~Mory  oRb>onpyYMy0C>o
|3jlvyy  o|3{l{>ofi|3{l{lnpy> npy>yYu!ynyYyY6zjl>onpyYfiybj}~>jlj}|3{l{Wj}~rnyYyY u>>o
|Fo  |0npj}|0>{loRy3v>o "!# oC~>Ry>fij}~>ky3v>ofiyYr|3j}~<|0npo>owfiy0jljlyY~gy3>ornpy>fiy0|0~><>o
wfiybjljlyY~ryF>ofi|3{l{lC>owfiy0jljlyY~^y3>onpyYfiybjoRjl>oCn  nyYyY

u yYnvq  nyYyYfi R3z>j{lo>o
wfiybjljlyY~Wy3|fi|F{{(C|0~Wfio  npy>yY

u Rq  npyYy> fi Rfi  j}~W{loffkYnpj}wfiwMoCn vyYnv  j}~WnpjlkYbkYnpj}wfiwfioCqn R
i	y>n>our BA2 COkYnpj}wfiwfioCnwfinpy>>{oC>or~YmfifioCny3w>{}|0~<oCw>j}~:|b~WyYw>j}r|F{w>{}|0~`kYnpy3z{j}~b
oC|0np{lOzjl^>owfinpyY>{loC~>mfifioCnRs nyY>{loCqRyY~0|3j}~>rfi|3{l{lC|0~>O>o~>mfifioCnvyFfi|3{l{lkYnpy3z
bpzvyy>noC|3@<wfinpy>>{oC>onpoRm>{yF>ooRdwMoCnpj}oC~0jl>y3z~j~h|0>{lo^y0kboR>oCnzjl
>ornpoRm>{lyF>orw>{}|0~fi~>oCnpj~g>our BA C^RyYrwMoRjljlyY~u k>n|0wfi>jlC|3{npoCwfinpoRoC~b|3jlyY~`y3v>o
w>{}|0~fi~>j}~>kj}oj}~`>o|0>{lojl>y3z~Wj}~<ijlkYmfinpoO%q Cfi$
 tWkboC~>oCn|3oRj}~>j}mfi{loC~>k0:w>{}|0~>
	m>ory`jlwfi|0n|3{l{oR{vfinpoC|3fibffnoC|bnp@<|3{lk0y>npjlfiWrufioCw>jlRoRQj}~:ijlkYmfino%q Cfijl|  ybjfi>o
oRfiwfiyY~>oC~bj}|3{kYnpy3z^y3>ow>{}|0~fi~>j}~>kWj}ofi|3Rfi|0n|3RoCnpjlRoR|3{l{yF>oRy>rwfioRjljlyY~<w>{}|0~fi~>oCnp
oR>RoCw>  SC>oC~Om>j}~>k|wfi|0njljlyY~>oRWn|0~>jjy>~npoR{}|3jy>~$rjl>oyY~>{lw>{}|0~fi~>oCnvC|bwfi|0>{lo
y3k0oC~>oCn|Fj}~>kyYw>j}r|F{w>{}|0~>y>n|3{l{>owfinpy>>{oCCWi	y>n>jlfiy>r|3j}~:>on|0~>jljlyY~:npoR{}|3jy>~
y3|0~ "!'# fioRCnpj}w>jlyY~<C|0~<Mofij  jlfioR-j}~bv
y uKwqfi|3jlrwfi|0npjjy>~>Cz>oCnp$
o u:jl>or~>mfiMoCn
y3fi|3{l{lCufijlCm>oR<jx
~ A>oRRjlyY~<sfiM>oyYw>j^|3{~>mfiMoCnyFwfi|0npjljlyY~>jl~>y0~>oRRoR|bnpjl{`>o
{}|0npk0oR~>mfifioCny3wfi|0njljlyY~>Ci	yYn(>ovnpoRm>{lj}~|0>{looC|3@wfi|0njljlyY~RyY~>jloRy3|RyY~/mfi~>
jlyY~`y3qCWfi|Fjlwfi|bnpjljlyY~>Cy
 #yYrwfi|0npoR`yO>oryY~>y0{ljl>jln|0~>jljlyY~<npoR{}|3jlyY~<npoCwfinoRoC~0|3jlyY~
>onpoRm>{lvyY>|Fj~>oROzjl^>owfi|0njljlyY~>oRWn|0~>jjy>~rnpoR{}|3jlyY~^zvoCnpojk>~>j	C|b~0{lWfioRoCnyY~^>o
{}|0npk0oCnwfinpyY>{loCCW>oOoCyYnp:m>|Fk0ory3wfinpyY>{loC 0Wzjlh|wfi|0npjjy>~>oR-n|0~>jljlyY~:npoR{}|3jy>~
z|3[ Cb D YoR/0z>jl{lojloR>RoRoRfioRW>o{ljjvy3vqC C D YoR|3vwfinpyY>{loCqCffyYn>oyY~>yb{jl>jl
n|b~>jljlyY~WnpoR{}|3jlyY~
j$kd,ml/n3opop,0/*-;<1:=3>5

 ~>oy  jlofiyY^|3j}~[>o`|3hjlyhk0oRR>j}w>Cvfij}wwMyYwv@>oRoRo

|0~>:Cn|3@FoCnpC$npoRzj}~>-|`y  jlor|0~>:oR>orRyYmfi~boCnyWRoCnpyr>oryY~>{l<j}~0oCnoCnpoC~>RoWfioRpzvoRoC~
j$kd,{za;|N3G,\*-;71x=365

}~7~ sq6sn6%gg 6@fid](@fiqn"ndnfi rr%p nd@g~
 ~7n6Un
"q$]@r$r][qn fi6nfi]nrfig6@fi 6@qfir% q]
nrNr"6]]@~UNnfi6B<
pr[6]dns>nq
]7@gnfidsq@g
N
 r%nfi"%]prfiqnfir"]r@7]@Unsdd6@s
p  %]fi%@r"r6q[
d6sqyfi@fi
N  r%nnqy]s@g~

0@

fi
	fifffffi vfi	  
  

  q%





}






 




}



q

N2  ~
q
 


 
q
@
q
@}@
"
 q}@

  
 
q@@
q
}
q
q}q@
}"
 
@@

    
    

q@}"

  
"
q@}

 }@"
 
  "

}cq@

%


























N2 

 qn~


@

 @
q
q@
q
}@
"
@

q@
 
@
q
}  q
q
"
}"
 
" @ 

"}@
%cq

  "}

}@}q
"
  }}@










r"
}
  
q@}
@@

















@




































fi}
r
q 

q
}@
"
  
  
q}
}}  
 q@
}@
} 
cq@
cq"
}@}
@
}
}  
""
@"

 
"
%
 
 

}@
 


@
%
@

@

@
"
 
}



@
  
}}@
}@}
















 
q
 

 































"]


 q






































|0>{lofit `npj}wfiwfioCnfiyYr|3j}~WnpoRm>{l/B#vy0{}mfir~OyY~>o|b~>WzvyyYnoC|3RWw>{}|0~fi~>oCnv>y3z>ow>{}|0~fi~>j}~>k
j}oj}~j{l{ljloRRyY~>fi|0~>^>ow>{}|0~^{loC~>k0 t-|0np/|0~>W$ D yY~>y>y*z>o
w>{}|0~fi~>j}~>kjoffyYn(m>j}~>k|Owfi|0npjljlyY~>oR:|0~>:|ryY~>y0{ljl>jlrn|0~>jjy>~noR{|FjlyY~
npoRwfioRRj  oR{l0i	yYn$(zjlwfi|0njljlyY~>oRn|0~>jljlyY~npoR{}|3jlyY~>ov>j}npRy0{}mfir~>y3z
>ov~YmfiMoCn$y3	wfi|0npjjy>~>C   (oC|0~>fi|3$>ow>{}|0~fi~>oCnm>oRyYnofi|0~rq/ C D >oR$y3
oCyYnpy>n$z|3oCnj}~fi|3oRrfioffyYnponoRmfin~>j}~>k|yb{m>jy>~ ~>{lnoRm>{ly>noRYoRCm>jlyY~>
m>j}~>kO{loRfi|0~<qC C D YoR|0npo>y3z~^ffyYnv$
>omfi>k0yY|F{jlfi|3>oy  jom>MonoRzy>mfi~>$YfioffyYnpo>oRyYmfi~boCnC|0~OfiooRvyRoCnpy>o
wfinpyY>{loCj}~:>oOy  jlorfiyY^|3j}~<yY~>{l<fij$oCnb>o~>mfifioCny3yY>poRRy3oC|F@<pfiwfioryFffyYy>$
>o~>mfifioCny3y>>oRRj}~>CnpoC|FoR{lj}~>oC|0np{lnpy>sffyYfin npyY>{loC qyr3ffyYfin nyY>{loC 0M
mfin "!# fioRCnpj}w>jlyY~y3>oy  jofiy>r|3j}~noCwfinpoRoC~boC|3Rrpfiwfioy3$ffyYy>|3|~YmfioCnpjlC|3{
|Fo  |0npj}|0>{loWzjlh|n|0~>kbo"o !fimfi|3{y>oO~YmfifioCny3yY>poRRy3fi|3pfiwfioryFffy>yY$O|0>{loW
>y3z>ow>{}|0~fi~>j}~>kjoffyYn$|0~>>ovRyYrwMoRjljlyY~rw>{}|0~fi~>oCnpffyYn>oy  jlofiy>r|3j}~wfinpyYb
{loCC ~r>joRdwMoCnpj}oC~b|0~>r>onpoCr|3j}~>j}~>kroRfiwfioCnpj}oC~b$rm>oRjlfio|0m>{lwfi|0npjjy>~>j~>k
y3>oWn|0~>jljlyY~npoR{}|3jlyY~Qi	y>no  oCnpwfinyY>{loC |3{l{>ow>{|b~fi~>oCnpr~>[>oWyYw>j}r|3{y0{}m>jlyY~
^ j}3oyby3>oRyYrwfioRjjy>~Ww>{|b~fi~>oCnp$^fi|3|{ly3zcRyYrwfim>|3jlyY~^j}o0fifim>jlvjlv>oyY~>{l
w>{}|0~fi~>oCn~>y0>y*zj~>k<|0~bj}~>CnpoC|Foj}~hRyYrwfim>|FjlyY~<j}oro  oC~:>y>m>kY$>o^jlRoy3>o^|3o
wfi|3RoyFjloC~>RyYfij}~>kOj}~>CnpoC|3oRnyY0py *
 >oW{ly0k0jljlRrfiy>r|3j}~  oR{ly0y	qC0FRyY~>jry3RjljloRCvnm>@bC
[
|3j}nw>{}|0~>oRv|0~>rwfi|3@/|3k0oRCnm>R0C|0~yY~>{lry  ovfioRpzvoRoC~{lyYC|3jy>~>j~>ov|0ovRjlp0uj}nw>{}|0~>oR
C|0~^yY~>{ly  oMoRpzoRoC~^|3j}nwfiyYn{ly>C|3jlyY~>vj}~rfij$oCnpoC~bvRjjoR/>o|3jlvyy  owfi|3@/|3k0oRvy
wMoRRj	^{y>C|3jlyY~>/nyY>{loCfij&oCnbW>o~>mfifioCny3wfi|3RC|3k0oR/	RjljloRC$|3j}nw>{}|0~>oR|0~>nm>@bC
>oW{ly0k0jljROfiyYr|Fj~Qjlfi|bnp$|b~>[yY~>{l
 npyY>{loC q0MsfiW|0~>qbqWy3>obwfinyY>{loCrzvoCnpo
y0{  oRcb-|0~b-w>{}|0~fi~>oCnj}~Q>oOu BA C`RyYrwfioRjjy>~  oRoW|0>{loW 8>o "!# fioRCnpj}w>jlyY~
j$kd,\s;U(38".%398*-;71x=365

0

fi00	0Q03	

10000

1000



Time (Sec.)

100

10
UMOP Part.
UMOP Mono.
STAN
HSP
IPP
BLACKBOX

1

0.1

0.01
0

2

4

6

8
10
12
Problem Number

14

16

18

20

ijlkYmfinporq%CMt {}|0~fi~>j}~>kj}oy>n$`|0~>>ourBA2CrRyY^wfioRjljlyY~<w>{}|0~fi~>oCnpffyYnv>okYnpj}wfiwfioCn
fiyYr|3j}~:wfinpy>>{oCCtx|0np/|0~>Q$ D yY~>y	>y3z>orw>{}|0~fi~>j}~>k:j}offyYn
$Wm>j}~>kW|rwfi|0npjljlyY~>oR<|0~>|^yY~>y0{ljl>jln|0~>jljlyY~noR{|FjlyY~	npoRwMoRRj  oR{l0
y3>oW{ybk0jljlRrfiyYr|Fj~[m>oR~YmfioCnpjlC|3{|Fo  |0npj}|0>{loROynpoCwfinoRoC~0^{y>C|3jlyY~>^y3wfi|3RC|3kboRC
z>oCnponm>@b|0~>W|3j}nw>{}|0~>oR|bnponpoC|3oRW|3vwfioRRj}|3{({y>C|3jlyY~>/  oC~r>yYm>kY^>o|3owfi|FRoy3
>o^|3{l{wfinpyY>{loCjly>fioCn|3o0d(O|3jl{lyyb{  o|0~0Oy3>owfinpyY>{loCj}~W>ofiyYr|3j}~
m>RRoRoRfiyrk0oC~>oCn|3o>on|0~>jljlyY~noR{|FjlyY~Wfim>|3jl{ly~>jl`>owfinoRj^|3k0oC|F{Cm>{}|3jy>~>C
hofi|  om>fijloR<>o{ybk0jljlRfiyYr|3j}~oRYoC~>j  oR{0$noRRoC~0{ly>Cm>j}~>ky>~	$fi|FoR<fio
oCnj~>jlj:w>{}|0~fi~>j}~>k	>o`{ybk0jljlRWfiyYr|3j}~6oRoCOy-fio<fi|0np!m>j}~>kc|:w>{}|3j}~	(fi|3oR
|0wfiwfinpy>|3@|3>ojlRoRy3>owfinpoRj}r|3k0oRkYnpy3z!y>y|3C(y|3fi	npoR>jlRyY^w>{oR>jlp0	zvofi|  o
fio  oR{lyYwMoR|0~`|0>n|3RjlyY~`oRRfi~>j !fim>offyYn$fi|3oRQfioRoCnj~>jljOw>{|b~fi~>j~>kr~<|W~Ym>>oR{l{
|WwfinpyY>{loCjlny0{  oR:m>j}~>k|b~|b>n|3Rn|b~>jljlyY~`YoCWz>oCnpooC|3R`n|0~>jjy>~Ry>nnpo
wMyY~>fiy|WoRy3oCnpj}|3{ljlC|0>{lo|3RjlyY~>CW>oC~h>oroCw>j}~h>o|0>n|3Rw>{}|0~Q|0nporoCnj|F{jlRoR
m>j}~>k|0~yYnpfij}~fi|0npWn|0~>jljlyY~WYoCWv!jW>jl~>oRz |3{lk0yYnpjlfiWMzofi|  oMoRoC~|b>{oyOy0{  o
o  oCn|3{y3>oRyYrw>{loRWur BA2 CRyYrwMoRjljlyY~`{ybk0jljlRwfinyY>{loC  oC~>oC~WoR|3{ff}fi000 8
	 GFGF(f  S R $d T Q  
>oy>>|3R{lofiyY^|3j}~fi|3MoRoC~rRy>~>nm>RoR^yfioCy>~>n|3o>ok0oC~>oCn|3{ljlpy3mfi~>j  oCnp|3{w>{|b~>C
ffRyY~>jly3|WkYnjQzvyYnp{l:zjlh RoR{l{lCB u[y>>|3R{loR|b~>-|npy>fiy0|3k0oC~bC>oOwfiy0jljlyY~>y3
>ory>>|3R{loR|bnpor~>y0fio~>oR$>o^k0yY|3{wMy0jljlyY~<y3v>onpy>fiy0jl>omfiwfiwfioCnnpjlkY0RyYn~>oCny3
>ork>npjl$|b~>:>oO|3yYn>onpy>fiy0jlyy  onpyY|b~0:wfiybjljlyY~:j}~h>orkYnjQyW>o^k0yY|3{
wfiybjljlyY~WoRC|bm>o>orj}~>jj|F{{ly>C|3jlyY~>y3yY>|3R{loR|0npormfi~fiY~>y3z~>oOmfi~>j  oCnp|3{vw>{|b~-m>
|0Fo|0~bwfiy0j}>{owfiy0jjy>~Wy3$yY>|3R{loRj}~0yr|3RRy>mfi~0C>z>jlROk0j  oR n<   j}~>jlj}|3{$|3oRC
i	y>n|wMoRRj	j}~>jlj}|3{|Fo|"o !dm>oC~bj}|3{w>{|b~rC|0~^fiok0oC~>oCn|3oRnpyY >omfi~>j  oCnp|3{w>{|b~>m>C
 n<   W"o !dm>oC~bj}|3{w>{}|0~>|0nporRy>rwfinpjloR:j}~<yY~>oOmfi~>j  oCnp|3{vw>{|b~
 y0ofi|3|Wmfi~>j  oCnp|3{
0@

fi
	fifffffi vfi	  
  

  





}





 




}



q
 

q
q

q}
 
q
q


|0>{lofit

N2





}






}

}

}

}

}

}

}
}
}


}

"2



q



q

q}
 
q
q

"

"
 


"
"

q

q
 
}"
}
} 

r
"
"
"
"
@q
} 
}

" 
@
" 
"q}
@  
@  
@ 
}
}"
}}
   
  


}}
@
 
@
q
@
@
% 

%






q
q
 

 
q
 
 

 

 
q
q
q
q
q
 
 
q
 
q
 



fi}
"%]
@
c

}


@

}

@

@}

@
@
%
@

 
@
"

@
 

}@
}@
}@
}@}

y  jlo<fiyYr|Fj~!npoRm>{C >o<|0>{lo<>y3zO>o:nmfi~j}o<j}~jl{l{jloRRy>~>fi<y>nroC|3R
>w {}|0~fi~>oCnR  oC|b~>fi|3>ow>{}|0~fi~>oCnm>oR[yYnpo^fi|0~[qC2C D >oRyFoCyYnp`yYn
z|3voCnj~fi|FoRMoyYnonpoRmfin~>j}~>kW|y0{}m>jlyY~u{l{w>{}|0~fi~>oCnpk0oC~>oCn|3oRWyYw>j}r|F{&w>{}|0~>
y3{oC~>kbM[ tOm>oRO|0n{loRfi|0~`qC C D >oRffyYnv|0~bwfinyY>{loCj}~O>jlfiy>r|3j}~
D

w>{}|0~OzjluyY>|3R{loRvj}~>R{}m>fioR|0~0Omfi~>j  oCnp|3{$w>{}|0~Ozjl<qyuWyY>|3R{loRC	|3vyY>|3R{loRC|b~Mo
w>{}|3RoR|F>o|0o{ly>C|3jlyY~B y0oy>npoRy  oCnR0fi|3v>omfi~>j  oCnp|3{w>{|b~>~>o  oCnRy  oCn|F{{(j~>jlj}|3{
|FoRCMoRC|0m>oWyY>|3R{loRC|0~QfioWw>{}|3RoRc|3>ork0yY|F{wfiybjljlyY~|0~>:y>>|3R{loRC|0~Q>{y>R<>o
npyYMy0C
umfi~>j  oCnp|F{w>{}|0~yYn^|0~[yY>|3R{lofiyY^|3j}~[zjls<yY>|FR{oROz|3k0oC~>oCn|3oRczjl$
j}~QY0oRRyY~>fir|0~>QRyY~0|3j}~>oR[ CC00bW$~>y>fioR  qCfi D >oR@ R
 A>"o !dm>oC~bj}|3{w>{}|0~>zvoCnpo
oR>n|3RoRnyY>omfi~>j  oCn|3{w>{|b~`y>n|^wfioRRj	wMy0jljlyY~<yF>oyY>|3R{loRCijlkYmfinpoWqC^>y3z
>ooR>n|3Rjy>~rj}oyF"o !fim>oC~bj}|3{w>{}|0~>vffyYnv|0~Oj}~>CnpoC|3j}~>kW~YmfiMoCnvy3oCw>j}~W>ow>{}|0~  oC~
>yYm>k><>o	$!npoCwfinoRoC~0j~>k<>oOmfi~>j  oCnp|3{w>{}|0~hj{}|0npk0o0>oroR>n|FRjlyY~<jl  oCnpW|3|0~>
yY~>{lWkYnpy3zv{lj}~>oC|0np{lzjO>ow>{}|0~W{loC~>k0
>ooRy3|FRjlyY~>|3y>Rj}|3oR<zjl<|^|3oj}~:|Omfi~>j  oCnp|3{w>{}|0~O<jloRYn|3RoR<0`RyY
wfim>j}~>k>oRy>~/mfi~>RjlyY~gy3>or$cnoCwfinpoRoC~b|3jlyY~`y3r|b~>$ufioRCnpj}fioRQj
~ A>oRRjlyY~:fi
>jly>wfioCn|3jy>~:fi|3|b~:mfiwfiwfioCnfiy>mfi~>:RyYrw>{loR>jp:yF       8i	yYn>omfi~>j  oCnp|3{w>{}|0~<j}~h>o
0@

fi00	0Q03	
  






N2


r"
} 
 
%
" 
}@@"
q 


cq@}
















r
 }
  
@
 @
@}

@
@ 
}
c
@


@
 }


q@



}@
@


q@}"
 
}@}
"




}"
"

|0>{lo	t ^ by k0jljlRvfiyYr|3j}~OnpoRm>{lCiyYnoC|3Rw>{}|0~fi~>oCnRy0{}mfir~Oy>~>o|0~>Ozvy>y3z>onmfi~^j}o
j}~:jl{l{ljloRRyY~>fi|0~>h>orw>{|b~:{loC~>k0  oC|b~>fi|3>ow>{}|0~fi~>oCnm>oR-y>npofi|0~
qC2 C D >oRy3oCyYnyYnz|3oCnj}~fi|3oRMoyYnonpoRmfin~>j~>kO|y0{}m>jlyY~
0.008

0.007



Time (Sec.)

0.006

0.005

0.004

0.003

0.002

0.001
0

2

4

6
8
10
Number of Plan Steps

12

14

16

ijlkYmfinporqCMtvj}oyYnoR>n|3Rj}~>kWo"!fim>oC~bj}|3{w>{}|0~>nyY|Wmfi~>j  oCnp|F{w>{}|0~`ffyYnv>oy>>|3R{lofiy3
r|3j}~WzjlWsyY>|3R{loRC
yY>|3R{lofiy>r|3j}~`zjlO  oyY>|3R{loRC>jlRyYrwfim>|3jy>~z|3|3  {loRfi|0~WyY~>ojl{l{jloRRy>~>
|0~>WzvyYm>{l|F{{ly3z|0~oRYoRCm>j~>kWnpyYMy0vyoRoR{y3znoC|3RjlyY~WjoRyY~>n|Fj~bC

 7

s-7ggnr7mdBmsg

 ~`>jl|0njlR{o^zofi|  orwfinpoRoC~boR:|~>oRz 	$fi|3oR-w>{}|0~fi~>j}~>kYoCW$$My>nw>{}|0~fi~>j}~>k
j}~!~>yY~bfioRoCnj}~>jljl0m>{lj|3k0oC~bWfiyYr|Fj~>/u~[oRfiwfinpoRj  ofiy>r|3j}~fioRCnpj}w>jlyY~!{}|0~>kYmfi|Fk0o0

	!	fi|3MoRoC~Wfio  oR{lyYwMoR:|0~>|0~Ofio ffORjoC~b$[npoCwfinpoRoC~b|3jy>~Oy3jl ifiuoC^|0~0jRfi|3
fioRoC~gfioRCnpj}fioR$hofi|  o|0~fi|3{l>RoRgwfino  jlyYm>w>{}|0~fi~>j}~>k|F{kbyYnpjlfiffyYn	$fi|3oR-w>{}|0~fi~>j}~>k
|0~>fioRoCwfioC~>oR>omfi~>fioCn|0~>fij}~>ky3dz>oC~>oRovw>{}|0~fi~>j}~>k|3{lk0yYnpjlfi|0npo|0wfiwfinpy>wfinpj}|3o0ij}~fi|3{l{0
zvofi|  owfinpy>wfiy0oRQ|0~<yYw>j}jljWw>{}|0~fi~>j}~>k-|3{lk0yYnjfiffyYn~>fij}~>k<oC~>j}>{loWy0{}m>jlyY~>j}~:yYo
fiyYr|Fj~>z>oCnpo~>ynpyY~>kyYnvnpyY~>krR>R{ljlry0{}m>jlyY~oRYjl/>onpoRm>{yY>|3j}~>oRzjl`$
|0npooC~>RyYmfin|3k0j}~>k	fi|3(rfi|3v|k0y>yYOwfioCn ffyYnr|b~>RoRyY^wfi|0npoRryyYoy3$>o|3oRR{}|3jlC|3{
w>{}|0~fi~>oCnpY~>y3z~OyY	|/0
C

fi
	fifffffi vfi	  
  

fi}

mfin^npoRoC|0npRQfi|3r	n|/z~hyYmfin|3oC~bjlyY~hy:|:~>mfiMoCny3y>wfioCe
~ !fim>oRjlyY~>rfi|FzvoWzvyYm>{l
{lj}3oy|3fi	npoRj}~h>om>mfinpo0r~hwfi|0npjlCm>{}|0nzvorzvyY~>fioCn>y3zzvoR{l{yYmfinoC~>Ry>fij}~>k<y3w>{}|0~fi~>j}~>k
wfinpyY>{loCC|3{loRRy>rwfi|0npoRy>ovoC~>Ry>fij}~>km>oR0s #mfinnpoC~b{lr oC~>RyYfij}~>kfiy>oR~>y0
mfiwfiwMyYnp|rwfi|0npjljlyY~>oR<npoCwfinpoRoC~b|3jy>~Wy3>on|0~>jljlyY~WnpoR{}|3jy>~fim>>ooC~>Ry>fij}~>kr|/fi|  o
y0>oCnwfinpyYwMoCnpjloRfi|3CfioRw>jlor>oryY~>yb{jl>jlnoCwfinpoRoC~b|3jlyY~$r|/<r|03oj|WMoRoCn@>ybjRo0
>oOpzyWYoCr|/:|3{ly<fi|  o|0~<"o !fimfi|3{wfioCn ffyYn^|0~>Rorz>oC~Qfiy0-|0nom>j}~>k:|yY~>yb{jl>jl
npoCwfinpoRoC~0|FjlyY~  |3j}~h>oMoC|0 z|3{}<oRd|brw>{l%o Rz>jl@Q>y>m>{[kbj  o$:|0~Q|3  |0~b|3kborj}~
fiyYr|Fj~>z>oCnpo|^wfi|0npjljlyY~>j}~>kOy3>on|0~>jljlyY~WnoR{|FjlyY~WC|0~`fiofio~>oR$
u~>y0>oCnj}~boCnpoRj~>k !dm>oRjlyY~rjlyj}~  oRjlkY|3ovz>jlRbj}~>ry3&w>{}|0~fi~>j}~>krfiy>r|3j}~>jlm>j|0>{lo
ffyYn$fi|3oRw>{}|0~fi~>j}~>k	z|3mfinwfinpjlj}~>kyYnm>fi|3>o{ly0kbjjlRfiy>r|3j}~mfin~>oRryYm>yMo
yrfi|0npOffyYnv$voRRoC~b{lWzvofi|  om>fijloR`>jlfiyYr|3j}~O>y>npyYm>kY>{l0 j}~>k|0~O|0>n|3RjlyY~
oRRfi~>j !dm>orzvofi|  o~>y3zfioRoC~h|0>{loyry0{  oo  oCn|3{$y3>o{ly0k0jljRwfinyY>{loCj}~>ou BA C
RyYrwMoRjljlyY~  oC~>oC~WoR|3{ff}	00b R
>oCmfinnpoC~bfio~>jljlyY~y3 
	:jlwfiy3zvoCn m>{>fim>>y>m>{fiooRYoC~>fioRyoC~fi|0>{loy>fioR{l{j}~>ky3
RyY~>nm>Rj  ovfi~>oCnpk0oRjlvo$oRR|FfioRCnjMoRj}$
~ A>oRRjlyY~	u{y	3zvooC~  jljlyY~ryYnpooRfiwfioCnjoC~0
RyYrwfi|bnpj}~>km>{lj|3k0oC~b|b~>:j}~>k0{lo|3k0oC~bfiyYr|3j}~>yj}~  oRjlkY|3o^>orRyYrw>{loR>jlhy3
 	 
npoCwfinpoRoC~0|FjlyY~Oy3RyY~>CmfinnpoC~b|3RjlyY~>/
A>o  oCn|3{w>{}|0~fi~>oCnp/	j}~<wfi|0npjlCm>{}|0n  $   oR{ly0yoR|3{ff}$qCb0s Rfifi|  o>y*z~Ofi|3fiyYr|3j}~
>~>y*z{oRfikbo>y>m>{Wfiom>oRbr|w>{}|0~fi~>j}~>kr>oCj}~^yYnpfioCnyC|3{lomfiwrynoC|3{zy>np{lwfinpy>>{oCC
u{y  v|3R@>m>y
 Z Y |0fi|b~>C|fiqC002 >y3z>y3z>ooC|0npR`npoRoyF|^ffyYnpz|0npWRfi|3j}~>j}~>kgw>{}|0~fi~>oCn
C|0~Ofiofio ffRjloC~b{lwfinmfi~>oR0r|3j}~>k>ok0y>|3{|3v|yYnm>{}|j~OoCrwfiyYn|3{	{ly0k0jlyY~^>o"o !fim>oC~>Ro
y3$|3RjlyY~>{loC|3fij}~>kry>ok0yY|F{~^>jlz|/>ok0yY|3{>C|0~j}~>R{}m>fio>~>y3z{loRfik0o|0MyYm>>ofiyYr|3j}~
 o0 k}Cfi|3y*zvoCnpj}~>o>{y>R0zy>np{lm>fiofim>j{lnpy>My0yYyyY
w R(ucj}jl{}|0n|0wfiwfinpyY|3R
ffyYnnpoR	m>Rj}~>k:>orRyYrw>{loR>jpQy3$fi|3oRcw>{|b~fi~>j~>khoRoCrwfinpyYjlj}~>k	oRwfioRRj}|3{l{lcfioRC|0m>o
oRRfi~>j !dm>oRy>nvoRj}~>kroCrwfiyYn|3{ffyYnm>{}|3|3{}npoC|3fifi|  ofioRoC~fio  oR{lyYwfioR<j}~yYfioR{$R>oRR0j}~>k	
>oCnm>mfinpoW@fi|F{{loC~>k0oRWj}~>R{}m>fio:j}~bnpy>	m>Rj~>k[|0>n|3RjlyY~Qj}~c	(fi|3oRw>{}|0~fi~>j}~>kc|0~>
fio~>j}~>kwfioRRj}|3{ljlRoRw>{}|0~fi~>j}~>k|3{lk0y>npjlfiffyYnm>{lj|3k0oC~bfiyY^|3j}~>  o0 k	}3|3{lk0yYnpjlfim>j}~>k>o
{loC|3~>mfifioCny3|3k0oC~byYny0{  j}~>kW|wfinpyY>{loC
 R

  sBU
\U7
AfiwfioRRj}|3{fi|0~fibvy|3y0{lyn|

 oCnpy	 D |0npRyrvy  oCnj&|b~>>oyb>oCnvoCfioCnpvy3>o Afi!k>npyYmfiw
ffyYnj}~0npyY	m>Rj}~>k[m>yg	|0~>[ffyYnr|0~b[npoRz|0npfij}~>k:fijlCm>jlyY~>Wy>~c	$fi|FoRw>{}|0~fi~>j}~>k
|0~>[y>fioR{R>oRR0j}~>k	h:o|F{y:zjlhy<fi|0~fi:|b~>	|3{vnpY|0~bC	mfi~>
 #v{}|0n3o0oC~finpj}
(u~>fioCnpoC~   n~ ^ j}~>> jloR{}oC~|0~> ^ |0npj}nFoR	|3{ffyYn|F  jlRoOyY~:	$!jlm>oR|b~><y>nr|3{
npoCwfinpoRoC~0|FjlyY~ij~fi|F{{l0zvofi|0~fi>o|0~>yY~bdyYm>npo  jloRzvoCnpyYn>oRjnRyY^oC~0fi|3kYnpoC|3{
j}rwfinpy  oRW>owfinpoRoC~b|3jlyY~OyF>jl|bnpjlR{lo0
>jWzvyYn:z|3OC|0nnpjloR!yYm>Oz>jl{lo:>o`npW|0m>>yYn^z|3  jljlj}~>k #|0n~>oRk0jlo D oR{l{lyY!
~ ~>j
{ ~>j  oCnpjp:yF EoC~fi^|0n>oWnpoRoC|0npRhjwMyY~>yYnoR-j}~[wfi|0np0
 oCnpjp`npyY>o(oR@fi~>jlC|3"
D  Y j}~>oRf
 Z+#vyYrwfi|b~007 A>oR{}oC$
n Zn|0~>o0 i	y>~>$$>
o EoffoC~>ou  |0~>RoR[voRoC|0n@4
 npy0poRR

^

^
uk0oC~>R Eu 

u |b~>>ouj}ni	y>npRovoRoC|0n@ |0MyYn|3yYnp ui mfi~>fioCn|3kYnoRoCoC~0~>mfiMoCn
i00b0/0/*00s0fi>o  jloRz|b~>rRyY~>R{}m>jlyY~>vRyY~b|3j}~>oR>oCnoRj~O|0npov>yboy3>o|0m>>y>np|0~>
>yYm>{l-~>ybfiorj}~boCnwfinpoRoRQ|3~>oRRoR|0npjl{l-npoCwfinpoRoC~0j}~>k`>o#y ffRj}|3{wfiyb{jlRjloRryYnoC~>fiyYnpoCoC~0C
C0

fi00	0Q03	
oRjl>oCnvoRfiwfinpoRoRWy>nj}rw>{ljloR$	y3>oEoffoC~>ou  |0~>RoRoRoC|0npRnpy0poRRuk0oC~>R  Eu	u
R
>ouj}niyYnpRovoRoC|bnp@ ^ |0MyYn|3y>np  ui ^ vy>nv>o$A`y  oCn~fioC~0/

&%%

('  *)
+-,-.{gBgK/B1032 s\54
j$kd,;(/,17698;: =<?>A@CB0DC@<E3BFG>GHIfi<KJL(< E3<EM>RDCFNPORQS@ET UV@GTVFGW
LX FYEZLXVFI[F\F^]_<;>`L>-@CB0DC@<E
BF^>`HYIa<bJcL(< Ede<fEd
	hg<LXiLX FC>G@DCFP>`FYDC@EL(<HG>@#>\Pj
klpI / : t ^ oRmn\o qpZrastrau fioC~>y0o>o ifiu  oRo[Eo~>jljlyY~`q%(RyYnnpoRwfiyY~>fij}~>ky>ovoCr|0~bjlR
y3v[
 |3fio~>oRb `j}mfi~>@>jlk0{lj}|oR|3{ff  qC002Ru~w-	 fiyYr|Fj~fioRCnjw>jy>~x
 zjloCr|0~bjlR
o"!fimfi|3{fiy$m n C|0~MoRyY~>nm>RoRj}~r>offy0{l{ly3zj}~>kz|/t ^ oRA
 fio|j}~>k0{lo|3kboC~0vfiyYr|Fj~z>oCnpo
|3{l{y m>oC~b|0npooC~>Ry>fioR|3~>mfioCnpjlC|3{|Fo  |0npj}|0>{loR|0~>W>oCnojl|b~W|3RjlyY~^ffyYnvoC|3ROoR{loCoC~0
j}~<>or|3{}wfifi|0MoR s y3zm n y#yY~>jlfioCn>or|3RjlyY~&
{ |Fy>Rj|FoR<yWj}~fiwfim>A| s  ^ oR>ooRy3
RyY~>n|3j}~>oR<|3o  |0nj|b>{oRy3l
{ o"!dmfi|F{>ooRyF|Fo  |0npj}|0>{loRj}~} >orwfinpoRRyY~>fijljlyY~<y3
{ jl|0~`oRdwfinpoRjlyY~<fi|3fio~>oR>ooRy3v|3oRfi|  j}~>k<|0~`yYm>k0y0j}~>kOn|0~>jjy>~`y>nj~fiwfim>
W
>oro$oRRRyY~>fijljlyY~:yF*`
{ jlr|ORyY~/mfi~>RjlyY~:yFRy>~>fijjy>~fi|3{vo&oRR (~ C RO>oCnporjlyY~>o
RyY~>fijljlyY~fi|3{o$oRR^y>nroC|3R|3oWfi|3Wfi|3|0~[y>m>k0y0j}~>kQn|0~>jljlyY~6ffyYn^j~fiwfim> ~
 j}~>o
RyY~>fijljlyY~fi|3{o$oRR|3yYRj}|3oRzj|3ojl>oRfi|0n|3RoCnpjljloRdwfinpoRjlyY~`ffyYn^|0~>  jl|
Rfi|0n|3RoCnjjloRfiwfinpoRjlyY~yYn>ooRy3~>oRY|3oR u   r rR
 V@ 2BU
v|3RRYm>Ci}pZ Y |bfi|0~>C|fii  q/00R3 j~>k<oC^wfiyYn|F{{ly0k0jlOyRyY~bnpyb{oC|0npR:j}~Q|OffyYnpz|0np
@fi|3j}~>j}~>krw>{}|0~fi~>oCnR$~K`fi|3{l{}|0 D }Z D jl{|b~>j>u  fiCRVFgB<I[FaHYL(< E>A<fEC 8 JcQ@EE<E>T 
wfiw$qR	qfi qCs0fi>A npoRC
v|0n|3{ff#%ZI`oR{y>~>$ D   qCb0R0voC|FyY~>j}~>k|0MyYm>o&oRRy3	Ry>~>CmfinnpoC~b|3RjlyY~>CX F*d UVIfiE@Q
 : $	 T <Hk-pI T I[@DCDC<EYT C0s_ q0qCfi
UBg

oC~>	|fi D   |3kY|0~fi~fi|3fi|0~  }UZmEy>	>j|/z|3{}|fi	  qCCbRN~yYw>j}r|3{(RyYyYwMoCn|3jlyY~y3Y~>y3z{
oRfik0oy>mfinpRoR$$|0~oC^w>jnjC|F{fij~  oRjk>|3jlyY~Y(oRR0npoCwCfi# A0`0fiqCYfiCfiCy>oRj~>ku  |b~>RoR
$oRRfi~>y0{ly0k0
 #voC~boCnR	y>oRj~>
k #vyY^wfim>j}~>kA>oCn  jRoR/
{}mfiWfiu} ZimfinpC D  ^   qC0b Ri|3vw>{}|0~fi~>j}~>kfinyYm>kYOw>{|b~fi~>j~>kOkYn|bwfir|0~fi|3{l>jlCIaL(< lHY<f@Q
8 EL(FYQfQ<T FEHaF/ #0	2 Cfifiq d0bfi
{lY>o0    qCb CRV k-QS@EE<ETPU EBFI-UVEHfiFIfiL(@<fEL(<fEwB#E@DC<HB0 DC@<E>R0  Er/>oRjlC #vyY^wfim>oCn
A>RjoC~>R$
o EoCwfi|0noC~bC #|0n~>oRk0jlo D oR{l{lyYi
~ ~>j  oCnpjlp0[ # D  # Ab C*qRYfi
{lY>o0  } Z  oR{ly0y D  D   qCb0 R(u~fi|3{ly0kbjC|F{npoCw>{}|/rffyYnfio ffRjloC~0RyY~>fijljlyY~fi|3{w>{|b~fi~>j~>k ~
klIp HfiFaFfiB<ET> : LX F^LXZ@L(< E@Qv E : FYI[FEHfiF EZIfiL(< lH<@Q 8 EL(FYQfQ<T FEHaFPf"\ 8 00wfiw
0 CYdb0fi0uuug npoR/
yY~>oRC0} ^ yYoCnj~>R/0 `^}] Z\`o~>oCnR0  qC0b R0u!npyYfim>|b~>|F|FRjlyY~oR{loRRjlyY~roR@fi|0~>jl
yYnw>{}|0~fi~>j}~>k	 3
~ klIp HfiFfiFaB<ET> : LX FCGLXM@L(< E@Q E : FYIFYEHfiF EMIaL(< lHY<f@Q 8 EL(FQQS<T FYEHfiF
f"\ 8 0Ywfiw	MqR# dfiq/fi0uuufi npoRC
vnpY|0~bC$  qC C02 R
 `n|bwfibfi|3oR-|3{lk0y>npjlfiffyYnMyYy0{loC|0~gmfi~>RjlyY~-^|0~>j}wfim>{|FjlyY~ 8
I@E>G@HL(< E> E D*JcUVL(FI>@v b	00Y fi0fiq0
CC

fi
	fifffffi vfi	  
  

vmfinpR  }#v{}|0n3o0>}Z
jlyY~npoR{}|3jy>~>C	~
yYnbyb{{}|0~>$

^

fi}

y>~>k	Er  Cq bfiq%RA>fifiy0{ljly>fioR{@>oRR0j}~>kzjlOwfi|0npjljlyY~>oRWn|0~>j

8 EL(FIfiE@L(<E@QE : FI[FEHaFvE}FIfi"	v@IT F-Ha@QSF 8 E L(FT I[@L(<E	/wfiw3YYdsCM

#vj}r|3jff$u}N`jmfi~>R>jlk0{lj}|fi}N`j}mfi~>@>jlk0{lj}|fii}7Z

n|  oCnpy	7   qCb0R${}|0~fi~>j}~>k  j}|y>fioR{
@>oRR0j}~>k	t(u[fioRRjljlyY~rwfinpy>RoR	mfinpoy>nN$O<Y ~Mk-IpHfiFaFfiB<ET> : LX FlLX  UVIpfiJFfi@ExE : FI[FEHaF
 EMk-Q@EE<fETZ  k  0 ^ oRRmfinpo y0oRj}~unpj	Rj}|3{~boR{l{jlk0oC~>Ro0wfiwfiqC0Yq8Yfi%Afiwfinpj}~>k0oCn
 oCnp{}|3k	

#vj}r|3jffuy

 B E
Efi|FoR[k0oC~>oCn|3jy>~:y3
 oCnpjff D }B Zn|  oCny	s   qC0 C02| ROum>y>r|3jl
mfi~>j  oCnp|3{w>{}|0~>Wj}~~>yY~bfioRoCnj}~>jljl:fiy>r|3j}~>C~klIpHfiFfiFaB<ET> : LX F#LX1@L(< E@Q
 E : FI[FYEHfiFExIaL(< lHY<f@Q 8 EL(FQQ<TVFEHaFM\" 8 fiF>wfiwC00s_0CCfiq0buuu[npoRC

#vj}r|3jffvu}y

 A>npyY~>k[w>{}|0~fi~>j}~>kcj}~!~>yY~bfioRoCnj}~>jljl
 oCnpjff D }fi Zxn|  oCnpy	[   qC02 C0 RI
fiyYr|3j}~>  }j |Wy>fioR{R>oR@bj}~>k	 ~klIpHfiFfiFaB<ET> : LX FMLX 8 EL(FIfiE@L(< E@QE : FI[FYEHfiFE
IaL(< lHY<f@Q 8 EL(FQQ<TVFEHaF"k-QS@EE<ETCY>`L(FYDef 8 k  #fi0Ywfiw	0_ YYfi>uuuB npoRC

#v{}|0nFo0	}`nmfifioCnpk	}UZ

oR{loR$Er 

q/00RYBFQX FaH8_<ET>

D

\npoRC

#v{}|0nFo0 D

}oCnpyY~uB Z A>jl{}|fivu   qC2 C0 R<um>yYr|Fjl  oCnj	C|FjlyY~[y3~>jo
|3oRyY~>CmfinnpoC~bYoCm>j}~>k<oCrwfiy>n|3{{ybk0jlwMoRRj	C|3jlyY~>CxCI@E>G@HL(<E>E
klIp TVI@DDC<ETw	@ET UV@GTVFG>@EB3>GL(FDP>R    R30# db0fi
}7Z|3o0u  /q 0fiq%R 
 w>{|b~t>oyYwMoC~:w>{}|0~fi~>j}~>k-|0npR>jloRRmfinpo0MtIaL(< lHY<f@Q
FEHaF/c #b	Y_ 0C0fi

#mfinnjo0 Y

T

8 EL(FQQS<;

EoC|0~} Y

|3oR{}>{lj}~>k	 ^   Y j}nr|0~  } Z jl@>yb{yY~u  q/00sRa{}|0~fi~>j}~>kmfi~>fioCnj}o
RyY~>n|Fj~bvj}~WyYRfi|3jlfiyY^|3j}~>CltIaL(< lHY<f@Q 8 EL(FQQ<TVFEHfiFCvY3	0sYfi3	

Ej D

|0~>Ry D }`j}mfi~>@>jlk0{lj}|fi}ZmffW~>y	A  qC0C8({|b~fi~>j~>k  j}|y>fioR{R>oRR0j}~>kj}~OfioRoCn 
j}~>jjlfiyY^|3j}~>CtsnpoR{lj}j}~fi|0npWnpoCwfiy>npC~3k-IYHaFfiFfiB<fET>v : LX FPLX 8 EL(FIfiE@L(<E@QE : FI
FEHfiF EiIfiL(< lH<@Q 8 EL(FQQ<TVFEHaF#-&FLXYY B0 Ql TVW->GL(FDP>$@EBM-JJcQ<fHfi@L(<E>Z 8 i  #fi0
wfiw	0fifiq fi00fi Afiwfinpj}~>k0oCn   oCnp{}|3k	
  Cq C08A>jlmfi|FoRcRyY~bnpyb{nm>{loRC!~=klIpHfiFfiFaB<ET>r : LX F  >GL 8 EL(FIfiE@L(<E@Q
E : FI[FYEHfiF Ek-Ia<EHY<KJcQF^> :C E	glQFfiBGT FwlFJcI[FG>GFEL(@L(< E@EBlFfi@#>RE<ET     0$wfiw
qC0YqbqCfi D yYnkY|0~ Y |0mb^|0~fi~

EnmfiryY~>$ D

EnmfiryY~>$ D

  ZvnpoRj}~fi|fi    qC008 u~0>j}o:d~b>oRjl-wfinpyboRRjy>~t D |3Yj}jlRj}~>k!>o
wfinpyYfi|0>jl{ljl[yFk0yY|3{v|3j|3Rjy>~c~=klIpYHaFfiFaB<fET>r : LXVFLX!E : FI[FYEHfiFWE1IaL(< lHY<f@Q
8 EL(FYQfQ<T FEHaF/0wfiw$qC C_qR0	buuu[ npoRC

Rjy>~>j }|0~fi0/ A}hoR{$Er}fiEn|0wMoCnRgEr} ^ oR} Z!jl{l{lj|byY~ D   qCb0R[u~
|0wfiwfinpyY|3RyYnw>{|b~fi~>j~>kWzjWj}~>RyYrw>{loRoj~bffyYn^|3jlyY~ ~&klIpYHaFfiFaB<fET> : LXVFM  IB 8 EL(FI
E@L(< E@Q
 E : FI[FEHaF EdklIfi<fEH<bJcQFG> : E	 glQFfiBGT FP-F[JcI[FG>GFEL(@L(< Ei@EBM-Fa@>@E<fETY
ij}3oRC}BZ j{lyY~
    qC0fiqR AfigAtvux~>oRz|0wfiwfinpyY|3R[y->o<|0wfiw>{ljlC|3jlyY~!y3
>oRyYnpoCwfiny  j}~>kryrwfinpyY>{loCy0{  j~>klIfiL(< lH<@Q 8 EL(FQQS<T FYEHfiFC0(q%C0YdbCfi
CC

fi00	0Q03	
`|FC	 

qCb0 R ~0oRk>n|3j}~>kWw>{|b~fi~>j~>k<|0~>:npoC|3Rj}~>kOj}~:|>oRoCnpy0k0oC~>oRyYm>|3fi~>RfinpyY~>yYm>|0n@>j
oRRmfinpoyYnRyY~bnpy0{l{lj}~>knpoC|F{zvyYn{:y>>j{lonyYfiy0C~}klIpHfiFfiFaB<ET> : LXVF#LXi@L(< E@Q
 E : FI[FYEHfiF ExIaL(< lHY<f@Q 8 EL(FQQ<TVFEHaFM\" 8 fiF>wfiw C00_ 0CfiqCsfibuuu[npoRC
`oR{ffyY~>$ D }(
 Z ^ j@>jl0    qCb0 RoCwfinpoRoC~bj}~>kg|FRjlyY~:|0~>:Rfi|0~>k0orb<{ybk0jlwfinpybkYn|0/
X FPd U IfiE@Q( : 	$ T <Hk-Ip T I[@DCDC<ETY R	0Mfiq d00M
`oRy>npk0o D d
 } Z ^ |0~>b0	u ^   qC C0 8voC|3Rj  onpoC|3y>~>j~>kW|0~><w>{|b~fi~>j~>k &
~ k-IY HaFfiFfiB<fET>
 : LX F1#LX@L(< E@Q$ E : FI[FEHaF< EIaL(< lHY<f@Q 8 EL(FQQ<TVFEHfiF"\ 8 0wfiw0bY d C0M
D y>npkY|0~ Y |0mbr|0~fi~
`j}~>MoCnpk	 D  ^   qC C02
 Rc ~>j  oCnp|F{&w>{}|0~fi~>j}~>k	tu~  |F{y0@ mfi~>j  oCnp|3{$fi|3OjlfioC|fi  8 @^T @#G<fEFC
#   R>YY Yb	
`j}mfi~>R>jlk0{lj|M} Y |0npfi|fi
 `r } Z ^ j@>jl0   qC0b R$voCwfinpoRoC~0j}~>k|FRjlyY~t~>fioRoCnj}~fi|3R
|0~>n|0j	C|3jy>~>C IfiL(< -H<@Q 8 EL(FQQ<TVFEHfiFC 3fiY0_ YY Cfi
`j}mfi~>R>jlk0{lj|M}7
 Z ^ jR>jl0    qC02 CRu~:|FRjlyY~<{}|0~>kYmfi|3kbofi|3oR:y>~<C|0m>|3{oRfiw>{}|0~fi|3jlyY~t
npoR{lj}j}~fi|0np<npoCwfiyYnC 
~ k-Ip HfiFaFfiB<ET> : LX F#LXi@L(< E@Q E : FYI[FEHfiF EiIfiL(< -H<@Q 8 E
L(FQQS<T FYEHfiFMf"" 8 #fi0fiwfiw	00_ d00fibuuu[ npoRC
|3jlkY Y  } Z  oR{ly0y D  D   q/0 CRd {}|0~fi~>j}~>k	MoRYoRCm>jlyY~`|0~>O{oC|bn~>j}~>krj}~W|nyYfiy0j|3k0oC~bC
d
~ klIpY HaFfiFaB<fET> : LX FLX 8 EL(FIfiE@L(<  E@Qv E : FI[FEHaF EZIfiL(< -H<@Q 8 EL(FYQfQ<T FYEHfiFAk-QS@EE<ET
cY>`L(FYDP> 8 k  #fi0Ywfiw(qC0Y q/0fi0uuufi noRC
|3{}mfiWd ( Z `o~>oCnR  000 8u	jlj}>{lo>oCmfinpjljlRyYnyYw>j}r|3{w>{}|0~fi~>j}~>k	&
~ k-Ip HfiFaFfiB#
<fET> : LXVF#LX 8 EL(FYIaE@L(< E@Q* E : FYIFYEHfiFW EtIaL(< lHY<f@Q 8 EL(FYQfQ<T FYEHfiFMk-QS@EE<ET&Y>`L(FYD
f 8 k  fiF>wfiw$qRY_ qRYfibuuu[ npoRC
 oC~>oC~> D   qCb0 R] g
 E
Efi|3oRrmfi~>j  oCnp|3{fiw>{}|0~fi~>j}~>kj}~m>{j|Fk0oC~0/0~>yY~bfioRoCnj}~>jljlfiy3
r|3j}~>C D |3oCnR >oRjlCfi$oRRfi~>jlC|3c{ ~>j  oCnpjlpy3( EoC~fi^|0n] EoCwfi|0npoC~by3um>y>r|3jlyY~
ff\
u 03i0M
 oC~>oC~fi D }  oR{ly0y	 D } Z!nY|0~bC00  0b0 R>u>n|3Rjy>~oR@fi~>j !fim>oRffyYN
n g E
Efi|3oR
w>{|b~fi~>j~>kiyYnp>RyYj~>k
Y |0fi|0~>C|M3i}*v|0nfioC|0m D }] ZIA>p EoC~>jlC0  qCb0 R {|b~fi~>j~>kRyY~bnpy0{Ynm>{oRffyYnnpoC|3Rj  ov|3k0oC~bC
IaL(< lHY<f@Q 8 EL(FQQ<TVFEHaF/ 3	0_ q0qCfi
Y |0m>0} Z A>oR{}r|0~  qC0b R m>>j}~>kc>o:oC~  oR{lyYwMo0t {|b~fi~>j~>kwfinpyYwMy0jljlyY~fi|3{{ybk0jl
|0~>:y>Rfi|3jlOoC|0npRW
~ klIp HfiFaFfiB<ET> : LXVF#LX}@L(< E@QA E : FI[FEHaFr EIaL(< lHY<f@Q

8 EL(FYQfQ<T FEHaFC\" 8 aF y0{ff	fiwfiw$q0qC3 qC0fiqb3uuufi npoRC
Y |0m>00}] Z A>oR{}r|0~>  q/00 R ~>j>j}~>k A>ufi|FoRr|0~>kYn|0wfibfi|3oRrw>{}|0~fi~>j}~>k		 M
~ k-Ip HfiFaFfiB#
<fET> : LXVF#LX 8 EL(FYIaE@L(< E@Qd <fELA E : FYI[FEHfiF E3IaL(< lHY<f@Q 8 EL(FQQ<TVFEHaFC 8 c
 8 0
 y0{ff$q0	wfiw	M%q CYd0bsfi D yYnpk>|0~ Y |bmbr|b~fi~
Y y>oC>{oCn8  }
 oCMoR{Yy3r|0~fi~  } ZIEj}yYwfiy>m>{ybC   qCb0 R	YoC~>fij~>krw>{}|0~fi~>j}~>krkYn|0wfi>
y|0~
u E ^ mfi>oR/x 
~ k-IY HaFfiFfiB<fET>< : LX FLX  UVI` JFfi@E E : FYIFYEHfiF: Ek-QS@EE<ET
  k  0Ywfiw	b0Y d Cbsfi Afiwfinpj}~>k0oCn  oCnp{}|3k	
C"

fi
	fifffffi vfi	  
  

y>oC~>jk A[Z A>j}ryY~>/[`^  q/00sRvoC|3{j}o:oC|0npR[j}~~>yY~bfioRoCnj}~>jjl-fiyY^|3j}~>C
~klIpYHaFfiFaB<fET> : LX FGLX 8 EL(FIfiE@L(< E@Qfi<ELwE : FI[FYEHfiFE1IfiL(< lH<@Q 8 EL(FQQS<T FYEHfiF
 8 
 8 F>wfiw$qC0bYqC0bfi D yYnpkY|0~ Y |0mbr|0~fi~

Y

o  Co n8  } 7Zvjl@fi|0nfiC  qC038Ckl@I[Hw>{|b~t|w>{}|0~fi~>j}~>k:|0npR>jloRRmfinporzj:wfi|0n|3{l{loR{|FRjlyY~>
|0~>-Ry>~>n|3j}~bC ~1	vFfiHL(UVI[FML(FG>x<EIaL(< lHY<f@Q 8 EL(FYQfQ<T FEHaF/$wfiwfiqCYdb0fiA D AF	
Afiwfinpj}~>k0oCn   oCnp{}|3k	
^

j}~>>joR{loC~(    qCb0RQmE
Ehuj}~fi|0nEoRRjjy>~ Ej}|3k>n|0 |3RC|3k0o0[(oR@vnpoCw 
t	q/00/02Cfi0~>jm>oyF~bffyYnr|FjlyY~r(oR@fi~>y0{ly0kb0	$oRRfi~>jlC|3{v~>j  oCnjlOy37EoC~fir|0n	
VA V fi-z[Y  
^

^

j}~>kY|0n$fiu}Zvjl@fi|bnpfiCfi  Cq 02CR7{}|0~fi~>j}~>kWwfi|0n|3{l{loR{|3RjlyY~>CIfiL(< -H<@Q
3	0Mfiq d03

8 EL(FYQfQ<T FYEHfiFC

qC008>oqC0C^uw>{}|0~fi~>j}~>k<>oCRyYrwMoRjljlyY~MIfiL(< -H<@Q
mfifijloR 8

8 EL(FQQS<T FYEHfiF

yY~>kEr}Z 	i y3 D   qCbCRdEyY^|3j}~Oj~>fioCwMoC~>fioC~0w>{}|0~fi~>oCnRyYrw>jl{}|3jlyY~~i 8 k  #WI
>aXY` J  Eg-QSFfiBGTVF  ETV<fEFaFYIa<ET@EB"HfiUV<?>G<fL(<E : Idk-QS@EE<ET&AIfi<fB^T <ET!X FIfi@EB
klI[@HL(<HfiFC>uuuoR@fi~>jlC|3{oCwfiyYnp\AbC/0M
^

D " EoCny0CEr 

@GTV@#`<EF/

D

fi}



 D j {l{}|0~ Y  ^   Cq 008#DQ<HYBFQX FfiHR_<ET> Y }{ mbzoCnuC|3fioCjl  mfi>{

qC0bR # B
tRu[yYmfi~>$/RyYrw>{loRo00wfi|bnpj}|3{3yYnpfioCn$w>{}|0~fi~>oCn(ffyYn
klI YHaFfiFaB<fET> : LX F  I[B 8 EL(FIfiE@L(<E@QE : FYIFYEHfiFE3klIfi<fEH<bJcQFG> :A Eg-QSFfiBGTVF
lF[JIF^>`FYEL(@L(<Ed@EBwlFfi@#>RE<ET>YwfiwqC0YqbqR	 D yYnpkY|b~ Y |0mb^|0~fi~
oRy0C D }dZ Afij7Er  qC002R
#vyY~>fijljlyY~fi|3{~>yY~>{lj}~>oC|0nw>{|b~fi~>j~>kr~k-I YHaFfiFfiB<fET
>  : LX Fx  >GL
8 EL(FYIaE@L(< E@Q E : FI[FEHaF EdIaL(< lHY<f@Q 8 EL(FQQ<TVFEHfiFk-Q@EE<fETZY>`L(FYD$>xf 8 k  #fi0 	wfiw
q%C0Y q/0fi D yYnkY|0~ Y |0mb^|0~fi~
|0~/|0~$ Y 	uRjl0$u}vn|/>yY~ Y }7{loRjloCnR}(Z+j>{loR0N#  qCb0sR
ff RjloC~0gE
E
|3{lk0yYnpjlfi^ffyYnOiBA D fi~0>oRjl|0~>  oCnpj	C|3jy>~ ~ 8 C7klpI HfiFfiFaB<ET
>  : LXVF
8 EL(FYIaE@L(< E@Q-W I >fiYX fi-J Ed$	 TV<fHtc#ELXVFG>G<?R> 
A>R>yYwfiwfioCnp/ D     q/C0R
 ~>j  oCnp|3{w>{}|0~>ffyYnnpoC|FRj  onpyYMy0j}~Wmfi~fiwfinpoRfijlR|0>{looC~  j}npy>~fioC~0C
~klpI YHaFfiFaB<fET
>  : LX F#LX 8 EL(FIfiE@L(< E@Qfi <ELw E : FI[FYEHfiF E1IfiL(< lH<@Q 8 EL(FQQS<T FYEHfiF
 8 
 8 F >wfiw$qC0bY qC3>fi D yYnpkY|0~ Y |0mbr|0~fi~
Afij}n~>y  
 } Y yYoC~>jlk	7A}  oR{ly0y	 D }UZ A>j}ryY~>C$  qC0bRvff RjloC~bk0yY|3{fij}npoRRoR<oRfiw>{yF
n|3jlyY~ ~&k-I YHaFfiFfiB<fET>AWLXVFiX <IaL(FfiFELXM@L(< E@Q E : FYIFYEHfi
F ExIfiL(< -H<@Q 8 EL(FQQS<T FYEHfiF
f"\ 8 0 Ywfiw	b0Yd 0bfi0uuufinpoRC
oC~>fioCnpb0  %A}%Z6:oR{l$]Er"A 
uE ^ ~ p


A>yY~>o0U0Z

 oR{ly0y	 D  D   qCbCR$y3z|0npfivRy0{l{}|0fiy>n|3j  o|0~>|3  oCnp|0npj}|3{${loC|0n~>j}~>k	tuC|3o
m>firj}~WnpyYMy0jlyYRRoCn8 8 EL(FIfiE@L(<E@QfiU IfiE@Q : UVDC@EfiD*JcUVL(FI"cL(UVB<FG>M 8   0

Afim>y>~dA}Z|bnpy	bu`r 

qC0C8v-F<E :  I[HaFYDFYELQFa@IfiE<ET-@Ed<fEL(IYBUVHL(<E	
C%?

D

InpoRC

fi00	0Q03	
oR{ly0y	 D }#|0nMyY~>oR{l{  }CoC npoR0buYyYnn|Fy]Er}0ij}~fid}Z{l>>o0    qC00s2R	~boRkYn|3j}~>k
w>{|b~fi~>j~>k[|0~>[{loC|0n~>j}~>k	t`>ofi E`t |bnp@>jloRRmfinpo01fiUVIaE@Q :3 ]JFIfi<DFYEL(@Q-@EB
X F I[FL(<Ha@QIfiL(< lH<@Q 8 EL(FQQS<T FYEHfiFCv  q%RUCfiqfiqCbfi
 oR{ly0y	 D  D   q/03
 R klQ@EE<ETZ@EB3QSFfi@IfiE<ETxGM@E@Ql TV<fHfi@QI[Fa@#>R E<fETYN Afiwfinj~>kboCn   oCnp{}|3k	
 oR{ly0y	 D  D }
 yb{{}|3R D }

 Z #y* D   qC0 C8 |3jlyY~fi|F{ofi|FoRyY~>jlyYnpj}~>k[ffyYn
w>{|b~fi~>j~>kj}~fid~fi|bjloC~  jnyY~fioC~bC	 M
~ k-Ip HfiFaFfiB<ET> : LXVFALX 8 EL(FIfiE@L(< E@Q E : FI[FEHaF
 ExIaL(< lHY<f@Q 8 EL(FQQ<TVFEHaF"k-QS@EE<ETCY>`L(FYD$>M 8 k  #fi0fiwfiw$qCfifiq qC0fibuuug npoRC
:oR{l$ Er  qC002 RoRRoC~0|F  |0~>RoRj}~Ouw>{}|0~fi~>j}~>k	 IfiL(< -H<@Q 8 EL(FQQ<TVFEHaF*@GTV@#G<fEFCYbY qC0M
!jl{bj}~>C Er3 D 0oCnp/ Y  ^  ^ y3zn|0~>Ro0   Er}n Z:oR{oR0 ^ ]   qCb3 R {|b~fi~>j~>k|b~>noC|3Rj}~>k
j~mfi~>RoCnp|3j}~!|0~>fid~fi|0j`oC~  j}npyY~fioC~bC
 fi UVIaE@Q :d ]JFIfi<DFYEL(@Q*@EBX F I[FL(<Ha@Q
IaL(< lHY<f@Q 8 EL(FQQ<TVFEHaF/ 3$qC0_ d00fi


C%

fiJournal of Artificial Intelligence Research 13 (2000) 305-338

Submitted 6/00; published 12/00

Conformant Planning via Symbolic Model Checking
cimatti@irst.itc.it

Alessandro Cimatti

ITC-irst

, Via Sommarive 18, 38055 Povo, Trento, Italy

roveri@irst.itc.it

Marco Roveri

ITC-irst

, Via Sommarive 18, 38055 Povo, Trento, Italy

DSI, University of Milano, Via Comelico 39, 20135 Milano, Italy

Abstract

We tackle the problem of planning in nondeterministic domains, by presenting a new
approach to conformant planning. Conformant planning is the problem of finding a sequence of actions that is guaranteed to achieve the goal despite the nondeterminism of the
domain. Our approach is based on the representation of the planning domain as a finite
state automaton. We use Symbolic Model Checking techniques, in particular Binary Decision Diagrams, to compactly represent and eciently search the automaton. In this paper
we make the following contributions. First, we present a general planning algorithm for
conformant planning, which applies to fully nondeterministic domains, with uncertainty in
the initial condition and in action effects. The algorithm is based on a breadth-first, backward search, and returns conformant plans of minimal length, if a solution to the planning
problem exists, otherwise it terminates concluding that the problem admits no conformant
solution. Second, we provide a symbolic representation of the search space based on Binary
Decision Diagrams (Bdds), which is the basis for search techniques derived from symbolic
model checking. The symbolic representation makes it possible to analyze potentially large
sets of states and transitions in a single computation step, thus providing for an ecient
implementation. Third, we present Cmbp (Conformant Model Based Planner), an ecient
implementation of the data structures and algorithm described above, directly based on
Bdd manipulations, which allows for a compact representation of the search layers and an
ecient implementation of the search steps. Finally, we present an experimental comparison of our approach with the state-of-the-art conformant planners Cgp, Qbfplan and
Gpt. Our analysis includes all the planning problems from the distribution packages of
these systems, plus other problems defined to stress a number of specific factors. Our approach appears to be the most effective: Cmbp is strictly more expressive than Qbfplan
and Cgp and, in all the problems where a comparison is possible, Cmbp outperforms its
competitors, sometimes by orders of magnitude.
1. Introduction

In recent years, there has been a growing interest in planning in nondeterministic domains.
Rejecting some fundamental (and often unrealistic) assumptions of classical planning, domains are considered where actions can have uncertain effects, exogenous events are possible,
and the initial state can be only partly specified. The challenge is to find a strong plan,
that is guaranteed to achieve the goal despite the nondeterminism of the domain, regardless
of the uncertainty on the initial condition and on the effect of actions. Conditional planning (Cassandra, Kaelbling, & Littman, 1994; Weld, Anderson, & Smith, 1998; Cimatti,
Roveri, & Traverso, 1998b) tackles this problem by searching for a conditional course of
c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiCimatti & Roveri
actions, that depends on information that can be gathered at run-time. In certain domains,
however, run-time information gathering may be too expensive or simply impossible. Conformant planning (Goldman & Boddy, 1996) is the problem of finding an unconditioned
course of actions, i.e. a classical plan, that does not depend on run-time information gathering to guarantee the achievement of the goal. Conformant planning has been recognized
as a significant problem in Artificial Intelligence since the work by Michie (1974): the Blind
Robot problem requires to program the activity for a sensorless agent, which can be positioned in any location of a given room, so that it will be guaranteed to achieve a given
goal. Conformant planning can be also seen as a problem of control for a system with
an unobservable and unknown state, such as a microprocessor at power-up, or a software
system under black-box testing.
Because of uncertainty, a plan is associated to potentially many different executions,
which must be all taken into account in order to guarantee goal achievement. This makes
conformant planning significantly harder than classical planning (Rintanen, 1999a; De Giacomo & Vardi, 1999). Despite this increased complexity, several approaches to conformant
planning have been recently proposed, based on (extensions of) the main planning techniques for classical planning. The most interesting are Cgp (Smith & Weld, 1998) based
on Graphplan, Qbfplan (Rintanen, 1999a) which extends the SAT-plan approach to
QBF, and Gpt (Bonet & Geffner, 2000) which encodes conformant planning as heuristic
search. In this paper, we propose a new approach to conformant planning, based on Symbolic Model Checking (McMillan, 1993). Symbolic Model Checking is a formal verification
technique, which allows one to analyze finite state automata of high complexity, relying on
symbolic techniques, Binary Decision Diagrams (Bdds) (Bryant, 1986) in particular, for
the compact representation and ecient search of the automaton. Our approach builds on
the planning via model checking paradigm presented by Cimatti and his colleagues (1997,
1998b, 1998a), where finite state automata are used to represent complex, nondeterministic
planning domains, and planning is based on (extensions of) the basic model checking steps.
We make the following contributions.

 First, we present a general algorithm for conformant planning, which applies to any

nondeterministic domain with uncertain action effects and initial condition, expressed
as a nondeterministic finite-state automaton. The algorithm performs a breadth-first
search, exploring plans of increasing length, until a plan is found or no more candidate
plans are available. The algorithm is complete, i.e. it returns with failure if and only
if the problem admits no conformant solution. If the problem admits a solution, the
algorithm returns a conformant plan of minimal length.

 Second, we provide a symbolic representation of the search space based on Binary

Decision Diagrams, which allows for the application of search techniques derived from
symbolic model checking. The symbolic representation makes it possible to analyze
sets of transitions in a single computation step. These sets can be compactly represented and eciently manipulated despite their potentially large cardinality. This
way it is possible to overcome the enumerative nature of the other approaches to
conformant planning, for which the degree of nondeterminism tends to be a limiting
factor.
306

fiConformant Planning via Symbolic Model Checking

 Third, we developed Cmbp (Conformant Model Based Planner), which is an ecient

implementation of the data structures and algorithm described above. Cmbp is developed on top of Mbp, the planner based on symbolic model checking techniques
developed by Cimatti, Roveri and Traveso (1998b, 1998a). Cmbp implements several
new techniques, directly based on Bdd manipulations, to compact the search layers
and optimize termination checking.
 Finally, we provide an experimental evaluation of the state-of-the-art conformant planners, comparing Cmbp with Cgp, Qbfplan and Gpt. Because of the difference in
expressivity, not all the problems which can be tackled by Cmbp can also be represented in the other planners. However, for the problems where a direct comparison
was possible, Cmbp outperforms its competitors. In particular, it features a better
qualitative behavior, not directly related to the number of initial states and uncertain
action effects, and more stable with respect to the use of heuristics.
The paper is structured as follows. In Section 2 we review the representation of (nondeterministic) planning domains as finite state automata. In Section 3 we provide the
intuitions and a formal definition of conformant planning in this setting. In Section 4 we
present the planning algorithm, and in Section 5 we discuss the symbolic representation
of the search space, which allows for an ecient implementation. In Section 6 we present
the Cmbp planner, and in Section 7 we present the experimental results. In Section 8 we
discuss some further related work. In Section 9 we draw the conclusions and discuss future
research directions.
2. Planning Domains as Finite State Automata

We are interested in complex, nondeterministic planning domains, where actions can have
preconditions, conditional effects, and uncertain effects, and the initial state can be only
partly specified. In the rest of this paper, we use a very simple though paradigmatic domain
for explanatory purposes, a variation of Moore's bomb in the toilet domain (McDermott,
1987) (from now on called BTUC | BT with Uncertain Clogging). There are two packages,
and one of them contains an armed bomb. It is possible to dunk either package in the
toilet (actions Dunk1 and Dunk2 ), provided that the toilet is not clogged. Dunking either
package has the uncertain effect of clogging the toilet. Furthermore, dunking the package
containing the bomb has the effect of disarming the bomb. The action F lush has the effect
of unclogging the toilet.
We represent such domains as finite state automata. Figure 1 depicts the automaton for
the BTUC domain. Each state is given a number, and contains all the propositions holding
in that state. For instance, state 1 represents the state where the bomb is in package 1, is
not defused, and the toilet is not clogged. Given that there is only one bomb, we write In2
as an abbreviation for the negation of In1 . Arrows between states depict the transitions of
the automaton, representing the possible behavior of actions. The transition from state 2
to state 1 labeled by F lush represents the fact that the action F lush, if executed in state
2, only has the effect of removing the clogging. The execution of Dunk1 in state 1, which
has the uncertain effect of clogging the toilet, is represented by the multiple transitions to
states 5 and 6. Since there is no transition outgoing from state 2 and labelled by Dunk1 ,
307

fiCimatti & Roveri
Flush
Flush

Dunk_1
In_1 5
Defused
!Clogged

In_1 1
!Defused
!Clogged

Dunk_1,
Dunk_2

Dunk_2
Flush

Flush
In_1 2
!Defused
Clogged

In_1 6
Defused
Clogged

Flush
Flush

Dunk_2
In_2 7
Defused
!Clogged

3

In_2
!Defused
!Clogged

Dunk_1,
Dunk_2

Dunk_1
Flush

Flush
In_2 8
Defused
Clogged

In_2 4
!Defused
Clogged

Figure 1: The automaton for the BTUC domain
state 2 does not satisfy the preconditions of action Dunk1 , i.e. Dunk1 is not applicable in
state 2.
We formally define nondeterministic planning domains as follows.
Definition 1 (Planning Domain) A Planning Domain is a 4-tuple D = (P ; S ; A; R),

where P is the (finite) set of atomic propositions, S  2P is the set of states,
(finite) set of actions, and R  S  A  S is the transition relation.

A is the

Intuitively, a proposition is in a state if and only if it holds in that state. In the following
we assume that a planning domain D is given. We use s, s0 and s00 to denote states of D,
and ff to denote actions. R(s; ff; s0 ) holds iff when executing the action ff in the state s the
state s0 is a possible outcome. We say that an action ff is applicable in s iff there is at least
one state s0 such that R(s; ff; s0 ) holds. We say that an action ff is deterministic in s iff
there is a unique state s0 such that R(s; ff; s0 ) holds. An action ff has an uncertain outcome
in s if there are at least two distinct states s0 and s00 such that R(s; ff; s0 ) and R(s; ff; s00 )
hold. As described by Cimatti and his colleagues (1997), the automaton for a given domain
can be eciently built starting from a compact description given in an expressive high level
action language, for instance AR (Giunchiglia, Kartha, & Lifschitz, 1997).
3. Conformant Planning

Conformant planning (Goldman & Boddy, 1996) can be described as the problem of finding
a sequence of actions that is guaranteed to achieve the goal regardless of the nondeterminism
of the domain. That is, for all possible initial states, and for all uncertain action effects,
the execution of the plan results in a goal state.
Consider the following problem for the BTUC domain. Initially, the bomb is armed but
its position and the status of the toilet are uncertain, i.e. the initial state can be any of the
states in f1; 2; 3; 4g . The goal is to reach a state where the bomb is defused, and the toilet
308

fiConformant Planning via Symbolic Model Checking

In_1 1
!Defused
!Clogged

In_1 2
!Defused
Clogged

In_2 3
!Defused
!Clogged

In_2 4
!Defused
Clogged

In_1 5
Defused
!Clogged

Flush

Flush

Flush

Flush

In_1 5
Defused
!Clogged

Flush

Flush
In_1 1
!Defused
!Clogged

Dunk_1

In_1 6
Defused
Clogged

In_1 5
Defused
!Clogged

Dunk_2

In_1 6
Defused
Clogged

In_2 3
!Defused
!Clogged

Dunk_2

In_2 7
Defused
!Clogged

Flush
In_2 3
!Defused
!Clogged

Dunk_1

In_2 3
!Defused
!Clogged

In_2 4
!Defused
Clogged

Flush

In_2 8
Defused
Clogged

Flush

Flush

Flush

In_1 5
Defused
!Clogged

In_2 7
Defused
!Clogged

Flush

Figure 2: A conformant solution for the BTUC problem
is not clogged, i.e. the set of goal states is f5; 7g. A conformant plan solving this problem
is
F lush; Dunk1 ; F lush ; Dunk2 ; F lush
(1)
Figure 2 outlines the possible executions of the plan, for all possible initial states and
uncertain action effects. The initial uncertainty lies in the fact that the domain might be
in any of the states in f1; 2; 3; 4g . The possible initial states of the planning domain are
collected into a set by a dashed line. We call such a set a belief state. Intuitively, a belief
state expresses a condition of uncertainty about the domain, by collecting together all the
states which are indistinguishable from the point of view of an agent reasoning about the
domain. The first action, F lush, is used to remove the possible clogging. This reduces the
uncertainty to the belief state f1; 3g. Despite the remaining uncertainty (i.e. it is still not
known in which package the bomb is), action Dunk1 is now guaranteed to be applicable
because its precondition is met in both states. Dunk1 has the effect of defusing the bomb if
it is contained in package 1, and has the uncertain effect of clogging the toilet. The resulting
belief state is f3; 4; 5; 6g . The following action, F lush, removes the clogging, reducing the
uncertainty to the belief state f3; 5g, and guarantees the applicability of Dunk2 . After
Dunk2 , the bomb is guaranteed to be defused, but the toilet might be clogged again (states
6 and 8 in the belief state f5; 6; 7; 8g ). The final F lush reduces the uncertainty to the belief
state f5; 7g, and guarantees the achievement of the goal.
In general, in order for a plan to be a conformant solution, no action must be executed
in states which do not satisfy the preconditions, and any state that can result from the
execution of the plan (for all the initial states and for all the uncertain action effects) is
a goal state. The main diculty in achieving these conditions is that no information is
(assumed to be) available at run-time. Therefore, at planning time we face the problem of
reasoning about action execution in a belief state, i.e. under a condition of uncertainty.
Definition 2 (Action Applicability) Let Bs  S be a Belief State. The action ff is
applicable in Bs iff Bs 6= ; and ff is applicable in every state s 2 Bs.
309

fiCimatti & Roveri
In order for an action to be applicable in a belief state, we require that its preconditions
must be guaranteed notwithstanding the uncertainty. In other words, we reject \reckless"
plans, which take the chance of applying an action without the guarantee of its applicability.
This choice is strongly motivated in practical domains, where possibly fatal consequences
can follow from the attempt to apply an action when its preconditions might not be satisfied
(e.g. starting to fix an electrical device without being sure that it is not powered). The effect
of action execution from an uncertain condition is defined as follows.
Definition 3 (Action Image) Let Bs  S be a belief state, and let ff be an action applicable in Bs. The image (also called execution) of ff in Bs, written Image [ff](Bs), is defined

as follows.

Image [ff](Bs) =_

fs0 j there exists s 2 Bs such that R(s; ff; s0 )g

Notice that the image of an action combines the uncertainty in the belief state with the uncertainty on the action effects. (Consider for instance that Image [Dunk1 ](f1; 3g)=f3; 4; 5; 6g .)
In the following, we write Image [ff](s) instead of Image [ff](fsg).
Plans are elements of A , i.e. finite sequences of actions. We use  for the 0-length
plan,  and  to denote generic plans, and ;  for plan concatenation. The notions of
applicability and image generalize to plans as follows.
Definition 4 (Plan Applicability and Image) Let  2 A , and let Bs  S .  is applicable in Bs iff one of the following holds:
1.  =  and Bs 6= ;;
2.  = ff; , ff is applicable in Bs, and  is applicable in Image [ff](Bs).
The image (also called execution) of  in Bs, written Image [](Bs), is defined as:
1. Image [](Bs) =_ Bs;
2. Image [ff;  ](Bs) =_ Image [](Image [ff](Bs));

A planning problem is formally characterized by the set of initial and goal states. The
following definition captures the intuitive meaning of conformant plan given above.
Definition 5 (Conformant Planning) Let D = (P ; S ; A; R) be a planning domain. A
Planning Problem for D is a triple (D; I ; G ), where ; 6= I  S and ; 6= G  S .
The plan  is a conformant plan for (that is, a conformant solution to) the planning
problem (D; I ; G ) iff the following conditions hold:

(i)  is applicable in I ;
(ii) Image [](I )  G .
In the following, when clear from the context, we omit the domain from the planning
problem, and we simply write (I ; G ).
310

fiConformant Planning via Symbolic Model Checking
4. The Conformant Planning Algorithm

Our conformant planning algorithm is based on the exploration of the space of plans, limiting
the exploration to plans which are conformant by construction. The algorithm builds Belief
state-Plan (BsP) pairs of the form hBs : i, where Bs is a non-empty belief state and 
is a plan. The idea is to use a BsP pair to associate each explored plan with the maximal
belief state where it is applicable, and from which it is guaranteed to result in goal states.
The exploration is based on the basic function SPreImage [ff](Bs), that, given a belief state
Bs and an action ff, returns the belief state containing all the states where ff is applicable,
and whose image under ff is contained in Bs.
Definition 6 (Strong Pre-Image) Let ; 6= Bs  S be a belief state and let ff be an
action. The strong pre-image of Bs under ff, written SPreImage [ff](Bs), is defined as
follows.

SPreImage [ff](Bs) =_ fs j ff is applicable in s; and Image [ff](s)

 Bsg

If SPreImage [ff](Bs) is not empty, then ff is applicable in it, and it is a conformant solution to the problem (SPreImage [ff](Bs); Bs). Therefore, if the plan  is a conformant
solution for the problem (Bs; G ), then the plan ff;  is a conformant solution to the problem
(SPreImage [ff](Bs); G ).
Figure 3 depicts the space of BsP pairs built by the algorithm while solving the BTUC
problem. The levels are built from the goal, on the right, towards the initial states, on the
left. At level 0, the only BsP pair is hf5; 7g : i, composed by the set of goal states indexed
by the 0-length plan . (Notice that  is a conformant solution to every problem with goal set
f5; 7g and initial states contained in f5; 7g.) The dashed arrows represent the application
of SPreImage . At level 1, only the BsP pair hf5; 6; 7; 8g : F lushi is built, since the strong
pre-image of the belief state 0 for the actions Dunk1 and Dunk2 is empty. At level 2, there
are three BsP pairs, with (overlapping) belief states Bs2 , Bs3 and Bs4 , indexed, respectively,
by the length 2 plans Dunk1 ; F lush, F lush; F lush and Dunk2 ; F lush. (A plan associated
with a belief state Bsi is a sequence of actions labeling the path from Bsi to Bs0 .) Notice
that Bs3 is equal to Bs1 , and therefore deserves no further expansion. The expansion of
belief states 2 and 4 gives the belief states 5 and 6, both obtained by the strong pre-image
under F lush, while the strong pre-image under actions Dunk1 and Dunk2 returns empty
belief states. The further expansion of Bs5 results in three belief states. The one resulting
from the strong pre-image under F lush is not reported, since it is equal to Bs5 . Belief state
7 is also equal to Bs2 , and deserves no further expansion. Belief state 8 can be obtained by
expanding both Bs5 and Bs6 . At level 5, the expansion produces Bs10 , which contains all
the initial states. Therefore, both of the corresponding plans are conformant solutions to
the problem.
The conformant planning algorithm ConformantPlan is presented in Figure 4. It
takes as input the planning problem in the form of the set of states I and G (the domain
D is assumed to be globally available). The algorithm performs a backwards breadth-first
search, exploring BsP pairs corresponding to plans of increasing length at each step. The
status of the search (each level in Figure 3) is represented by a BsP table, i.e. a set of BsP
pairs
BsPT = fhBs1 : 1 i; : : : ; hBsn : n ig
311

fiCimatti & Roveri
Level

5

4

3

2

1

0

In_1 1
!Defused
!Clogged

In_1 1
!Defused
!Clogged
Dunk_1

In_1 1
!Defused
!Clogged

In_1 5
Defused
!Clogged

In_1 2
!Defused
Clogged

In_2 7
Defused
!Clogged

In_1 5
Defused
!Clogged

In_1 1
!Defused
!Clogged
Flush

In_1 5
Defused
!Clogged

Dunk_1

Bs 7

In_1 6
Defused
Clogged

In_1 2
!Defused
Clogged

In_2 7
Defused
!Clogged
Bs 2

In_2 3
!Defused
!Clogged

In_1 1
!Defused
!Clogged

In_2 4
!Defused
Clogged

In_2 3
!Defused
!Clogged

Flush

In_1 5
Defused
!Clogged

In_1 5
Defused
!Clogged

In_1 6
Defused
Clogged

In_2 7
Defused
!Clogged

In_2 7
Defused
!Clogged

In_1 5
Defused
!Clogged

Dunk_2
In_2 8
Defused
Clogged

In_1 6
Defused
Clogged

Bs 5

In_2 3
!Defused
!Clogged

Flush

In_1 6
Defused
Clogged

In_2 7
Defused
!Clogged

In_2 7
Defused
!Clogged

In_2 8
Defused
Clogged

In_2 8
Defused
Clogged

Flush

In_1 5
Defused
!Clogged

In_2 7
Defused
!Clogged
Bs 0

Dunk_1

Bs 8

In_1 5
Defused
!Clogged

In_2 4
!Defused
Clogged

Bs 3

Bs 1

7

In_2
Defused
!Clogged

In_1 5
Defused
!Clogged

In_2 3
!Defused
!Clogged

8

In_2
Defused
Clogged

Bs 10

In_2 3
!Defused
!Clogged

In_1 5
Defused
!Clogged

Flush

In_1 6
Defused
Clogged
Dunk_2

In_1 5
Defused
!Clogged

Dunk_2

In_2 7
Defused
!Clogged

In_2 7
Defused
!Clogged

Bs 4

In_2 7
Defused
!Clogged

In_2 8
Defused
Clogged
Bs 9

Bs 6

Figure 3: The BsP tables for the BTUC problem
where the i are plans of the same length, such that i 6= j for all 1  j 6=i  n. We
call Bsi the belief set indexed by i . When no ambiguity arises, we write BsPT(i ) for
Bsi. The array BsPTables is used to store the BsP tables representing the levels of the
search. The algorithm first checks (line 4) if there are plans of length 0, i.e. if  is a
solution. If no conformant plan of such length exists ((P lans = ;) in line 4), then the
while loop is entered. At each iteration, conformant plans of increasing length are explored
(lines 5 to 8). The step at line 6 expands the BsP table in BsPTables[i 1] and stores the
resulting BsP table in BsPTables[i]. BsP pairs which are redundant with respect to the
current search are eliminated from BsPTables[i] (line 7). The possible solutions contained
in BsPTables[i] are extracted and stored in P lans (line 8). The loop terminates if either a
plan is found (P lans 6= ;), or the space of conformant plans has been completely explored
(BsPTables[i] = ;).
The definitions of the basic functions used in the algorithm are reported in Figure 5.
The function ExpandBsPTable expands the BsP table provided as argument, containing
conformant plans of length i 1, and returns a BsP table with conformant plans of length
i. Each BsP in the input BsP table is expanded by ExpandBsPPair. For each possible
312

fiConformant Planning via Symbolic Model Checking

0
1
2
3
4
5
6
7
8
9
10
11
12
13

function ConformantPlan(I ,G )
begin

i = 0;
BsPTables[0] := f hG : i g;
Plans := ExtractSolution(I ; BsPTables[0]);
while ((BsPTables[i] 6= ;) ^ (P lans = ;)) do
i := i + 1;
BsPTables[i] := ExpandBsPTable(BsPTables[i-1]);
BsPTables[i] := PruneBsPTable(BsPTables[i]; BsPTables; i);
Plans := ExtractSolution(I ; BsPTables[i]);

done
if (BsPTables[i] = ;) then
return Fail;
else return Plans;
end

Figure 4: The conformant planning algorithm.
action ff, the strong pre-image of Bs is computed, and if the resulting belief state Bs0 is
not empty, i.e. there is a belief state from which ff guarantees the achievement of Bs, then
the plan  is extended with ff and hBs0 : ff;  i is returned. The expansion of a BsP table
is the union of the expansions of each BsP pair. The function ExtractSolution takes as
input a BsP table and returns the (possibly empty) set of plans which index a belief states
containing I . PruneBsPTable takes as input the BsP table to be pruned, an array of
previously constructed BsP tables BsPTables, and an index of the current step. It removes
from the BsP table in the input the plans which are not worth being explored because the
corresponding belief states have already been visited.
The algorithm has the following properties. First, it always terminates. This follows
from the fact that the set of explored belief sets (stored in BsPTables) is monotonically
increasing | at each step we proceed only if at least one new belief state is generated.
Because of its finiteness (the set of accumulated belief states is contained in 2S which is
finite), a fix point is eventually reached. Second, it is correct, i.e. when a plan is returned
it is a conformant solution to the given problem. The correctness of the algorithm follows
from the properties of SPreImage : each plan is associated with a belief state for which it
is conformant, i.e. where it is guaranteed to be applicable and from which it results in a
belief state contained in the goal. Third, the algorithm is optimal, i.e. it returns plans of
minimal length. This property follows from the breadth-first style of the search. Finally,
the algorithm is able to decide whether a problem admits no solution, returning Fail in such
cases. Indeed, a conformant solution is always associated with a belief state containing the
initial states. SPreImage generates the maximal belief state associated with a conformant
plan, each new belief state generated in the exploration is compared with the initial states
to check if it is a solution, and a plan is pruned only if an equivalent plan has already been
explored.
313

fiCimatti & Roveri

(BsPT) =_

ExpandBsPTable

[
hBs : i2BsPT

(hBs : i)

ExpandBsPPair

(hBs : i) =_ fhBs0 : ff;  ij such that Bs0 = SPreImage [ff](Bs) 6= ;g

ExpandBsPPair

(BsPT; BsPTables; i) =_
fhBs : i 2 BsPT j for all j < i; there is no hBs : 0 i 2 BsPTables[j ] such that (Bs0 = Bs)g
PruneBsPTable

(I ; BsPT) =_ f j there exists hBs : i 2 BsPT such that I  Bsg

ExtractSolution

Figure 5: The primitives used by the conformant planning algorithm.
5. Conformant Planning via Symbolic Model Checking

Model checking is a formal verification technique based on the exploration of finite state
automata (Clarke, Emerson, & Sistla, 1986). Symbolic model checking (McMillan, 1993) is
a particular form of model checking using Binary Decision Diagrams to compactly represent
and eciently analyze finite state automata. The introduction of symbolic techniques into
model checking led to a breakthrough in the size of model which could be analyzed (Burch
et al., 1992), and made it possible for model checking to be routinely applied in industry,
especially in logic circuits design (for a survey see Clarke & Wing, 1996).
In the rest of this section, we will provide an overview of Binary Decision Diagrams,
and we will describe the representation of planning domains, based on the Bdd-based
representation of finite state automata used in model checking. Then, we will discuss the
extension which allows to symbolically represent BsP tables and their transformations, thus
allowing for an ecient implementation of the algorithm described in the previous section.
5.1 Binary Decision Diagrams

A Reduced Ordered Binary Decision Diagram (Bryant, 1992, 1986) (improperly called Bdd)
is a directed acyclic graph (DAG). The terminal nodes are either T rue or F alse. Each nonterminal node is associated with a boolean variable, and two Bdds, called left and right
branches. Figure 6 (a) depicts a Bdd for (a1 $ b1 ) ^ (a2 $ b2 ) ^ (a3 $ b3 ). At each
non-terminal node, the right [left, respectively] branch is depicted as a solid [dashed, resp.]
line, and represents the assignment of the value T rue [F alse, resp.] to the corresponding
variable. A Bdd represents a boolean function. For a given truth assignment to the variables
in the Bdd, the value of the function is determined by traversing the graph from the root
to the leaves, following each branch indicated by the value assigned to the variables1. The
1. A path from the root to a leaf can visit nodes associated with a subset of all the variables of the Bdd.
See for instance the path associated with a1 ; :b1 in Figure 6(a).

314

fiConformant Planning via Symbolic Model Checking

a1

a1

b1

a2

b1

a3

a2

b2

b2

b1

b1

a3

b1

b2

a3

a2

a3

b1

b1

b2

a3

b1

b2

b1

b2

b3

b3

b3

b3

True

False

True

False

(a)

b1

(b)

Figure 6: Two Bdds for the formula (a1 $ b1 ) ^ (a2 $ b2 ) ^ (a3 $ b3 ).
reached leaf node is labeled with the resulting truth value. If v is a Bdd, its size jvj is the
number of its nodes. If n is a node, var(n) indicates the variable indexing node n.
Bdds are a canonical representation of Boolean functions. The canonicity follows by
imposing a total order < over the set of variables used to label nodes, such that for any
node n and respective non-terminal child m, their variables must be ordered, i.e. var(n) <
var(m), and requiring that the Bdd contains no isomorphic subgraphs.
Bdds can be combined with the usual boolean transformations (e.g. negation, conjunction, disjunction). Given two Bdds, for instance, the conjunction operator builds and
returns the Bdd corresponding to the conjunction of its arguments. Substitution can also
be represented as Bdd transformations. In the following, if v is a variable, and  and are
Bdds, we indicate with [v= ] the Bdd resulting from the substitution of v with in . If
v1 and v2 are vectors of (the same number of) distinct variables, we indicate with [v1 =v2 ]
the parallel substitution in  of the variables in vector v1 with the (corresponding) variables
in v2 .
Bdds also allow for transformations described as quantifications, in the style of Quantified Boolean Formulae (QBF). QBF is a definitional extension to propositional logic, where
propositional variables can be universally and existentially quantified. In terms of Bdd
computations, a quantification corresponds to a tranformation mapping the Bdd of  and
the variable vi being quantified into the Bdd of the resulting (propositional) formula. If 
is a formula, and vi is one of its variables, the existential quantification of vi in , written
9vi:(v1 ; : : : ; vn ), is equivalent to (v1; : : : ; vn )[vi=F alse] _ (v1; : : : ; vn )[vi=T rue]. Analogously, the universal quantification 8vi :(v1 ; : : : ; vn ) is equivalent to (v1 ; : : : ; vn )[vi =F alse]^
315

fiCimatti & Roveri
(v1 ; : : : ; vn )[vi =T rue]. In QBF, quantifiers can be arbitrarily applied and nested. In general, a QBF formula has an equivalent propositional formula, but the conversion is subject
to an exponential blow-up.
The time complexity of the algorithm for computing a truth-functional boolean transformation f1 <op> f2 is O(jf1 j  jf2 j). As far as quantifications are concerned, the time
complexity is quadratic in the size of the Bdd being quantified, and linear in the number
of variables being quantified, i.e. O(jvj  jf j2 ) (Bryant, 1992, 1986).
Bdd packages are ecient implementations of such data structures and algorithms (Brace
et al., 1990; Somenzi, 1997; Yang et al., 1998; Coudert et al., 1993). Basically, a Bdd package deals with a single multi-rooted DAG, where each node represents a boolean function.
Memory eciency is obtained by using a \unique table", and by sharing common subgraphs
between Bdds. The unique table is used to guarantee that at each time there are no isomorphic subgraphs and no redundant nodes in the multi-rooted DAG. Before creating a
new node, the unique table is checked to see if the node is already present, and only if this
is not the case a new node is created and stored in the unique table. The unique table
allows to perform the equivalence check between two Bdds in constant time (since two
equivalent functions always share the same subgraph) (Brace et al., 1990; Somenzi, 1997).
Time eciency is obtained by maintaining a \computed table", which keeps track of the
results of recently computed transformations, thus avoiding the recomputation.
A critical computational factor with Bdds is the order of the variables used. (Figure 6
shows an example of the impact of a change in the variable ordering on the size of a Bdd.)
For a certain class of boolean functions, the size of the corresponding Bdd is exponential in
the number of variables for any possible variable ordering (Bryant, 1991). In many practical
cases, however, finding a good variable ordering is rather easy. Beside affecting the memory
used to represent a Boolean function, finding a good variable ordering can have a big impact
on computation times, since the complexity of the transformation algorithms depends on
the size of the operands. Most Bdd packages provide heuristic algorithms for finding good
variable orderings, which can be called to try to reduce the overall size of the stored Bdds.
The reordering algorithms can also be activated dynamically by the package, during a Bdd
computation, when the total number of nodes in the package reaches a predefined threshold
(dynamic reoredering).
5.2 Symbolic Representation of Planning Domains
A planning domain (P ; S ; A; R) can be represented symbolically using Bdds, as follows. A

set of (distinct) Bdd variables, called state variables, is devoted to the representation of the
states S of the domain. Each of these variables has a direct association with a proposition
of the domain in P used in the description of the domain. For instance, for the BTUC
domain, each of In1 , Defused and Clogged is associated with a unique Bdd variable. In
the following we write x for the vector of state variables. Because the particular order is
irrelevant but for performance issues, in the rest of this section we will not distinguish a
proposition and the corresponding Bdd variable.
A state is a set of propositions of P (specifically, the propositions which are intended
to hold in it). For each state s, there is a corresponding assignment to the state variables
x , i.e. the assignment where each variable corresponding to a proposition p 2 s is assigned
316

fiConformant Planning via Symbolic Model Checking
to T rue, and each other variable is assigned to F alse. We represent s with the Bdd  (s),
having such an assignment as its unique satisfying assignment. For instance,  (6) =_ (In1 ^
Defused ^ Clogged) is the Bdd representing state 6, while  (4) =_ :In1 ^ :Defused ^
Clogged represents state 4, and so on. (Without loss of generality, in the following we do
not distinguish a propositional formula from the corresponding Bdd.) This representation
naturally extends to any set of states Q  S as follows:
 (Q) =_

_ (s)

s2Q

In other words, we associate a set of states with the generalized disjunction of the Bdds
representing each of the states. Notice that the satisfying assignments of the  (Q) are
exactly the assignment representations of the states in Q. This representation mechanism
is very natural. For instance, the Bdd  (I ) representing the the set of initial states of
the BTUC I =_ f1; 2; 3; 4g is :Defused, while for the set of goal states G =_ f5; 7g the
corresponding Bdd is Defused ^ :Clogged. A Bdd is also used to represent the set S of
all the states of the domain automaton. In the BTUC,  (S ) = T rue because S = 2P . In a
different formulation, where two independent propositions In1 and In2 are used to represent
the position of a bomb,  (S ) would be the Bdd In1 $ :In2 .
In general, a Bdd represents the set of (states which correspond to) its models. As
a consequence, set theoretic transformations are naturally represented by propositional
operations, as follows.
 (SnQ)
=_  (S ) ^ : (Q)
 (Q1 [ Q2 ) =_  (Q1 ) _  (Q2 )
 (Q1 \ Q2 ) =_  (Q1 ) ^  (Q2 )

The main eciency of this symbolic representation lies in the fact that the cardinality
of the represented set is not directly related to the size of the Bdd. For instance,  (G ) uses
two (non-terminal) nodes to represent two states, while  (I ) uses one node to represent four
states. As limit cases,  (S ) and  (fg) are (the leaf Bdds) T rue and F alse, respectively. As
a further advantage, symbolic representation is extremely ecient in dealing with irrelevant
information. Notice, for instance, that only the variable Defused occurs in  (f5; 6; 7; 8g ).
For this reason, a symbolic representation can have a dramatic improvement over an explicit,
enumerative representation. This is what allows symbolic, Bdd-based model checkers to
handle finite state automata with a very large number of states (see for instance Burch
et al., 1992). In the following, we will collapse a set of states and the Bdd representing it.
Another set of Bdd variables, called action variables, written ff , is used to represent
actions. We use one action variable for each possible action in A. Intuitively, a Bdd action
variable is true if and only if the corresponding action is being executed. If we assume that
a sequential encoding is used, i.e. no concurrent actions are allowed, we also use a Bdd,
Seq(ff ), to express that exactly one of the action variables must be true at each time2 . For
2. In the specific case of sequential encoding, an alternative approach using only dlog jAje is possible: an
assignment to the action variables denotes a specific action to be executed. Two assignments being
mutually exclusive, the constraint Seq(ff ) needs not to be represented. When the cardinality of the
set of actions is not a power of two, the standard solution is to associate more than one assignment to
certain values. This optimized solution, which is actually used in the implementation, is not described
here for the sake of simplicity.

317

fiCimatti & Roveri
the BTUC problem, where A contains three actions, we use the three Bdd variables Dunk1 ,
Dunk2 and F lush, while we express the serial encoding constraint with the following Bdd:

Seq(ff) =_ (Dunk1 _ Dunk2 _ F lush) ^:(Dunk1 ^ Dunk2 ) ^:(Dunk1 ^ F lush) ^:(Dunk2 ^ F lush)

As for state variables, we are referring to Bdd action variables with symbolic names for
the sake of simplicity. In practice, they will be internally represented as integers, but their
position in the ordering of the Bdd package is totally irrelevant in logical terms.
A Bdd in the variables x and ff represents a set of state-action pairs, i.e. a relation
between states and actions. For instance, the applicability relation in the BTUC (i.e., all
actions are possible in all states, except for dunking actions which require the toilet not
to be clogged) is represented by the Bdd :(Clogged ^ (Dunk1 _ Dunk2 )). Notice that it
represents a set of 16 state-action pairs, each associating a state with an applicable action.
A transition is a 3-tuple composed of a state (the initial state of the transition), an
action (the action being executed), and a state (the resulting state of the transition). To
represent transitions, another vector x 0 of Bdd variables, called next state variables, is
allocated in the Bdd package. We write  0 (s) for the representation of the state s in the
next state variables. With  0 (Q) we denote the construction of the Bdd corresponding to
the set of states Q, using each variable in the next state vector x 0 instead of each current
state variables x . We require that jx j = jx 0 j, and assume that the i-th variable in x and the
i-th variable in x 0 correspond. We define the representation of a set of states in the next
variables as follows.
 0 (s) =_  (s)[x =xx0 ]
We call the operation [x =xx0 ] \forward shifting", because it transforms the representation of
a set of \current" states in the representation of a set of \next" states. The dual operation
[x 0 =xx] is called backward shifting. In the following, we call x current state variables to
distinguish them from next state variables. A transition is represented as an assignment
to x , ff and x 0 . For the BTUC, the transition corresponding to the application of action
Dunk1 in state 1 resulting in state 5 is represented by the following Bdd
 (h1; Dunk1 ; 5i) =_  (1) ^ Dunk1 ^  0 (5)
The transition relation R of the automaton corresponding to a planning domain is
simply a set of transitions, and is thus represented by a Bdd in the Bdd variables x , ff and
x0 , where each satisfying assignment represents a possible transition.
_
 (R) =_ Seq(ff ) ^  (t)
t2R

In the rest of this paper, we assume that the Bdd representation of a planning domain
is given. In particular, we assume as given the vectors of variables x ;xx0 ;ffff, the encoding
functions  and  0 , and we simply call S , R, I and G the Bdd representing the states of the
domain, the transition relation, the initial states and the goal states, respectively. We write
(v) to stress that the Bdd  depends on the variables in v. With this representation, it
is possible to reason about plans, simulating symbolically the execution of sets of actions in
sets of states, by means of QBF transformations. The Bdd representing the applicability
relation can be directly obtained with the following computation.
ff) =_ 9x 0 :R(x ;ffff;xx0 )
Applicable(x ;ff
318

fiConformant Planning via Symbolic Model Checking
The resulting Bdd, Applicable(x ;ffff), represents the set of state-action pairs such that
the action is applicable in the state. The Bdd representing the states reachable from Q in
one step is obtained with the following computation.

9x:9ff:(R(x ;ffff;xx0 )^Q(x))[x 0=xx]
Notice that, with this single operation, we symbolically simulate the effect of the application
of any applicable action in A to any of the states in Q. Similarly, the following transformation allows to symbolically compute the SPreImage of a set of states Q under all possible
actions in one single computation:

8x0:(R(x ;ffff;xx0 ) ! Q(x )[x=xx0 ]) ^

(x ;ffff)

Applicable

The resulting Bdd represents all the state-action pairs hx : ffi such that ff is applicable in
x and the execution of ff in x results in states in Q.
5.3 Symbolic Search in the Space of Belief States

The main strength of the symbolic approach is that it allows to perform a symbolic breadthfirst search, and it provides a way for compactly representing and eciently expanding the
frontier. For instance, plans can be constructed by symbolic breadth-first search in the
space of states, repeatedly applying the strong pre-image to the goal states (Cimatti et al.,
1998b). However, the machinery presented in the previous section cannot be directly applied
to tackle conformant planning. The basic difference is that with conformant planning we are
searching in the space of belief states3 , and therefore the frontier of the search is basically
a set of sets of states. We introduce a way to symbolically represent BsP tables. Basically,
this can be seen as a construction on demand, based on the algorithm steps, of increasingly
large portions of the space of belief states. The key intuition is that a BsP table

fhfs11 ; : : : ; s1n1 g : 1i; : : : ; hfsk1 ; : : : ; skn g : k ig
k

is represented as a relation between plans (of the same length) and states, by associating
the plan directly with each state in the belief state indexed by the plan, as follows:

fhs11 : 1i; : : : ; hs1n1 : 1i; : : : ; hsk1 : k i; : : : ; hskn : k ig
k

(2)

We use additional variables to represent the plans in the BsP tables. In order to represent
plans of increasing length, at each step of the algorithm, a vector of new Bdd variables,
called plan variables, is introduced. The vector of plan variables introduced at the i-th step
of the algorithm is written  [i], with j [i]j = jff j, and is used to encode the i-th to last action
in the plan4. At step one of the algorithm, we introduce the vector of plan variables  [1]
to represent the action corresponding to each 1-length possible conformant plan. The BsP
3. In principle, the machinery for symbolic search could be used to do conformant planning if applied to
the determinization of the domain automaton, i.e. an automaton having 2S as its state space. However,
this would require the introduction of an exponential number of state variables, which is impractical
even for very small domains.
4. The search being performed backwards, plans need to be reversed once found.

319

fiCimatti & Roveri
table BsPT1 at level 1 is built by ExpandBsPTable by performing the following Bdd
computation starting from the BsP table at level 0, i.e. G (x ):
(8x 0 :(R(x ;ffff;xx0 ) ! G (x )[x =xx0 ]) ^ Applicable(x ;ffff))[ff = [1]]
The computation collects those state-action pairs hx : ff i such that (the action represented
by) ff is applicable in (the state represented by) x , and such that all the resulting (states
represented by) x 0 are goal states. Then we replace the vector of action variables ff with
the first vector of plan variables  [1]. The resulting Bdd, BsPT(x ; [1]), represents a BsP
table containing plans of length one in the form of a relation between states and plans as
in (2). In the general case, after step i 1, the BsP table BsPTi 1 , associating belief states
to plans of length i 1, is represented by a Bdd in the state variables x and in the plan
variables  [i 1] ; : : : ; [1]. The computation performed by ExpandBsPTable at step i is
implemented as the following Bdd transformation on BsPTi 1
(8x 0 :(R(x ;ffff;xx0 ) ! BsPTi 1 (x ; [i 1]; : : : ; [1] )[x =xx0 ]) ^ Applicable(x ;ffff))[ff = [i]](3)
The next state variables in R and in BsPTi 1 (resulting from the forward shifting) disappear
because of the universal quantification. The action variables ff are renamed to the newly
introduced plan variables  [i], so that in the next step of the algorithm the construction can
be repeated.
ExtractSolution extracts the assignments to plan variables such that the corresponding set contains the initial states. In terms of Bdd transformations, ExtractSolution is
implemented as follows:
8x:(I (x) ! BsPTi(x; [i]; : : : ; [1]))
(4)
The result is a Bdd in the plan variables  [i]; : : : ; [1]. If the Bdd is F alse, then there are
no solutions of length i. Otherwise, each of the satisfying assignments to the resulting Bdd
represents a conformant solution to the problem.
To guarantee the termination of the algorithm, at each step the BsP table returned
by ExpandBsPTable is simplified by PruneBsPTable by removing all the belief states
which do not deserve further expansion. This requires the comparison of the belief states
contained in the BsP table with the belief states contained in each of the BsP tables built at
previous levels. This is one of the crucial steps in terms of eciency. An earlier implementation of this step with logical Bdd transformations, following directly from the set-theoretical
definition of PruneBsPTable, was extremely inecient (Cimatti & Roveri, 1999). Furthermore, we noticed that the serial encoding could yield BsP tables containing a large
number of equivalent plans, all indexing exactly the same belief state. Often these equivalent plans only differ in the order of some independent actions, and this is a potential source
of combinatorial explosion. This occurs even in the simple version of the BTUC (in Figure 3,
two equivalent conformant plans are associated with Bs8 ). Therefore, we developed a new
implementation which could tackle these two problems by operating directly on the BsP
table. The idea is depicted in Figure 7. Initially, the cache contains Bs1 , Bs2 and Bs3 . The
simplification performs a traversal of the Bdd, by accumulating the subtrees representing
belief states, comparing them with the ones built at previous levels, and inserting the new
ones in the cache (in Figure 7, Bs4 , Bs5 and Bs6 ). Each time a path is identified which
320

fiConformant Planning via Symbolic Model Checking
BsP Table

Bs4

Bs2

Bs5

Pruned BsP Table

Bs6

Bs4

Bs5

Cached Belief States
Bs1

Bs2

Bs6

Cached Belief States

Bs3

Bs1

Bs2

Bs3

Bs4

Bs5

Bs6

Figure 7: An example of pruning of a BsP table
represents a plan indexing an already cached belief state, the plan is redundant and the
corresponding path is pruned5. The cost of the simplification is linear in the size of the BsP
being simplified and is highly effective in pruning.
6. CMBP: a BDD-based Conformant Planner

Cmbp (Conformant Model Based Planner) is a conformant planner implementing the data
structures and algorithms for conformant planning described in the previous sections. Cmbp
inherits the features of Mbp (Cimatti et al., 1997, 1998b, 1998a), a planner based on
symbolic model checking techniques. Mbp is built on top of NuSMV, a symbolic model
checker jointly developed by ITC-IRST and CMU (Cimatti et al., 2000), and uses the
CUDD (Somenzi, 1997) state-of-the-art Bdd package. Mbp is a two-stage system. In
the first stage, an internal Bdd-based representation of the domain is built, while in the
second stage planning problems can be solved. Currently, planning domains are described
by means of the high-level action language AR (Giunchiglia et al., 1997). AR allows to
specify (conditional and uncertain) effects of actions by means of high level assertions. For
instance, Figure 8 shows the AR description of the BTUC problem6. The semantics of
AR yields a serial encoding, i.e. exactly one action is assumed to be executed at each

5. This pruning mechanism is actually weaker than the earlier one (Cimatti & Roveri, 1999). Here we
require that the same belief state must not be expanded twice during the search, while in the earlier
version we prune belief states contained in previously explored ones. This may increase the number of
explored belief states. However, it allows for a much more ecient implementation, without impacting
on the properties of the algorithm.
6. ! and & stand for negation and conjunction, respectively. The description is slightly edited for the sake of
readability. In particular, Mbp currently does not accept parameterized AR descriptions. In practice we
use a script language to generate ground instances of different complexity from a parameterized problem
description.

321

fiCimatti & Roveri

DOMAIN BTUC
ACTIONS Dunk_1, Dunk_2, Flush;
FLUENTS In_1, In_2, Defused, Clogged : boolean;
INERTIAL Clogged, Defused, In_1, In_2;
ALWAYS In_1 <-> !In_2;
Flush CAUSES !Clogged;
for i in [1, 2] {
Dunk_<i> HAS PRECONDITIONS !Clogged;
Dunk_<i> CAUSES Defused IF In_<i>;
Dunk_<i> POSSIBLY CHANGES Clogged;
}
INITIALLY !Defused;
CONFORMANT Defused & !Clogged;

Figure 8: An AR description for the BTUC problem
time. The automaton corresponding to an AR description is obtained by means of the
minimization procedure by Giunchiglia (1996). This procedure solves the frame problem
and the ramification problem, and is eciently implemented in Mbp (Cimatti et al., 1997).
Because of the separation between the domain construction and the planning phases, Mbp
is not bound to AR. Standard deterministic domains specified in Pddl (Ghallab et al.,
1998) can also be given to Mbp by means of a (prototype) compiler. We are also starting
to investigate the potential use of the C action language (Giunchiglia & Lifschitz, 1998),
which allows to represent domains with parallel actions.
Different planning algorithms can be applied to the specified planning problems. They
operate solely on the automaton representation, and are completely independent of the
particular language used to specify the domain. Mbp allows for automatic construction of
conditional plans under total observability, by implementing the algorithms for strong planning (Cimatti et al., 1998b), and for strong cyclic plannig (Cimatti et al., 1998a; Daniele,
Traverso, & Vardi, 1999). In Cmbp, we implemented the ideas described in the previous
sections. The primitives to construct and prune BsP tables required a lot of tuning, in
particular with the ordering of Bdd variables. We found a general ordering strategy which
works reasonably well: action variables are positioned at the top of the ordering, followed by
plan variables, followed by state variables, with current state and next state variables interleaved. The specific ordering within action variables, plan variables, and state variables is
determined by the standard mechanism implemented in NuSMV. Cmbp implements several
algorithms for conformant planning. In addition to the backward algorithm presented in
322

fiConformant Planning via Symbolic Model Checking
Section 4, Cmbp implements an algorithm based on forward search, which allows to exploit
the initial knowledge of the problem, sometimes resulting in significant speed ups (Cimatti
& Roveri, 2000). Backward and forward search can also be combined, to tackle the exponential growth of the search time with the depth of search. For all these algorithms,
different options enable and disable different versions of the termination check.
7. Experimental Evaluation

In this section we present an experimental evaluation of our approach, which was carried
out by comparing Cmbp with state-of-the-art conformant planners. We first describe the
other conformant planners considered in the analysis, and then we present the experimental
comparison that was carried out.
7.1 Other Conformant Planners

Cgp (Smith & Weld, 1998) extends the ideas of Graphplan (Blum & Furst, 1995, 1997) to
deal with uncertainty. Basically, a planning graph is built of every possible sequence of possible worlds, and constraints among planning graphs are propagated to ensure conformance.
The Cgp system takes as input domains described in an extension of Pddl (Ghallab et al.,
1998), where it is possible to specify uncertainty in the initial state. Cgp inherits from
Graphplan the ability to deal with parallel actions. Cgp was the first ecient conformant planner: it was shown to outperform several other planners such as Buridan (Peot,
1998) and UDTPOP (Kushmerick, Hanks, & Weld, 1995). The detailed comparison reported by Smith and Weld (1998) leaves no doubt on the superiority of Cgp with respect
to these systems. Therefore, we compared Cmbp with Cgp and did not consider the other
systems analyzed by Smith and Weld (1998). Cmbp is more expressive than Cgp in two
respects. First, Cgp can only handle uncertainty in the initial state. For instance, Cgp
cannot analyze the BTUC domain presented in Section 3. Smith and Weld (1998) describe
how the approach can be extended to actions with uncertain effects. Second, Cgp cannot
conclude that a planning problem has no conformant solutions.
Qbfplan is (our name for) the planning system by Rintanen (1999a). Qbfplan generalizes the idea of SAT-based planning (Kautz, McAllester, & Selman, 1996; Kautz & Selman,
1996, 1998) to nondeterministic domains, by encoding problems in QBF. The Qbfplan
approach is not limited to conformant planning, but can be used to do conditional planning
under uncertainty, also under partial observability: different encodings, corresponding to
different structures in the resulting plan, can be synthesized. In this paper, we are only
considering encodings which enforce the resulting plan to be a sequence. Given a bound on
the length of the plan, first a QBF encoding of the problem is generated, and then a QBF
solver (Rintanen, 1999b) is called. If no solution is found, a new encoding for a longer plan
must be generated and solved. Qbfplan is able to handle actions with uncertain effects.
This is done by introducing auxiliary (choice) variables, the assignments to which the different possible outcomes of actions correspond. These variables are universally quantified
to ensure conformance of the solution. Differently from e.g. Blackbox (Kautz & Selman,
1998), Qbfplan does not have a heuristic to guess the \right" length of the plan. Given
a limit in the length of the plan, it generates all the encodings up to the specified length,
and repeatedly calls the QBF solver on encodings of increasing length until a plan is found.

323

fiCimatti & Roveri
As Cgp, Qbfplan cannot conclude that a planning problem has no conformant solutions.
Similarly to Cmbp, Qbfplan relies on a symbolic representation of the problem, although
QBF transformations are performed by a theorem prover rather than with Bdds.
Gpt (Bonet & Geffner, 2000) is a general planning framework, where the conformant
planning problem is seen as deterministic search problem in the space of belief states. Gpt
uses an explicit representation of the search space, where each belief state is represented
as a separate data structure. The search is based on the A algorithm (Nilsson, 1980),
driven by domain dependent heuristics which are automatically generated from the problem
description. Gpt accepts problem descriptions in a syntax based on Pddl, extended to deal
with probabilities and uncertainty. It is possible to represent domains with uncertain action
effects (although the representation of actions resulting in a large number of different states
is rather awkward). As for the planning algorithm, Gpt is able to conclude that a given
planning problem has no conformant solution by exhaustively exploring the space of belief
states.
7.2 Experiments and Results

The evaluation was performed by running the systems on a number of parameterized problem domains. We considered all the problems from the Cgp and Gpt distributions, plus
other problems which were defined to test specific features of the planners. We considered
domains with uncertainty limited to the initial state, and domains with uncertain action
effects. Besides problems admitting a solution, we also considered problems not admitting
a solution, in which case we measured the effectiveness of the plannner in returning with
failure.
Given their different expressivity, it was not possible to run all the systems on all the
examples. Cmbp was run on all the classes of examples, while Gpt was run on all but one.
Cgp was run only on the problems which admit a solution, and with uncertainty limited
to the initial condition. Qbfplan was run on all the examples for which an encoding was
already available from the Qbfplan distribution. This is only a subset of the problems
expressible in Cgp. The main limiting factor was the low level of the input format of
Qbfplan: problem descriptions must be specified as ML code which generates the QBF
encodings. Writing new encodings turned out to be a very dicult task, especially due to
the lack of documentation.
We ran Cgp, Qbfplan and Cmbp on an Intel 300MHz Pentium-II, 512MB RAM,
running Linux. The comparison between Cmbp and Gpt was run on a Sun Ultra Sparc
270MHz, 128Mb RAM running Solaris (Gpt was available as a binary). However, the
performance of the two machines is comparable | the run times for Cmbp were almost
identical. CPU time was limited to 7200 sec (two hours) for each test. To avoid swapping,
the memory limit was fixed to the physical memory of the machine. In the following, we
write \|" or \===" for a test that did not complete within the above time and memory
limits, respectively. The performance of the systems are reported in tables listing only the
search time. This excludes the time needed by Qbfplan to generate the encodings, the
time spent by Cmbp to construct the automaton representation into Bdd, and the time
needed by Gpt to generate the source code of its internal representation, and to compile
it. Overall, the most significant time ignored is the automaton construction of Cmbp.
324

fiConformant Planning via Symbolic Model Checking
Currently, the automaton construction is not fully optimized. Even in the most complex
examples, however, the construction never required more than a couple of minutes7.
7.2.1 Bomb in the Toilet

Bomb in the Toilet. The first domain we tackled is the classical bomb in the toilet,
where there is no notion of clogging. We call the problem BT(p), where the parameter p

is the number of packages. The only uncertainty is in the initial condition, where it is not
known which package contains the bomb. The goal is to defuse the bomb. The results for
the BT problem are shown in Table 1. The columns relative to Cmbp are the length of the
plan (jPj), the number of cached belief states and the number of hits in the cache (#BS and
#NBS respectively), the time (expressed in seconds) needed for searching the automaton
under Pentium/Linux (Time(L)) and under Sparc/Solaris (Time(S)). In the following, when
clear from the context, the execution platform is omitted. The columns relative to Cgp are
the number of levels in the planning graphs (jLj) and the search time. The column relative
to Gpt is the search time.
BT(2)
BT(3)
BT(4)
BT(5)
BT(6)
BT(7)
BT(8)
BT(9)
BT(10)

jPj

2
3
4
5
6
7
8
9
10

Cmbp

#BS/#BSH
2/2
6 / 11
14 / 36
30 / 103
62 / 266
126 / 641
254 / 1496
510 / 3463
1022 / 7862

Time(L)
0.000
0.000
0.000
0.000
0.010
0.010
0.030
0.070
0.150

Time(S)
0.000
0.000
0.000
0.000
0.010
0.030
0.030
0.070
0.140

jLj
1
1
1
1
1
1
1
1
1

Cgp

Time
0.000
0.000
0.000
0.000
0.010
0.010
0.020
0.020
0.020

Gpt

Time
0.074
0.077
0.080
0.087
0.102
0.139
0.230
0.481
1.018

Table 1: Results for the BT problems.
The BT problem is intrinsically parallel, i.e. the depth of the planning graph is always
one, because all the packages can be dunked at the same time. Cgp inherits from Graphplan the ability to deal with parallel actions eciently, and therefore it is almost insensitive
to the problem size. For this problem Cgp outperforms both Cmbp and Gpt. Notice that
the number of levels explored by Cgp is always 1, while the length of the plan produced by
Cmbp and Cgp grows linearly. Cmbp performs slightly better than Gpt.
Bomb in the Toilet with Clogging. We call BTC(p) the extension of the BT(p) where
dunking a package (always) clogs the toilet, ushing can remove the clogging, and no clogging is a precondition for dunking a package. Again, p is the number of packages. The toilet
is initially not clogged. With this modification, the problem no longer allows for a parallel
solution. The results for this problem are listed in Table 2. The impact of the depth of
the plan length becomes significant for all systems. Both Cmbp and Gpt outperform Cgp.
In this case Cmbp performs better than Gpt, especially on large instances (see BTC(16)).
7. More precisely, the maximum time in building the automaton was required for the BMTC(10,6) examples
(88 secs.), the RING(10) example (77 secs.), the BMTC(9,6) examples (40 secs.), and the BMTC(10,5)
examples (41 secs.). For most of the other examples, the time required for the automaton construction
was less than 10 seconds.

325

fiCimatti & Roveri
Qbfplan

BTC(2)
BTC(3)
BTC(4)
BTC(5)
BTC(6)
BTC(7)
BTC(8)
BTC(9)
BTC(10)

jPj

3
5
7
9
11
13
15
17
19

Cmbp

Cgp

#BS/#BSH Time(L) Time(S) jLj Time
6/8
0.000
0.010 3
0.000
14 / 23
0.000
0.000 5
0.010
30 / 61
0.010
0.010 7
0.030
62 / 150
0.020
0.020 9
0.130
126 / 347
0.020
0.020 11
0.860
254 / 796
0.070
0.080 13
2.980
510 / 1844
0.150
0.160 15 13.690
1022 / 4149
0.320
0.330 17 41.010
2046 / 9190
0.710
0.700 19 157.590

BTC(16) 31 131070 / 921355

99.200

99.800

Gpt

Time
0.074
0.077
0.082
0.094
0.113
0.166
0.288
0.607
1.309
351.457

BTC(6)
jPj Time
1
0.00
2
0.01
3
0.26
4
0.63
5
1.53
6
2.82
7
6.80
8
14.06
9
35.59
10
93.34
11 (+) 2.48

BTC(10)
Time
1
0.02
2
0.03
3
0.78
4
2.30
5
4.87
6
8.90
7
22.61
8
52.72
9
156.12
10
410.86
11 1280.88
13 3924.96
14
|

jPj

:::
:::
18
|
19 (+) 16.84

Table 2: Results for the BTC problems.
The comparison with Qbfplan is limited to the 6 and 10 package instances (the ones available from the distribution package). The performance of Qbfplan is reported in the left
table in Table 2. Each line reports the time needed to decide whether there is a plan of
length i. The performance of Qbfplan is rather good when tackling an encoding admitting a solution (in Table 2 these entries are labeled by (+)). For instance, in the BTC(10)
Qbfplan finds the solution solving the encodings at depth 19 reasonably fast. However,
when a solution cannot be found, i.e. the QBF formula admits no model, the performance
of Qbfplan degrades significantly (for the depth 18 encoding, we let the solver run for 10
CPU hours and it did not complete the search). Because of the difference in performance,
and the diculty in writing new domains, in the rest of the comparison we will not consider
Qbfplan.
Bomb in Multiple Toilets. The next domain, called BMTC(p,t), is the generalization
of the BTC problem to the case of multiple toilets (p is the number of packages, while t
is the number of toilets). The problem becomes more parallelizable when the number of
toilets increases. Furthermore, we considered three versions of the problem with increasing
uncertainty in the initial states. In the first class of tests (\Low Uncertainty" columns), the
only uncertainty is the position of the bomb which is unknown, while toilets are known to
be not clogged. The \Mid Uncertainty" and \High Uncertainty" columns show the results
in presence of more uncertainty in the initial state. In the second [third, respectively] class
of tests, the status of every odd [every, resp.] toilet can be either clogged or not clogged.
This increases the number of possible initial states.
The results are reported in Table 3 (for the comparison with Cgp) and in Table 4
(for the comparison with Gpt). The IS column represents the number of initial states of
the corresponding problem. Cgp is able to fully exploit the parallelism of the problem.
However, Cgp is never able to explore more than 9 levels in the planning graph, with depth
decreasing with the number of initial states. The results also show that Cmbp and Gpt
are much less sensitive to the number of initial states than Cgp. With increasing initial
326

fi(p,t)
(2,2)
(3,2)
(4,2)
(5,2)
(6,2)
(7,2)
(8,2)
(9,2)
(10,2)
(2,3)
(3,3)
(4,3)
(5,3)
(6,3)
(7,3)
(8,3)
(9,3)
(10,3)
(2,4)
(3,4)
(4,4)
(5,4)
(6,4)
(7,4)
(8,4)
(9,4)
(10,4)
(2,5)
(3,5)
(4,5)
(5,5)
(6,5)
(7,5)
(8,5)
(9,5)
(10,5)
(2,6)
(3,6)
(4,6)
(5,6)
(6,6)
(7,6)
(8,6)
(9,6)
(10,6)

bmtc

IS
2
3
4
5
6
7
8
9
10
2
3
4
5
6
7
8
9
10
2
3
4
5
6
7
8
9
10
2
3
4
5
6
7
8
9
10
2
3
4
5
6
7
8
9
10

2
4
6
8
10
12
14
16
18
2
3
5
7
9
11
13
15
17
2
3
4
6
8
10
12
14
16
2
3
4
5
7
9
11
13
15
2
3
4
5
6
8
10
12
14

jPj

Low Uncertainty
Cmbp
#BS/#BSH Time
10 / 18 0.000
26 / 84 0.000
58 / 250 0.020
122 / 652 0.030
250 / 1552 0.070
506 / 3586 0.180
1018 / 8262 0.400
2042 / 18484 0.940
4090 / 40676 1.820
18 / 42 0.000
47 / 202 0.010
110 / 736 0.030
237 / 2034 0.080
492 / 5106 0.230
1003 / 12128 0.560
2026 / 27836 1.300
4073 / 62470 3.330
8168 / 138046 7.280
29 / 75 0.010
92 / 492 0.020
206 / 1686 0.060
457 / 4987 0.190
964 / 12456 0.410
1983 / 29453 1.040
4026 / 68466 2.740
8117 / 153895 6.690
16304 / 339160 14.420
43 / 117 0.010
164 / 1031 0.040
416 / 4304 0.150
872 / 11763 0.490
1875 / 31695 1.300
3901 / 78009 3.990
7974 / 183036 9.670
16142 / 416333 24.250
32501 / 927329 54.910
60 / 168 0.010
270 / 1848 0.070
786 / 9294 0.300
1777 / 29075 1.160
3613 / 71123 3.290
7625 / 180127 9.060
15726 / 429198 20.710
32012 / 986188 50.610
64675 / 2.21106e+06 111.830
Time
0.000
0.020
0.030
1.390
3.490
508.510
918.960
|
0.010
0.010
0.110
0.170
0.340
6248.010
|

Cgp

327

Mid Uncertainty
Cmbp
IS
#BS/#BSH Time
4
12 / 34 0.000
6
28 / 106 0.000
8
60 / 286 0.020
10
124 / 702 0.030
12
252 / 1614 0.080
14
508 / 3662 0.190
16
1020 / 8362 0.430
18
2044 / 18602 0.960
20
4092 / 40810 1.990
8
24 / 99 0.000
12
56 / 349 0.020
16
120 / 942 0.040
20
248 / 2335 0.110
24
504 / 5520 0.250
28
101 / 12673 0.590
32
204 / 28530 1.350
36
408 / 63331 3.370
40
818 / 139092 7.460
8
29 / 75 0.000
12
108 / 808 0.030
16
236 / 2356 0.080
20
492 / 5888 0.230
24
1004 / 13648 0.470
28
2028 / 31004 1.120
32
4076 / 70584 2.870
36
8172 / 15654 6.900
40
16364 / 34234 14.630
16
43 / 117 0.010
24
212 / 2008 0.080
32
475 / 6375 0.260
40
987 / 15928 0.700
48
2011 / 37759 1.890
56
4059 / 86716 4.480
64
8155 / 195055 10.590
72
16347 / 432408 25.600
80
32731 / 948279 56.420
16
60 / 168 0.010
24
270 / 1848 0.070
32
920 / 13810 0.500
40
1958 / 37636 1.940
48
4005 / 90111 4.080
56
8100 / 208050 10.130
64
16291 / 469277 22.620
72 32674 / 1.04173e+06 53.510
80 65441 / 2.28585e+06 116.440
0.020
0.290
0.730
|

1 0.200
1 0.830
2 30.630
2 30.140
2 57.300
2
|

1 0.130
2 3.540
2 6.320
2 37,959
2
|

1
2
2
2

0.090
0.200
0.990
|

2
2
3
3

2
3
4
5
5

Time
0.010
0.040
0.460
13,180
|

Cgp

jLj

Table 3: Results for the BMTC problems.

1
0.000
1
0.010
1
0.010
3
0.500
3
1.160
3
2.410
3
8.540
4
|
1
0.010
1
0.020
1
0.020
1
0.050
3
5.920
3 18.410
3 62.040
3 194.640
3 289,680
1
0.010
1
0.010
1
0.040
1
0.060
1
0.100
3 211.720
3 1015.160
3 3051.990
2
|

1
3
3
5
5
7
7
7
1
1
3
3
3
5
4

jLj

High Uncertainty
Cmbp
IS
#BS/#BSH Time
8
12 / 40 0.000
12
28 / 112 0.010
16
60 / 294 0.010
20
124 / 710 0.040
24
252 / 1622 0.080
28
508 / 3670 0.190
32
1020 / 8372 0.450
36
2044 / 18612 0.950
40
4092 / 40820 2.030
16
24 / 126 0.010
24
56 / 373 0.020
32
120 / 972 0.040
40
248 / 2371 0.120
48
504 / 5562 0.240
56
1016 / 12721 0.640
64
2040 / 28584 1.330
72
4088 / 63391 3.390
80
8184 / 139158 7.430
32
48 / 332 0.020
48
112 / 960 0.040
64
240 / 2532 0.090
80
496 / 6092 0.240
96
1008 / 13876 0.470
112
2032 / 31260 1.160
128
4080 / 70912 2.910
144
8176 / 156904 6.970
160
16368 / 342736 14.770
64
93 / 751 0.030
96
224 / 2591 0.120
128
480 / 6740 0.260
160
992 / 16393 0.730
192
2016 / 38334 1.980
224
4064 / 87411 4.540
256
8160 / 195880 10.640
288
16352 / 433373 25.370
320
32736 / 949394 56.290
128
171 / 1533 0.040
192
448 / 6248 0.310
256
960 / 16344 0.690
320
1984 / 39710 2.120
384
4032 / 92772 4.600
448
8128 / 211370 10.400
512
16320 / 473328 23.000
576 32704 / 1.04658e+06 54.010
640 65472 / 2.29158e+06 116.240
1.610
8.690
32.190
|

0.170
0.690
|

Time
0.030
13.560
145.830
|

Cgp

2 337.604
2 1459.110
2 5643.450
2
|

2 21.120
2 138.430
2 551.210
2 1523.840
2
|

2
2
2
3

2
2
3

2
4
4
4

jLj

Conformant Planning via Symbolic Model Checking

fiCimatti & Roveri
bmtc

(p,t)
(2,2)
(3,2)
(4,2)
(5,2)
(6,2)
(7,2)
(8,2)
(9,2)
(10,2)
(2,4)
(3,4)
(4,4)
(5,4)
(6,4)
(7,4)
(8,4)
(9,4)
(10,4)
(2,6)
(3,6)
(4,6)
(5,6)
(6,6)
(7,6)
(8,6)
(9,6)
(10,6)

Low Unc.

Cmbp

Time
0.000
0.010
0.000
0.040
0.080
0.190
0.390
0.910
1.850
0.000
0.010
0.050
0.180
0.370
1.080
2.700
8.970
14.210
0.010
0.050
0.310
1.110
3.400
8.910
21.240
49.880
113.680

Gpt

Time
0.079
0.087
0.105
0.146
0.227
0.441
0.922
2.211
5.169
0.109
0.156
0.270
0.616
1.435
3.484
8.767
23.858
59.966
0.303
0.562
1.354
3.257
8.691
25.677
68.427
289.000
486.969

High Unc.

Cmbp

Time
0.010
0.010
0.020
0.040
0.070
0.200
0.400
0.950
1.900
0.010
0.040
0.100
0.240
0.460
1.190
2.830
6.920
114.690
0.060
0.260
0.620
2.060
4.660
10.430
23.860
54.190
118.590

Gpt

Time
0.079
0.091
0.121
0.198
0.376
0.850
1.966
4.743
10.620
0.121
0.284
1.016
3.282
9.374
27.348
72.344
180.039
440.308
0.482
2.471
17.406
74.623
243.113
701.431
===

Table 4: Results for the BMTC problems.
uncertainty, Cgp is almost unable to solve what were trivial problems. Gpt performs better
than Cgp, but it suffers from the explicit representation of the search space.
Bomb in the Toilet with Uncertain Clogging. The BTUC(p) domain is the domain
described in Section 2, where clogging is an uncertain outcome of dunking a package. This
kind of problem cannot be expressed in Cgp. The results for Cmbp and Gpt are reported
in Table 5. Although Cmbp performs better than Gpt (by a factor of two to three), there
is no significant difference in the behavior. It is interesting to compare the results of Cmbp
for the BTC and BTUC problems. For Gpt a slight difference is noticeable, resulting from
the increased branching factor in the search space due to the uncertainties in the effects of
action executions. In the performance of Cmbp, the number of uncertainties is not a direct
factor | for example, in the BTC(16) and BTUC(16), the performance is almost the same.
7.2.2 Ring of Rooms

Simple Ring of Room. We considered another domain, where a robot can move in a
ring of rooms. Each room has a window, which can be either open, closed or locked. The
robot can move (either clockwise or counterclockwise), close the window of the room where
it is, and lock it if closed. The goal is to have all windows locked.
328

fiConformant Planning via Symbolic Model Checking
Cmbp

jPj

BTUC(2)
BTUC(3)
BTUC(4)
BTUC(5)
BTUC(6)
BTUC(7)
BTUC(8)
BTUC(9)
BTUC(10)
BTUC(16)

#BS/#BSH
6/8
14 / 23
30 / 61
62 / 150
126 / 347
254 / 796
510 / 1844
1022 / 4149
2046 / 9190
131070 / 921355

3
5
7
9
11
13
15
17
19
31

Time
0.000
0.000
0.010
0.010
0.030
0.050
0.170
0.310
0.720
98.270

Gpt

Time
0.076
0.078
0.085
0.098
0.128
0.205
0.380
0.812
1.828
486.252

Table 5: Results for the BTUC problems.
N-1

N

1

2

In the problem RING(r), where r is the number of rooms, the uncertainty is only in the
initial condition: both the position of the robot and the status of the windows can be
uncertain. These problems do not have a parallel solution, and have a large number of initial
states (r  3r ), corresponding to full uncertainty on the position of the robot and on the
status of each window. The results8 are reported on the left in Table 6. Cmbp outperforms
RING(2)
RING(3)
RING(4)
RING(5)
RING(6)
RING(7)
RING(8)
RING(9)
RING(10)

jPj

5
8
11
14
17
20
23
26
29

Cmbp

#BS/#BSH
8 / 24
26 / 78
80 / 240
242 / 726
728 / 2184
2186 / 6558
6560 / 19680
19682 / 59046
59048 / 177144

Time
0.000
0.020
0.040
0.120
0.370
1.420
4.950
27.330
106.870

jLj
3
4

Cgp

Time
0.070
|

Gpt

Time
0.085
0.087
0.392
1.150
6.620
23.636
105.158
===

IS
1
2
4
8
16

Cgp on RING(5)

jLj
5
5
5
5
5

Time
0.010
0.060
0.420
6.150
|

jLj
9
9
9
9
9

Time
0.020
0.140
1.950
359.680
|

Table 6: The results for the RING problems.
both Cgp and Gpt, although Gpt performs much better than Cgp. Both Cgp and Gpt
suffer from the increasing complexity of the problem. On the right in Table 6, we plot (for
the RING(5) problem) the dependency of Cgp on the number of initial states combined
with the number of levels to be explored (different goals were provided which require the
exploration of different levels). It is clear that the number of initial states and the depth of
the search are both critical factors for Cgp.
8. The times reported for Cgp refer to a scaled-down version of the problem, where locking is not taken
into account, and thus the maximum number of initial states is r  2r .

329

fiCimatti & Roveri
Ring of Rooms with Uncertain Action Effects. We considered a variation of the

RING domain, called URING, first introduced by Cimatti and Roveri (1999), which is not
expressible in Cgp. If a window is not locked and the robot is not performing an action
which will determine its status (e.g. closing it), then the window can open or close nondeterministically. For instance, while the robot is moving from room 1 to room 2, the windows in
room 3 and 4 could be open or closed by the wind. This domain is clearly designed to stress
the ability of a planner to deal with actions having a large number of resulting states. In the
worst case (e.g. a move action performed when no window is locked), there are 2r possible
resulting states. Although seemingly artificial, this captures the fact that environments can
be in practice highly nondeterministic. We tried to compare Cmbp and Gpt on the URING
problem. In principle Gpt is able to deal with uncertainty in the action effects. However,
we failed to codify the URING in the Gpt language, because it requires a conditional description of uncertain effects. Therefore, we experimented with a variation of the RING
domain featuring a higher degree of nondeterminism, called NDRING in the following. The
NDRING domain contains an increasing number of additional propositions, called in the
following noninertial propositions, which are initially unknown and are nondeterministically
altered by each action. If i is the number of noninertial propositions, each action has 2i
NDRING(2)
NDRING(3)
NDRING(4)
NDRING(5)
NDRING(6)
NDRING(7)
NDRING(8)
NDRING(9)
NDRING(10)

jPj

5
8
11
14
17
20
23
26
29

Cmbp

#BS/#BSH
8 / 24
26 / 78
80 / 240
242 / 726
728 / 2184
2186 / 6558
6560 / 19680
19682 / 59046
59048 / 177144

Time (5)
0.000
0.020
0.040
0.110
0.350
1.350
4.990
27.060
103.760

Time (2)
0.140
0.256
1.046
4.550
18.758
108.854
===

Gpt

Time (3)
0.384
0.679
3.025
12.960
57.300
===

Time (4)
0.948
2.574
12.548
48.426
===

Time (5)
4.544
13.960
67.714
===

Table 7: The results for the NDRING problems.
possible outcomes. The results are listed in Table 7, with columns labeled with Time(i).
The growing branching factor during the search has a major impact on the performance of
Gpt, while Cmbp is insensitive to this kind of uncertainty. (The performance of Cmbp for
a lower number of noninertial propositions are not reported because they are basically the
same.)
The URING problem was run only on Cmbp. The results are listed in Table 8. It can
be noticed that the performances of Cmbp improve significantly with respect to the RING
problem. This can be explained considering that, despite the larger number of transitions,
the number of explored belief states is significantly smaller (see the Bs cache statistics in
Tables 6 and 8).
7.2.3 Square and Cube

The following domains are the SQUARE(n) and CUBE(n) from the Gpt distribution (Bonet
& Geffner, 2000). These problems consist of a robot navigating in a square or cube of side
n. In both domains there are actions for moving the robot in all the possible directions.
Moving the robot against a boundary leaves the robot in the same position. The original
330

fiConformant Planning via Symbolic Model Checking

URING(2)
URING(3)
URING(4)
URING(5)
URING(6)
URING(7)
URING(8)
URING(9)
URING(10)

jPj
5
8
11
14
17
20
23
26
29

Cmbp

#BS/#BSH
5 / 16
11 / 34
23 / 70
47 / 142
95 / 286
191 / 574
383 / 1150
767 / 2302
1535 / 4606

Time
0.000
0.010
0.020
0.040
0.080
0.190
0.410
0.980
2.2300

Table 8: Results for the URING problems.
problems, called CORNER in the following, require the robot to reach a corner, starting
from a completely unspecified position. We introduced two variations. In the first, called
FACE, the initial position is any position of a given side [face] of the square [cube], while
the goal is to reach the central position of the opposite side [face]. In the second, called
CENTER, the initial position is completely unspecified, and the goal is the center of the
square [cube]. For the corner problem, a simple heuristic is to perform only steps towards
the corner, thus pruning half of the actions. The variations are designed not to allow for a
simple heuristic | for instance, in the CENTER problem, no action can be eliminated.
SQUARE(i)
SQUARE(2)
SQUARE(4)
SQUARE(6)
SQUARE(8)
SQUARE(10)
SQUARE(12)
SQUARE(14)
SQUARE(16)
SQUARE(18)
SQUARE(20)
CUBE(i)
CUBE(2)
CUBE(3)
CUBE(4)
CUBE(5)
CUBE(6)
CUBE(7)
CUBE(8)
CUBE(9)
CUBE(10)
CUBE(15)

jPj

3
6
9
12
15
18
21
24
27
42

jPj

2
6
10
14
18
22
26
30
34
38

CORNER
Cmbp
#BS/#BSH Time
2 / 4 0.000
15 / 37 0.000
35 / 93 0.000
63 / 173 0.020
99 / 277 0.030
143 / 405 0.050
195 / 557 0.070
255 / 733 0.080
323 / 933 0.120
399 / 1157 0.160

CORNER
Cmbp
#BS/#BSH Time
6 / 19 0.000
26 / 99 0.010
63 / 261 0.020
124 / 537 0.040
215 / 957 0.050
342 / 1551 0.100
511 / 2349 0.160
728 / 3381 0.330
999 / 4677 0.440
3374 / 16167 1.940

Gpt

Time
0.332
0.168
0.430
0.276
0.500
0.567
1.082
1.765
2.068
9.207

Gpt

Time
0.074
0.080
0.092
0.115
0.149
0.196
0.261
0.357
0.503
0.638

jPj

3
6
11
14
19
22
27
30
35
54

jPj

2
7
12
17
22
27
32
37
42
47

FACE
Cmbp
#BS/#BSH Time
2 / 4 0.000
33 / 83 0.000
86 / 232 0.020
163 / 453 0.040
264 / 746 0.090
389 / 1111 0.150
538 / 1548 0.230
711 / 2057 0.320
908 / 2638 0.540
1129 / 3291 0.650

Gpt

Time
0.058
0.065
0.089
0.139
0.228
0.371
0.582
0.908
1.343
1.883

jPj

2
8
14
20
26
32
38
44
50
56

CENTER
Cmbp
#BS/#BSH Time
2 / 4 0.000
76 / 190 0.010
218 / 592 0.040
432 / 1210 0.090
718 / 2044 0.190
1076 / 3094 0.360
1506 / 4360 0.560
2008 / 5842 0.820
2582 / 7540 1.330
3228 / 9454 1.790

Gpt

Time
0.060
0.083
0.216
0.695
2.135
5.340
12.284
26.241
52.091
94.204

FACE
CENTER
Cmbp
Gpt
Cmbp
Gpt
#BS/#BSH Time Time jPj #BS/#BSH Time Time
6 / 19 0.000 0.061 3
6 / 19 0.010 0.061
26 / 99 0.000 0.069 6
26 / 99 0.010 0.144
319 / 1360 0.050 0.193 12
722 / 3091 0.130 0.569
709 / 3095 0.220 0.412 15
1696 / 7402 0.430 2.010
1343 / 6116 0.430 1.479 21 3365 / 15432 0.910 10.717
2255 / 10377 0.840 3.323 24 5797 / 26814 1.860 34.074
3519 / 16464 1.400 8.161 30 9248 / 43541 3.520 109.852
5169 / 24331 2.810 16.272 33 13786 / 65237 7.260 701.910
7279 / 34564 4.550 32.226 39 19667 / 93898 9.990
===
26439 / 127825 28.560
=== 60 74041 / 359354 58.930

Table 9: Results for the SQUARE and CUBE problems.
The results for these problems are reported in Table 9. The tests were run only with
Cmbp and Gpt. The experiments highlight that the eciency of Gpt strongly depends on
the quality of the heuristic function. If, as in the first set of experiments, the heuristics are
331

fiCimatti & Roveri
effective, then Gpt is almost as good as Cmbp. Otherwise, Gpt degrades significantly. In
general, finding heuristics which are effective in the belief space appears to be a nontrivial
problem. Cmbp appears to be more stable9 , as it performs a blind, breadth-first search,
and relies on the cleverness of the symbolic representation to achieve eciency.
7.2.4 Omelette

Finally, we considered the OMELETTE(i) problem (Levesque, 1996). The goal is to have i
good eggs and no bad ones in one of two bowls of capacity i. There is an unlimited number of
eggs, each of which can be unpredictably good or bad. The eggs can be grabbed and broken
into a bowl. The content of a bowl can be discarded, or poured to the other bowl. Breaking
a rotten egg in a bowl has the effect of spoiling the bowl. A bowl can always be cleaned
by discarding its content. The problem is originally presented as a partial observability
problem, with a sensing action allowing to test if a bowl is spoiled or not. We considered
the variation of the problem without sensing action: in this case no conformant solution
exists. We used the OMELETTE problems to test the ability of Cmbp and Gpt to discover
that the problem admits no conformant solution. The results are reported in Table 10. The
table shows that Cmbp is very effective in checking the absence of a conformant solution,
and outperforms Gpt by several orders of magnitude.
OMELETTE(3)
OMELETTE(4)
OMELETTE(5)
OMELETTE(6)
OMELETTE(7)
OMELETTE(8)
OMELETTE(9)
OMELETTE(10)
OMELETTE(15)
OMELETTE(20)
OMELETTE(30)

# steps
9
11
13
15
17
19
21
23
33
43
63

CMBP
#BS/#BSH
15 / 34
19 / 42
23 / 50
27 / 58
31 / 66
35 / 74
39 / 82
43 / 90
63 / 130
83 / 170
123 / 250

Time
0.020
0.030
0.040
0.050
0.060
0.090
0.110
0.120
0.210
0.440
0.890

GPT
Time
0.237
0.582
1.418
2.904
5.189
10.307
18.744
32.623
225.530
===

Table 10: Results for the OMELETTE problems.
7.3 Summarizing Remarks

Overall, Cmbp appears to implement the most effective approach to conformant planning,
both in terms of expressivity and performance. Cgp is only able to deal with uncertainties
in the initial states, and cannot conclude that the problem does not admit a conformant
solution. The main problem in Cgp seems to be its enumerative approach to uncertainties,
and the increased number of initial states severely affects the performance (see Table 3 and
Table 6).
Qbfplan is in principle able to deal with uncertain action effects, but cannot conclude
that the problem does not admit a conformant solution. From the small number of ex9. Consider also that the problems are increasingly more dicult (see for instance the plan length).

332

fiConformant Planning via Symbolic Model Checking
periments that we could perform, the approach implemented by Qbfplan is limited by
the Satplan style of search: the intermediate results obtained while solving an encoding
at depth k are not reused while solving encodings of increasing depth. Furthermore, the
solver appears to be specialized in finding a model, rather than in proving unsatisfiability.
However, the latter ability is needed in all encodings but the final one.
Gpt is a very expressive system, which allows eciently dealing with a wide class of
planning problems. As far as conformant planning is concerned, it is as expressive as
Cmbp. It allows dealing with uncertain action effects, and can conclude that a problem
does not have a conformant solution. However, Cmbp appears to outperform Gpt in
several respects. First, the behaviour of Gpt appears to be directly related to the number
of possible outcomes in an action. Furthermore, the eciency of Gpt depends on the
effectiveness of the heuristic functions, which can be sometimes dicult to devise, and
cannot help when the problem does not admit a solution.
The main strength of Cmbp is its independence on the number of uncertainties, which
is achieved with the use of symbolic techniques. Being fully symbolic, Cmbp does not
exhibit the enumerative behaviour of its competitors. Compared to the original approach
described by Cimatti and Roveri (1999), a substantial improvement of the performance
has been obtained by the new implementation of the pruning step. A disclaimer is in
order. It is well known that Bdd based computations are subject to a blow-up in memory
requirements when computing certain classes of boolean functions, e.g. multipliers (Bryant,
1986). It would be trivial to make up an example where the performance of Cmbp degrades
exponentially. However, in none of the examples we considered, which included all the
examples in the distribution of Cgp and Gpt, this phenomenon occurred.
8. Other Related Work

The term conformant planning was first introduced by Goldman (1996), while presenting a
formalism for constructing conformant plans based on an extension of dynamic logic. Recently, Ferraris and Giunchiglia (2000) presented another conformant planner based on SAT
techniques. The system is not available for a direct comparison with Cmbp. The effectiveness of the approach is dicult to evaluate, as only a limited testing is described (Ferraris &
Giunchiglia, 2000). The performance is claimed to be comparable with Cgp. However, the
results are reported only for the enconding corresponding to the solution, and the behaviour
of Qbfplan reported in Table 2 suggests that this kind of analysis might be limited.
Several works share the idea of planning based on automata theory. The most closely
related are the works in the lines of planning via model checking (Cimatti et al., 1997), upon
which our work is based. This approach allows, for instance, to automatically construct
universal plans which are guaranteed to achieve the goal in a finite number of steps (Cimatti
et al., 1998b), or which implement trial-and-error strategies (Cimatti et al., 1998a; Daniele
et al., 1999). These results are obtained under the hypothesis of total observability, while
here run-time observation is not available. The main difference is that a substantial extension is required to lift symbolic techniques to search in the space of belief states. De
Giacomo and Vardi (1999) analyze several forms of planning in the automata theoretic
framework. Goldman, Musliner and Pelican (2000) present a method where model checking
in timed automata is interleaved with the plan formation activity, to make sure that the
333

fiCimatti & Roveri
timing constraints are met. Finally, Hoey and his colleagues (1999) use algebraic decision
diagrams to tackle the problem of stochastic planning.
9. Conclusions and Future Work

In this paper we presented a new approach to conformant planning, based on the use
of Symbolic Model Checking techniques. The algorithm is very general, and applies to
complex planning domains, with uncertainty in the initial condition and in action effects,
which can be described as finite state automata. The algorithm is based on a breadthfirst, backward search, and returns conformant plans of minimal length, if a solution to the
planning problem exists. Otherwise, it terminates with failure. The algorithm is designed
to take full advantage of the symbolic representation based on Bdds. The implementation
of the approach in the Cmbp system has been highly optimized, in particular in the crucial
step of termination checking. We performed an experimental comparison of our approach
with the state of the art conformant planners Cgp, Qbfplan and Gpt. Cmbp is strictly
more expressive than Qbfplan and Cgp. On all the problems for which a comparison
was possible, Cmbp outperformed its competitors in terms of run times, sometimes by
orders of magnitude. Thanks to the use of symbolic data structures, Cmbp is able to deal
eciently with problems with large numbers of initial states and action outcomes. On
the other hand, the qualitative behavior of Cgp and Gpt seems to depend heavily on the
enumerative nature of their algorithms. Differently from Gpt, Cmbp is independent of the
effectiveness of the heuristic used to drive the search.
The research presented in this paper will be extended in the following directions. First,
we are investigating an alternative approach to conformant planning, where the breadthfirst style of the search is given up. These techniques appear to be extremely promising |
preliminary experiments have led to speed ups of up to two orders of magnitude over the
results presented in this paper for problems which admit a solution. Second, we will tackle
the problem of conditional planning under partial observability, under the hypothesis that
a limited amount of information can be acquired at run time. As conformant planning, this
problem can be seen as search in the belief space. However, it appears to be significantly
complicated by the need for dealing with run-time observation and conditional plans. Finally, we are considering the extension of the domain construction of the planner with more
expressive input language, such as C , and invariant detection techniques.
Acknowledgements

Fausto Giunchiglia provided continuous encouragement and feedback on this work. We
thank Piergiorgio Bertoli, Blai Bonet, Marco Daniele, Hector Geffner, Enrico Giunchiglia,
Jussi Rintanen, David Smith, Paolo Traverso, Dan Weld for valuable discussions on conformant planning and various comments on this paper. David Smith provided the code of
Cgp, a large number of examples, and the time-out mechanism used in the experimental
evaluation. Jussi Rintanen made Qbfplan available under Linux.
334

fiConformant Planning via Symbolic Model Checking
References

Blum, A. L., & Furst, M. L. (1995). Fast planning through planning graph analysis. In
Proc. Ijcai.
Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artificial Intelligence 1{2, 90, 279{298.
Bonet, B., & Geffner, H. (2000). Planning with Incomplete Information as Heuristic Se
arch in Belief Space. In Chien, S., Kambhampati, S., & Knoblock, C. (Eds.), 5th
International Conference on Artificial Intelligence Planning and Scheduling, pp. 52{
61. AAAI-Press.
Brace, K., Rudell, R., & Bryant, R. (1990). Ecient Implementation of a BDD Package. In 27th ACM/IEEE Design Automation Conference, pp. 40{45 Orlando, Florida.
ACM/IEEE, IEEE Computer Society Press.
Bryant, R. E. (1986). Graph-Based Algorithms for Boolean Function Manipulation. IEEE
Transactions on Computers, C-35 (8), 677{691.
Bryant, R. E. (1991). On the complexity of VLSI implementations and graph representations
of Boolean functions with application to integer multiplication. IEEE Transactions
on Computers, 40 (2), 205{213.
Bryant, R. E. (1992). Symbolic Boolean manipulation with ordered binary-decision diagrams. ACM Computing Surveys, 24 (3), 293{318.
Burch, J. R., Clarke, E. M., McMillan, K. L., Dill, D. L., & Hwang, L. J. (1992). Symbolic
Model Checking: 1020 States and Beyond. Information and Computation, 98 (2),
142{170.
Cassandra, A., Kaelbling, L., & Littman, M. (1994). Acting optimally in partially observable
stochastic domains. In Proc. of AAAI-94. AAAI-Press.
Cimatti, A., Clarke, E., Giunchiglia, F., & Roveri, M. (2000). NuSMV : a new symbolic
model checker. International Journal on Software Tools for Technology Transfer
(STTT), 2 (4).
Cimatti, A., Giunchiglia, E., Giunchiglia, F., & Traverso, P. (1997). Planning via Model
Checking: A Decision Procedure for AR. In Steel, S., & Alami, R. (Eds.), Proceeding
of the Fourth European Conference on Planning, No. 1348 in Lecture Notes in Artificial
Intelligence, pp. 130{142 Toulouse, France. Springer-Verlag. Also ITC-IRST Technical
Report 9705-02, ITC-IRST Trento, Italy.
Cimatti, A., & Roveri, M. (1999). Conformant Planning via Model Checking. In Biundo,
S. (Ed.), Proceeding of the Fifth European Conference on Planning, Lecture Notes
in Artificial Intelligence Durham, United Kingdom. Springer-Verlag. Also ITC-IRST
Technical Report 9908-01, ITC-IRST Trento, Italy.
335

fiCimatti & Roveri
Cimatti, A., & Roveri, M. (2000). Forward Conformant Planning via Symbolic Model
Checking. In Proceeding of the AIPS2k Workshop on Model-Theoretic Approaches to
Planning Breckenridge, Colorado.
Cimatti, A., Roveri, M., & Traverso, P. (1998a). Automatic OBDD-based Generation of
Universal Plans in Non-Deterministic Domains. In Proceeding of the Fifteenth National
Conference on Artificial Intelligence (AAAI-98) Madison, Wisconsin. AAAI-Press.
Also IRST-Technical Report 9801-10, Trento, Italy.
Cimatti, A., Roveri, M., & Traverso, P. (1998b). Strong Planning in Non-Deterministic
Domains via Model Checking. In Proceeding of the Fourth International Conference
on Artificial Intelligence Planning Systems (AIPS-98) Carnegie Mellon University,
Pittsburgh, USA. AAAI-Press.
Clarke, E. M., & Wing, J. M. (1996). Formal methods: State of the art and future directions.
ACM Computing Surveys, 28 (4), 626{643.
Clarke, E., Emerson, E., & Sistla, A. (1986). Automatic verification of finite-state concurrent systems using temporal logic specifications. ACM Transactions on Programming
Languages and Systems, 8 (2), 244{263.
Coudert, O., Madre, J. C., & Touati, H. (1993). TiGeR Version 1.0 User Guide. Digital
Paris Research Lab.
Daniele, M., Traverso, P., & Vardi, M. Y. (1999). Strong Cyclic Planning Revisited. In
Biundo, S. (Ed.), Proceeding of the Fifth European Conference on Planning, Lecture
Notes in Artificial Intelligence Durham, United Kingdom. Springer-Verlag.
De Giacomo, G., & Vardi, M. (1999). Automata-Theoretic Approach to Planning for Temporally Extended Goals. In Biundo, S. (Ed.), Proceeding of the Fifth European Conference on Planning, Lecture Notes in Artificial Intelligence Durham, United Kingdom.
Springer-Verlag.
Ferraris, P., & Giunchiglia, E. (2000). Planning as satisfiability in nondeterministic domains. In Proceedings of Seventeenth National Conference on Artificial Intelligence
(AAAI'00) Austin, Texas. AAAI Press.
Ghallab, M., Howe, A., Knoblock, C., McDermott, D., Ram, A., Weld, D., & Wilkins,
D. (1998). PDDL | The Planning Domain Definition Language. Tech. rep. CVC
TR-98-003/DCS TR-1165, Yale Center for Computational Vision and Control.
Giunchiglia, E. (1996). Determining Ramifications in the Situation Calculus. In In Fifth
International Conference on Principles of Knowledge Representation and Reasoning
(KR'96) Cambridge, Massachusetts. Morgan Kaufmann Publishers.

Giunchiglia, E., Kartha, G. N., & Lifschitz, V. (1997). Representing action: Indeterminacy
and ramifications. Artificial Intelligence, 95 (2), 409{438.
336

fiConformant Planning via Symbolic Model Checking
Giunchiglia, E., & Lifschitz, V. (1998). An action language based on causal explanation:
Preliminary report. In Proceedings of the 15th National Conference on Artificial Intelligence (AAAI-98) and of the 10th Conference on Innovative Applications of Artificial
Intelligence (IAAI-98), pp. 623{630 Menlo Park. AAAI Press.

Goldman, R. P., Musliner, D. J., & Pelican, M. J. (2000). Using Model Checking to
Plan Hard Real-Time Controllers. In Proceeding of the AIPS2k Workshop on ModelTheoretic Approaches to Planning Breckenridge, Colorado.
Goldman, R., & Boddy, M. (1996). Expressive Planning and Explicit Knowledge. In
Proceedings of the 3rd International Conference on Artificial Intelligence Planning
Systems (AIPS-96), pp. 110{117. AAAI Press.

Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). Spudd: Stochastic planning using decision diagrams. In Proceedings of the Fifteenth Conference on Uncertainty in
Articial Intelligence (1999), pp. 279{288. AAAI Press.
Kautz, H., & Selman, B. (1998). BLACKBOX: A New Approach to the Application of
Theorem Proving to Problem Solving. In Working notes of the Workshop on Planning
as Combinatorial Search Pittsburgh, PA, USA.
Kautz, H. A., McAllester, D., & Selman, B. (1996). Encoding Plans in Propositional Logic.
In Proc. KR-96.
Kautz, H. A., & Selman, B. (1996). Pushing the Envelope: Planning, Propositional Logic,
and Stochastic Search. In Proc. AAAI-96.
Kushmerick, N., Hanks, S., & Weld, D. S. (1995). An algorithm for probabilistic planning.
Artificial Intelligence, 76 (1-2), 239{286.
Levesque, H. J. (1996). What is planning in the presence of sensing?. In Proceedings of the
Thirteenth National Conference on Artificial Intelligence and the Eighth Innovative
Applications of Artificial Intelligence Conference, pp. 1139{1146 Menlo Park. AAAI

Press / MIT Press.
McDermott, D. (1987). A critique of pure reason. Computational Intelligence, 3 (3), 151{
237.
McMillan, K. (1993). Symbolic Model Checking. Kluwer Academic Publ.
Michie, D. (1974). Machine Intelligence at Edinburgh. In On Machine Intelligence, pp.
143{155. Edinburgh University Press.
Nilsson, N. (1980). Principles of Artificial Intelligence. Morgan Kaufmann Publishers, Inc.,
Los Altos, CA.
Peot, M. (1998). Decision-Theoretic Planning. Ph.D. thesis, Dept. Engineering-Economic
Systems | Stanford University.
Rintanen, J. (1999a). Constructing conditional plans by a theorem-prover. Journal of
Artificial Intellegence Research, 10, 323{352.
337

fiCimatti & Roveri
Rintanen, J. (1999b). Improvements to the Evaluation of Quantified Boolean Formulae.
In Dean, T. (Ed.), 16th Iinternational Joint Conference on Artificial Intelligence, pp.
1192{1197. Morgan Kaufmann Publishers.
Smith, D. E., & Weld, D. S. (1998). Conformant graphplan. In Proceedings of the 15th
National Conference on Artificial Intelligence (AAAI-98) and of the 10th Conference
on Innovative Applications of Artificial Intelligence (IAAI-98), pp. 889{896 Menlo

Park. AAAI Press.
Somenzi, F. (1997). CUDD: CU Decision Diagram package | release 2.1.2. Department of
Electrical and Computer Engineering | University of Colorado at Boulder.
Weld, D. S., Anderson, C. R., & Smith, D. E. (1998). Extending graphplan to handle
uncertainty and sensing actions. In Proceedings of the 15th National Conference on
Artificial Intelligence (AAAI-98) and of the 10th Conference on Innovative Applications of Artificial Intelligence (IAAI-98), pp. 897{904 Menlo Park. AAAI Press.

Yang, B., Bryant, R. E., O'Hallaron, D. R., Biere, A., Coudert, O., Janssen, G., Ranjan,
R. K., & Somenzi, F. (1998). A performance study of BDD-based model checking. In
Proceedings of the Formal Methods on Computer-Aided Design, pp. 255{289.

338

fiJournal of Artificial Intelligence Research 13 (2000) 1-31

Submitted 8/99; published 8/00

Space Efficiency of Propositional Knowledge Representation
Formalisms
Marco Cadoli

cadoli@dis.uniroma1.it

Dipartimento di Informatica e Sistemistica
Universita di Roma La Sapienza
Via Salaria 113, I-00198, Roma, Italy

Francesco M. Donini

donini@dis.uniroma1.it

Politecnico di Bari
Dipartimento di di Elettrotecnica ed Elettronica
Via Orabona 4, I-70125, Bari, Italy

Paolo Liberatore
Marco Schaerf

liberato@dis.uniroma1.it
schaerf@dis.uniroma1.it

Dipartimento di Informatica e Sistemistica
Universita di Roma La Sapienza
Via Salaria 113, I-00198, Roma, Italy

Abstract
We investigate the space efficiency of a Propositional Knowledge Representation (PKR)
formalism. Intuitively, the space efficiency of a formalism F in representing a certain piece
of knowledge , is the size of the shortest formula of F that represents . In this paper we
assume that knowledge is either a set of propositional interpretations (models) or a set of
propositional formulae (theorems). We provide a formal way of talking about the relative
ability of PKR formalisms to compactly represent a set of models or a set of theorems. We
introduce two new compactness measures, the corresponding classes, and show that the
relative space efficiency of a PKR formalism in representing models/theorems is directly
related to such classes. In particular, we consider formalisms for nonmonotonic reasoning,
such as circumscription and default logic, as well as belief revision operators and the stable
model semantics for logic programs with negation. One interesting result is that formalisms
with the same time complexity do not necessarily belong to the same space efficiency class.

1. Introduction
During the last years a large number of formalisms for knowledge representation (KR) have
been proposed in the literature. Such formalisms have been studied from several perspectives, including semantical properties, and computational complexity. Here we investigate
space efficiency, a property that has to do with the minimal size needed to represent a certain piece of knowledge in a given formalism. This study is motivated by the fact that the
same piece of knowledge can be represented by two formalisms using a different amount of
space. Therefore, all else remaining the same, a formalism could be preferred over another
one because it needs less space to store information.
The definition of space efficiency, however, is not simple. Indeed, a formalism may allow
several different ways to represent the same piece of knowledge. For example, let us assume
that we want to represent the piece of knowledge today is Monday. In Propositional

c
2000
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiCadoli, Donini, Liberatore, & Schaerf

Logic we may decide to use a single propositional variable monday. The fact that today is
Monday can be represented by the formula monday, but also by the formula monday,
as well as monday  (rain  rain), because all formulae of the Propositional Logic that
are logically equivalent to monday represent exactly the same information.
In Propositional Logic, we should consider the shortest of the equivalent formulae used
to represent the information we have. The same principle can be applied to a generic
formalism: if it allows several formulae to represent the same information, then we only take
into account the shortest one. Therefore, we say that the space efficiency of a formalism F
in representing a certain piece of knowledge  is the size of the shortest formula of F that
represents . Space efficiency also called succinctness or compactness of a formalism
is a measure of its ability in representing knowledge in a small amount of space.
In this paper we focus on propositional KR (PKR) formalisms. We do not give a
formal definition of which formalisms are propositional and which one are not: intuitively,
in a propositional formalism, quantifications are not allowed, and thus the formulae are
syntactically bounded to be formed only using propositional connectives, plus some other
kind of nonclassical connectives (for instance, negation in logic programs, etc.).
So far, we have not discussed what knowledge represents. A possible way to think of a
piece of knowledge is that it represents all facts that can be inferred from it. In other words,
knowing something is the same as knowing everything that can be logically implied. The
second way  which is in some cases more natural  is to think of a piece of knowledge
as the set of states of the world that we consider possible.
In a more formal way, we say that knowledge is represented either by a set of propositional interpretations (those describing states of the world we consider plausible) or a set
of formulae (those implied from what we know). Consequently, we focus on both reasoning
problems of model checking and theorem proving. The following example shows that we
can really think of knowledge in both ways.
Example 1 We want to eat in a fast food, and want to have either a sandwich or a salad
(but not both), and either water or coke (but not both).
In Propositional Logic, each choice can be represented as a model, and the following
models represent all possible choices (models are represented by writing down only the letters
mapped to true).
A = {{sandwich, water}, {sandwich, coke}, {salad, water}, {salad, coke}}
For representing the set of choices we can use formulae instead of models. In this case,
we write down a set of formulae whose models represent exactly the allowed choices, as
follows.
C = (sandwich  salad)  (sandwich  salad)  (sandwich  salad) 
(water  coke)  (water  coke)  (coke  water)

Actually, we can get rid of redundancies, and end up with the following formula.
F = (sandwich  salad)  (sandwich  salad)  (water  coke)  (water  coke)
2

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

More formally, F represents the set of models A, because for each interpretation I, I  A
holds if and only if I |= F . The formula F also represents the set of formulae C, because
Cn(F ) = Cn(C), where Cn(.) is the function that gives the set of all conclusions that can
be drawn from a propositional formula.
1.1 State of the Art
A question that has been deeply investigated, and is related to space efficiency, is the
possibility of translating a formula expressed in one formalism into a formula expressed in
another formalism (under the assumption, of course, that these formulae represent the same
knowledge).
In most cases, the analysis is about the possibility of translating formulae from different
formalisms to Propositional Logic (PL). For example, Ben-Eliyahu and Dechter (1991, 1994)
proposed a translation from default logic to PL, and a translation from disjunctive logic
programs to PL, while Winslett (1989) introduced a translation from revised knowledge
bases to PL, and Gelfond, Przymusinska, and Przymusinskyi (1989) defined a translation
from circumscription to PL.
All the above translations, as well as many other ones in the literature, lead to an
exponential increase of the size of the formula, in the worst case. When the best known
translation yields a formula in the target formalism which has exponential size w.r.t. the
formula in the source formalism, a natural question arising is whether such exponential
blow up is due to the specific translation, or is intrinsic of the problem. For example,
although all proposed translations from default logic to PL lead to the exponential blow
up, we cannot conclude that all possible translations suffer from this problem: it could be
that a polynomial translation exists, but it has not discovered so far.
Some works have focussed on the question of whether this kind of exponential increase
in the size is intrinsic or not. Cadoli, Donini, and Schaerf (1996) have shown that many interesting fragments of default logic and circumscription cannot be expressed by polynomialtime fragments of PL without super-polynomially increasing the size of formulae. It has
been proved that such a super-polynomial increase of size is necessary when translating
unrestricted propositional circumscription (Cadoli, Donini, Schaerf, & Silvestri, 1997) and
most operators for belief revision into PL (Cadoli, Donini, Liberatore, & Schaerf, 1999;
Liberatore, 1995).
Gogic and collegues (1995) analyzed the relative succinctness of several PKR formalisms
in representing sets of models. Among other results, they showed that skeptical default logic
can represent sets of models more succinctly than circumscription.
Kautz, Kearns, and Selman (1995) and Khardon and Roth (1996, 1997) considered
representations of knowledge bases based on the notion of characteristic model, comparing
them to other representations, e.g., based on clauses. They showed that the representation of
knowledge bases with their characteristic models is sometimes exponentially more compact
than other ones, and that the converse is true in other cases.
However, all the above results are based on specific proofs, tailored to a specific reduction, and do not help us to define equivalence classes for the space efficiency of KR
formalisms. In a recent paper (Cadoli, Donini, Liberatore, & Schaerf, 1996b), a new complexity measure for decision problems, called compilability, has been introduced. In the

3

fiCadoli, Donini, Liberatore, & Schaerf

present paper we show how this new measure can be directly used to characterize the space
efficiency of PKR formalisms. We emphasize methodological aspects, expressing in a more
general context many of the results presented before.
1.2 Goal
The notion of polynomial time complexity has a great importance in KR (as well as many
other fields of computer science), as problems that can be solved in polynomial time are to
be considered easy, from a computational point of view.
The notion of polynomial many-one reducibility also has a very intuitive meaning when
applied to KR: if there exists a polynomial many-one reduction from one formalism to
another one, then the time complexity of reasoning in the two formalisms is comparable.
This allows to say, e.g., that inference in PL is coNP-complete, i.e. it is one of the hardest
problems among those in the complexity class coNP.
As a result, we have a formal tool for comparing the difficulty of reasoning in two
formalisms. What is missing is a way for saying that one formalism is able to represent the
same information in less space.
Example 2 We consider again the lunch scenario of the previous example. We show that
we can reduce the size of the representation using circumscription instead of Propositional
Logic. In PL, the knowledge of the previous example was represented by the formula F :
F = (sandwich  salad)  (sandwich  salad)  (water  coke)  (water  coke)
The set of models of this formula is A, and the models of A are exactly the minimal
models of the formula Fc defined as follows.
Fc = (sandwich  salad)  (water  coke)
By the definition of circumscription (McCarthy, 1980) it holds that F is equivalent to
CIRC(Fc ; {sandwich, salad, water, coke}, , ). Note that Fc is shorter than F . If this result
can be proved to hold for arbitrary sets of models, we may conclude that circumscription is
more space efficient than Propositional Logic in representing knowledge expressed as sets of
models.
Our goal is to provide a formal way of talking about the relative ability of PKR formalisms to compactly represent information, where the information is either a set of models
or a set of theorems. In particular, we would like to be able to say that a specific PKR
formalism provides one of the most compact ways to represent models/theorems among
the PKR formalisms of a specific class.
1.3 Results
We introduce two new compactness measures (model and theorem compactness) and the
corresponding classes (model-C and thm-C, where C is a complexity class like P, NP, coNP,
etc.). Such classes form two hierarchies that are isomorphic to the polynomial-time hierarchy
(Stockmeyer, 1976). We show that the relative space efficiency of a PKR formalism is
4

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

directly related to such classes. In particular, the ability of a PKR formalism to compactly
represent sets of models/theorems is directly related to the class of the model/theorem
hierarchy it belongs to. Problems higher up in the model/theorem hierarchy can represent
sets of models/theorems more compactly than formalisms that are in lower classes.
This classification is obtained through a general framework and not by making direct
comparisons and specific translations between the various PKR formalisms. Furthermore,
our approach also allows for a simple and intuitive notion of completeness for both model
and theorem hierarchies. This notion precisely characterizes both the relation between
formalisms at different levels, and the relations between formalisms at the same level. An
interesting result is that two PKR formalisms in which model checking or inference belong
to the same time complexity class may belong to different compactness classes. This may
suggest a criterion for choosing between two PKR formalisms in which reasoning has the
same time complexitynamely, choose the more compact one. Also, two PKR formalisms
may belong to the same theorem compactness class, yet to different model compactness
classes. This stresses the importance of clarifying whether one wants to represent models
or theorems when choosing a PKR formalism.
1.4 Outline
In the next section we introduce the notation and the assumptions that we adopt in this
work. In Section 3 (Compilability) we briefly recall some notions on non-uniform computation that are important for what follows and we recall the basic definitions of compilability
classes (Cadoli et al., 1996b). In Section 4 (Reductions) we describe the constraints we
impose on reductions, while in Section 5 (Space Efficiency) we introduce our compactness
classes. In Section 6 (Applications) we actually compare many known PKR formalisms
using our framework. Finally, in Section 7 (Related Work and Conclusions) we compare
our work with other proposals presented in the literature and draw some conclusions.

2. Notations and Assumptions
In this section we define what knowledge bases and formalisms are. Since we want to
consider formalisms that are very different both in syntax and in semantics, we need very
general definitions. Let us consider, as a base case, the formalism of propositional calculus.
Formally, we can assume that it is composed of three parts:
1. a syntax, which is used to define the well-formed formulae;
2. a proof theory, which allows for saying when a formula follows from another one; and
3. a model-theoretic semantics, which establishes when a model satisfies a formula.
The syntax is defined from a finite alphabet of propositional symbols L = {a, b, c, . . .},
possibly with subscripts, and the usual set of propositional connectives , , .
In terms of knowledge representation, the proof theory can be seen as a way for extracting knowledge from a knowledge base. For example, if our knowledge base is a  c, then the
fact a  b holds. We can thus say that the formula a  b is part of the knowledge represented
by a  c.
5

fiCadoli, Donini, Liberatore, & Schaerf

In some cases, we want knowledge bases to represent models rather than sets of formulas.
An interpretation for an alphabet of propositional variables L is a mapping from L in
{true, false}. The model-theoretic semantics of the propositional calculus is the usual way
of extending an interpretation for L to well-formed formulas.
Let us now extend such definition to generic formalisms: a formalism is composed of a
syntax, a proof theory, and a model-theoretic semantics.
We remark that each formalism has its own syntax: for instance, default logic includes
a ternary connective : for denoting default rules, while logic programming has a special
unary connective not(), and so on. A knowledge base of a formalism F is simply a wellformed formula, according to the syntax of the formalism.
Each formalism has its own proof theory as well. The proof theory of a formalism F is a
binary relation `F on the set of knowledge bases and formulae. Intuitively, F B `F  means
that  is a consequence of the knowledge base KB, according to the rules of the formalism
F . As a result, the set of formulae  that are implied by a knowledge base KB is exactly
the knowledge represented by KB.
The base of a comparison between two different formalisms is a concept of equivalence,
allowing for saying that two knowledge bases (of two different formalisms) represent the
same piece of knowledge. Since the knowledge represented by a knowledge base is the set
of formulas it implies, we have to assume that the syntax of these formulae is the same
for all formalisms. Namely, we always assume that the formulae implied by a knowledge
base are well-formed formulae of the propositional calculus. In other words, each formalism
has a syntax for the knowledge bases: however, we assume that the proof theory relates
knowledge bases (formulae in the syntax of the formalism) with propositional formulae. So,
while writing KB `F , we assume that KB is a knowledge base in the syntax of F , while
 is a propositional formula.
This allows for saying that two knowledge bases KB1 and KB2 , expressed in two different formalisms F1 and F2 , represent the same piece of knowledge: this is true when, for
any propositional formula  it holds KB1 `F1  if and only if KB2 `F2 .
The model-theoreric semantics of a formalism is a relation |=F between propositional
models and knowledge bases. In this case, we assume a fixed alphabet L, thus the set of
all interpretations is common to all formalisms. When a model M and a knowledge base
KB are in the relation, we write M |=F KB. Intuitively, this means that the model M
supports the piece of knowledge represented by KB.
We remark that some formalisms, e.g. credolous default logic (Reiter, 1980), have a
proof theory, but do not have a model-theoretic semantics. It is also possible to conceive
formalisms with a model-theoretic semantics but no proof theory. When both of them are
defined, we assume that they are related by the following formula:
KB `F 

iff

I . I |= KB implies I |= 

Regarding the proof theory of formalisms, we only consider formulae that are shorter
than the knowledge base, that is, we assume that the knowledge represented by a knowlegde
base KB is the set of formulae  such that KB `F , and the size of  is at most the size
of KB. This is done for two reasons: first, formulas that are larger than KB are likely to

6

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

contain large parts that are actually independent from KB; second, we can give technicals
result in a very simple way by using the compilability classes introduced in the next section.
Assumption 1 We consider only formulae whose size is less than or equal to that of the
knowledge base.
All formalisms we consider satisfy the right-hand side distruibutivity of conjunction,
that is, KB `F    if and only if KB `F  and KB `F . The assumption on the size
of  is not restrictive in this case, if  is a CNF formula.

3. Compilability Classes
We assume the reader is familiar with basic complexity classes, such as P, NP and (uniform)
classes of the polynomial hierarchy (Stockmeyer, 1976; Garey & Johnson, 1979). Here we
just briefly introduce non-uniform classes (Johnson, 1990). In the sequel, C, C0 , etc. denote
arbitrary classes of the polynomial hierarchy.
We assume that the input instances of problems are strings built over an alphabet .
We denote with  the empty string and assume that the alphabet  contains a special
symbol # to denote blanks. The length of a string x   is denoted by |x|.
Definition 1 An advice A is a function that takes an integer and returns a string.
Advices are important in complexity theory because definitions and results are often
based on special Turing machines that can determine the result of an oracle for free, that
is, in constant time.
Definition 2 An advice-taking Turing machine is a Turing machine enhanced with the
possibility to determine A(|x|) in constant time, where x is the input string.
Of course, the fact that A(|x|) can be determined in constant time (while A can be
an intractable or even undecidable function) makes all definitions based on advice-taking
Turing machine different from the same ones based on regular Turing machine. For example,
an advice-taking Turing machine can calculate in polynomial time many functions that a
regular Turing machine cannot (including some untractable ones).
Note that the advice is only a function of the size of the input, not of the input itself.
Hence, advice-taking Turing machines are closely related to non-uniform families of circuits
(Boppana & Sipser, 1990). Clearly, if the advice were allowed to access the whole instance,
it would be able to determine the solution of any problem in constant time.
Definition 3 An advice-taking Turing machine uses polynomial advice if there exists a
polynomial p such that the advice oracle A satisfies |A(n)|  p(n) for any nonnegative
integers n.
The non-uniform complexity classes are based on advice-taking Turing machines. In
this paper we consider a simplified definition, based on classes of the polynomial hierarchy.

7

fiCadoli, Donini, Liberatore, & Schaerf

Definition 4 If C is a class of the polynomial hierarchy, then C/poly is the class of languages defined by Turing machines with the same time bounds as C, augmented by polynomial advice.
Any class C/poly is also known as non-uniform C, where non-uniformity is due to the
presence of the advice. Non-uniform and uniform complexity classes are related: Karp and
Lipton (1980) proved that if NP  P/poly then p2 = p2 = PH, i.e., the polynomial hierarchy collapses at the second level, while Yap (1983) generalized their results, in particular
by showing that if NP  coNP/poly then p3 = p3 = PH, i.e., the polynomial hierarchy
collapses at the third level. An inprovement of this results has been given by Kobler and
Watanabe (1998): they proved that kp  pk /poly implies that the polynomial hierarchy collapses to ZPP(pk+1 ). The collapse of the polynomial hierarchy is considered very
unlikely by most researchers in structural complexity.
We now summarize some definitions and results proposed to formalize the compilability
of problems (Cadoli et al., 1996b), adapting them to the context and terminology of PKR
formalisms. We remark that it is not the aim of this paper to give a formalization of
compilability of problems, or to analyze problems from this point of view. Rather, we show
how to use the compilability classes as a technical tool for proving results on the relative
efficiency of formalisms in representing knowledge in little space.
Several papers in the literature focus on the problem of reducing the complexity of
problems via a preprocessing phase (Kautz & Selman, 1992; Kautz et al., 1995; Khardon
& Roth, 1997). This motivates the introduction of a measure of complexity of problems
assuming that such preprocessing is allowed. Following the intuition that a knowledge base
is known well before questions are posed to it, we divide a reasoning problem into two parts:
one part is fixed or accessible off-line (the knowledge base), and the second one is varying, or
accessible on-line (the interpretation/formula). Compilability aims at capturing the on-line
complexity of solving a problem composed of such inputs, i.e., complexity with respect to
the second input when the first one can be preprocessed in an arbitrary way. In the next
section we show the close connection between compilability and the space efficiency of PKR
formalisms.
A function f is called poly-size if there exists a polynomial p such that for all strings
x it holds |f (x)|  p(|x|). An exception to this definition is when x represents a number:
in this case, we impose |f (x)|  p(x). As a result, we can say that the function A used in
advice-taking turing machine is a polysize function.
A function g is called poly-time if there exists a polynomial q such that for all x, g(x)
can be computed in time less than or equal to q(|x|). These definitions easily extend to
binary functions as usual.
We define a language of pairs S as a subset of    . This is necessary to represent
the two inputs to a PKR reasoning problem, i.e., the knowledge base (KB), and the formula
or interpretation. As an example, the problem of Inference in Propositional Logic (pli) is
defined as follows.
pli = {hx, yi | x is a set of propositional formulae (the KB), y is a formula, and x ` y}
It is well known that pli is coNP-complete, i.e., it is one of the hardest problems
among those belonging to coNP. Our goal is to prove that pli is the hardest theorem8

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

proving problem among those in coNP that can be solved by preprocessing the first input
in an arbitrary way, i.e., the KB. To this end, we introduce a new hierarchy of classes, the
non-uniform compilability classes, denoted as k;C, where C is a generic uniform complexity
class, such as P, NP, coNP, or p2 .
Definition 5 (k;C classes) A language of pairs S     belongs to k;C iff there
exists a binary poly-size function f and a language of pairs S 0  C such that for all hx, yi  S
it holds:
hf (x, |y|), yi  S 0 iff hx, yi  S
Notice that the poly-size function f takes as input both x (the KB) and the size of y
(either the formula or the interpretation). This is done for technical reason, that is, such
assumption allows obtaining results that are impossible to prove if the function f only
takes x as input (Cadoli et al., 1996b). Such assuption is useful for proving negative results,
that is, theorems of impossibility of compilation: indeed, if it is impossible to reduce the
complexity of a problem using a function that takes both x and |y| as input, then such
reduction is also impossible using a function taking x only as its argument.
Theorem 1 (Cadoli, Donini, Liberatore, & Schaerf, 1997, Theorem 6) Let C be
a class in the polynomial hierarchy and S     . A problem S belongs to k;C if
and only if there exists a poly-size function f and a language of pairs S 0 , such that for all
hx, yi     it holds that:
1. for all y such that |y|  k, hf (x, k), yi  S 0 if and only if hx, yi  S;
2. S 0  C.
Clearly, any problem whose time complexity is in C is also in k;C: just take f (x, |y|) = x
and S 0 = S. What is interesting is that some problem in C may belong to k;C0 with
C0  C, e.g.,, some problems in NP are in k;P. This is true for example for some problems
in belief revision (Cadoli et al., 1999). In the rest of this paper, however, we mainly focus
on complete problems, defined below. A pictorial representation of the class k;C is in
Figure 1, where we assume that S 0  C.
For the problem pli no method proving that it belongs to k;P is known. In order
to show that it (probably) does not belong to k;P, we define a notion of reduction and
completeness.
Definition 6 (Non-uniform comp-reducibility) Given two problems A and B, A is
non-uniformly comp-reducible to B (denoted as A nucomp B) iff there exist two poly-size
binary functions f1 and f2 , and a polynomial-time binary function g such that for every
pair hx, yi it holds that hx, yi  A if and only if hf1 (x, |y|), g(f2 (x, |y|), y)i  B.
The nucomp reductions can be represented as depicted in Figure 2. Such reductions
satisfy all important properties of a reduction.
Theorem 2 (Cadoli et al., 1996b, Theorem 5) The reductions nucomp satisfy transitivity and are compatible (Johnson, 1990) with the class k;C for every complexity class C.
9

fiCadoli, Donini, Liberatore, & Schaerf

f
!
!1

x

| | ! |y|
y

6

hx, yi  S
-

S0

-

Figure 1: A representation of k;C.
x
|y|
||
y

- f1
- f2

6

-x

?

g
-

-y

0

0

Figure 2: The nu-comp-C reductions.

Therefore, it is possible to define the notions of hardness and completeness for k;C for
every complexity class C.
Definition 7 (k;C-completeness) Let S be a language of pairs and C a complexity class.
S is k;C-hard iff for all problems A  k;C we have that A nucomp S. Moreover, S is
k;C-complete if S is in k;C and is k;C-hard.
We now have the right complexity class to completely characterize the problem pli. In
fact pli is k;coNP-complete (Cadoli et al., 1996b, Theorem 7). Furthermore, the hierarchy
formed by the compilability classes is proper if and only if the polynomial hierarchy is
proper (Cadoli et al., 1996b; Karp & Lipton, 1980; Yap, 1983)  a fact widely conjectured
to be true.
Informally, we may say that k;NP-hard problems are not compilable to P, as from
the above considerations we know that if there exists a preprocessing of their fixed part that
makes them on-line solvable in polynomial time, then the polynomial hierarchy collapses.
The same holds for k;coNP-hard problems. In general, a problem which is k;C-complete
for a class C can be regarded as the toughest problem in C, even after arbitrary preprocessing of the fixed part. On the other hand, a problem in k;C is a problem that, after
preprocessing of the fixed part, becomes a problem in C (i.e., it is compilable to C).
We close the section by giving another example of use of the compilability classes through
the well-known formalism of Circumscription (McCarthy, 1980). Let x be any propositional
formula. The minimal models of x are the truth assignments satisfying x having as few
positive values as possible (w.r.t. set containment). The problem we consider is: check
whether a given model is a minimal model of a propositional formula. This problem, called
Minimal Model checking (mmc), can be reformulated as the problem of model checking in
Circumscription, which is known to be co-NP-complete (Cadoli, 1992).
10

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

If we consider the knowledge base x as given off-line, and the truth assignment y as
given on-line, we obtain the following definition:
mmc = {hx, yi | y is a minimal model of x }
This problem can be shown to be k;coNP-complete (Cadoli et al., 1996b, Theorem 13).
Hence, it is very unlikely that it can be in k;P; that is, it is very unlikely that there exists
some off-line processing of the knowledge base, yielding (say) some data structure x0 , such
that given y, it can now checked in polynomial time whether y is a minimal model of x.
This, of course, unless x0 has exponential size. This observation applies also when x0 is a
knowledge base in Propositional Logic, and led to the interpretation that Circumscription
is more compact, or succint, than PL (Cadoli, Donini, & Schaerf, 1995; Gogic et al., 1995).
Our framework allows to generalize these results for all PKR formalisms, as shown in the
sequel.

4. Reductions among KR Formalisms
We now define the forms of reduction between PKR formalisms that we analyze in the
following sections. A formula can always be represented as a string over an alphabet ,
hence from now on we consider translations as functions transforming strings.
Let F1 and F2 be two PKR formalisms. There exists a poly-size reduction from F1 to
F2 , denoted as f : F1 7 F2 , if f is a poly-size function such that for any given knowledge
base KB in F1 , f (KB) is a knowledge base in F2 . Clearly, reductions should be restricted
to produce a meaningful output. In particular, we now discuss reductions that preserve the
models of the original theory.
The semantic approach by Gogic and collegues (1995) is that the models of the two
knowledge bases must be exactly the same. In other words, if a knowledge base KB of the
formalism F1 is translated into a knowledge base KB 0 of the formalism F2 , then M |=F1 KB
if and only if M |=F2 KB 0 . This approach can be summarized by: a reduction between
formalisms F1 and F2 is a way to translate knowledge bases of F1 into knowledge bases of F2 ,
preserving their sets of models. While this semantics is intuitively grounded, it is very easy
to show examples in which two formalisms that we consider equally space-efficient cannot be
translated to each other. Let us consider for instance a variant of the propositional calculus
in which the syntax is that formulas must be of the form x1  F , where F is a regular
formula over the variables x2 , . . .. Clearly, this formalism is able to represent knowledge
in the same space than the propositional calculus (apart a polynomial factor). However,
according to the definition, this formalism cannot be translated to propositional calculus:
there is no knowledge base that is equivalent to KB = x1 . Indeed, the only model of
KB is , while any model of any consistent knowledge base of the modified propositional
calculus contains x1 .
We propose a more general approach that can deal also with functions f that change
the language of the KB. To this end, we allow for a translation gKB from models of KB to
models of f (KB). We stress that, to be as general as possible, the translation may depend
on KB  i.e., different knowledge bases may have different translations of their models.
We want this translation easy to compute, since otherwise the computation of gKB could
hide the complexity of reasoning in the formalism. However, observe that to this end, it is
11

fiCadoli, Donini, Liberatore, & Schaerf

not sufficient to impose that gKB is computable in polynomial time. In fact, once KB is
fixed, its models could be trivially translated to models of f (KB) in constant time, using
a lookup table. This table would be exponentially large, though; and this is what we want
to forbid. Hence, we impose that gKB is a circuit of polynomial-size wrt KB. We still use
a functional notation gKB (M ) to denote the result of applying a model M to the circuit
gKB . A formal definition follows.
Definition 8 (Model Preservation) A poly-size reduction f : F1 7 F2 satisfies modelpreservation if there exists a polynomial p such that, for each knowledge base KB in F1 there
exists a circuit gKB whose size is bounded by p(|KB|), and such that for every interpretation
M of the variables of KB it holds that M |=F1 KB iff gKB (M ) |=F2 f (KB).
The rationale of model-preserving reduction is that the knowledge base KB of the first
formalism F1 can be converted into a knowledge base f (KB) in the second one F2 , and this
reduction should be such that each model M in F1 can be easily translated into a model
gKB (M ) in F2 .
We require g to depend on KB, because the transformation f , in general, could take
the actual form of KB into account. This happens in the following example of a modelpreserving translation.
Example 3 We reduce a fragment of skeptical default logic (Kautz & Selman, 1991) to
circumscription with varying letters, using the transformation introduced by Etherington
(1987). Let hD, W i be a prerequisite-free normal (PFN) default theory, i.e., all defaults
are of the form : , where  is a generic formula. Let Z be the set of letters occurring in
hD, W i. Define PD as the set of letters {a | :  D}. The function f can be defined in
the following way: f (hD, W i) = CIRC(T ; PD ; Z), where T = W  {a  |a  PD },
PD are the letters to be minimized, and Z (the set of letters occurring in hD, W i) are
varying letters. We show that f is a model-preserving poly-size reduction. In fact, given
a set of PFN defaults D let gD be a function such that for each interpretation M for Z,
gD (M ) = M  {a  PD |M |= }. Clearly, f is poly-size, gD can be realized by a circuit
whose size is polynomial in |D|, and M is a model of at least one extension of hD, W i iff
gD (M ) |= CIRC(T ; PD ; Z). The dependence of g only on D stresses the fact that, in this
case, the circuit g does not depend on the whole knowledge base hD, W i, but just on D.
Clearly, when models are preserved, theorems are preserved as well. A weaker form of
reduction is the following one, where only theorems are preserved. Also in this case we
allow theorems of KB to be translated by a simple circuit gKB to theorems of KB.
Definition 9 (Theorem Preservation) A poly-size reduction f : F1 7 F2 satisfies theorempreservation if there exists a polynomial p such that, for each knowledge base KB in F1 ,
there exists a circuit gKB whose size is bounded by p(|KB|), and such that for every formula
 on the variables of KB, it holds that KB `F1  iff f (KB) `F2 gKB ().
The theorem-preserving reduction has a property similar to that of the model-preserving
reduction, when the knowledge bases are used to represent theorems rather than models.
Namely, a knowledge base KB is translated into another knowledge base f (KB) which can
12

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

be used to represent the same set of theorems. More precisely, we have that each theorem
 of KB is represented by a theorem gKB () of f (KB).
Winslett (1989) has shown an example of a reduction from updated knowledge bases to
PL that is theorem-preserving but not model-preserving. Using Winsletts reduction, one
could use the same machinery for propositional reasoning in the KB, both before and after
the update (plus the reduction). Also the reduction shown in the previous Example 3 is
theorem-preserving, this time g being the identity circuit.
We remark that our definitions of reduction are more general than those proposed by
Gogic and collegues (1995). In fact, these authors consider only a notion analogous to
Definition 8. and only for the case when g is the identity  i.e., models in the two formalisms
should be identical. By allowing a simple translation g between models Definition 8 covers
more general forms of reductions preserving models, like the one of Example 3.

5. Comparing the Space Efficiency of PKR Formalisms
In this section we show how to use the compilability classes defined in Section 3 to compare
the succinctness of PKR formalisms.
Let F1 and F2 be two formalisms representing sets of models. We prove that any
knowledge base  in F1 can be reduced, via a poly-size reduction, to a knowledge base 
in F2 satisfying model-preservation if and only if the compilability class of the problem of
model checking (first input: KB, second input: interpretation) in F2 is higher than or equal
to the compilability class of the problem of model checking in F1 .
Similarly, we prove that theorem-preserving poly-size reductions exist if and only if the
compilability class of the problem of inference (first input: KB, second input: formula, cf.
definition of the problem pli) in F1 is higher than or equal to the compilability class of the
problem of inference in F2 .
In order to simplify the presentation and proof of the theorems we introduce some
definitions.
Definition 10 (Model hardness/completeness) Let F be a PKR formalism and C be
a complexity class. If the problem of model checking for F belongs to the compilability class
k;C, where the model is the varying part of the instances, we say that F is in model-C.
Similarly, if model checking is k;C-complete (hard), we say that F is model-C-complete
(hard).
Definition 11 (Theorem hardness/completeness) Let F be a PKR formalism and
C be a complexity class. If the problem of inference for the formalism F belongs to the
compilability class k;C, whenever the formula is the varying part of the instance, we say
that F is in thm-C. Similarly, if inference is k;C-complete (hard), we say that F is thmC-complete (hard).
These definitions implicitly define two hierarchies, which parallel the polynomial hierarchy (Stockmeyer, 1976): the model hierarchy (model-P,model-NP,model-p2 ,etc.) and the
theorem hierarchy (thm-P,thm-NP,thm-2p ,etc.). The higher a formalism is in the model
hierarchy, the more its efficiency in representing models is  and analogously for theorems.
As an example (Cadoli et al., 1996, Thm. 6), we characterize model and theorem classes of
Propositional Logic.
13

fiCadoli, Donini, Liberatore, & Schaerf

Theorem 3 PL is in model-P and it is thm-coNP-complete.
We can now formally establish the connection between succinctness of representations
and compilability classes. In the following theorems, the complexity classes C, C1 , C2 belong
to the polynomial hierarchy (Stockmeyer, 1976). In Theorems 5 and 7 we assume that the
polynomial hierarchy does not collapse.
We start by showing that the existence of model-preserving reductions from a formalism
to another one can be easily obtained if their levels in the model hierarchy satisfy a simple
condition.
Theorem 4 Let F1 and F2 be two PKR formalisms. If F1 is in model-C and F2 is modelC-hard, then there exists a poly-size reduction f : F1 7 F2 satisfying model preservation.
Proof. Recall that since F1 is in model-C, model checking in F1 is in k;C, and since F2 is
model-C-hard, model checking in F1 is non-uniformly comp-reducible to model checking in
F2 . That is, (adapting Def. 6) there exist two poly-size binary functions f1 and f2 , and a
polynomial-time binary function g such that for every pair hKB, M i it holds that
M |=F1 KB if and only if g(f2 (KB, |M |), M ) |=F2 f1 (KB, |M |)
(note that g is the poly-time function appearing in Def. 6, different from gKB which is the
poly-size circuit appearing in Def. 8).
Now observe that |M | can be computed from KB by simply counting the letters appearing in KB; let f3 be such a counting function, i.e., |M | = f3 (KB). Clearly, f3 is poly-size.
Define the reduction f as f (KB) = f1 (KB, f3 (KB)). Since poly-size functions are closed
under composition, f is poly-size. Now we show that f is a model-preserving reduction. By
Definition 8, we need to prove that there exists a polynomial p such that for each knowledge
base KB in F1 , there exists a poly-size circuit gKB such that for every interpretation M of
the variables of KB it holds that M |=F1 KB iff gKB (M ) |=F2 f (KB).
We proceed as follows: Given a KB in F1 , we compute z = f2 (KB, |M |) = f2 (KB, f3 (KB)).
Since f2 and f3 are poly-size, z has size polynomial with respect to |KB|. Define the circuit
gKB (M ) as the one computing g(z, M ) = g(f2 (KB, f3 (KB)), M ). Since g is a poly-time
function over both inputs, and z is poly-size in KB, there exists a representation of g(z, M )
as a circuit gKB whose size is polynomial wrt KB. From this construction, M |=F1 KB iff
gKB (M ) |=F2 f (KB). Hence, the thesis follows.
The following theorem, instead, gives a simple method to prove that there is no modelpreserving reduction from one formalism to another one.

Theorem 5 Let F1 and F2 be two PKR formalisms. If the polynomial hierarchy does not
collapse, F1 is model-C1 -hard, F2 is in model-C2 , and C2  C1 , then there is no poly-size
reduction f : F1 7 F2 satisfying model preservation.
Proof. We show that if such a reduction exists, then C1 /poly  C2 /poly which implies that
the polynomial hierarchy collapses at some level (Yap, 1983). Let A be a complete problem
for class C1  e.g., if C1 is p3 then A may be validity of -quantified boolean formulae
(Stockmeyer, 1976). Define the problem A as follows.
A = {hx, yi | x =  (the empty string) and y  A}
14

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

We already proved (Cadoli et al., 1996b, Thm. 6) that A is k;C1 -complete. Since model
checking in F1 is model-C1 -hard, A is non-uniformly comp-reducible to model checking in
F1 . That is, (adapting Def. 6) there exist two poly-size binary functions f1 and f2 , and
a polynomial-time binary function g such that for every pair h, yi, it holds h, yi  A if
and only if g(f2 (, |y|), y) |=F1 f1 (, |y|). Let |y| = n. Clearly, the knowledge base f1 (, |y|)
depends only on n, i.e., there is exactly one knowledge base for each integer. Call it KBn .
Moreover, f2 (, |y|) = f2 (, n) also depends on n only: call it On (for Oracle). Observe that
both KBn and On have polynomial size with respect to n.
If there exists a poly-size reduction f : F1 7 F2 satisfying model preservation, then given
the knowledge base KBn there exists a poly-size circuit hn such that g(On , y) |=F1 KBn if
and only if hn (g(On , y)) |=F2 f (KBn ).
Therefore, the k;C1 -complete problem A can be non-uniformly reduced to a problem
in k;C2 as follows: Given y, from its size |y| = n one obtains (with a preprocessing)
f (KBn ) and On . Then one checks whether the interpretation hn (g(On , y)) (computable in
polynomial time given n, y and On ) is a model in F2 for f (KBn ). From the fact that model
checking in F2 is in k;C2 , we have that k;C1  k;C2 . We proved in a previous paper
that such result implies that C1 /poly  C2 /poly (Cadoli et al., 1996b, Thm. 9), which in
turns implies that the polynomial hierarchy collapses (Yap, 1983).
The above theorems show that the hierarchy of classes model-C exactly characterizes
the space efficiency of a formalism in representing sets of models. In fact, two formalisms
at the same level in the model hierarchy can be reduced into each other via a poly-size
reduction (Theorem 4), while there is no poly-size reduction from a formalism (F1 ) higher
up in the hierarchy into one (F2 ) in a lower class (Theorem 5). In the latter case we say
that F1 is more space-efficient than F2 .
Analogous results (with similar proofs) hold for poly-size reductions preserving theorems.
Namely, the next theorem shows how to infer the existence of theorem-preserving reductions,
while the other one gives a way to prove that there is no theorem-preserving reduction from
one formalism to another one.
Theorem 6 Let F1 and F2 be two PKR formalisms. If F1 is in thm-C and F2 is thm-Chard, then there exists a poly-size reduction f : F1 7 F2 satisfying theorem preservation.
Proof. Recall that since F1 is in thm-C, inference in F1 is in k;C, and since F2 is thm-Chard, inference in F1 is non-uniformly comp-reducible to inference in F2 . That is, (adapting
Def. 6) there exist two poly-size binary functions f1 and f2 , and a polynomial-time binary
function g1 such that for every pair hKB, i it holds that
KB `F1  if and only if f1 (KB, ||) `F2 g(f2 (KB, ||), )
(here we distinguish the poly-time function g appearing in Def. 6 and the poly-size circuit
gKB appearing in Def. 9).
Using Theorem 1 we can replace || with an upper bound in the above formula. From
Assumption 1, we know that the size of  is less than or equal to the size of KB; therefore
we replace || with |KB|. The above formula now becomes
KB `F1  if and only if f1 (KB, |KB|) `F2 g(f2 (KB, |KB|), )
15

fiCadoli, Donini, Liberatore, & Schaerf

Define the reduction f as f (KB) = f1 (KB, f3 (KB)), where f3 is the poly-size function
that computes the size of its input. Since poly-size functions are closed under composition,
f is poly-size.
Now, we show that f is a theorem-preserving reduction, i.e.,f satisfies Def. 9. This
amounts to proving that for each knowledge base KB in F1 there exists a circuit gKB ,
whose size is poynomial wrt KB, such that for every formula  on the variables of KB it
holds that KB `F1  iff f (KB) `F2 gKB ().
We proceed as in the proof of Theorem 4: Given a KB in F1 , let z = f2 (KB, f3 (KB)).
Since f2 and f3 are poly-size, z has polynomial size with respect to |KB|. Define gKB () =
g(z, ) = g(f2 (KB, f3 (KB)), ). Clearly, gKB can be represented by a circuit of polynomial
size wrt KB. From this construction, KB `F1  iff f (KB) `F2 gKB (). Hence, the claim
follows.
Theorem 7 Let F1 and F2 be two PKR formalisms. If the polynomial hierarchy does not
collapse, F1 is thm-C1 -hard, F2 is in thm-C2 , and C2  C1 , then there is no poly-size
reduction f : F1 7 F2 satisfying theorem preservation.
Proof. We show that if such a reduction exists, then C1 /poly  C2 /poly and the polynomial
hierarchy collapses at some level (Yap, 1983). Let A be a complete problem for class C1 .
Define the problem A as in the proof of Theorem 5: this problem is k;C1 -complete (Cadoli
et al., 1996b, Thm. 6). Since inference in F1 is thm-C1 -hard, A is non-uniformly compreducible to inference in F1 . That is, (adapting Def. 6) there exist two poly-size binary
functions f1 and f2 , and a polynomial-time binary function g such that for every pair h, yi,
h, yi  A if and only if f1 (, |y|) `F1 g(f2 (, |y|), y). Let |y| = n. Clearly, the knowledge
base f1 (, |y|) depends just on n, i.e., there is one knowledge base for each integer. Call
it KBn . Moreover, also f2 (, |y|) = f2 (, n) depends just on n: call it On (for Oracle).
Observe that both KBn and On have polynomial size with respect to n.
If there exists a poly-size reduction f : F1 7 F2 satisfying theorem preservation, then
given the knowledge base KBn there exists a poly-time function hn such that KBn `F1
g(On , y) if and only if f (KBn ) `F2 hn (g(On , y)).
Therefore, the k;C1 -complete problem A can be non-uniformly reduced to a problem in
k;C2 as follows: Given y, from its size |y| = n one obtains (with an arbitrary preprocessing)
f (KBn ) and On . Then one checks whether the formula hn (g(On , y)) (computable in polytime given y and On ) is a theorem in F2 of f (KBn ). From the fact that inference in F2 is
in k;C2 , we have that k;C1  k;C2 . It follows that C1 /poly  C2 /poly (Cadoli et al.,
1996b, Thm. 9), which implies that the polynomial hierarchy collapses (Yap, 1983).
Theorems 4-7 show that compilability classes characterize very precisely the relative
capability of PKR formalisms to represent sets of models or sets of theorems. For example,
as a consequence of Theorems 3 and 7 there is no poly-size reduction from PL to the
syntactic restriction of PL allowing only Horn clauses that preserves the theorems, unless
the polynomial hierarchy collapses. Kautz and Selman (1992) proved non-existence of such
a reduction for a problem strictly related to pli using a specific proof.

16

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

6. Applications
This section is devoted to the application of the theorems presented in the previous section.
Using Theorems 4-7 and results previously known from the literature, we are able to asses
model- and theorem-compactness of some PKR formalisms.
We assume that definitions of Propositional Logic, default logic (Reiter, 1980), and
circumscription (McCarthy, 1980) are known. Definitions of WIDTIO, SBR, GCWA, and
stable model semantics are in the appropriate subsections.
In the following proofs we refer to the problem 3QBF, that is, the problem of verifying
whether a quantified Boolean formula XY.F is valid, where X and Y are disjoint sets of
variables, and F is a set of clauses on the alphabet X  Y , each composed of three literals.
As an example, a simple formula belonging to this class is: x1 , x2 y1 , y2 ((x1  y2 ) 
(x1  x2  y1 )  (y1  x2  y2 )  (x1  x2 )).
The problem of deciding validity of a 3QBF is complete for the class p2 . As a consequence, the corresponding problem 3QBF, that is deciding whether an input composed
of any string () as the fixed part and a quantified Boolean formula XY.F as the varying
one, is complete for the class k;2p (Liberatore, 1998). Notice that in most of the hardness
proofs we show in the sequel we use problems without any meaningful fixed part.
6.1 Stable Model Semantics
Stable model semantics (SM) was introduced by Gelfond and Lifschitz (1988) as a tool to
provide a semantics for logic programs with negation. their original proposal is now one
of the standard semantics for logic programs. We now recall the definition of propositional
stable model.
Let P be a propositional, general logic program. Let M be a subset (i.e., an interpretation) of the atoms of P . Let P M be the program obtained from P in the following way: if a
clause C of P contains in its body a negated atom A such that A  M then C is deleted;
if a body of a clause contains a negated atom A such that A 6 M then A is deleted from
the body of the clause. If M is a least Herbrand model of P M then M is a stable model of
P.
For the formalism sm, we consider the program P as the knowledge base. We write
P |=sm Q to denote that query Q is implied by a logic program P under Stable Model
semantics.
In order to prove our result, we need to define the kernel of a graph.
Definition 12 (Kernel) Let G = (V, E) be a graph. A kernel of G is a set K  V such
that, denoting H = V  K, it holds:
1. H is a vertex cover of G
2. for all j  H, there exists an i  K such that (i, j)  E.
We can now state the theorem on the compilability class of inference in the stable model
semantics, and the corresponding theorem compactness class.
Theorem 8 The problem of inference for the Stable Model semantics is k;coNP-complete,
thus Stable Model Semantics is thm-coNPcomplete.
17

fiCadoli, Donini, Liberatore, & Schaerf

Proof. Membership in the class follows from the fact that the problem is coNP-complete
(Marek & Truszczynski, 1991). For the hardness, we adapt the proof of Marek and
Truszczynski (1991) showing that deciding whether a query is true in all stable models
is coNP-hard.
Let kernel be the language {, G} such that G is a graph with at least one kernel.
Let |G| = n, and observe that G cannot have more vertices than its size n.
We show that for each n, there exists a logic program Pn such that for every graph G
with at most n vertices, there exists a query QG such that G has a kernel iff Pn 6|=sm QG .
Let the alphabet of Pn be composed by the following 2n2 + n propositional letters:
{ai |i  {1..n} }  {rij , sij |i, j  {1..n} }.
The program Pn is defined as:
aj
sij
rij

:
:
:



ai , rij 

rij
for i, j  {1..n}


sij

Given a graph G = (V, E), the query QG is defined as
QG = (

_

(i,j)E

rij )  (

_

rij )

(i,j)6E

The reduction from kernel to sm is defined as: f1 (x, n) = Pn , i.e., f1 depends only on
its second argument, f2 (x, n) = , i.e., f2 is a constant function, and g = Qy , i.e., given a
graph G, the circuit g computes the query QG .
As a result, this is a k; reduction. We now show that this reduction is correct, i.e.,
h, Gi  kernel (G has a kernel) iff Pn 6|=SM QG .
If-part. Suppose Pn 6|=SM QG . Then, there exists a stable model M of Pn such that
M |= QG . Observe that QG is equivalent to the conjunction of all rij such that (i, j)  E,
and all rij such that (i, j) 6 E. Simplifying Pn with QG we obtain the clauses:
aj : ai , for (i, j)  E

(1)

Observe that M contains all sij such that (i, j) 6 E, and in order to be stable,  i.e., to
support atoms rij such that (i, j)  E  M contains no atom sij such that (i, j)  E.
Let H = {j|aj  M }, K = {i|ai 6 M }. Now H is a vertex cover of G, since for each
edge (i, j)  E, M should satisfy the corresponding clause (1) aj :  ai , hence either
ai  M , or aj  M . Moreover, for each j in H, the atom aj is in M , and since M is a
stable model, there exists a clause aj : ai such that ai 6 M , that is, i  K. Therefore,
K is a kernel of G.
Only-if part. Suppose G = (V, E) has a kernel K, and let H = V  K. Let M be the
interpretation
M = {rij |(i, j)  E}  {sij |(i, j) 6 E}  {aj |j  H}

Obviously, M 6|= QG . We now show that M is a stable model of Pn , i.e., M is a least
Herbrand model of PnM . In fact, PnM contains the following clauses:
sij
rij
aj

: rij

for (i, j) 6 E

(2)

for i  K

(4)

for (i, j)  E

18

(3)

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

Clauses in the last line are obtained from clauses in Pn of the form aj : ai , rij , where the
clauses such that i  H (hence ai  M ) are deleted, while in the other clauses the negated
atom ai is deleted, since i  K, hence ai 6 M . Now for each aj  M , the vertex j is in H,
hence there is an edge (i, j)  E, and i  K. Hence clauses (4) and (3) are in PnM , hence in
the least Herbrand model of PnM there are exactly all aj such that j  H.
6.2 Minimal Model Reasoning
One of the most successful form of non-monotonic reasoning is based on the selection of
minimal models. Among the various formalisms based on minimal model semantics we consider here Circumscription (McCarthy, 1980) and the Generalized Closed World Assumption
(GCWA) (Minker, 1982), which is a formalism to represent knowledge in a closed world.
We assume that the reader is familiar with Circumscription, we briefly present the
definition of GCWA. The model semantics for GCWA is defined as (a is a letter):
M |=GCW A KB iff M |= KB{a | for any positive clause , if KB 6`  then KB 6`   a}
We can now present the results for these two formalisms.
Theorem 9 The problem of model checking for Circumscription is k;coNP-complete, thus
Circumscription is model-coNP-complete.
This result is a trivial corollary of a theorem already proved (Cadoli et al., 1997, Theorem 6). In fact, that proof implicitly shows that model checking for circumscription is
k;coNP-complete.
Theorem 10 The problem of model checking for GCWA is in k;P, thus GCWA is in
model-P.
Proof. As already pointed out (Cadoli et al., 1997), it is possible to rewrite GCW A(T )
into a propositional formula F such that, for any given model M , M |= GCW A(T ) if and
only if M |= F . Moreover, the size of F is polynomially bounded by the size of T . As a
consequence, the model compactness for GCWA is in the same class of PL. By Theorem 3
the thesis follows.
Theorem 11 The problem of inference for Circumscription is k;p2 -complete, thus Circumscription is thm-p2 -complete.
This result is a trivial corollary of a theorem published in a previous paper (Cadoli
et al., 1997, Theorem 7) which implicitly shows that inference for circumscription is k;2p complete.
Theorem 12 The problem of inference for GCWA is k;coNP-complete, thus GCWA is
thm-coNP-complete.
Proof. As already pointed out in the proof of Theorem 10, it is possible to rewrite
GCW A(T ) into a formula F that is equivalent to it. As a consequence, a formula  is
a theorem of GCW A(T ) if and only if it is a theorem of F . Thus, GCWA has at most the
theorem compexity of PL. Since GCWA is a generalization of PL, it follows that GCWA is
in the same theorem compactness class of PL. Hence, GCWA is thm-coNP-complete.
19

fiCadoli, Donini, Liberatore, & Schaerf

6.3 Default Logic
In this subsection we present the results for default logic, in its two variants (credulous
and skeptical). For more details on these two main variants of default logic, we refer the
reader to the paper by Kautz and Selman (1991). Notice that model-compactness is only
applicable to skeptical default logic.
Theorem 13 The problem of model checking for skeptical default logic is k;p2 complete,
thus skeptical default logic is model-p2 complete.
Proof. The proof of membership is straightforward: since model checking for skeptical
default logic is in p2 (Liberatore & Schaerf, 1998), it follows that it is also in k;p2 .
The proof of k;p2 -hardness is similar to the proof of p2 -hardness (Liberatore & Schaerf,
1998). The reduction is from the problem 3QBF. Let h, i be an instance of 3QBF,
where  = XY.F represents a valid 3QBF formula, and  is any string.
Let n be the size of the formula F . This implies that the variables in the formula are at
most n. Let  = {1 , . . . , k } be the set of all the clauses of three literals over this alphabet.
The number of clauses of three literals over an alphabet of n variables is less than O(n3 ),
thus bounded by a polynomial in n.
We prove that XY.F is valid if and only if M is a model of some extension of hW, Di,
where
= 

W

D =

[  : ci

i 

ci

: ci
,
ci

= {ci | i  F }

M





[  : w  (w  xi ) : w  (w  xi ) 

,

w  xi

xi X

w  xi



(

:w

V

i  ci

w

 i

The set {ci | 1  i  k} is a set of new variables, one-to-one with the elements of .
Note that W and D only depends on the size n of F , while M depends on F . As a result,
this is a nucomp reduction.
We now prove that the formula is valid if and only if M is a model of some extension
of the default theory hW, Di. This is similar to an already published proof (Liberatore &
Schaerf, 1998). Consider an evaluation C1 of the variables {ci } and an evaluation X1 of the
variables X. Let D0 be the following set of defaults.
0

D =

[  : ci  [  : ci 

ci C1

ci

ci 6C1

ci



[  : w  (w  xi ) 

w  xi

xi X1

[

xi X1



: w  (w  xi )
w  xi



This set of defaults has been chosen so that the set R of its consequences corresponds
to the sets C1 and X1 . Namely, we have:
ci  C1 iff R |= ci

ci 6 C1 iff R |= ci

xi  X1 iff R |= w  xi

xi 6 X1 iff R |= w  xi
20

)

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

Now, we prove that the consequences of this set of defaults are an extension of the
default theory if and only if the QBF formula is valid. Since all defaults are semi-normal,
we have to prove that:
1. the set of consequences of D0 is consistent; and
2. no other default is applicable, that is, there is no other default whose precondition is
consistent with R.
Consistency of R follows by construction: assigning ci to true for each ci  C1 , etc., we
obtain a model of R.
:ci
We have then to prove that no other default is applicable. If ci  C1 , the default c
i
:ci
is not applicable, and vice versa, if ci  C1 , then ci is not applicable. Moreover, none

i)
, is applicable if xi 6 X1 , because in this case w  xi  R, thus
of the defaults :w(wx
wxi
w would follow (while w is a justification of the default). A similar statement holds for
:w(wxi )
if xi  X1 .
wxi

V

:w

ci i

i 
As a result, the only applicable default may be the last one,
(recall that
w
F is negated). This default is applicable if and only if, for the given evaluation of the ci s
and xi s, the set of clauses is satisfiable. This amount to say: there is an extension in
which the last default is not applicable if and only if the QBF formula is valid. Now,
if the last default is applicable, then M is not a model of the extension because w is the
consequence of the last default while w 6|= M . The converse also holds: if the last default is
not applicable then M is a model of the default theory.
As a result, the QBF is valid if and only if M is a model of the given default theory.

Theorem 14 The inference problem for skeptical default logic is k;p2 complete, thus skeptical default logic is thm-2p complete.
Proof. Since inference in skeptical default logic is in p2 , it is also in k;p2 . k;p2 -hardness
comes from a simple reduction from circumscription. Indeed, the circumscription of a
formula T is equivalent to the conjunction of the extensions of the default theory hT, Di,
where (Etherington, 1987):
D=

[  : xi 

xi

As a result, CIRC(T ) |= Q if and only if Q is implied by hT, Di under skeptical semantics. Since hT, Di only depends on T (and not on Q) this is a nucomp reduction. Since
inference for circumscription is k;2p -complete (see Theorem 11), it follows that skeptical
default logic is k;2p -hard.
Theorem 15 The inference problem for credulous default logic is k;p2 complete, thus
credulous default logic is thm-p2 complete.

21

fiCadoli, Donini, Liberatore, & Schaerf

Proof. The proof is very similar to the proof for model checking of skeptical default logic.
Indeed, both problems are k;p2 complete. Since the problem is in p2 , as proved by Gottlob
(1992), it is also in k;p2 . Thus, what we have to prove is that is hard for that class.
We prove that the 3QBF problem can be reduced to the problem of verifying whether
a formula is implied by some extensions of a default theory (that is, inference in credulous
default logic).
Namely, a formula XY.F is valid if and only if Q is derived by some extension of
the default theory hD, W i, where W and D are defined as follows ( is the set of all the
clauses of three literals over the alphabet of F , and C is a set of new variables, one-to-one
with ).
W

= 

D =

[  : ci

ci

ci C

Q =

^

i F

ci 

: ci
,
ci

^

i 6F





[  : xi

xi X

xi

: xi
,
xi





(

V

(

ci C ci

w

 i ) :

)

ci  w

Informally, the proof goes as follows: for each truth evaluation of the variables in C
and X there is a set of defaults which are both justified and consistent. A simple necessary
and sufficient condition for the consequences of this set of defaults to be an extension is the
following. If, in this evaluation, the formula


^

i

ci =true

is valid, then the last default is applicable, thus the extension also contains w. The converse
also holds: if the formula is not valid in the evaluation, then the variable w is not in the
extension.
As a result, there exists an extension in which Q holds if and only if there exists an
extension in which each ci is true if and only if i  F , and such that w also holds. When
the variables ci have the given value, the above formula is equivalent to F . As a result,
such an extension exists if and only if there exists a truth evaluation of the variables X in
which F is valid.
6.4 Belief Revision
Many formalisms for belief revision have been proposed in the literature, here we focus on
two of them: WIDTIO (When In Doubt Throw it Out) and SBR (Skeptical Belief Revision).
Let K be a set of propositional formulae, representing an agents knowledge about the world.
When a new formula A is added to K, the problem of the possible inconsistency between K
and A arises. The first step is to define the set of sets of formulae W (K, A) in the following
way:
W (K, A) = {K 0 K 0 is a maximal consistent subset of K  {A} containing A }
22

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

Any set of formulae K 0  W (K, A) is a maximal choice of formulae in K that are
consistent with A and, therefore, we may retain when incorporating A. The definition of
this set leads to two different revision operators: SBR and WIDTIO.
SBR Skeptical Belief Revision (Fagin, Ullman, & Vardi, 1983; Ginsberg, 1986). The revised
.
theory is defined as a set of theories: K  A = {K 0 | K 0  W (K, A)}. Inference in the
revised theory is defined as inference in each of the theories:
K  A `SBR Q iff

for all K 0  W (K, A) , we have that K 0 ` Q

The model semantics is defined as:
M |=SBR K  A iff

there exists a K 0  W (K, A) such that M |= K 0

WIDTIO When In Doubt Throw It Out (Winslett, 1990). A simpler (but somewhat
drastical) approach is the so-called WIDTIO, where we retain only the formulae of K
that belong to all sets of W (K, A). Thus, inference is defined as:
K  A `W IDT IO Q iff

T

W (K, A) ` Q

The model semantics of this formalism is defined as:
M |=W IDT IO K  A

iff

M |=

\

W (K, A)

The results on model compactness have been shown by Liberatore and Schaerf (2000).
Here we recall them.
Theorem 16 (Liberatore & Schaerf, 2000, Theorem 11) The problem of model checking for WIDTIO is in k;P, thus WIDTIO is in model-P.
Theorem 17 (Liberatore & Schaerf, 2000, Theorem 5) The problem of model checking for Skeptical Belief Revision is k;coNP-complete, thus Skeptical Belief Revision is
model-coNP-complete.
The results on theorem compactness are quite simple and we provide here the proofs.
Theorem 18 The problem of inference for WIDTIO is k;coNP-complete, thus WIDTIO
is thm-coNP-complete.
Proof. Membership in the class thm-coNP immediately follows from the definition. In fact,
we can rewrite K  A into a propositional formula by computing the set W (K, A) and then
constructing their intersection. By construction their intersection has size less than or equal
to the size of K  A. As a consequence, after preprocessing, deciding whether a formula Q
follows from K  A is a problem in coNP. Hardness follows from the obvious fact that PL
can be reduced to WIDTIO and PL is thm-coNP-complete (see Theorem 3).
Theorem 19 The problem of inference for Skeptical Belief Revision is k;p2 -complete,
thus Skeptical Belief Revision is thm-p2 -complete.
23

fiCadoli, Donini, Liberatore, & Schaerf

Propositional
Logic
WIDTIO
Skeptical
Belief Revision
Circumscription
GCWA

Skeptical
Default Reasoning
Credulous
Default Reasoning
Stable Model
Semantics

Time Complexity
P

p2 -complete
(Liberatore & Schaerf, 1996)
coNP-complete
(Liberatore & Schaerf, 1996)
coNP-complete
(Cadoli, 1992)
coNP-hard,
in p2 [log n]
(Eiter & Gottlob, 1993)
p2 -complete
(Liberatore & Schaerf, 1998)
N/A

Space Efficiency
model-P

model-P
Th. 16
model-coNP-complete
Th. 17
model-coNP-complete
Th. 9
model-P
Th. 10

P


model-P


model-p2 -complete
Th. 13
N/A

Table 1: Complexity of model checking and Space Efficiency of Model Representations
Proof. Membership follows from the complexity results of Eiter and Gottlob (1992), where
they show that deciding whether K  A `SBR Q is a p2 -complete problem. Hardness
follows easily from Theorem 17. In fact, M |=SBR K  A iff K  A 6`SBR f orm(M ), where
f orm(M ) is the formula that represents the model M . As a consequence, model checking
can be reduced to the complement of inference. Thus inference is k;p2 -complete.
6.5 Discussion
Tables 1 and 2 summarize the results on space efficiency of PKR formalisms and where they
were proved (a dash  denotes a folklore result).
First of all, notice that space efficiency is not always related to time complexity. As an
example, we compare in detail WIDTIO and circumscription. From the table it follows that
model checking is harder for WIDTIO than for circumscription, and that inference has the
same complexity in both cases. Nevertheless, since circumscription is thm-p2 -complete and
WIDTIO is thm-coNP-complete (and thus in thm-p2 ), there exists a poly-size reduction
from WIDTIO to circumscription satisfying theorem preservation. The converse does not
hold: since circumscription is thm-2p -complete and WIDTIO is thm-coNP, unless the Polynomial Hierarchy does not collapse there is no theorem-preserving poly-size reduction from
the former formalism to the latter. Hence, circumscription is a more compact formalism
than WIDTIO to represent theorems. Analogous considerations can be done for models.
Intuitively, this is due to the fact that for WIDTIO both model checking and inference
require a lot of work on the revised knowledge base alonecomputing the intersection of

24

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

Propositional
Logic
WIDTIO
Skeptical
Belief Revision
Circumscription
GCWA
Skeptical
Default Reasoning
Credulous
Default Reasoning
Stable Model
Semantics

Time Complexity
coNP-complete
(Cook, 1971)
p2 -complete
(Eiter & Gottlob, 1992) & (Nebel, 1998)
p2 -complete
(Eiter & Gottlob, 1992)
p2 -complete
(Eiter & Gottlob, 1993)
p2 -complete
(Eiter & Gottlob, 1993) & (Nebel, 1998)
p2 -complete
(Gottlob, 1992)
p2 -complete
(Gottlob, 1992)
coNP-complete
(Marek & Truszczynski, 1991)

Space Efficiency
thm-coNP-complete
(Cadoli et al., 1996)
thm-coNP-complete
Th. 18
thm-p2 -complete
Th. 19
thm-p2 -complete
Th. 11
thm-coNP-complete
Th. 12
thm-p2 -complete
Th. 14
thm-p2 -complete
Th. 15
thm-coNP-complete
Th. 8

Table 2: Complexity of inference and Space Efficiency of Theorem Representations
all elements of W (K, A). Once this is done, one is left with model checking and inference in
PL. Hence, WIDTIO has the same space efficiency as PL, which is below circumscription.
Figures 3 and 4 contain the same information of Tables 1 and 2, but highlight existing reductions. Each figure contains two diagrams, the left one showing the existence of
polynomial-time reductions among formalisms, the right one showing the existence of polysize reductions. An arrow from a formalism to another denotes that the former can be
reduced to the latter one. We use a bidirectional arrow to denote arrows in both directions
and a dashed box to enclose formalisms that can be reduced one into another. Note that
some formalisms are more appropriate in representing sets of models, while others perform
better on sets of formulae. An interesting relation exists between skeptical default reasoning
and circumscription. While there is no model-preserving poly-size reduction from circumscription to skeptical default reasoning (Gogic et al., 1995), a theorem-preserving poly-size
reduction exists, as shown by Theorem 14.

7. Related Work and Conclusions
The idea of comparing the compactness of KR formalisms in representing information is not
novel in AI. It is well known that first-order circumscription can be represented in secondorder logic (Schlipf, 1987). Kolaitis and Papadimitriou (1990) discuss several computational
aspects of circumscription. Among many interesting results they show a reduction from a
restricted form of first-order circumscription into first-order logic. The proposed reduction
will increase the size of the original formula by an exponential factor. It is left as an open
problem to show whether this increase is intrinsic, because of the different compactness
properties of the two formalisms, or there exists a more space-efficient reduction. When a

25

fiCadoli, Donini, Liberatore, & Schaerf

- Skeptical

WIDTIO 

Default

Skeptical Default

6

6

GCWA
6

SBR  - Circumscription

SBR  - Circumscription

.

6

6

PL  - Stable Model

PL  - WIDTIO 

a. Time Complexity

- GCWA  - Stable
Model

b. Space Efficiency

Figure 3: Complexity of Model Checking vs. Space Efficiency of Model Representation

WIDTIO - GCWA

6

?
Skeptical
SBR  - Circum  - Default
S
o
S
PL -

Credulous
Default



SBR - Circum-



Skeptical
Default

AK
A

Stable
Model

Credulous
Default






Stable
PL- WIDTIO - GCWA - Model

a. Time complexity

b. Space efficiency

Figure 4: Complexity of Inference vs. Space Efficiency of Theorem Representation

26

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

first-order language is used, more results on compactness and existence of reductions are
reported by Schlipf (1995).
Khardon and Roth (1996, 1997), and Kautz, Kearns and Selman (1995) propose modelbased representations of a KB in Propositional Logic, and compare it with formula-based
representations. Although their results are significant for comparing representations within
PL, they refer only to this formalism, hence they are not applicable to our comparison between different PKR formalisms. The same comment applies also to the idea of representing
a KB with an efficient basis by Moses and Tennenholz (1996), since it refers only to one
PKR formalism, namely, PL.
An active area of research studies the connections of the various non-monotonic logics.
In particular, there are several papers discussing the existence of translations that are polynomial in time and satisfy other intuitive requirements such as modularity and faithfulness.
Janhunen (1998), improving on results of Imielinski (1987) and Gottlob (1995), shows that
default logic is the most expressive, among the non-monotonic logics examined, since both
circumscription and autoepistemic logic can be modularly and faithfully embedded in default logic, but not the other way around. While these results are of interest and help to
fully understand the relation among many knowledge representation formalisms, they are
not directly related to ours. In fact, we allow for translations that are more general than
polynomial time, while in all of the above papers they only consider translations that use
polynomial time and also satisfy additional requirements.
The first result on compactness of representations for a propositional language is presented, to the best of our knowledge, by Kautz and Selman (1992). They show that, unless
there is a collapse in the polynomial hierarchy, the size of the smallest representation of
the least Horn upper bound of a propositional theory is superpolynomial in the size of the
original theory. These results are also presented in a different form in the more comprehensive paper (Selman & Kautz, 1996). The technique used in the proof has been then
used by us and other researchers to prove several other results on the relative complexity of
propositional knowledge representation formalisms (Cadoli et al., 1996, 1997, 1999; Gogic
et al., 1995).
In a recent paper (Cadoli et al., 1996b) we introduced a new complexity measure, i.e.,
compilability. In this paper we have shown how this measure is inherently related to the
succinctness of PKR formalisms. We analyzed PKR formalisms with respect to two succinctness measures: succinctness in representing sets of models and succinctness in representing
sets of theorems.
The main advantage of our framework is the machinery necessary for a formal way of
talking about the relative ability of PKR formalisms to compactly represent information. In
particular, we were able to formalize the intuition that a specific PKR formalism provides
one of the most compact ways to represent models/theorems among the PKR formalisms
of a specific class.
In our opinion, the proposed framework improves over the state of the art in two different
aspects:
1. All the proofs presented in the previous papers only compare pairs of PKR formalisms, for example propositional circumscription and Propositional Logic (Cadoli
et al., 1997). These results do not allow for a precise classification of the level of
27

fiCadoli, Donini, Liberatore, & Schaerf

compactness of the considered formalisms. Rephrasing and adapting these results
in our framework allows us to infer that circumscription is model-coNP-complete and
thm-p2 -complete. As a consequence, we also have that it is more space-efficient of the
WIDTIO belief revision formalism in representing sets of models or sets of theorems.
2. Using the proposed framework it is now possible to find criteria for adapting existent
polynomial reductions showing C-hardness into reductions that show model-C or thmC-hardness, where C is a class in the polynomial hierarchy (Liberatore, 1998).

Acknowledgments
This paper is an extended and revised version of a paper by the same authors appeared
in the proceedings of the fifth international conference on the principles of knowledge representation and reasoning (KR96) (Cadoli, Donini, Liberatore, & Schaerf, 1996a). Partial
supported has been given by ASI (Italian Space Agency) and CNR (National Research
Council of Italy).

References
Ben-Eliyahu, R., & Dechter, R. (1991). Default logic, propositional logic and constraints.
In Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI91),
pp. 379385.
Ben-Eliyahu, R., & Dechter, R. (1994). Propositional semantics for disjunctive logic programs. Annals of Mathematics and Artificial Intelligence, 12, 5387.
Boppana, R., & Sipser, M. (1990). The complexity of finite functions. In van Leeuwen, J.
(Ed.), Handbook of Theoretical Computer Science, Vol. A, chap. 14. Elsevier Science
Publishers (North-Holland), Amsterdam.
Cadoli, M. (1992). The complexity of model checking for circumscriptive formulae. Information Processing Letters, 44, 113118.
Cadoli, M., Donini, F., Liberatore, P., & Schaerf, M. (1996a). Comparing space efficiency
of propositional knowledge representation formalisms. In Proceedings of the Fifth International Conference on the Principles of Knowledge Representation and Reasoning
(KR96), pp. 364373.
Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1996b). Feasibility and unfeasibility of off-line processing. In Proceedings of the Fourth Israeli Symposium on Theory
of Computing and Systems (ISTCS96), pp. 100109. IEEE Computer Society Press.
url = ftp://ftp.dis.uniroma1.it/PUB/AI/papers/cado-etal-96.ps.gz.
Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1997).
Preprocessing of intractable problems.
Tech. rep. DIS 24-97, Dipartimento di
url =
Informatica e Sistemistica, Universita di Roma La Sapienza.
http://ftp.dis.uniroma1.it/PUB/AI/papers/cado-etal-97-d-REVISED.ps.gz.
28

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1999). The size of a revised
knowledge base. Artificial Intelligence, 115 (1), 2564.
Cadoli, M., Donini, F. M., & Schaerf, M. (1995). On compact representations of propositional circumscription. In Proceedings of the Twelfth Symposium on Theoretical Aspects of Computer Science (STACS95), pp. 205216. Extended version as RAP.14.95
DIS, Univ. of Roma La Sapienza, July 1995.
Cadoli, M., Donini, F. M., & Schaerf, M. (1996). Is intractability of non-monotonic reasoning
a real drawback?. Artificial Intelligence, 88 (12), 215251.
Cadoli, M., Donini, F. M., Schaerf, M., & Silvestri, R. (1997). On compact representations
of propositional circumscription. Theoretical Computer Science, 182, 183202.
Cook, S. A. (1971). The complexity of theorem-proving procedures. In Proceedings of the
Third ACM Symposium on Theory of Computing (STOC71), pp. 151158.
Eiter, T., & Gottlob, G. (1992). On the complexity of propositional knowledge base revision,
updates and counterfactuals. Artificial Intelligence, 57, 227270.
Eiter, T., & Gottlob, G. (1993). Propositional circumscription and extended closed world
reasoning are 2p -complete. Theoretical Computer Science, 114, 231245.
Etherington, D. V. (1987). Reasoning with incomplete information. Morgan Kaufmann,
Los Altos, Los Altos, CA.
Fagin, R., Ullman, J. D., & Vardi, M. Y. (1983). On the semantics of updates in databases.
In Proceedings of the Second ACM SIGACT SIGMOD Symposium on Principles of
Database Systems (PODS83), pp. 352365.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the
Theory of NP-Completeness. W.H. Freeman and Company, San Francisco, Ca.
Gelfond, M., & Lifschitz, V. (1988). The stable model semantics for logic programming.
In Proceedings of the Fifth Logic Programming Symposium, pp. 10701080. The MIT
Press.
Gelfond, M., Przymusinska, H., & Przymusinsky, T. (1989). On the relationship between
circumscription and negation as failure. Artificial Intelligence, 38, 4973.
Ginsberg, M. L. (1986). Conterfactuals. Artificial Intelligence, 30, 3579.
Gogic, G., Kautz, H. A., Papadimitriou, C., & Selman, B. (1995). The comparative linguistics of knowledge representation. In Proceedings of the Fourteenth International
Joint Conference on Artificial Intelligence (IJCAI95), pp. 862869.
Gottlob, G. (1992). Complexity results for nonmonotonic logics. Journal of Logic and
Computation, 2, 397425.
Gottlob, G. (1995). Translating default logic into standard autoepistemic logic. Journal of
the ACM, 42, 711740.
29

fiCadoli, Donini, Liberatore, & Schaerf

Imielinski, T. (1987). Results on translating defaults to circumscription. Artificial Intelligence, 32, 131146.
Janhunen, T. (1998). On the intertranslatability of autoepistemic, default and priority
logics, and parallel circumscription. In Proceedings of the Sixth European Workshop
on Logics in Artificial Intelligence (JELIA98), No. 1489 in Lecture Notes in Artificial
Intelligence, pp. 216232. Springer-Verlag.
Johnson, D. S. (1990). A catalog of complexity classes. In van Leeuwen, J. (Ed.), Handbook
of Theoretical Computer Science, Vol. A, chap. 2. Elsevier Science Publishers (NorthHolland), Amsterdam.
Karp, R. M., & Lipton, R. J. (1980). Some connections between non-uniform and uniform
complexity classes. In Proceedings of the Twelfth ACM Symposium on Theory of
Computing (STOC80), pp. 302309.
Kautz, H. A., Kearns, M. J., & Selman, B. (1995). Horn approximations of empirical data.
Artificial Intelligence, 74, 129145.
Kautz, H. A., & Selman, B. (1991). Hard problems for simple default logics. Artificial
Intelligence, 49, 243279.
Kautz, H. A., & Selman, B. (1992). Forming concepts for fast inference. In Proceedings of
the Tenth National Conference on Artificial Intelligence (AAAI92), pp. 786793.
Khardon, R., & Roth, D. (1996). Reasoning with models. Artificial Intelligence, 87, 187
213.
Khardon, R., & Roth, D. (1997). Defaults and relevance in model-based reasoning. Artificial
Intelligence, 97, 169193.
Kobler, J., & Watanabe, O. (1998). New collapse consequences of NP having small circuits.
SIAM Journal on Computing, 28 (1), 311324.
Kolaitis, P. G., & Papadimitriou, C. H. (1990). Some computational aspects of circumscription. Journal of the ACM, 37 (1), 114.
Liberatore, P. (1995). Compact representation of revision of Horn clauses. In Yao, X. (Ed.),
Proceedings of the Eighth Australian Joint Artificial Intelligence Conference (AI95),
pp. 347354. World Scientific.
Liberatore, P. (1998). Compilation of intractable problems and its application to artificial
intelligence.
Ph.D.
thesis,
Dipartimento di Informatica e Sistemistica, Universita di Roma La Sapienza. URL =
ftp://ftp.dis.uniroma1.it/pub/AI/papers/libe-98-c.ps.gz.
Liberatore, P., & Schaerf, M. (1996). The complexity of model checking for belief revision and update. In Proceedings of the Thirteenth National Conference on Artificial
Intelligence (AAAI96), pp. 556561.

30

fiSpace Efficiency of Propositional Knowledge Representation Formalisms

Liberatore, P., & Schaerf, M. (1998). The complexity of model checking for propositional
default logics. In Proceedings of the Thirteenth European Conference on Artificial
Intelligence (ECAI98), pp. 1822.
Liberatore, P., & Schaerf, M. (2000). The compactness of belief revision and update operators. Tech. rep., Dipartimento di Informatica e Sistemistica, Universita di Roma La
Sapienza.
Marek, W., & Truszczynski, M. (1991). Autoepistemic logic. Journal of the ACM, 38 (3),
588619.
McCarthy, J. (1980). Circumscription - A form of non-monotonic reasoning. Artificial
Intelligence, 13, 2739.
Minker, J. (1982). On indefinite databases and the closed world assumption. In Proceedings
of the Sixth International Conference on Automated Deduction (CADE82), pp. 292
308.
Moses, Y., & Tennenholtz, M. (1996). Off-line reasoning for on-line efficiency: knowledge
bases. Artificial Intelligence, 83, 229239.
Nebel, B. (1998). How hard is it to revise a belief base?. In Dubois, D., & Prade, H. (Eds.),
Belief Change - Handbook of Defeasible Reasoning and Uncertainty Management Systems, Vol. 3. Kluwer Academic.
Reiter, R. (1980). A logic for default reasoning. Artificial Intelligence, 13, 81132.
Schlipf, J. S. (1987). Decidability and definability with circumscription. Annals of Pure
and Applied Logic, 35, 173191.
Schlipf, J. S. (1995). A survey of complexity and undecidability results for logic programming. Annals of Mathematics and Artificial Intelligence, 15, 257288.
Selman, B., & Kautz, H. A. (1996). Knowledge compilation and theory approximation.
Journal of the ACM, 43, 193224.
Stockmeyer, L. J. (1976). The polynomial-time hierarchy. Theoretical Computer Science,
3, 122.
Winslett, M. (1989). Sometimes updates are circumscription. In Proceedings of the Eleventh
International Joint Conference on Artificial Intelligence (IJCAI89), pp. 859863.
Winslett, M. (1990). Updating Logical Databases. Cambridge University Press.
Yap, C. K. (1983). Some consequences of non-uniform conditions on uniform classes. Theoretical Computer Science, 26, 287300.

31

fi	ff
fiff 	ff


 ! #"$ % 	ff'&)(+*-,/.0001324+5(+4*

6789: ;)(ff<ff00!=>8
%&;?2@<ff00

ACBD7EGF-HID@JLKMACNOJLPIQRD@HTSUACVWSXKYQB

Z\[]$^_]a`cb-dfe$gihjek^

lkmknpokmkqrWs$tvuwxq3npyzwxqpsi{3|_wx}Xty

~L7Oz'@@_ffY@+ff-@ _z'ff
~LXffffkff'X!Y
Ofifi'RWRfi! $


cj/L+77+ff)7i+++Yf!/+O
+xYj)ffTxz 3k7Y+c+7i7Yi+)7iff/++LYxk
ff+xi+xYz7xffx)?!+7++ficfici+7fiz7fix:x!+@
/ff++v+c7xzO+ X+xz7fix+$+@
/ff/fi:Ic?'Y
 k#ffzx7++ vfixX7!ix+Y7ffp/pvx:z$i+
jx@ +++c+/7:/ffc+:!Tx:fi/?)7xcz7 
+zff+ffOk+czx/#fiff ffi+Lx)/z7p
iff/W+Wff vcx7x!pcT7+z7+)++3
'++z7+xx7:+?@++xiff+Lfix!p+W/zX+)+x+7pLc
7@W/--+)T@@z@O+++xO7-7xz+
iff@i+/! z O+xYf! 7_ x+x7fx+@
x+?+T+x7+ff+ff++i!+O+7+ v_7?
fiffR+ +@@z++xi+Y:fffx7+L+7xz7?77!v! 
z7+ffff@@iX77i+k77xzxcv:Lx!+cff+ff
+-)+x+7ff
Oi :-X 
T@	ff
fivpv
@ffvp

Rff
!#"$fi%fi&'fi()'%fi*,+-fi./vp

v 012345fi6v7
fi
*
8@ff4v,Rfi09fi(W@ff6pfffi(04%fi':;<ff430=v>?pff
@ fi 099+ff40Av?pff90ff:B'fi:;ffC4ff!6DffpE9
()34?fiA+F?04<'fi7R()fi
 ff4@ffG'.1ff!IHT?pffvff*JKpffpL
3MfiN%fi0	()fi@ff4O
E
3P'
ffQ?0L+F44
p
 
fi3
R
3 @ ffR1S?3Pfi !CT$p
? @ ()fi@ *Jv 0fi G'fiU30/v pC Rfi 7+F?G04 0*V
4c
 ff 26'fiS4fi 0 P?3fi 7'fiU ff W Rfi/()SL04 N'fiSL'fiG3
3 M()fi@ ff 


30P'
 ff!XT$p
? Y 'fi/3
4fi Zfi([ ff  pM*,?Mfi\+  ff*Afi()#P. ff?pKfi ]^R
 ff?ff/fi
 0`_a
 [?0p
 V'fi[@ ff2
! ( T$p
? fifi(M?4J@ ff ff
E?72V'fiF E(bQ?p9R
 ff?ff/fiJfi(0 
 !,ffc a4
34d*fi3MAfi @ ff
 L4-'fip
fi
  fffi
 e
v $O?fi1-()fi5p
 Pv 0v p>+fp
? ?p
?pAR
 ff?ff/fiOfi(av ff v p[fi <@ ffPv g+F4?v L?3-%fi3
3 0Jfi(a@ ff'R
 ff
`k
_ ff7
fi 'v Ahi
2 ff
 vffkl6()$ ff  pM!gTF?4, 
43p
 ff< ()1v 3>?-fiR
 ff9@ [@ ff  ffC()fi-v p 
j fiR
4 mfi YC
+ ff4Q E(b/v pZ?fi0[fiR
 vff@ m ff  ffn(bfiK834fi 
'/'ffP$ 	+F?4
E?Kfi pNfifPfi@ Lfi [Pd	0!
 o/P0 Gfi(7XfiR
 pq4
 [4fi\a]^KrOCstff+u'h [4fi\a*IvffwxylE!zT$?4C4ff+>*F+F?2
?
?74'fiYR
 W3 ffm1
{ ff4W 0R|<'}fffi 0fh~vffwwlE*<'ff8?8Wfi 8Pd
 Mfi7?E

a~IEd\LQ^5^M~^~E9~B^\>5pE^\p9;ff\d^\[^d9'EI\i;ffB7B'\\~
idiB~XdB'\^;^^^b82fftdB)~t5~-pB~ffBE~59^'B^;


.000j %%a!;	+L	ff;A?@	ff9	ff!x9	ff,8!
%&%;

&@%$%2@ ;\

fi

mknpokmkq

?13PYfiQ4fi\+?30PY'fi	
fiO8'fiC?EK!5T$?p7vU
fi'03fiKfi(<{ff4SG|<'}fffit4
 j8 
4<'fiUP]>
 ()fi@ 
+ P@ ff ff@ P3M'fiMfifi37fi 8v 'fiK@ ff`:B+-fi4
 14fi O
 *V
+ 
pff'fiO
 	
@ ffa  Z
fi03Mfi 04'
0v CO
 ff 0>fi(F.1v 3Y?pfffiR
 a
 f2fid]^
rO'7sJff+>!!!^?Mfid+MfiY
+ ''fiZfi3M7()
>(bfi
3 pU3L?0v m?3fi/
 ff>fi(-fiR
 /v p
fi3MQfip
 EEffk[(9
fi3M *?2f4d+4['fi
fi pg()fiL@ ff
[40v ffO
 4fi G 0 pff0f'fiKR
 
fiR
 fi 2} ff	v 'fi'R
 ff
`_
Ffi
 ff-ff'0 Nfi KI'/'ffK*30
?K j T
H  Fp
 ff L30 ff]^
_a !^kGT$?2V0R
 t0M@ ff fft
{ ff4I 8|<'}fffi 0]^t
2'fiQPOv >?p-
fi o/gfi(a Afi ff!
Ttfi7@ fffi 0'fi7?3f
40'fi8*
+ f f+-fi./v pL'fi\+- j [4fi\12 k>0 [fi */+F?2
?
+ I3

 k
_ p>'fi	R
 I fi Q?N
 m ()*% SC@ ff'fi 00 e
 NP 3pd*+fp
? ?p

.

3 :;p
 k
_ pffmfi
 ffL@ P0@ ff  ffR()Ifi !
3
E?Rfi 7+F42_
 ff4?p7
fi 'v 
?pffO4fi 8
 ?Mfi10*fig 0g?pffP ff ffOv 73
E?8f+Aff7g'fiQ@ ff@  9?3ff 90fiR
 ff!
T$3
?  `_a
fi =O
 ?Mfi/3O
 ff=p
? @ *IId7/)*N
fi 4'Kfi(80342v pW&k
_ 
fi/p
 ffVfi(9'/'ff G
p
? ff
.1v 3+F3
? ?pf?37p
 ff4@ ffYfi
 ;Y?Mfi40Fv U?[Pfi1p
 ffB!5+c S?p

fi o1Qfi(6?4F0R
 ff*0fi/p
 fft
p
? ff
.1v 3Cp
 EPv pffF+Fp
? ?pNn =(bfi[04 Sq SfiR
 p
*M! !*/+Fp
? ?3-a4 	 j Pfi1p
 ff2kChi4~$
_ ffEl<fiR
 p!<T$3
? ffi3M'a3M-2 ff4?p j  ffkIfi j Mfik
 *a( j fiM*^kPfi p7fiffi@ 7
fi3

3  o/a ffF@ 7fi\143
 ff!,Sfi1p
 fft
p
? ff
.1 p?$fid S'fiR
 
 \

 Rff
 ()fiN( p:;
E4
O042
fi k
* ! M!*VCPfi1p
 fft
p
? ff
. >3
3 
fi\ @ fffi444
4''fi3c
 fi7v WU'/'ffp
 ff pffR'fiSP. C0342v p[fi@ ff?13.  ff4'!KT$?4
fiA+-fi34	?d
 Q$
3  ffp
? ffCI''3
3(bfi
 Q'fiI+-fi ff?13. Q/fi 0*E?pA? 
R
 	?pffhi|,4 ff2M
 f!*VvffwwlE!
|9 44*fi/p
 ff1
p
? ff
.1 pL4g3MA()fi
 5 ff
E??Mfip
3 ?8?p5 6fi(41@ ff
?00 $'ff6fi(
?p502 7'fiL
Ep
? ff
.(0?p5fiR
 p7?Mfi40!Jc(0?p5a4 I?6[k
_ ?13>R
 Ofi(%'ff*?04gfi/
 ff
P ff!6Ufi/p
 ffM
Ep
? ff
./v p4fi0MfiR
 vffOfi(N834fi <02 ?64O
 5
fia o/4;8?
4Y
 o/%fi p2<v m?p137
 Nfi(5fi ! * {?RY2@ 30>R
 Nfi(Afi *O?2N4N
fi34R
 
U@ fi3N0fi0 ffK!+c W()
*j ZPfi1p
 ff,
p
? ff
.1v 3SUv p Pfi 7a4 m+F?RS?3
3 30>R
 
fi($'ffI
 Z
 
fi03Mfi 04mfi?40 
!  @ ffIpff<fi(5@ ff@ ff
?Xv ?3 `_
fi 

fiP83
3 p82<
3M@ 7(bfi/
3 fffi @ ff3
4fi Iff
3
? 2p
3 ff<(bfi,?0 4v 3f 4 F'f0
 ff
hBA4. G{v pM*NvffwwlE!Y
 pYfi(N?pS4@ ff'C'1ffPPfi1p
 ffF
p
? ff
. ff'fiZG3v p?3ff 
@ ff30

fi Gff
3
? 2p
3 ffQ?Xvy (+.0 'ffhiA3ME

? N!*<vffwwlE!7T
H  ?pffv ff*?p024
0444;
fi(IP fi(>?pff@ @ ff3
fi ff
E3
? 41p
3 ff	4	@ ff'4
ff ( +@ 
fi0 ff4n3M'fiff!
r3M?pfi@ p
* Mfi 3[fi(J?pffz@ Q4fi@ ffC()fi?
 e
 Aff `_a
fi 	()F ff 0v p7?0-4@ ff
?p1'ffY!  fiO
 
 ?Mfi10Lv m?p23M@ @ p
 ff pffR(bfi>fi(bp+-@ ?>
E? pff!KY
 p


?Iff0?4} ff
 e
 0
*<>fi3ME>M
fi ff*O4 fi.fi2'.R 0 fi4./]^Ch~vffwwlE!Sffid
+  
 Mfi p
fi(O?pffK*v 0
43v 3  fi.fi4.K   fi./]^5O
 ?Mfi/*0@ >0044
0v f'fi832fi F1'ff$v 
+F?2
?>v p 5fi 9
fi34*?3@ Ev pN?pL4fi0MR
 ff?ff/fi<fi(%?pFfid 4a'/'ffK!
ffc Y

fi 'E'*0fi3Mffi
E?YM@ ff ff5?pL2O
 ff4v pff9fi(O4 >7304fi $'1ffP!
-fi 04p
 C?Mfid+@  _a
fi q_0	v 'fifi3MCfid 2[0 mfi K()O
 +-fi.!ffc ?4
()O
 +-fi.Shi NrO 3M@ 8vdlE*/?p@ N@ [fi pffi5fi [fi A+F? j  14O
 ffk802 fhB[ ( '
"F *PvffwwlE*L! !*Q04 C?Y R
fi v1344 
o ff
3Mff=v n@ fffi 0 m'fi#v 7 N 
o/
 ?
 1fi O
 -
fi 0fi ff!m|9
?&fi ]^P02 Z4I3O
 ff'fim
 	v ?pC(bfiEfi(f
_k4:;'3M'fiP'fihir  NlE!$r  fK?0ff R
 ?MfidT
+ 'fiXR
 ff
4 @ @ ff fi 	fi(
ffa%p[B<E^5'^'>V<'p`<~'OBp[B<\E\B^EdB'\^;^<EFB^5~
da%<i;B~O^L5-BEE~g\ELB<`B5'Q[;d^\$B\9%B~E>\BffdtE0i;B~g^NB<^dff\E
EE~VE\<~9d~B^EQ;
d

fi[

pI
B
 
 "
 !
%
 &

QrOrgsJcH[|$


*


04

t}-m{t/s$q



os0pt{a

l qM 
 a

ffERb0bigd0a B;
  (	ff
fi  f  
ff
fi    1`)ff1Va
 $#Y 
 1`  ffffJa`
p/	 A` ;   '
 E i  0    	( 

QHfsJcH[|$
 fi]) ^Fa4 

/

)

,+.- )Kff (   1   d0V`


)

p+04



*

10E '2 
ff
fi3 ~ff4   1 iff1Ja
5~
 & Ei/ # 'ffE $#U 1`  ffffJa`
 68
 7 /ff  P0)/ffL4)YE~B9'p/
a B;   ~EMffE  0   ( 

rO3@Pv;::`_0N7fi!

@ff
GfiC040=<''ffGhiA3M./?*Qvffww>@?BAL0M}ff/*Nvffwwx@?>AOfffG.fid/
?V*
vffww C@?arfi ff*tvffww ClE!
s F35R
#
 v 	+F4?K'ZvL Krgv 3M@ Pv!,T$3
? @ L@ >[ff'$P
fi3Ma Nfi(O+-d1F?$?p>r  
04 0f
fi32UR
 8(bfi
 ffG 444!FrMfiLfi p*	?13P G04 Gp
 ff 3[
fi32
 p pQ?pv 4
04 0!-TF?4$PdK@ ff13@ >
fi 04p
 a cfifU
. Mfi\+F ffp
 L
! U0
 ff2v pP 0 74F'fi
fi
 hi! !*O ff R3v p fi43M4fi  fi?0PElf?pCv 4g02 Lv RS4834ff
 /fi /:

 !,rMfi ff,h~vffww ClAfi3M4v pffA8fi/
 ff3M F()fi)
O
 fi1 pr f-?$4
 Rff
 N()fi$
 137R
 -fi(
fia ffP*Mv 
23v p8 YffU fi Yfi(t?pED<4fi pff]^5DQ4 ffP/!
[3P n04  p pCfi
 fi43Mfi  fi4?PC
 p
  fffi04 0?Y4'()q 
 ]^O
fi
 fi4I'fiRm? ?p
 @ !Wffid
+  ff*-'fimfid/4p
 	''2
7Rff?d1fiEX
 3E ff*-()fiP
`_
fi q44'fiR@ ff13 ff!T$3
? @ (bfi Y+ G3
 Y?0fi'fiRk
_ ff4v p?3Ufi ff*$?p
hi83blfi ta4 L?tR
 L `k
_ ffL3fi Fv 3,'fif3
 Pv p,+Fp
? ?pOJ2~$
_ ffg
4
fiR
 ff
hi'0L	 0G
 >lE!8c( Mfi*?pa4 S2Q@ 0@ ffXhi'RlE
! D,2 S 0N4Y
 MfiNM@ ff ffmv G?4
0R
 ff*4?Mfip
3 ?P94< P4Pfi6'fi02
5(bfi9()33M@ 5@ ff ffE
?!  a,Q?Mfi3
3 ?PL@ ff13 $'fiO
 

4_a
fi !c($?p@ K4Imv 3 fi *9?348?Ifi 3	r   04 X ?4I4-?4
`k
_ ffW W@ 0@ ffV*t?
( pffp
 ff!K
{ C
4<?048
  (1fiH=I,J !RhiT$?2
 Mfifi V*g7+ ff2,7fi?p
Mfi
fi C3 ffv ?pf0R
 ff*49v 
430p
 ffv ?pTfi8fi
( fR
 
o 7! lOc(?3@ f@ [7300 
 9?0-
fi1fiR
fi
 */+ [
fi 4p
 9;+-fi7%fi0444ff!ffc 	
  (	K
fi z
*  Pfi 53 ff,?pQO
 
834fi F02 *+F?2
?Y4f j fi130
kfi(6?p7 /43fi f02 !5T$?45832fi f04 
4Q()fiO
 ffR 0m `$
_ ffm'fiU@ (94Q4~$
_ ffc
 4fi067304fi L
fifi0v fi mfi
 ff!IT$p
? 
834fi 7a4 Z2>@ a@ ff(5 `_a
4fi Wfi130
 ff7  fi*6! !*<(i443M Pfi($?pC04 Z'fi
4(bKP0fiR
 ;!-+c m
 '2 
ff
fi $
* ff
E?mfi Qvp
 R
 3
 3 ffF4$fidT
+ Uv 0123a04 !
Ttfi8 E(b
 fi0fiR
 ffff*fi pQfi(J?3Qfi 5. ff5?3ffi130
9fi(V?3ff Nv /430/04 9'fi
()fi C834fi F02 !FT$?2$7304fi f02 U4F+F?Q4$ $
_ ff!FrMfi>
  
K
fi *fi p8fi
fi@ 7v /430M04 5 L@ 0 ffK(J?p>fiR
 ;	4)
 Mfif4~$
_ ffV!
f(bN?3I 4a4
 JhiElF?ff R
 S `$
_ ffm G@ a@ ff*?pfi >@ 8$
_ ff4p
 ff![{?4 
_$ff43
 ffh)fi 4v p\lE*9?pUfi 00Zv ff v p#h ! M!)
* fi43fi XfiR
 'fiElI'fiR?pffPa4 JhiEl

 3ffp
 ffhi'xlE!
s ff 0v pRPd&
 K ff34@ ffX'fiZ0?pYa4 &'fiZ?0  Y3
3 3o1R
 ff
ff
ML

fi

mknpokmkq

30fiQfi>'fiU_kp:B333?pP04V!cp($ (1fiNHOI M fiI (	ff
fi *J?3vpvUhi734blfi>04
4K0ff!cp(P
 '2 
ff
fi *N fi UY4	fid+Wr  7*F()Y+F?2
??pR834fi
h)fi/3
Elfa4 4N:B()fiO
 ff!Prfi74643fi *Jfi pfi >?pE04Yff`_$ffN?3p+
hi83blfi 04 Z'fi3M@ 	'42,4'$
_ ffI?pC@ ff30@ ffZfiR
 ffKhi'0P
 CG #lE!WW
" :
()fiPfi fi(8?pR834fi K04  0@  `_a
4fi @ R@ ff13 ff'fiq
 K2O
 : e
 
Q%fia >R
 ff
3 8?pG@ R
 (bfiEO
 ffUfi 04v p*R
 ?0[v m	? ?K2O
 :;
4
O43fi !
{p
? p $h)@ \lp `_
fi >(i44ff*Jfi/3
 ffJ[
fi3
3 @ oM0 9?g2J3 ff7'fi34p
 <?39
?fi4
 
fi(O U 0 L ff  pIfi
 E'fiFfi$fi?pf04 	@ a$L
 pffp
 ffZhi'8
 QlE!,TF?4Afi/
 ff5fi(

o ff
3MvpM*00vpM*M K  ()1v 3702 5
1
v ff$v p
 
_ ffPL
 3ffp
 ff!,T$3
? Qv 	(bfi/
35fi(
?4Aa
 f45'0R
 CP S!
"f04N@  E`_a
fi 7(bO ff  p$4JF. N'fif
E? /v p$4
 ffNfi g@ff'%fi  ffff!OQ3Mtfi p:
U
 fit2$'fioMPv 374Vv ff v pP
 ?Mfi10F G4%fi$fiR
 pK
4 ffF'fiCp
 EPv p
?p134
. ff'L@ E`_a
fi R
 ?Mfi1m()fi
 ff
E?Z
fi70v 4fi fi(-v ff v pKO
 ?fi1R 0fiR
 p

4ff!qffc #?4Ia
 + S@ ff p+  ff3?0
v q3 (i3$ ff 0v pmfiR
 'fi S
4fi
 3 ff&'fiRR
  j ( ffkW+F?@ ff'R
 ff
'fiW2%fi
4 fffi([fiR
 ff!ffc Xfi?p
+-fi*,4($?p	fiR
 pZ?Mfi40>()fi?pK04 Z4fi8'fiRv ff v pM*O?p&4I2I
 30 ff#'fiR'44
?Mfi4&()v ff v pMT! S c([ #fi 30 ffI?pff Uv ff v pmfiR
 'fiff*-+f449
  30 ffq'fi
@ ff@  I?pfiR
 fff+F?GUWVff `_a
4fi m@ ff30@ ff*%! !*J'a9
 C?fip
3 X
? QCv mrO 3M@ Kv
pff
 fi<R)
o ff
3MffV!<T$?464<?p$R
 ff'6fipF
fi304P?MfiR
 5()fi,v  Pfi 04v p530fi +Fp
?  504
@ff'%fi @ C2O
 U4
44
!#rMfifi?p ff 0v pmfiR
 'fi 0&fiR
 pZ
4@ ffPfi3Mm0fi
@ff30f 3 !LFfi\+  d*V(bfiQ?p
 ffN U+F?04
?S+ I?d p I@ ff34*a
+ I@ ff 
Mfi\
 ffZ
 Y UW[K \Yff U']_^a`5  `_a
fi  fi4?P!YT$3
? ff O
 ?fi18fi/
4} ?p@  E`_a
fi v 
fip
 ,'fi7d F4O
 Ffid -'fi0@  E`_a
fi ()fi

?V! 4 
{ Q4'fiN0@ ff ,Mfi\ ff0 fiE?
()fiT
 e
v K :B(bfiv p7344fi [04 V*(bfi[?p83fi hB
 '2 
ff
fi l$v Y+F?04
?U?p@ 
@ >8340 Qfi k
* ff
?S ff v 3I p
 R
 p
 !
T$3
? TMfi\ ff;Pfi(Jfi3M90fi
?	4 Mfi-P
E?v pFv ff v p>fi9 _a
fi 
 - */03M<E?p
?pW
 ?pff4	fi(8?pRp+9fiM! T$p
? @ R  30O
 fi3K4PfiK%fi4N042
fi 	fi(8fi3M
fi
?!<rMfi?
 oM0 */(V /3 ffX
 fi Nfi@ Tff
 QR
 ff?d1fiE,'fiI
fi>a-/30 ff*
+ 
pffC'fiO3@ F?5?3MO
fi Mfi?fi L3
3 3
 ff0v 51E3~:;4. fR
 ff?ff/fiff:
! Mfi?p)
 oM0 Q4
8Pv  pNfi A?-
 Pbko/a80,?pff4,04 0<'fiIM
 0P4
5
fi03vp14fi O
 
03M8+F?Mfi@ CR
 ff?d14fiI2p
 ff30ffZ
fi 'v pff()fifi
 Efi +F?v  ff
3M Cfi0fi 
MfiP !  ? o/a 4704 3fid EI?807'fi3
3 (bfi@ ff@ Z
fi 0fi >+F?4 
@ffP v pP+f?v Y
E4
J4fi Y0O
 ffR
! cQMfi?pY
 oM0 84f3'fiPffG()
'fiv ff
?CP'fi ff340O
 P()243M@ ffI03
fi p
3 Ufi
 Efi #+F4?v  ff 4F'fi  
 ffC 
fi?p$'R
 ff
`_
fi :
! [4'fiM*/?p@ L@ Qfi pfi p4
3fi 95?pBdT4 ff  a
 QW
" ff ff
E?
['fi/
4
fi #%fi3M23
3 
?0v pmfi0p3
3 P 33ffX ff?4
 ff'fiZ3
3 &4%fi'fi o/R
 4O
 !
T$p
? fo/
 E4O
 >+-fi34ZR
 C ff43M'fiPff*9 +9fi32W?138@ ff13@ fi?X0fi & 
Rff?ff/fi3M 
 ffff!
T$3
? Y4'4PfiPa44
fi X?+-Y+f44AO
 fi #4P X?pUMfiP &fi(f%fi\+ 
 4
 	ff ff
fi73

3 04
fi )
 pp+9fi.1!9T$p
? [(bfi44fid+Fv 384A  $?5fi/

3M@ ffffh eJ/B
 f8 gih0 
eg)  *  ff>R F/v*gvffww/v*aA3v 3ff  ff
fi alE!,+c vffww/v7v KWH j
+ c<fi.*0fi/
R
 ff ff
'4
L344ff
?S3
 ffP Zfid fiV!Gffc 'ffvpG'fim4'8v'fi/v pY?pC@ fi 09?fi'()2j
* 6TfNT
03ML4QfidW
+  p'fi>fi R?pPfi/
6%fid
+ 
 2!PT$?4[+A>Y 360fi *g03MN3
E?

k a%^V\'9EMdBbKl2dB~Bff^-^'B^A;i EB$B^EnmEWo\Bit^Bff\\~'Fffqpg;\8iNrOr=s;
fft v)u \B~5~;5BffdVB6E B~Q\'[^[~E5dBV~^~O`Et^5\Bff^<BOB^512l 15
w ~^~\$E m BE
Kx

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a

4fi>@o1Rff
ffR'fiUff
fiv
@ffvpG3M'fiPffvm?pP(i3M3M@!fLY@ff3Nfi(
6TFNTL]^8
4
fi *6?p@ C+A8Gfi/
-fi\+ Ifi\ fiX  6TFNT fi'I7fidW
+ %fid
+ ff*6+F?2
?
@ff30ff C7@ ff./Mfi\W
+ Cfi(J?pc6TFNTn@ 4fi %
fiP7$
3 4
fi ?
 pp+9fi.a!,TFp
? f fi 
 p~:
+-fi.@ ff./MfidT
+ Wfi0fi ffW'fim
@ ffK
 0fi <0@ ff.1fidW
+ v R
fiP83
3 2
fi 8'1ffP!
T$?2$@ ff./Mfi\W
+ S2'fi'v @ ffG()423M@ ff$fi(< Yfi?3Q
fi 'fi
 pp+9fi./Q
fi[?pI
fi3
3 '*
3
E?	A?pNA'e
N
fi 'fi
 p;+-fi.X
! [9'Eff ff
 4fi +F2p
 f+A5?13MAMfidW
+ V!6+c C?pQ(i3M3M@ *
<4<@ ff'fi 0v 5'fio1R
 ff
<?09'fiO
 Tp;+-fi.
fi 'fi4 6+f441R
 $40 ff
 ff3v pN7300 *
4'03MffS
fifiR
 pK'fi(bp+- fi ff!ITF?4T
 o/a MP4
2U4423''ff[?pI%fi/:
4,13 p042;Yfi(5fi3I
 fi 06@ ff'fi3ME
 ff73
3 0 ffN?pff@ fi 84'()G
 ^@`y`Afi(5?pP()fi4fi\+Fv p

E4/6
fi v13Mfi3
 
o ff
3Mfi z<Pfi 'fiv 3Ma
* bko/a F4fi 	'fi8(i443@ ff*/( p{<\ ff44042;*
 K2O

 ff ff'%fi  ff!9Q3$fi
E? 3M ff5?Ffif4'()C4%fi(g?pff !
T$?04V0R
 g4Vfi@  0} ffIJ()fi4fi\+F!  ff
fi I5fi\12p
 fft 84423'' :oM0v ,?O4g3 ff
?Mfi3
3 ?Mfi3Mg?p9a
 d!  ff
fi |>f?g?p?pff
 ff70

. fi3
3 07p
 
_ fi Jfi(0r  [*fiR
 p
;1R
 ff*F()fiPf `_
fi *F P
E?v pm ff 0v pWfiR
 'fiE! fiE$@ ff30()fiK'R
 ff
`_a

P
E?v p- ff v 35fi
 E'fiO@ 9v   ff
fi 7M!gTFp
? ff 9v ff v pFfi
 E'fit4g3M'fi'fiIff3
 ff
 ?pm}   ))nE0))  'fi/
4ffX+F? ffp

 ffa
!  ' 4fi Z
fi 0fi 'R
 ff
`$
_ ff7?p

fi 4fi 3
3 p
 O+F?04
?N':B'fi:;N' 4fi 8ffIR
 Ap
 !O
{ F@ ff@ 6%fi4 AQ0fi
@ff30V()fi6'fiO
 -fi(?pff -fiR
 'fi*+F3
? @ 9 j %fi -FEfi ff3kfO
 ff t?O?p-v ff v p
fiR
 'fi[@ ff  fff'R
 ff
`k
_ ffK
4$fi(60fiR
 ff!5Y
 K?p7fi?pf? 0*0
fi3
3 @ oM0 ff[@ 
@ ff@ ff&'fiR?Mfid+ ?0P'fiO
 Kfi([?pY ff  pSfiR
 'fiEPMa
fi Mfipff
 ff4Z@ ff  C?3ff 
fiR
 vff!  ff
fi U
x o/$?p>Efi@ ff34A(bfi$?3>7304fi $3fi U
 '2 
ff
fi3 !

rMfi74O
 ffN+Fp
?  8
+ Ifiv p4 P0fit@ ff34* ff
fi XCCfi\12p
 ffQv 
@ ffO
 
 fi4?PL()fi> :B(bfiv pY?pC7304fi >04 Z R  ()1v 3Y*6fi pY+F4?WY+-fi'~:;
@ 

fi0v o/p 412C 0
 ffP04
f4O
 
fia o/4;q ff3!T$p
? ff0E4
F ff3	?Mfi\+
83
?& ( :B0244fi /:B()fi4R
 ff03MZ()fifi p	fi(F?3	 
@ ffO
 5 fi?0P>fi\ '0
.
`_
fi !RT$p
? 0R
 
fi 
43p
 ff8+F4?Zm4
3fi Rfi(F@ ff4ff+-fi.Z 02p
 ff8(bfiI(i3M3M@ 
@ff ffE
?!

~OM  -fik.W  

{,RvL+F?Lf7344fi#o/a<()fiO (	ff
fi fiO'2 
ff
fi3 ?t2V3ff>?Mfi3p?Mfi3V?p
0R
 A'fiP44430''F?3Q3
 k
_ fi 09 	43
 ff!<T$?pQff
fiYACM@ffvp'2 
ff
fi3 *
+Fp
?  I8340 7fi L?d P?pffQfi\W
+ v p
 R
 3
 f04 !7sJ>v S?pP ff
4fi m+ P@ff
  (	ff
fi *+Fp? @ cff
E?Ufi f3@ ff$ @ fiv $832fi $a4 !
c~Pfi v p$>
 fi>+Fp
? @ $N ff?04
 $?<2 p
 fffi PL04 p6()fi<?p$a3M%fi AfiR
( o104fifi 
 L0 <
fi4v ff
fi *(bfi

 oM0v <Jv N?32DO?/k
_ p
 V4fi Q'fifSff!gst. <?p2DO?/
_ p
 ff*
?p@ P2[K2 p
 hi
4 fffi  j stkl[(bfi+F?4
E?GKPfi04 8fid I
 ffO
 @ ff!ffid
+  ff*Ov G?4

 N?p L@ N;+-fifid E<?pN(i>h j rOkl-fid F(bfi$2'?o/0fi4fi *M 	?pNv O
 ff4
h j c~klAfi\ F()fi$' '(  pP Y0 ffA()fi rX'fis,!
{ U3O

 Y #fi 3ff pP?3
  fffiR
 ff&?pYv 44,a4 I()fir-*6cE*< qs<*<?Mfi\W
+ 
v rO 3M
 ffL	 X
 >1!PT$p
? ff P P402`$
_ ff*?pN? R@ ff44'2
*04 
 U()fiN?pP03%fi 7fi(
4423''fi V!JA4
4*fid ,rG4j
 ff?p6
fi4 ff
pN0 ffO <hiv I'f$QstsJ|-5T5cHNLlVfi
,2,p
 ff44 v pN?pff'fi7fid -cFh)+Fp
? rW4<v ,'QDQ|,sJ,c :Q|,"FcHNLlE!"Ffid Ac6
 ff4?p9R
 
@ff
 ff/v pP0 ff= <0P()fizfi\ Qrh)+fp
? YcA4F Y$"f|-A|,,c :[cHN\l5fi[F
 U3
 ff4 
?pff 'fi4 p
 fsnh)+F3
? Y$4$ Y$DQ|,sJ,c :Q|,"FcHN\lE!-c(OsZ4Fv Y$"f|-A|,,c :[c~HQ*
d

fi

K bl ^^~'  { 
bl \^p1

RR 

mknpokmkq

rJ:;
fi2ff
qcp:;pff4

$QsJst|-AT$cHN



/

ff4

ff4

RR  ff2
 {  "F|AA|<c1:fc~HN   ul4B~~~^
/ rt:;pff2
cp:;pff44
cp:B@ff
ff
 s:B'P
sV:B'
)
 DQ|,sJc,Q: |,"FcHN   ubl \~^^1
RR  ff2

)
K lb\^p1  DQ|,sJc,:Q|,"FcHN 
RR  rJ:;3ff4qcp:B@ff
ff
rO3M>12,
D 40A(bfiffidfrhi(bEl5KcNh)?ElE!
RR  ff2

K 4l ;E5{'  T$"W[H  Uc~T$T5c~HN   
c;:B@ff
ff4.
l2E

sV:B03

ff

4

@


 
/

 {
   K  l2E\
c;:Bff
ff   
ff2
D
R
d  c~HQ
s:B'0P
 
)
K l4B~~^ {   "f|-A|,c,:[c~HQ       c;s:B:B0ff
ff30 .
c;:;3ff4 R
R 
s:B@ff
ff4

rO3M>12D,4Y(bfi$?3L4pFs,!

?pK5
Kff
ffL?pLP0ff=<8()fic!a[?p+F2*MsR
fi34	RQ03''P4'vpI
'fi|9?WhivY>T$"W[H  Uc~T$T5c~HNLl9fiF030vp8R;+U
fi8hivK'.Dd  c~HQ>lE!
[6O
 fi pffI%fid *a4 g@ 5@ @ ff@ ffI3 p[r  [
! Tr  ?Ofk
_ A 6fi(aff
hi! !*?p> 4
 ffElF U44fid+A0 7:B'fi:;'P' fi >hiB! !*a?pI ff
ff\
 ffp
 fffR
 ;+
4
 fflE!>T$p
? 803M%fi 7fi(,?d1v 3C'ff>4F'fiK4143
 >?pIfi ]^Lfid 4O.Sv 'fiC3M'./!
'N+f?
 	v 0
fiPv p>fid+ Mfi-()fi Pfi?3$'N4A G)))  B;
! D,4 
o ff
3fi 
Rv 5v K Uv 4%'!
D94 Oo ff
3Mfi fi/

3M<,?pFfi ,. ff9
fi *13
?,fi 9rm./v pL
fi rJ:;
fi2 ff

fiPrt:;p
 ff4 d!|9
?#fi ?m@ R
 'fi4@ fi([%fia 
fi 0*930 Ifi(F+F?04
?&PdZR
 
. ()fi ff
?Rfi(9Q'ff
!  a4 mp
 ff pN
 'R
 ff
()K?4Q30 Q(bfiY
 ff
E?W'!8T$p
? 

?fi4
 Pfi(A	04
32L
fi ()fi?4N30 L4Nfi/p
 ff ffmv m?3Pr  
 fi p
 P 4'4
!
c[4[30O
 ffG?Q()3M?pf
44/
* MfiQ'R
 ff
`$
_ ffU3
? @ *@ 3 ffS'fi	. 8?37
_ V$
3 /:B4O
 

?fi4
 Lfi(6v pv L
fi K()fi P'!
s [3['R
#
 ff
()	?p7 [fi(<
fi [()fiT
 ff
E?mfi(6?pIfi hir-*%cE*sgl5 Yfi3M
 o/P0 !frq?
;+-fiL%fi40 A
fi Ort:;
fi4 ff
,rt:;p
 ff2 ff!OT$p
? A_0O
4fi PO
 ff 6?0,rm
fi4 ff
<P0 ff
 z

 <\fi/*< 0W?p	 ff
fi 
fi XO
 ff 8?I8pff4 7?pff 	ffP7'fiGcE!O"$fi\ cL4'fi
?Ip+9fiW
4fi Kcp:B@ ff
 ff Y Xcp:;p
 ff4 d!T$3
? _a'
fi #O
 ff 0Ic>@ ff
 ff ff0v ff= <
()fir-*6 Z?p	 ff
fi XO
 ff 8?8pff4 7?pff 	ffP8'fims<!Os?8?M@ 	
fi s:
' P*tsV:B03 *O Zs:B@ ff
 ff !YTFp
? _0>
fi O
 ff >s' PLS'fiS|9?*g?p
 ff

fi 	?0$9a3 ffAR
 ;+	fiR
 fi *M C?pQ?4?0$A@ff
 ff ff5a ff= <8()fi
c!/rfiL
 ff
?Sr  >*/?pN 5fi(g4fid+A0 N
fi 05(bfi ff
E?U'L2-
 ff
$
_ ff	v 	rgv 3M@ ff5I 
 >

=K

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a

vmP2t()fipo1Q'fiK?p'!rMfio/a*fid8r
RfiY.
4fiRrt:;pff4Q()fi
$DQ|9sV1c :[|9"$c~HQ!
T$3
? [} ~  biKabi  hiB! !*?p-4fi 4

 o1 fffi g4R
 ff4v 3$?pffp
 ffEltv 8 8r  #04 
pff
ER
 [?3> $fi(6
fi F?La LP':B'fi:;''E fi 	'fifi1

3ff!,T$p
? NfiR
 'fiB
 
Off  j fH[D8*^
k XO
 ff  j N"L*^kP   
 ff  j HNQTN!^kPT$3
? Q
fi 0fi  j ff4 ffkP+F44aR
 Q3
 k
_ pff
?Mfi!6T$p
? $' 0fi P
fi 0fi ,fi(fi pffi A
 @ ( -'fi>?p[
fi 9fi(fi pFfiAfi@ ffi?p
fi!9T$?04A45R
 ff
3 cff
E?Ufi f453O
 ff	'fiPR
 N@ ff
 >'fiP+F?$$?0Afi0  ffUfi?p
fifMfi pM!6cp
( Mfi5/4a *1fi [
fiP7$
3 4
L?pff4$
fi U
E?Mfi4
 !
 0
 P Rfi ]^8
fi W@ R
Y
 'fi@  >4fid+A0 
fi 0L()fi ff
?Z'?d R
 
pk
_ 3ff* j ff4 ffkS
 RR
 Pp
 k
_ 3ff!T$p
? 'E fi W
fi fi  j ff2 ffkU4R
 ff4 pC Wfi3Mfiv pffp
 
()fi8L4A C0@ /4fi Cp
 Mfivp8?pN Afi(t20@ ffv v p8
fi A?5PdR
 [. 
()fi?p'?8@ Mfi8@ ffMm
fid  ffZ1mfi?p>'E fi W
fi fi 0!rMfi
 o/P0 *
v rO 3M >1*ts<]^L?M@ '
 fi 
fi 4fi Q()fi'TFW
" [H  Sc~T$T5c~HQ @ Gh)c;:B@ ff
 ff4 
s:B'E PElE*Vh)cp:B@ ff
 ff Zs:B030 \lE*M  j ff4 !^kRs
 	fi 4. Ls:B' P-fiAsV:B03 [()fi
?4A'!-Ffi\+  d*0fi\ $c9
fi32. >c;:;p
 ff2 $v 'ffCfi(gc;:B ff
 ff !,T$p
?  (bfi@ * ?04A
 
 ff13 A'fiSh'h)cp:;p
 ff44 q
 Xs:B'E PElh)c;:;p
 ff2 q
 XsV:B03 \l'lE!
j ff4 ffk4)
Kr  02
  @ ff A 5fi(t44fid+A0 Q
fi Y ff1p
3 
 ff!6ffc 	02
34ff*/804 	4-?p
 5fi(O4%

fi Y ff1p
3 
 ff$?5Rv 	v K Kv 2a'7 	fiR
 ?pN' 0fi 	
fi 4fi !
 o/P0 Y

fi  ff3
3 
 Y4fi\+ ffqXr-]^P04 &2Yh'hirJ:;
fi2 ff
cp:;p
 ff44 lE*Qhirt:;
fi4v ff

cp:B@ ff
 ff \lE*<hirt:;p
 ff4 B
 &cp:B@ ff
 ff \lE*O!!! lC+F3
? @ >r&. ffQ[
fi [ Yfia  fffcE]^[
fi [
ff
?G'U 	?p> ff1p
3 
 !
A93
3 /:B4
 *?pff fr  04 0<@ Fv  ffPv I?p$()fi4fi\+Fv pL 3pff
! AX 2
@ 
4O
 PV#
*  fi hir-*Vc*Jsgl[4N>fipPfi(-?pP'ff8v Q04 V*V RLffv ff
N?ppo/

fi 'fiL. X
! T-
?Mfi1fi $?3ff,
fi ,v p
 R
 p
 !gT$p
? IM
fi MfiXpffP'fi7'
 
?fi } 
fi	
fi 	
?Mfi2
 !,T$p
? Q
?Mfi2
 [fi(t
fi KP ?,R[0 ff*1(bfi?
 oM0 */fi 	 fiPv M03<()fi
?p/fi O
 ![?Mfip
3 ?Y
fiP0 P02 m+9fi32v 
433
 7?pa4Q()fi>
fi W
E?Mfi4
 *O
 fi pffZfi\ *6p
O
? @ P
+  ffd C733'R
 ff
`$
_ ffmv R?pr  04 0!CQ3M>fi  (bfi8Mfiv p
?474L?0>?7?p()fi1
30>fi(5?2N0R
 74>fi W?3 `_
fi Rfi(5fi
 ffL%fi3M7
fi@ ff


fi S ff3
3 
 ff!,T$3
? N04A()fiF
fi S
?Mfi4
 745@ ffv  5'fiP?pff >fiR
 ff!
 0
 ff
E?&fi ?I
?fi X 
4fi *<4-fi @ K3O
Y
 ffZ'fiSfi0@  	?pC
4fi 
fi(<?p8fi?pNfi Q?N@ 8
 fi 3ffSv G[r  ' fi U
fi 0fi ff!frMfiY
 oM0 *r-]^
' fi 
fi 4fi NO
 fi c]^L
fi ff*t'fiYr pff0N'fiYfi0  P+F?NcQ4!A fffi Z
fidT
+ 
fi C P?fi Ffi(%?pffi?p,@ ffv  9fidhiElE*M fi 9.Mfi\+F<?ppo/,'['fi>+F?2
?
Q+F42V' fi !LT$3
? @ 4[fi Yfi p%fi0 3o1L'R
 ff
3@ 8?3Ir  [N@ P3O
 ffG'fi
Rp
 EPv 44
!SrMfi
 oM0 *<4($r=48v IFQsJst|-AT5c~HN '*9 Z48
E?Mfifi@ ff
fi 
rt:;
fi4 ff
*g RQfi0  ffLcF./v pY
4fi Rcp:;p
 ff4 ff*?3RQ+F44g'dm >$QstsJ|-5T5cHN
'!TFp
? fi/
 ffNfi(-R
 ffv pKv mY'*O
?Mfi1fiv 3Y R
fi *tfi0 /v p	?pP
fi 0Nfi(Afi?p
 *0?pUfid/v p'fi
fi
 po1f'*a4A 
 ffffSv p
 
_ ff!
 fim(iff*<
+ K?ff KR
 X3v pm
 '2 
ff
fi3 +fp
? @ ff
?#fi P?8fi\W
+ &v 41403
04 V![c(<
+ 3O
 P
  (	ff
fi *?pff
?Rfi L3 ff[?pO
 834fi [04 S'fiYp
 ff
4p
 I


fi ff!  832fi I04 48()fiO
 ffX1R.1 p j fi/3
kXhip
 k
_ pffXv 
3Ma ff
fi >1!4vdl
fi(<?p804 0$()fi[r-*cE*% Gs<!aTF?4$0fi103
Ffi/p
 ff4f?p7
 
E?Mfi Mfi30FR
 ff?d1fi[fi(6?pIfi ff*
+Fp
?   j '
 0
?Mfi fi3kKO
 ff >?>ff
E?W2O
 P'a
  Rfi >. ff8 R
fi V*Jfi0  ff

fi [fi(<fi?3[fi *V S?pS' 4fi $'fiKf
 po1['!NT$p
? 7fi/3
$02 S4F(bfiEO
 ff*
ff 444*11C.1v 38?37Aff4
 	0fi103
9fi(g?pNv 01233M'fi'fiU'ff$ 0	?pLv /:

=

fi

mknpokmkq

ff
4fiUfi(O?pL'EfiK
fi0fiff!,S304fif
fiW0L:B'fi:;'P'fi

vW?p0fi103
L04V!UrMfio/P0*<(A?p	fi @ fivm.	?pC
4fiIrJ:;3ff4IWcp:
@ff
 ff Q s:B' P*?34afi -+F24M' fi()fi?p @ fiv9'hB$QstsJ|-5T5cHN8*
"F|AA|<1c :[cHN8*%T$W
" [H  Uc~T$T5c~HQ>lA'fi	?p @ fiv Q'UhiDQ|,sV1c :Q|,"$c~HN8*DQ|9sVc1:[|9"$c~HQ8*"f|6:
A|,,c :[cHNLl5@ @ ff ffY1K'a ff5fi(6'ffQv Y?3>r  ff()fifr-*0c*a Ss<
! 832fi f04 

fi 2'[fi(,?p Qfi(92t
fi @ ffp
3 0
 ffQ?NRv S S @ fiv Nv04V'fi(9?pIfi/3

04 K 0KfiR
 ?p>' 4fi C
fi 0fi ff!
{p
? ?p6?p543fi I4<
 2 
ff
fi3 fi9
  (	ff
fi *[832fi O04 pff0O'fi[R
 -(bfiEO
 ff
'fiE(bfiaV7304fiF
fifiEvfiYfiRff>hi@7'mfi(6rO3M@vdlE!::_a
fiYfi(
fi0Q0fiRff	
fi4Kfi(8.1v3+f?p?pW)9fi(8?pR
fi=ff33
ffY4fi\+-ffn?p
fi/3
A02 K4'()?3LfiR
 p!

 3-
4tfi(ghfi0blJfiRvffVfi(0a4
34g4%fi
*+f?4
?84gM@ffff7?3@*4t?
Y
fi(g(bfia4pC7304fiA
4fi5?5+N+-5fi3M$fi$'fi4+-d1$dfi4*
4vff \i0E
fiR
 vff! T o/P0 Y40fiR
 ;DFv  h)c;:;3
 ff4 
 sV:B' ElE*9+f?4
?q'ff?0
?Mfi304Y+Aff/[
 >?p7
 I?fc$M
fi ffT
 Mfi[pff4 [f?37O
 84O
 >?[sX4F' 'vpM!
T$?2f0fiR
 ;Y0@  Lfi0 fff?0LPffm4 I()fi?p4 3
 L483pfi3K@ ff
 ff/v p
p+0	(bficf+F?04 8'
 P4'vp	fi4p
 LY'fiY|,?!PTFp
? P ff
fi R2%fiN
4L/:
M@ ff ffUp
?  74 &   /  [fiR
 vff!5T$p
? ff 7fiR
 fff'I?f(<04
304f834fi 

fi &h)?p j ' klf?ffi/

3M@ ff*?p  304U Mfi?3[834fi Q
fi &h)?p3ff
 ff~:
 j  ff'%fi  ffklQ+f446fi/

3Mff\
!  oM0v 4L0fiR
 ;XD-18c($rt:;p
 ff4 8?7fi1

3M@ ff*g?p

 34	sR+F24
 
o ff
3M8s:B@ ff
 ff4 !
c(%?pF02 ,v rgv 3M@ ff9> 
 >L@ [
fi>0 pffv 'fi777304fi ,04 V*+F44/?04,834fi 
04U4'()	fiRffDfv>D9'+vp?04$13pff'fiS45fia0	`e
34-fi[4%fi~:
a >()fiNfi'[@ffp
 N(6?38p
 EPv fi G4[0 ffGfi G/43gv 'R
 ff
fi Ufi(,?pIr  [
! c
?p@ @ fi 4mU
fi3M0 Pfi(A RP4*g40v Ir  [> ?4c
 oM0v CT$?04L4423''ffN?Mfi\+

 GP( +20 Lfi *+Fp
? U 
pM*a
  o/?00$
fia \
o fi0J
 ff?0ff/fi*?3@ 
P./v pfi06fi LRff?d1fiL0`e
3f'fiK@ ff2
!Av ffY?p I2N pffm(bfi> fifi3[R
 :
?d1fiE
 3 ff
* ff'R
 ff
444G>?p137
 7 R
fi0v o/pmfi($fi 8v 
@ ff@ ff!KUfi/p
 ff

3
? ff
./v pY(i34S3M'fiPff7?4Qfi/
 ff
! [

fi p	'fiYfi3M>fi/p
 ff6
Ep
? ff
. ff*O?p0fi103
Q04 
()fiFr-*cE* 0YsW2~$
_ ff5fi
 ff
 DFvL 0
 D91!
"ffifi3f3ffY@m4'fi pff3ffn()Yff0vpM!  3M%fiU2p	s<]^C''
Pfi
 ff!ZT$3
? Zfi 3	v ff v pSfiR
 'fi?P
fi34R
 	04 ff48'fiRp
 ff Ks,]^
fi 
s:B'E P*5+f?4
?q?p@ ff()	@  ?04
fi n()fi R
 ffv 3R. (bfi 'mT$T
" fH  :
ScT$T$cHN8
! f(bC0/v pm ff  pmfi
 E'fiff*9  `_a
fi qPffXR
 K@ ff13@ ff!XrMfi?4
02
345fiR
 'fi7hip
 ffv vpP Y
fi lEk
* Mfi@  E`_a
fi Y2)
 pffp
 ffWhi   ff
fi KlE!

ffc U7344fi[34fi*+F?TfPfi1_$ffK1	vffvpm{n?Mfi(bfiEPFU`_$ffF?p
fi/3
7r  QZ+F?MfiS(bfiL@04>(A_a
fi()24*OW+f?747@04@ffaT$?p
'
+ Q'fiY?3ff p
3 fffi Np
 R
 Gfi m+Fp
? ?3LN4>
  (	ff
fi fi7
 '2 
ff
fi3 !7c(A
  (	K
fi *
?p>fi F+f?K?p ffff'f
fia3Mfi Jfi\+ d3
* ! M!*2 p
 $sv 	fi3ML
 oM0 *aPv v 
?pfi130
f02 mG0/v p	 ff  p'fiY4*V ()1 pC4*V@ 0Ev p4Lc
 pffp
 ff*J 0m?p
 0v p7I
fi1fi(g9'fiP20fi(t?pNfi
 5'fi3@ !6cp(g
 '2 
ff
fi3 *M Cfi $02 ff9v ff v p7'fi
6fi\W
+ Pv 0/4304 !OT$p
? $v 0/4304 O@ $?3P <'fiL?pF
fiP03Mfi 2>%fid
+ (i3
 *+F?Mfif(bfiEPg?pAfi/3
t07 E`$
fi
_ fft?OfiR
 ffg@ 54~k
_ ff!Jc(@ 04t ?3ffp
 ff*
fipLfiffi@ >fi $@ a5?pff$fi\W
+ Yv 0/43004 !

=J

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a

c$4$3O
 ffKp
? @ N?0FP
?0v pLff0vpIfiR'fi$@704ffCfip:;~:;:B2O>R$fi
?pO?Iv70
E?*(a 
K
fi *?3Afi6
fi:fi$a4tL./vp[3M0gvffvp
yh Dgfi'ff*VvffwwlE!, fi C?pff [3Pfi*?46a-Mfiff-Mfi9(bfi/
3,fi?p[ffpLfiR:
'fiOR
 < Lh)fi?p<?0 I'fiLp
 k
_ pA?pfflE!OcO(bfi/
3 ff6v 0'ff8fi ?pAfi3M
fiO
 $ ff3v3F()fi?p
024
fi Gfi(-Kv ff v pCfiR
 'fiff!Iffc m04
32ff*+ P0M@ ffQ?p  `_a
fi 2p
3 !8T$p
? 
po/Fff

fi \ ffF3@ ()30
z
. fi3
3 U3
 k
_ fi 0 pff3
 ffK(bfi[3
3 p
 E'v 3>@  _a
fi !

|    ,

{

  R

    

T$?2 ff
fi fid/4p
 ffp
 k
_ 4fi Ifi(Nr  f*Afi
 ff*- `_a
fi V*A 0qP
E?v pUv ff v p
fiR'fi!Yrfi8S
ffff*O33703Mfi3>33pvp	fi(5?p@ff34Lv?2L0Rff*gPfi(
?pff 7p
 k
_ 4fi A@ N(bfiEP!

 }b ez {] ]G ekgz^KE]k^

r  [L?ffLff'N()fi3MLMfiff7fid8
44
g02PhiHf24'fi*6vffwQy?6DffZ {ff42P*
vffww/vdlE!YrMfi8fip*O334.P
24
6a4*t?pP;1RPfi(A_k4:;'3M'fi'fiW02L3ffW?p@
4fi\+F[%fi42Yv /k
_ 0Chiv 0p
 Pv 0\l$ 3?G
4fi  ff3
3 
 ff! &T$?2Ffid/4p
 ffQf
 fifi/
fi/p
 ffAfiW
( ff7R
 ffp
 ff&fi P?P Y
fi v1344W@ ff'%fi  'fiW?pffO
 1fi O
 8+f?Mfi3M
U`_a
2VP fi Y'fiC?pff[R
 ff?ff/fiff!f|6
o ff
3Mfi G Sv ff v pPffUR
 > v ffff ffmv 
3MP 3pd
! TMfi?3A fiN4,?0Ar  04 -?ff N'ffff* ?3f02 p
 ff p

 	3 [?pff N'ffA'fiI@  ff A3M0'.19fi(V?pQfi\ 4'.!,T$?493M/4p
 ffO?3f02 v 'fi
P2 -$
3 *?p@ 1%fi24Pv 
 ffv p7?p[
fiP@ ffp
? 0444;8fi(J?p[04 ff!  ff54'fi
0v I0`
 L
fi W
?Mfi4
 ff>>`R@ [2O
 ff
*  R4(,?pP 0'fiS M03MQ@ ?pPO
 !
 ?
 fifi(5r  04 >4Q?>?3m P04
304Y
+ ff4`:;3ffm'fiSPfi1p
 ff2v p	?p

fi 
3M@ [Rff?ff/fi[fi(,830v Lfi 
! G0'
 137R
 ffi(,v p :;fi fa4 F
 mR
 
  fffiR
p
 ffP p
 R
 p
 > ?pP
fi%fi ffv 'fi>L'
 
?fi Mfi3<832fi 904 Yh)()fi,+F?2
?
fi0O0fiR
 ff[PffGR
 8ffffalQv mK''v ?'()fi+AP 3pd!7rOv 44*%r  04 Q
 R
 
`k
_ ffR3v 3C?3P m%fi034N 
 Rff
 m /BB 4)/ 'iYPfi1p
 ff6
Ep
? ff
./v pSO
 ?Mfi/*
! M!*a@ AL3M?
 h~vffwwlE!
2M
 fi8fi(<r  04 Fffi0fi@ ffY'fi
24
t04 F4$?[?p@ 84F
 @ ffQpff
fi(J ff ff
?Y?$?AR
 	Mfi pQfi K3M'fiP4
2()fiPv 38
44
%04 0z
* ! M!*a L
D ff Y 

{ff44 =h~vffww/vdlE!cp4P3
3 
v ff?Mfid+ 730
?#fi(Q?2PP ?PRY042
0 	'fiWr [!c
 &?p
fi?pC? * fi43Mfi  fiE?PP
 qR
 Y3 ffX'fi fi4 Sr   04 YhirMfi ffB*Lvffww ClE! 
4M fi[fi(Jr  04 0,-fi%fi ff'fi702 ,
fi%fi ffCfi(VE3 F A49?-?pQ4'5Pff
o/@ ff6[04
 Ifi A3

v 
!OT
H  ?pff ffO()fi604 0J?6ff34@ 9()fiP1 _a
fi *r  [
@ P0@ ( E0 R
 ff
3 P?3
fi0 oRv 
fi 0N?7
 Wfi1

3>R
 ;+WE3 ffNP. ?pff

Y?EY'fi E(b!5rMfiEPJ E`_a
fi U(bfiQr ff4F137'fi0?44
ffU U+F4p
 ff43 ffSv 
( ;:;
4
tv 30''4042
fi !
T$?04<3Ma ff
fi *+F?4
E?46a fffi AL3ME? Uh~vffwwlE*ME $b0I3PPE} ff<?pf04
,fi(?p
r  [830 ff'fiRfi13
 ff9fi 04 ff!mrO 3@ ffS 0
 >G4243''?pKp
 k
_ 4fi !mT$?470R
 

()fi1
3@ ffLfi r f>?7fi/p
 ff9fi 8+F?WY%fi24v /k
_ 04( 4O
 *J @ ff ffW8 
v /
_ :; p? j ''Ev pkYhi! !*0 ff3
3 
 Lfi(O
fi ElE!

dvJ~B%`O~BWmVBqo\BO^^ B^5~J$'$O \f^9pg;\EIiNr=rOsdNr=rOr;
=K

fi

mknpokmkq

()fi@	3vpUfi3M804
3fiRfi(F3'fiP/*,+0$b0ff>'fip_pC-fi1fiffX`:

/!X|6oM0vffI?Mfi3p?Mfi38?0480RI?0ffU3M'fi'fi#'fiX
fi4fio/@ffff

v 	-fifivffY/*MRff
3@L9fi1fiffYv3


30PP}ff-?pff@N'4fiK
fi`:
fiff!<9fi1fiffS452'fiP3(i3(bfif3

v
4o/@ffpI?pLfiRvff!6r3M?pEfi@*
C4ffvC()fi	3'fiXpff
ERY;+-fiXfi(N?pG
@ffOF`_a
finfi4?P4([+m3
-fifiv ff U EO
 Mfifi !,TFp
? @ ()fi@ *
+ N $b0C3P} Q?pL04
Afi(69fi1fi ff S /!
-fifiv ff
 Z 0
  4>Y >fi?
( ff ff
 >+F?R2'vp32p
? ff ff ffO
 >yU 0#v*g
4fi ff
3 3
3
 <?pF-fi1fi ff A
* -*1   fi
 Efi * 04(b/v pN?p['EfiR
 fffh  .fi'./*
vffw CwlE!9rfi
 ffv ffO
 K 
 7fi
( Ia
* q.I49
4 ff?p7PEfi
( 	 0
 aa
* q.I49
4 ff?p)
fi'
(  
 0*1   C46
4 ff?37Fa4Pfffi'
( J!6rMfi9?Mfi $ ffp
 <+f?MfiL@ F3
3 ()P244g+F?
-fifiv ff Cv A +F?Mfi>+A -'fi
 [v 34fi P()fi9?pff@ ffiR
 fi 0*149Pdp
? ffC'fi82Pfi v p
?ff
? ff ffO
 >fi
(  47ff(5U@ *:! M!*<U 8fi($
fi !S
 * @ fiv *O 
fi0 ffO
 
+-fi34Z?pXR
 C v ff
4fi *<3
3 fi V*< X
fi0 ffO
 *<@ ff
 ff
4 ff!R|, ff
 Iym v*
v W?4>
@ *6+9fi32RR
 ?pfffpR Kh}lL Z?pC 7fi(f4
 ff ff
 8v R?3C3
3 0  S,h LlE*
@ff'R
 ff
 ff4!
T$3
? U-fifi ff  @ U3O
 ff&'fiZ
 	
_ !qT$3
? @ U4PR045fiEp
 fi pZ?p
ff ffO
 '
* L*1+F?4
E?49p
 k
_ pffCZ
 4(% 0fi 4P'
( q.P
 J!6c9Pdp
? ffC'fi8?v M.Ifi;
( 
 fi fi30f'fi()fiQ !fTFp

? ff ff
 [y 0Wv8@ 83
 k
_ pffU
 zI*%
y t* 
 zI*
8v!AT$p? ^a]1V3\Chi fi fi3F'fi p : ff ffO
 Fl-fi;
( Iv
* 9yh >lE*%@ L?3Mfi M} 
fi ff ffO
 
fi
(  Pv 04P+F?K ff'R
 ff
$'fiL!9+c K?3Lfid EW
 oM0 *afi [r-*cE*a Ys ff
?G?ff 7?pff
fidT
+ q-fifiv ff q ER+F?&4'fiP!T$p
? S'fiPfi(Nr-]^-fi1fi ff n R U
4fi 
rt:;
fi4 ff
ACrJ:;3
 ff4 ff ?1?pf'fi9fi(c]^- 8@ Fcp:B@ ff
 ff Q c;:;3
 ff4 ff ??p['fi9fi(Vs,]^
 E@ 8s:B' P*asV:B03 * UsV:B@ ff
 ff !FTFp
? ff ff
 IhirJ:;
fi2 ff
Bqrt:;p
 ff4 lAfi(<r-]^
-fifiv ff U Ep
 ff
R
 ffA?pL@ $fi(6
fi   rt:;
fi4 ff
*%rt:;p
 ff4 ff !
-fifiv ff
  09
 qV4<.
 K=^@`1K 3 ^8fiW
( ='
( qV4<
 Mfi 3ff;30 <fiW
( ?094<
4fi ff
3 3
3
 f?pIfiR
 fi 0B
 An
* A*V   * m4'fiK?[?p4'p343
? ff ff ffO
 Ny	 Xv.
!  
4>?p
 ~ff Md>K fffi($3M0 09
 q~\
! TZ'fifi(5?3fi/3
Lv U2L?pCO
 7fi(
?pQ'fiPAfi(V?pQ3Ma !6rMfi?
 oM0 *Mn
( ^ (K ^  @ N'fiP-fi(t3Ma 
  (J   *
@ff'R
 ff
 ff4*M?p^ (   ^  45 S'fi fi(   ( q~!
T$3
? L9fi1fi ff S P
 ()fiFfi [r-]^F
fi f4A?p>P4 ff'5fi3L
fi v 0v p?p>'fi
fi(,r-]^Q /!fcpQ
fi v Q4J-fi1fi ff ff ffO
 [(bfi
 ffU()fi r9]^Q'fiPN30v pP?38-fi1fi ff 
fiR
 'fiG
 -
* -*N   *Nv 
23v py#  v! T$p
? ff RO
 Wp
 k
_ 0fi 	?Mfi4()fiUc	 s,]^
 E
 K 
 t
! n4?pY0fi103
 R30 ffX(bfiC4A' fi &
fi 4fi 
v ?pR832fi
 K02 hiB! !*N?pfi/3
Cfi(7?3Wr-*5cE*$ sr  flE! c
 pR'fi fi(8?p
fi/3
[ 
 qR28hirt:;
fi4 ff
Eqcp:B@ ff
 ff s:B030 \lE!NT$?4[4f?pI(bfi fi(9
4fi 
. Y2734pfi3I1?p[?M@ Nfi ! [ EZ
   {
*   * 
   @ N3M0 0,fi(t?p
fi/3
5 0P
 ffJ!
s 3@ 3  Mfi\+ 'fiZ3M'fi/!nrMfiEP4*A qr   fi(Q?pY;1R
#
 U
fi 2p
 @ ff&p
? @ S4P
?M@ :B30 U h ChB<l  hB<l  ahB<l'lI+Fp
? @ ChB<l84I?pY@ fi(f 2
 ffUhi'ffElfi(N-Z
* 
47?p-fi1fi ff X G
fi@ fffi 0v pK'fiW-*  hB<lP;
 hB<B
l hB,
l   2L?p	P'`oRfi(
' fi 8
fi fi 0t+f?4
?8@ ?ff ffO
 6fi
( I* 
 ahB<
l hB,lJ 5?p5v 4'ff! f[4'fiM*
 hB,R
l hB<l  h
l 
 @y C2N?p@ >fi(5@ ff
ff ffp ff>
fi $pff
vpS0Nfi(
 hB,lL  
4
 ffFfi(6A!  h\lE*a+F?4
E?K45 U0@ /4fi C(bfi  hB<lhlE*a45?3N' 4fi K
fi fi 	fi(

ffa%pBt\$E^9gE\ M\J;= _EE%^6\BELiNr=r k ;%\^m%E5~^F\^iB^E^[FEff 
NV
m NEpBE BE&B6MEB\tEE~B~tN%m ~pV^J^J5B'$BO EJ;B^E1^5\^~b
=	

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a

 {


ff

$NsJsJ|AAT5c~HQI*/"f|-A|,c,:[c~HQ8*TF"W[H  Sc~T$T5c~HQ

fi

rt:;pff4qXc;:B@ff
ff4.Xs:B'EP

ff

)

DQ|9sVc1:[|9"$c~HQ8*aD[|9sVc1:[|9"$c~HQI*0"F|AA|<c1:fc~HN

fi

rO3M@NMDg$fi(O?pNfi/3
502	(bfiFfi[r9*Mc*0Us<!



hB<l[
fi@ ff'%fi v 3P'fi ffp
 P!>Hffi?Q+8fiYffpffN4Rffff j y/!^k=-Ufi3MNp_fi*
 ffp>+F?MfiN'0fi	
fi4fiK45yPMfiffLMfi)oM4'!6{8
Y0ff	pMfi   hl
   hM  l6()fi-?pf'EfiC
fi4fi
fiff'%fivp>'fi7?pffpfivp8(bfioMV'fi
o!rfioM0*Jrgv3M@>1*  h'hiT$"W[H  Uc~T$T5c~HN8*nDRd  c~HNLl'lN4h)c;:B@ff
ff4
s:Ba3 \lE!
rO 3M ffFI 
 >2443''Ef?pff@ >r  3
 k
_ fi 0!gTFp
? @ L@ >r  04 0A(bfi$?@ Lfi ff*
r-*ac*% SsX+f?U 4
 ffR
* ffp
 ff* G' fi U
fi 0fi ffT
! Sv 0
fiPv pfid+'fiK*
Mfi$()fi 
 Cfi?pf'* $
_ ff-?$?452$ Yv 44%'!
834fi
 $04 U45()fiO
 ffK()fi v p Nfi f02 A	./v pP?3;   A'
 ff /d5hi4'fi


4 ffG?p j '
 
E?Mfi Mfi3[fi130
kfiN20 j fi130
kl$fi(9?pr fQ
fi ff'%fi v p'fiK?p
v 0123a4
 !6rMfi4*?pN0'fiFfi130
A25p
 k
_ pffK

  (  R

    hB;l  ahBBl'l
+F?p2$?pAff4Sfi/3
*aU?3>'fi[fi/3
  hBB l     hB  lAfi(;UW'/:
4fi&P'4
ffP2Pp_kpff&  hB ( l     hB  lh'h (J  ( l  h '   l'lP  hB ( lh (J  ( l
    hB  lh z    lQ()fiCh (J  (  l.  hB ( lE*  h     lE  hB  lE!	c+R+9fi0*t?3Pfi/3



h$hB;l

r   4f()fiOffGK./vpC?pAff4U0fi103
Ffi(<?372
ffNG?p8@ff
fiGfi(<?p
'fiG
fifi0!Qc+44J'ffLfi(<?pIfi/3
[r  @I3M0vffF()fiOffG()fi ?pv4
'ffffi(t?3>v41403r  [!
T$3
? ffi130
-r  Pfi1p
 ff2-I Afi(g'
 0
?Mfi fi3Ar  f!<TF?p[-fi1fiffKE8
fi@ff'%fi/:

v pI'fiP?3Nfi/3
5r
2A?pNfi/3
5 E/!<rMfifrO 3M ffFI
 
 >1*'fiP()fi834N?pLr  
 fi/p
 ff4v 3?pO@ 834fi N04 *%
+ . P?pfiLfi/3
   r  c  s&fi(-?p
?M@ Yr  [!qrfi?4I0'fiPfi/3
*Zah  l  hB$QsJst|-AT$cHN8*<"F|AA|<1c :[cHN8*<T$T
" fH  :

ScT$T$cHNLlE*5hB$QsJst|-AT$cHN8*J"f|-A|,,c :[cHN8
* Dd c~HQ>lE*5hBFQsJst|-AT5c~HN8*J"F|AA|<1c :[cHN8*
"F|AA|<1c :[cHNLl 2
! DO$fi(O?pN0'fiFfi130
Ar  4$?MfidW
+ K YrO 3M@ QM!

H o1g+9p
T
 k
_ 3,?pZ`	^@Uaz^1-fi( 8r 7*d+F?2
?>2V?p- tfi(2
fi 8 ff1p
3 
 fft
 EP'ff
1>?p5r  #a4 !tTtfiLMfiQ?4ff*
+ -_0'6p
_ p-B
 K]3 YUa*+F?4
E?82gQ ff1p
3 
 5fi(
fi $hi'fiPlE!
rMfi4*VK'' p#2[ Wv /k
_ 4:;4O
 fi % ff
'fiff*A	h  0  9
l "-yh >l V*J! !*J	''Ev pK4Q 
v /
_ Rh -l 3?# ff1p
3 
 Ufi(N
fi 0Gh)+F3
? @ 2?pS-fi1fi ff qv W30 ff&1#,lE! 
O1  C } b!
 4Im ff1p
3 
 Wh  0  l7fi(F 4
 ff3
E?X?EY'*      h     #" ( 
l   y/*
! !
* vR  h J  M#" ( l[R
 ff
3 ?pPz- 'fiP!+c fi?3L+9fiE*t	E3
3 mfi(5Y''v 3	2N?p
 ff1p
3 
 Cfi($ 4
 ff8/4ffv Z r   +Fp
? R?3C''Ev pS4~$
_ ff7?pC' 4fi W
fi 4fi 
fi p?3ffp
 ff!

=%$

fi

T$?3L4p3fiLfi(Or 
&

hB<lq



mknpokmkq

X45p_pffK

   -hy>l 

W?0$33'G

h
0

 l$vU+F4?( 0 ahB<l

 
3 
E?K332N
4ffqEEbG=/M*gW4L4m'fi^@[O[)v]['vp<!G@ff13@ff
fiY

pP330Afi(OUr  @>+F?$45
4vffK?p>r  pB0EKE);iM!,cff	?4F
*
?pI

 
 
Efi G
fi 4Ffi(<fi 3I
fi 0fi $

 0vp	3
3 f730'fRv Gv G mv 4
'! T$p
? G `_a
fi 443M@ m
44C?pff r  [*F+f?4
?

 Y/k
_ :;v p?q''Ev p*
6: ^@ ]_VM\^a]_^Gyh AL3?
 *vffwwlE!
z( +fi@ 	p
 k
_ 4fi L@ pffp
 ff
! Tr  z4.
 [$VM\*)`iff ]~	4(p*g(bfi
 ff
E?&'' hB<lE*
 q2t
fia 5(O'R
h 
6
l



v
O
!
c
+
8


fi
?
p


O

+

fi







*


I

r

ff




`$
_ fft+F?6':B'fi:;'
+,.-0/
43
,211 
' fi C?pLfi F?Mfi32C. N(bfiF2a%fi0 [
fi 5. U?pNfi?pFfi !9TF?4A4
C G@ ff'fi 0v 83fi G'fiKP. R
 ff
3 Ifi?p+F4 I?pfi N+9fi32
 fi[.Mfi\++F?
'fiYMfiYv m'fi
 P

3P
 ffO
! Tr  46
 5ff ]~d3 \YUWY K]1Y[I>7U( 3   38:9  h  43 l
   h  43 l<y/!tffc >fi?3g+-fiff*?p-
E?Mfi4
 Afi(0
fi I33 0433 ff[3 Pv pfft+F?2
I? ffp 9+f44R 
. ()fiQ'
! r  q4Op
 v 4'2
,(04g463
 Pv 2'4
<:ff
E?fi(4gff;
! dT0 ff
fi?p+f4 K'ff*-4I3O
 ff&3
? @ 	?45r  f 	
fiP0 Y Xp
 v 4'2
!RT$p
? 

@ ff''E4

fi I'fi>p
 EPv 44
9r [<4:
 fi<LP @ fi<fi0v ff
 ff
30 5(bfiX
  O
 Mfi p
 v 4'2

r  ?p@ >45p
 Pv 04'4
ffi p>

 pP?p>O
 L4 p3fi Cyh AL3M?0 *vffwwlE!
{ 4'

fi pffS?pp
 k
_ 4fi Kfi(,
 [$a[$`v S
 a?![Sfi/p
 fft
Ep
? ff
./v p	p02
4K
fi 4ffi(
fi1fi.1 p()fiL
/
 ff*JL3
 ff
R
 ffGv   ff
fi X>1! >1O
! ;)^ ]<v mr   4QK ff3
3 
 fi(, 2
 ff
"

(
h 
l ChB<l
*t(bfiE
 U>=zv3
E??hM  J#" ( 9
l   hB<l[()fi
 Y$y  U@?v*g! !*
03   B
h M MA" 
l


/
y
Z
!
p
c
(
B


C
 *9?pD=4Im
1
v !Z|,
E?&
1
v Yv Z

&

r   04 4fi\+FI?p
  (

0
%fia44pC?0N?pPfi 7
 Rv /k
_ 0ffKfi()*VfiLLfi 3	>p
 ff@ ffV*@ 12[?p 4
 ffNfi(
?p>
/
 !<c$4'fiP204 ff-?fP3M0'v pI
 YR
 Q@ R
 ffffUv 0p
 k
_ ff4!
{ 3o154423''Q'fiO

 Nfi(t?pff Lp
 k
_ 0fi 
! o/a N''v pv C?pL2 p3fi Qfi(gr  
 *M?p77344fi $r  ?$25?pNfi/3
5fi(Or-*c* Ys<*a4
h'hirt:;
fi4 ff
Bcp:B@ ff
 ff &s:B' PElE*
hirt:;p
 ff44 q
 cp:B@ ff
 ff &s:B@ ff
 ff \lE*
hirt:;p
 ff44 q
 cp:B@ ff
 ff &s:B' PElE*
hirt:;p
 ff44 q
 cp:;p
 ff4 q
 sV:B@ ff
 ff \lE*t!!! lE!
T$?2545P ff1p
3 
 Lfi(O'fiPFfi(  3
! =$
3 	fi(g?045''v 3I2
h'hB$NsJsJ|AAT5c~HQI*"F|-5|<1c :fc~HQI*T$W
" [H  Uc~T$T5c~HQ>lE*
hiDQ|,sJ,c :Q|,"FcHN8*0"f|-A|,,c :[c~HQ8*a"F|-5|<1c :fc~HQ>lE*
hiDQ|,sJ,c :Q|,"FcHN8*0"f|-A|,,c :[c~HQ8*aT$W
" [H  Uc~T$T5c~HQ>lE*
hiDQ|,sJ,c :Q|,"FcHN8*0DQ|9sV1c :[|9"$c~HQ8*a"F|-5|<1c :fc~HQ>lE*
hB$NsJsJ|AAT5c~HQI*"F|-5|<1c :fc~HQI*"F|-5|<1c :fc~HQ>lE*!!! lE!
[4Vr  [$v YrO 3@ ffF
 
 >@ >
fia > Sp
 Pv 04'4
!6rMfi
 o/P0 *0v KrO 3@ >1*
fi\ 5c9
 Cfi P. L
4fi 	c;:;p
 ff2 9()fiADQ|,sJ,c :Q|,"FcHN'!9ffid
+  L
  fi0 

fi m
?fi4
 7fi(9s3
 Pv pff[C3
3 41p
3 po/[8()fi[c$(bfi DQ|,sJ,c :Q|,"FcHN8!%rMfiY
 o/P0 *
(6s. ffNs:B' 0Pf?pUc$83'['ffSv U'DQ|,sJ,c :Q|,"FcHN8*% G(6sZ. ffNsV:B@ ff
 ff 8fi
s:Ba3 N?pKcA83'LfiP'fi'7"f|-A|,,c :[cHN8!

=d

fi[



bFE

t}-m{t/s$q



os0pt{a

l qM 
 a

gieHG2zgJ[	a

Hffid+?N+?ffffffm?3Ir   (bfi44 3ffG()fiLfiN02*%+-
@ff[?p
1p
3 ff'fi 	fi(O?Mfid+'fi(bfi4} QfiR
 ffff!Orfi$`_a
4fi*MfiRffA@Npa4
4o/@ffff

ff?pffr
[7h)()fif3M'fi:B?pfi@ 4
7
 `_a
4fi al-fif Kff%fiVfi 2
!-T
 @ *0
+ >3O
 
`yYUVff ^1[ff%fifi 2
!gffc fi?p,+9fi0*+ F30O
 5?<4
 5fi/
 ff<4v 3ff> P+ F
fi Mfi

fi 2p
 >4833fi3Q%fi0v 7(i3M3M@ ffff
! dfv pK?pv fi?fi
( :<0< {fiR
 h~vffw QClE*
fip
 
fi  7R4v pffNffPfiE<fi 4
()fi834Y 'fiU Z3M'fiP'fih)R
 ff
3 3M'fi
@ Cfi@ fo1 ff P?0 Z4 pff>ff%fi9fi 4
dlE!U-fi?Z@ @ ff@ fi 07@ 	3 ffZp
? @ !YTtfi

4P04()Lfi3M9fi1fi()9v  ff
fi M*1fiR
 ff< Wo/@ ff ffv ff%fiafi 4
!6rMfi5'fiO
 ffi(?p
v 

 ffO
 O  `_a
fi WO
 ?Mfi/Nv   ff
fi XC1*t+ 3 3M'fiP:B?pfi 4

 ?Mfi10Q+F?
 Yr  @  ff 4fi K(bfiF?pLfiR

 p!
s 93,
#
  p
 
_ v pQff%fiafi 4
5fi
 ff!OS fi(?pFp
 
_ fi 6 F0 fffi 
G 3C 
 D-3
3 ff49h~vffww/vdlE!>TtfiC4p
 7?pGR
 p+-m3M'fiPRh)()fiQ04 lA Gff%fi
fi 4
Ph)()fi$fi
 ffElE*M
+ pffK'fip
 
_ pLU$'
 1B)a  B;zyh [E,: K]1^a]~lEX
! 
fi03M4fi Y4
 v /

_ 7@ ffp
3 0
 fi(,ff%fi44:Bfi3
 @ ffm'fiP*J! !*tK''v pMI
! 
:;'C4Q 'fiv m

fi03fi !Pffc fi?p7+9fi0*JL4>Whiv pv 8fi87304fi ElQ
fi R?0Lfi1

3L>Yv p 
4O
 5'P 8N
fiP03Mfi !g
{ F
fi v1p
3 A'fiN@ ( 6'fiL 3'fiP'fiP'F640L j '!^k
 4LUfiR
 ;m?84N'3
3 Gh)(i4 \lQ()fi7 Zr   -!U  zhB 8lE*O4(- Wfi (A
4F'p
3 L()fiT
  U''v 3Pv Y?p84 p3fi  & hB,lLh)(i4 >()fiffiO
 8'v pPv  & hB,l'lE!$T$p
? Ififi 
= 
Ih   7lQ
 ff >''Ev p(q2~$
_ ffhiM
fi ff
 fi>4'()lffiR
 ;m*g! !*t?3PfiR
 p
?Mfi40Ihiz
fi ffc
 MfiN?Mfi2al5()fi6
 6!7
 ()fi@ Pp
 k
_ 0v pC+F?NQOff Q()fiNfiR
 ff['fiKR
 7'Ep
3 Yhi! !*
?Mfi4l9(bfi['' pM*M+ >_0'$3k
_ pL+F?0F$Off F(bfi[P()fi834I?[4$-fi1fi ff \o/@ fffi 
'fi	R
 >'p
3 IN	
:;'
!  K 4  B;   .
 1J
 )R4f'p
3 Kh)()4@ \lFQ
:;'Pzp*! !'
* z5 K
 )q	h z9
)%lf(-
 Gfi S4;
(   K)	h     )%lE*J! !n
*   L)   yRhB ylf
 ff
30 J)W4N	9fi1fi ff o/@ fffi 
+F?Mfi20 ffffi U?pIO
 -fi1fi ff m C3 ffSYr   -* 
 z,4[ m'fi fi(,?
 E/!8rMfi
 o/a *Vrt:;
fi4 ff
 hirt:;
fi4 ff
.nrt:;p
 ff2 lF(bfiL
:;rt:;
fi4 ff
L
:;
()fi7304Whirt:;
fi4 ff
=rJ:;3
 ff4 lE!SY
 3
 4'fiU4.W%fi3M>G
:;'	(bfi834Y
 ff pY'p
3 fi
(i4 N()fiF Y'fiK*v 
 LP
:;745 S'fiK!

:;'S(bfi834'
 )4I'p
3 UfiP()2 Yv &04
34P
:;'fffi(NR'' pM
! D<fiR
 p4
 k
p
_ 3ffv P9fiM
( )J*/ C4<'3
3 ffi-()2 Ffi(J @ Q''v pM!Offc P04
34ff
* Z fiN
  
()fi$?p>''Ev pO6!
{ L()fi/
35fi Kp+9fiP0fiR

 ;C
4 ff5?0F@ Lfi p?Mfi Lfi'F(b@ ff1p
3 
 0
fi3
3  ffYv 
?p `_a
fi 4E3M@ 8+c E4 
  0ZW
" ff'%fi  Pfi
 ff!	+c 4 
 C ZW
" ff'%fi  
fiR
 vff$@ 72. ff	'fiR
 >3@ ()3V(bfiQfi ff!FrMfiQ?p7
@ >fi(<v p >fi hB
  (1fiNH=I,M lE*0ffc /:
4 
 AfiR
 vffg
 o1 ffO?pA@ ff13@ ffO
 O?<Q02
34O
fi p <R
 
o ff
3ffQ! P
" fffi 0 fiR
L
 ffN 4'fiY3@ ()3O()fi>Yv p fi !T$p
? m
 RR
 P3@ ffm'fiU ()U?07
0fi(Q?pSfi ]^C
fi +F44-fi/

3Mv X?3U
fi@ ff
PfiEp
 Ghi! !*A j @ fffi 0 ffkR+Aff/P()fi`:
fi\+Ff j ' EklF Y?p804 !5ffc S?p8
fi o/[fi(97340 Lfi PhB
  (	ff
fi fiL
 '2 
ff
fi3 l
ffc 4
 
 Ifi
 ffW
 o/@ fff?33ffS()fiN02 fft832fi N
fi1fiv 4fi !Q+c G02
34ff*
?p
 o/@ ff,?983a 5fi A?Mfi304O
 Mfi94730pfi34>R
 ()fi 'fiO
 [
fi ba4
pL ,fi(

sda%^a~\ $^BpBB^^AJ^5\5~B'AEOBJl4B^5;~;daB\~ABBWm5J,Ed;EJB
B,V
m BE\6B\-B^8^<$ff8E E<R\E5\^LBB5tE%\NmG%m ~^B<B~ff^'7\ 7BB^yo ~
B9EE~ OE^5Bt'EdB\B'N^>7So Bg`\B^%Tff;VUJBBB~B9EB^4BEfV~EN
5B'F B#B<tEaB^:WaAX,;ZYdg^NE~;/\<$'fV
m ^NB5B^<dOB^~^N',B\
BBENE\[EJ\BEB^~V5EVpEE

=ML

fi

mknpokmkq


fiff!f"Lfffi0>fiffLo/@fff?ppffS(bfiQff13p2t834fiQ
fifi0vfi!$rfi
o/@ ff$?pN@ ff13@ ffO
 A?$fipLfi ]^[
fi Y83'5()fi4fid+ 	@ ff'%fi  
'fia4
34 j 'E v pk
4fi Yfi(O Mfi?pffi !
 @ *M
T
+ ffi 4@ ff 5vM()fiP03
 k
_ fi 0<fi(V?pff@ [0fiR
 ff ??pf()fiP%p
 
_ fi 9@ 
v  FR
 0`om[
! TZffc 4 
 fiR
 pR^
 ]  )h j ffc 4 Mfi8)klL4>'p
3 Cfi($S'v p
_
( )4 j p EkR'p
3 *A! !*5`
( )4
 Mfi'p
3 Yv # X
:;'Sfi(Q?pS''v pM!#a
 ][bh )  cedl
48SW
" ff'%fi  CfiR
 p*6+Fp
? @ (c O
 ff  j  304!^k
{ K
4.
 )X?3 j 'v km f
 dU?p
" ff'%fi  S(bfi834Z'ff	? ' 	4
  344hiv qk
_ S4O
 \l
j @ ff'%fi @ !^k W
()fi4fid
+ ffK1CP@ ff'%fi  !
TtfiW4423''	?pff UfiR
 ;;1R
 ff*A+ S
fi v1p
3 Y?3Yfid E q4 p
 O
 oM0 !T$p
? 
fiR
 p"Dfv	(bfi  ff
fi n1*9+F?04
?&'ff?4P?Mfi34X+Aff/PR
 K?pS
 Y?PcIM
fi ff
MfiF3ff4
 fF?p7
 >4O
 N?0fs4$' 0P'v3M*45()fiP4f
 o/@ ff ffSF U+c E4 
 
fiR
 pDFv8p
 k
_ pffYff
 DFv8g
 ] h  h)cp:;p
 ff4 B
 &sV:B' P4El'lER
! D<fiR
 pD9P(bfi  ff
fi m1*
+F?2
?m'ffL?0L(9rt:;p
 ff44 L?Qfi1

3@ ffm?p 344Ss#+f44
 
o ff
3MsV:B@ ff
 ff *g4N 
oM0 Lfi(<L
" ff'%fi @ NfiR
 ;!5T$?04$4)
 o/@ ff ffUv Yff%fiJfi 2
L
 D-h
 ]hirt:;p
 ff2 
ic s:B@ ff
 ff \lE!
H o1-
fi 43
T
 <?pFr  @ @ ff@ fi (bfi9fiR
 ffff_
! [<+F42R
 Lo/04 pffv   ff
fi >1! >
fiX `_
fi *,+f?I+-K@ ff2 pffX'fio/@ ffI(bfi3'fiP:B?pfi@ 4
U `_a
4fi &4I?p
p
fi Sfi(6?p7fiR
 ;*! !*  !  'v 3Fv K?374 p30fi 7fi(<r    14fi4>fi
 ;K!
cff>?2J0R
 ff*
+ -3O
 -?  4_
 o/@ ff ff83 p$?p-%fi034gk
 30j 
?l
 6:;3M'fiP'fiYhi*
 3j 
E?*
vffw ClE!6
{ Ap
 ff
4p
 ff>'fi[3@ ,?p-k
 30j 
?1r  XR
 ff
3 ,fi 3,fi(M?pA4a ff'V7fi'jff  Ofi/p
 ff

3
? ff
./v pI fi?,v ?p[23M@ f3O
 ff9?49;1R
 Ffi(Jr  ()fi-?pf0fiR
 ;*/ P
+ Q3 
?N fi4? hi   3M0 ff
4fi q
 >1! >	 0G
 C1!4vdlEI
! k
 3j 
?0g3'fiP'fiR4[p
 k
_ pffS'fiKR
 7C()fi3M':
3M0v Ch ChB<l  hB<l  ahB,l :m hB<l'lE*O+Fp
?   m hB<.
l  hB<l>47U 8fi( j 0kS'ff!UTtfi
 k
p
_ 3L?p>4 33fi >fi(6*
 3j 
E?J3'fiP'fi*+ 7@ ff30@ N?pL()fi4fi\+Fv p0@ ff44P p
 
_ fi !
rMfiIY$
3 #fi($r   -:
* n,Ih glL  hB<lo
 MFp
 G()fi7v /
_ ffGP q
 M>v R3
3 !Kffc 
fi?p5+-fir
* n,Ih tl ff134-?pN 5fi(g40 4
 ff5fi(OZ?0-fi/

3M$v Mk
_ ffIfi()Kv C?pQ3
3 '6!
T$p
? C()fi$Ik
 30j 
?3M'fi'fiS-* & hB<lX    9h sRl  l
 m?057E3
3 'Sh  03 lAv KZ+F?

0 GahB<l$ t
 n,Ih t.
l u m hB<
l   a!5ffc Yfi?p[+9fiE*?p8k
 3j 
?t3M'fiP'fim?f S

 0
 

Efi ?7@ff30@ ffN/4pK'fiO
 aW'	v /k
_ 0ffYfi()*6L
+ ff4,>R
 v 3v 3	 R 
v 44%'!
\oM0 7p
 Pv 04'4
Qk
 3j 
?0Vr  (bfi  DFv*a+Fp
? @ Nffc 4 0
 LfiR
 pDfv7p
 ]  h)cp:
 ff44 
p
 Xs:B' 0PElE*4Fv KrO 3M@ >xKh)fi U?p> ()El9+F? m h  Dfvdl<  a
 !9HffiL?F14vp
'v  m h  DFvdl5 /k
_ ff4fi()S204 ffFk
 30j 
?g

 
 *V SR
 ff
3 7?p8r   o/@ ff ff
?p)pfi Ifi(?pAfiR
 p*12vp[ j 0kQ'Fv  m h  DFvdltv Mk
_ ffQfi()P4g3
3 p
 ff0 !
rMfirgv 3M@ PxC+ P
 W ?LG''Ev pC?Lv
430p
 ffh)c;:;p
 ff2 
 ns:B' PElF+F44t/4
'SRv /k
_ 4fffi(bV*-  m h  DFvdlP  a
 !qT$?13 ''Ev pm?'v &'ZvY 
v 
23p
 ffNh)c;:;3
 ff4 q
 XsV:B' El94$v  & h  DFvdl5 	?p@ ()fi@ L/fi4ffFfiR
 ;DFv!
H o1
fi 4p
T
 7W
" ff'%fi  fiR
 ff>fi($?pC(bfiv
 ]Qbh ) cedlE!GrMfiI?4La
 d*g?3fi 4

;1R
 Ffi(Jr ?0,+pff()fi9 4(b/v pLW
" ff'%fi  F0fiR
 ff<4,?p[ P4a $p
 P 4'4

 3j 
?06r  ()fiN?ppfi Rfi(- j rO~:;L
k
" ff'%fi  ffk	0fiR
 ;! 2 hi
D Pv 24c
 pffp
 ffm()fi
fi3M
 eC
 -v /@  ff 4fi !    30 ff
fi C1!4v! 
l rO~:;L
" ff'%fi  50fiR
 ;I
p
? ff
.1
oM0*M?p	


rdURi;E^`}m;I\\B^AB5ffONmVOB<^<^6\E<^\ABf~\iB,f\BpB5\^iBNYVw ;
EdB$BV
m BBo B$ff-p9gi;B~9 EAB\$~B^EVB$`\^'t F\BE)xJzy{}|Z~
FQE\~ty%
m EB\BE~59^'B^;
=Kx

fi[

RR 

 { 

ff4

v




)

RR 

t}-m{t/s$q

os0pt{a

l qM 
 a



c;:;3ff4XsV:B'P4





 { 

R 



v

>
RR 
v


*

sV:B@ff
ff

ff4@



v



)

R 



rt:;pff44


ff4@

rO3M@>x1Fc+E4
Pfi;  DFvYhi()ElN0?prO~:;"Lff'%fiEfifi(AfiRp
h) ?ElA$k
 3j 
?0Vr  fff*+F?p@ m hB<l<  a7()fi$%fi?K3M'fiP/!

 D9

+F?p?3t?pN'YBff]%'vH)v>''v3F4V()fi4fi\+-ff71LF@fffi0`d1!grO3@-x7h)fi7?3,?El
?Mfi\+F5Ik
 3j 
?r  =(bfi5?3QrOE'~:;L
" ff'%fi @ ffi
 ;
fi@ ff'%fi v 3>'fi  D-1*1+fp
? @ QfiR
 p

D9&
 ]hirt:;p
 ff2 
  csV:B@ ff
 ff \lE!rfiY?4Cr >* m h  D-lC  a
 
! T''Ev pW+f?Mfi 


 v3L3
3 I/4,'[N /k
_ ff4Nfi(b+F44/v 
23p
 A?p5_0E'O' 9 0
 Mfi<?pF@ ff'%fi  

?>?Mfi34m()fi4fi\+![>2
3 ff  3M0 ff
4fi C1!x1*t E(b/v pYrOE'~:;L
" ff'%fi @ fiR
 ff

 Yv 	'fi
 Q
4
3P'0
 ffLhiv 
430v p>4%fi(tfi3M)
 o/
 E4O
 El6R
 Yff13 A'fi 4(b/v p8?p
(i34JW
" ff'%fi  7fiR
 ;]Qbh )8cdlE![W
 0
 (bfi?V*+Fp
? Y
+ 3 7?p7E j W
" ff'%fi  ffk?4[4
30O
 ffK'fiv 0
43p
 [%fi?K?pN(i34%L
" ff'%fi @ L K?pLrO~:;L
" ff'%fi  N Efi !

 bp
 

@8*['^" ekg79gfi[F]{i[e^

eh

Hffid+?8+-C?ff	fi3M7@@fffi7()fi702>ZfiRvff*t84L%fi0'fimpff
4
fi/p
 ff1
p
? ff
.1v 3M*! !*(bfig04 U >0fiR
 ;Lp
 v v pF+Fp
? ?p<# !OrO'*?Mfi\+  d*
+pff'fiKR
 v G+F?mp+9\
fi ff 26p
 k
_ 4fi [fi(9

 ff4044paN

 ff0444;Yfi(-fi p o
()fi  Mfi?pd*0 Y

 ff0444;fi(O Y'fi (bfi I o%!

^j[[ek^  ::o  47  y  ~ o 0 (OKfi4	4(t?po/2'Fa?Y()fi

0 'fi7  !
Z
 ^j[ [ek^E A'fi ^ l(  9hy>lQ2IE  y  o@ 0 (,0mfiU(,?p@OoM4'N
0?K()fih 0 'fi7  ^ l(   h (K   lE!
Z

[


 ffa44pL(bfiv41'ff<26
''fiLfi13ff1
E?pff
./vpM!<TF?p9ff'fiP4O?pA()fi4fi\+FvpM!
"Lff
2()fi  ff
fi>1!N?9fiRp846'3p8h)()4@\lg(bfi-r  -*%hi!!*/# hB7l'lE*
(L fi #4(L2'33S()fi''EvpZ#?pm4p3fi & hB<lSh)()4@S()fiK'fiOm''vpv
 k
_ fi V
*  R''v 3Yv ?pC4 p3fi ?0> 

 vpS3
3 !TFp
? @ ()fi@ *g4>4
& hB<l'lE!K9R3
fif
 pff
 ff'fi 4(bC?pNfiR
 p()fi$''v 3A?$?ff 7 Y

 vpE3
3 !6-Cp
 
_ fi *

 	

 vpI$
3 R
 v 0,+F4?C 	v 04'!,T$p
?  (bfi@ 
*  	'Lv 	 K

 pI3
3 
4L

 ff0 (bfi Wv 04O'*6 a
  R'fi hi
:;'\l>v RS''v 3Kfi(-?34 p3fi C4


 ff40 [(bfi  	v 2'!-Av ff*1?pffi 0P'ff5 C'fiPA?)pff'fiI
 [v fi ff
v 	
 _a
fi Y@ >?Mfi >

 ff0 N()fi v 04'ff!
ffc 4
 0
 >fiR
 vff$
 G
 >@ : o10@ ff ffGv KFfi(<

 ff0444;!5ffc 4 
 7fiR
 p
]  )R
fi34GR
 7@ ffffRNff/v p	?N?3@ Mz
fi ffc
 fioM4'NS'fi ^a*+Fp
?  .^G )J*%?
4f

 ff0v L(bfi  U 4'!Fcp[4$83
?Gfi@ 8`e
34-'fio10@ ff[L
" ff'%fi  >fiR
 ff

=d

fi

mknpokmkq

3

 
Z30v p

 ff4044p!qT
H ?pffffff*A

ff042;Z04ff/P.XfivUvX()/vp4
fiRvff*M$+F420RLS?Mfi!
T$3
? @ U@ m 137
 fi(N+-d1C'fiWR()fi Pfi1pfff
?pff
.1pM*$03M?p@Y+U()fi1
30fiq;+-fiM!
T$p
? f_09O?Mfi/C49
 ff
_a
4I4fi ff()fiAfi p[
2-fi(VfiR
 ffff ??pQ ff
fi 	4-3/eC
 4
p()fi53 Qv 
 E(b/v p8P 
4 ff-fi(tfiR
 ff!OT$p
? [fi 0 f()fi5
?Mfi1fiv 388'R
 ff
`_a

 c

 3 fi4?4O?<?04O4fid+f6(bfi9N
fi04fi I'fi>p
 P pA?p$
fi03M4fi 
e
 0

 v pffC1fiR
 p:;
 ff
_a
$44fiv phi@   3M0 ff
4fi C1!xlE!<ffc C?4- ff
fi *M
+  
? ?M:;  fff. 
p
? ffUfi(L?pff m;+-fi#fi/p
 ffQ
p
? ff
.1 p& fi?!T$p
? G()32f fiE?PC@ v 
 ff
fi C1!
T$3
? O_0' fi4?45 [20 6 Y
 e
v J?Mfi1L4fi@ ffQ()fiVffc 4 
 ,fiR
 ff



 ]  )J!WrMfi
   49' *9?4IO
 ?Mfi/R
 v I  X/4I X'fi ^ 


 ff40 K()fii
 Mp!&c([?4'fiu?0
 Mfi ffMZR
 &
Ep
? ff
. ff*$4P
p
? ff
.1'fiW Y+fp
? ?p
^\)J!GcR( ^)J*6?pZ?4848
fi 43 @ ffZS `_
fi ()423M@ !Uc(5?p@ 	@  MfiG()243M@ ff*
`_
fi Y3

 ff0!
T$3
?  ff
fi XO
 ?fi1*<3'fiP:B?pfi@ 4
R'h 6T[l>Pfi1p
 ff-
3
? ff
./v pM*,4> W%fi0304>v Z?p
`_
fi 443M@ >h ! M*/ R:<E {fiR
 ff*0vffw QCl9 P
 
 f3 ff'fi7 ().
 ^@UW8fiM:
po/@ ff40 >Nk
_ :;I3'fiP'fi!Lc[2f3@ ffG3
? @ 7()fiNrO'~:;W
" ff'%fi  >0fiR
 ff!
cff6Tfi/p
 ff0
Ep
? ff
./v pM*/'./v p>+fp
? ?pA 4
 ff13 <'fi8.1v 3>+Fp
? ?3 & hB<Z
l  & hi7l
()fiLfiR
 pS!T$?2[2Y
 ff34 Q'fi & hB<Z
l u & hi7lN h)+F3
? @  & hi7lNp
 MfiffN?pP
fiP0 :
 fi( & hi7l'lE*-+F?4
E?X4 fi?0P4
4ffff#1R_08./v p?3K'fifi/3
Ifi([?p
O
04 Sr   # S?p8r  
fi@ ff'%fi  pI'fi  hi! !*V   7lE!$T$p
? 7r  
fi@ ff'%fi  p'fi
 L4 p3fi Cv  ff
fi !YTFp
?  fiE?
  

  & hi8lE!YT$p? 0'fi70fi103
>4a ffO
l   mhB 8lE!LT$?4
?pmp
 v pff[+Fp
? ?p & hB   7P
l   1*+F?4
E?S204 ff & hB,.
l u & hi7
 v fi 4,4P0 ffO
p
 ffA>
Ep
? ff
.(bfiA
/
 ffAv P?pffi/3
,r     ?A@ Q
:

 ff40 5()fi'fi

 fv 44M'Q P?-4'() fi?39
fi 4fi <v ?pFr  

 0
 

Efi !6L
" ff
4a?A7
/
 N497 ff1p
3 
 Qfi(V 4
 ffNh  03   l<3
?C?N  
  0 
! 
/
 
45

 ff0 L()fi  Uv 4%'7(tfi 3Nfi(O45 4
 fff45

 ffa N(bfiz?p>v 04%'! 

/
 N?545

 ff40 [(bfiz K 4'L 	?$4~$
_ ff-?pLr  

 
 >
4fi 
4a4 ffAO
 Mfi pff0;K4 p3fi !9T$?454AR
 ff
3@ >P'' p45v K?pL2 p3fi Lfi(6 Yr  (O$4
  /k

_ :;v p?P ff1p
3 
 ffi(%
4fi 94'()1v 3N?pfr  

 0
 Q
Efi *+F?4
E?P+Aff/
v 
23p
 ff$?37@ ff13 ffO
 f?NQ

 vpC3
3 G83'[ Sv G mv 2V'
! [4t /k
_ 
Rff?ff/fi)
  34
 053MUv KP
/
 NR
 ff
3 >?pLr  ?$8k
_ 437R
 5fi(gff!
T$3
? @ (bfi *,'fiRR
 K
 v &?0?pY4 p3fi Y2O
 Mfi pffp*-4I2O
 pff
 ff'fiWp
 EPv p
+Fp
? ?3f Y

 ff0v >
1
v 74'$
_ ff$?p8r  

 
 
4fi !-TFp
? >
Efi Kfi(<v ':
ff'I4L?3C*
 3j 
E?,
4fi *O()fi7?p(bfi2fid+F pY@ ff'fi V!Uc7473O
 ffp
? @ ?8?pf3fi 
fi(A?pPfi
 ;&h  7lQ4
 o1 ff ffW7Yk
 3j 
?0<3M'fiP'fi!KT$?04N402 ff[?L?pr   R
 :
v pK ffE
p
? ff*O! !*g   P*J4L4fiYKk
 30j 
?<3'fiP'fi*g
 ff
30 .1 pK?pPfiLfi/3

@ ff@  ffF?04F
4fi !$T$3
? Nk
_ t
p
? ff
.Yfi(<?4f fi?0 4F+Fp
? ?p[ S

 ff0 8
1
 8v 
    2~$
_ ffN?pk
 30j 
?<

 0
 C
Efi *J
 ff
30 v m?7
 ?p4 p3fi 2
 Mfi
ffpI
! 0fi103
[4Qv  m hB   7l$+Fp
? p L4[?QK
fi%fi pQ'v  m h  7lE*
! M!*[hBFQsJst|-AT5c~HN8*t"f|-A|,,c :[cHN8*O"F|AA|<1c :fc~HN8*Ol748v  m hB   D-lN()fiIfiR
 ;D9
Rff
3 Y8()fi3M?&
fi%fi pP4'UGfi( m h  D9lE! [

fiv p'fi?3Yk
 30j 
?5

 0
 

Efi *</4pG m hB   7l8v /k
_ 0ffmfi(bhi3P pqW4

 ff0 	()fi
 q 4A'\l402 ff & hB   78

l   1!T$?2I+F445?0
 &4J
( 40Pfi(N q

 ff40 

/
 !$ffc U?Q
 *Jj
  S `_
fi Y(i44!fQ?p+F4@ *a:
( MfiC

 ff0v >fi/3
f'

K=

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a



hB   7lQ4>12ffRvM_kffUfi(bhi!!*6>4Mfi7vWU
/
\lE*O?p & hB   7lL 
m
?p@()fi@ & hB<l & hi7lE*,B!!*9z  0W_a
fiX3

ffff!a @ff44ffe

 fi4?(bfi
 6T`_
fiK()fiz?p>23M@N4A0@ffffUv  ff
4fiC1!




bA


 Z

efhj]G]$^

]\_['^  ]3gi^j['^

 802IS8fiRp>*fi/pff/
?pff
.1pNpPv3ffJ+F?3?p,# !OHWo/O+-5
fi4p
Q
?pI
Ifi(,ff0vpM*0+f?4
?S4[
E?pI'fiU-!fT$?04F30ff
fiGM@ffffF?3843p>fi(9?Mfid+
 ff
 0v pIfiR
 'fiF
 URff
fP04 U'fi37
 3+04 U  !
{ KR

 v ZW@ ff vpmGo1fi Mfi7fi(Fr    ff7 v pYfiR
 'fiE!cp4I44. ff?
 ff
 0v p7
 ?Mfi1	()fi5
fi0 L3
 Pv 2'4
$r  [A+F440R
 Q
fiPfi@ fffi(tfi pQfi$fi@ Qfi(J?3ff 
fiR
 'fi!PHffi?v 3Yfi3[fi3>fi
E?@ ff13@ ffY
 fi23Mfi m ff 0v pC
 L@ ?g?Mfi\+  7'fi
P. S?pU4
3fi X
fi 
 *5?44?3Y(bfiE fi(N ff v 3G?04P3O
 ff#p
?  !qffc &?p

fi o1Qfi
( fi43fi Uv fi?Pff*0?pIr   ff  pPfiR
 'fiQ@ 8R
 3M04fi *030
?m
83Mfi *a04 ffC'fiP?3Lr  f!
0l%2e\
o l  %      B
      	l%    \ 
         
 % 	I    	       %
!M %6 4       J
          4
		  %I  4
4         4
  l 
  O042





rO3M@C19T$?pNfi3M4pQfi(gfi43MfiCfiE?K!
{ 3OI?Lff7vpfi1

3MENvG;+-fiY0?fffff?pfi3FCvpImfi4v370?0ffhirO:

3M@7vdlE!<D[3vpN?pFfi3Fvp50?0*ff
E?Cfi9,+f?PN0MfiPIv24}ff8%fi034fiIfi(

2fr  a4 !OT$?4,%fi034fi 4
 fi ffK30v pL?3Wfi43M4fi P fi4?fi34v pff
vZrgv 3M@ C1!GT$p
? P Wfi1fiZfi(5?04> fiE?
fi 04'7fi($ ff ff
v3U0@ >a4 L()fi?p
%fi0344fi *0/v p	R
 3Mafi Gfi
 E'fiN'fiY?p0@ N'fiYfi/3
 Ifi' pM
* 23~:
v p	?p8_0@3ffQfi(9?38fi'Ev pM*
 m?pm@ 3 v pC?pfi' p'fiK?38%fi0304fi G(,?p
@ 3/e
 Y_a!F()>?04c
 fi43M4fi *V `_
fi R  0>@ Mfi p'fiY?pff@ v 24
pffU04
 ff!
A5?pL5fi(J?pN
fi 04v pf0? p
* ff
E?Yfi fff ff
5fi p j R
 ff'kYhi

fiEv pI'fiP j _0@3ff
(i3
3 
fi klL04 Z()fi47%fi0304fi R()fi
 
o ff
3fi !RT$p
? 	fi @ 	?pZ$
_ ff4p
 ff Z04 

o ff
3Mfi 84Jv  ffd ff>+f?L ff 0v p>hi4fi alE*@  `_
fi * L02 L@ 04J_
 3ffp
 ff!
T$p
? 03M%fi fi($v ff v pS3Mv 3K?pfi 2v p0?@ 4>'fiUk
_ 3:B33p?304 Z W084L'fi
.X0
 +f?W
 344G?0(bp /fi 0O
 *Ov 0
 Mfi4G@ ff`:B+-fi4
 14fi O
 
@ )Mfi6'4
!6T$3
? Xfi43M4fi > fiE?fi(arO 3M ZC[4O4'fiQ3 ffI3MEv p$?2J0?0 *03Mg?p
30fi C4-7%fi0304fi } Qfi(Vfi pN 	v 
@ ff
 % ff 0v pChiB! !*1fi 3Qv ff v p7fiR
 'fi

KK

fi

mknpokmkq

02ffC$r  R)pfilE!,T$?452A
4
()fiF43fiAv	+F?04
?	?pc/fiO

?0 pffL
 034*M?3$?K04
4!
rMfiEP4*,mP
E?v pK ff7 v pSfi
 E'fiPVW9    
?3ffP&h)fi/3
8fiPv/430bl
r  'fi7%fi'~:; ff  pNr  =  :
! P00v pQR
 ;+p+9fiI3M'fiP 0C  4,p
 k
_ pffP-
Pav pR
 ;+G?pffW
 ffv ffO
 IhiAff ff*6vffw Q>lEY
! Af?37? 3
? ff'F ff*a+ 8
 m3M/4p
 [?p
ff 0v pIfiR
 'fi$

fi0v pP'fiP?3cff ffO
 $fi(O?pLr  ?$?pKff
 p
47fi(FfiE'fi70*6pffff*6fifi\ffffpC'4fiW
fi4fi!YcffZfi?p
c
+-fiff* VI  hB<l;  hB  lE!


Mfi?p[
45fi(gfiR'fif*pffvff*Mfiffi\ffLffpffff*a!!*


VI 

hB<l;



hB  lE!

TF?pm?n
4	fi(7fiE'fiYCfiYpffvff	4
ff*NfipX+F4??3ffffpff*N!!*
VIa hB<;l  hB  lA 0
 V8  hB<2
l   hB  lE!




TF?p(bfi3?
4Kfi(8fiR'fiY
E?pffU?pR-fififf=&3@ffvn?p'fi

fi 0fi ff*/B! !v
* V8     !

 @ *
T
+ PMf
fi fiQ3k
_ p8fi
 E'fiQ?NGfiLp
 ff 'ff!7+c Gfi?pN+-fiff*+Mfi Mfi
Mff[?pI?G
4Qfi(9fiE'fi!>T$?37@fffi4[?N+F?S?pI;1R8fi(-r  fN30ffm?p@*
 pfi7p
 ff vpKK'Mz
fi ff
 Mfi*Jv @ff(p*VRff
Lfi
 ff!8cpL4Q+f?Q+-fiC+f?m?p
ffp
 ff'fia<\(bfi R'G &?pff'E fi q
fi fi 07?0
 #+Fp
? ?pfiR
 p
4'p
3 Ufi	(i4 S(bfi	R02 !T$?44PR
 ff
3@ Sfi
 ffP@ G'p
3 Sfi(i4 S(bfi	
fiM:;'ff
hi'fiPElfE?pf?0 S()fiNr   'ff!Lr03M?pPfi@ *%?4f0R
 [M
fi ffY
 MfiQM@ ff[
?0 pffQ'fi
?p89fi1fi ff m E/*a+F?04
?S4$?p7(bfi3?S
2Ffi(6fiR
 'fiE!FT$?2F
4ffi(6fiR
 'fi*+F?2
?
v 
23p
 ffA0''
4fi *0450M@ ff ffK U[fiMfi h~vffww QlE!
T$3
? @ (bfi <+ -@ ,()fi/
3v p5fi >?p<_a'V7 ff
fi 8
4 ffJfi(Mfi
 E'fi!J
{ Ap
 k
_ 3<fiR
 'fi

Ep
? ffP*1?p-? PfiR
 'fiE
! P
? pF ff 0v pNfiR
 'fiA
p
? ff7a4 ffO'fiI3
3 'R
 ff
`$
_ ff
h)40 \l 4
 ffW
* ffp
 ff*f 0q' 0fi 
fi fi 0!{n3
? v 0'2ffq+F4?q04
34
4
 ffff
* ff3
 ff*M 'E fi 
fi 4fi *4,Rff
fiO
 ffA>P
E?v pQ ff 0v pLfiR
 'fiff!6ffc PfiEp
 
'fiRfffi2&ff43Y*<?p	fiR
 'fi
Ep
? ffPp
 k
_ 0fi 8
fi 43
 8fi 0W?p	@ ffv  0Ifi([?p
r  7z
* ! M!*/?Mfi [09?5@ ff!6T$p
?  f4- 2044
4O34fi P?A43
3 'R
 ff
`$
_ ff

0Ffi(O?p>r @ ffPv Y?p>O
 8(bffiR
 'fi[0044
fi V!,T$p
? @ L2F4'fi Y3Pfi 
?A?pQ ff p
 3@ ff,?0A20fiR
 'fi-. K?pQ3M'fiP'fiU
fi0 Q 	p
 P 4'4
!
T$3
? 7fiR
 'fiQ
 mR
 7@ v U?37o/fi Mfi7Xh)0fi al5fi(9rO 3M@ P!Q
{ Pp
 k
_ pIff
?fi(
?pQ
fi@ ff'%fi 0v pLfiR
 'fi5
Ep
? ffPA-(bfi44fid+F*1R
 v 3 pN+F??p[Pfi'-3fi p*M
4 ff
V\ fiKH=I *+F?4
E?K
? 3ffL ffp 7' 4fi C
fi 0fi ff
 EQ
 M g6b)*
 [ffUJ~B  a8ffzV\ fiKHOI 
 G;g]{e$g(o\ ]fe0 - ff%
X   eJ/g,F ( aV\ fiKHOI    hB<l   hB  l  Q/Eiff1   dM  _P   h (   . l 
n
 y    5h (J  . l  hB<l$0   h (J  * l  Ah (K  * lZ  hB<l  eJM9V\ fiKHOI h  h (K  . l'l
B ;  0%V\ fiJH=I h   h (   * l'l    h (   * lZ! B ;p   	)/
   h  (   . lZ  
g< ~   V\ fiKHOI I # E    $  }g<  ;p  )M (   AB'P0bi  ~ff1h (J  . l 
0m)M  E0mBm  )/    abiBff1	h (J  * l  Q ;ffE0b #  V\ fiJH=I  #
    5  M # aI  )/  8} g<  ;p  

KO

fi[

Zh~vdl^@Uo5'hl


ff~:;
fi  




V  fi



(  * 

 

V\ I1
I, I " HOI 

 










V _ I  







(  *














os0pt{a



t}-m{t/s$q







l qM 
 a

V\ fiJH=I





ff':;
fi




   M 


V\ I,
I  I " fi 

V ffI  " fi 






'h~vdlVI'XhlAfip










'Zh~vdl 

V I,
I  I ffI 
 
V\ I1
I, I

 

V KI 


M 



'Xhl

V fi 4 3HOI 

 

V fi 4

 

V H=I,

V KI  " HOI 
rO3M@7,Tgo/fiMfi>Rh)0fial9fi(6ff0vp8fiE'fi!

[4Vfi(O?p7@ffPv0vpfiR'fi[@fff'fi	pff
ERLvKPffi(<@Ffi(6()fi3MFE4P

fiR'fi!RTF?p@()fi@*6+- po/p_kp?pff	()fi3MI4P4ff*g+F?4
E?Z@	fip:;&fiE'fi
?-@ f'R
 ff
40
 ff,fin
( V\ fiJH=I 0Rff99?pF%fi''fi?,9vffffffAv?p[?E
?8fi(
rO 3M@ >!,TFp
? [_0'A;+-fiPE4PffiR'fi$pffvhyV I,
I  I l-KRhyV fi 4l-ffpff!<{7p_kp
V\ I1
I, I 'fip ffv ffp h  (J  . l-+F?	?3Nfi E'fif
Ep? ffP/

; {
 ]DE>e\4 - ff,  Vr g6b)(fdZ 1K~P  0	4V\ I1
I, I 
     S ( ag6b)GV I,
I  I   hB<lP  hB<lV  h (J  . l  K4;ZffUh (J  . lK   
& ))1,aa4  ;,ffQ1  }  ))m0))@ * M~BV\ I1
I, I E1`)/'  'EE
  i'  Md)  N  V fiJH=I )16      M #  5 ;p p 0Kbi00))
)1A   7P  aP #  )1\V  I1
I, I h   h (   . l'l     h (   . lq      
{8p_kpBV fi 4>'fi ff3h (J  * l9+f?	?pLfiR'fi[
?pff/
 G;
 g]{ e$g(o\ ]  eH	0H - Jj NVr6g ))O[ff4GJ~9  0PffWV fi 4N/X
   f ( a6
g b)BV fi 4>  hB<l  hB,l  h (3  * l  $ 8fffh (J  * l5   k M~ B;V fi 4

 E LE  iff~   Md)J  L  \V  fiJH=I )1,E      M #  f  ; Bff 0)/L))a
0)))/  h (K  * l  N i>
 BKEa # )|V fi  
T$?3Pfi?p8;+-fiSE4PIfiR'fi8@C'Rff
44}fffi#hyV ffI  lLa34}fffinhyV H=I  lE!
G g] e$g(o\



 Rff
444}fffi70p4}ff4fi@5fiR'fi6
fiPfi0>()fi33Iv8?p$P
E?vp5ff7vpQ4~:
Q>lE!-+c ?pQ
fi o/5fi(J 	r  7*/ ff
24}fffi fid+ -?pQ  ff
fi(A	04
32Q:B'fi:;'	' 4fi G
fi 0fi mv m?pa4gfip
 |
 L*J+Fp
? @ ffc
 3`:
}fffi C2 ff94*1Av CS4
p
? ff4B]^2
 ::fi   0
 ffNhiS4
p
? ff4B*vffw3 QlE!9+c 04
304ff*7' fi 

fi 4fi 
 
 m'R
 ff
444} ffq+F4?nXO
 Y0n
 
 p2} ff+F?n @ fi *$+F?2
?4
 fi fi30>'fiGv pYS
fi  @ 3

3 
>'fiG'R
 ff
44} P 0WU04 @ 3
3 
N'fi34} C7v WS4
E?4'./
h~vffw Q>lE!
rMfiEP4*
+ Lp
 k
_ 3L'R
 ff
44}fffi K 0 p44}fffi *M@ ff'R
 ff
 ff4*$()fi4fid+f
3M*!M!*@QG4
E?4'./th~vffw





; {
 ]!Ke   - a$*Vr 6g b)[ff4	J~R  0L4{V ffI  /X
e
J
/

ff

X

,
g
P

E

Z ( a9V ffI    hB<l  hB  l  g
g /ff~V ffI  h  h (J  . l'l5  h (J  . l
 
G g] e$g(o\

K=

fi

mknpokmkq

 y O MffBRV ffI  E1`8E  iff~K  ffitE  8  V\ fiJH=I )/
      B  
       #  [ ;p  0C)/8)i0g}g<CE0)i  V KI  h  h (J  . l'l    h (J  . l
  lqC
        V ffI  
 V\ I,
I  I   a  h (J  . l|     b08)/Og6  qV KI  /  0	[ff  
 G;
 g]{ e$g(o\ ]D>e 	  - t ELVrg6))L[ff 1Kff  04WV H=I, /X
  J
e /8,g 8 ( aV H=I,   hB,l;  hB  l  gg/ff~qV H=I, h  h (ff  * l'l<  h (J  * lW 
   P    y ; ~ B|V H=I, 1 E Y  i'   Md)5E  U  V fiJH=I )/
  ;p B 0S)/}<g S))a-E0))  )1  h (J  * l
      M #  I
        V H=I,
i
 V fi 4  a   h (J  * l      E E  I)/ff=6g   6m  ' / 00 E#  E i7 BEV H=I,
HTo1*QvyRff7vpGfiR'fiP@Y3_kpff(bfi?pffY()fi3M2Pff!Z-ff4fid+ \V  fiKHOI v

?pLfiR'fi[?E
?Cfi(6rO3M7I@Lp+9fiC3M'@ffff!9T$?pNE?f3M'@L
fi04'5fi(Ofip:;
fiR'fi*G?p()Q3M'@8
fi4ffi(9;+-fi:;'RfiR'fiff![{P3_kp>?38p+9fi	fip:;
fiR
 'fi @ 3'ORfffid
+ V\ fiJH=I _0'$hi 
 A+  @ 3'6pk
_ pff8?pAE4P -fiR
 'fiOR
 fffi\+?3fflE

 ]f e\4   0 eJ  E~B>E        Ea # b	ffb)MQ  )M9b4
V  I1
I, I V ffI  
\
 G;g]{e$g(o\ ]eH	0  	 eJ  E~BQ        E # )P))/$  )/6E)Ib)
EMffB  V fi 4KRV HOI 

; {

G g] e$g(o\



)b7M~B 

c94,@ ff -9?046%fiv<'fi8'fi/3
5p+9fiIfi@ffiR'fi9?-TMfi-v?p[?
E?
fi(7rO3@! TF?pq@Mfi	v?p?
E?qff
30S?3@O@ff4qPvMfiC4fi(
V\ I1
I, IaffI N XV fi 4 3HOI  ?3RMfiMfiLRfffi3Y''4
GRfffid+fi3M>fip,fiR'fi
V\ fiKH=I !,T$p? ff [fiR 'fi5@ Nv 'fi/3
 ffCp? @ [R ff
3 Q?p [ 3 (i3a 	4'fiIR ff
3 
?p@ 3 ff'fiq@ ff  R
fiP0 pff	fi(Ir  [!+c fi?pU+9fi0*[4(>?pRr  4

fi0v 4fi>'fiS00/v pK?pff fiR
 'fi8?pW4>+F42O
 
fi0 	(bI041v 3K?pffK!
" ff
2a()fi  ff
fi K7?ff
?Kr  'L29fi1
4ffC+F4?I -fi(J4fi\+-a [
4fi -?
L
PdZR
 C. #()fi?'!&T$p
? ff 	fiR
 'fip
 ff 	fiP& &
fi &()fi?pK fi(
4fi\+-a L
fi F(bfiz'


G g] e$g(o\

; {



]f



; {



]

5'QEa`ii  
d    (
G g] e$g(o\

'Ea`ii  
d    (





e\4  	%
EM B \


 4;	#bg< Uffi     B;L (  # 
PE iiY # fB78 ['0t/4b

ff V 1I 
I, I aKI  

eH	0  	%r
4


EMffBV

fi 3HOI 

g<a 4Rd)
&Ea i)=I

F&)`


 ~   B; (  # am
# E UBX ['I/4b

Ttfiq3
3 3
 'n+F?
 V\ I1
I, I4vfi      
fi 4'	fi(8fi pRfiUfi R044
4fi Cfi.
( V\ I1
I, IaKI *

fi2p$?pL()fi4fi\+FvpoM0!9c+Urgv3M@>1*a3ffvpPrt:;
fi4ff
[FS4fid+A0L
4fiY()fi
r-]^<$QsJst|-AT$cHN#- ff3gv 8rJ:;3
 ff4 OR
 ffv p$?pAfi 4L4fid+A0 A
fi 8()fi?g!
r3M?pfi@ *?4F@ ff34Fv U?pffp
 KhB$QstsJ|AAT5c~HQ8*$QstsJ|-5T5cHNLl-R
 ffv 3p
 ff ffG 
?pOffp
 UhBFQsJst|-AT5c~HN8*D[|9sV1c :[|9"$c~HQ>lf
 ff pC
 ff
24} ff!8T$p
? I@ ff'fi  pK4Q4P42f()fi
+F?
 V fi 4 vfi      4Afi p>fiFfi@ >0044
fi 09fi
( V fi  3H=I, !
T$3
? [ ffPv v 3>fiR
 'fi*M+f?4
?C@ L4fi(t?pQfi
 E'fiAfi 	?pN ()A3M'@Qfi
( V\ fiJH=I v 
rO 3M@ *J
fi 04'Nfi(9;+-fiU'0Q?pI_a'Q'fiK@ fffi\ 
fi 4fi q	(bfiG
 ffp
 Uh  (J  . lE*t m?p

K

fi[



t}-m{t/s$q



os0pt{a



 Tj6T$|Fv




)
 Tj6T$|9

l qM 
 a





 
 







 Tj6T$|>


)

 Tj6T$|<



rO3M@Q1,Sfid/vp'4fiC
fi0fiARp+-ffpff!
 ff

fi R'fiU#h)?pO
 \lQ
fi 4fi qK'fi\ffp
 Uh  (J  * lE!PT$p
? I_a'N'W
fi 4'Nfi(A0/v p
fip$04P4,fiR'fiff*1?p$ff
fi0'
fi04'6fi(%041v3[Mfi?3<4-fiE'fiff!
|< fi pNfi(g?pN()fi4fi\+Fv p8fiR
 'fiA@ ff@  ff$p
 v 4  K
fia pff5fi(g?pLr  [!
cfffi?p5+-fiff*1(?3Qr  =493
 Pv 2'4
$ C
fi0v [0fi,'fiIfiR
 'fi$a44
fi ?p
5+F24
 Np
 EPv 44
Q K
fi0 7(b+Aff!

e   EMff B  /Y  i0)EgBY)1F  V\ fiJH=Iff g6))
 
] 
	 @efiff 	  J
aG4E)  & pa0%g6))0K)q)/U ( 0bi  n)/g<~   g,	1
V _ I h  h (J  . l'l   h (K  . l  G^@oU 5V _  I h  h (J  * l'l   h (K  * l.     
h (J  . l  h (J  * lZ  hB<l  J
e /ff~   ~ 9V   I     ~  aff18B	0)/ff 
[4fi(0?p9ffPvv3$fiR'fiO@A'Rff
4
ffOfi( V   I !g{ARvL+F4?L?p-?t3'@
  I 
fi(V _
 G;
 g]{ e$g(o\ ]vf
 @eH\4 " 	0 Aa # \V  I,
I  I BUdPh (J  . lI0U)/Ea # V fi 4UB
dIh (J  * l 
aoM0fi(
V  I,
I  I " fi 4*g3vpYrO3M@1Q *O2N'fiG3ffffph  Tj6T$|Fv*  Tj6T$|> lhi!!*
P.   h  Tj6T$|fv*  T_6T$|Z> lJ#ylL0>W3+ ffpQh  T_6TF|Fv*  Tj6T$|Fvdl+f?N'fi

fi4fi  h  Tj6T$|fv*  Tj6T$|Fvdl$
!
 G;
 g]{ e$g(o\ ] E>@e   " 	0 - # V ffI  BIff1$h (J  . l50>)/	E # V fi 4>B8ff1
h (   * l 
rMfifoM0*53pRrO3M@1
Q *5+S
fid j 0kW(bfi ffph  T_6T$|fv*  Tj6T$|9l8'fiX


p+F4U
@ffffffpYh Tj6T$|fv* Tj6T$|FvdlF'fiYP.  h  Tj6T$|fv*  Tj6T$|9lQ   

 h  Tj6T$|Fv*  Tj6T$|fvdl7 V!UT$?04L48'Rff
44}ff4fiRfi(5?p	
fi4fiRfi ffpmh  T_6T$|fv*
 Tj6T$|9lA(bfi44fid+ffK	4fi	fi(_ff3h  Tj6T$|Fv*  T_6T$|fvdlE!
  I ! A?4I%fiv*9P4P@ff'fi o/PpK?p
HTo1
fi4p?pY()3M'Kfi(qV _
  I ![4tfi(<?p8fiE'fifU?pI()f3'@
@ff'fiG()fiQ?p7'a4Fv'fiC?p7p+9fiK3'@ffffi(2V _


; {

G g] e$g(o\

4(bY
fi 0fi ?84L
4v ff?p j 

 ff042;m
fi4fi!^kT$?04N
fi4fiR'ff7?
4fi$'fiCvffvpGhiS2'fi()[vffvplE*(Oo ( 2F

ff40>()fi 'fiO8v4J'
?pW f
o  * 4
 3 ffW'fim4'fiYR
 

 ff0 (bfi?8v46!YT$?pP@fffiW()fi

?4Aafi 	+F24
 ff
fi
 >
 ffFv  3M0 ff
4fi 	M!1*+Fp
? @ N
+ >?fid+=?0FI?pfi@ ff ?Mfi4A()fi
?p[;+-fi:;'YfiR
 'fiA(V 0fi 4P(?pQ

 ff024;P
fi 0fi 4,'Ep
3 !<T$p
? f@ ff'fi C?A?p

K$

fi

mknpokmkq

;+-fi	fiR'fiQvS?p7E?[3M'7fi(2V   I ()2J'fi	4'()Y?pI

ff4044pK
fifiG4[3p
'fiP?pff4$?ff/vpPV fi >$?3ff$ff
fiYV!,T$?pL3_kfiCfi(V fi 4>'ffF?0  h (J  * l$y
4fi<'fi7fiR
 'fiA024
fi * 0P?p@ ()fi@ f+ Q?ff YM
fi 30Qfi(  * ]^9

ff042;*z
?  ( 2-

 ff0 [()fi Kv 4a'!,T$3
? f()fi4fi\+Fv p8@ Q?pNp
 k
_ 4fi <fi(t?pQfi
 E'fi
()fi$+F?4
E?	?p>

 ff0444;
fi 0fi K4A'Ep
3 


; {

 
]  

G g] e$g(o\

dIh

J
(



*

l

@eH\4 " 	H



-a

# V  ,I 
I  I

BSdh

J  . l0Y)/ffZEa # V HOI 
(

B

PQ1*V+-P
WfidP?3P
fifi j 0k	()fi ffpUh  T_6T$|fv*
 Tj6T$|9l9'fiffph  Tj6T$|Fv*  T_6TF|> l9'fiP.
  h  Tj6T$|Fv*   T_6T$|-l$ yP	'fiP.
h T_6T$|fv*  Tj6T$|>l,
Rm
 ES
 V!6T$?04O46pfffiPfi(kffp8h T_6TF|Fv* T_6T$|-lt(bfi2fid+ff

1pE4}fffiKfi(O?pN'EfiK
fi0fi	fiffph  Tj6T$|Fv*  T_6TF|> lE!
 G;g]{e$g(o\ ]l
 K@e   "   Aa # V KI  B8ff1fh (   . l$a>)/ffCa # V H=I  BIff1
h (J  * l 
[tIoM0*v7rgv3M@1
Q *+-
8fidA?p-4 @ 33
 j akF()fi ff3Qh  Tj6T$|fv*  T_6T$|-l


'ficffp>h Tj6T$|fv* Tj6T$|> lt'fiNP.  h  T_6T$|fv*  Tj6T$|9l6n9    h  T_6T$|fv*
 Tj6T$|> lN

 V!7T$?4Q4QK'Rff
444}fffiUfi(9?p'fiG
fifiGfiffpUh  T_6T$|fv*
 Tj6T$|9l%(bfi44fid+ffNQ?p2}fffiNfi(/?p6'0fiN
fi0fiQficffp[h  T_6TF|Fv*  T_6TF|> lE!
 G;
 g]{ e$g(o\ ] >@eH   Je /	 ( ))&  )/  P	  )1N  V   I  6g b)Za	44
Ei  & aEI ;4* * 6
g ))Y ;4* ( ffff # gg /ff~   G)/E<g     )MIEM~ B
 E     
 \bKCE0)i  ~ ff1Ih (   . lLBYff18h (   ( l 
Hffi$?:ff
E?fiR'fiF)  B0)iCfi(?p5
?pffQ(bfi;V   4fi  +f44R5N'Rff
4/
5fi(fip
fi(6?pL()fi4fi\+FvpMZ\V  I,
I  I " fi *zV ffI  " fi 4*z\V  I1
I, I " H=I  *0fiqV ffI  " H=I, !Acp[4$
fi2p@ffV   4fi  (6
fiP4(fi?pQff
fiC'	fi(?p[fiE'fi-?3F'fi
fi0fi49PfidffC'fiffpIh (K  ( lE!
rMfio/P0*O3vpYrO3@1
Q *O+F?p+004ffmfiR'fiV ffI  " fi 4UhivW?poM0%fi\\l
[LoM0*JGrO3@

'fiSPfid?34 @ 330
 j akK(bfiGffpSh  T_6T$|fv*  T_6TF|9l['fi ffpGh  Tj6T$|fv*  Tj6T$|fvdl['fi
P.  h  T_6TF|Fv*  T_6T$|-l   #  h  T_6T$|fv*  T_6T$|fvdl V*<?2
fi34
RS
fi 4p
 @ ffn  '4fifi(V  fi *FC+ff4fV ffI  " fi 4!sJ.+F2U+F?3q+G004ff
V\ I1
I, I " fi 4L'fi	p ff ff3 Ch  jT 6T$|Fv*  Tj6T$|>l$YffpCh  T_6T$|fv*  T_6T$|fvdlA+F4? j 
k
$?3N' 4fi K
fi fi V*1?04$
fi34K2'fiR
 L
fi 4p
  ffK Uv 44fi	fi(V   fi4 !
[R
 'fi
 V   fi4 46 ff'R
 ff
44>30 ()30/fi
 E'fiff!Ocp,. ff<?p$ ff'fi 0 $34fi I?
+Fp
? & qfi fiRfi p+A 'fiR' 4fi 'fiW Mfi?pZh ! M!*5  ffp
 U43
 ff ffalE*
?pIfi  @ 3'fff/Nv Uf
3M@ Q'!fffc Yfi?3[+-fiff*0?pI
fi 4fi Y()fif' 0fi  p'fi
 Mfi?pPY48' '( @ ffZ'fi?pffp

 K ffv 3U0
.Z'fim?pK
3M '!rMfiO
 o/P0 *
3M0fi@ fi\ cLR
 ff
fiO
 ff'3
.ZI?pK4 p
 I 
 3MfiI@3
 }fi3I+F?r=()fi Xv p
 :
P 	
 Efi1Zfi(F2O
 !Wc
fi34 pYmff%fiW04 hi YrO 3M Kl7?08.a
cf G4[DQ|9sV1c :[|9"$c~HQ 'U3
 ff vpffp
 YhiDQ|,sJ,c :Q|,"FcHN8*V"F|AA|<1c :[cHNLlf P./v p
i
h
Q
D
,
|
J
s
c
,
Q
:
,
|
F
"

c
N
H
8

*
[
D
9
|
V
s
c
1
[
:
9
|
$
"
~
c
Q
H
>


l


W
v
hi #DQ|9sV1c :[|9"$c~HQ+9fi32&?ff Y'fiW
 ff
fi
 U 
 
v 44%'\lE!
" ff
4<?7

 ff024;G4LU. m2p
W
3 ()fiL `_
fi !Hffid+?>+?d Y@ >fi(AfiM:
'fi>
p
? ffPff*t N3N
fi 4p
 N?Mfid+?pff fiR
 'fiNRff
7

 ff0444;Y()fiv 44J'ffff!

KE

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a

A4E(b/v p?4[+F42JR
 I@ ff  Q()fi>3
3 3
 ' pP%fi?m?pP	4fiJ0fifi(iQ%fi3MNfiR
 p
@ff@4fi*tR?pfifi()fi>?3v
@ff6`_a
fiWfi?!T$?p@
;+-fi()3
3 0O
 $+Aff/?Cfi3MC ff v 3fi
 E'fiff#Rff
	

 ff042;
 `yV3[$^a`y`yhiM:
@ /4ff j sJklE*9B! !*5& ff
4v pR?3U

 ffa44pfi(N'fiPfi	'ff*5fi
 a`yV3O^@`y`	
hi 14ff j NklE*,! !*<1ZEv p

 ffa44pfi(['ffPfiP'fiP?P
fi32ZR
 C12ff
  ; ?370Ffi(<?p8r   fi/`$
_ ffYK?pI ff 0v pPfiR
 'fiff!f+c S02
34ff* U
E? p8'fi
?p8

 ff024;Cfi
(  ( H
*  . 
*  * fi['fifv   h  (J  . l5fi  h  (J  * lE*0 ( @ 0
 ffSv Y?pLfiR
 ':
'fif3
 k
_ fi V*/2$
fi 43
 @ ffKfi/
!-A? 3ff5'fi

 ff4044pfi(O Cfi?3f'ffffiF'fiP[4

fi 2p
 @ ff
 fi0B!
[A
 o/a ffi(t Ks#hifi1
)l<
E? pN'fi

 ffa44p*13v p7rO 3@ qQ1*3%fi F?pQfi 
4
fid f
 p+
4fi  j k?0f$
 U. !-cF0 j k'fiC$
fi U@ R
 'fi@ *0F+ ff2V
'fiY?p Lfi(54fi\+-0v P
fi N()fifi pfi(-?3P'ff7 Lr  7!V+c ma4
34d*V?pPfi 
 ff
43
p
 ff<'fiI4fid+ j k7(bfi  j
T 6T$|Fvf 3
 ff
4p
 ff<'fiI04.
 V H=I, 'fi7?pf' 4fi 
fi 0fi ()fi
h j
T 6T$|Fv*  j
T 6T$
| >lg'fi9
fi 4fi  j 

 V!^kGTFp
? 'fi j kL+- Mfi<@ /fi3>

 ff40 
()fi Cv 4a'*M03MA(V
+ N3
   j
T 6T$|fv[29

 ff0 [(bfi  K 4a'N?pC?p
024
fi Cfi
( V H=I, P3
 N?pN'fi j k

 ff0 ;
! d[v prO 3M BQ8'fiP2443''E[zh 4fi0bl

?0 p'fiG

 ff024;*g3M%fi 
+ 3
 ff ffp
 mh  _
T 6TF|Fv*  j
T 6T$
| >lQ ?L_k3M@ !	T$p
? 
 j

T 6T$|<M*0+F?4
E?K+-F@ /fi34

 ff40 h)R
 ff
3 L
+ 73O
  j
T 6T$|fvL4$

 ff0 \lA2L
 Mfi
fi p9

 ff0v !<c
 ?p5fi?p-? *?pF()
9?  j
T 6T$
| >Q2
 MfiLfi p9

 ff0v $4<Lfi/


?0 p!
Hffid+ + Y@ K ffMZ'fiR3PP4} C+f?I?pY ff  pmfi
 E'fi
 #Mfi'fiW

 ff042;!
rO'*6
+ 	v 'fi130
 fi p	fi@ Mfifi A
fi   
 !RTFp
? C17%fi4
 R 
 Rp
 Mfi j 
 
v 

 ff ffk>  j 
 	p
 ff
@ ff *^k8@ ff
 ff
4 ff*1 0X
 
  X
 I
 p Mfi j 
 3MfiAv
@ ff ffk7  j 
 /:
Mfi>3ff
@ ff *^kG@ ff'R
 ff
 ff!C
{ 30 ?pff C'17%fi4Q+F?z Zs<
* ! M!
* m O
 ff L?07
 ff
 0v pQfiR
 'fi9
 mh)03M<M
fi ffX
 fiXpff
 ffE4lgv 
@ ff Wfi0

 ff0444;* X
 7
 sS
 ff 
?FYfi
 E'fif
 3Mfifpff
@ ff >4fi1
V

 ff042;!
T$3
? N@ ff3A()fi$?p>4P4 ffiR
 'fiEF@ Lv 34 fffi114fi3





V\ I1
I, I 	sj	   s
V ffI  C sj	   s
V fi 4B	   sC s
V H=I, C   sj	 s

T ?3K4P4fiR'fiEPfi\12pK'+%fi3MP
E?pff&

ff024;()fi4Afi(Q?p
$
fip:;'KfiR'fiE!,rMfi5?pQp+9fi:;'SfiR'fiLhi!!*{V _ I K4afiE'fiARfffid+AvC?p
?E
?fi(OrO 3M@ 7lE*0+ 3ffK'fiP
fi 43
 $?p9UV ]Rff
!9rMfif?pN@ ff345v 	?4A0R
 ff*/
+ 
fi pffG'fiK(bfi/
3[fi mfi p4'
fi Y?p@ 0
 8v G?pOpYRff
N(bfiN?fi Ifi
 E'fi
?64(b7?p-

 ff042;>
fi 0fi 	hi! !*?p5 ()O3M'@Afiv
( V   I lJ 3g?p)pff
6()fi
?Mfi NfiR
 'fiEA?$MO
fi Mfi$4(b?4A
fi 4fi mhiB! !*M?pQ ?A3M'@\lE!,TFp
? 3)ff
$fi(
?Mfi 5fiR
 'fiE6?,2'(b7?p$

 ff4044p>
fi 4fi 4O?<

 ff0444;Ch 4fi0M fi/
bl
+F42p
 p 9R
 F 
@ ff ff*1! !
* 
 = X
 I
 s<!T$3? $@ ff'fi C4,9()fi4fid+f!69fi1fi./v p>9?pF@ ff3
()fi9?pf4 AfiR
 'fi*/92,0 9?,?pF_0'9'C P?pff f;+-fi:;'KfiR
 'fi-
 
p
 Iv 
@ ff@ 

 ff4044p*tR
 ff
3@ ?p_0'7'2>+Aff/
 V I,
I  I fi
 V ffI  !YT$p
? @ ()fi@ *O'fi
3 3
3
 'K?37v 3fi Y
 ff?0v K?2$@ ff30$+IpffS'fioMPv 3L?pI ff
fi G'V![-fi 04p
 

K$L

fi

mknpokmkq

V\ I1
I, I " H=I,

 X

 V KI  " HOI  !KHffi?.V HOI  M
fi ff
 Mfi8v
@ ff fi0,

 ff042;XOh 
 LlE*g03M
N
v
fffi1
6

ff042;Wh!UsOlE!c"Us#pcRff
L3pI'fiK?pp2}fffi'
ff
3 'fiPQ@ 7
 ff pP' (  ffU()fi fi 37fi3Mfi pff3
 >fi(,'fi
 7 o  ( 'fiC fi?p
fi3Mfiv 3fff3
 fi_
(  ( +f?U?3ff ;+-fiKfiR
 'fi*1Up
 k
_ 0fi S?pfi1
O

 ff0444;Kfi(,?fi 
'fiPF(bfi  Uv 44a>+F42
 fi$RLv 
@ ff ffU$
 pLRff
!-+c 	fi?pf+9fiE*M?p>'fi
@ K

 ff40 	(bfi Xv 04,U($ fi Z8
(  ( 2*< ?pff Kp+9fiR ff7 v pSfi
 E'fi
M
fi MfiQv
@ ff 8?pI

 ffa44pKfi`
(  ( ![r3?pfi *1Up
 
_ fi   h  (J  * P
l 
 yCfif'fi
 ff
 0v pM*M'fiP?pL

 ff042;fi
(  * 4)
 fiFv
@ ff@ ff!6
{ >
fi 
430p
 N?PsZ45
 p)Rff
!
X444V4v 3gfi(M
 ff'fi v p)o/04 %+F?[fiR
 'fi
 V  fi +f44fi MfiJv0
@ ff ,fi/


 ff042;!
[R
 'fi
 V  fi 
 3fi-v0
@ ff W4fi00

 ff0444;
*  	(9, ffp
 */R
 ff
3 f?pFfi 4
ffp
 7?$?4$fi
 E'fif
fi34UY4>h  (   ( lE!9ffc Y
fi 
23fi *M4V?M@ LfiR
 'fiF?F2'(b
?p[

 ff024;I
fi 4fi P?d f
 3XRff
,fin
( UWVM]6v 
@ ff pL

 ffa44p	Oh P
  X
 8
 sglE!MY 
?p8fi?pN? *aR
 ff
3 >fiR
 'fiEq
 V\ I1
I, I " fi 4I 8
 V ffI  " fi 4>?d |V fi 4IF?3ff[@ ff
fi m'V*
?pK
 Yv 0
@ ff L

 ff4044p!
" ff35()fi 4fid
W
+ fv 	?p7? 
E?Cfi(6rO 3M@ 7I >v p
? EffK3MK?pL'!-rMfiT
 o/P0 *
Rff
3 V\ I1
I, I " fi 4C
 Wv 0
@ ff fia,

 ff4044p
* V _ I 
 Z>
+ ff4!	T$p
? ()fi4fi\+Fv pY2>
3P	fi(6?p8@ ff  f@ff3F+ 8?ff 'fi()Q%fi3Mf?fid+?p>p+9fi:;'Rv ff v pfi
 E'fi

 
?0 pP

 ffa44p!>TJfiKdfi4mfid +fp
? ff4Pv 3?pI@ ff3
 ff*+ 0@ ff [fi0Y?Mfi I@ ff3
pff

 ff	(bfiF$
3 p
 '0v p8?4A0R
 ff!

V   fi4 *vV\ I,
I  I " H=I, *vV ffI  " H=I  C   s
 V\ I1
I, I " fi 4*zV ffI  " fi * V   I *zV fiJH=I #K
()fi@
fi
430vpC?4Qff
fiV*V+-$0
b S
fi4pNK@Q04fimfi(9?pPvffvp
fiR'fi-??-@$$
b ff
ffKv?pfo/fiMfi7fi(JrO3@Q!<T$?04,@,afi4-3ff
ff~:


I()fi9330p'0vp[?pFL0fi1fi1fi(i,%fi3M6?pf@fffifi("Wff'%fi5fiRvffFhiv
 ff
fi 	lE!9rMfiF?29afi */
+ N+F4?'fi2'vp32??Mfi NfiR
 'fiE5?$
 Yv 'fi/3
 Q
ff'Afipcp+'' p7+f? 	v Mk
_ ffI@ R
 ffv3830''v 3h ! M!*thi/*V*^
*^* *^V* *^* *!!! l+fp
? @ 
?pOff4204F@ @ ff@ Nv Mk
_ 7@ R
 fi Gfi(-m()fi4fi\+ ffm1
 \l[ 'fi	?pPr  4 p3fi  3
?Mfi f?,
 3fi!TIfi
 E'fi9?9
 'fiP,'fi7?p$' 0fi P
fi 0fi I()fi9 ffp
 
v G
/
 *m
  ffp
 I'fi	 oM4'v3
/
 *%fiQ0S ffp
 I'fi	
@ ffP
 3+
/
 8R
 fffi p
'fi?pQ_0E'5
4>h)?pL
25?$
 Y0K3
?U3M0''Ev pElE!OT$?13A?4-_0E'5
45v 
23p
 ff9fi3M
fiR
 'fi5?0$
 Y
@ ffp+
/
 ff>h ! M!
* V  fi R
 ff
3 N$
 Y0KO
 p+ ffp
 h  (J  ( l'lE*0
+ff4<fi3,fiR
 'fi,?9
 p4} F?p$' fi P
fi 0fi Pfi 3>'fiO
 Lff3
 Ffi(>
/
 
h ! M!n
* V\ I1
I, I " H=I, 
 ff
30 7[
 p4}    h  (   ( l'lE!fT$p
? 7fiR
 'fi[@ 8012p
 ff	R
 ;+
?pff >;+-fi
4@ ff$$()fi4fi\+F

fi 4*MV HOI  J* V fi 4 3H=I, *3V fi 4 vfi      *3V  fi *MV I,
I  I " H=I, *JV ffI  " H=I, *MV\ I1
I, I " fi 4*MV ffI  " fi *MV _ I *
V\ fiJH=I
1!R\V  I1
I, I *zV ffI  * V I,
I  I ffI E* V\ I1
I, Ivfi     
c$4$4%fiA'fiMfi>?R^@`	`gfi(g?pLp+9fi:;'GfiR'fif@Lv	?pN_a'$
4ff!
v!RV

A-?4,fi<+f?0ffNp_kpff79fi(J3(i3MfiR'fiE[h)/4L?3ff,fiR'fiA
?3ffPEl<?

fipY
fi3200W'fiR&r   04()fi4fi!Z{?X?pff@KfiE'fi*94I2Ifi0'fi
40fid 5?pWff
 3ff,fi(%N02*'fi7,6'fi7?A1fi30>33(bfi@ff@Oo17

K=x

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a

R0<
fi4fi!PTtfi\3M?p3(i3v3ffQfi(A?pffff7vp	fiR'fi*g?pvffp
pffP'fiW
p
? ff
.&?P?0
 MfipEffqR3@ ff ffI04 hiB! !* & hB<
l   lE! [?Mfi3
3 ? Mfi
M ff ffPv I?0460R
 ff*
+ F@ [
3M@ 8p
  fffi0v 3ce
 9O?Mfi/6()fi,P./v p>?46
3
? ff
.
3 pI?pN
. fid+F ff3
 Lfi(t?3> ff  pI?$+A$Mfi p!
T$3
? 904
34g
?Mfi4
 Afi( ff v 3$fiR
 'fiO@ ff@ ffIp
? @ -+A6fiff71L()fi3MO()
'fiE!
rO'*?pff 8fiR
 'fi[' 48 'fifff':B'fi:;40 ff
 QR3Mafi 5fi
( ' ffQv G0 *


+F?2
?4N?3@ @ ff fi Wfi(-r [L3 ffWv mfi3M>4P0 ffO
 fi #hi  ff
fi ClE!  ff
fi *
?pff CfiR
 'fi8
+ @ C '0 ffm?p	4E3M@ !UrMfiO
 oM0v j
* 34}fffi  'R
 ff
4`:
}fffi fiR
 'fiA@ N
fi 2p
 @ ffP(i3
3 0O
 a()fiA 3
 fv M( @ 
 8hiG4
E?4'./*%vffw Q>lE*0 
 ff pa<v 3Qr  ffp
p
 ff9@ LRff
 F()fiX
 fi/v pLr  [FhirMfi ffB*%vffww ClE!9T$?V*?3ff $fiR
 ':
'fi$p
 N
4
@  Nv C?pL
fi o1$fi(O024
fi 9?$+@ N
fi 43
 @ ff!<rMfi3M?*M?p
02
348o1fi Mfi ffI@ ff ffXp
? @ 	(i
44C%fid
+ ()30,?pfi 4
5  ff0E4
<@ ff3
()fi5@ ff3
v 3>?pQ4O
 N
fia o/4;fi(t@  `_
fi *M5?Mfi\W
+ Kv ?pQ@ ffPv 3
 -fi(t?4-0R
 ff!
$



G$fi  R)#: ('2*),+13ff



 &%

-+/.

 R

0'-  21 3R   34

3

 M
3 aff
fi>1!p_pffE30()300ff0vpLfiR'fi5
E?pffP-'fifi10(b3M'fi'fiff3ff
yh VI  hB<l;  hB  l'lAK?3L'4fiK
fi4fi5fipOffpffIhyVI   hB,l;   hB  l'lE!9T$?p
@ff30<v ?04< ff
fi ff'042?I+F?4
E?fi(?pff Ffi
 E'fi9
Ep
? ffPZ
 VN@ fN0fi
 3E ff'fi
@ ff@  7p+9fi	fi
 ;U
4 ff[fi(,v @ ff'h)+c E4 
 I SW
" ff'%fi  \lE!QT$?4f ff
fi m30O
 ff
?P4A ff7 v pSfi
 E'fiP@ K02 ffW'fiRv p 	r   02 *<! !*A
  (1fiNH=I,J fi
  (	K
fi !
 ff
fi 	x7M@ ff@ ff,?pf' 2fi Pfi(V?3FfiR
 'fiA02 ff'fiI7 p $02 v 'fi>?pff4X
 Rff

fi>fi/3
602 Uh)(bfi5
 '2 
ff
fi lE*/ ?Mfi\+q?2<ff
-?pF@ ff3ff!t
{ [R
 v II()fiP44
 k
p
_ 0v p8+F?$+>O
 ff K1 j ( 7P
? p> ff 0v p8fi
 E'fiff!^k

}

b 65M]

 87


 
^ '['^



 Z



]\_['^  z]$gi^j['^

 3M6fi @ ff
f46'fi>fi\+-9?p54O$
fiP0oM;8fi(%@`_a
4fi!6T$?p54pfffi43Mfi4O'fi74p/:
Q
()    >)a>4)LO?fi1Nh  GsJlE*1+f?4
?	@NP
E?vpQff0vp>fiR'fiEA?$@
CfiE3ffm'fi	@ff80fiRffIhi4'fiK
4ff j 
fi@ff
@pffN@ff1v3Pavpkl
 G@ ff13@ MfiC3

3 /:B4
 >@  _a
fi !LrMfiLC02 mGfiRpU*3M0fi@>`_
fi
?N30

 ffp
 ff4fif'fiY ff7 v pM*! !'
* g
*  & hB<l[4a4 ffB
 n  hiB! !*g 7lE!8T$p
? 
P
E?v p5 ff 0v pFfiR
 'fi;
 VMhB<lg2t   GsY(a 8fi >(0 `_a
fi I4j
 3E ff'fiL3

 ff
()f ff 0v pM!O+c Yfi?p$+-fi*(<  
 VMhB<lE*?pm#  204 ff$   !
 30 ff
fi 	M!Pfid/4p
 ff$@ ff35%fi3M5?3>4fi%( pKfi(g
?v 3> ff  pIfi
 E'fi!
 fiO

 7fi(6?p> ff3fv  30 ff
fi KM!@ p ![W
H  ?3ff ff*%?Mfip
3 ?U+ IM
fi fi[?d 
 &GfiE

 3 K()fi?pff K ff  pUfiR
 'fiff*  ff
fi  Cm?Mfi\+F8?I+-K
 XR
 ()fi
@ _a
fi Yfi ce
 	? K'fiV  `_a
fi Y(bfi 

?V!

 Ma[I]v;@KK

bFE:9* e$g

s$309RvC1C
fi04pp>?pN04P4FfiR'fi!<TF?pQ@ff3-()fi$4%2PffiE'fi
@ I
fifi44E ffFfi(9;+-fi	()3
3 0O
 J?pfi ffP*T$p
? fi@ ff8v8 1*%+F?4
E?Sff fifRI4PO
 :
4ff4Wv 3 !Wrfi
 o/P0 *,ff@ ff'fi a 	'fiR3'R
 ff
8?P(F  ffp
 Y283
 ff ff
'fiO
 +fp
? @ fi pK?pI0?R(bfiC'v L'fiY	@ ff'%fi  *?3m?4Q
fi34m
3 (i443@ 7fi(A
"Lfffi 0 NfiR
 p'fi?fi4	R
 ff
3 N?pL ff'%fi  L4)
 Mfifi pF

 ff0 !6ffc K(i
*0?452L
 Mfi

KE

fi

mknpokmkq

'3p!O{?,
32?RO46?9pfffi<@ff3
5?pT37R<fi(''v36vI?pF433fi!
c(a?p5fiEv/4p30fi$4~_$ff<?p5fiRp>?pPfiLMfizff,4O?p$P2<4p3fi!6T$?3fi@ff
vN()fiP44} ffA?4!

 M@  -

,   Vr
bi0Fff        VmO  
eJ/ff & hB  lZ & hB<l 

9* e$g

g6))([ff4XJ~P  - -P)ffi6B   /g6))
   ( aR  m
V   hB  lE  hB<l  gg/'  hB  l  hB<l 

gieevb

T$?3-433fi-ff>R4@ff71L?p54fiLfi($p+ ffpffO?0O?0ff)p+FLvffpff
'fiI
fi 4fi !Oc
 7?3-fi?3,? *R
 ff
3 ? I

 v3N337@ffPvO

p
33R@ ffNfi)
( pC
+ ffp
 ffffZ
*  & hB  lQ4a4 ffJ
  & hB,lE*g W+ @ p 7@ ff3
 pK?p
}Nfi(O?pL2 p3fi !,T$3
? @ (bfi * & hB  q
l  & hB<lEB
! ]
T$?pff3Q%fi3MN?p
?v3Ivffvp	fiR'fi>
?pffPEV\
4v ff

I,
I  I

V

fi 4(bfi2fid+

7
fifi`:

	]3g=<  V  I,
I  I   ?> - g6))~  MffAB	 # 'E #  
gieev b [3  P!8T$?3O* & hB<l[204ff8 !8DT_pPV\ I,
I  I hB<l[  !I9
T$?pfiffuv* & hB  lE & hB<lE!CTF?p@()fi@*W6* & hB  lQ402ffJ  !{C
fi
433?
   *B!!*v\V  I,
I  I hB<l$ P!8]
Iekge

Ttfi8R[
fi4'A+F?T$?pfi@ffv*v	-fifi44>fi0Uh)03M?MfiAv?3f@ff-fi(J?pfalE*+
3 7  ()fi$?pN0@ : V fi 47r   G(bfiF?pL%fi'~: V fi 47r  >*B! !v
* V fi 4hB  l6=-!

	]3g=<KE V fi 4W 
&   /  f' Ei  
gieev b [3O	   
Iekge

0LaE  Eb # @>

- Y #  M # 

)0ff M)

\i0EGa

 !CT$?386*  & hB  lQ204ff= P!C-T$?pfi@ff v* & hB  lE
P
& hB<lE!T$?pq+-m
3MfiS
v?K P*$!!*5?V 4hB  l !rMfiKv0'
*
fi

fi33@oM0Y()fiffc 4 
 K0fiR
 ; ]  )fi1

3ME(F
+ U&#

ff0 ff3K+F?
'fiK
fi4fi*)J
! ]
H fid+z+K
fi4pISfi,@ff34>()fi|V
f
?pfi@ff (bfiFfiE'fif
E?pffPPV1!

ffI 

"V

H=I,

!W*6+-	RvW+F?Sff

 M@ E - 5   	qVr 6g b)[ff4 1Kff  0G4ZVUt    	 ( am 
VG  hB  lB  hB<lgg/'BAr   z
 y  h (J  * lE  hB  l   MW)/V/h  h (J  * l'l 

h
 ln
J
e
M




B
h


l

B
h
<

l
&
&
 (J *



gieev b  4P42-'fi?pNfi1fi(tfi(6T$?3fi@ffv!8]
  # ~ EM # 
Iekge	 ]3g=<  V ffI    ?> - >
gieev b  4P42-'fi?pNfi1fi(tfi(,-fifi44GvQfi(OTF?pfi@ffv!8]
 b # >?> -  K
  # ~ EM #  )0ff M) \i0EGa
Iekge	 ]3g=<> V HOI    0LE  E
&   /  f' E i  
9* e$g

Off

fi[

gieevb

t}-m{t/s$q



os0pt{a

l qM 
 a

 4P42-'fi?pNfi1fi(tfi(,-fifi44	8fi(OTF?pfi@ffv!8]

{[
Md+#?p$()fi4fi\+FvpL
fi0
43fi0O(bfi

?p$?pfi@ff,P
fifi44ff @ 36@ffff

 (f?pKfi3:;'qff0vpGfiE'fi*-?MfiK? 3ff#'fiR  GsJI()fi
Q
0fiR;U.V\ I1
I, I *WV KI  *GV I,
I  I ffI >h)+F?4
E?S204ff$?BV I,
I  Ivfi      4[4'fi	
 SsW(bfiF 	fiR
 pMlE!




{)pffp6R-
fi
pff8+F?7?p9_0'g'v8Fp+9fi:;'fiE'fiff!OcpO4_3ff
'fi
 L   Ssnh)R
 ff
3 V\ I1
I, I fiq
 V ffI  45+Aff/$?pN_a'5'lE!

H o19
fi4p6?3fi@ffP<?0,@LpffpffP'fi>M@ff6?p5;+-fi:;'CfiR'fi!:[?Mfi3p?I+
T
()fi33C@ff34A(bfi5?pNfi3:;'YfiR'fi5?05+-@c3Mfi3p?	'fiP0M@ff^@UWPfi;*
+7
+ @ 7$
3 0 L'fi	MfiC4. +F4@ L(bfi[?p8;+-fi:;'fiR
 'fi!QQ3MF ff3F(bfi[?p8;+-fi:;'fiM:
'fiAp
 EPv pf+Fp
? ?p-?pff [fiR
 'fiA@ Ypff
 ff2  Sst,()fiA+c 4 
 ffiAW
" ff'%fi  
fiR
 vffLv 04
34ff!r3M3M@ +9fi.+f44O
fi 43
 >fi?p7fiR
 ;
4 ff!	T$p
? P?pfi ffP
@ F13$ 34 !gTFp
? A_0'6?3fi@ ff2'vp32p
? fft?Mfi F ff7 v pQfiR
 'fi6?0,+F422'(b
cff4 
 N0fiR
 ffA()fiz?Mfi >?$+F42k
 Mfi

 M@  zI)aI4E)GEMffB   M;UBE@@> - g6))8'  ff5BU #
 Ei07~EMffE #    0R #   

 Cz0  - ~)}=  ggi   C@C}g<4  ;
EMffB   bFa`i  )1A)/IE~B   (   )/IEE  }b`b # abi  

9* e$g

 3M%fiP
 P sm@[%fi?'33!<s-c+E4
F0fiR;P  ]  )J!:[3O[

4,'3pFfi(Vr =R4fi,'fiIffv3M!gT$?3P()fi-''vp <  & hB<lE*M973,RF?p[
[?
 
 )W4Q'Ep3 v  
:;'Pfi( < !Icp(-

 ffa44pYfi(-'fi>4Y MfiLv0
@ ff ff#hi! !* G  
sglE*0?pY583'$RQ?p>
 >?L K
:;'7fij
(  	'v pL & hB  lE*+F3
? @ 7   VMhB<lE*
4Q4'fi	K
:;'fi(-'fiO
 ''v 3Cv  & hB<lE!7T$3
? @ (bfi *()fic
  G''v p & hB  lE*VQ73[R
?pL
 N?0  )Y4A'p
3 Nv  C
:;'7fiN
( 6!<+c 	fi?3$+9fiE*Mfid/v pI' 0fi 	
fi 4fi 

fi3
3 0Yv Y Sr +f?Mfi3M$v0
@ ffv pP

 ff042;+F44
 MfifF?p7'3M?Kfi(6 U+c E4 
 
fiR
 p*M+F?4
E?K?Mfi45  Y
:;'>fi
(  K''Ev pv 	?p>2 p3fi Nfi(O?p>r  7!
 3%fi QXfi fs<!ffc 
@ ff p$

 ffa44pFfi(M'fiPg404v ff0?p,fi042;5fi(Mv 'fi/3
v p
K
:;'Cv 'fiO
 P''Ev p( & hB  lE*V+fp
? @    VMhB<lE*J?N+A
 fiLvm '' pfi( & hB<lE!
T$?2f
 R
30 14fi4fi Gfi(9 mffc 4 0
 80fiR
 ;*VL U?pP
fi$
3 @ oM0v I S?3fi1fi(
fi(g-fifi441;
! Afid+Fv 3>?  )K4,'Ep
3 [  
:;'Nfi
(  C''v p7fi( & hB<l60fid/4p
 ff
 Mfi
3
 >?0  )S4A'Ep
3 Lv  K
:;'8fi_
(  	'v pfi( & hB  lEB
! ]

gieevb

 v0
P+-C@ffMR?0ffC@ff3>'fiS
fi\?pfip:;fiR'fiE*O+-fpffWfi0
fi4p7?p
;+-fi:;'GfiR'fi!

	]3g=<K Je /C)a4)RE~B  /  V\ I1
I, I " H=I   V KI  " HOI   0V  fi ~
M~;EBXE7?> -O g6b)'  MdNB # \Ei0>~EM #      C)-  )/  
EMff B  2
 Ca  - 
e /X1ba& b=EMff B  /  \V  I,
I  I " fi   V KI  " fi 4  V _ I  a
J
Iekge	]3g=< 
V\ fiKH=I ~ R0aE  E b # @> g-  6g b)~   MffPB# #X \ i0E	~EMffE #  EE    
)t  )M  IM~B  2C 
Iekge

O

fi

 { 

 Tj6T$|Fv 

/













mknpokmkq

 { 



 Tj6T$|fv 

)  R 
Tj6T$|9 R
 
/  
) 
Tj6T$|>

/

















)  R 
Tj6T$|- R
  . X

/

Tj6T$|Z>

rO3M@>w1,TF?pL3M'fiP  vPhi()ElA  v


h)?ElE!

T$?3Ipo/F?pfiff 
E?
}ffQ?fi8vffvpfiR'fiEF?Q
3Mfi[R3ffG'fi
R  G
 sJ5+F4?	@ff'Rff
5'fi"Lfffi0NfiRffff!

 M@  N # I)a4)mEM~B  /S)1Q)}dME	Sag  }E)8g6))
m) ( 
 ); # 'pMb     }bC)BC)/Vr Mff18a[M~;EKBI?>    &   /   f'Ei  
9* e$g

gieevb

[3
 r   4'$
_ ff7GL
" fffi 0 fi
 ;4fi7'fim ff  pM!ST$3
? @ (bfi  
' 'pS

ff#&=4~_$ff8?p	fiRp!RrMfiOff
?q

ff&''v3M*:Zv0'
Rh)fi
?p	_0E'v'
Y4(f4rO~:;"Lff'%fi	fi;l7fi(f?pK'E434(bfi2fid+ff
1 ff'%fi  !  3M%fi C?pYP
E?v pYv ff v pmfiR
 'fi 'fi/3
 ff
 3+ ''v 3G+f?& 
v /
_ ffC@ R
 ffvp	3M0'v p 'fiC?pI4 p3fi !LT$3
? SQ4f%fi0v L?Q?p8@ _oUfi(,?4
'' pPR
 (bfi L?p7 /k
_ ff4P@ R
 ffvp3M0'' pPv 
430p
 ff5P'v [  MfiP@ fffi 0 *a U?p
v /
_ ff@ R
 ffv3P3M0''Ev pIMz
fi ffW
 MfiFv
23p
 Q@ ff'%fi  
! ]

 v0
P+-C@ffMR?0ffC@ff3>'fiS
fi\?pfip:;fiR'fiE*O+-fpffWfi0
fi4p7?p
;+-fi:;'GfiR'fi!

	 ]3g=< [bO  )/}<g 4  ;4)UM~B  00ELM~;ESBO?> -O g6))
'  fffB &   /  L~EMffE  EE  )/ # 'bZ)/ (   [  )R)/Q/bi~`;SB
)  )/~ff       )/ #  # b}~ffM  })  g6))Yb ( b; # ~Mb     }E)  

Iekge

 fi04pON
fi3M0-fi(%4443'XoM0vfftfi(%T$?pfiffL0846
fifi44*30vpQrO3M@5w1!
D<EfiI'fiRffv3Xh)?pYr   fiX?pY()Ifi([rO3@YwlE*6*9+F?p@  & h  vdlE*`  DZ>1*,()fi
"Lfffi 0 	fiR
 ;D>R ] hi
  calE! [3O
 KfiR
 'fi
 V   fi4   
v   v  p
 ff ff
 ffp
 
h j
T 6T$|91*  j
T 6T$
| >l$ \
 34} ffF?p>' 0fi Y
fi 0fi Kfi  ffp
 	h  _
T 6TF|91*  _
T 6T$|-l
'fi j BZk	hi NrO 3M@ NwLfi C?p[ ?ElE!<T$p
? ?pQ''v 37
fi 2'vpLfi(JC()fi4fi\+ ff1v /k
_ 0ff
P Y/]^8h)J*^/*^/*^/*!!! G
l  & h  v  l-a3MP i
 D>1!5T$?4Fp
? ff0$30$'fi	 >+F?
 V  fi 2L
 MfiT3ff
 ff44
   GsX(bfiLW

" ff'%fi  IfiR
 vff!NT$p
? O
 oM0 4423''fff+F?
 V\ I1
I, I " H=I, 
 3MfiNR
3
 ffK'fiIR
 [   Gs()fi5L
" ff'%fi  [fiR
 vff!grfiR
 V ffI  " H=I, *M3M%fi f?pN
fi 4fi ()fi
h j
T 6T$|91*  _
T 6T$Z
| >l$2 j 
( #k	v   v*  j Z
(   k	v   v  03MT 1?v pfff4 I4f?pO
 
F Krgv 3M@ >w1X
! Tv *M
+ >
 U@ L?pNfia ff(bfifL
" ff'%fi  N0fiR
 ff!



{8
fi
43pQ1K3PPE}ffvp8?pN%fiLPfia@ff30

V\ I1
I, I *V KI  *V\ I1
I, I ffI N0V\ I1
I, I4vfi     

ffPfiEVfi4
dlE!

OK

@  SstN()fi7G0fiR;#ho/@ff40v

fi[

V\ I1
I, I " H=I  * V  ffI  " H=I,


V

t}-m{t/s$q

  fi4



os0pt{a

l qM 
 a

@  GsJA()fiFcff2
NfiRffff!

K?3p7Pfi@ff3


V fi 4*{V H=I, *{V fi 4 H=I, *{V fi 4 vfi      *V  ffI  " fi 4* \V  1I 
I, I " fi 4*{V _ I



V\ I1
I, I " H=I  * V  ffI  " H=I,

4

Sst5()fi$c+4
NfiF"Wff'%fiNfiff!
V

  fi4

V\

fiJH=I

Yfi?3ff
ff~:

@MfiL3ff
ff44  GsJA()fiF"Wff'%fiNfiRvff!

T ?3Y()
?4$?M@Uff7vpmfiR'fi?2'(b?pU

ff042;X
fi4fi#@
$
3ffI'fi[  GsJt(bfiOcff4
9fiRfft4g`_
*Rff
3-c+E4
-fiRff
@Oo/'@ffO
 ffm3 (i3O 
fiPPfi (bfi> ()1v 3C'/'ffP> P G4PfiN024
fi 
pffK
fi 0ff$fiR
 ffAfi(g?45
27hiT
 ffO T
 F!*Jvffww QlE!
rOv 2*,()fiuTFp
? fi@ ffPSvY qR+ Uv ff pff&??pUp
? ffPfi(Q?pUfi0 ff()fi45fi(
?pOp @ ff3Q4
 ff?3L V HOI  fiL V fi 4P'J!7sJ7v G?4[0R
 Q+ P@ ff
?pff Q'fi30 ff'fiO
 Q'0A1k
_ 0v p8fi@ Ye
 5O?Mfi/-(bfi5p
 ff4 p7+f??3ff? C'fi
@ _a
fi (bfi

E?!,ffid
+  ff*/_0'*1v P?3po/9@ff
fi *1+ Q
fi 43
 9?Mfi\+fi3MAL0fi
@ff305@ L' 04ffK()fi Pv p Q'fi0fi103
5r  =()fi[
  
ff
fi !

 3\fi   :X-: fi 
cffW?28@ff
fi+-	M@ff'2 
ff
fi3 +F?3@ff
E?&fiPv70Z3@ff847fi\+WXv/:
  

DBE

 k 3 R1 3R

  34

/430tr  7*V03MN()fiL_a
fi?pfi/3
Nr   3ffN'fiYR()fiOffRR`_$ff!Prfi
 
ff
fi *[ffv3$fiR'fig2ta4ffN'fiQ7/43fi6r  &>?3>?p9fi/3

4Q()fiO
 ff!T$?3@(bfi*JN4cpff
ffm'fiY
fi4pN?3'44fimfi(-ff
E?Wff0vp	fiR'fi
()fiv 41403'fiYfi130
[r  7*V ?Mfi\+ ?>Rff
L?pP	0fi  Gs&@ ff34Q@ ff ff
%fid !
rMfi,fi
 E'fi
 V ffI  " H=I, * V\ I1
I, I " H=I, * V ffI  " fi 4* 0.
 V\ I1
I, I " H=I  *+ 5
fi 4p
 Ofi L?3-'E ~:
4fi 0Qfi(9?p4P4 7fiR
 'fiE!PT$?4Q4QR
 ff
3@ ?pP' 4fi [fi(A?pff fiR
 'fi7@ 
4P0P' 04fi Afi(O?pff454P4 [
fiPfi 3!9T$p
? N@ ffP v p8' 2fi $@ 








V ffI  'E4ffF'fiV ffI  z<\fiRV\ I1
I, I !
V\ I1
I, I '4ff$'fiV ffI  z<\fiRV\ I1
I, I !
V H=I, 'E4ffF'fiV H=I  z<\fiRV fi 4!
V fi 4L'04ff5'fiV H=I, 0z<\fiV fi !
V   4fi  '2ff5'fiV  fi  0z\< fiRV _  I !
V   I 'E4ff5'fiV   I !
V\ fiJH=I '04ff5'fiV  fiJH=I !

cNPff
 MfiNR8v 34 >'fiK?pI@ ffp
 L?fid+ V H=I  
 m' 4I'fiV fi !>Ttfi	2443''E*a+
3IrO3M@Cvy/*+F?p@8?p8'4fiU
fi0fi*%3
E?SPhi
dlE*VpMfiIQfi(2\z`]_Y^ 1ffU']

fi ff!  3M%fi .V H=I  4f004 ffS'fiffp
 Uh~v*Jl[v S?p (bPfi'Nr  'fi	?Q?pI' fi

fi 4fi 72_
 Mfi\+hi|
 UlE!OT$p
? 8
 p+ ffp
 Lh~vv  */v  lt4gp
 ff7'fiQ?pAfi/3
Jr  h) ?fi'

Off

fi




R 

 { 

/


FG



v



R 

 { 

mknpokmkq

 
v

J;

8





 { 



ZLK

M9

 





/v




vv



)

HI

FG

R 



HI




rgv3M@Pvy/AYp4}ff4fiY
KRff
fiO>0fiKv	fi130
!
v rgv3M@7vyl<+F??p['4fi
fi4fiPV!6"Wff
40?09'fi8(bfiE?pffi/3
,r  +-f.
?pZAff4fi130
Cfi(8?p2
ffU?3RvEff
fifi(8?pR'0fi
fi4fi!
sJ4. +F4* V ffI  '04ff5'fiff4?pRV KI  fiRV\ I1
I, I 	?pNfi/3
5r  7!
TtfiW4423''C+F?
 V  fi 
 q
 ff
fi
 V   I  X?pKfi/3
*<
+ U3@ 	rO 3@ >1!  3M%fi 
+p
 ff ?pff3
 GhiT$T
" fH  ScT$T$cHN8*t"f|-A|,,c :[c~HQLlN Wfi\ ?3' fi R
fi 0fi 
'fi ff3
 hiT$T
" fH  ScT$T$cHN8*,T$T
" fH  ScTFT5cHNLlE!,TFp
? &?p\4fi0F'XhiDQ|,sV1c :Q|,"$c~HN8*

DQ|,sJ,c :Q|,"FcHN8*0T$T
" fH ScTFT5cHNLl9R
 ff
fiO
 fff

 ff0v N(bfi v 4%'	hB$QstsJ|-5T5cHN8*
"F|AA|<1c :[cHN8*%T$W
" [H  Uc~T$T5c~HQ>l51Y./v p	7344fi Q
fi #hirt:;p
 ff44 B
 qcp:B@ ff
 ff s:
' PElE
! D<@ /fi3*?A834fi -
4fi ()fi
 ff?p[fi/3
,r  'fifi8'fi	hiDQ|,sJ,c :Q|,"$:
c~HQ8*aD[|9sV1c :[|9"$c~HQI*0"F|AA|<1c :fc~HNLlE!
{?4024
fi LfiS?pff@ ' 04fi 8?d C()fiI?p	( pWfi($?3C ff7 v pSfi
 E'fi
()fi>?pfi/3
Lr  NTFp
? P%fi PUfiEO@ ff3L(bfi.
 V I,
I  I " H=I, 
* V ffI  " H=I, *g 
 V  fi ()fi
@ ff@ 1 pQ+c E4 
 $fi
 ffOR
 ff
fiO
 Wp f(bfi9?p$fi130
!gT$?04646R
 ff
3 V HOI  Pff
Rff
fiO
 EV fi 47 0
 V  fi ffYR
 ff
fiO
 .V   I !FY
 U?p>fi?3[? 0*?pL%fi4 >P0fi%@ ff3
()fi.
 V\ I1
I, I 
* V ffI  
* V I,
I  I ffI N 
 V\ I1
I, I4vfi      0@ ff /v pU26fiR
 ff>@ ffPv Rfi P()fi
?pIfi/3
!7rMfi
 V\ I1
I, I n
* V KI  n
* V\ I1
I, IaffI *V 
 V\ I1
I, I4vfi      *V?4Q4P04 ffF?N?pIfi/3

r  C3 
 pffQ'fiKR
 I(bfiEO
 ff*@  _a
fi mM
fi ffc
 MfiL?ff 'fiK
 IMfi p*t G?3Q?3@ P4
0mE3
3 /:B4O
 C
fi'*X X()fi7300 fi Pv ff v pG3M'fiMfifi30a
! [8
 fi 3ff%fi\ *
?pY'fi3M0 ff'fi
 Y0fi(L45fiR
 'fi@ S3
3 K'fiW?pff
 V H=I  fi
 V fi 4S
fiPfi 3!n+c q?p
po/ff

fi &
+ Kp
  fffi#O
 ?Mfi/I()fi@ ff3
 pU?pY
fiP0 oM;Zfi(F@  _a
fi Xfid 'fi
@ _a
fi K()fi 
E
?Y+fp
? K?pff NfiR
 'fiEF?d >R
 K04v ff!

v

 %G{3}?  
"Lff
2<?7fiE'fiV ffI  0V\ I1
I, I 
3fi7
3fi0vffPN+F?R?p(;Rfi(5ff7vpM*
+F?pff;V H=I  PV fi F@52'.	hi!!*WMfi,N0fi3ffP'fi>  SstElE!6r3M?pEfi@*
V H=I, 0V fi 4	

3Cfi0vffPN+F?pW?pW@?p	ff
fi'WYp+9fi:;&fiE'fiff!
NOi 

 

r fi3$ff*A+-Y?0ffS3fffiRff&v
@ffO5@E`_a
fi&fiE?PI()fiP?pffUfiE'fi
M
?F
S_a
4pff
@ff7?pN4OL
fiP0oM;Cfid['fiV@E`_a
fiK()fi 

?!
" ff
4,?08?3@ C p+9fim+Aff/7?08fiR
W
 'fiE7
 X

 ff4044pa
 fia4#hBLl>fi
fi/
4&hisglE!gr3M?pEfi@ *O@ ff
46?0.V fi 
 v 
@ ff C

 ffa44p ff?3>+Affq!h W:
 msglE*
+Fp
?  ff
 V H=I, 
 Wfi v 
 ff 

 ff4044pG4fi1
44XOh W
 :
 GsglE!J
{ 	d?EV H=I, ?Lfi 4

O

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a

 j fi1
2} ff\
k Rff
IfiZ

ff042;*O+F?pff>?pff
Ifi(V fi 4CPffRE0?fi3p?P
0Qfi(9?pr  >!T$?384P044
fiS4[?Q+-P
?0ff e
Q
@ffO6O?Mfi/
()fi,@  _a
fiP2fi@ffP()fiV H=I, *+F?p@ff,+-[
3Mfi-Mfi744.+F4A()fiZV fi !Oc+fi?p9+-fiff*
fi@ ffi1
2} ffO
 Rff
,fi

 ff042;I402 ffg?,,2:
 ff <'fi8fi/
4} 5@_a
fiP'fiv
'R
 ff!6T$?264,4fiL'p
3 f(bfi9fi?;+-fi:;'	fiR
 'fi9?9?d qV H=I, ,?pff4, ff
fi C'V*/! !*
V\ I1
I, I " H=I,  
 V KI  " HOI  @ UO
 a Y'fiZ 
@ ffO
 7hifi/
4} ffl8@  `_
fi ! ff
30 
MfiM
 fiG4
 v pffX1Z
fi 04p
  p8V fi 4KR
  *9
+ Yp
  fffi# 
@ ffO
 A@  `_
fi 
 fi4?PN()fiL?pPfi'p,fiR
 'fiE
 V\ fiKHOI !TFp
? ff P fi4?PLaU'fi8V fi 4 0R4
fi?p['R
 ff
4
 ff$fi
( V\ fiKH=I !
{ Q?ff Qp

  fffiR
 ff;+-fi7pR
 ff9fi(Vv 
@ ff
 0@  `_a
4fi  fiE?Pg?Mfi f?-(bfi44fid+
?pC044
4fi fiR
( V H=I, *6 W?fi ?8()fi4fid+?p0044
fi Rfi
( V fiJH=I !Yrfi
 ff
E?fi(5fi3M
 ff
 0v pGfi
 E'fi*,fi 3CfiPPfi@ Kfi(F?pff@ Y fi?82044
a !R
 (bfi@ K0@ ff vpm?p
v 

 ffO
  fi4?P*  3M0 ff
4fi C1!4v[@ ff ,p+9fiI fi4?P9(bfi-'fi@  `_a
4fi P()fi


E?p
* O
 ff*fi pF()fi-+c E4 
 F0fiR
 ff< ?pFfi?39()fi-2fiR
 vff:
 o10@ ffa $
r  [*M9
+ ff45 	 fi?()fi-./v p8?p['fi5fi130
,fi(V?pQr  [!<T$p
? ff Q fi4?P
04'fiY
  (1fiNHOI M *%
  (	K
fi *0fiL
 '2 
ff
fi !  3M0@ ff
fi C1!
  fff 
@ ffO
 V fi $fi(
4-?pK fi4?P8v   3M0 ff
4fi "C1!4v!T$p
? ff 	 fi?0PI@ K044
a +F3
? Z?3	v ff v p
fiR
 'fi$4
 V\ fiKH=I fi$ fi(t5'R
 ff
4
 ff!,r3?pfi *1?304'fiP fi(O
  (1fiH=I M *
  (	ff
fi *$fiK '2 
ff
fi3 !  3M0@ ff
fi C1! >?Cv 
@ ff
 f fi?P()fiK
  (1fiH=I,J  
  (	ff
fi */v ff v p7fiR 'fi V H=I  */ 0ffc 2 
 [ 0(i34aL" ff'%fi @ ffi ff,v 02
34ff!
T$p
? $@ ff
fi P
fi 
430p
 ffO+F?I?pfi 4
0 
 ffP04
1@ ff36
fi0Ev pN?p54
 $
fi0 oM;
fi(<?p8v 
@ ff
 g fi?0P$+F4?Y?p84O
 8
fi0 oM;Kfi(6?38
fi ff'%fi v p'fig fi 
hi$
+ ff4F+F?ff
?Sfi?plE!
T$3
? Ifi< Sp
  ff4fi0v p	4tfi(,?pv 
 ffO
 g@  E`_a
fi  fiE?P[4QPoM4Pj
 eP:

 
!9T$p
? ff >v fi?P5. >?pL3fi 	?0[q fiA'fiC ff v 3M*1+f?4
?K
 ff 
?-
 fiE<()fi3
3 fi @ /fi3O `_
fi JhiEl<?ff [@ ffMIR
 I_
o ff!6T$p
? Pv ff v pQfi/
:

3M7yh V/hB,l,  lE*()fi4fi\+ ffY	?p> 
@ ffO
 @  _a
fi Y fiE?hi 7rgv 3M@ vdlE!-W
H o/







 -3,

fi 4p
 -?p  /0a
 SEFaff;a

fi(V?p[ fi?*+F3
? @ F
+ f3O
 TfiP
P 
fi :
! [4fi(V?3Q 
@ ffO
 0  `_a
fi 	 fiE?P,@ ff ff	p
? @ Q@ [fi3
3 mhi! !*
+Fp
? 3 ?p&
fi 
430p
 K?P@ E`_a
fi #3

 ff*5P4v &(i
P'p
3 Y?0C   7l8()fi
+ ''@ffk0fiR
 ff[  j 0@ ff
fi v ffkP0fiR
 fff(bfiQ+F?2
?S?p3fi 4T
 o1 ff~:
j Mfi\W
0v L[k
 3j 
?gr  7!aD[fidW
+ 0''@ff fiR
 ff7h)+F?4
E?Uv 
23p
 LL
" fffi 0 \l5
Ep
? ff
.U@ ffp
3 0
 ffffi(

 fv Yff%fifip
 dp
* ! M!*a+F3
? ?pL
  *
 )G2A(bfi44fid
+ ffY1C
 d!,+c Y
fi ''* j 3M0'@ffPk
fiR
 vffF
3
? ff
.U()fi
  Nv U@   Iff%fiJfip
 ff
* ! M!*+Fp
? ?3T
  
 dP4f@ ff
 ffp
 ffS
 )J! (+0 DQ ff
fi  ffffiR

 vff*a30
?m[+c E4 
 *%4%fi Ififi3
 f()fiN
p
? ff
.1v 3M!  fiO
 7fi(
?p8v 
@ ffO
 g fi?F@ I4'fiC
fi0 *%! !*%+Fp
? p [?pY
fi 
23p
 >?0f@ E`_a
:
fi (i44*6I4Iv Z()
I'p
3 	?   !qhiT$p
? C@ ffp
 ?fi34fffi4&
fi M(i3 p j 
fia 
 fi4?PkI+F? j 
fiP0 >r  7!^kl
{p
? X@  _a
fi ()24*<IM
fi ff'fim
 ff
30 	fi(Ffi p	fiPfi@ fi*6+Fp
?  	  j fik
4a4 ff5?p 84[fiR
 ;K/fi44fi ZhB  7lE!QT$p
? @ I@ 8;+-fiC+Aff/['fi	@ ff'fi I3
E
? fi!

|,4?p5@ 3M 	'fi?3L@ ff ff7 v pIr 8hil- K
?Mfi1fi > fi?pf ff7 v p8fiR
 'fiF K  ()
 v *tfi>. ?pP@ ff3Qfi(5 ff  p	03MN@04L?pr  IhiElQv fiO
fi
 Pfi?p7+-d'fiY_om?p
fid!{?n
fi po/
 4fi *$?p
fi0v G fiE?PCv #?04@ ff
fi k
_ 
 ^a`y`L'Ep
3 fi

POdRQK^^^ qd'B<BE~59^'B^ d~BTo'$BO\iB'8[E\NmV\iB'E&d^iB\B^NV~^-B~Tl
~EE%B-B\6E\^^'E\^bQE1B6^B5~;EE^BB5J\~pB~[BE

O$

fi

T
"

D



"
y
y



y
T
y



"

D

y


M
T
y
y


M
y
"
y

mknpokmkq



T
y
T

M
"
y
y

M
y
T
y

M
"

D

y


T
y
y

0M
y
"
y

M
T
y
T

Tg0PvFT$?3'fiR(i33
4fiR()fi8fis<]^8r  z04!YTF?pfi\+F8
fi@ff'%fiZ'fiGff
 Y?pL
fi43

 0$
fi@ ff'%fi 	'fi7344fi $
4fi!

v'fi/3
ffm1mvffvpM!T$?pPfi4?PN?8@Mfi7
fi0CPff4'fiY_k(i4Ofi!
I fi?0?0O_kO4/
 Ifi 4>'p
3 ?fi6
 @ ff'fi4 5?pff )fiE6v ff?p6fi(?p5;+-fi
+Aff/X
! Cv fi??5M
fi ff
 fi,_k04k
 fiE9fi-k
_ 0,(i4 [fi pff-@ ff13@ ff-fi@ [@ ff''E4
ff
fi6
 ff'fi43Mfi V!tffc 804
32ff*6
 fi >
 53@ ffI+F?8?pA_0O?Mfi1I()fi6@ ff'fi41v 3Tfi*
+F?2
?>
fi 04'tfi(
?Mfi1fiv 3[ Mfi?3g ff7 v p$fiR
 'fid!gT$3
? 9 fi4?PV?0g@ A'fi3
3 0L03MjMfi

fi0v Chi
 Sk
_ U()4@ fil5@ 8fid 4Y
3Mfi3!F+c Ufi?pf+-fiff*0?pUffK@ ff
fiP
 
fffi2v pPPv ff v pIfiR
 'fi$+fp
? Yv 	(i
$?pLfiR
 'fifPff	R
 L( L'fi04!
 ()fi@$@ff@v3[?3$v
@ffOMfi4?P*+A_0'Off<vfi?PO()fi<'fiM':

`_a
4fi7()fi

E?!6T$?pff5fiE?PgMficMfi639?<vffvp[?gfi1

3@ff*8?p
04P'fi4%30fi !6T$p
? 	@ Lfi 3,'h Mfi544fi@ ff	(bfif ff v 3lE*03Avff?
 e
 *
? Kfi3Fv 
@ ff
 Vv fi?Pff!

} gieh.f]$^_hS9e ]vBT;H  gfi[F]{i[e^	$ekgi[%P ekgy8[J_]{[ek^

b

rMfi-40fffieC

*2/fi(afi3M9fi?63O5?0,r  f,@5@@ff@ffP30vp
70 Ffi(J?pF' 0fi ()3
3 0
fi VU1h  (M ^/l,
  . *+F?04
?O
 ff 9?9()fi5' ( *1./vpI
fi
^X ff'fi po1' . *5P?Mfi\W+ #v #Tg0 Rv!n"$fi\+F
fi ff'%fi X'fiW'ffC#
fi430

fi@ fffi 0W'fi834fi 
fi !T$?2>@ 0@ ff fi X4
 ff34 >'fim?pKfi /432
v3 Q@ 0@ ff fi Yfi(OrO 3M@ ffFP 
 >1!<ffc K02
34ff*MTg0 PvL4?
 ff13 5'fiP?p>r  
vGrO 3M@ P>(bfiQ?p4 0p
 Nfi Ls,!ffc mTta Cv*t'ffL@  14ffG1U?pff4F_0E'[v'ff*
C?pL834fi 5
fi 5@ L0@ /4ffC?pff-_0'A'ff!,rMfiL
 oM0 * j 
k
 ff 
fi7rn. ff7
4fi hirt:  fi4 ff
ElE*gc[. ffCh)cp: g ff
 ff4 \lE*O 0Rs#. ff	his:  E PElE!T$3
? 0 

fi 2'<fi
( ' ff9(bfi9?pT3o1-'*M! !*14,
fi@ ff'%fi 06'fi7?3F' fi (i3
3 
fi Vj
!  j yk8v 
?pf0 QO
 ff 9?9?3@ f4 Mfi8fi0 A' 0fi (bfi-?49':;
fi K04ff!6c
 pf43fi 
v m+F?04
?m?2Nfi1

3L4Q+F3
? R R
fi Z4c
 Mfi>44fid
+ ff(bfiY'!K-fi 43
 L o/P0 
3 fi(9?p0 P()fiPL()fiL
_ :;'3'fiP/! f

fi0v pY'fiY?3_0'hi3R
 L (bPfi'El
'v Tta Yv*g4(9sq2Lv 'CT$W
" [H  Uc~T$T5c~HQh j T$klQ Rr. ff8
fi ZrJ:;
fi2 ff
*tc
. ffIcp:B@ ff
 ff *< Zs. ffsV:B' h)+F?2
?R'fi?3P4>834fi 7
4fi  j 
klE*6?p
s+f449' 4fi X'fiRP"F|AA|<1c :[cHNh j "FklP'*5! !
* U1hiTN*,
El "L!<{?X?4Ia34
@ ff 4fi v
* V\ fiKH=I 4$4a ffO
 ffK$R
 3Mafi hi73M4fi al-fiR
 'fiF?0F
? 3ff
Z0 '#'fiX Mfi?p	 0MfiPX
?Mfi@ 4p
3 S(bfi	?ppo/C'! [R
 'fi
 V H=I  4C
R3Mafi fiR
 'fi-?-
E? pff-7
y ''fi8I
 po1-'Q@ ffMPR
 ff pLv P?09fi\+L!
rMfiI
 o/a _
* 34}ffv 3C?3P' 4fi 
fi 4fi Rfi p ffp
 mhiTN*^"[lN
 RR
 P

fi042p
? ff
1
? 3v p8fi pffi(J?p[yA'fiI 	"v ?pF_a',fid+fi(JTg0 Iv!<T$?494,R
 ff
3@ f?p[' fi 

fi 4fi I'fi/
4ff+F?ffp
 >hiTL*^"[lg4g?p$ 6fi(4/7304fi <
fi 6?6'E fi 7()fi

Off

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a



W %	 P
 [   Z\L]_^"\!PX   %
 	Ha`  
    PX b W c`#\ 
 X   ZY    
   edgf
h X  NPX  
 	   PX            _s
   \  P
 X `#\I` 
 z 	0l    ei   i_jkalnmomplqi_jr   
042	0l 
`#\H   X 	20    ti 
 ` z i j ^ l t\vd/\ Yw[
   `
`#\   X      Z
 ` !|  i j ^ l x{z  
   |\ i?[ x{z 
  a`#\
  `#\
  *	204l



k lnmomplqi j r  
i j u
     [ PX   i

 

 

	20 V	     

   yx{z 
 X  y|  i
l x{z  

 `#\H` d [ Z\}d~\ Yw[ P
a
j
k
_
j
r
i
[
[
[
i
[

!|\
x{z  fpfof \
| 
x{z 4    `







rO3MPvv;VM]_^a` u Qfi/3
$fi4?K!
T'fi	"L*!!*  
*%$7vSTt0vv!FT$?2$f2Lo/@ffffSvU-fi1fiffG8h)cp:B@ff
ffP
s:B'E PElQhi@>rO3M@>lE!
rMfi,
  (	K
fi fi6
 '2 
ff
fi3 *fiV'fi[`_
fiL?p-7304fiJfi/3
tr  &pffJ'fi
R$()fiO
 ff(bfi?p[v /430fi 5r  fQhi Qrgv 3M@ 7vdlE!6
{ Q
 	40v ffO
 ,?pf fiE?
VM]_^a` u L()fiT 3vpC?p8fi/3
Fr   30v pP?37	''30
3M@ >fi(<Tg0 	v8f?fidW+ Gv 
rO 3M@ Svv!K+c W?pfi130
>r  >*t  oM0 fi/3
>'	 W' 4fi R4~
 U1hBA"OTN*gMEl
DQD["R
 ff
3@ U1hB[*MEl[ D8w
* U1hi"L*MEl[ D8* }
 UhiTL*%Elf"()fiQfi Nr-*%cE*% Gs<*
@ ff'R
 ff
 ff4!mTFp
? C 4<fffi($?p	fi/3
8r   @ C(bfiEO
 ffRffvpm+Fp
? ?p
  
v 0123V'fi(-?pIfi/3
Q4N
 v 04J!Irfi
 o/P0 *V(,D8*VD8*V "  Pv 4
'ff[()fi[r-*0c*a 0Ss,*0@ ff'R
 ff
 ff4*0?pGDQD["+f44%R
 > Gv 48()fifr  c  s<
! F()
()fiPv pW?pUfi130
'ffK 'R
 ff
(b/v p+f?4
?@ mv 4B*,?pm fi?0 fi(LrO 3M@ vv
'R
 ff
`$
_ ffA?3~U7' 4fi 	(bfiL
  C0fi103
5'7 U7344fi $
4fi !
HffiP?L?3P fi? rgv 3M@ Kvv()fiPN?p0fi103
[r  zn()fiLff'v3K+c E4 
 
fiR
 vff!CTtfiUff8rO~:;L
" ff'%fi  0fiR
 ff>30v p\6T `_a
4fi *t
+ 3ffR'fiG(bfi?p

fi/3
-r    !<TtfiMfi8?4-4048 ff34@ ff9
fi 04p
  p  'fiIR
 f?3Iyh U&vdl'5fi!
T$p
? G fi?0 v rO 3M@ XvvS2fi/`k
_ ff#1#
? p p"U='fi U}v  +fp
? @ !cp	4C4'fi
4%fi$'fiMfi>?0[v^a`y`<43fi 8hiv 
430v p
  (1fiNHOI M lEv
* qVJ]_^@` u >83'fR
o ff
3ff
'fiU()fi?pPfi/3
7    ?
( 6T  _a
fi W2L'fiYR
 Mfi 3!ffc 
  (1fiH=I,J *<4 @ 3L?p
v 3 Pfi 8r   X
 U#4v!rMfiP
  (	ff
fi 
* U# vP4'fiM!Cffc fi?p>+-fiff*J()fiI
  (	ff
fi ?p
834fi 504 *1fi 
 N()fiO
 ff*2
 3 F3M/4p
 ff 	?p@ ()fi@ L5
fi34	R
 Q
fi 43
 @ ffK4. 
Cv p 8fi Q02 !$ffc U%fi?Sfi(<?pff 
 ff*
( 6T _a
fi G4ffi p7?3>fi/3
[4f. 
fi(O?pLv 3 Q04 Kr   K?pL0fiR
 ;	r  >!
Q4 ?<?pFfi/3
6r  ?06R
 I(bfi
 ffPk
( 3ffp
 ff*?3I?p5k
_ Vhi83)lfi ,r  

 RR
  $
_ ff!
{ P_a'L
fi 2p
 >K 20 Ifi/p
 ff<
3
? ff
./v pU fiE?K*J
2 ffG
 VM]_^a`y*
4fi ff&'R
 ff
`_a
4()fi ()/v pG+c E4 
 	fiR
 ff8fi(f?pK(bfiE
 ]  )t!ZT$p
? K fiE?K*
?Mfi\W
+  PrO 3M@ >vff1*M
fi 4'<fi(V>p
 ?/:_0E',ff
E?fi(tR
 v 3v 3Q Off
?	v 44M'X
! 


 ff40 F'fi ^ %?-4606fi(>' 4fi 
fi fi V*+Fp
? @ ^aq)J*/fi4ff,?p$fi
 ;!
h)
{ 7'fi@ L?pL Ffi(O2'fiPR
 ^aq)U()fi$04K

 ff! l

OJL

fi

mknpokmkq

H204l 
 	  `
`#\H   X   ti~ !# %
       i 2
  `#\
`#\H   XJ     tiJ !# %
 `         i Z2% PX   `#  i    
  `#\
  *	0l%2
H204l `#  i 
       i 
`#\H   X8  x{z  W [ x{zV [0
 ` |  i
l x z   8PX       l  
  `#\
`#\H   X8  x z  W  e  |  i8l x z 
 ` !  
          ! Z*% PX
  `#\
  *	0l%2





`

 	I\      `
 %
 6
  
 `#  4     `

rO3M@Pvff1qVJ]_^@`	QE`_a
fiYvfi?K!
H o15+L
fi2p5Kfi? ()fi5()1p.^@UWP0fiR;P+f?MficpfiY2-o/@ff0
T
N	k3j 
?Or  7*v
430vpPrO'~:;"Wff'%fi8fiRffff!QTF?pI@ffpNPffS+F4?S'fiK@/+?p
? ?M:;  ffp
 ff
4fi[fi(/?46TWPfi1pff
E?pff
./vpffi?0@ffff>v  30ff
fi9>1!>AR()fi@

fi v30v pM!Orgv 3M@ Qffv > ff6[04
- 4fi 7fi(a?4O fi4?(bfi -fi3
fi3MR
 4j
 6!Ah~vffwwl
Pffi}ffP $O,!$h~vffww ClE! (( 
{ [
4M?46v fi? qVJ]_^@`!RYR
 ff
3 F<26'fi03M'fi:
?pfi@ 2
L `_
fi !9W
" ff
4V?0Fv\6Tfi13
 ffV
p
? ff
.1 pM*0?p>fiR
 ;K4$@ @ ff ffUf 
r  7*1 	'./v p7+Fp
? ?p$q 2X
 ff13 ,'fi'./v p>+fp
? ?p & hB,
l  & hi8l<()fiAfiR
 p
!YT$?04L4
 ff34 L'fi & hB<
l u & hi7l8 1*O+F?2
?W2L fi?0P4
4Gff'ffZ./v pS?p
'fi50fi103
,fi(t?pQ04
 	r   	?pQr  =
fi@ ff'%fi v 3>'fi  !6c( & hB   7lA 7?p
l  & hi7lE*/! !*Mq =  `_
fi 30

 ff ?1fi?p+F4@ *M
   `_a
4fi P(i44!
& hB<Z
T$p
? Q fi4?fi(grO 3M@ Iffv >83O
 ff-?A?pYpfi 	fi(t?p[fiR
 ;Gh  7l,4
 o1 ff ff	
P*
 3j 
E?V3M'fiP'fiS K?p7r  =R
 ffv p ffE
p
? ffS4F   P!
[ fi?0 qVJ]_^@`!R<*Vv Grgv 3M@ 	ff
v >1*J
34S
p
? ff
.1N+fp
? ?p>  ()fiL SfiR
 pU!
TtfiP
p
? ff
.C(O
 *M
+ N
 Yp
 P pf+fp
? ?p & hB   7
l 
  1!,TF?4-4-'p
3 N(V?p Q25'fiO
 
j 0km'Kv  m hB   7lN ff
?0v C()fi Xv 2,'Y Z@ ff
E?0 	()fiff(*<! !*
09fi(J C

 ff0v f
/
 [ 0P?p@ ()fi@ [12ff /k
_ ff4>fi()!,T$p
? f fi?fi(VrO 3M@ 8ffv >
R()fiP6?2<
p
? ff
.3v pL
 3ff'ffp
 0?/:_0'9ff
?fi P?3Ffi/3
<r  =   !6T$p
? $_0'
 ?M:_0'F@ff
?GR
p
 v ffv44V'ffQ S12F2J

 ff40 7ff!F{p
? p NC'
 m hB   7l-254
fi\ @ ff*0$25
4 ffY j  ffV*^k YO
 3ff'ffU ffE
?YR
 v 09'fiCfi1fi.()fif

/
 7?0f@3 $'fiC?pI ff!fcp(6?p 84[C
1
 *?4[4a4 ffA?p m hB   7lLhi@ ffalf'

 mR
 >/4ffGv /
_ ff	fi()*V S?p@ ()fi@ I?p4 p3fi 4
 Mfi pff0;hiB! !*?3@ 4Q'fiO
 

fi S ff3
3 
 Lv K?pNa4 	?FM
fi ffL
 Mfif4'()?3NfiR
 ;l- K E`_a
fi K(i44ff!
%^6^EBB\n^O~L^LB;m%~^Tl2ffNmV7d^LidiB~=Ag~$MNrOrff;`U5ff\To'B^Bm%EO$E\<B
Bt\^^'FE^BB&`JB~^^)\EW
 m%E% EJ1 w5ff`VBO'
 mVBg^V^J\^;^OB-E
 BVB
 o\BiJBT o'B^QB'a%^V5ff\T o'B^Q$~%Bg~iB'F'E^ oBiJ^[\B'\dB<d`~

Offx

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a

H204l 
 	  `
`#\H   X   ti~ !c,W 
       i 2
  `#\
`#\H   XJ     tiJ !,W 
 
`         i Z2% PX   `#  i 
     `
  `#\
  *	0l%2
H204l `#  i 
       i 
 `,iBM !B,W PX  
  i 
`#\   X   tiJB !B,W %
       i *
  a`#\
 `#  i 
   `
`#\H   X 4\ of  f [         a`i
 `         Z*% PX   `#  4     `
  `#\
  *	0l%2
H204l  a`#  i  PX       2 XJ
       i %
`#\H   X 4\ of  f [         a`i
   l    	I2\ 
 ` ! Z   PX   	 P eP   ffi 
	 
    `       =  Z*% PX  e a`#       `
   `
  `#\
  *	0l%2

rO3M@Pvff>1;VM]1^@`gR_a
fiYfiE?K!

Off

fi

mknpokmkq



W %
 	 PX   ZY    [     w. _`0  
[ t\-\ Y f
 		  \Ru_ ruP `0   |\ i z [ x nPT   *	  z #` \H  ti z  *
      Z    yx TPT f
\L]^"\PX   		`    PX b W `#\   #
 d@f
h X  NPX    \  PX   _s



042	0l 
`#\H   X 	20    ti   i juk lnmompmoli z lnmompmoli jr  `#\I``lD
 ` |  i j ^ l x nPT   
 `#\H` d [ Z\}d~\ Yw[ PX  "|
   |\ i8[ x TPP  !|\ i j k [ x TPT  [ fofpf [ #z! [ fofpf [ |\ i j r [ x
  `#\
  *	204l



rO3M@PvMZ



 Zi z 
 i
l x TPT  
TPT      `



UW[ u Qfi/3
5fi4?K!

VM]_^a`y< 0B VM]_^a`R@9fi33>0>
fi0v[h)()fi^@UW[fiR;Q+F?fiXpfi84#o1ff~:
0vQ[P*3j 
E?Jr  NlE*0U?pC_kK4E`_a
fifi!-(bfi@Iff4%fivpfiU?4*_0'
MfiN?5?3N
j `_a
4fi fikI?50`5
fi3Mfi4fiK(bfiqVJ]_^@`	QqVJ]_^@`R<!
rMfi
 VM]1^@`y fi>2L R

 ff40 0R'fi hi! !*t Z'fi ^)Z+Fp
? @ ?p0fiR
 ;m4
]  )%lE!$rMfi
 VM]1^@` g F4f U

 ff0 70U'8?Q4$a$fi(<C
1
 !$TFp
? L@ ff'fi VM]_^a` 
4A'fi3
3 0C29?0$Zbafi 5)
 fiE9fi 0P?Mfi L'fi
 ^)J!6cp$2-
fiP0 N Ck
_ 094
 fi
Rff
3 [9M
fi ff
 o/?03' [ ff
?	 ff'v37fi(V4

 ff40 f'fi
! VM]1^@`gm4-4'fi7fi3
3 
 K
fia *()fiF fi fi3$@ ff'fi !9
 ff
30 BVM]_^a`yN 
 VM]_^a`Rmk
_ K4R
 fi*/?pK
 
R-30 ff>+f?ff4?p6O
 ?Mfi/7fi$
( fiO@ ff'fi43fi ChiB! !*
?Mfi1fi $ Mfi?pOfiR
 'fi6fig_o>?3-r  NlE!
M@^]v	$ekgi[%| e$g \V  fi KH=I

bFE^Zg

y [J_]{[ek^

]$^_h

[4fi(6?p8fi?FvK?3>@/fi3F3M0ff
4fiU
S7''@ffP4pff S?f4f45.Mfi\+W
?N	ffpfiR'fihivS?4[
*V\ fiJH=I lf?[RG004ff!QrMfiL2044
4;*a2tfi(,fi3M
 fi4?P83OV\ fiKH=I 4802ffR'fiGvpC'fi hi7304fiI
4fialE!rMfiOo/P0*
+I3O
 8?N
( U1h J  ^ fi  fi, l$ 3 p*%?pGV\ fiKH=I h Uh M  ^ fi  fi, l'l$ 3   +fp
? @  3 ,  3   @ 
'ffCh)fi7y/*t20/v pfM\
fi po/L'\lE*g 0X
 ^ fi  fi ff 4NY834fi L
fi !   
 I+ 30 ?p
0325@ @ ff fi *0?4A' 4ff$'fi
E? pv 3fi pL0 c'!
rO 3M >v7?fid+F9 v 
 ffO
  fi fiz
( qVJ]_^@` a *1
4 ffP
 aUW[ a *1+F?4
E?P462fi@ ff()fi
@:B()fiPv 3?p7fi/3
Fr   ()B
 V\ fiJH=I ?fR
 Sa4 ff!$T$3
? > fi?0 fi(,rO 3@ v	4
()fi,ffc 4 
 FfiR
 ff ?(bfi
 6T# _a
fi 
E? pUK'fiEURvFv ?p[ fi? 3O
 
?  UW[ u N45024
0 Qv K443fi A+F3
? K.1 p?p
  2A?pyh Uvdl'Ffi!-[?Mfi33 
fi/3
5+f?	?pLfi
 ;	r >*?3L4fi4fi U(bfifp
  fffi0v 3?4$ fiE?z+A
?pN834fi FR 
K
fi !<W
" ff
4?5vC?4-34fi z
*  4
 [ ff7 v p84A04 ff'fi

 mv 41403fi 7r >*?pIfi/3
Q730'fRI:B()fiO

 ffG'fiK ()\
 fi0gfi
 ff!QT$p
? 
+A'ffU
fi'Ffi(Ofiv pI?4$PfiffY?p>p
  fffiaO
 Ffi(t?2$ fi?0K!
[ fi?0
 UW[ u U3O
 ff??3Sfi130
P+-()fiO
 fffi v 2nh)R
 ()fi@ S ff7 v pl
3 pVM]_^a` u 
!  UW[ u C
04} ff>fi ?p
. Mfid+f ffp
 Cfi(5+F?Khiv /430tfiI834fi El
'qh MBl 0n832fi K
fi  yh ^ fi  fi, l' fi 'fiX p+ po/K'K'R
 ff
`$
_ ff

fiR
 'fi2
 V\ fiKHOI !OT$?46 fiE?30O
 ffO?<?p50@ ff ff  pFfi130
Or n46''fi ff!6T$p
? 
?pfi 4Sfi130
Qr   'ff>+F?Mfi po/L'fpffQ'fiYR
 Pfi/`$
_ ffm@ ?fi 'ff>?

=K

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a

042	0l 
 !# 
#` \H   X 	20    ti   i_jukulnmompmoli z lnmompmoli_jr  `#\I``lD  Zi z 
 `       i  PX    ! #   #e i
      `
 ` |  i j ^ l x nPT   
 `#\H` d [ Z\}d~\ Yw[ PX  "|  i
l x TPT  
T

P



P


   |\ i8[ x
 ! |\ i j ku[ x TPT  [ fofpf [ #z  [ fpfof [ |\ i j r@[ x TPP 4     `
  `#\
  *	204l



rO3M@Pvffx1

UW[ u  fi 



fi130
tfi4?KJ[4fi8fi(zaUW[



u 9?p+&v04'ffff!

v 
23p8MJ'fiPfi	
fi^ fi  fi, !<T$?3fO?Mfi/(bfiA@_a
fiC?A493Off'fi
()fi4fid+  UW[ u N4A'fiV@E`_a
fi*a!!*VM]1^@`yQfiVM]_^a`R<!
H o1*9
fi 4p
T
 8 Mfi?pI08fi(5fi/3
8Z@  `_a
4fi ZfiE?P7?I4o/Rff
ff
'fiR
 *Mfi\ 4*afi444
  Sfi@ e
v !AT$p
? fit45'fiC''@ffP4 pN@  `_
fi Y()
V\ fiKH=I !ST$?2N@ ff13@ ff7U( + 40 P
E? pffI'fiG?pv fi?Pff!UT$p? Pfi4fi W()fi7?3ff 

?0 pff92<?9+Fp
? Pfi/p
 ff
Ep
? ff
./v p8MfidT
+ ''@fffiR
 ffa
* V fiJH=I ?<fi 4 j fidW
+ '@ff
Rff
*^kI! !*6fi48Rff
,?3$


 ff024;8fi( 4
 ff, 0P'fiP,4@ ffP1.
 V\ fiJH=I fi<?fi 
?$+-fi34	R
 N/4ff	1C `_a
fi Z  ;ff 8?Mfi > ffK
 V\ fiKH=I !
-fi 04p
 [?pP
?0 pff!8
{ P'N1U0324v pP	 Nfi(9?pAff4 Gfi/3
Q'ffe
 Y
h 
M
 l7?@

K

)
(

fi



O
ff

X

)
(



fi


?
p
Y

'




J[?+APRff
ffq1Z ff7




v
p

M


!
T$p
? 
 
( 
_09+Aff?A+-N
 	?MfiC@  E`_a
fi 	4-13v 3>?pff N'ff5A?3p+v 040ff
()fi>@  _a
fi !Pffc R(i
*g+ 3ffRfi m ff ff
>?Mfi ?0L+- /4ff3v p	?pPfiE v 
 `_
fi hi! !*9@ K

 ff0 C(bfi?3CfiE v -v 2,'ffElE!Z+c fi?p+-fiff*<3M%fi 

()fiFfi qY'v
* V\ fiKHOI fi/`$
_ ffL
 U1h J  ^ fi  fi  lE!9T$p
? 	
+ N@ ffv 24} [?pL $fi(6v 44affF'fiPR
 
Y 0WZ4<fi130
>'ff8(bfiEO
 ffR()fi^
 M5?8+@ P. ff j /4ff0kY3M p	@ /fi3
 `_
fi !6T$?46
 R

 $Mfi p$1fi/()1v 3[?3$fi/3
< fiE?fi(VrO 3M Lv7-?MfidT
+ Pv 

rO 3M@ Pvffx1!9T$p
? L fi4?zfi(Orgv 3M@ Pvffx4$'fiP(bfiEz?pNfi/3
$r ()fi$ ()1 pI+c E4 
 
fiR
 vff!TJfim()fi?p	fi/3
>()fiO
 6T `_a
4fi *,3Ma'3MahB   7l>(bfi|
 hB<l8 
yh UvdlF(bfi UWv GrO 3M@ Cvffx1!f
{ 
4t?4[v 
 ffO
 tfi/3
[ fi?0  UW[ u  fi  *a+fp
? @ 
 Mfiff$?p>()
$?0$+-7@ c'vpfp+v 2ff!
j HfckP3
T$3
? P ff
fi R+Aff'fiG''@ffP2v pI@  `_
fi W4Lmfi G
fi 43
 v pKY'E fi mfi Z
:
fi  ^ fi  fi  *Q?pR
fi =+F?Mfi UZ4p
3 m+AUfi/`$
_ ffn1nv ff v pM*f(bfi ?pff ap+v 4
'ff!TFp
? @ ff()ff*$v 
@ ff
 F@  `_a
4fi #fi/
 ff
 o/
#2. Y'fiIh)@ \lp `_a
4fi !
{?W?3ff C
E? pff
* qVJ]_^@`  R
 ff
fiO
 ff
  UW[  fi  *<?MfidW
+ Zv WrO 3@ Sffv C1!msJ. +f4 *g+F?R?3ff 

?0 pffZ
 VM]1^@`gmR
 ff
fiO
 ff
  UW[R fi *MA?Mfi\W
+ 	v Crgv 3M@ Ivd!,rO 3M@ 8vd8?Mfid+f9fi 0P
? 3ff
'fi8fi/
 ff3M@ ff<()9 P 4(b
 ?0M()946?pQO
 [9 PrO 3M@ 8ffv >1!9Y
 p$
_ ''@ff4v v 3Lp
 ff
'fiB UW[$ fi *03MjMfi UW[g fi *4t?OfiL?3p+&v 04'ff,?ff  j /4ffkf@ ffv 044} ff
'fiKy/!NT$?4f
 m
 8Mfi p8(bfiQffc 4 0
 7fi
 ff$R
 ff
3 I?pS@ fiQ
fi 0
  pffG+F?S?p
+
(
.
fip
 Ffi(g'fiP[v K''Ev p!

ff%d\EynPn$E\-N1mZ'\E7aP;ub-_m-ff^^B~PEP\B~ff^\9pBo'B^EO|u)ffEBE\~
\BE)P` Bfi;BAB6BEM
 a;B~7E^aB5OB6~E8-d^B'7 Bp
 Zm%E8B'ff8
~~LB~iB'L`,B\-dBbZ
 gLBAB<E mV~>B~iB^  Bi} lJ~\A\BEB^~6B-;\<

=

fi

mknpokmkq

042N  `
`#\   X Y  >     ti~M !# %
       i 2
  a`#\
`#\   X Y  >     ti~M !# %
 `         i Z2 PX   a`#  i 4     `
  a`#\
  *	204l
042a`#  i 
       i %
[ X 	2   |  i
l x nPT  [ PX  
 `iJ !#     
 ` !x TPT   PX       l    	I\      `
 `         Z2 PX   `# ! 4     `
  
`#\H   X8  x z  W
  [ x z V [0
 ` |  i
l x z   8PX       l    	I\      `
  `#\
`#\H   X8  x{z  W
   e  |  i8l x{z   6%


 ` !  




     ! Z*% PX   `#  4     `

  `#\
   `
  *	204l







rgv3M@PvffC1aUW[$

fi

Q`_a
fiUfi?Y!

aUW[O fi $2,'fi33 0P()fi,ffc 4 
 ffiR ffff* 
 aUW[g fi $49'fi3
3 P(bfiAPfid+W'@ff
fi4@ff
fiffIfiR;Z+F?Mfi\3fiq4o/@ff0	*3j 
E?$r  7*<v
430vpGrg4'~:
" fffi 0  ffc 4 
 O
L
! [3v pY z0fiQ'fiU041v 3V\ fiKH=I 'fiY()fi  *t(<?3ff
v 

 ffO
 %  `_a
fi Kv fi?P5
fi 0
43p
 Q?f   *M?3K'fi@  _a
fi 	+-fi34
4'fiK
fi 
23p
 I?7   !IL
" ff
4O?N'fi<@  `_a
4fi 4N'fi3
3 V!>T$p
? @ ()fi@ *V?3PO
 
4['p
3 8()fiN?pff v 
@ ff
 O fi4?P!Lr3?pfi *?3ff Pv 
@ ff
 g@  _a
fi m`:
fi?0PP+F24-k
_ &4Ffi(N?pp+ 14fi4fi fi(N?3UfiR
 p&v 'fi130
 ff& V\ fiKHOI !T$p
? 
@ff'fi Z?pC fi?>@ C'fi3
3 Z k
_ 0R4X
 3C
+ fiEh)()fi8Mfi\W
+ ''fffiI@ ff
4fi  ff
fiR
 vffEl<29?05?p@ N@ Nfi ;+-fi+-d15?0$

 ff024;
 	
 Nfi/`k
_ ff1 fi(tfi3M
 ff
 0v pLfiR
 'fiff*1v 0
43v 3V\ fiJH=I 6fi/
4Ifi 4fi04!6L
" ff
20?Afi/
0
E? pN9?p


 ff4044pYfi(9'fi ^ fi  fi  fiN?pPUh M  ^ fi  fi ff lE*J m
 fi0O
E? pPQ?3I

 ff':
a44pUfi($'ffIfiI'fiP8?>+-fi34WR
 P12ff  ;ff Uh M  ^ fi  fi, lE!Yffc  pff4?p7
 hifi/

fiY
 fi0bl$+F42?pI ff v 3fiR
 'fiNfi/()	

 ff042;Kfi(,'fiQfiQffQ14ffUR
 ()fi@ *
03MfiL(bff
* ^ fi  fi, !CQ3M7 fi?Q@  () oM?3 ff&hi! !*J?pm@  E(bG>83
?Z
'fi-@  `_
fi W+-fi34l[(bfiI49'fiPI Zff7/44ffW8fi8()|
 ^ fi  fi  !  v 
 P?3ff 
v 

 ffO
 g fi4?P$R
 ()fi @  `_
fi oM
Y?38
 7+AffGf?3ffF'fiO fi 
BE5O^OB~^~EE'907,  m%E6\Bd^E>d^B'^\~A<5^O\E6-~~>ff^^B~> BE  B\
\\B^FEa P_M~\ F\f<,mKiB^;mV^Bf<1mYBZE;\p%B5^ff^BgB  Bi}lJ~E
\BE)Z%\B`B#
 >\~'dfBP>Bd^B'	`  Bi} lt~E7\BpB~Od[\Ef`Equ)ffEBE\~
\BEB^~~

=J

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a

H204l 
 	  `
`#\H   X   ti~ !c,W 
       i 2
  `#\
`#\H   X Y   	     tiJ c,W
 `         i Z2% PX   `#  i   
  `#\
  *	0l%2
H204l `#  i 
       i 
 `,iBM !B,W PX  
  i 
`#\   X   tiJB !B,W %
       i *
  a`#\
 `#  i 
   `
               
 `,iJ ,W
X 	2   |  i
l x TPT  [ PX   `# ! 
  
`#\   X 44\ of  f [  4     
 `         Z2 PX   a`#
  a`#\
   `
  *	0l%2



rO3@PvdD,fi1
ff03M@ffA()	YM(iAfi(O?p9

UW[ R

%
 `

! Z2 [

 `i%
 
     `

fi



`_a
fiUfi?Y!

MfiU()>?3P0Lfi(A?pr  ?7+-7fi/`_$ffm1GvffvpM*J?pm+F44t_k2p+ fi
v'fi/3
 ff	1C ff7 v pM!
aUW[O fi Y2P
fi0 S(bfic+4
KfiffRff
3U4|bafifi30vpm?pGO
O?Mfi/#P
 VM]1^@`  *- #
 ff
30 Yffc 4 
 K0fiR
 ff@ G@ ff
fi 0 ffI #@ U?p@ ()fi@ 
4R
 /fi3P'fi?3Gfi/
fi nfi(>'fiPK X'' pM!Y
 n?pGfi?pK? VR
*  UW[R fi G4f
 Mfi

fi0v S()fiC2FMfi\W
+ ''@ff fiR
 vff!rMfi oM0 *$C4
 fiC
fia U()fi	fiR
 ff
?5
3
? ff
.C(bfiA?p6'YB ff]6fi/

3M 
 Qfi(J80' 	v C8'v pM
* ! M!*rO'~:;W
" ff'%fi  F0fiR
 ff!
 ff
3 | UW[R fi >M

fi ffT
 fi[43
 ()	+Fp
? ?p[?pp+v 4J'ffN@ 8
 ()fi@ 7fiQ()Q?p
_0fi1

3M@ 
 *?p@ <4
 fi5+-d['fif
. fid+W(?p6_0E'fi1

3@ 
 <4V
 ff p5
p
? ff
. ff7()t ff7 v pM!
H  ?pff ffff*?04F4
.Yfi(,
fia pfff(bfiQrO'~:;W
T
" ff'%fi  7fiR
 ff$
304K3M 0Ffi3Mf'fi
 LI K3 (i3%'*FR
+ >+F44%4
fid Fv   3M0 ff
fi C1!x1!
b



M@^]v	$ekgi[%| e$g V H=I,

^Zg

]$^_h



1fiH=I M@(	ff
fi
(

{3o1L0@ffLfi3MN_k06;+-fiSv0
@ffO6@`_
fiWfiE?P*J+F?4
E?R@a44
0
fiv q
  (1fiH=I M  q
  (	ff
fi *-+Fp
? X?p Y4fi pYr   'fiR@  4(b!qTFp
? ff Y@ K%fid
+ (i3
 fi4?PAv CPAfi(t?pff5
0042;P'fi ff3
 Q?pL
fi0v o/pPfi(g@  `_a
4fi !<Ffi\+  d*
?pff$fi3
3 3
 pff9 ff4 ffAfi 	?pL3fi C?F?pL ff  p7fiR
 'fid]^)
 ff
ffiK

 ffa44p

=K

fi

mknpokmkq

H204l  X  4      4  	2	  
 `i k .
        l        8[ PX     %    l    4% f 
  
 ` !/     PX      J 	  l    4 f 
  N  %     PX	     4Na` Pr f       `
   `
  *	0l%2

rO3@PvffQ12

UW[ H=I, Q@`_a
4fiYfi?0K!

4fi1
2}ff*5!!*5?C4V HOI  +F?n (1fiNH=I,M fiY (	ff
fi 03MMfiK'2 
ff
fi h)+f?p@
V H=I, P ?NRff
fiO
 PV fi 4\lEO
! R4%fiQMfifi(-?pffPvfi?PN2f?0L?pp
@ff13@ (bfiEPv pGS0fi103
7r  7:
* MfiI n   *,@  ff>fi(f+Fp
? ?pI?pCfiR
 pW4
;1R
 LL
" ff'%fi @ !OTFp
? N fi??
 v e
 
R
 ffv p|=VM]<K44fi@ ffC'fiI'R
 ff
`_a
[fiR
 p
;1R
 ^@Uo5P'fi'R
 ff
`_a
N ff 0v pIfiR
 'fiff!9TFp
? Nfi @ ff
 7v K3
  fffi0v p?3ff > fi4?P5+A
PoM4P
 e
v 
S m?3@ (bfi P?pS
`_a
 P
fia pffL 0z
 <\fiN?pP042;Y'fiYk
_ m4
fiE!
T$3
? ff ;+-fiqv 
 ffO
 Nv fi?PK 4fi@ ff()fiK@  `_
fi ()YfiR
 'fi
 V H=I, !
[3O
 >?f0fiR
 ;K ?Mfi45()fiQ#4fi$'fiCv ff v pM*a! !*% =!$HFfi\++ Ip4} 7?p
' fi U
fi 0fi   h  (K  * l5 K'fi	(bfi  14
 V H=I  h  h  (ff  * l'lQ 9tM*%+F3
? @ E
y/!<
{ 7+- $'fiP E(bC?[   P!
 3Yfi 0-3
Y
 k
_ fi X4
 pffp
 ff&R
 ()fi@ K@ ff vpmfi3M fi?!&
{ Y@ /fi3
 k
p
_ 3ff+F?KYOff 	()fiU&
:;'Z(bfiE732
 )'fiq
 m'3
3 mS&
:;'*L03M	'fi#204()
?pL fiE?P-
+ L4'fip
 k
_ pQ+F?0$5Off A()fi$
:;'>()fi8347'fiPR
 ['Ep
3 Qfi(g8' fi 

fi 4fi 
! 
:;'f(bfiE732 )46p
 k
_ 3ffI'fiLR
 -'Ep
3 5fi(Q' 4fi 
fi fi |a*! !* j 	 
 )J*^k
(< Gfi 4Y;
( >)&h)+F?04
?m
 GR
 84P0 ffO
 ffY1Yff'v3C+fp
? ?p[(bfiY
  G'fi ^ja*
^q)J! l
s 	30R
#
 v +F?n?pm fi4?  UW[ H=I,l Xh)+F?4
E?
fi 4'Cfi(Lp+9fi& 4P0 UffEl
4fi ff()fi
 V H=I   C+c E4 
 [fiR
 ff*/?Mfi\W
+ 	v Crgv 3M@ Iffv Q1!,L
" ff
4?  h  (J  * l6
 
 

 V HOI  h  h  (J  * l'l-j
 tM
!  UW[ H=I,l *0+f?4
?Yff' j Y   )J*^kfi1
2} ff5@  _a
fi Y'fi	
@ff''E4
ff%fifi fi(A?pr  >!9hirfi
 e
 0
:
* Z   )Z4N4a ffO
 ffR>KffL(bfi*
 G)
?pf?   )SR
 ff
3 )G4F;104
4f
 o/
 ff
ffS'fiR
 Lfi 73

 
$?   )J! 
l f3O
 
?p	ffc 4 
 	fi
 ;W4I ]  )q #z !WTFp
?   'v p v  & hB<l74'$
_ ff
cff4 
 7fiR
 pY*fi()fiT
 ff
E@
? 6*  )m4f'p
3 7fi
(  U'fiv 6!fTF?4$4P04 ff
 m   )J!
T$?26'ffO
 -4,0 fffi fi3M934fi ?` ( 4,

 ff0v $(bfi  4/'!<cR
( Mfi*
@ _a
fi P4X
 MfiXpffp
 ffV!OTFp
? )p44}fffi +F42p
 Mfi614fi4F!6T$p
?  (bfi@ *?pF fiE?
Rv 1Xff'vpZ+Fp
? ?p
  ( +-12ff&fi q@ /fi3 _a
fi !cY
( fi*-?3Ufi3M'034
fi ffW
 MfiF$?3>

 ffa44pfi
(  ( !l
j 3

 ff!^khiHffi>?V HOI  Mz
aUW[ HOI   4A'fi33 K K
fiP0 Q(bfiF+c E4 
 QfiR ffff!<Y p4}ff4fi 	fi(   h  (   * l
4t044
4fi Nfiv
( V H=I, h  h  (J  * l'lg ' $'fi[(bfi  !gTF?4fiR
 'fi
 V H=I   ff  fft+c E4 
 
fiR
 pY4(O 0Yfi Y4(<   *0+f?4
?S4f'p
3 74(O 0Yfi Y4
( Y   )J!fT$p
? > ff'fi U()fi[?4
4A?F+-L
. Mfi\+X4'$
_ ff$()fizfi3MFfi v  _a
fi *0 0	?p@ ()fi@   )S2A'p
3 N()fiF4
'fiPIv R2,'' pLv  & hB<lE!YTFp
? Pfi m%fi40 Op+'fiP7  & hB  lN03Mfi7v & hB<l>@ 
v  M!Nc`
( S   )t*?3  )2F'3
3 7()fiL4g'fiPNv  & hB  lE*+F?2
?G404v ff$?Y G''v pCv 

=	

fi[

t}-m{t/s$q

H204l  X  4 4	  
  PX  
 ` !/    J  
  N  %    
  
 ` !/     PX   
  N  %    
   `
  *	0l%2



os0pt{a

l qM 
 a

 l		  

 `

rgv3M@Pvffw12aUW[

  P
 X     %    l 
    4% f 
PX	      4Na` Pr       `
   l    4 f 
  J
  	
 Na` Pr       `
PX	      4

HOI fi

`_a
fiUfi?Y!

Bh   lL4~_kff>!	cffRfi?p8+9fiE*O   z!YT$?p@()fi@* UW[ H=I,l 2>'fi330!{C4'fiS.Mfi\+
?I4I
fi0	Rff
3	("A^a*2^ h*2^ 
 )J*<?34883'8R?pK
	?   !
cffG
fi 0
43fi V'
* aUW[ H=I  *+F?2
?m
fi 4ffi(-?pIff' j    )J*^kK4Q'fi33m
fiP0!7rfi
PoM4P
 e
v 
*Jfi3ML20 ffO
 fi mfi
(  UW[ H=I,l I?4L(b>?p_0fiff*J?fi3p?
4$40 Q'fiCfi10(b	5'fik
_ Y4
 fi>hi 0Y?4FMz
fi ffT
 Mfif `_a
 Cff
[?pffa4

4O
 A
fi0v o/p>@ ff3tfi(  3Ma ff
fi C1!x1z
* Mfi6M
fi ffO6ff
6?pA+-fi'~:;
 52O
 5
fi0 oM;lE!
 UW[ H=I, N4$v 
 ffO
 V
 ff
30 >F2$fi/
4} ff	'fi @ 3'$
Ep
? ff
./v p+Fp
? ?pF?pLfiR
 pC?fi4
fi(J?3Y3+F3
 ffC'fi$v M*/?p5? K2'fi$v  & hB  lE!<rOv 2*?04A fi? fi 4
pfff'fiC
 I
o ff
3ffm(bfi
 V H=I  *aa3MTMfi[()fiq
 V ffI  " H=I  fi
 V I,
I  I " H=I, *
 ff
30 EV H=I, 4$?37fi 4
4fi P?-
 
 UV 3 'fiP914
 p4}ff4fi !6L
" ff
40?V ffI  " H=I   
 V\ I1
I, I " HOI  @ 
 GsJ5()fi$ffc 4 0
 NfiR
 ff!
[P
  oM0v Kfiq
( aUW[ HOI   *-3%fi K/*-J*9
*5*9 0 U@ Y'fiPff*A 0&?pK' fi 

fi 4fi XR
 ;+  _
T 6TF|FvG   j
T 6T$|-
 ff134C/!=
s hi/*$V*5V*5*$V*!!! lE*$+Fp
? @ S?p

ff444048v 2
ffv /k
_ 4@ R
 4fi Zfi(Q*-R
 	W''v pv  & hB<l8?v
433
 ff j
T 6T$|fvY 
 j
T 6T$|9P$?pN_a'5;+-fi 4
 ffFv U$

 v3P3
3 !9T$p
? NfiR
 pC2$  ]  
! f3O
 
?pI(i
[?0[?04f'v p	4~k
_ ff  8+-Q0fid ffv S?pIfi  V _a
fi !  3%fi V H=I,
p2
} ff  h  j
T 6T$|Fv*  j
T 6T$|9lF()fi 'fiRhi
 q
dl7hi! !*Q[
 3+44fid+A0 I
fi 

7(bfi  j
T 6T$|FvdlE*%+F?04
?Gf?pI''v pmhi
*%V*aJ**%*!!! lY'fi & hB  lE!QT$p
? S?pQ? Sff'
+Fp
? ?3F4fi(6?pff ffO
 $fi(  /*aV*0
*a
 8@ P  *a+ 7@ ff4Cfi 
 pffY'fiff'F+fp
? ?p

   *R
|
 ff
3 L
L4A?3Lfi f
 p+FCp
 ffY'fiK!
T$3
? Tpo/9 fiE?K@
*  UW[ H=I,lfi *4,(bfi
 p44}fffi  0P(i34W
" ff'%fi  $fi
 ffFhi C4
Mfi? p7fi
 [?0 CfiO
 [40 fff'ElE!,st.  UW[ H=I,l {
*  UW[ H=I,lfi 4fi1
44} ff9  `_a
fi C'fi
@ff''E4
ffY%fifi Yfi(g?37r  7k
! [3O
 N?37L
" fffi 0 LfiR
 pC2F h
 ][bh ) cdlE*+Fp
?   )
45?3L' F 0t
 d45?pN ff'%fi  N()fif
:;'7(bfi834fi )S t
 d1! [30O
 NfiR
 pC ?fi4
()fi[4fi-'fi ff v 3	hB 7lE!,Hffid++ cpE4}   h  (K  * l<
 'fi(bfiE   	0/v p
V H=I, h  h  (J  * l'l$ B'M*+fp? @ 9BCy/!,
{ pffK'fiP E(bC?[   !
aUW[ HOI fi ()fiR V H=I,  	(i34%W" ff'%fi  NfiR vff545v KrO 3M@ Pvffw1![yh  UW[ H=I,lfi 454'fi04`:

0 Q()fi
 V I,
I  I " H=I,  
 V ffI  " H=I, ! l8T$p
? N fiE?_0'A
p
? ff
.1F+Fp
? ?p58@ ff'%fi @ [
fi32R
 
@ff13@ ffSfi(,?38'E fi m
fi 0fi    h  (   * lE
!  @ fffi 0 82f ff34@ ffS4(p*%()fiLQvff'Nfip
'' pSv  & hB,lN+F?fi $
3 Wv 
23p
 ffh  (J  * lE*6?pC@ _ofi($?27''Ev pYR
 ()fi@ /4pY o

 ffA?pL'v V
 ) MfiF(bfi44fid
+ ffY1C ff'%fi  6d1*0 Y?3>''v 3P3/ePo	(b
  * Mz
fi ffT
 Mfi
( v 
430p
v 
23p
 Odff4?pff!  30
?Y''Ev pY4'$
_ ffQ?pPfi
 ;m(9 Rfi m
( Z  d1!PT$?13L4
(   d
&

=%$

fi

mknpokmkq

 K?3NfiR

 ;	4A'3
3 Nfi5'fiv ffvpKhi!!*()fi[<lE*0?pY$4A%fi0[?Fff'%fiN4
@ff13@ffV!5c+U?4[3fihi!!*'G  dlE*a?p8fi	+AffS'fiR73M@L+88(ChB   7lF4F4(
?pp+FKp
 ffS
fi 4fi C4'fi?0$?p7@ff'%fi*!!*Y  d1!$cp(:Mfi*?pU?p@I
fi34YR
p+''v pFv  & hB  l-+F?Mfi 7


 vp3
3 $ 
43p
 Ph  (J  * lA03FM
fi Mfif4'()	?p>fi
 ;!
rMfi
 o/a *-3M%fi K/*9J*-
*A ## Y'fiP*5 &?pK'E fi &
fi 0fi  ZR
 ;+
 j
T 6T$|<C   j
T 6T$|-O
x ff134FV!-#
s 8X hi/*aJ*V*aV*V!!! lR
 >''Ev pPv  & hB<l5?fv0
43p
 ff
 j
T 6T$|<   j
T 6T$|-x7-?pQ()fi3M?	 _0()?C 4
 ff5v 	A

 v38E3
3 !6T$p
? [fiR
 p4
  ]hi
 iclE*1 ?p@ ()fi@ BK 
 dL 
  !  3M%fi RV H=I, p4} ff  h  _
T 6T$|,M*
 j
T 6T$|9xlN(bfiR'fiXhi
 
dlE*O+Fp
?  U47
*t+F?04
?R0N?p'v p(  hi/*OV*tV*t
*A!!! l#'fi
?   d!>cp(<?3I'v p	3/ePoG(bhi/*VJ*J*V
dlFz
fi ffc
 MfiNv0
43p
 >V*?3m?p@ 4
& hB  lE!8T$p
Mfi\+ K''v pC?Lv0
43p
 fff?p' Na3MNMz
fi ffc
 fiLv
433
 >?pI@ ff'%fi @ !>ffc mfi?pN+9fiE*

!ArOv 44*/4
( Y 
 d 0t
 	 
 d*  o/'
Ep
? ff
.Y4FPp
 L'fi
 L3M@ 	   )UR
 ff
3@ > 

'fi 
fi304KR
 Qfi?U@ ff'%fi  N U' ff!9W
H +'E $?fi34	R
 Lfffi43
 ff!
T$3
? $ ff
fi 06fi(a?p$v fi?'ff,?,
( 
 df 
 M
fi p+&' E<@ Fv 'fi/3
 ff
1
 p2}fffi *?pG?pIfiR
 'fi>2 j ( ffkK'fiKMfiM!7cpN4
 3 ff'fi	R
 ( YhB    7l
v W?48
 	
 ff
30 C4
(  h
 d1*6?pU ff'%fi  [$^@UWUWVM]N
 @ ff13@ ffZp
? @ !G+c Zfi?pI+9fiE*
Rff
3 C *g(bfiI
  R'' pYv  & hB<lQ+f?Mfi 

 pU$
3 Rv 
430p
 ffPh  (J  * lEj
* ff?p
 Mfi
' 8fi/

3M@ ffWfiL'fiS12vpt ( *gfiI
  ' 8+-7(bfi2fid
+ ffW1Y@ fffi 0 P4fiL'fi
/4v3* ( *MfifI ff'%fi  Nfi/

3M@ ffY()$/4pL * !
aUW[ HOI fi 4,fi33 Pa3M-Mfi-
fi0 [()fi9(i34W" ff'%fi  ffiR ff!gc9'fi$3 3 pff94<a ff
fiS?p7(i
[?qV H=I, M
fi ffT
 Mfi[v
@ ff@ 7

 ff4044p	fi(< 4
 ff[fiQ'fiP[14ffU()N'

i
h


!




!
*






fi
a



4





6
l






?
p



@
()fi@ [@ 
 `_a
4fi 
 CR
 [4fi1
44} ffP'fiIfi   h  (J  * lE;
!  UW[ H=I,fi
*
4
 MfiC
fiP0 UR
 ff
3@ GPff#fi3'03M j ffi4?4v '
 Sfi
( V H=I, kW+fp
? qv q()
V H=I,
4( Y'fiRMfiM!#rfi
 oM0 *-
(  }
 dG03O }
 d1*-?pY fi4?+F449fi3M'03 j Ffi4#?4
v 

 ffi
( V H=I  !^8
k c$APffCR
 f?pQ
 N?F   _
( MfiI' :
 )Y@ ff
 ffp
 ff-@ fffi 0 Jd7v 

ff

'

%

fi





7

2

F



)
(




 !f{np
?


8


W
U
[

fi

3
'
0
M
3

f






`

a
_







fi

S

i
(


443M@ *fz
fi ff
 Mfi
& hB lE*%fiQ(<P
*
H=I,fi

3M003/e
 $vM()fiPfi Y(bfi[r   0ff!-|<fiE$730'$RL@ ff'fi4 ffY1	@ ff ff
vp fi?p
 ff
 0v pIfiR
 'fiff!AHFfi8? j fik?fP`R@ F
fi 3Mfi4fi Y()fi
  UW[ H=I,fi ? U(bfif?p
6T
 E`_a
fi Yv fi?Pff
!  j ffi4!!!^kRfi3M'03M$4F
fi 4p
  ffK  fiff!
Mfi?pC4M
 fiYfiq
( aUW[ H=I fi 4I?Pz
fi ff
 MfiP4fi\+ p44}fffi ?C
' E!tc(MJ4tp
 ffE0 O'fi[0
 p+Z' t3Mv 3)p4}ff4fi *\?3Lfi ppffV'fiQfi/()
 UW[ H=I,fi 'fiQ
4 aUW[ R +fp? N@  _a
fi L+f?9 UW[ H=I,fi (i44ff*\v ffNfi(Mfi3M'03M'p j ffi4
?4N '
 fi
( V H=I  !^kT$?4NPfi1_a
fi m4'fi	_
o ffN?p(i4 OfiLfia ffKn
* ^@Uo5Y@ ff@  ff

?pfifi3A4
 Lff/v p8hi  ff
fi C1!xlA+Fp
? 	  `_a
fi U3

 ff!

 Ma[I]v

bA9* e$g

MuI] K9[	qezfG	  [_<^#]v!<n[	

e$g

" ff
2%?$fipNfi(gfi3M52PPfi @ ff
ff$2A4OffCfi$ff'%fiff!<T$?04Aff
fiY
fia@ff
L
?p+-fi'~:;
4O
fi0oMpSfi(-?pfi?0P!IsL3QRvG+F?G?p2O
fi0oM;
fi(ZVM]1^@` u !T$?4N4/h'h   ( ThB  l^l7 9hy>lffUJlf'fiU()fi?pP0fi103
Qfi(-?3v41403
 fr  [5(bfi5ffc 2 
 NfiR
fi
 p _a
fi * c
 h'h   ( T hB
 Bl^l [  [ -yh >lff 2UJl9'fi
()fi ?pffi/3
6()fi 6T `_a
4fi !6W
 @ qUY46?3T13>R
 <fi(fi *JT ChB  l4<?3W30>R
 
fi(A'ffLv  p Ifi >r  z
 *9 4Q?p137R
 [fi(A'ffL m?pIfiR
 ;Gr  P*J 
 9yh >l4Q?p'fi
 137
 Nfi(A'fiPhi832fi >
4fi ElE!T$p
? I@ ff'fi R(bfiL?04Q
fi0 oM;
@ff30,49?-?p@ f     ( T hB
 Blfi/3
,'ffA()fi-+c E4 
 F0fiR
 ; _a
fi */ 

=d

fi[



t}-m{t/s$q

os0pt{a

l qM 
 a

h    ( TChBBl^lf fi/3
A'ff5()fiL6T`_
fi!<T$?pQfi3M$4fifiKfi(VM]1^@` u QEff
?Mfi33?G2Jfi130
f'ff!LTF?p83pf4fifimfi(VM]_^a` u 8ff[?Mfi3p?m45 9hy>ld0'fi!
Hffi5?Fh  ( T ChB
 Bl^(l V -yh >lff\ Kh    ( T hB
 Bl^{l J V -hy>lff\@5?p5}ffOfi(a?pAfi/3

r  ' 4fi S(i3
3 
4fi m0 ffQ032f()fiB
 VM]_^a`	 8
 VM]_^a`R<*V@ ff'R
 ff
 ff4E
! qVJ]_^@` a IM
fi ff
FfiRUfi1fi.13MaA(bfiL
 ff
E?Y0v '!
-K
fi02'fi *fi3Mfv 
@ ffO
 V fiE? aUW[ a Q()fiW
 pEvpP?p>fi/3
5r  ?
l



(




(
4O
 [
fi0 oMpVPh'h  (  ( T hB
 Bl^g
l nUJl<fi&
 Ph'h    ( T hB'
 l^g
l 9 T nUJl-'fiIfi10(b8?p[fi/3

r  &()fi6+c 4 
 -fiR
 pL@  `_
fi 8fi
 6TX  `_a
fi V*@ ff'R
 ff
 ff!OT$?4t4gR
 ff
3 
l



(




(
?p'fi
 137R
 >fi(5@ /4 ffWfi/3
>'ffI4Ch   ( T hB
 ;llNfiKh    ( T hB
 ;l
l 8  *< 
fifi pL'fi 45
fi 04p
 @ ffh)R
 ff
3 N
+ L30O
 9V\ fiJH=I 
E? pff5?3po1$L(bfi$v p 
'fi ^ fi  fi  lE!9T$p
? N4O
 L
fiP0 oM;Cfi
( aUW[ u  fi  4$?pLO
 >F?$fi
(  UW[ u !
H o1f
fi 4p
T
 5?3L+9fiE'~:;
 >2O
 L
fi0 oMpfi(g'fi9h)@ \lp E`_a
fi S(bf?pNfi/3

?KR
 ()fiO
 ffV! cpY2
 Ph'h    ( T hB  l^y
l K 9yh >ldl	(bfi
 qVJ]_^@`  ! T$?4	4KR
 ff
3@ *Lv ?p
+-fi'f
@ $
*  Kfi130
F'72$

 ffa > U?p@ ()fi@   'Kv K?3>fi/3
$r  
' fi N(i3
3 
fi Qa ,4/44ff_
! [3v p> m 4?p-37R
 Vfi( j 0k8hiv N?p,*
 3j 
E?@  \l
'ff5v ?p[fi/3
-r  >*1?pC?p[+9fi~:;
 N4O
 Q
fi0 oMpPfiW
( VM]_^a`Rm4Z
 Ph'h m T &vd
l 

h    ( T ChB Bl^fil 5  5 -yh >lfflE!<T$?2949R ff
3 *Mv ?p[+9fiA
@ p*  
 'v ?p[fi/3

r  z' 0fi W()3
3 0
fi Wa C2>/4ffWfi 
 	fi ?pC3
 ?/:_07ff
E?X *6()fiI
 ff
E?&0
'*/fi v fi ?3Lpff'ffPp
 0?/:_0'6ffE
?
! dM(bfi33ff*?3$+9fiE'~:;
 $4
 $
fi0 oM;
fi;
(  UW[$ fi L 8
  UW[R fi L@ >?37O
 8F?0ffi
( VM]_^a`	> 0
 VM]_^a`R<*0@ ff'R
 ff
 ff4!$T$?4
4R
 ff
3 *$v q?pS+-fi'	
 T
*  #fi/3
'444$

 ff0 !T$p
? U ff''4
fi q'fi
' fi 	fi fi ^ fi  fi, $_a'5M
fi ffL
 Mfi$@ff03
 N?p j 0v Qk
fi0 oMp!
rOv 2*9
+ S
fi 04p
 P?3Y+9fiE'~:;
 m
fi0 oM;Xfi9
( aUW[ HOI  Y 
 aUW[ H=I fi !rO*9
+ 
 k
p
_ 3F()fi- -fifiv ff o/@ fffi t* 62<?pY30>R
 ,fi
( ffv ffO
 Av   ^S ^-yh >l< 0
 ^
n!6rMfi,+c E4 
 -fiR ffOfi(0?3-()fip
 ]  )J*0  ff134$ )O\v 0
 -
+ Aff'6()fi:
 ff
?P'fi
 ^
+Fp
? ?3Z
 ^K f
 )	?p-? ^K   )t*R
 ff
3 f+ o/
 ff
N )Og
   )Ov pE!6T$p
?  UW[ H=I,l 
@ff13@ ffF4O
 Ph a( N )Ol$'fi	p
 Pv 3>+Fp
? ?3J
 U   )t!7hBAp
? ff
.1v 3+F3
? ?pJ
  ( +A[12ff
@ff13@ ff[
fi 'Q4O
 ! 
l [3v pm )6fi
     ( T hB
 ;lgh)+F?4
E?S?fi34YR
 8'p
3 o/
 0Q3$p
 
0}ff@ P

3'
 ffElE*g v 
 S 
 9yh >lE
*  UW[ H=I,l  ff ff74O
 fid 
 VM]1^@`  
!  UW[ H=I,fi
@ff13@ ff74O
 Ph'h 

 I dM
l h a8
 8h )6   dl'l'l>'fim3
 Pv p+Fp
? ?p|
   d1*< Z?p'fi
+
(
*
 v p,+fp
p
? ?pZ
 	 
 d$ *
    )J! 5 ffPh o K 
l  9yh >ldR
 ff
3 9>?p9p
 
_ fi 
fi
( V H=I, {
* R7y/!,T$p
? @ ()fi@ *M30Pv pKh )On n d"
l  h'h m _ qvd,
l FT    ( T hB
 ;l_ F lQh)+F?2
?*
 v *t?Mfi34GR
fi
 I'p
3 oM
 >vG0}ff@ 

3P
 ffElE*V?p+-fi'~:;
@ P4O
 
fi0v o/pSfi(
 UW[ H=I,fi 4$4fid+ $? Y?0$fi( qVJ]_^@`R<!

BfGj[gfi[]v9[yIezfG  [<

bF



	 

Ie !Gj]3gfi[ ek^

{fi'~:;
@P4OI
fi0oMpS4MfiN+Aff/NC30()30tOff3M@!LT$?p(bfi@I+3M0vffOf?p
+-fi'~:;
 m / ff+F4? ff02
A@ ff3Pfi #
03&4
 !Q3M04PZfi @ ff
 Sv X?3ff 
o/
 E4O
 N4['fiU
fi0@ ?3Pv 
@ ffO
 6 fi4?PQ+F?G'fi6  `_a
fi V*JN
+ ff46
+F?ff
?Gfi?pff*%()fif?37
fi o1Qfi:
( fi/v p
 ff?0ff/fi4K
fi ff
[r  [!FT$3
? >4O
 7@ ff13@ ff
'dfiOBpB5\^9mVB\"  \y	'ECL\\Q\BB5^\\EmV\B"LDy UJ^\`"0  J
\\B^EB5e
 A  W0A X,  0c { ^J\~'d'fBA\ffB "mV[^\-BOB~dB~~;B^EQE0/^A
%^J\J\Etn J\V5^ Rd)F~E5BEVEJ~E~^\E~

=ML

fi

mknpokmkq

()fi$@`_
fiY45v`_a
A'fiM@ff$(J+L+AF4OffCfif@ff'%fiff*/Rff
3N':
`_a
4fi 	fi1

3MEF()8  #  ff7 v p8fiR
 'fifa44
fi !
 ()fi@ p

 ff
av pP?po/
 E4O
 J@ ff3* [30[
fi 04p
 [?po1R
 4
 tO
 ?Mfi/Mfi`:

fiX
! [40
fi/p
 f+--+$4'v 	# 3
3 fi  3
3 Pd['vy8+9fi./'fi V!6+c fi3M o/R
 4O
 *
r  [9
+ @ N 0MfiPPv 044} ffV*3M @ ff
-'fiI
 v 	@ ff''2
fi !6T$p
? [@ ff'fi 	()fiA Mfi
 pff
4I??44mp02
5+-dX'fiWv 24}  /432>v X%fi032fi (bfiP  fi23Mfi 
 fi4?K!9TFp
? @ >@ 7;+-fi ff''4
fi 05fi Y?p7r  f!5rO*0?Mfip
3 ?Sp
 P 4 S
fiI:
0 3fffi(Lr  [C@ 
o ff
3Mfi *F?pC? q E`_a
fi *$2p
3 ff #?p (bfi@ pff Mfi
R(bfi
 ff()fiI?pff o1R
 2O
 ff*gfi3I
E?Mfi4
 	fi($0304I@ @ ff fi fi($?pKr  [Khi 

Tg0 	vdl$ ff''4
f?pr f['fi	R
 ffv p3
 Pv 2'4
!  ff
fi *%R
 ff
3 8?pv 
@ ffO
 g fi:
?0Pf3O
 =   fiQ'fiK ff  pM*a
+ @ ff''E4
f?3Ir  [Q'fiY
fi04Y+F?S?4!LTFp
? @ 

@ $p+9fiI 0 FO
 ?Mfi/,(bfi M(bfiE
v pN?4,v ?pLo/R
 4O
 5h~vdl63@ F'0@ $r f[hi! !*
+F?K KyElA 0K. 
 pEvpp+r  ff3
3 2a'fit `_
fi Y3

 ff07h)+F?2
?KM
fi ff
Mfi[
. fi p+f?S0 8r  fElE*%fihlf3 73
  Ir  fT
 pv p ffU'fif3 fiR
 p
4()
fi V!>ffc Sa4
34d*%3
  Ir  fN@ ()fi
 ffm'fiY2'(bU+c E4 
 IfiR
 ffO
 ]  )
v @ vpIy$v f
 K
fi43
 fi(g?pN' 0fi 	(i3
3 
fi C0v hi3
?KFTt0v Pvdl-4R
 ff ffC+F?
 G'fi ^)J!f

D @ 8r  [f 7()fi
 ffS'fiC2'(bYrOE'~:;L
" ff'%fi @ >fiR
 vff$+F?U' _
 )
 @ ff'%fi  dQ1v  pNy9v  P
fi43

 4R
 ff ffI+F?P C'fi ^q)t!OTF?4:
 ff42Pv ff
' E<v 24!JHffiF?Xff?p<fi(%?pff fO
 ?Mfi/<46N/40v 5+-d8'fi7v 42} -Lfia34fi 

fi(r f<(bfi fi43Mfi R
 ff
3@ F:3@ ff:
 ffI3

 ff,v P4'()1v 3[?3$fiR
 p!OTF?4O0R
 
@ ff@ Qfi 4Y?pI@ ff34f+f?mp
  r  [!  [fifi #h~vffwwwlQ()fiN?pI@ ff3[+F4?S'a 
r  [! ( S
Mfi?p o/R
 4O
 f3
 ff p
 ff
44fi q+A	'fi#?Mfi\+
 ff3Mv n?p} Gfi(>?pr  [!
T$?Mfip
3 ?Mfi3M9?pWo/R
 4O
 ,?3@ f
+ @ Q3
 ff'fi8R
 F?M@ Qfi p
* ff
?	+F4?P?pQO
 Ivff
834fi N
fi !L|9
?v 0123fi Lr   ?xfiNxff! (+4 {?GxffN?p
' fi 	0 L
fi v Fx * fvff
 ' ff!
30Nfi(O_0
 Lffc 4 0
 7 	_a 7L
" fffi 0 LfiR
 vffA+-[3 ff*0+F?4
E?Y4F \F0
 0`o
[!+c 4 
 7fiR
 fff+  Io/@ ff@ ffU1U''fi p?38 [fi(,4g'fiPB
 ^)m()fiQfiR
 p
]  )J!T$?23/eC

 ffP()fiC4Ffi(Lfi3MC fi?P4fi ffq()fiffc 2 
 YfiR
 vff!rMfi 6T
`_
fi *%L
" ff'%fi  7fiR
 ffF+  Io/@ ff@ ffU+F4?UCrg4'~:;L
" fffi 0 7*
 3j 
E?tr  ()fiQ?p
p
fi fi(5?pfiR
 p
! T o/04 fi Wfi($+F?m?484>p
 ff13C(bfi8fi3M
 o1R
 2O
 74
Rfffid+>!grMfi
  UW[ H=I,lfi *' 
 )J* 07 ff'%fi  Nd*4/'fiP;
 ^aq) 0.
 ^\BKdF
+ @ $''fi@ ffV!  o
v 3
 
 0p
 Wo1R
 4
 F+ @ 8R
 ()fiO
 ffS'fi 4(b ff
E?mfi(<?p8fiR
 ff!F+c Sfi?pQ+9fiE*

 C@  `_
fi K fi?0+-5ffffK+F?>y$
3 
 `o$
3 -(bfi)
 ff
E?Yfi(V_a N+c E4 
 
fi$_a >L
" ff'%fi @ NfiR
 ff!9rMfiW
  Kfi 3Lfi(O?pff L$
3 *P@ $ 0Mfi  ffU+-f3 ff
()fi
 ppY?pP?@ r  [!Cffid
+  ff*6>2L4%fiN'fiU%fiv Qfi3M>?74< fi4?P
Rffv p
fi0 ff	+F?ff
?Ufi?pFd+=?pJ^@\YNr  fff!,rMfiW
 oM0v *v 	Tt0v >8+ L
fia@ 
 UW[ u h)fi\+vdlE*  UW[ u  fi  h)fid+=lE* 
 VM]_^a` u Ph)fi\+lE!AT$p
? 	4Vv Ma3MA?p>O
 >?M@ 

r [!r3M?pfi@ *t?p ff7 v p	fiR
 'fihi'R
 ff
`_
v '4fi fi(-?pPfiR
 'fi8
3
? ffPl
+A$?pL
 L(bfif4 fi?0P5R
 ffv p
fi0 ff!
k %
 dB  %Ut'OQ\\B^E\EE;\E\5^9B\5B~$$B~B^^FB< BV~E^\B^%\
d;E,^gB^g~\Bg`<1w5~^~OR\~\B^EO^L-\^B^~B)[5ff\~^941~\~\[~
N r=sOr;
tff%6B   UtENO t k tff
 $t9i;B~~%At~~\;B6B5^\9B~\^BW
 mVB[B\<\~\  % UJ~\B\E
$t,i;B~BP B'[-~F B~60EF^ffBp`B^-\B~~J EgE[\\B'Ef^AB^5

=Kx

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a

s Q30[
fi 04p
#
 Q?pI@ ff30f GTg0 ffNC 
 >1!Lffc Gfi?Gfi(9?pff I0 ff
* ff
E?mfid+ 
fi@ :
'%fiQ'fiGWfiE?K!"Ffid+F7@137ff(bfi84>(@0
!T$?p'vffcR()fi':
P 
 @ ff3ff*V'fiYR
 Pp
 ff
4
 ff?Mfi!Tg0 Y
fi0@ ff>?pPR
 ()fiP 
 fi(A'fi<  ':
`_a
4fi #+F?#?pS fiE?Pfi(  3M0 ff
fi C1!1*5+F?4
E?#
+ @ Sp
 ff pff&()fi
 V\ fiKHOI  
 ^a`y`
30fi !XT$p
? K30fi &3
 ffX(bfi?pff@  o1R
 4
 I+A
  
ff
fi !&T$?M 	3
  
 fihi3M @ ff
['fi	?p%fid :;
 fi 3ff@ ff''2
fi ElFr  f[
+ @ pff*t G?pm?p
fi/3
+A(bfi
 ff!T$p
? Y@ ff3P+AWfi/3
Pr  2'(b/v p?3UfiR
 p!=[R
 'fi
V\ fiKH=I +AF?3Sa4 ff*+f?4
?S
fi 4ffUfi(< Mfih)03Mffi F'fiCC'Iv 'ffSfi(<yl

?0 p'fiKE MfiPY
E?Mfi m0v 'Gv U?3Ir   ' 4fi U0v 8()fiNC Mfi
?fi4
 
fi(Vfi p[fi(V?p[?M@ Qfi !,rOv 44*?p[fi/3
-r  +--@ :B()fiO
 ffC @  `_
fi 	Mfi p!
T$3
? O
 ?Mfi/Mfifi ()fi
 pvpmTg0 >Y+-I2P44L'fiG?8(bfiITg0 1:
* oM
 8?
V H=I, +A?pY ff 0v pGfi E'fi X?pY43fi X+AP3
 ffX'fiRR
 Y
  (	ff
fi !Xffc Xfi?p
+-fi*?p$fi/3
<r  +A<()fiO
 ffV* ?pV H=I, 004 ffI'fi7?p)a3 V\5 z[=]<r  nfi(%?pf?M@ 
 *<?pCfi/3
7+-8. X+F?W?pCfiR
fi
 ;Rr   )
( pffp
 ff(bfi
 6T  `_a
4fi *< 
?pm@  E`_a
fi mR
 ()fiO
 ff!7Q
 E'fi9
 V HOI  
fi 4'ffmfi(9
E?Mfi1fiv pK	 fi ff- 
G Mfi
fi ^aF()fi8+f?4

? U}h K  ^ ;l8 *< 
?Mfi1fiv pmG Mfi
fi  ^\(bfiI+F?2
?
U1}h ff  ^\\l$y/*a
 	?pY 'pVU1}h ff  ^\\l$ =!
Y
fi23
 Yv STta fffPfi
 >4R
 ff ff j  ff

k  ff[O
 ff *afid 9
 >y3
3 *Mfi(6?37
03U4O
 
fi(6?p8 fi?Y!f-fi43
 054R
 ff ff j '
k  >?p8'R
 ff3MYfi\ N'fi*%! !*a?37
03U4O
 >fi(
?pv 
 ffO
 O fiE? v G?Qfid+ 4143
 ffKS?p
03G4O
 8fi(,?p
fi ff'%fi v p'fi
 fi4?K!UrMfi
 oM0 *O?p j 'k 'R()fi
  UW[ u v Wfid+
v  ffI7
a3R4O
 	/4p
 ff
1?pC
a3W4
 fi
( VM]_^a` u v Zfid+ !m-fi23
 74R
 ff ff j kG?Mfi\+?pd fi \30>R
 
fi(6 `_
fi \fi$fi\ B
 >y3
3 0!-TF?4$4f4%fi$'fiPfi 'fiFR
 ff
3@ *0()fiT
 oM0 *a?p

03W4O
 47fi'8''fi pm
fi@ ff2ffW+F4?R?p137R
 Lfi($'ff j 12ffkY03Mv pKM(i*O 
? 6T `_a
fi C493 ff!6|< fiR
 ;Ofi9
3@ ff
 0M(),'fi7R
 
j /4ff0k73M pM(i<+F3

4 ffG+F4?Sf
 pff'ff ffE
?*+F?04
?GPffSR
 71384O
 :;
fi 30Pv pMY
! f2'fiM*[2[2%fif'fi
MfiN?F+-L2f
 fi5(bfi
 L 	 `_a
4fi fiA'fiPfi/

3Mff!6c5+A5fi3M5fi @ ff
 L'fiPfi 'fi

03&2O
 U3
3 3
 
 3M$
4
3P'0
 ffP()fi
 fi/v pWr  [!q{p
?  fifi Y?pX
+ @ 
?p\3-@ ff3Ifi([0/v pG ff v 3SfiR
 'fiff!XT$p
?  j Ek
fi43
 I@ Y4v 3U()fi
Tg0 >8R
 ff
3 L?3N43
3 ff$@ L4y/*0B! !$
* MO
fi fiE5fi1

3@ ffK3MEv p8?pco1R
 2O
 Fp
3 Q'fi
041v 3PV fiJH=I *%?Mfip
3 ?S
+ 7?d 7fi0@  ff
 fiF'fifi/

3M[+F?Y?045fiR
 'fic
 Mfif03Mv p
?pTo1R
 4
 !OT$p
? f4
.PfiR
( fi,v ?pWo/R
 4O
 6@ ff3ffP()fi?pF04
34<E Mfi
r  [7?0P?R
 pffR'fiR
 pff#3v pY?3fo/R
 4O
 !|<fiI@ K30C
fifi 
+F?U?pI'R
 ff
`_a9

 V H=I,  fi Sfi;
( V\ fiJH=I *[
 G
 8 Gv UTg0 >1!fHffi8?0 j B
H <ifk	4fv 
?p j kY
fi43
 m(bfi8 1?v pKfi?3>? RY `_a
fi Z fi?0
 ff
30  j kY@ ( EN'fi
`_
fi  fiE!
T$3
?  fi?0Ph)fi\+FElN?Mfi34mR
 
fi 4p
 @ ffR '0v ff j V*^k j a*^km  j J*^kSfiI
 ff4 8
v 3 $ff j g
 [V!^
k  j 0I
k po/,'fi7  fi?0C
 O
 fv Tg0 fLfiZ
 >>p
 Mfiff-,4<>fi/3

 fi4?K* j /k?F$25I `_a
fi U fi?Y* K j 0kI?f$4A?p>3fi(t?3 j 0k
  j 1

k 'v ff*! !*1?p$2O
 $()fiZ
 OVM]<@ :B()fiPv 3N?pFfi130
<P@  E(b/v pM!OrMfi
 o/P0 *
 UW[$h)%lA4$
fi 4p @ ffK'fiR > Y fi4? 04$
fi 4'pfi;(  UW[ u h)l-()fi4fi\-+ ffY
 VM]_^a`y
h)lfhi@ [fid+FQv,: >7fi(tTg0 NlE!6cp_
( Mfi70fi103
-pff09'fiIR
 [(bfi
 ff*?3?3 j 0k8 fi fi(t?p
 fi4? 494p
 4
M'fi>?3 j /kL 4fi *v P+F?4
E?P
 [?p@ f26fi Ifi p$fid+4R
 ff ff j g
 [V!^k
Tg0 ffFM*x1*a 0
 C:B0@ ff fP30 7hi
03U4O
 Lfi 0MlAfi(g?37(bfi Tt0v fffP 0
 >
v mC(bfiEPN?N(i
44ffQ'fiO
 P
fiP04'fi 0!Lffc Sfi?3L+9fiE*Tta ffQM*Jx1*J 
 C	
fi v 

=d

fi

mknpokmkq

>1!<ffc KTg0 ff5M*ax1* 0
 C1*@ ff305@ 
j V*^k j a*^kfi j V!^k
s#t3ff2fiE<fiLfip9fi@,v@ff'pF43pO()fi@,24'vpAfi3M#o1R4?1fi?3ffff!
"Lff
2M?<+$@ F3v pNNrO'~:;W
" ff'%fi  5fiR
 p7r  n ?,?46r  n
p
? ff
.19fi 7?
?pL'YBff]5'E Nv \ G''v 34f(bfi2fid
+ ffGYC@ ff'%fi @ !frMfiNfi3M
 fi43M4fi Y0E 
h)+F?	p
 0 Nr  v 044}ff4fi alO+F3
? C30v p UW[ R fi  */ 4(b/v pIrO'~:;W
" ff'%fi  QfiR
 p
4T
 ff30v ['fi	 ()1v 3?p8()304JW
" ff'%fi  8fiR
 ;!NT$3
? 7(i4 fif()fi3
3 S
 aUW[g fi 
p
3 8'fiKQv 
fia pff[@ v S()
Q/fi4fi ffi(,?37(i34tL
" ff'%fi  8fiR
 p! (  T$p
? @ ()fi@ 
()fi
  UW[R fi *rO'~:;W
" ff'%fi  $r  [,@ T4@ ffIp
 ff30F()fi,@  _a
fi Pfi((i34MW
" ff'%fi  
fiR
 vff!,
 ff
3 7+ 83 ffK?pfi43Mfi Ca   K?pff o/R
 4O
 *0 0KR
 ff
3 
 UW[ R fi  ()fi33 ?pmO
 30>R
 Cfi
( fi	
 VM]_^a` g hi! !
* aUW[ g fi  (bfi3
3 0
 Mfi(i4 
fiEElE*()fiA?pfr  [Av P?3ff To/R
 4O
 9ff'vpIrO'~:;W
" ff'%fi  $fi
 ff<+A
 ff34 
'fiPff'pP(i34%L
" ff'%fi @ [0fiR
 ff!
rMfiffi3ML
 o/R
 4O
 */_0 L?%fi?pff@ ff$
+ @ Nff'ff
Mfip+/*fi@()fiP'ffU0(bfizTt0vffFI

fi3MRff	1

7vW[fi4?PC4fi@ffff
_a
4&()fiKc+4
fiff()'U??Mfi()fi
6T 
 `_a
fi V*VR
 ff
3 I?pP4'> Op`:B03%fi 	hi 0?pfi/3
Q fi4?P
 
43p
 Q U0fi %r  NlE!
[1FTF?pfv0
@ffOafiE?P9@F(i'A??pf'fi%fiE?P,(bfi-%fi?Pfi/3
-
  `_a
fi V!,T$?4A4L

 o1R
 ff
ffK'fiPR
 N'3
3 N
 ff
30 L?pC+  L4fi@ ffY(bfiFv ff v pM!
q>1FTF?p j HfckKEfiNfi(A?pv
@ffO,vfi?PN(i'>?R?pff>
fi330*
+f?4
?nMfi MfiC_kp+v4f'ff! T$?044fo/Rff
ffRff
3@Sfi(7?pG
@ffmv
'@ffP4v 0vpM!
FMRaUW[ H=I  [aUW[ H=I fi @Q?p[()ff'Afi(J4a?p[vfi?Pff*Rff
3[?p[2fi@ff
()fiQPffWp4
7ffv3fiE'fi8hi!!*'V H=I  ?p[?V fiJH=I lE*)z`yz>?pY@84'fi
2fi@ ffK()fi$fi p>
 ff
_a
[fiR
 ;C;1R
 * K?pK
E`_a
Q_k0vpI4Rfiff!
[x1RaUW[ H=I  $ UW[ H=I,lfi +F44/?dF?pfRff'<
ff3MfiRffff!tT$?pI+F24Mfi<.Qfi@
2OPNr  }v
@ffffff!8T$?04Q4'o/ff
4fiR
fiOffN()fi?p+-fi'~:;
4O

fiP0 oM;	/4!
 3Ma44	23pff[+Oo/Pp@?pR
fiPfi(,+$fip	@ff4
4fih)(bfi9aUW[ R fi  
 UW[ H=I,fi *+F?04
?K@ IMfi$
fi0 7 fi?ElE* K?pLo/483fi0@  ffU'R ff3V!
T$3
? N@ ff35 L?pN()fi4fid+fv pYhi3
3  ff5'ffUfi?3+F4 *4fifi.	$?p j  ff
k
fi43
 0ElE
7vFTtfiXm?pSff3*5vnTg0GZfi1fi.#	fid+FGvS?Mfi3p?w
fiP0@ff
?fi\+ 
 R?47 8+F?Rfid+(
 Lw1!Y+c Zfi?p8+9fiE*6
fi0@ Cfi\+ v+F?Wfid+vy/*6fid+ Y+F?
fid+ vv*O RfiYfi !"Ffid+Fv?Mfip
3 ?ZwY@ v fi?PQ()fi>ffc 2 
 fiR
 ffff*J 

Nd%B'$^0B\`^^NmV^\fiO~\  %UC^BE^'BE[B~B~  %Ut{mVB$OB^EB~HU	^'B\^<;BE
^%B~[E\^^'
 UJ BV^'B\^\{  R ~^B~BTo'BENPaP5i;BO BEmV^p[-,mUB^E
~E5'J~~F\\'5A^'B^d%ff\,PaP,BEV^5B  %UmV^^g;~;'-B,~t^%^ ^^m'
AgBEH
 Ua~p-E~;B^$Ed^\B^EA~B^\,;E\^Qff5EiEV^'B\^<;BE
^6E\^^'L   U,EQB^O^O^55'\B~[`E^N m'8[B~BT o'BEINBEgB~^\B^EC\~'d';
%pB`Bg~p51 mB^p'
 mV^J'E\%t
 0 R E1\Ea`^^N m%'F$OB~\ffBJ\B\^~
mV^^1O^55'd B~$B~^'

J

fi[



t}-m{t/s$q

os0pt{a

l qM 
 a

] K `6W
K
 Gh
	

E8

] K `6W
K
 Gh
	

8

 UW[ u  G
!^yyyMvffx
!^yyw
!^yyyw
!^yyxx
VM]_^a`	O
!^y>wQ
!wxCC>
!yCyC
!w\ >y
E




W
U
[
^
!

y



>

w

x
x
^
!

y



y


>

!



y

C

Q

w
Q
!x/vvff>>

 UW[ u   fi  G !^yyyyC !^yyCx !^yyyC/vd !^yy>y

 UW[$ fi * 
!^yyyMvffCw
!^yyCQy
!^yyyxQ
!^yy>y

 UW[$ fi  
!^yyy>x
!^yyMvvy
!^yyMvd3C
!^yy >x

VM]_^a` u   G
!^y>/vffxw
v!^y
!4vffw\
v!^y

VM]_^a`	O
 
!^y Q
v!^y
!/vvffQx/v
v!^y

VM]_^a`	 
!>yQ/vd
v!^y
!y Cx
v!^y

!^yyyw>
!^yyxy
!^yyMvffx/v
!^yyxw

	  UW[ u   G
!^y/vvy>
!wQwy>
!4vd3CCx
!wCQCw
v VM]_^a` g>
!^ywQ
!yy
!4vffQyy
!>v
 E  UW[ R 
   UW[ u   fi  G !^yyyx\ !^yyxwy !^yyMvd3QC !^yy>y
l  UW[ R fi k  !^yywyMvv !>\xy !^ywyQ !wxy
   UW[ R fi   !^yywxQx !^ywyy !^ywQ !4vffyMvff>
!^ywC
v!^y
!xQ\wC
v!^y
 VM]_^a` u   G
!^yyC
v!^y
!4vffQ>yw
v!^y
M VM]_^a` g>
!4vff/vff>
v!^y
! ywyx
v!^y
 VM]_^a` g
Tg0>1Wffi$R()fiP
9fid>y[3$-hxffiff*Cf33_ff
?alt+f?>fiR'fi;V\ fiJH=I


Sp7r  f!$"Ffid+F8vL?fi3p?Sw@>()fiF`_a
fiUfi(6c+4
7fiRff
Yfid+F>vy?Mfi3p?WvffQ@L()fiW6T`_a
fiYfi(g"Wff'%fiNfiRvff!

ff]{K `W
	Gh
zgig

]{K`n
K
ff 
Gh
 gg

!^yyyyyMv
M!xi:px
!y
!^yyyyy
w1! xi:,C
!^y
 aUW[ HOI  O 
aUW[O fi O 
!^yyyyy
Q1!x/v:px
!y
!^yyyyy>
v! iC :px
!^y
E
!^y>xyy
v!^y
!y
!yxyQ 
v!^y
!^y
 VM]_^a`yO 
aUW[ HOI fi   !^yyyyy !>i:,Q
! 3>
!^yyyyyC
1!^ywi:pw
! 3>

aUW[ a  fi  G
!^yyyyyC
x1!i:px
HB<i
!^yyyyyC
Q !x/v:,C
1
HBi< 

aUW[_R fi O
wM!CCyyy
!wQyww
>xCw1!>> 1> !xxyyyy !Q  vffxx1> !y

aUW[ R fi 
wM!CCyyC
!wwQ
HB<i
1
> !xxyyyC !Q /v
HBi< 

VM]_^a` u  G
!4vv Qx
v!^y
HB<i
! yw> 
v!^y
HBi< 

VM]_^a`R>
wC1!wxyy
v!^y
>xCw1!>> Q y/!^yQ yyyy
v!^y
vffxx1
> !y

wC1!C/vyx
v!^y
HB<i
Q y/! 3Q w> 
v!^y
HBi< 
8	 VM]_^a`R 
Tg0>1WffiR()fiP
7fid9>yC338hxPfiff*
C 330Lff
?l$+f?YfiR'fiBV H=I,

Sp7r f!$"Ffid+F8vL?fi3p?
> @>()fiF`_a
fiUfi(6c+4
7fiRff
EH8

?

Yfid+F$P?Mfi3p?WvyP@L()fi$@_a
fiKfi(O"Lff'%fi@NfiRff!

 

fi

E



]{J

EH?

 WU [ u   G
 UW[ u  fi 
VM]1^@` u  G
VM]1^@`yO
 
 UW[  fi  
VM]1^@`yO
 
 UW[$ 
 UW[  fi 
VM]1^@`y 










mknpokmkq

G

n

`

]{J`n

?

!^yyyMvffx
!^yyyy C
!^y >/vffxw
!^y >w Q
!^yyyMffv Cw
!^y Q
!^y >wxx
!^yyy >x
!>y Q/vd

!^yyyw
!^yyy C/vd
!4vffw\
!y Cy C
!^yyyx Q
!/vffv Qx/v
!y CQw Q
!^yyMvd3 C
!y Cx

Tg0NMWffiI
03K4OhiKff
fiElAfidB>yP330A+F?	fiR'fiV\ fiJH=I K_0Lcff`:
 
 >fiR

 ff!6T$?04A0 L4F3M042
fi Cfi(OfiO
 Nfi(O?p>P2vKTta>1!

 WU [ u   G
 UW[ u  fi  G
VM]1^@` u  G
VM]1^@` g 
 UW[ R fi O
VM]1^@` g>
 UW[R 
 UW[ R fi  
VM]1^@` g 


E











]{J

EH?

n
!^yyyw>
`

]{J`n

?

!^yyMvffx/v
!^yyMvd3 QC
!x Q\w C
!4vd3 CCx
!^ywy Q
!4ffv Q>yw
!4ffv Qyy
!^yw Q
! ywyx

!^yyyx\
!^yw C
!^y/vvy >
!^yywyMvv
!^yy C
!^yw Q
!^yywx Qx
!4vff/ffv >

Tg0>x1Wffi5
0374O[hi>ff
fi0Elfi\;>yF3$V+F4?LfiR'fiV\ fiJH=I >_0-"Wff'%fi
fi
 ff!6T$?4Aa L4$3044
fi Cfi(6'fiO
 Nfi(O?pL4VvKTg0>1!


E











aUW[ HOI fi G
aUW[ a   G
VM]_^a` u   G
aUW[ HOI fi 
aUW[_R>
VM]_^a` R 
aUW[ HOI  fi 
aUW_[ R 
VM]_^a` R*

]{K

EH?

n

`

!^yyyyyC
!4vv Qx
!^yyyyy
wM!CCyyy
wC1!wxyy
!^yyyyy
wM! CCyy C
wC1! C/vyx

y

ff]{K

8

W

`

!^yyyyyC
! yw >
!^yyyyy C
>1!xxyyyy
Qy/!^y Qyyyy
!^yyyyy C
 >1!xxyyy C
Qy/! 3 Qw >

y

Tg0C1WffiI
03K4OhiYff
fil-fidB>y330A+F?	fiR'fiV H=I, 	_0>"Wff'%fi
fi
 ff!6T$?4Aa L4$3044
fi Cfi(6'fiO
 Nfi(O?pL4VvKTg0>1!

M

fi[

t}-m{t/s$q



os0pt{a

l qM 
 a

fid+FCvyU?fi3p?nvffQU@Cfi?0PL()fi6T`_
fi!YcffWTg0>1*gfid+FCv?fi3p?
>@ I fi?0P$()fifffc 2 
 70fiR ff*0 0Ufi\+FQxP?Mfip3 ?ZvyC@ I fi?0P$()fi
6T 
 _a
fi !R-fi0 fi\+zS+F4?*<  >U+F?qvy/!#hi"Ffid+FKvC ZG
 3fi8R

fiP0@ ffXR
 ff
3 	fi\+zm?0 v fi?4fi ffZ()fiPL
" fffi 0 0fiR
 ff! lHffi

?0[?3ff 8
fiP04'fi 0f@ IR
 ;+m j @
 [ak G j V!^k
v 

  j@
 [akO
 ff 0 j /kCfi
j J*^kS?04L4LS
fi@ ff
8
fi04fi ! 9*aff gMaJK8_e %_]{ [	G ezJ!<  
fi M_0O
 ffC()fi-4a@ ff309 Tg0 B>1!9Y
 ?pQfi?p
^_e Le!G	 J@!<.ek^.jg3h-b cA49

?0 *M?pN@ ff3$@ L`
o ff	()fifTg0 >1!
[1FTF?pffff'+Aff'fiR
fi0@K4I'fioMPv3	Tg0ff8M*x1*-C1!c+?pffY
@ff?p

fiP04'fi 	4-R
 ;+K?p[_0E'9p+9fifid+F52
 ffv ff j 0kKh)fi j /k8fi j 0kl9 30A?pQ?
fid+zfi(F?0IO
 K4R
 ff!T$p
? 	@ ff'fi ()fiPP.1 pS?3ff K
fi04fi I48?I?pC_0'
p+9fifid+F9fi(tI
  	4R
 ff0
fi ff'%fi 'fiI 	v 
@ ff
  fi?0 h oM
 5()fi9fi\+8fi(
Tg0 ff[	 mxlF 0U?p8?EYfi\+fi(9
  m4R
 fft
fi@ ff'%fi F'fiK'fi6 fiE?K!
[
 0 ff*/fi pN
fi34f
 oMPv pNTg0 ff58 
 >1!6+c 	Tg0 L1*/fid+FNv[?Mfip
3 
? CCh)fi?3
?0 lO KvyN?fip
3 ?KvffxPh)fi?p,? YvvdlO@ fv 
@ ffO
 M fi4?P* Ifid+F,1*vv*
?Mfip
3 ?w1*% 0ffv C?Mfip
3 ?Xffv Q 8'fiO  `_a
fi m fiE?P!QT$p
? Ifi4

fiP04'fi 0- [R
 p+-	fi\+FQvQ Y*1 Y*x8 
 Q1* >7 0	w1v
* C7 Kw1*VvyI mffv C1*
ffv >I 0Gffv C1*Vv mvd*Vvff8 mffv Q1* mvffxI mffv Q1!<ffc Tg0 9>1*/fi\+FNv*1* C8?fip3 ?
L@ Fv 0
@ ffO
 0 fi4?P* 0Pfi\+F
 >> 
 QN?fip
3 ?Yvy7@ F'fi!<TFp
? Ffi4

fiP04'fi 0L@ PR
 ;+Rfid+FPv 
 >1*gY 
 >1*JS #vy/*6x	 
 Q1
* CK Rw1*O Z
 0Wvy/! 	?gM@KKek^.gM Eb T$3

? L'4'4
t _a
 
 Nfi(6?p>
fiP04'fi 0$v 
Tg0 fffP 
 >+ @ >ff'ff
! d[v 3P \oM
F{44
fiffo/fi YE M.:;3ff'*a2V
fi02'fi 
 ff  P'fiR?%fi?pff4[m &Tt0v Um@ Y'4'2
4Z `_a
 Yhb) y  yMvY *-v 

Pfi'F
 ff
* )cny  yyyMvdlE!5+c YTg0 >1*?Mfid
+  ff*?p>@ 0
 ff5R
 ;+ UW[R fi L 
qVJ]_^@`RWh)%fi?I?pLh)Mlg Yh)%lV Efi Elt@ LfiO4'4
447 0`_a
 g6?p:)cy  yMv
v 
 ffX
! [4%fi?pF
fia4'fi Fv KTta >@ Lv `_a
 $$?pN)cy  yMv>  ff!
q>1FTF?4?%fi?pff2PMfiffMfiaZ'fiW?pUfi4?P()fi@:B()fiPpm?pYfi/3
Pr  
 ff
3@ *ffi114fi3*F4C+f44$ ff34@ Sfi G2O
R
 m'fi K?pp+v 4f'ffK()fiK?p
{ +F4?S'fiKff'N?p|V\dM ^@`y`<4
 Iff/v pQfi(,?3 j Hfc~k Efi *'fi	
+ 
j Hfc~k	 fi ff!N

fi 0
 'Kfi Z?pCfid+f74R
 ff ff j V!^k TFp
? @ ffv  8
fiP04'fi 0>@ Cfid+ Y 3
 Q
 RTta S
 Rfid+zC 3.
 QKv Tg0 x1!m'h f4  ff*gfi p
fi304R
fi0 fi\+ >
 E3

 C1* Kfi\+vffP 3Lvffxv YTg0 >1! l|,
E?Ufi(O?pff 7
fi04fi $45R
 p+-G 
3 0-Efi 	fi(J?3Qv fi? ?$4-?pN
 LA?p j HFc~k
j Hfc~kI fi K 	
fi3
 Efi o/
 A?A-Mz

fi ff?
 Mfi9_k
 p+v 40'ff!,Tg0 ff
 >> 0
 C7@ YMfi9ff  
 ff
3@ 8?3Ufi S?ff P?3 j Hfc~k Efi Q03MfiQ?pff4Q
fi$
R
3 a!Ch)
{ fi 0Sd+
?3pffm'fiYP. fi p
fi0E4'fi mR
 ;+R4 j Hfck	 fi Q m?pffN
fi3
3 0*
+f?4
?G4f@ $b$ff
ffmv GTta 81! l yLgMaKKek^.gM < Gev%a[		  b f()[ffvp
?3>'4'2
V _a
 
 *545()fi3
3 	?0$?pN@ ff30$@ Lv `_a
 >hb)cqy  yMvdlE!

FMFTtfiNpPp-fQ@ff13@fft
fi4pEvpfTg0>f03MjMfi6Tt0vA1!6T$?2J4gRff
3-+-Afi4
3ff'fi>
fi0
 F fi?0PO(bfi,+F?4
E|
? V HOI  ?6R
 P02 ff!g-fiP0@ 5fi\+vA 30,1*
v9 30
 >1*f 36* L[ 309vyf'fi[@ 9?p-@ ff30_
! [4@ ff3t?Mfi\"
+ aUW[ HOI  Fh)fi\+
vdl6 P
  UW[ H=I,fi h)fi\+#lg'fiLR
 5,ff'96()<<?p5fi?p9 fi?! 9*zgMae$gM

  UW[ H=I,l Lh)fi\+vdl6 3
  UW[$ fi Lh)fid+lE*1?p@ 
[	eek^.jg3h-b ffc P40
 ff,fi?3,?

J

fi

mknpokmkq

2$Mfi4
ffaL'Rff3MV!6cffKfi'f
ff*0?3>'Rff3MK4$13LP4
!X[4Mfi2
ff0

 ff03M05@ L'2'4
4	 0`_a
 Nhb)qy  yyyMvdlE!
[x1FTtfi&ff'YQx1*[
fiP0@m?pm_0 j 'k
fi43h)()fiUx\:;'r  fEl+F4??3m@ff
fi

fi23
 G+F?S?4Q4R
 ff9h)()fiNx\:;'r  [ElE!fi@pff40>
ff3M?Mfi\+FN	fid+
4p
3 7(bfi j 'kP[?p8} 7fi(6?p8r  
@ffff!Fcp[404vffA?f?374fifi(6?pI
03
2O
 fi(F?pv 0
@ ffO
 9 fiE?'fiG?3C
03Z4O
 fi($?pC'fiA fi?p
 ff
@ ff@ ff
Pfi@ h)fiFv 0
@ ff ff$ ffEl-5?pLr  } Lv 0
@ ff ff!-c
 pN?Mfi34	P. >?4A;+-fi:;
fi43
 

fiP04'fi 7()figfid+FAv9?Mfip
3 
? CIh)03M_fiOlt Cvy[?Mfip
3 ?CvffxIh)03M_fiAvvdlJfi(0Tg0 A1*
 0mfid+FvI R1*t GK?Mfip

3 ?Rfi(9Tg0 P>R
 ff
3 I?pff P I2t?3Pv 
@ ffO
 
v fi?Pff!Ah)
{ [Mfi ]<
@ F%fi3M6?3$'fi0 fiE?POR
 ff
3  j %0k[2*18p
 
_ fi *
4+-d1Pv!^y	()fiL?pffY! l ( &c(,fi pP
fi 4p
 [?pI@ ff30[fi(- fiE?PQR
 ffv 3Cv G%fi?
a fffh ! M!{
*  UW[  fi  ?fid+F<0` ,
 ff3MPfi
 ff<v ?3$;+-fi>0v ff*03<+-Tpff
'fiN
fi 4p
 O%fi?7 Ofi(a@ ff34ElE*?pI
 ff9
 aUW[ H=I  fh)fid+vdlt 
  UW[ H=I,lfi h)fid+&ltv 
Tg0 >Q?fid+#?pAR
 ff'<
v ff3MIfi(41?p$ 
@ ffO
 M fi?0P!  [	ek^.jg3zh-b c
250@ A(bfi?3 j @ ff
kP
fi43
 09?05?pQ4O
 N
fi0 oMpfi(g?pff Q;+-fi fi4?P
z
fi ff?
 Mfi5v
 ff Ih)fi?35? 	Pv Mfi2
 ba3
3fi 0El<5r  =} Qv 
@ ff ffNhi NTg0 B>lE!

fi3M0fi(f3Ma44G43pff8@ffid+ Mffff!mrMfifip*<ff
4-?| UW[ g fi  
 UW[ H=I,fi @ cMfi5
fi0 !<TFp? @ ()fi@ *M54,ffA'fiP
fi439?pQR
fiNfi(tv0
fi@ff

@ff04
fi Qhi! !*M(i4 Yfil6?p	Pp
 ;
! aUW[g fi [Pp
 cMfi p!<rMfiF?pQ@ ff3Av 	Tg0 B>1*
>>fiff fi(  UW[ H=I,lfi ]^A@ ff2
fi --+  N+$fi pYhi! !*M(i4 YfiEEl,(bfi$?3Q} >xIr  fff*/ 0Yxy ff
+@ L+$fi 3(bfi$?3>} Nxr  [!
 4  
rOv 2*-
fi 04p
 ?pSPoM483 fi0@ a U'R
 ff3MJ
! aUW[ HOI fi ?Mfid+FC ( O: ff2)`)v
.

  fffi\ 
 VM]1^@`gfi 7} <xfr fi0v ffP,hiff Efi ffIfid 
 >yF$
3 E=l JT$?04V4 /4fft73
E?
fi(%?pF
fi 
 7 %fi3M;aUW[ HOI fi ]^,()2 )fi9!<rMfi
 o/a 
*  ?p$a4I@  `_
fi 
4O
 fi
(  UW[ H=I,lfi *g fi I
fi34W3 >'fiS@  E(bmU4fi pU ff1p
3 
 fi($ ff v 3Kfi
 E'fi

34v vpfv >fi 39?62~$
_ ffg?p-fiR
 ;7v >
fi 4p
 04L fft4O
 -? It. ff
 qVJ]_^@`R
'fiP@  E(bCfi p>v ff v p8fiR
 'fiff!
{ 
fi 0
43p

 ?2[@ ff
fi 1G30PP}ff pM*v Tta *J?p(i'ff'>v fi? h)0@ fffi 
fi3MP ff3El>()fi
  XfiR
 'fiff*A30fi *9 0&fiR
 pZpR
 !#ffc #Tg0 S*AP4P3O
 ff

?Grg4'~:;L
" fffi 0 r  273@ ffW()fiO
 6T E`_a
fi fi(FW
" ff'%fi  0fiR
 ff![R
 'fi
V fi 4 vfi      4ffiP'ffG()fi ?4f0 8R ff
3 I[2T MfiN
 ffNQ?4f4O
 8+Fp
? ?3[[+-fi34
R()>'fiUaU'fi9@  `_
fi fiLR
 ()fi830v I0044
fi 0ffi(A?pv 
@ ffO
 

 fi4?h)fi p7(bfiT
 ff
?S4P4 Qfi
 E'fif024
fi alE! ff
fi Q
fi 4p
 E$ U7  
'fi43fi R7()3M3@ P+-fi.a!	ffc WTg0 C* j Hffi pffkGO
 ff 
 fiU@  _a
fi W2L@ ff13@ ffV*J! !*O?p
 ff
 0v p>fiR
 'fi54-8fiE3
 3 ff	'fiIR
 [   Gsm(bfi-?4A43fi C 0fiR
 ;
2!





G  k$U#fi
%

T$?pG?CRn ff	3ffffi(>@ff
Kffff
?fifi/pff[
?pff
.1pM*f fifi/pff

3
? ff
./v pCfi(,4''E03MffK'/'ffPhiffi}ffP3V*gvffww/vdlE!NHT?pffffff*a?p84fY44'7vU?p
4E3M@ %fi3M>fi/p
 ff6
3
? ff
./v pUa4 ffG'fiU'/'ffPN?0>
? 3!T5+-
fi Mfi0v Oo/
 4fi 


@ ?pP@ ff ffE
?Zfi( fi.fi4'.1m 0 fi.Wh~vffwwlLfi Wv 0
@ ffO
 <@  _a
fi W 0?>fi(
ffzub:S)T
  O-`g,BE;EE^EEBBQ\B^VJdgB-B\Oi;B^iB^'/BB^Q^[B\fB^5



fi[

t}-m{t/s$q

 fiNH=I,J  K
fi
0Kc+4


V fiJH=I
aUW[$ fi 
V I,
I  I
 V3UV
V ffI 
 V3UV
V fi 4
aUW[$ fi 
V HOI 
a UW[ =H I  QfiRaUW[O fi 
V I,
I  I  ffI 
 V3UV
V I,
I  I vfi     
 V3UV
V fi 4 H=I 
a UW[$ fi 
V   I
a UW[$ fi 
V I,
I  I " fi 4
a UW[$ fi 
V ffI  " fi 
a UW[$ fi 
V I,
I  I " H=I,
 3V UV

V ffI " H=I,
 3V UV
V   4fi 
 3V UV



os0pt{a

l qM 
 a

 fiH=I M  ff
fi
 K"Wff'%fi


aUW[_R fi 
 VMUV
 VMUV
aUW_[ R fi 
 UW[ H=I,fi
 VMVU 
 VMVU 
aUW_[ R fi 
aUW_[ R fi 
aUW_[ R fi 
aUW_[ R fi 
 UW[ H=I,fi
 UW[ H=I, fi
aUW_[ R fi 

'2 
ff
fi3
Kcff4


 UW[$ fi
 V3UV
 V3UV
 UW[$ fi
 UW[$ fi
 V3UV
 V3UV
 UW[$ fi
 UW[$ fi
 UW[$ fi
 UW[$ fi
 UW[  fi
 UW[$ fi
 UW[$ fi





















 
K
fi
0K"Lff'%fi

 UW[g fi
 V3UV
 V3UV
 UW[ g fi
 UW[ g fi
 V3UV
 V3UV
 UW[ g fi
 UW[ g fi
 UW[ g fi
 UW[ g fi
 UW[ g fi
 UW[ g fi
 UW[ g fi












Tg079sffpfiR'fi5+F4?	?pL(i'ff'$@E`_a
fiY?Mfi1V!

 .IB!qh~vffwwlE!Z-fi?Xfi(F?pff@CaE7@K%fi3M8@`_
fifi(f'fi();+A@Y()P3
fff?pQ?m08fi!>HT?pffvff[?p7+-fi.G4Fff4ff!  fi.fi4'.1U  fi4.
3 ?3Pfi10
 n%:;
4
3430Q'fi\o10@ ffQffc 4 
  sJ 3ffNfiR
 vff!T$p
? G@ ff L
v 

 ffO
  fi >fi(Ffi/p
 ff
p
? ff
. g?0gM
fi ffVafi1
.:B1:B0fi/
Y
. fi0
fi03Mfi 0Jfi(/_
o ff
%fiv *\E?pO? 6TZfiOfi
 ;:;'R
 ff
`_a
<fi/p
 ff1
p
? ff
.1 pNt
+ AMfiM!gTFp
? -v ff v pFfi
 E'fi
30O
 ff>1L?pff4g fiE?@ -ff3
 -p
 ffv fi = <0fi Jfi 7f@ @ ff@ fi I2P44t'fi[r  [

4 ffXs/T  h)0383304. Cfi3M7344fi I+-fi.*,?p3O
 Kmv 3 Cs/T  lE!OTFp
? +-fi'~:;
@ 
4O
 m
fi0v o/p#fi(L?pff4	v fi? 4?pO
 m	?	fi(L'fiQ@  `_
fi *f4?Mfip
3 ?
?pff ffP04
[@ ff3	@ afi1fi1V!HffiR?K+W?d WX4fi[@ ff3C()fi\
 ffp
 W3
 ff fi !
ffid
+  9
+ $Mc
fi Mfi<?d F Pv 
 ffO
 Mv fi?'R
 ff
`_
4N4fi ffI(bfi:
 ffp
 ffi Kh)()fi
830v Ifi I  6Tfi70fiR
 ;:;'R
 ff
`_a
fi/p
 ff<
Ep
? ff
./v p=l ?,?3>?4LffR
 Y()34'()3
 ff
fi #()fi(i3M3M@ U@ ff ff
E?!  .
 C!]^Cfi
E?
fi 4'fi(L
fi  pW30 U 'fi
r  [*6?p pvpm Zff'vpG(i3
3 
fi 7?IPX(bfi?pC%fi'~:Q'fiS?pC@ ffv ff v p
r  # 7fiR
 p!tc(?pAp
 ff4@ ffL(i3
3 
fi 8
 8R
 ,()fi3
3 *?p>aLF?3fi@ ff(bfi AL3? 
h~vffwwlE*6+F?2

? 3E ff7?L?pC ff  pK4 j ( !^k [?Mfi3
3 a
? MfiU
fia o/4;m@ ff34L@ 
fi\12p
 ff*?pp:; 0/:Bff'5fi
E??09?pP3
 ff
R
 $R
 ff,'fi7R
 F
fia3Mfi 44
o/
 0 !ffc q
fi ''	'fi  .f
 	!*5+ m?d Sfifi(i 0 ff044
L
 /4p
 0
 U?fi3M
 ?Mfi/@ e
v PV*9v X'fiO
O
 K
 ffff*9??3@ K30'44WPfi@ e
 ? 
'fiV  `_a
fi Y(bfi 

?V!
T$3
? @ L4$4fiP@ ff4ffU@ ff ff
E?Uv K?3N$
_ ff4Kfi(6
44
02 3v pM!Offc K02
34ff*M
{ ff4S 
|<'}ff4fiFh~vffwwl7?ffCYO?Mfi/R'fiGv
@ff44Uff'8Wfi]^7a4'fiUpff
2pP+F?p?3L'fi
 p+
fi 0>'fiS?p02 ! f
fi 0>@ Cp
 ffRfi 0m+Fp
? R?3ff
 Rff
8fi Mfi71fi2

v U;1R
 Lfi(O+c E4 
 NfiR
 p!9T$3
? ff$O
 ?Mfi/U?F'fiO
 >2P444ff-+F?	fi3Mq
  UW[ H=I,l 
 fi4?K!Ic
 p`R@ 
 4[?Nfi3MLO
 ?fi14Q()fiN@ ff
 ?pN? mfi @ ff
 a4 !

$

fi

mknpokmkq

Mfi?pI4L?8fi3M8`_a
4fiWO?Mfi/4o/@ff@ffZ3vpK?pC(bfiEP<()fi334fi>vW?p

fi/pff
?3ff
./vp443M@!

 fQ?dIm?fid+WS'fi	Iff
4
@ff4fiAfi(V@ff
Lfi5040=<''ffLhiA3M.1?E*vffww>@?zAL0M}ff/*Vvffwwx@?5Off
S.fid/
?*tvffwwC@?0rMfiff*VvffwwClE!-r  0409?dQ	3@ ff%fi?(bfi5834fi 5
fiR
 `:

fi		
fi1fiv4fi!6rMfiLoM0*MrMfiff]^7h~vffwwClA
fi: fi/v pPr fi 5()fi$
fiR
  
OS04ff/vp+@Ufi3ffq%fid! 4P42;Z+F?Xfi3M+9fi.X2?rM
fi ff[30O
 ff
fi]0025@co/@ffffU 6:;3M'fiP/!-W
H  ?pff ff*arMfi ff_
 3 f4
3 ff- `_
fi 
fi(<?pff@702!QQfi4PGG"$fi
p
? ffv Xh~vffwwl[ ff [	O
 ?Mfi/U()fiN7344fi [
fifiE`:
fiS?N30Offfr   04
 ![S300 Lfi L
fi1fiR
 71Y./v p	
fi ['fi	()dfi0
A?pff?14fi O
 !6T$p
? Q
fifiR
 fi 	''EC4-4a ffO
 ff1804 Cp
  fffiR
 -+F?Mfi
P 134 ff4?pmr  [!T$3
? U@ ff44fi ?q'fiZ?pG+-fi.#p
? @ m4P?0C?3&@ ff Cr  
' (bfifi 8?O3@ C832fi 
fi1fiv 4fi !RsJ. +f4 *<v fi3MI@ ff ffE
?*-Gv ff /:
v pCfiR
 'fiN?N4QCfiE#
 3E ff j ( ffkK()fiN'fiO
 7304fi Q
fi1fi fi GfiR
 p
' (bfi7?3	r   +F?4 3MEv pS
fi1fi fi 
! [?Mfip
3 ?%fi??pffIO
 ?Mfi/& fi3M
3
 >?045
fifiEv fi V*1?3ffF'fi43fi K45P 13+F3
? @ ff5fi3MF4?
 4@ ff	3M'fiPff!
[QO4fipffmvU?38v'fi/3
fiUfi(<?4f0Rff*%r

 fi
 -fi(0?p5fi@ -@ ff
 6ff ff
?Ifi Ifi <
fifiEv fi 802 ffV()fiP1 `_
fi IO
 ?/:
fi/!LrMfioM0v*%s#PmDQ3M(Kh~vffwwlNfi13fft?3ffNfi]V@ffP2
Q+F?mC(bfi44
444-'fiDj'
 p>h)E?pF? Ur  fElE!<TFp
?  E(b	'
 
?fi }fffi Rh)ffc 2 
 \l9fiR
 ':
ff*+F?2
?Y0@  Npff0fi1
.a*%3 pfi/p
 ffJ
Ep
? ff
./v pM!Qr3M?pfi@ *%
s  SDQ3M( 83
3 ff'
@ff
fi\ ()fi()4v ffP `_
fi 30v pLp+9fiO
 ?fi1ffg
fi 
 0Avff v pM* 8O
 ?Mfi/C fi:
fi3-'fiI?A3 ff"fPp
 N {fi ?h~vffw QwlE!--3.1?0Sh~vffww >lA 0
 ALa M}ffCh~vffwwxl
30O
 Ffi -a4 <@ f@ @ ff@ ffCV
 6:;3'fiP/*M P?pP0M@ ff94p
3 ff,fi(fi13
 ff0
3
? ff
.:
v pXff%fiQ
fi 2
S0fiR
 ffCfi(7?3 @ fiv Rhi7344fi El	04 !T$?30?3@ R4	 fid+fv p
@ ff
 ff3
 7(bfi8M@ ffv pY834fi 7
fi1fiv 0fi 1
 o/@ ffv pK02 L
 6:;3M'fiPm 
()/v pP?pff +F4?Sfi/p
 fft
p
? ff
.1v 3M!LN3MF+-fi.Y030445fi S?4f@ ff
 ff3
 * G4'f
fi o10
*VR
 ff
3@ Mfi pfi(-?4Q@ /fi3[@ ff@ ff
?ZM@ ff@ ffY
 e
 > `_
fi m(bfi7fi >?
 ff
 V!
r v2*6?p@Y@Y4YO?fi18()fiP
fi''vpG?pKRff?ff/fiIfi(Qfi*-+F?2
?
O
@[
fi0ffOI'fi>`_a
fi@ff(2:B0ff!grMfi?o/a*  ?fi? T#33?Mfi'}
h~vffwwxlgp
 ffv 7fi O?OfiR
 L'fi1
24d+Ffi* ! M!*( ;8
fi  fi ff*N ff''4
v3F?pAfi ff]

fi ff!<W
H  ?3ff ff*1?pFa4 p
 ff p9ff
 Mfi9RF0 f'fi7 4
0Q 
 3v p9204d+F
v'fiK?pfi 7R
 (bfi ff? R
* ff'R
 ff
444U(,?3Pfi 7?ff 'fiU0!Y
 p'fi43M4fi G2Q'fiY3 
4d+F-?54fid+PoM483i
 b$o/4044pYhirg4'fi3%#
T 33?Mfi'}*vffww QlE!9ffid
+  $?29'fi23Mfi 
M
fi ffT
 fi[44fid+(bfiN
 v G
?0 pffNv Y?3702 *a30
?Gf?380fi KfiNp
 ff 4fi Ufi(<
fi 0!
KR
 ff4v pI 4 L+9fi304C
 Q'fi
fi30 Nv 2$
 p pv pIfi(O'fi/
44ff+fA+F?fe
 
@ _a
fi Y()f ff7 v pM!
O
 ?Mfi/(bfi 3Mv pZ0?12
4&%fi3
3 p
 ffR
 ff?ff/fi	fi(Ifi Y2 j `_a
4[0?/4
k
h  RffK [fifi*7vffwwwlE!{?`_a
4[0?/4
*F832fi	Rff?ff/fiK4C@ff''E4
ff
`_
4V()fi
ff[R;+S?38fi!NHW?pffff*%+F?3
fi3$pI3$2
0ff


30P'
 ff*`_
40?12

 pffJ'fiFR
 <
fi0 ff
 ff>+F?N@  E`_a
fi 7  j ' pk
()fiF ff(b:B@ 0>hBQfiMfi  FB!*JvffwwwlE!

\

fi[



2 

):

t}-m{t/s$q



os0pt{a

l qM 
 a

 !#"

  

$&%')(*+*,')-/.(10fi230%465879%:;0=<>58(%	:;?A@B58C284D58(E@F0@HG28?A:;53*I4JKL0D.?(CB23'M:;')?2ONP<Q0:/28C6C10fiRS?58(B7T?(C658(UN
*,'V:;?-W*X530fi(B7><>58*X.Y@F'V0@H23'ZB?A%')(*X7&RG7,*\[]'^?CB?A@*X?A[B23'Z@B:X')C58-W*X?A[B28'Z_`o5a:;?A@B58CB234
:X')7X@F0fi(B7X53b'J $c(
?A@@:;0fi?-;.D*,0a:X')7,0fi23bU58(1%d*X.1')7X'^@]0*,')(*X5e?28234a-W0fi(1fH58-W*X58(1%g:;')hG58:X')Rg')(*X7\587i@:;')7,')(*,')Cj.1'V:X'JQkl(j7XGRdN
RS?A:X4Z<Q'&.?)b'c7X.10=<&(d*X.?A* -W'V:X*X?58(aRS?-;.B58(1'\23')?A:;(B58(1%90@]'V:;?A*,0:;7 ?A:;'\?T@:;530:/5]mn<&53*X.S(10T:;GB(UNP*X58Rg'
:X'Vb'V:;53oH-V?A*X530fi(Hp 7X?Aqn'>*,0d@]'V:Xqr0:/R	JLks(a0*X.1'V:t<Q0:;CB7VZ<&.1')(
-W'V:X*X?5e(uC')7X53:;?A[B28'i@:X0@]'V:X*X53')7Q.10fi28Ca@:;530:
*,0+23')?A:;(B58(1%1ZA*X.1'V4d?A:X'i%fiG?A:;?(*,'V')Cg*,0+.10fi28Cd@]0fi7,*sNv28')?A:;(58(1%1JwK\.1't@:X0@]'V:X*I4^-V28?7X7X')7-W0fi(7X58C'V:X')Cd.1'V:X'
?A:X'ckl(bA?A:;58?(-W'c?(Cax>')7,@]0fi(7,'Jy')?A:;(58(1%^0@F'V:/?A*,0:;7tz ;{I|}{v~8{=ZUzr {  Z1\z X{l|{I~8{  e { /ZU?(BCa\z X{l|{I~8{ B ~3A l
<'V:X'aqn0fiG(C*,0@:X')7,'V:Xb'a@:X0@]'V:X*X53')7M58(')53*X.1'V:0qi*X.1')7,'Y-V28?7X7,')7VJ10:
fi  X {v~ ?(BCfi   |  FZ
<>.1'V:;'>*X.1'V:X'9587Q?M7X5e(1%fi23'MmRG23*X5rpl?A%')(*iU$@B28?(Z\z X{I|}{v~8{s  {IBZUz e { v  {I ?(C
z  ~ 4  <'V:X'&qn0fiG(C
*,0
@:X')7X'V:Xb'Tkl(bA?A:;58?(-W'T@:;0@F'V:;*X53')7VJ$&2e2H0q*X.1'+?d@B:;530:;5F:X')7XGB23*X7t?A:X'^58(C'V@F')(BC1')(*0qw*X.1'+7X53V'T0qw*X.1'
U$?(CD?A:X'+*X.'V:X'Vqr0:;'^?A@@B285e-V?A[B23'c*,0
?(4	U$*X.?A*>.?7\[F'V')(Rg0C')2-;.1')-;')C0:;58%fi58(?28234J
 't*X.1')(dC587;-VG7X7,')C^*,:;?(7,qn0:;RS?A*X530fi(B70qB28')?A:;(58(1%>0@F'V:/?A*,0:;7?(C^*X.')53:w-W0:;:X')7,@]0fi(C58(%>?>@B:;530:;5
:X')7XGB23*X7*,09?\@:X0UCG-W*@H28?(JLK>.587?CC:X')7X7,')7] =|~  |    ZA<>.1'V:X' RG23*X58@B23'?A%')(*X7w')?-/..B?)b'*X.1')53:
0=<&(@B28?(j[BG1*c*X.1'gRMG28*X58?A%')(*9@B2e?(jRG7,*c[]'M:X'WNPqn0:;Rg')C6?(BCj:;'Vb'V:;5OoB')Cj*,0DC1'V*,'V:;RS5e(1'M<&.1'V*X.1'V:
RG23*X58?A%')(*T@:X0@]'V:X*X53')7T?A:;'g@:X')7,'V:Xb')CJgkI*T<t?7+C587;-W0=b'V:X')C*X.?A*T0fi(234zAX{I|}{v~8{=Zze {I ZzAX{l|{I~8{l e {IZ
?(CDzAX{l|{I~8{;  ~33la@B:X')7,'V:Xb'T*X.1')58:\?g@:;530:/5H:;')7XG23*X7tqn0:>*X.5e7i7X53*XG?A*X530fi(J
58(?2e234Z)<'@:X')7,')(*,')C+(10b')258(B-W:X')Rg')(*X?2A:X'Vb'V:;5OoH-V?A*X580fi(+?23%0:;53*X.BRS7]qr0:L?282fi-V?7,')7L58(9<>.58-/.9*X.1'
?i@:/530:;5:X')7XG28*X7?A:X' (1'V%fi?A*X53b'Jkl*<?7L7X.10<>(+58(9[]0*X.T*X.1'V0:;'V*X58-V?2?(C+')Rg@B53:;5e-V?2-W0fiRS@B?A:;587,0fi(B7]*X.?A*
*X.1')7,'?23%0:;53*X.BRS7\-V?(E7XG1[B7,*X?(*X58?28284a58Rg@B:X0=b'+*X.'^*X58Rg'M-W0fiRg@B23'W153*v4Y0q:X'V b'V:;53oH-V?A*X530fi(D0b'V:9*,0*X?2
:X'Vb'V:;53oH-V?A*X530fi(Eqr:;0fiR7X-W:;?A*X-;.J^ Rg@H53:;58-V?2:X')7XG23*X797X.0=<')C6?7TRMG-/.6?79?  NP[B582828580fi(UNPqr0fi2eCD7,@]'V')CG1@J
K\.1')7X'>?A:X'&5e(53*X58?2U:X')7;G23*X7VZfi[HG1* -W0fi(*X58(G')CS:X')7,')?A:/-;.Y?230fi(1%+*X.')7,'&285e(1')7<>582e212853')234[]'>?A@@B285e-V?A[B23't*,0
?a<>58C1'+:/?(1%'M0qQ58Rg@]0:X*X?(*>@:X0[H23')RS7VZB5e(-V28GC5e(1%g?
bA?A:;53'V*I4D0q ?A%')(*9C0fiRS?58(7c?7><')282w?79Rg0:X'
%')(1'V:;?27,0qr*I<?A:;'M?A@@B2e58-V?A*X530fi(7VJ
 .1')(23')?A:;(5e(1%587a:X')hG53:X')CZt<'67XG1%%')7,*Y*X.?A*u*X.1'?@:/530:;5\:;')7XG23*X7u7X.10fiG2eC[]'E-W0fi(7;G23*,')C
oB:;7X*VJMklqQ(10	@]0fi7X58*X53b'd:X')7XG23*X7gm5J'J3Z*X.1'S23')?A:;(5e(1%a0@]'V:;?A*,0:M587T?(UEywp>'W1587,*VZ*X.1')(58(-W:X')Rg')(*X?2
:X'Vb'V:;53oH-V?A*X530fi(	@:;0-W'V')C7)J
KL0	*,')7,*+0fiG1:T0b'V:;?282qn:;?Rg'V<0:XFZ<'S.?)b'
5eRg@B23')Rg')(*,')CE*X.1'g:X0b'V:;7T'W1?Rg@B23'g0qQ*X.5879@B?A@]'V:
?7^-W0ANP'Vb0fi28b58(%?A%')(*X7?7X7XGBRS58(1%fiFQ=|~  |    Zw5J'J3ZLRG23*X58@B23'd?A%')(*X7M')?-;.<>53*X.653*X7+0=<>(@B28?(J
 4uGB7X58(1%*X.1'+?d@:;580:;5B:;')7XG23*X7t?(C	58(B-W:X')Rg')(*X?2?23%0:;53*X.R
7VZU<Q'+?-/.53'Vb')CD7X58%fi(5OoH-V?(*i7,@]'V')CG1@B7VJ
 'j.B?)b'j?2e7,0C1'Vb')230@]')C?Rg0:X'7X0@B.587,*X5e-V?A*,')C?A@@B285e-V?A*X530fi(*X.?A*aGB7,')7S:X'Vb'V:/5OoH-V?A*X530fi(CBG1:;58(1%
'Vb0fi28G1*X580fi(JLKi<0&?A%')(*X7-W0fiRg@]'V*,'5e(+?\[]0fi?A:;CT%fi?RS'Zfi?(C+0fi(1'Q0q1*X.1' ?A%')(*X7w'Vb0fi23b')753*X77,*,:/?A*,'V%4+*,0
58Rg@B:X0=b'>58*VJK>.1'i'V4S23')7X7,0fi(S*X.?A*Q.?7 []'V')(S23')?A:;(1')CSqr:X0fiR*X.5875eRg@B23')Rg')(*X?A*X530fi(S5e7*X.?A*Q?28*X.10fiG1%fi.
*X.1'9*v4@]')7i0qLU$c7i?(C	23')?A:/(58(1%0@]'V:;?A*,0:;7i?A:X'+7X2e53%fi.*X284SC5O]'V:X')(*tqn:X0fiR*X.10fi7,'T@B:X')7,')(*,')C	58(u*X.587
@B?A@]'V:)Z1?(Cu*X.1'9@:X0@]'V:X*I4a587thG53*,'9C5O]'V:X')(*Tm58*t5e7?g-/.1')-Xuqn0:\?g-W'V:X*X?58(Y*v4@]'90qL-W4U-V2858-c[]').?)bU530:
0fi(E*X.1'^[]0fi?A:;CFp/ZF58(B53*X58?2'WU@]'V:;53')(-W')7c7X.10<*X.B?A*&*X.1'dRg'V*X.10UC10fi230%4j?(C[H?7X58-+:;')7XG23*X7&.1'V:X'd-W0fiG28C
@]0*,')(*X5e?28234
[]'T')?7X5e234
'WU*,')(C1')C*,0
?gbA?A:;53'V*v4u0qRMGB23*X58?A%')(*\?A@@B2858-V?A*X580fi(7VJ
G*XG1:X'E<Q0:X<>58282iqn0U-VG7
@B:;58RS?A:;5e2340fi('WU*,')(C58(1%*X.1'?@:/530:;5\:;')7XG23*X7
*,0!0*X.'V:Y28')?A:;(58(1%
0@]'V:;?A*,0:;7/RS'V*X.10CB7+?(C@:X0@]'V:X*I4j-V28?7;7,')7VZC1'Vb')230@H58(1%Y0*X.1'V:^58(B-W:X')Rg')(*X?2:X'Vb'V:;5OoH-V?A*X580fi(6?23%0AN
:;53*X.BRS7VZ1?(C	'WU@B230:;5e(1%M@H28?(	:X'V@B?58:i*,0S:X')-W0=b'V:cqn:X0fiR:X'Vb'V:;53oH-V?A*X530fi(	q?5828G1:;')7VJ 9(1'T<t?)4	58(Y<>.5e-;.
*X.1'^?S@:;530:;5]:;')7XG23*X7\R
53%fi.*i[]'T'WU*,')(C1')Cj587i[4YCB587X-W0=b'V:/58(1%g<>.1')(23')?A:;(58(%g0@F'V:/?A*,0:;7\<>5e282]RS?A'
?g@:;0@F'V:;*v4
*,:/G1'Z'Vb')(58qL53*i<t?7>(10*>*,:;G1'T[]'Vqr0:;'+23')?A:;(58(%1J
$hG1')7,*X530fi(d*X.?A* <?7Q(10* ?CC1:;')7X7,')CS.1'V:;'\587<>.1'V*X.1'V: *X.1'>58(-W:;')Rg')(*X?2BRg'V*X.10UC7?A:X'>G7X'VqnG2158q
RG23*X53@B28'RS?-/.58(1'\28')?A:;(58(1%c0@]'V:;?A*,0:;7Q?A:X'\?A@@B2858')C5e(d[B?A*X-;.Emn'J%1J3Z?70fi('>RS53%fi.*<>587X.*,0^C0T<>53*X.
;A

fi9H1HH
0@]'V:;?A*,0:^z  ;W  ~33lBp/Jdkl(*X.1'gqG1*XG1:;'<'S<0fiG28C2853'd*,0D'WU@B230:;'d.0=<*,0.B?(C23'*X.587T7X58*XG?A*X530fi(
 587a58*aRg0:X'E'Wa-V53')(*u*,0!*,:;')?A*u*X.'0@]'V:;?A*,0:;7Y?7Y.?)bU58(1%![]'V')(C0fi(1'0fi(1'WNv?A*sNv?NP*X5eRg'?(CG7,'
58(-W:;')Rg')(*X?2L:X'Vb'V:;53oH-V?A*X530fi(Eqr0:9')?-/.H9:c587&*,0*X?2:X'Vb'V:;53oH-V?A*X530fi(jqr:X0fiR7X-W:/?A*X-;.@:X'Vqn'V:;?A[B28'c:)Z
[]'V*,*,'V:\4'V*VZH-V?(<Q'MC1'Vb')230@D'Wa-V58')(*>5e(-W:X')Rg')(*X?2?23%0:;58*X.RS7tqn0:&)s^0q23')?A:;(58(%0@]'V:;?A*,0:/7/
 28?(:;'V@B?53:c<?7T(10*TC5e7X-VG7X7,')Cj58(j*X.587&@B?A@]'V:c?(C587c?(658Rg@]0:X*X?(*>qG1*XG1:;'MC53:;')-W*X530fi(J9K\.1'
:X')7,')?A:/-;.0q9c'x>?A')C*
?(C  :;G14U(100%fi.1'ms)p/Zi<>.5e-;.G7,')7
-W0fiG(*,'V:;'WU?Rg@H23')7g*,0%fiG5eC1'	*X.1'
:X'VbU587X530fi(0qT*X.'V0:;53')7
7;G1[U,')-W*g*,0nvP/n/Usnn/ZcRS?)4@B:X0=bU58C1'D7,0fiRS'j58C1')?7)JK\.1'V:;'?A:X'
?287,0u@B28?(:X'V@B?53:9Rg'V*X.10UC7c58(D*X.'-V28?7;7X58-V?2L@B28?((5e(1%S2853*,'V:;?A*XG:X'^*X.?A*9RS53%fi.*c[F'M:X')23'VbA?(*c*,0u0fiG1:
?A@@:;0fi?-;.	m0fi7X2e58(^  0fi2e28?-XFZ)1  ')28Cd*,)530fi(5PZ)p/Jkl*<0fiG28C^[F'58(*,'V:X')7,*X58(1%>*,0c-W0fiRg@H?A:X'
*X.1'+*X5eRg'T*,0S:X'V@B?58:i@B28?(7tb'V:;7;G7i*,:X4U58(1%g?(10*X.1'V:c23')?A:;(5e(1%d0@F'V:/?A*,0:>?(C	:X'Vb'V:/53qr4U58(1%1J
$2858R
53*X?A*X530fi(0qt0fiG1:M?A@B@:X0fi?-;.587T*X.?A*M53*^C10')7^(10*M.?(C23'g7,*,0U-;.B?7,*X58-S@H28?(7T0:M@:X0@]'V:X*X53')7
<>53*X.*X5eRg'D2858RS58*X7VZ'J%1J3Z\?x>')7,@]0fi(7,'	@:X0@]'V:X*I4qn0:S<>.B58-;.*X.1':X')7,@]0fi(7,'DRG7,*g0U-V-VG1:
<&53*X.58(?
7,@]')-V5OoB')Cj*X58Rg'g?Aqn*,'V:T*X.1'd*,:;53%%'V:)J  'g<Q0fiGB28C2853'*,0u'WU*,')(C*X.5e7&:;')7,')?A:;-;.*,0Y7X*,0-/.?7,*X58-gU$c7
mKiV')(1%1Z)fipu?(C*X58Rg')CU$&7@:X0@]'V:X*X53')7m$&2eG1:Y958282Z+)1M+?A[B?(1)?UZ)fip/Z9?7u<')282
?7M0*X.1'V:-W0fiR
Rg0fi(?A%')(*d:X'V@:X')7X')(*X?A*X530fi(B7+[]')7X58C1')7^1$&7VJD$c(10*X.1'V:dC53:;')-W*X530fi(6qn0:MqG1*XG1:X'
<Q0:X
<0fiG28C[]'D*,0'W*,')(BC0fiG1:u:X')7XGB23*X7g*,07,4RM[]0fi2858-	Rg0UC1')2&-;.1')-;58(%1Zi<>.58-/.G7,')7
[B58(?A:;4!C1')-V5e7X530fi(
C58?A%:/?RS7+m  c97/p\7,0g*X.?A*\*X.'TqnG2e2F7X*X?A*,'^7,@B?-W'^('V')C	(10*i[]'T'W@B2e58-V53*X234g'WU@B230:X')C	CBG1:;58(1%dRg0UC1')2
-;.')-XU58(1%Sm  G1:;-;.'V*?2J3Z)p/Jks(d7,0fiRg't-V?7,')7VZ7,4URM[F0fi2e58- Rg0UC1')2-;.1')-;5e(1%T-V?(M@B:X0CBG-W'QC1:/?RS?A*X587,@]'V')CG1@J>0<Q'Vb'V:=Z9(0fi(1'j0q^*X.1'E-VG:X:X')(*
:X')7,')?A:/-;.0fi(7,4UR^[]0fi2858-Rg0C')2&-/.1')-XU58(1%?CC1:;')7X7,')7
?C?A@*X58b'^7,4U7,*,')RS7VJ
$cCC53*X580fi(?28234ZQ*X.1'58C1')?7
.1'V:X'?A:X'E?A@@B285e-V?A[B23'Y*,07,0fiRS'D0qT*X.1'U$>NP[B?7,')C-W0fi(*,:X0fi2&*X.1'V0:X4
<0:XHJ10:'W1?Rg@B28'ZAx&?RS?C1%'i?(C  0fi(.?Rms)fip?7X7XGRg'tU$:X'V@B:X')7,')(*X?A*X530fi(7wqr0:[]0*X.*X.1'
@B28?(*Mmn<>.B58-;.j587\?7X7XGBRg')CD*,0u[]'^?
C5e7X-W:X'V*,'WNP'Vb')(*c7,4U7,*,')Rap\?(CD*X.'M7XG1@]'V:XbU587,0:^mn<&.58-;.j-W0fi(*,:;0fi287
*X.1'?-W*X530fi(7a0q+*X.1'E@B28?(*/p/J  '6?A:X'-VG1:X:X')(*X234?A@@B234U58(1%7,0fiRg'E0qT*X.1'j@B:;58(-V53@H23')7d0qT'Wa-V53')(*
:X'Vb'V:;53oH-V?A*X530fi(a*,0-/.?(1%'c*X.1'c7;G1@]'V:Xb5e7,0: 58(
:X')7,@]0fi(7,'\*,0d-;.B?(1%')7i58(S*X.'>@B28?(*Q5e(
?RS?((1'V:Q*X.?A*
@:X')7X'V:Xb')7i@:X0@]'V:X*X58')7+mPc0:;C10fi(E+53:;5e?A58CB587VZAp/J
58(?2e234Z1qnG*XG1:X'T<0:X	7X.10fiG28CYqn0U-VG7\0fi(j7,*XGC14U58(1%g.10<*,0
0@]'V:;?A*X580fi(?2853V'+$c7X58Rg0bH7&y?=<>7\qn0:
58(*,')282853%')(*g?A%')(*X7)J  .?A*a7X0:X*X7g0q9@:X0@]'V:X*X53')7d[]')7,*g'WU@:X')7;7S*X.1')7,'j28?)<>7  ')28C?(C *,)530fi(5
ms)p\@:X0=bU58C1'T7,0fiRS'^58(53*X5e?2F7;G1%%')7,*X530fi(7)Z1[BG1*\RG-;.DRS0:X'+:X')RS?5e(7t*,0S[]'+C10fi(1'J
6fi "BjvBLHfi
K\.5e7+:X')7,')?A:/-;.5877XG1@@]0:X*,')C[4*X.1'	ca-W'a0q&&?=b?2ix>')7,')?A:;-;.mc1W  x&A1Vpu58(-W0fi(UN
XG(-W*X530fi(Y<>58*X.a*X.'D;')RS?(*X58-M0fi(7;587,*,')(-W4Udjcx\kJ1k?R%:;?A*,'VqnGB2*,0  5e282858?R@]')?A:;7VZfi0fi7,'V@B.
c0:;C0fi(ZU*X?(!U?C5e(Zwt.58*,00:U:;58(53bA?7X?(ZLx>?Rg')7X.  .?A:;?C1<t?WZ9?(&0'V4Z?(BC*X.1'
?(0fi(4N
Rg0fiG7>:X'VbU53'V<Q'V:/7>qn0:&G7X'VqnG27XG1%%')7,*X530fi(B7&?(Cj?C1bU58-W'J\K\.1'+@B:X')7,')(*X?A*X530fi(0q*X.'MRS?A*,'V:;5e?2L58(D*X.587
@B?A@]'V:i<t?7\')(10:;RS0fiG7X234u58Rg@:X0b')C	*X.?(1U7i*,0  58282e58?R@]')?A:;7V17XG1%%')7,*X580fi(7VJ

;

fic/3  O    1   fi 
	
iH 
fffi6v"AF""FffP"


 w!1X!"Ar
$K
fi # X {v~
fi #  |  
fiFQ=|~  |   
 %
$ &
' mPp
( mP p
sUn;finn
)

*
2 3 Pm p
4
2 3 6m 5 87 59p
4

HmP p
_l;z :D
n
< mPp
= Nv?G1*,0fiRS?A*,0fi(
? >
fi//A @]rDB >U
fi//A @]PB/S)rv)/
C

;  @D3)vFEHGJI
 )vV  neFEHGJI
@UK
) 3
L NI)s_s
fi///N M!8P
 Os 
R

Q

S UT/B@FsB@1)/
@ lB@ V
Y WU@ 1VZ]
EQr;N_ Y WU@ 1VZ]
@ lB@V

a

` mPp
ba
c

i

bc

 e
2 d
W >	; rK 
;  @D3)vg; rK 

j0C')287^m7X?A*X587,oB')7/p
$b'V:;5OoF-V?A*X530fi(DRg'V*X.10UC	')(*X?582858(%[B:;G1*,'WNPqn0:;-W'+7,')?A:;-/.
$cG1*,0fiRS?A*X?NP*X.1'V0:;'V*X58-MRS0C1')2-/.1')-XU58(1%
U5e(1%fi23'T?A%')(*c7X53*XG?A*X580fi(
EG23*X58?A%')(*>7X58*XG?A*X530fi(Y<>.1'V:;'T')?-;.E?A%')(*cG7,')7\?gRG23*X58?A%')(*\@B2e?(
EG23*X58?A%')(*>7X58*XG?A*X530fi(Y<>.1'V:;'T')?-;.E?A%')(*cG7,')7\?(D5e(C53bU58CG?2B@B28?(
58(58*,'WNv7,*X?A*,'M?G1*,0fiR
?A*,0fi(
K\.'+7,'V*\0q7,*X?A*,')7Mmnb'V:;*X58-W')7/pt0qU$
K\.'+7,'V*\0q7,*X?A*,'WNP*,0ANv7,*X?A*,'d*,:;?(7;53*X530fi(7+mn')C1%')7pt0q1$
y0%fi58-V?2C1')7;-W:;53@*X530fi(u0q*X.1'+7,'V*>0q?-W*X580fi(7i')(?A[B2e58(1%g?g*,:;?(7;53*X530fi(
$  00fi28')?(?23%'V[:/?
 +10 -  +
 00fi23')?(?23%'V[B:;?g@B?A:X*X5e?2]0:;C1'V:), + *.- 53/
K\.'+RS?A*,:;5OY0qL*,:/?(7X53*X530fi(	-W0fi(BC53*X530fi(7t0qU$
KL:;?(7X58*X530fi(	-W0fi(C53*X580fi(	?7X7,0U-V58?A*,')CD<>58*X.Y')C1%'a6m 5 8 7 5!9p
K\.'+7,'V*\0q58(53*X5e?2H7,*X?A*,')7&0q1$
 :;58RS53*X58b'c')28')Rg')(*X7>0qw?  00fi23')?(j?23%'V[:;?U?A*,0fiRS7&?A:X'^?-W*X530fi(B7
')hG1')(-W'^0qw?-W*X530fi(B7Mm?A*,0fiRS7/p
K\.'+28?(1%fiG?A%'+0q\m7,'V*>0qw7,*,:/58(1%fi7\?-V-W'V@*,')C[4ptU$
$c(DU$*X.?A*>?-V-W'V@*X7&58(UoH(B53*,'WNv23')(1%*X.Y7,*,:;5e(1%fi7
K\.'+7,')hG')(-W'+0qU$b'V:;*X58-W')7\bU587X53*,')CY[4Y?S7,*,:;58(%
K\.'T:;G(Y0q?g7,*,:;58(%S58(Y*X.1'+U$ 28?(1%fiG?A%'
$:X')hG53:X')RS')(*t0q?-V-W'V@*X58(1%
:;G(7t0q?(U$
K\.'T*,')(7,0:m7X4(-/.1:X0fi(10fiGB7/pQ@:X0UCG-W*t0qU$&7
@]')-V5OoH')7i?g*,:;?(7X58*X530fi(Yqr0:>'Vb'V:X4Y@F0fi7;7X53[B23'9?-W*X530fi(
K\.'+-;.10fi58-W'^0q?-W*X580fi(G(5ehG1')284SC1'V*,'V:;RS58(')7i*X.1'+(1'WU*>7,*X?A*,'
')hG1')(-W'^0qLb'V:X*X5e-W')7>-W0fi((1')-W*,')Cj[4a')C1%')7
$@B?A*X.	<&53*X.	7,*X?A:X*>?(BC	')(CYb'V:X*X58-W')7&58C1')(*X58-V?2
0fiRS@BG1*X?A*X530fi(?27,*X?A*,'H?(D?-W*X580fi(D0U-V-VG1:X:;58(%d5e(	?g-W0fiRg@BG1*X?A*X530fi(
K\.'V:X'T'WU5e7,*X7\?g@B?A*X.Dqr:X0fiR
KL')Rg@]0:;?2230%fi5e-	,58(bA?A:;58?(*X
KL')Rg@]0:;?2230%fi5e-	s'Vb')(*XG?2e234
QWVX Z5J'J3ZQskl(b?A:/58?(*&(10* X 
Q m X\[ R^] p/Z5J'J3Z,b'V:;4 X 587i'Vb')(*XG?28234uqr0fi2e230=<')C	[4 ] 
K\.'9oB:;7,* X mn*,:/53%%'V:pt587tqn0fi28230<Q')CD[4	? ] mn:X')7,@]0fi(7X'p
K\.'+7,'V*\0qTs[B?CDmn*,0a[F'+?=b0fi58C1')CHpi7,*X?A*,')7\0qU$
t?(58(-W:X')?7,'+?-V-W')7;7X53[B582e53*v4
t?(B(10*>58(-W:;')?7,'+?-V-W')7X7X53[H582853*I4
t?(C1')-W:X')?7,'?-V-W')7X7X53[B5e2853*v4
t?(B(10*>C1')-W:X')?7X'^?-V-W')7X7X53[H582853*I4
U?Aqn'^RS?-/.58(1'+28')?A:;(58(1%d0@]'V:;?A*,0:)ZH5J'J3Z1@:X')7,'V:;b')7\@:X0@]'V:X*X58')7
9('T*X.?A*>587\-W0:;:X')-W*\<>.1')(53*i7,*X?A*,')7>*X.B?A*c  gf
9('T*X.?A*>587\-W0:;:X')-W*\<>.1')(53*i7,*X?A*,')7>*X.B?A*chbgf
K\.'+1$*,:/?(7X53*X530fi(uqG(-W*X530fi(
;kj

fi9H1HH

	
iH 
fffiel	nmcBhi"&v"offIp fi"oiffPB
K\.5e7w?A@B@F')(BC5O]Z<&.58-;.d587[H?7,')Cd0fi(SE?((?T?(C  (G1')2e5]ms)U=p/ZUqn0:;RS?28284^C1'WoH(1')7ks(b?A:;5e?(-W'\?(C
x\')7X@F0fi(B7,'\@:X0@]'V:X*X58')7 58(g*,')RS@F0:/?2H230%fi5e-AJ  'c[]'V%fi58(S[4SC1'WoH(B58(1%T*X.1'&[B?7X58->*,')Rg@]0:;?20@]'V:;?A*,0:&q
mc(*X582np/J  '9?7X7XGBRg'&?M7,*,:;58(1%uKm +r 7ststs p0q-Nv7,*X?A*,')7t0qU$Z<&.1'V:X'>1
 uwv 7yx7{z J K>.1')(Sqn0:-Nv7X*X?A*,'


qn0:;RMGB28?A' X ?(C ] Z1<'^C1'WoH('+&(*X582?|
7 +J9 X q ]~} qn0:>7,0fiRg' z\4x UZ +D   ] Z?(BC	qr0:>'Vb'V:X
4 v
7XG-/.	*X.?A* x u.v z UZ +    X J
ks(bA?A:;58?(B-W'^@:X0@]'V:X*X58')7\?A:X'dC1'WoH(1')Cj58(	*,'V:;RS7&0q  b')(*XG?2e234Y@:X0@]'V:X*X53')7VZH7,0a<'MC'WoH(1'^ b')(UN
*XG?28284doB:;7,*VJ10:\-Nv7,*X?A*,'Tqn0:;RG28? X ?(CaU$ Z<Q'TC1'WoH('&@:;0@F'V:;*v4 f R X mW,b')(*XG?28234 X p
?7T?a@:X0@]'V:X*v4*X.?A*T587>*,:/G1'Ymnq?287,'p>qn0:T?u7X*,:;58(1%u53q53*9587c*,:;G1'	mnqn?287X'p>?A*9*X.1'd58(58*X58?2-Nv7,*X?A*,1
' + r 0q
*X.1'd7,*,:;58(%1J&10:;R
?28234Z]53
q   Km +r 7ststs p&587&?a7X*,:;58(1%a0q U$ ZF*X.1')/
(    R X } +Dr   8 ;H
 q X Z
5J'J3Z9s'Vb')(*XGB?28234 X J$@:;0@F'V:;*v4 f QnVX mWskl(bA?A:;58?(*M(10* X pT587MC1'WoH(1')C?^
7    QnVX }
   V R X ZH5J'J3Zi,(1'Vb'V: X J58(?28284ZH?ux\')7,@]0fi(7,'Mqr0:/RMG2e?
587&0q*X.1'qr0:/R Q m X/[ RF] p/ZF<>.1'V:X' X
587&-V?2823')C	*X.1'Es*,:;53%%'V:;u?(C ] *X.1'js:X')7X@F0fi(B7,'J$x\')7,@]0fi(7,'^qr0:;RG28?
7,*X?A*,')7>*X.B?A*&'Vb'V:X4	*,:/53%%'V:
587t'Vb')(*XG?28234Yqn0fi28230<Q')C	[4Y?g:X')7,@]0fi(7,'J
	
iH 
fffi
"iffPHl"/fiiffIBL
K\.1'Tqn0fi28230<>58(1%oBb'Tks(b?A:;5e?(-W'T@:X0@]'V:X*X53')7t<'V:X'^GB7,')CD58(Y*X.1'T*,')7X*&7XG58*,'
: 0yNP*,:;?(7XR
53*/p,p
Q m V mnkvNvC')2853b'V
: 0yNP@B?G7,'p,p
Q m V mnkvNvC')2853b'V
* 0kvNvC1')2e53b'V:p,p
Q m V mLNv-W0fi2823')-W
* 0kvNvC1')2e53b'VZ
: 0yNP:;')-W')53b'p,p
Q m V mLNv-W0fi2823')-W
: 0kvNP:X')-W')58b
' 0yNP@B?G7,'p,p
Q m V mLNvC1')2853b'V
K\.1'Tqn0fi28230<>58(1%oBb'+x>')7,@]0fi(7,'T@:;0@F'V:;*X53')7t<Q'V:;'^G7,')C	58(Y*X.'+*,')7,*>7XGB53*,' 
Q mNvC')2853b'V: [ R yNP:;')-W')53b'p
Q mNvC')2853b'V: [ R kINP:X')-W')53b'p
Q mNv-W0fi2e23')-W* [ R yNP*,:/?(7XRS53*/p
* 0!kINvC1')2858b'V:p [ R yNP:X')-W')53b'p
Q m,mNv-W0fi2e23')-W

m


v
N

C
)
'
8
2
3
5

b
V
'
:
' 0yNP:X')-W')53b'p,p
Q
[ R mnkINP:X')-W')53b
 UsfiBQUB

$c28G1:)Zx+J3Z9582e2ZJ ms)p/JS$*X.1'V0:;4j0q*X58Rg')C?G1*,0fiRS?A*X?UJ4o1X,V;;  @D>UvVG])VBWZ
 ZL)    J
$c7X58Rg0bHZkJms)Ap/J S{Y MXfiJic:;'V')(<>5e-;.ZFiK~1?)<t-W'V*,*  G1[B2e58-V?A*X530fi(7VZks(-AJ
 ?)b')2ZwJms)fip/J S l>1)PuKU\oU;/	8OI>UP  PAJ x>')7,*,0fi(ZU$^  :X')(*X58-W'WNvc?282J
G -;.B5ZAJms)fip/J1T(+?>C1')-V5e7X530fi(TRg'V*X.10UC+58(+:X')7,*,:;58-W*,')CM7,')-W0fi(CUNP0:/C1'V:L?A:;58*X.Rg'V*X58-AJAks()Kfi;3X
fi ^3W? @S8 OGF)VB  is)// nA+8 OcK UZGHP OW8  S BvVBB; HsW;Z@B@J 
AJ*X?(1qn0:;CZBt
$ F*X?(1qn0:;C	c(53b'V:;7;53*v4  :X')7;7VJ
?

fic/3  O    1   fi 
 G1:;-/.ZB1J3ZHt2e?A:X'Z1\J3Zy0fi(%1ZBJ3Zj-VE582828?(ZUSJ3ZB c5e282ZUMJms)p/Ji4UR^[]0fi2858-cRg0UC1')2]-;.1')-;5e(1%
qn0:+7,')hG1')(*X58?2-V53:;-VG58*&b'V:;53oH-V?A*X530fi(J S
P Bl1Wfi=n1ug  @D>UvV?_6I96{dW 8O
S BvPfisv{ rs >UnnM GHWv  /Z U ZHfi1  1J
( \l=/;{ nA
 G1:XU.?A:;CZ^J1ms) fip/J1y53b')(')7X7L?(BC+q?53:;(1')7X7@:X0@]'V:X*X53')758(^RG23*X5ONv?A%')(*7,47X*,')RS7VJks
 OuK U4on/v/VK S vVBB; n1n
  O)V,VBD
 I9/ Q=n;  S v O VB  S I S  Z
@B@J   AUJt.?RM[F'V:;4ZU:;?(-W'J
t?A:;RS')2ZJ3ZBE?A:X0=bU53*X-;.ZFFJms) fip/Jy')?A:;(B58(1%dRg0C')287 0qw58(*,')282853%')(*?A%')(*X7VJks\
( \l=/;{ nA
 O
K Upon/v/VK /MB;  O)V,VBY
 I9 Q=n  S v! KO VB  IZII S  Z@@J   J
 0:X*X2e?(CZH9x^J
t28?A:;'Z\J3Z  58(%1Z 1Jims) p/J10:;R
?2iRg'V*X.10UC7 u*X?A*,'Y0q&*X.1'u?A:;*d?(BCqG1*XG1:;'aC53:;')-W*X530fi(7VJ
I  @>n
 G>U? T)=Z UK Z     J
0fiG1:/-W0fiG1[]'V*X587VZHcJ3UZ ?A:/C5ZBJ3Z  0fi23@]'V:)Z  J3Z]
  ?((B?A?AU587VZFJLms)fip/J&')Rg0:;4NP'Wa-V58')(*c?23%0AN
:/53*X.RS7Qqr0:t*X.1'9b'V:;53oH-V?A*X530fi(a0q*,')Rg@]0:;?2F@:X0@]'V:X*X58')7VJ E  ; DVK ATn\
 GF=Wv  dW 1Z
 ZH   J
c'x>?A')C*VZy J3Z   :;G14U(100%fi.'ZQJ>ms)p/Jks(*,'V:;?-W*X58b'D*X.1'V0:X4:X'VbU587X530fi(Jks(j58-/.?287,U5Zx+J3Z
 K')-VG-V5PZtJ&mQC7VJ}p/W
Z fi rF
 LXn SJ Z@B@J    1JiU?(j?A*,'V01Z>t
$ 0:;%fi?(
^?G1qnR
?((J
c')?(ZLK+J3Z  ')2e28RS?(ZJ ms)U=p/
J n3Br 4Bl;  JaU?(6E?A*,'V01Zi^
$ j0:X%fi?(+?G1qrN
R
?((J
 2e7,')?58C14Z  J3ZBt28')?)b')28?(CZx^J3Z1  ?G1%fi.ZJ]ms)p/&
J 'V:;53qn4U58(1%^?(Y58(*,')282853%')(*Q7X*,:;G-W*XG1:X'c-W0fi(*,:;0fi2
7X47,*,')
R $-V?7,'7,*XGC4Jkl
( \l=/;{ nA8 O6K U Y X; _w  
 GHv  /
 GH  @UW >  ZT@@J
    AJU?(jfiG?(Z  G1'V:X*,0Sx&58-W01J
53*,0fiG7X7;5ZfiJ3ZK')(B(1')(.10fi23*,ZJBms)fip/JE58(58RS?2U7,0U-V58?2U28?=<>7VJBks
( is); firc8 O>K 1WE  OWv;VBK
MB; n O)V,VBg
 I9 );  S v! KO VBWZ@@J1   UAJ]E?C587,0fi(Z  kJ
10%')2ZJms) fip/JT9(*X.1'^:;')28?A*X530fi(7X.B53@D[]'V*v<'V')(jCBG1:;?A*X530fi(0q?(')(-W0fiGB(*,'V:T?(C*X.1'^'Vb0fi28G*X530fi(
0qi-W00@]'V:;?A*X530fi(58(*X.1'S53*,'V:;?A*,')C@:;587,0fi('V:)7TC5823')R
RS?UJ  T >UB
   @>PfinZ y! Z
  J
c0fi28CBRS?(ZwFJ3Zwx\0fi7,')(B7X-;.1')5e(ZwJ ms)p/JYQRg'V:X%')(*^-W00:;C58(?A*X580fi(6*X.1:X0fiG1%fi.*X.'
G7,'S0qt-W00@1N
'V:/?A*X53b'D7,*X?A*,'WNv-;.B?(1%fi58(1%6:;GB23')7VJEkl
( is); fir8 OuK 1 !  OWK pMnfiB; ^ O)V,VH/Dfi
IT/ Q);  S Bv! K O)B/VZ1@@J1fifi   J')?A*,*X23'Z  $MJ
c0:;C0fi(ZJ ms)fip/J  ')282ONP[]').?)b')C[F0:;%fi7VZ[]0fi230fi7VZ?(C[]'V:;7,'V:X'V:/7VJ+ks
( is)// nAa OSK UEQ O?_
v/VK S v)/BfinH;  OV)s)B/c1
 fi nHP
 LXn  S   Z@B@J   Jj?C5e7,0fi(Z
 kJ
c0:;C0fi(ZMJ ms)fip/Jx\'WNPb'V:;53oH-V?A*X530fi(0q ?C?A@B*X53b'g?A%')(*X7V@H28?(7VJTKL')-;.J:X'V@J3Z]c?)b4t')(*,'V:Tqn0:
$&@@B2e53')CYx\')7,')?A:;-/.j5e(	$>:X*X53oH-V58?2Fkl(*,')282858%')(-W'J
c0:;C0fi(ZMJ3Z+53:;58?AU58C5e7VZ=gJUmAp/J$&C?A@B*X53b'Q7XG@F'V:;b587X0:X4c-W0fi(*,:X0fi20q58(*,'V:;-W0fi((1')-W*,')CdC5e7X-W:X'V*,'
'Vb')(* 7,4U7,*,')RS7)Jks
( is); fir&8 O\K U S v)/BfinH;  OV)s)B/c	
 s; JIn@@O;U
 S WI  Z@@JA   J]$&(B-;.10:;?A%'ZF$&SJ
?

fi9H1HH
c0:;C0fi(ZBJ3ZU@F')?A:/7VZ  J3Z00fi287X4ZJ3ZHy'V'ZBkJLms)fip/J>9587,*,:/53[BG1*,')C	7,@H?A*X58?2-W0fi(*,:X0fi2ZH%fi230[B?2
RS0fi(53*,0:;58(%?(C7,*,'V'V:;58(1%d0qRg0[B5828'&@B.4U7X58-V?2]?A%')(*X7)Jkl(\l=/;{nAO^KU S
PwS v)B_
HnfiB; fi OV)s)B/dfi S  OV  nfi S v OOVH/  fi GHv    S  SS G  Z@@J,U  J
 ?7;.58(1%*,0fi(ZHMJ}9J
c:X'Vqn')(7,*,'V*,*,'ZT1J3Zt >
x ?RS7,'V4Z9cJ9ms)fip/J$c(?A@@:X0fi?-/.*,0?(4*X58RS'E23')?A:/(58(1%1J ks(g\l=/;{_
nM8 O^rBK S B vVBB;~D{"=BB@fi!rH~LXnZH@@J)  )J$&[]'V:;C1'V')(Z
1-W0*X28?(CJ
&')53*XRg'V4'V:)ZcJ3Z+58:X[4Z1J3ZfiyL?A[B?)<^Z  J3Zfi$&:;-;.'V:)ZfiJ3Z  .?A:;?C1<t?VZAx^JUms)fip/Jc7X58(%&?A[B7,*,:/?-W*X530fi(
?(BC6Rg0C')2w-/.1')-XU58(1%	*,0DC'V*,')-W*^7X?Aqn'V*v4Eb530fi2e?A*X530fi(7958(E:X')hG53:X')Rg')(*X797,@]')-V5OoH-V?A*X530fi(B7VJ S
P
BsUW)Udfi/
 GF8 OWN s  rH/VrZ k? Z]   J
&0fi23)RS?((ZTJ3Z  ')28')CZ&J3ZT
  ?(B(?A?AU587VZ9JTms) fip/J9((1')7,*,')C C'V@*X.UNoB:;7X*a7,')?A:;-/.Jks(
\l=/;{ nAg8 O^K U^G];;
 G@Fng
  "{ ?@HZB@@JBU  JFx&G1*,%'V:;7VZB91J
&0fi23)RS?((ZHdJH1Jms)U=p/|
J dOfi   ;  6 fi8 O  @D>UvV
 \lfiP);; }/Jt9,   :X')(*X5e-W'WNv&?282PJ
0fi7;2858(ZJ3Z   0fi2e28?-XFZQJ\ms)p/J!y')?7,*sNv-W0fi7,*gfH?=<:X'V@B?53:a$@B28?(!:X'WoF(1')Rg')(*7,*,:/?A*,'V%4qn0:
@H?A:X*X58?2ONP0:;C'V:a@B2e?((58(1%1Jks(g\l=/;{nA8OjK1 ! OK S vVBB;OV)s)B/6fi
IT/ Q);  S Bv! K O)B/VZ1@@JVA  VfiJL')?A*,*X23'Z  $MJ
+?A[B?()?UZUJms)fip/J 4U(-;.1:;0fi(53)58(1%MRMG28*X58?A%')(*t@B2e?(7 G7;58(1%^*,')RS@F0:/?2H230%fi5e-&7,@]')-V5OoF-V?A*X530fi(7VJks(
\l=/;{ nA	8 O
K UEQnXW S BvVBB; ~ O)V,VB	
 >O))W
 GHWv    S IG  Z
@B@JBU   1JU?(	1:;?(-V5e7X-W01Z]t$MJ
+G1:/7X.?(Zx+Jims)p/
J   @D>UvV
 ITK {   V Q;8 O	= firBfirp
 \l)/VJ  :;5e(-W'V*,0fi(Z
9,   :;58(B-W'V*,0fi(	&(58b'V:;7X53*I4  :X')7X7VJ
y'V'ZJ3Z 9G1:Xqn'V'Z\Jms)p/J9(j'W@B2e58-V53*>@H28?(28?(%fiG?A%')79qr0:9-W00:;C58(?A*X5e(1%aRG23*X58?A%')(*c@B28?(
'WU')-VG1*X530fi(J^ks(p\l=/;{nAa8OgKUE>U/K S vVBBD{"=BB@	I Vo1XW 
ITl rv;)N >U,   1fi >1)  IFI  Z1@@J  )J  :X0b5eC1')(-W'Zx\kJ
E?((?UZwJ3Z  (G1')285Z]$^Jms)U=p/Jg0fiRg@B23'V*X5e(1%
*X.1'*,')Rg@]0:;?2w@B58-W*XG:X'JoUX,V;Z  @D>UvV
G])VBWZ A? ZF  AUJ

E58-;.?2e7,5PZx+JQms)fip/JY$*X.'V0:X46?(C6Rg'V*X.10UC10fi230%40qt58(CG-W*X58b'd28')?A:;(58(1%1Jdks(6j5e-;.?287X5ZLx+J3Z
i?A:X[]0fi(1')282ZU1J3ZBj58*X-;.1')282PZUK+JmQC7VJ}p/Z fi! nH
 ;/Br S Z1@@JU    1J  ?280d$c23*,01ZHt
$ 
K>530%fi?UJ
E53*X-;.1')2e2ZcK+JTms)Afip/J  V;G@Ufi/wI9hI@@]l)!PH/A@];rJ  .JMJi*X.')7X587VZ
U*X?(1qr0:/CD&(B53b'V:;7X53*I4J
c58287X7,0fi(ZUMJLms)Ap/J|irH)@3WM8OI9 Q); S vO VBWJ  ?230S$c23*,01ZFi$^BK>530%fi?UJ
 0*,*,'V:)Z9J+ms)p/JoUdOfi 4I9B eE8O6  @D>UPBZ+8O)?@1VsKT
 T >U1J  .JJ*X.1')7X5e7VZHc'V0:X%'j?7,0fi(j&(B53b'V:;7X53*I4J
x>?R
?C1%'Z  J3Z1  fi0 (.?R	Z  J]ms)fip/JK\.1'c-W0fi(*,:X0fi2F0qCB587X-W:X'V*,'&'Vb')(*i7,4U7,*,')RS7VJ&is)//nA^8O
K 1 S

 Z  ZHU   J
?

fic/3  O    1   fi 
'VA?A:)Zx+J3Zy58(ZoJ NIJ3Z x>?RS?A:;587;.(?(Zw9JQms)p/Jj0C1')2e58(1%	*,')-;.B(58hG1')7+qn0:^'Vb0fi23bU58(1%jC587sN
*,:/53[BG1*,')C?A@@H2858-V?A*X530fi(7)Jks(.is)//nAjOE  ;dV)@FH;!BK>1W  EW Y    Z
@B@JB  J  'V:;('ZF<>53*,V'V:/28?(CJ
U.10fi.B?R	Z dJ3ZBK')(B(1')(.10fi23ZUJms)fip/JQ9(Y7,0-V5e?2F2e?)<>7qn0:t?A:;*X5OoH-V58?2]?A%')(*\7,0-V58'V*X53')7 Q&FNv2858(1'
C')7X53%fi(P
J I9 Q);  S v OOVH/WH
Z  UA _  ZF U  J
U530:/7,5PZUx^Jms) fip/W
J c) 3;
 Ik MVs/J &'V
< 0:;HZB
 F@:;5e(1%'V:,N 'V:;28?A%1J
00fi287X4ZJ3ZFURg0fi28?UZFJLms)p/Jckl(-W:;')Rg')(*X?2wRg0UC1')2-/.1')-XU58(1%u58(D*X.'MRg0UC?2RGUNv-V?28-VGB28G7VJ
ks
( \l);{ rAd O  @D>UvV? _6I96 {   V ;  I   Z1@@,J U  J*X?(qr0:;CZHt$^J
@]')?A:;7VZ  J3ZBc0:;C10fi(ZJms)fip/J c7X58(1%g?A:X*X53oH-V58?2H@B.4U7X58-V7*,0S-W0fi(*,:X0fi2?A%')(*X7VJkl
( \l=/;{ nA
 O+K U S

wS vVBB  O)V,VH/ S  OWfi   HS v! K O)B/   GHv  Z@@J
U  J  ?7X.B58(1%*,0fi(ZBJ}cJ
KL')((1')(.10fi28*,ZJ3Z 0fi7,')7)&Z J\ms)fip/J9(!-W00@]'V:;?A*X580fi(!5e(?ERMGB23*X5ONP')(*X53*v4RS0C1')2PJDkl
( \l _
;{ rAM8 O+K U  3 TVBK S v)/BfinH; o1nZ
  OV)s)B/
 I9 );  S v! K O)B/VZ1@@J
U)   J
KiV')(1%1Z  Jms)fip/J]y')?A:;(5e(1%>@:;0[B?A[B58285e7,*X58-?G1*,0fiR
?A*X?T?(CdRS?A:X0=bd-;.B?58(7bU58?&hG1'V:;58')7VJ fi rF
LXnZ  Z)U   J
?A:;C5PZwJ3Z  0fi23@]'V:)Z  Jtms) fip/J$&(?G1*,0fiRS?A*X?NP*X.1'V0:;'V*X58-	?A@@:X0fi?-/.*,0?G1*,0fiRS?A*X5e-
@:;0%:;?R
b'V:/5OoH-V?A*X530fi(Jks.
( is)// nj8 ODK UEQr;
 I9U >1; GH  @UW >  fi
 XfiEr
   @D>UvV
G])VB   S G  Z@@,J   Jt?RM[:;5eC1%'Zj$MJ
( is)// nAD8 OuK U !  OWK
 ')28CZJ3Z  *,)530fi(5ZJms)p/J6K\.'goB:;7,*28?)< 0qi:X0[]0*X58-V7VJ	ks
MB; n O)V,VBg
 I9 );  S v! KO VBWZ@@J]VA  VA JLU')?A*,*X23'Z  $^J

?

fiJournal of Artificial Intelligence Research 13 (2000) 227-303

Submitted 11/99; published 11/00

Hierarchical Reinforcement Learning with the MAXQ Value
Function Decomposition
Thomas G. Dietterich

Department of Computer Science, Oregon State University
Corvallis, OR 97331

Abstract

tgd@cs.orst.edu

This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs
and decomposing the value function of the target MDP into an additive combination of the
value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics|as a subroutine hierarchy|and a declarative
semantics|as a representation of the value function of a hierarchical policy. MAXQ unifies
and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and
Dayan and Hinton. It is based on the assumption that the programmer can identify useful
subgoals and define subtasks that achieve these subgoals. By defining such subgoals, the
programmer constrains the set of policies that need to be considered during reinforcement
learning. The MAXQ value function decomposition can represent the value function of any
policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can
ignore large parts of the state space. This is important for the practical application of the
method. This paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper
presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges
with probability 1 to a kind of locally-optimal policy known as a recursively optimal policy,
even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ
representation and MAXQ-Q through a series of experiments in three domains and shows
experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal
policy much faster than at Q learning. The fact that MAXQ learns a representation of
the value function has an important benefit: it makes it possible to compute and execute
an improved, non-hierarchical policy via a procedure similar to the policy improvement
step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical
execution experimentally. Finally, the paper concludes with a comparison to related work
and a discussion of the design tradeoffs in hierarchical reinforcement learning.

c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiDietterich

1. Introduction
The area of Reinforcement Learning (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998)
studies methods by which an agent can learn optimal or near-optimal plans by interacting
directly with the external environment. The basic methods in reinforcement learning are
based on the classical dynamic programming algorithms that were developed in the late
1950s (Bellman, 1957; Howard, 1960). However, reinforcement learning methods offer two
important advantages over classical dynamic programming. First, the methods are online.
This permits them to focus their attention on the parts of the state space that are important
and to ignore the rest of the space. Second, the methods can employ function approximation algorithms (e.g., neural networks) to represent their knowledge. This allows them to
generalize across the state space so that the learning time scales much better.
Despite recent advances in reinforcement learning, there are still many shortcomings.
The biggest of these is the lack of a fully satisfactory method for incorporating hierarchies
into reinforcement learning algorithms. Research in classical planning has shown that hierarchical methods such as hierarchical task networks (Currie & Tate, 1991), macro actions
(Fikes, Hart, & Nilsson, 1972; Korf, 1985), and state abstraction methods (Sacerdoti, 1974;
Knoblock, 1990) can provide exponential reductions in the computational cost of finding
good plans. However, all of the basic algorithms for probabilistic planning and reinforcement learning are \at" methods|they treat the state space as one huge at search space.
This means that the paths from the start state to the goal state are very long, and the
length of these paths determines the cost of learning and planning, because information
about future rewards must be propagated backward along these paths.
Many researchers (Singh, 1992; Lin, 1993; Kaelbling, 1993; Dayan & Hinton, 1993;
Hauskrecht, et al., 1998; Parr & Russell, 1998; Sutton, Precup, & Singh, 1998) have experimented with different methods of hierarchical reinforcement learning and hierarchical
probabilistic planning. This research has explored many different points in the design space
of hierarchical methods, but several of these systems were designed for specific situations.
We lack crisp definitions of the main approaches and a clear understanding of the relative
merits of the different methods.
This paper formalizes and clarifies one approach and attempts to understand how it
compares with the other techniques. The approach, called the MAXQ method, provides a
hierarchical decomposition of the given reinforcement learning problem into a set of subproblems. It simultaneously provides a decomposition of the value function for the given
problem into a set of value functions for the subproblems. Hence, it has both a declarative
semantics (as a value function decomposition) and a procedural semantics (as a subroutine
hierarchy).
The decomposition into subproblems has many advantages. First, policies learned in
subproblems can be shared (reused) for multiple parent tasks. Second, the value functions
learned in subproblems can be shared, so when the subproblem is reused in a new task,
learning of the overall value function for the new task is accelerated. Third, if state abstractions can be applied, then the overall value function can be represented compactly as
the sum of separate terms that each depends on only a subset of the state variables. This
more compact representation of the value function will require less data to learn, and hence,
learning will be faster.
228

fiMAXQ Hierarchical Reinforcement Learning

Previous research shows that there are several important design decisions that must
be made when constructing a hierarchical reinforcement learning system. To provide an
overview of the results in this paper, let us review these issues and see how the MAXQ
method approaches each of them.
The first issue is how to specify subtasks. Hierarchical reinforcement learning involves
breaking the target Markov decision problem into a hierarchy of subproblems or subtasks.
There are three general approaches to defining these subtasks. One approach is to define
each subtask in terms of a fixed policy that is provided by the programmer (or that has
been learned in some separate process). The \option" method of Sutton, Precup, and Singh
(1998) takes this approach. The second approach is to define each subtask in terms of a nondeterministic finite-state controller. The Hierarchy of Abstract Machines (HAM) method
of Parr and Russell (1998) takes this approach. This method permits the programmer to
provide a \partial policy" that constrains the set of permitted actions at each point, but
does not specify a complete policy for each subtask. The third approach is to define each
subtask in terms of a termination predicate and a local reward function. These define what
it means for the subtask to be completed and what the final reward should be for completing
the subtask. The MAXQ method described in this paper follows this approach, building
upon previous work by Singh (1992), Kaelbling (1993), Dayan and Hinton (1993), and Dean
and Lin (1995).
An advantage of the \option" and partial policy approaches is that the subtask can
be defined in terms of an amount of effort or a course of action rather than in terms of
achieving a particular goal condition. However, the \option" approach (at least in the
simple form described in this paper), requires the programmer to provide complete policies
for the subtasks, which can be a dicult programming task in real-world problems. On the
other hand, the termination predicate method requires the programmer to guess the relative
desirability of the different states in which the subtask might terminate. This can also be
dicult, although Dean and Lin show how these guesses can be revised automatically by
the learning algorithm.
A potential drawback of all hierarchical methods is that the learned policy may be
suboptimal. The hierarchy constrains the set of possible policies that can be considered. If
these constraints are poorly chosen, the resulting policy will be suboptimal. Nonetheless, the
learning algorithms that have been developed for the \option" and partial policy approaches
guarantee that the learned policy will be the best possible policy consistent with these
constraints.
The termination predicate method suffers from an additional source of suboptimality.
The learning algorithm described in this paper converges to a form of local optimality that
we call recursive optimality. This means that the policy of each subtask is locally optimal
given the policies of its children. But there might exist better hierarchical policies where
the policy for a subtask must be locally suboptimal so that the overall policy is optimal.
For example, a subtask of buying milk might be performed suboptimally (at a more distant
store) because the larger problem also involves buying film (at the same store). This problem
can be avoided by careful definition of termination predicates and local reward functions,
but this is an added burden on the programmer. (It is interesting to note that this problem
of recursive optimality has not been noticed previously. This is because previous work
229

fiDietterich

focused on subtasks with a single terminal state, and in such cases, the problem does not
arise.)
The second design issue is whether to employ state abstractions within subtasks. A
subtask employs state abstraction if it ignores some aspects of the state of the environment.
For example, in many robot navigation problems, choices about what route to take to
reach a goal location are independent of what the robot is currently carrying. With few
exceptions, state abstraction has not been explored previously. We will see that the MAXQ
method creates many opportunities to exploit state abstraction, and that these abstractions
can have a huge impact in accelerating learning. We will also see that there is an important
design tradeoff: the successful use of state abstraction requires that subtasks be defined
in terms of termination predicates rather than using the option or partial policy methods.
This is why the MAXQ method must employ termination predicates, despite the problems
that this can create.
The third design issue concerns the non-hierarchical \execution" of a learned hierarchical policy. Kaelbling (1993) was the first to point out that a value function learned
from a hierarchical policy could be evaluated incrementally to yield a potentially much
better non-hierarchical policy. Dietterich (1998) and Sutton, et al. (1999) generalized this
to show how arbitrary subroutines could be executed non-hierarchically to yield improved
policies. However, in order to support this non-hierarchical execution, extra learning is
required. Ordinarily, in hierarchical reinforcement learning, the only states where learning
is required at the higher levels of the hierarchy are states where one or more of the subroutines could terminate (plus all possible initial states). But to support non-hierarchical
execution, learning is required in all states (and at all levels of the hierarchy). In general,
this requires additional exploration as well as additional computation and memory. As a
consequence of the hierarchical decomposition of the value function, the MAXQ method
is able to support either form of execution, and we will see that there are many problems
where the improvement from non-hierarchical execution is worth the added cost.
The fourth and final issue is what form of learning algorithm to employ. An important advantage of reinforcement learning algorithms is that they typically operate online.
However, finding online algorithms that work for general hierarchical reinforcement learning
has been dicult, particularly within the termination predicate family of methods. Singh's
method relied on each subtask having a unique terminal state; Kaelbling employed a mix of
online and batch algorithms to train her hierarchy; and work within the \options" framework usually assumes that the policies for the subproblems are given and do not need to be
learned at all. The best previous online algorithms are the HAMQ Q learning algorithm of
Parr and Russell (for the partial policy method) and the Feudal Q algorithm of Dayan and
Hinton. Unfortunately, the HAMQ method requires \attening" the hierarchy, and this has
several undesirable consequences. The Feudal Q algorithm is tailored to a specific kind of
problem, and it does not converge to any well-defined optimal policy.
In this paper, we present a general algorithm, called MAXQ-Q, for fully-online learning
of a hierarchical value function. This algorithm enables all subtasks within the hierarchy to
be learned simultaneously and online. We show experimentally and theoretically that the
algorithm converges to a recursively optimal policy. We also show that it is substantially
faster than \at" (i.e., non-hierarchical) Q learning when state abstractions are employed.
230

fiMAXQ Hierarchical Reinforcement Learning

The remainder of this paper is organized as follows. After introducing our notation in
Section 2, we define the MAXQ value function decomposition in Section 3 and illustrate
it with a simple example Markov decision problem. Section 4 presents an analytically
tractable version of the MAXQ-Q learning algorithm called the MAXQ-0 algorithm and
proves its convergence to a recursively optimal policy. It then shows how to extend MAXQ0 to produce the MAXQ-Q algorithm, and shows how to extend the theorem similarly.
Section 5 takes up the issue of state abstraction and formalizes a series of five conditions
under which state abstractions can be safely incorporated into the MAXQ representation.
State abstraction can give rise to a hierarchical credit assignment problem, and the paper
briey discusses one solution to this problem. Finally, Section 7 presents experiments with
three example domains. These experiments give some idea of the generality of the MAXQ
representation. They also provide results on the relative importance of temporal and state
abstractions and on the importance of non-hierarchical execution. The paper concludes with
further discussion of the design issues that were briey described above, and in particular, it
addresses the tradeoff between the method of defining subtasks (via termination predicates)
and the ability to exploit state abstractions.
Some readers may be disappointed that MAXQ provides no way of learning the structure of the hierarchy. Our philosophy in developing MAXQ (which we share with other
reinforcement learning researchers, notably Parr and Russell) has been to draw inspiration
from the development of Belief Networks (Pearl, 1988). Belief networks were first introduced
as a formalism in which the knowledge engineer would describe the structure of the networks and domain experts would provide the necessary probability estimates. Subsequently,
methods were developed for learning the probability values directly from observational data.
Most recently, several methods have been developed for learning the structure of the belief
networks from data, so that the dependence on the knowledge engineer is reduced.
In this paper, we will likewise require that the programmer provide the structure of the
hierarchy. The programmer will also need to make several important design decisions. We
will see below that a MAXQ representation is very much like a computer program, and
we will rely on the programmer to design each of the modules and indicate the permissible
ways in which the modules can invoke each other. Our learning algorithms will fill in
\implementations" of each module in such a way that the overall program will work well.
We believe that this approach will provide a practical tool for solving large real-world
MDPs. We also believe that it will help us understand the structure of hierarchical learning
algorithms. It is our hope that subsequent research will be able to automate most of the
work that we are currently requiring the programmer to do.

2. Formal Definitions
We begin by introducing definitions for Markov Decision Problems and Semi-Markov Decision Problems.

2.1 Markov Decision Problems
We employ the standard definition for Markov Decision Problems (also known as Markov
decision processes). In this paper, we restrict our attention to situations in which an agent
231

fiDietterich

is interacting with a fully-observable stochastic environment. This situation can be modeled
as a Markov Decision Problem (MDP) hS; A; P; R; P0 i defined as follows:
 S : the finite set of states of the environment. At each point in time, the agent can
observe the complete state of the environment.
 A: a finite set of actions. Technically, the set of available actions depends on the
current state s, but we will suppress this dependence in our notation.
 P : When an action a 2 A is performed, the environment makes a probabilistic transition from its current state s to a resulting state s0 according to the probability
distribution P (s0 js; a).
 R: Similarly, when action a is performed and the environment makes its transition
from s to s0 , the agent receives a real-valued (possibly stochastic) reward r whose
expected value is R(s0 js; a). To simplify the notation, it is customary to treat this
reward as being given at the time that action a is initiated, even though it may in
general depend on s0 as well as on s and a.
 P0 : The starting state distribution. When the MDP is initialized, it is in state s with
probability P0 (s).
A policy, , is a mapping from states to actions that tells what action a = (s) to perform
when the environment is in state s.
We will consider two settings: episodic and infinite-horizon.
In the episodic setting, all rewards are finite and there is at least one zero-cost absorbing
terminal state. An absorbing terminal state is a state in which all actions lead back to the
same state with probability 1 and zero reward. For technical reasons, we will only consider
problems where all deterministic policies are \proper"|that is, all deterministic policies
have a non-zero probability of reaching a terminal state when started in an arbitrary state.
(We believe this condition can be relaxed, but we have not verified this formally.) In the
episodic setting, the goal of the agent is to find a policy that maximizes the expected
cumulative reward. In the special case where all rewards are non-positive, these problems
are referred to as stochastic shortest path problems, because the rewards can be viewed as
costs (i.e., lengths), and the policy attempts to move the agent along the path of minimum
expected cost.
In the infinite horizon setting, all rewards are also finite. In addition, there is a discount
factor  , and the agent's goal is to find a policy that minimizes the infinite discounted sum
of future rewards.
The value function V  for policy  is a function that tells, for each state s, what the
expected cumulative reward will be of executing policy  starting in state s. Let rt be a
random variable that tells the reward that the agent receives at time step t while following
policy . We can define the value function in the episodic setting as
V  (s) = E frt + rt+1 + rt+2 +    jst = s; g :
In the discounted setting, the value function is
fi

n

o

V  (s) = E rt + rt+1 +  2 rt+2 +   fifi st = s;  :
232

fiMAXQ Hierarchical Reinforcement Learning

We can see that this equation reduces to the previous one when  = 1. However, in infinitehorizon MDPs this sum may not converge when  = 1.
The value function satisfies the Bellman equation for a fixed policy:

V  (s) =

X

s0

P (s0 js; (s)) R(s0 js; (s)) + V  (s0 ) :




The quantity on the right-hand side is called the backed-up value of performing action a in
state s. For each possible successor state s0 , it computes the reward that would be received
and the value of the resulting state and then weights those according to the probability of
ending up in s0 .
The optimal value function V  is the value function that simultaneously maximizes the
expected cumulative reward in all states s 2 S . Bellman (1957) proved that it is the unique
solution to what is now known as the Bellman equation:

V  (s) = max
a

X

s0

P (s0 js; a) R(s0 js; a) + V  (s0 ) :




(1)

There may be many optimal policies that achieve this value. Any policy that chooses a
in s to achieve the maximum on the right-hand side of this equation is an optimal policy.
We will denote an optimal policy by  . Note that all optimal policies are \greedy" with
respect to the backed-up value of the available actions.
Closely related to the value function is the so-called action-value function, or Q function
(Watkins, 1989). This function, Q (s; a), gives the expected cumulative reward of performing action a in state s and then following policy  thereafter. The Q function also satisfies
a Bellman equation:

Q (s; a) =

X

s0

P (s0 js; a) R(s0 js; a) + Q (s0 ; (s0 )) :




The optimal action-value function is written Q (s; a), and it satisfies the equation
X
Q (s; a) = P (s0 js; a)

s0





R(s0 js; a) +  max Q(s0 ; a0 )
a0

:

(2)

Note that any policy that is greedy with respect to Q is an optimal policy. There may be
many such optimal policies|they differ only in how they break ties between actions with
identical Q values.
An action order, denoted !, is a total order over the actions within an MDP. That is, !
is an anti-symmetric, transitive relation such that !(a1 ; a2 ) is true iff a1 is strictly preferred
to a2 . An ordered greedy policy, ! is a greedy policy that breaks ties using !. For example,
suppose that the two best actions at state s are a1 and a2 , that Q(s; a1 ) = Q(s; a2 ), and
that !(a1 ; a2 ). Then the ordered greedy policy ! will choose a1 : ! (s) = a1 . Note that
although there may be many optimal policies for a given MDP, the ordered greedy policy,
! , is unique.
233

fiDietterich

2.2 Semi-Markov Decision Processes

In order to introduce and prove some of the properties of the MAXQ decomposition, we
need to consider a simple generalization of MDPs|the semi-Markov decision process.
A discrete-time semi-Markov Decision Process (SMDP) is a generalization of the Markov
Decision Process in which the actions can take a variable amount of time to complete. In
particular, let the random variable N denote the number of time steps that action a takes
when it is executed in state s. We can extend the state transition probability function to
be the joint distribution of the result states s0 and the number of time steps N when action
a is performed in state s: P (s0 ; N js; a). Similarly, the expected reward can be changed to
be R(s0 ; N js; a).1
It is straightforward to modify the Bellman equation to define the value function for a
fixed policy  as
h
i
X
V  (s) = P (s0; N js; (s)) R(s0 ; N js; (s)) +  N V  (s0 ) :
s0 ;N

The only change is that the expected value on the right-hand side is taken with respect to
both s0 and N , and  is raised to the power N to reect the variable amount of time that
may elapse while executing action a.
Note that because expectation is a linear operator, we can write each of these Bellman
equations as the sum of the expected reward for performing action a and the expected value
of the resulting state s0 . For example, we can rewrite the equation above as
X
(3)
V  (s) = R(s; (s)) + P (s0 ; N js; (s)) N V  (s0 ):
s0 ;N

where R(s; (s)) is the expected reward of performing action (s) in state s, and the expectation is taken with respect to s0 and N .
All of the results given in this paper can be generalized to apply to discrete-time semiMarkov Decision Processes. A consequence of this is that whenever this paper talks of
executing a primitive action, it could just as easily talk of executing a hand-coded openloop \subroutine". These subroutines would not be learned, and nor could their execution
be interrupted as discussed below in Section 6. But in many applications (e.g., robot
control with limited sensors), open-loop controllers can be very useful (e.g., to hide partialobservability). For an example, see Kalmar, Szepesvari, and A. Lorincz (1998).
Note that for the episodic case, there is no difference between a MDP and a Semi-Markov
Decision Process, because the discount factor  is 1, and therefore neither the optimal policy
nor the optimal value function depend on the amount of time each action takes.

2.3 Reinforcement Learning Algorithms

A reinforcement learning algorithm is an algorithm that tries to construct an optimal policy
for an unknown MDP. The algorithm is given access to the unknown MDP via the following
1. This formalization is slightly different from the standard formulation of SMDPs, which separates
P (s0js; a) and F (tjs; a), where F is the cumulative distribution function for the probability that a will
terminate in t time units, and t is real-valued rather than integer-valued. In our case, it is important
to consider the joint distribution of s0 and N , but we do not need to consider actions with arbitrary
real-valued durations.

234

fiMAXQ Hierarchical Reinforcement Learning

reinforcement learning protocol. At each time step t, the algorithm is told the current state
s of the MDP and the set of actions A(s)  A that are executable in that state. The
algorithm chooses an action a 2 A(s), and the MDP executes this action (which causes it to
move to state s') and returns a real-valued reward r. If s is an absorbing terminal state, the
set of actions A(s) contains only the special action reset, which causes the MDP to move
to one of its initial states, drawn according to P0 .
In this paper, we will make use of two well-known learning algorithms: Q learning
(Watkins, 1989; Watkins & Dayan, 1992) and SARSA(0) (Rummery & Niranjan, 1994). We
will apply these algorithms to the case where the action value function Q(s; a) is represented
as a table with one entry for each pair of state and action. Every entry of the table is
initialized arbitrarily.
In Q learning, after the algorithm has observed s, chosen a, received r, and observed s0 ,
it performs the following update:

Qt (s; a) := (1 , fft )Qt,1 (s; a) + fft [r +  max
Q (s0 ; a0 )];
a0 t,1
where fft is a learning rate parameter.
Jaakkola, Jordan and Singh (1994) and Bertsekas and Tsitsiklis (1996) prove that if the
agent follows an \exploration policy" that tries every action in every state infinitely often
and if
T
T
X
X
lim
ff
=
1
and
lim
ff2t < 1
(4)
t
T !1
T!1
t=1

t=1

then Qt converges to the optimal action-value function Q with probability 1. Their proof
holds in both settings discussed in this paper (episodic and infinite-horizon).
The SARSA(0) algorithm is very similar. After observing s, choosing a, observing r,
observing s0 , and choosing a0 , the algorithm performs the following update:

Qt (s; a) := (1 , fft )Qt,1 (s; a) + fft [r + Qt,1 (s0 ; a0 )];
where fft is a learning rate parameter. The key difference is that the Q value of the chosen
action a0 , Q(s0 ; a0 ), appears on the right-hand side in the place where Q learning uses the
Q value of the best action. Singh, et al. (1998) provide two important convergence results:
First, if a fixed policy  is employed to choose actions, SARSA(0) will converge to the
value function of that policy provided fft decreases according to Equations (4). Second, if a
so-called GLIE policy is employed to choose actions, SARSA(0) will converge to the value
function of the optimal policy, provided again that fft decreases according to Equations (4).
A GLIE policy is defined as follows:

Definition 1 A GLIE (Greedy in the Limit with Infinite Exploration) policy is any policy

satisfying

1. Each action is executed infinitely often in every state that is visited infinitely often.
2. In the limit, the policy is greedy with respect to the Q-value function with probability
1.
235

fiDietterich

4

R

G

3
2
1
0 Y
0

B
1

2

3

4

Figure 1: The Taxi Domain.

3. The MAXQ Value Function Decomposition
At the center of the MAXQ method for hierarchical reinforcement learning is the MAXQ
value function decomposition. MAXQ describes how to decompose the overall value function
for a policy into a collection of value functions for individual subtasks (and subsubtasks,
recursively).

3.1 A Motivating Example

To make the discussion concrete, let us consider the following simple example. Figure 1
shows a 5-by-5 grid world inhabited by a taxi agent. There are four specially-designated
locations in this world, marked as R(ed), B(lue), G(reen), and Y(ellow). The taxi problem
is episodic. In each episode, the taxi starts in a randomly-chosen square. There is a
passenger at one of the four locations (chosen randomly), and that passenger wishes to be
transported to one of the four locations (also chosen randomly). The taxi must go to the
passenger's location (the \source"), pick up the passenger, go to the destination location
(the \destination"), and put down the passenger there. (To keep things uniform, the taxi
must pick up and drop off the passenger even if he/she is already located at the destination!)
The episode ends when the passenger is deposited at the destination location.
There are six primitive actions in this domain: (a) four navigation actions that move the
taxi one square North, South, East, or West, (b) a Pickup action, and (c) a Putdown action.
There is a reward of ,1 for each action and an additional reward of +20 for successfully
delivering the passenger. There is a reward of ,10 if the taxi attempts to execute the
Putdown or Pickup actions illegally. If a navigation action would cause the taxi to hit a
wall, the action is a no-op, and there is only the usual reward of ,1.
To simplify the examples throughout this section, we will make the six primitive actions deterministic. Later, we will make the actions stochastic in order to create a greater
challenge for our learning algorithms.
We seek a policy that maximizes the total reward per episode. There are 500 possible
states: 25 squares, 5 locations for the passenger (counting the four starting locations and
the taxi), and 4 destinations.
This task has a simple hierarchical structure in which there are two main sub-tasks:
Get the passenger and Deliver the passenger. Each of these subtasks in turn involves the
236

fiMAXQ Hierarchical Reinforcement Learning

subtask of navigating to one of the four locations and then performing a Pickup or Putdown
action.
This task illustrates the need to support temporal abstraction, state abstraction, and
subtask sharing. The temporal abstraction is obvious|for example, the process of navigating to the passenger's location and picking up the passenger is a temporally extended
action that can take different numbers of steps to complete depending on the distance to
the target. The top level policy (get passenger; deliver passenger) can be expressed very
simply if these temporal abstractions can be employed.
The need for state abstraction is perhaps less obvious. Consider the subtask of getting
the passenger. While this subtask is being solved, the destination of the passenger is
completely irrelevant|it cannot affect any of the nagivation or pickup decisions. Perhaps
more importantly, when navigating to a target location (either the source or destination
location of the passenger), only the target location is important. The fact that in some
cases the taxi is carrying the passenger and in other cases it is not is irrelevant.
Finally, support for subtask sharing is critical. If the system could learn how to solve the
navigation subtask once, then the solution could be shared by both the \Get the passenger"
and \Deliver the passenger" subtasks. We will show below that the MAXQ method provides
a value function representation and learning algorithm that supports temporal abstraction,
state abstraction, and subtask sharing.
To construct a MAXQ decomposition for the taxi problem, we must identify a set of
individual subtasks that we believe will be important for solving the overall task. In this
case, let us define the following four tasks:
 Navigate(t). In this subtask, the goal is to move the taxi from its current location to
one of the four target locations, which will be indicated by the formal parameter t.
 Get. In this subtask, the goal is to move the taxi from its current location to the
passenger's current location and pick up the passenger.
 Put. The goal of this subtask is to move the taxi from the current location to the
passenger's destination location and drop off the passenger.
 Root. This is the whole taxi task.
Each of these subtasks is defined by a subgoal, and each subtask terminates when the
subgoal is achieved.
After defining these subtasks, we must indicate for each subtask which other subtasks or
primitive actions it should employ to reach its goal. For example, the Navigate(t) subtask
should use the four primitive actions North, South, East, and West. The Get subtask should
use the Navigate subtask and the Pickup primitive action, and so on.
All of this information can be summarized by a directed acyclic graph called the task
graph, which is shown in Figure 2. In this graph, each node corresponds to a subtask or a
primitive action, and each edge corresponds to a potential way in which one subtask can
\call" one of its child tasks. The notation formal=actual (e.g., t=source) tells how a formal
parameter is to be bound to an actual parameter.
Now suppose that for each of these subtasks, we write a policy (e.g., as a computer
program) to achieve the subtask. We will refer to the policy for a subtask as a \subroutine", and we can view the parent subroutine as invoking the child subroutine via ordinary
237

fiDietterich

Root

Get

Put
t/source

Pickup

t/destination

Navigate(t)

North

South

East

Putdown

West

Figure 2: A task graph for the Taxi problem.
subroutine-call-and-return semantics. If we have a policy for each subtask, then this gives
us an overall policy for the Taxi MDP. The Root subtask executes its policy by calling
subroutines that are policies for the Get and Put subtasks. The Get policy calls subroutines
for the Navigate(t) subtask and the Pickup primitive action. And so on. We will call this
collection of policies a hierarchical policy. In a hierarchical policy, each subroutine executes
until it enters a terminal state for its subtask.

3.2 Definitions

Let us formalize the discussion so far.
The MAXQ decomposition takes a given MDP M and decomposes it into a finite set of
subtasks fM0 ; M1 ; : : : ; Mn g with the convention that M0 is the root subtask (i.e., solving
M0 solves the entire original MDP M ).
Definition 2 An unparameterized subtask is a three-tuple, hTi; Ai ; R~i i, defined as follows:
1. Ti is a termination predicate that partitions S into a set of active states, Si , and a set
of terminal states, Ti : The policy for subtask Mi can only be executed if the current
state s is in Si . If, at any time that subtask Mi is being executed, the MDP enters a
state in Ti , then Mi terminates immediately (even if it is still executing a subtask, see
below).
2. Ai is a set of actions that can be performed to achieve subtask Mi . These actions can
either be primitive actions from A, the set of primitive actions for the MDP, or they
can be other subtasks, which we will denote by their indexes i. We will refer to these
actions as the \children" of subtask i. The sets Ai define a directed graph over the
subtasks M0 ; : : : ; Mn , and this graph may not contain any cycles. Stated another way,
no subtask can invoke itself recursively either directly or indirectly.
If a child subtask Mj has formal parameters, then this is interpreted as if the subtask
occurred multiple times in Ai , with one occurrence for each possible tuple of actual
238

fiMAXQ Hierarchical Reinforcement Learning

values that could be bound to the formal parameters. The set of actions Ai may differ
from one state to another and from one set of actual parameter values to another, so
technically, Ai is a function of s and the actual parameters. However, we will suppress
this dependence in our notation.
3. R~ i (s0 ) is the pseudo-reward function, which specifies a (deterministic) pseudo-reward
for each transition to a terminal state s0 2 Ti . This pseudo-reward tells how desirable
each of the terminal states is for this subtask. It is typically employed to give goal
terminal states a pseudo-reward of 0 and any non-goal terminal states a negative
reward. By definition, the pseudo-reward R~ i (s) is also zero for all non-terminal states
s. The pseudo-reward is only used during learning, so it will not be mentioned further
until Section 4.
Each primitive action a from M is a primitive subtask in the MAXQ decomposition
such that a is always executable, it always terminates immediately after execution, and its
pseudo-reward function is uniformly zero.

If a subtask has formal parameters, then each possible binding of actual values to the
formal parameters specifies a distinct subtask. We can think of the values of the formal
parameters as being part of the \name" of the subtask. In practice, of course, we implement
a parameterized subtask by parameterizing the various components of the task. If b specifies
the actual parameter values for task Mi , then we can define a parameterized termination
predicate Ti (s; b) and a parameterized pseudo-reward function R~ i (s0 ; b). To simplify notation
in the rest of the paper, we will usually omit these parameter bindings. However, it should
be noted that if a parameter of a subtask takes on a large number of possible values, this
is equivalent to creating a large number of different subtasks, each of which will need to be
learned. It will also create a large number of candidate actions for the parent task, which
will make the learning problem more dicult for the parent task as well.

Definition 3 A hierarchical policy, , is a set containing a policy for each of the subtasks
in the problem:  = f0 ; : : : ; n g:
Each subtask policy i takes a state and returns the name of a primitive action to
execute or the name of a subroutine (and bindings for its formal parameters) to invoke. In
the terminology of Sutton, Precup, and Singh (1998), a subtask policy is a deterministic
\option", and its probability of terminating in state s (which they denote by fi (s)) is 0 if
s 2 Si , and 1 if s 2 Ti .
In a parameterized task, the policy must be parameterized as well so that  takes a
state and the bindings of formal parameters and returns a chosen action and the bindings
(if any) of its formal parameters.
Table 1 gives a pseudo-code description of the procedure for executing a hierarchical
policy. The hierarchical policy is executed using a stack discipline, similar to ordinary
programming languages. Let Kt denote the contents of the pushdown stack at time t.
When a subroutine is invoked, its name and actual parameters are pushed onto the stack.
When a subroutine terminates, its name and actual parameters are popped off the stack.
Notice (line 16) that if any subroutine on the stack terminates, then all subroutines below
239

fiDietterich

Table 1: Pseudo-Code for Execution of a Hierarchical Policy.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

Procedure ExecuteHierarchicalPolicy()

st is the state of the world at time t
Kt is the state of the execution stack at time t
Let t = 0; Kt = empty stack; observe st
push (0; nil) onto stack Kt (invoke the root task with no parameters)

repeat
while top(Kt ) is not a primitive action

Let (i; fi ) := top(Kt), where
i is the name of the \current" subroutine, and
fi gives the parameter bindings for i
Let (a; fa ) := i (s; fi ), where
a is the action and fa gives the parameter bindings chosen by policy i
push (a; fa ) onto the stack Kt
end // while
Let (a; nil) := pop(Kt) be the primitive action on the top of the stack.
Execute primitive action a, observe st+1, and receive reward R(st+1jst ; a)
If any subtask on Kt is terminated in st+1 then
Let M 0 be the terminated subtask that is highest (closest to the root) on the stack.
while top(Kt) 6= M 0 do pop(Kt)
pop(Kt)
Kt+1 := Kt is the resulting execution stack.
until Kt+1 is empty
end ExecuteHierarchicalPolicy

it are immediately aborted, and control returns to the subroutine that had invoked the
terminated subroutine.
It is sometimes useful to think of the contents of the stack as being an additional part of
the state space for the problem. Hence, a hierarchical policy implicitly defines a mapping
from the current state st and current stack contents Kt to a primitive action a. This action is
executed, and this yields a resulting state st+1 and a resulting stack contents Kt+1 . Because
of the added state information in the stack, the hierarchical policy is non-Markovian with
respect to the original MDP.
Because a hierarchical policy maps from states s and stack contents K to actions, the
value function for a hierarchical policy must assign values to combinations of states s and
stack contents K .

Definition 4 A hierarchical value function, denoted V  (hs; K i), gives the expected cumu-

lative reward of following the hierarchical policy  starting in state s with stack contents
K.

This hierarchical value function is exactly what is learned by Ron Parr's (1998b) HAMQ
algorithm, which we will discuss below. However, in this paper, we will focus on learning
only the projected value functions of each of the subtasks M0 ; : : : ; Mn in the hierarchy.
240

fiMAXQ Hierarchical Reinforcement Learning

Definition 5 The projected value function of hierarchical policy  on subtask Mi, denoted
V  (i; s), is the expected cumulative reward of executing i (and the policies of all descendents
of Mi ) starting in state s until Mi terminates.
The purpose of the MAXQ value function decomposition is to decompose V (0; s) (the
projected value function of the root task) in terms of the projected value functions V (i; s)
of all of the subtasks in the MAXQ decomposition.

3.3 Decomposition of the Projected Value Function

Now that we have defined a hierarchical policy and its projected value function, we can show
how that value function can be decomposed hierarchically. The decomposition is based on
the following theorem:

Theorem 1 Given a task graph over tasks M0 ; : : : ; Mn and a hierarchical policy , each
subtask Mi defines a semi-Markov decision process with states Si , actions Ai , probability
transition function Pi (s0 ; N js; a), and expected reward function R(s; a) = V  (a; s), where
V  (a; s) is the projected value function for child task Ma in state s. If a is a primitive
action,
V  (a; s) is defined as the expected immediate reward of executing a in s: V  (a; s) =
P
0
0
s0 P (s js; a)R(s js; a).
Proof: Consider all of the subroutines that are descendents of task Mi in the task graph.

Because all of these subroutines are executing fixed policies (specified by hierarchical policy
), the probability transition function Pi (s0 ; N js; a) is a well defined, stationary distribution
for each child subroutine a. The set of states Si and the set of actions Ai are obvious. The
interesting part of this theorem is the fact that the expected reward function R(s; a) of the
SMDP is the projected value function of the child task Ma .
To see this, let us write out the value of V  (i; s):

V  (i; s) = E frt + rt+1 +  2 rt+2 +    jst = s; g
(5)
This sum continues until the subroutine for task Mi enters a state in Ti .
Now let us suppose that the first action chosen by i is a subroutine a. This subroutine
is invoked, and it executes for a number of steps N and terminates in state s0 according to
Pi (s0 ; N js; a). We can rewrite Equation (5) as
V  (i; s) = E

(

NX
,1
u=0

 u rt+u

+

1
X

u=N

fi
fi
fi
t+u fifi

ur

st = s; 

)

(6)

The first summation on the right-hand side of Equation (6) is the discounted sum of rewards
for executing subroutine a starting in state s until it terminates, in other words, it is V  (a; s),
the projected value function for the child task Ma . The second term on the right-hand side
of the equation is the value of s0 for the current task i, V  (i; s0 ), discounted by  N , where
s0 is the current state when subroutine a terminates. We can write this in the form of a
Bellman equation:
X
(7)
V  (i; s) = V  (i (s); s) + Pi (s0 ; N js; i (s)) N V  (i; s0 )
s0 ;N

241

fiDietterich

This has the same form as Equation (3), which is the Bellman equation for an SMDP, where
the first term is the expected reward R(s; (s)). Q.E.D.
To obtain a hierarchical decomposition of the projected value function, let us switch
to the action-value (or Q) representation. First, we need to extend the Q notation to
handle the task hierarchy. Let Q (i; s; a) be the expected cumulative reward for subtask
Mi of performing action a in state s and then following hierarchical policy  until subtask
Mi terminates. Action a may be either a primitive action or a child subtask. With this
notation, we can re-state Equation (7) as follows:

Q (i; s; a) = V  (a; s) +

X

s0 ;N

Pi (s0 ; N js; a) N Q (i; s0 ; (s0 ));

(8)

The right-most term in this equation is the expected discounted reward of completing task

Mi after executing action a in state s. This term only depends on i, s, and a, because the
summation marginalizes away the dependence on s0 and N . Let us define C  (i; s; a) to be
equal to this term:

Definition 6 The completion function, C  (i; s; a), is the expected discounted cumulative

reward of completing subtask Mi after invoking the subroutine for subtask Ma in state s.
The reward is discounted back to the point in time where a begins execution.

C  (i; s; a) =

X

s0 ;N

Pi (s0; N js; a) N Q (i; s0 ; (s0 ))

(9)

With this definition, we can express the Q function recursively as

Q (i; s; a) = V  (a; s) + C  (i; s; a):

(10)

Finally, we can re-express the definition for V  (i; s) as

V  (i; s) =

(

 (i; s; i (s))
Q
if i is composite
P
0
0
s0 P (s js; i)R(s js; i) if i is primitive

(11)

We will refer to equations (9), (10), and (11) as the decomposition equations for the
MAXQ hierarchy under a fixed hierarchical policy . These equations recursively decompose
the projected value function for the root, V  (0; s) into the projected value functions for
the individual subtasks, M1 ; : : : ; Mn and the individual completion functions C  (j; s; a)
for j = 1; : : : ; n. The fundamental quantities that must be stored to represent the value
function decomposition are just the C values for all non-primitive subtasks and the V values
for all primitive actions.
To make it easier for programmers to design and debug MAXQ decompositions, we have
developed a graphical representation that we call the MAXQ graph. A MAXQ graph for the
Taxi domain is shown in Figure 3. The graph contains two kinds of nodes, Max nodes and
Q nodes. The Max nodes correspond to the subtasks in the task decomposition|there is
one Max node for each primitive action and one Max node for each subtask (including the
Root) task. Each primitive Max node i stores the value of V  (i; s). The Q nodes correspond
to the actions that are available for each subtask. Each Q node for parent task i, state s
242

fiMAXQ Hierarchical Reinforcement Learning

MaxRoot

QPickup

QGet

QPut

MaxGet

MaxPut

QNavigateForPut

QNavigateForGet

t/source

QPutdown

t/destination

Pickup

Putdown

MaxNavigate(t)

QNorth(t)

QEast(t)

QSouth(t)

QWest(t)

North

East

South

West

Figure 3: A MAXQ graph for the Taxi Domain.
and subtask a stores the value of C  (i; s; a). The children of any node are unordered|that
is, the order in which they are drawn in Figure 3 does not imply anything about the order in
which they will be executed. Indeed, a child action may be executed multiple times before
its parent subtask is completed.
In addition to storing information, the Max nodes and Q nodes can be viewed as performing parts of the computation described by the decomposition equations. Specifically,
each Max node i can be viewed as computing the projected value function V  (i; s) for its
subtask. For primitive Max nodes, this information is stored in the node. For composite
Max nodes, this information is obtained by \asking" the Q node corresponding to i (s).
Each Q node with parent task i and child task a can be viewed as computing the value of
Q (i; s; a). It does this by \asking" its child task a for its projected value function V  (a; s)
and then adding its completion function C  (i; s; a).
243

fiDietterich

As an example, consider the situation shown in Figure 1, which we will denote by s1 .
Suppose that the passenger is at R and wishes to go to B. Let the hierarchical policy we
are evaluating be an optimal policy denoted by  (we will omit the superscript * to reduce
the clutter of the notation). The value of this state under  is 10, because it will cost 1
unit to move the taxi to R, 1 unit to pickup the passenger, 7 units to move the taxi to B,
and 1 unit to putdown the passenger, for a total of 10 units (a reward of ,10). When the
passenger is delivered, the agent gets a reward of +20, so the net value is +10.
Figure 4 shows how the MAXQ hierarchy computes this value. To compute the value
V  (Root; s1 ), MaxRoot consults its policy and finds that Root (s1) is Get. Hence, it \asks"
the Q node, QGet to compute Q (Root; s1 ; Get). The completion cost for the Root task
after performing a Get, C  (Root; s1 ; Get), is 12, because it will cost 8 units to deliver the
customer (for a net reward of 20 , 8 = 12) after completing the Get subtask. However, this
is just the reward after completing the Get, so it must ask MaxGet to estimate the expected
reward of performing the Get itself.
The policy for MaxGet dictates that in s1 , the Navigate subroutine should be invoked with
t bound to R, so MaxGet consults the Q node, QNavigateForGet to compute the expected
reward. QNavigateForGet knows that after completing the Navigate(R) task, one more action
(the Pickup) will be required to complete the Get, so C  (MaxGet; s1 ; Navigate(R)) = ,1.
It then asks MaxNavigate(R) to compute the expected reward of performing a Navigate to
location R.
The policy for MaxNavigate chooses the North action, so MaxNavigate asks QNorth to
compute the value. QNorth looks up its completion cost, and finds that C  (Navigate; s1 ; North)
is 0 (i.e., the Navigate task will be completed after performing the North action). It consults
MaxNorth to determine the expected cost of performing the North action itself. Because
MaxNorth is a primitive action, it looks up its expected reward, which is ,1.
Now this series of recursive computations can conclude as follows:

 Q (Navigate(R); s1 ; North) = ,1 + 0
 V  (Navigate(R); s1 ) = ,1
 Q (Get; s1 ; Navigate(R)) = ,1 + ,1
(,1 to perform the Navigate plus ,1 to complete the Get.
 V  (Get; s1) = ,2
 Q (Root; s1; Get) = ,2 + 12
(,2 to perform the Get plus 12 to complete the Root task and collect the final reward).
The end result of all of this is that the value of V  (Root; s1 ) is decomposed into a sum
of C terms plus the expected reward of the chosen primitive action:

V  (Root; s1 ) = V  (North; s1 ) + C  (Navigate(R); s1 ; North) +
C  (Get; s1 ; Navigate(R)) + C  (Root; s1; Get)
= ,1 + 0 + ,1 + 12
= 10
244

fiMAXQ Hierarchical Reinforcement Learning

10
MaxRoot
10
12

QGet

QPut

-2
MaxGet

MaxPut
-2

QPickup

QNavigateForPut

QNavigateForGet

QPutdown

-1
-1
Pickup

Putdown

MaxNavigate(t)
-1
0

QNorth(t)

QEast(t)

QSouth(t)

QWest(t)

East

South

West

-1
North

Figure 4: Computing the value of a state using the MAXQ hierarchy. The C value of each
Q node is shown to the left of the node. All other numbers show the values being
returned up the graph.
In general, the MAXQ value function decomposition has the form

V  (0; s) = V  (am ; s) + C  (am,1 ; s; am ) + : : : + C  (a1 ; s; a2 ) + C  (0; s; a1 ); (12)
where a0 ; a1 ; : : : ; am is the \path" of Max nodes chosen by the hierarchical policy going
from the Root down to a primitive leaf node. This is summarized graphically in Figure 5.
We can summarize the presentation of this section by the following theorem:

Theorem 2 Let  = fi; i = 0; : : : ; ng be a hierarchical policy defined for a given MAXQ
graph with subtasks M0 ; : : : ; Mn ; and let i = 0 be the root node of the graph. Then there
exist values for C  (i; s; a) (for internal Max nodes) and V  (i; s) (for primitive, leaf Max
245

fiDietterich



V (0X; s)

XXXXX




X
V  (a ; s)
P
 PPPP
1

.
V  (am,1 ; s)

. .

ZZ

ZZ


V  (am ; s) C  (am,1 ; s; am )
r1

r2

r3

r4

r5

C  (a1 ; s; a2 )
. . .

r8

r9

C  (0; s; a1 )

r10 r11 r12 r13 r14

Figure 5: The MAXQ decomposition; r1 ; : : : ; r14 denote the sequence of rewards received
from primitive actions at times 1; : : : ; 14.
nodes) such that V  (0; s) (as computed by the decomposition equations (9), (10), and (11))
is the expected discounted cumulative reward of following policy  starting in state s.

Proof: The proof is by induction on the number of levels in the task graph. At each

level i, we compute values for C  (i; s; (s)) (or V  (i; s); if i is primitive) according to the
decomposition equations. We can apply the decomposition equations again to compute
Q (i; s; (s)) and apply Equation (8) and Theorem 1 to conclude that Q (i; s; (s)) gives
the value function for level i. When i = 0, we obtain the value function for the entire
hierarchical policy. Q. E. D.
It is important to note that this representation theorem does not mention the pseudoreward function, because the pseudo-reward is used only during learning. This theorem
captures the representational power of the MAXQ decomposition, but it does not address
the question of whether there is a learning algorithm that can find a given policy. That is
the subject of the next section.

4. A Learning Algorithm for the MAXQ Decomposition
This section presents the central contributions of the paper. First, we discuss what optimality criteria should be employed in hierarchical reinforcement learning. Then we introduce
the MAXQ-0 learning algorithm, which can learn value functions (and policies) for MAXQ
hierarchies in which there are no pseudo-rewards (i.e., the pseudo-rewards are zero). The
central theoretical result of the paper is that MAXQ-0 converges to a recursively optimal
policy for the given MAXQ hierarchy. This is followed by a brief discussion of ways of
accelerating MAXQ-0 learning. The section concludes with a description of the MAXQ-Q
learning algorithm, which handles non-zero pseudo-reward functions.
246

fiMAXQ Hierarchical Reinforcement Learning

4.1 Two Kinds of Optimality

In order to develop a learning algorithm for the MAXQ decomposition, we must consider
exactly what we are hoping to achieve. Of course, for any MDP M , we would like to find
an optimal policy  . However, in the MAXQ method (and in hierarchical reinforcement
learning in general), the programmer imposes a hierarchy on the problem. This hierarchy
constrains the space of possible policies so that it may not be possible to represent the
optimal policy or its value function.
In the MAXQ method, the constraints take two forms. First, within a subtask, only
some of the possible primitive actions may be permitted. For example, in the taxi task,
during a Navigate(t), only the North, South, East, and West actions are available|the Pickup
and Putdown actions are not allowed. Second, consider a Max node Mj with child nodes
fMj ; : : : ; Mjk g. The policy learned for Mj must involve executing the learned policies of
these child nodes. When the policy for child node Mji is executed, it will run until it enters
a state in Tji . Hence, any policy learned for Mj must pass through some subset of these
terminal state sets fTj ; : : : ; Tjk g.
The HAM method shares these same two constraints and in addition, it imposes a
partial policy on each node, so that the policy for any subtask Mi must be a deterministic
refinement of the given non-deterministic initial policy for node i.
In the \option" approach, the policy is even further constrained. In this approach, there
are only two non-primitive levels in the hierarchy, and the subtasks at the lower level (i.e.,
whose children are all primitive actions) are given complete policies by the programmer.
Hence, any learned policy at the upper level must be constructed by \concatenating" the
given lower level policies in some order.
The purpose of imposing these constraints on the policy is to incorporate prior knowledge
and thereby reduce the size of the space that must be searched to find a good policy.
However, these constraints may make it impossible to learn the optimal policy.
If we can't learn the optimal policy, the next best target would be to learn the best
policy that is consistent with (i.e., can be represented by) the given hierarchy.
1

1

Definition 7 A hierarchically optimal policy for MDP M is a policy that achieves the
highest cumulative reward among all policies consistent with the given hierarchy.

Parr (1998b) proves that his HAMQ learning algorithm converges with probability 1
to a hierarchically optimal policy. Similarly, given a fixed set of options, Sutton, Precup,
and Singh (1998) prove that their SMDP learning algorithm converges to a hierarchically
optimal value function. Incidentally, they also show that if the primitive actions are also
made available as \trivial" options, then their SMDP method converges to the optimal
policy. However, in this case, it is hard to say anything formal about how the options speed
the learning process. They may in fact hinder it (Hauskrecht et al., 1998).
Because the MAXQ decomposition can represent the value function of any hierarchical
policy, we could easily construct a modified version of the HAMQ algorithm and apply it
to learn hierarchically optimal policies for the MAXQ hierarchy. However, we decided to
pursue an even weaker form of optimality, for reasons that will become clear as we proceed.
This form of optimality is called recursive optimality.
247

fiDietterich

MaxRoot

G

QExit

QGotoGoal

MaxExit

MaxGotoGoal

*

*

QExitNorth

QExitSouth

QExitEast

North

QNorthG

South

QSouthG

QEastG

East

Figure 6: A simple MDP (left) and its associated MAXQ graph (right). The policy shown in
the left diagram is recursively optimal but not hierarchically optimal. The shaded
cells indicate points where the locally-optimal policy is not globally optimal.

Definition 8 A recursively optimal policy for Markov decision process M with MAXQ
decomposition fM0 ; : : : ; Mk g is a hierarchical policy  = f0 ; : : : ; k g such that for each

subtask Mi , the corresponding policy i is optimal for the SMDP defined by the set of states
Si , the set of actions Ai , the state transition probability function P  (s0 ; N js; a), and the
reward function given by the sum of the original reward function R(s0 js; a) and the pseudoreward function R~ i (s0 ).

Note that the state transition probability distribution, P  (s0 ; N js; a) for subtask Mi is
defined by the locally optimal policies fj g of all subtasks that are descendents of Mi in
the MAXQ graph. Hence, recursive optimality is a kind of local optimality in which the
policy at each node is optimal given the policies of its children.
The reason to seek recursive optimality rather than hierarchical optimality is that recursive optimality makes it possible to solve each subtask without reference to the context
in which it is executed. This context-free property makes it easier to share and re-use
subtasks. It will also turn out to be essential for the successful use of state abstraction.
Before we proceed to describe our learning algorithm for recursive optimality, let us see
how recursive optimality differs from hierarchical optimality.
It is easy to construct examples of policies that are recursively optimal but not hierarchically optimal (and vice versa). Consider the simple maze problem and its associated
MAXQ graph shown in Figures 6. Suppose a robot starts somewhere in the left room, and
it must reach the goal G in the right room. The robot has three actions, North, South, and
East, and these actions are deterministic. The robot receives a reward of ,1 for each move.
Let us define two subtasks:
248

fiMAXQ Hierarchical Reinforcement Learning

 Exit. This task terminates when the robot exits the left room. We can set the pseudo-

reward function R~ to be 0 for the two terminal states (i.e., the two states indicated
by *'s).
 GotoGoal. This task terminates when the robot reaches the goal G.
The arrows in Figure 6 show the locally optimal policy within each room. The arrows
on the left seek to exit the left room by the shortest path, because this is what we specified
when we set the pseudo-reward function to 0. The arrows on the right follow the shortest
path to the goal, which is fine. However, the resulting policy is neither hierarchically optimal
nor optimal.
There exists a hierarchical policy that would always exit the left room by the upper
door. The MAXQ value function decomposition can represent the value function of this
policy, but such a policy would not be locally optimal (because, for example, the states
in the \shaded" region would not follow the shortest path to a doorway). Hence, this
example illustrates both a recursively optimal policy that is not hierarchically optimal and
a hierarchically optimal policy that is not recursively optimal.
If we consider for a moment, we can see a way to fix this problem. The value of the
upper starred state under the optimal hierarchical policy is ,2 and the value of the lower
starred state is ,6. Hence, if we changed R~ to have these values (instead of being zero),
then the recursively-optimal policy would be hierarchically optimal (and globally optimal).
In other words, if the programmer can guess the right values for the terminal states of a
subtask, then the recursively optimal policy will be hierarchically optimal.
This basic idea was first pointed out by Dean and Lin (1995). They describe an algorithm
that makes initial guesses for the values of these starred states and then updates those
guesses based on the computed values of the starred states under the resulting recursivelyoptimal policy. They proved that this will converge to a hierarchically optimal policy. The
drawback of their method is that it requires repeated solution of the resulting hierarchical
learning problem, and this does not always yield a speedup over just solving the original,
at problem.
Parr (1998a) proposed an interesting approach that constructs a set of different R~ functions and computes the recursively optimal policy under each of them for each subtask. His
method chooses the R~ functions in such a way that the hierarchically optimal policy can be
approximated to any desired degree. Unfortunately, the method is quite expensive, because
it relies on solving a series of linear programming problems each of which requires time
polynomial in several parameters, including the number of states jSi j within the subtask.
This discussion suggests that while, in principle, it is possible to learn good values for
the pseudo-reward function, in practice, we must rely on the programmer to specify a single
pseudo-reward function, R~ , for each subtask. If the programmer wishes to consider a small
number of alternative pseudo-reward functions, they can be handled by defining a small
number of subtasks that are identical except for their R~ functions, and permitting the
learning algorithm to choose the one that gives the best recursively-optimal policy.
In our experiments, we have employed the following simplified approach to defining
R~ . For each subtask Mi, we define two predicates: the termination predicate, Ti , and a
goal predicate, Gi . The goal predicate defines a subset of the terminal states that are \goal
states", and these have a pseudo-reward of 0. All other terminal states have a fixed constant
249

fiDietterich

pseudo-reward (e.g., ,100) that is set so that it is always better to terminate in a goal state
than in a non-goal state. For the problems on which we have tested the MAXQ method,
this worked very well.
In our experiments with MAXQ, we have found that it is easy to make mistakes in
defining Ti and Gi . If the goal is not defined carefully, it is easy to create a set of subtasks
that lead to infinite looping. For example, consider again the problem in Figure 6. Suppose
we permit a fourth action, West, in the MDP and let us define the termination and goal
predicates for the right hand room to be satisfied iff either the robot reaches the goal or it
exits the room. This is a very natural definition, since it is quite similar to the definition
for the left-hand room. However, the resulting locally-optimal policy for this room will
attempt to move to the nearest of these three locations: the goal, the upper door, or the
lower door. We can easily see that for all but a few states near the goal, the only policies
that can be constructed by MaxRoot will loop forever, first trying to leave the left room by
entering the right room, and then trying to leave the right room by entering the left room.
This problem is easily fixed by defining the goal predicate Gi for the right room to be true
if and only if the robot reaches the goal G. But avoiding such \undesired termination" bugs
can be hard in more complex domains.
In the worst case, it is possible for the programmer to specify pseudo-rewards such that
the recursively optimal policy can be made arbitrarily worse than the hierarchically optimal
policy. For example, suppose that we change the original MDP in Figure 6 so that the state
immediately to the left of the upper doorway gives a large negative reward ,L whenever
the robot visits that square. Because rewards everywhere else are ,1, the hierarchicallyoptimal policy exits the room by the lower door. But suppose the programmer has chosen
instead to force the robot to exit by the upper door (e.g., by assigning a pseudo-reward of
,10L for leaving via the lower door). In this case, the recursively-optimal policy will leave
by the upper door and suffer the large ,L penalty. By making L arbitrarily large, we can
make the difference between the hierarchically-optimal policy and the recursively-optimal
policy arbitrarily large.

4.2 The MAXQ-0 Learning Algorithm
Now that we have an understanding of recursively optimal policies, we present two learning
algorithms. The first one, called MAXQ-0, applies only in the case when the pseudo-reward
function R~ is always zero. We will first prove its convergence properties and then show
how it can be extended to give the second algorithm, MAXQ-Q, which works with general
pseudo-reward functions.
Table 2 gives pseudo-code for MAXQ-0. MAXQ-0 is a recursive function that executes
the current exploration policy starting at Max node i in state s. It performs actions until it
reaches a terminal state, at which point it returns a count of the total number of primitive
actions that have been executed. To execute an action, MAXQ-0 calls itself recursively
(line 9). When the recursive call returns, it updates the value of the completion function
for node i. It uses the count of the number of primitive actions to appropriately discount
the value of the resulting state s0 . At leaf nodes, MAXQ-0 updates the estimated one-step
expected reward, V (i; s). The value fft (i) is a \learning rate" parameter that should be
gradually decreased to zero in the limit.
250

fiMAXQ Hierarchical Reinforcement Learning

Table 2: The MAXQ-0 learning algorithm.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

function MAXQ-0(MaxNode i, State s)
if i is a primitive MaxNode
execute i, receive r, and observe result state s0
Vt (i; s) := (1 , fft (i))  Vt (i; s) + fft (i)  rt
return 1
else
let count = 0
while Ti (s) is false do
choose an action a according to the current exploration policy x(i; s)
let N = MAXQ-0(a; 0s) (recursive call)
observe result state s
Ct (i; s; a) := (1 , fft (i))  Ct (i; s; a) + fft (i)   N Vt (i; s0 )
+1

+1

count := count + N
s := s0

end
return count

end MAXQ-0

// Main program
initialize V (i; s) and C (i; s; j ) arbitrarily
MAXQ-0(root node 0, starting state s0 )

There are three things that must be specified in order to make this algorithm description
complete.
First, to keep the pseudo-code readable, Table 2 does not show how \ancestor termination" is handled. Recall that after each action, the termination predicates of all of the
subroutines on the calling stack are checked. If the termination predicate of any one of
these is satisfied, then the calling stack is unwound up to the highest terminated subroutine. In such cases, no C values are updated in any of the subroutines that were interrupted
except as follows. If subroutine i had invoked subroutine j , and j 's termination condition
is satisfied, then subroutine i can update the value of C (i; s; j ).
Second, we must specify how to compute Vt (i; s0 ) in line 11, since it is not stored in
the Max node. It is computed by the following modified versions of the decomposition
equations:
(

maxa Qt (i; s; a) if i is composite
(13)
Vt (i; s)
if i is primitive
Qt(i; s; a) = Vt(a; s) + Ct (i; s; a):
(14)
These equations reect two important changes compared with Equations (10) and (11).
First, in Equation (13), Vt (i; s) is defined in terms of the Q value of the best action a, rather
than of the action chosen by a fixed hierarchical policy. Second, there are no  superscripts,
because the current value function, Vt (i; s), is not based on a fixed hierarchical policy .
To compute Vt (i; s) using these equations, we must perform a complete search of all
paths through the MAXQ graph starting at node i and ending at the leaf nodes. Table 3

Vt (i; s) =

251

fiDietterich

Table 3: Pseudo-code for Greedy Execution of the MAXQ Graph.
function EvaluateMaxNode(i; s)
1
2
3
4
5
6
7

if i is a primitive Max node
return hVt (i; s); ii
else
for each j 2 Ai ,
let hVt (j; s); aj i = EvaluateMaxNode(j; s)
let j hg = argmaxj Vt(j; s) + Ct (i; s; j )
return hVt (j hg ; s); ajhg i
end // EvaluateMaxNode

gives pseudo-code for a recursive function, EvaluateMaxNode, that implements a depthfirst search. In addition to returning Vt (i; s), EvaluateMaxNode also returns the action
at the leaf node that achieves this value. This information is not needed for MAXQ-0, but it
will be useful later when we consider non-hierarchical execution of the learned recursivelyoptimal policy.
This search can be computationally expensive, and a problem for future research is
to develop more ecient methods for computing the best path through the graph. One
approach is to perform a best-first search and use bounds on the values within subtrees to
prune useless paths through the MAXQ graph. A better approach would be to make the
computation incremental, so that when the state of the environment changes, only those
nodes whose values have changed as a result of the state change are re-considered. It should
be possible to develop an ecient bottom-up method similar to the RETE algorithm (and
its successors) that is used in the SOAR architecture (Forgy, 1982; Tambe & Rosenbloom,
1994).
The third thing that must be specified to complete our definition of MAXQ-0 is the
exploration policy, x . We require that x be an ordered GLIE policy.

Definition 9 An ordered GLIE policy is a GLIE policy (Greedy in the Limit with Infinite

Exploration) that converges in the limit to an ordered greedy policy, which is a greedy policy
that imposes an arbitrary fixed order ! on the available actions and breaks ties in favor of
the action a that appears earliest in that order.

We need this property in order to ensure that MAXQ-0 converges to a uniquely-defined
recursively optimal policy. A fundamental problem with recursive optimality is that in
general, each Max node i will have a choice of many different locally optimal policies given
the policies adopted by its descendent nodes. These different locally optimal policies will
all achieve the same locally optimal value function, but they can give rise to different probability transition functions P (s0 ; N js; i). The result will be that the Semi-Markov Decision
Problems defined at the next level above node i in the MAXQ graph will differ depending
on which of these various locally optimal policies is chosen by node i. These differences may
lead to better or worse policies at higher levels of the MAXQ graph, even though they make
no difference inside subtask i. In practice, the designer of the MAXQ graph will need to
design the pseudo-reward function for subtask i to ensure that all locally optimal policies
252

fiMAXQ Hierarchical Reinforcement Learning

are equally valuable for the parent subroutine. But to carry out our formal analysis, we will
just rely on an arbitrary tie-breaking mechanism.2 If we establish a fixed ordering over the
Max nodes in the MAXQ graph (e.g., a left-to-right depth-first numbering), and break ties
in favor of the lowest-numbered action, then this defines a unique policy at each Max node.
And consequently, by induction, it defines a unique policy for the entire MAXQ graph. Let
us call this policy r . We will use the r subscript to denote recursively optimal quantities
under an ordered greedy policy. Hence, the corresponding value function is Vr , and Cr and
Qr denote the corresponding completion function and action-value function. We now prove
that the MAXQ-0 algorithm converges to r .

Theorem 3 Let M = hS; A; P; R; P0 i be either an episodic MDP for which all deterministic

policies are proper or a discounted infinite horizon MDP with discount factor  . Let H be
a MAXQ graph defined over subtasks fM0 ; : : : ; Mk g such that the pseudo-reward function
R~ i (s0 ) is zero for all i and s0. Let fft (i) > 0 be a sequence of constants for each Max node i
such that
T
T
X
X
lim
ff
(
i
)
=
1
and
lim
ff2t (i) < 1
(15)
t
T !1
T !1
t=1

t=1

Let x (i; s) be an ordered GLIE policy at each node i and state s and assume that the
immediate rewards are bounded. Then with probability 1, algorithm MAXQ-0 converges to
r , the unique recursively optimal policy for M consistent with H and x.

Proof: The proof follows an argument similar to those introduced to prove the convergence

of Q learning and SARSA(0) (Bertsekas & Tsitsiklis, 1996; Jaakkola et al., 1994). We will
employ the following result from stochastic approximation theory, which we state without
proof:

Lemma 1 (Proposition 4.5 from Bertsekas and Tsitsiklis, 1996) Consider the iteration
rt+1 (i) := (1 , fft (i))rt (i) + fft (i)((Urt )(i) + wt (i) + ut(i)):
Let Ft = fr0 (i); : : : ; rt (i); w0 (i); : : : ; wt,1 (i); ff0 (i); : : : ; fft (i); 8ig be the entire history of the
iteration.
If

(a) The fft (i)  0 satisfy conditions (15)
(b) For every i and t, the noise terms wt (i) satisfy E [wt (i)jFt ] = 0
(c) Given any norm jj  jj on Rn , there exist constants A and B such that E [wt2 (i)jFt ] 
A + B jjrtjj2 .
(d) There exists a vector r , a positive vector  , and a scalar fi 2 [0; 1), such that for all
t,

jjUrt , rjj  fi jjrt , rjj

2. Alternatively, we could break ties by using a stochastic policy that chose randomly among the tied
actions.

253

fiDietterich

(e) There exists a nonnegative random sequence t that converges to zero with probability
1 and is such that for all t
jut (i)j  t(jjrt jj + 1)
then rt converges to r with probability 1. The notation jj  jj denotes a weighted maximum
norm
jA(i)j :
jjAjj = max
i  (i)

The structure of the proof of Theorem 3 will be inductive, starting at the leaves of the
MAXQ graph and working toward the root. We will employ a different time clock at each
node i to count the number of update steps performed by MAXQ-0 at that node. The
variable t will always refer to the time clock of the current node i.
To prove the base case for any primitive Max node, we note that line 3 of MAXQ-0 is
just the standard stochastic approximation algorithm for computing the expected reward
for performing action a in state s, and therefore it converges under the conditions given
above.
To prove the recursive case, consider any composite Max node i with child node j . Let
Pt (s0; N js; j ) be the transition probability distribution for performing child action j in state
s at time t (i.e., while following the exploration policy in all descendent nodes of node j ).
By the inductive assumption, MAXQ-0 applied to j will converge to the (unique) recursively optimal value function Vr (j; s) with probability 1. Furthermore, because MAXQ-0
is following an ordered GLIE policy for j and its descendents, they will all converge to executing a greedy policy with respect to their value functions, so Pt (s0 ; N js; j ) will converge
to Pr (s0 ; N js; j ), the unique transition probability function for executing child j under the
locally optimal policy r . What remains to be shown is that the update assignment for C
(line 11 of the MAXQ-0 algorithm) converges to the optimal Cr function with probability
1.
To prove this, we will apply Lemma 1. We will identify the x in the lemma with a
state-action pair (s; a). The vector rt will be the completion-cost table Ct (i; s; a) for all
s; a and fixed i after t update steps. The vector r will be the optimal completion-cost
Cr (i; s; a) (again, for fixed i). Define the mapping U to be
(UC )(i; s; a) =

X

s0





0 0
 0 0
Pr (s0 ; N js; a) N max
0 [C (i; s ; a ) + Vr (a ; s )]
a

This is a C update under the MDP Mi assuming that all descendent value functions,
Vr (a; s), and transition probabilities, Pr(s0 ; N js; a), have converged.
To apply the lemma, we must first express the C update formula in the form of the
update rule in the lemma. Let s be the state that results from performing a in state s. Line
11 can be written

Ct+1 (i; s; a) :=

(1 , fft (i))  Ct (i; s; a) + fft (i)   N





max[Ct (i; s; a0 ) + Vt (a0 ; s)]
a0

:= (1 , fft (i))  Ct (i; s; a) + fft (i)  [(UCt )(i; s; a) + wt (i; s; a) + ut (i; s; a)]
254

fiMAXQ Hierarchical Reinforcement Learning

where





wt (i; s; a) =  N max
[C (i; s; a0 ) + Vt (a0 ; s)] ,
a0 t
X

s0 ;N

ut (i; s; a) =

X

s0 ;N
X

s0 ;N









Pt (s0 ; N js; a) N max
[C (i; s0 ; a0 ) + Vt (a0 ; s0 )]
a0 t

0 0
0 0
Pt (s0 ; N js; a) N max
0 [Ct (i; s ; a ) + Vt (a ; s )]


a

,



Pr (s0; N js; a) N max
[C (i; s0 ; a0 ) + Vr (a0 ; s0 )]
a0 t

Here wt (i; s; a) is the difference between doing an update at node i using the single sample
point s drawn according to Pt (s0 ; N js; a) and doing an update using the full distribution
Pt (s0; N js; a). The value of ut (i; s; a) captures the difference between doing an update using
the current probability transitions Pt (s0 ; N js; a) and current value functions of the children
Vt (a0; s0 ) and doing an update using the optimal probability transitions Pr (s0 ; N js; a) and
the optimal values of the children Vr (a0 ; s0 ).
We now verify the conditions of Lemma 1.
Condition (a) is assumed in the conditions of the theorem with fft (s; a) = fft (i).
Condition (b) is satisfied because s is sampled from Pt (s0 ; N js; a), so the expected value
of the difference is zero.
Condition (c) follows directly from the fact that jCt (i; s; a)j and jVt (i; s)j are bounded.
We can show that these are bounded for both the episodic case and the discounted case as
follows. In the episodic case, we have assumed all policies are proper. Hence, all trajectories
terminate in finite time with a finite total reward. In the discounted case, the infinite sum
of future rewards is bounded if the one-step rewards are bounded. The values of C and V
are computed as temporal averages of the cumulative rewards received over a finite number
of (bounded) updates, and hence, their means, variances, and maximum values are all
bounded.
Condition (d) is the condition that U is a weighted max norm pseudo-contraction. We
can derive this by starting with the weighted max norm for Q learning. It is well known
that Q is a weighted max norm pseudo-contraction (Bertsekas & Tsitsiklis, 1996) in both
the episodic case where all deterministic policies are proper (and the discount factor  = 1)
and in the infinite horizon discounted case (with  < 1). That is, there exists a positive
vector  and a scalar fi 2 [0; 1), such that for all t,

jjTQt , Qjj  fi jjQt , Qjj ;

(16)

where T is the operator
(TQ)(s; a) =

X

s0 ;N

P (s0; N js; a) N [R(s0 js; a) + max
Q(s0 ; a0 )]:
a0

Now we will show how to derive the pseudo-contraction for the C update operator U . Our
plan is to show first how to express the U operator for learning C in terms of the T operator
for updating Q values. Then we will replace TQ in the pseudo-contraction equation for Q
255

fiDietterich

learning with UC , and show that U is a weighted max-norm pseudo-contraction under the
same weights  and the same fi .
Recall from Eqn. (10) that Q(i; s; a) = C (i; s; a) + V (a; s). Furthermore, the U operator
performs its updates using the optimal value functions of the child nodes, so we can write
this as Qt (i; s; a) = Ct (i; s; a) + V  (a; s). Now once the children of node i have converged,
the Q-function version of the Bellman equation for MDP Mi can be written as

Q(i; s; a) =

X

s0 ;N

Pr(s0 ; N js; a) N [Vr (a; s) + max
Q(i; s0 ; a0 )]:
a0

As we have noted before, Vr (a; s) plays the role of the immediate reward function for Mi .
Therefore, for node i, the T operator can be rewritten as
(TQ)(i; s; a) =

X

s0 ;N

Pr (s0 js; a) N [Vr(a; s) + max
Q(i; s0 ; a0 )]:
a0

Now we replace Q(i; s; a) by C (i; s; a) + Vr (a; s), and obtain
(TQ)(i; s; a) =

X

s0 ;N

Pr (s0; N js; a) N (Vr (a; s) + max
[C (i; s0 ; a0 ) + Vr (a0 ; s0 )]):
a0

Note that Vr (a; s) does not depend on s0 or N , so we can move it outside the expectation
and obtain
(TQ)(i; s; a) = Vr (a; s) +

X

s0 ;N

Pr (s0 ; N js; a) N (max
[C (i; s0 ; a0 ) + Vr (a0 ; s0 )])
a0

= Vr (a; s) + (UC )(i; s; a)

Abusing notation slightly, we will express this in vector form as TQ(i) = Vr + UC (i).
Similarly, we can write Qt (i; s; a) = Ct (i; s; a)+ Vr (a; s) in vector form as Qt (i) = Ct (i)+ Vr .
Now we can substitute these two formulas into the max norm pseudo-contraction formula
for T , Eqn. (16) to obtain

jjVr + UCt (i) , (Cr(i) + Vr)jj  fi jjVr + Ct (i) , (Cr(i) + Vr)jj :
Thus, U is a weighted max-norm pseudo-contraction,

jjUCt (i) , Cr(i)jj  fi jjCt (i) , Cr(i)jj ;
and condition (d) is satisfied.
Finally, it is easy to verify (e), the most important condition. By assumption, the
ordered GLIE policies in the child nodes converge with probability 1 to locally optimal
policies for the children. Therefore Pt (s0 ; N js; a) converges to Pr (s0 ; N js; a) for all s0; N; s;
and a with probability 1 and Vt (a; s) converges with probability 1 to Vr (a; s) for all child
actions a. Therefore, jut j converges to zero with probability 1. We can trivially construct a
sequence t = jut j that bounds this convergence, so

jut (s; a)j  t  t (jjCt (s; a)jj + 1):
256

fiMAXQ Hierarchical Reinforcement Learning

We have verified all of the conditions of Lemma 1, so we can conclude that Ct (i) converges
to Cr(i) with probability 1. By induction, we can conclude that this holds for all nodes in
the MAXQ including the root node, so the value function represented by the MAXQ graph
converges to the unique value function of the recursively optimal policy r . Q.E.D.
The most important aspect of this theorem is that it proves that Q learning can take
place at all levels of the MAXQ hierarchy simultaneously|the higher levels do not need to
wait until the lower levels have converged before they begin learning. All that is necessary
is that the lower levels eventually converge to their (locally) optimal policies.

4.3 Techniques for Speeding Up MAXQ-0

Algorithm MAXQ-0 can be extended to accelerate learning in the higher nodes of the graph
by a technique that we call \all-states updating". When an action a is chosen for Max node
i in state s, the execution of a will move the environment through a sequence of states
s = s1; : : : ; sN ; sN +1 = s0. Because all of our subroutines are Markovian, the same resulting
state s0 would have been reached if we had started executing action a in state s2 , or s3 , or
any state up to and including sN . Hence, we can execute a version of line 11 in MAXQ-0
for each of these intermediate states as shown in this replacement pseudo-code:
11a
for j from 1 to N do
11b
Ct (i; sj ; a) := (1 , fft (i))  Ct(i; sj ; a) + fft (i)   N ,j maxa Qt (i; s0 ; a0 )
11c
end // for
In our implementation, as each composite action is executed by MAXQ-0, it constructs
a linked list of the sequence of primitive states that were visited. This list is returned when
the composite action terminates. The parent Max node can then process each state in this
list as shown above. The parent Max node concatenates the state lists that it receives from
its children and passes them to its parent when it terminates. All experiments in this paper
employ all-states updating.
Kaelbling (1993) introduced a related, but more powerful, method for accelerating hierarchical reinforcement learning that she calls \all-goals updating." To understand this
method, suppose that for each primitive action, there are several composite tasks that could
have invoked that primitive action. In all-goals updating, whenever a primitive action is
executed, the equivalent of line 11 of MAXQ-0 is applied in every composite task that could
have invoked that primitive action. Sutton, Precup, and Singh (1998) prove that each of
the composite tasks will converge to the optimal Q values under all-goals updating. Furthermore, they point out that the exploration policy employed for choosing the primitive
actions can be different from the policies of any of the subtasks being learned.
It is straightforward to implement a simple form of all-goals updating within the MAXQ
hierarchy for the case where composite tasks invoke primitive actions. Whenever one of the
primitive actions a is executed in state s, we can update the C (i; s; a) value for all parent
tasks i that can invoke a.
However, additional care is required to implement all-goals updating for non-primitive
actions. Suppose that by executing the exploration policy, the following sequence of world
states and actions has been obtained: s0 ; a0 ; s1 ; : : : ; ak,1 ; sk,1 ; ak ; sk+1 . Let j be a composite task that is terminated in state sk+1 , and let sk,n; ak,n ; : : : ; ak,1 ; ak be a sequence of
actions that could have been executed by subtask j and its children. In other words, suppose
(

+1

257

+1

)

0

fiDietterich

it is possible to \parse" this state-action sequence in terms of a series of subroutine calls and
returns for one invocation of subtask j . Then for each possible parent task i that invokes j ,
we can update the value of C (i; sk,n ; j ). Of course, in order for these updates to be useful,
the exploration policy must be an ordered GLIE policy that will converge to the recursively
optimal policy for subtask j and its descendents. We cannot follow an arbitrary exploration
policy, because this would not produce accurate samples of result states drawn according to
P  (s0 ; N js; j ). Hence, unlike the simple case described by Sutton, Precup, and Singh, the
exploration policy cannot be different from the policies of the subtasks being learned.
Although this considerably reduces the usefulness of all-goals updating, it does not
completely eliminate it. A simple way of implementing non-primitive all-goals updating
would be to perform MAXQ-Q learning as usual, but whenever a subtask j was invoked in
state s and returned, we could update the value of C (i; s; j ) for all potential calling subtasks
i. We have not implemented this, however, because of the complexity involved in identifying
the possible actual parameters of the potential calling subroutines.

4.4 The MAXQ-Q Learning Algorithm
Now that we have shown the convergence of MAXQ-0, let us design a learning algorithm that
can work with arbitrary pseudo-reward functions, R~ i (s0 ). We could just add the pseudoreward into MAXQ-0, but this would have the effect of changing the MDP M to have
a different reward function. The pseudo-rewards \contaminate" the values of all of the
completion functions computed in the hierarchy. The resulting learned policy will not be
recursively optimal for the original MDP.
This problem can be solved by learning one completion function for use \inside" each
Max node and a separate completion function for use \outside" the Max node. The quantities used \inside" a node will be written with a tilde: R~ , C~ , and Q~ . The quantities used
\outside" a node will be written without the tilde.
The \outside" completion function, C (i; s; a) is the completion function that we have
been discussing so far in this paper. It computes the expected reward for completing task
Mi after performing action a in state s and then following the learned policy for Mi . It is
computed without any reference to R~ i . This completion function will be used by parent
tasks to compute V (i; s), the expected reward for performing action i starting in state s.
The second completion function C~ (i; s; a) is a completion function that we will use only
\inside" node i in order to discover the locally optimal policy for task Mi . This function
will incorporate rewards both from the \real" reward function, R(s0 js; a), and from the
pseudo-reward function, R~ i (s0 ). It will also be used by EvaluateMaxNode in line 6 to
choose the best action j hg to execute. Note, however, that EvaluateMaxNode will still
return the \external" value Vt (j hg ; s) of this chosen action.
We will employ two different update rules to learn these two completion functions. The
C~ function will be learned using an update rule similar to the Q learning rule in line 11 of
MAXQ-0. But the C function will be learned using an update rule similar to SARSA(0)|
its purpose is to learn the value function for the policy that is discovered by optimizing C~ .
Pseudo-code for the resulting algorithm, MAXQ-Q is shown in Table 4.
The key step is at lines 15 and 16. In line 15, MAXQ-Q first updates C~ using the value
of the greedy action, a , in the resulting state. This update includes the pseudo-reward R~ i .
258

fiMAXQ Hierarchical Reinforcement Learning

Table 4: The MAXQ-Q learning algorithm.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

function MAXQ-Q(MaxNode i, State s)
let seq = () be the sequence of states visited while executing i
if i is a primitive MaxNode
execute i, receive r, and observe result state s0
Vt (i; s) := (1 , fft (i))  Vt (i; s) + fft (i)  rt
push s onto the beginning of seq
else
let count = 0
while Ti (s) is false do
choose an action a according to the current exploration policy x(i; s)
let childSeq = MAXQ-Q(a;s), where childSeq is the sequence of states visited
+1

while executing action a. (in reverse order)
observe result state s0
let a = argmaxa [C~t (i; s0 ; a0 ) + Vt (a0 ; s0 )]
let N = 1
for each s in childSeq do
C~t+1 (i; s; a) := (1 , fft (i))  C~t (i; s; a) + fft (i)   N [R~ i (s0 ) + C~t (i; s0 ; a ) + Vt(a ; s)]
Ct+1 (i; s; a) := (1 , fft (i))  Ct (i; s; a) + fft (i)   N [Ct (i; s0 ; a ) + Vt(a ; s0 )]
N := N + 1
end // for
append
childSeq onto the front of seq
s := s0
end // while
end // else
return seq
end MAXQ-Q
0

Then in line 16, MAXQ-Q updates C using this same greedy action a , even if this would
not be the greedy action according to the \uncontaminated" value function. This update,
of course, does not include the pseudo-reward function.
It is important to note that whereever Vt (a; s) appears in this pseudo-code, it refers to
the \uncontaminated" value function of state s when executing the Max node a. This is
computed recursively in exactly the same way as in MAXQ-0.
Finally, note that the pseudo-code also incorporates all-states updating, so each call
to MAXQ-Q returns a list of all of the states that were visited during its execution, and
the updates of lines 15 and 16 are performed for each of those states. The list of states is
ordered most-recent-first, so the states are updated starting with the last state visited and
working backward to the starting state, which helps speed up the algorithm.
When MAXQ-Q has converged, the resulting recursively optimal policy is computed at
each node by choosing the action a that maximizes Q~ (i; s; a) = C~ (i; s; a)+ V (a; s) (breaking
ties according to the fixed ordering established by the ordered GLIE policy). It is for this
reason that we gave the name \Max nodes" to the nodes that represent subtasks (and
learned policies) within the MAXQ graph. Each Q node j with parent node i stores both
C~ (i; s; j ) and C (i; s; j ), and it computes both Q~ (i; s; j ) and Q(i; s; j ) by invoking its child
Max node j . Each Max node i takes the maximum of these Q values and computes either
V (i; s) or computes the best action, a using Q~ .
259

fiDietterich

Corollary 1 Under the same conditions as Theorem 3, MAXQ-Q converges to the unique

recursively optimal policy for MDP M defined by MAXQ graph H , pseudo-reward functions
R~ , and ordered GLIE exploration policy x.
Proof: The argument is identical to, but more tedious than, the proof of Theorem 3. The
proof of convergence of the C~ values is identical to the original proof for the C values, but
it relies on proving convergence of the \new" C values as well, which follows from the same
weighted max norm pseudo-contraction argument. Q.E.D.

5. State Abstraction

There are many reasons to introduce hierarchical reinforcement learning, but perhaps the
most important reason is to create opportunities for state abstraction. When we introduced
the simple taxi problem in Figure 1, we pointed out that within each subtask, we can ignore
certain aspects of the state space. For example, while performing a MaxNavigate(t), the
taxi should make the same navigation decisions regardless of whether the passenger is in
the taxi. The purpose of this section is to formalize the conditions under which it is safe
to introduce such state abstractions and to show how the convergence proofs for MAXQ-Q
can be extended to prove convergence in the presence of state abstraction. Specifically, we
will identify five conditions that permit the \safe" introduction of state abstractions.
Throughout this section, we will use the taxi problem as a running example, and we will
see how each of the five conditions will permit us to reduce the number of distinct values
that must be stored in order to represent the MAXQ value function decomposition. To
establish a starting point, let us compute the number of values that must be stored for the
taxi problem without any state abstraction.
The MAXQ representation must have tables for each of the C functions at the internal
nodes and the V functions at the leaves. First, at the six leaf nodes, to store V (i; s), we
must store 500 values at each node, because there are 500 states; 25 locations, 4 possible
destinations for the passenger, and 5 possible current locations for the passenger (the four
special locations and inside the taxi itself). Second, at the root node, there are two children,
which requires 2  500 = 1000 values. Third, at the MaxGet and MaxPut nodes, we have 2
actions each, so each one requires 1000 values, for a total of 2000. Finally, at MaxNavigate(t),
we have four actions, but now we must also consider the target parameter t, which can take
four possible values. Hence, there are effectively 2000 combinations of states and t values for
each action, or 8000 total values that must be represented. In total, therefore, the MAXQ
representation requires 14,000 separate quantities to represent the value function.
To place this number in perspective, consider that a at Q learning representation must
store a separate value for each of the six primitive actions in each of the 500 possible states,
for a total of 3,000 values. Hence, we can see that without state abstraction, the MAXQ
representation requires more than four times the memory of a at Q table!

5.1 Five Conditions that Permit State Abstraction

We now introduce five conditions that permit the introduction of state abstractions. For
each condition, we give a definition and then prove a lemma which states that if the condition is satisfied, then the value function for some corresponding class of policies can be
260

fiMAXQ Hierarchical Reinforcement Learning

represented abstractly (i.e., by abstract versions of the V and C functions). For each condition, we then provide some rules for identifying when that condition can be satisfied and
give examples from the taxi domain.
We begin by introducing some definitions and notation.

Definition 10 Let M be a MDP and H be a MAXQ graph defined over M . Suppose that

each state s can be written as a vector of values of a set of state variables. At each Max
node i, suppose the state variables are partitioned into two sets Xi and Yi , and let i be a
function that projects a state s onto only the values of the variables in Xi . Then H combined
with i is called a state-abstracted MAXQ graph.

In cases where the state variables can be partitioned, we will often write s = (x; y)
to mean that a state s is represented by a vector of values for the state variables in X
and a vector of values for the state variables in Y . Similarly, we will sometimes write
P (x0 ; y0; N jx; y; a), V (a; x; y), and R~ a (x0 ; y0 ) in place of P (s0; N js; a), V (a; s), and R~ a (s0 ),
respectively.

Definition 11 (Abstract Policy) An abstract hierarchical policy for MDP M with state-

abstracted MAXQ graph H and associated abstraction functions i , is a hierarchical policy
in which each policy i (corresponding to subtask Mi ) satisfies the condition that for any two
states s1 and s2 such that i (s1 ) = i (s2 ), i (s1 ) = i (s2 ). (When i is a stochastic policy,
such as an exploration policy, this is interpreted to mean that the probability distributions
for choosing actions are the same in both states.)

In order for MAXQ-Q to converge in the presence of state abstractions, we will require
that at all times t its (instantaneous) exploration policy is an abstract hierarchical policy.
One way to achieve this is to construct the exploration policy so that it only uses information from the relevant state variables in deciding what action to perform. Boltzmann exploration based on the (state-abstracted) Q values, -greedy exploration, and counter-based
exploration based on abstracted states are all abstract exploration policies. Counter-based
exploration based on the full state space is not an abstract exploration policy.
Now that we have introduced our notation, let us describe and analyze the five abstraction conditions. We have identified three different kinds of conditions under which
abstractions can be introduced. The first kind involves eliminating irrelevant variables
within a subtask of the MAXQ graph. Under this form of abstraction, nodes toward the
leaves of the MAXQ graph tend to have very few relevant variables, and nodes higher in
the graph have more relevant variables. Hence, this kind of abstraction is most useful at
the lower levels of the MAXQ graph.
The second kind of abstraction arises from \funnel" actions. These are macro actions
that move the environment from some large number of initial states to a small number of
resulting states. The completion cost of such subtasks can be represented using a number of
values proportional to the number of resulting states. Funnel actions tend to appear higher
in the MAXQ graph, so this form of abstraction is most useful near the root of the graph.
The third kind of abstraction arises from the structure of the MAXQ graph itself. It
exploits the fact that large parts of the state space for a subtask may not be reachable
because of the termination conditions of its ancestors in the MAXQ graph.
261

fiDietterich

We begin by describing two abstraction conditions of the first type. Then we will present
two conditions of the second type. And finally, we describe one condition of the third type.
5.1.1 Condition 1: Max Node Irrelevance

The first condition arises when a set of state variables is irrelevant to a Max node.

Definition 12 (Max Node Irrelevance) Let Mi be a Max node in a MAXQ graph H

for MDP M . A set of state variables Y is irrelevant for node i if the state variables of M
can be partitioned into two sets X and Y such that for any stationary abstract hierarchical
policy  executed by the descendents of i, the following two properties hold:

 the state transition probability distribution P  (s0; N js; a) at node i can be factored into
the product of two distributions:

P  (x0 ; y0 ; N jx; y; a) = P  (y0jx; y; a)  P  (x0 ; N jx; a);

(17)

where y and y0 give values for the variables in Y , and x and x0 give values for the
variables in X .

 for any pair of states s1 = (x; y1 ) and s2 = (x; y2 ) such that (s1) = (s2) = x, and
any child action a, V  (a; s1 ) = V  (a; s2 ) and R~ i (s1 ) = R~ i (s2 ).

Note that the two conditions must hold for all stationary abstract policies  executed
by all of the descendents of the subtask i. We will discuss below how these rather strong
requirements can be satisfied in practice. First, however, we prove that these conditions are
sucient to permit the C and V tables to be represented using state abstractions.

Lemma 2 Let M be an MDP with full-state MAXQ graph H , and suppose that state vari-

ables Yi are irrelevant for Max node i. Let i (s) = x be the associated abstraction function
that projects s onto the remaining relevant variables Xi . Let  be any abstract hierarchical
policy. Then the action-value function Q at node i can be represented compactly, with only
one value of the completion function C  (i; s; j ) for each equivalence class of states s that
share the same values on the relevant variables.
Specifically Q (i; s; j ) can be computed as follows:

Q (i; s; j ) = V  (j; i (s)) + C  (i; i (s); j )
where

C  (i; x; j ) =

X

x0 ;N

P  (x0 ; N jx; j )   N [V  ((x0 ); x0 ) + R~ i (x0 ) + C  (i; x0 ; (x0 ))];

where V  (j 0 ; x0 ) = V  (j 0 ; x0 ; y0 ), R~ i (x0 ) = R~ i (x0 ; y0 ), and (x) = (x; y0 ) for some arbitrary
value y0 for the irrelevant state variables Yi .
262

fiMAXQ Hierarchical Reinforcement Learning

Proof: Define a new MDP i(Mi ) at node i as follows:
 States: X = fx j i(s) = x; for some s 2 S g.
 Actions: A.
 Transition probabilities: P  (x0 ; N jx; a)
 Reward function: V  (a; x) + R~ i(x0)

Because  is an abstract policy, its decisions are the same for all states s such that i (s) = x
for some x. Therefore, it is also a well-defined policy over i (Mi ). The action-value function
for  over i (Mi ) is the unique solution to the following Bellman equation:
X
Q (i; x; j ) = V  (j; x) + P  (x0 ; N jx; j )   N [R~ i(x0 ) + Q (i; x0 ; (x0 ))]
(18)
x0 ;N

Compare this to the Bellman equation over Mi :
X
Q (i; s; j ) = V  (j; s) + P  (s0; N js; j )   N [R~i (s0) + Q (i; s0 ; (s0 ))]
s0 ;N

(19)

and note that V  (j; s) = V  (j; (s)) = V  (j; x) and R~ i (s0 ) = R~ i ((s0 )) = R~ i (x0 ). Furthermore, we know that the distribution P  can be factored into separate distributions for Yi
and Xi . Hence, we can rewrite (19) as
X
X
Q (i; s; j ) = V  (j; x) + P (y0 jx; y; j ) P  (x0 ; N jx; j )   N [R~ i (x0 ) + Q (i; s0 ; (s0 ))]
y0

x0 ;N

The right-most sum does not depend on y or y0 , so the sum over y0 evaluates to 1, and can
be eliminated to give
X
Q (i; s; j ) = V  (j; x) + P  (x0 ; N jx; j )   N [R~ i(x0 ) + Q (i; s0 ; (s0 ))]:
(20)
x0 ;N

Finally, note that equations (18) and (20) are identical except for the expressions for
the Q values. Since the solution to the Bellman equation is unique, we must conclude that
Q (i; s; j ) = Q (i; (s); j ):
We can rewrite the right-hand side to obtain
Q (i; s; j ) = V  (j; (s)) + C  (i; (s); j );
where
X
C  (i; x; j ) = P (x0; N jx; j )   N [V  ((x0 ); x0 ) + R~ i (x0 ) + C  (i; x0 ; (x0 ))]:

Q.E.D.

x0 ;N

Of course we are primarily interested in being able to discover and represent the optimal
policy at each node i. The following corollary shows that the optimal policy is an abstract
policy, and hence, that it can be represented abstractly.
263

fiDietterich

Corollary 2 Consider the same conditions as Lemma 2, but with the change that the ab-

stract hierarchical policy  is executed only by the descendents of node i, but not by node
i. Let ! be an ordering over actions. Then the optimal ordered policy ! at node i is an
abstract policy, and its action-value function can be represented abstractly.

Proof: Define the policy ! to be the optimal ordered policy over the abstract MDP

(M ), and let Q (i; x; j ) be the corresponding optimal action-value function. Then by the
same argument given above, Q is also a solution to the optimal Bellman equation for the
original MDP. This means that the policy ! defined by ! (s) =  ((s)) is an optimal

ordered policy, and by construction, it is an abstract policy. Q.E.D.
As stated, the Max node irrelevance condition appears quite dicult to satisfy, since it
requires that the state transition probability distribution factor into X and Y components
for all possible abstract hierarchical policies. However, in practice, this condition is often
satisfied.
For example, let us consider the Navigate(t) subtask. The source and destination of
the passenger are irrelevant to the achievement of this subtask. Any policy that successfully completes this subtask will have the same value function regardless of the source and
destination locations of the passenger. By abstracting away the passenger source and destination, we obtain a huge savings in space. Instead of requiring 8000 values to represent
the C functions for this task, we require only 400 values (4 actions, 25 locations, 4 possible
values for t).
The advantages of this form of abstraction are similar to those obtained by Boutilier,
Dearden and Goldszmidt (1995) in which belief network models of actions are exploited
to simplify value iteration in stochastic planning. Indeed, one way of understanding the
conditions of Definition 12 is to express them in the form of a decision diagram, as shown
in Figure 7. The diagram shows that the irrelevant variables Y do not affect the rewards
either directly or indirectly, and therefore, they do not affect either the value function or
the optimal policy.
One rule for noticing cases where this abstraction condition holds is to examine the
subgraph rooted at the given Max node i. If a set of state variables is irrelevant to the leaf
state transition probabilities and reward functions and also to all pseudo-reward functions
and termination conditions in the subgraph, then those variables satisfy the Max Node
Irrelevance condition:

Lemma 3 Let M be an MDP with associated MAXQ graph H , and let i be a Max node in

H . Let Xi and Yi be a partition of the state variables for M . A set of state variables Yi is
irrelevant to node i if
 For each primitive leaf node a that is a descendent of i,
P (x0 ; y0 jx; y; a) = P (y0 jx; y; a)P (x0 jx; a) and
R(x0; y0 jx; y; a) = R(x0 jx; a),

 For each internal node j that is equal to node i or is a descendent of i , R~ j (x0 ; y0) =
R~j (x0) and the termination predicate Tj (x0 ; y0 ) is true iff Tj (x0).
264

fiMAXQ Hierarchical Reinforcement Learning

j

V

X

X

Y

Y

Figure 7: A dynamic decision diagram that represents the conditions of Definition 12. The
probabilistic nodes X and Y represent the state variables at time t, and the nodes
X 0 and Y 0 represent the state variables at a later time t + N . The square action
node j is the chosen child subroutine, and the utility node V represents the value
function V (j; x) of that child action. Note that while X may inuence Y 0 , Y
cannot affect X 0 , and therefore, it cannot affect V .

Proof: We must show that any abstract hierarchical policy will give rise to an SMDP at

node i whose transition probability distribution factors and whose reward function depends
only on Xi . By definition, any abstract hierarchical policy will choose actions based only
upon information in Xi . Because the primitive probability transition functions factor into
an independent component for Xi and since the termination conditions at all nodes below i
are based only on the variables in Xi , the probability transition function Pi (x0 ; y0 ; N jx; y; a)
must also factor into Pi (y0 jx; y; a) and Pi (x0 ; N jx; a). Similarly, all of the reward functions
V (j; x; y) must be equal to V (j; x), because all rewards received within the subtree (either
at the leaves or through pseudo-rewards) depend only on the variables in Xi . Therefore,
the variables in Yi are irrelevant for Max node i. Q.E.D.
In the Taxi task, the primitive navigation actions, North, South, East, and West only
depend on the location of the taxi and not on the location of the passenger. The pseudoreward function and termination condition for the MaxNavigate(t) node only depend on the
location of the taxi (and the parameter t). Hence, this lemma applies, and the passenger
source and destination are irrelevant for the MaxNavigate node.
5.1.2 Condition 2: Leaf Irrelevance

The second abstraction condition describes situations under which we can apply state abstractions to leaf nodes of the MAXQ graph. For leaf nodes, we can obtain a stronger result
than Lemma 2 by using a slightly weaker definition of irrelevance.
265

fiDietterich

Definition 13 (Leaf Irrelevance) A set of state variables Y is irrelevant for a primitive
action a of a MAXQ graph if for all states s the expected value of the reward function,
X
V (a; s) = P (s0 js; a)R(s0 js; a)
s0

does not depend on any of the values of the state variables in Y . In other words, for any
pair of states s1 and s2 that differ only in their values for the variables in Y ,
X

s01

P (s01 js1 ; a)R(s01 js1 ; a) =

X

s02

P (s02 js2 ; a)R(s02 js2 ; a):

If this condition is satisfied at leaf a, then the following lemma shows that we can
represent its value function V (a; s) compactly.

Lemma 4 Let M be an MDP with full-state MAXQ graph H , and suppose that state vari-

ables Y are irrelevant for leaf node a. Let (s) = x be the associated abstraction function
that projects s onto the remaining relevant variables X . Then we can represent V (a; s) for
any state s by an abstracted value function V (a; (s)) = V (a; x).

Proof: According to the definition of Leaf Irrelevance, any two states that differ only on

the irrelevant state variables have the same value for V (a; s). Hence, we can represent this
unique value by V (a; x). Q.E.D.
Here are two rules for finding cases where Leaf Irrelevance applies. The first rule shows
that if the probability distribution factors, then we have Leaf Irrelevance.

Lemma 5 Suppose the probability transition function for primitive action a, P (s0js; a), factors as P (x0 ; y0 jx; y; a) = P (y0 jx; y; a)P (x0 jx; a) and the reward function satisfies R(s0 js; a) =
R(x0jx; a). Then the variables in Y are irrelevant to the leaf node a.
Proof: Plug in to the definition of V (a; s) and simplify.
X
V (a; s) =
P (s0 js; a)R(s0 js; a)
=
=
=

s0

X

x0 ;y0
X

y0

X

x0

P (y0 jx; y; a)P (x0 jx; a)R(x0 jx; a)

P (y0 jx; y; a)

X

x0

P (x0 jx; a)R(x0 jx; a)

P (x0 jx; a)R(x0 jx; a)

Hence, the expected reward for the action a depends only on the variables in X and not on
the variables in Y . Q.E.D.
The second rule shows that if the reward function for a primitive action is constant,
then we can apply state abstractions even if P (s0 js; a) does not factor.

Lemma 6 Suppose R(s0js; a) (the reward function for action a in MDP M ) is always equal

to a constant ra . Then the entire state s is irrelevant to the primitive action a.
266

fiMAXQ Hierarchical Reinforcement Learning

Proof:
V (a; s) =

X

s0

P (s0 js; a)R(s0 js; a)

X

=
P (s0 js; a)ra
0
s
= ra :
This does not depend on s, so the entire state is irrelevant to the primitive action a. Q.E.D.
This lemma is satisfied by the four leaf nodes North, South, East, and West in the taxi
task, because their one-step reward is a constant (,1). Hence, instead of requiring 2000
values to store the V functions, we only need 4 values|one for each action. Similarly, the
expected rewards of the Pickup and Putdown actions each require only 2 values, depending
on whether the corresponding actions are legal or illegal. Hence, together, they require 4
values, instead of 1000 values.
5.1.3 Condition 3: Result Distribution Irrelevance

Now we consider a condition that results from \funnel" actions.

Definition 14 (Result Distribution Irrelevance). A set of state variables Yj is irrelevant for the result distribution of action j if, for all abstract policies  executed by node j
and its descendents in the MAXQ hierarchy, the following holds: for all pairs of states s1
and s2 that differ only in their values for the state variables in Yj ,

P  (s0 ; N js1 ; j ) = P  (s0 ; N js2 ; j )
for all s0 and N .

If this condition is satisfied for subtask j , then the C value of its parent task i can be
represented compactly:

Lemma 7 Let M be an MDP with full-state MAXQ graph H , and suppose that the set of

state variables Yj is irrelevant to the result distribution of action j , which is a child of Max
node i. Let ij be the associated abstraction function: ij (s) = x. Then we can define an
abstract completion cost function C  (i; ij (s); j ) such that for all states s,

C  (i; s; j ) = C  (i; ij (s); j ):

Proof: The completion function for fixed policy  is defined as follows:
X
C  (i; s; j ) = P (s0 ; N js; j )   N Q (i; s0 ):
s0 ;N

(21)

Consider any two states s1 and s2 , such that ij (s1 ) = ij (s2 ) = x. Under Result Distribution Irrelevance, their transition probability distributions are the same. Hence, the
right-hand sides of (21) have the same value, and we can conclude that

C  (i; s1 ; j ) = C  (i; s2 ; j ):
267

fiDietterich

Therefore, we can define an abstract completion function, C  (i; x; j ) to represent this quantity. Q.E.D.
In undiscounted cumulative reward problems, the definition of result distribution irrelevance can be weakened to eliminate N , the number of steps. All that is needed is
that for all pairs of states s1 and s2 that differ only in the irrelevant state variables,
P  (s0 js1 ; j ) = P  (s0 js2; j ) (for all s0 ). In the undiscounted case, Lemma 7 still holds under
this revised definition.
It might appear that the result distribution irrelevance condition would rarely be satisfied, but we often find cases where the condition is true. Consider, for example, the Get
subroutine for the taxi task. No matter what location the taxi has in state s, the taxi
will be at the passenger's starting location when the Get finishes executing (i.e., because
the taxi will have just completed picking up the passenger). Hence, the starting location
is irrelevant to the resulting location of the taxi, and P (s0 js1 ; Get) = P (s0 js2 ; Get) for all
states s1 and s2 that differ only in the taxi's location.
Note, however, that if we were maximizing discounted reward, the taxi's location would
not be irrelevant, because the probability that Get will terminate in exactly N steps would
depend on the location of the taxi, which could differ in states s1 and s2 . Different values
of N will produce different amounts of discounting in (21), and hence, we cannot ignore the
taxi location when representing the completion function for Get.
But in the undiscounted case, by applying Lemma 7, we can represent C (Root; s; Get)
using 16 distinct values, because there are 16 equivalence classes of states (4 source locations
times 4 destination locations). This is much less than the 500 quantities in the unabstracted
representation.
Note that although state variables Y may be irrelevant to the result distribution of a
subtask j , they may be important within subtask j . In the Taxi task, the location of the
taxi is critical for representing the value of V (Get; s), but it is irrelevant to the result state
distribution for Get, and therefore it is irrelevant for representing C (Root; s; Get). Hence, the
MAXQ decomposition is essential for obtaining the benefits of result distribution irrelevance.
\Funnel" actions arise in many hierarchical reinforcement learning problems. For example, abstract actions that move a robot to a doorway or that move a car onto the entrance
ramp of a freeway have this property. The Result Distribution Irrelevance condition is
applicable in all such situations as long as we are in the undiscounted setting.
5.1.4 Condition 4: Termination

The fourth condition is closely related to the \funnel" property. It applies when a subtask
is guaranteed to cause its parent task to terminate in a goal state. In a sense, the subtask
is funneling the environment into the set of states described by the goal predicate of the
parent task.

Lemma 8 (Termination). Let Mi be a task in a MAXQ graph such that for all states s
where the goal predicate Gi (s) is true, the pseudo-reward function R~ i (s) = 0. Suppose there
is a child task a and state s such that for all hierarchical policies ,

8 s0 Pi (s0; N js; a) > 0 ) Gi(s0 ):
268

fiMAXQ Hierarchical Reinforcement Learning

(i.e., every possible state s0 that results from applying a in s will make the goal predicate,
Gi , true.)
Then for any policy executed at node i, the completion cost C (i; s; a) is zero and does
not need to be explicitly represented.

Proof: When action a is executed in state s, it is guaranteed to result in a state s0 such

that Gi (s) is true. By definition, goal states also satisfy the termination predicate Ti (s), so
task i will terminate. Because Gi(s) is true, the terminal pseudo-reward will be zero, and
hence, the completion function will always be zero. Q.E.D.
For example, in the Taxi task, in all states where the taxi is holding the passenger, the
Put subroutine will succeed and result in a goal terminal state for Root. This is because the
termination predicate for Put (i.e., that the passenger is at his or her destination location)
implies the goal condition for Root (which is the same). This means that C (Root; s; Put) is
uniformly zero, for all states s where Put is not terminated.
It is easy to detect cases where the Termination condition is satisfied. We only need to
compare the termination predicate Ta of a subtask with the goal predicate Gi of the parent
task. If the first implies the second, then the termination lemma is satisfied.
5.1.5 Condition 5: Shielding

The shielding condition arises from the structure of the MAXQ graph.
Lemma 9 (Shielding). Let Mi be a task in a MAXQ graph and s be a state such that in
all paths from the root of the graph down to node Mi there is a subtask j (possibly equal to i)
whose termination predicate Tj (s) is true, then the Q nodes of Mi do not need to represent
C values for state s.
Proof: In order for task i to be executed in state s, there must exist some path of ancestors
of task i leading up to the root of the graph such that all of those ancestor tasks are not
terminated. The condition of the lemma guarantees that this is false, and hence that task
i cannot be executed in state s. Therefore, no C values need to be represented. Q.E.D.
As with the Termination condition, the Shielding condition can be verified by analyzing
the structure of the MAXQ graph and identifying nodes whose ancestor tasks are terminated.
In the Taxi domain, a simple example of this arises in the Put task, which is terminated
in all states where the passenger is not in the taxi. This means that we do not need
to represent C (Root; s; Put) in these states. The result is that, when combined with the
Termination condition above, we do not need to explicitly represent the completion function
for Put at all!
5.1.6 Dicussion

By applying these five abstraction conditions, we obtain the following \safe" state abstractions for the Taxi task:
 North, South, East, and West. These terminal nodes require one quantity each, for a
total of four values. (Leaf Irrelevance).
269

fiDietterich

 Pickup and Putdown each require 2 values (legal and illegal states), for a total of four.
(Leaf Irrelevance.)

 QNorth(t), QSouth(t), QEast(t), and QWest(t) each require 100 values (four values for
t and 25 locations). (Max Node Irrelevance.)

 QNavigateForGet requires 4 values (for the four possible source locations). (The passenger destination is Max Node Irrelevant for MaxGet, and the taxi starting location
is Result Distribution Irrelevant for the Navigate action.)

 QPickup requires 100 possible values, 4 possible source locations and 25 possible taxi
locations. (Passenger destination is Max Node Irrelevant to MaxGet.)

 QGet requires 16 possible values (4 source locations, 4 destination locations). (Result
Distribution Irrelevance.)

 QNavigateForPut requires only 4 values (for the four possible destination locations).

(The passenger source and destination are Max Node Irrelevant to MaxPut; the taxi
location is Result Distribution Irrelevant for the Navigate action.)

 QPutdown requires 100 possible values (25 taxi locations, 4 possible destination locations). (Passenger source is Max Node Irrelevant for MaxPut.)

 QPut requires 0 values. (Termination and Shielding.)
This gives a total of 632 distinct values, which is much less than the 3000 values required
by at Q learning. Hence, we can see that by applying state abstractions, the MAXQ
representation can give a much more compact representation of the value function.
A key thing to note is that with these state abstractions, the value function is decomposed into a sum of terms such that no single term depends on the entire state of the MDP,
even though the value function as a whole does depend on the entire state of the MDP. For
example, consider again the state described in Figures 1 and 4. There, we showed that the
value of a state s1 with the passenger at R, the destination at B, and the taxi at (0,3) can
be decomposed as

V (Root; s1 ) = V (North; s1 ) + C (Navigate(R); s1 ; North) +
C (Get; s1 ; Navigate(R)) + C (Root; s1 ; Get)
With state abstractions, we can see that each term on the right-hand side only depends on
a subset of the features:

 V (North; s1) is a constant
 C (Navigate(R); s1 ; North) depends only on the taxi location and the passenger's source
location.

 C (Get; s1; Navigate(R)) depends only on the source location.
 C (Root; s1 ; Get) depends only on the passenger's source and destination.
270

fiMAXQ Hierarchical Reinforcement Learning

Without the MAXQ decomposition, no features are irrelevant, and the value function depends on the entire state.
What prior knowledge is required on the part of a programmer in order to identify
these state abstractions? It suces to know some qualitative constraints on the one-step
reward functions, the one-step transition probabilities, and termination predicates, goal
predicates, and pseudo-reward functions within the MAXQ graph. Specifically, the Max
Node Irrelevance and Leaf Irrelevance conditions require simple analysis of the one-step
transition function and the reward and pseudo-reward functions. Opportunities to apply
the Result Distribution Irrelevance condition can be found by identifying \funnel" effects
that result from the definitions of the termination conditions for operators. Similarly, the
Shielding and Termination conditions only require analysis of the termination predicates of
the various subtasks. Hence, applying these five conditions to introduce state abstractions is
a straightforward process, and once a model of the one-step transition and reward functions
has been learned, the abstraction conditions can be checked to see if they are satisfied.

5.2 Convergence of MAXQ-Q with State Abstraction

We have shown that state abstractions can be safely introduced into the MAXQ value
function decomposition under the five conditions described above. However, these conditions only guarantee that the value function of any fixed abstract hierarchical policy can be
represented|they do not show that recursively optimal policies can be represented, nor do
they show that the MAXQ-Q learning algorithm will find a recursively optimal policy when
it is forced to use these state abstractions. The goal of this section is to prove these two
results: (a) that the ordered recursively-optimal policy is an abstract policy (and, hence,
can be represented using state abstractions) and (b) that MAXQ-Q will converge to this
policy when applied to a MAXQ graph with safe state abstractions.

Lemma 10 Let M be an MDP with full-state MAXQ graph H and abstract-state MAXQ
graph (H ) where the abstractions satisfy the five conditions given above. Let ! be an
ordering over all actions in the MAXQ graph. Then the following statements are true:
 The unique ordered recursively-optimal policy r defined by M , H , and ! is an abstract policy (i.e., it depends only on the relevant state variables at each node; see
Definition 11),
 The C and V functions in (H ) can represent the projected value function of r.

Proof: The five abstraction lemmas tell us that if the ordered recursively-optimal policy
is abstract, then the C and V functions of (H ) can represent its value function. Hence,
the heart of this lemma is the first claim. The last two forms of abstraction (Shielding and
Termination) do not place any restrictions on abstract policies, so we ignore them in this
proof.
The proof is by induction on the levels of the MAXQ graph, starting at the leaves. As
a base case, let us consider a Max node i all of whose children are primitive actions. In this
case, there are no policies executed within the children of the Max node. Hence if variables
Yi are irrelevant for node i, then we can apply our abstraction lemmas to represent the
value function of any policy at node i|not just abstract policies. Consequently, the value
271

fiDietterich

function of any optimal policy for node i can be represented, and it will have the property
that
Q(i; s1 ; a) = Q (i; s2 ; a)
(22)
for any states s1 and s2 such that i (s1 ) = i (s2 ).
Now let us impose the action ordering ! to compute the optimal ordered policy. Consider
two actions a1 and a2 such that !(a1 ; a2 ) (i.e., ! prefers a1 ), and suppose that there is a
\tie" in the Q function at state s1 such that the values

Q (i; s1 ; a1 ) = Q (i; s1 ; a2 )
and they are the only two actions that maximize Q in this state. Then the optimal ordered
policy must choose a1 . Now in all other states s2 such that i (s1 ) = i (s2 ), we have just
established in (22) that the Q values will be the same. Hence, the same tie will exist
between a1 and a2 , and hence, the optimal ordered policy must make the same choice in all
such states. Hence, the optimal ordered policy for node i is an abstract policy.
Now let us turn to the recursive case at Max node i. Make the inductive assumption that
the ordered recursively-optimal policy is abstract within all descendent nodes and consider
the locally optimal policy at node i. If Y is a set of state variables that are irrelevant to
node i, Corollary 2 tells us that Q (i; s1 ; j ) = Q (i; s2 ; j ) for all states s1 and s2 such that
i(s1) = i (s2 ). Similarly, if Y is a set of variables irrelevant to the result distribution of
a particular action j , then Lemma 7 tells us the same thing. Hence, by the same ordering
argument given above, the ordered optimal policy at node i must be abstract. By induction,
this proves the lemma. Q.E.D.
With this lemma, we have established that the combination of an MDP M , an abstract
MAXQ graph H , and an action ordering defines a unique recursively-optimal ordered abstract policy. We are now ready to prove that MAXQ-Q will converge to this policy.

Theorem 4 Let M = hS; A; P; R; P0 i be either an episodic MDP for which all deterministic

policies are proper or a discounted infinite horizon MDP with discount factor  < 1. Let H
be an unabstracted MAXQ graph defined over subtasks fM0 ; : : : ; Mk g with pseudo-reward
functions R~ i (s0 ). Let (H ) be a state-abstracted MAXQ graph defined by applying state
abstractions i to each node i of H under the five conditions given above. Let x (i; i (s))
be an abstract ordered GLIE exploration policy at each node i and state s whose decisions
depend only on the \relevant" state variables at each node i. Let r be the unique recursivelyoptimal hierarchical policy defined by x , M , and R~ . Then with probability 1, algorithm
MAXQ-Q applied to (H ) converges to r provided that the learning rates fft (i) satisfy
Equation (15) and the one-step rewards are bounded.

Proof: Rather than repeating the entire proof for MAXQ-Q, we will only describe what

must change under state abstraction. The last two forms of state abstraction refer to states
whose values can be inferred from the structure of the MAXQ graph, and therefore do not
need to be represented at all. Since these values are not updated by MAXQ-Q, we can
ignore them. We will now consider the first three forms of state abstraction in turn.
We begin by considering primitive leaf nodes. Let a be a leaf node and let Y be a set of
state variables that are Leaf Irrelevant for a. Let s1 = (x; y1 ) and s2 = (x; y2 ) be two states
272

fiMAXQ Hierarchical Reinforcement Learning

that differ only in their values for Y . Under Leaf Irrelevance, the probability transitions
P (s01 js1 ; a) and P (s02 js2 ; a) need not be the same, but the expected reward of performing a
in both states must be the same. When MAXQ-Q visits an abstract state x, it does not
\know" the value of y, the part of the state that has been abstracted away. Nonetheless,
it draws a sample according to P (s0 jx; y; a), receives a reward R(s0 jx; y; a), and updates
its estimate of V (a; x) (line 4 of MAXQ-Q). Let Pt (y) be the probability that MAXQ-Q is
visiting (x; y) given that the unabstracted part of the state is x. Then Line 4 of MAXQ-Q
is computing a stochastic approximation to
X

s0 ;N;y

We can write this as

X

y

Pt (y)Pt (s0 ; N jx; y; a)R(s0 jx; y; a):

Pt (y)

X

s0 ;N

Pt (s0 ; N jx; y; a)R(s0 jx; y; a):

According to Leaf Irrelevance, the inner sum has the same value for all states s such that
(s) = x. Call this value r0 (x). This gives
X

y

Pt (y)r0 (x);

which is equal to r0 (x) for any distribution Pt (y). Hence, MAXQ-Q converges under Leaf
Irrelevance abstractions.
Now let us turn to the two forms of abstraction that apply to internal nodes: Max Node
Irrelevance and Result Distribution Irrelevance. Consider the SMDP defined at each node i
of the abstracted MAXQ graph at time t during MAXQ-Q. This would be an ordinary SMDP
with transition probability function Pt (x0 ; N jx; a) and reward function Vt (a; x) + R~ i (x0 )
except that when MAXQ-Q draws samples of state transitions, they are drawn according to
the distribution Pt (s0 ; N js; a) over the original state space. To prove the theorem, we must
show that drawing (s0 ; N ) according to this second distribution is equivalent to drawing
(x0 ; N ) according to the first distribution.
For Max Node Irrelevance, we know that for all abstract policies applied to node i and
its descendents, the transition probability distribution factors as

P (s0 ; N js; a) = P (y0 jx; y; a)P (x0 ; N jx; a):
Because the exploration policy is an abstract policy, Pt (s0 ; N js; a) factors in this way. This
means that the Yi components of the state cannot affect the Xi components, and hence,
sampling from Pt (s0 ; N js; a) and discarding the Yi values gives samples for Pt (x0 ; N jx; a).
Therefore, MAXQ-Q will converge under Max Node Irrelevance abstractions.
Finally, consider Result Distribution Irrelevance. Let j be a child of node i, and suppose
Yj is a set of state variables that are irrelevant to the result distribution of j . When
the SMDP at node i wishes to draw a sample from Pt (x0 ; N jx; j ), it does not \know"
the current value of y, the irrelevant part of the current state. However, this does not
matter, because Result Distribution Irrelevance means that for all possible values of y,
Pt (x0 ; y0; N jx; y; j ) is the same. Hence, MAXQ-Q will converge under Result Distribution
Irrelevance abstractions.
273

fiDietterich

In each of these three cases, MAXQ-Q will converge to a locally-optimal ordered policy
at node i in the MAXQ graph. By Lemma 10, this produces a locally-optimal ordered
policy for the unabstracted SMDP at node i. Hence, by induction, MAXQ-Q will converge
to the unique ordered recursively optimal policy r defined by MAXQ-Q H , MDP M , and
ordered exploration policy x . Q.E.D.

5.3 The Hierarchical Credit Assignment Problem

There are still some situations where we would like to introduce state abstractions but
where the five properties described above do not permit them. Consider the following
modification of the taxi problem. Suppose that the taxi has a fuel tank and that each time
the taxi moves one square, it costs one unit of fuel. If the taxi runs out of fuel before
delivering the passenger to his or her destination, it receives a reward of ,20, and the trial
ends. Fortunately, there is a filling station where the taxi can execute a Fillup action to fill
the fuel tank.
To solve this modified problem using the MAXQ hierarchy, we can introduce another
subtask, Refuel, which has the goal of moving the taxi to the filling station and filling the
tank. MaxRefuel is a child of MaxRoot, and it invokes Navigate(t) (with t bound to the
location of the filling station) to move the taxi to the filling station.
The introduction of fuel and the possibility that we might run out of fuel means that
we must include the current amount of fuel as a feature in representing every C value
(for internal nodes) and V value (for leaf nodes) throughout the MAXQ graph. This is
unfortunate, because our intuition tells us that the amount of fuel should have no inuence
on our decisions inside the Navigate(t) subtask. That is, either the taxi will have enough
fuel to reach the target t (in which case, the chosen navigation actions do not depend on the
fuel), or else the taxi will not have enough fuel, and hence, it will fail to reach t regardless
of what navigation actions are taken. In other words, the Navigate(t) subtask should not
need to worry about the amount of fuel, because even if there is not enough fuel, there is
no action that Navigate(t) can take to get more fuel. Instead, it is the top-level subtasks
that should be monitoring the amount of fuel and deciding whether to go refuel, to go pick
up the passenger, or to go deliver the passenger.
Given this intuition, it is natural to try abstracting away the \amount of remaining
fuel" within the Navigate(t) subtask. However, this doesn't work, because when the taxi
runs out of fuel and a ,20 reward is given, the QNorth, QSouth, QEast, and QWest nodes
cannot \explain" why this reward was received|that is, they have no consistent way of
setting their C tables to predict when this negative reward will occur, because their C
values ignore the amount of fuel in the tank. Stated more formally, the diculty is that
the Max Node Irrelevance condition is not satisfied because the one-step reward function
R(s0js; a) for these actions depends on the amount of fuel.
We call this the hierarchical credit assignment problem. The fundamental issue here is
that in the MAXQ decomposition all information about rewards is stored in the leaf nodes
of the hierarchy. We would like to separate out the basic rewards received for navigation
(i.e., ,1 for each action) from the reward received for exhausting fuel (,20). If we make the
reward at the leaves only depend on the location of the taxi, then the Max Node Irrelevance
condition will be satisfied.
274

fiMAXQ Hierarchical Reinforcement Learning

One way to do this is to have the programmer manually decompose the reward function
and
indicate which nodes in the hierarchy will \receive" each reward. Let R(s0 js; a) =
P
0
0
i R(i; s js; a) be a decomposition of the reward function, such that R(i; s js; a) specifies
that part of the reward that must be handled by Max node i. In the modified taxi problem,
for example, we can decompose the reward so that the leaf nodes receive all of the original
penalties, but the out-of-fuel rewards must be handled by MaxRoot. Lines 15 and 16 of the
MAXQ-Q algorithm are easily modified to include R(i; s0 js; a).
In most domains, we believe it will be easy for the designer of the hierarchy to decompose
the reward function. It has been straightforward in all of the problems we have studied.
However, an interesting problem for future research is to develop an algorithm that can
solve the hierarchical credit assignment problem autonomously.

6. Non-Hierarchical Execution of the MAXQ Hierarchy

Up to this point in the paper, we have focused exclusively on representing and learning
hierarchical policies. However, often the optimal policy for a MDP is not strictly hierarchical. Kaelbling (1993) first introduced the idea of deriving a non-hierarchical policy from the
value function of a hierarchical policy. In this section, we exploit the MAXQ decomposition
to generalize her ideas and apply them recursively at all levels of the hierarchy. We will
describe two methods for non-hierarchical execution.
The first method is based on the dynamic programming algorithm known as policy
iteration. The policy iteration algorithm starts with an initial policy 0 . It then repeats
the following two steps until the policy converges. In the policy evaluation step, it computes
the value function V k of the current policy k . Then, in the policy improvement step, it
computes a new policy, k+1 according to the rule

k+1(s) := argmax
a

X

s0

P (s0 js; a)[R(s0 js; a) + V k (s0 )]:

(23)

Howard (1960) proved that if k is not an optimal policy, then k+1 is guaranteed to be
an improvement. Note that in order to apply this method, we need to know the transition
probability distribution P (s0 js; a) and the reward function R(s0 js; a).
If we know P (s0 js; a) and R(s0 js; a), we can use the MAXQ representation of the value
function to perform one step of policy iteration. We start with a hierarchical policy  and
represent its value function using the MAXQ hierarchy (e.g.,  could have been learned via
MAXQ-Q). Then, we can perform one step of policy improvement by applying Equation (23)
using V  (0; s0 ) (computed by the MAXQ hierarchy) to compute V  (s0 ).

Corollary 3 Let g (s) = argmaxa Ps0 P (s0js; a)[R(s0 js; a) + V  (0; s)], where V  (0; s) is
the value function computed by the MAXQ hierarchy and a is a primitive action. Then, if
 was not an optimal policy, g is strictly better for at least one state in S .

Proof: This is a direct consequence of Howard's policy improvement theorem. Q.E.D.
Unfortunately, we can't iterate this policy improvement process, because the new policy,

g is very unlikely to be a hierarchical policy (i.e., it is unlikely to be representable in
275

fiDietterich

Table 5: The procedure for executing the one-step greedy policy.
procedure ExecuteHGPolicy(s)
1
repeat
2
Let hV (0; s); ai := EvaluateMaxNode(0; s)
3
4

execute primitive action a
Let s be the resulting state
end // ExecuteHGPolicy

terms of local policies for each node of the MAXQ graph). Nonetheless, one step of policy
improvement can give very significant improvements.
This approach to non-hierarchical execution ignores the internal structure of the MAXQ
graph. In effect, the MAXQ hierarchy is just viewed as a way to represent V  |any other
representation would give the same one-step improved policy g .
The second approach to non-hierarchical execution borrows an idea from Q learning.
One of the great beauties of the Q representation for value functions is that we can compute
one step of policy improvement without knowing P (s0 js; a), simply by taking the new policy
to be g (s) := argmaxa Q(s; a). This gives us the same one-step greedy policy as we
computed above using one-step lookahead. With the MAXQ decomposition, we can perform
these policy improvement steps at all levels of the hierarchy.
We have already defined the function that we need. In Table 3 we presented the function
EvaluateMaxNode, which, given the current state s, conducts a search along all paths
from a given Max node i to the leaves of the MAXQ graph and finds the path with the
best value (i.e., with the maximum sum of C values along the path, plus the V value at
the leaf). This is equivalent to computing the best action greedily at each level of the
MAXQ graph. In addition, EvaluateMaxNode returns the primitive action a at the end
of this best path. This action a would be the first primitive action to be executed if the
learned hierarchical policy were executed starting in the current state s. Our second method
for non-hierarchical execution of the MAXQ graph is to call EvaluateMaxNode in each
state, and execute the primitive action a that is returned. The pseudo-code is shown in
Table 5.
We will call the policy computed by ExecuteHGPolicy the hierarchical greedy policy,
and denote it hg , where the superscript * indicates that we are computing the greedy
action at each time step. The following theorem shows that this can give a better policy
than the original, hierarchical policy.

Theorem 5 Let G be a MAXQ graph representing the value function of hierarchical policy

 (i.e., in terms of C  (i; s; j ), computed for all i; s, and j ). Let V hg (0; s) be the value
computed by ExecuteHGPolicy (line 2), and let hg be the resulting policy. Define
V hg to be the value function of hg. Then for all states s, it is the case that
V  (s)  V hg (0; s)  V hg (s):
(24)

Proof: (sketch) The left inequality in Equation (24) is satisfied by construction by line 6
of EvaluateMaxNode. To see this, consider that the original hierarchical policy, , can
276

fiMAXQ Hierarchical Reinforcement Learning

be viewed as choosing a \path" through the MAXQ graph running from the root to one of
the leaf nodes, and V  (0; s) is the sum of the C  values along this chosen path (plus the
V  value at the leaf node). In contrast, EvaluateMaxNode performs a traversal of all
paths through the MAXQ graph and finds the best path, that is, the path with the largest
sum of C  (and leaf V  ) values. Hence, V hg (0; s) must be at least as large as V  (0; s).
To establish the right inequality, note that by construction V hg (0; s) is the value function
of a policy, call it hg , that chooses one action greedily at each level of the MAXQ graph
(recursively), and then follows  thereafter. This is a consequence of the fact that line
6 of EvaluateMaxNode has C  on its right-hand side, and C  represents the cost of
\completing" each subroutine by following , not by following some other, greedier, policy.
(In Table 3, C  is written as Ct .) However, when we execute ExecuteHGPolicy (and
hence, execute hg ), we have an opportunity to improve upon  and hg at each time step.
Hence, V hg (0; s) is an underestimate of the actual value of hg . Q.E.D.
Note that this theorem only works in one direction. It says that if we can find a state
where V hg (0; s) > V  (s), then the greedy policy, hg , will be strictly better than .
However, it could be that  is not an optimal policy and yet the structure of the MAXQ
graph prevents us from considering an action (either primitive or composite) that would
improve . Hence, unlike the policy improvement theorem of Howard (where all primitive
actions are always eligible to be chosen), we do not have a guarantee that if  is suboptimal,
then the hierarchically greedy policy is a strict improvement.
In contrast, if we perform one-step policy improvement as discussed at the start of this
section, Corollary 3 guarantees that we will improve the policy. So we can see that in
general, neither of these two methods for non-hierarchical execution is always better than
the other. Nonetheless, the first method only operates at the level of individual primitive
actions, so it is not able to produce very large improvements in the policy. In contrast, the
hierarchical greedy method can obtain very large improvements in the policy by changing
which actions (i.e., subroutines) are chosen near the root of the hierarchy. Hence, in general,
hierarchical greedy execution is probably the better method. (Of course, the value functions
of both methods could be computed, and the one with the better estimated value could be
executed.)
Sutton, et al. (1999) have simultaneously developed a closely-related method for nonhierarchical execution of macros. Their method is equivalent to ExecuteHGPolicy for
the special case where the MAXQ hierarchy has only one level of subtasks. The interesting
aspect of ExecuteHGPolicy is that it permits greedy improvements at all levels of the
tree to inuence which action is chosen.
Some care must be taken in applying Theorem 5 to a MAXQ hierarchy whose C values
have been learned via MAXQ-Q. Being an online algorithm, MAXQ-Q will not have correctly learned the values of all states at all nodes of the MAXQ graph. For example, in the
taxi problem, the value of C (Put; s; QPutdown) will not have been learned very well except
at the four special locations R, G, B, and Y. This is because the Put subtask cannot be
executed until the passenger is in the taxi, and this usually means that a Get has just been
completed, so the taxi is at the passenger's source location. During exploration, both children of Put will be tried in such states. The PutDown will usually fail (and receive a negative
reward), whereas the Navigate will eventually succeed (perhaps after lengthy exploration)
277

fiDietterich

and take the taxi to the destination location. Now because of all-states updating, the values
for C (Put; s; Navigate(t)) will have been learned at all of the states along the path to the
passenger's destination, but the C values for the Putdown action will only be learned for
the passenger's source and destination locations. Hence, if we train the MAXQ representation using hierarchical execution (as in MAXQ-Q), and then switch to hierarchically-greedy
execution, the results will be quite bad. In particular, we need to introduce hierarchicallygreedy execution early enough so that the exploration policy is still actively exploring. (In
theory, a GLIE exploration policy never ceases to explore, but in practice, we want to find
a good policy quickly, not just asymptotically).
Of course an alternative would be to use hierarchically-greedy execution from the very
beginning of learning. However, remember that the higher nodes in the MAXQ hierarchy
need to obtain samples of P (s0 ; N js; a) for each child action a. If the hierarchical greedy
execution interrupts child a before it has reached a terminal state (i.e., because at some state
along the way, another subtask appears better to EvaluateMaxNode), then these samples
cannot be obtained. Hence, it is important to begin with purely hierarchical execution
during training, and make a transition to greedy execution at some point.
The approach we have taken is to implement MAXQ-Q in such a way that we can
specify a number of primitive actions L that can be taken hierarchically before the hierarchical execution is \interrupted" and control returns to the top level (where a new action
can be chosen greedily). We start with L set very large, so that execution is completely
hierarchical|when a child action is invoked, we are committed to execute that action until
it terminates. However, gradually, we reduce L until it becomes 1, at which point we have
hierarchical greedy execution. We time this so that it reaches 1 at about the same time our
Boltzmann exploration cools to a temperature of 0.1 (which is where exploration effectively
has halted). As the experimental results will show, this generally gives excellent results
with very little added exploration cost.

7. Experimental Evaluation of the MAXQ Method
We have performed a series of experiments with the MAXQ method with three goals in
mind: (a) to understand the expressive power of the value function decomposition, (b) to
characterize the behavior of the MAXQ-Q learning algorithm, and (c) to assess the relative
importance of temporal abstraction, state abstraction, and non-hierarchical execution. In
this section, we describe these experiments and present the results.

7.1 The Fickle Taxi Task
Our first experiments were performed on a modified version of the taxi task. This version
incorporates two changes to the task described in Section 3.1. First, each of the four
navigation actions is noisy, so that with probability 0.8 it moves in the intended direction,
but with probability 0.1 it instead moves to the right (of the intended direction) and with
probability 0.1 it moves to the left. The purpose of this change is to create a more realistic
and more dicult challenge for the learning algorithms. The second change is that after the
taxi has picked up the passenger and moved one square away from the passenger's source
location, the passenger changes his or her destination location with probability 0.3. The
278

fiMAXQ Hierarchical Reinforcement Learning

purpose of this change is to create a situation where the optimal policy is not a hierarchical
policy so that the effectiveness of non-hierarchical execution can be measured.
We compared four different configurations of the learning algorithm: (a) at Q learning,
(b) MAXQ-Q learning without any form of state abstraction, (c) MAXQ-Q learning with
state abstraction, and (d) MAXQ-Q learning with state abstraction and greedy execution.
These configurations are controlled by many parameters. These include the following: (a)
the initial values of the Q and C functions, (b) the learning rate (we employed a fixed
learning rate), (c) the cooling schedule for Boltzmann exploration (the GLIE policy that we
employed), and (d) for non-hierarchical execution, the schedule for decreasing L, the number
of steps of consecutive hierarchical execution. We optimized these settings separately for
each configuration with the goal of matching or exceeding (with as few primitive training
actions as possible) the best policy that we could code by hand. For Boltzmann exploration,
we established an initial temperature and then a cooling rate. A separate temperature is
maintained for each Max node in the MAXQ graph, and its temperature is reduced by
multiplying by the cooling rate each time that subtask terminates in a goal state.
The process of optimizing the parameter settings for each algorithm is time-consuming,
both for at Q learning and for MAXQ-Q. The most critical parameter is the schedule
for cooling the temperature of Boltzmann exploration: if this is cooled too rapidly, then
the algorithms will converge to a suboptimal policy. In each case, we tested nine different
cooling rates. To choose the different cooling rates for the various subtasks, we started by
using fixed policies (e.g., either random or hand-coded) for all subtasks except the subtasks
closest to the leaves. Then, once we had chosen schedules for those subtasks, we allowed
their parent tasks to learn their policies while we tuned their cooling rates, and so on. One
nice effect of our method of cooling the temperature only when a subtask terminates is that
it naturally causes the subtasks higher in the MAXQ graph to cool more slowly. This meant
that good results could often be obtained just by using the same cooling rate for all Max
nodes.
The choice of learning rate is easier, since it is determined primarily by the degree
of stochasticity in the environment. We only tested three or four different rates for each
configuration. The initial values for the Q and C functions were set based on our knowledge
of the problems|no experiments were required.
We took more care in tuning these parameters for these experiments than one would
normally take in a real application, because we wanted to ensure that each method was
compared under the best possible conditions. The general form of the results (particularly
the speed of learning) is the same for wide ranges of the cooling rate and learning rate
parameter settings.
The following parameters were selected based on the tuning experiments. For at Q
learning: initial Q values of 0.123 in all states, learning rate 0.25, and Boltzmann exploration
with an initial temperature of 50 and a cooling rate of 0.9879. (We use initial values that
end in .123 as a \signature" during debugging to detect when a weight has been modified.)
For MAXQ-Q learning without state abstraction, we used initial values of 0.123, a learning rate of 0.50, and Boltzmann exploration with an initial temperature of 50 and cooling
rates of 0.9996 at MaxRoot and MaxPut, 0.9939 at MaxGet, and 0.9879 at MaxNavigate.
279

fiDietterich

200
MAXQ Abstract

Mean Cumulative Reward

0
MAXQ
Abstract+
Greedy

-200

MAXQ
No Abstract

-400

Flat Q

-600

-800

-1000
0

20000

40000

60000
80000
100000
Primitive Actions

120000

140000

Figure 8: Comparison of performance of hierarchical MAXQ-Q learning (without state abstractions, with state abstractions, and with state abstractions combined with
hierarchical greedy evaluation) to at Q learning.
For MAXQ-Q learning with state abstraction, we used initial values of 0.123, a learning
rate of 0.25, and Boltzmann exploration with an initial temperature of 50 and cooling rates
of 0.9074 at MaxRoot, 0.9526 at MaxPut, 0.9526 at MaxGet, and 0.9879 at MaxNavigate.
For MAXQ-Q learning with non-hierarchical execution, we used the same settings as
with state abstraction. In addition, we initialized L to 500 and decreased it by 10 with each
trial until it reached 1. So after 50 trials, execution was completely greedy.
Figure 8 shows the averaged results of 100 training runs. Each training run involves
performing repeated trials until convergence. Because the different trials execute different
numbers of primitive actions, we have just plotted the number of primitive actions on the
horizontal axis rather than the number of trials.
The first thing to note is that all forms of MAXQ learning have better initial performance
than at Q learning. This is because of the constraints introduced by the MAXQ hierarchy.
For example, while the agent is executing a Navigate subtask, it will never attempt to pickup
or putdown the passenger, because those actions are not available to Navigate. Similarly, the
agent will never attempt to putdown the passenger until it has first picked up the passenger
(and vice versa) because of the termination conditions of the Get and Put subtasks.
The second thing to notice is that without state abstractions, MAXQ-Q learning actually takes longer to converge, so that the Flat Q curve crosses the MAXQ/no abstraction
280

fiMAXQ Hierarchical Reinforcement Learning

curve. This shows that without state abstraction, the cost of learning the huge number
of parameters in the MAXQ representation is not really worth the benefits. We suspect
this is a consequence of the model-free nature of the MAXQ-Q algorithm. The MAXQ decomposition represents some information redundantly. For example, the cost of performing
a Put subtask is computed both as C (Root; s; Get) and also as V (Put; s). A model-based
algorithm could compute both of these from a learned model, but MAXQ-Q must learn
each of them separately from experience.
The third thing to notice is that with state abstractions, MAXQ-Q converges very
quickly to a hierarchically optimal policy. This can be seen more clearly in Figure 9, which
focuses on the range of reward values in the neighborhood of the optimal policy. Here
we can see that MAXQ with abstractions attains the hierarchically optimal policy after
approximately 40,000 steps, whereas at Q learning requires roughly twice as long to reach
the same level. However, at Q learning, of course, can continue onward and reach optimal
performance, whereas with the MAXQ hierarchy, the best hierarchical policy is slow to
respond to the \fickle" behavior of the passenger when he/she changes the destination.
The last thing to notice is that with greedy execution, the MAXQ policy is also able
to attain optimal performance. But as the execution becomes \more greedy", there is a
temporary drop in performance, because MAXQ-Q must learn C values in new regions
of the state space that were not visited by the recursively optimal policy. Despite this
drop in performance, greedy MAXQ-Q recovers rapidly and reaches hierarchically optimal
performance faster than purely-hierarchical MAXQ-Q learning. Hence, there is no added
cost|in terms of exploration|for introducing greedy execution.
This experiment presents evidence in favor of three claims: first, that hierarchical reinforcement learning can be much faster than at Q learning; second, that state abstraction
is required by MAXQ-Q learning for good performance; and third, that non-hierarchical
execution can produce significant improvements in performance with little or no added
exploration cost.

7.2 Kaelbling's HDG Method
The second task that we will consider is a simple maze task introduced by Leslie Kaelbling
(1993) and shown in Figure 11. In each trial of this task, the agent starts in a randomlychosen state and must move to a randomly-chosen goal state using the usual North, South,
East, and West operators (we employed deterministic operators). There is a small cost for
each move, and the agent must minimize the undiscounted sum of these costs.
Because the goal state can be in any of 100 different locations, there are actually 100
different MDPs. Kaelbling's HDG method starts by choosing an arbitrary set of landmark
states and defining a Voronoi partition of the state space based on the Manhattan distances
to these landmarks (i.e., two states belong to the same Voronoi cell iff they have the same
nearest landmark). The method then defines one subtask for each landmark l. The subtask
is to move from any state in the current Voronoi cell or in any neighboring Voronoi cell to
the landmark l. Optimal policies for these subtasks are then computed.
Once HDG has the policies for these subtasks, it can solve the abstract Markov Decision
Problem of moving from each landmark state to any other landmark state using the subtask
solutions as macro actions (subroutines). So it computes a value function for this MDP.
281

fiDietterich

10

MAXQ Abstract+Greedy

Mean Cumulative Reward

5

Optimal Policy
Flat Q
Hier-Optimal Policy

0
MAXQ Abstract

MAXQ No Abstract

-5

-10

-15
0

50000

100000

150000
200000
Primitive Actions

250000

300000

Figure 9: Close-up view of the previous figure. This figure also shows two horizontal lines
indicating optimal performance and hierarchically optimal performance in this
domain. To make this figure more readable, we have applied a 100-step moving
average to the data points (which are themselves the average of 100 runs).
Finally, for each possible destination location g within a Voronoi cell for landmark l, the
HDG method computes the optimal policy of getting from l to g.
By combining these subtasks, the HDG method can construct a good approximation
to the optimal policy as follows. In addition to the value functions discussed above, the
agent maintains two other functions: NL(s), the name of the landmark nearest to state s,
and N (l), a list of the landmarks of the cells that are immediate neighbors of cell l. By
combining these, the agent can build a list for each state s of the current landmark and the
landmarks of the neighboring cells. For each such landmark, the agent computes the sum
of three terms:
(t1) the expected cost of reaching that landmark,
(t2) the expected cost of moving from that landmark to the landmark in the goal cell, and
(t3) the expected cost of moving from the goal-cell landmark to the goal state.
Note that while terms (t1) and (t3) can be exact estimates, term (t2) is computed using
the landmark subtasks as subroutines. This means that the corresponding path must pass
through the intermediate landmark states rather than going directly to the goal landmark.
282

fiMAXQ Hierarchical Reinforcement Learning

10
9
8
7
6
5
4
3
2
1
1

2

3

4

5

6

7

8

9

10

Figure 10: Kaelbling's 10-by-10 navigation task. Each circled state is a landmark state,
and the heavy lines show the boundaries of the Voronoi cells. In each episode, a
start state and a goal state are chosen at random. In this figure, the start state
is shown by the black square, and the goal state is shown by the black hexagon.

Hence, term (t2) is typically an overestimate of the required distance. (Also note that (t3)
is the same for all choices of the intermediate landmarks, so it does not need to be explicitly
included in the computation of the best action until the agent enters the cell containing the
goal.)
Given this information, the agent then chooses to move toward the best of the landmarks
(unless the agent is already in the goal Voronoi cell, in which case the agent moves toward
the goal state). For example, in Figure 10, term (t1) is the cost of reaching the landmark
in row 6, column 6, which is 4. Term (t2) is the cost of getting from row 6, column 6 to
the landmark at row 1 column 4 (by going from one landmark to another). In this case,
the best landmark-to-landmark path is to go directly from row 6 column 6 to row 1 column
4. Hence, term (t2) is 6. Term (t3) is the cost of getting from row 1 column 4 to the goal,
which is 1. The sum of these is 4 + 6 + 1 = 11. For comparison, the optimal path has
length 9.
In Kaelbling's experiments, she employed a variation of Q learning to learn terms (t1)
and (t3), and she computed (t2) at regular intervals via the Floyd-Warshall all-sources
shortest paths algorithm.
Figure 11 shows a MAXQ approach to solving this problem. The overall task Root,
takes one argument g, which specifies the goal cell. There are three subtasks:
283

fiDietterich

MaxRoot(g)
gl/NL(g)

QGotoGoalLmk(gl)

QGotoGoal(g)

MaxGotoGoalLmk(gl)

QGotoLmk(l,gl)

MaxGotoLmk(l)

QNorthLmk(l)

QSouthLmk(l)

MaxGotoGoal(g)

QEastLmk(l)

North

QWestLmk(l)

South

QNorthG(g)

East

QSouthG(g)

QEastG(g)

QWestG(g)

West

Figure 11: A MAXQ graph for the HDG navigation task.

 GotoGoalLmk, go to the landmark nearest to the goal location. The termination
predicate for this subtask is true if the agent reaches the landmark nearest to the
goal. The goal predicate is the same as the termination predicate.

 GotoLmk(l), go to landmark l. The termination predicate for this is true if either (a)

the agent reaches landmark l or (b) the agent is outside of the region defined by the
Voronoi cell for l and the neighboring Voronoi cells, N (l). The goal predicate for this
subtask is true only for condition (a).

 GotoGoal(g), go to the goal location g. The termination predicate for this subtask is

true if either the agent is in the goal location or the agent is outside of the Voronoi
cell NL(g) that contains g. The goal predicate for this subtask is true if the agent is
in the goal location.
284

fiMAXQ Hierarchical Reinforcement Learning

The MAXQ decomposition is essentially the same as Kaelbling's method, but somewhat
redundant. Consider a state where the agent is not inside the same Voronoi cell as the goal
g. In such states, HDG decomposes the value function into three terms (t1), (t2), and (t3).
Similarly, MAXQ also decomposes it into these same three terms:
 V (GotoLmk(l); s; a) the cost of getting to landmark l. This is represented as the sum
of V (a; s) and C (GotoLmk(l); s; a).
 C (GotoGoalLmk(gl); s; MaxGotoLmk(l)) the cost of getting from landmark l to the
landmark gl nearest the goal.
 C (Root; s; GotoGoalLmk(gl)) the cost of getting to the goal location after reaching gl
(i.e., the cost of completing the Root task after reaching gl).
When the agent is inside the goal Voronoi cell, then again HDG and MAXQ store
essentially the same information. HDG stores Q(GotoGoal(g); s; a), while MAXQ breaks
this into two terms: C (GotoGoal(g); s; a) and V (a; s) and then sums these two quantities to
compute the Q value.
Note that this MAXQ decomposition stores some information twice|specifically, the
cost of getting from the goal landmark gl to the goal is stored both as C (Root; s; GotoGoalLmk(gl))
and as C (GotoGoal(g); s; a) + V (a; s).
Let us compare the amount of memory required by at Q learning, HDG, and MAXQ.
There are 100 locations, 4 possible actions, and 100 possible goal states, so at Q learning
must store 40,000 values.
To compute quantity (t1), HDG must store 4 Q values (for the four actions) for each
state s with respect to its own landmark and the landmarks in N (NL(s)). This gives a
total of 2,028 values that must be stored.
To compute quantity (t2), HDG must store, for each landmark, information on the
shortest path to every other landmark. There are 12 landmarks. Consider the landmark at
row 6, column 1. It has 5 neighboring landmarks which constitute the five macro actions
that the agent can perform to move to another landmark. The nearest landmark to the
goal cell could be any of the other 11 landmarks, so this gives a total of 55 Q values that
must be stored. Similar computations for all 12 landmarks give a total of 506 values that
must be stored.
Finally, to compute quantity (t3), HDG must store information, for each square inside
each Voronoi cell, about how to get to each of the other squares inside the same Voronoi
cell. This requires 3,536 values.
Hence, the grand total for HDG is 6,070, which is a huge savings over at Q learning.
Now let's consider the MAXQ hierarchy with and without state abstractions.
 V (a; s): This is the expected reward of each primitive action in each state. There are
100 states and 4 primitive actions, so this requires 400 values. However, because the
reward is constant (,1), we can apply Leaf Irrelevance to store only a single value.
 C (GotoLmk(l); s; a), where a is one of the four primitive actions. This requires the
same amount of space as (t1) in Kaelbling's representation|indeed, combined with
V (a; s), this represents exactly the same information as (t1). It requires 2,028 values.
No state abstractions can be applied.
285

fiDietterich

 C (GotoGoalLmk(gl); s; GotoLmk(l)): This is the cost of completing the GotoGoalLmk

task after going to landmark l. If the primitive actions are deterministic, then
GotoLmk(l) will always terminate at location l, and hence, we only need to store
this for each pair of l and gl. This is exactly the same as Kaelbling's quantity (t2),
which requires 506 values. However, if the primitive actions are stochastic|as they
were in Kaelbling's original paper|then we must store this value for each possible
terminal state of each GotoLmk action. Each of these actions could terminate at its
target landmark l or in one of the states bordering the set of Voronoi cells that are
the neighbors of the cell for l. This requires 6,600 values. When Kaelbling stores
values only for (t2), she is effectively making the assumption that GotoLmk(l) will
never fail to reach landmark l. This is an approximation which we can introduce into
the MAXQ representation by our choice of state abstraction at this node.

 C (GotoGoal; s; a): This is the cost of completing the GotoGoal task after executing one
of the primitive actions a. This is the same as quantity (t3) in the HDG representation,
and it requires the same amount of space: 3,536 values.

 C (Root; s; GotoGoalLmk): This is the cost of reaching the goal once we have reached

the landmark nearest the goal. MAXQ must represent this for all combinations of
goal landmarks and goals. This requires 100 values. Note that these values are the
same as the values of C (GotoGoal(g); s; a) + V (a; s) for each of the primitive actions.
This means that the MAXQ representation stores this information twice, whereas the
HDG representation only stores it once (as term (t3)).

 C (Root; s; GotoGoal). This is the cost of completing the Root task after we have exe-

cuted the GotoGoal task. If the primitive action are deterministic, this is always zero,
because GotoGoal will have reached the goal. Hence, we can apply the Termination
condition and not store any values at all. However, if the primitive actions are stochastic, then we must store this value for each possible state that borders the Voronoi cell
that contains the goal. This requires 96 different values. Again, in Kaelbling's HDG
representation of the value function, she is ignoring the probability that GotoGoal will
terminate in a non-goal state. Because MAXQ is an exact representation of the value
function, it does not ignore this possibility. If we (incorrectly) apply the Termination
condition in this case, the MAXQ representation becomes a function approximation.

In the stochastic case, without state abstractions, the MAXQ representation requires
12,760 values. With safe state abstractions, it requires 12,361 values. With the approximations employed by Kaelbling (or equivalently, if the primitive actions are deterministic),
the MAXQ representation with state abstractions requires 6,171 values. These numbers are
summarized in Table 6. We can see that, with the unsafe state abstractions, the MAXQ
representation requires only slightly more space than the HDG representation (because of
the redundancy in storing C (Root; s; GotoGoalLmk).
This example shows that for the HDG task, we can start with the fully-general formulation provided by MAXQ and impose assumptions to obtain a method that is similar
to HDG. The MAXQ formulation guarantees that the value function of the hierarchical
policy will be represented exactly. The assumptions will introduce approximations into the
286

fiMAXQ Hierarchical Reinforcement Learning

Table 6: Comparison of the number of values that must be stored to represent the value
function using the HDG and MAXQ methods.
HDG MAXQ
HDG MAXQ MAXQ MAXQ
item item
values no abs safe abs unsafe abs
V (a; s)
0
400
1
1
(t1) C (GotoLmk(l); s; a)
2,028 2,028
2,028
2,028
(t2) C (GotoGoalLmk; s; GotoLmk(l))
506 6,600
6,600
506
(t3) C (GotoGoal(g); s; a)
3,536 3,536
3,536
3,536
C (Root; s; GotoGoalLmk)
0
100
100
100
C (Root; s; GotoGoal)
0
96
96
0
Total Number of Values Required
6,070 12,760 12,361
6,171
value function representation. This might be useful as a general design methodology for
building application-specific hierarchical representations. Our long-term goal is to develop
such methods so that each new application does not require inventing a new set of techniques. Instead, off-the-shelf tools (e.g., based on MAXQ) could be specialized by imposing
assumptions and state abstractions to produce more ecient special-purpose systems.
One of the most important contributions of the HDG method was that it introduced
a form of non-hierarchical execution. As soon as the agent crosses from one Voronoi cell
into another, the current subtask of reaching the landmark in that cell is \interrupted",
and the agent recomputes the \current target landmark". The effect of this is that (until
it reaches the goal Voronoi cell), the agent is always aiming for a landmark outside of its
current Voronoi cell. Hence, although the agent \aims for" a sequence of landmark states, it
typically does not visit many of these states on its way to the goal. The states just provide
a convenient set of intermediate targets. By taking these \shortcuts", HDG compensates
for the fact that, in general, it has overestimated the cost of getting to the goal, because its
computed value function is based on a policy where the agent goes from one landmark to
another.
The same effect is obtained by hierarchical greedy execution of the MAXQ graph (which
was directly inspired by the HDG method). Note that by storing the NL (nearest landmark)
function, Kaelbing's HDG method can detect very eciently when the current subtask
should be interrupted. This technique only works for navigation problems in a space with
a distance metric. In contrast, ExecuteHGPolicy performs a kind of \polling", because
it checks after each primitive action whether it should interrupt the current subroutine and
invoke a new one. An important goal for future research on MAXQ is to find a general
purpose mechanism for avoiding unnecessary \polling"|that is, a mechanism that can
discover eciently-evaluable interrupt conditions.
Figure 12 shows the results of our experiments with HDG using the MAXQ-Q learning algorithm. We employed the following parameters: for Flat Q learning, initial values
of 0.123, a learning rate of 1.0, initial temperature of 50, and cooling rate of 0.9074; for
MAXQ-Q without state abstractions: initial values of ,25:123, learning rate of 1.0, initial
287

fiDietterich

0
Flat Q

MAXQ +
Abstract

-20

Mean Cumulative Reward

-40

MAXQ No Abstract

-60
-80
-100
-120
-140
0

200000

400000

600000
800000
Primitive Actions

1e+06

1.2e+06

1.4e+06

Figure 12: Comparison of Flat Q learning with MAXQ-Q learning with and without state
abstraction. (Average of 100 runs.)
temperature of 50, and cooling rates of 0.9074 for MaxRoot, 0.9999 for MaxGotoGoalLmk,
0.9074 for MaxGotoGoal, and 0.9526 for MaxGotoLmk; for MAXQ-Q with state abstractions:
initial values of ,20:123, learning rate of 1.0, initial temperature of 50, and cooling rates of
0.9760 for MaxRoot, 0.9969 for MaxGotoGoal, 0.9984 for MaxGotoGoalLmk, and 0.9969 for
MaxGotoLmk. Hierarchical greedy execution was introduced by starting with 3000 primitive actions per trial, and reducing this every trial by 2 actions, so that after 1500 trials,
execution is completely greedy.
The figure confirms the observations made in our experiments with the Fickle Taxi task.
Without state abstractions, MAXQ-Q converges much more slowly than at Q learning.
With state abstractions, it converges roughly three times as fast. Figure 13 shows a close-up
view of Figure 12 that allows us to compare the differences in the final levels of performance
of the methods. Here, we can see that MAXQ-Q with no state abstractions was not able to
reach the quality of our hand-coded hierarchical policy|presumably even more exploration
would be required to achieve this, whereas with state abstractions, MAXQ-Q is able to do
slightly better than our hand-coded policy. With hierarchical greedy execution, MAXQ-Q
is able to reach the goal using one fewer action, on the average|so that it approaches the
performance of the best hierarchical greedy policy (as computed by value iteration). Notice
however, that the best performance that can be obtained by hierarchical greedy execution
of the best recursively-optimal policy cannot match optimal performance. Hence, Flat Q
288

fiMAXQ Hierarchical Reinforcement Learning

-6
Optimal Policy

Mean Cumulative Reward

Hierarchical Greedy Optimal Policy
MAXQ Abstract + Greedy
MAXQ + Abstract

-8

-10

Flat Q

Hierarchical Hand-coded Policy

MAXQ No Abstract

-12

-14

0

200000

400000

600000
800000
Primitive Actions

1e+06

1.2e+06

1.4e+06

Figure 13: Expanded view comparing Flat Q learning with MAXQ-Q learning with and
without state abstraction and with and without hierarchical greedy execution.
(Average of 100 runs.)
learning achieves a policy that reaches the goal state, on the average, with about one fewer
primitive action. Finally notice that as in the taxi domain, there was no added exploration
cost for shifting to greedy execution.
Kaelbling's HDG work has recently been extended and generalized by Moore, Baird
and Kaelbling (1999) to any sparse MDP where the overall task is to get from any given
start state to any desired goal state. The key to the success of their approach is that each
landmark subtask is guaranteed to terminate in a single resulting state. This makes it
possible to identify a sequence of good intermediate landmark states and then assemble a
policy that visits them in sequence. Moore, Baird and Kaelbling show how to construct a
hierarchy of landmarks (the \airport" hierarchy) that makes this planning process ecient.
Note that if each subtask did not terminate in a single state (as in general MDPs), then
the airport method would not work, because there would be a combinatorial explosion of
potential intermediate states that would need to be considered.

7.3 Parr and Russell: Hierarchies of Abstract Machines

In his (1998b) dissertation work, Ron Parr considered an approach to hierarchical reinforcement learning in which the programmer encodes prior knowledge in the form of a hierarchy
of finite-state controllers called a HAM (Hierarchy of Abstract Machines). The hierarchy
289

fiDietterich

Intersection
Vertical Hallway
Horizontal Hallway
Goal

Figure 14: Parr's maze problem (on left). The start state is in the upper left corner, and
all states in the lower right-hand room are terminal states. The smaller diagram
on the right shows the hallway and intersection structure of the maze.
is executed using a procedure-call-and-return discipline, and it provides a partial policy for
the task. The policy is partial because each machine can include non-deterministic \choice"
machine states, in which the machine lists several options for action but does not specify
which one should be chosen. The programmer puts \choice" states at any point where
he/she does not know what action should be performed. Given this partial policy, Parr's
goal is to find the best policy for making choices in the choice states. In other words, his
goal is to learn a hierarchical value function V (hs; mi), where s is a state (of the external
environment) and m contains all of the internal state of the hierarchy (i.e., the contents
of the procedure call stack and the values of the current machine states for all machines
appearing in the stack). A key observation is that it is only necessary to learn this value
function at choice states hs; mi. Parr's algorithm does not learn a decomposition of the value
function. Instead, it \attens" the hierarchy to create a new Markov decision problem over
the choice states hs; mi. Hence, it is hierarchical primarily in the sense that the programmer
structures the prior knowledge hierarchically. An advantage of this is that Parr's method
can find the optimal hierarchical policy subject to constraints provided by the programmer.
A disadvantage is that the method cannot be executed \non-hierarchically" to produce a
better policy.
Parr illustrated his work using the maze shown in Figure 14. This maze has a large-scale
structure (as a series of hallways and intersections), and a small-scale structure (a series of
obstacles that must be avoided in order to move through the hallways and intersections).
290

fiMAXQ Hierarchical Reinforcement Learning

In each trial, the agent starts in the top left corner, and it must move to any state in the
bottom right corner room. The agent has the usual four primitive actions, North, South,
East, and West. The actions are stochastic: with probability 0.8, they succeed, but with
probability 0.1 the action will move to the \left" and with probability 0.1 the action will
move to the \right" instead (e.g., a North action will move east with probability 0.1 and
west with probability 0.1). If an action would collide with a wall or an obstacle, it has no
effect.
The maze is structured as a series of \rooms", each containing a 12-by-12 block of states
(and various obstacles). Some rooms are parts of \hallways", because they are connected
to two other rooms on opposite sides. Other rooms are \intersections", where two or more
hallways meet.
To test the representational power of the MAXQ hierarchy, we want to see how well it
can represent the prior knowledge that Parr is able to represent using the HAM. We begin
by describing Parr's HAM for his maze task, and then we will present a MAXQ hierarchy
that captures much of the same prior knowledge.3
Parr's top level machine, MRoot, consists of a loop with a single choice state that
chooses among four possible child machines: MGo(East), MGo(South), MGo(West), and
MGo(North). The loop terminates when the agent reaches a goal state. MRoot will only
invoke a particular machine if there is a hallway in the specified direction. Hence, in the
start state, it will only consider MGo(South) and MGo(East).
The MGo(d) machine begins executing when the agent is in an intersection. So the first
thing it tries to do is to exit the intersection into a hallway in the specified direction d. Then
it attempts to traverse the hallway until it reaches another intersection. It does this by first
invoking an MExitIntersection(d) machine. When that machine returns, it then invokes an
MExitHallway(d) machine. When that machine returns, MGo also returns.
The MExitIntersection and MExitHallway machines are identical except for their termination conditions. Both machines consist of a loop with one choice state that chooses among
four possible subroutines. To simplify their description, suppose that MGo(East) has chosen MExitIntersection(East). Then the four possible subroutines are MSniff (East; North),
MSniff (East; South), MBack(East; North), and MBack(East; South).
The MSniff (d; p) machine always moves in direction d until it encounters a wall (either
part of an obstacle or part of the walls of the maze). Then it moves in perpendicular
direction p until it reaches the end of the wall. A wall can \end" in two ways: either the
agent is now trapped in a corner with walls in both directions d and p or else there is no
longer a wall in direction d. In the first case, the MSniff machine terminates; in the second
case, it resumes moving in direction d.
The MBack(d; p) machine moves one step backwards (in the direction opposite from d)
and then moves five steps in direction p. These moves may or may not succeed, because the
actions are stochastic and there may be walls blocking the way. But the actions are carried
out in any case, and then the MBack machine returns.
The MSniff and MBack machines also terminate if they reach the end of a hall or the
end of an intersection.
3. The author thanks Ron Parr for providing the details of the HAM for this task.

291

fiDietterich

These finite-state controllers define a highly constrained partial policy. The MBack,
MSniff, and MGo machines contain no choice states at all. The only choice points are
in MRoot, which must choose the direction in which to move, and in MExitIntersection
and MExitHall, which must decide when to call MSniff, when to call MBack, and which
\perpendicular" direction to tell these machines to try when they cannot move forward.

MaxRoot

Go(d)
r/ROOM
MaxGo(d,r)

QExitInter(d,r)

QExitHall(d,r)

MaxExitInter(d,r)

MaxExitHall(d,r)

QSniffEI(d,p)

QBackEI(d,p)

QSniffEH(d,p)
x/X

QBackEH(d,p)
x/X
y/Y

y/Y
MaxSniff(d,p)

MaxBack(d,p,x,y)

QFollowWall(d,p)

QToWall(d)

QBackOne(d)

QPerpThree(p)

MaxFollowWall(d,p)

MaxToWall(d)

MaxBackOne(d)

MaxPerpThree(p)

d/p

d/d

QMoveFW(d)

d/Inv(d)

QMoveTW(d)

QMoveBO(d)

d/p
QMoveP3(d)

MaxMove(d)

Figure 15: MAXQ graph for Parr's maze task.
Figure 15 shows a MAXQ graph that encodes a similar set of constraints on the policy.
The subtasks are defined as follows:
292

fiMAXQ Hierarchical Reinforcement Learning

 Root. This is exactly the same as the MRoot machine. It must choose a direction d

and invoke Go. It terminates when the agent enters a terminal state. This is also its
goal condition (of course).

 Go(d; r). (Go in direction d leaving room r.) The parameter r is bound to an identi-

fication number corresponding to the current 12-by-12 \room" in which the agent is
located. Go terminates when the agent enters the room at the end of the hallway in
direction d or when it leaves the desired hallway (e.g., in the wrong direction). The
goal condition for Go is satisfied only if the agent reaches the desired intersection.

 ExitInter(d; r). This terminates when the agent has exited room r. The goal condition
is that the agent exit room r in direction d.

 ExitHall(d; r). This terminates when the agent has exited the current hall (into some

intersection). The goal condition is that the agent has entered the desired intersection
in direction d.

 Sniff (d; r). This encodes a subtask that is equivalent to the MSniff machine. However,

Sniff must have two child subtasks, ToWall and FollowWall, that were simply internal
states of MSniff. This is necessary, because a subtask in the MAXQ framework cannot

contain any internal state, whereas a finite-state controller in the HAM representation
can contain as many internal states as necessary. In particular, it can have one state
for when it is moving forward and another state for when it is following a wall sideways.

 ToWall(d). This is equivalent to one part of MSniff. It terminates when there is a

wall in \front" of the agent in direction d. The goal condition is the same as the
termination condition.

 FollowWall(d; p). This is equivalent to the other part of MSniff. It moves in direction

p until the wall in direction d ends (or until it is stuck in a corner with walls in both
directions d and p). The goal condition is the same as the termination condition.

 Back(d; p; x; y). This attempts to encode the same information as the MBack machine,

but this is a case where the MAXQ hierarchy cannot capture the same information.

MBack simply executes a sequence of 6 primitive actions (one step back, five steps in
direction p). But to do this, MBack must have 6 internal states, which MAXQ does
not allow. Instead, the Back subtask has the subgoal of moving the agent at least

one square backwards and at least 3 squares in the direction p. In order to determine
whether it has achieved this subgoal, it must remember the x and y position where
it started to execute, so these are bound as parameters to Back. Back terminates if
it achieves the desired change in position or if it runs into walls that prevent it from
achieving the subgoal. The goal condition is the same as the termination condition.

 BackOne(d; x; y). This moves the agent one step backwards (in the direction opposite

to d. It needs the starting x and y position in order to tell when it has succeeded. It
terminates if it has moved at least one unit in direction d or if there is a wall in this
direction. Its goal condition is the same as its termination condition.
293

fiDietterich

 PerpThree(p; x; y). This moves the agent three steps in the direction p. It needs the

starting x and y positions in order to tell when it has succeeded. It terminates when it
has moved at least three units in the direction p or if there is a wall in that direction.
The goal condition is the same as the termination condition.

 Move(d). This is a \parameterized primitive" action. It executes one primitive move
in direction d and terminates immediately.

From this, we can see that there are three major differences between the MAXQ representation and the HAM representation. First, a HAM finite-state controller can contain
internal states. To convert them into a MAXQ subtask graph, we must make a separate
subtask for each internal state in the HAM. Second, a HAM can terminate based on an
\amount of effort" (e.g., performing 5 actions), whereas a MAXQ subtask must terminate
based on some change in the state of the world. It is impossible to define a MAXQ subtask that performs k steps and then terminate regardless of the effects of those steps (i.e.,
without adding some kind of \counter" to the state of the MDP). Third, it is more dicult
to formulate the termination conditions for MAXQ subtasks than for HAM machines. For
example, in the HAM, it was not necessary to specify that the MExitHallway machine terminates when it has entered a different intersection than the one where the MGo was executed.
However, this is important for the MAXQ method, because in MAXQ, each subtask learns
its own value function and policy|independent of its parent tasks. For example, without
the requirement to enter a different intersection, the learning algorithms for MAXQ will
always prefer to have MaxExitHall take one step backward and return to the room in which
the Go action was started (because that is a much easier terminal state to reach). This
problem does not arise in the HAM approach, because the policy learned for a subtask
depends on the whole \attened" hierarchy of machines, and returning to the state where
the Go action was started does not help solve the overall problem of reaching the goal state
in the lower right corner.
To construct the MAXQ graph for this problem, we have introduced three programming
tricks: (a) binding parameters to aspects of the current state (in order to serve as a kind
of \local memory" for where the subtask began executing), (b) having a parameterized
primitive action (in order to be able to pass a parameter value that specifies which primitive
action to perform), and (c) employing \inheritance of termination conditions"|that is, each
subtask in this MAXQ graph (but not the others in this paper) inherits the termination
conditions of all its ancestor tasks. Hence, if the agent is in the middle of executing a ToWall
action when it leaves an intersection, the ToWall subroutine terminates because the ExitInter
termination condition is satisfied. This behavior is very similar to the standard behavior of
MAXQ. Ordinarily, when an ancestor task terminates, all of its descendent tasks are forced
to return without updating their C values. With inheritance of termination conditions, on
the other hand, the descendent tasks are forced to terminate, but after updating their C
values. In other words, the termination condition of each child task is the logical disjuntion
of all of the termination conditions of its ancestors (plus its own termination condition).
This inheritance made it easier to write the MAXQ graph, because the parents did not need
to pass down to their children all of the information necessary for the children to define the
complete termination and goal predicates.
294

fiMAXQ Hierarchical Reinforcement Learning

There are essentially no opportunities for state abstraction in this task, because there
are no irrelevant features of the state. There are some opportunities to apply the Shielding
and Termination properties, however. In particular, ExitHall(d) is guaranteed to cause its
parent task, MaxGo(d), to terminate, so it does not require any stored C values. There are
many states where some subtasks are terminated (e.g., Go(East) in any state where there
is a wall on the east side of the room), and so no C values need to be stored.
Nonetheless, even after applying the state elimination conditions, the MAXQ representation for this task requires much more space than a at representation. An exact
computation is dicult, but after applying MAXQ-Q learning, the MAXQ representation
required 52,043 values, whereas at Q learning requires fewer than 16,704 values. Parr
states that his method requires only 4,300 values.
To test the relative effectiveness of the MAXQ representation, we compare MAXQ-Q
learning with at Q learning. Because of the very large negative values that some states
acquire (particularly during the early phases of learning), we were unable to get Boltzmann
exploration to work well|one very bad experience would cause an action to receive such
a low Q value, that it would never be tried again. Hence, we experimented with both
-greedy exploration and counter-based exploration. The -greedy exploration policy is an
ordered, abstract GLIE policy in which a random action is chosen with probability , and 
is gradually decreased over time. The counter-based exploration policy keeps track of how
many times each action a has been executed in each state s. To choose an action in state
s, it selects the action that has been executed the fewest times until all actions have been
executed T times. Then it switches to greedy execution. Hence, it is not a genuine GLIE
policy. Parr employed counter-based exploration policies in his experiments with this task.
As in the other domains, we conducted several experimental runs (e.g., testing Boltzmann, -greedy, and counter-based exploration) to determine the best parameters for each
algorithm. For Flat Q learning, we chose the following parameters: learning rate 0.50, greedy exploration with initial value for  of 1.0,  decreased by 0.001 after each successful
execution of a Max node, and initial Q values of ,200:123. For MAXQ-Q learning, we chose
the following parameters: counter-based exploration with T = 10, learning rate equal to the
reciprocal of the number of times an action had been performed, and initial values for the C
values selected carefully to provide underestimates of the true C values. For example, the
initial values for QExitInter were ,40:123, because in the worst case, after completing an
ExitInter task, it takes about 40 steps to complete the subsequent ExitHall task and hence,
complete the Go parent task. Performance was quite sensitive to these initial C values,
which is a potential drawback of the MAXQ approach.
Figure 16 plots the results. We can see that MAXQ-Q learning converges about 10
times faster than Flat Q learning. We do not know whether MAXQ-Q has converged to a
recursively optimal policy. For comparison, we also show the performance of a hierarchical
policy that we coded by hand, but in our hand-coded policy, we used knowledge of contextual
information to choose operators, so this policy is surely better than the best recursively
optimal policy. HAMQ learning should converge to a policy equal to or slightly better than
our hand-coded policy.
This experiment demonstrates that the MAXQ representation can capture most|but
not all|of the prior knowledge that can be represented by the HAMQ hierarchy. It also
295

fiDietterich

-100
-150

Mean Reward Per Trial

-200

Hand-coded hierarchical policy

-250
-300
-350

MAXQ Q Learning

Flat Q Learning

-400
-450
-500
0

1e+06

2e+06

3e+06
Primitive Steps

4e+06

5e+06

6e+06

Figure 16: Comparison of Flat Q learning and MAXQ-Q learning in the Parr maze task.
shows that the MAXQ representation requires much more care in the design of the goal
conditions for the subtasks.

7.4 Other Domains
In addition to the three domains discussed above, we have developed MAXQ graphs for
Singh's (1992) \ag task", the treasure hunter task described by Tadepalli and Dietterich
(1997), and Dayan and Hinton's (1993) Feudal-Q learning task. All of these tasks can be
easily and naturally placed into the MAXQ framework|indeed, all of them fit more easily
than the Parr and Russell maze task.
MAXQ is able to exactly duplicate Singh's work and his decomposition of the value
function|while using exactly the same amount of space to represent the value function.
MAXQ can also duplicate the results from Tadepalli and Dietterich|however, because
MAXQ is not an explanation-based method, it is considerably slower and requires substantially more space to represent the value function.
In the Feudal-Q task, MAXQ is able to give better performance than Feudal-Q learning.
The reason is that in Feudal-Q learning, each subroutine makes decisions using only a Q
function learned at its own level of the hierarchy|that is, without information about the
estimated costs of the actions of its descendents. In contrast, the MAXQ value function
decomposition permits each Max node to make decisions based on the sum of its completion
function, C (i; s; j ), and the costs estimated by its descendents, V (j; s). Of course, MAXQ
296

fiMAXQ Hierarchical Reinforcement Learning

also supports non-hierarchical execution, which is not possible for Feudal-Q, because it does
not learn a value function decomposition.

8. Discussion

Before concluding this paper, we wish to discuss two issues: (a) design tradeoffs in hierarchical reinforcement learning and (b) methods for automatically learning (or at least
improving) MAXQ hierarchies.

8.1 Design Tradeoffs in Hierarchical Reinforcement Learning

In the introduction to this paper, we discussed four issues concerning the design of hierarchical reinforcement learning architectures: (a) the method for defining subtasks, (b) the
use of state abstraction, (c) non-hierarchical execution, and (d) the design of learning algorithms. In this subsection, we want to highlight a tradeoff between the first two of these
issues.
MAXQ defines subtasks using a termination predicate Ti and a pseudo-reward function
R~ . There are at least two drawbacks of this method. First, it can be hard for the programmer to define Ti and R~ correctly, since this essentially requires guessing the value function
of the optimal policy for the MDP at all states where the subtask terminates. Second, it
leads us to seek a recursively optimal policy rather than a hierarchically optimal policy.
Recursively optimal policies may be much worse than hierarchically optimal ones, so we
may be giving up substantial performance.
However, in return for these two drawbacks, MAXQ obtains a very important benefit:
the policies and value functions for subtasks become context-free. In other words, they
do not depend on their parent tasks or the larger context in which they are invoked. To
understand this point, consider again the MDP shown in Figure 6. It is clear that the
optimal policy for exiting the left-hand room (the Exit subtask) depends on the location
of the goal. If it is at the top of the right-hand room, then the agent should prefer to
exit via the upper door, whereas if it is at the bottom of the right-hand room, the agent
should prefer to exit by the lower door. However, if we define the subtask of exiting the
left-hand room using a pseudo-reward of zero for both doors, then we obtain a policy that
is not optimal in either case, but a policy that we can re-use in both cases. Furthermore,
this policy does not depend on the location of the goal. Hence, we can apply Max node
irrelevance to solve the Exit subtask using only the location of the robot and ignore the
location of the goal.
This example shows that we obtain the benefits of subtask reuse and state abstraction because we define the subtask using a termination predicate and a pseudo-reward
function. The termination predicate and pseudo-reward function provide a barrier that
prevents \communication" of value information between the Exit subtask and its context.
Compare this to Parr's HAM method. The HAMQ algorithm finds the best policy
consistent with the hierarchy. To achieve this, it must permit information to propagate
\into" the Exit subtask (i.e., the Exit finite-state controller) from its environment. But
this means that if any state that is reached after leaving the Exit subtask has different
values depending on the location of the goal, then these different values will propagate
back into the Exit subtask. To represent these different values, the Exit subtask must know
297

fiDietterich

the location of the goal. In short, to achieve a hierarchically optimal policy within the Exit
subtask, we must (in general) represent its value function using the entire state space. State
abstractions cannot be employed without losing hierarchical optimality.
We can see, therefore, that there is a direct tradeoff between achieving hierarchical
optimality and employing state abstractions. Methods for hierarchical optimality have more
freedom in defining subtasks (e.g., using partial policies, as in the HAM approach). But
they cannot (safely) employ state abstractions within subtasks, and in general, they cannot
reuse the solution of one subtask in multiple contexts. Methods for recursive optimality, on
the other hand, must define subtasks using some method (such as pseudo-reward functions
for MAXQ or fixed policies for the options framework) that isolates the subtask from its
context. But in return, they can apply state abstraction and the learned policy can be
reused in many contexts (where it will be more or less optimal).
It is interesting that the iterative method described by Dean and Lin (1995) can be
viewed as a method for moving along this tradeoff. In the Dean and Lin method, the
programmer makes an initial guess for the values of the terminal states of each subtask
(i.e., the doorways in Figure 6). Based on this initial guess, the locally optimal policies
for the subtasks are computed. Then the locally optimal policy for the parent task is
computed|while holding the subtask policies fixed (i.e., treating them as options). At
this point, their algorithm has computed the recursively optimal solution to the original
problem, given the initial guesses. Instead of solving the various subproblems sequentially
via an oine algorithm as Dean and Lin suggested, we could use the MAXQ-Q learning
algorithm.
But the method of Dean and Lin does not stop here. Instead, it computes new values
of the terminal states of each subtask based on the learned value function for the entire
problem. This allows it to update its \guesses" for the values of the terminal states. The
entire solution process can now be repeated to obtain a new recursively optimal solution,
based on the new guesses. They prove that if this process is iterated indefinitely, it will
converge to the hierarchically optimal policy (provided, of course, that no state abstractions
are used within the subtasks).
This suggests an extension to MAXQ-Q learning that adapts the R~ values online. Each
time a subtask terminates, we could update the R~ function based on the computed value
of the terminated state. To be precise, if j is a subtask of i, then when j terminates in
state s0 , we should update R~ j (s0 ) to be equal to V~ (i; s0 ) = maxa0 Q~ (i; s0 ; a0 ). However, this
will only work if R~ j (s0 ) is represented using the full state s0. If subtask j is employing state
abstractions, x = (s), then R~ j (x0 ) will need to be the average value of V~ (i; s0 ), where
the average is taken over all states s0 such that x0 = (s0 ) (weighted by the probability of
visiting those states). This is easily accomplished by performing a stochastic approximation
update of the form
R~j (x0 ) = (1 , fft )R~ j (x0 ) + fftV~ (i; s0 )
each time subtask j terminates. Such an algorithm could be expected to converge to the
best hierarchical policy consistent with the given state abstractions.
This also suggests that in some problems, it may be worthwhile to first learn a recursively
optimal policy using very aggressive state abstractions and then use the learned value
function to initialize a MAXQ representation with a more detailed representation of the
states. These progressive refinements of the state space could be guided by monitoring the
298

fiMAXQ Hierarchical Reinforcement Learning

degree to which the values of V~ (i; x0 ) vary for each abstract state x0 . If they have a large
variance, this means that the state abstractions are failing to make important distinctions
in the values of the states, and they should be refined.
Both of these kinds of adaptive algorithms will take longer to converge than the basic
MAXQ method described in this paper. But for tasks that an agent must solve many times
in its lifetime, it is worthwhile to have learning algorithms that provide an initial useful
solution but then gradually improve that solution until it is optimal. An important goal for
future research is to find methods for diagnosing and repairing errors (or sub-optimalities)
in the initial hierarchy so that ultimately the optimal policy will be discovered.

8.2 Automated Discovery of Abstractions

The approach taken in this paper has been to rely upon the programmer to design the
MAXQ hierarchy including the termination conditions, pseudo-reward functions, and state
abstractions. But the results of this paper, particularly concerning state abstraction, suggest
ways in which we might be able to automate the construction of the hierarchy.
The main purpose of the hierarchy is to create opportunities for subtask sharing and
state abstraction. These are actually very closely related. In order for a subtask to be shared
in two different regions of the state space, it must be the case that the value function in those
two different regions is identical except for an additive offset. In the MAXQ framework,
that additive offset would be the difference in the C values of the parent task. So one way to
find reusable subtasks would be to look for regions of state space where the value function
exhibits these additive offsets.
A second way would be to search for structure in the one-step probability transition
function P (s0 js; a). A subtask will be useful if it enables state abstractions such as Max
Node Irrelevance. We can formulate this as the problem of identifying some region of
state space such that, conditioned on being in that region, P (s0 js; a) factors according to
Equation 17. A top-down divide-and-conquer algorithm similar to decision-tree algorithms
might be able to do this.
A third way would be to search for funnel actions by looking for bottlenecks in the state
space through which all policies must travel. This would be useful for discovering cases of
Result Distribution Irrelevance.
In some ways, the most dicult kinds of state abstractions to discover are those in
which arbitrary subgoals are introduced to constrain the policy (and sacrifice optimality).
For example, how could an algorithm automatically decide to impose landmarks onto the
HDG task? Perhaps by detecting a large region of state space without bottlenecks or
variations in the reward function?
The problem of discovering hierarchies is an important challenge for the future, but at
least this paper has provided some guidelines for what constitute good state abstractions,
and these can serve as objective functions for guiding the automated search for abstractions.

9. Concluding Remarks

This paper has introduced a new representation for the value function in hierarchical reinforcement learning|the MAXQ value function decomposition. We have proved that the
MAXQ decomposition can represent the value function of any hierarchical policy under
299

fiDietterich

both the finite-horizon undiscounted, cumulative reward criterion and the infinite-horizon
discounted reward criterion. This representation supports subtask sharing and re-use, because the overall value function is decomposed into value functions for individual subtasks.
The paper introduced a learning algorithm, MAXQ-Q learning, and proved that it
converges with probability 1 to a recursively optimal policy. The paper argued that although
recursive optimality is weaker than either hierarchical optimality or global optimality, it is
an important form of optimality because it permits each subtask to learn a locally optimal
policy while ignoring the behavior of its ancestors in the MAXQ graph. This increases the
opportunities for subtask sharing and state abstraction.
We have shown that the MAXQ decomposition creates opportunities for state abstraction, and we identified a set of five properties (Max Node Irrelevance, Leaf Irrelevance,
Result Distribution Irrelevance, Shielding, and Termination) that allow us to ignore large
parts of the state space within subtasks. We proved that MAXQ-Q still converges in the
presence of these forms of state abstraction, and we showed experimentally that state abstraction is important in practice for the successful application of MAXQ-Q learning|at
least in the Taxi and HDG tasks.
The paper presented two different methods for deriving improved non-hierarchical policies from the MAXQ value function representation, and it has formalized the conditions
under which these methods can improve over the hierarchical policy. The paper verified
experimentally that non-hierarchical execution gives improved performance in the Fickle
Taxi Task (where it achieves optimal performance) and in the HDG task (where it gives a
substantial improvement).
Finally, the paper has argued that there is a tradeoff governing the design of hierarchical
reinforcement learning methods. At one end of the design spectrum are \context free"
methods such as MAXQ-Q learning. They provide good support for state abstraction and
subtask sharing but they can only learn recursively optimal policies. At the other end
of the spectrum are \context-sensitive" methods such as HAMQ, the options framework,
and the early work of Dean and Lin. These methods can discover hierarchically optimal
policies (or, in some cases, globally optimal policies), but their drawback is that they cannot
easily exploit state abstractions or share subtasks. Because of the great speedups that are
enabled by state abstraction, this paper has argued that the context-free approach is to be
preferred|and that it can be relaxed as needed to obtain improved policies.

Acknowledgements
The author gratefully acknowledges the support of the National Science Foundation under
grant number IRI-9626584, the Oce of Naval Research under grant number N00014-95-10557, the Air Force Oce of Scientific Research under grant number F49620-98-1-0375, and
the Spanish government under their program of Estancias de Investigadores Extranjeros en
Regimen de A~no Sabatico en Espa~na. In addition, the author is indebted to many colleagues
for helping develop and clarify the ideas in this paper including Valentina Zubek, Leslie
Kaelbling, Bill Langford, Wes Pinchot, Rich Sutton, Prasad Tadepalli, and Sebastian Thrun.
I particularly want to thank Eric Chown for encouraging me to study Feudal reinforcement
learning, Ron Parr for providing the details of his HAM machines, and Sebastian Thrun
for encouraging me to write a single comprehensive paper. I also thank Andrew Moore
300

fiMAXQ Hierarchical Reinforcement Learning

(the action editor), Valentina Zubek, and the two sets of anonymous reviewers of previous
drafts of this paper for their suggestions and careful reading, which have improved the paper
immeasurably.

References

Bellman, R. E. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure in policy construction. In Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence, pp. 1104{1111.
Currie, K., & Tate, A. (1991). O-plan: The open planning architecture. Artificial Intelligence, 52 (1), 49{86.
Dayan, P., & Hinton, G. (1993). Feudal reinforcement learning. In Advances in Neural
Information Processing Systems, 5, pp. 271{278. Morgan Kaufmann, San Francisco,
CA.
Dean, T., & Lin, S.-H. (1995). Decomposition techniques for planning in stochastic domains.
Tech. rep. CS-95-10, Department of Computer Science, Brown University, Providence,
Rhode Island.
Dietterich, T. G. (1998). The MAXQ method for hierarchical reinforcement learning. In
Fifteenth International Conference on Machine Learning, pp. 118{126. Morgan Kaufmann.
Fikes, R. E., Hart, P. E., & Nilsson, N. J. (1972). Learning and executing generalized robot
plans. Artificial Intelligence, 3, 251{288.
Forgy, C. L. (1982). Rete: A fast algorithm for the many pattern/many object pattern
match problem. Artificial Intelligence, 19 (1), 17{37.
Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., & Boutilier, C. (1998). Hierarchical
solution of Markov decision processes using macro-actions. In Proceedings of the
Fourteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI{98), pp.
220{229 San Francisco, CA. Morgan Kaufmann Publishers.
Howard, R. A. (1960). Dynamic Programming and Markov Processes. MIT Press, Cambridge, MA.
Jaakkola, T., Jordan, M. I., & Singh, S. P. (1994). On the convergence of stochastic iterative
dynamic programming algorithms. Neural Computation, 6 (6), 1185{1201.
Kaelbling, L. P. (1993). Hierarchical reinforcement learning: Preliminary results. In Proceedings of the Tenth International Conference on Machine Learning, pp. 167{173 San
Francisco, CA. Morgan Kaufmann.
301

fiDietterich

Kalmar, Z., Szepesvari, C., & Lorincz, A. (1998). Module based reinforcement learning for
a real robot. Machine Learning, 31, 55{85.
Knoblock, C. A. (1990). Learning abstraction hierarchies for problem solving. In Proceedings
of the Eighth National Conference on Artificial Intelligence, pp. 923{928 Boston, MA.
AAAI Press.
Korf, R. E. (1985). Macro-operators: A weak method for learning. Artificial Intelligence,
26 (1), 35{77.
Lin, L.-J. (1993). Reinforcement learning for robots using neural networks. Ph.D. thesis,
Carnegie Mellon University, Department of Computer Science, Pittsburgh, PA.
Moore, A. W., Baird, L., & Kaelbling, L. P. (1999). Multi-value-functions: Ecient automatic action hierarchies for multiple goal MDPs. In Proceedings of the International Joint Conference on Artificial Intelligence, pp. 1316{1323 San Francisco. Morgan Kaufmann.
Parr, R. (1998a). Flexible decomposition algorithms for weakly coupled Markov decision
problems. In Proceedings of the Fourteenth Annual Conference on Uncertainty in
Artificial Intelligence (UAI{98), pp. 422{430 San Francisco, CA. Morgan Kaufmann
Publishers.
Parr, R. (1998b). Hierarchical control and learning for Markov decision processes. Ph.D.
thesis, University of California, Berkeley, California.
Parr, R., & Russell, S. (1998). Reinforcement learning with hierarchies of machines. In Advances in Neural Information Processing Systems, Vol. 10, pp. 1043{1049 Cambridge,
MA. MIT Press.
Pearl, J. (1988). Probabilistic Inference in Intelligent Systems. Networks of Plausible Inference. Morgan Kaufmann, San Mateo, CA.
Rummery, G. A., & Niranjan, M. (1994). Online Q-learning using connectionist systems.
Tech. rep. CUED/FINFENG/TR 166, Cambridge University Engineering Department, Cambridge, England.
Sacerdoti, E. D. (1974). Planning in a hierarchy of abstraction spaces. Artificial Intelligence,
5 (2), 115{135.
Singh, S., Jaakkola, T., Littman, M. L., & Szepesvari, C. (1998). Convergence results
for single-step on-policy reinforcement-learning algorithms. Tech. rep., University of
Colorado, Department of Computer Science, Boulder, CO. To appear in Machine
Learning.
Singh, S. P. (1992). Transfer of learning by composing solutions of elemental sequential
tasks. Machine Learning, 8, 323{339.
Sutton, R. S., Singh, S., Precup, D., & Ravindran, B. (1999). Improved switching among
temporally abstract actions. In Advances in Neural Information Processing Systems,
Vol. 11, pp. 1066{1072. MIT Press.
302

fiMAXQ Hierarchical Reinforcement Learning

Sutton, R., & Barto, A. G. (1998). Introduction to Reinforcement Learning. MIT Press,
Cambridge, MA.
Sutton, R. S., Precup, D., & Singh, S. (1998). Between MDPs and Semi-MDPs: Learning,
planning, and representing knowledge at multiple temporal scales. Tech. rep., University of Massachusetts, Department of Computer and Information Sciences, Amherst,
MA. To appear in Artificial Intelligence.
Tadepalli, P., & Dietterich, T. G. (1997). Hierarchical explanation-based reinforcement
learning. In Proceedings of the Fourteenth International Conference on Machine
Learning, pp. 358{366 San Francisco, CA. Morgan Kaufmann.
Tambe, M., & Rosenbloom, P. S. (1994). Investigating production system representations
for non-combinatorial match. Artificial Intelligence, 68 (1), 155{199.
Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. Ph.D. thesis, King's College,
Oxford. (To be reprinted by MIT Press.).
Watkins, C. J., & Dayan, P. (1992). Technical note Q-Learning. Machine Learning, 8, 279.

303

fiJournal of Artificial Intelligence Research 13 (2000) 155-188

Submitted 6/00; published 10/00

AIS-BN: An Adaptive Importance Sampling Algorithm for
Evidential Reasoning in Large Bayesian Networks
Jian Cheng
Marek J. Druzdzel

jcheng@sis.pitt.edu
marek@sis.pitt.edu

Decision Systems Laboratory
School of Information Sciences and Intelligent Systems Program
University of Pittsburgh, Pittsburgh, PA 15260 USA

Abstract
Stochastic sampling algorithms, while an attractive alternative to exact algorithms in
very large Bayesian network models, have been observed to perform poorly in evidential
reasoning with extremely unlikely evidence. To address this problem, we propose an adaptive importance sampling algorithm, AIS-BN, that shows promising convergence rates
even under extreme conditions and seems to outperform the existing sampling algorithms
consistently. Three sources of this performance improvement are (1) two heuristics for
initialization of the importance function that are based on the theoretical properties of importance sampling in finite-dimensional integrals and the structural advantages of Bayesian
networks, (2) a smooth learning method for the importance function, and (3) a dynamic
weighting function for combining samples from different stages of the algorithm.
We tested the performance of the AIS-BN algorithm along with two state of the art
general purpose sampling algorithms, likelihood weighting (Fung & Chang, 1989; Shachter
& Peot, 1989) and self-importance sampling (Shachter & Peot, 1989). We used in our
tests three large real Bayesian network models available to the scientific community: the
CPCS network (Pradhan et al., 1994), the PathFinder network (Heckerman, Horvitz,
& Nathwani, 1990), and the ANDES network (Conati, Gertner, VanLehn, & Druzdzel,
1997), with evidence as unlikely as 1041 . While the AIS-BN algorithm always performed
better than the other two algorithms, in the majority of the test cases it achieved orders of
magnitude improvement in precision of the results. Improvement in speed given a desired
precision is even more dramatic, although we are unable to report numerical results here,
as the other algorithms almost never achieved the precision reached even by the first few
iterations of the AIS-BN algorithm.

1. Introduction
Bayesian networks (Pearl, 1988) are increasingly popular tools for modeling uncertainty in
intelligent systems. With practical models reaching the size of several hundreds of variables
(e.g., Pradhan et al., 1994; Conati et al., 1997), it becomes increasingly important to address the problem of feasibility of probabilistic inference. Even though several ingenious
exact algorithms have been proposed, in very large models they all stumble on the theoretically demonstrated NP-hardness of inference (Cooper, 1990). The significance of this
result can be observed in practice  exact algorithms applied to large, densely connected
practical networks require either a prohibitive amount of memory or a prohibitive amount
of computation and are unable to complete. While approximating inference to any desired
precision has been shown to be NP-hard as well (Dagum & Luby, 1993), it is for very comc
2000
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiCheng & Druzdzel

plex networks the only alternative that will produce any result at all. Furthermore, while
obtaining the result is crucial in all applications, precision guarantees may not be critical
for some types of problems and can be traded off against the speed of computation.
A prominent subclass of approximate algorithms is the family of stochastic sampling
algorithms (also called stochastic simulation or Monte Carlo algorithms). The precision
obtained by stochastic sampling generally increases with the number of samples generated
and is fairly unaffected by the network size. Execution time is fairly independent of the
topology of the network and is linear in the number of samples. Computation can be
interrupted at any time, yielding an anytime property of the algorithms, important in timecritical applications.
While stochastic sampling performs very well in predictive inference, diagnostic reasoning, i.e., reasoning from observed evidence nodes to their ancestors in the network often
exhibits poor convergence. When the number of observations increases, especially if these
observations are unlikely a-priori, stochastic sampling often fails to converge to reasonable estimates of the posterior probabilities. Although this problem has been known since
the very first sampling algorithm was proposed by Henrion (1988), little has been done
to address it effectively. Furthermore, various sampling algorithms proposed were tested
on simple and small networks, or networks with special topology, without the presence of
extremely unlikely evidence and the practical significance of this problem has been underestimated. Given a typical number of samples used in real-time that are feasible on
todays hardware, say 106 samples, the behavior of a stochastic sampling algorithm will be
drastically different for different size networks. While in a network consisting of 10 nodes
and a few observations, it may be possible to converge to exact probabilities, in very large
networks only a negligibly small fraction of the total sample space will be probed. One of
the practical Bayesian network models that we used in our tests, a subset of the CPCS
network (Pradhan et al., 1994), consists of 179 nodes. Its total sample space is larger than
1061 . With 106 samples, we can sample only 1055 fraction of the sample space.
We believe that it is crucial (1) to study the feasibility and convergence properties of
sampling algorithms on very large practical networks, and (2) to develop sampling algorithms that will show good convergence under extreme, yet practical conditions, such as
evidential reasoning given extremely unlikely evidence. After all, small networks can be
updated using any of the existing exact algorithms  it is precisely the very large networks
where stochastic sampling can be most useful. As to the likelihood of evidence, we know
that stochastic sampling will generally perform well when it is high (Henrion, 1988). So, it
is important to look at those cases in which evidence is very unlikely. In this paper, we test
two existing state of the art stochastic sampling algorithms for Bayesian networks, likelihood weighting (Fung & Chang, 1989; Shachter & Peot, 1989) and self-importance sampling
(Shachter & Peot, 1989), on a subset of the CPCS network with extremely unlikely evidence. We show that they both exhibit similarly poor convergence rates. We propose a new
sampling algorithm, that we call the adaptive importance sampling for Bayesian networks
(AIS-BN), which is suitable for evidential reasoning in large multiply-connected Bayesian
networks. The AIS-BN algorithm is based on importance sampling, which is a widely
applied method for variance reduction in simulation that has also been applied in Bayesian networks (e.g., Shachter & Peot, 1989). We demonstrate empirically on three large
practical Bayesian network models that the AIS-BN algorithm consistently outperforms
156

fiAdaptive Importance Sampling in Bayesian Networks

the other two algorithms. In the majority of the test cases, it achieved over two orders of
magnitude improvement in convergence. Improvement in speed given a desired precision
is even more dramatic, although we are unable to report numerical results here, as the
other algorithms never achieved the precision reached even by the first few iterations of
the AIS-BN algorithm. The main sources of improvement are: (1) two heuristics for the
initialization of the importance function that are based on the theoretical properties of importance sampling in finite-dimensional integrals and the structural advantages of Bayesian
networks, (2) a smooth learning method for updating the importance function, and (3) a
dynamic weighting function for combining samples from different stages of the algorithm.
We study the value of the two heuristics used in the AIS-BN algorithm: (1) initialization
of the probability distributions of parents of evidence nodes to uniform distribution and
(2) adjusting very small probabilities in the conditional probability tables, and show that
they both play an important role in the AIS-BN algorithm but only a moderate role in the
existing algorithms.
The remainder of this paper is structured as follows. Section 2 first gives a general
introduction to importance sampling in the domain of finite-dimensional integrals, where it
was originally proposed. We show how importance sampling can be used to compute probabilities in Bayesian networks and how it can draw additional benefits from the graphical
structure of the network. Then we develop a generalized sampling scheme that will aid us
in reviewing the previously proposed sampling algorithms and in describing the AIS-BN
algorithm. Section 3 describes the AIS-BN algorithm. We propose two heuristics for initialization of the importance function and discuss their theoretical foundations. We describe
a smooth learning method for the importance function and a dynamic weighting function
for combining samples from different stages of the algorithm. Section 4 describes the empirical evaluation of the AIS-BN algorithm. Finally, Section 5 suggests several possible
improvements to the AIS-BN algorithm, possible applications of our learning scheme, and
directions for future work.

2. Importance Sampling Algorithms for Bayesian Networks
We feel that it is useful to go back to the theoretical roots of importance sampling in order
to be able to understand the source of speedup of the AIS-BN algorithm relative to the
existing state of the art importance sampling algorithms for Bayesian networks. We first
review the general idea of importance sampling in finite-dimensional integrals and how it
can reduce the sampling variance. We then discuss the application of importance sampling
to Bayesian networks. Readers interested in more details are directed to literature on
Monte Carlo methods in computation of finite integrals, such as the excellent exposition by
Rubinstein (1981) that we are essentially following in the first section.
2.1 Mathematical Foundations
Let g(X) be a function of m variables X = (X1 , ..., Xm ) over a domain   Rm , such that
computing g(X) for any X is feasible. Consider the problem of approximate computation
of the integral
Z
I=

g(X) dX .


157

(1)

fiCheng & Druzdzel

Importance sampling approaches this problem by writing the integral (1) as
Z

I=


g(X)
f (X) dX ,
f (X)

where f (X), often referred to as the importance function, is a probability density function
over . f (X) can be used in importance sampling if there exists an algorithm for generating
samples from f (X) and if the importance function is zero only when the original function
is zero, i.e., g(X) 6= 0 = f (X) 6= 0.
After we have independently sampled n points s1 , s2 , . . . , sn , si  , according to the
probability density function f (X), we can estimate the integral I by
n
1X
g(si )
In =
n i=1 f (si )

(2)

and estimate the variance of In by
b (In ) =

2

n
X
1
g(si ) 
 In
n  (n  1) i=1 f (si )



2

.

(3)

It is straightforward to show that this estimator has the following properties:
1. E(In ) = I
2. limn In = I

n
3. n  (In  I)  Normal(0, f2(X) ), where
f2(X)


Z 

=


g(X)
I
f (X)

2

f (X) dX

(4)



b 2 (In ) =  2 (In ) = f2 (X) /n
4. E 

The variance of In is proportional to f2(X) and inversely proportional to the number of
samples. To minimize the variance of In , we can either increase the number of samples or
try to decrease f2(X) . With respect to the latter, Rubinstein (1981) reports the following
useful theorem and corollary.
Theorem 1 The minimum of f2(X) is equal to
f2(X)

2

Z

|g(X)| dX

=

 I2



and occurs when X is distributed according to the following probability density function
f (X) = R

|g(X)|
.
 |g(X)| dX

158

fiAdaptive Importance Sampling in Bayesian Networks

Corollary 1 If g(X) >0, then the optimal probability density function is
f (X) =

g(X)
I

and f2(X) = 0.
Although in practice sampling from precisely f (X) = g(X)/I will occur rarely, we expect
that functions that are close enough to it can still reduce the variance effectively. Usually,
the closer the shape of the function f (X) is to the shape of the function g(X), the smaller
is f2(X) . In high-dimensional integrals, selection of the importance function, f (X), is far
more critical than increasing the number of samples, since the former can dramatically
affect f2(X) . It seems prudent to put more energy in choosing an importance function
whose shape is as close as possible to that of g(X) than to apply the brute force method of
increasing the number of samples.
It is worth noting here that if f (X) is uniform, importance sampling becomes a general
Monte Carlo sampling. Another noteworthy property of importance sampling that can be
derived from Equation 4 is that we should avoid f (X)  |g(X)  I  f (X)| in any part
of the domain of sampling, even if f (X) matches well g(X)/I in important regions. If
f (X)  |g(X)  I  f (X)|, the variance can become very large or even infinite. We can
avoid this by adjusting f (X) to be larger in unimportant regions of the domain of X.
While in this section we discussed importance sampling for continuous variables, the
results stated are valid for discrete variables as well, in which case integration should be
substituted by summation.
2.2 A Generic Importance Sampling Algorithm for Bayesian Networks
In the following discussion, all random variables used are multiple-valued, discrete variables.
Capital letters, such as A, B, or C, denote random variables. Bold capital letters, such as
A, B, or C, denote sets of variables. Bold capital letter E will usually be used to denote
the set of evidence variables. Lower case letters a, b, c denote particular instantiations
of variables A, B, and C respectively. Bold lower case letters, such as a, b, c, denote
particular instantiations of sets A, B, and C respectively. Bold lower case letter e, in
particular, will be used to denote the observations, i.e., instantiations of the set of evidence
variables E. Anc(A) denotes the set of ancestors of node A. Pa(A) denotes the set of
parents (direct ancestors) of node A. pa(A) denotes a particular instantiation of Pa(A). \
denotes set difference. Pa(A)|E=e denotes that we use the extended vertical bar to indicate
substitution of e for E in A.
We know that the joint probability distribution over all variables of a Bayesian network model, Pr(X), is the product of the probability distributions over each of the nodes
conditional on their parents, i.e.,
Pr(X) =

n
Y

Pr(Xi |Pa(Xi )) .

(5)

i=1

In order to calculate Pr(E = e), we need to sum over all Pr(X\E, E = e).
Pr(E = e) =

X

Pr(X\E, E = e)

X\E

159

(6)

fiCheng & Druzdzel

We can see that Equation 6 is almost identical to Equation 1 except that integration is
replaced by summation and the domain  is replaced by X\E. The theoretical results
derived for the importance sampling that we reviewed in the previous section can thus be
directly applied to computing probabilities in Bayesian networks.
While there has been previous work on importance sampling-based algorithms for Bayesian networks, we will postpone the discussion of this work until the next section. Here
we will present a generic stochastic sampling algorithm that will help us in both reviewing
the prior work and in presenting our algorithm.
The posterior probability Pr(a|e) can be obtained by first computing Pr(a, e) and Pr(e)
and then combining these based on the definition of conditional probability
Pr(a|e) =

Pr(a, e)
.
Pr(e)

(7)

In order to increase the accuracy of results of importance sampling in computing the posterior probabilities over different network variables given evidence, we should in general use
different importance functions for Pr(a, e) and for Pr(e). Doing so increases the computation time only linearly while the gain in accuracy may be significant given that obtaining
a desired accuracy is exponential in nature. Very often, it is a common practice to use the
same importance function (usually for Pr(e)) to sample both probabilities. If the difference
1. Order the nodes according to their topological order.
2. Initialize importance function Pr0 (X\E), the desired number of samples
m, the updating interval l, and the score arrays for every node.
3. k  0, T  
4. for i  1 to m do
5.

if (i mod l == 0) then

6.

k k+1

7.

Update importance function Prk (X\E) based on T .
end if

8.

si  generate a sample according to Prk (X\E)

9.

T  T  {si }

10.

Calculate Score(si , Pr(X\E, e), Prk (X\E)) and add it to the corresponding entry of every score array according to the instantiated states.
end for

11. Normalize the score arrays for every node.
Figure 1: A generic importance sampling algorithm.
160

fiAdaptive Importance Sampling in Bayesian Networks

between the optimal importance functions for these two quantities is large, the perforc
c
mance may deteriorate significantly. Although Pr(a,
e) and Pr(e)
are unbiased estimators
c
according to Property 1 (Section 2.1), Pr(a|e)
obtained by means of Equation 7 is not an
unbiased estimator. However, as the number of samples increases, the bias decreases and
can be ignored altogether when the sample size is large enough (Fishman, 1995).
Figure 1 presents a generic stochastic sampling algorithm that captures most of the
existing sampling algorithms. Without the loss of generality, we restrict ourselves in our
description to so-called forward sampling, i.e., generation of samples in the topological
order of the nodes in the network. The forward sampling order is accomplished by the
initialization performed in Step 1, where parents of each node are placed before the node
itself. In forward sampling, Step 8 of the algorithm, the actual generation of samples, works
as follows. (i) each evidence node is instantiated to its observed state and is further omitted
from sample generation; (ii) each root node is randomly instantiated to one of its possible
states, according to the importance prior probability of this node, which can be derived
from Prk (X\E); (iii) each node whose parents are instantiated is randomly instantiated to
one of its possible states, according to the importance conditional probability distribution
of this node given the values of the parents, which can also be derived from Prk (X\E); (iv)
this procedure is followed until all nodes are instantiated. A complete instantiation si of the
network based on this method is one sample of the joint importance probability distribution
Prk (X\E) over all variables of the network. The scoring of Step 10 amounts to calculating
Pr(si , e)/Prk (si ), as required by Equation 2. The ratio between the total score sum and the
number of samples is an unbiased estimator of Pr(e). In Step 10, if we also count the score
sum under the condition A = a, i.e., that some unobserved variables A have the values a,
the ratio between this score sum and the number of samples is an unbiased estimator of
Pr(a, e).
Most existing algorithms focus on the posterior probability distributions of individual
nodes. As we mentioned above, for the sake of efficiency they count the score sum corresponding to Pr(A = a, e), A  X\E, and record it in an score array for node A. Each
entry of this array corresponds to a specified state of A. This method introduces additional
variance, as opposed to using the importance function derived from Prk (X\E) to sample
Pr(A = a, e), A  X\E, directly.
2.3 Existing Importance Sampling Algorithms for Bayesian Networks
The main difference between various stochastic sampling algorithms is in how they process
Steps 2, 7, and 8 in the generic importance sampling algorithm of Figure 1.
Probabilistic logic sampling (Henrion, 1988) is the simplest and the first proposed sampling algorithm for Bayesian networks. The importance function is initialized in Step 2 to
Pr(X) and never updated (Step 7 is null). Without evidence, Pr(X) is the optimal importance function for the evidence set, which is empty anyway. It escapes most authors
that Pr(X) may be not the optimal importance function for Pr(A = a), A  X, when A
is not a root node. A mismatch between the optimal and the actually used importance
function may result in a large variance. The sampling process with evidence is the same
as without evidence except that in Step 10 we do not count the scores for those samples
that are inconsistent with the observed evidence, which amounts to discarding them. When
161

fiCheng & Druzdzel

the evidence is very unlikely, there is a large difference between Pr(X) and the optimal
importance function. Effectively, most samples are discarded and the performance of logic
sampling deteriorates badly.
Likelihood weighting (LW) (Fung & Chang, 1989; Shachter & Peot, 1989) enhances the
logic sampling in that it never discards samples. In likelihood weighting, the importance
function in Step 2 is
fi
fi
fi
Pr(X\E) =
Pr(xi |Pa(Xi ))fifi
fi
xi e
/
Y

.
E=e

Likelihood weighting does not update the importance function in Step 7. Although likelihood weighting is an improvement on logic sampling, its convergence rate can be still very
slow when there is large difference between the optimal importance function and Pr(X\E),
again especially in situations when evidence is very unlikely. Because of its simplicity, the
likelihood weighting algorithm has been the most commonly used simulation method for
Bayesian network inference. It often matches the performance of other, more sophisticated
schemes because it is simple and able to increase its precision by generating more samples
than other algorithms in the same amount of time.
Backward sampling (Fung & del Favero, 1994) changes Step 1 of our generic algorithm
and allows for generating samples from evidence nodes in the direction that is opposite to
the topological order of nodes in the network. In Step 2, backward sampling uses the likelihood of some of the observed evidence and some instantiated nodes to calculate Pr0 (X\E).
Although Fung and del Favero mentioned the possibility of dynamic node ordering, they
did not propose any scheme for updating the importance function in Step 7. Backward
sampling suffers from problems that are similar to those of likelihood weighting, i.e., a possible mismatch between its importance function and the optimal importance function can
lead to poor convergence.
Importance sampling (Shachter & Peot, 1989) is the same as our generic sampling algorithm. Shachter and Peot introduced two variants of importance sampling: self-importance
(SIS) and heuristic importance. The importance function used in the first step of the
self-importance algorithm is
fi
fi
fi
0
Pr (X\E) =
Pr(xi |Pa(Xi ))fifi
fi
xi e
/
Y

.
E=e

This function is updated in Step 7. The algorithm tries to revise the conditional probability
tables (CPTs) periodically in order to make the sampling distribution gradually approach
the posterior distribution. Since the same data are used to update the importance function
and to compute the estimator, this process introduces bias in the estimator. Heuristic
importance first removes edges from the network until it becomes a polytree, and then
uses a modified version of the polytree algorithm (Pearl, 1986) to compute the likelihood
functions for each of the unobserved nodes. Pr0 (X\E) is a combination of these likelihood
functions with Pr(X\E, e). In Step 7 heuristic importance does not update Prk (X\E). As
Shachter and Peot (1989) point out, this heuristic importance function can still lead to a
bad approximation of the optimal importance function. There exist also other algorithms
such as a combination of self-importance and heuristic importance (Shachter & Peot, 1989;
162

fiAdaptive Importance Sampling in Bayesian Networks

Shwe & Cooper, 1991). Although some researchers suggested that this may be a promising
direction for the work on sampling algorithms, we have not seen any results that would
follow up on this.
A separate group of stochastic sampling methods is formed by so-called Markov Chain
Monte Carlo (MCMC) methods that are divided into Gibbs sampling, Metropolis sampling,
and Hybrid Monte Carlo sampling (Geman & Geman, 1984; Gilks, Richardson, & Spiegelhalter, 1996; MacKay, 1998). Roughly speaking, these methods draw random samples from
an unknown target distribution f (X) by biasing the search for this distribution towards
higher probability regions. When applied to Bayesian networks (Pearl, 1987; Chavez &
Cooper, 1990) this approach determines the sampling distribution of a variable from its
previous sample given its Markov blanket (Pearl, 1988). This corresponds to updating
Prk (X\E) when sampling every node. Prk (X\E) will converge to the optimal importance
function for Pr(e) if Pr0 (X\E) satisfies some ergodic properties (York, 1992). Since the
convergence to the limiting distribution is very slow and calculating updates of the sampling distribution is costly, these algorithms are not used in practice as often as the simple
likelihood weighting scheme.
There are also some other simulation algorithms, such as bounded variance algorithm
(Dagum & Luby, 1997) and the AA algorithm (Dagum et al., 1995), which are essentially
based on the LW algorithm and the Stopping-Rule Theorem (Dagum et al., 1995). Cano
et al. (1996) proposed another importance sampling algorithm that performed somewhat
better than LW in cases with extreme probability distributions, but, as the authors state, in
general cases it produced similar results to the likelihood weighting algorithm. Hernandez
et al. (1998) also applied importance sampling and reported a moderate improvement on
likelihood weighting.
2.4 Practical Performance of the Existing Sampling Algorithms
The largest network that has been tested using sampling algorithms is QMR-DT (Quick
Medical Reference  Decision Theoretic) (Shwe et al., 1991; Shwe & Cooper, 1991), which
contains 534 adult diseases and 4,040 findings, with 40,740 arcs depicting disease-to-finding
dependencies. The QMR-DT network belongs to a class of special bipartite networks
and its structure is often referred to as BN2O (Henrion, 1991), because of its two-layer
composition: disease nodes in the top layer and finding nodes in the bottom layer. Shwe
and colleagues used an algorithm combining self-importance and heuristic importance and
tested its convergence properties on the QMR-DT network. But since the heuristic method
iterative tabular Bayes (ITB) that makes use of a version of Bayes rule is designed for
the BN2O networks, it cannot be generalized to arbitrary networks. Although Shwe and
colleagues concluded that Markov blanket scoring and self-importance sampling significantly
improve the convergence rate in their model, we cannot extend this conclusion to general
networks. The computation of Markov blanket scoring is more complex in a general multiconnected network than in a BN2O network. Also, the experiments conducted lacked a
gold-standard posterior probability distribution that could serve to judge the convergence
rate.
Pradhan and Dagum (1996) tested an efficient version of the LW algorithm  bounded
variance algorithm (Dagum & Luby, 1997) and the AA algorithm (Dagum et al., 1995) on
163

fiCheng & Druzdzel

a 146 node, multiply connected medical diagnostic Bayesian network. One limitation in
their tests is that the probability of evidence in the cases selected for testing was rather
high. Although over 10% of the cases had the probability of evidence on the order of
108 or smaller, a simple calculation based on the reported mean  = 34.5 number of
evidence nodes, shows that the average probability of an observed state of an evidence node
conditional on its direct predecessors was on the order of (108 )1/34.5  0.59. Given that
their algorithm is essentially based on the LW algorithm, based on our tests we suspect
that the performance will deteriorate on cases where the evidence is very unlikely. Both
algorithms focus on the marginal probability of one hypothesis node. If there are many
queried nodes, the efficiency may deteriorate.
We have tested the algorithms discussed in Section 2.3 on several large networks. Our
experimental results show that in cases with very unlikely evidence, none of these algorithms
converges to reasonable estimates of the posterior probabilities within a reasonable amount
of time. The convergence becomes worse as the number of evidence nodes increases. Thus,
when using these algorithms in very large networks, we simply cannot trust the results. We
will present results of tests of the LW and SIS algorithms in more detail in Section 4.

3. AIS-BN: Adaptive Importance Sampling for Bayesian Networks
The main reason why the existing stochastic sampling algorithms converge so slowly is that
they fail to learn a good importance function during the sampling process and, effectively,
fail to reduce the sampling variance. When the importance function is optimal, such as
in probabilistic logic sampling without any evidence, each of the algorithms is capable
of converging to fairly good estimates of the posterior probabilities within relatively few
samples. For example, assuming that the posterior probabilities are not extreme (i.e., larger
than say 0.01), as few as 1,000 samples may be sufficient to obtain good estimates. In this
section, we present the adaptive importance sampling algorithm for Bayesian networks
(AIS-BN) that, as we will demonstrate in the next section, performs very well on most
tests. We will first describe the details of the algorithm and prove two theorems that are
useful in learning the optimal importance sampling function.
3.1 Basic Algorithm  AIS-BN
Compared with importance sampling used in normal finite-dimensional integrals, importance sampling used in Bayesian networks has several significant advantages. First, the
network joint probability distribution Pr(X) is decomposable and can be factored into
component parts. Second, the network has a clear structure, which represents many conditional independence relationships. These properties are very helpful in estimating the
optimal importance function.
The basic AIS-BN algorithm is presented in Figure 2. The main differences between
the AIS-BN algorithm and the basic importance sampling algorithm in Figure 1 is that
we introduce a monotonically increasing weight function wk and two effective heuristic
initialization methods in Step 2. We also introduce a special learning component in Step 7
to let the updating process run more smoothly, avoiding oscillation of the parameters. The
164

fiAdaptive Importance Sampling in Bayesian Networks

1. Order the nodes according to their topological order.
2. Initialize importance function Pr0 (X\E) using some heuristic methods, initialize weight w0 , set the desired number of samples m and the updating
interval l, initialize the score arrays for every node.
3. k  0, T  , wT Score  0, wsum  0
4. for i  1 to m do
5.

if (i mod l == 0) then

6.

k k+1

7.

Update the importance function Prk (X\E) and wk based on T .
end if

8.

si  generate a sample according to Prk (X\E)

9.

T  T  {si }

10.

wiScore  Score (si , Pr(X\E, e), Prk (X\E), wk )

11.

wT Score  wT Score + wiScore
(Optional: add wiScore to the corresponding entry of every score array)

12.

wsum  wsum + wk
end for

13. Output estimate of Pr(E) as wT Score /wsum
(Optional: Normalize the score arrays for every node)

Figure 2: The adaptive importance sampling for Bayesian Networks (AIS-BN) algorithm.
score processing in Step 10 is
Pr(si , e)
wiScore = wk k
.
Pr (si )
Note that in this respect the algorithm in Figure 1 becomes a special case of AIS-BN
when wk = 1. The reason why we use wk is that we want to give different weights to
the sampling results obtained at different stages of the algorithm. As each stage updates
the importance function, they will all have different distance from the optimal importance
b k , where 
b k is the standard deviation estimated in
function. We recommend that wk  1/
1
k
stage k using Equation 3. In order to keep w monotonically increasing, if wk is smaller
than wk1 , we adjust its value to wk1 . This weighting scheme may introduce bias into
1. A similar weighting scheme based on variance was apparently developed independently by Ortiz and
Kaelbling (2000), who recommend the weight wk  1/(
bk )2 .

165

fiCheng & Druzdzel

the final result. Since the initial importance sampling functions are often inefficient and
introduce big variance into the results, we also recommend that wk = 0 in the first few
stages of the algorithm. We have designed this weighting scheme to reflect the fact that in
practice estimates with very small estimated variance are usually good estimates.
3.2 Modifying the Sampling Distribution in AIS-BN
Based on the theoretical considerations of Section 2.1, we know that the crucial element of
the algorithm is converging on a good approximation of the optimal importance function.
In what follows, we first give the optimal importance function for calculating Pr(E = e)
and then discuss how to use the structural advantages of Bayesian networks to approximate
this function. In the sequel, we will use the symbol  to denote the importance sampling
function and  to denote the optimal importance sampling function.
Since Pr(X\E, E = e) > 0, from Corollary 1 we have
(X\E) =

Pr(X\E, E = e)
= Pr(X|E = e) .
Pr(E = e)

The following corollary captures this result.
Corollary 2 The optimal importance sampling function  (X\E) for calculating Pr(E = e)
in Equation 6 is Pr(X|E = e).
Although we know the mathematical expression for the optimal importance sampling
function, it is difficult to obtain this function exactly. In our algorithm, we use the following
importance sampling function
(X\E) =

n
Y

Pr(Xi |Pa(Xi ), E) .

(8)

i=1

This function partially considers the effect of all the evidence on every node during the
sampling process. When the network structure is the same as that of the network which
has absorbed the evidence, this function is the optimal importance sampling function. It
is easy to learn and, as our experimental results show, it is a good approximation to the
optimal importance sampling function. Theoretically, when the posterior structure of the
model changes drastically as the result of observed evidence, this importance sampling
function may perform poorly. We have tried to find practical networks where this would
happen, but to the day have not encountered a drastic example of this effect.
From Section 2.2, we know that the score sums corresponding to {xi , pa(Xi ), e} can
yield an unbiased estimator of Pr(xi , pa(Xi ), e). According to the definition of conditional
probability, we can get an estimator of Pr0 (xi |pa(Xi ), e). This can be achieved by maintaining an updating table for every node, the structure of which mimicks the structure of
the CPT. Such tables allow us to decompose the above importance function into components that can be learned individually. We will call these tables the importance conditional
probability tables (ICPT).
Definition 1 An importance conditional probability table (ICPT) of a node X is a table
of posterior probabilities Pr(X|Pa(X), E = e) conditional on the evidence and indexed by
its immediate predecessors, Pa(X).
166

fiAdaptive Importance Sampling in Bayesian Networks

The ICPT tables will be modified during the process of learning the importance function.
Now we will prove a useful theorem that will lead to considerable savings in the learning
process.
Theorem 2
Xi  X, Xi 
/ Anc(E)  Pr(Xi |Pa(Xi ), E) = Pr(Xi |Pa(Xi )) .

(9)

Proof: Suppose we have set the values of all the parents of node Xi to pa(Xi ). Node Xi is
dependent on evidence E given pa(Xi ) only when Xi is d-connecting with E given pa(Xi )
(Pearl, 1988). According to the definition of d-connectivity, this happens only when there
exists a member of Xi s descendants that belongs to the set of evidence nodes E. In other
words Xi 
/ Anc(E).
2
Theorem 2 is very important for the AIS-BN algorithm. It states essentially that
the ICPT tables of those nodes that are not ancestors of the evidence nodes are equal to
the CPT tables throughout the learning process. We only need to learn the ICPT tables
for the ancestors of the evidence nodes. Very often this can lead to significant savings in
computation. If, for example, all evidence nodes are root nodes, we have our ICPT tables for
every node already and the AIS-BN algorithm becomes identical to the likelihood weighting
algorithm. Without evidence, the AIS-BN algorithm becomes identical to the probabilistic
logic sampling algorithm.
It is worth pointing out that for some Xi , Pr(Xi |Pa (Xi ), E) (i.e., the ICPT table for
Xi ), can be easily calculated using exact methods. For example, when Xi is the only parent
of an evidence node Ej and Ej is the only child of Xi , the posterior probability distribution
of Xi is straightforward to compute exactly. Since the focus of the current paper is on
Input: Initialized importance function Pr0 (X\E), learning rate (k).
Output: An estimated importance function PrS (X\E).
for stage k  0 to S do
1. Sample l points sk1 , sk2 , . . . , skl independently according to the current importance function Prk (X\E).
2. For every node Xi such that Xi  X\E and Xi 
/ Anc(E) count score sums
corresponding to {xi , pa(Xi ), e} and estimate Pr0 (xi |pa(Xi ), e) based on sk1 ,
sk2 , . . . , skl .
3. Update Prk (X\E) according to the following formula:
Prk+1 (xi |pa(Xi ), e) =




Prk (xi |pa(Xi ), e) + (k)  Pr0 (xi |pa(Xi ), e)  Prk (xi |pa(Xi ), e)
end for

Figure 3: The AIS-BN algorithm for learning the optimal importance function.
167

fiCheng & Druzdzel

sampling, the test results reported in this paper do not include this improvement of the
AIS-BN algorithm.
Figure 3 lists an algorithm that implements Step 7 of the basic AIS-BN algorithm listed
in Figure 2. When we estimate Pr0 (xi |pa(Xi ), e), we only use the samples obtained at the
current stage. One reason for this is that the information obtained in previous stages has
been absorbed by Prk (X\E). The other reason is that in principle, each successive iteration
is more accurate than the previous one and the importance function is closer to the optimal
importance function. Thus, the samples generated by Prk+1 (X\E) are better than those
generated by Prk (X\E). Pr0 (Xi |pa(Xi ), e)  Prk (Xi |pa(Xi ), e) corresponds to the vector
of first partial derivatives in the direction of the maximum decrease in the error. (k) is
a positive function that determines the learning rate. When (k) = 0 (lower bound), we
do not update our importance function. When (k) = 1 (upper bound), at each stage we
discard the old function. The convergence speed is directly related to (k). If it is small,
the convergence will be very slow due to the large number of updating steps needed to
reach a local minimum. On the other hand, if it is large, convergence rate will be initially
very fast, but the algorithm will eventually start to oscillate and thus may not reach a
minimum. There are many papers in the field of neural network learning that discuss how
to choose the learning rate and let estimated importance function converge quickly to the
destination function. Any method that can improve learning rate should be applicable to
this algorithm. Currently, we use the following function proposed by Ritter et al. (1991)
 k/kmax

(k) = a

b
a

,

(10)

where a is the initial learning rate and b is the learning rate in the last step. This function
has been reported to perform well in neural network learning (Ritter et al., 1991).
3.3 Heuristic Initialization in AIS-BN
The dimensionality of the problem of Bayesian network inference is equal to the number of
variables in a network, which in the networks considered in this paper can be very high.
As a result, the learning space of the optimal importance function is very large. Choice of
the initial importance function Pr0 (X\E) is an important factor affecting the learning 
an initial value of the importance function that is close to the optimal importance function
can greatly affect the speed of convergence. In this section, we present two heuristics that
help to achieve this goal.
Due to their explicit encoding of the structure of a decomposable joint probability distribution, Bayesian networks offer computational advantages compared to finite-dimensional
integrals. A possible first approximation of the optimal importance function is the prior
probability distribution over the network variables, Pr(X). We propose an improvement on
this initialization. We know that the effect of evidence nodes on a node will be attenuated
as the path length of that node to evidence nodes is increased (Henrion, 1989) and the
most affected nodes are the direct ancestors of the evidence nodes. Initializing the ICPT
tables of the parents of the evidence nodes to uniform distributions in our experience improves the convergence rate. Furthermore, the CPT tables of the parents of an evidence
node E may be not favorable to the observed state e if the probability of E = e without
168

fiAdaptive Importance Sampling in Bayesian Networks

any condition is less than a small value, such as Pr(E = e) < 1/(2  nE ), where nE is the
number of outcomes of node E. Based on this observation, we change the CPT tables of
the parents of an evidence node E to uniform distributions in our experiment only when
Pr(E = e) < 1/(2  nE ), otherwise we leave them unchanged. This kind of initialization
involves the knowledge of Pr(E = e), the marginal probability without evidence. Probabilistic logic sampling (Henrion, 1988) enhanced by Latin hypercube sampling (Cheng &
Druzdzel, 2000b) or quasi-Monte Carlo methods (Cheng & Druzdzel, 2000a) will produce a
very good estimate of Pr(E = e). This is an one-time effort that can be made at the model
building stage and is worth pursuing to any desired precision.
Another serious problem related to sampling are extremely small probabilities. Suppose
there exists a root node with a state s that has the prior probability Pr(s) = 0.0001. Let
the posterior probability of this state given evidence be Pr(s|E) = 0.8. A simple calculation
shows that if we update the importance function every 1, 000 samples, we can expect to
hit s only once every 10 updates. Thus ss convergence rate will be very slow. We can
overcome this problem by setting a threshold  and replacing every probability p <  in the
network by .2 At the same time, we subtract (  p) from the largest probability in the
same conditional probability distribution. For example, the value of  = 10/l, where l is
the updating interval, will allow us to sample 10 times more often in the first stage of the
algorithm. If this state turns out to be more likely (having a large weight), we can increase
its probability even more in order to converge to the correct answer faster. Considering
that we should avoid f (X)  |g(X)  I  f (X)| in an unimportant region as discussed in
Section 2.1, we need to make this threshold larger. We have found that the convergence
rate is quite sensitive to this threshold. Based on our empirical tests, we suggest to use
 = 0.04 in networks whose maximum number of outcomes per node does not exceed five.
A smaller threshold might lead to fast convergence in some cases but slow convergence in
others. If one threshold does not work, changing it in a specific network will usually improve
convergence rate.
3.4 Selection of Parameters
There are several tunable parameters in the AIS-BN algorithm. We base the choice of
these parameters on the Central Limit Theorem (CLT). According to CLT, if Z1 , Z2 , . . . ,
Zn are independent and identically distributed random variables with E(Zi ) = Z and
Var(Zi ) = Z2 , i = 1, ..., n, then Z = (Z1 +...+Zn )/n is approximately normally distributed
when n is sufficiently large. Thus,
lim P (

n

fi
fi
fi
fi
fiZ  z fi

z


Z 
2
Z / n
2
ex /2 dx .
 t) = 

z
2 t

(11)

Although this approximation holds when n approaches infinity, CLT is known to be very
robust and lead to excellent approximations even for small n. The formula of Equation 11
is an (r , ) Relative Approximation, which is an estimate  of  that satisfies
P(

|  |
 r )   .


2. This initialization heuristic was apparently developed independently by Ortiz and Kaelbling (2000).

169

fiCheng & Druzdzel

If  has been fixed,


Z / n

r =
 1
Z ( ),
z
2

where Z (z) = 12 z ex /2 dx. Since in our sampling problem, z (corresponding to

Pr(E) in Figure 2) has been fixed, setting r to a smaller value amounts to letting Z / n

be smaller. So, we can adjust the parameters based on Z / n, which can be estimated
bk
using Equation 3. It is also the theoretical intuition behind our recommendation wk  1/
in Section 3.1. While we expect that this should work well in most networks, no guarantees
can be given here  there exist always some extreme cases in sampling algorithms in which
no good estimate of variance can be obtained.
R

2

3.5 A Generalization of AIS-BN: The Problem of Estimating Pr(a|e)
A typical focus of systems based on Bayesian networks is the posterior probability of various
outcomes of individual variables given evidence, Pr(a|e). This can be generalized to the
computation of the posterior probability of a particular instantiation of a set of variables
given evidence, i.e., Pr(A = a|e). There are two methods that are capable of performing
this computation. The first method is very efficient at the expense of precision. The second
method is less efficient, but offers in general better convergence rates. Both methods are
based on Equation 7.
The first method reuses the samples generated to estimate Pr(e) in estimating Pr(a, e).
Estimation of Pr(a, e) amounts to counting the scored sum under the condition A = a.
The main advantage of this method is its efficiency  we can use the same set of samples
to estimate the posterior probability of any state of a subset of the network given evidence.
Its main disadvantage is that the variance of the estimated Pr(a, e) can be large, especially
when the numerical value of Pr(a|e) is extreme. This method is the most widely used
approach in the existing stochastic sampling algorithms.
The second method, used much more rarely (e.g., Cano et al., 1996; Pradhan & Dagum,
1996; Dagum & Luby, 1997), calls for estimating Pr(e) and Pr(a, e) separately. After
estimating Pr(e), an additional call to the algorithm is made for each instantiation a of
the set of variables of interest A. Pr(a, e) is estimated by sampling the network with the
set of observations e extended by A = a. The main advantage of this method is that it
is much better at reducing variance than the first method. Its main disadvantage is the
computational cost associated with sampling for possibly many combinations of states of
nodes of interest.
Cano et al. (1996) suggested a modified version of the second method. Suppose that we
are interested in the posterior distribution Pr(ai |e) for all possible values ai of A, i = 1,
2, . . . , k. We can estimate Pr(ai , e) for each i = 1, . . . , k separately, and use the value
Pk
i=1 Pr(ai , e) as an estimate for Pr(e). The assumption behind this approach is that the
estimate of Pr(e) will be very accurate because of the large sample from which it is drawn.
However, even if we can guarantee small variance in every Pr(ai , e), we cannot guarantee
that their sum will also have a small variance. So, in the AIS-BN algorithm we only use
the pure form of each of the methods. The algorithm listed in Figure 2 is based on the first
method when the optional computations in Steps 12 and 13 are performed. An algorithm
170

fiAdaptive Importance Sampling in Bayesian Networks

corresponding to the second method skips the optional steps and calls the basic AIS-BN
algorithm twice to estimate Pr(e) and Pr(a, e) separately.
The first method is very attractive because of its simplicity and possible computational
efficiency. However, as we have shown in Section 2.2, the performance of a sampling algorithm that uses just one set of samples (as in the first method above) to estimate Pr(a|e)
will deteriorate if the difference between the optimal importance functions for Pr(a,e) and
Pr(e) is large. If the main focus of the computation is high accuracy of the posterior probability distribution of a small number of nodes, we strongly recommend to use the algorithm
based on the second method. Also, this algorithm can be easily used to estimate confidence
intervals of the solution.

4. Experimental Results
In this section, we first describe the experimental method used in our tests. Our tests focus
on the CPCS network, which is one of the largest and most realistic networks available
and for which we know precisely which nodes are observable. We were, therefore, able
to generate very realistic test cases. Since the AIS-BN algorithm uses two initialization
heuristics, we designed an experiment that studies the contribution of each of these two
heuristics to the performance of the algorithm. To probe the extent of AIS-BN algorithms
excellent performance, we test it on several real and large networks.
4.1 Experimental Method
We performed empirical tests comparing the AIS-BN algorithm to the likelihood weighting
(LW) and the self-importance sampling (SIS) algorithms. The two algorithms are basically
the state of the art general purpose belief updating algorithms. The AA (Dagum et al.,
1995) and the bounded variance (Dagum & Luby, 1997) algorithms, which were suggested
by a reviewer, are essentially enhanced special purpose versions of the basic LW algorithm.
Our implementation of the three algorithms relied on essentially the same code with separate
functions only when the algorithms differed. It is fair to assume, therefore, that the observed
differences are purely due to the theoretical differences among the algorithms and not due to
the efficiency of implementation. In order to make the comparison of the AIS-BN algorithm
to LW and SIS fair, we used the first method of computation (Section 3.5), i.e., one that
relies on single sampling rather than calling the basic AIS-BN algorithm twice.
We measured the accuracy of approximation achieved by the simulation in terms of the
Mean Square Error (MSE), i.e., square root of the sum of square differences between Pr0 (xij )
and Pr(xij ), the sampled and the exact marginal probabilities of state j (j = 1, 2, . . . , ni )
of node i, such that Xi 
/ E. More precisely,
v
u
u
MSE = t P

1

Xi N\E ni

X

ni
X

(Pr0 (xij )  Pr(xij ))2 ,

Xi N\E j=1

where N is the set of all nodes, E is the set of evidence nodes, and ni is the number of
outcomes of node i. In all diagrams, the reported MSE is averaged over 10 runs. We used
the clustering algorithm (Lauritzen & Spiegelhalter, 1988) to compute the gold standard
171

fiCheng & Druzdzel

results for our comparisons of the mean square error. We performed all experiments on a
Pentium II, 333 MHz Windows computer.
While MSE is not perfect, it is the simplest way of capturing error that lends itself to
further theoretical analysis. For example, it is possible to derive analytically the idealized
convergence rate in terms of MSE, which, in turn, can be used to judge the quality of the
algorithm. MSE has been used in virtually all previous tests of sampling algorithms, which
allows interested readers to tie the current results to the past studies. A reviewer offered
an interesting suggestion of using cross-entropy or some other technique that weights small
changes near zero much more strongly than the equivalent size change in the middle of
the [0, 1] interval. Such measure would penalize the algorithm for imprecisions of possibly
several orders of magnitude in very small probabilities. While this idea is interesting, we
are not aware of any theoretical reasons as to why this measure would make a difference in
comparisons between AIS-BN, LW and SIS algorithms. The MSE, as we mentioned above,
will allow us to compare the empirically determined convergence rate to the theoretically
derived ideal convergence rate. Theoretically, the MSE is inversely proportional to the
square root of the sample size.
Since there are several tunable parameters used in the AIS-BN algorithm, we list the
values of the parameters used in our test: l = 2, 500; wk = 0 for k  9 and wk = 1
otherwise. We stopped the updating process in Step 7 of Figure 2 after k  10. In other
words, we used only the samples collected in the last step of the algorithm. The learning
parameters used in our algorithm are kmax = 10, a = 0.4, and b = 0.14 (see Equation 10).
We used an empirically determined value of the threshold  = 0.04 (Section 3.3). We only
change the CPT tables of the parents of a special evidence node A to uniform distributions
when Pr(A = a) < 1/(2  nA ). Some of the parameters are a matter of design decision
(e.g., the number of samples in our tests), others were chosen empirically. Although we
have found that these parameters may have different optimal values for different Bayesian
networks, we used the above values in all our tests of the AIS-BN algorithm described in
this paper. Since the same set of parameters led to spectacular improvement in accuracy
in all tested networks, it is fair to say that the superiority of the AIS-BN algorithm to the
other algorithms is not too sensitive to the values of the parameters.
For the SIS algorithm, wk = 1 by the design of the algorithm. We used l = 2, 500. The
updating function in Step 7 of Figure 1 is that of (Shwe et al., 1991; Cousins, Chen, &
Frisse, 1993):
Prknew (xi |pa(Xi ), e) =

c
Pr(xi |pa(Xi )) + k  Pr
current (xi |pa(Xi ), e) ,
1+k

c
where Pr(xi |pa(Xi )) is the original sampling distribution, Pr
current (xi |pa(Xi ), e) is an
equivalent of our ICPT tables estimator based on all currently available information, and
k is the updating step.

4.2 Results for the CPCS Network
The main network used in our tests is a subset of the CPCS (Computer-based Patient Case
Study) model (Pradhan et al., 1994), a large multiply-connected multi-layer network consisting of 422 multi-valued nodes and covering a subset of the domain of internal medicine.
172

fiAdaptive Importance Sampling in Bayesian Networks

Among the 422 nodes, 14 nodes describe diseases, 33 nodes describe history and risk factors, and the remaining 375 nodes describe various findings related to the diseases. The
CPCS network is among the largest real networks available to the research community at
the present time. The CPCS network contains many extreme probabilities, typically on
the order of 104 . Our analysis is based on a subset of 179 nodes of the CPCS network,
created by Max Henrion and Malcolm Pradhan. We used this smaller version in order to
be able to compute the exact solution for the purpose of measuring approximation error in
the sampling algorithms.
The AIS-BN algorithm has some learning overhead. The following comparison of execution time vs. number of samples may give the reader an idea of this overhead. Updating
the CPCS network with 20 evidence nodes on our system takes the AIS-BN algorithm a
total of 8.4 seconds to learn. It generates subsequently 3,640 samples per second, while the
SIS algorithm generates 2,631 samples per second, and the LW algorithm generates 4,167
samples per second. In order to remain conservative towards the AIS-BN algorithm, in all
our experiments we fixed the execution time of the algorithms (our limit was 60 seconds)
rather than the number of samples. In the CPCS network with 20 evidence nodes, in 60
seconds, AIS-BN generates about 188,000 samples, SIS generates about 158,000 samples
and LW generates about 250,000 samples.

12
100%
90%

10

80%

Frequency

8

70%
60%

6

50%
40%

4
30%
20%

2

10%
0

0%
1E-40

1E-34

1E-28

1E-22

1E-16

1E-10

Probability of evidence

Figure 4: The probability distribution of evidence Pr(E = e) in our experiments.
We generated a total of 75 test cases consisting of five sequences of 15 test cases each. We
ran each test case 10 times, each time with a different setting of the random number seed.
Each sequence had a progressively higher number of evidence nodes: 15, 20, 25, 30, and
35 evidence nodes respectively. The evidence nodes were chosen randomly (equiprobable
sampling without replacement) from those nodes that described various plausible medical
173

fiCheng & Druzdzel

findings. Almost all of these nodes were leaf nodes in the network. We believe that this
constituted very realistic test cases for the algorithms. The distribution of the prior probability of evidence, Pr(E = e), across all test runs of our experiments is shown in Figure 4.
The least likely evidence was 5.54  1042 , the most likely evidence was 1.37  109 , and
the median was 7  1024 .

0.30

AIS-BN

SIS

LW

Mean Square Error

0.25

0.20

0.15

0.10

0.05

0.00
15

30

45

60

75

90

105

120

135

150

Sample time (seconds)

Figure 5: A typical plot of convergence of the tested sampling algorithms in our experiments
 Mean Square Error as a function of the execution time for a subset of the
CPCS network with 20 evidence nodes chosen randomly among plausible medical
observations (Pr(E = e) = 3.33  1026 in this particular case) for the AIS-BN,
the SIS, and the LW algorithms. The curve for the AIS-BN algorithm is very
close to the horizontal axis.

Figures 5 and 6 show a typical plot of convergence of the tested sampling algorithms in
our experiments. The case illustrated involves updating the CPCS network with 20 evidence
nodes. We plot the MSE after the initial 15 seconds during which the algorithms start
converging. In particular, the learning step of the AIS-BN algorithm is usually completed
within the first 9 seconds. We ran the three algorithms in this case for 150 seconds rather
than the 60 seconds in the actual experiment in order to be able to observe a wider range of
convergence. The plot of the MSE for the AIS-BN algorithm almost touches the X axis in
Figure 5. Figure 6 shows the same plot in a finer scale in order to show more detail in the
AIS-BN convergence curve. It is clear that the AIS-BN algorithm dramatically improves
the convergence rate. We can also see that the results of AIS-BN converge to exact results
very fast as the sampling time increases. In the case captured in Figures 5 and 6, a tenfold
increase in the sampling time (after subtracting the overhead for the AIS-BN algorithm, it
174

fiAdaptive Importance Sampling in Bayesian Networks

0.0025

AIS-BN

Mean Square Error

0.0020

0.0015

0.0010

0.0005

0.0000
15

30

45

60

75

90

105

120

135

150

Sample time (seconds)

Figure 6: The lower part of the plot of Figure 5 showing the convergence of the AIS-BN
algorithm to correct posterior probabilities.

corresponds to a 21.5-fold increase in the number of samples) results in a 4.55-fold decrease
of the MSE (to MSE 0.00048). The observed convergence of both SIS and LW algorithms
was poor. A tenfold increase in sampling time had practically no effect on accuracy. Please
note that this is a very typical case observed in our experiments.

Absent
Mild
Moderate
Severe

Original CPT
0.99631
0.00183
0.00093
0.00093

Exact ICPT
0.0037
0.1560
0.1190
0.7213

Learned ICPT
0.015
0.164
0.131
0.690

Table 1: A fragment of the conditional probability table of a node of the CPCS network
(node gasAcute, parents hepAcute=Mild and wbcTotTho=False) in Figure 6.

Figure 7 illustrates the ICPT learning process of the AIS-BN algorithm for the sample
case shown in Figure 6. The displayed conditional probabilities belong to the node gasAcute
which is a parent of two evidence nodes, difInfGasMuc and abdPaiExaMea. The node
gasAcute has four states: absent, mild, moderate, and severe, and two parents.
We randomly chose a combination of its parents states as our displayed configuration. The
original CPT for this configuration without evidence, the exact ICPT with evidence and
the learned ICPT with evidence are summarized numerically in Table 1. Figure 7 illustrates
175

fiCheng & Druzdzel

0.8

Absent

Mild

Moderate

Severe

0.7

Probability

0.6
0.5
0.4
0.3
0.2
0.1
0
0

1

2

3

4

5

6

7

8

9

10

Updating step

Figure 7: Convergence of the conditional probabilities during the example run of the AISBN algorithm captured in Figure 6. The displayed fragment of the conditional
probability table belongs to node gasAcute which is a parent of one of the evidence
nodes.

that the learned importance conditional probabilities begin to converge to the exact results
stably after three updating steps. The learned probabilities in Step 10 are close to the
exact results. In this example, the difference between Pr(xi |pa(Xi ), e) and Pr(xi |pa(Xi )) is
very large. Sampling from Pr(xi |pa(Xi )) instead of Pr(xi |pa(Xi ), e) would introduce large
variance into our results.



min
median
max

AIS-BN
0.00082
0.00022
0.00049
0.00078
0.00184

SIS
0.110
0.076
0.0016
0.105
0.316

LW
0.148
0.093
0.0031
0.154
0.343

Table 2: Summary of the simulation results for all of the 75 simulation cases on the CPCS
network. Figure 8 shows each of the 75 cases graphically.

Figure 8 shows the MSE for all 75 test cases in our experiments with the summary
statistics in Table 2. A paired one-tailed t-test resulted in statistically highly significant
differences between the AIS-BN and SIS algorithms (p < 3.1  1020 ), and also between
176

fiAdaptive Importance Sampling in Bayesian Networks

1

AIS-BN

SIS

LW

Mean Square Error

0.1

0.01

0.001

0.0001
1.4E-09 2.1E-14 3.1E-18 5.7E-22 6.7E-24 1.4E-27 1.3E-32 3.8E-39

Probability of evidence

Figure 8: Performance of the AIS-BN, SIS, and LW algorithms: Mean Square Error for
each of the 75 individual test cases plotted against the probability of evidence.
The sampling time is 60 seconds.

the SIS and LW algorithms (p < 1.7  108 ). As far as the magnitude of difference is
concerned, AIS-BN was significantly better than SIS. SIS was better than LW, but the
difference was small. The mean MSEs of SIS and LW algorithms were both greater than
0.1, which suggests that neither of these algorithms is suitable for large Bayesian networks.
The graph in Figure 9 shows the MSE ratio between the AIS-BN and SIS algorithms.
We can see that the percentage of the cases whose ratio was greater than 100 (two orders
of magnitude improvement!) is 60%. In other words, we obtained two orders of magnitude
improvement in MSE in more than half of the cases. In 80% cases, the ratio was greater
than 50. The smallest ratio in our experiments was 2.67, which happened when posterior
probabilities were dominated by the prior probabilities. In that case, even though the LW
and SIS algorithms converged very fast, their MSE was still far larger than that of AIS-BN.
Our next experiment aimed at showing how close the AIS-BN algorithm can approach
the best possible sampling results. If we know the optimal importance sampling function,
the convergence of the AIS-BN algorithm should be the same as that of forward sampling
without evidence. In other words, the results of the probabilistic logic sampling algorithm
without evidence approach the limit of how well stochastic sampling can perform. We ran
the logic sampling algorithm on the CPCS network without evidence mimicking the test
runs of the AIS-BN algorithm, i.e., 5 blocks of 15 runs, each repeated 10 times with a
different random number seed. The number of samples generated was equal to the average
number of samples generated by the AIS-BN algorithm for each series of 15 test runs.
177

fiCheng & Druzdzel

18

100%

16

90%
80%

14

70%

Frequency

12

60%
10
50%
8
40%
6

30%

4

20%

2
0

10%
 

	
 fffiff ffff   

0%

The ratio of MSE between SIS and AIS-BN

Figure 9: The ratio of MSE between SIS and AIS-BN versus percentage.

We obtained the average MSE  = 0.00057, with  = 0.000025, min = 0.00052, and
max = 0.00065. The best results should be around this range. From Table 2, we can
see that the minimum MSE for the AIS-BN algorithm was 0.00049, within the range of
the optimal result. The mean MSE in AIS-BN is 0.00082, not too far from the optimal
results. The standard deviation, , is significantly larger in the AIS-BN algorithm, but
this is understandable given that the process of learning the optimal importance function is
heuristic in nature. It is not difficult to understand that there exist a difference between the
AIS-BN results and the optimal results. First, the AIS-BN algorithm in our tests updated
the sampling distribution only 10 times, which may be too few times to let it converge
to the optimal importance distribution. Second, even if the algorithm has converged to
the optimal importance distribution, the sampling algorithm will still let the parameter
oscillate around this distribution and there will be always small differences between the two
distributions.
Figure 10 shows the convergence rate for all tested cases for a four-fold increase in
sampling time (between 15 and 60 seconds). We adjusted the convergence ratio of the
AIS-BN algorithm by dividing it by a constant. According to Equation 3, the theoretically
expected convergence ratio for a four-fold increase in the number of samples should be
around two. There are about 96% cases among the AIS-BN runs whose ratio lays in
the interval (1.75, 2.25], in a sharp contrast to 11% and 13% cases in the SIS and LW
algorithms. The ratios of the remaining 4% cases in AIS-BN lay in the interval [2.25, 2.5].
In the SIS and LW algorithms, the percentage of cases whose ratio were smaller than 1.5
was 71% and 77% respectively. Less than 1.5 means that the number of samples was too
small to estimate variance and the results cannot be trusted. The ratio greater than 2.25
178

fiAdaptive Importance Sampling in Bayesian Networks

70%

AIS-BN

SIS

LW
59%

60%

Frequency

50%

40%

37%

36%
35%

30%
20%

20%

20%
19%

11%

9%9%

10%
5%

8%
5%

5%5%

3%
0%

0%

4%

3%

3%

4%

0%

0%
0.5 - 0.75 0.75 - 1.0 1.0 - 1.25 1.25 - 1.5 1.5 - 1.75 1.75 - 2.0 2.0 - 2.25 2.25 - 2.5 2.5 - 2.75 2.75 - 3.0

more

Convergence rate

Figure 10: The distribution of the convergence ratio of the AIS-BN, SIS, and LW algorithms when the number of samples increases four times.

means possibly that 60 seconds was long enough to estimate the variance, but 15 seconds
was too short.
4.3 The Role of AIS-BN Heuristics in Performance Improvement
From the above experimental results we can see that the AIS-BN algorithm can improve
the sampling performance significantly. Our next series of tests focused on studying the role
of the two AIS-BN initialization heuristics. The first is initializing the ICPT tables of the
parents of evidence to uniform distributions, denoted by U. The second is adjusting small
probabilities, denoted by S. We denote AIS-BN without any heuristic initialization method
to be the AIS algorithm. AIS+U+S equals AIS-BN. We compared the following versions
of the algorithms: SIS, AIS, SIS+U, AIS+U, SIS+S, AIS+S, SIS+U+S, AIS+U+S. All
algorithms with SIS used the same number of samples as SIS. All algorithms with AIS used
the same number of samples as AIS-BN. We tested these algorithms on the same 75 test
cases used in the previous experiment. Figure 11 shows the MSE for each of the sampling
algorithms with the summary statistics in Table 3. Even though the AIS algorithm is better
than the SIS algorithm, the difference is not as large as in case of the AIS+U, AIS+S, and
AIS-BN algorithms. It seems that heuristic initialization methods help much. The results
for the SIS+S, SIS+U, SIS+U+S algorithms suggest that although heuristic initialization
methods can improve performance, they alone cannot improve too much. It is fair to say
that significant performance improvement in the AIS-BN algorithm is coming from the
combination of AIS with heuristic methods, not any method alone. It is not difficult to
179

fiCheng & Druzdzel

understand that, as only with good heuristic initialization methods is it possible to let the
learning process quickly exit oscillation areas. Although both S and U methods alone can
improve the performance, the improvement is moderate compared to the combination of
the two.

0.12

0.110

Mean Square Error

0.10

0.075

0.08

0.060
0.06

0.050

0.050

0.04

0.02

0.008
0.00151
0.00

   

  
fi	 ff     
fi	 ff

  ff     fiff

0.00082
  	  ff        ff

Different Algorithms

Figure 11: A comparison of different algorithms in the CPCS network. Each bar is based
on 75 test cases. The dotted bar shows the MSE for the SIS algorithm while
the gray bar shows the MSE for the AIS algorithm.



min
median
max

SIS
0.110
0.076
0.0016
0.105
0.316

AIS
0.060
0.049
0.00074
0.045
0.207

SIS+U
0.050
0.052
0.0011
0.031
0.212

AIS+U
0.0084
0.025
0.00058
0.0014
0.208

SIS+S
0.075
0.074
0.00072
0.052
0.279

AIS+S
0.0015
0.0016
0.00056
0.00087
0.0085

SIS+U+S
0.050
0.059
0.00086
0.028
0.265

AIS-BN
0.00082
0.00022
0.00049
0.00078
0.0018

Table 3: Summary of the simulation results for different algorithms in the CPCS network.

4.4 Results for Other Networks
In order to make sure that the AIS-BN algorithm performs well in general, we tested it on
two other large networks.
The first network that we used in our tests is the PathFinder network (Heckerman
et al., 1990), which is the core element of an expert system that assists surgical pathologists
180

fiAdaptive Importance Sampling in Bayesian Networks

with the diagnosis of lymph-node diseases. There are two versions of this network. We used
the larger version, consisting of 135 nodes. In contrast to the CPCS network, PathFinder
contains many conditional probabilities that are equal to 1, which reflects deterministic
relationships in certain settings. To make the sampling challenging, we randomly selected
20 evidence nodes from among the leaf nodes. Each of these was an observable node (David
Heckerman, personal communication). We verified in each case that the probability of so
selected evidence was not equal to zero.
We fixed the execution time of the algorithms to be 60 seconds. The learning overhead
for the AIS-BN algorithm in the PathFinder network was about 3.5 seconds. In 60
seconds, AIS-BN generated about 366,000 samples, SIS generated about 250,000 samples
and LW generated about 2,700,000 samples. The reason why LW could generate more than
10 times as many samples as SIS within the same amount of time is that the LW algorithm
terminates sample generation at a very early stage in many samples, when the weight of a
sample becomes zero. This is a result of determinism in the probability tables, mentioned
above. We will see that LW benefits greatly from generating more samples. The other
parameters used in AIS-BN were the same as those used in the CPCS network.
We tested 20 cases, each with randomly selected 20 evidence nodes. The reported MSE
for each case was averaged over 10 runs. Some of the runs of the SIS and LW algorithms did
not manage to generate any effective samples (the weight score sum was equal to zero). SIS
had only 75% effective runs and LW had only 89% effective runs, which means that in some
runs SIS and LW were unable to yield any information about the posterior distributions.
In all those cases, we discarded the run and only averaged over the effective runs. All
runs in the AIS-BN algorithm were effective. We report our experimental results with the
summary statistics in Table 4. From these data, we can see that the AIS-BN algorithm
is still significantly better than the SIS and LW algorithms. Since the LW algorithm can
generate more than ten times the number of samples than the SIS algorithm, its performance
is better than that of the SIS algorithm.



min
median
max
effective runs

AIS-BN
0.00050
0.00037
0.00025
0.00037
0.0017
200

SIS
0.166
0.107
0.00116
0.184
0.467
150

LW
0.089
0.0707
0.00080
0.0866
0.294
178

Table 4: Summary of the simulation results for all of the 20 simulation cases on the
PathFinder network.

The second network that we tested was one of the ANDES networks (Conati et al.,
1997). ANDES is an intelligent tutoring system for classical Newtonian physics that is
being developed by a team of researchers at the Learning Research and Development Center
at the University of Pittsburgh and researchers at the United States Naval Academy. The
student model in ANDES uses a Bayesian network to do longterm knowledge assessment,
181

fiCheng & Druzdzel

plan recognition, and prediction of students actions during problem solving. We selected
the largest ANDES network that was available to us, consisting of 223 nodes.
In contrast to the previous two networks, the depth of the ANDES network was significantly larger and so was its connectivity. There were only 22 leaf nodes. It is quite
predictable that this kind of networks will pose difficulties to learning. We selected 20
evidence nodes randomly from the potential evidence nodes and tested 20 cases. All parameters were the same as those used in the CPCS network. We fixed the execution time
of the algorithms to be 60 seconds. The learning overhead for the AIS-BN algorithm in
the ANDES network was 13.4 seconds. In 60 seconds, AIS-BN generated about 114,000
samples, SIS generated about 98,000 samples and LW generated about 180,000 samples.
In this network, LW still can generate almost two times the number of samples generated
by the SIS algorithm.
We report our experimental results with the summary statistics in Table 5. The results
show that also in the ANDES network the AIS-BN algorithm was significantly better than
the SIS and LW algorithms. Since LW generated almost two times the number of samples
that were generated by the SIS algorithm, its performance was better than that of the SIS
algorithm.



min
median
max

AIS-BN
0.0059
0.0049
0.0023
0.0045
0.0237

SIS
0.0628
0.102
0.0028
0.0190
0.321

LW
0.0404
0.0539
0.0028
0.0198
0.221

Table 5: Summary of the simulation results for all of the 20 simulation cases on the ANDES
network.

While the AIS-BN algorithm is on the average an order of magnitude more precise
than the other two algorithms, the performance improvement is smaller than in the other
two networks. The reason why the performance improvement of the AIS-BN algorithm
over the SIS and LW algorithms in the ANDES network is smaller compared to that in
the CPCS and PathFinder networks is that: (1) The ANDES network used in our tests
was apparently not challenging enough for sampling algorithms in general. In the ANDES
network, SIS and LW also can perform well in some cases. The minimum MSE of SIS and
LW in our tested cases is almost the same as that of AIS-BN. (2) The number of samples
generated by AIS-BN in this network is significantly smaller than that in the previous
two networks and AIS-BN needs more time to learn. Although increasing the number of
samples will improve the performance of all three algorithms, it improves the performance
of AIS-BN more since the convergence ratio of the AIS-BN algorithm is usually larger than
that of SIS and LW (see Figure 10). (3) The parameters that we used in this network were
tuned for the CPCS network. (4) The large depth and fewer leaf nodes of the ANDES
network pose some difficulties to learning.
182

fiAdaptive Importance Sampling in Bayesian Networks

5. Discussion
There is a fundamental trade-off in the AIS-BN algorithm between the time spent on
learning the importance function and the time spent on sampling. Our current approach,
which we believe to be reasonable, is to stop learning at the point when the importance
function is good enough. In our experiments we stopped learning after 10 iterations.
There are several ways of improving the initialization of the conditional probability
tables at the outset of the AIS-BN algorithm. In the current version of the algorithm, we
initialize the ICPT table of every parent N of an evidence node E (N  Pa(E), E  E)
to the uniform distribution when Pr(E = e) < 1/(2  nE ). This can be improved further.
We can extend the initialization to those nodes that are severely affected by the evidence.
They can be identified by examining the network structure and local CPTs.
We can view the learning process of the AIS-BN algorithm as a network rebuilding
process. The algorithm constructs a new network whose structure is the same as the original
network (except that we delete the evidence nodes and corresponding arcs). The constructed
network models the joint probability distribution (X\E) in Equation 8, which approaches
the optimal importance function. We use the learned 0 to approximate this distribution.
If 0 approximates Pr(X|E) accurately enough, we can use this new network to solve other
approximate tasks, such as the problem of computing the Maximum A-Posterior assignment
(MAP) (Pearl, 1988), finding k most likely scenarios (Seroussi & Golmard, 1994), etc. A
large advantage of this approach is that we can solve each of these problems as if the network
had no evidence nodes.
We know that Markov blanket scoring can improve convergence rates in some sampling
algorithms (Shwe & Cooper, 1991). It may also be applied to the AIS-BN algorithm to
improve its convergence rate. According to Property 4 (Section 2.1), any technique that can
2
c
reduce the variance Pr
will reduce the variance of Pr(e)
and correspondingly improve
(e)
the sampling performance. Since the variance of stratified sampling (Rubinstein, 1981) is
never much worse than that of random sampling, and can be much better, it can improve the
convergence rate. We expect some other variance reduction methods in statistics, such as:
(i) the expected value of a random variable; (ii) antithetic variants correlations (stratified
sampling, Latin hypercube sampling, etc.); and (iii) systematic sampling, will also improve
the sampling performance.
Current learning algorithm used a simple approach. Some heuristic learning methods,
such as adjusting learning rates according to changes of the error (Jacobs, 1988), should
also be applicable to our algorithm. There are several tunable parameters in the AIS-BN
algorithm. Finding the optimal values of these parameters for any given network is another
interesting research topic.
It is worth observing that the plots presented in Figure 8 are fairly flat. In other words,
in our tests the convergence of the sampling algorithms did not depend too strongly on the
probability of evidence. This seems to contradict the common belief that forward sampling
schemes suffer from unlikely evidence. AIS-BN for one shows a fairly flat plot. The
convergence of the SIS and LW algorithms seems to decrease slightly with unlikely evidence.
It is possible that all three algorithms will perform much worse when the probability of
evidence drops below some threshold value, which our tests failed to approach. Until this
183

fiCheng & Druzdzel

relationship has been studied carefully, we conjecture that the probability of evidence is not
a good measure of difficulty of approximate inference.
Given that the problem of approximating probabilistic inference is NP-hard, there exist
networks that will be challenging for any algorithm and we have no doubt that even the
AIS-BN algorithm will perform poorly on them. To the day, we have not found such
networks. There is one characteristic of networks that may be challenging to the AIS-BN
algorithm. In general, when the number of parameters that need to be learned by the AISBN algorithm increases, its performance will deteriorate. Nodes with many parents, for
example, are challenging to the AIS-BN learning algorithm, as it has to update the ICPT
tables under all combinations of the parent nodes. It is possible that conditional probability
distributions with causal independence properties, such as Noisy-OR distributions (Pearl,
1988; Henrion, 1989; Diez, 1993; Srinivas, 1993; Heckerman & Breese, 1994), common in
very large practical networks, can be treated differently and lead to considerable savings in
the learning time.
One direction of testing approximate algorithms, suggested to us by a reviewer, is to use
very large networks for which exact solution cannot be computed at all. In this case, one
can try to infer from the difference in variance at various stages of the algorithm whether
it is converging or not. This is a very interesting idea that is worth exploring, especially
when combined with theoretical work on stopping criteria in the line of the work of Dagum
and Luby (1997).

6. Conclusion
Computational complexity remains a major problem in application of probability theory
and decision theory in knowledge-based systems. It is important to develop schemes that
improve the performance of updating algorithms  even though the theoretically demonstrated worst case will remain NPhard, many practical cases may become tractable.
In this paper, we studied importance sampling in Bayesian networks. After reviewing
the most important theoretical results related to importance sampling in finite-dimensional
integrals, we proposed a new algorithm for importance sampling in Bayesian networks that
we call adaptive importance sampling (AIS-BN). While the process of learning the optimal
importance function for the AIS-BN algorithm is computationally intractable, based on the
theory of importance sampling in finite-dimensional integrals we proposed several heuristics
that seem to work very well in practice. We proposed heuristic methods for initializing the
importance function that we have shown to accelerate the learning process, a smooth learning method for updating importance function using the structural advantages of Bayesian
networks, and a dynamic weighting function for combining samples from different stages
of the algorithm. All these methods help the AIS-BN algorithm to get fairly accurate
estimates of the posterior probabilities in a limited time. Of the two applied heuristics,
adjustment of small probabilities, seems to lead to the largest improvement in performance,
although the largest decrease in MSE is achieved by a combination of the two heuristics
with the AIS-BN algorithm.
The AIS-BN algorithm can lead to a dramatic improvement in the convergence rates in
large Bayesian networks with evidence compared to the existing state of the art algorithms.
We compared the performance of the AIS-BN algorithm to the performance of likelihood
184

fiAdaptive Importance Sampling in Bayesian Networks

weighting and self-importance sampling on a large practical model, the CPCS network,
with evidence as unlikely as 5.54  1042 and typically 7  1.024 . In our experiments, we
observed that the AIS-BN algorithm was always better than likelihood weighting and selfimportance sampling and in over 60% of the cases it reached over two orders of magnitude
improvement in accuracy. Tests performed on the other two networks, PathFinder and
ANDES, yielded similar results.
Although there may exist other approximate algorithms that will prove superior to AISBN in networks with special structure or distribution, the AIS-BN algorithm is simple
and robust for general evidential reasoning problems in large multiply-connected Bayesian
networks.

Acknowledgments
We thank anonymous referees for several insightful comments that led to a substantial
improvement of the paper. This research was supported by the National Science Foundation
under Faculty Early Career Development (CAREER) Program, grant IRI9624629, and by
the Air Force Office of Scientific Research grants F496209710225 and F49620001
0112. An earlier version of this paper has received the 2000 School of Information Sciences
Robert R. Korfhage Award, University of Pittsburgh. Malcolm Pradhan and Max Henrion
of the Institute for Decision Systems Research shared with us the CPCS network with a kind
permission from the developers of the Internist system at the University of Pittsburgh. We
thank David Heckerman for the PathFinder network and Abigail Gerner for the ANDES
network used in our tests. All experimental data have been obtained using SMILE, a
Bayesian inference engine developed at the Decision Systems Laboratory and available at
http://www2.sis.pitt.edu/genie.

References
Cano, J. E., Hernandez, L. D., & Moral, S. (1996). Importance sampling algorithms for the
propagation of probabilities in belief networks. International Journal of Approximate
Reasoning, 15, 7792.
Chavez, M. R., & Cooper, G. F. (1990). A randomized approximation algorithm for probabilistic inference on Bayesian belief networks. Networks, 20 (5), 661685.
Cheng, J., & Druzdzel, M. J. (2000a). Computational investigations of low-discrepancy
sequences in simulation algorithms for Bayesian networks. In Proceedings of the Sixteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI2000), pp.
7281 San Francisco, CA. Morgan Kaufmann Publishers.
Cheng, J., & Druzdzel, M. J. (2000b). Latin hypercube sampling in Bayesian networks. In
Proceedings of the 13th International Florida Artificial Intelligence Research Symposium Conference (FLAIRS-2000), pp. 287292 Orlando, Florida.
Conati, C., Gertner, A. S., VanLehn, K., & Druzdzel, M. J. (1997). On-line student modeling
for coached problem solving using Bayesian networks. In Proceedings of the Sixth
185

fiCheng & Druzdzel

International Conference on User Modeling (UM96), pp. 231242 Vienna, New York.
Springer Verlag.
Cooper, G. F. (1990). The computational complexity of probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42 (23), 393405.
Cousins, S. B., Chen, W., & Frisse, M. E. (1993). A tutorial introduction to stochastic
simulation algorithm for belief networks. In Artificial Intelligence in Medicine, chap. 5,
pp. 315340. Elsevier Science Publishers B.V.
Dagum, P., Karp, R., Luby, M., & Ross, S. (1995). An optimal algorithm for Monte
Carlo estimation (extended abstract). In Proceedings of the 36th IEEE Symposium
on Foundations of Computer Science, pp. 142149 Portland, Oregon.
Dagum, P., & Luby, M. (1993). Approximating probabilistic inference in Bayesian belief
networks is NP-hard. Artificial Intelligence, 60 (1), 141153.
Dagum, P., & Luby, M. (1997). An optimal approximation algorithm for Bayesian inference.
Artificial Intelligence, 93, 127.
Diez, F. J. (1993). Parameter adjustment in Bayes networks. The generalized noisy ORgate. In Proceedings of the Ninth Annual Conference on Uncertainty in Artificial
Intelligence (UAI93), pp. 99105 San Francisco, CA. Morgan Kaufmann Publishers.
Fishman, G. S. (1995). Monte Carlo: concepts, algorithms, and applications. SpringerVerlag.
Fung, R., & Chang, K.-C. (1989). Weighing and integrating evidence for stochastic simulation in Bayesian networks. In Uncertainty in Artificial Intelligence 5, pp. 209219
New York, N. Y. Elsevier Science Publishing Company, Inc.
Fung, R., & del Favero, B. (1994). Backward simulation in Bayesian networks. In Proceedings
of the Tenth Annual Conference on Uncertainty in Artificial Intelligence (UAI94),
pp. 227234 San Francisco, CA. Morgan Kaufmann Publishers.
Geman, S., & Geman, D. (1984). Stochastic relaxations, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6 (6), 721742.
Gilks, W., Richardson, S., & Spiegelhalter, D. (1996). Markov chain Monte Carlo in practice. Chapman and Hall.
Heckerman, D., & Breese, J. S. (1994). A new look at causal independence. In Proceedings
of the Tenth Annual Conference on Uncertainty in Artificial Intelligence (UAI94),
pp. 286292 San Mateo, CA. Morgan Kaufmann Publishers, Inc.
Heckerman, D. E., Horvitz, E. J., & Nathwani, B. N. (1990). Toward normative expert
systems: The Pathfinder project. Tech. rep. KSL9008, Medical Computer Science
Group, Section on Medical Informatics, Stanford University, Stanford, CA.
186

fiAdaptive Importance Sampling in Bayesian Networks

Henrion, M. (1988). Propagating uncertainty in Bayesian networks by probabilistic logic
sampling. In Uncertainty in Artificial Intellgience 2, pp. 149163 New York, N. Y.
Elsevier Science Publishing Company, Inc.
Henrion, M. (1989). Some practical issues in constructing belief networks. In Kanal, L.,
Levitt, T., & Lemmer, J. (Eds.), Uncertainty in Artificial Intelligence 3, pp. 161173.
Elsevier Science Publishers B.V., North Holland.
Henrion, M. (1991). Search-based methods to bound diagnostic probabilities in very large
belief nets. In Proceedings of the Seventh Annual Conference on Uncertainty in Artificial Intelligence (UAI91), pp. 142150 San Mateo, California. Morgan Kaufmann
Publishers.
Hernandez, L. D., Moral, S., & Antonio, S. (1998). A Monte Carlo algorithm for probabilistic
propagation in belief networks based on importance sampling and stratified simulation
techniques. International Journal of Approximate Reasoning, 18, 5391.
Jacobs, R. A. (1988). Increased rates of convergence through learning rate adaptation.
Neural Networks, 1, 295307.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computations with probabilities on
graphical structures and their application to expert systems. Journal of the Royal
Statistical Society, Series B (Methodological), 50 (2), 157224.
MacKay, D. (1998). Intro to Monte Carlo methods. In Jordan, M. I. (Ed.), Learning in
Graphical Models. The MIT Press, Cambridge, Massachusetts.
Ortiz, L. E., & Kaelbling, L. P. (2000). Adaptive importance sampling for estimation in
structured domains. In Proceedings of the Sixteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI2000), pp. 446454 San Francisco, CA. Morgan
Kaufmann Publishers.
Pearl, J. (1986). Fusion, propagation, and structuring in belief networks. Artificial Intelligence, 29 (3), 241288.
Pearl, J. (1987). Evidential reasoning using stochastic simulation of causal models. Artifical
Intelligence, 32, 245257.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann Publishers, Inc., San Mateo, CA.
Pradhan, M., & Dagum, P. (1996). Optimal Monte Carlo inference. In Proceedings of the
Twelfth Annual Conference on Uncertainty in Artificial Intelligence (UAI96), pp.
446453 San Francisco, CA. Morgan Kaufmann Publishers.
Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering
for large belief networks. In Proceedings of the Tenth Annual Conference on Uncertainty in Artificial Intelligence (UAI94), pp. 484490 San Francisco, CA. Morgan
Kaufmann Publishers.
187

fiCheng & Druzdzel

Ritter, H., Martinetz, T., & Schulten, K. (1991). Neuronale Netze. Addison-Wesley,
Munchen.
Rubinstein, R. Y. (1981). Simulation and the Monte Carlo Method. John Wiley & Sons.
Seroussi, B., & Golmard, J. L. (1994). An algorithm directly finding the K most probable
configurations in Bayesian networks. International Journal of Approximate Reasoning,
11, 205233.
Shachter, R. D., & Peot, M. A. (1989). Simulation approaches to general probabilistic
inference on belief networks. In Uncertainty in Artificial Intelligence 5, pp. 221231
New York, N. Y. Elsevier Science Publishing Company, Inc.
Shwe, M. A., & Cooper, G. F. (1991). An empirical analysis of likelihood-weighting simulation on a large, multiply-connected medical belief network. Computers and Biomedical
Research, 24 (5), 453475.
Shwe, M., Middleton, B., Heckerman, D., Henrion, M., Horvitz, E., & Lehmann, H. (1991).
Probabilistic diagnosis using a reformulation of the INTERNIST1/QMR knowledge
base: I. The probabilistic model and inference algorithms. Methods of Information in
Medicine, 30 (4), 241255.
Srinivas, S. (1993). A generalization of the noisy-OR model. In Proceedings of the Ninth
Annual Conference on Uncertainty in Artificial Intelligence (UAI93), pp. 208215
San Francisco, CA. Morgan Kaufmann Publishers.
York, J. (1992). Use of the Gibbs sampler in expert systems. Artificial Intelligence, 56,
115130.

188

fiJournal of Artificial Intelligence Research 13 (2000) 33{94

Submitted 9/99; published 8/00

Value-Function Approximations for Partially Observable
Markov Decision Processes

Milos Hauskrecht

milos@cs.brown.edu

Computer Science Department, Brown University
Box 1910, Brown University, Providence, RI 02912, USA

Abstract

Partially observable Markov decision processes (POMDPs) provide an elegant mathematical framework for modeling complex decision and planning problems in stochastic
domains in which states of the system are observable only indirectly, via a set of imperfect
or noisy observations. The modeling advantage of POMDPs, however, comes at a price |
exact methods for solving them are computationally very expensive and thus applicable
in practice only to very simple problems. We focus on ecient approximation (heuristic)
methods that attempt to alleviate the computational problem and trade off accuracy for
speed. We have two objectives here. First, we survey various approximation methods,
analyze their properties and relations and provide some new insights into their differences.
Second, we present a number of new approximation methods and novel refinements of existing techniques. The theoretical results are supported by experiments on a problem from
the agent navigation domain.
1. Introduction

Making decisions in dynamic environments requires careful evaluation of the cost and benefits not only of the immediate action but also of choices we may have in the future. This
evaluation becomes harder when the effects of actions are stochastic, so that we must pursue and evaluate many possible outcomes in parallel. Typically, the problem becomes more
complex the further we look into the future. The situation becomes even worse when the
outcomes we can observe are imperfect or unreliable indicators of the underlying process
and special actions are needed to obtain more reliable information. Unfortunately, many
real-world decision problems fall into this category.
Consider, for example, a problem of patient management. The patient comes to the
hospital with an initial set of complaints. Only rarely do these allow the physician (decisionmaker) to diagnose the underlying disease with certainty, so that a number of disease options
generally remain open after the initial evaluation. The physician has multiple choices in
managing the patient. He/she can choose to do nothing (wait and see), order additional tests
and learn more about the patient state and disease, or proceed to a more radical treatment
(e.g. surgery). Making the right decision is not an easy task. The disease the patient suffers
can progress over time and may become worse if the window of opportunity for a particular
effective treatment is missed. On the other hand, selection of the wrong treatment may
make the patient's condition worse, or may prevent applying the correct treatment later.
The result of the treatment is typically non-deterministic and more outcomes are possible.
In addition, both treatment and investigative choices come with different costs. Thus, in
c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiHauskrecht

a course of patient management, the decision-maker must carefully evaluate the costs and
benefits of both current and future choices, as well as their interaction and ordering. Other
decision problems with similar characteristics | complex temporal cost-benefit tradeoffs,
stochasticity, and partial observability of the underlying controlled process | include robot
navigation, target tracking, machine mantainance and replacement, and the like.
Sequential decision problems can be modeled as Markov decision processes (MDPs)
(Bellman, 1957; Howard, 1960; Puterman, 1994; Boutilier, Dean, & Hanks, 1999) and their
extensions. The model of choice for problems similar to patient management is the partially
observable Markov decision process (POMDP) (Drake, 1962; Astrom, 1965; Sondik, 1971;
Lovejoy, 1991b). The POMDP represents two sources of uncertainty: stochasticity of the
underlying controlled process (e.g. disease dynamics in the patient management problem),
and imperfect observability of its states via a set of noisy observations (e.g. symptoms,
findings, results of tests). In addition, it lets us model in a uniform way both control and
information-gathering (investigative) actions, as well as their effects and cost-benefit tradeoffs. Partial observability and the ability to model and reason with information-gathering
actions are the main features that distinguish the POMDP from the widely known fully
observable Markov decision process (Bellman, 1957; Howard, 1960).
Although useful from the modeling perspective, POMDPs have the disadvantage of being hard to solve (Papadimitriou & Tsitsiklis, 1987; Littman, 1996; Mundhenk, Goldsmith,
Lusena, & Allender, 1997; Madani, Hanks, & Condon, 1999), and optimal or -optimal solutions can be obtained in practice only for problems of low complexity. A challenging goal in
this research area is to exploit additional structural properties of the domain and/or suitable
approximations (heuristics) that can be used to obtain good solutions more eciently.
We focus here on heuristic approximation methods, in particular approximations based
on value functions. Important research issues in this area are the design of new and ecient
algorithms, as well as a better understanding of the existing techniques and their relations,
advantages and disadvantages. In this paper we address both of these issues. First, we
survey various value-function approximations, analyze their properties and relations and
provide some insights into their differences. Second, we present a number of new methods
and novel refinements of existing techniques. The theoretical results and findings are also
supported empirically on a problem from the agent navigation domain.
2. Partially Observable Markov Decision Processes

A partially observable Markov decision process (POMDP) describes a stochastic control
process with partially observable (hidden) states. Formally, it corresponds to a tuple
(S; A; ; T; O; R) where S is a set of states, A is a set of actions,  is a set of observations,
T : S  A  S ! [0; 1] is a set of transition probabilities that describe the dynamic behavior
of the modeled environment, O : S  A   ! [0; 1] is a set of observation probabilities that
describe the relationships among observations, states and actions, and R : S  A  S ! IR
denotes a reward model that assigns rewards to state transitions and models payoffs associated with such transitions. In some instances the definition of a POMDP also includes an
a priori probability distribution over the set of initial states S .

34

fiValue-Function Approximations for POMDPs

o0

o t2

o

ot

t1

o t+1

st

a0

a t2

s t+1

at

a t1

r

t

Figure 1: Part of the inuence diagram describing a POMDP model. Rectangles correspond
to decision nodes (actions), circles to random variables (states) and diamonds to
reward nodes. Links represent the dependencies among the components. st ; at ; ot
and rt denote state, action, observation and reward at time t. Note that an action
at time t depends only on past observations and actions, not on states.

2.1 Objective Function
Given a POMDP, the goal is to construct a control policy that maximizes an objective (value)
function. The objective function combines partial (stepwise) rewards over multiple steps
using various kinds of decision models. Typically, the models are cumulative and based on
expectations. Two models are frequently used in practice:

 a finite-horizon model in which we maximize E (PTt=0 rt ), where rt is a reward obtained
at time t.

 an infinite-horizon discounted model in which we maximize E (P1t=0  t rt ), where 0 <
 < 1 is a discount factor.

Note that POMDPs and cumulative decision models provide a rich language for modeling
various control objectives. For example, one can easily model goal-achievement tasks (a
specific goal must be reached) by giving a large reward for a transition to that state and
zero or smaller rewards for other transitions.
In this paper we focus primarily on discounted infinite-horizon model. However, the
results can be easily applied also to the finite-horizon case.

2.2 Information State
In a POMDP the process states are hidden and we cannot observe them while making a
decision about the next action. Thus, our action choices are based only on the information available to us or on quantities derived from that information. This is illustrated in
the inuence diagram in Figure 1, where the action at time t depends only on previous
observations and actions, not on states. Quantities summarizing all information are called
information states. Complete information states represent a trivial case.
35

fiHauskrecht

I t+1

It

st

I t+1

It

o t+1

s t+1

at

at

rt

rt

Figure 2: Inuence diagram for a POMDP with information states and corresponding
information-state MDP. Information states (It and It+1 ) are represented by
double-circled nodes. An action choice (rectangle) depends only on the current
information state.

Definition 1 (Complete information state). The complete information state at time t (denoted ItC ) consists of:




a prior belief b0 on states in S at time 0;
a complete history of actions and observations fo0 ; a0 ; o1 ; a1 ;    ; ot 1 ; at 1 ; ot g starting from time t = 0.

A sequence of information states defines a controlled Markov process that we call an
information-state Markov decision process or information-state MDP. The policy for the
information-state MDP is defined in terms of a control function  : I ! A mapping
information state space to actions. The new information state (It ) is a deterministic function
of the previous state (It 1 ), the last action (at 1 ) and the new observation (ot ):

It =  (It 1 ; ot ; at 1 ):
 : I    A ! I is the update function mapping the information state space, observations
and actions back to the information space.1 It is easy to see that one can always convert
the original POMDP into the information-state MDP by using complete information states.
The relation between the components of the two models and a sketch of a reduction of a
POMDP to an information-state MDP, are shown in Figure 2.
2.3 Bellman Equations for POMDPs
An information-state MDP for the infinite-horizon discounted case is like a fully-observable
MDP and satisfies the standard fixed-point (Bellman) equation:
(

)

X
V  (I ) = max (I; a) +  P (I 0 jI; a)V  (I 0 ) :
a2A
I0

(1)

1. In this paper,  denotes the generic update function. Thus we use the same symbol even if the information
state space is different.
36

fiValue-Function Approximations for POMDPs

P
t
Here, V  (I ) denotes the optimal value function maximizing E ( 1
t=0  rt ) for state I . (I; a)
is the expected one-step reward and equals
X
XX
(I; a) = (s; a)P (sjI ) =
R(s; a; s0 )P (s0 js; a)P (sjI ):
s2S
s2 S s0 2 S

(s; a) denotes an expected one-step reward for state s and action a.
Since the next information state I 0 =  (I; o; a) is a deterministic function of the previous
information state I , action a, and the observation o, the Equation 1 can be rewritten more
compactly by summing over all possible observations :
V  (I ) = max
a2A

(

X
s2S

(s; a)P (sjI ) + 

X
o2

)

P (ojI; a)V  ( (I; o; a)) :

(2)

The optimal policy (control function)  : I ! A selects the value-maximizing action
(

)

X
X
 (I ) = arg max
(s; a)P (sjI ) +  P (ojI; a)V  ( (I; o; a)) :
a2A s2S
o2

(3)

The value and control functions can be also expressed in terms of action-value functions
(Q-functions)
V  (I ) = max Q (I; a)
 (I ) = arg max Q (I; a);
a2A
a2A
X
X

Q (I; a) = (s; a)P (sjI ) +  P (ojI; a)V  ( (I; o; a)):
(4)
s2S
o2
A Q-function corresponds to the expected reward for chosing a fixed action (a) in the first
step and acting optimally afterwards.
2.3.1 Sufficient Statistics

To derive Equations 1|3 we implicitly used complete information states. However, as
remarked earlier, the information available to the decision-maker can be also summarized
by other quantities. We call them sucient information states. Such states must preserve
the necessary information content and also the Markov property of the information-state
decision process.

Definition 2 (Sucient information state process). Let I be an information state space
and  : I  A   ! I be an update function defining an information process It =
 (It 1 ; at 1 ; ot ). The process is sucient with regard to the optimal control when, for any
time step t, it satisfies
P (st jIt ) = P (st jItC )
P (ot jIt 1 ; at 1 ) = P (ot jItC 1 ; at 1 );
where ItC and ItC 1 are complete information states.
It is easy to see that Equations 1 | 3 for complete information states must hold also for
sucient information states. The key benefit of sucient statistics is that they are often
37

fiHauskrecht

easier to manipulate and store, since unlike complete histories, they may not expand with
time. For example, in the standard POMDP model it is sucient to work with belief states
that assign probabilities to every possible process state (Astrom, 1965).2 In this case the
Bellman equation reduces to:
(

V (b) = max
a2A

X
s2S

(s; a)b(s) + 

XX
o2 s2S

)

P (ojs; a)b(s)V ( (b; o; a)) ;

(5)

where the next-step belief state b0 is
X
b0 (s) =  (b; o; a)(s) = fiP (ojs; a)
P (sja; s0 )b(s0 ):
0
s 2S

fi = 1=P (ojb; a) is a normalizing constant. This defines a belief-state MDP which is a
special case of a continuous-state MDP. Belief-state MDPs are also the primary focus of
our investigation in this paper.
2.3.2 Value-Function Mappings and their Properties

The Bellman equation 2 for the belief-state MDP can be also rewritten in the value-function
mapping form. Let V be a space of real-valued bounded functions V : I ! IR defined on
the belief information space I , and let h : I  A  B ! IR be defined as

h(b; a; V ) =

X

s2S

(s; a)b(s) + 

XX

o2 s2S

P (ojs; a)b(s)V ( (b; o; a)):

Now by defining the value function mapping H : V ! V as (HV )(b) = maxa2A h(b; a; V ),
the Bellman equation 2 for all information states can be written as V  = HV  : It is well
known that H (for MDPs) is an isotone mapping and that it is a contraction under the
supremum norm (see (Heyman & Sobel, 1984; Puterman, 1994)).

Definition 3 The mapping H is isotone, if V; U

2 V and V  U implies HV  HU .

Definition 4 Let k:k be a supremum norm. The mapping H is a contraction under the
supremum norm, if for all V; U 2 V , kHV HU k  fi kV U k holds for some 0  fi < 1.
2.4 Value Iteration
The optimal value function (Equation 2) or its approximation can be computed using dynamic programming techniques. The simplest approach is the value iteration (Bellman,
1957) shown in Figure 3. In this case, the optimal value function V  can be determined
in the limit by performing a sequence of value-iteration steps Vi = HVi 1 , where Vi is the
ith approximation of the value function (ith value function).3 The sequence of estimates
2. Models in which belief states are not sucient include POMDPs with observation and action channel
lags (see Hauskrecht (1997)).
3. We note that the same update V = HV 1 can be applied to solve the finite-horizon problem in a
standard way. The difference is that V now stands for the i-steps-to-go value function and V0 represents
the value function (rewards) for end states.
i

i

i

38

fiValue-Function Approximations for POMDPs

Value iteration (P OMDP , )
initialize V for all b 2 I ;
repeat
V0 V;
update V HV 0 for all b 2 I ;
until supb j V (b) V 0 (b) j 
return V;
Figure 3: Value iteration procedure.
converges to the unique fixed-point solution which is the direct consequence of Banach's
theorem for contraction mappings (see, for example, Puterman (1994)).
In practice, we stop the iteration well before it reaches the limit solution. The stopping
criterion we use in our algorithm (Figure 3) examines the maximum difference between value
functions obtained in two consecutive steps | the so-called Bellman error (Puterman, 1994;
Littman, 1996). The algorithm stops when this quantity falls below the threshold . The
accuracy of the approximate solution (ith value function) with regard to V  can be expressed
in terms of the Bellman error .

Theorem 1 Let  = supb jVi (b) Vi 1 (b)j = kVi Vi 1 k be the magnitude of the Bellman
error. Then kVi V  k  1 and kVi 1 V  k  1   hold.
Then, to obtain the approximation of V  with precision  the Bellman error should fall
below (1  ) .
2.4.1 Piecewise Linear and Convex Approximations of the Value Function

The major diculty in applying the value iteration (or dynamic programming) to beliefstate MDPs is that the belief space is infinite and we need to compute an update Vi = HVi 1
for all of it. This poses the following threats: the value function for the ith step may not
be representable by finite means and/or computable in a finite number of steps.
To address this problem Sondik (Sondik, 1971; Smallwood & Sondik, 1973) showed that
one can guarantee the computability of the ith value function as well as its finite description
for a belief-state MDP by considering only piecewise linear and convex representations of
value function estimates (see Figure 4). In particular, Sondik showed that for a piecewise
linear and convex representation of Vi 1 , Vi = HVi 1 is computable and remains piecewise
linear and convex.

Theorem 2 (Piecewise linear and convex functions). Let V0 be an initial value function
that is piecewise linear and convex. Then the ith value function obtained after a finite
number of update steps for a belief-state MDP is also finite, piecewise linear and convex,
and is equal to:
X
Vi (b) = max b(s)ffi (s);
ffi 2 i s2S

where b and ffi are vectors of size jS j and i is a finite set of vectors (linear functions) ffi .
39

fiHauskrecht

Vi (b)

0

1

b(s1 )

Figure 4: A piecewise linear and convex function for a POMDP with two process states
fs1 ; s2g. Note that b(s1) = 1 b(s2 ) holds for any belief state.

The key part of the proof is that we can express the update for the ith value function
in terms of linear functions i 1 defining Vi 1 :
8
<X

Vi (b) = max :
a2A

s2S

(s; a)b(s) + 

X

max

o2 ffi 1 2

i

"
X X
1 s0 2S s2S

#

9
=

P (s0 ; ojs; a)b(s) ffi 1 (s0 ); :

(6)

This leads to a piecewise linear and convex value function Vi that can be represented by
a finite set of linear functions ffi , one linear function for every combination of actions and
j
permutations of ffi 1 vectors of size jj. Let W = (a; fo1 ; ffji 1 1 g; fo2 ; ffji 2 1 g;    fojj ; ffi j1j g)
be such a combination. Then the linear function corresponding to it is defined as
XX
ffW
P (s0 ; ojs; a)ffji 1 (s0 ):
(7)
i (s) = (s; a) + 
o2 s0 2S
o

Theorem 2 is the basis of the dynamic programming algorithm for finding the optimal
solution for the finite-horizon models and the value-iteration algorithm for finding nearoptimal approximations of V  for the discounted, infinite-horizon model. Note, however,
that this result does not imply piecewise linearity of the optimal (fixed-point) solution V  .
2.4.2 Algorithms for Computing Value-Function Updates

The key part of the value-iteration algorithm is the computation of value-function updates
Vi = HVi 1 . Assume an ith value function Vi that is represented by a finite number of linear
segments (ff vectors). The total number of all its possible linear functions is jAjj i 1 jjj (one
for every combination of actions and permutations of ffi 1 vectors of size jj) and they can
be enumerated in O(jAjjS j2 j i 1 jjj ) time. However, the complete set of linear functions
is rarely needed: some of the linear functions are dominated by others and their omission
does not change the resulting piecewise linear and convex function. This is illustrated in
Figure 5.

40

fiValue-Function Approximations for POMDPs

Vi (b)

redundant linear
function
0

1

b(s1 )

Figure 5: Redundant linear function. The function does not dominate in any of the regions
of the belief space and can be excluded.

A linear function that can be eliminated without changing the resulting value function
solution is called redundant. Conversely, a linear function that singlehandedly achieves the
optimal value for at least one point of the belief space is called useful.4
For the sake of computational eciency it is important to make the size of the linear
function set as small as possible (keep only useful linear functions) over value-iteration steps.
There are two main approaches for computing useful linear functions. The first approach is
based on a generate-and-test paradigm and is due to Sondik (1971) and Monahan (1982).
The idea here is to enumerate all possible linear functions first, then test the usefulness
of linear functions in the set and prune all redundant vectors. Recent extensions of the
method interleave the generate and test stages and do early pruning on a set of partially
constructed linear functions (Zhang & Liu, 1997a; Cassandra, Littman, & Zhang, 1997;
Zhang & Lee, 1998).
The second approach builds on Sondik's idea of computing a useful linear function for a
single belief state (Sondik, 1971; Smallwood & Sondik, 1973), which can be done eciently.
The key problem here is to locate all belief points that seed useful linear functions and
different methods address this problem differently. Methods that implement this idea are
Sondik's one- and two-pass algorithms (Sondik, 1971), Cheng's methods (Cheng, 1988), and
the Witness algorithm (Kaelbling, Littman, & Cassandra, 1999; Littman, 1996; Cassandra,
1998).
2.4.3 Limitations and Complexity

The major diculty in solving a belief-state MDP is that the complexity of a piecewise
linear and convex function can grow extremely fast with the number of update steps. More
specifically, the size of a linear function set defining the function can grow exponentially (in
the number of observations) during a single update step. Then, assuming that the initial
value function
is linear, the number of linear functions defining the ith value function is
O(jAjjj 1 ).
i

4. In defining redundant and useful linear functions we assume that there are no linear function duplicates,
i.e. only one copy of the same linear function is kept in the set .
i

41

fiHauskrecht

The potential growth of the size of the linear function set is not the only bad news. As
remarked earlier, a piecewise linear convex value function is usually less complex than the
worst case because many linear functions can be pruned away during updates. However,
it turned out that the task of identifying all useful linear functions is computationally
intractable as well (Littman, 1996). This means that one faces not only the potential
super-exponential growth of the number of useful linear functions, but also ineciencies
related to the identification of such vectors. This is a significant drawback that makes the
exact methods applicable only to relatively simple problems.
The above analysis suggests that solving a POMDP problem is an intrinsically hard
task. Indeed, finding the optimal solution for the finite-horizon problem is PSPACE-hard
(Papadimitriou & Tsitsiklis, 1987). Finding the optimal solution for the discounted infinitehorizon criterion is even harder. The corresponding decision problem has been shown to be
undecidable (Madani et al., 1999), and thus the optimal solution may not be computable.
2.4.4 Structural Refinements of the Basic Algorithm

The standard POMDP model uses a at state space and full transition and reward matrices.
However, in practice, problems often exhibit more structure and can be represented more
compactly, for example, using graphical models (Pearl, 1988; Lauritzen, 1996), most often
dynamic belief networks (Dean & Kanazawa, 1989; Kjaerulff, 1992) or dynamic inuence
diagrams (Howard & Matheson, 1984; Tatman & Schachter, 1990).5 There are many ways
to take advantage of the problem structure to modify and improve exact algorithms. For
example, a refinement of the basic Monahan algorithm to compact transition and reward
models has been studied by Boutilier and Poole (1996). A hybrid framework that combines
MDP-POMDP problem-solving techniques to take advantage of perfectly and partially observable components of the model and the subsequent value function decomposition was
proposed by Hauskrecht (1997, 1998, 2000). A similar approach with perfect information
about a region (subset of states) containing the actual underlying state was discussed by
Zhang and Liu (1997b, 1997a). Finally, Casta~non (1997) and Yost (1998) explore techniques
for solving large POMDPs that consist of a set of smaller, resource-coupled but otherwise
independent POMDPs.

2.5 Extracting Control Strategy
Value iteration allow us to compute an ith approximation of the value function Vi . However,
our ulimate goal is to find the optimal control strategy  : I ! A or its close approximation.
Thus our focus here is on the problem of extraction of control strategies from the results of
value iteration.
2.5.1 Lookahead Design

The simplest way to define the control function  : I ! A from the value function Vi is via
greedy one-step lookahead:
(

(b) = arg max
a2A

X
s2S

(s; a)b(s) + 

X
o2

)

P (ojb; a)Vi ( (b; o; a)) :

5. See the survey by Boutilier, Dean and Hanks (1999) for different ways to represent structured MDPs.
42

fiValue-Function Approximations for POMDPs

Vi (b)
a1
a3

a2
a1

0

b

1

b(s1 )

Figure 6: Direct control design. Every linear function defining Vi is associated with an
action. The action is selected if its linear function (or Q-function) is maximal.
As Vi represents only the ith approximation of the optimal value function, the question
arises how good the resulting controller really is.6 The following theorem (Puterman, 1994;
Williams & Baird, 1994; Littman, 1996) relates the accuracy of the (lookahead) controller
and the Bellman error.

Theorem 3 Let  = kVi Vi 1 k be the magnitude of the Bellman error. Let ViLA be the
expected reward for the lookahead controller designed for Vi . Then kViLA V  k  12 .
The bound can be used to construct the value-iteration routine that yields a lookahead
strategy with a minimum required precision. The result can be also extended to the kstep lookahead design in a straightforward way; with k steps, the error bound becomes
kViLA(k) V  k  (12) .
k

2.5.2 Direct Design

To extract the control action via lookahead essentially requires computing one full update.
Obviously, this can lead to unwanted delays in reaction times. In general, we can speed up
the response by remembering and using additional information. In particular, every linear
function defining Vi is associated with the choice of action (see Equation 7). The action is a
byproduct of methods for computing linear functions and no extra computation is required
to find it. Then the action corresponding to the best linear function can be selected directly
for any belief state. The idea is illustrated in Figure 6.
The bound on the accuracy of the direct controller for the infinite-horizon case can be
once again derived in terms of the magnitude of the Bellman error.

Theorem 4 Let  = kVi Vi 1 k be the magnitude of the Bellman error. Let ViDR be an
expected reward for the direct controller designed for Vi . Then kViDR V  k  12 .
The direct action choice is closely related to the notion of action-value function (or
Q-function). Analogously to Equation 4, the ith Q-function satisfies
Vi (b) = max Qi (b; a);
a2A

6. Note that the control action extracted via lookahead from V is optimal for (i + 1) steps-to-go and the
finite-horizon model. The main difference here is that V is the optimal value function for i steps to go.
i

i

43

fiHauskrecht

a1

o2
o1

a2

o1 , o

2

o2

o1 , o

a2

o1 , o2

2

o2
a2

a1

o1

o1

a2

o1 , o2
a1

Figure 7: A policy graph (finite-state machine) obtained after two value iteration steps.
Nodes correspond to linear functions (or states of the finite-state machine) and
links to dependencies between linear functions (transitions between states). Every
linear function (node) is associated with an action. To ensure that the policy can
be also applied to the infinite-horizon problem, we add a cycle to the last state
(dashed line).

Qi (b; a) = R(b; a) + 

X
o2

P (ojb; a)Vi 1 ( (b; a; o)):

From this perspective, the direct strategy selects the action with the best (maximum) Qfunction for a given belief state.7
2.5.3 Finite-State Machine Design

A more complex refinement of the above technique is to remember, for every linear function
in Vi , not only the action choice but also the choice of a linear function for the previous
step and to do this for all observations (see Equation 7). As the same idea can be applied
recursively to the linear functions for all previous steps, we can obtain a relatively complex
dependency structure relating linear functions in Vi ; Vi 1 ;    V0 , observations and actions
that itself represents a control strategy (Kaelbling et al., 1999).
To see this, we model the structure in graphical terms (Figure 7). Here different nodes
represent linear functions, actions associated with nodes correspond to optimizing actions,
links emanating from nodes correspond to different observations, and successor nodes correspond to linear functions paired with observations. Such graphs are also called policy graphs
(Kaelbling et al., 1999; Littman, 1996; Cassandra, 1998). One interpretation of the dependency structure is that it represents a collection of finite-state machines (FSMs) with many
possible initial states that implement a POMDP controller: nodes correspond to states of
the controller, actions to controls (outputs), and links to transitions conditioned on inputs
7. Williams and Baird (1994) also give results relating the accuracy of the direct Q-function controller to
the Bellman error of Q-functions.

44

fiValue-Function Approximations for POMDPs

(observations). The start state of the FSM controller is chosen greedily by selecting the
linear function (controller state) optimizing the value of an initial belief state.
The advantage of the finite-state machine representation of the strategy is that for the
first i steps it works with observations directly; belief-state updates are not needed. This
contrasts with the other two policy models (lookahead and direct models), which must keep
track of the current belief state and update it over time in order to extract appropriate
control. The drawback of the approach is that the FSM controller is limited to i steps
that correspond to the number of value iteration steps performed. However, in the infinitehorizon model the controller is expected to run for an infinite number of steps. One way
to remedy this deficiency is to extend the FSM structure and to create cycles that let us
visit controller states repeatedly. For example, adding a cycle transition to the end state of
the FSM controller in Figure 7 (dashed line) ensures that the controller is also applicable
to the infinite-horizon problem.

2.6 Policy Iteration
An alternative method for finding the solution for the discounted infinite-horizon problem
is policy iteration (Howard, 1960; Sondik, 1978). Policy iteration searches the policy space
and gradually improves the current control policy for one or more belief states. The method
consists of two steps performed iteratively:




policy evaluation: computes expected value for the current policy;
policy improvement: improves the current policy.

As we saw in Section 2.5, there are many ways to represent a control policy for a
POMDP. Here we restrict attention to a finite-state machine model in which observations
correspond to inputs and actions to outputs (Platzman, 1980; Hansen, 1998b; Kaelbling
et al., 1999).8
2.6.1 Finite-State Machine Controller

A finite-state machine (FSM) controller C = (M; ; A; ; ; ) for a POMDP is described
by a set of memory states M of the controller, a set of observations (inputs) , a set of
actions (outputs) A, a transition function  : M   ! M mapping states of the FSM to
next memory states given the observation, and an output function  : M ! A mapping
memory states to actions. A function : I0 ! M selects the initial memory state given
the initial information state. The initial information state corresponds either to a prior or
a posterior belief state at time t0 depending on the availability of an initial observation.
2.6.2 Policy Evaluation

The first step of the policy iteration is policy evaluation. The most important property
of the FSM model is that the value function for a specific FSM strategy can be computed
eciently in the number of controller states M . The key to ecient computability is the
8. A policy-iteration algorithm in which policies are defined over the regions of the belief space was described
first by Sondik (1978).
45

fiHauskrecht

x2
o1
o2

a2

a1

x1

o2
o

o1
a1

o2

o1

x3

2

a2

o1

x4

Figure 8: An example of a four-state FSM policy. Nodes represent states, links transitions between states (conditioned on observations). Every memory state has an
associated control action (output).

fact that the value function for executing an FSM strategy from some memory state x is
linear (Platzman, 1980).9

Theorem 5 Let C be a finite-state machine controller with a set of memory states M .
The value function for applying C from a memory state x 2 M , V C (x; b), is linear. Value
functions for all x 2 M can be found by solving a system of linear equations with jS jjM j
variables.
We illustrate the main idea by an example. Assume an FSM controller with four memory
states fx1 ; x2 ; x3 ; x4 g, as in Figure 8, and a stochastic process with two hidden states S =
fs1 ; s2g. The value of the policy for an augmented state space S  M satisfies a system of
linear equations
V (x1 ; s1 ) = (s1 ; (x1 )) + 
V (x1 ; s2 ) = (s2 ; (x1 )) + 
V (x2 ; s1 ) = (s1 ; (x2 )) + 



V (x4 ; s2 ) = (s2 ; (x4 )) + 

XX

o2 s2S

XX

o2 s2S

XX

o2 s2S

XX
o2 s2S

P (o; sjs1 ; (x1 ))V ((x1 ; o); s)
P (o; sjs2 ; (x1 ))V ((x1 ; o); s)
P (o; sjs1 ; (x2 ))V ((x2 ; o); s)
P (o; sjs2 ; (x4 ))V ((x4 ; o); s);

where (x) is the action executed in x and (x; o) is the state to which one transits after
seeing an input (observation) o. Assuming we start the policy from the memory state x1 ,
the value of the policy is:
X
V C (x1 ; b) = V (x1 ; s)b(s):
s2S
9. The idea of linearity and ecient computability of the value functions for a fixed FSM-based strategy
has been addressed recently in different contexts by a number of researchers (Littman, 1996; Cassandra,
1998; Hauskrecht, 1997; Hansen, 1998b; Kaelbling et al., 1999). However, the origins of the idea can be
traced to the earlier work by Platzman (1980).
46

fiValue-Function Approximations for POMDPs

Thus the value function is linear and can be computed eciently by solving a system of
linear equations.
Since in general the FSM controller can start from any memory state, we can always
choose the initial memory state greedily, maximizing the expected value of the result. In
such a case the optimal choice function is defined as:
(b) = arg max V C (x; b);
x2M
and the value for the FSM policy C and belief state b is:

V C (b) = max V C (x; b) = V C ( (b); b):
x2M

Note that the resulting value function for the strategy C is piecewise linear and convex and
represents expected rewards for following C . Since no strategy can perform better that the
optimal strategy, V C  V  must hold.
2.6.3 Policy Improvement

The policy-iteration method, searching the space of controllers, starts from an arbitrary initial policy and improves it gradually by refining its finite-state machine (FSM) description.
In particular, one keeps modifying the structure of the controller by adding or removing controller states (memory) and transitions. Let C and C 0 be an old and a new FSM controller.
In the improvement step we must satisfy
0

V C (b)  V C (b) for all b 2 I ;

9b 2 I such that V C 0 (b) > V C (b):
To guarantee the improvement, Hansen (1998a, 1998b) proposed a policy-iteration algorithm that relies on exact value function updates to obtain a new improved policy structure.10 The basic idea of the improvement is based on the observation that one can switch
back and forth between the FSM policy description and the piecewise-linear and convex
representation of a value function. In particular:

 the value function for an FSM policy is piecewise-linear and convex and every linear
function describing it corresponds to a memory state of a controller;

 individual linear functions comprising the new value function after an update can be
viewed as new memory states of an FSM policy, as described in Section 2.5.3.

This allows us to improve the policy by adding new memory states corresponding to linear
functions of the new value function obtained after the exact update. The technique can be
refined by removing some of the linear functions (memory states) whenever they are fully
dominated by one of the other linear functions.
10. A policy-iteration algorithm that exploits exact value function updates but works with policies defined
over the belief space was used earlier by Sondik (1978).

47

fiHauskrecht

b 1,1

o1

o2

a1

b

a2

b 1,2

Figure 9: A two-step decision tree. Rectangles correspond to the decision nodes (moves
of the decision-maker) and circles to chance nodes (moves of the environment).
Black rectangles represent leaves of the tree. The reward for a specific path
is associated with every leaf of the tree. Decision nodes are associated with
information states obtained by following action and observation choices along the
path from the root of the tree. For example, b1;1 is a belief state obtained by
performing action a1 from the initial belief state b and observing observation o1 .

2.7 Forward (Decision Tree) Methods
The methods discussed so far assume no prior knowledge of the initial belief state and treat
all belief states as equally likely. However, if the initial state is known and fixed, methods
can often be modified to take advantage of this fact. For example, for the finite-horizon
problem, only a finite number of belief states can be reached from a given initial state. In
this case it is very often easier to enumerate all possible histories (sequences of actions and
observations) and represent the problem using stochastic decision trees (Raiffa, 1970). An
example of a two-step decision tree is shown in Figure 9.
The algorithm for solving the stochastic decision tree basically mimics value-function
updates, but is restricted only to situations that can be reached from the initial belief state.
The key diculty here is that the number of all possible trajectories grows exponentially
with the horizon of interest.
2.7.1 Combining Dynamic-Programming and Decision-Tree Techniques

To solve a POMDP for a fixed initial belief state, we can apply two strategies: one constructs the decision tree first and then solves it, the other solves the problem in a backward
fashion via dynamic programming. Unfortunately, both these techniques are inecient, one
suffering from exponential growth in the decision tree size, the other from super-exponential
growth in the value function complexity. However, the two techniques can be combined in
48

fiValue-Function Approximations for POMDPs

a way that at least partially eliminates their disadvantages. The idea is based on the fact
that the two techniques work on the solution from two different sides (one forward and the
other backward) and the complexity for each of them worsens gradually. Then the solution
is to compute the complete kth value function using dynamic programming (value iteration)
and cover the remaining steps by forward decision-tree expansion.
Various modifications of the above idea are possible. For example, one can often replace
exact dynamic programming with two more ecient approximations providing upper and
lower bounds of the value function. Then the decision tree must be expanded only when
the bounds are not sucient to determine the optimal action choice. A number of search
techniques developed in the AI literature (Korf, 1985) combined with branch-and-bound
pruning (Satia & Lave, 1973) can be applied to this type of problem. Several researchers
have experimented with them to solve POMDPs (Washington, 1996; Hauskrecht, 1997;
Hansen, 1998b). Other methods applicable to this problem are based on Monte-Carlo
sampling (Kearns, Mansour, & Ng, 1999; McAllester & Singh, 1999) and real-time dynamic
programming (Barto, Bradtke, & Singh, 1995; Dearden & Boutilier, 1997; Bonet & Geffner,
1998).
2.7.2 Classical Planning Framework

POMDP problems with fixed initial belief states and their solutions are closely related to
work in classical planning and its extensions to handle stochastic and partially observable
domains, particularly the work on BURIDAN and C-BURIDAN planners (Kushmerick,
Hanks, & Weld, 1995; Draper, Hanks, & Weld, 1994). The objective of these planners is
to maximize the probability of reaching some goal state. However, this task is similar to
the discounted reward task in terms of complexity, since a discounted reward model can
be converted into a goal-achievement model by introducing an absorbing state (Condon,
1992).
3. Heuristic Approximations

The key obstacle to wider application of the POMDP framework is the computational
complexity of POMDP problems. In particular, finding the optimal solution for the finitehorizon case is PSPACE-hard (Papadimitriou & Tsitsiklis, 1987) and the discounted infinitehorizon case may not even be computable (Madani et al., 1999). One approach to such
problems is to approximate the solution to some -precision. Unfortunately, even this
remains intractable and in general POMDPs cannot be approximated eciently (Burago,
Rougemont, & Slissenko, 1996; Lusena, Goldsmith, & Mundhenk, 1998; Madani et al.,
1999). This is also the reason why only very simple problems can be solved optimally or
near-optimally in practice.
To alleviate the complexity problem, research in the POMDP area has focused on various
heuristic methods (or approximations without the error parameter) that are more ecient.11
Heuristic methods are also our focus here. Thus, when referring to approximations, we mean
heuristics, unless specifically stated otherwise.
11. The quality of a heuristic approximation can be tested using the Bellman error, which requires one exact
update step. However, heuristic methods per se do not contain a precision parameter.

49

fiHauskrecht

The many approximation methods and their combinations can be divided into two often
very closely related classes: value-function approximations and policy approximations.

3.1 Value-Function Approximations
The main idea of the value-function approximation approach is to approximate the optimal
value function V : I ! IR with a function Vb : I ! IR defined over the same information
space. Typically, the new function is of lower complexity (recall that the optimal or nearoptimal value function may consist of a large set of linear functions) and is easier to compute
than the exact solution. Approximations can be often formulated as dynamic programming
problems and can be expressed in terms of approximate value-function updates Hb . Thus,
to understand the differences and advantages of various approximations and exact methods,
it is often sucient to analyze and compare their update rules.
3.1.1 Value-Function Bounds

Although heuristic approximations have no guaranteed precision, in many cases we are
able to say whether they overestimate or underestimate the optimal value function. The
information on bounds can be used in multiple ways. For example, upper- and lowerbounds can help in narrowing the range of the optimal value function, elimination of some
of the suboptimal actions and subsequent speed-ups of exact methods. Alternatively, one
can use knowledge of both value-function bounds to determine the accuracy of a controller
generated based on one of the bounds (see Section 3.1.3). Also, in some instances, a lower
bound alone is sucient to guarantee the control choice that always achieves an expected
reward at least as high as the one given by that bound (Section 4.7.2).
The bound property of different methods can be determined by examining the updates
and their bound relations.

Definition 5 (Upper bound). Let H be the exact value-function mapping and Hb its apb )(b)  (HV )(b) holds
proximation. We say that Hb upper-bounds H for some V when (HV
for every b 2 I .
An analogous definition can be constructed for the lower bound.
3.1.2 Convergence of Approximate Value Iteration

Let Hb be a value-function mapping representing an approximate update. Then the approximate value iteration computes the ith value function as Vbi = Hb Vbi 1 . The fixed-point
solution Vc = Hb Vb  or its close approximation would then represent the intended output of
the approximation routine. The main problem with the iteration method is that in general
it can converge to unique or multiple solutions, diverge, or oscillate, depending on Hb and
the initial function Vb0 . Therefore, unique convergence cannot be guaranteed for an arbitrary
mapping Hb and the convergence of a specific approximation method must be proved.

Definition 6 (Convergence of Hb ). The value iteration with Hb converges for a value function V0 when limn!1(Hb n V0 ) exists.

50

fiValue-Function Approximations for POMDPs

Definition 7 (Unique convergence of Hb ). The value iteration converges uniquely for V
when for every V 2 V , limn!1(Hb n V ) exists and for all pairs V; U 2 V , limn!1(Hb n V ) =
limn!1(Hb n U ).
A sucient condition for the unique convergence is to show that Hb be a contraction. The
contraction and the bound properties of Hb can be combined, under additional conditions, to
show the convergence of the iterative approximation method to the bound. To address this
issue we present a theorem comparing fixed-point solutions of two value-function mappings.
Theorem 6 Let H1 and H2 be two value-function mappings defined on V1 and V2 such that
1. H1 , H2 are contractions with fixed points V1 , V2 ;
2. V1 2 V2 and H2 V1  H1 V1 = V1 ;

3. H2 is an isotone mapping.
Then V2  V1 holds.

Note that this theorem does not require that V1 and V2 cover the same space of value
functions. For example, V2 can cover all possible value functions of a belief-state MDP,
while V1 can be restricted to a space of piecewise linear and convex value functions. This
gives us some exibility in the design of iterative approximation algorithms for computing
value-function bounds. An analogous theorem also holds for the lower bound.
3.1.3 Control

Once the approximation of the value-function is available, it can be used to generate a
control strategy. In general, control solutions correspond to options presented in Section
2.5 and include lookahead, direct (Q-function) and finite-state machine designs.
A drawback of control strategies based on heuristic approximations is that they have
no precision guarantee. One way to find the accuracy of such strategies is to do one exact
update of the value function approximation and adopt the result of Theorems 1 and 3 for
the Bellman error. An alternative solution to this problem is to bound the accuracy of
such controllers using the upper- and the lower-bound approximations of the optimal value
function. To illustrate this approach, we present and prove (in the Appendix) the following
theorem that relates the quality of bounds to the quality of a lookahead controller.

Theorem 7 Let VbU and VbL be upper and lower bounds of the optimal value function for
the discounted infinite-horizon problem. Let  = supb jVbU (b) VbL (b)j = kVbU VbL k be
the maximum bound difference. Then the expected reward for a lookahead controller Vb LA ,
constructed for either VbU or VbL , satisfies kVb LA V  k  (1(2 )) .
3.2 Policy Approximation
An alternative to value-function approximation is policy approximation. As shown earlier,
a strategy (controller) for a POMDP can be represented using a finite-state machine (FSM)
model. The policy iteration searches the space of all possible policies (FSMs) for the optimal or near-optimal solution. This space is usually enormous, which is the bottleneck of the
51

fiHauskrecht

method. Thus, instead of searching the complete policy space, we can restrict our attention
only to its subspace that we believe to contain the optimal solution or a good approximation. Memoryless policies (Platzman, 1977; White & Scherer, 1994; Littman, 1994; Singh,
Jaakkola, & Jordan, 1994), policies based on truncated histories (Platzman, 1977; White &
Scherer, 1994; McCallum, 1995), or finite-state controllers with a fixed number of memory
states (Platzman, 1980; Hauskrecht, 1997; Hansen, 1998a, 1998b) are all examples of a
policy-space restriction. In the following we consider only the finite-state machine model
(see Section 2.6.1), which is quite general; other models can be viewed as its special cases.
States of an FSM policy model represent the memory of the controller and, in general,
summarize information about past activities and observations. Thus, they are best viewed
as approximations of the information states, or as feature states. The transition model of
the controller () then approximates the update function of the information-state MDP
( ) and the output function of an FSM () approximates the control function () mapping
information states to actions. The important property of the model, as shown Section
2.6.2, is that the value function for a fixed controller and fixed initial memory state can be
obtained eciently by solving a system of linear equations (Platzman, 1980).
To apply the policy approximation approach we first need to decide (1) how to restrict
a space of policies and (2) how to judge the policy quality.
A restriction frequently used is to consider only controllers with a fixed number of
states, say k. Other structural restrictions further narrowing the space of policies can
restrict either the output function (choice of actions at different controller states), or the
transitions between the current and next states. In general, any heuristic or domain-related
insight may help in selecting the right biases.
Two different policies can yield value functions that are better in different regions of
the belief space. Thus, in order to decide which policy is the best, we need to define the
importance of different regions and their combinations. There are multiple solutions to this.
For example, Platzman (1980) considers the worst-case measure and optimizes the worst
(minimal) value for all initial belief states. Let C be a space of FSM controllers satisfying
given restrictions. Then the quality of a policy under the worst case measure is:
max min max V C (x; b):
C 2C b2I x2M
C

Another option is to consider a distribution over all initial belief states and maximize the
expectation of their value function values. However, the most common objective is to choose
the policy that leads to the best value for a single initial belief state b0 :
max max V C (x; b0 ):
C 2C x2M
C

Finding the optimal policy for this case reduces to a combinatorial optimization problem.
Unfortunately, for all but trivial cases, even this problem is computationally intractable.
For example, the problem of finding the optimal policy for a memoryless case (only current observations are considered) is NP-hard (Littman, 1994). Thus, various heuristics are
typically applied to alleviate this diculty (Littman, 1994).

52

fiValue-Function Approximations for POMDPs

Valuefunction
approximations

Gridbased linear
function methods
Section 4.7

Fully observable MDP
approximations
Section 4.1

Fast informed bound
approximations
Section 4.2

Fixed strategy
approximations
Section 4.4

Curvefitting
approximations
Section 4.6

Gridbased value interpolation
extrapolation methods
Section 4.5

Unobservable MDP
approximations
Section 4.3

Figure 10: Value-function approximation methods.
3.2.1 Randomized Policies

By restricting the space of policies we simplify the policy optimization problem. On the
other hand, we simultaneously give up an opportunity to find the best optimal policy, replacing it with the best restricted policy. Up to this point, we have considered only deterministic
policies with a fixed number of internal controller states, that is, policies with deterministic
output and transition functions. However, finding the best deterministic policy is not always the best option: randomized policies, with randomized output and transition functions,
usually lead to the far better performance. The application of randomized (or stochastic)
policies to POMDPs was introduced by Platzman (1980). Essentially, any deterministic
policy can be represented as a randomized policy with a single action and transition, so
that the best randomized policy is no worse than the best deterministic policy. The difference in control performance of two policies shows up most often in cases when the number
of states of the controller is relatively small compared to that in the optimal strategy.
The advantage of stochastic policies is that their space is larger and parameters of
the policy are continuous. Therefore the problem of finding the optimal stochastic policy
becomes a non-linear optimization problem and a variety of optimization methods can be
applied to solve it. An example is the gradient-based approach (see Meuleau et al., 1999).
4. Value-Function Approximation Methods

In this section we discuss in more depth value-function approximation methods. We focus on approximations with belief information space.12 We survey known techniques, but
also include a number of new methods and modifications of existing methods. Figure 10
summarizes the methods covered. We describe the methods by means of update rules they
12. Alternative value-function approximations may work with complete histories of past actions and observations. Approximation methods used by White and Scherer (1994) are an example.

53

fiHauskrecht

15

16

17

18

19

10

11

12

13

14

5

6

7

8

9

0

1

2

3

4

Moves

Sensors

Figure 11: Test example. The maze navigation problem: Maze20.
implement, which simplifies their analysis and theoretical comparison. We focus on the following properties: the complexity of the dynamic-programming (value-iteration) updates;
the complexity of value functions each method uses; the ability of methods to bound the
exact update; the convergence of value iteration with approximate update rules; and the
control performance of related controllers. The results of the theoretical analysis are illustrated empirically on a problem from the agent-navigation domain. In addition, we use the
agent navigation problem to illustrate and give some intuitions on other characteristics of
methods with no theoretical underpinning. Thus, these results should not be generalized
to other problems or used to rank different methods.
Agent-Navigation Problem

Maze20 is a maze-navigation problem with 20 states, six actions and eight observations.
The maze (Figure 11) consists of 20 partially connected rooms (states) in which a robot
operates and collects rewards. The robot can move in four directions (north, south, east
and west) and can check for the presence of walls using its sensors. But, neither \move"
actions nor sensor inputs are perfect, so that the robot can end up moving in unintended
directions. The robot moves in an unintended direction with probability of 0.3 (0.15 for
each of the neighboring directions). A move into the wall keeps the robot in the same
position. Investigative actions help the robot to navigate by activating sensor inputs. Two
such investigative actions allow the robot to check inputs (presence of a wall) in the northsouth and east-west directions. Sensor accuracy in detecting walls is 0.75 for a two-wall
case (e.g. both north and south wall), 0.8 for a one-wall case (north or south) and 0.89 for
a no-wall case, with smaller probabilities for wrong perceptions.
The control objective is to maximize the expected discounted rewards with a discount
factor of 0.9. A small reward is given for every action not leading to bumping into the wall
(4 points for a move and 2 points for an investigative action), and one large reward (150
points) is given for achieving the special target room (indicated by the circle in the figure)
and recognizing it by performing one of the move actions. After doing that and collecting
the reward, the robot is placed at random in a new start position.
Although the Maze20 problem is of only moderate complexity with regard to the size
of state, action and observation spaces, its exact solution is beyond the reach of current
exact methods. The exact methods tried on the problem include the Witness algorithm
(Kaelbling et al., 1999), the incremental pruning algorithm (Cassandra et al., 1997)13 and
13. Many thanks to Anthony Cassandra for running these algorithms.
54

fiValue-Function Approximations for POMDPs

VMDP

*
VMDP
(s 1 )

*
VMDP
(s 2 )

VQMDP

Q *MDP(s 2 ,a 1 )

Q *MDP(s 1 ,a 1)

Q *MDP(s 2 ,a 2)

Q *MDP(s 1 ,a 2)

*
VPOMDP
0

*
VPOMDP
1

b(s1 )

0

(a)

1

b(s1 )

(b)

Figure 12: Approximations based on the fully observable version of a two state POMDP
(with states s1 ; s2 ): (a) the MDP approximation; (b) the QMDP approximation.
Values at extreme points of the belief space are solutions of the fully observable
MDP.
policy iteration with an FSM model (Hansen, 1998b). The main obstacle preventing these
algorithms from obtaining the optimal or close-to-optimal solution was the complexity of
the value function (the number of linear functions needed to describe it) and subsequent
running times and memory problems.

4.1 Approximations with Fully Observable MDP
Perhaps the simplest way to approximate the value function for a POMDP is to assume
that states of the process are fully observable (Astrom, 1965; Lovejoy, 1993). In that case
the optimal value function V  for a POMDP can be approximated as:
Vb (b) =

X

s2S

 (s);
b(s)VMDP

(8)

 (s) is the optimal value function for state s for the fully observable version of
where VMDP
the process. We refer to this approximation as to the MDP approximation. The idea of
the approximation is illustrated in Figure 12a. The resulting value function is linear and
is fully defined by values at extreme points of the belief simplex. These correspond to the
optimal values for the fully observable case. The main advantage of the approximation
is that the fully observable MDP (FOMDP) can be solved eciently for both the finitehorizon problem and discounted infinite-horizon problems.14 The update step for the (fully
observable) MDP is:
8
<

ViMDP
(s; a) + 
+1 (s) = max
a :

X
s0 2S

9
=

P (s0 js; a)ViMDP (s0 ); :

14. The solution for the finite-state fully observable MDP and discounted infinite-horizon criterion can be
found eciently by formulating an equivalent linear programming task (Bertsekas, 1995)

55

fiHauskrecht

4.1.1 MDP Approximation

The MDP-approximation approach (Equation 8) can be also described in terms of valuefunction updates for the belief-space MDP. Although this step is strictly speaking redundant
here, it simplifies the analysis and comparison of this approach to other approximations.
Let Vbi be a linear value function described by a vector ffMDP
corresponding to values
i
of ViMDP (s0 ) for all states s0 2 S . Then the (i + 1)th value function Vbi+1 is

Vbi+1 (b) =

X
s2S

2

b(s) max 4(s; a) + 
a2A

= (HMDP Vbi )(b):

X
s0 2S

3

P (s0 js; a)ffMDP
(s0 )5
i

Vbi+1 is described by a linear function with components
MDP
ffMDP
i+1 (s) = Vi+1 (s) = max
a

(

(s; a) + 

X
s2S

)

P (s0 js; a)ffMDP
(s0 ) :
i

The MDP-based rule HMDP can be also rewritten in a more general form that starts from
an arbitrary piecewise linear and convex value function Vi , represented by a set of linear
functions i :

Vbi+1 (b) =

X
s2S

8
<

b(s) max :(s; a) + 
a2A

X
s0 2S

9
=

P (s0 js; a) max ffi (s0 ); :
ffi 2

i

The application of the HMDP mapping always leads to a linear value function. The
update is easy to compute and takes O(jAjjS j2 + j i jjS j) time. This reduces to O(jAjjS j2 )
time when only MDP-based updates are strung together. As remarked earlier, the optimal
solution for the infinite-horizon, discounted problem can be solved eciently via linear
programming.
The update for the MDP approximation upper-bounds the exact update, that is, H Vbi 
HMDP Vbi . We show this property later in Theorem 9, which covers more cases. The intuition
is that we cannot get a better solution with less information, and thus the fully observable
MDP must upper-bound the partially observable case.
4.1.2 Approximation with Q-Functions (QMDP)

A variant of the approximation based on the fully observable MDP uses Q-functions (Littman,
Cassandra, & Kaelbling, 1995):
X
Vb (b) = max b(s)QMDP (s; a);
a2A s2S
where
X
 (s0 )
QMDP (s; a) = (s; a) + 
P (s0 js; a)VMDP
s0 2S
is the optimal action-value function (Q-function) for the fully observable MDP. The QMDP
approximation Vb is piecewise linear and convex with jAj linear functions, each corresponding
56

fiValue-Function Approximations for POMDPs

to one action (Figure 12b). The QMDP update rule (for the belief state MDP) for Vbi with
linear functions ffki 2 i is:

Vbi+1 (b) = max

X

a2A s2S

2

b(s) 4(s; a) + 

= (HQMDP Vbi )(b):

X
s0 2S

3

P (s0 js; a) max ffi (s0 )5
ffi 2

i

HQMDP generates a value function with jAj linear functions. The time complexity of the
update is the same as for the MDP-approximation case { O(jAjjS j2 + j i jjS j), which reduces
to O(jAjjS j2 ) time when only QMDP updates are used. HQMDP is a contraction mapping
and its fixed-point solution can be found by solving the corresponding fully observable MDP.
The QMDP update upper-bounds the exact update. The bound is tighter than the
MDP update; that is, H Vbi  HQMDP Vbi  HMDP Vbi , as we prove later in Theorem 9. The
same inequalities hold for both fixed-point solutions (through Theorem 6).
To illustrate the difference in the quality of bounds for the MDP approximation and
the QMDP method, we use our Maze20 navigation problem. To measure the quality of a
bound we use the mean of value-function values. Since all belief states are equally important
we assume that they are uniformly distributed. We approximate this measure using the
average of values for a fixed set of N = 2000 belief points. The points in the set were
selected uniformly at random at the beginning. Once the set was chosen, it was fixed
and remained the same for all tests (here and later). Figure 13 shows the results of the
experiment; we include also results for the fast informed bound method that is presented in
the next section.15 Figure 13 also shows the running times of the methods. The methods
were implemented in Common Lisp and run on Sun Ultra 1 workstation.
4.1.3 Control

The MDP and the QMDP value-function approximations can be used to construct controllers based on one-step lookahead. In addition, the QMDP approximation is also suitable
for the direct control strategy, which selects an action corresponding to the best (highest
value) Q-function. Thus, the method is a special case of the Q-function approach discussed
in Section 3.1.3.16 The advantage of the direct QMDP method is that it is faster than both
lookahead designs. On other the hand, lookahead tends to improve the control performance.
This is shown in Figure 14, which compares the control performance of different controllers
on the Maze20 problem.
The quality of a policy b , with no preference towards a particular initial belief state, can
be measured by the mean of value-function values for b and uniformly distributed initial
belief states. We approximate this measure using the average of discounted rewards for

15. The confidence interval limits for probability level 0.95 range in (0:45; 0:62) from their respective
average scores and this holds for all bound experiments in the paper. As these are relatively small we
do not include them in our graphs.
16. As pointed out by Littman et al. (1995), in some instances, the direct QMDP controller never selects
investigative actions, that is, actions that try to gain more information about the underlying process
state. Note, however, that this observation is not true in general and the QMDP-based controller with
direct action selection may select investigative actions, even though in the fully observable version of the
problem investigative actions are never chosen.
57

fiHauskrecht

bound quality
MDP
approximation

QMDP
approximation

running times

fast informed
bound

60

140

50
time [sec]

score

120
100

40
30
20

80
10

60

0
MDP
approximation

40

QMDP
approximation

fast informed
bound

Figure 13: Comparison of the MDP, QMDP and fast informed bound approximations:
bound quality (left); running times (right). The bound-quality score is the
average value of the approximation for the set of 2000 belief points (chosen uniformly at random). As the methods upper-bound the optimal value function, we
ip the bound-quality graph so that longer bars indicate better approximations.
2000 control trajectories obtained for the fixed set of N = 2000 initial belief states (selected
uniformly at random at the beginning). The trajectories were obtained through simulation
and were 60 steps long.17
To validate the comparison along the averaged performance scores, we must show that
these scores are not the result of randomness and that methods are indeed statistically
significantly different. To do this we rely on pairwise significance tests.18 To summarize the
obtained results, the score differences of 1.54, 2.09 and 2.86 between any two methods (here
and also later in the paper) are sucient to reject the method with a lower score being
the better performer at significance levels 0.05, 0.01 and 0.001 respectively.19 Error-bars in
Figure 14 reect the critical score difference for the significance level 0.05.
Figure 14 also shows the average reaction times for different controllers during these
experiments. The results show the clear dominance of the direct QMDP controller, which
need not do a lookahead in order to extract an action, compared to the other two MDPbased controllers.

4.2 Fast Informed Bound Method
Both the MDP and the QMDP approaches ignore partial observability and use the fully
observable MDP as a surrogate. To improve these approximations and account (at least to
17. The length of the trajectories (60 steps) for the Maze20 problem was chosen to ensure that our estimates
of (discounted) cumulative rewards are not far from the actual rewards for an infinite number of steps.
18. An alternative way to compare two methods is to compute confidence limits for their scores and inspect
their overlaps. However, in this case, the ability to distinguish two methods can be reduced due to
uctuations of scores for different initializations. For Maze20, confidence interval limits for probability
level 0.95 range in (1:8; 2:3) from their respective average scores. This covers all control experiments
here and later. Pairwise tests eliminate the dependency by examining the differences of individual values
and thus improve the discriminative power.
19. The critical score differences listed cover the worst case combination. Thus, there may be some pairs for
which the smaller difference would suce.
58

fiValue-Function Approximations for POMDPs

reaction times

control performance
0.025

70

lookahead
0.02

lookahead

lookahead

time [sec]

score

60
50

direct
40

lookahead

direct

0.015
0.01

30

0.005

20

0
MDP
approximation

direct

fast informed
bound

QMDP
approximation

lookahead

lookahead

MDP
approximation

direct

QMDP
approximation

fast informed
bound

Figure 14: Comparison of control performance of the MDP, QMDP and fast informed bound
methods: quality of control (left); reaction times (right). The quality-of-control
score is the average of discounted rewards for 2000 control trajectories obtained
for the fixed set of 2000 initial belief states (selected uniformly at random).
Error-bars show the critical score difference value (1.54) at which any two methods become statistically different at significance level 0.05.
some degree) for partial observability we propose a new method { the fast informed bound
method. Let Vbi be a piecewise linear and convex value function represented by a set of linear
functions i . The new update is defined as
8
<X

XX

9
=

X

Vbi+1 (b) = max : (s; a)b(s) + 
max
P (s0 ; ojs; a)b(s)ffi (s0 );
a2A s2S
ff
2
0
o2 s2S
s 2S
8
9
i

2

<X

i

X

X

= max : b(s) 4(s; a) + 
max
a2A s2S
o2 ff 2 s0 2S
= (HF IB Vbi )(b):
i

i

3
=
P (s0 ; ojs; a)ffi (s0 )5;

The fast informed bound update can be obtained from the exact update by the following
derivation:
8
<X

X

XX

9
=

X

9
=

(H Vbi )(b) = max : (s; a)b(s) + 
max
P (s0 ; ojs; a)b(s)ffi (s0 );
a2A s2S
ff
2
o2
s0 2S s2S
i

8
<X

 max
(s; a)b(s) + 
a2A :
s2S

X

2

XX

i

max

o2 s2S ffi 2 i s0 2S

X

X

P (s0 ; ojs; a)b(s)ffi (s0 );
3

= max b(s) 4(s; a) + 
max
P (s0 ; ojs; a)ffi (s0 )5
a2A s2S
ff
2
o2
s0 2S
i

X

i

= max b(s)ffai+1 (s)
a2A s2S
= (HF IB Vbi )(b):

The value function Vbi+1 = HF IB Vbi one obtains after an update is piecewise linear and
convex and consists of at most jAj different linear functions, each corresponding to one
59

fiHauskrecht

action

ffai+1 (s) = (s; a) + 

X

X
max
P (s0 ; ojs; a)ffi (s0 ):
ff
2
o2
s0 2S
i

i

The HF IB update is ecient and can be computed in O(jAjjS j2 jjj i j) time. As the method
always outputs jAj linear functions, the computation can be done in O(jAj2 jS j2 jj) time,
when many HF IB updates are strung together. This is a significant complexity reduction
compared to the exact approach: the latter can lead to a function consisting of jAjj i jjj
linear functions, which is exponential in the number of observations and in the worst case
takes O(jAjjS j2 j i jjj) time.
As HF IB updates are of polynomial complexity one can find the approximation for the
finite-horizon case eciently. The open issue remains the problem of finding the solution
for the infinite-horizon discounted case and its complexity. To address it we establish the
following theorem.

Theorem 8 A solution for the fast informed bound approximation can be found by solving
an MDP with jS jjAjjj states, jAj actions and the same discount factor  .
The full proof of the theorem is deferred to the Appendix. The key part of the proof
is the construction of an equivalent MDP with jS jjAjjj states representing HF IB updates.
Since a finite-state MDP can be solved through linear program conversion, the fixed-point
solution for the fast informed bound update is computable eciently.
4.2.1 Fast Informed Bound versus Fully-Observable MDP Approximations

The fast informed update upper-bounds the exact update and is tighter than both the MDP
and the QMDP approximation updates.

Theorem 9 Let Vbi corresponds to a piecewise linear convex value function defined by i
linear functions. Then H Vbi  HF IB Vbi  HQMDP Vbi  HMDP Vbi :
The key trick in deriving the above result is to swap max and sum operators (the
proof is in the Appendix) and thus obtain both to the upper-bound inequalities and the
subsequent reduction in the complexity of update rules compared to the exact update.
This is also shown in Figure 15. The UMDP approximation, also included in Figure 15,
is discussed later in Section 4.3. Thus, the difference among the methods boils down to
simple mathematical manipulations. Note that the same inequality relations as derived for
updates hold also for their fixed-point solutions (through Theorem 6).
Figure 13a illustrates the improvement of the bound over MDP-based approximations
on the Maze20 problem. Note, however, that this improvement is paid for by the increased
running-time complexity (Figure 13b).
4.2.2 Control

The fast informed bound always outputs a piecewise linear and convex function, with one
linear function per action. This allows us to build a POMDP controller that selects an action
associated with the best (highest value) linear function directly. Figure 14 compares the
control performance of the direct and the lookahead controllers to the MDP and the QMDP
controllers. We see that the fast informed bound leads not only to tighter bounds but also
60

fiValue-Function Approximations for POMDPs

UMDP update:


V i + 1 ( b ) = m ax   b ( s )  ( s , a ) +  m ax 
aA
 i  i s S

 s S



exact update:


V i + 1 ( b ) = m ax   b ( s )  ( s , a ) + 
aA
 s S

 P ( s ' | s , a )b ( s )

s ' S

 m ax  

o   i  i s  S s ' S



fast informed bound update:



V i + 1 ( b ) = m ax   b ( s )   ( s , a ) + 
aA

 s S

 m ax 




P ( s ' | s , a ) m ax  i ( s ' )  
 i  i






MDP approx. update:


V i + 1 ( b ) =  b ( s ) m ax   ( s , a ) + 
aA
s S



P ( s ' , o | s , a ) i ( s ' )  


 i  i s S
'

s ' S


( s' ) 



P ( s ' , o | s , a )b ( s ) i ( s ' ) 


o 

QMDP approx. update:


V i + 1 ( b ) = m ax   b ( s )   ( s , a ) + 
a A
s
S




i



s 'S


P ( s ' | s , a ) m ax  i ( s ' ) 
 i  i


Figure 15: Relations between the exact update and the UMDP, the fast informed bound,
the QMDP and the MDP updates.
to improved control on average. However, we stress that currently there is no theoretical
underpinning for this observation and thus it may not be true for all belief states and any
problem.
4.2.3 Extensions of the Fast Informed Bound Method

The main idea of the fast informed bound method is to select the best linear function for
every observation and every current state separately. This differs from the exact update
where we seek a linear function that gives the best result for every observation and the
combination of all states. However, we observe that there is a great deal of middle ground
between these two extremes. Indeed, one can design an update rule that chooses optimal
(maximal) linear functions for disjoint sets of states separately. To illustrate this idea,
assume a partitioning S = fS1 ; S2 ;    ; Sm g of the state space S . The new update for S is:

Vbi+1 (b) = max
a2A

(

X
s2S

(s; a)b(s) + 

X

2
4 max

X X

o2 ffi 2 i s2S1 s0 2S

P (s0 ; ojs; a)b(s)ffi (s0 )+

X X
max
P (s0 ; ojs; a)b(s)ffi (s0 ) +    +
ff 2 s2S s0 2S
2
i

i

max

X X

ffi 2 i s2S s0 2S
m

39
=
P (s0 ; ojs; a)b(s)ffi (s0 )5;

It is easy to see that the update upper-bounds the exact update. Exploration of this
approach and various partitioning heuristics remains an interesting open research issue.
61

fiHauskrecht

4.3 Approximation with Unobservable MDP
The MDP-approximation assumes full observability of POMDP states to obtain simpler
and more ecient updates. The other extreme is to discard all observations available to
the decision maker. An MDP with no observations is called unobservable MDP (UMDP)
and one may choose its value-function solution as an alternative approximation.
To find the solution for the unobservable MDP, we derive the corresponding update
rule, HUMDP , similarly to the update for the partially observable case. HUMDP preserves
piecewise linearity and convexity of the value function and is a contraction. The update
equals:
8
<X

9
=

XX

Vbi+1 (b) = max : (s; a)b(s) +  max
P (s0 js; a)b(s)ffi (s0 );
a2A s2S
ff 2 s2S s0 2S
= (HUMDP Vbi )(b);
i

i

where i is a set of linear functions describing Vbi . Vbi+1 remains piecewise linear and convex
and it consists of at most j i jjAj linear functions. This is in contrast to the exact update,
where the number of possible vectors in the next step can grow exponentially in the number
of observations and leads to jAjj i jjj possible vectors. The time complexity of the update is
O(jAjjS j2 j i j). Thus, starting from Vb0 with one linear function, the running-time complexity
for k updates is bounded by O(jAjk jS j2 ). The problem of finding the optimal solution for the
unobservable MDP remains intractable: the finite-horizon case is NP-hard(Burago et al.,
1996), and the discounted infinite-horizon case is undecidable (Madani et al., 1999). Thus,
it is usually not very useful approximation.
The update HUMDP lower-bounds the exact update, an intuitive result reecting the
fact that one cannot do better with less information. To provide some insight into how
the two updates are related, we do the following derivation, which also proves the bound
property in an elegant way:
8
<X

X

9
=

XX

(H Vbi )(b) = max : (s; a)b(s) + 
max
P (s0 ; ojs; a)b(s)ffi (s0 );
a2A s2S
ff
2
o2
s0 2S s2S
8
9
i

<X

(s; a)b(s) +  max
 max
a2A :
ff2
s2S

8
<X

i

i

i

XXX

o2 s2S s0 2S

XX

=

P (s0 ; ojs; a)b(s)ffi (s0 );
9
=

= max : (s; a)b(s) +  max
P (s0 js; a)b(s)ffi (s0 );
a2A s2S
ff 2 s2S s0 2S
= (HUMDP Vbi )(b):
i

i

We see that the difference between the exact and UMDP updates is that the max and
the sum over next-step observations are exchanged. This causes the choice of ff vectors in
HUMDP to become independent of the observations. Once the sum and max operations are
exchanged, the observations can be marginalized out. Recall that the idea of swaps leads
to a number of approximation updates; see Figure 15 for their summary.
62

fiValue-Function Approximations for POMDPs

4.4 Fixed-Strategy Approximations
A finite-state machine (FSM) model is used primarily to define a control strategy. Such a
strategy does not require belief state updates since it directly maps sequences of observations
to sequences of actions. The value function of an FSM strategy is piecewise linear and convex
and can be found eciently in the number of memory states (Section 2.6.1). While in the
policy iteration and policy approximation contexts the value function for a specific strategy
is used to quantify the goodness of the policy in the first place, the value function alone can
be also used as a substitute for the optimal value function. In this case, the value function
(defined over the belief space) equals
V C (b) = max V C (x; b);
x2M

P

where V C (x; b) = s2S V C (x; s)b(s) is obtained by solving a set of jS jjM j linear equations
(Section 2.6.2). As remarked earlier, the value for the fixed strategy lower-bounds the
optimal value function, that is V C  V  .
To simplify the comparison of the fixed-strategy approximation to other approximations,
we can rewrite its solution also in terms of fixed-strategy updates
8
<X

9
=

XXX

Vbi+1 (b) = max : (s; (x))b(s) + 
P (o; s0 js; (x))b(s)ffi ((x; o); s0 ); ;
x2M s2S
o2 s2S s0 2S
8
9
<X

2

XX

3

=

= max : b(s) 4(s; (x)) + 
P (o; s0 js; (x))ffi ((x; o); s0 )5;
x2M s2S
o2 s0 2S
= (HF SM Vbi )(b):

The value function Vbi is piecewise linear and convex and consists of jM j linear functions
ffi (x; :). For the infinite-horizon discounted case ffi (x; s) represents the ith approximation of
V C (x; s). Note that the update can be applied to the finite-horizon case in a straightforward
way.
4.4.1 Quality of Control

Assume we have an FSM strategy and would like to use it as a substitute for the optimal
control policy. There are three different ways in which we can use it to extract the control.
The first is to simply execute the strategy represented by the FSM. There is no need
to update belief states in this case. The second possibility is to choose linear functions
corresponding to different memory states and their associated actions repeatedly in every
step. We refer to such a controller as a direct (DR) controller. This approach requires
updating of belief states in every step. On the other hand its control performance is no
worse than that of the FSM control. The final strategy discards all the information about
actions and extracts the policy by using the value function Vb (b) and one-step lookahead.
This method (LA) requires both belief state updates and lookaheads and leads to the worst
reactive time. Like DR, however, this strategy is guaranteed to be no worse than the FSM
controller. The following theorem relates the performances of the three controllers.
63

fiHauskrecht

control performance

reaction times

70

0.025
60

50

time [sec]

score

0.02

40

0.015
0.01

30

0.005

20

0

0.0001
DR controller

FSM controller

FSM controller

LA controller

DR controller

LA controller

Figure 16: Comparison of three different controllers (FSM, DR and LA) for the Maze20
problem and a collection of one-action policies: control quality (left) and response time (right). Error-bars in the control performance graph indicate the
critical score difference at which any two methods become statistically different
at significance level 0.05.

Theorem 10 Let CF SM be an FSM controller. Let CDR and CLA be the direct and the
one-step-lookahead controllers constructed based on CF SM . Then V C (b)  V C (b) and
V C (b)  V C (b) hold for all belief states b 2 I .
Though we can prove that both the direct controller and the lookahead controller are
always better than the underlying FSM controller (see Appendix for the full proof of the
theorem), we cannot show the similar property between the first two controllers for all initial
belief states. However, the lookahead approach typically tends to dominate, reecting the
usual trade-off between control quality and response time. We illustrate this trade-off on
our running Maze20 example and a collection of jAj one-action policies, each generating a
sequence of the same action. Control quality and response time results are shown in Figure
16. We see that the controller based on the FSM is the fastest of the three, but it is also the
worst in terms of control quality. On the other hand, the direct controller is slower (it needs
to update belief states in every step) but delivers better control. Finally, the lookahead
controller is the slowest and has the best control performance.
F SM

F SM

DR

LA

4.4.2 Selecting the FSM Model

The quality of a fixed-strategy approximation depends strongly on the FSM model used.
The model can be provided a priori or constructed automatically. Techniques for automatic
construction of FSM policies correspond to a search problem in which either the complete or
a restricted space of policies is examined to find the optimal or the near-optimal policy for
such a space. The search process is equivalent to policy approximations or policy-iteration
techniques discussed earlier in Sections 2.6 and 3.2.

64

fiValue-Function Approximations for POMDPs

4.5 Grid-Based Approximations with Value Interpolation-Extrapolation
A value function over a continuous belief space can be approximated by a finite set of grid
points G and an interpolation-extrapolation rule that estimates the value of an arbitrary
point of the belief space by relying only on the points of the grid and their associated values.
Definition 8 (Interpolation-extrapolation rule) Let f : I ! IR be a real-valued function
defined over the information space I , G = fbG1 ; bG2 ;    bGk g be a set of grid points and 	G =
f(bG1 ; f (bG1 )); (bG2 ; f (bG2 ));    ; (bGk ; f (bGk))g be a set of point-value pairs. A function RG :
I  (I  IR)jGj ! IR that estimates f at any point of the information space I using only
values associated with grid points is called an interpolation-extrapolation rule.
The main advantage of an interpolation-extrapolation model in estimating the true value
function is that it requires us to compute value updates only for a finite set of grid points
G. Let Vbi be the approximation of the ith value function. Then the approximation for the
(i + 1)th value function Vbi+1 can be obtained as
Vbi+1 (b) = RG (b; 	Gi+1 );
where values associated with every grid point bGj 2 G (and included in 	Gi+1 ) are:

'i+1 (bGj ) = (H Vbi )(bGj ) = max
a2A

(

(b; a) + 

X
o 2

P (ojb; a)Vbi ( (bGj ; o; a))

)

:

(9)

The grid-based update can also be described in terms of a value-function mapping HG :
Vbi+1 = HG Vbi . The complexity of such an update is O(jGjjAjjS j2 jjCEval (RG ; jGj)) where
CEval (RG ; jGj) is the computational cost of evaluating the interpolation-extrapolation rule
RG for jGj grid points. We show later (Section 4.5.3), that in some instances, the need to
evaluate the interpolation-extrapolation rule in every step can be eliminated.
4.5.1 A Family of Convex Rules

The number of all possible interpolation-extrapolation rules is enormous. We focus on a
set of convex rules that is a relatively small but very important subset of interpolationextrapolation rules.20

Definition 9 (Convex rule) Let f be some function defined over the space I , G = fbG1 ; bG2 ;    bGk g
be a set of grid points, and 	G = f(bG1 ; f (bG1 )); (bG2 ; f (bG2 ));    ; (bGk ; f (bGk ))g be a set of pointvalue pairs. The rule RG for estimating f using 	G is called convex when for every b 2 I ,
the value fb(b) is:
fb(b) = RG (b; 	G ) =
such that 0  bj  1 for every j = 1;    ; jGj, and

jGj
X
bj f (bj );

j =1

PjGj

b
j =1 j

= 1.

20. We note that convex rules used in our work are a special case of averagers introduced by Gordon (1995).
The difference is minor; the definition of an averager includes a constant (independent of grid points and
their values) that is added to the convex combination.
65

fiHauskrecht

The key property of convex rules is that their corresponding grid-based update HG is a
contraction in the max norm (Gordon, 1995). Thus, the approximate value iteration based
on HG converges to the unique fixed-point solution. In addition, HG based on convex rules
is isotone.
4.5.2 Examples of Convex Rules

The family of convex rules includes approaches that are very commonly used in practice,
like nearest neighbor, kernel regression, linear point interpolations and many others.
Take, for example, the nearest-neighbor approach. The function for a belief point b is
estimated using the value at the grid point closest to it in terms of some distance metric M
defined over the belief space. Then, for any point b, there is exactly one nonzero parameter
bj = 1 such that k b bGj kM k b bGi kM holds for all i = 1; 2;    ; k. All other s are
zero. Assuming the Euclidean distance metric, the nearest-neighbor approach leads to a
piecewise constant approximation, in which regions with equal values correspond to regions
with a common nearest grid point.
The nearest neighbor estimates the function value by taking into an account only one
grid point and its value. Kernel regression expands upon this by using more grid points. It
adds up and weights their contributions (values) according to their distance from the target
point. For example, assuming Gaussian kernels, the weight for a grid point bGj is

bj = fi exp kb

bG
k2M =22 ;
j

P
where fi is a normalizing constant ensuring that jjG=1j bj = 1 and  is a parameter that
attens or narrows weight functions. For the Euclidean metric, the above kernel-regression
rule leads to a smooth approximation of the function.
Linear point interpolations are a subclass of convex rules that in addition to constraints
in Definition 9 satisfy
jGj
X
b = bj bGj :
j =1

That is, a belief point b is a convex combination of grid points and the s are the corresponding coecients. Because the optimal value function for the POMDP is convex, the
new constraint is sucient to prove the upper-bound property of the approximation. In
general, there can be many different linear point-interpolations for a given grid. A challenging problem here is to find the rule with the best approximation. We discuss these issues
in Section 4.5.7.
4.5.3 Conversion to a Grid-Based MDP

Assume that we would like to find the approximation of the value function using our gridbased convex rule and grid-based update (Equation 9). We can view this process also as
a process of finding a sequence of values '1 (bGj ); '2 (bGj );    ; 'i (bGj );    for all grid-points
bGj 2 G. We show that in some instances the sequence of values can be computed without
applying an interpolation-extrapolation rule in every step. In such cases, the problem can
66

fiValue-Function Approximations for POMDPs

be converted into a fully observable MDP with states corresponding to grid-points G.21 We
call this MDP a grid-based MDP.

Theorem 11 Let G be a finite set of grid points and RG be a convex rule such that parameters bj are fixed. Then the values of '(bGj ) for all bGj 2 G can be found by solving a fully
observable MDP with jGj states and the same discount factor  .
Proof For any grid point bGj we can write:
(

'i+1 (bGj ) = max (bGj ; a) + 
a2A

8
<

X
o2

X

P (ojbGj ; a)VbiG ( (bGj ; a; o))

)

39
jGj
=
X
G )5
P (ojbGj ; a) 4 o;a
'
(
b
j;k i k ;
2

= max :(bGj ; a) + 
a2A
o2
k=1
8
"
#9
jGj
<h
=
i
X
X
= max : (bGj ; a) +  'Gi (bGk )
P (ojbGj ; a)o;a
j;k ;
a2A
k=1
o2
P

G G
Now denoting [ o2 P (ojbj ; a)G o;a
j;k ] as P (bk jbj ; a), we can construct a fully observable
MDP problem with states corresponding to grid points G and the same discount factor  .
The update step equals:

8
<

'i+1 (bGj ) = max :(bGj ; a) + 
a2A

jGj
X
k=1

9
=

P (bGk jbGj ; a)'Gi (bGk ); :

P
The prerequisite 0  bj  1 for every j = 1;    ; jGj and jjG=1j bj = 1 guarantees that
P (bGk jbGj ; a) can be interpreted as true probabilities. Thus, one can compute values '(bGj )
by solving the equivalent fully-observable MDP. 2
4.5.4 Solving Grid-Based Approximations

The idea of converting a grid-based approximation into a grid-based MDP is a basis of
our simple but very powerful approximation algorithm. Briey, the key here is to find
the parameters (transition probabilities and rewards) of a new MDP model and then solve
it. This process is relatively easy if the parameters  used to interpolate-extrapolate the
value of a non-grid point are fixed (the assumption of Theorem 11). In such a case, we
can determine parameters of the new MDP eciently in one step, for any grid set G. The
nearest neighbor or the kernel regression are examples of rules with this property. Note that
this leads to polynomial-time algorithms for finding values for all grid points (recall that an
MDP can be solved eciently for both finite and discounted, infinite-horizon criteria).
The problem in solving grid-based approximation arises only when the parameters 
used in the interpolation-extrapolation are not fixed and are subject to the optimization
itself. This happens, for example, when there are multiple ways of interpolating a value
21. We note that a similar result has been also proved independently by Gordon (1995).
67

fiHauskrecht

at some point of the belief space and we would like to find the best interpolation (leading
to the best values) for all grid points in G. In such a case, the corresponding \optimal"
grid-based MDP cannot be found in a single step and iterative approximation, solving a
sequence of grid-based MDPs, is usually needed. The worst-case complexity of this problem
remains an open question.
4.5.5 Constructing Grids

An issue we have not touched on so far is the selection of grids. There are multiple ways to
select grids. We divide them into two classes { regular and non-regular grids.
Regular grids (Lovejoy, 1991a) partition the belief space evenly into equal-size regions.22
The main advantage of regular grids is the simplicity with which we can locate grid points
in the neighborhood of any belief point. The disadvantage of regular grids is that they
are restricted to a specific number of points, and any increase in grid resolution is paid for
in an exponential increase in the grid size. For example, a sequence of regular grids for a
20-dimensional belief space (corresponds to a POMDP with 20 states) consists of 20, 210,
1540, 8855, 42504,    grid points.23 This prevents one from using the method with higher
grid resolutions for problems with larger state spaces.
Non-regular grids are unrestricted and thus provide for more exibility when grid resolution must be increased adaptively. On the other hand, due to irregularities, methods for
locating grid points adjacent to an arbitrary belief point are usually more complex when
compared to regular grids.
4.5.6 Linear Point Interpolation

The fact that the optimal value function V  is convex for a belief-state MDPs can be used
to show that the approximation based on linear point interpolation always upper-bounds
the exact solution (Lovejoy, 1991a, 1993). Neither kernel regression nor nearest neighbor
can guarantee us any bound.

Theorem 12 (Upper bound property of a grid-based point interpolation update). Let Vbi be
a convex value function. Then H Vbi  HG Vbi .
The upper-bound property of HG update for convex value functions follows directly
from Jensen's inequality. The convergence to an upper-bound follows from Theorem 6.
Note that the point-interpolation update imposes an additional constraint on the choice
of grid points. In particular, it is easy to see that any valid grid must also include extreme points of the belief simplex (extreme points correspond to (1; 0; 0;    ); (0; 1; 0;    ),
22. Regular grids used by Lovejoy (1991a) are based on Freudenthal triangulation (Eaves, 1984). Essentially, this is the same idea as used to partition evenly the n-dimensional subspace of IR . In fact, an
ane transform allows us to map isomorphically grid points in the belief space to grid points in the
n-dimensional space (Lovejoy, 1991a).
23. The number of points in the regular grid sequence is given by (Lovejoy, 1991a):
n

+ jS j 1)!
;
jGj = (M
M !(jS j 1)!

where M = 1; 2;    is a grid refinement parameter.

68

fiValue-Function Approximations for POMDPs

etc.). Without extreme points one would be unable to cover the whole belief space via
interpolation. Nearest neighbor and kernel regression impose no restrictions on the grid.
4.5.7 Finding the best interpolation

In a general, there are multiple ways to interpolate a point of a belief space. Our objective
is to find the best interpolation, that is, the one that leads to the tightest upper bound of
the optimal value function.
Let b be a belief point and f(bj ; f (bj ))jbj 2 Gg a set of grid-value pairs. Then the best
interpolation for point b is:
jGj
X
fb(b) = min j f (bj )
 j =1
P
= 1;    ; jGj, jjG=1j j

P
subject to 0  j  1 for all j
= 1, and b = jjG=1j j bGj .
This is a linear optimization problem. Although it can be solved in polynomial time
(using linear programming techniques), the computational cost of doing this is still relatively
large, especially considering the fact that the optimization must be repeated many times.
To alleviate this problem we seek more ecient ways of finding the interpolation, sacrificing
the optimality.
One way to find a (suboptimal) interpolation quickly is to apply regular grids proposed
by Lovejoy (1991a). In this case the value at a belief point is approximated using the
convex combination of grid points closest to it. The approximation leads to piecewise linear
and convex value functions. As all interpolations are fixed here, the problem of finding
the approximation can be converted into an equivalent grid-based MDP and solved as a
finite-state MDP. However, as pointed in the previous section, the regular grids must use a
specific number of grid points and any increase in the resolution of a grid is paid for by an
exponential increase in the grid size. This feature makes the method less attractive when
we have a problem with a large state space and we need to achieve high grid resolution.24
In the present work we focus on non-regular (or arbitrary) grids. We propose an interpolation approach that searches a limited space of interpolations and is guaranteed to run
in time linear in the size of the grid. The idea of the approach is to interpolate a point
b of a belief space of dimension jS j with a set of grid points that consists of an arbitrary
grid point b0 2 G and jS j 1 extreme points of the belief simplex. The coecients of this
interpolation can be found eciently and we search for the best such interpolation. Let
b0 2 G be a grid point defining one such interpolation. Then the value at point b satisfies
0

bb
Vbi (b) = min
0 Vi (b);
b 2G

where Vbib0 is the value of the interpolation for the grid point b0 . Figure 17 illustrates the
resulting approximation. The function is characterized by its \sawtooth" shape, which is
inuenced by the choice of the interpolating set.
To find the best value-function solution or its close approximation we can apply a value
iteration procedure in which we search for the best interpolation after every update step.
24. One solution to this problem may be to use adaptive regular grids in which grid resolution is increased
only in some parts of the belief space. We leave this idea for future work.
69

fiHauskrecht

V(b)

V(b)
*

V(b)

0 b
0

b1

b2

b3

b 4 1 b(s )
1

Figure 17: Value-function approximation based on the linear-time interpolation approach
(a two-dimensional case). Interpolating sets are restricted to a single internal
point of the belief space.
The drawback of this approach is that interpolations may remain unchanged for many
update steps, thus slowing down the solution process. An alternative approach is to solve
a sequence of grid-based MDPs instead. In particular, at every stage we find the best
(minimum value) interpolations for all belief points reachable from grid points in one step, fix
coecients of these interpolations (s), construct a grid-based MDP and solve it (exactly or
approximately). This process is repeated until no further improvement (or no improvement
larger than some threshold) is seen in values at different grid points.
4.5.8 Improving Grids Adaptively

The quality of an approximation (bound) depends strongly on the points used in the grid.
Our objective is to provide a good approximation with the smallest possible set of grid
points. However, this task is impossible to achieve, since it cannot be known in advance
(before solving) what belief points to pick. A way to address this problem is to build grids
incrementally, starting from a small set of grid points and adding others adaptively, but
only in places with a greater chance of improvement. The key part of this approach is a
heuristic for choosing grid points to be added next.
One heuristic method we have developed attempts to maximize improvements in bound
values via stochastic simulations. The method builds on the fact that every interpolation
grid must also include extreme points (otherwise we cannot cover the entire belief space).
As extreme points and their values affect the other grid points, we try to improve their
values in the first place. In general, a value at any grid point b improves more the more
precise values are used for its successor belief points, that is, belief states that correspond
to  (b; a ; o) for a choice of observation o. a is the current optimal action choice for b.
Incorporating such points into the grid then makes a larger improvement in the value at
the initial grid point b more likely. Assuming that our initial point is an extreme point, we
have a heuristic that tends to improve a value for that point. Naturally, one can proceed
further with this selection by incorporating the successor points for the first-level successors
into the grid as well, and so forth.
70

fiValue-Function Approximations for POMDPs

generate new grid points (G; Vb G )
set Gnew = fg
for all extreme points b do
repeat until b 2= G [ Gnew
n
o
P
set a = arg maxa (b; a) +  o2 P (ojb; a)Vb G ( (b; a; o))
select observation o according to P (ojb; a )
update b =  (b; a ; o)
add b into Gnew
return Gnew
Figure 18: Procedure for generating additional grid points based on our bound improvement heuristic.

bound quality
MDP

fast interpolation
QMDP
informed regular grid

interpolation
adaptive grid

interpolation
random grid

140
40 80 120 160 200 240 280 320 360 400

120

score

40
100

80

80

120 160 200

240 280 320 360 400

60

40

Figure 19: Improvement in the upper bound quality for grid-based point-interpolations
based on the adaptive-grid method. The method is compared to randomly
refined grid and the regular grid with 210 points. Other upper-bound approximations (the MDP, QMDP and fast informed bound methods) are included for
comparison.
To capture this idea, we generate new grid points via simulation, starting from one
of the extremes of the belief simplex and continuing until a belief point not currently in
the grid is reached. An algorithm that implements the bound improvement heuristic and
expands the current grid G with a set of jS j new grid points while relying on the current
value-function approximation Vb G is shown in Figure 18.
Figure 19 illustrates the performance (bound quality) of our adaptive grid method on
the Maze20 problem. Here we use the combination of adaptive grids with our linear-time
interpolation approach. The method gradually expands the grid in 40 point increments up to
400 grid points. Figure 19 also shows the performance of the random-grid method in which
71

fiHauskrecht

running times
5000
4500
4000
400

time [sec]

3500
3000

360

2500
320

400

2000
360

280

1500

280 320

240

1000

240

200

500
1.26

1.26

50.02

40 80

120

200

160
120
40 80

160

0
MDP QMDP fast
informed

interpolation
regular grid

interpolation
adaptive grid

interpolation
random grid

Figure 20: Running times of grid-based point-interpolation methods. Methods tested include the adaptive grid, the random grid, and the regular grid with 210 grid
points. Running times for the adaptive-grid are cumulative, reecting the dependencies of higher grid resolutions on the lower-level resolutions. The running
time results for the MDP, QMDP, and fast informed bound approximations are
shown for comparison.
new points of the grid are selected iniformly at random (results for 40 grid point increments
are shown). In addition, the figure gives results for the regular grid interpolation (based
on Lovejoy (1991a)) with 210 belief points and other upper-bound methods: the MDP, the
QMDP and the fast informed bound approximations.
We see a dramatic improvement in the quality of the bound for the adaptive method.
In contrast to this, the uniformly sampled grid (random-grid approach) hardly changes the
bound. There are two reasons for this: (1) uniformly sampled grid points are more likely to
be concentrated in the center of the belief simplex; (2) the transition matrix for the Maze20
problem is relatively sparse, the belief points one obtains from the extreme points in one
step are on the boundary of the simplex. Since grid points in the center of the simplex
are never used to interpolate belief states reachable from extremes in one step they cannot
improve values at extremes and the bound does not change.
One drawback of the adaptive method is its running time (for every grid size we need
to solve a sequence of grid-based MDPs). Figure 20 compares running times of different
methods on the Maze20 problem. As grid-expansion of the adaptive method depends on
the value function obtained for previous steps, we plot its cumulative running times. We
see a relatively large increase in running time, especially for larger grid sizes, reecting
the trade-off between the bound quality and running time. However, we note that the
adaptive-grid method performs quite well in the initial few steps, and with only 80 grid
points outperforms the regular grid (with 210 points) in bound quality.
Finally, we note that other heuristic approaches to constructing adaptive grids for point
interpolation are possible. For example, a different approach that refines the grid by ex-

72

fiValue-Function Approximations for POMDPs

control performance
70

score

60

400

50

200
40 200 400

40
40

200
40

400
200

400

40

30

20

fast
interpolation interpolation
MDP QMDP informed (regular grid) (adaptive grid)

interpolation
(random grid)

nearest-neighbor nearest-neighbor
(adaptive grid)
(random grid)

Figure 21: Control performance of lookahead controllers based on grid-based point interpolation and nearest neighbor methods and varying grid sizes. The results are
compared to the MDP, the QMDP and the fast informed bound controllers.
amining differences in values at current grid points has recently been proposed by Brafman
(1997).
4.5.9 Control

Value functions obtained for different grid-based methods define a variety of controllers. Figure 21 compares the performances of lookahead controllers based on the point-interpolation
and nearest-neighbor methods. We run two versions of both approaches, one with the adaptive grid, the other with the random grid, and we show results obtained for 40, 200 and 400
grid points. In addition, we compare their performances to the interpolation with regular
grids (with 210 grid points), the MDP, the QMDP and the fast informed bound approaches.
Overall, the performance of the interpolation-extrapolation techniques we tested on
the Maze20 problem was a bit disappointing. In particular, better scores were achieved
by the simpler QMDP and fast informed bound methods. We see that, although heuristics
improved the bound quality of approximations, they did not lead to the similar improvement
over the QMDP and the fast informed bound methods in terms of control. This result
shows that a bad bound (in terms of absolute values) does not always imply bad control
performance. The main reason for this is that the control performance is inuenced mostly
by relative rather than absolute value-function values (or, in other words, by the shape
of the function). All interpolation-extrapolation techniques we use (except regular grid
interpolation) approximate the value function with functions that are not piecewise linear
and convex; the interpolations are based on the linear-time interpolation technique with a
sawtooth-shaped function, and the nearest-neighbor leads to a piecewise-constant function.
This does not allow them to match the shape of the optimal function correctly. The other
factor that affects the performance is a large sensitivity of methods to the selection of grid
points, as documented, for example, by the comparison of heuristic and random grids.

73

fiHauskrecht

In the above tests we focused on lookahead controllers only. However, an alternative way
to define a controller for grid-based interpolation-extrapolation methods is to use Q-function
approximations instead of value functions, and either direct or lookahead designs.25 Qfunction approximations can be found by solving the same grid-based MDP, and by keeping
values (functions) for different actions separate at the end.

4.6 Approximations of Value Functions Using Curve Fitting (Least-Squares
Fit)
An alternative way to approximate a function over a continuous space is to use curve-fitting
techniques. This approach relies on a predefined parametric model of the value function
and a set of values associated with a finite set of (grid) belief points G. The approach
is similar to interpolation-extrapolation techniques in that it relies on a set of belief-value
pairs. The difference is that the curve fitting, instead of remembering all belief-value pairs,
tries to summarize them in terms of a given parametric function model. The strategy seeks
the best possible match between model parameters and observed point values. The best
match can be defined using various criteria, most often the least-squares fit criterion, where
the objective is to minimize
Error(f ) =

1X
[y
2 j j

f (bj )]2 :

Here bj and yj correspond to the belief point and its associated value. The index j ranges
over all points of the sample set G.
4.6.1 Combining Dynamic Programming and Least-Squares Fit

The least-squares approximation of a function can be used to construct a dynamic-programming
algorithm with an update step: Vbi+1 = HLSF Vbi . The approach has two steps. First, we
obtain new values for a set of sample points G:

'i+1 (b) = (H Vbi )(b) = max
a2A

(

X
s2S

(s; a)b(s) + 

XX
o2 s2S

)
b
P (ojs; a)b(s)Vi ( (b; a; o)) :

Second, we fit the parameters of the value-function model Vbi+1 using new sample-value pairs
and the square-error cost function. The complexity of the update is O(jGjjAjjS j2 jjCEval (Vbi )+
CFit (Vbi+1 ; jGj)) time, where CEval(Vbi ) is the computational cost of evaluating Vbi and
CFit (Vbi+1 ; jGj) is the cost of fitting parameters of Vbi+1 to jGj belief-value pairs.
The advantage of the approximation based on the least-squares fit is that it requires us
to compute updates only for the finite set of belief states. The drawback of the approach
is that, when combined with the value-iteration method, it can lead to instability and/or
divergence. This has been shown for MDPs by several researchers (Bertsekas, 1994; Boyan
& Moore, 1995; Baird, 1995; Tsitsiklis & Roy, 1996).
25. This is similar to the QMDP method, which allows both lookahead and greedy designs. In fact, QMDP
can be viewed as a special case of the grid-based method with Q-function approximations, where grid
points correspond to extremes of the belief simplex.
74

fiValue-Function Approximations for POMDPs

4.6.2 On-line Version of the Least-Squares Fit

The problem of finding a set of parameters with the best fit can be solved by any available
optimization procedure. This includes the on-line (or instance-based) version of the gradient
descent method, which corresponds to the well-known delta rule (Rumelhart, Hinton, &
Williams, 1986).
Let f denote a parametric value function over the belief space with adjustable weights
w = fw1 ; w2 ;    ; wk g. Then the on-line update for a weight wi is computed as:

wi

wi ffi (f (bj ) yj )

@f
j ;
@wi b
j

where ffi is a learning constant, and bj and yj are the last-seen point and its value. Note
that the gradient descent method requires the function to be differentiable with regard to
adjustable weights.
To solve the discounted infinite-horizon problem, the stochastic (on-line) version of a
least-squares fit can be combined with either parallel (synchronous) or incremental (GaussSeidel) point updates. In the first case, the value function from the previous step is fixed
and a new value function is computed from scratch using a set of belief point samples and
their values computed through one-step expansion. Once the parameters are stabilized (by
attenuating learning rates), the newly acquired function is fixed, and the process proceeds
with another iteration. In the incremental version, a single value-function model is at the
same time updated and used to compute new values at sampled points. Littman et al. (1995)
and Parr and Russell (1995) implement this approach using asynchronous reinforcement
learning backups in which sample points to be updated next are obtained via stochastic
simulation. We stress that all versions are subject to the threat of instability and divergence,
as remarked above.
4.6.3 Parametric Function Models

To apply the least-squares approach we must first select an appropriate value function
model. Examples of simple convex functions are linear or quadratic functions, but more
complex models are possible as well.
One interesting and relatively simple approach is based on the least-squares approximation of linear action-value functions (Q-functions) (Littman et al., 1995). Here the
value function Vbi+1 is approximated as a piecewise linear and convex combination of Qb i+1
functions:
Vbi+1 (b) = max Qb i+1 (b; a);
a2A
where Qb i+1 (b; a) is the least-squares fit of a linear function for a set of sample points G.
Values at points in G are obtained as

'ai+1 (b) = (b; a) + 

X

o2

P (ojb; a)Vbi ( (b; o; a)):

The method leads to an approximation with jAj linear functions and the coecients of these
functions can be found eciently by solving a set of linear equations. Recall that other two
approximations (the QMDP and the fast informed bound approximations) also work with
75

fiHauskrecht

jAj linear functions. The main differences between the methods are that the QMDP and

fast informed bound methods update linear functions directly, and they guarantee upper
bounds and unique convergence.
A more sophisticated parametric model of a convex function is the softmax model (Parr
& Russell, 1995):
2

Vb (b) = 4

"
X X

ff2

s2S

#k 3 1
ff(s)b(s) 5

k

;

where is the set of linear functions ff with adaptive parameters to fit and k is a \temperature" parameter that provides a better fit to the underlying piecewise linear convex function
for larger values. The function represents a soft approximation of a piecewise linear convex
function, with the parameter k smoothing the approximation.
4.6.4 Control

We tested the control performance of the least-squares approach on the linear Q-function
model (Littman et al., 1995) and the softmax model (Parr & Russell, 1995). For the softmax
model we varied the number of linear functions, trying cases with 10 and 15 linear functions
respectively. In the first set of experiments we used parallel (synchronous) updates and
samples at a fixed set of 100 belief points. We applied stochastic gradient descent techniques
to find the best fit in both cases. We tested the control performance for value-function
approximations obtained after 10, 20 and 30 updates, starting from the QMDP solution. In
the second set of experiments, we applied the incremental stochastic update scheme with
Gauss-Seidel-style updates. The results for this method were acquired after every grid point
was updated 150 times, with learning rates decreasing linearly in the range between 0:2 and
0:001. Again we started from the QMDP solution. The results for lookahead controllers are
summarized in Figure 22, which also shows the control performance of the direct Q-function
controller and, for comparison, the results for the QMDP method.
The linear-Q function model performed very well and the results for the lookahead design
were better than the results for the QMDP method. The difference was quite apparent for
direct approaches. In general, the good performance of the method can be attributed to
the choice of a function model that let us match the shape of the optimal value function
reasonably well. In contrast, the softmax models (with 10 and 15 linear functions) did not
perform as expected. This is probably because in the softmax model all linear functions are
updated for every sample point. This leads to situations in which multiple linear functions
try to track a belief point during its update. Under these circumstances it is hard to capture
the structure of the optimal value function accurately. The other negative feature is that
the effects of on-line changes of all linear functions are added in the softmax approximation,
and thus could bias incremental update schemes. In the ideal case, we would like to identify
one vector ff responsible for a specific belief point and update (modify) only that vector.
The linear Q-function approach avoids this problem by always updating only a single linear
function (corresponding to an action).

76

fiValue-Function Approximations for POMDPs

control performance
70

60
lookahead

10 20
iter iter 30 stoch
iter

10 20
iter iter

10 20 30
iter iter iter

score

50

40

stoch

30
iter stoch

20
10 iter
iter

30
iter

stoch

direct

30

20
QMDP
approximation

linear Q-function
lookahead

linear Q-function
direct

softmax
(10 linear functions)

softmax
(15 linear functions)

Figure 22: Control performance of least-squares fit methods. Models tested include: linear
Q-function model (with both direct and lookahead control) and softmax models with 10 and 15 linear functions (lookahead control only). Value functions
obtained after 10, 20 and 30 synchronous updates and value functions obtained
through the incremental stochastic update scheme are used to define different
controllers. For comparison, we also include results for two QMDP controllers.

4.7 Grid-Based Approximations with Linear Function Updates
An alternative grid-based approximation method can be constructed by applying Sondik's
approach for computing derivatives (linear functions) to points of the grid (Lovejoy, 1991a,
1993). Let Vbi be a piecewise linear convex function described by a set of linear functions i .
Then a new linear function for a belief point b and an action a can be computed eciently
as (Smallwood & Sondik, 1973; Littman, 1996)
ffb;a
i+1 (s) = (s; a) + 

XX
o2 s0 2S

P (s0 ; ojs; a)ffi(b;a;o) (s0 );

where (b; a; o) indexes a linear function ffi in a set of linear functions
maximizes the expression
"
X X

s0 2S s2S

(10)
i

(defining Vbi ) that

#

P (s0 ; ojs; a)b(s) ffi (s0 )

for a fixed combination of b; a; o. The optimizing function for b is then acquired by choosing
the vector with the best overall value from all action vectors. That is, assuming bi+1 is a
set of all candidate linear functions, the resulting functions satisfies
 = arg max X ffb (s)b(s):
ffb;i+1
i+1
ff +1 2 +1 s2S
A collection of linear functions obtained for a set of belief points can be combined into
a piecewise linear and convex value function. This is the idea behind a number of exact
b
i

b
i

77

fiHauskrecht

V(b)
*

V(b)
V(b)
new linear function

0

b

1

b(s1 )

Figure 23: An incremental version of the grid-based linear function method. The piecewise
linear lower bound is improved by a new linear function computed for a belief
point b using Sondik's method.
algorithms (see Section 2.4.2). However, in the exact case, a set of points that cover all
linear functions defining the new value function must be located first, which is a hard task
in itself. In contrast, the approximation method uses an incomplete set of belief points that
are fixed or at least easy to locate, for example via random or heuristic selection. We use
HGL to denote the value-function mapping for the grid approach.
The advantage of the grid-based method is that it leads to more ecient updates. The
time complexity of the update is polynomial and equals O(jGjjAjjS j2 jj). It yields a set of
jGj linear functions, compared to jAjj i jjj possible functions for the exact update.
Since the set of grid-points is incomplete, the resulting approximation lower-bounds the
value function one would obtain by performing the exact update (Lovejoy, 1991a).

Theorem 13 (Lower-bound property of the grid-based linear function update). Let Vbi be a
piecewise linear value function and G a set of grid points used to compute linear function
updates. Then HGL Vbi  H Vbi .
4.7.1 Incremental Linear-Function Approach

The drawback of the grid-based linear function method is that HGL is not a contraction
for the discounted infinite-horizon case, and therefore the value iteration method based on
the mapping may not converge (Lovejoy, 1991a). To remedy this problem, we propose an
incremental version of the grid-based linear function method. The idea of this refinement is
to prevent instability by gradually improving the piecewise linear and convex lower bound
of the value function.
Assume that Vbi  V  is a convex piecewise linear lower bound of the optimal value
function defined by a linear function set i , and let ffb be a linear function for a point b
that is computed from Vbi using Sondik's method. Then one can construct a new improved
value function Vbi+1  Vbi by simply adding the new linear function ffb into i . That is:
i+1 = i [ ffb . The idea of the incremental update, illustrated in Figure 23, is similar
to incremental methods used by Cheng (1988) and Lovejoy (1993). The method can be
78

fiValue-Function Approximations for POMDPs

running times

bound quality

2500

65
9

2000

60

10

8

score

55
50
45
40

2

3

4

5

6

7

8

9

10

2

3

4

5

6

7

8

9

time [sec]

7

10

1

6

1500

10

5

9
8
1000

4

7
6

3

5
4

500

1

2

3

35
1.26

50.02

1

2

1

0

30
standard approach

QMDP

incremental approach

fast
informed

standard approach

incremental approach

Figure 24: Bound quality and running times of the standard and incremental version of
the grid-based linear-function method for the fixed 40-point grid. Cumulative
running times (including all previous update cycles) are shown for both methods.
Running times of the QMDP and the fast informed bound methods are included
for comparison.
extended to handle a set of grid points G in a straightforward way. Note also that after
adding one or more new linear functions to i , some of the previous linear functions may
become redundant and can be removed from the value function. Techniques for redundancy
checking are the same as are applied in the exact approaches (Monahan, 1982; Eagle, 1984).
The incremental refinement is stable and converges for a fixed set of grid points. The
price paid for this feature is that the linear function set i can grow in size over the iteration
steps. Although the growth is at most linear in the number of iterations, compared to
the potentially exponential growth of exact methods, the linear function set describing
the piecewise linear approximation can become huge. Thus, in practice we usually stop
incremental updates well before the method converges. The question that remains open is
the complexity (hardness) of the problem of finding the fixed-point solution for a fixed set
of grid points G.
Figure 24 illustrates some of the trade-offs involved in applying incremental updates
compared to the standard fixed-grid approach on the Maze20 problem. We use the same
grid of 40 points for both techniques and the same initial value function. Results for 1-10
update cycles are shown. We see that the incremental method has longer running times
than the standard method, since the number of linear functions can grow after every update.
On the other hand, the bound quality of the incremental method improves more rapidly
and it can never become worse after more update steps.
4.7.2 Minimum Expected Reward

The incremental method improves the lower bound of the value function. The value function, say Vbi , can be used to create a controller (with either the lookahead or direct-action
choice). In the general case, we cannot say anything about the performance quality of
such controllers with regard to Vbi . However, under certain conditions the performance of
both controllers is guaranteed never to fall below Vbi . The following theorem (proved in the
Appendix) establishes these conditions.

Theorem 14 Let Vbi be a value function obtained via the incremental linear function method,
starting from Vb0 , which corresponds to some fixed strategy C0 . Let CLA;i and CDR;i be two
79

fiHauskrecht

controllers based on Vbi : the lookahead controller and the direct action controller, and V C ,
VC
be their respective value functions. Then Vbi  V C
and Vbi  V C
hold.
We note that the same property holds for the incremental version of exact value iteration.
That is, both the lookahead and the direct controllers perform no worse than Vi obtained
after i incremental updates from some V0 corresponding to a FSM controller C0 .
LA;i

DR;i

LA;i

DR;i

4.7.3 Selecting Grid Points

The incremental version of the grid-based linear-function approximation is exible and
works for an arbitrary grid.26 Moreover, the grid need not be fixed and can be changed on
line. Thus, the problem of finding grids reduces to the problem of selecting belief points to
be updated next. One can apply various strategies to do this. For example, one can use a
fixed set of grid points and update them repeatedly, or one can select belief points on line
using various heuristics.
The incremental linear function method guarantees that the value function is always
improved (all linear functions from previous steps are kept unless found to be redundant).
The quality of a new linear function (to be added next) depends strongly on the quality of
linear functions obtained in previous steps. Therefore, our objective is to select and order
points with better chances of larger improvement. To do this we have designed two heuristic
strategies for selecting and ordering belief points.
The first strategy attempts to optimize updates at extreme points of the belief simplex
by ordering them heuristically. The idea of the heuristic is based on the fact that states
with higher expected rewards (e.g. some designated goal states) backpropagate their effects
(rewards) locally. Therefore, it is desirable that states in the neighborhood of the highest
reward state be updated first, and the distant ones later. We apply this idea to order
extreme points of the belief simplex, relying on the current estimate of the value function
to identify the highest expected reward states and on a POMDP model to determine the
neighbor states.
The second strategy is based on the idea of stochastic simulation. The strategy generates
a sequence of belief points more likely to be reached from some (fixed) initial belief point.
The points of the sequence are then used in reverse order to generate updates. The intent
of this heuristic is to \maximize" the improvement of the value function at the initial fixed
point. To run this heuristic, we need to find an initial belief point or a set of initial belief
points. To address this problem, we use the first heuristic that allows us to order the
extreme points of the belief simplex. These points are then used as initial beliefs for the
simulation part. Thus, we have a two-tier strategy: the top-level strategy orders extremes
of the belief simplex, and the lower-level strategy applies stochastic simulation to generate
a sequence of belief states more likely reachable from a specific extreme point.
We tested the order heuristics and the two-tier heuristics on our Maze20 problem, and
compared them also to two simple point selection strategies: the fixed-grid strategy, in
which a set of 40 grid points was updated repeatedly, and the random-grid strategy, in
which points were always chosen uniformly at random. Figure 25 shows the bound quality
26. There is no restriction on the grid points that must be included in the grid, such as was required for
example in the linear point-interpolation scheme, which had to use all extreme points of the belief
simplex.
80

fiValue-Function Approximations for POMDPs

bound quality
65
60

score

55
2

50

5 6 7 8 9 10
3 4
2

1

3

7
4 5 6

8 9 10
2

4

3

2

7 8 9 10
5 6

3

1

1

1

45

7 8 9 10
4 5 6

40
35
30
fixed grid

random grid

order heuristic

2-tier heuristic

Figure 25: Improvements in the bound quality for the incremental linear-function method
and four different grid-selection heuristics. Each cycle includes 40 grid-point
updates.
of the methods for 10 update cycles (each cycle consists of 40 grid point updates) on the
Maze20 problem. We see that the differences in the quality of value-function approximations
for different strategies (even the very simple ones) are relatively small. We note that we
observed similar results also for other problems, not just Maze20.
The relatively small improvement of our heuristics can be explained by the fact that
every new linear function inuences a larger portion of the belief space and thus the method
should be less sensitive to a choice of a specific point.27 However, another plausible explanation is that our heuristics were not very good and more accurate heuristics or combinations
of heuristics could be constructed. Ecient strategies for locating grid points used in some
of the exact methods, e.g. the Witness algorithm (Kaelbling et al., 1999) or Cheng's methods (Cheng, 1988) can potentially be applied to this problem. This remains an open area
of research.
4.7.4 Control

The grid-based linear-function approach leads to a piecewise linear and convex approximation. Every linear function comes with a natural action choice that lets us choose the
action greedily. Thus we can run both the lookahead and the direct controllers. Figure 26
compares the performance of four different controllers for the fixed grid of 40 points, combining standard and incremental updates with lookahead and direct greedy control after 1,
5 and 10 update cycles. The results (see also Figure 24) illustrate the trade-offs between the
computational time of obtaining the solution and its quality. We see that the incremental
approach and the lookahead controller design tend to improve the control performance. The
prices paid are worse running and reaction times, respectively.
27. The small sensitivity of the incremental method to the selection of grid points would suggest that one
could, in many instances, replace exact updates with simpler point selection strategies. This could
increase the speed of exact value-iteration methods (at least in their initial stages), which suffer from
ineciencies associated with locating a complete set of grid points to be updated in every step. However,
this issue needs to be investigated.
81

fiHauskrecht

control performance
70

5

60

10

10
1

lookahead

lookahead

5

10

5

10

1

5
1

1

score

50
direct
40

direct

30

20
QMDP

fast
informed

direct
standard

lookahead
standard

direct
incremental

lookahead
incremental

Figure 26: Control performance of four different controllers based on grid-based linear function updates after 1, 5 and 10 update cycles for the same 40-point grid. Controllers represent combinations of two update strategies (standard and incremental) and two action-extraction techniques (direct and lookahead). Running
times for the two update strategies were presented in Figure 24. For comparison we include also performances of the QMDP and the fast informed bound
methods (with both direct and lookahead designs).
control performance
70
5

score

5

10

10

1

1

60

1

5

10
1

5

10

50

40

30

20
QMDP

fast
informed

fixed grid

random grid

order heuristic

2-tier heuristic

Figure 27: Control performances of lookahead controllers based on the incremental linearfunction approach and different point-selection heuristics after 1, 5 and 10 improvement cycles. For comparison, scores for the QMDP and the fast informed
bound approximations are shown as well.
Figure 27 illustrates the effect of point selection heuristics on control. We compare the
results for lookahead control only, using approximations obtained after 1, 5 and 10 improvement cycles (each cycle consists of 40 grid point updates). The test results show that, as

82

fiValue-Function Approximations for POMDPs

for the bound quality, there are no big differences among various heuristics, suggesting a
small sensitivity of control to the selection of grid points.

4.8 Summary of Value-Function Approximations
Heuristic value-function approximations methods allow us to replace hard-to-compute exact
methods and trade off solution quality for speed. There are numerous methods we can employ, each with different properties and different trade-offs of quality versus speed. Tables 1
and 2 summarize main theoretical properties of the approximation methods covered in this
paper. The majority of these methods are of polynomial complexity or at least have ecient (polynomial) Bellman updates. This makes them good candidates for more complex
POMDP problems that are out of reach of exact methods.
All of the methods are heuristic approximations in that they do not give solutions of a
guaranteed precision. Despite this fact we proved that solutions of some of the methods are
no worse than others in terms of value function quality (see Figure 15). This was one of the
main contributions of the paper. However, there are currently minimal theoretical results
relating these methods in terms of control performance; the exception are some results
for FSM-controllers and FSM-based approximations. The key observation here is that for
the quality of control (lookahead control) it is more important to approximate the shape
(derivatives) of the value function correctly. This is also illustrated empirically on gridbased interpolation-extrapolation methods in Section 4.5.9 that are based on non-convex
value functions. The main challenges here are to find ways of analyzing and comparing
control performance of different approximations also theoretically and to identify classes of
POMDPs for which certain methods dominate the others.
Finally, we note that the list of methods is not complete and other value-function approximation methods or the refinements of existing methods are possible. For example, White
and Scherer (1994) investigate methods based on truncated histories that lead to upper
and lower bound estimates of the value function for complete information states (complete
histories). Also, additional restrictions on some of the methods can change the properties
of a more generic method. For example, it is possible that under additional assumptions
we will be able to ensure convergence of the least-squares fit approximation.
5. Conclusions

POMDPs offers an elegant mathematical framework for representing decision processes
in stochastic partially observable domains. Despite their modeling advantages, however,
POMDP problems are hard to solve exactly. Thus, the complexity of problem solvingprocedures becomes the key aspect in the sucessful application of the model to real-world
problems, even at the expense of the optimality. As recent complexity results for the
approximability of POMDP problems are not encouraging (Lusena et al., 1998; Madani
et al., 1999), we focus on heuristic approximations, in particular approximations of value
functions.

83

fiHauskrecht

Method
MDP approximation
QMDP approximation
Fast informed bound
UMDP approximation
Fixed-strategy method
Grid-based interpolation-extrapolation
Nearest neighbor
Kernel regression
Linear point interpolation
Curve-fitting (least-squares fit)
linear Q-function
Grid-based linear function method
Incremental version (start from a lower bound)

Bound
upper
upper
upper
lower
lower
upper
lower
lower

Isotonicity

p
p
p
p
p
-p
p
p

Contraction

-p

-*

p
p
p
p
p
-p
p
p

Table 1: Properties of different value-function approximation methods: bound property,
isotonicity and contraction property of the underlying mappings for 0   < 1.
(*) Although incremental version of the grid-based linear-function method is not
a contraction it always converges.
Method
MDP approximation
QMDP approximation
Fast informed bound
UMDP approximation
Fixed-strategy method
Grid-based interpolation-extrapolation
Nearest neighbor
Kernel regression
Linear point interpolation
Fixed interpolation
Best interpolation
Curve-fitting (least-squares fit)
linear Q-function
Grid-based linear function method
Incremental version

Finite-horizon
P
P
P
NP-hard
P
varies
P
P
P
P
P
varies
P
P
NA

Discounted infinite-horizon
P
P
P
undecidable
P
NA
P
P
varies
P
?
NA
NA
NA
?

Table 2: Complexity of value-function approximation methods for finite-horizon problem
and discounted infinite-horizon problem. The objective for the discounted infinitehorizon case is to find the corresponding fixed-point solution. The complexity
results take into account, in addition to components of POMDPs, also all other
approximation specific parameters, e.g., the size of the grid G in grid-based methods. ? indicates open instances and NA methods that are not applicable to one
of the problems (e.g. because of possible divergence).

84

fiValue-Function Approximations for POMDPs

5.1 Contributions
The paper surveys new and known value-function approximation methods for solving POMDPs.
We focus primarily on the theoretical analysis and comparison of the methods, with findings and results supported experimentally on a problem of moderate size from the agent
navigation domain. We analyze the methods from different perspectives: their computational complexity, capability to bound the optimal value function, convergence properties of
iterative implementations, and the quality of derived controllers. The analysis includes new
theoretical results, deriving the properties of individual approximations, and their relations
to exact methods. In general, the relations between and trade-offs among different methods
are not well understood. We provide some new insights on these issues by analyzing their
corresponding updates. For example, we showed that the differences among the exact, the
MDP, the QMDP, the fast-informed bound, and the UMDP methods boil down to simple
mathematical manipulations and their subsequent effect on the value-function approximation. This allowed us to determine relations among different methods in terms of quality of
their respective value functions which is one of the main results of the paper.
We also presented a number of new methods and heuristic refinements of some existing
techniques. The primary contributions in this area include the fast-informed bound, gridbased point interpolation methods (including adaptive grid approaches based on stochastic sampling), and the incremental linear-function method. We also showed that in some
instances the solutions can be obtained more eciently by converting the original approximation into an equivalent finite-state MDP. For example, grid-based approximations with
convex rules can be often solved via conversion into a grid-based MDP (in which grid points
correspond to new states), leading to the polynomial-complexity algorithm for both the finite and the discounted infinite-horizon cases (Section 4.5.3). This result can dramatically
improve the run-time performance of the grid-based approaches. A similar conversion to
the equivalent finite-state MDP, allowing a polynomial-time solution for the discounted
infinite-horizon problem, was shown for the fast informed bound method (Section 4.2).
5.2 Challenges and Future Directions
Work on POMDPs and their approximations is far from complete. Some complexity results
remain open, in particular, the complexity of the grid-based approach seeking the best interpolation, or the complexity of finding the fixed-point solution for the incremental version
of the grid-based linear-function method. Another interesting issue that needs more investigation is the convergence of value iteration with least-squares approximation. Although
the method can be unstable in the general case, it is possible that under certain restrictions
it will converge.
In the paper we use a single POMDP problem (Maze20) only to support theoretical
findings or to illustrate some intuitions. Therefore, the results not supported theoretically (related mostly to control) cannot be generalized and used to rank different methods,
since their performance may vary on other problems. In general, the area of POMDPs
and POMDP approximations suffers from a shortage of larger-scale experimental work with
multiple problems of different complexities and a broad range of methods. Experimental
work is especially needed to study and compare different methods with regard to control
quality. The main reason for this is that there are only few theoretical results relating the
85

fiHauskrecht

control performance. These studies should help focus theoretical exploration by discovering
interesting cases and possibly identifying classes of problems for which certain approximations are more or less suitable. Our preliminary experimental results show that there are
significant differences in control performance among different methods and that not all of
them may be suitable to approximate the control policies. For example, the grid-based
nearest-neighbor approach with piecewise-constant approximation is typically inferior to
and outperformed by other simpler (and more ecient) value-function methods.
The present work focused on heuristic approximation methods. We investigated general (at) POMDPs and did not take advantage of any additional structural refinements.
However, real-world problems usually offer more structure that can be exploited to devise
new algorithms and perhaps lead to further speed-ups. It is also possible that some of the
restricted versions of POMDPs (with additional structural assumptions) can be solved or
approximated eciently, even though the general complexity results for POMDPs or their approximations are not very encouraging (Papadimitriou & Tsitsiklis, 1987; Littman, 1996;
Mundhenk et al., 1997; Lusena et al., 1998; Madani et al., 1999). A challenge here is to
identify models that allow ecient solutions and are at the same time interesting enough
from the point of application.
Finally, a number of interesting issues arise when we move to problems with large state,
action, and observation spaces. Here, the complexity of not only value-function updates
but also belief state updates becomes an issue. In general, partial observability of hidden
process states does not allow us to factor and decompose belief states (and their updates),
even when transitions have a great deal of structure and can be represented very compactly.
Promising directions to deal with these issues include various Monte-Carlo approaches (Isard
& Blake, 1996; Kanazawa, Koller, & Russell, 1995; Doucet, 1998; Kearns et al., 1999)),
methods for approximating belief states via decomposition (Boyen & Koller, 1998, 1999),
or a combination of the two approaches (McAllester & Singh, 1999).
Acknowledgements

Anthony Cassandra, Thomas Dean, Leslie Kaelbling, William Long, Peter Szolovits and
anonymous reviewers provided valuable feedback and comments on this work. This research
was supported by grant RO1 LM 04493 and grant 1T15LM07092 from the National Library
of Medicine, by DOD Advanced Research Project Agency (ARPA) under contract number
N66001-95-M-1089 and DARPA/Rome Labs Planning Initiative grant F30602-95-1-0020.
Appendix A. Theorems and proofs

A.1 Convergence to the Bound
Theorem 6 Let H1 and H2 be two value-function mappings defined on V1 and V2 s.t.
1. H1 , H2 are contractions with fixed points V1 , V2 ;
2. V1 2 V2 and H2 V1  H1 V1 = V1 ;

3. H2 is an isotone mapping.
Then V2  V1 holds.
86

fiValue-Function Approximations for POMDPs

Proof By applying H2 to condition 2 and expanding the result with condition 2 again we
get: H22 V1  H2 V1  H1 V1 = V1 . Repeating this we get in the limit V2      H2n V1 
   H22 V1  H2V1  H1V1 = V1 , which proves the result. 2
A.2 Accuracy of a Lookahead Controller Based on Bounds
Theorem 7 Let VbU and VbL be upper and lower bounds of the optimal value function for
the discounted infinite-horizon problem. Let  = supb jVbU (b) VbL (b)j = kVbU VbL k be
the maximum bound difference. Then the expected reward for a lookahead controller Vb LA ,
constructed for either VbU or VbL , satisfies kVb LA V  k  (1(2 )) .

Proof Let Vb denotes either an upper or lower bound approximation of V  and H LA be the
value function mapping corresponding to the lookahead policy for Vb . Note, that since the
lookahead policy always optimizes its actions with regard to Vb , H Vb = H LA Vb must hold.
The error of Vb LA can be bounded using the triangle inequality

kVb LA V  k  kVb LA Vb k + kVb V k:
The first component satisfies:

kVb LA Vb k = kH LA Vb LA Vb k
 kH LA Vb LA H Vb k + kH Vb Vb k
= kH LA Vb LA H LA Vb k + kH Vb Vb k
  kVb LA Vb k + 
The inequality: kH Vb Vb k   follows from the isotonicity of H and the fact that Vb is either
an upper or a lower bound. Rearranging the inequalities, we obtain: kVb LA Vb k = (1   ) .
The bound on the second term kVb V  k   is trivial.
(2  )
Therefore, kVb LA V  k  [ (1 1  ) + 1] =  (1
) . 2
A.3 MDP, QMDP and the Fast Informed Bounds
Theorem 8 A solution for the fast informed bound approximation can be found by solving
an MDP with jS jjAjjj states, jAj actions and the same discount factor  .
Proof Let ffai be a linear function for action a defining Vbi . Let ffi (s; a) denote parameters
of the function. The parameters of Vbi+1 satisfy:
ffi+1 (s; a) = (s; a) + 
Let

X

X
0
0 0
max
0 2A 0 P (s ; ojs; a)ffi (s ; a ):
a
o2
s 2S

ffi+1 (s; a; o) = max
0

X

a 2 A s0 2 S

87

P (s0 ; ojs; a)ffi (s0 ; a0 ):

fiHauskrecht

Now, we can rewrite ffi+1 (s; a; o) for every s; a; o as:
8
<X

2

X

ffi+1 (s; a; o) = max
P (s0 ; ojs; a) 4(s0 ; a0 ) + 
a0 2A :s0 2S
o0 2
8
2
< X
4
= max
a0 2A : 0

3

2

39
=
ffi (s0 ; a0 ; o0 )5;

P (s0 ; ojs; a)(s0 ; a0 )5 +  4

X X

o0 2 s0 2S

s 2S

39
=
P (s0 ; ojs; a)ffi (s0 ; a0 ; o0 )5;

These equations define an MDP with state space S  A  , action space A and discount
factor  . Thus, a solution for the fast informed bound update can be found by solving an
equivalent finite-state MDP. 2

Theorem 9 Let Vbi corresponds to a piecewise linear convex value function defined by
linear functions. Then H Vbi  HF IB Vbi  HQMDP Vbi  HMDP Vbi :
Proof
8
<X

X

i

9
=

XX

max : (s; a)b(s) + 
max
P (s0 ; ojs; a)b(s)ffi (s0 );
a2A s2S
ff
2
0
o2
s 2S s2S
= (HVi )(b)
i

 max
a2A

X
s2S

2

b(s) 4(s; a) + 

= (HF IB Vi )(b)

 max
a2A

X
s2S

b(s) 4(s; a) + 



s2S

a2A

2

= (HMDP Vbi )(b)

3

P (s0 ; ojs; a)ffi (s0 )5
3

X
s0 2S

2

b(s) max 4(s; a) + 

max

X

o2 ffi 2 i s0 2S

2

= (HQMDP Vbi )(b)
X

X

i

P (s0 js; a) max ffi (s0 )5
ffi 2

i

3

X
s0 2S

P (s0 js; a) max ffi (s0 )5
ffi 2

i

A.4 Fixed-Strategy Approximations
Theorem 10 Let CF SM be an FSM controller. Let CDR and CLA be the direct and the
one-step-lookahead controllers constructed based on CF SM . Then V C (b)  V C (b) and
V C (b)  V C (b) hold for all belief states b 2 I .
Proof The value function for the FSM controller CF SM satisfies:
F SM

F SM

LA

VC

F SM

where

(b) = max V (x; b) = V ( (b); b)
x2M

V (x; b) = (b; (x)) + 

X
o2

P (ojb; (x))V ((x; o);  (b; (x); o)):
88

DR

fiValue-Function Approximations for POMDPs

The direct controller CDR selects the action greedily in every step, that is, it always
chooses according to (b) = arg maxx2M V (x; b). The lookahead controller CLA selects the
action based on V (x; b) one step away:

LA (b) = arg max
a2A

"

#

X

0
(b; a) +  P (ojb; a) max
0 2M V (x ;  (b; a; o)) :
x
o2

By expanding the value function for CF SM for one step we get:

VC

F SM

(b) = max V (x; b)
x2M
"

#

X

= max (b; (x)) +  P (ojb; (x))V ((x; o);  (b; (x); o))
x2M
o2
= (b; ( (b))) + 

 (b; ( (b))) + 
"

X

o2

X

o2

(1)

P (ojb; ( (b)))V ((x; o);  (b; ( (b)); o))

P (ojb; ( (b))) max
V (x0 ;  (b; ( (b)); o))
0
x 2M

X

(2)

#

0
 max
(b; a) +  P (ojb; a) max
0 2M V (x ;  (b; a; o))
a2A
x
o2
X
LA
0
LA
= (b;  (b)) +  P (ojb; LA (b)) max
0 2M V (x ;  (b;  (b); o))
x
o2

(3)

Iteratively expanding maxx0 2M V (x; :) in 2 and 3 with expression 1 and substituing improved
(higher value) expressions 2 and 3 back we obtain value functions for both the direct and
the lookahead controllers. (Expansions of 2 lead to the value for the direct controller
and expansions of 3 to the value for the lookahead controller.) Thus V C
 VC
C
C
and V
 V must hold. Note, however, that action choices (b) and LA(b)
in expressions 2 and 3 can be different leading to different next step belief states and
subsequently to different expansion sequences. Therefore, the above result does not imply
that V DR (b)  V LA (b) for all b 2 I . 2
F SM

F SM

DR

LA

A.5 Grid-Based Linear-Function Method
Theorem 14 Let Vbi be a value function obtained via the incremental linear function method,
starting from Vb0 , which corresponds to some fixed strategy C0 . Let CLA;i and CDR;i be two
controllers based on Vbi : the lookahead controller and the direct action controller, and V C ,
VC
be their respective value functions. Then Vbi  V C
and Vbi  V C
hold.
Proof By initializing the method with a value function for some FSM controller C0 , the
incremental updates can be interpreted as additions of new states to the FSM controller (a
new linear function corresponds to a new state of the FSM). Let Ci be a controller after
step i. Then V C
= Vbi holds and the inequalities follow from Theorem 10. 2
LA;i

DR;i

LA;i

F SM;i

89

DR;i

fiHauskrecht

References

Astrom, K. J. (1965). Optimal control of Markov decision processes with incomplete state
estimation. Journal of Mathematical Analysis and Applications, 10, 174{205.
Baird, L. C. (1995). Residual algorithms: Reinforcement learning with function approximation. In Proceedings of the Twelfth International Conference on Machine Learning,
pp. 30{37.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic
programming. Artificial Intelligence, 72, 81{138.
Bellman, R. E. (1957). Dynamic programming. Princeton University Press, Princeton, NJ.
Bertsekas, D. P. (1994). A counter-example to temporal differences learning. Neural Computation, 7, 270{279.
Bertsekas, D. P. (1995). Dynamic programming and optimal control. Athena Scientific.
Bonet, B., & Geffner, H. (1998). Learning sorting and classification with POMDPs. In
Proceedings of the Fifteenth International Conference on Machine Learning.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. Artificial Intelligence, 11, 1{94.
Boutilier, C., & Poole, D. (1996). Exploiting structure in policy construction. In Proceedings
of the Thirteenth National Conference on Artificial Intelligence, pp. 1168{1175.
Boyan, J. A., & Moore, A. A. (1995). Generalization in reinforcement learning: safely
approximating the value function. In Advances in Neural Information Processing
Systems 7. MIT Press.
Boyen, X., & Koller, D. (1998). Tractable inference for complex stochastic processes. In
Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, pp.
33{42.
Boyen, X., & Koller, D. (1999). Exploiting the architecture of dynamic systems. In Proceedings of the Sixteenth National Conference on Artificial Intelligence, pp. 313{320.
Brafman, R. I. (1997). A heuristic variable grid solution method for POMDPs. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pp. 727{233.
Burago, D., Rougemont, M. D., & Slissenko, A. (1996). On the complexity of partially
observed Markov decision processes. Theoretical Computer Science, 157, 161{183.
Cassandra, A. R. (1998). Exact and approximate algorithms for partially observable Markov
decision processes. Ph.D. thesis, Brown University.
Cassandra, A. R., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: a simple,
fast, exact algorithm for partially observable Markov decision processes. In Proceedings
of the Thirteenth Conference on Uncertainty in Artificial Intelligence, pp. 54{61.
90

fiValue-Function Approximations for POMDPs

Casta~non, D. (1997). Approximate dynamic programming for sensor management. In
Proceedings of Conference on Decision and Control.
Cheng, H.-T. (1988). Algorithms for partially observable Markov decision processes. Ph.D.
thesis, University of British Columbia.
Condon, A. (1992). The complexity of stochastic games. Information and Computation,
96, 203{224.
Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistence and causation.
Computational Intelligence, 5, 142{150.
Dearden, R., & Boutilier, C. (1997). Abstraction and approximate decision theoretic planning. Artificial Intelligence, 89, 219{283.
Doucet, A. (1998). On sequential simulation-based methods for Bayesian filtering. Tech.
rep. CUED/F-INFENG/TR 310, Department of Engineering, Cambridge University.
Drake, A. (1962). Observation of a Markov process through a noisy channel. Ph.D. thesis,
Massachusetts Institute of Technology.
Draper, D., Hanks, S., & Weld, D. (1994). Probabilistic planning with information gathering
and contingent execution. In Proceedings of the Second International Conference on
AI Planning Systems, pp. 31{36.
Eagle, J. N. (1984). The optimal search for a moving target when search path is constrained.
Operations Research, 32, 1107{1115.
Eaves, B. (1984). A course in triangulations for soving differential equations with deformations. Springer-Verlag, Berlin.
Gordon, G. J. (1995). Stable function approximation in dynamic programming. In Proceedings of the Twelfth International Conference on Machine Learning.
Hansen, E. (1998a). An improved policy iteration algorithm for partially observable MDPs.
In Advances in Neural Information Processing Systems 10. MIT Press.
Hansen, E. (1998b). Solving POMDPs by searching in policy space. In Proceedings of the
Fourteenth Conference on Uncertainty in Artificial Intelligence, pp. 211{219.
Hauskrecht, M. (1997). Planning and control in stochastic domains with imperfect information. Ph.D. thesis, Massachusetts Institute of Technology.
Hauskrecht, M., & Fraser, H. (1998). Planning medical therapy using partially observable
Markov decision processes. In Proceedings of the Ninth International Workshop on
Principles of Diagnosis (DX-98), pp. 182{189.
Hauskrecht, M., & Fraser, H. (2000). Planning treatment of ischemic heart disease with
partially observable Markov decision processes. Artificial Intelligence in Medicine, 18,
221{244.
91

fiHauskrecht

Heyman, D., & Sobel, M. (1984). Stochastic methods in operations research: stochastic
optimization. McGraw-Hill.
Howard, R. A. (1960). Dynamic Programming and Markov Processes. MIT Press, Cambridge.
Howard, R. A., & Matheson, J. (1984). Inuence diagrams. Principles and Applications of
Decision Analysis, 2.
Isard, M., & Blake, A. (1996). Contour tracking by stochastic propagation of conditional
density. In Proccedings of Europian Conference on Computer Vision, pp. 343{356.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1999). Planning and acting in
partially observable stochastic domains. Artificial Intelligence, 101, 99{134.
Kanazawa, K., Koller, D., & Russell, S. J. (1995). Stochastic simulation algorithms for
dynamic probabilistic networks. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, pp. 346{351.
Kearns, M., Mansour, Y., & Ng, A. Y. (1999). A sparse sampling algorithm for near
optimal planning in large Markov decision processes. In Proceedings of the Sixteenth
International Joint Conference on Artificial Intelligence, pp. 1324{1331.
Kjaerulff, U. (1992). A computational scheme for reasoning in dynamic probabilistic networks. In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence, pp. 121{129.
Korf, R. (1985). Depth-first iterative deepening: an optimal admissible tree search. Artificial
Intelligence, 27, 97{109.
Kushmerick, N., Hanks, S., & Weld, D. (1995). An algorithm for probabilistic planning.
Artificial Intelligence, 76, 239{286.
Lauritzen, S. L. (1996). Graphical models. Clarendon Press.
Littman, M. L. (1994). Memoryless policies: Theoretical limitations and practical results.
In Cliff, D., Husbands, P., Meyer, J., & Wilson, S. (Eds.), From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive
Behavior. MIT Press, Cambridge.
Littman, M. L. (1996). Algorithms for sequential decision making. Ph.D. thesis, Brown
University.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995). Learning policies for partially
observable environments: scaling up. In Proceedings of the Twelfth International
Conference on Machine Learning, pp. 362{370.
Lovejoy, W. S. (1991a). Computationally feasible bounds for partially observed Markov
decision processes. Operations Research, 39, 192{175.
92

fiValue-Function Approximations for POMDPs

Lovejoy, W. S. (1991b). A survey of algorithmic methods for partially observed Markov
decision processes. Annals of Operations Research, 28, 47{66.
Lovejoy, W. S. (1993). Suboptimal policies with bounds for parameter adaptive decision
processes. Operations Research, 41, 583{599.
Lusena, C., Goldsmith, J., & Mundhenk, M. (1998). Nonapproximability results for Markov
decision processes. Tech. rep., University of Kentucky.
Madani, O., Hanks, S., & Condon, A. (1999). On the undecidability of probabilistic planning
and infinite-horizon partially observable Markov decision processes. In Proceedings of
the Sixteenth National Conference on Artificial Intelligence.
McAllester, D., & Singh, S. P. (1999). Approximate planning for factored POMDPs using
belief state simplification. In Proceedings of the Fifteenth Conference on Uncertainty
in Artificial Intelligence, pp. 409{416.
McCallum, R. (1995). Instance-based utile distinctions for reinforcement learning with
hidden state. In Proceedings of the Twelfth International Conference on Machine
Learning.
Monahan, G. E. (1982). A survey of partially observable Markov decision processes: theory,
models, and algorithms. Management Science, 28, 1{16.
Mundhenk, M., Goldsmith, J., Lusena, C., & Allender, E. (1997). Encyclopaedia of complexity results for finite-horizon Markov decision process problems. Tech. rep., CS
Dept TR 273-97, University of Kentucky.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). The complexity of Markov decision processes. Mathematics of Operations Research, 12, 441{450.
Parr, R., & Russell, S. (1995). Approximating optimal policies for partially observable
stochastic domains. In Proceedings of the Fourteenth International Joint Conference
on Artificial Intelligence, pp. 1088{1094.
Pearl, J. (1988). Probabilistic reasoning in intelligent systems. Morgan Kaufman.
Platzman, L. K. (1977). Finite memory estimation and control of finite probabilistic systems.
Ph.D. thesis, Massachusetts Institute of Technology.
Platzman, L. K. (1980). A feasible computational approach to infinite-horizon partiallyobserved Markov decision problems. Tech. rep., Georgia Institute of Technology.
Puterman, M. L. (1994). Markov decision processes: discrete stochastic dynamic programming. John Wiley, New York.
Raiffa, H. (1970). Decision analysis. Introductory lectures on choices under uncertainty.
Addison-Wesley.
Rumelhart, D., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations
by error propagation. In Parallel Distributed Processing, pp. 318{362.
93

fiHauskrecht

Satia, J., & Lave, R. (1973). Markovian decision processes with probabilistic observation
of states. Management Science, 20, 1{13.
Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Learning without state-estimation
in partially observable Markovian decision processes. In Proceedings of the Eleventh
International Conference on Machine Learning, pp. 284{292.
Smallwood, R. D., & Sondik, E. J. (1973). The optimal control of partially observable
processes over a finite horizon. Operations Research, 21, 1071{1088.
Sondik, E. J. (1971). The optimal control of partially observable Markov decision processes.
Ph.D. thesis, Stanford University.
Sondik, E. J. (1978). The optimal control of partially observable processes over the infinite
horizon: Discounted costs. Operations Research, 26, 282{304.
Tatman, J., & Schachter, R. D. (1990). Dynamic programming and inuence diagrams.
IEEE Transactions on Systems, Man and Cybernetics, 20, 365{379.
Tsitsiklis, J. N., & Roy, B. V. (1996). Feature-based methods for large-scale dynamic
programming. Machine Learning, 22, 59{94.
Washington, R. (1996). Incremental Markov model planning. In Proceedings of the Eight
IEEE International Conference on Tools with Artificial Intelligence, pp. 41{47.
White, C. C., & Scherer, W. T. (1994). Finite memory suboptimal design for partially
observed Markov decision processes. Operations Research, 42, 439{455.
Williams, R. J., & Baird, L. C. (1994). Tight performance bounds on greedy policies based
on imperfect value functions. In Proceedings of the Tenth Yale Workshop on Adaptive
and Learning Systems Yale University.
Yost, K. A. (1998). Solution of large-scale allocation problems with partially observable
outcomes. Ph.D. thesis, Naval Postgraduate School, Monterey, CA.
Zhang, N. L., & Lee, S. S. (1998). Planning with partially observable Markov decision
processes: Advances in exact solution method. In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, pp. 523{530.
Zhang, N. L., & Liu, W. (1997a). A model approximation scheme for planning in partially
observable stochastic domains. Journal of Artificial Intelligence Research, 7, 199{230.
Zhang, N. L., & Liu, W. (1997b). Region-based approximations for planning in stochastic
domains. In Proceedings of the Thirteenth Conference on Uncertainty in Artificial
Intelligence, pp. 472{480.

94

fi