Journal of Artificial Intelligence Research 5 (1996) 139-161

Submitted 4/96; published 10/96

Learning First-Order Definitions of Functions
J. R. Quinlan

quinlan@cs.su.oz.au

Basser Department of Computer Science
University of Sydney
Sydney 2006 Australia

Abstract

First-order learning involves finding a clause-form definition of a relation from examples
of the relation and relevant background information. In this paper, a particular first-order
learning system is modified to customize it for finding definitions of functional relations.
This restriction leads to faster learning times and, in some cases, to definitions that have
higher predictive accuracy. Other first-order learning systems might benefit from similar
specialization.

1. Introduction
Empirical learning is the subfield of AI that develops algorithms for constructing theories
from data. Most classification research in this area has used the attribute-value formalism,
in which data are represented as vectors of values of a fixed set of attributes and are labelled
with one of a small number of discrete classes. A learning system then develops a mapping
from attribute values to classes that can be used to classify unseen data.
Despite the well-documented successes of algorithms developed for this paradigm (e.g.,
Michie, Spiegelhalter, and Taylor, 1994; Langley and Simon, 1995), there are potential
applications of learning that do not fit within it. Data may concern objects or observations
with arbitrarily complex structure that cannot be captured by the values of a predetermined
set of attributes. Similarly, the propositional theory language employed by attribute-value
learners may be inadequate to express patterns in such structured data. Instead, it may
be necessary to describe learning input by relations, where a relation is just a set of tuples
of constants, and to represent what is learned in a first-order language. Four examples of
practical learning tasks of this kind are:

 Speeding up logic programs (Zelle and Mooney, 1993). The idea here is to learn a

guard for each nondeterministic clause that inhibits its execution unless it will lead
to a solution. Input to the learner consists of a Prolog program and one or more
execution traces. In one example Dolphin, the system cited above, transformed a
program from complexity O(n!) to O(n2 ).

 Learning search control heuristics (Leckie and Zukerman, 1993). Formulation of pref-

erence criteria that improve eciency in planning applications has a similar avor.
One task investigated here is the familiar `blocks world' in which varying numbers of
blocks must be rearranged by a robot manipulator. Each input to learning concerns
a particular situation during search and includes a complete description of the current planning state and goals. As the amount of this information increases with the

c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiQuinlan

number of blocks and their inter-relationships, it cannot be encoded as a fixed set of
values.
 Recovering software specifications. Cohen (1994) describes an application based on a
software system consisting of over a million lines of C code. Part of the system implements virtual relations that compute projections and joins of the underlying base
relations, and the goal is to reconstruct their definitions. Input to learning consists of
queries, their responses, and traces showing the base relations accessed while answering the queries. The output is a logical description of the virtual relation; since this
involves quantified variables, it lies beyond the scope of propositional attribute-value
languages.
 Learning properties of organic molecules (Muggleton, King, and Sternberg, 1992;
Srinivasan, Muggleton, Sternberg, and King, 1996). The approach to learning in
these papers is based on representing the structure of the molecules themselves in
addition to properties of molecules and molecule segments. The latter paper notes
the discovery of a useful indicator of mutagenicity expressed in terms of this structure.
The development of learning methods based on this more powerful relational formalism
is sometimes called inductive logic programming (Muggleton, 1992; Lavrac and Dzeroski,
1994; De Raedt, 1996). Input typically consists of tuples that belong, or do not belong,
to a target relation, together with relevant information expressed as a set of background
relations. The learning task is then to formulate a definition of the target relation in terms
of itself and the background relations.
This relational learning task is described in more detail in the following section. Several
algorithms for relational learning have been developed recently, and Section 3 introduces
one such system called foil (Quinlan, 1990). While foil can be used with relations of
any kind, one particularly common use of relations is to represent functions. Changes to
foil that in effect customize it for learning functional relations are outlined in Section 4.
Several comparative studies, presented in Section 5, show that this specialization leads to
much shorter learning times and, in some cases, to more accurate definitions. Related work
on learning functional relations is discussed in Section 6, and the paper ends with some
conclusions from this study and directions for further development.

2. Relational Learning

An n-ary relation RE consists of a set of n-tuples of ground terms (here constants). All
constants in the ith position of the tuples belong to some type, where types may be differentiated or all constants may be taken to belong to a single universal type.
As an alternative to this extensional definition as a (possibly infinite) set, a relation can
be specified intensionally via an n-argument predicate RI defined by a Prolog program. If

hc1 ; c2 ; :::cn i 2 RE if and only if RI (c1 ; c2 ; :::; cn ) is true
for any constants fci g, then the intensional and extensional definitions are equivalent. For
convenience, the subscripts of RE and RI will be omitted and R will be used to denote
either the set of tuples or the predicate.
140

fiLearning First-Order Definitions of Functions

Input to a relational learning task consists of extensional information about a target
relation R and extensional or intensional definitions of a collection of background relations.
Examples of tuples known to belong to the target relation are provided and, in most cases,
so are examples of tuples known not to belong to R. The goal is to learn a Prolog program
for R that covers all tuples known to belong to R but no tuples known not to belong to R
or, in other words, a program that agrees with the extensional information provided about
R.
Many relations of interest are infinite. An alternative to selecting examples that belong
or do not belong to R is to define a finite vocabulary V and to specify relations with respect
to this vocabulary. That is, R is represented as the finite set of tuples, all constants in
which belong to V . Since this specification of R is complete over the vocabulary, the tuples
that do not belong to R can be inferred by the closed world assumption as the complement
of the tuples in R.
A function f (X1 ; X2 ; :::; Xk ) of k arguments can be represented by a k+1-ary relation
F (X1 ; X2 ; :::; Xk ; Xk+1 ) where, for each tuple in F , the value of the last argument is the
result of applying f to the first k arguments. (Rouveirol (1994) proves that this attening
can be used to remove all non-constant function symbols from any first-order language.)
Such functional relations have an additional property that for any constants fc1 ; c2 ; ::; ck g
there is exactly one value of ck+1 such that hc1 ; c2 ; :::; ck+1 i belongs to F .
As an example, consider the three-argument predicate append(A,B,C) whose meaning is
that the result of appending list A to list B is list C.1 The corresponding relation append is
infinite, but a restricted vocabulary can be defined as all at lists containing only elements
from f1,2,3g whose length is less than or equal to 3. There are 40 such lists
[ ], [1], [2], [3], [1,2], ...., [3,3,2], [3,3,3]

and 64,000 3-tuples of lists. With respect to this vocabulary, append consists of 142 of these
3-tuples, viz.:

h[ ],[ ],[ ]i, h[ ],[1],[1]i, ..., h[2],[1,3],[2,1,3]i, ..., h[3,3,3],[ ],[3,3,3]i.
There is also a background relation components, where components(A,B,C) means that list
A has head B and tail C. The goal is then to learn an intensional definition of append given
the background relation components. A suitable result might be expressed as
append([ ],A,A).
append(A,B,C) :- components(A,D,E), append(E,B,F), components(C,D,F).

which is recognizable as a Prolog definition of append.
1. In Prolog, append can be invoked with any combination of its arguments bound so as to find possible
values for the unbound arguments. Here and in Section 5.1, however, append is treated as a function
from the first two arguments to the third.

141

fiQuinlan

Initialization:
definition := null program
remaining := all tuples belonging to target relation R
While remaining is not empty
/* Grow a new clause */
clause := R(A; B; :::) :While clause covers tuples known not to belong to R
/* Specialize clause */
Find appropriate literal(s) L
Add L to body of clause
Remove from remaining tuples in R covered by clause
Add clause to definition
Figure 1: Outline of foil

3. Description of foil

In common with many first-order learning systems, foil requires that the background relations are also defined extensionally by sets of tuples of constants.2 Although the intensional
definition is learned from a particular set of examples, it is intended to be executable as a
Prolog program in which the background relations may also be specified intensionally by
definitions rather than by sets of ground tuples. For instance, although the append definition
above might have been learned from particular examples of lists, it will correctly append
arbitrary lists, provided that components is specified by a suitable clausal definition. (The
applicability of learned definitions to unseen examples cannot be guaranteed, however; Bell
and Weber (1993) call this the open domain assumption.)
The language in which foil expresses theories is a restricted form of Prolog that omits
cuts, fail, disjunctive goals, and functions other than constants, but allows negated literals
not(L(...)). This is essentially the Datalog language specified by Ullman (1988), except that
there is no requirement that all variables in a negated literal appear also in the head or in
another unnegated literal; foil interprets not using negation as failure (Bratko, 1990).

3.1 Broad-brush overview
As outlined in Figure 1, foil uses the separate-and-conquer method, iteratively learning
a clause and removing the tuples in the target relation R covered by the clause until none
remain. A clause is grown by repeated specialization, starting with the most general clause
2. Prominent exceptions include focl (Pazzani and Kibler, 1992), filp (Bergadano and Gunetti, 1993),
and Foidl (Mooney and Califf, 1995), that allow background relations to be defined extensionally, and
Progol (Muggleton, 1995), in which information about all relations can be in non-ground form.

142

fiLearning First-Order Definitions of Functions

head and adding literals to the body until the clause does not cover any tuples known not
to belong to R.
Literals that can appear in the body of a clause are restricted by the requirement that
programs be function-free, other than for constants appearing in equalities. The possible
literal forms that foil considers are:

 Q(X1 ; X2 ; :::; Xk ) and not (Q(X1 ; X2 ; :::; Xk )), where Q is a relation and the Xi's de-

note known variables that have been bound earlier in the clause or new variables. At
least one variable must have been bound earlier in the partial clause, either by the
head or a literal in the body.

 Xi =Xj and Xi 6=Xj , for known variables Xi and Xj of the same type.
 Xi =c and Xi6=c, where Xi is a known variable and c is a constant of the appropriate

type. Only constants that have been designated as suitable to appear in a definition
are considered { a reasonable definition for append might reference the null list [ ] but
not an arbitrary list such as [1,2].

 Xi  Xj , Xi > Xj , Xi  t, and Xi > t, where Xi and Xj are known variables with
numeric values and t is a threshold chosen by foil.

If the learned definition must be pure Prolog, negated literal forms not (Q(:::)) and Xi 6=...
can be excluded by an option.
Clause construction is guided by different possible bindings of the variables in the partial
clause that satisfy the clause body. If the clause contains k variables, a binding is a k-tuple
of constants that specifies a value for all variables in sequence. Each possible binding is
labelled  or 	 according to whether the tuple of values for the variables in the clause head
does or does not belong in the target relation.
As an illustration, consider the tiny task of constructing a definition of plus(A,B,C),
meaning A+B = C, using the background relation dec(A,B), denoting B = A,1. The
vocabulary is restricted to just the integers 0, 1, and 2, so that plus consists of the tuples

h0,0,0i, h1,0,1i, h2,0,2i, h0,1,1i, h1,1,2i, h0,2,2i
and dec contains only h1,0i and h2,1i.
The initial clause consists of the head
plus(A,B,C) :-

in which each variable is unique. The labelled bindings corresponding to this initial partial
clause are just the tuples that belong, or do not belong, to the target relation, i.e.:

h0,0,0i 
h0,0,1i 	
h1,0,0i 	
h1,2,2i 	
h2,2,0i 	

h1,0,1i 
h0,0,2i 	
h1,0,2i 	
h2,0,0i 	
h2,2,1i 	

h2,0,2i 
h0,1,0i 	
h1,1,0i 	
h2,0,1i 	
h2,2,2i 	

h0,1,1i 
h0,1,2i 	
h1,1,1i 	
h2,1,0i 	

143

h1,1,2i 
h0,2,0i 	
h1,2,0i 	
h2,1,1i 	

h0,2,2i 
h0,2,1i 	
h1,2,1i 	
h2,1,2i 	

.

fiQuinlan

foil repeatedly tries to construct a clause that covers some tuples in the target relation

R but no tuples that are definitely not in R. This can be restated as finding a clause that

has some  bindings but no 	 bindings, so one reason for adding a literal to the clause is
to move in this direction by increasing the relative proportion of  bindings. Such gainful
literals are evaluated using an information-based heuristic. Let the number of  and 	
bindings of a partial clause be n and n	 respectively. The average information provided
by the discovery that one of the bindings has label  is
 !
n

	
I (n ; n ) = , log2 n + n	 bits.
If a literal L is added, some of these bindings may be excluded and each of the rest will give
rise to one or more bindings for the new partial clause. Suppose that k of the n bindings
are not excluded by L, and that the numbers of bindings of the new partial clause are m
and m	 respectively. If L is chosen so as to increase the proportion of  bindings, the total
information gained by adding L is then

k  (I (n; n	) , I (m ; m	 )) bits.
Consider the result of specializing the above clause by the addition of the literal A=0.
All but nine of the bindings are eliminated because the corresponding values of the variables
do not satisfy the new partial clause. The bindings are reduced to

h0,0,0i  h0,1,1i  h0,2,2i 
h0,0,1i 	 h0,0,2i 	 h0,1,0i 	 h0,1,2i 	 h0,2,0i 	 h0,2,1i 	
in which the proportion of  bindings has increased from 6/27 to 3/9. The information
gained by adding this literal is therefore 3  (I (6; 21) , I (3; 6)) or about 2 bits. Adding the
further literal B=C excludes all the 	 bindings, giving a complete first clause
plus(A,B,C) :- A=0, B=C.

or, as it would be more commonly written,
plus(0,B,B).

This clause covers three tuples of plus which are then removed from the set of tuples to be
covered by subsequent clauses. At the commencement of the search for the second clause,
the head is again plus(A,B,C) and the bindings are now

h1,0,1i 
h0,0,1i 	
h1,0,0i 	
h1,2,2i 	
h2,2,0i 	

h2,0,2i 
h0,0,2i 	
h1,0,2i 	
h2,0,0i 	
h2,2,1i 	

h1,1,2i 
h0,1,0i 	 h0,1,2i 	 h0,2,0i 	 h0,2,1i 	
h1,1,0i 	 h1,1,1i 	 h1,2,0i 	 h1,2,1i 	
h2,0,1i 	 h2,1,0i 	 h2,1,1i 	 h2,1,2i 	
h2,2,2i 	
.

The above literals were added to the body of the first clause because they gain information. A quite different justification for adding a literal is to introduce new variables that
may be needed in the final clause. Determinate literals are based on an idea introduced
144

fiLearning First-Order Definitions of Functions

by Golem (Muggleton and Feng, 1992). A determinate literal is one that introduces new
variables so that the new partial clause has exactly one binding for each  binding in the
current clause, and at most one binding for each 	 binding. Determinate literals are useful because they introduce new variables, but neither reduce the potential coverage of the
clause nor increase the number of bindings.
Now that the  bindings do not include any with A=0, the literal dec(A,D) is determinate
because, for each value of A, there is one value of D that satisfies the literal. Similarly, since
the  bindings contain none with C=0, the literal dec(C,E) is also determinate.
In Figure 1, the literals L added by foil at each step are

 the literal with greatest gain, if this gain is near the maximum possible
(namely n  I (n ; n	 )) ; otherwise
 all determinate literals found; otherwise
 the literal with highest positive gain; otherwise
 the first literal investigated that introduces a new variable.
At the start of the second clause, no literal has near-maximum gain and so all determinate
literals are added to the clause body. The partial clause
plus(A,B,C) :- dec(A,D), dec(C,E),

has five variables and the bindings that satisfy it are

h1,0,1,0,0i  h2,0,2,1,1i  h1,1,2,0,1i 
h1,0,2,0,1i 	 h1,1,1,0,0i 	 h2,0,1,1,0i 	 h2,1,1,1,0i 	 h2,1,2,1,1i 	
The literal plus(B,D,E), which uses these newly-introduced variables, is now satisfied by all
three  bindings but none of the 	 bindings, giving a complete second clause
plus(A,B,C) :- dec(A,D), dec(C,E), plus(B,D,E).

All tuples in plus are covered by one or other of these clauses, so they constitute a complete
intensional definition of the target relation.

3.2 Details omitted
foil is a good deal more complex than this overview would suggest. Since they are not

important for this paper, matters such as the following are not discussed here, but are
covered in (Quinlan and Cameron-Jones, 1993; 1995):

 Recursive soundness. If the goal is to be able to execute the learned definitions as

ordinary Prolog programs, it is important that they terminate. foil has an elaborate
mechanism to ensure that any recursive literal (such as plus(B,D,E) above) that is
added to a clause body will not cause problems in this respect, at least for ground
queries.
145

fiQuinlan

 Pruning. In practical applications with numerous background relations, the number

of possible literals L that could be added at each step grows exponentially with the
number of variables in the partial clause. foil employs some further heuristics to limit
this space, such as Golem's bound on the depth of a variable (Muggleton and Feng,
1992). More importantly, some regions of the literal space can be pruned without
examination because they can be shown to contain neither determinate literals, nor
literals with higher gain than the best gainful literal found so far.
 More complete search. As presented above, foil is a straightforward greedy hillclimbing algorithm. In fact, because foil can sometimes reach an impasse in its
search for a clause, it contains a limited non-chronological backtracking facility to
allow it to recover from such situations.
 Simplifying definitions. The addition to the partial clause of all determinate literals
found may seem excessive. However, as a clause is completed, foil examines each
literal in the clause body to see whether it could be discarded without causing the
simpler clause to match tuples not in the target relation R. Similarly, when the
definition is complete, each clause is checked to see whether it could be omitted
without leaving any tuples in R uncovered. There are also heuristics that aim to
make clauses more understandable by substituting simpler literals (such as variable
equalities) for literals based on more complex relations.
 Recognizing boundaries of closed worlds. Some literals that appear to discriminate 
from 	 bindings do so only as a consequence of boundary effects attributable to a
limited vocabulary.3 When a definition including such literals is executed with larger
vocabularies, the open domain assumption mentioned above may be violated. foil
contains an optional mechanism for describing when literals might be satisfied by
bindings outside the closed world, allowing some literals with unpredictable behavior
to be excluded.
Quinlan (1990) and Quinlan and Cameron-Jones (1995) summarize several applications
successfully addressed by foil, some of which are also discussed in Section 5.

4. Learning Functional Relations
The learning approach used by foil makes no assumptions about the form of the target

relation R. However, as with append and plus above, the relation is often used to represent a
function { in any tuple of constants that satisfies R, the last constant is uniquely determined
by the others. Bergadano and Gunetti (1993) show that this property can be exploited to
make the learning task more tractable.

4.1 Functional relations and foil
Although foil can learn definitions for functional relations, it is handicapped in two ways:
 Ground queries: foil's approach to recursive soundness assumes that only ground
queries will be made of the learned definition. That is, a definition of R(X1 ; X2 ; :::; Xn )

3. An example of this arises in Section 5.1.

146

fiLearning First-Order Definitions of Functions

will be used to provide true-false answers to queries of the form R(c1 ; c2 ; :::; cn )? where
the ci 's are constants. If R is a functional relation, however, a more sensible query
would seem to be R(c1 ; c2 ; :::; cn,1 ; X )? to determine the value of the function for
specified ground arguments. In the case of plus, for instance, we would not expect to ask plus(1,1,2)? (\is 1+1=2?"), but rather plus(1,1,X)? (\what is 1+1?").
R(c1 ; c2 ; :::; cn,1 ; X )? will be called the standard query for functional relations.
 Negative examples: foil needs both tuples that belong to the target relation and at
least some that do not. In common with other ILP systems such as Golem (Muggleton
and Feng, 1992), the latter are used to detect when a partial clause is still too general.
These can be specified to foil directly or, more commonly, are derived under the
closed world assumption that, with respect to the vocabulary, all tuples in R have
been given. This second mechanism can often lead to very large collections of tuples
not in R; there were nearly 64,000 of them in the append illustration earlier. Every
tuple not belonging to R results in a binding at the start of each clause, so there can
be uncomfortably many bindings that must be maintained and tested at each stage
of clause development.4 However, functional relations do not need explicit counterexamples, even when the set of tuples belonging to R is not complete with respect to
some vocabulary { knowing that hc1 ; c2 ; :::; cn i belongs to R implies that there is no
other constant c0n such that hc1 ; c2 ; :::; c0n i is in R.
These problematic aspects of foil vis a vis functional relations suggest modifications to
address them. The alterations lead to a new system, ffoil, that is still close in spirit to its
progenitor.

4.2 Description of ffoil

Since the last argument of a functional relation has a special role, it will be referred to as
the output argument of the relation. Similarly, the variable corresponding to this argument
in the head of a clause is called the output variable.
The most fundamental change in ffoil concerns the bindings of partial clauses and the
way that they are labelled. A new constant 2 is introduced to indicate an undetermined
value of the output variable in a binding. Bindings will be labelled according to the value
of the output variable, namely  if this value is correct (given the value of the earlier
constants), 	 if the value is incorrect, and fi if the value is undetermined.
The outline of ffoil (Figure 2) is very similar to Figure 1, the only differences being
those highlighted. At the start of each clause there is one binding for every remaining tuple
in the target relation. The output variable has the value 2 in these bindings and this value
is changed only when some subsequent literal assigns a value to the variable. In the small
plus example of Section 3.1, the initial bindings for the first clause are
h0,0,2i fi h1,0,2i fi h2,0,2i fi h0,1,2i fi h1,1,2i fi h0,2,2i fi
Like its ancestor, ffoil also assesses potential literals for adding to the clause body as
gainful or determinate, although both concepts must be adjusted to accommodate the new
label fi. Suppose that there are r distinct constants in the range of the target function.
4. For this reason, foil includes an option to sample the 	 bindings instead of using all of them.

147

fiQuinlan

Initialization:
definition := null program
remaining := all tuples belonging to target relation R
While remaining is not empty
/* Grow a new clause */
clause := R(A; B; :::) :While clause has 	 or fi bindings
/* Specialize clause */
Find appropriate literal(s) L
Add L to body of clause
Remove from remaining tuples in R covered by clause
Add clause to definition
Simplify final definition
Add default clause
Figure 2: Outline of ffoil

Each fi binding can be converted to a  binding only by changing 2 to the correct value
of the function, and to a 	 binding by changing 2 to any of the r , 1 incorrect values. In
computing information gain, ffoil thus counts each fi binding as 1  binding and r , 1 	
bindings. A determinate literal is now one that introduces one or more variables so that, in
the new partial clause, there is exactly one binding for each current  or fi binding and at
most one binding for each current 	 binding. ffoil uses the same preference criterion for
adding literals L: a literal with near-maximum gain, then all determinate literals, then the
most gainful literal, and finally a non-determinate literal that introduces a new variable.
The first literal chosen by foil in Section 3.1 was A=0 since this increases the concentration of  bindings from 9 in 64 to 3 in 9 (with a corresponding information gain). From
ffoil's perspective, however, this literal simply reduces six fi bindings to three and so gives
no gain; as the range of plus is the set f0,1,2g, r = 3, and so the putative concentration of
 bindings would alter from 6 in 18 to 3 in 9. The literal A=C, on the other hand, causes
the value of the output variable to be determined and results in the bindings

h0,0,0i  h1,0,1i  h2,0,2i 
h0,1,0i 	 h1,1,1i 	 h0,2,0i 	 .
This corresponds to an increase in concentration of  bindings from a notional 6 in 18 to

3 in 6, with an information gain of about 2 bits. Once this literal has been added to the
clause body, ffoil finds that a further literal B=0 eliminates all 	 bindings, giving the
148

fiLearning First-Order Definitions of Functions

complete clause
plus(A,B,C) :- A=C, B=0.

The remaining tuples of plus give the bindings

h0,1,2i fi h1,1,2i fi h0,2,2i fi
at the start of the second clause. The literals dec(B,D) and dec(E,A) are both determinate
and, when they are added to the clause, the bindings become

h0,1,2,0,1i fi h1,1,2,0,2i fi h0,2,2,1,1i fi
in which the output variable is still undetermined. If the partial clause is further specialized
by adding the literal plus(E,D,C), the new bindings

h0,1,1,0,1i  h1,1,2,0,2i  h0,2,2,1,1i 
give the correct value for C in each case. Since there are no fi or 	 bindings, this clause is

also complete.
One important consequence of the new way that bindings are initialized at the start of
each clause is easily overlooked. With foil, there is one 	 binding for each tuple that does
not belong in R; since each clause excludes all 	 bindings, it discriminates some tuples in
R from all tuples not in R. This is the reason that the learned clauses can be regarded as
a set and can be executed in any order without changing the set of answers to a query. In
ffoil, however, the initial bindings concern only the remaining tuples in R, so a learned
clause depends on the context established by earlier clauses. For example, suppose a target
relation S and a background relation T are defined as
S = fhv,1i, hw,1i, hx,1i, hy,0i, hz,0ig
T = fhvi, hwi, hxig .

The first clause learned by ffoil might be
S(A,1) :- T(A).

and the remaining bindings fhy,0i, hz,0ig could then be covered by the clause
S(A,0).

The latter clause is clearly correct only for standard queries that are not covered by the
first clause. As this example illustrates, the learned clauses must be interpreted in the order
in which they were learned, and each clause must be ended with a cut `!' to protect later
clauses from giving possibly incorrect answers to a query. Since the target relation R is
functional, so that there is only one correct response to a standard query as defined above,
this use of cuts is safe in that it cannot rule out a correct answer.
Both foil and ffoil tend to give up too easily when learning definitions to explain noisy
data. This can result in over-specialized clauses that cover the target relation only partially.
On tasks for which the definition learned by ffoil is incomplete, a final global simplification
149

fiQuinlan

phase is invoked. Clauses in the definition are generalized by removing literals so long as the
total number of errors on the target relation does not increase. In this way, the accuracy of
individual clauses is balanced against the accuracy of the definition as a whole; simplifying
a clause by removing a literal may increase the number of errors made by the clause, but
this can be offset by a reduction in the number of uncovered bindings and a consequently
lower global error rate. When all clauses have been simplified as much as possible, entire
clauses that contribute nothing to the accuracy of the definition are removed.
In the final step of Figure 2, the target relation is assumed to represent a total function,
with the consequence that a response must always be returned for a standard query. As a
safeguard, ffoil adds a default clause

R(X1 ; X2 ; :::; Xn,1 ; c):
where c is the most common value of the function.5 The most common value of the output
argument of plus is 2, so the complete definition for this example, in normal Prolog notation,
becomes
plus(A,0,A) :- !.
plus(A,B,C) :- dec(B,D), dec(E,A), plus(E,D,C), !.
plus(A,B,2).

4.3 Advantages and disadvantages of ffoil

Although the definitions for plus in Sections 3.1 and 4.2 are superficially similar, there are
considerable differences in the learning processes by which they were constructed and in
their operational characteristics when used.

 ffoil generally needs to maintain fewer bindings and so learns more quickly. Whereas

foil keeps up to 27 bindings while learning a definition of plus, ffoil never uses more
than 6.

 The output variable is guaranteed to be bound in every clause learned by ffoil. This
is not necessarily the case with foil, since there is no requirement that every variable
appearing in the head must also appear in the clause body.

 Definitions found by ffoil often execute more eciently than their foil counterparts.

Firstly, ffoil definitions, through the use of cuts, exploit the fact that there cannot be
more than one correct answer to a standard query. Secondly, clause bodies constructed
by ffoil tend not to use the output variable until it has been bound, so there is less
backtracking during evaluation. As an illustration, the foil definition of Section
3.1 evaluates 81 goals in answering the query plus(1,1,X)?, many more than the six
evaluations needed by the ffoil definition for the same query.

There are also entries on the other side of the ledger:
5. No default clause is added if each value of the function occurs only once.

150

fiLearning First-Order Definitions of Functions

Task

Bkgd
Relns

append
last element
reverse
left shift
translate

2
3
10
12
14

Length 3
Bindings
Time



	

foil ffoil

142 63,858
3.0
39
81
0.0
40 1560
2.6
39 1561
0.5
40 3120 817.9

Length 4
Bindings
Time



	

foil ffoil

0.5 1593 396,502 22.4
0.0 340
1024
0.5
0.3 341 115,940 195.9
0.3 340 115,940 26.6
1.1 341 115,940 495.9

10.9
0.3
9.0
6.8
28.0

Table 1: Results on tasks from (Bratko, 1990).

 foil is applicable to more learning tasks that ffoil, which is limited to learning
definitions of functional relations.

 The implementation of ffoil is more complex than that of foil. For example, many of

the heuristics for pruning the literal search space and for checking recursive soundness
require special cases for the constant 2 and for fi bindings.

5. Empirical Trials

In this section the performance of ffoil on a variety of learning tasks is summarized and
compared with that of foil (release 6.4). Since the systems are similar in most respects,
this comparison highlights the consequences of restricting the target relation to a function.
Times are for a DEC AXP 3000/900 workstation. The learned definitions from the first
three subsections may be found in the Appendix.

5.1 Small list manipulation programs

Quinlan and Cameron-Jones (1993) report the results of applying foil to 16 tasks taken
from Bratko's (1990) well-known Prolog text. The list-processing examples and exercises
of Chapter 3 are attempted in sequence, where the background information for each task
includes all previously-encountered relations (even though most of them are irrelevant to
the task at hand). Two different vocabularies are used: all 40 lists of length up to 3 on
three elements and all 341 lists of length up to 4 on four elements.
Table 1 describes the five functional relations in this set and presents the performance
of foil and ffoil on them. All the learned definitions are correct for arbitrary lists, with
one exception { foil's definition of reverse learned from the larger vocabulary includes the
clause
reverse(A,A) :- append(A,A,C), del(D,E,C).

that exploits the bounded length of lists.6 The times reveal a considerable advantage to
6. If C is twice the length of A and E is one element longer than C while still having length  4, then the
length of A must be 0 or 1. In that case A is its own reverse.

151

fiQuinlan

Task
foil ffoil
quicksort
4.7
2.2
bubblesort 7.3
0.4
Table 2: Times (sec) for learning to sort.

[3,3]
0.7
0.8
Golem 4.8
Progol 43.0

ffoil
foil

Time (secs)
Ratio to [3,3]
[3,4] [4,4]
[4,5] [3,4] [4,4]
[4,5]
1.5
4.5
15.0 2.1
6.4
21.4
4.3
11.9
146.3 5.4 14.9
182.9
14.6
59.6
>395 3.0 12.4 >82.3
447.9 5271.9 >76575 10.4 122.6 >1780.8

Table 3: Comparative times for quicksort task.

ffoil in all tasks except the second. In fact, for the first and last task with the larger
vocabulary, these times understate ffoil's advantage. The total number of bindings for
append is 3413 , or about 40 million, so a foil option was used to sample only 1% of the
	 bindings to prevent foil exceeding available memory. Had it been possible to run foil

with all bindings, the time required to learn the definition would have been considerably
longer. Similarly, foil exhausted available memory on the translation task when all 232,221
possible bindings were used, so the above results were obtained using a sample of 50% of
the 	 bindings.

5.2 Learning quicksort and bubblesort
These tasks concern learning how to sort lists from examples of sorted lists. In the first,
the target relation qsort(A,B) means that B is the sorted form of A. Three background
relations are provided: components and append as before, and partition(A,B,C,D), meaning
that partitioning list B on value A gives the list C of elements less than A and list D of
elements greater than A. In the second task, the only background relations for learning
bsort(A,B) are components and lt(A,B), meaning A<B. The vocabulary used for both tasks
is all lists of length up to 4 with non-repeated elements drawn from f1,2,3,4g. There are
thus 65  and 4160 	 bindings for each task.
Both foil and ffoil learn the \standard" definition of quicksort. Times shown in Table
2 are comparable, mainly because ffoil learns a superuous over-specialized clause that is
later discarded in favor of the more general recursive clause. The outcome for bubblesort
is quite different { ffoil learns twenty times faster than foil but its definition is more
verbose.
The quicksort task provides an opportunity to compare ffoil with two other wellknown relational learning systems. Like ffoil and foil, both Golem (Muggleton and
152

fiLearning First-Order Definitions of Functions

Task
foil ffoil
Ackermann's function
12.3
0.2
greatest common divisor 237.5
1.2
Table 4: Times (sec) for arithmetic functions.

Feng, 1992) and Progol (release 4.1) (Muggleton, 1995) are implemented in C, so that
timing comparisons are meaningful. Furthermore, both systems include quicksort among
their demonstration learning tasks, so it is reasonable to assume that the parameters that
control these systems have been set to appropriate values.
The four learning systems are evaluated using four sets of training examples, obtained
by varying the maximum length S of the lists and the size A of the alphabet of nonrepeating elements that can appear in the lists, as in (Quinlan, 1991). Denoting each set
by a pair [S ,A], the four datasets are [3,3], [3,4], [4,4], and [4,5]. The total numbers of
possible bindings for these tasks, 256, 1681, 4225, and 42,436 respectively, span two orders
of magnitude. Table 3 summarizes the execution times7 required by the systems on these
datasets. Neither Golem nor Progol completed the last task; Golem exhausted the available
swap space of 60Mb, and Progol was terminated after using nearly a day of cpu time. The
table also shows the ratio of the execution time of the latter three to the simplest dataset
[3,3]. The growth in ffoil's execution time is far slower than that of the other systems,
primarily because ffoil needs only the  tuples while the others use both  and 	 tuples.
Golem's execution time seems to grow slightly slower than foil's, while Progol's growth
rate is much higher.

5.3 Arithmetic functions

The systems have been also used to learn definitions of complex functions from arithmetic.
Ackermann's function
8
>
if m = 0
< n+1
f (m; n) = > f (m , 1; 1)
if n = 0
: f (m , 1; f (m; n , 1)) otherwise
provides a testing example for recursion control; the background relation succ(A,B) represents B=A+1. Finding the greatest common divisor of two numbers is another interesting
task; the background relation is plus. For these tasks the vocabulary consists of the integers
0 to 20 and 1 to 20 respectively, giving 51 tuples in Ackermann(A,B,C) such that A, B and
C are all less than or equal to 20, and 400 tuples in gcd(A,B,C).
As shown in Table 4, ffoil is about 60 times faster than foil when learning a definition
for Ackermann and about 200 times faster for gcd. This is due solely to ffoil's smaller
numbers of bindings. In gcd, for example, foil starts with 203 or 8,000 bindings whereas
ffoil never uses more than 400 bindings.
7. Diculties were experienced running Golem on an AXP 3000/900, so all times in this table are for a
DECstation 5000/260.

153

fiQuinlan

Both foil and ffoil learn exactly the same program for Ackermann's function that
mirrors the definition above. In the case of gcd, however, the definitions highlight the
potential simplification achievable with ordered clauses. The definition found by foil is
gcd(A,A,A).
gcd(A,B,C) :- plus(B,D,A), gcd(B,A,C).
gcd(A,B,C) :- plus(A,D,B), gcd(A,D,C).

while that learned by ffoil (omitting the default clause) is
gcd(A,A,A) :- !.
gcd(A,B,C) :- plus(A,D,B), gcd(A,D,C), !.
gcd(A,B,C) :- gcd(B,A,C), !.

The last clause exploits the fact that all cases in which A is less than or equal to B have
been filtered out by the first two clauses.

5.4 Finding the past tense of English verbs
The previous examples have all concerned tasks for which a compact, correct definition
is known to exist. This application, learning how to change an English verb in phonetic
notation from present to past tense, has more of a real-world avor in that any totally correct
definition would be extremely complex. A considerable literature has built up around this
task, starting in the connectionist community, moving to symbolic learning through the
work of Ling (1994), then to relational learning (Quinlan, 1994; Mooney and Califf, 1995).
Quinlan (1994) proposes representing this task as a relation past(A,B,C), interpreted as
the past tense of verb A is formed by stripping off the ending B and then adding string C. The
single background relation split(A,B,C) shows all ways in which word A can be split into two
non-empty substrings B and C. Following the experiment reported in (Ling, 1994), a corpus
of 1391 verbs is used to generate ten randomly-selected learning tasks, each containing 500
verbs from which a definition is learned and 500 different verbs used to test the definition.
A Prolog interpreter is used to evaluate the definitions learned by foil, each unseen word
w being mapped to a test query past(w,X,Y)?. The result of this query is judged correct
only when both X and Y are bound to the proper strings. If there are multiple responses to
the query, only the first is used { this disadvantages foil somewhat, since the system does
not attempt to reorder learned clauses for maximum accuracy on single-response queries.
The average accuracy of the definitions found by foil is 83.7%.
To apply ffoil to this task, the relation past(A,B,C) must be factored into two functional
relations delete(A,B) and add(A,C) since ffoil can currently learn only functions with a
single output variable. The same training and test sets of verbs are used, each giving rise
to two separate learning tasks, and a test is judged correct only when both delete and add
give the correct results for the unseen verb. The definitions learned by ffoil have a higher
average accuracy of 88.9%; on the ten trials, ffoil outperforms foil on nine and is inferior
on one, so the difference is significant at about the 1% level using a one-tailed sign test. The
average time required by ffoil to learn a pair of definitions, approximately 7.5 minutes, is
somewhat less than the time taken by foil to learn a single definition.
154

fiLearning First-Order Definitions of Functions

Object Edges
A
B
C
D
E
Total

54
42
28
57
96
277

Correct

Time (sec)

foil ffoil mfoil Golem fors foil ffoil

16
21
22
17
22
2.5
9
15
12
9
12
1.7
8
11
9
5
8
3.3
10
22
6
11
16
2.4
16
54
10
10
29
4.7
59
123
59
52
87 14.6
(21%) (44%) (21%) (19%) (31%)

9.1
11.0
9.7
11.1
5.9
46.8

Table 5: Cross-validation results for finite element mesh data.

5.5 Finite element mesh design
This application, first discussed by Dolsak and Muggleton (1992), concerns the division of
an object into an appropriate number of regions for finite element simulation. Each edge in
the object is cut into a number of intervals and the task is to learn to determine a suitable
number { too fine a division requires excessive computation in the simulation, while too
coarse a partitioning results in a poor approximation of the object's true behavior.
The data concern five objects with a total of 277 edges. The target relation mesh(A,B)
specifies for each edge A the number of intervals B recommended by an expert, ranging from
1 to 12. Thirty background relations describe properties of each edge, such as its shape and
its topological relationship to other edges in the object. Five trials are conducted, in each
of which all information about one object is withheld, a definition learned from the edges
in the remaining objects, and this definition tested on the edges in the omitted object.
Table 5 shows, for each trial, the number of edges on which the definitions learned by
foil and ffoil predict the number of intervals specified by the expert. Table 5 also shows
published results on the mesh task for three other relational learning systems. The numbers
of edges for which mfoil and Golem predict the correct number of intervals are taken from
(Lavrac and Dzeroski, 1994). These are both general relational learning systems like foil,
but fors (Karalic, 1995), like ffoil, is specialized for learning functional relations of this
kind. Since the general relational learning systems could return multiple answers to the
query mesh(e,X)? for edge e, only the first answer is used; this puts them at a disadvantage
with respect to foil and fors and accounts at least in part for their lower accuracy. Using
a one-tailed sign test at the 5% level, ffoil's accuracy is significantly higher than that
achieved by foil and Golem, but no other differences are significant.
The time required by ffoil for this domain is approximately three times that used by
foil. This turnabout is caused by ffoil's global pruning phase, which requires many literal
eliminations in order to maximize overall accuracy on the training data. In one ply of the
cross-validation, for instance, the initial definition, consisting of 30 clauses containing 64
body literals, fails to cover 146 of the 249 given tuples in the target relation mesh. After
global pruning, however, the final definition has just 9 clauses with 15 body literals, and
makes 101 errors on the training data.
155

fiQuinlan

6. Related Research

Mooney and Califf's (1995) recent system Foidl has had a strong inuence on the development of ffoil. Three features that together distinguish Foidl from earlier systems like
foil are:

 Following the example of focl (Pazzani and Kibler, 1992), background relations

are defined intensionally by programs rather than extensionally as tuple sets. This
eliminates a problem in some applications for which a complete extensional definition
of the background relations would be impossibly large.

 Examples of tuples that do not belong to the target relation are not needed. Instead,
each argument of the target relation has a mode as above and Foidl assumes output
completeness, i.e., the tuples in the relation show all valid outputs for any inputs that
appear.

 The learned definition is ordered and every clause ends with a cut.
Output completeness is a weaker restriction than functionality since there may be several
correct answers to a standard query R(c1 ; c2 ; :::; cn,1 ; X )?. However, the fact that each
clause ends with a cut reduces this exibility somewhat, since all answers to a query must
be generated by a single clause.
Although Foidl and ffoil both learn ordered clauses with cuts, they do so in very
different ways. ffoil learns a clause, then a sequence of clauses to cover the remaining
tuples, so that the first clause in the definition is the first clause learned. Foidl instead
follows Webb and Brkic (1993) in learning the last clause first, then prepending a sequence
of clauses to filter out all exceptions to the learned clause. This strategy has the advantage
that general rules can be learned first and still act as defaults to clauses that cover more
specialized situations.
The principal differences between Foidl and ffoil are thus the use of intensional versus
extensional background knowledge and the order in which clauses are learned. There are
other subsidiary differences { for example, Foidl never manipulates 	 bindings explicitly
but estimates their number syntactically. However, in many ways ffoil may be viewed as
an intermediate system lying mid-way between foil and Foidl.
Foidl was motivated by the past tense task described in Section 5.4, and performs
extremely well on it. The formulation of the task for Foidl uses the relation past(A,B)
to indicate that B is the past tense of verb A, together with the intensional background
relation split(S,H,T) to denote all possible ways of dividing string S into substrings H and
T. Definitions learned by Foidl are compact and intelligible, and have a slightly higher
accuracy (89.3%) than ffoil's using the same ten sets of training and test examples. It
will be interesting to see how the systems compare in other applications.
Bergadano and Gunetti (1993) first pointed out the advantages for learning systems of
restricting relations to functions. Their filp system assumes that all relations, both target
and background, are functional, although they allow functions with multiple outputs. This
assumption greatly reduces the number of literals considered when specializing a clause,
leading to shorter learning times. (On the other hand, many of the tasks discussed in the
previous section involve non-functional background relations and so would not satisfy filp's
156

fiLearning First-Order Definitions of Functions

functionality assumption.) In theory, filp also requires an oracle to answer non-ground
queries regarding unspecified tuples in the target and background relations, although this
would not be required if all relevant tuples were provided initially. filp guarantees that the
learned definition is completely consistent with the given examples, and so is inappropriate
for noisy domains such as those discussed in Sections 5.4 and 5.5.
In contrast to ffoil and Foidl, the definitions learned by filp consist of unordered
sets of clauses, despite the fact that the target relation is known to be functional. This
prevents a clause from exploiting the context established by earlier clauses. In the gcd task
(Section 5.3), a definition learned by filp would require the bodies of both the second and
third clauses to include a literal plus(...,...,...). In domains such as the past tense task, the
complexity of definitions learned by ffoil and Foidl would be greatly increased if they
were constrained to unordered clauses.

7. Conclusion
In this study, a mature relational learning system has been modified to customize it for
functional relations. The fact that the specialized ffoil performs so much better than
the more general foil on relations of this kind lends support to Bergadano and Gunetti's
(1993) thesis that functional relations are easier to learn. It is interesting to speculate that
a similar improvement might well be obtainable by customizing other general first-order
systems such as Progol (Muggleton, 1995) for learning functional relations.
Results from the quicksort experiments suggest that ffoil scales better than general
first-order systems when learning functional relations, and those from the past tense and
mesh design experiments demonstrate its effectiveness in noisy domains.
Nevertheless, it is hoped to improve ffoil in several ways. The system should be
extended to multifunctions with more than one output variable, as permitted by both
filp and Foidl. Secondly, many real-world tasks such as those of Sections 5.4 and 5.5
result in definitions in which the output variable is usually bound by being equated to a
constant rather than by appearing in a body literal. In such applications, ffoil is heavily
biased towards constructing the next clause to cover the most frequent function value in
the remaining tuples, as this binding tends to have the highest gain. By the time that the
clause has been specialized to exclude exceptions, however, it can end up covering just a
few tuples of the relation. If a few special cases could be filtered out first, clauses like this
would be simpler and would cover more tuples of the target relation. A better learning
strategy in these situations would seem to be to grow a new clause for every function value
in the uncovered tuples, then retain the one with greatest coverage and discard the rest.
This would involve an increase in computation but should lead to better, more concise
definitions.
Although the conceptual changes in moving from foil to ffoil are relatively slight,
their effects at the code level are substantial (with only three of the 19 files that make up
foil escaping modification). As a result it has been decided to preserve them as separate
systems, rather than incorporating ffoil as an option in foil. Both are available (for
academic research purposes) by anonymous ftp from ftp.cs.su.oz.au, directory pub, file names
foil6.sh and ffoil2.sh.
157

fiQuinlan

Acknowledgements
This research was made possible by a grant from the Australian Research Council. Thanks
to William Cohen, Ray Mooney, Michael Pazzani, and the anonymous reviewers for comments that helped to improve this paper.

Appendix: Learned Definitions
The definition learned by foil appears on the left and that by ffoil on the right. As the
latter's default clauses are irrelevant for these tasks, they are omitted.

List processing functions (Section 5.1)
(a) Using lists of length 3:
append([ ],B,B).
append(A,B,C) :- components(A,D,E),
components(C,D,F), append(E,B,F).
last(A,B) :- components(A,B,[ ]).
last(A,B) :- components(A,C,D), last(D,B).
reverse(A,A) :- append(A,C,D),
components(D,E,A).
reverse(A,B) :- last(A,C), last(B,D),
components(A,D,E),
components(B,C,F), reverse(E,G),
del(D,B,G).
shift(A,B) :- components(A,C,D), del(C,B,D),
append(D,E,B).
translate([ ],[ ]).
translate(A,B) :- components(A,C,D),
components(B,E,F), translate(D,F),
means(C,E).

append([ ],B,B) :- !.
append(A,B,C) :- components(A,D,E),
append(E,B,F), components(C,D,F), !.
last(A,B) :- components(A,C,D), last(D,B), !.
last(A,B) :- member(B,A), !.
reverse(A,A) :- append(A,C,D),
components(D,E,A), !.
reverse(A,B) :- components(A,C,D),
reverse(D,E), append(F,D,A),
append(E,F,B).

append([ ],B,B).
append(A,B,C) :- components(A,D,E),
components(C,D,F), append(E,B,F).
last(A,B) :- components(A,B,[ ]).
last(A,B) :- components(A,C,D), last(D,B).
reverse(A,A) :- append(A,A,C), del(D,E,C).
reverse(A,B) :- components(A,C,D),
reverse(D,E), append(F,D,A),
append(E,F,B).

append([ ],B,B) :- !.
append(A,B,C) :- components(A,D,E),
append(E,B,F), components(C,D,F), !.
last(A,B) :- components(A,C,D), last(D,B), !.
last(A,B) :- member(B,A), !.
reverse(A,A) :- append(A,C,D),
components(D,E,A), !.
reverse(A,B) :- components(A,C,D),
reverse(D,E), append(F,D,A),
append(E,F,B).
shift(A,B) :- components(A,C,D),
append(E,D,A), append(D,E,B).

shift(A,B) :- components(A,C,D),
append(E,D,A), append(D,E,B).
translate([ ],[ ]) :- !.
translate(A,B) :- components(A,C,D),
translate(D,E), means(C,F),
components(B,F,E).

(b) Using lists of length 4:

shift(A,B) :- components(A,C,D), del(C,B,D),
append(D,E,B).

158

fiLearning First-Order Definitions of Functions

translate([ ],[ ]).
translate(A,B) :- components(A,C,D),
components(B,E,F), translate(D,F),
means(C,E).

translate([ ],[ ]) :- !.
translate(A,B) :- components(A,C,D),
translate(D,E), means(C,F),
components(B,F,E).

Quicksort and bubblesort (Section 5.2)
qsort([ ],[ ]).
qsort(A,B) :- components(A,C,D),
partition(C,D,E,F), qsort(E,G),
qsort(F,H), components(I,C,H),
append(G,I,B).
bsort([ ],[ ]).
bsort(A,A) :- components(A,C,[ ]).
bsort(A,B) :- components(A,C,D),
components(B,C,E), bsort(D,E),
components(E,F,G), lt(C,F).
bsort(A,B) :- components(A,C,D),
components(B,E,F), bsort(D,G),
components(G,E,H), lt(E,C),
components(I,C,H), bsort(I,F).

qsort([ ],[ ]) :- !.
qsort(A,B) :- components(A,C,D),
partition(C,D,E,F), qsort(E,G),
qsort(F,H), components(I,C,H),
append(G,I,B), !.
bsort([ ],[ ]) :- !.
bsort(A,A) :- components(A,C,[ ]), !.
bsort(A,B) :- components(A,C,D), bsort(D,E),
components(E,F,G),
components(B,C,E), lt(C,F), !.
bsort(A,B) :- components(A,C,D), bsort(D,E),
components(E,F,G),
components(D,H,I),
components(J,C,I), bsort(J,K),
components(B,F,K), !.
bsort(A,B) :- components(A,C,D), bsort(D,E),
components(F,C,E), bsort(F,B), !.

Arithmetic functions (Section 5.3)
Ackermann(0,B,C) :- succ(B,C).
Ackermann(A,0,C) :- succ(D,A),
Ackermann(D,1,C).
Ackermann(A,B,C) :- succ(D,A), succ(E,B),
Ackermann(A,E,F),
Ackermann(D,F,C).
gcd(A,A,A).
gcd(A,B,C) :- plus(B,D,A), gcd(B,A,C).
gcd(A,B,C) :- plus(A,D,B), gcd(A,D,C).

Ackermann(0,B,C) :- succ(B,C), !.
Ackermann(A,0,C) :- succ(0,D), succ(E,A),
Ackermann(E,D,C), !.
Ackermann(A,B,C) :- succ(D,A), succ(E,B),
Ackermann(A,E,F),
Ackermann(D,F,C), !.
gcd(A,A,A) :- !.
gcd(A,B,C) :- plus(A,D,B), gcd(A,D,C), !.
gcd(A,B,C) :- gcd(B,A,C), !.

References

Bell, S., & Weber, S. (1993). On the close logical relationship between foil and the frameworks of Helft and Plotkin. In Proceedings Third International Workshop on Inductive
Logic Programming, Bled, Slovenia, pp. 127{147.
Bergadano, F., & Gunetti, D. (1993). An interactive system to learn functional logic programs. In Proceedings Thirteenth International Joint Conference on Artificial Intelligence, Chambery, France, pp. 1044{1049. San Francisco: Morgan Kaufmann.
Bratko, I. (1990). Prolog Programming for Artificial Intelligence (2nd edition). Wokingham,
UK: Addison-Wesley.
159

fiQuinlan

Cameron-Jones, R. M., & Quinlan, J. R. (1994). Ecient top-down induction of logic
programs. SIGART, 5, 33{42.
De Raedt, L. (Ed.). (1996). Advances in Inductive Logic Programming. Amsterdam: IOS
Press.
Dolsak, B., & Muggleton, S. (1992). The application of inductive logic programming to
finite element mesh design. In Muggleton, S. (Ed.), Inductive Logic Programming, pp.
453{472. London: Academic Press.
Karalic, A. (1995). First Order Regression. Ph.D. thesis, Faculty of Electrical Engineering
and Computer Science, University of Ljubljana, Slovenia.
Langley, P., & Simon, H. A. (1995). Applications of machine learning and rule induction.
Communications of the ACM, 38 (11), 55{64.
Lavrac, N., & Dzeroski, S. (1994). Inductive Logic Programming. London: Ellis Horwood.
Ling, C. X. (1994). Learning the past tense of english verbs: the symbolic pattern associator
versus connectionist models. Journal of Artificial Intelligence Research, 1, 209{229.
Michie, D., Spiegelhalter, D. J., & Taylor, C. C. (Eds.). (1994). Machine Learning, Neural
and Statistical Classification. Hertfordshire, UK: Ellis Horwood.
Mooney, R. J., & Califf, M. E. (1995). Induction of first-order decision lists: results on
learning the past tense of english verbs. Journal of Artificial Intelligence Research, 3,
1{24.
Muggleton, S. (Ed.). (1992). Inductive Logic Programming. London: Academic Press.
Muggleton, S. (1995). Inverse entailment and progol. New Generation Computing, 13,
245{286.
Muggleton, S., & Feng, C. (1992). Ecient induction of logic programs. In Muggleton, S.
(Ed.), Inductive Logic Programming, pp. 281{298. London: Academic Press.
Muggleton, S., King, R. D., & Sternberg, M. J. (1992). Protein secondary structure prediction using logic-based machine learning. Protein Engineering, 5, 646{657.
Pazzani, M. J., & Kibler, D. (1992). The utility of knowledge in inductive learning. Machine
Learning, 9, 57{94.
Quinlan, J. R. (1990). Learning logical definitions from relations. Machine Learning, 5,
239{266.
Quinlan, J. R. (1991). Determinate literals in inductive logic programming. In Proceedings
Twelfth International Joint Conference on Artificial Intelligence, Sydney, pp. 746{750.
San Francisco: Morgan Kaufmann.
Quinlan, J. R. (1994). Past tenses of verbs and first-order learning. In Proceedings AI'94
Seventh Australian Joint Conference on Artificial Intelligence, Armidale, Australia,
pp. 13{20. Singapore: World Scientific.
160

fiLearning First-Order Definitions of Functions

Quinlan, J. R., & Cameron-Jones, R. M. (1993). Foil: a midterm report. In Proceedings European Conference on Machine Learning, Vienna, pp. 3{20. Berlin: Springer-Verlag.
Quinlan, J. R., & Cameron-Jones, R. M. (1995). Induction of logic programs: foil and
related systems. New Generation Computing, 13, 287{312.
Rouveirol, C. (1994). Flattening and saturation: two representation changes for generalization. Machine Learning, 14, 219{232.
Srinivasan, A., Muggleton, S. H., Sternberg, M. J. E., & King, R. D. (1996). Theories for
mutagenicity: a study in first-order and feature-based induction. Artificial Intelligence, 84, 277{299.
Ullman, J. D. (1988). Principles of Database and Knowledge-Base Systems. Rockville, MD:
Computer Science Press.
Webb, G. I., & Brkic, N. (1993). Learning decision lists by prepending inferred rules. In Proceedings Australian Workshop on Machine Learning and Hybrid Systems, Melbourne,
Australia, pp. 6{10.
Zelle, J. M., & Mooney, R. J. (1993). Combining foil and ebg to speed-up logic programs.
In Proceedings Thirteenth International Joint Conference on Artificial Intelligence,
Chambery, France, pp. 1106{1111. San Francisco: Morgan Kaufmann.

161

fiJournal of Artificial Intelligence Research 5 (1996) 329349

Submitted 5/96; published 12/96

Quantitative Results Comparing Three Intelligent Interfaces for
Information Capture: A Case Study Adding Name Information into an
Electronic Personal Organizer
Jeffrey C. Schlimmer
School of Electrical Engineering & Computer Science
Washington State University, Pullman, WA 99164-2752, U.S.A.
Patricia Crane Wells
AllPen Software, Inc.
16795 Lark Avenue, Suite 200, Los Gatos, CA 95030, U.S.A.

SCHLIMME@EECS.WSU.EDU

PATRICIA@ALLPEN.COM

Abstract
Efficiently entering information into a computer is key to enjoying the benefits of
computing. This paper describes three intelligent user interfaces: handwriting recognition,
adaptive menus, and predictive fillin. In the context of adding a persons name and address
to an electronic organizer, tests show handwriting recognition is slower than typing on an
on-screen, soft keyboard, while adaptive menus and predictive fillin can be twice as fast.
This paper also presents strategies for applying these three interfaces to other information
collection domains.

1. Introduction
When you meet someone new, you often wish to get their name and phone number. You may
write this in a small notebook or personal organizer. This takes a few minutes to do, so you
put their business card or a small slip of paper in your organizer, promising to copy it over at
a later time.1 When that later time comes, you face the tedious task of finding where the nowseveral names should go in your organizer and recopying the information. If you are
comfortable with computers, you may use an electronic organizer (a small computer that
includes software for managing names and appointments). Looking up someones phone
number is faster with these devices, but adding is more tedious, and owning is more costly.
As a concession to reality, these devices often include pockets for holding queued slips of
paper.
What solutions could we propose to eliminate this procrastination? If adding a persons
name2 to your organizer were fun (say with your choice of inspirational message, gratuitous
violence, or a lottery ticket), you might add names more readily. Avoiding this whimsy, we
could get the desired effect by just making it faster to add a persons name. For this reason
paper organizers use index tabs; electronic organizers use automatic filing. To be faster still,
an organizer could read a card or handwritten note (via optical character or handwriting

1. We have a friend who places a rubber band around his organizer to ensure that these paper slips dont escape
before their time.
2. For brevity, well refer to a persons name, address, phone numbers, e-mail, etc. as a name. Whether the
aggregate information or just a persons first or last name is intended should be clear from context.

 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiS CHLIMMER & W ELLS

recognition). Applying artificial intelligence ideas, we could even imagine an organizer that
predicts what you need to write and does it for you.
This paper describes an electronic organizer that is almost as fast as a slip of paper, and
certainly much faster than previous organizers. It uses commercial hardware (Newton,
described in Section 2). Its software has three interface components designed to speed
adding a persons name (described in Section 3): handwriting recognition, adaptive menus
with recent values, and predictive fillin. The primary contributions of this paper are detailed
evaluations of the benefits of these three components (described in Section 4).
Adding a persons name into an organizer is a special case of capturing and organizing
information. This is a ubiquitous task. Big businesses institute careful procedures with
custom forms and databases, but there are billions of smaller, one-or-two-person tasks which
could be done more efficiently and accurately if getting information into a computer were
easier. Even small gains would be repeated many times over whenever someone needed to
collect information to make a decision, monitor a process, or investigate something new. The
secondary contributions of this paper are a consideration of how the three components may
be applied more broadly (described in Section 5).
These three interface components are robust if familiar. Whether they are intelligent is
arguable. Some advocate a behavior-based definition which may also apply here, i.e., that the
question of whether a device is intelligence or not should be answered by examining its
behavior rather than its internal processes and representations (e.g., Agre & Chapman, 1987;
Horswill & Brooks, 1988). Even if it does not, our goal is to address the question of how
much intelligence, agency, or support one wants in an interface (Lee, 1990; Rissland, 1984).
We assert: as much as will speed the users performance of the task. Furthermore, much
research is directed at automatically learning what we will hard-code in this study (e.g.,
Dent, Boticario, McDermott, Mitchell, & Zabowski, 1992; Hermens & Schlimmer, 1994;
Schlimmer & Hermens, 1993; Yoshida, 1994). Even if learning works perfectly, is the result
worthwhile? We claim the answer can be found in an empirical study of the usefulness of
various user interface components.

2. Newton
Newton is an operating system introduced by Apple Computer, Inc. in 1993. It was designed
for a single-user, highly-portable computer. Frames are the central data structure in Newton.
They are stored in persistent object databases maintained in RAM (Smith, 1994).
Each Newton computer includes a pressure-sensitive, bitmapped display on which the
user writes, draws, or taps to enter information (Culbert, 1994). All are small enough to hold
in one hand and weigh around one US pound. Battery life is about one days worth of
continuous use. For a thorough overview of the hardware and software context of current pen
computers, the reader may wish to consult (Meyer, 1995).
From the very lowest levels, Newton supports recognition. Its handwriting recognition
was highly publicized when first introduced. The recognizer allows free-form input of
printed and cursive writing.3 It uses on-line recognition to convert writing to Unicode4 text.
The recognizer uses contextual information to limit both types of characters within specific
fields and combinations of characters within words. The latter relies heavily on a
3. Throughout this paper references to handwriting also refer to handprinting. Where a distinction is required, the
latter term will be used explicitly.
4. A character encoding similar to ASCII but with two-bytes per character for languages with larger character
sets.

330

fiT HREE I NTERFACES FOR I NORMATION C APTURE: A C ASE S TUDY A DDING N AME I NFORMATION

dictionaryonly words appearing in the dictionary can be recognized. If the user types a
new word using an on-screen soft keyboard,5 Newton volunteers to add it to the dictionary
for future recognition. Optionally, the user can invoke a secondary recognizer that does not
use a dictionary and attempts to recognize what is written letter by letter; Section 4
describes the accuracy of this option. Application developers can customize handwriting
recognition by providing special purpose dictionaries or a regular expression describing the
syntax of words to be recognized.
Most Newton computers include several applications in ROM so they can serve as an
electronic organizer. Relevant to the point of this paper, all Newton computers to date
include an application called Names for storing and retrieving peoples names, addresses,
etc.; Figure 1 depicts this application. Section 4 describes experiments using a standard and
enhanced version of the Names application to add peoples names.

Figure 1: Names application included with all Newton computers depicted at one
quarter life size of the Apple Newton MessagePad 100 used in the experiments. As
the user taps on a field, it expands to ease writing. In this picture, the First Name field
is expanded. The folder tab button at the top of the screen is for displaying names from
one of eleven user-defined folders. From left to right, buttons at the bottom of the
application screen are for showing the time and battery state (labeled with a clock
face), changing the display of this name (labeled Show), adding a new name
(labeled New), refiling this name (labeled with the file folder picture), printing/
faxing/infrared beaming/mailing/duplicating/deleting this name (labeled with the
envelope picture), and closing the application (labeled with a large X). Below these
are universal buttons visible in all applications. From left to right, they provide access
to the Names application, a calendar application, a storage place for all other
applications, scrolling buttons, undo, find, and natural language recognition.

3. Names++
A Newton computers built-in Names application includes one of the three components
suggested by Section 1 to speed adding a new persons name. It recognizes handwriting, and
5. Throughout this paper references to typing refer to tapping on an on-screen soft keyboard.

331

fiS CHLIMMER & W ELLS

the recognition dictionary can be expanded as needed. Names++ is an extended version of
Names which we wrote to include the other two components.
3.1

Adaptive Menus

Names++ extends Names by adding an adaptive menu to 9 of Names 17 fields: 7 menus
consisting of 4 recently entered values and 2 menus with 4 recently entered values prepended
to fixed choices. If the word the user needs is in a menu, they can choose it rather than write
it out. Figure 2 depicts Names++ with a menu open for the City field. The choices in the

Figure 2: Names++ application. In this picture, the user has tapped on the word
City and opened a menu of recently used city names. If the user chooses one of
these cities, it will be copied into the City field for this name. Compare this to
Figure 1. Note that Names++ includes all features of Names relevant to adding a new
name.
menu are the four most recently entered values for this specific field. (Each field has a
separate menu.) This may be the most convenient when the user has a series of related names
to add, perhaps for people from the same company or city. Of course, when the user adds
four unusual values in a row, common choices are inadvertently dropped from the menu. A
more sophisticated approach would list some number of the most recent values and some
number of the most common; Names++ doesnt explore this for the sake of simplicity and
speed. Because these menus are adaptive, users will have to use linear search to examine
choices and cannot rely on muscle-level memory of choice locations. If the menus include
more than a few choices, the cost of this search will likely dominate any Fitts law effect.6
Two of Names fields already had a menu. The Honorific field offered the user Ms.,
Mrs., Mr., and Dr.; the Country field offered a menu of thirteen countries. For
completeness, Names++ prepends the four most recent values to these fields menus.
Technically these are split menus.7 Mitchell and Shneiderman (1989) compared large
statically ordered menus (unsplit) to those that also prepended most-recently-used choices
6. Fitts law states that the time to move a given distance D to a target of width W is proportional to the log of D/
W.
7. Not to be confused with splitting menu choices across multiple menus in (Witten, Cleary, & Greenberg, 1984).

332

fiT HREE I NTERFACES FOR I NORMATION C APTURE: A C ASE S TUDY A DDING N AME I NFORMATION

(split, exactly our condition). Static were faster than split menus on one task; there was no
difference on another. Sears and Shneiderman (1994) later found evidence in favor of split
menus including a 1758% improvement in selection time compared to unsplit menus. They
also compared alternative organizations for the split part and recommend limiting the
number of split choices to four or less (which Names++ does) and sorting split choices by
frequency (which Names++ approximates by most-recently-used). In the context of menu
hierarchies, Snowberry, Parkinson, and Sisson (1985) found that adding items containing
upcoming selections resulted in greater accuracy and faster search times. This result has not
been confirmed (Kreigh, Pesot, and Halcomb, 1990) and may be as much an effect of
preventing users from getting lost in menu hierarchies as of assisting them in making
selections per se. We test adaptive (split) menus in Names++ to understand the relative
contribution of such compared to other interfaces in a data entry task.
The four Phone Number fields have menus, but these give the user a way to categorize
the phone number rather than enter the number itself. They are category menus (Norman,
1991). The phone menus include choices for Phone, Home, Work, Fax, Car,
Beeper, Mobile, and Other. All phone fields have identical menus. Names++ does not
modify them. No menus were provided for the First Name, Last Name, and Birthday fields.
Section 5 describes which input fields should have menus.
To understand the computational space and time demands of adaptive menus, note that
Names++ stores all menus in a single object in its own object database. The size of the object
is linear in the number of fields with menus (f) and in the number of choices in each menu
(c), or fc. If each menu were implemented as a circular queue, the time to update the object
would be constant for each menu, or just f. Names++ uses a slightly slower array
implementation for menus and takes fc time. In practice this works out to slightly more than
one half second for nine fields and four choices.
3.2

Predictive Fillin

Names++ also extends Names by automatically filling up to 11 empty fields in a new name
with predicted values. It treats previous names as a case base (Kolodner, 1993) and copies
information from a relevant case. Specifically, when the user adds a company to a new name
that matches a previous names company, Names++ copies most of the address from the
previous name into the new one. Values are copied verbatim from the two address lines and
the City, State, Zip Code, and Country fields. The user ID from the electronic mail address is
dropped before the e-mail address is copied into the new name. (The remaining components
of the e-mail address are likely to be the same for people from the same company.) The last
word of Phone Number values is dropped; only the area code and prefix are copied into the
new name for a typical area code-prefix-extension phone number. If the user writes or
chooses another value for Company, replacing the the value of that field, predictive fillin
recopies dependent values from a previous name. Figure 3 illustrates the sequence of events
from the users perspective.
Names++ behaves similarly when the user adds a city or state that matches a previous
name, but it copies over less information than when a matching company is found. If a value
copied by predictive fillin is incorrect, the user can write or choose the correct value
manually. Table 1 summarizes which fields have menus and predictive fillin. The structure of
predictive fillin is fixed in the design of Names++. Other work attempts to learn comparable
structure from examples (e.g., Dent et al., 1992; Hermens & Schlimmer, 1994; Schlimmer &

333

fiS CHLIMMER & W ELLS

Figure 3: Names++ application after the user adds a company for a new name. In the
left panel, the application finds a previous name with a matching company, displays
a dialog, and fills remaining fields with predicted information copied from the
previous name. The center panel shows how much information was filled. The right
panel shows the completed name. In this example the user has written only four
additional words to complete the name.
Hermens, 1993; Yoshida, 1994). Our goal is to determine whether the end result of such
learning is worthwhile.
If more than one previous name matches the company, city, or state of the new name,
Names++ fills fields with values from the most recent name. Values from the second-most
recent occurrence of that name are added to menus. This gives the user a chance to select
between alternate addresses for the same company or alternate zip codes for the same city.
In terms of computational requirements, Names++ needs no additional storage for
predictive fillin; the object database of previously added names is reused as a case base.
Matching a new names company, city, or state to a previous name is implemented with a
Newton primitive; an informal study depicted in Figure 4 indicates that Newtons proprietary
algorithm appears to run in time linear in the number of names if no match is found and
logarithmic if matches are found.
Names++ and its source code is in on-line Appendix A.

4. Experiments
We hypothesize that recognition, adaptive menus, and predictive fillin speed adding a new
name. To find out if this is so and to what extent, we conducted an experiment in which
subjects added names using different combinations of the three interface components.
4.1

Method

Five computer science students between the ages of 18 and 35 years of age participated as
subjects in the experiments. Prior to the experiments they had used a Newton computer for at
least six months and were familiar with Newtons handwriting recognition and QWERTY
layout of Newtons on-screen keyboard.
The experiment used a within-subject design where each subject participated in each of
six conditions summarized in Table 2. Conditions were designed to assess the contribution of
334

fiTime in Seconds

T HREE I NTERFACES FOR I NORMATION C APTURE: A C ASE S TUDY A DDING N AME I NFORMATION

J

5.2

J
1.6

BJ
BJ
0.1 B
JBJ
0.7

10

E

J
BG

B

B
640
Number of Names

200

Figure 4: Time to find a matching name using Newton as a function of the number of
names in the database if no match exists (circles and upper line) and if several matches
exist (squares and lower line). Both axes are linear scale. The upper line is a linear fit;
the lower line is logarithmic. For comparison to the experiment, interpolated values
for 200 names are shown with open symbols.
each interface component separately and collectively. In the control, Typed condition, the
subject types all values without using any of the components. In the Null condition, the
subject writes all words using remedial recognition steps (to be described) and types words
only if they are not recognizable. The subject does not add words to Newtons dictionary
when asked and does not have the assistance of either adaptive menus or predictive fillin. The
D condition extends Null by requiring the subject to add words to Newtons dictionary when
asked. The AM condition extends Null by adding adaptive menus. The PF condition extends
Null by adding predictive fillin. The All condition combines the extensions of D, AM, and
PF.
We used a pair of Apple Newton MessagePad 100 computers (running Newton OS
version 1.3) for the experiment and three versions of the Names++ application. One version
has all interface components disabled and was used for the Typed, Null, and D conditions. A
second version has adaptive menus and was used for AM. A third version has adaptive menus
and predictive fillin and was used for PF and All.
A set of 448 name records for the experiments was donated by a development officer
from Washington State University. Her job involves contacting alumni and others to solicit
support for university programs. Almost all of the records include a first and last name, a full
mailing address, and one to three phone numbers. Few include an honorific, country, or email address. Informal tests indicated that the MessagePads could hold about 250 names
after Names++ was installed, so we selected a random set of 200 of these records.

335

fiS CHLIMMER & W ELLS

Menu Choices
Field
Honorific

Predictive Fillin by

Built-in Adaptive
4

Company

City

State

Notes
Ms., Mrs., Mr., Dr.

4

First Name
Last Name
Title

4

Company

4

Address (1)

4

Address (2)

Yes
No label to tap for menu.

Yes

City

4

Yes

State

4

Yes

Yes

Zip Code

4

Yes

Yes

4

Yes

Yes

4

Yes

Country

13

E-Mail
Phone 1

8

Phone 2

8

Phone 3

8

Phone 4

8

Area Code
and Prefix

Yes
User ID is removed.
Category menu is used to select
type of phone number rather than
the phone number itself. Choices
include Phone, Home,
Work, Fax, Car, Beeper,
Mobile, and Other.

Area Code

Birthdate

Table 1: Name++ fields with adaptive menus and predictive fillin.
Condition

Writing

Add to Dictionary

Adaptive Menus

Predictive Fillin

Typed
Null

Yes

D

Yes

AM

Yes

PF

Yes

All

Yes

Yes
Yes
Yes
Yes

Yes

Yes

Table 2: Experimental conditions, one row per condition. Columns indicate which
user interface components were used. Blank cells represent No.
To simulate a worst case for recognition, adaptive menus, and predictive fillin, we chose
5 names (listed below) from the residual 248 such that each names company was not in the
preload set of 200 names. (To preserve anonymity here, first and last names are swapped and
phone numbers replaced with artificial values. Actual first and last name pairs and phone
numbers were used in the experiment.)
336

fiT HREE I NTERFACES FOR I NORMATION C APTURE: A C ASE S TUDY A DDING N AME I NFORMATION

Robert Anderson
Account Marketing Rep
IBM
W 201 N River Drive
Spokane, WA 99201
509 555 0000
509 555 1111

Eric Brice
Director of Engineering
RAIMA Corp
3245 146th Place SE
Bellevue, WA 98007
206 555 2222
206 555 3333
205 555 4444

Peter Friedman
President
NOVA Information Systems
12277 134th Court NE
Suite 203
Redmond, WA 98052
206 555 7777

Thomas Leland
Staffing Manager
Aldus Corporation
411 First Ave South
Seattle WA 98104 2871
206 555 8888
206 555 9999

Mike Carlson
VP Engineering & Estimating
General Construction
2111 N Northgate Way
Suite 305
Seattle, WA 98133
206 555 5555
206 555 6666

To score how words in names were entered and the total time, we used the sheet in
Figure 5. Fictitious data corresponding to a subjects entering of the second name in the All
condition is also depicted.

Figure 5: Scoring sheet used each time a name was added. A 1 in the center to right
columns indicates how the first word of a field value was entered using recognition
(cf. Figure 6), an adaptive menu (cf. Figure 2), or predictive fillin (cf. Figure 3). A 2
indicates the second word, and so on. The highest digit in a row corresponds to the
number of words in that fields value.
To facilitate setting up each condition, we constructed backup images of MessagePads
correctly configured for each of the six conditions. For all images, the 200 names and the
appropriate version of Names++ was installed. To the images for D and All, we added all
First, Last, and Company names to its dictionary using a built-in feature of Newton. To
initialize the adaptive menus in the images for AM and All, we used a special purpose
application. Prior to each use the MessagePads were completely erased and then restored
from the backup image appropriate to the condition to be tested.

337

fiS CHLIMMER & W ELLS

The task of the subject was to enter each of the five names twice in each of the six
conditions. The first time a name is entered in a condition simulates a worst-case scenario;
the second time, a best.
4.2

Procedure

Subjects were given a listing of one of the five names and a MessagePad initialized for one of
the six experimental conditions. Each subject entered each name in each condition; name/
condition pairs were randomly ordered for each subject to counteract subject learning
effects. They were instructed to enter names quickly. Subjects made few mistakes. They were
instructed to correct these before finishing. Times reported below include time to correct
mistakes.
Subjects were given a precise script to follow when entering a name. This was done
partially to bias results against the hypotheses and partially to minimize individual variation.
Specifically, the subject was instructed to enter values for each field in order, from top to
bottom, completing one before going to the next (cf. Figure 1). In conditions involving
handwriting, if a word was not correctly recognized, the subject was to check the menu of
alternate recognitions (depicted in the left panel of Figure 6). If the intended word was not in

Figure 6: Remedial steps when a handwritten word is not correctly recognized. In this
example, the subject wrote Brice which was misrecognized as Brian. When the
subject double-taps on the word, a menu of alternative recognitions appears (left
panel). If none of these are correct, the subject requests recognition without the
dictionary (or letter by letter). Another double-tap on the word generates a second
menu of alternatives (middle panel). If none of these are correct, the subject entered
the word by tapping on the buttons of an on-screen keyboard (right panel).
this list, they were to select Try letters which attempts recognition without the dictionary.
If the result of this was not correct, they were to check a second menu of alternative
recognitions (depicted in the center panel of Figure 6). If the intended word was not in this
second menu, they were to tap the button with the keyboard picture, type in the word using
the on-screen keyboard, and close the keyboard. If this word was not already part of the
dictionary, Newton asked if they would like to add it (depicted in the right panel of Figure 6).
Note that for each of the recognition menus, the original handwriting is shown near the
338

fiT HREE I NTERFACES FOR I NORMATION C APTURE: A C ASE S TUDY A DDING N AME I NFORMATION

bottom. The first choice is Newtons best guess, and the second choice is its best guess with
different capitalization. The subject was instructed to ensure that words were correctly
capitalized.
For Typed, the subject was instructed to enter all data using Newtons on-screen soft
keyboard. For Null, the subject was to enter all data by handwriting. For D and All, the
subject was instructed to add any words to Newtons dictionary if asked. For AM and All, the
subject was instructed to check a fields menu (if there was one) before writing any data. No
special instructions were required for PF beyond the default of not adding words to the
dictionary.
A stopwatch was started when a subject tapped the New button and stopped when the
last field value had been correctly entered. Choosing a manual timing method simplified
development of the experimental software. The method by which each word of each field was
entered was recorded on a scoring sheet as indicated in Figure 5.
The experiment took between three and five hours for each subject and was spread over
two or more sessions of approximately two hours within the same week. Subjects took short
breaks after adding each name to minimize fatigue.
After each subject completed the experiment, they were asked to rank their favorite
methods for entering names from most to least.
4.3

Results

Table 3 summarizes the median and standard deviation of the subjects time to enter a name
in the six conditions. Times include all user input, predictive fillin computation, and time to
correct errors (if any). The first row reports time to add a novel name, our simulation of a
worst case. The second row reports time to repeat the name, our simulation of a best case. An
ANOVA reveals a significant main effect for condition F(5, 21.07) < 0.001. The interaction
of number of times a name is entered and condition is also significant F(5, 19.61) < 0.001.
Comparing worst cases across conditions, a post-hoc multiple comparisons test using
Tukeys HSD indicates that only Typed is significantly different (faster) than other
conditions. (All p < 0.05.) Comparing worst to best cases within the same condition, D, AM,
PF, and All are significantly faster. Comparing best cases across conditions, Typed, AM, and
PF are significantly faster than D and Null; All is significantly faster than Typed, PF, D, and
Null. No other pairwise comparisons are significant.
Typed

Null

D

AM

PF

All

Worst

2.72 (0.86)

4.25 (1.31)

4.50 (1.45)

4.32 (1.70)

4.07 (1.26)

4.15 (1.13)

Best

2.52 (0.60)

3.65 (1.24)

3.30 (1.09)

1.37 (0.51)

2.02 (0.45)

1.08 (0.24)

Table 3: Median time in minutes to add a new name over five names and five subjects
(25 samples per cell, standard deviation in parentheses). Columns list six
experimental conditions.
The difference within D, AM, and PF across worst and best cases confirms our
hypothesis that these interfaces can speed entering names, by 29%, 210%, 110% compared
to Null, respectively. We were surprised to find that predictive fillin was not as fast as
adaptive menus (though the difference is not statistically significant). When designing a data
entry system one might be tempted to implement just adaptive menus given their algorithmic
simplicity, especially compared to sophisticated methods in machine learning that have been
proposed for predictive fillin. However, the latter do not suffer from recency effects imposed
339

fiS CHLIMMER & W ELLS

by the limited size of adaptive menus; when entering new data related to some in the distant
past, predictive fillin would have little difficulty providing assistance where adaptive menus
could not. Adaptive menus could be further refined to use a frequency or frequency-recency
combination, but the performance of All suggests implementing both adaptive menus and
predictive fillin. Combined with adding words to a dictionary, they can speed entering names
by 294%. In practical terms, these interfaces could make entering a name into an electronic
organizer faster than writing it down on paper and certainly fast enough to capture the
information during a phone conversation.
Prior work confirms the difference between the Typed and D conditions. Ward and
Blesser (1986) state that normal writing speed is rarely greater than 69 characters per minute
(cpm) for a single line of text. Using the fact that the mean number of characters per name in
our experiment is 98.2, our subjects achieved 30 cpm. MacKenzie, Nonnecke, Riddersma,
McQueen, and Meltz (1994) compare four interfaces for entering numeric and text data on
pen-based computers, including hand printing and using an on-screen keyboard. (The other
two interfaces were experimental gesture-based techniques for entering single characters.)
For numeric entry conditions, they found that the on-screen keyboard was 30 words per
minute (wpm) with 1.2% error whereas hand printing was 18.5 wpm with 10.4% error. For
text entry conditions, the keyboard was 23 wpm with 1.1% error whereas hand printing was
16 wpm with 8.1% error. Using the fact that the mean number of words per name in our
experiment is 20.8, our subjects achieved 8.3 wpm typing and 6.3 wpm handwriting for
mixed numeric/text input. The key point of comparison is that both their and our studies
found that using a stylus to tap an on-screen keyboard is faster than handwriting or printing.
Differences in speed between these studies and ours is likely a result of differences between
experimental procedures (theirs versus ours): single versus multiple field fillin, copying
information from memory or screen versus paper, and block or comb-type (letter) versus
open (word) interface.
Figure 7 presents Box plot summaries of the time data. Of interest is reduction in
variance of time by adaptive menus and predictive fillin in the best case (right plot).
Differences between individual performance is reduced by these interface components.
The left half of Table 4 lists the recognition accuracy for each field over all conditions,
subjects, and names. The first row indicates that 94% of the first names written were
correctly recognized immediately. By checking the first menu of alternate recognitions, that
accuracy rises to 95%. Similarly, the second row indicates that 59% of all second names
written were correctly recognized immediately. This rate rose to 74% when letter-by-letter
recognition was invoked and again to 79% by checking the second menu of alternate
recognitions. Phone numbers enjoyed the second highest recognition rate below first names.
For reference, Cesar and Shinghal (1990) report over 90% recognition rate on hand
printed, Canadian postal codes which are {letter, digit, letter, space, digit, letter, digit}. This
is comparable to our observed rates for first names, second address lines, and phone
numbers.
The right half of Table 4 lists the percentage of words entered using typing, adaptive
menus, or predictive fillin by field over all conditions, subjects, and names. The first row
indicates that 5% of first names were typed. The row for State indicates that 32% of state
names were typed, 20% were chosen from an adaptive menu, and 39% were predictively
filled in. (Note that the numbers in each row do not total to 100% because the left half of the
table lists percentages for words that were written while the right half lists percentages of all
words.)
340

fiTime in Minutes

T HREE I NTERFACES FOR I NORMATION C APTURE: A C ASE S TUDY A DDING N AME I NFORMATION

*
10

10
*
*
*
*
*

5

*

*

*

5
*

Worst Case

All

Fillin

Menu

Dictionary

Null

0

Typed

*

All

Fillin

Menu

Dictionary

Typed

0

Null

*

Best Case
Experimental Condition

Figure 7: Box plots of time to enter a name by condition in the worst and best cases.
Each box summarizes 25 values. Values outside the inner fences are plotted with
asterisks. Values outside the outer fences are plotted with circles (Wilkinson, Hill,
Vang, 1992).
Combining the left and right halves of Table 4 reveal that many of the difficult-torecognize fields have considerable assistance from adaptive menus and predictive fillin. This
accentuates the speed improvements by providing help where it is most needed. Figure 8
depicts the relationship between the fields, their recognition accuracy, and which have
adaptive menus or predictive fillin. Several fields have near perfect recognition accuracy;
they can be recognized without resorting to typing. For instance, numeric fields are easier to
recognize; the Phone Number fields were recognized at nearly 90% even though the area
code, prefix, and suffix varied from name to name. The First and Last name fields also had
high recognition accuracy. All of the first names were in the built-in dictionary. All but two
of the last names were, and the others were often recognized letter by letter. Recognition was
poorer in the Company and Address fields. Words in full capitals (e.g., RAIMA) and words
with a combination of numbers and letters (e.g., 146th) were difficult to recognize. The
low recognition accuracy of the State field is apparently due to an oversight in Newtons
dictionary. WA is not included but many other two-letter abbreviations for US states are.
To compensate for low accuracy, Names++ includes an adaptive menu and/or predictive fillin
for each of the difficult-to-recognize fields.
Table 5 summarizes subjects preference for condition to enter a name. It lists frequency
of ranking over the five subjects. Subjects partitioned conditions into non-overlapping
groups of (Typed, Null), (D, AM, PF), and (ALL). (The authors know of no suitable statistic
for asserting these differences.) These results contradict those of MacKenzie et al. (1994)
who found that subjects preferred typing to handwriting, mildly for text entry and more
341

fiS CHLIMMER & W ELLS

100

XFirst Name

GPhone

GAddress (2)
XLast Name
75

I Title

Recognition (%)

City

I Company

Address (1)

Zip Code
50

X
I
G

25
State

Recognition
Adaptive Menus
Predictive Fillin
Both

0
0

200

400

600
800
Number of Words

1000

1200

Figure 8: Recognition rate as a function of the number of total words entered in all
conditions by all subjects for all names. Fields with adaptive menus or predictive fillin
(or both) are marked. Note that every field with less than 75% accuracy has either an
adaptive menu or predictive fillin (or both).
strongly for numeric entry. They restricted hand printing input to block or comb-type
interface; this unnaturalness may account for some of the dispreference toward handwriting.
Writing with a stylus does have its advantages. As Meyer (1995) points out, keyboards are
faster for linear text entry, but a pen input device can be more natural, can handle text and
graphic input, and can jump quickly from point to point. Writing with a pen also supports
heads up writing, allowing the user to visually attend to other aspects of the task at hand.
Typing with an on-screen keyboard requires heads down entry.
One subject experimented with Names++ outside the experimental setting and offered a
number of observations. First, the adaptive menus were too short, and sometimes menus
would be useless no matter how long they were. She wished that the City and Company
fields menus were longer (especially City). It was frustrating to have one of the common
city names for a large metropolitan region bumped from the short list. In contrast, the Title
fields menu was rarely useful, and she did not see the point of maintaining it. The principles
to be outlined in Section 5 suggest similar revisions.
342

fiT HREE I NTERFACES FOR I NORMATION C APTURE: A C ASE S TUDY A DDING N AME I NFORMATION

Field

Cumulative Recognition Accuracy
Letter by
Correct
1st Menu
2nd Menu
Letter

Percent Words Entered
Adaptive Predictive
Typed
Menu
Fillin

First Name

94

95

95

95

5

Second
Name

59

59

74

80

21

Title

52

62

66

68

26

20

Company

42

49

59

61

31

20

Address

48

60

62

67

23

10

Address 2

81

85

87

87

10

City

62

62

67

71

19

12

20

State

22

22

22

22

32

20

39

Zip

51

52

58

59

29

10

20

Phone

86

89

89

89

10

20
20

15

Table 4: The left columns list cumulative recognition accuracy by field over all words
that were written in all conditions, names and subjects. The right columns list
percentage of all words by field entered by typing, adaptive menus, and predictive
fillin. 5190 values total. Blank cells represent 0.
5th

6th

Typed

1

4

Null

4

1

Condition

1st

2nd

D

3rd

1

4
1

AM

2

2

PF

3

2

All

4th

5

Table 5: Subjects frequency of ranking of preference for different conditions as a
means to enter a name. 30 values total. Blanks cells represent 0.
Second, she found the predictive fillin helpful. Sometimes it filled when she didnt
expect it to. She also noted that because predictive fillin copies over many fields, it
encourages the user to add a more complete name. This may be an advantage in a harried
setting.

5. Design Recommendations
Given these experimental results, how should we configure handwriting recognition,
adaptive menus, and predictive fillin in another application (or a redesigned Names++)? For
handwritten input, recognition should use dictionaries specific to each type of field: numbers
only for numeric fields, and lists of domain terms for text fields.
5.1

Adaptive Menus

For adaptive menus, add a menu to any field that might have repeated values. If we
accidentally added an adaptive menu to a field that never had the same value twice, our
343

fiS CHLIMMER & W ELLS

Last Name

John
Jim
Bob
Jerry
Paul
David
Steve
Ron
Bill
Tom
Robert
Jack
Dennis
Robin
Rich
Julie
James
Edmund
Don
Dave

Title

President
Executive Director
Vice President - Human Resources
Vice President
Travel Consultant
Staffing Specialist
Sales Representative
Program Officer
Principal
Manager
Industrial Research Marketing Manager
Human Resources Manager
Human Resources
General Manager
Chair
Account Manager
consultant to Seattle Govt. Relations
Western Regional Sales Manager
Vice President/Managing Principal
Vice President, Finance & Administra
0%
25%
50%
75%
100%
Percent of Values

Brown
Wood
Smith
Lee
Baker
Thomas
Schroeder
Ray
Nelson
Jones
Johnson
Hoffman
Hand
Frost
Evans
Erickson
Dalpiaz
Anderson
Adams
Zipp

Boeing
Hewlett-Packard Company
Tektronix, Inc.
Battelle
Fluke Corporation
Microsoft Corporation
Mentor Graphics Corporation
ELDEC Corporation
Puget Sound Power & Light
Motorola Inc.
ARCO Products Company
University of Washington
Sundstrand
Sandia National Laboratories
Honeywell
Washington Technology Center
Intel Corporation
Asymetrix Corporation
Oregon State University
Digital Equipment Corporation
0%
25%
50%
75%

Company

First Name

mistake would be harmless. The user would surely notice that the choices were useless and
avoid checking the menu. When the menu was appropriate, the user would save time by
choosing common values from it.
How long should each menu be? Long enough to include the most common values but
short enough to be checked quickly. To make sure the menu is long enough, study how often
a fields values repeat. For Names++, Figures 9a and 9b depict a frequency histogram for the

100%

Figure 9a: Frequency of values in the First Name, Last Name, Title, and Company
fields for the 448 names used in Section 4. Each plot is a histogram of the 20 most
common values. Dark lines indicate what percent of values could be chosen by
different sized menus. If the menu includes choices from the top down to its vertical
position, it would allow the user to choose the percentage of field values indicated by
its horizontal position.

344

fiP.O. Box 3707
P.O. Box 3999
Pacific Northwest Laboratories
P.O. Box 999
P.O. Box 500
One Microsoft Way
P.O. Box C9090
P.O. Box 100
8005 S.W. Boeckman Road
160520 Microsoft BVUE
P.O. Box 97034
Cherry Point Refinery
Boeing Commercial Airplane Company
Post Office Box 8100
P.O. Box 97001
Battelle Boulevard
TAF C-34
P.O. Box 9090
P.O. Box 1970
FJ-15

City

Address

T HREE I NTERFACES FOR I NORMATION C APTURE: A C ASE S TUDY A DDING N AME I NFORMATION

WA

State

ID
TX
IL
NY
MA
AZ
UT
OH
NM
MN
MI
KS
FL
British Columbia
Virginia
VA
Texas
0%
25%
50%
75%
Percent of Values

99352
98124-2207
98124-2499
99220
98206
98195
98124
98052-6399
98009
98046-0100
98006
99336
98206-9090
98073-9701
98004
97077
97070-7777
99163
97070
98477
0%
25%

Zip Code

OR
CA

Seattle
Bellevue
Richland
Redmond
Spokane
Everett
Beaverton
Hillsboro
Wilsonville
Lynnwood
Vancouver
Pullman
Kennewick
Tacoma
San Francisco
Portland
Blaine
San Ramon
Kirkland
Corvallis

100%

50%

75%

100%

Figure 9b: Frequency of values in Address, City, State, and Zip Code fields for the
448 names used in Section 4.
20 most common values for 8 fields drawn from the 448 name records used in the
experiments. Overlaid on each plot is a line indicating what percent of field values could be
chosen from a particular size menu. For instance, for the First Name field in Figure 9a, the
histogram is almost flat. A menu including only John would allow the user to choose a
value for that field less than 5% of the time. If a menu included all 20 of the first names
shown, the user could choose a value 25% of the time. This field should not have a menu
because if it were long enough to include the most common values it would take too long to
check. (Also, the Newton computer we used in Section 4 limits menus to 23 choices because
of its screen size.) In contrast, for the Company field in Figure 9a, a menu including only

345

fiS CHLIMMER & W ELLS

Boeing would allow the user to choose a value more than 10% of the time. If it included
the 20 values shown, the user could choose a value 50% of the time.
Studying the histograms and aiming for menus that include 50% of the fields values, we
might re-engineer Names++ to have a menus of size 20 for the Company Field, size 10 for
the City field, and size 5 for the State field. Other fields have very flat histograms and would
need large menus to include a high percentage of field values. Recall that Section 4 reports
one subjects frustration with the Title field. Only President seems to be repeated for this
field in the 448 names we used.
5.2

Predictive fillin

Set up predictive fillin for any field that is functionally dependent (Ullman, 1988) on another.
A functional dependency is related to the artificial intelligence idea of a determination
(Russell, 1989). Intuitively, one field R, for range, functionally depends on another field D,
for domain, if, given a value for D, we can compute a unique value for R. If predictive fillin
can find a previous entry with the same value for D as the new entry, it copies over the
previous entrys value for R into the new entry. In Names++, the Company field is the
domain and the Address field is the range of a functional dependency.
Predictive fillin for all and only functionally dependent fields is probably too strict a
strategy. Some functional dependencies are not useful for predictive fillin because their
domain values are unique in the database. When this is so, predictive fillin cannot find a
previously matching entry and cannot copy over relevant information. For instance, a US
citizens address is functionally dependent on their Social Security number. In an application
like Names++ we dont expect to see the same Social Security number twice, so predictive
fillin would never have the opportunity to help the user by filling the address. Functional
dependencies with repeated domain values in the database, or dense functional
dependencies, should be used to set up predictive fillin.
Conversely, some non-functional dependencies may be close enough to functional to be
useful for predictive fillin. Technically, a dependency is not functional unless only one value
in the range can be computed for every value in the domain. If only a few values in the range
were computed for most values in the domain, the dependency might still be useful (Raju &
Majumdar, 1988, Russell, 1989, Ziarko, 1992). For instance, most companies have a single
office and address, but some may have more than one. It is still quite useful to fill address
fields when Names++ finds a previous name with a matching Company field. Other user
interface strategies can compensate for the other possible range values when they arise; for
instance, Names++ puts alternate addresses into the Address fields adaptive menu.
Therefore, dense dependencies that are functional or nearly so, or dense approximatelyfunctional dependencies, should be used to set up predictive fillin.
To determine which dense approximately-functional dependencies hold for a new
application area, it may be necessary to repeat the type of empirical domain analysis
described above for adaptive menus. For Names++, we used common sense knowledge about
people, companies, and addresses to set up predictive fillin. Recall that our goal is to
discover if the end result of automatic learning is worthwhile (e.g., Dent et al., 1992;
Hermens & Schlimmer, 1994; Schlimmer & Hermens, 1993; Yoshida, 1994). We recommend
considering each field as a number of logical components because dependencies may exist
between parts rather than whole fields. For instance, each person in a company may share a
common telephone number area code and prefix, but they are likely to have different

346

fiT HREE I NTERFACES FOR I NORMATION C APTURE: A C ASE S TUDY A DDING N AME I NFORMATION

extensions. By predictively filling in all but the last component of a phone number, Names++
fills as much as it can without adding poor quality information.

6. Related Work
Though interested in different tasks, other researchers have studied using intelligent user
interfaces to speed information capture. For instance, Hermens and Schlimmer (1994) built
an electronic form filler that tried to provide default values for every field on the form. Each
field of the form had a decision tree to calculate a default value. Like Names++, the
calculations used previously entered information to generate defaults and predictively fill in
fields. Unlike Names++, the calculations themselves were constructed at run-time using a
machine learning method. (Names++ does not alter predictive fillin at run-time. cf. Table 1.)
They field tested their system with a single electronic form filled out several hundred times
over an eight month period. They report an 87% reduction in keystrokes; loosely translating
this into a speedup yields 669% speedup or approximately 3 times the 210% speedup we
observed for entering a name.
Studying text prediction without field boundaries, Pomerleau (1995) built a typing
completion aid. Without relying on note-taking properties, his system predicts a completion
for the current word being typed (presumably in an editor). A connectionist network
estimates the probability of a number of possible completions for the current word; the most
likely, over some threshold, is offered to the user. Pomerleau tested his system with a pair of
subjects over a two-week period and reports an increase in typing speed of 2% for English
text and 1318% for computer program code. This modest gain may be due to inefficiencies
in the learning method, to lack of redundancy in the task, or to limitations in the user
interface itself.
As a complement to earlier research, this paper reports the individual and collective
accuracy of three user interface components. It reports user task time showing that the
components significantly improve efficiency. This paper also clarifies an issue confounded in
earlier work. If a learning interface is less effective than expected, is this due to an inherent
limitation in the interface itself, or does its learning method perform inadequately? To
answer the second question, other work compares two or more learning methods. In this
paper, we hand-built the predictive fillin structures (cf. Table 1) and were able to assess the
quality of the predictive fillin interface directly.

7. Conclusion
This paper makes two main contributions. First, it presents a study of the impact of three
user interface components on the time to enter information into a computer: handwriting
recognition, adaptive menus, and predictive fillin. Handwriting recognition is slower than
typing but is preferred by users. Advances in handwriting recognition may make it faster, but
recognition would still be much slower than choosing a value from a menu or predictive
fillin. All three components work well together and are preferred by users.
Second, this paper discusses principles for applying adaptive menus and predictive fillin
to new application areas. Fields with a few, frequently repeated values are candidates for
adaptive menus; functional dependencies indicate candidates for predictive fillin. Whether
these characteristics can be learned at run-time is a topic for future research.

347

fiS CHLIMMER & W ELLS

Acknowledgments
Kerry Hersh Raghavendra provided the names used in Section 4. Apple Computer developed
and supports Newton and the Newton ToolKit programming environment. The Newton AI
group at WSU provided many useful comments on an earlier draft of this paper. Geoff Allen,
Karl Hakimian, Mike Kibler, and the EECS staff provided a consistent and reliable
computing environment. Anonymous reviewers of an earlier draft of this paper provided
many (many) valuable suggestions. This work was supported in part by NASA under grant
number NCC 2-794.

References
Agre, P. E., & Chapman, D. (1987). Pengi: An implementation of a theory of activity.
Proceedings of the Sixth National Conference on Artificial Intelligence (pp. 268272).
Seattle, WA: AAAI Press.
Cesar, M., & Shinghal, R. (1990). An algorithm for segmenting handwriting postal codes.
Int. J. Man-Machine Studies, 33, 6380.
Culbert, M. (1994). Low power hardware for a high performance PDA. Proceedings of the
1994 IEEE Computer Conference. San Francisco, CA: IEEE.
Dent, L., Boticario, J., McDermott, J., Mitchell, T., & Zabowski, D. (1992). A personal
learning apprentice. Proceedings of the Tenth National Conference on Artificial
Intelligence (pp. 96103). San Jose, CA: AAAI Press.
Hermens, L. A., & Schlimmer, J. C. (1994). A machine learning apprentice for the
completion of repetitive forms. IEEE Expert, 9, 1, 2833.
Horswill, I. D., & Brooks, R. A. (1988). Situated vision in a dynamic world: Chasing objects.
Proceedings of the Seventh National Conference on Artificial Intelligence (pp. 796800).
St. Paul, MN: AAAI Press.
Kolodner, J. (1993). Case-based reasoning. San Francisco, CA: Morgan Kaufmann.
Kreigh, R. J., Pesot, J. F., & Halcomb, C. G. (1990). An evaluation of look-ahead help fields
on various types of menu hierarchies. Int. J. Man-Machine Studies, 32, 649661.
Lee, J. (1990). Intelligent interfaces and UIMS. In D. A. Duce, M. R. Gomes, F. R. A.
Hopgood, & J. R. Lee (Eds.), User interface management and design. NY: SpringerVerlag.
MacKenzie, S. I., Nonnecke, B., Riddersma, S., McQueen, C., & Meltz, M. (1994).
Alphanumeric entry on pen-based computers. Int. J. of Human-Computer Studies, 41,
755792.
Meyer, A. (1995). Pen computing: A technology overview and a vision. SIGCHI Bulletin, 27,
3, 4690.
Mitchell, J., & Shneiderman, B. (1989). Dynamic versus static menus: An exploratory
comparison. SIGCHI Bulletin, 20, 4, 33-37.
348

fiT HREE I NTERFACES FOR I NORMATION C APTURE: A C ASE S TUDY A DDING N AME I NFORMATION

Norman, K. L. (1991). The psychology of menu selection: Designing cognitive control of the
human/computer interface. Norwood, NJ: Ablex.
Pomerleau, D. A. (1995). A connectionist technique for accelerated textual input: Letting a
network do the typing. In Advances in Neural Information Processing Systems 7.
Cambridge, MA: MIT Press.
Rissland, E. L. (1984). Ingredients of intelligent user interfaces. Int. J. Man-Machine
Studies, 21, 377388.
Raju, K. V. S. V. N., & Majumdar, A. K. (1988). Fuzzy functional dependencies and lossless
join decomposition of fuzzy relational database systems. ACM Trans. Database Syst. 13,
2, 129166.
Russell, S. J. (1989). The use of knowledge in analogy and induction. San Francisco, CA:
Morgan Kaufmann.
Schlimmer, J. C., & Hermens, L. A. (1993). Software agents: Completing patterns and
constructing interfaces. Journal of Artificial Intelligence Research, 1, 6189.
Sears, A. & Shneiderman, B. (1994). Split menus: Effectively using selection frequency to
organize menus. ACM Trans. on Computer-Human Interaction, 1, 1, 2751.
Smith, W. R. (1994). The Newton application architecture. Proceedings of the 1994 IEEE
Computer Conference. San Francisco, CA: IEEE.
Snowberry, K., Parkinson, S., & Sisson, N. (1985). Effects of help fields on navigating
through hierarchical menu structures. Int. J. Man-Machine Studies, 22, 479491.
Ullman, J. D. (1988). Principles of database and knowledge-base systems: Volume 1.
Rockville, MD: Computer Science Press.
Ward, J. R., & Blesser, B. (1986). Interactive recognition of handprinted characters for
computer input. SIGCHI Bulletin, 18, 1, 4457.
Wilkinson, L., Hill, M., & Vang, E. (1992). SYSTAT: Graphics, Version 5.2 Edition.
Evanston, IL: SYSTAT, Inc.
Witten, I. H., Cleary, J. G., & Greenberg, S. (1984). On frequency-based menu-splitting
algorithms. Int. J. Man-Machine Studies, 21, 135-148.
Yoshida, K. (1994). User command prediction by graph-based induction. Sixth IEEE
International Conference on Tools with Artificial Intelligence (pp. 732735). New
Orleans, LA: IEEE.
Ziarko, W. (1992). The discovery, analysis, and representation of data dependencies. In
Piatetsky-Shapiro, G., & Frawley, W. (Eds.), Knowledge discovery in databases. Palo
Alto, CA: AAAI Press.

349

fiJournal of Artificial Intelligence Research 5 (1996) 239-288

Submitted 3/96; published 11/96

MUSE CSP: An Extension to
the Constraint Satisfaction Problem
Randall A. Helzerman
Mary P. Harper

School of Electrical and Computer Engineering
1285 Electrical Engineering Building
Purdue University
West Lafayette, IN 47907-1285 USA

helz@ecn.purdue.edu
harper@ecn.purdue.edu

Abstract

This paper describes an extension to the constraint satisfaction problem (CSP) called
MUSE CSP (MU ltiply SE gmented C onstraint S atisfaction P roblem). This extension is
especially useful for those problems which segment into multiple sets of partially shared
variables. Such problems arise naturally in signal processing applications including computer vision, speech processing, and handwriting recognition. For these applications, it is
often dicult to segment the data in only one way given the low-level information utilized
by the segmentation algorithms. MUSE CSP can be used to compactly represent several
similar instances of the constraint satisfaction problem. If multiple instances of a CSP have
some common variables which have the same domains and constraints, then they can be
combined into a single instance of a MUSE CSP, reducing the work required to apply the
constraints. We introduce the concepts of MUSE node consistency, MUSE arc consistency,
and MUSE path consistency. We then demonstrate how MUSE CSP can be used to compactly represent lexically ambiguous sentences and the multiple sentence hypotheses that
are often generated by speech recognition algorithms so that grammar constraints can be
used to provide parses for all syntactically correct sentences. Algorithms for MUSE arc
and path consistency are provided. Finally, we discuss how to create a MUSE CSP from a
set of CSPs which are labeled to indicate when the same variable is shared by more than
a single CSP.

1. Introduction
This paper describes an extension to the constraint satisfaction problem (CSP) called MUSE
CSP (MU ltiply SE gmented C onstraint S atisfaction P roblem). This extension is especially
useful for those problems which segment into multiple sets of partially shared variables.
First, we describe the constraint satisfaction problem and then define our extension.

1.1 The Constraint Satisfaction Problem
Constraint satisfaction problems (CSP) have a rich history in Artificial Intelligence (Davis
& Rosenfeld, 1981; Dechter, Meiri, & Pearl, 1991; Dechter & Pearl, 1988; Freuder, 1989,
1990; Mackworth, 1977; Mackworth & Freuder, 1985; Villain & Kautz, 1986; Waltz, 1975)
(for a general reference, see Tsang, 1993). Constraint satisfaction provides a convenient way
to represent and solve certain types of problems. In general, these are problems which can
be solved by assigning mutually compatible values to a predetermined number of variables
c 1996


AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiHelzerman & Harper

under a set of constraints. This approach has been used in a variety of disciplines including
machine vision, belief maintenance, temporal reasoning, graph theory, circuit design, and
diagnostic reasoning. When using a CSP approach (e.g., Figure 1), the variables are typically depicted as vertices or nodes, where each node is associated with a finite set of possible
values, and the constraints imposed on the variables are depicted using arcs. An arc looping
from a node to itself represents a unary constraint (a constraint on a single variable), and
an arc between two nodes represents a binary constraint (a constraint on two variables). A
classic example of a CSP is the map coloring problem (e.g., Figure 1), where a color must
be assigned to each country such that no two neighboring countries have the same color. A
variable represents a country's color, and a constraint arc between two variables indicates
that the two joined countries are adjacent and should not be assigned the same color.
Formally, a CSP (Mackworth, 1977) is defined in Definition 1.

Definition 1 (Constraint Satisfaction Problem)
N = fi; j; : : :g is the set of nodes (or variables), with jN j = n,
L = fa; b; : : :g is the set of labels, with jLj = l,
Li = faja 2 L and (i; a) is admissibleg,
R1 is a unary constraint, and (i; a) is admissible if R1 (i; a),
R2 is a binary constraint, (i; a) , (j; b) is admissible if R2 (i; a; j; b).
A CSP network contains all n-tuples in Ln which satisfy R1 and R2 . Since some of the

labels associated with a node may be incompatible with labels assigned to other nodes, it
is desirable, when the constraints are suciently tight (van Beek, 1994), to eliminate as
many of these labels as possible by enforcing local consistency conditions before a globally
consistent solution is extracted (Dechter, 1992). Node and arc consistency are defined in
Definitions 2 and 3, respectively. In addition, it may be desirable to eliminate as many label
pairs as possible using path consistency, which is defined in Definition 4.

Definition 2 (Node Consistency) An instance of CSP is said to be node consistent if and only if
each node's domain contains only labels for which the unary constraint R1 holds, i.e.:
8i 2 N : 8a 2 Li : R1 (i; a)
Definition 3 (Arc Consistency) An instance of CSP is said to be arc consistent if and only if for

every pair of nodes i and j , each element of Li (the domain of i) has at least one element of Lj for
which the binary constraint R2 holds, i.e.:
8i; j 2 N : 8a 2 Li : 9b 2 Lj : R2 (i; a; j; b)
{red, green, blue}
Different
Color

1
2

3

{red, green, blue}

Different
Color

1
2

3

{red, green, blue}

Different
Color

Figure 1: The map coloring problem as an example of CSP.
240

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

Definition 4 (Path Consistency) An instance of CSP is said to be path consistent if and only if:
8i; j 2 N : i 6= j ) (8a 2 Li : 8b 2 Lj : 8k 2 N : k 6= i ^ k 6= j ^ P ath(i; k; j ) )
(R2(i,a,j,b)) 9c 2 Lk : R2 (i; a; k; c) ^ R2 (k; c; j; b)))
where Path(i; k; j ) indicates that there is a path of arcs of length two connecting i and j which
goes through k.

Node consistency is easily enforced by the operation Li = Li \ fxjR1 (i; x)g, requiring
O(nl) time (where n is the number of variables and l is the maximum domain size). Arc
consistency is enforced by ensuring that every label for a node is supported by at least one
label for each node with which it shares a binary constraint (Mackworth, 1977; Mackworth
& Freuder, 1985; Mohr & Henderson, 1986). The arc consistency algorithm AC-4 (Mohr
& Henderson, 1986) has an worst-case running time of (el2) (where e is the number of
constraint arcs). AC-3 (Mackworth & Freuder, 1985) often performs better than AC-4 in
practice, though it has a slower running time in the worst case. AC-6 (Bessiere, 1994) has
the same worst-case running time as AC-4 and is faster than AC-3 and AC-4 in practice.
Path consistency ensures that any pair of labelings (i; a) , (j; b) allowed by the (i; j ) arc
directly are also allowed by all arc paths from i to j . Montanari has proven that to ensure
path consistency for a complete graph, it suces to check every arc path of length two
(Montanari, 1974). The path consistency algorithm PC-4 (Han & Lee, 1988) has a worstcase running time of O(n3 l3) time (where n is the number of variables in the CSP).

1.2 The Multiply Segmented Constraint Satisfaction Problem

There are many types of problems which can be solved by using CSP in a more or less direct
fashion. There are also problems which might benefit from the CSP approach, but which are
dicult to represent with a single CSP. This is the class of problems our paper addresses.
For example, suppose the map represented in Figure 1 is scanned by a noisy computer
vision system, with a resulting uncertainty as to whether the line between regions 1 and 2
is really a border or an artifact of the noise. This situation would yield two CSP problems
as depicted in Figure 2. A brute-force approach would be to solve both of the problems,
which would be reasonable for scenes containing only a few ambiguous borders. However,
as the number of ambiguous borders increases, the number of CSP networks would grow in
a combinatorially explosive fashion. In the case of ambiguous segmentation, it can be more
ecient to merge the constraint networks into a single network which would compactly
represent all of the instances simultaneously, as shown in Figure 3. Notice that the CSP
instances are combined into a directed acyclic graph where the paths through the DAG
from start to end correspond to those CSPs that were combined. In this paper, we develop
an extension to CSP called MUSE CSP (MU ltiply SE gmented C onstraint S atisfaction
P roblem), which represents multiple instances of a CSP problem as a DAG.
If there are multiple, similar instances of a CSP, then separately applying constraints
for each instance can result in much duplicated work. To avoid this duplication, we have
provided a way to combine the multiple instances of CSP into a MUSE CSP, and we have
241

fiHelzerman & Harper

{red, green, blue}

{red, green, blue}

1

1

3
Different
Color

3

1

{red, green, blue}

2

3
1

Different
Color

1

Different
Color

2

3
{red, green, blue}

{red, green, blue}

3

2

Different
Color

Figure 2: An ambiguous map yields two CSP problems.

start

{red, green, blue}

Different
Color

Different
Color

1
{red, green, blue}

3
{red, green, blue}

Different
Color

1

{red, green,
blue}

Different
Color

Different
Color

1

3

2

2

3

end

{red, green, blue}

{red, green, blue}

{red, green, blue}

Different
Color

{red, green, blue}

Different
Color

Figure 3: How the two CSP problems of Figure 2 can be captured by a single instance of
MUSE CSP. The directed edges form a DAG such that the directed paths through
the DAG correspond to instances of those CSPs that were combined.

242

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

developed the concepts of MUSE node consistency, MUSE arc consistency, and MUSE path
consistency. Formally, we define MUSE CSP as follows:

Definition 5 (MUSE CSP)
N = fi; j; : : :g is the set of nodes (or variables), with jN j = n,
  2N is a set of segments with jj = s,
L = fa; b; : : :g is the set of labels, with jLj = l,
Li = faja 2 L and (i; a) is admissible in at least one segmentg,
R1 is a unary constraint, and (i; a) is admissible if R1 (i; a),
R2 is a binary constraint, (i; a) , (j; b) is admissible if R2 (i; a; j; b).
The segments in  are the different sets of nodes representing CSP instances which are
combined to form a MUSE CSP. A solution to a MUSE CSP is defined to be a solution to
any one of its segments:

Definition 6 (Solution to a MUSE CSP) A solution to a MUSE CSP is an assignment ff to a
segment  = fi1; : : :; ip g such that  2  and ff 2 Li1      Lip such that R1(ix ; ff(ix )) holds for
every node ix 2 , and R2(ix ; ff(ix); iy ; ff(iy )) holds for every pair of nodes ix ; iy 2 , such that
ix 6= iy .
Depending on the application, the solution for a MUSE CSP could also be the set of all
consistent labels for a single path through the MUSE CSP, a single set of labels for each of
the paths (or CSPs), or all compatible sets of labels for each of the paths.
A MUSE CSP can be solved with a modified backtracking algorithm which finds a
consistent label assignment for a segment. However, when the constraints are suciently
tight, the search space can be pruned by enforcing local consistency conditions, such as node,
arc, and path consistency. To gain the eciency resulting from enforcing local consistency
conditions before backtracking, node, arc, and path consistency must be modified for MUSE
CSP. The definitions for MUSE CSP node consistency, arc consistency, and path consistency
appear in Definitions 7, 8, and 9, respectively.

Definition 7 (MUSE Node Consistency) An instance of MUSE CSP is said to be node consistent
if and only if each node's domain Li contains only labels for which the unary constraint R1 holds,
i.e.:
8i 2 N : 8a 2 Li : R1 (i; a)
Definition 8 (MUSE Arc Consistency) An instance of MUSE CSP is said to be MUSE arc consis-

tent if and only if for every label a in each domain Li there is at least one segment  whose nodes'
domains contain at least one label b for which the binary constraint R2 holds, i.e.:

8i 2 N : 8a 2 Li : 9 2  : i 2  ^ 8j 2  : j 6= i ) 9b 2 Lj : R2 (i; a; j; b)

Definition 9 (MUSE Path Consistency) An instance of MUSE CSP is said to be path consistent
if and only if:

8i; j 2 N : i 6= j ) (8a 2 Li : 8b 2 Lj : 9 2  : i; j 2  ^ 8k 2  : k 6= i ^ k 6= j ^ P ath(i; k; j ) )
(R2 (i; a; j; b) ) 9c 2 Lk : R2 (i; a; k; c) ^ R2 (k; c; j; b)))
243

fiHelzerman & Harper

a.

b.

3

{c,d}

start

1

{e}

3

{e}

{d}

end

2

{a,b}

d
e 1

b
e 1

c d
e 0 1

a b
e 0 1

start

end

2

1
{b}

4

4
{f}

{f}
c d
a 1 0
b 0 1

d
a b
f 1 1

c d
f 0 1

b

1

b
f 1

f

d
1

Figure 4: a. A MUSE CSP before MUSE arc consistency is achieved; b. A MUSE CSP
after MUSE arc consistency is achieved.
A MUSE CSP is node consistent if all of its segments are node consistent. Unfortunately,
MUSE CSP arc consistency requires more attention. When enforcing arc consistency in a
CSP, a label a 2 Li can be eliminated from node i whenever any other domain Lj has
no labels which together with a satisfy the binary constraints. However, in a MUSE CSP,
before a label can be eliminated from a node, it must be unsupported by the arcs of every
segment in which it appears, as required by the definition of MUSE arc consistency shown in
Definition 8. Notice that Definition 8 reduces to Definition 3 when the number of segments
is one.
To demonstrate how MUSE arc consistency applies to a MUSE CSP, consider the MUSE
CSP in Figure 4a. Notice that label c 2 L2 is not supported by any of the labels in L3
and L4 , but does receive support from the labels in L1. Should this label be considered
to be MUSE arc consistent? The answer is no because node 2 is only a member of paths
through the DAG which contain node 3 or node 4, and neither of them support the label
c. Because there is no segment such that all of its nodes have some label which supports
c, c should be eliminated from L2. Once c is eliminated from L2, a will also be eliminated
from L1 . This is because the elimination of c from L2 causes a to loose the support of node
2. Since node 2 is a member of every path, no other segment provides support for a. The
MUSE arc consistent DAG is depicted in Figure 4b. Note that MUSE arc consistency does
not ensure that the individual segments are arc consistent as CSPs. For example, Figure
5 is MUSE arc consistent even though its segments are not CSP arc consistent. This is
because c receives arc support (which is a very local computation) from the arcs of at least
one of the paths. We cannot ensure that the values that support a label are themselves
mutually consistent by considering MUSE arc consistency alone. For this case, MUSE path
consistency together with MUSE arc consistency would be needed to eliminate the illegal
labels c and a.
When enforcing path consistency in a CSP, R2 (i; a; j; b) becomes false if, for any third
node k, there is no label c 2 Lk such that R2 (i; a; k; c) and R2 (k; c; j; b) are true. In
244

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

c d
e 1 1

a b
e 0 1

3

{c,d}

start

1

{e}

end

2

{a,b}

4
{f}

c d
a 1 0
b 0 1

a b
f 1 1

c d
f 0 1

Figure 5: A MUSE CSP which is MUSE arc consistent, but not arc consistent for each
segment.
MUSE CSP, if a binary constraint becomes path inconsistent in one segment, it could still
be allowed in another. Therefore, the definition of MUSE path consistency is modified as
shown in Definition 9.
Enforcement of MUSE arc and path consistency requires modification of the traditional
CSP algorithms. These algorithms will be described after we introduce several applications
for which MUSE CSP has proven useful.

2. MUSE CSP and Constraint-based Parsing
It is desirable to represent a MUSE CSP as a directed acyclic graph (DAG) where the
directed paths through the DAG correspond to instances of CSP problems. It is often
easy to determine which variables should be shared and how to construct the DAG. The
application presented in this section is one for which MUSE CSP is useful. A parsing
problem is naturally represented as a DAG because of the presence of ambiguity. In many
cases, the same word can have multiple parts of speech; it is convenient to represent those
words as nodes in a MUSE CSP. In speech recognition systems, the identification of the
correct words in a sentence can be improved by using syntactic constraints. However, a word
recognition algorithm often produces a lattice of word candidates. Clearly, individually
parsing each of the sentences in a lattice can be inecient.

2.1 Parsing with Constraint Dependency Grammar
Maruyama developed a new grammar called Constraint Dependency Grammar (CDG)
(Maruyama, 1990a, 1990b, 1990c). He then showed how CDG parsing can be cast as a
CSP with a finite domain, so constraints can be used to rule out ungrammatical sentences.
A CDG is a four-tuple, h; R; L; C i, where:
245

fiHelzerman & Harper

 = a finite set of preterminal symbols, or lexical categories.
R = a finite set of uniquely named roles (or role-ids) = fr1; : : :; rp g.
L = a finite set of labels = fl1 ; : : :; lq g.
C = a finite set of constraints that an assignment A must satisfy.

A sentence s = w1 w2w3 : : :wn 2  is a string of length n. For each word wi 2  of a
sentence s, we must keep track of p different roles (or variables). A role is a variable which
takes on role values of the form <l; m>, where l 2 L and m 2 fnil; 1; 2; : : :ng. Role values
are denoted in examples as label-modifiee. In parsing, each label in L indicates a different
syntactic function. The value of m in the role value <l; m>, when assigned to a particular
role of wi, specifies the position of the word that wi is modifying when it takes on the
function specified by the label, l (e.g., subj-3 indicates that the word with that label is
a subject when it modifies the third word in the sentence). The sentence s is said to be
generated by the grammar G if there exists an assignment A which maps a role value to
each of the n  p roles for s such that the constraint set C (described in the next paragraph)
is satisfied.
A constraint set is a logical formula of the form: 8x1 ; x2; : : :; xa (and P1 P2 : : : Pm ),
where each xi ranges over all of the role values in each of the roles for each word of s. Each
subformula Pi in C must be of the form: (if Antecedent Consequent), where Antecedent
and Consequent are predicates or predicates joined by the logical connectives. Below are
the basic components used to express constraints.

 Variables: x1 , x2, : : : xa (a = 2 in (Maruyama, 1990a)).
 Constants: elements and subsets of  [ L [ R [ fnil, 1, 2, : : :, ng, where n corresponds to
the number of words in a sentence.

 Functions:
(pos x) returns the position of the word for role value x.
(rid x) returns the role-id for role value x.
(lab x) returns the label for role value x.
(mod x) returns the position of the modifiee for role value x.
(cat y) returns the category (i.e., the element in ) for the word at position y.
 Predicates: =, >, <1 .
 Logical Connectives: and, or, not.
A subformula Pi is called a unary constraint if it contains one variable and a binary constraint if it contains two. A CDG grammar has two associated parameters, degree and
arity. The degree of a grammar G is the number of roles. The arity of the grammar, a,
corresponds to the maximum number of variables in the subformulas of C .
Consider the example grammar, G1 , which is defined using the following four-tuple:
h1 = fdet; noun; verbg; R1 = fgovernorg, L1 = fdet; root; subjg, C1 (see constraints in
Figure 6)i. G1 has a degree of one and an arity of two. To illustrate the process of parsing
1. Note that 1 > nil or 1 < nil is false, because nil is not an integer. For MUSE networks, we relate position
intervals using <, >, and =.

246

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

with constraint satisfaction, Figure 6 shows the steps for parsing the sentence The dog
eats. To simplify the presentation of this example, the grammar uses a single role, the
governor role, which is denoted as G in the constraint network in Figure 6. The governor
role indicates the function a word fills in a sentence when it is governed by its head word.
A word is called the head of a phrase when it forms the basis of the phrase (e.g., the verb
is the head of the sentence). In useful grammars, we would also include several needs roles
(e.g, need1, need2) to make certain that a head word has all of the constituents it needs to
be complete (e.g., a singular count noun needs a determiner to be a complete noun phrase).
To determine whether the sentence, The dog eats, is generated by the grammar, the CDG
parser must be able to assign at least one role value to each of the n  p roles that satisfies the
grammar constraints (n = 3 is sentence length, and p = 1 is the number of roles). Because
the values for a role are selected from the finite set L1  fnil, 1, 2, 3g, CDG parsing can
be viewed as a constraint satisfaction problem over a finite domain. Therefore, constraint
satisfaction can be used to determine the possible parses of this sentence.
Initially, for each word, all possible role values are assigned to the governor role. We
assume that a word must either modify another word (other than itself) or modify no
word (m=nil). Nothing is gained in CDG by having a word modify itself. Next the unary
constraints are applied to all of the role values in the constraint network. A role value is
incompatible with a unary constraint if and only if it satisfies the antecedent, but not the
consequent. Notice in Figure 6 that all the role values associated with the governor role of
the first word (the) satisfy the antecedent of the first unary constraint, but det-nil, subjnil, subj-2, subj-3, root-nil, root-2, and root-3 do not satisfy the consequent, and so they
are incompatible with the constraint. When a role value violates a unary constraint, node
consistency eliminates those role values from their role because they can never participate
in a parse for the sentence. After all unary constraints are applied to the top constraint
network in Figure 6, the second network is produced.
Next, binary constraints are applied. Binary constraints determine which pairs of role
values can legally coexist. To keep track of pairs of role values, arcs are constructed connecting each role to all other roles in the network, and each arc has an associated arc matrix,
whose row and column indices are the role values associated with the two roles it connects.
The entries in an arc matrix can either be 1 (indicating that the two role values indexing
the entry are compatible) or 0 (indicating that the role values cannot simultaneously exist). Initially, all entries in each matrix are set to 1, indicating that the pair of role values
indexing the entry are initially compatible (because no constraints have been applied). In
our example, the single binary constraint (shown in Figure 6) is applied to the pairs of
role values indexing the entries in the matrices. For example, when x=det-3 for the and
y=root-nil for eats, the consequent of the binary constraint fails; hence, the role values are
incompatible. This is indicated by replacing the entry of 1 with 0.
Following the binary constraints, the roles of the constraint network can still contain
role values which are incompatible with the parse for the sentence. Role values that are not
supported by the binary constraints can be eliminated by achieving arc consistency. For
example, det-3 for the is not supported by the remaining role value for eats and is thus
deleted from the role.
After arc consistency, the example sentence has a single parse because there is only one
value per role in the sentence. A parse for a sentence consists of an assignment of role values
247

fiHelzerman & Harper

The
det
1

dog

eats

noun
2

verb
3

G

G
{detnil, det2, det3,
subjnil, subj2, subj3,
rootnil, root2, root3}

G

{detnil, det1, det3,
subjnil, subj1, subj3,
rootnil, root1, root3}

{detnil, det1, det2,
subjnil, subj1, subj2,
rootnil, root1, root2}

1. (if (= (cat (pos x)) det)
2. (if (= (cat (pos x)) noun)
(and (= (lab x) det)
(and (= (lab x) subj)
(< (pos x) (mod x))))
(< (pos x) (mod x))))

Apply Unary Constraints and
Enforce Node Consistency:

3. (if (= (cat (pos x)) verb)
(and (= (lab x) root)
(= (mod x) nil)))
The
det
1

dog

eats

noun
2

verb
3

G

G

G
{subj3}

{det2, det3}

{rootnil}
rootnil

subj3
det2

1

det3

1

subj3

1

rootnil
det2

1

det3

1

(if (and (= (lab x) det)
(= (mod x) (pos y)))
(= (cat (pos y)) noun))

Apply Binary Constraints:

The
det
1

dog

eats

noun
2

G

verb
3

G

G
{subj3}

{det2, det3}

{rootnil}
rootnil

subj3
det2

1

det3

1

subj3

1

rootnil
det2

1

det3

0

Enforce Arc Consistency:

The
det
1

dog

eats

noun
2

verb
3

G

G

{det2}

{subj3}

G
{rootnil}

Figure 6: Using constraints to parse the sentence: The dog eats.
248

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

to roles such that the unary and binary constraints are satisfied for that assignment. In
general, there can be more than one parse for a sentence; hence, there can be more than one
assignment of values to the roles of the sentence. Note that the assignment for the example
sentence is:

pos word cat governor role's value
1 the det
det-2
2 dog noun
subj-3
3 eats verb
root-nil
If there is only one possible sentence such that the part of speech of each of the words
is known in advance, then the parsing problem can be cast as a CSP. However, for the
ambiguity present in written and spoken sentences to be handled uniformly requires the use
of MUSE CSP.

2.2 Processing Lexically Ambiguous Sentences with CDG
One shortcoming of Maruyama's constraint-based parser is that it requires a word to have
a single part of speech; however, many words in the English language have more than one
lexical category. This assumption is captured in the way that Maruyama writes constraints
involving category information; the category is determined based on the position of the
word in the sentence. However, even in our simple example, the word dog could have been
either a noun or a verb prior to the propagation of syntactic constraints. Since parsing can
be used to lexically disambiguate the sentence, ideally, the parsing algorithm should not
require that the part of speech for the words be known prior to parsing.
Lexically ambiguous words can easily be accommodated by creating a CSP for each
possible combination of lexical categories; however, this would be combinatorially explosive.
In contrast, using a MUSE CSP, we can create a separate word node for each legal part of
speech of a word, sharing those words that are not ambiguous across all segments. Since
position does not uniquely define the category of a word, we must allow category information
to be accessed through the role value rather than the position of the word in the sentence
(i.e., use (cat x) rather than (cat (pos x))). Once we associate category information
with a role value, we could instead create role values for each lexical category for a word
and store all of the values in a single word node. However, this representation is not as
convenient as the MUSE CSP representation for the problem. In the lexically augmented
CSP, when there is more than one role per word (this is usually the case), the role values
associated with one lexical category for one role cannot support the role values associated
with another lexical category in another role for the same word. Additional constraints
must be propagated to enforce this requirement. The MUSE CSP representation does not
suffer from this problem. By using a separate node for each part of speech, the MUSE CSP
directly represents the independence of the alternative lexical categories for a given word.
The space requirements for the arc matrices in the MUSE representation is lower than for
the lexicalized CSP as there is no arc between the roles for the different lexical categories
for a word in the MUSE representation. Note that MUSE arc consistency is equivalent to
performing arc consistency on the lexically augmented CSP (after the additional constraints
249

fiHelzerman & Harper

are propagated)2. Most importantly, MUSE CSP can represent lattices that cannot be
combined into a single CSP.
The technique of creating separate nodes for different instances of a word can also be
used to handle feature analysis (like number and person) in parsing (Harper & Helzerman,
1995b). Since some words have multiple feature values, it is often more ecient to create a
single node with a set of feature values, apply syntactic constraints, and then split the node
into a set of nodes with a single feature value prior to applying the constraints pertaining
to the feature type. Node splitting can also be used to support the use of context-specific
constraints (Harper & Helzerman, 1995b).

2.3 Lattice Example

Much of the motivation for extending CSP comes from our work in spoken language parsing
(Harper & Helzerman, 1995a; Harper, Jamieson, Zoltowski, & Helzerman, 1992; Zoltowski,
Harper, Jamieson, & Helzerman, 1992). The output of a hidden-Markov-model-based
speech recognizer can be thought of as a lattice of word candidates. Unfortunately, a
lattice contains many word candidates that can never appear in a sentence covering the
duration of a speech utterance. By converting the lattice to a word graph, many word
candidates in the lattice can be eliminated. Figure 7 depicts a word graph constructed
from a simple lattice. Notice that the word tour can be eliminated when the word graph
is constructed. In order to accommodate words that occur over time intervals that may
overlap, each word's position in the lattice is now represented as a tuple (b; e) such that
b < e. The positional relations defined for constraints are easily modified to operate on
tuples (Harper & Helzerman, 1995a).
After construction, the word graph often contains spurious sentence hypotheses which
can be pruned by using a variety of constraints (e.g., syntactic, semantic, etc.). We can
apply constraints to individual sentences to rule out those that are ungrammatical; however,
individually processing each sentence hypothesis is inecient since many have a high degree
of similarity. If the spoken language parsing problem is structured as a MUSE CSP problem,
then the constraints used to parse individual sentences would be applied to the word graph
of sentence hypotheses, eliminating from further consideration many hypotheses which are
ungrammatical.
We have developed a MUSE CSP constraint-based parser, PARSEC (Harper & Helzerman, 1995a, 1995b; Harper et al., 1992; Zoltowski et al., 1992), which is capable of parsing
word graphs containing multiple sentences produced by a speech recognition module. We
have developed syntactic and semantic constraints for parsing single sentences, which when
applied to a word graph, eliminate those hypotheses that are syntactically or semantically
incorrect. The MUSE CSP used by our parser can be thought of as a parse forest which
is pruned by using constraints. By applying constraints from a wide variety of knowledge
sources, the parser prunes the composite structure of many of the role values associated
with a role, as well as word nodes with no remaining role values. Several experiments
(Harper et al., 1992; Zoltowski et al., 1992) have considered how effective syntactic and
2. As a simple demonstration, consider merging nodes 3 and 4 from Figure 5 into a single node such that the
value e and f keep track of the fact that they have type 3 and 4, respectively. Under these circumstances,
CSP arc consistency will give the same results as MUSE CSP arc consistency; even though a and c appear
in no solutions, they are not eliminated. Note that this example uses only one role per node.

250

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

tour
Its

wreck
hard

Its
1

a

nice

beach

to
recognizes

2

3

4

5

wreck

Its

(4,6)

(1,2)

hard

start

Its
(1,2)

6

(2,3)

speech

7

8

a

nice

(6,7)

(7,8)

9

beach
(8,9)

to
(3,4)

end

recognizes
(4,8)

speech
(8,9)

Figure 7: Multiple sentence hypotheses can be parsed simultaneously by applying constraints over a word graph rather than individual sentences extracted from a
lattice.
semantic constraints are at pruning word nodes that can appear in no sentence hypothesis.
For our work in speech processing, the MUSE arc consistency algorithm is very effective
at pruning the role values from the composite structure that can never appear in a parse
for a sentence (i.e., an individual CSP). Constraints are usually tight enough that MUSE
arc consistency eliminates role values that do not participate in at least one parse for the
represented sentences.
MUSE CSP is a useful way to process multiple sentences because the arc consistency
algorithm is effective at eliminating role values that cannot appear in sentence parses.
Several factors contribute to the effectiveness of the arc consistency algorithm for this
problem. First, the syntactic constraints are fairly tight constraints. Second, the role
values contain some segmental information that constrain the problem. Consider the word
graph in Figure 8. The value s-(3,4) associated with the role marked N for the word are
cannot support any of the values for the role marked G for the word dogs at position (3,5),
because it is not legal in a segment involving position (3,5). In the figure, we mark those
entries where a value associated with one role is segmentally incompatible with the values
of another with an N. These entries are equivalent to 0. Third, many times constraints
create symmetric dependencies between words in the sentence. For example, one constraint
might indicate that a verb needs a subject to its left, and another that a subject must be
governed by a verb to its right.

2.4 A Demonstration of the Utility of MUSE CSP Parsing

To demonstrate the utility of MUSE CSP for simultaneously parsing multiple CSP instances,
consider the problem of determining which strings of length 3n consisting of a's, b's, and c's
251

fiHelzerman & Harper

obj(1,2) obj(2,3)
s(3,4)

N

N

s(3,5)

0

1

{obj(1,2),
obj(2,3)}

{rootnil}

G

start

N

G

N

N {npnil}

G
dogs
(3,5)

{s(3,4),
s(3,5)}

end

{blanknil}
{subj(2,3),
subj(3,4),
subj(3,5)}

they
(1,2)

are
(2,3)
{obj(1,2),
obj(2,3)}

G

N {np(1,2),
np(2,3)}
dog
(3,4)

obj(1,2) obj(2,3)
s(3,4)

0

1

s(3,5)

N

N

Figure 8: In parsing word graphs, some of the values assigned to roles contain segmental
information which make them incompatible with the values associated with some
of the other roles. For example, s-(3,4) cannot support any of the values associated
with the G or N roles of the word dogs.
are in the language an bn cn . For the value of n = 3, this problem can be represented as the
single MUSE CSP problem shown in Figure 9 (the roles and role values are not depicted to
simplify the figure). We have devised constraints for this language (see Figure 10) which
eliminate all role values for all sentences not in the language as well as all ungrammatical
role values for a sentence in the language. When these constraints are applied followed by
MUSE arc consistency to a lattice like that in Figure 9 with a length divisible by three,
then only the grammatical sentence will remain with a single parse. For lattices containing
only sentences with lengths that are not divisible by three, all role values are eliminated
by MUSE arc consistency (there is no grammatical sentence). Hence, there is no search
required to extract a parse if there is one. For the n = 3 case of Figure 9, the parse appears
in Figure 11. A single parse will result regardless of the n chosen. Note that the modifiees
for the role values in the parse are used to ensure that for each a, there is a corresponding
c; for each b, there is a corresponding a; and for each c, there is a corresponding b. Figure
12 examines the time needed to extract a parse for sentences in the language an bn cn from
MUSE CSPs representing all strings of length 3n, 1  n  7, containing a, b, and c. The
time to perform MUSE AC-1 and extract the solution is compared to the time to extract
the solution without any preprocessing. The time to perform MUSE AC-1 and extract the
parse is stable as sentence length grows, but the time to extract a parse grows quickly for
sentence lengths greater than 15 when MUSE arc consistency is not used.
The previous example involves a grammar where there can only be one parse for a single
sentence in the lattice; however, it is a simple matter to provide similar demonstrations for
252

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

start

a

a

a

a

a

a

a

a

a

(1,2)

(2,3)

(3,4)

(4,5)

(5,6)

(6,7)

(7,8)

(8.9)

(9,10)

b

b

b

b

b

b

b

b

b

(1,2)

(2,3)

(3,4)

(4,5)

(5,6)

(6,7)

(7,8)

(8.9)

(9,10)

c

c

c

c

c

c

(1,2)

(2,3)

(3,4)

(4,5)

(5,6)

(6,7)

c

c

c

(7,8)

(8.9)

(9,10)

end

Figure 9: A single MUSE CSP can simultaneously test all possible orderings of a's, b's, and
c's for membership in the language anbncn , n = 3.

2

= fa, b, cg
= fgovernorg
L2 = fa, b, cg
C2 = see below:
R2

; 3 Unary Constraints
(if (and (=
(=
(and (=
(>

(cat
(rid
(lab
(mod

x)
x)
x)
x)

a)
governor))
a)
(pos x))))

(if (and (=
(=
(and (=
(<

(cat
(rid
(lab
(mod

x)
x)
x)
x)

c)
governor))
c)
(pos x))))

(if (and (=
(=
(and (=
(<

(cat
(rid
(lab
(mod

x)
x)
x)
x)

b)
governor))
b)
(pos x))))

; 8 Binary Constraints
(if (and (= (lab x) a)
(or (= (lab y) b)
(= (lab y) c)))
(< (pos x) (pos y)))

(if (and (= (lab x) b)
(= (lab y) c))
(< (pos x) (pos y)))

(if (and (=
(=
(>
(< (mod

(lab x)
(lab y)
(pos x)
x) (mod

a)
a)
(pos y)))
y)))

(if (and (=
(=
(=
(= (lab

(if (and (=
(=
(>
(< (mod

(lab x)
(lab y)
(pos x)
x) (mod

b)
b)
(pos y)))
y)))

(if (and (= (lab x) b)
(= (mod x) (pos y))
(= (rid y) governor))
(= (lab y) a))

(if (and (=
(=
(>
(< (mod

(lab x)
(lab y)
(pos x)
x) (mod

c)
c)
(pos y)))
y)))

(if (and (=
(=
(=
(= (lab

(lab x) a)
(mod x) (pos y))
(rid y) governor))
y) c))

(lab x) c)
(mod x) (pos y))
(rid y) governor))
y) b))

Figure 10: G2 = h2; R2; L2; C2i accepts the language an bn cn , n  0.
253

fiHelzerman & Harper

pos
(1,2)
(2,3)
(3,4)
(4,5)
(5,6)
(6,7)
(7,8)
(8,9)
(9,10)

cat governor role's value
a
a
a
b
b
b
c
c
c

a-(9,10)
a-(8,9)
a-(7,8)
b-(3,4)
b-(2,3)
b-(1,2)
c-(6,7)
c-(5,6)
c-(4,5)

Figure 11: The single parse remaining in the network depicted in Figure 9 after the applying
the constraints in G2 and enforcing MUSE arc consistency.

2500

CPU Time in seconds

2000

Extract without MUSE AC1
1500

1000

500

Extract plus MUSE AC1
0
2

4

6

8

10

12
14
Lattice Length

16

18

20

22

Figure 12: This graph depicts the time to extract the parse for the language an bn cn from
a MUSE CSP representing all sentences of length 3n, where n varies from 1 to
7. The time to extract the parse without MUSE arc consistency is compared to
the time to perform MUSE AC-1 and extract the parse.

254

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

3

= fa, b, cg
= fgovernorg
L3 = fw1, w2g
C3 = see below:
R3

; 2 Unary Constraints
(if (= (lab x) w1)
(< (pos x) (mod y)))

(if (= (lab x) w2)
(> (pos x) (mod y)))
; 6 Binary Constraints

(if (and (= (lab x) w1)
(= (lab y) w2))
(< (pos x) (pos y)))
(if (and (=
(=
(>
(> (mod

(lab x)
(lab y)
(pos x)
x) (mod

w1)
w1)
(pos y)))
y)))

(if (and (=
(=
(and (=
(=

(lab
(mod
(lab
(cat

w1)
(pos y)))
w2)
(cat y))))

x)
x)
y)
x)

(if (and (= (lab x) w1)
(= (lab y) w2))
(> (mod x) (mod y)))
(if (and (= (lab x) w2)
(= (lab y) w2)
(> (pos x) (pos y)))
(< (mod x) (mod y)))
(if (and (= (lab x) w2)
(= (mod x) (pos y)))
(= (lab y) w1))

Figure 13: G3 = h3 ; R3; L3; C3i accepts the language ww.
more complex cases. For example, the constraint grammar shown in Figure 13 can be
to parse all possible sentences of a given length in the the language ww, such that w is
in fa; b; cg+. Consider the MUSE CSP in Figure 14 (the roles and role values are not
depicted to simplify the figure). After applying the constraints and performing MUSE arc
consistency on this MUSE CSP, there are precisely 81 strings that are in ww, and their
parses are compactly represented in the constraint network. The constraints plus MUSE
arc consistency eliminate every value that cannot appear in a parse. For lattices containing
odd length sentences, no role values remain after MUSE arc consistency. Figure 15 shows
the time needed to extract all of the parses for sentences in the language ww from the
MUSE CSPs as we vary the length of w from 1 to 8. The time to perform MUSE AC-1 and
extract the parses grows slowly as sentence length increases because the number of parses
increases with sentence length; however, it grows more slowly than the time to extract the
parses when MUSE arc consistency is not used.
Similar results have also been obtained with grammars used to parse word graphs constructed from spoken sentences in the resource management and ATIS domains (Harper
et al., 1992; Zoltowski et al., 1992; Harper & Helzerman, 1995a).

3. The MUSE CSP Arc Consistency Algorithm
In this section, we introduce an algorithm, MUSE AC-1, to achieve MUSE CSP arc consistency. Because our algorithm builds upon the AC-4 algorithm (Mohr & Henderson, 1986),
we present that algorithm first for comparison purposes.
255

fiHelzerman & Harper

start

a

a

a

a

a

a

a

a

(1,2)

(2,3)

(3,4)

(4,5)

(5,6)

(6,7)

(7,8)

(8.9)

b

b

b

b

b

b

b

b

(1,2)

(2,3)

(3,4)

(4,5)

(5,6)

(6,7)

(7,8)

(8.9)

c

c

c

c

c

c

(1,2)

(2,3)

(3,4)

(4,5)

(5,6)

(6,7)

c

c

(7,8)

(8.9)

end

Figure 14: A single MUSE CSP can simultaneously test all possible orderings of a's, b's,
and c's for membership in the language ww where jwj = 4 .

3000

CPU Time in seconds

2500

2000
Extract without MUSE AC1
1500

1000

500
Extract plus MUSE AC1
0
2

4

6

8
10
Lattice Length

12

14

16

Figure 15: This graph depicts the time to extract all parses for the language ww from a
MUSE CSP representing all sentences of length 2 to 16 such that w 2 fa; b; cg+.
The time to extract all parses without MUSE arc consistency is compared to
the time to perform MUSE AC-1 and extract all parses.

256

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

Notation

Meaning

An ordered pair of nodes.
All node pairs (i; j ). If (i; j ) 2 E , then (j; i) 2 E .
E
An ordered pair of node i and label a 2 Li .
(i; a)
faja 2 L and (i; a) is permitted by the constraints (i.e., admissible)g
Li
R2 (i; a; j; b) = 1 indicates the admissibility a 2 Li and b 2 Lj given
R2 (i; a; j; b)
binary constraints.
Counter[(i; j ); a] The number of labels in Lj which are compatible with a 2 Li .
(j; b) 2 S [i; a] means that a 2 Li and b 2 Lj are simultaneously
S [i; a]
admissible. This implies that a supports b.
M [i; a] = 1 indicates that the label a is not admissible for (and
M [i; a]
has already been eliminated from) node i.
A queue of arc support to be deleted.
List
(i; j )

Figure 16: Data structures and notation for the arc consistency algorithm, AC-4.

3.1 CSP Arc Consistency: AC-4
AC-4 builds and maintains several data structures, described in Figure 16, to allow it
to eciently achieve arc consistency in a CSP. Note that we have modified the notation
slightly to eliminate subscripts (which become quite cumbersome for the path consistency
algorithm). Figure 17 shows the code for initializing the data structures, and Figure 18
contains the algorithm for eliminating inconsistent labels from the domains. This algorithm
requires (el2) time, where e is the number of constraint arcs, and l is the domain size (Mohr
& Henderson, 1986).
In AC-4, if the label a 2 Li is compatible with b 2 Lj , then a supports b (and vice
versa). To keep track of how much support each label a has, the number of labels in Lj
which are compatible with a in Li are counted and the total stored in Counter[(i; j ); a]
by the algorithm in Figure 17. If any Counter[(i; j ); a] is zero, then a is removed from Li
(because it cannot appear in any solution), the ordered pair (i; a) is placed on the List, and
M[i; a] is set to 1 (to avoid removing the element a from Li more than once). The algorithm
must also keep track of which labels that label a supports by using S[i; a], a set of arc and
label pairs. For example, S[i; a] = f(j; b); (j; c)g means that a in Li supports b and c in Lj .
If a is ever removed from Li , then b and c will loose some of their support.
After the preprocessing step in Figure 17, the algorithm in Figure 18 loops until List
becomes empty, at which point the CSP is arc consistent. When (i; a) is popped off List
by this procedure, for each element (j; b) in S[i; a], Counter[(j; i); b] is decremented. If
Counter[(j; i); b] becomes zero, b would be removed from Lj , (j; b) placed on List, and
M[j; b] set to 1.
257

fiHelzerman & Harper

1. List := ;
2. for i 2 N do
3.
for a 2 Li do f
4.
S [i; a] := ;
5.
M [i; a] := 0; g
6. for (i; j ) 2 E do
7.
for a 2 Li do f
8.
Total := 0;
9.
for b 2 Lj do
10.
if R2 (i; a; j; b) then f
11.
Total := Total+1;
12.
S [j; b] := S [j; b] [ f(i; a)g; g
13.
if Total = 0 then f
14.
Li := Li , fag;
15.
List := List [ f(i; a)g;
16.
M [i; a] := 1; g
17.
Counter[(i; j ); a] := Total; g

Figure 17: Initialization of the data structures for AC-4.

1. while List 6=  do f
2.
pop (i; a) from List;
3.
for (j; b) 2 S[i; a] do f
4.
Counter[(j; i); b] := Counter[(j; i); b] , 1;
5.
if Counter[(j; i); b] = 0 ^ M [j; b] = 0 then f
6.
Lj := Lj , fbg;
7.
List := List [ f(j; b)g;
8.
M [j; b] := 1; g g g

Figure 18: Eliminating inconsistent labels from the domains in AC-4.

258

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

Next, we describe the MUSE arc consistency algorithm for a MUSE CSP, called MUSE
AC-1. We purposely keep our notation and presentation of MUSE AC-1 as close as possible
to that of AC-4 so that the reader can benefit from the similarity of the two algorithms.

3.2 MUSE AC-1

MUSE arc consistency is enforced by removing those labels in each Li which violate the conditions of Definition 8. MUSE AC-1 builds and maintains several data structures, described
in Figure 19, to allow it to eciently perform this operation. Many of these data structures
are borrowed from AC-4, while others exploit the DAG representation of the MUSE CSP
to determine when values are incompatible in all of the segments. Figure 22 shows the
code for initializing the data structures, and Figures 23 and 24 contain the algorithm for
eliminating inconsistent labels from the domains.
In MUSE AC-1 as in AC-4, if label a at node i is compatible with label b at node j , then
a supports b. To keep track of how much support each label a has, the number of labels in Lj
which are compatible with a in Li are counted, and the total is stored in Counter[(i; j ); a].
For CSP arc consistency, if Counter[(i; j ); a] is zero, a would be immediately removed from
Li, because that would mean that a could never appear in any solution. However, in MUSE
arc consistency, this may not be the case, because even though a does not participate in
a solution for any of the segments which contain i and j , there could be another segment
for which a would be perfectly legal. A label cannot become globally inadmissible until it
is incompatible with every segment. Hence, in MUSE CSP, if Counter[(i; j ); a] is zero, the
algorithm simply places [(i; j ); a] on List and records that fact by setting M[(i; j ); a] to 1.
By placing [(i; j ); a] on List, the algorithm is indicating that the segments containing i and
j do not support the label a.
MUSE AC-1 must also keep track of those labels in j that label a in Li supports by
using S[(i; j ); a], a set of node-label pairs. For example, S[(i; j ); a] = f(j; b); (j; c)g means
that a in Li supports b and c in Lj . If a is ever invalid for Li , then b and c will loose some
of their support.
Because  is a DAG, MUSE AC-1 is able to use the properties of the DAG to identify
local (and hence eciently computable) conditions under which labels become globally
inadmissible. Segments are defined as paths through the MUSE CSP from start to end. If
a value associated with a variable is not supported by any of the variables which precede it
or follow it, then there is no way that the value can be used by any segment, so it can be
deleted by the arc consistency algorithm. In addition, if a value in a variable's domain is
supported by the constraints for values associated with a second variable, but the second
variable is preceded or followed by variables that have no values supporting the value, then
because a solution involves a path of variables in the MUSE DAG, the value cannot be
supported for any segment involving the two variables. These two ideas provide the basis
for the remaining data structures used by MUSE AC-1.
Consider Figure 20, which shows the nodes which are adjacent to node i in the DAG.
Because every segment in the DAG which contains node i is represented as a directed path in
the DAG going through node i, either node j or node k must be in every segment containing
i. Hence, if the label a is to remain in Li, it must be compatible with at least one label in
either Lj or Lk . Also, because either n or m must be contained in every segment containing
259

fiHelzerman & Harper

Notation

Meaning

(i; j )

An ordered pair of nodes.
All node pairs (i; j ) such that there exists a path of directed edges in G
between i and j . If (i; j ) 2 E , then (j; i) 2 E .
An ordered pair of node i and label a 2 Li .

E

(i; a)
[(i; j ); a]

An ordered pair of a node pair (i; j ) and a label a 2 Li .
faja 2 L and (i; a) is permitted by the constraints (i.e., admissible)g

Li

2 (i; a; j; b)

R

Counter[(i; j ); a]
S[(i; j ); a]
M[(i; j ); a]
List

G

Next-Edgei
Prev-Edgei
Local-Prev-Support(i; a)
Local-Next-Support(i; a)
Prev-Support[(i; j ); a]
Next-Support[(i; j ); a]

2 (i; a; j; b) = 1 indicates the admissibility of a 2 Li and b 2 Lj given
binary constraints.
The number of labels in Lj which are compatible with a 2 Li .
(j; b) 2 S [(i; j ); a] means that a 2 Li and b 2 Lj are simultaneously
admissible. This implies that a supports b.
M[(i; j ); a] = 1 indicates that the label a is not admissible for (and
has already been eliminated from) all segments containing i and j .
A queue of arc support to be deleted.
G is the set of node pairs (i; j ) such that there exists a directed
edge from i to j .
Next-Edgei contains all node pairs (i; j ) such that there exists a
directed edge (i; j ) 2 G. It also contains (i; end) if i is the last node
in a segment.
Prev-Edgei contains all node pairs (j; i) such that there exists a
directed edge (j; i) 2 G. It also contains (start; i) if i is the first node
in a segment.
A set of elements (i; j ) such that (j; i) 2 Prev-Edgei , and if j 6= start,
a must be compatible with at least one of j 's labels. If
Local-Prev-Support(i; a) becomes empty, a in i is no longer admissible.
A set of elements (i; j ) such that (i; j ) 2 Next-Edgei , and if j 6= end,
a must be compatible with at least one of j 's labels. If
Local-Next-Support(i; a) becomes empty, a in i is no longer admissible.
(i; k) 2 Prev-Support[(i; j ); a] implies that (k; j ) 2 Prev-Edgej , and
if k 6= start, then a 2 Li is compatible with at least one of j 's and
one of k's labels. If Prev-Support[(i; j ); a] becomes empty, then a is
no longer admissible in segments containing i and j .
(i; k) 2 Next-Support[(i; j ); a] implies that (j; k) 2 Next-Edgej , and
if k 6= end, then a 2 Li is compatible with at least one of j 's and
one of k's labels. If Next-Support[(i; j ); a] becomes empty, then a is
no longer admissible in segments containing i and j .
R

Figure 19: Data structures and notation for MUSE AC-1.

260

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

n

j
i

m

k
{...,a,...}

LocalPrevSupport(i,a) = {(i,n),(i,m)}
LocalNextSupport(i,a) = {(i,j)}

Figure 20: Local-Prev-Support and Local-Next-Support for an example DAG. The sets indicate that the label a is allowed for every segment which contains n, m, and j ,
but is disallowed for every segment which contains k. The solid directed lines
are members of G, and the solid undirected lines represent members of E .

i, if label a is to remain in Li, it must also be compatible with at least one label in either
Ln or Lm .
In order to track this dependency, two sets are maintained for each label a at node i,
Local-Next-Support(i; a) and Local-Prev-Support(i; a). Local-Next-Support(i; a) is a set of
ordered node pairs (i; j ) such that (i; j ) 2 Next-Edgei , and if (i; j ) 2 E , there is at least one
label b 2 Lj which is compatible with a. Local-Prev-Support(i; a) is a set of ordered pairs
(i; j ) such that (j; i) 2 Prev-Edgei , and if (i; j ) 2 E , there is at least one label b 2 Lj which
is compatible with a. Dummy ordered pairs are also created to handle cases where a node
is at the beginning or end of a network: when (start; i) 2 Prev-Edgei , (i; start) is added to
Local-Prev-Support(i; a), and when (i; end) 2 Next-Edgei , (i; end) is added to Local-NextSupport(i; a). This is to prevent a label from being ruled out because no nodes precede or
follow it in the DAG. Whenever one of i's adjacent nodes, j , no longer has any labels b in
its domain which are compatible with a, then (i; j ) should be removed from Local-PrevSupport(i; a) or Local-Next-Support(i; a), depending on whether the edge is from j to i
or from i to j , respectively. If either Local-Prev-Support(i; a) or Local-Next-Support(i; a)
becomes empty, then a is no longer a part of any MUSE arc consistent instance, and should
be eliminated from Li . In Figure 20, the label a is admissible for the segments containing
both i and j , but not for the segments containing i and k. If because of constraints, the
labels in j become inconsistent with a on i, (i; j ) would be eliminated from Local-NextSupport(a; i), leaving an empty set. In that case, a would no longer be supported by any

segment.
The algorithm can utilize similar conditions for nodes which are not directly connected
to i by Next-Edgei or Prev-Edgei . Consider Figure 21. Suppose that the label a at node i is
compatible with a label in Lj , but it is incompatible with the labels in Lx and Ly , then it is
reasonable to eliminate a for all segments containing both i and j , because those segments
would have to include either node x or y . To determine whether a label is admissible
for a set of segments containing i and j , we calculate Prev-Support[(i; j ); a] and NextSupport[(i; j ); a] sets. Next-Support[(i; j ); a] includes all (i; k) arcs which support a in i
261

fiHelzerman & Harper

z
{...,a,...} i

x
j

w

y

Figure 21: If Next-Edgej = f(j; x); (j; y )g; Counter[(i; x); a] = 0, and Counter[(i; y ); a] = 0,
then a is inadmissible for every segment containing both i and j . The solid directed lines are members of G, and the solid undirected lines represent members
of E .
given that there is a directed edge from j to k, and (i; j ) supports a. Prev-Support[(i; j ); a]
includes all (i; k) arcs which support a in i given that there is a directed edge from k
to j , and (i; j ) supports a. Note that Prev-Support[(i; j ); a] will contain an ordered pair
(i; j ) if (i; j ) 2 Prev-Edgej , and Next-Support[(i; j ); a] will contain an ordered pair (i; j ) if
(j; i) 2 Next-Edgej . These elements are included because the edge between nodes i and
j is sucient to allow j 's labels to support a in the segment containing i and j . Dummy
ordered pairs are also created to handle cases where a node is at the beginning or end of
a network: when (start; j ) 2 Prev-Edgej , (i; start) is added to Prev-Support[(i; j ); a], and
when (j; end) 2 Next-Edgej , (i; end) is added to Next-Support[(i; j ); a]. This is to prevent
a label from being ruled out because no nodes precede or follow it in the DAG.
Figure 22 shows the Prev-Support, Next-Support, Local-Next-Support, and Local-PrevSupport sets that the initialization algorithm creates for a simple example DAG. After the
initialization step, these sets contain all node pairs that are allowed based on the connectivity of G. Later, during the consistency step those node pairs which do not support the
associated label are eliminated from each set.
To illustrate how these data structures are used by the second step of MUSE AC-1 shown
in Figure 23, consider what happens if initially [(1; 3); a] 2 List for the MUSE CSP depicted
in Figure 22. [(1; 3); a] is placed on List to indicate that the label a in L1 is not supported by
any of the labels associated with node 3. When that value is popped off List, it is necessary
for each (3; x) 2 S[(1; 3); a] to decrement Counter[(3; 1); x] by one. If any Counter[(3; 1); x]
becomes 0, and [(3; 1); x] has not already been placed on the List, then it is added for future
processing. Once this is done, it is necessary to remove [(1; 3); a]'s inuence on the MUSE
DAG. To handle this, we examine the two sets Prev-Support[(1; 3); a] = f(1; 2); (1; 3)g and
262

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.

List := ;
E := f(i; j )j9 2  : i; j 2  ^ i 6= j ^ i; j 2 N g;
for (i; j) 2 E do
for a 2 Li do f
S[(i; j ); a] := ;
M[(i; j ); a] := 0;
Local-Prev-Support(i; a) := ; Local-Next-Support(i; a) := ;
Prev-Support[(i; j ); a] := ; Next-Support[(i; j ); a] := ; g
for (i; j) 2 E do
for a 2 Li do f

Total := 0;

for b 2 Lj do
if R2 (i; a; j; b) then f

Total := Total+1;
S[(j; i); b] := S[(j; i); b] [ f(i; a)g; g
if Total=0 then f
List := List [ f[(i; j ); a]g;
M[(i; j ); a] := 1; g
Counter[(i; j ); a] := Total;
Prev-Support[(i; j ); a] := f(i; x)j(i; x) 2 E ^ (x; j ) 2 Prev-Edgej g
[ f(i; j )j(i; j ) 2 Prev-Edgej g
[ f(i; start)j(start; j ) 2 Prev-Edgej g;
Next-Support[(i; j ); a] := f(i; x)j(i; x) 2 E ^ (j; x) 2 Next-Edgej g
[ f(i; j )j(j; i) 2 Next-Edgej g
[ f(i; end)j(j; end) 2 Next-Edgej g;
Local-Prev-Support(i; a) := f(i; x)j(i; x) 2 E ^ (x; i) 2 Prev-Edgei g
[ f(i; start)j(start; i) 2 Prev-Edgei g;
Local-Next-Support(i; a) := f(i; x)j(i; x) 2 E ^ (i; x) 2 Next-Edgei g
[ f(i; end)j(i; end) 2 Next-Edgei g; g
{c}

2

start

1

3

{a,b}

end
{d}

Prev-Support[(1; 2);a] = f(1; 2)g
Prev-Support[(1; 3);a] = f(1; 2); (1; 3)g
Prev-Support[(1; 2);b] = f(1; 2)g
Prev-Support[(1; 3);b] = f(1; 2); (1; 3)g
Prev-Support[(2; 1);c] = f(2; start)g
Prev-Support[(2; 3);c] = f(2; 3); (2; 1)g
Prev-Support[(3; 1);d] = f(3; start)g
Prev-Support[(3; 2);d] = f(3; 1)g
Local-Prev-Support(1;a) = f(1; start)g
Local-Prev-Support(1;b) = f(1; start)g
Local-Prev-Support(2;c) = f(2; 1)g
Local-Prev-Support(3;d) = f(3; 1); (3; 2)g

Next-Support[(1; 2);a] = f(1; 3)g
Next-Support[(1; 3);a] = f(1; end)g
Next-Support[(1; 2);b] = f(1; 3)g
Next-Support[(1; 3);b] = f(1; end)g
Next-Support[(2; 1);c] = f(2; 1); (2; 3)g
Next-Support[(2; 3);c] = f(2; end)g
Next-Support[(3; 1);d] = f(3; 1); (3; 2)g
Next-Support[(3; 2);d] = f(3; 2)g
Local-Next-Support(1;a) = f(1; 2); (1; 3)g
Local-Next-Support(1;b) = f(1; 2); (1; 3)g
Local-Next-Support(2;c) = f(2; 3)g
Local-Next-Support(3;d) = f(3; end)g

Figure 22: Initialization of the data structures for MUSE AC-1 along with a simple example.
263

fiHelzerman & Harper

1. while List 6=  do f
2.
Pop [(i; j ); a] from List;
3.
for (j; b) 2 S[(i; j ); a] do f
4.
Counter[(j; i); b] := Counter[(j; i); b] , 1;
5.
if Counter[(j; i); b] = 0 ^ M[(j; i); b] = 0 then f
6.
List := List [ f[(j; i); b]g;
7.
M[(j; i); b] := 1; g g
8.
Update-Support-Sets([(i; j ); a]); (see Figure 24) g

Figure 23: Eliminating inconsistent labels from the domains in MUSE AC-1.
Update-Support-Sets ([(i; j ); a]) f
1. for (i; x) 2 Prev-Support[(i; j ); a] ^ x 6= j ^ x 6= start do f
2.
Prev-Support[(i; j ); a] := Prev-Support[(i; j ); a] , f(i; x)g ;
3.
Next-Support[(i; x); a] := Next-Support[(i; x); a] , f(i; j )g;
4.
if Next-Support[(i; x); a] =  ^ M[(i; x); a] = 0 then f
5.
List := List [ f[(i; x); a]g;
6.
M[(i; x); a] := 1; g g
7. for (i; x) 2 Next-Support[(i; j ); a] ^ x 6= j ^ x 6= end do f
8.
Next-Support[(i; j ); a] := Next-Support[(i; j ); a] , f(i; x)g;
9.
Prev-Support[(i; x); a] := Prev-Support[(i; x); a] , f(i; j )g;
10.
if Prev-Support[(i; x); a] =  ^ M[(i; x); a] = 0 then f
11.
List := List [ f[(i; x); a]g;
12.
M[(i; x); a] := 1; g g
13. if (j; i) 2 Prev-Edgei then
14. Local-Prev-Support(i; a) := Local-Prev-Support(i; a) , f(i; j )g;
15. if Local-Prev-Support(i; a) =  then f
16. Li := Li , fag;
17. for (i; x) 2 Local-Next-Support(i; a) ^ x 6= j ^ x 6= end do f
18.
Local-Next-Support(i; a) := Local-Next-Support(i; a) , f(i; x)g;
19.
if M[(i; x); a] = 0 then f
20.
List := List [ f[(i; x); a]g;
21.
M[(i; x); a] := 1; g g g
22. if (i; j ) 2 Next-Edgei then
23. Local-Next-Support(i; a) := Local-Next-Support(i; a) , f(i; j )g;
24. if Local-Next-Support(i; a) =  then f
25. Li := Li , fag;
26. for (i; x) 2 Local-Prev-Support(i; a) ^ x 6= j ^ x 6= start do f
27.
Local-Prev-Support(i; a) := Local-Prev-Support(i; a) , f(i; x)g;
28.
if M[(i; x); a] = 0 then f
29.
List := List [ f[(i; x); a]g;
30.
M[(i; x); a] := 1; g g g g

Figure 24: The function Update-Support-Sets([(i; j ); a]) for MUSE AC-1.
264

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

Next-Support[(1; 3); a] = f(1; end)g. Note that the value (1; end) in Next-Support[(1; 3); a]
and the value (1; 3) in Prev-Support[(1; 3); a], require no further action because they are
dummy values. However, the value (1; 2) in Prev-Support[(1; 3); a] indicates that (1; 3) is
a member of Next-Support[(1; 2); a], and since a is not admissible for (1; 3), (1; 3) should
be removed from Next-Support[(1; 2); a], leaving an empty set. Note that because NextSupport[(1; 2); a] is empty, and assuming that M[(1; 2); a] = 0, [(1; 2); a] is added to List for
further processing. Next, (1; 3) is removed from Local-Next-Support(1; a), leaving a set of
f(1; 2)g. During the next iteration of the while loop [(1; 2); a] is popped from List. When
Prev-Support[(1; 2); a] and Next-Support[(1; 2); a] are processed, Next-Support[(1; 2); a] = 
and Prev-Support[(1; 2); a] contains only a dummy, requiring no action. Finally, when (1; 2)
is removed from Local-Next-Support(1; a), the set becomes empty, so a is no longer compatible with any segment containing node 1 and can be eliminated from further consideration as
a possible label for node 1. Once a is eliminated from node 1, it is also necessary to remove
the support of a 2 L1 from all labels on nodes that precede node 1, that is for all nodes x
such that (1; x) 2 Local-Prev-Support(1; a). Since Local-Prev-Support(1; a) = f(1; start)g,
and start is a dummy node, there is no more work to be done.
In contrast, consider what happens if initially [(1; 2); a] 2 List for the MUSE CSP in
Figure 22. In this case, Prev-Support[(1; 2); a] contains (1; 2) which requires no additional
work; whereas, Next-Support[(1; 2); a] contains (1; 3), indicating that (1; 2) must be removed
from Prev-Support[(1; 3); a]'s set. After the removal, Prev-Support[(1; 3); a] is non-empty,
so the segment containing nodes 1 and 3 still supports the label a in L1. The reason that
these two cases provide different results is that the constraint arc between nodes 1 and 3 is
contained in every segment; whereas, the constraint arc between nodes 1 and 2 is found in
only one of them.

3.3 The Running Time and Space Complexity of MUSE AC-1
The worst-case running time of the routine to initialize the MUSE AC-1 data structures
(in Figure 22) is O(n2 l2 + n3 l), where n is the number of nodes in a MUSE CSP and l
is the number of labels. Given that the number of (i; j ) elements in E is O(n2 ) and the
domain size is O(l), the size of the Counter and S arrays is O(n2 l). To determine the
number of supporters for a given arc-label pair requires O(l) work; hence, initializing the
Counter and S arrays requires O(n2 l2) time. However, there are O(n2 l) Prev-Support and
Next-Support sets, where each Prev-Support[(i; j ); a] and Next-Support[(i; j ); a] requires
O(n) time to compute, so the time to calculate all Prev-Support and Next-Support sets
is O(n3 l). Finally, the time needed to calculate all Local-Next-Support and Local-PrevSupport sets is O(n2 l) because there are O(nl) sets with up to O(n) elements per set.
The worst-case running time for the algorithm which prunes labels that are not MUSE
arc consistent (in Figures 23 and 24) also operates in O(n2 l2 + n3 l) time. Clearly the
Counter array contains O(n2 l) entries (a similar argument can be made for the S array)
to keep track of in the algorithm. Each Counter[(i; j ); a] can be at most l in magnitude,
and it can never become negative, so the maximum running time for line 4 in Figure 23
(given that elements appear on List only once because of M) is O(n2 l2). Because there
are O(n2 l) Next-Support and Prev-Support lists, each up to O(n) in size, the maximum
running time required for lines 3 and 9 in Figure 24 is O(n3 l). Finally, since there are O(nl)
265

fiHelzerman & Harper

Approach
CSPs
MUSE CSP

Nodes
Degree of
Number of
Number of
per Path Node splitting Constraint Networks Nodes

n
n

kn

k
k

1

n
kn

Asymptotic
Time

kn n2 l2
2
(kn) l2 + (kn)3 l

Table 1: Comparison of the space and time complexity for MUSE arc consistency on a
MUSE CSP to arc consistency on multiple CSPs representing a node splitting
problem (e.g., lexical ambiguity in parsing).
Local-Prev-Support and Local-Next-Support sets from which to eliminate O(n) elements,
the maximum running time of lines 14 and 23 in Figure 24 is O(n2 l). Hence, the maximum
running time of the MUSE CSP arc consistency algorithm is O(n2 l2 + n3 l).
The space complexity of MUSE CSP AC-1 is also O(n2 l2 + n3 l) because the arrays
Counter and M contain O(n2 l) elements, and there are O(n2 l) S sets, each containing O(l)
items; O(n2 l) Prev-Support and Next-Support sets, each containing O(n) items; and O(nl)
Local-Next-Support and Local-Prev-Support sets, each containing O(n) items.
By comparison, the worst-case running time and space complexity for CSP arc consistency is O(n2 l2), assuming that there are n2 constraint arcs. Note that for applications
where l = n, the worst-case running times of the algorithms are the same order (this is
true for parsing spoken language with a MUSE CSP). Also, if  is representable as planar
DAG (in terms of Prev-Edge and Next-Edge, not E), then the running times of the two
algorithms are the same order because the average number of values in Prev-Support and
Next-Support would be a constant. On the other hand, if we compare MUSE CSP to the
use of multiple CSPs for problems where there are k alternative variables for a particular
variable in a CSP, then MUSE CSP AC-1 is asymptotically more attractive, as shown in
Table 1.

3.4 The Correctness of MUSE AC-1

Next we prove the correctness of MUSE AC-1.
Theorem 1 A label a is eliminated from Li by MUSE AC-1 if and only if that label is
unsupported by all the arcs (i; x) of every segment.

Proof:
1. We must show that if a label is eliminated, it is inadmissible in every segment. A
label is eliminated from a domain by MUSE AC-1 (see lines 16 and 25 in Figure 24) if
and only if its Local-Prev-Support set or its Local-Next-Support set becomes empty
(see lines 15 and 24 in Figure 24). In either case, the label should be eliminated
to make the MUSE CSP instance MUSE arc consistent. We prove that if a label's
local support sets become empty, that label cannot participate in any MUSE arc
consistent instance of MUSE CSP. This is proven for Local-Next-Support (LocalPrev-Support follows by symmetry.) Observe that if a 2 Li , and it is unsupported by
266

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

all of the nodes which immediately follow i in the DAG, then it cannot participate
in any MUSE arc consistent instance of MUSE CSP. In line 23 of Figure 24, if (i; j )
is removed from Local-Next-Support(i; a) set then [(i; j ); a] must have been popped
off List. The removal of (i; j ) from Local-Next-Support(i; a) indicates that, in the
segment containing i and j , a 2 Li is inadmissible. It remains to be shown that
[(i; j ); a] is put on List if a 2 Li is unsupported by every segment which contains i
and j . This is proven by induction on the number of iterations of the while loop in
Figure 23.
Base case: The initialization routine only puts [(i; j ); a] on List if a 2 Li is incompatible with every label in Lj (line 17 of Figure 22). Therefore, a 2 Li is unsupported
by all segments containing i and j .
Induction step: Assume that at the start of the kth iteration of the while loop
all [(x; y ); c] which have ever been put on List indicate that c 2 Lx is inadmissible
in every segment which contains x and y . It remains to show that during the kth
iteration, if [(i; j ); a] is put on List, then a 2 Li is unsupported by every segment
which contains i and j . There are several ways in which a new [(i; j ); a] can be put
on List:
(a) All labels in Lj which were once compatible with a 2 Li have been eliminated.
This item could have been placed on List either during initialization (see line 17
in Figure 22) or during a previous iteration of the while loop (see line 6 in Figure
23)), just as in the CSP AC-4 algorithm. It is obvious that, in this case, a 2 Li
is inadmissible in every segment containing i and j .
(b) Prev-Support[(i; j ); a] =  (see line 10 in Figure 24) indicating that a 2 Li
is incompatible with all nodes k for (k; j ) 2 Prev-Edgej . The only way for
[(i; j ); a] to be placed on List for this reason (at line 11) is because all tuples
of the form [(i; k); a] (where (k; j ) 2 Prev-Edgej ) were already put on List. By
the induction hypothesis, these [(i; k); a] items were placed on the List because
a 2 Li is inadmissible in with all segments containing i and k in the DAG. But if
a is not supported by any node which immediately precedes j in the DAG, then
a is unsupported by every segment which contains j . Therefore, it is correct to
put [(i; j ); a] on List.
(c) Next-Support[(i; j ); a] =  (see line 4 in Figure 24) indicating that a 2 Li is
incompatible with all nodes k for (j; k) 2 Next-Edgej . The only way for [(i; j ); a]
to be placed on List (at line 5) for this reason is because all tuples of the form
[(i; k); a] (where (j; k) 2 Next-Edgej ) were already put on List. By the induction
hypothesis, these [(i; k); a] items were placed on the List because a 2 Li is inadmissible in all segments containing i and k in the DAG. But if a is not supported
by any node which immediately follows j in the DAG, then a is inadmissible in
every segment which contains j . Therefore, it is correct to put [(i; j ); a] on List.
(d) Local-Next-Support(i; a) =  (see line 24 in Figure 24) indicating that a 2 Li is
incompatible with all nodes k such that (i; k) 2 Next-Edgei . The only way for
[(i; j ); a] to be placed on List (at line 29) for this reason is because no node which
follows i in the DAG supports a, and so all pairs (i; k) have been legally removed
267

fiHelzerman & Harper

a

a

1

...

1

...

c

...

b

...

Local_Prev_Support(i,a) = {(i,j),...}
Local_Next_Support(i,a) = {(i,k},...}

j

i

k

{b,...}

Prev_Support[(i,j),a] is nonempty
{c,...}

{a,...}
c

Prev_Support[(i,k),a] = {(i,k),...}

...

Next_Support[(i,k),a] is nonempty

1

b

...

Next_Support[(i,j),a] = {(i,j),...}

1

Figure 25: If a 2 Li after MUSE AC-1, it must be preceded by some node j and followed
by some node k which support a.
from Local-Next-Support(i; a) during previous iterations. Because there is no
segment containing i which supports a, it follows that no segment containing i
and j supports that label.
(e) Local-Prev-Support(i; a) =  (see line 15 in Figure 24) indicating that a 2 Li
is incompatible with all nodes k such that (k; i) 2 Prev-Edgei . The only way
for [(i; j ); a] to be placed on List (at line 20) for this reason is because no node
which precedes i in the DAG supports a, and so all pairs (i; k) have been legally
removed from Local-Prev-Support(i; a) during previous iterations. Because there
is no segment containing i which supports a, it follows that no segment containing
i and j supports that label.
At the beginning of the (k + 1)th iteration of the while loop, every [(x; y ); c] on List
implies that c is not supported by any segment which contains x and y . Therefore,
by induction, it is true for all iterations of the while loop in Figure 23. Hence, if a
label's local support sets become empty, that label cannot participate in a MUSE arc
consistent instance of MUSE CSP.
2. We must also show that if a is not eliminated from Li by the MUSE arc consistency
algorithm, then it must be MUSE arc consistent. For a to be MUSE arc consistent,
there must exist at least one path from start to end which goes through node i such
that all nodes n on that path contain at least one label which is compatible with
a 2 Li . If a is not deleted after MUSE AC-1, then Local-Next-Support(i; a) 6=  and
Local-Prev-Support(i; a) 6= . Hence, i must be preceded and followed by at least
one node which supports a 2 Li ; otherwise, a would have been deleted. As depicted
in Figure 25, we know that there must be some node j which precedes i such that,
if it is not start, it must contain at least one label b which supports a, and NextSupport[(i; j ); a] and Prev-Support[(i; j ); a] must be non-empty. Similarly, there must
be some node k which follows i such that, if it is not end, it must contain at least one
label c which supports a, and Next-Support[(i; k); a] and Prev-Support[(i; k); a] must
be non-empty.
268

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

To show there is a path through the DAG, we must show that there is a path beginning
at start which reaches i such that all the nodes along that path support a 2 Li , and
that there is a path beginning at i which reaches end such that all the nodes along
that path support a 2 Li . We will show the necessity of the path from i to end such
that all nodes along that path support a 2 Li given that a remains after MUSE AC-1;
the necessity of the path from start to i can be shown in a similar way.
Base case: If a 2 Li after MUSE AC-1, then there must exist at least one node
which follows i, say k, such that [(i; k); a] has never been placed on List. Hence,
R2 (i; a; k; c) = 1 for at least one c 2 Lk and Next-Support[(i; k); a] and PrevSupport[(i; k); a] must be non-empty.
Induction Step: Assume that there is a path of n nodes that follows i that supports
a 2 Li, but none of those nodes is the end node. This implies that each of the n
nodes contains at least one label compatible with a and that Next-Support[(i; n); a]
and Prev-Support[(i; n); a] must be non-empty for each of the n nodes.
Next, we show that a path of length (n + 1) must also support a 2 Li ; otherwise,
the label a would have been deleted by MUSE AC-1. We have already noted that
for the nth node on the path in the induction step, Next-Support[(i; n); a] must be
non-empty; hence, there must exist at least one node, say n0 , which follows the nth
node in the path of length n which supports a 2 Li . If n0 is the end node, then
this is the case. If n0 is not end, then the only way that (i; n0) can be a member
of Next-Support[(i; n); a] is if [(i; n0); a] has not been placed on List. If it hasn't,
then R2 (i; a; n0; l) = 1 for at least one l 2 Ln and Next-Support[(i; n0); a] and PrevSupport[(i; n0); a] must be non-empty. If this were not the case, then (i; n0) would have
been removed from Next-Support[(i; n); a], and n would no longer support a 2 Li .
Hence, if a 2 Li after MUSE AC-1, then there must be a path of nodes to end such
that for each node n which is not the end node, R2 (i; a; n; l) = 1 for at least one l 2 Ln
and Next-Support[(i; n); a] and Prev-Support[(i; n); a] must be non-empty. Hence a
is MUSE arc consistent.
0

2

From this theorem, we may conclude that MUSE AC-1 builds the largest MUSE arc
consistent structure. Because MUSE arc consistency takes into account all of its segments,
if a single CSP were selected from the MUSE CSP after MUSE arc consistency is enforced,
CSP arc consistency could eliminate additional labels.

3.5 A Profile of MUSE AC-1

Given the fact that MUSE AC-1 operates on a composite data structure, the benefits of
using this algorithm can have a high payoff over individually processing CSPs. In section 2.4,
we provided several examples where the payoff is obvious. To gain some insight into factors
inuencing the effectiveness of MUSE CSP, we have conducted an experiment in which
we randomly generate MUSE CSP instances with two different graph topologies. The tree
topology is characterized by two parameters: the branching factor (how many nodes follow
each non-leaf node in the tree) and the path length (how many nodes there are in a path
from the root node to a leaf node). The lattice topology is characteristic of a MUSE CSP
269

fiHelzerman & Harper

which is produced by a hidden-Markov-model-based spoken language recognition system
for our constraint-based parser. Lattices are also characterized by their length and their
branching factor.
For this experiment, we examined trees with a path length of four and a branching
factor of two or three, and lattices with a path length of four and a branching factor of
two or three. We initialized each variable to have either 3 or 6 labels. We then randomly
generated constraints in the network, varying the probability that R2 (i; a; j; b) = 1 from
0.05 to .95 in steps of 0.05. For each probability, 6 instances were generated. The lower
the probability that R2 (i; a; j; b) = 1, the tighter the constraints. Note that the probability
of a constraint between two nodes should be understood as the probability of a constraint
between two nodes given that a constraint is allowed between them. For example, nodes
that are on the same level in the tree topology are in different segments, and so constraints
cannot occur between them.
The results of this experiment are displayed in Figures 26 and 27. In each of the four
panels of each figure, four curves are displayed. After MUSE AC-1 appears on curves
displaying the average number of labels remaining after MUSE AC-1 is applied to instances
of a MUSE CSP as the probability of a constraint varies. The curves labeled Solution
indicate the average number of labels remaining after MUSE AC-1 that are used in a
solution. CSP AC is associated with curves that display the number of labels that remain
in at least one segment when the segment is extracted from the MUSE CSP and CSP
arc consistency is applied. Unused indicates the difference between the number of labels
that remain after MUSE AC-1 and the number that are CSP arc consistent in at least one
segment.
For both of the topologies, if the probability R2 (i; a; j; b) = 1 is low (e.g., .1) or high
(e.g., .8), then MUSE AC-1 tracks the performance of arc consistency performed on the
individual instances for either topology. However, the topology does impact the range of
low and high probabilities for which this is true. When constraints are randomly generated,
after MUSE AC-1 is performed, the tree topology has fewer remaining values than the lattice
topology that are not CSP arc consistent. These results suggest that MUSE CSP AC-1 may
be more effective for some topologies than for others. However, in the tree topology the
randomly generated constraints between the values of two variables are independent of the
other probabilities generated. This is not the case for the lattice; once a pair of variables has
a set of randomly generated constraints, they are shared by all paths through the lattice.
Notice that increasing the number of values in a domain seems to have more impact on
the tree than increasing the branching factor, probably because as the branching factor
increases, so does the number of independent nodes.
This experiment does show that if a problem is tightly constrained, MUSE AC-1 can
be effectively used to eliminate values that are unsupported by the constraints. Clearly,
this was the case for the parsing problems presented in section 2.4. A small set of syntactic
constraints effectively eliminates values that can never be used in a parse for a sentence,
even in a lattice with a branching factor of three and arbitrarily long paths.
270

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

a. Tree with branching factor 2, path length 4, 3 labels per variable, 15 variables.

b. Tree with branching factor 3, path length 4, 3 labels per variable, 90 variables.
3

Average number of role values per role out of 3

Average number of role values per role out of 3

3

2.5
After MUSE AC1
2

CSP AC

1.5

Solution

1

0.5

2.5
After MUSE AC1
2

CSP AC

1.5
Solution
1

0.5
Unused

Unused
0
0

0.1

0.2

0.3

0.4
0.5
0.6
0.7
Probability that R2(i,a,j,b)=1

0.8

0.9

0
0

1

c. Tree with branching factor 2, path length 4, 6 labels per variable, 15 variables.

0.2

0.3

0.8

0.9

1

Average number of role values per role out of 6

6

5
After MUSE AC1
4

CSP AC
3

2
Solution
1

5

4

After MUSE AC1
CSP AC

3

Solution

2

1

Unused
0
0

0.4
0.5
0.6
0.7
Probability that R2(i,a,j,b)=1

d. Tree with branching factor 3, path length 4, 6 labels per variable, 90 variables.

6

Average number of role values per role out of 6

0.1

0.1

0.2

0.3

0.4
0.5
0.6
0.7
Probability that R2(i,a,j,b)=1

Unused
0.8

0.9

0
0

1

0.1

0.2

0.3

0.4
0.5
0.6
0.7
Probability that R2(i,a,j,b)=1

0.8

0.9

1

Figure 26: Simulation results for trees with a path length of 4, a branching factor of 2 or
3, and 3 or 6 labels per variable.

271

fiHelzerman & Harper

a. Lattice with branching factor 2, path length 4, 3 labels per variable, 8 variables.

b. Lattice with branching factor 3, path length 4, 3 labels per variable, 12 variables.

3

3
After MUSE AC1

Average number of role values per role out of 3

Average number of role values per role out of 3

CSP AC
After MUSE AC1

2.5

2
Solution
1.5

1

0.5
Unused

0
0

0.1

0.2

0.3

0.4
0.5
0.6
0.7
Probability that R2(i,a,j,b)=1

0.8

0.9

2

Solution

1.5

1

Unused
0.5

0
0

1

c. Lattice with branching factor 2, path length 4, 6 labels per variable, 8 variables.

0.1

0.2

0.3

0.4
0.5
0.6
0.7
Probability that R2(i,a,j,b)=1

0.8

0.9

1

d. Lattice with branching factor 3, path length 4, 6 labels per variable, 12 variables.

6

6
After MUSE AC1

After MUSE AC1

Average number of role values per role out of 6

Average number of role values per role out of 6

CSP AC

2.5

CSP AC

5

4
Solution
3

2

1

5
CSP AC

4
Solution
3

2

Unused

1

Unused
0
0

0.1

0.2

0.3

0.4
0.5
0.6
0.7
Probability that R2(i,a,j,b)=1

0.8

0.9

0
0

1

0.1

0.2

0.3

0.4
0.5
0.6
0.7
Probability that R2(i,a,j,b)=1

0.8

0.9

1

Figure 27: Simulation results for lattices with a path length of 4, a branching factor of 2 or
3, and 3 or 6 labels per variable.

272

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

Local-Next-Support(B, b1) = {(B, E)}
Local-Next-Support(B, b2) = {(B, C), (B, E)}
Local-Next-Support(B, b3) = {(B, C)}

Next-Support[(B, C), b3] = {(B, D)}
Next-Support[(B, C), b2] = {(B, F)}

C
{c1}

start

A
{a1}

D
{d1}

B

end

{b1, b2, b3}
{e1}

{f1}

E

F

Figure 28: Using MUSE arc consistency data structures to guide a backtracking search.

3.6 Extracting Solutions from a MUSE CSP after MUSE AC-1
Solutions to regular CSP problems are typically generated by using backtracking (or fancier
search algorithms) to assemble a set of labels, one for each node, which are consistently
admissible. Extracting solutions from MUSE CSPs can be done in a similar way, but it
is desirable to make a few modifications to the search algorithms to take advantage of the
extra information which is contained in the MUSE AC-1 data structures.
Consider the example shown in Figure 28. This figure presents a simple MUSE CSP.
Suppose we are only interested in solutions to the segment which is highlighted: fA, B, C,
Dg. Suppose also that there is only one solution to this segment: a1 for A, b3 for B, c1 for
C, and d1 for D. We wish to find this solution by depth-first search.
We begin by assigning a1 to A. However, the domain of B, in addition to the desired
label b3, also contains the labels b1 and b2, which are valid only for other segments. If
we initially (and naively) choose b1 for B and continue doing depth-first search, we would
waste a lot of time backtracking. Fortunately, after enforcing MUSE arc consistency, the
MUSE data structures contain useful information concerning the segments for which the
labels are valid. In this case, the backtracking algorithm can check Local-Next-Support(B,
b1) to determine which of the outgoing nodes b1 is compatible with. Since (B, C) is not an
element of Local-Next-Support(B, b1), a smart search algorithm would not choose b1 as a
label for B.
However, just looking at the local support sets might not be enough. After the search
algorithm has rejected b1 as a label for B, it would go on to consider b2. Local-NextSupport(B, b2) indicates that b2 is a valid label for some of the segments which contain
C, but it fails to tell us that b2 is not valid for the segment we are examining. Despite
this, the search algorithm can still eliminate b2 by looking at Next-Support[(B, C), b2],
which indicates that b2 is only compatible with segments containing the node F. Clearly,
this type of information will more effectively guide the search for a solution along a certain
path. Improved search strategies for MUSE CSPs will be the focus of future research efforts.
273

fiHelzerman & Harper

4. The MUSE CSP Path Consistency Algorithm
In this section, we introduce an algorithm to achieve MUSE CSP path consistency, MUSE
PC-1, which builds upon the PC-4 algorithm (Han & Lee, 1988).

4.1 MUSE PC-1

MUSE path consistency is enforced by setting R2 (i; a; j; b) to false when it violates the
conditions of Definition 9. MUSE PC-1 builds and maintains several data structures comparable to the data structures defined for MUSE AC-1, described in Figure 29, to allow it to
eciently perform this operation. Figure 32 shows the code for initializing the data structures, and Figures 33 and 34 contain the algorithm for eliminating MUSE path inconsistent
binary constraints.
MUSE PC-1 must keep track of which labels in Lk support R2 (i; a; j; b). To keep track
of how much path support each R2 (i; a; j; b) has, the number of labels in Lk which satisfy
R2 (i; a; k; c) and R2 (k; c; j; b) are counted using Counter[(i; j ); k; a; b]. Additionally, the
algorithm must keep track of the set S[(i; j ); k; a; b], which contains members of the form
(k; c) where R2 (i; a; k; c) and R2 (k; c; j; b) are supported by R2 (i; a; j; b). If R2 (i; a; j; b)
ever becomes false in the segment containing i, j , and k, then R2 (i; a; k; c) and R2 (k; c; j; b)
will loose some of their support. MUSE PC-1 also uses the Local-Next-Support, Local-PrevSupport, Prev-Support, and Next-Support sets similar to those in MUSE AC-1.
MUSE PC-1 is able to use the properties of the DAG to identify local (and hence
eciently computable) conditions under which binary constraints fail because of lack of path
support. Consider Figure 30, which shows the nodes which are adjacent to node i and j in
the DAG. Because every segment in the DAG which contains node i and j is represented as
a directed path in the DAG going through both node i and node j , some node must precede
and follow nodes i and j for R2 (i; a; j; b) to hold. In order to track this dependency, two sets
are maintained for each [(i; j ); a; b] tuple: Local-Prev-Support[(i; j ); a; b] and Local-NextSupport[(i; j ); a; b]. Note that we distinguish Local-Prev-Support[(i; j ); a; b] from LocalPrev-Support[(j; i); b; a] to separately keep track of those elements directly preceding i and
those directly preceding j . We also distinguish Local-Next-Support[(i; j ); a; b] from LocalNext-Support[(j; i); b; a]. If any of these sets become empty, then the (i; j ) arc can no
longer support R2 (i; a; j; b). Local-Prev-Support[(i; j ); a; b] is a set of ordered node pairs
(i; x) such that (x; i) 2 Prev-Edgei , and if (i; x) 2 E , there is at least one label d 2 Lx
which is compatible with R2 (i; a; j; b). Local-Next-Support[(i; j ); a; b] is a set of ordered
node pairs (i; x) such that (i; x) 2 Next-Edgei , and if (i; x) 2 E , there is at least one label
d 2 Lx which is compatible with R2 (i; a; j; b). Dummy ordered pairs are also created to
handle cases where a node is at the beginning or end of a network: when (start; i) 2 PrevEdgei , (i; start) is added to Local-Prev-Support[(i; j ); a; b], and when (i; end) 2 Next-Edgei ,
(i; end) is added to Local-Next-support[(i; j ); a; b].
The algorithm can utilize similar conditions for nodes which may not be directly connected to i and j . Consider Figure 31. Suppose that R2 (i; a; j; b) is compatible with
a label in Lk , but is incompatible with the labels in Lx and Ly , then R2 (i; a; j; b) and
R2 (j; b; i; a) are false for all segments containing i, j , and k because those segments would
have to include either node x or y . To determine whether a constraint is admissible for
a set of segments containing i, j , and k, we calculate Prev-Support[(i; j ); k; a; b], Prev274

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

Notation

Meaning
An ordered pair of nodes.

(i; j )

All node pairs (i; j ) such that there exists a path of directed edges in G
between i and j . If (i; j ) 2 E , then (j; i) 2 E .
An ordered quadruple of a node pair (i; j ), a node k, and the labels
a 2 Li and b 2 Lj .

E

[(i; j ); k; a; b]

faja 2 L and (i; a) is permitted by the constraints (i.e., admissible)g

Li

2 (i; a; j; b) = 1 indicates the admissibility a 2 Li and b 2 Lj given
binary constraints.

2 (i; a; j; b)

R

R

Counter[(i; j ); k; a; b]
S[(i; j ); k; a; b]
M[(i; j ); k; a; b]
List

G

Next-Edgei
Prev-Edgei
Local-Prev-Support[(i; j ); a; b]
Local-Next-Support[(i; j ); a; b]
Prev-Support[(i; j ); k; a; b]
Next-Support[(i; j ); k; a; b]

The number of labels in Lk which are compatible with R2 (i; a; j; b).
(k; c) 2 S [(i; j ); k; a; b] means that c 2 Lk is compatible with
R2 (i; a; j; b).
M[(i; j ); k; a; b] = 1 indicates that R2 (i; a; j; b) is false for paths
including i, j , and k.
A queue of path support to be deleted.
G is the set of node pairs (i; j ) such that there exists a directed
edge from i to j .
Next-Edgei contains all node pairs (i; j ) such that there exists a
directed edge (i; j ) 2 G. It also contains (i; end) if i is the last
node in a segment.
Prev-Edgei contains all node pairs (j; i) such that there exists a
directed edge (j; i) 2 G. It also contains (start; i) if i is the first
node in a segment.
A set of elements (i; k) such that (k; i) 2 Prev-Edgei , and if k 6= start,
R2 (i; a; j; b) must be compatible with one of k 's labels. If
Local-Prev-Support[(i; j ); a; b] becomes empty, R2 (i; a; j; b) becomes false.
A set of elements (i; k) such that (i; k) 2 Next-Edgei , and if k 6= end,
R2 (i; a; j; b) must be compatible with one of k 's labels. If
Local-Next-Support[(i; j ); a; b] becomes empty, R2 (i; a; j; b) becomes false.
(i; x) 2 Prev-Support[(i; j ); k; a; b] implies that (x; k) 2 Prev-Edgek , and
if x 6= start, then R2 (i; a; j; b) is compatible with at least one of k's and
one of x's labels. If Prev-Support[(i; j ); k; a; b] becomes empty, then
R2 (i; a; j; b) is no longer true in segments containing i, j , and k .
(i; x) 2 Next-Support[(i; j ); k; a; b] means that (k; x) 2 Next-Edgek , and
if x 6= end, then R2 (i; a; j; b) is compatible with at least one of k's and
one of x's labels. If Next-Support[(i; j ); k; a; b] becomes empty, then
R2 (i; a; j; b) is no longer true in segments containing i, j , and k .

Figure 29: Data structures and notation for MUSE PC-1.

275

fiHelzerman & Harper

l

{...,a,...} n
i

m

o

p

r
j

q

{...,b,...}

s

LocalPrevSupport[(i,j), a, b] = {(i,l), (i,m)}
LocalPrevSupport[(j,i), b, a] = {(j,p), (j,q)}
LocalNextSupport[(i,j), a, b] = {(i,n), (i,o)}
LocalNextSupport[(j,i), b, a] = {(j,r), (j,s)}

Figure 30: Local-Prev-Support and Local-Next-Support for the path consistency of an example DAG. The solid directed lines are members of G, and the solid undirected
line represents the (i; j ) and (j; i) members of E .
Support[(j; i); k; b; a], Next-Support[(i; j ); k; a; b], and Next-Support[(j; i); k; b; a] sets. NextSupport[(i; j ); k; a; b] includes all (i; x) arcs which support R2 (i; a; j; b) given that there is a
directed edge from k to x, R2 (i; a; j; b) = 1, R2 (i; a; k; c) = 1, and R2 (k; c; j; b) = 1 (NextSupport[(j; i); k; b; a] is defined similarly). Prev-Support[(i; j ); k; a; b] includes all (i; x) arcs
which support R2 (i; a; j; b) given that there is a directed edge from x to k, R2 (i; a; j; b) = 1,
R2 (i; a; k; c) = 1, and R2 (k; c; j; b) = 1 (Prev-Support[(j; i); k; b; a] is defined similarly).
Note that Prev-Support[(i; j ); k; a; b] will contain an ordered pair (i; k) if (i; k) 2 PrevEdgek , and (i; j ) if (j; k) 2 Prev-Edgek . Next-Support[(i; j ); k; a; b] will contain an ordered
pair (i; k) if (k; i) 2 Next-Edgek and (i; j ) if (k; j ) 2 Next-Edgek . These elements are included because the edge between those nodes is sucient to allow the support. Dummy
ordered pairs are also created to handle cases where a node is at the beginning or end of
a network: when (start; k) 2 Prev-Edgek , (i; start) is added to Prev-Support[(i; j ); k; a; b],
and when (k; end) 2 Next-Edgek , (i; end) is added to Next-Support[(i; j ); k; a; b].

4.2 The Running Time, Space Complexity, and Correctness of MUSE PC-1
The worst-case running time of the routine to initialize the MUSE PC-1 data structures (in
Figure 32) is O(n3 l3 + n4 l2), where n is the number of nodes in a MUSE CSP and l is the
number of labels. Given that the number of (i; j ) elements in E is O(n2 ) and the domain size
is O(l), there are O(n3 l2) entries in the Counter array for which to determine the number
of supporters, requiring O(l) work; hence, initializing the Counter array requires O(n3 l3)
time. Additionally, there are O(n3 l2) S sets to determine, each with O(l) values, so the
time required to initialize them is O(n3 l3). Determining each Prev-Support[(i; j ); k; a; b]
276

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

z

x
{...,c,...}

{...,a,...}

i

k
w

j {...,b,...}
y

Figure 31: If it is found that Next-Edgek = f(k; x); (k; y )g; Counter[(i; j ); x; a; b] =
0; and Counter[(i; j ); y; a; b] = 0, then R2 (i; a; j; b) is ruled out for every segment containing i, j , and k. The solid directed lines are members of G, and the
solid undirected lines represent members of E .

277

fiHelzerman & Harper

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.

List := ;
E := f(i; j )j9 2  : i; j 2  ^ i 6= j ^ i; j 2 N g;
for (i; j) 2 E do
for a 2 Li do
for b 2 Lj do f
Local-Prev-Support[(i; j ); a; b] := ; Local-Next-Support[(i; j ); a; b] := ;
for k 2 N such that (i; k) 2 E ^ (j; k) 2 E do f
S[(i; j ); k; a; b] := ;
M[(i; j ); k; a; b] := 0;
Prev-Support[(i; j ); k; a; b] := ; Next-Support[(i; j ); k; a; b] := ; g g
for (i; j) 2 E do
for a 2 Li do
for b 2 Lj such that R2 (i; a; j; b) do f
for k 2 N such that (i; k) 2 E ^ (j; k) 2 E do f

Total := 0;

for c 2 Lk do
if R2 (i; a; k; c) and R2 (k; c; j; b) then f

Total := Total+1;
S[(i; k); j; a; c] := S[(i; k); j; a; c] [ f(j; b)g; g
if Total = 0 then f
List := List [ f[(i; j ); k; a; b]g;
M[(i; j ); k; a; b] := 1; g
Counter[(i; j ); k; a; b] := Total;
Prev-Support[(i; j ); k; a; b] :=
f(i; x)j(i; x) 2 E ^ (x = j _ (j; x) 2 E ) ^ (x; k) 2 Prev-Edgek g
[ f(i; k)j(i; k) 2 Prev-Edgek g
[ f(i; start)j(start; k) 2 Prev-Edgek g;
Next-Support[(i; j ); k; a; b] :=
f(i; x)j(i; x) 2 E ^ (x = j _ (j; x) 2 E ) ^ (k; x) 2 Next-Edgek g
[ f(i; k)j(k; i) 2 Next-Edgek g
[ f(i; end)j(k; end) 2 Next-Edgek g; g
Local-Prev-Support[(i; j ); a; b] :=
f(i; x)j(i; x) 2 E ^ (x = j _ (j; x) 2 E ) ^ (x; i) 2 Prev-Edge ig
[ f(i; start)j(start; i) 2 Prev-Edgei g;
Local-Next-Support[(i; j ); a; b] :=
f(i; x)j(i; x) 2 E ^ (x = j _ (j; x) 2 E ) ^ (i; x) 2 Next-Edgei g
[ f(i; end)j(i; end) 2 Next-Edgeig; g

Figure 32: Initialization of the data structures for MUSE PC-1.

278

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

1. while List 6=  do
2.
Pop [(i; j ); k; a; b] from List;
3.
for (k; c) 2 S[(i; j ); k; a; b] do f
4.
Counter[(i; k); j; a; c] := Counter[(i; k); j; a; c] , 1;
5.
Counter[(k; i); j; c; a] := Counter[(k; i); j; c; a] , 1;
6.
if Counter[(i; k); j; a; c] = 0 ^ M[(i; k); j; a; c] = 0 then f
7.
List := List [ f[(i; k); j; a; c]; [(k; i); j; c; a]g;
8.
M[(i; k); j; a; c] := 1; M[(k; i); j; c; a] := 1; g g
9.
Update-Support-Sets([(i; j ); k; a; b]); (see Figure 34) g

Figure 33: Eliminating inconsistent binary constraints in MUSE PC-1.
Update-Support-Sets ([(i; j ); k; a; b])f
1. for (i; x) 2 Prev-Support[(i; j ); k; a; b] ^ x 6= j ^ x 6= k ^ x 6= start do f
2.
Prev-Support[(i; j ); k; a; b] := Prev-Support[(i; j ); k; a; b] , f(i; x)g;
3.
Next-Support[(i; j ); x; a; b] := Next-Support[(i; j ); x; a; b] , f(i; k)g;
4.
if Next-Support[(i; j); x; a; b] =  ^ M[(i; j ); x; a; b] = 0 then f
5.
List := List [ f[(i; j ); x; a; b]; [(j; i); x; b; a]g;
6.
M[(i; j ); x; a; b] := 1; M[(j; i); x; b; a] := 1; g g
7. for (i; x) 2 Next-Support[(i; j ); k; a; b] ^ x 6= j ^ x 6= k ^ x 6= end do f
8.
Next-Support[(i; j ); k; a; b] := Next-Support[(i; j ); k; a; b] , f(i; x)g;
9.
Prev-Support[(i; j ); x; a; b] := Prev-Support[(i; j ); x; a; b] , f(i; k)g;
10.
if Prev-Support[(i; j); x; a; b] =  ^ M[(i; j ); x; a; b] = 0 then f
11.
List := List [ f[(i; j ); x; a; b]; [(j; i); x; a; b]g;
12.
M[(i; j ); x; a; b] := 1; M[(j; i); x; b; a] := 1; g g
13. if (k; i) 2 Prev-Edgei then
14. Local-Prev-Support[(i; j ); a; b] := Local-Prev-Support[(i; j ); a; b] , f(i; k)g;
15. if Local-Prev-Support[(i; j ); a; b] =  then f
16. R2 (i; a; j; b) := 0; R2 (j; b; i; a) := 0;
17. for (i; x) 2 Local-Next-Support[(i; j ); a; b] ^ x 6= j ^ x 6= k ^ x 6= end do f
18.
Local-Next-Support[(i; j ); a; b] := Local-Next-Support[(i; j ); a; b] , f(i; x)g;
19.
if M[(i; j ); x; a; b] = 0 then f
20.
List := List [ f[(i; j ); x; a; b]; [(j; i); x; b; a]g;
21.
M[(i; j ); x; a; b] := 1; M[(j; i); x; b; a] := 1; g g g
22. if (i; k) 2 Next-Edgei then
23. Local-Next-Support[(i; j ); a; b] := Local-Next-Support[(i; j ); a; b] , f(i; k)g;
24. if Local-Next-Support[(i; j ); a; b] =  then f
25. R2 (i; a; j; b) := 0; R2 (j; b; i; a) := 0;
26. for (i; x) 2 Local-Prev-Support[(i; j ); a; b] ^ x 6= j ^ x 6= k ^ x 6= start dof
27.
Local-Prev-Support[(i; j ); a; b] := Local-Prev-Support[(i; j ); a; b] , f(i; x)g;
28.
if M[(i; j ); x; a; b] = 0 then f
29.
List := List [ f[(i; j ); x; a; b]; [(j; i); x; b; a]g;
30.
M[(i; j ); x; a; b] := 1; M[(j; i); x; b; a] := 1; g g g g

Figure 34: The function Update-Support-Sets([(i; j ); k; a; b]) for MUSE PC-1.
279

fiHelzerman & Harper

Approach
CSPs
MUSE CSP

Nodes
Degree of
Number of
Number of
per Path Node splitting Constraint Networks Nodes

n
n

kn

k
k

1

n
kn

Asymptotic
Time

knn3 l3
3
(kn) l3 + (kn)4 l2

Table 2: Comparison of the space and time complexity for MUSE path consistency on a
MUSE CSP to path consistency on multiple CSPs representing a node splitting
problem (e.g., lexical ambiguity in parsing).
and Next-Support[(i; j ); k; a; b] requires O(n) time, so the time required to calculate all
Prev-Support and Next-Support sets is O(n4 l2). Finally, the time needed to calculate all
Local-Next-Support and Local-Prev-Support sets is O(n3 l2) because there are O(n2 l2) sets
with up to O(n) elements per set.
The worst-case running time for the algorithm which enforces MUSE path consistency
(in Figures 33 and 34) also operates in O(n3 l3 + n4 l2) time. Clearly there are O(n3 l2)
entries in the Counter array to keep track of in the algorithm. Each Counter[(i; j ); k; a; b]
can be at most l in magnitude, and it can never become negative, so the maximum running
time for lines 4 and 5 in Figure 33 (given that elements, because of M, appear on the list
only once) is O(n3 l3). Because there are O(n3 l2) Prev-Support and Next-Support lists,
each up to O(n) in size, the maximum running time required to eliminate O(n) elements
from those support sets is O(n4 l2). Finally, since there are O(n2 l2) Local-Next-Support
and Local-Prev-Support sets from which to eliminate O(n) elements, the worst-case time
to eliminate items from the local sets is O(n3 l2). Hence, the worst-case running time of the
MUSE CSP path consistency algorithm is O(n3 l3 + n4 l2).
The space complexity of MUSE CSP PC-1 is also O(n3 l3 + n4 l2 ) because the arrays
Counter and M contain O(n3 l2) elements and there are O(n3 l2) S sets, each containing
O(l) items; O(n3 l2) Prev-Support and Next-Support sets, each containing O(n) items; and
O(n2 l2) Local-Next-Support and Local-Prev-Support sets, each containing O(n) items.
By comparison, the worst-case running time and space complexity for CSP path consistency, PC-4, is O(n3 l3). Note that for applications where  is representable as planar DAG
or l = n, the worst-case running times of the algorithms are the same order. If we compare
MUSE CSP to the use of multiple CSPs for problems where are k alternative variables for
a particular variable in a CSP, then MUSE CSP path consistency can be asymptotically
more attractive, as shown in Table 2.
Because the proof of correctness for MUSE PC-1 is similar to our proof for MUSE AC-1,
we will only briey outline the proof here. A binary constraint looses support in MUSE
PC-1 (see lines 16 and 25 in Figure 34) only if its Local-Prev-Support set or its Local-NextSupport set becomes empty (see lines 15 and 24 in Figure 34, respectively). In either case,
it is inadmissible in any MUSE path consistent instance. We prove that a constraint's local
support sets become empty if and only if it cannot participate in a MUSE path consistent
instance of MUSE CSP. This is proven for Local-Next-Support (Local-Prev-Support follows
by symmetry). Observe that if R2 (i; a; j; b) = 1, and all of the nodes which immediately
280

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

2 = f 1j, 2j, 3jg
1 = f 4j, 5jg

1

2

3

start

end
4

5

Figure 35: An example set of CSP problems which would not be a good candidate for MUSE
CSP because of the lack of node sharing.
follow i (and similarly j ) in the DAG are incompatible with the truth of the constraint,
then it cannot participate in a MUSE path consistent instance. In line 23 of Figure 34,
(i; k) is removed from the Local-Next-Support[(i; j ); a; b] only after [(i; j ); k; a; b] has been
popped off List. The removal of (i; k) from Local-Next-Support[(i; j ); a; b] indicates that
the segment containing i, j , and k does not support R2 (i; a; j; b). It remains to be shown
that [(i; j ); k; a; b] is put on List only if R2 (i; a; j; b) must be false for every segment which
contains i, j , and k. This can be proven by induction on the number of iterations of the
while loop in Figure 33 (much like the proof for MUSE AC-1). We must also show that
if R2 (i; a; j; b) = 1 after MUSE PC-1, then it is MUSE path consistent. For R2 (i; a; j; b)
to be MUSE path consistent, there must exist at least one path from start to end which
goes through nodes i and j such that all nodes n on that path contain at least one label
consistent with the constraint. This proof would be similar to the second half of the proof
for MUSE AC-1 correctness. From this, we may conclude that MUSE PC-1 builds the
largest MUSE path consistent structure.

5. Combining CSPs into a MUSE CSP
Problems which have an inherent lattice structure or problems which can be solved by
the node splitting approach are natural areas of application for MUSE CSP, because an
exponential number of CSPs are replaced by a single instance of MUSE CSP, and the DAG
representation of  is inherent in the problem. In this section we discuss DAG construction
for other application areas which would benefit from the MUSE CSP approach, but for
which it is not as obvious how to construct the DAG. Any set of CSP problems can be
used as the segments of a MUSE CSP. For example, Figure 35 illustrates how two instances
of a CSP can be combined into a single MUSE CSP. However, using MUSE CSP for this
example would not be the right choice; node sharing cannot offset the cost of using the
extra MUSE AC-1 data structures.
Multiple nodes which have the same name in various CSPs can potentially be represented
as a single node in a MUSE CSP. We assume that if two nodes, k1 and k2 are given the
same name (say k) in two instances of CSP, then they have the same domain and obey the
same constraints, i.e.:
1. Lk1 = Lk2 (i.e., their domains are equal.)
2. R1(k1; a) = R1(k2; a) for every a 2 Lk1 ; Lk2 (i.e., their unary constraints are the
same.)
281

fiHelzerman & Harper

1 = f 1j, 2jg
2 = f 1j, 3jg
3 = f 2j, 3jg

1

2
start

end

1

3

2

start

end

2

3

Figure 36: An example of how maximal node sharing leads to spurious segments. The
first DAG contains two paths, f1,2,3g and f2g, which correspond with none of
the segments. The second DAG presents a preferred sharing as created by the
Create-DAG routine.
3. R2(k1; a; i; b) = R2(k2; a; i; b) for labels a 2 Lk1 ; Lk2 and b 2 Li , where i is in both
segments (i.e., their binary constraints are the same.)
However, as illustrated in Figure 36, too much sharing of common nodes can introduce
additional segments that do not appear in the original list of CSPs. Because the extra
segments can cause extra work to be done, it is often desirable to create a DAG which
shares nodes without introducing extra segments. The algorithm Create-DAG, shown in
Figure 38 takes an arbitrary set of CSP problems as input (a list of segments), and outputs
a DAG representation for those CSPs which shares nodes without introducing spurious
segments. Create-DAG calls an auxiliary procedure Order-Sigma defined in Figure 39.
The data structures used in these two routines are defined in Figure 37.
To hold the individual segments in , the routine Create-DAG uses a special data
structure for ordered sets which supports some useful operations. If  is a segment and n
is an integer, then  [n] is the node at position n in  .  [0] is always the start node, and
 [j j , 1] is always the end node.  [k::m] is the ordered subset of  consisting of all the
nodes in positions k through m. In addition, the ordered set allows us to insert a node i
immediately after any node j which is already in the set. Each node  [pos] is a structure
with a name field and a next-set field, which is the set of names of nodes that follow the
node  [pos] in a set of segments.
Create-DAG begins by adding special purpose start and end nodes to each segment.
It then calls the routine Order-Sigma shown in Figure 39 to order the nodes in each
segment. Order-Sigma orders the nodes of each segment such that the ones that are the
most common tend to occur earlier in the set. To order the elements, it uses the operator
> (i.e., larger than) which is defined on nodes. Note that the start node is defined to be
the \largest" node, and the end node the \smallest" node. In addition, i > j means either
that i appears in more segments than j does, or if they both appear in the same number of
segments, then i has a lower ordinal number than j . Thus the operator > induces a total
ordering on the nodes in N .
When Order-Sigma is first called by Create-DAG it selects the largest node i which
is smaller than the start node. It then constructs the set S , which is a set of those segments
containing i. At this point, the segments in S are ordered such that the start node is first
and i is second. It then calls Order-Sigma to order S for nodes smaller than i. Once the
recursive call is done, any segments that were not in S are considered (i.e., Z ). Note that
282

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

Notation

Meaning



A set of node sets. Each node set represents a CSP.
 is a node set or segment in . Each  set is modified to include
begin and end nodes for the Create-DAG algorithm to work
properly. Note that [0] is always the start node, and [jj , 1] is
always the end node. Each node [pos] is a structure with a name
and a next-set (names of nodes that follow the node in the DAG).
G is the set of node pairs (i; j ) such that there exists a
directed edge from i to j in the DAG created by Create-DAG.
N is the set of nodes that have been placed in the DAG by
Create-DAG.
Z is a set of segments to order with respect to node j in
Order-Sigma.
The node j is used in Order-Sigma to order the remaining
elements which are smaller than that node.
U is a set of nodes already considered in the current call to
Order-Sigma.
R is a set of nodes in Z in Order-Sigma.
The node i is the largest node which is smaller than j in R , U
(if non-empty) or R in Order-Sigma.
In Order-Sigma, S is a set of segments in Z which contain node i.


G
N
Z
j
U
R
i
S

Figure 37: Data structures used in Create-DAG and Order-Sigma.

283

fiHelzerman & Harper

Create-DAG () f

1.
2.
3.
4.
5.
6.
7.
8.
9.

10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.

Add start as the first node and end as the last node for every segment in ;
Order-Sigma(, start);
for pos := 1 to maximum segment length f
 := copy();
for  2  ^ jj , 1 > pos f
if [pos].name = end then f
G := G [ f([pos , 1], [pos])g; g
0

0

else f

SAME EDGE SET := f1j 1[pos , 1].name = [pos , 1].name ^
1[pos].name = [pos].nameg;
next-set := f[pos + 1].name j 2 SAME EDGE SETg;
 :=  , SAME EDGE SET;
if a node with [pos].name is not in N then f
N := N [ [pos];
[pos].next := next-set;
G := G [ f([pos , 1], [pos])g; g
0

0

else f
node := get the node in N with the name [pos].name;
if node.next = next-set then f
G := G [ f([pos , 1]; [pos])g; g
else f

new-node : = Create a new node;
new-node.name := concatenate([pos].name, ');
node := get the node in N named new-node.name (if there is one);
while node && node.next !=next-set do f
new-node.name := concatenate(new-node.name, ');
node := get the node in N named new-node.name (if there is one); g
if (node = NULL) then
N := N [ new-node;
else new-node := node;
new-node.next := next-set;
Replace [pos].name with new-node.name in [pos , 1].next;
G := G [ f([pos , 1]; new-node)g;
Replace every occurance of [pos] at pos with new-node
in all segments of SAME EDGE SET; g g g g g
Eliminate start and end from G and from each  2 ; g

Figure 38: Routine to create a DAG to represent .

284

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

Order-Sigma (Z; j ) f
1. U := ;
2. while Z 6=  f
3.
R :=
;

[

4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.

 2Z

if R , U 6=  then
i := the \largest" node in R , U which is less than j ;
else
i := the \largest" node in R which is less than j ;
S := fj 2 Z ^ i 2 g ;
Z := Z , S ;
if i =6 end then f
for  2 S f
Put i after j in ;
U := U [ ; g
Order-Sigma(S; i) g g g

Figure 39: The routine to arrange the nodes within the segments for convenient merging.
after the first iteration of the loop, there is a preference to select the largest node that was
not contained in the segments that were ordered by the recursive call to Order-Sigma.
These items are independent of the ordered segments, and so will not create spurious paths
when placed early in the DAG; however, items that occur in the already ordered segments, if
placed earlier than items that do not occur in the ordered segments would tend to introduce
spurious paths. The while loop continues until all segments in  are ordered. The worstcase running time of Order-Sigma is O(n2 ), where n is the sum of the cardinalities of the
segments in .
Once Order-Sigma orders the nodes in the segments, Create-DAG begins to construct the DAG, which is represented as a set of nodes N and a set of directed edges G.
The DAG is constructed by going through each segment beginning with the position of the
second element (the position after start). The for loop on line 3 looks at nodes in a left to
right order, one position at a time, until all the elements of each segment have been added
to G. If a node with a certain name has not already been placed in N (i.e., the set of nodes
already in the DAG being created) then adding the node to the graph (as well as a directed
edge between  [pos , 1] and  [pos] to G) cannot create any spurious paths in the DAG. On
the other hand, if a node with the same name as  [pos] had already been placed in N , then
it is possible that the current segment could add paths to the DAG that do not correspond
to any of the segments in . To avoid adding spurious segments, we deal with all segments
at one time that share the same previous node and have a node with the same name at the
current position. The basic idea is to add that edge only once and to keep track of all nodes
that can follow that node in the DAG. By doing this, we can easily determine whether that
same node can be used if it occurs in another segment in a later position. The same node
can be used only if it is followed by precisely the same set of next nodes that follow the
node already placed in the graph; otherwise, the second node would have to be renamed to
avoid adding spurious segments. In such an event, we create a new name for the node.
285

fiHelzerman & Harper

Note that once the DAG is complete, we eliminate the start and end nodes from G
(and their corresponding outgoing and incoming edges) to make G consistent with its use
in the MUSE arc consistency and MUSE path consistency algorithms. The running time of
Create-DAG is also O(n2 ), where n is the sum of the cardinalities of the segments in .
Even though the DAGs produced by the routine Create-DAG do have nice properties,
this routine should probably be used only as a starting point for custom combining routines
which are specific to the intended application area. We believe that domain-specific information can play an important role in MUSE combination. An example of a domain specific
combining algorithm is presented in (Harper et al., 1992), which describes a spoken-language
parsing system which uses MUSE CSP. A distinguishing feature of this application's combining algorithm is that instead of avoiding the creation of extra segments, it allows controlled
introduction of extra segments because the extra segments often represent sentences which
an N-Best sentence spoken language recognition system would miss.

6. Conclusion

In conclusion, MUSE CSP can be used to eciently represent several similar instances of the
constraint satisfaction problem simultaneously. If multiple instances of a CSP have some
common variables with the same domains and compatible constraints, then they can be
combined into a single instance of a MUSE CSP, and much of the work required to enforce
node, arc, and path consistency need not be duplicated across the instances, especially if
the constraints are suciently tight.
We have developed a MUSE CSP constraint-based parser, PARSEC (Harper & Helzerman, 1995a; Harper et al., 1992; Zoltowski et al., 1992), which is capable of parsing word
graphs containing multiple sentence hypotheses. We have developed syntactic and semantic
constraints for parsing sentences, which when applied to a word graph, eliminate those hypotheses that are syntactically or semantically incorrect. For our work in speech processing,
the MUSE arc consistency algorithm is very effective at pruning the incompatible labels for
the individual CSPs represented in the composite structure. When extracting each of the
parses for sentences remaining in the MUSE CSP after MUSE AC-1, it is usually unnecessary to enforce arc consistency on the CSP represented by that directed path through the
network because of the tightness of the syntactic and semantic constraints.
Speech processing is not the only area where segmenting the signal into higher-level
chunks is problematic. Vision systems and handwriting analysis systems have comparable
problems. In addition, problems that allow for parallel alternative choices for the type of
a variable, as in parsing lexically ambiguous sentences, are also excellent candidates for
MUSE CSP.
C++ implementations of the algorithms described in this paper are available at the following location: ftp://transform.ecn.purdue.edu/pub/speech/harper code/. This directory
contains a README file and a file called muse csp.tar.Z.

286

fiMUSE CSP: An Extension to the Constraint Satisfaction Problem

Acknowledgements
This work was supported in part by the Purdue Research Foundation and a grant from
Intel Research Council. We would like to thank the anonymous reviewers for their insightful
recommendations for improving this paper.

References

Bessiere, C. (1994). Arc-consistency and arc-consistency again. Artificial Intelligence, 65,
170{190.
Davis, A. L., & Rosenfeld, A. (1981). Cooperating processes for low-level vision: A survey.
Artificial Intelligence, 17, 245{263.
Dechter, R. (1992). From local to global consistency. Artificial Intelligence, 55, 87{107.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49, 61{95.
Dechter, R., & Pearl, J. (1988). Network-based heuristics for constraint-satisfaction problems. Artificial Intelligence, 34, 1{38.
Freuder, E. (1989). Partial constraint satisfaction. In Proceedings of the International Joint
Conference on Artificial Intelligence, pp. 278{283.
Freuder, E. (1990). Complexity of K-tree-structured constraint-satisfaction problems. In
Proceedings of the Eighth National Conference on Artificial Intelligence, pp. 4{9.
Han, C., & Lee, C. (1988). Comments on Mohr and Henderson's path consistency algorithm.
Artificial Intelligence, 36, 125{130.
Harper, M. P., & Helzerman, R. A. (1995a). Extensions to constraint dependency parsing
for spoken language processing. Computer Speech and Language, 9 (3), 187{234.
Harper, M. P., & Helzerman, R. A. (1995b). Managing multiple knowledge sources in
constraint-based parsing of spoken language. Fundamenta Informaticae, 23 (2,3,4),
303{353.
Harper, M. P., Jamieson, L. H., Zoltowski, C. B., & Helzerman, R. (1992). Semantics and
constraint parsing of word graphs. In Proceedings of the International Conference on
Acoustics, Speech, and Signal Processing, pp. II{63{II{66.
Mackworth, A. K. (1977). Consistency in networks of relations. Artificial Intelligence, 8 (1),
99{118.
Mackworth, A. K., & Freuder, E. (1985). The complexity of some polynomial networkconsistency algorithms for constraint-satisfaction problems. Artificial Intelligence, 25,
65{74.
287

fiHelzerman & Harper

Maruyama, H. (1990a). Constraint dependency grammar. Tech. rep. #RT0044, IBM,
Tokyo, Japan.
Maruyama, H. (1990b). Constraint dependency grammar and its weak generative capacity.
Computer Software.
Maruyama, H. (1990c). Structural disambiguation with constraint propagation. In The
Proceedings of the Annual Meeting of ACL, pp. 31{38.
Mohr, R., & Henderson, T. C. (1986). Arc and path consistency revisited. Artificial Intelligence, 28, 225{233.
Montanari, U. (1974). Networks of constraints: Fundamental properties and applications
to picture processing. Information Science, 7, 95{132.
van Beek, P. (1994). On the inherent level of local consistency in constraint networks. In
Proceedings of the Twelfth National Conference on Artificial Intelligence, pp. 368{373.
Villain, M., & Kautz, H. (1986). Constraint-propagation algorithms for temporal reasoning.
In Proceedings of the Fifth National Conference on Artificial Intelligence, pp. 377{382.
Waltz, D. L. (1975). Understanding line drawings of scenes with shadows. In Winston, P.
(Ed.), The Psychology of Computer Vision. McGraw Hill, New York.
Zoltowski, C. B., Harper, M. P., Jamieson, L. H., & Helzerman, R. (1992). PARSEC: A
constraint-based framework for spoken language understanding. In Proceedings of the
International Conference on Spoken Language Understanding, pp. 249{252.

288

fiJournal of Artificial Intelligence Research 5 (1996) 1-26

Submitted 12/95; published 8/96

Spatial Aggregation: Theory and Applications
Kenneth Yip

MIT Artificial Intelligence Laboratory, 545 Technology Square
Cambridge, MA 02139 USA

yip@martigny.ai.mit.edu

Feng Zhao

fz@cis.ohio-state.edu

Department of Computer and Information Science, The Ohio State University
Columbus, OH 43210 USA

Abstract

Visual thinking plays an important role in scientific reasoning. Based on the research in
automating diverse reasoning tasks about dynamical systems, nonlinear controllers, kinematic mechanisms, and uid motion, we have identified a style of visual thinking, imagistic
reasoning. Imagistic reasoning organizes computations around image-like, analogue representations so that perceptual and symbolic operations can be brought to bear to infer
structure and behavior. Programs incorporating imagistic reasoning have been shown to
perform at an expert level in domains that defy current analytic or numerical methods.
We have developed a computational paradigm, spatial aggregation, to unify the description of a class of imagistic problem solvers. A program written in this paradigm has the
following properties. It takes a continuous field and optional objective functions as input,
and produces high-level descriptions of structure, behavior, or control actions. It computes
a multi-layer of intermediate representations, called spatial aggregates, by forming equivalence classes and adjacency relations. It employs a small set of generic operators such
as aggregation, classification, and localization to perform bidirectional mapping between
the information-rich field and successively more abstract spatial aggregates. It uses a data
structure, the neighborhood graph, as a common interface to modularize computations.
To illustrate our theory, we describe the computational structure of three implemented
problem solvers { kam, maps, and hipair | in terms of the spatial aggregation generic
operators by mixing and matching a library of commonly used routines.

1. Introduction

It is commonly believed that there are two styles of scientific thinking: analytical, a logical
chain of symbolic reasoning from premises to conclusions, and visual, the holding of imagistic, analogue representations of a problem in one's mind so that perceptual and symbolic
operations can be brought to bear to make inferences. Neither style is to be preferred a
priori over the other. However, for problems whose complexity precludes a direct analytical
approach, a certain amount of qualitative and visual imagination is needed to provide the
necessary \feel" or \understanding" of the physical phenomena. Once the picture is clear,
the analytical mathematics can take over and lead more eciently to logical conclusions.
This \feel and physical understanding" is often considered to be informal, imprecise, and
apparently unteachable, but necessary for scientists and engineers.
We believe part of this ability to visualize and imagine must consist of skills to generate
images, discover structures and relations in the images, transform the structures, and predict
how the structures respond to internal dynamics or external forcing.

c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiYip & Zhao

While most AI work in visual reasoning has focused on diagrams and their role in
controlling search, in recent years we have seen the development of a class of problem
solvers that are imagistic, i.e., the problem solvers derive their power primarily from the
use of visual apparatus and only secondarily from search and analytical methods. These
problem solvers have been designed to perform tasks in many different domains: control
and interpretation of numerical experiments (Yip, 1991; Nishida & et al., 1991; Zhao,
1994), kinematics analysis of mechanisms (Joskowicz & Sacks, 1991), design of controllers
(Zhao, 1995; Bradley, 1992), analysis of seismic data (Junker & Braunschweug, 1995), and
reasoning about uid motion (Yip, 1995). However, there are important commonalities
underlying them. In this paper, we present a framework to provide a unified description of
this class of problem solvers. Our framework consists of three ideas:

 The field ontology: The input is a field, a mapping from one continuum to another.

It is an image-like analogue representation. The field is assumed to have a metric so
that it is meaningful to talk about closeness and continuity.1
 Structure discovery: A central problem to be solved is the transformation of the
information-rich input to abstractions well-suited for concise structural and behavioral
descriptions. The transformation can be thought of as successive mappings of the
input space into more abstract spaces that hide details and group similar objects into
equivalence classes.
 Multi-layer spatial aggregates: We propose (1) as representation the neighborhood
graph to encode explicitly adjacency relations among objects at one level of abstraction,
and (2) as building blocks of computational processes a small set of generic operators
to construct, transform, classify, and search the neighborhood graph. The operators
are recursively used to implement task-specific applications. The multi-layer theory
has two advantages: (1) A nonlocal property of a lower layer can be redescribed as
a local property of a higher layer, and (2) On each layer the neighborhood graph
provides a common interface to support identical modular computations.
A field is a mapping from one continuum (say Rm ) to another (say Rn ). More concretely,
one can visualize a m-dimensional space with a n-vector attached to each point in the space.
Fields are commonplace in science and engineering applications. They are used to describe
how physical quantities vary over space and time. Temperature in a room is a threedimensional scalar field. Weather data can be described as a 4D-spacetime field with a 6vector attached to each point: velocity (three components) of air ow, temperature (scalar),
pressure (scalar), and density (scalar). Other examples of fields include the brightness
intensity array in vision, the configuration space in mechanism analysis, and the phase
space (vector field) of dynamical systems.
In actual computer representations, we often approximate a field with a grid. The grid
may be uniform or non-uniform. The field can be reconstructed from numerical simulation
1. Forbus et al. (1991) proposed a general methodology for qualitative spatial reasoning: the Metric Diagram/Place Vocabulary (MD/PV). We generally agree with their methodology. Their paper inspired
us to look for a more refined framework to unify a class of problem solvers that integrate visual and
symbolic reasoning.

2

fiSpatial Aggregation: Theory and Applications

or measurements. A field does not contain any symbolic abstractions; it is completely
numerical. Fields are composable. One can extend the dimension of the underlying space
and/or the number of components in the vector attached to each point of the space.
As a representation for physical systems, a field has two distinguishing characteristics.
First, it is information-rich in the sense of the Shannon-Weaver measurement of information.
An instantaneous field of a 1283 -grid ow simulation may contain on the order of 108 bits
of information. Second, it is pictorial in the sense that structures and relations are only
implicitly represented in the field.
As a consequence of both the information-richness and the pictorial quality, we argue
that in reasoning about fields the central computational problem is the ecient transformation of a pointwise field description of a physical system into economical symbolic abstractions well suited for explaining the structure and behavior of the system.2 Figure 1 illustrates
how the field ontology relates to the other commonly used ontologies in Qualitative Physics:
device (DeKleer & Brown, 1984), process (Forbus, 1984), and constraint (Kuipers, 1986).
To be useful, the symbolic descriptions must impose a conceptual structure on the system
so that the complexity of the system can be understood in terms of well-defined parts and
subparts and interactions among them. The relevant parts and interactions are often abstract global properties of the field. An abstract property is a property whose support is
large and nonlocal, whereas the support of a property is defined as the subset of a field
on which the property depends. On the other hand, for computational complexity reasons
we prefer to build the recognition procedures from basic routines that are local and independent of task-level information as much as possible. These considerations lead us to
adopt an architecture where the pointwise description and the final symbolic descriptions
are mediated by layers of equivalence classes of objects with explicit adjacency relations.
We call such a layer of objects a spatial aggregate.
Where do spatial aggregates come from? In a real field, there tend to be continuities of
properties (such as intensity or temperature or pressure) so that the field can be divided into
equivalence classes, i.e., open regions where a particular property varies in an approximately
uniform way. With continuities we can achieve an economy of description by focusing on the
open regions and their boundaries instead of the pointwise field. Higher-order continuities,
i.e., continuities of properties defined on the open regions, can similarly be used to build
more abstract spatial aggregates.
The formation of equivalence classes presupposes the existence of continuity. This brings
us to a methodological point. It is important to clearly identify the source of continuities in
the field or equivalently in the physical system the field represents. The discovery of valid
and general continuities in the physical system is as much a scientific contribution as the
subsequent computational use of them to form an articulated conceptual model to explain
structure and behavior.
Our motivation for this paper comes from the desire to understand the computational
structures shared by a class of automatic problem solvers that integrate visual, symbolic,
and numerical methods. We would like to make this computational structure explicit so that
comparisons and generalizations can be made. Our goal is to develop a way of organizing
2. Inferring structural descriptions from a field can be an ill-posed problem (e.g., recovering 3D shapes from
2D images). To avoid these diculties, we will assume the structure-recovery problem to be well-posed
so that our main concerns are computational eciency and appropriate abstractions.

3

fiYip & Zhao

modeling

HARD!
analytical methods

differential
equations
numerical
simulation

physical
system

measurement

modeling

field

interpret

equivalence
class clustering

structural
description

device

modeling

process

modeling

constraint

analytical
functions

qualitative
behavioral
description

envisionment
incremental analysis
process inference
qualitative simulation

Figure 1: Field as an ontological abstraction for reasoning about physical systems. The
diagram depicts the relationships among different ontologies used in Qualitative
Physics. The central computational problem in field reasoning is the recovery
of economical structural descriptions for qualitative behavior description and explanation. A key step in the structural recovery is the formation of equivalence
classes. Identifying general and valid continuities on which useful equivalence
class relations are based is an important scientific contribution.
programs around image-like analogue representations, and an appropriate language to make
programs written in this style clear.
The next section develops the theory of spatial aggregation in detail. Section 3 describes
a language to support programs that are organized around neighborhood graphs. Section 4
illustrates the usefulness of the language by describing succinctly the computation structure
of three implemented programs { kam, maps, and hipair. We choose these programs as
illustrations largely because of our familiarity with them. Section 5 shows how to program
in the spatial aggregation language, using an example from image analysis. We plan to
investigate the applicability of our framework to several other programs, such as those
constructed by Kuipers and Levitt (1988), Forbus et al. (1991), Gelsey (1995), and Junker
and Braunschweug (1995).

4

fiSpatial Aggregation: Theory and Applications

2. Spatial Aggregation Theory

Given a field, a spectrum of reasoning tasks can be defined. The following list is roughly in
the order of increasing complexity:
 Infer structural descriptions. Find out objects, if any, that exist in the field.
What are their shapes, sizes, and locations? How are they distributed? How are they
created? How do they evolve as some parameter (say time) is varied?
 Classify. Assign semantic labels to objects and configurations.
 Infer correlations. Determine how the geometry and distribution of one type of
objects correlate with those of another type?
 Check consistency. Given two objects or configurations, test if they are equivalent
or if they are pairwise consistent.
 Infer incremental behavior. Given an instantaneous configuration, predict its
possible short-term behaviors.
 Infer behavioral descriptions. Explain and summarize the evolution of objects by
a set of domain-specific interaction rules.

2.1 Requirements of imagistic reasoning

Partly motivated by Ullman's theory of visual analysis (Ullman, 1984), we find desirable
the following general requirements on imagistic reasoning:

 Abstractness. The problem solver should be able to find objects defined by abstract
global properties.

 Open-endedness. The problem solver architecture should be applicable to a variety

of domains (uid motion, seismic data, weather data, phase space, or configuration
space). This requirement implies that the basic recognition routines must be modular
and composable. Task-specific knowledge affects the choice and ordering of these
routines.
 Eciency. The \building blocks" of the recognition machinery must be local and
non-goal-specific. \Non-goal-specific" means the operations of the building blocks do
not depend on the interpretation of the objects they manipulate. This requirement
implies that the basic routines should have local supports and in principle can run in
parallel.
 Soundness. The structural and behavioral descriptions must be consistent with
known physical and mathematical principles.
 Succinctness. The structural and behavioral descriptions should contain the qualitatively important distinctions relevant to the high-level tasks at hand.
5

fiYip & Zhao

2.2 Theory

Our theory of imagistic reasoning postulates the existence of multi-layers of spatial aggregates. Figure 2 shows the layers of spatial aggregates and computations organized around
them. A primitive aggregate is defined as an equivalence class of subsets of the pointwise
field representation. An aggregate is composed of equivalence classes of primitive aggregates. The field is assumed to have a task-dependent metric. The metric induces a topology
on the space and hence it is meaningful to talk about adjacency. The data structure for a
spatial aggregate is a neighborhood graph whose nodes represent objects and edges represent adjacency relations among the objects. The input field is sampled to form the lowest
layer of abstraction; the field can also be affected by control actions from the higher-level
abstraction layers.
Just as the stream construct in the scheme programming language provides a common
interface for organizing signal processing computations, the neighborhood graph is our conceptual glue for piecing together operations that manipulate fields. We like to visualize
nodes of neighborhood graph as open sets (in topology) in some appropriate space. Two
nodes are adjacent if their respective open sets are contiguous.3
The topological notion of adjacency is amazingly useful in reasoning about physical systems. In grouping objects into equivalence classes, a cluster tends to give rise to a connected
component of the neighborhood graph. In reasoning about kinematics, the neighborhood
graph provides the essential connectivity information among free space regions. In finding
\interesting" structures, the pairwise consistency of the adjacent nodes localizes search regions. In isolating bifurcation patterns, the mismatch of adjacent objects provides a hint
for further analysis. In constraint propagation and path search, the adjacency structure
imposes locality to increase computational eciency. Prevalence and simplicity { these two
aspects of the neighborhood graph make it a powerful data structure for unifying many
spatial computations.
Our theory revolves around the computation of the neighborhood graph and the nature
of the processes that construct, filter, transform, and compare neighborhood graphs. We
isolate a set of generic operators aggregate, classify, re-describe, and search which correspond
to the important conceptual pieces common to a class of imagistic problem solvers such as
kam (Yip, 1991), maps (Zhao, 1994), and hipair (Joskowicz & Sacks, 1991).
The next section discusses these operators in detail. Section 4 illustrates the use of these
operators in a rational reconstruction of three implemented computer programs.

3. The Language of Spatial Aggregation

We present a language for describing computational processes organized around spatial
aggregates. The language provides a small set of operators to construct and manipulate
neighborhood graphs. The operators make the conceptual structure of several implemented
programs clear.
3. Let A and B be two open sets. A and B are contiguous if either A \ B 6= ; or B \ A 6= ; where A is the
closure of the set A. In particular, if A and B overlap, then they are contiguous.

6

fiSpatial Aggregation: Theory and Applications

Model

classify
search

aggregate

Structural
description

N-graph
consistent?

primitive
objects
filter

map

incremental
analyze

re-describe

Behavioral
description

localize

classify
search

aggregate

Structural
description

N-graph
consistent?

primitive
objects
filter

map

incremental
analyze

Behavioral
description

control

sample

FIELD

Figure 2: A schematic representation of the computational structure for analysis of a field
ontology. There are multi-layers of spatial abstraction. An abstraction level is defined by the neighborhood graph, a data structure representing spatial aggregates
and adjacency relations. The input field is fed to the lowest abstraction layer.
Note the identical computational structure on each layer. The aggregate operator
computes adjacency relations based on a task-specific metric. The neighborhood
graph is the common interface for map and filter routines. The remaining operations correspond to the generic analysis tasks. A repertoire of task-independent
geometric manipulation routines (which are not shown) are accessible by the
generic operators.
7

fiYip & Zhao

3.1 Task-level operators

The task-level generic operators consist of aggregate, classify, re-describe, localize,
search, incremental-analyze, together with the predicates pairwise-consistent? and
consistent?. The neighborhood graph is the \conceptual glue": it allows the computation
of hierarchical structural descriptions to be organized in a uniform manner. The following
box summarizes what the language provides and what a user needs to supply in order to
write programs in spatial aggregation.

Language Features
 User interface functions:
aggregate, classify, re-describe, localize, search,
incremental-analyze, pairwise-consistent?, consistent?

A user must specify the neighborhood relation, field metric, and equivalence
relation for these operators.

 Data types:
{ N-graph and its constructors, accessors, modifiers.

Examples of N-graph include 4-adjacency arrays, minimal spanning tree,
and Voronoi diagram.
{ Fields:
bitmap, vector field, etc.
 Libraries:
{ Geometric utilities:
intrinsic-geometry, contain?, intersect, @ ,  .
{ Numerical and image processing routines:
FFT, convolution, integrator, linear system solver, vector/matrix algebra.
1.

2.

aggregate(objects combiner)

The aggregate operator assembles a collection of objects into a spatial structure
using the combiner procedure and explicates the spatial relations among the objects
in terms of the neighborhood graph.4 The operator returns a neighborhood graph
(N-graph). The N-graph can be lazily built.
For example, to recognize a trajectory in a phase space, the aggregate operator might
be given a set of discrete points and a combiner procedure (such as minimal spanning
tree) to establish adjacency relations. The combiner procedure might use a metric or
topological properties of the underlying space.
classify(N-graph cluster-proc class-rules)

4. Recall the nodes in a neighborhood graph are objects and edges are adjacency relations.

8

fiSpatial Aggregation: Theory and Applications

The classify operator forms equivalence classes according to an equivalence relation
(using the cluster-proc), and assigns a semantic label to each equivalence class | a
subgraph of the input N-graph | according to the classification rules. For example,
the orbit clustering procedure groups orbits into ow pipes.5 The classification rules
are a set of production rules. The operator returns a labeled N-graph.
The catalog of the classification labels is domain-specific. These classification labels
serve as indices for storage and retrieval of shared class properties and methods for
instantiating them.
3.

re-describe(N-graph desc-type)

4.

localize(N-graph select-proc enumerate-proc)

5.

6.

The re-describe operator changes the representation of a primitive object. Like a
lambda abstraction in scheme, this operator allows a compound object (say a subset
of a N-graph) to be treated as a primitive.
Given a classified object, the description-type procedure instantiates additional properties specific to that class of objects. For example, if a point set is classified as
a space curve, it becomes sensible to compute additional geometric properties like
length, curvature, and torsion.
The localize operator systematically enumerates members of an equivalence class
(nodes of N-graph) and selects those according to the select procedure. This operator
\opens up" an abstraction to allow individual members of the equivalence class to be
singled out.
search(N-graph initial-states goal-p combiner)

The search operator returns paths starting from the initial-states and satisfying the
goal-p predicate. The combiner procedure controls the order in which the graph is
traversed.
incremental-analyze(N-graph state-desc delta)

Given a N-graph and a description of states and constituent laws, the incrementalanalyze operator computes the infinitesimal change to the qualitative state due to
a small perturbation. The perturbation delta might be in the temporal, state, or
parameter space.
There are predicates pairwise-consistent? and consistent?:



pairwise-consistent?(obj1 obj2 consistency-rules)



consistent?(obj consistency-rules)

The pairwise-consistent? predicate decides if two objects are consistent according
to the consistency-rules. The objects can be primitive objects such as nodes of an
N-graph or N-graphs themselves.
Consistent?

tests if an object is well-formed according to the consistency-rules.

5. A ow pipe is a class of orbits that can be continuously deformed into each other. It is an example of
the homotopy equivalence class.

9

fiYip & Zhao

3.2 Generic data structure and routines
The neighborhood graph is constructed by



N-graph-constructor(objects neighbor-p)



map(N-graph proc)



filter(N-graph mask)

The N-graph-constructor takes a set of primitive objects and a neighborhood predicate
as arguments, and returns a neighborhood graph. An example of such a neighborhood graph is the Voronoi diagram. The predicate neighbor-p tests if two nodes are
neighbors.
The set of task-independent routines operate on the objects in the neighborhood graphs
and support the task-level operations.
The map routine transforms a neighborhood graph using a prespecified procedure.
A filter selects a subset of the neighborhood graph for further processing.

In addition to the generic operators, the language provides routines to perform common
geometric manipulation. The following routines are especially useful:
1.
2.
3.
4.
5.
6.

computes intrinsic geometric properties of
objects (e.g., area, curvature, surface normal).
contain?(obj1 obj2) checks if obj2 is inside obj1.
intersect(obj1 obj2) computes intersection of two objects.
@(object) is the boundary operator that returns the boundary of an object. The
dimension of boundary is co-dimension 1.
(object) is the co-boundary operator that returns a new object whose boundary is
the object. The dimension of the new object is one higher than that of the object.
convolve(object mask) performs pointwise convolution with the given mask.

intrinsic-geometry(obj properties)

4. Examples of Spatial Aggregation

In this section, we describe the architecture of three implemented systems kam (Yip, 1991),
maps (Zhao, 1994), and hipair (Joskowicz & Sacks, 1991) in terms of the spatial aggregation
framework. Although these programs are designed for different tasks, their computations
share a strikingly similar pattern: These programs construct spatial objects, and interpret
them via multi-layers of abstraction by object aggregation, classification, and re-description.
Composite objects at a lower level are labeled and manipulated as primitive units at the
next higher level.
Despite the fact that we are the authors of two of these programs, the structural similarities among these programs are not apparent to us until we carefully reconstructed these
10

fiSpatial Aggregation: Theory and Applications

programs by defining the appropriate neighborhood graphs and generic operators. Analyzing these programs in a common framework will help us to understand not only what the
programs do, but also greatly enhance our ability to construct future programs by a few
spatial aggregation operators.

4.1 KAM

The task for kam is to explore the dynamics of Hamiltonian systems and produce high-level
summaries of their qualitative behaviors.
Given the state equations of a Hamiltonian system, kam derives a symbolic description
of its qualitative behavior | in terms of orbit types,6 orbit bundles, phase portraits, and
bifurcation patterns | from a collection of point sets representing orbits (or trajectories) in
the phase space (see Figure 3). The point sets can be obtained from numerical simulation
or measurements. To provide a useful interpretation of the point set, kam has to decide (1)
where to look for interesting orbits, and (2) how to group these orbits into larger structures.
Kam proceeds via a sequence of intermediate representations that allow the gradual recovery
of orbit structures and eventually the more global dynamical properties of the system. Kam
is able to view an object at multiple levels of abstraction. For example, an orbit can be
viewed as points in the phase space or a curve or part of an orbit bundle.
The computations in kam are organized into four layers (as shown in Figure 4): (1) orbit,
(2) orbit bundle, (3) phase portrait, and (4) bifurcation pattern. We will walk through the
first level in sucient detail to illustrate how the computation is synthesized from the
spatial aggregation operators and neighborhood graph. Details of the remaining levels are
described by Yip (1991).
The input is a point set. The aggregate operator imposes an adjacency relation on
the point set by constructing a minimal spanning tree (MST). Two points are adjacent or
neighbors if they are connected by an edge in the MST. Although the MST is appropriate
for orbit interpretation, other applications might require different adjacency relations (such
as Voronoi diagrams or k-nearest neighbors). The output of the aggregate operator is a
neighborhood graph that encodes the edges of the MST.
The consistent? predicate checks if there are any inconsistent edges, i.e., edges that
are significantly longer than their nearby edges, in the neighborhood graph. Deleting the
inconsistent edge will partition the graph into subgraphs each of which represents a cluster
of the original point set.
Next, the classify operator assigns a label, an orbit type, to the neighborhood graph
according to the shape of the MST and the number of clusters. If the assignment is unsuccessful, kam assumes the input point set does not contain enough points to reveal the
structure of the orbit. Kam will request more points and repeat the aggregation step.
If the assignment is successful, the re-describe operator takes the labeled neighborhood
graph and fills in information that is relevant to that particular orbit type. For example,
if the orbit is a periodic orbit, the period of the orbit is determined. After filling in the
details, the re-describe operator packages the orbit as a primitive object and passes it to
6. We introduce some useful terminology here. A dynamical system is a smooth vector field. An orbit is
an integral curve of the vector field. An orbit bundle is a collection of adjacent orbits having the same
qualitative behavior. A phase portrait is the collection of orbits that fill the phase space. A bifurcation
pattern is a characteristic change in the structure of a phase portrait as some system parameters vary.

11

fiYip & Zhao

(a)

(b-1)

(b-2)

Figure 3: Top: (a) The phase portrait of a Hamiltonian system. The geometric structures
in the phase portrait can vary drastically as the system parameter A changes.
Like an expert dynamicist, kam explores the dynamics of a nonlinear Hamiltonian
system by finding interesting structures in the phase space. It decides what initial
conditions and parameter values to try. It interprets what it finds and uses the
structures it draws for itself to guide further exploration.
Bottom: (b-1) The minimal spanning tree representation of a point set. (b-2)
Magnifying the boxed region | crosses () are inconsistent edges. Kam imposes
adjacency relations on a point set representing a trajectory in phase space. The
structure of the minimal spanning tree reveals the type of the trajectory.

12

fiSpatial Aggregation: Theory and Applications

bifurcation
consistency
rules
phase
portraits

consistent?

aggregate

N-graph

portrait
consistency
rules

bifurcation properties

bifurcation classification rules

consistent?

aggregate

redescribe

N-graph

phase
portrait

portrait properties
classify

wavefront
propagation
orbit bundle
consistency
rules
orbits

bifurcation
pattern

classify
nearest
neighbors

orbit
bundles

redescribe

portrait classification rules

consistent?

aggregate

redescribe

N-graph

orbit
bundle
orbit bundle properties
classify

wavefront
propagation
tree
consistency
rules
point
set

orbit bundle classification rules

consistent?

aggregate

redescribe

N-graph

MST
algorithm

orbit

orbit properties
classify
orbit classification rules

Figure 4: The computational structure of kam viewed as spatial aggregation operators acting on neighborhood graphs. It has four layers of abstraction: orbit, orbit bundle,
phase portrait, and bifurcation pattern. The computation is organized around
neighborhood graphs. The structural similarities among the layers are apparent.

13

fiYip & Zhao

force control
is switched on

control u 1
S

flow pipe
Region R is projected
onto the initial phase plane.

control u 2

G
R

Figure 5: Left: Buckling of a beam due to an axial load.
Right: Phase spaces for the buckling beam (upper) and locally controlled beam
(lower). To stabilize the buckling beam far from the unbuckled state | the
unstable equilibrium G, maps (1) finds a ow pipe, a group of qualitatively similar
trajectories, that reaches G, (2) deforms the trajectory emanating from the initial
state via a force control until the trajectory is close to G, and then (3) switches to
a conventional linear controller to achieve the desired stabilization. Let region R
in the lower phase plane be a linearly controllable region with control u2 . Starting
from an initial state S and initial control u1 , the system evolves along a trajectory
within the ow pipe until it is close to the projection of the region R. The force
control u1 is turned on to deform the trajectory so that the system moves into
the region R where a linear controller drives the system to the desired unbuckled
state G.
the next level of abstraction, the orbit bundle level, where the same process of aggregation,
consistency checks, classification, and re-description is repeated.

4.2 MAPS

Maps' task is to analyze the qualitative phase-space structures of dissipative systems and

use the analysis results to guide the synthesis of control laws.
Like kam, maps extracts high-level dynamical information from the phase space structures. But maps goes beyond kam in two important aspects: (1) maps deals with threedimensional structures explicitly (whereas kam reasons with cross-sections of three-dimensional
structures), and (2) maps uses the phase space structures to synthesize nonlinear control
actions.
Maps synthesizes a global control path geometrically (see Figure 5). Given an initial
state and a desired state for the system under control, maps searches for a path in the phase
space that connects the initial and the desired state. If the goal is not directly reachable
from the initial state, maps pieces together multiple path segments by varying the control
actions. A brute-force search for individual control paths in a continuum is clearly infeasible.
Maps partitions the continuous phase space into a manageable discrete set of objects |
14

fiSpatial Aggregation: Theory and Applications

ow pipes | by defining appropriate equivalence relations, and searches out the ow pipes
for good control paths.
The computations in maps are organized into four layers (as shown in Figure 6): (1) stability region, (2) ow pipe, (3) phase portrait, and (4) ow pipe graph. The input are the
fixed points of the dynamical system7 . Two fixed points are adjacent if they are connected
to the same saddle by trajectories. The adjacency relation is represented by a neighborhood
graph. The trajectories passing through the saddles are classified into equivalence classes
and assigned stability region boundary labels. The re-describe operator computes the regions delimited by the stability region boundaries and represents them by polyhedra. The
stability regions are fed to the next layer.
In the second layer, a stability region is triangulated by the Delaunay method. The
aggregate operator constructs a neighborhood graph of the triangulation using the adjacency relation defined by the Voronoi diagram, the dual of the Delaunay triangulation.
The triangulated sub-regions are classified into equivalence classes according to a topological criterion which states that two adjacent sub-regions are equivalent if the trajectories
passing through them can be connected in a consistent manner. Equivalence classes of
sub-regions are classified as ow pipes. Recall each ow pipe is a coarse representation of a
set of trajectories having the same qualitative properties. The use of ow pipes simplifies
considerably the control path planning problem.
The third layer aggregates the ow pipes to form a phase portrait.
The fourth layer is where control decisions are made. Flow pipes from different phase
portraits are aggregated to form a larger structure, the ow pipe graph, which is the fundamental data structure supporting path planning in the phase space. Two ow pipes
are adjacent if the phase space regions covered by the ow pipes overlap. Intuitively, one
can switch from one ow pipe to an adjacent one by setting appropriate control parameters
that generate the phase portraits in question. Given an initial and desired state, the search
operator searches the ow pipe graph for solution paths.
Information can also be passed down the abstraction layer. Once a connected sequence
of ow paths is found to satisfy a control objective, individual trajectory segments within
the ow pipe are found by the localize operator using a shooting method.

4.3 HIPAIR

Hipair performs kinematic analysis of fixed-axes mechanisms built of rigid parts. Given a
description of the shapes and motion types (such as translation and rotation) of the parts,
hipair derives realizable configurations of the mechanism.
Hipair derives realizable configurations of a mechanism by constructing and manipulating the configuration space of the mechanism (see Figure 7). The configuration space is
the space of positions and orientations of the parts that make up the mechanism. hipair
partitions the configuration space into free space regions where parts do not overlap, and
blocked space regions where they overlap. Only configurations that correspond to the free
space regions are realizable. The boundaries of the free space regions are determined by the
7. Fixed points, or equilibrium points, are critical points in the phase space where the velocity vector
vanishes. Fixed points are classified into three types according to the behavior of the nearby trajectories.
A fixed point is an attractor if the nearby trajectories all move towards it. It is a repellor if they all
move away from it. It is a saddle if some move towards and some move away from it.

15

fiYip & Zhao

flow pipe graph
consistency
rules
phase
portraits

aggregate

N-graph

portrait
consistency
rules

search

consistent?

aggregate

N-graph

classify
localize

shooting
method

redescribe

phase
portrait

portrait properties
classify

wavefront
propagation
sub-region
consistency
rules
stability
regions

flow
pipe
graph

reachability rules

flow pipe region overlap

flow
pipes

redescribe

consistent?

portrait classification rules

consistent?

aggregate

redescribe

N-graph

flow
pipes

flow pipe properties
classify

Voronoi diagram
flow pipe classification rules
stability region
consistency
rules
fixed
points

consistent?

aggregate

redescribe

N-graph

stability
regions

stability region properties
classify

saddle trajectories
stability region classification rules

Figure 6: The computational structure of maps viewed as spatial aggregation operators
acting on neighborhood graphs. It has four layers of abstraction: stability regions,
ow pipes, phase portrait, and ow pipe graph. Note the structural similarities
between kam and maps. Control synthesis is implemented by the search and
localize operators acting on the neighborhood graph representing the ow pipe
graph.

16

fiSpatial Aggregation: Theory and Applications

x
5

cam


x
follower

5






Figure 7: Left: The 3-finger cam-follower. Right: The configuration space for the camfollower.  is the cam rotation. x is the follower displacement. The shaded
regions are the blocked space, indicating that the parts overlap. The free space
regions are the realizable configurations of the cam-follower. The boundaries of
the free space regions are determined by the contact relations between the cam
fingers and the follower.
contact relations among the parts that touch each other. A region diagram is a graph whose
nodes are free space regions and edges specify region adjacencies. The region diagram of
the mechanism is composed of the regions diagrams of its pairwise interacting parts. For
example, the region diagram of a mechanism with 10 parts is constructed from the region
diagrams of 45 possibly interacting pairs.
The computations in hipair are organized into three layers (as shown in Figure 8): (1)
free space region, (2) subassembly region diagram, (3) mechanism region diagram. The
input are the shapes of parts and their motion types. Hipair first considers a pair of interacting parts. It looks up the equations of the contact curves, i.e., curves in the configuration
space for the pair corresponding to the configurations where the two parts touch, from a
pre-compiled table of common contact curves. A contact curve is partitioned into segments
by intersection points of the curve with either another contact curve or the boundaries of the
configuration space. Two segments are adjacent if they share an endpoint. The aggregate
operator assembles the segments and their adjacency relations into a neighborhood graph.
The search operator traverses the neighborhood graph to find all closed chains of segments,
where a closed chain of segments is a sequence of segments that intersect itself. Each closed
chain of segment encloses a free space region. The consistent? predicate discards closed
chains that lie inside other closed chains. The classify operator assigns a label to each
closed chain, and the re-describe operator computes the free space regions delimited by
the closed chains. Each free space region is subdivided into convex regions.
The input to the second layer are free space regions. They are aggregated into a neighborhood graph. Two free space regions are adjacent (or neighbors) if they touch. Given
an initial configuration S0 of an interacting pair, the search operator finds the free space
regions reachable from S0 by a depth first search. The neighborhood graph is re-described
as a subassembly region diagram.
17

fiYip & Zhao

sub-region
consistency
rules

consistent?

subassembly
aggregate
region
daigrams

region adjacency

sub-region
consistency
rules
free
space
regions

aggregate

mechanism
region
diagram

N-graph region diagram properties
classify
search

consistent?

mechanism classification rules

redescribe

N-graph

subassembly
region
diagram

region diagram properties

classify
region adjacency

closed chain
consistency
rules
contact
curve
segments

redescribe

search

consistent?

aggregate

region diagram classification rules

free
space
regions
free space properties

redescribe

N-graph

classify
shared endpoint

search
free space classification rules

Figure 8: The computational structure of hipair viewed as spatial aggregation operators
acting on neighborhood graphs. It has three layers of abstraction: free space
regions, subassembly region diagram, and mechanism region diagram. Note the
structural similarities between hipair, kam, and maps. The search operator
determines reachability conditions in all three layers.
On the third layer, hipair combines all the subassembly region diagrams into a mechanism region diagram. The mechanism region diagram is a neighborhood graph whose nodes
are realizable sets of free space regions and edges specify the adjacency of free space regions.
A set of free space regions is realizable if their intersections are non-empty. For example,
let M0 = fR0 ; S0 ; T0 g be a set of free space regions containing the initial configuration
of a mechanism with three parts P1 ; P2 ; and P3 , where R0 , S0 , and T0 are the free space
regions in the subassembly region diagrams of the pairs fP1 ; P2 g; fP1 ; P3 g, and fP2 ; P3 g
respectively. Suppose R0 has one neighbor R1 , S0 has one neighbor S1 , and T0 has none.
Then there are three candidate neighbors of M0 given by:
M1 = fR1 ; S0 ; T0 g
18

fiSpatial Aggregation: Theory and Applications

= fR0 ; S1 ; T0 g
= fR1 ; S1 ; T0 g

M2
M3

The consistent? predicate checks each of the candidate neighbors and discards the unrealizable ones.

5. An Illustration

In this section, we show what it is like to program in the spatial aggregation language.
The example is a boundary tracer for line drawings.8 We pick this example because image
analysis routines can be quite naturally written in the spatial aggregation style.
Boundary tracing is a basic operation in image analysis.9 The operation might be used
to identify and group boundary segments from the same object. For example, consider a
line drawing of overlapping 2D objects (see Figure 9). To group the boundary segments,
one might first decompose the figure into segments, and junctions. A tracing process then
joins colinear segments.
The input to the boundary tracing program is a bitmap:
0
0
0
0
0
0
0
0
0
0
0
0
0

0
1
1
1
1
1
1
1
0
0
0
0
0

0
1
0
0
0
0
0
1
0
0
0
0
0

0
1
0
0
1
1
1
1
1
1
1
1
0

0
1
0
0
1
0
0
1
0
0
0
1
0

0
1
0
0
1
0
0
1
0
0
0
1
0

0
1
1
1
1
1
1
1
0
0
0
1
0

0
0
0
0
1
0
0
0
0
0
0
1
0

0
0
0
0
1
0
0
0
0
0
0
1
0

0
0
0
0
1
0
0
0
0
0
0
1
0

0
0
0
0
1
0
0
0
0
0
0
1
0

0
0
0
0
1
0
0
0
0
0
0
1
0

0
0
0
0
1
1
1
1
1
1
1
1
0

0
0
0
0
0
0
0
0
0
0
0
0
0

The bitmap is rendered in Figure 10(a). Figure 11 illustrates how the output in Figure 10(b) and (c) is computed from the input bitmap, using the spatial aggregation operators.
We first define a neighborhood relation between pixels by the 4-adjacency (namely, the
neighbors of a pixel are the pixels in its immediate north, east, south, and west). Because
there is often no ecient way to construct N-graphs directly from neighborhood relations, we
define an explicit N-graph neighborhood constructor that finds all the 4-adjacency neighbors
of a given pixel.
Next the aggregate operator assembles the pixels into an N-graph by the N-graph
constructor. Pixels in the N-graph are considered similar if they are neighbors and neither
is a junction, where a junction is defined as a pixel whose value is one and which has more
than two one-value neighbors. The classify operator groups the pixels into equivalence
8. The details of the interpretor for the language, implemented in scheme, are discussed elsewhere (BaileyKellogg, Zhao, & Yip, 1996).
9. Jim Mahoney introduced us to a unified description of high-level operations on images.

19

fiYip & Zhao

Figure 9: A line drawing of two overlapping objects.

(a)

(b)

(c)

Figure 10: Boundary tracing operation in image analysis: (a) Pixels on boundaries of two
overlapping objects; (b) Pixels are grouped into boundary segments; (b) Boundary segments are grouped into distinct object contours.
classes using a similarity threshold and returns the foreground equivalence classes, shown
in Figure 10(b).
The foreground equivalence classes are then re-described as higher-level objects, boundary segments, which are in turn aggregated into a new N-graph using a different neighborhood relation. Specifically two boundary segments are neighbors if their minimum separation distance is less than a specified separation. Next, adjacent boundary segments which
are colinear are grouped into equivalence classes, called contours. A contour represents the
complete boundary of an object. Figure 10(c) shows the result of grouping.
We might want to check for impossible contours. A contour is legal if it is closed and not
self-intersecting. Such conditions are expressed in a standard pattern language. Pairwise
consistency rules can likewise be defined.
The program written in the spatial aggregation language is shown in Figure 12 and
Figure 13.10
10. In the actual implementation of the language described by Kellogg, Zhao, and Yip (1996), the syntax of
the operators differs slightly from those in Section 3.

20

fiSpatial Aggregation: Theory and Applications

contour
consistency
rules
boundary
segments

consistent?
aggregate

redescribe
segment
N-graph

nearness
neighborhood

object
contours

boundary segment classes
classify
colinearity,
threshold

redescribe
pixels

aggregate

pixel
N-graph

4-adjacency
neighborhood

boundary
segments

pixel classes
classify
pixel similarity,
threshold

Figure 11: Boundary tracing operation: data ow in the spatial aggregation implementation.

6. Related Work

The literature in visual and spatial reasoning is enormous (e.g., Kosslyn, 1994; Glasgow,
1993). In this section, we discuss only the computationally oriented approaches.
The first line of work investigates how diagram-like representations aid heuristic search.
Gelernter (1963) used diagrams in his geometry theorem prover to prune goals that are
obviously false. Nevins' geometry theorem prover constrained forward deduction to conclude facts about objects explicitly depicted in the diagrams (Nevins, 1975). Stallman
and Sussman (1977) exploited the connectivity and locality of lumped-parameter model
to guide forward reasoning and implement symbolic constraint propagation. In a similar
spirit, Larkin and Simon (1987) showed that in elementary mechanics problem a diagrammatic representation can reduce search because the diagram provides convenient indices for
clustering objects and relations.
The second line of work concerns analogue simulations in naive physics. Funt's whisper
program is the first AI program that uses primarily perceptual primitives to predict dynamical events in a simple blocks world (Funt, 1980). Arguing that the commonsense predictions
of solid or uid behavior cannot possibly depend on the solution of complicated equations,
Gardin and Meltzer (1989) proposed a \molecular" simulation of strings and uids. A
body of uid, for example, is decomposed into macro-molecules interacting with each other
according to a small set of local rules. Chandrasekaran and Narayanan (1990) proposed
a direct analogue simulation of the motion of a sliding block on an inclined plane. Their
21

fiYip & Zhao

;; 4-adjacency pixel neighborhood:
;; neighbors are pixels one unit away using nearness ngraph
(define image-ngraph-fac
(ngraph-near/instantiate image-field-fac 1))
;; Form a neighborhood graph for pixels
(define image-ngraph
(aggregate pixels image-ngraph-fac))
;; Pixel classifier: two adjacent nodes are equivalent if they
;; have the same value and neither is a junction.
(define pixel/classify
(classify-standard/instantiate
image-ngraph-fac
(lambda (n1 n2)
(if (and (not (is-junction? n1))
(not (is-junction? n2))
(= (pixel/value n1) (pixel/value n2)))
0 1))))
;; Form equivalence classes of foreground pixels
(define pixel-classes
(filter
(lambda (cl) (= (pixel/value (car cl)) 1))
(pixel/classify image-ngraph pixels *threshold1*)))
;;; Form boundary segments
(define segments
(redescribe pixel-classes segment/create))

Figure 12: Boundary tracing operation program (part 1): group pixels into boundary segments.
objective is to develop a cognitive architecture for visual perception and mental imagery.
The direct representation of a scene they propose consists of a hierarchical, multi-resolution
symbol structure encoding spatial relations among objects, and is linked to an analogical
representation of the scene (image). The major challenge in analogue simulation is how
to provide a reliable simulation without incorporating extensive physics and geometrical
modeling.
The third line of work consists of spatial reasoning research in qualitative physics.
Kuipers and Levitt (1988) described an approach to spatial reasoning in robot navigation
and mapping of large-scale spaces. They proposed a four-level hierarchical representation
incorporating topological and metric descriptions in terms of entities such as places, paths,
distances, and angles. Forbus et al. (1991) developed the Metric Diagram/Place Vocabulary
theory. The metric diagram contains both numerical and symbolic descriptions of a scene,
22

fiSpatial Aggregation: Theory and Applications

;; Boundary segment neighborhood defined by separation distance
(define segment-ngraph-fac
(ngraph-near/instantiate segment-field-fac separation))
;; Form a neighborhood graph for boundary segments
(define segment-ngraph
(aggregate segments segment-ngraph-fac))
;; Boundary segments classifier: two adjacent segments are
;; equivalent if they are colinear. Two thresholds are used in
;; determining colinearity: delta is the threshold for separation
;; distance between two end-points and epsilon is for the angle
;; between the tangent vectors at these end-points.
(define segment/classify
(classify-standard/instantiate
segment-ngraph-fac
(lambda (s1 s2)
(if (and (> (length (segment/points s1)) 1)
(> (length (segment/points s2)) 1)
(segment/colinear s1 s2 delta epsilon))
0 1))))
;; Form contours, i.e., equivalence classes of boundary segments
(define segment-classes
(segment/classify segment-ngraph segments *threshold2*))
;; Contour consistency check: closed and not self-intersecting
(define contour-consistency-rules
'(if (and (closed? ?c)
(not (self-intersecting? ?c)))
#t #f))

Figure 13: Boundary tracing operation program (part 2): group boundary segments into
distinct object contours.
while the place vocabulary is a quantization of the space according to task-specific criteria (see also footnote 1). Comparing the spatial aggregation framework and the MD/PV
framework, we note two major differences. First, whereas a metric diagram is a mixed
symbolic/quantitative representation, a field is purely numerical and does not encode any
structures explicitly. Second, our theory postulates multi-layer spatial aggregates with identical computational structure at each layer. By focusing on the field ontology, which can be
thought of as a special class of metric diagrams, we are able to emphasize the importance
of the structure-recovery problem, and the commonalities underlying several implemented
programs.

23

fiYip & Zhao

7. Conclusion

We have developed the spatial aggregation paradigm as a realization of imagistic reasoning.
The paradigm systematizes the important task of interpreting time-varying information-rich
fields. The paradigm consists of three ideas: (1) a field ontology, an image-like analogue
representation, as input, (2) structural discovery { the ecient transformation from pointwise field representation to economical symbolic descriptions { as the central computational
problem, and (3) a multi-layer neighborhood graph as the common interface and a small
set of generic operators { aggregate, classify, redescribe, and search { as building blocks for
computational processes that derive symbolic abstractions from the analogue representation. The paradigm relies on the important observations that the physical constraints on
a real field (such as continuity and conservation) provide useful equivalence relations and
economical descriptions, and a nonlocal property of a lower layer can often be redescribed
as a local property of a higher layer.
The spatial aggregation paradigm supports the recovery of abstract properties via the
multi-layer neighborhood graphs. It produces concise descriptions by manipulating equivalence classes of objects as primitives. It constructs modular programs from generic operators by mixing and matching a library of commonly used routines. It expresses task-specific
knowledge in terms of field metric, adjacency relations, consistency predicates, classification
rules, and redescription properties.
To illustrate our theory, we examine the computational structure of three implemented
programs { kam, maps, and hipair { that integrate symbolic, numerical, and visual reasoning. We show a small set of generic operators that construct, transform, filter, classify,
and search neighborhood graphs capture the commonalities of these programs. We develop
a language, a way of organizing programs around neighborhood graphs, to make programs
written in this style clear.
We are currently developing a toolkit to support problem solving using the generic
operators of the spatial aggregation paradigm. Many research questions are still open.
Can the operators be interfaced with computational geometry and with numerical analysis
to build robust, ecient programs? What scientific problems can be solved by spatial
aggregation?
Imagistic reasoning is a powerful strategy for mapping between analog signals generated
by physical systems and discrete, symbolic representations of the systems. Spatial aggregation is only one of its many realizations. We believe that reasoning methods that derive their
power primarily from perceptual operations on analog representations and only secondarily from search and analytical methods might prove effective in automating commonsense
reasoning as well.

Acknowledgements
We thank Chris Bailey-Kellogg for the help in implementing the spatial aggregation language, and the following people for helpful discussions and comments on the earlier drafts
of this paper: Harold Abelson, Andy Berlin, B. Chandrasekaran, Gregor Kiczales, John
Lamping, Shiou Loh, Jim Mahoney, Jeff May, Neal McDonald, Pandurang Nayak, Toyoaki
Nishida, Elisha Sacks, Brian Smith, Jack Smith, Gerry Sussman, and Brian Williams.
24

fiSpatial Aggregation: Theory and Applications

KY is supported by an NSF National Young Investigator Award ECS-935777. FZ is
supported by an NSF National Young Investigator Award CCR-9457802, an Alfred P. Sloan
Foundation Research Fellowship, a grant from Xerox Palo Alto Research Center, a grant
from AT&T Foundation, and an NSF grant CCR-9308639.

References

Bailey-Kellogg, C., Zhao, F., & Yip, K. (1996). Spatial aggregation: language and applications. In Proceedings of AAAI. To appear.
Bradley, E. (1992). Taming chaotic circuits. Tech. rep. AI-TR-1388, MIT Artificial Intelligence Lab.
Chandrasekaran, B., & Narayanan, N. (1990). Towards a theory of commonsense visual
reasoning. In Nori, K., & Madhavan, C. (Eds.), Foundations of Software Technology
and Theoretical Computer Science. Springer.
DeKleer, J., & Brown, J. (1984). A qualitative physics based on conuences. Artificial
Intelligence, 24.
Forbus, K. (1984). Qualitative process theory. Artificial Intelligence, 24.
Forbus, K., Nielsen, P., & Faltings, B. (1991). Qualitative spatial reasoning: the CLOCK
project. Artificial Intelligence, 51.
Funt, B. (1980). Problem solving with diagrammatic representations. Artificial Intelligence,
13.
Gardin, F., & Meltzer, B. (1989). Analogical representations of naive physics. Artificial
Intelligence, 38.
Gelernter, H. (1963). Realization of a geometry-theorem proving machine. In Computers
and Thought. McGraw-Hill.
Gelsey, A. (1995). Automated reasoning about machines. Artificial Intelligence, 74.
Glasgow, J. (1993). The imagery debate revisited: a computational perspective. Computational Intelligence.
Joskowicz, L., & Sacks, E. (1991). Computational kinematics. Artificial Intelligence, 51,
381{416.
Junker, U., & Braunschweug, B. (1995). History-based interpretation of finite element
simulations of seismic wave fields. In Proceedings of IJCAI.
Kosslyn, S. M. (1994). Image and Brain: the resolution of the imagery debate. MIT Press.
Kuipers, B. (1986). Qualitative simulation. Artificial Intelligence, 29.
Kuipers, B., & Levitt, T. (1988). Navigation and mapping in large-scale space. AI Magazine,
9(2).
25

fiYip & Zhao

Larkin, J., & Simon, H. (1987). Why a diagram is (sometimes) worth ten thousand words.
Cognitive Science, 11.
Nevins, A. (1975). Plane geometry theorem proving using forward chaining. Artificial
Intelligence, 6.
Nishida, T., & et al. (1991). Automated phase portrait analysis by integrating qualitative
and quantitative analysis. In Proceedings of AAAI.
Stallman, R., & Sussman, G. J. (1977). Forward reasoning and dependency-directed backtracking in a system for computer-aided circuit analysis. Artificial Intelligence, 9.
Ullman, S. (1984). Visual routines. Cognition, 18.
Yip, K. M. (1991). KAM: A system for intelligently guiding numerical experimentation by
computer. MIT Press.
Yip, K. M. (1995). Reasoning about uid motion: finding structures. In Proceedings of
IJCAI.
Zhao, F. (1994). Extracting and representing qualitative behaviors of complex systems in
phase spaces. Artificial Intelligence, 69(1-2), 51{92.
Zhao, F. (1995). Intelligent simulation in designing complex dynamical control systems. In
Tzafestas, & Verbruggen (Eds.), Artificial intelligence in industrial decision making,
control, and automation. Kluwer Academic Publishers.

26

fiJournal of Artificial Intelligence Research 5 (1996) 301-328

Submitted 4/96; published 12/96

Exploiting Causal Independence in Bayesian Network Inference
Nevin Lianwen Zhang

LZHANG @ CS . UST. HK

Department of Computer Science,
University of Science & Technology, Hong Kong

David Poole

POOLE @ CS . UBC . CA

Department of Computer Science, University of British Columbia,
2366 Main Mall, Vancouver, B.C., Canada V6T 1Z4

Abstract
A new method is proposed for exploiting causal independencies in exact Bayesian network inference. A Bayesian network can be viewed as representing a factorization of a joint probability
into the multiplication of a set of conditional probabilities. We present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even
smaller factors and consequently obtain a finer-grain factorization of the joint probability. The new
formulation of causal independence lets us specify the conditional probability of a variable given its
parents in terms of an associative and commutative operator, such as or, sum or max, on the
contribution of each parent. We start with a simple algorithm VE for Bayesian network inference
that, given evidence and a query variable, uses the factorization to find the posterior distribution of
the query. We show how this algorithm can be extended to exploit causal independence. Empirical
studies, based on the CPCS networks for medical diagnosis, show that this method is more efficient
than previous methods and allows for inference in larger networks than previous algorithms.

1. Introduction
Reasoning with uncertain knowledge and beliefs has long been recognized as an important research
issue in AI (Shortliffe & Buchanan, 1975; Duda et al., 1976). Several methodologies have been
proposed, including certainty factors, fuzzy sets, Dempster-Shafer theory, and probability theory.
The probabilistic approach is now by far the most popular among all those alternatives, mainly due
to a knowledge representation framework called Bayesian networks or belief networks (Pearl, 1988;
Howard & Matheson, 1981).
Bayesian networks are a graphical representation of (in)dependencies amongst random variables.
A Bayesian network (BN) is a DAG with nodes representing random variables, and arcs representing
direct influence. The independence that is encoded in a Bayesian network is that each variable is
independent of its non-descendents given its parents.
Bayesian networks aid in knowledge acquisition by specifying which probabilities are needed.
Where the network structure is sparse, the number of probabilities required can be much less than the
number required if there were no independencies. The structure can be exploited computationally to
make inference faster (Pearl, 1988; Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990; Shafer &
Shenoy, 1990).
The definition of a Bayesian network does not constrain how a variable depends on its parents.
Often, however, there is much structure in these probability functions that can be exploited for knowledge acquisition and inference. One such case is where some dependencies depend on particular
values of other variables; such dependencies can be stated as rules (Poole, 1993), trees (Boutilier

c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiZ HANG & P OOLE

et al., 1996) or as multinets (Geiger & Heckerman, 1996). Another is where the the function can be
described using a binary operator that can be applied to values from each of the parent variables. It
is the latter, known as causal independencies, that we seek to exploit in this paper.
Causal independence refers to the situation where multiple causes contribute independently to
a common effect. A well-known example is the noisy OR-gate model (Good, 1961). Knowledge
engineers have been using specific causal independence models in simplifying knowledge acquisition (Henrion, 1987; Olesen et al., 1989; Olesen & Andreassen, 1993). Heckerman (1993) was the
first to formalize the general concept of causal independence. The formalization was later refined by
Heckerman and Breese (1994).
Kim and Pearl (1983) showed how the use of noisy OR-gate can speed up inference in a special
kind of BNs known as polytrees; DAmbrosio (1994, 1995) showed the same for two level BNs with
binary variables. For general BNs, Olesen et al. (1989) and Heckerman (1993) proposed two ways
of using causal independencies to transform the network structures. Inference in the transformed
networks is more efficient than in the original networks (see Section 9).
This paper proposes a new method for exploiting a special type of causal independence (see Section 4) that still covers common causal independence models such as noisy OR-gates, noisy MAXgates, noisy AND-gates, and noisy adders as special cases. The method is based on the following
observation. A BN can be viewed as representing a factorization of a joint probability into the multiplication of a list of conditional probabilities (Shachter et al., 1990; Zhang & Poole, 1994; Li &
DAmbrosio, 1994). The type of causal independence studied in this paper leads to further factorization of the conditional probabilities (Section 5). A finer-grain factorization of the joint probability
is obtained as a result. We propose to extend exact inference algorithms that only exploit conditional
independencies to also make use of the finer-grain factorization provided by causal independence.
The state-of-art exact inference algorithm is called clique tree propagation (CTP) (Lauritzen &
Spiegelhalter, 1988; Jensen et al., 1990; Shafer & Shenoy, 1990). This paper proposes another algorithm called variable elimination (VE ) (Section 3), that is related to SPI (Shachter et al., 1990; Li
& DAmbrosio, 1994), and extends it to make use of the finer-grain factorization (see Sections 6, 7,
and 8). Rather than compiling to a secondary structure and finding the posterior probability for each
variable, VE is query-oriented; it needs only that part of the network relevant to the query given the
observations, and only does the work necessary to answer that query. We chose VE instead of CTP
because of its simplicity and because it can carry out inference in large networks that CTP cannot
deal with.
Experiments (Section 10) have been performed with two CPCS networks provided by Pradhan.
The networks consist of 364 and 421 nodes respectively and they both contain abundant causal independencies. Before this paper, the best one could do in terms of exact inference would be to first
transform the networks by using Jensen et al.s or Heckermans technique and then apply CTP. In
our experiments, the computer ran out of memory when constructing clique trees for the transformed
networks. When this occurs one cannot answer any query at all. However, the extended VE algorithm has been able to answer almost all randomly generated queries with twenty or less observations
(findings) in both networks.
One might propose to first perform Jensen et al.s or Heckermans transformation and then apply
VE . Our experiments show that this is significantly less efficient than the extended VE algorithm.
We begin with a brief review of the concept of a Bayesian network and the issue of inference.
302

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

2. Bayesian Networks
We assume that a problem domain is characterized by a set of random variables. Beliefs are represented by a Bayesian network (BN)  an annotated directed acyclic graph, where nodes represent
the random variables, and arcs represent probabilistic dependencies amongst the variables. We use
the terms node and variable interchangeably. Associated with each node is a conditional probability of the variable given its parents.
In addition to the explicitly represented conditional probabilities, a BN also implicitly represents
conditional independence assertions. Let x1 , x2 , ..., xn be an enumeration of all the nodes in a BN
such that each node appears after its children, and let xi be the set of parents of a node xi . The
Bayesian network represents the following independence assertion:
Each variable xi is conditionallyindependent of the variables in fx1 ; x2; : : :; xi,1 g given
values for its parents.
The conditional independence assertions and the conditional probabilities together entail a joint probability over all the variables. By the chain rule, we have:

P (x1; x2; : : :; xn)

=

=

n
Y
i=1
n
Y
i=1

P (xi jx1; x2; : : :; xi,1)
P (xi jx );
i

(1)

where the second equation is true because of the conditional independence assertions. The conditional probabilities P (xi jxi ) are given in the specification of the BN. Consequently, one can, in
theory, do arbitrary probabilistic reasoning in a BN.
2.1 Inference
Inference refers to the process of computing the posterior probability P (X jY =Y0 ) of a set X of
query variables after obtaining some observations Y =Y0 . Here Y is a list of observed variables and
Y0 is the corresponding list of observed values. Often, X consists of only one query variable.
In theory, P (X jY =Y0 ) can be obtained from the marginal probability P (X; Y ), which in turn
can be computed from the joint probability P (x1 ; x2; : : :; xn) by summing out variables outside
X [Y one by one. In practice, this is not viable because summing out a variable from a joint probability requires an exponential number of additions.
The key to more efficient inference lies in the concept of factorization. A factorization of a joint
probability is a list of factors (functions) from which one can construct the joint probability.
A factor is a function from a set of variables into a number. We say that the factor contains a variable if the factor is a function of that variable; or say it is a factor of the variables on which it depends.
Suppose f1 and f2 are factors, where f1 is a factor that contains variables x1 ; : : :; xi; y1; : : :; yj 
we write this as f1 (x1 ; : : :; xi; y1; : : :; yj )  and f2 is a factor with variables y1 ; : : :; yj ; z1; : : :; zk ,
where y1 ; : : :; yj are the variables in common to f1 and f2 . The product of f1 and f2 is a factor that
is a function of the union of the variables, namely x1 ; : : :; xi; y1; : : :; yj ; z1; : : :; zk , defined by:

f1 f2)(x1; : : :; xi; y1; : : :; yj ; z1; : : :; zk) = f1(x1; : : :; xi; y1; : : :; yj )f2(y1; : : :; yj ; z1; : : :; zk )

(

303

fiZ HANG & P OOLE

c

b

a

e

e

2

1

e3

Figure 1: A Bayesian network.
Let f (x1 ; : : :; xi ) be a function of variable x1; : : :; xi . Setting, say x1 in f (x1 ; : : :; xi) to a particular
value ff yields f (x1 =ff; x2; : : :; xi), which is a function of variables x2 ; : : :; xi.
If f (x1; : : :; xi) is a factor, we can sum out a variable, say x1 , resulting in a factor of variables
x2 ; : : :; xi, defined

X

(

x1

f )(x2; : : :; xi) = f (x1 =ff1 ; x2; : : :; xi) +    + f (x1=ffm; x2; : : :; xi)

where ff1 ; : : :; ffm are the possible values of variable x1.
Because of equation (1), a BN can be viewed as representing a factorization of a joint probability.
For example, the Bayesian network in Figure 1 factorizes the joint probability P (a; b; c; e1; e2; e3)
into the following list of factors:

P (a); P (b); P (c); P (e1ja; b; c); P (e2ja; b; c); P (e3je1; e2 ):
Multiplying those factors yields the joint probability.
Suppose a joint probability P (z1 ; z2; : : :; zm ) is factorized into the multiplication of a list of factors f1 , f2 , ..., fm . While obtaining P (z2 ; : : :; zm ) by summing out z1 from P (z1 ; z2; : : :; zm ) requires an exponential number of additions, obtaining a factorization of P (z2 ; : : :; zm ) can often be
done with much less computation. Consider the following procedure:
Procedure sum-out(F ; z ):




Inputs: F  a list of factors; z  a variable.
Output: A list of factors.

1. Remove from the F all the factors, say f1 , ..., fk , that contain z ,
2. Add the new factor

P Qk f to F and return F .
z i=1 i

Theorem 1 Suppose a joint probability P (z1 ; z2; : : :; zm) is factorized into the multiplication of a
list F of factors. Then sum-out(F ; z1 ) returns a list of factors whose multiplicationis P (z2 ; : : :; zm ).
304

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

Proof: Suppose F consists of factors f1 , f2 , ..., fm and suppose z1 appears in and only in factors
f1, f2, ..., fk . Then

P (z2 ; : : :; zm)

=

=

X
z1

P (z1 ; z2; : : :; zm )

m
XY
z1 i=1

fi = [

k
XY
z1 i=1

m
Y

fi ][

i=k+1

fi ]:

The theorem follows. 2
Only variables that appear in the factors f1 , f2 , ..., fk participated in the computation of sum-out(F ; z1 ),
and those are often only a small portion of all the variables. This is why inference in a BN can be
tractable in many cases, even if the general problem is NP-hard (Cooper, 1990).

3. The Variable Elimination Algorithm
Based on the discussions of the previous section, we present a simple algorithm for computing P (X jY =Y0 ).
The algorithm is based on the intuitions underlying DAmbrosios symbolic probabilistic inference
(SPI) (Shachter et al., 1990; Li & DAmbrosio, 1994), and first appeared in Zhang and Poole (1994).
It is essentially Dechter (1996)s bucket elimination algorithm for belief assessment.
The algorithm is called variable elimination (VE ) because it sums out variables from a list of
factors one by one. An ordering  by which variables outside X [Y to be summed out is required as
an input. It is called an elimination ordering.
Procedure VE (F ; X; Y; Y0; )





Inputs: F  The list of conditional probabilities in a BN;
X  A list of query variables;
Y  A list of observed variables;
Y0  The corresponding list of observed values;
  An elimination ordering for variables outside X [Y .
Output: P (X jY =Y0 ).

1. Set the observed variables in all factors to their corresponding observed values.
2. While  is not empty,

(a) Remove the first variable z from ,
(b) Call sum-out(F ; z ). Endwhile

3. Set h = the multiplication of all the factors on F .
/* h is a function of variables in X . */

4. Return h(X )=

P h(X ). /* Renormalization */
X

Theorem 2 The output of VE(F ; X; Y; Y0; ) is indeed P (X jY =Y0 ).
Proof: Consider the following modifications to the procedure. First remove step 1. Then the factor
h produced at step 3 will be a function of variables in X and Y . Add a new step after step 3 that sets
the observed variables in h to their observed values.
305

fiZ HANG & P OOLE

Let f (y; A) be a function of variable y and of variables in A. We use f (y; A)jy=ff to denote
f (y=ff; A). Let f (y; ,), g (y; ,), and h(y; z; ,) be three functions of y and other variables. It is
evident that

f (y; ,)g (y; ,)jy=ff = f (y; ,)jy=ff g(y; ,)jy=ff;
X
X
[
h(y; z; ,)]jy=ff = [h(y; z; ,)jy=ff ]:
z

z

Consequently, the modifications do not change the output of the procedure.
According to Theorem 1, after the modifications the factor produced at step 3 is simply the marginal
probability P (X; Y ). Consequently, the output is exactly P (X jY =Y0 ). 2
The complexity of VE can be measured by the number of numerical multiplications and numerical summations it performs. An optimal elimination ordering is one that results in the least complexity. The problem of finding an optimal elimination ordering is NP-complete (Arnborg et al., 1987).
Commonly used heuristics include minimum deficiency search (Bertele & Brioschi, 1972) and maximum cardinality search (Tarjan & Yannakakis, 1984). Kjrulff (1990) has empirically shown that
minimum deficiency search is the best existing heuristic. We use minimum deficiency search in our
experiments because we also found it to be better than the maximum cardinality search.
3.1

VE

versus Clique Tree Propagation

Clique tree propagation (Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990; Shafer & Shenoy,
1990) has a compilation step that transforms a BN into a secondary structure called clique tree or
junction tree. The secondary structure allows CTP to compute the answers to all queries with one
query variable and a fixed set of observations in twice the time needed to answer one such query in
the clique tree. For many applications this is a desirable property since a user might want to compare
the posterior probabilities of different variables.
CTP takes work to build the secondary structure before any observations have been received.
When the Bayesian network is reused, the cost of building the secondary structure can be amortized
over many cases. Each observation entails a propagation though the network.
Given all of the observations, VE processes one query at a time. If a user wants the posterior
probabilities of several variables, or for a sequence of observations, she needs to run VE for each of
the variables and observation sets.
The cost, in terms of the number of summations and multiplications, of answering a single query
with no observations using VE is of the same order of magnitude as using CTP. A particular clique
tree and propagation sequence encodes an elimination ordering; using VE on that elimination ordering results in approximately the same summations and multiplications of factors as in the CTP (there
is some discrepancy, as VE does not actually form the marginals on the cliques, but works with conditional probabilities directly). Observations make VE simpler (the observed variables are eliminated
at the start of the algorithm), but each observation in CTP requires propagation of evidence. Because
VE is query oriented, we can prune nodes that are irrelevant to specific queries (Geiger et al., 1990;
Lauritzen et al., 1990; Baker & Boult, 1990). In CTP, on the other hand, the clique tree structure is
kept static at run time, and hence does not allow pruning of irrelevant nodes.
CTP encodes a particular space-time tradeoff, and VE another. CTP is particularly suited to the
case where observations arrive incrementally, where we want the posterior probability of each node,
306

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

and where the cost of building the clique tree can be amortized over many cases. VE is suited for
one-off queries, where there is a single query variable and all of the observations are given at once.
Unfortunately, there are large real-world networks that CTP cannot deal with due to time and
space complexities (see Section 10 for two examples). In such networks, VE can still answer some
of the possible queries because it permits pruning of irrelevant variables.

4. Causal Independence
Bayesian networks place no restriction on how a node depends on its parents. Unfortunately this
means that in the most general case we need to specify an exponential (in the number of parents)
number of conditional probabilities for each node. There are many cases where there is structure in
the probability tables that can be exploited for both acquisition and for inference. One such case that
we investigate in this paper is known as causal independence.
In one interpretation, arcs in a BN represent causal relationships; the parents c1; c2; : : :; cm of a
variable e are viewed as causes that jointly bear on the effect e. Causal independence refers to the
situation where the causes c1 ; c2; : : :; cm contribute independently to the effect e.
More precisely, c1; c2; : : :; cm are said to be causally independent w.r.t. effect e if there exist
random variables 1; 2; : : :; m that have the same frame, i.e., the same set of possible values, as e
such that
1. For each i, i probabilistically depends on ci and is conditionally independent of all other cj s
and all other j s given ci , and
2. There exists a commutative and associative binary operator
e = 1 2 : : : m .
Using the independence notion of Pearl (1988), let
given Z , the first condition is:

 over the frame of e such that

I (X; Y jZ ) mean that X is independent of Y

I (1; fc2; : : :; cm; 2; : : :; mgjc1)
and similarly for the other variables. This entails I (1; cj jc1) and I (1; j jc1) for each cj and j
where j 6= 1.
We refer to i as the contribution of ci to e. In less technical terms, causes are causally independent w.r.t. their common effect if individual contributions from different causes are independent and
the total influence on the effect is a combination of the individual contributions.
We call the variable e a convergent variable as it is where independent contributions from different sources are collected and combined (and for the lack of a better name). Non-convergent variables
will simply be called regular variables. We call  the base combination operator of e.
The definition of causal independence given here is slightly different than that given by Heckerman and Breese (1994) and Srinivas (1993). However, it still covers common causal independence
models such as noisy OR-gates (Good, 1961; Pearl, 1988), noisy MAX-gates (Dez, 1993), noisy
AND-gates, and noisy adders (Dagum & Galper, 1993) as special cases. One can see this in the following examples.
Example 1 (Lottery) Buying lotteries affects your wealth. The amounts of money you spend on
buying different kinds of lotteries affect your wealth independently. In other words, they are causally
307

fiZ HANG & P OOLE

independent w.r.t. the change in your wealth. Let c1; : : :; ck denote the amounts of money you spend
on buying k types of lottery tickets. Let 1; : : :; k be the changes in your wealth due to buying the
different types of lottery tickets respectively. Then, each i depends probabilistically on ci and is
conditionally independent of the other cj and j given ci . Let e be the total change in your wealth
due to lottery buying. Then e=1 +    +k . Hence c1 ; : : :; ck are causally independent w.r.t. e. The
base combination operator of e is numerical addition. This example is an instance of a causal independence model called noisy adders.
If c1 ; : : :; ck are the amounts of money you spend on buying lottery tickets in the same lottery,
then c1 ; : : :; ck are not causally independent w.r.t. e, because winning with one ticket reduces the
chance of winning with the other. Thus, 1 is not conditionally independent of 2 given c1. However,
if the ci represent the expected change in wealth in buying tickets in the same lottery, then they would
be causally independent, but not probabilistically independent (there would be arcs between the ci s).
Example 2 (Alarm) Consider the following scenario. There are m different motion sensors each
of which are connected to a burglary alarm. If one sensor activates, then the alarm rings. Different
sensors could have different reliability. We can treat the activation of sensor i as a random variable.
The reliability of the sensor can be reflected in the i . We assume that the sensors fail independently1.
Assume that the alarm can only be caused by a sensor activation2. Then alarm=1 _   _m ; the
base combination operator here is the logical OR operator. This example is an instance of a causal
independence model called the noisy OR-gate.
The following example is not an instance of any causal independence models that we know:
Example 3 (Contract renewal) Faculty members at a university are evaluated in teaching, research,
and service for the purpose of contract renewal. A faculty members contract is not renewed, renewed without pay raise, renewed with a pay raise, or renewed with double pay raise depending on
whether his performance is evaluated unacceptable in at least one of the three areas, acceptable in
all areas, excellent in one area, or excellent in at least two areas.
Let c1 , c2, and c3 be the fractions of time a faculty member spends on teaching, research, and
service respectively. Let i represent the evaluation he gets in the ith area. It can take values 0, 1,
and 2 depending on whether the evaluation is unacceptable, acceptable, or excellent. The variable
i depends probabilistically on ci. It is reasonable to assume that i is conditionally independent of
other cj s and other j s given ci .
Let e represent the contract renewal result. The variable can take values 0, 1, 2, and 3 depending
on whether the contract is not renewed, renewed with no pay raise, renewed with a pay raise, or
renewed with double pay raise. Then e=1 23, where the base combination operator  is given
in this following table:

0
1
2
3

0
0
0
0
0

1
0
1
2
3

2
0
2
3
3

3
0
3
3
3

1. This is called the exception independence assumption by Pearl (1988).
2. This is called the accountability assumption by Pearl (1988). The assumption can always be satisfied by introducing
a node that represent all other causes (Henrion, 1987).

308

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

So, the fractions of time a faculty member spends in the three areas are causally independent
w.r.t. the contract renewal result.
In the traditional formulation of a Bayesian network we need to specify an exponential, in the
number of parents, number of conditional probabilities for a variable. With causal independence,
the number of conditional probabilities P (i jci ) is linear in m. This is why causal independence
can reduce complexity of knowledge acquisition (Henrion, 1987; Pearl, 1988; Olesen et al., 1989;
Olesen & Andreassen, 1993). In the following sections we show how causal independence can also
be exploited for computational gain.
4.1 Conditional Probabilities of Convergent Variables
VE allows us to exploit structure in a Bayesian network by providing a factorization of the joint probability distribution. In this section we show how causal independence can be used to factorize the
joint distributioneven further. The initial factors in the VE algorithm are of the form P (ejc1; : : :; cm).
We want to break this down into simpler factors so that we do not need a table exponential in m. The
following proposition shows how causal independence can be used to do this:

Proposition 1 Let e be a node in a BN and let c1 ; c2; : : :; cm be the parents of e. If c1; c2; : : :; cm
are causally independent w.r.t. e, then the conditional probability P (ejc1; : : :; cm) can be obtained
from the conditional probabilities P (i jci) through

P (e=ffjc1; : : :; cm) =

X

ff1 :::ffk =ff

P (1 =ff1 jc1): : :P (m =ffmjcm );

(2)

for each value ff of e. Here  is the base combination operator of e.
Proof:3 The definition of causal independence entails the independence assertions

I (1; fc2; : : :; cmgjc1) and I (1; 2jc1):
By the axiom of weak union (Pearl, 1988, p. 84), we have I (1; 2jfc1; : : :; cmg). Thus all of the i
mutually independent given fc1; : : :; cmg.
Also we have, by the definition of causal independence I (1; fc2; : : :; cm gjc1), so
P (1jfc1; c2; : : :; cmg) = P (1jc1)
Thus we have:

P (e=ffjc1; : : :; cm)
= P (1   m =ffjc1; : : :; cm)
X
=
P (1=ff1 ; : : :; m=ffm jc1; : : :; cm)
ff1 :::ff =ff
X
=
P (1=ff1 jc1; : : :; cm )P (2=ff2 jc1; : : :; cm)    P (m=ffm jc1; : : :; cm)
ff1 :::ff =ff
X
=
P (1=ff1 jc1)P (2=ff2jc2)    P (m =ffm jcm)
ff1 :::ff =ff
2
k

k

k

The next four sections develop an algorithm for exploiting causal independence in inference.
3. Thanks to an anonymous reviewer for helping us to simplify this proof.

309

fiZ HANG & P OOLE

5. Causal Independence and Heterogeneous Factorizations
In this section, we shall first introduce an operator for combining factors that contain convergent
variables. The operator is a basic ingredient of the algorithm to be developed in the next three sections. Using the operator, we shall rewrite equation (2) in a form that is more convenient to use in
inference and introduce the concept of heterogeneous factorization.
Consider two factors f and g . Let e1 , ..., ek be the convergent variables that appear in both f and
g , let A be the list of regular variables that appear in both f and g , let B be the list of variables that
appear only in f , and let C be the list of variables that appear only in g . Both B and C can contain
convergent variables, as well as regular variables. Suppose i is the base combination operator of
ei. Then, the combination f 
g of f and g is a function of variables e1, ..., ek and of the variables
in A, B , and C . It is defined by:4

f 
g (e1=X
ff1 ; : : :; ek =ffkX
; A; B; C )
=
:::
f (e1=ff11; : : :; ek =ffk1 ; A; B)
ff11 1 ff12 =ff1

ffk1 k ffk2 =ffk

g (e1=ff12; : : :; ek=ffk2; A; C );

(3)

for each value ffi of ei . We shall sometimes write f 
g as f (e1 ; : : :; ek ; A; B )
g (e1; : : :; ek ; A; C )
to make explicit the arguments of f and g .
Note that base combination operators of different convergent variables can be different.
The following proposition exhibits some of the basic properties of the combination operator 
.
Proposition 2 1. If f and g do not share any convergent variables, then f 
g is simply the multiplication of f and g . 2. The operator 
 is commutative and associative.
Proof: The first item is obvious. The commutativity of 
 follows readily from the commutativity of
multiplication and the base combination operators. We shall prove the associativity of 
 in a special
case. The general case can be proved by following the same line of reasoning.
Suppose f , g , and h are three factors that contain only one variable e and the variable is convergent. We need to show that (f 
g )
h=f 
(g 
h). Let  be the base combination operator of e. By
the associativity of , we have, for any value ff of e, that

f 
g )
h(e=ff)

(

=
=
=
=

X

f 
g (e=ff4)h(e=ff3)
X
X
[
f (e=ff1 )g(e=ff2)]h(e=ff3 )
ff4 ff3 =ff ff1 ff2 =ff4
X
f (e=ff1)g (e=ff2 )h(e=ff3 )
ff1 ff2 ff3 =ff
X
X
f (e=ff1)[
g (e=ff2 )h(e=ff3)]
ff4 ff3 =ff

ff1 ff4 =ff

ff2ff3 =ff4

4. Note that the base combination operators under the summations are indexed. With each convergent variable is an associated operator, and we always use the binary operator that is associated with the corresponding convergent variable.
In the examples, for ease of exposition, we will use one base combination operator. Where there is more than one type
of base combination operator (e.g., we may use or, sum and max for different variables in the same network), we
have to keep track of which operators are associated with which convergent variables. This will, however, complicate
the description.

310

fiE XPLOITING C AUSAL I NDEPENDENCE

X

=

ff1 ff4 =ff

IN

B AYESIAN N ETWORK I NFERENCE

f (e=ff1)g 
h(e=ff4)

f 
(g 
h)(e=ff):

=

The proposition is hence proved.2
The following propositions give some properties for 
 that correspond to the operations that we
exploited for the algorithm VE . The proofs are straight forward and are omitted.
Proposition 3 Suppose f and g are factors and variable z appears in f and not in g , then

X

Xz
z

X

fg)

=

(

f 
g )

=

(

(

(

z
X
z

f )g; and
f )
g:

Proposition 4 Suppose f , g and h are factors such that g and h do not share any convergent variables, then

g (f 
h) = (gf )
h:

(4)

5.1 Rewriting Equation 2
Noticing that the contribution variable i has the same possible values as e, we define functions

fi(e; ci) by

fi(e=ff; ci ) = P (i =ffjci);
for any value ff of e. We shall refer to fi as the contributing factor of ci to e.
By using the operator 
, we can now rewrite equation (2) as follows
P (ejc1; : : :; cm) = 
mi=1 fi (e; ci):

(5)

It is interesting to notice the similarity between equation (1) and equation (5). In equation (1)
conditional independence allows one to factorize a joint probability into factors that involve less
variables, while in equation (5) causal independence allows one to factorize a conditional probability
into factors that involve less variables. However, the ways by which the factors are combined are
different in the two equations.
5.2 Heterogeneous Factorizations
Consider the Bayesian network in Figure 1. It factorizes the joint probability P (a; b; c; e1; e2; e3)
into the following list of factors:

P (a); P (b); P (c); P (e1ja; b; c); P (e2ja; b; c); P (e3je1; e2 ):
We say that this factorization is homogeneous because all the factors are combined in the same way,
i.e., by multiplication.
Now suppose the ei s are convergent variables. Then their conditional probabilities can be further factorized as follows:

P (e1ja; b; c)
P (e2ja; b; c)
P (e3 je1 ; e2)

=
=
=

f11 (e1; a)
f12 (e1 ; b)
f13 (e1; c);
f21 (e2; a)
f22 (e2 ; b)
f23 (e2; c);
f31 (e3; e1)
f32 (e3; e2 );
311

fiZ HANG & P OOLE

where the factor f11(e1 ; a), for instance, is the contributing factor of a to e1 .
We say that the following list of factors

f11(e1 ; a); f12(e1; b); f13(e1; c); f21(e2 ; a); f22(e2 ; b); f23(e2 ; c); f31(e3 ; e1); f32(e3 ; e2);
P (a); P (b); and P (c)
(6)
constitute a heterogeneous factorization of P (a; b; c; e1; e2; e3) because the joint probability can be
obtained by combining those factors in a proper order using either multiplication or the operator 
.
The word heterogeneous is to signify the fact that different factor pairs might be combined in different ways. We call each fij a heterogeneous factor because it needs to be combined with the other
fik s by the operator 
 before it can be combined with other factors by multiplication. In contrast,
we call the factors P (a), P (b), and P (c) homogeneous factors.
We shall refer to that heterogeneous factorization as the heterogeneous factorization represented
by the BN in Figure 1. It is obvious that this heterogeneous factorization is of finer grain than the
homogeneous factorization represented by the BN.

6. Flexible Heterogeneous Factorizations and Deputation
This paper extends VE to exploit this finer-grain factorization. We will compute the answer to a query
by summing out variables one by one from the factorization just as we did in VE .
The correctness of VE is guaranteed by the fact that factors in a homogeneous factorization can
be combined (by multiplication) in any order and by the distributivity of multiplication over summations (see the proof of Theorem 1).
According to Proposition 3, the operator 
 is distributive over summations. However, factors in
a heterogeneous factorization cannot be combined in arbitrary order. For example, consider the heterogeneous factorization (6). While it is correct to combine f11(e1 ; a) and f12 (e1; b) using 
, and to
combine f31 (e3; e1 ) and f32 (e3; e2 ) using 
, it is not correct to combine f11 (e1; a) and f31 (e3; e1)
with 
. We want to combine these latter two by multiplication, but only after each has been combined with its sibling heterogeneous factors.
To overcome this difficulty, a transformation called deputation will be performed on our BN.
The transformation does not change the answers to queries. And the heterogeneous factorization
represented by the transformed BN is flexible in the following sense:
A heterogeneous factorization of a joint probability is flexible if:
The joint probability
=

multiplication of all homogeneous factors

 combination (by 
) of all heterogeneous factors:

(7)

This property allows us to carry out multiplication of homogeneous factors in arbitrary order,
and since 
 is associative and commutative, combination of heterogeneous factors in arbitrary order. If the conditions of Proposition 4 are satisfied, we can also exchange a multiplication with a
combination by 
. To guarantee the conditions of Proposition 4, the elimination ordering needs to
be constrained (Sections 7 and 8).
The heterogeneous factorization of P (a; b; c; e1; e2; e3) given at the end of the previous section is
not flexible. Consider combining all the heterogeneous factors. Since the operator 
 is commutative
312

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

c

b

a
e1

e2

e1

e2
e3
e3

Figure 2: The BN in Figure 1 after the deputation of convergent variables.
and associative, one can first combine, for each i, all the fik s, obtaining the conditional probability
of ei , and then combine the resulting conditional probabilities. The combination

P (e1 ja; b; c)
P (e2 ja; b; c)
P (e3je1 ; e2)
is not the same as the multiplication

P (e1 ja; b; c)P (e2ja; b; c)P (e3je1; e2)
because the convergent variables e1 and e2 appear in more than one factor. Consequently, equation
(7) does not hold and the factorization is not flexible. This problem arises when a convergent variable is shared between two factors that are not siblings. For example, we do not want to combine
f11 (e1; a) and f31 (e3 ; e1) using 
. In order to tackle this problem we introduce a new deputation
variable so that each heterogeneous factor contains a single convergent variable.
Deputation is a transformation that one can apply to a BN to make the heterogeneous factorization represented by the BN flexible. Let e be a convergent variable. To depute e is to make a copy
e0 of e, make the parents of e be parents of e0 , replace e with e0 in the contributing factors of e, make
e0 the only parent of e, and set the conditional probability P (eje0 ) as follows:

P (eje0 ) =

(

1
0

if e = e0
otherwise

(8)

We shall call e0 the deputy of e. The deputy variable e0 is a convergent variable by definition. The
variable e, which is convergent before deputation, becomes a regular variable after deputation. We
shall refer to it as a new regular variable. In contrast, we shall refer to the variables that are regular
before deputation as old regular variables. The conditional probability P (e0 je) is a homogeneous
factor by definition. It will sometimes be called the deputing function and written as I (e0; e) since it
ensures that e0 and e always take the same value.
A deputation BN is obtained from a BN by deputing all the convergent variables. In a deputation
BN, deputy variables are convergent variables and only deputy variables are convergent variables.
313

fiZ HANG & P OOLE

Figure 2 shows the deputation of the BN in Figure 1. It factorizes the joint probability

P (a; b; c; e1; e01; e2; e02; e3; e03)
into homogeneous factors

P (a); P (b); P (c); I1(e01; e1); I2(e02; e2); I3(e03; e3);
and heterogeneous factors

f11(e01 ; a); f12(e01; b); f13(e01 ; c); f21(e02 ; a); f22(e02; b); f23(e02; c); f31(e03; e1); f32(e03; e2):
This factorization has three important properties.
1. Each heterogeneous factor contains one and only one convergent variable. (Recall that the ei s
are no longer convergent variables and their deputies are.)
2. Each convergent variable e0 appears in one and only one homogeneous factor, namely the
deputing function I (e0; e).
3. Except for the deputing functions, none of the homogeneous factors contain any convergent
variables.
Those properties are shared by the factorization represented by any deputation BN.
Proposition 5 The heterogeneous factorization represented by a deputation BN is flexible.
Proof: Consider the combination, by 
, of all the heterogeneous factors in the deputation BN. Since
the combination operator 
 is commutative and associative, we can carry out the combination in following two steps. First for each convergent (deputy) variable e0 , combine all the heterogeneous factors that contain e0 , yielding the conditional probability P (e0 je ) of e0 . Then combine those resulting
conditional probabilities. It follows from the first property mentioned above that for different convergent variables e01 and e02, P (e01 je1 ) and P (e02 je2 ) do not share convergent variables. Hence the
combination of the P (e0 je )s is just the multiplication of them. Consequently, the combination,
by 
, of all heterogeneous factors in a deputation BN is just the multiplication of the conditional
probabilities of all convergent variables. Therefore, we have
0

0

0

0

The joint probability of variables in a deputation BN
= multiplication of conditional probabilities of all variables
=

multiplication of conditional probabilities of all regular variables

=

multiplication of all homogeneous factors

multiplication of conditional probabilities of all convergent variables

combination (by 
) of all heterogeneous factors:
The proposition is hence proved. 2
Deputation does not change the answer to a query. More precisely, we have
Proposition 6 The posterior probability P (X jY =Y0 ) is the same in a BN as in its deputation.
314

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

Proof: Let R, E , and E 0 be the lists of old regular, new regular, and deputy variables in the deputation BN respectively. It suffices to show that P (R; E ) is the same in the original BN as in the
deputation BN. For any new regular variable e, let e0 be its deputy. It is easy to see that the quantity
P I (e0; e)P (e0j ) in the deputation BN is the same as P (ej ) in the original BN. Hence,
e
e
e
0

0

P (R; EX) in the deputation BN
=
P (R; E; E 0)
E Y
X
Y
=
P (rjr ) [P (eje)P (e0 je )]
E r2R
2E
Y
Y eX
=
P (rjr ) [ I (e0; e)P (e0je )]
r2R
e2E e
Y
Y
=
P (rjr ) P (eje )
0

0

0

0

0

r2R

=

The proposition is proved.

2

e2 E

P (R; E ) in the original BN:

7. Tidy Heterogeneous Factorizations
So far, we have only encountered heterogeneous factorizations that correspond to Bayesian networks.
In the following algorithm, the intermediate heterogeneous factorizations do not necessarily correspond by BNs. They do have the property that they combine to form the appropriate marginal probabilities. The general intuition is that the heterogeneous factors must combine with their sibling heterogeneous factors before being multiplied by factors containing the original convergent variable.
In the previous section, we mentioned three properties of the heterogeneous factorization represented by a deputation BN, and we used the first property to show that the factorization is flexible.
The other two properties qualify the factorization as a tidy heterogeneous factorization, which is defined below.
Let z1 , z2 , ..., zk be a list of variables in a deputation BN such that if a convergent (deputy)
variable e0 is in fz1; z2 ; : : :; zk g, so is the corresponding new regular variable e. A flexible heterogeneous factorization of P (z1 ; z2; : : :; zk ) is said to be tidy If

1. For each convergent (deputy) variable e02fz1; z2; : : :; zk g, the factorization contains the deputing function I (e0; e) and it is the only homogeneous factor that involves e0 .

2. Except for the deputing functions, none of the homogeneous factors contain any convergent
variables.
As stated earlier, the heterogeneous factorization represented by a deputation BN is tidy.
Under certain conditions, to be given in Theorem 3, one can obtain a tidy factorization of P (z2 ; : : :; zk )
by summing out z1 from a tidy factorization of P (z1 ; z2; : : :; zk ) using the the following procedure.
Procedure sum-out1(F1 ; F2; z )



Inputs: F1  A list of homogeneous factors,
F2  A list of heterogeneous factors,
z  A variable.
315

fiZ HANG & P OOLE



Output: A list of heterogeneous factors and a list of homogeneous factors.

1. Remove from F1 all the factors that contain z , multiply them resulting in, say, f .
If there are no such factors, set f =nil.

2. Remove from F2 all the factors that contain z , combine them by using 
 resulting
in, say, g . If there are no such factors, set g =nil.

P f to F .
1
z
P
Else add the new (heterogeneous) factor z fg to F2.
Return (F1; F2).

3. If g =nil, add the new (homogeneous) factor
4.
5.

Theorem 3 Suppose a list of homogeneous factors F1 and a list of heterogeneous factors F2 constitute a tidy factorization of P (z1 ; z2; : : :; zk ). If z1 is either a convergent variable, or an old regular
variable, or a new regular variable whose deputy is not in the list fz2; : : :; zk g, then the procedure
sum-out1(F1 ; F2; z1) returns a tidy heterogeneous factorization of P (z2 ; : : :; zk ).
The proof of this theorem is quite long and hence is given in the appendix.

8. Causal Independence and Inference
Our task is to compute P (X jY =Y0 ) in a BN. According to Proposition 6, we can do this in the
deputation of the BN.
An elimination ordering consisting of the variables outside X [Y is legitimate if each deputy
variable e0 appears before the corresponding new regular variable e. Such an ordering can be found
using, with minor adaptations, minimum deficiency search or maximum cardinality search.
The following algorithm computes P (X jY =Y0 ) in the deputation BN. It is called VE 1 because
it is an extension of VE .
Procedure VE 1 (F1; F2; X; Y; Y0; )





Inputs: F1  The list of homogeneous factors
in the deputation BN;
F2  The list of heterogeneous factors
in the deputation BN;
X  A list of query variables;
Y  A list of observed variables;
Y0  The corresponding list of observed values;
  A legitimate elimination ordering.
Output: P (X jY =Y0 ).

1. Set the observed variables in all factors to their observed values.
2. While  is not empty,

 Remove the first variable z from .
 (F1; F2) = sum-out1(F1; F2; z). Endwhile
316

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

3. Set h=multiplication of all factors in F1
 combination (by 
) of all factors in F2.
/* h is a function of variables in X . */
4. Return h(X )=

P h(X ). /* renormalization */
X

Theorem 4 The output of VE 1 (F1; F2; X; Y; Y0; ) is indeed P (X jY =Y0 ).
Proof: Consider the following modifications to the algorithm. First remove step 1. Then the factor
h produced at step 3 is a function of variables in X and Y . Add a new step after step 3 that sets
the observed variables in h to their observed values. We shall first show that the modifications do
not change the output of the algorithm and then show that the output of the modified algorithm is
P (X jY =Y0 ).
Let f (y; ,), g (y; ,), and h(y; z; ,) be three functions of y and other variables. It is evident that

f (y; ,)g (y; ,)jy=ff = f (y; ,)jy=ff g(y; ,)jy=ff;
X
X
[
h(y; z; ,)]jy=ff = [h(y; z; ,)jy=ff ]:
z

z

If y is a regular variable, we also have

f (y; ,)
g (y; ,)jy=ff = f (y; ,)jy=ff 
g (y; ,)jy=ff :
Consequently, the modifications do not change the output of the procedure.
Since the elimination ordering  is legitimate, it is always the case that if a deputy variable e0 has
not been summed out, neither has the corresponding new regular variable e. Let z1 , ..., zk be the remaining variables in  at any time during the execution of the algorithm. Then, e0 2fz1 ; : : :; zk g implies e2fz1 ; : : :; zk g. This and the fact that the factorization represented by a deputation BN is tidy
enable us to repeatedly apply Theorem 3 and conclude that, after the modifications, the factor created
at step 3 is simply the marginal probability P (X; Y ). Consequently, the output is P (X jY =Y0 ). 2
8.1 An Example
This subsection illustrates VE 1 by walking through an example. Consider computing the P (e2 je3=0)
in the deputation Bayesian network shown in Figure 2. Suppose the elimination ordering  is: a, b,
c, e01, e02, e1, and e03 . After the first step of VE1 ,

F1 = fP (a); P (b); P (c); I1(e01; e1); I2(e02; e2); I3(e03; e3=0)g;
F2 = ff11(e01; a); f12(e01; b); f13(e01; c); f21(e02; a); f22(e02; b); f23(e02; c); f31(e03; e1); f32(e03; e2)g:
Now the procedure enters the while-loop and it sums out the variables in  one by one.
After summing out a,
F1 = fP (b); P (c); I1(e01; e1); I2(e02; e2); I3(e03; e3=0)g;
F2 = ff12(e01; b); f13P(e01; c); f22(e02; b); f23(e02; c); f31(e03; e1); f32(e03; e2); 1(e01; e02)g;
where 1 (e01; e02) = a P (a)f11(e01 ; a)f21(e02 ; a).
After summing out b,
F1 = fP (c); I1(e01; e1); I2(e02; e2); I3(e03; e3=0)g;
F2 = ff13(e01; c); f23P(e02; c); f31(e03; e1); f32(e03; e2); 1(e01; e02); 2(e01; e02)g;
where 2 (e01; e02) = b P (b)f12(e01 ; b)f22(e02; b).
317

fiZ HANG & P OOLE

After summing out c,
F1 = fI1(e01; e1); I2(e02; e2); I3(e03; e3=0)g;
F2 = ff31(e03; e1); fP
32(e03 ; e2); 1(e01 ; e02); 2(e01 ; e02); 3(e01 ; e02)g;
0
0
where 3 (e1; e2) = c P (c)f23 (e02; c)f13(e01 ; c).
After summing out e01 ,
F1 = fI2(e02; e2); I3(e03; e3=0)g;
F2 = ff31(e03; e1); fP
32(e03 ; e2); 4(e1 ; e02)g;
where 4 (e1; e02) = e I1(e01 ; e1)[ 1(e01; e02)
 2 (e01; e02)
 3 (e01; e03)].
1
After summing out e02 ,
F1 = fI3(e03; e3=0)g;
F2 = ff31(e03; e1); fP
32(e03 ; e2); 5(e1 ; e2)g;
where 5 (e1; e2) = e I2(e02 ; e2) 4(e1 ; e02).
2
After summing out e1 ,
F1 = fI3(e03; e3=0)g;
F2 = ff32(e03; e2); P6(e03; e2)g;
where 6 (e03; e2) = e1 f31(e03 ; e1) 5(e1 ; e2).
Finally, after summing out e03 ,
F 1 = ;;
F2 = f 7(e2)g; P
where 7 (e2) = e I3(e03 ; e3=0)[f32(e03; e2 )
 6(e03; e2 )]. Now the procedure enters step 3, where
3
P
there is nothing to do in this example. Finally, the procedure returns 7(e2 )= e2 7(e2 ), which is
P (e2je3 =0), the required probability.
0

0

0

8.2 Comparing VE and VE 1
In comparing VE and VE 1 , we notice that when summing out a variable, they both combine only
those factors that contain the variable. However, the factorization that the latter works with is of
finer grain than the factorization used by the former. In our running example, the latter works with a
factorization which initially consists of factors that contain only two variables; while the factorization the former uses initially include factors that contain five variables. On the other hand, the latter
uses the operator 
 which is more expensive than multiplication. Consider, for instance, calculating
f (e; a)
g(e; b). Suppose e is a convergent variable and all variables are binary. Then the operation requires 24 numerical multiplications and 24 , 23 numerical summations. On the other hand,
multiplying f (e; a) and g (e; b) only requires 23 numerical multiplications.

Despite the expensiveness of the operator 
, VE 1 is more efficient than VE . We shall provide
empirical evidence in support of this claim in Section 10. To see a simple example where this is true,
consider the BN in Figure 3(1), where e is a convergent variable. Suppose all variables are binary.
Then, computing P (e) by VE using the elimination ordering c1 , c2, c3, and c4 requires 25 + 24 +
23 + 22=60 numerical multiplications and (25 , 24) + (24 , 23 ) + (23 , 22 ) + (22 , 2)=30
numerical additions. On the other hand, computing P (e) in the deputation BN shown in Figure 3(2)
by VE 1 using the elimination ordering c1, c2, c3 , c4, and e0 requires only 22 + 22 + 22 + 22 +
(322 + 22)=32 numerical multiplications and 2 + 2 + 2 + 2 + (32 + 2)=16 numerical additions.
Note that summing out e0 requires 322 + 22 numerical multiplications because after summing out
ci s, there are four heterogeneous factors, each containing the only argument e0 . Combining them
318

fiE XPLOITING C AUSAL I NDEPENDENCE

c1

c3

c2

IN

B AYESIAN N ETWORK I NFERENCE

c1

c4

c3

c2

c4

e

e

e
(1)

c1

(2)

c3

c2
e1

c4
e2

c1

c2

c3

c4

e1

e2

e

e

3

e
(4)

(3)

Figure 3: A BN, its deputation and transformations.
pairwise requires 322 multiplications. The resultant factor needs to be multiplied with the deputing
factor I (e0; e), which requires 22 numerical multiplications.

9. Previous Methods
Two methods have been proposed previously for exploiting causal independence to speed up inference in general BNs (Olesen et al., 1989; Heckerman, 1993). They both use causal independence to
transform the topology of a BN. After the transformation, conventional algorithms such as CTP or
VE are used for inference.
We shall illustrate those methods by using the BN in Figure 3(1). Let  be the base combination
operator of e, let i denote the contribution of ci to e, and let fi (e; ci) be the contributing factor of ci
to e.
The parent-divorcing method (Olesen et al., 1989) transforms the BN into the one in Figure 3(3).
After the transformation, all variables are regular and the new variables e1 and e2 have the same
possible values as e. The conditional probabilities of e1 and e2 are given by

P (e1 jc1; c2)=f1(e; c1)
f2 (e; c2);
P (e2 jc3; c4)=f3(e; c3)
f4 (e; c4):

The conditional probability of e is given by

P (e=ffje1=ff1; e2=ff2 ) = 1 if ff=ff1 ff2,
for any value ff of e, ff1 of e1 , and ff2 of e2 . We shall use PD to refer to the algorithm that first
performs the parent-divorcing transformation and then uses VE for inference.
319

fiZ HANG & P OOLE

The temporal transformation by Heckerman (1993) converts the BN into the one in Figure 3(4).
Again all variables are regular after the transformation and the newly introduced variables have the
same possible values as e. The conditional probability of e1 is given by

P (e1=ffjc1) = f1(1=ff; c1);
for each value ff of e1 . For i=2; 3; 4, the conditional probability of ei (e4 stands for e) is given by

P (ei =ffjei,1 =ff1 ; ci) =

X

ff1 ff2 =ff

fi(e=ff2; ci);

for each possible value ff of ei and ff1 of ei,1 . We shall use TT to refer to the algorithm that first
performs the temporal transformation and then uses VE for inference.
The factorization represented by the original BN includes a factor that contain five variables,
while factors in the transformed BNs contain no more than three variables. In general, the transformations lead to finer-grain factorizations of joint probabilities. This is why PD and TT can be more
efficient than VE .
However, PD and TT are not as efficient as VE 1 . We shall provide empirical evidence in support
of this claim in the next section. Here we illustrate it by considering calculating P (e). Doing this in
Figure 3(3) by VE using the elimination ordering c1, c2, c3 , c4, e1 , and e2 would require 23 + 22 +
23 + 22 + 23 + 22 =36 numerical multiplications and 18 numerical additions.5 Doing the same in
Figure 3(4) using the elimination ordering c1 , e1 , c2, e2 , c3, e3 , c4 would require 22 + 23 + 22 +
23 + 22 + 23 + 22 =40 numerical multiplications and 20 numerical additions. In both cases, more
numerical multiplications and additions are performed than VE 1 . The differences are more drastic
in complex networks, as will be shown in the next section.
The saving for this example may seem marginal. It may be reasonable to conjecture that, as
Olesons method produces families with three elements, this marginal saving is all that we can hope
for; producing factors of two elements rather than cliques of three elements. However, interacting
causal variables can make the difference more extreme. For example, if we were to use Olesons
method for the BN of Figure 1, we produce6 the network of Figure 4. Any triangulation for this
network has at least one clique with four or more elements, yet VE 1 does not produce a factor with
more than two elements.
Note that as far as computing P (e) in the networks shown in Figure 3 is concerned, VE 1 is more
efficient than PD, PD is more efficient than TT, and TT is more efficient than VE . Our experiments
show that this is true in general.
5. This is exactly the same number of operations required to determine P (e) using clique-tree propagation on the same
network. The clique tree for Figure 3(3) has three cliques, one containing fc1 ; c2 ; e1 g, one containing fc3 ; c4 ; e2 g, and
once containing fe1 ; e2 ; eg. The first clique contains 8 elements; to construct it requires 22 + 23 = 12 multiplications.
The message that needs to be sent to the third clique is the marginal on e1 which is obtained by summing out c1 and
c2 . Similarly for the second clique. The third clique again has 8 elements and requires 12 multiplications to construct.
In order to extract P (e) from this clique, we need to sum out e1 and e2 . This shown one reason why VE 1 can be more
efficient that CTP or VE; VE 1 never constructs a factor with three variables for this example. Note however, that an
advantage of CTP is that the cost of building the cliques can be amortized over many queries.
6. Note that we need to produce two variables both of which represent noisy a  b. We need two variables as the noise
applied in each case is independent. Note that if there was no noise in the network  if e1 = a  b  c  we only
need to create one variable, but also e1 and e2 would be the same variable (or at least be perfectly correlated). In this
case we would need a more complicated example to show our point.

320

fiE XPLOITING C AUSAL I NDEPENDENCE

a

IN

B AYESIAN N ETWORK I NFERENCE

b

e

c

11

e21

e

e

2

1

e3

Figure 4: The result of Applying Olesons method to the BN of Figure 1.

10. Experiments
The CPCS networks are multi-level, multi-valued BNs for medicine. They were created by Pradhan
et al. (1994) based on the Computer-based Patient Case Simulation system (CPCS-PM) developed
by Parker and Miller (1987). Two CPCS networks7 were used in our experiments. One of them
consists of 422 nodes and 867 arcs, and the other contains 364 nodes. They are among the largest
BNs in use at the present time.
The CPCS networks contain abundant causal independencies. As a matter of fact, each non-root
variable is a convergent variable with base combination operator MAX. They are good test cases for
inference algorithms that exploit causal independencies.
10.1 CTP-based Approaches versus VE -based Approaches
As we have seen in the previous section, one kind of approach for exploiting causal independencies
is to use them to transform BNs. Thereafter, any inference algorithms, including CTP or VE , can be
used for inference.
We found the coupling of the network transformation techniques and CTP was not able to carry
out inference in the two CPCS networks used in our experiments. The computer ran out memory
when constructing clique trees for the transformed networks. As will be reported in the next subsection, however, the combination of the network transformation techniques and VE was able to answer
many queries.
This paper has proposed a new method of exploiting causal independencies. We have observed
that causal independencies lead to a factorization of a joint probability that is of finer-grain than
the factorization entailed by conditional independencies alone. One can extend any inference algorithms, including CTP and VE , to exploit this finer-grain factorization. This paper has extended
VE and obtained an algorithm called VE 1 . VE 1 was able to answer almost all queries in the two
CPCS networks. We conjecture, however, that an extension of CTP would not be able to carry out
inference with the two CPCS networks at all. Because the resources that VE 1 takes to answer any
query in a BN can be no more than those an extension of CTP would take to construct a clique tree
7. Obtained from ftp://camis.stanford.edu/pub/pradhan.
V1.0.txt and CPCS-networks/std1.08.5.

321

The file names are CPCS-LM-SM-K0-

fi50
45
40
35
30
25
20
15
10
5
0

Number of queries

Number of queries

Z HANG & P OOLE

"5ve1"
"5pd"
"5tt"
"5ve"

Number of queries

0 1 2 3 4 5 6 7 8 9
CPU time in seconds

50
45
40
35
30
25
20
15
10
5
0

"10ve1"
"10pd"
"10tt"
"10ve"

0 1 2 3 4 5 6 7 8 9
CPU time in seconds

50
45
40
35
30
25
20
15
10
5
0

"15ve1"
"15pd"
"15tt"

0 1 2 3 4 5 6 7 8 9 10
CPU time in seconds

Figure 5: Comparisons in the 364-node BN.

for the BN and there are, as will be seen in the next subsection, queries in the two CPCS networks
that VE 1 was not able to answer.
In summary, CTP based approaches are not or would not be able to deal with the two CPCS
networks, while VE -based approaches can (to different extents).
10.2 Comparisons of VE -based Approaches
This subsection provides experimental data to compare the VE -based approaches namely PD, TT,
and VE 1 . We also compare those approaches with VE itself to determine how much can be gained
by exploiting causal independencies.
In the 364-node network, three types of queries with one query variable and five, ten, or fifteen
observations respectively were considered. Fifty queries were randomly generated for each query
type. A query is passed to the algorithms after nodes that are irrelevant to it have been pruned. In general, more observations mean less irrelevant nodes and hence greater difficulty to answer the query.
The CPU times the algorithms spent in answering those queries were recorded.
In order to get statistics for all algorithms, CPU time consumption was limited to ten seconds
and memory consumption was limited to ten megabytes.
The statistics are shown in Figure 5. In the charts, the curve 5ve1, for instance, displays the
time statistics for VE 1 on queries with five observations. Points on the X-axis represent CPU times
322

fi50
45
40
35
30
25
20
15
10
5
0

IN

B AYESIAN N ETWORK I NFERENCE

Number of queries

Number of queries

E XPLOITING C AUSAL I NDEPENDENCE

"5ve1"
"5pd"
"5tt"
"5ve"

0 1 2 3 4 5 6 7 8 9
CPU time in seconds

40
35
30
25
20
15
10
5
0

"10ve1"
"10pd"
"10tt"

0 1 2 3 4 5 6 7 8 9 10
CPU time in seconds

Figure 6: Comparisons in the 422-node BN.
in seconds. For any time point, the corresponding point on the Y-axis represents the number of fiveobservation queries that were each answered within the time by VE 1 .
We see that while VE 1 was able to answer all the queries, PD and TT were not able to answer
some of the ten-observation and fifteen-observation queries. VE was not able to answer a majority
of the queries.
To get a feeling about the average performances of the algorithms, regard the curves as representing functions of y , instead of x. The integration, along the Y -axis, of the curve 10PD, for instance,
is roughly the total amount of time PD took to answer all the ten-observation queries that PD was
able to answer. Dividing this by the total number of queries answered, one gets the average time PD
took to answer a ten-observation query.
It is clear that on average, VE 1 performed significantly better than PD and TT, which in turn
performed much better than VE . The average performance of PD on five- or ten-observation queries
are roughly the same as that of TT, and slightly better on fifteen-observation queries.
In the 422-node network, two types of queries with five or ten observations were considered and
fifty queries were generated for each type. The same space and time limits were imposed as in the
364-node networks. Moreover, approximations had to be made; real numbers smaller than 0.00001
were regarded as zero. Since the approximations are the same for all algorithms, the comparisons
are fair.
The statistics are shown in Figure 6. The curves 5ve1 and 10ve1 are hardly visible because
they are very close to the Y -axis.
Again we see that on average, VE 1 performed significantly better than PD, PD performed significantly better than TT, and TT performed much better than VE .
One might notice that TT was able to answer thirty nine ten-observation queries, more than that
VE 1 and PD were able to. This is due to the limit on memory consumption. As we will see in the
next subsection, with the memory consumption limit increased to twenty megabytes, VE 1 was able
to answer forty five ten-observation queries exactly under ten seconds.
10.3 Effectiveness of VE 1
We have now established that VE 1 is the most efficient VE -based algorithm for exploiting causal
independencies. In this section we investigate how effective VE 1 is.
323

fiZ HANG & P OOLE

The 422-node BN
Number of queries

Number of queries

The 364-node BN
50
45
40
35
30
25
20
15
10
5
0

"5ve1"
"10ve1"
"15ve1"
"20ve1"

0 1 2 3 4 5 6 7 8 9 10
CPU time in seconds

50
45
40
35
30
25
20
15
10
5
0

"5ve1"
"10ve1"
"15ve1"

0

5 10 15 20 25 30 35 40
CPU time in seconds

Figure 7: Time statistics for VE 1 .
Experiments have been carried out in both of the two CPCS networks to answer this question. In
the 364-node network, four types of queries with one query variable and five, ten, fifteen, or twenty
observations respectively were considered. Fifty queries were randomly generated for each query
type. The statistics of the times VE 1 took to answer those queries are given by the left chart in Figure
7. When collecting the statistics, a ten MB memory limit and a ten second CPU time limit were
imposed to guide against excessive resource demands. We see that all fifty five-observation queries
in the network were each answered in less than half a second. Forty eight ten-observation queries,
forty five fifteen-observation queries, and forty twenty-observation queries were answered in one
second. There is, however, one twenty-observation query that VE 1 was not able to answer within
the time and memory limits.
In the 364-node network, three types of queries with one query variable and five, ten, or fifteen,
observations respectively were considered. Fifty queries were randomly generated for each query
type. Unlike in the previous section, no approximations were made. A twenty MB memory limit
and a forty-second CPU time limit were imposed. The time statistics is shown in the right hand side
chart. We see that VE 1 was able to answer all most all queries and a majority of the queries were
answered in little time. There are, however, three fifteen-observation queries that VE 1 was not able
to answer.

11. Conclusions
This paper has been concerned with how to exploit causal independence in exact BN inference. Previous approaches (Olesen et al., 1989; Heckerman, 1993) use causal independencies to transform
BNs. Efficiency is gained because inference is easier in the transformed BNs than in the original
BNs.
A new method has been proposed in this paper. Here is the basic idea. A Bayesian network
can be viewed as representing a factorization of a joint probability into the multiplication of a list of
conditional probabilities. We have studied a notion of causal independence that enables one to further
factorize the conditional probabilities into a combination of even smaller factors and consequently
obtain a finer-grain factorization of the joint probability.
We propose to extend inference algorithms to make use of this finer-grain factorization. This
paper has extended an algorithm called VE . Experiments have shown that the extended VE algo324

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

rithm, VE 1 , is significantly more efficient than if one first performs Olesen et al.s or Heckermans
transformation and then apply VE .
The choice of VE instead of the more widely known CTP algorithm is due to its ability to work in
networks that CTP cannot deal with. As a matter of fact, CTP was not able to deal with the networks
used in our experiments, even after Olesen et al.s or Heckermans transformation. On the other hand,
VE 1 was able to answer almost all randomly generated queries with and a majority of the queries
were answered in little time. It would be interesting to extend CTP to make use of the finer-grain
factorization mentioned above.
As we have seen in the previous section, there are queries, especially in the 422-node network,
that took VE 1 a long time to answer. There are also queries that VE 1 was not able to answer. For
those queries, approximation is a must. We employed an approximation technique when comparing
algorithms in the 422-node network. The technique captures, to some extent, the heuristic of ignoring
minor distinctions. In future work, we are developing a way to bound the error of the technique and
an anytime algorithm based on the technique.

Acknowledgements
We are grateful to Malcolm Pradhan and Gregory Provan for sharing with us the CPCS networks.
We also thank Jack Breese, Bruce DAmbrosio, Mike Horsch, Runping Qi, and Glenn Shafer for
valuable discussions, and Ronen Brafman, Chris Geib, Mike Horsch and the anonymous reviewers for very helpful comments. Mr. Tak Yin Chan has been a great help in the experimentations.
Research was supported by NSERC Grant OGPOO44121, the Institute for Robotics and Intelligent
Systems, Hong Kong Research Council grant HKUST658/95E and Sino Software Research Center
grant SSRC95/96.EG01.

Appendix A. Proof of Theorem 3
Theorem 3 Suppose a list of homogeneous factors F1 and a list of heterogeneous factors F2 constitute a tidy factorization of P (z1 ; z2; : : :; zk ). If z1 is either a convergent variable, or an old regular
variable, or a new regular variable whose deputy is not in the list fz2 ; : : :; zk g, then the procedure
sum-out1(F1 ; F2; z1) returns a tidy heterogeneous factorization of P (z2 ; : : :; zk ).
Proof: Suppose f1 , ..., fr are all the heterogeneous factors and g1, ..., gs are all the homogeneous
factors. Also suppose f1 , ..., fl , g1, ..., gm are all the factors that contain z1 . Then

P (z2 ; : : :; zk )

=

=

=

=

X

P (z1 ; z2; : : :; zk)

z1

X
z1


rj=1fj

X

s
Y
i=1

gi


lj=1fj ) 
 (
rj=l+1 fj )]

m
Y

s
Y

gi
i=1 i=m+1
m
s
X l
Y
Y
[(
j =1 fj
gi) 
 (
rj=l+1 fj )]
gi
z1
i=1
i=m+1
z1

[(

325

gi

(9)

fiZ HANG & P OOLE

=

X

m
s
Y
Y
l
r
[(

j=1 fj gi) 
 (
j=l+1fj )]
gi ;
z1
i=1
i=m+1

(10)

where equation (10) is due to Proposition 3. Equation (9) is follows from Proposition 4. As a matter
Q g due to the
of fact, if z1 is a convergent variable, then it is the only convergent variable in m
i=1 i
first condition of tidiness. The condition of Proposition 4 is satisfied because z1 does not appear
in fl+1 , ..., fr . On the other hand, if z1 is an old regular variable or a new regular variable whose
Q g contains no convergent variables due to the
deputy does not appear in the list z2 , ..., zk , then m
i=1 i
second condition of tidiness. Again the condition of Proposition 4 is satisfied. We have thus proved
that sum-out1(F1 ; F2; z1) yields a flexible heterogeneous factorization of P (z2 ; : : :; zk ).
Let e0 be a convergent variable in the list z2 , ..., zk . Then z1 cannot be the corresponding new regular variable e. Hence the factor I (e0; e) is not touched by sum-out1(F1 ; F2; z1). Consequently, if
we can show that the new factor created by sum-out1(F1 ; F2; z1) is either a heterogeneous factor
or a homogeneous factor that contain no convergent variable, then the factorization returned is tidy.
Suppose sum-out1(F1 ; F2; z1) does not create a new homogeneous factor. Then no heterogeneous factors in F1 contain z1 . If z1 is a convergent variable, say e0, then I (e0; e) is the only homoP
geneous factor that contain e0 . The new factor is e I (e0; e), which does contain any convergent
variables. If z1 is an old regular variable or a new regular variable whose deputy is not in the list z2 ,
..., zk , all the factors that contain z1 do not contain any convergent variables. Hence the new factor
again does not contain any convergent variables. The theorem is thus proved. 2
0

References
Arnborg, S., Corneil, D. G., & Proskurowski, A. (1987). Complexity of finding embedding in a
k-tree. SIAM J. Alg. Disc. Meth., 8(2), 277284.
Baker, M., & Boult, T. (1990). Pruning Bayesian networks for efficient computation. In Proc. Sixth
Conf. on Uncertainty in Artificial Intelligence, pp. 257264 Cambridge, Mass.
Bertele, U., & Brioschi, F. (1972). Nonserial dynamic programming, Vol. 91 of Mathematics in
Science and Engineering. Academic Press.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence in
Bayesian networks. In E. Horvitz and F. Jensen (Ed.), Proc. Twelthth Conf. on Uncertainty in
Artificial Intelligence, pp. 115123 Portland, Oregon.
Cooper, G. F. (1990). The computational complexity of probabilistic inference using Bayesian belief
networks. Artificial Intelligence, 42(2-3), 393405.
Dagum, P., & Galper, A. (1993). Additive belief-network models. In D. Heckerman and A. Mamdani
(Ed.), Proc. Ninth Conf. on Uncertainty in Artificial Intelligence, pp. 9198 Washington D.C.
DAmbrosio (1995). Local expression languages for probabilistic dependence. International Journal of Approximate Reasoning, 13(1), 6181.
DAmbrosio, B. (1994). Symbolic probabilistic inference in large BN2O networks. In R. Lopez de
Mantaras and D. Poole (Ed.), Proc. Tenth Conf. on Uncertainty in Artificial Intelligence, pp.
128135 Seattle.
326

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

Dechter, R. (1996). Bucket elimination: A unifying framework for probabilistic inference. In E.
Horvits and F. Jensen (Ed.), Proc. Twelthth Conf. on Uncertainty in Artificial Intelligence, pp.
211219 Portland, Oregon.
Dez, F. J. (1993). Parameter adjustment in bayes networks. the generalized noisy or-gate. In D.
Heckerman and A. Mamdani (Ed.), Proc. Ninth Conf. on Uncertainty in Artificial Intelligence,
pp. 99105 Washington D.C.
Duda, R. O., Hart, P. E., & Nilsson, N. J. (1976). Subjective Bayesian methods for rule-based inference systems. In Proc. AFIPS Nat. Comp. Conf., pp. 10751082.
Geiger, D., & Heckerman, D. (1996). Knowledge representation and inference in similarity networks
and Bayesian multinets. Artificial Intelligence, 82, 4574.
Geiger, D., Verma, T., & Pearl, J. (1990). d-separation: from theorems to algorithms. In M. Henrion
et. al. (Ed.), Uncertainty in Artificial Intelligence 5, pp. 139148. North Holland, New York.
Good, I. (1961). A causal calculus (i). British Journal of Philosophy of Science, 11, 305318.
Heckerman, D. (1993). Causal independence for knowledge acquisition and inference. In Proc. of
the Ninth Conference on Uncertainty in Artificial Intelligence, pp. 122127.
Heckerman, D., & Breese, J. (1994). A new look at causal independence. In Proc. of the Tenth
Conference on Uncertainty in Artificial Ingelligence, pp. 286292.
Henrion, M. (1987). Some practical issues in constructing belief networks. In L. Kanal and T. Levitt
and J. Lemmer (Ed.), Uncertainty in Artificial Intelligence, pp. 161174. North-Holland.
Howard, R. A., & Matheson, J. E. (1981). Influence diagrams. In Howard, R. A., & Matheson,
J. (Eds.), The Principles and Applications of Decision Analysis, pp. 720762. Strategic Decisions Group, CA.
Jensen, F. V., Lauritzen, S. L., & Olesen, K. G. (1990). Bayesian updating in causal probabilistic
networks by local computations. Computational Statistics Quaterly, 4, 269282.
Kim, J., & Pearl, J. (1983). A computational model for causal and diagnostic reasoning in inference
engines. In Proc. of the Eighth International Joint Conference on Artificial Intelligence, pp.
190193 Karlsruhe, Germany.
Kjrulff, U. (1990). Triangulation of graphs - algorithms giving small total state space. Tech. rep.
R 90-09, Department of Mathematics and Computer Science, Strandvejen, DK 9000 Aalborg,
Denmark.
Lauritzen, S. L., Dawid, A. P., Larsen, B. N., & Leimer, H. G. (1990). Independence properties of
directed markov fields. Networks, 20, 491506.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computations with probabilities on graphical
structures and their application to expert systems. Journal of the Royal Statistical Society,
Series B, 50(2), 157224.
327

fiZ HANG & P OOLE

Li, Z., & DAmbrosio, B. (1994). Efficient inference in Bayes networks as a combinatorial optimization problem. International Journal of Approximate Reasoning, 11(1), 5581.
Olesen, K. G., & Andreassen, S. (1993). Specification of models in large expert systems based on
causal probabilistic networks. Artificial Intelligence in Medicine, 5, 269281.
Olesen, K. G., Kjrulff, U., Jensen, F., Falck, B., Andreassen, S., & Andersen, S. K. (1989). A
munin network for the median nerve - a case study on loops. Applied Artificial Intelligence,
3, 384403.
Parker, R., & Miller, R. (1987). Using causal knowledge to creat simulated patient cases: the CPSC
project as an extension of Internist-1. In Proc. 11th Symp. Comp. Appl. in Medical Care, pp.
473480 Los Alamitos, CA. IEEE Comp Soc Press.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
Morgan Kaufmann, San Mateo, CA.
Poole, D. (1993). Probabilistic Horn abduction and Bayesian networks. Artificial Intelligence, 64(1),
81129.
Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering for large
belief networks. In R. Lopez de Mantaras and D. Poole (Ed.), Proc. Tenth Conf. on Uncertainty
in Artificial Intelligence, pp. 484490 Seattle.
Shachter, R. D., DAmbrosio, B. D., & Del Favero, B. D. (1990). Symbolic probabilistic inference
in belief networks. In Proc. 8th National Conference on Artificial Intelligence, pp. 126131
Boston. MIT Press.
Shafer, G., & Shenoy, P. (1990). Probability propagation. Annals of Mathematics and Artificial
Intelligence, 2, 327352.
Shortliffe, E., & Buchanan, G. B. (1975). A model of inexact reasoning in medicine. Math. Biosci.,
23, 351379.
Srinivas, S. (1993). A generalization of noisy-or model. In Proc. of the Ninth Conference on Uncertainty in Artificial Intelligence, pp. 208215.
Tarjan, R. E., & Yannakakis, M. (1984). Simple linear time algorithm to test chordality of graphs,
test acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs. SIAM J. Comput.,
13, 566579.
Zhang, N. L., & Poole, D. (1994). A simple approach to Bayesian network computations. In Proc.
of the Tenth Canadian Conference on Artificial Intelligence, pp. 171178.

328

fiJournal of Artificial Intelligence Research 5 (1996) 53-94

Submitted 3/95; published 9/96

Cue Phrase Classification Using Machine Learning
Diane J. Litman

AT&T Labs - Research, 600 Mountain Avenue
Murray Hill, NJ 07974 USA

diane@research.att.com

Abstract

Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but
also in a sentential sense to convey semantic rather than structural information. Correctly
classifying cue phrases as discourse or sentential is critical in natural language processing
systems that exploit discourse structure, e.g., for performing tasks such as anaphora resolution and plan recognition. This paper explores the use of machine learning for classifying
cue phrases as discourse or sentential. Two machine learning programs (cgrendel and
C4.5) are used to induce classification models from sets of pre-classified cue phrases and
their features in text and speech. Machine learning is shown to be an effective technique
for not only automating the generation of classification models, but also for improving
upon previous results. When compared to manually derived classification models already
in the literature, the learned models often perform with higher accuracy and contain new
linguistic insights into the data. In addition, the ability to automatically construct classification models makes it easier to comparatively analyze the utility of alternative feature
representations of the data. Finally, the ease of retraining makes the learning approach
more scalable and exible than manual methods.

1. Introduction

Cue phrases are words and phrases that may sometimes be used to explicitly signal discourse
structure in both text and speech. In particular, when used in a discourse sense, a cue
phrase explicitly conveys structural information. When used in a sentential sense, a cue
phrase instead conveys semantic rather than structural information. The following examples
(taken from a spoken language corpus that will be described in Section 2) illustrate sample
discourse and sentential usages of the cue phrases \say" and \further":
 Discourse
\: : : we might have the concept of say a researcher who has worked for fifteen years
on a certain project : : : "
\Further, and this is crucial in AI and probably for expert databases as well : : : "
 Sentential
\: : : let me just say that it bears a strong resemblance to much of the work that's
done in semantic nets and even frames."
\: : : from a place that is even stranger and further away : : : "
For example, when used in the discourse sense, the cue phrase \say" conveys the structural
information that an example is beginning. When used in the sentential sense, \say" does
not convey any structural information and instead functions as a verb.
c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiLitman

The ability to correctly classify cue phrases as discourse or sentential is critical for
natural language processing systems that need to recognize or convey discourse structure,
for tasks such as improving anaphora resolution (Grosz & Sidner, 1986; Reichman, 1985).
Consider the following example, again taken from the corpus that will be described in
Section 21 :
If the system attempts to hold rules, say as an expert database for an expert system,
then we expect it not only to hold the rules but to in fact apply them for us in
appropriate situations.
In this example, the cue phrases \say" and \then" are discourse usages, and explicitly
signal the boundaries of an intervening subtopic in the discourse structure. Furthermore,
the referents of the noun phrases \the system," \an expert database," and \an expert
system" are all possible referents for the pronoun \it." With the structural information
conveyed by the cue phrases, the system can determine that \the system" is more relevant
for interpreting the pronoun \it," as both \an expert database" and \an expert system"
occur within the embedded (and now concluded) subtopic. Without the cue phrases, the
reasoning required to determine that the referent of the \the system" is the intended referent
of \it" would be much more complex.
Correctly classifying cue phrases as discourse or sentential is important for other natural
language processing tasks as well. The discourse/sentential distinction can be used to
improve the naturalness of synthetic speech in text-to-speech systems (Hirschberg, 1990).
Text-to-speech systems generate synthesized speech from unrestricted text. If a cue phrase
can be classified as discourse or sentential using features of the input text, it can then be
synthesized using different intonational models for the discourse and sentential usages. In
addition, by explicitly identifying rhetorical and other relationships, discourse usages of cue
phrases can be used to improve the coherence of multisentential texts in natural language
generation systems (Zuckerman & Pearl, 1986; Moser & Moore, 1995). Cue phrases can
also be used to reduce the complexity of discourse processing in such areas as argument
understanding (Cohen, 1984) and plan recognition (Litman & Allen, 1987; Grosz & Sidner,
1986).
While the problem of cue phrase classification has often been noted (Grosz & Sidner,
1986), until recently, models for classifying cue phrases were neither developed nor evaluated
based on careful empirical analyses. Even though the literature suggests that some features
might be useful for cue phrase classification, there are no quantitative analyses of any actual
classification algorithms that use such features (nor any suggestions as to how different types
of features might be combined). Most systems that recognize or generate cue phrases simply
assume that discourse uses are utterance or clause initial (Reichman, 1985; Zuckerman &
Pearl, 1986). While there are empirical studies showing that the intonational prominence
of certain word classes varies with respect to discourse function (Halliday & Hassan, 1976;
Altenberg, 1987), these studies do not investigate cue phrases per se.
To address these limitations, Hirschberg and Litman (1993) conducted several empirical
studies specifically addressing cue phrase classification in text and speech. Hirschberg and
Litman pre-classified a set of naturally occurring cue phrases, described each cue phrase in
terms of prosodic and textual features (the features were posited in the literature or easy
1. This example is also described in more detail by Hirschberg and Litman (1993).

54

fiCue Phrase Classification Using Machine Learning

to automatically code), then manually examined the data to construct classification models
that best predicted the classifications from the feature values.
This paper examines the utility of machine learning for automating the construction
of models for classifying cue phrases from such empirical data. A set of experiments are
described that use two machine learning programs, cgrendel (Cohen, 1992, 1993) and
C4.5 (Quinlan, 1993), to induce classification models from sets of pre-classified cue phrases
and their features. The features, classes and training examples used in the studies of
Hirschberg and Litman (1993), as well as additional features, classes and training examples, are given as input to the machine learning programs. The results are evaluated both
quantitatively and qualitatively, by comparing both the error rates and the content of the
manually derived and learned classification models. The experimental results show that
machine learning is indeed an effective technique, not only for automating the generation
of classification models, but also for improving upon previous results. The accuracy of
the learned classification models is often higher than the accuracy of the manually derived
models, and the learned models often contain new linguistic implications. The learning
paradigm also makes it easier to compare the utility of different knowledge sources, and to
update the model given new features, classes, or training data.
The next section summarizes previous work on cue phrase classification. Section 3
then describes the machine learning approach to cue phrase classification that is taken in
this paper. In particular, the section describes four sets of experiments that use machine
learning to automatically induce cue phrase classification models. The types of inputs and
outputs of the machine learning programs are presented, as are the methodologies that are
used to evaluate the results. Section 4 presents and discusses the experimental results, and
highlights the many benefits of the machine learning approach. Section 5 discusses the
practical utility of the results of this paper. Finally, Section 6 discusses the use of machine
learning in other studies of discourse, while Section 7 concludes.

2. Previous Work on Classifying Cue Phrases
This section summarizes Hirschberg's and Litman's empirical studies of the classification of
cue phrases in speech and text (Hirschberg & Litman, 1987, 1993; Litman & Hirschberg,
1990). Hirschberg's and Litman's data (cue phrases taken from corpora of recorded and
transcribed speech, classified as discourse or sentential, and coded using both speech-based
and text-based features) will be used to create the input for the machine learning experiments. Hirschberg's and Litman's results (performance figures for manually developed cue
phrase classification models) will be used as a benchmark for evaluating the performance
of the classification models produced by machine learning.
The first study by Hirschberg and Litman investigated usage of the cue phrase \now"
by multiple speakers in a radio call-in show (Hirschberg & Litman, 1987). A classification
model based on prosodic features was developed based on manual analysis of a \training"
set of 48 examples of \now", then evaluated on a previously unseen test set of 52 examples
of \now". In a follow-up study (Hirschberg & Litman, 1993), Hirschberg and Litman tested
this classification model on a larger set of cue phrases, namely all single word cue phrases
in a technical keynote address by a single speaker. This corpus yielded 953 instances of 34
55

fiLitman

Prosodic Model:

if composition of intermediate phrase = alone then
elseif composition of intermediate phrase = :alone then
if position in intermediate phrase = first then
if accent = deaccented then
elseif accent = L* then
elseif accent = H* then
elseif accent = complex then
elseif position in intermediate phrase = :first then

discourse

discourse

discourse

sentential

sentential

sentential

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

Textual Model:

if preceding orthography = true then
elseif preceding orthography = false then

(9)
(10)

discourse
sentential

Figure 1: Decision tree representation of the manually derived classification models of
Hirschberg and Litman.
different single word cue phrases derived from the literature.2 Hirschberg and Litman also
used the cue phrases in the first 17 minutes of this corpus to develop a complementary cue
phrase classification model based on textual features (Litman & Hirschberg, 1990), which
they then tested on the full corpus (Hirschberg & Litman, 1993). The first study will be
referred to as the \now" study, and the follow-up study as the \multiple cue phrase" study.
Note that the term \multiple" means that 34 different single word cue phrases (as opposed
to just the cue phrase \now") are considered, not that cue phrases consisting of multiple
words (e.g. \by the way") are considered.
The method that Hirschberg and Litman used to develop their prosodic and textual classification models was as follows. They first separately classified each example cue phrase in
the data as discourse, sentential or ambiguous while listening to a recording and reading a
transcription.3 Each example was also described as a set of prosodic and textual features.4
Previous observations in the literature correlating discourse structure with prosodic information, and discourse usages of cue phrases with initial position in a clause, contributed to
the choice of features. The set of classified and described examples was then examined in
order to manually develop the classification models shown in Figure 1. These models are
shown here using decision trees for ease of comparison with the results of C4.5 and will be
explained below.
Prosody was described using Pierrehumbert's theory of English intonation (Pierrehumbert, 1980). In Pierrehumbert's theory, intonational contours are described as sequences
of low (L) and high (H) tones in the fundamental frequency (F0) contour (the physical
2. Figure 2 contains a list of the 34 cue phrases. Hirschberg and Litman (1993) provide full details regarding
the distribution of these cue phrases. The most frequent cue phrase is \and", which occurs 320 times.
The next most frequent cue phrase is \now", which occurs 69 times. \But," \like," \or" and \so" also
each occur more than fifty times. The four least frequent cue phrases { \essentially," \otherwise," \since"
and \therefore" { each occur 2 times.
3. The class ambiguous was not introduced until the multiple cue phrase study (Hirschberg & Litman, 1993;
Litman & Hirschberg, 1990).
4. Although a limited set of textual features were noted in the \now" data, the analysis of the \now" data
did not yield a textual classification model.

56

fiCue Phrase Classification Using Machine Learning

correlate of pitch). Intonational contours have as their domain the intonational phrase.
A finite-state grammar describes the set of tonal sequences for an intonational phrase. A
well-formed intonational phrase consists of one or more intermediate phrases followed by a
boundary tone. A well-formed intermediate phrase has one or more pitch accents followed
by a phrase accent. Boundary tones and phrase accents each consist of a single tone, while
pitch accents consist of either a single tone or a pair of tones. There are two simple pitch
accents (H* and L*) and four complex accents (L*+H, L+H*, H*+L, and H+L*). The
* indicates which tone is aligned with the stressed syllable of the associated lexical item.
Note that not every stressed syllable is accented. Lexical items that bear pitch accents are
called accented, while those that do not are called deaccented.
Prosody was manually transcribed by Hirschberg by examining the fundamental frequency (F0) contour, and by listening to the recording. This transcription process was
performed separately from the process of discourse/sentential classification. To produce the
F0 contour, the recording of the corpus was digitized and pitch-tracked using speech analysis software. This resulted in a display of the F0 where the x-axis represented time and the
y-axis represented frequency in Hz. Various phrase final characteristics (e.g., phrase accents,
boundary tones, as well as pauses and syllable lengthening) helped to identify intermediate
and intonational phrases, while peaks or valleys in the display of the F0 contour helped to
identify pitch accents. Similar manual transcriptions of prosodic phrasing and accent have
been shown to be reliable across coders (Pitrelli, Beckman, & Hirschberg, 1994).
Once prosody was coded, Hirschberg and Litman represented every cue phrase in terms
of the following prosodic features.5 Accent corresponded to the pitch accent (if any) that
was associated with the cue phrase. For both the intonational and intermediate phrases
containing each cue phrase, the feature composition of phrase represented whether or not
the cue phrase was alone in the phrase (the phrase contained only the cue phrase, or only
the cue phrase and other cue phrases). Position in phrase represented whether the cue
phrase was first (the first lexical item in the prosodic phrase unit { possibly preceded by
other cue phrases) or not.
The textual features used in the multiple cue phrase study (Hirschberg & Litman, 1993;
Litman & Hirschberg, 1990) were extracted automatically from the transcript. The part of
speech of each cue phrase was obtained by running a program for tagging words with one of
approximately 80 parts of speech (Church, 1988) on the transcript.6 Several characteristics
of the cue phrase's immediate context were also noted, in particular, whether it was immediately preceded or succeeded by orthography (punctuation or a paragraph boundary),
and whether it was immediately preceded or succeeded by a lexical item corresponding to
another cue phrase.
With this background, the classification models shown in Figure 1 can now be explained.
The prosodic model uniquely classifies any cue phrase using the features composition of
intermediate phrase, position in intermediate phrase, and accent. When a cue phrase is
uttered as a single intermediate phrase { possibly with other cue phrases (i.e., line (1) in
Figure 1), or in a larger intermediate phrase with an initial position (possibly preceded by
5. Only the features used in Figure 1 are discussed here.
6. Another syntactic feature - dominating constituent - was obtained by running the parser Fidditch (Hindle,
1989) on the transcript. However, since this feature did not appear in any models manually derived from
the training data (Litman & Hirschberg, 1990), the feature was not pursued.

57

fiLitman

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
Prosodic
24.6  3.0
14.7  3.2
Textual
19.9  2.8
16.1  3.4
Default Class
38.8  3.2
40.8  4.4

Table 1: 95% confidence intervals for the error rates (%) of the manually derived classification models of Hirschberg and Litman, testing data (multiple cue phrase corpus).
other cue phrases) and a L* accent or deaccented, it is classified as discourse. When part of
a larger intermediate phrase and either in initial position with a H* or complex accent, or
in a non-initial position, it is sentential. The textual model classifies cue phrases using only
the single feature preceding orthography.7 When a cue phrase is preceded by any type of
orthography, it is classified as discourse; otherwise, the cue phrase is classified as sentential.
When the prosodic model was used to classify each cue phrase in its training data, i.e.,
the 100 examples of \now" from which the model was developed, the error rate was 2.0%.8
The error rate of the textual model on the training examples from the multiple cue phrase
corpus was 10.6% (Litman & Hirschberg, 1990).
The prosodic and textual models were evaluated by quantifying their performance in
correctly classifying example cue phrases in two test sets of data, as shown in the rows
labeled \Prosodic" and \Textual" in Table 1. Each test set is a subset of the 953 examples
from the multiple cue phrase corpus. The first test set (878 examples) consists of only the
classifiable cue phrases, i.e., the cue phrases that both Hirschberg and Litman classified as
discourse or that both classified as sentential. Note that those cue phrases that Hirschberg
and Litman classified as ambiguous or that they were unable to agree upon are not included
in the classifiable subset. (These cue phrases will be considered in the learning experiments
described in Section 4.4, however.) The second test set, the classifiable non-conjuncts
(495 examples), was created from the classifiable cue phrases by removing all instances of
\and", \or" and \but". This subset was considered particularly reliable since 97.2% of nonconjuncts were classifiable compared to 92.1% of all example cue phrases. The error rate of
the prosodic model was 24.6% for the classifiable cue phrases and 14.7% for the classifiable
non-conjuncts (Hirschberg & Litman, 1993). The error rate of the textual model was 19.9%
for the classifiable cue phrases and 16.1% for the classifiable non-conjuncts (Hirschberg &
Litman, 1993). The last row of the table shows error rates for a simple \Default Class"
baseline model that always predicts the most frequent class in the corpus (sentential). These
rates are 38.8% for the classifiable cue phrases and 40.8% for the classifiable non-conjuncts.
7. A classification model based on part-of-speech was also developed (Litman & Hirschberg, 1990;
Hirschberg & Litman, 1993); however, it did not perform as well as the model based on orthography
(the error rate of the part-of-speech model was 36.1% in the larger test set, as opposed to 19.9% for the
orthographic model). Furthermore, a model that combined orthography and part-of-speech performed
comparably to the simpler orthographic model (Hirschberg & Litman, 1993). Hirschberg and Litman
also had preliminary observations suggesting that adjacency of cue phrases might prove useful.
8. Following Hirschberg and Litman (1993), the original 48- and 52-example sets (Hirschberg & Litman,
1987) are combined.

58

fiCue Phrase Classification Using Machine Learning

Although not computed by Hirschberg and Litman, Table 1 also associates margins of errors with each error percentage, which are used to compute confidence intervals (Freedman,
Pisani, & Purves, 1978). (The margin of error is  2 standard errors for a 95% confidence
interval using a normal table.) The lower bound of a confidence interval is computed by
subtracting the margin of error from the error rate, while the upper bound is computed by
adding the margin of error. Thus, the 95% confidence interval for the prosodic model on
the classifiable cue phrase test set is (21.6%, 27.6%). Analysis of the confidence intervals
indicates that the improvement of both the prosodic and textual models over the default
model is significant. For example, the upper bounds of the error rates of the prosodic and
textual models on the classifiable cue phrase test set - 27.6% and 22.7% - are both lower
than the lower bound of the default class error rate - 35.6%. This methodology of using statistical inference to determine whether differences in error rates are significant is discussed
more fully in Section 3.3.

3. Experiments using Machine Learning

This section describes experiments that use the machine learning programs C4.5 (Quinlan,
1993) and cgrendel (Cohen, 1992, 1993) to automatically induce cue phrase classification
models. cgrendel and C4.5 are similar to each other and to other learning methods such
as neural networks and cart (Brieman, Friedman, Olshen, & Stone, 1984) in that all induce
classification models from preclassified examples. Each program takes the following inputs:
names of the classes to be learned, names and possible values of a fixed set of features, and
the training data (i.e., a set of examples for which the class and feature values are specified).
The output of each program is a classification model, expressed in C4.5 as a decision tree
and in cgrendel as an ordered set of if-then rules. Both cgrendel and C4.5 learn the
classification models using greedy search guided by an \information gain" metric.
The first group of machine learning experiments replicate the training and testing conditions used by Hirschberg and Litman (1993) (reviewed in the previous section), to support
a direct comparison of the manual and machine learning approaches. The second group of
experiments evaluate the utility of training from larger amounts of data than was feasible
for the manual analysis of Hirschberg and Litman. The third set of experiments allow the
machine learning algorithms to distinguish among the 34 cue phrases, to evaluate the utility of developing classification models specialized for particular cue phrases. The fourth set
of experiments consider all the examples in the multiple cue phrase corpus, not just the
classifiable cue phrases. This set of experiments attempt to predict a third classification
unknown, as well as the classifications discourse and sentential. Finally, within each of these
four sets of experiments, each individual experiment learns a classification model using a
different feature representation of the training data. Some experiments consider features in
isolation, to comparatively evaluate the utility of each individual feature for classification.
Other experiments consider linguistically motivated sets of features, to gain insight into
feature interactions.

3.1 The Machine Learning Inputs

This section describes the inputs to both of the machine learning programs, namely, the
names of the classifications to be learned, the names and possible values of a fixed set of
59

fiLitman

Classification
Judge1/Judge2
All Cue Phrases
Non-Conjuncts

Total
953
509

Classifiable Cue Phrases
Discourse Sentential
D/D
S/S
341
537
202
293

Unknown
?/? D/S S/D D/? S/? ?/D ?/S
59
5
0
0
0
5
6
11
1
0
0
0
0
2

Table 2: Determining the classification of cue phrases.
features, and training data specifying the class and feature values for each example in the
training set.
3.1.1 Classifications

The first input to each learning program specifies the names of a fixed set of classifications.
Hirschberg and Litman's 3-way classification of cue phrases by 2 judges (Hirschberg &
Litman, 1993) is transformed into the classifications used by the machine learning programs
as shown in Table 2. Recall from Section 2 that each judge classified each cue phrase as
discourse, sentential, or ambiguous; these classifications are shown as D, S, and ? in Table 2.
As discussed in Section 2, the classifiable cue phrases are those cue phrases that the judges
both classified as either discourse or as sentential usages. Thus, in the machine learning
experiments, a cue phrase is assigned the classification discourse if both judges classified it
as discourse (D/D, as shown in column 3 of Table 2). Similarly, a cue phrase is assigned the
classification sentential if both judges classified it as sentential (S/S, as shown in column
4). 878 (92.1%) of the 953 examples in the full corpus were classifiable, while 495 (97.2%)
of the 509 non-conjuncts were classifiable.
For some of the machine learning experiments, a third cue phrase classification will also
be considered. In particular, a cue phrase is assigned the classification unknown if both
Hirschberg and Litman classified it as ambiguous (?/?, as shown in column 5), or if they
were unable to agree upon its classification (D/S, S/D, D/?, S/?, ?/D, ?/S, as shown in
columns 6-11). In the full corpus, 59 cue phrases (6.2%) were judged ambiguous by both
judges (?/?). There were only 5 cases (.5%) of true disagreement (D/S). 11 cue phrases
(1.2%) were judged ambiguous by the first judge but classified by the second judge (?/D
and ?/S). When the conjunctions \and," \or" and \but" were removed from the corpus,
only 11 examples (2.2%) were judged ambiguous by both judges: 3 instances of \actually,"
2 instances each of \because" and \essentially," and 1 instance of \generally," \indeed,"
\like" and \now." There was only 1 case (.2%) of true disagreement (an instance of \like").
2 cue phrases (.4%) - an instance each of \like" and \otherwise" - were judged ambiguous
by the first judge.
3.1.2 Features

A second component of the input to each learning program specifies the names and potential
values of a fixed set of features. The set of primitive features considered in the learning
experiments are shown in Figure 2. Feature values can either be a numeric value or one of a
fixed set of user-defined symbolic values. The feature representation shown here follows the
representation of Hirschberg and Litman except as noted. Length of intonational phrase (P60

fiCue Phrase Classification Using Machine Learning

 Prosodic Features

{ length of intonational phrase (P-L): integer.
{ position in intonational phrase (P-P): integer.
{ length of intermediate phrase (I-L): integer.
{ position in intermediate phrase (I-P): integer.
{ composition of intermediate phrase (I-C): only, only cue phrases, other.
{ accent (A): H*, L*, L*+H, L+H*, H*+L, H+L*, deaccented, ambiguous.
{ accent* (A*): H*, L*, complex, deaccented, ambiguous.
 Textual Features
{ preceding cue phrase (C-P): true, false, NA.
{ succeeding cue phrase (C-S): true, false, NA.
{ preceding orthography (O-P): comma, dash, period, paragraph, false, NA.
{ preceding orthography* (O-P*): true, false, NA.
{ succeeding orthography (O-S): comma, dash, period, false, NA.
{ succeeding orthography* (O-S*): true, false, NA.
{ part-of-speech (POS): article, coordinating conjunction, cardinal numeral, subordinating conjunction,
preposition, adjective, singular or mass noun, singular proper noun, intensifier, adverb, verb base form,
NA.

 Lexical Feature

{ token (T): actually, also, although, and, basically, because, but, essentially, except, finally, first, further,

generally, however, indeed, like, look, next, no, now, ok, or, otherwise, right, say, second, see, similarly,
since, so, then, therefore, well, yes.

Figure 2: Representation of features, for use by C4.5 and cgrendel.
L) and length of intermediate phrase (I-L) represent the number of words in the intonational
and intermediate phrases containing the cue phrase, respectively. This feature was not coded
in the \now" data, but was coded (although not used) in the later multiple cue phrase
data. Position in intonational phrase (P-P) and position in intermediate phrase (I-P) use
numeric values rather than the earlier symbolic values (e.g., first in Figure 1). Composition
of intermediate phrase (I-C) replaces the value alone (meaning that the phrase contained
only the example cue phrase, or only the example plus other cue phrases) from Figure 1
with the more primitive values only and only cue phrases (whose disjunction is equivalent to
alone); I-C also uses the value other rather than :alone (as was used in Figure 1). Accent
(A) uses the value ambiguous to represent all cases where the prosodic analysis yields a
disjunction (e.g., \H*+L or H*"). Accent* (A*) re-represents some of the symbolic values
of the feature accent (A) using a more abstract level of description. In particular, L*+H,
L+H*, H*+L, and H+L* are represented as separate values in A but as a single value { the
superclass complex { in A*. While useful abstractions can often result from the learning
process, A* is explicitly represented in advance as it is a prosodic feature representation
that has the potential to be automated (see Section 5).
In all the textual features, the value NA (not applicable) reects the fact that 39 recorded
examples were not included in the transcription, which was done independently of the
61

fiLitman

studies performed by Hirschberg and Litman (1993). In the coding used by Hirschberg and
Litman, preceding cue phrase (C-P) and succeeding cue phrase (C-S) represented the actual
cue phrase (e.g., \and") when there was a preceding or succeeding cue phrase; here the value
true encodes all such cases. As with the prosodic feature set A*, preceding orthography*
(O-P*) and succeeding orthography* (O-S*) re-represent some of the symbolic values of
preceding orthography (O-P) and succeeding orthography (O-S), respectively, using a more
abstract level of description (e.g., comma, dash, and period are represented as separate values
in O-S but as the single value true in O-S*). This is done because the reliability of coding
detailed transcriptions of orthography is not known. Part-of-speech (POS) represents the
part of speech assigned to each cue phrase by Church's program for tagging part of speech in
unrestricted text (Church, 1988); while the program can assign approximately 80 different
values, only the subset of values that were actually assigned to the cue phrases in the
transcripts of the corpora are shown in the figure. Finally, the lexical feature token (T) is
new to this study, and represents the actual cue phrase being described.
3.1.3 Training Data

The final input to each learning program is training data, i.e., a set of examples for which
the class and feature values are specified. Consider the following utterance, taken from the
multiple cue phrase corpus (Hirschberg & Litman, 1993):
Example 1 [(Now) (now that we have all been welcomed here)] it's time to get on with
the business of the conference.
This utterance contains two cue phrases, corresponding to the two instances of \now". The
brackets and parentheses illustrate the intonational and intermediate phrases, respectively,
that contain the example cue phrases. Note that a single intonational phrase contains both
examples, but that each example is uttered in a different intermediate phrase. If we were
only interested in the feature length of intonational phrase (P-L), the two examples would
be represented in the training data as follows:
P-L Class
9 discourse
9 sentential
The first column indicates the value assigned to the feature P-L, while the second column
indicates how the example was classified. Thus, the length of the intonational phrase
containing the first instance of \now" is 9 words, and the example cue phrase is classified
as a discourse usage. If we were only interested in the feature composition of intermediate
phrase (I-C), the two examples would instead be represented in the training data as follows:
I-C Class
only discourse
other sentential
That is, the intermediate phrase containing the first instance of \now" contains only the
cue phrase \now", while the intermediate phrase containing the second instance of \now"
contains \now" as well as 7 other lexical items that are not cue phrases. Note that while
the value of P-L is the same for both examples, the value of I-C is different.
62

fiCue Phrase Classification Using Machine Learning

3.2 The Machine Learning Outputs

The output of both machine learning programs are classification models. In C4.5 the model
is expressed as a decision tree, which consists of either a leaf node (a class assignment), or a
decision node (a test on a feature, with one branch and subtree for each possible outcome of
the test). The following example illustrates the non-graphical representation for a decision
node testing a feature with n possible values:
if test1 then : : :
:::

elseif testn then

:::

Tests are of the form \feature operator value"9 . \Feature" is the name of a feature (e.g.
accent), while \value" is a valid value for that feature (e.g., deaccented). For features with
symbolic values (e.g., accent), there is one branch for each symbolic value, and the operator
\=" is used. For features with numeric values (e.g., length of intonational phrase), there
are two branches, each comparing the numeric value with a threshold value; the operators
\" and \>" are used. Given a decision tree, a cue phrase is classified by starting at the
root of the tree and following the appropriate branches until a leaf is reached. Section 4
shows example decision trees produced by C4.5.
In cgrendel the classification model is expressed as an ordered set of if-then rules of
the following form:
if test1 ^ : : : ^ testk then class
The \if" part of a rule is a conjunction of tests on the values of (varying) features, where
tests are again of the form \feature operator value." As in C4.5, \feature" is the name of
a feature, and \value" is a valid value for that feature. Unlike C4.5, the operators = or =
6
are used for features with symbolic values, while  or  are used for features with numeric
values. The \then" part of a rule specifies a class assignment (e.g, discourse). Given a set
of if-then rules, a cue phrase is classified using the rule whose \if" part is satisfied. If there
or two or more such rules and the rules disagree on the class of an example, cgrendel
applies one of two conict resolution strategies (chosen by the user): choose the first rule,
or choose the rule that is most accurate on the data. The experiments reported here use
the second strategy. If there are no such rules, cgrendel assigns a default class. Section 4
shows example rules produced by cgrendel.
Both C4.5 and cgrendel learn their classification models using greedy search guided
by an \information gain" metric. C4.5 uses a divide and conquer process: training examples
are recursively divided into subsets (using the tests discussed above), until all of the subsets
belong to a single class. The test chosen to divide the examples is that which maximizes
a metric called a gain ratio (a local measure of progress, which does not consider any
subsequent tests); this metric is based on information theory and is discussed in detail by
Quinlan (1993). Once a test is selected, there is no backtracking. Ideally, the set of chosen
tests should result in a small final decision tree. cgrendel generates its set of if-then rules
using a method called separate and conquer (to highlight the similarity with divide and
conquer):
9. An additional type of test may be invoked by a C4.5 option.

63

fiLitman

Many rule learning systems generate hypotheses using a greedy strategy in which
rules are added to the rule set one by one in an effort to form a small cover of
the positive examples; each rule, in turn is created by adding one condition
after another to the antecedent until the rule is consistent with the negative
data. (Cohen, 1993)
Although cgrendel is claimed to have two advantages over C4.5, these advantages do
not come into play for the experiments reported here. First, if-then rules appear to be easier
for people to understand than decision trees (Quinlan, 1993). However, for the cue phrase
classification task, the decision trees produced by C4.5 are quite compact and thus easily
understood. Furthermore, a rule representation can be derived from C4.5 decision trees,
using the program C4.5rules. Second, cgrendel allows users to exploit prior knowledge of
a learning problem, by constraining the syntax of the rules that can be learned. However, no
prior knowledge is exploited in the cue phrase experiments. The main reason for using both
C4.5 and cgrendel is to increase the reliability of any comparisons between the machine
learning and manual results. In particular, if comparable results are obtained using both
C4.5 and cgrendel, then any performance differences between the learned and manually
derived classification models are less likely to be due to the specifics of a particular learning
program, and more likely to reect the learned/manual distinction.

3.3 Evaluation

The output of each machine learning experiment is a classification model that has been
learned from the training data. These learned models are qualitatively evaluated by examining their linguistic content, and by comparing them with the manually derived models of
Figure 1. The learned models are also quantitatively evaluated by examining their error
rates on testing data and by comparing these error rates to each other and to the error
rates shown in Table 1. The error rate of a classification model is computed by using the
model to predict the classifications for a set of examples where the classifications are already
known, then comparing the predicted and known classifications. In the cue phrase domain,
the error rate is computed by summing the number of discourse examples misclassified as
sentential with the number of sentential examples misclassified as discourse, then dividing
by the total number of examples.
The error rates of the learned classification models are estimated using two methodologies. Train-and-test error rate estimation (Weiss & Kulikowski, 1991) \holds out" a test
set of examples, which are not seen until after training is completed. That is, the model is
developed by examining only the training examples; the error of the model is then estimated
by using the model to classify the test examples. This was the evaluation method used by
Hirschberg and Litman. The resampling method of cross-validation (Weiss & Kulikowski,
1991) estimates error rate using multiple train-and-test experiments. For example, in 10fold cross-validation, instead of dividing examples into training and test sets once, 10 runs of
the learning program are performed. The total set of examples is randomly divided into 10
disjoint test sets; each run thus uses the 90% of the examples not in the test set for training
and the remaining 10% for testing. Note that for each iteration of the cross-validation, the
learning process begins from scratch; thus a new classification model is learned from each
training sample. An estimated error rate is obtained by averaging the error rate on the test64

fiCue Phrase Classification Using Machine Learning

ing portion of the data from each of the 10 runs. While this method does not make sense for
humans, computers can truly ignore previous iterations. For sample sizes in the hundreds
(the classifiable subset of the multiple cue phrase sample and the classifiable non-conjunct
subset provide 878 and 495 examples, respectively) 10-fold cross-validation often provides
a better performance estimate than the hold-out method (Weiss & Kulikowski, 1991). The
major advantage is that in cross-validation all examples are eventually used for testing, and
almost all examples are used in any given training run.
The best performing learned models are identified by comparing their error rates to
the error rates of the other learned models and to the manually derived error rates. To
determine whether the fact that an error rate E1 is lower than another error rate E2 is
also significant, statistical inference is used. In particular, confidence intervals for the two
error rates are computed, at a 95% confidence level. When an error rate is estimated using
only a single error rate on a test set (i.e., the train-and-test methodology), the confidence
interval is computed using a normal approximation to the binomial distribution (Freedman
et al., 1978). When the error rate is estimated using the average from multiple error
rates (i.e., the cross-validation methodology), the confidence interval is computed using a
t-Table (Freedman et al., 1978). If the upper bound of the 95% confidence interval for E1
is lower than the lower bound of the 95% confidence interval for the error rate E2, then the
difference between E1 and E2 is assumed to be significant.10

3.4 The Experimental Conditions
This section describes the conditions used in each set of machine learning experiments. The
experiments differ in their use of training and testing corpora, methods for estimating error
rates, and in the features and classifications used. The actual results of the experiments are
presented in Section 4.
3.4.1 Four Sets of Experiments

The learning experiments can be conceptually divided into four sets. Each experiment in
the first set estimates error rate using the train-and-test method, where the training and
testing samples are those used by Hirschberg and Litman (1993) (the \now" data and the
two subsets of the multiple cue phrase corpus, respectively). This allows a direct comparison
of the manual and machine learning approaches. However, only the prosodic experiments
conducted by Hirschberg and Litman (1993) are replicated. The textual training and testing
conditions are not replicated as the original training corpus (the first 17 minutes of the
multiple cue phrase corpus) (Litman & Hirschberg, 1990) is a subset of, rather than disjoint
from, the test corpus (the full 75 minutes of the multiple cue phrase corpus) (Hirschberg &
Litman, 1993).
In contrast, each experiment in the second set uses cross-validation to estimate error
rate. Furthermore, both training and testing samples are taken from the multiple cue
phrase corpus. Each experiment uses 90% of the examples from the multiple cue phrase
data for training, and the remaining 10% for testing. Thus each experiment in the second
set trains from much larger amounts of data (790 classifiable examples, or 445 classifiable
10. Thanks to William Cohen for suggesting this methodology.

65

fiLitman

prosody
hl93features
phrasing
length
position
intonational
intermediate
text
adjacency
orthography
preceding
succeeding
speech-text

P-L
X
X
X
X

P-P I-L I-P
X X X
X
X X X
X
X
X
X
X X

I-C A A*
X X X
X X X
X
X

C-P

X
X
X

X

X

X

X

X

X

X

X

C-S O-P

X
X
X
X

O-P* O-S

O-S* POS

X

X

X

X

X
X

X
X

X

X

X

X

X
X

X
X

X

X

Table 3: Multiple feature sets and their components.
non-conjuncts) than each experiment in the first set (100 \nows"). The reliability of the
testing is not compromised due to the use of cross-validation (Weiss & Kulikowski, 1991).
Each experiment in the third set replicates an experiment in the second set, with the exception that the learning program is now allowed to distinguish between cue phrases. This
is done by adding a feature representing the cue phrase (the feature token from Figure 2)
to each experiment from the second set. Since the potential use of such a lexical feature
was noted but not used by Hirschberg and Litman (1993), these experiments provide qualitatively new linguistic insights into the data. For example, the same features may now be
used differently to predict the classifications of different cue phrases or sets of cue phrases.
Finally, each experiment in the fourth set replicates an experiment in the first, second,
and third set, with the exception that all 953 examples in the multiple cue phrase corpus
are now considered. This is because in practice, any learned cue phrase classification model
will likely be used to classify all cue phrases, even those that are dicult for human judges
to classify. The experiments in the fourth set allow the learning programs to attempt to
learn the class unknown, in addition to the classes discourse and sentential.
3.4.2 Feature Representations within Experiment Sets

Within each of these four sets of experiments, each individual experiment represents the
data using a different subset of the available features. First, the data is represented in
each of 14 single feature sets, corresponding to each prosodic and textual feature shown in
Figure 2. These experiments comparatively evaluate the utility of each individual feature
for classification. The representations of Example 1 shown above illustrate how data is
represented using the single feature set P-L, and using the single feature set I-C.
Second, the data is represented in each of the 13 multiple feature sets shown in Table 3.
Each of these sets contains a linguistically motivated subset of at least 2 of the 14 features.
The first 7 sets use only prosodic features. Prosody considers all the prosodic features that
were coded for each example cue phrase. Hl93features considers only the coded features
that were also used in the model shown in Figure 1. Phrasing considers all features of both
the intonational and intermediate phrases containing the example cue phrase (i.e., length
66

fiCue Phrase Classification Using Machine Learning

Example 1 [(

) (now that we have all been welcomed here)] it's time to get on with the business of the conference.
P-P I-L I-P I-C
A
A*
C-P C-S O-P O-P* O-S O-S* POS Class
1
1
1 only H*+L complex f
t
par. t
f
f
adv. disc.
2
8
1 other H*
H*
t
f
f
f
f
f
adv. sent.
Now

P-L
9
9

Figure 3: Representation of Example 1 in feature set speech-text.
of phrase, position of example in phrase, and composition of phrase). Length and position
each consider only one of these features, but with respect to both the intonational and
intermediate phrase. Conversely, intonational and intermediate each consider only one type
of phrase, but consider all of the features. The next 5 sets use only textual features. Text
considers all the textual features. Adjacency and orthography each consider a single textual
feature, but consider both the preceding and succeeding immediate context. Preceding and
succeeding consider contextual features relating to both orthography and cue phrases, but
limit the context. The last set, speech-text, uses all of the prosodic and textual features.
Figure 3 illustrates how the two example cue phrases in Example 1 would be represented
using speech-text. Consider the feature values for the first example cue phrase. Since this
example is the first lexical item in both the intonational and intermediate phrases which
contain it, its position in both phrases (P-P and I-P) is 1. Since the intermediate phrase
containing the cue phrase contains no other lexical items, its length (I-L) is 1 word and its
composition (I-C) is only the cue phrase. The values for A and A* indicate that when the
intonational phrase is described as a sequence of tones, the complex pitch accent H*+L is
associated with the cue phrase. With respect to the textual features, the utterance was
transcribed such that it began a new paragraph. Thus the example cue phrase was not
preceded by another cue phrase (C-P), but it was preceded by a form of orthography (O-P
and O-P*). Since the example cue phrase was immediately followed by another instance
of \now" in the transcription, the cue phrase was succeeded by another cue phrase (C-S)
but was not succeeded by orthography (O-S and O-S*). Finally, the output of the part of
speech tagging program when run on the transcript of the corpus yields the value adverb
for the cue phrase's part of speech (POS).
The first set of experiments replicate only the prosodic experiments conducted by
Hirschberg and Litman (1993); cue phrases are represented using the subset of the feature sets that only consist of prosodic features. In the second set of experiments, examples
are represented using all 27 different feature sets (the 14 single feature sets and the 13
multiple feature sets). In the third set of experiments, examples are represented using 27
tokenized feature sets, constructed by adding the lexical feature token from Figure 2 (the
cue phrase being described) to each of the 14 single and 13 multiple feature sets from the
second set of experiments. These tokenized feature sets will be referred to using the names
of the single and multiple feature sets, concatenated with \+". The following illustrates
how the two cue phrases in Example 1 would be represented using P-L+:
P-L T
Class
9 now discourse
9 now sentential
67

fiLitman

The representation is similar to the P-L representation shown earlier, except for the second
column which indicates the value assigned to the feature token (T).

4. Results

This section examines the results of running the two learning programs { C4.5 and cgrendel { in the four sets of cue phrase classification experiments described above. The learned
classification models will be compared with the classification models shown in Figure 1,
while the error rates of the learned classification models will be compared with the error
rates shown in Table 1 and with the error rates of the other learned models. As will be
seen, the results suggest that machine learning is useful for automating the generation of
linguistically viable classification classification models, for generating classification models
that perform with lower error rates than manually developed hypotheses, and for adding to
the body of linguistic knowledge regarding cue phrases.

4.1 Experiment Set 1: Replicating Hirschberg and Litman

The first group of experiments replicate the training, testing, and evaluation conditions
used by Hirschberg and Litman (1993), in order to investigate how well machine learning
performs in comparison to the manual development of cue phrase classification models.
Figure 4 shows the best performing prosodic classification models learned by the two
machine learning programs; the top of the figure replicates the manually derived prosodic
model from Figure 1 for ease of comparison. When all of the prosodic features are used
to represent the 100 training examples of \now" (i.e., each example is represented using
feature set prosody from Table 3)11, the classification models that are learned are shown
after the manually derived model at the top of Figure 4. Note that using both learning
programs, the same decision tree is also learned when the smaller feature sets phrasing and
position are used to represent the \now" data. The bottom portion of the figure shows the
classification models that are learned when the same examples are represented using only
the single prosodic feature position in intonational phrase (P-P); the same model is also
learned when the examples are represented using the multiple feature set intonational.
Recall that C4.5 represents each learned classification model as a decision tree. Each
level of the tree (shown by indentation) specifies a test on a single feature, with a branch for
every possible outcome of the test. A branch can either lead to the assignment of a class, or
to another test. For example, the C4.5 classification model learned from prosody classifies
cue phrases using the two features position in intonational phrase (P-P) and position in
intermediate phrase (I-P). Note that not all of the available features in prosody (recall
Table 3) are used in the decision tree. The tree initially branches based on the value of
the feature position in intonational phrase.12 The first branch leads to the class assignment
discourse. The second branch leads to a test of the feature position in intermediate phrase.
The first branch of this test leads to the class assignment discourse, while the second branch
leads to sentential. C4.5 produces both unsimplified and pruned decision trees. The goal
11. In Experiment Set 1, the feature set prosody does not contain the features P-L and I-L. Recall that
phrasal length was only coded in the later multiple cue phrase study.
12. For ease of comparison to Figure 1, the original symbolic representation of the feature value is used
rather than the integer representation shown in Figure 2.

68

fiCue Phrase Classification Using Machine Learning

Manually derived prosodic model (repeated from Figure 1):

if composition of intermediate phrase = alone then
elseif composition of intermediate phrase = :alone then
if position in intermediate phrase = first then
if accent = deaccented then
elseif accent = L* then
elseif accent = H* then
elseif accent = complex then
elseif position in intermediate phrase = :first then

discourse

discourse

discourse

sentential

sentential

sentential

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

Decision tree learned from prosody, from phrasing, and from position using C4.5:

if position in intonational phrase = first then
elseif position in intonational phrase = :first then
if position in intermediate phrase = first then
elseif position in intermediate phrase = :first then
discourse

discourse
sentential

Ruleset learned from prosody, from phrasing, and from position using CGRENDEL:

if (position in intonational phrase 6= first) ^ (position in intermediate phrase 6= first) then
default is on discourse

sentential

Decision tree learned from P-P and from intonational using C4.5:

if position in intonational phrase = first then
elseif position in intonational phrase = :first then

discourse
sentential

Ruleset learned from P-P and from intonational using CGRENDEL:

if position in intonational phrase 6= first then
default is on discourse

sentential

Figure 4: Example C4.5 and cgrendel classification models learned from different prosodic
feature representations of the \now" data.

69

fiLitman

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
P-P
18.3  2.6
16.6  3.4
prosody
27.3  3.0
17.8  3.4
phrasing
27.3  3.0
17.8  3.4
position
27.3  3.0
17.8  3.4
intonational
18.3  2.6
16.6  3.4
manual prosodic
24.6  3.0
14.7  3.2

Table 4: 95%-confidence intervals for the error rates (%) of the best performing cgrendel
prosodic classification models, testing data. (Training data was the \now" corpus;
testing data was the multiple cue phrase corpus.)
of the pruning process is to take a complex decision tree that may also be overfitted to the
training data, and to produce a tree that is more comprehensible and whose accuracy is
not comprised (Quinlan, 1993). Since almost all trees are improved by pruning (Quinlan,
1993), only simplified decision trees are considered in this paper.
In contrast, cgrendel represents each learned classification model as a set of if-then
rules. Each rule specifies a conjunction of tests on various features, and results in the
assignment of a class. For example, the cgrendel ruleset learned from prosody classifies
cue phrases using the two features position in intonational phrase (P-P) and position in
intermediate phrase (I-P) (the same two features used in the C4.5 decision tree). If the
values of both features are not first, the if-then rule applies and the cue phrase is classified
as sentential. If the value of either feature is first, the default applies and the cue phrase is
classified as discourse.
An examination of the learned classification models of Figure 4 shows that they are
comparable in content to the portion of the manually derived model that classifies cue
phrases solely on phrasal position (line (8)). In particular, all of the classification models
say that if the cue phrase is not in an initial phrasal position classify it as sentential.
On the other hand, the manually derived model also assigns the class sentential given an
initial phrasal position in conjunction with certain combinations of phrasal composition and
accent; the learned classification models instead classify the cue phrase as discourse in all
other cases. As will be shown, the further discrimination of the manually obtained model
does not significantly improve performance when compared to the learned classification
models, and in fact in one case significantly degrades performance.
The error rates of the learned classification models on the \now" training data from
which they were developed is as follows: 6% for the models learned from prosody, phrasing
and position, and 9% for the models learned from P-P and intonational. Recall from
Section 2 that the error rate of the manually developed prosodic model of Figure 1 on
the same training data was 2%.
Table 4 presents 95% confidence intervals for the error rates of the best performing
cgrendel prosodic classification models. For ease of comparison, the row labeled \manual
prosodic" presents the error rates of the manually developed prosodic model of Figure 1 on
the same two test sets, which were originally shown in Table 1. The table includes all the
cgrendel models whose performance matches or exceeds the manual performance.
70

fiCue Phrase Classification Using Machine Learning

Comparison of the error rates of the learned and manually developed models suggests
that machine learning is an effective technique for automating the development of cue phrase
classification models. In particular, within each test set, the 95% confidence interval for
the error rate of the classification models learned from the multiple feature sets prosody,
phrasing, and position each overlaps with the confidence interval for the error rate of the
manual prosodic model. This is also true for the error rates of P-P and intonational in the
classifiable non-conjunct test set. Thus, machine learning supports the automatic construction of a variety of cue phrase classification models that achieve similar performance as the
manually constructed models.
The results from P-P and from intonational in the classifiable cue phrase test set are
shown in italics, as they suggest that machine learning may also be useful for improving
performance. Although the very simple classification model learned from P-P and intonational performs worse than the manually derived model on the training data, when tested
on the classifiable cue phrases, the learned model (with an upper bound error rate of 20.9%)
outperforms the manually developed model (with a lower bound error rate of 21.6%). This
suggests that the manually derived model might have been overfitted to the training data,
i.e., that the prosodic feature set most useful for classifying \now" did not generalize to
other cue phrases. As noted above, the use of simplified learned classification models helps
to guard against overfitting in the learning approach. The ease of inducing classification
models from many different sets of features using machine learning supports the generation
and evaluation of a wide variety of hypotheses (e.g. P-P, which was a high performing but
not the optimal performing model on the training data).
Note that the manual prosodic manual performs significantly better in the smaller test
set (which does not contain the cue phrases \and", \or", and \but"). In contrast, the
performance improvement for P-P and intonational in the smaller test set is not significant.
This also suggests that the manually derived model does not generalize as well as the learned
models.
Finally, for the feature sets shown in Table 4, the decision trees produced by C4.5 perform
with the same error rates as the rulesets produced by cgrendel, for both test sets. Recall
from Figure 4 that the C4.5 decision trees and cgrendel rules are in fact semantically
equivalent for each feature set. The fact that comparable results are obtained using C4.5
and cgrendel adds an extra degree of reliability to the experiments. In particular, the
duplication of the results suggests that the ability to match and perhaps even to improve
upon manual performance by using machine learning is not due to the specifics of either
learning program.

4.2 Experiment Set 2: Using Different Training Sets
The second group of experiments evaluate the utility of training from larger amounts of
data. This is done by using 10-fold cross-validation to estimate error, where for each run
90% of the examples in a sample are used for training (and over the 10 runs, all of the
examples are used for testing). In addition, the experiments in this second set take both
the training and testing data from the multiple-cue phrase corpus, in contrast to the previous
set of experiments where the training data was taken from the \now" corpus. As will be
seen, these changes improve the results, such that more of the learned classification models
71

fiLitman

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
P-L
33.0  5.9
(33.2  1.9)
P-P
16.1  3.5
18.8  4.2
I-L
25.5  3.7
(25.6  2.8)
I-P
25.9  4.9
19.4  3.1
I-C
(36.5  5.4)
(35.2  3.4)
A
28.6  3.6
(30.2  3.1)
A*
28.3  4.3
(28.4  1.7)
prosody
15.5  2.6
17.2  3.1
hl93features
29.4  3.3
18.2  4.2
phrasing
16.1  3.4
19.6  3.9
length
26.1  3.8
(27.4  3.4)
position
18.2  2.3
19.4  2.8
intonational
17.0  4.0
20.6  3.6
intermediate
21.9  2.3
19.4  5.7
manual prosodic
24.6  3.0
14.7  3.2

Table 5: 95%-confidence intervals for the error rates (%) of all cgrendel prosodic classification models, testing data. (Training and testing were done from the multiple
cue phrase corpus using cross-validation.)
perform with lower or comparable error rates when compared to the manually developed
models.
4.2.1 Prosodic Models

Table 5 presents the error rates of the classification models learned by cgrendel, in
the 28 different prosodic experiments. (For Experiment Sets 2 and 3, the C4.5 error rates
are presented in Appendix A.) Each numeric cell shows the 95% confidence interval for the
error rate, which is equal to the error percentage obtained by cross-validation  the margin
of error ( 2.26 standard errors, using a t-Table). The top portion of the table considers
the models learned from the single prosodic feature sets (Figure 2), the middle portion
considers the models learned from the multiple feature sets (Table 3), while the last row
considers the manually developed prosodic model. The error rates shown in italics indicate
that the performance of the learned classification model exceeds the performance of the
manual model (given the same test set). The error rates shown in parentheses indicate the
opposite case - that the performance of the manual model exceeds the performance of the
learned model. Such cases were omitted in Table 4.
As in Experiment Set 1, comparison of the error rates of the learned and manually
developed models suggests that machine learning is an effective technique for not only
automating the development of cue phrase classification models, but also for improving
performance. When evaluated on the classifiable cue phrase test set, five learned models
have improved performance compared to the manual model; all of the models except I-C
perform at least comparably to the manual model. Note that in Experiment Set 1, only two
learned models outperformed the manual model, and only five learned models performed
at least comparably. The ability to use large training sets thus appears to be an advantage
of the automated approach.
72

fiCue Phrase Classification Using Machine Learning

Manually derived prosodic model (repeated from Figure 1):

if composition of intermediate phrase = alone then
elseif composition of intermediate phrase = :alone then
if position in intermediate phrase = first then
if accent = deaccented then
elseif accent = L* then
elseif accent = H* then
elseif accent = complex then
elseif position in intermediate phrase = :first then

discourse

discourse

discourse

sentential

sentential

sentential

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

Decision tree learned from P-P using C4.5:

if position in intonational phrase  1 then
elseif position in intonational phrase > 1 then

discourse
sentential

Ruleset learned from P-P using CGRENDEL:

if position in intonational phrase  2 then
default is on discourse

sentential

Decision tree learned from prosody using C4.5:

if position in intonational phrase  1 then
if position in intermediate phrase  1 then
elseif position in intermediate phrase > 1 then
elseif position in intonational phrase > 1 then
if length of intermediate phrase  1 then
elseif length of intermediate phrase > 1 then

discourse
sentential

discourse
sentential

Ruleset learned from prosody using CGRENDEL:

if (position in intonational phrase  2) ^ (length of intermediate phrase  2) then
if (7  position in intonational phrase  4) ^ (length of intonational phrase  10) then
if (length of intermediate phrase  2) ^ (length of intonational phrase  7) ^ (accent = H*) then
if (length of intermediate phrase  2) ^ (length of intonational phrase  9) ^ (accent = H*+L) then
if (length of intermediate phrase  2) ^ (accent = deaccented) then
if (length of intermediate phrase  8) ^ (length of intonational phrase  9) ^ (accent = L*) then
sentential

sentential

sentential
sentential

sentential

sentential

default is on discourse

Figure 5: Example C4.5 and cgrendel classification models learned from different prosodic
feature representations of the classifiable cue phrases in the multiple cue phrase
corpus.
When tested on the classifiable non-conjuncts (where the error rate of the manually
derived model decreases), machine learning is useful for automating but not for improving
performance. This might reect the fact that the manually derived theories already achieve
optimal performance with respect to the examined features in this less noisy subcorpus,
and/or that the automatically derived theory for this subcorpus was based on a smaller
training set than used in the larger subcorpus.
An examination of some of the best performing learned classification models shows that
they are quite comparable in content to relevant portions of the prosodic model of Figure 1,
and often contain further linguistic insights. Consider the classification model learned from
the single feature position in intonational phrase (P-P), shown near the top of Figure 5.
73

fiLitman

Both of the learned classification models say that if the cue phrase is not in the initial
position of the intonational phrase, classify as sentential; otherwise classify as discourse.
Note the correspondence with line (8) in the manually derived prosodic model. Also note
that the classification models are comparable13 to the P-P classification models learned
from Experiment Set 1 (shown in Figure 4), despite the differences in training data. The
fact that the single prosodic feature position in intonational phrase (P-P) can classify cue
phrases at least as well as the more complicated manual and multiple feature learned models
is again a new result of the learning experiments.
Figure 5 also illustrates the more complex classification models learned using prosody,
the largest prosodic feature set. The C4.5 model is similar to lines (1) and (8) of the manual
model. (The length value 1 is equivalent to the composition value alone.) In the ruleset
induced from prosody by cgrendel, the first 2 if-then rules correlate sentential status with
(among other things) non-initial position14 , and the second 2 rules with H* and H*+L
accents; these rules are similar to lines (6)-(8) in Figure 1. However, the last 2 if-then rules
in the ruleset also correlate no accent and L* with sentential status when the phrase is of a
certain length, while lines (4) and (5) in Figure 1 provide a different interpretation and do
not take length into account. Recall that length was coded by Hirschberg and Litman only
in their test data. Length was thus never used to generate or revise their prosodic model.
The utility of length is a new result of this experiment set.
Although not shown, the models learned from phrasing, position, and intonational also
outperform the manual model. As can be seen from Table 3, these models correspond to
all of the feature sets that are supersets of P-P but subsets of prosody.
4.2.2 Textual Models

Table 6 presents the error rates of the classification models learned by cgrendel, in the
24 different textual experiments. Unlike the experiments involving the prosodic feature sets,
none of the learned textual models perform significantly better than the manually derived
model. However, the results suggest that machine learning is still an effective technique
for automating the development of cue phrase classification models. In particular, five
learned models (O-P, O-P*, text, orthography, and preceding) perform comparably to the
manually derived model, in both test sets. Note that these five models are learned from
the five textual feature sets that include either the feature O-P or O-P* (recall Figure 2
and Table 3). These models perform significantly better than all of the remaining learned
textual models.
Figure 6 shows the best performing learned textual models. Note the similarity to the
manually derived model. As with the prosodic results, the best performing single feature
models perform comparably to those learned from multiple features. In fact, in cgrendel,
the rulesets learned from the multiple feature sets orthography and preceding are identical
to the rulesets learned from the single features O-P and O-P*, even though more features
were available for use. (The corresponding error rates in Table 6 are not identical due to the
13. The different feature values in the two figures reect the fact that phrasal position was represented in
the \now" corpus using symbolic values (as in Figure 1), and in the multiple cue phrase corpus using
integers (as in Figure 2).
14. Tests such as \feature  x" and \feature  y" are merged in the figure for simplicity, e.g., \y  feature
 x."

74

fiCue Phrase Classification Using Machine Learning

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
C-P
(40.7  6.2)
(40.2  4.5)
C-S
(41.3  5.9)
(39.8  4.2)
O-P
20.6  5.7
17.6  3.3
O-P*
18.4  3.7
17.2  2.4
O-S
(34.1  6.3)
(30.2  1.8)
O-S*
(35.2  5.5)
(32.6  3.0)
POS
(37.7  4.1)
(38.2  4.6)
text
18.8  4.2
19.0  3.6
adjacency
(39.7  5.7)
(40.2  3.4)
orthography
18.9  3.4
18.8  3.0
preceding
18.8  3.8
17.6  3.2
succeeding
(33.9  6.0)
(30.0  2.7)
manual textual
19.9  2.8
16.1  3.4

Table 6: 95%-confidence intervals for the error rates (%) of all cgrendel textual classification models, testing data. (Training and testing were done from the multiple
cue phrase corpus using cross-validation.)
Manually derived textual model (repeated from Figure 1):
if preceding orthography = true then discourse
elseif preceding orthography = false then sentential
Decision tree learned from O-P*, from text, from orthography, and from preceding using C4.5:

if preceding orthography* = NA then
elseif preceding orthography* = false then
elseif preceding orthography* = true then

discourse
sentential
discourse

Ruleset learned from O-P, from O-P*, from orthography, and from preceding using CGRENDEL:

if preceding orthography* = false then
default is on discourse

sentential

Ruleset learned from text using CGRENDEL:

if preceding orthography* = false then
if part-of-speech = article then
default is on discourse

sentential

sentential

Figure 6: Example C4.5 and cgrendel classification models learned from different textual
feature representations of the classifiable cue phrases in the multiple cue phrase
corpus.
estimation using cross-validation.) The cgrendel model text also incorporates the feature
part-of-speech. In C4.5, the models text, orthography and preceding are all identical to O-P*.
4.2.3 Prosodic/Textual Models

Table 7 presents the error rates of the classification models learned by cgrendel when the
data is represented using speech-text, the complete set of prosodic and textual features (recall
75

fiLitman

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
speech-text
15.9  3.2
14.6  4.6
manual prosodic
24.6  3.0
14.7  3.2
manual textual
19.9  2.8
16.1  3.4

Table 7: 95%-confidence intervals for the error rates (%) of the cgrendel prosodic/textual
classification model, testing data. (Training and testing were done from the multiple cue phrase corpus using cross-validation.)
Table 3). Since Hirschberg and Litman did not develop a similar classification model that
combined both types of features, for comparison the last two rows show the error rates of
the separate prosodic and textual models. Only when the learned model is compared to the
manual prosodic model, using the classifiable cue phrases for testing, does learning result in
a significant performance improvement. This is consistent with the results discussed above,
where several learned prosodic models performed better than the manually derived prosodic
model in this test set. The performance of speech-text is not significantly better or worse
than the performance of either the best prosodic or textual learned models (Tables 5 and 6,
respectively).
Figure 7 shows the C4.5 and cgrendel hypotheses learned from speech-text. The C4.5
model classifies cue phrases using the prosodic and textual features that performed best in
isolation (position in intonational phrase and preceding orthography*, as discussed above), in
conjunction with the additional feature length of intermediate phrase (which also appears
in the model learned from prosody in Figure 5). Like line (9) in the manually derived
textual model, the learned model associates the presence of preceding orthography with
the class discourse. Unlike line (10), however, cue phrases not preceded by orthography
may be classified as either discourse or sentential, based on prosodic feature values (which
were not available for use by the textual model). The branch of the learned decision tree
corresponding to the last three lines is also similar to lines (1), (2), and (8) of the manually
derived prosodic model. (Recall that a length value of 1 is equivalent to a composition value
alone.)
The cgrendel model uses similar features to those used by C4.5 as well as the prosodic
feature accent (also used in prosody in Figure 5), and the textual features part-of-speech
(also used in text in Figure 6) and preceding cue phrase. Like C4.5, and unlike line (10)
of the manually derived textual model, the cgrendel model classifies cue phrases lacking
preceding orthography as sentential only in conjunction with certain other feature values.
Unlike line (9) in the manual model, the learned model also classifies some cue phrases with
preceding orthography as sentential (if the orthography is a comma, and other feature values
are present). Finally, the third and fifth learned rules elaborate line (6) with additional
prosodic as well as textual features, while the first and last learned rules elaborate line (8).

4.3 Experiment Set 3: Adding the Feature token

Each experiment in the third group replicates an experiment from the second group, with
the exception that the data representation now also includes the lexical feature token from
76

fiCue Phrase Classification Using Machine Learning

Manually derived prosodic model (repeated from Figure 1):

if composition of intermediate phrase = alone then
elseif composition of intermediate phrase = :alone then
if position in intermediate phrase = first then
if accent = deaccented then
elseif accent = L* then
elseif accent = H* then
elseif accent = complex then
elseif position in intermediate phrase = :first then

discourse

discourse

discourse

sentential

sentential

sentential

Manually derived textual model (repeated from Figure 1):

if preceding orthography = true then
elseif preceding orthography = false then

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)

discourse
sentential

Decision tree learned from speech-text using C4.5:

if position in intonational phrase  1 then
if preceding orthography* = NA then
elseif preceding orthography* = true then
elseif preceding orthography* = false then
if length of intermediate phrase > 12 then
elseif length of intermediate phrase  12 then
if length of intermediate phrase  1 then
elseif length of intermediate phrase > 1 then
elseif position in intonational phrase > 1 then
if length of intermediate phrase  1 then
elseif length of intermediate phrase > 1 then
discourse

discourse

discourse
discourse
sentential

discourse

sentential

Ruleset learned from speech-text using CGRENDEL:

if (preceding orthography = false) ^ (4  position in intonational phrase  6) ^ then
if (preceding orthography = false) ^ (length of intermediate phrase  2) then
if (preceding orthography = false) ^ (length of intonational phrase  7) ^ (preceding cue phrase = NA)
^ (accent = H*) then
if (preceding orthography = comma) ^ (length of intermediate phrase  5) ^ (length of intonational phrase  17)
^ (part-of-speech = adverb) then
if (preceding orthography = comma) ^ (3  length of intonational phrase  8) ^ (accent = H*) then
if (preceding orthography = comma) ^ (3  length of intermediate phrase  8)
^ (length of intonational phrase  15) then
if (position in intonational phrase  2) ^ (length of intermediate phrase 2)
^ (preceding cue phrase = NA) then
sentential

sentential

sentential

sentential

sentential

sentential

default is on discourse

sentential

Figure 7: C4.5 and cgrendel classification models learned from the prosodic/textual feature representation of the classifiable cue phrases in the multiple cue phrase corpus.

77

fiLitman

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
P-L+
21.8  4.6
17.4  2.7
P-P+
16.7  2.8
14.8  5.0
I-L+
20.3  3.4
16.0  3.3
I-P+
25.1  4.1
17.0  3.6
I-C+
27.0  3.6
18.4  3.4
A+
19.8  3.2
12.8  3.1
A*+
18.6  3.8
15.4  2.8
prosody+
16.7  2.9
15.8  3.1
hl93features+
24.0  4.5
17.4  4.3
phrasing+
14.5  3.3
12.6  3.3
length+
18.6  2.0
16.2  3.5
position+
15.6  3.3
13.0  3.9
intonational+
15.1  2.2
16.6  4.6
intermediate+
18.5  3.7
16.6  4.0
manual prosodic
24.6  3.0
14.7  3.2

Table 8: 95%-confidence intervals for the error rates (%) of all cgrendel prosodic, tokenized classification models, testing data. (Training and testing were done from
the multiple cue phrase corpus using cross-validation.)
Figure 2. These experiments investigate how performance changes when classification models are allowed to treat different cue phrases differently. As will be seen, learning from
tokenized feature sets often further improves the performance of the learned classification
models. In addition, the classification models now contain new linguistic information regarding particular tokens (e.g., \so").
4.3.1 Prosodic Models

Table 8 presents the error of the learned classification models on both test sets from the
multiple cue phrase corpus, for each of the tokenized prosodic feature sets. Again, the error
rates in italics indicate that the performance of the learned classification model meaningfully
exceeds the performance of the \manual prosodic" model (which did not consider the feature
token).
One way that the improvement obtained by adding the feature token can be seen is by
comparing the performance of the learned and manually derived models. In Table 8, six
cgrendel classification models have lower (italicized) error rates than the manual model.
In Table 5, only five of these models are italicized. Thus, adding the feature token results
in an additional learned model - length+ - outperforming the manually derived model.
Conversely, in Table 8, no learned models perform significantly worse than the manually
derived manual. In contrast, in Table 5, several non-tokenized models perform worse than
the manual model (I-C in the larger test set, and P-L, I-L, I-C, A, A*, and length in the
non-conjunct test set).
The improvement obtained by adding the feature token can also be seen by comparing
the performance of the tokenized (Table 8) and non-tokenized (Table 5) versions of each
model to each other. For convenience, cases where tokenization yields improvement are
highlighted in Table 9. The table shows that the error rate of the tokenized versions of the
feature sets is significantly lower than the error of the non-tokenized versions, for P-L, I-C,
78

fiCue Phrase Classification Using Machine Learning

Model
P-L
I-L
I-C
A
A*
length

Classifiable Cue Phrases (N=878)
Non-Tokenized Tokenized (+)
33.0  5.9
21.8  4.6
36.5  5.4
27.0  3.6
28.6  3.6
19.8  3.2
28.3  4.3
18.6  3.8
26.1  3.8
18.6  2.0

Classifiable Non-Conjuncts (N=495)
Non-Tokenized
Tokenized (+)
33.2  1.9
17.4  2.7
25.6  2.8
16.0  3.3
35.2  3.4
18.4  3.4
30.2  3.1
12.8  3.1
28.4  1.7
15.4  2.8
27.4  3.4
16.2  3.5

Table 9: Cases where adding the feature token improves the performance of a prosodic
model.
A, A*, and length in both test sets, and for I-L in only the non-conjunct test set. Note the
overlap between the feature sets of Table 9 and those discussed in the previous paragraph.
Figure 8 shows several tokenized single feature prosodic classification models. The first
cgrendel model in the figure shows the ruleset learned from P-L+, which reduces the
33.2%  1.9% error rate of P-L (length of intonational phrase) to 17.4%  2.7%, when
trained and tested using the classifiable non-conjuncts (Table 9). Note that the first rule
uses only a prosodic feature (like the rules of Experiment Sets 1 and 2), and is in fact
similar to line (1) of the manual model. (Recall that the length value 1 is equivalent to
the composition value alone.) However, unlike the rules of the previous experiment sets,
the next 5 rules use both the prosodic feature and the lexical feature token. Also unlike
the rules of the previous experiment sets, the remaining rules classify cue phrases using
only the feature token. Examination of the learned rulesets in Figures 8 and 9 shows that
the same cue phrases often appear in this last type of rule. Some of these cue phrases, for
example, \finally", \however", and \ok", are in fact always discourse usages in the multiple
cue phrase corpus. For the other cue phrases, classifying cue phrases using only token
corresponds to classifying cue phrases using their default class (the most frequent type of
usage in the multiple cue phrase corpus). Recall the use of a non-tokenized default class
model in Table 1.
The second example shows the ruleset learned from I-C+ (composition of intermediate
phrase+). The first rule corresponds to line (1) of the manually derived model.15 The
next six rules classify particular cue phrases as discourse, independently of the value of I-C.
Note that although in this model the cue phrase \say" is classified using only token, in the
previous model a more sophisticated strategy for classifying \say" could be found.
The third example shows the cgrendel ruleset learned from A+ (accent+). The first
rule corresponds to line (5) of the manually derived prosodic model. In contrast to line
(4), however, cgrendel uses deaccenting to predict discourse for only the tokens \say"
and \so." If the token is \finally", \however", \now" or \ok", discourse is assigned (for all
accents). In all other deaccented cases, sentential is assigned (using the default). Similarly,
in contrast to line (7), the complex accent L+H* predicts discourse for the cue phrases
\further" and \indeed" (and also for \finally", \however", \now" and \ok"), and sentential
otherwise.
15. As discussed in relation to Figure 2, the I-C values only and only cue phrases in the multiple cue phrase
corpus replace the value alone in the \now" corpus.

79

fiLitman

Manually derived prosodic model (repeated from Figure 1):

if composition of intermediate phrase = alone then
elseif composition of intermediate phrase = :alone then
if position in intermediate phrase = first then
if accent = deaccented then
elseif accent = L* then
elseif accent = H* then
elseif accent = complex then
elseif position in intermediate phrase = :first then

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

discourse

discourse

discourse

sentential

sentential

sentential

Ruleset learned from P-L+ using CGRENDEL:

if length of intonational phrase  1 then
if (7  length of intonational phrase  11) ^ (token = although) then
if (9  length of intonational phrase  16) ^ (token = indeed) then
if (length of intonational phrase  20) ^ (token = say) then
if (11  length of intonational phrase  13) ^ (token = then) then
if (length of intonational phrase = 5) ^ (token = well) then
if token = finally then
if token = further then
if token = however then
if token = now then
if token = ok then
if token = otherwise then
if token = so then
discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default is on sentential

Ruleset learned from I-C+ using CGRENDEL:

if composition of intermediate phrase = only then
if token = finally then
if token = however then
if token = now then
if token = ok then
if token = say then
if token = so then

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default is on sentential

Ruleset learned from A+ using CGRENDEL:

if accent = L* then
if (accent = deaccented) ^ (token = say) then
if (accent = deaccented) ^ (token = so) then
if (accent = L+H*) ^ (token = further) then
if (accent = L+H*) ^ (token = indeed) then
if token = finally then
if token = however then
if token = now then
if token = ok then
discourse

discourse

discourse
discourse

discourse

discourse

discourse

discourse

discourse

default is on sentential

Figure 8: Example cgrendel classification models learned from different tokenized,
prosodic feature representations of the classifiable non-conjuncts in the multiple
cue phrase corpus.

80

fiCue Phrase Classification Using Machine Learning

Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
C-P+
(28.2  3.9)
16.4  4.6
C-S+
(28.9  3.6)
17.2  4.0
O-P+
17.5  4.4
10.0  3.1
O-P*+
17.7  2.9
12.2  2.9
O-S+
26.9  4.7
18.4  3.9
O-S*+
(27.3  3.5)
16.0  3.2
POS+
(27.4  3.6)
17.2  3.9
text+
18.4  3.0
12.0  2.6
adjacency+
(28.6  4.1)
15.2  3.1
orthography+
17.6  3.0
13.6  3.9
preceding+
17.0  4.1
13.6  2.6
succeeding+
25.6  3.9
18.0  4.5
manual textual
19.9  2.8
16.1  3.4

Table 10: 95%-confidence intervals for the error rates (%) of all cgrendel textual, tokenized classification models, testing data. (Training and testing were done from
the multiple cue phrase corpus using cross-validation.)
To summarize, new prosodic results of Experiment Set 3 are that features relating to
length, composition, and accent, while not useful (in isolation) for predicting the classification of all cue phrases, are in fact quite useful for predicting the class of individual cue
phrases or subsets of cue phrases. (Recall that the result of Experiment Sets 1 and 2 was
that without token, only the prosodic feature position in intonational phrase was useful in
isolation.)
4.3.2 Textual Models

Table 10 presents the error of the learned classification models on both test sets from
the multiple cue phrase corpus, for each of the tokenized textual feature sets. As in Experiment Set 2 (Table 6), none of the cgrendel classification models have lower (italicized)
error rates than the manual model. However, adding the feature token does improve the
performance of many of the learned rulesets, in that the following models (unlike their
non-tokenized counterparts) are no longer outperformed by the manual model: O-S+ and
succeeding+ in the larger test set, and C-P+, C-S+, O-S+, O-S*+, POS+, adjacency+,
and succeeding+ in the non-conjunct test set.
The improvement obtained by adding the feature token can also be seen by comparing
the performance of the tokenized (Table 10) and non-tokenized (Table 6) versions of each
model to each other, as shown in Table 11. The table shows that the error rates of the
tokenized versions of the feature sets are significantly lower than the error of the nontokenized versions, for C-P, C-S, POS, and adjacency in both test sets, and for O-P, O-S,
O-S*, text, and succeeding in the non-conjunct test set. Note the overlap between the feature
sets of Table 11 and those discussed in the previous paragraph.
Figure 9 shows several tokenized single textual feature classification models. The first
cgrendel model shows the ruleset learned from C-P+ (preceding cue phrase+), which
reduces the 40.2%  4.5% error rate of C-P to 16.4%  4.6% when trained and tested using
the classifiable non-conjuncts (Table 11). This ruleset correlates preceding cue phrases with
discourse usages of \indeed", and omitted transcriptions of \further", \now", and \so" with
81

fiLitman

Manually derived textual model (repeated from Figure 1):

if preceding orthography = true then
elseif preceding orthography = false then

discourse
sentential

Ruleset learned from C-P+ using CGRENDEL:

if (preceding cue phrase = true) ^ (token = indeed) then
if (preceding cue phrase = NA) ^ (token = further) then
if (preceding cue phrase = NA) ^ (token = now) then
if (preceding cue phrase = NA) ^ (token = so) then
if token = although then
if token = finally then
if token = however then
if token = ok then
if token = say then
if token = similarly then

discourse
discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default is on sentential

discourse

Ruleset learned from O-P+ using CGRENDEL:

if preceding orthography = false then
if (preceding orthography = comma) ^ (token = then) then
sentential

default is on discourse

sentential

Ruleset learned from O-S+ using CGRENDEL:

if succeeding orthography = comma then
if (succeeding orthography = false) ^ (token = so) then
if succeeding orthography = NA then
if token = although then
if token = finally then
if token = now then
if token = ok then
if token = say then
discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default is on sentential

Ruleset learned from POS+ using CGRENDEL:

if (part-of-speech = adverb) ^ (token = finally) then
if (part-of-speech = singular proper noun) ^ (token = further) then
if (part-of-speech = adverb) ^ (token = however) then
if (part-of-speech = adverb) ^ (token = indeed) then
if (part-of-speech = subordinating conjunction) ^ (token = so) then
if token = although then
if token = now then
if token = say then
if token = ok then
discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

discourse

default is on sentential

Figure 9: Example cgrendel classification models learned from different tokenized, textual
feature representations of the classifiable non-conjuncts in the multiple cue phrase
corpus.

82

fiCue Phrase Classification Using Machine Learning

Model
C-P
C-S
O-P
O-S
O-S*
POS
text
adjacency
succeeding

Classifiable Cue Phrases (N=878)
Non-Tokenized Tokenized (+)
40.7  6.2
28.2  3.9
41.3  5.9
28.9  3.6
37.7  4.1
27.4  3.6
39.7  5.7
28.6  4.1
-

Classifiable Non-Conjuncts (N=495)
Non-Tokenized
Tokenized (+)
40.2  4.5
16.4  4.6
39.8  4.2
17.2  4.0
17.6  3.3
10.0  3.1
30.2  1.8
18.4  3.9
32.6  3.0
16.0  3.2
38.2  4.6
17.2  3.9
19.0  3.6
12.0  2.6
40.2  3.4
15.2  3.1
30.0  2.7
18.0  4.5

Table 11: Cases where adding the feature token improves the performance of a textual
model.
discourse usages. The classifications for the rest of the cue phrases are predicted using only
the feature token.
The second example shows the cgrendel ruleset learned from O-P+ (preceding orthography+). This ruleset correlates no preceding orthography with sentential usages of cue
phrases (as in both the manually derived model and the learned models from Experiment
Set 2). Unlike those models, however, the cue phrase \then" is also classified as sentential,
even when it is preceded by orthography (namely, by a comma).
The third example shows the cgrendel ruleset learned from O-S+ (succeeding orthography). This ruleset correlates the presence of succeeding commas with discourse usages of
cue phrases, except for the cue phrase \so", which is classified as a discourse usage without
any succeeding orthography. The model also correlates cue phrases that were omitted from
the transcript with discourse usages. The classifications for the rest of the cue phrases are
predicted using only the feature token.
The last example shows the cgrendel ruleset learned from POS+ (part-of-speech+).
This ruleset classifies certain cue phrases as discourse usages depending on both part-ofspeech and token, as well as independently of part-of-speech.
Finally, Figure 10 shows the classification model learned from text+, the largest tokenized textual feature set. Note that three of the four features used in the tokenized, single
textual feature models of Figure 9 are incorporated into this tokenized, multiple textual
feature model.
To summarize, new textual results of Experiment Set 3 are that features based on adjacent cue phrases, succeeding orthography, and part-of-speech, while not useful (in isolation)
for predicting the classification of all cue phrases, are in fact quite useful in conjunction with
only the feature token. (Recall that the result of Experiment Set 2 was that without token,
only the textual features preceding orthography and preceding orthography* were useful in
isolation.)
4.3.3 Prosodic/Textual Models

Table 12 presents the error rates of the classification models learned by cgrendel
when the data is represented using speech-text+, the complete set of prosodic and textual
83

fiLitman

Ruleset learned from text+ using CGRENDEL:

if preceding orthography = false then
if (preceding orthography = comma) ^ (token = although) then
if (preceding orthography = comma) ^ (token = no) then
if (preceding orthography = comma) ^ (token = then) then
if (succeeding orthography = false) ^ (preceding cue phrase = NA) ^ (token = similarly) then
if token = actually then
if token = first then
if token = since then
if token = yes then
sentential

sentential

sentential

sentential

sentential

sentential

sentential

sentential

sentential

default is on discourse

Figure 10: cgrendel classification model learned from a tokenized, multiple textual feature
representation of the classifiable non-conjuncts in the multiple cue phrase corpus.
Model
Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495)
speech-text+
16.9  3.4
16.6  4.1
manual prosodic
24.6  3.0
14.7  3.2
manual textual
19.9  2.8
16.1  3.4

Table 12: 95%-confidence intervals for the error rates (%) of the cgrendel
prosodic/textual, tokenized classification models, testing data. (Training and
testing were done from the multiple cue phrase corpus using cross-validation.)
features. As in Experiment Set 2, the performance of speech-text+ is not better than the
performance of either the best learned (tokenized) prosodic or textual models (Tables 8
and 10, respectively).
Comparison of Tables 7 and 12 also shows that for the feature set speech-text, tokenization does not improve performance. This is in contrast to the prosodic and textual feature
sets, where tokenization improves the performance of many learned models (namely those
shown in Tables 9 and 11).

4.4 Experiment Set 4: Adding the Classification ambiguous
In practice, a cue phrase classification model will have to classify all the cue phrases in a
recording or text, not just those that are \classifiable." The experiments in the fourth set
replicate the experiments in Experiment Sets 1, 2, and 3, with the exception that all 953 cue
phrases in the multiple cue phrase corpus are now used. This means that cue phrases are
now classified as discourse, sentential, as well as unknown (defined in Table 2). Experiment
Set 4 investigates whether machine learning can explicitly recognize the new class unknown.
Recall that the studies of Hirschberg and Litman did not attempt to predict the class
unknown, as it did not occur in their \now" training corpus. Thus in Experiment Set 1, the
class unknown similarly can not be learned from the training data. However, the unknown
examples can be added to the testing data of Experiment Set 1. Obviously performance will
degrade, as the models must incorrectly classify each unknown example as either discourse
84

fiCue Phrase Classification Using Machine Learning

or sentential. For example, when tested on the full corpus of 953 example cue phrases,
the 95% confidence intervals for the error rates of P-P and intonational are 24.8%  2.8%;
recall that when tested on the subset of the corpus corresponding to the 878 classifiable cue
phrases, the error was 18.3%  2.6% (Table 4).
Unfortunately, the results of rerunning Experiment Sets 2 and 3 do not show promising
results for classifying cue phrases as unknown. Despite the presence of 75 examples of
unknown, most of the learned models still classify unknown cue phrases as only discourse or
sentential. For example, when cgrendel is used for learning, only 2 of the possible 27 nontokenized models16 (phrasing and speech-text) contain rules that predict the class unknown.
Furthermore, each of these models only contains one rule for unknown, and each of these
rules only applies to 2 of the possible 953 examples! Similarly, only four of the possible 27
tokenized models (length+, phrasing+, prosody+, and speech-text+) contain at least one rule
for the class unknown. When compared to training and testing using only the classifiable
cue phrases in the corpus, the error rate on the full corpus is typically (but not always)
significantly higher. The best performing model in Experiment Set 4 is speech-text+, with
a 22.4%  4.1% error rate (95% confidence interval).
In sum, Experiment Set 4 addressed a problem that was previously unexplored in the
literature - the ability to develop classification models that predict not only discourse and
sentential usages of cue phrases, but also usages which human judges find dicult to classify.
Unfortunately, the results of the experiments suggest that learning how to classify cue
phrases as unknown is a dicult problem. Perhaps with more training data (recall that
there are only 75 examples of unknown) or with additional features better results could be
obtained.

4.5 Discussion
The experimental results suggest that machine learning is a useful tool for both automating
the generation of classification models and improving upon manually derived results. In
Experiment Sets 1 and 2 the performance of many of the learned classification models is
comparable to the performance of the manually derived models. In addition, when tested
on the classifiable cue phrases, several learned prosodic classification models (as well as
the learned prosodic/textual model) outperform Hirschberg and Litman's manually derived
prosodic model. Experiment Set 3 shows that learning from tokenized feature sets even
further improves performance, especially in the non-conjunct test set. More tokenized than
non-tokenized learned models perform at least as well as the manually derived models.
Many tokenized learned models also outperform their non-tokenized counterparts.
While the textual classification models do not outperform the better prosodic classification models, they have the advantage that the textual feature values are obtained directly
from the transcript, while determining the values of prosodic features requires manual analysis. (See, however, Section 5 for a discussion of the feasibility of automating the prosodic
analysis. In addition, a transcript may not always be available.) On the other hand, almost
all the high performing textual models are dependent on orthography. While manual tran16. Recall that Experiment Sets 2 and 3 constructed 14 prosodic models, 12 textual models, and 1
prosodic/textual model.

85

fiLitman

scriptions of prosodic features have been shown to be reliable across coders (Pitrelli et al.,
1994), there are no corresponding results for the reliability of orthography.
Examination of the best performing learned models shows that they are often comparable in content to the relevant portions of the manually derived models. Examination
of the models also provides new contributions to the cue phrase literature. For example,
Experiment Sets 1 and 2 demonstrate the utility of classifying cue phrases based on only a
single prosodic feature - phrasal position.17 Experiment Set 2 also demonstrates the utility
of the prosodic feature length and the textual feature preceding cue phrase for classifying
cue phrases - in conjunction with other prosodic and textual features. Finally, the results of
Experiment Set 3 demonstrate that even though many features are not useful by themselves
for classifying all cue phrases, they may nonetheless be very informative in their tokenized
form. This is true for the prosodic features based on phrasal length, phrasal composition,
and accent, and for the textual features based on adjacent cue phrases, succeeding position,
and part-of-speech.18

5. Utility
The results of the machine learning experiments are quite promising, in that when compared
to manually derived classification models already in the literature, the learned classification
models often perform with comparable if not higher accuracy. Thus, machine learning
appears to be an effective technique for automating the generation of classification models.
However, given that the experiments reported here still rely on manually created training
data, a discussion of the practical utility of the results is in order.
Even given manually created training data, the results established by Hirschberg and
Litman (1993) - obtained using even less automation than the experiments of this paper
- are already having practical import. In particular, the manually derived cue phrase
classification models are used to improve the naturalness of the synthetic speech in a text-tospeech system (Hirschberg, 1990). Using the text-based model, the text-to-speech system
classifies each cue phrase in a text to be synthesized as either a discourse or sentential
usage. Using the prosodic model, the system then conveys this usage by synthesizing the
cue phrase with the appropriate type of intonation. The speech synthesis could be further
improved (and the output made more varied) by using any one of the higher performing
learned prosodic models presented in this paper.
The results of this paper could also be directly applied in the area of text generation.
For example, Moser and Moore (1995) are concerned with the implementation of cue selection and placement strategies in natural language generation systems. Such systems could
be enhanced by using the text-based models of cue phrase classification (particularly the
17. The empirical studies performed by Holte (1993) show that for many other datasets, the accuracy of
single feature rules and decision trees is often competitive with the accuracy of more complex learned
models.
18. In contrast, the prosodic features phrasal composition and accent were previously known to be useful
in conjunction with each other and with phrasal position (Hirschberg & Litman, 1993), while part-ofspeech was known to be useful only in conjunction with orthography (Hirschberg & Litman, 1993).
Length, adjacent cue phrases, and succeeding position were not used in either of the manually derived
models (Hirschberg & Litman, 1993) (although length and adjacent cue phrases were shown to be useful
- again only in conjunction with other prosodic and textual features - in Experiment Set 2).

86

fiCue Phrase Classification Using Machine Learning

tokenized models) to additionally specify preceding and succeeding orthography, part-ofspeech, and adjacent cue phrases that are appropriate for discourse usages.
Finally, if the results of this paper could be fully automated, they could also be used in
natural language understanding systems, by enhancing their ability to recognize discourse
structure. The results obtained by Litman and Passonneau (1995) and Passonneau and
Litman (in press) suggest that algorithms that use cue phrases (in conjunction with other
features) to predict discourse structure outperform algorithms that do not take cue phrases
into account. In particular, Litman and Passonneau develop several algorithms that explore
how features of cue phrases, prosody and referential noun phrases can be best combined
to predict discourse structure. Quantitative evaluations of their results show that the best
performing algorithms all incorporate the use of discourse usages of cue phrases (where cue
phrases are classified as discourse using only phrasal position). As discussed in Section 1,
discourse structure is useful for performing tasks such as anaphora resolution and plan
recognition. Recent work has also shown that if discourse structure can be recognized, it
can be used to improve retrieval of text (Hearst, 1994) and speech (Stieman, 1995).
Although the prosodic features were manually labeled by Hirschberg and Litman, there
are recent results suggesting that at least some aspects of prosody can be automatically
labeled directly from speech. For example, Wightman and Ostendorf (1994) develop an
algorithm that is able to automatically recognize prosodic phrasing with 85-86% accuracy
(measured by comparing automatically derived labels with hand-marked labels); this accuracy is only slightly less than human-human accuracy. Recall that the experimental results
of this paper show that models learned from the single feature position in intonational
phrase - which could be automatically computed given such an automatic prosodic phrasing algorithm - perform at least as well as any other learned prosodic model. Similarly,
accenting versus deaccenting can be automatically labeled with 88% accuracy (Wightman
& Ostendorf, 1994), while a more sophisticated labeling scheme that distinguishes between
four types of accent classes (and is somewhat similar to the prosodic feature accent* used
in this paper) can be labeled with 85% accuracy (Ostendorf & Ross, in press). Recall from
Experiment Set 3 that the tokenized models learned using accent* also classify cue phrases
with good results.
Although the textual features were automatically extracted from a transcript, the transcript itself was manually created. Many natural language understanding systems do not
deal with speech at all, and thus begin with such textual representations. In spoken language systems the transcription process is typically automated using a speech recognition
system (although this introduces further sources of error).

6. Related Work
This paper has both compared the results obtained using machine learning to previously
existing manually-obtained results, and has also used machine learning as a tool for developing theories given new linguistic data (as in the models resulting from Experiment Set 3,
where the new feature token was considered). Siegel (1994) similarly uses machine learning
(in particular, a genetic learning algorithm) to classify cue phrases from a previously unstudied set of textual features: a feature corresponding to token, as well as textual features
containing the lexical or orthographic item immediately to the left of and in the 4 positions
87

fiLitman

to the right of the example. Siegel's input consists of one judge's non-ambiguous examples
taken from the data used by Hirschberg and Litman (1993) as well as additional examples;
his output is in the form of decision trees. Siegel reports a 21% estimated error rate, with
half of the corpus used for training and half for testing. Siegel and McKeown (1994) also
propose a method for developing linguistically viable rulesets, based on the partitioning of
the training data produced during induction.
Machine learning has also been used in several other areas of discourse analysis. For example, learning has been used to develop rules for structuring discourse into multi-utterance
segments. Grosz and Hirschberg (1992) use the classification and regression tree system
cart (Brieman et al., 1984) to construct decision trees for classifying aspects of discourse
structure from intonational feature values. Litman and Passonneau (1995) and Passonneau
and Litman (in press) use the system C4.5 to construct decision trees for classifying utterances as discourse segment boundaries, using features relating to prosody, referential noun
phrases, and cue phrases. In addition, C4.5 has been used to develop anaphora resolution
algorithms, by training on corpora tagged with appropriate discourse information (Aone &
Bennett, 1995). Similarly, McCarthy and Lehnert (1995) use C4.5 to learn decision trees
to classify pairs of phrases as coreferent or not. Soderland and Lehnert (1994) use the
machine learning program ID3 (a predecessor of C4.5) to support corpus-driven knowledge
acquisition in information extraction. Machine learning often results in algorithms that
outperform manually derived alternatives (Litman & Passonneau, 1995; Passonneau & Litman, in press; Aone & Bennett, 1995; McCarthy & Lehnert, 1995), although statistical
inference is not always used to evaluate the significance of the performance differences.
Finally, machine learning has also been used with great success in many other areas of
natural language processing. As discussed above, the work of most researchers in discourse
analysis has concentrated on the direct application of existing symbolic learning approaches
(e.g., C4.5), and on the comparison of learning and manual methods. While researchers
in other areas of natural language processing have also addressed these issues, they have
in addition applied a much wider variety of learning approaches, and have been concerned
with the development of learning methods particularly designed for language processing. A
recent survey of learning for natural language (Wermter, Riloff, & Scheler, 1996) illustrates
both the type of learning approaches that have been used and modified (in particular,
symbolic, connectionist, statistical, and hybrid approaches), as well as the scope of the
problems that have proved amenable to the use of learning techniques (e.g., grammatical
inference, syntactic disambiguation, and word sense disambiguation).

7. Conclusion
This paper has demonstrated the utility of machine learning techniques for cue phrase
classification. Machine learning supports the automatic generation of linguistically viable
classification models. When compared to manually derived models already in the literature,
many of the learned models contain new linguistic insights and perform with at least as
high (if not higher) accuracy. In addition, the ability to automatically construct classification models makes it easier to comparatively analyze the utility of alternative feature
representations of the data. Finally, the ease of retraining makes the learning approach
more scalable and extensible than manual methods.
88

fiCue Phrase Classification Using Machine Learning

A first set of experiments were presented that used the machine learning programs

cgrendel (Cohen, 1992, 1993) and C4.5 (Quinlan, 1993) to induce classification models

from the preclassified cue phrases and their features that were used as training data by
Hirschberg and Litman (1993). These results were then evaluated with the same testing data
and methodology used by Hirschberg and Litman (1993). A second group of experiments
used the method of cross-validation to both train and test from the testing data used by
Hirschberg and Litman (1993). A third set of experiments induced classification models
using the new feature token. A fourth set of experiments induced classification models
using the new classification unknown.
The experimental results indicate that several learned classification models (including
extremely simple one feature models) have significantly lower error rates than the models
developed by Hirschberg and Litman (1993). One possible explanation is that the handbuilt classification models were derived using very small training sets; as new data became
available, this data was used for testing but not for updating the original models. In contrast, machine learning in conjunction with cross-validation (Experiment Set 2) supported
the building of classification models using a much larger amount of the data for training.
Even when the learned models were derived using the same small training set (Experiment
Set 1), the results showed that the learning approach helped guard against overfitting on
the training data.
While the prosodic classification model developed by Hirschberg and Litman demonstrated the utility of combining phrasal position with phrasal composition and accent, the
best performing prosodic models of Experiment Sets 1 and 2 demonstrated that phrasal
position was in fact even more useful for predicting cue phrases when used by itself. The
other high performing classification models of Experiment Set 2 also demonstrated the utility of classifying cue phrases based on the prosodic feature length and the textual feature
preceding cue phrase, in combination with other features.
Just as the machine learning approach made it easy to retrain when new training examples became available (Experiment Set 2), machine learning also made it easy to retrain
when new features become available. In particular, when the value of the feature token
was added to all the representations in Experiment Set 2, it was trivial to relearn all of the
models (Experiment Set 3). Allowing the learning programs to treat cue phrases individually further improved the accuracy of the learned classification models, and added to the
body of linguistic knowledge regarding cue phrases. Experiment Set 3 demonstrated that
while not useful by themselves for classifying all cue phrases, the prosodic features based
on phrasal length, phrasal composition, and accent, and textual features based on adjacent
cue phrases, succeeding position, and part-of-speech, were in fact useful when used only in
conjunction with the feature token.
A final advantage of the machine learning approach is that the ease of inducing classification models from many different sets of features supports an exploration of the comparative
utility of different knowledge sources. This is especially useful for understanding the tradeoffs between the accuracy of a model and the set of features that are considered. For
example, it might be worth the effort to code a feature that is not automatically obtainable
or that is expensive to automatically obtain if adding the feature results in a significant
improvement in performance.
89

fiLitman

In sum, the results of this paper suggest that machine learning is a useful tool for
cue phrase classification, when the amount of data precludes effective human analysis,
when the exibility afforded by easy retraining is needed (e.g., due to additional training
examples, new features, new classifications), and/or when an analysis goal is to gain a better
understanding of the different aspects of the data.
Several areas for future work remain. First, there is still room for performance improvement. The error rates of the best performing learned models, even though they outperform
the manually derived models, perform with error rates in the teens. Note that only the
features that were coded or discussed by Hirschberg and Litman (1993) were considered in
this paper. It may be possible to further lower the error rates by considering new types
of prosodic and textual features (e.g., other contextual textual features (Siegel, 1994), or
features that have been proposed in connection with the more general topic of discourse
structure), and/or by using different kinds of learning methods. Second, Experiment Set
4 (and the previous literature) show that as yet, there are no models for predicting when
a cue phrase usage should be classified as unknown, rather than as discourse or sentential.
Again, it may be possible to improve the performance of the existing learned models by
considering new features and/or learning methods, or perhaps performance could be improved by providing more training data. Finally, it is currently an open question whether
the textual models developed here, which were based on transcripts of speech, are applicable
to written texts. Textual models thus need to be developed using written texts as training
data. Machine learning should continue to be a useful tool for helping to address these
issues.

Appendix A. C4.5 Results for Experiment Sets 2 and 3
Tables 13, 14 and 15 present the C4.5 error rates for Experiment Sets 2 and 3. The C4.5
results for Experiment Set 2 are shown in the \Non-Tokenized" columns. A comparison of
Tables 13 and 5 shows that except for A in the larger test set, the C4.5 prosodic error rates
fall within the cgrendel confidence intervals. A similar comparison of Tables 14 and 6
shows that except for O-P in the larger test set, the C4.5 textual error rates fall within the
cgrendel confidence intervals. Finally, a comparison of Tables 15 and 7 shows that the
C4.5 error rate of speech-text falls within the cgrendel confidence interval. The fact that
comparable cgrendel and C4.5 results are generally obtained suggests that the ability to
automate as well as to improve upon manual performance is not due to the specifics of
either learning program.
The C4.5 results for Experiment Set 3 are shown in the \Tokenized" columns of Tables 13, 14 and 15. Comparison with Tables 8, 10 and 12 shows that the error rates of C4.5
and cgrendel are not as similar as in Experiment Set 2. However, the error rates reported
in the tables use the default C4.5 and cgrendel options when running the learning programs. Comparable performance between the two learning programs can in fact generally
be achieved by overriding one of the default C4.5 options. As detailed by Quinlan (1993),
the default C4.5 approach { which creates a separate subtree for each possible feature value
{ might not be appropriate when there are many values for a feature. This situation characterizes the feature token. When the C4.5 default option is changed to allow feature values
to be grouped into one branch of the decision tree, the problematic C4.5 error rates do
90

fiCue Phrase Classification Using Machine Learning

Model
P-L
P-P
I-L
I-P
I-C
A
A*
prosody
hl93features
phrasing
length
position
intonational
intermediate

Classifiable Cue Phrases (N=878)
Non-Tokenized Tokenized (+)
32.5
31.7
16.2
18.4
25.6
26.8
25.9
26.3
36.5
36.6
40.7
40.7
28.3
26.7
16.0
15.2
30.2
29.0
15.9
15.2
24.8
24.4
18.1
18.0
16.8
16.6
21.2
22.3

Classifiable Non-Conjuncts (N=495)
Non-Tokenized
Tokenized (+)
32.2
31.4
18.8
19.0
25.6
25.6
19.4
18.8
35.8
32.8
29.6
29.2
28.8
31.2
19.4
16.0
18.8
18.8
18.0
17.4
26.2
24.2
19.6
17.6
18.8
19.8
21.6
18.4

Table 13: Error rates (%) of the C4.5 prosodic classification models, testing data. (Training
and testing were done from the multiple cue phrase corpus using cross-validation.)
Model
C-P
C-S
O-P
O-P*
O-S
O-S*
POS
text
adjacency
orthography
preceding
succeeding

Classifiable Cue Phrases (N=878)
Non-Tokenized Tokenized (+)
40.7
39.3
40.7
39.9
40.7
35.7
18.4
20.3
35.0
31.6
34.4
32.5
40.7
34.7
19.0
20.6
40.9
39.4
18.9
19.3
18.7
19.3
34.1
32.9

Classifiable Non-Conjuncts (N=495)
Non-Tokenized
Tokenized (+)
39.2
33.6
39.2
39.2
18.6
14.6
17.2
15.0
31.8
31.8
31.0
32.4
41.8
31.8
20.0
15.0
40.6
43.6
17.8
18.0
19.2
16.0
30.0
31.8

Table 14: Error rates (%) of the C4.5 textual classification models, testing data. (Training
and testing were done from the multiple cue phrase corpus using cross-validation.)
indeed improve. For example, the A+ error rate for the classifiable non-conjuncts changes
from 29.2% (Table 13) to 11%, which is within the 12.8%  3.1% cgrendel confidence
interval (Table 8).

Acknowledgements
I would like to thank William Cohen and Jason Catlett for their helpful comments regarding
the use of cgrendel and C4.5, and Sandra Carberry, Rebecca Passonneau, and the three
anonymous JAIR reviewers for their helpful comments on this paper. I would also like to
91

fiLitman

Model
speech-text

Classifiable Cue Phrases (N=878)
Non-Tokenized Tokenized (+)
15.3
13.6

Classifiable Non-Conjuncts (N=495)
Non-Tokenized
Tokenized (+)
16.8
17.6

Table 15: Error rates (%) of the C4.5 prosodic/textual classification model, testing data.
(Training and testing were done from the multiple cue phrase corpus using crossvalidation.)
thank William Cohen, Ido Dagan, Julia Hirschberg, and Eric Siegel for comments on a
preliminary version of this paper (Litman, 1994).

References

Altenberg, B. (1987). Prosodic Patterns in Spoken English: Studies in the Correlation
between Prosody and Grammar for Text-to-Speech Conversion, Vol. 76 of Lund Studies
in English. Lund University Press, Lund.
Aone, C., & Bennett, S. W. (1995). Evaluating automated and manual acquisition of
anaphora resolution strategies. In Proceedings of the Thirty-Third Annual Meeting of
the Association for Computational Linguistics (ACL).
Brieman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification and Regression
Trees. Monterey, CA: Wadsworth and Brooks.
Church, K. W. (1988). A stochastic parts program and noun phrase parser for unrestricted
text. In Proceedings of the Second Conference on Applied Natural Language Processing.
Cohen, R. (1984). A computational theory of the function of clue words in argument understanding. In Proceedings of the Tenth International Conference on Computational
Linguistics (COLING).
Cohen, W. W. (1992). Compiling knowledge into an explicit bias. In Proceedings of the
Ninth International Conference on Machine Learning.
Cohen, W. W. (1993). Ecient pruning methods for separate-and-conquer rule learning
systems. In Proceedings of the Thirteenth International Joint Conference on Artificial
Intelligence (IJCAI).
Freedman, D., Pisani, R., & Purves, R. (1978). Statistics. W. W. Norton and Company.
Grosz, B., & Hirschberg, J. (1992). Some intonational characteristics of discourse structure. In Proceedings of the International Conference on Spoken Language Processing
(ICSLP).
Grosz, B. J., & Sidner, C. L. (1986). Attention, intentions, and the structure of discourse.
Computational Linguistics, 12 (3), 175{204.
92

fiCue Phrase Classification Using Machine Learning

Halliday, M. A. K., & Hassan, R. (1976). Cohesion in English. Longman.
Hearst, M. A. (1994). Multi-paragraph segmentation of expository text. In Proceedings of
the Thirty-Second Annual Meeting of the Association for Computational Linguistics
(ACL).
Hindle, D. M. (1989). Acquiring disambiguation rules from text. In Proceedings of the
Twenty-Seventh Annual Meeting of the Association for Computational Linguistics
(ACL).
Hirschberg, J. (1990). Accent and discourse context: Assigning pitch accent in synthetic
speech. In Proceedings of the Eighth National Conference on Artificial Intelligence
(AAAI).
Hirschberg, J., & Litman, D. (1987). Now let's talk about \now": Identifying cue phrases
intonationally. In Proceedings of the Twenty-Fifth Annual Meeting of the Association
for Computational Linguistics (ACL).
Hirschberg, J., & Litman, D. (1993). Empirical studies on the disambiguation of cue phrases.
Computational Linguistics, 19 (3), 501{530.
Holte, R. C. (1993). Very simple classification rules perform well on most commonly used
datasets. Machine Learning, 11 (1), 63{90.
Litman, D., & Hirschberg, J. (1990). Disambiguating cue phrases in text and speech. In
Proceedings of the Thirteenth International Conference on Computational Linguistics
(COLING).
Litman, D. J. (1994). Classifying cue phrases in text and speech using machine learning.
In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI).
Litman, D. J., & Allen, J. F. (1987). A plan recognition model for subdialogues in conversation. Cognitive Science, 11, 163{200.
Litman, D. J., & Passonneau, R. J. (1995). Combining multiple knowledge sources for
discourse segmentation. In Proceedings of the Thirty-Third Annual Meeting of the
Association for Computational Linguistics (ACL).
McCarthy, J. F., & Lehnert, W. G. (1995). Using decision trees for coreference resolution. In
Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence
(IJCAI).
Moser, M., & Moore, J. D. (1995). Investigating cue selection and placement in tutorial
discourse. In Proceedings of the Thirty-Third Annual Meeting of the Association for
Computational Linguistics (ACL).
Ostendorf, M., & Ross, K. (in press). A multi-level model for recognition of intonation labels.
In Y. Sagisaka, N. C., & Higuchi, N. (Eds.), Computing Prosody. Springer-Verlag.
Passonneau, R. J., & Litman, D. J. (in press). Discourse segmentation by human and
automated means. Computational Linguistics, 23.
93

fiLitman

Pierrehumbert, J. B. (1980). The Phonology and Phonetics of English Intonation. Ph.D.
thesis, Massachusetts Institute of Technology. Distributed by the Indiana University
Linguistics Club.
Pitrelli, J., Beckman, M., & Hirschberg, J. (1994). Evaluation of prosodic transcription
labeling reliability in the ToBI framework. In Proceedings of the International Conference on Spoken Language Processing (ICSLP).
Quinlan, J. R. (1993). C4.5 : Programs for Machine Learning. San Mateo, CA: Morgan
Kaufmann.
Reichman, R. (1985). Getting Computers to Talk Like You and Me: Discourse Context,
Focus, and Semantics. Cambridge, MA: MIT Press.
Siegel, E. V. (1994). Competitively evolving decision trees against fixed training cases
for natural language processing. In K. E. Kinnear, J. (Ed.), Advances in Genetic
Programming. Cambridge, MA: MIT Press.
Siegel, E. V., & McKeown, K. R. (1994). Emergent linguistic rules from the automatic
grouping of training examples: Disambiguating clue words with decision trees. In
Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI).
Soderland, S., & Lehnert, W. (1994). Corpus-driven knowledge acquisition for discourse
analysis. In Proceedings of the Twelfth National Conference on Artificial Intelligence
(AAAI).
Stieman, L. J. (1995). A discourse analysis approach to structured speech. In Working
Notes of AAAI Spring Symposium Series: Empirical Methods in Discourse Interpretation and Generation.
Weiss, S. M., & Kulikowski, C. (1991). Computer Systems That Learn: Classification
and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert
Systems. San Mateo, CA: Morgan Kaufmann.
Wermter, S., Riloff, E., & Scheler, G. (1996). Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing. Berlin, Germany: SpringerVerlag.
Wightman, C. W., & Ostendorf, M. (1994). Automatic labeling of prosodic patterns. IEEE
Transactions on Speech and Audio Processing, 2 (4), 469{481.
Zuckerman, I., & Pearl, J. (1986). Comprehension-driven generation of meta-technical
utterances in math tutoring. In Proceedings of the Fifth National Conference on
Artificial Intelligence (AAAI).

94

fiJournal of Artificial Intelligence Research 5 (1996) 163{238

Submitted 5/94; published 10/96

Mechanisms for Automated Negotiation
in State Oriented Domains
Gilad Zlotkin

giladz@agentsoft.com

AgentSoft Ltd.
P.O. Box 53047
Jerusalem, Israel

Jeffrey S. Rosenschein

jeff@cs.huji.ac.il

Institute of Computer Science
Hebrew University
Givat Ram, Jerusalem, Israel

Abstract

This paper lays part of the groundwork for a domain theory of negotiation, that is,
a way of classifying interactions so that it is clear, given a domain, which negotiation
mechanisms and strategies are appropriate. We define State Oriented Domains, a general
category of interaction. Necessary and sucient conditions for cooperation are outlined.
We use the notion of worth in an altered definition of utility, thus enabling agreements in a
wider class of joint-goal reachable situations. An approach is offered for conict resolution,
and it is shown that even in a conict situation, partial cooperative steps can be taken by
interacting agents (that is, agents in fundamental conict might still agree to cooperate up
to a certain point).
A Unified Negotiation Protocol (UNP) is developed that can be used in all types of
encounters. It is shown that in certain borderline cooperative situations, a partial cooperative agreement (i.e., one that does not achieve all agents' goals) might be preferred by all
agents, even though there exists a rational agreement that would achieve all their goals.
Finally, we analyze cases where agents have incomplete information on the goals and
worth of other agents. First we consider the case where agents' goals are private information, and we analyze what goal declaration strategies the agents might adopt to increase
their utility. Then, we consider the situation where the agents' goals (and therefore standalone costs) are common knowledge, but the worth they attach to their goals is private
information. We introduce two mechanisms, one \strict," the other \tolerant," and analyze their affects on the stability and eciency of negotiation outcomes.

1. Introduction
Negotiation has been a major research topic in the distributed artificial intelligence (DAI)
community (Smith, 1978; Malone, Fikes, Grant, & Howard, 1988; Kuwabara & Lesser,
1989; Conry, Meyer, & Lesser, 1988; Kreifelts & von Martial, 1991). The term negotiation,
however, has been used in a variety of different ways. To some researchers, negotiation
serves as an important mechanism for assigning tasks to agents, for resource allocation, and
for deciding which problem-solving tasks to undertake. In these systems, there is generally
some notion of global utility that the system is trying to maximize.
c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiZlotkin & Rosenschein

Other researchers have focused on negotiation that might take place among agents that
serve the interests of truly distinct parties (Rosenschein & Genesereth, 1985; Sycara, 1988;
Kraus & Wilkenfeld, 1990; Zlotkin & Rosenschein, 1989). The agents are autonomous in
the sense that they have their own utility functions, and no global notion of utility (not
even an implicit one) plays a role in their design. Negotiation can be used to share the work
associated with carrying out a joint plan, or to resolve outright conict arising from limited
resources.
Despite the varied use of terminology, it is clear to the DAI community as a whole
that the operation of interacting agents would be enhanced if they were able to exchange
information to reach mutually beneficial agreements.
The work described in this paper follows the general direction of previous research
by the authors (Rosenschein & Genesereth, 1985; Zlotkin & Rosenschein, 1989) in treating
negotiation in the spirit of game theory. The focus of this research is to analyze the existence
and properties of certain kinds of deals and protocols among agents. We are not here
examining the computational issues that arise in discovering such deals, though the design
of ecient, possibly domain-specific, algorithms will constitute an important future phase
of this research. Initial work in building a domain theory of negotiation was previously
undertaken (Zlotkin & Rosenschein, 1993a), and is expanded and generalized in the current
paper. This analysis serves as a critical step in applying the theory of negotiation to realworld applications.

1.1 Applying Game Theory Tools to Protocol Design for Automated Agents

Our ongoing research has been motivated by one, focused premise: the problem of how to
get computers to interact effectively in heterogeneous systems can be tackled through the
use of game theory tools.
Our concern is with computer systems made up of machines that have been programmed
by different entities to pursue differing goals. One approach for achieving coordination under
these circumstances is to establish mutually accepted protocols for the machines to use in
coming to agreements.
The perspective of our research is that one can use game theory tools to design and
evaluate these high-level protocols. We do not intend, with this paper, to make contributions
to game theory itself. We are not defining new notions of equilibria, nor are we providing
new mathematical tools to be used in general game theory. What we are doing is taking
the game theory approach, and some of its tools, to solve specific problems of high-level
protocol design.
While game theory makes contributions to the understanding of many different fields,
there is a particularly serendipitous match between game theory and heterogeneous computer systems. Computers, being pre-programmed in their behavior, make concrete the
notion of \strategy" that plays such a central role in game theory|the idea that a player
adopts rules of behavior before starting to play a given game, and that these rules entirely
control his responses during the game. This idealized player is an imperfect model of human
behavior, but one that is quite appropriate for computers.
While we are not the first to apply game theoretic ideas to computer science, we are
using the tools in a new way. While others have used game theory to answer the question,
164

fiMechanisms for Automated Negotiation

\How should one program a computer to act in a given specific interaction?" we are addressing the question of how to design the rules of interaction themselves for automated
agents. The approach taken in this paper is, therefore, strongly based on previous work
in game theory, primarily on what is known as \Nash's Bargaining Problem" (Nash, 1950;
Luce & Raiffa, 1957) or \Nash's Model of Bargaining" (Roth, 1979), \mechanism design"
or \implementation theory" (Binmore, 1992; Fudenberg & Tirole, 1992), and \correlated
equilibrium theory" (Aumann, 1974, 1987; Myerson, 1991; Forges, 1993). A short overview
of game theory results that are used or referred to in this paper can be found in Section 9.1.

1.2 Overview of the Paper
In previous work, we began laying the groundwork for a domain theory of negotiation, that
is, a way of classifying interactions so that it is clear, given a domain, which negotiation
mechanisms and strategies are appropriate. Previously, we considered Task Oriented Domains (Zlotkin & Rosenschein, 1989, 1993a), a restricted category of interactions. In this
paper, we define State Oriented Domains, a more general category of interaction.
In Section 4.4 we examine scenarios where interacting agents in State Oriented Domains
can find themselves in cooperative, compromise, and conict encounters. In conict situations, the agents' goals cannot be simultaneously achieved. A joint-goal reachable situation
(i.e., where agents' goals can be simultaneously achieved) can be cooperative or compromise, depending on the cost of reaching a state that satisfies all agents compared to the
cost of each agent (alone) achieving his stand-alone goal.
In Section 4.1, necessary and sucient conditions for cooperation are outlined. Cooperative situations lend themselves to mixed-joint-plan-based negotiation mechanisms. However,
compromise situations require special treatment. We propose using the notion of worth in
an altered definition of utility, thereby enabling agreements in a wider class of joint-goal
reachable situations. An approach is offered for conict resolution, and it is shown that
even in a conict situation, partial cooperative steps can be taken by interacting agents
(that is, agents in fundamental conict might still agree to cooperate up to a certain point).
A Unified Negotiation Protocol (UNP) is developed in Section 5.4 that can be used in all
types of encounters. It is shown that in certain borderline cooperative situations, a partial
cooperative agreement (i.e., one that does not achieve all agents' goals) might be preferred
by all agents, even though there exists a rational agreement that would achieve all their
goals.
The UNP is further enhanced in Section 6 to deal with the case where agents have
assigned unlimited worth to their goals and this fact is common knowledge. Our solution
depends on the concept of \cleaning up after yourself," or tidiness, as a new method of
evaluating agent utility. We show that two tidy agents are able to reach agreements in all
joint-goal reachable situations in State Oriented Domains.
In Section 7 we analyze cases where agents have incomplete information about the goals
and worth of other agents. First, we consider the case where agents' goals are private
information, and we consider what goal declaration strategies the agents might adopt to
increase their utility.
We then consider, in Section 8, the situation where the agents' goals (and therefore
stand-alone costs) are common knowledge, but the worth they attach to their goals is
165

fiZlotkin & Rosenschein

private information. There are many situations where an agent's goals might be known,
but his worth is private. For example, two cars approaching an intersection may know
each other's goals (because of the lanes in which they are located). The worth that each
associates with passing through the intersection to his target lane, however, is private. Goal
recognition techniques are suitable for discovering the other agent's intentions; his worth,
however, is harder to discern from short-term external evidence.
The agents declare, in a ,1-phase, their worths, which are then used as a baseline to
the utility calculation (and thus affect the negotiation outcome). We are concerned with
analyzing what worth declaration strategies the agents might adopt to increase their utility.
We introduce two mechanisms, one \strict," the other \tolerant," and analyze their affects
on the stability and eciency of negotiation outcomes. The strict mechanism turns out to
be more stable, while the tolerant mechanism is more ecient.

2. Negotiation in State Oriented Domains
How can machines decide how to share resources, or which machine will give way while
the other proceeds? Negotiation and compromise are necessary, but how do we build our
machines to do these things? How can the designers of these separate machines decide on
techniques for agreement that enable mutually beneficial behavior? What techniques are
appropriate? Can we make definite statements about the techniques' properties?
The way we address these questions is to synthesize ideas from artificial intelligence with
the tools of game theory. Assuming that automated agents, built by separate, self-interested
designers, will interact, we are interested in designing protocols for specific domains that
will get those agents to interact in useful ways.
The word \protocol" means different things to different people. When we use the word
protocol, we mean the rules by which agents will come to agreements. It specifies the kinds
of deals they can make, as well as the sequence of offers and counter-offers that are allowed.
Protocols are intimately connected with domains, by which we mean the environment in
which our agents operate. Automated agents who control telecommunications networks are
operating in a different domain (in a formal sense) than robots moving boxes. Much of our
research is focused on the relationship between different kinds of domains, and the protocols
that are suitable for each.
Given a protocol, we need to consider what agent strategy is appropriate. A strategy
is the way an agent behaves in an interaction. The protocol specifies the rules of the
interaction, but the exact deals that an agent proposes are a result of the strategy that his
designer has put into him. As an analogy, a protocol is like the rules governing movement
of pieces in the game of chess. A strategy is the way in which a chess player decides on his
next move.

2.1 Attributes of Standards
What are the attributes that might interest protocol designers? The set of attributes, and
their relative importance, will ultimately affect their choice of interaction rules.
We have considered several attributes that might be important to system designers.
166

fiMechanisms for Automated Negotiation

1.

2.

3.

4.
5.

Eciency: The agents should not squander resources when they come to an agree-

ment; there should not be wasted utility when an agreement is reached. For example,
it makes sense for the agreements to satisfy the requirement of Pareto Optimality
(no agent could derive more from a different agreement, without some other agent
deriving less from that alternate agreement). Another consideration might be Global
Optimality, which is achieved when the sum of the agents' benefits are maximized.
Global Optimality implies Pareto Optimality, but not vice versa. Since we are speaking about self-motivated agents (who care about their own utilities, not the sum of
system-wide utilities|no agent in general would be willing to accept lower utility just
to increase the system's sum), Pareto Optimality plays a primary role in our eciency
evaluation. Among Pareto Optimal solutions, however, we might also consider as a
secondary criterion those solutions that increase the sum of system-wide utilities.
Stability: No agent should have an incentive to deviate from agreed-upon strategies.
The strategy that agents adopt can be proposed as part of the interaction environment
design. Once these strategies have been proposed, however, we do not want individual
designers (e.g., companies) to have an incentive to go back and build their agents with
different, manipulative, strategies.
Simplicity: It will be desirable for the overall interaction environment to make low
computational demands on the agents, and to require little communication overhead.
This is related both to eciency and to stability: if the interaction mechanism is
simple, it increases eciency of the system, with fewer resources used up in carrying
out the negotiation itself. Similarly, with stable mechanisms, few resources need to be
spent on outguessing your opponent, or trying to discover his optimal choices. The
optimal behavior has been publicly revealed, and there is nothing better to do than
just carry it out.
Distribution: Preferably, the interaction rules will not require a central decision
maker, for all the obvious reasons. We do not want our distributed system to have a
performance bottleneck, nor collapse due to the single failure of a special node.
Symmetry: We may not want agents to play different roles in the interaction scenario. This simplifies the overall mechanism, and removes the question of which agent
will play which role when an interaction gets under way.

These attributes need not be universally accepted. In fact, there will sometimes be tradeoffs between one attribute and another (for example, eciency and stability are sometimes
in conict with one another; see Section 8). But our protocols are designed, for specific
classes of domains, so that they satisfy some or all of these attributes. Ultimately, these are
the kinds of criteria that rate the acceptability of one interaction mechanism over another.
As one example, the attribute of stability assumes particular importance when we consider open systems, where new agents are constantly entering and leaving the community of
interacting machines. Here, we might want to maintain stability in the face of new agents
who bring with them new goals and potentially new strategies as well. If the mechanism is
\self-perpetuating," in that it is not only to the benefit of society as a whole to follow the
rules, but also to the benefit of each individual member, then the social behavior remains
167

fiZlotkin & Rosenschein

stable even when the society's members change dynamically. When the interaction rules
create an environment in which a particular strategy is optimal, beneficial social behavior
is resistant to outside invasion.

2.2 Side Effects in Encounters
Various kinds of encounters among agents, in various types of domains, are possible. In previous work (Zlotkin & Rosenschein, 1989, 1993a, 1994, 1996b) we examined Task Oriented
Domains (TODs), which encompass only certain kinds of encounters among agents. State
Oriented Domains (SODs) describe a larger class of scenarios for multiagent encounters
than do TODs. In fact, as we will see below, the set of Task Oriented Domains is actually a
proper subset of State Oriented Domains. Most classical domains in Artificial Intelligence
have been instances of State Oriented Domains.
The main attribute of general SODs is that agents' actions can have side effects. In Task
Oriented Domains, no side effects exist and in general all common resources are unrestricted.
Thus, when an agent achieves his own set of tasks in a TOD it has no positive nor negative
effects on the other agent whatsoever. It does not hinder the other agent from achieving his
goal, and it never satisfies the other agent's goals \by accident." To enable another agent to
carry out your task, such as for example in the Postmen Domain (Zlotkin & Rosenschein,
1989), it is necessary explicitly to declare the existence of the letter, and hand it over, so
that it will be delivered. The absence of side effects rules out some positive and all negative
interactions among agent goals. The only positive interactions that remain are those that
are explicitly coordinated by the agents.
In general State Oriented Domains, where side effects exist, agents can unintentionally
achieve one another's goals, and thus benefit from one another's actions. The ip side of
side effects, however, is that negative interactions between goals can also exist. Thus, an
SOD is a domain that is (unlike TODs) not necessarily cooperative, because of those action
side effects. In SODs, agents have to deal with goal conict and interference, as well as the
possibility of unintended cooperation.1
For example, consider the Blocks World situation in Figure 1. The simplest plan to
achieve On(White; Gray) has the side effect of achieving Clear(Black).

Figure 1: Side Effects in State Oriented Domains
1. For interesting discussions of the issue of conict and its role in human encounters, see (Schelling, 1963,
1984).

168

fiMechanisms for Automated Negotiation

2.3 Domain Definition
Consider a group of agents who co-exist in some environment. Each agent has a goal that it
is interested in achieving. What does it mean to achieve a goal? In State Oriented Domains,
it is the classic AI notion of goal achievement: it means to carry out a sequence of actions
(a plan) that results in the transformation of the environment to a state where the goal is
satisfied.
Imagine, for example, a person who is interested in getting to work. His goal is to be
at work; in the current state, he is not at work. His plan will be the sequence of actions
that get him to work (driving his car, or calling a taxi, or walking, or riding a bicycle,. . . ).
The final, or goal, state, may differ depending on which plan was executed (e.g., where his
car is, where his bicycle is). All the states in which he is at work, however, satisfy his goal.
Let's assume that the optimal plan (from the time point of view) involves driving the car
to work.
The specification of the goal states may be implicit. The fact that needs to be true
(the goal) may be given. Any situation in which that fact is true, i.e., the goal is satisfied,
is acceptable. In a State Oriented Domain, any goal is described by the set of states that
satisfy it.
Now imagine that this person's wife is interested in being at her own place of work.
There are states that will satisfy both the husband's and wife's goals, and plans that will
achieve such a state (e.g., one of them takes the car, while the other calls a taxi). However,
there are certain plans that are suitable for either spouse in isolation, but which cannot
coexist. For example, the husband taking the car is a perfectly good plan (and optimal) if
he were alone in the world. Similarly, his wife's taking the car is a good plan (and optimal)
if she were alone. Together, another plan may be suitable (husband drives wife to her
work, continues on with car to his work). In this case, extra work was required from the
husband's point of view, because the wife is present in his world; there is a certain burden
to the coordination.
In the example above, the agents carry out a sequence of activities, suitably synchronized, to reach the goal state satisfying both. The husband and wife enter the car, after
which the husband drives to a particular location, the wife exits, and so on. In any environment, there are primitive operations that each agent alone can do. When these operations
are combined into a coherent sequence of actions specifying what both agents are to do (and
the order in which they are to be done), we say that the agents are executing a joint plan. A
joint plan in general transforms the world from some initial state to a goal state satisfying
both agents (when possible). The plan above transforms the world from the initial state
where both husband and wife are at home to the goal state (satisfying both agents) where
the wife is at work, and the car and husband are at his place of work. This is the final state,
one of many goal states.
In Task Oriented Domains the cost of the coordinated plan need never be worse than
the stand-alone plan|at the very worst, each agent just achieves his own set of tasks. In
our husband/wife sharing one car example, however, the coordinated plan may be worse
for one or both agents than their stand-alone plans. This is an example of one attribute
of State Oriented Domains, namely negative interactions, or what are sometimes called
169

fiZlotkin & Rosenschein

\deleted-condition interactions" (Gupta & Nau, 1992). This is because taking the car has
the side effect of depriving the other agent of the car.
Imagine a new situation, that arises during the weekend. The husband is interested
in doing carpentry in the garage (currently occupied by the car). The wife is interested in
taking the car to the baseball game. By themselves, each agent has an optimal plan to reach
a goal state (e.g., the husband moves the car out of the garage, parks it outside, does his
carpentry). However, when his wife takes the car to the game, executing her stand-alone
optimal plan, the husband benefits from the side effect of the car being moved, namely,
the garage is emptied. This is an example of another typical attribute of State Oriented
Domains|accidental achievement of goals, or \enabling-condition interactions" (Gupta &
Nau, 1992) or \favor relations" (von Martial, 1990) among goals.
When agents carry out a joint plan, each one plays some \role." Our theory assumes
that there is some way of assessing the cost of each role. This measure of cost is essential
to how an agent evaluates a given joint plan. Among all joint plans that achieve his goal,
he will prefer those in which his role has lower cost.
We express the intuitive ideas above in the precise definition below.

Definition 1 A State Oriented Domain (SOD) is a tuple < S ; A; J ; c > where:
1. S is the set of all possible world states;

2. A = fA1; A2; : : :A g is an ordered list of agents;
n

3. J is the set of all possible joint (i.e., n-agent) plans. A joint plan J 2 J moves the
world from one state in S to another. The actions taken by agent k are called k's role
in J , and will be written as J . We can also write J as (J1; J2; : : :; J );
k

n

4. c is a function c: J ! (IR+ ) : For each joint plan J in J , c(J ) is a vector of n positive
real numbers, the cost of each agent's role in the joint plan. c(J ) is the i-th element
of the cost vector, i.e., it is the cost of the i-th role in J . If an agent plays no role in
J , his cost is 0.
n

i

Our use of the term joint plan differs from other uses in the AI literature (Levesque &
Cohen, 1990; Cohen & Levesque, 1991). There, the term joint plan implies a joint goal,
and mutual commitment by the agents to full implementation of the plan (e.g., if one agent
dropped out suddenly, the other would still continue). In our use of the term, the agents
are only committed to their own goal and their part of the combined plan. Each may do
its part of the plan for different reasons, because each has a different goal to achieve. Were
one agent to drop out, the other agent may or may not continue, depending on whether it
suited his own goal.
The details of the description of the joint plans in J are not critical to our overall theory.
The minimal requirement is that it must be possible to evaluate the cost of the joint plan
for each agent (i.e., the cost of his role). In many domains, a joint plan will be a sequence of
actions for each agent with an associated schedule (partial order) constraining the actions'
parallel execution.
Note also that our cost function above relates only to the joint plan itself and not,
for example, to the initial state of the world. In fact, the cost function could be altered to
170

fiMechanisms for Automated Negotiation

include other parameters (like the initial state of the world), without affecting our discussion
below. Our model is not sensitive to the details of the cost function definition, other than the
requirement that the cost of a role be the same for all agents. This is called the symmetric
abilities assumption (see below, Section 2.4).

Definition 2 An encounter within an SOD < S ; A; J ; c > is a tuple < s; (G1; G2; : : :; G ) >

such that s 2 S is the initial state of the world, and for all k 2 f1 : : :ng; G is the set of all
acceptable final world states from S for agent A . G will also be called A 's goal.
n

k

k

k

k

An agent's goal is a fixed, pre-determined, set of states. An agent will, at the conclusion
of the joint plan, either achieve his goal or not achieve his goal. Goals cannot be partially
achieved. Domains in which goals can be partially achieved are called Worth Oriented
Domains (WODs) and are discussed in detail elsewhere (Zlotkin & Rosenschein, 1991c,
1996a).
One thing that we are specifically ruling out in SODs is one agent having a goal that
makes reference to another agent's (as yet) unknown goal. For example, a specification
such as \Agent 1's goal is to make sure that Agent 2's goal will not be achieved, whatever
the latter's goal is" cannot constitute part of the description of an encounter in a State
Oriented Domain, because it cannot be described as a static set of goal states. However,
this meta-goal might exist within an agent, and give rise to a well-defined set of states in a
specific encounter (e.g., given G2, G1 is its complement). Similarly, one agent might have
as its goal that another agent have a specific goal G2|the first agent wants the world to
be in a state where the other agent has the specific goal G2.
We will only consider sets of goal states that can be specified in a finite way, either
because the set itself is finite, or the infinite set can be specified by a closed formula in
first-order logic (i.e., no free variables; all states that satisfy the formula, and only those
states, are in the goal set). As an example, an agent might have the goal that \There exists
a block x such that block B is on x."
We will also consider further restrictions on the kind of goals agents may have. For
example, below we will consider domains in which agents' goals are restricted to sets of
grounded predicates (i.e., no variables) rather than to any closed formula.
2.3.1 Reachability

It may be the case that there exist goal states that satisfy both agents' goals, but that
there are constraints as to the reachability of those states. For example, it may be the case
that a state satisfying each goal can be reached by an agent alone, but a state satisfying
the combined goal cannot be reached by any agent alone. More generally, reaching any
state might require n agents working together, and be unreachable if fewer than n agents
are involved (we will call n the \parallelism factor" of the goal). When the goal in the
intersection cannot be reached by any number of agents working in parallel, we will say the
parallelism factor is infinite. The parallelism factor is a particularly appropriate concept
when there are multiagent actions that are possible or required in the domain (e.g., carrying
a heavy table).
171

fiZlotkin & Rosenschein

2.4 Assumptions

Throughout this paper, we will be making a number of simplifying assumptions that enable
us to lay out the foundation for our theory of mechanism design for automated agents.
Here, we present those assumptions.
1.
2.

3.
4.
5.

6.

Expected Utility Maximizer: Designers will design their agents to maximize expected utility. For example, we assume that a designer will build his agent to prefer
a 51% chance of getting $100, rather than a sure $50.
Isolated Negotiation: An agent cannot commit himself as part of the current negotiation to some behavior in a future negotiation, nor can he expect that his current
behavior will in any way affect a future negotiation. Similarly, an agent cannot expect
others to behave in a particular way based on their previous interaction history, nor
to act differently with him because of his own past behavior. Each negotiation stands
alone.
Interagent Comparison of Utility: The designers have a means of transforming
the utilities held by different agents into common utility units.
Symmetric Abilities: All agents are able to perform the same set of operations in
the world, and the cost of each operation is independent of the agent carrying it out.
Binding Commitments: Designers will design their agents to keep explicit public
commitments. We assume nothing about the relationship between private preferences
and public behavior, only that public commitment be followed by public performance
of the commitment. This can be monitored, and if necessary, enforced.
No Explicit Utility Transfer: Although agents can compare their respective utilities, they have no way of explicitly transferring utility units from one to the other.
There is, for example, no \money" that can be used to compensate one agent for
a disadvantageous agreement. Utility transfer does occur, however, implicitly. This
implicit transfer of utility forms the basis for agreement among agents.

3. Examples of State Oriented Domains

In this section, we present several examples of State Oriented Domains. These specific
examples illustrate some of the nuances of describing this class of domains.

3.1 The Blocks Domain

In the Blocks Domain, there is a table of unlimited size, and a set of blocks. A block can
be on the table or on some other block, and there is no limit to the height of a stack of
blocks. One state in this domain can be seen in Figure 2.
World States and Goals: The basic predicates that make up world states and goals are:

 On(x; y): such that x and y are blocks; its meaning is that block x is (directly)
on block y .

172

fiMechanisms for Automated Negotiation

Figure 2: A State in the Blocks Domain

1

2

3

Figure 3: A State in the Slotted Blocks Domain

 On(x; Table): such that x is a block; its meaning is that block x is (directly) on

the table.
 Clear(x): such that x is a block; its meaning is that there is no block on x, i.e.,
Clear(x)  :9y On(y; x).
As this is an SOD, goals are sets of world states. These world states can be expressed
as a first order closed formula over the above predicates. Sample goals are:

 :Clear(R) | Block R is not clear.
 9xOn(R; x) | Block R is not on the table.
 8xOn(x; Table) | All blocks are on the table (and therefore, implicitly, all blocks
are also Clear).

Atomic Operation: There is one operation in this world: Move(x; y ). This operation moves
a clear block x onto the top of another clear block y .
Cost: Each move operation has a cost of 2.

3.2 The Slotted Blocks Domain

The domain here is the same as the Blocks Domain above. However, on the table there are
only a bounded number of slots into which blocks can be placed. One state in this domain
can be seen in Figure 3.
World States and Goals: The basic predicates that make up world states and goals are:

 On(x; y): such that x and y are blocks; its meaning is that block x is (directly)
on block y .

173

fiZlotkin & Rosenschein

 At(x; n): such that x is a block and n is a slot name; its meaning is that block

x is (directly) on the table at slot n.
 Clear(x): such that x is a block; its meaning is that there is no block which is
on x, i.e., Clear(x)  :9y On(y; x).
Atomic Operations: There are two operations in the Slotted Blocks Domain:

 PickUp(i) | Pick up the top block in slot i (can be executed whenever slot i is

not empty);
 PutDown(i) | Put down the block that is currently being held into slot i. An
agent can hold no more than one block at a time.
Cost: Each operation has a cost of one.

This Slotted Blocks Domain is different from the Blocks Domain above in two ways:
1. The table of unlimited size is replaced by a bounded table with distinguishable locations that we call \slots."
2. The atomic \Move" operation is broken into two sub-operations PickUp and PutDown.
This allows more cooperation among the agents. For example, if we want to swap the
blocks in slot 1 in Figure 3 it would take one or more agents minimally a total of
4 Move operations, i.e., each block (Black and White) is touched twice. However, if
we allow the agents to use the PickUp and PutDown operations two agents can do
the swap with two PickUp and two PutDown operations (which is equivalent to two
move operations), i.e., each block is touched only once. The finer granularity of the
operations allows more exibility in scheduling within the joint plan.

3.3 The Delivery Domain with Bounded Storage Space

In this Delivery Domain, there is a weighted graph G = G(V; E ). Each v 2 V represents a
warehouse, and each e 2 E represents a road. The weight function w: E ! IR+ is the length
of any given road. For each edge e 2 E , w(e) is the length of e or the \cost" of e. Each
agent has to deliver containers from one warehouse to another. To do the deliveries, agents
can rent trucks, an unlimited supply of which are available for rental at every node. A
truck can carry up to 5 containers. Each warehouse also has a limited capacity for holding
containers.
Atomic Operations: The operations in this domain are:

 Load(c; t) | loads a container c onto a truck t. The preconditions are:

{ Container c and truck t are at the same warehouse h;
{ Truck t has less than 5 containers on board, where 5 is the capacity limit of

each truck.
The results of the operation are:
{ Warehouse h has one container less;
174

fiMechanisms for Automated Negotiation

{ Truck t has one container more.

A Load operation costs 1.
 Unload(c; t) | unloads a container c from a truck t. The preconditions are:
{ Container c is on truck t;
{ Truck t is at some warehouse h;
{ Warehouse h is not full.
The results of the operation are:
{ Warehouse h has one container more;
{ Truck t has one container less.
The Unload operation costs 1.
 Drive(t; h) | Truck t drives to warehouse h. There are no preconditions on this
operation. The result is that truck t is at warehouse h. The cost of this operation
is equal to the distance (i.e., the minimal weighted path) between the current
position of truck t and warehouse h.
World States and Goals: The full description of a world state includes the location of each
container (either in some warehouse or on some truck) and the location of each truck
(either in some warehouse or on some road). However, we will restrict goals so that
they can only specify which containers need to be at which warehouses.

3.4 The Restricted Usage Shared Resource Domain

In this domain, there is a set of agents that are able to use a shared resource (such as a
communication line, a shared memory device, a road, a bridge. . . ). There is a restriction
that no more than m  1 agents can use the resource at the same time (m denotes the
maximal capacity of the resource).
Atomic Operations: The atomic operations in the Shared Resource Domain are:

 Use | an agent is using the shared resource for one time unit. The Use operation

costs 0.
 Wait | an agent is waiting to use the shared resource for one time unit. The
operation costs 1, i.e., waiting for one time unit to access the shared resource
costs 1.
 NOP | an agent does not need the resource and therefore neither uses it nor
waits for it. This operation costs 0.
The objective here is to find a schedule such that at any time unit no more than m
agents are performing the Use operation.
World States and Goals: A world state describes the current activity of the agents and their
accumulated resource usage since time 0 (i.e., not their accumulated cost). The goal
of an agent is to be in a state where it has accumulated a target number of time units
using the resource, and is currently doing the NOP operation. Formally, a state is an
175

fiZlotkin & Rosenschein

T
i
m
e

A1

Joint Plan
A2

A3

A1

0 Use Use Wait
1 Use Use Wait
2 NOP Use Use
3 NOP NOP Use
4 NOP NOP NOP

World States

(Use,0)
(Use,1)
(NOP,2)
(NOP,2)
(NOP,2)

A2

(Use,0)
(Use,1)
(Use,2)
(NOP,3)
(NOP,3)

A3

(Wait,0)
(Wait,0)
(Use,0)
(Use,1)
(NOP,2)

Figure 4: Joint Plan and States in the Restricted Usage Shared Resource Domain

n-element vector, one element for each agent, where each element is a pair consisting

of the agent's current operation and his accumulated number of time units using the
resource (i.e., the set of all states is (fWait,Use,NOPg  IN) ).
Assume, as an example, that there are three agents, and one resource that has a maximal
capacity of two. Agents 1 and 3 need two units of the resource, while agent 2 needs three
units of the resource. A joint plan can be seen at the left side of Figure 4, described by a
matrix. For each time t and agent A , the entry in column i and row t is agent A 's action
at time t. The resulting world state after each time unit of the joint plan can be seen at
the right side of Figure 4. The final state satisfies all agents' goals.
n

i

i

4. Deals, Utility, and Negotiation Mechanisms

Now that we have defined the characteristics of a State Oriented Domain, and looked at a
few simple examples, we turn our attention to how agents in an SOD can reach agreement
on a joint plan that brings them to some agreed-upon final state. Hopefully, this final state
will satisfy both agents' goals. However, this isn't always possible. There are three such
cases:
1. It might be the case that there doesn't exist a state that satisfies both agents' goals
(i.e., the goals contradict one another);
2. It might be the case that there exists a state that satisfies them both, but it cannot
be reached with the primitive operations in the domain (see Section 2.3.1 above);
3. It might be the case that there exists a reachable state that satisfies them both, but
which is so expensive to get to that the agents are unwilling to expend the required
effort.

4.1 A Negotiation Mechanism

We will start by presenting a simple mechanism that is suitable for cases where there exists a
reachable final state (that is, reachable by a suciently inexpensive plan) that satisfies both
agents' goals. We call this a cooperative situation. Later, we will enhance the mechanism
so that it can handle all possible encounters in State Oriented Domains, i.e., so that it can
handle conict resolution.
176

fiMechanisms for Automated Negotiation

Definition 3 Given an SOD < S ; A; J ; c >, we define:
 P  J to be the set of all one-agent plans, i.e., all joint plans in which only one agent
has an active role.

 The cost c(P ) of a one-agent plan in which agent k has the active role, P 2 P ,
is a vector that has at most one non-zero element, in position k. When there is no
likelihood of confusion, we will use c(P ) to stand for the k-th element (i.e., for c(P ) ),
rather than the entire vector.
k

Definition 4 Best Plans

 s ! f is the minimal cost one-agent plan in P in which agent k plays the active role
and moves the world from state s to state f in S .
 If a plan like this does not exist then s ! f will stand for some constant plan ./ that
k

k

k

costs infinity to agent k and 0 to all other agents.

 If s = f then s ! f will stand for the empty plan  that costs 0 for all agents.
 s ! F (where s is a world state and F is a set of world states) is the minimal cost
one-agent plan in P in which agent k plays the active role and moves the world from
k

k

state s to one of the states in F :

c(s ! F ) = min
c(s ! f ):
2
k

k

f

F

As we mentioned above, for the moment we will be restricting our attention to encounters
where there does exist one or more states that satisfy both agents' goals. What if more
than one such state exists? Which state should the agents choose to reach? And what if
there is more than one joint plan to reach those states? Which joint plan should the agents
choose?
For example, let's say that there are two states that satisfy both agents' goals. State 1
has two possible roles, with one of the roles costing 6 and the other costing 3. State 2 has
two roles also, with both of them costing 5. While State 1 is cheaper overall to reach, State
2 seems to allow for a fairer division of labor among the agents.
Assuming that the agents want their agreement to be ecient, they will decide to reach
State 1. But which agent should do the role that costs 6, and which should do the role that
costs 3? Our approach will be to allow them to agree on a \lottery" that will equalize the
benefit they derive from the joint plan. Although eventually one agent will do more than
the other, the expected benefit for the two agents will be identical (prior to holding the
lottery). These plans that include a probabilistic component are called mixed joint plans.
Throughout this paper, we limit the bulk of our discussion about mechanisms to twoagent encounters. Initial work on the generalization of these techniques to encounters among
more than two agents can be found elsewhere (Zlotkin & Rosenschein, 1994). That research
considers issues of coalition formation in n-agent Task Oriented Domains.

Definition 5 Deals Given an encounter in a two-agent SOD < s; (G1; G2) >:
177

fiZlotkin & Rosenschein

 We define a Pure Deal to be a joint plan J 2 J that moves the world from state s to
a state in G1 \ G2.
 We define a Deal to be a mixed joint plan J : p; 0  p  1 2 IR such that J is a Pure
Deal.

The semantics of a Deal is that the agents will perform the joint plan (J1; J2) with
probability p, or the symmetric joint plan (J2 ; J1) with probability 1 , p (where the agents
have switched roles in J ). Under the symmetric abilities assumption from Section 2.4,
both agents are able to execute both parts of the joint plan, and the cost of each role is
independent of which agent executes it.

Definition 6

 If  = (J : p) is a Deal, then Cost () is defined to be pc(J ) + (1 , p)c(J ) (where k
i

is i's opponent).

i

k

 If  is a Deal, then Utility () is defined to be c(s ! G ) , Cost ():
i

i

i

The utility (or benefit) for an agent from a deal is simply the difference between the
cost of achieving his goal alone and his expected part of the deal. Note that we write (for
example) c(s ! G ) rather than c(s ! G ); since the cost of the plan is independent of the
agent that is executing it.
k

i

i

Definition 7

 A Deal  is individual rational if, for all i, Utility ()  0:
 A Deal  is pareto optimal if there does not exist another Deal that dominates it|
i

there does not exist another Deal that is better for one of the agents and not worse
for the other.

 The negotiation set NS is the set of all the deals that are both individual rational and
pareto optimal.

A necessary condition for the negotiation set not to be empty is that there be no contradiction between the two agents' goals, i.e., G1 \ G2 6= ;. All the states that exist in the
intersection of the agents' goal sets might, of course, not be reachable given the domain of
actions that the agents have at their disposal. The condition of reachability is not sucient
for NS not to be empty, however, because even when there is no contradiction between
agents' goals, there may still not be a cooperative solution for them. In such a situation,
any joint plan that satisfies the union of goals will cost one agent (or both) more than he
would have spent achieving his own goal in isolation (that is, no deal is individual rational).
As an example in the Slotted Blocks Domain, consider the following encounter. The
initial state can be seen at the left in Figure 5. A1's goal is \The White block is at slot 2
but not on the table" and A2 's goal is \The Black block is at slot 1 but not on the table."
To achieve his goal alone, each agent has to execute one PickUp and then one PutDown;
c(s ! G ) = 2. The two goals do not contradict one another, because there exists a state
i

178

fiMechanisms for Automated Negotiation

in the world that satisfies them both (where the White and Black blocks are each placed
on a Gray block). There does not exist a joint plan that moves the world from the initial
state to a state that satisfies the two goals with total cost less than eight2 |that is, no deal
is individual rational.
Initial State

1

A1s
goal

2

3

1

.
.
.

2

3

Joint plan

1

A2s
goal

2

.
.
.

1

2

3

1

2

3

Figure 5: Conict Exists Even Though Union of Goals is Achievable
The existence of a joint plan that moves the world from its initial state s to a mutuallydesired state in G1 \ G2 is a necessary (but not sucient) condition for the negotiation set
to be non-empty. For the agents to agree on any joint plan, it should be individual rational.
This means that the sum of the roles that the agents play should not exceed the sum of their
individual stand-alone costs (otherwise, at least one of the agents would not get positive
utility, i.e., do more work than in his stand-alone plan). But even this condition is not
sucient to guarantee an individual rational deal, since it can be the case that the minimal
role in the joint plan is still too expensive for the agent with the minimum stand-alone cost.
Even a probabilistic mixture of the two roles will not reduce the expected cost for that
agent below the cost of the minimal role (and thus that role will not be individual rational
for him).
We now show, however, that the combination of these conditions is necessary and sucient for the negotiation set not to be empty.

Theorem 1 A necessary and sucient condition for the negotiation set not to be empty is
the existence of a joint plan that moves the world from its initial state s to a state in G1 \ G2
and also satisfies the following two conditions (the sum condition and the min condition):

 A joint plan J satisfies the sum condition if

X2 c(s ! G )  X2 c(J ) :
i

=1

i

=1

i

i

2. One agent lifts the white block, while the other agent rearranges the other blocks suitably (by picking
up and putting down each block once), whereupon the white block is put down. This is the best plan
because each block is picked up and put down only once.

179

fiZlotkin & Rosenschein

 A joint plan J satisfies the min condition if
2

2

min
c(s ! G )  min
c(J ) :
=1
=1
i

i

i

i

Proof:

 If NS 6= ;; then let J : p be some mixed joint plan in NS; thus, it is individual rational.
8i 2 f1; 2g
Utility (J : p)  0
c(s ! G ) , Cost (J : p)  0
c(s ! G )  Cost (J : p)
c(s ! G )  pc(J ) + (1 , p)c(J )
 min 2f1 2gc(J )
c(s ! G ) 
c (J )
i

i

i

i

i

X

X

i

k

;

l

i

2f1 2g

;

i

;

min c(s ! G )  min c(J )

2f1 2g

i

i

l

2f1 2g

i

i

2f1 2g

i

;

i

i

;

 Let J be a minimal total cost joint plan that moves the world from state s to a state
in G1 \ G2 ; and also satisfies the sum and min conditions. To show that NS =
6 ;; it is
sucient to show that there exists a deal that is both individual rational and pareto
optimal. Without loss of generality, we can assume that c(s ! G2)  c(s ! G1 ) and
c(J )2  c(J )1: From the min condition, we see that c(s ! G1)  c(J )1: There are
two cases:

{ If c(s ! G2)  c(J )2; then the deal J : 1 is individual rational.
{ If c(s ! G2) < c(J )2; then the deal J : p (where p = 1 ,
individual rational.

( ! 1 ), ( )1 )
( )2 , ( )1

c s

c J

G

c J

c J

is

J : p is also pareto optimal, because if there is another deal J 0 : q that dominates J : p
then J 0 : q is also individual rational and therefore satisfies the min and sum conditions

(see the proof, above, of the initial half of this theorem).
Since J 0 : q dominates J : p it also implies that

X

i

2f1 2g

Utility (J 0 : q ) >
i

;

X Utility (J : p):

X

This can be true only if

2f1 2g

i

;

X c(J ) :

c(J 0) <
i

;

i

2f1 2g

i

2f1 2g

i

i

;

But this contradicts the fact that J is the minimal total cost joint plan that satisfies
the sum and min conditions.
180

fiMechanisms for Automated Negotiation

The sum condition states that the sum of roles does not exceed the sum of the individual
agents' stand-alone costs. The min condition states that the minimal role in the joint plan
is less than the minimum stand-alone cost.
When the conditions of Theorem 1 are true, we will say that the encounters are cooperative. In such encounters, the agents can use some negotiation mechanism over mixed joint
plans. The question we next examine is what kind of negotiation mechanism they should
use.

4.2 Mechanisms that Maximize the Product of Utilities

In general we would like the negotiation mechanism to be symmetrically distributed, and
we would also like for there to be a negotiation strategy (for that mechanism) that is in
equilibrium with itself. A symmetrically distributed mechanism is one in which all agents
play according to the same rules, e.g., there are no special agents that have a different responsibility in the negotiation process. When asymmetric negotiation mechanisms are used,
the problem of responsibility assignment needs to be resolved first (e.g., who will be the
coordinator agent). We would then need a special mechanism for the responsibility assignment negotiation. If this mechanism is also asymmetric we will need another mechanism,
and so on. Therefore, it is better to have a symmetric negotiation mechanism to start with.
Among the symmetric mechanisms, we will prefer those that have a symmetric negotiation strategy that is in equilibrium. Given a negotiation mechanism M , we will say that
a negotiation strategy S from M is in equilibrium if, under the assumption that all other
agents are using strategy S when using M , I (or my agent) cannot do better by using a
negotiation strategy different than S .
Among all symmetrically distributed negotiation mechanisms that have a symmetric
negotiation strategy that is in equilibrium, we will prefer those that maximize the product
of agents' utilities. This means that if agents play the equilibrium strategy, they will
agree on a deal that maximizes the product of their utilities. If there is more than one
product-maximizing deal, they will agree on a deal (among those product maximizers) that
maximizes the sum of utilities. If there is more than one such sum-maximizing product
maximizer, the protocol will choose among those deals with some arbitrary probability.
This definition implies both individual rationality and pareto optimality of the agreed-upon
deals.
Note that maximization of the product of the utilities is not a decision that agents are
assumed to be making at run-time; it is a property of the negotiation mechanism agreed
upon by the agent designers (i.e., we are exploring what happens when the protocol designers
would agree on this property). In more classic game theory terms (see Section 9.1), the
protocol acts as a kind of \mediator," recommending \maximization of product of the
utilities" in all cases.
We will call this class of mechanisms the Product Maximizing Mechanisms, or PMMs. In
previous work on TODs (Zlotkin & Rosenschein, 1989, 1993a) we presented the Monotonic
Concession Protocol and the One-Step Protocol, both of which are PMMs. As mentioned
above, this paper does not examine the computational issues that arise in discovering deals.
There are a number of existing approaches to the bargaining problem in game theory.
One of the earliest and most popular was Nash's axiomatic approach (Nash, 1950; Luce
181

fiZlotkin & Rosenschein

& Raiffa, 1957). Nash was trying to axiomatically define a \fair" solution to a bargaining
situation. He listed the following criteria as ones that a fair solution would satisfy:
1. Individual rationality (it would not be fair for a participant to get less than he would
anyway without an agreement);
2. Pareto Optimality (a fair solution will not specify an agreement that could be improved
for one participant without harming the other);
3. Symmetry (if the situation is symmetric, i.e., both agents would get the same utility
without an agreement, and for every possible deal, the symmetric deal is also possible,
then a fair solution should also be symmetric, i.e., give both participants the same
utility);
4. Invariance with respect to linear utility transformations. For example, imagine two
agents negotiating over how to divide $100. If one agent measures his utility in
dollars while the other measures his in cents, it should not inuence the fair solution.
Similarly, if one agent already has $10 in the bank, and evaluates the deal that gives
him x dollars as having utility 10 + x while the other evaluates such a deal as having
utility x, it should not inuence the fair solution (i.e., change of origin doesn't affect
the solution);
5. Independence of irrelevant alternatives. Imagine two agents negotiating about how
to divide 10,000 cents. The Nash solution will be 5,000 cents for each, due to the
symmetry assumption above. Now imagine that the same agents are negotiating over
$100. Even though there are now some deals that they can't reach (for example, the
one where one agent gets $49.99, and the other gets $50.01), the solution should be
the same, because the original solution of 5,000 cents can still be found in the new
deal space.
Nash showed that the product maximizing solution not only satisfies the above criteria,
but it is the only solution that satisfies them. The first four criteria above are explicitly
or implicitly assumed in our own approach (in fact, for example, our version of the fourth
assumption above is more restrictive than Nash's). The fifth criteria above is not assumed
in our work, but turns out to be true in some cases anyway. We use the Nash solution,
in general, as a reasonable bargaining outcome, when it is applicable. Nash, however,
had some assumptions about the space of deals that we do not have. For example, the
Nash bargaining problem assumes a bounded, convex, continuous, and closed region of
negotiation. In our agent negotiations, we do not assume that the space of deals is convex,
nor that it is continuous.

4.3 Worth of a Goal

When the encounter is cooperative, then agents can use some PMM over mixed joint plans.
Such a mechanism guarantees a fair and ecient cooperative agreement. The question now,
however, is what can be done in non-cooperative encounters?
Consider again the encounter from the Restricted Usage Shared Resource Domain where
there are three agents, and one resource which has a maximal capacity of two. Agents 1
182

fiMechanisms for Automated Negotiation

T
i
m
e

A1

Agents
A2

A3

0 Use Use Wait
1 Use Use Wait
2 NOP Use Use
3 NOP NOP Use
4 NOP NOP NOP

0
1
2
3
4
5

A1

Agents
A2

A3

Use Wait Use
Use Wait Use
NOP Use NOP
NOP Use NOP
NOP Use NOP
NOP NOP NOP

Figure 6: Two Joint Plans in the Restricted Usage Shared Resource Domain
and 3 need two units of the resource while agent 2 needs 3 units of the resource. Each
agent, were it alone in the world, could achieve its goal at no cost (i.e., without waiting
for the resource). However, since the maximal capacity of the resource is two, the three
agents together cannot achieve their combined goal without some agent having to wait.
Two possible joint plans that achieve all agents' goals can be seen in Figure 6. The left
joint plan gives agents 1 and 2 utility of 0, while giving agent 3 utility of ,2. The right joint
plan gives agents 1 and 3 utility of 0, while giving agent 2 utility of ,2. Globally, the plan
on the left finishes sooner. But from the perspective of the individual agents, the two plans
are really comparable|in one, agent 3 suffers by waiting two time units, and in the other,
agent 2 suffers by exactly the same amount. We have assumed, however, that the agents
are not concerned with the global aspects of resource utilization, and are only concerned
about their own local cost. In addition, both plans are Pareto Optimal, and neither of them
is individual rational (because one agent gets negative utility).
If there exists a joint plan J that brings the world to a state that satisfies all agents'
goals, but either the min condition or the sum condition is not true, then for the agents
cooperatively to bring the world to a state that satisfies all agents' goals, at least one of
them will have to do more than if he were alone in the world and achieved only his own
goals. Will either one of them agree to do extra work? It depends on how important each
goal is to each agent i, i.e., how much i is willing to pay to bring the world to a state in G .
For example, in the Shared Resource encounter above, agent 2 or 3 might be willing
to wait two time units so as to get its turn at the resource. Although they could have
done better were they alone in the world, they must cope with the presence of those other
agents. With our original definition of utility, no deal that achieves all agents' goals will
be individual rational|someone will have to wait, and thus get negative utility. So with
that utility definition, no agent should be willing to wait. The agents will fail to reach
agreement, and no one will achieve his goal. This is because utility was calculated as the
difference between the cost of an agent's plan were it alone in the world and the cost of his
role in the joint plan with other agents.
However, why should the agents use stand-alone cost as their baseline for determining
utility? It may be the case that agents will be willing, in the presence of other agents, to
admit the need to pay an extra cost, a sort of \coordination overhead." The fact that with
other agents around they have to do more does not necessarily make it irrational to do more.
i

183

fiZlotkin & Rosenschein

In Task Oriented Domains (Zlotkin & Rosenschein, 1989, 1993a, 1994), it is reasonable to
use stand-alone cost as the utility baseline since there was never any coordination overhead.
In the worst case, an agent could always achieve his goal at the stand-alone price, and
coordination could only improve the situation. In State Oriented Domains, however, it
makes sense to consider altering the utility baseline, so that agents can rationally coordinate
even when there exists a coordination overhead. One way of doing this is to assume that
each agent has some upper bound on the cost that he is willing to bear to achieve his goal.
Then, the agent's utility can be measured relative to this upper bound. We call this upper
bound the worth of the agent's goal.
Even in TODs, one can conceive of stand-alone cost as the worth that an agent assigns
to achieving a goal. The stand-alone cost is then the maximum that an agent is willing to
expend. In a TOD, this maximum need never be violated, so it's a reasonable worth value
to use.
When such an upper bound does not exist, i.e., an agent is willing to achieve his goal
at any cost, other techniques can be used (see Section 6 below).

Definition 8 Given an encounter in a two-agent SOD < s; (G1; G2) >, let w be the maxi

imum expected cost that agent i is willing to pay in order to achieve his goal G . w
will be called the worth of goal G to agent i. We will denote this enhanced encounter by
< s; (G1; G2); (w1; w2) > :
i

i

i

The definition of Utility can be usefully altered as follows:

Definition 9 Given an encounter < s; (G1; G2); (w1; w2) >; if  is a deal, i.e., a mixed
joint plan satisfying both agents' goals, then Utility ( ) is defined to be w , Cost ( ):
i

i

i

The utility for an agent of a deal is the difference between the worth of its goal that is
being achieved, and the cost of his role in the agreed-upon joint plan. If an agent achieves
his goal alone, his utility is the difference between the worth of the goal and the cost that
he pays to achieve the goal. The point is, that an agent might be better off alone but still
derive positive utility from a joint plan, when we use worth as the utility baseline. With
the new definition of utility, it may be rational to compromise.

Theorem 2 If in Theorem 1 we change every occurrence of c(s ! G ) to w , then that
i

theorem is still true.

i

Proof: Substitute w for every occurrence of c(s ! G ) in the proof of Theorem 1.
i

i

By introducing the worth concept into the definition of an encounter, we have enlarged
the number of encounters that have a non-empty negotiation set. Cooperative behavior is
enhanced. Our negotiation mechanism, that makes use of any product maximizing protocol,
becomes applicable to more SOD encounters.

4.4 Interaction Types

From the discussion above, we have begun to see emerging different kinds of encounters
between agents. In TOD meetings, agents really benefit from coordination. In SODs, this
184

fiMechanisms for Automated Negotiation

isn't necessarily the case. Sometimes agents benefit, but sometimes they are called upon to
bear a coordination overhead so that everyone can achieve their goals. In even more extreme
situations, agents' goals may simply be in conict, and it might just be impossible to satisfy
all of them at the same time, or the coordination overhead may exceed the willingness of
agents to bear the required burden.
We have four possible interactions, from the point of view of an individual agent:

 A symmetric cooperative situation is one in which there exists a deal in the negotiation

set that is preferred by both agents over achieving their goals alone. Here, both agents
welcome the existence of the other agent.
 A symmetric compromise situation is one where there are individual rational deals
for both agents. However, both agents would prefer to be alone in the world, and to
accomplish their goals alone. Since each agent is forced to cope with the presence of
the other, he would prefer to agree on a reasonable deal. All of the deals in NS are
better for both agents than leaving the world in its initial state s.
 A non-symmetric cooperative/compromise situation is one in which one agent views
the interaction as cooperative (he welcomes the existence of the other agent), while
the second views the interaction as compromise (he would prefer to be alone in the
world).
 A conict situation is one in which the negotiation set is empty|no individual rational
deals exist.
In a general SOD, all four types of interaction can arise. In a TOD, only the symmetric
cooperative situation ever exists.

s

u

 
 


@@
@@
@@@@
@R

G1

G2

Figure 7: The Symmetric Cooperative Situation
Each of these situations can be visualized informally using diagrams. The symmetric
cooperative situation can be seen in Figure 7, the symmetric compromise situation in Figure 8, the non-symmetric cooperative/compromise situation in Figure 9, and the conict
situation in Figure 10. A point on the plane represents a state of the world. Each oval
represents a collection of world states that satisfies an agent's goal. s is the initial state of
the world. The triple lines emanating from s represent a joint plan that moves the world
to a final state. Each of the agents will share in the carrying out of that joint plan. The
185

fiZlotkin & Rosenschein

s

u 
 
 
@
@@
@@

@@@R G1

G2

Figure 8: The Symmetric Compromise Situation

s

u 
 
 
@
@@ G
@@@@ 2
@R

G1

Figure 9: The Non-Symmetric Cooperative/Compromise Situation
overlap between ovals represents final states that satisfy the goals of both agents A1 and
A2 . Informally, the distance between s and either oval represents the cost associated with
a single-agent plan that transforms the world to a state satisfying that agent's goal.
Note that in Figure 8, the distance from s to either agent's oval is less than the distance
to the overlap between ovals. This represents the situation where it would be easier for each
agent to simply satisfy his own goal, were he alone in the world. In Figure 7, each agent
actually benefits from the existence of the other, since they will share the work of the joint

u  
  
 

s

-

q

G1

?

G2

Figure 10: The Conict Situation
186

fiMechanisms for Automated Negotiation

plan. Note that in Figure 9, one agent benefits from the existence of the other, while the
other would prefer to be alone in the world.
Let's consider some simple examples in the slotted blocks world domain of cooperative,
compromise, and conict situations. In the initial situation depicted in Figure 11, the white
block is in slot 1 and the black block is in slot 2. Agent A1 wants the white block alone in
slot 2, while agent A2 wants the black block alone in slot 1. Were either of the agents alone
in the world, it would cost each of them 4 pickup/putdown operations to achieve their goal.
For example, A1 would have to pick up the black block in slot 2 and move it to slot 3, then
pick up the white block in slot 1 and move it to slot 2. The two agents together, however,
are able to achieve both goals with a total cost of 4. They can execute a joint plan where
they simultaneously pick up both blocks, and then put them in their appropriate places.
Each role in this joint plan costs 2, and each agent derives a utility of 2 from reaching an
agreement with the other. This is a cooperative situation. Coordination results in actual
benefit for both agents.

Initial State

1

2

A1s goal

3

1

A2s goal

2

Joint plan
1

2

1

2

3

3

1

2

3

Figure 11: Cooperative Situation
Now let's consider a more complicated, compromise, situation. In the initial state shown
in Figure 12, there is a white block in slot 1, a black block in slot 2, and two gray blocks
in slot 3. Agent A1 's goal is to have the white block somewhere at slot 2, but not on the
table. Similarly, agent A2 's goal is to have the black block somewhere at slot 1, but not on
the table. Alone in the world, agent A1 would only have to do one pickup and one putdown
operation, just moving the white block onto the black block in slot 2. In the same way,
agent A2 alone in the world can achieve his goal with two operations. But since each is
(in his stand-alone plan) using the other's block as a base, the achievement of a state that
satisfies both agents' goals requires additional blocks and operations.
The best plan for achieving both agents' goals requires moving one gray block from slot
3 to slot 1 and the other gray block to slot 2 so that they act as bases for the white and
black blocks. Each block needs to be picked up and put down at least once; the best plan
187

fiZlotkin & Rosenschein

has each block moving only once at a total cost of 8. Obviously, one or both agents will need
to do extra work (greater than in the stand-alone situation) to bring about this mutually
satisfying state.
Initial State

1

2

A1s
goal
3

1
1

.
.
.

2

3

Joint plan

A2s
goal

2

.
.
.

1

2

3

1

2

3

Figure 12: Compromise Situation
The best plan has two roles, one requiring 6 operations and one requiring 2 operations.
One agent will lift the black (or white) block, while the other agent rearranges all the
other blocks appropriately. The first agent will then put down the black (or white) block,
completing the plan. If both agents' worths satisfy the min and sum conditions (meaning,
here, that the sum of the worths is greater than or equal to 8, and each worth is greater
than or equal to 2), then they can reach an agreement that gives them both positive utility
(using worth as the new baseline for evaluating utility).
For example, let's say that agent A1 assigned a worth of 3 to achieving his goal, while
agent A2 assigned a worth of 6 to achieving his goal. Since one role in the best joint plan
costs 2 while the other costs 6, there is one unit of utility to be shared between the agents.
Any mechanism that maximizes the product of their utilities will split this one unit equally
between the agents. How is this done in our case? There is one deal in the negotiation set
that gives both agents the same expected utility of 21 , namely the mixed joint plan that
has agent A1 doing the cost-2 role with probability 78 , and the cost-6 role with probability
1
A2 of course assumes the complementary role. Agent A1 's expected utility is
8 . Agent
7
3 , 2( 8 ) , 6( 18 ) = 12 ; which is equal to agent A2 's expected utility of 6 , 2( 18 ) , 6( 78 ) = 12 :
This division of utility maximizes the product of expected utility among the agents.
There is an interesting phenomenon to note in this deal. Both agents are apparently
in a symmetric situation, apart from their internal attitude towards achieving their goals
(i.e., how much they are willing to pay). But as can be seen above, the more you are
willing to pay, the more you will have to pay. The agent that has a worth of 3 ends up
having less expected work than the agent with a worth of 6. This gives agents an incentive
to misrepresent their true worth values, pretending that the worth values are smaller than
they really are, so that the agents' positions within the negotiation will be strengthened. An
agent can feign indifference, claim that it really doesn't care all that much about achieving
188

fiMechanisms for Automated Negotiation

its goal, and come out better in the negotiation (with lower expected cost). We will examine
this question in greater detail below in Section 8.
Our final example here is of a conict situation, shown in Figure 13. Again, the white
block is in slot 1 and the black block is in slot 2|the same initial state that we had above in
the cooperative and compromise examples. In the cooperative example, the agents wanted
the blocks moved to another, empty slot. In the compromise example, the agents wanted to
blocks moved to a specific non-empty slot. Here, in the conict example, the agents want
the blocks moved onto a specific other block in a specific slot. Agent A1 wants the white
block on top of the black block in slot 2; agent A2 wants the black block on top of the white
block in slot 1. Here, there is a real contradiction between the two agents' goals. There
exists no world state that satisfies them both. In the next section, we will discuss what
kinds of coordination mechanisms can be used in a conict situation.
Initial State

1

2

A1s
goal
3

1

A2s
goal
1

2

1

3

1

?

2

3

2

3

2

Figure 13: Conict Situation
When the negotiation set is not empty, we can distinguish between compromise and
cooperative situations for a particular agent i using the following algorithm:
1. If w  c(s ! G ); then agent i is in a cooperative situation.
2. If w > c(s ! G ); then agent i might be in a cooperative or a compromise situation.
The way to distinguish between them is as follows:
(a) Set w = c(s ! G ); and leave the other agent's worths unchanged.
(b) If the resulting NS is empty, then agent i is in a compromise situation.
(c) Otherwise, agent i is in a cooperative situation.
i

i

i

i

i

i

5. Conict Resolution and Cooperation

We have seen that in both cooperative and compromise encounters there exist deals that are
individual rational for both agents. Agents will negotiate over which of these deals should
be reached, if there is more than one. What, however, can be done when the agents are in
189

fiZlotkin & Rosenschein

a conict situation, i.e., there are no individual rational deals? Here, the agents have a true
conict that needs to be resolved, and are not merely choosing among mutually acceptable
outcomes.

5.1 Conict Resolution

A simple approach to conict resolution would be for the agents to ip a coin to decide who
is going to achieve his goal and who is going to be disappointed. See Figure 14. In this
case they will negotiate on the probabilities (weightings) of the coin toss. If they run into
a conict during the negotiation (fail to agree on the coin toss weighting), the world will
stay in its initial state s.3
Initial State

1

2

A1s Goal

3

1

2

3

Flip a coin
A2s Goal
1

2

1

2

3

1

2

3

Figure 14: Conict Resolution
This deal can be visualized graphically in Figure 15. Single lines represent one agent
plans.
In conict situations the agents can use some utility product maximizing protocol to
decide on the weighting of the coin. However, it turns out that in that case the probability
of 12 always results in the maximum product of the two agents' utilities. If the agents are
to maximize their utility product, they will always agree on a symmetric coin. The only
exception is when the initial state already satisfies one agent's goal. Then, that agent will
simply cause the negotiation to fail, rather than risk moving away from his goal-satisfying
state. Nevertheless, even here, the product maximizing deal would have the agents ip a
symmetric coin.
Why is a symmetric coin going to maximize the product of agent utilities? Some simple
mathematics shows the reason. Assume that agent A1 has a worth of w1, and the cost of
achieving his goal alone is c1 . When A1 wins the coin toss, he will have a utility of w1 , c1.
His utility from a deal with coin weighting p will be p(w1 , c1). His opponent's utility
3. There is a special case where the initial state s already satisfies one of the agent's goals, let's say agent
1 (s cannot satisfy both goals since then we would not have a conict situation). In this case, the only
agreement that can be reached is to leave the world in state s. Agent 1 will not agree to any other deal
and will cause the negotiation to fail.

190

fiMechanisms for Automated Negotiation

u  
  
 

s

-

q

G

?

a

G

b

Figure 15: The Conict Situation
from this deal will be (1 , p)(w2 , c2). The product of the two agents' utilities will be
(p , p2 )(w1 , c1)(w2 , c2). This function of p is maximized when p equals 21 for any values
of w1; w2; c1; and c2 (simply take the derivative of the function and set it equal to zero).
This may seem like a \fair" solution, but it is certainly not an ecient one. While
maximizing the product of the agents' utilities, it does not maximize their sum. The sum
of utilities will be maximized by simply having the agent with a larger w , c achieve his
goal. This, on the other hand, is certainly not a fair solution.
We might be able to be both fair and ecient if agents are able to transfer utility to one
another. In that case, one agent could achieve its goal but share part of that utility with
the other agent. The negotiation would then center on how much of the utility should be
transferred! Any product maximizing mechanism used to resolve this question will transfer
half of the gained utility to the other agent, because it is a constant sum game, and dividing
the utility equally maximizes the utility product.
The entire subject of explicit utility transfer through side payments is a complicated one
that has been treated at length in the game theory community. It is not our intention to
examine these questions in this paper. Even if utility is not explicitly transferable, agents
can make commitments to perform future actions, and in effect transfer utility through
these promises. Again, there are many complicated issues involved in assessing the value
of promises, when they should be believed, discount factors, and limits on the amount of
promising and debts that an agent can accrue. If agents can accumulate debt indefinitely,
it will be possible for them to always pay off previous commitments by making additional
commitments to others. Here, too, we are leaving these issues aside, returning to our
assumptions that each interaction stands on its own, and no explicit side payments are
possible.
i

i

5.2 Cooperation in Conict Resolution
In both cooperative and compromise situations, agents were implicitly able to transfer utility
in a single encounter by doing more actions in the joint plan. The agent that does more
work in the joint plan relieves the other agent, increasing the latter's utility. This can be
thought of as a kind of utility transfer. Here, we will see a similar kind of implicit utility
transfer that is possible even in conict situations.
191

fiZlotkin & Rosenschein

The agents may find that, instead of simply ipping a coin in a conict situation, it
is better for them to cooperatively reach a new world state (not satisfying either of their
goals) and then to ip the coin to decide whose goal will ultimately be satisfied.
Consider the following example. One agent wants the block currently in slot 2 to be in
slot 3; the other agent wants it to be in slot 1. In addition, both agents share the goal of
swapping the two blocks currently in slot 4 (i.e., reverse the stack's order). See Figure 16.
Assume that W1 = W2 = 12. The cost for an agent of achieving his goal alone is 10. If the
agents decide to ip a coin in the initial state, they will agree on a weighting of 21 , which
brings them a utility of 1 (i.e., 12 (12 , 10)). If, on the other hand, they decide to do the swap
cooperatively (at cost of 2 each), and then ip a coin, they will still agree on a weighting
of 12 , which brings them an overall utility of 4 (i.e., 12 (12 , 2 , 2)).
Initial State

1

2

3

A1s goal
4

1

2

3

4

Semi-cooperative
deal

A2s goal

2

1

1

2

3

4

1

2

3

4

Figure 16: Cooperation up to a Certain Point
The fact that agents, even in a conict situation, can get more utility by first cooperatively working together, and only then ipping a coin, can be exploited by defining a new
kind of deal, called a Semi-Cooperative Deal. We want agents to be able to negotiate over
and agree on a deal that allows them this mixed cooperative/conict resolution interaction.
Changing the deal type is enough to make this possible. It ends up increasing the expected
utility that agents can derive from an encounter.

Definition 10 A Semi-Cooperative Deal is a tuple (t; J; q) where t is a world state, J is a

mixed joint plan that moves the world from the initial state s to intermediate state t, and
0  q  1 2 IR is the weighting of the coin toss|the probability that agent A1 will achieve
his goal.

The semantics of this kind of deal is that the two agents will perform the mixed joint
plan J , and will bring the world to intermediate state t. Then, in state t, they will ip a
coin with weighting q to decide who continues the plan towards their own goal. This allows
the agents to handle conict between their goals, while still cooperating up to a certain
point.
192

fiMechanisms for Automated Negotiation

The utility of a semi-cooperative deal for an agent can be defined as follows. If he loses
the coin toss in intermediate state t, he simply has a negative expected utility equal to the
expected cost of his role in the joint plan that reached state t. If he wins the coin toss in
intermediate state t, then his expected utility is the difference between the worth of his goal
and the costs of his role in the joint plan that reached t as well as the stand-alone cost of
moving from t to his goal state. This can be written formally as follows:

Definition 11
Utility (t; J; q ) = q (w , c(J ) , c(t ! G ) ) , (1 , q )c(J )
= q (w , c(t ! G ) ) , c(J )
i

i

i

i

i

i

i

i

i

i

i

i

i

This assumes, of course, that the agents' goals are in conict|the state that satisfies
one agent will be of no worth to the other.
The Semi-Cooperative Deal can be visualized graphically in Figure 17. This figure is
similar in spirit to the figures presented above in Section 4.4, which represented cooperative,
compromise, and conict encounters. Again, a triple line represents a joint plan while a
single line represents a one-agent plan.

u 
u
  
 

s

@@
@@@R t

q

- G1

?

G2

Figure 17: Semi-Cooperative Deal

5.3 Semi-Cooperative Deals in Non-Conict Situations

In cooperative and compromise situations, the agents negotiate on deals that are mixed
joint plans, J : p (cooperative deals). In a conict situation, the agents negotiate on deals
of the form (t; J; q ) (semi-cooperative deals).
Even though semi-cooperative deals were intended to be used in conict situations, they
can also be used in cooperative and compromise situations (with a minor generalization in
the definition of utility, discussed below). The question is, what kinds of agreements will
agents in a non-conict situation reach, if they are negotiating over semi-cooperative deals?
Will they do better than using standard cooperative (mixed joint plan) deals? Or will they
do worse?
A cooperative deal which is a mixed joint plan J : p can also be represented as (J (s); J : p; 0)
where J (s) is the final world state resulting from the joint plan J when the initial state is s.
In other words, mixed deals are a proper subset of semi-cooperative deals, and any mixed
193

fiZlotkin & Rosenschein

deal can be represented as a semi-cooperative deal of a special form. The intermediate state
t is taken to be the final state of the agents' cooperative joint plan. Since that final state
satisfies both agents' goals, the result of the coin ip is irrelevant|neither of the agents
wants to change the world state anyway.
Therefore, by having agents in a non-conict situation negotiate over semi-cooperative
deals, we are only enlarging their space of agreements. Any deal that can be reached when
negotiating over the subset (i.e., mixed joint plans) can also be reached when negotiating
over the larger set (i.e., semi-cooperative plans). So the agents in a non-conict situation
will certainly do no worse, when using semi-cooperative deals. But will they do better?
There are two potential ways in which agents could do better. The first would be if
agents find a cheaper way to achieve both goals. It turns out that this is impossible|
semi-cooperative deals will not uncover a more ecient way of achieving both agents' goals.
However, there is a more surprising way in which agents can benefit from semi-cooperative
deals. Agents can benefit by not always achieving their goals. When using semi-cooperative
deals, they can give up guaranteed goal satisfaction, and gain expected utility.
To see what we mean, consider the following example in the Slotted Blocks World. The
initial situation in Figure 18 consists of 5 duplications of the example from Figure 5, in slots
1 to 15. In addition, two slots (16 and 17) each contain a stack of 2 blocks. Agent A1 's goal
is \White blocks are in slots 2; 5; 8; 11 and 14 but not on the table; the blocks in slots 16
are swapped, and the blocks in slot 17 are swapped (i.e., each tower is reversed)." Agent
A2's goal is \Black blocks are in slots 1; 4; 7; 10 and 13 but not on the table; the blocks in
slot 16 are swapped, and the blocks in slot 17 are swapped."
Initial State

1

2

...

3

13

14

15

16

17

15

16

17

A1s goal
.
1

2

.
3

...

13

14

A2s goal
.
1

.
2

3

...

13

14

15

16

17

Figure 18: Semi-Cooperative Agreement in a Cooperative Situation
The stand-alone cost of both agents is: c(s ! G ) = 26 = (5  2)+(2  8). Let's assume
that w = 26 is also the worth of each goal. The minimal cost joint plan that achieves both
agents' goals has 7 parts:
i

i

 Cooperative swap in slot 17 in which each agent does one pickup and one putdown;
 The same swap in slot 16;
194

fiMechanisms for Automated Negotiation

 Five duplications of the joint plan from Example 5. Each of those joint plans has a
role that costs 6 and a role that costs 2.

Thus the average cost of each agent's role in the joint plan is 24, namely (2  2) +
(5  12 (6 + 2)): Since the stand-alone cost is 26, this situation is cooperative|each agent
welcomes the existence of the other. For both agents, the expected utility of the joint plan
is 2 (i.e., 26 , 24). This cooperative deal achieves both agents' goals.
Can we find a semi-cooperative deal that is better? What if the agents only cooperated
on swapping the blocks in slots 16 and 17, and then tossed a coin to see who gets to fulfill his
own goal (leaving the other's goal unsatisfied)? This semi-cooperative deal actually turns
out to be better for both agents.
Let the intermediate state t be the state where the blocks in slots 16 and 17 are swapped,
and the other slots are unchanged. Each agent invests 4 operations as his part of the two
swaps. He then has a chance of 12 of continuing on his own to achieve his goal, and a chance
of 12 of losing the coin toss and having wasted his initial investment. If he wins the coin toss,
he will have an additional 10 operations (5  2), but achieve his goal of worth 26. Overall
utility will be 26 , 10 , 4; i.e., 12. If he loses the coin toss, he has just wasted his initial
investment of 4, so his utility is ,4. The expected utility is the average of these two cases,
i.e., 4. This is better than the utility of 2 the agents got using the cooperative deal!
In other words, in this case, the agents would prefer not to guarantee themselves their
goal, and take a gamble with a semi-cooperative deal. Their expected utility doubles, if
they are willing to take a risk. So even in this cooperative situation, the agents benefit from
negotiating over semi-cooperative deals.
Now, it turns out that this is a borderline situation, brought about because w is low. As
long as w is high enough, any semi-cooperative deal that agents agree on in a cooperative
situation will be equivalent to a cooperative deal. If achieving your goal isn't worth too much
to you (your profit margin is small), you might be willing to forgo guaranteed achievement
in exchange for a higher expected utility.
So semi-cooperative deals, used in a non-conict situation, will sometimes result in better
agreements (when forgoing guaranteed goal achievement is beneficial), and will never result
in worse agreements. Clearly, it is worthwhile for agents to negotiate over semi-cooperative
deals, regardless of whether they are in cooperative, compromise, or conict situations.
i

i

5.4 Unified Negotiation Protocols (UNP)

In this section, we will make the necessary generalizations so that agents can use semicooperative deals in all types of encounters. We will call all product maximizing mechanisms
based on semi-cooperative deals \Unified Negotiation Protocols (UNP)," since they can be
used for conict resolution, as well as for cooperative agreements.4
As mentioned above, we will need to generalize the previous definition of the utility
of a semi-cooperative deal, to enable UNPs. Before, we assumed (since we had a conict
situation) that the final state would be of no benefit to the agent that lost the coin toss.
4. An earlier version of this subsection and the next two appeared in (Zlotkin & Rosenschein, 1991a). The
current treatment incorporates new material on multi-plan deals, recasts the protocols in the context of
domain theory, and alters the notation to correspond to the more general domain framework.

195

fiZlotkin & Rosenschein

Now, even though a semi-cooperative deal is being used, the final state might still satisfy
both agents' goals, and not just the goal of the agent that wins the coin toss.
If (t; J; q ) is a semi-cooperative deal, then we'll define f to be the final state of the world
when agent i wins the coin toss in state t. f = (t ! G )(t) 2 G . The worth for agent i of
any state r, which we will write as W (r), will be his goal worth w if r is a goal state, and
0 otherwise. Now, we can revise our definition of utility for semi-cooperative deals:
i

i

i

i

i

i

Definition 12 Utility (t; J; q) = q (w , c(t ! G ) ) + (1 , q )w (f ) , c(J )
i

i

i

i i

i

i

j

i

The utility of a semi-cooperative deal (t; J; q ) for an agent is now defined to be the
expected worth of the final state minus the expected cost. The worth of the expected final
state, of course, depends on the weighting of the coin, and whether both possible final
states (or only one) are goal states for the agent. Similarly, the expected cost depends on
the weighting of the coin (whether the agent only participates in the first, joint, plan, or
also continues with the second, lone, plan).
The definition of utility given above is completely consistent with the earlier definition
of the utility of cooperative deals, and can be viewed as a generalization of that earlier
definition. In other words, if a cooperative deal (a mixed joint plan) is mapped into a
semi-cooperative deal (t; J; q ) using the transformation discussed above, then the definition
of utility for a mixed joint plan (Definition 9) and this definition of utility (Definition 12)
for a semi-cooperative deal yield the same number.
A sucient condition for the negotiation set to be non-empty over semi-cooperative
deals is that agents' worths be high enough, so that each agent would be able to achieve its
own goal alone:

Theorem 3 If for each agent i the worth of its goal is greater than or equal to his standalone cost (i.e., 8i w  c(s ! G )), then the negotiation set over semi-cooperative deals is
not empty.
i

i

Proof: To show that NS 6= ;; it is sucient to show that there is an individual rational

semi-cooperative deal. The existence of pareto-optimal deals among the individual rational
deals is due to the compactness of the deal space (since there is only a finite number of
agent operations, and the worth of agent goals is bounded). (s; ; q ); where  is the empty
joint deal, is individual rational for any q:
The above condition is sucient, but is not necessary, for the negotiation set to be nonempty. For example, consider again the situation given in Figure 16, but with the agents'
worths being equal to 8 (instead of 12). Neither agent can achieve its goal alone, and thus
the conditions of the theorem above are not satisfied. However, there is a perfectly good
semi-cooperative deal that gives both agents positive utility|they perform a joint plan that
swaps the blocks in slot 4, then ip a coin to see whether the block in slot 2 goes to slot 1
or 3. This mixed deal gives each agent an expected utility of 1. So the negotiation set is
not empty.
It turns out that if there is a semi-cooperative deal in the negotiation set, and one of
the agents, winning the coin toss, will bring the world into a state that satisfies both agents'
goals, then there exists another deal in the negotiation set with the same utility for both
agents in which the intermediate state already satisfies both agents' goals.
196

fiMechanisms for Automated Negotiation

Theorem 4 For a semi-cooperative deal (t; J; q) 2 NS, if there exists an i such that f 2
G1 \ G2, then this semi-cooperative deal is equivalent to some cooperative deal.
Proof: There are two cases: both final states, or only one final state, is in G1 \ G2:
 If f1; f2 2 G1 \ G2; then we can view the last step as performing a mixed joint plan
that moves the world from state t to a state in G1 \ G2 .
c(t ! G1) = c(t ! G1 \ G2) = c(t ! G2 );
because if X  Y and (t ! Y )(t) 2 X; then c(t ! X ) = c(t ! Y ): f1 ; f2 are not
necessarily the same state, but this deal is equivalent to a deal where f1 = f2 : We
can look at the concatenation of the two mixed joint plans (the first being J from s
to t, the second t ! G1 \ G2 ), as a mixed joint plan P from s to G1 \ G2 : P is a
cooperative deal that is equivalent to (t; J; q ); because
Utility (t; J; q ) = q (w , c(t ! G )) + (1 , q )(w ) , c(J )
= w , (q c(t ! G ) + c(J ) )
= w , c(P )
= Utility (P ):
i

i

i

i

i

i

i

i

i

i

i

i

i

i

i

 If f1 2 G1 \ G2 and f2 62 G1 \ G2; then agent 2 would prefer to lose the coin toss
at state t and let agent 1 achieve the goal for him without his spending any more.
The deal (t; J; 1) is better for 1 and can only be better for 2 as well, so it dominates
(t; J; q ); but (t; J; q ) 2 NS; so they are equivalent. (t; J; 1) is equivalent to the mixed
joint plan P in which the agents perform the joint plan J until t; and then agent 1
performs the one-agent plan t ! G1 \ G2: P is a cooperative deal.

In other words, if there exists a semi-cooperative deal in the negotiation set that sometimes satisfies both agents' goals (depending on who wins the coin toss), then there also
exists another semi-cooperative deal in the negotiation set that always satisfies both agents'
goals (equivalent to a cooperative deal). Even though semi-cooperative deals constitute a
superset of cooperative deals, no extra utility is derived from using semi-cooperative deals
if the agreement preserves mutual satisfaction of both agents (i.e., if it's equivalent to a
cooperative deal).
In a cooperative situation, agents cannot extract more utility from a semi-cooperative
deal, unless they are willing to agree on a deal that will never satisfy both agents' goals.
The example above (Section 5.3) is the prototype of that situation. Agents increase their
utility by using a semi-cooperative deal in a cooperative situation. They do this by forgoing
guaranteed mutual satisfaction. The above theorem implies that this is the only way they
can increase their utility with semi-cooperative deals.

5.5 Multi-Plan Deals

In semi-cooperative deals, we assume that the agents cooperate, ip a coin, then the winner
proceeds alone to achieve his goal. This arrangement only requires that the agents engage
197

fiZlotkin & Rosenschein

in \pre-ip cooperation." What if the agents were willing (or required) to also engage in
\post-ip cooperation"? Then, an entirely new dimension of agreements would be opened
up. In this section, we consider a kind of deal that exploits cooperation after the coin toss.
To illustrate the potential of this new kind of deal, consider the following encounter,
shown in Figure 19.
A1s
Goal

Initial
State
1

2

3

1

2
1

3

?

2

A2s
Goal
1

2

3

1

2

3

Figure 19: Post-Flip Cooperation can be Helpful
The initial state s of the world can be seen in Figure 19. A1 's goal is to swap the position
of the blocks in slot 3, but to leave the blocks in slot 2 in their initial position. A2 's goal is
to swap the position of the blocks in slot 2, but to leave the blocks in slot 3 in their initial
position.
To achieve his goal alone, each agent needs to do at least 8 pickup or putdown operations.
Apparently, there is very little room for cooperation. Not only is there no final state that
satisfies both agents' goals, there is no intermediate state (other than the initial state) to
which the agents can cooperatively bring the world, before tossing a coin (as in a semicooperative deal).
Negotiating over semi-cooperative deals, agents will agree to ip a coin in the initial
state, and whoever wins the coin toss will by himself bring the world into his goal state
(at a cost of 8). Assume that the worth of each agent's goal is 10. Then negotiating over
semi-cooperative deals brings each agent an expected utility of 1. This is a compromise
situation (alone in the world, each agent would have utility of 2).
What if the agents could reach the following agreement (as shown in Figure 20): they
ip a coin in the initial state. Whoever wins the toss gets his goal satisfied. However, no
matter who wins, the agents commit themselves to work together in a joint plan to achieve
the chosen goal.
Doing either swap jointly costs a total of 4 for the two agents (2 each). The agent that
wins the coin toss gets a utility of 10 , 2 (his goal is satisfied and he expends 2 in the
joint plan). The agent that loses gets a utility of ,2 (he just expends 2 in the joint plan
that achieves his opponent's goal). If each agent has an equal chance of winning the coin
toss, his expected utility will be 3. This is better than the semi-cooperative deal that gave
198

fiMechanisms for Automated Negotiation

2

1
1

1

2

2
3
1

2

3

Multi-plan deal
1

2

1

2

3

Figure 20: Multi-Plan Deal
the agents each a utility of 1. It's even better than the stand-alone utility of 2 that the
agents could get if they were alone! Suddenly, the situation has become cooperative. The
agents welcome each other's existence, even though their goals have nothing in common.
There is no goal state that satisfies both agents; there are no subgoals that the agents have
in common; there are no positive interactions between the agents' stand-alone plans. The
goals are completely decoupled, and yet the situation is cooperative.
The agreement above, of course, requires \post-ip cooperation." With semi-cooperative
deals, the \pre-ip cooperation" contributed potentially to either agents' benefit|either
agent might win the coin toss and exploit the early work. But with this new deal type, even
the agent who loses the coin toss will be required to expend effort, knowing that it is just
for the benefit of the other agent.
If agents will commit themselves to post-ip cooperation, then this new deal type is
possible. Agents could then negotiate over deals that are pairs of mixed joint plans. We
will call these new deals multi-plan deals. By committing to post-ip cooperation, agents
enlarge the space of agreements, and this potentially improves their expected utility.

Definition 13

 A Multi-Plan Deal is (1; 2; q); where each  is a mixed joint plan that moves the
world to a state that satisfies i's goal. 0  q  1 2 IR is the probability that the agents
will perform 1 (they will perform 2 with probability 1 , q ).
 Assuming j is i's opponent, we have Utility (1; 2; q) = q(w , Cost ( )) , (1 ,
i

i

q )Cost ( ).
i

i

i

i

j

So a multi-plan deal has agents agreeing on two joint plans, and deciding which to execute by tossing a coin. This deal can be visualized informally in Figure 21, as in Section 4.4
above. A triple line represents a joint plan, carried out by multiple agents.
Note that here the symmetric abilities assumption from Section 2.4 may not be essential
(i.e., with the multi-plan deal type agents may not need to be able to perform the same
199

fiZlotkin & Rosenschein

u  
  
 

s

-

q

G1

?

G2

Figure 21: Multi-Plan Deal

plans at equivalent cost). The two mixed joint plans that comprise a multi-plan deal might
be pure (i.e., p can be 0 or 1) without overly restricting the agents' ability to divide the
utility accurately, since the agents have the additional q probability that they can adjust.
Just as semi-cooperative deals can be used in cooperative situations, multi-plan deals can
also be used in cooperative situations (since, as we will see below, they are a generalization
of semi-cooperative deals). All that is needed is to enhance the definition of multi-plan deal
utility appropriately, as was done for semi-cooperative deals (Definition 12).
Consider the following example, which shows the increased utility available for the agents
to share when they negotiate over multi-plan deals instead of over mixed joint plans.

2

1
1

1

2

2
3
1

2

3

Multi-plan deal
1

2

1

2

3

Figure 22: Relationship of the Multi-Plan Deal Type to Mixed Joint Plans
The initial state s of the world can be seen in Figure 22. A1 's goal is to swap the position
of the blocks in slot 3, but to leave the blocks in slot 2 in their initial position (there is only
one state that satisfies this goal; call it f1 ). A2 's goal is to swap the position of the blocks
in slot 2, but to leave the blocks in slot 3 in their initial position (f2 ).
To achieve his goal alone, each agent needs to do at least 8 pickup or putdown operations
(each with a cost of 1). Assume that A1 's worth function assigns 10 to f1 and 0 to all other
states, and that A2 's worth function assigns 10 to f2 and 0 to all other states. In this case,
200

fiMechanisms for Automated Negotiation

the negotiation set includes the deals (s ! f1 ; ): 1 and (; s ! f2 ): 0. Using the protocol
mentioned above, the agents will break the symmetry of this situation by ipping a coin.
The utility of each agent from this deal is 1 = 12 (10 , 8).
Negotiation over the multi-plan deal type will cause the agents to agree on (1 ; 2): 12 ,
where  is the mixed joint plan in which both agents cooperatively achieve i's goal. The
best joint plan for doing the swap in one of the slots costs 2 pickup/putdown operations
for each agent. The utility for each agent from this deal is 3 = ( 12 (10 , 2) + 21 (,2)). By
negotiating using the multi-plan deal type instead of mixed joint plans, there is more utility
for the agents to share, 6 instead of 2.
i

5.6 The Hierarchy of Deal Types | Summary

There exists an ordering relationship among the various kinds of deals between agents that
we have considered; we call this relationship the \deal hierarchy." At the bottom of the
hierarchy are pure deals and mixed deals. These first two types of deals in the hierarchy can
be used only in cooperative situations. For negotiation in general non-cooperative domains,
additional types of deals were needed.
Next in the hierarchy come semi-cooperative deals. As we have shown, semi-cooperative
deals are a superset of mixed deals. Even in cooperative situations, there may be some
semi-cooperative deals that do not achieve all goals, but which dominate all other mixed
joint plans that do achieve both agents' goals.
Finally, at the top of the hierarchy, come multi-plan deals, which are a superset of semicooperative deals. This is the most general deal type in our deal hierarchy. This deal type
can also serve as the foundation for a class of Unified Negotiation Protocols.
In summary, our hierarchy looks as follows:

fJ g  fJ : pg  ft; ; qg  f(1; 2): qg
Pure Deals  Mixed Deals  Semi-Cooperative Deals  Multi-Plan Deals

6. Unbounded Worth of a Goal|Tidy Agents

In Section 4.3, we assumed that each agent assigns a finite worth to achieving his goal,
which is the upper bound on cost that he is willing to spend to achieve the goal. What if
such an upper bound does not exist? There may be situations and domains in which there is
no limit to the cost that an agent is willing to pay in order to achieve his goal|he would be
willing to pay any cost. Similarly, there may be situations when an agent is simply unable,
by design, to evaluate the worth of its goal. However, even though the worth is unbounded
or unevaluable, the agent is still interested in expending the minimum necessary to achieve
its goal. The agent gets more utility when it spends less, and can determine an ordinal
ranking over all possible deals, even though it has diculty assigning cardinal values to the
utility derived from those deals.
Nevertheless, we really are interested in having cardinal values that can be used in our
negotiation mechanisms. Our whole approach to negotiation is founded on the existence of
these inter-agent comparable cardinal utility functions. When worth is unbounded for both
agents, we seem to be deprived of the tool on which we have depended.
201

fiZlotkin & Rosenschein

We would like to identify a different baseline by which to define the concept of utility.
Originally, above, we used the baseline of \stand-alone cost," c(s ! G ), taking the utility
of a deal for an agent to be the difference between the cost of achieving the goal alone and
the agent's part of the deal. Then, we used the baseline of \worth" in a similar manner,
linearly transforming the utility calculation. Utility of a deal for an agent was then the
difference between the maximum cost he was willing to pay and the agent's part of the
deal. When worth is unbounded, however, that linear transformation obviously cannot be
used.
In other work (Zlotkin & Rosenschein, 1993b), we present an alternative baseline that
can satisfy our desire for symmetry, fairness, simplicity, stability, and eciency. It turns
out to constitute the minimum sucient baseline for agents to reach agreements.
The minimum cost that an agent must offer to bear in a compromise encounter, where
neither agent has an upper bound on its worth, is that which leaves the other agent with
less cost than the latter's stand-alone cost. In other words, the first agent will offer to
\clean up after himself," to carry out a sucient portion of the joint plan that achieves
both goals such that the other agent's remaining part of the joint plan will cost him less
than his stand-alone cost. We call an agent who is willing to clean up after himself a tidy
agent; the formal definition appears elsewhere (Zlotkin & Rosenschein, 1993b). It is shown
that in any joint-goal reachable encounter (i.e., there exists a joint plan that achieves both
agents' goals), if both agents are tidy, the negotiation set is not empty.
i

7. Negotiation with Incomplete Information
All the mechanisms considered in the sections above can be straightforwardly implemented
only if both agents have full information about each other's goals and worths. In many
situations, this won't be the case, and in this section we will examine what happens to our
negotiating mechanisms in State Oriented Domains when agents don't necessarily have full
information about each other.
We consider incomplete information about goals, and incomplete information about
worths, as two separate issues. An agent, for example, might have particular information
about worth, but not about goals, or vice versa. There are thus four possible cases, where
worths are known or not known, combined with goals that are known or not known. In
previous sections, we considered the case where both goals and worth were known. In this
section we consider two of the other three situations, where neither goals nor worth are
known, and where goals are known and worth is not. We do not analyze situations where
worth is known but the goals are not.
The general conclusion is that a strategic player can gain benefit by pretending that its
worth is lower than it actually is. This can be done directly, by declaring low worth (in
certain mechanisms), or by declaring a cheaper goal (in the case where stand-alone cost is
taken to be the implicit worth baseline).
In this first section, we consider the space of lies that are available in different types of
interactions, and with different types of mechanisms.
There are several frameworks for dealing with incomplete information, such as incremental goal recognition techniques (Allen, Kautz, Pelavin, & Tenenberg, 1991), but the
framework we explore here is that of a \,1 negotiation phase" in which agents simultane202

fiMechanisms for Automated Negotiation

ously declare private information before beginning the negotiation (this was also introduced
elsewhere (Zlotkin & Rosenschein, 1989, 1993a) for the case of TODs). The negotiation then
proceeds as if the revealed information were true. In the TOD case, we have analyzed the
strategy that an agent should adopt for playing the extended negotiation game, and in
particular, whether the agent can benefit by declaring something other than his true goal.
Here, we will take a similar approach, and consider the ,1-phase game in State Oriented
Domains. Will agents benefit by lying about their private information? What kinds of
mechanisms can be devised that will give agents a compelling incentive to only tell the
truth?5
A negotiation mechanism that gives agents a compelling incentive to only tell the truth
is called (in game theory) incentive compatible. Although we are able to construct an
incentive compatible mechanism to be used when worths are unknown, we are unable to
construct such a mechanism in State Oriented Domains to be used when the other's goals
are unknown.

7.1 Worth of a Goal and its Role in Lies
We again assume that agents associate a worth with the achievement of a particular goal.
Sometimes, this worth is exactly equal to what it would cost the agent to achieve that goal
by himself. At other times, the worth of a goal to an agent exceeds the cost of the goal to
that agent. The worth of a goal is the baseline for calculating the utility of a deal for an
agent; in this section, we will always assume that worth is bounded.
The worth of a goal is intimately connected with what specific deals agents will agree on.
First, an agent will not agree on a deal that costs him more than his worth (he would have
negative utility from such a deal). Second, since agents will agree on a deal that maximizes
the product of their utilities, if an agent has a lower worth, it will ultimately reduce the
amount of work in his part of the deal. Thus, one might expect that if agent A1 wants to
do less work, he will try to fool agent A2 into thinking that, for any particular goal, A1's
worth is lower than it really is. This strategy, in fact, often turns out to be beneficial, as
seen below.
Let's consider the following example from the Slotted Blocks World.
The initial state can be seen at the left in Figure 23. G1 is \The Black block is on a
Gray block which is on the table at slot 2" and G2 is \The White block is on a Gray block
which is on the table at slot 1".
To achieve his goal alone, each agent has to execute four PickUp and four PutDown
operations that cost (in total) 8. The two goals do not contradict each other, because there
exists a state in the world that satisfies them both. There also exists a joint plan that moves
the world from the initial state to a state that satisfies both goals with total cost of 8|one
agent lifts the black block, while the other agent rearranges the other blocks suitably (by
picking up and putting down each block once), whereupon the black block is put down. The
agents will agree to split this joint plan with probability 12 , leaving each with an expected
utility of 4.
5. Some of these issues, in everyday human contexts, are explored in (Bok, 1978). Our immediate motivation
for discouraging lies among agents is so that our negotiation mechanisms will be ecient.

203

fiZlotkin & Rosenschein

Initial State

1

2

A1s
goal
3

1

2

3

Joint plan

1

A2s
goal
1

2

2

3

1

2

3

Figure 23: Agents Work Together Equally

7.2 Beneficial Lies with Mixed Deals

What if agent A1 lies about his true goal above, claiming that he wants a black block on
any other block at slot 2? See Figure 24. If agent A1 were alone in the world, he could
apparently satisfy this relaxed goal at cost 2. Assuming that agent A2 reveals his true goal,
the agents can only agree on one plan: agent A1 will lift a block (either the white or black
one), while agent A2 does all the rest of the work. The apparent utility for agent A1 is then
0 (still individual rational), while agent A2 has a utility of 2. In reality, agent A1 has an
actual utility of 6. Agent A1 's lie has benefited him.

...
1

2

3

1

1

2

2

3

3

1

2

1

2

3

Figure 24: Agent A1 Relaxes his Goal
This works because agent A1 is able to reduce the apparent cost of his carrying out his
goal alone (which ultimately causes him to carry less of a burden in the final plan), while
not compromising the ultimate achievement of his real goal. The reason his real goal is
204

fiMechanisms for Automated Negotiation

\accidentally" satisfied is because there is only one state that satisfies agent A2 's real goal
and agent A1's apparent goal, coincidentally the same state that satisfies both of their real
goals.
The lie above is not agent A1 's only beneficial lie in this example. What if agent A1
claimed that his goal is \Slot 3 is empty and the Black block is clear"? See Figure 25.
Interestingly, this goal is quite different from his real goal. If agent A1 were alone in the
world, he could apparently satisfy this variant goal at cost 4. The agents will then be forced
again to agree on the deal above: A1 does two operations, with apparent utility of 2, and
agent A2 does six operations, with utility of 2. Again, agent A1's actual utility is 6.


1

2

3

1



2

1

2

3

3

1

2

1

2

3

Figure 25: Agent A1 Makes Up an Entirely New Goal
In Task Oriented Domains (Zlotkin & Rosenschein, 1989, 1993a), we also saw something
similar to this lying about a goal. There, for example, an agent could hide a task, and lower
the apparent cost of its stand-alone plan. Similarly, in the first lie above the agent in the
Blocks World relaxed his true goal, and lowered the apparent cost of his stand-alone plan
(and thus of his worth). The set of states that will satisfy his relaxed goal is then a superset
of the set of states satisfying his true goal.
However, there is a major difference between lying in SODs and lying in TODs: in the
latter, there can never be any \accidental" achievement of hidden goals. The lying agent
will always find it necessary to carry out the hidden goal by himself, and this is the main
reason why in subadditive TODs hiding goals is not beneficial. In SODs, a hidden goal
might be achieved by one's opponent, who carries out actions that have side effects. Thus,
even when you hide your goal, you may fortuitously find your goal satisfied in front of your
eyes.
This situation can be visualized informally in Figure 26, as other SOD interactions were
in Section 4.4 above. In the figure, agent A1 's expanded apparent goal states are represented
by the thicker oval and labeled G01. Note that the expansion of the goal states is toward
the initial state s. This is the meaning of lowering one's apparent cost, and is necessary for
a beneficial lie.
205

fiZlotkin & Rosenschein

s

u  
 
 

@
@@
@@@
@@R

G01
G1

G2

Figure 26: Expanding Apparent Goal States with a Lie
Alternatively, the agent can manufacture a totally different goal for the purposes of
reducing his apparent cost, as we saw in Figure 25. Agent A1 did this when he said he
wanted slot 3 empty and the Black block clear. Consider Figure 27, where agent A1's
altered apparent goal states are again represented by the thick outline and labeled G01.
Note again, that the expansion of the goal states is toward the initial state s.

s

u 
 
 

@
@@
@@@@
R

G01

G1

G2

Figure 27: Altering Apparent Goal States with a Lie
The agent then needs to make sure that the intersection of his apparent goal states and
his true goal states is not empty. Although this is a necessary precondition for a successful
lie, it is of course not a sucient precondition for a successful lie. Both of the lies in the
above example will be useful to agent A1 regardless of the negotiation protocol that is being
used: pure deal, mixed deal, semi-cooperative deal, or multi-plan deal.

7.3 Beneficial Lies with Semi-Cooperative Deals

It might seem that when agents are in a conict situation, the potential for beneficial lies
is reduced. In fact, beneficial lying can exist in conict situations.
\Conict" between agents' goals means that there does not exist a mixed joint plan
that achieves both goals and is also individual rational. This is either because such a state
does not exist, or because the joint plan is too costly to be individual rational. Even when
conict exists between goals, there might be common subgoals, and therefore a beneficial
lie may exist.
206

fiMechanisms for Automated Negotiation

Taking Advantage of a Common Subgoal in a Conict Situation: Let the initial

state of the world be as in Figure 28. One agent wants the block currently in slot 2 to be
in slot 1; the other agent wants it to be in slot 3. In addition, both agents share the goal of
swapping the two blocks currently in slot 4 (i.e., reverse the stack's order).
The cost for an agent of achieving his goal alone is 10. Negotiating over the true goals
using semi-cooperative deals would lead the agents to agree to do the swap cooperatively
(at cost of 2 each), and then ip a coin, with a weighting of 12 , to decide whose goal
will be individually satisfied. This deal brings them an overall expected utility of 2 (i.e.,
1
2 (10 , 2) , 2).

1

2

3

..

4

1

2

3

2

. ..

.

1

2

3

4

4

1

1

2

3

4

Figure 28: Taking Advantage of a Common Subgoal
What if agent A1 lies and tells agent A2 that his goal is: \The Black block is clear at
slot 1 and the White block is on the Gray block"? Agent A1 thus hides the fact that his
real goal has the stack of blocks at slot 4, and claims that he does not really care if the
stack is at slot 2, 3 or 4. The cost for agent A1 of achieving his apparent goal is 6, because
now he can supposedly build the reversed stack at slot 3 with a cost of 4. Assuming that
agent A2 reveals his true goal, the agents will still agree to do the swap cooperatively, but
now the weighting of the coin will be 47 . This deal would give agent A1 an apparent utility
of 1 37 (i.e., 47 (8 , 2) , 2) which is also A2 's real utility (i.e., 73 (10 , 2) , 2). A1's real utility,
however, is 2 74 = 47 (10 , 2) , 2. This lie is beneficial for A1 .
The situation is illustrated in Figure 29, where agent A1 's lie modifies his apparent goal
states so that they are closer to the initial state, but the plan still ends up bringing the
world to one of his real goal states.
In the example above, the existence of a common subgoal between the agents allowed
one agent to exploit the common subgoals (assuming, of course, that the lying agent knew
its opponent's goals). The lying agent relaxes his true goal by claiming that the common
subgoal is mainly its opponent's demand|as far as he is concerned (he claims), he would
be satisfied with a much cheaper subgoal. If it is really necessary to achieve the expensive
subgoal (he claims), more of the burden must fall on his opponent.
207

fiZlotkin & Rosenschein




u
u



 
 

s

G0

@@
@@@R t

a

- G1

q

?

G2

Figure 29: Lying in a Conict Situation

One might think that in the absence of such a common subgoal, there would be no
opportunity for one agent to beneficially lie to the other. This, however, is not true, as we
see below.

7.4 Beneficial Lies with Multi-Plan Deals
Another Example of Beneficial Lying in a Conict Situation: The initial state s

can be seen in Figure 30, similar to the example used in Section 5.5 above. A1 's goal is
to reverse the blocks in slot 2, and to leave the blocks in slot 1 in their initial position.
A2 's goal is to reverse the blocks in slot 1, and to leave the blocks in slot 2 in their initial
position. To achieve his goal alone, each agent needs to do at least 8 PickUp/PutDown
operations. This is a conict situation.

Or

1

2

3

1

2

3

1
1

2

2

3

3

2

1

1

2

3

Figure 30: Example of Interference Decoy Lie
Negotiation over multi-plan deals will cause the agents to agree on (1 ; 2): 21 , where 
is the mixed joint plan in which both agents cooperatively achieve i's goal. The best joint
plan for doing the reverse in either one of the slots costs 2 PickUp/PutDown operations for
each agent. Each agent's utility from this deal is 2 = ( 12 (8 , 2) , 21 (2)).
i

208

fiMechanisms for Automated Negotiation

Agent A1 might lie and claim that his goal is to reverse the blocks in slot 1 and leave the
blocks in slot 2 in their initial position (his real goal) OR to have the white block be alone
in slot 2. It costs A1 6 to achieve his apparent goal alone. To do the reverse alone would
cost him 8, and thus to achieve the imaginary part of his goal is cheaper. The agreement
will be (1; 2 ): 47 , where  is again the mixed joint plan in which both agents cooperatively
achieve i's goal. It turns out to be cheaper for both agents to cooperatively carry out
A1's real goal than it is to cope with A1 's imaginary alternative. A1's apparent utility will
be 1 37 = 47 (6 , 2) , 37 (2). This is also A2 's utility. A1 's actual utility, however, will be
2 47 = 47 (8 , 2) , 73 (2), which is greater than the unvarnished utility of 2 that A1 would get
without lying. So even without a common subgoal, A1 had a beneficial lie. Here we have
been introduced to a new type of lie, a kind of \interference decoy," that can be used even
when the agents' have no common subgoals.
i

8. Incomplete Information about Worth of Goals
Consider the situation where two agents encounter one another in a shared environment.
Their individual goals are commonly known (because of prior knowledge about the type of
agent, some goal recognition process, etc.), as well as the cost of achieving those goals, were
each agent to be alone in the world. In addition, there is no conict between these goals.
There exists some non-empty set of states that satisfies both agents' goals.
The agents have upper bounds on their worth, but (in contrast to the public goals)
each upper bound is private information, not known to the other agent. This is a common
situation; as agents queue up to access a common resource, their goals will often be selfevident. For example, two agents approaching a narrow bridge from opposite ends may
know that the other wants to cross, but not know what the crossing is worth for the other
(e.g., how long it is willing to wait). The agents need to agree on a deal (for example, who
will go first, and who will wait).
One simple way to design a negotiation mechanism that handles the lack of information
is to have agents exchange private information prior to the actual negotiation process. This
pre-negotiation exchange of information is another variant on the ,1-phase game mentioned
above. In the current case, agents exchange private information about worth. In this section,
we only consider the situation where agents are negotiating over mixed joint plans.
One question, then, is how should agents play this ,1-phase game to best advantage?
As was mentioned above in Sections 4.4 and 7.2, an agent generally has an incentive to
misrepresent the worth of his goal by lowering it|the less an agent is willing to pay, the
less it will have to pay in a utility product maximizing mechanism (PMM). However, if
everyone lowers their worth they may not be able to reach any agreement at all, whereas
if they declared their true worth agreement would have been reached. Agents might lower
their worth too much and be driven to an inecient outcome. This is an instance of the
free rider problem. Every agent is individually motivated to lower his worth, and have
someone else carry more of the burden. The group as a whole stands to suffer, particularly
if agreements are not reached when they otherwise would have been.
We can exert control over this tendency to lower one's apparent worth by careful design
of the post-exchange part of the negotiation mechanism. We are interested in designing a
mechanism that satisfies our desire for ecient, symmetric, simple, and stable outcomes. In
209

fiZlotkin & Rosenschein

our research on TODs, we managed (in certain cases) to provide a post-exchange mechanism
that satisfied all these attributes, and was also found to be incentive compatible|the agents'
best strategy was to declare their true goals. In this section, we introduce two mechanisms
for private-worth SODs, one \strict," the other \tolerant," and analyze their affects on the
stability and eciency of negotiation outcomes. The strict mechanism turns out to be more
stable, while the tolerant mechanism is more ecient.

8.1 Strict and Tolerant Mechanisms

There are several cases that need to be addressed by any mechanism, and can be treated
differently by different mechanisms. For example, what should happen if one agent declares
his worth as being lower than his stand-alone cost (i.e., apparently he would not achieve
his goal were he to be alone, it is not worth it to him)? Should the other agent then still
be allowed to offer a deal, or is the negotiation considered to have failed at this point?
Both mechanisms that we present below start the same way. The agents simultaneously
declare a worth value, claimed to be the worth they assign to the achievement of their goal.

 Both goals are apparently achievable alone: If both agents declare a worth

greater than their stand-alone cost (which is commonly known), the negotiation proceeds as if the worth declarations were true. The agents then use some product
maximizing mechanism over the negotiation set of mixed joint plans, with their declared worths as the baseline of the utility calculations. The result will be an equal
division of the apparent available utility between them.
 Only one agent's goal is apparently achievable alone: If one agent declares a
worth greater than stand-alone cost, and the other doesn't, then the former agent is
free to decide what to do. He can either propose a take-it-or-leave-it deal to the other
agent (if it's refused, he'll carry out his own goal alone), or he can simply bypass the
offer and just carry out his own goal. Since his declared worth is greater than his
stand-alone cost, it is rational for him to accomplish his goal by himself.
 Both agents' goals are apparently unachievable alone: If both agents declare
worths lower than their stand-alone costs, our two mechanisms differ as to how the
situation is handled:

{ Strict Mechanism: There is a conict, and no actions are carried out. The
{

agents derive the utility of the conict deal.
Tolerant Mechanism: The agents continue in their negotiation as in the first
case above (i.e., they use mixed joint plans, and divide the apparent available
utility between them). Even though both agents claim to be unwilling to achieve
their goals alone, it may certainly be the case that together, they can carry out
a rational joint plan for achieving both of their goals.

The tolerant mechanism gives the agents a \second chance" to complete the negotiation
successfully and reach a rational agreement, whereas the strict mechanism does not forgive
their low worth declarations, and \punishes" them both by causing a conict. Of course, if
both agents' true worths are really lower than their stand-alone costs, the strict mechanism
210

fiMechanisms for Automated Negotiation

causes an unnecessary failure (and is thus inecient), while the tolerant mechanism still
allows them to reach a deal when it is possible. We will see below, however, that tolerance
can sometimes lead to instability.
Our approach through the rest of this section will be to consider the various relationships
among the two agents' worth values, their cost values, the interaction types, and the joint
plans that achieve both agents' goals. For each such relationship, we'll analyze the strategies
available to the agents. As mentioned above, we are here only considering situations where
both agents' goals are achievable by two-agent mixed joint plans (e.g., there are reachable
states that satisfy both agents' goals).
The idea of tidy agents and an agent cleaning up after himself, introduced above in
Section 6, was used in situations where agents were willing to pay any price to achieve their
goals|their worths were unbounded. There, worth could not be used as a baseline for the
utility calculation. Instead, we found that there was a \minimal sucient" value to the
utility baseline that gave rise to an ecient and fair mechanism. A similar idea will also
be useful in our analysis below. The tidy agent baseline, explored above, also serves as a
minimal sucient declaration point when worths are private information.

8.2 The Variables of Interest
In general, an agent would like to declare as low a worth as possible, but without risking a
conict. The lower the declaration of worth, the smaller his share of the joint plan will be.
Unfortunately for the agent, if his declared worth is too low, it may eliminate the possibility
of reaching an agreement. A necessary and sucient condition for the negotiation set not
to be empty is that the sum and min conditions, from Section 4.1, will hold (given the
declarations of worth). Since we assume that there is a joint plan that achieves both
agents' goals, agreement will still be possible if among those plans there is at least one that
satisfies the sum and min conditions.
There are several variables that will play a role in our analysis below. First, each agent
i has a stand-alone cost (known to all, and dependent only on his goal), denoted by c .
Second, each agent has a true worth (privately known) that he assigns to the achievement
of his goal, denoted by w . T is the total cost of the minimal (total cost) joint plan that
achieves both agents' goals. M is the cost of the minimal role among all such joint plans
of cost T . Below, we will analyze all possible configurations of these variables.
The analysis is presented according to each interaction type other than conict, i.e., symmetric cooperative, non-symmetric cooperative/compromise, and symmetric compromise.
For each type, we will consider three subcases that depend on the relationships between c ,
w , T , and M .
i

i

r

i

i

r

8.3 Symmetric Cooperative Situation
In symmetric cooperative situations, one strategy that an agent can use is to declare as his
worth the minimum between his true worth, and the maximum of his stand-alone cost and
the minimal role in the joint plan:
Min-Sucient Strategy  min(w ; max(c ; M )):
i

211

i

r

fiZlotkin & Rosenschein

The motivation here is that the agent wants to declare the minimal worth sucient for
there to be an agreement. Declaring c satisfies the sum condition, but to make sure that
it also satisfies the min condition, the agent must declare max(c ; M ). To make sure that
this declaration is individual rational, he must not make a declaration greater than his true
worth, w ; thus, he takes the minimum between w and the (c ; M ) maximum.
The Min-Sucient Strategy is only one possible strategy that might be adopted. However, if both agents adopt it, the strategy is in equilibrium with itself (in most cases), and
agreement is guaranteed. We will analyze the characteristics of this strategy below in six
cases.
i

i

i

i

i

r

r

8.3.1 Both Goals are Achievable Alone

In this situation (as shown in Figure 31), both agents would be able to achieve positive
utility if the other agent were not around, and they achieved their stand-alone goal by
themselves.
T
Equilibrium Point
W1

conflict
C1

A1 decides

Mr

Negotiation
A2 decides
Mr

C2

T W2

Figure 31: Both Goals are Achievable Alone
The diagram in Figure 31 describes, in a sense, the game in normal form. Each agent
can declare as its worth any number from 0 to infinity. The outcome depends on the two
numbers declared; every point in the plain is a possible result. The colors of the regions
denote the types of outcomes.
Note, for example, that if agent A1 declares less than c1 , while agent A2 declares more
than c2 , the outcome is that A2 will decide what to do (offering A1 a take-it-or-leave-it deal,
or going it alone). If A1 and A2 both offer too little (so that the sum is less than T ), they
will reach conict. Because we assume the agents are rational, we only consider the areas
in the plain framed by w1 and w2 (rational agents would not declare a worth greater than
their true worths).
The difference between the Strict and Tolerant mechanisms mentioned above is the color
of the triangle to the lower left of the c1=c2 point. With the Strict mechanism, it would be
white (conict), while with the Tolerant mechanism it is still a region that allows subsequent
negotiation to occur. The only point that is in equilibrium in both mechanisms is the c1 =c2
point, which is reached by the Min-Sucient Strategy given above. Thus, that strategy is
both stable and ecient in both Strict and Tolerant mechanisms in this situation.
212

fiMechanisms for Automated Negotiation

8.3.2 One Goal is Achievable Alone

Assume that we have the situation shown in Figure 32, where only one agent would be able
to achieve positive utility if the other agent were not around (though they both ultimately
can benefit from the other's existence, one more than the other).
T

Equilibrium Point

W1
conflict
C1

A1 decides

Mr

Negotiation
A2 decides
Mr

W2 C2

T

Figure 32: One Goal is Achievable Alone
We have a phenomenon similar to that in Section 8.3.1. The negotiation triangle to the
lower left of c1 =w2 will be white (conict) in the Strict mechanism and negotiable in the
Tolerant mechanism. In both mechanisms, the c1=w2 point is in equilibrium, which is the
point that results if both agents play the Min-Sucient Strategy. Again, that strategy is
both stable and ecient in both Strict and Tolerant mechanisms in this situation.
8.3.3 Both Goals are Not Achievable Alone

Now consider the situation as shown in Figure 33, where neither agent could achieve positive
utility were it alone in the world|the only way to achieve their goals is by cooperating.
T
Resulting
Non-equilibrium Point

conflict
C1
W1

A1 decides

Mr

Negotiation
A2 decides
Mr

W2 C2

T

Figure 33: Both Goals are Not Achievable Alone
Again, the negotiation triangle to the lower left of w1=w2 will be white (conict) in the
Strict mechanism, and no agreement can be reached in any situation (the whole plain is, in
fact, white). Though the Min-Sucient Strategy is not ecient with the Strict mechanism,
213

fiZlotkin & Rosenschein

it is stable. With the Tolerant mechanism, the Min-Sucient Strategy is ecient (it results
in the w1=w2 point), but it unfortunately is not stable|assuming that one agent declares
w1, the other agent can benefit by declaring T , w1 instead of w2 .
In fact, the agents do not actually know what situation they are in (the one in Figure 32
or Figure 33), so guaranteed beneficial divergence from the equilibrium point would really
require total knowledge of the situation and what your opponent is playing. Thus, although
the Min-Sucient Strategy is not stable, agents may be unlikely to diverge because of
real-world constraints on their knowledge.

8.4 Non-Symmetric Cooperative/Compromise Situation
In this section we continue our analysis into situations where for one agent, the situation is
cooperative, while for the other, it is a compromise situation. We will continue to analyze
the case where both agents use the Min-Sucient Strategy. Agreement can only be reached
when the compromising agent contributes more than his stand-alone cost to the joint plan;
this is because the minimal role is greater than his stand-alone cost. The only way for
agents to reach an agreement is when the compromising agent is willing to do more than
its stand-alone cost|otherwise, there will be a conict.
8.4.1 Compromise is Sufficient

In the situation described by Figure 34, the true worth for the compromising agent (w2) is
greater than both the minimal role and c2.
Equilibrium Point
T
W1
C1

conflict
A1 decides
Mr

Negotiation
A2 decides
C2 Mr

W2

T

Figure 34: Compromise is Sucient
It is sucient for the compromising agent to declare M as his true worth. If he declared
less than that, and the other agent declared more than c1, they can reach conict; by
declaring M , he guarantees that his goal will be achieved. The diagram is the same for
both the Strict and Tolerant Mechanisms. The Min-Sucient Strategy brings both agents
to the c1=M point, which is both a stable and ecient result (for both mechanisms).
r

r

r

214

fiMechanisms for Automated Negotiation

8.4.2 Can Compromise, But Not Enough

Consider the situation, portrayed in Figure 35, where w2 is less than M , and it is not
rational for agent A2 to compromise and declare a worth greater than w2. The MinSucient Strategy brings the agents to the c1=w2 point, which is a conict.
r

T
W1
C1

conflict
A1 decides
Negotiation

Mr

A2 decides
C2 W2 Mr

T

Figure 35: Can Compromise, But Not Enough
The picture is identical for both the Strict and Tolerant mechanisms. If the agents
use the Min-Sucient Strategy, the resulting point (c1=w2) is not ecient, even though
it is stable.6 However, if we enhanced the mechanism with conict-resolution techniques,
and allowed the agents to negotiate over multi-plan deals from Section 5.5 (or even semicooperative deals from Section 5.3), we conjecture that the result c1 =w2 would be both
stable and ecient. That enhancement, however, is beyond the scope of the work described
in this paper.
8.4.3 No Reason to Compromise

In the situation shown in Figure 36, the non-compromising agent A1 cannot achieve his
goal alone. The Min-Sucient Strategy will have him declare something less than c1 (either
w1 or M ), and the result will be that agent A2 will have the option to decide on what to
do|and the only reasonable decision will be for A2 to achieve his goal alone (there is no
reason to compromise).
This result is both ecient and stable, in both the Strict and Tolerant mechanisms.
r

8.5 Symmetric Compromise Situation
In this section we continue our analysis into situations where for both agents, they are in
a compromise situation. Both agents will have to do more than their stand-alone costs in
order to achieve both goals.
6. Conict is not ecient because the result in which one agent achieves his goal, rather than both agents
doing nothing, would be more ecient.

215

fiZlotkin & Rosenschein

T

Equilibrium Point

C1
W1

conflict
A1 decides
Mr
Negotiation
A2 decides
C2

Mr

W2

T

Figure 36: No Reason to Compromise
In this section, we will propose another strategy that agents could use, namely the
Min-Concession Strategy:
Min-Concession Strategy  min(w ; (c + T , (c21 + c2 ) )):
i

i

In this situation, the agent is choosing to propose (as his true worth) more than his
stand-alone cost, to ensure that an agreement can be reached. However, he would like to
propose the minimal sucient concession, just enough to enable an agreement. The MinConcession Strategy has both agents make the same concession. The overall strategy that we
are analyzing (and that covers all cases in this section) is to use the Min-Concession Strategy
in symmetric compromise situations, but otherwise to use the Min-Sucient Strategy (as
presented above). Agents know which kind of situation they are in (and thus what strategy
to use) because stand-alone costs are common knowledge.
8.5.1 Agents Can Compromise Equally

If agents are in a situation where both can compromise equally (as shown in Figure 37),
and they both use the Min-Concession Strategy, they end up at the point (c1 +)=(c2 +)
(where  = (T , c1 , c2)=2). This point is both ecient and stable, under both the Strict
and Tolerant mechanisms.
8.5.2 Non-Symmetric Compromise, but Goals can be Achieved

If agents are in a symmetric compromise situation, but one in which one agent needs to
compromise more than the other (as in Figure 38), the use of the Min-Concession Strategy
results in the point (c1 +)=w2. This point is a conict, and is unfortunately neither stable
nor ecient.
The result is not stable because A1 could make a greater compromise and benefit from
it. The result is not ecient because even if only one agent could achieve its goal, that
would be superior to the conict outcome. It is not dicult to imagine other strategies that
would lead the agents to ecient solutions (e.g., declare T , c for each agent i, as a Tidy
Agent would do in Section 6), but they would not be stable, either. On the other hand, if
j

216

fiMechanisms for Automated Negotiation

T

Equilibrium Point

W1

conflict
C1+

A1 decides

C1
Negotiation

Mr

A2 decides
Mr

C2 C2+

W2 T

Figure 37: Agents Can Compromise Equally

T

W1

conflict
C1+
C1

A1 decides

Mr

Negotiation
A2 decides
Mr

C2 W2 C2+ 

T

Figure 38: Non-Symmetric Compromise, but Goals can be Achieved

217

fiZlotkin & Rosenschein

the negotiation mechanism is enhanced with conict-resolution techniques (such as multiplan deals or semi-cooperative deals), we conjecture that the Min-Concession Strategy will
be both stable and ecient. This enhancement, however, is also beyond the scope of the
work described in this paper.
8.5.3 One Agent Cannot Compromise

Consider the situation where one agent cannot compromise (because he could not even
achieve his own goal alone), shown in Figure 39. In this case, if both agents use the MinConcession Strategy, the result will be (c1 + )=w2. Agent 1 will choose to then achieve his
own goal alone (and not compromise). This outcome is both stable and ecient.
T
Equilibrium Point
W1

conflict
C1+
C1

A1 decides

Mr

Negotiation
A2 decides
Mr

W2 C2 C2+ 

T

Figure 39: One Agent Cannot Compromise

8.6 Summary of Strict and Tolerant Mechanisms

The results of the analysis above are summarized in Figure 40. The tradeoff between
eciency and stability is apparent only in the symmetric cooperative case, where neither
agent is able to achieve its goal alone. With the strict mechanism, a conict is caused simply
because each agent will not declare a worth higher than its stand-alone cost, and thus will
bring about immediate conict. The tolerant mechanism gives the agents a second chance
to reach agreement, but is unstable (as described above).
The above mechanism is not incentive compatible. The agents do not have an incentive
to declare their true worths; rather, they use the Min-Sucient Strategy to decide what
their optimal declaration is.

9. Related Work in Game Theory and DAI

In this section we review research in game theory and in distributed artificial intelligence
related to our own work.

9.1 Related Work in Game Theory

As mentioned at the beginning of this paper, our research relies heavily on existing game
theory tools that we use to design and evaluate protocols for automated agents. Here, we
218

fiMechanisms for Automated Negotiation

Strict
Efficient

Stable

Tolerant
Efficient

Stable

Symmetric Cooperation
Both boals are achievable alone
One goal is achievable alone
Both goals arent achievable alone

Non-Symmetric Cooperation/Compromise
Compromise is sufficient
Compromise is insufficient
No reason to compromise

Symmetric Compromise
Agents can compromise equally
Agents cant compromise equally
One agent cant compromise

Figure 40: Summary of Strict and Tolerant Mechanisms
review the game theory work on Bargaining Theory, Mechanism Design and Implementation
Theory, and Correlated Equilibria.
9.1.1 Bargaining Theory

Classic game theory (Nash, 1950; Zeuthen, 1930; Harsanyi, 1956; Roth, 1979; Luce & Raiffa,
1957) talks about players reaching \deals," which are defined as vectors of utilities (one for
each player). A bargaining game can end up in some possible outcome (i.e., a \deal"). Each
player has a full preference order over the set of possible outcomes; this preference order
is expressed by his utility function. For each deal, there is a utility vector which is the
list of the utilities of this deal for every participant. There is a special utility vector called
\conict" (or sometimes the \status quo point") which is the utility each player assigns to
a conict (that is, lack of a final agreement). Classic game theory deals with the following
question: given a set of utility vectors, what will be the utility vector that the players
will agree on (under particular assumptions)? In other words, classic bargaining theory is
focused on prediction of outcomes, under certain assumptions about the players and the
outcomes themselves.
Nash (Nash, 1950, 1953) showed that under some rational behavior assumptions (i.e.,
individual rational and pareto optimal behavior), and symmetry assumptions, players will
219

fiZlotkin & Rosenschein

reach an agreement on a deal that maximizes the product of the players' utility (see Section 4.2 for a more complete discussion).
An alternative approach to negotiation, which looks upon it as a dynamic, iterative process, is discussed in the work of Rubinstein and Osborne (Rubinstein, 1982, 1985; Osborne
& Rubinstein, 1990).
Game theory work on negotiation assumes that the negotiation game itself is welldefined. It assumes that there is a set of possible deals that the players are evaluating using
certain utility functions. Therefore, the deals and the players' utility functions induce a set
of utility vectors that forms the basis for the negotiation game.
In contrast to this analysis of a given, well-defined negotiation encounter, we are exploring the design space of negotiation games. Given a multiagent encounter (involving, for
example, task redistribution), we design an assortment of negotiation games, by formulating
various sets of possible deals and various kinds of utility functions the agents may have.
For any given negotiation game, we then use the above game theory approaches to analyze
it and to evaluate the negotiation mechanisms we propose.
Game theorists are usually concerned with how games will be played, from both a
descriptive and normative point of view. Ours is essentially a constructive point of view;
since game theory tells us, for any given game, how it will be played, we endeavor to design
games that have good properties when played as game theory predicts.
9.1.2 Equilibrium

Game solutions in game theory consist of strategies in equilibrium; if somehow a social
behavior reaches an equilibrium, no agent has any incentive to diverge from that equilibrium
behavior. That equilibrium is considered to be a solution to the game. There may be one
or more (or no) strategies in equilibrium, and there are also different notions of equilibrium
in the game theory literature.
Three levels of equilibrium that are commonly used in game theory are Nash equilibrium,
perfect equilibrium, and dominant equilibrium (Binmore, 1990; Rasmusen, 1989). Each level
of equilibrium enumerated above is stronger than the previous one. Two strategies S; T are
in Nash equilibrium if, assuming that one agent is using S , the other agent cannot do better
by using some strategy other than T , and vice versa. Perfect equilibrium means that when
the game has multiple steps, and one player is using S , there exists no state in the game
where the other player can do better by not sticking to his strategy T . There do exist
situations where strategies might be in Nash equilibrium, but not in perfect equilibrium;
in that case, although strategy T was best at the start of the game, as the game unfolds it
would be better to diverge from T . Dominant strategy equilibrium means that no matter
what strategy your opponent chooses, you cannot do better than play strategy T ; strategies
S and T are in dominant strategy equilibrium when S is the dominant strategy for one
player, and T is the dominant strategy for the other.
In our work, we generally use Nash equilibrium (the weakest equilibrium concept) as our
requirement of a solution; this provides us with the widest range of interaction solutions.
At times, because the solution is not inherently in perfect equilibrium, we have introduced
additional rules on the interaction, to compel agents to follow particular Nash equilibrium
220

fiMechanisms for Automated Negotiation

strategies as the game progresses (such as introducing a penalty mechanism for breaking a
public commitment).
This provides an interesting example of the power we wield as designers of the game.
First, we would normally require perfect equilibria in multiagent encounters, but we can
adopt Nash equilibria as sucient for our needs, then impose rules that keep agents from
deviating from their Nash equilibrium strategies. Second, the very strong requirement of
dominant equilibrium, which might be desirable when two arbitrary agents play a given
game, is not needed when the recommended strategies are commonly known|Nash equilibrium is then sucient.
9.1.3 Mechanism Design and Implementation Theory

There are also groups of game theorists who consider the problem of how to design games
that have certain attributes. It is this area of mechanism design that is closest to our own
concerns, as we design protocols for automated agents.
Mechanism design is also known in the game theory literature as the implementation
problem. The implementation question (Binmore, 1992; Fudenberg & Tirole, 1992) asks
whether there is a mechanism (also called a game form ) with a distinguishable equilibrium
point (dominant strategy, or perfect, or merely Nash) such that each social profile (i.e.,
group behavior) is associated, when the players follow their equilibrium strategies, with the
desired outcome.
In other words, there are assumed to be a group of agents, each with its own utility
function and preferences over possible social outcomes. There is also a social welfare function
that rates all those possible social outcomes (e.g., a socially ecient agreement may be
rated higher than a non-ecient one) (Arrow, 1963). The question is then, can one design
a game such that it has a unique solution (equilibrium strategies), and such that when each
individual agent behaves according to this equilibrium strategy, the social behavior will
maximize the social welfare function. If such a game can be designed, then it is said that
the game implements the social welfare function.
As an example of a social welfare function, consider minimization of pollution. While
everyone may be interested in lowering pollution levels, everyone is interested in others
bearing the associated costs. A mechanism to implement this social welfare function might
include, for example, taxes on polluting industries and tax credits given for the purchase of
electric cars. This is precisely the kind of mechanism that would cause agents, following an
equilibrium strategy, to minimize pollution.
Given a negotiation game that we have designed (i.e., a set of deals and utility functions),
we also have to design the actual negotiation mechanism. One of the important attributes
of the negotiation mechanism is eciency, i.e., maximization of the total group's utility.
This is the social welfare function that we are trying to implement. When we assume that
agents have incomplete information about one another's utility function, we basically have
a (negotiation) mechanism design problem.
However, unlike classic mechanism design in game theory, we are satisfied with a (negotiation) mechanism that has some Nash equilibrium point that implements eciency. We do
not need uniqueness, nor do we need a stronger notion of equilibrium (i.e., dominant equilibrium). The negotiation mechanism we design is intended as a suggestion to the community
221

fiZlotkin & Rosenschein

of agents' designers, along with a negotiation strategy. The negotiation mechanism and the
strategy are both part of the suggested standard. To make the standard self-enforcing it is
sucient that the strategy that is part of the standard be in Nash equilibrium.
9.1.4 Correlated Equilibrium

Players can sometime communicate prior to actually playing the game. By communicating,
the players can coordinate their strategies or even sign binding contracts about the strategies
they are about to use. Contracts can be of various types. An agent can commit himself to
playing a pure strategy if the other agent commits to playing another pure strategy. Agents
can also commit themselves to a contract in which they ip a coin and play their strategy
according to the coin.
A contract can thus be seen as an agreement between the players to correlate their
strategies. A correlated strategy in the general case is a probability distribution over all
possible joint activities (i.e., strategy combinations) of the players. In order for the players
to play according to some correlated strategy, there should be a mediator to conduct the
lottery, choose the joint activity according to the agreed probabilities, and then suggest this
strategy to the players. In some cases the mediator is assumed to release to each player
information only about that player's action (strategy) in the chosen joint action, but not
the other player's action.
Contracts between players can be binding; however, we cannot assume that contracts
are binding in all cases. Even when contracts are not binding, some of them can be selfenforcing. A contract is self-enforcing if each player that signs the contract cannot do
better by not following the contract, under the assumption that other agents are following
the contract. If the mediator's communications are observable by all the players, then
the only self-enforcing non-binding contracts are those that randomize among the Nash
equilibria of the original game ((Myerson, 1991), pp. 251).
Self-enforcing contracts on correlated strategy are called correlated equilibria. Aumann
introduced the term correlated equilibrium (Aumann, 1974); he defined the correlated equilibrium of a given game to be a Nash equilibrium of some extension of the game, where
the players receive private signals before the original game is actually played. Aumann also
showed (Aumann, 1987) that correlated equilibrium can be defined in terms of Bayesian
rationality. Forges extended this approach to games with incomplete information (Forges,
1993).
Myerson showed that correlated equilibrium is a specific case of a more general concept of equilibrium, which he called communication equilibrium, in games with incomplete
information (Myerson, 1982, 1991).
Some of the deal types that we have defined above involve coin ipping. This is, of
course, directly related to the notion of correlated strategies. As in the correlated equilibrium theory, we also assume that agents are able to agree on deals (i.e., contracts) that
involve some jointly observed random process (e.g., a coin toss). However, unlike correlated
equilibrium theory, we do assume that contracts are binding. Therefore, we assume that
agents will follow the contract (whatever the result was of the coin ip) even if it is no
longer rational for the agent to do so. Relaxation of the binding agreement assumption,
222

fiMechanisms for Automated Negotiation

and designing negotiation mechanisms that are based on self-enforcing correlated strategies,
are part of our future research plans.

9.2 Related Work in Distributed Artificial Intelligence

There have been several streams of research in Distributed Artificial Intelligence (DAI)
that have approached the problem of multiagent coordination in different ways. We here
briey review some of this work, categorizing it in the general areas of multiagent planning,
negotiation, social laws, and economic approaches.
9.2.1 Multiagent Planning

One focus of DAI research has been that of \planning for multiple agents," which considers
issues inherent in centrally directed multiagent execution. Smith's Contract Net (Smith,
1978, 1980) falls into this category, as does other DAI work (Fox, Allen, & Strohm, 1982;
Rosenschein, 1982; Pednault, 1987; Katz & Rosenschein, 1993). A second focus for research
has been \distributed planning," where multiple agents all participate in coordinating and
deciding upon their actions (Konolige & Nilsson, 1980; Corkill, 1982; Rosenschein & Genesereth, 1985; Rosenschein, 1986; Durfee, Lesser, & Corkill, 1987; Zlotkin & Rosenschein,
1991b; Ephrati & Rosenschein, 1991; Pollack, 1992; Pope, Conry, & Mayer, 1992).
The question of whether the group activity is fashioned centrally or in a distributed
manner is only one axis of comparison. Another important issue that distinguishes between
various DAI research efforts is whether the goals themselves need to be adjusted, that is,
whether there may be any fundamental conicts among different agents' goals. Thus, for
example, Georgeff's early work on multiagent planning assumed that there was no basic
conict among agent goals, and that coordination was all that was necessary to guarantee
success (Georgeff, 1983, 1984; Stuart, 1985). Similarly, planning in the context of Lesser,
Corkill, Durfee, and Decker's research (Decker & Lesser, 1992, 1993b, 1993a) often involves
coordination of activities (e.g., sensor network computations) among agents who have no
inherent conict with one another (though surface conict may exist). \Planning" here
means avoidance of redundant or distracting activity, ecient exploration of the search
space, etc.
Another important issue is the relationship that agents have to one another, e.g., the
degree to which they are willing to compromise their goals for one another (assuming that
such compromise is necessary). Benevolent Agents are those that, by design, are willing to
accommodate one another (Rosenschein & Genesereth, 1985); they have been built to be
cooperative, to share information, and to coordinate in pursuit of some (at least implicit)
notion of global utility. In contrast, Multiagent System agents will cooperate only when
it is in their best interests to do so (Genesereth, Ginsberg, & Rosenschein, 1986). Still
another potential relationship among agents is a modified master-slave relationship, called
a \supervisor-supervised" relationship, where non-absolute control is exerted by one agent
over another (Ephrati & Rosenschein, 1992a, 1992b).
The synthesis, synchronization, or adjustment process for multiple agent plans thus constitute some of the (varied) foci of DAI planning research. Synchronization through conict
avoidance (Georgeff, 1983, 1984; Stuart, 1985), distribution of a single-agent planner among
multiple agents (Corkill, 1979), the use of a centralized multiagent planner (Rosenschein,
223

fiZlotkin & Rosenschein

1982), and the use of consensus mechanisms for aggregating subplans produced by multiple agents (Ephrati & Rosenschein, 1993b), have all been explored, as well as related
issues (Cohen & Perrault, 1979; Morgenstern, 1987; von Martial, 1992a, 1992b; Kreifelts
& von Martial, 1991; Kamel & Syed, 1989; Grosz & Sidner, 1990; Kinny, Ljungberg, Rao,
Sonenberg, Tidhar, & Werner, 1992; Ferber & Drogoul, 1992; Kosoresow, 1993).
In this paper, we have not been dealing with the classical problems of planning research
(e.g., the construction of sequences of actions to accomplish goals). Instead, we have taken
as a given that the agents are capable of deriving joint plans in a domain, and then considered how they might choose from among alternative joint plans so as to satisfy potentially
conicting notions of utility. To help the agents bridge conicts, we have introduced frameworks for plan execution (such as ipping a coin to decide which of two joint plans will be
carried out), but the actual base planning mechanism is not the subject of our work.
9.2.2 Axiomatic Approaches to Group Activity

There exists a large and growing body of work within artificial intelligence that attempts
to capture notions of rational behavior through logical axiomatization (Cohen & Levesque,
1990, 1991; Rao, Georgeff, & Sonenberg, 1991; Rao & Georgeff, 1991, 1993; Georgeff &
Lansky, 1987; Georgeff, 1987; Belegrinos & Georgeff, 1991; Grosz & Kraus, 1993; Konolige,
1982; Morgenstern, 1990, 1986; Kinny & Georgeff, 1991). The approach usually centers
on a formalized model of the agent's beliefs, desires, and intentions (the so-called \BDI
model") (Hughes & Cresswell, 1968; Konolige, 1986). The purpose of the formal model is
to characterize precisely what constitutes rational behavior, with the intent to impose such
rational behavior on an automated agent. The formal axioms might be used at run-time
to directly constrain an agent's decision process, or (more likely) they could be used at
compile-time to produce a more ecient executable module.
The focus of this research, coming as it does from a single-agent artificial intelligence
perspective, is on the architecture of a single automated agent. For example, Cohen and
Levesque have explored the relationship between choice, commitment, and intention (Cohen
& Levesque, 1987, 1990)|an agent should commit itself to certain plans of action, and
remain loyal to these plans as long as it is appropriate (for example, when the agent discovers
a plan is infeasible, the plan should be dropped).
Even when looking at multiagent systems, these researchers have examined how a member of a group should be designed|again, looking at how to design an individual agent so
that it is a productive group member. For example, in certain work (Kinny et al., 1992)
axioms are proposed that cause an agent, when he discovers that he will fail to fulfill his
role in a joint plan, to notify the other members of his group. Axiomatizations, however,
might need to deal with how groups of agents could have a joint commitment to accomplishing some goal (Cohen & Levesque, 1991), or how each agent can make interpersonal
commitments without the use of such notions (Grosz & Kraus, 1993). Another use for the
BDI abstractions is to allow one agent to reason about other agents, and relativize one's
intentions in terms of beliefs about other agents' intentions or beliefs.
Axiomatic approaches tend to closely link definitions of behavior with internal agent
architecture. Thus, the definition of commitment explored by Cohen and Levesque is intended to constrain the design of an agent, so that it will behave in a certain way. Our
224

fiMechanisms for Automated Negotiation

work, on the other hand, takes an arms-length approach to the question of constraining
agents' public behavior. The rules of an encounter are really a specification of the domain
(not of the agent), and an agent designer is free to build his agent internally however he
sees fit. The rules themselves, however, will induce rational designers to build agents that
behave in certain ways, independent of the agents' internal architectures.
9.2.3 Social Laws for Multiple Agents

Various researchers in Distributed Artificial Intelligence have suggested that it would be
worthwhile to isolate \aspects of cooperative behavior," general rules that would cause
agents to act in ways conducive to cooperation. The hypothesis is that when agents act in
certain ways (e.g., share information, act in predictable ways, defer globally constraining
choices), it will be easier for them to carry out effective joint action (Steeb, Cammarata,
Hayes-Roth, & Wesson, 1980; Cammarata, McArthur, & Steeb, 1983; McArthur, Steeb, &
Cammarata, 1982).
Moses, Shoham, and Tennenholtz (Tennenholtz & Moses, 1989; Moses & Tennenholtz,
1990; Shoham & Tennenholtz, 1992b, 1992a; Moses & Tennenholtz, 1993; Shoham & Tennenholtz, 1995), for example, have suggested applying the society metaphor to artificial
systems so as to improve the performance of agents operating in the system. The issues
that are to be dealt with when analyzing a multiagent environment concern synchronization, coordination of the agents' activities, cooperative ways to achieve tasks, and how safety
and fairness constraints on the system can be guaranteed. They propose coordinating agent
activity to avoid conicts; the system will be structured so that agents will not arrive at
potential conict situations.
Thus these social laws are seen as a method to avoid the necessity for costly coordination
techniques, like planning or negotiation. With agents following the appropriate social laws,
the need for run-time coordination will be reduced. This is important, because although
agent designers may be willing to invest a large amount of effort at design time in building
effective multiagent systems, it is often critical that the run-time overhead be as low as
possible.
There is a similarity between this use of pre-compiled, highly structured social laws,
and our development of pre-defined interaction protocols. However, the social law approach
assumes that the designers of the laws have full control over the agents; agents are assumed to follow the social laws simply because they were designed to, and not because they
individually benefit from the social laws. Obeying the social laws may not be \stable";
assuming that everyone else obeys the laws, an agent might do better by breaking them.
Our approach is concerned with social conventions that are stable, which will be suitable
for individually motivated agents.
9.2.4 Decision Theoretic Approaches

There is related work in Artificial Intelligence that addresses the reasoning process of a single
agent in decision-theoretic terms. In certain work (Horvitz, 1988; Horvitz, Cooper, & Heckerma, 1989; Russell & Wefald, 1989), decision-theoretic approaches are used to optimize the
value of computation under uncertain and varying resource limitations. Etzioni considered
using a decision-theoretic architecture, with learning capabilities, to control problem solving
225

fiZlotkin & Rosenschein

search (Etzioni, 1991). For an introductory treatment of decision theory itself, see Raiffa's
classic text on the subject (Raiffa, 1968).
Classical decision theory research considers an agent that is \playing against nature,"
trying to maximize utility in uncertain circumstances. A key assumption is that \nature's"
behavior is independent of the decision made by the agent. Of course, this assumption does
not hold in a multiagent encounter.
The concept of \rationality," usually expressed in decision-theoretic terms, has been
used to model agent activity in multiagent encounters (Rosenschein & Genesereth, 1985;
Genesereth et al., 1986). Here, axioms defining different types of rationality, along with
assumptions about the rationality of others, led agents to particular choices of action. In
contrast to this work, our research employs standard game theory notions of equilibrium
and rationality. Other discussions of the use of rationality in general reasoning can be found
in Doyle's research (Doyle, 1985, 1992).
Another decision theoretic approach, taken by Gmytrasiewicz and Durfee, has been
used to model multiagent interactions (Gmytrasiewicz, Durfee, & Wehe, 1991a, 1991b;
Gmytrasiewicz & Durfee, 1992, 1993). It assumes no predefined protocol or structure to
the interaction (in marked contrast to our research on protocol design). The research uses
a decision-theoretic method for coordinating the activities of autonomous agents called the
Recursive Modeling Method. Each agent models the other agents in a recursive manner,
allowing evaluation of the expected utility attached to potential actions or communication.
9.2.5 Economic Approaches

There have been several attempts to consider market mechanisms as a way of eciently
allocating resources in a distributed system. Among the AI work is that of Smith's Contract
Net (Smith, 1978, 1980; Sandholm, 1993), Malone's Enterprise system (Malone et al., 1988),
and Wellman's WALRAS system (Wellman, 1992).
The Contract Net is a high-level communication protocol for a Distributed Problem
Solving system. It enables the distribution of the tasks among the nodes that operate in
the system. A contract between two nodes is established so that tasks can be executed;
each node in the net can act either as a manager or as a contractor. A task that has been
assigned to a node can be further decomposed by the contractor. A contract is established
by a bidding scheme that includes the announcement of the task by the manager, and bids
sent in by the potential contractors.
Enterprise (Malone et al., 1988) is a system that was built using a variation of the Contract Net protocol. The Distributed Scheduling Protocol locates the best available machine
to perform a task. This protocol is similar to the Contract Net, but makes use of more
well-defined assignment criteria.
Another system (Wellman, 1992) that takes an economic approach in solving a distributed problem through the use of a price mechanism has been explored by Wellman.
Wellman uses the consumer/producer metaphor to establish a market pricing-based mechanism for task redistribution that ensures stability and eciency. All agents act as both
consumers and producers. Each distinct good has an auction associated with it, and agents
can get the good by submitting bids in the auction for that good. The system developed
by Wellman, WALRAS, computes for each market the equilibrium price.
226

fiMechanisms for Automated Negotiation

There are two main differences between these economic approaches and our work on
mechanism design. First, there is an underlying assumption in the economic approach that
utility is explicitly transferable (e.g., money can be used). Our work does not involve any
need for explicit utility transfer. Instead, we exploit various methods for implicit utility
transfer, for example, sharing work in a joint plan, tossing a coin, etc. Of course, this
constrains the available coordination mechanism, but removes an assumption (that is, the
existence of money) that may not be suitable in certain multiagent environments. Second,
the economic models can deal with n agents in a market, while our work above deals with
two-agent encounters; however, other work of ours deals with n-agent negotiation as a
coalition formation problem (Zlotkin & Rosenschein, 1994).
9.2.6 Negotiation

Negotiation has been a subject of central interest in DAI, as it has been in economics and
political science (Raiffa, 1982). The word has been used in a variety of ways, though in
general it refers to communication processes that further coordination (Smith, 1978; Lesser
& Corkill, 1981; Kuwabara & Lesser, 1989; Conry et al., 1988; Kreifelts & von Martial,
1991; Kraus, Ephrati, & Lehmann, 1991). These negotiating procedures have included the
exchange of Partial Global Plans (Durfee, 1988; Durfee & Lesser, 1989), the communication
of information intended to alter other agents' goals (Sycara, 1988, 1989), and the use of
incremental suggestions leading to joint plans of action (Kraus & Wilkenfeld, 1991).
Interagent collaboration in Distributed Problem Solving systems has been explored in
the ongoing research of Lesser, Durfee, and colleagues. Much of this work has focused on
the implementation and analysis of data fusion experiments, where systems of distributed
sensors absorb and interpret data, ultimately arriving at a group conclusion (Durfee &
Lesser, 1987; Decker & Lesser, 1993a; L^aasri, L^aasri, & Lesser, 1990). Agents exchange
partial solutions at various levels of detail to construct global solutions; much of the work has
examined effective strategies for communication of data and hypotheses among agents, and
in particular the kinds of relationships among nodes that can aid effective group analysis.
For example, different organizations, and different methods for focusing node activity, can
help the system as a whole be far more ecient.
There are two main distinctions between our work and the work of Lesser and his
colleagues. First, the underlying assumption of the bulk of Lesser's work is that agents
are designed and implemented as part of a unified system, and work towards a global goal.
Our agents, on the other hand, are motivated to achieve individual goals. Second, unlike
our formal approach to mechanism design, Lesser's work has historically been heuristic
and experimental, although his more recent work has explored the theoretical basis for
system-level phenomena (Decker & Lesser, 1992, 1993a, 1993b).
Sycara has examined a model of negotiation that combines case-based reasoning and
optimization of multi-attribute utilities (Sycara, 1988, 1989). In particular, while we assume
that agents' goals are fixed during the negotiation, Sycara is specifically interested in how
agents can inuence one another to change their goals through a process of negotiation
(information transfer, etc.).
Kraus and her colleagues have explored negotiation where the negotiation time itself is an
issue (Kraus & Wilkenfeld, 1991; Kraus, 1993; Kraus, Wilkenfeld, & Zlotkin, 1995). Agents
227

fiZlotkin & Rosenschein

may lose value from a negotiation that drags on too long, and different agents are asymmetric
with regard to the cost of negotiation time. Agents' attitudes towards negotiation time
directly inuences the kinds of agreements they will reach. Interestingly, however, those
agreements can be reached without delay. There is an avoidable ineciency in delaying
agreement. Our work, in contrast, assumes that agent utility remains constant throughout
the negotiation process, and so negotiation time does not inuence the agreement. Some
of Kraus' work also assumes explicit utility transfer (while our work, as mentioned above,
does not).
Gasser has explored the social aspects of agent knowledge and action in multiagent
systems (\communities of programs") (Gasser, 1991, 1993). Social mechanisms can dynamically emerge; communities of programs can generate, modify, and codify their own
local languages of interaction. Gasser's approach may be most effective when agents are
interacting in unstructured domains, or in domains where their structure is continuously
changing. The research we present, on the other hand, exploits a pre-designed social layer
for multiagent systems.
Other work that focuses on the organizational aspects of societies of agents exists (Fox,
1981; Malone, 1986).
Ephrati and Rosenschein used the Clarke Tax voting procedure as a consensus mechanism, in essence to avoid the need for classical negotiation (Ephrati & Rosenschein, 1991,
1992c, 1993a). The mechanism assumes the ability to transfer utility explicitly. The Clarke
Tax technique assumes (and requires) that agents are able to transfer utility out of the
system (taxes that are paid by the agents). The utility that is transferred out of the system
is actually wasted, and reduces the eciency of the overall mechanism. This, however, is
the price that needs to be paid to ensure stability. Again, the work we present in this paper
does not assume the explicit transfer of utility. Also, the negotiation mechanism ensures
stability without the ineciency of transferring utility out of the system. However, voting
mechanisms like the Clarke Tax can deal with n-agent agreement (not the two-agent agreement of our research), and also demonstrates a kind of dominant equilibrium (in contrast
to our weaker notion of Nash equilibrium).

10. Conclusions
In this paper we have explored State Oriented Domains (SODs). In State Oriented Domains
the current description of the world is modeled as a state, and operators cause the world
to move from one state to another. The goal of an agent is to transform the world into
one of some collection of target states. In SODs, real conict is possible between agents,
and in general, agents may find themselves in four possible types of interactions, symmetric
cooperative, symmetric compromise, non-symmetric cooperative/compromise, and conict.
Agents can negotiate over different deal types in each of these kinds of interactions; in
particular, we introduced the semi-cooperative deal, and multi-plan deals, for use in conict situations. The Unified Negotiation Protocols, product maximizing mechanisms based
on either semi-cooperative deals or multi-plan deals, provide a suitable basis for conict
resolution, as well as for reaching cooperative agreements.
Strategic manipulation is possible in SODs. In a State Oriented Domain, an agent might
misrepresent his goals, or his worth function, to gain an advantage in a negotiation. The
228

fiMechanisms for Automated Negotiation

general approach by a deceitful agent would be to pretend that its worth is lower than it
actually is. This can be done directly, by declaring low worth (in certain mechanisms), or
by declaring a cheaper goal (in the case where stand-alone cost is taken to be the implicit
worth baseline). We were able to construct incentive compatible mechanisms to be used
when worths are unknown, but were unable to do so for SODs when goals are unknown.

Acknowledgements
This paper was submitted while Gilad Zlotkin was aliated with the Center for Coordination Science, Sloan School of Management, MIT. This research began while Zlotkin was
aliated with the Institute of Computer Science at the Hebrew University of Jerusalem,
and was supported by the Leibniz Center for Research in Computer Science. Some material
in this paper has appeared in preliminary form in AAAI, IJCAI, and ICICIS conference
papers (Zlotkin & Rosenschein, 1990, 1991b; Rosenschein, 1993; Zlotkin & Rosenschein,
1993c) and in a journal article (Zlotkin & Rosenschein, 1991a) (earlier version of material
on the UNP protocol). This research has been partially supported by the Israeli Ministry
of Science and Technology (Grant 032-8284) and by the Israel Science Foundation (Grant
032-7517). We would like to thank the anonymous reviewers who contributed to the improvement of this paper.

References

Allen, J. F., Kautz, H. A., Pelavin, R. N., & Tenenberg, J. D. (1991). Reasoning About
Plans. Morgan Kaufmann Publishers, Inc., San Mateo, California.
Arrow, K. J. (1963). Social Choice and Individual Values. John Wiley, New York.
Aumann, R. (1987). Correlated equilibrium as an expression of bayesian rationality. Econometrica, 55, 1{18.
Aumann, R. J. (1974). Subjectivity and correlation in randomized strategies. Journal of
Mathematical Economics, 1, 67{96.
Belegrinos, P., & Georgeff, M. P. (1991). A model of events and processes. In Proceedings
of the Twelfth International Joint Conference on Artificial Intelligence, pp. 506{511
Sydney, Australia.
Binmore, K. (1990). Essays on the Foundations of Game Theory. Basil Blackwell, Cambridge, Massachusetts.
Binmore, K. (1992). Fun and Games, A Text on Game Theory. D. C. Heath and Company,
Lexington, Massachusetts.
Bok, S. (1978). Lying: Moral Choice in Public and Private Life. Vintage Books, New York.
Cammarata, S., McArthur, D., & Steeb, R. (1983). Strategies of cooperation in distributed
problem solving. In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, pp. 767{770 Karlsruhe, West Germany.
229

fiZlotkin & Rosenschein

Cohen, P. R., & Levesque, H. J. (1987). Intention = choice + commitment. In Proceedings of the Sixth National Conference on Artificial Intelligence, pp. 410{415 Seattle,
Washington.
Cohen, P. R., & Levesque, H. J. (1990). Intention is choice with commitment. Artificial
Intelligence, 42 (2{3), 213{261.
Cohen, P. R., & Levesque, H. J. (1991). Teamwork. Technote 503, SRI International, Menlo
Park, California.
Cohen, P. R., & Perrault, C. R. (1979). Elements of a plan-based theory of speech acts.
Cognitive Science, 3, 177{212.
Conry, S. E., Meyer, R. A., & Lesser, V. R. (1988). Multistage negotiation in distributed
planning. In Bond, A., & Gasser, L. (Eds.), Readings in Distributed Artificial Intelligence, pp. 367{384. Morgan Kaufmann Publishers, Inc., San Mateo.
Corkill, D. D. (1979). Hierarchical planning in a distributed environment. In Proceedings of
the Sixth International Joint Conference on Artificial Intelligence, pp. 168{175 Tokyo.
Corkill, D. D. (1982). A Framework for Organizational Self-Design in Distributed Problem
Solving Networks. Ph.D. thesis, University of Massachusetts, Amherst, MA.
Decker, K. S., & Lesser, V. R. (1992). Generalizing the partial global planning algorithm.
International Journal of Intelligent Cooperative Information Systems, 1(2), 319{346.
Decker, K. S., & Lesser, V. R. (1993a). An approach to analyzing the need for meta-level
communication. In Proceedings of the Thirteenth International Joint Conference on
Artificial Intelligence, pp. 360{366 Chambery, France.
Decker, K. S., & Lesser, V. R. (1993b). A one-shot dynamic coordination algorithm for
distributed sensor networks. In Proceedings of the Eleventh National Conference on
Artificial Intelligence, pp. 210{216 Washington, DC.
Doyle, J. (1985). Reasoned assumptions and pareto optimality. In Proceedings of the
Ninth International Joint Conference on Artificial Intelligence, pp. 87{90 Los Angeles,
California.
Doyle, J. (1992). Rationality and its roles in reasoning. Computational Intelligence, 8 (2),
376{409.
Durfee, E. H. (1988). Coordination of Distributed Problem Solvers. Kluwer Academic
Publishers, Boston.
Durfee, E. H., & Lesser, V. R. (1987). Using partial global plans to coordinate distributed
problem solvers. In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, pp. 875{883 Milan.
Durfee, E. H., & Lesser, V. R. (1989). Negotiating task decomposition and allocation using
partial global planning. In Gasser, L., & Huhns, M. N. (Eds.), Distributed Artificial
Intelligence, Vol. II, pp. 229{243. Morgan Kaufmann, San Mateo, California.
230

fiMechanisms for Automated Negotiation

Durfee, E. H., Lesser, V. R., & Corkill, D. D. (1987). Cooperation through communication
in a distributed problem solving network. In Huhns, M. N. (Ed.), Distributed Artificial Intelligence, chap. 2, pp. 29{58. Morgan Kaufmann Publishers, Inc., Los Altos,
California.
Ephrati, E., & Rosenschein, J. S. (1991). The Clarke Tax as a consensus mechanism among
automated agents. In Proceedings of the Ninth National Conference on Artificial
Intelligence, pp. 173{178 Anaheim, California.
Ephrati, E., & Rosenschein, J. S. (1992a). Constrained intelligent action: Planning under
the inuence of a master agent. In Proceedings of the Tenth National Conference on
Artificial Intelligence, pp. 263{268 San Jose, California.
Ephrati, E., & Rosenschein, J. S. (1992b). Planning to please: Planning while constrained by
a master agent. In Proceedings of the Eleventh International Workshop on Distributed
Artificial Intelligence, pp. 77{94 Glen Arbor, Michigan.
Ephrati, E., & Rosenschein, J. S. (1992c). Reaching agreement through partial revelation of preferences. In Proceedings of the Tenth European Conference on Artificial
Intelligence, pp. 229{233 Vienna, Austria.
Ephrati, E., & Rosenschein, J. S. (1993a). Distributed consensus mechanisms for selfinterested heterogeneous agents. In First International Conference on Intelligent and
Cooperative Information Systems, pp. 71{79 Rotterdam.
Ephrati, E., & Rosenschein, J. S. (1993b). Multi-agent planning as a dynamic search for
social consensus. In Proceedings of the Thirteenth International Joint Conference on
Artificial Intelligence, pp. 423{429 Chambery, France.
Etzioni, O. (1991). Embedding decision-analytic control in a learning architecture. Artificial
Intelligence, 49, 129{159.
Ferber, J., & Drogoul, A. (1992). Using reactive multi-agent systems in simulation and problem solving. In Avouris, N. M., & Gasser, L. (Eds.), Distributed Artificial Intelligence:
Theory and Praxis, pp. 53{80. Kluwer Academic Press.
Forges, F. (1993). Five legitimate definitions of correlated equilibrium in games with incomplete information. Theory and Decision, 35, 277{310.
Fox, M. S. (1981). An organizational view of distributed systems. IEEE Transactions on
Systems, Man, and Cybernetics, SMC-11 (1), 70{80.
Fox, M. S., Allen, B., & Strohm, G. (1982). Job-shop scheduling: An investigation of
constraint-directed reasoning. In Proceedings of The National Conference on Artificial
Intelligence, pp. 155{158 Pittsburgh, Pennsylvania.
Fudenberg, D., & Tirole, J. (1992). Game Theory. The MIT Press, Cambridge, Massachusetts.
231

fiZlotkin & Rosenschein

Gasser, L. (1991). Social conceptions of knowledge and action: DAI foundations and open
systems semantics. Artificial Intelligence, 47 (1{3), 107{138.
Gasser, L. (1993). Social knowledge and social action. In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, pp. 751{757 Chambery, France.
Genesereth, M. R., Ginsberg, M. L., & Rosenschein, J. S. (1986). Cooperation without
communication. In Proceedings of The National Conference on Artificial Intelligence,
pp. 51{57 Philadelphia, Pennsylvania.
Georgeff, M. P. (1983). Communication and interaction in multi-agent planning. In Proceedings of the National Conference on Artificial Intelligence, pp. 125{129 Washington,
D.C.
Georgeff, M. P. (1984). A theory of action for multi-agent planning. In Proceedings of the
National Conference on Artificial Intelligence, pp. 121{125 Austin, Texas.
Georgeff, M. P. (1987). Actions, processes, and causality. In Georgeff, M. P., & Lansky, A. L.
(Eds.), Reasoning About Actions & Plans, pp. 99{122. Morgan Kaufmann Publishers,
Inc., Los Altos, California.
Georgeff, M. P., & Lansky, A. L. (1987). Reactive reasoning and planning. In Proceedings of the Sixth National Conference on Artificial Intelligence, pp. 677{682 Seatle,
Washington.
Gmytrasiewicz, P. J., & Durfee, E. H. (1992). A logic of knowledge and belief for recursive
modeling: Preliminary report. In Proceedings of the Tenth National Conference on
Artificial Intelligence, pp. 628{634 San Jose, California.
Gmytrasiewicz, P. J., & Durfee, E. H. (1993). Elements of a utilitarian theory of knowledge and action. In Proceedings of the Thirteenth International Joint Conference on
Artificial Intelligence, pp. 396{402 Chambery, France.
Gmytrasiewicz, P. J., Durfee, E. H., & Wehe, D. K. (1991a). A decision theoretic approach
to coordinating multiagent interaction. In Proceedings of the Twelfth International
Joint Conference on Artificial Intelligence, pp. 62{68 Sydney, Australia.
Gmytrasiewicz, P. J., Durfee, E. H., & Wehe, D. K. (1991b). The utility of communication
in coordinating intelligent agents. In Proceedings of the Ninth National Conference
on Artificial Intelligence, pp. 166{172.
Grosz, B. J., & Kraus, S. (1993). Collaborative plans for group activities. In Proceedings of
the Thirteenth International Joint Conference on Artificial Intelligence, pp. 367{373
Chambery, France.
Grosz, B. J., & Sidner, C. (1990). Plans for discourse. In Cohen, P. R., Morgan, J., &
Pollack, M. E. (Eds.), Intentions in Communication. MIT Press.
Gupta, N., & Nau, D. S. (1992). On the complexity of blocks-world planning. Artificial
Intelligence, 56 (2{3), 223{254.
232

fiMechanisms for Automated Negotiation

Harsanyi, J. C. (1956). Approaches to the bargaining problem before and after the theory
of games: a critical discussion of Zeuthen's, Hick's and Nash theories. Econometrica,
pp. 144{157.
Horvitz, E., Cooper, G., & Heckerma, D. (1989). Reection and action under scare resources:
Theoretical principles and empirical study. In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, pp. 1121{1127 Detroit, Michigan.
Horvitz, E. J. (1988). Reasoning under varying and uncertain resource constraints. In
Proceedings of the Seventh National Conference on Artificial Intelligence, pp. 111{
116.
Hughes, G. E., & Cresswell, J. M. (1968). An Introduction to Modal Logic. Methuen and
Co. Ltd.
Kamel, M., & Syed, A. (1989). An object-oriented multiple agent planning system. In
Gasser, L., & Huhns, M. N. (Eds.), Distributed Artificial Intelligence, Volume II, pp.
259{290. Pitman Publishing/Morgan Kaufmann Publishers, San Mateo, CA.
Katz, M. J., & Rosenschein, J. S. (1993). Verifying plans for multiple agents. Journal of
Experimental and Theoretical Artificial Intelligence, 5, 39{56.
Kinny, D., Ljungberg, M., Rao, A., Sonenberg, E., Tidhar, G., & Werner, E. (1992). Planned
team activity. In Pre-Proceedings of the Fourth European Workshop on Modeling
Autonomous Agents in a Multi-Agent World Rome, Italy.
Kinny, D. N., & Georgeff, M. P. (1991). Commitment and effectiveness of situated agents.
In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence,
pp. 82{88 Sydney, Australia.
Konolige, K. (1982). A first-order formalization of knowledge and action for a multi-agent
planning system. Machine Intelligence, 10.
Konolige, K. (1986). A Deduction Model of Belief. Pitman Publishers/Morgan Kaufmann,
San Matheo, CA.
Konolige, K., & Nilsson, N. J. (1980). Multiple-agent planning systems. In Proceedings of
the First Annual National Conference on Artificial Intelligence, pp. 138{142 Stanford,
California.
Kosoresow, A. P. (1993). A fast first-cut protocol for agent coordination. In Proceedings of
the Eleventh National Conference on Artificial Intelligence, pp. 237{242 Washington,
DC.
Kraus, S. (1993). Agents contracting tasks in non-collaborative environments. In Proceedings
of the Eleventh National Conference on Artificial Intelligence, pp. 243{248.
Kraus, S., Ephrati, E., & Lehmann, D. (1991). Negotiation in a non-cooperative environment. Journal of Experimental and Theoretical Artificial Intelligence, 3 (4), 255{282.
233

fiZlotkin & Rosenschein

Kraus, S., & Wilkenfeld, J. (1990). The function of time in cooperative negotiations: Extended abstract. In Proceedings of the Tenth Workshop on Distributed Artificial Intelligence Bandera, Texas.
Kraus, S., & Wilkenfeld, J. (1991). Negotiations over time in a multi-agent environment:
Preliminary report. In Proceedings of the Twelfth International Joint Conference on
Artificial Intelligence, pp. 56{61 Sydney.
Kraus, S., Wilkenfeld, J., & Zlotkin, G. (1995). Multiagent negotiation under time constraints. Artificial Intelligence, 75 (2), 297{345.
Kreifelts, T., & von Martial, F. (1991). A negotiation framework for autonomous agents. In
Demazeau, Y., & Muller, J.-P. (Eds.), Decentralized A. I. 2, Proceedings of the Second
European Workshop on Modelling Autonomous Agents in a Multi-Agent World, pp.
71{88. North-Holland, Amsterdam.
Kuwabara, K., & Lesser, V. R. (1989). Extended protocol for multistage negotiation. In
Proceedings of the Ninth Workshop on Distributed Artificial Intelligence, pp. 129{161
Rosario, Washington.
L^aasri, B., L^aasri, H., & Lesser, V. R. (1990). Negotiation and its role in cooperative distributed problem problem solving. In Proceedings of the Tenth International Workshop
on Distributed Artificial Intelligence Bandera, Texas. Chapter 8.
Lesser, V. R., & Corkill, D. D. (1981). Functionally-accurate, cooperative distributed systems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-11 (1), 81{96.
Levesque, H. J., & Cohen, P. R. (1990). On acting together. In Proceedings of the Eighth
National Conference on Artificial Intelligence, pp. 94{99 Boston, Massachusetts.
Luce, R. D., & Raiffa, H. (1957). Games and Decisions. John Wiley & Sons, Inc., New
York.
Malone, T. W. (1986). Organizing information processing systems: Parallels between human
organizations and computer systems. In Zacharai, W., Robertson, S., & Black, J.
(Eds.), Cognition, Computation, and Cooperation. Ablex Publishing Corp., Norwood,
NJ.
Malone, T. W., Fikes, R. E., Grant, K. R., & Howard, M. T. (1988). Enterprise: A
market-like task scheduler for distributed computing environments. In Huberman,
B. A. (Ed.), The Ecology of Computation, pp. 177{205. North-Holland Publishing
Company, Amsterdam.
McArthur, D., Steeb, R., & Cammarata, S. (1982). A framework for distributed problem
solving. In Proceedings of The National Conference on Artificial Intelligence, pp.
181{184 Pittsburgh, Pennsylvania.
Morgenstern, L. (1986). A first order theory of planning, knowledge, and action. In Halpern,
J. Y. (Ed.), Theoretical Aspects of Reasoning About Knowledge, pp. 99{114. Morgan
Kaufmann, Los Altos.
234

fiMechanisms for Automated Negotiation

Morgenstern, L. (1987). Knowledge preconditions for actions and plans. In Proceedings
of the Tenth International Joint Conference on Artificial Intelligence, pp. 867{874
Milan, Italy.
Morgenstern, L. (1990). A formal theory of multiple agent nonmonotonic reasoning. In
Proceedings of the Eighth National Conference on Artificial Intelligence, pp. 538{544
Boston, Massachusetts.
Moses, Y., & Tennenholtz, M. (1990). Artificial social systems part 1: Basic principles.
Tech. rep. CS90-12, Weizmann Institute.
Moses, Y., & Tennenholtz, M. (1993). Off-line reasoning for on-line eciency. In Proceedings
of the Thirteenth International Joint Conference on Artificial Intelligence, pp. 490{
495 Chambery, France.
Myerson, R. (1982). Optimal coordination mechanisms in generalized principal-agent problems. Journal of Mathematical Economics, 10, 67{81.
Myerson, R. (1991). Game Theory: Analysis of Conict. Harvard University Press, Cambridge, Massachusetts.
Nash, J. F. (1950). The bargaining problem. Econometrica, 28, 155{162.
Nash, J. F. (1953). Two-person cooperative games. Econometrica, 21, 128{140.
Osborne, M. J., & Rubinstein, A. (1990). Bargaining and Markets. Academic Press Inc.,
San Diego, California.
Pednault, E. P. D. (1987). Formulating multiagent dynamic-world problems in the classical
planning framework. In Georgeff, M. P., & Lansky, A. L. (Eds.), Reasoning about Actions and Plans: Proceedings of the 1986 Workshop, pp. 47{82 San Mateo, California.
Morgan Kaufmann.
Pollack, M. E. (1992). The uses of plans. Artificial Intelligence, 57 (1).
Pope, R. P., Conry, S. E., & Mayer, R. A. (1992). Distributing the planning process in
a dynamic environment. In Proceedings of the Eleventh International Workshop on
Distributed Artificial Intelligence, pp. 317{331 Glen Arbor, Michigan.
Raiffa, H. (1968). Decision Analysis, Introductory Lectures on Choices under Uncertainty.
Addison-Wesley Publishing Company, Reading, Massachusetts.
Raiffa, H. (1982). The Art and Science of Negotiation. The Belknap Press of Harvard
University Press, Cambridge, Massachusetts.
Rao, A. S., & Georgeff, M. P. (1991). Asymmetry thesis and side-effect problems in lineartime and branching-time intention logics. In Proceedings of the Twelfth International
Joint Conference on Artificial Intelligence, pp. 498{504 Sydney, Australia.
235

fiZlotkin & Rosenschein

Rao, A. S., & Georgeff, M. P. (1993). A model-theoretic approach to the verification
of situated reasoning systems. In Proceedings of the Thirteenth International Joint
Conference on Artificial Intelligence, pp. 318{324 Chambery, France.
Rao, A. S., Georgeff, M. P., & Sonenberg, E. (1991). Social plans: a preliminary report.
In Pre-Proceedings of the Third European Workshop on Modeling Autonomous Agents
and Multi-Agent Worlds Germany.
Rasmusen, E. (1989). Games and Information, An Introduction to Game Theory. Basil
Blackwell, Cambridge, Massachusetts.
Rosenschein, J. S. (1982). Synchronization of multi-agent plans. In Proceedings of the
National Conference on Artificial Intelligence, pp. 115{119 Pittsburgh, Pennsylvania.
Rosenschein, J. S. (1986). Rational Interaction: Cooperation Among Intelligent Agents.
Ph.D. thesis, Stanford University.
Rosenschein, J. S. (1993). Consenting agents: Negotiation mechanisms for multi-agent
systems. In Proceedings of the Thirteenth International Joint Conference on Artificial
Intelligence, pp. 792{799 Chambery, France.
Rosenschein, J. S., & Genesereth, M. R. (1985). Deals among rational agents. In Proceedings
of the Ninth International Joint Conference on Artificial Intelligence, pp. 91{99 Los
Angeles, California.
Roth, A. E. (1979). Axiomatic Models of Bargaining. Springer-Verlag, Berlin.
Rubinstein, A. (1982). Perfect equilibrium in a bargaining model. Econometrica, 50 (1),
97{109.
Rubinstein, A. (1985). Choice of conjectures in a bargaining game with incomplete information. In Roth, A. E. (Ed.), Game-theoretic models of bargaining, pp. 99{114.
Cambridge University Press, Cambridge, New York.
Russell, S., & Wefald, E. (1989). Principles of metareasoning. In Proceedings of the First
International Conference on Principles of Knowledge Representation and Reasoning,
pp. 400{411. Morgan Kaufmann.
Sandholm, T. (1993). An implementation of the contract net protocol based on marginal
calculations. In Proceedings of the Eleventh National Conference on Artificial Intelligence, pp. 256{262.
Schelling, T. C. (1963). The Strategy of Conict. Oxford University Press, New York.
Schelling, T. C. (1984). Choice and Consequence. Harvard University Press, Cambridge,
Massachusetts.
Shoham, Y., & Tennenholtz, M. (1992a). Emergent conventions in multi-agent systems:
initial experimental results and observations (preliminary report). In Principles of
knowledge representation and reasoning: Proceedings of the Third International Conference Cambridge, Massachusetts.
236

fiMechanisms for Automated Negotiation

Shoham, Y., & Tennenholtz, M. (1992b). On the synthesis of useful social laws for artificial
agent societies (preliminary report). In Proceedings of the National Conference on
Artificial Intelligence San Jose, California.
Shoham, Y., & Tennenholtz, M. (1995). On social laws for artificial agent societies: Off-line
design. Artificial Intelligence. To appear.
Smith, R. G. (1978). A Framework for Problem Solving in a Distributed Processing Environment. Ph.D. thesis, Stanford University.
Smith, R. G. (1980). The contract net protocol: High-level communication and control in a
distributed problem solver. IEEE Transactions on Computers, C-29 (12), 1104{1113.
Steeb, R., Cammarata, S., Hayes-Roth, F., & Wesson, R. (1980). Distributed intelligence
for air eet control. Tech. rep. WD-839-ARPA, The Rand Corporation.
Stuart, C. J. (1985). An implementation of a multi-agent plan synchronizer. In Proceedings
of the Ninth International Joint Conference on Artificial Intelligence, pp. 1031{1035
Los Angeles, California.
Sycara, K. P. (1988). Resolving goal conicts via negotiation. In Proceedings of the Seventh
National Conference on Artificial Intelligence, pp. 245{250 St. Paul, Minnesota.
Sycara, K. P. (1989). Argumentation: Planning other agents' plans. In Proceedings of
the Eleventh International Joint Conference on Artificial Intelligence, pp. 517{523
Detroit.
Tennenholtz, M., & Moses, Y. (1989). On cooperation in a multi-entity model (preliminary
report). In Proceedings of the Eleventh International Joint Conference on Artificial
Intelligence, pp. 918{923 Detroit, Michigan.
von Martial, F. (1990). Coordination of plans in multiagent worlds by taking advantage of
the favor relation. In Proceedings of the Tenth International Workshop on Distributed
Artificial Intelligence Bandera, Texas.
von Martial, F. (1992a). Coordinating Plans of Autonomous Agents. No. 610 in Lecture
Notes in Artificial Intelligence. Springer Verlag, Heidelberg, Germany.
von Martial, F. (1992b). Coordination by negotiation based on a connection of dialogue
states with actions. In Proceedings of the Eleventh International Workshop on Distributed Artificial Intelligence, pp. 227{246 Glen Arbor, Michigan.
Wellman, M. P. (1992). A general equilibrium approach to distributed transportation planning. In Proceedings of the Tenth National Conference on Artificial Intelligence San
Jose, California.
Zeuthen, F. (1930). Problems of Monopoly and Economic Walfare. G. Routledge & Sons,
London.
237

fiZlotkin & Rosenschein

Zlotkin, G., & Rosenschein, J. S. (1989). Negotiation and task sharing among autonomous
agents in cooperative domains. In Proceedings of the Eleventh International Joint
Conference on Artificial Intelligence, pp. 912{917 Detroit, Michigan.
Zlotkin, G., & Rosenschein, J. S. (1990). Negotiation and conict resolution in noncooperative domains. In Proceedings of the Eighth National Conference on Artificial
Intelligence, pp. 100{105 Boston, Massachusetts.
Zlotkin, G., & Rosenschein, J. S. (1991a). Cooperation and conict resolution via negotiation among autonomous agents in noncooperative domains. IEEE Transactions on
Systems, Man, and Cybernetics, 21 (6), 1317{1324.
Zlotkin, G., & Rosenschein, J. S. (1991b). Incomplete information and deception in multiagent negotiation. In Proceedings of the Twelfth International Joint Conference on
Artificial Intelligence, pp. 225{231 Sydney, Australia.
Zlotkin, G., & Rosenschein, J. S. (1991c). Negotiation and goal relaxation. In Demazeau,
Y., & Muller, J.-P. (Eds.), Decentralized A. I. 2, Proceedings of the Second European
Workshop on Modelling Autonomous Agents in a Multi-Agent World, pp. 273{286.
North-Holland, Amsterdam.
Zlotkin, G., & Rosenschein, J. S. (1993a). A domain theory for task oriented negotiation. In
Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence,
pp. 416{422 Chambery, France.
Zlotkin, G., & Rosenschein, J. S. (1993b). The extent of cooperation in state-oriented domains: Negotiation among tidy agents. Computers and Artificial Intelligence, 12 (2),
105{122.
Zlotkin, G., & Rosenschein, J. S. (1993c). Negotiation with incomplete information about
worth: Strict versus tolerant mechanisms. In Proceedings of the First International
Conference on Intelligent and Cooperative Information Systems, pp. 175{184 Rotterdam, The Netherlands.
Zlotkin, G., & Rosenschein, J. S. (1994). Coalition, cryptography, and stability: Mechanisms for coalition formation in task oriented domains. In Proceedings of the National
Conference on Artificial Intelligence, pp. 432{437 Seattle, Washington.
Zlotkin, G., & Rosenschein, J. S. (1996a). Compromise in negotiation: Exploiting worth
functions over states. Artificial Intelligence, 84 (1{2), 151{176.
Zlotkin, G., & Rosenschein, J. S. (1996b). Mechanism design for automated negotiation,
and its application to task oriented domains. Artificial Intelligence. To appear.

238

fiJournal of Artificial Intelligence Research 5 (1996) 289-300

Submitted 6/96; published 12/96

Research Note

Characterizations of Decomposable Dependency Models

Luis M. de Campos

Departamento de Ciencias de la Computacion e I.A.
E.T.S. Ingeniera Informatica, Universidad de Granada
18071 - Granada SPAIN

lci@decsai.ugr.es

Abstract

Decomposable dependency models possess a number of interesting and useful properties. This paper presents new characterizations of decomposable models in terms of independence relationships, which are obtained by adding a single axiom to the well-known
set characterizing dependency models that are isomorphic to undirected graphs. We also
briey discuss a potential application of our results to the problem of learning graphical
models from data.

1. Introduction

Graphical models are knowledge representation tools commonly used by an increasing number of researchers, particularly from the Artificial Intelligence and Statistics communities.
The reason for the success of graphical models is their capacity to represent and handle
independence relationships, which have proved crucial for the ecient management and
storage of information (Pearl, 1988).
There are different kinds of graphical models, although we are particularly interested
in undirected and directed graphs (which, in a probabilistic context, are usually called
Markov networks and Bayesian networks, respectively). Each one has its own merits and
shortcomings, but neither of these two representations has more expressive power than the
other: there are independence relationships that can be represented by means of directed
graphs (using the d-separation criterion) and cannot be represented by using undirected ones
(through the separation criterion), and reciprocally. However, there is a class of models that
can be represented by means of both directed and undirected graphs, which is precisely the
class of decomposable models (Haberman, 1974; Pearl, 1988). Decomposable models also
possess important properties, relative to factorization and parameter estimation, which
make them quite useful. So, these models have been studied and characterized in many
different ways (Beeri, Fagin, Maier, & Yannakakis, 1983; Haberman, 1974; Lauritzen, Speed,
& Vijayan, 1984; Pearl, 1988; Wermuth & Lauritzen, 1983; Whittaker, 1991). For example,
decomposable models have been characterized as the kind of dependency models isomorphic
to chordal graphs (Lauritzen et al., 1984; Whittaker, 1991).
However, we do not know any characterization of decomposable models in terms of the
kind of independence relationships that they are capable of representing. This is somewhat
surprising, because it seems quite natural to us to characterize a type of object using the
same terms as those used to define it; in our case, the object is a special type of dependency
model, i.e., a collection of conditional independence statements about a set of variables in
a given domain of knowledge, and therefore we should be able to describe it in terms of
c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiDe Campos

properties of these independence relationships. The objective of this paper is precisely to
obtain such a characterization of decomposable models.
Our approach to the problem will be based on identifying the set of properties or axioms
that a collection of independence relationships must satisfy, in order to be representable
by a chordal graph. This approach has been successfully used to study other kinds of
dependency models: Pearl and Paz (1985) identified the set of properties characterizing
models isomorphic to undirected graphs, and de Campos (1996) determined the axioms
that characterize models isomorphic to undirected and directed singly connected graphs
(i.e., trees and polytrees, respectively).
The rest of the paper is organized as follows. In Section 2 we briey describe several
concepts which are basic for subsequent development. Section 3 introduces decomposable
models and their representation using chordal graphs. In Section 4 we prove two characterizations of decomposable models. These characterizations turn out to be surprisingly
simple: we only have to add a single property to the set of axioms characterizing dependency
models isomorphic to undirected graphs. Section 5 discusses the relationships between our
results and Lauritzen's characterization of chordal graphs. Finally, Section 6 contains the
concluding remarks and some proposals for future work, which include the application of
the results developed here to the problem of learning graphical models from data.

2. Preliminaries

In this section, we are going to describe the notation as well as some basic concepts and
results used throughout the paper.
A Dependency Model (Pearl, 1988) is a pair M = (U; I ), where U is a finite set of elements or variables, and I (:; :j:) is a rule that assigns truth values to a three place predicate
whose arguments are disjoint subsets of U . Single elements of U will be denoted by standard
or Greek lowercase letters, whereas subsets of U will be represented by capital letters. The
interpretation of the conditional independence assertion I (X; Y jZ ) is that having observed
Z , no additional information about X could be obtained by also observing Y . For example, in a probabilistic model (Dawid, 1979; Lauritzen, Dawid, Larsen, & Leimer, 1990),
I (X; Y jZ ) holds if and only if

P (xjz; y) = P (xjz) whenever P (z; y) > 0;
for every instantiation x, y and z of the sets of variables X , Y and Z . However, dependency
models are applicable to many situations far beyond probabilistic models (de Campos, 1995;
Pearl, 1988; Shenoy, 1992).
A graphical representation of a dependency model M = (U; I ) is a direct correspondence
between the elements in U and the set of nodes in a given graph, G, such that the topology of
G reects some properties of I . The topological property selected to represent independence
assertions depends on the type of graph we use: separation for undirected graphs and dseparation (Pearl, 1988; Verma & Pearl, 1990) for directed acyclic graphs (dags):

 Separation: Given an undirected graph G, two subsets of nodes, X and Y , are said to
be separated by the set of nodes Z , and this is denoted by hX; Y jZ iG , if Z intercepts
all chains between the nodes in X and those in Y .
290

fiCharacterizations of Decomposable Dependency Models

 d-separation: Given a dag G, a chain C (a chain in a directed graph is a sequence of
adjacent nodes, the direction of the arrows does not matter) from node ff to node fi
is said to be blocked by the set of nodes Z , if there is a vertex  2 C such that, either
{  2 Z and arrows of C do not meet head to head at  , or
{  62 Z , nor has  any descendants in Z , and the arrows of C do meet head to
head at  .
Two subsets of nodes, X and Y , are said to be d-separated by Z , and this is also
denoted by hX; Y jZ iG , if all chains between the nodes in X and the nodes in Y
are blocked by Z . There exists a criterion equivalent to d-separation, based on the
separation of X from Y by Z in the moral graph of the smallest ancestral set containing
X [ Y [ Z (Lauritzen et al., 1990).
Given a dependency model, M , we say that an undirected graph (a dag, respectively), G,
is an I-map if every separation (d-separation, respectively) in G implies an independence
in M : hX; Y jZ iG ) I (X; Y jZ ). On the other hand, an undirected graph (a dag, resp.),
G, is called a D-map if every independence relation in the model implies a separation (dseparation resp.) in the graph: I (X; Y jZ ) ) hX; Y jZ iG . A graph, G, is a Perfect map of
M if it is both an I-map and a D-map. M is said to be graph-isomorphic if a graph exists
which is a perfect map of M .

The class of dependency models isomorphic to undirected graphs has been completely
characterized (Pearl & Paz, 1985) in terms of five properties or axioms satisfied by the
independence relationships within the model:
(C1) Symmetry:
(I (X; Y jZ ) ) I (Y; X jZ )) 8X; Y; Z  U:
(C2) Decomposition:
(I (X; Y [ W jZ ) ) I (X; Y jZ )) 8X; Y; W; Z  U:
(C3) Strong Union:
(I (X; Y jZ ) ) I (X; Y jZ [ W )) 8X; Y; W; Z  U:
(C4) Intersection:
(I (X; Y jZ [ W ) and I (X; W jZ [ Y ) ) I (X; Y [ W jZ )) 8X; Y; W; Z  U:
(C5) Transitivity:
(I (X; Y jZ ) ) I (X;  jZ ) or I (; Y jZ ) 8 2 U n (X [ Y [ Z )) 8X; Y; Z  U:
Pearl and Paz also tacitly assumed that an additional, trivial, axiom holds, namely I (X; ;jZ )
8X; Z  U . They also assumed all through that the sets X; Y; Z; W involved in the axioms
are pairwise disjoint.

Theorem 1 (Pearl and Paz, 1985) A dependency model M is isomorphic to an undirected graph if, and only if, it satisfies the axioms C1{C5.

The graph associated with the dependency model M , such that conditional independence
in M is equivalent to separation in this graph, is GM = (U; EM ), where the set of edges
291

fiDe Campos

EM is

EM = fff{fi j ff; fi 2 U; :I (ff; fi jU n fff; fi g)g:

On the other hand, the class of dependency models isomorphic to dags is considerably
more dicult to characterize. It has been suggested (Geiger, 1987; Pearl, 1988) that the
number of axioms required for a complete characterization of the d-separation in dags is
probably unbounded. However, some more restricted models, namely polytree-isomorphic
models, can be fully characterized by using a finite number of axioms (de Campos, 1996).
Graphical models are not only convenient means of expressing conditional independence
statements in a given domain of knowledge, they also convey information necessary for
decisions and inference, in the form of numerical parameters quantifying the strength of each
link. The assignment of numerical parameters to a graphical model is also quite different
for undirected and directed graphs (here we restrict the discussion to probabilistic models).
In the case of directed acyclic graphs, this is a simple matter: we only have to assign to
each variable xi in the dag a conditional probability distribution for every instantiation of
the variables that form the parent set of xi ,  (xi). The product of these local distributions
constitutes a complete and consistent specification, i.e., a joint probability distribution
(which also preserves the independence relationships displayed by the dag):

P (x1 ; x2; : : :; xn) =

Yn P (xij(xi))

i=1

However, the case of undirected graphs is different: constructing a complete and consistent
quantitative specification while preserving the dependence structure of an arbitrary undirected graph can be done using the method of Gibb's potentials (Lauritzen, 1982) (which
assigns compatibility functions to the cliques of the graph), but it is considerably more
complicated, in terms of both computational effort and meaningfulness of the parameters,
than the simple method used for dags.

3. Decomposable Models and Chordal Graphs
Some dependency models representable by means of a special class of undirected graphs
do not present the quantification problem described above. These are the so called decomposable models, which also exhibit a number of important and useful additional properties.
There are several ways of defining decomposable models. The most appropriate to our
interests, which mainly lie in graphical modelling, is based on a graph-theoretic concept:
chordal graphs, also called triangulated graphs (Rose, 1970).

Definition 1 An undirected graph is said to be chordal if every cycle of length four or more
has a chord, i.e., an edge linking two non-adjacent nodes in the cycle.

The simplest example of a non-chordal graph is the diamond-shaped graph displayed in
Figure 1 (a).

Definition 2 A dependency model is decomposable if it is isomorphic to a chordal graph.
292

fiCharacterizations of Decomposable Dependency Models

fi


@
,
,

fi


@
,
,
@
@
,
,
@
@
@fi
@fi fi
fi
fi
,
,

ff
fi
ff






Z
Z
,
,
,
,
Z
Z
Z fi,
Z fi,
Z
Z
fi ,
 ,


(a)

(b)

Figure 1: (a) The simplest example of a non-chordal graph (b) Non-chordal graph satisfiying
C1{C5 and C7
One important property satisfied by every chordal graph G, which in fact characterizes
chordal graphs (Beeri et al., 1983), is that the edges of G can be directed acyclically so that
every pair of converging arrows emanates from two adjacent nodes. From this property, it
can be deduced (Pearl, 1988) that the class of dependency models that may be represented
by both a dag and an undirected graph is precisely the class of decomposable models (note
that in non-chordal graphs, no matter how we direct the arrows, there will always be a pair
of nonadjacent parents sharing a common child, a configuration that causes separation in
undirected graphs but does not produce d-separation in dags).
Another crucial property of chordal graphs is that their cliques (i.e., the largest subgraphs whose nodes are all adjacent to each other) can be joined to form a tree T , called
the join tree, such that any two cliques containing a node ff are either adjacent in T or
connected by a chain of T made entirely of cliques that contain ff (Beeri et al., 1983) (an
example is depicted in Figure 2).

,
,
,

yl

, Z
Z
,
,
Z

xl

zl

@
%
%
@
@ %

, @
,
@
@
,
w

tl

(a)

ul

vl

l

ffyzt 

 	
ffxy  bbbffuzt 

 	

 A	
,
ffuv,, ffuwAA 

 	 
 	
(b)

Figure 2: Chordal graph (a) and its join tree (b)
This result has important consequences for probabilistic modelling: the joint probability
distribution factorises into the product of marginal distributions on cliques (Lauritzen et al.,
1984; Pearl, 1988; Whittaker, 1991); moreover, maximum likelihood estimates of the model
are directly calculable (Whittaker, 1991). As a consequence the compatibility functions
293

fiDe Campos

used to quantitatively specify the model, have a clear meaning and can be easily estimated.
Additionally, the tree structure of the cliques in a chordal graph facilitates recursive updating of probabilities. In fact, one of the most important algorithms for propagation (i.e.,
updating using local computations) of probabilities in dags, is based on a transformation of
the given dag into a chordal graph, by moralising and next triangulating the dag (Lauritzen
& Spiegelhalter, 1988).

4. Characterizing Decomposable Models

Our purpose is to find a characterization of decomposable models (or equivalently, of chordal
graphs) in terms of properties of independence relationships. This will be carried out by
adding a single property to the set of axioms, C1{C5, characterizing dependency models
isomorphic to undirected graphs.
Let us consider the following axiom:
(C6) Strong Chordality:
(I (ff; fi jZ [  [  ) and I (;  jU nf;  g) ) I (ff; fi jZ [  ) or I (ff; fi jZ [  )) 8ff; fi; ;  2 U 8Z 

U n fff; fi; ;  g:

This axiom establishes a condition that allows us to reduce the size of the conditioning
set separating two variables ff and fi , namely that two of the variables in this set are
conditionally independent. We are going to demonstrate that by adding the axiom of strong
chordality to the axioms found by Pearl and Paz, C1{C5, the associated graph necessarily
becomes a chordal graph and vice versa. Therefore, we shall obtain a characterization of
decomposable models. Pearl (1988) proposed an axiom slightly different from C6, which is
a necessary, though not sucient condition for chordality. He called this axiom chordality:
(C7) Chordality:
(I (ff; fi j [  ) and I (;  jff [ fi ) ) I (ff; fi j ) or I (ff; fi j )) 8ff; fi; ;  2 U:
Observe that in our context, i.e., assuming that C1{C5 hold, C6 implies C7: from
I (ff; fij [ ) and I (; jff [ fi ), as strong union (C3) guarantees that I (;  jU n f;  g) is
implied by I (;  jW ) for any W  U nf;  g (in particular for W = fff; fi g), then by applying
C6 with Z = ;, we obtain I (ff; fi j ) or I (ff; fi j ). However, the set of axioms C1{C5 and
C7 do not constitute a characterization of chordal graphs, as the graph depicted in Figure
1 (b) shows: this graph is not chordal, but it satisfies C1{C5 and C7. By using C6 instead
of C7 we shall obtain the desired result.

Theorem 2 A dependency model M is isomorphic to a chordal graph if, and only if, it
satisfies the axioms C1{C6.

Proof: First, let us prove the sucient condition. Using the Pearl and Paz result, from C1{

C5 we deduce that M is isomorphic to its associated graph G, and therefore independence
in M is equivalent to separation in G. We only have to prove that G is chordal.
Let us suppose that G is not chordal. Then, in G, there is a cycle t1 t2 : : :tn,1 tn t1 , n  4,
without a chord, i.e., 8i; j s.t. 1  i < i + 1 < j  n, the edges ti {tj do not belong to EM
(except the edge t1 {tn ).
294

fiCharacterizations of Decomposable Dependency Models

Let us consider the nodes t1 and tn,1 , and the set of nodes Z = U n ft1 ; : : :; tn g. First,
we are going to prove that the independence statement I (t1; tn,1 jZ [ t2 [ tn ) has to be
true: if it were :I (t1 ; tn,1 jZ [ t2 [ tn ) then we could find a chain linking t1 and tn,1 not
containing nodes from Z [ t2 [ tn , i.e., a chain linking t1 and tn,1 containing only nodes
from ft3 ; : : :; tn,2 g; but in this case we would have an edge linking t1 and some node tj ,
3  j  n , 2, and this contradicts the assumption that the cycle has no chord. Therefore,
we have I (t1; tn,1 jZ [ t2 [ tn ).
On the other hand, the nodes t2 and tn are not connected by any edge (once again
because the cycle has not any chord), so they are separated by U nft2 ; tn g, and therefore we
have I (t2 ; tn jU nft2 ; tn g). Now, using C6, we deduce either I (t1; tn,1 jZ [ t2 ) or I (t1; tn,1 jZ [
tn ). In either case there is a chain linking t1 and tn,1 which is not blocked by the separating
set: in the first case the chain is t1 tn tn,1 , and in the second case it is t1 t2 : : :tn,2 tn,1 .
Therefore, we obtain a contradiction, hence the graph G has to be chordal.
Now, let us prove the necessary condition. Once again using Pearl and Paz's result, as M
is isomorphic to a graph G, then the properties C1{C5 hold.
Let us suppose that C6 does not hold. Then, we can find nodes ff; fi; ;  and a subset
of nodes Z such that I (ff; fi jZ [  [  ), I (;  jU n f;  g), :I (ff; fi jZ [  ) and :I (ff; fi jZ [  ).
From :I (ff; fi jZ [  ) we deduce that a chain fft1 : : :tn fi exists in G, such that ti 62 Z [ 
8i, i.e., ft1 : : :tng \ (Z [  ) = ;. However, from I (ff; fijZ [  [ ) we know that every chain
linking ff and fi must contain some node from Z [  [  . In particular, for the previously
found chain, we have ft1 : : :tn g \ (Z [  [  ) 6= ;. Therefore, there is a node tk such
that tk =  . Let us consider the node tk,1 : from I (ff; fi jZ [  [  ) and transitivity (C5),
we obtain I (ff; tk,1jZ [  [  ) or I (tk,1 ; fi jZ [  [  ). The first independence assertion
cannot be true, because the chain fft1 : : :tk,2 tk,1 does not contain any node from Z [  [  .
Therefore, we have I (tk,1 ; fi jZ [  [  ). If it were I (tk,1 ; fi jZ [  ), then, from transitivity,
we would obtain I (ff; tk,1jZ [  ) or I (ff; fi jZ [  ), and both statements are false, the first
one because of the existence of the chain fft1 : : :tk,2 tk,1 and the second one because of the
hypothesis. So, we have :I (tk,1 ; fi jZ [  ). The same reasoning allows us to assert from
:I (ff; fijZ [ ) that :I (tk,1; fijZ [ ). So, we have found a node tk,1 adjacent to  = tk
satisfying the same properties as ff. A completely analogous reasoning applied to node tk+1
proves I (tk,1 ; tk+1 jZ [  [  ), :I (tk,1 ; tk+1 jZ [  ), and :I (tk,1 ; tk+1 jZ [  ). So, we have
replaced nodes ff and fi by two nodes adjacent to  satisfying the same properties. Note
that the case in which one or the other of tk,1 and tk+1 is ff or fi does not matter to the
subsequent argument.
Now, from :I (tk,1 ; tk+1 jZ [  ) and I (tk,1 ; tk+1 jZ [  [  ) we deduce that there is a
chain tk,1 s1 : : :sm tk+1 in G such that si 62 Z [  8i and for some node sh , sh =  . To
simplify the notation, let us call s0 = tk,1 , sm+1 = tk+1 . We can assume that 8i; j such
that 0 < i + 1 < j  h, there is no edge linking si and sj (if this is not the case, we can
simply replace the subchain si si+1 : : :sj ,1 sj by the single edge si {sj , i.e., we consider the
shortest subchain between tk,1 and sh ). For the same reason, we can also suppose that
8p; q such that h < p + 1 < q  m + 1, there is not any edge linking sp and sq .
We have found a cycle s0 s1 : : :sh,1 sh+1 : : :sm sm+1  in G. Now, let sf and sg be two
nodes satisfying f < h < g , sf and sg are adjacent to  but  is not adjacent to sj for all
j s.t. f < j < g and j 6= h (note that we can always find these two nodes, starting from
f = 0 and g = m + 1). We still have a cycle sf : : :sh,1 sh+1 : : :sg  of length four or more,
295

fiDe Campos

so that, according to the hypothesis, this cycle must have some chord. However, taking
into account how the cycle has been constructed, the only possible chords are the edge  {
or an edge linking a node si , f < i < h, and a node sp , h < p < g . The first possibility
contradicts the hypothesis I (;  jU nf;  g), and the second one implies the existence of the
chain tk,1 s1 : : :sf : : :si sp : : :sg : : :sm tk+1 linking tk,1 and tk+1 , which does not contain any
node from Z [  [  , in contradiction with the statement I (tk,1 ; tk+1 jZ [  [  ). Therefore,
the property C6 has to be true.
We can establish another interesting characterization of chordal graphs, by also adding
only one axiom to those of Pearl and Paz. This new axiom is the following:
(C8) Clique-separability:
(I (ff; fi jU n fff; fi g) ) 9W  U n fff; fi g such that I (ff; fi jW ) and either jW j  1 or
:I (; jU n f; g) 8;  2 W ) 8ff; fi 2 U .
Axiom C8 asserts that whenever two nodes ff and fi are not adjacent (are independent),
we can find a separating set whose nodes are all adjacent to each other, i.e., a complete
separating set.

Theorem 3 A dependency model M is isomorphic to a chordal graph if, and only if, it
satisfies the axioms C1{C5 and C8.

Proof: Let us prove the necessary condition. As the graph G associated to M is chordal,

from Theorem 2 we know that the properties C1{C5 and C6 hold.
Let us suppose that C8 does not hold. Then, there are ff and fi , such that I (ff; fi jU n
fff; fig) but for all W  U n fff; fig it is either :I (ff; fijW ), or jW j > 1 and 9;  2 W such
that I (;  jU n f;  g).
Let W0 be any separating set of minimal size for ff and fi , i.e., I (ff; fi jW0) and :I (ff; fi jS )
8S  W0 (we know that at least one separating set of this type has to exist, because
I (ff; fijU n fff; fig) holds). Then, we can deduce that jW0j > 1 and 9;  2 W0 such that
I (; jU n f;  g). Let us define Z = W0 n f;  g. Thus, we have I (ff; fi jZ [  [ ) and
I (; jU n f;  g) and, by applying C6, we obtain either I (ff; fi jZ [  ) or I (ff; fi jZ [ ), i.e.,
I (ff; fijW0 n fg) or I (ff; fijW0 n f g), which contradicts the minimality of W0 . Therefore,
C8 has to be true.
To prove the sucient condition, let us suppose that G is not chordal. Then, there is a
cycle t1 t2 : : :tn,1 tn t1 , n  4, without a chord. So, the nodes t1 and tn,1 are not adjacent,
hence they are separated, and let W be any separating set of t1 and tn,1 , i.e., satisfying
I (t1; tn,1 jW ). Then tn 2 W and ft2; : : :; tn,2 g \ W 6= ;, otherwise we could find a chain
linking t1 and tn,1 which would not be blocked by W , thus contradicting I (t1 ; tn,1 jW ).
So, every separating set W contains tn and some ti ; 2  i  n , 2, hence jW j > 1. Now, by
applying C8, we deduce that :I (tn ; tijU n ftn ; ti g), i.e., tn and ti are adjacent nodes, which
contradicts the assumption that the cycle had no chord. Then, the conclusion is that the
graph has to be chordal.
296

fiCharacterizations of Decomposable Dependency Models

5. Relationships with other Characterizations of Decomposable Models
There is a characterization of decomposable models1 (Lauritzen, 1989) which is quite related
to ours: an undirected graph is chordal if, and only if, every subset of nodes that separates
any two nodes ff and fi and is minimal is complete.
In order to rewrite this result using our notation, let us consider the following axiom:
(C9) Completeness:
(I (ff; fi jZ ) and :I (ff; fi jS ) 8S  Z ) jZ j  1 or :I (;  jU n f;  g) 8;  2 Z ) 8ff; fi 2
U 8Z  U n fff; fig.
Axiom C9 says exactly that any minimal separator of ff and fi has to be complete. An
equivalent formulation of this axiom reads: any separator of ff and fi which is not complete
cannot be minimal. In symbols:
(C9') Completeness:
(I (ff; fi jZ [  [  ) and I (;  jU nf;  g) ) 9W  Z [  [  such that I (ff; fi jW )) 8ff; fi; ;  2

U 8Z  U n fff; fi; ; g:

Then, Lauritzen's result can be reformulated as follows: A dependency model M is
isomorphic to a chordal graph if, and only if, it satisfies the axioms C1{C5 and either C9
or C9'.
Note the similarity between C9 and C8 and between C9' and C6. Taking into account
Theorems 2 and 3, we can deduce that all these axioms, C6, C8, C9 and C9', are equivalent
among each other (assuming that C1{C5 hold). However, this equivalence is not evident, in
spite of the similarities among axioms: it is clear that C6 implies C9' and C9 implies C8, but
the opposite implications are not obvious. In fact, strong chordality and clique-separability
seem stronger and weaker, respectively, than completeness. This becomes clearer if we
express the axioms in the following way: Assuming that two nodes ff and fi can be separated:

 Completeness (C9 or C9'): If a separator of ff and fi is minimal, then it is complete;
or, equivalently, if a separator of ff and fi is not complete, then it has a proper subset
which is still a separator of ff and fi .
 Clique-separability (C8): There exists a separator of ff and fi which is complete.
 Strong chordality (C6): If a separator of ff and fi is not complete, then it has a proper
subset which is still a separator of ff and fi ; moreover, we can find this subset by
removing, from the initial separator, one of the nodes causing its incompleteness.

Observe that both C6 and C9' share the same antecedent, but the consequent of C9' only
says that there exists a separator, whereas the consequent of C6 gives more information
about the identity of this separator. Note also that both C8 and C9 assert the existence of
a complete separator, but C9 requires a previous condition (minimality) and C8 does not.
1. The existence of this result was pointed out to me by a reviewer.

297

fiDe Campos

6. Concluding Remarks
We have found two new characterizations of the class of decomposable dependency models,
in terms of properties of independence relationships. We believe that these results are
theoretically interesting, because they provide a new perspective of this important and well
studied class of graphical models. Moreover, our results are quite concise, since only one
property has to be added to the set of properties characterizing independence relationships
in undirected graphs. They could also be useful for proving results about models of this
sort.
From a more practical point of view, the axiomatic characterizations create desiderata
that could drive automated construction of chordal graphs from data. As we have already
commented, practical use of graphical models and, particularly, of bayesian networks, requires that the dag representing the model be transformed into a chordal graph. From the
perspective of learning models from data, it may be interesting to estimate directly the
chordal graph from the available data, instead of first learning the dag and after converting it into a chordal graph. We believe that the basic independence properties of chordal
graphs identified by our theoretical study, C6 and C8, could guide us in the design of efficient algorithms for learning chordal graphs. It is known that the problem of learning
bayesian networks from data is computationally very complex. For example, some algorithms (Spirtes, Glymour, & Scheines, 1993) start from a complete undirected graph, and
then try to remove edges by testing for conditional independence between the linked nodes,
but using conditioning sets as small as possible (thus reducing the complexity and increasing
reliability). In this context, if we rewrite the property C6 in the following way:

:I (ff; fijZ [  ) and :I (ff; fijZ [ ) and I (ff; fijZ [  [ ) ) :I (; jU n f; g);
then we could use it as a rule that simultaneously allows us to remove the edge ff{fi from
the current graph, and to fix the edge  { as a true edge in the graph.
Similarly, the property C8 could give rise to the following rule: if we are trying to remove
an edge ff{fi from the current graph, by testing conditional independence statements like
I (ff; fijW ), then discard as candidate separating sets those sets W whose nodes are not all
adjacent to each other.
This topic of designing ecient algorithms for learning chordal graphs will be the object
of future research.

Acknowledgements
This work has been supported by the Spanish Comision Interministerial de Ciencia y Tecnologa (CICYT) under Project n. TIC96-0781. I would like to thank Milan Studeny and
three anonymous reviewers for helpful comments and suggestions. I am particularly grateful to the reviewer who pointed out to me the existence of Lauritzen's characterization of
chordal graphs.
298

fiCharacterizations of Decomposable Dependency Models

References

Beeri, C., Fagin, R., Maier, D., & Yannakakis, M. (1983). On the desirability of acyclic
database schemes. JACM, 30, 479{513.
Dawid, A. P. (1979). Conditional independence in statistical theory. J.R. Statist. Soc. Ser.
B, 41, 1{31.
de Campos, L. M. (1995). Independence relationships in possibility theory and their application to learning belief networks. In Della Riccia, G., Kruse, R., & Viertl, R.
(Eds.), Mathematical and Statistical Methods in Artificial Intelligence, CISM Courses
and Lectures 363, pp. 119{130. Wien: Springer Verlag.
de Campos, L. M. (1996). Independency relationships and learning algorithms for singly
connected networks. Tech. rep. DECSAI 960204, University of Granada.
Geiger, D. (1987). The non-axiomatizability of dependencies in directed acyclic graphs.
Tech. rep. R-83, Cognitive Systems Laboratory, UCLA.
Haberman, S. J. (1974). The Analysis of Frequency Data. Chicago: University of Chicago
Press.
Lauritzen, S. L. (1982). Lectures on Contingency Tables (2nd ed.). Aalborg: University of
Aalborg Press.
Lauritzen, S. L. (1989). Mixed graphical association models. Scand. J. Statist., 16, 273{306.
Lauritzen, S. L., Dawid, A. P., Larsen, B. N., & Leimer, H. G. (1990). Independence
properties of directed markov fields. Networks, 20, 491{505.
Lauritzen, S. L., Speed, T. P., & Vijayan, K. (1984). Decomposable graphs and hypergraphs.
J. Autral. Math. Soc. A, 36, 12{29.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computations with probabilities on
graphical structures and their application to expert systems. J.R. Statist. Soc. Ser.
B, 50, 157{224.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. San Mateo: Morgan and Kaufmann.
Pearl, J., & Paz, A. (1985). Graphoids: A graph-based logic for reasoning about relevance
relations. Tech. rep. 850038 (R-53-L), Cognitive Systems Laboratory, UCLA.
Rose, D. J. (1970). Triangulated graphs and the elimination process. Journal of Mathematical Analysis and Applications, 32, 597{609.
Shenoy, P. P. (1992). Conditional independence in uncertainty theories. In Dubois, D.,
Wellman, M. P., D'Ambrosio, B., & Smets, P. (Eds.), Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence, pp. 284{291. San Mateo: Morgan
and Kaufmann.
299

fiDe Campos

Spirtes, P., Glymour, C., & Scheines, R. (1993). Causation, Prediction and Search. Lecture
Notes in Statistics 81. New York: Springer Verlag.
Verma, T., & Pearl, J. (1990). Causal networks: Semantics and expressiveness. In Shachter,
R. D., Levitt, T. S., Kanal, L. N., & Lemmer, J. (Eds.), Uncertainty in Artificial
Intelligence, 4, pp. 69{76. Amsterdam: North-Holland.
Wermuth, N., & Lauritzen, S. L. (1983). Graphical and recursive models for contingency
tables. Biometrika, 70, 537{552.
Whittaker, J. (1991). Graphical Models in Applied Multivariate Statistics. Chichester:
Wiley.

300

fiJournal of Artificial Intelligence Research 5 (1996) 95{137

Submitted 3/96; published 9/96

Accelerating Partial-Order Planners: Some Techniques for
Effective Search Control and Pruning
Alfonso Gerevini

gerevini@ing.unibs.it

Lenhart Schubert

schubert@cs.rochester.edu

Dipartimento di Elettronica per l'Automazione, Universita di Brescia
Via Branze 38, I-25123 Brescia, Italy
Department of Computer Science, University of Rochester
Rochester, NY 14627-0226, USA

Abstract

We propose some domain-independent techniques for bringing well-founded partialorder planners closer to practicality. The first two techniques are aimed at improving
search control while keeping overhead costs low. One is based on a simple adjustment to
the default A* heuristic used by ucpop to select plans for refinement. The other is based
on preferring \zero commitment" (forced) plan refinements whenever possible, and using
LIFO prioritization otherwise. A more radical technique is the use of operator parameter
domains to prune search. These domains are initially computed from the definitions of
the operators and the initial and goal conditions, using a polynomial-time algorithm that
propagates sets of constants through the operator graph, starting in the initial conditions.
During planning, parameter domains can be used to prune nonviable operator instances and
to remove spurious clobbering threats. In experiments based on modifications of ucpop,
our improved plan and goal selection strategies gave speedups by factors ranging from 5
to more than 1000 for a variety of problems that are nontrivial for the unmodified version.
Crucially, the hardest problems gave the greatest improvements. The pruning technique
based on parameter domains often gave speedups by an order of magnitude or more for
dicult problems, both with the default ucpop search strategy and with our improved
strategy. The Lisp code for our techniques and for the test problems is provided in on-line
appendices.

1. Introduction

We are concerned here with improving the performance of \well-founded" domain-independent planners { planners that permit proofs of soundness, completeness, or other desirable
theoretical properties. A state-of-the-art example of such a planner is ucpop (Barrett
et al., 1994; Penberthy & Weld, 1992), whose intellectual ancestry includes strips (Fikes &
Nilsson, 1971), tweak (Chapman, 1987), and snlp (McAllester & Rosenblitt, 1991). Such
planners unfortunately do not perform well at present, in comparison with more practically
oriented planners such as sipe (Wilkins, 1988), prs (Georgeff & Lansky, 1987), or O-Plan
(Currie & Tate, 1991).
However, there appear to be ample opportunities for bringing well-founded planners
closer to practicality. In the following, we begin by suggesting some improvements to
search control in planning, based on more carefully formulated strategies for selecting partial
plans for refinement, and for choosing open conditions in a selected partial plan. Our planc 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiGerevini & Schubert
selection strategy uses S+OC { the number of steps in a plan plus the number of open
conditions still to be established { as a heuristic measure for ucpop's A* search of the
plan space. (Addition of an attenuated term reecting the number of threats or \unsafe
conditions" UC, such as 0.1UC, is sometimes advantageous.)1 Our aw-selection strategy,
which we term ZLIFO, prefers \zero commitment" plan refinements to others, and otherwise
uses a LIFO (stack) discipline. Zero commitment refinements are logically necessary ones:
they either eliminate a plan altogether because it contains an irremediable aw, or they add
a unique step or unique causal link (from the initial state) to establish an open condition
that cannot be established in any other way. The strategy is closely related to ones proposed
by Peot & Smith (1993) and Joslin & Pollack (1994) but generally appears to perform better
than either.
We describe these two classes of techniques in Section 2 below, and in Section 3 we
report our experimental results based on slightly modified versions of ucpop.2 For the
more dicult problems taken from the available ucpop test suite and elsewhere, we obtain
improvements by factors ranging from 5 to more than 1000, with the hardest problems
giving the greatest improvements.
We then turn to our proposal for using computed operator parameter domains during
planning. In particular, in Section 4 we motivate and describe a method of precomputing parameter domains based on propagating sets of constants forward from the initial
conditions.3 The process is iterative, but the algorithm runs within a time bound that is
polynomial in the size of the problem specification. We provide details of the algorithm,
along with theorems about its correctness and tractability, in Sections 4.2{4.3 and Online
Appendix 1.
In Section 5 we show how to use parameter domain information in a ucpop-style planner. During planning, parameter domains can be used to prune operator instances whose
parameter domains are inconsistent with binding constraints, and to eliminate spurious
threats that cannot, in fact, be realized without violating domain constraints. We illustrate
the effectiveness of this technique with examples drawn from the ucpop test suite as well as
from the trains transportation planning world developed at Rochester (Allen & Schubert,
1991; Allen et al., 1995). In some of these tests, we apply the parameter domain information
in the context of the default ucpop search strategy. We demonstrate significant gains on
most problems, particularly the more challenging ones (e.g., speedups of more than an order
of magnitude for several problems in the strips world, and a more than 900-fold speedup
for a trains problem).
In another set of tests in the trains world, we use our own improved search strategies
as baseline, i.e., we ask whether additional speedups are obtainable by use of parameter
1. The search strategy is described as \A* or IDA*" search in (Penberthy & Weld, 1992); in the code for
ucpop 2.0 it is described more generally as best-first, since arbitrary ranking functions, not necessarily
corresponding to A* heuristics, may be plugged in. But with choices like S+OC or S+OC+UC as
plan-ranking heuristic (as discussed in Section 2.2), it is natural to view the strategy as an A* strategy.
2. While the techniques we describe are applicable to other planners, our focus is on ucpop because it is
well-known and the Lisp code is readily available. The system can be obtained via anonymous ftp from
cs.washington.edu.
3. We hope that the notion of a parameter domain, as a set of admissible bindings (constants), will cause
no confusion with the notion of a planning domain, as a specified set of operators, along with constraints
on admissible initial conditions and goal conditions.

96

fiAccelerating Partial-Order Planners
domains, above those obtainable with the S+OC and ZLIFO search strategies. Our experimental results again show speedups by about a factor of 10 through use of parameter
domains, on top of those obtained by the improved search strategies (the combined speedup
is over 2000).
As evidence that the effectiveness of using parameter domains in combination with our
search strategy is not dependent on some peculiarity of the latter, we also include some
results for ucpop's default strategy, Joslin and Pollack's \least cost aw repair" (LCFR)
strategy (Joslin & Pollack, 1994) and for Peot and Smith's \least commitment" (LC) open
condition selection strategy (Peot & Smith, 1993) in Section 5.
In Section 6, we state our conclusions, comment on some related work and mention
possible extensions of our techniques.

2. Plan Selection and Goal Selection

We will be basing our discussion and experiments on ucpop, an algorithm exemplifying the
state of the art in well-founded partial-order planning. Thus we begin with a sketch of this
algorithm, referring the reader to (Barrett et al., 1994; Penberthy & Weld, 1992) for details.
In the next two subsections we then motivate and describe our improved plan-selection and
goal-selection strategies.

2.1 UCPOP

ucpop uses strips-like operators, with positive or negative preconditions and positive or

negative effects. The initial state consists of positive predications with constant arguments
(if any), and all other ground predications are false by default. Unlike strips, ucpop also
allows conditional effects, expressed by 2-part when-clauses specifying a (possibly complex)
extra condition needed by that effect and the (possibly complex) effect itself. For instance,
an action PUTON(?x ?y ?z) (\put ?x on ?y from ?z") might have conditional effects stating
that when ?y is not the table, it will not be clear at the end of the action, and when z is
not the table, it will be clear at the end of the action. The \U" in ucpop indicates
that universally quantified conditions and effects are permitted as well. For instance, it is
permissible to have a precondition for a PICKUP(?x) action that says that for all ?y, (not
(on ?y ?x)) holds. Universal statements are handled by explicit substitution of domain
constants and need not concern us at this point.
In essence, ucpop explores a space of partially specified plans, each paired with an
agenda of goals still to be satisfied and threats still to be averted. The initial plan contains
a dummy *start* action whose effects are the given initial conditions, and a dummy
*end* action whose preconditions are the given goals. Thus goals are uniformly viewed as
action preconditions, and are uniformly achieved through the effects of actions, including
the *start* action.
The plans themselves consist of a collection of steps (i.e., actions obtained by instantiating the available operators), along with a set of causal links, a set of binding constraints,
and a set of ordering constraints. When an open goal (precondition) is selected from the
agenda, it is established (if possible) either by adding a step with an effect that unifies
with the goal, or by using an existing step with an effect that unifies with the goal. (In
the latter case, it must be consistent with current ordering constraints to place the existing
97

fiGerevini & Schubert
step before the goal, i.e., before the step whose preconditions generated the goal.) When a
new or existing step is used to establish a goal in this way, there are several side effects:
 A causal link (S ; Q; S ) is also added, where S indicates the step \producing" the
goal condition Q and S indicates the step \consuming" Q. This causal link serves to
protect the intended effect of the added (or reused) step from interference by other
steps.
 Binding constraints are added, corresponding to the unifier for the action effect in
question and the goal (precondition) it achieves.
 An ordering constraint is added, placing the step in question before the step whose
precondition it achieves.
 If the action in question is new, its preconditions are added to the agenda as new
goals (except that EQ/NEQ conditions are integrated into the binding constraints { see
below).
 New threats (unsafe conditions) are determined. For a new step and its causal link,
other steps threaten the causal link if they have effects unifiable with the condition
protected by the causal link (and these effects can occur temporally during the causal
link); and the effects of the new step may similarly threaten other causal links. In
either case, new threats are placed on the agenda. It is useful to distinguish definite
threats from potential threats: the former are those in which the unification that
confirmed the threat involved no new binding of variables.
Binding constraints assert the identity (EQ) or nonidentity (NEQ) of two variables or a variable
and a constant. EQ-constraints arise from unifying open goals with action effects, and NEQconstraints arise (i) from NEQ-preconditions of newly instantiated actions, (ii) from matching
negative goals containing variables to the initial state, and (iii) from averting threats by
\separation", i.e., forcing non-equality of two variables or a variable and a constant that
were unified in threat detection. NEQ-constraints may be disjunctive, but are handled simply
by generating separate plans for each disjunct.
The overall control loop of ucpop consists of selecting a plan from the current list of
plans (initially the single plan based on *start* and *end*), selecting a goal or threat from
its agenda, and replacing the plan by the corresponding refined plans. If the agenda item is
a goal, the refined plans are those corresponding to all ways of establishing the goal using
a new or existing step. If the agenda item is a definite threat to a causal link (S ; Q; S ),
then there are at most three refined plans. Two of these constrain the threatening step
to be before step S (demotion) or after step S (promotion), thus averting the threat.
A third possibility arises if the effect threatening (S ; Q; S ) is a conditional effect of the
threatening action. Such a conditional threat can be averted by creating a goal denying
some precondition needed by the conditional effect.
ucpop has a \delay separation" switch, *d-sep*, and when this is turned on, only
definite threats are dealt with. Note that potential threats may become definite as a result
of added binding constraints. (They may also \expire" as a result of added binding and
ordering constraints, i.e., the threatening effect may no longer unify with the threatened
condition or it may be forced to occur before or after the threatened causal link. Expired
p

c

p

c

p

p

c

p

98

c

c

fiAccelerating Partial-Order Planners
threats are removed from the agenda when selected.) When *d-sep* is off, potential threats
as well as definite ones are averted, with separation as an additional method of doing so
besides the three methods above.
Inconsistencies in binding constraints and ordering constraints are detected when they
first occur (as a result of adding a new constraint) and the corresponding plans are eliminated. Planning fails if no plans remain. The success condition is the creation of a plan
with consistent binding and ordering constraints and an empty agenda.
The allowance for conditional effects and universal conditions and effects causes only
minor perturbations in the operation of ucpop. For instance, conditional effects can lead
to multiple matches against operators for a given goal, each match generating different
preconditions. (Of course, there can be multiple matches even without conditional effects,
if some predicates occur more than once in the effects.)
The key issues for us right now are the strategic ones: how plans are selected from the
current set of plans (discussed in Section 2.2), and how goals are selected for a given plan
(discussed in Section 2.3).

2.2 The Trouble with Counting Unsafe Conditions

The choice of the next plan to refine in the ucpop system is based on an A* best-first
search. Recall that A* uses a heuristic estimate f (p) of overall solution cost consisting of
a part g (p) = cost of the current partial solution (plan) p and a part h(p) = estimate of
the additional cost of the best complete solution that extends p. In the current context it
is helpful to think of f (p) as a measure of plan complexity, i.e., \good" plans are simple
(low-complexity) plans.
There are two points of which the reader should be reminded. First, in order for A*
to guarantee discovery of an optimal plan (i.e., the \admissibility" condition), h(p) should
not overestimate the remaining solution cost (Nilsson, 1980). Second, if the aim is not
necessarily to find an optimal solution but to find a satisfactory solution quickly, then f (p)
can be augmented to include a term that estimates the remaining cost of finding a solution.
One common way of doing that is to use a term proportional to h(p) for this as well, i.e.,
we emphasize the h-component of f relative to the g -component. This is reasonable to the
extent that the plans that are most nearly complete (indicated by a low h-value) are likely
to take the least effort to complete. Thus we will prefer to pursue a plan p0 that seems closer
to being complete to a plan p further from completion, even though the overall complexity
estimate for p0 may be greater than for p (Nilsson, 1980) (pages 87{88). Alternatively, we
could add a heuristic estimate of the remaining cost of finding a solution to f (p) that is
more or less independent of the estimate h(p).
With these considerations in mind, we now evaluate the advisability of including the
various terms in ucpop's function for guiding its A* search, namely
S, OC, CL, and UC,
where S is the number of steps in the partial plan, OC is the number of open conditions
(unsatisfied goals and preconditions), CL is the number of causal links, and UC is the
number of unsafe conditions (the number of pairs of steps and causal links where the step
99

fiGerevini & Schubert
threatens the causal link). The default combination used by ucpop is S+OC+UC.4 This
becomes S+OC+UC+F if special open conditions called \facts" are present. These are
conditions that are not state-dependent (e.g., a numerical relation like (add-one ?x ?y), or
a geometrical one like (loc-in-room ?x ?y ?room)) and are established by Lisp functions
(Barrett et al., 1994). Since few of our test problems involved facts, we will not discuss the
F term further except to say that we followed the ucpop default strategy of including this
term where it is relevant (see the TileWorld problems in Section 3.2 and also some remarks
in Section 5.2 in connection with the parameter-domain experiments).
2.2.1 S: the number of steps currently in the plan

This can naturally be viewed as comprising g (p), the plan complexity so far. Intuitively, a
plan is complex to the extent that it contains many steps. While in some domains we might
want to make distinctions among the costs of different kinds of steps, a simple step count
seems like a reasonable generic complexity measure.
2.2.2 OC: the number of open conditions

This can be viewed as playing the role of h(p), since each remaining open condition must be
established by some step. The catch is that it may be possible to use existing steps in the
plan (including *start*, i.e., the initial conditions) to establish remaining open conditions.
Thus OC can overestimate the number of steps still to be added, forfeiting admissibility.
Despite this criticism, several considerations favor retention of the OC term. First, a
better estimator of residual plan complexity seems hard to come by. Perhaps one could
modify OC by discounting open conditions that are matched by existing actions, but this
presumes that all such open conditions can actually be achieved by action re-use, which is
improbable if there are remaining threats, or remaining goals requiring new steps.5 Second,
the possibility that OC will overestimate the residual plan complexity will rarely be actualized, since typically further steps still need to be added to achieve some of the goals, and
those steps will typically introduce further open conditions again requiring new steps. Finally, to the extent that OC does at times overestimate the residual plan complexity, it can
be viewed as emphasizing the the h(p) term of f (p), thus promoting faster problem-solving
as explained above.
2.2.3 CL: the number of causal links

One might motivate the inclusion of this term by arguing that numerous causal links are
indicative of a complex plan. As such, CL appears to be an alternative to step-counting.
4. This is in no way the \recommended" strategy. The ucpop implementation makes available various
options for controlling search, to be used at the discretion of experimenters. Our present work has
prompted the incorporation of our particular strategies as an option in ucpop 4.0.
5. Note that threats and remaining goals impose constraints that may not be consistent with seemingly
possible instances of action re-use. This is clear enough for threats, which often imply temporal ordering
constraints inconsistent with re-use of an action. It is also fairly clear for remaining goals. For instance,
in Towers of Hanoi the small disk D1 is initially on the medium disk D2, which in turn is on the big disk
D3, and D3 is on peg P1. The goal is to move the tower to the third peg P3, so it seems to ucpop initially
as if (on D1 D2) and (on D2 D3) could be achieved by \re-use" of *start*. However, the third goal (on
D3 P3) implies that various actions must be added to the plan which are inconsistent with those two
seemingly possible instances of action re-use.

100

fiAccelerating Partial-Order Planners
However, note that CL is in general larger than S, since every step of a plan establishes
at least one open condition and thus introduces at least one causal link. The larger CL is
relative to S, the more subgoals are achieved by action re-use. Hence, if we use CL instead
of (or in addition to) S in the g (p) term, we would in effect be saying that achieving multiple
subgoals with a single step is undesirable; we would tend to search for ways of achieving
multiple goals with multiple steps, even when they can be achieved with a single step. This
is clearly not a good idea, and justifies the exclusion of CL from f (p).
2.2.4 UC: the number of unsafe conditions

We note first of all that this is clearly not a g -measure. While the number of threats will
tend to increase if we establish more and more subgoals without curtailing threats, threats
as such are not elements of the plan being constructed and so do not contribute to its
complexity. In fact, when the plan is done all threats will be gone.
Can UC then be viewed as an h-measure? One argument of sorts for the armative is
the following. Not all partial plans are expandable into complete plans, and a high value of
UC makes it more likely that the partial plan contains irresolvable conicts. If we regard
impossible plans as having infinite cost, then inclusion of a term increasing with UC as part
of the h-measure is reasonable. This carries a serious risk, though, since in the case where
the partial plan does have a consistent completion (despite a high UC-count), inclusion of
such a term can greatly overestimate the residual plan complexity.
Another possible armative argument is that conditional threats are sometimes resolved
by \confrontation", which introduces a new goal denying a condition required for the threatening conditional effect. This new goal may in turn require new steps for its achievement,
adding to the plan complexity. However, this link to complexity is very tenuous. In the first
place, many of the ucpop test domains involve no conditional effects, and threat removal
by promotion, demotion or separation adds no steps. Even when conditional effects are
present, many unconditional as well as conditional threats are averted by these methods.
Furthermore, UC could swamp all other terms since threats may appear and expire in
groups of size O(n), where n is the number of steps in the plan. For instance, consider
a partial plan that involves moves by a robot R to locations L1, ..., Ln, so that there
are n causal links labeled (at R L1), ..., (at R Ln). If a new move to location L is
now added, initially with an indefinite point of departure ?x, this produces effects (at
R L) and (not (at R ?x)). The latter can threaten all of the above n causal links, at
least if the new move is at first temporally unordered relative to the n existing moves. If
this new action subsequently happens to be demoted so as to precede the first move (or
promoted so as to follow the last), or if ?x becomes bound to a constant distinct from
L1, ..., Ln, all n threats expire. Keeping in mind that different steps in a plan may
have similar effects, we can see that half of the steps could threaten the causal links of the
others. In such a case we could have O(n2 ) unsafe conditions, destined to expire as a result
of O(n) promotions/demotions. In fact even a single new binding constraint may cause
O(n2 ) threats to expire. For instance, if there are n=2 effects (not (P ?x)) threatening
n=2 causal links labeled (P ?y), then if binding constraint (NEQ ?x ?y) is added, all n2 =4
threats expire. Recall that when expired threats are selected from the agenda by ucpop,
they are recognized as such and discarded without further action.
101

fiGerevini & Schubert
Our conclusion is that it would be a mistake to include UC in full in a general h-measure,
though some increasing function of UC that remains small enough not to mask OC may be
worth including in h.
Finally, can UC be regarded as a measure of the remaining cost of finding a solution?
Here, similar arguments to those above apply. On the armative side, we can argue that
a high value of UC indicates that we may be facing a combinatorially explosive, timeconsuming search for a set of promotions and demotions that produce a conict-free step
ordering. In other words, a high value of UC may indicate a high residual problem-solving
cost. (And at the end of such a search, we may still lack a solution, if no viable step
ordering exists.) On the other hand, we have already noted that unsafe conditions include
many possible conicts which may expire as a result of subsequent partial ordering choices
and variable binding choices not specifically aimed at removing these conicts. So counting
unsafe conditions can arbitrarily overestimate the number of genuine refinement steps, and
hence the problem-solving effort, still needed to complete the plan.
So UC is scarcely more trustworthy as a measure of residual planning cost than as a
measure of residual plan cost.
Thus we conclude that the most promising general heuristic measure for plan selection is
S+OC, possibly augmented with an attenuated form of the UC term that will not dominate
the S+OC component. (For instance, one might add a small fraction of the term, such as
UC/10, or more subtly { to avoid swamping by a quadratic component { a term proportional
to UC 5.)
:

2.3 The Goal Selection Strategy

An important opportunity for improving planning performance independently of the domain
lies in identifying forced refinements, i.e., refinements that can be made deterministically.
Specifically, in considering possible refinements of a given partial plan, it makes sense to
give top priority to open conditions that cannot be achieved; and then preferring open
conditions that can be achieved in only one way { either through addition of an action not
yet in the plan, or through a unique match against the initial conditions.
The argument for giving top priority to unachievable goals is just that a plan containing
such goals can be eliminated at once. Thus we prevent allocation of effort to the refinement
of doomed plans, and to the generation and refinement of their doomed successor plans.
The argument for preferring open conditions that can be achieved in only one way
is equally apparent. Since every open condition must eventually be established by some
action, it follows that if this action is unique, it must be part of every possible completion
of the partial plan under consideration. So, adding the action is a \zero-commitment"
refinement, involving no choices or guesswork. At the same time, adding any refinement in
general narrows down the search space by adding binding constraints, ordering constraints
and threats, which constrain both existing steps and subsequently added steps. For unique
refinements this narrowing-down is monotonic, never needing revocation. For example,
suppose some refinement happens to add constraints that eliminate a certain action instance
A as a possible way of achieving a certain open condition C . If the refinement is unique,
then we are assured that no completion of the plan contains A as a way of establishing C .
If it is not unique, we have no such assurance, since some alternative refinement may be
102

fiAccelerating Partial-Order Planners
compatible with the use of A to achieve C . In short, the zero-commitment strategy cuts
down the search space without loss of access to viable solutions.
Peot and Smith (1993) studied the strategy of preferring forced threats to unforced
threats, and also used a \least commitment" (LC) strategy for handling open conditions.
Least commitment always selects an open condition which generates the fewest refined
plans. Thus it entails the priorities for unachievable and uniquely achievable goals above
(while also entailing a certain prioritization of nonuniquely achievable goals). Joslin and
Pollack (1994) studied the uniform application of such a strategy to both threats and open
conditions in ucpop, terming this strategy \least cost aw repair" (LCFR). Combining this
with ucpop's default plan selection strategy, they obtained significant search reductions
(though less significant running time reductions, mainly for implementation reasons, but
also because of the intrinsic overhead of computing the \repair costs") for a majority of the
problems in the ucpop test suite.
Joslin & Pollack (1994) and subsequently Srinivasan & Howe (1995) proposed some
variants of LCFR designed to reduce the overhead incurred by LCFR for aw selection.
These strategies employ various assumptions about the aw repair costs, allowing the more
arduous forms of cost estimation (requiring look-ahead generation of plans) to be confined
to a subset of the aws in the plan, while for the rest an approximation is used that does
not significantly increase the overhead. Both teams obtained quite significant reductions
in overhead costs in many cases, e.g., by factors ranging from about 3 to about 20 for the
more dicult problems. However, overall performance was sometimes adversely affected.
Joslin and Pollack found that their variant (QLCFR) solved fewer problems than LCFR,
because of an increase in the number of plans generated in some cases. Each of Srinivasan &
Howe's four strategies did slightly better than LCFR in some of their 10 problem domains
but significantly worse in others. In terms of plans examined during the search, their best
overall strategy, which uses similar action instances for similar aws, did slightly better on
4 of the domains, slightly worse on 4, and significantly worse on 2 (and in those cases the
number of plans examined was also more than a factor of 20 above that of default ucpop).
In the unmodified form of ucpop, goals are selected from the agenda according to a
LIFO (last-in first-out, i.e., stack) discipline. Based on experience with search processes
in AI in general, such a strategy has much to recommend it, as a simple default. In the
first place, its overhead cost is low compared to strategies that use heuristic evaluation or
lookahead to prioritize goals. As well, it will tend to maintain focus on the achievement of
a particular higher-level goal by regression { very much as in Prolog goal chaining { rather
than attempting to achieve multiple goals in breadth-first fashion.
Maintaining focus on a single goal should be advantageous at least when some of the
goals to be achieved are independent. For instance, suppose that two goals G1 and G2 can
both be achieved in various ways, but choosing a particular method of achieving G1 does
not rule out any of the methods of achieving G2. Then if we maintain focus on G1 until
it is solved, before attempting G2, the total cost of solving both goals will just be the sum
of the costs of solving them individually. But if we switch back and forth, and solutions
of both goals involve searches that encounter many dead ends, the combined cost can be
much larger. This is because we will tend to search any unsolvable subtree of the G1 search
tree repeatedly, in combination with various alternatives in the G2 search tree (and vice
versa). This argument should still have some validity even if G1 and G2 are not entirely
103

fiGerevini & Schubert
independent; i.e., as long as G1 gives rise to subproblems that tend to fail in the same
way regardless of choices made in the attempt to solve G2 (or vice versa), then shifting
attention between G1 and G2 will tend to generate a set of partial plans that unnecessarily
\cross-multiplies" alternatives.
We have therefore chosen to stay with ucpop's LIFO strategy whenever there are no
zero commitment choices. This has led to very substantial improvements over LCFR in our
experiments.
Thus our strategy, which we term ZLIFO (\zero-commitment last-in first-out"), chooses
the next aw according to the following preferences:
1. a definite threat (*d-sep* is turned on), using LIFO to pick among these;
2. an open condition that cannot be established in any way;
3. an open condition that can be resolved in only one way, preferring open conditions
that can be established by introducing a new action to those that can be established
by using *start*;6
4. an open condition, using LIFO to pick among these.
Hence the overhead incurred by ZLIFO for aw selection is limited to the open conditions, and is lower for these than the overhead incurred by LCFR. Furthermore, it can
also be significantly lower in practice than the overhead incurred by LC, because testing
whether an OC is not a zero-commitment choice (i.e., whether it can be established in more
than one way) is less expensive than computing the total number of ways to achieve it.
In Online Appendix 1 we give the pseudocode of ZLIFO for the selection of the open
condition (preferences 2{4). Very recently this implementation has also been packaged into
ucpop 4.0, a new version of ucpop which is available by anonymous ftp to cs.washington.edu.

3. Experiments Using UCPOP

In order to test our ideas we modified version 2.0 of ucpop (Barrett et al., 1994), replacing its default plan-selection strategy (S+OC+UC) and goal-selection strategy (LIFO) to
incorporate strategies discussed in the previous sections.
We tested the modified planner on several problems in the ucpop suite, emphasizing
those that had proved most challenging for previous strategies, on some artificial problems
due to Kambhampati et al. (1995), in the trains transportation domain developed in
Rochester (Allen & Schubert, 1991; Allen et al., 1995), and in Joslin & Pollack's TileWorld
domain (Joslin & Pollack, 1994). We briey describe the test problems and the platforms
and parameter settings we used, and then present the experimental results for our improved
search strategies.
6. 2. and 3. are zero-commitment choices. In our experiments, which are described in the next section, the
sub-preference in 3. gave improvements in the context of Russell's tire changing domain (in particular
with Fix3), without significant deterioration of performance in the other domains.

104

fiAccelerating Partial-Order Planners

3.1 Test Problems and Experimental Settings

The ucpop problems include Towers of Hanoi (T of H), Fixa, Fix3, Fixit, Tower-Invert4,
Test-Ferry, and Sussman-Anomaly. In the case of T of H, we added a 3-operator version to
the ucpop single-operator version, since T of H is a particularly hard problem for ucpop
and its diculty has long been known to be sensitive to the formalization (e.g., (Green,
1969)). Fixa is a problem from Dan Weld's \fridge domain", in which the compressor
in the fridge is to be exchanged, requiring unscrewing several screws, stopping the fridge,
removing the backplane, and making the exchange. Fix3 is from Stuart Russell's \at tire
domain", where a new wheel is to be mounted and lowered to the ground (the old wheel has
been jacked up already and the nuts loosened); this requires unscrewing the nuts holding
the old wheel, removing the wheel, putting on the new wheel, screwing on the nuts, jacking
down the hub, and tightening the nuts. Fixit is more complicated, as the wheel is not yet
jacked up initially and the nuts not yet loosened, the spare tire needs to be inated, and
the jack, wrench and pump all need to be taken out of the trunk and stowed again at the
end. Tower-Invert4 is a problem in the blocks world, requiring the topmost block in a stack
of four blocks to be made bottom-most. Test-Ferry is a simple problem requiring two cars
to be moved from A to B using a one-car ferry, by boarding, sailing, and unboarding for
each car.
The artificial problems correspond to two parameter settings for ART-# -# , one
of the two artificial domains that served as a testbed for Kambhampati et al.'s extensive
study of the behavior of various planning strategies as a function of problem parameters
(Kambhampati et al., 1995). ART-# -# provides two layers of 10 operators each,
where those in layer 1 achieve the preconditions of those in layer 2, and each operator in
layer 2 achieves one of the 10 goals. However, some operators in each layer can establish
or clobber the preconditions of their neighbors, and this can force operators to be used in
a certain order.
The version of the trains domain that we encoded involves four cities (Avon, Bath,
Corning, Dansville) connected by four tracks in a diamond pattern, with a fifth city (Elmira)
connected to Corning by a fifth track. The available resources, which are located at various
cities, consist of a banana warehouse, an orange warehouse, an orange juice factory, three
train engines (not coupled to any cars), 4 boxcars (suitable for transporting oranges or
bananas), and a tanker car (suitable for transporting orange juice). Goals are typically to
deliver oranges, bananas, or orange juice to some city, requiring engine-car coupling, car
loading and unloading, engine driving, and possibly OJ-manufacture.
The TileWorld domain consists of a grid on which holes and tiles are scattered. A given
tile may or may not fit into a particular hole. The goals are to fill one or more holes by
using three possible actions: picking up a tile, going to an x-y location on the grid, and
dropping a tile into a hole. The agent can carry at most four tiles at a time.
Formalizations of these domains in terms of ucpop's language are provided in Online
Appendix 2. The experiments for all problems except Fixit, the trains problems and the
TileWorld problems were conducted on a sun 10 using Lucid Common Lisp 4.0.0, while
the rest (Tables X{XI in the next subsection) were conducted on a sun 20 using Allegro
Common Lisp 4.2. Judging from some repeated experiments, we do not think that the
est

est

clob

105

clob

fiGerevini & Schubert

Goal-selection Plan-selection CPU sec
LIFO
LIFO
ZLIFO
ZLIFO

S+OC+UC
S+OC
S+OC+UC
S+OC

204.51
0.97
6.90
0.54

Plans

160,911/107,649
751/511
1816/1291
253/184

Table I: Performance of plan/goal selection strategies on T-of-H1
differences in the platforms significantly impact performance improvements.7 Among the
search control functions provided by ucpop, we used the default bestf-search when the
problem was solvable within the search limit of 40,000 plans generated, while we used the
function id-bf-search (an implementation of the linear-space best-first search algorithm
given by Korf, 1992), when this limit was exceeded.8 In all of the experiments the delayseparation switch, *d-sep*, was on, except for those using the LCFR strategy.

3.2 Experimental Results for ZLIFO and S+OC

Tables I{XI show the CPU time (seconds) and the number of plans created/explored by
ucpop on twelve problems in the domains described above: Towers of Hanoi with three
disks and either one operator (T-of-H1) or three operators (T-of-H3), the fridge domain
(Fixa), the tire changing domain (Fix3 and Fixit), the blocks world (Tower-Invert4 and
Sussman-anomaly), the ferry domain (Test-Ferry), the artificial domain ART-# -#
(specifically, ART-3-6 and ART-6-3), the trains domain (Trains1, Trains2 and Trains3)
and the TileWorld domain (tw-1, ..., tw-6). Both the number of plans created/explored and
the CPU time are important performance measures. The number of plans, which indicates
search space size, is a more stable measure in the sense that it depends only on the search
algorithm, not the implementation.9 But the time is still of interest since an improvement
in search may have been purchased at the price of a more time-consuming evaluation of
alternatives. It turns out that we do pay some price in overhead when we substitute our
strategies for the defaults (factors ranging from about 1.2 to 1.9, and rarely higher, per plan
created). This may be due to slightly greater inherent complexity of ZLIFO versus LIFO,
but we think the differences could be reduced by substituting modified data structures for
those of ucpop { we were committed to not altering these.
Tables I and II show that for the T of H the plan selection strategy S+OC gives dramatic
improvements over the default S+OC+UC strategy. (In these tests the default LIFO goal
selection strategy was used.) In fact, ucpop solved T-of-H1 in 0.97 seconds using S+OC
versus 204.5 seconds using S+OC+UC. T-of-H3 proved harder to solve than T-of-H1, reest

clob

7. The differences were the result of what was available at different times and locales over the course of
nearly two years of experimentation.
8. This choice was motivated by the observation that when the problem is relatively easy to solve
bestf-search appears to be more ecient than id-bf-search, while for hard problems it can be very
inecient because of the considerable amount of space used at run time and the CPU time spent on
garbage collection, which in some cases made Lisp crash, reporting an internal error.
9. It is also worth noting that the number of plans created implicitly takes into account plan size, since
addition of a step to a plan is counted as creation of a new plan in ucpop.

106

fiAccelerating Partial-Order Planners

Goal-selection Plan-selection CPU sec Plans
LIFO
S+OC+UC
> 600 > 500,000
LIFO
ZLIFO
ZLIFO

S+OC
S+OC+UC
S+OC

8.54

> 600
1.24

5506/3415
> 500,000
641/420

Table II: Performance of plan/goal selection strategies on T-of-H3

Goal-selection Plan-selection CPU sec
LIFO
LIFO
ZLIFO
ZLIFO

S+OC+UC
S+OC
S+OC+UC
S+OC

2.45
2.48
0.33
0.33

Plans

2131/1903
2131/1903
96/74
96/74

Table III: Performance of plan/goal selection strategies on Fixa
quiring 8.5 seconds using S+OC and an unknown time in excess of 600 CPU seconds using
S+OC+UC.
Our ZLIFO goal-selection strategy can significantly accelerate planning compared with
the simple LIFO strategy. In particular, when ZLIFO was combined with the S+OC planselection strategy in solving T of H, it further reduced the number of plans generated by a
factor of 3 in T-of-H1 and by a factor of 8 in T-of-H3. The overall performance improvement
for T-of-H1 was thus a factor of 636 in terms of plans created and factor of 379 in terms of
CPU time (from 204.5 to 0.54 seconds).
Tables III{VIII provide data for problems that are easier than T of H, but still challenging to ucpop operating with its default strategy, namely Fixa (Table III), Fix3 (Table IV),
Tower-Invert4 (Table V), Test-Ferry (Table VI) and the artificial domain ART-# -#
with # = 3 and # = 6 (Table VII) and with # = 6 and # = 3 (Table VII).
The results show that the combination of S+OC and ZLIFO substantially improves the
performance of ucpop in comparison with its performance using S+OC+UC and LIFO.
The number of plans generated dropped by a factor of 22 for Fixa, by a factor of 5.9 for
est

est

clob

est

Goal-selection Plan-selection CPU sec
LIFO
LIFO
ZLIFO
ZLIFO

S+OC+UC
S+OC
S+OC+UC
S+OC

6.50
0.43
1.12
1.53

clob

Plans

3396/2071
351/215
357/221
574/373

Table IV: Performance of plan/goal selection strategies on Fix3

107

clob

fiGerevini & Schubert

Goal-selection Plan-selection CPU sec Plans
LIFO
LIFO
ZLIFO
ZLIFO

S+OC+UC
S+OC
S+OC+UC
S+OC

1.35
0.19
2.81
0.36

808/540
148/105
571/378
142/96

Table V: Performance of plan/goal selection strategies on Tower-Invert4

Goal-selection Plan-selection CPU sec Plans
LIFO
LIFO
ZLIFO
ZLIFO

S+OC+UC
S+OC
S+OC+UC
S+OC

0.63
0.32
0.24
0.22

718/457
441/301
136/91
140/93

Table VI: Performance of plan/goal selection strategies on Test-Ferry

Goal-selection Plan-selection CPU sec
LIFO
LIFO
ZLIFO
ZLIFO

S+OC+UC
S+OC
S+OC+UC
S+OC

.67
1.36
0.16
0.18

Plans

568/392
1299/840
72/49
79/54

Table VII: Performance of plan/goal selection strategies on ART-# -#
and # = 6 (averaged over 100 problems)
est

clob

with # = 3
est

clob

Goal-selection Plan-selection CPU sec
LIFO
LIFO
ZLIFO
ZLIFO

S+OC+UC
S+OC
S+OC+UC
S+OC

1.32
2.08
0.14
0.14

Plans

985/653
1743/1043
57/37
57/37

Table VIII: Performance of plan/goal selection strategies on ART-# -#
and # = 3 (averaged over 100 problems)
est

clob

with # = 6

clob

Goal-selection Plan-selection CPU sec Plans
LIFO
LIFO
ZLIFO
ZLIFO

S+OC+UC
S+OC
S+OC+UC
S+OC

0.06
0.04
0.12
0.07

44/26
36/21
67/43
41/25

Table IX: Performance of plan/goal selection strategies on Sussman-anomaly
108

est

fiAccelerating Partial-Order Planners

1000

Fixit
Trains1
2

T-of-H1 2

Performance
Improvement

2

100

2
ART-6-3 2 Fixa
2 Search space reduction
ART-3-6 222
2 Fix3
Speedup
Tower-invert4
Test-ferry
Sussman-anomaly
2
1e+07
100
1000
10000 100000 1e+06
Problem size

10
1
10

Figure 1: Performance improvement due to ZLIFO and S+OC, relative to the number of
plans generated by LIFO and S+OC+UC (log-log scale). The improvements for
the problems that ucpop was unable to solve even with a very high search limit
(Trains2, Trains3, and T-of-H3) are not included.
Fix3, by a factor of 5.7 for Tower-Invert4, by a factor of 5.1 for Test-Ferry, by a factor of 7
for ART-3-6, and by a factor of 17 for ART-6-3.
Concerning ART-# -# , note that the performance we obtained with unenhanced
ucpop (568 plans generated for ART-3-6 and 985 for ART-6-3) was much the same as
(just marginally better than) reported by Kambhampati et al. (1995) for the best planners
considered there (700 { 1500 plans generated for ART-3-6, and 1000-2000 for ART-6-3).
This is to be expected, since ucpop is a generalization of the earlier partial-order planners.
Relative to standard ucpop and its predecessors, our \accelerated" planner is thus an order
of magnitude faster. Interestingly, the entire improvement here can be ascribed to ZLIFO
(rather than S+OC plan selection, which is actually a little worse than S+OC+UC). This
is probably due to the unusual arrangement of operators in ART-# -# into a \clobbering chain" (A , 1 clobbers A , ,1 1 's preconditions, ..., A1 1 clobbers A0 1's preconditions;
similarly for A 2 ), which makes immediate attention to new unsafe conditions an unusually
good strategy.
In experimenting with various combinatorially trivial problems that unmodified ucpop
handles with ease, we found that the S+OC and ZLIFO strategy is neither beneficial nor
harmful in general; there may be a slight improvement or a slight degradation in performance. Results for the Sussman anomaly in Table IX provide an illustrative example.
We summarize the results of Tables I{X in Figure 1, showing the performance improvements obtained with the combined ZLIFO goal selection strategy and S+OC plan selection
est

clob

est

n

;

n

;

;

i;

109

clob
;

fiGerevini & Schubert

ZLIFO &
S+OC
LC &
S+OC
LCFR &
S+OC
LIFO &
S+OC+UC

Trains1

Trains2

Trains3

Fixit

Plans
4097/2019
17,482/10,907 31,957/19,282
5885/3685
Time
13.7
80.6
189.8
32.5
Plans
438/242
34,805/24,000 253,861/168,852
71,154/46,791
Time
2.6
368.9
1879.9
547.8
Plans
1093/597
>1,000,000
>1,000,000
190,095/117,914
Time
10.65
>10,905
>9918
4412.36
Plans 1,071,479/432,881 > 10,000,000
> 1,000,000 8,090,014/4,436,204
Time
3050.15
> 37,879
> 2539
27,584.9

Table X: Performance of the plan selection strategy S+OC in combination with the goal
selection strategies ZLIFO, LCFR and LC in solving problems which are very
hard for the default strategies of ucpop (S+OC+UC/LIFO). (The CPU seconds
do not include Lisp garbage collection. The number of plans generated for LCFR
does not include those created in order to estimate the repair cost of the aws.)

Problem
ZLIFO*
LCFR
name CPU time Plans CPU time Plans
tw-1
tw-2
tw-3
tw-4
tw-5
tw-6

0.09
0.61
2.55
7.80
19.41
42.57

26/15
72/39
138/71
224/111
330/159
456/215

0.10
0.66
3.17
10.97
30.17
71.10

26/15
72/39
139/72
227/114
336/165
466/225

Table XI: Performance of UCPOP in the TileWorld domain using ZLIFO* and LCFR for
goal selection, and S+OC+F+0.1UC for plan selection
strategy as a function of problem diculty (as indicated by the number of plans generated
by the default LIFO plus S+OC+UC strategy). The trend toward greater speedups for
more complex problems (though somewhat dependent on problem type) is quite apparent
from the log-log plot.
For direct comparison with Joslin and Pollack's LCFR strategy and Peot and Smith's
LC strategy, we implemented their strategies and applied them to several problems. They
did very well (sometimes better than ZLIFO) for problems on the lower end of the diculty
spectrum, but poorly for harder problems. (For all the problems we ran, LC with the
*d-sep* switch on performed better than LCFR in terms of plans explored and CPU
time required.) For T-of-H1 LCFR in combination both with the default S+OC+UC plan
selection strategy, and with our S+OC plan strategy did not find a solution within a search
limit of 200,000 plans generated (cf. 253 for ZLIFO with S+OC, and 751 for ZLIFO with
S+OC+UC), requiring an unknown CPU time in excess of 4254 seconds with S+OC+UC,
110

fiAccelerating Partial-Order Planners
and in excess of 4834 seconds with S+OC (cf. 0.54 seconds for ZLIFO with S+OC).10
LC performed much better than LCFR but still considerably worse than ZLIFO, solving
T-of-H1 by generating/exploring 8313/6874 plans with S+OC and 8699/6441 plans with
S+OC+UC, and requiring 44.4 CPU secs. and 48.95 CPU secs. respectively. For T-ofH3, LC found a solution by generating/exploring 21,429/15,199 plans with S+OC+UC
and 17,539/14,419 plans with S+OC, requiring 145.18 CPU secs. and 77.84 CPU secs.
respectively.
Table X shows the results for the plan strategy S+OC, with the goal strategies ZLIFO,
LCFR and LC, applied to three problems (Trains1, Trains2 and Fixit). As shown by the data
in the table these are very hard for the default strategies of ucpop (LIFO & S+OC+UC),
but become relatively easy when S+OC is used in combination either with ZLIFO, LCFR
or LC. While LCFR and LC did slightly better than ZLIFO for Trains1 (the easiest of
these problems), they performed quite poorly for Fixit, Trains2 and Trains3 (the hardest
problems) compared to ZLIFO.
Joslin and Pollack (1994) tested their LCFR strategy on six problems in the TileWorld
(tw-1, ..., tw-6), five of which are very hard for default ucpop, but easy for ucpop using
LCFR.11 We tested our ZLIFO strategy in the TileWorld using the same six problems.
ZLIFO did well for tw-1{4, but for tw-5 and tw-6 its performance dropped well below that
of LCFR. This raised the question whether for these particular problems it is crucial to
minimize \repair cost" in aw selection uniformly, rather than just in certain special cases
(ZLIFO does minimize the repair cost when no threat is on the aw list, and at least one zerocommitment open condition is present). However, further experiments aimed at answering
this question suggested that the poor choices made by ZLIFO for some TileWorld problems
were not due to selection of \high cost" over \low cost" aws. Instead two factors appear be
crucial for improving ZLIFO: (a) emphasizing zero-commitment open conditions by giving
them higher priority than threats; (b) when there are no zero-commitment open conditions,
resolving threats as soon as they enter the agenda. (We realized the relevance of (b) by
observing that the performance of a modified versions of LCFR, where the *d-sep* switch
is implicitly forced on, dramatically degraded for tw-6 in a slightly different formulation of
the TileWorld.)
We extended our ZLIFO strategy to include (a) and (b), and we briey tested the
resulting variant of ZLIFO (ZLIFO*). Table XI shows the results for ZLIFO* together with
the plan selection strategy S+OC+0.1UC+F, where as discussed in Section 2.3 we included
an attenuated form of the UC term (UC/10), and an F term equal to the number of facts
since TileWorld uses facts to track the number of tiles carried by the agent.12 ZLIFO*
10. This was with *d-sep* turned off, which is the implicit setting in LCFR (Joslin, 1995). In our experiments
we also tested a variant of LCFR, where the switch is forced to be on. The resulting goal strategy in
combination with our plan strategy S+OC performed significantly better for T-of-H1, solving the problem
generating/exploring 7423/6065 plans, and using 110.45 CPU seconds. Note also that a comparison of
our implementation of LCFR and Joslin & Pollack's implementation used for the experiments discussed
in (Joslin & Pollack, 1994) showed that our implementation is considerably faster (Joslin, 1995).
11. In their experiments tw-2, the easiest among tw-2{6, was not solved by ucpop even when allowed to run
for over eight hours. On the other hand, ucpop using LCFR solves tw-6, the hardest problem, without
ever reaching a dead-end node in the search tree.
12. In the ZLIFO* experiments the refined plans generated by resolving a threat were added to the aw list
in the following order: first the plan generated by promotion, then the plan generated by demotion, and
finally the plan generated by confrontation or separation.

111

fiGerevini & Schubert
performed very eciently for all six TileWorld problems, in fact a little better than LCFR.
Note that for these problems ZLIFO* is more ecient than LCFR in terms of the CPU time,
even though the number of plans generated/explored by the two strategies is approximately
the same. This is because the overhead of selecting the next aw to be handled is higher
in LCFR than in ZLIFO* (and ZLIFO). In fact, while LCFR needs to compute the \repair
cost" of each aw (including the threats) in the current plan, ZLIFO* (ZLIFO) only needs to
check for the presence of zero-commitment open conditions, without processing the threats.
Additional experiments indicated that the average performance of ZLIFO* is comparable
to that of ZLIFO for most of the other problems we used in our experiments, in terms of
plans created/explored. However, the CPU time tends to increase since the overhead of
computing the goal selection function is higher for ZLIFO* than for ZLIFO, because of the
extra agenda-management costs. Because of this overhead, we do not regard ZLIFO* as
generally preferable to ZLIFO. However, the TileWorld experiments underscored for us that
in some worlds refinements of ZLIFO are advantageous.
Finally, another possible variant of ZLIFO, which was suggested to us by David Smith,
is based on the following preferences of the next aw to be handled: (i) a threat that cannot
be resolved; (ii) an open condition that cannot be established; (iii) a threat that has only
one possible resolution; (iv) an open condition that can only be established in one way; (v)
other threats; (vi) other open conditions (using LIFO to pick among these). We observe
that while this strategy could give further savings in terms of plans created/explored, it
also imposes an additional overhead with respect to both ZLIFO and ZLIFO* which could
degrade performance in terms of CPU time.

4. Precomputing Parameter Domains

Even with the speedups obtained through improved search, a ucpop-like algorithm remains
severely limited in the complexity of problems it can solve. We believe that significant
further progress requires fuller use of global properties of the search space, as determined by
the structure of the operators, initial conditions, and goals. One way to do that would be
through a more in-depth analysis of alternatives during the search, but this can lead to high
overhead costs. Another is to precompute constraints on the search space, and to use these
during planning to prune the search. The parameter domain method we now motivate and
describe is of the latter type.

4.1 How Can Parameter Domains Help?

In our previous experimentation with ucpop strategies, we found that ucpop goal regression often hypothesized steps that were doomed to be abandoned eventually, because they
stipulated impossible parameter bindings. A clear example of this occurred in the Molgen
domain, as encoded in the ucpop test suite. The goal of the \Rat-insulin" test problem is
(and (bacterium ?b) (molecule ?m)
(contains IG ?m) (contains ?m ?b) (pure ?b))

,

where ?b and ?m are existentially quantified variables. What this means is that we wish
to create a purified bacterial culture ?b, where ?b contains a molecule ?m (necessarily an
112

fiAccelerating Partial-Order Planners
exosome, it turns out), and this molecule in turn contains the insulin gene, IG. We are
using the abbreviations IG, EE, JE, L for insulin-gene, e-coli-exosome, junk-exosome,
and linker; and E, J, A1 for e-coli, junk, and antibiotic-1. Roughly speaking, the solution
involves processing the initially given mRNA form of the insulin gene so as to produce a
form of insulin DNA that can be spliced into the e-coli-exosome, using a ligate operator.
In turn, the exosome is inserted into the e-coli bacterium using a transform operator, and
the bacterial culture is then purified using a screen operator, with antibiotic-1. (The junk
bacterium and exosome merely serve to complicate the task { they are nearly, but not quite,
substitutable for the e-coli bacterium and exosome; the junk exosome, unlike e-coli-exosome,
is not resistant to antibiotic-1, violating a precondition of screen.)
Now, in the initial regression the goals (bacterium ?b) and (molecule ?m) can be
established only with the *start* operator, i.e., with the initial conditions, and thus will
not be instantiated to bizarre values. (The initial conditions supply E and J as the only
instances of bacterium, and IG, EE, JE, and L as the only instances of molecule.) On
the other hand, the remaining goals turn out to match the effects of various instances of
the ligate, transform, and screen operators of Molgen, as follows:
(contains IG ?m): (ligate IG ?m), (transform IG ?m)
(contains ?m ?b): (ligate ?m ?b) (transform ?m ?b)
(pure ?b):
(screen ?b ?y ?z)

,

ucpop will happily regress on these actions. Yet two of them,

(transform IG ?m) and
are doomed to fail, perhaps after a great deal of effort has been expended
on trying to satisfy their preconditions. In particular, examination of the constants that can
\ow into" the transform operator from the initial conditions and other Molgen operators
shows that its first argument is restricted to domain fEE, JEg, i.e., it must be one of
the given exosomes, and the second is restricted to fE, Jg, i.e., it must be one of the
given bacteria. Consequently the instance (transform IG ?m) is unrealizable, as its first
argument IG is not in fEE, JEg. (Note that distinct constants denote distinct entities
according to the unique-names assumption made by ucpop.) The (ligate ?m ?b) action
is doomed for slightly more subtle reasons. It is the result of a match between (contains ?m
?b) and a \when-clause" (conditional effect) of the ligate operator, whose preconditions
can be reached only if the second parameter ?b lies in the set of molecules fIG, JE, EEg;
yet ?b is also restricted to the set of bacteria fE, Jg, as a result of the goal condition
(bacterium ?b). The fact that these sets are disjoint should allow us to eliminate the
(transform IG ?m) action.
Note that elimination of action candidates as above increases the number of zero commitment plan refinements that can be made. In the example, we are left with exactly one
action for each of the three goals, and so the ZLIFO and LCFR strategies will prefer to
regress on these goals rather than regressing on (bacterium ?b) and (molecule ?m) {
which would prematurely make arbitrary choices of ?b and ?m from the initial state.
(ligate ?m ?b),

4.2 Description of the Algorithm

In any completed plan, each precondition of each action must be instantiated by an effect
of some earlier action. So the values of the parameters of the action can only be values that
113

fiGerevini & Schubert
can be \produced" by earlier actions, starting with the initial action, *start*. Moreover,
suppose that a parameter x of a certain action occurs in each of preconditions P1, ..., Pk.
Then a constant c is a possible value of x only if earlier actions can instantiate x to c in
each of P1, ..., Pk.
Our algorithm find-parameter-domains is based on these observations. Beginning in
the initial state, it propagates positive atomic predications to all possible operator preconditions. For a propagated ground atom, if the atom matches an operator precondition,
the algorithm adds the constants in that ground atom to the individual domains of the
parameters they were unified with. These individual domains are particular to specific preconditions. For instance, the individual domain of ?x for an operator with preconditions
(on ?x ?y), (clear ?x) will in general be distinct for these two preconditions.
As soon as we have nonempty individual domains for all parameters in all preconditions
of an operator, we form the intersection of the individual domains of each parameter of
the operator. For example, if (on ?x ?y) has (so far) been matched by (on A B) and
(on B C), and (clear ?x) has (so far) been matched by (clear A) and (clear Table),
then the individual domain of x will be fA,Bg in the first precondition and fA,Tableg
in the second. Thus (assuming there are no other preconditions) the intersected domain
of ?x will be fAg at this point. If later (clear B) is also matched against (clear ?x),
the intersected domain of ?x will grow to fA,Bg. When both ?x and ?y have nonempty
intersected domains, the effects (postconditions) of the operator can in turn be propagated,
with ?x and ?y \bound" to their intersected domains.
The propagated effects are again matched against all possible operator preconditions,
and when a variable \bound" to an intersected domain is successfully unified with a variable in a precondition, it passes its intersected domain to the individual domain of that
precondition-variable (via a union operation). This can again lead to growth of the intersected domains of the operator whose precondition was matched, the effects of that operator
may then be propagated, and so on. The individual domains and intersected domains grow
monotonically during the propagation process, and in the end represent the desired parameter domains of the operators.
We illustrate this process through an example. Consider the simple planning problem
depicted in Figure 2 where an \operator graph" (Smith & Peot, 1993) is used to describe the
logical dependencies among the operators, while the iterative computation of the parameter
domains is graphically illustrated with a \domain-propagation graph" below the operator
graph.
The initial conditions (P A) and (P B) unify with the precondition (P ?x) of op1. So,
the individual domain of ?x relative to the precondition P of op1 is fA,Bg. On the other
hand, the precondition (Q ?x) of op1 cannot be satisfied by the initial state, and so the
individual domain of ?x relative to Q is initially the empty set. Hence the intersected domain
of ?x for op1 is also the empty set.
For op2 we have a different situation, since here we have only one precondition and it
can be established by the initial state. Therefore, the individual domain of ?y relative to
precondition R of op2 is the set of constants fB,Cg, and the intersected domain of ?y for
op2 is the same set (because R is the only precondition of op2 involving ?y). Since the
intersected domain of ?y has been enlarged (initially it was empty), it is propagated to the
individual domains of the other operators through the effect (Q ?y) of op2. In particular,
114

fiAccelerating Partial-Order Planners

(P ?x)

indicates bundle of edges
op1

(Q ?x)
(T B)

*start*
(R ?y)

*end*

op2
Init state: (P A),(P B),(R B),(R C),(S C)
(S ?z)

op1:

preconds: (P ?x),(Q ?x)

Goal: (T B)

op3
op2:

preconds: (R ?y)

op3:

preconds: (S ?z)
effects: (T ?z)

effects: (Q ?y)

effects: (S ?x)

(P A)
(P ?x)
(Q ?x)

id(?x)={B}

id(?x)={}
op1

(P B)

(Q ?x)

op1
ID(P,?x)={A,B}
ID(Q,?x)={B,C}

ID(P,?x)={A,B}
ID(Q,?x)={}
(Q ?y)
id(?y)={B,C}

(R B)
(R ?y)
(R C)

(S ?x)

op2
ID(R,?y)={B,C}

id(?z)={A}
S(A)

(S ?z)

id(?z)={A,B}

op3

(S ?z)

ID(S,?z)={A}

op3
ID(S,?z)={A,B}

Figure 2: Operator and domain-propagation graphs for a simple planning problem.
ID(?x,P) indicates the individual domain of the parameter ?x relative to precondition P; id(?x) indicates the intersected domain of the parameter ?x; final
intersected domains are indicated using bold fonts.
matches the precondition (Q ?x) of op1. So, the individual domain of ?x relative
to precondition Q of op1 is updated by adding the constants of the intersected domain of ?y
to it. Thus the intersected domain of ?x is enlarged to fBg, and can be propagated through
the effect (S ?x) of op1.
Similarly, the propagation of (S ?x) will enlarge the individual domain of ?z for op3,
and also the intersected domain, to the set fA,Bg. Therefore, the final intersected domains
are: fBg for ?x in op1; fB,Cg for ?y in op2; fA,Bg for ?z in op3.
Before presenting the algorithm a little more formally, we note that the parameter domains will sometimes be \too large", including values that would be found to be impossible
(Q ?y)

115

fiGerevini & Schubert
if a more detailed state space exploration were conducted. However, all that is required
for soundness in our use of the domains is that they not be \too small" (i.e., that they
contain all parameter values that can actually occur in the problem under consideration).
Of course, to be of practical use the parameter domains of an operator should exclude
some of the constants occurring in the problem specification, particularly those for which
it is intuitively obvious that they are of the wrong sort to fill particular argument slots of
the operator. This has turned out to be the case for all problem domains we have so far
experimented with.
The preceding sketch of our method is an oversimplification since preconditions and
effects of ucpop operators may be particular to a when-clause. In this case we compute
individual domains and intersected domains separately for each when-clause. For example,
consider the following schematic representation of an operator:
(define (operator op1)
:parameters (?x ?y)
:precondition (and P1 P2)
:effect (and E1 E2
(when 0 0)
(when " ") )),

PE
P E

where all conditions starting with P or E denote atomic formulas that may involve ?x and ?y.
We can think of this operator as consisting of a primary when-clause whose preconditions
P1 and P2 must always be satisfied and whose effects E1 and E2 are always asserted, and
two secondary when-clauses whose respective preconditions P 0 and P " may or may not
be satisfied, and when they are, the corresponding effects E 0 and E " are asserted. Here
our algorithm would maintain individual domains for ?x and ?y for each of preconditions
P1, P2, P 0 , and P ", and it would maintain intersected domains for ?x and ?y for the
primary when-clause and each of the two secondary clauses. The intersected domains for
the secondary clauses would be based on the individual domains of ?x and ?y not only
relative to P 0 and P ", but also on those relative to P1 and P2, since (as noted) the primary
preconditions must hold for the operator to have any of its effects, including conditional
effects.
Some further complications arise when ucpop operators contain universally quantified preconditions or effects, disjunctive preconditions, or facts (mentioned in Section 2.2).
Rather than dealing with these complications directly, we will assume that no such operators occur in the input to the algorithm. Later we describe a semi-automated way of
handling operators containing the additional constructs.
The algorithm is outlined below (a more detailed description is given in Online Appendix 1). W is a list of (names of) when-clauses whose effects are to be propagated.
Individual parameter domains are initially nil, and intersected parameter domains are initially either nil or T (where T is the universal domain). The intersected domain of a
parameter, relative to a given when-clause, is T just in case the parameter occurs neither
in the preconditions of the when-clause nor in the primary preconditions. (In such a case
the successful instantiation of the when-clause is clearly independent of the choice of value
for the parameter in question.) Unification in step 2(a) is as usual, except that when an
effect variable v is unified with a constant c in a precondition, the unification succeeds,
116

fiAccelerating Partial-Order Planners
with unifier v = c, just in case c is an element of the intersected domain of v (for the relevant when-clause). The given inits (initial conditions) and goals (which may be omitted,
i.e., nil) are treated as an operator *start* with no preconditions and an operator *end*
with no effects. Variables in goals are treated like operator parameters. We use the terms
\parameters" and \variables" interchangeably here.
Algorithm: find-parameter-domains(operators,inits,goals)
1. Initialize W to the initial conditions, so that it contains just the (primary) when-clause
of *start*.
2. Repeat steps (a{c) until W = nil:
(a) Unify the positive effects of all when-clauses in W with all possible operator
preconditions, and mark the preconditions successfully matched in this way as
\matched". (This marking is permanent.) Augment the individual domain of
each matched precondition variable with a certain set C of constants, defined as
follows. If the precondition variable was unified with a constant c, then C = fcg;
if it was unified with an effect variable, then C is the intersected domain of that
effect variable (relative to the when-clause to which the effect belongs).
(b) Mark those when-clauses as \propagation candidates" that have all their preconditions (including corresponding primary preconditions) marked as \matched"
and that involve at least one variable for which some relevant individual domain
was augmented in step (a).
(c) Reset W to nil. For all when-clauses that are propagation candidates, compute
new intersected domains for their variables. If an intersected domain of a whenclause is thereby enlarged, and all intersected domains for the when-clause are
now nonempty, then add the when-clause to W.
3. Further restrict intersected domains using equative preconditions of form (EQ u v),
i.e., form a common intersected domain if both u and v are variables. If u is a
constant and v is a variable, reduce the intersected domain of v by intersecting it
with fug; similarly if u is a variable and v is a constant. If the equation belongs to a
primary when-clause, use it to reduce the intersected domains of u and v (whichever
are variables) in the secondary clauses as well.
4. Return the intersected domains as the parameter domains, producing a sequence of
lists with each list of form
(op (x1 a1 b1 c1 :::) (x2 a2 b2 c2 :::) :::),
where each operator op appears at least once. If op has k conditional effects, there
will be k + 1 successive lists headed by op, where the first provides the parameter
domains for the primary effects of op and the rest provide the parameter domains for
the conditional effects (in the order of appearance in the ucpop definition of op).
Note that we do not match or propagate negative conditions. The problem with negative
conditions is that a very large number of them may be implicit in the initial conditions, given
117

fiGerevini & Schubert
the use of the Closed World Assumption in ucpop. For instance, in a world of n blocks,
with at most O(n) on-relations (assuming that a block can be on only one other block), we
necessarily have O(n2 ) implicit (not (on ...)) relations. In fact, the individual variable
domains of negative preconditions or goals can really be infinitely large. For instance, given
an empty initial state and a (paint-red ?x) operation with precondition (not (red ?x))
and effect (red ?x), we can achieve (red c) for infinitely many constants c. Perhaps
negative conditions could be effectively dealt with by maintaining anti-domains for them,
but we have not explored this since in practice ignoring negative conditions seems to cause
only minimal \domain bloating". (We have proved that no actual domain elements can be
lost through neglect of some preconditions.)
Our use of EQ-conditions could be refined by making use of them during the propagation
process, and NEQ-conditions could also be used. However, doing so would probably have
marginal impact.
As a final comment, we note that the output format specified in step 4 of the algorithm
is actually generalized in our implementation so as to report inaccessible preconditions
and goals. These inaccessible conditions are simply appended to the list of parameter
domains for the appropriate when-clause of the appropriate operator. For instance, if the
preconditions (oj ?oj) and (at ?oj ?city) in the ld-oj (\load orange juice") operator
of the trains world (see Online Appendix 2) are unreachable (say, because no oranges for
producing orange juice have been provided), the parameter domain list for the (unique)
when-clause of ld-oj will have the appearance
(ld-oj (?oj ...) (?car ...) (?city ...) (oj ?oj) (at ?oj ?city)).
This feature turns out to be very useful for debugging operator specifications and detecting
unreachable goals.

4.3 Correctness and Tractability

In keeping with the remarks in the previous section, we will call an algorithm for computing
parameter domains correct if the domains it computes subsume all possible parameter values
that can actually occur (in a given primary or secondary when-clause) if we consider all
possible sequences of operator applications starting at the given initial state.
The point is that this property will maintain the soundness of a planning algorithm that
uses the precomputed parameter domains to prune impossible actions (as well as spurious
threats) from a partially constructed plan. We assert the following:

Theorem 1 The find-parameter-domains algorithm is correct for computing parameter

domains of ucpop-style sets of operators (without quantification, disjunction, or facts),
initial conditions, and (possibly) goal conditions.
The proof is given in Appendix A. A preliminary step is to establish termination, using the
monotonic growth of domains and the finiteness of the set of constants involved. Correctness
is then established by showing that if there exists a valid sequence A0 A1 :::A of actions
(operator instances) starting with A0 = *start*, and if A is an instance of the operator
Op, then the bindings that the parameters of Op received in instance A are eventually added
to the relevant intersected domains of Op (where \relevant" refers to the when-clauses of Op
whose preconditions are satisfied at the beginning of A ). This is proved by induction on n.
n

n

n

n

118

fiAccelerating Partial-Order Planners
We now indicate how we can deal with universally quantified preconditions and effects,
disjunctive preconditions, and facts. We make some simple changes to operator definitions
by hand in preparation for parameter domain precomputation, and then use the domains
computed by find-parameter-domains, together with the original operators, in running
the planner. The steps for preparing an operator for parameter domain precomputation are
as follows:
 Delete disjunctive preconditions, fact-preconditions,13 and universally quantified preconditions (this includes universally quantified goals; it would also include universally
quantified sentences embedded within the antecedents of when-clauses, e.g., in the
manner (:when (:forall (?x) ) 	), though these do not occur in any problem
domains we have seen).
 Drop universal quantifiers occurring positively in operator effects, i.e., occurring at
the top level or embedded by one or more :and's. For example, an effect
(:and (at robot ?to)
(:not (at robot ?from))
(:forall (?x)
(:when (:and (grasping ?x) (object ?x))
(:and (at ?x ?to) (:not (at ?x ?from))) )))

would become

(:and (at robot ?to)
(:not (at robot ?from))
(:when (:and (grasping ?x) (object ?x))
(:and (at ?x ?to) (:not (at ?x ?from))) )))

Note that the universally quantified variable should be renamed, if necessary, to be
distinct from all other such variables and from all operator parameters.
In the example above the universally quantified variable is unrestricted. When the
quantified variable includes a type restriction, as in (:forall (object ?x) ), then
this type restriction needs to become an antecedent of the matrix sentence . In
the example at hand,  should be rewritten as the equivalent of (:when (object ?x)
). Since  is often a when-clause, this can be done by adding (object ?x) as a
conjunct to the antecedent of the when-clause. In some cases  is a conjunction of
when-clauses, and in such a case the quantifier restriction can be added into each
when-clause antecedent.
 Drop existential quantifiers in preconditions and goals, adding any restrictions on the
quantified variables as conjuncts to the matrix sentence. For example, the goal
(:exists (bacterium ?y)
(:exists (molecule ?x)
(:and (contains IG ?x)
(contains ?x ?y)
(pure ?y) )))

13. E.g., in the strips-world we would drop (fact
the given coordinates lie in the given room.

(loc-in-room ?x ?y ?room)),

119

which checks whether

fiGerevini & Schubert
becomes
(:and (bacterium ?y) (molecule ?x) (contains IG ?x)
(contains ?x ?y) (pure ?y) )

(Actually, the :and is dropped as well, when supplying goals to find-parameterdomains.)
With these reductions, find-parameter-domains will then compute correct parameter
domains for the operators and goals. To see this, note first of all that dropping preconditions (in the initial step above) will not forfeit correctness, since doing so can only
weaken the constraints on admissible parameter values, and thus can only add constants
to the domains. The effect of dropping a universal quantifier, from the perspective of
find-parameter-domains, is to introduce a new parameter in place of the universal variable. (The operator normalization subroutine detects variables in operator preconditions
and effects that are not listed as parameters, and treats them as additional parameters.)
While this is of course a drastic change in the meaning of the operator, it preserves correctness of the parameter domain calculation. This is because the domain of the new parameter
will certainly contain all constants (and hence, under the Closed World Assumption, all objects) over which the quantified variable ranges. For example, if ?x is treated as a parameter
rather than a universally quantified variable in the conditional effect
(:forall (?x) (:when (object ?x) (in ?x box))),
then the domain of ?x for the when-clause will consist of everything that can be an object, in
any state where the operator can be applied. Thus the effect (in ?x box) will also be propagated for all such objects, as is required. Finally, the elimination of existential quantifiers
from preconditions and goals can be seen to preserve the meaning of those preconditions
and goals, and hence preserves the correctness of the parameter domain calculation.
Next we formally state our tractability claim for the algorithm, as follows (with some
tacit assumptions, mentioned in the proof).
Theorem 2 Algorithm find-parameter-domains can be implemented to run in O(mn n (n +
n )) time and O(mn ) space in the worst case, where m is the number of constants in the
problem specification, n is the combined number of preconditions for all operators (and
goals, if included), and n is the combined number of operator effects (including those of
*start*).
Again the proof is in Appendix A. The time complexity of find-parameter-domains is
determined as the sum of (1) the cost of all the unifications performed, (2) the costs of all
the individual domain updates attempted, and (3) the cost of all the intersected domain
updates attempted. The space complexity bound is easily derived by assuming that there
is a fixed upper bound on the number of arguments that a predicate (in a precondition or
effect) can have, and from the fact that for each when-clause at most O(m) constants are
stored.
By adding some additional data structures in find-parameter-domains we can obtain
a version of the algorithm whose worst-case time complexity is slightly improved. In fact,
in step 2.(c) instead of propagating all of the effects of a when-clause with an enlarged
p

e

p

p

e

120

e

p

fiAccelerating Partial-Order Planners
intersected domain (i.e., adding such a when-clause to the list W), it is sucient to propagate
just those effects of the when-clause that involve an enlarged intersected-domain. This could
be done by setting up for each when-clause a table that maps each parameter to a list of
effects (of that when-clause) involving that parameter.
In the improved algorithm we use W to store the list of effects (instead of the list of whenclauses) that will be propagated in the next cycle of the algorithm, and steps 1, and 2 of
find-parameter-domains are modified in the following way:
10. Initialize W to the list of the effects of *start*.
20. Repeat steps (a{c) until W = nil:
(a0) Unify the positive effects in W with all possible operator preconditions, and mark
the preconditions successfully matched in this way as \matched" ...
0
(b ) same as 2.(b).
(c0) Reset W to nil. For all when-clauses that are propagation candidates, compute new intersected domains for their variables. If an intersected domain of a
when-clause is thereby enlarged, and all intersected domains for the when-clause
are now nonempty, then add to W the subset of the effects of the when-clause
involving at least one parameter whose intersected domain is enlarged.
Note that the worst-case time complexity of the revised algorithm is improved, because now
each effect of each when-clause is propagated at most O(m) times. This decreases the upper
bound on the number of unifications performed, reducing the complexity estimated in step
(1) of the proof of Theorem 2 to O(mn n ). Hence we have proved the following corollary.
e

p

Corollary 1 There exists an improved version of find-parameter-domains that can be
implemented to run in O(mn2 n ) time in the worst case.
p

e

5. Using Parameter Domains for Accelerating a Planner

We have already used the example of Molgen to motivate the use of precomputed parameter
domains in planning, showing how such domains may allow us to prune non-viable actions
from a partial plan.
More fundamentally, they can be used each time the planner needs to unify two predications involving a parameter, either during goal regression or during threat detection. (In
either case, one predication is a (sub)goal and the other is an effect of an action or an
initial condition.) If the unifier is inconsistent with a parameter domain, it should count
as a failure even if it is consistent with other binding constraints in the current (partial)
plan. And if there is no inconsistency, we can use the unifier to intersect and thus refine
the domains of parameters equated by the unifier.
For example, suppose that G = (at ?x ?y) is a precondition of a step in the current
plan, and that E = (at ?w ?z) is an effect of another (possibly new) step, where ?x, ?y,
?w and ?z are parameters (or, in the case of ?w and ?z, existentially quantified variables)
which have no binding constraints associated with them in the current plan. Assume also
that the domains of the parameters are:
121

fiGerevini & Schubert
?x : {Agent1, Agent2, Agent3}
?w : {Agent1, Agent2}

?y : {City1, City2}
?z : {City3, City4}

The unification of G and E gives the binding constraints f?x = ?w, ?y = ?zg, which are
not viable because the parameter domains of ?y and of ?z have an empty intersection.
On the other hand, if the domain of ?z had been fCity2, City3, City4g, then the unification of G and E would have been judged viable, and the domains of the parameters would
have been refined to:
?x : {Agent1, Agent2}
?w : {Agent1, Agent2}

?y : {City2}
?z : {City2}

Thus parameter domains can be incrementally refined as the planning search progresses;
and the narrower they become, the more often they lead to pruning.

5.1 Incorporating Parameter Domains into UCPOP

The preceding consistency checks and domain refinements can be used in a partial-order,
causal-link planner like ucpop as follows. Given a goal (open condition) G selected by
ucpop as the next aw to be repaired, we can
(1) restrict the set of new operator instances that ucpop would use for establishing G; an
instance of an operator with effect E (matching G) is disallowed if the precomputed
parameter domains relevant to E are incompatible with the current parameter domains or binding constraints relevant to G; (note that the current parameter domains
associated with G may be refinements of the initial domains);
(2) restrict the set of existing steps that ucpop would reuse for establishing G; reusing a
step with effect E (matching G) is disallowed if the current parameter domains relevant
to E are incompatible with the current parameter domains or binding constraints
relevant to G.
Moreover, given a potential threat by an effect Q against a protected condition P, inspection
of the relevant parameter domains may reveal that the threat is actually spurious. This
happens if the unifier of P and Q violates the (possibly refined) domain constraints of a
parameter in P or Q. Thus we can often
(3) reduce the number of threats that are generated by the planner when a new causal
link is introduced into the plan (this happens when an open condition is established
either by reusing a step or by introducing a new one);
(4) recognize that a threat on the list of the aws to be processed is redundant, allowing
its elimination. (Note that since parameter domains are incrementally refined during
planning, even if we use (3) during the generation of the threats, it is still possible for
a threat to becomes spurious after it has been added to the aw list).
These four uses of parameter domains cut down the search space without loss of viable
solutions, since the options that are eliminated cannot lead to a correct, complete plan.
122

fiAccelerating Partial-Order Planners
Note that (3) and (4) can be useful even when the planner only deals with definite
threats (i.e., *d-sep* switch is turned on) for at least three reasons. First, determining
that a threat is not a definite threat when *d-sep* is on incurs an overhead cost. So,
earlier elimination of a spurious threat could lead to considerable savings if the threat is
delayed many times during the search. The second reason relates to the plan-selection
strategies adopted. If one uses a function that includes an (attenuated) term corresponding
to the number of threats currently on the aw list, then eliminating spurious threats in
advance can give a more accurate measure of the \badness" of a plan. Finally, parameter
domains could be used in threat processing so as to prune the search even when *dsep* is
on. In particular, suppose that we modify the notion of a definite threat, when we have
parameter domains, so that e.g., (P ?x) and (not (P ?y)) comprise a definite threat if
the parameter domains associated with ?x and ?y are both c. So in that case, even with
d-sep* on, we may discover early that a threat has become definite { in which case it might
also be a forced threat, i.e., the choice between promotion and demotion may be dictated
by ordering constraints; and that can prune the search space. However, in our current
implementation we do not exploit this third point.
We have incorporated these techniques into ucpop (version 2.0), along with our earlier
improvements to the plan and goal selection strategies. Parameter domains are handled
through an extension of the \varset" data structure (Weld, 1994) to include the domains
of the variables (parameters), and by extending the unification process to implement the
filtering discussed above.14 We now describe our experiments with this enhanced system.

5.2 Experimental Results Using Parameter Domains

Our main goal here is to show that while the overhead determined by computing the parameter domains is not significant (both at preprocessing time and at planning time), exploitation of the parameter domains during planning can significantly prune the search. In the
experiments we used the version of find-parameter-domains which is described in Section
4.2 and in Online Appendix 1. Note that for domains more complex than the ones we have
considered it might be worthwhile to use the improved version of the algorithm discussed in
Section 4.3. (However, it remains to be seen whether problems significantly more complex
than those we consider here can be solved by any ucpop-style planner.)
The CPU times needed by our implementation of find-parameter-domains are negligible for the problems we have looked at. They were 10 msec or less for many problems
in the ucpop test suite (when running compiled Allegro CL 4.2 on a sun 20), 20 msec for
two problems (Fixa from the fridge repair domain and Fixit from the at tire domain), and
30msec on the trains world problems described below.
In our first set of tests, we relied on the search strategy used as default in ucpop. The
function used for A* plan selection was thus S+OC+UC+F (allowing for problems that
involve \facts"), and the goals were selected from the agenda according to a pure LIFO
discipline.15
14. In the current implementation new threats are filtered only when the protected condition is established
by a step already in the plan.
15. In all experiments the *d-sep* switch was on. The default delay-separation strategy for selecting unsafe
conditions was slightly modified in the version of ucpop using parameter domains. In particular, the

123

fiGerevini & Schubert
We began by experimenting with a variety of problems from ucpop's test suite, comparing performance with and without the use of parameter domains. While relatively easy
problems such as Sussman-anomaly, Fixa, Test-ferry, and Tower-invert4 showed no improvement through the use of parameter domains, most problems { particularly the harder
ones { were solved more easily with parameter domains. For example, the Rat-insulin
problem from the Molgen domain was solved nearly twice as fast, and some strips-world
problems (Move-boxes and variants)16 and Towers of Hanoi (T-of-H1) were solved about
10 times as fast. Note that the strips-world problems involve both facts and universally
quantified conditional effects. Two problems from the oce world, Oce5 and Oce6,
which we knew to be readily solvable with our improved search strategy, remained dicult (in the case of Oce6, unsolvable) with the default ucpop strategy, despite the use
of parameter domains.17 Further experiments revealed that the source of this ineciency
was the default plan-selection strategy of ucpop. In fact, using our S+OC+F strategy
instead of S+OC+UC+F, without parameter domains Oce5 and Oce6 were solved generating/exploring 3058/2175 and 8770/6940 plans respectively; while using the parameter
domains the plans numbered 1531/1055 and 2954/2204 respectively.
These initial experiments suggested to us that the most promising application of computed parameter domains would be for nontrivial problems that involved a variety of types of
entities and relationships, and significant amounts of goal chaining (i.e., with each successive
action establishing preconditions for the next). From this perspective, the trains world
struck us as a natural choice for further experimentation, with the additional advantage
that its design was independently motivated by research at Rochester into mixed-initiative
problem solving through natural-language interaction. (Refer again to the formalization in
Online Appendix 2.) Recall from Table X that the Trains1 problem was extremely hard for
unmodified ucpop, requiring about 50 minutes and generating over a million plans.
Running the same problem with parameter domains produced a solution in 3.3 seconds
(with 1207 plans generated), i.e., 927 times faster.
Intuitively, the use of parameter domains to constrain planning is analogous to using
type constraints on the parameters (although parameter domains also take account of initial
conditions). So it is of interest to see whether adding type constraints can provide similar
eciency gains as the use of parameter domains. Our first set of experiments therefore
included T-Trains1, a \typed" version of Trains1; the operators have been slightly changed
by adding new preconditions stating the types of the parameters involved. For example,
the operator uncouple has been augmented with the preconditions (engine ?eng) and
(car ?car). This problem was also extremely hard for the unmodified ucpop, exceeding
the search limit of 1,000,000 plans generated and requiring more than 2600 seconds. With
parameter domains, the solution was obtained in one second.
threats that can be resolved by separation and which are recognized to be redundant through the use of
parameter domains were selected to be eliminated.
16. Move-boxes-2 differs slightly from the Move-boxes problem in the ucpop suite, in that its goal is (in-room
box2 rclk); Move-boxes-a differs slightly from the Move-boxes-2, in that its initial state contains two
boxes.
17. Oce5 is directly from ucpop's test suite and Oce6 is minor variant of Oce5. In Oce5, all persons
are to be furnished with checks made out to them, using a check printer at the oce and a briefcase for
picking up the checks and bringing them home. \Sam" and \Sue" are the given persons, and in Oce6
we have added (person Alan) and (person Smith) in the initial conditions.

124

fiAccelerating Partial-Order Planners

Problems

without domains
with domains
Domain
Plans
CPU sec
Plans
CPU sec ratio
Trains1
1,071,479/432,881 3050.15
1207/824
3.29
0.425
T-Trains1
> 1,000,000
> 2335
404/296
0.98
0.425
Move-boxes
608,231/167,418 1024.04
5746/3253
18.8
0.705
Move-boxes-1
> 1,000,000
> 6165
1264/645
3.59
0.705
Move-boxes-2
13,816/3927
45.05
1175/587
2.66
0.705
Move-boxes-a
13,805/3918
46.11
1175/587
2.54
0.702
T-of-H1
160,911/107,649 204.51 17,603/12,250
37.5
0.722
Rat-insulin
364/262
0.36
196/129
0.19
0.714
Monkey-test1
96/62
0.12
75/46
0.11
0.733
Monkey-test2
415/262
0.61
247/149
0.50
0.529
Fix3
3395/2070
5.77
3103/1983
6.02
0.532
Oce5
809,345/500,578 1927.4 575,224/358,523 1556.8
0.625
Oce6
> 1,000,000
> 2730
> 1,000,000
> 2640 0.667
Tower-invert4
806/538
1.55
806/538
1.59
0.733
Sussman-anomaly
44/26
0.05
44/26
0.06
0.917
Fixa
2131/1903
2.2
2131/1903
2.34
1
Test-ferry
718/457
0.65
718/457
0.71
1

Table XII: Plans generated/visited and CPU time (secs) for standard ucpop with and
without parameter domains. ( This result was obtained on a sun 10 with
Lucid Common Lisp; the others on a sun 20 with Allegro Common Lisp.)
These results indicate that adding type constraints to operator specifications is not
nearly as effective as the use of parameter domains in boosting planning eciency. We
discuss this point further in the context of the second set of tests (below).
Table XII summarizes the experimental results for all of the experiments that used the
default ucpop search strategy. The table gives the number of plans generated/visited by
the planner and the CPU time (seconds) required to solve the problems.18 Note that the
use of the parameter domains gave very dramatic improvements not only in the trains domain, but also in the strips-world domain. The rightmost column supplies \domain ratio"
data, as a metric that we hoped would predict the likely effectiveness of using parameter
domains. The idea is that parameter domains should be effective to the extent that they
filter out many parameter bindings that can be reached by chaining back from individual
preconditions of an operator to the initial state. These bindings can be found by using a
variant of the algorithm for propagating intersected domains that instead propagates unions
of individual domains, and comparing these union domains to the intersected domains.19
18. The systems were compiled under Allegro CL 4.2, with settings (space 0) (speed 3) (safety 1) (debug
0), and run on a sun 20. The CPU time includes the Lisp garbage collection (it is the time given in the
output by ucpop).
19. Actually, we do not need to explicitly propagate union domains, but can propagate (partial) bindings for
one predication at a time, starting with the initial conditions. We match the predication to all possible
preconditions, adding the constant arguments it contains to the union domains of the matched operator

125

fiGerevini & Schubert

trains
without domains
with domains
Domain
problems
Plans
CPU sec
Plans
CPU sec ratio
Trains1
4097/2019
13.7
297/238
1.4
0.425
Trains2 17,482/10,907 80.6 1312/1065 7.16
0.425
Trains3 31,957/19,282 189.8 3885/3175 25.1
0.411
Table XIII: Plans generated/visited and CPU time (secs) for ucpop with and without
parameter domains in the trains domain using the ZLIFO strategy.

trains without domains
with domains
Domain
problems Plans CPU sec Plans CPU sec ratio
Trains1 1093/597
8.1
265/194
2.3
0.425
Trains2 >50,000 >607 >50,000 >534
0.425
Trains3 >50,000 >655 >50,000 >564
0.411
Table XIV: Plans generated/visited and CPU time (secs) for ucpop with and without
parameter domains in the trains domain using the LCFR strategy.
The \domain ratio" provides this comparison, dividing the average union domain size by the
average intersected domain size, with averages taken over all parameters of all when-clauses
of all operators.
The largest speedups (e.g., for the trains problems) do tend to correlate with the
smallest domain ratios, and the smallest speedups with the largest domain ratio (unity {
see the last few rows). However, it can be seen from the table that the problem diculty (as
measured by plans or CPU time) is much more useful than the domain ratio as a predictor
of speedups to be expected when using parameter domains. Problems that generate on the
order of a million plans or more with standard ucpop tend to produce speedups by 3 orders
of magnitude, whereas the domain ratio for some of these problems (e.g., Move-boxes-1) is
no better (or even worse) than for problems with much smaller speedups (e.g., Move-boxesa, Rat-insulin, Monkey-test1, Monkey-test2). The much lower diculty of these problems
predicts their reduced speedup. But to complicate matters, not all dicult problems give
high speedups (see T-of-H1 and especially Oce5); we do not know what subtleties of
problem structure account for these unusual cases.
In our second round of experiments, we tested the effectiveness of the parameter domain
technique in combination with our improved search strategy, i.e., S+OC/ZLIFO. In addition, we combined S+OC with LCFR (least cost aw selection) (Joslin & Pollack, 1994), so
(or when-clause). We then find corresponding (partially bound) effects, and add any new effects to the
list of predications still to be propagated. A partially bound effect such as (P A ?x ?y) is new if there
is no identical or similar predication such as (P A ?u ?v) among the previously propagated predications
or among those still to be propagated.

126

fiAccelerating Partial-Order Planners

t-trains

without domains
with domains
Domain
problems
Plans
CPU sec
Plans
CPU sec ratio
T-Trains1 3134/2183
17.2
505/416
3.4
0.425
T-Trains2 5739/4325
37.3
3482/2749
27.3
0.425
T-Trains3 17,931/13,134 130.4 11,962/9401 105.1
0.425

Table XV: Plans generated/visited and CPU time (secs) for ucpop with and without parameter domains in the \typed" trains domain using the ZLIFO strategy.

t-trains

without domains
with domains
Domain
problems
Plans
CPU sec
Plans
CPU sec ratio
T-Trains1 3138/2412 31.5 1429/1157 14.5
0.425
T-Trains2 >50,000 >1035 >50,000 >1136
0.425
T-Trains3 >50,000
>976
>50,000
>962
0.425

Table XVI: Plans generated/visited and CPU time (secs) for ucpop with and without
parameter domains in the \typed" trains domain using the LCFR strategy.
as to test for possible sensitivity of the parameter-domains technique to the precise strategy
used. For the present set of tests we used a search limit of 50,000 plans generated.
Once again we began by sampling some problems from the ucpop test suite, and these
initial trials yielded results quite analogous to those for the default ucpop strategy. We
obtained no improvements for several easier problems and significant improvements for
harder ones (e.g., again close to a factor of 2 for Rat-insulin). Noteworthy members of the
latter category were Oce5 and Oce6 { recall that Oce5 had shown little speedup with
standard ucpop and Oce6 had been unsolvable. However, in view of the computational
expense of testing both ZLIFO and LCFR, we then decided to narrow our focus to the
trains world. As mentioned, the advantages of this world are its inherent interest and
relative complexity.
Tables XIII-XVI provide experimental results for the trains domain with the S+OC/
ZLIFO strategy and the S+OC/LCFR strategy, in each case with and without parameter
domains.
The results in Tables XIII and XIV show that using parameter domains can still give very
significant improvements in performance, over and above those obtained through the use
of better search strategies. For example, the use of parameter domains provided an 11-fold
speedup for Trains2, for the S+OC/ZLIFO strategy. In this particular problem the speedup
(on all metrics) was the result of pruning 1482 plans (more than half of those generated)
during the search., and recognizing 305 unsafe conditions as redundant. Evidently, the
effect of this pruning is amplified by an order of magnitude in the overall performance,
because of the futile searches that are cut short. Note that the speedups for Trains1-3 are
127

fiGerevini & Schubert
roughly comparable (within a factor of 2) to those obtained for problems in the previous set
with comparable initial diculty (e.g., see Move-boxes-2 and Move-boxes-a in Table XII).
This again points to a rather consistent correlation between problem diculty and speedups
obtainable using parameter domains. The constant domain ratios are also compatible with
the more or less invariant speedups here, though this is of little import, given the earlier
results. For S+OC/LCFR the gains appear to be less, though the single result showing
a 3.5-fold speedup provides only anecdotal evidence for such a conclusion. Trains2 and
Trains3 remained too dicult for solution by LCFR. Similar gains were observed for the
S+OC/LC strategies where the best observed gain in the Trains domain was a 1.7-fold
speedup for Trains2. In any case, all results confirm the effectiveness of the parameterdomains technique.
Tables XV and XVI are again for the \typed" version of trains. In this case parameter
typing gave modest improvements in the absence of parameter-domains, and (in contrast
with the results for Trains1 under the default search strategy) significant deterioration in
their presence. While we do not know how to account for these results in detail, it seems
clear that contrary effects are involved. On the one hand, typing does tend to help in that
it tends to limit choices of parameter values to \sensible" ones. For example, a precondition
(engine ?eng) will be satisfiable only through use of *start*, and the initial state will thus
constrain ?eng to assume sensible values. On the other hand, adding type-preconditions
will tend to broaden the search space, by adding further open conditions to the aw list.
The lesson from the \typed" experiments appears to be that it is best not to supply
explicit type constraints on operator parameters, instead using our automated method of
calculating and updating domains to constrain parameter bindings.

6. Conclusions and Further Work

We began by exploring some simple, domain-independent improvements to search strategies
in partial order planning, and then described a method of using precomputed parameter domains to prune the search space. We now summarize our conclusions about these techniques
and then point to promising directions for further work.

6.1 Improving Search

Our proposed improvements to search strategies were based on the one hand on a carefully
considered choice of terms in the A* heuristic for plan selection, and on the other on a
preference for choosing open conditions that cannot be achieved at all or can be achieved
in only one way (with a default LIFO prioritization of other open conditions). Since the
plan refinements corresponding to uniquely achievable goals are logically necessary, we have
termed the latter strategy a zero-commitment strategy. One advantage of this technique
over other similar strategies is that it incurs a lower computational overhead.
Our experiments based on modifications of ucpop indicate that our strategies can give
large improvements in planning performance, especially for problems that are hard for
ucpop (and its \relatives") to begin with. The best performance was achieved when our
strategies for plan selection and goal selection were used in combination. In practical terms,
we were able to solve nearly every problem we tried from the ucpop test suite in a fraction
of a second (except for Fixit, which required 38.2 seconds), where some of these problems
128

fiAccelerating Partial-Order Planners
previously required minutes or were unsolvable on the same machine. This included a
sucient variety of problems to indicate that our techniques are of broad potential utility.
Further, our results suggest that zero-commitment is best supplemented with a LIFO
strategy for open conditions achievable in multiple ways, rather than a generalization of
zero-commitment favoring goals with the fewest children. This somewhat surprising result
might be thought to be due to the way in which the designer of a domain orders the
preconditions of operators; i.e., the \natural" ordering of preconditions may correlate with
the best planning order, giving a fortuitous advantage to a LIFO strategy relative to a
strategy like LC.20
However, some preliminary experiments we performed with randomized preconditions
for T-of-H1 and Trains1 indicate otherwise. In 5 randomizations of the preconditions of
T-of-H1, both LC and ZLIFO were slowed down somewhat, by average factors of 2.2 (2)
and 3.3 (4.2) in terms of plans expanded (CPU time used) respectively. (In both cases,
S+OC was used for plan search.) This still left ZLIFO with a performance advantage of
a factor of 22 in terms of plans created and 39 in terms of CPU time. For Trains1 the
performance of LC greatly deteriorated in 2 out of 5 cases (by a factor close to 70 in terms
of both plans and time), while that of ZLIFO actually improved marginally. This now left
ZLIFO with an average performance advantage over LC (whereas it had been slightly slower
in the unrandomized case) { a factor of 3.3 in terms of plans and 6.7 in terms of CPU time
(though these values are very unreliable, in view of the fact that the standard deviations
are of the same order as the means).
Despite these results we believe that a satisfactory understanding of the dependence of
aw-selection strategies on the order of operator preconditions will require a more extensive
experimental investigation. We are currently undertaking this work.

6.2 Using Parameter Domains

We described an implemented, tractable algorithm for precomputing parameter domains of
planning operators, relative to given initial conditions. We showed how to use the precomputed domains during the planning process to prune non-viable actions and bogus threats,
and how to update them dynamically for maximum effect.
The idea of using precomputed parameter domains to constrain planning was apparently
first proposed in a technical report by Goldszmidt et al. (1994). This contains the essential
idea of accumulating domains by forward propagation from the initial conditions. Though
the report only sketches a single-sweep propagation process from the initial conditions
to the goals, the implemented Rockwell Planner (RNLP) handles cyclic operator graphs,
repeatedly propagating bindings until quiescence, much as in our algorithm. Our algorithm
deals with the additional complexities of conditional effects and equalities (and in semiautomated fashion with quantification) and appears to be more ecient (Smith, 1996).
Other distinctive features of our work are the method of incrementally refining domains
20. This was suggested to us by David Smith as well as Mike Williamson. Williamson tried ZLIFO with 5
randomized versions of T-of-H1, and reported a large performance degradation (Williamson & Hanks,
1996). We recently ran these versions using our implementation, obtaining far more favorable results
(three of the five versions were easier to solve than the original version of T-of-H1, while the other two
versions slowed down ZLIFO by a factor of 1.84 and 4.86 in terms of plans explored.)

129

fiGerevini & Schubert
during planning, the theoretical analysis of our algorithm, and the systematic experimental
tests.
Another closely related study is that of Yang and Chan (1994), who used hand-supplied
parameter domains in planning much as we use precomputed domains. An interesting
aspect of their work is the direct use of sets of constants as variable bindings. For instance,
in establishing a precondition (P ?x) using an initial state containing (P a), (P b) and
(P c), they would bind ?x to fa, b, cg rather than to a specific constant. They refine
these \noncommittal" bindings during planning much as we refine variable domains, and
periodically use constraint satisfaction methods to check their consistency with current
EQ/NEQ constraints. They conclude that delaying variable bindings works best for problems
with low solution densities (while degrading performance for some problems with high
solution densities), and that the optimal frequency of making consistency checks depends on
whether dead ends tend to occur high or low in the search tree. Our work is distinguished
from theirs by our method of precomputing parameter domains, our use of specific bindings
when matching initial conditions to OCs, our use of parameter domains in threat detection
and resolution, and our handling of the enriched syntax of ucpop operators as compared
snlp operators.
Judging from the examples we have experimented with, our techniques are well-suited
to nontrivial problems that involve diverse types of objects, relations and actions, and significant logical interdependencies among the steps needed to solve a problem. When used in
conjunction with the default search strategy of ucpop, our method gave significant speedups
for nontrivial problems, reaching a speedup factor of 927 in the trains transportation planning domain, and more than 1717 for the hardest strips-world problem we tried . When
combined with our S+OC and ZLIFO search strategies, the parameter domain technique
still gave speedups by a factor of around 10 for some trains problems. Though our implementation is aimed at a ucpop-style planner, essentially the same techniques would be
applicable to many other planners.
We also found the parameter domain precomputations to be a very useful debugging
aid. In fact, the domain precomputation for our initial formulation of the trains world
immediately revealed several errors. For instance, the domain of the ?eng parameter of
mv-engine turned out to contain oranges, bananas, and an OJ factory, indicating the need
for a type constraint on ?eng. (Without this, transportation problems would have been
solvable without the benefit of engines and trains!) Another immediately apparent problem
was revealed by the parameter domains for ?city1 and ?city2 in mv-engine: the domain
for ?city1 excluded Elmira, and that for ?city2 excluded Avon. The obvious diagnosis
was that we had neglected to assert both (connected c1 c2) and (connected c2 c1) for
each track connecting two cities. Furthermore, the parameter domains can quickly identify
unreachable operators and goals in some cases. For instance, without the make-oj operator,
the computed domains show that the ld-oj operator is unreachable, and that a goal like
(and (oj ?oj) (at ?oj Bath)) (getting some orange juice to Bath) is unattainable (the
parameter domain for ?oj will be empty).
Of course, running the planner itself can also be used for debugging a formalization, but
planning is in general far more time-consuming than our form of preprocessing (especially
if the goal we pose happens to be unachievable in the formalization!), and the trace of
130

fiAccelerating Partial-Order Planners
an anomalous planning attempt can be quite hard to interpret, compared to a listing of
parameter domains, obtained in a fraction of a second.

6.3 Further work

First of all, some additional experimentation would be of interest, to further assess and
perhaps refine our search strategies. Some of this experimentation might focus on threathandling strategies, including the best general form of an attenuated UC-term in plan
selection, and the best way to combine threat selection with open condition selection. The
preference for definite threats over open conditions used by ZLIFO does appear to be a
good default according to our experience, but the TileWorld experiments indicated that a
re-ordering of priorities between threats and open conditions is sometimes desirable. Concerning the choice of a UC-related term for inclusion in the heuristic for plan selection, we
should mention that we have briey tried using S+OC+UC , where UC is the number of
definite threats, but did not obtain significant uniform improvements.
One promising direction for further development of our search strategy is to make the
zero-commitment strategy apply more often by finding ways of identifying false options as
early as possible. That is, if a possible action instance (obtained by matching an open
condition against available operators as well as against existing actions) is easily recognizable as inconsistent with the current plan, then its elimination may leave us with a single
remaining match and hence an opportunity to apply the zero-commitment strategy.
One way of implementing this strategy would be to check at once, before accepting
a matched action as a possible way to attain an open condition, whether the temporal
constraints on that action force it to violate a causal link, or alternatively, force its causal
link to be violated. In that case the action could immediately be eliminated, perhaps
leaving only one (or even no) alternative. This could perhaps be made even more effective
by broadening the definition of threats so that preconditions as well as effects of actions
can threaten causal links, and hence bring to light inconsistencies sooner. Note that if a
precondition of an action is inconsistent with a causal link, it will have to be established
with another action whose effects violate the causal link; so the precondition really poses a
threat from the outset.
Two possible extensions to our parameter domain techniques are (i) fully automated
handling of universally quantified preconditions and effects, disjunctions and facts in the
preprocessing algorithm; and (ii) more \intelligent" calculation of domains, by applying a
constraint propagation process to the sets of ground predications that have been matched to
the preconditions of an operator; this can be shown to yield tighter domains, though at some
computational expense. Blum and Furst (1995) recently explored a similar idea, but rather
than computing parameter domains, they directly stored sets of ground atoms that could be
generated by one operator application (starting in the initial state), two successive operator
applications, and so on, and then used these sets of atoms (and exclusivity relations among
the atoms and the actions connecting them) to guide the regressive search for a plan. The
algorithm they describe does not allow for conditional effects, though this generalization
appears entirely possible. For the examples used in their tests, they obtained dramatic
speedups.
d

131

d

fiGerevini & Schubert
Finally, we are also working on another preprocessing technique, namely the inference
of state constraints from operator specifications. One useful form of constraint is implicational (e.g., (implies (on ?x ?y) (not (clear ?y)))), and another is single-valuedness
conditions (e.g., (on ?x ?y) may be single-valued in both ?x and ?y). We conjecture that
such constraints can be tractably inferred and used for further large speedups in domainindependent, well-founded planning.
In view of the results we have presented and the possibilities for further speedups we
have mentioned, we think it plausible that well-founded, domain-independent planners may
yet become competitive with more pragmatically designed planners.

Acknowledgements

This work amalgamates and extends two conference papers on improving search (Schubert
& Gerevini, 1995) and using computed parameter domains (Gerevini & Schubert, 1996) to
accelerate partial-order planners. The research was supported in part by Rome Lab contract F30602-91-C-0010 and NATO Collaborative Research Grant CRG951285. Some of the
work by AG was carried out at IRST, 38050 Povo (TN), Italy, and at the CS Department
of the University of Rochester, Rochester NY USA. The helpful comments and perceptive
questions of Marc Friedman, David Joslin, Rao Kambhampati, Colm O'Riain, Martha Pollack, David Smith, Dan Weld, Mike Williamson, and of Associate Editor Michael Wellman
and the anonymous reviewers are gratefully acknowledged.

Appendix A (Proofs of the Theorems)
Theorem 1 The find-parameter-domains algorithm is correct for computing parameter

domains of ucpop-style sets of operators (without quantification, disjunction, or facts),
initial conditions, and (possibly) goal conditions.
Proof. As a preliminary observation, the intersected parameter domains computed iteratively by the algorithm eventually stabilize, since they grow monotonically and there
are only finitely many constants that occur in the initial conditions and in operator effects.
Thus the algorithm terminates.
In order to prove correctness we need to show that if there exists a valid sequence
A0 A1 :::A of actions (operator instances) starting with A0 = *start*, and if A is an
instance of the operator Op, then the bindings that the parameters of Op received in instance
A are eventually added to the relevant intersected domains of Op (where \relevant" refers
to the when-clauses of Op whose preconditions are satisfied at the beginning of A ). We
prove this by induction on n.
If n = 0, then A = A0 = *start*, so there are no parameters and the claim is trivially
true.
Now assume that the claim holds for n = 1; 2; :::; k. Then consider any operator instance
A +1 that can validly follow A0 A1 :::A , i.e., such that A +1 is an instance of an operator
Op whose primary preconditions, possibly along with some secondary ones, are satisfied at
the end of A0 A1 :::A . Let p be such a precondition, and write its instance in A +1 as
(P c1 c2 ..). Then (P c1 c2 ..) must be an effect of some A , where 0  i  k. If i = 0
n

n

n

n

n

k

k

k

k

k

i

132

fiAccelerating Partial-Order Planners
then (P c1 c2 ..) holds in the initial state, and hence this predication is propagated and
successfully matched to p in the initial propagation phase of find-parameter-domains. If
i > 0, then A is an instance of some operator Op' and (P c1 c2 ..) is the corresponding
instance of some effect (P t1 t2 ..) of Op', where each t is either a parameter of Op' or is
equal to cj. Diagrammatically,
i

j

A0 . . . A
j

i

. . .

A

A +1
j

k

k

Op'

Op

effect (P t1 t2 ..) ,,,,! precond p
(P c1 c2 .. )

(P c1 c2 .. )

By the induction assumption, the bindings of the parameters in A are eventually added
to the relevant intersected domains of Op'. This also implies that the intersected domains
of Op' become nonempty, and so the effect (P t1 t2 ..) is eventually propagated, where
any variables among the t have the corresponding constant cj in the relevant intersected
domain. Consequently, much as in the case i = 0, effect (P t1 t2 ..) is successfully matched
to precondition p of Op at some stage of the propagation. Given these observations, it is
clear that for both i = 0 and i > 0, p will be marked \matched" in Op eventually, and
furthermore any parameters of Op that occur in p will have the bindings resulting from
the unification with (P c1 c2 ..) added to the appropriate individual domains associated
with p.
This argument applies to all preconditions of Op satisfied in its instance A +1 , in particular to all the primary preconditions. Since these are all marked \matched", the algorithm
will compute intersected domains for all Op-parameters that occur in them. In view of the
individual domain updates just confirmed, and since individual domains grow monotonically, these intersected domains will eventually contain the parameter bindings of A +1 .
For instance, if a parameter ?x of Op occurs in a primary precondition and is bound to
c in A +1 , we have shown that c will eventually be added to the intersected domain of
?x associated with the primary when-clause of Op. If a parameter does not occur in the
primary preconditions of Op, then its intersected domain is set to T at the outset, and this
implicitly contains whatever binding the parameter has in A +1 .
A very similar argument can be made for any secondary when-clause of Op whose preconditions are also satisfied for A +1 . Again, all preconditions of the secondary clause,
as well as the primary preconditions, will be marked \matched", and so for any parameter
occurring in these combined preconditions, its intersected domain (relative to the secondary
clause) will be updated to include its binding in A +1 . For parameters of Op not occurring
in any of these preconditions, the intersected domains will again be set to T initially, and
this implicitly contains any possible binding. Finally, we note that since the intersected
domains relative to both primary and secondary when-clauses grow monotonically, the augmentations of intersected domains we have just confirmed is permanent. (In the case of
T-domains, these remain T.)
We leave some additional details concerned with the ultimate use of EQ-preconditions in
find-parameter-domains to the reader. 2
i

j

k

k

k

k

k

k

133

fiGerevini & Schubert

Theorem 2 Algorithm find-parameter-domains can be implemented to run in O(mn n (n +
n )) time and O(mn ) space in the worst case, where m is the number of constants in the
problem specification, n is the combined number of preconditions for all operators (and
goals, if included), and n is the combined number of operator effects (including those of
p

e

e

p

p

p

e

*start*).

Proof. The time complexity of find-parameter-domains can be determined as the sum
of (1) the cost of all the unifications performed, (2) the costs of all the individual domain
updates attempted, and (3) the cost of all the intersected domain updates attempted. We
estimate an upper bound for each of these terms under the following assumptions:

(a) the unification of any operator effect with any operator precondition requires constant
time;
(b) there is a fixed upper bound on the number of arguments that a predicate (in a
precondition or effect) can have. It follows that O(n ) is an upper bound on the total
number of intersected domains;21
(c) individual domains and intersected domains are stored in hash tables (indexed by the
constants in the domain). So, we can check whether an element belongs to a particular
(individual or intersected) domain, and possibly add it to that domain essentially in
constant time. Furthermore for each individual and intersected domain, appropriate
data structures are used to keep track of the (possibly empty) set of new elements
that have been added to the domain in the last update attempt.
(1) For any particular intersected domain of any particular operator, there can be at
most m updates of this domain. Each such update causes all of the effects of the whenclause to which the intersected domain belongs to be propagated. An upper bound on this
number is n . Each propagated effect may then be unified with O(n ) preconditions. Thus
the O(m) updates of an intersected domain may cause O(mn n ) unifications. Hence from
(b), the overall number of unifications caused by the propagation of intersected domains
to individual domains is O(mn2 n ). To these unifications we have to add those which are
initially performed between the effects of *start* and the preconditions of the operators.
There are O(mn ) such unifications, and so they do not increase the previous upper bound
on the number of unifications. Thus, from (a), the cost of all of the unifications performed
by the algorithm is O(mn2 n ).
(2) Each unification is potentially followed by an attempt to update the individual
domain(s) of the relevant parameter(s). However, with assumption (c) the number of such
attempts is limited to those where the set of new elements in the intersected domain(s)
of the unifying effect is (are) not empty. Furthermore, when we attempt to update an
individual domain D by performing the union of a relevant intersected domain D and D ,
only the subset of the new elements of D need to be added to D (if they are not already
there). Thus, since any intersected domain grows monotonically, from (b) and (c) we have
that the overall cost of all the update attempts for one particular individual domain caused
e

e

p

e

e

p

p

p

e

p

I

i

i

I

I

21. Note that if a parameter appears in a precondition of a when-clause, but in none of its effects, then the
intersected domain of the parameter will not be propagated by the algorithm. Hence in implementing
the algorithm we can ignore such parameters.

134

fiAccelerating Partial-Order Planners
by one particular effect is O(m). But in the worst case one effect can unify with all the
O(n ) preconditions of all the operators, yielding an overall bound on all of the attempts
to update the individual domains of O(mn n ).
(3) There can be an attempt to update a particular intersected domain for each relevant
individual domain update, and each relevant individual domain can be updated O(m) times
(because the domains grow monotonically). Therefore from (b) there are at most O(mn )
attempts to update one intersected domain. By (c) the total cost of these attempts is
O(mn2 ), because checking whether a new element of an individual domain belongs to all
the other O(n ) relevant individual domains takes O(n ) time. So, since from (b) there
are no more than O(n ) intersected domains, the total cost incurred by the algorithm for
updating all of the intersected domains is O(mn n2 ).
It follows that the time complexity of find-parameter-domains is:
O(mn2n ) + O(mn n ) + O(mn n2) = O(mn n (n + n )).
The space complexity bound is easily derived from (b), and from the fact that for each
when-clause at most O(m) constants are stored. 2
p

e

p

p

p

p

p

e

e

p

e

p

References

e

p

e

p

p

e

p

e

Allen, J., & Schubert, L. (1991). The TRAINS project. Tech. rep. 382, Dept. of Computer
Science, Univ. of Rochester, Rochester, NY. Also slightly revised as Language and
discourse in the TRAINS project, in A. Ortony, J. Slack, and O. Stock (eds.), Communication from an Artificial Intelligence Perspective: Theoretical Springer-Verlag,
Heidelberg, pp. 91-120.
Allen, J., Schubert, L., Ferguson, G., Heeman, P., Hwang, C., Kato, T., Light, M., Martin,
N., Miller, B., Poesio, M., & Traum, B. (1995). The TRAINS project: A case study
in building a conversational planning agent. Experimental and Theoretical Artificial
Intelligence, 7, 7{48.
Barrett, A., Golden, K., Penberthy, S., & Weld, D. (1994). UCPOP user's manual. Tech.
rep. 93-09-06, Dept. of Computer Science and Engineering, University of Washington,
Seattle, WA 98105.
Blum, A., & Furst, M. (1995). Fast planning through planning graph analysis. In Proceedings
of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95),
pp. 1636{1642 Montreal, CA. Morgan Kaufmann.
Chapman, D. (1987). Planning for conjunctive goals. Artificial Intelligence, 32 (3), 333{377.
Currie, K., & Tate, A. (1991). O-Plan: The open planning architecture. Artificial Intelligence, 51 (1).
Fikes, R., & Nilsson, N. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2, 189{208.
Georgeff, M., & Lansky, A. (1987). Reactive reasoning and planning. In Proceedings of the
Sixth National Conference of the American Association for Artificial Intelligence, pp.
677{682 Seattle, WA. Morgan Kaufmann.
135

fiGerevini & Schubert
Gerevini, A., & Schubert, L.K. (1995). Computing parameter domains as an aid to planning.
In Proc. of the 3rd Int. Conf. on Artificial Intelligence Planning Systems (AIPS-96),
pp. 94{101 Menlo Park, CA. The AAAI Press.
Goldszmidt, M., Darwiche, A., Chavez, T., Smith, D., & White, J. (1994). Decision-theory
for crisis management. Tech. rep. RL-TR-94-235, Rome Laboratory.
Green, C. (1969). Application of theorem proving to problem solving. In Proceedings of
the First International Joint Conference on Artificial Intelligence (IJCAI-69), pp.
219{239.
Joslin, D. (1995). Personal communication.
Joslin, D., & Pollack, M. (1994). Least-cost aw repair: a plan refinement strategy for
partial-order planning. In Proceedings of the Twelfth National Conference of the
American Association for Artificial Intelligence (AAAI-94), pp. 1004{1009 Seattle,
WA. Morgan Kaufmann.
Kambhampati, S., Knoblock, C. A., & Yang, Q. (1995). Planning as refinement search: A
unified framework for evaluating design tradeoff in partial-order planning. Artificial
Intelligence. Special Issue on Planning and Scheduling, 76 (1-2).
Korf, R. (1992). Linear-space best-first search: Summary of results. In Proceedings of
the Tenth National Conference of the American Association for Artificial Intelligence
(AAAI-92), pp. 533{538.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings
of the Ninth National Conference on Artificial Intelligence (AAAI-91), pp. 634{639
Anheim, Los Angeles, CA. Morgan Kaufmann.
Nilsson, N. (1980). Principles of Artificial Intelligence. Tioga Pub. Co., Palo Alto, CA.
Penberthy, J., & Weld, D. (1992). UCPOP: A sound, complete, partial order planner
for ADL. In Nebel, B., Rich, C., & Swartout, W. (Eds.), Proceedings of the Third
International Conference on Principles of Knowledge Representation and Reasoning
(KR92), pp. 103{114 Boston, MA. Morgan Kaufmann.
Peot, M. A., & Smith, D. E. (1993). Threat-removal strategies for partial-order planning.
In Proceedings of the Eleventh National Conference of the American Association for
Artificial Intelligence (AAAI-93), pp. 492{499 Washington, D.C. Morgan Kaufmann.
Schubert, L., & Gerevini, A. (1995). Accelerating partial order planners by improving
plan and goal choices. In Proc. of the 7th IEEE Int. Conf. on Tools with Artificial
Intelligence, pp. 442{450 Herndon, Virginia. IEEE Computer Society Press.
Smith, D. E., & Peot, M. A. (1993). Postponing threats in partial-order planning. In
Proceedings of the Eleventh National Conference of the American Association for Artificial Intelligence (AAAI-93), pp. 500-506 Washington, D.C. Morgan Kaufmann.
Smith, D. E. (1996). Personal communication.
136

fiAccelerating Partial-Order Planners
Srinivasan, R., & Howe, A. (1995). Comparison of methods for improving search eciency
in a partial-order planner. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), pp. 1620{1626.
Weld, D. (1994). An introduction to least commitment planning. AI Magazine, 15 (4),
27{62.
Wilkins, D. (1988). Practical Planning: Extending the Classical AI Planning Paradigm.
Morgan Kaufmann, San Mateo, CA.
Williamson, M., & Hanks, S. (1995). Flaw selection strategies for value-directed planning. In
Proceedings of the Third International Conference on Artificial Intelligence Planning
Systems, pp. 237{244.
Yang, Q., & Chan, A.Y.M. (1994). Delaying variable binding commitments in planning. In
Proceedings of the Second International Conference on Artificial Intelligence Planning
Systems, pp. 182{187.

137

fiJournal of Artificial Intelligence Research 5 (1996) 27-52

Submitted 9/95; published 8/96

A Hierarchy of Tractable Subsets
for Computing Stable Models
Rachel Ben-Eliyahu

rachel@cs.bgu.ac.il

Mathematics and Computer Science Department
Ben-Gurion University of the Negev
P.O.B. 653, Beer-Sheva 84105, Israel

Abstract

Finding the stable models of a knowledge base is a significant computational problem
in artificial intelligence. This task is at the computational heart of truth maintenance
systems, autoepistemic logic, and default logic. Unfortunately, it is NP-hard. In this
paper we present a hierarchy of classes of knowledge bases, 
1 
2 , with the following
properties: first, 
1 is the class of all stratified knowledge bases; second, if a knowledge
base  is in 
 , then  has at most stable models, and all of them may be found in time
( ), where is the length of the knowledge base and the number of atoms in ; third,
for an arbitrary knowledge base , we can find the minimum such that  belongs to 

in time polynomialSin1 the size of ; and, last, where K is the class of all knowledge bases,
it is the case that =1 
 = K, that is, every knowledge base belongs to some class in the
hierarchy.
;

k

k

O lnk

; :::

l

n

k

i

k

i

1. Introduction
The task of computing the stable models of a knowledge base lies at the heart of three of
the fundamental systems in Artificial Intelligence (AI): truth maintenance systems (TMSs),
default logic, and autoepistemic logic. Yet, this task is intractable (Elkan, 1990; Kautz &
Selman, 1991; Marek & Truszczynski, 1991). In this paper, we introduce a hierarchy of
classes of knowledge bases which achieves this task in polynomial time. Membership in a
certain class in the hierarchy is testable in polynomial time. Hence, given a knowledge base,
the cost of computing its stable models can be bounded prior to the actual computation (if
the algorithms on which this hierarchy is based are used).
First, let us elaborate the relevance of computing stable models to AI tasks. We define
a knowledge base to be a set of rules of the form

C ,A1 ; :::; Am; not B1; :::; not Bn

(1)

where C , all As, and all B s are atoms in some propositional language. Substantial efforts to
give a meaning, or semantics, to a knowledge base have been made in the logic programming
community (Przymusinska & Przymusinski, 1990). One of the most successful semantics for
knowledge bases is stable model semantics (Bidoit & Froidevaux, 1987; Gelfond & Lifschitz,
1988; Fine, 1989), which associates any knowledge base with a (possibly empty) set of
models called stable models. Intuitively, each stable model represents a set of coherent
c 1996


AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBen-Eliyahu
conclusions one might deduce from the knowledge base. It turns out that stable models
play a central role in some major deductive systems in AI. 1

1.1 Stable Models and TMSs

TMSs (Doyle, 1979) are inference systems for nonmonotonic reasoning with default assumptions. The TMS manages a set of nodes and a set of justifications, where each node
represents a piece of information and the justifications are rules that state the dependencies
between the nodes. The TMS computes a grounded set of nodes and assigns this set to be
the information believed to be true at a given point in time. Intuitively, a set of believed
nodes is grounded if it satisfies all the rules, but no node is believed true solely on the basis
of a circular chain of justifications. Elkan (1990) pointed out that the nodes of a TMS
can be viewed as propositional atoms, and the set of its justifications as a knowledge base.
He showed that the task of computing grounded interpretations for a set of TMS justifications corresponds exactly to the task of computing the stable models of the knowledge base
represented by the set of TMS justifications.

1.2 Stable Models and Autoepistemic Logic

Autoepistemic logic was invented by Moore (1985) in order to formalize the process of an
agent reasoning about its own beliefs. The language of autoepistemic logic is a propositional
language augmented by a modal operator L. Given a theory (a set of formulas) T in
autoepistemic logic, a theory E is called a stable expansion of T iff
E = (T SfLF jF 2 E gSf:LF jF 2= E g)
where T  denotes the logical closure of T . We will now restrict ourselves to a subset of
autoepistemic logic in which each formula is of the form
A1 ^ ::: ^ Am ^ :LB1 ^ ::: ^ :LBn ,!C
(2)
where C , each of the As, and each of the B s are propositional atoms. We call this subset
the class of autoepistemic programs. Every autoepistemic program T can be translated into
a knowledge base T by representing the formula (2) as the knowledge base rule (1). Elkan
(1990) has shown that M is a stable model of T iff there is an expansion E of T such
that M is the set of all positive atoms in E . Thus, algorithms for computing stable models
may be used in computing expansions of autoepistemic programs. The relationship between
stable model semantics and autoepistemic logic has also been explored by Gelfond (1987)
and Gelfond and Lifschitz (1988, 1991).

1.3 Stable Models and Default Logic

Default logic is a formalism developed by Reiter (1980) for reasoning with default assumptions. A default theory can be viewed as a set of defaults, and a default is defined as an
expression of the form
ff : fi1; :::; fin
(3)



1. In logic programming terminology, the knowledge bases discussed in this paper are called normal logic
programs.

28

fiA Hierarchy of Tractable Subsets
where ff;  , and fi1 ; :::; fin are formulas in some first-order language. According to Reiter, E
is an extension for a default theory  iff E coincides with one of the minimal deductively
closed sets of sentences E 0 satisfying the condition2 that for any grounded instance of a
default (3) from , if ff 2 E 0 and :fi1 ; :::; :fin 2= E , then  2 E 0.
Now consider the subset of default theories that we call default programs. A default
program is a set of defaults of the form

A1 ^ ::: ^ Am : :B1; :::; :Bn
(4)
C
in which C , each of the As, and each of the B s are atoms in a propositional language.

Each default program  can be associated with a knowledge base  by replacing each
default of the form (4) with the rule (1).
Gelfond and Lifschitz (1991) have shown that the logical closure of a set of atoms E is
an extension of  iff E is a stable model of  . Algorithms for computing stable models
can thus be used in computing extensions of Reiter's default theories.


The paper is organized as follows. In the next section, we define our terminology.
Section 3 presents two algorithms for computing all stable models of a knowledge base.
The complexity of the first of these algorithms depends on the number of atoms appearing
negatively in the knowledge base, while the complexity of the other algorithm depends
on the number of rules having negative atoms in their bodies. In Section 4, we present
the main algorithm of the paper, called algorithm AAS. Algorithm AAS works from the
bottom up on the superstructure of the dependency graph of the knowledge base and uses
the two algorithms presented in Section 3 as subroutines. Section 5 explains how the AAS
algorithm can be generalized to handle knowledge bases over a first-order language. Finally,
in Sections 6 and 7, we discuss related work and make concluding remarks.

2. Preliminary Definitions
Recall that here a knowledge base is defined as a set of rules of the form

C ,A1 ; :::; Am; not B1; :::; not Bn

(5)

where C , each of the As, and each of the B s are propositional atoms. The expression to the
left of , is called the head of the rule, while the expression to the right of , is called
the body of the rule. Each of the As is said to appear positive in the rule, and, accordingly,
each of the B s is said to appear negative in the rule. Rule (5) is said to be about C . A rule
with an empty body is called a unit rule. Sometimes we will treat a truth assignment (in
other words, interpretation) in propositional logic as a set of atoms | the set of all atoms
assigned true by the interpretation. Given an interpretation I and a set of atoms A, IA
denotes the projection of I over A. Given two interpretations, I and J , over sets of atoms
2. Note the appearance of E in the condition.

29

fiBen-Eliyahu
A and B , respectively, the interpretation I + J is defined as follows:
8>
if P 2 A n B
>< IJ((PP))
if P 2 BTn A
I + J (P ) = > I (P )
if P 2 A B and I (P ) = J (P )

>:
undefined otherwise
T
If I (P ) = J (P ) for every P 2 A B , we say that I and J are consistent.

A partial interpretation is a truth assignment over a subset of the atoms. Hence, a partial
interpretation can be represented as a consistent set of literals: positive literals represent
the atoms that are true, negative literals the atoms that are false, and the rest are unknown.
A knowledge base will be called Horn if all its rules are Horn. A model for a theory (set
of clauses) in propositional logic is a truth assignment that satisfies all the clauses. If one
looks at a knowledge base as a theory in propositional logic, a Horn knowledge base has a
unique minimal model (recall that a model m is minimal among a set of models M iff there
is no model m0 2 M such that m0  m).
Given a knowledge base  and a set of atoms m, Gelfond and Lifschitz defined what is
now called the Gelfond-Lifschitz (GL) transform of  w.r.t. m, which is a knowledge base
m obtained from  by deleting each rule that has a negative literal not P in its body with
P 2 m and deleting all negative literals in the bodies of the remaining rules. Note that m
is a Horn knowledge base. A model m is a stable model of a knowledge base  iff it is the
unique minimal model of m (Gelfond & Lifschitz, 1988).
Example 2.1 Consider the following knowledge base 0, which will be used as one of the
canonical examples throughout this paper:
(6)
warm blooded , mammal
live on land , mammal; not ab1
(7)
female , mammal; not male
(8)
male , mammal; not female
(9)
mammal , dolphin
(10)
ab1 , dolphin
(11)
mammal , lion
(12)
lion ,
(13)
m = flion; mammal; warm blooded; live on land; femaleg is a stable model of 0 . Indeed,
0m (the GL transform of 0 w.r.t. m) is

,
,
,
,
,
,
,

warm blooded
live on land
female
mammal
ab1
mammal
lion

30

mammal
mammal
mammal
dolphin
dolphin
lion

fiA Hierarchy of Tractable Subsets
and m is a minimal model of 0m .
A set of atoms S satisfies the body of a rule  iff each atom that appears positive in the
body of  is in S and each atom that appears negative in the body of  is not in S . A set
of atoms S satisfies a rule iff either it does not satisfy its body, or it satisfies its body and
the atom that appears in its head belongs to S .
A proof of an atom is a sequence of rules from which the atom can be derived. Formally,
we can recursively define when an atom P has a proof w.r.t. a set of atoms S and a
knowledge base :
 If the unit rule P , is in , then P has a proof w.r.t.  and S .
 If the rule P ,A1; :::; Am; not B1; :::; not Bn is in , and for every i = 1; :::; n Bi is
not in S , and for every i = 1; :::; m Ai already has a proof w.r.t.  and S , then P has
a proof w.r.t.  and S .
Theorem 2.2 (Elkan, 1990; Ben-Eliyahu & Dechter, 1994) A set of atoms S is a stable
model of a knowledge base  iff
1. S satisfies each rule in , and
2. for each atom P in S , there is a proof of P w.r.t  and S .
It is a simple matter to show that the following lemma is true.
Lemma 2.3 Let  be a knowledge base, and let S be a set of atoms. Define:
1. S0 = ;, and
S
2. Si+1 = Si fP jP ,A1 ; :::; Am; not B1 ; :::; not Bn is in ;
all of the A's belong to Si and none of the B 's belong to S g.
S S.
Then S is a stable model of  iff S = 1
0 i
Observe that although every stable model is a minimal model of the knowledge base
viewed as a propositional theory, not every minimal model is a stable model.
Example 2.4 Consider the knowledge base
b , not a
Both fag and fbg are minimal models of the knowledge base above, but only fbg is a stable
model of this knowledge base.
Note that a knowledge base may have one or more stable models, or no stable model at all.
If a knowledge base has at least one stable model, we say that it is consistent.
The dependency graph of a knowledge base  is a directed graph where each atom is a
node and where there is a positive edge directed from P to Q iff there is a rule about Q
in  in which P appears positive in the body. Accordingly, there is a negative edge from
P to Q iff there is a rule about Q in which P appears negative in the body. Recall that a
source of a directed graph is a node with no incoming edges, while a sink is a node with no
outgoing edges. Given a directed graph G and a node s in G, the subgraph rooted by s is
the subgraph of G having only nodes t such that there is a path directed from t to s in G.
The children of s in G are all nodes t such that there is an arc directed from t to s in G.
31

fiBen-Eliyahu

Example 2.5 The dependency graph of 0 is shown in Figure 1. Negative edges are
marked \not." The children of mammal are lion and dolphin. The subgraph rooted by
on land is the subgraph that include the nodes lion, mammal, dolphin, ab1, and on land.
not
male

warm_blood
female

not

on_land
mammal

lion

not

ab1
dolphin

Figure 1: The dependency graph of 0
A knowledge base is stratified iff we can assign each atom C a positive integer iC such
that for every rule in the form of (5) above, for each of the As, iA  iC , and for each of
the B s, iB < iC . It can be readily demonstrated that a knowledge base is stratified iff in
its dependency graph there are no directed cycles going through negative edges. It is well
known in the logic programming community that a stratified knowledge base has a unique
stable model that can be found in linear time (Gelfond & Lifschitz, 1988; Apt, Blair, &
Walker, 1988).

Example 2.6 0 is not a stratified knowledge base. The following knowledge base, 1, is

stratified (we can assign ab2 and penguin the number 1, and each of the other atoms the
number 2):

live on land
fly
bird
ab2

,
,
,
,
32

bird
bird; not ab2
penguin
penguin

fiA Hierarchy of Tractable Subsets
The strongly connected components of a directed graph G make up a partition of its
set of nodes such that, for each subset S in the partition and for each x; y 2 S , there are
directed paths from x to y and from y to x in G. The strongly connected components are
identifiable in linear time (Tarjan, 1972).

male

not

not
female
warm_blood

on_land
mammal

lion

not

ab1
dolphin

Figure 2: The super dependency graph of 0
The super dependency graph of a knowledge base , denoted G , is the superstructure of
the dependency graph of . That is, G is a directed graph built by making each strongly
connected component in the dependency graph of  into a node in G . An arc exists from
a node s to a node v iff there is an arc from one of the atoms in s to one of the atoms in v
in the dependency graph of . Note that G is an acyclic graph.

Example 2.7 The super dependency graph of 0 is shown in Figure 2. The nodes in the

square are grouped into a single node.

3. Two Algorithms for Computing Stable Models
The main contribution of this paper is the presentation of an algorithm whose eciency
depends on the \distance" of the knowledge base from a stratified knowledge base. This
distance will be measured precisely in Section 4. We will first describe two other algorithms
for computing stable models. These two algorithms do not take into account the level of
\stratifiability" of the knowledge base, that is, they will still work in exponential time for
stratified knowledge bases. Our main algorithm will use these two algorithms as procedures.
33

fiBen-Eliyahu
Given a truth assignment for a knowledge base, we can verify in polynomial time whether
it is a stable model by using Lemma 2.3. Therefore, a straightforward algorithm for computing all stable models can simply check all possible truth assignments and determine whether
each of them is a stable model. The time complexity of this straightforward procedure will
be exponential in the number of atoms used in the knowledge base. Below, we present two
algorithms that can often function more eciently than the straightforward procedure.

3.1 An Algorithm That Depends on the Number of Negative Atoms in the
Knowledge Base
Algorithm All-Stable1 (Figure 3) enables us to find all the stable models in time expo-

nential in the number of the atoms that appear negative in the knowledge base.
The algorithm follows from work on abductive extensions of logic programming in which
stable models are characterized in terms of sets of hypotheses that can be drawn as additional information (Eshghi & Kowalski, 1989; Dung, 1991; Kakas & Mancarella, 1991).
This is done by making negative atoms abductible and by imposing appropriate denials
and disjunctions as integrity constraints. The work of Eshghi and Kowalski (1989), Dung
(1991), and Kakas and Mancarella (1991) implies the following.
Theorem 3.1 Let  be a knowledge base, and let H be the set of atoms that appear negated
in . M is a stable model of  iff there is an interpretation I over H such that
1. for every atom P 2 H , if P 2 I , then P 2 M 0 ,
2. M 0 and I are consistent, and
3. M = I +M 0 ,
where M 0 is the unique stable model of I .
Proof: The proof follows directly from the definition of stable models. Suppose M is a
stable model of a knowledge base , and let H be the set of atoms that appear negative in
. Then, by definition, M is a stable model of M . But note that M = MH . Hence, the
conditions of Theorem 3.1 hold for M , taking M 0 = M and I = MH . Now, suppose  is
a knowledge base and M = M 0 + I , where M 0 and I are as in Theorem 3.1. Observe that
M = I and, hence, since M 0 is a stable model of I , M 0 is a stable model of M . We
will show that M is a stable model of M . First, note that by condition 1, M  M 0 . Thus,
M satisfies all the rules in M and, if an atom P has a proof w. r. t. M 0 and M , it has
also a proof w. r. t. M and M . So, by Theorem 2.2, M is a stable model of M and, by
definition, M is a stable model of .
Theorem 3.1 implies algorithm All-Stable1 (Figure 3), which computes all stable
models of a knowledge base . Hence, we have the following complexity analysis.
Proposition 3.2 A knowledge base in which at most k atoms appear negated has at most
2k stable models and all of them can be found in time O(nl2k ), where l is the size of the
knowledge base and n the number of atoms used in the knowledge base.
Proof: Follows from the fact that computing I and computing the unique stable model
of a positive knowledge base is O(nl).
34

fiA Hierarchy of Tractable Subsets

All-Stable1()

Input: A knowledge base .
Output: The set of all stable models of .
1. M := ;;
2. For each possible interpretation I for the set of all atoms that appear negative in ,
do:
(a) Compute M 0 , the unique stable model of I ;
S
(b) If M 0 and I are consistent, let M := M fM 0 + I g;
3. Return M;
Figure 3: Algorithm All-Stable1

3.2 An Algorithm That Depends on the Number of Non-Horn Rules
Algorithm All-Stable2 (Figure 4) depends on the number of rules in which there are

negated atoms. It gets as input a knowledge base , and, it outputs the set of all stable
models of . This algorithm is based upon the observation that a stable model can be
built by attempting all possible means of satisfying the negated atoms in bodies of nonHorn rules. Two procedures are called by All-Stable2: UnitInst, shown in Figure 5; and
NegUnitInst, shown in Figure 6. Procedure UnitInst gets as input a knowledge base  and
a partial interpretation m. UnitInst looks recursively for unit rules in . For each unit rule
P , , if P is assigned false in m, it follows that m cannot be a part of a model for , and
the procedure returns false. If P is not false in m, the procedure instantiates P to true in
the interpretation m and deletes the positive appearances of P from the body of each rule.
It also deletes from  all the rules about P and all the rules in which P appears negative.
Procedure NegUnitInst receives as input a knowledge base , a partial interpretation
m, and a set of atoms Neg. It first instantiates each atom in Neg to false and then updates
the knowledge base to reect this instantiation. All the instantiations are recorded in m.
In case of a conict, namely, where the procedure tries to instantiate to true an atom that
is already set to false, the procedure returns false; otherwise, it returns true.

Proposition 3.3 Algorithm All-Stable2 is correct, that is, m is a stable model of a
knowledge base  iff it is generated by All-Stable2().
Proof: Suppose m is a stable model of a knowledge base . Then, by Theorem 2.2, every
atom set to true in m has a proof w. r. t. m and . Let S be the set of all non-Horn
rules whose bodies are satisfied by m. Clearly, at some point this S is checked at step 3 of
algorithm All-Stable2. When this happens, all atoms that have a proof w. r. t. m and
 will be set to true by the procedure NegUnitInst (as can be proved by induction on the
length of the proof). Hence, m will be generated.
Suppose m is generated by All-Stable2(). Obviously, every rule in  is satisfied by
m (step 3.c.ii), and every atom set to true by NegUnitInst has a proof w. r. t. m and 
35

fiBen-Eliyahu

All-Stable2()
Input: A knowledge base .
Output: The set of all stable models of .
1. M := ;;
2. Let  be the set of all non-Horn rules in .
3. For each subset S of , do:
(a) Neg = fP jnot P is in the body of some rule in S g;
(b) 0 := ; m := ;;
(c) If NegUnitInst(0 ; Neg; m), then
i. For each P such that m[P ] = null, let m[P ] := false;
S
ii. If m satisfies all the rules in , then M := M fmg;
4. EndFor;
5. Return M ;
Figure 4: Algorithm All-Stable2
UnitInst(; m)
Input: A knowledge base  and a partial interpretation m.
Output: Updates m using the unit rules of . Returns false if there is a conict between
a unit rule and the value assigned to some atom in m; otherwise, returns true .
1. While  has unit rules, do:
(a) Let P , be a unit rule in ;
(b) If m[P ] = false, return false;
(c) m[P ] := true;
(d) Erase P from the body of each rule in ;
(e) Erase from  all rules about P ;
(f) Erase from  all rules in which P appears negative;
2. EndWhile;
3. Return true;
Figure 5: Procedure UnitInst
36

fiA Hierarchy of Tractable Subsets
NegUnitInst(; Neg; m)
Input: A knowledge base , a set of atoms Neg , and a partial interpretation m.
Output: Updates m assuming the atoms in Neg are false. Returns false if inconsistency is
detected; otherwise, returns true.
1. For each atom P in Neg
(a) m[P ] := false;
(b) Delete from the body of each rule in  each occurrence of not P ;
(c) Delete from  each rule in which P appears positive in the body;
2. EndFor;
3. Return UnitInst(; m);
Figure 6: Procedure NegUnitInst
S
S

1
2

lion dolphin ab1 mammal warm b on land male female
T

F

F

T

T

T

F

T

T

F

F

T

T

T

T

F

Table 1: Models generated by Algorithm All-Stable2
(as is readily observable from the way NegUnitInst works). Hence, by Theorem 2.2, m is a
stable model of .

Proposition 3.4 A knowledge base having c non-Horn rules has at most 2c stable models

and all of them can be found in time O(nl2c), where l is the size of the knowledge base and
n the number of atoms used in the knowledge base.

Proof: Straightforward, by induction on c.
Example 3.5 Suppose we call All-Stable2 with 0 as the input knowledge base. At
step 2,  is the set of rules (7), (8), and (9). When subsets of  which include both rules
(8) and (9) are considered at step 3, NegUnitInst will return false because UnitInst will

detect inconsistency. When the subset containing both rules (7) and (8) is considered, the
stable model S 1 of Table 1 will be generated. When the subset containing both rules (7)
and (9) is considered, the stable model S 2 of Table 1 will be generated. When all the other
subsets that do not contain both rules (8) and (9) are tested at step 3, the m generated will
not satisfy all the rules in  and, hence, will not appear in the output.
Algorithms All-Stable1 and All-Stable2 do not take into account the structure
of the knowledge base. For example, they are not polynomial for the class of stratified
knowledge bases. We present next an algorithm that exploits the structure of the knowledge
base.
37

fiBen-Eliyahu

4. A Hierarchy of Tractable Subsets Based on the Level of Stratifiability
of the Knowledge Base

Algorithm Acyclic-All-Stable (AAS) in Figure 7 exploits the structure of the knowledge
base as it is reected in the super dependency graph of the knowledge base. It computes all
stable models while traversing the super dependency graph from the bottom up, using the
algorithms for computing stable models presented in the previous section as subroutines.
Let  be a knowledge base. With each node s in G (the super dependency graph of
), we associate s , As , and Ms . s is the subset of  containing all the rules about the
atoms in s, As is the set of all atoms in the subgraph of G rooted by s, and Ms is the set of
stable models associated with the subset of the knowledge base  which contains only rules
about atoms in As . Initially, Ms is empty for every s. The algorithm traverses G from
the bottom up. When at a node s, it first combines all the submodels of the children of s
into a single set of models Mc(s) . If s is a source, then Mc(s) is set to f;g3. Next, for each
model m in Mc(s) , AAS converts s to a knowledge base sm using the GL transform and
other transformations that depend on the atoms in m; then, it finds all the stable models
of sm and combines them with m. The set Ms is obtained by repeating this operation for
each m in Mc(s) . AAS uses the procedure CartesProd (Figure 8), which receives as input
several sets of models and returns the consistent portion of their Cartesian product. If one
of the sets of models which CartesProd gets as input is the empty set, CartesProd will
output an empty set of models. The procedure Convert gets as input a knowledge base ,
a model m, and a set of atoms s, and performs the following: for each atom P in m, each
positive occurrence of P is deleted from the body of each rule in ; for each rule in , if
not P is in the body of the rule and P 2 m, then the rule is deleted from ; if not P is
in the body of a rule and P 2= m, then, if P 2= s, not P is deleted from that body. The
procedure All-Stable called by AAS may be one of the procedures previously presented
(All-Stable1 or All-Stable2) or it may be any other procedure that generates all stable
models.

Example 4.1 Suppose AAS is called to compute the stable models of 0. Suppose further

that the algorithm traverses the super dependency graph in Figure 2 in the order flion,
dolphin, mammal, ab1, on land, warm blooded, female-maleg (recall that all the nodes inside the square make up one node that we are calling female-male or, for short, FM).
After visiting all the nodes except the last, we have Mlion = ffliongg, Mdolphin = f;g,
Mmammal = fflion; mammalgg, Mon land = fflion; mammal; onlandgg, Mwarm blooded =
fflion; mammal; warm bloodedgg. When visiting the node FM, we have after step 1.c that
Mc(FM ) = Mmammal . So step 1.d loops only once, for m = flion; mammalg. Recall that
FM is the knowledge base

female
male

, mammal; not male
, mammal; not female

3. Note the difference between f;g, which is a set of one model - the model that assigns
atoms, and ;, which is a set that contains no models.

38

false

to all the

fiA Hierarchy of Tractable Subsets

Acyclic-All-Stable()

Input: A knowledge base .
Output: The set of all stable models of .
1. Traverse G from the bottom up. For each node s, do:
(a) Ms := ;;
(b) Let s1 ; :::; sj be the children of s.
(c) If j = 0, then Mc(s) := f;g;
else Mc(s) := CartesProd(fMs1 ; :::; Msj g);
(d) For each m 2 Mc(s) , do:
i. sm := Convert(s ; m; s);
ii. M := All-Stable(sm );
S
iii. If M 6= ;, then Ms := Ms CartesProd(ffmg; M g);
2. Output CartesProd(fMs1 ; :::; Msk g), where s1 ; :::; sk are the sinks of G .
Figure 7: Algorithm Acyclic-All-Stable (AAS)
CartesProd(M)
Input: A set of sets of models M.
Output: A set of models which is the consistent portion of the Cartesian product of the
sets in M.
1. If M has a single element fE g, then return E ;
2. M := ;;
3. Let M 0 2 M;
4. D := CartesProd(M n fM 0g);
5. For each d in D, do:
(a) For each m in M 0 , do:
S
If m and d are consistent, then M := M fm + dg;
(b) EndFor;
6. EndFor;
7. Return M ;
Figure 8: Procedure CartesProd
39

fiBen-Eliyahu
After executing step 1.d.i, we have FM m set to

female
male

, not male
, not female

The above knowledge base has two stable models: ffemaleg and fmaleg. The Cartesian
product of the above set with flion; mammalg yields MFM = fflion; mammal; femaleg;
flion; mammal; malegg. At step 2, the Cartesian product of Mwarm blooded , Mon land, and
MFM is taken. Thus, the algorithm outputs fflion; mammal; on land; warm blooded; femaleg,
flion; mammal; on land; warm blooded; malegg, and these are indeed the two stable models
of 0 . Note that algorithm AAS is more ecient than either All-Stable1 or All-Stable2
on the knowledge base 0 .

Theorem 4.2 Algorithm AAS is correct, that is, m is a stable model of a knowledge base
 iff m is generated by AAS when applied to .
Proof: Let s0; s1; :::; sn be the ordering of the nodes of the super dependency graph by

which the algorithm is executed. We can show by induction on i that AAS, when at node
si, generates all and only the stable models of the portion of the knowledge base composed
of rules that only use atoms from Asi .
case i = 0: In this case, at step 1.d.ii of AAS, sm = s; thus, the claim follows from the
correctness of the algorithm All-Stable called in step 1.d.ii.
case i > 0: Showing that every model generated is stable is straightforward, by the induction hypothesis and Theorem 2.2. The other direction is: suppose m is a stable model
of s ; show that m is generated. Clearly, for each child s of si , the projection of m
onto As is a stable model of the part of the knowledge base that uses only atoms from
As. By induction, mc , which is the projection of m onto the union of As for every
child s of si , must belong to Mc(si ) computed at step 1.c. Therefore, to show that m
is generated, we need only show that m0 = m , mc is a stable model of simc . This
is easily done using Theorem 2.2.
We will now analyze the complexity of AAS. First, given a knowledge base  and a
set of atoms s, we define ^ s to be the knowledge base obtained from  by deleting each
negative occurrence of an atom that does not belong to s from the body of every rule.
For example, if  = fa ,not b; c ,not d; ag and s = fbg, then ^ s = fa ,not b; c ,ag.
While visiting a node s during the execution of AAS, we have to compute at step 1.d.ii all
stable models of some knowledge base sm . Using either All-Stable1 or All-Stable2,
the estimated time required to find all stable models of sm is shorter than or equal to the
time required to find all stable models of ^ s . This occurs because the number of negative
atoms and the number of rules with negative atoms in their bodies in ^ s is higher than
or equal to the number of negative atoms and the number of rules with negative atoms in
their bodies in sm , regardless of what m is. Thus, if ^ s is a Horn knowledge base, we can
find the stable model of ^ s , and hence of sm , in polynomial time, no matter what m is.
40

fiA Hierarchy of Tractable Subsets
If ^ s is not positive, then we can find all stable models of ^ s , and hence of sm , in time
min(ln  2k ; ln  2c ), where l is the length of ^ s , n the number of atoms used in ^ s , c the
number of rules in ^ s that contain negative atoms, and k the number of atoms that appear
negatively in ^ s .
Then, with each knowledge base , we associate a number t as follows. Associate a
number vs with every node in G . If ^ s is a Horn knowledge base, then vs is 1; else, vs is
min(2k ; 2c), where c is the number of rules in ^ s that contain negative atoms from s, and
k is the number of atoms from s that appear negatively in ^ s . Now associate a number ts
with every node s. If s is a leaf node, then ts = vs . If s has children s1 ; :::; sj in G , then
ts = vs  ts1  :::  tsj . Define t to be ts1  :::  tsk , where s1 ; :::; sk are all the sink nodes in
G .
Definition 4.3 A knowledge base  belongs to 
j if t = j .
Theorem 4.4 If a knowledge base belongs to 
j for some j , then it has at most j stable
models that can be computed in time O(lnj ).
Proof: By induction on j . The dependency graph and the super dependency graph are
both built in time linear in the size of the knowledge base. So we may only consider the
time it takes to compute all stable models with the super dependency graph given.
case j = 1:  2 
1 means that for every node s in G, ^ s is a Horn knowledge base. In
other words,  is stratified, and therefore it has exactly one stable model. There are
at most n nodes in the graph. At each node, the loop in step 1.d is executed at most
once, because at most one model is generated at every node. Procedure Convert runs
in time O(ls), where ls is the length of s (we assume that m is stored in an array
where the access to each atom is in constant time). Since, for every node s, ^ s is a
Horn knowledge base, sm is computed in time O(lsn). Thus, the overall complexity
is O(ln).
case j > 1: By induction on n, the number of nodes in the super dependency graph of .
case n = 1: Let s be the single node in G . Thus, j = vs. Using the algorithms from
Section 3, all stable models of  = s can be found in time O(lnvs ), and  has
at most vs models.
case n > 1: Assume without loss of generality that G has a single sink s (to get a
single sink, we can add to the program the rule P , s1 ; ::; sk, where s1 ; :::; sk are
all the sinks and P is a new atom). Let c1 ; :::; ck be the children of s. For each
child ci , (ci ), the part of the knowledge base which corresponds to the subgraph
rooted by ci, must belong to 
ti for some ti  j . By induction on n, for each
child node ci, all stable models of (ci ) can be computed in time O(lnti ), and
(ci) has at most ti stable models. Now let us observe what happens when AAS
is visiting node s. First, the Cartesian product of all the models computed at the
child nodes is taken. This is executed in time O(n  t1  :::  tk), and yields at most
t1  :::  tk models in Mc(s) . For every m 2 Mc(s) , we call Convert (O(ln)) and
compute all the stable models of sm (O(lnvs)). We then combine them with m
using CartesProd (O(nvs )). Thus, the overall complexity of computing Ms , that
is, of computing all the stable models of , is O(lnt1  :::  tk  vs ) = O(lnj ).
41

fiBen-Eliyahu

Note that all stratified knowledge bases belong to 
1 , and the more that any knowledge
base looks stratified, the more ecient algorithm AAS will be.
Given a knowledge base , it is easy to find the minimum j such that  belongs to 
j .
This follows because building G and finding c and k for every node in G are polynomialtime tasks. Hence,
Theorem 4.5 Given a knowledge base , we can find the minimum j such that  belongs
to 
j in polynomial time.
Example 4.6 For all the nodes s in G0 except FM, vs =1. vFM = 2. Thus, 0 2 
2. 1
is a stratified knowledge base and therefore belongs to 
1.

not

male

not
female
warm_blood

on_land
mammal

lion

fly

not

not
ab1
dolphin

bird

ab2

penguin

S

Figure 9: The super dependency graph of 0 1
The next example shows that step 5 of procedure CartesProd is necessary.
Example 4.7 Consider knowledge base 4:
a , not b
b , not a

c
d
e
f

,
,
,
,

42

a
b
c; d
c

fiA Hierarchy of Tractable Subsets

f

e

c

d

not

a

b
not

Figure 10: Super dependency graph of 4

not

not

c

c

not

not

a

a
b

b

not

not

(1)

(2)

Figure 11: Dependency graph (1) and super dependency graph (2) of 2
43

fiBen-Eliyahu
The super dependency graph of 4 is shown in Figure 10. During the run of algorithm AAS,
Mab (the set of models computed at the node fa; bg) is set to ffa; :bg; f:a; bgg. When AAS
visits nodes c and d, we get Mc = ffa; :b; cg; f:a; bgg, Md = ff:a; b; dg; fa; :bgg. When
AAS visits node e, CartesProd is called on the input fMc ; Mdg, yielding the output Me =
ffa; :b; cg; f:a; b; dgg. Note that CartesProd does not output any model in which both c
and d are true, because the models fa; :b; cg and f:a; b; dg are inconsistent and CartesProd
checks for consistency in step 5. When visiting node f , we get Mf = ffa; :b; c; f g; f:a; bgg.
AAS then returns CartesProd(fMe; Mf g), which is ffa; :b; c; f g; f:a; b; dgg.
The next example demonstrates that some models generated at some nodes of the super dependency graph during the run of AAS may later be deleted, since they cannot be
completed to a stable model of the whole knowledge base.

Example 4.8 Consider knowledge base 2:
a
b
c

, not b
, not a
, a; not c

The dependency graph and the super dependency graph of 2 are shown in Figure 11.
During the run of algorithm AAS, Mab (the set of models computed at the node fa; bg) is
set to ffag; fbgg. However, only fbg is a stable model of 2 .
Despite the deficiency illustrated in Example 4.8, algorithm AAS does have desirable
features. First, AAS enables us to compute stable models in a modular fashion. We can use
G as a structure in which to store the stable models. Once the knowledge base is changed,
we need to resume computation only at the nodes affected by the change. For example,
suppose that after computing the stable models of the knowledge base 0 , we add toS 0
the knowledge base 1 of Example 2.6, which gives us a new knowledge base, 3 = 0 1.
The super dependency graph of the new knowledge base 3 is shown in Figure 9. Now we
need only to compute the stable models at the nodes penguin, bird, ab2, y, and on land
and then to combine the models generated at the sinks. We do not have to re-compute the
stable models at all the other nodes as well.
Second, in using the AAS algorithm, we do not always have to compute all stable models
up to the root node. If we are queried about an atom that is somewhere in the middle of
the graph, it is often enough to compute only the models of the subgraph rooted by the
node that represents this atom. For example, suppose we are given the knowledge base
2 and asked if mammal is true in every stable model of 2 . We can run AAS for the
nodes dolphin, lion, and mammal | and then stop. If mammal is true in all the stable
models computed at the node mammal (i.e., in all the models in Mmammal ), we answer
\yes", otherwise, we must continue the computation.
Third, the AAS algorithm is useful in computing the labeling of a TMS subject to
nogoods. A set of nodes of a TMS can be declared nogood, which means that all acceptable
labeling should assign false to at least one node in the nogood set.4 In stable models
terminology, this means that when handling nogoods, we look for stable models in which
4. In logic programming terminology nogoods are simply integrity constraints.

44

fiA Hierarchy of Tractable Subsets
at least one atom from a nogood is false. A straightforward approach would be to first
compute all the stable models and then choose only the ones that comply with the nogood
constraints. But since the AAS algorithm is modular and works from the bottom up,
in many cases it can prevent the generation of unwanted stable models at an early stage.
During the computation, we can exclude the submodels that do not comply with the nogood
constraints and erase these submodels from Ms once we are at a node s in the super
dependency graph such that As includes all the members of a certain nogood.

5. Computing Stable Models of First-Order Knowledge Bases
In this section, we show how we can generalize algorithm AAS so that it can find all stable
models of a knowledge base over a first-order language with no function symbols. The new
algorithm will be called First-Acyclic-All-Stable (FAAS).
We will now refer to a knowledge base as a set of rules of the form

C ,A1; A2; :::; Am; not B1; :::; not Bn

(14)

where all As, B s, and C are atoms in a first-order language with no function symbols. The
definitions of head, body, and positive and negative appearances of an atom are the same
as in the propositional case. In the expression p(X1; :::; Xn), p is called a predicate name.
As in the propositional case, every knowledge base  is associated with a directed graph
called the dependency graph of , in which (a) each predicate name in  is a node, (b)
there is a positive arc directed from a node p to a node q iff there is a rule in  in which
p is a predicate name in one of the Ai s and q is a predicate name in the head, and (c)
there is a negative arc directed from a node p to a node q iff there is a rule in  in which
p is a predicate name in one of the Bi s and q is a predicate name in the head. The super
dependency graph, G , is defined in an analogous manner. We define a stratified knowledge
base to be a knowledge base in which there are no cycles through the negative edges in the
dependency graph of the knowledge base.
A knowledge base will be called safe iff each of its rules is safe. A rule is safe iff all the
variables appearing in the head of the rule or in predicates appearing negative in the rule
also appear in positive predicates in the body of the rule. In this section, we assume that
knowledge bases are safe. The Herbrand base of a knowledge base is the set of all atoms
constructed using predicate names and constants from the knowledge base. The set of
ground instances of a rule is the set of rules obtained by consistently substituting variables
from the rule with constants that appear in the knowledge base in all possible ways. The
ground instance of a knowledge base is the union of all ground instances of its rules. Note
that the ground instance of a first-order knowledge base can be viewed as a propositional
knowledge base.
A model for a knowledge base is a subset M of the knowledge base's Herbrand base.
This subset has the property that for every rule in the grounded knowledge base, if all the
atoms that appear positive in the body of the rule belong to M and all the atoms that
appear negative in the body of the rule do not belong to M , then the atom in the head of
the rule belongs to M . A stable model for a first-order knowledge base  is a Herbrand
model of , which is also a stable model of the grounded version of .
45

fiBen-Eliyahu

First-Acyclic-All-Stable()

Input: A first-order knowledge base .
Output: All the stable models of .
1. Traverse G from the bottom up. For each node s, do:
(a) Ms := ;;
(b) Let s1 ; :::; sj be the children of s;
(c) Mc(s) := CartesProd(fMs1 ; :::; Msj g);
(d) For each m 2 Mc(s) do
Ms := MsSall-stable(s SfP ,jP 2 mg)
2. Output CartesProd(fMs1 ; :::; Msk g), where s1 ; :::; sk are the sinks of G .
Figure 12: Algorithm First-Acyclic-All-Stable (FAAS)
We now present FAAS, an algorithm that computes all stable models of a first-order
knowledge base. Let  be a first-order knowledge base. As in the propositional case, with
each node s in G (the super dependency graph of ), we associate s , As , and Ms . s is
the subset of  containing all the rules about predicates whose names are in s. As is the
set of all predicate names P that appear in the subgraph of G rooted by s. Ms are the
stable models associated with the sub{knowledge base of  that contains only rules about
predicates whose names are in As . Initially, Ms is empty for every s. Algorithm FAAS
traverses G from the bottom up. When at a node s, the algorithm first combines all the
submodels of the children of s into a single set of models, Mc(s) . Then, for each model
m in Mc(s), it calls a procedure that finds all the stable models of s union the set of all
unit clauses P , where P 2 m. The procedure All-Stable called by FAAS can be any
procedure that computes all the stable models of a first-order knowledge base. Because
procedure All-Stable computes stable models for only parts of the knowledge base, it
may take advantage of some fractions of the knowledge base being stratified or having any
other property that simplifies computation of the stable models of a fraction.

Theorem 5.1 Algorithm FAAS is correct, that is, m is a stable model of a knowledge base
 iff m is one of the models in the output when applying FAAS to .

Proof: As the proof of Theorem 4.2.

Note that the more that a knowledge base appears stratified, the more ecient algorithm
FAAS becomes.

Example 5.2 Consider knowledge base 5:
warm blooded(X )
live on land(X )
female(X )

, mammal(X )
, mammal(X ); not ab1(X )
, mammal(X ); not male(X )
46

fiA Hierarchy of Tractable Subsets
male(X )
mammal(X )
ab1(X )
mammal(X )
dolphin(flipper)

,
,
,
,
,

mammal(X ); not female(X )
dolphin(X )
dolphin(X )
lion(X )

, bird(X )
, bird(X ); not ab2(X )
, penguin(X )
, penguin(X )
,
The super dependency graph of 5 , G5 , is the same as the super dependency graph of
live on land(X )
fly (X )
bird(X )
ab2(X )
bird(bigbird)

the knowledge base 2 (see Figure 9). Observe that when at node mammal, for example,
S
in step 1.d the algorithm looks for all stable models of the knowledge base 0 = mammal
f ,dolphin(flipper)g, where mammal =fmammal(X ) ,dolphin(X ); mammal(X ) ,lion(X )g.
0 is a stratified knowledge base that has a unique stable model that can be found eciently.
Hence, algorithm FAAS saves us from having to ground all the rules of the knowledge base
before starting to calculate the models, and it can take advantage of parts of the knowledge
base being stratified.

6. Related Work

In recent years, quite a few algorithms have been developed for reasoning with stable models.
Nonetheless, as far as we know, the work presented here is original in the sense that it
provides a partition of the set of all the knowledge bases into a hierarchy of tractable
classes. The partition is based on the structure of the dependency graph. Intuitively, the
task of computing all the stable models of a knowledge base using algorithm AAS becomes
increasingly complex as the \distance" of the knowledge base from being stratified becomes
larger. Next, we summarize the work that seems to us most relevant.
Algorithm AAS is based on an idea that appears in the work of Lifschitz and Turner
(1994), where they show that in many cases a logic program can be divided into two parts,
such that one part, the \bottom" part, does not refer to the predicates defined in the \top"
part. They then explain how the task of computing the stable models of a program can be
simplified when the program is split into parts. Algorithm AAS, using the superstructure
of the dependency graph, exploits a specific method for splitting the program.
Bell et al. (1994) and Subrahmanian et al. (1995) implement linear and integer programming techniques in order to compute stable models (among other nonmonotonic logics). However, it is dicult to assess the merits of their approaches in terms of complexity.
Ben-Eliyahu and Dechter (1991) illustrate how a knowledge base  can be translated into
a propositional theory T such that each model of the latter corresponds to a stable model
of the former. It follows from this that the problem of finding all the stable models of
a knowledge base corresponds to the problem of finding all the models of a propositional
theory. Satoh and Iwayama (1991) provide a nondeterministic procedure for computing
47

fiBen-Eliyahu
the stable models of logic programs with integrity constraints. Junker and Konolige (1990)
present an algorithm for computing TMS' labels. Antoniou and Langetepe (1994) introduce
a method for representing some classes of default theories as normal logic programs in such
a way that SLDNF-resolution can be used to compute extensions. Pimentel and Cuadrado
(1989) develop a label-propagation algorithm that uses data structures called compressible
semantic trees in order to implement a TMS; their algorithm is based on stable model semantics. The algorithms developed by Marek and Truszczynski (1993) for autoepistemic
logic can also be adopted for computing stable models. The procedures by Marek and
Truszczynski (1993), Antoniou and Langetepe (1994), Pimentel and Cuadrado (1989), BenEliyahu and Dechter (1991), Satoh and Iwayama (1991), Bell et al. (1994), Subrahmanian
et al. (1995), and Junker and Konolige (1990) do not take advantage of the structure of
the knowledge base as reected in its dependency graph, and therefore are not ecient for
stratified knowledge bases.
Sacca and Zaniolo (1990) present a backtracking fixpoint algorithm for constructing one
stable model of a first-order knowledge base. This algorithm is similar to algorithm AllStable2 presented here in Section 3 but its complexity is worse than the complexity of
All-Stable2. They show how the backtracking fixpoint algorithm can be modified to
handle stratified knowledge bases in an ecient manner, but the algorithm needs further
adjustments before it can deal eciently with knowledge bases that are very close to being
stratified. Leone et al. (1993) present an improved backtracking fixpoint algorithm for
computing one stable model of a Datalog: program and discuss how the improved algorithm
can be implemented. One of the procedures called by the improved algorithm is based on
the backtracking fixpoint algorithm of Sacca and Zaniolo (1990). Like the backtracking
fixpoint algorithm, the improved algorithm as is does not take advantage of the structure
of the program, i.e., it is not ecient for programs that are close to being stratified.
Several tractable subclasses for computing extensions of default theories (and, hence,
computing stable models) are known (Kautz & Selman, 1991; Papadimitriou & Sideri,
1994; Palopoli & Zaniolo, 1996; Dimopoulos & Magirou, 1994; Ben-Eliyahu & Dechter,
1996). Some of these tractable subclasses are characterized using a graph that reects
dependencies in the program between atoms and rules. The algorithms presented in these
papers are complete only for a subclass of all knowledge bases, however. Algorithms for
computing extensions of stratified default theories or extensions of default theories that
have no odd cycles (in some precise sense) are given by Papadimitriou and Sideri (1994)
and Cholewinski (1995a, 1995b).
Algorithms for handling a TMS with nogoods have been developed in the AI community by Doyle (1979) and Charniak et al. (1980). But, as Elkan (1990) points out, these
algorithms are not always faithful to the semantics of the TMS and their complexities have
not been analyzed. Dechter and Dechter (1994) provide algorithms for manipulating a TMS
when it is represented as a constraint network. The eciency of their algorithms depends
on the structure of the constraint network representing the TMS, and the structure they
employ differs from the dependency graph of the knowledge base.
48

fiA Hierarchy of Tractable Subsets

7. Conclusion
The task of computing stable models is at the heart of several systems central to AI,
including TMSs, autoepistemic logic, and default logic. This task has been shown to be
NP-hard. In this paper, we present a partition of the set of all knowledge bases to classes

1 ; 
2; :::, such that if a knowledge base  is in 
k , then  has at most k stable models,
and they may all be found in time O(lnk), where l is the length of the knowledge base and
n the number of atoms in . Moreover, for an arbitrary knowledge base , we can find the
minimum k such that  belongs to 
k in time linear in the size of . Intuitively, the more
the knowledge base is stratified, the more ecient our algorithm becomes. We believe that
beyond stratified knowledge bases, the more expressive the knowledge base is (i.e. the more
rules with nonstratified negation in the knowledge base), the less likely it will be needed.
Hence, our analysis should be quite useful. In addition, we show that algorithm AAS has
several advantages in a dynamically changing knowledge base, and we provide applications
for answering queries and implementing a TMS's nogood strategies. We also illustrate a
generalization of algorithm AAS for the class of first-order knowledge bases.
Algorithm AAS can easily be adjusted to find only one stable model of a knowledge
base. While traversing the super dependency graph, we generate only one model at each
node. If we arrive at a node where we cannot generate a model based on what we have
computed so far, we backtrack to the most recent node where several models were available
to choose from and take the next model that was not yet chosen. The worst-case time
complexity of this algorithm is equal to the worst-case time complexity of the algorithm for
finding all stable models because we may have to exhaust all possible ways of generating a
stable model before finding out that a certain knowledge base does not have a stable model
at all. Nevertheless, we believe that in the average case, finding just one model will be
easier than finding them all. A similar modification of the AAS algorithm is required if we
are interested in finding one model in which one particular atom gets the value true.
This work is another attempt to bridge the gap between the declarative systems (e.g.,
default logic, autoepistemic logic) and the procedural systems (e.g., ATMs, Prolog) of the
nonmonotonic reasoning community. It is argued that while the declarative methods are
sound, they are impractical since they are computationally expensive, and while the procedural methods are more ecient, it is dicult to completely understand their performance
or to evaluate their correctness. The work presented here illustrates that the declarative
and the procedural approaches can be combined to yield an ecient yet formally supported
nonmonotonic system.

Acknowledgments
Thanks to Luigi Palopoli for useful comments on earlier draft of this paper and to Michelle
Bonnice and Gadi Dechter for editing on parts of the manuscript. Many thanks to the
anonymous referees for very useful comments.
Some of this work was done while the author was visiting the Cognitive Systems Laboratory, Computer Science Department, University of California, Los Angeles, California,
USA. This work was partially supported by NSF grant IRI-9420306 and by Air Force Oce
of Scientific Research grant #F49620-94-1-0173.
49

fiBen-Eliyahu

References

Antoniou, G., & Langetepe, E. (1994). Soundness and completeness of a logic programming
approach to default logic. In AAAI-94: Proceedings of the 12th national conference
on artificial intelligence, pp. 934{939. AAAI Press, Menlo Park, Calif.
Apt, K., Blair, H., & Walker, A. (1988). Towards a theory of declarative knowledge. In
Minker, J. (Ed.), Foundations of deductive databases and logic programs, pp. 89{148.
Morgan Kaufmann.
Bell, C., Nerode, A., Ng, R., & Subrahmanian, V. (1994). Mixed integer programming
methods for computing non-monotonic deductive databases. Journal of the ACM,
41 (6), 1178{1215.
Ben-Eliyahu, R., & Dechter, R. (1994). Propositional semantics for disjunctive logic programs. Annals of Mathematics and Artificial Intelligence, 12, 53{87. A short version
appears in JICSLP-92: Proceedings of the 1992 joint international conference and
symposium on logic programming.
Ben-Eliyahu, R., & Dechter, R. (1996). Default reasoning using classical logic. Artificial
Intelligence, 84 (1-2), 113{150.
Bidoit, N., & Froidevaux, C. (1987). Minimalism subsumes default logic and circumscription
in stratified logic programming. In LICS-87: Proceedings of the IEEE symposium on
logic in computer science, pp. 89{97. IEEE Computer Science Press, Los Alamitos,
Calif.
Charniak, E., Riesbeck, C. K., & McDermott, D. V. (1980). Artificial Intelligence Programming, chap. 16. Lawrence Erlbaum, Hillsdale, NJ.
Cholewinski, P. (1995a). Reasoning with stratified default theories. In Marek, W. V.,
Nerode, A., & Truszczynski, M. (Eds.), Logic programming and nonmonotonic reasoning: proceedings of the 3rd international conference, pp. 273{286. Lecture notes in
computer science, 928. Springer-Verlag, Berlin.
Cholewinski, P. (1995b). Stratified default theories. In Pacholski, L., & Tiuryn, A. (Eds.),
Computer science logic: 8th workshop, CSL'94: Selected papers, pp. 456{470. Lecture
notes in computer science, 933. Springer-Verlag, Berlin.
Dechter, R., & Dechter, A. (1996). Structure-driven algorithms for truth maintenance.
Artificial Intelligence, 82 (1-2), 1{20.
Dimopoulos, Y., & Magirou, V. (1994). A graph-theoretic approach to default logic. Journal
of Information and Computation, 112, 239{256.
Doyle, J. (1979). A truth-maintenance system. Artificial Intelligence, 12, 231{272.
Dung, P. M. (1991). Negation as hypothesis: An abductive foundation for logic programming. In Furukawa, K. (Ed.), ICLP-91: Proceedings of the 8th international conference
on logic programming, pp. 3{17. MIT Press.
50

fiA Hierarchy of Tractable Subsets
Elkan, C. (1990). A rational reconstruction of nonmonotonic truth maintenance systems.
Artificial Intelligence, 43, 219{234.
Eshghi, K., & Kowalski, R. A. (1989). Abduction compared with negation by failure. In Levi,
G., & Martelli, M. (Eds.), ICLP-89: Proceedings of the 6th international conference
on logic programming, pp. 234{254. MIT Press.
Fine, K. (1989). The justification of negation as failure. Logic, Methodology and Philosophy
of Science, 8, 263{301.
Gelfond, M. (1987). On stratified autoepistemic theories. In AAAI-87: Proceedings of the
5th national conference on artificial intelligence, pp. 207{211. Morgan Kaufmann.
Gelfond, M., & Lifschitz, V. (1988). The stable model semantics for logic programming. In
Kowalski, R. A., & Bowen, K. A. (Eds.), Logic Programming: Proceedings of the 5th
international conference, pp. 1070{1080. MIT Press.
Gelfond, M., & Lifschitz, V. (1991). Classical negation in logic programs and disjunctive
databases. New Generation Computing, 9, 365{385.
Junker, U., & Konolige, K. (1990). Computing the extensions of autoepistemic and default logics with a TMS. In AAAI-90: Proceedings of the 8th national conference on
artificial intelligence, pp. 278{283. AAAI Press.
Kakas, A. C., & Mancarella, P. (1991). Stable theories for logic programs. In Saraswat,
V., & Udea, K. (Eds.), ISLP-91: Proceedings of the 1991 international symposium on
logic programming, pp. 85{100. MIT Press.
Kautz, H. A., & Selman, B. (1991). Hard problems for simple default logics. Artificial
Intelligence, 49, 243{279.
Leone, N., Romeo, N., Rullo, M., & Sacca, D. (1993). Effective implementation of negation
in database logic query languages. In Atzeni, P. (Ed.), LOGIDATA+: Deductive
database with complex objects, pp. 159{175. Lecture notes in computer science, 701.
Springer-Verlag, Berlin.
Lifschitz, V., & Turner, H. (1994). Splitting a logic program. In Van Hentenryck, P. (Ed.),
ICLP-94: Proceedings of the 11th international conference on logic programming, pp.
23{37. MIT Press.
Marek, V. W., & Truszczynski, M. (1993). Nonmonotonic logic: Context-dependent reasoning. Springer Verlag, Berlin.
Marek, W., & Truszczynski, M. (1991). Autoepistemic logic. Journal of the ACM, 38,
588{619.
Moore, R. C. (1985). Semantical consideration on nonmonotonic logic. Artificial Intelligence, 25, 75{94.
Palopoli, L., & Zaniolo, C. (1996). Polynomial-time computable stable models.. Annals of
Mathematics and Artificial Intelligence, in press.
51

fiBen-Eliyahu
Papadimitriou, C. H., & Sideri, M. (1994). Default theories that always have extensions.
Artificial Intelligence, 69, 347{357.
Pimentel, S. G., & Cuadrado, J. L. (1989). A truth maintenance system based on stable
models. In Lusk, E. L., & Overbeek, R. A. (Eds.), ICLP-89: Proceedings of the 1989
North American conference on logic programming, pp. 274{290. MIT Press.
Przymusinska, H., & Przymusinski, T. (1990). Semantic issues in deductive databases and
logic programs. In Banerji, R. B. (Ed.), Formal techniques in artificial intelligence:
A sourcebook, pp. 321{367. North-Holland, New York.
Reiter, R. (1980). A logic for default reasoning. Artificial Intelligence, 13, 81{132.
Sacca, D., & Zaniolo, C. (1990). Stable models and non-determinism in logic programs with
negation. In PODS-90: Proceedings of the 9th ACM SIGACT-SIGMOD-SIGART
symposium on principles of database systems, pp. 205{217. ACM Press.
Satoh, K., & Iwayama, N. (1991). Computing abduction by using the TMS. In Furukawa, K.
(Ed.), ICLP-91: Proceedings of the 8th international conference on logic programming,
pp. 505{518. MIT Press.
Subrahmanian, V., Nau, D., & Vago, C. (1995). WFS + branch and bound = stable models.
IEEE Transactions on Knowledge and Data Engineering, 7 (3), 362{377.
Tarjan, R. (1972). Depth-first search and linear graph algorithms. SIAM Journal on
Computing, 1, 146{160.

52

fi